- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:33:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:33:29'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2404.06114] Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2404.06114] 通信高效的大规模分布式深度学习：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06114](https://ar5iv.labs.arxiv.org/html/2404.06114)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06114](https://ar5iv.labs.arxiv.org/html/2404.06114)
- en: 'Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通信高效的大规模分布式深度学习：全面调查
- en: 'Feng Liang, , Zhen Zhang, Haifeng Lu, Victor C. M. Leung, , Yanyi Guo^∗, Xiping Hu^∗
    ^∗ Corresponding authors.Feng Liang and Xiping Hu are with a) Artificial Intelligence
    Research Institute, Shenzhen MSU-BIT University, Shenzhen 518000, China, and b)
    Guangdong-Hong Kong-Macao Joint Laboratory for Emotional Intelligence and Pervasive
    Computing, Shenzhen MSU-BIT University, Shenzhen 518107, China. (e-mail: {fliang,
    huxp}@smbu.edu.cn)Zhen Zhang and Haifeng Lu are with a) Gansu Provincial Key Laboratory
    of Wearable Computing, School of information Science and Engineering, Lanzhou
    University, Gansu 730000, China, and b) Guangdong-Hong Kong-Macao Joint Laboratory
    for Emotional Intelligence and Pervasive Computing, Shenzhen MSU-BIT University,
    Shenzhen 518107, China. (e-mail: {zhangzhen19, luhf18}@lzu.edu.cn)Victor C. M.
    Leung is with a) Artificial Intelligence Research Institute, Shenzhen MSU-BIT
    University, Shenzhen 518000, China, and b) the Department of Electrical and Computer
    Engineering, The University of British Columbia, Vancouver, Canada. (e-mail: vleung@ieee.org)Yanyi
    Guo is with a) Frontier Cross Disciplinary Research Institute, Shenzhen MSU-BIT
    University, Shenzhen 518000, China, and b) the School of Mechanical and Electrical
    Engineering, Beijing Institute of Technology, Beijing 10081, China. (e-mail: guoyy@smbu.edu.cn)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Feng Liang**、**Zhen Zhang**、**Haifeng Lu**、**Victor C. M. Leung**、**Yanyi
    Guo**^∗，**Xiping Hu**^∗ ^∗ 通讯作者。**Feng Liang** 和 **Xiping Hu** 供职于 a) 人工智能研究所，深圳MSU-BIT大学，深圳
    518000，中国，以及 b) 粤港澳情感智能与普适计算联合实验室，深圳MSU-BIT大学，深圳 518107，中国。（电子邮件: {fliang, huxp}@smbu.edu.cn）**Zhen
    Zhang** 和 **Haifeng Lu** 供职于 a) 甘肃省可穿戴计算重点实验室，信息科学与工程学院，兰州大学，甘肃 730000，中国，以及 b)
    粤港澳情感智能与普适计算联合实验室，深圳MSU-BIT大学，深圳 518107，中国。（电子邮件: {zhangzhen19, luhf18}@lzu.edu.cn）**Victor
    C. M. Leung** 供职于 a) 人工智能研究所，深圳MSU-BIT大学，深圳 518000，中国，以及 b) 电气与计算机工程系，英属哥伦比亚大学，加拿大温哥华。（电子邮件:
    vleung@ieee.org）**Yanyi Guo** 供职于 a) 前沿跨学科研究所，深圳MSU-BIT大学，深圳 518000，中国，以及 b) 机械与电气工程学院，北京理工大学，北京
    10081，中国。（电子邮件: guoyy@smbu.edu.cn）'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the rapid growth in the volume of data sets, models, and devices in the
    domain of deep learning, there is increasing attention on large-scale distributed
    deep learning. In contrast to traditional distributed deep learning, the large-scale
    scenario poses new challenges that include fault tolerance, scalability of algorithms
    and infrastructures, and heterogeneity in data sets, models, and resources. Due
    to intensive synchronization of models and sharing of data across GPUs and computing
    nodes during distributed training and inference processes, communication efficiency
    becomes the bottleneck for achieving high performance at a large scale. This article
    surveys the literature over the period of 2018-2023 on algorithms and technologies
    aimed at achieving efficient communication in large-scale distributed DL at various
    levels, including algorithms, frameworks, and infrastructures. Specifically, we
    first introduce efficient algorithms for model synchronization and communication
    data compression in the context of large-scale distributed training. Next, we
    introduce efficient strategies related to resource allocation and task scheduling
    for use in distributed training and inference. After that, we present the latest
    technologies pertaining to modern communication infrastructures used in distributed
    deep learning, including GPU interconnects, programmable network devices, collective
    communication protocols, and communication topologies. We focus our discussion
    of these topics on examining the impact of the communication overhead in a large-scale
    and heterogeneous setting. Finally, we conduct a case study on the distributed
    training of large language models at a large scale to illustrate how to apply
    these technologies in real cases. This article aims to offer researchers a comprehensive
    understanding of the current landscape of large-scale distributed deep learning
    and to reveal promising future research directions toward communication-efficient
    solutions in this scope.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习领域中数据集、模型和设备数量的快速增长，大规模分布式深度学习受到了越来越多的关注。与传统的分布式深度学习相比，大规模场景带来了新的挑战，包括容错能力、算法和基础设施的可扩展性以及数据集、模型和资源的异质性。由于在分布式训练和推理过程中模型的密集同步以及跨GPU和计算节点的数据共享，通信效率成为在大规模环境中实现高性能的瓶颈。本文回顾了2018年至2023年间在大规模分布式深度学习中实现高效通信的算法和技术的文献，涵盖了算法、框架和基础设施等各个层面。具体来说，我们首先介绍了在大规模分布式训练中，模型同步和通信数据压缩的高效算法。接下来，我们介绍了与资源分配和任务调度相关的高效策略，这些策略用于分布式训练和推理。之后，我们呈现了现代通信基础设施的最新技术，包括GPU互连、可编程网络设备、集体通信协议和通信拓扑。我们讨论这些主题时，重点考察了通信开销在大规模和异质性环境中的影响。最后，我们通过对大规模语言模型分布式训练的案例研究，展示了如何在实际案例中应用这些技术。本文旨在为研究人员提供对大规模分布式深度学习当前状况的全面了解，并揭示未来在该领域中通信高效解决方案的有前景的研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Communication-efficient, distributed deep learning, scalability, federated learning,
    heterogeneity, LLM, large models, large scale, pipeline parallelism.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通信效率、分布式深度学习、可扩展性、联邦学习、异质性、LLM、大模型、大规模、流水线并行。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 介绍
- en: Due to the rapid advancements in computational power of GPUs [[1](#bib.bib1)]
    and the development of foundation artificial neural network models such as ResNet [[2](#bib.bib2)]
    and Transformer [[3](#bib.bib3)], deep learning (DL) has become the state-of-the-art
    approach across diverse fields over the past decade. The fields include natural
    language processing (NLP) [[4](#bib.bib4), [5](#bib.bib5)], multimedia processing [[6](#bib.bib6),
    [7](#bib.bib7)], biomedical engineering [[8](#bib.bib8), [9](#bib.bib9)], and
    autonomous driving solutions [[10](#bib.bib10), [11](#bib.bib11)]. Traditional
    DL models and the associated training data sets can run on a single GPU or server
    node without inter-GPU or inter-node communication. However, with the increase
    in the sizes of data sets and DL models, a standalone GPU or node cannot handle
    DL tasks efficiently, and distributed DL emerges to help.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU计算能力的快速提升[[1](#bib.bib1)]和基础人工神经网络模型如ResNet[[2](#bib.bib2)]和Transformer[[3](#bib.bib3)]的发展，深度学习（DL）在过去十年里已成为各个领域的最先进方法。这些领域包括自然语言处理（NLP）[[4](#bib.bib4),
    [5](#bib.bib5)]、多媒体处理[[6](#bib.bib6), [7](#bib.bib7)]、生物医学工程[[8](#bib.bib8), [9](#bib.bib9)]以及自动驾驶解决方案[[10](#bib.bib10),
    [11](#bib.bib11)]。传统的DL模型及其训练数据集可以在单个GPU或服务器节点上运行，而无需GPU间或节点间的通信。然而，随着数据集和DL模型规模的增加，单独的GPU或节点无法高效处理DL任务，因此出现了分布式DL技术。
- en: Distributed DL has become the state-of-the-art solution for addressing challenges
    present in complex scenarios of artificial intelligence. Distributed DL entails
    the training or inference of deep neural network (DNN) models on multiple CPUs
    or GPUs in one or multiple computing nodes to handle large training data sets
    and extensive learning models. The benefits of distributed DL are threefold. Firstly,
    distributed DL enhances the training parallelism on large data sets and optimizes
    hyperparameters tuning. For instance, numerous DL models used in the fields of
    remote sensing [[12](#bib.bib12), [13](#bib.bib13)] require processing vast volumes
    of high-revolution multimedia data across multiple modalities to improve classification
    accuracy. To enhance training parallelism, data sets can be distributed across
    multiple nodes, with each node training models independently and sharing its efforts
    through a specific synchronization mechanism. Secondly, distributed DL facilitates
    the training and inference of large artificial neural network models. In particular,
    to train a large language model (LLM) [[14](#bib.bib14)] with tens of billions
    of parameters, a large number of GPUs and nodes are required to collaborate in
    accommodating the entire model and to perform parallel training with distributed
    data for rapid convergence. Thirdly, the evolution of Internet of Things (IoT)
    and Internet of Vehicles (IoV) has led to intricate scenarios that require use
    of distributed DL. Distributed DL empowers IoT and IoV solutions [[15](#bib.bib15)]
    by enabling them to make intelligent decisions by leveraging the computational
    and communication capabilities of the servers, network infrastructures, and end
    devices.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式DL已成为应对人工智能复杂场景中挑战的最先进解决方案。分布式DL涉及在一个或多个计算节点上的多个CPU或GPU上进行深度神经网络（DNN）模型的训练或推断，以处理大规模训练数据集和广泛的学习模型。分布式DL的好处有三方面。首先，分布式DL提高了在大数据集上的训练并行性，并优化了超参数调整。例如，许多应用于遥感领域的DL模型[[12](#bib.bib12),
    [13](#bib.bib13)]需要处理大量的高分辨率多媒体数据，以提高分类准确性。为了增强训练并行性，可以将数据集分布到多个节点，每个节点独立训练模型，并通过特定的同步机制共享其工作成果。其次，分布式DL促进了大规模人工神经网络模型的训练和推断。特别是，要训练一个具有数十亿参数的大型语言模型（LLM）[[14](#bib.bib14)]，需要大量的GPU和节点协同工作，以容纳整个模型并进行分布式数据的并行训练，从而实现快速收敛。第三，物联网（IoT）和车联网（IoV）的发展导致了需要使用分布式DL的复杂场景。分布式DL通过利用服务器、网络基础设施和终端设备的计算和通信能力，使IoT和IoV解决方案[[15](#bib.bib15)]能够做出智能决策。
- en: Communications and distributed DL are intricately intertwine themes, forming
    integral components of each other’s core functionalities. On the one hand, distributed
    DL has emerged as a prominent technique for addressing and optimizing diverse
    communications problems. With the surge in communication devices and network traffic
    volume, distributed DL offers a real-time and agile approach for tasks encompassing
    traffic analytics, routing, and network resource management across diverse communications
    domains, such as wireless communications, IoT, and network security. The inherent
    distributed nature of this approach also contributes to robustness and fault tolerance,
    thereby ensuring reliable communication in dynamic environments. On the other
    hand, efficient communications technologies play a pivotal role in achieving high-performance
    distributed DL. Given the collaborative and coordination-driven nature of distributed
    DL, communications permeate nearly all aspects of this domain and act as its driving
    force. Technologies aimed at optimizing high-performance distributed DL fall predominantly
    into the category of efficient communications technologies. Because of this mutual
    dependence, advancements in one domain impact significantly the capability of
    the other. While the former case, exploring distributed DL for communications,
    has been investigated extensively in numerous studies [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)], this article emphasizes
    strategically the latter case, communications for distributed DL, for focused
    discussion.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通信与分布式深度学习（DL）是密切交织的主题，形成了彼此核心功能的 integral 组成部分。一方面，分布式深度学习已经成为解决和优化各种通信问题的突出技术。随着通信设备和网络流量的激增，分布式深度学习为包括流量分析、路由和网络资源管理在内的任务提供了一种实时且灵活的方法，涵盖了无线通信、物联网（IoT）和网络安全等多种通信领域。这种方法固有的分布式特性也有助于增强鲁棒性和容错能力，从而确保在动态环境中的可靠通信。另一方面，高效的通信技术在实现高性能的分布式深度学习方面发挥着关键作用。鉴于分布式深度学习的协作和协调驱动特性，通信渗透到该领域的几乎所有方面，并且是其驱动力。旨在优化高性能分布式深度学习的技术主要属于高效通信技术的范畴。由于这种相互依赖，某一领域的进展会显著影响另一领域的能力。虽然前者，即探索用于通信的分布式深度学习，已经在许多研究中得到了广泛探讨[[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]，但本文战略性地强调了后者，即通信在分布式深度学习中的作用，以便进行重点讨论。
- en: Efficient communication is crucial for achieving high performance at different
    levels in distributed DL. 1) At the algorithm level, synchronizing models during
    distributed training involves intensive inter-GPU and inter-node communication
    to ensure model consistency and convergence performance [[21](#bib.bib21)]. Optimizing
    communication frequency and traffic volume to reduce communication overhead can
    help models converge swiftly with significantly less training time. 2) At the
    framework level, underutilizing communication resources and having inferior task
    scheduling cause communication traffic congestion and straggler problems in distributed
    DL [[22](#bib.bib22)]. The performance of distributed DL can be enhanced greatly
    by fully utilizing communication resources, balancing the allocation of computational
    and communication resources, and overlapping computational and communication tasks
    to prevent blocking communication. 3) At the infrastructure level, low-performance
    communication links, devices, protocols, and topologies can offset the power of
    high-performance computing units and are prone to becoming the bottleneck of the
    overall system of distributed DL [[23](#bib.bib23)]. A high-performance and cost-effective
    solution for all these communication infrastructure layers [[24](#bib.bib24)]
    can maximize the computational and communication capacity of the entire distributed
    DL system. Addressing these communication challenges at various levels in diverse
    environments is crucial for high-performance distributed DL.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的通信对于在不同层级上实现分布式深度学习的高性能至关重要。1) 在算法层面上，分布式训练期间的模型同步涉及密集的 GPU 之间和节点之间的通信，以确保模型的一致性和收敛性能[[21](#bib.bib21)]。优化通信频率和流量，以减少通信开销，可以帮助模型更快地收敛，显著减少训练时间。2)
    在框架层面上，通信资源的低利用率和任务调度不良会导致通信流量拥堵和分布式深度学习中的滞后问题[[22](#bib.bib22)]。通过充分利用通信资源、平衡计算资源和通信资源的分配以及重叠计算和通信任务以防止通信阻塞，可以大幅提升分布式深度学习的性能。3)
    在基础设施层面上，低性能的通信链路、设备、协议和拓扑会削弱高性能计算单元的能力，并容易成为分布式深度学习整体系统的瓶颈[[23](#bib.bib23)]。一个高性能且具成本效益的解决方案可以最大化整个分布式深度学习系统的计算和通信能力[[24](#bib.bib24)]。在各种环境中解决这些不同层级的通信挑战，对于实现高性能的分布式深度学习至关重要。
- en: Recently, the growing number of computational and communication devices in systems
    has led to the emergence of large-scale distributed DL and posed additional challenges
    for this context. The first challenge pertains to algorithmic complexity. In large-scale
    distributed DL, the increased number of devices and volume of workloads introduce
    additional computational and communication overhead. Efficient algorithms used
    for model synchronization and communication data compression that can scale linearly
    are crucial. Otherwise, large-scale distributed DL can introduce more overhead
    than benefits. Simultaneously, given that the optimization solution search space
    for distributed DL algorithm can grow exponentially with the scale, designing
    optimal algorithms for larger-scale distributed DL can be significantly more challenging.
    The second challenge concerns heterogeneity. In large-scale distributed DL, heterogeneity
    is prevalent in various aspects, including the data distribution, model specification,
    and geographical location and resource capacity of computational and communication
    devices. The heterogeneity not only degrades the convergence performance of distributed
    DL, but also impacts communication efficiency, and is exacerbated by problems
    of resource underutilization and stragglers. The heterogeneous factors further
    contribute to the algorithmic complexity in designing communication-efficient
    distributed DL algorithms. The third challenge concerns large models. The size
    of large models has grown exponentially compared to previous sizes. Distributed
    training of large models at scale has raised both theoretical and practical concerns
    about various factors, including convergence performance, training efficiency,
    and the costs of computational and communication resources. Given the increased
    number of devices and extended training time required, fault tolerance has also
    become more critical for large-model training compared to traditional cases with
    medium-sized models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，系统中计算和通信设备的数量不断增加，导致了大规模分布式深度学习（DL）的出现，并为此带来了额外的挑战。第一个挑战涉及算法复杂性。在大规模分布式深度学习中，设备数量和工作负载的增加引入了额外的计算和通信开销。用于模型同步和通信数据压缩的高效算法，必须能够线性扩展，否则，大规模分布式深度学习可能带来更多开销而非收益。同时，考虑到分布式深度学习算法的优化解决方案搜索空间可能随规模呈指数级增长，设计适用于大规模分布式深度学习的最优算法会变得更加具有挑战性。第二个挑战涉及异质性。在大规模分布式深度学习中，异质性在多个方面普遍存在，包括数据分布、模型规范以及计算和通信设备的地理位置和资源容量。异质性不仅降低了分布式深度学习的收敛性能，还影响了通信效率，并且通过资源利用不足和滞后问题使情况更为严重。异质因素进一步增加了设计通信高效的分布式深度学习算法的算法复杂性。第三个挑战涉及大模型。相比于以前的规模，大模型的尺寸已呈指数级增长。大规模分布式训练大模型在理论和实践上都引发了对收敛性能、训练效率及计算和通信资源成本等各种因素的担忧。鉴于设备数量的增加和所需训练时间的延长，故障容忍在大模型训练中变得比传统的中型模型训练更为重要。
- en: With the rapid development of large-scale distributed DL, there is an urgent
    need for conducting a comprehensive survey on communication-efficient large-scale
    distributed DL technologies that aim to empower researchers in the fields of communications,
    computer science, and artificial intelligence to understand critical research
    problems in this domain and to make valuable contributions. Existing surveys on
    distributed DL do express concerns about communication acting as a bottleneck
    at various levels. However, they lack a systematic and comprehensive investigation
    into the communication problems and solutions in the large-scale scenario.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大规模分布式深度学习的快速发展，迫切需要对旨在赋能通信、计算机科学和人工智能领域的研究人员理解这一领域关键研究问题并做出有价值贡献的通信高效大规模分布式深度学习技术进行全面的调查。现有的分布式深度学习调查确实对通信在各个层级作为瓶颈表示了关切。然而，它们缺乏对大规模场景中通信问题及解决方案的系统性和全面的研究。
- en: 'This article surveys the literature over the period of 2018-2023 on communication-efficient
    technologies that operate at various levels of large-scale distributed DL, including
    the algorithm, framework, and infrastructure levels. For a comprehensive understanding
    of the development history of a specific topic, some milestone works before 2018
    may be included. The topics covered in this article include model-synchronization
    and communication-data-compression algorithms, resource-allocation and task-scheduling
    strategies, and communication infrastructures. The discussion of these topics
    focuses on two dimensions: 1) how resolving the bottleneck of communication can
    enhance the performance of distributed DL; and 2) how they can be high-performance
    in large-scale settings. We also summarize some lessons learned from each topic
    to emphasize promising future research directions toward high-performance large-scale
    distributed DL. At the end this article, we conduct a case study on the large-scale
    distributed training of LLM to explore how these communication-efficient solutions
    can be applied practically in real cases.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了2018年至2023年期间关于通信效率技术的文献，这些技术在大型分布式深度学习（DL）的各个层面上运行，包括算法、框架和基础设施层面。为了全面了解特定主题的发展历史，可能会包括2018年前的一些里程碑式的工作。本文涵盖的主题包括模型同步和通信数据压缩算法、资源分配和任务调度策略以及通信基础设施。这些主题的讨论重点关注两个维度：1）解决通信瓶颈如何提升分布式深度学习的性能；2）它们在大规模环境中的高性能表现。我们还总结了每个主题的经验教训，以强调未来高性能大规模分布式深度学习的研究方向。在本文的最后，我们对大规模分布式训练LLM进行案例研究，以探讨这些通信效率解决方案在实际案例中的应用。
- en: 'TABLE I: A Comparison of Related Surveys'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 相关调查的比较'
- en: '|   Ref. | Year | Comm-Efficient Synchro-nization | Comm Data Compression |
    Comm-Efficient Resource Allocation | Comm-Efficient Task Scheduling | Comm Infrastructure
    | Edge-Cloud Heterogeneity | Large-Scale Distributed DL | Distributed Training
    of Large Models |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|   参考 | 年份 | 通信效率同步 | 通信数据压缩 | 通信效率资源分配 | 通信效率任务调度 | 通信基础设施 | 边缘-云异质性 | 大规模分布式DL
    | 大模型的分布式训练 |'
- en: '|   [[25](#bib.bib25)] | 2020 | ✓ | ✓ |  |  |  |  |  |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|   [[25](#bib.bib25)] | 2020 | ✓ | ✓ |  |  |  |  |  |  |'
- en: '| [[26](#bib.bib26)] | 2019 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] | 2019 | ✓ | ✓ |  | ✓ |  |  |  |  |'
- en: '| [[27](#bib.bib27)] | 2020 | ✓ | ✓ |  | ✓ | ✓ |  | ✓ |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] | 2020 | ✓ | ✓ |  | ✓ | ✓ |  | ✓ |  |'
- en: '| [[28](#bib.bib28)] | 2016 | ✓ |  |  | ✓ |  |  | ✓ |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] | 2016 | ✓ |  |  | ✓ |  |  | ✓ |  |'
- en: '| [[29](#bib.bib29)] | 2021 | ✓ | ✓ |  | ✓ | ✓ |  |  |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | 2021 | ✓ | ✓ |  | ✓ | ✓ |  |  |  |'
- en: '| [[30](#bib.bib30)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
- en: '| [[31](#bib.bib31)] | 2023 | ✓ | ✓ | ✓ |  |  |  |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| [[31](#bib.bib31)] | 2023 | ✓ | ✓ | ✓ |  |  |  |  |  |'
- en: '| [[32](#bib.bib32)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| [[32](#bib.bib32)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
- en: '| [[33](#bib.bib33)] | 2019 | ✓ |  |  |  | ✓ | ✓ | ✓ |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bib33)] | 2019 | ✓ |  |  |  | ✓ | ✓ | ✓ |  |'
- en: '| [[34](#bib.bib34)] | 2019 | ✓ | ✓ | ✓ |  |  | ✓ | ✓ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bib34)] | 2019 | ✓ | ✓ | ✓ |  |  | ✓ | ✓ |  |'
- en: '| [[35](#bib.bib35)] | 2020 | ✓ | ✓ |  | ✓ |  | ✓ | ✓ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | 2020 | ✓ | ✓ |  | ✓ |  | ✓ | ✓ |  |'
- en: '| [[36](#bib.bib36)] | 2020 | ✓ | ✓ |  |  |  | ✓ |  |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| [[36](#bib.bib36)] | 2020 | ✓ | ✓ |  |  |  | ✓ |  |  |'
- en: '| [[37](#bib.bib37)] | 2021 | ✓ | ✓ | ✓ |  |  | ✓ |  |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bib37)] | 2021 | ✓ | ✓ | ✓ |  |  | ✓ |  |  |'
- en: '| [[38](#bib.bib38)] | 2021 | ✓ | ✓ |  |  | ✓ | ✓ |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] | 2021 | ✓ | ✓ |  |  | ✓ | ✓ |  |  |'
- en: '| [[39](#bib.bib39)] | 2022 | ✓ | ✓ |  |  |  | ✓ |  |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| [[39](#bib.bib39)] | 2022 | ✓ | ✓ |  |  |  | ✓ |  |  |'
- en: '| [[40](#bib.bib40)] | 2022 | ✓ | ✓ |  |  |  | ✓ | ✓ |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bib40)] | 2022 | ✓ | ✓ |  |  |  | ✓ | ✓ |  |'
- en: '| [[41](#bib.bib41)] | 2022 |  | ✓ |  |  | ✓ | ✓ | ✓ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bib41)] | 2022 |  | ✓ |  |  | ✓ | ✓ | ✓ |  |'
- en: '|   Ours | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|   我们的 | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: '|   |  |  |  |  |  |  |  |  |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |  |  |  |  |  |'
- en: I-A Related Surveys
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 相关调查
- en: 'Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") compares our survey with other
    related surveys on various topics. An empty cell denotes insufficient coverage
    of the topic in the corresponding survey. Existing surveys explore distributed
    DL from various perspectives, but do not address thoroughly key aspects regarding
    the large-scale setting, especially the critical role of communication in handling
    large volumes of data, models, devices, and infrastructures. They typically do
    not explore sufficiently the scalability of algorithms and infrastructures at
    different levels of the distributed DL ecosystems in the large-scale setting.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") 将我们的调研与其他相关调研在各个主题上的比较展示出来。空白单元格表示对应调研中对该主题的覆盖不足。现有调研从不同角度探讨分布式深度学习，但未彻底解决大规模设置中的关键方面，尤其是通信在处理大量数据、模型、设备和基础设施中的关键作用。它们通常没有充分探讨在大规模设置中分布式深度学习生态系统不同层级的算法和基础设施的可扩展性。'
- en: 'Generally, existing surveys on distributed DL have the following drawbacks
    regarding the large-scale setting:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，现有关于分布式深度学习的调研在大规模设置中存在以下缺陷：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: About heterogeneity. Few surveys, when discussing technologies for communication
    optimization in distributed DL, highlight the impacts of heterogeneity, including
    model synchronization and compression of models for communication. The issue of
    heterogeneity in data, models, devices, and infrastructures is pervasive and salient
    in modern distributed DL scenarios, especially in large-scale distributed DL.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于异质性。很少有调研在讨论分布式深度学习中通信优化技术时，强调异质性的影响，包括模型同步和模型压缩以便通信。数据、模型、设备和基础设施中的异质性问题在现代分布式深度学习场景中普遍存在，尤其是在大规模分布式深度学习中。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: About resource allocation. Existing surveys lack a thorough study on communication-efficient
    resource-allocation strategies. In a multi-tenant environment with a large number
    of servers and devices for computing and network infrastructures for communication,
    optimizing the allocation of computational and communication resources for better
    resource utilization is critical to the performance of large-scale distributed
    DL.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于资源分配。现有调研缺乏对通信高效资源分配策略的深入研究。在拥有大量服务器和设备的多租户环境中，以及用于通信的网络基础设施中，优化计算和通信资源的分配以提高资源利用率，对于大规模分布式深度学习的性能至关重要。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: About communication infrastructures. Few surveys study the infrastructure for
    efficient communication in large-scale distributed DL. Communication infrastructures,
    encompassing interconnects, network devices, collective communication libraries,
    and network topologies, are of paramount importance for high-performance large-scale
    distributed DL. The rapid development of these communication infrastructure technologies
    deserves timely attention and follow-up.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于通信基础设施。很少有调研研究大规模分布式深度学习中的高效通信基础设施。通信基础设施，包括互连、网络设备、集体通信库和网络拓扑，对于高性能大规模分布式深度学习至关重要。这些通信基础设施技术的快速发展值得及时关注和跟进。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: About large models. Existing surveys lack a focus on distributed DL with large
    models. Given the success of extremely large foundation models in fields such
    as NLP, there is an urgent need for the study of various technologies for distribute
    DL in the scenario of large models.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于大模型。现有调研缺乏对大模型分布式深度学习的关注。鉴于极大规模基础模型在自然语言处理等领域的成功，有必要研究在大模型场景下分布式深度学习的各种技术。
- en: Specifically, several surveys [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]
    have focused on distributed DL from various perspectives. In particular, Verbraeken
    et al. [[25](#bib.bib25)] provide a review of distributed DL algorithms and frameworks.
    Ben-Nun and Hoefler [[26](#bib.bib26)] provide an analysis on the concurrency
    of parallel and distributed DL architectures and models. Mayer and Jocobsen [[27](#bib.bib27)]
    focus on parallelization methods to enable scalable distributed training and cover
    various topics such as distributed resource and multi-tenant management. However,
    these surveys do not emphasize adequately the communication as the critical bottleneck
    when discussing the ecosystem of distributed DL.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，几项调查[[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]从不同角度集中讨论了分布式深度学习。特别地，Verbraeken等人[[25](#bib.bib25)]提供了关于分布式深度学习算法和框架的综述。Ben-Nun和Hoefler[[26](#bib.bib26)]分析了并行和分布式深度学习架构和模型的并发性。Mayer和Jocobsen[[27](#bib.bib27)]关注于使分布式训练可扩展的并行化方法，并涉及了如分布式资源和多租户管理等多种话题。然而，这些调查在讨论分布式深度学习生态系统时，并未充分强调通信作为关键瓶颈的重要性。
- en: Several surveys [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)] have addressed communication topics in distributed DL at various
    levels. Particularly, Xing et al. [[28](#bib.bib28)] provide a survey on strategies
    and principles for distributing DL algorithms in a cluster, aiming to enhance
    inter-node communication efficiency. Ouyang et al. [[29](#bib.bib29)] focus on
    communication strategies aimed at reducing network communication traffic through
    algorithm-level optimization and accelerating network communication speed through
    network-level optimization. Both Yu et al. [[30](#bib.bib30)] and Cao et al. [[31](#bib.bib31)]
    further summarize and categorize communication-optimization strategies for distributed
    DL such as communication frequency reduction and communication data compression,
    at the algorithm level. In addition, Cao et al. [[31](#bib.bib31)] also explore
    radio-resource-management strategies and game theory approaches for improving
    communication efficiency in distributed DL. Tang et al. [[32](#bib.bib32)] conduct
    a survey of efficient communication technologies used in distributed DL at the
    architecture and application levels. Organized in a presentation structure that
    is partially similar to this paper, the survey focuses on topics like communication
    synchronization, system architectures, compression techniques, and parallelism
    of computational and communication tasks. However, it do not tackle the influence
    of heterogeneity issues and the scalability of the related technologies when discussing
    these topics. It also overlooks certain important topics for large-scale distributed
    DL, such as resource allocation and communication infrastructures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 几项调查[[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)]已在不同层面上涉及分布式深度学习中的通信话题。特别地，Xing等人[[28](#bib.bib28)]提供了关于在集群中分布深度学习算法的策略和原则的调查，旨在提高节点间的通信效率。Ouyang等人[[29](#bib.bib29)]关注于通过算法层面的优化减少网络通信流量，并通过网络层面的优化加速网络通信速度的通信策略。Yu等人[[30](#bib.bib30)]和Cao等人[[31](#bib.bib31)]进一步总结并分类了分布式深度学习的通信优化策略，如通信频率降低和通信数据压缩，主要集中在算法层面。此外，Cao等人[[31](#bib.bib31)]还探索了用于提高分布式深度学习通信效率的无线资源管理策略和博弈论方法。Tang等人[[32](#bib.bib32)]对分布式深度学习中应用于架构和应用层面的高效通信技术进行了调查。该调查采用的展示结构与本文部分相似，重点关注通信同步、系统架构、压缩技术和计算与通信任务的并行性等话题。然而，它在讨论这些话题时并未涉及异质性问题的影响和相关技术的可扩展性，也忽视了大规模分布式深度学习中的一些重要话题，如资源分配和通信基础设施。
- en: Several surveys address the communication challenges in specific distributed
    DL paradigms, including cloud-edge-based [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)], P2P-based [[39](#bib.bib39)],
    and hybrid [[40](#bib.bib40), [41](#bib.bib41)] paradigms. Notably, surveys that
    focus on the edge-based paradigm cover different aspects of communication-efficient
    technologies, including edge-specific architectures [[33](#bib.bib33), [34](#bib.bib34)],
    algorithms [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)], and infrastructures [[38](#bib.bib38)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些调查研究了特定分布式深度学习范式中的通信挑战，包括基于云边的[[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]，基于P2P的[[39](#bib.bib39)]，以及混合型[[40](#bib.bib40),
    [41](#bib.bib41)]范式。值得注意的是，关注边缘计算范式的调查覆盖了通信高效技术的不同方面，包括边缘特定架构[[33](#bib.bib33),
    [34](#bib.bib34)]、算法[[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]和基础设施[[38](#bib.bib38)]。
- en: 'To fill the gap in existing surveys on distributed DL, this article particularly
    surveys communication-efficient technologies in large-scale scenarios. Specifically,
    this article focuses on:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为填补现有分布式深度学习调查中的空白，本文特别调查了大规模场景中的通信高效技术。具体来说，本文重点关注：
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Communication-efficient distributed DL algorithms in heterogeneous settings.
    We highlight the impact of heterogeneity in data, model, and resources when presenting
    various optimization and compression algorithms, discussing how to improve communication
    performance in this context.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在异构环境下的通信高效分布式深度学习算法。我们重点讨论数据、模型和资源的异质性对各种优化和压缩算法的影响，讨论如何在这种背景下提高通信性能。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Strategies for resource allocation and task scheduling aimed at fully utilizing
    computational and communication resources and increasing distributed DL throughput
    in large-scale settings. We explore various resource-allocation strategies for
    large-scale distributed training and inference, considering framework and container
    levels. We also study various task-scheduling strategies for overlapping computational
    and communication tasks, aiming to increase distributed training and inference
    throughput with diverse heterogeneous resources and workloads.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 资源分配和任务调度策略，旨在充分利用计算和通信资源，提高大规模设置中的分布式深度学习吞吐量。我们探索了大规模分布式训练和推断的各种资源分配策略，考虑了框架和容器层次。我们还研究了各种任务调度策略，以重叠计算和通信任务，旨在利用多样的异构资源和工作负载，提高分布式训练和推断的吞吐量。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Contemporary communication infrastructures for high-performance communication.
    We study various state-of-the-art communication infrastructure technologies on
    different layers, including GPU interconnects, programmable network devices, collective
    communication interfaces, and communication topologies.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高性能通信的现代通信基础设施。我们研究不同层次上的各种先进通信基础设施技术，包括GPU互连、可编程网络设备、集体通信接口和通信拓扑。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A case study of various technologies for large foundation DL models in large-scale
    distributed DL. We use a question-and-answer approach to discuss applying these
    communication-efficient technologies to the distributed training of LLMs in real
    cases. This case study helps researchers identify practical, high-performance,
    and cost-effective solutions for large-model training in a large-scale and heterogeneous
    environment.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模分布式深度学习中大规模基础深度学习模型的各种技术案例研究。我们通过问答方式讨论将这些通信高效技术应用于实际案例中的LLM分布式训练。此案例研究帮助研究人员识别在大规模和异构环境中进行大模型训练的实际、高性能和具有成本效益的解决方案。
- en: '![Refer to caption](img/320df778c60a115accab99f338312dfe.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/320df778c60a115accab99f338312dfe.png)'
- en: 'Figure 1: The organization of the survey'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：调查组织结构
- en: I-B Survey Organization
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 调查组织
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ I-A Related Surveys ‣ I Introduction ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") outlines the detailed
    organization of the remaining sections in this survey. Section [II](#S2 "II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey") provides fundamental knowledge about distributed
    DL. Sections [III](#S3 "III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") and [IV](#S4 "IV
    Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") present works on communication-efficient
    algorithms for model synchronization and communication data compression in large-scale
    distributed DL, respectively. Section [V](#S5 "V Large-Scale Resource Allocation
    and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") examines various communication-efficient strategies for
    resource allocation and task scheduling in large-scale distributed training and
    inference. Section [VI](#S6 "VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") introduces works
    on communication infrastructure technologies at different system layers for high-performance
    communication in large-scale DL clusters. We present large-scale distributed training
    of large foundation DL models as a case study in Section [VII](#S7 "VII Large-Scale
    Distributed Training of Large Models: A Case Study on LLMs ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"), and conclude
    this survey in Section [VIII](#S8 "VIII Conclusion ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "图 1 ‣ I-A 相关调查 ‣ I 引言 ‣ 高效通信的大规模分布式深度学习：全面调查") 概述了本调查中其余部分的详细组织。第 [II](#S2
    "II 深度学习与分布式深度学习基础 ‣ 高效通信的大规模分布式深度学习：全面调查")节提供了关于分布式深度学习的基础知识。第 [III](#S3 "III
    高效通信的模型同步 ‣ 高效通信的大规模分布式深度学习：全面调查")节和第 [IV](#S4 "IV 高效通信的数据压缩 ‣ 高效通信的大规模分布式深度学习：全面调查")节分别介绍了在大规模分布式深度学习中的模型同步和数据压缩的高效通信算法。第 [V](#S5
    "V 大规模资源分配与任务调度 ‣ 高效通信的大规模分布式深度学习：全面调查")节考察了大规模分布式训练和推断中的各种高效通信资源分配和任务调度策略。第 [VI](#S6
    "VI 大规模通信基础设施 ‣ 高效通信的大规模分布式深度学习：全面调查")节介绍了针对大规模深度学习集群中高性能通信的不同系统层面的通信基础设施技术。第 [VII](#S7
    "VII 大规模模型的分布式训练：以大型语言模型为例 ‣ 高效通信的大规模分布式深度学习：全面调查")节以大型基础深度学习模型的分布式训练作为案例研究，并在第 [VIII](#S8
    "VIII 结论 ‣ 高效通信的大规模分布式深度学习：全面调查")节总结了本调查。 '
- en: 'TABLE II: A List of Common Abbreviations'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：常见缩略语列表
- en: '|   Abbreviation | Description |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|   缩略语 | 描述 |'
- en: '|   AIMD | Additive-Increase Multiplicative-Decrease |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|   AIMD | 加法增量乘法递减 |'
- en: '| CNN | Convolutional Neural Network |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| DL | Deep Learning |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| DL | 深度学习 |'
- en: '| DLRM | Deep Learning Recommendation Model |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DLRM | 深度学习推荐模型 |'
- en: '| DNN | Deep Neural Network |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 |'
- en: '| FL | Federated Learning |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| FL | 联邦学习 |'
- en: '| FPGA | Field Programmable Gate Array |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | 现场可编程门阵列 |'
- en: '| GPU | Graphics Processing Unit |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 图形处理单元 |'
- en: '| GRU | Gated Recurrent Unit |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GRU | 门控递归单元 |'
- en: '| IID | Independent and Identically Distributed |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| IID | 独立同分布 |'
- en: '| INA | In-Network Aggregation |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| INA | 网络内聚合 |'
- en: '| IoT | Internet of Things |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| IoT | 物联网 |'
- en: '| IoV | Internet of Vehicles |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| IoV | 车联网 |'
- en: '| LARS | Layer-wise Adaptive learning Rate Scaling |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LARS | 分层自适应学习率缩放 |'
- en: '| LLM | Large Language Model |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 大型语言模型 |'
- en: '| LSTM | Long Short-Term Memory |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆 |'
- en: '| MPI | Message Passing Interface |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| MPI | 消息传递接口 |'
- en: '| MPS | Multiple Process Sharing |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| MPS | 多进程共享 |'
- en: '| NCCL | NVIDIA Collective Communications Library |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| NCCL | NVIDIA 集体通信库 |'
- en: '| NIC | Network Interface Card |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| NIC | 网络接口卡 |'
- en: '| NLP | Natural Language Processing |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| NLP | 自然语言处理 |'
- en: '| PaaS | Platform as a Service |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PaaS | 平台即服务 |'
- en: '| PCIe | Peripheral Component Interconnect Express |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| PCIe | 外设组件互连快速通道 |'
- en: '| PS | Parameter Server |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| PS | 参数服务器 |'
- en: '| P2P | Peer-to-Peer |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| P2P | 点对点 |'
- en: '| RNN | Recurrent Neural Network |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 递归神经网络 |'
- en: '| SGD | Stochastic Gradient Descent |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| SGD | 随机梯度下降 |'
- en: '| SiP | Silicon Photonic |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SiP | 硅光子 |'
- en: '| SLO | Service-Level Objective |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SLO | 服务级目标 |'
- en: '|   |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|   |   |'
- en: II Fundamentals of DL and Distributed DL
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习和分布式深度学习的基础
- en: 'In this section, we present the fundamentals of DL and distributed DL. Table [II](#S1.T2
    "TABLE II ‣ I-B Survey Organization ‣ I Introduction ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") includes common
    abbreviations used in this survey.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们介绍了深度学习和分布式深度学习的基础。表 [II](#S1.T2 "TABLE II ‣ I-B Survey Organization
    ‣ I Introduction ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") 包含了本调查中使用的常见缩略语。'
- en: II-A Deep Learning
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度学习
- en: DL is a subfield of machine learning that utilizes deep artificial neural networks,
    also known as deep neural networks (DNN), to extract complex patterns from training
    data in a hierarchical manner. The trained DNN is capable to recognize/predict
    patterns in unseen data. DL has been used in various fields, including NLP [[4](#bib.bib4)],
    computer vision [[6](#bib.bib6)], and biomedical engineering [[9](#bib.bib9)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习的一个子领域，利用深度人工神经网络，也称为深度神经网络（DNN），以层次化的方式从训练数据中提取复杂模式。训练后的 DNN 能够识别/预测未见数据中的模式。DL
    已在多个领域中得到应用，包括 NLP [[4](#bib.bib4)]、计算机视觉 [[6](#bib.bib6)] 和生物医学工程 [[9](#bib.bib9)]。
- en: '![Refer to caption](img/a25cbc22bede4616288487fe93ddc64b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a25cbc22bede4616288487fe93ddc64b.png)'
- en: (a) Fully Connected DNN
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 全连接 DNN
- en: '![Refer to caption](img/98c3f9e1520fa6974b027fdd882c3c4a.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98c3f9e1520fa6974b027fdd882c3c4a.png)'
- en: (b) CNN
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CNN
- en: '![Refer to caption](img/a89c1a36e066e144d13474eac1c9ef35.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a89c1a36e066e144d13474eac1c9ef35.png)'
- en: (c) RNN
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (c) RNN
- en: 'Figure 2: Common artificial neural network models for DL'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：深度学习常见的人工神经网络模型
- en: II-A1 DL models
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 DL 模型
- en: 'A DNN consists of multiple hidden layers. Each layer is comprised of neurons,
    which are typically activated by non-linear functions. Based on the connections
    between neurons within and between layers, there can be various types of DNN models.
    In this survey, when referring to models or DL models, we mean DNNs unless the
    context otherwise specifies. Fig. [2](#S2.F2 "Figure 2 ‣ II-A Deep Learning ‣
    II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") illustrates three basic DNN
    models: the fully connected DNN, convolutional neural network (CNN), and recurrent
    neural network (RNN).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'DNN 由多个隐藏层组成。每一层包含神经元，这些神经元通常由非线性函数激活。根据层内及层间神经元之间的连接，可以有多种类型的 DNN 模型。在本调查中，当提及模型或
    DL 模型时，我们指的是 DNN，除非上下文另有说明。图 [2](#S2.F2 "Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey") 展示了三种基本的 DNN 模型：全连接 DNN、卷积神经网络（CNN）和递归神经网络（RNN）。'
- en: '$\bullet$ Fully connected DNN: The fully connected DNN, also known as the feedforward
    neural network, constitutes a dense network with an input layer, a number of hidden
    layers, and an output layer, as depicted in Fig. [2a](#S2.F2.sf1 "In Figure 2
    ‣ II-A Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"). Neurons in a
    preceding layer connect to all neurons in the subsequent layer, and each connection
    has a learnable weight parameter indicating the strength of the connection. This
    architecture enables the fully connected DNN to capture complex relationships
    within data, finding extensive application in tasks such as classification [[42](#bib.bib42)],
    regression [[43](#bib.bib43)], and feature representation embedding [[44](#bib.bib44)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 全连接 DNN：全连接 DNN，也称为前馈神经网络，构成了一个密集的网络，包括一个输入层、若干个隐藏层和一个输出层，如图 [2a](#S2.F2.sf1
    "In Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") 所示。前一层的神经元与后一层的所有神经元连接，每个连接都有一个可学习的权重参数，表示连接的强度。这种架构使全连接 DNN 能够捕捉数据中的复杂关系，广泛应用于分类
    [[42](#bib.bib42)]、回归 [[43](#bib.bib43)] 和特征表示嵌入 [[44](#bib.bib44)] 等任务。'
- en: '$\bullet$ CNN: CNN stands as a prevalent model designed for feature extraction
    and classification, primarily tailored for image and video data. As depicted in
    Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals of DL
    and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey"), in addition to the input and output layers, CNN comprises
    a stack of convolutional layers and pooling layers for feature extraction, succeeded
    by fully connected (FC) layers for classification. Unlike the fully connected
    layer, which assigns a weight parameter to each neuron connection, the convolutional
    layer substantially reduces the number of weight parameters by utilizing a number
    of kernels, or filters, each containing shared weights for feature extraction.
    The feature extraction process of the convolutional layer’s feature extraction
    process is empowered by the convolution operation, wherein kernels traverse the
    receptive fields of an image, extracting new features through weighted summations
    followed by a non-linear activation function. The pooling layer, typically using
    max-pooling or average-pooling functions, downsamples the data in the convolutional
    layer to reduce feature dimensions and alleviate overfitting issues. CNN has found
    widespread applications in various computer vision tasks, including image classification [[45](#bib.bib45)],
    semantic segmentation [[46](#bib.bib46)], and object detection [[47](#bib.bib47)].'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ CNN：CNN 是一种用于特征提取和分类的广泛应用模型，主要针对图像和视频数据设计。如图 [2b](#S2.F2.sf2 "在图 2
    ‣ II-A 深度学习 ‣ II 深度学习基础和分布式深度学习 ‣ 通信高效的大规模分布式深度学习：全面调查") 所示，CNN 除了输入层和输出层，还包括一系列卷积层和池化层用于特征提取，然后是用于分类的全连接（FC）层。与将权重参数分配给每个神经元连接的全连接层不同，卷积层通过使用多个卷积核（或滤波器）显著减少权重参数的数量，每个卷积核都包含用于特征提取的共享权重。卷积层的特征提取过程通过卷积操作得到增强，其中卷积核遍历图像的感受野，通过加权求和和非线性激活函数提取新特征。池化层通常使用最大池化或平均池化函数，对卷积层中的数据进行下采样，以减少特征维度并缓解过拟合问题。CNN
    在各种计算机视觉任务中得到了广泛应用，包括图像分类 [[45](#bib.bib45)]、语义分割 [[46](#bib.bib46)] 和目标检测 [[47](#bib.bib47)]。
- en: '$\bullet$ RNN: RNN, a DL model that deals with sequential data like time-series
    data, natural language, and speech audio, is illustrated in Fig. [2c](#S2.F2.sf3
    "In Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"). The general architecture of RNN includes hidden units that capture and
    propagate temporal context from the input sequence to subsequent hidden unites.
    It updates continuously and utilizes the temporal context based on the current
    input and previous temporal context to make predictions. To address the challenge
    of capturing long-range temporal dependencies, two common variants of RNNs, known
    as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed,
    providing a trade-off between modeling such dependencies and reducing computation
    complexity effectively. Common applications of RNN include tasks such as time
    series forecasting [[48](#bib.bib48)], NLP [[49](#bib.bib49)], and automated planning [[50](#bib.bib50)].'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ RNN：RNN 是一种处理序列数据（如时间序列数据、自然语言和语音音频）的深度学习模型，如图 [2c](#S2.F2.sf3 "在图
    2 ‣ II-A 深度学习 ‣ II 深度学习基础和分布式深度学习 ‣ 通信高效的大规模分布式深度学习：全面调查") 所示。RNN 的一般架构包括隐藏单元，这些单元捕获并传播从输入序列到后续隐藏单元的时间上下文。它会持续更新，并利用基于当前输入和先前时间上下文的时间上下文来进行预测。为了解决捕获长期时间依赖关系的挑战，开发了两种常见的
    RNN 变体，即长短期记忆网络（LSTM）和门控循环单元（GRU），它们在建模这些依赖关系和有效降低计算复杂性之间提供了权衡。RNN 的常见应用包括时间序列预测 [[48](#bib.bib48)]、自然语言处理 [[49](#bib.bib49)]
    和自动规划 [[50](#bib.bib50)]。
- en: II-A2 Training and inference
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 训练与推理
- en: 'The training of a DL model is the process of optimizing its parameters to minimize
    the prediction error on a training data set, as determined by a specified loss
    function, or objective function. Loss functions can be either convex or non-convex,
    leading to convex or non-convex optimization problems. Training can be decomposed
    into two key processes: feedforward and backpropagation. In the feedforward process,
    training data are passed into the model’s input layer, and the output prediction
    is computed by forwarding data through the network using the current model parameters.
    In the backpropagation process, the prediction error and gradients are calculated
    with respect to the loss function, and trainable parameters are updated iteratively
    in a backward manner, optimizing the model for the minimum loss. Common optimizers
    for backpropagation updating include minibatch Stochastic Gradient Descent (SGD) [[51](#bib.bib51)],
    SGD with momentum [[52](#bib.bib52)], Adagrad [[53](#bib.bib53)], and Adam [[54](#bib.bib54)].
    The training process usually operates on batches of training data iteratively
    over multiple epochs until the model converges. A model is said to have converged
    when the training error settles to within a predefined error range, and additional
    training will not further decrease the error. After completing the training process,
    the weight parameters in the DL model are learned and fixed. Following the training
    process, there is typically a validation process for validating the performance
    of the trained model, providing information for fine-tuning hyperparameters and
    retraining the model for better performance.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的训练是优化其参数以最小化训练数据集上的预测误差的过程，误差由指定的损失函数或目标函数确定。损失函数可以是凸的或非凸的，从而导致凸优化或非凸优化问题。训练可以分解为两个关键过程：前向传播和反向传播。在前向传播过程中，训练数据被传递到模型的输入层，通过使用当前模型参数将数据传递通过网络来计算输出预测。在反向传播过程中，根据损失函数计算预测误差和梯度，并以反向方式迭代更新可训练参数，从而优化模型以达到最小损失。常见的反向传播优化器包括小批量随机梯度下降（SGD）[[51](#bib.bib51)]，带动量的
    SGD [[52](#bib.bib52)]，Adagrad [[53](#bib.bib53)] 和 Adam [[54](#bib.bib54)]。训练过程通常在多个周期内对训练数据批次进行迭代操作，直到模型收敛。当训练误差稳定在预定义的误差范围内，且额外训练不会进一步降低误差时，模型被认为已经收敛。完成训练过程后，深度学习模型中的权重参数被学习并固定。训练过程完成后，通常会有一个验证过程，用于验证训练模型的性能，提供调整超参数和重新训练模型以获得更好性能的信息。
- en: The inference process passes forward unseen data through the trained DL model
    to make predictions. Depending on the specific requirements of an application,
    the resulting prediction can be extracted either either from the output layer
    or from the predicted latent representation in an intermediate hidden layer. For
    example, in the context of network traffic analysis, an end-to-end DNN model may
    be trained to classify traffic types directly [[55](#bib.bib55)], Alternatively,
    an encoder-decoder model trained on traffic data can utilize the latent representation
    generated by the encoder for subsequent tasks such as attack detection [[56](#bib.bib56)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 推断过程将未见过的数据通过训练好的深度学习模型进行预测。根据应用的具体要求，结果预测可以从输出层提取，或从中间隐藏层的预测潜在表示中提取。例如，在网络流量分析的背景下，可以训练一个端到端的深度神经网络模型直接分类流量类型[[55](#bib.bib55)]。或者，基于流量数据训练的编码器-解码器模型可以利用编码器生成的潜在表示进行后续任务，如攻击检测[[56](#bib.bib56)]。
- en: Computing tasks related to a specific portion of the DL model for specific epochs
    during the training or inference process are generally referred to as DL tasks
    in this survey, when it is not necessary to distinguish training tasks and inference
    tasks in the context.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练或推断过程中，与深度学习模型特定部分相关的计算任务通常在本调查中被称为深度学习任务，当在上下文中不需要区分训练任务和推断任务时。
- en: '![Refer to caption](img/698f7e231a9db2be5cfc57e1a365625b.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/698f7e231a9db2be5cfc57e1a365625b.png)'
- en: (a) Residual Block
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 残差块
- en: '![Refer to caption](img/61707a34cded52657d18bc04e1f0e5df.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/61707a34cded52657d18bc04e1f0e5df.png)'
- en: (b) Transformer Block
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Transformer 块
- en: 'Figure 3: Fundamental blocks for deep neural networks'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：深度神经网络的基本块
- en: II-A3 Fundamental neural network blocks
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 基本神经网络块
- en: 'DL models have experienced an exponential increase in terms of both depth and
    scale, with some models comprising thousands of neural layers [[57](#bib.bib57)]
    or tens of billions of parameters [[14](#bib.bib14)]. However, the growth in the
    complexity of models has introduced various challenges, including the vanishing
    gradient problem [[58](#bib.bib58)] and issues related to training and inference
    efficiency. To overcome these challenges, certain neural network structures have
    been developed as fundamental blocks for building various DNN models that can
    capture complex patterns efficiently. In practice, a DNN model can be built easily
    by stacking these fundamental blocks on top of each other. Fig. [3](#S2.F3 "Figure
    3 ‣ II-A2 Training and inference ‣ II-A Deep Learning ‣ II Fundamentals of DL
    and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") illustrates two widely recognized fundamental neural
    network blocks: the Residual [[2](#bib.bib2)] and Transformer [[3](#bib.bib3)]
    blocks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习模型在深度和规模上经历了指数增长，有些模型包含数千个神经层 [[57](#bib.bib57)] 或数百亿个参数 [[14](#bib.bib14)]。然而，模型复杂性的增长引入了各种挑战，包括梯度消失问题 [[58](#bib.bib58)]
    以及与训练和推断效率相关的问题。为了克服这些挑战，某些神经网络结构被开发为构建能够有效捕获复杂模式的各种 DNN 模型的基本模块。在实践中，通过将这些基本模块堆叠在一起，可以轻松构建
    DNN 模型。图 [3](#S2.F3 "图3 ‣ II-A2 训练和推断 ‣ II-A 深度学习 ‣ II 深度学习和分布式深度学习基础 ‣ 高效的大规模分布式深度学习:
    一项全面调查") 展示了两种广为认可的基本神经网络块，即残差 [[2](#bib.bib2)] 和 Transformer [[3](#bib.bib3)]
    块。'
- en: '$\bullet$ Residual: The Residual block, depicted in Fig. [3a](#S2.F3.sf1 "In
    Figure 3 ‣ II-A2 Training and inference ‣ II-A Deep Learning ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"), features a shortcut connection that adds the
    identity input to its mapping after passing through several layers. The shortcut
    connection facilitates the smooth flow of data through multiple layers, ensuring
    that important gradient updates can be propagated efficiently back to shallower
    layers. As a result, the Residual block addresses the issue of vanishing gradients
    efficiently, enabling efficient training in very deep models.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ Residual: 残差块，如图 [3a](#S2.F3.sf1 "在图3 ‣ II-A2 训练和推断 ‣ II-A 深度学习 ‣
    II 深度学习和分布式深度学习基础 ‣ 高效的大规模分布式深度学习: 一项全面调查") 所示，具有一个快捷连接，经过多个层后，将身份输入添加到其映射中。快捷连接促进数据在多个层之间的顺畅流动，确保重要的梯度更新能够有效传播回更浅的层。因此，残差块有效解决了梯度消失问题，在非常深的模型中实现了高效训练。'
- en: '$\bullet$ Transformer: The Transformer block is a ubiquitous choice for DL
    models in learning tasks with sequential data, such as NLP problems. It adopts
    an autoencoder architecture characterized by a combined encoder and decoder, as
    shown in Fig. [3b](#S2.F3.sf2 "In Figure 3 ‣ II-A2 Training and inference ‣ II-A
    Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"). The Transformer
    block benefits from the self-attention mechanism, aligning the data with the context
    and enabling the data to attend to the important parts within that context. This
    mechanism can be calculated in parallel, making the Transformer block less time-consuming
    to train compared to previous RNN architectures such as LSTM and GRU. This facilitates
    the building and training of large DL models, particularly giving rise to LLMs [[59](#bib.bib59),
    [14](#bib.bib14)], which serve as foundation models for various NLP tasks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ Transformer: Transformer 块是在处理顺序数据学习任务中深度学习模型中的普遍选择，比如 NLP 问题。它采用由编码器和解码器组成的自动编码器架构，如图 [3b](#S2.F3.sf2
    "在图3 ‣ II-A2 训练和推断 ‣ II-A 深度学习 ‣ II 深度学习和分布式深度学习基础 ‣ 高效的大规模分布式深度学习: 一项全面调查") 所示。Transformer
    块受益于自注意机制，将数据与上下文对齐，并使数据关注上下文中的重要部分。这种机制可以并行计算，使得与先前的 RNN 架构（如 LSTM 和 GRU）相比，训练
    Transformer 块所需的时间更少。这有利于构建和训练大型深度学习模型，特别是为各种 NLP 任务提供基础模型的 LLMs [[59](#bib.bib59),
    [14](#bib.bib14)]。'
- en: II-B Distributed DL Parallelism Modes
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 分布式深度学习并行模式
- en: In distributed DL, data, models, and training or inference tasks are partitioned
    across multiple processing units (commonly GPUs in the DL context) within a single
    computing node or across multiple nodes in a cluster. Training DNN models in distributed
    environments involves intensive computation and communication within a computing
    cluster, making distributed training a focus of significant research. During the
    distributed training process, leveraging computational capabilities of available
    GPUs and nodes enhances training parallelism, thereby improving efficiency. Furthermore,
    it enables the successful completion of training tasks that involve extensive
    data or models exceeding the capacity of a single GPU or node. Distributed Training
    requires intensive exchanges of model parameters or gradients across GPUs and
    nodes, where the communication process typically becomes the performance bottleneck.
    Crucially, designing communication-efficient algorithms for model synchronization,
    resource management, task scheduling, and infrastructures is essential for making
    distributed training a versatile solution for large-scale distributed DL tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式深度学习中，数据、模型以及训练或推理任务被分配到单个计算节点或集群中的多个处理单元（通常是深度学习上下文中的 GPU）。在分布式环境中训练深度神经网络模型涉及到计算集群内的密集计算和通信，因此分布式训练成为了重点研究领域。在分布式训练过程中，利用可用
    GPU 和节点的计算能力可以增强训练并行性，从而提高效率。此外，这还使得能够成功完成涉及大量数据或模型的训练任务，这些数据或模型超出了单个 GPU 或节点的容量。分布式训练需要在
    GPU 和节点之间进行大量的模型参数或梯度交换，而通信过程通常成为性能瓶颈。至关重要的是，为模型同步、资源管理、任务调度和基础设施设计高效通信的算法是使分布式训练成为大规模分布式深度学习任务的多功能解决方案的关键。
- en: '![Refer to caption](img/06db9eb7fe5bd5e6719facce29580841.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/06db9eb7fe5bd5e6719facce29580841.png)'
- en: (a) Data Parallelism
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 数据并行
- en: '![Refer to caption](img/869697b20f758da91d67cb387ef90fb5.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/869697b20f758da91d67cb387ef90fb5.png)'
- en: (b) Model Parallelism
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 模型并行
- en: '![Refer to caption](img/8fa9061f75f6c03b74707013b9a36c82.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8fa9061f75f6c03b74707013b9a36c82.png)'
- en: (c) Pipeline Parallelism
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 流水线并行
- en: 'Figure 4: Parallelism modes of distributed DL'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：分布式深度学习的并行模式
- en: 'Various partitioning strategies in distributed DL result in three prevalent
    parallelism modes, as illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ II-B Distributed
    DL Parallelism Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"): data parallelism
    (Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II
    Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")), model parallelism (Fig. [4b](#S2.F4.sf2
    "In Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals of DL and
    Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")), and pipeline parallelism (Fig. [4c](#S2.F4.sf3 "In
    Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals of DL and Distributed
    DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式深度学习中的各种分区策略导致三种流行的并行模式，如图 [4](#S2.F4 "图 4 ‣ II-B 分布式深度学习并行模式 ‣ II 深度学习与分布式深度学习基础
    ‣ 高效通信的大规模分布式深度学习：综合调查")所示：数据并行（图 [4a](#S2.F4.sf1 "在图 4 ‣ II-B 分布式深度学习并行模式 ‣ II
    深度学习与分布式深度学习基础 ‣ 高效通信的大规模分布式深度学习：综合调查)），模型并行（图 [4b](#S2.F4.sf2 "在图 4 ‣ II-B 分布式深度学习并行模式
    ‣ II 深度学习与分布式深度学习基础 ‣ 高效通信的大规模分布式深度学习：综合调查)），以及流水线并行（图 [4c](#S2.F4.sf3 "在图 4 ‣
    II-B 分布式深度学习并行模式 ‣ II 深度学习与分布式深度学习基础 ‣ 高效通信的大规模分布式深度学习：综合调查)")。
- en: II-B1 Data parallelism
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 数据并行
- en: In this mode, the entire training data set is divided into several splits, which
    are then distributed across multiple GPUs within a cluster [[60](#bib.bib60)],
    aiming to increase the level of training parallelism and reduce training time.
    The DL model is replicated on each GPU, and each GPU trains a local model with
    an identical structure, concentrating on a specific partition of the data set.
    Throughout the distributed training process, these local models share their knowledge
    to update a global model using a specific model-synchronization mechanism. The
    convergence rate of distributed training is tied to the communication efficiency
    of the synchronization mechanism, considering factors such as the communication
    pattern and the underlying network infrastructure. Moreover, it is influenced
    significantly by the trade-off between the model computation and synchronization
    communication performance, including factors such as communication frequency and
    the volume of data to be transferred across GPUs and computing nodes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，整个训练数据集被分割成若干份，然后分配到集群中的多个GPU上[[60](#bib.bib60)]，旨在提高训练并行度并缩短训练时间。DL模型在每个GPU上进行复制，每个GPU训练一个结构相同的本地模型，专注于数据集的特定部分。在分布式训练过程中，这些本地模型通过特定的模型同步机制共享知识，以更新全局模型。分布式训练的收敛速度与同步机制的通信效率有关，需要考虑通信模式和基础网络设施等因素。此外，它还受到模型计算与同步通信性能之间权衡的显著影响，包括通信频率和在GPU及计算节点之间传输的数据量等因素。
- en: '![Refer to caption](img/f5cf8782c9df2a1990a7f56b628c2b05.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f5cf8782c9df2a1990a7f56b628c2b05.png)'
- en: (a) Centralized
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集中式
- en: '![Refer to caption](img/ece317b4c258456fbb08a2f42e9c7db4.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ece317b4c258456fbb08a2f42e9c7db4.png)'
- en: (b) Decentralized
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 去中心化
- en: 'Figure 5: Distributed SGD'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: 分布式SGD'
- en: 'Distributed SGD is a ubiquitous model-synchronization mechanism employed for
    distributed training in the context of data parallelism. Fig. [5](#S2.F5 "Figure
    5 ‣ II-B1 Data parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey") illustrates two common architectures of distributed
    SGD: parameter-server-based centralized SGD and gossip-based decentralized SGD.
    In centralized SGD with a parameter server (PS) [[61](#bib.bib61)] and multiple
    workers, SGD updates from local workers are transmitted to the parameter servers,
    either synchronously or asynchronously, to determine up-to-date parameters. Subsequently,
    these updated parameters are sent back to the workers for local model updating.
    In decentralized SGD without a central PS, workers synchronize their models with
    other workers either in a gossip manner or via a hierarchical structure of multiple
    parameter servers.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式SGD是一种广泛使用的模型同步机制，用于数据并行背景下的分布式训练。图[5](#S2.F5 "图5 ‣ II-B1 数据并行 ‣ II-B 分布式DL并行模式
    ‣ II DL和分布式DL基础知识 ‣ 通信高效的大规模分布式深度学习：全面调查")展示了两种常见的分布式SGD架构：基于参数服务器的集中式SGD和基于gossip的去中心化SGD。在具有参数服务器（PS）[[61](#bib.bib61)]和多个工作节点的集中式SGD中，来自本地工作节点的SGD更新被传输到参数服务器，无论是同步还是异步，以确定最新的参数。随后，这些更新的参数被发送回工作节点，用于本地模型更新。在没有中央PS的去中心化SGD中，工作节点通过gossip方式或通过多个参数服务器的分层结构与其他工作节点同步其模型。
- en: II-B2 Model parallelism
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 模型并行
- en: When an entire DL model exceeds the capacity of a single GPU or node, the model
    parallelism mode [[62](#bib.bib62)] employs a specific model-division strategy
    to separate the model into multiple neural network chunks, or called submodels.
    These submodels are then distributed across multiple GPUs within a cluster, with
    connections maintaining data communication for feedforward and backpropagation.
    This enables a seamless flow of data and gradients throughout the entire model
    between GPUs and nodes, framing the training and inference process as if it were
    operating within a unified, black-box GPU. This parallelism mode introduces several
    crucial communication considerations. Initially, there is a need for optimizing
    the division of the DL model to minimize communication overhead. Additionally,
    establishing an efficient strategy to locate submodels is essential to ensure
    an optimized communication pattern with respect to submodel dependency.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当整个深度学习模型超出单个 GPU 或节点的容量时，模型并行模式 [[62](#bib.bib62)] 采用一种特定的模型分割策略，将模型分为多个神经网络块，或称为子模型。
    然后，这些子模型在集群中的多个 GPU 上分布，并通过连接维持前向传播和反向传播的数据通信。 这使得数据和梯度能够在整个模型之间 GPU 和节点之间无缝流动，非常类似于在统一的黑匣子
    GPU 中运行训练和推断过程。 此并行模式引入了几个重要的通信注意事项。 首先，需要优化 DL 模型的分割，以最小化通信开销。 另外，建立一个有效的策略来定位子模型是至关重要的，以确保在子模型依赖性方面有优化的通信模式。
- en: II-B3 Pipeline parallelism
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 流水线并行
- en: The pipeline parallelism mode further enhances the parallelism of DL tasks by
    reducing the complexity of submodel dependency in the model parallelism mode and
    preventing computational resources from idling [[63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]. In this mode, distributed training tasks are decomposed layer-by-layer
    into multiple subtasks, with their completions depending on previous layers. The
    associated submodels for handling the subtasks are distributed across the GPU
    cluster. Thus, when multiple batches of data flow through all submodels in a pipeline
    manner, each submodel is trained by different batches simultaneously. This mode
    is particularly applicable in the domains of edge-computing, IoT, and IoV, where
    devices have heterogeneous computational and communication capabilities to handle
    various distributed DL subtasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行模式通过减少模型并行模式中子模型依赖的复杂性并阻止计算资源处于空闲状态，进一步增强了 DL 任务的并行性[[63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]。 在此模式下，分布式培训任务被逐层分解为多个子任务，并且它们的完成取决于先前的层。 用于处理子任务的关联子模型分布在 GPU
    集群中。 因此，当多批数据以管道方式流经所有子模型时，每个子模型同时由不同的批次训练。 此模式特别适用于边缘计算、物联网和智能物联网领域，设备具有异构计算和通信能力来处理各种分布式
    DL 子任务。
- en: 'In practice, these parallelism modes can be combined [[66](#bib.bib66)] to
    tackle complex DL tasks with large model structures, e.g., LLMs, which we will
    explore in a case study in Section [VII](#S7 "VII Large-Scale Distributed Training
    of Large Models: A Case Study on LLMs ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这些并行模式可以组合[[66](#bib.bib66)]，以应对具有大型模型结构的复杂 DL 任务，例如，我们将在第 [VII](#S7 "VII
    大规模分布式大模型培训：关于 LLMs 的案例研究 ‣ 通信高效的大规模分布式深度学习：综合调查") 节中探索的 LLMs 案例研究。
- en: '![Refer to caption](img/9463636999ab4fbb00db602309fb998c.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9463636999ab4fbb00db602309fb998c.png)'
- en: (a) Cluster/Cloud
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集群/云
- en: '![Refer to caption](img/5cb13007afc3fb94d5fff47800201670.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5cb13007afc3fb94d5fff47800201670.png)'
- en: (b) Edge/Federated
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 边缘/联邦
- en: '![Refer to caption](img/b87a41339c6b286c01d64fd32e7f47b3.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b87a41339c6b286c01d64fd32e7f47b3.png)'
- en: (c) P2P
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 点对点
- en: 'Figure 6: Typical distributed DL paradigms'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：典型的分布式 DL 范式
- en: II-C Distributed DL Paradigms
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 分布式 DL 范式
- en: 'We introduce mainstream paradigms of distributed DL, discussing their characteristics,
    research challenges, and applications. Based on the communication pattern, training
    and inference locality, and hardware platform, we classify these paradigms into
    three types: cluster/cloud-based, edge-based, and peer-to-peer-based (P2P-based).
    Fig. [6](#S2.F6 "Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism
    Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") depicts each paradigm, using
    solid and dashed arrows to represent inter-node communications and intra-node
    communications, respectively.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了主流的分布式DL范式，讨论了它们的特征、研究挑战和应用。根据通信模式、训练和推断的局部性以及硬件平台，我们将这些范式分为三种类型：基于集群/云、基于边缘和基于对等网络（P2P）。图[6](#S2.F6
    "图6 ‣ II-B3 管道并行 ‣ II-B 分布式DL并行模式 ‣ II 深度学习与分布式DL基础 ‣ 面向通信高效的大规模分布式深度学习：综述")展示了每种范式，实线箭头表示节点间通信，虚线箭头表示节点内通信。
- en: II-C1 Cluster/Cloud-based Centralized Distributed DL
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 基于集群/云的集中式分布式DL
- en: 'The cluster-based paradigm, depicted in Fig. [6a](#S2.F6.sf1 "In Figure 6 ‣
    II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"), is a traditional scale-out solution for centralized
    distributed DL. A cluster of computing servers, interconnected via local networks,
    provide extended computing capability for training and inference beyond what a
    single computing server can achieve. End devices, such as desktop computers, perceive
    the cluster as a unified entity, accessing its computational resources through
    unified network interfaces without being concerned with the communication mechanisms
    within the cluster. The cluster comprises four key modules: the resource manager,
    data storage manager, task scheduler, and computing framework. The resource manager
    oversees the computational resources (e.g., GPU, CPU, and memory) and network
    communication resources (e.g., network topology and bandwidth) in the cluster,
    allocating them to DL tasks. The data storage manager partitions and manages large
    training and testing data in a distributed manner, considering factors such as
    storage resource and network topology to ensure data availability, efficiency,
    and reliability. The task scheduler breaks down DL tasks into various levels of
    granularity and schedules them within the cluster, taking into account resource
    consumption, data locality, workloads, and task characteristics to improve parallelism
    and failure tolerance. The computing framework implements the kernel and API to
    run the distributed DL algorithms efficiently with available resources allocated
    by the resource manager. This paradigm is commonly employed for accelerating model
    training [[67](#bib.bib67)], accommodating large models [[62](#bib.bib62)], and
    facilitating hyperparameter tuning [[68](#bib.bib68)]. In this context, efficient
    strategies for resource management and task scheduling are essential, aiming to
    maximize computational and communication-resource utilization, as well as the
    parallelism of task execution. This optimization is crucial for achieving high
    performance in this domain.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6a](#S2.F6.sf1 "图6 ‣ II-B3 管道并行 ‣ II-B 分布式DL并行模式 ‣ II 深度学习与分布式DL基础 ‣ 面向通信高效的大规模分布式深度学习：综述")所示的基于集群的范式，是集中式分布式DL的传统扩展解决方案。通过本地网络互联的计算服务器集群，提供了超越单台计算服务器的训练和推断计算能力。终端设备（如桌面计算机）将集群视为一个统一的实体，通过统一的网络接口访问其计算资源，而无需关心集群内的通信机制。集群由四个关键模块组成：资源管理器、数据存储管理器、任务调度器和计算框架。资源管理器负责集群中的计算资源（如GPU、CPU和内存）和网络通信资源（如网络拓扑和带宽）的管理，并将其分配给DL任务。数据存储管理器以分布式方式分区和管理大量的训练和测试数据，考虑存储资源和网络拓扑等因素，以确保数据的可用性、效率和可靠性。任务调度器将DL任务拆解成不同粒度，并在集群内进行调度，考虑资源消耗、数据局部性、工作负载和任务特性，以提高并行性和容错能力。计算框架实现内核和API，以利用资源管理器分配的可用资源高效运行分布式DL算法。该范式通常用于加速模型训练[[67](#bib.bib67)]、适应大型模型[[62](#bib.bib62)]和促进超参数调整[[68](#bib.bib68)]。在这种情况下，高效的资源管理和任务调度策略至关重要，旨在最大化计算和通信资源的利用率以及任务执行的并行性。这种优化对于在该领域实现高性能至关重要。
- en: 'The cloud-based distributed DL paradigm can be viewed as an extension of the
    cluster-based paradigm, as shown in Fig. [6a](#S2.F6.sf1 "In Figure 6 ‣ II-B3
    Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"). Cloud service providers encapsulate the cluster-based
    distributed DL platform into Platform-as-a-Service (PaaS) products and deliver
    the distributed DL services in a pay-as-you-go manner. End devices are charged
    by the resources they actually use, including storage for the training data, computational
    resources for training and inference, and traffic for network communication. Compared
    to the cluster-based paradigm, the advantages of the cloud-based paradigm are
    threefold: virtualization, elasticity, and utility. First, the cloud employs virtualization
    technologies that include resource isolation and network virtualization to facilitate
    the sharing of computational and communication resources among multiple tenants
    in the cloud, thus reducing the resource costs for tenants. Second, the cloud
    can adjust elastically resource capacities based on runtime learning workloads.
    This allows the distributed DL platform to expand to a larger scale to handle
    larger models and peak workloads or contract when not necessary to conserve resources.
    Third, the cloud typically provides more utility tools for managing and monitoring
    data, models, resources, and tasks. For instance, AWS SageMaker [[69](#bib.bib69)]
    offers visualization and model versioning tools for rapidly fine-tuning distributed
    DL models. The cloud-based paradigm is commonly applied in enterprise solutions
    across various industries, such as image and video intelligence [[70](#bib.bib70)],
    medical diagnosis [[71](#bib.bib71)], and cloud manufacturing [[72](#bib.bib72),
    [73](#bib.bib73)].'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于云的分布式深度学习范式可以被视为集群基础范式的扩展，如图[6a](#S2.F6.sf1 "在图6 ‣ II-B3 流水线并行 ‣ II-B 分布式深度学习并行模式
    ‣ II 深度学习与分布式深度学习基础 ‣ 高效通信的大规模分布式深度学习：全面调查")所示。云服务提供商将集群基础的分布式深度学习平台封装成平台即服务（PaaS）产品，并以按需付费的方式提供分布式深度学习服务。终端设备按实际使用的资源收费，包括训练数据的存储、训练和推理的计算资源以及网络通信的流量。与集群基础范式相比，基于云的范式具有三方面的优势：虚拟化、弹性和实用性。首先，云采用包括资源隔离和网络虚拟化在内的虚拟化技术，以促进云中多个租户之间计算和通信资源的共享，从而降低租户的资源成本。其次，云可以根据运行时学习工作负载弹性调整资源容量。这使得分布式深度学习平台可以扩展到更大规模以处理更大的模型和峰值工作负载，或者在不需要时收缩以节省资源。第三，云通常提供更多用于管理和监控数据、模型、资源和任务的实用工具。例如，AWS
    SageMaker [[69](#bib.bib69)] 提供可视化和模型版本控制工具，用于快速微调分布式深度学习模型。基于云的范式在各种行业的企业解决方案中得到了广泛应用，如图像和视频智能
    [[70](#bib.bib70)]、医疗诊断 [[71](#bib.bib71)] 和云制造 [[72](#bib.bib72), [73](#bib.bib73)]。
- en: II-C2 Edge-based Distributed DL and Federated Learning
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 基于边缘的分布式深度学习与联邦学习
- en: 'With the rapid development of IoT and IoV technologies, the edge-based distributed
    DL paradigm emerges. Compared to the cluster/cloud-based paradigm, the edge-based
    paradigm follows a hierarchical architecture, which includes an additional edge
    layer connecting the cluster/cloud and heterogeneous end devices (as shown in
    Fig. [6b](#S2.F6.sf2 "In Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed
    DL Parallelism Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). The key idea
    of this paradigm is to place training and inference capabilities close to data
    to enhance the efficiency and latency of distributed DL. End devices, such as
    mobile devices and vehicles, typically have limited storage and computational
    capacities and unstable wireless network connections but require real-time data
    collection and DL inference results. The cloud paradigm may not be the ideal solution
    for such real-time tasks due to the high communication latency. The edge layer
    places edge servers close to the end devices to expand the storage, training,
    and inference capability of the end devices in an efficient manner for communication.
    The edge-based paradigm is applied widely in scenarios that demand high resource
    efficiency and low latency, such as AR/VR [[74](#bib.bib74)], intelligent transportation
    systems [[75](#bib.bib75)], and smart industry and manufacturing [[76](#bib.bib76)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '随着物联网（IoT）和车联网（IoV）技术的快速发展，基于边缘的分布式深度学习（DL）范式应运而生。与基于集群/云的范式相比，基于边缘的范式采用了层次化架构，其中包含一个额外的边缘层，用于连接集群/云和异构终端设备（如图[6b](#S2.F6.sf2
    "In Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes
    ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")所示）。这一范式的关键思想是将训练和推断能力接近数据，以提高分布式深度学习的效率和延迟。终端设备，如移动设备和车辆，通常具有有限的存储和计算能力，以及不稳定的无线网络连接，但需要实时数据采集和深度学习推断结果。由于通信延迟较高，云范式可能不是这种实时任务的理想解决方案。边缘层将边缘服务器放置在接近终端设备的位置，以高效地扩展终端设备的存储、训练和推断能力。基于边缘的范式广泛应用于需要高资源效率和低延迟的场景，如增强现实/虚拟现实[[74](#bib.bib74)]、智能交通系统[[75](#bib.bib75)]和智能工业与制造[[76](#bib.bib76)]。'
- en: Federated Learning (FL) has recently emerged as a rapidly growing research topic,
    primarily exploiting the capacity of edge-based paradigms in collaborative DL.
    To uphold privacy and data security while multiple parties train a global model
    collaboratively, each party trains a model with local data on its edge servers
    and shares only gradients, avoiding sharing raw data with other parties and the
    central cluster. FL finds applications in various domains, with a particular focus
    on privacy and data security issues across multiple parties, including healthcare [[77](#bib.bib77),
    [78](#bib.bib78)], finance [[79](#bib.bib79)], and autonomous driving [[80](#bib.bib80)].
    Nevertheless, FL is characterized by the heterogeneity of 1) geographical locations
    and computational and communication capacities of devices, 2) model structures,
    and 3) non-independent-and-identically-distributed (non-IID) data within each
    party. This heterogeneity poses a major challenge for achieving high-performance
    FL.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）最近成为一个快速增长的研究主题，主要利用基于边缘的范式在协作深度学习中的能力。为了在多个参与方协作训练全球模型时保持隐私和数据安全，每个参与方在其边缘服务器上用本地数据训练模型，并仅分享梯度，避免与其他方和中央集群共享原始数据。FL在多个领域找到应用，特别关注多个参与方之间的隐私和数据安全问题，包括医疗保健[[77](#bib.bib77),
    [78](#bib.bib78)]、金融[[79](#bib.bib79)]和自动驾驶[[80](#bib.bib80)]。然而，FL的特点是1）地理位置、计算和通信能力的异质性，2）模型结构的异质性，以及3）各方内部非独立同分布（non-IID）数据的异质性。这些异质性对实现高性能FL构成了重大挑战。
- en: II-C3 P2P-based Decentralized Distributed DL
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 P2P基础的去中心化分布式深度学习
- en: 'The P2P-based distributed DL paradigm (as shown in Fig. [6c](#S2.F6.sf3 "In
    Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes
    ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) utilizes decentralized networks
    for collaborative DL model training. Unlike the above paradigms that follow a
    between-layer communication pattern, the P2P-based distributed DL paradigm is
    distinguished by the communication between peers at each layer. Peers collaborate
    to complete training tasks by exchanging learning information, such as the data,
    models, parameters, or gradients, in a gossip manner. Due to its decentralized
    nature, the P2P-based paradigm is expected to demonstrate scalability and fault
    tolerance. Addressing concerns related to privacy protection, data integrity,
    and model security is crucial in both design and implementation of this paradigm.
    It finds applications in decentralized trading systems [[81](#bib.bib81)], swarm
    intelligence of autonomous vehicles [[82](#bib.bib82)], and mobile robotic systems [[83](#bib.bib83)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 基于P2P的分布式深度学习范式（如图 [6c](#S2.F6.sf3 "图6 ‣ II-B3 管道并行 ‣ II-B 分布式深度学习并行模式 ‣ II
    深度学习及分布式深度学习基础 ‣ 通信高效的大规模分布式深度学习：全面综述")所示）利用去中心化网络进行协作式深度学习模型训练。与遵循层间通信模式的上述范式不同，基于P2P的分布式深度学习范式以每层之间的对等通信为特征。对等体通过以闲聊方式交换学习信息（如数据、模型、参数或梯度）来协作完成训练任务。由于其去中心化的性质，P2P基础的范式预计将展示出可扩展性和容错性。在该范式的设计和实现中，解决隐私保护、数据完整性和模型安全性的问题至关重要。它在去中心化交易系统 [[81](#bib.bib81)]、自动驾驶车辆的群体智能 [[82](#bib.bib82)]和移动机器人系统 [[83](#bib.bib83)]中找到应用。
- en: There are also hybrid distributed DL paradigms, which are mixed architectures
    combining the above paradigms for complex DL scenarios [[84](#bib.bib84)]. In
    a hybrid paradigm, communication can happen between adjacent or skipping layers,
    or among peers within each layer. It capitalizes on the computational power of
    the cluster and cloud, while maintaining low-latency through the edge and ensuring
    privacy and network robustness through the P2P-based component.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 还有混合分布式深度学习范式，这些是将上述范式结合起来用于复杂深度学习场景的混合架构 [[84](#bib.bib84)]。在混合范式中，通信可以发生在相邻层之间、跳过层之间，或在每层内部的对等体之间。它利用了集群和云的计算能力，同时通过边缘保持低延迟，并通过基于P2P的组件确保隐私和网络鲁棒性。
- en: 'TABLE III: Studies on communication-efficient model-synchronization algorithms
    for large-scale distributed DL'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：大规模分布式深度学习的通信高效模型同步算法研究
- en: '|                                                  Category | Strategy&Ref.
    | Year | Highlight |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|                                                  类别 | 策略与参考文献 | 年份 | 亮点 |'
- en: '|   Distributed SGD variants ([III-A](#S3.SS1 "III-A Synchronous, Asynchronous,
    and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Synchronous ([III-A1](#S3.SS1.SSS1 "III-A1 Synchronous SGD ‣ III-A
    Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | OSP [[85](#bib.bib85)] | 2023 | Overlapping unimportant
    gradient synchronization with computation of the next iteration. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|   分布式SGD变种 ([III-A](#S3.SS1 "III-A 同步、异步和其他分布式SGD ‣ III 通信高效的模型同步 ‣ 通信高效的大规模分布式深度学习：全面综述"))
    | 同步 ([III-A1](#S3.SS1.SSS1 "III-A1 同步SGD ‣ III-A 同步、异步和其他分布式SGD ‣ III 通信高效的模型同步
    ‣ 通信高效的大规模分布式深度学习：全面综述")) | OSP [[85](#bib.bib85)] | 2023 | 将不重要的梯度同步与下一次迭代的计算重叠。
    |'
- en: '| Asynchronous ([III-A2](#S3.SS1.SSS2 "III-A2 Asynchronous SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Downpour SGD [[86](#bib.bib86)] | 2012 | Asynchronous with independent
    workers and PS shards. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 异步 ([III-A2](#S3.SS1.SSS2 "III-A2 异步SGD ‣ III-A 同步、异步和其他分布式SGD ‣ III 通信高效的模型同步
    ‣ 通信高效的大规模分布式深度学习：全面综述")) | Downpour SGD [[86](#bib.bib86)] | 2012 | 使用独立工作者和PS分片的异步方法。
    |'
- en: '| PS+ [[87](#bib.bib87)] | 2022 | Pulling the global model eagerly before pushing
    the latest local updates. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| PS+ [[87](#bib.bib87)] | 2022 | 在推送最新的本地更新之前，急切地拉取全球模型。 |'
- en: '| Stale synchronous ([III-A3](#S3.SS1.SSS3 "III-A3 Stale synchronous SGD ‣
    III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | SSP [[88](#bib.bib88)] | 2013 | Allowing asynchronous
    model synchronization within a limited staleness bound. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 过时同步 ([III-A3](#S3.SS1.SSS3 "III-A3 Stale synchronous SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | SSP [[88](#bib.bib88)] | 2013 | 在有限的过时界限内允许异步模型同步。 |'
- en: '| AdaptiveRevision [[89](#bib.bib89)] | 2014 | Adaptive learning rate for minimum
    regret bound in stale SGD. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 自适应修订 [[89](#bib.bib89)] | 2014 | 在过时 SGD 中自适应学习率以获得最小遗憾界限。 |'
- en: '|  | R²P [[90](#bib.bib90)] | 2019 | Workers synchronize in a fixed round-robin
    order with adaptive minibatch size to avoid network contention. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | R²P [[90](#bib.bib90)] | 2019 | 工作节点按照固定的轮询顺序同步，并通过自适应小批量大小避免网络争用。 |'
- en: '|  | HSP [[91](#bib.bib91)] | 2022 | Workers synchronize in a fixed round-robin
    order and switch between synchronous and stale modes. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | HSP [[91](#bib.bib91)] | 2022 | 工作节点按照固定的轮询顺序进行同步，并在同步模式和过时模式之间切换。 |'
- en: '| Adaptive local ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | EASGD [[92](#bib.bib92)] | 2015 | Incrementing synchronization frequencies
    iteratively. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 自适应本地 ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous, Asynchronous,
    and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | EASGD [[92](#bib.bib92)] | 2015 | 迭代增加同步频率。 |'
- en: '| AdaComm [[93](#bib.bib93)] | 2019 | Low synchronization frequency first for
    fast convergence and high frequency later for lower errors. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| AdaComm [[93](#bib.bib93)] | 2019 | 首先低频同步以快速收敛，随后高频同步以降低误差。 |'
- en: '|  | Post-local [[94](#bib.bib94)] | 2020 | Adding large-minibatch local SGD
    as the second stage; hierarchical local SGD at different infrastructure levels.
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 后本地 [[94](#bib.bib94)] | 2020 | 将大批量本地 SGD 作为第二阶段；在不同基础设施级别的层次化本地 SGD。
    |'
- en: '|  | SlowMo [[95](#bib.bib95)] | 2020 | Local SGD with momentum updates. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | SlowMo [[95](#bib.bib95)] | 2020 | 带动量更新的本地 SGD。 |'
- en: '| Event-triggered local ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | LAG [[96](#bib.bib96)] | 2018 | Triggering synchronization when accumulated
    parameter changes are smaller than current parameter changes. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 事件触发本地 ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous, Asynchronous,
    and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | LAG [[96](#bib.bib96)] | 2018 | 当累积的参数变化小于当前参数变化时触发同步。 |'
- en: '| DETSGRAD [[97](#bib.bib97)] | 2020 | Triggering synchronization when the
    current parameter change in the PS is larger than a threshold. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DETSGRAD [[97](#bib.bib97)] | 2020 | 当 PS 中当前参数变化大于阈值时触发同步。 |'
- en: '|  | FSP [[98](#bib.bib98)] | 2023 | Fitting loss into an empirical two-stage
    pattern and triggering synchronization at the end of the first stage. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | FSP [[98](#bib.bib98)] | 2023 | 将损失拟合到经验性的两阶段模式中，并在第一阶段结束时触发同步。 |'
- en: '| Decentralized ([III-A5](#S3.SS1.SSS5 "III-A5 Decentralized SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | D-PSGD [[99](#bib.bib99)] | 2017 | Theoretically and experimentally
    showing that decentralized SGD has linear speedup performance for convergence.
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 去中心化 ([III-A5](#S3.SS1.SSS5 "III-A5 Decentralized SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | D-PSGD [[99](#bib.bib99)] | 2017 | 理论上和实验上展示了去中心化 SGD 在收敛方面具有线性加速性能。
    |'
- en: '| D² [[100](#bib.bib100)] | 2018 | Aggregating gradients based on differences
    between parameters and between gradients for heterogeneous data. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| D² [[100](#bib.bib100)] | 2018 | 基于参数和梯度之间的差异来聚合梯度，以处理异质数据。 |'
- en: '| Hybrid ([III-A6](#S3.SS1.SSS6 "III-A6 Hybrid SGD for the straggler problem
    ‣ III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | GSSP [[101](#bib.bib101)] | 2022 | Clustering workers
    with similar performance into multiple groups; stale SGD within a group and decentralized
    SGD across groups. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 混合 ([III-A6](#S3.SS1.SSS6 "III-A6 处理滞后问题的混合SGD ‣ III-A 同步、异步及其他分布式SGD ‣ III
    高效通信的模型同步 ‣ 高效通信的大规模分布式深度学习：综合调查")) | GSSP [[101](#bib.bib101)] | 2022 | 将性能相似的工作节点聚集到多个组中；组内使用滞后SGD，组间使用去中心化SGD。
    |'
- en: '| A2S [[102](#bib.bib102)] | 2022 | Stale SGD for fast workers and asynchronous
    SGD for slow workers. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| A2S [[102](#bib.bib102)] | 2022 | 对快速工作节点使用滞后SGD，对慢速工作节点使用异步SGD。 |'
- en: '|  | ASHL [[103](#bib.bib103)] | 2023 | Asynchronous local SGD for faster convergence
    at an early stage and stale SGD for model consistency at a late stage. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | ASHL [[103](#bib.bib103)] | 2023 | 在早期阶段使用异步本地SGD以加速收敛，在晚期阶段使用滞后SGD以保持模型一致性。
    |'
- en: '|   Guarantee | Local ([III-B1](#S3.SS2.SSS1 "III-B1 Convergence of local SGD
    ‣ III-B Convergence Guarantees of Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106),
    [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111)] | 2018-2021 | Convergence guarantees of local SGD w.r.t. the
    number of workers, local updating iterations, entire iterations, etc. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|   保证 | 本地 ([III-B1](#S3.SS2.SSS1 "III-B1 本地SGD的收敛性 ‣ III-B 分布式SGD的收敛保证 ‣
    III 高效通信的模型同步 ‣ 高效通信的大规模分布式深度学习：综合调查")) | [[104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106), [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109),
    [110](#bib.bib110), [111](#bib.bib111)] | 2018-2021 | 本地SGD的收敛保证，涉及工作节点数量、本地更新迭代次数、总迭代次数等。
    |'
- en: '| Asynchronous ([III-B2](#S3.SS2.SSS2 "III-B2 Convergence of asynchronous SGD
    ‣ III-B Convergence Guarantees of Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116)] | 2020-2022 | Convergence guarantees of
    asynchronous SGD w.r.t. the gradient delay, gradient noise, stationary point,
    etc. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 异步 ([III-B2](#S3.SS2.SSS2 "III-B2 异步SGD的收敛性 ‣ III-B 分布式SGD的收敛保证 ‣ III 高效通信的模型同步
    ‣ 高效通信的大规模分布式深度学习：综合调查")) | [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116)] | 2020-2022 | 异步SGD的收敛保证，涉及梯度延迟、梯度噪声、静态点等。
    |'
- en: '|   Heterogeneous FL ([III-C](#S3.SS3 "III-C Model Synchronization in FL ‣
    III Communication-Efficient Model Synchronization ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Random workers ([III-C1](#S3.SS3.SSS1
    "III-C1 Randomly selected workers ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | FedAvg [[117](#bib.bib117)] | 2017 | Randomly selecting
    a fraction of workers in every synchronization round. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|   异构FL ([III-C](#S3.SS3 "III-C FL中的模型同步 ‣ III 高效通信的模型同步 ‣ 高效通信的大规模分布式深度学习：综合调查"))
    | 随机工作节点 ([III-C1](#S3.SS3.SSS1 "III-C1 随机选择的工作节点 ‣ III-C FL中的模型同步 ‣ III 高效通信的模型同步
    ‣ 高效通信的大规模分布式深度学习：综合调查")) | FedAvg [[117](#bib.bib117)] | 2017 | 在每次同步轮次中随机选择一部分工作节点。
    |'
- en: '| NetMax [[118](#bib.bib118)] | 2021 | Optimizing probabilities of selecting
    a peer w.r.t network bandwidth status in decentralized SGD. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| NetMax [[118](#bib.bib118)] | 2021 | 在去中心化SGD中优化选择对等节点的概率，以适应网络带宽状态。 |'
- en: '| Model breaking-down ([III-C2](#S3.SS3.SSS2 "III-C2 Breaking down the model
    ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | ASTW_FedAvg [[21](#bib.bib21)] | 2020 | Synchronizing shallow layers
    more frequently than deep layers. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 模型拆解 ([III-C2](#S3.SS3.SSS2 "III-C2 模型拆解 ‣ III-C FL中的模型同步 ‣ III 高效通信的模型同步
    ‣ 高效通信的大规模分布式深度学习：综合调查")) | ASTW_FedAvg [[21](#bib.bib21)] | 2020 | 比深层网络更频繁地同步浅层网络。
    |'
- en: '| APF [[119](#bib.bib119)] | 2021 | Freezing stable parameters with AIMD frozen
    periods. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| APF [[119](#bib.bib119)] | 2021 | 使用AIMD冻结周期来冻结稳定参数。 |'
- en: '|  | APF# [[120](#bib.bib120)] | 2023 | Aggressive APF by freezing unstable
    parameters with a probability. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | APF# [[120](#bib.bib120)] | 2023 | 通过以一定概率冻结不稳定的参数来实现激进的APF。 |'
- en: '|  | YOGA [[121](#bib.bib121)] | 2023 | Selecting certain layers and peers
    based on data discrepancy and bandwidth for synchronization in decentralized SGD.
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | YOGA [[121](#bib.bib121)] | 2023 | 基于数据差异和带宽选择特定层和对等节点，以进行去中心化SGD的同步。
    |'
- en: '| FL-tailored aggregation ([III-C3](#S3.SS3.SSS3 "III-C3 FL-tailored aggregation
    strategies ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model
    Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | FedProx [[122](#bib.bib122)] | 2020 | Finding local
    parameters that minimize the local loss and magnitude of local updates inexactly.
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| FL定制聚合 ([III-C3](#S3.SS3.SSS3 "III-C3 FL-tailored aggregation strategies
    ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | FedProx [[122](#bib.bib122)] | 2020 | 找到局部参数以不精确地最小化局部损失和局部更新的幅度。
    |'
- en: '| FedNova [[123](#bib.bib123)] | 2020 | Computing global gradients as a scaled
    weighted sum of normalized local gradients of randomly selected workers. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| FedNova [[123](#bib.bib123)] | 2020 | 将全局梯度计算为随机选择工作节点的标准化局部梯度的加权缩放和。 |'
- en: '|  | CGA [[124](#bib.bib124)] | 2021 | Addressing non-IID data in decentralized
    SGD by training local and neighbor models at each worker. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | CGA [[124](#bib.bib124)] | 2021 | 通过在每个工作节点训练本地模型和邻居模型来解决去中心化SGD中的非IID数据问题。
    |'
- en: '|  | AsyNG [[125](#bib.bib125)] | 2023 | Addresses non-IID data in decentralized
    SGD by selecting appropriate neighbors based on loss difference and model staleness.
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | AsyNG [[125](#bib.bib125)] | 2023 | 通过根据损失差异和模型过时程度选择适当的邻居来解决去中心化SGD中的非IID数据问题。
    |'
- en: '| Hierarchical aggregation ([III-C4](#S3.SS3.SSS4 "III-C4 Hierarchical gradient
    aggregation ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Two-level [[126](#bib.bib126)] | 2019 | A global aggregator
    spawning sub-aggregators for membership maintenance and encrypted aggregation.
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 层次聚合 ([III-C4](#S3.SS3.SSS4 "III-C4 Hierarchical gradient aggregation ‣ III-C
    Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | 两级 [[126](#bib.bib126)] | 2019 | 一个全局聚合器生成子聚合器以维护成员身份和加密聚合。 |'
- en: '| FedCH [[127](#bib.bib127)] | 2023 | Optimizing allocation of workers to groups
    in two-level aggregation to minimize loss with resource and time constraints.
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| FedCH [[127](#bib.bib127)] | 2023 | 在两级聚合中优化工作节点分配，以在资源和时间约束下最小化损失。 |'
- en: '|  | TT-HF [[128](#bib.bib128)] | 2021 | Adaptive learning rate and synchronization
    frequency for decentralized SGD within an edge and local SGD across edges. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | TT-HF [[128](#bib.bib128)] | 2021 | 针对边缘内的去中心化SGD和跨边缘的本地SGD调整学习率和同步频率。
    |'
- en: '|  | Moshoit SGD [[129](#bib.bib129)] | 2021 | Clustering pairs of peers into
    different groups iteratively for decentralized SGD in environments with unstable
    networks. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | Moshoit SGD [[129](#bib.bib129)] | 2021 | 在网络不稳定的环境中，通过迭代将对等节点配对为不同组以进行去中心化SGD。
    |'
- en: '| Adaptive training ([III-C5](#S3.SS3.SSS5 "III-C5 Adaptive hyperparameters
    throughout training ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Adaptive FL [[130](#bib.bib130)] | 2019 | Optimizing
    synchronization frequency to minimize errors with heterogeneous resource constraints.
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 自适应训练 ([III-C5](#S3.SS3.SSS5 "III-C5 Adaptive hyperparameters throughout
    training ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model
    Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | 自适应FL [[130](#bib.bib130)] | 2019 | 优化同步频率以最小化具有异质资源约束的错误。
    |'
- en: '| FedLamp [[131](#bib.bib131)] | 2023 | Jointly optimizing synchronization
    frequency and compression ratio with convergence and resource constraints. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| FedLamp [[131](#bib.bib131)] | 2023 | 联合优化同步频率和压缩比，以满足收敛性和资源约束。 |'
- en: '|  | AdaSFL [[132](#bib.bib132)] | 2023 | Jointly optimizing synchronization
    frequency and minibatch size with convergence and resource constraints. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | AdaSFL [[132](#bib.bib132)] | 2023 | 联合优化同步频率和小批量大小，以满足收敛性和资源约束。 |'
- en: '|  | AAFL [[133](#bib.bib133)] | 2023 | Optimizing synchronization frequency
    with bandwidth and convergence constraints via deep reinforcement learning. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | AAFL [[133](#bib.bib133)] | 2023 | 通过深度强化学习在带宽和收敛约束下优化同步频率。'
- en: '|  | FAST [[134](#bib.bib134)] | 2023 | Optimizing synchronization frequency
    and data sampling jointly with resource and time constraints. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | FAST [[134](#bib.bib134)] | 2023 | 优化同步频率和数据采样，同时考虑资源和时间约束。 |'
- en: '|  | AMBLE [[135](#bib.bib135)] | 2022 | Different learning rate, minibatch
    size, and synchronization frequency for fast and slow workers empirically. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | AMBLE [[135](#bib.bib135)] | 2022 | 针对快慢工作者的不同学习率、小批量大小和同步频率进行经验研究。 |'
- en: '| Unlearning ([III-C6](#S3.SS3.SSS6 "III-C6 Ability to forget over time ‣ III-C
    Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | FL unlearning [[136](#bib.bib136)] | 2022 | Efficient unlearning for
    non-stationary data distribution over time. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 反学习 ([III-C6](#S3.SS3.SSS6 "III-C6 Ability to forget over time ‣ III-C Model
    Synchronization in FL ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | FL 反学习 [[136](#bib.bib136)]
    | 2022 | 对非静态数据分布进行高效反学习。 |'
- en: '|   |  |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |'
- en: III Communication-Efficient Model Synchronization
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 通信高效的模型同步
- en: 'Distributed SGD stands as the state-of-art algorithm for model synchronization
    in the distributed training of DNN models. The performance of different algorithms
    of distributed SGD is primarily associated to the trade-off between communication
    efficiency and model consistency. In this section, we first introduce various
    distributed SGD algorithms. Subsequently, we present theoretical analyses on the
    convergence guarantee of distributed SGD. Moreover, we discuss distributed SGD
    algorithms in large-scale FL environments, where heterogeneity across various
    aspects dominates the research focus. Finally, we summarize key lessons learned
    from existing literature, guiding future endeavors in achieving high-performance
    model synchronization for large-scale distributed DL. Table [III](#S2.T3 "TABLE
    III ‣ II-C3 P2P-based Decentralized Distributed DL ‣ II-C Distributed DL Paradigms
    ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") provides a summary of the
    related studies.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '分布式 SGD 是在分布式训练 DNN 模型中用于模型同步的最先进算法。不同分布式 SGD 算法的性能主要与通信效率和模型一致性之间的权衡有关。本节首先介绍各种分布式
    SGD 算法。随后，我们对分布式 SGD 的收敛保证进行理论分析。此外，我们讨论了在大规模联邦学习环境中分布式 SGD 算法，其中异质性主导了研究重点。最后，我们总结了从现有文献中获得的关键经验，以指导未来在大规模分布式深度学习中实现高性能模型同步的努力。表
    [III](#S2.T3 "TABLE III ‣ II-C3 P2P-based Decentralized Distributed DL ‣ II-C
    Distributed DL Paradigms ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") 提供了相关研究的总结。'
- en: '![Refer to caption](img/e528e7dcf292620aa33bdcdbae7492f1.png)![Refer to caption](img/a417be9e1048ad275a00eef657b50ff4.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e528e7dcf292620aa33bdcdbae7492f1.png)![参见说明](img/a417be9e1048ad275a00eef657b50ff4.png)'
- en: (a) Synchronous SGD
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 同步 SGD
- en: '![Refer to caption](img/557b3bbee3e96d946575eae6f3e52dba.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/557b3bbee3e96d946575eae6f3e52dba.png)'
- en: (b) Asynchronous SGD
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 异步 SGD
- en: '![Refer to caption](img/9fa751f4ac091a65a6769c3cef369980.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9fa751f4ac091a65a6769c3cef369980.png)'
- en: (c) Stale SGD
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 过时的 SGD
- en: '![Refer to caption](img/ec3ac60c58892d649d87fa9fbccc6153.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec3ac60c58892d649d87fa9fbccc6153.png)'
- en: (d) Local SGD
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 本地 SGD
- en: 'Figure 7: Typical variants of distributed SGD'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：分布式 SGD 的典型变体
- en: III-A Synchronous, Asynchronous, and Other Distributed SGD
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 同步、异步及其他分布式 SGD
- en: 'This subsection introduces optimizations for various distributed SGD algorithms,
    including synchronous, asynchronous, stale, local, decentralized, and hybrid SGD
    algorithms. Fig. [7](#S3.F7 "Figure 7 ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") illustrates some typical distributed SGD algorithms.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '本小节介绍了各种分布式 SGD 算法的优化，包括同步、异步、过时、本地、去中心化和混合 SGD 算法。图 [7](#S3.F7 "Figure 7 ‣
    III Communication-Efficient Model Synchronization ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") 说明了一些典型的分布式 SGD 算法。'
- en: III-A1 Synchronous SGD
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 同步 SGD
- en: 'In fully synchronous SGD, also known as minibatch distributed SGD, every gradient
    iteration serves as a synchronization round (see Fig. [7a](#S3.F7.sf1 "In Figure
    7 ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). Workers retrieve
    the most up-to-date model parameters from the PS during every gradient iteration,
    where the PS updates the global model based on the gradients from all workers,
    resulting in more consistent models across all workers and a more deterministic
    and coordinated training progress. However, since the training process on any
    worker is blocked until all other workers have synchronized to the same progress,
    this strictly consistent-pace strategy introduces significant communication overhead,
    such as idle time caused by straggling workers. Some work, such as Overlapped
    Synchronization Parallel (OSP) [[85](#bib.bib85)], introduces a technique of overlapping
    computation and communication aimed at alleviating this blocking communication
    overhead of fully synchronous SGD. OSP divides the synchronization communication
    of synchronous SGD into two stages, a predecessor synchronization stage for important
    gradients and a successor stage for unimportant gradients. It enables the successor
    communication stage to overlap with the feedforward and backpropagation computation
    of the next training iteration. However, the blocking communication overhead cannot
    be eliminated completely in synchronous SGD because of the existence of a strict
    consistency constraint. This motivates the exploration of numerous variants of
    distributed SGD algorithms that relax the consistency constraint.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全同步的SGD中，也称为小批量分布式SGD，每次梯度迭代都作为一次同步轮次（参见图 [7a](#S3.F7.sf1 "在图7中 ‣ III 传播效率模型同步
    ‣ 传播效率的大规模分布式深度学习：综合调查")）。在每次梯度迭代中，工作节点从PS中获取最新的模型参数，PS基于所有工作节点的梯度更新全局模型，从而使所有工作节点中的模型更加一致，并且训练进度更加确定和协调。然而，由于任何工作节点上的训练过程在所有其他工作节点同步到相同进度之前被阻塞，这种严格的一致步伐策略引入了显著的通信开销，例如因滞后的工作节点造成的空闲时间。一些工作，例如重叠同步并行（OSP）[[85](#bib.bib85)]，引入了计算和通信重叠的技术，旨在缓解完全同步SGD的阻塞通信开销。OSP将同步SGD的同步通信分为两个阶段，一个是针对重要梯度的前导同步阶段，另一个是针对不重要梯度的后继阶段。它使得后继通信阶段能够与下一个训练迭代的前向传播和反向传播计算重叠。然而，由于存在严格的一致性约束，阻塞通信开销在同步SGD中无法完全消除。这促使了对放松一致性约束的分布式SGD算法的众多变体的探索。
- en: III-A2 Asynchronous SGD
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 异步SGD
- en: 'The impact of blocking communication overhead of synchronous SGD can be challenging
    in large-scale distributed DL when many heterogeneous workers participate in the
    synchronization. Asynchronous SGD addresses the blocking communication overhead
    by allowing workers to synchronize their models with the PS independently, without
    waiting for gradient updates from other workers (see Fig. [7b](#S3.F7.sf2 "In
    Figure 7 ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). Compared to
    synchronous SGD, asynchronous SGD eliminates the synchronization blocking time
    and is more suitable for large-scale distributed training in heterogeneous environments.
    To enhance asynchronous SGD for large data, Downpour SGD [[86](#bib.bib86)], a
    fully asynchronous SGD algorithm for data-parallel training with a centralized
    PS, segments the PS into multiple shards, each of which is responsible for storing
    and applying SGD updates to a portion of the global model parameters. Downpour
    SGD is asynchronous in two aspects: model replicas in workers operate independently
    of each other, which is the common form of asynchronous SGD, and PS shards also
    operate independently of each other. Downpour SGD has been verified experimentally
    to utilize network bandwidth efficiently and stabilize volatile parameters using
    the Adagrad optimizer for non-convex objectives.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模分布式深度学习中，当许多异构工作节点参与同步时，同步SGD的通信开销的影响可能会很具挑战性。异步SGD通过允许工作节点独立于其他工作节点同步其模型到PS，从而解决了阻塞通信开销的问题（参见图[7b](#S3.F7.sf2
    "在图7 ‣ III 传输高效模型同步 ‣ 传输高效的大规模分布式深度学习：综合调查")）。与同步SGD相比，异步SGD消除了同步阻塞时间，更适合在异构环境中进行大规模分布式训练。为了提升异步SGD在大数据上的表现，Downpour
    SGD [[86](#bib.bib86)]，一种用于数据并行训练的完全异步SGD算法，采用了中心化PS的多个分片，每个分片负责存储和应用SGD更新到全局模型参数的一部分。Downpour
    SGD在两个方面是异步的：工作节点中的模型副本相互独立，这是异步SGD的常见形式，而PS分片也相互独立。实验验证表明，Downpour SGD能有效利用网络带宽，并使用Adagrad优化器稳定非凸目标的波动参数。
- en: Efficient asynchronous SGD is facilitated by a PS that optimizes computation
    and communication parallelism. PS+ [[87](#bib.bib87)] accelerates asynchronous
    SGD by decoupling gradient pushing and the successive parameter pulling operations,
    enabling workers to pull the global parameters eagerly from the PS for the next
    gradient iteration even before pushing the latest local gradients. This early-pulling
    strategy overlaps computational and communication workloads in every synchronization
    round, though it introduces additional model staleness. Nevertheless, the impact
    of staleness can be mitigated by a delay-adaptive strategy for the global learning
    rate, adjusting it smaller dynamically when the staleness grows large throughout
    the training process.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的异步SGD通过优化计算和通信并行性的PS得到支持。PS+ [[87](#bib.bib87)]通过解耦梯度推送和随后的参数拉取操作来加速异步SGD，使得工作节点即使在推送最新的本地梯度之前，也能够提前从PS中拉取全局参数进行下一次梯度迭代。这种提前拉取策略在每个同步轮次中重叠计算和通信负载，尽管它引入了额外的模型过时。然而，通过对全局学习率进行延迟自适应策略可以减轻过时的影响，在训练过程中当过时变大时动态调整学习率。
- en: III-A3 Stale synchronous SGD
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 过时的同步SGD
- en: 'However, asynchronous SGD introduces staleness among models in different workers,
    leading to less consistent models across workers, causing oscillations during
    the training process potentially. Stale synchronous SGD, a family of distributed
    SGD variants, relaxes model consistency among workers within a bounded model staleness
    (Fig. [7c](#S3.F7.sf3 "In Figure 7 ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")). This trade-off between model consistency and communication overhead
    aims for improved synchronization performance. In Stale Synchronous Parallel (SSP) [[88](#bib.bib88)],
    the PS maintains historical model parameters and gradients, enabling workers to
    retrieve a historical global model that is a bounded number of synchronization
    intervals ago. Consequently, faster workers can proceed without waiting for slower
    workers in every synchronization round. They use the latest updated model, as
    long as slower workers are not falling behind beyond the bounded interval. Ensuring
    a theoretical convergence guarantee, the bounded staleness reduces synchronization
    waiting time among workers and results in a faster convergence rate than fully
    synchronous SGD. To limit the impact of model staleness, AdaptiveRevision [[89](#bib.bib89)]
    adjusts the learning rate appropriately and efficiently for gradient updates between
    periodic synchronization rounds. This adjustment minimizes the regret bound, defined
    based on the difference between the actual and optimal model parameters.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，异步SGD会导致不同工人之间的模型过时，导致工人之间的模型一致性降低，从而在训练过程中可能会引发振荡。过时同步SGD是一类分布式SGD变体，它在有界模型过时的范围内放宽了工人之间的模型一致性（见图 [7c](#S3.F7.sf3
    "在图7 ‣ III 通信高效模型同步 ‣ 通信高效的大规模分布式深度学习：综合调查")）。这种模型一致性与通信开销之间的权衡旨在改善同步性能。在过时同步并行（SSP） [[88](#bib.bib88)]
    中，PS 维护历史模型参数和梯度，使工人能够检索到一个有界数量同步间隔之前的历史全局模型。因此，较快的工人可以在每次同步轮次中不必等待较慢的工人。他们使用最新的更新模型，只要较慢的工人没有落后超过有界间隔。确保理论收敛保证，有界的过时性减少了工人之间的同步等待时间，并导致比完全同步SGD更快的收敛速度。为了限制模型过时的影响，AdaptiveRevision [[89](#bib.bib89)]
    在周期性同步轮次之间适当地并有效地调整学习率用于梯度更新。这种调整最小化了遗憾界限，该界限是基于实际模型参数与最优模型参数之间的差异定义的。
- en: Workers synchronizing models with the PS at the consistent pace can lead to
    the thundering herd problem in bulk synchronization of synchronous SGD, causing
    network contention in the PS bandwidth. To address this network contention issue,
    R²P [[90](#bib.bib90)] and HSP [[91](#bib.bib91)] introduce similar schemes for
    stale synchronous SGD, where workers update models to the PS in a fixed round-robin
    order. Consequently, this minimizes contention in the PS network bandwidth, and
    models in workers are evenly staggered, indicating bounded model staleness. Additionally,
    to utilize computational resources efficiently in heterogeneous computing clusters,
    R²P assigns larger minibatch sizes for faster workers dynamically, keeping them
    engaged throughout the round-robin iteration when it is other worker’s turn to
    synchronize with the PS. In contrast, HSP allows workers to switch between synchronous
    and stale modes based on the resource utilization status.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在与PS（参数服务器）以一致的节奏同步模型时，可能会导致同步SGD中的雷鸣般的群体问题，从而在PS带宽中引发网络争用。为了解决这一网络争用问题，R²P [[90](#bib.bib90)]
    和 HSP [[91](#bib.bib91)] 引入了类似的过时同步SGD方案，其中工人以固定的轮询顺序更新模型到PS。因此，这可以最小化PS网络带宽中的争用，并且工人中的模型被均匀地错开，表明模型过时是有界的。此外，为了在异构计算集群中高效利用计算资源，R²P
    动态地为较快的工人分配较大的小批量大小，使他们在其他工人轮到与PS同步时保持忙碌。相比之下，HSP 允许工人根据资源利用状态在同步模式和过时模式之间切换。
- en: III-A4 Local SGD
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 本地SGD
- en: 'The above synchronous and asynchronous SGD algorithms demand frequent communication
    between workers and the PS, leading to significant communication overhead, particularly
    in large-scale clusters. To decrease communication frequency in large-scale distributed
    training, an alternative model-synchronization algorithm, known as local distributed
    SGD or local SGD, has been proposed. Local SGD allows workers to update local
    models for multiple iterations and only synchronize models with the PS periodically
    to maintain the most current global model (Fig. [7d](#S3.F7.sf4 "In Figure 7 ‣
    III Communication-Efficient Model Synchronization ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")). In addition to concerns
    about synchronization overhead, allowing local updates in the worker implies more
    exploration in the existence of local optima with distributed SGD, potentially
    resulting in enhanced optimization results. A general form of local SGD can be
    characterized by four parameters: synchronous or asynchronous synchronization
    patterns, the faction of workers performing updates in each synchronization round,
    the number of local gradient update iterations in each synchronization round (corresponding
    to the synchronization frequency), and the local minibatch size and learning rate
    used for each local iteration.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的同步和异步SGD算法需要工人和参数服务器之间频繁的通信，导致了显著的通信开销，尤其是在大规模集群中。为了减少大规模分布式训练中的通信频率，提出了一种替代的模型同步算法，称为局部分布式SGD或局部SGD。局部SGD允许工人在多个迭代中更新本地模型，并且仅定期与参数服务器同步模型，以保持最新的全局模型（图[7d](#S3.F7.sf4
    "在图7 ‣ III 通信高效的模型同步 ‣ 高效的大规模分布式深度学习：全面调查")）。除了对同步开销的关注，允许工人进行本地更新还意味着在分布式SGD中可能会更多地探索局部最优解的存在，潜在地导致更好的优化结果。局部SGD的一般形式可以通过四个参数来表征：同步或异步的同步模式、每次同步轮次中进行更新的工人比例、每次同步轮次中的本地梯度更新迭代次数（对应于同步频率），以及每次本地迭代中使用的本地小批量大小和学习率。
- en: '$\bullet$ Adaptive local SGD: Various local SGD algorithms use all workers
    in each synchronization round and adaptive settings for the synchronization frequency
    and minibatch size, whether synchronously or asynchronously. In asynchronous Elastic
    Averaging SGD (EASGD) [[92](#bib.bib92)], the synchronization frequency of each
    worker is controlled by a locally maintained clock, which is incremented by one
    after every local update iteration. The PS updates the global model in a moving
    average style for every synchronization from a worker. The stability of asynchronous
    EASGD is guaranteed theoretically, and experimental results demonstrate its significantly
    faster convergence compared to Downpour SGD. By exploring the trade-off between
    convergence and wall-clock runtime with different numbers of local update iterations,
    AdaComm [[93](#bib.bib93)] adjusts the number of local update iterations over
    time to minimize the optimization error at a given wall-clock time. AdaComm suggests
    an optimal strategy for local SGD convergence concerning synchronization frequency:
    starting with infrequent synchronization can reduce communication delay and improve
    convergence rate initially; increasing synchronization frequency gradually over
    time contributes to achieving lower optimization error. Lin et al. [[94](#bib.bib94)]
    propose two local SGD variants: Post-local SGD, aiming to reach high generalization,
    and Hierarchical Local SGD, to optimize the trade-off between computation and
    communication. Large-minibatch distributed SGD has advantages in training speed,
    but it often does not generalize as well as local SGD [[137](#bib.bib137)]. Post-local
    SGD closes this generalization gap by adding large-minibatch local SGD as the
    second training phase after the initial phase of standard minibatch distributed
    SGD. The initial phase can be viewed as the warm-up phase for tuning the learning
    rate. Hierarchical Local SGD employs local SGD as an inner loop on each layer
    of a hierarchical system comprising GPUs, computing nodes, racks, or even data
    centers. This hierarchical approach limits intensive model synchronization within
    each layer and mitigates communication overhead between different layers. SlowMo [[95](#bib.bib95)]
    is a momentum-based variant of local SGD, where the global model in the PS is
    updated through SGD with local update momentum. Supported by experimental results
    on various image and NLP benchmarks, the momentum-based local SGD is believed
    to converge faster and generalize better than vanilla local SGD.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 自适应局部SGD：各种局部SGD算法在每次同步轮次中使用所有工作节点，并且在同步频率和小批量大小的设置上采用自适应方法，无论是同步还是异步。在异步Elastic
    Averaging SGD (EASGD) [[92](#bib.bib92)]中，每个工作节点的同步频率由本地维护的时钟控制，该时钟在每次本地更新迭代后递增。PS以移动平均的方式更新全局模型，并进行每次来自工作节点的同步。异步EASGD的稳定性在理论上得到保证，实验结果表明其收敛速度明显快于Downpour
    SGD。通过探索不同数量的本地更新迭代与墙钟时间的权衡，AdaComm [[93](#bib.bib93)]会随着时间的推移调整本地更新迭代次数，以在给定的墙钟时间内最小化优化误差。AdaComm为局部SGD收敛建议了一种关于同步频率的最佳策略：初期进行不频繁的同步可以减少通信延迟并提高初始收敛速度；随着时间的推移逐渐增加同步频率有助于实现更低的优化误差。Lin等人 [[94](#bib.bib94)]提出了两种局部SGD变体：Post-local
    SGD，旨在达到高泛化性，以及Hierarchical Local SGD，旨在优化计算与通信之间的权衡。大批量分布式SGD在训练速度上具有优势，但通常不如局部SGD泛化性能好 [[137](#bib.bib137)]。Post-local
    SGD通过在标准小批量分布式SGD的初始阶段后，添加大批量局部SGD作为第二训练阶段，从而缩小了这种泛化差距。初始阶段可以视为调优学习率的热身阶段。Hierarchical
    Local SGD在包括GPU、计算节点、机架甚至数据中心的层级系统中的每一层上采用局部SGD作为内循环。这种层级方法限制了每层内部的模型同步强度，并减轻了不同层之间的通信开销。SlowMo [[95](#bib.bib95)]是局部SGD的一个基于动量的变体，其中PS中的全局模型通过带有局部更新动量的SGD进行更新。实验结果表明，基于动量的局部SGD收敛速度更快，泛化能力优于普通局部SGD。
- en: '$\bullet$ Event-triggered local SGD: One category of asynchronous local SGD
    algorithms is event-triggered SGD. Representative works in this category include
    Lazily Aggregated Gradient (LAG) [[96](#bib.bib96)] and Distributed Event-Triggered
    Stochastic GRAdient Descent (DETSGRAD) [[97](#bib.bib97)], with model synchronization
    triggered by a specific condition on gradients. Workers and the PS synchronize
    models only when the gradients of the local or global model change significantly.
    In the case of LAG, the synchronization is triggered in the PS or a worker when
    the accumulated parameter change within a specific period is smaller than the
    most recent parameter change in the PS or gradient change in a worker. LAG is
    proven theoretically to converge for objectives that are smooth and strongly convex,
    convex, or non-convex. In the case of DETSGRAD, the triggering condition is met
    in the PS when the current parameter change exceeds a threshold, which decreases
    as the training iteration increases. The convergence of DETSGRAD is proved under
    a series of assumptions for the non-convex objectives. Rather than using a synchronization
    trigger conditioned on gradient changes, Flexible Synchronous Parallel (FSP) [[98](#bib.bib98)]
    uses a trigger conditioned on optimization gains. This is based on an empirical
    observation that the loss change w.r.t the objective function can be divided into
    two stages, where the loss decreases linearly faster in the former stage than
    in the latter stage. FSP fits the loss pattern of the local SGD process into this
    two-stage pattern and identifies the barrier between these two stages as the optimal
    moment for model synchronization, as further local model synchronization in the
    latter stage delivers inferior optimization gain. FSP provides convergence guarantee
    for smooth and strongly convex objectives with Lipschitz-continuous gradients.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 事件触发的局部 SGD：异步局部 SGD 算法的一类是事件触发的 SGD。这类算法的代表作包括 Lazily Aggregated
    Gradient (LAG) [[96](#bib.bib96)] 和 Distributed Event-Triggered Stochastic GRAdient
    Descent (DETSGRAD) [[97](#bib.bib97)]，其模型同步由梯度的特定条件触发。只有在局部或全局模型的梯度发生显著变化时，工人和参数服务器才会同步模型。在
    LAG 的情况下，当特定时间段内的累计参数变化小于 PS 中最近的参数变化或工人中的梯度变化时，会在 PS 或工人中触发同步。理论上，LAG 被证明对平滑且强凸的、凸的或非凸的目标函数收敛。在
    DETSGRAD 的情况下，当当前参数变化超过一个阈值（该阈值随着训练迭代的增加而减小）时，PS 中会满足触发条件。DETSGRAD 的收敛性在一系列非凸目标函数的假设下得到了证明。与基于梯度变化的同步触发不同，Flexible
    Synchronous Parallel (FSP) [[98](#bib.bib98)] 使用基于优化增益的触发条件。这基于一个经验观察，即相对于目标函数的损失变化可以分为两个阶段，其中前一阶段的损失下降速度线性更快。FSP
    将局部 SGD 过程的损失模式拟合到这种两阶段模式中，并将这两个阶段之间的屏障识别为模型同步的最佳时刻，因为在后一阶段进一步的局部模型同步会带来较差的优化增益。FSP
    为平滑且强凸目标函数提供了收敛保证，且具有 Lipschitz 连续的梯度。
- en: III-A5 Decentralized SGD
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 去中心化 SGD
- en: The distributed SGD algorithms mentioned above are centralized approaches, featuring
    a central PS. This centralized setup is susceptible to traffic jam and single-point
    failure in a large-scale distributed DL setting. Decentralized SGD addresses this
    bottleneck by eliminating the central PS during model synchronization and typically
    relies on gossip-based communication among workers. To explore the advantage of
    decentralized SGD over centralized SGD, Lian et al. [[99](#bib.bib99)] conduct
    a theoretical and experimental comparison of the convergence performance between
    these two approaches. The results demonstrate that decentralized SGD converges
    as effectively as centralized SGD but outperforms it in terms of the maximum communication
    cost at any single point. To the best of our knowledge, this is the first work
    to offer a theoretical analysis of the linear speedup performance concerning the
    number of workers for decentralized SGD. Nevertheless, this result assumes similar
    data distributions across workers and its robustness to data in heterogeneous
    distributions remains unknown. By investigating the convergence performance of
    decentralized SGD with data variance among workers, D² [[100](#bib.bib100)] is
    proposed as a decentralized SGD variant where the parameters shared among peers
    for aggregation are calculated based on the difference between local parameters
    and the difference between gradients of consecutive iterations. This parameter
    updating strategy yields a theoretically improved convergence rate for decentralized
    SGD, particularly when dealing with heterogeneous data across workers, assuming
    Lipschitz-continuous and variance-bounded gradients.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的分布式SGD算法是集中式方法，具有中央PS。这种集中式设置在大规模分布式深度学习环境中容易出现交通堵塞和单点故障。去中心化SGD通过在模型同步过程中消除中央PS来解决这一瓶颈，并通常依赖于工作节点之间的基于闲聊的通信。为了探索去中心化SGD相对于集中式SGD的优势，Lian等人[[99](#bib.bib99)]进行了这两种方法收敛性能的理论和实验比较。结果表明，去中心化SGD的收敛效果与集中式SGD一样有效，但在任何单点的最大通信成本方面优于集中式SGD。据我们所知，这是首次对去中心化SGD关于工作节点数量的线性加速性能进行理论分析的研究。然而，这一结果假设了工作节点之间的数据分布相似，其对异构数据的鲁棒性仍然未知。通过调查去中心化SGD在工作节点之间数据变异的收敛性能，D²[[100](#bib.bib100)]被提出作为去中心化SGD的一个变体，其中用于聚合的共享参数是基于局部参数之间的差异和连续迭代梯度之间的差异来计算的。这种参数更新策略为去中心化SGD提供了理论上改进的收敛速度，特别是在处理工作节点之间的异构数据时，假设梯度是Lipschitz连续和方差有界的。
- en: III-A6 Hybrid SGD for the straggler problem
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A6 处理滞后问题的混合SGD
- en: In heterogeneous large-scale distributed DL, the issue of straggling worker
    is exacerbated, impacting both synchronous and asynchronous SGD. Workers lagging
    significantly behind others can impede the synchronization in the case of synchronous
    SGD and contribute to increased model staleness in asynchronous SGD. To address
    the straggler problem, a few studies have employed hybrid SGD approaches.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在异构的大规模分布式深度学习中，滞后工作节点的问题被加剧，影响了同步和异步SGD。显著滞后的工作节点可能会阻碍同步SGD中的同步过程，并在异步SGD中导致模型过时。为了解决滞后问题，一些研究采用了混合SGD方法。
- en: The straggler problem can be mitigated by identifying stragglers during training
    and organizing workers into groups with varied model-synchronization strategies.
    Grouping Stale Synchronous Parallel (GSSP) [[101](#bib.bib101)] categorizes workers
    with comparable performance into specific groups and implements distinct strategies
    for intra- and inter-group model synchronization. Within each group, workers apply
    stale synchronous SGD with the group PS, whereas each group synchronizes its model
    with other groups using an asynchronous gossip approach. Consequently, GSSP mitigates
    stragglers during intra-group synchronization and improves convergence performance
    through inter-group synchronization. In contrast, A2S [[102](#bib.bib102)] applies
    distinct model-synchronization strategies to fast and slow workers. These groups
    are identified based on maintaining synchronization speeds of workers in the PS,
    fast and slow workers are separated dynamically into synchronous and asynchronous
    groups, respectively. Fast workers apply synchronous SGD with a relaxed adaptive
    synchronization barrier, akin to stale synchronous SGD. In contrast, slow workers
    apply asynchronous SGD. This approach limits the model staleness through stale
    synchronous SGD and mitigates the communication delay caused by stragglers through
    asynchronous SGD. The convergence performance of these grouping SGD approaches
    is guaranteed theoretically and experimentally.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在训练过程中识别滞后者并将工作者组织成不同的模型同步策略组，可以减轻滞后问题。分组陈旧同步并行 (GSSP) [[101](#bib.bib101)]
    将性能相似的工作者分为特定组，并实施不同的组内和组间模型同步策略。在每个组内，工作者使用组内 PS 应用陈旧同步 SGD，而每个组则使用异步 gossip
    方法与其他组同步其模型。因此，GSSP 在组内同步过程中减轻了滞后者的问题，并通过组间同步改善了收敛性能。相比之下，A2S [[102](#bib.bib102)]
    对快慢工作者应用不同的模型同步策略。这些组是基于维持 PS 中工作者的同步速度来识别的，快工作者和慢工作者动态地分为同步组和异步组。快工作者使用带有放宽适应性同步屏障的同步
    SGD，类似于陈旧同步 SGD。相比之下，慢工作者使用异步 SGD。这种方法通过陈旧同步 SGD 限制模型的过时，并通过异步 SGD 缓解滞后者造成的通信延迟。这些分组
    SGD 方法的收敛性能在理论和实验上都得到了保证。
- en: Some studies address the straggler problem by implementing various model-synchronization
    strategies at different training stages. To accelerate distributed training in
    heterogeneous environments with stragglers, ASHL [[103](#bib.bib103)] divides
    the training process into coarse- and fine-grained stages depending on whether
    the global loss exceeds a predefined threshold. In the coarse-grained stage, where
    the model needs to converge quickly to a certain extent, ASHL employs asynchronous
    local SGD to reduce communication frequency and waiting time for faster convergence;
    in the fine-grained stage, where the model should be refined to enhance the convergence
    bound, ASHL employs stale synchronous SGD with bounded staleness to ensure model
    consistency. The synchronization frequency of each worker in both stages are determined
    by profiling the updating speeds of workers, aiming for a roughly consistent synchronization
    pace for both fast and slow workers.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究通过在不同的训练阶段实施各种模型同步策略来解决滞后问题。为了加速异质环境下的分布式训练，ASHL [[103](#bib.bib103)] 将训练过程分为粗粒度和细粒度阶段，这取决于全局损失是否超过预定义的阈值。在粗粒度阶段，即模型需要迅速收敛到某种程度时，ASHL
    采用异步本地 SGD 来减少通信频率和等待时间，以实现更快的收敛；在细粒度阶段，即模型需要被细化以增强收敛界限时，ASHL 采用带有界限过时的同步 SGD
    来确保模型的一致性。两个阶段中每个工作者的同步频率通过分析工作者的更新速度来确定，旨在使快慢工作者的同步速度大致一致。
- en: 'TABLE IV: A Comparison of convergence analyses of local SGD'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：本地 SGD 的收敛分析比较
- en: '|   Ref. | Convergence Rate | Synchronization Round (R) | Setting |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|   参考文献 | 收敛速度 | 同步轮次 (R) | 设置 |'
- en: '|   Zhou and Cong (2018) [[104](#bib.bib104)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(T)$ | Non-convex; Lipschitz-continuous gradients |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|   Zhou 和 Cong (2018) [[104](#bib.bib104)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(T)$ | 非凸；Lipschitz 连续梯度 |'
- en: '| Stich (2019) [[105](#bib.bib105)] | $\mathcal{O}(\frac{G^{2}}{NT})$ | $\Omega(N^{\frac{1}{2}}T^{\frac{1}{2}})$
    | Bounded gradient; strongly convex |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Stich (2019) [[105](#bib.bib105)] | $\mathcal{O}(\frac{G^{2}}{NT})$ | $\Omega(N^{\frac{1}{2}}T^{\frac{1}{2}})$
    | 有界梯度；强凸 |'
- en: '| Yu et al. (2019) [[106](#bib.bib106)] | $\mathcal{O}(\frac{G^{2}}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{4}}T^{\frac{3}{4}})$ | Bounded gradient; non-convex |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Yu 等 (2019) [[106](#bib.bib106)] | $\mathcal{O}(\frac{G^{2}}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{4}}T^{\frac{3}{4}})$ | 有界梯度；非凸 |'
- en: '| Haddadpour et al. (2019) [[107](#bib.bib107)] | $\mathcal{O}(\frac{1}{NT})$
    | $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$ | Non-convex under PL Condition |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Haddadpour 等人 (2019) [[107](#bib.bib107)] | $\mathcal{O}(\frac{1}{NT})$ |
    $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$ | 在 PL 条件下非凸 |'
- en: '| Yu and Jin (2019) [[108](#bib.bib108)] | $\mathcal{O}(\frac{1}{NT})$ | $\Omega(logT)$
    | Increasing minibatch size; non-convex under PL Condition |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Yu 和 Jin (2019) [[108](#bib.bib108)] | $\mathcal{O}(\frac{1}{NT})$ | $\Omega(logT)$
    | 增加小批量大小；在 PL 条件下非凸 |'
- en: '| Woodworth et al. (2020) [[109](#bib.bib109)] | $\mathcal{O}(\frac{1}{\sqrt{NT}}+\frac{1}{\sqrt[3]{TR}})$
    | $\Omega(N\times poly(logT))$ | Convex |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Woodworth 等人 (2020) [[109](#bib.bib109)] | $\mathcal{O}(\frac{1}{\sqrt{NT}}+\frac{1}{\sqrt[3]{TR}})$
    | $\Omega(N\times poly(logT))$ | 凸 |'
- en: '| Spiridonoff et al. (2021) [[110](#bib.bib110)] | $\mathcal{O}(\frac{1}{NT})$
    | $\Omega(N)$ | Smooth and strongly convex under PL Condition |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Spiridonoff 等人 (2021) [[110](#bib.bib110)] | $\mathcal{O}(\frac{1}{NT})$
    | $\Omega(N)$ | 平滑且在 PL 条件下强凸 |'
- en: '| Wang and Joshi (2021) [[111](#bib.bib111)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{2}}T^{\frac{1}{2}})$ | Non-convex |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Wang 和 Joshi (2021) [[111](#bib.bib111)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{2}}T^{\frac{1}{2}})$ | 非凸 |'
- en: '|   |  |  |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Convergence rate, the expected different between the optimization objectives
    with the averaged model and minibatch SGD: $\mathbb{E}[f(\bar{x}_{T})-f(x^{\ast})]$,
    where $f$ is the objective function, $x_{t}$ is the local model after $t$ local
    SGD update iterations, and $x^{\ast}$ is the optimal model;'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收敛速率，优化目标与平均模型和小批量 SGD 的差异期望值：$\mathbb{E}[f(\bar{x}_{T})-f(x^{\ast})]$，其中 $f$
    是目标函数，$x_{t}$ 是经过 $t$ 次局部 SGD 更新迭代后的局部模型，$x^{\ast}$ 是最优模型；
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $G$, the uniform upper bound for the L2-norm of gradients;
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $G$，梯度的 L2 范数的均匀上界；
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $N$, the number of worker;
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N$，工作者的数量；
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $T$, the total number of SGD update iterations at each worker.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $T$，每个工作者的 SGD 更新迭代总数。
- en: III-B Convergence Guarantees of Distributed SGD
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 分布式 SGD 的收敛保证
- en: Many studies analyze theoretical convergence guarantees of different distributed
    SGD algorithms across various optimization objectives. Two major branches of these
    analyses focus on local SGD and asynchronous SGD,
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究分析了不同分布式 SGD 算法在各种优化目标下的理论收敛保证。这些分析的两个主要方向集中在局部 SGD 和异步 SGD，
- en: III-B1 Convergence of local SGD
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 局部 SGD 的收敛性
- en: 'A significant portion of these studies focus on local SGD. These works formalize
    local SGD across various optimization objective settings, studying the convergence
    performance concerning the number of workers ($N$), number of local updating iterations
    within each synchronization round, entire updating iterations ($T$) at each worker,
    and minibatch size. Therefore, they prove the scalability advantage of local SGD
    over other distributed SGD algorithms when applied in large-scale distributed
    DL. Table [IV](#S3.T4 "TABLE IV ‣ III-A6 Hybrid SGD for the straggler problem
    ‣ III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") presents a comparison of analysis results of local SGD
    in various objective settings.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '这些研究的很大一部分集中在局部 SGD 上。这些工作在各种优化目标设置下形式化局部 SGD，研究有关工作者数量（$N$）、每次同步轮次内的局部更新迭代次数、每个工作者的整个更新迭代次数（$T$）以及小批量大小的收敛性能。因此，它们证明了局部
    SGD 相比于其他分布式 SGD 算法在大规模分布式深度学习中的可扩展性优势。表 [IV](#S3.T4 "TABLE IV ‣ III-A6 Hybrid
    SGD for the straggler problem ‣ III-A Synchronous, Asynchronous, and Other Distributed
    SGD ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") 展示了局部 SGD 在各种目标设置下的分析结果对比。'
- en: These analyses provides bounds on the convergence rate and synchronization round.
    Zhou and Cong [[104](#bib.bib104)] analyze the convergence of local SGD for non-convex
    objectives with Lipschitz-continuous gradients, under various settings of the
    learning rate and minibatch size. The analysis shows that local SGD allows larger
    learning rate, which is a common practice in large-batch training for large-scale
    distributed DL, and scales more efficiently than asynchronous SGD. Stich [[105](#bib.bib105)]
    demonstrates that both synchronous and asynchronous local SGD algorithms share
    the same convergence rate but reduce synchronization rounds by a factor of the
    root square of the number of local gradient updates compared to synchronous SGD
    for strongly convex objectives. Assuming a uniform upper bound on the L2-norm
    of gradients, local SGD can scale with a linear speedup w.r.t the number of workers
    and minibatch size. With the same bounded gradient assumption as in [[105](#bib.bib105)],
    Yu et al. [[106](#bib.bib106)] ensure convergence for non-convex objectives. However,
    the convergence rate is not proven a linearly speedup w.r.t the number of workers.
    Eliminating this bounded gradient assumption, both Haddadpour et al. [[107](#bib.bib107)]
    and Yu and Jin [[108](#bib.bib108)] enhance the analysis of [[106](#bib.bib106)].
    They offer tighter bounds on both the convergence rate and synchronization round
    than previous analyses and prove a linear speedup for non-convex objectives under
    the Polyak-Łojasiewicz (PL) condition [[138](#bib.bib138)], which can be viewed
    as a generalization of strong convexity for non-convex optimization. Compared
    to [[107](#bib.bib107)], whose asymptotic lower bound on the synchronization round
    is $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$, the analysis of [[108](#bib.bib108)]
    provides a tighter bound of $\Omega(log(T))$ for local SGD with dynamically increasing
    minibatch size, implying a promising direction to scale local SGD. Woodworth et
    al. [[109](#bib.bib109)] concentrate on comparing the convergence of minibatch
    distributed SGD and local SGD through a theoretical analysis. In comparison to
    minibatch distributed SGD, local SGD exhibits superior convergence performance
    for quadratic objectives. Yet, for general convex objectives, local SGD demonstrates
    better convergence than minibatch distributed SGD with a large number of workers,
    but worse with a large minibatch size.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分析提供了关于收敛速度和同步轮次的界限。Zhou 和 Cong [[104](#bib.bib104)] 分析了在不同学习率和小批量大小设置下，局部
    SGD 对于具有 Lipschitz 连续梯度的非凸目标的收敛性。分析显示，局部 SGD 允许更大的学习率，这在大规模分布式深度学习的大批量训练中是一种常见做法，并且比异步
    SGD 更高效地扩展。Stich [[105](#bib.bib105)] 证明了同步和异步局部 SGD 算法具有相同的收敛速度，但相比于同步 SGD 对于强凸目标，减少了同步轮次，减少因子为局部梯度更新数量的平方根。假设梯度的
    L2 范数有一个统一的上界，局部 SGD 可以相对于工人数和小批量大小线性加速。Yu 等人 [[106](#bib.bib106)] 在具有与 [[105](#bib.bib105)]
    相同的有界梯度假设下，确保了非凸目标的收敛性。然而，收敛速度未被证明相对于工人数线性加速。去除这一有界梯度假设，Haddadpour 等人 [[107](#bib.bib107)]
    和 Yu 与 Jin [[108](#bib.bib108)] 强化了 [[106](#bib.bib106)] 的分析。他们提供了比之前分析更紧的收敛速度和同步轮次界限，并在
    Polyak-Łojasiewicz (PL) 条件 [[138](#bib.bib138)] 下证明了非凸目标的线性加速，该条件可以视为非凸优化中强凸性的推广。相比于
    [[107](#bib.bib107)]，其同步轮次的渐近下界为 $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$，[[108](#bib.bib108)]
    的分析提供了局部 SGD 在动态增加小批量大小下的更紧界限 $\Omega(log(T))$，这暗示了扩展局部 SGD 的有前景的方向。Woodworth
    等人 [[109](#bib.bib109)] 专注于通过理论分析比较小批量分布式 SGD 和局部 SGD 的收敛性。与小批量分布式 SGD 相比，局部 SGD
    对于二次目标表现出更优的收敛性能。然而，对于一般的凸目标，局部 SGD 在工人数较多时比小批量分布式 SGD 收敛更好，但在小批量大小较大时表现较差。
- en: Some studies investigate the convergence guarantee of an extreme case of local
    SGD known as one-shot averaging. This method uses very few, or even only a single,
    synchronization rounds at the very end of the distributed training process. In
    smooth and strongly convex problems under the PL condition, Spiridonoff et al.
    (2021) [[110](#bib.bib110)] demonstrate that one-shot averaging can converge in
    $\Omega(N)$ synchronization rounds, irrespective of the total number of gradient
    iterations $T$. This finding suggests the potential application of one-shot averaging
    to large models, typically requiring numerous training iterations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究调查了本地 SGD 的极端情况，即一次性平均。这种方法在分布式训练过程的最后只使用非常少的同步轮次，甚至只有一次。在 PL 条件下的平滑和强凸问题中，Spiridonoff
    et al. (2021) [[110](#bib.bib110)] 表明，一次性平均可以在 $\Omega(N)$ 次同步轮次中收敛，无论梯度迭代总数 $T$。这一发现表明，一次性平均有可能应用于大型模型，通常需要大量训练迭代。
- en: Some studies focus on building a unified framework for the convergence analysis
    of local SGD. Wang and Joshi [[111](#bib.bib111)] present a unified framework
    named Cooperative SGD to design variants and analyze convergence performance of
    local SGD, expanding the design space significantly by incorporating various model
    averaging protocols and adjusting the number of local updates. Cooperative SGD
    reveals that vanilla local SGD, EASGD [[92](#bib.bib92)], and decentralized local
    SGD are all special cases within this unified framework. Leveraging the convergence
    analysis of this unified framework, Cooperative SGD offers the first convergence
    guarantee for EASGD with non-convex objective functions, applicable to both IID
    and non-IID data.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究集中于建立本地 SGD 收敛性分析的统一框架。Wang 和 Joshi [[111](#bib.bib111)] 提出了一个名为 Cooperative
    SGD 的统一框架，以设计变体并分析本地 SGD 的收敛性能，通过引入各种模型平均协议并调整本地更新次数，显著扩展了设计空间。Cooperative SGD
    显示了 vanilla 本地 SGD、EASGD [[92](#bib.bib92)] 和去中心化本地 SGD 都是这个统一框架中的特例。利用这个统一框架的收敛性分析，Cooperative
    SGD 为 EASGD 在非凸目标函数下提供了第一个收敛保证，适用于 IID 和非 IID 数据。
- en: 'TABLE V: A Comparison of convergence analyses of asynchronous SGD'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：异步 SGD 收敛性分析比较
- en: '|   Ref. | Synchronization Iteration | Setting |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|   参考文献 | 同步迭代 | 设置 |'
- en: '|   Stich and Karimireddy (2020) [[112](#bib.bib112)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{max}}{\epsilon})$
    | General quasi-convex and smooth non-convex |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|   Stich 和 Karimireddy (2020) [[112](#bib.bib112)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{max}}{\epsilon})$
    | 一般准凸和平滑非凸 |'
- en: '| Aviv et al. (2021) [[113](#bib.bib113)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Strongly convex; delay-adaptive learning rate |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Aviv et al. (2021) [[113](#bib.bib113)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | 强凸；延迟自适应学习率 |'
- en: '| Cohen et al. (2021) [[114](#bib.bib114)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{4}}+\frac{\tau_{avg}}{\epsilon^{2}})$
    | Smooth non-convex |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Cohen et al. (2021) [[114](#bib.bib114)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{4}}+\frac{\tau_{avg}}{\epsilon^{2}})$
    | 平滑非凸 |'
- en: '| Koloskova et al. (2022) [[115](#bib.bib115)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Smooth non-convex; delay-adaptive learning rate |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Koloskova et al. (2022) [[115](#bib.bib115)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | 平滑非凸；延迟自适应学习率 |'
- en: '| Mishchenko et al. (2022) [[116](#bib.bib116)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Non-convex, convex, or strongly convex; Lipschitz-continuous gradients; |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Mishchenko et al. (2022) [[116](#bib.bib116)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | 非凸、凸或强凸；Lipschitz 连续梯度； |'
- en: '|  |  | delay-adaptive learning rate |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 延迟自适应学习率 |'
- en: '|   |  |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |'
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\tau_{max}$, the maximum synchronization delay;
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tau_{max}$，最大同步延迟；
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\tau_{avg}$, the average synchronization delay;
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tau_{avg}$，平均同步延迟；
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\sigma^{2}$, the upper bound on the gradient variance within a worker;
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\sigma^{2}$，工人在梯度方差上的上界；
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\epsilon$, the approximate stationary point, which bounds the squared gradient
    norm.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\epsilon$，近似静态点，限制了平方梯度范数。
- en: III-B2 Convergence of asynchronous SGD
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 异步 SGD 的收敛性
- en: 'Several studies offer theoretical convergence guarantees for asynchronous SGD,
    emphasizing the the synchronization iteration bound concerning the gradient delay,
    upper bound on gradient noise, and approximate stationary point for training termination.
    Table [V](#S3.T5 "TABLE V ‣ III-B1 Convergence of local SGD ‣ III-B Convergence
    Guarantees of Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") presents a comparison of analysis results of various asynchronous SGD
    algorithms.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究为异步 SGD 提供了理论收敛保证，强调了与梯度延迟相关的同步迭代界限、梯度噪声的上界以及训练终止的近似稳定点。表格 [V](#S3.T5 "TABLE
    V ‣ III-B1 Convergence of local SGD ‣ III-B Convergence Guarantees of Distributed
    SGD ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") 展示了各种异步 SGD 算法分析结果的比较。'
- en: 'Stich and Karimireddy [[112](#bib.bib112)] conduct a convergence analysis for
    asynchronous SGD involving gradient compression and error feedback, whose mechanisms
    will be introduced in Section [IV](#S4 "IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"), and incorporating bounded gradient noise in the model for both general
    quasi-convex and smooth non-convex objectives. The complexity of synchronization
    iteration is bounded linearly by the maximum synchronization delay, indicating
    the maximum model staleness. However, these findings lack robustness in heterogeneous
    environments, where the maximum delay can surpass the average delay significantly.
    Aviv et al. [[113](#bib.bib113)] introduce a delay-adaptive learning rate scheme
    for asynchronous SGD. Smaller learning rates are assigned to updates with a larger
    delay, providing a tighter synchronization iteration bound proportional to the
    average synchronization delay rather than the maximum delay. However, the result
    is confined to strongly convex problems, and the proof relies heavily on the assumption
    of an upper bound on the variance of delays, which is can be closely relevant
    to the maximum delay. In contrast, Cohen et al. [[114](#bib.bib114)] focus on
    smooth non-convex objectives and present a synchronization iteration bound proportional
    to the average synchronization delay. However, this analysis requires twice as
    many communication rounds at every step and relies on the appropriate tuning of
    hyperparameters. Eliminating the assumption of the upper bound on the delay variance
    in [[113](#bib.bib113)] and the hyperparameter tuning in [[114](#bib.bib114)],
    Koloskova et al. [[115](#bib.bib115)] apply another delay-adaptive learning rate
    scheme, achieving the same synchronization iteration bound as in [[113](#bib.bib113)].
    Mishchenko et al. [[116](#bib.bib116)] employ analyzing techniques and achieve
    convergence guarantees similar to those of [[115](#bib.bib115)], but cover a broader
    range of optimization problems assuming Lipschitz-continuous gradients, including
    non-convex, convex, and strongly convex.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 'Stich 和 Karimireddy [[112](#bib.bib112)] 对涉及梯度压缩和误差反馈的异步 SGD 进行了收敛性分析，其机制将在第 [IV](#S4
    "IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") 节中介绍，并在模型中纳入了有界梯度噪声，适用于一般的拟凸和光滑非凸目标。同步迭代的复杂性按最大同步延迟线性有界，指示最大模型陈旧性。然而，这些发现缺乏在异质环境中的鲁棒性，其中最大延迟可能远超平均延迟。Aviv
    等人 [[113](#bib.bib113)] 引入了一种延迟自适应学习率方案用于异步 SGD。较小的学习率分配给延迟较大的更新，提供了一个与平均同步延迟成比例的更紧的同步迭代界限，而不是最大延迟。然而，结果仅限于强凸问题，并且证明很大程度上依赖于对延迟方差上界的假设，该方差可能与最大延迟密切相关。相比之下，Cohen
    等人 [[114](#bib.bib114)] 专注于光滑的非凸目标，并提出了一个与平均同步延迟成比例的同步迭代界限。然而，这种分析在每一步需要两倍的通信轮次，并依赖于超参数的适当调整。在 [[113](#bib.bib113)]
    中消除对延迟方差上界的假设，以及在 [[114](#bib.bib114)] 中的超参数调整，Koloskova 等人 [[115](#bib.bib115)]
    应用了另一种延迟自适应学习率方案，达到了与 [[113](#bib.bib113)] 相同的同步迭代界限。Mishchenko 等人 [[116](#bib.bib116)]
    采用分析技术，实现了类似于 [[115](#bib.bib115)] 的收敛保证，但覆盖了更广泛的优化问题，假设梯度是 Lipschitz 连续的，包括非凸、凸和强凸问题。'
- en: III-C Model Synchronization in FL
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 模型同步在联邦学习中的应用
- en: Model synchronization in the domain of FL presents additional challenges. Firstly,
    and most importantly, there can be a huge amount of workers, represented by various
    types of devices, exhibit heterogeneity in geographical locations, computational
    and communication-resource capacities, model structures and objectives, and data
    distributions. Secondly, the membership of workers participating in the synchronization
    can be dynamic and unstable. Thirdly, concerns arise regarding privacy and data
    security issues. These challenges are addressed by numerous studies employing
    approaches in various directions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习（FL）领域，模型同步面临额外的挑战。首先，最重要的是，参与的工作者数量庞大，涵盖各种类型的设备，表现出在地理位置、计算和通信资源能力、模型结构和目标以及数据分布上的异质性。其次，参与同步的工作者的成员资格可能是动态和不稳定的。第三，隐私和数据安全问题也引起了关注。这些挑战已经通过许多研究得到了解决，这些研究采用了不同方向的方法。
- en: III-C1 Randomly selected workers
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 随机选择的工作者
- en: To reduce communication overhead among numerous heterogeneous workers in FL,
    FedAvg [[117](#bib.bib117)] randomly selects only a fraction of workers in each
    synchronization round of local SGD. In the context of decentralized SGD, NetMax [[118](#bib.bib118)]
    enables each worker to select a peer stochastically based on a fine-tuned probability
    for model synchronization in each round. The optimal selection probabilities for
    all workers are derived by a network monitor, aiming to minimize the total convergence
    time w.r.t the network bandwidth capacity and utilization status. While the method
    of selecting workers randomly has become the state-of-the-art practice for distributed
    SGD in FL, it does not address various other heterogeneity issues such as non-IID
    data and heterogeneous models. This limitation has prompted a series of studies
    on this topic, focusing on different strategies tailored to cope with heterogeneity
    in FL, some of which are variants of FedAvg.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少 FL 中众多异质工作者之间的通信开销，FedAvg [[117](#bib.bib117)] 在每轮本地 SGD 的同步中仅随机选择部分工作者。在去中心化
    SGD 的背景下，NetMax [[118](#bib.bib118)] 使每个工作者可以基于精细调整的概率随机选择一个同行进行模型同步。所有工作者的最佳选择概率由网络监控器推导，旨在最小化总的收敛时间，以适应网络带宽容量和利用状态。虽然随机选择工作者的方法已成为
    FL 中分布式 SGD 的前沿实践，但它并未解决非独立同分布（non-IID）数据和异质模型等其他异质性问题。这一局限性促使了对这一主题的一系列研究，重点关注不同的策略以应对
    FL 中的异质性，其中一些是 FedAvg 的变体。
- en: III-C2 Breaking down the model
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 拆解模型
- en: A branch of variants [[21](#bib.bib21), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121)] distinguishes model components and synchronizes specific components
    intermittently, enabling varied synchronization frequencies for different components
    and enhancing communication efficiency for large models. Leveraging the observation
    that shallow layers of DNN models capture general features, while deep layers
    acquire ad-hoc features specific to data sets, ASTW_FedAvg [[21](#bib.bib21)]
    synchronizes shallow layers more frequently than deep layers, excluding deep layers
    in certain synchronization rounds. This approach enhances FedAvg for heterogeneous
    data sets; however, the coarse division of the so-called shallow and deep layers
    is empirical and not robust. Exploiting the observation that many parameters stabilize
    before the model reaches its ultimate convergence, Adaptive Parameter Freezing
    (APF) [[119](#bib.bib119)] introduces the parameter-wise freezing scheme. In this
    scheme, specific stable local parameters are fixed in local feedforward and backpropagation
    iterations and excluded during synchronization for specified periods. The periods
    of parameter-wise freezing are not predetermined based on prior knowledge of model
    convergence behaviors. Instead, they are adjusted dynamically in an Additive-Increase
    Multiplicative-Decrease (AIMD) manner, depending on the stability of parameters
    that were previously frozen in subsequent iterations. Nevertheless, empirical
    observations [[139](#bib.bib139)] suggest that some parameters remain unstable
    even when the model reaches convergence, especially in the case of over-parameterized
    large models. This instability weakens the ability of these parameters to freeze
    and to reduce communication overhead in APF. APF# and APF++ [[120](#bib.bib120)]
    extend APF to tackle the jittering issue in over-parameterized large models by
    employing aggressive methods for parameter freezing. In APF#, unstable parameters
    have a probability of being frozen for one round, and in the more more aggressive
    APF++, both the probability and the number of freezing rounds increase over time.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 一类变体[[21](#bib.bib21), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121)]
    区分模型组件并间歇性地同步特定组件，从而为不同组件提供不同的同步频率，并提升大模型的通信效率。利用深度神经网络模型的浅层捕捉一般特征，而深层获取特定数据集的特有特征的观察，ASTW_FedAvg
    [[21](#bib.bib21)] 使浅层比深层更频繁地同步，并在某些同步轮次中排除深层。这种方法改进了针对异质数据集的FedAvg；然而，所谓的浅层和深层的粗略划分是经验性的，并不具备鲁棒性。利用观察到的许多参数在模型达到最终收敛之前会稳定的现象，自适应参数冻结（APF）[[119](#bib.bib119)]
    引入了按参数冻结的方案。在该方案中，特定的稳定局部参数在本地前向和反向传播迭代中被固定，并在指定的时间段内排除在同步之外。参数冻结的时间段不是基于对模型收敛行为的先验知识来预设的。而是根据先前冻结的参数在后续迭代中的稳定性，以加性增加乘法减少（AIMD）方式动态调整。然而，经验观察[[139](#bib.bib139)]
    表明，即使在模型达到收敛时，一些参数仍然不稳定，尤其是在过度参数化的大模型情况下。这种不稳定性削弱了这些参数冻结的能力，并减少了APF中的通信开销。APF#
    和 APF++ [[120](#bib.bib120)] 扩展了APF，以通过采用更激进的参数冻结方法来解决过度参数化大模型中的抖动问题。在APF#中，不稳定参数有一定概率被冻结一轮，而在更加激进的APF++中，冻结的概率和轮次数量随着时间的推移而增加。
- en: However, these fine-grained distributed SGD algorithms can be computationally
    expensive. APF and its extensions offer precise controls over the trade-off between
    model consistency and communication overhead. However, maintaining parameter-wise
    freezing period introduces memory and computation overhead, with complexity is
    linear to the model size. This suggests a careful consideration of these methods
    for large models. To achieve efficient and fine-grained SGD for FL, YOGA [[121](#bib.bib121)]
    adopts a layer-wise gradient aggregation approach based on ranks in decentralized
    SGD. YOGA assigns ranks to both layers and peers, employing a greedy algorithm
    to aggregate gradients from layers and peers selectively. Layers are ranked according
    to their learning speed and discrepancy, identifying the significance of each
    layers effectively. Peers are ranked based on data distribution divergence and
    available bandwidth, addressing the issues of non-IID data and heterogeneous resource
    capacities in FL.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些细粒度的分布式SGD算法可能计算开销较大。APF及其扩展提供了对模型一致性和通信开销之间权衡的精确控制。然而，维持参数级别的冻结期引入了内存和计算开销，复杂度与模型大小成线性关系。这表明在大规模模型中需要谨慎考虑这些方法。为了实现FL的高效且细粒度的SGD，YOGA
    [[121](#bib.bib121)] 采用了基于等级的分层梯度聚合方法。YOGA为层和同伴分配等级，使用贪婪算法选择性地聚合来自层和同伴的梯度。层根据其学习速度和差异性进行排名，有效识别每层的重要性。同伴则根据数据分布的差异性和可用带宽进行排名，解决了FL中的非IID数据和异质资源容量问题。
- en: III-C3 FL-tailored aggregation strategies
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 量身定制的FL聚合策略
- en: Some studies focus [[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125)] on diverse aggregation strategies to determine local and global
    parameters, addressing challenges related to heterogeneity in both data and resources
    within FL environments. FedProx [[122](#bib.bib122)], akin to FedAvg, presents
    a new local updating strategy among workers. Instead of updating local parameters
    through SGD, FedProx identifies local parameters that minimize a local proximal
    objective inexactly, which restricts the magnitude of local updates to alleviate
    model inconsistency arising from data heterogeneity. Addressing resource heterogeneity
    involves adjusting the inexact level of local parameters in each worker. This
    adjustment correlates with the number of local iterations and reduces computation
    and communication overhead of each worker during a synchronization round. However,
    the computation of finding these inexact local parameters in FedProx remains sophisticated,
    hindering its convergence rate in large-scale FL. FedNova [[123](#bib.bib123)]
    addresses this issue by dealing with inconsistent local objectives arising from
    the heterogeneity in the number of local updates and non-IID data in FL. Gradients
    for updating the global model are computed as a scaled weighted sum of the normalized
    local gradients from randomly selected workers. These weights serve as flexible
    hyperparameters unique to each worker, and the scale is associated with the number
    of local update iterations in each worker. An illustrative example of the scale
    is the weighted sum of the numbers of local update iterations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究专注于[[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124), [125](#bib.bib125)]
    多样化的聚合策略，以确定局部和全局参数，解决FL环境中数据和资源异质性相关的挑战。FedProx [[122](#bib.bib122)] 类似于FedAvg，提出了一种在工作节点之间的新局部更新策略。FedProx并不是通过SGD更新局部参数，而是通过不完全的方法确定最小化局部接近目标的局部参数，这限制了局部更新的幅度，以缓解由于数据异质性引起的模型不一致性。处理资源异质性涉及在每个工作节点中调整局部参数的不完全水平。该调整与局部迭代次数相关，并减少了每个工作节点在同步轮次中的计算和通信开销。然而，FedProx中找到这些不完全局部参数的计算依然复杂，妨碍了其在大规模FL中的收敛速度。FedNova
    [[123](#bib.bib123)] 通过处理因局部更新次数和非IID数据中的异质性导致的不一致局部目标来解决此问题。全局模型的梯度通过对随机选择的工作节点的归一化局部梯度的加权总和来计算。这些权重作为每个工作节点独特的灵活超参数，尺度与每个工作节点中的局部更新迭代次数相关。尺度的一个说明性例子是局部更新迭代次数的加权总和。
- en: In contrast to the studies above that focus on centralized SGD, some studies
    address FL heterogeneity through decentralized SGD approaches. Cross-Gradient
    Aggregation (CGA) [[124](#bib.bib124)] tackles non-IID data by computing the so-called
    cross gradients during model aggregation. In addition to computing local gradients
    by training local models at each worker, each worker retrieves neighboring models
    and derives cross gradients, which are computed by the neighboring models using
    local data on this worker. These local and cross gradients are then projected
    into aggregated gradients using quadratic programming and updated into each model
    via SGD with momentum. CGA exhibits superior performance over many other decentralized
    SGD algorithms, especially for non-IID data. On the other hand, AsyNG [[125](#bib.bib125)]
    tackles non-IID data through the dynamical selection of suitable neighbors for
    P2P model synchronization. Selection of neighbors relies on a priority-based algorithm,
    wherein the priority of a neighbor is determined by the difference in loss and
    the gap in training iterations compared to the worker. A neighbor is deemed more
    favorable for synchronization where there is a larger loss difference and a smaller
    training iteration gap.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述集中式SGD研究不同，一些研究通过去中心化SGD方法来解决FL的异质性问题。交叉梯度聚合（CGA）[[124](#bib.bib124)]通过在模型聚合过程中计算所谓的交叉梯度来处理非IID数据。除了通过在每个工作节点上训练本地模型来计算本地梯度外，每个工作节点还会检索邻近的模型并推导交叉梯度，这些交叉梯度是通过邻近模型使用该工作节点的本地数据计算的。这些本地梯度和交叉梯度随后通过二次规划投影到聚合梯度中，并通过带有动量的SGD更新到每个模型中。CGA在许多其他去中心化SGD算法中表现出优越的性能，特别是在非IID数据的情况下。另一方面，AsyNG
    [[125](#bib.bib125)]通过动态选择适当的邻居进行P2P模型同步来处理非IID数据。邻居的选择依赖于基于优先级的算法，其中邻居的优先级由与工作节点相比的损失差异和训练迭代间隙决定。邻居在存在较大的损失差异和较小的训练迭代间隙时被认为更适合同步。
- en: III-C4 Hierarchical gradient aggregation
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C4 分层梯度聚合
- en: Some of studies [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129)] focus on clustering workers into groups for gradient aggregation,
    similar to [[101](#bib.bib101), [102](#bib.bib102)], but with considerations for
    the heterogeneity issues in FL. To address membership and data security issues,
    Bonawitz et al. [[126](#bib.bib126)] propose a two-level hierarchical aggregating
    scheme with encrypted computation for model synchronization among numerous mobile
    devices in FL, based on FedAvg. This scheme employs a global aggregator that is
    capable of spawning several sub-aggregators, each being responsible for membership
    maintenance and model synchronization within a subset of devices. The scheme adopts
    the Secure Aggregation protocol [[140](#bib.bib140)], performing model aggregation
    in a logical incorruptible third party. This ensures encrypted model updating,
    preventing the exposure of the model to untrusted devices and cloud providers.
    To optimize the topology for hierarchical SGD, FedCH [[127](#bib.bib127)] proposes
    a two-level gradient aggregating architecture comprising a specific number of
    groups. Each group aggregates gradients from different workers within it through
    local SGD. Additionally, there is a global PS that aggregates gradients from different
    groups using asynchronous SGD. FedCH aims to determine the optimal allocation
    of workers to groups, known as the cluster topology, to minimize training loss
    while considering resource and time budget constraints within this architecture.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129)]集中于将工作节点聚类成组进行梯度聚合，类似于[[101](#bib.bib101),
    [102](#bib.bib102)]，但考虑了FL中的异质性问题。为了解决成员和数据安全问题，Bonawitz等人[[126](#bib.bib126)]提出了一种基于FedAvg的两级分层聚合方案，该方案采用加密计算在FL中众多移动设备之间进行模型同步。该方案使用一个全球聚合器，能够生成多个子聚合器，每个子聚合器负责在一个设备子集内进行成员维护和模型同步。该方案采用了安全聚合协议[[140](#bib.bib140)]，在逻辑上不可篡改的第三方进行模型聚合。这确保了加密模型更新，防止模型暴露给不可信的设备和云服务提供商。为了优化分层SGD的拓扑结构，FedCH
    [[127](#bib.bib127)]提出了一种两级梯度聚合架构，由特定数量的组组成。每个组通过本地SGD从组内不同的工作节点聚合梯度。此外，还有一个全球参数服务器（PS）通过异步SGD从不同组中聚合梯度。FedCH旨在确定工作节点到组的最佳分配，即集群拓扑，以在考虑资源和时间预算约束的情况下最小化训练损失。
- en: In contrast to the grouping methods above that focus on the centralized SGD,
    certain studies focus on decentralized SGD, wherein each group employs gossip-based
    gradient aggregation. TT-HF [[128](#bib.bib128)] examines FL across wireless edge
    devices, where devices within an edge function naturally as workers belonging
    to the same group. Workers apply local SGD, synchronize their models aperiodically
    through gossip within an edge, and synchronizes models aperiodically across edges
    via a global PS. Drawing on the convergence analysis of this synchronization strategy,
    TT-HF adjusts the learning rate and synchronization frequencies within the edge
    and globally adaptively to achieve the optimal trade-off among delay, model accuracy,
    and energy consumption. To mitigate the impact of constantly joining, leaving,
    and failing participating workers with convergence guarantees, Moshoit SGD [[129](#bib.bib129)]
    clusters them into different groups for gradient aggregation iteratively in every
    synchronization round. This ensures that any pair of workers will not be clustered
    into the same group in consecutive rounds. Moshoit SGD is designed for cloud-based
    and edge-based distributed DL environments with unreliable devices and unstable
    networks. It is theoretically and experimental proven to be more effective than
    the completely gossip-based approach in these environments.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 与以上集中式 SGD 分组方法相比，某些研究聚焦于去中心化 SGD，其中每个组使用基于 gossip 的梯度聚合。TT-HF [[128](#bib.bib128)]
    研究了无线边缘设备中的联邦学习（FL），其中边缘设备在功能上自然作为同一组的工作节点。工作节点应用局部 SGD，通过 gossip 在边缘内不定期地同步模型，并通过全局参数服务器（PS）在不同边缘之间不定期地同步模型。基于对这种同步策略的收敛分析，TT-HF
    在边缘和全局范围内自适应地调整学习率和同步频率，以实现延迟、模型准确性和能耗之间的最佳权衡。为了减轻参与工作节点不断加入、离开和失败对收敛性的影响，Moshoit
    SGD [[129](#bib.bib129)] 在每次同步轮次中将它们分成不同的组进行梯度聚合。这确保了任何一对工作节点在连续轮次中不会被分到同一组。Moshoit
    SGD 旨在用于云计算和边缘计算环境中的分布式深度学习（DL），适用于不可靠的设备和不稳定的网络。理论和实验均证明，在这些环境中，它比完全基于 gossip
    的方法更有效。
- en: III-C5 Adaptive hyperparameters throughout training
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C5 自适应超参数调整
- en: 'Various studies [[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)] address issues of
    heterogeneous data and resources in FL by focusing on the adapting hyperparameters
    of distributed SGD, including the synchronization frequency, minibatch size, learning
    rate, and data-sampling strategy. Wang et al. [[130](#bib.bib130)] analyze the
    convergence bound of local SGD incorporating non-IID data initially. Based on
    this analysis, they propose a local SGD algorithm with an adaptive synchronization
    frequency, optimized to minimize errors in optimization with heterogeneous resource
    constraints in workers and the PS, common in edge-based distributed DL paradigms.
    FedLamp [[131](#bib.bib131)] and AdaSFL [[132](#bib.bib132)] employ approaches
    similar to [[130](#bib.bib130)] for determining the optimal synchronization frequency
    based on convergence bounds within heterogeneous resource constraints adaptively.
    The difference is that FedLamp optimizes the synchronization frequency and data
    compression ratio jointly, as detailed in Section [IV](#S4 "IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey"), aimed at minimizing training time while considering
    convergence and resource constraints. Conversely, AdaSFL optimizes the synchronization
    frequency and minibatch size jointly to achieve the same objective under identical
    constraints. In Adaptive Asynchronous Federated Learning (AAFL) [[133](#bib.bib133)],
    the number of local updates in each synchronization round is determined adaptively
    by an experience-driven deep reinforcement learning algorithm, minimizes the maximum
    local accumulated training time of workers while considering bandwidth budget
    and convergence performance constraints.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '各种研究[[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133),
    [134](#bib.bib134), [135](#bib.bib135)] 通过关注分布式SGD的超参数调整，如同步频率、小批量大小、学习率和数据采样策略，来解决FL中的异构数据和资源问题。Wang等人[[130](#bib.bib130)]最初分析了局部SGD的收敛界限，纳入了非IID数据。基于这一分析，他们提出了一种具有自适应同步频率的局部SGD算法，旨在优化异构资源约束下的优化误差，这在基于边缘的分布式DL范式中很常见。FedLamp[[131](#bib.bib131)]和AdaSFL[[132](#bib.bib132)]采用了类似[[130](#bib.bib130)]的方法，根据异构资源约束自适应地确定最优同步频率。不同之处在于，FedLamp在第[IV](#S4
    "IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey)节中详细说明，联合优化同步频率和数据压缩比例，旨在减少训练时间，同时考虑收敛性和资源约束。相对而言，AdaSFL在相同约束下联合优化同步频率和小批量大小，以实现相同的目标。在自适应异步联邦学习（AAFL）[[133](#bib.bib133)]中，通过经验驱动的深度强化学习算法自适应地确定每轮同步中的本地更新数量，最小化工人在带宽预算和收敛性能约束下的最大本地累计训练时间。'
- en: Recently, several studies have focused on reconstructing data based on sampled
    data on each worker to address the challenge of non-IID heterogeneous data in
    FL. For instance, FAST [[134](#bib.bib134)] has developed an online learning algorithm
    that optimizes the synchronization frequency and data sampling on each device
    jointly to minimize global training loss within resource and time budgets. This
    online algorithm can offer the optimized data sampling results in each iteration
    dynamically without prior knowledge of the data distribution on each worker.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，几项研究集中于基于每个工人的采样数据重建数据，以应对FL中的非IID异构数据挑战。例如，FAST[[134](#bib.bib134)]开发了一种在线学习算法，能够联合优化每个设备上的同步频率和数据采样，以在资源和时间预算内最小化全局训练损失。该在线算法可以在每次迭代中动态提供优化的数据采样结果，而无需预先了解每个工人上的数据分布。
- en: There are empirical approaches in addition to the aforementioned optimization-based
    methods. In AMBLE [[135](#bib.bib135)], the learning rate, minibatch size, and
    synchronization frequency are adjusted concurrently for local updating in heterogeneous
    workers, employing empirical strategies. AMBLE formulates the local updating time
    for workers, assigning greater values to the number of local updating iterations
    and minibatch size for faster workers. The learning rate is then adjusted linearly
    in relation to the adaptive synchronization frequency and minibatch size. AMBLE
    demonstrates superior experimental prediction results compared to FedAvg in both
    IDD and non-IDD scenarios.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述基于优化的方法，还有经验方法。在AMBLE[[135](#bib.bib135)]中，学习率、小批量大小和同步频率同时调整，以便在异质工作者中进行本地更新，采用经验策略。AMBLE为工作者制定了本地更新时间，赋予更快的工作者更大的本地更新迭代次数和小批量大小。然后，学习率根据自适应同步频率和小批量大小线性调整。AMBLE在IDD和非IDD场景下均显示出比FedAvg更优的实验预测结果。
- en: III-C6 Ability to forget over time
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C6 随时间遗忘的能力
- en: Some studies [[136](#bib.bib136)] focus on FL heterogeneity in the temporal
    direction, distinct from the heterogeneity observed in the spatial direction.
    During incremental training, where the model retains previously learned knowledge
    while acquiring new information, the data distribution may be non-stationary over
    time. The model must learn to forget old knowledge during the retraining to acquire
    new knowledge; this ability to forget is known as unlearning. However, unlearning
    the FL model is challenging because training data are not shared with a central
    PS server. Investigating the unlearning problem in FL, Liu et al. [[136](#bib.bib136)]
    introduce a time-saving and energy-efficient retraining method. They utilize the
    first-order Taylor expansion to approximate the objective and the diagonal empirical
    Fisher Information Matrix [[141](#bib.bib141)] to approximate the inverse Hessian
    in the Quasi-Newton method. This rapid retraining method allows different FL parties
    to collaborate efficiently in the unlearning process without sharing data, thus
    erasing data samples from a trained FL model completely.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[136](#bib.bib136)]集中于FL在时间方向上的异质性，这与在空间方向上观察到的异质性不同。在增量训练期间，模型保留之前学到的知识并获取新信息，数据分布可能随时间而非平稳。模型必须在重新训练过程中学习遗忘旧知识以获得新知识；这种遗忘能力被称为“去学习”。然而，去学习FL模型是具有挑战性的，因为训练数据不会与中央PS服务器共享。针对FL中的去学习问题，刘等人[[136](#bib.bib136)]提出了一种节省时间和能源的重新训练方法。他们利用一阶泰勒展开来逼近目标，并使用对角经验费舍尔信息矩阵[[141](#bib.bib141)]来逼近准牛顿法中的逆赫希矩阵。这种快速重新训练方法允许不同的FL参与方在去学习过程中高效协作而无需共享数据，从而彻底清除已训练FL模型中的数据样本。
- en: III-D Lessons Learned toward Communication-efficient Large-scale Model Synchronization
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 朝向通信高效的大规模模型同步的经验教训
- en: In this section, we summarize lessons learned from developing communication-efficient
    algorithms for model synchronization in large-scale distributed DL. These insights
    have ensued a number of interesting research topics.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了开发用于大规模分布式DL中模型同步的通信高效算法的经验教训。这些见解引发了许多有趣的研究话题。
- en: $\bullet$ The bottleneck lies in communication overhead, and employing large-batch
    training can accelerate large-scale distributed DL. When dealing with large data
    sets and models in distributed DL with numerous powerful workers, using a minibatch
    size that is significantly greater than those traditionally used can diminish
    communication overhead and expedite training. Because distributed SGD has been
    demonstrated to exhibit linear convergence speedup w.r.t the number of workers
    in various optimization settings, large-batch training can fully exploit the computational
    power offered by high parallelism in a large-scale cluster. However, large-batch
    training poses challenges that need attention, including increased memory requirements,
    potential decay in model generalization, and the need for more complicated hyperparameter
    tuning. For instance, a staleness-aware learning rate is proposed to adjust the
    learning rate of different models adaptively during different training stages,
    aiming to coordinate the model consistency across workers in large-batch distributed
    training.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 瓶颈在于通信开销，采用大批量训练可以加速大规模分布式深度学习。在处理大数据集和模型时，利用许多强大工作节点的大规模分布式深度学习，使用显著大于传统方法的迷你批量大小可以减少通信开销并加速训练。由于分布式SGD已被证明在各种优化设置下相对于工作节点的数量展现出线性收敛加速，大批量训练可以充分发挥大规模集群中高并行性提供的计算能力。然而，大批量训练也带来了需要关注的挑战，包括增加的内存需求、模型泛化能力的潜在下降以及更复杂的超参数调整需求。例如，提出了一种感知陈旧性的学习率，以在不同训练阶段自适应地调整不同模型的学习率，旨在协调大批量分布式训练中各工作节点的模型一致性。
- en: $\bullet$ Heterogeneity is inevitable, and distributed SGD must adapt to heterogeneous
    environments dynamically. Various forms of heterogeneity are unavoidable in large-scale
    distributed DL. For optimal performance, distributed SGD should adjust its hyperparameters
    to match the characteristics of data, models, resources, and even the temporal
    dynamics of these factors. The analysis of theoretical convergence guarantees
    for new adaptive distributed SGD algorithms can follow existing analyzing frameworks
    presented in other literature, controlling certain hyperparameters as fixed variables
    while keeping other dynamic. Challenges of this approach include efficient profiling
    of these characteristics, the development of efficient algorithms to search for
    optimal solutions in dynamic training environment, and ensuring robustness across
    diverse workloads, including those with non-IID and non-stationary data sets.
    The isolation and opacity of environmental characteristics in FL scenarios further
    complicate finding a practical solution to address these challenges.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 异质性是不可避免的，分布式SGD必须动态适应异质环境。在大规模分布式深度学习中，各种形式的异质性是不可避免的。为了获得最佳性能，分布式SGD应调整其超参数以匹配数据、模型、资源甚至这些因素的时间动态特征。对新型自适应分布式SGD算法的理论收敛保证分析可以遵循现有文献中提出的分析框架，将某些超参数作为固定变量，同时保持其他变量动态。该方法面临的挑战包括高效地描述这些特征、开发有效的算法以在动态训练环境中搜索最优解，并确保在各种负载下，包括那些具有非IID和非平稳数据集的负载下的鲁棒性。在FL场景中，环境特征的隔离和不透明性进一步加剧了寻求实际解决方案的复杂性。
- en: $\bullet$ Stragglers pose obstacles, and hierarchical SGD can alleviate the
    straggler problem caused by heterogeneous device resources effectively. In the
    large-scale distributed DL scenario with a considerable number of heterogeneous
    devices having diverse computational and communication resources, devices can
    be clustered into different groups based on their resource capacities, localities,
    and data profiles. Hierarchical SGD, which applies different model-synchronization
    strategies for different groups at different levels, can limit the impact of stragglers
    to a minimal extent. However, this strategy also poses challenges pending to be
    tackled. Examples include the complexity in devising the optimal clustering topology,
    balancing workload, and ensuring model consistency across groups and levels. Additionally,
    the strategy must demonstrate adaptability to dynamic device membership throughout
    the entire training process.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 拖延者带来障碍，而分层SGD可以有效缓解由异构设备资源引起的拖延者问题。在拥有大量异构设备的分布式深度学习场景中，设备可以根据其资源能力、本地性和数据特征被分组。分层SGD对不同组和不同层级应用不同的模型同步策略，可以将拖延者的影响降到最低。然而，这种策略也面临待解决的挑战。例如，制定最佳的聚类拓扑、平衡工作负载和确保组间及层级间的模型一致性具有复杂性。此外，该策略必须在整个训练过程中对动态设备成员具有适应性。 '
- en: 'TABLE VI: Studies on communication-efficient compression techniques for large-scale
    distributed DL'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：大规模分布式深度学习的通信高效压缩技术研究
- en: '|                                                 Category | Strategy&Ref.
    | Year | Highlight |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|                                                 分类 | 策略&参考文献 | 年份 | 亮点 |'
- en: '|   Quantization ([IV-A](#S4.SS1 "IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Error-feedback ([IV-A1](#S4.SS1.SSS1 "IV-A1 Deterministic
    quantization with error feedback ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | 1-bit SGD [[142](#bib.bib142)] | 2014 | The first
    quantization for distributed DL, reducing gradients to 1 bit with error feedback.
    |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '|   量化（[IV-A](#S4.SS1 "IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")） | 错误反馈（[IV-A1](#S4.SS1.SSS1 "IV-A1 Deterministic quantization
    with error feedback ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")） | 1-bit SGD [[142](#bib.bib142)] | 2014 | 分布式深度学习中的首次量化，将梯度减少到1位并加入错误反馈。
    |'
- en: '| SignSGD [[143](#bib.bib143)] | 2018 | Quantizing gradients to 1-bit signs
    based on majority votes among workers. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| SignSGD [[143](#bib.bib143)] | 2018 | 基于工人之间的多数投票将梯度量化为1位符号。 |'
- en: '|  | EF-SignSGD [[144](#bib.bib144)] | 2019 | SignSGD with error feedback.
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | EF-SignSGD [[144](#bib.bib144)] | 2019 | 带有错误反馈的SignSGD。 |'
- en: '|  | 1-bit Adam [[145](#bib.bib145)] | 2021 | Applying 1-bit quantization with
    error feedback to the Adam optimizer. |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | 1-bit Adam [[145](#bib.bib145)] | 2021 | 对Adam优化器应用带有错误反馈的1位量化。'
- en: '| Stochastic ([IV-A2](#S4.SS1.SSS2 "IV-A2 Stochastic quantization ‣ IV-A Gradient
    Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Gupta et al. [[146](#bib.bib146)]
    | 2015 | Stochastically rounding to 16-bit computation. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 随机（[IV-A2](#S4.SS1.SSS2 "IV-A2 Stochastic quantization ‣ IV-A Gradient Quantization
    ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")） | Gupta等人 [[146](#bib.bib146)]
    | 2015 | 随机舍入到16位计算。 |'
- en: '| DoReFa-Net [[147](#bib.bib147)] | 2016 | I ntroducing noise in uniform distribution
    to compensate quantization errors. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| DoReFa-Net [[147](#bib.bib147)] | 2016 | 在均匀分布中引入噪声以补偿量化误差。 |'
- en: '|  | QNN [[148](#bib.bib148)] | 2017 | Stochastic binary quantization based
    on a hard sigmoid probability function. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | QNN [[148](#bib.bib148)] | 2017 | 基于硬Sigmoid概率函数的随机二值量化。 |'
- en: '|  | ZipML [[149](#bib.bib149)] | 2017 | Stochastic quantization based on empirical
    distributions derived via double sampling. |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | ZipML [[149](#bib.bib149)] | 2017 | 基于通过双重采样得出的经验分布的随机量化。 |'
- en: '|  | NaturalComp [[150](#bib.bib150)] | 2022 | Stochastically rounding gradients
    to the upper or lower nearest powers of two. |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | NaturalComp [[150](#bib.bib150)] | 2022 | 随机地将梯度舍入到上下最接近的2的幂。 |'
- en: '|  | Suresh et al. [[151](#bib.bib151)] | 2017 | Quantizing rotated gradients
    multiplied by a random rotation matrix, eliminating gradient distribution assumption.
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | Suresh et al. [[151](#bib.bib151)] | 2017 | 对旋转梯度进行量化，并乘以随机旋转矩阵，消除梯度分布假设。
    |'
- en: '| Matrix decomposition ([IV-A3](#S4.SS1.SSS3 "IV-A3 Matrix decomposition quantization
    ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ATOMO [[152](#bib.bib152)]
    | 2018 | Applying entry-wise or singular value decomposition (SVD) on gradients.
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 矩阵分解 ([IV-A3](#S4.SS1.SSS3 "IV-A3 Matrix decomposition quantization ‣ IV-A
    Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ATOMO [[152](#bib.bib152)]
    | 2018 | 对梯度应用逐项或奇异值分解（SVD）。 |'
- en: '| PowerSGD [[153](#bib.bib153)] | 2019 | Applying two low-rank matrices for
    decomposing and composing gradients efficiently. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| PowerSGD [[153](#bib.bib153)] | 2019 | 应用两个低秩矩阵高效地分解和组合梯度。'
- en: '|  | Vogels et al. [[154](#bib.bib154)] | 2020 | Increasing the number of rank
    1 power iterations to enhance PowerSGD for higher rank quantization approximation.
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  | Vogels et al. [[154](#bib.bib154)] | 2020 | 增加rank 1的幂迭代次数，以增强PowerSGD对高阶量化逼近的能力。
    |'
- en: '|  | PCA-AWFL [[155](#bib.bib155)] | 2023 | Leveraging principle component
    analysis to reduce uplink gradients in wireless FL scenarios. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | PCA-AWFL [[155](#bib.bib155)] | 2023 | 利用主成分分析来减少无线FL场景中的上行梯度。 |'
- en: '| Guarantee ([IV-A4](#S4.SS1.SSS4 "IV-A4 Convergence guarantees for quantization
    ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160),
    [161](#bib.bib161), [162](#bib.bib162)] | 2017-2022 | Analyses of theoretical
    convergence guarantees of quantization w.r.t. the quantization level. |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 保证 ([IV-A4](#S4.SS1.SSS4 "IV-A4 Convergence guarantees for quantization ‣
    IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160),
    [161](#bib.bib161), [162](#bib.bib162)] | 2017-2022 | 量化的理论收敛保证的分析，涉及量化水平。 |'
- en: '| Quantization for FL ([IV-A5](#S4.SS1.SSS5 "IV-A5 Gradient quantization in
    FL ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | AQG [[163](#bib.bib163)] | 2022 | Adaptive quantization level; skipping
    stable quantized gradients and amplifying certain quantized gradient regarding
    device dropouts. |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| FL的量化 ([IV-A5](#S4.SS1.SSS5 "IV-A5 Gradient quantization in FL ‣ IV-A Gradient
    Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | AQG [[163](#bib.bib163)]
    | 2022 | 自适应量化水平；跳过稳定的量化梯度，并放大与设备掉线相关的某些量化梯度。 |'
- en: '| AdaGQ [[164](#bib.bib164)] | 2023 | Adaptive quantization level for each
    training iteration on each device. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| AdaGQ [[164](#bib.bib164)] | 2023 | 每次训练迭代中对每个设备进行自适应量化水平调整。 |'
- en: '|   Sparsification ([IV-B](#S4.SS2 "IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Threshold ([IV-B1](#S4.SS2.SSS1 "IV-B1 Sparsification
    with a threshold ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient Data
    Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | Strom [[165](#bib.bib165)] | 2015 | The first sparsification
    method for DL, selecting gradients beyond a fixed threshold bound and computing
    residuals of these gradients and the threshold. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|   稀疏化 ([IV-B](#S4.SS2 "IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | 阈值 ([IV-B1](#S4.SS2.SSS1 "IV-B1 Sparsification with
    a threshold ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Strom [[165](#bib.bib165)] | 2015 | 第一个用于深度学习的稀疏化方法，选择超出固定阈值的梯度，并计算这些梯度与阈值的残差。
    |'
- en: '|  | top-$k$ [[166](#bib.bib166)] | 2017 | top-$k$ sparsification dropping
    a significantly large portion of gradients. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | top-$k$ [[166](#bib.bib166)] | 2017 | top-$k$稀疏化，丢弃大量梯度。 |'
- en: '|  | Mem-SGD [[167](#bib.bib167)] | 2018 | Sparsification with error feedback
    for distributed SGD. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | Mem-SGD [[167](#bib.bib167)] | 2018 | 使用误差反馈进行分布式SGD的稀疏化。 |'
- en: '|  | DGC [[168](#bib.bib168)] | 2018 | top-$k$ sparsification with momentum
    correction, local gradient clipping, momentum factor masking, and warmup training
    for SGD with momentum. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | DGC [[168](#bib.bib168)] | 2018 | 带有动量修正、局部梯度裁剪、动量因子掩蔽和动量SGD的预热训练的top-$k$稀疏化。
    |'
- en: '|  | EGC [[169](#bib.bib169)] | 2021 | Sparsification based on layer-wise thresholds
    determined by the entropy of gradient bins of each layer. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '|  | EGC [[169](#bib.bib169)] | 2021 | 基于每层梯度箱的熵确定的逐层阈值的稀疏化。 |'
- en: '|  | MIPD [[170](#bib.bib170)] | 2022 | Sparsification based on layer-wise
    thresholds determined by the L2-Norm of gradients of each layer. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  | MIPD [[170](#bib.bib170)] | 2022 | 基于每层梯度L2范数确定的逐层阈值的稀疏化。 |'
- en: '| Scalability considerations ([IV-B2](#S4.SS2.SSS2 "IV-B2 Scalability considerations
    for sparsification ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Global top-$k$ [[171](#bib.bib171)] | 2019 | Selecting
    a portion of globally largest gradients across all workers and aggregating sparsified
    gradients hierarchically. |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 扩展性考虑 ([IV-B2](#S4.SS2.SSS2 "IV-B2 稀疏化的扩展性考虑 ‣ IV-B 梯度稀疏化 ‣ IV 通信高效的数据压缩
    ‣ 通信高效的大规模分布式深度学习：综合调查")) | Global top-$k$ [[171](#bib.bib171)] | 2019 | 选择所有工作节点中的部分全局最大梯度，并以层次化方式聚合稀疏化梯度。
    |'
- en: '| ScaleCom [[172](#bib.bib172)] | 2020 | Workers taking turns to be the leading
    worker and using its local top-$k$ selection indices for the selection in all
    other workers. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| ScaleCom [[172](#bib.bib172)] | 2020 | 工作者轮流成为主工作节点，并使用其本地top-$k$选择索引进行所有其他工作节点的选择。
    |'
- en: '|  | SIDCo [[173](#bib.bib173)] | 2021 | Sampling and matching gradients to
    some heuristic distribution, whose optimal sparsification threshold is determined
    empirically. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '|  | SIDCo [[173](#bib.bib173)] | 2021 | 采样并将梯度匹配到某些启发式分布，其最佳稀疏化阈值通过经验确定。 |'
- en: '|  | MSTopK [[174](#bib.bib174)] | 2021 | Using binary search to determine
    an approximate sparsification threshold efficiently. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | MSTopK [[174](#bib.bib174)] | 2021 | 使用二分搜索高效确定近似稀疏化阈值。 |'
- en: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | Global top-$k$ with adaptive threshold.
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | 带有自适应阈值的全局top-$k$。'
- en: '|  | JointSpar [[176](#bib.bib176)] | 2022 | Dropping certain layers based
    on a probability distribution. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | JointSpar [[176](#bib.bib176)] | 2022 | 基于概率分布丢弃某些层。 |'
- en: '| Guarantee ([IV-B3](#S4.SS2.SSS3 "IV-B3 Convergence guarantees for sparsification
    ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient Data Compression ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | [[177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)]
    | 2018-2021 | Analyses of theoretical convergence guarantees of sparsification
    w.r.t. the sparsification level. |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 保证 ([IV-B3](#S4.SS2.SSS3 "IV-B3 稀疏化的收敛保证 ‣ IV-B 梯度稀疏化 ‣ IV 通信高效的数据压缩 ‣ 通信高效的大规模分布式深度学习：综合调查"))
    | [[177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)]
    | 2018-2021 | 关于稀疏化水平的理论收敛保证的分析。 |'
- en: '| Communication-computation trade-off ([IV-B4](#S4.SS2.SSS4 "IV-B4 Communication-computation
    trade-off optimization ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | OMGS-SGD [[181](#bib.bib181)] | 2020 | Optimizing
    scheduling of layer-wise aggregation of top-$k$ sparsification to overlap communication
    with computation. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 通信-计算权衡 ([IV-B4](#S4.SS2.SSS4 "IV-B4 通信-计算权衡优化 ‣ IV-B 梯度稀疏化 ‣ IV 通信高效的数据压缩
    ‣ 通信高效的大规模分布式深度学习：综合调查")) | OMGS-SGD [[181](#bib.bib181)] | 2020 | 优化逐层聚合top-$k$稀疏化的调度，以重叠通信与计算。
    |'
- en: '| DRAGONN [[182](#bib.bib182)] | 2022 | Applying sparsification when the communication
    benefit surpasses computation overhead. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| DRAGONN [[182](#bib.bib182)] | 2022 | 当通信收益超过计算开销时应用稀疏化。 |'
- en: '| Sparsification for FL ([IV-B5](#S4.SS2.SSS5 "IV-B5 Sparsification in FL ‣
    IV-B Gradient Sparsification ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | STC [[183](#bib.bib183)]
    | 2020 | top-$k$ sparsification and ternary quantization for bidirectional communication
    data compression in FL. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| FL中的稀疏化 ([IV-B5](#S4.SS2.SSS5 "IV-B5 FL中的稀疏化 ‣ IV-B 梯度稀疏化 ‣ IV 通信高效的数据压缩
    ‣ 通信高效的大规模分布式深度学习：综合调查")) | STC [[183](#bib.bib183)] | 2020 | top-$k$稀疏化和三值量化用于FL中的双向通信数据压缩。
    |'
- en: '|  | GossipFL [[184](#bib.bib184)] | 2022 | Synchronizing sparsified gradients
    among workers in a gossip manner based on a bandwidth-aware matrix. |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | GossipFL [[184](#bib.bib184)] | 2022 | 基于带宽感知矩阵的八卦式同步稀疏梯度。 |'
- en: '|  | QSFL [[185](#bib.bib185)] | 2022 | Worker-level sparsification and model-level
    sparsification. |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | QSFL [[185](#bib.bib185)] | 2022 | 工人级稀疏化和模型级稀疏化。'
- en: '|  | FAB-top-$k$ [[186](#bib.bib186)] | 2020 | Optimizing the sparsification
    level to minimize training time with constraints of the fairness of sparsity among
    workers. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | FAB-top-$k$ [[186](#bib.bib186)] | 2020 | 在工人之间公平稀疏的约束下优化稀疏化水平，以最小化训练时间。
    |'
- en: '|  | FedDD [[187](#bib.bib187)] | 2023 | Optimizing the sparsification level
    to minimize training time with constraints of heterogeneous resources, data, and
    models. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | FedDD [[187](#bib.bib187)] | 2023 | 在异构资源、数据和模型的约束下，优化稀疏化水平以最小化训练时间。 |'
- en: '|   Others ([IV-C](#S4.SS3 "IV-C Other Gradient Compression Technologies ‣
    IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Combined methods ([IV-C1](#S4.SS3.SSS1
    "IV-C1 Combining quantization and sparsification ‣ IV-C Other Gradient Compression
    Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[165](#bib.bib165),
    [166](#bib.bib166), [188](#bib.bib188), [179](#bib.bib179), [170](#bib.bib170)]
    | 2015-2022 | Sequentially applying sparsification and quantization. |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|   Others ([IV-C](#S4.SS3 "IV-C Other Gradient Compression Technologies ‣
    IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Combined methods ([IV-C1](#S4.SS3.SSS1
    "IV-C1 Combining quantization and sparsification ‣ IV-C Other Gradient Compression
    Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[165](#bib.bib165),
    [166](#bib.bib166), [188](#bib.bib188), [179](#bib.bib179), [170](#bib.bib170)]
    | 2015-2022 | 依次应用稀疏化和量化。 |'
- en: '| Guarantee ([IV-C2](#S4.SS3.SSS2 "IV-C2 Convergence guarantees for quantization
    and sparsification combination ‣ IV-C Other Gradient Compression Technologies
    ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] | 2018-2022 | Analyses of theoretical convergence guarantees
    of combining quantization and sparsification. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Guarantee ([IV-C2](#S4.SS3.SSS2 "IV-C2 Convergence guarantees for quantization
    and sparsification combination ‣ IV-C Other Gradient Compression Technologies
    ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] | 2018-2022 | 分析量化和稀疏化组合的理论收敛保证。 |'
- en: '| Sparsification with encoding ([IV-C3](#S4.SS3.SSS3 "IV-C3 Combining sparsification
    with compression encoding ‣ IV-C Other Gradient Compression Technologies ‣ IV
    Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | 3LC [[192](#bib.bib192)]
    | 2019 | Combining sparsification and quantization with lossless encoding algorithms.
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Sparsification with encoding ([IV-C3](#S4.SS3.SSS3 "IV-C3 Combining sparsification
    with compression encoding ‣ IV-C Other Gradient Compression Technologies ‣ IV
    Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | 3LC [[192](#bib.bib192)]
    | 2019 | 将稀疏化和量化与无损编码算法相结合。 |'
- en: '| DFS [[193](#bib.bib193)] | 2023 | Encoding sparsified gradient blocks into
    a zero-compacted format for bidirectional communication. |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| DFS [[193](#bib.bib193)] | 2023 | 将稀疏化的梯度块编码为零压缩格式，用于双向通信。 |'
- en: '| Residual ([IV-C4](#S4.SS3.SSS4 "IV-C4 Residual compression ‣ IV-C Other Gradient
    Compression Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ResFed [[194](#bib.bib194)]
    | 2023 | Compressing residuals based on the difference of a sequence of updated
    model for bidirectional communication. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Residual ([IV-C4](#S4.SS3.SSS4 "IV-C4 Residual compression ‣ IV-C Other Gradient
    Compression Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ResFed [[194](#bib.bib194)]
    | 2023 | 基于更新模型序列差异的残差压缩，用于双向通信。 |'
- en: '| Autoencoder ([IV-C5](#S4.SS3.SSS5 "IV-C5 Autoencoder compression ‣ IV-C Other
    Gradient Compression Technologies ‣ IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | LGC [[195](#bib.bib195)] | 2021 | Auto-encoding the common component
    of sparsified gradients. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 自编码器 ([IV-C5](#S4.SS3.SSS5 "IV-C5 自编码器压缩 ‣ IV-C 其他梯度压缩技术 ‣ IV 通信高效数据压缩 ‣
    通信高效的大规模分布式深度学习：全面综述")) | LGC [[195](#bib.bib195)] | 2021 | 对稀疏化梯度的公共组件进行自编码。
    |'
- en: '|   |  |  |  |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |'
- en: IV Communication-Efficient Data Compression
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 通信高效数据压缩
- en: 'Compressing data, including model parameters and gradients, can reduce communication
    overhead across workers and the PS during distributed training and inference effectively.
    This process entails two crucial trade-offs: the balance between computation and
    communication, and that between accuracy and workloads. The former trade-off expects
    that allocating computational resources for compression can significantly enhance
    communication performance and consequently, overall efficiency. This requires
    compression algorithms to be computationally efficient. The latter anticipates
    reduced communication overhead or training and inference time, without significantly
    compromising model convergence performance and prediction accuracy. As listed
    in Table [VI](#S3.T6 "TABLE VI ‣ III-D Lessons Learned toward Communication-efficient
    Large-scale Model Synchronization ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"), communication data compression technologies for large-scale distributed
    DL mainly fall into the following categories: gradient quantization, gradient
    sparsification, and others. Fig. [8](#S4.F8 "Figure 8 ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") illustrates an example of these compression technologies.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩数据，包括模型参数和梯度，可以有效减少分布式训练和推理过程中跨工作节点和参数服务器的通信开销。这个过程涉及两个关键的权衡：计算与通信之间的平衡，以及准确性与工作负载之间的平衡。前者期望为压缩分配计算资源可以显著提升通信性能，从而提高整体效率。这要求压缩算法在计算上具有高效性。后者则期望减少通信开销或训练和推理时间，而不显著影响模型收敛性能和预测准确性。正如表[VI](#S3.T6
    "表 VI ‣ III-D 迈向通信高效的大规模模型同步的经验教训 ‣ III 通信高效模型同步 ‣ 通信高效的大规模分布式深度学习：全面综述")所列，大规模分布式深度学习的通信数据压缩技术主要分为以下几类：梯度量化、梯度稀疏化和其他。图[8](#S4.F8
    "图 8 ‣ IV 通信高效数据压缩 ‣ 通信高效的大规模分布式深度学习：全面综述")展示了这些压缩技术的一个示例。
- en: This section reviews existing works in these categories, with a special focus
    on applying communication-efficient data compression technologies for distributed
    DL in large-scale scenarios. We also summarize lessons learned from these works
    for future high-performance communication data compression technologies in large-scale
    distributed DL.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了这些类别中的现有工作，特别关注在大规模场景中将通信高效的数据压缩技术应用于分布式深度学习。我们还总结了这些工作的经验教训，以指导未来的大规模分布式深度学习中的高性能通信数据压缩技术。
- en: '![Refer to caption](img/0692c6d921595d8266f26ffc5fd336ce.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/0692c6d921595d8266f26ffc5fd336ce.png)'
- en: 'Figure 8: An example of applying various communication data compression technologies
    to the data in distributed DL'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：将各种通信数据压缩技术应用于分布式深度学习数据的示例
- en: IV-A Gradient Quantization
  id: totrans-367
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 梯度量化
- en: Gradient quantization [[196](#bib.bib196)], or quantization, is a lossy compression
    technique used in distributed training, particularly the data-parallel training
    with distributed SGD. Exchanging high-precision gradients across the cluster and
    updating the model iteratively consume computational and communication resources
    intensively. Quantization methods reduce the data, typically gradients, originally
    represented by 32- or 64-bit floating-point values into lower-precision values
    with fewer bits. This helps mitigate computation and communication overhead for
    large-scale distributed DL.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度量化 [[196](#bib.bib196)]，或量化，是一种在分布式训练中使用的有损压缩技术，特别是在分布式SGD的数据并行训练中。跨集群交换高精度梯度并迭代更新模型会大量消耗计算和通信资源。量化方法将原本以32位或64位浮点数表示的数据（通常是梯度）减少到具有更少位数的低精度值。这有助于减轻大规模分布式深度学习的计算和通信开销。
- en: Various quantization methods primarily differ in the number of bits used and
    the data-rounding strategy. Common bit numbers for quantized gradients, also referred
    to as quantization levels, include 16 [[146](#bib.bib146)], 9 [[150](#bib.bib150)],
    8 [[197](#bib.bib197)], 2 [[158](#bib.bib158)], 1 [[143](#bib.bib143)], or of
    variable length [[151](#bib.bib151)]. Typical data-rounding strategies encompass
    deterministic rounding with error feedback, stochastic rounding, and matrix decomposition.
    This subsection initially introduces quantization methods categorized by various
    data-rounding strategies, subsequently presents theoretical analyses on the convergence
    performance of quantization, and finally, discusses special considerations for
    applying quantization to FL in large-scale and heterogeneous settings.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 各种量化方法主要在于使用的位数和数据舍入策略的不同。量化梯度的常见位数，也称为量化水平，包括16 [[146](#bib.bib146)]、9 [[150](#bib.bib150)]、8 [[197](#bib.bib197)]、2 [[158](#bib.bib158)]、1 [[143](#bib.bib143)]，或者可变长度 [[151](#bib.bib151)]。典型的数据舍入策略包括带有误差反馈的确定性舍入、随机舍入和矩阵分解。本小节首先介绍按各种数据舍入策略分类的量化方法，随后对量化的收敛性能进行理论分析，最后讨论在大规模和异构环境下将量化应用于联邦学习（FL）的特殊考虑。
- en: IV-A1 Deterministic quantization with error feedback
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 确定性量化与误差反馈
- en: Some quantization methods utilize deterministic functions to reduce model parameters
    or gradients into lower-precision values. The 1-bit SGD method [[142](#bib.bib142)]
    is recognized as the first quantization method with error feedback, or error compensation,
    for distributed DL. 1-bit SGD condenses 32-bit gradients into one bit per value
    by using a constant quantization threshold of 0. To uphold model accuracy, its
    error-feedback mechanism ensures that previous quantization errors are reflected
    in the subsequent minibatch gradient quantization. Similarly, SignSGD [[143](#bib.bib143)]
    transmits only 1-bit signs of gradients. It employs a majority-vote strategy to
    aggregate these gradient signs in the PS before broadcasting them back to workers.
    SignSGD demonstrates efficient convergence in non-convex problems using optimizers
    of SGD and SGD with momentum, particularly when gradients exhibit density comparable
    to, or greater than than stochasticity and curvature. However, Karimireddy et
    al. [[144](#bib.bib144)] show that SignSGD may fail to converge in certain convex
    problems or exhibit poor generalization due to the biased nature of the sign quantization.
    To address this bias, they introduce EF-SignSGD, incorporating an error-feedback
    mechanism akin to 1-bit SGD [[142](#bib.bib142)] into SignSGD. This modification
    results in the model converging as rapidly as SGD and demonstrate superior generalization
    compared to SignSGD.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 一些量化方法利用确定性函数将模型参数或梯度减少为低精度值。1位SGD方法 [[142](#bib.bib142)] 被认为是第一个具有误差反馈或误差补偿的量化方法，适用于分布式深度学习（DL）。1位SGD通过使用0的恒定量化阈值，将32位梯度压缩为每个值一个比特。为了保持模型的准确性，其误差反馈机制确保了之前的量化误差在后续的小批量梯度量化中得到反映。类似地，SignSGD [[143](#bib.bib143)]
    仅传输梯度的1位符号。它采用多数投票策略在参数服务器（PS）中聚合这些梯度符号，然后再广播回工作节点。SignSGD在处理非凸问题时展示了高效的收敛，使用了SGD及带动量的SGD优化器，特别是当梯度的密度与随机性和曲率相当或更大时。然而，Karimireddy
    等人 [[144](#bib.bib144)] 表明SignSGD在某些凸问题中可能无法收敛或由于符号量化的偏差性质表现出较差的泛化能力。为了解决这个偏差，他们引入了EF-SignSGD，将类似于1位SGD的误差反馈机制融入到SignSGD中。这一修改使得模型的收敛速度与SGD一样快，并且相比SignSGD表现出更优的泛化能力。
- en: 'The quantization with error feedback needs adaptation when applied to large
    models. Though error feedback can compensate for accumulated gradient errors for
    SGD and SGD with momentum, it cannot address the non-linear gradient errors in
    the Adam optimizer. The Adam optimizer is utilized widely in Transformer-based
    models such as Bert [[5](#bib.bib5)] and LLMs [[14](#bib.bib14)]. To overcome
    this limitation in quantization for Adam optimizers, 1-bit Adam [[145](#bib.bib145)]
    capitalizes the observation that Adam’s variance remains stable in the early stage
    of distributed training. It divides the distributed training process into two
    phases: Adam warmup and momentum quantization. The warmup phase employs vanilla
    Adam for a few epochs. Subsequently, the Adam’s variance term becomes fixed, and
    the momentum quantization phase utilizes the SGD with momentum optimizer, applying
    1-bit quantization with error feedback.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用于大模型时，带有误差反馈的量化需要调整。尽管误差反馈可以补偿 SGD 和带动量的 SGD 的累计梯度误差，但它不能解决 Adam 优化器中的非线性梯度误差。Adam
    优化器广泛用于基于 Transformer 的模型，如 Bert [[5](#bib.bib5)] 和 LLMs [[14](#bib.bib14)]。为了克服
    Adam 优化器量化中的这一限制，1-bit Adam [[145](#bib.bib145)] 利用 Adam 的方差在分布式训练的早期阶段保持稳定的观察结果。它将分布式训练过程分为两个阶段：Adam
    预热和动量量化。预热阶段采用原始 Adam 进行几个 epoch。随后，Adam 的方差项变得固定，动量量化阶段使用带动量的 SGD 优化器，应用 1-bit
    量化与误差反馈。
- en: Without randomization, deterministic quantization methods are easy to compute.
    However, biased quantization errors are commonly acknowledged as a prevalent issue
    in most deterministic methods. To mitigate quantization bias, the error-feedback
    mechanism is one approach; nevertheless, the focus of numerous studies is on stochastic
    quantization methods to address this problem.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 没有随机化的情况下，确定性量化方法计算起来很容易。然而，偏差量化误差被普遍认为是大多数确定性方法中的一个常见问题。为了减轻量化偏差，误差反馈机制是一种方法；然而，许多研究的重点是随机量化方法，以解决这个问题。
- en: IV-A2 Stochastic quantization
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 随机量化
- en: Stochastic quantization methods usually make distribution assumptions on the
    data and employ stochastic functions to ensure that quantized gradients serve
    as unbiased approximations to the original gradients. To the best of our knowledge,
    Gupta et al. [[146](#bib.bib146)] are the first to introduce a stochastic rounding
    scheme to gradient quantization, which condenses distributed training into 16-bit
    fixed point computation. The likelihood of rounding a floating-point value to
    a specific fixed-point value is proportional to the proximity of these two values.
    Deterministic and stochastic methods can also be combined. DoReFa-Net [[147](#bib.bib147)]
    quantizes CNN model parameters and activations deterministically, while quantizing
    gradients stochastically. When quantizing gradients, DoReFa-Net introduces gradient
    noise in a uniform distribution to compensate for the quantization error. However,
    assuming a uniform distribution for gradient errors in the stochastic functions
    of these methods may not fully eliminate quantization bias.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 随机量化方法通常对数据进行分布假设，并使用随机函数来确保量化梯度作为对原始梯度的无偏近似。据我们所知，Gupta 等人[[146](#bib.bib146)]
    是首个将随机舍入方案引入梯度量化的人，这种方法将分布式训练压缩为 16 位定点计算。将浮点值舍入为特定定点值的可能性与这两个值的接近程度成正比。确定性和随机方法也可以结合使用。DoReFa-Net
    [[147](#bib.bib147)] 确定性地量化 CNN 模型参数和激活值，同时随机量化梯度。在量化梯度时，DoReFa-Net 以均匀分布引入梯度噪声，以补偿量化误差。然而，假设这些方法中的随机函数中的梯度误差服从均匀分布，可能无法完全消除量化偏差。
- en: To address the limitation of the uniform distribution assumption on gradient
    errors, some studies opt for alternative distributions to ensure unbiased quantization.
    QNN [[148](#bib.bib148)] also quantizes gradients into binary values, similar
    to [[142](#bib.bib142)], but bases it on a hard sigmoid probability function.
    This stochastic binarization method is verified to reduce training time and memory
    consumption significantly without compromising the model prediction accuracy.
    ZipML [[149](#bib.bib149)] employs an empirical distribution of gradients, derived
    through a double sampling strategy, to minimize stochastic quantization variance
    in linear models. Nevertheless, its performance on non-linear models is not well
    investigated. Natural Compression [[150](#bib.bib150)] rounds gradients stochastically
    to the nearest powers of two, whether upper or lower. In contrast, Suresh et al. [[151](#bib.bib151)]
    propose a stochastic rotated quantization method that makes no assumption on the
    data distribution. This method generates a random rotation matrix to rotate gradients
    and quantizes the rotated results, yielding a lower mean square error (MSE) between
    the original and quantized gradients. To further reduce the MSE, a variable-length
    coding approach is approach, encoding quantized data into variable-length formats.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对梯度误差的均匀分布假设的局限性，一些研究选择了替代分布以确保量化的无偏性。QNN [[148](#bib.bib148)] 也将梯度量化为二进制值，类似于 [[142](#bib.bib142)]，但它基于硬sigmoid概率函数。这种随机二值化方法被验证能显著减少训练时间和内存消耗，而不会影响模型预测的准确性。ZipML [[149](#bib.bib149)]
    使用通过双重采样策略得到的梯度经验分布，以最小化线性模型中的随机量化方差。然而，其在非线性模型上的表现尚未得到充分研究。Natural Compression [[150](#bib.bib150)]
    随机地将梯度四舍五入到最接近的二的幂，无论是向上还是向下。相较之下，Suresh 等人 [[151](#bib.bib151)] 提出了一个随机旋转量化方法，该方法对数据分布没有假设。该方法生成一个随机旋转矩阵来旋转梯度，并对旋转后的结果进行量化，从而在原始梯度和量化梯度之间产生较低的均方误差
    (MSE)。为了进一步减少MSE，采用了一种可变长度编码方法，将量化数据编码为可变长度格式。
- en: IV-A3 Matrix decomposition quantization
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 矩阵分解量化
- en: Recently, some studies have proposed to apply quantization to the decomposed
    matrices of gradients. ATOMO [[152](#bib.bib152)] applies entry-wise or singular
    value decomposition (SVD) on gradients, and the decomposed results are sparsified
    subsequently. ATOMO is an unbiased compression scheme and represents a generalized
    version of QSGD [[156](#bib.bib156)] and TernGrad [[158](#bib.bib158)]. However,
    deriving SVD iteratively is computationally expensive. To address this issue,
    PowerSGD [[153](#bib.bib153)] employs a low-rank gradient compressor based on
    the power iteration [[198](#bib.bib198)]. This method introduces two low-rank
    matrices for quantization and the decomposition and composition involve only low-rank
    matrix multiplication and orthogonalization, ensuring computation efficiency.
    Claiming to be the pioneering gradient-quantization method, it achieves end-to-end
    wall-clock training speedup compared to vanilla SGD, enhancing scalability to
    large clusters, and exhibits superior generalization performance. On one hand,
    in contrast to prior works such as sign-based quantization with a majority vote [[143](#bib.bib143),
    [144](#bib.bib144)], which require a gather operation, PowerSGD allows the hierarchical
    addition of the decomposed matrix. This approach enables efficient leverage of
    communication optimization for the allreduce primitive in a large-scale cluster.
    On the other hand, a low-rank gradient update can be interpreted as a spectral
    regularization [[199](#bib.bib199)], contributing to the model’s enhanced generalization.
    A follow-up study by Vogels et al. [[154](#bib.bib154)] enhances the practicality
    of PowerSGD by achieving a higher-rank quantization approximation through increasing
    the number of rank-1 power iterations. In the scenarios of wireless federated
    learning, where the uplink transmission from the worker to the PS is a communication
    bottleneck, PCA-AWFL [[155](#bib.bib155)] leverages principle component analysis
    to reduce the dimension of uploaded gradients of Nesterov’s momentum.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究提出将量化应用于梯度的分解矩阵。ATOMO [[152](#bib.bib152)] 对梯度应用逐项或奇异值分解（SVD），然后对分解结果进行稀疏化处理。ATOMO
    是一种无偏的压缩方案，代表了 QSGD [[156](#bib.bib156)] 和 TernGrad [[158](#bib.bib158)] 的广义版本。然而，迭代地推导
    SVD 是计算上昂贵的。为了解决这个问题，PowerSGD [[153](#bib.bib153)] 使用了一种基于幂迭代的低秩
- en: IV-A4 Convergence guarantees for quantization
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 量化的收敛性保证
- en: 'Lower-precision computation through quantization can degrade the convergence
    performance of DL models, thereby reducing prediction accuracy. Consequently,
    minimizing communication costs when ensuring theoretic guarantees on convergence
    performance becomes a primary focus of quantization methods. Alistarh et al. [[156](#bib.bib156)]
    analyze tight bounds on the trade-off between precision and quantization variance
    trade-off, and propose QSGD, a family of stochastic gradient-quantization algorithms.
    QSGD provides theoretical guarantees on convergence rate with a bound on the gradient
    precision. In line with QSGD, Konečnỳ and Richtárik [[157](#bib.bib157)] investigate
    the trade-off between the communication cost and MSE through a family of stochastic
    quantization methods. These methods vary in terms of variable-size or fixed-size
    encoding, as well as dense or sparse communication protocols. This study provides
    quantization MSE bounds associated with various communication costs. Wen et al. [[158](#bib.bib158)]
    further prove the convergence from the perspective of statistical bound on gradients.
    Based on this convergence analysis, they propose a special case of QSGD, known
    as TernGrad, which map gradients stochastically into ternary values: positive,
    zero, and negative. Workers and the PS synchronize these ternary values along
    with a gradient scalar. Subsequently, this scalar multiplies the ternary values
    in each worker, producing actual gradients for updating models. ECQ-SGD [[159](#bib.bib159)]
    further combines the error-feedback mechanism with stochastic gradient quantization,
    ensuring a tighter bound on the worst-case error compared to stochastic quantization
    methods without error feedback, such as QSGD. NUQSGD [[160](#bib.bib160)] offers
    strictly tighter bounds on the trade-off between communication cost and quantization
    variance than QSGD. This improvement is achieved by substituting the uniform quantization
    scheme in QSGD with an unbiased nonuniform logarithmic scheme.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 通过量化进行的低精度计算可能会降低深度学习模型的收敛性能，从而减少预测准确性。因此，在确保理论收敛性能保证的同时，最小化通信成本成为量化方法的主要关注点。Alistarh
    等人 [[156](#bib.bib156)] 分析了精度与量化方差之间的权衡的紧 bounds，并提出了 QSGD，一系列随机梯度量化算法。QSGD 提供了收敛速率的理论保证，并对梯度精度设定了界限。与
    QSGD 一致，Konečnỳ 和 Richtárik [[157](#bib.bib157)] 通过一系列随机量化方法研究了通信成本与均方误差 (MSE)
    之间的权衡。这些方法在变量大小或固定大小编码以及密集或稀疏通信协议方面有所不同。这项研究提供了与各种通信成本相关的量化 MSE 界限。Wen 等人 [[158](#bib.bib158)]
    从梯度的统计界限角度进一步证明了收敛性。基于这一收敛分析，他们提出了 QSGD 的一个特例，称为 TernGrad，它将梯度随机映射到三个值：正、零和负。工人和参数服务器
    (PS) 同步这些三值及一个梯度标量。随后，该标量在每个工人中乘以三值，生成实际梯度用于更新模型。ECQ-SGD [[159](#bib.bib159)]
    进一步将误差反馈机制与随机梯度量化结合起来，与没有误差反馈的随机量化方法（如 QSGD）相比，确保了在最坏情况下误差的更紧界限。NUQSGD [[160](#bib.bib160)]
    提供了比 QSGD 更严格的通信成本与量化方差之间的权衡界限。这一改进通过将 QSGD 中的均匀量化方案替换为无偏非均匀对数方案实现。
- en: In the pursuit of adaptive quantization, Faghri et al. [[161](#bib.bib161)]
    suggest quantization methods that adjust quantization levels among different iterations
    dynamically based on runtime statistics of data distribution. IntSGD [[162](#bib.bib162)]
    employs an adaptive scaling factor to multiply gradients before quantizing them
    stochastically into integer values. These values are then scaled back for model
    updating by the reciprocal of the scaling factor. This scaling factor is calibrated
    using the moving average of model parameters, facilitating computation efficiency
    of quantization. Theoretical and empirical demonstrations show that these adaptive
    methods offer tighter variance and precision bounds than non-adaptive ones.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在追求自适应量化方面，Faghri 等人 [[161](#bib.bib161)] 建议根据数据分布的运行时统计信息动态调整不同迭代中的量化水平。IntSGD
    [[162](#bib.bib162)] 使用自适应缩放因子在随机将梯度量化为整数值之前对梯度进行乘法操作。这些值随后通过缩放因子的倒数进行缩放回，以便用于模型更新。这个缩放因子是通过模型参数的移动平均进行校准的，从而提高了量化的计算效率。理论和实证证明显示，这些自适应方法提供了比非自适应方法更紧的方差和精度界限。
- en: IV-A5 Gradient quantization in FL
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A5 联邦学习中的梯度量化
- en: Recently, emerging studies have focuses on gradient quantization in FL, which
    presents unique challenges. First, the heterogeneity of computing devices and
    non-IID nature of data in FL amplify the challenges for unbiased quantization.
    Second, as the norm of gradients varies during training in FL, a fixed quantization
    level failed to minimize the communication cost while guaranteeing convergence
    throughout the training process. Third, ensuring the reliability of devices in
    FL is challenging, and frequent device dropouts pose a significant concern. To
    tackle these challenges, Mao et al. [[163](#bib.bib163)] introduce Augmented Adaptive
    Quantized Gradient (Augmented AQG). Augmented AQG enables devices to skip transmitting
    slowly varying quantized gradients, addressing the heterogeneity issue. It determines
    the quantization level during training adaptively to aim for the minimum communication
    cost and amplifies certain quantized gradients appropriately to mitigate potential
    device dropouts. On the other hand, AdaGQ [[164](#bib.bib164)] addresses the fluctuating
    gradient norm problem on diverse edge devices in FL. It employs an adaptive quantization
    method, allowing each device to decide its own suitable quantization level for
    every training iteration. Both approaches improve the distributed training performance
    when dealing with non-IID data.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，涌现的研究聚焦于FL中的梯度量化，提出了独特的挑战。首先，FL中计算设备的异质性和数据的非IID性放大了不偏量化的挑战。其次，在FL训练过程中，渐变的范数变化，固定的量化水平无法保证在整个训练过程中最小化通信成本。第三，确保FL中设备的可靠性具有挑战性，频繁的设备掉线是一个重要问题。为了解决这些挑战，Mao等人[[163](#bib.bib163)]
    提出了增强自适应量化渐变（Augmented AQG）。增强AQG使设备跳过传输慢变化的量化梯度，解决了异质性问题。它在训练过程中自适应确定量化水平，以达到最小的通信成本，并适当放大某些量化梯度，以减轻潜在的设备掉线问题。另一方面，AdaGQ
    [[164](#bib.bib164)] 解决了FL中各种边缘设备上波动的渐变范数问题。它采用自适应量化方法，允许每个设备在每个训练迭代中决定自己适合的量化水平。这两种方法在处理非IID数据时改善了分布式训练性能。
- en: IV-B Gradient Sparsification
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 渐变稀疏化
- en: Gradient sparsification, or sparsification, is another lossy mechanism orthogonal
    to gradient quantization for communication data compression in distributed training,
    particularly the data-parallel training with distributed SGD. Sparsification takes
    advantage of the observation known as gradient sparsity during backpropagation
    when updating model parameters, where gradients exhibit positive skewness, with
    many being close to zero or insignificant. It involves discarding less significant
    gradients and sparsifying more substantial gradients, aiming to reduce communication
    costs in distributed training. Typically, sparse data are encoded into index and
    values pairs, which can undergo additional quantization and compression.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 渐变稀疏化，或者稀疏化，是另一种与梯度量化正交的通信数据压缩机制，在分布式训练中特别是数据并行训练中的分布式SGD中使用。稀疏化利用了在反向传播期间已知的渐变稀疏性观察，当更新模型参数时，渐变呈现出正偏斜性，其中许多渐变接近零或微不足道。它涉及丢弃不太重要的梯度，并使更重要的梯度稀疏化，旨在减少分布式训练中的通信成本。通常，稀疏数据被编码为索引和值对，可以进行额外的量化和压缩。
- en: In this subsection, we introduce various sparsification methods, consider their
    scalability issues, provide theoretical analyses of convergence guarantees, and
    examine the trade-off between computation and communication. Additionally, we
    discuss optimizations for sparsification in high-performance FL within large-scale
    and heterogeneous settings.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们介绍了各种稀疏化方法，考虑了它们的可扩展性问题，提供了收敛保证的理论分析，并且研究了计算和通信之间的权衡。此外，我们还讨论了在大规模和异构设置中在高性能联邦学习中进行稀疏化的优化。
- en: IV-B1 Sparsification with a threshold
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 带阈值的稀疏化
- en: 'To the best of our knowledge, Strom [[165](#bib.bib165)] introduces the first
    gradient-sparsification method for distributed training. This method only selects
    gradients with absolute values above a fixed threshold and sends sparse residual
    values calculated by the difference between these gradients and the threshold.
    This threshold represents the sparsification level, indicating the level of sparsity
    after sparsification. Since an appropriate fixed threshold is hard to determine
    in practice, Aji and Heafield [[166](#bib.bib166)] propose an approach known as
    top-$k$ sparsification, whereby a large portion (e.g., 99%) of gradients with
    smaller values are dropped. Similar to the quantization error, sparsification
    can also accumulate gradient errors during iterative training. Mem-SGD [[167](#bib.bib167)]
    incorporates the error-feedback mechanism into sparsification to compensate for
    sparsification errors, thus avoiding error explosion. However, these methods are
    designed for SGD but may result in significant gradient errors if applied to SGD
    with momentum, which is a pervasive replacement of vanilla SGD. To ensure the
    convergence performance of sparsification when applied to SGD with momentum, DGC [[168](#bib.bib168)]
    utilizes four other techniques on top of the top-$k$ sparsification: momentum
    correction, local gradient clipping [[200](#bib.bib200)], momentum factor masking [[201](#bib.bib201)],
    and warmup training. The momentum correction technique sparsifies momentum terms
    instead of gradients to maintain the accumulated discounting factor in SGD with
    momentum. In addition, DGC applies local gradient clipping to prevent exploding
    gradients by limiting the upper bounds of gradients. It employs momentum factor
    masking to address staleness problems by preventing momentum for stale gradients.
    Furthermore, warmup training accelerates training by utilizing a smaller learning
    rate and less aggressive gradient sparsity during the early stage.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，Strom [[165](#bib.bib165)] 首次引入了用于分布式训练的梯度稀疏化方法。该方法仅选择绝对值高于固定阈值的梯度，并发送通过这些梯度与阈值之间的差异计算出的稀疏残差值。这个阈值代表了稀疏化水平，指示了稀疏化后的稀疏程度。由于在实际操作中很难确定合适的固定阈值，Aji
    和 Heafield [[166](#bib.bib166)] 提出了一种称为 top-$k$ 稀疏化的方法，即丢弃大部分（例如 99%）值较小的梯度。类似于量化误差，稀疏化在迭代训练过程中也会累积梯度误差。Mem-SGD
    [[167](#bib.bib167)] 将误差反馈机制纳入稀疏化，以补偿稀疏化误差，从而避免误差爆炸。然而，这些方法是为 SGD 设计的，如果应用于具有动量的
    SGD（这是标准 SGD 的普遍替代品），可能会导致显著的梯度误差。为了确保在应用于具有动量的 SGD 时稀疏化的收敛性能，DGC [[168](#bib.bib168)]
    在 top-$k$ 稀疏化基础上利用了其他四种技术：动量修正、局部梯度裁剪 [[200](#bib.bib200)]、动量因子掩蔽 [[201](#bib.bib201)]
    和预热训练。动量修正技术稀疏化动量项而不是梯度，以保持具有动量的 SGD 中的累积折扣因子。此外，DGC 应用局部梯度裁剪，通过限制梯度的上限来防止梯度爆炸。它利用动量因子掩蔽来解决陈旧问题，防止对陈旧梯度应用动量。此外，预热训练通过在早期阶段使用较小的学习率和较少的梯度稀疏性来加速训练。
- en: However, the top-$k$ sparsification methods above have limitations in identifying
    the significance of gradients. Initially, the threshold is determined based on
    the gradient distribution of the entire DNN model, which is a coarse metric because
    gradients of different layers of the model can follow different distributions.
    Moreover, the magnitude of gradients may not be the best measurement for gradient
    significance. Addressing these limitations, EGC [[169](#bib.bib169)] employs the
    layer-wise threshold, determined by the entropy of gradient bins of each layer,
    indicating the significance of that layer. In contrast, MIPD [[170](#bib.bib170)]
    determines the layer-wise sparsification threshold adaptively across different
    layers and employs the layer-wise L2-Norm as the indicator of gradient significance.
    These finer-grained approaches for determining the sparsification threshold are
    expected to contain aggregated sparsification errors within a small range and
    preserve prediction accuracy; however, calibrating the threshold for every layer
    in every training iteration adds to computation complexity, which may exacerbate
    scalability problems.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述的 top-$k$ 稀疏化方法在识别梯度的重要性方面存在局限性。首先，阈值是根据整个深度神经网络模型的梯度分布确定的，这是一个粗略的度量，因为模型不同层的梯度可能遵循不同的分布。此外，梯度的大小可能并不是梯度重要性的最佳衡量标准。针对这些局限性，EGC[[169](#bib.bib169)]采用了逐层阈值，由每一层的梯度分布熵确定，表示该层的重要性。相比之下，MIPD[[170](#bib.bib170)]自适应确定了不同层的逐层稀疏化阈值，并采用逐层
    L2 范数作为梯度重要性的指标。这些更精细的方法来确定稀疏化阈值预期将包含在一个小范围内的聚合稀疏化误差，并保持预测准确性；然而，为每一层在每一次训练迭代中校准阈值增加了计算复杂度，可能加剧可伸缩性问题。
- en: IV-B2 Scalability considerations for sparsification
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 稀疏化的可伸缩性考虑
- en: Threshold-based sparsification methods may encounter scalability problems when
    applied in large-scale clusters for three main reasons. Firstly, the local top-$k$
    methods mandate every worker to select a specific portion of gradients, resulting
    in an accumulated gradient volume that increases linearly with the number of workers
    and becomes non-negligible in large clusters. Secondly, frequent determination
    of the dynamic threshold in top-$k$ sparsification is computationally expensive,
    which hinders its application in large DNN models. Thirdly, as training with large
    batch sizes [[202](#bib.bib202)] becomes popular for large models in large clusters,
    issues such as the increasing computation cost per iteration and Layer-wise Adaptive
    learning Rate Scaling (LARS) [[203](#bib.bib203), [204](#bib.bib204)] pose new
    challenges to gradient sparsification; however, there is a lack of evidences that
    these sparsification methods work well in this scenario.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 基于阈值的稀疏化方法在大规模集群中应用时可能会遇到可伸缩性问题，主要有三个原因。首先，局部的 top-$k$ 方法要求每个工作节点选择一定比例的梯度，导致累积梯度体积随着工作节点数量呈线性增长，并且在大型集群中变得不可忽视。其次，频繁确定
    top-$k$ 稀疏化的动态阈值会带来计算开销，这妨碍了其在大型深度神经网络模型中的应用。第三，随着大型模型的大批量训练[[202](#bib.bib202)]在大型集群中变得常见，诸如迭代中的逐层自适应学习率缩放（LARS）[[203](#bib.bib203),
    [204](#bib.bib204)]等问题给梯度稀疏化带来了新的挑战；然而，目前缺乏证据表明这些稀疏化方法在这种场景下表现良好。
- en: To tackle the first issue of large gradient volume, Shi et al. [[171](#bib.bib171)]
    propose a global top-$k$ sparsification method to reduce the communication cost
    in large-cluster environments. This method selects a portion of globally largest
    gradients among all workers and synchronizes through a hierarchical aggregation
    strategy. Notably, this method demonstrate significant improvements in scalability
    efficiency compared to earlier top-$k$ sparsification methods. Another relevant
    development is ScaleCom [[172](#bib.bib172)], which capitalizes on the observation
    that distributions of sparsification errors are similar among workers. ScaleCom
    employs a Cyclic Local top-$k$ (CLT-k) method, where workers take turns as the
    leading worker, utilizing its local top-$k$ selection indices for the gradient
    selection in all other workers. The calculation of CLT-k is commutative, ensuring
    efficiency in allreduce implementations.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 针对大梯度体积的第一个问题，Shi 等人 [[171](#bib.bib171)] 提出了全球 top-$k$ 稀疏化方法，以降低大型集群环境中的通信成本。该方法在所有工作节点中选择一部分全球最大梯度，并通过分层聚合策略进行同步。值得注意的是，与早期的
    top-$k$ 稀疏化方法相比，该方法在扩展效率上表现出显著改进。另一个相关的发展是 ScaleCom [[172](#bib.bib172)]，它利用了稀疏化误差分布在工作节点之间相似的观察。ScaleCom
    采用了循环局部 top-$k$（CLT-k）方法，其中工作节点轮流作为主节点，利用其局部 top-$k$ 选择索引来选择所有其他工作节点的梯度。CLT-k
    的计算是可交换的，确保了所有reduce实现的效率。
- en: 'To tackle the second issue of calculating overhead, SIDCo [[173](#bib.bib173)]
    employs the heuristic that DNN gradients can be approximated by three representative
    sparsity-inducing distributions. The thresholds of these heuristic distributions
    can be determined efficiently at a designated compression ratio. SIDCo concentrates
    on sampling gradients and matching them to one of the distributions, thus obtaining
    the threshold for sparsification naturally. On the other hand, MSTopK [[174](#bib.bib174)]
    introduces an approximate top-$k$ sparsification approach, which uses binary search
    to determine an approximate sparsification threshold efficiently. Similar to [[171](#bib.bib171)],
    Ok-Topk [[175](#bib.bib175)] also adopts the global top-$k$ method but only updates
    the threshold every few iterations instead of every iteration to further reduce
    the threshold computation overhead. The rationale of this periodic strategy derives
    from empirical observations that gradients change very slowly across multiple
    consecutive training iterations. In addition, collective communication libraries
    tailored for sparse data enhance collective operations of sparsified gradients
    during distributed training, as detailed in Section [VI-C3](#S6.SS3.SSS3 "VI-C3
    Collective communication for sparse gradients ‣ VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '针对计算开销的第二个问题，SIDCo [[173](#bib.bib173)] 采用了一个启发式方法，即 DNN 梯度可以通过三种代表性的稀疏分布进行近似。这些启发式分布的阈值可以在指定的压缩比下有效确定。SIDCo
    重点对梯度进行采样，并将其匹配到其中一种分布，从而自然地获得稀疏化的阈值。另一方面，MSTopK [[174](#bib.bib174)] 引入了一种近似
    top-$k$ 稀疏化方法，通过二分查找来有效确定近似稀疏化阈值。类似于 [[171](#bib.bib171)]，Ok-Topk [[175](#bib.bib175)]
    也采用了全球 top-$k$ 方法，但仅在每几次迭代后更新一次阈值，而不是每次迭代都更新，从而进一步降低阈值计算开销。这种周期性策略的原理源于经验观察，即梯度在多个连续训练迭代中变化非常缓慢。此外，专门针对稀疏数据的集体通信库增强了分布式训练过程中稀疏梯度的集体操作，详见第
    [VI-C3](#S6.SS3.SSS3 "VI-C3 Collective communication for sparse gradients ‣ VI-C
    Inter-GPU Collective Communication ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") 节。'
- en: To address the third issue arising with LARS in large-batch training, where
    learning rates scale differently in different model layers, ScaleCom additionally
    applies low-pass filtering to accumulated sparsification errors locally to accommodate
    rapid model changes and large gradient noise caused by the scaled learning rate.
    The scalability of ScaleCom is ensured because its sparsification performance
    is independent of the number of workers and the batch size per worker. To reduce
    computation costs in large-batch training, JointSpar [[176](#bib.bib176)] sparsifies
    gradient computation and communication jointly to avoid redundant gradient computation
    for those deemed to be dropped. JointSpar first decomposes gradients into gradient
    blocks layer by layer, and then decides which gradient blocks to drop based on
    a probability distribution set over gradient blocks, where each element indicates
    the probability of dropping a specific gradient block. JointSpar avoids both computation
    and communication of these blocks to drop. The convergence performance of JointSpar
    has been demonstrated theoretically and experimentally for large-batch training
    with LARS.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对大批量训练中LARS带来的第三个问题，即不同模型层中的学习率比例不同，ScaleCom 额外对累积的稀疏化误差进行低通滤波，以适应由于学习率缩放引起的快速模型变化和大量梯度噪声。ScaleCom
    的可扩展性得到了保证，因为其稀疏化性能与工作者数量和每个工作者的批量大小无关。为了降低大批量训练中的计算成本，JointSpar [[176](#bib.bib176)]
    通过联合稀疏化梯度计算和通信来避免对那些被认为会被丢弃的梯度进行冗余计算。JointSpar 首先逐层将梯度分解成梯度块，然后根据设定在梯度块上的概率分布决定丢弃哪些梯度块，其中每个元素表示丢弃特定梯度块的概率。JointSpar
    避免了这些被丢弃的块的计算和通信。JointSpar 在理论上和实验中都证明了其在使用 LARS 的大批量训练中的收敛性能。
- en: IV-B3 Convergence guarantees for sparsification
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 稀疏化的收敛保证
- en: Similar to quantization, there is a trade-off between convergence and communication
    in sparsification. The communication performance is associated with the sparsification
    compression ratio, i.e., sparsification level. The convergence performance of
    the sparsification methods above is supported by experimental results on commodity
    cloud infrastructures, but many methods lack theoretical guarantees. In an effort
    to develop sparsification methods with theoretical convergence guarantees, GSpar [[177](#bib.bib177)]
    adopts a random approach, which drops gradients with probabilities and amplifies
    the remaining gradients to ensure unbiased sparsification. The probabilities are
    calibrated to minimize the communication cost after sparsification while adhering
    to a specific gradient error variance constraint. Alistarh et al. [[178](#bib.bib178)]
    provide the first convergence theoretical analysis of the top-$k$ sparsification.
    By giving non-trivial upper bounds on the convergence rate, this analysis demonstrates
    that top-$k$ sparsification methods provide convergence guarantees for distributed
    SGD with either convex or non-convex training objectives. Wang et al. [[179](#bib.bib179)]
    analyze the convergence performance of top-$k$ sparsification with respect to
    the compression ratio. They propose a Fast-Fourier-Transform-based (FFT-based)
    method for top-$k$ sparsification, employing FFT to transform gradients into the
    frequency domain and drops low-energy gradients according to a compression ratio
    that is bound to converge. Compared to previous gradient quantization and top-$k$
    sparsification methods, the distribution of reconstructed gradients using this
    FFT-based method is closer to that of the original gradients prior to sparsification,
    indicating that the FFT-based method preserves more relevant information.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于量化，稀疏化中存在收敛性和通信之间的权衡。通信性能与稀疏化压缩比，即稀疏化水平相关。上述稀疏化方法的收敛性性能由商品云基础设施上的实验结果支持，但许多方法缺乏理论保障。为了开发具有理论收敛保障的稀疏化方法，GSpar
    [[177](#bib.bib177)] 采用了一种随机方法，通过概率丢弃梯度并放大剩余梯度以确保无偏稀疏化。这些概率被校准以最小化稀疏化后的通信成本，同时遵守特定的梯度误差方差约束。Alistarh
    等人 [[178](#bib.bib178)] 提供了 top-$k$ 稀疏化的首个收敛性理论分析。通过给出收敛速率的非平凡上界，该分析证明了 top-$k$
    稀疏化方法为分布式 SGD 提供了收敛性保障，无论是对凸训练目标还是非凸训练目标。Wang 等人 [[179](#bib.bib179)] 分析了 top-$k$
    稀疏化在压缩比方面的收敛性能。他们提出了一种基于快速傅里叶变换（FFT）的 top-$k$ 稀疏化方法，利用 FFT 将梯度转换到频域，并根据一个收敛绑定的压缩比丢弃低能量梯度。与以前的梯度量化和
    top-$k$ 稀疏化方法相比，该基于 FFT 的方法重建梯度的分布更接近于稀疏化前的原始梯度，表明该方法保留了更多相关信息。
- en: When local or global top-$k$ sparsification methods apply constant thresholds
    across different training iterations, they result in sub-optimal convergence or
    communication performance. To guarantee the convergence performance in the presence
    of varying thresholds across different training iterations, Sahu et al. [[180](#bib.bib180)]
    propose an adaptive top-$k$ sparsification method with a communication constraint.
    Given a communication budget for the entire training, this method selects the
    threshold adaptively for each iteration, with the goal of minimizing the accumulated
    sparsification error of the entire training. A theoretical analysis shows that
    it converges for both convex and non-convex objectives.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 当局部或全局 top-$k$ 稀疏化方法在不同训练迭代中应用常量阈值时，它们会导致次优的收敛性或通信性能。为了在不同训练迭代中存在变化的阈值时保证收敛性能，Sahu
    等人 [[180](#bib.bib180)] 提出了一个具有通信约束的自适应 top-$k$ 稀疏化方法。鉴于整个训练的通信预算，该方法为每次迭代自适应选择阈值，目标是最小化整个训练的累积稀疏化误差。理论分析表明，该方法对凸优化和非凸优化目标都能收敛。
- en: IV-B4 Communication-computation trade-off optimization
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B4 通信-计算权衡优化
- en: Besides concerns about convergence, there are also concerns about the computation
    overhead introduced by sparsification, such as the additional encoding and synchronization
    overhead for sparsified gradients. To tackle the trade-off between computation
    and communication in gradient sparsification, OMGS-SGD [[181](#bib.bib181)] defines
    top-$k$ sparsification as an optimization problem of scheduling a set of layer-wise
    aggregation tasks of sparsified gradients. These tasks of different gradient layers
    involve computation and communication stages and can be scheduled in a pipeline,
    allowing the communication stage of an earlier task can overlap with the computation
    stage of subsequent tasks. OMGS-SGD find the optimal scheduling of layer-wise
    gradient aggregation, aiming to minimize the training time. From an analytical
    perspective, DRAGONN [[182](#bib.bib182)] implements sparsification selectively,
    activating it only when the estimated communication time saved exceeds the computing
    overhead associated with sparsification. DRAGONN also minimizes the sparsification
    computing overhead through the use of an efficient hashing algorithm to obtain
    gradients above the threshold, instead of relying on the mask-based (e.g., Hadamard-product-based)
    selection algorithm widely adopted by other sparsification methods.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对收敛性的担忧外，还有对稀疏化引入的计算开销的担忧，例如稀疏化梯度的额外编码和同步开销。为了解决梯度稀疏化中的计算与通信之间的权衡，OMGS-SGD [[181](#bib.bib181)]
    将 top-$k$ 稀疏化定义为一个优化问题，即调度一组层级聚合任务来处理稀疏化梯度。这些不同梯度层的任务涉及计算和通信阶段，并可以在管道中调度，使得早期任务的通信阶段可以与后续任务的计算阶段重叠。OMGS-SGD
    寻找层级梯度聚合的最佳调度，旨在最小化训练时间。从分析的角度来看，DRAGONN [[182](#bib.bib182)] 选择性地实现稀疏化，仅在估计的通信时间节省超过与稀疏化相关的计算开销时才激活它。DRAGONN
    还通过使用高效的哈希算法来获取超过阈值的梯度，从而最小化稀疏化计算开销，而不是依赖于其他稀疏化方法广泛采用的基于掩码（例如 Hadamard 乘积）的选择算法。
- en: IV-B5 Sparsification in FL
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B5 FL中的稀疏化
- en: 'Similar to quantization, challenges regarding heterogeneity of computing devices,
    non-IID data, and communication patterns in FL also present in gradient sparsification.
    Numerous gradient-sparsification methods have been developed to address the FL
    heterogeneity from various perspectives. STC [[183](#bib.bib183)] represents the
    pioneering work in addressing bidirectional communication data compression in
    FL. When many studies focus gradient quantization in the worker-to-server direction,
    STC employs top-$k$ sparsification and ternary quantization for compressing communication
    in both the worker-to-server uploading and server-to-worker downloading. Focusing
    on the heterogeneity in bandwidth and communication pattern in FL, GossipFL [[184](#bib.bib184)]
    utilizes a bandwidth-aware gossip matrix to guide how a peer worker synchronizes
    sparsified gradients with a single other peer in a gossip-based P2P communication
    pattern in FL. This gossip matrix is designed based on the bandwidth information
    among peer workers for high-bandwidth communication, thus requiring little communication
    time in the synchronization. Concerning the heterogeneity of gradient contributions
    among workers in FL, QSFL [[185](#bib.bib185)] employs a two-level gradient-sparsification
    method: worker-level sparsification for worker-to-server communication and model-level
    sparsification for server-to-worker communication. On the worker level, QSFL selects
    qualified workers for uploading gradients to the PS server considering both the
    worker’s contribution to the loss and the relevance between the local worker model
    and the global server model. On the model level, QSFL divides the global synchronized
    model into segments and sends one segment to each qualified worker. This worker-level
    sparsification is considered exclusive for FL scenarios where worker models can
    be heterogeneous, distinguishing it from the standard tensor-level [[166](#bib.bib166)]
    or layer-level [[176](#bib.bib176)] sparsification used in homogeneous model synchronization.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于量化，FL中的计算设备异质性、非IID数据和通信模式的挑战在梯度稀疏化中也存在。为应对FL中的异质性，已经开发了许多梯度稀疏化方法。STC[[183](#bib.bib183)]代表了在FL中解决双向通信数据压缩的开创性工作。当许多研究集中于工人到服务器方向的梯度量化时，STC采用top-$k$稀疏化和三值量化来压缩工人到服务器的上传和服务器到工人的下载的通信。关注FL中带宽和通信模式的异质性，GossipFL[[184](#bib.bib184)]利用带宽感知的gossip矩阵来指导一个对等工人与另一个对等工人同步稀疏梯度的方式，该模式基于gossip的P2P通信模式。这个gossip矩阵是根据对等工人之间的带宽信息设计的，以实现高带宽通信，因此在同步中需要很少的通信时间。关于FL中工人之间梯度贡献的异质性，QSFL[[185](#bib.bib185)]采用了两级梯度稀疏化方法：工人级稀疏化用于工人到服务器的通信，模型级稀疏化用于服务器到工人的通信。在工人级别，QSFL选择合格的工人将梯度上传到PS服务器，考虑到工人对损失的贡献以及本地工人模型与全局服务器模型之间的相关性。在模型级别，QSFL将全局同步模型划分为多个部分，并将每个部分发送给一个合格的工人。这种工人级稀疏化被认为是FL场景中工人模型可以异质的独特方法，与用于同质模型同步的标准张量级[[166](#bib.bib166)]或层级[[176](#bib.bib176)]稀疏化方法不同。
- en: To address the trade-off between computation and communication in FL, Han et
    al. [[186](#bib.bib186)] present a Fairness-Aware Bidirectional top-$k$ (FAB-top-$k$)
    sparsification method, which formulates sparsification as an online learning algorithm.
    This approach determines the optimal sparsification level to minimize overall
    training time, considering factors including the estimated derivative sign, adjustable
    search interval, and fairness of sparsity among workers, thus addressing non-IID
    data issues. Similarly, FedDD [[187](#bib.bib187)] formulates sparsification in
    FL as a convex optimization problem to find the optimal sparsification level for
    minimizing the overall training time, considering the heterogeneity of device
    computational resources, data distribution and quality, and model size and structure.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对FL中计算与通信之间的权衡，Han等人[[186](#bib.bib186)]提出了一种公平感知的双向top-$k$（FAB-top-$k$）稀疏化方法，该方法将稀疏化公式化为在线学习算法。此方法确定最佳稀疏化水平以最小化整体训练时间，考虑因素包括估计的导数符号、可调整的搜索区间以及工人之间稀疏性的公平性，从而解决非IID数据问题。类似地，FedDD[[187](#bib.bib187)]将FL中的稀疏化公式化为一个凸优化问题，以找到最优的稀疏化水平来最小化整体训练时间，考虑设备计算资源、数据分布和质量以及模型规模和结构的异质性。
- en: IV-C Other Gradient Compression Technologies
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 其他梯度压缩技术
- en: Other gradient compression technologies mainly include combining quantization
    and sparsification, integrating sparsification with compression encoding methods,
    residual compression, and autoencoder compression. This subsection introduces
    recent advancements in these technologies and provides an analysis of the convergence
    guarantees on some hybrid compression methods.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 其他梯度压缩技术主要包括量化与稀疏化的结合、稀疏化与压缩编码方法的集成、残差压缩以及自编码器压缩。本小节介绍了这些技术的最新进展，并对一些混合压缩方法的收敛保证进行了分析。
- en: IV-C1 Combining quantization and sparsification
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 量化与稀疏化的结合
- en: Gradient sparsification is usually applied in conjunction with other orthogonal
    compression technologies, such as gradient quantization. In [[165](#bib.bib165),
    [166](#bib.bib166)], sparsified gradients are quantized using 1-bit [[142](#bib.bib142)]
    and 2-bit quantization, respectively, to further reduce communication costs. Similarly,
    RedSync [[188](#bib.bib188)] applies gradient sparsification sequentially with
    a binary-searched threshold, similar to [[174](#bib.bib174)], and 1-bit quantization,
    similar to [[142](#bib.bib142)]. In [[179](#bib.bib179)], after FFT-based sparsification,
    the sparsified gradient frequency data are packed into a dense vector with another
    status vector, indicating the index locations. The dense vector is further quantized
    by a range-based quantization method, which converts 32-bit IEEE-754-format data
    to a lower-precision format with a range that matches the original format and
    has a number of bits constrained by training convergence requirements. In [[170](#bib.bib170)],
    following layer-wise sparsification, MIPD further applies quantization based on
    layer-wise gradient distributions, aiming to reduce the aggregated approximation
    error based on the gradient distribution of the entire model.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度稀疏化通常与其他正交压缩技术结合使用，例如梯度量化。在 [[165](#bib.bib165), [166](#bib.bib166)]中，稀疏化的梯度分别使用1位 [[142](#bib.bib142)]和2位量化进行量化，以进一步降低通信成本。类似地，RedSync [[188](#bib.bib188)]采用带有二进制搜索阈值的梯度稀疏化，与 [[174](#bib.bib174)]类似，并且使用1位量化，与 [[142](#bib.bib142)]类似。在 [[179](#bib.bib179)]中，经过基于FFT的稀疏化后，稀疏化的梯度频率数据被打包成一个密集向量，另附有一个状态向量，指示索引位置。密集向量进一步通过基于范围的量化方法进行量化，该方法将32位IEEE-754格式的数据转换为匹配原始格式的低精度格式，且位数受到训练收敛要求的限制。在 [[170](#bib.bib170)]中，在逐层稀疏化之后，MIPD进一步应用了基于逐层梯度分布的量化，旨在减少基于整个模型梯度分布的聚合近似误差。
- en: IV-C2 Convergence guarantees for quantization and sparsification combination
  id: totrans-407
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 量化与稀疏化结合的收敛保证
- en: Theoretical analyses are used demonstrate the convergence performance of the
    combination of sparsification and quantization. PQASGD [[189](#bib.bib189)] presents
    a special combination form of sparsification and quantization, where the synchronization
    of quantized gradients is triggered only periodically but not iteratively. This
    periodic synchronization can be perceived as the sparsification over training
    iterations but not over the model, as discussed earlier. Therefore, PAQSGD can
    be viewed as a combination of quantization and iteration-wise sparsification.
    Theoretically, this combination has been proven to converge at a rate inversely
    proportional to the square root of the product of the total minibatch size across
    all workers and the number of iterations within a period, indicating its ability
    to scale linearly as the number of worker increases. Similar to PAQSGD, Qsparse-local-SGD [[190](#bib.bib190)]
    also synchronizes gradients periodically. The difference is that in the local
    computation between synchronization intervals, each worker additionally undergoes
    ordinary gradient sparsification and quantization sequentially, while maintaining
    a local error memory for the combined error compensation. Qsparse-local-SGD has
    been demonstrated theoretically to converge for smooth non-convex objectives with
    a fixed learning rate and smooth convex objectives with a decaying learning rate.
    To adjust the compression level of the combined methods adaptively to meet the
    convergence guarantee, AC-SGD [[191](#bib.bib191)] optimizes the quantization
    and sparsification level jointly to minimize the gradient variance, regarding
    factors including the communication budget, gradient norm, and remaining number
    of iterations. This jointly optimized compression level guarantees convergence
    for non-convex and quadratic objectives.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 理论分析用于展示稀疏化与量化相结合的收敛性能。PQASGD [[189](#bib.bib189)] 提出了稀疏化与量化的一种特殊结合形式，其中量化梯度的同步仅在周期性地触发，而不是迭代地触发。这种周期性同步可以被看作是对训练迭代的稀疏化，而不是对模型的稀疏化，如前所述。因此，PAQSGD
    可以被视为量化和迭代稀疏化的结合。理论上，这种结合已被证明以与所有工作节点的总小批量大小和每周期内的迭代次数的乘积的平方根成反比的速率收敛，表明它可以随着工作节点数量的增加而线性扩展。类似于
    PAQSGD，Qsparse-local-SGD [[190](#bib.bib190)] 也周期性地同步梯度。不同之处在于，在同步间隔的本地计算中，每个工作节点还会依次经历普通的梯度稀疏化和量化，同时保持一个用于组合误差补偿的本地误差记忆。Qsparse-local-SGD
    已被理论证明在固定学习率下对平滑非凸目标和在衰减学习率下对平滑凸目标收敛。为了适应性地调整组合方法的压缩水平以满足收敛保证，AC-SGD [[191](#bib.bib191)]
    共同优化量化和稀疏化水平，以最小化梯度方差，考虑包括通信预算、梯度范数和剩余迭代次数等因素。这种共同优化的压缩水平保证了非凸和二次目标的收敛。
- en: IV-C3 Combining sparsification with compression encoding
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 结合稀疏化与压缩编码
- en: Gradient sparsification is also usually combined with efficient encoding algorithms.
    To maximize the compression ratio, both STC [[183](#bib.bib183)] (which has been
    introduced) and 3LC [[192](#bib.bib192)] combine sparsification and quantization
    with lossless encoding algorithms. 3LC first quantizes gradients into ternary
    values using a sparsity multiplier, which indicates the threshold for rounding
    gradients to zero, one of the ternary values. It then uses lossless based-3⁵ encoding
    to compact every five ternary values into a one-byte representation, followed
    subsequently by another lossless zero-run-length encoding [[205](#bib.bib205)]
    to shorten consecutive runs of the same bytes. DFS [[193](#bib.bib193)] segments
    sparsified gradients dynamically into data blocks and encodes them into a zero-compacted
    format for bidirectional communication of model synchronization. Given the increased
    compression complexity, designing algorithms that integrate various lossy and
    lossless compression technologies for distributed SGD requires a careful trade-off
    between computation and communication performance. It is essential to ensure that
    the communication benefit provided by compression surpasses its computation overhead.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度稀疏化通常还会与高效编码算法结合。为了最大化压缩比，已经引入的 STC [[183](#bib.bib183)] 和 3LC [[192](#bib.bib192)]
    将稀疏化和量化与无损编码算法结合起来。3LC 首先使用稀疏度乘数将梯度量化为三值，这个稀疏度乘数表示将梯度舍入为零（三值之一）的阈值。然后，它使用基于 3⁵
    的无损编码将每五个三值压缩成一个字节表示，随后再进行另一种无损的零运行长度编码 [[205](#bib.bib205)] 以缩短连续的相同字节序列。DFS
    [[193](#bib.bib193)] 将稀疏化后的梯度动态地分段为数据块，并将其编码为零压缩格式，以实现模型同步的双向通信。由于压缩复杂性的增加，为分布式
    SGD 设计集成各种有损和无损压缩技术的算法需要在计算和通信性能之间进行谨慎权衡。必须确保压缩所提供的通信收益超过其计算开销。
- en: IV-C4 Residual compression
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 残差压缩
- en: Some compression technologies for distributed training adopt the residual-based
    approach. This approach exploits the heuristic that residuals calculated from
    the difference between updated and predicted models within a specific period contain
    denser information than gradients. ResFed [[194](#bib.bib194)] predicts a model
    in local iterations in the worker through memorizing the model trajectory, which
    consists of a sequence of updated models across multiple training updates, and
    then derives the residuals. It communicates residuals between workers and the
    PS server through compression methods such as sparsification and quantization.
    This residual-based compression is used for both uploading and downloading communications,
    making it appropriate for FL scenarios with constrained bandwidth. The residual-based
    sparsification and quantization can be perceived as a generalized version of the
    gradient sparsification and quantization, because when the predicted model is
    set to the last updated model, residuals essentially become gradients.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一些用于分布式训练的压缩技术采用基于残差的方法。这种方法利用了一个启发式，即在特定时间段内，从更新模型和预测模型之间的差异计算出的残差包含比梯度更密集的信息。ResFed
    [[194](#bib.bib194)] 通过记忆模型轨迹来预测本地迭代中的模型，模型轨迹由多个训练更新中的一系列更新模型组成，然后推导出残差。它通过稀疏化和量化等压缩方法在工作节点和参数服务器之间传输残差。这种基于残差的压缩用于上传和下载通信，使其适用于带宽受限的联邦学习场景。基于残差的稀疏化和量化可以视为梯度稀疏化和量化的一种广义版本，因为当预测模型设置为最后更新的模型时，残差本质上变成了梯度。
- en: IV-C5 Autoencoder compression
  id: totrans-413
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C5 自编码器压缩
- en: 'Some gradient-compression methods adopt a DNN autoencoder to encode gradients.
    By exploiting the observation that gradients share similarities across workers
    during distributed training [[172](#bib.bib172)], LGC [[195](#bib.bib195)] conceptually
    decomposes gradient values into two components: the common and innovation components,
    which represents the parts making gradients in a worker similar to or distinguished
    from those in other workers, respectively. The innovation component is captured
    by sparsified gradients with a very aggressive sparsification rate, e.g., 0.001%,
    The common component is generated by inputting 0.1% spasified gradients into the
    encoder side of a lightweight autoencoder, which has been trained for extracting
    the similarity among gradients of different workers during the early stage of
    distributed training. During gradient synchronization, workers and the PS server
    exchange the innovation and encoded common components, which are highly compacted
    for fast communication and can be decoded by the decoder side of the lightweight
    autoencoder efficiently.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一些梯度压缩方法采用 DNN 自编码器来编码梯度。通过利用梯度在分布式训练过程中在不同工作节点之间共享相似性的观察[[172](#bib.bib172)]，LGC
    [[195](#bib.bib195)] 从概念上将梯度值分解为两个组成部分：共同组件和创新组件，分别表示使得一个工作节点的梯度与其他工作节点的梯度相似或不同的部分。创新组件通过具有非常激进的稀疏化率（例如，0.001%）的稀疏梯度来捕获。共同组件通过将
    0.1% 稀疏化的梯度输入到一个轻量级自编码器的编码器部分生成，该自编码器在分布式训练的早期阶段已被训练用于提取不同工作节点梯度之间的相似性。在梯度同步过程中，工作节点和
    PS 服务器交换创新和编码后的共同组件，这些组件高度紧凑，以便快速通信，并且可以通过轻量级自编码器的解码器部分高效解码。
- en: IV-D Lessons Learned toward Communication-efficient Large-scale DL Compression
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 经验教训：面向通信效率的大规模深度学习压缩
- en: In this section, we explore lessons learned from developing communication-efficient
    compression algorithms for large-scale distributed DL, highlighting potential
    research problems for future studies in this area.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了在开发面向通信效率的大规模分布式深度学习压缩算法过程中学到的经验教训，并突出了该领域未来研究中潜在的研究问题。
- en: $\bullet$ The environment is complex and dynamic, and environment-aware adaptive
    compression can optimize the communication overhead in diverse settings. The compression
    ratio can be adjusted based on diverse environment information dynamically, including
    but not limited to the data distribution, model architecture, and communication
    topology. However, the diversity and heterogeneity of these factors pose challenges
    to finding efficient methods for discovering the optimal solution and providing
    real-time adaption.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 环境复杂且动态，环境感知的自适应压缩可以在不同的环境设置中优化通信开销。压缩比可以基于不同的环境信息动态调整，包括但不限于数据分布、模型架构和通信拓扑。然而，这些因素的多样性和异质性对发现有效的方法、找到最佳解决方案和提供实时适应提出了挑战。
- en: $\bullet$ Gradients vary in significance, and finer-grained assessment of gradient
    significance leads to improved overall performance in terms of convergence and
    communication in distributed DL with large models. In large models with different
    learning characteristics in different components, a coarse criterion for gradient
    significance can fail to capture important information for model updating. Utilizing
    different strategies to calibrate gradient significance at the granularity of
    edges, workers, model components, or even model layers can optimize the convergence
    performance with the minimal communication overhead. Yet, fine-grained compression
    entails a high computation overhead. The primary challenge is finding a balance
    between the computation and communication overhead.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 梯度的重要性各不相同，对梯度重要性的更精细评估可以改善大模型分布式深度学习中的收敛性和通信性能。在具有不同组件学习特性的巨大模型中，对梯度重要性的粗略标准可能无法捕捉到对模型更新重要的信息。利用不同策略以边、工作节点、模型组件或甚至模型层的粒度来校准梯度重要性，可以在最小通信开销下优化收敛性能。然而，细粒度的压缩会带来较高的计算开销。主要挑战在于找到计算开销和通信开销之间的平衡。
- en: $\bullet$ Various compression methods can be compatible with each other, and
    hybrid and hierarchical compression methods can collaborate to reduce communication
    overhead significantly. For large-scale distributed DL, a combination of various
    lossy and lossless compression algorithms can be utilized to optimize the communication
    at different hierarchical levels of model synchronization. The convergence performance
    of the hybrid and hierarchical approach is guaranteed theoretically in certain
    optimization settings. However, there is a lack of detailed studies on the specific
    contributions of each component in the hybrid and hierarchical approach to the
    overall convergence and communication performance. This approach approach usually
    relies on empirical combinations of different compression technologies, and achieving
    the optimal combination is challenging due to the large search space.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 各种压缩方法可以相互兼容，混合和分层压缩方法可以协作以显著减少通信开销。对于大规模分布式 DL，可以利用多种有损和无损压缩算法的组合，以优化模型同步的不同层级的通信。这种混合和分层方法在某些优化设置下在理论上保证了收敛性能。然而，缺乏详细的研究来探讨混合和分层方法中每个组件对整体收敛和通信性能的具体贡献。这种方法通常依赖于不同压缩技术的经验组合，由于搜索空间庞大，实现最佳组合具有挑战性。
- en: '![Refer to caption](img/179305ee3d0d4bfda1fafd0a1183230c.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/179305ee3d0d4bfda1fafd0a1183230c.png)'
- en: 'Figure 9: An illustration of resource-management and task-scheduling mechanisms
    in large-scale distributed DL'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：大规模分布式 DL 中资源管理和任务调度机制的示意图
- en: V Large-Scale Resource Allocation and Task Scheduling
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 大规模资源分配和任务调度
- en: 'This section introduces resource-allocation and task-scheduling strategies
    for high-performance distributed DL at a large scale. These strategies are typically
    integrated in cluster-level and distributed-DL-level frameworks. Fig. [9](#S4.F9
    "Figure 9 ‣ IV-D Lessons Learned toward Communication-efficient Large-scale DL
    Compression ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") illustrates a
    procedure of resource-allocation and task-scheduling mechanisms for large-scale
    distributed DL within a cluster. This procedure comprises six major steps. First,
    for a queue of distributed DL jobs, the task scheduler conducts job profiling
    based on various workload characters, such as resource-utilization status and
    working progress. Second, the resource manager allocates GPU and network resources
    distributed within the cluster for jobs based on their characteristic profiling.
    The resources can be represented in a physical or virtual manner, and virtual
    resources can be encapsulated in virtual machines or containers. Third, the job-level
    scheduling determines job-execution priorities based on resource constraints and
    job performance estimation. Fourth, the task-pipeline-level scheduling divides
    the job into subtasks and locates them onto available resources for the pipeline
    execution of the subtasks, aiming to increase task parallelism and overlap computational
    and communication workloads. Fifth, the network-flow-level scheduling optimizes
    the coflows of numerous subtasks by considering the relation and dependency of
    network flows. Sixth, the scheduled jobs, task pipelines, and network flows run
    efficiently on the allocated GPU and network resources within the cluster.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了大规模高性能分布式深度学习（DL）的资源分配和任务调度策略。这些策略通常集成在集群级和分布式 DL 级框架中。图 [9](#S4.F9 "图
    9 ‣ IV-D 从通信高效的大规模 DL 压缩中吸取的经验教训 ‣ IV 通信高效的数据压缩 ‣ 通信高效的大规模分布式深度学习：全面调查") 展示了在集群内进行的大规模分布式
    DL 的资源分配和任务调度机制的流程。该流程包括六个主要步骤。首先，对于一个分布式 DL 作业队列，任务调度器基于各种工作负载特性（如资源利用状态和工作进展）进行作业分析。第二，资源管理器根据作业的特性分析分配集群内的
    GPU 和网络资源。这些资源可以以物理或虚拟的形式存在，虚拟资源可以封装在虚拟机或容器中。第三，作业级调度根据资源约束和作业性能估计确定作业执行优先级。第四，任务管道级调度将作业分解为子任务，并将它们分配到可用资源上进行子任务的管道执行，旨在增加任务并行性并重叠计算和通信负载。第五，网络流级调度通过考虑网络流的关系和依赖性来优化多个子任务的协流。第六，调度后的作业、任务管道和网络流在集群内分配的
    GPU 和网络资源上高效运行。
- en: In this section, we first introduce large-scale GPU management aimed at achieving
    efficient computational and communication-resource utilization in distributed
    DL. We then introduce task scheduling to overlap computational and communication
    tasks of distributed DL, thereby increasing parallelism. During the discussion
    of these strategies, we focus on their applications in the large-scale setting,
    with heterogeneous resource capacities and task workloads. Finally, we summarize
    some lessons learned from these strategies to assist in uncovering promising research
    directions within this scope.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了旨在实现分布式深度学习中计算和通信资源高效利用的大规模GPU管理。然后，我们介绍了任务调度以重叠分布式深度学习的计算和通信任务，从而增加并行性。在讨论这些策略时，我们重点关注它们在大规模环境中的应用，以及异构资源能力和任务工作负载。最后，我们总结了一些从这些策略中获得的经验教训，以帮助揭示这一领域有前景的研究方向。
    |
- en: 'TABLE VII: Studies on resource-allocation strategies for large-scale distributed
    DL'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VII: 大规模分布式深度学习的资源分配策略研究 |'
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|                                                  分类 | 技术与参考 | 年份 | 亮点 |'
- en: '|   Training ([V-A1](#S5.SS1.SSS1 "V-A1 Resource management for distributed
    training ‣ V-A Resource Management ‣ V Large-Scale Resource Allocation and Task
    Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | GPU Sharing. Focus: (1) production cluster; (2) context
    switching; (3) performance estimate; (4) elasticity; (5) hyperparameter tuning
    | Gandiva [[206](#bib.bib206)] | 2018 | (1) Using the profiles of the DL workload
    to improve efficiency of training DL models and latency in a GPU cluster. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|   培训 ([V-A1](#S5.SS1.SSS1 "V-A1 资源管理 ‣ V-A 资源管理 ‣ V 大规模资源分配与任务调度 ‣ 通信高效的大规模分布式深度学习：综合调查"))
    | GPU共享。重点: (1) 生产集群; (2) 上下文切换; (3) 性能估计; (4) 弹性; (5) 超参数调优 | Gandiva [[206](#bib.bib206)]
    | 2018 | (1) 利用深度学习工作负载的配置文件提高深度学习模型训练的效率和GPU集群中的延迟。 |'
- en: '| AntMan [[207](#bib.bib207)] | 2020 | (1) Introducing co-designing the cluster
    scheduler and dynamic scaling mechanisms. |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| AntMan [[207](#bib.bib207)] | 2020 | (1) 引入集群调度器和动态扩展机制的共同设计。 |'
- en: '|  | FGD [[208](#bib.bib208)] | 2023 | (1) Monitoring the individual evaluation
    functions of DL jobs at runtime to make placement decisions and resource allocations
    elastically. |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | FGD [[208](#bib.bib208)] | 2023 | (1) 在运行时监控深度学习作业的个体评估函数，以便弹性地进行位置决策和资源分配。
    |'
- en: '|  | TGS [[209](#bib.bib209)] | 2023 | (1) Designing adaptive rate-control
    and transparent unified-memory mechanisms . |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | TGS [[209](#bib.bib209)] | 2023 | (1) 设计自适应速率控制和透明统一内存机制。 |'
- en: '|  | Salus [[210](#bib.bib210)] | 2019 | (2) Achieving fine-grained GPU sharing
    among multiple DL applications. |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | Salus [[210](#bib.bib210)] | 2019 | (2) 实现多个深度学习应用之间的细粒度GPU共享。 |'
- en: '|  | PipeSwitch [[211](#bib.bib211)] | 2020 | (2)Exploiting the profiles of
    DL applications to achieve millisecond-scale context switching. |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | PipeSwitch [[211](#bib.bib211)] | 2020 | (2) 利用深度学习应用的配置文件实现毫秒级的上下文切换。
    |'
- en: '|  | Optimus [[212](#bib.bib212)] | 2018 | (3) Estimating a DL task’s remaining
    execution time and designing a marginal gain-based allocation algorithm. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | Optimus [[212](#bib.bib212)] | 2018 | (3) 估算深度学习任务剩余执行时间并设计基于边际增益的分配算法。
    |'
- en: '|  | Harmony [[213](#bib.bib213)] | 2019 | (3) Placing training jobs in a manner
    that minimizes interference and maximizes performance. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | Harmony [[213](#bib.bib213)] | 2019 | (3) 以最小化干扰和最大化性能的方式放置训练作业。 |'
- en: '|  | Horus [[214](#bib.bib214)] | 2021 | (3) Proposing a data-driven approach
    to predict the GPU utilization of heterogeneous DL tasks. |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | Horus [[214](#bib.bib214)] | 2021 | (3) 提出了一种数据驱动的方法来预测异构深度学习任务的GPU利用率。
    |'
- en: '|  | Pollux [[215](#bib.bib215)] | 2021 | (4) Combining system throughput with
    statistical efficiency and introducing a formulation of goodput. |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | Pollux [[215](#bib.bib215)] | 2021 | (4) 将系统吞吐量与统计效率相结合，并引入良品率的公式。 |'
- en: '|  | Zico [[216](#bib.bib216)] | 2021 | (4) Monitoring the memory-usage pattern
    of individual DL jobs by tracking computational progress of training jobs. |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | Zico [[216](#bib.bib216)] | 2021 | (4) 通过跟踪训练任务的计算进度来监控单个深度学习作业的内存使用模式。
    |'
- en: '|  | AFS [[217](#bib.bib217)] | 2021 | (4) Handling future jobs requires proactive
    preparation based on current share calculations. |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | AFS [[217](#bib.bib217)] | 2021 | (4) 处理未来任务需要基于当前共享计算的主动准备。 |'
- en: '|  | FlowCon [[218](#bib.bib218)] | 2023 | (4) Minimizing the growth of GPU
    fragmentation through packing tasks. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | FlowCon [[218](#bib.bib218)] | 2023 | (4) 通过打包任务来最小化GPU碎片化的增长。 |'
- en: '|  | Fluid [[219](#bib.bib219)] | 2021 | (5) Utilizing a water-filling approach
    to accelerate the hyperparameter optimization process. |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | Fluid [[219](#bib.bib219)] | 2021 | (5) 采用水填充方法加速超参数优化过程。 |'
- en: '|  | Titan [[220](#bib.bib220)] | 2022 | (5) Merging several fine-tuning workloads
    into one to improve resource utilization. |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | Titan [[220](#bib.bib220)] | 2022 | (5) 将多个微调工作负载合并为一个，以提高资源利用率。 |'
- en: '|  | DISC [[221](#bib.bib221)] | 2022 | (5) Leveraging adaptive scaling to
    adjust the size of GPU time slices and formalizing the dynamic allocation of GPU
    time slices into an optimization problem. |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  | DISC [[221](#bib.bib221)] | 2022 | (5) 利用自适应缩放调整GPU时间片的大小，并将GPU时间片的动态分配形式化为一个优化问题。
    |'
- en: '|  | Hydro [[222](#bib.bib222)] | 2023 | (5) Extending resources of hyperparameter
    tuning workloads by interleaving them with pipeline-enabled large-model training
    tasks. |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | Hydro [[222](#bib.bib222)] | 2023 | (5) 通过将超参数调优工作负载与支持流水线的大模型训练任务交替进行，扩展资源。
    |'
- en: '| Network Bandwidth Sharing. Granularity: (1) job; (2) gradient block; (3)
    coflow | Liquid [[223](#bib.bib223)] | 2021 | (1) Proposing intelligent cluster
    network-efficient scheduling methods in both immediate and batch modes. |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 网络带宽共享。粒度： (1) 任务； (2) 梯度块； (3) 共同流 | Liquid [[223](#bib.bib223)] | 2021
    | (1) 提出智能集群网络高效调度方法，适用于即时和批处理模式。 |'
- en: '| Prophet [[224](#bib.bib224)] | 2021 | (2) Employing the monitored network
    bandwidth and the profiled gradient time interval to predict the number of gradients
    into gradient blocks. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Prophet [[224](#bib.bib224)] | 2021 | (2) 利用监测的网络带宽和配置的梯度时间间隔预测梯度块中的梯度数量。
    |'
- en: '|  | Parrot [[225](#bib.bib225)] | 2020 | (3) Using a linear program (LP) solution
    to derive a weighted bandwidth scaling strategy to minimize the time cost in the
    communication stage. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | Parrot [[225](#bib.bib225)] | 2020 | (3) 使用线性规划（LP）解决方案推导加权带宽缩放策略，以最小化通信阶段的时间成本。
    |'
- en: '|   Inference ([V-A2](#S5.SS1.SSS2 "V-A2 Resource management for distributed
    inference ‣ V-A Resource Management ‣ V Large-Scale Resource Allocation and Task
    Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | Spatial Sharing | GSLICE [[226](#bib.bib226)] | 2020
    | Developing self-learning and adaptive GPU-resource allocation and batching schemes.
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|   推理 ([V-A2](#S5.SS1.SSS2 "V-A2 资源管理用于分布式推理 ‣ V-A 资源管理 ‣ V 大规模资源分配与任务调度 ‣
    通信高效的大规模分布式深度学习：综合调查")) | 空间共享 | GSLICE [[226](#bib.bib226)] | 2020 | 开发自学习和自适应GPU资源分配及批处理方案。
    |'
- en: '| iGniter [[227](#bib.bib227)] | 2022 | Leveraging inference performance model
    to calculate the appropriate batch size and lower bound of allocated GPU resources.
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| iGniter [[227](#bib.bib227)] | 2022 | 利用推理性能模型计算适当的批量大小和分配的GPU资源下限。 |'
- en: '|  | SLO-aware [[228](#bib.bib228)] | 2022 | Distributing inference requests
    to the deployed functions based on the autoscaling decision. |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | SLO-aware [[228](#bib.bib228)] | 2022 | 根据自动缩放决策将推理请求分配给已部署的函数。 |'
- en: '| Temporal Sharing | Nexus [[229](#bib.bib229)] | 2019 | Applying a heuristic
    approach to select the requests. |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 时间共享 | Nexus [[229](#bib.bib229)] | 2019 | 应用启发式方法选择请求。 |'
- en: '| INFaaS [[230](#bib.bib230)] | 2021 | Identifying the colocation interference
    caused by the shared hardware resources. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| INFaaS [[230](#bib.bib230)] | 2021 | 识别由共享硬件资源造成的共置干扰。 |'
- en: '|  | Cocktail [[231](#bib.bib231)] | 2022 | Building a distributed-weighted
    auto-scaling policy that utilizes the importance sampling technique. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | Cocktail [[231](#bib.bib231)] | 2022 | 构建利用重要性采样技术的分布式加权自动缩放策略。 |'
- en: '| Hybrid Sharing | Gpulet [[232](#bib.bib232)] | 2022 | Allowing heterogeneous
    ML models to be mapped to multiple gpulets in the most cost-effective way. |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 混合共享 | Gpulet [[232](#bib.bib232)] | 2022 | 允许异质ML模型以最具成本效益的方式映射到多个gpulet上。
    |'
- en: '| FaST-GShare [[233](#bib.bib233)] | 2023 | Introducing the FaST-Manager to
    limit and isolate spatio-temporal resources. |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| FaST-GShare [[233](#bib.bib233)] | 2023 | 引入FaST-Manager以限制和隔离时空资源。 |'
- en: '|   |  |  |  |  |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |'
- en: V-A Resource Management
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 资源管理
- en: 'According to the different stages of DL, we categorize distinct resource-management
    techniques applicable to training and inference stages of DL. Table [VII](#S5.T7
    "TABLE VII ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") provides a summary
    of relevant computational and communication-resource-management technologies in
    the domain of large-scale distributed DL.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '根据深度学习的不同阶段，我们将不同的资源管理技术分类应用于训练和推理阶段。表格 [VII](#S5.T7 "TABLE VII ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") 提供了在大规模分布式深度学习领域中相关计算和通信资源管理技术的总结。'
- en: V-A1 Resource management for distributed training
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 分布式训练的资源管理
- en: The training process of distributed DL requires an intensive consumption of
    computing power and memory of GPUs and network communication bandwidth across
    GPUs. Therefore, GPU and network bandwidth sharing is the focus of our discussion.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式深度学习的训练过程需要大量的计算能力、GPU内存和GPU之间的网络通信带宽。因此，GPU和网络带宽共享是我们讨论的重点。
- en: '$\bullet$ GPU Sharing: Although GPUs have found extensive applications in distributed
    DL, a prevalent issue of underutilization is observed in production clusters.
    The recorded low GPU utilization typically ranges from 25% to below 50% [[234](#bib.bib234),
    [235](#bib.bib235), [236](#bib.bib236), [237](#bib.bib237), [208](#bib.bib208),
    [238](#bib.bib238)]. This concern is particularly noteworthy in large-scale distributed
    computing environments. To address this issue, various distributed techniques
    have been developed to enable DL tasks to run efficiently on numerous devices [[239](#bib.bib239)].'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ GPU共享：虽然GPU在分布式深度学习中得到了广泛应用，但在生产集群中普遍存在低利用率的问题。记录到的低GPU利用率通常在25%到50%以下[[234](#bib.bib234),
    [235](#bib.bib235), [236](#bib.bib236), [237](#bib.bib237), [208](#bib.bib208),
    [238](#bib.bib238)]。在大规模分布式计算环境中，这一问题尤为值得关注。为了解决这一问题，已开发出多种分布式技术，以使深度学习任务能够高效地在众多设备上运行[[239](#bib.bib239)]。
- en: GPU-sharing techniques leverage partial resource allocation through virtualization,
    presenting a feasible strategy to mitigate the challenge of low GPU utilization
    in large-scale distributed DL. NVIDIA, acknowledged as the leading GPU provider,
    introduces Multiple Process Sharing (MPS) [[240](#bib.bib240)] that offers an
    operating-system-level virtualization solution. Nevertheless, its implementation
    requires application-specific expertise to define resource limits for ensuring
    performance isolation. Moreover, MPS lacks compatibility with various DL frameworks.
    To address the performance isolation issue with MPS, another NVIDIA technology,
    Multi-Instance GPU (MIG) [[241](#bib.bib241)], enables the partitioning of a GPU
    into multiple discrete instances, each with dedicated resources. However, MIG
    is exclusively available to only a certain specifications of GPUs (e.g., A100
    and H100) and primarily supports resource sharing at a coarse-grained level.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: GPU共享技术通过虚拟化利用部分资源，提供了一种可行的策略来缓解大规模分布式深度学习中的低GPU利用率问题。NVIDIA被公认为领先的GPU供应商，其推出的多进程共享（MPS）[[240](#bib.bib240)]提供了一种操作系统级虚拟化解决方案。然而，其实施需要特定应用的专业知识，以定义资源限制以确保性能隔离。此外，MPS与多种深度学习框架不兼容。为了解决MPS的性能隔离问题，另一项NVIDIA技术——多实例GPU（MIG）[[241](#bib.bib241)]，允许将GPU划分为多个独立的实例，每个实例具有专用资源。然而，MIG仅适用于某些规格的GPU（例如A100和H100），并主要支持粗粒度的资源共享。
- en: 'Some comprehensive solutions have been proposed to tackle complex characteristics
    of production clusters for large-scale distributed DL. Gandiva [[206](#bib.bib206)]
    leverages the profiles of distributed DL tasks and addresses the issue of GPU
    underutilization in three key ways. Initially, Gandiva allows incoming jobs to
    time-share GPUs with existing jobs when overloaded. Then, it permits time-sliced
    jobs to migrate to other GPUs. Lastly, it supports elastic GPU capacity, increasing
    the number of GPUs during idle times and reducing the number of GPUs as the load
    grows dynamically, thereby utilizing idle GPUs effectively. The performance of
    Gandiva is demonstrated on production clusters at Microsoft. AntMan [[207](#bib.bib207)]
    is a production solution for distributed DL clusters at Alibaba. It analyzes the
    cause of GPU underutilization in distributed DL clusters for production use in
    three aspects: hardware, cluster scheduling, and job behavior. Exploiting the
    profiles of fluctuating resource demands from distributed training jobs, AntMan
    co-designs the cluster scheduler and distributed DL framework with dynamic scaling
    mechanisms for GPU resources during job execution. This approach ensures jobs’
    service-level objectives (SLOs) in large-scale clusters while enhancing cluster
    utilization through opportunistic scheduling. Leveraging the analysis of the production
    trace at Alibaba, Fragmentation Gradient Descent (FGD) [[208](#bib.bib208)] addresses
    severe GPU fragmentation in large clusters. FGD minimizes GPU fragmentation growth
    through task packing to achieve maximum GPU allocation rates. TGS [[209](#bib.bib209)]
    provides transparent GPU sharing at OS layer for distributed DL tasks in production
    clusters of containers. TGS addresses challenges of the lack of application profiling
    knowledge and the potential oversubscription of GPU memory during the sharing
    of GPU resources. TGS tackles the first challenge by monitoring and controlling
    the rate of sending GPU kernels to the GPU for each container adaptively, aiming
    to maximize the rate of opportunistic jobs while not affecting that of production
    jobs. TGS tackles the second challenge by unifying GPU memory and host memory
    in a single address space via CUDA unified-memory allocation [[242](#bib.bib242)]
    that enables both performance isolation and transparency of GPU memory allocation.
    Oversubscribed memory of opportunistic jobs is evicted to the host memory automatically,
    ensuring the performance of production jobs.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出一些全面的解决方案来应对大规模分布式深度学习生产集群的复杂特征。**Gandiva** [[206](#bib.bib206)] 利用分布式深度学习任务的配置文件，并通过三种主要方式解决GPU利用不足的问题。最初，**Gandiva**
    允许在超负荷时，新进任务与现有任务共享GPU的时间。然后，它允许时间切片的任务迁移到其他GPU。最后，它支持弹性GPU容量，在空闲时增加GPU数量，并在负载动态增长时减少GPU数量，从而有效利用空闲GPU。**Gandiva**
    的性能在微软的生产集群上得到了验证。**AntMan** [[207](#bib.bib207)] 是阿里巴巴用于分布式深度学习集群的生产解决方案。它从硬件、集群调度和任务行为三个方面分析了分布式深度学习集群中GPU利用不足的原因。**AntMan**
    通过利用分布式训练任务的资源需求波动配置，结合动态扩展机制，对集群调度器和分布式深度学习框架进行共同设计，在任务执行过程中对GPU资源进行动态扩展。这种方法在大规模集群中确保了任务的服务级别目标（SLOs），同时通过机会性调度提升集群利用率。利用阿里巴巴生产跟踪的分析，**Fragmentation
    Gradient Descent (FGD)** [[208](#bib.bib208)] 解决了大集群中严重的GPU碎片化问题。**FGD** 通过任务打包来最小化GPU碎片化的增长，以实现最大化的GPU分配率。**TGS** [[209](#bib.bib209)]
    在容器的生产集群中为分布式深度学习任务提供了透明的GPU共享。**TGS** 解决了应用程序配置知识缺乏和在共享GPU资源时潜在的GPU内存过度分配的问题。**TGS**
    通过自适应监控和控制每个容器向GPU发送GPU内核的速率，来应对第一个挑战，旨在最大化机会性任务的速率，同时不影响生产任务的速率。**TGS** 通过CUDA统一内存分配 [[242](#bib.bib242)]
    将GPU内存和主机内存统一在一个地址空间中，解决了第二个挑战，这种方法实现了性能隔离和GPU内存分配的透明性。机会性任务的过度分配内存会自动迁移到主机内存中，从而确保生产任务的性能。
- en: 'Some works use fast-context switching to improve GPU utilization. Salus [[210](#bib.bib210)]
    achieves fine-grained GPU sharing with flexible scheduling policies, exposing
    two GPU-sharing primitives: fast job switching and memory sharing. The former
    enables rapid preemption and efficient time sharing for the currently active DL
    job on a GPU, whereas the latter packs smaller distributed DL tasks on the same
    device to ensure high memory utilization and prevent memory fragmentation. In
    contrast, PipeSwitch [[211](#bib.bib211)] supports fast-context switching for
    task pipelines of distributed DL jobs. PipeSwitch optimizes the context switching
    overhead through model-aware grouping for pipelines and proactive allocating of
    GPU memory. The model-aware grouping of layers aims to minimize the overhead of
    transferring the model between CPUs and GPUs during context switching. The proactive
    allocation of GPU memory for standby workers before it should be active expedites
    the speed of context switching. To prevent job interference, PipeSwitch enforces
    process-level isolation, by initialing a new separate process for each active-worker
    task.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究使用快速上下文切换来改善GPU利用率。**Salus** [[210](#bib.bib210)] 通过灵活的调度策略实现了精细粒度的GPU共享，暴露了两种GPU共享原语：快速作业切换和内存共享。前者实现了对当前活跃深度学习作业的快速抢占和高效时间共享，而后者则将较小的分布式深度学习任务打包在同一设备上，以确保高内存利用率并防止内存碎片化。相比之下，**PipeSwitch** [[211](#bib.bib211)]
    支持分布式深度学习作业的任务管道的快速上下文切换。**PipeSwitch** 通过模型感知分组来优化上下文切换开销，并主动分配GPU内存。模型感知分组的目标是最小化在上下文切换期间将模型在CPU和GPU之间传输的开销。对待激活工作者的GPU内存的主动分配加快了上下文切换的速度。为防止作业干扰，**PipeSwitch**
    强制进行进程级隔离，为每个活跃的工作任务初始化一个新的单独进程。
- en: Some works employ performance-estimate-guided approaches to enhance GPU-resource
    allocation. Both the performance-estimation methods and the performance goals
    can vary in these approaches. Optimus [[212](#bib.bib212)] introduces a dynamic
    allocation algorithm based on marginal gains, estimating the remaining execution
    time of a distributed DL task. In this greedy policy, a job’s larger marginal
    gain results in a higher allocation of resources. Harmony [[213](#bib.bib213)]
    uses a deep reinforcement learning algorithm to place distributed DL jobs on allocated
    GPU resources with minimum interference and minimum training time. The learning
    rewards for unseen placements are guided by historical allocation samples. Horus [[214](#bib.bib214)]
    builds a model to predict GPU utilization of heterogeneous distributed DL tasks
    from the computation graph features. It identifies GPU utilization as a general
    proxy metric for making optimal placement decisions.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究采用性能估计引导的方法来增强GPU资源分配。这些方法中的性能估计方法和性能目标可能会有所不同。**Optimus** [[212](#bib.bib212)]
    引入了一种基于边际收益的动态分配算法，估算分布式深度学习任务的剩余执行时间。在这种贪婪策略中，作业的边际收益越大，资源分配就越高。**Harmony** [[213](#bib.bib213)]
    使用深度强化学习算法，将分布式深度学习作业放置在分配的GPU资源上，以实现最小干扰和最短训练时间。对未见过的放置的学习奖励由历史分配样本引导。**Horus** [[214](#bib.bib214)]
    构建了一个模型，以预测异构分布式深度学习任务的GPU利用率，基于计算图特征。它将GPU利用率确定为做出最佳放置决策的通用代理指标。
- en: Elastic training, which involves extending and shrinking resource capacity dynamically,
    is an important strategy to improve resource utilization and save costs for distributed
    DL in the cloud environment. Some studies focus on elastic resources at the granularity
    of GPU memory. Pollux [[215](#bib.bib215)] adjusts the GPU resources available
    to distributed DL jobs dynamically, aiming to maximize the overall training goodput
    within the cluster. To improve the efficiency of GPU memory sharing, Zico [[216](#bib.bib216)]
    monitors the memory-usage patterns of individual distributed DL jobs by tracking
    computational progress during training. Based on this monitoring, Zico allocates
    and deallocates memory among concurrent jobs automatically, ensuring no exceeding
    of the memory budget. AFS [[217](#bib.bib217)] points out that handling future
    jobs requires proactive preparation of resources based on current share calculations.
    When the resource scheduler estimates that the resource contention will be heavy
    in the future, it allocates more resources to long-lasting jobs; otherwise it
    allocates more resources to short jobs. In contrast, some studies focus on elastic-container
    resources. For instance, FlowCon [[218](#bib.bib218)] introduces a container-placement
    strategy based on growth efficiency and dynamic resource configuration for elastic
    allocation and withdrawal of resources during runtime.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性训练，即动态扩展和缩减资源容量，是提高资源利用率和降低云环境下分布式深度学习成本的重要策略。一些研究集中在GPU内存粒度的弹性资源上。Pollux [[215](#bib.bib215)]
    动态调整分布式深度学习作业可用的GPU资源，旨在最大化集群内的整体训练良品率。为了提高GPU内存共享的效率，Zico [[216](#bib.bib216)]
    通过跟踪训练过程中的计算进度来监控单个分布式深度学习作业的内存使用模式。基于这种监控，Zico 自动分配和回收内存，确保不超出内存预算。AFS [[217](#bib.bib217)]
    指出，处理未来作业需要基于当前的共享计算主动准备资源。当资源调度器预计未来资源争夺将很激烈时，它会为长期作业分配更多资源；否则，为短期作业分配更多资源。相比之下，一些研究集中在弹性容器资源上。例如，FlowCon [[218](#bib.bib218)]
    引入了一种基于增长效率和动态资源配置的容器放置策略，用于在运行时弹性分配和撤回资源。
- en: Several studies explore strategies for improving resource utilization during
    hyperparameter tuning in distributed DL clusters. Fluid [[219](#bib.bib219)] is
    a distributed DL hyperparameter tuning execution engine that abstracts the hyperparameter
    tuning process as a sequence of trial groups. It employs a water-filling approach
    to expedite the hyperparameter tuning process, thereby enhancing resource utilization.
    Titan [[220](#bib.bib220)] adopts a different approach by consolidating multiple
    fine-tuning workloads into one, aiming to improve resource utilization. This consolidation
    is particularly advantageous given that multiple fine-tuning workloads often share
    the same model parameters. DISC [[221](#bib.bib221)] leverages adaptive scaling
    to adjust the size of GPU time slices occupied by hyperparameter-tuning jobs at
    runtime. This dynamic allocation of GPU time slices for each hyperparameter tuning
    job is based on its potential to create a steep increase in the inference accuracy.
    Hydro [[222](#bib.bib222)] addresses cluster-wide resource utilization and tuning
    efficiency by incorporating a heterogeneity-aware allocation strategy. This method
    extends the resources of hyperparameter-tuning workloads by interleaving them
    with pipeline-enabled large-model training tasks. By effectively utilizing idle
    time intervals on each node, caused by the gaps between the forward and backward
    processing of microbatches, Hydro enhances overall resource utilization and tuning
    efficiency in large-scale distributed DL clusters.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了在分布式深度学习集群中改进资源利用率的策略。Fluid [[219](#bib.bib219)] 是一个分布式深度学习超参数调优执行引擎，它将超参数调优过程抽象为一系列试验组。它采用水填充方法加快超参数调优过程，从而提高资源利用率。Titan [[220](#bib.bib220)]
    采用了不同的方法，将多个微调工作负载合并为一个，以期提高资源利用率。由于多个微调工作负载通常共享相同的模型参数，这种合并特别有利。DISC [[221](#bib.bib221)]
    利用自适应缩放在运行时调整超参数调优作业所占的GPU时间片的大小。这种对每个超参数调优作业的GPU时间片的动态分配是基于其在推断准确性上可能带来的急剧提升。Hydro [[222](#bib.bib222)]
    通过结合异质感知分配策略解决了集群范围内的资源利用和调优效率问题。这种方法通过将超参数调优工作负载与启用管道的大模型训练任务交错，从而扩展了超参数调优工作负载的资源。通过有效利用每个节点上由于微批次的前向和后向处理之间的间隔所产生的空闲时间，Hydro
    提高了大规模分布式深度学习集群的整体资源利用率和调优效率。
- en: '$\bullet$ Network bandwidth sharing: In large-scale distributed environments,
    network bandwidth is another significant factor determining the efficiency of
    distributed training. The focus of network bandwidth sharing can be in various
    granularities, including jobs and tasks, gradient blocks, and network coflows.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 网络带宽共享：在大规模分布式环境中，网络带宽是决定分布式训练效率的另一个重要因素。网络带宽共享的重点可以在不同的粒度上，包括作业和任务、梯度块和网络共同流。
- en: Some work focuses on optimizing network bandwidth sharing in the granularity
    of jobs and tasks. Liquid [[223](#bib.bib223)] proposes a computational and communication-resource-estimation
    algorithm and a network-efficient job-placement strategy for distributed training
    jobs. The resource-estimation algorithm models resource requirements of distributed
    training jobs, including GPU computing power, GPU memory, and network-bandwidth
    requirements. The job-placement strategy assigns distributed training jobs to
    a cluster of computing nodes and containers, finding a best-fit job placement
    solution that satisfies the estimated computational and communication-resource
    requirements and exhibits less GPU fragmentation and network communication cost
    across containers.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究专注于以作业和任务的粒度优化网络带宽共享。Liquid [[223](#bib.bib223)] 提出了一个计算和通信资源估计算法以及一个网络高效的作业调度策略用于分布式训练作业。资源估计算法建模了分布式训练作业的资源需求，包括
    GPU 计算能力、GPU 内存和网络带宽需求。作业调度策略将分布式训练作业分配到计算节点和容器的集群中，找到满足估计的计算和通信资源需求的最佳作业放置解决方案，并在容器间展现较少的
    GPU 片段和网络通信成本。
- en: Some work focuses on network bandwidth sharing in the granularity of gradient
    blocks. For instance, Prophet [[224](#bib.bib224)] groups into certain gradient
    blocks based on the profiled time interval and models the distributed training
    time in terms of the network bandwidth and order of network transfers of gradient
    blocks. Based on this model, Prophet searches for an optimal order of the network
    transfers of gradient blocks, aiming to minimize the distributed training time.
    This optimal order of gradient block transfers optimizes both the network bandwidth
    sharing among gradient blocks and the overlapping between network transfers and
    GPU computation.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究专注于以梯度块（gradient blocks）的粒度进行网络带宽共享。例如，Prophet [[224](#bib.bib224)] 基于分析时间间隔将梯度块分组，并以网络带宽和梯度块的网络传输顺序建模分布式训练时间。基于此模型，Prophet
    寻找梯度块网络传输的最佳顺序，旨在最小化分布式训练时间。这种梯度块传输的最佳顺序优化了梯度块之间的网络带宽共享以及网络传输与 GPU 计算之间的重叠。
- en: Some work focuses on network bandwidth sharing in the granularity of coflows.
    For instance, Parrot [[225](#bib.bib225)] perceives the communication pattern
    of a distributed training job as a series of dependent coflows and estimates the
    remaining processing time of distributed training jobs based on the amount of
    information carried per coflow. Parrot allocates network bandwidth to active coflows
    of concurrent jobs within the cluster, so that the effective completion time of
    coflows of the job with a shorter remaining processing time has a higher priority
    to be minimized.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究专注于以共同流（coflows）的粒度进行网络带宽共享。例如，Parrot [[225](#bib.bib225)] 将分布式训练作业的通信模式视为一系列依赖的共同流，并根据每个共同流承载的信息量估计分布式训练作业的剩余处理时间。Parrot
    为集群内并发作业的活跃共同流分配网络带宽，从而优先最小化剩余处理时间较短的作业的共同流的有效完成时间。
- en: V-A2 Resource management for distributed inference
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 分布式推断的资源管理
- en: 'In contrast to distributed training that caters to long-term offline workloads,
    distributed inference necessitates real-time execution with more stringent requirements
    on latency and accuracy. This difference in demands necessitates the development
    of novel resource management solutions to address the distinct characteristics
    of inference workloads effectively. In the distributed inference process, GPU
    sharing is the focus of various resource-allocation approaches that can be divided
    into two major categories: spatial sharing and temporal sharing. In the context
    of multiple distributed DL jobs, the spatial sharing of GPUs involves the sharing
    of GPU space partitions while the temporal sharing involves the sharing of computation
    time slices of an entire GPU. Hybrid approaches combine techniques from these
    two categories.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 与适用于长期离线工作负载的分布式训练相比，分布式推断需要实时执行，并对延迟和准确性有更严格的要求。这种需求差异要求开发新的资源管理解决方案，以有效应对推断工作负载的不同特性。在分布式推断过程中，GPU共享是各种资源分配方法的重点，这些方法可以分为两个主要类别：空间共享和时间共享。在多个分布式深度学习作业的背景下，GPU的空间共享涉及GPU空间分区的共享，而时间共享涉及整个GPU计算时间片的共享。混合方法结合了这两类技术。
- en: '$\bullet$ Spatial Sharing: Many existing works exploit spatial sharing of GPUs
    to optimize the performance of distributed inference tasks. GSLICE [[226](#bib.bib226)]
    introduces an inference system that achieves safe and efficient GPU sharing through
    spatial GPU multiplexing systematically. It utilizes MPS [[240](#bib.bib240)],
    a GPU spatial-multiplexing framework with virtualization, to handle various inference
    requests. iGniter [[227](#bib.bib227)] employs an inference performance model
    to calculate an appropriate batch size and the lower bound of allocated GPU resources.
    Subsequently, it allocates GPU resources for each inference workload by employing
    a greedy approach to identify the placement GPU devices that can achieve minimal
    performance interference. The SLO-aware ML Inference Framework [[228](#bib.bib228)]
    designs a resource auto-scaling strategy in the cloud by leveraging rich and precise
    workload-specific metrics, with a special consideration of the heterogeneity in
    the GPU computational capability. This effective and elastic management of resources
    ensures meeting the SLO for diverse inference workloads in the cloud.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 空间共享：许多现有工作利用GPU的空间共享来优化分布式推断任务的性能。GSLICE [[226](#bib.bib226)] 引入了一个推断系统，通过系统地进行空间GPU复用，实现了安全高效的GPU共享。它利用MPS [[240](#bib.bib240)]，一个具有虚拟化功能的GPU空间复用框架，来处理各种推断请求。iGniter
    [[227](#bib.bib227)] 采用了一个推断性能模型来计算合适的批量大小和分配的GPU资源的下界。随后，它通过贪婪方法分配每个推断工作负载的GPU资源，以确定可以实现最小性能干扰的放置GPU设备。SLO-aware
    ML推断框架 [[228](#bib.bib228)] 在云中设计了一种资源自动扩展策略，利用丰富且精确的工作负载特定指标，特别考虑了GPU计算能力的异质性。这种有效且弹性的资源管理确保了在云中满足不同推断工作负载的SLO。
- en: '$\bullet$ Temporal Sharing: Recent temporal-sharing approaches designed for
    specific distributed inference systems have shown improvements in GPU utilization,
    especially in cloud environments shared by numerous tenants. Nexus [[229](#bib.bib229)]
    employs a heuristic approach to select requests for co-location on the same GPU.
    Initially, it determines the most suitable batch size to meet throughput and SLO
    requirements for the existing inference workloads. Subsequently, Nexus identifies
    all possible combinations within a GPU’s duty cycle on a single GPU in a best-fit
    manner, maximizing utilization without violating latency requirements. Focusing
    on inference services in the cloud, INFaaS [[230](#bib.bib230)] addresses the
    problem of co-location interference arising from shared hardware resources. It
    allocates available resources to interfered instances through workload migration
    or virtual-machine-level scaling, aiming to reduce monetary costs through GPU
    sharing while meeting latency requirements via virtual-machine-level scaling.
    Cocktail [[231](#bib.bib231)] scales the virtual machine resources for various
    inference models in the cloud automatically and proactively based on the predicted
    workload and popularity of these models. This approach enhances the efficiency
    of resource allocation in distributed DL inference systems with a specific set
    of supported inference models.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 时间共享：最近为特定分布式推理系统设计的时间共享方法在GPU利用率方面取得了改进，特别是在多个租户共享的云环境中。Nexus [[229](#bib.bib229)]
    采用启发式方法选择请求以在同一GPU上共同放置。最初，它确定最适合的批量大小以满足现有推理工作负载的吞吐量和SLO要求。随后，Nexus 以最佳适配方式识别单个GPU的所有可能组合，以最大化利用率而不违反延迟要求。关注于云中的推理服务，INFaaS [[230](#bib.bib230)]
    解决了因共享硬件资源而产生的共存干扰问题。它通过工作负载迁移或虚拟机级别的扩展，将可用资源分配给受到干扰的实例，旨在通过GPU共享降低成本，同时通过虚拟机级别的扩展满足延迟要求。Cocktail [[231](#bib.bib231)]
    根据预测的工作负载和这些模型的流行程度，自动并主动地扩展云中各种推理模型的虚拟机资源。这种方法提高了分布式深度学习推理系统中资源分配的效率，支持特定的推理模型集。
- en: '$\bullet$ Hybrid Sharing: Several works study the hybrid GPU-sharing approaches,
    considering both spatial and temporal sharing. Gpulet [[232](#bib.bib232)] supports
    spatial sharing of GPUs via the abstraction of virtual GPUs that are split partitions
    derived from physical GPUs. Given allocated virtual GPU resources, Gpulet supports
    temporal sharing by scheduling the batch sizes of inference jobs of multiple tenants,
    with a goal to guarantee the SLO. This hybrid design enables cost-effective cloud-resource
    allocation for the inference of numerous heterogeneous DL models. FaST-GShare [[233](#bib.bib233)]
    utilizes spatial and temporal sharing of GPUs to maximize inference function throughput
    in the Function-as-a-Service serverless architecture for distributed DL. It supports
    auto-scaling of inference resources in the cloud based on the profiling of function
    throughput and resource allocation, maximizing GPU utilization while ensuring
    the SLO.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 混合共享：一些研究探讨了混合GPU共享方法，考虑了空间和时间共享。Gpulet [[232](#bib.bib232)] 通过将物理GPU分割成虚拟GPU的抽象，支持GPU的空间共享。给定分配的虚拟GPU资源，Gpulet
    通过调度多个租户的推理作业的批量大小来支持时间共享，旨在保证SLO。此混合设计使得对多个异构深度学习模型的推理进行成本效益的云资源分配成为可能。FaST-GShare [[233](#bib.bib233)]
    利用GPU的空间和时间共享，以最大化在Function-as-a-Service无服务器架构中的推理功能吞吐量。它基于对功能吞吐量和资源分配的分析，支持云中推理资源的自动扩展，最大化GPU利用率，同时确保SLO。
- en: 'TABLE VIII: Studies on Task-Scheduling Strategies for Large-scale Distributed
    DL'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：大规模分布式深度学习任务调度策略研究
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|            类别 | 技术及参考文献 | 年份 | 亮点 |'
- en: '|   Training Scheduling ([V-B](#S5.SS2 "V-B Training-Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Efficiency: Job-level Scheduling
    ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency ‣ V-B Training-Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Amaral et al. [[243](#bib.bib243)]
    | 2017 | Proposing a new topology-aware workload-placement strategy to schedule
    DL jobs on multi-GPU systems. |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|   训练调度 ([V-B](#S5.SS2 "V-B Training-Task Scheduling ‣ V Large-Scale Resource
    Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | 效率：任务级调度 ([V-B1](#S5.SS2.SSS1 "V-B1
    Efficiency ‣ V-B Training-Task Scheduling ‣ V Large-Scale Resource Allocation
    and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Amaral et al. [[243](#bib.bib243)] | 2017 | 提出了一种新的拓扑感知负载放置策略，用于在多GPU系统上调度深度学习（DL）任务。
    |'
- en: '| Tiresias [[244](#bib.bib244)] | 2019 | Using LAS algorithm to prioritize
    jobs based on their service, a metric defined as the multiplication of requested
    GPU resources and execution time. |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| Tiresias [[244](#bib.bib244)] | 2019 | 使用LAS算法根据服务优先级来调度任务，服务被定义为请求的GPU资源与执行时间的乘积。
    |'
- en: '|  | FfDL [[245](#bib.bib245)] | 2019 | Using the operating lessons from the
    industry practice to guide the balance dependability with scalability, elasticity,
    flexibility, and efficiency. |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | FfDL [[245](#bib.bib245)] | 2019 | 利用行业实践中的操作经验来平衡可靠性、可扩展性、弹性和效率。 |'
- en: '|  | Philly [[246](#bib.bib246)] | 2019 | Correlating scheduler logs with logs
    from individual jobs and conducting a thorough analysis about the impact of gang
    scheduling and locality constraints on the queuing delay and job runtime. |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | Philly [[246](#bib.bib246)] | 2019 | 将调度器日志与单个任务的日志相关联，并对帮调度和地方性约束对排队延迟和任务运行时间的影响进行深入分析。
    |'
- en: '|  | E-LAS [[247](#bib.bib247)] | 2020 | Using real-time epoch progress rates
    specific to distributed training jobs, as well as services obtained from the temporal
    and spatial domains, to guide scheduling decisions |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | E-LAS [[247](#bib.bib247)] | 2020 | 使用针对分布式训练任务的实时纪元进度率以及从时域和空间域获取的服务，来指导调度决策。
    |'
- en: '|  | SMD [[248](#bib.bib248)] | 2021 | Allowing multiple jobs to compete for
    the communication bandwidth |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | SMD [[248](#bib.bib248)] | 2021 | 允许多个任务竞争通信带宽。 |'
- en: '|  | OSDL [[249](#bib.bib249)] | 2022 | Designing job-placement and scheduling
    algorithms in networks that involve both OCS and EPS. |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  | OSDL [[249](#bib.bib249)] | 2022 | 设计涉及OCS和EPS的网络中的任务放置和调度算法。 |'
- en: '|  | Sched² [[250](#bib.bib250)] | 2019 | Using DRL to perform smart locality-aware
    scheduling of DLT jobs. |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | Sched² [[250](#bib.bib250)] | 2019 | 使用深度强化学习（DRL）来执行智能的地方性感知调度以处理分布式学习任务（DLT）。
    |'
- en: '|  | MLFS [[251](#bib.bib251)] | 2020 | Leveraging the data from the heuristic
    scheduling method for training a DRL model and making decisions on job scheduling
    using this trained DRL model automatically. |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | MLFS [[251](#bib.bib251)] | 2020 | 利用启发式调度方法中的数据来训练DRL模型，并使用该训练好的DRL模型自动进行任务调度决策。
    |'
- en: '| Efficiency: Task-pipeline-level Scheduling ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency
    ‣ V-B Training-Task Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | PipeDream [[252](#bib.bib252)] | 2019 | Partitioning DNN layers among
    workers automatically to balance workload and reduce communication. |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| 效率：任务流水线级调度 ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency ‣ V-B Training-Task Scheduling
    ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | PipeDream [[252](#bib.bib252)]
    | 2019 | 自动将DNN层在工作节点之间分配，以平衡负载并减少通信。 |'
- en: '| GPipe [[253](#bib.bib253)] | 2019 | Employs an innovative batch-splitting
    pipelining algorithm, achieving nearly linear convergence speedups when a model
    is distributed across multiple accelerators. |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| GPipe [[253](#bib.bib253)] | 2019 | 采用创新的批量拆分流水线算法，当模型在多个加速器上分布时，能实现接近线性的收敛加速。
    |'
- en: '|  | Piper [[66](#bib.bib66)] | 2021 | Leveraging tensor parallelization techniques
    within a single layer to reduce the search space significantly. |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | Piper [[66](#bib.bib66)] | 2021 | 在单层内部利用张量并行化技术显著减少搜索空间。 |'
- en: '|  | Chimera [[63](#bib.bib63)] | 2021 | Integrating bidirectional pipelines
    for the efficient training of large-scale models, striking an optimal balance
    among pipeline efficiency, memory cost, and convergence friendliness. |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  | Chimera [[63](#bib.bib63)] | 2021 | 整合双向流水线以高效训练大规模模型，在流水线效率、内存成本和收敛友好性之间找到最佳平衡。
    |'
- en: '|  | AutoPipe [[64](#bib.bib64)] | 2022 | Introducing a method for achieving
    balanced partitioning and reducing startup overhead automatically. |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | AutoPipe [[64](#bib.bib64)] | 2022 | 引入一种自动实现平衡分区和减少启动开销的方法。 |'
- en: '|  | OOO BackProp [[65](#bib.bib65)] | 2022 | Leveraging gradient computation
    dependencies to reorder pipeline executions, which maximize GPU resource utilization.
    |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | OOO BackProp [[65](#bib.bib65)] | 2022 | 利用梯度计算依赖关系重新排序管道执行，从而最大化GPU资源利用率。
    |'
- en: '|  | DeAR [[254](#bib.bib254)] | 2023 | Addressing the issues of startup latency
    and sub-optimal training performance. |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | DeAR [[254](#bib.bib254)] | 2023 | 解决启动延迟和次优训练性能的问题。 |'
- en: '|  | HetPipe [[255](#bib.bib255)] | 2020 | Presenting a DNN training system
    that integrates PMP with DP. |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | HetPipe [[255](#bib.bib255)] | 2020 | 提出了一种将PMP与DP集成的DNN训练系统。 |'
- en: '| Efficiency: Network-flow-level Scheduling ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency
    ‣ V-B Training-Task Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | JPAS [[256](#bib.bib256)] | 2020 | Using a simple greedy mechanism
    to order all DDL jobs periodically. |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 效率：网络流级调度 ([V-B1](#S5.SS2.SSS1 "V-B1 效率 ‣ V-B 训练任务调度 ‣ V 大规模资源分配和任务调度 ‣ 通信高效的大规模分布式深度学习：全面综述"))
    | JPAS [[256](#bib.bib256)] | 2020 | 使用简单的贪心机制来周期性地排序所有DDL作业。 |'
- en: '| Geryon [[257](#bib.bib257)] | 2020 | Employing multiple flows with varying
    priorities to transfer parameters of different urgency levels. |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| Geryon [[257](#bib.bib257)] | 2020 | 使用具有不同优先级的多个流来传输不同紧急级别的参数。 |'
- en: '|  | TensorExpress [[258](#bib.bib258)] | 2020 | Enables each switch to transmit
    tensor packets according to their priority using multiple queues. |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | TensorExpress [[258](#bib.bib258)] | 2020 | 使每个交换机能够根据优先级通过多个队列传输张量包。
    |'
- en: '|  | Beamer [[259](#bib.bib259)] | 2021 | Reducing the SCT by considering stage
    information in its scheduling approach. |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '|  | Beamer [[259](#bib.bib259)] | 2021 | 通过在调度方法中考虑阶段信息来减少SCT。 |'
- en: '|  | Tereis [[260](#bib.bib260)] | 2023 | Exploring the utilization of idle
    GPU computational resources during data transmission periods. |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | Tereis [[260](#bib.bib260)] | 2023 | 探索在数据传输期间利用闲置GPU计算资源。 |'
- en: '|  | Mercury [[261](#bib.bib261)] | 2023 | Working on packet granularity to
    shift priority scheduling to the transport layer. |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | Mercury [[261](#bib.bib261)] | 2023 | 处理数据包粒度问题，将优先级调度转移到传输层。 |'
- en: '| Cost-Effective ([V-B2](#S5.SS2.SSS2 "V-B2 Cost-Effective ‣ V-B Training-Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Cynthia [[262](#bib.bib262)]
    | 2019 | Providing predictable distributed training performance and reducing the
    training budget. |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 成本效益 ([V-B2](#S5.SS2.SSS2 "V-B2 成本效益 ‣ V-B 训练任务调度 ‣ V 大规模资源分配和任务调度 ‣ 通信高效的大规模分布式深度学习：全面综述"))
    | Cynthia [[262](#bib.bib262)] | 2019 | 提供可预测的分布式训练性能，并减少训练预算。 |'
- en: '| FC² [[263](#bib.bib263)] | 2019 | A scheduler that recommends cost-effective
    cloud-resource allocations for distributed training tasks with a PS. |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| FC² [[263](#bib.bib263)] | 2019 | 一种调度器，为具有PS的分布式训练任务推荐成本效益高的云资源分配。 |'
- en: '|  | Jahani [[264](#bib.bib264)] | 2019 | Modeling the scheduling process as
    a MILP problem to reduce the leasing cost in a global manner while maintaining
    the job latency. |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  | Jahani [[264](#bib.bib264)] | 2019 | 将调度过程建模为MILP问题，以全球性地降低租赁成本，同时保持作业延迟。
    |'
- en: '|  | GPOEO [[265](#bib.bib265)] | 2022 | Saving power in GPU data centers and
    using a customized scheduler to orchestrate jobs. |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | GPOEO [[265](#bib.bib265)] | 2022 | 在GPU数据中心节省电力，并使用定制的调度器来协调作业。 |'
- en: '|  | STS [[266](#bib.bib266)] | 2023 | Exploiting the probability distribution
    of early termination and adapting the resource assignment during the execution
    of the jobs to minimize the expected energy cost |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | STS [[266](#bib.bib266)] | 2023 | 利用早期终止的概率分布，并在作业执行过程中调整资源分配，以最小化预期能源成本。
    |'
- en: '| Deadline Guarantee ([V-B3](#S5.SS2.SSS3 "V-B3 Deadline Guarantee ‣ V-B Training-Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | GENIE [[267](#bib.bib267)]
    | 2020 | Proposing a prediction model derived from lightweight profiling to estimate
    the processing rate and response latency for diverse DL workloads. |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| 截止时间保证 ([V-B3](#S5.SS2.SSS3 "V-B3 Deadline Guarantee ‣ V-B Training-Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | GENIE [[267](#bib.bib267)]
    | 2020 | 提出一种基于轻量级分析的预测模型，以估计各种DL工作负载的处理速率和响应延迟。 |'
- en: '| Chronus [[268](#bib.bib268)] | 2021 | Providing deadline guarantee for SLO
    jobs and maximizing the performance of best-effort jobs. |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| Chronus [[268](#bib.bib268)] | 2021 | 为SLO作业提供截止时间保证，并最大化最佳努力作业的性能。 |'
- en: '|  | Hydra [[269](#bib.bib269)] | 2023 | Adopting a sampling approach that
    exploits the inherent iterative periodicity of DL jobs to estimate job completion
    times accurately on heterogeneous GPUs. |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  | Hydra [[269](#bib.bib269)] | 2023 | 采用一种采样方法，利用DL作业固有的迭代周期性，以在异构GPU上准确估计作业完成时间。'
- en: '|   Inference Scheduling ([V-C](#S5.SS3 "V-C Inference Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Efficiency ([V-C1](#S5.SS3.SSS1
    "V-C1 Efficiency ‣ V-C Inference Scheduling ‣ V Large-Scale Resource Allocation
    and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Sniper [[270](#bib.bib270)] | 2022 | Using non-invasive
    performance characterization network based on neural network similarity (NNS)
    to predict the inference time of DNNs accurately. |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|   推理调度 ([V-C](#S5.SS3 "V-C Inference Scheduling ‣ V Large-Scale Resource
    Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | 效率 ([V-C1](#S5.SS3.SSS1 "V-C1 Efficiency
    ‣ V-C Inference Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Sniper [[270](#bib.bib270)] | 2022 | 使用基于神经网络相似度 (NNS) 的非侵入性性能表征网络来准确预测DNN的推理时间。
    |'
- en: '| AutoDeep [[271](#bib.bib271)] | 2020 | Leveraging Bayesian Optimization and
    Deep Reinforcement Learning to unearth the optimal cloud configuration and device
    placement with limited search time adaptively. |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| AutoDeep [[271](#bib.bib271)] | 2020 | 利用贝叶斯优化和深度强化学习在有限的搜索时间内自适应地发现最佳的云配置和设备位置。
    |'
- en: '|  | Clipper [[272](#bib.bib272)] | 2017 | Observing the corresponding accuracy
    and latency feedback to make a choice to use a best-effort search approach. |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | Clipper [[272](#bib.bib272)] | 2017 | 观察相应的准确性和延迟反馈，以选择使用最佳努力搜索方法。 |'
- en: '| Throughout capability ([V-C2](#S5.SS3.SSS2 "V-C2 Throughout capability ‣
    V-C Inference Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Rafiki [[273](#bib.bib273)] | 2018 | Using a practical AIMD algorithm
    to adjust inference batch size. |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐能力 ([V-C2](#S5.SS3.SSS2 "V-C2 Throughout capability ‣ V-C Inference Scheduling
    ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Rafiki [[273](#bib.bib273)]
    | 2018 | 使用实用的AIMD算法来调整推理批处理大小。 |'
- en: '| Nanily [[274](#bib.bib274)] | 2019 | Deriving the corresponding batch size
    so that the inference execution time is equal to or close to the maximum remaining
    time. |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| Nanily [[274](#bib.bib274)] | 2019 | 推导出相应的批处理大小，以使推理执行时间等于或接近最大剩余时间。 |'
- en: '|  | RRL [[275](#bib.bib275)] | 2019 | Focusing on optimizing parallel configurations
    at different levels. |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  | RRL [[275](#bib.bib275)] | 2019 | 专注于优化不同层次的并行配置。 |'
- en: '|  | Morphling [[276](#bib.bib276)] | 2021 | Adapting the meta-model to a new
    inference service by sampling a small number of configurations and using it to
    find the optimal one. |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | Morphling [[276](#bib.bib276)] | 2021 | 通过采样少量配置并使用这些配置找到最佳方案来适应新的推理服务。
    |'
- en: '|   |  |  |  |  |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   |   |   |'
- en: V-B Training-Task Scheduling
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 训练-任务调度
- en: In large-scale GPU clusters with complex network connections, scheduling distributed
    DL workloads effectively is critical for ensuring the high performance of task
    execution, optimal hardware utilization, and achievement of various scheduling
    objectives. Training and inference stages of distributed DL are widely recognized
    as particularly resource-intensive [[239](#bib.bib239)]. The following subsections
    study task-scheduling strategies on the training and inference workloads, respectively,
    and focus on providing efficient communication or overlapping computation and
    communication for overall efficiency in large-scale distributed DL.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有复杂网络连接的大规模 GPU 集群中，有效地调度分布式深度学习工作负载对于确保任务执行的高性能、硬件利用的最优化以及实现各种调度目标至关重要。 分布式深度学习的训练和推理阶段被广泛认为是特别资源密集型的[[239](#bib.bib239)]。
    以下小节分别研究了训练和推理工作负载上的任务调度策略，重点是提供高效的通信或重叠计算和通信，以实现大规模分布式深度学习的整体效率。
- en: 'As the distributed training of a model can consume a lot of computational and
    communication resources, efficient task-scheduling strategies for training workloads
    are crucial. The performance goals of scheduling can include efficiency, cost-effective,
    and deadline guarantees while the granularity levels can be on jobs, task pipelines,
    and network flows. In this section, we survey the task scheduling of large-scale
    distributed training with various performance goals and granularity levels. Table [VIII](#S5.T8
    "TABLE VIII ‣ V-A2 Resource management for distributed inference ‣ V-A Resource
    Management ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") summarizes the
    related literature.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '由于模型的分布式训练可能消耗大量的计算和通信资源，因此高效的任务调度策略对训练工作负载至关重要。调度的性能目标可以包括效率、成本效益和截止日期保证，而粒度级别可以在作业、任务管道和网络流量上。
    在本节中，我们调查了具有各种性能目标和粒度级别的大规模分布式训练的任务调度。表 [VIII](#S5.T8 "TABLE VIII ‣ V-A2 Resource
    management for distributed inference ‣ V-A Resource Management ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") 总结了相关文献。'
- en: V-B1 Efficiency
  id: totrans-520
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 效率
- en: Efficiency is one of the most critical performance goals of the scheduling of
    distributed training [[239](#bib.bib239)]. This subsection categorizes task-scheduling
    strategies based on various scheduling granularity levels, including jobs, task
    pipelines, and network flows.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 效率是分布式训练调度中最关键的性能目标之一[[239](#bib.bib239)]。 本小节根据不同的调度粒度级别对任务调度策略进行了分类，包括作业、任务管道和网络流量。
- en: $\bullet$ Job-level scheduling is one of the most common and effective scheduling
    methods that focus on reordering priorities of jobs [[244](#bib.bib244), [245](#bib.bib245)].
    Amaral et al. [[243](#bib.bib243)] introduce a novel topology-aware workload-placement
    strategy, designed for scheduling distributed DL jobs on multi-GPU systems. This
    strategy satisfies workload requirements efficiently while minimizing interference
    simultaneously. Building upon this foundation, Tiresias [[244](#bib.bib244)],
    drawing inspiration from the classic Multi-Level Feedback Queue (MLFQ) algorithm [[277](#bib.bib277)],
    develops a priority discretization approach to mitigate issues related to frequent
    preemption. A Least Attained Service (LAS) algorithm is conceptualized to prioritize
    jobs based on their service level that is quantified by the product of requested
    GPU resources and execution time. FfDL [[245](#bib.bib245)], an open-source scheduling
    platform developed by IBM, incorporates operational insights from industry practices
    to strike a balance between dependability and scalability, while maintaining elasticity,
    flexibility, and efficiency. In a related study, Philly [[246](#bib.bib246)] performs
    a comprehensive analysis by correlating scheduler logs with logs from individual
    jobs, examining the impact of gang scheduling and locality constraints on queuing
    delay and job runtime. Drawing on insights from this analysis, Philly advocates
    relaxing locality constraints to enhance job-time efficiency. E-LAS [[247](#bib.bib247)],
    with the objective of reducing the average completion time for distributed training
    jobs, shifts the focus away from reliance on job runtime estimates or prior knowledge.
    Instead, it utilizes real-time epoch progress rates specific to distributed training
    jobs, combined with service metrics derived from temporal and spatial domains,
    to inform scheduling decisions. This innovative approach enables E-LAS to surpass
    the efficiency of Tiresias. Additionally, SMD [[248](#bib.bib248)] presents a
    resource-scheduling analytical model that accommodates multiple jobs vying for
    communication bandwidth. This model treats the scheduling problem as a non-convex
    integer non-linear program with bin-packing constraints and introduces an $\epsilon$-approximation
    algorithm, termed the sum-of-retios multi-dimensional-knapsack decomposition,
    for its resolution. OSDL [[249](#bib.bib249)] designs and proposes algorithms
    for job placement and scheduling in networks that involve both optical circuit
    switching (OCS) and electrical packet switching (EPS). Simulation results demonstrate
    that OSDL surpasses multiple well-established scheduling methods in terms of performance.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 作业级调度是最常见且有效的调度方法之一，重点是重新排序作业的优先级[[244](#bib.bib244), [245](#bib.bib245)]。Amaral
    等人[[243](#bib.bib243)]引入了一种新颖的拓扑感知工作负载放置策略，旨在为多 GPU 系统上的分布式深度学习作业进行调度。该策略高效地满足工作负载需求，同时最小化干扰。在此基础上，Tiresias
    [[244](#bib.bib244)] 从经典的多级反馈队列 (MLFQ) 算法[[277](#bib.bib277)]中汲取灵感，开发了一种优先级离散化方法，以缓解频繁抢占相关的问题。概念化的最少服务
    (LAS) 算法根据服务水平（由请求的 GPU 资源和执行时间的乘积量化）来优先处理作业。由 IBM 开发的开源调度平台 FfDL [[245](#bib.bib245)]
    融入了来自行业实践的操作见解，以在可靠性和可扩展性之间取得平衡，同时保持弹性、灵活性和效率。在相关研究中，Philly [[246](#bib.bib246)]
    通过将调度器日志与单个作业的日志进行关联，全面分析了集体调度和位置约束对排队延迟和作业运行时间的影响。根据这项分析的见解，Philly 主张放宽位置约束以提高作业时间效率。E-LAS
    [[247](#bib.bib247)] 的目标是减少分布式训练作业的平均完成时间，转而不依赖于作业运行时间估算或先前知识。相反，它利用特定于分布式训练作业的实时纪元进度率，结合源于时间和空间域的服务指标来指导调度决策。这种创新方法使
    E-LAS 超越了 Tiresias 的效率。此外，SMD [[248](#bib.bib248)] 提出了一个资源调度分析模型，该模型适用于多个作业争夺通信带宽的情况。该模型将调度问题视为一个具有
    bin-packing 约束的非凸整数非线性规划问题，并引入了一种 $\epsilon$-近似算法，称为比率和多维背包分解，用于解决该问题。OSDL [[249](#bib.bib249)]
    设计并提出了用于涉及光路交换 (OCS) 和电路包交换 (EPS) 的网络中的作业放置和调度的算法。仿真结果表明，OSDL 在性能上超越了多种成熟的调度方法。
- en: Several scheduling methods have integrated machine-learning techniques, particularly
    deep reinforcement learning (DRL), to enhance task-scheduling efficiency. Sched² [[250](#bib.bib250)]
    utilizes DRL to schedule distributed training jobs with a locality-aware approach
    intelligently. This method is capable of understanding both the locality sensitivity
    of jobs and the fragmentation condition of clusters comprehensively within the
    entire learning stack. Through this heightened awareness, the DRL model adjusts
    its scheduling decisions dynamically and adaptively, responding effectively to
    the varying locality sensitivities of individual jobs and the evolving state of
    cluster fragmentation. MLFS [[251](#bib.bib251)] employs data from heuristic scheduling
    methods to train a DRL model, subsequently using this model to make informed decisions
    about job scheduling autonomously.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 许多调度方法已经结合了机器学习技术，特别是深度强化学习（DRL），以提高任务调度的效率。 Sched² [[250](#bib.bib250)] 采用
    DRL 以智能的局部感知方法调度分布式训练任务。 这种方法能够全面理解任务的局部敏感性和集群的碎片化情况，贯穿整个学习堆栈。 通过这种增强的感知，DRL 模型动态而自适应地调整其调度决策，有效地响应各个任务的不同局部敏感性以及集群碎片化状态的变化。
    MLFS [[251](#bib.bib251)] 使用启发式调度方法的数据来训练 DRL 模型，随后利用该模型自主做出关于任务调度的明智决策。
- en: $\bullet$ Task-pipeline-level scheduling is specialized in managing the sequential
    processing of tasks within a pipeline architecture, especially for large distributed
    training jobs with large models. It orchestrates the flow of data and tasks across
    various stages or processes expertly in a pre-defined order. By ensuring each
    task is executed efficiently and in adherence to the specified sequence, the scheduler
    optimizes overall pipeline performance. For example, PipeDream [[252](#bib.bib252)]
    introduces a system that incorporates inter-batch pipelining into intra-batch
    parallelism. This approach enhances parallel training throughput by overlapping
    computation with communication and minimizing communication when feasible. Moreover,
    PipeDream partitions DNN layers among workers automatically to balance workload
    and reduce communication. Addressing the need for efficient and task-independent
    model parallelism, GPipe [[253](#bib.bib253)] emerges as a pipeline parallelism
    library. It facilitates scaling of any network describable as a sequence of layers.
    GPipe employs an innovative batch-splitting and pipelining algorithm, achieving
    nearly linear convergence speedups when a model is distributed across multiple
    accelerators. This method offers the flexibility to scale various DNN models to
    immense sizes efficiently. Piper [[66](#bib.bib66)] aims to partition the DNN
    computation graph across numerous accelerators optimally while combining various
    parallelism modes and optimizations. As an efficient optimization algorithm based
    on two-level dynamic programming, Piper leverages tensor parallelization within
    a single layer to reduce the search space significantly. Chimera [[63](#bib.bib63)]
    integrates bidirectional pipelines for efficient training of large-scale models,
    striking an optimal balance among pipeline efficiency, memory cost, and convergence
    friendliness. AutoPipe [[64](#bib.bib64)] introduces a method for achieving balanced
    partitioning and reducing startup overhead automatically. It includes a planner
    for rapid and automated generation of balanced pipeline partition schemes and
    a micro-batch slicer that adjusts micro-batches in line with planner outcomes
    to minimize pipeline startup overhead. Out-Of-Order (OOO) BackProp [[65](#bib.bib65)]
    leverages gradient computation dependencies to reorder pipeline executions, maximizing
    GPU-resource utilization. In data-parallel training, OOO reorders the sequence
    of gradient computations to optimize the overlap between computation and parameter
    communication. In pipeline-parallel training, it prioritizes critical gradient
    computations to minimize pipeline stalls. DeAR [[254](#bib.bib254)] proposes decoupling
    the all-reduce primitive into two continuous operations, enabling both backpropagation
    and feed-forward computations without extra communication. This method addresses
    the issues of startup latency and sub-optimal training performance. For heterogeneous
    pipelines, HetPipe [[255](#bib.bib255)] presents a DNN training system that integrates
    pipelined model parallelism (PMP) with data parallelism (DP). In this system,
    a wave-synchronous-parallel approach is proposed to support both PMP and DP for
    virtual workers.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 任务级流水线调度专注于管理流水线架构中任务的顺序处理，特别是对于大型分布式训练作业和大模型。它在预定义的顺序中，专业地协调数据和任务在各个阶段或过程中的流动。通过确保每个任务高效执行并遵循指定的顺序，调度器优化了整个流水线的性能。例如，PipeDream [[252](#bib.bib252)]
    引入了一个将批间流水线技术融入批内并行性的系统。这种方法通过将计算与通信重叠并在可能时最小化通信，从而提升了并行训练吞吐量。此外，PipeDream 自动将
    DNN 层分配给工作者，以平衡工作负载并减少通信。为了满足高效且任务独立的模型并行需求，GPipe [[253](#bib.bib253)] 作为一个流水线并行库出现。它支持将任何可描述为层序列的网络进行扩展。GPipe
    采用创新的批拆分和流水线算法，当模型分布在多个加速器上时，实现了几乎线性的收敛加速。这种方法提供了将各种 DNN 模型高效扩展到巨大规模的灵活性。Piper [[66](#bib.bib66)]
    旨在在多个加速器之间最优地划分 DNN 计算图，同时结合各种并行模式和优化。作为一种基于两级动态规划的高效优化算法，Piper 利用单层内的张量并行化显著减少了搜索空间。Chimera [[63](#bib.bib63)]
    整合了双向流水线，以高效训练大规模模型，在流水线效率、内存成本和收敛友好性之间达成了最佳平衡。AutoPipe [[64](#bib.bib64)] 引入了一种自动实现平衡分区和减少启动开销的方法。它包括一个用于快速自动生成平衡流水线分区方案的规划器和一个根据规划器结果调整微批次的微批次切割器，以最小化流水线启动开销。Out-Of-Order
    (OOO) BackProp [[65](#bib.bib65)] 利用梯度计算依赖关系重新排序流水线执行，以最大化 GPU 资源利用率。在数据并行训练中，OOO
    重新排序梯度计算的顺序，以优化计算与参数通信之间的重叠。在流水线并行训练中，它优先处理关键梯度计算，以最小化流水线停顿。DeAR [[254](#bib.bib254)]
    提出了将全还原原语解耦为两个连续操作的方法，从而在没有额外通信的情况下实现反向传播和前向计算。这种方法解决了启动延迟和训练性能亚优化的问题。对于异构流水线，HetPipe [[255](#bib.bib255)]
    提出了一个将流水线模型并行 (PMP) 与数据并行 (DP) 集成的 DNN 训练系统。在这个系统中，提出了一种波同步并行方法，以支持虚拟工作者的 PMP
    和 DP。
- en: $\bullet$ Network-flow-level scheduling manages network bandwidth effectively,
    reduces latency, and ensures efficient utilization of network resources for network
    flows related to distributed DL jobs. By adeptly balancing these critical elements,
    the scheduler enhances both performance and reliability of network operations
    significantly. JPAS [[256](#bib.bib256)] implements a straightforward greedy mechanism
    to organize all distributed training jobs periodically. This approach enables
    each host machine to prioritize its network flows according to the established
    job order, delegating the task of flow scheduling and rate allocation to the underlying
    priority-enabled networks. Geryon [[257](#bib.bib257)] employs multiple flows
    with varying priorities to transfer parameters of different urgency levels. This
    approach coordinates multiple parameter servers effectively and gives precedence
    to urgent parameter transfers across the entire network fabric. To address in-network
    delays, such as queuing delays, TensorExpress [[258](#bib.bib258)] enables each
    switch to transmit tensor packets according to their priorities using multiple
    queues. This method ensures that high-priority data packets are handled efficiently
    to minimize delays. Beamer [[259](#bib.bib259)] focuses on reducing the stage-completion
    time (SCT) by considering stage information in its scheduling approach. It proposes
    a stage-aware coflow-scheduling method to minimize the average SCT. Tereis [[260](#bib.bib260)]
    explores the utilization of idle GPU computational resources during data transmission
    periods. It predicts the execution time for a distributed DL job and its corresponding
    data transmission time, allowing for the simultaneous packaging of two jobs on
    the same GPU. This ensures that one job is completed before the other concludes
    its data transfer. Mercury [[261](#bib.bib261)] shifts priority scheduling to
    the transport layer innovatively, focusing on packet granularity. In this system,
    packets in the Mercury buffer with the highest priority are transmitted first.
    Additionally, Mercury incorporates immediate aggregation at the transport layer,
    enabling full overlapping of gradient push-and-pull operations. This approach
    not only streamlines data flow but also maximizes the efficiency of network resource
    utilization.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 网络流级调度有效地管理网络带宽，减少延迟，并确保与分布式 DL 任务相关的网络流的资源高效利用。通过巧妙地平衡这些关键因素，调度器显著提升了网络操作的性能和可靠性。JPAS [[256](#bib.bib256)]
    实现了一个简单的贪婪机制，以周期性地组织所有分布式训练任务。这种方法使每台主机能够根据既定的任务顺序优先处理其网络流，将流调度和速率分配的任务委托给底层的优先级启用网络。Geryon [[257](#bib.bib257)]
    采用多个具有不同优先级的流来传输不同紧急级别的参数。这种方法有效地协调了多个参数服务器，并优先处理整个网络结构中的紧急参数传输。为了解决网络中的延迟问题，如排队延迟，TensorExpress [[258](#bib.bib258)]
    使每个交换机能够根据优先级使用多个队列传输张量包。这种方法确保高优先级的数据包得到有效处理，以最小化延迟。Beamer [[259](#bib.bib259)]
    通过在其调度方法中考虑阶段信息，专注于减少阶段完成时间（SCT）。它提出了一种阶段感知的共同流调度方法，以最小化平均SCT。Tereis [[260](#bib.bib260)]
    探索了在数据传输期间利用闲置的GPU计算资源。它预测分布式DL任务及其对应的数据传输时间，从而允许在同一GPU上同时打包两个任务。这确保了一个任务在另一个任务完成数据传输之前完成。Mercury [[261](#bib.bib261)]
    创新地将优先级调度转移到传输层，关注数据包粒度。在这个系统中，Mercury缓冲区中优先级最高的数据包首先被传输。此外，Mercury 在传输层加入了即时聚合，支持梯度推送和拉取操作的完全重叠。这种方法不仅简化了数据流，还最大化了网络资源利用的效率。
- en: V-B2 Cost-Effective
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 成本效益
- en: The primary objective of cost-effective scheduling is to minimize operational
    costs while ensuring optimal performance for distributed training. It achieves
    a balance between resource utilization, energy consumption, and monetary expenditures
    in the scheduling decisions. During the training phase, cost efficiency emerges
    as a significant goal for the scheduling of training workloads. Cynthia [[262](#bib.bib262)]
    offers predictable distributed training performance while reducing the training
    budget. This scheduler identifies the optimal resource type and maintains training
    throughput effectively, thereby minimizing monetary costs. FC² [[263](#bib.bib263)],
    similar to Cynthia, is a scheduler that recommends cost-effective cloud resource
    allocations for parameter servers in distributed training tasks. It prioritizes
    instances with the largest network bandwidth within the budget to circumvent communication
    bottlenecks. Furthermore, it introduces a heuristic named Scale-Opt for determining
    worker instances, ensuring job throughput, and maximizing cost savings. Jahani [[264](#bib.bib264)]
    considers computing nodes with varying numbers of GPUs as distinct virtual machines.
    The scheduling process is modeled as a mixed-integer linear programming (MILP)
    problem, aiming to reduce leasing costs globally while maintaining job latency.
    GPOEO [[265](#bib.bib265)] achieves significant power savings for training workloads.
    It can be integrated into GPU data centers easily, utilizing a customized scheduler
    to manage job orchestration. STS [[266](#bib.bib266)] optimizes the scheduling
    of distributed training jobs from the perspective of cloud service providers operating
    data centers. It leverages the probability distribution of early job termination
    to adapt resource assignments during job execution, with the aim of minimizing
    the expected energy cost.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 成本效益调度的主要目标是最小化运营成本，同时确保分布式训练的最佳性能。它在资源利用、能量消耗和货币支出之间实现平衡。在训练阶段，成本效率成为调度训练负载的重要目标。**Cynthia**
    [[262](#bib.bib262)] 提供了可预测的分布式训练性能，同时减少了训练预算。该调度器识别最优的资源类型，并有效维持训练吞吐量，从而最小化货币成本。**FC²**
    [[263](#bib.bib263)]，类似于Cynthia，是一个推荐成本效益云资源分配的调度器，专用于分布式训练任务中的参数服务器。它优先考虑预算内带宽最大的实例，以规避通信瓶颈。此外，它引入了一种名为**Scale-Opt**的启发式方法来确定工作实例，确保作业吞吐量，并最大化成本节省。**Jahani**
    [[264](#bib.bib264)] 将不同数量GPU的计算节点视为独立的虚拟机。调度过程被建模为一个混合整数线性规划（MILP）问题，旨在全球范围内降低租赁成本，同时保持作业延迟。**GPOEO**
    [[265](#bib.bib265)] 为训练工作负载实现了显著的电力节省。它可以轻松集成到GPU数据中心中，利用定制的调度器来管理作业编排。**STS**
    [[266](#bib.bib266)] 从云服务提供商运营数据中心的角度优化了分布式训练作业的调度。它利用早期作业终止的概率分布在作业执行期间调整资源分配，旨在最小化预期的能源成本。
- en: V-B3 Deadline Guarantee
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 截止日期保证
- en: Diverging from the aforementioned scheduling methods, deadline-guaranteed scheduling
    focuses on ensuring that jobs are completed before a specified deadline. This
    approach is critical for tasks for which timing is a key factor. GENIE [[267](#bib.bib267)],
    a trailblazing deadline-aware scheduler for distributed training workloads, explores
    the key factors that impact the performance of distributed DL tasks. It introduces
    a predictive model based on lightweight profiling, enabling the accurate estimation
    of processing rates and response latencies for a variety of distributed DL workloads.
    A significant limitation of GENIE, however, is its inability to handle mixed workloads
    that include both deadline-sensitive tasks and best-effort tasks simultaneously [[239](#bib.bib239)].
    Chronus [[268](#bib.bib268)], an end-to-end scheduling system, meets SLOs by guaranteeing
    deadlines for SLO-aware jobs while also enhancing the performance of best-effort
    jobs. This dual-focused strategy enables Chronus to manage a wide range of workload
    requirements. Furthering these developments, Hydra [[269](#bib.bib269)] emerges
    as a dynamic and multifaceted scheduler, equipped to tackle various scheduling
    challenges, including adhering to deadlines and reducing job completion times.
    Hydra introduces an innovative sampling approach that capitalizes on the iterative
    periodicity inherent in distributed DL jobs. This technique allows for the precise
    estimation of job completion times in heterogeneous GPU environments, thereby
    elevating the efficiency and effectiveness of scheduling for various distributed
    DL workloads.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述调度方法不同，保证截止时间的调度方法专注于确保作业在指定截止时间之前完成。这种方法对那些时间至关重要的任务至关重要。GENIE [[267](#bib.bib267)]，一个开创性的截止时间感知调度器，用于分布式训练工作负载，探索了影响分布式深度学习任务性能的关键因素。它引入了一个基于轻量级分析的预测模型，使得能够准确估算各种分布式深度学习工作负载的处理速度和响应延迟。然而，GENIE的一个显著局限性是无法同时处理包括截止时间敏感任务和尽力而为任务的混合工作负载[[239](#bib.bib239)]。Chronus [[268](#bib.bib268)]，一个端到端调度系统，通过保证截止时间来满足SLO，同时还提高了尽力而为任务的性能。这种双重聚焦策略使得Chronus能够管理各种工作负载需求。在这些发展基础上，Hydra [[269](#bib.bib269)]作为一个动态多面向的调度器，能够应对各种调度挑战，包括遵守截止时间和减少作业完成时间。Hydra引入了一种创新的采样方法，利用分布式深度学习作业中固有的迭代周期性。这种技术允许在异构GPU环境中精确估算作业完成时间，从而提高了各种分布式深度学习工作负载的调度效率和效果。
- en: V-C Inference Scheduling
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 推理调度
- en: 'As distributed-DL-based applications permeate our daily lives increasingly
    in the form of online services, the management and scheduling of large-scale inference
    workloads on GPUs have become critical. These inference jobs, distinct from the
    resource-intensive and long-duration training workloads mentioned earlier, possess
    unique characteristics and requirements. Consequently, they necessitate novel
    scheduling solutions tailored to their specific needs [[239](#bib.bib239), [278](#bib.bib278)].
    In alignment with the approach of the preceding section, this section categorizes
    various scheduling strategies with a particular focus on two primary goals: efficiency
    and throughput. This categorization facilitates a comprehensive understanding
    of the different methodologies employed in inference scheduling, highlighting
    their significance in enhancing the overall performance of distributed inference
    workloads.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于分布式深度学习的应用越来越多地以在线服务的形式渗透到我们的日常生活中，大规模推理工作负载在GPU上的管理和调度变得至关重要。这些推理作业不同于前面提到的资源密集型和长期训练工作负载，具有独特的特征和需求。因此，它们需要针对其特定需求的创新调度解决方案[[239](#bib.bib239),
    [278](#bib.bib278)]。与前一节的方法相一致，本节将各种调度策略进行分类，重点关注两个主要目标：效率和吞吐量。这种分类有助于全面理解推理调度中采用的不同方法，并突出其在提升分布式推理工作负载整体性能方面的重要性。
- en: V-C1 Efficiency
  id: totrans-532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 效率
- en: Unlike the training phase, where the focus is often on maximizing accuracy and
    model robustness, the inference phase primarily emphasizes the reduction of latency
    and cost.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练阶段不同，训练阶段通常专注于最大化准确性和模型鲁棒性，而推理阶段主要强调减少延迟和成本。
- en: To maintain satisfactory latency, inference schedulers are designed to scale
    resources proactively in response to request density and to reorder execution
    sequences strategically at the job level. Sniper [[270](#bib.bib270)] stands out
    as a self-updating cloud-edge collaborative inference scheduling system with a
    focus on time awareness. It employs a non-invasive performance characterization
    network based on neural network similarity to predict the inference time of DNNs
    accurately. This system achieves a stable increase in throughput successfully,
    demonstrating its effectiveness in optimizing the scheduling process in dynamic
    cloud-edge environments.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持令人满意的延迟，推断调度器被设计为在响应请求密度时主动扩展资源，并在作业级别上战略性地重新排序执行顺序。Sniper [[270](#bib.bib270)]
    作为一个自我更新的云-边缘协作推断调度系统，突出时间意识。它采用基于神经网络相似性的非侵入性性能表征网络，准确预测深度神经网络的推断时间。该系统成功实现了吞吐量的稳定增加，证明了其在动态云-边缘环境中优化调度过程的有效性。
- en: In practice, cost efficiency is another critical factor in the inference phase,
    prompting some schedulers to incorporate various mechanisms aimed at achieving
    cost-effective inference. AutoDeep [[271](#bib.bib271)] endeavors to automate
    cloud deployment for real-time online DNN inference, focusing on minimizing costs
    while maintaining acceptably low latency. To achieve this, AutoDeep utilizes Bayesian
    optimization combined with DRL. This innovative approach enables the adaptive
    discovery of the optimal cloud configuration and device placement, reducing the
    required search time significantly. Through this method, AutoDeep balances the
    trade-off between operational costs and latency in DNN inference workloads efficiently.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，成本效率是推断阶段的另一个重要因素，这促使一些调度器采用各种机制来实现成本效益。AutoDeep [[271](#bib.bib271)] 力图自动化实时在线深度神经网络推断的云部署，重点在于在保持可接受的低延迟的同时，尽可能减少成本。为了实现这一目标，AutoDeep
    利用贝叶斯优化结合深度强化学习。这种创新方法使得能够自适应地发现最佳的云配置和设备布置，显著减少所需的搜索时间。通过这种方法，AutoDeep 高效地平衡了深度神经网络推断工作负载中的运营成本和延迟之间的权衡。
- en: Meanwhile, latency and cost are recognized as interdependent factors in system
    design. Improving one aspect may inadvertently compromise the other if the solution
    is not designed meticulously. This interplay motivates researchers to develop
    scheduler systems that strike a balance between these objectives. Clipper [[272](#bib.bib272)]
    introduces an innovative model-selection abstraction, accommodating both single-model
    selection and model-integration selection. This system conducts inferences across
    all models and integrates their results. Clipper monitors accuracy and latency
    feedback continuously, employing a best-effort search approach to optimize model
    selection based on these parameters.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，延迟和成本被认为是系统设计中的相互依赖因素。如果解决方案设计不够精细，改善一个方面可能会无意中损害另一个方面。这种相互作用促使研究人员开发出在这些目标之间取得平衡的调度系统。Clipper
    [[272](#bib.bib272)] 引入了一种创新的模型选择抽象，支持单模型选择和模型集成选择。该系统在所有模型上进行推断，并整合其结果。Clipper
    持续监控准确性和延迟反馈，采用尽力而为的搜索方法根据这些参数优化模型选择。
- en: V-C2 Throughout capability
  id: totrans-537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 吞吐能力
- en: Throughput capability represents another crucial objective for scheduling inference
    workloads. Generally, the scheduling system is fine-tuned to enhance throughput
    through strategic batch execution and configuration adjustments.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 吞吐能力代表了调度推断工作负载的另一个关键目标。通常，调度系统通过战略性批处理执行和配置调整来优化吞吐量。
- en: Batching inference has been identified as an efficient method to enhance utilization
    and reduce system overhead [[239](#bib.bib239)]. In recent times, various schedulers
    have incorporated heuristic methods to fine-tune the batch size for optimal performance.
    For instance, Rafiki [[273](#bib.bib273)] employs a practical Additive-Increase
    Multiplicative-Decrease (AIMD) algorithm to adjust the inference batch size dynamically.
    This approach allows for responsive adaptation to varying workload conditions.
    Nanily [[274](#bib.bib274)] establishes an upper limit on the batch size by calculating
    the maximum remaining time for a request. This is determined by subtracting the
    minimum queuing time of available resources from the remaining time. It then computes
    an appropriate batch size such that the inference execution time is equal to or
    approximates this maximum remaining time.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 对推断进行批处理被认为是一种提高利用率和减少系统开销的有效方法 [[239](#bib.bib239)]。近来，各种调度程序已经采用了启发式方法来优化批处理大小以获得最佳性能。例如，Rafiki
    [[273](#bib.bib273)] 使用了实用的加法增加-乘法减少（AIMD）算法来动态调整推断批处理大小。该方法允许对不同的工作负载条件作出响应性调整。Nanily
    [[274](#bib.bib274)] 通过计算请求的最大剩余时间来确定批处理大小的上限。这个时间是通过将可用资源的最小等待时间从剩余时间中减去得到的。然后，它计算一个适当的批处理大小，使得推断执行时间等于或接近这个最大剩余时间。
- en: In addition to the aforementioned approaches, certain schedulers employ end-to-end
    configuration tuning to enhance system throughput. RRL [[275](#bib.bib275)] emphasizes
    the optimization of parallel configurations at various levels, including both
    request-level parallelism and intra-request level parallelism. These optimizations
    play a significant role in reducing the overall system latency and improving throughput.
    Morphling [[276](#bib.bib276)], on the other hand, presents a rapid and near-optimal
    auto-configuration framework designed specifically for cloud-native model serving.
    This framework adapts to new inference services by sampling a limited set of configurations
    and then employs a meta-model to identify the most optimal configuration. This
    strategy allows Morphling to adjust quickly and efficiently to varying service
    requirements while maintaining high system performance.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的方法之外，某些调度程序使用端到端的配置调优来增强系统的吞吐量。RRL [[275](#bib.bib275)] 强调在各个层次上优化并行配置，包括请求级别的并行性和请求内部的并行性。这些优化在减少整体系统延迟和提高吞吐量方面起到了重要作用。而
    Morphling [[276](#bib.bib276)] 则提供了一个快速且接近最优的自动配置框架，专门设计用于云原生模型服务。该框架通过对一组有限的配置进行采样，然后利用元模型来识别最优配置，以适应新的推断服务。这种策略使得
    Morphling 能够在保持高系统性能的同时，快速高效地适应不同的服务需求。
- en: V-D Lessons Learned toward Large-scale Resource Allocation and Task Scheduling
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 向大规模资源分配和任务调度的经验教训
- en: In this section, we discuss some lessons learned from designing resource-allocation
    and task-scheduling strategies for large-scale distributed DL. These lessons learned
    help to reveal promising directions for future studies.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了为大规模分布式深度学习设计资源分配和任务调度策略的一些经验教训。这些经验教训有助于揭示未来研究的有前途的方向。
- en: $\bullet$ Fine-grained and elastic GPU memory and bandwidth sharing strategies
    are critical to improve GPU utilization. Conventional GPU-allocation methods often
    assign individual distributed DL jobs exclusive access to a GPU and can lead to
    extremely low utilization. Exploring the resource-allocation optimization for
    diverse distributed training and inference workloads running on heterogeneous
    GPU networks in large-scale clusters comprehensively is necessary. GPU-sharing
    strategies should consider various important factors, including performance isolation,
    elastic allocation, orchestration of computational and communication-resource
    allocation, and resource fragmentation.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$细粒度和弹性的GPU内存和带宽共享策略对于提高GPU利用率至关重要。传统的GPU分配方法通常将单个分布式深度学习任务分配给一个独占的GPU，这可能导致极低的利用率。全面探索在大规模集群中运行在异构GPU网络上的多样化分布式训练和推断工作负载的资源分配优化是必要的。GPU共享策略应考虑各种重要因素，包括性能隔离、弹性分配、计算和通信资源分配的编排和资源碎片化。
- en: $\bullet$ High-performance large-scale distributed DL requires the orchestration
    of efficient allocation of GPU and network resources. The allocation of network
    resources can frequently be overlooked as a bottleneck for efficient resource
    utilization in distributed DL. Many resource-allocation strategies of distributed
    DL focus on addressing computation issues, such as low utilization, load imbalance,
    and long queuing delays. However, with the increase of the cluster scale, the
    complexity of GPU network connections increases exponentially, and lack of consideration
    to efficient network-resource allocation can result in significant low job-execution
    performance of large-scale distributed DL. Efficient network-bandwidth-allocation
    strategies can alleviate communication contention. Fully utilizing resources of
    both GPU and network bandwidth leads to enhanced overall performance of distributed
    training and inference at a large scale.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 高性能大规模分布式深度学习需要高效的GPU和网络资源分配的协调。网络资源的分配常常被忽视，可能成为分布式深度学习中资源利用效率的瓶颈。许多分布式深度学习的资源分配策略集中于解决计算问题，如低利用率、负载不平衡和长排队延迟。然而，随着集群规模的增大，GPU网络连接的复杂性呈指数级增长，如果未能考虑高效的网络资源分配，可能会导致大规模分布式深度学习的作业执行性能显著降低。高效的网络带宽分配策略可以缓解通信争用。充分利用GPU和网络带宽资源将提升大规模分布式训练和推理的整体性能。
- en: $\bullet$ These framework-level strategies can orchestrate with optimizations
    for distributed DL algorithms to meet common and distinct requirements of distributed
    training and inference. Allocation and scheduling efficiencies are important for
    both distributed training and inference. Examples of distinct performance objectives
    of distributed training include promoting fairness and increasing training throughput.
    In contrast, distributed inference requires achieving low latency and ensuring
    the SLO. Efficient distributed DL is the result of the orchestration of efficient
    resource-allocation and task-scheduling strategies. To tackle complex challenges
    posed by large data sets, large cluster scale, and large models, distributed DL
    also requires the orchestration of these strategies and contemporary distributed
    DL algorithms, for developing more efficient, effective, and comprehensive distributed
    DL solutions for various large-scale scenarios.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 这些框架级策略可以与分布式深度学习算法的优化进行协调，以满足分布式训练和推理的共同及独特需求。分配和调度效率对分布式训练和推理都很重要。分布式训练的独特性能目标的例子包括促进公平性和提高训练吞吐量。相比之下，分布式推理则需要实现低延迟并确保服务水平协议（SLO）。高效的分布式深度学习是高效资源分配和任务调度策略的协调结果。为了应对大数据集、大规模集群和大型模型带来的复杂挑战，分布式深度学习还需要协调这些策略和现代分布式深度学习算法，以开发适用于各种大规模场景的更高效、更有效、更全面的分布式深度学习解决方案。
- en: 'TABLE IX: Studies on communication-efficient infrastructures for large-scale
    distributed DL'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：大规模分布式深度学习的通信高效基础设施研究
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|                                                  分类 | 技术与参考文献 | 年份 | 亮点 |'
- en: '|   GPU interconnects ([VI-A](#S6.SS1 "VI-A GPU Interconnects ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | Intra-node ([VI-A1](#S6.SS1.SSS1 "VI-A1
    Intra-node ‣ VI-A GPU Interconnects ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | PCIe [[279](#bib.bib279)] | - | Wired point-to-point link connection
    for high-speed serial communication between GPUs. |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '|   GPU互连 ([VI-A](#S6.SS1 "VI-A GPU Interconnects ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | 节点内 ([VI-A1](#S6.SS1.SSS1 "VI-A1 Intra-node ‣ VI-A
    GPU Interconnects ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | PCIe [[279](#bib.bib279)]
    | - | 用于GPU之间高速串行通信的有线点对点链接。 |'
- en: '| NVLink [[280](#bib.bib280)] | - | Direct GPU-to-GPU interconnects based on
    wired mesh networking. |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| NVLink [[280](#bib.bib280)] | - | 基于有线网状网络的直接GPU到GPU互连。 |'
- en: '| Inter-node ([VI-A2](#S6.SS1.SSS2 "VI-A2 Inter-node ‣ VI-A GPU Interconnects
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | GPUDirect-RDMA [[281](#bib.bib281)]
    | - | Direct GPU memory access between nodes within a cluster via PCIe. |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 节点间 ([VI-A2](#S6.SS1.SSS2 "VI-A2 Inter-node ‣ VI-A GPU Interconnects ‣ VI
    Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | GPUDirect-RDMA [[281](#bib.bib281)]
    | - | 通过PCIe在集群内节点之间直接访问GPU内存。 |'
- en: '| NVSwitch [[280](#bib.bib280)] | - | All-to-all GPU communication across multiple
    nodes. |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| NVSwitch [[280](#bib.bib280)] | - | 跨多个节点的全到全GPU通信。 |'
- en: '|   Programmable network devices ([VI-B](#S6.SS2 "VI-B Programmable Network
    Devices ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Switch ([VI-B1](#S6.SS2.SSS1
    "VI-B1 Programmable switches ‣ VI-B Programmable Network Devices ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | SwitchML [[282](#bib.bib282)] | 2021
    | Resource: reusing a fixed-size memory pool for INA within a rack. |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|   可编程网络设备 ([VI-B](#S6.SS2 "VI-B Programmable Network Devices ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | 交换机 ([VI-B1](#S6.SS2.SSS1 "VI-B1 Programmable
    switches ‣ VI-B Programmable Network Devices ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | SwitchML [[282](#bib.bib282)] | 2021 | 资源：在机架内重复使用固定大小的内存池用于INA。 |'
- en: '| ATP [[283](#bib.bib283)] | 2021 | Resource: allocating memory dynamically
    upon their arrival for multiple training jobs. |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| ATP [[283](#bib.bib283)] | 2021 | 资源：为多个训练作业在到达时动态分配内存。 |'
- en: '|  | INAlloc [[284](#bib.bib284)] | 2023 | Resource: an additional switch memory
    manager to manage switch memory allocation for multiple jobs. |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  | INAlloc [[284](#bib.bib284)] | 2023 | 资源：一个额外的交换机内存管理器，用于管理多个作业的交换机内存分配。
    |'
- en: '|  | GRID [[285](#bib.bib285)] | 2023 | Routing: randomized-rounding-based
    gradient routing with INA to maximize the gradient sending rate with resource
    constraints. |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  | GRID [[285](#bib.bib285)] | 2023 | 路由：基于随机取整的梯度路由与INA结合，以最大化在资源限制下的梯度发送速率。
    |'
- en: '|  | GOAT [[286](#bib.bib286)] | 2023 | Routing: dividing the model into submodels
    and finding the optimal submodel gradient routing to minimize communication overhead.
    |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  | GOAT [[286](#bib.bib286)] | 2023 | 路由：将模型划分为子模型，并找到最佳的子模型梯度路由以最小化通信开销。
    |'
- en: '|  | PANAMA [[287](#bib.bib287)] | 2021 | Congestion: multiple aggregation
    tree to disperse gradient traffic and ensure fairness among multiple jobs. |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  | PANAMA [[287](#bib.bib287)] | 2021 | 拥塞：多个聚合树用于分散梯度流量并确保多个作业之间的公平性。 |'
- en: '|  | A²TP [[288](#bib.bib288)] | 2023 | Congestion: two congestion windows
    for independent congestion control of switch resources and link bandwidth. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  | A²TP [[288](#bib.bib288)] | 2023 | 拥塞：两个拥塞窗口用于对交换机资源和链路带宽进行独立的拥塞控制。 |'
- en: '|  | NetReduce [[289](#bib.bib289)] | 2023 | Congestion: transport-transparent
    gradient INA, reusing transport layer’s congestion control. |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  | NetReduce [[289](#bib.bib289)] | 2023 | 拥塞：传输透明的梯度INA，重用传输层的拥塞控制。 |'
- en: '| SmartNIC ([VI-B2](#S6.SS2.SSS2 "VI-B2 Programmable NICs ‣ VI-B Programmable
    Network Devices ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Guo et al. [[290](#bib.bib290)]
    | 2023 | Multi-level in-SmartNIC caching and computing system for recommendation
    models. |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| SmartNIC ([VI-B2](#S6.SS2.SSS2 "VI-B2 Programmable NICs ‣ VI-B Programmable
    Network Devices ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Guo et al. [[290](#bib.bib290)]
    | 2023 | 用于推荐模型的多级智能网卡缓存和计算系统。 |'
- en: '| FCsN [[291](#bib.bib291)] | 2022 | Offloading control logic, scheduling,
    and computation of distributed inference jobs to SmartNICs. |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| FCsN [[291](#bib.bib291)] | 2022 | 将分布式推理作业的控制逻辑、调度和计算卸载到SmartNICs。 |'
- en: '|   Collective communication ([VI-C](#S6.SS3 "VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Interface ([VI-C1](#S6.SS3.SSS1
    "VI-C1 Collective communication libraries ‣ VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | [[292](#bib.bib292), [293](#bib.bib293),
    [294](#bib.bib294)] | - | Point-to-point and collective communication interfaces
    for gradient computation and communication via GPU interconnects. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|   集合通信 ([VI-C](#S6.SS3 "VI-C Inter-GPU Collective Communication ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | 接口 ([VI-C1](#S6.SS3.SSS1 "VI-C1 Collective
    communication libraries ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | [[292](#bib.bib292), [293](#bib.bib293),
    [294](#bib.bib294)] | - | 用于通过GPU互连进行梯度计算和通信的点对点和集合通信接口。 |'
- en: '| NCCL [[295](#bib.bib295)] | - | A high-bandwidth and low-latency collective
    communication library tailored for NVLink. |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| NCCL [[295](#bib.bib295)] | - | 一个高带宽和低延迟的集合通信库，专为NVLink量身定制。 |'
- en: '|  | MSCCLang [[296](#bib.bib296)] | 2023 | A unified framework for writing
    customized collective communication algorithms. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '|  | MSCCLang [[296](#bib.bib296)] | 2023 | 一个统一的框架，用于编写自定义集合通信算法。 |'
- en: '| Heterogeneous networks ([VI-C2](#S6.SS3.SSS2 "VI-C2 Collective communication
    in heterogeneous networks ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | BlueConnect [[297](#bib.bib297)] |
    2019 | Decomposing allreduce into multiple reduce-scatter and all-gather primitives
    in symmetric topologies. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 异构网络 ([VI-C2](#S6.SS3.SSS2 "VI-C2 Collective communication in heterogeneous
    networks ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | BlueConnect [[297](#bib.bib297)] | 2019 | 在对称拓扑中将allreduce分解为多个reduce-scatter和all-gather原语。
    |'
- en: '| Blink [[298](#bib.bib298)] | 2020 | Leveraging packing spanning trees to
    decomposing allreduce for optimal bandwidth utilization in asymmetric topologies.
    |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| Blink [[298](#bib.bib298)] | 2020 | 利用打包跨度树在非对称拓扑中将allreduce分解，以实现最佳带宽利用。
    |'
- en: '|  | Plink [[299](#bib.bib299)] | 2020 | Probing the network to capture the
    physical network topology and bandwidth information to optimize collective communication.
    |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '|  | Plink [[299](#bib.bib299)] | 2020 | 通过探测网络以捕捉物理网络拓扑和带宽信息，从而优化集合通信。 |'
- en: '| Sparse data ([VI-C3](#S6.SS3.SSS3 "VI-C3 Collective communication for sparse
    gradients ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | SparcML [[300](#bib.bib300)] | 2019 | Supporting sparse
    data streaming and sparse-to-dense switching for the minimal collective communication
    cost. |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏数据 ([VI-C3](#S6.SS3.SSS3 "VI-C3 Collective communication for sparse gradients
    ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | SparcML [[300](#bib.bib300)] | 2019 | 支持稀疏数据流和稀疏到密集的切换，以实现最小的集合通信成本。'
- en: '| OmniReduce [[301](#bib.bib301)] | 2021 | Splitting sparse data into blocks
    to increase INA parallelism for non-zero blocks. |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| OmniReduce [[301](#bib.bib301)] | 2021 | 将稀疏数据分割成块，以增加非零块的INA并行性。 |'
- en: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | Balancing sparse collective communication
    overhead across workers depending on consecutive buffer size. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | 根据连续缓冲区大小，在工作节点之间平衡稀疏集合通信开销。 |'
- en: '|  | CommLib [[174](#bib.bib174)] | 2021 | A hierarchical aggregation and caching
    scheme for sparse collective communication to fully utilize GPU bandwidth and
    reduce I/O. |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | CommLib [[174](#bib.bib174)] | 2021 | 一种层次化聚合和缓存方案，用于稀疏集合通信，以充分利用GPU带宽并减少I/O。
    |'
- en: '|  | DeepReduce [[302](#bib.bib302)] | 2021 | Decoupling indices and values
    of sparse data to apply different compression algorithms. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepReduce [[302](#bib.bib302)] | 2021 | 解耦稀疏数据的索引和值以应用不同的压缩算法。 |'
- en: '| Synthesis ([VI-C4](#S6.SS3.SSS4 "VI-C4 Synthesizing optimal collective communication
    ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | SCCL [[303](#bib.bib303)] | 2021 | Synthesizing latency- and bandwidth-optimal
    collective implementations given a network topology with bandwidth constraints.
    |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 综合（[VI-C4](#S6.SS3.SSS4 "VI-C4 Synthesizing optimal collective communication
    ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")） | SCCL [[303](#bib.bib303)] | 2021 | 在带宽受限的网络拓扑下，综合延迟和带宽最优的集体实现。 |'
- en: '| TACCL [[304](#bib.bib304)] | 2023 | Synthesizing collective algorithms for
    heterogeneous multi-rack networks efficiently by dividing the problem into two
    independent sub-problems: routing and scheduling. |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| TACCL [[304](#bib.bib304)] | 2023 | 通过将问题分解为两个独立的子问题（路由和调度），有效地综合异构多机架网络的集体算法。
    |'
- en: '|   Topology ([VI-D](#S6.SS4 "VI-D Communication Topologies ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | Fixed ([VI-D1](#S6.SS4.SSS1 "VI-D1
    Fixed topologies ‣ VI-D Communication Topologies ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | BML [[305](#bib.bib305)] | 2020 | A BCube topology
    for fully distributed data-parallel training. |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|   拓扑（[VI-D](#S6.SS4 "VI-D Communication Topologies ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")） | 固定的（[VI-D1](#S6.SS4.SSS1 "VI-D1 Fixed topologies ‣
    VI-D Communication Topologies ‣ VI Large-Scale Communication Infrastructures ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")） | BML [[305](#bib.bib305)] | 2020 | 用于完全分布式数据并行训练的 BCube 拓扑。'
- en: '| HammingMesh [[306](#bib.bib306)] | 2022 | Local connectivity by affordable
    PCB-mesh interconnects and global connectivity by sparsely connected switches.
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| HammingMesh [[306](#bib.bib306)] | 2022 | 通过经济实惠的 PCB 网格互连提供本地连接，通过稀疏连接的交换机提供全球连接。
    |'
- en: '| Configurable ([VI-D2](#S6.SS4.SSS2 "VI-D2 Reconfigurable topologies ‣ VI-D
    Communication Topologies ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | SiP-ML [[307](#bib.bib307)]
    | 2021 | Commercially affordable fully-connected and ring-based topologies powered
    by SiP interfaces. |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 可配置的（[VI-D2](#S6.SS4.SSS2 "VI-D2 Reconfigurable topologies ‣ VI-D Communication
    Topologies ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")） | SiP-ML [[307](#bib.bib307)]
    | 2021 | 商业实惠的全连通和基于环形的拓扑，支持 SiP 接口。 |'
- en: '| TopoOpt[[308](#bib.bib308)] | 2023 | Offline permuting ring-allreduce topologies
    for the optimal topology of multi-NIC nodes. |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| TopoOpt [[308](#bib.bib308)] | 2023 | 离线排列环形全规约拓扑以优化多 NIC 节点的拓扑。 |'
- en: '|   |  |  |  |  |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |'
- en: '![Refer to caption](img/dd26c39d9a617b76d6be45297ef6f79a.png)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dd26c39d9a617b76d6be45297ef6f79a.png)'
- en: 'Figure 10: A Torus topology for large-scale distributed DL with GPU interconnects,
    as well as programmable network interface cards and switches'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：一个用于大规模分布式深度学习的环形拓扑，配备了 GPU 互连、可编程网络接口卡和交换机
- en: VI Large-Scale Communication Infrastructures
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 大规模通信基础设施
- en: 'In this section, we introduce high-performance communication infrastructures
    for large-scale distributed DL, including GPU interconnects, programmable network
    devices, collective communication protocols, and communication topologies. Table [IX](#S5.T9
    "TABLE IX ‣ V-D Lessons Learned toward Large-scale Resource Allocation and Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") summarizes the
    related technologies and studies on these topics. We conclude this section with
    some lessons learned in this domain. Fig. [10](#S5.F10 "Figure 10 ‣ V-D Lessons
    Learned toward Large-scale Resource Allocation and Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") depicts an example Torus [[309](#bib.bib309)]
    topology with high-bandwidth intra- and inter-node GPU interconnects, as well
    as programmable network devices. Implementing efficient collective communication
    built upon the high-performance communication connection and topology enhances
    distributed DL at a large scale.'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '本节介绍了大规模分布式深度学习的高性能通信基础设施，包括 GPU 互连、可编程网络设备、集体通信协议和通信拓扑。表 [IX](#S5.T9 "TABLE
    IX ‣ V-D Lessons Learned toward Large-scale Resource Allocation and Task Scheduling
    ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") 总结了这些主题的相关技术和研究。我们通过一些在该领域的经验教训来总结本节内容。图
    [10](#S5.F10 "Figure 10 ‣ V-D Lessons Learned toward Large-scale Resource Allocation
    and Task Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") 展示了一个带有高带宽节点内和节点间 GPU 互连的 Torus [[309](#bib.bib309)] 拓扑示例，以及可编程网络设备。基于高性能通信连接和拓扑构建高效的集体通信，增强了大规模分布式深度学习的能力。'
- en: VI-A GPU Interconnects
  id: totrans-584
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A GPU 互连
- en: The communication between GPUs within a computing node and across nodes is significantly
    accelerated by high-speed GPU interconnects [[310](#bib.bib310), [311](#bib.bib311)].
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 节点内部及节点间的 GPU 通信通过高速 GPU 互连显著加速[[310](#bib.bib310), [311](#bib.bib311)]。
- en: VI-A1 Intra-node
  id: totrans-586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 节点内
- en: In a scale-up solution, Peripheral Component Interconnect Express (PCIe) [[279](#bib.bib279)]
    and NVLink [[280](#bib.bib280)] are prominent data link protocols for inter-GPU
    communication within a node. PCIe serves as a high-speed serial communication
    interface between GPUs via a wired point-to-point link connection. It supports
    up to 16 lanes per GPU, with the maximum bandwidth of 242 GBps for each GPU in
    PCIe version 7.0. NVLink is a direct GPU-to-GPU interconnect based on wired mesh
    networking. It achieves significantly higher throughput compared to PCIe, supporting
    up to 18 links per GPU and providing a maximum bandwidth of 900 GBps for each
    GPU in NVLink version 4.0. This elevated bandwidth enhances the speed and efficiency
    of inter-GPU communication, making NVLink particularly advantageous in scenarios
    demanding extensive inter-GPU data exchange.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展方案中，外围组件互连快速通道（PCIe）[[279](#bib.bib279)]和 NVLink [[280](#bib.bib280)] 是节点内
    GPU 之间通信的主要数据链路协议。PCIe 作为一种高速度串行通信接口，通过有线点对点连接在 GPU 之间传输数据。它支持每个 GPU 高达 16 条通道，PCIe
    7.0 版本的每个 GPU 最大带宽为 242 GBps。NVLink 是一种基于有线网状网络的直接 GPU 对 GPU 互连。它比 PCIe 实现了显著更高的吞吐量，支持每个
    GPU 高达 18 条链路，并为 NVLink 4.0 版本的每个 GPU 提供最大 900 GBps 带宽。这种提升的带宽增强了 GPU 之间通信的速度和效率，使
    NVLink 在需要大量 GPU 数据交换的场景中尤为有利。
- en: VI-A2 Inter-node
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 节点间
- en: In a scale-out solution, the scalability of high-speed inter-node GPU interconnects
    can be achieved through the technologies of GPUDirect-RDMA [[281](#bib.bib281)]
    and NVSwitch [[280](#bib.bib280)]. GPUDirect-RDMA facilitates direct GPU memory
    access between nodes within a cluster via PCIe, thereby eliminating unnecessary
    memory copies through the CPU. NVSwitch extends the capability of NVLink by supporting
    all-to-all GPU communication across multiple nodes. NVSwitch can empower an ultra-large
    GPU server cluster with a capacity of up to 256 GPUs, delivering a staggering
    57.6 TBps all-to-all bandwidth and supporting very large DL models with trillions
    of parameters.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展方案中，通过 GPUDirect-RDMA [[281](#bib.bib281)] 和 NVSwitch [[280](#bib.bib280)]
    技术可以实现高速节点间 GPU 互连的可扩展性。GPUDirect-RDMA 通过 PCIe 实现节点间直接 GPU 内存访问，从而避免了通过 CPU 的不必要的内存拷贝。NVSwitch
    通过支持跨多个节点的全对全 GPU 通信，扩展了 NVLink 的能力。NVSwitch 可以支持多达 256 个 GPU 的超大 GPU 服务器集群，提供惊人的
    57.6 TBps 全对全带宽，并支持具有数万亿参数的大型深度学习模型。
- en: VI-B Programmable Network Devices
  id: totrans-590
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 可编程网络设备
- en: Programmable network devices, including programmable switches and network interface
    cards (NICs), facilitate software-hardware co-design to improve the communication
    efficiency of distributed DL algorithms.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 可编程网络设备，包括可编程开关和网络接口卡（NICs），促进了软硬件共同设计，以提高分布式深度学习算法的通信效率。
- en: VI-B1 Programmable switches
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 可编程开关
- en: Recently, interest has been growing in leveraging programmable switches to execute
    in-network aggregation (INA) [[312](#bib.bib312)] for allreduce operations on
    gradients, aiming to mitigate network traffic during distributed training. Key
    considerations within this domain include on-switch resource utilization [[282](#bib.bib282),
    [283](#bib.bib283), [284](#bib.bib284)], gradient routing [[285](#bib.bib285),
    [286](#bib.bib286)], and congestion control [[287](#bib.bib287), [288](#bib.bib288),
    [289](#bib.bib289)].
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人们对利用可编程开关来执行网络内聚合（INA） [[312](#bib.bib312)] 以进行梯度的allreduce操作产生了越来越大的兴趣，旨在减轻分布式训练期间的网络流量。在这一领域的关键考虑因素包括开关资源利用 [[282](#bib.bib282),
    [283](#bib.bib283), [284](#bib.bib284)]、梯度路由 [[285](#bib.bib285), [286](#bib.bib286)]
    和拥塞控制 [[287](#bib.bib287), [288](#bib.bib288), [289](#bib.bib289)]。
- en: '$\bullet$ On-switch resource utilization: The implementation of INA for gradients
    in the switch is significantly constrained by limited on-switch memory. SwitchML [[282](#bib.bib282)]
    employs a fixed-size memory pool to aggregate gradients during allreduce communication
    within a rack switch. The reuse mechanism of the memory pool enables SwitchML
    to accommodate streaming gradients in a switch with limited computational and
    storage capabilities. However, SwitchML falls short in addressing challenges presented
    by practical scenarios featuring more complex topologies and workloads, such as
    a multi-rack cluster shared by multiple DL tenants. This hinders its scalability
    across multiple racks accommodating multiple training jobs. ATP [[283](#bib.bib283)]
    extends upon the work of SwitchML to enhance INA for multiple training jobs in
    a multi-rack setting by sharing limited switch resources across concurrently running
    training jobs efficiently. ATP employs a decentralized memory-allocation mechanism
    to allocate memory space on each switch to gradient packets from multiple training
    jobs dynamically upon their arrival at the switch. In contrast to SwitchML that
    conducts entire gradient aggregation in the rack switch, potentially encountering
    network bandwidth underutilization during heavy resource contention, ATP employs
    a best-effort approach to send gradients for aggregation. To further enhance the
    efficient utilization of switch resources for INA in a cluster, INAlloc [[284](#bib.bib284)]
    introduces a additional switch memory management layer. This layer enables the
    distributed training job scheduler to optimize the management of physical memory
    on switches, thus improving job completion time.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 开关资源利用：在开关中实现梯度的 INA 受到有限开关内存的显著限制。SwitchML [[282](#bib.bib282)] 采用固定大小的内存池来汇总机架开关内所有reduce通信中的梯度。内存池的重用机制使
    SwitchML 能够在计算和存储能力有限的开关中容纳流式梯度。然而，SwitchML 在应对具有更复杂拓扑和工作负载的实际场景（如多个深度学习租户共享的多机架集群）时仍显不足。这限制了它在多个机架中处理多个训练任务的扩展性。ATP [[283](#bib.bib283)]
    在 SwitchML 的基础上进行扩展，以提升在多机架环境下处理多个训练任务的 INA，通过高效地在并发运行的训练任务之间共享有限的开关资源来实现。ATP
    采用去中心化的内存分配机制，在梯度包到达开关时动态分配内存空间。与 SwitchML 进行机架开关内全部梯度汇总相比，可能在资源争用激烈时遇到网络带宽利用不足的问题，ATP
    采用尽力而为的方法发送梯度进行汇总。为了进一步提升集群中开关资源的有效利用，INAlloc [[284](#bib.bib284)] 引入了额外的开关内存管理层。该层使分布式训练任务调度器能够优化开关上物理内存的管理，从而改善任务完成时间。
- en: '$\bullet$ Gradient routing: Optimized gradient routing algorithms yield benefits
    for INA in multi-rack environments, including improved load balancing, mitigation
    of bandwidth bottlenecks, and the optimal utilization of switch resources. However,
    gradient routing paths in SwitchML and ATP are fixed and not optimized. To tackle
    this issue, GRID [[285](#bib.bib285)] presents a randomized-rounding-based algorithm
    for gradient routing with INA, aiming to maximize the gradient sending rate of
    workers with resource constraints. GOAT [[286](#bib.bib286)] introduces a comparable
    approach with a focus on minimizing the network communication overhead among workers,
    switches, and the PS. This approach first partitions the DL model into submodels
    and then utilizes a knapsack-based randomized rounding algorithm to determine
    the switch responsible for aggregating specific submodel gradients.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 梯度路由：优化的梯度路由算法为多机架环境中的INA带来了好处，包括改善负载均衡、缓解带宽瓶颈以及优化交换机资源的利用。然而，SwitchML和ATP中的梯度路由路径是固定的且未优化。为解决这个问题，GRID [[285](#bib.bib285)]提出了一种基于随机舍入的梯度路由算法，旨在最大化在资源约束下工人的梯度发送速率。GOAT [[286](#bib.bib286)]引入了一种类似的方法，侧重于最小化工人、交换机和PS之间的网络通信开销。该方法首先将深度学习模型划分为子模型，然后利用基于背包的随机舍入算法来确定负责聚合特定子模型梯度的交换机。
- en: '$\bullet$ Congestion control: Because of the absence of an end-to-end byte
    stream abstraction in the transport layer for INA of distributed training, traditional
    congestion-control strategies at the transport layer, such as DCTCP [[313](#bib.bib313)]
    and pFabric [[314](#bib.bib314)], are not readily applicable. Many existing INA
    methods opt to implement their own straightforward congestion-control strategies.
    For example, SwitchML implements basic retransmission timeout and self-clock mechanisms,
    while ATP utilizes the Explicit Congestion Notification (ECN) flag and Additive-Increase
    Multiplicative-Decrease (AIMD) mechanism, akin to TCP. However, these simplified
    congestion control implementations often fall short in optimizing performance
    for multiple jobs within large-scale shared cluster scenarios, or tend to couple
    the congestion control of switch resources and link bandwidth resources. To tackle
    the former issue, PANAMA [[287](#bib.bib287)] employs multiple aggregation trees
    to disperse traffic, thus mitigating congestion hotspots and ensuring fair sharing
    of network resources across multiple jobs. To tackle the latter issue, A²TP [[288](#bib.bib288)]
    employs two congestion windows for independent congestion control of switch resources
    and link bandwidth resources. Decoupling INA with the transport layer provides
    another direction in tackling the congestion control issue in this context. NetReduce [[289](#bib.bib289)]
    preserves end-to-end abstracted connections by keeping packet data volume unchanged
    within the switch. As a result, the INA for gradients becomes transport-transparent,
    allowing the reuse of advanced congestion-control strategies of the transport
    layer, such as RoCEv2 [[315](#bib.bib315)].'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 拥塞控制：由于在分布式训练的INA的传输层中缺乏端到端字节流抽象，传统的传输层拥塞控制策略，如DCTCP [[313](#bib.bib313)]和pFabric [[314](#bib.bib314)]，不易应用。许多现有的INA方法选择实施自己的简单拥塞控制策略。例如，SwitchML实现了基本的重传超时和自时钟机制，而ATP利用了显式拥塞通知（ECN）标志和加法增量乘法减少（AIMD）机制，类似于TCP。然而，这些简化的拥塞控制实现通常在大规模共享集群场景中无法优化多个作业的性能，或往往将交换机资源和链路带宽资源的拥塞控制耦合在一起。为解决前者问题，PANAMA [[287](#bib.bib287)]采用多个聚合树来分散流量，从而缓解拥塞热点，并确保多个作业之间网络资源的公平共享。为解决后者问题，A²TP [[288](#bib.bib288)]采用两个拥塞窗口来独立控制交换机资源和链路带宽资源的拥塞。将INA与传输层解耦提供了在此背景下解决拥塞控制问题的另一个方向。NetReduce [[289](#bib.bib289)]通过保持交换机中的数据包数据量不变来保留端到端的抽象连接。因此，梯度的INA变得对传输透明，允许重用传输层的高级拥塞控制策略，如RoCEv2 [[315](#bib.bib315)]。
- en: VI-B2 Programmable NICs
  id: totrans-597
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 可编程NICs
- en: Commercial Field-Programmable-Gate-Array-based (FPGA-based) SmartNICs [[316](#bib.bib316),
    [317](#bib.bib317)] can server as computational resources for offloading domain-specific
    tasks from GPUs, as well as mitigating network communication overhead in distributed
    training and inference scenarios. Guo et al. [[290](#bib.bib290)] have developed
    a heterogeneous SmartNIC system to address communication, memory, and computation
    bottlenecks inherent in the distributed training of Deep Learning Recommendation
    Models (DLRMs). To tackle the challenge of all-to-all communication in exchanging
    massive embedding tables within DLRMs, this system introduces a remote caching
    mechanism that buffers frequently used remote embedding lookup results on the
    SmartNIC, thereby reducing communication overhead during the feedforward process.
    This system further integrates local cache and in-SmartNIC computation mechanisms
    to access local embedding tables and execute irregular computations within the
    SmartNIC, obviating GPU intervention. These mechanisms alleviate memory bandwidth
    burdens and kernel launching overheads of GPUs effectively, optimizing both feedforward
    and backpropagation processing during the distributed training of DLRMs. On the
    other hand, FCsN [[291](#bib.bib291)] enhances distributed inference by offloading
    control logic, system scheduling, network communication, and neural network kernel
    computation to SmartNICs, eliminating CPU intervention. The strategic offloading
    maximizes the overlap of computation and network communication during distributed
    inference, facilitating non-conflict streaming neural network kernel executions
    at line rate.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 基于商用现场可编程门阵列（FPGA-based）的SmartNICs [[316](#bib.bib316), [317](#bib.bib317)]
    可以作为计算资源，将领域特定任务从GPU中卸载，同时减轻分布式训练和推理场景中的网络通信开销。郭等人[[290](#bib.bib290)] 开发了一种异构SmartNIC系统，以解决深度学习推荐模型（DLRMs）分布式训练中固有的通信、内存和计算瓶颈。为了应对在DLRMs中交换大量嵌入表的全对全通信挑战，该系统引入了一种远程缓存机制，将频繁使用的远程嵌入查找结果缓存在SmartNIC上，从而减少了前馈过程中的通信开销。该系统进一步集成了本地缓存和SmartNIC内计算机制，以访问本地嵌入表并在SmartNIC内执行不规则计算，从而无需GPU干预。这些机制有效缓解了GPU的内存带宽负担和内核启动开销，优化了DLRMs的分布式训练过程中的前馈和反向传播处理。另一方面，FCsN
    [[291](#bib.bib291)] 通过将控制逻辑、系统调度、网络通信和神经网络内核计算卸载到SmartNICs上，增强了分布式推理，消除了CPU干预。战略性地卸载最大化了分布式推理过程中计算与网络通信的重叠，实现了以行速率进行的无冲突流式神经网络内核执行。
- en: VI-C Inter-GPU Collective Communication
  id: totrans-599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C GPU间集体通信
- en: 'The efficiency of collective communication among GPUs plays a pivotal role
    in determining the overall performance of distributed DL. We introduce this topic
    in four dimensions: the collective communication library, optimization for heterogeneous
    network, optimization for sparse gradients, and the synthesis approach.'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: GPU之间的集体通信效率在决定分布式深度学习的整体性能中起着关键作用。我们从四个维度介绍这个话题：集体通信库、异构网络优化、稀疏梯度优化以及综合方法。
- en: VI-C1 Collective communication libraries
  id: totrans-601
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C1 集体通信库
- en: Collective communication libraries, such as Message Passing Interface (MPI) [[292](#bib.bib292)],
    Gloo [[293](#bib.bib293)], and Horovod [[294](#bib.bib294)], provide convenient
    programming interfaces for point-to-point and collective communication to exchange
    gradients on top of GPU interconnects. Specifically tailored to NVIDIA GPUs, NVIDIA
    Collective Communications Library (NCCL) [[295](#bib.bib295)] is designed for
    compatibility with MPI and optimized for delivering high bandwidth and low latency
    for multi-GPU and multi-node interconnects over PCIe and NVLink. In addition to
    point-to-point communication, NCCL supports various collective communication primitives,
    including but not limited to allgather, allreduce, and broadcast. Microsoft MSCCLang [[296](#bib.bib296)]
    provides a unified library for writing customized collective communication algorithms
    that can be compiled into NCCL built-in primitives. It enables high-performance
    customized collective implementations for various network environments. However,
    these collective communication libraries lack native-optimization for heterogeneous
    network environments and sparse gradient scenarios.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 集体通信库，如消息传递接口（MPI）[[292](#bib.bib292)]、Gloo [[293](#bib.bib293)] 和 Horovod [[294](#bib.bib294)]，提供了方便的编程接口用于点对点和集体通信，以便在
    GPU 互连上交换梯度。专为 NVIDIA GPU 量身定制的 NVIDIA 集体通信库（NCCL）[[295](#bib.bib295)] 旨在与 MPI
    兼容，并优化了在 PCIe 和 NVLink 上的多 GPU 和多节点互连的高带宽和低延迟。除了点对点通信外，NCCL 还支持各种集体通信原语，包括但不限于
    allgather、allreduce 和 broadcast。微软的 MSCCLang [[296](#bib.bib296)] 提供了一个统一的库，用于编写自定义的集体通信算法，这些算法可以编译成
    NCCL 内置原语。这使得在各种网络环境中实现高性能的自定义集体通信成为可能。然而，这些集体通信库缺乏对异构网络环境和稀疏梯度场景的本地优化。
- en: VI-C2 Collective communication in heterogeneous networks
  id: totrans-603
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C2 异构网络中的集体通信
- en: The performance of collective communication can be improved by fully utilizing
    network bandwidth in heterogeneous networks. BlueConnect [[297](#bib.bib297)]
    decomposes an allreduce operation into a series of reduce-scatter and all-gather
    primitives by leveraging the hierarchical topology information of communication
    bandwidth in a cluster. As a result, it minimizes the communication overhead for
    distributed training in heterogeneous networks and outperforms NCCL. However,
    BlueConnect is limited to symmetric topologies. Blink [[298](#bib.bib298)] further
    considers asymmetric topologies and heterogeneous links through the employment
    of packing spanning trees, optimizing the utilization of high-link bandwidth in
    the presence of topology heterogeneity. In the cloud-based network environment
    with an opaque network topology, Plink [[299](#bib.bib299)] probes the network
    as to capture physical network topology and bandwidth/latency constraints that
    are then utilized to optimize collective communication in data center networks.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 通过充分利用异构网络中的网络带宽，可以提高集体通信的性能。BlueConnect [[297](#bib.bib297)] 通过利用集群中通信带宽的层次拓扑信息，将
    allreduce 操作分解为一系列 reduce-scatter 和 all-gather 原语。结果是，它最小化了异构网络中分布式训练的通信开销，并且优于
    NCCL。然而，BlueConnect 限于对称拓扑。Blink [[298](#bib.bib298)] 通过采用打包跨度树来进一步考虑非对称拓扑和异构链路，优化了在存在拓扑异质性时高链路带宽的利用。在具有不透明网络拓扑的云网络环境中，Plink
    [[299](#bib.bib299)] 通过探测网络来捕获物理网络拓扑以及带宽/延迟约束，然后利用这些信息优化数据中心网络中的集体通信。
- en: VI-C3 Collective communication for sparse gradients
  id: totrans-605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C3 稀疏梯度的集体通信
- en: Gradient sparsity is a prevalent and substantial characteristic in numerous
    distributed DL models, and treating sparse gradients as dense data in collective
    communication can result in a waste of communication bandwidth. Unfortunately,
    most existing collective communication libraries are primarily designed for dense
    data communication and lack native supports for sparse data, including considerations
    for sparse data representation and aggregation in communication streams. Addressing
    this limitation, SparcML [[300](#bib.bib300)] supports streaming sparse data in
    index-value pair representations natively and designs an efficient method for
    sparse-sparse and sparse-dense summation. Sparse representations can switch to
    dense ones to ensure minimal communication cost during collective communication.
    To increase the parallelism for sparse data aggregation, OmniReduce [[301](#bib.bib301)]
    introduces a streaming-aggregation mechanism that splits sparse data into blocks
    and achieves high INA parallelism for blocks containing non-zero values, thereby
    reducing communication overhead and maximizing communication bandwidth utilization
    effectively. Imbalanced workloads for key-value pair collective operations within
    the cluster lead to straggler performance for sparse gradients. To handle imbalance
    allreduce operations for top-$k$ sparsified gradients, Ok-Topk [[175](#bib.bib175)]
    generates a workload balancing scheme across workers based on the information
    about consecutive buffer sizes on all workers. This workload-balancing approach
    makes collective communication for sparse gradients more scalable, whereas the
    additional communication overhead is narrowly bounded.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度稀疏性是许多分布式深度学习模型中普遍且显著的特征，将稀疏梯度视为密集数据进行集合通信可能导致通信带宽的浪费。不幸的是，大多数现有的集合通信库主要针对密集数据通信设计，缺乏对稀疏数据的原生支持，包括稀疏数据表示和通信流中的聚合考虑。针对这一限制，SparcML [[300](#bib.bib300)]
    原生支持以索引-值对表示的流式稀疏数据，并设计了一种高效的稀疏-稀疏和稀疏-密集求和方法。稀疏表示可以切换为密集表示，以确保集合通信中的最小通信成本。为了增加稀疏数据聚合的并行性，OmniReduce [[301](#bib.bib301)]
    引入了一种流式聚合机制，将稀疏数据划分为块，并实现了对包含非零值的块的高INA并行性，从而有效减少通信开销，最大限度地利用通信带宽。集群内键值对集合操作的不平衡工作负载导致稀疏梯度的性能瓶颈。为了处理顶级$k$稀疏梯度的工作负载不平衡allreduce操作，Ok-Topk [[175](#bib.bib175)]
    基于所有工作节点上的连续缓冲区大小信息生成了一种工作负载平衡方案。这种工作负载平衡方法使稀疏梯度的集合通信更加可扩展，同时额外的通信开销也被严格控制。
- en: The heterogeneity of cloud and FL environments poses new challenges to collective
    communication of sparse data. For cloud environments in which interconnects within
    a node are fast and those across nodes are slow in the GPU cluster, CommLib [[174](#bib.bib174)]
    adopts a hierarchical communication architecture for aggregating top-$k$ sparsified
    gradients to better utilize the GPU bandwidth within and across nodes. It also
    introduces a multi-level data caching scheme to reduce I/O on public clouds. For
    FL environments in which devices have constrained computational and communication
    resources and are separated geographically, the data volume of collective communication
    operations is a critical performance factor. To compress sparse gradients in FL
    environments, DeepReduce [[302](#bib.bib302)] decouples indices and values of
    sparse gradients into two sets, so each set can apply compressors that are optimized
    for its type independently. Specifically, DeepReduce introduces a Bloom-filter
    based compressor for indices and a curve-fitting based compressor for values,
    and significantly reduces the gradient volume compared to the traditional key-value
    pair representation.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 云环境和联邦学习环境的异质性对稀疏数据的集合通信提出了新的挑战。对于GPU集群中节点内部互连较快而节点间互连较慢的云环境，CommLib [[174](#bib.bib174)]
    采用了分层通信架构来聚合顶级$k$稀疏梯度，从而更好地利用节点内部和节点间的GPU带宽。它还引入了多级数据缓存方案，以减少公共云上的I/O。在计算和通信资源受限且地理位置分散的联邦学习环境中，集合通信操作的数据量是一个关键性能因素。为了压缩联邦学习环境中的稀疏梯度，DeepReduce [[302](#bib.bib302)]
    将稀疏梯度的索引和值解耦为两个集合，以便每个集合可以独立应用针对其类型优化的压缩器。具体来说，DeepReduce 引入了一种基于布隆过滤器的索引压缩器和一种基于曲线拟合的值压缩器，与传统的键值对表示相比，显著减少了梯度体积。
- en: VI-C4 Synthesizing optimal collective communication
  id: totrans-608
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-C4 合成最佳集合通信
- en: 'Recently, a rising trend of synthesis approaches aims at crafting collective
    communication algorithms for distributed DL. Given a network topology with bandwidth
    constraints on GPUs and edges, SCCL [[303](#bib.bib303)] synthesizes Pareto-efficient
    latency- and bandwidth-optimal collective implementations from scratch. However,
    SCCL lacks consideration for multi-rack network environments, impeding its scalability
    within a large-scale GPU cluster. To address this scalability issue, TACCL [[304](#bib.bib304)]
    enhances the synthesizer for multi-rack networks with heterogeneous links. On
    one hand, it uses a communication sketch containing information about the logical
    network topology and switch-hyperedge policy as the synthesizer input. On the
    other hand, TACCL adopts a heuristic approach, breaking down the problem into
    two independent steps: routing and scheduling. This division significantly reduces
    the search space, offering a pragmatic synthesis solution for collective communication
    in large-scale GPU clusters.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了一种合成方法的上升趋势，旨在为分布式深度学习设计集体通信算法。考虑到GPU和边缘的带宽限制，SCCL[[303](#bib.bib303)]
    从头合成帕累托有效的延迟和带宽最优集体实现。然而，SCCL缺乏对多机架网络环境的考虑，阻碍了其在大规模GPU集群中的扩展性。为了解决这一扩展性问题，TACCL[[304](#bib.bib304)]
    增强了多机架网络中异构链路的合成器。一方面，它使用包含逻辑网络拓扑和交换机超边政策的信息的通信草图作为合成器输入。另一方面，TACCL采用启发式方法，将问题分解为路由和调度两个独立步骤。这种划分显著减少了搜索空间，为大规模GPU集群中的集体通信提供了实际的合成解决方案。
- en: VI-D Communication Topologies
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 通信拓扑
- en: Enhancing communication topologies is crucial for leveraging full computational
    and communication capabilities of a large-scale cluster in distributed DL. Conventional
    high-performance computing topologies for data centers can be used for reasonable-scale
    distributed DL [[318](#bib.bib318)] that include fix topologies such as Fat-tree [[319](#bib.bib319)],
    BCube [[320](#bib.bib320)], Jellyfish [[321](#bib.bib321)], and DCell [[322](#bib.bib322)],
    and reconfigurable topologies such as Helios [[323](#bib.bib323)], c-Through [[324](#bib.bib324)],
    and OSA [[325](#bib.bib325)]. However, these topologies are primary designed for
    electrical and optical networks and encounter challenges related to bandwidth
    and cost when dealing with all-to-all communication patterns inherent in distributed
    DL scenarios at ultra-scale, involving hundreds or even thousands of GPUs [[23](#bib.bib23),
    [236](#bib.bib236)]. Recently, a number of communication topologies that address
    these challenges considering communication patterns of distributed DL and high
    bandwidth among GPU interconnects have been proposed.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 增强通信拓扑对于充分利用大规模集群在分布式深度学习中的计算和通信能力至关重要。传统的数据中心高性能计算拓扑可以用于合理规模的分布式深度学习[[318](#bib.bib318)]，包括固定拓扑如Fat-tree[[319](#bib.bib319)]、BCube[[320](#bib.bib320)]、Jellyfish[[321](#bib.bib321)]和DCell[[322](#bib.bib322)]，以及可重构拓扑如Helios[[323](#bib.bib323)]、c-Through[[324](#bib.bib324)]和OSA[[325](#bib.bib325)]。然而，这些拓扑主要设计用于电气和光学网络，并在处理分布式深度学习场景中固有的全对全通信模式时遇到带宽和成本相关的挑战，涉及到数百甚至上千个GPU[[23](#bib.bib23),
    [236](#bib.bib236)]。最近，提出了若干种通信拓扑，针对这些挑战考虑了分布式深度学习的通信模式和GPU互连的高带宽。
- en: VI-D1 Fixed topologies
  id: totrans-612
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-D1 固定拓扑
- en: To leverage the advantage of multiple NICs per computing node and provide a
    scalable and fault-tolerant network over Ethernet and commodity devices, BML [[305](#bib.bib305)]
    introduces a fully distributed gradient-synchronization algorithm on top of the
    BCube topology. However, it is optimized exclusively for data-parallel training.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用每个计算节点的多个NIC的优势，并提供可扩展和容错的以太网及商品设备网络，BML[[305](#bib.bib305)] 在BCube拓扑之上引入了一种完全分布式的梯度同步算法。然而，它仅针对数据并行训练进行了优化。
- en: To address the constraints of BML and pursue a communication topology that is
    both high-bandwidth while cost-effective for large-scale distributed training
    involving data and model parallelisms, HammingMesh [[306](#bib.bib306)] integrates
    concepts from Fat-tree and Torus [[309](#bib.bib309)]. This results in a two-dimensional
    topology with local and global connectivity. The local connectivity ensures high
    local bandwidth at low cost through the utilization of affordable Printed-Circuit-Board-mesh
    (PCB-mesh) interconnects, while the global connectivity can establish a global
    network by using a small number of sparsely connected switches to connect the
    meshes in rows and columns.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决BML的限制，并追求一种既具有高带宽又经济高效的通信拓扑，用于涉及数据和模型并行的大规模分布式训练，HammingMesh [[306](#bib.bib306)]结合了Fat-tree和Torus [[309](#bib.bib309)]的概念。这导致了一种具有本地和全球连接的二维拓扑。本地连接通过利用经济实惠的印刷电路板网状（PCB-mesh）互连来确保高本地带宽，而全球连接则通过使用少量稀疏连接的交换机来连接行和列中的网格，从而建立全球网络。
- en: VI-D2 Reconfigurable topologies
  id: totrans-615
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-D2 可重构拓扑
- en: 'The iterative nature of distributed training requires high bandwidth and low
    reconfiguration latency in reconfigurable topologies. To address these challenges,
    SiP-ML [[307](#bib.bib307)] proposes two topologies driven by Silicon Photonic
    (SiP) interfaces: SiP-OCS and SiP-Ring. SiP-OCS adopts a fully connected topology
    to deliver high bandwidth using commercially available optical circuit switches,
    connecting GPUs to all switches through Tbps SiP interfaces. SiP-Ring employs
    a switch-less ring topology, minimizing reconfiguration latency through the use
    of Micro-ring resonators embedded in SiP interfaces. On the other hand, TopoOpt[[308](#bib.bib308)]
    adopts an offline method to find the best communication topology by exploring
    from ring-allreduce permutations across a set of computing nodes, each equipped
    with multiple NICs, and reconfigures switches to realize this target topology.
    TopoOpt aims to co-optimize the network-topology and parallelism strategy for
    efficient large allreduce operations in both data and model parallelism modes.
    By employing alternating optimization, TopoOpt narrows down the search space for
    co-optimization targets strategically, ensuring a network topology that provides
    ample bandwidth and a parallelism strategy that requires only a small hop count
    during model-parallelism transfers.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练的迭代特性要求在可重构拓扑中具有高带宽和低重配置延迟。为了应对这些挑战，SiP-ML [[307](#bib.bib307)]提出了两种由硅光子（SiP）接口驱动的拓扑结构：SiP-OCS和SiP-Ring。SiP-OCS采用全连接拓扑，使用商业上可用的光电路交换机提供高带宽，通过Tbps
    SiP接口将GPU连接到所有交换机。SiP-Ring采用无交换机的环形拓扑，通过使用嵌入在SiP接口中的微环谐振器来最小化重配置延迟。另一方面，TopoOpt[[308](#bib.bib308)]采用离线方法，通过从一组计算节点中进行环形全规约排列的探索（每个计算节点配备多个NIC），找到最佳通信拓扑，并重新配置交换机以实现这一目标拓扑。TopoOpt旨在共同优化网络拓扑和并行策略，以高效地进行大规模全规约操作，无论是在数据还是模型并行模式下。通过采用交替优化，TopoOpt战略性地缩小了共同优化目标的搜索空间，确保提供充足带宽的网络拓扑和在模型并行传输中只需少量跳数的并行策略。
- en: VI-E Lessons Learned toward High-Performance Large-scale Communication Infrastructures
  id: totrans-617
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 高性能大规模通信基础设施的经验教训
- en: We discuss some lessons learned from existing technologies introduced in this
    section, helping researchers working toward high-performance communication infrastructures
    for large-scale distributed DL.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了本节中引入的现有技术的一些经验教训，帮助研究人员致力于大规模分布式深度学习的高性能通信基础设施。
- en: $\bullet$ High performance communication is monetarily costly, and economically
    affordable devices and topologies with best cost benefit are more practical. The
    ultimate pursuit of the highest performance, regardless of cost, is only applicable
    in some specialized cases. In most cases, we aim to achieve the best training
    throughput within a monetary budget for computational and communication resources.
    The optimal solution requires a co-design that involves adopting cost-effective
    communication devices and topologies for the high communication throughput and
    balancing computational and communication capacities for the overall training
    throughput.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 高性能通信在经济上成本高昂，经济实惠的设备和具有最佳成本效益的拓扑结构更为实际。追求最高性能而不顾成本的最终目标只适用于一些专业案例。在大多数情况下，我们的目标是在计算和通信资源的预算内实现最佳训练吞吐量。最佳解决方案需要一种共同设计，涉及采用具有高通信吞吐量的经济高效通信设备和拓扑结构，并在总体训练吞吐量上平衡计算和通信能力。
- en: $\bullet$ INA is efficient, and co-designing communication infrastructures and
    algorithms can reduce communication overhead and increase scheduling efficiency.
    Many modern communication devices also possess non-negligible computational capacity.
    With a thorough understanding of the characteristics of model synchronization
    in large-scale distributed DL, designing a solution incorporating in-network computation
    to reduce communication traffic and implementing efficient transport scheduling
    at the package, flow, or coflow level to enhance communication throughput is feasible.
    Challenges of such a solution include congestion-control strategies for INA, workload
    balancing among devices, fairness among multiple distributed DL jobs and tenants,
    and interference isolation of distributed DL traffic and other network traffic.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ INA是高效的，协同设计通信基础设施和算法可以减少通信开销并提高调度效率。许多现代通信设备也具备不可忽视的计算能力。通过深入了解大规模分布式深度学习中的模型同步特性，设计一个包含网络内计算以减少通信流量的解决方案，并在包、流或协流级别实施高效的传输调度以提高通信吞吐量是可行的。这种解决方案的挑战包括INA的拥塞控制策略、设备间的工作负载平衡、多种分布式深度学习任务和租户间的公平性，以及分布式深度学习流量和其他网络流量的干扰隔离。
- en: $\bullet$ Heterogeneous networks and sparse data are prevalent, and collective
    communication protocols should be fine-tuned for these scenarios. Collective communication
    implementations should take into account the heterogeneity in the bandwidth of
    network links, the computational capacity of network devices, and the topology
    to reduce communication overhead and enhance workload balancing, particularly
    in large-scale clusters. Given the widespread occurrence of data sparsity in large
    DL models, implementing efficient collective communication for sparse data atop
    heterogeneous networks is imperative. Implementing such efficient collective communication
    algorithms poses challenges, including discovering the true status of the underlying
    networks, routing and scheduling gradient aggregations in complex networks efficiently,
    and devising compatible libraries tailored for specialized high-performance interconnects.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 异构网络和稀疏数据普遍存在，因此集体通信协议应针对这些场景进行精细调整。集体通信实现应考虑网络链路带宽的异质性、网络设备的计算能力和拓扑结构，以减少通信开销并增强工作负载平衡，尤其是在大规模集群中。鉴于大型深度学习模型中数据稀疏的广泛存在，在异构网络上实现高效的集体通信对于稀疏数据至关重要。实现这种高效的集体通信算法面临挑战，包括发现底层网络的真实状态、在复杂网络中高效路由和调度梯度聚合，以及设计适用于专用高性能互连的兼容库。
- en: 'VII Large-Scale Distributed Training of Large Models: A Case Study on LLMs'
  id: totrans-622
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 大规模分布式训练大模型：LLM的案例研究
- en: Recently, LLMs [[59](#bib.bib59), [14](#bib.bib14), [326](#bib.bib326), [327](#bib.bib327),
    [328](#bib.bib328)] have been used successfully in various applications of NLP.
    They have also initiated the trend of developing ultra-large foundation models
    for domain-specific applications [[329](#bib.bib329), [330](#bib.bib330)] in diverse
    fields, including communications, computer science, and artificial intelligence.
    As ultra-large foundation models can contain tens of billions of parameters and
    take hundreds of GPUs and tens of days to train, the distributed training of these
    models at scale encounters more and tougher challenges than conventional distributed
    DL. By inspecting the cases of training LLMs to examine how to apply those communication-efficient
    technologies in practical scenarios, researchers can identify more intrinsic and
    literal research challenges for achieving high performance in large-scale distributed
    DL.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LLM[[59](#bib.bib59), [14](#bib.bib14), [326](#bib.bib326), [327](#bib.bib327),
    [328](#bib.bib328)]在各种自然语言处理应用中成功使用。它们还引发了为领域特定应用[[329](#bib.bib329), [330](#bib.bib330)]开发超大规模基础模型的趋势，涉及通信、计算机科学和人工智能等多个领域。由于超大规模基础模型可能包含数十亿个参数，训练需要数百个GPU和数十天的时间，因此这些模型的大规模分布式训练面临比传统分布式深度学习更为严峻的挑战。通过检查训练LLM的案例，研究人员可以探究如何在实际场景中应用那些高效的通信技术，从而识别出实现大规模分布式深度学习高性能的更多内在和实际的研究挑战。
- en: This section discusses practical aspects of the distributed training of LLMs
    by examining the real cases in [[331](#bib.bib331), [332](#bib.bib332), [333](#bib.bib333),
    [334](#bib.bib334), [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337),
    [338](#bib.bib338)]. The goal is to revisit key themes introduced earlier in this
    article and provide insights derived from related practices. Additionally, future
    trends in the use of large foundation models within the domain of communications
    are discussed. The presentation adopts a question-and-answer style.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过检查[[331](#bib.bib331), [332](#bib.bib332), [333](#bib.bib333), [334](#bib.bib334),
    [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337), [338](#bib.bib338)]中的实际案例，讨论LLMs分布式训练的实际方面。目标是重新审视本文早期介绍的关键主题，并提供从相关实践中得出的见解。此外，还讨论了在通信领域中大规模基础模型使用的未来趋势。报告采用问答风格。
- en: VII-A Model Synchronization
  id: totrans-625
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 模型同步
- en: $\bullet$ Can data-parallel training be communication-efficient for LLMs? Data-parallel
    training alone is not a good choice for LLMs with slow GPU interconnects, but
    is acceptable with high-performance interconnects. According to [[336](#bib.bib336)],
    concerning the volume of data to be transmitted across GPUs in data-parallel model
    synchronization, the time spent on communication can dominate the training time
    and leaves little chance for overlapping with the computation time. For example,
    training a 100-billion-parameter model on 100 GPUs in the data-parallel mode,
    the volume of transmitted data can reach 10 trillion parameters in one model synchronization
    round. Using 16-bit precision with slow interconnects of 1000 Mbps bandwidth,
    the time for one synchronization round is 1,600 seconds, which is completely unacceptable.
    But for very fast interconnects such as NVLink with 900 GBps point-to-point bandwidth,
    the synchronization time reduces to only 0.22 seconds, making it negligible if
    the synchronization frequency is reduced by local SGD.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 数据并行训练对于LLMs是否能够高效通信？仅仅依靠数据并行训练对具有慢GPU互连的LLMs并不是一个好的选择，但在高性能互连下是可以接受的。根据[[336](#bib.bib336)]，在数据并行模型同步中，传输数据的体积可能会使通信时间主导训练时间，并且几乎没有机会与计算时间重叠。例如，在数据并行模式下，在100个GPU上训练一个1000亿参数的模型，每次模型同步中传输的数据体积可以达到10万亿参数。使用1000
    Mbps带宽的慢互连时，一个同步回合的时间为1600秒，这完全不可接受。但对于例如NVLink这种具有900 GBps点对点带宽的非常快速互连，同步时间减少到仅0.22秒，如果通过本地SGD减少同步频率，几乎可以忽略不计。
- en: VII-B Communication Data Compression
  id: totrans-627
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 通信数据压缩
- en: $\bullet$ What is the effect of communication data compression on LLMs? Though
    applying simple implementations of quantization and sparsification algorithms
    can reduce the communication overhead and training time of LLM, the effect of
    adaptive and fine-grained compression methods remains insufficiently explored.
    According to SWARM [[336](#bib.bib336)], in the previous example with 100 billion
    parameters, the 16-bit precision can be reduced to 8-bit, resulting in the communication
    volume and time of model synchronization being only half of the origin values.
    On the contrary, adaptive and fine-grained compression algorithms can introduce
    significant additional decision and computation overhead to LLMs. Considering
    this complexity, caution should be exercised when we applying these algorithms
    to the distributed training of LLMs. The trade-off between computation and communication
    becomes more critical and requires further exploration in the large model scenario.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 通信数据压缩对LLMs的影响是什么？尽管应用简单的量化和稀疏化算法可以减少LLM的通信开销和训练时间，但自适应和精细化压缩方法的效果仍未充分探索。根据SWARM[[336](#bib.bib336)]，在之前的例子中，1000亿参数的16位精度可以减少到8位，从而使模型同步的通信体积和时间仅为原始值的一半。相反，自适应和精细化压缩算法可能会给LLMs引入显著的额外决策和计算开销。考虑到这种复杂性，当我们将这些算法应用于LLMs的分布式训练时，应谨慎对待。计算与通信之间的权衡变得更加关键，需要在大模型场景中进一步探索。
- en: $\bullet$ Can hybrid communication-data-compression algorithms perform effectively
    on LLMs? We can utilize hybrid communication-data-compression algorithms to reduce
    communication overhead on slow networks aggressively. For instance, CocktailSGD [[335](#bib.bib335)]
    employs random sparsification, top-$k$ sparsification, and quantization on the
    gradients sequentially when training an LLM with 20 billion parameters on a 500
    Mbps network. This hybrid method, achieving 117$\times$ compression, is only slightly
    slower than training on high-performance data center networks.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 混合通信-数据压缩算法能否在大型语言模型（LLMs）上有效运行？我们可以利用混合通信-数据压缩算法来大幅降低慢速网络上的通信开销。例如，CocktailSGD [[335](#bib.bib335)]
    在训练一个具有 200 亿参数的 LLM 时，采用了随机稀疏化、top-$k$ 稀疏化和梯度量化的混合方法，尽管压缩率达到了 117$\times$，但与在高性能数据中心网络上训练相比，仅稍慢。
- en: VII-C Resource Allocation and Task Scheduling
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 资源分配与任务调度
- en: $\bullet$ How crucial is pipeline parallelism for LLM training? Pipeline parallelism
    is essential for LLM training. As indicated in [[331](#bib.bib331)], by diminishing
    the impact of the communication volume and worker idle time during pipeline flushes,
    heuristic pipeline-parallelism proves effective in practice with trillion-scale
    LLMs on more than 3,000 GPU. In contrast to layer-slicing parallelism, multiple-layer-slicing
    pipeline parallelism only communicates end-of-layer activations and gradients,
    which can be 300$times$ smaller in the communication volume in a 2.2-billion-parameter
    example [[333](#bib.bib333)]. It is a common practice to use what is known as
    3D parallelism [[334](#bib.bib334)] for LLM training, which combines data, pipeline,
    and layer-slicing parallelisms, to maximize the training throughput.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 管道并行性对 LLM 训练有多重要？管道并行性对 LLM 训练至关重要。如 [[331](#bib.bib331)] 所示，通过减少通信量和管道清空期间的工作者空闲时间，启发式管道并行在超过
    3000 个 GPU 上的万亿规模 LLM 中表现出色。与层切分并行相比，多层切分管道并行仅通信层末激活和梯度，这在 22 亿参数的例子中通信量可以小 300$times$ [[333](#bib.bib333)]。在
    LLM 训练中，常常使用所谓的 3D 并行化 [[334](#bib.bib334)]，将数据、管道和层切分并行化相结合，以最大化训练吞吐量。
- en: $\bullet$ How to schedule tasks of LLM training on heterogeneous and low-bandwidth
    networks efficiently? LLMs can be trained on heterogeneous and low-bandwidth networks
    efficiently even with unstable worker devices, by employing fault-tolerant pipelines
    and redistributing tasks among workers. Tackling the issue of unstable links between
    workers, SWARM parallelism [[336](#bib.bib336)] employs decentralized model-parallelism
    to randomize fault-tolerant pipelines and rebalance workers between pipeline stages
    dynamically. The use of randomized fault-tolerant pipelines permits a worker in
    one pipeline stage to establish a connection to a stochastic worker in the next
    pipeline stage between different iterations. This approach enables the rerouting
    of tasks from a disconnected worker to other workers within the same swarm in
    the next iteration. This allows for the tolerance of worker failures in the pipeline,
    ensuring continuity in task execution. The dynamic-rebalancing strategy enables
    workers to switch between pipeline stages for the purpose of balancing the throughput
    of different stages in instances of dynamic membership with unstable workers.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 如何在异构和低带宽网络上高效调度 LLM 训练任务？即使在不稳定的工作设备下，利用容错管道和在工作者之间重新分配任务，LLMs 也可以在异构和低带宽网络上高效训练。SWARM
    并行化 [[336](#bib.bib336)] 通过采用去中心化的模型并行化来随机化容错管道，并在管道阶段之间动态地重新平衡工作者，解决了工作者之间的不稳定链接问题。使用随机化的容错管道允许一个管道阶段的工作者在不同的迭代中与下一个管道阶段的随机工作者建立连接。这种方法使得在下一次迭代中可以将任务从断开连接的工作者重新分配到同一群体中的其他工作者，从而容忍管道中的工作者失败，确保任务执行的连续性。动态平衡策略允许工作者在管道阶段之间切换，以平衡不同阶段的吞吐量，适用于动态成员和不稳定工作者的情况。
- en: $\bullet$ How crucial is fault-tolerant scheduling for LLM training? Given the
    involvement of a large number of workers in prolonged training sessions for LLMs,
    ensuring fault-tolerance is of utmost importance for efficient scheduling. Frequent
    failure in devices or networks can potentially block the training process, degrade
    the convergence performance, and necessitate redundant restarting of failed tasks
    and pipelines. SWARM parallelism [[336](#bib.bib336)] incorporates the dynamic
    membership of unstable workers into account for efficient pipeline scheduling.
    According to Oobleck [[337](#bib.bib337)], a failed pipeline can be recovered
    by using pipeline replicas and templates swiftly. We can instantiate some logically
    equivalent pipeline replicas, which possess replicated model states. Additionally,
    we define pipeline templates, which include information about the number of workers
    and stages in the pipeline, as well as the mapping of stages to GPUs. Once a pipeline
    failure occurs, a new pipeline can be restored based on the pipeline template
    and replicas instantly.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 对于LLM训练，容错调度有多重要？鉴于LLM训练过程中涉及大量工作者且训练时间较长，确保容错性对高效调度至关重要。设备或网络的频繁故障可能会阻碍训练过程，降低收敛性能，并且需要重复重新启动失败的任务和管道。SWARM并行性 [[336](#bib.bib336)]
    考虑了不稳定工作者的动态成员资格，以实现高效的管道调度。根据Oobleck [[337](#bib.bib337)]，可以通过快速使用管道副本和模板来恢复失败的管道。我们可以实例化一些逻辑上等效的管道副本，这些副本拥有复制的模型状态。此外，我们定义管道模板，包括关于工作者数量和管道阶段的信息，以及阶段到GPU的映射。一旦发生管道失败，可以基于管道模板和副本瞬时恢复新的管道。
- en: $\bullet$ Can we utilize GPU resources in decentralized networks for LLM training?
    LLMs can undergo training on GPUs in decentralized, heterogeneous, and low-bandwidth
    networks using optimized data and model partitioning scheme, along with pipeline
    scheduling. According to [[332](#bib.bib332)], there are numerous idle GPU clusters
    dispersed globally that can be utilized collectively for LLM training. We can
    divide the entire model into submodels, utilize a cost model to formulate the
    communication cost of both data and pipeline parallelisms based on the partitioning
    of these submodels, and partition submodels with a goal of minimizing this cost.
    For instance, when training GPT-3 [[59](#bib.bib59)] with 1.3 billion parameters,
    this scheduling strategy can achieve about 4$\times$ speedup in a global cloud
    across 8 worldwide regions, and is only 1.7-3.5$\times$ slower compared to the
    centralized data center solution with 100$\times$ faster networks.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们能否利用去中心化网络中的GPU资源进行LLM训练？LLM可以在去中心化、异构和低带宽的网络中，通过优化的数据和模型分区方案以及管道调度进行训练。根据 [[332](#bib.bib332)]，全球范围内存在众多闲置的GPU集群，可以集体用于LLM训练。我们可以将整个模型划分为子模型，利用成本模型根据这些子模型的分区制定数据和管道并行的通信成本，并以最小化该成本为目标对子模型进行分区。例如，当训练GPT-3 [[59](#bib.bib59)]时，具有13亿参数，该调度策略可以在全球8个区域的云环境中实现约4$\times$的加速，并且仅比具有100$\times$更快网络的集中式数据中心解决方案慢1.7-3.5$\times$。
- en: VII-D Communication Infrastructure
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 通信基础设施
- en: $\bullet$ How can we construct cost-effective communication infrastructures
    for LLM training? Commodity communication infrastructures can be cost-effective
    for LLM training when we employ optimized technologies at various levels for distributed
    DL. High-performance specialized communication infrastructures, such as NVIDIA
    A100 with NVLink interconnects, are frequently used for high-performance LLM training.
    However, considering the monetary cost, many researchers also use commodity networks,
    including data center networks, decentralized cloud infrastructures, and other
    low-bandwidth networks. When data, model, and pipeline parallelism modes are combined,
    and optimized algorithms for communication data compression, resource allocation,
    and task scheduling are applied, commodity communication infrastructures can also
    achieve comparable performance to some specialized ones regarding the cost.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 我们如何构建具有成本效益的LLM训练通信基础设施？当我们在分布式DL的各个层面采用优化技术时，商业通信基础设施可以具有成本效益。高性能的专用通信基础设施，如配备NVLink互连的NVIDIA
    A100，通常用于高性能LLM训练。然而，考虑到经济成本，许多研究人员还使用包括数据中心网络、去中心化云基础设施和其他低带宽网络在内的商业网络。当数据、模型和管道并行模式相结合，并应用通信数据压缩、资源分配和任务调度的优化算法时，商业通信基础设施也可以在成本方面实现与一些专用基础设施相媲美的性能。
- en: VII-E Large foundation Models for Communications
  id: totrans-637
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 大型基础模型在通信中的应用
- en: $\bullet$ What is the future trend of large foundation models within the domain
    of communications? Large foundation models can be trained with abundant domain-specific
    knowledge of communications to generate optimized strategies for diverse topics,
    including IoT, wireless networks, and network cybersecurity. We refer to such
    models as large communications models here. In the context of industrial IoT,
    large communications models can be exploited effectively in mobile edge computing
    (MEC) clusters to generate optimized codes for communication and scheduling strategies
    autonomously [[338](#bib.bib338)]. In the context of wireless networks, possessing
    capability to handle diverse multi-modal data, large communications models can
    conduct intricate data analyses for wireless sensor networks (WSN) based on unstructured
    mobile network logs, radio signals, and multimedia data, which are challenging
    for alternative models [[16](#bib.bib16)]. Leveraging rich domain knowledge, large
    communications models can also be utilized to improve solutions for various optimization
    tasks of communications, such as routing, network-resource allocation, and radio
    control. In the context of network cybersecurity, particularly within the topics
    of the Internet, WSN, and IoT, large communications models can be used to detect
    anomalies, intrusions, and Distributed Denial of Service (DDoS) attacks [[18](#bib.bib18)].
    They can also detect botnet attacks and generate verification strategies for anti-crawler
    protection. Overall, large communications models have the promising potential
    to extend their applications in various communications topics, enhancing the performance
    of existing technologies and innovating new solutions to address contemporary
    challenges.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 大型基础模型在通信领域的未来趋势是什么？大型基础模型可以通过丰富的通信领域特定知识进行训练，以生成针对物联网、无线网络和网络安全等各种主题的优化策略。我们在此将这些模型称为大型通信模型。在工业物联网（IoT）的背景下，大型通信模型可以在移动边缘计算（MEC）集群中有效利用，以自主生成通信和调度策略的优化代码[[338](#bib.bib338)]。在无线网络的背景下，具有处理多模态数据的能力，大型通信模型可以对无线传感器网络（WSN）进行复杂的数据分析，基于非结构化的移动网络日志、无线电信号和多媒体数据，这些是其他模型难以处理的[[16](#bib.bib16)]。利用丰富的领域知识，大型通信模型还可以用于改进各种通信优化任务的解决方案，如路由、网络资源分配和无线电控制。在网络安全的背景下，特别是在互联网、WSN和物联网主题中，大型通信模型可以用于检测异常、入侵和分布式拒绝服务（DDoS）攻击[[18](#bib.bib18)]。它们还可以检测僵尸网络攻击，并生成反爬虫保护的验证策略。总体而言，大型通信模型具有在各种通信主题中扩展应用的良好潜力，提高现有技术的性能，并创新新的解决方案来应对当代挑战。
- en: VIII Conclusion
  id: totrans-639
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: This paper surveys communication-efficient technologies at the algorithm, framework,
    and infrastructure levels for high-performance distributed DL in large-scale scenarios
    involving a large number of devices, large data, and large models, where heterogeneity
    and scalability are major research concerns. We cover various topics, including
    distributed model synchronization, communication data compression, resource allocation,
    task scheduling, as well as communication interconnects, devices, protocols, and
    topologies. During the discussion, We highlight the pros and cons of applying
    these technologies in a large-scale setting. We present promising future research
    trends for each topic. Moreover, a case study on the distributed training of large
    language models is showcased to illustrate the practical use of these communication-efficient
    technologies in real-world scenarios.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 本文调查了在涉及大量设备、大数据和大模型的大规模场景下，高性能分布式深度学习（DL）的算法、框架和基础设施层面的通信效率技术，其中异质性和可扩展性是主要的研究关注点。我们涵盖了各种主题，包括分布式模型同步、通信数据压缩、资源分配、任务调度，以及通信互连、设备、协议和拓扑结构。在讨论过程中，我们强调了在大规模环境中应用这些技术的利弊。我们展示了每个主题的有前景的未来研究趋势。此外，展示了一个关于大语言模型分布式训练的案例研究，以说明这些通信效率技术在现实世界场景中的实际应用。
- en: References
  id: totrans-641
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew, “Deep
    learning with cots hpc systems,” in *International conference on machine learning*.   PMLR,
    2013, pp. 1337–1345.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, 和 N. Andrew，“使用商用高性能计算系统的深度学习，”在*国际机器学习会议*上。
    PMLR, 2013, 第1337–1345页。'
- en: '[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，” 在 *IEEE计算机视觉与模式识别会议论文集*，2016年，页码770–778。'
- en: '[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力机制就是你所需要的，” *神经信息处理系统进展*，第30卷，2017年。'
- en: '[4] D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *3rd International Conference on Learning
    Representations, ICLR 2015*, 2015.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Bahdanau, K. H. Cho, 和 Y. Bengio，“通过联合学习对齐和翻译的神经机器翻译，” 在 *第三届国际学习表征会议，ICLR
    2015*，2015年。'
- en: '[5] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019, pp. 4171–4186.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. D. M.-W. C. Kenton 和 L. K. Toutanova，“Bert：用于语言理解的深度双向变换器预训练，” 在 *NAACL-HLT会议论文集*，2019年，页码4171–4186。'
- en: '[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像胜过16x16个单词：大规模图像识别的变换器，”
    在 *国际学习表征会议*，2020年。'
- en: '[7] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework
    for self-supervised learning of speech representations,” *Advances in neural information
    processing systems*, vol. 33, pp. 12 449–12 460, 2020.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Baevski, Y. Zhou, A. Mohamed, 和 M. Auli，“wav2vec 2.0：自监督学习语音表示的框架，”
    *神经信息处理系统进展*，第33卷，页码12 449–12 460，2020年。'
- en: '[8] T. Hollon, C. Jiang, A. Chowdury, M. Nasir-Moin, A. Kondepudi, A. Aabedi,
    A. Adapa, W. Al-Holou, J. Heth, O. Sagher *et al.*, “Artificial-intelligence-based
    molecular classification of diffuse gliomas using rapid, label-free optical imaging,”
    *Nature Medicine*, vol. 29, no. 4, pp. 828–832, 2023.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Hollon, C. Jiang, A. Chowdury, M. Nasir-Moin, A. Kondepudi, A. Aabedi,
    A. Adapa, W. Al-Holou, J. Heth, O. Sagher *等*，“基于人工智能的弥漫性胶质瘤分子分类，使用快速无标签光学成像，”
    *自然医学*，第29卷，第4期，页码828–832，2023年。'
- en: '[9] A. H. Thieme, Y. Zheng, G. Machiraju, C. Sadee, M. Mittermaier, M. Gertler,
    J. L. Salinas, K. Srinivasan, P. Gyawali, F. Carrillo-Perez *et al.*, “A deep-learning
    algorithm to classify skin lesions from mpox virus infection,” *Nature medicine*,
    vol. 29, no. 3, pp. 738–747, 2023.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. H. Thieme, Y. Zheng, G. Machiraju, C. Sadee, M. Mittermaier, M. Gertler,
    J. L. Salinas, K. Srinivasan, P. Gyawali, F. Carrillo-Perez *等*，“深度学习算法用于分类由mpox病毒感染引起的皮肤病变，”
    *自然医学*，第29卷，第3期，页码738–747，2023年。'
- en: '[10] C. Yan, J. Qin, Q. Liu, Q. Ma, and Y. Kang, “Mapless navigation with safety-enhanced
    imitation learning,” *IEEE Transactions on Industrial Electronics*, vol. 70, no. 7,
    pp. 7073–7081, 2022.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] C. Yan, J. Qin, Q. Liu, Q. Ma, 和 Y. Kang，“无地图导航与安全增强的模仿学习，” *IEEE工业电子学报*，第70卷，第7期，页码7073–7081，2022年。'
- en: '[11] K. Lee, D. Isele, E. A. Theodorou, and S. Bae, “Spatiotemporal costmap
    inference for mpc via deep inverse reinforcement learning,” *IEEE Robotics and
    Automation Letters*, vol. 7, no. 2, pp. 3194–3201, 2022.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K. Lee, D. Isele, E. A. Theodorou, 和 S. Bae，“通过深度逆强化学习进行MPC的时空成本图推断，”
    *IEEE机器人与自动化信函*，第7卷，第2期，页码3194–3201，2022年。'
- en: '[12] M. Aspri, G. Tsagkatakis, and P. Tsakalides, “Distributed training and
    inference of deep learning models for multi-modal land cover classification,”
    *Remote Sensing*, vol. 12, no. 17, p. 2670, 2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Aspri, G. Tsagkatakis, 和 P. Tsakalides，“用于多模态土地覆盖分类的深度学习模型的分布式训练和推断，”
    *遥感*，第12卷，第17期，页码2670，2020年。'
- en: '[13] J. M. Haut, M. E. Paoletti, S. Moreno-Álvarez, J. Plaza, J.-A. Rico-Gallego,
    and A. Plaza, “Distributed deep learning for remote sensing data interpretation,”
    *Proceedings of the IEEE*, vol. 109, no. 8, pp. 1320–1349, 2021.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. M. Haut, M. E. Paoletti, S. Moreno-Álvarez, J. Plaza, J.-A. Rico-Gallego,
    和 A. Plaza，“用于遥感数据解释的分布式深度学习，” *IEEE期刊*，第109卷，第8期，页码1320–1349，2021年。'
- en: '[14] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *et al.*, “Lamda: Language models for dialog
    applications,” *arXiv preprint arXiv:2201.08239*, 2022.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *等*，“Lamda：对话应用的语言模型，” *arXiv预印本 arXiv:2201.08239*，2022年。'
- en: '[15] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng, and C.-T. Chou,
    “Semisupervised distributed learning with non-iid data for aiot service platform,”
    *IEEE Internet of Things Journal*, vol. 7, no. 10, pp. 9266–9277, 2020.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng, 和 C.-T. Chou，“针对AIoT服务平台的半监督分布式学习与非独立同分布数据，”
    *IEEE物联网期刊*，第7卷，第10期，第9266–9277页，2020年。'
- en: '[16] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and wireless
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 3,
    pp. 2224–2287, 2019.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] C. Zhang, P. Patras, 和 H. Haddadi，“移动和无线网络中的深度学习：综述，” *IEEE通信调查与教程*，第21卷，第3期，第2224–2287页，2019年。'
- en: '[17] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim, “Applications of deep reinforcement learning in communications and
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 4,
    pp. 3133–3174, 2019.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, 和
    D. I. Kim，“深度强化学习在通信与网络中的应用：综述，” *IEEE通信调查与教程*，第21卷，第4期，第3133–3174页，2019年。'
- en: '[18] E. Rodríguez, B. Otero, N. Gutiérrez, and R. Canal, “A survey of deep
    learning techniques for cybersecurity in mobile networks,” *IEEE Communications
    Surveys & Tutorials*, vol. 23, no. 3, pp. 1920–1955, 2021.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] E. Rodríguez, B. Otero, N. Gutiérrez, 和 R. Canal，“针对移动网络中的网络安全的深度学习技术综述，”
    *IEEE通信调查与教程*，第23卷，第3期，第1920–1955页，2021年。'
- en: '[19] M. A. Al-Garadi, A. Mohamed, A. K. Al-Ali, X. Du, I. Ali, and M. Guizani,
    “A survey of machine and deep learning methods for internet of things (iot) security,”
    *IEEE Communications Surveys & Tutorials*, vol. 22, no. 3, pp. 1646–1685, 2020.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. A. Al-Garadi, A. Mohamed, A. K. Al-Ali, X. Du, I. Ali, 和 M. Guizani，“针对物联网（iot）安全的机器学习与深度学习方法综述，”
    *IEEE通信调查与教程*，第22卷，第3期，第1646–1685页，2020年。'
- en: '[20] E. Baccour, N. Mhaisen, A. A. Abdellatif, A. Erbad, A. Mohamed, M. Hamdi,
    and M. Guizani, “Pervasive ai for iot applications: A survey on resource-efficient
    distributed artificial intelligence,” *IEEE Communications Surveys & Tutorials*,
    vol. 24, no. 4, pp. 2366–2418, 2022.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] E. Baccour, N. Mhaisen, A. A. Abdellatif, A. Erbad, A. Mohamed, M. Hamdi,
    和 M. Guizani，“针对物联网应用的普适人工智能：资源高效分布式人工智能综述，” *IEEE通信调查与教程*，第24卷，第4期，第2366–2418页，2022年。'
- en: '[21] Y. Chen, X. Sun, and Y. Jin, “Communication-efficient federated deep learning
    with layerwise asynchronous model update and temporally weighted aggregation,”
    *IEEE Transactions on Neural Networks and Learning Systems*, vol. 31, no. 10,
    pp. 4229–4238, 2020.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Chen, X. Sun, 和 Y. Jin，“基于层级异步模型更新和时间加权聚合的通信高效联邦深度学习，” *IEEE神经网络与学习系统汇刊*，第31卷，第10期，第4229–4238页，2020年。'
- en: '[22] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy efficient
    federated learning over wireless communication networks,” *IEEE Transactions on
    Wireless Communications*, vol. 20, no. 3, pp. 1935–1949, 2020.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Z. Yang, M. Chen, W. Saad, C. S. Hong, 和 M. Shikh-Bahaei，“无线通信网络中的节能联邦学习，”
    *IEEE无线通信汇刊*，第20卷，第3期，第1935–1949页，2020年。'
- en: '[23] S. Wang, D. Li, J. Geng, Y. Gu, and Y. Cheng, “Impact of network topology
    on the performance of dml: Theoretical analysis and practical factors,” in *IEEE
    INFOCOM 2019-IEEE conference on computer communications*.   IEEE, 2019, pp. 1729–1737.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Wang, D. Li, J. Geng, Y. Gu, 和 Y. Cheng，“网络拓扑对分布式机器学习性能的影响：理论分析与实际因素，”
    收录于 *IEEE INFOCOM 2019-IEEE计算机通信会议*。IEEE，2019年，第1729–1737页。'
- en: '[24] J. Xue, Y. Miao, C. Chen, M. Wu, L. Zhang, and L. Zhou, “Fast distributed
    deep learning over rdma,” in *Proceedings of the Fourteenth EuroSys Conference
    2019*, 2019, pp. 1–14.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Xue, Y. Miao, C. Chen, M. Wu, L. Zhang, 和 L. Zhou，“基于RDMA的快速分布式深度学习，”
    收录于 *第十四届EuroSys会议论文集 2019*，2019年，第1–14页。'
- en: '[25] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and
    J. S. Rellermeyer, “A survey on distributed machine learning,” *Acm computing
    surveys (csur)*, vol. 53, no. 2, pp. 1–33, 2020.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, 和 J. S.
    Rellermeyer，“分布式机器学习综述，” *Acm计算机调查（csur）*，第53卷，第2期，第1–33页，2020年。'
- en: '[26] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep
    learning: An in-depth concurrency analysis,” *ACM Computing Surveys (CSUR)*, vol. 52,
    no. 4, pp. 1–43, 2019.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] T. Ben-Nun 和 T. Hoefler，“揭示并行与分布式深度学习的神秘面纱：深入的并发分析，” *ACM计算机调查（CSUR）*，第52卷，第4期，第1–43页，2019年。'
- en: '[27] R. Mayer and H.-A. Jacobsen, “Scalable deep learning on distributed infrastructures:
    Challenges, techniques, and tools,” *ACM Computing Surveys (CSUR)*, vol. 53, no. 1,
    pp. 1–37, 2020.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. Mayer 和 H.-A. Jacobsen，“在分布式基础设施上进行可扩展深度学习：挑战、技术与工具，” *ACM计算机调查（CSUR）*，第53卷，第1期，第1–37页，2020年。'
- en: '[28] E. P. Xing, Q. Ho, P. Xie, and D. Wei, “Strategies and principles of distributed
    machine learning on big data,” *Engineering*, vol. 2, no. 2, pp. 179–195, 2016.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] E. P. Xing, Q. Ho, P. Xie, 和 D. Wei，“大数据上的分布式机器学习策略与原则，” *Engineering*，第
    2 卷，第 2 期，页码 179–195，2016 年。'
- en: '[29] S. Ouyang, D. Dong, Y. Xu, and L. Xiao, “Communication optimization strategies
    for distributed deep neural network training: A survey,” *Journal of Parallel
    and Distributed Computing*, vol. 149, pp. 52–65, 2021.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Ouyang, D. Dong, Y. Xu, 和 L. Xiao，“分布式深度神经网络训练的通信优化策略：综述，” *Journal
    of Parallel and Distributed Computing*，第 149 卷，页码 52–65，2021 年。'
- en: '[30] E. Yu, D. Dong, and X. Liao, “Communication optimization algorithms for
    distributed deep learning systems: A survey,” *IEEE Transactions on Parallel and
    Distributed Systems*, vol. 34, no. 12, pp. 3294–3308, 2023.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] E. Yu, D. Dong, 和 X. Liao，“分布式深度学习系统的通信优化算法：综述，” *IEEE Transactions on
    Parallel and Distributed Systems*，第 34 卷，第 12 期，页码 3294–3308，2023 年。'
- en: '[31] X. Cao, T. Başar, S. Diggavi, Y. C. Eldar, K. B. Letaief, H. V. Poor,
    and J. Zhang, “Communication-efficient distributed learning: An overview,” *IEEE
    journal on selected areas in communications*, 2023.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] X. Cao, T. Başar, S. Diggavi, Y. C. Eldar, K. B. Letaief, H. V. Poor,
    和 J. Zhang，“通信高效的分布式学习：概述，” *IEEE journal on selected areas in communications*，2023
    年。'
- en: '[32] Z. Tang, S. Shi, X. Chu, W. Wang, and B. Li, “Communication-efficient
    distributed deep learning: A comprehensive survey,” *arXiv preprint arXiv:2003.06307*,
    2023.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Tang, S. Shi, X. Chu, W. Wang, 和 B. Li，“通信高效的分布式深度学习：综合综述，” *arXiv
    preprint arXiv:2003.06307*，2023 年。'
- en: '[33] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network intelligence
    at the edge,” *Proceedings of the IEEE*, vol. 107, no. 11, pp. 2204–2239, 2019.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Park, S. Samarakoon, M. Bennis, 和 M. Debbah，“边缘的无线网络智能，” *Proceedings
    of the IEEE*，第 107 卷，第 11 期，页码 2204–2239，2019 年。'
- en: '[34] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence:
    Paving the last mile of artificial intelligence with edge computing,” *Proceedings
    of the IEEE*, vol. 107, no. 8, pp. 1738–1762, 2019.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, 和 J. Zhang，“边缘智能：用边缘计算铺平人工智能的最后一公里，”
    *Proceedings of the IEEE*，第 107 卷，第 8 期，页码 1738–1762，2019 年。'
- en: '[35] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-efficient
    edge ai: Algorithms and systems,” *IEEE Communications Surveys & Tutorials*, vol. 22,
    no. 4, pp. 2167–2191, 2020.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Shi, K. Yang, T. Jiang, J. Zhang, 和 K. B. Letaief，“通信高效的边缘 AI：算法与系统，”
    *IEEE Communications Surveys & Tutorials*，第 22 卷，第 4 期，页码 2167–2191，2020 年。'
- en: '[36] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, “Edge
    intelligence: The confluence of edge computing and artificial intelligence,” *IEEE
    Internet of Things Journal*, vol. 7, no. 8, pp. 7457–7469, 2020.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, 和 A. Y. Zomaya，“边缘智能：边缘计算与人工智能的汇流，”
    *IEEE Internet of Things Journal*，第 7 卷，第 8 期，页码 7457–7469，2020 年。'
- en: '[37] M. Chen, N. Shlezinger, H. V. Poor, Y. C. Eldar, and S. Cui, “Communication-efficient
    federated learning,” *Proceedings of the National Academy of Sciences*, vol. 118,
    no. 17, p. e2024789118, 2021.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] M. Chen, N. Shlezinger, H. V. Poor, Y. C. Eldar, 和 S. Cui，“通信高效的联邦学习，”
    *Proceedings of the National Academy of Sciences*，第 118 卷，第 17 期，页码 e2024789118，2021
    年。'
- en: '[38] M. S. Murshed, C. Murphy, D. Hou, N. Khan, G. Ananthanarayanan, and F. Hussain,
    “Machine learning at the network edge: A survey,” *ACM Computing Surveys (CSUR)*,
    vol. 54, no. 8, pp. 1–37, 2021.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. S. Murshed, C. Murphy, D. Hou, N. Khan, G. Ananthanarayanan, 和 F. Hussain，“网络边缘的机器学习：综述，”
    *ACM Computing Surveys (CSUR)*，第 54 卷，第 8 期，页码 1–37，2021 年。'
- en: '[39] J. Liu, J. Huang, Y. Zhou, X. Li, S. Ji, H. Xiong, and D. Dou, “From distributed
    machine learning to federated learning: A survey,” *Knowledge and Information
    Systems*, vol. 64, no. 4, pp. 885–917, 2022.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Liu, J. Huang, Y. Zhou, X. Li, S. Ji, H. Xiong, 和 D. Dou，“从分布式机器学习到联邦学习：综述，”
    *Knowledge and Information Systems*，第 64 卷，第 4 期，页码 885–917，2022 年。'
- en: '[40] S. Duan, D. Wang, J. Ren, F. Lyu, Y. Zhang, H. Wu, and X. Shen, “Distributed
    artificial intelligence empowered by end-edge-cloud computing: A survey,” *IEEE
    Communications Surveys & Tutorials*, 2022.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Duan, D. Wang, J. Ren, F. Lyu, Y. Zhang, H. Wu, 和 X. Shen，“由端边云计算赋能的分布式人工智能：综述，”
    *IEEE Communications Surveys & Tutorials*，2022 年。'
- en: '[41] J. Yao, S. Zhang, Y. Yao, F. Wang, J. Ma, J. Zhang, Y. Chu, L. Ji, K. Jia,
    T. Shen *et al.*, “Edge-cloud polarization and collaboration: A comprehensive
    survey for ai,” *IEEE Transactions on Knowledge and Data Engineering*, vol. 35,
    no. 7, pp. 6866–6886, 2022.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Yao, S. Zhang, Y. Yao, F. Wang, J. Ma, J. Zhang, Y. Chu, L. Ji, K.
    Jia, T. Shen *等*，“边缘云极化与协作：人工智能的全面综述，” *IEEE Transactions on Knowledge and Data
    Engineering*，第 35 卷，第 7 期，页码 6866–6886，2022 年。'
- en: '[42] Y. E. Sagduyu, S. Ulukus, and A. Yener, “Task-oriented communications
    for nextg: End-to-end deep learning and ai security aspects,” *IEEE Wireless Communications*,
    vol. 30, no. 3, pp. 52–60, 2023.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. E. Sagduyu, S. Ulukus, 和 A. Yener, “面向任务的下一代通信: 端到端深度学习和 AI 安全方面，”
    *IEEE 无线通信*, 第30卷，第3期, 页码 52–60, 2023年。'
- en: '[43] P. Saikia, S. Biswas, K. Singh, and C.-P. Li, “Signal detection in gsm-based
    in-band full-duplex communication using dnn,” *IEEE Transactions on Vehicular
    Technology*, vol. 72, no. 2, pp. 2661–2666, 2022.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Saikia, S. Biswas, K. Singh, 和 C.-P. Li, “使用 DNN 的 GSM 基带全双工通信中的信号检测，”
    *IEEE 车载技术汇刊*, 第72卷，第2期, 页码 2661–2666, 2022年。'
- en: '[44] P. Ferrand, A. Decurninge, and M. Guillaud, “Dnn-based localization from
    channel estimates: Feature design and experimental results,” in *GLOBECOM 2020-2020
    IEEE Global Communications Conference*.   IEEE, 2020, pp. 1–6.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] P. Ferrand, A. Decurninge, 和 M. Guillaud, “基于 DNN 的通道估计定位: 特征设计和实验结果，”
    收录于 *GLOBECOM 2020-2020 IEEE 全球通信大会*。IEEE, 2020年, 页码 1–6。'
- en: '[45] I. Amerini, C.-T. Li, and R. Caldelli, “Social network identification
    through image classification with cnn,” *IEEE access*, vol. 7, pp. 35 264–35 273,
    2019.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] I. Amerini, C.-T. Li, 和 R. Caldelli, “通过 CNN 的图像分类进行社交网络识别，” *IEEE Access*,
    第7卷, 页码 35,264–35,273, 2019年。'
- en: '[46] C. Zhang, W. Jiang, Y. Zhang, W. Wang, Q. Zhao, and C. Wang, “Transformer
    and cnn hybrid deep neural network for semantic segmentation of very-high-resolution
    remote sensing imagery,” *IEEE Transactions on Geoscience and Remote Sensing*,
    vol. 60, pp. 1–20, 2022.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] C. Zhang, W. Jiang, Y. Zhang, W. Wang, Q. Zhao, 和 C. Wang, “用于语义分割的 Transformer
    和 CNN 混合深度神经网络,” *IEEE 地球科学与遥感汇刊*, 第60卷, 页码 1–20, 2022年。'
- en: '[47] R. Bi, J. Xiong, Y. Tian, Q. Li, and K.-K. R. Choo, “Achieving lightweight
    and privacy-preserving object detection for connected autonomous vehicles,” *IEEE
    Internet of Things Journal*, vol. 10, no. 3, pp. 2314–2329, 2022.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Bi, J. Xiong, Y. Tian, Q. Li, 和 K.-K. R. Choo, “实现轻量级和隐私保护的面向连接自动驾驶车辆的目标检测，”
    *IEEE 物联网杂志*, 第10卷，第3期, 页码 2314–2329, 2022年。'
- en: '[48] Y. Hua, Z. Zhao, R. Li, X. Chen, Z. Liu, and H. Zhang, “Deep learning
    with long short-term memory for time series prediction,” *IEEE Communications
    Magazine*, vol. 57, no. 6, pp. 114–119, 2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Hua, Z. Zhao, R. Li, X. Chen, Z. Liu, 和 H. Zhang, “使用长短期记忆的深度学习进行时间序列预测，”
    *IEEE 通信杂志*, 第57卷，第6期, 页码 114–119, 2019年。'
- en: '[49] Q. Feng, D. He, Z. Liu, H. Wang, and K.-K. R. Choo, “Securenlp: A system
    for multi-party privacy-preserving natural language processing,” *IEEE Transactions
    on Information Forensics and Security*, vol. 15, pp. 3709–3721, 2020.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Q. Feng, D. He, Z. Liu, H. Wang, 和 K.-K. R. Choo, “Securenlp: 一个多方隐私保护自然语言处理系统,”
    *IEEE 信息取证与安全汇刊*, 第15卷, 页码 3709–3721, 2020年。'
- en: '[50] B. Say, “A unified framework for planning with learned neural network
    transition models,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 35, no. 6, 2021, pp. 5016–5024.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] B. Say, “具有学习神经网络过渡模型的统一规划框架，” 收录于 *AAAI 人工智能大会论文集*, 第35卷，第6期, 2021年,
    页码 5016–5024。'
- en: '[51] M. Li, T. Zhang, Y. Chen, and A. J. Smola, “Efficient mini-batch training
    for stochastic optimization,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014, pp. 661–670.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] M. Li, T. Zhang, Y. Chen, 和 A. J. Smola, “用于随机优化的高效小批量训练，” 收录于 *第20届 ACM
    SIGKDD 国际知识发现与数据挖掘会议论文集*, 2014年, 页码 661–670。'
- en: '[52] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
    initialization and momentum in deep learning,” in *Proceedings of the 30th International
    Conference on Machine Learning*, vol. 28, no. 3.   Atlanta, Georgia, USA: PMLR,
    17–19 Jun 2013, pp. 1139–1147.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] I. Sutskever, J. Martens, G. Dahl, 和 G. Hinton, “深度学习中初始化和动量的重要性，” 收录于
    *第30届国际机器学习会议论文集*, 第28卷，第3期。亚特兰大, Georgia, USA: PMLR, 2013年6月17–19日, 页码 1139–1147。'
- en: '[53] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online
    learning and stochastic optimization.” *Journal of machine learning research*,
    vol. 12, no. 7, 2011.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Duchi, E. Hazan, 和 Y. Singer, “用于在线学习和随机优化的自适应子梯度方法。” *机器学习研究杂志*, 第12卷，第7期,
    2011年。'
- en: '[54] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *3rd International Conference on Learning Representations, ICLR 2015, San Diego,
    CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. P. Kingma 和 J. Ba, “Adam: 一种随机优化方法，” 收录于 *第3届国际学习表示会议, ICLR 2015, 圣地亚哥,
    CA, USA, 2015年5月7-9日, 会议论文集*, 2015年。'
- en: '[55] M. Lotfollahi, M. Jafari Siavoshani, R. Shirali Hossein Zade, and M. Saberian,
    “Deep packet: A novel approach for encrypted traffic classification using deep
    learning,” *Soft Computing*, vol. 24, no. 3, pp. 1999–2012, 2020.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Lotfollahi, M. Jafari Siavoshani, R. Shirali Hossein Zade 和 M. Saberian，“Deep
    Packet：一种基于深度学习的加密流量分类新方法，”*软计算*，第24卷，第3期，第1999–2012页，2020年。'
- en: '[56] L. Vu, Q. U. Nguyen, D. N. Nguyen, D. T. Hoang, E. Dutkiewicz *et al.*,
    “Learning latent representation for iot anomaly detection,” *IEEE Transactions
    on Cybernetics*, vol. 52, no. 5, pp. 3769–3782, 2020.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. Vu, Q. U. Nguyen, D. N. Nguyen, D. T. Hoang, E. Dutkiewicz *等*，“为物联网异常检测学习潜在表示，”*IEEE网络学报*，第52卷，第5期，第3769–3782页，2020年。'
- en: '[57] G. Li, M. Müller, B. Ghanem, and V. Koltun, “Training graph neural networks
    with 1000 layers,” in *International conference on machine learning*.   PMLR,
    2021, pp. 6437–6449.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] G. Li, M. Müller, B. Ghanem 和 V. Koltun，“训练1000层的图神经网络，”发表于*国际机器学习会议*。PMLR，2021年，第6437–6449页。'
- en: '[58] B. Hanin, “Which neural net architectures give rise to exploding and vanishing
    gradients?” *Advances in neural information processing systems*, vol. 31, 2018.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] B. Hanin，“哪些神经网络架构会导致梯度爆炸和消失？”*神经信息处理系统进展*，第31卷，2018年。'
- en: '[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少样本学习者，”*神经信息处理系统进展*，第33卷，第1877–1901页，2020年。'
- en: '[60] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania, and S. Chintala, “Pytorch distributed: Experiences
    on accelerating data parallel training,” *Proc. VLDB Endow.*, vol. 13, no. 12,
    p. 3005–3018, aug 2020.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania 和 S. Chintala，“PyTorch分布式：加速数据并行训练的经验，”*VLDB会议录*，第13卷，第12期，第3005–3018页，2020年8月。'
- en: '[61] M. Li, D. G. Andersen, A. J. Smola, and K. Yu, “Communication efficient
    distributed machine learning with the parameter server,” in *Advances in Neural
    Information Processing Systems*, vol. 27, 2014, pp. 19–27.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] M. Li, D. G. Andersen, A. J. Smola 和 K. Yu，“通过参数服务器进行通信高效的分布式机器学习，”发表于*神经信息处理系统进展*，第27卷，2014年，第19–27页。'
- en: '[62] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst, J. Dean, and Y. Gal,
    “Interlocking backpropagation: Improving depthwise model-parallelism,” *J. Mach.
    Learn. Res.*, vol. 23, no. 1, jan 2022.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst, J. Dean 和 Y. Gal，“互锁反向传播：改进深度模型并行性，”*机器学习研究*，第23卷，第1期，2022年1月。'
- en: '[63] S. Li and T. Hoefler, “Chimera: efficiently training large-scale neural
    networks with bidirectional pipelines,” in *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis*, 2021, pp. 1–14.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Li 和 T. Hoefler，“Chimera：通过双向管道高效训练大规模神经网络，”发表于*国际高性能计算、网络、存储和分析会议论文集*，2021年，第1–14页。'
- en: '[64] W. Liu, Z. Lai, S. Li, Y. Duan, K. Ge, and D. Li, “Autopipe: A fast pipeline
    parallelism approach with balanced partitioning and micro-batch slicing,” in *2022
    IEEE International Conference on Cluster Computing (CLUSTER)*.   IEEE, 2022, pp.
    301–312.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] W. Liu, Z. Lai, S. Li, Y. Duan, K. Ge 和 D. Li，“Autopipe：一种具有平衡分区和微批处理切片的快速管道并行方法，”发表于*2022
    IEEE 国际集群计算会议 (CLUSTER)*。IEEE，2022年，第301–312页。'
- en: '[65] H. Oh, J. Lee, H. Kim, and J. Seo, “Out-of-order backprop: An effective
    scheduling technique for deep learning,” in *Proceedings of the Seventeenth European
    Conference on Computer Systems*, 2022, pp. 435–452.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] H. Oh, J. Lee, H. Kim 和 J. Seo，“无序反向传播：一种有效的深度学习调度技术，”发表于*第十七届欧洲计算机系统会议论文集*，2022年，第435–452页。'
- en: '[66] J. M. Tarnawski, D. Narayanan, and A. Phanishayee, “Piper: Multidimensional
    planner for dnn parallelization,” *Advances in Neural Information Processing Systems*,
    vol. 34, pp. 24 829–24 840, 2021.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. M. Tarnawski, D. Narayanan 和 A. Phanishayee，“Piper：用于DNN并行化的多维规划器，”*神经信息处理系统进展*，第34卷，第24,829–24,840页，2021年。'
- en: '[67] C. Unger, Z. Jia, W. Wu, S. Lin, M. Baines, C. E. Q. Narvaez, V. Ramakrishnaiah,
    N. Prajapati, P. McCormick, J. Mohd-Yusof *et al.*, “Unity: Accelerating dnn training
    through joint optimization of algebraic transformations and parallelization,”
    in *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22)*, 2022, pp. 267–284.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] C. Unger, Z. Jia, W. Wu, S. Lin, M. Baines, C. E. Q. Narvaez, V. Ramakrishnaiah,
    N. Prajapati, P. McCormick, J. Mohd-Yusof *等*，“Unity：通过代数变换和并行化的联合优化加速DNN训练，”发表于*第16届USENIX操作系统设计与实现研讨会
    (OSDI 22)*，2022年，第267–284页。'
- en: '[68] J. Zhou, Q. Shi, Y. Ding, L. Wang, L. Li, and F. Zhu, “Anttune: An efficient
    distributed hyperparameter optimization system for large-scale data,” in *International
    Conference on Database Systems for Advanced Applications*.   Springer, 2023, pp.
    477–489.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Zhou, Q. Shi, Y. Ding, L. Wang, L. Li, 和 F. Zhu，"Anttune：一个高效的分布式超参数优化系统，适用于大规模数据"，在*高级应用数据库系统国际会议*，Springer，2023年，第477–489页。'
- en: '[69] A. V. Joshi and A. V. Joshi, “Amazon’s machine learning toolkit: Sagemaker,”
    *Machine learning and artificial intelligence*, pp. 233–243, 2020.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. V. Joshi 和 A. V. Joshi，"亚马逊的机器学习工具包：Sagemaker"，*机器学习与人工智能*，第233–243页，2020年。'
- en: '[70] W. Ma, T. Zhou, J. Qin, X. Xiang, Y. Tan, and Z. Cai, “A privacy-preserving
    content-based image retrieval method based on deep learning in cloud computing,”
    *Expert Systems with Applications*, vol. 203, p. 117508, 2022.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] W. Ma, T. Zhou, J. Qin, X. Xiang, Y. Tan, 和 Z. Cai，"一种基于深度学习的隐私保护内容检索方法，应用于云计算"，*专家系统与应用*，第203卷，第117508页，2022年。'
- en: '[71] F. Desai, D. Chowdhury, R. Kaur, M. Peeters, R. C. Arya, G. S. Wander,
    S. S. Gill, and R. Buyya, “Healthcloud: A system for monitoring health status
    of heart patients using machine learning and cloud computing,” *Internet of Things*,
    vol. 17, p. 100485, 2022.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] F. Desai, D. Chowdhury, R. Kaur, M. Peeters, R. C. Arya, G. S. Wander,
    S. S. Gill, 和 R. Buyya，"Healthcloud：一个通过机器学习和云计算监控心脏病患者健康状况的系统"，*物联网*，第17卷，第100485页，2022年。'
- en: '[72] X. Wang, L. Zhang, Y. Liu, C. Zhao, and K. Wang, “Solving task scheduling
    problems in cloud manufacturing via attention mechanism and deep reinforcement
    learning,” *Journal of Manufacturing Systems*, vol. 65, pp. 452–468, 2022.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. Wang, L. Zhang, Y. Liu, C. Zhao, 和 K. Wang，"通过注意力机制和深度强化学习解决云制造中的任务调度问题"，*制造系统期刊*，第65卷，第452–468页，2022年。'
- en: '[73] L. Zhang, C. Yang, Y. Yan, and Y. Hu, “Distributed real-time scheduling
    in cloud manufacturing by deep reinforcement learning,” *IEEE Transactions on
    Industrial Informatics*, vol. 18, no. 12, pp. 8999–9007, 2022.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Zhang, C. Yang, Y. Yan, 和 Y. Hu，"基于深度强化学习的云制造分布式实时调度"，*IEEE工业信息学事务*，第18卷，第12期，第8999–9007页，2022年。'
- en: '[74] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse:
    Visions, enabling technologies, and challenges,” *IEEE Communications Surveys
    & Tutorials*, 2022.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, 和 C. Miao，"全面探索实现边缘驱动的元宇宙：愿景、支持技术与挑战"，*IEEE通信调查与教程*，2022年。'
- en: '[75] P. Arthurs, L. Gillam, P. Krause, N. Wang, K. Halder, and A. Mouzakitis,
    “A taxonomy and survey of edge cloud computing for intelligent transportation
    systems and connected vehicles,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 23, no. 7, pp. 6206–6221, 2022.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] P. Arthurs, L. Gillam, P. Krause, N. Wang, K. Halder, 和 A. Mouzakitis，"智能交通系统和连接车辆的边缘云计算分类与调查"，*IEEE智能交通系统事务*，第23卷，第7期，第6206–6221页，2022年。'
- en: '[76] Y. Wu, B. Yang, D. Zhu, Q. Liu, C. Li, C. Chen, and X. Guan, “To transmit
    or predict: An efficient industrial data transmission scheme with deep learning
    and cloud-edge collaboration,” *IEEE Transactions on Industrial Informatics*,
    vol. 19, no. 11, pp. 11 322–11 332, 2023.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Wu, B. Yang, D. Zhu, Q. Liu, C. Li, C. Chen, 和 X. Guan，"传输还是预测：一种高效的工业数据传输方案，结合深度学习和云边协作"，*IEEE工业信息学事务*，第19卷，第11期，第11 322–11 332页，2023年。'
- en: '[77] Q. Wu, X. Chen, Z. Zhou, and J. Zhang, “Fedhome: Cloud-edge based personalized
    federated learning for in-home health monitoring,” *IEEE Transactions on Mobile
    Computing*, vol. 21, no. 8, pp. 2818–2832, 2022.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Q. Wu, X. Chen, Z. Zhou, 和 J. Zhang，"Fedhome：基于云-边缘的个性化联合学习，用于家庭健康监测"，*IEEE移动计算事务*，第21卷，第8期，第2818–2832页，2022年。'
- en: '[78] J. A. Alzubi, O. A. Alzubi, A. Singh, and M. Ramachandran, “Cloud-iiot-based
    electronic health record privacy-preserving by cnn and blockchain-enabled federated
    learning,” *IEEE Transactions on Industrial Informatics*, vol. 19, no. 1, pp.
    1080–1087, 2023.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. A. Alzubi, O. A. Alzubi, A. Singh, 和 M. Ramachandran，"基于云-物联网的电子健康记录隐私保护，采用CNN和区块链支持的联合学习"，*IEEE工业信息学事务*，第19卷，第1期，第1080–1087页，2023年。'
- en: '[79] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne,
    J. Li, D. Niyato, and H. V. Poor, “Federated learning meets blockchain in edge
    computing: Opportunities and challenges,” *IEEE Internet of Things Journal*, vol. 8,
    no. 16, pp. 12 806–12 825, 2021.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne,
    J. Li, D. Niyato, 和 H. V. Poor，"联合学习与区块链在边缘计算中的结合：机会与挑战"，*IEEE物联网期刊*，第8卷，第16期，第12 806–12 825页，2021年。'
- en: '[80] Y. Li, X. Tao, X. Zhang, J. Liu, and J. Xu, “Privacy-preserved federated
    learning for autonomous driving,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 23, no. 7, pp. 8423–8434, 2022.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. 李, X. 陶, X. 张, J. 刘, 和 J. 许, “用于自主驾驶的隐私保护联邦学习,” *IEEE 智能交通系统学报*, 卷
    23, 第 7 期, 页 8423–8434, 2022年。'
- en: '[81] O. Bouachir, M. Aloqaily, Ö. Özkasap, and F. Ali, “Federatedgrids: Federated
    learning and blockchain-assisted p2p energy sharing,” *IEEE Transactions on Green
    Communications and Networking*, vol. 6, no. 1, pp. 424–436, 2022.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] O. 布阿希尔, M. 阿洛凯利, Ö. 厄兹卡萨普, 和 F. 阿里, “Federatedgrids: 联邦学习和区块链辅助的 P2P
    能量共享,” *IEEE 绿色通信与网络学报*, 卷 6, 第 1 期, 页 424–436, 2022年。'
- en: '[82] S. Batra, Z. Huang, A. Petrenko, T. Kumar, A. Molchanov, and G. S. Sukhatme,
    “Decentralized control of quadrotor swarms with end-to-end deep reinforcement
    learning,” in *Proceedings of the 5th Conference on Robot Learning*, vol. 164.   PMLR,
    08–11 Nov 2022, pp. 576–586.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. 巴特拉, Z. 黄, A. 佩特连科, T. 库马尔, A. 莫尔查诺夫, 和 G. S. 苏赫特梅, “通过端到端深度强化学习对四旋翼蜂群的去中心化控制,”
    在 *第五届机器人学习会议论文集*, 卷 164. PMLR, 2022年11月8–11日, 页 576–586。'
- en: '[83] X. Zhou, W. Liang, K. I.-K. Wang, Z. Yan, L. T. Yang, W. Wei, J. Ma, and
    Q. Jin, “Decentralized p2p federated learning for privacy-preserving and resilient
    mobile robotic systems,” *IEEE Wireless Communications*, vol. 30, no. 2, pp. 82–89,
    2023.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] X. 周, W. 梁, K. I.-K. 王, Z. 燕, L. T. 杨, W. 魏, J. 马, 和 Q. 金, “用于隐私保护和韧性的移动机器人系统的去中心化
    P2P 联邦学习,” *IEEE 无线通信*, 卷 30, 第 2 期, 页 82–89, 2023年。'
- en: '[84] D. Liu, X. Chen, Z. Zhou, and Q. Ling, “Hiertrain: Fast hierarchical edge
    ai learning with hybrid parallelism in mobile-edge-cloud computing,” *IEEE Open
    Journal of the Communications Society*, vol. 1, pp. 634–645, 2020.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] D. 刘, X. 陈, Z. 周, 和 Q. 凌, “Hiertrain: 移动边缘云计算中的混合并行加速层次边缘 AI 学习,” *IEEE
    开放通信学会期刊*, 卷 1, 页 634–645, 2020年。'
- en: '[85] Z. Chen, L. Shi, X. Liu, J. Li, S. Liu, and Y. Xu, “Osp: Boosting distributed
    model training with 2-stage synchronization,” in *Proceedings of the 52nd International
    Conference on Parallel Processing*, New York, NY, USA, 2023, p. 102–111.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Z. 陈, L. 石, X. 刘, J. 李, S. 刘, 和 Y. 许, “Osp: 通过两阶段同步提升分布式模型训练,” 在 *第52届国际并行处理会议论文集*,
    纽约, NY, USA, 2023年, 页 102–111。'
- en: '[86] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. a. Ranzato,
    A. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng, “Large scale distributed deep
    networks,” in *Advances in Neural Information Processing Systems*, vol. 25, 2012,
    pp. 1223–1231.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. 丁, G. 科拉多, R. 蒙加, K. 陈, M. 德文, M. 毛, M. a. 兰扎托, A. 西尼奥, P. 塔克, K. 杨,
    Q. 乐, 和 A. 吴, “大规模分布式深度网络,” 在 *神经信息处理系统进展*, 卷 25, 2012年, 页 1223–1231。'
- en: '[87] A.-L. Jin, W. Xu, S. Guo, B. Hu, and K. Yeung, “Ps+: A simple yet effective
    framework for fast training on parameter server,” *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 33, no. 12, pp. 4625–4637, 2022.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] A.-L. 金, W. 许, S. 郭, B. 胡, 和 K. 杨, “Ps+: 一种简单而有效的参数服务器快速训练框架,” *IEEE 并行与分布式系统学报*,
    卷 33, 第 12 期, 页 4625–4637, 2022年。'
- en: '[88] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson,
    G. Ganger, and E. P. Xing, “More effective distributed ml via a stale synchronous
    parallel parameter server,” in *Advances in Neural Information Processing Systems*,
    vol. 26, 2013, pp. 1223–1231.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Q. 霍, J. 西帕尔, H. 崔, S. 李, J. K. 金, P. B. 吉本斯, G. A. 吉布森, G. 甘杰, 和 E. P.
    星, “通过过时的同步并行参数服务器实现更有效的分布式 ML,” 在 *神经信息处理系统进展*, 卷 26, 2013年, 页 1223–1231。'
- en: '[89] B. McMahan and M. Streeter, “Delay-tolerant algorithms for asynchronous
    distributed online learning,” in *Advances in Neural Information Processing Systems*,
    vol. 27, 2014, pp. 2915–2923.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] B. 麦克马汉 和 M. 斯特里特, “适应性分布式在线学习的延迟容忍算法,” 在 *神经信息处理系统进展*, 卷 27, 2014年, 页
    2915–2923。'
- en: '[90] C. Chen, W. Wang, and B. Li, “Round-robin synchronization: Mitigating
    communication bottlenecks in parameter servers,” in *IEEE INFOCOM 2019-IEEE Conference
    on Computer Communications*.   IEEE, 2019, pp. 532–540.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. 陈, W. 王, 和 B. 李, “轮询同步: 缓解参数服务器中的通信瓶颈,” 在 *IEEE INFOCOM 2019-IEEE 计算机通信会议*.
    IEEE, 2019年, 页 532–540。'
- en: '[91] Y. Li, J. Huang, Z. Li, S. Zhou, W. Jiang, and J. Wang, “Hsp: Hybrid synchronous
    parallelism for fast distributed deep learning,” in *Proceedings of the 51st International
    Conference on Parallel Processing*, New York, NY, USA, 2022, pp. 1–11.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. 李, J. 黄, Z. 李, S. 周, W. 江, 和 J. 王, “Hsp: 混合同步并行加速分布式深度学习,” 在 *第51届国际并行处理会议论文集*,
    纽约, NY, USA, 2022年, 页 1–11。'
- en: '[92] S. Zhang, A. E. Choromanska, and Y. LeCun, “Deep learning with elastic
    averaging sgd,” in *Advances in Neural Information Processing Systems*, vol. 28,
    2015, pp. 685–693.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Zhang, A. E. Choromanska 和 Y. LeCun, “使用弹性平均SGD进行深度学习，”发表于*神经信息处理系统进展*，第28卷，2015年，页685–693。'
- en: '[93] J. Wang and G. Joshi, “Adaptive communication strategies to achieve the
    best error-runtime trade-off in local-update sgd,” in *Proceedings of Machine
    Learning and Systems*, vol. 1, 2019, pp. 212–229.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Wang 和 G. Joshi, “自适应通信策略以实现本地更新SGD中的最佳误差-运行时间权衡，”发表于*机器学习与系统会议论文集*，第1卷，2019年，页212–229。'
- en: '[94] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, “Don’t use large mini-batches,
    use local sgd,” in *International Conference on Learning Representations*, 2020.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] T. Lin, S. U. Stich, K. K. Patel 和 M. Jaggi, “不要使用大型小批量，使用本地SGD，”发表于*国际学习表征会议*，2020年。'
- en: '[95] J. Wang, V. Tantia, N. Ballas, and M. Rabbat, “Slowmo: Improving communication-efficient
    distributed sgd with slow momentum,” in *International Conference on Learning
    Representations*, 2020.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] J. Wang, V. Tantia, N. Ballas 和 M. Rabbat, “Slowmo: 通过慢动量改进通信效率的分布式SGD，”发表于*国际学习表征会议*，2020年。'
- en: '[96] T. Chen, G. Giannakis, T. Sun, and W. Yin, “Lag: Lazily aggregated gradient
    for communication-efficient distributed learning,” *Advances in neural information
    processing systems*, vol. 31, p. 5055–5065, 2018.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Chen, G. Giannakis, T. Sun 和 W. Yin, “Lag: 懒惰聚合梯度以提高通信效率的分布式学习，”*神经信息处理系统进展*，第31卷，页5055–5065，2018年。'
- en: '[97] J. George and P. Gurram, “Distributed stochastic gradient descent with
    event-triggered communication,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 34, no. 05, 2020, pp. 7169–7178.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. George 和 P. Gurram, “具有事件触发通信的分布式随机梯度下降，”发表于*人工智能领域的AAAI会议论文集*，第34卷，第05期，2020年，页7169–7178。'
- en: '[98] Z. Wang, Y. Tu, N. Wang, L. Gao, J. Nie, Z. Wei, Y. Gu, and G. Yu, “Fsp:
    Towards flexible synchronous parallel frameworks for distributed machine learning,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 34, no. 2, pp. 687–703,
    2023.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Wang, Y. Tu, N. Wang, L. Gao, J. Nie, Z. Wei, Y. Gu 和 G. Yu, “Fsp:
    迈向用于分布式机器学习的灵活同步并行框架，”*IEEE并行与分布式系统学报*，第34卷，第2期，页687–703，2023年。'
- en: '[99] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can decentralized
    algorithms outperform centralized algorithms? a case study for decentralized parallel
    stochastic gradient descent,” in *Advances in Neural Information Processing Systems*,
    vol. 30, 2017, pp. 5330–5340.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang 和 J. Liu, “去中心化算法能否超越中心化算法？去中心化并行随机梯度下降的案例研究，”发表于*神经信息处理系统进展*，第30卷，2017年，页5330–5340。'
- en: '[100] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, “D²: Decentralized training
    over decentralized data,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 4848–4856.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Tang, X. Lian, M. Yan, C. Zhang 和 J. Liu, “D²: 在去中心化数据上进行去中心化训练，”发表于*国际机器学习会议*。PMLR，2018年，页4848–4856。'
- en: '[101] H. Sun, Z. Gui, S. Guo, Q. Qi, J. Wang, and J. Liao, “Gssp: Eliminating
    stragglers through grouping synchronous for distributed deep learning in heterogeneous
    cluster,” *IEEE Transactions on Cloud Computing*, vol. 10, no. 4, pp. 2637–2648,
    2022.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] H. Sun, Z. Gui, S. Guo, Q. Qi, J. Wang 和 J. Liao, “Gssp: 通过分组同步消除异构集群中分布式深度学习的拖延者，”*IEEE云计算学报*，第10卷，第4期，页2637–2648，2022年。'
- en: '[102] M. Tan, W.-X. Liu, J. Luo, H. Chen, and Z.-Z. Guo, “Adaptive synchronous
    strategy for distributed machine learning,” *International Journal of Intelligent
    Systems*, vol. 37, no. 12, pp. 11 713–11 741, 2022.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Tan, W.-X. Liu, J. Luo, H. Chen 和 Z.-Z. Guo, “用于分布式机器学习的自适应同步策略，”*智能系统国际期刊*，第37卷，第12期，页11 713–11 741，2022年。'
- en: '[103] Z. Shen, Q. Tang, T. Zhou, Y. Zhang, Z. Jia, D. Yu, Z. Zhang, and B. Li,
    “Ashl: An adaptive multi-stage distributed deep learning training scheme for heterogeneous
    environments,” *IEEE Transactions on Computers*, pp. 1–14, 2023.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Z. Shen, Q. Tang, T. Zhou, Y. Zhang, Z. Jia, D. Yu, Z. Zhang 和 B. Li,
    “Ashl: 一种适应性多阶段分布式深度学习训练方案，用于异质环境，”*IEEE计算机学报*，页1–14，2023年。'
- en: '[104] F. Zhou and G. Cong, “On the convergence properties of a k-step averaging
    stochastic gradient descent algorithm for nonconvex optimization,” in *Proceedings
    of the 27th International Joint Conference on Artificial Intelligence*, 2018,
    pp. 3219–3227.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] F. Zhou 和 G. Cong, “关于k步平均随机梯度下降算法在非凸优化中的收敛性质，”发表于*第27届国际联合人工智能会议论文集*，2018年，页3219–3227。'
- en: '[105] S. U. Stich, “Local sgd converges fast and communicates little,” in *ICLR
    2019-International Conference on Learning Representations*, 2019.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. U. Stich，“本地 SGD 收敛快且通信少”，发表在*ICLR 2019-国际学习表征会议*，2019年。'
- en: '[106] H. Yu, S. Yang, and S. Zhu, “Parallel restarted sgd with faster convergence
    and less communication: Demystifying why model averaging works for deep learning,”
    ser. AAAI’19/IAAI’19/EAAI’19.   AAAI Press, 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] H. Yu, S. Yang 和 S. Zhu，“具有更快收敛和更少通信的并行重启 SGD：揭示为什么模型平均对深度学习有效”，系列 AAAI’19/IAAI’19/EAAI’19，AAAI
    Press，2019年。'
- en: '[107] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe, “Local sgd with
    periodic averaging: Tighter analysis and adaptive synchronization,” in *Advances
    in Neural Information Processing Systems*, vol. 32, 2019, pp. 11 082–11 094.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] F. Haddadpour, M. M. Kamani, M. Mahdavi 和 V. Cadambe，“周期性平均的本地 SGD：更紧的分析和自适应同步”，发表在*《神经信息处理系统进展》*，第32卷，2019年，页码
    11 082–11 094。'
- en: '[108] H. Yu and R. Jin, “On the computation and communication complexity of
    parallel SGD with dynamic batch sizes for stochastic non-convex optimization,”
    in *Proceedings of the 36th International Conference on Machine Learning*, vol. 97.   PMLR,
    09–15 Jun 2019, pp. 7174–7183.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] H. Yu 和 R. Jin，“关于具有动态批量大小的并行 SGD 的计算与通信复杂性，用于随机非凸优化”，发表在*《第36届国际机器学习大会论文集》*，第97卷，PMLR，2019年6月9–15日，页码
    7174–7183。'
- en: '[109] B. Woodworth, K. K. Patel, S. Stich, Z. Dai, B. Bullins, B. Mcmahan,
    O. Shamir, and N. Srebro, “Is local SGD better than minibatch SGD?” in *Proceedings
    of the 37th International Conference on Machine Learning*, vol. 119.   PMLR, 13–18
    Jul 2020, pp. 10 334–10 343.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] B. Woodworth, K. K. Patel, S. Stich, Z. Dai, B. Bullins, B. Mcmahan,
    O. Shamir 和 N. Srebro，“本地 SGD 是否优于小批量 SGD？” 发表在*《第37届国际机器学习大会论文集》*，第119卷，PMLR，2020年7月13–18日，页码
    10 334–10 343。'
- en: '[110] A. Spiridonoff, A. Olshevsky, and Y. Paschalidis, “Communication-efficient
    sgd: From local sgd to one-shot averaging,” in *Advances in Neural Information
    Processing Systems*, vol. 34, 2021, pp. 24 313–24 326.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Spiridonoff, A. Olshevsky 和 Y. Paschalidis，“通信高效的 SGD：从本地 SGD 到一次性平均”，发表在*《神经信息处理系统进展》*，第34卷，2021年，页码
    24 313–24 326。'
- en: '[111] J. Wang and G. Joshi, “Cooperative sgd: A unified framework for the design
    and analysis of local-update sgd algorithms,” vol. 22, no. 1, jan 2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Wang 和 G. Joshi，“合作 SGD：本地更新 SGD 算法设计与分析的统一框架”，第22卷，第1期，2021年1月。'
- en: '[112] S. U. Stich and S. P. Karimireddy, “The error-feedback framework: Better
    rates for sgd with delayed gradients and compressed updates,” *The Journal of
    Machine Learning Research*, vol. 21, no. 1, pp. 9613–9648, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] S. U. Stich 和 S. P. Karimireddy，“误差反馈框架：延迟梯度和压缩更新下的 SGD 更好速率”，*《机器学习研究杂志》*，第21卷，第1期，页码
    9613–9648，2020年。'
- en: '[113] R. Z. Aviv, I. Hakimi, A. Schuster, and K. Y. Levy, “Asynchronous distributed
    learning : Adapting to gradient delays without prior knowledge,” in *Proceedings
    of the 38th International Conference on Machine Learning*, vol. 139.   PMLR, 18–24
    Jul 2021, pp. 436–445.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] R. Z. Aviv, I. Hakimi, A. Schuster 和 K. Y. Levy，“异步分布式学习：在没有先验知识的情况下适应梯度延迟”，发表在*《第38届国际机器学习大会论文集》*，第139卷，PMLR，2021年7月18–24日，页码
    436–445。'
- en: '[114] A. Cohen, A. Daniely, Y. Drori, T. Koren, and M. Schain, “Asynchronous
    stochastic optimization robust to arbitrary delays,” *Advances in Neural Information
    Processing Systems*, vol. 34, pp. 9024–9035, 2021.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] A. Cohen, A. Daniely, Y. Drori, T. Koren 和 M. Schain，“对任意延迟鲁棒的异步随机优化”，*《神经信息处理系统进展》*，第34卷，页码
    9024–9035，2021年。'
- en: '[115] A. Koloskova, S. U. Stich, and M. Jaggi, “Sharper convergence guarantees
    for asynchronous sgd for distributed and federated learning,” *Advances in Neural
    Information Processing Systems*, vol. 35, pp. 17 202–17 215, 2022.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] A. Koloskova, S. U. Stich 和 M. Jaggi，“更锋利的异步 SGD 收敛保证用于分布式和联邦学习”，*《神经信息处理系统进展》*，第35卷，页码
    17 202–17 215，2022年。'
- en: '[116] K. Mishchenko, F. Bach, M. Even, and B. E. Woodworth, “Asynchronous sgd
    beats minibatch sgd under arbitrary delays,” *Advances in Neural Information Processing
    Systems*, vol. 35, pp. 420–433, 2022.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] K. Mishchenko, F. Bach, M. Even 和 B. E. Woodworth，“异步 SGD 在任意延迟下优于小批量
    SGD”，*《神经信息处理系统进展》*，第35卷，页码 420–433，2022年。'
- en: '[117] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
    learning of deep networks from decentralized data,” in *Artificial intelligence
    and statistics*.   PMLR, 2017, pp. 1273–1282.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] B. McMahan, E. Moore, D. Ramage, S. Hampson 和 B. A. y Arcas，“从分散数据中学习深度网络的通信高效性”，发表在*《人工智能与统计》*，PMLR，2017年，页码
    1273–1282。'
- en: '[118] P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y. Wu, and H. Yu, “Communication-efficient
    decentralized machine learning over heterogeneous networks,” in *2021 IEEE 37th
    International Conference on Data Engineering (ICDE)*, 2021, pp. 384–395.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y. Wu, 和 H. Yu, “在异质网络上的通信高效去中心化机器学习，”
    在 *2021 IEEE第37届数据工程国际会议（ICDE）*，2021年，第384–395页。'
- en: '[119] C. Chen, H. Xu, W. Wang, B. Li, B. Li, L. Chen, and G. Zhang, “Communication-efficient
    federated learning with adaptive parameter freezing,” in *2021 IEEE 41st International
    Conference on Distributed Computing Systems (ICDCS)*, 2021, pp. 1–11.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Chen, H. Xu, W. Wang, B. Li, B. Li, L. Chen, 和 G. Zhang, “具有自适应参数冻结的通信高效联合学习，”
    在 *2021 IEEE第41届国际分布式计算系统会议（ICDCS）*，2021年，第1–11页。'
- en: '[120] ——, “Synchronize only the immature parameters: Communication-efficient
    federated learning by freezing parameters adaptively,” *IEEE Transactions on Parallel
    and Distributed Systems*, pp. 1–18, 2023.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] ——, “仅同步不成熟参数：通过自适应冻结参数实现通信高效联合学习，” *IEEE并行与分布式系统学报*，第1–18页，2023年。'
- en: '[121] J. Liu, J. Liu, H. Xu, Y. Liao, Z. Wang, and Q. Ma, “Yoga: Adaptive layer-wise
    model aggregation for decentralized federated learning,” *IEEE/ACM Transactions
    on Networking*, 2023.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Liu, J. Liu, H. Xu, Y. Liao, Z. Wang, 和 Q. Ma, “Yoga：用于去中心化联合学习的自适应分层模型聚合，”
    *IEEE/ACM网络学报*，2023年。'
- en: '[122] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
    “Federated optimization in heterogeneous networks,” in *Proceedings of Machine
    Learning and Systems*, vol. 2, 2020, pp. 429–450.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, 和 V. Smith, “异质网络中的联合优化，”
    在 *机器学习与系统会议论文集*，第2卷，2020年，第429–450页。'
- en: '[123] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective
    inconsistency problem in heterogeneous federated optimization,” *Advances in neural
    information processing systems*, vol. 33, pp. 7611–7623, 2020.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Wang, Q. Liu, H. Liang, G. Joshi, 和 H. V. Poor, “解决异质联合优化中的目标不一致性问题，”
    *神经信息处理系统进展*，第33卷，第7611–7623页，2020年。'
- en: '[124] Y. Esfandiari, S. Y. Tan, Z. Jiang, A. Balu, E. Herron, C. Hegde, and
    S. Sarkar, “Cross-gradient aggregation for decentralized learning from non-iid
    data,” in *Proceedings of the 38th International Conference on Machine Learning*,
    vol. 139.   PMLR, 18–24 Jul 2021, pp. 3036–3046.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Esfandiari, S. Y. Tan, Z. Jiang, A. Balu, E. Herron, C. Hegde, 和 S.
    Sarkar, “用于去中心化学习的跨梯度聚合来自非独立同分布数据，” 在 *第38届国际机器学习大会论文集*，第139卷。 PMLR，2021年7月18–24日，第3036–3046页。'
- en: '[125] M. Chen, Y. Xu, H. Xu, and L. Huang, “Enhancing decentralized federated
    learning for non-iid data on heterogeneous devices,” in *2023 IEEE 39th International
    Conference on Data Engineering (ICDE)*, 2023, pp. 2289–2302.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Chen, Y. Xu, H. Xu, 和 L. Huang, “增强异质设备上非独立同分布数据的去中心化联合学习，” 在 *2023
    IEEE第39届数据工程国际会议（ICDE）*，2023年，第2289–2302页。'
- en: '[126] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
    C. Kiddon, J. Konečný, S. Mazzocchi, B. McMahan, T. Van Overveldt, D. Petrou,
    D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,”
    in *Proceedings of Machine Learning and Systems*, vol. 1, 2019, pp. 374–388.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
    C. Kiddon, J. Konečný, S. Mazzocchi, B. McMahan, T. Van Overveldt, D. Petrou,
    D. Ramage, 和 J. Roselander, “迈向大规模联合学习：系统设计，” 在 *机器学习与系统会议论文集*，第1卷，2019年，第374–388页。'
- en: '[127] Z. Wang, H. Xu, J. Liu, Y. Xu, H. Huang, and Y. Zhao, “Accelerating federated
    learning with cluster construction and hierarchical aggregation,” *IEEE Transactions
    on Mobile Computing*, vol. 22, no. 7, pp. 3805–3822, 2023.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Wang, H. Xu, J. Liu, Y. Xu, H. Huang, 和 Y. Zhao, “通过集群构建和分层聚合加速联合学习，”
    *IEEE移动计算学报*，第22卷，第7期，第3805–3822页，2023年。'
- en: '[128] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and N. Michelusi,
    “Semi-decentralized federated learning with cooperative d2d local model aggregations,”
    *IEEE Journal on Selected Areas in Communications*, vol. 39, no. 12, pp. 3851–3869,
    2021.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, 和 N. Michelusi,
    “具有协作d2d本地模型聚合的半去中心化联合学习，” *IEEE选择通信领域期刊*，第39卷，第12期，第3851–3869页，2021年。'
- en: '[129] M. Ryabinin, E. Gorbunov, V. Plokhotnyuk, and G. Pekhimenko, “Moshpit
    sgd: Communication-efficient decentralized training on heterogeneous unreliable
    devices,” in *Advances in Neural Information Processing Systems*, vol. 34, 2021,
    pp. 18 195–18 211.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] M. Ryabinin, E. Gorbunov, V. Plokhotnyuk, 和 G. Pekhimenko, “Moshpit sgd：在异质不可靠设备上的通信高效去中心化训练，”
    在 *神经信息处理系统进展*，第34卷，2021年，第18 195–18 211页。'
- en: '[130] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
    “Adaptive federated learning in resource constrained edge computing systems,”
    *IEEE Journal on Selected Areas in Communications*, vol. 37, no. 6, pp. 1205–1221,
    2019.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, 和 K. Chan，“资源受限边缘计算系统中的自适应联邦学习，”*IEEE通信选择领域杂志*，第37卷，第6期，第1205–1221页，2019年。'
- en: '[131] Y. Xu, Y. Liao, H. Xu, Z. Ma, L. Wang, and J. Liu, “Adaptive control
    of local updating and model compression for efficient federated learning,” *IEEE
    Transactions on Mobile Computing*, vol. 22, no. 10, pp. 5675–5689, 2023.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Xu, Y. Liao, H. Xu, Z. Ma, L. Wang, 和 J. Liu，“高效联邦学习的本地更新和模型压缩的自适应控制，”*IEEE移动计算学报*，第22卷，第10期，第5675–5689页，2023年。'
- en: '[132] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Wang, and C. Qiao, “Accelerating federated
    learning with data and model parallelism in edge computing,” *IEEE/ACM Transactions
    on Networking*, pp. 1–15, 2023.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Wang, 和 C. Qiao，“在边缘计算中通过数据和模型并行加速联邦学习，”*IEEE/ACM网络学报*，第1–15页，2023年。'
- en: '[133]'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133]'
- en: '[134] Z. Wang, H. Xu, Y. Xu, Z. Jiang, J. Liu, and S. Chen, “Fast: Enhancing
    federated learning through adaptive data sampling and local training,” *IEEE Transactions
    on Parallel and Distributed Systems*, pp. 1–15, 2023.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Wang, H. Xu, Y. Xu, Z. Jiang, J. Liu, 和 S. Chen，“Fast: 通过自适应数据采样和本地训练提升联邦学习，”*IEEE并行与分布式系统学报*，第1–15页，2023年。'
- en: '[135] J. Park, D. Yoon, S. Yeo, and S. Oh, “Amble: Adjusting mini-batch and
    local epoch for federated learning with heterogeneous devices,” *Journal of Parallel
    and Distributed Computing*, vol. 170, pp. 13–23, 2022.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] J. Park, D. Yoon, S. Yeo, 和 S. Oh，“Amble: 为异构设备调整小批量和本地周期的联邦学习，”*并行与分布式计算杂志*，第170卷，第13–23页，2022年。'
- en: '[136] Y. Liu, L. Xu, X. Yuan, C. Wang, and B. Li, “The right to be forgotten
    in federated learning: An efficient realization with rapid retraining,” in *IEEE
    INFOCOM 2022 - IEEE Conference on Computer Communications*, 2022, pp. 1749–1758.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Y. Liu, L. Xu, X. Yuan, C. Wang, 和 B. Li，“联邦学习中的被遗忘权：一种高效的快速再训练实现，”发表于*IEEE
    INFOCOM 2022 - IEEE计算机通信会议*，2022年，第1749–1758页。'
- en: '[137] C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and
    G. E. Dahl, “Measuring the effects of data parallelism on neural network training,”
    *Journal of Machine Learning Research*, vol. 20, no. 112, pp. 1–49, 2019.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, 和
    G. E. Dahl，“测量数据并行性对神经网络训练的影响，”*机器学习研究杂志*，第20卷，第112期，第1–49页，2019年。'
- en: '[138] H. Karimi, J. Nutini, and M. Schmidt, “Linear convergence of gradient
    and proximal-gradient methods under the polyak-łojasiewicz condition,” in *Machine
    Learning and Knowledge Discovery in Databases*, 2016, pp. 795–811.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] H. Karimi, J. Nutini, 和 M. Schmidt，“在Polyak-Łojasiewicz条件下梯度和近端梯度方法的线性收敛性，”发表于*机器学习与数据库中的知识发现*，2016年，第795–811页。'
- en: '[139] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro, “The role
    of over-parametrization in generalization of neural networks,” in *International
    Conference on Learning Representations*, 2019.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, 和 N. Srebro，“过度参数化在神经网络泛化中的作用，”发表于*国际学习表征会议*，2019年。'
- en: '[140] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
    D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for privacy-preserving
    machine learning,” in *Proceedings of the 2017 ACM SIGSAC Conference on Computer
    and Communications Security*, New York, NY, USA, 2017, p. 1175–1191.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
    D. Ramage, A. Segal, 和 K. Seth，“隐私保护机器学习的实用安全聚合，”发表于*2017年ACM SIGSAC计算机与通信安全会议论文集*，纽约，NY，美国，2017年，第1175–1191页。'
- en: '[141] Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, and M. Mahoney,
    “Adahessian: An adaptive second order optimizer for machine learning,” in *proceedings
    of the AAAI conference on artificial intelligence*, vol. 35, no. 12, 2021, pp.
    10 665–10 673.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, 和 M. Mahoney，“Adahessian:
    一种用于机器学习的自适应二阶优化器，”发表于*AAAI人工智能会议论文集*，第35卷，第12期，2021年，第10 665–10 673页。'
- en: '[142] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu, “1-bit stochastic gradient
    descent and its application to data-parallel distributed training of speech dnns,”
    in *Fifteenth annual conference of the international speech communication association*,
    2014.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] F. Seide, H. Fu, J. Droppo, G. Li, 和 D. Yu，“1-bit随机梯度下降及其在数据并行分布式语音DNN训练中的应用，”发表于*国际语音通信协会第十五届年会*，2014年。'
- en: '[143] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, “signsgd:
    Compressed optimisation for non-convex problems,” in *International Conference
    on Machine Learning*.   PMLR, 2018, pp. 560–569.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, 和 A. Anandkumar, “signsgd：用于非凸问题的压缩优化，”见
    *国际机器学习会议*。PMLR, 2018, 第560–569页。'
- en: '[144] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi, “Error feedback
    fixes signsgd and other gradient compression schemes,” in *International Conference
    on Machine Learning*.   PMLR, 2019, pp. 3252–3261.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] S. P. Karimireddy, Q. Rebjock, S. Stich, 和 M. Jaggi, “错误反馈修正signsgd及其他梯度压缩方案，”见
    *国际机器学习会议*。PMLR, 2019, 第3252–3261页。'
- en: '[145] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, and Y. He, “1-bit adam: Communication efficient large-scale training
    with adam’s convergence speed,” in *Proceedings of the 38th International Conference
    on Machine Learning*, vol. 139.   PMLR, 18–24 Jul 2021, pp. 10 118–10 129.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, 和 Y. He, “1-bit adam：具有Adam收敛速度的大规模训练的通信效率，”见 *第38届国际机器学习会议论文集*，第139卷。PMLR,
    2021年7月18–24日，第10,118–10,129页。'
- en: '[146] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning
    with limited numerical precision,” in *International conference on machine learning*.   PMLR,
    2015, pp. 1737–1746.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Gupta, A. Agrawal, K. Gopalakrishnan, 和 P. Narayanan, “有限数字精度下的深度学习，”见
    *国际机器学习会议*。PMLR, 2015, 第1737–1746页。'
- en: '[147] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-net: Training
    low bitwidth convolutional neural networks with low bitwidth gradients,” *arXiv
    preprint arXiv:1606.06160*, 2016.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, 和 Y. Zou, “Dorefa-net：用低位宽梯度训练低位宽卷积神经网络，”
    *arXiv预印本arXiv:1606.06160*，2016年。'
- en: '[148] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Quantized
    neural networks: Training neural networks with low precision weights and activations,”
    *The Journal of Machine Learning Research*, vol. 18, no. 1, pp. 6869–6898, 2017.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, 和 Y. Bengio, “量化神经网络：用低精度权重和激活训练神经网络，”
    *机器学习研究期刊*，第18卷，第1期，第6869–6898页，2017年。'
- en: '[149] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, “Zipml:
    Training linear models with end-to-end low precision, and a little bit of deep
    learning,” in *International Conference on Machine Learning*.   PMLR, 2017, pp.
    4035–4043.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, 和 C. Zhang, “Zipml：用端到端低精度和一点深度学习训练线性模型，”见
    *国际机器学习会议*。PMLR, 2017, 第4035–4043页。'
- en: '[150] S. Horvóth, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik,
    “Natural compression for distributed deep learning,” in *Mathematical and Scientific
    Machine Learning*.   PMLR, 2022, pp. 129–141.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Horvóth, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, 和 P. Richtárik,
    “用于分布式深度学习的自然压缩，”见 *数学与科学机器学习*。PMLR, 2022, 第129–141页。'
- en: '[151] A. T. Suresh, X. Y. Felix, S. Kumar, and H. B. McMahan, “Distributed
    mean estimation with limited communication,” in *International conference on machine
    learning*.   PMLR, 2017, pp. 3329–3337.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] A. T. Suresh, X. Y. Felix, S. Kumar, 和 H. B. McMahan, “有限通信下的分布式均值估计，”见
    *国际机器学习会议*。PMLR, 2017, 第3329–3337页。'
- en: '[152] H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright,
    “Atomo: Communication-efficient learning via atomic sparsification,” *Advances
    in neural information processing systems*, vol. 31, 2018.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, 和 S. Wright,
    “Atomo：通过原子稀疏化进行通信高效学习，” *神经信息处理系统进展*，第31卷，2018年。'
- en: '[153] T. Vogels, S. P. Karimireddy, and M. Jaggi, “Powersgd: Practical low-rank
    gradient compression for distributed optimization,” *Advances in Neural Information
    Processing Systems*, vol. 32, 2019.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] T. Vogels, S. P. Karimireddy, 和 M. Jaggi, “Powersgd：分布式优化的实际低秩梯度压缩，”
    *神经信息处理系统进展*，第32卷，2019年。'
- en: '[154] ——, “Practical low-rank communication compression in decentralized deep
    learning,” *Advances in Neural Information Processing Systems*, vol. 33, pp. 14 171–14 181,
    2020.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] ——, “去中心化深度学习中的实际低秩通信压缩，” *神经信息处理系统进展*，第33卷，第14,171–14,181页，2020年。'
- en: '[155] Y. Dong, L. Wang, J. Wang, X. Hu, H. Zhang, F. R. Yu, and V. C. M. Leung,
    “Accelerating wireless federated learning via nesterov’s momentum and distributed
    principle component analysis,” *IEEE Transactions on Wireless Communications*,
    pp. 1–1, 2023.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Y. Dong, L. Wang, J. Wang, X. Hu, H. Zhang, F. R. Yu, 和 V. C. M. Leung,
    “通过Nesterov动量和分布式主成分分析加速无线联邦学习，” *IEEE无线通信汇刊*，第1–1页，2023年。'
- en: '[156] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “Qsgd: Communication-efficient
    sgd via gradient quantization and encoding,” *Advances in neural information processing
    systems*, vol. 30, 2017.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] D. Alistarh, D. Grubic, J. Li, R. Tomioka, 和 M. Vojnovic，“Qsgd：通过梯度量化和编码实现通信高效的sgd，”
    *神经信息处理系统进展*，第30卷，2017年。'
- en: '[157] J. Konečnỳ and P. Richtárik, “Randomized distributed mean estimation:
    Accuracy vs. communication,” *Frontiers in Applied Mathematics and Statistics*,
    vol. 4, p. 62, 2018.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J. Konečnỳ 和 P. Richtárik，“随机分布式均值估计：准确性与通信，” *应用数学与统计前沿*，第4卷，第62页，2018年。'
- en: '[158] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li, “Terngrad:
    Ternary gradients to reduce communication in distributed deep learning,” *Advances
    in neural information processing systems*, vol. 30, 2017.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, 和 H. Li，“Terngrad：三值梯度以减少分布式深度学习中的通信，”
    *神经信息处理系统进展*，第30卷，2017年。'
- en: '[159] J. Wu, W. Huang, J. Huang, and T. Zhang, “Error compensated quantized
    sgd and its applications to large-scale distributed optimization,” in *International
    Conference on Machine Learning*.   PMLR, 2018, pp. 5325–5333.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] J. Wu, W. Huang, J. Huang, 和 T. Zhang，“误差补偿量化sgd及其在大规模分布式优化中的应用，” 在 *国际机器学习会议*。
    PMLR，2018年，第5325–5333页。'
- en: '[160] A. Ramezani-Kebrya, F. Faghri, I. Markov, V. Aksenov, D. Alistarh, and
    D. M. Roy, “Nuqsgd: Provably communication-efficient data-parallel sgd via nonuniform
    quantization,” *Journal of Machine Learning Research*, vol. 22, no. 1, jan 2021.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Ramezani-Kebrya, F. Faghri, I. Markov, V. Aksenov, D. Alistarh, 和
    D. M. Roy，“Nuqsgd：通过非均匀量化证明通信高效的数据并行sgd，” *机器学习研究杂志*，第22卷，第1期，2021年1月。'
- en: '[161] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya,
    “Adaptive gradient quantization for data-parallel sgd,” *Advances in neural information
    processing systems*, vol. 33, pp. 3174–3185, 2020.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, 和 A. Ramezani-Kebrya，“数据并行sgd的自适应梯度量化，”
    *神经信息处理系统进展*，第33卷，第3174–3185页，2020年。'
- en: '[162] K. Mishchenko, B. Wang, D. Kovalev, and P. Richtárik, “IntSGD: Adaptive
    floatless compression of stochastic gradients,” in *International Conference on
    Learning Representations*, 2022.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] K. Mishchenko, B. Wang, D. Kovalev, 和 P. Richtárik，“IntSGD：自适应无浮点压缩的随机梯度，”
    在 *国际学习表征会议*，2022年。'
- en: '[163] Y. Mao, Z. Zhao, G. Yan, Y. Liu, T. Lan, L. Song, and W. Ding, “Communication-efficient
    federated learning with adaptive quantization,” *ACM Transactions on Intelligent
    Systems and Technology (TIST)*, vol. 13, no. 4, pp. 1–26, 2022.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Y. Mao, Z. Zhao, G. Yan, Y. Liu, T. Lan, L. Song, 和 W. Ding，“具有自适应量化的通信高效联邦学习，”
    *ACM智能系统与技术交易*，第13卷，第4期，第1–26页，2022年。'
- en: '[164] H. Liu, F. He, and G. Cao, “Communication-efficient federated learning
    for heterogeneous edge devices based on adaptive gradient quantization,” in *IEEE
    INFOCOM 2023-IEEE Conference on Computer Communications*.   IEEE, 2023, pp. 1–10.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] H. Liu, F. He, 和 G. Cao，“基于自适应梯度量化的异构边缘设备通信高效联邦学习，” 在 *IEEE INFOCOM 2023-IEEE计算机通信会议*。
    IEEE，2023年，第1–10页。'
- en: '[165] N. Ström, “Scalable distributed dnn training using commodity gpu cloud
    computing,” in *Interspeech 2015*, 2015.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] N. Ström，“使用商品gpu云计算的可扩展分布式dnn训练，” 在 *Interspeech 2015*，2015年。'
- en: '[166] A. F. Aji and K. Heafield, “Sparse communication for distributed gradient
    descent,” in *Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing*, 2017, pp. 440–445.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] A. F. Aji 和 K. Heafield，“用于分布式梯度下降的稀疏通信，” 在 *2017年自然语言处理实证方法会议论文集*，2017年，第440–445页。'
- en: '[167] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, “Sparsified sgd with memory,”
    in *Advances in Neural Information Processing Systems*, vol. 31.   Curran Associates,
    Inc., 2018.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] S. U. Stich, J.-B. Cordonnier, 和 M. Jaggi，“带有内存的稀疏化sgd，” 在 *神经信息处理系统进展*，第31卷。
    Curran Associates, Inc.，2018年。'
- en: '[168] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally, “Deep gradient compression:
    Reducing the communication bandwidth for distributed training,” in *International
    Conference on Learning Representations*, 2018.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Lin, S. Han, H. Mao, Y. Wang, 和 B. Dally，“深度梯度压缩：减少分布式训练的通信带宽，” 在
    *国际学习表征会议*，2018年。'
- en: '[169] D. Xiao, Y. Mei, D. Kuang, M. Chen, B. Guo, and W. Wu, “Egc: Entropy-based
    gradient compression for distributed deep learning,” *Information Sciences*, vol.
    548, pp. 118–134, 2021.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] D. Xiao, Y. Mei, D. Kuang, M. Chen, B. Guo, 和 W. Wu，“Egc：基于熵的梯度压缩用于分布式深度学习，”
    *信息科学*，第548卷，第118–134页，2021年。'
- en: '[170] Z. Zhang and C. Wang, “Mipd: An adaptive gradient sparsification framework
    for distributed dnns training,” *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 33, no. 11, pp. 3053–3066, 2022.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Z. Zhang 和 C. Wang，“Mipd: 一种用于分布式深度神经网络训练的自适应梯度稀疏化框架，”*IEEE并行与分布式系统汇刊*，第33卷，第11期，第3053–3066页，2022年。'
- en: '[171] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X. Chu, “A
    distributed synchronous sgd algorithm with global top-k sparsification for low
    bandwidth networks,” in *2019 IEEE 39th International Conference on Distributed
    Computing Systems (ICDCS)*, 2019, pp. 2238–2247.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang 和 X. Chu，“一种带有全局Top-k稀疏化的分布式同步SGD算法，用于低带宽网络，”发表于*2019年IEEE第39届分布式计算系统国际会议（ICDCS）*，2019年，第2238–2247页。'
- en: '[172] C.-Y. Chen, J. Ni, S. Lu, X. Cui, P.-Y. Chen, X. Sun, N. Wang, S. Venkataramani,
    V. V. Srinivasan, W. Zhang *et al.*, “Scalecom: Scalable sparsified gradient compression
    for communication-efficient distributed training,” *Advances in Neural Information
    Processing Systems*, vol. 33, pp. 13 551–13 563, 2020.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] C.-Y. Chen, J. Ni, S. Lu, X. Cui, P.-Y. Chen, X. Sun, N. Wang, S. Venkataramani,
    V. V. Srinivasan, W. Zhang *等*，“Scalecom: 可扩展稀疏化梯度压缩用于通信高效的分布式训练，”*神经信息处理系统进展*，第33卷，第13,551–13,563页，2020年。'
- en: '[173] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini, “An efficient
    statistical-based gradient compression technique for distributed training systems,”
    *Proceedings of Machine Learning and Systems*, vol. 3, pp. 297–322, 2021.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini 和 M. Canini，“一种高效的基于统计的梯度压缩技术用于分布式训练系统，”*机器学习与系统会议录*，第3卷，第297–322页，2021年。'
- en: '[174] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie *et al.*, “Towards scalable distributed training of deep learning
    on public cloud clusters,” *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 401–412, 2021.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie *等*，“面向公共云集群的可扩展深度学习分布式训练，”*机器学习与系统会议录*，第3卷，第401–412页，2021年。'
- en: '[175] S. Li and T. Hoefler, “Near-optimal sparse allreduce for distributed
    deep learning,” in *Proceedings of the 27th ACM SIGPLAN Symposium on Principles
    and Practice of Parallel Programming*, 2022, pp. 135–149.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Li 和 T. Hoefler，“近优的稀疏AllReduce用于分布式深度学习，”发表于*第27届ACM SIGPLAN并行编程原理与实践研讨会会议录*，2022年，第135–149页。'
- en: '[176] R. Liu and B. Mozafari, “Communication-efficient distributed learning
    for large batch optimization,” in *International Conference on Machine Learning*.   PMLR,
    2022, pp. 13 925–13 946.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] R. Liu 和 B. Mozafari，“大批量优化的通信高效分布式学习，”发表于*国际机器学习会议*。PMLR，2022，第13,925–13,946页。'
- en: '[177] J. Wangni, J. Wang, J. Liu, and T. Zhang, “Gradient sparsification for
    communication-efficient distributed optimization,” *Advances in Neural Information
    Processing Systems*, vol. 31, 2018.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Wangni, J. Wang, J. Liu 和 T. Zhang，“用于通信高效分布式优化的梯度稀疏化，”*神经信息处理系统进展*，第31卷，2018年。'
- en: '[178] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat,
    and C. Renggli, “The convergence of sparsified gradient methods,” *Advances in
    Neural Information Processing Systems*, vol. 31, 2018.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat 和
    C. Renggli，“稀疏梯度方法的收敛性，”*神经信息处理系统进展*，第31卷，2018年。'
- en: '[179] L. Wang, W. Wu, J. Zhang, H. Liu, G. Bosilca, M. Herlihy, and R. Fonseca,
    “Fft-based gradient sparsification for the distributed training of deep neural
    networks,” in *Proceedings of the 29th International Symposium on High-Performance
    Parallel and Distributed Computing*, 2020, pp. 113–124.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] L. Wang, W. Wu, J. Zhang, H. Liu, G. Bosilca, M. Herlihy 和 R. Fonseca，“基于FFT的梯度稀疏化用于深度神经网络的分布式训练，”发表于*第29届高性能并行与分布式计算国际研讨会会议录*，2020年，第113–124页。'
- en: '[180] A. Sahu, A. Dutta, A. M. Abdelmoniem, T. Banerjee, M. Canini, and P. Kalnis,
    “Rethinking gradient sparsification as total error minimization,” in *Advances
    in Neural Information Processing Systems*, vol. 34, 2021, pp. 8133–8146.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Sahu, A. Dutta, A. M. Abdelmoniem, T. Banerjee, M. Canini 和 P. Kalnis，“重新思考梯度稀疏化为总误差最小化，”发表于*神经信息处理系统进展*，第34卷，2021年，第8133–8146页。'
- en: '[181] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu, and X. Zhao, “Communication-efficient
    distributed deep learning with merged gradient sparsification on gpus,” in *IEEE
    INFOCOM 2020-IEEE Conference on Computer Communications*.   IEEE, 2020, pp. 406–415.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu 和 X. Zhao，“在GPU上使用合并梯度稀疏化的通信高效分布式深度学习，”发表于*IEEE
    INFOCOM 2020-IEEE计算机通信会议*。IEEE，2020，第406–415页。'
- en: '[182] Z. Wang, Z. Xu, X. Wu, A. Shrivastava, and T. S. E. Ng, “DRAGONN: Distributed
    randomized approximate gradients of neural networks,” in *Proceedings of the 39th
    International Conference on Machine Learning*, vol. 162.   PMLR, 17–23 Jul 2022,
    pp. 23 274–23 291.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Z. Wang, Z. Xu, X. Wu, A. Shrivastava, 和 T. S. E. Ng，“DRAGONN: 分布式随机近似神经网络梯度，”
    收录于 *第 39 届国际机器学习大会论文集*，第 162 卷。 PMLR，2022 年 7 月 17–23 日，第 23 274–23 291 页。'
- en: '[183] F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Robust and communication-efficient
    federated learning from non-i.i.d. data,” *IEEE Transactions on Neural Networks
    and Learning Systems*, vol. 31, no. 9, pp. 3400–3413, 2020.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] F. Sattler, S. Wiedemann, K.-R. Müller, 和 W. Samek，“来自非独立同分布数据的鲁棒且通信高效的联邦学习，”
    *IEEE 神经网络与学习系统汇刊*，第 31 卷，第 9 期，第 3400–3413 页，2020 年。'
- en: '[184] Z. Tang, S. Shi, B. Li, and X. Chu, “Gossipfl: A decentralized federated
    learning framework with sparsified and adaptive communication,” *IEEE Transactions
    on Parallel and Distributed Systems*, vol. 34, no. 3, pp. 909–922, 2022.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Z. Tang, S. Shi, B. Li, 和 X. Chu，“Gossipfl: 具有稀疏化和自适应通信的去中心化联邦学习框架，”
    *IEEE 并行与分布式系统汇刊*，第 34 卷，第 3 期，第 909–922 页，2022 年。'
- en: '[185] L. Yi, W. Gang, and L. Xiaoguang, “QSFL: A two-level uplink communication
    optimization framework for federated learning,” in *Proceedings of the 39th International
    Conference on Machine Learning*, vol. 162.   PMLR, 17–23 Jul 2022, pp. 25 501–25 513.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. Yi, W. Gang, 和 L. Xiaoguang，“QSFL: 联邦学习的双层上行通信优化框架，” 收录于 *第 39 届国际机器学习大会论文集*，第
    162 卷。 PMLR，2022 年 7 月 17–23 日，第 25 501–25 513 页。'
- en: '[186] P. Han, S. Wang, and K. K. Leung, “Adaptive gradient sparsification for
    efficient federated learning: An online learning approach,” in *2020 IEEE 40th
    international conference on distributed computing systems (ICDCS)*.   IEEE, 2020,
    pp. 300–310.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] P. Han, S. Wang, 和 K. K. Leung，“高效联邦学习的自适应梯度稀疏化：一种在线学习方法，” 收录于 *2020
    IEEE 第 40 届分布式计算系统国际会议 (ICDCS)*。 IEEE，2020 年，第 300–310 页。'
- en: '[187] Z. Feng, X. Chen, Q. Wu, W. Wu, X. Zhang, and Q. Huang, “Feddd: Toward
    communication-efficient federated learning with differential parameter dropout,”
    *IEEE Transactions on Mobile Computing*, pp. 1–18, 2023.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Z. Feng, X. Chen, Q. Wu, W. Wu, X. Zhang, 和 Q. Huang，“Feddd: 迈向通信高效的联邦学习，具有差分参数丢弃，”
    *IEEE 移动计算汇刊*，第 1–18 页，2023 年。'
- en: '[188] J. Fang, H. Fu, G. Yang, and C.-J. Hsieh, “Redsync: reducing synchronization
    bandwidth for distributed deep learning training system,” *Journal of Parallel
    and Distributed Computing*, vol. 133, pp. 30–39, 2019.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] J. Fang, H. Fu, G. Yang, 和 C.-J. Hsieh，“Redsync: 减少分布式深度学习训练系统的同步带宽，”
    *并行与分布式计算杂志*，第 133 卷，第 30–39 页，2019 年。'
- en: '[189] P. Jiang and G. Agrawal, “A linear speedup analysis of distributed deep
    learning with sparse and quantized communication,” *Advances in Neural Information
    Processing Systems*, vol. 31, 2018.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] P. Jiang 和 G. Agrawal，“稀疏和量化通信的分布式深度学习的线性加速分析，” *神经信息处理系统进展*，第 31 卷，2018
    年。'
- en: '[190] D. Basu, D. Data, C. Karakus, and S. Diggavi, “Qsparse-local-sgd: Distributed
    sgd with quantization, sparsification and local computations,” *Advances in Neural
    Information Processing Systems*, vol. 32, 2019.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] D. Basu, D. Data, C. Karakus, 和 S. Diggavi，“Qsparse-local-sgd: 具有量化、稀疏化和本地计算的分布式
    sgd，” *神经信息处理系统进展*，第 32 卷，2019 年。'
- en: '[191] G. Yan, T. Li, S.-L. Huang, T. Lan, and L. Song, “Ac-sgd: Adaptively
    compressed sgd for communication-efficient distributed learning,” *IEEE Journal
    on Selected Areas in Communications*, vol. 40, no. 9, pp. 2678–2693, 2022.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] G. Yan, T. Li, S.-L. Huang, T. Lan, 和 L. Song，“Ac-sgd: 自适应压缩的 sgd，用于通信高效的分布式学习，”
    *IEEE 选定领域通信杂志*，第 40 卷，第 9 期，第 2678–2693 页，2022 年。'
- en: '[192] H. Lim, D. G. Andersen, and M. Kaminsky, “3lc: Lightweight and effective
    traffic compression for distributed machine learning,” *Proceedings of Machine
    Learning and Systems*, vol. 1, pp. 53–64, 2019.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] H. Lim, D. G. Andersen, 和 M. Kaminsky，“3lc: 轻量级且有效的分布式机器学习流量压缩，” *机器学习与系统会议论文集*，第
    1 卷，第 53–64 页，2019 年。'
- en: '[193] C. Yang, Y. Zhao, G. Zhao, and H. Xu, “Dfs: Joint data formatting and
    sparsification for efficient communication in distributed machine learning,” *Computer
    Networks*, vol. 229, p. 109777, 2023.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] C. Yang, Y. Zhao, G. Zhao, 和 H. Xu，“Dfs: 分布式机器学习中高效通信的联合数据格式化和稀疏化，” *计算机网络*，第
    229 卷，第 109777 页，2023 年。'
- en: '[194] R. Song, L. Zhou, L. Lyu, A. Festag, and A. Knoll, “Resfed: Communication
    efficient federated learning with deep compressed residuals,” *IEEE Internet of
    Things Journal*, pp. 1–1, 2023.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] R. Song, L. Zhou, L. Lyu, A. Festag, 和 A. Knoll，“Resfed: 具有深度压缩残差的通信高效联邦学习，”
    *IEEE 物联网杂志*，第 1–1 页，2023 年。'
- en: '[195] L. Abrahamyan, Y. Chen, G. Bekoulis, and N. Deligiannis, “Learned gradient
    compression for distributed deep learning,” *IEEE Transactions on Neural Networks
    and Learning Systems*, vol. 33, no. 12, pp. 7330–7344, 2021.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] L. Abrahamyan, Y. Chen, G. Bekoulis, 和 N. Deligiannis，"用于分布式深度学习的学习梯度压缩"，*IEEE神经网络与学习系统汇刊*，第33卷，第12期，第7330–7344页，2021年。'
- en: '[196] Z. Wang, M. Wen, Y. Xu, Y. Zhou, J. H. Wang, and L. Zhang, “Communication
    compression techniques in distributed deep learning: A survey,” *Journal of Systems
    Architecture*, p. 102927, 2023.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Z. Wang, M. Wen, Y. Xu, Y. Zhou, J. H. Wang, 和 L. Zhang，"分布式深度学习中的通信压缩技术：综述"，*系统架构期刊*，第102927页，2023年。'
- en: '[197] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, “Scalable methods for
    8-bit training of neural networks,” in *Advances in Neural Information Processing
    Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] R. Banner, I. Hubara, E. Hoffer, 和 D. Soudry，"8位神经网络训练的可扩展方法"，在*神经信息处理系统进展*上，第31卷。Curran
    Associates, Inc.，2018年。'
- en: '[198] G. Stewart and J. Miller, “Methods of simultaneous iteration for calculating
    eigenvectors of matrices,” *Topics in Numerical Analysis II*, vol. 2, 1975.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] G. Stewart 和 J. Miller，"计算矩阵特征向量的同步迭代方法"，*数值分析专题 II*，第2卷，1975年。'
- en: '[199] S. Gunasekar, J. Lee, D. Soudry, and N. Srebro, “Characterizing implicit
    bias in terms of optimization geometry,” in *International Conference on Machine
    Learning*.   PMLR, 2018, pp. 1832–1841.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] S. Gunasekar, J. Lee, D. Soudry, 和 N. Srebro，"从优化几何学角度描述隐式偏差"，在*国际机器学习会议*上。PMLR，2018年，第1832–1841页。'
- en: '[200] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training
    recurrent neural networks,” in *International conference on machine learning*.   Pmlr,
    2013, pp. 1310–1318.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] R. Pascanu, T. Mikolov, 和 Y. Bengio，"递归神经网络训练的难度"，在*国际机器学习会议*上。Pmlr，2013年，第1310–1318页。'
- en: '[201] I. Mitliagkas, C. Zhang, S. Hadjis, and C. Ré, “Asynchrony begets momentum,
    with an application to deep learning,” in *2016 54th Annual Allerton Conference
    on Communication, Control, and Computing (Allerton)*.   IEEE, 2016, pp. 997–1004.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] I. Mitliagkas, C. Zhang, S. Hadjis, 和 C. Ré，"异步产生动量，应用于深度学习"，在*2016年第54届Allerton通信、控制与计算会议（Allerton）*上。IEEE，2016年，第997–1004页。'
- en: '[202] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training imagenet
    in 1 hour,” *arXiv preprint arXiv:1706.02677*, 2017.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, 和 K. He，"准确的大批量SGD：1小时内训练ImageNet"，*arXiv预印本 arXiv:1706.02677*，2017年。'
- en: '[203] Y. You, I. Gitman, and B. Ginsburg, “Scaling sgd batch size to 32k for
    imagenet training,” *arXiv preprint arXiv:1708.03888*, vol. 6, no. 12, p. 6, 2017.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. You, I. Gitman, 和 B. Ginsburg，"将SGD批量大小扩展到32k用于ImageNet训练"，*arXiv预印本
    arXiv:1708.03888*，第6卷，第12期，第6页，2017年。'
- en: '[204] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimization for deep learning:
    Training bert in 76 minutes,” *arXiv preprint arXiv:1904.00962*, 2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, 和 C.-J. Hsieh，"深度学习的大批量优化：在76分钟内训练BERT"，*arXiv预印本 arXiv:1904.00962*，2019年。'
- en: '[205] A. H. Robinson and C. Cherry, “Results of a prototype television bandwidth
    compression scheme,” *Proceedings of the IEEE*, vol. 55, no. 3, pp. 356–364, 1967.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] A. H. Robinson 和 C. Cherry，"原型电视带宽压缩方案的结果"，*IEEE汇刊*，第55卷，第3期，第356–364页，1967年。'
- en: '[206] W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel,
    X. Peng, H. Zhao, Q. Zhang *et al.*, “Gandiva: Introspective cluster scheduling
    for deep learning,” in *13th USENIX Symposium on Operating Systems Design and
    Implementation (OSDI 18)*, 2018, pp. 595–610.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P.
    Patel, X. Peng, H. Zhao, Q. Zhang *等*，"Gandiva：深度学习的自省集群调度"，在*第13届USENIX操作系统设计与实现研讨会（OSDI
    18）*上，2018年，第595–610页。'
- en: '[207] W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, and
    Y. Jia, “Antman: Dynamic scaling on gpu clusters for deep learning,” in *14th
    USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)*, 2020,
    pp. 533–548.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, 和 Y.
    Jia，"Antman：用于深度学习的GPU集群动态扩展"，在*第14届USENIX操作系统设计与实现研讨会（OSDI 20）*上，2020年，第533–548页。'
- en: '[208] Q. Weng, L. Yang, Y. Yu, W. Wang, X. Tang, G. Yang, and L. Zhang, “Beware
    of fragmentation: Scheduling gpu-sharing workloads with fragmentation gradient
    descent,” in *2023 USENIX Annual Technical Conference (USENIX ATC 23)*, 2023,
    pp. 995–1008.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Q. Weng, L. Yang, Y. Yu, W. Wang, X. Tang, G. Yang, 和 L. Zhang，"警惕碎片化：用碎片化梯度下降调度GPU共享工作负载"，在*2023年USENIX年度技术会议（USENIX
    ATC 23）*上，2023年，第995–1008页。'
- en: '[209] B. Wu, Z. Zhang, Z. Bai, X. Liu, and X. Jin, “Transparent gpu sharing
    in container clouds for deep learning workloads,” in *20th USENIX Symposium on
    Networked Systems Design and Implementation (NSDI 23)*, 2023, pp. 69–85.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] B. Wu, Z. Zhang, Z. Bai, X. Liu, 和 X. Jin, “容器云中透明的GPU共享以支持深度学习工作负载，”在
    *第20届USENIX网络系统设计与实现研讨会（NSDI 23）*，2023年，页码69–85。'
- en: '[210] P. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives for
    deep learning applications,” in *Proceedings of the 3rd MLSys Conference*, 2020,
    pp. 1–14.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] P. Yu 和 M. Chowdhury, “Salus: 深度学习应用的细粒度GPU共享原语，”在 *第3届MLSsys会议论文集*，2020年，页码1–14。'
- en: '[211] Z. Bai, Z. Zhang, Y. Zhu, and X. Jin, “Pipeswitch: Fast pipelined context
    switching for deep learning applications,” in *14th USENIX Symposium on Operating
    Systems Design and Implementation (OSDI 20)*, 2020, pp. 499–514.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Z. Bai, Z. Zhang, Y. Zhu, 和 X. Jin, “Pipeswitch: 深度学习应用中的快速管道上下文切换，”在
    *第14届USENIX操作系统设计与实现研讨会（OSDI 20）*，2020年，页码499–514。'
- en: '[212] Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, “Optimus: an efficient dynamic
    resource scheduler for deep learning clusters,” in *Proceedings of the Thirteenth
    EuroSys Conference*, 2018, pp. 1–14.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Y. Peng, Y. Bao, Y. Chen, C. Wu, 和 C. Guo, “Optimus: 深度学习集群中的高效动态资源调度器，”在
    *第十三届EuroSys会议论文集*，2018年，页码1–14。'
- en: '[213] Y. Bao, Y. Peng, and C. Wu, “Deep learning-based job placement in distributed
    machine learning clusters,” in *IEEE INFOCOM 2019 - IEEE Conference on Computer
    Communications*, 2019, pp. 505–513.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Y. Bao, Y. Peng, 和 C. Wu, “基于深度学习的分布式机器学习集群中的作业调度，”在 *IEEE INFOCOM 2019
    - IEEE计算机通信会议*，2019年，页码505–513。'
- en: '[214] G. Yeung, D. Borowiec, R. Yang, A. Friday, R. Harper, and P. Garraghan,
    “Horus: Interference-aware and prediction-based scheduling in deep learning systems,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 33, no. 1, pp. 88–100,
    2021.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] G. Yeung, D. Borowiec, R. Yang, A. Friday, R. Harper, 和 P. Garraghan,
    “Horus: 在深度学习系统中的干扰感知和基于预测的调度，” *IEEE并行与分布式系统汇刊*，第33卷，第1期，页码88–100，2021年。'
- en: '[215] A. Qiao, S. K. Choe, S. J. Subramanya, W. Neiswanger, Q. Ho, H. Zhang,
    G. R. Ganger, and E. P. Xing, “Pollux: Co-adaptive cluster scheduling for goodput-optimized
    deep learning,” in *15th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI 21)*, Jul. 2021, pp. 1–18.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] A. Qiao, S. K. Choe, S. J. Subramanya, W. Neiswanger, Q. Ho, H. Zhang,
    G. R. Ganger, 和 E. P. Xing, “Pollux: 协同适应性集群调度以优化深度学习的吞吐量，”在 *第15届USENIX操作系统设计与实现研讨会（OSDI
    21）*，2021年7月，页码1–18。'
- en: '[216] G. Lim, J. Ahn, W. Xiao, Y. Kwon, and M. Jeon, “Zico: Efficient GPU memory
    sharing for concurrent DNN training,” in *2021 USENIX Annual Technical Conference
    (USENIX ATC 21)*, Jul. 2021, pp. 161–175.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] G. Lim, J. Ahn, W. Xiao, Y. Kwon, 和 M. Jeon, “Zico: 高效的GPU内存共享以支持并发DNN训练，”在
    *2021年USENIX年会技术会议（USENIX ATC 21）*，2021年7月，页码161–175。'
- en: '[217] C. Hwang, T. Kim, S. Kim, J. Shin, and K. Park, “Elastic resource sharing
    for distributed deep learning,” in *18th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 21)*, 2021, pp. 721–739.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] C. Hwang, T. Kim, S. Kim, J. Shin, 和 K. Park, “分布式深度学习中的弹性资源共享，”在 *第18届USENIX网络系统设计与实现研讨会（NSDI
    21）*，2021年，页码721–739。'
- en: '[218] Y. Mao, V. Sharma, W. Zheng, L. Cheng, Q. Guan, and A. Li, “Elastic resource
    management for deep learning applications in a container cluster,” *IEEE Transactions
    on Cloud Computing*, vol. 11, no. 2, pp. 2204–2216, 2023.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Mao, V. Sharma, W. Zheng, L. Cheng, Q. Guan, 和 A. Li, “容器集群中深度学习应用的弹性资源管理，”
    *IEEE云计算汇刊*，第11卷，第2期，页码2204–2216，2023年。'
- en: '[219] P. Yu, J. Liu, and M. Chowdhury, “Fluid: Resource-aware hyperparameter
    tuning engine,” in *Proceedings of Machine Learning and Systems*, vol. 3, 2021,
    pp. 502–516.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] P. Yu, J. Liu, 和 M. Chowdhury, “Fluid: 资源感知超参数调优引擎，”在 *机器学习与系统会议论文集*，第3卷，2021年，页码502–516。'
- en: '[220] W. Gao, P. Sun, Y. Wen, and T. Zhang, “Titan: a scheduler for foundation
    model fine-tuning workloads,” in *Proceedings of the 13th Symposium on Cloud Computing*,
    2022, pp. 348–354.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] W. Gao, P. Sun, Y. Wen, 和 T. Zhang, “Titan: 基础模型微调工作负载的调度器，”在 *第13届云计算研讨会论文集*，2022年，页码348–354。'
- en: '[221] L. Liu, J. Yu, and Z. Ding, “Adaptive and efficient gpu time sharing
    for hyperparameter tuning in cloud,” in *Proceedings of the 51st International
    Conference on Parallel Processing*, 2022, pp. 1–11.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] L. Liu, J. Yu, 和 Z. Ding, “云环境中超参数调优的自适应和高效GPU时间共享，”在 *第51届国际并行处理会议论文集*，2022年，页码1–11。'
- en: '[222] Q. Hu, Z. Ye, M. Zhang, Q. Chen, P. Sun, Y. Wen, and T. Zhang, “Hydro:
    Surrogate-based hyperparameter tuning service in datacenters,” in *17th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 23)*, 2023, pp.
    757–777.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Q. Hu, Z. Ye, M. Zhang, Q. Chen, P. Sun, Y. Wen, 和 T. Zhang, “Hydro:
    数据中心中的基于替代模型的超参数调优服务，” 见于 *第17届USENIX操作系统设计与实现研讨会（OSDI 23）*，2023年，第757–777页。'
- en: '[223] R. Gu, Y. Chen, S. Liu, H. Dai, G. Chen, K. Zhang, Y. Che, and Y. Huang,
    “Liquid: Intelligent resource estimation and network-efficient scheduling for
    deep learning jobs on distributed gpu clusters,” *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 33, no. 11, pp. 2808–2820, 2022.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] R. Gu, Y. Chen, S. Liu, H. Dai, G. Chen, K. Zhang, Y. Che, 和 Y. Huang,
    “Liquid: 深度学习作业在分布式GPU集群上的智能资源估算与网络高效调度，” *IEEE并行与分布式系统学报*，第33卷，第11期，第2808–2820页，2022年。'
- en: '[224] Z. Zhang, Q. Qi, R. Shang, L. Chen, and F. Xu, “Prophet: Speeding up
    distributed dnn training with predictable communication scheduling,” in *Proceedings
    of the 50th International Conference on Parallel Processing*, 2021, pp. 1–11.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Z. Zhang, Q. Qi, R. Shang, L. Chen, 和 F. Xu, “Prophet: 通过可预测通信调度加速分布式DNN训练，”
    见于 *第50届国际并行处理会议论文集*，2021年，第1–11页。'
- en: '[225] W. Li, S. Chen, K. Li, H. Qi, R. Xu, and S. Zhang, “Efficient online
    scheduling for coflow-aware machine learning clusters,” *IEEE Transactions on
    Cloud Computing*, vol. 10, no. 4, pp. 2564–2579, 2020.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] W. Li, S. Chen, K. Li, H. Qi, R. Xu, 和 S. Zhang, “面向协同流的机器学习集群的高效在线调度，”
    *IEEE云计算学报*，第10卷，第4期，第2564–2579页，2020年。'
- en: '[226] A. Dhakal, S. G. Kulkarni, and K. Ramakrishnan, “Gslice: controlled spatial
    sharing of gpus for a scalable inference platform,” in *Proceedings of the 11th
    ACM Symposium on Cloud Computing*, 2020, pp. 492–506.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] A. Dhakal, S. G. Kulkarni, 和 K. Ramakrishnan, “Gslice: 控制GPU空间共享的可扩展推断平台，”
    见于 *第11届ACM云计算研讨会论文集*，2020年，第492–506页。'
- en: '[227] F. Xu, J. Xu, J. Chen, L. Chen, R. Shang, Z. Zhou, and F. Liu, “igniter:
    Interference-aware gpu resource provisioning for predictable dnn inference in
    the cloud,” *IEEE Transactions on Parallel and Distributed Systems*, vol. 34,
    no. 3, pp. 812–827, 2022.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] F. Xu, J. Xu, J. Chen, L. Chen, R. Shang, Z. Zhou, 和 F. Liu, “Igniter:
    具有干扰感知的GPU资源配置，以实现云中可预测的DNN推断，” *IEEE并行与分布式系统学报*，第34卷，第3期，第812–827页，2022年。'
- en: '[228] J. Cho, D. Zad Tootaghaj, L. Cao, and P. Sharma, “Sla-driven ml inference
    framework for clouds with heterogeneous accelerators,” *Proceedings of Machine
    Learning and Systems*, vol. 4, pp. 20–32, 2022.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] J. Cho, D. Zad Tootaghaj, L. Cao, 和 P. Sharma, “基于SLA的异构加速器云计算中的机器学习推断框架，”
    *机器学习与系统会议论文集*，第4卷，第20–32页，2022年。'
- en: '[229] H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy,
    and R. Sundaram, “Nexus: A gpu cluster engine for accelerating dnn-based video
    analysis,” in *Proceedings of the 27th ACM Symposium on Operating Systems Principles*,
    2019, pp. 322–337.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy,
    和 R. Sundaram, “Nexus: 一种加速DNN视频分析的GPU集群引擎，” 见于 *第27届ACM操作系统原理研讨会论文集*，2019年，第322–337页。'
- en: '[230] F. Romero, Q. Li, N. J. Yadwadkar, and C. Kozyrakis, “Infaas: Automated
    model-less inference serving,” in *2021 USENIX Annual Technical Conference (USENIX
    ATC 21)*, 2021, pp. 397–411.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] F. Romero, Q. Li, N. J. Yadwadkar, 和 C. Kozyrakis, “Infaas: 自动化无模型推断服务，”
    见于 *2021年USENIX年度技术会议（USENIX ATC 21）*，2021年，第397–411页。'
- en: '[231] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. Kandemir,
    and C. R. Das, “Cocktail: A multidimensional optimization for model serving in
    cloud,” in *19th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 22)*, 2022, pp. 1041–1057.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. Kandemir,
    和 C. R. Das, “Cocktail: 用于云中模型服务的多维优化，” 见于 *第19届USENIX网络系统设计与实现研讨会（NSDI 22）*，2022年，第1041–1057页。'
- en: '[232] S. Choi, S. Lee, Y. Kim, J. Park, Y. Kwon, and J. Huh, “Serving heterogeneous
    machine learning models on multi-gpu servers with spatio-temporal sharing,” in
    *2022 USENIX Annual Technical Conference (USENIX ATC 22)*, 2022, pp. 199–216.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] S. Choi, S. Lee, Y. Kim, J. Park, Y. Kwon, 和 J. Huh, “在多GPU服务器上通过时空共享服务异构机器学习模型，”
    见于 *2022年USENIX年度技术会议（USENIX ATC 22）*，2022年，第199–216页。'
- en: '[233] J. Gu, Y. Zhu, P. Wang, M. Chadha, and M. Gerndt, “Fast-gshare: Enabling
    efficient spatio-temporal gpu sharing in serverless computing for deep learning
    inference,” in *Proceedings of the 52nd International Conference on Parallel Processing*,
    2023, pp. 635–644.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] J. Gu, Y. Zhu, P. Wang, M. Chadha, 和 M. Gerndt, “Fast-gshare: 在无服务器计算中实现深度学习推断的高效时空GPU共享，”
    见于 *第52届国际并行处理会议论文集*，2023年，第635–644页。'
- en: '[234] D. Narayanan, K. Santhanam, F. Kazhamiaka, A. Phanishayee, and M. Zaharia,
    “Heterogeneity-aware cluster scheduling policies for deep learning workloads,”
    in *14th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    20)*, 2020, pp. 481–498.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] D. Narayanan, K. Santhanam, F. Kazhamiaka, A. Phanishayee, 和 M. Zaharia，“深度学习工作负载的异构感知集群调度策略，”发表于*第14届
    USENIX 操作系统设计与实现研讨会 (OSDI 20)*，2020年，第481–498页。'
- en: '[235] Q. Hu, P. Sun, S. Yan, Y. Wen, and T. Zhang, “Characterization and prediction
    of deep learning workloads in large-scale gpu datacenters,” in *Proceedings of
    the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2021, pp. 1–15.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Q. Hu, P. Sun, S. Yan, Y. Wen, 和 T. Zhang，“大规模 GPU 数据中心中的深度学习工作负载特征及预测，”发表于*国际高性能计算、网络、存储与分析会议论文集*，2021年，第1–15页。'
- en: '[236] Q. Weng, W. Xiao, Y. Yu, W. Wang, C. Wang, J. He, Y. Li, L. Zhang, W. Lin,
    and Y. Ding, “Mlaas in the wild: Workload analysis and scheduling in large-scale
    heterogeneous gpu clusters,” in *19th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 22)*, 2022, pp. 945–960.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Q. Weng, W. Xiao, Y. Yu, W. Wang, C. Wang, J. He, Y. Li, L. Zhang, W.
    Lin, 和 Y. Ding，“野外的 MLaaS：大规模异构 GPU 集群中的工作负载分析与调度，”发表于*第19届 USENIX 网络系统设计与实现研讨会
    (NSDI 22)*，2022年，第945–960页。'
- en: '[237] J. Li, H. Xu, Y. Zhu, Z. Liu, C. Guo, and C. Wang, “Lyra: Elastic scheduling
    for deep learning clusters,” in *Proceedings of the Eighteenth European Conference
    on Computer Systems*, 2023, pp. 835–850.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] J. Li, H. Xu, Y. Zhu, Z. Liu, C. Guo, 和 C. Wang，“Lyra: 用于深度学习集群的弹性调度，”发表于*第十八届欧洲计算机系统会议论文集*，2023年，第835–850页。'
- en: '[238] R. Cheng, C. Cai, S. Yilmaz, R. Mitra, M. Bag, M. Ghosh, and T. Xu, “Towards
    gpu memory efficiency for distributed training at scale,” in *Proceedings of the
    2023 ACM Symposium on Cloud Computing*, 2023, pp. 281–297.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] R. Cheng, C. Cai, S. Yilmaz, R. Mitra, M. Bag, M. Ghosh, 和 T. Xu，“面向大规模分布式训练的
    GPU 内存效率，”发表于*2023年 ACM 云计算研讨会论文集*，2023年，第281–297页。'
- en: '[239] Z. Ye, W. Gao, Q. Hu, P. Sun, X. Wang, Y. Luo, T. Zhang, and Y. Wen,
    “Deep learning workload scheduling in gpu datacenters: A survey,” *ACM Computing
    Surveys*, vol. 56, no. 6, pp. 1–38, 2024.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Z. Ye, W. Gao, Q. Hu, P. Sun, X. Wang, Y. Luo, T. Zhang, 和 Y. Wen，“GPU
    数据中心中的深度学习工作负载调度：综述，”*ACM 计算机调查*，第56卷，第6期，第1–38页，2024年。'
- en: '[240] “Nvidia multi-process service,” 2024\. [Online]. Available: [https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] “Nvidia 多进程服务，”2024年。[在线]。可用：[https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
- en: '[241] “Nvidia multi-instance gpu.” 2024\. [Online]. Available: [https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] “Nvidia 多实例 GPU。”2024年。[在线]。可用：[https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)'
- en: '[242] “Unified memory for cuda,” 2024\. [Online]. Available: [https://developer.nvidia.com/blog/unified-memory-cuda-beginners/](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] “CUDA 的统一内存，”2024年。[在线]。可用：[https://developer.nvidia.com/blog/unified-memory-cuda-beginners/](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)'
- en: '[243] M. Amaral, J. Polo, D. Carrera, S. Seelam, and M. Steinder, “Topology-aware
    gpu scheduling for learning workloads in cloud environments,” in *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2017, pp. 1–12.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] M. Amaral, J. Polo, D. Carrera, S. Seelam, 和 M. Steinder，“云环境中学习工作负载的拓扑感知
    GPU 调度，”发表于*国际高性能计算、网络、存储与分析会议论文集*，2017年，第1–12页。'
- en: '[244] J. Gu, M. Chowdhury, K. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo,
    “Tiresias: A GPU cluster manager for distributed deep learning,” *Networked Systems
    Design and Implementation,Networked Systems Design and Implementation*, pp. 485–500,
    Jan 2019.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] J. Gu, M. Chowdhury, K. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, 和 C.
    Guo，“Tiresias: 一个用于分布式深度学习的 GPU 集群管理器，”*网络系统设计与实现*，第485–500页，2019年1月。'
- en: '[245] K. R. Jayaram, V. Muthusamy, P. Dube, V. Ishakian, C. Wang, B. Herta,
    S. Boag, D. Arroyo, A. Tantawi, A. Verma, F. Pollok, and R. Khalaf, “Ffdl: A flexible
    multi-tenant deep learning platform.” in *Proceedings of the 20th International
    Middleware Conference*, Dec 2019, pp. 82–95.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] K. R. Jayaram, V. Muthusamy, P. Dube, V. Ishakian, C. Wang, B. Herta,
    S. Boag, D. Arroyo, A. Tantawi, A. Verma, F. Pollok, 和 R. Khalaf，“Ffdl: 一个灵活的多租户深度学习平台。”发表于*第20届国际中间件会议论文集*，2019年12月，第82–95页。'
- en: '[246] M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang,
    “Analysis of large-scalemulti-tenantgpu clusters for dnn training workloads,”
    in *2019 USENIX Annual Technical Conference (USENIX ATC 19)*, 2019, pp. 947–960.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, 和 F. Yang，“大规模多租户GPU集群的DNN训练负载分析”，发表于
    *2019 USENIX年度技术会议 (USENIX ATC 19)*，2019年，第947–960页。'
- en: '[247] A. Sultana, L. Chen, F. Xu, and X. Yuan, “E-las: Design and analysis
    of completion-time agnostic scheduling for distributed deep learning cluster,”
    in *49th International Conference on Parallel Processing - ICPP*, Aug 2020, pp.
    1–11.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] A. Sultana, L. Chen, F. Xu, 和 X. Yuan，“E-las：设计与分析面向分布式深度学习集群的时间完成无关调度”，发表于
    *第49届国际并行处理大会 - ICPP*，2020年8月，第1–11页。'
- en: '[248] M. Yu, C. Wu, B. Ji, and J. Liu, “A sum-of-ratios multi-dimensional-knapsack
    decomposition for dnn resource scheduling,” in *IEEE INFOCOM 2021 - IEEE Conference
    on Computer Communications*, May 2021, pp. 1–10.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] M. Yu, C. Wu, B. Ji, 和 J. Liu，“用于DNN资源调度的比率和多维背包分解”，发表于 *IEEE INFOCOM
    2021 - IEEE计算机通信会议*，2021年5月，第1–10页。'
- en: '[249] C. Wang, N. Yoshikane, F. Balasis, and T. Tsuritani, “Osdl: Dedicated
    optical slice provisioning in support of distributed deep learning,” *Computer
    Networks*, vol. 214, p. 109191, 2022.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] C. Wang, N. Yoshikane, F. Balasis, 和 T. Tsuritani，“Osdl：支持分布式深度学习的专用光学切片配置”，*计算机网络*，第214卷，第109191页，2022年。'
- en: '[250] Y. Luan, X. Chen, H. Zhao, Z. Yang, and Y. Dai, “Sched²: Scheduling deep
    learning training via deep reinforcement learning,” in *2019 IEEE Global Communications
    Conference (GLOBECOM)*.   IEEE, 2019, pp. 1–7.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Y. Luan, X. Chen, H. Zhao, Z. Yang, 和 Y. Dai，“Sched²：通过深度强化学习调度深度学习训练”，发表于
    *2019 IEEE全球通信会议 (GLOBECOM)*。IEEE，2019年，第1–7页。'
- en: '[251] H. Wang, Z. Liu, and H. Shen, “Job scheduling for large-scale machine
    learning clusters,” in *Proceedings of the 16th International Conference on emerging
    Networking EXperiments and Technologies*, Nov 2020, pp. 108–120.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] H. Wang, Z. Liu, 和 H. Shen，“大规模机器学习集群的作业调度”，发表于 *第16届国际网络实验与技术会议论文集*，2020年11月，第108–120页。'
- en: '[252] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,
    G. R. Ganger, P. B. Gibbons, and M. Zaharia, “Pipedream: Generalized pipeline
    parallelism for dnn training,” in *Proceedings of the 27th ACM Symposium on Operating
    Systems Principles*, 2019, pp. 1–15.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,
    G. R. Ganger, P. B. Gibbons, 和 M. Zaharia，“Pipedream：DNN训练的通用管道并行性”，发表于 *第27届ACM操作系统原理研讨会论文集*，2019年，第1–15页。'
- en: '[253] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam,
    Q. V. Le, Y. Wu *et al.*, “Gpipe: Efficient training of giant neural networks
    using pipeline parallelism,” *Advances in neural information processing systems*,
    vol. 32, 2019.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J.
    Ngiam, Q. V. Le, Y. Wu *等*，“Gpipe：利用管道并行性高效训练巨型神经网络”，*神经信息处理系统进展*，第32卷，2019年。'
- en: '[254] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu, “Dear: Accelerating
    distributed deep learning with fine-grained all-reduce pipelining,” in *2023 IEEE
    43rd International Conference on Distributed Computing Systems (ICDCS)*.   IEEE,
    2023, pp. 142–153.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, 和 C. Liu，“Dear：通过细粒度全规约管道加速分布式深度学习”，发表于
    *2023 IEEE第43届国际分布式计算系统会议 (ICDCS)*。IEEE，2023年，第142–153页。'
- en: '[255] J. H. Park, G. Yun, M. Y. Chang, N. T. Nguyen, S. Lee, J. Choi, S. H.
    Noh, and Y.-r. Choi, “Hetpipe: Enabling large dnn training on (whimpy) heterogeneous
    gpu clusters through integration of pipelined model parallelism and data parallelism,”
    in *2020 USENIX Annual Technical Conference (USENIX ATC 20)*, 2020, pp. 307–321.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] J. H. Park, G. Yun, M. Y. Chang, N. T. Nguyen, S. Lee, J. Choi, S. H.
    Noh, 和 Y.-r. Choi，“Hetpipe：通过管道模型并行性和数据并行性的结合，在（弱）异构GPU集群上实现大规模DNN训练”，发表于 *2020
    USENIX年度技术会议 (USENIX ATC 20)*，2020年，第307–321页。'
- en: '[256] P. Zhou, X. He, S. Luo, H. Yu, and G. Sun, “Jpas: Job-progress-aware
    flow scheduling for deep learning clusters,” *Journal of Network and Computer
    Applications*, p. 102590, May 2020.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] P. Zhou, X. He, S. Luo, H. Yu, 和 G. Sun，“Jpas：面向深度学习集群的作业进度感知流调度”，*网络与计算机应用期刊*，第102590页，2020年5月。'
- en: '[257] S. Wang, D. Li, and J. Geng, “Geryon: Accelerating distributed cnn training
    by network-level flow scheduling,” in *IEEE INFOCOM 2020-IEEE Conference on Computer
    Communications*.   IEEE, 2020, pp. 1678–1687.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] S. Wang, D. Li, 和 J. Geng，“Geryon：通过网络级流调度加速分布式CNN训练”，发表于 *IEEE INFOCOM
    2020-IEEE计算机通信会议*。IEEE，2020年，第1678–1687页。'
- en: '[258] M. Kang, G. Yang, Y. Yoo, and C. Yoo, “Tensorexpress: In-network communication
    scheduling for distributed deep learning,” in *2020 IEEE 13th international conference
    on cloud computing (CLOUD)*.   IEEE, 2020, pp. 25–27.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] M. Kang, G. Yang, Y. Yoo, 和 C. Yoo，“Tensorexpress：分布式深度学习的网络内通信调度，”在
    *2020 IEEE 第十三届国际云计算会议 (CLOUD)*。 IEEE，2020年，第25–27页。'
- en: '[259] Y. He, W. Cai, P. Zhou, G. Sun, S. Luo, H. Yu, and M. Guizani, “Beamer:
    Stage-aware coflow scheduling to accelerate hyper-parameter tuning in deep learning
    clusters,” *IEEE Transactions on Network and Service Management*, vol. 19, no. 2,
    pp. 1083–1097, 2021.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Y. He, W. Cai, P. Zhou, G. Sun, S. Luo, H. Yu, 和 M. Guizani，“Beamer：阶段感知的共流调度以加速深度学习集群中的超参数调整，”
    *IEEE 网络与服务管理汇刊*，第19卷，第2期，第1083–1097页，2021年。'
- en: '[260] C. Chen, S. Wang, Y. Chen, and J. Han, “Tereis: A package-based scheduling
    in deep learning systems,” in *2022 IEEE 28th International Conference on Parallel
    and Distributed Systems (ICPADS)*.   IEEE, 2023, pp. 867–874.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] C. Chen, S. Wang, Y. Chen, 和 J. Han，“Tereis：深度学习系统中的基于包的调度，”在 *2022 IEEE
    第二十八届国际并行与分布式系统会议 (ICPADS)*。 IEEE，2023年，第867–874页。'
- en: '[261] Q. Duan, C. Peng, Z. Wang, Y. Xu, S. Liu, J. Wu, and J. C. Lui, “Accelerating
    distributed dnn training via transport layer scheduling,” *IEEE Transactions on
    Parallel and Distributed Systems*, vol. 34, no. 5, pp. 1650–1666, 2023.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Q. Duan, C. Peng, Z. Wang, Y. Xu, S. Liu, J. Wu, 和 J. C. Lui，“通过传输层调度加速分布式
    DNN 训练，” *IEEE 并行与分布式系统汇刊*，第34卷，第5期，第1650–1666页，2023年。'
- en: '[262] H. Zheng, F. Xu, L. Chen, Z. Zhou, and F. Liu, “Cynthia: Cost-efficient
    cloud resource provisioning for predictable distributed deep neural network training,”
    in *Proceedings of the 48th International Conference on Parallel Processing*,
    2019, pp. 1–11.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] H. Zheng, F. Xu, L. Chen, Z. Zhou, 和 F. Liu，“Cynthia：可预测分布式深度神经网络训练的成本效益云资源配置，”在
    *第48届国际并行处理会议会议录*，2019年，第1–11页。'
- en: '[263] N. B. D. Ta, “$fc^{2}$: cloud-based cluster provisioning for distributed
    machine learning,” *Cluster Computing*, p. 1299–1315, Dec 2019.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] N. B. D. Ta，“$fc^{2}$：基于云的集群配置用于分布式机器学习，” *集群计算*，第1299–1315页，2019年12月。'
- en: '[264] A. Jahani, M. Lattuada, M. Ciavotta, D. Ardagna, E. Amaldi, and L. Zhang,
    “Optimizing on-demand gpus in the cloud for deep learning applications training,”
    in *2019 4th International Conference on Computing, Communications and Security
    (ICCCS)*, Oct 2019.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] A. Jahani, M. Lattuada, M. Ciavotta, D. Ardagna, E. Amaldi, 和 L. Zhang，“优化云端按需
    GPU 以用于深度学习应用培训，”在 *2019 第四届国际计算、通信与安全会议 (ICCCS)*，2019年10月。'
- en: '[265] F. Wang, W. Zhang, S. Lai, M. Hao, and Z. Wang, “Dynamic gpu energy optimization
    for machine learning training workloads,” *IEEE Transactions on Parallel and Distributed
    Systems*, p. 1–1, Jan 2022.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] F. Wang, W. Zhang, S. Lai, M. Hao, 和 Z. Wang，“机器学习训练负载的动态 GPU 能源优化，”
    *IEEE 并行与分布式系统汇刊*，第1–1页，2022年1月。'
- en: '[266] F. Filippini, J. Anselmi, D. Ardagna, and B. Gaujal, “A stochastic approach
    for scheduling ai training jobs in gpu-based systems,” *IEEE Transactions on Cloud
    Computing*, pp. 1–17, 2023.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] F. Filippini, J. Anselmi, D. Ardagna, 和 B. Gaujal，“用于 GPU 系统中的 AI 训练任务调度的随机方法，”
    *IEEE 云计算汇刊*，第1–17页，2023年。'
- en: '[267] Z. Chen, W. Quan, M. Wen, J. Fang, J. Yu, C. Zhang, and L. Luo, “Deep
    learning research and development platform: Characterizing and scheduling with
    qos guarantees on gpu clusters,” *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 31, no. 1, pp. 34–50, 2020.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Z. Chen, W. Quan, M. Wen, J. Fang, J. Yu, C. Zhang, 和 L. Luo，“深度学习研发平台：在
    GPU 集群上具有 QoS 保证的特性化和调度，” *IEEE 并行与分布式系统汇刊*，第31卷，第1期，第34–50页，2020年。'
- en: '[268] W. Gao, Z. Ye, P. Sun, Y. Wen, and T. Zhang, “Chronus: A novel deadline-aware
    scheduler for deep learning training jobs,” in *Proceedings of the ACM Symposium
    on Cloud Computing*, 2021, pp. 609–623.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] W. Gao, Z. Ye, P. Sun, Y. Wen, 和 T. Zhang，“Chronus：一个新颖的截止时间感知调度器用于深度学习训练任务，”在
    *ACM 云计算研讨会会议录*，2021年，第609–623页。'
- en: '[269] Z. Yang, H. Wu, Y. Xu, Y. Wu, H. Zhong, and W. Zhang, “Hydra: Deadline-aware
    and efficiency-oriented scheduling for deep learning jobs on heterogeneous gpus,”
    *IEEE Transactions on Computers*, vol. 72, no. 8, pp. 2224–2236, 2023.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Z. Yang, H. Wu, Y. Xu, Y. Wu, H. Zhong, 和 W. Zhang，“Hydra：针对异构 GPU 上深度学习任务的截止时间感知和效率导向调度，”
    *IEEE 计算机汇刊*，第72卷，第8期，第2224–2236页，2023年。'
- en: '[270] W. Liu, J. Geng, Z. Zhu, J. Cao, and Z. Lian, “Sniper: Cloud-edge collaborative
    inference scheduling with neural network similarity modeling,” in *Proceedings
    of the 59th ACM/IEEE Design Automation Conference*, ser. DAC ’22, New York, NY,
    USA, 2022, p. 505–510.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] W. Liu, J. Geng, Z. Zhu, J. Cao, 和 Z. Lian，“Sniper：云边协作推理调度与神经网络相似性建模，”在
    *第59届 ACM/IEEE 设计自动化会议会议录*，DAC ’22 系列，美国纽约，2022年，第505–510页。'
- en: '[271] Y. Li, Z. Han, Q. Zhang, Z. Li, and H. Tan, “Automating cloud deployment
    for deep learning inference of real-time online services,” in *IEEE INFOCOM 2020-IEEE
    Conference on Computer Communications*.   IEEE, 2020, pp. 1668–1677.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Y. Li, Z. Han, Q. Zhang, Z. Li, 和 H. Tan, “自动化云部署用于实时在线服务的深度学习推理，”发表于*IEEE
    INFOCOM 2020-IEEE计算机通信会议*。IEEE，2020年，第1668–1677页。'
- en: '[272] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica,
    “Clipper: A low-latency online prediction serving system,” in *14th USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 17)*, 2017, pp. 613–627.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, 和 I.
    Stoica, “Clipper: 一个低延迟在线预测服务系统，”发表于*第14届USENIX网络系统设计与实现研讨会（NSDI 17）*，2017年，第613–627页。'
- en: '[273] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J. Shao,
    and M. Reyad, “Rafiki,” *Proceedings of the VLDB Endowment*, p. 128–140, Oct 2018.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J.
    Shao, 和 M. Reyad, “Rafiki，” *VLDB基金会会议录*，第128–140页，2018年10月。'
- en: '[274] X. Tang, P. Wang, Q. Liu, W. Wang, and J. Han, “Nanily: A qos-aware scheduling
    for dnn inference workload in clouds,” in *2019 IEEE 21st International Conference
    on High Performance Computing and Communications; IEEE 17th International Conference
    on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)*,
    Aug 2019.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] X. Tang, P. Wang, Q. Liu, W. Wang, 和 J. Han, “Nanily: 一种面向云中DNN推理工作负载的QoS感知调度，”发表于*2019
    IEEE第21届高性能计算与通信国际会议；IEEE第17届智能城市国际会议；IEEE第5届数据科学与系统国际会议（HPCC/SmartCity/DSS）*，2019年8月。'
- en: '[275] H. Qin, S. Zawad, Y. Zhou, L. Yang, D. Zhao, and F. Yan, “Swift machine
    learning model serving scheduling: a region based reinforcement learning approach,”
    in *Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis*, 2019, pp. 1–23.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] H. Qin, S. Zawad, Y. Zhou, L. Yang, D. Zhao, 和 F. Yan, “Swift机器学习模型服务调度：基于区域的强化学习方法，”发表于*国际高性能计算、网络、存储与分析会议论文集*，2019年，第1–23页。'
- en: '[276] L. Wang, L. Yang, Y. Yu, W. Wang, B. Li, X. Sun, J. He, and L. Zhang,
    “Morphling: fast, near-optimal auto-configuration for cloud-native model serving,”
    in *Proceedings of the ACM Symposium on Cloud Computing*, 2021, pp. 639–653.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] L. Wang, L. Yang, Y. Yu, W. Wang, B. Li, X. Sun, J. He, 和 L. Zhang, “Morphling:
    快速、接近最优的云原生模型服务自动配置，”发表于*ACM云计算研讨会论文集*，2021年，第639–653页。'
- en: '[277] M. Chowdhury and I. Stoica, “Efficient coflow scheduling without prior
    knowledge,” *ACM SIGCOMM Computer Communication Review*, vol. 45, no. 4, pp. 393–406,
    2015.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] M. Chowdhury 和 I. Stoica, “在没有先验知识的情况下高效的coflow调度，” *ACM SIGCOMM计算机通信评论*，第45卷，第4期，第393–406页，2015年。'
- en: '[278] S. Tang, Y. Yu, H. Wang, G. Wang, W. Chen, Z. Xu, S. Guo, and W. Gao,
    “A survey on scheduling techniques in computing and network convergence,” *IEEE
    Communications Surveys & Tutorials*, 2023.'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] S. Tang, Y. Yu, H. Wang, G. Wang, W. Chen, Z. Xu, S. Guo, 和 W. Gao, “计算与网络融合中的调度技术调查，”
    *IEEE通讯调查与教程*，2023年。'
- en: '[279] A. Nukada, “Performance optimization of allreduce operation for multi-gpu
    systems,” in *2021 IEEE International Conference on Big Data (Big Data)*.   IEEE,
    2021, pp. 3107–3112.'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] A. Nukada, “多GPU系统中的allreduce操作性能优化，”发表于*2021 IEEE国际大数据会议（Big Data）*。IEEE，2021年，第3107–3112页。'
- en: '[280] (2024) Nvlink and nvswitch. [Online]. Available: [https://www.nvidia.com/en-us/data-center/nvlink/](https://www.nvidia.com/en-us/data-center/nvlink/)'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] (2024) Nvlink 和 nvswitch. [在线]. 访问网址: [https://www.nvidia.com/en-us/data-center/nvlink/](https://www.nvidia.com/en-us/data-center/nvlink/)'
- en: '[281] (2024) Gpudirect rdma. [Online]. Available: [https://developer.nvidia.com/gpudirect](https://developer.nvidia.com/gpudirect)'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] (2024) Gpudirect rdma. [在线]. 访问网址: [https://developer.nvidia.com/gpudirect](https://developer.nvidia.com/gpudirect)'
- en: '[282] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports, and P. Richtárik, “Scaling distributed machine learning
    with in-network aggregation,” in *18th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 21)*, 2021, pp. 785–808.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports, 和 P. Richtárik, “通过网络内聚合扩展分布式机器学习，”发表于*第18届USENIX网络系统设计与实现研讨会（NSDI
    21）*，2021年，第785–808页。'
- en: '[283] C. Lao, Y. Le, K. Mahajan, Y. Chen, W. Wu, A. Akella, and M. Swift, “Atp:
    In-network aggregation for multi-tenant learning,” in *18th USENIX Symposium on
    Networked Systems Design and Implementation (NSDI 21)*, 2021, pp. 741–761.'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] C. Lao, Y. Le, K. Mahajan, Y. Chen, W. Wu, A. Akella, 和 M. Swift, “Atp:
    多租户学习的网络内聚合，”发表于*第18届USENIX网络系统设计与实现研讨会（NSDI 21）*，2021年，第741–761页。'
- en: '[284] B. Zhao, C. Liu, J. Dong, Z. Cao, W. Nie, and W. Wu, “Enabling switch
    memory management for distributed training with in-network aggregation,” in *IEEE
    INFOCOM 2023-IEEE Conference on Computer Communications*.   IEEE, 2023, pp. 1–10.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] B. 赵, C. 刘, J. 董, Z. 曹, W. 聂, 和 W. 吴, “通过网络内聚合启用分布式训练的交换机内存管理,” *IEEE
    INFOCOM 2023-IEEE计算机通信会议*. IEEE, 2023, 第1–10页。'
- en: '[285] J. Fang, G. Zhao, H. Xu, C. Wu, and Z. Yu, “Grid: Gradient routing with
    in-network aggregation for distributed training,” *IEEE/ACM Transactions on Networking*,
    2023.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] J. 方, G. 赵, H. 徐, C. 吴, 和 Z. 于, “Grid: 用于分布式训练的梯度路由与网络内聚合,” *IEEE/ACM网络事务*,
    2023。'
- en: '[286] J. Fang, G. Zhao, H. Xu, Z. Yu, B. Shen, and L. Xie, “Goat: Gradient
    scheduling with collaborative in-network aggregation for distributed training,”
    in *2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)*.   IEEE,
    2023, pp. 1–10.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] J. 方, G. 赵, H. 徐, Z. 于, B. 申, 和 L. 谢, “Goat: 用于分布式训练的梯度调度与协作网络内聚合,” *2023
    IEEE/ACM第31届质量服务国际研讨会 (IWQoS)*. IEEE, 2023, 第1–10页。'
- en: '[287] N. Gebara, M. Ghobadi, and P. Costa, “In-network aggregation for shared
    machine learning clusters,” *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 829–844, 2021.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] N. 盖巴拉, M. 戈巴迪, 和 P. 科斯塔, “用于共享机器学习集群的网络内聚合,” *机器学习与系统会议论文集*, 第3卷, 第829–844页,
    2021。'
- en: '[288] Z. Li, J. Huang, Y. Li, A. Xu, S. Zhou, J. Liu, and J. Wang, “A2tp: Aggregator-aware
    in-network aggregation for multi-tenant learning,” in *Proceedings of the Eighteenth
    European Conference on Computer Systems*, 2023, pp. 639–653.'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] Z. 李, J. 黄, Y. 李, A. 徐, S. 周, J. 刘, 和 J. 王, “A2tp: 适用于多租户学习的聚合器感知网络内聚合,”
    *第十八届欧洲计算机系统会议论文集*, 2023, 第639–653页。'
- en: '[289] S. Liu, Q. Wang, J. Zhang, W. Wu, Q. Lin, Y. Liu, M. Xu, M. Canini, R. C.
    Cheung, and J. He, “In-network aggregation with transport transparency for distributed
    training,” in *Proceedings of the 28th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems, Volume 3*, 2023, pp.
    376–391.'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] S. 刘, Q. 王, J. 张, W. 吴, Q. 林, Y. 刘, M. 徐, M. 卡尼尼, R. C. 张, 和 J. 赫, “用于分布式训练的网络内聚合与传输透明度,”
    *第28届ACM国际编程语言和操作系统体系结构支持会议论文集, 第3卷*, 2023, 第376–391页。'
- en: '[290] A. Guo, Y. Hao, C. Wu, P. Haghi, Z. Pan, M. Si, D. Tao, A. Li, M. Herbordt,
    and T. Geng, “Software-hardware co-design of heterogeneous smartnic system for
    recommendation models inference and training,” in *Proceedings of the 37th International
    Conference on Supercomputing*, 2023, pp. 336–347.'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] A. 郭, Y. 郝, C. 吴, P. 哈吉, Z. 潘, M. 西, D. 陶, A. 李, M. 赫博特, 和 T. 耿, “异构智能网卡系统的软件硬件协同设计用于推荐模型推理和训练,”
    *第37届国际超级计算会议论文集*, 2023, 第336–347页。'
- en: '[291] A. Guo, T. Geng, Y. Zhang, P. Haghi, C. Wu, C. Tan, Y. Lin, A. Li, and
    M. Herbordt, “A framework for neural network inference on fpga-centric smartnics,”
    in *2022 32nd International Conference on Field-Programmable Logic and Applications
    (FPL)*.   IEEE, 2022, pp. 01–08.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] A. 郭, T. 耿, Y. 张, P. 哈吉, C. 吴, C. 谭, Y. 林, A. 李, 和 M. 赫博特, “用于FPGA中心智能网卡的神经网络推理框架,”
    *2022年第32届现场可编程逻辑与应用国际会议 (FPL)*. IEEE, 2022, 第01–08页。'
- en: '[292] (2024) Open mpi. [Online]. Available: [https://www.open-mpi.org/](https://www.open-mpi.org/)'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] (2024) Open mpi. [在线]. 可用: [https://www.open-mpi.org/](https://www.open-mpi.org/)'
- en: '[293] (2024) Facebook gloo. [Online]. Available: [https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] (2024) Facebook gloo. [在线]. 可用: [https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)'
- en: '[294] (2024) Horovod. [Online]. Available: [https://horovod.readthedocs.io/en/stable/](https://horovod.readthedocs.io/en/stable/)'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] (2024) Horovod. [在线]. 可用: [https://horovod.readthedocs.io/en/stable/](https://horovod.readthedocs.io/en/stable/)'
- en: '[295] (2024) Nccl. [Online]. Available: [https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] (2024) Nccl. [在线]. 可用: [https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)'
- en: '[296] M. Cowan, S. Maleki, M. Musuvathi, O. Saarikivi, and Y. Xiong, “Mscclang:
    Microsoft collective communication language,” in *Proceedings of the 28th ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, Volume 2*, 2023, pp. 502–514.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] M. 考恩, S. 马列基, M. 穆苏瓦提, O. 萨里基维, 和 Y. 熊, “Mscclang: 微软集体通信语言,” *第28届ACM国际编程语言和操作系统体系结构支持会议论文集,
    第2卷*, 2023, 第502–514页。'
- en: '[297] M. Cho, U. Finkler, D. Kung, and H. Hunter, “Blueconnect: Decomposing
    all-reduce for deep learning on heterogeneous network hierarchy,” *Proceedings
    of Machine Learning and Systems*, vol. 1, pp. 241–251, 2019.'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] M. 楚, U. 芬克勒, D. 孔, 和 H. 亨特, “Blueconnect: 在异构网络层次结构上解构全归约以进行深度学习,” *机器学习与系统会议论文集*,
    第1卷, 第241–251页, 2019。'
- en: '[298] G. Wang, S. Venkataraman, A. Phanishayee, N. Devanur, J. Thelin, and
    I. Stoica, “Blink: Fast and generic collectives for distributed ml,” in *Proceedings
    of Machine Learning and Systems*, vol. 2, 2020, pp. 172–186.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] G. Wang, S. Venkataraman, A. Phanishayee, N. Devanur, J. Thelin, 和 I.
    Stoica，“Blink: 快速且通用的分布式ML集体操作，”在 *机器学习与系统会议论文集*，第2卷，2020年，第172–186页。'
- en: '[299] L. Luo, P. West, J. Nelson, A. Krishnamurthy, and L. Ceze, “Plink: Discovering
    and exploiting locality for accelerated distributed training on the public cloud,”
    *Proceedings of Machine Learning and Systems*, vol. 2, pp. 82–97, 2020.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] L. Luo, P. West, J. Nelson, A. Krishnamurthy, 和 L. Ceze，“Plink: 发现并利用局部性以加速公共云上的分布式训练，”*机器学习与系统会议论文集*，第2卷，第82–97页，2020年。'
- en: '[300] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, and T. Hoefler,
    “Sparcml: High-performance sparse communication for machine learning,” in *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2019, pp. 1–15.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, 和 T. Hoefler，“Sparcml:
    高性能稀疏通信用于机器学习，”在 *国际高性能计算、网络、存储与分析会议论文集*，2019年，第1–15页。'
- en: '[301] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio, “Efficient sparse
    collective communication and its application to accelerate distributed deep learning,”
    in *Proceedings of the 2021 ACM SIGCOMM 2021 Conference*, 2021, pp. 676–691.'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, 和 A. Sapio，“高效的稀疏集体通信及其在加速分布式深度学习中的应用，”在
    *2021 ACM SIGCOMM 2021 会议论文集*，2021年，第676–691页。'
- en: '[302] H. Xu, K. Kostopoulou, A. Dutta, X. Li, A. Ntoulas, and P. Kalnis, “Deepreduce:
    A sparse-tensor communication framework for federated deep learning,” in *Advances
    in Neural Information Processing Systems*, vol. 34, 2021, pp. 21 150–21 163.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] H. Xu, K. Kostopoulou, A. Dutta, X. Li, A. Ntoulas, 和 P. Kalnis，“Deepreduce:
    联邦深度学习的稀疏张量通信框架，”在 *神经信息处理系统进展*，第34卷，2021年，第21,150–21,163页。'
- en: '[303] Z. Cai, Z. Liu, S. Maleki, M. Musuvathi, T. Mytkowicz, J. Nelson, and
    O. Saarikivi, “Synthesizing optimal collective algorithms,” in *Proceedings of
    the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming*,
    2021, pp. 62–75.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] Z. Cai, Z. Liu, S. Maleki, M. Musuvathi, T. Mytkowicz, J. Nelson, 和 O.
    Saarikivi，“合成最佳集体算法，”在 *第26届ACM SIGPLAN并行编程原则与实践研讨会论文集*，2021年，第62–75页。'
- en: '[304] A. Shah, V. Chidambaram, M. Cowan, S. Maleki, M. Musuvathi, T. Mytkowicz,
    J. Nelson, O. Saarikivi, and R. Singh, “TACCL: Guiding collective algorithm synthesis
    using communication sketches,” in *20th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 23)*.   Boston, MA: USENIX Association, Apr. 2023,
    pp. 593–612.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] A. Shah, V. Chidambaram, M. Cowan, S. Maleki, M. Musuvathi, T. Mytkowicz,
    J. Nelson, O. Saarikivi, 和 R. Singh，“TACCL: 使用通信草图指导集体算法合成，”在 *第20届USENIX网络系统设计与实施研讨会（NSDI
    23）*。波士顿，MA: USENIX协会，2023年4月，第593–612页。'
- en: '[305] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S. Xia, and J. Wu,
    “A scalable, high-performance, and fault-tolerant network architecture for distributed
    machine learning,” *IEEE/ACM Transactions on Networking*, vol. 28, no. 4, pp.
    1752–1764, 2020.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S. Xia, 和 J. Wu，“一个可扩展的高性能容错网络架构用于分布式机器学习，”*IEEE/ACM网络学报*，第28卷，第4期，第1752–1764页，2020年。'
- en: '[306] T. Hoefler, T. Bonato, D. De Sensi, S. Di Girolamo, S. Li, M. Heddes,
    J. Belk, D. Goel, M. Castro, and S. Scott, “Hammingmesh: A network topology for
    large-scale deep learning,” in *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*.   IEEE, 2022, pp. 1–18.'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] T. Hoefler, T. Bonato, D. De Sensi, S. Di Girolamo, S. Li, M. Heddes,
    J. Belk, D. Goel, M. Castro, 和 S. Scott，“Hammingmesh: 大规模深度学习的网络拓扑，”在 *SC22: 高性能计算、网络、存储与分析国际会议*。IEEE，2022年，第1–18页。'
- en: '[307] M. Khani, M. Ghobadi, M. Alizadeh, Z. Zhu, M. Glick, K. Bergman, A. Vahdat,
    B. Klenk, and E. Ebrahimi, “Sip-ml: high-bandwidth optical network interconnects
    for machine learning training,” in *Proceedings of the 2021 ACM SIGCOMM 2021 Conference*,
    2021, pp. 657–675.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] M. Khani, M. Ghobadi, M. Alizadeh, Z. Zhu, M. Glick, K. Bergman, A. Vahdat,
    B. Klenk, 和 E. Ebrahimi，“Sip-ml: 高带宽光网络互连用于机器学习训练，”在 *2021 ACM SIGCOMM 2021 会议论文集*，2021年，第657–675页。'
- en: '[308] W. Wang, M. Khazraee, Z. Zhong, M. Ghobadi, Z. Jia, D. Mudigere, Y. Zhang,
    and A. Kewitsch, “Topoopt: Co-optimizing network topology and parallelization
    strategy for distributed training jobs,” in *20th USENIX Symposium on Networked
    Systems Design and Implementation (NSDI 23)*, 2023, pp. 739–767.'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] W. Wang, M. Khazraee, Z. Zhong, M. Ghobadi, Z. Jia, D. Mudigere, Y. Zhang,
    和 A. Kewitsch，“Topoopt: 联合优化网络拓扑和并行策略以支持分布式训练任务，”在 *第20届USENIX网络系统设计与实施研讨会（NSDI
    23）*，2023年，第739–767页。'
- en: '[309] N. R. Adiga, M. A. Blumrich, D. Chen, P. Coteus, A. Gara, M. E. Giampapa,
    P. Heidelberger, S. Singh, B. D. Steinmacher-Burow, T. Takken *et al.*, “Blue
    gene/l torus interconnection network,” *IBM Journal of Research and Development*,
    vol. 49, no. 2.3, pp. 265–276, 2005.'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] N. R. 阿迪格，M. A. 布鲁姆里奇，D. 陈，P. 科特乌斯，A. 加拉，M. E. 吉安帕帕，P. 海德贝格，S. 辛格，B. D.
    斯坦马赫-布罗尔和T. 塔肯*等*，"蓝基因/ L 花环互连网络，" *IBM研究与发展杂志*，卷49，号2.3，页码 265–276，2005年。'
- en: '[310] A. Li, S. L. Song, J. Chen, J. Li, X. Liu, N. R. Tallent, and K. J. Barker,
    “Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 31, no. 1, pp. 94–110,
    2019.'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] A. 李，S. L. 宋，J. 陈，J. 李，X. 刘，N. R. 塔伦特和K. J. 巴克尔，"评估现代GPU互连：PCIe，NVLink，NV-SLI，NVSwitch和GPUDirect，"
    *IEEE并行与分布式系统交易*，卷31，号1，页码 94–110，2019年。'
- en: '[311] S. M. Nabavinejad, M. Baharloo, K.-C. Chen, M. Palesi, T. Kogel, and
    M. Ebrahimi, “An overview of efficient interconnection networks for deep neural
    network accelerators,” *IEEE Journal on Emerging and Selected Topics in Circuits
    and Systems*, vol. 10, no. 3, pp. 268–282, 2020.'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] S. M. 纳巴维内贾德，M. 巴哈鲁，K.-C. 陈，M. 帕莱西，T. 科格尔和M. 埃布拉西米，"用于深度神经网络加速器的高效互连网络概述，"
    *IEEE电路与系统新兴主题杂志*，卷10，号3，页码 268–282，2020年。'
- en: '[312] A. Feng, D. Dong, F. Lei, J. Ma, E. Yu, and R. Wang, “In-network aggregation
    for data center networks: A survey,” *Computer Communications*, vol. 198, pp.
    63–76, 2023.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] A. 冯，D. 董，F. 雷，J. 马，E. 于和R. 王，"数据中心网络中的网络内聚：一项调查，" *计算机通信*，卷 198，页码 63–76，2023年。'
- en: '[313] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar,
    S. Sengupta, and M. Sridharan, “Data center tcp (dctcp),” in *Proceedings of the
    ACM SIGCOMM 2010 Conference*, 2010, pp. 63–74.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] M. 阿里扎德，A. 格林伯格，D. A. 马尔兹，J. 帕迪耶，P. 帕特尔，B. 普拉巴卡，S. 森古普塔和M. 斯里德哈兰，"数据中心
    TCP（DCTCP），" 在*ACM SIGCOMM 2010年会议论文集*，2010年，页码 63–74。'
- en: '[314] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar,
    and S. Shenker, “pfabric: Minimal near-optimal datacenter transport,” *ACM SIGCOMM
    Computer Communication Review*, vol. 43, no. 4, pp. 435–446, 2013.'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] M. 阿里扎德，S. 杨，M. 沙利夫，S. 卡蒂，N. 麦克尼尔，B. 普拉巴卡和S. 申克，"pfabric：数据中心传输的最小近乎最佳解，"
    *ACM SIGCOMM计算机通信评论*，卷43，号4，页码 435–446，2013年。'
- en: '[315] N. Schelten, F. Steinert, A. Schulte, and B. Stabernack, “A high-throughput,
    resource-efficient implementation of the rocev2 remote dma protocol for network-attached
    hardware accelerators,” in *2020 International Conference on Field-Programmable
    Technology (ICFPT)*.   IEEE, 2020, pp. 241–249.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] N. 舍尔滕，F. 斯坦因特，A. 舒尔特和B. 施塔贝纳克，"ROCEv2远程DMA协议用于网络附加硬件加速器的高吞吐量、资源高效的实现，"
    在*2020国际可编程序技术会议（ICFPT）*。 IEEE，2020年，页码 241–249。'
- en: '[316] (2024) Nvidia connectx smartnic. [Online]. Available: [https://www.nvidia.com/en-us/networking/ethernet-adapters/](https://www.nvidia.com/en-us/networking/ethernet-adapters/)'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] (2024) Nvidia ConnectX SmartNIC. [在线]. 可访问：[https://www.nvidia.com/en-us/networking/ethernet-adapters/](https://www.nvidia.com/en-us/networking/ethernet-adapters/)'
- en: '[317] (2024) Intel ipu. [Online]. Available: [https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html](https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html)'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] (2024) Intel IPU. [在线]. 可访问：[https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html](https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html)'
- en: '[318] L. Liu, P. Zhou, G. Sun, X. Chen, T. Wu, H. Yu, and M. Guizani, “Topologies
    in distributed machine learning: Comprehensive survey, recommendations and future
    directions,” *Neurocomputing*, p. 127009, 2023.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] L. 刘，P. 周，G. 孙，X. 陈，T. 吴，H. 于和M. Guizani，"分布式机器学习中的拓扑结构：全面调查，建议和未来方向，"
    *神经计算*，页码 127009，2023年。'
- en: '[319] M. Al-Fares, A. Loukissas, and A. Vahdat, “A scalable, commodity data
    center network architecture,” *ACM SIGCOMM computer communication review*, vol. 38,
    no. 4, pp. 63–74, 2008.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] M. Al-Fares, A. Loukissas和A. Vahdat，"可扩展的通用数据中心网络架构，" *ACM SIGCOMM计算机通信评论*，卷38，号4，页码
    63–74，2008年。'
- en: '[320] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and
    S. Lu, “Bcube: a high performance, server-centric network architecture for modular
    data centers,” in *Proceedings of the ACM SIGCOMM 2009 conference on Data communication*,
    2009, pp. 63–74.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] C. 郭，G. 路，D. 李，H. 吴，X. 张，Y. 史，C. 田，Y. 张和S. 卢，"Bcube：面向模块化数据中心的高性能、服务器中心化网络架构，"
    在*2009年ACM SIGCOMM会议论文集*，2009年，页码 63–74。'
- en: '[321] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey, “Jellyfish: Networking
    data centers randomly,” in *9th USENIX Symposium on Networked Systems Design and
    Implementation (NSDI 12)*, 2012, pp. 225–238.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] A. 辛格拉，C.-Y. 洪，L. 波帕和P. B. 戈德弗雷，"Jellyfish：随机地网络数据中心，" 在*第9届USENIX网络系统设计与实施研讨会（NSDI
    12）*，2012年，页码 225–238。'
- en: '[322] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu, “Dcell: a scalable
    and fault-tolerant network structure for data centers,” in *Proceedings of the
    ACM SIGCOMM 2008 conference on Data communication*, 2008, pp. 75–86.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, 和 S. Lu，“Dcell: 一种可扩展且容错的数据中心网络结构，”
    收录于 *ACM SIGCOMM 2008年数据通信会议论文集*，2008年，页75–86。'
- en: '[323] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V. Subramanya,
    Y. Fainman, G. Papen, and A. Vahdat, “Helios: a hybrid electrical/optical switch
    architecture for modular data centers,” in *Proceedings of the ACM SIGCOMM 2010
    Conference*, 2010, pp. 339–350.'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V. Subramanya,
    Y. Fainman, G. Papen, 和 A. Vahdat，“Helios: 一种用于模块化数据中心的混合电气/光学交换架构，” 收录于 *ACM
    SIGCOMM 2010年会议论文集*，2010年，页339–350。'
- en: '[324] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. E. Ng, M. Kozuch,
    and M. Ryan, “c-through: Part-time optics in data centers,” in *Proceedings of
    the ACM SIGCOMM 2010 Conference*, 2010, pp. 327–338.'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. E. Ng, M. Kozuch,
    和 M. Ryan，“c-through: 数据中心中的部分时间光学，” 收录于 *ACM SIGCOMM 2010年会议论文集*，2010年，页327–338。'
- en: '[325] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu, Y. Zhang, X. Wen,
    and Y. Chen, “Osa: An optical switching architecture for data center networks
    with unprecedented flexibility,” *IEEE/ACM Transactions on networking*, vol. 22,
    no. 2, pp. 498–511, 2013.'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu, Y. Zhang, X. Wen,
    和 Y. Chen，“Osa: 一种具有前所未有灵活性的光学切换架构，用于数据中心网络，” *IEEE/ACM 网络事务*，第22卷，第2期，页498–511，2013年。'
- en: '[326] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “Llama: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *等*，“Llama: 开放且高效的基础语言模型，” *arXiv 预印本
    arXiv:2302.13971*，2023年。'
- en: '[327] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation
    and fine-tuned chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2: 开放基础和微调聊天模型，” *arXiv
    预印本 arXiv:2307.09288*，2023年。'
- en: '[328] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko,
    J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du,
    B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin,
    T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra,
    K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,
    R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,
    A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta,
    M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov,
    and N. Fiedel, “Palm: Scaling language modeling with pathways,” *Journal of Machine
    Learning Research*, vol. 24, no. 240, pp. 1–113, 2023.'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko,
    J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N.
    Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin,
    T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra,
    K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,
    R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,
    A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta,
    M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S.
    Petrov, 和 N. Fiedel，“Palm: 通过路径扩展语言建模，” *机器学习研究期刊*，第24卷，第240期，页1–113，2023年。'
- en: '[329] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” in *The Eleventh International Conference on Learning Representations*,
    2023.'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    和 C. Xiong，“Codegen: 一种用于代码的开放大型语言模型，具有多轮程序合成，” 收录于 *第十一届国际学习表征会议*，2023年。'
- en: '[330] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur,
    D. Rosenberg, and G. Mann, “Bloomberggpt: A large language model for finance,”
    *arXiv preprint arXiv:2303.17564*, 2023.'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur,
    D. Rosenberg, 和 G. Mann，“Bloomberggpt: 一种用于金融的大型语言模型，” *arXiv 预印本 arXiv:2303.17564*，2023年。'
- en: '[331] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti,
    D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro *et al.*, “Efficient large-scale
    language model training on gpu clusters using megatron-lm,” in *Proceedings of
    the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2021, pp. 1–15.'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti,
    D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro *等*，“在GPU集群上使用Megatron-LM进行高效的大规模语言模型训练”，载于
    *国际高性能计算、网络、存储和分析会议论文集*，2021年，第1–15页。'
- en: '[332] B. Yuan, Y. He, J. Davis, T. Zhang, T. Dao, B. Chen, P. S. Liang, C. Re,
    and C. Zhang, “Decentralized training of foundation models in heterogeneous environments,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 25 464–25 477,
    2022.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] B. Yuan, Y. He, J. Davis, T. Zhang, T. Dao, B. Chen, P. S. Liang, C.
    Re, 和 C. Zhang，“在异构环境中去中心化训练基础模型”，*神经信息处理系统进展*，第35卷，第25 464–25 477页，2022年。'
- en: '[333] S. Athlur, N. Saran, M. Sivathanu, R. Ramjee, and N. Kwatra, “Varuna:
    Scalable, low-cost training of massive deep learning models,” in *Proceedings
    of the Seventeenth European Conference on Computer Systems*, New York, NY, USA,
    2022, p. 472–487.'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] S. Athlur, N. Saran, M. Sivathanu, R. Ramjee, 和 N. Kwatra，“Varuna：可扩展、低成本的大规模深度学习模型训练”，载于
    *第十七届欧洲计算机系统会议论文集*，美国纽约，2022年，第472–487页。'
- en: '[334] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,
    Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti *et al.*, “Using deepspeed and
    megatron to train megatron-turing nlg 530b, a large-scale generative language
    model,” *arXiv preprint arXiv:2201.11990*, 2022.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,
    Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti *等*，“使用DeepSpeed和Megatron训练Megatron-Turing
    NLG 530B，一个大规模生成语言模型”，*arXiv预印本 arXiv:2201.11990*，2022年。'
- en: '[335] J. Wang, Y. Lu, B. Yuan, B. Chen, P. Liang, C. De Sa, C. Re, and C. Zhang,
    “CocktailSGD: Fine-tuning foundation models over 500Mbps networks,” in *Proceedings
    of the 40th International Conference on Machine Learning*, vol. 202.   PMLR, 23–29
    Jul 2023, pp. 36 058–36 076.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] J. Wang, Y. Lu, B. Yuan, B. Chen, P. Liang, C. De Sa, C. Re, 和 C. Zhang，“CocktailSGD：在500Mbps网络上微调基础模型”，载于
    *第40届国际机器学习大会论文集*，第202卷。 PMLR，2023年7月23–29日，第36 058–36 076页。'
- en: '[336] M. Ryabinin, T. Dettmers, M. Diskin, and A. Borzunov, “Swarm parallelism:
    Training large models can be surprisingly communication-efficient,” in *Proceedings
    of the 40th International Conference on Machine Learning*.   JMLR, 2023.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] M. Ryabinin, T. Dettmers, M. Diskin, 和 A. Borzunov，“Swarm并行性：训练大型模型的通信效率可能出乎意料”，载于
    *第40届国际机器学习大会论文集*。 JMLR，2023年。'
- en: '[337] I. Jang, Z. Yang, Z. Zhang, X. Jin, and M. Chowdhury, “Oobleck: Resilient
    distributed training of large models using pipeline templates,” in *Proceedings
    of the 29th Symposium on Operating Systems Principles*, New York, NY, USA, 2023,
    p. 382–395.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] I. Jang, Z. Yang, Z. Zhang, X. Jin, 和 M. Chowdhury，“Oobleck：使用管道模板进行大模型的弹性分布式训练”，载于
    *第29届操作系统原理研讨会论文集*，美国纽约，2023年，第382–395页。'
- en: '[338] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.
    Letaief, “Large language models empowered autonomous edge ai for connected intelligence,”
    *IEEE Communications Magazine*, pp. 1–7, 2024.'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, 和 K. B.
    Letaief，“大型语言模型赋能的自主边缘AI用于连接智能”，*IEEE通讯杂志*，第1–7页，2024年。'
- en: '| ![[Uncaptioned image]](img/0bc11e175fe17e7b57b9294560a83db2.png) | Feng Liang
    (Member, IEEE) received his Ph.D. degree in computer science from the University
    of Hong Kong. He was a senior research engineer at Huawei and CTO of Jiufeng Tech
    Co., Ltd and is currently an associate professor with Shenzhen MSU-BIT University.
    His research interests are in broad areas of distributed computing, including
    distributed databases and computing systems, distributed machine learning, and
    interdisciplinary studies in bioinformatics. He has published papers in prestigious
    venues including IEEE TPDS, IEEE TNSM, Information Sciences, HPDC, IPDPS, and
    ACSAC. |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/0bc11e175fe17e7b57b9294560a83db2.png) | Feng Liang（IEEE会员）获得了香港大学计算机科学博士学位。他曾是华为的高级研究工程师和九峰科技有限公司的首席技术官，目前是深圳MSU-BIT大学的副教授。他的研究兴趣包括广泛的分布式计算领域，如分布式数据库和计算系统、分布式机器学习以及生物信息学的交叉研究。他在包括IEEE
    TPDS、IEEE TNSM、信息科学、HPDC、IPDPS和ACSAC在内的顶级期刊上发表了论文。 |'
- en: '| ![[Uncaptioned image]](img/90794743754d90640cb7754bf0504ae5.png) | Zhen Zhang
    is currently working toward the Ph.D. degree with the Gansu Provincial Key Laboratory
    of Wearable Computing, School of Information Science and Engineering, Lanzhou
    University, Lanzhou, China. He is also a guest student with the Shenzhen MSU-BIT
    University, Shenzhen, China. His research interests includes affective computing,
    person re-identification, and deep learning. |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/90794743754d90640cb7754bf0504ae5.png) | 珍·张目前在中国兰州大学信息科学与工程学院甘肃省可穿戴计算重点实验室攻读博士学位，同时还是中国深圳MSU-BIT大学的客座学生。他的研究兴趣包括情感计算、行人再识别和深度学习。
    |'
- en: '| ![[Uncaptioned image]](img/dde7d6f205a3f731a44a9ead4f6a4323.png) | Haifeng
    Lu received the the M.Sc. degree in School of Information Science and Engineering
    in 2021 from Lanzhou University, China. He is currently pursuing the Ph.D. degree
    with with the Gansu Provincial Key Laboratory of Wearable Computing, School of
    information Science and Engineering, Lanzhou University, Lanzhou, China. He is
    also a guest student with the Shenzhen MSU-BIT University, Shenzhen, China. His
    research interests include affective computing and machine learning. |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/dde7d6f205a3f731a44a9ead4f6a4323.png) | 海峰·陆于2021年获得中国兰州大学信息科学与工程学院的硕士学位。他目前在中国兰州大学信息科学与工程学院甘肃省可穿戴计算重点实验室攻读博士学位，同时还是中国深圳MSU-BIT大学的客座学生。他的研究兴趣包括情感计算和机器学习。
    |'
- en: '| ![[Uncaptioned image]](img/dacad7a7089a56831b7f72c998c3760e.png) | Victor
    C. M. Leung (Life Fellow, IEEE) is currently a Distinguished Professor of computer
    science and software engineering with Shenzhen University, China. He is also an
    Emeritus Professor of electrical and computer engineering and the Director of
    the Laboratory for Wireless Networks and Mobile Systems, The University of British
    Columbia (UBC), Canada. His research interests include wireless networks and mobile
    systems. He has published widely in these areas. He is a Fellow of the Royal Society
    of Canada, the Canadian Academy of Engineering, and the Engineering Institute
    of Canada. He received the 1977 APEBC Gold Medal, the 1977–1981 NSERC Postgraduate
    Scholarships, the IEEE Vancouver Section Centennial Award, the 2011 UBC Killam
    Research Prize, the 2017 Canadian Award for Telecommunications Research, the 2018
    IEEE TCGCC Distinguished Technical Achievement Recognition Award, and the 2018
    ACM MSWiM Reginald Fessenden Award. He has coauthored papers that won the 2017
    IEEE ComSoc Fred W. Ellersick Prize, the 2017 IEEE Systems Journal Best Paper
    Award, the 2018 IEEE CSIM Best Journal Paper Award, and the 2019 IEEE TCGCC Best
    Journal Paper Award. He has been serving on the editorial boards of the IEEE Transactions
    on Green Communications and Networking, IEEE Transactions on Cloud Computing,
    IEEE Access, IEEE Network, and several other journals. He is named in the current
    Clarivate Analytics list of “Highly Cited Researchers.” |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/dacad7a7089a56831b7f72c998c3760e.png) | Victor C. M. Leung（IEEE
    终身会员）目前是中国深圳大学计算机科学与软件工程的杰出教授。他还担任加拿大不列颠哥伦比亚大学（UBC）电气与计算机工程荣誉教授及无线网络与移动系统实验室主任。他的研究兴趣包括无线网络和移动系统，并在这些领域发表了大量论文。他是加拿大皇家学会、加拿大工程院以及加拿大工程学会的院士。他曾获得1977年APEBC金奖、1977–1981年NSERC研究生奖学金、IEEE温哥华分会百年奖、2011年UBC
    Killam研究奖、2017年加拿大电信研究奖、2018年IEEE TCGCC杰出技术成就奖以及2018年ACM MSWiM Reginald Fessenden奖。他合著的论文曾获2017年IEEE
    ComSoc Fred W. Ellersick奖、2017年IEEE系统期刊最佳论文奖、2018年IEEE CSIM最佳期刊论文奖和2019年IEEE TCGCC最佳期刊论文奖。他还担任IEEE绿色通信与网络、IEEE云计算、IEEE
    Access、IEEE Network及其他几本期刊的编辑委员会成员。他被列入当前Clarivate Analytics的“高被引研究者”名单。 |'
- en: '| ![[Uncaptioned image]](img/4202586ff654c13edfe925fecf17d421.png) | Yanyi
    Guo received his Ph.D. degree in safety system engineering from Beiing Institute
    of Technology. He was an associate professor with BIT and is currently a senior
    researcher at Shenzhen MSU-BIT University. His research interests are mainly in
    the areas of safety modelling and evaluation. |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/4202586ff654c13edfe925fecf17d421.png) | 燕依·郭获得了北京理工大学安全系统工程的博士学位。他曾在BIT担任副教授，目前是深圳MSU-BIT大学的高级研究员。他的研究兴趣主要集中在安全建模和评估领域。
    |'
- en: '| ![[Uncaptioned image]](img/36af73c0c2bfe8679f4954cc106e2468.png) | Xiping
    Hu (Member, IEEE) received the Ph.D. degree from the University of British Columbia,
    Vancouver, BC, Canada. He is currently a professor with Beijing Institute of Technology,
    and with Shenzhen MSU-BIT University, China. He has more than 150 papers published
    and presented in prestigious conferences and journals, such as IEEE TPAMI/TMC/TPDS/TIP/JSAC,
    IEEE COMST, ACM MobiCom/MM/SIGIR/WWW, AAAI, and IJCAI. He has been serving as
    associate editor of IEEE TCSS, and the lead guest editors of IEEE IoT Journal
    and IEEE TASE etc. He has been granted several key national research projects
    as principal investigator. He was the Co-Founder and CTO of Bravolol Ltd., Hong
    Kong, a leading language learning mobile application company with over 100 million
    users, and listed as the top 2 language education platform globally. His research
    areas consist of mobile cyber-physical systems, crowd sensing and affective computing.
    |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/36af73c0c2bfe8679f4954cc106e2468.png) | Xiping Hu（IEEE 会员）获得了加拿大不列颠哥伦比亚大学的博士学位。他目前是北京理工大学和中国深圳MSU-BIT大学的教授。他在著名会议和期刊上发表和展示了超过150篇论文，如IEEE
    TPAMI/TMC/TPDS/TIP/JSAC、IEEE COMST、ACM MobiCom/MM/SIGIR/WWW、AAAI和IJCAI。他曾担任IEEE
    TCSS的副编辑，以及IEEE IoT Journal和IEEE TASE等期刊的主编。他作为首席研究员获得了多个关键国家研究项目。他是香港Bravolol
    Ltd.的联合创始人兼首席技术官，该公司是一家领先的语言学习移动应用公司，拥有超过1亿用户，并被列为全球第二大语言教育平台。他的研究领域包括移动网络物理系统、群体感知和情感计算。'
