- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:00:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:00:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2008.00364] A Survey on Text Classification: From Traditional to Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2008.00364] 文本分类调查：从传统方法到深度学习'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2008.00364](https://ar5iv.labs.arxiv.org/html/2008.00364)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2008.00364](https://ar5iv.labs.arxiv.org/html/2008.00364)
- en: 'A Survey on Text Classification: From Traditional to Deep Learning'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类调查：从传统方法到深度学习
- en: Qian Li Beihang UniversityHaidianBeijingChina [liqian@act.buaa.edu.cn](mailto:liqian@act.buaa.edu.cn)
    ,  Hao Peng Beihang UniversityHaidianBeijingChina [penghao@act.buaa.edu.cn](mailto:penghao@act.buaa.edu.cn)
    ,  Jianxin Li Beihang UniversityHaidianBeijingChina [lijx@act.buaa.edu.cn](mailto:lijx@act.buaa.edu.cn)
    ,  Congying Xia University of Illinois at ChicagoChicagoILUSA [cxia8@uic.edu](mailto:cxia8@uic.edu)
    ,  Renyu Yang University of LeedsLeedsEnglandUK [r.yang1@leeds.ac.uk](mailto:r.yang1@leeds.ac.uk)
    ,  Lichao Sun Lehigh UniversityBethlehemPAUSA [james.lichao.sun@gmail.com](mailto:james.lichao.sun@gmail.com)
    ,  Philip S. Yu University of Illinois at ChicagoChicagoILUSA [psyu@uic.edu](mailto:psyu@uic.edu)
     and  Lifang He Lehigh UniversityBethlehemPAUSA [lih319@lehigh.edu](mailto:lih319@lehigh.edu)(2021)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 钱李 北京航空航天大学海淀 北京 中国 [liqian@act.buaa.edu.cn](mailto:liqian@act.buaa.edu.cn)
    ,  郝鹏 北京航空航天大学海淀 北京 中国 [penghao@act.buaa.edu.cn](mailto:penghao@act.buaa.edu.cn)
    ,  李建新 北京航空航天大学海淀 北京 中国 [lijx@act.buaa.edu.cn](mailto:lijx@act.buaa.edu.cn) ,
     夏聪英 伊利诺伊大学芝加哥分校 芝加哥 IL 美国 [cxia8@uic.edu](mailto:cxia8@uic.edu) ,  杨仁宇 利兹大学 利兹
    英国 [r.yang1@leeds.ac.uk](mailto:r.yang1@leeds.ac.uk) ,  孙力超 莱海大学 伯利恒 PA 美国 [james.lichao.sun@gmail.com](mailto:james.lichao.sun@gmail.com)
    ,  尤哲  伊利诺伊大学芝加哥分校 芝加哥 IL 美国 [psyu@uic.edu](mailto:psyu@uic.edu)  和  何丽芳 莱海大学
    伯利恒 PA 美国 [lih319@lehigh.edu](mailto:lih319@lehigh.edu) (2021)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Text classification is the most fundamental and essential task in natural language
    processing. The last decade has seen a surge of research in this area due to the
    unprecedented success of deep learning. Numerous methods, datasets, and evaluation
    metrics have been proposed in the literature, raising the need for a comprehensive
    and updated survey. This paper fills the gap by reviewing the state-of-the-art
    approaches from 1961 to 2021, focusing on models from traditional models to deep
    learning. We create a taxonomy for text classification according to the text involved
    and the models used for feature extraction and classification. We then discuss
    each of these categories in detail, dealing with both the technical developments
    and benchmark datasets that support tests of predictions. A comprehensive comparison
    between different techniques, as well as identifying the pros and cons of various
    evaluation metrics are also provided in this survey. Finally, we conclude by summarizing
    key implications, future research directions, and the challenges facing the research
    area.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是自然语言处理中的最基本和最重要的任务。由于深度学习的前所未有的成功，过去十年在这一领域的研究激增。文献中提出了大量的方法、数据集和评估指标，因此需要进行全面和更新的调查。本文通过回顾从1961年到2021年的最先进方法来填补这一空白，重点关注从传统模型到深度学习的模型。我们根据所涉及的文本和用于特征提取和分类的模型创建了文本分类的分类法。然后，我们详细讨论了这些类别，涉及技术发展和支持预测测试的基准数据集。该调查还提供了不同技术之间的综合比较，以及各种评估指标的优缺点。最后，我们总结了关键的影响、未来的研究方向和研究领域面临的挑战。
- en: 'deep learning, traditional models, text classification, evaluation metrics,
    challenges.^†^†copyright: acmcopyright^†^†journalyear: 2021^†^†doi: 10.1145/1122445.1122456^†^†journal:
    TIST^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth:
    4'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，传统模型，文本分类，评估指标，挑战。^†^†版权：acmcopyright^†^†期刊年份：2021^†^†doi：10.1145/1122445.1122456^†^†期刊：TIST^†^†期刊卷号：37^†^†期刊期号：4^†^†文章：111^†^†出版月份：4
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 引言
- en: Text classification – the procedure of designating pre-defined labels for text
    – is an essential and significant task in many Natural Language Processing (NLP)
    applications, such as sentiment analysis ([DBLP:conf/acl/TaiSM15,](#bib.bib1)
    ; [DBLP:conf/icml/ZhuSG15,](#bib.bib2) ), topic labeling ([DBLP:journals/apin/ChenGL20,](#bib.bib3)
    ; [DBLP:journals/isci/ChenGL19,](#bib.bib4) ), question answering ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) and dialog act classification ([DBLP:conf/naacl/LeeD16,](#bib.bib7)
    ). In the era of information explosion, it is time-consuming and challenging to
    process and classify large amounts of text data manually. Besides, the accuracy
    of manual text classification can be easily influenced by human factors, such
    as fatigue and expertise. It is desirable to use machine learning methods to automate
    the text classification procedure to yield more reliable and less subjective results.
    Moreover, this can also help enhance information retrieval efficiency and alleviate
    the problem of information overload by locating the required information.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类——将预定义标签分配给文本的过程——是许多自然语言处理（NLP）应用中的一个基本且重要的任务，例如情感分析 ([DBLP:conf/acl/TaiSM15,](#bib.bib1)
    ; [DBLP:conf/icml/ZhuSG15,](#bib.bib2) )、主题标注 ([DBLP:journals/apin/ChenGL20,](#bib.bib3)
    ; [DBLP:journals/isci/ChenGL19,](#bib.bib4) )、问题回答 ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) 和对话行为分类 ([DBLP:conf/naacl/LeeD16,](#bib.bib7)
    )。在信息爆炸的时代，手动处理和分类大量文本数据既耗时又具挑战性。此外，手动文本分类的准确性容易受到人为因素的影响，如疲劳和专业知识。使用机器学习方法自动化文本分类过程可以提供更可靠、更少主观的结果。此外，这也有助于提高信息检索效率，缓解信息过载的问题，通过定位所需的信息。
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on Text Classification:
    From Traditional to Deep Learning") illustrates a flowchart of the procedures
    involved in the text classification, under the light of traditional and deep analysis.
    Text data is different from numerical, image, or signal data. It requires NLP
    techniques to be processed carefully. The first important step is to preprocess
    text data for the model. Traditional models usually need to obtain good sample
    features by artificial methods and then classify them with classic machine learning
    algorithms. Therefore, the effectiveness of the method is largely restricted by
    feature extraction. However, different from traditional models, deep learning
    integrates feature engineering into the model fitting process by learning a set
    of nonlinear transformations that serve to map features directly to outputs.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on Text Classification:
    From Traditional to Deep Learning")展示了文本分类过程中涉及的流程图，涵盖了传统分析和深度分析的视角。文本数据不同于数值、图像或信号数据。它需要使用NLP技术进行细致处理。第一步是为模型预处理文本数据。传统模型通常需要通过人工方法获得良好的样本特征，然后使用经典的机器学习算法进行分类。因此，该方法的有效性在很大程度上受到特征提取的限制。然而，与传统模型不同，深度学习通过学习一组非线性变换，将特征直接映射到输出，将特征工程集成到模型拟合过程中。'
- en: '![Refer to caption](img/4fcd0452efc814fba57563ba63589526.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fcd0452efc814fba57563ba63589526.png)'
- en: Figure 1\. Flowchart of the text classification with classic methods in each
    module. It is crucial to extract essential features for traditional methods, but
    features can be extracted automatically by deep learning methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 经典方法在每个模块中的文本分类流程图。对于传统方法来说，提取重要特征至关重要，但通过深度学习方法可以自动提取特征。
- en: From the 1960s until the 2010s, traditional text classification models dominated.
    Traditional methods mean statistics-based models, such as Naïve Bayes (NB) ([DBLP:journals/jacm/Maron61,](#bib.bib8)
    ), K-Nearest Neighbor (KNN) ([DBLP:journals/tit/CoverH67,](#bib.bib9) ), and Support
    Vector Machine (SVM) ([DBLP:conf/ecml/Joachims98,](#bib.bib10) ). Comparing with
    the earlier rule-based methods, this method has obvious advantages in accuracy
    and stability. However, these approaches still need to do feature engineering,
    which is time-consuming and costly. Besides, they usually disregard the natural
    sequential structure or contextual information in textual data, making it challenging
    to learn the semantic information of the words. Since the 2010s, text classification
    has gradually changed from traditional models to deep learning models. Compared
    with the methods based on traditional, deep learning methods avoid designing rules
    and features by humans and automatically provide semantically meaningful representations
    for text mining. Therefore, most of the text classification research works are
    based on Deep Neural Networks (DNNs) ([DBLP:conf/acl/AlyRB19,](#bib.bib11) ),
    which are data-driven approaches with high computational complexity. Few works
    focus on traditional models to settle the limitations of computation and data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从1960年代到2010年代，传统的文本分类模型占据主导地位。传统方法指的是基于统计的模型，如朴素贝叶斯（NB）([DBLP:journals/jacm/Maron61,](#bib.bib8)）、K-最近邻（KNN）([DBLP:journals/tit/CoverH67,](#bib.bib9)）和支持向量机（SVM）([DBLP:conf/ecml/Joachims98,](#bib.bib10)）。与早期的基于规则的方法相比，这些方法在准确性和稳定性上有明显的优势。然而，这些方法仍然需要进行特征工程，这既耗时又昂贵。此外，它们通常忽视了文本数据中的自然序列结构或上下文信息，使得学习词汇的语义信息变得具有挑战性。自2010年代以来，文本分类逐渐从传统模型转向深度学习模型。与传统方法相比，深度学习方法避免了人工设计规则和特征，并自动提供具有语义意义的文本挖掘表示。因此，大多数文本分类研究工作都基于深度神经网络（DNNs）([DBLP:conf/acl/AlyRB19,](#bib.bib11)），这些是以数据驱动的高计算复杂度的方法。很少有工作关注传统模型以解决计算和数据的限制。
- en: 1.1\. Major Differences and Contributions
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 主要差异和贡献
- en: 'There have been several works reviewing text classification and its subproblems
    recently. Two of them are reviews of text classification. Kowsari et al. ([DBLP:journals/information/KowsariMHMBB19,](#bib.bib12)
    ) surveyed different text feature extraction, dimensionality reduction methods,
    basic model structure for text classification, and evaluation methods. Minaee
    et al. ([DBLP:journals/corr/abs-2004-03705,](#bib.bib13) ) reviewed recent deep
    learning based text classification methods, benchmark datasets, and evaluation
    metrics. Unlike existing text classification reviews, we conclude existing models
    from traditional models to deep learning with works of recent years. Traditional
    models emphasize the feature extraction and classifier design. Once the text has
    well-designed characteristics, it can be quickly converged by training the classifier.
    DNNs can perform feature extraction automatically and learn well without domain
    knowledge. We then give the datasets and evaluation metrics for single-label and
    multi-label tasks and summarize future research challenges from data, models,
    and performance perspective. Moreover, we summarize various information in three
    tables, including the necessary information of classic deep learning models, primary
    information of main datasets, and a general benchmark of state-of-the-art methods
    under different applications. In summary, this study’s main contributions are
    as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近已有若干研究回顾了文本分类及其子问题。其中两个是关于文本分类的综述。Kowsari等([DBLP:journals/information/KowsariMHMBB19,](#bib.bib12)）调查了不同的文本特征提取、降维方法、文本分类的基本模型结构以及评估方法。Minaee等([DBLP:journals/corr/abs-2004-03705,](#bib.bib13)）回顾了最近的基于深度学习的文本分类方法、基准数据集和评估指标。与现有的文本分类综述不同，我们总结了从传统模型到深度学习的现有模型，并结合了近年来的研究工作。传统模型强调特征提取和分类器设计。一旦文本具有良好设计的特征，就可以通过训练分类器迅速收敛。DNNs可以自动进行特征提取，并在没有领域知识的情况下良好地学习。我们接着提供了单标签和多标签任务的数据集和评估指标，并总结了从数据、模型和性能角度出发的未来研究挑战。此外，我们在三个表格中总结了各种信息，包括经典深度学习模型的必要信息、主要数据集的信息以及在不同应用下的最新方法的通用基准。总之，本研究的主要贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We introduce the process and development of text classification and present
    comprehensive analysis and research on primary models – from traditional to deep
    learning models – according to their model structures. We summarize the necessary
    information of deep learning models in terms of basic model structures in Table [1](#S2.T1
    "Table 1 ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning"), including publishing
    years, methods, venues, applications, evaluation metrics, datasets and code links.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了文本分类的过程和发展，并根据模型结构对主要模型（从传统到深度学习模型）进行了全面分析和研究。我们在表[1](#S2.T1 "表 1 ‣ 2.2.
    深度学习模型 ‣ 2. 文本分类方法 ‣ 从传统到深度学习的文本分类调查")中总结了深度学习模型的必要信息，包括出版年份、方法、场所、应用、评估指标、数据集和代码链接。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We introduce the present datasets and give the formulation of main evaluation
    metrics with the comparison of metrics, including single-label and multi-label
    text classification tasks. We summarize the necessary information of primary datasets
    in Table [2](#S3.T2 "Table 2 ‣ 3\. Datasets and Evaluation Metrics ‣ A Survey
    on Text Classification: From Traditional to Deep Learning"), including the number
    of categories, average sentence length, the size of each dataset, related papers
    and data addresses.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了当前的数据集，并给出了主要评估指标的公式和指标比较，包括单标签和多标签文本分类任务。我们在表[2](#S3.T2 "表 2 ‣ 3. 数据集和评估指标
    ‣ 从传统到深度学习的文本分类调查")中总结了主要数据集的必要信息，包括类别数量、平均句子长度、每个数据集的大小、相关论文和数据地址。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We summarize classification accuracy scores of models given in their articles,
    on benchmark datasets in Table [4](#S4.T4 "Table 4 ‣ 4\. Quantitative Results
    ‣ A Survey on Text Classification: From Traditional to Deep Learning") and conclude
    the survey by discussing the main challenges facing the text classification and
    key implications stemming from this study.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结了在其文章中给出的模型分类准确率分数，在基准数据集中见表[4](#S4.T4 "表 4 ‣ 4. 定量结果 ‣ 从传统到深度学习的文本分类调查")，并通过讨论文本分类面临的主要挑战及本研究的关键影响，结束了调查。
- en: 1.2\. Organization of the Survey
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2. 调查的组织
- en: 'The rest of the survey is organized as follows. Section [2](#S2 "2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")
    summarizes the existing models related to text classification, including traditional
    and deep learning models, including a summary table. Section [3](#S3 "3\. Datasets
    and Evaluation Metrics ‣ A Survey on Text Classification: From Traditional to
    Deep Learning") introduces the primary datasets with a summary table and evaluation
    metrics on single-label and multi-label tasks. We then give quantitative results
    of the leading models in classic text classification datasets in Section [4](#S4
    "4\. Quantitative Results ‣ A Survey on Text Classification: From Traditional
    to Deep Learning"). Finally, we summarize the main challenges for deep learning
    text classification in Section [5](#S5 "5\. Future Research Challenges ‣ A Survey
    on Text Classification: From Traditional to Deep Learning") before concluding
    the article in Section [6](#S6 "6\. Conclusion ‣ A Survey on Text Classification:
    From Traditional to Deep Learning").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的调查组织如下。第[2](#S2 "2. 文本分类方法 ‣ 从传统到深度学习的文本分类调查")节总结了与文本分类相关的现有模型，包括传统模型和深度学习模型，附有总结表格。第[3](#S3
    "3. 数据集和评估指标 ‣ 从传统到深度学习的文本分类调查")节介绍了主要数据集，并附有总结表格和单标签及多标签任务的评估指标。接着在第[4](#S4 "4.
    定量结果 ‣ 从传统到深度学习的文本分类调查")节中给出经典文本分类数据集中的领先模型的定量结果。最后，在第[5](#S5 "5. 未来研究挑战 ‣ 从传统到深度学习的文本分类调查")节总结了深度学习文本分类的主要挑战，并在第[6](#S6
    "6. 结论 ‣ 从传统到深度学习的文本分类调查")节结束文章。
- en: 2\. Text Classification Methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 文本分类方法
- en: Text classification is referred to as extracting features from raw text data
    and predicting the categories of text data based on such features. Numerous models
    have been proposed in the past few decades for text classification. For traditional
    models, NB ([DBLP:journals/jacm/Maron61,](#bib.bib8) ) is the first model used
    for the text classification task. Whereafter, generic classification models are
    proposed, such as KNN ([DBLP:journals/tit/CoverH67,](#bib.bib9) ), SVM ([DBLP:conf/ecml/Joachims98,](#bib.bib10)
    ), and Random Forest (RF) ([DBLP:journals/ml/Breiman01,](#bib.bib14) ), which
    are called classifiers, widely used for text classification. Recently, the eXtreme
    Gradient Boosting (XGBoost) ([DBLP:conf/kdd/ChenG16,](#bib.bib15) ) and the Light
    Gradient Boosting Machine (LightGBM) ([DBLP:conf/nips/KeMFWCMYL17,](#bib.bib16)
    ) have arguably the potential to provide excellent performance. For deep learning
    models, TextCNN ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ) has the highest number
    of references in these models, wherein a Convolutional Neural Network (CNN) ([albawi2017understanding,](#bib.bib18)
    ) model has been introduced to solve the text classification problem for the first
    time. While not specifically designed for handling text classification tasks,
    the Bidirectional Encoder Representation from Transformers (BERT) ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ) has been widely employed when designing text classification models, considering
    its effectiveness on numerous text classification datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是指从原始文本数据中提取特征，并基于这些特征预测文本数据的类别。在过去几十年中，提出了许多文本分类模型。对于传统模型，NB ([DBLP:journals/jacm/Maron61,](#bib.bib8))
    是第一个用于文本分类任务的模型。之后，提出了通用分类模型，如 KNN ([DBLP:journals/tit/CoverH67,](#bib.bib9))、SVM
    ([DBLP:conf/ecml/Joachims98,](#bib.bib10)) 和随机森林 (RF) ([DBLP:journals/ml/Breiman01,](#bib.bib14))，这些被称为分类器，广泛用于文本分类。最近，eXtreme
    Gradient Boosting (XGBoost) ([DBLP:conf/kdd/ChenG16,](#bib.bib15)) 和 Light Gradient
    Boosting Machine (LightGBM) ([DBLP:conf/nips/KeMFWCMYL17,](#bib.bib16)) 具有提供优异性能的潜力。对于深度学习模型，TextCNN
    ([DBLP:conf/emnlp/Kim14,](#bib.bib17)) 在这些模型中被引用最多，其中介绍了一个卷积神经网络 (CNN) ([albawi2017understanding,](#bib.bib18))
    模型，这是第一次用于解决文本分类问题。虽然 BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)) 并非专门为处理文本分类任务而设计，但在设计文本分类模型时，因其在众多文本分类数据集上的有效性，已被广泛采用。
- en: 2.1\. Traditional Models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 传统模型
- en: 'Traditional models accelerate text classification with improved accuracy and
    make the application scope of traditional expand. The first thing is to preprocess
    the raw input text for training traditional models, which generally consists of
    word segmentation, data cleaning, and statistics. Then, text representation aims
    to express preprocessed text in a form that is much easier for computers and minimizes
    information loss, such as Bag-Of-Words (BOW) ([zhang2010understanding,](#bib.bib20)
    ), N-gram ([cavnar1994n,](#bib.bib21) ), Term Frequency-Inverse Document Frequency
    (TF-IDF) ([DBLP:reference/db/X09xxgr,](#bib.bib22) ), word2vec ([DBLP:journals/corr/abs-1301-3781,](#bib.bib23)
    ) and Global Vectors for word representation (GloVe)  ([DBLP:conf/emnlp/PenningtonSM14,](#bib.bib24)
    ). BOW means that all the words in the corpus are formed into a mapping array.
    According to the mapping array, a sentence can be represented as a vector. The
    $i$-th element in the vector represents the frequency of the $i$-th word in the
    mapping array of the sentence. The vector is the BOW of the sentence. At the core
    of the BOW is representing each text with a dictionary-sized vector. The individual
    value of the vector denotes the word frequency corresponding to its inherent position
    in the text. Compared to BOW, N-gram considers the information of adjacent words
    and builds a dictionary by considering the adjacent words. It is used to calculate
    the probability model of a sentence. The probability of a sentence is expressed
    as the joint probability of each word in the sentence. The probability of a sentence
    can be calculated by predicting the probability of the $N$-th word, given the
    sequence of the $(N-1)$-th words. To simplify the calculation, the N-gram model
    adopts the Markov hypothesis ([cavnar1994n,](#bib.bib21) ). A word appears only
    concerning the words that preceded it. Therefore, the N-gram model performs a
    sliding window with size N. By counting and recording the occurrence frequency
    of all fragments, the probability of a sentence can be calculated using the frequency
    of relevant fragments in the record. TF-IDF ([DBLP:reference/db/X09xxgr,](#bib.bib22)
    ) uses the word frequency and inverses the document frequency to model the text.
    TF is the word frequency of a word in a specific article, and IDF is the reciprocal
    of the proportion of the articles containing this word to the total number of
    articles in the corpus. TF-IDF is the multiplication of the two. TF-IDF assesses
    the importance of a word to one document in a set of files or a corpus. The importance
    of a word increases proportionally with the number of times it appears in a document.
    However, it decreases inversely with its frequency in the corpus as a whole. The
    word2vec ([DBLP:journals/corr/abs-1301-3781,](#bib.bib23) ) employs local context
    information to obtain word vectors, as shown in Fig. LABEL:word2vec_Glove. Word
    vector refers to a fixed-length real value vector specified as the word vector
    for any word in the corpus. The word2vec uses two essential models: CBOW and Skip-gram.
    The former is to predict the current word on the premise that the context of the
    current word is known. The latter is to predict the context when the current word
    is known. The GloVe ([DBLP:conf/emnlp/PenningtonSM14,](#bib.bib24) ) – with both
    the local context and global statistical features – trains on the nonzero elements
    in a word-word co-occurrence matrix, as shown in Fig. LABEL:word2vec_Glove. It
    enables word vectors to contain as much semantic and grammatical information as
    possible. The construction method of word vector is: firstly, the co-occurrence
    matrix of words is constructed based on the corpus, and then the word vector is
    learned based on the co-occurrence matrix and GloVe model. Finally, the represented
    text is fed into the classifier according to selected features. Here, we discuss
    some representative classifiers in detail:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型通过提高准确性来加速文本分类，并扩展了传统模型的应用范围。首先，需要对原始输入文本进行预处理，以训练传统模型，这通常包括分词、数据清理和统计。然后，文本表示旨在将预处理后的文本表达为计算机更容易处理的形式，并最小化信息丢失，如词袋模型（Bag-Of-Words,
    BOW）（[zhang2010understanding](#bib.bib20)）、N-gram（[cavnar1994n](#bib.bib21)）、词频-逆文档频率（TF-IDF）（[DBLP:reference/db/X09xxgr](#bib.bib22)）、word2vec（[DBLP:journals/corr/abs-1301-3781](#bib.bib23)）和全局词向量（GloVe）（[DBLP:conf/emnlp/PenningtonSM14](#bib.bib24)）。BOW模型意味着语料库中的所有词汇都形成了一个映射数组。根据映射数组，一个句子可以被表示为一个向量。向量中的第$i$个元素表示句子中第$i$个词在映射数组中的频率。这个向量就是句子的BOW表示。BOW的核心在于用字典大小的向量表示每个文本。向量的单个值表示对应于其在文本中的固有位置的词频。与BOW相比，N-gram考虑了相邻词的信息，并通过考虑相邻词构建了一个字典。它用于计算句子的概率模型。句子的概率表示为句子中每个词的联合概率。句子的概率可以通过预测给定前$(N-1)$个词序列的第$N$个词的概率来计算。为了简化计算，N-gram模型采用了马尔可夫假设（[cavnar1994n](#bib.bib21)）。一个词的出现仅与它之前的词有关。因此，N-gram模型执行一个大小为N的滑动窗口。通过统计和记录所有片段的出现频率，可以使用记录中相关片段的频率来计算句子的概率。TF-IDF（[DBLP:reference/db/X09xxgr](#bib.bib22)）利用词频和逆文档频率来建模文本。TF是一个词在特定文章中的词频，IDF是包含该词的文章数与语料库中文章总数的比例的倒数。TF-IDF是这两者的乘积。TF-IDF评估一个词在一组文件或语料库中的重要性。一个词的重要性与它在文档中出现的次数成正比。然而，它在整个语料库中的频率越高，重要性则成反比。word2vec（[DBLP:journals/corr/abs-1301-3781](#bib.bib23)）利用局部上下文信息来获得词向量，如图 LABEL:word2vec_Glove所示。词向量指的是指定为语料库中任何词的固定长度实值向量。word2vec使用两个基本模型：CBOW和Skip-gram。前者是基于当前词的上下文来预测当前词，后者是已知当前词的情况下预测上下文。GloVe（[DBLP:conf/emnlp/PenningtonSM14](#bib.bib24)）结合了局部上下文和全局统计特征，对词-词共现矩阵中的非零元素进行训练，如图 LABEL:word2vec_Glove所示。这使得词向量包含尽可能多的语义和语法信息。词向量的构建方法是：首先，基于语料库构建词共现矩阵，然后根据共现矩阵和GloVe模型学习词向量。最后，根据选定的特征将表示的文本输入分类器。在这里，我们详细讨论一些代表性的分类器：
- en: '![Refer to caption](img/aa9209cefa5ac9b71125e53d2ed2711e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa9209cefa5ac9b71125e53d2ed2711e.png)'
- en: (a) CBOW.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CBOW.
- en: '![Refer to caption](img/78b92112a7be854a30992df7e0cd7f71.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78b92112a7be854a30992df7e0cd7f71.png)'
- en: (b) Skip-gram.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Skip-gram.
- en: Figure 2\. The structure of word2vec, including CBOW and Skip-gram.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. word2vec的结构，包括CBOW和Skip-gram。
- en: '![Refer to caption](img/2ca71673474224bb384f54fecbc2168b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ca71673474224bb384f54fecbc2168b.png)'
- en: Figure 3\. The structure of Naïve Bayes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. Naïve Bayes的结构。
- en: 2.1.1\. PGM-based methods
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 基于PGM的方法
- en: Probabilistic Graphical Models (PGMs) express the conditional dependencies among
    features in graphs, such as the Bayesian network ([DBLP:conf/kdd/ZhangZ10,](#bib.bib25)
    ). It is combinations of probability theory and graph theory.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 概率图模型（PGMs）通过图形表达特征之间的条件依赖关系，如贝叶斯网络（[DBLP:conf/kdd/ZhangZ10,](#bib.bib25)）。这是概率论和图论的结合。
- en: 'Naïve Bayes (NB) ([DBLP:journals/jacm/Maron61,](#bib.bib8) ) is the simplest
    and most broadly used model based on applying Bayes’ theorem. The NB algorithm
    has an independent assumption: when the target value has been given, the conditions
    between text $T=[T_{1},T_{2},\cdots,T_{n}]$ are independent (see Fig. [3](#S2.F3
    "Figure 3 ‣ 2.1\. Traditional Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning")). The NB algorithm
    primarily uses the prior probability to calculate the posterior probability $\mathrm{P}\left(y\mid\mathrm{T}_{1},T_{2},\cdots,T_{n}\right)=\frac{p(y)\prod_{j=1}^{n}p\left(T_{j}\mid
    y\right)}{\prod_{j=1}^{\mathrm{n}}p\left(T_{j}\right)}$. Due to its simple structure,
    NB is broadly used for text classification tasks. Although the assumption that
    the features are independent is sometimes not actual, it substantially simplifies
    the calculation process and performs better. To improve the performance on smaller
    categories, Schneider ([DBLP:conf/acl/Schneider04,](#bib.bib26) ) proposes a feature
    selection score method through calculating KL-divergence ([10.5555/1146355,](#bib.bib27)
    ) between the training set and corresponding categories for multinomial NB text
    classification. Dai et al. ([DBLP:conf/aaai/DaiXYY07,](#bib.bib28) ) propose a
    transfer learning method named Naive Bayes Transfer Classification (NBTC) to settle
    the different distribution between the training set and the target set. It uses
    the EM algorithm ([A1977Maximum,](#bib.bib29) ) to obtain a locally optimal posterior
    hypothesis on the target set. NB classifier is also used for fake news detection
    ([8100379,](#bib.bib30) ), and sentiment analysis ([inproceedings2017,](#bib.bib31)
    ), which can be seen as a text classification task. Bernoulli NB, Gaussian NB
    and Multinomial NB are three popular approaches of NB text classification ([DBLP:journals/jis/Xu18,](#bib.bib32)
    ). Multinomial NB performs slightly better than Bernoulli NB on few labeled dataset([8776800,](#bib.bib33)
    ). Bayesian NB classifier with Gaussian event model ([DBLP:journals/jis/Xu18,](#bib.bib32)
    ) has been proven to be superior to NB with multinomial event model on 20 Newsgroups
    (20NG) ([datasets-for-single-label-textcategorization,](#bib.bib34) ) and WebKB
    ([DBLP:conf/aaai/CravenFMMNS98,](#bib.bib35) ) datasets.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Naïve Bayes（NB）（[DBLP:journals/jacm/Maron61,](#bib.bib8)）是基于应用贝叶斯定理的最简单和最广泛使用的模型。NB算法有一个独立假设：在给定目标值时，文本$T=[T_{1},T_{2},\cdots,T_{n}]$中的条件是独立的（见图[3](#S2.F3
    "Figure 3 ‣ 2.1\. Traditional Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning")）。NB算法主要使用先验概率来计算后验概率$\mathrm{P}\left(y\mid\mathrm{T}_{1},T_{2},\cdots,T_{n}\right)=\frac{p(y)\prod_{j=1}^{n}p\left(T_{j}\mid
    y\right)}{\prod_{j=1}^{\mathrm{n}}p\left(T_{j}\right)}$。由于其简单结构，NB广泛用于文本分类任务。尽管特征独立的假设有时并不实际，但它大大简化了计算过程并表现更好。为了提高在小类别上的表现，Schneider（[DBLP:conf/acl/Schneider04,](#bib.bib26)）提出了一种特征选择评分方法，通过计算训练集与相应类别之间的KL散度（[10.5555/1146355,](#bib.bib27)）用于多项式NB文本分类。Dai等人（[DBLP:conf/aaai/DaiXYY07,](#bib.bib28)）提出了一种称为Naive
    Bayes Transfer Classification（NBTC）的迁移学习方法，以解决训练集与目标集之间的不同分布问题。它使用EM算法（[A1977Maximum,](#bib.bib29)）在目标集上获得局部最优的后验假设。NB分类器也用于虚假新闻检测（[8100379,](#bib.bib30)）和情感分析（[inproceedings2017,](#bib.bib31)），可以看作是一种文本分类任务。Bernoulli
    NB、Gaussian NB和Multinomial NB是NB文本分类的三种流行方法（[DBLP:journals/jis/Xu18,](#bib.bib32)）。在少量标注数据集上，多项式NB的表现略优于Bernoulli
    NB（[8776800,](#bib.bib33)）。与多项式事件模型相比，具有Gaussian事件模型的贝叶斯NB分类器（[DBLP:journals/jis/Xu18,](#bib.bib32)）在20
    Newsgroups（20NG）（[datasets-for-single-label-textcategorization,](#bib.bib34)）和WebKB（[DBLP:conf/aaai/CravenFMMNS98,](#bib.bib35)）数据集上表现更优。'
- en: '![Refer to caption](img/93be02d988185e5b2bc819e4af213e42.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93be02d988185e5b2bc819e4af213e42.png)'
- en: Figure 4\. The structure of KNN where $k=4$ (left) and the structure of SVM
    (right). Each node represents a text and nodes with different contours represent
    different categories.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. KNN的结构，其中$k=4$（左侧）和SVM的结构（右侧）。每个节点代表一个文本，具有不同轮廓的节点代表不同类别。
- en: 2.1.2\. KNN-based Methods
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 基于KNN的方法
- en: 'At the core of the K-Nearest Neighbors (KNN) algorithm ([DBLP:journals/tit/CoverH67,](#bib.bib9)
    ) is to classify an unlabeled sample by finding the category with most samples
    on the $k$ nearest samples. It is a simple classifier without building the model
    and can decrease complexity through the fasting process of getting $k$ nearest
    neighbors. Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1\. PGM-based methods ‣ 2.1\. Traditional
    Models ‣ 2\. Text Classification Methods ‣ A Survey on Text Classification: From
    Traditional to Deep Learning") showcases the structure of KNN. We can find $k$
    training texts approaching a specific text to be classified through estimating
    the in-between distance. Hence, the text can be divided into the most common categories
    found in $k$ training set texts. The improvement of KNN algorithm mainly includes
    feature similarity ([7866706,](#bib.bib36) ), $K$ value ([10.1145/1039621.1039623,](#bib.bib37)
    ) and index optimization ([Chen_2018,](#bib.bib38) ). However, due to the positive
    correlation between model time/space complexity and the amount of data, the KNN
    algorithm takes an unusually long time on the large-scale datasets ([DBLP:journals/eswa/JiangPWK12,](#bib.bib39)
    ). To decrease the number of selected features, Soucy et al. ([DBLP:conf/icdm/SoucyM01,](#bib.bib40)
    ) propose a KNN algorithm without feature weighting. It manages to find relevant
    features, building the inter-dependencies of words by using a feature selection.
    When the data is extremely unevenly distributed, KNN tends to classify samples
    with more data. The Neighbor-Weighted K-Nearest Neighbor (NWKNN) ([DBLP:journals/eswa/Tan05,](#bib.bib41)
    ) is proposed to improve classification performance on the unbalanced corpora.
    It casts a significant weight for neighbors in a small category and a small weight
    for neighbors in a broad class.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'K-Nearest Neighbors (KNN) 算法的核心在于通过寻找$k$个最近样本中的类别来对未标记的样本进行分类 ([DBLP:journals/tit/CoverH67,](#bib.bib9)
    )。这是一种简单的分类器，无需构建模型，通过获取$k$个最近邻样本的快速过程来降低复杂性。图 [4](#S2.F4 "Figure 4 ‣ 2.1.1\.
    PGM-based methods ‣ 2.1\. Traditional Models ‣ 2\. Text Classification Methods
    ‣ A Survey on Text Classification: From Traditional to Deep Learning") 展示了KNN的结构。我们可以通过估算样本之间的距离来找到接近待分类文本的$k$个训练文本。因此，文本可以被划分到$k$个训练集文本中最常见的类别中。KNN算法的改进主要包括特征相似度
    ([7866706,](#bib.bib36) )、$K$值 ([10.1145/1039621.1039623,](#bib.bib37) ) 和索引优化
    ([Chen_2018,](#bib.bib38) )。然而，由于模型时间/空间复杂度与数据量之间的正相关性，KNN算法在大规模数据集上需要异常长的时间 ([DBLP:journals/eswa/JiangPWK12,](#bib.bib39)
    )。为了减少选择的特征数量，Soucy 等人 ([DBLP:conf/icdm/SoucyM01,](#bib.bib40) ) 提出了一个没有特征加权的KNN算法。该算法通过使用特征选择来找到相关特征，建立词语之间的依赖关系。当数据极其不均匀分布时，KNN倾向于分类数据量更多的样本。Neighbor-Weighted
    K-Nearest Neighbor (NWKNN) ([DBLP:journals/eswa/Tan05,](#bib.bib41) ) 被提出以提高在不平衡语料库上的分类性能。它对小类别的邻居施加显著的权重，对大类别的邻居施加较小的权重。'
- en: 2.1.3\. SVM-based Methods
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 基于SVM的方法
- en: 'Cortes and Vapnik ([DBLP:journals/ml/CortesV95,](#bib.bib42) ) propose Support
    Vector Machine (SVM) to tackle the binary classification of pattern recognition.
    Joachims ([DBLP:conf/ecml/Joachims98,](#bib.bib10) ), for the first time, uses
    the SVM method for text classification representing each text as a vector. As
    illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ 2.1.1\. PGM-based methods ‣ 2.1\. Traditional
    Models ‣ 2\. Text Classification Methods ‣ A Survey on Text Classification: From
    Traditional to Deep Learning"), SVM-based approaches turn text classification
    tasks into multiple binary classification tasks. In this context, SVM constructs
    an optimal hyperplane in the one-dimensional input space or feature space, maximizing
    the distance between the hyperplane and the two categories of training sets, thereby
    achieving the best generalization ability. The goal is to make the distance of
    the category boundary along the direction perpendicular to the hyperplane is the
    largest. Equivalently, this will result in the lowest error rate of classification.
    Constructing an optimal hyperplane can be transformed into a quadratic programming
    problem to obtain a globally optimal solution. Choosing the appropriate kernel
    function ([leslie2001spectrum,](#bib.bib43) ) and feature selection ([taira1999feature,](#bib.bib44)
    ) are of the utmost importance to ensure SVM can deal with nonlinear problems
    and become a robust nonlinear classifier. Furthermore, active learning ([li2013active,](#bib.bib45)
    ) and adaptive learning ([peng2008svm,](#bib.bib46) ) method are used for text
    classification to reduce the labeling effort based on the supervised learning
    algorithm SVM. To analyze what the SVM algorithms learn and what tasks are suitable,
    Joachims ([DBLP:conf/sigir/Joachims01,](#bib.bib47) ) proposes a theoretical learning
    model combining the statistical traits with the generalization performance of
    an SVM, analyzing the features and benefits using a quantitative approach. Transductive
    Support Vector Machine (TSVM) ([JOACHIMS1999Transductive,](#bib.bib48) ) is proposed
    to lessen misclassifications of the particular test collections with a general
    decision function considering a specific test set. It uses prior knowledge to
    establish a more suitable structure and study faster.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cortes和Vapnik（[DBLP:journals/ml/CortesV95,](#bib.bib42)）提出了支持向量机（SVM）来解决模式识别中的二分类问题。Joachims（[DBLP:conf/ecml/Joachims98,](#bib.bib10)）首次使用SVM方法进行文本分类，将每个文本表示为一个向量。如图[4](#S2.F4
    "Figure 4 ‣ 2.1.1\. PGM-based methods ‣ 2.1\. Traditional Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")所示，基于SVM的方法将文本分类任务转化为多个二分类任务。在这种背景下，SVM在一维输入空间或特征空间中构造一个最佳超平面，最大化超平面与两个类别训练集之间的距离，从而实现最佳的泛化能力。其目标是使得沿着与超平面垂直的方向，类别边界的距离最大化。等效地，这将导致分类错误率最低。构造最佳超平面可以转化为一个二次规划问题，以获得全局最优解。选择合适的核函数（[leslie2001spectrum,](#bib.bib43)）和特征选择（[taira1999feature,](#bib.bib44)）对于确保SVM能够处理非线性问题并成为一个强大的非线性分类器至关重要。此外，主动学习（[li2013active,](#bib.bib45)）和自适应学习（[peng2008svm,](#bib.bib46)）方法被用于文本分类，以减少基于监督学习算法SVM的标注工作量。为了分析SVM算法学习了什么以及适合哪些任务，Joachims（[DBLP:conf/sigir/Joachims01,](#bib.bib47)）提出了一种结合统计特征和SVM泛化性能的理论学习模型，采用定量方法分析特征和优点。转导支持向量机（TSVM）（[JOACHIMS1999Transductive,](#bib.bib48)）旨在减少特定测试集合的误分类，考虑特定测试集的通用决策函数。它利用先验知识建立更合适的结构，并进行更快的研究。'
- en: '![Refer to caption](img/05cc359a88a92ca5cf06c0fc33f0888f.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05cc359a88a92ca5cf06c0fc33f0888f.png)'
- en: Figure 5\. An example of DT (left) and the structure of RF (right). The nodes
    with the dotted outline represent the nodes of the decision route. It has five
    features to predict whether each text belongs to category A or B.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. DT的示例（左）和RF的结构（右）。带有虚线轮廓的节点表示决策路径的节点。它有五个特征来预测每个文本是否属于类别A或B。
- en: 2.1.4\. DT-based Methods
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 基于DT的方法
- en: 'Decision Trees (DT) ([DBLP:books/daglib/0087929,](#bib.bib49) ) is a supervised
    tree structure learning method – reflective of the idea of divide-and-conquer
    – and is constructed recursively. It learns disjunctive expressions and has robustness
    for the text with noise. As shown in Fig. [5](#S2.F5 "Figure 5 ‣ 2.1.3\. SVM-based
    Methods ‣ 2.1\. Traditional Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning"), decision trees can
    be generally divided into two distinct stages: tree construction and tree pruning.
    It starts at the root node and tests the data samples (composed of instance sets,
    which have several attributes), and divides the dataset into diverse subsets according
    to different results. A subset of datasets constitutes a child node, and every
    leaf node in the decision tree represents a category. Constructing the decision
    tree is to determine the correlation between classes and attributes, further exploited
    to predict the record categories of unknown forthcoming types. The classification
    rules generated by the decision tree algorithm are straight-forward, and the pruning
    strategy ([DBLP:journals/datamine/RastogiS00,](#bib.bib50) ) can also help reduce
    the influence of noise. Its limitation, however, mainly derives from inefficiency
    in coping with explosively increasing data size. More specifically, the Iterative
    Dichotomiser 3 (ID3) ([Ross1986Induction,](#bib.bib51) ) algorithm uses information
    gain as the attribute selection criterion in the selection of each node – It is
    used to select the attribute of each branch node, and then select the attribute
    having the maximum information gain value to become the discriminant attribute
    of the current node. Based on ID3, C4.5 ([10.5555/152181,](#bib.bib52) ) learns
    to obtain a map from attributes to classes, which effectively classifies entities
    unknown to new categories. DT based algorithms usually need to train for each
    dataset, which is low efficiency ([kamber1997generalization,](#bib.bib53) ). Thus,
    Johnson et al. ([DBLP:journals/ibmsj/JohnsonOZG02,](#bib.bib54) ) propose a DT-based
    symbolic rule system. The method represents each text as a vector calculated by
    the frequency of each word in the text, and induces rules from the training data.
    The learning rules are used for classifying the other data, being similar to the
    training data. Furthermore, to reduce the computational costs of DT algorithms,
    Fast Decision-Tree (FDT) ([DBLP:conf/icdm/VateekulK09,](#bib.bib55) ) uses a two-pronged
    strategy: pre-selecting a feature set and training multiple DTs on different data
    subsets. Results from multiple DTs are combined through a data-fusion technique
    to resolve the cases of imbalanced classes.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '决策树（DT）（[DBLP:books/daglib/0087929](#bib.bib49)）是一种监督式的树结构学习方法——反映了分治思想——并且是递归构建的。它学习析取表达式，对噪声文本具有鲁棒性。如图[5](#S2.F5
    "Figure 5 ‣ 2.1.3\. SVM-based Methods ‣ 2.1\. Traditional Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")所示，决策树一般可以分为两个不同的阶段：树构建和树剪枝。它从根节点开始，测试数据样本（由具有多个属性的实例集组成），并根据不同的结果将数据集划分为不同的子集。数据集的子集构成一个子节点，决策树中的每个叶节点表示一个类别。构建决策树是为了确定类别与属性之间的关联，进一步用于预测未知类型记录的类别。决策树算法生成的分类规则非常直观，剪枝策略（[DBLP:journals/datamine/RastogiS00](#bib.bib50)）也可以帮助减少噪声的影响。然而，它的限制主要来自于应对爆炸性增加的数据量时的低效。更具体地说，迭代二分法3（ID3）（[Ross1986Induction](#bib.bib51)）算法使用信息增益作为每个节点选择属性的标准——它用于选择每个分支节点的属性，然后选择具有最大信息增益值的属性作为当前节点的判别属性。在ID3的基础上，C4.5（[10.5555/152181](#bib.bib52)）学习获取从属性到类别的映射，有效地对未知类别的实体进行分类。基于DT的算法通常需要对每个数据集进行训练，这效率较低（[kamber1997generalization](#bib.bib53)）。因此，Johnson等人（[DBLP:journals/ibmsj/JohnsonOZG02](#bib.bib54)）提出了一种基于DT的符号规则系统。该方法将每个文本表示为由文本中每个词的频率计算出的向量，并从训练数据中引出规则。这些学习规则用于分类与训练数据相似的其他数据。此外，为了减少DT算法的计算成本，快速决策树（FDT）（[DBLP:conf/icdm/VateekulK09](#bib.bib55)）使用了双管齐下的策略：预选择特征集和在不同的数据子集上训练多个DT。通过数据融合技术将多个DT的结果结合起来，以解决类别不平衡的情况。'
- en: 2.1.5\. Integration-based Methods
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. 基于集成的方法
- en: 'Integrated algorithms aim to aggregate the results of multiple algorithms for
    better performance and interpretation. Conventional integrated algorithms are
    bootstrap aggregation, such as RF ([DBLP:journals/ml/Breiman01,](#bib.bib14) ),
    boosting such as the Adaptive Boosting (AdaBoost) ([DBLP:conf/eurocolt/FreundS95,](#bib.bib56)
    ), and XGBoost ([DBLP:conf/kdd/ChenG16,](#bib.bib15) ) and stacking. The bootstrap
    aggregation method trains multiple classifiers without strong dependencies and
    then aggregates their results. For instance, RF ([DBLP:journals/ml/Breiman01,](#bib.bib14)
    ) consists of multiple tree classifiers wherein all trees depend on the value
    of the random vector sampled independently (depicted in Fig. [5](#S2.F5 "Figure
    5 ‣ 2.1.3\. SVM-based Methods ‣ 2.1\. Traditional Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")).
    It is worth noting that each tree within the RF shares the same distribution.
    The generalization error of an RF relies on the strength of each tree and the
    relationship among trees, and will converge to a limit with the increment of tree
    number in the forest. In boosting based algorithms, all labeled data are trained
    with the same weight to initially obtain a weaker classifier ([DBLP:journals/ml/SchapireS99,](#bib.bib57)
    ). The weights of the data will then be adjusted according to the former result
    of the classifier. The training procedure will continue by repeating such steps
    until the termination condition is reached. Unlike bootstrap and boosting algorithms,
    stacking based algorithms break down the data into $n$ parts and use $n$ classifiers
    to calculate the input data in a cascade manner – Result from upstream classifier
    will feed into the downstream classifier as input. The training will terminate
    once a pre-defined iteration number is targeted. The integrated method can capture
    more features from multiple trees. However, it helps little for short text. Motivated
    by this, Bouaziz et al.  ([DBLP:conf/dawak/BouazizDPPL14,](#bib.bib58) ) combine
    data enrichment – with semantics in RFs for short text classification – to overcome
    the deficiency of sparseness and insufficiency of contextual information. In integrated
    algorithms, not all classifiers learn well. It is necessary to give different
    weights for each classifier. To differentiate contributions of trees in a forest,
    Islam et al.  ([DBLP:conf/cikm/IslamLL0019,](#bib.bib59) ) exploit the Semantics
    Aware Random Forest (SARF) classifier, choosing features similar to the features
    of the same class, for extracting features and producing the prediction values.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '集成算法旨在汇总多个算法的结果，以提高性能和解释能力。传统的集成算法包括自助聚合方法，例如 RF ([DBLP:journals/ml/Breiman01,](#bib.bib14)
    )、提升方法如自适应提升（AdaBoost） ([DBLP:conf/eurocolt/FreundS95,](#bib.bib56) ) 和 XGBoost
    ([DBLP:conf/kdd/ChenG16,](#bib.bib15) ) 以及堆叠方法。自助聚合方法训练多个分类器，没有强依赖关系，然后汇总它们的结果。例如，RF
    ([DBLP:journals/ml/Breiman01,](#bib.bib14) ) 由多个树分类器组成，其中所有树都依赖于独立采样的随机向量的值（如图
    [5](#S2.F5 "Figure 5 ‣ 2.1.3\. SVM-based Methods ‣ 2.1\. Traditional Models ‣
    2\. Text Classification Methods ‣ A Survey on Text Classification: From Traditional
    to Deep Learning") 所示）。值得注意的是，RF 中的每棵树共享相同的分布。RF 的泛化误差依赖于每棵树的强度以及树之间的关系，随着森林中树木数量的增加，误差会收敛到一个极限。在基于提升的算法中，所有标记数据以相同的权重进行训练，以初步获得一个较弱的分类器
    ([DBLP:journals/ml/SchapireS99,](#bib.bib57) )。然后，根据分类器的前一个结果调整数据的权重。训练过程将通过重复这些步骤继续进行，直到达到终止条件。与自助聚合和提升算法不同，基于堆叠的算法将数据分解为
    $n$ 部分，并使用 $n$ 个分类器以级联方式计算输入数据——上游分类器的结果将作为输入提供给下游分类器。训练将在达到预定义的迭代次数后终止。集成方法可以从多个树中捕捉更多特征。然而，对于短文本，它的帮助有限。受到此启发，Bouaziz
    等人 ([DBLP:conf/dawak/BouazizDPPL14,](#bib.bib58) ) 结合数据丰富性和 RF 的语义信息用于短文本分类，以克服稀疏性和上下文信息不足的问题。在集成算法中，并不是所有分类器都表现良好。需要为每个分类器赋予不同的权重。为了区分森林中树的贡献，Islam
    等人 ([DBLP:conf/cikm/IslamLL0019,](#bib.bib59) ) 开发了语义感知随机森林（SARF）分类器，选择与同一类别特征相似的特征，以提取特征并生成预测值。'
- en: Summary. The parameters of NB are more diminutive, less sensitive to missing
    data, and the algorithm is simple. However, it assumes that features are independent
    of each other. When the number of features is large, or the correlation between
    features is significant, the performance of NB decreases. SVM can solve high-dimensional
    and nonlinear problems. It has a high generalization ability, but it is sensitive
    to missing data. KNN mainly depends on the surrounding finite adjacent samples,
    rather than discriminating class domain to determine the category. Thus, for the
    dataset to be divided with more crossover or overlap of the class domain, it is
    more suitable than other methods. DT is easy to understand and interpret. Given
    an observed model, it is easy to deduce the corresponding logical expression according
    to the generated decision tree. The traditional method is a type of machine learning.
    It learns from data, which are pre-defined features that are important to the
    performance of prediction values. However, feature engineering is tough work.
    Before training the classifier, we need to collect knowledge or experience to
    extract features from the original text. The traditional methods train the initial
    classifier based on various textual features extracted from the raw text. Toward
    small datasets, traditional models usually present better performance than deep
    learning models under the limitation of computational complexity. Therefore, some
    researchers have studied the design of traditional models for specific domains
    with fewer data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要。NB的参数更为微小，对缺失数据的敏感度较低，且算法简单。然而，它假设特征彼此独立。当特征数量较大或特征之间的相关性显著时，NB的性能会下降。SVM可以解决高维和非线性问题。它具有较高的泛化能力，但对缺失数据较为敏感。KNN主要依赖于周围有限的邻近样本，而不是区分类别领域来确定类别。因此，对于那些类别领域有更多交叉或重叠的数据集，它比其他方法更为合适。DT易于理解和解释。给定一个观察模型，根据生成的决策树，很容易推导出相应的逻辑表达式。传统方法是一种机器学习类型。它从数据中学习，这些数据是预定义的对预测值性能重要的特征。然而，特征工程是一项艰巨的工作。在训练分类器之前，我们需要收集知识或经验从原始文本中提取特征。传统方法基于从原始文本中提取的各种文本特征来训练初始分类器。对于小型数据集，传统模型通常在计算复杂度的限制下表现优于深度学习模型。因此，一些研究人员研究了针对特定领域的传统模型设计，这些领域的数据较少。
- en: 2.2\. Deep Learning Models
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 深度学习模型
- en: 'The DNNs consist of artificial neural networks that simulate the human brain
    to automatically learn high-level features from data, getting better results than
    traditional models in speech recognition, image processing, and text understanding.
    Input datasets should be analyzed to classify the data, such as a single-label,
    multi-label, unsupervised, unbalanced dataset. According to the trait of the dataset,
    the input word vectors are sent into the DNN for training until the termination
    condition is reached. The performance of the training model is verified by the
    downstream task, such as sentiment classification, question answering, and event
    prediction. We show some DNNs over the years in Table [1](#S2.T1 "Table 1 ‣ 2.2\.
    Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey on Text Classification:
    From Traditional to Deep Learning"), including designs that are different from
    the corresponding basic models, evaluation metrics, and experimental datasets.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: DNNs由模拟人脑的人工神经网络组成，以自动从数据中学习高级特征，比传统模型在语音识别、图像处理和文本理解中获得更好的结果。输入数据集应进行分析，以对数据进行分类，如单标签、多标签、无监督、不平衡数据集。根据数据集的特性，将输入的词向量送入DNN进行训练，直到达到终止条件。通过下游任务验证训练模型的性能，如情感分类、问答和事件预测。我们在表[1](#S2.T1
    "表1 ‣ 2.2\. 深度学习模型 ‣ 2\. 文本分类方法 ‣ 文本分类综述：从传统到深度学习")中展示了一些多年来的DNN，包括与相应基本模型不同的设计、评估指标和实验数据集。
- en: 'Table 1\. Basic information based on different models. Trans: Transformer.
    Time: training time.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '表1\. 基于不同模型的基本信息。Trans: Transformer。Time: 训练时间。'
- en: '| Model | Year | Method | Venue | Applications | Code Link | Metrics | Datasets
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 年份 | 方法 | 会议 | 应用 | 代码链接 | 评估指标 | 数据集 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | 2011 | RAE ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60) ) | EMNLP | SA,
    QA | ([Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions,](#bib.bib61)
    ) | Accuracy | MPQA, MR, EP |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 2011 | RAE ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60) ) | EMNLP | SA,
    QA | ([Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions,](#bib.bib61)
    ) | 准确率 | MPQA, MR, EP |'
- en: '| ReNN | 2012 | MV-RNN ([DBLP:conf/emnlp/SocherHMN12,](#bib.bib62) ) | EMNLP
    | SA | ([MV_RNN,](#bib.bib63) ) | Accuracy, F1 | MR |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ReNN | 2012 | MV-RNN ([DBLP:conf/emnlp/SocherHMN12,](#bib.bib62) ) | EMNLP
    | SA | ([MV_RNN,](#bib.bib63) ) | 准确率, F1 | MR |'
- en: '|  | 2013 | RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) | EMNLP |
    SA | ([DeepSentiment,](#bib.bib65) ) | Accuracy | SST |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 2013 | RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) | EMNLP |
    SA | ([DeepSentiment,](#bib.bib65) ) | 准确率 | SST |'
- en: '|  | 2014 | DeepRNN ([DBLP:conf/nips/IrsoyC14,](#bib.bib66) ) | NIPS | SA;QA
    | - | Accuracy | SST-1;SST-2 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | DeepRNN ([DBLP:conf/nips/IrsoyC14,](#bib.bib66) ) | NIPS | SA;QA
    | - | 准确率 | SST-1;SST-2 |'
- en: '| MLP | 2014 | Paragraph-Vec ([DBLP:conf/icml/LeM14,](#bib.bib67) ) | ICML
    | SA, QA | ([paragraph-vectors,](#bib.bib68) ) | Error Rate | SST, IMDB |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| MLP | 2014 | Paragraph-Vec ([DBLP:conf/icml/LeM14,](#bib.bib67) ) | ICML
    | SA, QA | ([paragraph-vectors,](#bib.bib68) ) | 错误率 | SST, IMDB |'
- en: '|  | 2015 | DAN ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69) ) | ACL | SA, QA |
    ([dan,](#bib.bib70) ) | Accuracy, Time | RT, SST, IMDB |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | DAN ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69) ) | ACL | SA, QA |
    ([dan,](#bib.bib70) ) | 准确率, 时间 | RT, SST, IMDB |'
- en: '|  | 2015 | Tree-LSTM ([DBLP:conf/acl/TaiSM15,](#bib.bib1) ) | ACL | SA | ([TreeLSTMSentiment,](#bib.bib71)
    ) | Accuracy | SST-1, SST-2 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | Tree-LSTM ([DBLP:conf/acl/TaiSM15,](#bib.bib1) ) | ACL | SA | ([TreeLSTMSentiment,](#bib.bib71)
    ) | 准确率 | SST-1, SST-2 |'
- en: '|  | 2015 | S-LSTM ([DBLP:conf/icml/ZhuSG15,](#bib.bib2) ) | ICML | SA | -
    | Accuracy | SST |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | S-LSTM ([DBLP:conf/icml/ZhuSG15,](#bib.bib2) ) | ICML | SA | -
    | 准确率 | SST |'
- en: '|  | 2015 | TextRCNN ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72) ) | AAAI | SA,
    TL | ([rcnn-text-classification,](#bib.bib73) ) | Macro-F1, etc. | 20NG, Fudan,
    ACL, SST-2 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | TextRCNN ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72) ) | AAAI | SA,
    TL | ([rcnn-text-classification,](#bib.bib73) ) | Macro-F1, 等 | 20NG, Fudan, ACL,
    SST-2 |'
- en: '|  | 2015 | MT-LSTM ([DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) | EMNLP | SA,QA
    | ([MT-LSTM,](#bib.bib74) ) | Accuracy | SST-1, SST-2, QC, IMDB |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | MT-LSTM ([DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) | EMNLP | SA,QA
    | ([MT-LSTM,](#bib.bib74) ) | 准确率 | SST-1, SST-2, QC, IMDB |'
- en: '|  | 2016 | oh-2LSTMp ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ) | ICML |
    SA, TL | ([oh-2LSTMp,](#bib.bib76) ) | Error Rate | IMDB, Elec, RCV1, 20NG |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | oh-2LSTMp ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ) | ICML |
    SA, TL | ([oh-2LSTMp,](#bib.bib76) ) | 错误率 | IMDB, Elec, RCV1, 20NG |'
- en: '| RNN | 2016 | BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) |
    COLING | SA, QA, TL | ([NNForTextClassification,](#bib.bib78) ) | Accuracy | SST-1,
    Subj, TREC, etc. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 2016 | BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) |
    COLING | SA, QA, TL | ([NNForTextClassification,](#bib.bib78) ) | 准确率 | SST-1,
    Subj, TREC, 等 |'
- en: '|  | 2016 | Multi-Task ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) | IJCAI |
    SA | ([text_classification,](#bib.bib80) ) | Accuracy | SST-1, SST-2, Subj, IMDB
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | 多任务 ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) | IJCAI | SA | ([text_classification,](#bib.bib80)
    ) | 准确率 | SST-1, SST-2, Subj, IMDB |'
- en: '|  | 2017 | DeepMoji ([DBLP:conf/emnlp/FelboMSRL17,](#bib.bib81) ) | EMNLP
    | SA | ([DeepMoji,](#bib.bib82) ) | Accuracy | SS-Twitter, SE1604, etc. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | DeepMoji ([DBLP:conf/emnlp/FelboMSRL17,](#bib.bib81) ) | EMNLP
    | SA | ([DeepMoji,](#bib.bib82) ) | 准确率 | SS-Twitter, SE1604, 等 |'
- en: '|  | 2017 | TopicRNN ([DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ) | ICML | SA
    | ([topic-rnn,](#bib.bib84) ) | Error Rate | IMDB |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | TopicRNN ([DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ) | ICML | SA
    | ([topic-rnn,](#bib.bib84) ) | 错误率 | IMDB |'
- en: '|  | 2017 | Miyato et al. ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) | ICLR
    | SA | ([adversarial_text,](#bib.bib86) ) | Error Rate | IMDB, DBpedia, etc. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | Miyato 等 ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) | ICLR | SA
    | ([adversarial_text,](#bib.bib86) ) | 错误率 | IMDB, DBpedia, 等 |'
- en: '|  | 2018 | RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87) ) | TheWebConf
    | SA | ([Sentiment-Analysis-by-Capsules,](#bib.bib88) ) | Accuracy | MR, SST-1,
    etc. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87) ) | TheWebConf
    | SA | ([Sentiment-Analysis-by-Capsules,](#bib.bib88) ) | 准确率 | MR, SST-1, 等 |'
- en: '|  | 2019 | HM-DenseRNNs ([DBLP:conf/ijcai/ZhaoSY19,](#bib.bib89) ) | IJCAI
    | SA, TL | ([HM-DenseRNNs,](#bib.bib90) ) | Accuracy | IMDB, SST-5, AG |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | HM-DenseRNNs ([DBLP:conf/ijcai/ZhaoSY19,](#bib.bib89) ) | IJCAI
    | SA, TL | ([HM-DenseRNNs,](#bib.bib90) ) | 准确率 | IMDB, SST-5, AG |'
- en: '|  | 2014 | TextCNN ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ) | EMNLP | SA, QA
    | ([CNN-for-Sentence-Classification-in-Keras,](#bib.bib91) ) | Accuracy | MR,
    SST-2, Subj, etc. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | TextCNN ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ) | EMNLP | SA, QA
    | ([CNN-for-Sentence-Classification-in-Keras,](#bib.bib91) ) | 准确率 | MR, SST-2,
    Subj, 等 |'
- en: '|  | 2014 | DCNN ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ) | ACL | SA,
    QA | ([ATS_Project,](#bib.bib92) ) | Accuracy | MR, TREC, Twitter |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 2014 | DCNN ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ) | ACL | SA,
    QA | ([ATS_Project,](#bib.bib92) ) | 准确率 | MR, TREC, Twitter |'
- en: '|  | 2015 | CharCNN ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | NeurIPS |
    SA, QA, TL | ([CharCNN,](#bib.bib94) ) | Error Rate | AG, Yelp P, DBPedia, etc.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015 | CharCNN ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | NeurIPS |
    SA, QA, TL | ([CharCNN,](#bib.bib94) ) | 错误率 | AG, Yelp P, DBPedia 等 |'
- en: '|  | 2016 | SeqTextRCNN ([DBLP:conf/naacl/LeeD16,](#bib.bib7) ) | NAACL | Dialog
    act | ([short-text-classification,](#bib.bib95) ) | Accuracy | DSTC 4, MRDA, SwDA
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | SeqTextRCNN ([DBLP:conf/naacl/LeeD16,](#bib.bib7) ) | NAACL | 对话行为
    | ([short-text-classification,](#bib.bib95) ) | 准确率 | DSTC 4, MRDA, SwDA |'
- en: '|  | 2017 | XML-CNN ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96) ) | SIGIR | NC,
    TL, SA | ([XML-CNN,](#bib.bib97) ) | NDCG@K, etc. | EUR-Lex, Wiki-30K, etc. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | XML-CNN ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96) ) | SIGIR | NC,
    TL, SA | ([XML-CNN,](#bib.bib97) ) | NDCG@K 等 | EUR-Lex, Wiki-30K 等 |'
- en: '| CNN | 2017 | DPCNN ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | ACL | SA,
    TL | ([DPCNN,](#bib.bib99) ) | Error Rate | AG, DBPedia, Yelp.P, etc. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 2017 | DPCNN ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | ACL | SA,
    TL | ([DPCNN,](#bib.bib99) ) | 错误率 | AG, DBPedia, Yelp.P 等 |'
- en: '|  | 2017 | KPCNN ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | IJCAI | SA,
    QA, TL | - | Accuracy | Twitter, AG, Bing, etc. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | KPCNN ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | IJCAI | SA,
    QA, TL | - | 准确率 | Twitter, AG, Bing 等 |'
- en: '|  | 2018 | TextCapsule ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ) | EMNLP
    | SA, QA, TL | ([capsule_text_classification,](#bib.bib102) ) | Accuracy | Subj,
    TREC, Reuters, etc. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | TextCapsule ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ) | EMNLP
    | SA, QA, TL | ([capsule_text_classification,](#bib.bib102) ) | 准确率 | Subj, TREC,
    Reuters 等 |'
- en: '|  | 2018 | HFT-CNN ([DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103) ) | EMNLP
    | TL | ([HFT-CNN,](#bib.bib104) ) | Micro-F1, etc. | RCV1, Amazon670K |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | HFT-CNN ([DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103) ) | EMNLP
    | TL | ([HFT-CNN,](#bib.bib104) ) | Micro-F1 等 | RCV1, Amazon670K |'
- en: '|  | 2019 | CCRCNN ([DBLP:conf/aaai/XuC19,](#bib.bib105) ) | AAAI | TL | -
    | Accuracy | TREC, MR, AG |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | CCRCNN ([DBLP:conf/aaai/XuC19,](#bib.bib105) ) | AAAI | TL | -
    | 准确率 | TREC, MR, AG |'
- en: '|  | 2020 | Bao et al. ([DBLP:conf/iclr/BaoWCB20,](#bib.bib106) ) | ICLR |
    TL | ([Distributional-Signatures,](#bib.bib107) ) | Accuracy | 20NG, Reuters-2157,
    etc. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Bao et al. ([DBLP:conf/iclr/BaoWCB20,](#bib.bib106) ) | ICLR |
    TL | ([Distributional-Signatures,](#bib.bib107) ) | 准确率 | 20NG, Reuters-2157 等
    |'
- en: '|  | 2016 | HAN ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | NAACL | SA,
    TL | ([textClassifier,](#bib.bib109) ) | Accuracy | Yelp.F, YahooA, etc. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | HAN ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | NAACL | SA,
    TL | ([textClassifier,](#bib.bib109) ) | 准确率 | Yelp.F, YahooA 等 |'
- en: '|  | 2016 | BI-Attention ([DBLP:conf/emnlp/ZhouWX16,](#bib.bib110) ) | NAACL
    | SA | - | Accuracy | NLP&CC 2013 ([tcci.ccf.org.cn,](#bib.bib111) ) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | BI-Attention ([DBLP:conf/emnlp/ZhouWX16,](#bib.bib110) ) | NAACL
    | SA | - | 准确率 | NLP&CC 2013 ([tcci.ccf.org.cn,](#bib.bib111) ) |'
- en: '|  | 2016 | LSTMN ([DBLP:conf/emnlp/0001DL16,](#bib.bib112) ) | EMNLP | SA
    | ([Abstractive-Summarization,](#bib.bib113) ) | Accuracy | SST-1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 2016 | LSTMN ([DBLP:conf/emnlp/0001DL16,](#bib.bib112) ) | EMNLP | SA
    | ([Abstractive-Summarization,](#bib.bib113) ) | 准确率 | SST-1 |'
- en: '|  | 2017 | Lin et al. ([DBLP:conf/iclr/LinFSYXZB17,](#bib.bib114) ) | ICLR
    | SA | ([Structured-Self-Attention,](#bib.bib115) ) | Accuracy | Yelp, SNLI Age
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | Lin et al. ([DBLP:conf/iclr/LinFSYXZB17,](#bib.bib114) ) | ICLR
    | SA | ([Structured-Self-Attention,](#bib.bib115) ) | 准确率 | Yelp, SNLI Age |'
- en: '|  | 2018 | SGM ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116) ) | COLING |
    TL | ([SGM,](#bib.bib117) ) | HL, Micro-F1 | RCV1-V2, AAPD |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | SGM ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116) ) | COLING |
    TL | ([SGM,](#bib.bib117) ) | HL, Micro-F1 | RCV1-V2, AAPD |'
- en: '|  | 2018 | ELMo ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118) ) | NAACL
    | SA, QA, NLI | ([flair,](#bib.bib119) ) | Accuracy | SQuAD, SNLI, SST-5 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | ELMo ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118) ) | NAACL
    | SA, QA, NLI | ([flair,](#bib.bib119) ) | 准确率 | SQuAD, SNLI, SST-5 |'
- en: '| Attention | 2018 | BiBloSA ([DBLP:conf/iclr/ShenZL0Z18,](#bib.bib120) ) |
    ICLR | SA | ([BiBloSA,](#bib.bib121) ) | Accuracy, Time | CR, MPQA, SUBJ, etc.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Attention | 2018 | BiBloSA ([DBLP:conf/iclr/ShenZL0Z18,](#bib.bib120) ) |
    ICLR | SA | ([BiBloSA,](#bib.bib121) ) | 准确率, 时间 | CR, MPQA, SUBJ 等 |'
- en: '|  | 2019 | AttentionXML ([DBLP:conf/nips/YouZWDMZ19,](#bib.bib122) ) | NeurIPS
    | TL | ([AttentionXML,](#bib.bib123) ) | P@k, N@k, etc. | EUR-Lex, etc. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | AttentionXML ([DBLP:conf/nips/YouZWDMZ19,](#bib.bib122) ) | NeurIPS
    | TL | ([AttentionXML,](#bib.bib123) ) | P@k, N@k 等 | EUR-Lex 等 |'
- en: '|  | 2019 | HAPN ([DBLP:conf/emnlp/SunSZL19,](#bib.bib124) ) | EMNLP | RC |
    - | Accuracy | FewRel, CSID |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | HAPN ([DBLP:conf/emnlp/SunSZL19,](#bib.bib124) ) | EMNLP | RC |
    - | 准确率 | FewRel, CSID |'
- en: '|  | 2019 | Proto-HATT ([DBLP:conf/aaai/GaoH0S19,](#bib.bib125) ) | AAAI |
    RC | ([HATT-Proto,](#bib.bib126) ) | Accuracy | FewRel |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Proto-HATT ([DBLP:conf/aaai/GaoH0S19,](#bib.bib125) ) | AAAI |
    RC | ([HATT-Proto,](#bib.bib126) ) | 准确率 | FewRel |'
- en: '|  | 2019 | STCKA ([DBLP:conf/aaai/ChenHLXJ19,](#bib.bib127) ) | AAAI | SA,
    TL | ([STCKA,](#bib.bib128) ) | Accuracy | Weibo, Product Review, etc. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | STCKA ([DBLP:conf/aaai/ChenHLXJ19,](#bib.bib127) ) | AAAI | SA,
    TL | ([STCKA,](#bib.bib128) ) | 准确率 | Weibo, 产品评论 等 |'
- en: '|  | 2020 | HyperGAT ([DBLP:conf/emnlp/DingWLLL20,](#bib.bib129) ) | EMNLP
    | TL, NC | ([HyperGAT,](#bib.bib130) ) | Accuracy | 20NG, Ohsumed, MR, etc. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | HyperGAT ([DBLP:conf/emnlp/DingWLLL20,](#bib.bib129) ) | EMNLP
    | TL, NC | ([HyperGAT,](#bib.bib130) ) | 准确率 | 20NG, Ohsumed, MR 等 |'
- en: '|  | 2020 | MSMSA ([DBLP:conf/aaai/GuoQLXZ20,](#bib.bib131) ) | AAAI | ST,
    QA, NLI | - | Accuracy, F1 | IMDB, MR, SST, SNLI, etc. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | MSMSA ([DBLP:conf/aaai/GuoQLXZ20,](#bib.bib131) ) | AAAI | ST,
    QA, NLI | - | 准确率, F1 | IMDB, MR, SST, SNLI 等 |'
- en: '|  | 2020 | Choi ([DBLP:conf/emnlp/ChoiPYH20,](#bib.bib132) ) | EMNLP | SA,
    TL | - | Accuracy | SST2, IMDB, 20NG |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | Choi ([DBLP:conf/emnlp/ChoiPYH20,](#bib.bib132) ) | EMNLP | SA,
    TL | - | 准确率 | SST2, IMDB, 20NG |'
- en: '|  | 2019 | BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | NAACL | SA,
    QA | ([bert,](#bib.bib133) ) | Accuracy | SST-2, QQP, QNLI, CoLA |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | NAACL | SA,
    QA | ([bert,](#bib.bib133) ) | 准确率 | SST-2, QQP, QNLI, CoLA |'
- en: '|  | 2019 | BERT-BASE ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ACL
    | TL | ([lmtc-eurlex57k,](#bib.bib135) ) | P@K, R@K, etc. | EUR-LEX |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | BERT-BASE ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ACL
    | TL | ([lmtc-eurlex57k,](#bib.bib135) ) | P@K, R@K 等 | EUR-LEX |'
- en: '|  | 2019 | Sun et al. ([DBLP:conf/cncl/SunQXH19,](#bib.bib136) ) | CCL | SA,
    QA, TL | ([How_Fin,](#bib.bib137) ) | Error Rate | TREC, DBPedia, etc. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Sun et al. ([DBLP:conf/cncl/SunQXH19,](#bib.bib136) ) | CCL | SA,
    QA, TL | ([How_Fin,](#bib.bib137) ) | 错误率 | TREC, DBPedia 等 |'
- en: '|  | 2019 | XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | NeurIPS |
    SA, QA, NC | ([xlnet,](#bib.bib139) ) | EM, F1, etc. | Yelp-2, AG, MNLI, etc.
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | NeurIPS |
    SA, QA, NC | ([xlnet,](#bib.bib139) ) | EM, F1 等 | Yelp-2, AG, MNLI 等 |'
- en: '|  | 2019 | RoBERTa ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140) ) |
    arXiv | SA, QA | ([RoBERTa,](#bib.bib141) ) | F1, Accuracy | SQuAD, MNLI-m, SST-2
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | RoBERTa ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140) ) |
    arXiv | SA, QA | ([RoBERTa,](#bib.bib141) ) | F1, 准确率 | SQuAD, MNLI-m, SST-2 |'
- en: '| Trans | 2020 | GAN-BERT ([DBLP:conf/acl/CroceCB20,](#bib.bib142) ) | ACL
    | SA, NLI | ([GAN-BERT,](#bib.bib143) ) | F1, Accuracy | SST-5, MNLI |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Trans | 2020 | GAN-BERT ([DBLP:conf/acl/CroceCB20,](#bib.bib142) ) | ACL
    | SA, NLI | ([GAN-BERT,](#bib.bib143) ) | F1, 准确率 | SST-5, MNLI |'
- en: '|  | 2020 | BAE ([DBLP:conf/emnlp/GargR20,](#bib.bib144) ) | EMNLP | SA, QA
    | ([BAE,](#bib.bib145) ) | Accuracy | Amazon, Yelp, MR, MPQA |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | BAE ([DBLP:conf/emnlp/GargR20,](#bib.bib144) ) | EMNLP | SA, QA
    | ([BAE,](#bib.bib145) ) | 准确率 | Amazon, Yelp, MR, MPQA |'
- en: '|  | 2020 | ALBERT ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146) ) | ICLR | SA,
    QA | ([ALBERT,](#bib.bib147) ) | F1, Accuracy | SST, MNLI, SQuAD |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | ALBERT ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146) ) | ICLR | SA,
    QA | ([ALBERT,](#bib.bib147) ) | F1, 准确率 | SST, MNLI, SQuAD |'
- en: '|  | 2020 | TG-Transformer ([DBLP:conf/emnlp/ZhangZ20,](#bib.bib148) ) | EMNLP
    | SA, TL | - | Accuracy, Time | R8, R52, Ohsumed, etc. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | TG-Transformer ([DBLP:conf/emnlp/ZhangZ20,](#bib.bib148) ) | EMNLP
    | SA, TL | - | 准确率, 时间 | R8, R52, Ohsumed 等 |'
- en: '|  | 2020 | X-Transformer ([DBLP:conf/kdd/ChangYZYD20,](#bib.bib149) ) | KDD
    | SA, TL | ([X-Transformer,](#bib.bib150) ) | P@K, R@K | Eurlex-4K, Wiki10-31K,
    etc. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | X-Transformer ([DBLP:conf/kdd/ChangYZYD20,](#bib.bib149) ) | KDD
    | SA, TL | ([X-Transformer,](#bib.bib150) ) | P@K, R@K | Eurlex-4K, Wiki10-31K
    等 |'
- en: '|  | 2021 | LightXML ([DBLP:journals/corr/abs-2101-03305,](#bib.bib151) ) |
    arXiv | TL, ML, NLI | ([LightXML,](#bib.bib152) ) | P@K, Time | AmazonCat-13K,
    etc. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | 2021 | LightXML ([DBLP:journals/corr/abs-2101-03305,](#bib.bib151) ) |
    arXiv | TL, ML, NLI | ([LightXML,](#bib.bib152) ) | P@K, 时间 | AmazonCat-13K 等
    |'
- en: '|  | 2018 | DGCNN ([DBLP:conf/www/PengLHLBWS018,](#bib.bib153) ) | TheWebConf
    | TL | ([DeepGraphCNNforTexts,](#bib.bib154) ) | Macro-F1, etc. | RCV1, NYTimes
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | DGCNN ([DBLP:conf/www/PengLHLBWS018,](#bib.bib153) ) | TheWebConf
    | TL | ([DeepGraphCNNforTexts,](#bib.bib154) ) | 宏观-F1 等 | RCV1, NYTimes |'
- en: '|  | 2019 | TextGCN ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ) | AAAI | SA,
    TL | ([text_gcn,](#bib.bib156) ) | Accuracy | 20NG, Ohsumed, R52, etc. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | TextGCN ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ) | AAAI | SA,
    TL | ([text_gcn,](#bib.bib156) ) | 准确率 | 20NG, Ohsumed, R52 等 |'
- en: '|  | 2019 | SGC([DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ) | ICML | NC, TL,
    SA | ([SGC,](#bib.bib158) ) | Accuracy, Time | 20NG, R8, Ohsumed, etc. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | SGC([DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ) | ICML | NC, TL,
    SA | ([SGC,](#bib.bib158) ) | 准确率, 时间 | 20NG, R8, Ohsumed 等 |'
- en: '| GNN | 2019 | Huang et al. ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159) )
    | EMNLP | NC, TL | ([TextLevelGNN,](#bib.bib160) ) | Accuracy | R8, R52, Ohsumed
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GNN | 2019 | Huang et al. ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159) )
    | EMNLP | NC, TL | ([TextLevelGNN,](#bib.bib160) ) | 准确率 | R8, R52, Ohsumed |'
- en: '|  | 2019 | Peng et al. ([peng2019hierarchical,](#bib.bib161) ) | arXiv | NC,
    TL | - | Micro-F1, etc. | RCV1, EUR-Lex, etc. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Peng et al. ([peng2019hierarchical,](#bib.bib161) ) | arXiv | NC,
    TL | - | 微观-F1 等 | RCV1, EUR-Lex 等 |'
- en: '|  | 2020 | TextING ([DBLP:conf/acl/ZhangYCWWW20,](#bib.bib162) ) | ACL | SA,
    NC, TL | ([TextING,](#bib.bib163) ) | Accuracy | MR, R8, R52, Ohsumed |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | TextING ([DBLP:conf/acl/ZhangYCWWW20,](#bib.bib162) ) | ACL | SA,
    NC, TL | ([TextING,](#bib.bib163) ) | 准确率 | MR, R8, R52, Ohsumed |'
- en: '|  | 2020 | TensorGCN ([DBLP:conf/aaai/LiuYZWL20,](#bib.bib164) ) | AAAI |
    SA, NC, TL | ([TensorGCN,](#bib.bib165) ) | Accuracy | 20NG, R8, R52, Ohsumed,
    MR |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | TensorGCN ([DBLP:conf/aaai/LiuYZWL20,](#bib.bib164) ) | AAAI |
    SA, NC, TL | ([TensorGCN,](#bib.bib165) ) | 准确率 | 20NG, R8, R52, Ohsumed, MR |'
- en: '|  | 2020 | MAGNET ([DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ICAART | TL
    | ([MAGnet,](#bib.bib167) ) | Micro-F1, HL | Reuters, RCV1-V2, etc. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 2020 | MAGNET ([DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ICAART | TL
    | ([MAGnet,](#bib.bib167) ) | 微平均-F1, HL | Reuters, RCV1-V2, 等 |'
- en: '|  | 2017 | Miyato et al. ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) | ICLR
    | SA, NC | ([Miyato,](#bib.bib168) ) | Error Rate | IMDB, RCV1, et al. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 2017 | Miyato et al. ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) | ICLR
    | SA, NC | ([Miyato,](#bib.bib168) ) | 错误率 | IMDB, RCV1, 等 |'
- en: '| Others | 2018 | TMN ([DBLP:conf/emnlp/ZengLSGLK18,](#bib.bib169) ) | EMNLP
    | TL | - | Accuracy, F1 | Snippets, Twitter, et al. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 2018 | TMN ([DBLP:conf/emnlp/ZengLSGLK18,](#bib.bib169) ) | EMNLP |
    TL | - | 准确率, F1 | Snippets, Twitter, 等 |'
- en: '|  | 2019 | Zhang et al. ([DBLP:conf/naacl/ZhangLG19,](#bib.bib170) ) | NAACL
    | TL, NC | ([KG4ZeroShotText,](#bib.bib171) ) | Accuracy | DBpedia, 20NG. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | Zhang et al. ([DBLP:conf/naacl/ZhangLG19,](#bib.bib170) ) | NAACL
    | TL, NC | ([KG4ZeroShotText,](#bib.bib171) ) | 准确率 | DBpedia, 20NG. |'
- en: 'Numerous deep learning models have been proposed in the past few decades for
    text classification, as shown in Table [1](#S2.T1 "Table 1 ‣ 2.2\. Deep Learning
    Models ‣ 2\. Text Classification Methods ‣ A Survey on Text Classification: From
    Traditional to Deep Learning"). We tabulate primary information – including publication
    years, venues, applications, code links, evaluation metrics, and experiment datasets
    – of main deep learning models for text classification. The applications in this
    table include Sentiment Analysis (SA), Topic Labeling (TL), News Classification
    (NC), Question Answering (QA), Dialog Act Classification (DAC), Natural Language
    Inference (NLI) and Relation Classification (RC). The multilayer perceptron ([4809024,](#bib.bib172)
    ) and the recursive neural network ([DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173)
    ) are the first two deep learning approaches used for the text classification
    task, which improve performance compared with traditional models. Then, CNNs,
    Recurrent Neural Networks (RNNs), and attention mechanisms are used for text classification
    ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ; [DBLP:conf/aaai/QinCLN020,](#bib.bib174)
    ; [DBLP:conf/naacl/DengPHLY21,](#bib.bib175) ). Many researchers advance text
    classification performance for different tasks by improving CNN, RNN, and attention,
    or model fusion and multi-task methods. The appearance of BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ), which can generate contextualized word vectors, is a significant turning point
    in the development of text classification and other NLP technologies. Many researchers
    ([DBLP:conf/aaai/JinJZS20,](#bib.bib176) ; [DBLP:conf/acl/CroceCB20,](#bib.bib142)
    ) have studied text classification models based on BERT, which achieves better
    performance than the above models in multiple NLP tasks, including text classification.
    Besides, some researchers study text classification technology based on Graph
    Neural Network (GNN) ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [lichen2021ijcai,](#bib.bib177)
    ) to capture structural information in the text, which cannot be replaced by other
    methods. Here, we classify DNNs by structure and discuss some representative models
    in detail:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，已提出了许多深度学习模型用于文本分类，如表[1](#S2.T1 "表 1 ‣ 2.2\. 深度学习模型 ‣ 2\. 文本分类方法 ‣ 文本分类的调查：从传统到深度学习")所示。我们列出了主要的信息——包括出版年份、会议、应用、代码链接、评估指标和实验数据集——主要深度学习模型用于文本分类。该表中的应用包括情感分析（SA）、主题标注（TL）、新闻分类（NC）、问题回答（QA）、对话行为分类（DAC）、自然语言推理（NLI）和关系分类（RC）。多层感知机
    ([4809024,](#bib.bib172) ) 和递归神经网络 ([DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173)
    ) 是用于文本分类任务的前两种深度学习方法，相比传统模型提高了性能。随后，CNN、递归神经网络（RNN）和注意力机制被用于文本分类 ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/aaai/QinCLN020,](#bib.bib174) ; [DBLP:conf/naacl/DengPHLY21,](#bib.bib175)
    )。许多研究者通过改进CNN、RNN和注意力机制，或通过模型融合和多任务方法，推动了不同任务的文本分类性能。BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ) 的出现，它能够生成上下文相关的词向量，是文本分类和其他NLP技术发展的一个重要转折点。许多研究者 ([DBLP:conf/aaai/JinJZS20,](#bib.bib176)
    ; [DBLP:conf/acl/CroceCB20,](#bib.bib142) ) 研究了基于BERT的文本分类模型，这些模型在多个NLP任务中，包括文本分类，表现优于上述模型。此外，一些研究者研究了基于图神经网络（GNN）
    ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [lichen2021ijcai,](#bib.bib177) ) 的文本分类技术，以捕获文本中的结构信息，这是其他方法无法替代的。在这里，我们按结构分类深度神经网络（DNN），并详细讨论一些代表性模型：
- en: '![Refer to caption](img/368c33a26588d7e229edbe35c3002ec7.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/368c33a26588d7e229edbe35c3002ec7.png)'
- en: Figure 6\. The architecture of ReNN (left) and the architecture of MLP (right).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. ReNN的架构（左）和MLP的架构（右）。
- en: 2.2.1\. ReNN-based Methods
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 基于ReNN的方法
- en: 'Traditional models cost lots of time on design features for each task. Furthermore,
    in the case of deep learning, the meaning of ”word vectors” is different: each
    input word is associated with a fixed-length vector whose values are either drawn
    at random or derived from a previous traditional process, thus forming a matrix
    $L$ called word embedding matrix which represents the vocabulary words in a small
    latent semantic space, of generally 50 to 300 dimensions. The Recursive Neural
    Network (ReNN) ([DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173) ) can automatically
    learn the semantics of text recursively and the syntax tree structure without
    feature design, as shown in Fig. [6](#S2.F6 "Figure 6 ‣ 2.2\. Deep Learning Models
    ‣ 2\. Text Classification Methods ‣ A Survey on Text Classification: From Traditional
    to Deep Learning"). We give an example of ReNN based models. First, each word
    of input text is taken as the leaf node of the model structure. Then all nodes
    are combined into parent nodes using a weight matrix. The weight matrix is shared
    across the whole model. Each parent node has the same dimension with all leaf
    nodes. Finally, all nodes are recursively aggregated into a parent node to represent
    the input text to predict the label.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '传统模型在每个任务上都花费大量时间来设计特征。此外，在深度学习的情况下，“词向量”的含义有所不同：每个输入词与一个固定长度的向量相关联，这些向量的值要么是随机生成的，要么是从先前的传统过程中得出的，从而形成一个矩阵$L$，称为词嵌入矩阵，该矩阵在一个通常为50到300维的小型潜在语义空间中表示词汇。递归神经网络（ReNN）（[DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173)）可以自动学习文本的语义和语法树结构，无需特征设计，如图[6](#S2.F6
    "Figure 6 ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning")所示。我们举一个基于ReNN的模型的例子。首先，将输入文本的每个词作为模型结构的叶子节点。然后，所有节点通过权重矩阵组合成父节点。权重矩阵在整个模型中共享。每个父节点与所有叶子节点具有相同的维度。最后，所有节点递归地聚合成一个父节点，以表示输入文本以预测标签。'
- en: ReNN-based models improve performance compared with traditional models and save
    on labor costs due to excluding feature designs used for different text classification
    tasks. The Recursive AutoEncoder (RAE) ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60)
    ) is used to predict the distribution of sentiment labels for each input sentence
    and learn the representations of multi-word phrases. To learn compositional vector
    representations for each input text, the Matrix-Vector Recursive Neural Network
    (MV-RNN) ([DBLP:conf/emnlp/SocherHMN12,](#bib.bib62) ) introduces a ReNN model
    to learn the representation of phrases and sentences. It allows that the length
    and type of input texts are inconsistent. MV-RNN allocates a matrix and a vector
    for each node on the constructed parse tree. Furthermore, the Recursive Neural
    Tensor Network (RNTN) ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) is proposed
    with a tree structure to capture the semantics of sentences. It inputs phrases
    with different length and represents the phrases by parse trees and word vectors.
    The vectors of higher nodes on the parse tree are estimated by the equal tensor-based
    composition function. For RNTN, the time complexity of building the textual tree
    is high, and expressing the relationship between documents is complicated within
    a tree structure. The performance is usually improved, with the depth being increased
    for DNNs. Therefore, Irsoy et al. ([DBLP:conf/nips/IrsoyC14,](#bib.bib66) ) propose
    a Deep Recursive Neural Network (DeepReNN), which stacks multiple recursive layers.
    It is built by binary parse trees and learns distinct perspectives of compositionality
    in language.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基于ReNN的模型相比传统模型提高了性能，并且通过排除用于不同文本分类任务的特征设计，节省了劳动成本。递归自编码器（RAE）（[DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60)）用于预测每个输入句子的情感标签分布，并学习多词短语的表示。为了学习每个输入文本的组合向量表示，矩阵-向量递归神经网络（MV-RNN）（[DBLP:conf/emnlp/SocherHMN12,](#bib.bib62)）引入了一种ReNN模型来学习短语和句子的表示。它允许输入文本的长度和类型不一致。MV-RNN为构造的解析树上的每个节点分配一个矩阵和一个向量。此外，递归神经张量网络（RNTN）（[DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)）提出了一个树形结构来捕捉句子的语义。它输入不同长度的短语，并通过解析树和词向量表示短语。解析树上较高节点的向量通过相等的张量基础组合函数来估计。对于RNTN，构建文本树的时间复杂度较高，并且在树形结构中表达文档之间的关系比较复杂。通常，随着DNN深度的增加，性能会有所提高。因此，Irsoy等人（[DBLP:conf/nips/IrsoyC14,](#bib.bib66)）提出了一种深度递归神经网络（DeepReNN），它堆叠了多个递归层。该网络由二叉解析树构建，学习语言中的组合性不同视角。
- en: 2.2.2\. MLP-based Methods
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. 基于MLP的方法
- en: 'A MultiLayer Perceptron (MLP) ([4809024,](#bib.bib172) ), sometimes colloquially
    called ”vanilla” neural network, is a simple neural network structure that is
    used for capturing features automatically. As shown in Fig. [6](#S2.F6 "Figure
    6 ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey on
    Text Classification: From Traditional to Deep Learning"), we show a three-layer
    MLP model. It contains an input layer, a hidden layer with an activation function
    in all nodes, and an output layer. Each node connects with a certain weight $w_{i}$.
    It treats each input text as a bag of words and achieves high performance on many
    text classification benchmarks comparing with traditional models.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '多层感知器（MLP）（[4809024,](#bib.bib172)），有时被通俗地称为“普通”神经网络，是一种简单的神经网络结构，用于自动捕捉特征。如图[6](#S2.F6
    "Figure 6 ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning")所示，我们展示了一个三层的MLP模型。它包含一个输入层、一个具有激活函数的隐藏层以及一个输出层。每个节点通过一定的权重$w_{i}$连接。它将每个输入文本视为词袋，并在许多文本分类基准测试中与传统模型相比表现出色。'
- en: There are some MLP-based methods proposed by some research groups for text classification
    tasks. The Paragraph Vector (Paragraph-Vec) ([DBLP:conf/icml/LeM14,](#bib.bib67)
    ) is the most popular and widely used method, which is similar to the Continuous
    Bag-Of-Words (CBOW) ([DBLP:journals/corr/abs-1301-3781,](#bib.bib23) ). It gets
    fixed-length feature representations of texts with various input lengths by employing
    unsupervised algorithms. Comparing with CBOW, it adds a paragraph token mapped
    to the paragraph vector by a matrix. The model predicts the fourth word by the
    connection or average of this vector to the three contexts of the word. Paragraph
    vectors can be used as a memory for paragraph themes and are used as a paragraph
    function and inserted into the prediction classifier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究小组提出了基于 MLP 的方法用于文本分类任务。段落向量（Paragraph-Vec）（[DBLP:conf/icml/LeM14,](#bib.bib67)）是最受欢迎和广泛使用的方法，它类似于连续词袋模型（CBOW）（[DBLP:journals/corr/abs-1301-3781,](#bib.bib23)）。该方法通过采用无监督算法获取具有不同输入长度的固定长度特征表示。与
    CBOW 相比，它通过矩阵将段落标记映射到段落向量。模型通过将该向量与词的三个上下文连接或平均来预测第四个词。段落向量可以作为段落主题的记忆，并用作段落函数并插入到预测分类器中。
- en: '![Refer to caption](img/4b2d8fb916c1c344937834dd74258aa9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b2d8fb916c1c344937834dd74258aa9.png)'
- en: Figure 7\. The RNN based model (left) and the CNN based model (right).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 基于 RNN 的模型（左）和基于 CNN 的模型（右）。
- en: 2.2.3\. RNN-based Methods
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 基于 RNN 的方法
- en: 'The Recurrent Neural Network (RNN) ([DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173)
    ) is broadly used for capturing long-range dependency through recurrent computation.
    The RNN language model learns historical information, considering the location
    information among all words suitable for text classification tasks. We show an
    RNN model for text classification with a simple sample, as shown in Fig. [7](#S2.F7
    "Figure 7 ‣ 2.2.2\. MLP-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning"). Firstly, each input word is represented by a specific vector
    using a word embedding technology. Then, the embedding word vectors are fed into
    RNN cells one by one. The output of RNN cells are the same dimension with the
    input vector and are fed into the next hidden layer. The RNN shares parameters
    across different parts of the model and has the same weights of each input word.
    Finally, the label of input text can be predicted by the last output of the hidden
    layer.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）（[DBLP:journals/csur/PouyanfarSYTTRS19,](#bib.bib173)）广泛用于通过递归计算捕捉长程依赖。RNN
    语言模型学习历史信息，考虑所有词汇之间的位置关系，适合用于文本分类任务。我们展示了一个用于文本分类的 RNN 模型，配有一个简单示例，如图 [7](#S2.F7
    "图 7 ‣ 2.2.2\. 基于 MLP 的方法 ‣ 2.2\. 深度学习模型 ‣ 2\. 文本分类方法 ‣ 关于文本分类的调查：从传统到深度学习") 所示。首先，每个输入词汇通过词嵌入技术表示为一个特定的向量。然后，将嵌入的词向量逐一输入
    RNN 单元。RNN 单元的输出与输入向量具有相同的维度，并被输入到下一个隐藏层。RNN 在模型的不同部分之间共享参数，并且每个输入词具有相同的权重。最后，通过隐藏层的最后输出可以预测输入文本的标签。
- en: To diminish the time complexity of the model and capture contextual information,
    Liu et al. ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) introduce a model for catching
    the semantics of long texts. It is a biased model that parsed the text one by
    one, making the following inputs profit over the former and decreasing the semantic
    efficiency of capturing the whole text. For modeling topic labeling tasks with
    long input sequences, TopicRNN ([DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ) is
    proposed. It captures the dependencies of words in a document via latent topics
    and uses RNNs to capture local dependencies and latent topic models for capturing
    global semantic dependencies. Virtual Adversarial Training (VAT) ([DBLP:journals/corr/MiyatoMKI17,](#bib.bib178)
    ) is a useful regularization method applicable to semi-supervised learning tasks.
    Miyato et al. ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) apply adversarial and
    virtual adversarial training text and employ the perturbation into word embedding
    rather than the original input text. The model improves the quality of the word
    embedding and is not easy to overfit during training. Capsule network ([10.1007/978-3-642-21735-7_6,](#bib.bib179)
    ) captures the relationships between features using dynamic routing between capsules
    comprised of a group of neurons in a layer. Wang et al. ([DBLP:conf/www/WangSH0Z18,](#bib.bib87)
    ) propose an RNN-Capsule model with a simple capsule structure for the sentiment
    classification task.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少模型的时间复杂度并捕捉上下文信息，Liu 等人（[DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ）引入了一种用于捕捉长文本语义的模型。这是一个有偏模型，它逐个解析文本，使得后续输入在处理上优于前面的输入，从而降低了捕捉整篇文本语义的效率。为了建模具有长输入序列的主题标记任务，提出了
    TopicRNN（[DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ）。它通过潜在主题捕捉文档中单词的依赖关系，并使用 RNN
    捕捉局部依赖关系，同时利用潜在主题模型捕捉全局语义依赖关系。虚拟对抗训练（VAT）（[DBLP:journals/corr/MiyatoMKI17,](#bib.bib178)
    ）是一种适用于半监督学习任务的有用正则化方法。Miyato 等人（[DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ）将对抗性训练和虚拟对抗训练应用于文本，并将扰动引入词嵌入中，而不是原始输入文本。该模型提高了词嵌入的质量，并且在训练过程中不容易过拟合。胶囊网络（[10.1007/978-3-642-21735-7_6,](#bib.bib179)
    ）通过动态路由捕捉特征之间的关系，这些特征由层中的一组神经元组成。Wang 等人（[DBLP:conf/www/WangSH0Z18,](#bib.bib87)
    ）提出了一种具有简单胶囊结构的 RNN-Capsule 模型，用于情感分类任务。
- en: In the backpropagation process of RNN, the weights are adjusted by gradients,
    calculated by continuous multiplications of derivatives. If the derivatives are
    extremely small, it may cause a gradient vanishing problem by continuous multiplications.
    Long Short-Term Memory (LSTM) ([Hochreiter1997Long,](#bib.bib180) ), the improvement
    of RNN, effectively alleviates the gradient vanishing problem. It is composed
    of a cell to remember values on arbitrary time intervals and three gate structures
    to control information flow. The gate structures include input gates, forget gates,
    and output gates. The LSTM classification method can better capture the connection
    among context feature words, and use the forgotten gate structure to filter useless
    information, which is conducive to improving the total capturing ability of the
    classifier. Tree-LSTM ([DBLP:conf/acl/TaiSM15,](#bib.bib1) ) extends the sequence
    of LSTM models to the tree structure. The whole subtree with little influence
    on the result can be forgotten through the LSTM forgetting gate mechanism for
    the Tree-LSTM model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 的反向传播过程中，权重通过梯度调整，梯度是通过连续的导数相乘计算得到的。如果导数非常小，可能会导致梯度消失问题。长短期记忆（LSTM）（[Hochreiter1997Long,](#bib.bib180)
    ）作为 RNN 的改进，有效缓解了梯度消失问题。LSTM 由一个用于记住任意时间间隔的值的单元和三个门控结构组成，以控制信息流。这些门控结构包括输入门、遗忘门和输出门。LSTM
    分类方法可以更好地捕捉上下文特征词之间的关系，并使用遗忘门结构来过滤无用信息，从而有助于提高分类器的整体捕捉能力。Tree-LSTM（[DBLP:conf/acl/TaiSM15,](#bib.bib1)
    ）将 LSTM 模型的序列扩展到树结构。通过 LSTM 遗忘门机制，可以忘记对结果影响较小的整个子树，这样可以提高 Tree-LSTM 模型的效果。
- en: Natural Language Inference (NLI) ([DBLP:conf/emnlp/BowmanAPM15,](#bib.bib181)
    ) predicts whether one text’s meaning can be deduced from another by measuring
    the semantic similarity between each pair of sentences. To consider other granular
    matchings and matchings in the reverse direction, Wang et al. ([DBLP:conf/ijcai/WangHF17,](#bib.bib182)
    ) propose a model for the NLI task named Bilateral Multi-Perspective Matching
    (BiMPM). It encodes input sentences by the BiLSTM encoder. Then, the encoded sentences
    are matched in two directions. The results are aggregated in a fixed-length matching
    vector by another BiLSTM layer. Finally, the result is evaluated by a fully connected
    layer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推断（NLI） ([DBLP:conf/emnlp/BowmanAPM15,](#bib.bib181) ) 通过测量每对句子之间的语义相似性来预测一个文本的意义是否可以从另一个文本中推导出来。为了考虑其他细粒度的匹配以及反向匹配，Wang
    等人 ([DBLP:conf/ijcai/WangHF17,](#bib.bib182) ) 提出了一个名为双向多视角匹配（BiMPM）的NLI任务模型。它通过BiLSTM编码器对输入句子进行编码。然后，编码后的句子在两个方向上进行匹配。结果通过另一个BiLSTM层聚合成一个固定长度的匹配向量。最后，结果通过一个全连接层进行评估。
- en: 2.2.4\. CNN-based Methods
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 基于CNN的方法
- en: 'Convolutional Neural Networks (CNNs) ([albawi2017understanding,](#bib.bib18)
    ) are proposed for image classification with convolving filters that can extract
    features of pictures. Unlike RNN, CNN can simultaneously apply convolutions defined
    by different kernels to multiple chunks of a sequence. Therefore, CNNs are used
    for many NLP tasks, including text classification. For text classification, the
    text requires being represented as a vector similar to the image representation,
    and text features can be filtered from multiple angles, as shown in Fig. [7](#S2.F7
    "Figure 7 ‣ 2.2.2\. MLP-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning"). Firstly, the word vectors of the input text are spliced into
    a matrix. The matrix is then fed into the convolutional layer, which contains
    several filters with different dimensions. Finally, the result of the convolutional
    layer goes through the pooling layer and concatenates the pooling result to obtain
    the final vector representation of the text. The category is predicted by the
    final vector.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（CNNs） ([albawi2017understanding,](#bib.bib18) ) 是用于图像分类的，它通过卷积滤波器提取图片特征。与RNN不同，CNN可以同时对序列的多个片段应用由不同核定义的卷积。因此，CNN被用于许多自然语言处理任务，包括文本分类。对于文本分类，文本需要表示为类似于图像表示的向量，并且文本特征可以从多个角度进行过滤，如图[7](#S2.F7
    "Figure 7 ‣ 2.2.2\. MLP-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning")所示。首先，将输入文本的词向量拼接成一个矩阵。然后将矩阵输入到包含多个不同维度的滤波器的卷积层。最后，卷积层的结果经过池化层，并将池化结果拼接，以获得文本的最终向量表示。最终类别由这个向量预测。'
- en: To try using CNN for the text classification task, an unbiased model of convolutional
    neural networks is introduced by Kim, called TextCNN ([DBLP:conf/emnlp/Kim14,](#bib.bib17)
    ). It can better determine discriminative phrases in the max-pooling layer with
    one layer of convolution and learn hyperparameters except for word vectors by
    keeping word vectors static. Training only on labeled data is not enough for data-driven
    deep models. Therefore, some researchers consider utilizing unlabeled data. Johnson
    et al. ([DBLP:conf/nips/JohnsonZ15,](#bib.bib183) ) propose a CNN model based
    on two-view semi-supervised learning for text classification, which first uses
    unlabeled data to train the embedding of text regions and then labeled data. DNNs
    usually have better performance, but it increases the computational complexity.
    Motivated by this, a Deep Pyramid Convolutional Neural Network (DPCNN) ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    ) is proposed, with a little more computational accuracy, increasing by raising
    the network depth. The DPCNN is more specific than Residual Network (ResNet) ([DBLP:conf/eccv/HeZRS16,](#bib.bib184)
    ), as all the shortcuts are exactly simple identity mappings without any complication
    for dimension matching.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试使用 CNN 进行文本分类任务，Kim 引入了一种无偏见的卷积神经网络模型，称为 TextCNN ([DBLP:conf/emnlp/Kim14,](#bib.bib17)
    )。它可以在最大池化层中更好地确定判别性短语，通过保持词向量静态而学习超参数。仅仅在标记数据上训练对于数据驱动的深度模型来说是不够的。因此，一些研究人员考虑利用未标记的数据。Johnson
    等人 ([DBLP:conf/nips/JohnsonZ15,](#bib.bib183) ) 提出了一个基于双视图半监督学习的 CNN 模型用于文本分类，该模型首先使用未标记的数据来训练文本区域的嵌入，然后再使用标记的数据。DNN
    通常具有更好的性能，但它增加了计算复杂度。受到这一点的启发，提出了深度金字塔卷积神经网络（DPCNN） ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    )，通过增加网络深度来提高计算准确性。DPCNN 比 Residual Network（ResNet） ([DBLP:conf/eccv/HeZRS16,](#bib.bib184)
    ) 更具针对性，因为所有的快捷方式都是简单的恒等映射，没有任何维度匹配的复杂性。
- en: According to the minimum embedding unit of text, embedding methods are divided
    into character-level, word-level, and sentence-level embedding. Character-level
    embeddings can settle Out-Of-Vocabulary (OOV) ([bazzi2002modelling,](#bib.bib185)
    ) words. Word-level embeddings learn the syntax and semantics of the words. Moreover,
    sentence-level embedding can capture relationships among sentences. Motivated
    by these, Nguyen et al. ([DBLP:conf/pacling/NguyenN17,](#bib.bib186) ) propose
    a deep learning method based on a dictionary, increasing information for word-level
    embeddings through constructing semantic rules and deep CNN for character-level
    embeddings. Adams et al. ([DBLP:journals/tgis/AdamsM18,](#bib.bib187) ) propose
    a character-level CNN model, called MGTC, to classify multi-lingual texts written.
    TransCap ([DBLP:conf/acl/ChenQ19,](#bib.bib188) ) is proposed to encapsulate the
    sentence-level semantic representations into semantic capsules and transfer document-level
    knowledge.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文本的最小嵌入单元，嵌入方法被划分为字符级、词级和句子级嵌入。字符级嵌入可以解决词汇表外（OOV）([bazzi2002modelling,](#bib.bib185)
    ) 单词的问题。词级嵌入学习单词的语法和语义。此外，句子级嵌入可以捕捉句子之间的关系。受到这些启发，Nguyen 等人 ([DBLP:conf/pacling/NguyenN17,](#bib.bib186)
    ) 提出了基于词典的深度学习方法，通过构建语义规则和用于字符级嵌入的深度 CNN 增加词级嵌入的信息。Adams 等人 ([DBLP:journals/tgis/AdamsM18,](#bib.bib187)
    ) 提出了一个字符级 CNN 模型，称为 MGTC，用于分类多语言文本。TransCap ([DBLP:conf/acl/ChenQ19,](#bib.bib188)
    ) 被提出用于将句子级语义表示封装到语义胶囊中，并转移文档级知识。
- en: RNN based models capture the sequential information to learn the dependency
    among input words, and CNN based models extract the relevant features from the
    convolution kernels. Thus some works study the fusion of the two methods. BLSTM-2DCNN
    ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) integrates a Bidirectional LSTM
    (BiLSTM) with two-dimensional max pooling. It uses a 2D convolution to sample
    more meaningful information of the matrix and understands the context better through
    BiLSTM. Moreover, Xue et al. ([DBLP:conf/ijcnlp/XueZLW17,](#bib.bib189) ) propose
    MTNA, a combination of BiLSTM and CNN layers, to solve aspect category classification
    and aspect term extraction tasks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RNN 的模型捕捉序列信息以学习输入单词之间的依赖关系，而基于 CNN 的模型从卷积核中提取相关特征。因此，一些研究工作研究了这两种方法的融合。BLSTM-2DCNN
    ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) 将双向 LSTM（BiLSTM）与二维最大池化结合起来。它使用
    2D 卷积来采样矩阵中更有意义的信息，并通过 BiLSTM 更好地理解上下文。此外，Xue 等人 ([DBLP:conf/ijcnlp/XueZLW17,](#bib.bib189)
    ) 提出了 MTNA，这是 BiLSTM 和 CNN 层的组合，用于解决方面类别分类和方面术语提取任务。
- en: '![Refer to caption](img/b6e0865b09bb8297506dd67c45624452.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b6e0865b09bb8297506dd67c45624452.png)'
- en: Figure 8\. The architecture of hierarchical attention network (HAN) ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108)
    ).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 层次注意力网络（HAN）的结构（[DBLP:conf/naacl/YangYDHSH16,](#bib.bib108)）。
- en: 2.2.5\. Attention-based Methods
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5\. 基于注意力的方法
- en: 'CNN and RNN provide excellent results on tasks related to text classification.
    However, these models are not intuitive enough for poor interpretability, especially
    in classification errors, which cannot be explained due to the non-readability
    of hidden data. The attention-based methods are successfully used in the text
    classification. Bahdanau et al. ([DBLP:journals/corr/BahdanauCB14,](#bib.bib190)
    ) first propose an attention mechanism that can be used in machine translation.
    Motivated by this, Yang et al. ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) )
    introduce the Hierarchical Attention Network (HAN) to gain better visualization
    by employing the extremely informational components of a text, as shown in Fig. [8](#S2.F8
    "Figure 8 ‣ 2.2.4\. CNN-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning"). HAN includes two encoders and two levels of attention layers.
    The attention mechanism lets the model pay different attention to specific inputs.
    It aggregates essential words into sentence vectors firstly and then aggregates
    vital sentence vectors into text vectors. It can learn how much contribution of
    each word and sentence for the classification judgment, which is beneficial for
    applications and analysis through the two levels of attention.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN 和 RNN 在文本分类任务中表现出色。然而，这些模型的直观性不足，解释性较差，特别是在分类错误时，由于隐藏数据的不透明性，这些错误无法解释。基于注意力的方法在文本分类中得到了成功应用。Bahdanau
    等人（[DBLP:journals/corr/BahdanauCB14,](#bib.bib190)）首次提出了一种可用于机器翻译的注意力机制。受到这一工作的启发，Yang
    等人（[DBLP:conf/naacl/YangYDHSH16,](#bib.bib108)）引入了层次注意力网络（HAN），通过利用文本中的极其信息丰富的成分来获得更好的可视化，如图[8](#S2.F8
    "Figure 8 ‣ 2.2.4\. CNN-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning")所示。HAN 包括两个编码器和两个层次的注意力层。注意力机制使模型能够对特定输入给予不同的关注。它首先将重要的词汇聚合成句子向量，然后将重要的句子向量聚合成文本向量。它可以学习每个词和句子对分类判断的贡献程度，这对于通过这两个层次的注意力进行应用和分析是有益的。'
- en: The attention mechanism can improve the performance with interpretability for
    text classification, which makes it popular. There are some other works based
    on attention. LSTMN ([DBLP:conf/emnlp/0001DL16,](#bib.bib112) ) is proposed to
    process text step by step from left to right and does superficial reasoning through
    memory and attention. BI-Attention ([DBLP:conf/emnlp/ZhouWX16,](#bib.bib110) )
    is designed for cross-lingual text classification to catch bilingual long-distance
    dependencies. Hu et al. ([DBLP:conf/coling/HuLT0S18,](#bib.bib191) ) propose an
    attention mechanism based on category attributes for solving the imbalance of
    the number of various charges which contain few-shot charges. HAPN ([DBLP:conf/emnlp/SunSZL19,](#bib.bib124)
    ) is presented for few-shot text classification.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制可以提升文本分类的性能并提供解释性，这使其变得非常流行。还有一些其他基于注意力的方法。LSTMN（[DBLP:conf/emnlp/0001DL16,](#bib.bib112)）被提出用于从左到右逐步处理文本，并通过记忆和注意力进行浅层推理。BI-Attention（[DBLP:conf/emnlp/ZhouWX16,](#bib.bib110)）被设计用于跨语言文本分类，以捕捉双语的长距离依赖关系。Hu
    等人（[DBLP:conf/coling/HuLT0S18,](#bib.bib191)）提出了一种基于类别属性的注意力机制，用于解决包含少量样本的各种类别之间的不平衡问题。HAPN（[DBLP:conf/emnlp/SunSZL19,](#bib.bib124)）则被提出用于少样本文本分类。
- en: 'Self-attention ([DBLP:conf/nips/VaswaniSPUJGKP17,](#bib.bib192) ) captures
    the weight distribution of words in sentences by constructing K, Q and V matrices
    among sentences that can capture long-range dependencies on text classification.
    We give an example for self-attention, as shown in Fig. [9](#S2.F9 "Figure 9 ‣
    2.2.5\. Attention-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning").
    Each input word vector $a_{i}$ can be represented as three n-dimensional vectors,
    including $q_{i}$, $k_{i}$ and $v_{i}$. After self-attention, the output vector
    $b_{i}$ can be represented as $\sum_{j}softmax(a_{ij})v_{j}$ and $a_{ij}=q_{i}\cdot
    k_{j}/\sqrt{n}$. All output vectors can be parallelly computed. Lin et al. ([DBLP:conf/iclr/LinFSYXZB17,](#bib.bib114)
    ) used source token self-attention to explore the weight of every token to the
    entire sentence in the sentence representation task. To capture long-range dependencies,
    Bi-directional Block Self-Attention Network (Bi-BloSAN) ([DBLP:conf/iclr/ShenZL0Z18,](#bib.bib120)
    ) uses an intra-block Self-Attention Network (SAN) to every block split by sequence
    and an inter-block SAN to the outputs.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '自注意力（[DBLP:conf/nips/VaswaniSPUJGKP17](#bib.bib192)）通过在句子中构建K、Q和V矩阵来捕捉词的权重分布，这些矩阵可以捕捉文本分类中的长程依赖。我们举一个自注意力的例子，如图[9](#S2.F9
    "Figure 9 ‣ 2.2.5\. Attention-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\.
    Text Classification Methods ‣ A Survey on Text Classification: From Traditional
    to Deep Learning")所示。每个输入词向量$a_{i}$可以表示为三个n维向量，包括$q_{i}$、$k_{i}$和$v_{i}$。自注意力之后，输出向量$b_{i}$可以表示为$\sum_{j}softmax(a_{ij})v_{j}$和$a_{ij}=q_{i}\cdot
    k_{j}/\sqrt{n}$。所有输出向量可以并行计算。Lin等人（[DBLP:conf/iclr/LinFSYXZB17](#bib.bib114)）使用源token自注意力来探索每个token在句子表示任务中的权重。为了捕捉长程依赖，双向块自注意力网络（Bi-BloSAN）（[DBLP:conf/iclr/ShenZL0Z18](#bib.bib120)）使用一个块内自注意力网络（SAN）对按序列分块的每个块进行处理，并对输出使用块间SAN。'
- en: '![Refer to caption](img/fd765146fe56aec66bd84177f8a929cf.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fd765146fe56aec66bd84177f8a929cf.png)'
- en: Figure 9\. An example of self-attention for calculating output vector $b_{2}$.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 自注意力计算输出向量$b_{2}$的示例。
- en: 'Aspect-Based Sentiment Analysis (ABSA) ([inproceedings2017,](#bib.bib31) ;
    [DBLP:conf/aaai/MaPC18,](#bib.bib193) ) breaks down a text into multiple aspects
    and allocates each aspect a sentiment polarity. The sentiment polarity can be
    divided into three types: positive, neutral and negative. Some attention-based
    models are proposed to identify the fine-grained opinion polarity towards a specific
    aspect for aspect-based sentiment tasks. ATAE-LSTM ([DBLP:conf/emnlp/WangHZZ16,](#bib.bib194)
    ) can concentrate on different parts of each sentence according to the input through
    the attention mechanisms. MGAN ([DBLP:conf/emnlp/FanFZ18,](#bib.bib195) ) presents
    a fine-grained attention mechanism with a coarse-grained attention mechanism to
    learn the word-level interaction between context and aspect.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方面的情感分析（ABSA）（[inproceedings2017](#bib.bib31)；[DBLP:conf/aaai/MaPC18](#bib.bib193)）将文本拆解为多个方面，并为每个方面分配情感极性。情感极性可以分为三种类型：积极、中性和消极。一些基于注意力的模型被提出用于识别对特定方面的细粒度情感极性。ATAE-LSTM（[DBLP:conf/emnlp/WangHZZ16](#bib.bib194)）可以根据输入通过注意力机制集中于每个句子的不同部分。MGAN（[DBLP:conf/emnlp/FanFZ18](#bib.bib195)）提出了一种细粒度注意力机制与粗粒度注意力机制，以学习上下文和方面之间的词级交互。
- en: To catch the complicated semantic relationship among each question and candidate
    answers for the QA task, Tan et al. ([tan-etal-2016-improved,](#bib.bib196) )
    introduce CNN and RNN and generate answer embeddings by using a simple one-way
    attention mechanism affected through the question context. The attention captures
    the dependence among the embeddings of questions and answers. Extractive QA can
    be seen as the text classification task. It inputs a question and multiple candidates
    answers and classifies every candidate answer to recognize the correct answer.
    Furthermore, AP-BILSTM ([DBLP:journals/corr/SantosTXZ16,](#bib.bib197) ) with
    a two-way attention mechanism can learn the weights between the question and each
    candidate answer to obtain the importance of each candidate answer to the question.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉每个问题与候选答案之间复杂的语义关系，Tan等人（[tan-etal-2016-improved](#bib.bib196)）引入了CNN和RNN，并通过一个简单的单向注意机制生成答案嵌入，受问题上下文的影响。注意机制捕捉了问题和答案嵌入之间的依赖关系。抽取式QA可以被视为文本分类任务。它输入一个问题和多个候选答案，并对每个候选答案进行分类，以识别正确答案。此外，AP-BILSTM（[DBLP:journals/corr/SantosTXZ16](#bib.bib197)）通过双向注意机制学习问题和每个候选答案之间的权重，从而获得每个候选答案对问题的重要性。
- en: '![Refer to caption](img/0786e49dda400d17e179f353f8dfc0b4.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0786e49dda400d17e179f353f8dfc0b4.png)'
- en: Figure 10\. Differences in pre-trained model architectures ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ), including BERT, OpenAI GPT and ELMo. $E_{i}$ represents embedding of $i$ th
    input. Trm represents the transformer block. $T_{i}$ represents predicted tag
    of $i$ th input.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 预训练模型架构的差异 ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19))，包括 BERT、OpenAI
    GPT 和 ELMo。 $E_{i}$ 表示第 $i$ 个输入的嵌入。Trm 表示变换器块。 $T_{i}$ 表示第 $i$ 个输入的预测标签。
- en: 2.2.6\. Pre-trained Methods
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6\. 预训练方法
- en: Pre-trained language models ([DBLP:journals/corr/abs-2003-08271,](#bib.bib198)
    ) effectively learn global semantic representation and significantly boost NLP
    tasks, including text classification. It generally uses unsupervised methods to
    mine semantic knowledge automatically and then construct pre-training targets
    so that machines can learn to understand semantics.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型 ([DBLP:journals/corr/abs-2003-08271,](#bib.bib198)) 有效地学习全局语义表示，并显著提升自然语言处理任务，包括文本分类。它通常使用无监督方法自动挖掘语义知识，然后构建预训练目标，使机器能够学习理解语义。
- en: 'As shown in Fig. [10](#S2.F10 "Figure 10 ‣ 2.2.5\. Attention-based Methods
    ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey on Text
    Classification: From Traditional to Deep Learning"), we give differences in the
    model architectures among the Embedding from Language Model (ELMo) ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118)
    ), OpenAI GPT ([Radford2018ImprovingLU,](#bib.bib199) ), and BERT ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ). ELMo ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118) ) is a deep contextualized
    word representation model, which is readily integrated into models. It can model
    complicated characteristics of words and learn different representations for various
    linguistic contexts. It learns each word embedding according to the context words
    with the bi-directional LSTM. GPT ([Radford2018ImprovingLU,](#bib.bib199) ) employs
    supervised fine-tuning and unsupervised pre-training to learn general representations
    that transfer with limited adaptation to many NLP tasks. Furthermore, the domain
    of the target dataset does not need to be similar to the domain of unlabeled datasets.
    The training procedure of the GPT algorithm usually includes two stages. Firstly,
    the initial parameters of a neural network model are learned by a modeling objective
    on the unlabeled dataset. We can then employ the corresponding supervised objective
    to accommodate these parameters for the target task. To pre-train deep bidirectional
    representations from the unlabeled text through joint conditioning on both left
    and right context in every layer, BERT model ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ), proposed by Google, significantly improves performance on NLP tasks, including
    text classification. BERT applies the bi-directional encoder designed to pre-train
    the bi-directional representation of depth by jointly adjusting the context in
    all layers. It can utilize contextual information when predicting which words
    are masked. It is fine-tuned by adding just an additional output layer to construct
    models for multiple NLP tasks, such as SA, QA, and machine translation. Comparing
    with these three models, ELMo is a feature-based method using LSTM, and BERT and
    OpenAI GPT are fine-tuning approaches using Transformer. Furthermore, ELMo and
    BERT are bidirectional training models and OpenAI GPT is training from left to
    right. Therefore, BERT gets a better result, which combines the advantages of
    ELMo and OpenAI GPT.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[10](#S2.F10 "图 10 ‣ 2.2.5\. 基于注意力的方法 ‣ 2.2\. 深度学习模型 ‣ 2\. 文本分类方法 ‣ 从传统到深度学习的文本分类调查")所示，我们展示了语言模型的嵌入（ELMo）（[DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118)）、OpenAI
    GPT（[Radford2018ImprovingLU,](#bib.bib199)）和BERT（[DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)）之间模型架构的差异。ELMo（[DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118)）是一个深度上下文化的词表示模型，可以轻松集成到模型中。它能够建模词汇的复杂特征，并为不同的语言环境学习不同的表示。它根据上下文词汇通过双向LSTM学习每个词的嵌入。GPT（[Radford2018ImprovingLU,](#bib.bib199)）采用监督微调和无监督预训练来学习通用表示，这些表示在有限适应下可以转移到许多NLP任务中。此外，目标数据集的领域不需要与未标记数据集的领域相似。GPT算法的训练过程通常包括两个阶段。首先，通过未标记数据集上的建模目标来学习神经网络模型的初始参数。然后，我们可以使用相应的监督目标来调整这些参数以适应目标任务。BERT模型（[DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)），由Google提出，通过在每一层对左右上下文进行联合条件处理来预训练深度双向表示，显著提高了NLP任务的表现，包括文本分类。BERT应用双向编码器，旨在通过共同调整所有层的上下文来预训练深度的双向表示。它可以在预测被掩盖的词汇时利用上下文信息。它通过仅添加一个额外的输出层来微调，构建多个NLP任务的模型，如情感分析（SA）、问答（QA）和机器翻译。与这三种模型相比，ELMo是使用LSTM的特征基础方法，而BERT和OpenAI
    GPT是使用Transformer的微调方法。此外，ELMo和BERT是双向训练模型，而OpenAI GPT是从左到右的训练。因此，BERT获得了更好的结果，它结合了ELMo和OpenAI
    GPT的优点。
- en: 'Transformer-based models can parallelize computation without considering the
    sequential information suitable for large scale datasets, making it popular for
    NLP tasks. Thus, some other works are used for text classification tasks and get
    excellent performance. RoBERTa ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140)
    ), is an improved version of BERT, adopts the dynamic masking method that generates
    the masking pattern every time with a sequence to be fed into the model. It uses
    more data for longer pre-training and estimates the influence of various essential
    hyperparameters and the size of training data. To be specific: 1) The training
    time is longer (a total of nearly 200,000 training, nearly 1.6 billion training
    data have been seen), the batch size (8K) is larger, and the training data is
    more (30G Chinese training, including 300 million sentences and 10 billion words);
    2) It removes the next sentence prediction (NSP) task; 3) It employs more extended
    training sequence; 4) It dynamically adjusts the masking mechanism and use the
    full word mask.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的模型可以并行计算而无需考虑顺序信息，这使其适用于大规模数据集，因而在 NLP 任务中广受欢迎。因此，一些其他工作被用于文本分类任务并取得了优秀的表现。RoBERTa
    ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140)) 是 BERT 的改进版本，采用了动态掩码方法，每次生成掩码模式以供模型处理。它使用更多数据进行更长时间的预训练，并估计各种关键超参数和训练数据的影响。具体而言：1）训练时间更长（总共近
    200,000 次训练，已见近 16 亿训练数据），批量大小（8K）更大，训练数据更多（30G 中文训练，包括 3 亿句子和 100 亿词）；2）去除了下一个句子预测（NSP）任务；3）采用了更长的训练序列；4）动态调整掩码机制并使用完整词掩码。
- en: XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) is a generalized autoregressive
    pre-training approach. Unlike BERT, the denoising autoencoder with the mask is
    not used in the first stage, but the autoregressive LM is used. It maximizes the
    expected likelihood across the whole factorization order permutations to learn
    the bidirectional context. Furthermore, it can overcome the weaknesses of BERT
    by an autoregressive formulation and integrate ideas from Transformer-XL ([DBLP:conf/acl/DaiYYCLS19,](#bib.bib200)
    ) into pre-training.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138)) 是一种广义自回归预训练方法。与 BERT 不同，第一阶段没有使用带有掩码的去噪自编码器，而是使用自回归语言模型。它通过最大化整个因子分解顺序排列的期望似然性来学习双向上下文。此外，它通过自回归公式克服了
    BERT 的弱点，并将 Transformer-XL ([DBLP:conf/acl/DaiYYCLS19,](#bib.bib200)) 的思想融入到预训练中。
- en: BERT model has many parameters. In order to reduce the parameters, ALBERT ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146)
    ) uses two-parameter simplification schemes. It reduces the fragmentation vector’s
    length and shares parameters with all encoders. It also replaces the next sentence
    matching task with the next sentence order task and continuously blocks fragmentation.
    When the ALBERT model is pre-trained on a massive Chinese corpus, the parameters
    are less and better performance. In general, these methods adopt unsupervised
    objective functions for pre-training, including the next sentence prediction,
    masking technology, and permutation. These target functions based on the word
    prediction demonstrate a strong ability to learn the word dependence and semantic
    structure ([DBLP:conf/acl/JawaharSS19,](#bib.bib201) ).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型具有许多参数。为了减少参数，ALBERT ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146)) 使用了两种参数简化方案。它减少了碎片向量的长度，并与所有编码器共享参数。它还将下一个句子匹配任务替换为下一个句子顺序任务，并持续阻止碎片化。当
    ALBERT 模型在大规模中文语料库上进行预训练时，参数更少且性能更佳。通常，这些方法采用无监督目标函数进行预训练，包括下一个句子预测、掩码技术和排列。这些基于词预测的目标函数展现出强大的词依赖和语义结构学习能力
    ([DBLP:conf/acl/JawaharSS19,](#bib.bib201))。
- en: '![Refer to caption](img/c6209f6616000824b951ae1977cc34f2.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c6209f6616000824b951ae1977cc34f2.png)'
- en: Figure 11\. The architecture of BART ([DBLP:conf/acl/LewisLGGMLSZ20,](#bib.bib202)
    ) and SpanBERT ([DBLP:journals/tacl/JoshiCLWZL20,](#bib.bib203) ).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. BART ([DBLP:conf/acl/LewisLGGMLSZ20,](#bib.bib202)) 和 SpanBERT ([DBLP:journals/tacl/JoshiCLWZL20,](#bib.bib203))
    的架构。
- en: 'BART ([DBLP:conf/acl/LewisLGGMLSZ20,](#bib.bib202) ) is a denoising autoencoder
    based on the Seq2Seq model, as shown in Fig. [11](#S2.F11 "Figure 11 ‣ 2.2.6\.
    Pre-trained Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods
    ‣ A Survey on Text Classification: From Traditional to Deep Learning") (a). The
    pre-training of BART consists of two steps. Firstly, it uses a noise function
    to destroy the text. Secondly, the Seq2Seq model is used to reconstruct the original
    text. In various noise methods, by randomly shuffling the order of the original
    sentence and then using the first new text filling method to obtain optimal performance.
    The new text filling method is replacing the text fragment with a single mask
    token. It uses only a specific masked token to indicate that a token is masked.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'BART（[DBLP:conf/acl/LewisLGGMLSZ20,](#bib.bib202)）是基于Seq2Seq模型的去噪自动编码器，如图[11](#S2.F11
    "Figure 11 ‣ 2.2.6\. Pre-trained Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning")（a）所示。BART的预训练包括两个步骤。首先，它使用噪声函数破坏文本。其次，使用Seq2Seq模型重构原始文本。在各种噪声方法中，通过随机打乱原始句子的顺序，然后使用新的文本填充方法来获得最佳性能。新的文本填充方法是用单个掩码标记替换文本片段。它仅使用特定的掩码标记来表示一个标记被掩盖。'
- en: 'SpanBERT ([DBLP:journals/tacl/JoshiCLWZL20,](#bib.bib203) ) is specially designed
    to better represent and predict spans of text, as shown in Fig. [11](#S2.F11 "Figure
    11 ‣ 2.2.6\. Pre-trained Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")
    (b). It optimizes BERT from three aspects and achieves good results in multiple
    tasks such as QA. The specific optimization is embodied in three aspects. Firstly,
    the span mask scheme is proposed to mask a continuous paragraph of text randomly.
    Secondly, Span Boundary Objective (SBO) is added to predict span by the token
    next to the span boundary to get the better performance to finetune stage. Thirdly,
    the NSP pre-training task is removed.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'SpanBERT（[DBLP:journals/tacl/JoshiCLWZL20,](#bib.bib203)）专门设计用于更好地表示和预测文本的跨度，如图[11](#S2.F11
    "Figure 11 ‣ 2.2.6\. Pre-trained Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text
    Classification Methods ‣ A Survey on Text Classification: From Traditional to
    Deep Learning")（b）所示。它从三个方面优化了BERT，并在多个任务（如QA）中取得了良好的结果。具体优化体现在三个方面。首先，提出了跨度掩码方案，以随机掩盖一段连续的文本。其次，添加了跨度边界目标（SBO），通过跨度边界旁边的标记来预测跨度，以在微调阶段获得更好的性能。第三，移除了NSP预训练任务。'
- en: ERNIE ([DBLP:journals/corr/abs-1904-09223,](#bib.bib204) ) is based on the method
    of knowledge enhancement. It learns the semantic relations in the real world by
    modeling the prior semantic knowledge such as entity concepts in massive datasets.
    Specifically, ERNIE enables the model to learn the semantic representation of
    complete concepts by masking semantic units such as words and entities. It mainly
    consists of a Transformer encoder and task embedding. In the Transformer encoder,
    the context information of each token is captured by the self-attention mechanism,
    and the context representation is generated for embedding. Task embedding is used
    for tasks with different characteristics.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ERNIE（[DBLP:journals/corr/abs-1904-09223,](#bib.bib204)）基于知识增强的方法。它通过在大量数据集中建模先验语义知识（如实体概念）来学习现实世界中的语义关系。具体而言，ERNIE使模型能够通过掩盖诸如词汇和实体等语义单元来学习完整概念的语义表示。它主要由一个Transformer编码器和任务嵌入组成。在Transformer编码器中，通过自注意力机制捕获每个标记的上下文信息，并生成用于嵌入的上下文表示。任务嵌入用于具有不同特征的任务。
- en: 2.2.7\. GNN-based Methods
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7\. 基于GNN的方法
- en: 'The DNN models like CNN get great performance on regular structure, not for
    arbitrarily structured graphs. Some researchers study how to expand on arbitrarily
    structured graphs ([DBLP:conf/nips/DefferrardBV16,](#bib.bib205) ; [peng2021reinforced,](#bib.bib206)
    ). With the increasing attention of Graph Neural Networks (GNNs), GNN-based models
    ([peng2021lime,](#bib.bib207) ; [li2021higher,](#bib.bib208) ) obtain excellent
    performance by encoding syntactic structure of sentences on semantic role labeling
    task ([DBLP:conf/emnlp/MarcheggianiT17,](#bib.bib209) ), relation classification
    task ([DBLP:journals/jamia/LiJL19,](#bib.bib210) ) and machine translation task
    ([DBLP:conf/emnlp/BastingsTAMS17,](#bib.bib211) ). It turns text classification
    into a graph node classification task. We show a GCN model for text classification
    with four input texts, as shown in Fig. [12](#S2.F12 "Figure 12 ‣ 2.2.7\. GNN-based
    Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification Methods ‣ A Survey
    on Text Classification: From Traditional to Deep Learning"). Firstly, the four
    input texts $T=[T_{1},T_{2},T_{3},T_{4}]$ and the words $X=[x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}]$
    in the text, defined as nodes, are constructed into the graph structures. The
    graph nodes are connected by bold black edges, which indicates document-word edges
    and word-word edges. The weight of each word-word edge usually means their co-occurrence
    frequency in the corpus. Then, the words and texts are represented through the
    hidden layer. Finally, the label of all input texts can be predicted by the graph.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '类似CNN的DNN模型在规则结构上表现出色，但对于任意结构的图则不适用。一些研究人员研究如何在任意结构的图上扩展 ([DBLP:conf/nips/DefferrardBV16,](#bib.bib205)
    ; [peng2021reinforced,](#bib.bib206) )。随着图神经网络（GNNs）的关注度增加，基于GNN的模型 ([peng2021lime,](#bib.bib207)
    ; [li2021higher,](#bib.bib208) ) 通过对句子的句法结构进行编码，在语义角色标注任务 ([DBLP:conf/emnlp/MarcheggianiT17,](#bib.bib209)
    )、关系分类任务 ([DBLP:journals/jamia/LiJL19,](#bib.bib210) ) 和机器翻译任务 ([DBLP:conf/emnlp/BastingsTAMS17,](#bib.bib211)
    ) 中取得了优异的表现。它将文本分类转变为图节点分类任务。我们展示了一个用于文本分类的GCN模型，包含四个输入文本，如图[12](#S2.F12 "Figure
    12 ‣ 2.2.7\. GNN-based Methods ‣ 2.2\. Deep Learning Models ‣ 2\. Text Classification
    Methods ‣ A Survey on Text Classification: From Traditional to Deep Learning")所示。首先，将四个输入文本
    $T=[T_{1},T_{2},T_{3},T_{4}]$ 和文本中的词 $X=[x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}]$
    定义为节点，并构建成图结构。图节点通过粗体黑色边连接，表示文档-词边和词-词边。每个词-词边的权重通常表示它们在语料库中的共现频率。然后，通过隐藏层表示词和文本。最后，图可以预测所有输入文本的标签。'
- en: '![Refer to caption](img/a05fdb4fd6be1c599175c957bec1d457.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a05fdb4fd6be1c599175c957bec1d457.png)'
- en: Figure 12\. The GNN-based model. The initial graph differently depending on
    how the graph is designed. We give an example to establish edges between documents
    and documents, documents and sentences, and words to words.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 基于GNN的模型。初始图根据图的设计方式有所不同。我们给出了一个示例，建立文档与文档、文档与句子以及词与词之间的边。
- en: The GNN-based models can learn the syntactic structure of sentences, making
    some researchers study using GNN for text classification. DGCNN ([DBLP:conf/www/PengLHLBWS018,](#bib.bib153)
    ) is a graph-CNN converting text to graph-of-words, having the advantage of learning
    different levels of semantics with CNN models. Yao et al. ([DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ) propose the Text Graph Convolutional Network (TextGCN), which builds a heterogeneous
    word text graph for a whole dataset and captures global word co-occurrence information.
    To enable GNN-based models to underpin online testing, Huang et al. ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) build graphs for each text with global parameter sharing, not a corpus-level
    graph structure, to help preserve global information and reduce the burden. TextING
    ([DBLP:conf/acl/ZhangYCWWW20,](#bib.bib162) ) builds individual graphs for each
    document and learns text-level word interactions by GNN to effectively produce
    embeddings for obscure words in the new text.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GNN的模型可以学习句子的句法结构，这使得一些研究人员研究使用GNN进行文本分类。DGCNN ([DBLP:conf/www/PengLHLBWS018,](#bib.bib153)
    ) 是一种将文本转换为词图的图-CNN，具有利用CNN模型学习不同层次语义的优点。Yao等人 ([DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ) 提出了文本图卷积网络（TextGCN），它为整个数据集构建了异构词文本图，并捕捉全局词共现信息。为了使基于GNN的模型支持在线测试，Huang等人 ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) 为每个文本构建了图，通过全局参数共享而非语料库级别的图结构，以帮助保存全局信息并减少负担。TextING ([DBLP:conf/acl/ZhangYCWWW20,](#bib.bib162)
    ) 为每个文档构建了单独的图，并通过GNN学习文本级词交互，以有效地生成新文本中模糊词的嵌入。
- en: Graph ATtention network (GAT) ([DBLP:conf/iclr/VelickovicCCRLB18,](#bib.bib212)
    ) employs masked self-attention layers by attending over its neighbors. Thus,
    some GAT-based models are proposed to compute the hidden representations of each
    node. The Heterogeneous Graph ATtention networks (HGAT) ([DBLP:conf/emnlp/HuYSJL19,](#bib.bib213)
    ) with a dual-level attention mechanism learns the importance of different neighboring
    nodes and node types in the current node. The model propagates information on
    the graph and captures the relations to address the semantic sparsity for semi-supervised
    short text classification. MAGNET ([DBLP:conf/icaart/PalSS20,](#bib.bib166) )
    is proposed to capture the correlation among the labels based on GATs, which learns
    the crucial dependencies between the labels and generates classifiers by a feature
    matrix and a correlation matrix.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图注意力网络（GAT） ([DBLP:conf/iclr/VelickovicCCRLB18,](#bib.bib212) ) 通过对邻居进行掩蔽自注意力层处理。于是，一些基于GAT的模型被提出以计算每个节点的隐藏表示。异构图注意力网络（HGAT）
    ([DBLP:conf/emnlp/HuYSJL19,](#bib.bib213) ) 具有双层注意力机制，学习当前节点中不同邻居节点和节点类型的重要性。该模型在图上传播信息并捕捉关系，以解决半监督短文本分类中的语义稀疏问题。MAGNET
    ([DBLP:conf/icaart/PalSS20,](#bib.bib166) ) 提出基于GAT捕捉标签之间的相关性，学习标签之间的关键依赖关系，并通过特征矩阵和相关矩阵生成分类器。
- en: Event Prediction (EP) can be divided into generated event prediction and selective
    event prediction (also known as script event prediction). EP, referring to scripted
    event prediction in this review, infers the subsequent event according to the
    existing event context. Unlike other text classification tasks, texts in EP are
    composed of a series of sequential subevents. Extracting features of the relationship
    among such subevents is of critical importance. SGNN ([DBLP:conf/ijcai/LiDL18,](#bib.bib214)
    ) is proposed to model event interactions and learn better event representations
    by constructing an event graph to utilize the event network information better.
    The model makes full use of dense event connections for the EP task.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 事件预测（EP）可以分为生成事件预测和选择性事件预测（也称为脚本事件预测）。在本综述中提到的事件预测是指脚本事件预测，它根据现有事件上下文推断后续事件。与其他文本分类任务不同，EP中的文本由一系列顺序的子事件组成。提取这些子事件之间关系的特征至关重要。SGNN
    ([DBLP:conf/ijcai/LiDL18,](#bib.bib214) ) 提出通过构建事件图来建模事件交互并学习更好的事件表示，以更好地利用事件网络信息。该模型充分利用密集的事件连接来进行EP任务。
- en: 2.2.8\. Others
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.8\. 其他
- en: In addition to all the above models, there are some other individual models.
    Here we introduce some exciting models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述所有模型之外，还有一些其他独立模型。在这里我们介绍一些令人兴奋的模型。
- en: Siamese Neural Network.
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 孪生神经网络。
- en: The siamese neural network ([DBLP:conf/nips/BromleyGLSS93,](#bib.bib215) ) is
    also called a twin neural network (Twin NN). It utilizes equal weights while working
    in tandem using two distinct input vectors to calculate comparable output vectors.
    Mueller et al. ([DBLP:conf/aaai/MuellerT16,](#bib.bib216) ) present a siamese
    adaptation of the LSTM network comprised of couples of variable-length sequences.
    The model is employed to estimate the semantic similarity among texts, exceeding
    carefully handcrafted features and proposed neural network models of higher complexity.
    The model further represents text employing neural networks whose inputs are word
    vectors learned separately from a vast dataset. To settle unbalanced data classification
    in the medical domain, Jayadeva et al. ([JAYADEVA201934,](#bib.bib217) ) use a
    Twin NN model to learn from enormous unbalanced corpora. The objective functions
    achieve the Twin SVM approach with non-parallel decision boundaries for the corresponding
    classes, and decrease the Twin NN complexity, optimizing the feature map to better
    discriminate among classes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生神经网络 ([DBLP:conf/nips/BromleyGLSS93,](#bib.bib215) ) 也被称为双胞神经网络（Twin NN）。它在使用两个不同输入向量时利用相等的权重来计算可比的输出向量。Mueller等人
    ([DBLP:conf/aaai/MuellerT16,](#bib.bib216) ) 提出了LSTM网络的孪生适应，包括由可变长度序列组成的对。该模型用于估计文本之间的语义相似性，超越了精心设计的特征和复杂度更高的神经网络模型。该模型进一步使用神经网络表示文本，其输入是从大规模数据集中单独学习的词向量。为了在医疗领域解决不平衡数据分类问题，Jayadeva等人
    ([JAYADEVA201934,](#bib.bib217) ) 使用Twin NN模型从大量不平衡语料中学习。目标函数实现了Twin SVM方法，通过不平行的决策边界区分相应的类别，并减少Twin
    NN的复杂性，优化特征图以更好地区分类别。
- en: Virtual Adversarial Training (VAT)
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 虚拟对抗训练（VAT）
- en: Deep learning methods require many extra hyperparameters, which increase the
    computational complexity. VAT ([miyato2015distributional,](#bib.bib218) ), regularization
    based on local distributional smoothness can be used in semi-supervised tasks,
    requires only some hyperparameters, and can be interpreted directly as robust
    optimization. Miyato et al. ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) use VAT
    to effectively improve the robustness and generalization ability of the model
    and word embedding performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法需要许多额外的超参数，这增加了计算复杂性。VAT ([miyato2015distributional,](#bib.bib218))，基于局部分布平滑的正则化可以用于半监督任务，仅需要一些超参数，并且可以直接解释为鲁棒优化。Miyato
    等人 ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85)) 使用VAT有效地提高了模型的鲁棒性、泛化能力和词嵌入性能。
- en: Reinforcement Learning (RL)
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强化学习 (RL)
- en: RL learns the best action in a given environment through maximizing cumulative
    rewards. Zhang et al. ([DBLP:conf/aaai/ZhangHZ18,](#bib.bib219) ) offer an RL
    approach to establish structured sentence representations via learning the structures
    related to tasks. The model has Information Distilled LSTM (ID-LSTM) and Hierarchical
    Structured LSTM (HS-LSTM) representation models. The ID-LSTM learns the sentence
    representation by choosing essential words relevant to tasks, and the HS-LSTM
    is a two-level LSTM for modeling sentence representation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: RL 通过最大化累积奖励来学习在给定环境中的最佳动作。Zhang 等人 ([DBLP:conf/aaai/ZhangHZ18,](#bib.bib219))
    提出了一种RL方法，通过学习与任务相关的结构来建立结构化句子表示。该模型包括信息提取LSTM (ID-LSTM) 和层次结构化LSTM (HS-LSTM)
    表示模型。ID-LSTM 通过选择与任务相关的重要词汇来学习句子表示，HS-LSTM 是一种用于建模句子表示的两级LSTM。
- en: Memory Networks
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记忆网络
- en: Memory networks ([weston2015memory,](#bib.bib220) ) learn to combine the inference
    components and the long-term memory component. Li et al. ([DBLP:conf/emnlp/LiL17,](#bib.bib221)
    ) use two LSTMs with extended memories and neural memory operations for jointly
    handling the extraction tasks of aspects and opinions via memory interactions.
    Topic Memory Networks (TMN) ([DBLP:conf/emnlp/ZengLSGLK18,](#bib.bib169) ) is
    an end-to-end model that encodes latent topic representations indicative of class
    labels.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆网络 ([weston2015memory,](#bib.bib220)) 学习将推理组件与长期记忆组件结合起来。Li 等人 ([DBLP:conf/emnlp/LiL17,](#bib.bib221))
    使用两个扩展记忆的LSTM和神经记忆操作，通过记忆交互共同处理方面和意见的提取任务。主题记忆网络 (TMN) ([DBLP:conf/emnlp/ZengLSGLK18,](#bib.bib169))
    是一个端到端模型，用于编码指示类别标签的潜在主题表示。
- en: QA Style for Sentiment Classification Task.
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 情感分类任务的QA风格。
- en: It is an interesting attempt to treat the sentiment classification task as a
    QA task. Shen et al. ([DBLP:conf/emnlp/ShenSWKLLSZZ18,](#bib.bib222) ) create
    a high-quality annotated corpus. A three-stage hierarchical matching network was
    proposed to consider the matching information between questions and answers.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将情感分类任务视为QA任务是一种有趣的尝试。Shen 等人 ([DBLP:conf/emnlp/ShenSWKLLSZZ18,](#bib.bib222))
    创建了一个高质量的标注语料库。提出了一种三阶段的层次匹配网络，以考虑问题和答案之间的匹配信息。
- en: External Commonsense Knowledge.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 外部常识知识。
- en: Due to the insufficient information of the event itself to distinguish the event
    for the EP task, Ding et al. ([DBLP:conf/emnlp/DingLLLD19,](#bib.bib223) ) consider
    that the event extracted from the original text lacked common knowledge, such
    as the intention and emotion of the event participants. The model improves the
    effect of stock prediction, EP, and so on.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于事件自身信息不足以区分EP任务中的事件，Ding 等人 ([DBLP:conf/emnlp/DingLLLD19,](#bib.bib223)) 认为从原始文本中提取的事件缺乏常识，例如事件参与者的意图和情感。该模型改善了股票预测、EP等的效果。
- en: Quantum Language Model.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量子语言模型。
- en: In the quantum language model, the words and dependencies among words are represented
    through fundamental quantum events. Zhang et al. ([Zhang2019A,](#bib.bib224) )
    design a quantum-inspired sentiment representation method to learn both the semantic
    and the sentiment information of subjective text. By inputting density matrices
    to the embedding layer, the performance of the model improves.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在量子语言模型中，词汇和词汇之间的依赖关系通过基本的量子事件表示。Zhang 等人 ([Zhang2019A,](#bib.bib224)) 设计了一种量子启发的情感表示方法，以学习主观文本的语义和情感信息。通过将密度矩阵输入嵌入层，模型性能得到了改善。
- en: Summary. RNN computes sequentially and cannot be calculated in parallel. The
    shortcoming of RNN makes it more challenging to become mainstream in the current
    trend that models tend to have deeper and more parameters. CNN extracts features
    from text vectors through the convolution kernel. The number of features captured
    by the convolution kernel is related to its size. CNN is deep enough that, in
    theory, it can capture features at long distances. Due to insufficient optimization
    methods for parameters of the deep network and the loss of location information
    due to the pooling layer, the deeper layer does not bring significant improvement.
    Compared with RNN, CNN has parallel computing capability and can effectively retain
    location information for the improved version of CNN. Still, it has weak feature
    capture capability for long-distance. GNN builds a graph for text. When a valid
    graph structure is designed, the learned representation can better capture the
    structural information. Transformer treats the input text as a fully connected
    graph, with attention score weights on the edges. It is capable of parallel computing
    and is highly efficient in extracting features between different words by self-attention,
    solving short-term memory problems. However, the attention mechanism in Transformer
    is computation-heavy, especially when dealing with long sequences. Some improved
    models ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146) ; [DBLP:conf/nips/ZafrirBIW19,](#bib.bib225)
    ) for computing complexity in Transformer have recently been proposed. Overall,
    Transformer is a better choice for text classification. Deep Learning consists
    of multiple hidden layers in a neural network with a higher level of complexity
    and can be trained on unstructured data. Deep learning can learn language features
    and master higher level and more abstract language features based on words and
    vectors. Deep learning architecture can learn feature representations directly
    from the input without too many manual interventions and prior knowledge. However,
    deep learning technology is a data-driven method that requires enormous data to
    achieve high performance. Although self-attention based models can bring some
    interpretability among words for DNNs, it is not enough comparing with traditional
    models to explain why and how it works well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。RNN 按顺序计算，不能并行计算。RNN 的缺点使其在当前模型趋向于更深和更多参数的趋势下更具挑战性。CNN 通过卷积核从文本向量中提取特征。卷积核捕捉到的特征数量与其大小有关。CNN
    足够深，从理论上讲，可以捕捉长距离的特征。由于深层网络参数优化方法不足以及池化层导致的位置信息丢失，深层并未带来显著的改进。与 RNN 相比，CNN 具有并行计算能力，并且对于改进版的
    CNN 能有效保留位置信息。但它在长距离特征捕捉方面仍较弱。GNN 为文本构建图形。当设计出有效的图结构时，学习到的表示可以更好地捕捉结构信息。Transformer
    将输入文本视为完全连接的图形，边缘上有注意力分数权重。它具备并行计算能力，并通过自注意力高效提取不同单词之间的特征，解决了短期记忆问题。然而，Transformer
    中的注意力机制计算量大，特别是在处理长序列时。最近提出了一些改进的模型 ([DBLP:conf/iclr/LanCGGSS20,](#bib.bib146)
    ; [DBLP:conf/nips/ZafrirBIW19,](#bib.bib225) ) 以应对 Transformer 的计算复杂性。总体而言，Transformer
    是文本分类的更好选择。深度学习由多个隐藏层组成，具有更高的复杂性，可以在非结构化数据上进行训练。深度学习可以学习语言特征，并掌握基于词汇和向量的更高层次和更抽象的语言特征。深度学习架构可以直接从输入中学习特征表示，而无需过多的人工干预和先验知识。然而，深度学习技术是一种数据驱动的方法，需要大量数据才能实现高性能。尽管基于自注意力的模型可以为
    DNN 带来一些词汇间的可解释性，但与传统模型相比，仍不足以解释其良好工作的原因和方式。
- en: 3\. Datasets and Evaluation Metrics
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 数据集和评估指标
- en: 'Table 2\. Summary statistics for the datasets. C: Number of target classes.
    L: Average sentence length. N: Dataset size.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2\. 数据集的汇总统计。C: 目标类别数。L: 平均句子长度。N: 数据集大小。'
- en: '| Datasets | #C | #L | #N | Language | Related Papers | Sources | Applications
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | #C | #L | #N | 语言 | 相关论文 | 来源 | 应用 |'
- en: '| MR | 2 | 20 | 10,662 | English | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ;
    [DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/aaai/YaoM019,](#bib.bib155) ) | ([movie-review-data,](#bib.bib226)
    ) | SA |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| MR | 2 | 20 | 10,662 | 英语 | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ; [DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ) | ([movie-review-data,](#bib.bib226) ) | SA |'
- en: '| SST-1 | 5 | 18 | 11,855 | English | ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ) ([DBLP:conf/acl/TaiSM15,](#bib.bib1)
    ; [DBLP:conf/icml/ZhuSG15,](#bib.bib2) )([DBLP:conf/emnlp/0001DL16,](#bib.bib112)
    ) | ([sentiment,](#bib.bib227) ) | SA |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| SST-1 | 5 | 18 | 11,855 | 英语 | ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ) ([DBLP:conf/acl/TaiSM15,](#bib.bib1)
    ; [DBLP:conf/icml/ZhuSG15,](#bib.bib2) )([DBLP:conf/emnlp/0001DL16,](#bib.bib112)
    ) | ([sentiment,](#bib.bib227) ) | SA |'
- en: '| SST-2 | 2 | 19 | 9,613 | English | ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6)
    ) ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ; [DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ) | ([socher-etal-2013-recursive,](#bib.bib228) ) | SA |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 2 | 19 | 9,613 | 英语 | ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6)
    ) ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ; [DBLP:conf/naacl/DevlinCLT19,](#bib.bib19)
    ) | ([socher-etal-2013-recursive,](#bib.bib228) ) | SA |'
- en: '| MPQA | 2 | 3 | 10,606 | English | ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/iclr/ShenZL0Z18,](#bib.bib120)
    ) | ([mpqa,](#bib.bib229) ) | SA |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| MPQA | 2 | 3 | 10,606 | 英语 | ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60)
    ; [DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/iclr/ShenZL0Z18,](#bib.bib120)
    ) | ([mpqa,](#bib.bib229) ) | SA |'
- en: '| IMDB | 2 | 294 | 50,000 | English | ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69)
    )([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) ([DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6)
    ) ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85)
    ) | ([DBLP:conf/kdd/DiaoQWSJW14,](#bib.bib230) ) | SA |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| IMDB | 2 | 294 | 50,000 | 英语 | ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69) )([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108)
    ) ([DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79)
    ) ([DBLP:conf/iclr/MiyatoDG17,](#bib.bib85) ) | ([DBLP:conf/kdd/DiaoQWSJW14,](#bib.bib230)
    ) | SA |'
- en: '| Yelp.P | 2 | 153 | 598,000 | English | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | ([DBLP:conf/emnlp/TangQL15,](#bib.bib231)
    ) | SA |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Yelp.P | 2 | 153 | 598,000 | 英语 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | ([DBLP:conf/emnlp/TangQL15,](#bib.bib231)
    ) | SA |'
- en: '| Yelp.F | 5 | 155 | 700,000 | English | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    ) | ([DBLP:conf/emnlp/TangQL15,](#bib.bib231) ) | SA |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Yelp.F | 5 | 155 | 700,000 | 英语 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    ) | ([DBLP:conf/emnlp/TangQL15,](#bib.bib231) ) | SA |'
- en: '| Amz.P | 2 | 91 | 4,000,000 | English | ([DBLP:conf/nips/YouZWDMZ19,](#bib.bib122)
    ; [DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | ([amazon-review,](#bib.bib232) )
    | SA |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Amz.P | 2 | 91 | 4,000,000 | 英语 | ([DBLP:conf/nips/YouZWDMZ19,](#bib.bib122)
    ; [DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | ([amazon-review,](#bib.bib232) )
    | SA |'
- en: '| Amz.F | 5 | 93 | 3,650,000 | English | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ; [DBLP:conf/nips/YouZWDMZ19,](#bib.bib122)
    ) | ([amazon-review,](#bib.bib232) ) | SA |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Amz.F | 5 | 93 | 3,650,000 | 英语 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ; [DBLP:conf/nips/YouZWDMZ19,](#bib.bib122)
    ) | ([amazon-review,](#bib.bib232) ) | SA |'
- en: '| Twitter | 3 | 19 | 11,209 | English | ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    )([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | ([task2,](#bib.bib233) ) | SA
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Twitter | 3 | 19 | 11,209 | 英语 | ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    )([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | ([task2,](#bib.bib233) ) | SA
    |'
- en: '| NLP&CC 2013 | 2 | - | 115,606 | Multi-language | ([DBLP:conf/emnlp/ZhouWX16,](#bib.bib110)
    ) | ([tcci.ccf.org.cn,](#bib.bib111) ) | SA |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| NLP&CC 2013 | 2 | - | 115,606 | 多语言 | ([DBLP:conf/emnlp/ZhouWX16,](#bib.bib110)
    ) | ([tcci.ccf.org.cn,](#bib.bib111) ) | SA |'
- en: '| 20NG | 20 | 221 | 18,846 | English | ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72)
    ; [DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ; [DBLP:conf/iclr/BaoWCB20,](#bib.bib106)
    ; [DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157)
    ) | ([datasets-for-single-label-textcategorization,](#bib.bib34) ) | NC |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 20NG | 20 | 221 | 18,846 | 英语 | ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72) ;
    [DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ; [DBLP:conf/iclr/BaoWCB20,](#bib.bib106)
    ; [DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157)
    ) | ([datasets-for-single-label-textcategorization,](#bib.bib34) ) | NC |'
- en: '| AG News | 4 | 45/7 | 127,600 | English | ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    ; [DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | ([AG-News,](#bib.bib234) ) |
    NC |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| AG News | 4 | 45/7 | 127,600 | 英语 | ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98)
    ; [DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | ([AG-News,](#bib.bib234) ) |
    NC |'
- en: '| R8 | 8 | 66 | 7,674 | English | ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ;
    [DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ) ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) | ([textmining,](#bib.bib235) ) | NC |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| R8 | 8 | 66 | 7,674 | 英语 | ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157)
    ) ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159) ) | ([textmining,](#bib.bib235)
    ) | NC |'
- en: '| R52 | 52 | 70 | 9,100 | English | ([DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ) ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) | ([textmining,](#bib.bib235) ) | NC |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| R52 | 52 | 70 | 9,100 | 英语 | ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157)
    ) ([DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159) ) | ([textmining,](#bib.bib235)
    ) | NC |'
- en: '| Sogou | 6 | 578 | 510,000 | Chinese | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ) | ([DBLP:conf/www/WangZMR08,](#bib.bib236) ) | NC |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Sogou | 6 | 578 | 510,000 | 中文 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ) | ([DBLP:conf/www/WangZMR08,](#bib.bib236) ) | NC |'
- en: '| Newsgroup | 20 | - | 18,846 | English | ([DBLP:conf/cikm/LiLCOL18,](#bib.bib237)
    ) | ([DBLP:conf/cikm/LiLCOL18,](#bib.bib237) ) | NC |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Newsgroup | 20 | - | 18,846 | 英语 | ([DBLP:conf/cikm/LiLCOL18,](#bib.bib237)
    ) | ([DBLP:conf/cikm/LiLCOL18,](#bib.bib237) ) | NC |'
- en: '| DBPedia | 14 | 55 | 630,000 | English | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ; [DBLP:conf/iclr/MiyatoDG17,](#bib.bib85)
    ; [DBLP:conf/cncl/SunQXH19,](#bib.bib136) ) | ([DBLP:journals/semweb/LehmannIJJKMHMK15,](#bib.bib238)
    ) | TL |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| DBPedia | 14 | 55 | 630,000 | 英语 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ; [DBLP:conf/iclr/MiyatoDG17,](#bib.bib85)
    ; [DBLP:conf/cncl/SunQXH19,](#bib.bib136) ) | ([DBLP:journals/semweb/LehmannIJJKMHMK15,](#bib.bib238)
    ) | TL |'
- en: '| Ohsumed | 23 | 136 | 7,400 | English | ([DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ; [DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) | ([ohsumed,](#bib.bib239) ) | TL |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Ohsumed | 23 | 136 | 7,400 | 英语 | ([DBLP:conf/aaai/YaoM019,](#bib.bib155)
    ; [DBLP:conf/icml/WuSZFYW19,](#bib.bib157) ; [DBLP:conf/emnlp/HuangMLZW19,](#bib.bib159)
    ) | ([ohsumed,](#bib.bib239) ) | TL |'
- en: '| YahooA | 10 | 112 | 1,460,000 | English | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ) | TL |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| YahooA | 10 | 112 | 1,460,000 | 英语 | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ; [DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ) | TL |'
- en: '| EUR-Lex | 3,956 | 1,239 | 19,314 | English | ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96)
    ) ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ; [peng2019hierarchical,](#bib.bib161)
    ) ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ([eurlex,](#bib.bib240) )
    | TL |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| EUR-Lex | 3,956 | 1,239 | 19,314 | 英语 | ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96)
    ) ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ; [peng2019hierarchical,](#bib.bib161)
    ) ([DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ([eurlex,](#bib.bib240) )
    | TL |'
- en: '| Amazon670K | 670 | 244 | 643,474 | English | ([DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103)
    ; [DBLP:conf/nips/YouZWDMZ19,](#bib.bib122) ) | ([XMLRepository,](#bib.bib241)
    ) | TL |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Amazon670K | 670 | 244 | 643,474 | 英语 | ([DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103)
    ; [DBLP:conf/nips/YouZWDMZ19,](#bib.bib122) ) | ([XMLRepository,](#bib.bib241)
    ) | TL |'
- en: '| Google news | 152 | 6 | 11,109 | English | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ; [DBLP:journals/apin/ChenGL20,](#bib.bib3) ; [9152157,](#bib.bib243) ) | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ) | TL |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Google news | 152 | 6 | 11,109 | 英语 | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ; [DBLP:journals/apin/ChenGL20,](#bib.bib3) ; [9152157,](#bib.bib243) ) | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ) | TL |'
- en: '| TweetSet 2011-2012 | 89 | - | 2,472 | English | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ; [9152157,](#bib.bib243) ) | ([DBLP:conf/kdd/YinW14,](#bib.bib242) ) | TL |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| TweetSet 2011-2012 | 89 | - | 2,472 | 英语 | ([DBLP:conf/kdd/YinW14,](#bib.bib242)
    ; [9152157,](#bib.bib243) ) | ([DBLP:conf/kdd/YinW14,](#bib.bib242) ) | TL |'
- en: '| TweetSet 2011-2015 | 269 | 8 | 30,322 | English | ([DBLP:journals/isci/ChenGL19,](#bib.bib4)
    ; [DBLP:journals/apin/ChenGL20,](#bib.bib3) ) | ([DBLP:journals/isci/ChenGL19,](#bib.bib4)
    ) | TL |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| TweetSet 2011-2015 | 269 | 8 | 30,322 | 英语 | ([DBLP:journals/isci/ChenGL19,](#bib.bib4)
    ; [DBLP:journals/apin/ChenGL20,](#bib.bib3) ) | ([DBLP:journals/isci/ChenGL19,](#bib.bib4)
    ) | TL |'
- en: '| Bing | 4 | 20 | 34,871 | English | ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100)
    ) | ([DBLP:conf/cikm/WangWLW14,](#bib.bib244) ) | TL |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Bing | 4 | 20 | 34,871 | 英语 | ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100)
    ) | ([DBLP:conf/cikm/WangWLW14,](#bib.bib244) ) | TL |'
- en: '| Fudan | 20 | 2981 | 18,655 | Chinese | ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72)
    ) | ([datatang,](#bib.bib245) ) | TL |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Fudan | 20 | 2981 | 18,655 | 中文 | ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72)
    ) | ([datatang,](#bib.bib245) ) | TL |'
- en: '| SQuAD | - | 5,000 | 5,570 | English | ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118)
    ; [DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118) ; [DBLP:journals/corr/abs-1907-11692,](#bib.bib140)
    ; [DBLP:conf/iclr/LanCGGSS20,](#bib.bib146) ) | ([DBLP:conf/emnlp/RajpurkarZLL16,](#bib.bib246)
    ) | QA |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | - | 5,000 | 5,570 | 英语 | ([DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118)
    ; [DBLP:conf/naacl/PetersNIGCLZ18,](#bib.bib118) ; [DBLP:journals/corr/abs-1907-11692,](#bib.bib140)
    ; [DBLP:conf/iclr/LanCGGSS20,](#bib.bib146) ) | ([DBLP:conf/emnlp/RajpurkarZLL16,](#bib.bib246)
    ) | QA |'
- en: '| TREC-QA | - | 1,162 | 68 | English | ([DBLP:journals/corr/SantosTXZ16,](#bib.bib197)
    ) | ([DBLP:conf/naacl/YaoDCC13,](#bib.bib247) ) | QA |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| TREC-QA | - | 1,162 | 68 | 英语 | ([DBLP:journals/corr/SantosTXZ16,](#bib.bib197)
    ) | ([DBLP:conf/naacl/YaoDCC13,](#bib.bib247) ) | QA |'
- en: '| TREC | 6 | 10 | 5,952 | English | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ;
    [DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6)
    ) ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | ([QC,](#bib.bib248) ) | QA |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 6 | 10 | 5,952 | 英语 | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5)
    ; [DBLP:conf/emnlp/LiuQCWH15,](#bib.bib6) ) ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100)
    ) | ([QC,](#bib.bib248) ) | QA |'
- en: '| WikiQA | - | 873 | 243 | English | ([DBLP:conf/emnlp/YangYM15,](#bib.bib249)
    ; [DBLP:journals/corr/SantosTXZ16,](#bib.bib197) ) | ([DBLP:conf/emnlp/YangYM15,](#bib.bib249)
    ) | QA |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| WikiQA | - | 873 | 243 | 英语 | ([DBLP:conf/emnlp/YangYM15,](#bib.bib249) ;
    [DBLP:journals/corr/SantosTXZ16,](#bib.bib197) ) | ([DBLP:conf/emnlp/YangYM15,](#bib.bib249)
    ) | QA |'
- en: '| Subj | 2 | 23 | 10,000 | English | ([DBLP:conf/emnlp/Kim14,](#bib.bib17)
    ; [DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ) | ([pang-lee-2004-sentimental,](#bib.bib250) ) | QA |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Subj | 2 | 23 | 10,000 | 英语 | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/ijcai/LiuQH16,](#bib.bib79)
    ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ) | ([pang-lee-2004-sentimental,](#bib.bib250)
    ) | QA |'
- en: '| CR | 2 | 19 | 3,775 | English | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ) | ([DBLP:conf/kdd/HuL04,](#bib.bib251) ) | QA |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| CR | 2 | 19 | 3,775 | 英语 | ([DBLP:conf/emnlp/Kim14,](#bib.bib17) ; [DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ) | ([DBLP:conf/kdd/HuL04,](#bib.bib251) ) | QA |'
- en: '| Reuters | 90 | 168 | 10,788 | English | ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([nlp-reuters,](#bib.bib252) )
    | ML |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Reuters | 90 | 168 | 10,788 | 英语 | ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([nlp-reuters,](#bib.bib252) )
    | ML |'
- en: '| Reuters10 | 10 | 168 | 9,979 | English | ([DBLP:journals/ijon/KimJPC20,](#bib.bib253)
    ) | ([nlp-reuters10,](#bib.bib254) ) | ML |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Reuters10 | 10 | 168 | 9,979 | 英语 | ([DBLP:journals/ijon/KimJPC20,](#bib.bib253)
    ) | ([nlp-reuters10,](#bib.bib254) ) | ML |'
- en: '| RCV1 | 103 | 240 | 807,595 | English | ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75)
    ; [DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103) ; [DBLP:conf/www/PengLHLBWS018,](#bib.bib153)
    ; [DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255)
    ) | ML |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| RCV1 | 103 | 240 | 807,595 | 英语 | ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75)
    ; [DBLP:conf/emnlp/ShimuraLF18,](#bib.bib103) ; [DBLP:conf/www/PengLHLBWS018,](#bib.bib153)
    ; [DBLP:conf/acl/ChalkidisFMA19,](#bib.bib134) ) | ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255)
    ) | ML |'
- en: '| RCV1-V2 | 103 | 124 | 804,414 | English | ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([lyrl2004_rcv1v2_README.htm,](#bib.bib256)
    ) | ML |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| RCV1-V2 | 103 | 124 | 804,414 | 英语 | ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([lyrl2004_rcv1v2_README.htm,](#bib.bib256)
    ) | ML |'
- en: '| AAPD | 54 | 163 | 55,840 | English | ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([SGM,](#bib.bib117) ) | ML |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| AAPD | 54 | 163 | 55,840 | 英语 | ([DBLP:conf/coling/YangSLMWW18,](#bib.bib116)
    ; [DBLP:conf/icaart/PalSS20,](#bib.bib166) ) | ([SGM,](#bib.bib117) ) | ML |'
- en: 3.1\. Datasets
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 数据集
- en: 'The availability of labeled datasets for text classification has become the
    main driving force behind the fast advancement of this research field. In this
    section, we summarize the characteristics of these datasets in terms of domains
    and give an overview in Table  [2](#S3.T2 "Table 2 ‣ 3\. Datasets and Evaluation
    Metrics ‣ A Survey on Text Classification: From Traditional to Deep Learning"),
    including the number of categories, average sentence length, the size of each
    dataset, related papers, data sources to access and applications.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 标注数据集的可用性已成为推动文本分类研究领域快速发展的主要动力。在本节中，我们总结了这些数据集在领域方面的特征，并在表格 [2](#S3.T2 "表 2
    ‣ 3\. 数据集和评估指标 ‣ 文本分类调查：从传统到深度学习") 中提供了概述，包括类别数量、平均句子长度、每个数据集的大小、相关文献、数据源访问和应用情况。
- en: 3.1.1\. Sentiment Analysis (SA)
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 情感分析 (SA)
- en: SA is the process of analyzing and reasoning the subjective text within emotional
    color. It is crucial to get information on whether it supports a particular point
    of view from the text that is distinct from the traditional text classification
    that analyzes the objective content of the text. SA can be binary or multi-class.
    Binary SA is to divide the text into two categories, including positive and negative.
    Multi-class SA classifies text to multi-level or fine-grained labels. The SA datasets
    include Movie Review (MR) ([DBLP:conf/acl/PangL05,](#bib.bib257) ; [movie-review-data,](#bib.bib226)
    ), Stanford Sentiment Treebank (SST) ([sentiment,](#bib.bib227) ), Multi-Perspective
    Question Answering (MPQA) ([DBLP:journals/lre/WiebeWC05,](#bib.bib258) ; [mpqa,](#bib.bib229)
    ), IMDB ([DBLP:conf/kdd/DiaoQWSJW14,](#bib.bib230) ), Yelp ([DBLP:conf/emnlp/TangQL15,](#bib.bib231)
    ), Amazon Reviews (AM) ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ), NLP&CC 2013
    ([tcci.ccf.org.cn,](#bib.bib111) ), Subj ([pang-lee-2004-sentimental,](#bib.bib250)
    ), CR ([DBLP:conf/kdd/HuL04,](#bib.bib251) ), SS-Twitter ([DBLP:journals/jasis/ThelwallBP12,](#bib.bib259)
    ), SS-Youtube ([DBLP:journals/jasis/ThelwallBP12,](#bib.bib259) ), SE1604 ([Nakov2016SemEval,](#bib.bib260)
    ) and so on. Here we detail several datasets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: SA 是分析和推理带有情感色彩的主观文本的过程。与传统分析文本客观内容的文本分类不同，SA 的关键在于获取文本是否支持特定观点的信息。SA 可以是二元或多类的。二元
    SA 将文本划分为两类，包括正面和负面。多类 SA 将文本分类为多层次或细粒度标签。SA 数据集包括电影评论 (MR) ([DBLP:conf/acl/PangL05,](#bib.bib257)
    ; [movie-review-data,](#bib.bib226) )，斯坦福情感树库 (SST) ([sentiment,](#bib.bib227)
    )，多视角问答 (MPQA) ([DBLP:journals/lre/WiebeWC05,](#bib.bib258) ; [mpqa,](#bib.bib229)
    )，IMDB ([DBLP:conf/kdd/DiaoQWSJW14,](#bib.bib230) )，Yelp ([DBLP:conf/emnlp/TangQL15,](#bib.bib231)
    )，亚马逊评论 (AM) ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) )，NLP&CC 2013 ([tcci.ccf.org.cn,](#bib.bib111)
    )，Subj ([pang-lee-2004-sentimental,](#bib.bib250) )，CR ([DBLP:conf/kdd/HuL04,](#bib.bib251)
    )，SS-Twitter ([DBLP:journals/jasis/ThelwallBP12,](#bib.bib259) )，SS-Youtube ([DBLP:journals/jasis/ThelwallBP12,](#bib.bib259)
    )，SE1604 ([Nakov2016SemEval,](#bib.bib260) ) 等。这里我们详细介绍了几个数据集。
- en: MR. The MR is a movie review dataset, each of which corresponds to a sentence.
    The corpus has 5,331 positive data and 5,331 negative data. 10-fold cross-validation
    by random splitting is commonly used to test MR.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: MR。MR 是一个电影评论数据集，每条数据对应一个句子。该语料库包含 5,331 条正面数据和 5,331 条负面数据。常用 10 倍交叉验证进行 MR
    测试。
- en: SST. The SST is an extension of MR. It has two categories. SST-1 with fine-grained
    labels with five classes. It has 8,544 training texts and 2,210 test texts, respectively.
    Furthermore, SST-2 has 9,613 texts with binary labels being partitioned into 6,920
    training texts, 872 development texts, and 1,821 testing texts.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: SST。SST 是 MR 的扩展，分为两个类别。SST-1 有五个细粒度标签，包含 8,544 条训练文本和 2,210 条测试文本。进一步地，SST-2
    具有 9,613 条文本，二元标签被划分为 6,920 条训练文本、872 条开发文本和 1,821 条测试文本。
- en: MPQA. The MPQA is an opinion dataset. It has two class labels and also an MPQA
    dataset of opinion polarity detection sub-tasks. MPQA includes 10,606 sentences
    extracted from news articles from various news sources. It should be noted that
    it contains 3,311 positive texts and 7,293 negative texts without labels of each
    text.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: MPQA。MPQA 是一个意见数据集。它有两个类别标签，并且还有一个 MPQA 数据集用于意见极性检测子任务。MPQA 包含 10,606 条从各种新闻来源提取的新闻文章句子。需要注意的是，它包含
    3,311 条正面文本和 7,293 条负面文本，但没有每条文本的标签。
- en: IMDB reviews. The IMDB review is developed for binary sentiment classification
    of film reviews with the same amount in each class. It can be separated into training
    and test groups on average, by 25,000 comments per group.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: IMDB 评论。IMDB 评论用于电影评论的二元情感分类，每个类别的数量相等。可以将其平均分为训练组和测试组，每组 25,000 条评论。
- en: Yelp reviews. The Yelp review is summarized from the Yelp Dataset Challenges
    in 2013, 2014, and 2015\. This dataset has two categories. Yelp-2 of these were
    used for negative and positive emotion classification tasks, including 560,000
    training texts and 38,000 test texts. Yelp-5 is used to detect fine-grained affective
    labels with 650,000 training and 50,000 test texts in all classes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Yelp 评论。Yelp 评论数据集来源于 2013、2014 和 2015 年的 Yelp 数据集挑战。该数据集有两个类别。Yelp-2 用于负面和正面情感分类任务，包括
    560,000 条训练文本和 38,000 条测试文本。Yelp-5 用于检测细粒度情感标签，包含 650,000 条训练文本和 50,000 条测试文本。
- en: AM. The AM is a popular corpus formed by collecting Amazon website product reviews
    ([amazon-review,](#bib.bib232) ). This dataset has two categories. The Amazon-2
    with two classes includes 3,600,000 training sets and 400,000 testing sets. Amazon-5,
    with five classes, includes 3,000,000 and 650,000 comments for training and testing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: AM。AM是通过收集亚马逊网站产品评论形成的一个常用语料库。该数据集有两个类别。包括360万训练集和40万测试集的Amazon-2。Amazon-5包括300万和65万条评论用于训练和测试。
- en: 3.1.2\. News Classification (NC)
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 新闻分类（NC）
- en: 'News content is one of the most crucial information sources which has a critical
    influence on people. The NC system facilitates users to get vital knowledge in
    real-time. News classification applications mainly encompass: recognizing news
    topics and recommending related news according to user interest. The news classification
    datasets include 20 Newsgroups (20NG) ([datasets-for-single-label-textcategorization,](#bib.bib34)
    ), AG News (AG) ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ; [AG-News,](#bib.bib234)
    ), R8 ([textmining,](#bib.bib235) ), R52 ([textmining,](#bib.bib235) ), Sogou
    News (Sogou) ([DBLP:conf/cncl/SunQXH19,](#bib.bib136) ) and so on. Here we detail
    several datasets.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻内容是人们最关键的信息来源之一，对人们有着至关重要的影响。NC系统帮助用户实时获取重要知识。新闻分类应用主要包括：识别新闻主题和根据用户兴趣推荐相关新闻。新闻分类数据集包括20个新闻组（20NG）、AG新闻（AG）、R8、R52、搜狗新闻（Sogou）等。这里我们详细介绍几个数据集。
- en: 20NG. The 20NG is a newsgroup text dataset. It has 20 categories with the same
    number of each category and includes 18,846 texts.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 20NG。20NG是一个新闻组文本数据集。它有20个同样数量的类别，包括18,846条文本。
- en: AG. The AG News is a search engine for news from academia, choosing the four
    largest classes. It uses the title and description fields of each news. AG contains
    120,000 texts for training and 7,600 texts for testing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: AG。AG新闻是一个学术新闻搜索引擎，选择了四个最大的类别。它使用每条新闻的标题和描述字段。AG包含120,000条训练文本和7,600条测试文本。
- en: R8 and R52. R8 and R52 are two subsets which are the subset of Reuters ([nlp-reuters,](#bib.bib252)
    ). R8 has 8 categories, divided into 2,189 test files and 5,485 training courses.
    R52 has 52 categories, split into 6,532 training files and 2,568 test files.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: R8和R52。R8和R52是路透社的子集。R8有8个类别，分为2,189个测试文件和5,485个训练课程。R52有52个类别，分为6,532个训练文件和2,568个测试文件。
- en: Sogou. The Sogou combines two datasets, including SogouCA and SogouCS news sets.
    The label of each text is the domain names in the URL.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 搜狗。搜狗包含两个数据集，包括SogouCA和SogouCS新闻集。每个文本的标签是URL中的域名。
- en: 3.1.3\. Topic Labeling (TL)
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 主题标记（TL）
- en: The topic analysis attempts to get the meaning of the text by defining the sophisticated
    text theme. The topic labeling is one of the essential components of the topic
    analysis technique, intending to assign one or more subjects for each document
    to simplify the topic analysis. The topic labeling datasets include DBPedia ([DBLP:journals/semweb/LehmannIJJKMHMK15,](#bib.bib238)
    ), Ohsumed ([ohsumed,](#bib.bib239) ), Yahoo answers (YahooA) ([DBLP:conf/nips/ZhangZL15,](#bib.bib93)
    ), EUR-Lex ([eurlex,](#bib.bib240) ), Amazon670K ([XMLRepository,](#bib.bib241)
    ), Bing ([DBLP:conf/cikm/WangWLW14,](#bib.bib244) ), Fudan ([datatang,](#bib.bib245)
    ), and PubMed ([DBLP:journals/biodb/Lu11,](#bib.bib261) ). Here we detail several
    datasets.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 主题分析尝试通过定义复杂的文本主题来理解文本的含义。主题标记是主题分析技术的重要组成部分之一，旨在为每个文档分配一个或多个主题以简化主题分析。主题标记数据集包括DBPedia、Ohsumed、Yahoo答案（YahooA）、EUR-Lex、Amazon670K、Bing、Fudan、PubMed等。这里我们详细介绍几个数据集。
- en: DBpedia. The DBpedia is a large-scale multi-lingual knowledge base generated
    using Wikipedia’s most ordinarily used infoboxes. It publishes DBpedia each month,
    adding or deleting classes and properties in every version. DBpedia’s most prevalent
    version has 14 classes and is divided into 560,000 training data and 70,000 test
    data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: DBpedia。DBpedia是使用Wikipedia最常用的信息框生成的大规模多语言知识库。它每个月发布一次DBpedia，每个版本都添加或删除类别和属性。DBpedia最普遍的版本有14个类别，分为56万训练数据和7万测试数据。
- en: Ohsumed. The Ohsumed belongs to the MEDLINE database. It includes 7,400 texts
    and has 23 cardiovascular disease categories. All texts are medical abstracts
    and are labeled into one or more classes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Ohsumed。Ohsumed 属于 MEDLINE 数据库。它包含 7,400 篇文献，并有 23 个心血管疾病类别。所有文献都是医学摘要，并被标注为一个或多个类别。
- en: YahooA. The YahooA is a topic labeling task with 10 classes. It includes 140,000
    training data and 5,000 test data. All text contains three elements, being question
    titles, question contexts, and best answers, respectively.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: YahooA。YahooA 是一个具有 10 个类别的主题标注任务。它包括 140,000 条训练数据和 5,000 条测试数据。所有文本包含三个元素，分别是问题标题、问题背景和最佳答案。
- en: 3.1.4\. Question Answering (QA)
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 问答系统（QA）
- en: 'The QA task can be divided into two types: the extractive QA and the generative
    QA. The extractive QA gives multiple candidate answers for each question to choose
    which one is the right answer. Thus, the text classification models can be used
    for the extractive QA task. The QA discussed in this paper is all extractive QA.
    The QA system can apply the text classification model to recognize the correct
    answer and set others as candidates. The question answering datasets include Stanford
    Question Answering Dataset (SQuAD) ([DBLP:conf/emnlp/RajpurkarZLL16,](#bib.bib246)
    ), TREC-QA ([QC,](#bib.bib248) ), WikiQA ([DBLP:conf/emnlp/YangYM15,](#bib.bib249)
    ), Subj ([pang-lee-2004-sentimental,](#bib.bib250) ), CR ([DBLP:conf/kdd/HuL04,](#bib.bib251)
    ), MS MARCO ([DBLP:conf/nips/NguyenRSGTMD16,](#bib.bib262) ), and Quora ([QuestionPairs,](#bib.bib263)
    ). Here we detail several datasets.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: QA 任务可以分为两种类型：抽取式 QA 和生成式 QA。抽取式 QA 为每个问题提供多个候选答案，以选择哪个是正确答案。因此，文本分类模型可以用于抽取式
    QA 任务。本文讨论的 QA 全部是抽取式 QA。QA 系统可以应用文本分类模型来识别正确答案，并将其他答案设为候选答案。问答数据集包括斯坦福问答数据集（SQuAD）
    ([DBLP:conf/emnlp/RajpurkarZLL16,](#bib.bib246)）、TREC-QA ([QC,](#bib.bib248)）、WikiQA
    ([DBLP:conf/emnlp/YangYM15,](#bib.bib249)）、Subj ([pang-lee-2004-sentimental,](#bib.bib250)）、CR
    ([DBLP:conf/kdd/HuL04,](#bib.bib251)）、MS MARCO ([DBLP:conf/nips/NguyenRSGTMD16,](#bib.bib262)）和
    Quora ([QuestionPairs,](#bib.bib263)）。这里我们详细介绍几个数据集。
- en: SQuAD. The SQuAD is a set of question and answer pairs obtained from Wikipedia
    articles. The SQuAD has two categories. SQuAD1.1 contains 536 pairs of 107,785
    Q&A items. SQuAD2.0 combines 100,000 questions in SQuAD1.1 with more than 50,000
    unanswerable questions that crowd workers face in a form similar to answerable
    questions ([DBLP:conf/acl/RajpurkarJL18,](#bib.bib264) ).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: SQuAD。SQuAD 是一组从维基百科文章中获得的问题和答案对。SQuAD 有两个类别。SQuAD1.1 包含 536 对 107,785 条问答项。SQuAD2.0
    将 SQuAD1.1 中的 100,000 个问题与超过 50,000 个不可回答的问题结合，这些问题的形式类似于可回答问题 ([DBLP:conf/acl/RajpurkarJL18,](#bib.bib264)）。
- en: TREC-QA. The TREC-QA includes 5,452 training texts and 500 testing texts. It
    has two versions. TREC-6 contains 6 categories, and TREC-50 has 50 categories.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: TREC-QA。TREC-QA 包含 5,452 条训练文本和 500 条测试文本。它有两个版本。TREC-6 包含 6 个类别，而 TREC-50 有
    50 个类别。
- en: WikiQA. The WikiQA dataset includes questions with no correct answer, which
    needs to evaluate the answer.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: WikiQA。WikiQA 数据集包括没有正确答案的问题，这些问题需要评估答案。
- en: MS MARCO. The MS MARCO contains questions and answers. The questions and part
    of the answers are sampled from actual web texts by the Bing search engine. Others
    are generative. It is used for developing generative QA systems released by Microsoft.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: MS MARCO。MS MARCO 包含问题和答案。问题和部分答案是由 Bing 搜索引擎从实际的网络文本中抽样得到的。其他则是生成的。它用于开发微软发布的生成式
    QA 系统。
- en: 3.1.5\. Natural Language Inference (NLI)
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5\. 自然语言推理（NLI）
- en: NLI is used to predict whether the meaning of one text can be deduced from another.
    Paraphrasing is a generalized form of NLI. It uses the task of measuring the semantic
    similarity of sentence pairs to decide whether one sentence is the interpretation
    of another. The NLI datasets include Stanford Natural Language Inference (SNLI)
    ([DBLP:conf/emnlp/BowmanAPM15,](#bib.bib181) ), Multi-Genre Natural Language Inference
    (MNLI) ([DBLP:conf/naacl/WilliamsNB18,](#bib.bib265) ), Sentences Involving Compositional
    Knowledge (SICK) ([DBLP:conf/semeval/MarelliBBBMZ14,](#bib.bib266) ), Microsoft
    Research Paraphrase (MSRP) ([DBLP:conf/coling/DolanQB04,](#bib.bib267) ), Semantic
    Textual Similarity (STS) ([DBLP:journals/corr/abs-1708-00055,](#bib.bib268) ),
    Recognising Textual Entailment (RTE) ([DBLP:conf/mlcw/DaganGM05,](#bib.bib269)
    ), SciTail ([DBLP:conf/aaai/KhotSC18,](#bib.bib270) ), etc. Here we detail several
    of the primary datasets.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: NLI用于预测一个文本的含义是否可以从另一个文本中推断出来。改写是一种广义的NLI形式。它使用句子对的语义相似性任务来决定一个句子是否是另一个句子的解释。NLI数据集包括Stanford
    Natural Language Inference (SNLI) ([DBLP:conf/emnlp/BowmanAPM15,](#bib.bib181))、Multi-Genre
    Natural Language Inference (MNLI) ([DBLP:conf/naacl/WilliamsNB18,](#bib.bib265))、Sentences
    Involving Compositional Knowledge (SICK) ([DBLP:conf/semeval/MarelliBBBMZ14,](#bib.bib266))、Microsoft
    Research Paraphrase (MSRP) ([DBLP:conf/coling/DolanQB04,](#bib.bib267))、Semantic
    Textual Similarity (STS) ([DBLP:journals/corr/abs-1708-00055,](#bib.bib268))、Recognising
    Textual Entailment (RTE) ([DBLP:conf/mlcw/DaganGM05,](#bib.bib269))、SciTail ([DBLP:conf/aaai/KhotSC18,](#bib.bib270))等。这里我们详细介绍几个主要的数据集。
- en: 'SNLI. The SNLI is generally applied to NLI tasks. It contains 570,152 human-annotated
    sentence pairs, including training, development, and test sets, which are annotated
    with three categories: neutral, entailment, and contradiction.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: SNLI。SNLI通常应用于NLI任务。它包含570,152个由人类标注的句子对，包括训练集、开发集和测试集，这些句子对被标注为三类：中性、蕴涵和矛盾。
- en: MNLI. The MNLI is an expansion of SNLI, embracing a broader scope of written
    and spoken text genres. It includes 433,000 sentence pairs annotated by textual
    entailment labels.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: MNLI。MNLI是SNLI的扩展，涵盖了更广泛的书面和口语文本类型。它包括433,000个由文本蕴涵标签标注的句子对。
- en: SICK. The SICK contains almost 10,000 English sentence pairs. It consists of
    neutral, entailment and contradictory labels.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: SICK。SICK包含近10,000对英文句子。它包括中性、蕴涵和矛盾标签。
- en: MSRP. The MSRP consists of sentence pairs, usually for the text-similarity task.
    Each pair is annotated by a binary label to discriminate whether they are paraphrases.
    It respectively includes 1,725 training and 4,076 test sets.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: MSRP。MSRP由句子对组成，通常用于文本相似性任务。每对句子由二进制标签标注，用于区分它们是否为同义句。它分别包含1,725个训练集和4,076个测试集。
- en: 3.1.6\. Multi-Label (ML) datasets
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6\. 多标签 (ML) 数据集
- en: In multi-label classification, an instance has multiple labels, and each label
    can only take one of the multiple classes. There are many datasets based on multi-label
    text classification. It includes Reuters ([nlp-reuters,](#bib.bib252) ), Reuters
    Corpus Volume I (RCV1) ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255) ), RCV1-2K
    ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255) ), Arxiv Academic Paper Dataset
    (AAPD) ([SGM,](#bib.bib117) ), Patent, Web of Science (WOS-11967) ([DBLP:conf/icmla/KowsariBHMGB17,](#bib.bib271)
    ), AmazonCat-13K ([b1FRNnCLFL,](#bib.bib272) ), BlurbGenreCollection (BGC) ([blurb-genre-collection,](#bib.bib273)
    ), etc. Here we detail several datasets.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在多标签分类中，一个实例有多个标签，每个标签只能属于多个类别之一。有许多基于多标签文本分类的数据集。包括Reuters ([nlp-reuters,](#bib.bib252))、Reuters
    Corpus Volume I (RCV1) ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255))、RCV1-2K
    ([DBLP:journals/jmlr/LewisYRL04,](#bib.bib255))、Arxiv Academic Paper Dataset (AAPD)
    ([SGM,](#bib.bib117))、Patent、Web of Science (WOS-11967) ([DBLP:conf/icmla/KowsariBHMGB17,](#bib.bib271))、AmazonCat-13K
    ([b1FRNnCLFL,](#bib.bib272))、BlurbGenreCollection (BGC) ([blurb-genre-collection,](#bib.bib273))等。这里我们详细介绍几个数据集。
- en: Reuters. The Reuters is a popularly used dataset for text classification from
    Reuters financial news services. It has 90 training classes, 7,769 training texts,
    and 3,019 testing texts, containing multiple labels and single labels. There are
    also some Reuters sub-sets of data, such as R8, BR52, RCV1, and RCV1-v2.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Reuters。Reuters是一个广泛使用的文本分类数据集，来源于路透金融新闻服务。它有90个训练类别，7,769个训练文本和3,019个测试文本，包含多个标签和单一标签。此外，还有一些Reuters子集数据，如R8、BR52、RCV1和RCV1-v2。
- en: RCV1 and RCV1-2K. The RCV1 is collected from Reuters News articles from 1996-1997,
    which is human-labeled with 103 categories. It consists of 23,149 training and
    784,446 testing texts, respectively. The RCV1-2K dataset has the same features
    as the RCV1\. However, the label set of RCV1-2K has been expanded with some new
    labels. It contains 2456 labels.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: RCV1 和 RCV1-2K。RCV1 数据集收集自 1996-1997 年的路透新闻文章，人工标注了 103 个类别。它包含 23,149 篇训练文本和
    784,446 篇测试文本。RCV1-2K 数据集与 RCV1 的特征相同，但 RCV1-2K 的标签集扩展了一些新标签，包含 2,456 个标签。
- en: AAPD. The AAPD is a large dataset in the computer science field for the multi-label
    text classification from website ¹¹1https://arxiv.org/. It has 55,840 papers,
    including the abstract and the corresponding subjects with 54 labels in total.
    The aim is to predict the corresponding subjects of each paper according to the
    abstract.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: AAPD。AAPD 是计算机科学领域的大型多标签文本分类数据集，来自网站 ¹¹1https://arxiv.org/。它包含 55,840 篇论文，包括摘要和对应的
    54 个标签的主题。其目的是根据摘要预测每篇论文的对应主题。
- en: Patent Dataset. The Patent Dataset is obtained from USPTO ²²2https://www.uspto.gov/,
    which is a patent system grating U.S. patents containing textual details such
    title and abstract. It contains 100,000 US patents awarded in the real-world with
    multiple hierarchical categories.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 专利数据集。专利数据集来源于 USPTO ²²2https://www.uspto.gov/，这是一个授予包含文本详细信息（如标题和摘要）的美国专利的专利系统。它包含
    100,000 个真实世界中授予的美国专利，具有多个层次的类别。
- en: WOS-11967. The WOS-11967 is crawled from the Web of Science, consisting of abstracts
    of published papers with two labels for each example. It is shallower, but significantly
    broader, with fewer classes in total.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: WOS-11967。WOS-11967 数据集从 Web of Science 中抓取，包含每个示例的论文摘要和两个标签。它较浅，但显著更广泛，总类别较少。
- en: 3.1.7\. Others
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7\. 其他
- en: There are some datasets for other applications, such as SemEval-2010 Task 8
    ([DBLP:conf/naacl/HendrickxKKNSPP09,](#bib.bib274) ), ACE 2003-2004 ([DBLP:conf/lrec/StrasselPPSM08,](#bib.bib275)
    ), TACRED ([DBLP:conf/emnlp/ZhangZCAM17,](#bib.bib276) ), and NYT-10 ([DBLP:conf/pkdd/RiedelYM10,](#bib.bib277)
    ), FewRel ([FewRel,](#bib.bib278) ), Dialog State Tracking Challenge 4 (DSTC 4)
    ([DBLP:conf/iwsds/KimDBWH16,](#bib.bib279) ), ICSI Meeting Recorder Dialog Act
    (MRDA) ([DBLP:conf/icassp/AngLS05,](#bib.bib280) ), and Switchboard Dialog Act
    (SwDA) ([manualarticle,](#bib.bib281) ), and so on.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些用于其他应用的数据集，例如 SemEval-2010 Task 8 ([DBLP:conf/naacl/HendrickxKKNSPP09,](#bib.bib274)
    )，ACE 2003-2004 ([DBLP:conf/lrec/StrasselPPSM08,](#bib.bib275) )，TACRED ([DBLP:conf/emnlp/ZhangZCAM17,](#bib.bib276)
    )，以及 NYT-10 ([DBLP:conf/pkdd/RiedelYM10,](#bib.bib277) )、FewRel ([FewRel,](#bib.bib278)
    )、对话状态追踪挑战赛 4（DSTC 4） ([DBLP:conf/iwsds/KimDBWH16,](#bib.bib279) )、ICSI 会议记录对话行为
    (MRDA) ([DBLP:conf/icassp/AngLS05,](#bib.bib280) )、以及 Switchboard 对话行为 (SwDA)
    ([manualarticle,](#bib.bib281) ) 等。
- en: 3.2\. Evaluation Metrics
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 评估指标
- en: 'In terms of evaluating text classification models, accuracy and F1 score are
    the most used to assess the text classification methods. Later, with the increasing
    difficulty of classification tasks or the existence of some particular tasks,
    the evaluation metrics are improved. For example, evaluation metrics such as $P@K$
    and $Micro\!-\!\!F1$ are used to evaluate multi-label text classification performance,
    and MRR is usually used to estimate the performance of QA tasks. In Table  [3](#S3.T3
    "Table 3 ‣ 3.2\. Evaluation Metrics ‣ 3\. Datasets and Evaluation Metrics ‣ A
    Survey on Text Classification: From Traditional to Deep Learning"), we give the
    notations used in evaluation metrics.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估文本分类模型时，准确率和 F1 分数是最常用来评估文本分类方法的指标。随着分类任务的难度增加或存在一些特定任务，评估指标得到了改进。例如，$P@K$
    和 $Micro\!-\!\!F1$ 等评估指标用于评估多标签文本分类性能，而 MRR 通常用于估计 QA 任务的性能。在表 [3](#S3.T3 "表 3
    ‣ 3.2\. 评估指标 ‣ 3\. 数据集和评估指标 ‣ 关于文本分类的调查：从传统到深度学习") 中，我们给出了评估指标中使用的符号。
- en: Table 3\. The notations used in evaluation metrics.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 评估指标中使用的符号。
- en: '| Notations | Descriptions |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $TP$ | true positive |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| $TP$ | 真正例 |'
- en: '| $FP$ | false positive |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| $FP$ | 假正 |'
- en: '| $TN$ | true negative |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| $TN$ | 真负 |'
- en: '| $FN$ | false negative |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| $FN$ | 假负 |'
- en: '| $TP_{t}$ | true positive of the $t$ th label on a text |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| $TP_{t}$ | 文本中第 $t$ 个标签的真正例 |'
- en: '| $FP_{t}$ | false positive of the $t$ th label on a text |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| $FP_{t}$ | 文本中第 $t$ 个标签的假正 |'
- en: '| $TN_{t}$ | true negative of the $t$ th label on a text |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| $TN_{t}$ | 文本中第 $t$ 个标签的真负 |'
- en: '| $FN_{t}$ | false negative of the $t$ th label on a text |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| $FN_{t}$ | 文本中第 $t$ 个标签的假负 |'
- en: '| $\mathcal{S}$ | label set of all samples |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}$ | 所有样本的标签集 |'
- en: '| ${Q}$ | the number of predicted labels on each text |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ${Q}$ | 每个文本的预测标签数量 |'
- en: 3.2.1\. Single-label metrics
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 单标签指标
- en: Single-label text classification divides the text into one of the most likely
    categories applied in NLP tasks such as QA, SA, and dialogue systems ([DBLP:conf/naacl/LeeD16,](#bib.bib7)
    ). For single-label text classification, one text belongs to just one catalog,
    making it possible not to consider the relations among labels. Here, we introduce
    some evaluation metrics used for single-label text classification tasks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签文本分类将文本划分为最有可能的类别，这些类别应用于诸如QA、SA和对话系统的NLP任务中 ([DBLP:conf/naacl/LeeD16,](#bib.bib7))。对于单标签文本分类，每个文本仅属于一个目录，因此可以不考虑标签之间的关系。在这里，我们介绍一些用于单标签文本分类任务的评估指标。
- en: Accuracy and ErrorRate. The Accuracy and ErrorRate are the fundamental metrics
    for a text classification model. The $Accuracy$ and $ErrorRate$ are respectively
    defined as
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和错误率。准确率和错误率是文本分类模型的基本指标。$Accuracy$ 和 $ErrorRate$ 分别定义为
- en: '| (1) |  | $Accuracy=\frac{(TP+TN)}{N},$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $Accuracy=\frac{(TP+TN)}{N},$ |  |'
- en: '| (2) |  | $\quad ErrorRate=1-Accuracy=\frac{(FP+FN)}{N}.$ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\quad ErrorRate=1-Accuracy=\frac{(FP+FN)}{N}.$ |  |'
- en: Precision, Recall and F1. These are vital metrics utilized for unbalanced test
    sets, regardless of the standard type and error rate. For example, most of the
    test samples have a class label. $F1$ is the harmonic average of $Precision$ and
    $Recall$. $Precision$, $Recall$, and $F1$ as defined
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率、召回率和 $F1$。这些指标对于不平衡的测试集至关重要，无论标准类型和错误率如何。例如，大多数测试样本具有类别标签。$F1$ 是 $Precision$
    和 $Recall$ 的调和平均值。$Precision$、$Recall$ 和 $F1$ 的定义如下
- en: '| (3) |  | $Precision=\frac{TP}{TP+FP},\quad Recall=\frac{TP}{TP+FN},$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $Precision=\frac{TP}{TP+FP},\quad Recall=\frac{TP}{TP+FN},$ |  |'
- en: '| (4) |  | $F1=\frac{2Precision\times Recall}{Precision+Recall}.$ |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $F1=\frac{2Precision\times Recall}{Precision+Recall}.$ |  |'
- en: The desired results will be obtained when the accuracy, $F1$ and $Recall$ value
    reach 1. On the contrary, when the values become 0, the worst result is obtained.
    For the multi-class classification problem, the precision and recall value of
    each class can be calculated separately, and then the performance of the individual
    and whole can be analyzed.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 当准确率、$F1$ 和 $Recall$ 值达到 1 时，结果是理想的。相反，当这些值变为 0 时，结果最差。对于多类别分类问题，可以单独计算每个类别的精确率和召回率，然后分析个体和整体的表现。
- en: Exact Match (EM). The EM ([A1977Maximum,](#bib.bib29) ) is a metric for QA tasks,
    measuring the prediction that matches all the ground-truth answers precisely.
    It is the primary metric utilized on the SQuAD dataset.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 精确匹配（EM）。EM ([A1977Maximum,](#bib.bib29) ) 是一个用于QA任务的指标，衡量预测是否完全匹配所有真实答案。它是SQuAD数据集上使用的主要指标。
- en: Mean Reciprocal Rank (MRR). The MRR ([DBLP:conf/sigir/SeverynM15,](#bib.bib282)
    ) is usually applied for assessing the performance of ranking algorithms on QA
    and Information Retrieval (IR) tasks. $MRR$ is defined as
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 平均倒数排名（MRR）。MRR ([DBLP:conf/sigir/SeverynM15,](#bib.bib282) ) 通常用于评估QA和信息检索（IR）任务中排名算法的表现。$MRR$
    定义为
- en: '| (5) |  | $MRR=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{{rank}(i)},$ |  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $MRR=\frac{1}{Q}\sum_{i=1}^{Q}\frac{1}{{rank}(i)},$ |  |'
- en: where ${rank}(i)$ is the ranking of the ground-truth answer at answer $i$-th.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${rank}(i)$ 是第 $i$ 个答案的真实答案排名。
- en: Hamming-Loss (HL). The HL ([DBLP:journals/ml/SchapireS99,](#bib.bib57) ) assesses
    the score of misclassified instance-label pairs where a related label is omitted
    or an unrelated is predicted.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 汉明损失（HL）。HL ([DBLP:journals/ml/SchapireS99,](#bib.bib57) ) 评估错误分类实例-标签对的得分，其中遗漏了相关标签或预测了不相关标签。
- en: Among these single-label evaluation metrics, the Accuracy is the earliest metric
    that calculates the proportion of the sample size that is predicted correctly
    and is not considered whether the predicted sample is a positive or a negative
    sample. $Precision$ calculates how many of the positive samples are actually positive,
    and the $Recall$ calculates how many of the positive examples in the sample are
    predicted correctly. Furthermore, $F1$ is the harmonic average of them, which
    is the most commonly used evaluation metrics.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些单标签评估指标中，准确率是最早的指标，它计算预测正确的样本比例，不考虑预测样本是正样本还是负样本。$Precision$ 计算正样本中实际正样本的数量，$Recall$
    计算样本中正确预测的正例数量。此外，$F1$ 是它们的调和平均值，是最常用的评估指标。
- en: 3.2.2\. Multi-label metrics
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 多标签指标
- en: Compared with single-label text classification, multi-label text classification
    divides the text into multiple category labels, and the number of category labels
    is variable. These metrics are designed for single label text classification,
    which are not suitable for multi-label tasks. Thus, there are some metrics designed
    for multi-label text classification.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 与单标签文本分类相比，多标签文本分类将文本划分为多个类别标签，并且类别标签的数量是可变的。这些指标是为单标签文本分类设计的，不适用于多标签任务。因此，存在一些专为多标签文本分类设计的指标。
- en: '$Micro\!-\!\!F1$. The $Micro\!-\!\!F1$ ([DBLP:books/daglib/0021593,](#bib.bib283)
    ) is a measure that considers the overall accuracy and recall of all labels. The
    $Micro\!-\!\!F1$ is defined as:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: $Micro\!-\!\!F1$。$Micro\!-\!\!F1$ ([DBLP:books/daglib/0021593,](#bib.bib283)
    ) 是一个考虑所有标签的整体准确率和召回率的度量。$Micro\!-\!\!F1$ 定义为：
- en: '| (6) |  | $Micro\!-\!\!F1=\frac{2{P}_{t}\times R_{t}}{{P}+{R}},$ |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $Micro\!-\!\!F1=\frac{2{P}_{t}\times R_{t}}{{P}+{R}},$ |  |'
- en: 'where:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '| (7) |  | $P=\frac{\sum_{t\in\mathcal{S}}TP_{t}}{\sum_{t\in S}TP_{t}+FP_{t}},\quad
    R=\frac{\sum_{t\in S}TP_{t}}{\sum_{t\in\mathcal{S}}TP_{t}+FN_{t}}.$ |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $P=\frac{\sum_{t\in\mathcal{S}}TP_{t}}{\sum_{t\in S}TP_{t}+FP_{t}},\quad
    R=\frac{\sum_{t\in S}TP_{t}}{\sum_{t\in\mathcal{S}}TP_{t}+FN_{t}}.$ |  |'
- en: '$Macro\!-\!\!F1$. The $Macro\!-\!\!F1$ ([DBLP:books/daglib/0021593,](#bib.bib283)
    ) calculates the average $F1$ of all labels. Unlike $Micro\!-\!\!F1$, which sets
    even weight to every example, $Macro\!-\!\!F1$ sets the same weight to all labels
    in the average process. Formally, $Macro\!-\!\!F1$ is defined as:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: $Macro\!-\!\!F1$。$Macro\!-\!\!F1$ ([DBLP:books/daglib/0021593,](#bib.bib283)
    ) 计算所有标签的平均 $F1$ 值。与 $Micro\!-\!\!F1$ 不同，$Micro\!-\!\!F1$ 对每个例子的权重相同，而 $Macro\!-\!\!F1$
    在平均过程中对所有标签赋予相同的权重。形式上，$Macro\!-\!\!F1$ 定义为：
- en: '| (8) |  | ${Macro}\!-\!\!F1=\frac{1}{\mathcal{S}}\sum_{t\in\mathcal{S}}\frac{2{P}_{t}\times
    R_{t}}{{P_{t}}+{R_{t}}},$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | ${Macro}\!-\!\!F1=\frac{1}{\mathcal{S}}\sum_{t\in\mathcal{S}}\frac{2{P}_{t}\times
    R_{t}}{{P_{t}}+{R_{t}}},$ |  |'
- en: 'where:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '| (9) |  | $P_{t}=\frac{TP_{t}}{TP_{t}+FP_{t}},\quad R_{t}=\frac{TP_{t}}{TP_{t}+FN_{t}}.$
    |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $P_{t}=\frac{TP_{t}}{TP_{t}+FP_{t}},\quad R_{t}=\frac{TP_{t}}{TP_{t}+FN_{t}}.$
    |  |'
- en: In addition to the above evaluation metrics, there are some rank-based evaluation
    metrics for extreme multi-label classification tasks, including $P@K$ and $NDCG@K$.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述评价指标外，还有一些基于排名的评价指标用于极端多标签分类任务，包括 $P@K$ 和 $NDCG@K$。
- en: Precision at Top K (P@K). The $P@K$ ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96)
    ) is the precision at the top k. For $P@K$, each text has a set of $\mathcal{L}$
    ground truth labels $L_{t}=\left\{l_{0},l_{1},l_{2}\ldots,l_{\mathcal{L}-1}\right\}$,
    in order of decreasing probability $P_{t}=$ $\left[p_{0},p_{1},p_{2}\ldots,p_{Q-1}\right].$
    The precision at ${k}$ is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部 K 的精准率（P@K）。$P@K$ ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96) ) 是前 k 的精准率。对于
    $P@K$，每个文本有一组 $\mathcal{L}$ 个真实标签 $L_{t}=\left\{l_{0},l_{1},l_{2}\ldots,l_{\mathcal{L}-1}\right\}$，按概率
    $P_{t}=$ $\left[p_{0},p_{1},p_{2}\ldots,p_{Q-1}\right]$ 降序排列。精准率在 ${k}$ 为：
- en: '| (10) |  | $P@K=\frac{1}{k}\sum_{j=0}^{\min(\mathcal{L},k)-1}{rel}_{L_{i}}\left(P_{t}(j)\right),$
    |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $P@K=\frac{1}{k}\sum_{j=0}^{\min(\mathcal{L},k)-1}{rel}_{L_{i}}\left(P_{t}(j)\right),$
    |  |'
- en: '| (11) |  | $\operatorname{rel}_{L}(p)=\begin{cases}1&amp;\text{ if }p\in L\\
    0&amp;\text{ otherwise }\end{cases},$ |  |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\operatorname{rel}_{L}(p)=\begin{cases}1\&\text{ 如果 }p\in L\\
    0\&\text{ 否则 }\end{cases},$ |  |'
- en: where $\mathcal{L}$ is the number of ground truth labels or possible answers
    on each text and $k$ is the number of selected labels on extreme multi-label text
    classification.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 是每个文本的真实标签或可能答案的数量，$k$ 是极端多标签文本分类中选定的标签数量。
- en: Normalized Discounted Cummulated Gains (NDCG@K). The ${NDCG}@{K}$ ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96)
    ) is
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化折扣累计增益（NDCG@K）。${NDCG}@{K}$ ([DBLP:conf/sigir/LiuCWY17,](#bib.bib96) ) 为：
- en: '| (12) |  | ${NDCG}@K=\frac{1}{{IDCG}\left({L}_{i},k\right)}\sum_{j=0}^{n-1}\frac{rel_{L_{i}}\left(P_{t}(j)\right)}{\ln(j+1)},$
    |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | ${NDCG}@K=\frac{1}{{IDCG}\left({L}_{i},k\right)}\sum_{j=0}^{n-1}\frac{rel_{L_{i}}\left(P_{t}(j)\right)}{\ln(j+1)},$
    |  |'
- en: where $IDCG$ is ideal discounted cumulative gain and the particular rank position
    $n$ is
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $IDCG$ 是理想折扣累计增益，特定的排名位置 $n$ 为：
- en: '| (13) |  | $n=\min\left(\max\left(\left&#124;P_{i}\right&#124;,\left&#124;L_{i}\right&#124;\right),k\right).$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $n=\min\left(\max\left(\left|P_{i}\right|,\left|L_{i}\right|\right),k\right).$
    |  |'
- en: Among these multi-label evaluation metrics, $Micro\!-\!\!F1$ considers the number
    of categories, which makes it suitable for the unbalanced data distribution. $Macro\!-\!\!F1$
    does not take into account the amount of data that treats each class equally.
    Thus, it is easily affected by the classes with high Recall and Precision. When
    the number of categories is large or extremely large, either P@K or NDCG@K is
    used.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些多标签评估指标中，$Micro\!-\!\!F1$ 考虑了类别的数量，这使得它适用于不平衡的数据分布。$Macro\!-\!\!F1$ 不考虑数据的数量，对每个类别一视同仁。因此，它容易受到高召回率和精确度类别的影响。当类别数量较大或极大时，通常使用
    P@K 或 NDCG@K。
- en: 4\. Quantitative Results
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 定量结果
- en: 'There are many differences between sentiment analysis, news classification,
    topic labeling and natural language inference tasks, which can not be simplified
    modeled as a text classification task. In this section, we tabulate the performance
    of the main models given in their articles on classic datasets evaluated by classification
    accuracy, as shown in Table [4](#S4.T4 "Table 4 ‣ 4\. Quantitative Results ‣ A
    Survey on Text Classification: From Traditional to Deep Learning"), including
    MR, SST-2, IMDB, Yelp.P, Yelp.F, Amazon.F, 20NG, AG, DBpedia, and SNLI.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析、新闻分类、主题标记和自然语言推断任务之间存在许多差异，这些差异无法简化为文本分类任务。在本节中，我们汇总了文献中主要模型在经典数据集上的表现，并通过分类准确率进行了评估，如表
    [4](#S4.T4 "表 4 ‣ 4\. 定量结果 ‣ 从传统到深度学习的文本分类调查") 所示，包括 MR、SST-2、IMDB、Yelp.P、Yelp.F、Amazon.F、20NG、AG、DBpedia
    和 SNLI。
- en: 'We give the performance of NB and SVM algorithms from RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64)
    ) due to the less traditional text classification model has been an experiment
    on datasets in Table [4](#S4.T4 "Table 4 ‣ 4\. Quantitative Results ‣ A Survey
    on Text Classification: From Traditional to Deep Learning"). The accuracy of NB
    and SVM are 81.8% and 79.4% on SST-2, respectively. We can see that, in the SST-2
    data set with only two categories, the accuracy of NB is better than that of SVM.
    It may be because NB has relatively stable classification efficiency on new data
    sets. The performance is also stable on small data sets. Compared with the deep
    learning model, the performance of NB is lower. NB has the advantage of lower
    computational complexity than deep models. However, it requires manual classification
    features, making it difficult to migrate the model directly to other data sets.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给出了 RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) 的 NB 和 SVM 算法的表现，因为相较于传统文本分类模型，它们在表
    [4](#S4.T4 "表 4 ‣ 4\. 定量结果 ‣ 从传统到深度学习的文本分类调查") 中的数据集上的实验较少。NB 和 SVM 在 SST-2 上的准确率分别为
    81.8% 和 79.4%。我们可以看到，在只有两个类别的 SST-2 数据集中，NB 的准确率优于 SVM。这可能是因为 NB 在新数据集上的分类效率相对稳定，其在小数据集上的表现也较为稳定。与深度学习模型相比，NB
    的表现较低。虽然 NB 具有比深度模型更低的计算复杂度，但它需要手动分类特征，使得直接将模型迁移到其他数据集变得困难。
- en: For deep learning models, pre-trained models get better results on most datasets.
    It means that if you need to implement a text classification task, you can preferentially
    try pre-trained models, such as BERT, RoBERTa, and XLNET, etc., except MR and
    20NG, which have not been experimented on BERT based models. Pre-trained models
    are essential to NLP. It uses a deep model to learn a better feature of the text.
    It also demonstrates that the accuracy of NLP tasks can be significantly improved
    by a profound model that can be pre-trained from unlabeled datasets. For the MR
    dataset, the accuracy of RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87)
    ) is 83.8%, obtaining the best result. It suggests that RNN-Capsule builds a capsule
    in each category for sentiment analysis. It can output words including sentiment
    trends indicating attributes of capsules with no applying linguistic knowledge.
    For 20NG dataset, BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) gets
    96.5% score with the best accuracy score. It may demonstrate the effectiveness
    of applying the 2D max-pooling operation to obtain a fixed-length representation
    of the text and utilize 2D convolution to sample more meaningful matrix information.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习模型，预训练模型在大多数数据集上获得了更好的结果。这意味着，如果你需要实现文本分类任务，可以优先尝试预训练模型，如 BERT、RoBERTa
    和 XLNET 等，除了 MR 和 20NG，这两个数据集尚未在基于 BERT 的模型上进行实验。预训练模型对于自然语言处理（NLP）至关重要。它使用深度模型来学习文本的更好特征。它还表明，通过从未标记的数据集中预训练的深度模型可以显著提高
    NLP 任务的准确性。对于 MR 数据集，RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87) ) 的准确率为
    83.8%，获得了最佳结果。这表明 RNN-Capsule 为情感分析在每个类别中建立了一个胶囊。它可以输出包括情感趋势在内的词汇，指示胶囊的属性而无需应用语言学知识。对于
    20NG 数据集，BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) 获得了 96.5%
    的分数，取得了最佳准确率。这可能表明应用 2D 最大池化操作来获得固定长度的文本表示，并利用 2D 卷积来采样更有意义的矩阵信息的有效性。
- en: Table 4\. Accuracy of text classification models on primary datasets evaluated
    by classification accuracy (in terms of publication year). Bold is the most accurate.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 文本分类模型在主要数据集上的准确性（按出版年份评估）。**粗体**表示最准确。
- en: '| Model | Sentiment | News | Topic | NLI |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 情感 | 新闻 | 主题 | NLI |'
- en: '| MR | SST-2 | IMDB | Yelp.P | Yelp.F | Amz.F | 20NG | AG | DBpedia | SNLI
    |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| MR | SST-2 | IMDB | Yelp.P | Yelp.F | Amz.F | 20NG | AG | DBpedia | SNLI
    |'
- en: '| NB ([DBLP:journals/jacm/Maron61,](#bib.bib8) ) | - | 81.80 | - | - | - |
    - | - | - | - | - |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| NB ([DBLP:journals/jacm/Maron61,](#bib.bib8) ) | - | 81.80 | - | - | - |
    - | - | - | - | - |'
- en: '| SVM ([DBLP:journals/ml/CortesV95,](#bib.bib42) ) | - | 79.40 | - | - | -
    | - | - | - | - | - |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| SVM ([DBLP:journals/ml/CortesV95,](#bib.bib42) ) | - | 79.40 | - | - | -
    | - | - | - | - | - |'
- en: '| Tree-CRF ([DBLP:conf/naacl/NakagawaIK10,](#bib.bib284) ) | 77.30 | - | -
    | - | - | - | - | - | - | - |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Tree-CRF ([DBLP:conf/naacl/NakagawaIK10,](#bib.bib284) ) | 77.30 | - | -
    | - | - | - | - | - | - | - |'
- en: '| RAE ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60) ) | 77.70 | 82.40 | - |
    - | - | - | - | - | - | - |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| RAE ([DBLP:conf/emnlp/SocherPHNM11,](#bib.bib60) ) | 77.70 | 82.40 | - |
    - | - | - | - | - | - | - |'
- en: '| MV-RNN ([DBLP:conf/emnlp/SocherHMN12,](#bib.bib62) ) | 79.00 | 82.90 | -
    | - | - | - | - | - | - | - |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| MV-RNN ([DBLP:conf/emnlp/SocherHMN12,](#bib.bib62) ) | 79.00 | 82.90 | -
    | - | - | - | - | - | - | - |'
- en: '| RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) | 75.90 | 85.40 | -
    | - | - | - | - | - | - | - |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| RNTN ([DBLP:conf/emnlp/SocherPWCMNP13,](#bib.bib64) ) | 75.90 | 85.40 | -
    | - | - | - | - | - | - | - |'
- en: '| DCNN ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ) |  | 86.80 | 89.40 |
    - | - | - | - | - | - | - |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| DCNN ([DBLP:conf/acl/KalchbrennerGB14,](#bib.bib5) ) |  | 86.80 | 89.40 |
    - | - | - | - | - | - | - |'
- en: '| Paragraph-Vec ([DBLP:conf/icml/LeM14,](#bib.bib67) ) |  | 87.80 | 92.58 |
    - | - | - | - | - | - | - |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Paragraph-Vec ([DBLP:conf/icml/LeM14,](#bib.bib67) ) |  | 87.80 | 92.58 |
    - | - | - | - | - | - | - |'
- en: '| TextCNN([DBLP:conf/emnlp/Kim14,](#bib.bib17) ) | 81.50 | 88.10 | - | - |
    - | - | - | - | - | - |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| TextCNN([DBLP:conf/emnlp/Kim14,](#bib.bib17) ) | 81.50 | 88.10 | - | - |
    - | - | - | - | - | - |'
- en: '| TextRCNN ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72) ) | - | - | - | - | - |
    - | 96.49 | - | - | - |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| TextRCNN ([DBLP:conf/aaai/LaiXLZ15,](#bib.bib72) ) | - | - | - | - | - |
    - | 96.49 | - | - | - |'
- en: '| DAN ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69) ) | - | 86.30 | 89.40 | - |
    - | - | - | - | - | - |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| DAN ([DBLP:conf/acl/IyyerMBD15,](#bib.bib69) ) | - | 86.30 | 89.40 | - |
    - | - | - | - | - | - |'
- en: '| Tree-LSTM ([DBLP:conf/acl/TaiSM15,](#bib.bib1) ) |  | 88.00 | - | - | - |
    - | - | - | - | - |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Tree-LSTM ([DBLP:conf/acl/TaiSM15,](#bib.bib1) ) |  | 88.00 | - | - | - |
    - | - | - | - | - |'
- en: '| CharCNN ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | - | - | - | 95.12 |
    62.05 | - | - | 90.49 | 98.45 | - |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| CharCNN ([DBLP:conf/nips/ZhangZL15,](#bib.bib93) ) | - | - | - | 95.12 |
    62.05 | - | - | 90.49 | 98.45 | - |'
- en: '| HAN ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | - | - | 49.40 | - |
    - | 63.60 | - | - | - | - |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| HAN ([DBLP:conf/naacl/YangYDHSH16,](#bib.bib108) ) | - | - | 49.40 | - |
    - | 63.60 | - | - | - | - |'
- en: '| SeqTextRCNN ([DBLP:conf/naacl/LeeD16,](#bib.bib7) ) | - | - | - | - | - |
    - | - | - | - | - |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| SeqTextRCNN ([DBLP:conf/naacl/LeeD16,](#bib.bib7) ) | - | - | - | - | - |
    - | - | - | - | - |'
- en: '| oh-2LSTMp ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ) | - | - | 94.10 | 97.10
    | 67.61 | - | 86.68 | 93.43 | 99.16 | - |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| oh-2LSTMp ([DBLP:conf/icml/JohnsonZ16,](#bib.bib75) ) | - | - | 94.10 | 97.10
    | 67.61 | - | 86.68 | 93.43 | 99.16 | - |'
- en: '| LSTMN ([DBLP:conf/emnlp/0001DL16,](#bib.bib112) ) | - | 87.30 | - | - | -
    | - | - | - | - | - |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| LSTMN ([DBLP:conf/emnlp/0001DL16,](#bib.bib112) ) | - | 87.30 | - | - | -
    | - | - | - | - | - |'
- en: '| Multi-Task ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) | - | 87.90 | 91.30
    | - | - | - | - | - | - | - |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Multi-Task ([DBLP:conf/ijcai/LiuQH16,](#bib.bib79) ) | - | 87.90 | 91.30
    | - | - | - | - | - | - | - |'
- en: '| BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) | 82.30 | 89.50
    | - | - | - | - | 96.50 | - | - | - |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| BLSTM-2DCNN ([DBLP:conf/coling/ZhouQZXBX16,](#bib.bib77) ) | 82.30 | 89.50
    | - | - | - | - | 96.50 | - | - | - |'
- en: '| TopicRNN ([DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ) | - | - | 93.72 | -
    | - | - | - | - | - | - |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| TopicRNN ([DBLP:conf/iclr/Dieng0GP17,](#bib.bib83) ) | - | - | 93.72 | -
    | - | - | - | - | - | - |'
- en: '| DPCNN ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | - | - | - | 97.36 | 69.42
    | 65.19 | - | 93.13 | 99.12 | - |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| DPCNN ([DBLP:conf/acl/JohnsonZ17,](#bib.bib98) ) | - | - | - | 97.36 | 69.42
    | 65.19 | - | 93.13 | 99.12 | - |'
- en: '| KPCNN ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | 83.25 | - | - | - |
    - | - | - | 88.36 | - | - |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| KPCNN ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ) | 83.25 | - | - | - |
    - | - | - | 88.36 | - | - |'
- en: '| RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87) ) | 83.80 |  | - | -
    | - | - | - | - | - | - |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| RNN-Capsule ([DBLP:conf/www/WangSH0Z18,](#bib.bib87) ) | 83.80 |  | - | -
    | - | - | - | - | - | - |'
- en: '| ULMFiT ([DBLP:conf/acl/RuderH18,](#bib.bib285) ) | - | - | 95.40 | 97.84
    | 71.02 | - | - | 94.99 | 99.20 | - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| ULMFiT ([DBLP:conf/acl/RuderH18,](#bib.bib285) ) | - | - | 95.40 | 97.84
    | 71.02 | - | - | 94.99 | 99.20 | - |'
- en: '| LEAM([DBLP:conf/acl/HenaoLCSWWZZ18,](#bib.bib286) ) | 76.95 | - | - | 95.31
    | 64.09 | - | 81.91 | 92.45 | 99.02 | - |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| LEAM([DBLP:conf/acl/HenaoLCSWWZZ18,](#bib.bib286) ) | 76.95 | - | - | 95.31
    | 64.09 | - | 81.91 | 92.45 | 99.02 | - |'
- en: '| TextCapsule ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ) | 82.30 | 86.80
    | - | - | - | - | - | 92.60 | - | - |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| TextCapsule ([DBLP:conf/emnlp/YangZYLZZ18,](#bib.bib101) ) | 82.30 | 86.80
    | - | - | - | - | - | 92.60 | - | - |'
- en: '| TextGCN ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ) | 76.74 | - | - | - | -
    | - | 86.34 | 67.61 | - | - |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| TextGCN ([DBLP:conf/aaai/YaoM019,](#bib.bib155) ) | 76.74 | - | - | - | -
    | - | 86.34 | 67.61 | - | - |'
- en: '| BERT-base ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | - | 93.50 | 95.63
    | 98.08 | 70.58 | 61.60 | - | - | - | 91.00 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| BERT-base ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | - | 93.50 | 95.63
    | 98.08 | 70.58 | 61.60 | - | - | - | 91.00 |'
- en: '| BERT-large ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | - | 94.90 | 95.79
    | 98.19 | 71.38 | 62.20 | - | - | - | 91.70 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| BERT-large ([DBLP:conf/naacl/DevlinCLT19,](#bib.bib19) ) | - | 94.90 | 95.79
    | 98.19 | 71.38 | 62.20 | - | - | - | 91.70 |'
- en: '| MT-DNN([DBLP:conf/acl/LiuHCG19,](#bib.bib287) ) | - | 95.60 | 83.20 | - |
    - | - | - | - | - | 91.50 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| MT-DNN([DBLP:conf/acl/LiuHCG19,](#bib.bib287) ) | - | 95.60 | 83.20 | - |
    - | - | - | - | - | 91.50 |'
- en: '| XLNet-Large ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | - | 96.80 | 96.21
    | 98.45 | 72.20 | 67.74 | - | - | - | - |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| XLNet-Large ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | - | 96.80 | 96.21
    | 98.45 | 72.20 | 67.74 | - | - | - | - |'
- en: '| XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | - | 97.00 | - | - |
    - | - | - | 95.51 | 99.38 | - |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| XLNet ([DBLP:conf/nips/YangDYCSL19,](#bib.bib138) ) | - | 97.00 | - | - |
    - | - | - | 95.51 | 99.38 | - |'
- en: '| RoBERTa ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140) ) | - | 96.40
    | - | - | - | - | - | - | - | 92.60 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa ([DBLP:journals/corr/abs-1907-11692,](#bib.bib140) ) | - | 96.40
    | - | - | - | - | - | - | - | 92.60 |'
- en: 5\. Future Research Challenges
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 未来研究挑战
- en: Text classification – as efficient information retrieval and mining technology
    – plays a vital role in managing text data. It uses NLP, data mining, machine
    learning, and other techniques to automatically classify and discover different
    text types. Text classification takes multiple types of text as input, and the
    text is represented as a vector by the pre-training model. Then the vector is
    fed into the DNN for training until the termination condition is reached, and
    finally, the performance of the training model is verified by the downstream task.
    Existing models have already shown their usefulness in text classification, but
    there are still many possible improvements to explore.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类—作为高效的信息检索和挖掘技术—在管理文本数据中扮演着至关重要的角色。它使用自然语言处理、数据挖掘、机器学习及其他技术来自动分类和发现不同的文本类型。文本分类以多种文本作为输入，文本通过预训练模型表示为向量。然后，将该向量输入到深度神经网络中进行训练，直到达到终止条件，最终通过下游任务验证训练模型的性能。现有模型已显示出在文本分类中的有效性，但仍有许多可能的改进值得探索。
- en: Although some new text classification models repeatedly brush up the accuracy
    index of most classification tasks, it cannot indicate whether the model ”understands”
    the text from the semantic level like human beings. Moreover, with the emergence
    of the noise sample, the small sample noise may cause the decision confidence
    to change substantially or even lead to decision reversal. Therefore, the semantic
    representation ability and robustness of the model need to be proved in practice.
    Besides, the pre-trained semantic representation model represented by word vectors
    can often improve the performance of downstream NLP tasks. The existing research
    on the transfer strategy of context-free word vectors is still relatively preliminary.
    Thus, we conclude from data, models, and performance perspective, text classification
    mainly faces the following challenges.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些新的文本分类模型反复提高了大多数分类任务的准确性指标，但这不能表明模型是否像人类一样从语义层面“理解”文本。此外，噪声样本的出现可能导致小样本噪声显著改变决策信心，甚至导致决策反转。因此，模型的语义表示能力和鲁棒性需要在实践中证明。此外，以词向量为代表的预训练语义表示模型通常能提高下游NLP任务的性能。现有关于上下文无关词向量的迁移策略的研究仍然相对初步。因此，从数据、模型和性能的角度来看，文本分类主要面临以下挑战。
- en: 5.1\. Challenges from Data Perspective
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 数据视角的挑战
- en: 'For a text classification task, data is essential to model performance, whether
    it is traditional or deep learning method. The text data mainly studied includes
    multi-chapter, short text, cross-language, multi-label, less sample text. For
    the characteristics of these data, the existing technical challenges are as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本分类任务，无论是传统方法还是深度学习方法，数据对模型性能至关重要。主要研究的文本数据包括多章节、短文本、跨语言、多标签、少样本文本。针对这些数据的特点，现有的技术挑战如下：
- en: Zero-shot/Few-shot learning. Zero-shot or few-shot learning for text classification
    aim to classify text having no or few same labeled class data. However, the current
    models are too dependent on numerous labeled data. The performance of these models
    is significantly affected by zero-shot or few-shot learning. Thus, some works
    focus on tackling these problems. The main idea is to infer the features through
    learning kinds of semantic knowledge, such as learning relationship among classes
    ([DBLP:journals/corr/abs-1712-05972,](#bib.bib288) ) and incorporating class descriptions
    ([DBLP:conf/naacl/ZhangLG19,](#bib.bib170) ). Furthermore, latent features generation
    ([DBLP:conf/ijcai/SongZSXX20,](#bib.bib289) ) meta-Learning ([DBLP:conf/emnlp/GengLLZJS19,](#bib.bib290)
    ; [DBLP:conf/aaai/DengZSCC20,](#bib.bib291) ; [DBLP:conf/iclr/BaoWCB20,](#bib.bib106)
    ) and dynamic memory mechanism ([DBLP:conf/acl/GengLLSZ20,](#bib.bib292) ) are
    also efficient methods. Nevertheless, with the limitation of little unseen class
    data and different data distribution between seen class and unseen class, there
    is still a long way to go to reach the learning ability comparable to that of
    humans.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本/少样本学习。零样本或少样本学习的目标是对没有或只有少量相同标签数据的文本进行分类。然而，当前的模型过于依赖大量标记数据。这些模型的性能受到零样本或少样本学习的显著影响。因此，一些工作集中在解决这些问题上。主要思路是通过学习各种语义知识推断特征，例如学习类之间的关系（[DBLP:journals/corr/abs-1712-05972,](#bib.bib288)）和结合类描述（[DBLP:conf/naacl/ZhangLG19,](#bib.bib170)）。此外，潜在特征生成（[DBLP:conf/ijcai/SongZSXX20,](#bib.bib289)）、元学习（[DBLP:conf/emnlp/GengLLZJS19,](#bib.bib290)；[DBLP:conf/aaai/DengZSCC20,](#bib.bib291)；[DBLP:conf/iclr/BaoWCB20,](#bib.bib106)）和动态记忆机制（[DBLP:conf/acl/GengLLSZ20,](#bib.bib292)）也是有效的方法。然而，由于有限的未见类别数据和已见类别与未见类别之间的数据分布差异，要达到与人类相当的学习能力仍然任重道远。
- en: The external knowledge. As we all know, the more beneficial information is input
    into a DNN, its better performance. For example, a question answering system incorporating
    a common-sense knowledge base can answer questions about the real world and help
    solve problems with incomplete information. Therefore, adding external knowledge
    (knowledge base or knowledge graph) ([DBLP:conf/acl/RojasBOC20,](#bib.bib293)
    ; [DBLP:journals/mlc/ShanavasWLH21,](#bib.bib294) ) is an efficient way to promote
    the model’s performance. The existing knowledge includes conceptual information
    ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ; [DBLP:conf/aaai/ChenHLXJ19,](#bib.bib127)
    ; [DBLP:journals/corr/abs-1904-09223,](#bib.bib204) ), commonsense knowledge ([DBLP:conf/emnlp/DingLLLD19,](#bib.bib223)
    ), knowledge base information ([DBLP:conf/acl/HaoZLHLWZ17,](#bib.bib295) ; [DBLP:conf/ekaw/TurkerZKS18,](#bib.bib296)
    ), general knowledge graph ([DBLP:conf/naacl/ZhangLG19,](#bib.bib170) ) and so
    on, which enhances the semantic representation of texts. Nevertheless, with the
    limitation of input scale, how and what to add for different tasks is still a
    challenge.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 外部知识。众所周知，输入到深度神经网络（DNN）中的有益信息越多，其性能就越好。例如，结合常识知识库的问答系统可以回答关于现实世界的问题，并帮助解决信息不完整的问题。因此，添加外部知识（知识库或知识图谱）
    ([DBLP:conf/acl/RojasBOC20,](#bib.bib293) ; [DBLP:journals/mlc/ShanavasWLH21,](#bib.bib294)
    ) 是提升模型性能的有效方式。现有的知识包括概念信息 ([DBLP:conf/ijcai/WangWZY17,](#bib.bib100) ; [DBLP:conf/aaai/ChenHLXJ19,](#bib.bib127)
    ; [DBLP:journals/corr/abs-1904-09223,](#bib.bib204) ), 常识知识 ([DBLP:conf/emnlp/DingLLLD19,](#bib.bib223)
    ), 知识库信息 ([DBLP:conf/acl/HaoZLHLWZ17,](#bib.bib295) ; [DBLP:conf/ekaw/TurkerZKS18,](#bib.bib296)
    ), 通用知识图谱 ([DBLP:conf/naacl/ZhangLG19,](#bib.bib170) ) 等，提升了文本的语义表示。然而，由于输入规模的限制，如何为不同任务添加内容仍然是一个挑战。
- en: Special domain with many terminologies. Most of the existing models are supervised
    models, which over-rely on numerous labeled data. When the sample size is too
    small, or zero samples occur, the performance of the model will be significantly
    affected. New data set annotation takes a lot of time. Therefore, unsupervised
    learning and semi-supervised learning have great potential for text classification.
    Furthermore, texts in a particular field ([DBLP:conf/ijcai/LiangCYLQZ20,](#bib.bib297)
    ; [DBLP:journals/ijmei/BMHK21,](#bib.bib298) ), such as financial and medical
    texts, contain many specific words or domain experts intelligible slang, abbreviations,
    etc., which make the existing pre-trained word vectors challenging to work on.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 专业领域中的术语众多。大多数现有模型是监督模型，过度依赖大量标注数据。当样本数量过少或出现零样本时，模型性能将受到显著影响。新的数据集标注需要大量时间。因此，无监督学习和半监督学习在文本分类中具有巨大潜力。此外，特定领域的文本
    ([DBLP:conf/ijcai/LiangCYLQZ20,](#bib.bib297) ; [DBLP:journals/ijmei/BMHK21,](#bib.bib298)
    ), 如金融和医学文本，包含许多特定的词汇或领域专家能理解的俚语、缩写等，使得现有的预训练词向量在处理这些文本时面临挑战。
- en: The multi-label text classification task. Multi-label text classification requires
    full consideration of the semantic relationship among labels, and the embedding
    and encoding of the model is a process of lossy compression ([DBLP:journals/apin/WangLLZZF20,](#bib.bib299)
    ; [DBLP:journals/kbs/WangHLY21,](#bib.bib300) ). Therefore, how to reduce the
    loss of hierarchical semantics and retain rich and complex document semantic information
    during training is still a problem to be solved.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签文本分类任务。多标签文本分类需要充分考虑标签之间的语义关系，而模型的嵌入和编码过程是一个有损压缩的过程 ([DBLP:journals/apin/WangLLZZF20,](#bib.bib299)
    ; [DBLP:journals/kbs/WangHLY21,](#bib.bib300) )。因此，如何减少层次语义的损失，并在训练过程中保留丰富和复杂的文档语义信息仍然是需要解决的问题。
- en: 5.2\. Challenges from Model Perspective
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 模型视角的挑战
- en: Most existing structures of traditional and deep learning models are tried for
    text classification, including integration methods. BERT learns a language representation
    that can be used to fine-tune for many NLP tasks. The primary method is to increase
    data, improve computation power, and design training procedures for getting better
    results ([DBLP:conf/coling/DuHM20,](#bib.bib301) ; [DBLP:journals/tcyb/DuVC21,](#bib.bib302)
    ; [DBLP:journals/corr/abs-2102-00426,](#bib.bib303) ). How the tradeoff between
    data and compute resources and prediction performance is worth studying.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的传统和深度学习模型结构都已尝试用于文本分类，包括集成方法。BERT 学习了一种语言表示，可以用于微调多个 NLP 任务。主要方法是增加数据、提高计算能力，并设计训练程序以获得更好的结果
    ([DBLP:conf/coling/DuHM20,](#bib.bib301) ; [DBLP:journals/tcyb/DuVC21,](#bib.bib302)
    ; [DBLP:journals/corr/abs-2102-00426,](#bib.bib303) )。数据与计算资源和预测性能之间的权衡值得研究。
- en: Text representation. The text representation method based on the vector space
    model is simple and effective in the text preprocessing stage. However, it will
    lose the semantic information of the text, so the application performance based
    on this method is limited. The proposed semantically based text representation
    method is too time-consuming. Therefore, the efficient semantically based text
    representation method still needs further research. In the text representation
    of text classification based on deep learning, word embedding is the main concept,
    while the representation unit is described differently in different languages.
    Then, a word is represented in the form of a vector by learning mapping rules
    through the model. Therefore, how to design adaptive data representation methods
    is more conducive to the combination of deep learning and specific classification
    tasks.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 文本表示。基于向量空间模型的文本表示方法在文本预处理阶段简单有效。然而，它会丢失文本的语义信息，因此基于这种方法的应用性能受到限制。提出的基于语义的文本表示方法过于耗时。因此，高效的基于语义的文本表示方法仍需进一步研究。在基于深度学习的文本分类中的文本表示中，词嵌入是主要概念，而不同语言中的表示单元描述有所不同。然后，通过学习映射规则，将词表示为向量的形式。因此，如何设计自适应的数据表示方法更有利于深度学习与特定分类任务的结合。
- en: Model integration. Most structures of traditional and deep learning models are
    tried for text classification, including integration methods. RNN requires recursive
    step by step to get global information. CNN can obtain local information, and
    the sensing field can be increased through the multi-layer stack to capture more
    comprehensive contextual information. Attention mechanisms learn global dependency
    among words in a sentence. The transformer model is dependent on attention mechanisms
    to establish the depth of the global dependency relationship between the input
    and output. Therefore, designing an integrated model is worth trying to take advantage
    of these models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 模型集成。传统和深度学习模型的大多数结构都尝试用于文本分类，包括集成方法。RNN需要递归地逐步获取全局信息。CNN可以获取局部信息，通过多层堆叠增加感知领域，以捕获更全面的上下文信息。注意机制学习句子中词语之间的全局依赖关系。变换器模型依赖于注意机制建立输入与输出之间的全局依赖关系的深度。因此，设计一个集成模型值得尝试，以利用这些模型的优势。
- en: Model efficiency. Although text classification models based on deep learning
    are highly effective, such as CNNs, RNNs, and GNNs. However, there are many technical
    limitations, such as the depth of the network layer, regularization problem, network
    learning rate, etc. Therefore, there is still more broad space for development
    to optimize the algorithm and improve the speed of model training.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 模型效率。虽然基于深度学习的文本分类模型，如CNN、RNN和GNN，具有很高的效果，但也存在许多技术限制，如网络层的深度、正则化问题、网络学习率等。因此，优化算法和提高模型训练速度还有更广阔的发展空间。
- en: 5.3\. Challenges from Performance Perspective
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 性能视角下的挑战
- en: The traditional model and the deep model can achieve good performance in most
    text classification tasks, but the anti-interference ability of their results
    needs to be improved ([DBLP:conf/emnlp/ZhouJCW19,](#bib.bib304) ; [DBLP:conf/aaai/JinJZS20,](#bib.bib176)
    ; [DBLP:journals/corr/abs-2103-04264,](#bib.bib305) ). How to realize the interpretation
    of the deep model is also a technical challenge.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型和深度模型在大多数文本分类任务中可以取得良好性能，但其结果的抗干扰能力需要改进（[DBLP:conf/emnlp/ZhouJCW19,](#bib.bib304)
    ; [DBLP:conf/aaai/JinJZS20,](#bib.bib176) ; [DBLP:journals/corr/abs-2103-04264,](#bib.bib305)）。如何实现深度模型的解释也是一个技术挑战。
- en: The semantic robustness of the model. In recent years, researchers have designed
    many models to enhance the accuracy of text classification models. However, when
    there are some adversarial samples in the datasets, the model’s performance decreases
    significantly. Adversarial training is a crucial method to improve the robustness
    of the pre-training model. For example, a popular approach is converting attack
    into defense and using the adversarial sample training model. Consequently, how
    to improve the robustness of models is a current research hotspot and challenge.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的语义鲁棒性。近年来，研究人员设计了许多模型以提高文本分类模型的准确性。然而，当数据集中存在一些对抗样本时，模型的性能会显著下降。对抗训练是提高预训练模型鲁棒性的一种关键方法。例如，一种流行的方法是将攻击转化为防御，使用对抗样本训练模型。因此，如何提高模型的鲁棒性是当前的研究热点和挑战。
- en: The interpretability of the model. DNNs have unique advantages in feature extraction
    and semantic mining and have achieved excellent text classification tasks. Only
    a better understanding of the theories behind these models can accurately design
    better models for various applications. However, deep learning is a black-box
    model, the training process is challenging to reproduce, and the implicit semantics
    and output interpretability are poor. It makes the improvement and optimization
    of the model, losing clear guidelines. Why does one model outperform another on
    one data set but underperform on others? What does the deep learning model learn?
    Furthermore, we cannot accurately explain why the model improves performance.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的可解释性。深度神经网络在特征提取和语义挖掘方面具有独特优势，并在文本分类任务中取得了优异的成绩。只有更好地理解这些模型背后的理论，才能准确设计适用于各种应用的更好模型。然而，深度学习是一个黑箱模型，训练过程难以重现，隐含语义和输出可解释性较差。这使得模型的改进和优化缺乏明确的指导方针。为什么一个模型在一个数据集上表现优于另一个模型，而在其他数据集上却表现较差？深度学习模型学到了什么？此外，我们也无法准确解释模型为何能提高性能。
- en: 6\. Conclusion
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: This paper principally introduces the existing models for text classification
    tasks from traditional models to deep learning. Firstly, we introduce some primary
    traditional models and deep learning models with a summary table. The traditional
    model improves text classification performance mainly by improving the feature
    extraction scheme and classifier design. In contrast, the deep learning model
    enhances performance by improving the presentation learning method, model structure,
    and additional data and knowledge. Then, we introduce the datasets with a summary
    table and evaluation metrics for single-label and multi-label tasks. Furthermore,
    we give the quantitative results of the leading models in a summary table under
    different applications for classic text classification datasets. Finally, we summarize
    the possible future research challenges of text classification.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要介绍了从传统模型到深度学习的文本分类任务现有模型。首先，我们介绍了一些主要的传统模型和深度学习模型，并提供了总结表格。传统模型主要通过改进特征提取方案和分类器设计来提高文本分类性能。相比之下，深度学习模型通过改进表示学习方法、模型结构以及额外的数据和知识来提升性能。接着，我们介绍了数据集，并提供了单标签和多标签任务的评估指标总结表。进一步地，我们给出了不同应用下经典文本分类数据集领先模型的定量结果总结表。最后，我们总结了文本分类可能面临的未来研究挑战。
- en: Acknowledgements.
  id: totrans-385
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: The authors of this paper were supported by the National Key R&D Program of
    China through grant 2021YFB1714800, NSFC through grants (No.U20B2053 and 61872022),
    State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12). Philip
    S. Yu was supported by NSF under grants III-1763325, III-1909323, III-2106758,
    and SaTC-1930941. Lifang He was supported by NSF ONR N00014-18-1-2009 and Lehigh’s
    accelerator grant S00010293. This work was also sponsored by CAAI-Huawei MindSpore
    Open Fund. Thanks for computing infrastructure provided by Huawei MindSpore platform.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 本文作者得到中国国家重点研发计划资助，资助号 2021YFB1714800，国家自然科学基金（No.U20B2053 和 61872022），软件开发环境国家重点实验室（SKLSDE-2020ZX-12）。Philip
    S. Yu 得到 NSF 资助，资助号 III-1763325、III-1909323、III-2106758 和 SaTC-1930941。Lifang
    He 得到 NSF ONR N00014-18-1-2009 和 Lehigh’s accelerator grant S00010293 资助。本工作还由
    CAAI-Huawei MindSpore Open Fund 赞助。感谢华为 MindSpore 平台提供的计算基础设施。
- en: References
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations
    from tree-structured long short-term memory networks,” in Proc. ACL, 2015, pp. 1556–1566,
    2015.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) K. S. Tai, R. Socher, 和 C. D. Manning，“来自树结构长短期记忆网络的改进语义表示，” 见 Proc. ACL,
    2015, 页码 1556–1566, 2015。
- en: (2) X. Zhu, P. Sobhani, and H. Guo, “Long short-term memory over recursive structures,”
    in Proc. ICML, 2015, pp. 1604–1612, 2015.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) X. Zhu, P. Sobhani, 和 H. Guo，“递归结构上的长短期记忆，” 见 Proc. ICML, 2015, 页码 1604–1612,
    2015。
- en: (3) J. Chen, Z. Gong, and W. Liu, “A dirichlet process biterm-based mixture
    model for short text stream clustering,” Appl. Intell., vol. 50, no. 5, pp. 1609–1619,
    2020.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) J. Chen, Z. Gong, 和 W. Liu，“基于 Dirichlet 过程的双词混合模型用于短文本流聚类，” Appl. Intell.,
    卷 50, 期 5, 页码 1609–1619, 2020。
- en: (4) J. Chen, Z. Gong, and W. Liu, “A nonparametric model for online topic discovery
    with word embeddings,” Inf. Sci., vol. 504, pp. 32–47, 2019.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) J. Chen, Z. Gong, 和 W. Liu，“一种用于在线主题发现的非参数模型与词嵌入结合，” Inf. Sci., 卷 504, 页码
    32–47, 2019。
- en: (5) N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural
    network for modelling sentences,” in Proc. ACL, 2014, pp. 655–665, 2014.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) N. Kalchbrenner, E. Grefenstette, 和 P. Blunsom， “用于建模句子的卷积神经网络，” 见 Proc.
    ACL, 2014, 页码 655–665, 2014。
- en: (6) P. Liu, X. Qiu, X. Chen, S. Wu, and X. Huang, “Multi-timescale long short-term
    memory neural network for modelling sentences and documents,” in Proc. EMNLP,
    2015, pp. 2326–2335, 2015.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) P. Liu, X. Qiu, X. Chen, S. Wu, 和 X. Huang，“多时间尺度长短期记忆神经网络用于建模句子和文档，”在EMNLP会议论文集中，2015年，页2326–2335，2015年。
- en: (7) J. Y. Lee and F. Dernoncourt, “Sequential short-text classification with
    recurrent and convolutional neural networks,” in Proc. NAACL, 2016, pp. 515–520,
    2016.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) J. Y. Lee 和 F. Dernoncourt，“基于递归和卷积神经网络的短文本分类，”在NAACL会议论文集中，2016年，页515–520，2016年。
- en: '(8) M. E. Maron, “Automatic indexing: An experimental inquiry,” J. ACM, vol. 8,
    no. 3, pp. 404–417, 1961.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) M. E. Maron，“自动索引：实验研究，”《计算机学会期刊》，卷8，第3期，页404–417，1961年。
- en: (9) T. M. Cover and P. E. Hart, “Nearest neighbor pattern classification,” IEEE
    Trans. Inf. Theory, vol. 13, no. 1, pp. 21–27, 1967.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) T. M. Cover 和 P. E. Hart，“最近邻模式分类，”《IEEE信息理论汇刊》，卷13，第1期，页21–27，1967年。
- en: '(10) T. Joachims, “Text categorization with support vector machines: Learning
    with many relevant features,” in Proc. ECML, 1998, pp. 137–142, 1998.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) T. Joachims，“使用支持向量机的文本分类：带有许多相关特征的学习，”在ECML会议论文集中，1998年，页137–142，1998年。
- en: '(11) R. Aly, S. Remus, and C. Biemann, “Hierarchical multi-label classification
    of text with capsule networks,” in Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28 - August 2,
    2019, Volume 2: Student Research Workshop, pp. 323–330, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) R. Aly, S. Remus, 和 C. Biemann，“基于胶囊网络的文本层次多标签分类，”在第57届计算语言学协会会议论文集中，ACL
    2019，意大利佛罗伦萨，2019年7月28日至8月2日，第2卷：学生研究研讨会，页323–330，2019年。
- en: '(12) K. Kowsari, K. J. Meimandi, M. Heidarysafa, S. Mendu, L. E. Barnes, and
    D. E. Brown, “Text classification algorithms: A survey,” Information, vol. 10,
    no. 4, p. 150, 2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) K. Kowsari, K. J. Meimandi, M. Heidarysafa, S. Mendu, L. E. Barnes, 和 D.
    E. Brown，“文本分类算法：调查，”《信息》，卷10，第4期，页150，2019年。
- en: '(13) S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao,
    “Deep learning based text classification: A comprehensive review,” CoRR, vol. abs/2004.03705,
    2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, 和 J. Gao，“基于深度学习的文本分类：综合评述，”CoRR，卷abs/2004.03705，2020年。
- en: (14) L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, 2001.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) L. Breiman，“随机森林，”《机器学习》，卷45，第1期，页5–32，2001年。
- en: '(15) T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,” in
    Proc. ACM SIGKDD, 2016, pp. 785–794, 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) T. Chen 和 C. Guestrin，“Xgboost：一个可扩展的树提升系统，”在ACM SIGKDD会议论文集中，2016年，页785–794，2016年。
- en: '(16) G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. Liu,
    “Lightgbm: A highly efficient gradient boosting decision tree,” in Proc. NeurIPS,
    2017, pp. 3146–3154, 2017.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, 和 T. Liu，“Lightgbm：一种高效的梯度提升决策树，”在NeurIPS会议论文集中，2017年，页3146–3154，2017年。
- en: (17) Y. Kim, “Convolutional neural networks for sentence classification,” in
    Proc. EMNLP, 2014, pp. 1746–1751, 2014.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Y. Kim，“用于句子分类的卷积神经网络，”在EMNLP会议论文集中，2014年，页1746–1751，2014年。
- en: (18) S. Albawi, T. A. Mohammed, and S. Al-Zawi, “Understanding of a convolutional
    neural network,” in 2017 International Conference on Engineering and Technology
    (ICET), pp. 1–6, Ieee, 2017.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) S. Albawi, T. A. Mohammed, 和 S. Al-Zawi，“卷积神经网络的理解，”在2017年国际工程与技术会议（ICET）上，页1–6，Ieee，2017年。
- en: '(19) J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of
    deep bidirectional transformers for language understanding,” in Proc. NAACL, 2019,
    pp. 4171–4186, 2019.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) J. Devlin, M. Chang, K. Lee, 和 K. Toutanova，“BERT：深度双向变换器的预训练用于语言理解，”在NAACL会议论文集中，2019年，页4171–4186，2019年。
- en: '(20) Y. Zhang, R. Jin, and Z.-H. Zhou, “Understanding bag-of-words model: a
    statistical framework,” International Journal of Machine Learning and Cybernetics,
    vol. 1, no. 1-4, pp. 43–52, 2010.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) Y. Zhang, R. Jin, 和 Z.-H. Zhou，“理解词袋模型：一种统计框架，”《国际机器学习与网络安全杂志》，卷1，第1-4期，页43–52，2010年。
- en: (21) W. B. Cavnar, J. M. Trenkle, et al., “N-gram-based text categorization,”
    in Proceedings of SDAIR-94, 3rd annual symposium on document analysis and information
    retrieval, vol. 161175, Citeseer, 1994.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) W. B. Cavnar, J. M. Trenkle等，“基于N-gram的文本分类，”在SDAIR-94的会议记录中，第3届文档分析与信息检索年会，卷161175，Citeseer，1994年。
- en: (22) “Term frequency by inverse document frequency,” in Encyclopedia of Database
    Systems, p. 3035, 2009.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) “通过逆文档频率的词频，”在《数据库系统百科全书》中，页3035，2009年。
- en: (23) T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” in Proc. ICLR, 2013, 2013.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) T. Mikolov, K. Chen, G. Corrado, 和 J. Dean，“在向量空间中高效估计词表示，”在ICLR会议论文集中，2013年，2013年。
- en: '(24) J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in Proc. EMNLP, 2014, pp. 1532–1543, 2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) J. Pennington, R. Socher, 和 C. D. Manning，“Glove: 全局词向量表示”，在EMNLP会议论文集中，2014年，第1532–1543页，2014年。'
- en: (25) M. Zhang and K. Zhang, “Multi-label learning by exploiting label dependency,”
    in Proc. ACM SIGKDD, 2010, pp. 999–1008, 2010.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) M. Zhang 和 K. Zhang，“通过利用标签依赖进行多标签学习”，在ACM SIGKDD会议论文集中，2010年，第999–1008页，2010年。
- en: (26) K. Schneider, “A new feature selection score for multinomial naive bayes
    text classification based on kl-divergence,” in Proc. ACL, 2004, 2004.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) K. Schneider，“基于KL散度的多项式朴素贝叶斯文本分类的新特征选择评分”，在ACL会议论文集中，2004年，2004年。
- en: '(27) T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley Series
    in Telecommunications and Signal Processing). USA: Wiley-Interscience, 2006.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) T. M. Cover 和 J. A. Thomas，信息论要素（Wiley电信与信号处理系列）。美国：Wiley-Interscience，2006年。
- en: (28) W. Dai, G. Xue, Q. Yang, and Y. Yu, “Transferring naive bayes classifiers
    for text classification,” in Proc. AAAI, 2007, pp. 540–545, 2007.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) W. Dai, G. Xue, Q. Yang, 和 Y. Yu，“将朴素贝叶斯分类器迁移到文本分类中”，在AAAI会议论文集中，2007年，第540–545页，2007年。
- en: (29) A., P., Dempster, N., M., Laird, D., B., and Rubin, “Maximum likelihood
    from incomplete data via the em algorithm,” Journal of the Royal Statistical Society,
    1977.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) A., P., Dempster, N., M., Laird, D., B., 和 Rubin，“通过EM算法从不完全数据中获得最大似然估计”，《皇家统计学会期刊》，1977年。
- en: (30) M. Granik and V. Mesyura, “Fake news detection using naive bayes classifier,”
    in 2017 IEEE First Ukraine Conference on Electrical and Computer Engineering (UKRCON),
    pp. 900–903, 2017.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) M. Granik 和 V. Mesyura，“使用朴素贝叶斯分类器检测虚假新闻”，在2017年IEEE第一次乌克兰电气与计算机工程会议（UKRCON）上，第900–903页，2017年。
- en: (31) M. S. Mubarok, K. Adiwijaya, and M. Aldhi, “Aspect-based sentiment analysis
    to review products using naïve bayes,” vol. 1867, p. 020060, 08 2017.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) M. S. Mubarok, K. Adiwijaya, 和 M. Aldhi，“基于方面的情感分析以审查产品，使用朴素贝叶斯”，第1867卷，第020060页，2017年8月。
- en: (32) S. Xu, “Bayesian naïve bayes classifiers to text classification,” J. Inf.
    Sci., vol. 44, no. 1, pp. 48–59, 2018.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) S. Xu，“贝叶斯朴素贝叶斯分类器用于文本分类”，信息科学杂志，第44卷，第1期，第48–59页，2018年。
- en: (33) G. Singh, B. Kumar, L. Gaur, and A. Tyagi, “Comparison between multinomial
    and bernoulli naïve bayes for text classification,” in 2019 International Conference
    on Automation, Computational and Technology Management (ICACTM), pp. 593–596,
    2019.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) G. Singh, B. Kumar, L. Gaur, 和 A. Tyagi，“多项式与伯努利朴素贝叶斯在文本分类中的比较”，在2019年国际自动化、计算和技术管理会议（ICACTM）上，第593–596页，2019年。
- en: (34) “20NG Corpus.” [http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization),
    2007.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) “20NG语料库。” [http://ana.cachopo.org/datasets-for-single-label-text-categorization](http://ana.cachopo.org/datasets-for-single-label-text-categorization)，2007年。
- en: (35) M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. M. Mitchell, K. Nigam,
    and S. Slattery, “Learning to extract symbolic knowledge from the world wide web,”
    in Proceedings of the Fifteenth National Conference on Artificial Intelligence
    and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI
    98, IAAI 98, July 26-30, 1998, Madison, Wisconsin, USA, pp. 509–516, 1998.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. M. Mitchell, K. Nigam,
    和 S. Slattery，“从万维网中提取符号知识的学习”，在第十五届国家人工智能会议和第十届人工智能创新应用会议论文集中，AAAI 98, IAAI 98，1998年7月26-30日，美国威斯康星州麦迪逊，第509–516页，1998年。
- en: (36) T. Jo, “Using k nearest neighbors for text segmentation with feature similarity,”
    in 2017 International Conference on Communication, Control, Computing and Electronics
    Engineering (ICCCCEE), pp. 1–5, 2017.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) T. Jo，“使用K最近邻进行特征相似性的文本分割”，在2017年国际通信、控制、计算与电子工程会议（ICCCCEE）上，第1–5页，2017年。
- en: (37) L. Baoli, L. Qin, and Y. Shiwen, “An adaptive ¡i¿k¡/i¿-nearest neighbor
    text categorization strategy,” ACM Transactions on Asian Language Information
    Processing, vol. 3, p. 215–226, Dec. 2004.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) L. Baoli, L. Qin, 和 Y. Shiwen，“一种自适应¡i¿k¡/i¿-最近邻文本分类策略”，《ACM亚洲语言信息处理学报》，第3卷，第215–226页，2004年12月。
- en: '(38) S. Chen, “K-nearest neighbor algorithm optimization in text categorization,”
    IOP Conference Series: Earth and Environmental Science, vol. 108, p. 052074, jan
    2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) S. Chen，“文本分类中的K最近邻算法优化”，IOP会议系列：地球与环境科学，第108卷，第052074页，2018年1月。
- en: (39) S. Jiang, G. Pang, M. Wu, and L. Kuang, “An improved k-nearest-neighbor
    algorithm for text categorization,” Expert Syst. Appl., vol. 39, no. 1, pp. 1503–1509,
    2012.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) S. Jiang, G. Pang, M. Wu, 和 L. Kuang，“一种改进的K最近邻算法用于文本分类”，《专家系统应用》，第39卷，第1期，第1503–1509页，2012年。
- en: (40) P. Soucy and G. W. Mineau, “A simple KNN algorithm for text categorization,”
    in Proc. ICDM, 2001, pp. 647–648, 2001.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) P. Soucy 和 G. W. Mineau，《用于文本分类的简单KNN算法》，发表于《ICDM会议录》，2001年，页码647–648，2001年。
- en: (41) S. Tan, “Neighbor-weighted k-nearest neighbor for unbalanced text corpus,”
    Expert Syst. Appl., vol. 28, no. 4, pp. 667–671, 2005.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) S. Tan，《用于不平衡文本语料库的邻居加权k最近邻算法》，《专家系统应用》，第28卷，第4期，页码667–671，2005年。
- en: (42) C. Cortes and V. Vapnik, “Support-vector networks,” Mach. Learn., vol. 20,
    no. 3, pp. 273–297, 1995.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (42) C. Cortes 和 V. Vapnik，《支持向量网络》，《机器学习》，第20卷，第3期，页码273–297，1995年。
- en: '(43) C. Leslie, E. Eskin, and W. S. Noble, “The spectrum kernel: A string kernel
    for svm protein classification,” in Biocomputing 2002, pp. 564–575, World Scientific,
    2001.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (43) C. Leslie, E. Eskin 和 W. S. Noble，《谱内核：用于SVM蛋白质分类的字符串内核》，发表于《生物计算2002》，页码564–575，世界科学出版社，2001年。
- en: (44) H. Taira and M. Haruno, “Feature selection in svm text categorization,”
    in AAAI/IAAI, pp. 480–486, 1999.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) H. Taira 和 M. Haruno，《SVM文本分类中的特征选择》，发表于《AAAI/IAAI会议录》，页码480–486，1999年。
- en: (45) X. Li and Y. Guo, “Active learning with multi-label svm classification.,”
    in IjCAI, pp. 1479–1485, Citeseer, 2013.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (45) X. Li 和 Y. Guo，《多标签SVM分类的主动学习》，发表于《IjCAI会议录》，页码1479–1485，Citeseer，2013年。
- en: (46) T. Peng, W. Zuo, and F. He, “Svm based adaptive learning method for text
    classification from positive and unlabeled documents,” Knowledge and Information
    Systems, vol. 16, no. 3, pp. 281–301, 2008.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) T. Peng, W. Zuo 和 F. He，《基于SVM的适应性学习方法用于正样本和未标记文档的文本分类》，《知识与信息系统》，第16卷，第3期，页码281–301，2008年。
- en: (47) T. Joachims, “A statistical learning model of text classification for support
    vector machines,” in Proc. SIGIR, 2001, pp. 128–136, 2001.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) T. Joachims，《支持向量机的文本分类统计学习模型》，发表于《SIGIR会议录》，2001年，页码128–136，2001年。
- en: (48) T. JOACHIMS, “Transductive inference for text classification using support
    vector macines,” in International Conference on Machine Learning, 1999.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) T. JOACHIMS，《使用支持向量机的传导推断进行文本分类》，发表于《国际机器学习会议》，1999年。
- en: (49) T. M. Mitchell, Machine learning. McGraw Hill series in computer science,
    McGraw-Hill, 1997.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) T. M. Mitchell，《机器学习》。麦格劳-希尔计算机科学系列，麦格劳-希尔，1997年。
- en: '(50) R. Rastogi and K. Shim, “PUBLIC: A decision tree classifier that integrates
    building and pruning,” Data Min. Knowl. Discov., vol. 4, no. 4, pp. 315–344, 2000.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) R. Rastogi 和 K. Shim，《PUBLIC：一个集成构建和剪枝的决策树分类器》，《数据挖掘与知识发现》，第4卷，第4期，页码315–344，2000年。
- en: (51) R. J. Quinlan, “Induction of decision trees,” Machine Learning, vol. 1,
    no. 1, pp. 81–106, 1986.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) R. J. Quinlan，《决策树的归纳》，《机器学习》，第1卷，第1期，页码81–106，1986年。
- en: '(52) J. R. Quinlan, C4.5: Programs for Machine Learning. San Francisco, CA,
    USA: Morgan Kaufmann Publishers Inc., 1993.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) J. R. Quinlan，《C4.5：机器学习程序》。旧金山，加利福尼亚州，美国：摩根·考夫曼出版社，1993年。
- en: '(53) M. Kamber, L. Winstone, W. Gong, S. Cheng, and J. Han, “Generalization
    and decision tree induction: efficient classification in data mining,” in Proceedings
    Seventh International Workshop on Research Issues in Data Engineering. High Performance
    Database Management for Large-Scale Applications, pp. 111–120, IEEE, 1997.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) M. Kamber, L. Winstone, W. Gong, S. Cheng 和 J. Han，《泛化和决策树归纳：数据挖掘中的高效分类》，发表于《第七届国际数据工程研究问题研讨会：大规模应用的高性能数据库管理》，页码111–120，IEEE，1997年。
- en: (54) D. E. Johnson, F. J. Oles, T. Zhang, and T. Götz, “A decision-tree-based
    symbolic rule induction system for text categorization,” IBM Syst. J., vol. 41,
    no. 3, pp. 428–437, 2002.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) D. E. Johnson, F. J. Oles, T. Zhang 和 T. Götz，《基于决策树的符号规则归纳系统用于文本分类》，《IBM系统杂志》，第41卷，第3期，页码428–437，2002年。
- en: (55) P. Vateekul and M. Kubat, “Fast induction of multiple decision trees in
    text categorization from large scale, imbalanced, and multi-label data,” in Proc.
    ICDM Workshops, 2009, pp. 320–325, 2009.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) P. Vateekul 和 M. Kubat，《在大规模、不平衡和多标签数据中的文本分类中快速归纳多个决策树》，发表于《ICDM工作坊会议录》，2009年，页码320–325，2009年。
- en: (56) Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line
    learning and an application to boosting,” in Proc. EuroCOLT, 1995, pp. 23–37,
    1995.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Y. Freund 和 R. E. Schapire，《在线学习的决策理论泛化及其在提升中的应用》，发表于《EuroCOLT会议录》，1995年，页码23–37，1995年。
- en: (57) R. E. Schapire and Y. Singer, “Improved boosting algorithms using confidence-rated
    predictions,” Mach. Learn., vol. 37, no. 3, pp. 297–336, 1999.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) R. E. Schapire 和 Y. Singer，《使用置信度评分预测的改进提升算法》，《机器学习》，第37卷，第3期，页码297–336，1999年。
- en: (58) A. Bouaziz, C. Dartigues-Pallez, C. da Costa Pereira, F. Precioso, and
    P. Lloret, “Short text classification using semantic random forest,” in Proc.
    DAWAK, 2014, pp. 288–299, 2014.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) A. Bouaziz, C. Dartigues-Pallez, C. da Costa Pereira, F. Precioso 和 P.
    Lloret，《使用语义随机森林的短文本分类》，发表于《DAWAK会议录》，2014年，页码288–299，2014年。
- en: (59) M. Z. Islam, J. Liu, J. Li, L. Liu, and W. Kang, “A semantics aware random
    forest for text classification,” in Proc. CIKM, 2019, pp. 1061–1070, 2019.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) M. Z. Islam, J. Liu, J. Li, L. Liu, 和 W. Kang，“一种语义感知的随机森林用于文本分类，”发表于 Proc.
    CIKM，2019年，页码1061–1070，2019年。
- en: (60) R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, “Semi-supervised
    recursive autoencoders for predicting sentiment distributions,” in Proc. EMNLP,
    2011, pp. 151–161, 2011.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, 和 C. D. Manning，“用于预测情感分布的半监督递归自编码器，”发表于
    Proc. EMNLP，2011年，页码151–161，2011年。
- en: (61) “A MATLAB implementation of RAE.” [https://github.com/vin00/Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions](https://github.com/vin00/Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions),
    2011.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) “RAE 的 MATLAB 实现。” [https://github.com/vin00/Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions](https://github.com/vin00/Semi-Supervised-Recursive-Autoencoders-for-Predicting-Sentiment-Distributions)，2011年。
- en: (62) R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compositionality
    through recursive matrix-vector spaces,” in Proc. EMNLP, 2012, pp. 1201–1211,
    2012.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) R. Socher, B. Huval, C. D. Manning, 和 A. Y. Ng，“通过递归矩阵-向量空间的语义组合性，”发表于
    Proc. EMNLP，2012年，页码1201–1211，2012年。
- en: (63) “A Tensorflow implementation of MV_RNN.” [https://github.com/github-pengge/MV_RNN](https://github.com/github-pengge/MV_RNN),
    2012.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) “MV_RNN 的 Tensorflow 实现。” [https://github.com/github-pengge/MV_RNN](https://github.com/github-pengge/MV_RNN)，2012年。
- en: (64) R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and
    C. Potts, “Recursive deep models for semantic compositionality over a sentiment
    treebank,” in Proc. EMNLP, 2013, pp. 1631–1642, 2013.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, 和 C. Potts，“用于情感树库的语义组合性递归深度模型，”发表于
    Proc. EMNLP，2013年，页码1631–1642，2013年。
- en: (65) “A MATLAB implementation of RNTN.” [https://github.com/pondruska/DeepSentiment](https://github.com/pondruska/DeepSentiment),
    2013.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) “RNTN 的 MATLAB 实现。” [https://github.com/pondruska/DeepSentiment](https://github.com/pondruska/DeepSentiment)，2013年。
- en: (66) O. Irsoy and C. Cardie, “Deep recursive neural networks for compositionality
    in language,” in Proc. NIPS, 2014, pp. 2096–2104, 2014.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) O. Irsoy 和 C. Cardie，“用于语言组合性的深度递归神经网络，”发表于 Proc. NIPS，2014年，页码2096–2104，2014年。
- en: (67) Q. V. Le and T. Mikolov, “Distributed representations of sentences and
    documents,” in Proc. ICML, 2014, pp. 1188–1196, 2014.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) Q. V. Le 和 T. Mikolov，“句子和文档的分布式表示，”发表于 Proc. ICML，2014年，页码1188–1196，2014年。
- en: (68) “A PyTorch implementation of Paragraph Vectors (doc2vec).” [https://github.com/inejc/paragraph-vectors](https://github.com/inejc/paragraph-vectors),
    2014.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) “Paragraph Vectors (doc2vec) 的 PyTorch 实现。” [https://github.com/inejc/paragraph-vectors](https://github.com/inejc/paragraph-vectors)，2014年。
- en: (69) M. Iyyer, V. Manjunatha, J. L. Boyd-Graber, and H. D. III, “Deep unordered
    composition rivals syntactic methods for text classification,” in Proc. ACL, 2015,
    pp. 1681–1691, 2015.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) M. Iyyer, V. Manjunatha, J. L. Boyd-Graber, 和 H. D. III，“深度无序组合方法与句法方法在文本分类中的竞争，”发表于
    Proc. ACL，2015年，页码1681–1691，2015年。
- en: (70) “An implementation of DAN.” [https://github.com/miyyer/dan](https://github.com/miyyer/dan),
    2015.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) “DAN 的实现。” [https://github.com/miyyer/dan](https://github.com/miyyer/dan)，2015年。
- en: (71) “A PyTorch implementation of Tree-LSTM.” [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm),
    2015.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) “Tree-LSTM 的 PyTorch 实现。” [https://github.com/stanfordnlp/treelstm](https://github.com/stanfordnlp/treelstm)，2015年。
- en: (72) S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural networks
    for text classification,” AAAI’15, p. 2267–2273, AAAI Press, 2015.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) S. Lai, L. Xu, K. Liu, 和 J. Zhao，“用于文本分类的递归卷积神经网络，”AAAI’15，页码2267–2273，AAAI
    Press，2015年。
- en: (73) “A Tensorflow implementation of TextRCNN.” [https://github.com/roomylee/rcnn-text-classification](https://github.com/roomylee/rcnn-text-classification),
    2015.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) “TextRCNN 的 Tensorflow 实现。” [https://github.com/roomylee/rcnn-text-classification](https://github.com/roomylee/rcnn-text-classification)，2015年。
- en: (74) “An implementation of MT-LSTM.” [https://github.com/AlexAntn/MTLSTM](https://github.com/AlexAntn/MTLSTM),
    2015.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) “MT-LSTM 的实现。” [https://github.com/AlexAntn/MTLSTM](https://github.com/AlexAntn/MTLSTM)，2015年。
- en: (75) R. Johnson and T. Zhang, “Supervised and semi-supervised text categorization
    using LSTM for region embeddings,” in Proc. ICML, 2016, pp. 526–534, 2016.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) R. Johnson 和 T. Zhang，“使用 LSTM 进行区域嵌入的监督和半监督文本分类，”发表于 Proc. ICML，2016年，页码526–534，2016年。
- en: (76) “An implementation of oh-2LSTMp.” [http://riejohnson.com/cnn_20download.html](http://riejohnson.com/cnn_20download.html),
    2015.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) “oh-2LSTMp 的实现。” [http://riejohnson.com/cnn_20download.html](http://riejohnson.com/cnn_20download.html)，2015年。
- en: (77) P. Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu, “Text classification
    improved by integrating bidirectional LSTM with two-dimensional max pooling,”
    in Proc. COLING, 2016, pp. 3485–3495, 2016.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) P. Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, 和 B. Xu, “通过将双向 LSTM 与二维最大池化结合改进文本分类，”
    见 Proc. COLING, 2016，第3485–3495页，2016年。
- en: (78) “An implementation of BLSTM-2DCNN.” [https://github.com/ManuelVs/NNForTextClassification](https://github.com/ManuelVs/NNForTextClassification),
    2016.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (78) “BLSTM-2DCNN 的实现。” [https://github.com/ManuelVs/NNForTextClassification](https://github.com/ManuelVs/NNForTextClassification)，2016。
- en: (79) P. Liu, X. Qiu, and X. Huang, “Recurrent neural network for text classification
    with multi-task learning,” in Proc. IJCAI, 2016, pp. 2873–2879, 2016.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) P. Liu, X. Qiu, 和 X. Huang, “具有多任务学习的文本分类递归神经网络，” 见 Proc. IJCAI, 2016，第2873–2879页，2016年。
- en: (80) “A PyTorch implementation of Multi-Task.” [https://github.com/baixl/text_classification](https://github.com/baixl/text_classification),
    2016.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) “PyTorch 实现的 Multi-Task。” [https://github.com/baixl/text_classification](https://github.com/baixl/text_classification)，2016。
- en: (81) B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann, “Using millions
    of emoji occurrences to learn any-domain representations for detecting sentiment,
    emotion and sarcasm,” in Proc. EMNLP, 2017, pp. 1615–1625, 2017.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, 和 S. Lehmann, “利用数百万次表情符号出现来学习任何领域的表示，以检测情感、情绪和讽刺，”
    见 Proc. EMNLP, 2017，第1615–1625页，2017年。
- en: (82) “A Keras implementation of DeepMoji.” [https://github.com/bfelbo/DeepMoji](https://github.com/bfelbo/DeepMoji),
    2018.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) “Keras 实现的 DeepMoji。” [https://github.com/bfelbo/DeepMoji](https://github.com/bfelbo/DeepMoji)，2018。
- en: '(83) A. B. Dieng, C. Wang, J. Gao, and J. W. Paisley, “Topicrnn: A recurrent
    neural network with long-range semantic dependency,” in Proc. ICLR, 2017, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(83) A. B. Dieng, C. Wang, J. Gao, 和 J. W. Paisley, “Topicrnn: 一种具有长程语义依赖的递归神经网络，”
    见 Proc. ICLR, 2017，2017年。'
- en: (84) “A PyTorch implementation of TopicRNN.” [https://github.com/dangitstam/topic-rnn](https://github.com/dangitstam/topic-rnn),
    2017.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (84) “PyTorch 实现的 TopicRNN。” [https://github.com/dangitstam/topic-rnn](https://github.com/dangitstam/topic-rnn)，2017。
- en: (85) T. Miyato, A. M. Dai, and I. J. Goodfellow, “Adversarial training methods
    for semi-supervised text classification,” in Proc. ICLR, 2017, 2017.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) T. Miyato, A. M. Dai, 和 I. J. Goodfellow, “用于半监督文本分类的对抗训练方法，” 见 Proc. ICLR,
    2017，2017年。
- en: (86) “A Tensorflow implementation of Virtual adversarial training.” [https://github.com/tensorflow/models/tree/master/adversarial_text](https://github.com/tensorflow/models/tree/master/adversarial_text),
    2017.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) “Tensorflow 实现的虚拟对抗训练。” [https://github.com/tensorflow/models/tree/master/adversarial_text](https://github.com/tensorflow/models/tree/master/adversarial_text)，2017。
- en: (87) Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, “Sentiment analysis by capsules,”
    in Proc. WWW, 2018, pp. 1165–1174, 2018.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (87) Y. Wang, A. Sun, J. Han, Y. Liu, 和 X. Zhu, “通过胶囊进行情感分析，” 见 Proc. WWW, 2018,
    第1165–1174页，2018年。
- en: (88) “A PyTorch implementation of RNN-Capsule.” [https://github.com/wangjiosw/Sentiment-Analysis-by-Capsules](https://github.com/wangjiosw/Sentiment-Analysis-by-Capsules),
    2018.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (88) “PyTorch 实现的 RNN-Capsule。” [https://github.com/wangjiosw/Sentiment-Analysis-by-Capsules](https://github.com/wangjiosw/Sentiment-Analysis-by-Capsules)，2018。
- en: (89) Y. Zhao, Y. Shen, and J. Yao, “Recurrent neural network for text classification
    with hierarchical multiscale dense connections,” in Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019, pp. 5450–5456, 2019.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (89) Y. Zhao, Y. Shen, 和 J. Yao, “具有层次多尺度密集连接的文本分类递归神经网络，” 见第二十八届国际人工智能联合会议，IJCAI
    2019，澳门，中国，2019年8月10-16日，第5450–5456页，2019年。
- en: (90) “An implementation of HM-DenseRNNs.” [https://github.com/zhaoyizhaoyi/hm-densernns](https://github.com/zhaoyizhaoyi/hm-densernns),
    2019.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) “HM-DenseRNNs 的实现。” [https://github.com/zhaoyizhaoyi/hm-densernns](https://github.com/zhaoyizhaoyi/hm-densernns)，2019。
- en: (91) “A Keras implementation of TextCNN.” [https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras](https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras),
    2014.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (91) “Keras 实现的 TextCNN。” [https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras](https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras)，2014。
- en: (92) “A Tensorflow implementation of DCNN.” [https://github.com/kinimod23/ATS_Project](https://github.com/kinimod23/ATS_Project),
    2014.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) “Tensorflow 实现的 DCNN。” [https://github.com/kinimod23/ATS_Project](https://github.com/kinimod23/ATS_Project)，2014。
- en: (93) X. Zhang, J. J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in Proc. NeurIPS, 2015, pp. 649–657, 2015.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) X. Zhang, J. J. Zhao, 和 Y. LeCun, “字符级卷积网络用于文本分类，” 见 Proc. NeurIPS, 2015，第649–657页，2015年。
- en: (94) “A Tensorflow implementation of CharCNN.” [https://github.com/mhjabreel/CharCNN](https://github.com/mhjabreel/CharCNN),
    2015.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) “CharCNN 的 Tensorflow 实现。” [https://github.com/mhjabreel/CharCNN](https://github.com/mhjabreel/CharCNN),
    2015。
- en: (95) “A Keras implementation of SeqTextRCNN.” [https://github.com/ilimugur/short-text-classification](https://github.com/ilimugur/short-text-classification),
    2016.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) “SeqTextRCNN 的 Keras 实现。” [https://github.com/ilimugur/short-text-classification](https://github.com/ilimugur/short-text-classification),
    2016。
- en: (96) J. Liu, W. Chang, Y. Wu, and Y. Yang, “Deep learning for extreme multi-label
    text classification,” in Proc. ACM SIGIR, 2017, pp. 115–124, 2017.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) J. Liu, W. Chang, Y. Wu, 和 Y. Yang, “用于极端多标签文本分类的深度学习，” 收录于 Proc. ACM SIGIR,
    2017, 页码115–124, 2017。
- en: (97) “A Pytorch implementation of XML-CNN.” [https://github.com/siddsax/XML-CNN](https://github.com/siddsax/XML-CNN),
    2017.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) “XML-CNN 的 Pytorch 实现。” [https://github.com/siddsax/XML-CNN](https://github.com/siddsax/XML-CNN),
    2017。
- en: (98) R. Johnson and T. Zhang, “Deep pyramid convolutional neural networks for
    text categorization,” in Proc. ACL, 2017, pp. 562–570, 2017.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) R. Johnson 和 T. Zhang, “用于文本分类的深度金字塔卷积神经网络，” 收录于 Proc. ACL, 2017, 页码562–570,
    2017。
- en: (99) “A PyTorch implementation of DPCNN.” [https://github.com/Cheneng/DPCNN](https://github.com/Cheneng/DPCNN),
    2017.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) “DPCNN 的 PyTorch 实现。” [https://github.com/Cheneng/DPCNN](https://github.com/Cheneng/DPCNN),
    2017。
- en: (100) J. Wang, Z. Wang, D. Zhang, and J. Yan, “Combining knowledge with deep
    convolutional neural networks for short text classification,” in Proc. IJCAI,
    2017, pp. 2915–2921, 2017.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) J. Wang, Z. Wang, D. Zhang, 和 J. Yan, “将知识与深度卷积神经网络结合进行短文本分类，” 收录于 Proc.
    IJCAI, 2017, 页码2915–2921, 2017。
- en: (101) M. Yang, W. Zhao, J. Ye, Z. Lei, Z. Zhao, and S. Zhang, “Investigating
    capsule networks with dynamic routing for text classification,” in Proceedings
    of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,
    Belgium, October 31 - November 4, 2018, pp. 3110–3119, 2018.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) M. Yang, W. Zhao, J. Ye, Z. Lei, Z. Zhao, 和 S. Zhang, “探索具有动态路由的胶囊网络用于文本分类，”
    收录于2018年自然语言处理经验方法会议，布鲁塞尔，比利时，2018年10月31日 - 11月4日，页码3110–3119, 2018。
- en: (102) “A Tensorflow implementation of TextCapsule.” [https://github.com/andyweizhao/capsule_text_classification](https://github.com/andyweizhao/capsule_text_classification),
    2018.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) “TextCapsule 的 Tensorflow 实现。” [https://github.com/andyweizhao/capsule_text_classification](https://github.com/andyweizhao/capsule_text_classification),
    2018。
- en: '(103) K. Shimura, J. Li, and F. Fukumoto, “HFT-CNN: learning hierarchical category
    structure for multi-label short text categorization,” in Proc. EMNLP, 2018, pp. 811–816,
    2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(103) K. Shimura, J. Li, 和 F. Fukumoto, “HFT-CNN: 学习多标签短文本分类的层次类别结构，” 收录于 Proc.
    EMNLP, 2018, 页码811–816, 2018。'
- en: (104) “An implementation of HFT-CNN.” [https://github.com/ShimShim46/HFT-CNN](https://github.com/ShimShim46/HFT-CNN),
    2018.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) “HFT-CNN 的实现。” [https://github.com/ShimShim46/HFT-CNN](https://github.com/ShimShim46/HFT-CNN),
    2018。
- en: (105) J. Xu and Y. Cai, “Incorporating context-relevant knowledge into convolutional
    neural networks for short text classification,” in The Thirty-Third AAAI Conference
    on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications
    of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,
    USA, January 27 - February 1, 2019, pp. 10067–10068, 2019.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) J. Xu 和 Y. Cai, “将上下文相关知识融入卷积神经网络以进行短文本分类，” 收录于第三十三届 AAAI 人工智能会议，AAAI
    2019，第三十一届创新应用人工智能会议，IAAI 2019，第九届 AAAI 教育进展人工智能研讨会，EAAI 2019, 夏威夷檀香山，美国，2019年1月27日
    - 2月1日，页码10067–10068, 2019。
- en: (106) Y. Bao, M. Wu, S. Chang, and R. Barzilay, “Few-shot text classification
    with distributional signatures,” in Proc. ICLR, 2020, 2020.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Y. Bao, M. Wu, S. Chang, 和 R. Barzilay, “基于分布签名的少样本文本分类，” 收录于 Proc. ICLR,
    2020, 2020。
- en: (107) “A PyTorch implementation of few-shot text classification with distributional
    signatures.” [https://github.com/YujiaBao/Distributional-Signatures](https://github.com/YujiaBao/Distributional-Signatures),
    2020.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) “基于分布签名的少样本文本分类的 PyTorch 实现。” [https://github.com/YujiaBao/Distributional-Signatures](https://github.com/YujiaBao/Distributional-Signatures),
    2020。
- en: (108) Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy, “Hierarchical
    attention networks for document classification,” in Proc. NAACL, 2016, pp. 1480–1489,
    2016.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, 和 E. H. Hovy, “用于文档分类的层次注意力网络，”
    收录于 Proc. NAACL, 2016, 页码1480–1489, 2016。
- en: (109) “A Keras implementation of TextCNN.” [https://github.com/richliao/textClassifier](https://github.com/richliao/textClassifier),
    2014.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (109) “TextCNN 的 Keras 实现。” [https://github.com/richliao/textClassifier](https://github.com/richliao/textClassifier),
    2014。
- en: (110) X. Zhou, X. Wan, and J. Xiao, “Attention-based LSTM network for cross-lingual
    sentiment classification,” in Proc. EMNLP, 2016, pp. 247–256, 2016.
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (110) X. Zhou, X. Wan, 和 J. Xiao，“基于注意力的LSTM网络用于跨语言情感分类”，发表于EMNLP会议，2016年，pp.
    247–256，2016年。
- en: (111) “NLP&CC Corpus.” [http://tcci.ccf.org.cn/conference/2013/index.html](http://tcci.ccf.org.cn/conference/2013/index.html),
    2013.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (111) “NLP&CC语料库。” [http://tcci.ccf.org.cn/conference/2013/index.html](http://tcci.ccf.org.cn/conference/2013/index.html)，2013年。
- en: (112) J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks for
    machine reading,” in Proc. EMNLP, 2016, pp. 551–561, 2016.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (112) J. Cheng, L. Dong, 和 M. Lapata，“用于机器阅读的长短期记忆网络”，发表于EMNLP会议，2016年，pp. 551–561，2016年。
- en: (113) “A Tensorflow implementation of LSTMN.” [https://github.com/JRC1995/Abstractive-Summarization](https://github.com/JRC1995/Abstractive-Summarization),
    2016.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (113) “LSTMN的Tensorflow实现。” [https://github.com/JRC1995/Abstractive-Summarization](https://github.com/JRC1995/Abstractive-Summarization)，2016年。
- en: (114) Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio,
    “A structured self-attentive sentence embedding,” in Proc. ICLR, 2017, 2017.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (114) Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, 和 Y. Bengio，“一种结构化自注意句子嵌入”，发表于ICLR会议，2017年，2017年。
- en: (115) “A PyTorch implementation of Structured-Self-Attention.” [https://github.com/kaushalshetty/Structured-Self-Attention](https://github.com/kaushalshetty/Structured-Self-Attention),
    2017.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (115) “Structured-Self-Attention的PyTorch实现。” [https://github.com/kaushalshetty/Structured-Self-Attention](https://github.com/kaushalshetty/Structured-Self-Attention)，2017年。
- en: '(116) P. Yang, X. Sun, W. Li, S. Ma, W. Wu, and H. Wang, “SGM: sequence generation
    model for multi-label classification,” in Proc. COLING, 2018, pp. 3915–3926, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (116) P. Yang, X. Sun, W. Li, S. Ma, W. Wu, 和 H. Wang，“SGM：用于多标签分类的序列生成模型”，发表于COLING会议，2018年，pp.
    3915–3926，2018年。
- en: (117) “A PyTorch implementation of SGM.” [https://github.com/lancopku/SGM](https://github.com/lancopku/SGM),
    2018.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (117) “SGM的PyTorch实现。” [https://github.com/lancopku/SGM](https://github.com/lancopku/SGM)，2018年。
- en: (118) M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” in Proc. NAACL, 2018,
    pp. 2227–2237, 2018.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, 和 L.
    Zettlemoyer，“深度上下文化词表示”，发表于NAACL会议，2018年，pp. 2227–2237，2018年。
- en: (119) “A PyTorch implementation of ELMo.” [https://github.com/flairNLP/flair](https://github.com/flairNLP/flair),
    2018.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (119) “ELMo的PyTorch实现。” [https://github.com/flairNLP/flair](https://github.com/flairNLP/flair)，2018年。
- en: (120) T. Shen, T. Zhou, G. Long, J. Jiang, and C. Zhang, “Bi-directional block
    self-attention for fast and memory-efficient sequence modeling,” in Proc. ICLR,
    2018, 2018.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (120) T. Shen, T. Zhou, G. Long, J. Jiang, 和 C. Zhang，“双向块自注意力用于快速和内存高效的序列建模”，发表于ICLR会议，2018年，2018年。
- en: (121) “A PyTorch implementation of BiBloSA.” [https://github.com/galsang/BiBloSA-pytorch](https://github.com/galsang/BiBloSA-pytorch),
    2018.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (121) “BiBloSA的PyTorch实现。” [https://github.com/galsang/BiBloSA-pytorch](https://github.com/galsang/BiBloSA-pytorch)，2018年。
- en: '(122) R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu, “Attentionxml:
    Label tree-based attention-aware deep model for high-performance extreme multi-label
    text classification,” in Proc. NeurIPS, 2019, pp. 5812–5822, 2019.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (122) R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, 和 S. Zhu，“Attentionxml：基于标签树的注意力感知深度模型用于高性能极端多标签文本分类”，发表于NeurIPS会议，2019年，pp.
    5812–5822，2019年。
- en: (123) “A PyTorch implementation of AttentionXML.” [https://github.com/yourh/AttentionXML](https://github.com/yourh/AttentionXML),
    2019.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (123) “AttentionXML的PyTorch实现。” [https://github.com/yourh/AttentionXML](https://github.com/yourh/AttentionXML)，2019年。
- en: (124) S. Sun, Q. Sun, K. Zhou, and T. Lv, “Hierarchical attention prototypical
    networks for few-shot text classification,” in Proc. EMNLP, 2019, pp. 476–485,
    2019.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (124) S. Sun, Q. Sun, K. Zhou, 和 T. Lv，“用于少样本文本分类的层次化注意力原型网络”，发表于EMNLP会议，2019年，pp.
    476–485，2019年。
- en: (125) T. Gao, X. Han, Z. Liu, and M. Sun, “Hybrid attention-based prototypical
    networks for noisy few-shot relation classification,” in Proc. AAAI, 2019, pp. 6407–6414,
    2019.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (125) T. Gao, X. Han, Z. Liu, 和 M. Sun，“基于混合注意力的原型网络用于嘈杂的少样本关系分类”，发表于AAAI会议，2019年，pp.
    6407–6414，2019年。
- en: (126) “A PyTorch implementation of HATT-Proto.” [https://github.com/thunlp/HATT-Proto](https://github.com/thunlp/HATT-Proto),
    2019.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (126) “HATT-Proto的PyTorch实现。” [https://github.com/thunlp/HATT-Proto](https://github.com/thunlp/HATT-Proto)，2019年。
- en: (127) J. Chen, Y. Hu, J. Liu, Y. Xiao, and H. Jiang, “Deep short text classification
    with knowledge powered attention,” in Proc. AAAI, 2019, pp. 6252–6259, 2019.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (127) J. Chen, Y. Hu, J. Liu, Y. Xiao, 和 H. Jiang，“基于知识驱动注意力的深度短文本分类”，发表于AAAI会议，2019年，pp.
    6252–6259，2019年。
- en: (128) “A PyTorch implementation of STCKA.” [https://github.com/AIRobotZhang/STCKA](https://github.com/AIRobotZhang/STCKA),
    2019.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (128) “STCKA的PyTorch实现。” [https://github.com/AIRobotZhang/STCKA](https://github.com/AIRobotZhang/STCKA)，2019年。
- en: '(129) K. Ding, J. Wang, J. Li, D. Li, and H. Liu, “Be more with less: Hypergraph
    attention networks for inductive text classification,” in Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
    November 16-20, 2020, pp. 4927–4936, 2020.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (129) K. Ding, J. Wang, J. Li, D. Li, 和 H. Liu, “以少胜多：用于归纳文本分类的超图注意力网络”，发表于
    2020 年自然语言处理实证方法会议，EMNLP 2020, 在线, 2020年11月16-20日, 第4927–4936页, 2020年。
- en: (130) “A pytorch implementation of HyperGAT.” [https://github.com/kaize0409/HyperGAT](https://github.com/kaize0409/HyperGAT),
    2020.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (130) “HyperGAT 的 PyTorch 实现。” [https://github.com/kaize0409/HyperGAT](https://github.com/kaize0409/HyperGAT),
    2020年。
- en: (131) Q. Guo, X. Qiu, P. Liu, X. Xue, and Z. Zhang, “Multi-scale self-attention
    for text classification,” in The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7847–7854,
    2020.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (131) Q. Guo, X. Qiu, P. Liu, X. Xue, 和 Z. Zhang, “用于文本分类的多尺度自注意力”，发表于 第三十四届
    AAAI 人工智能会议, AAAI 2020, 第三十二届人工智能创新应用会议, IAAI 2020, 第十届 AAAI 教育进展研讨会, EAAI 2020,
    美国纽约, 2020年2月7-12日, 第7847–7854页, 2020年。
- en: '(132) S. Choi, H. Park, J. Yeo, and S. Hwang, “Less is more: Attention supervision
    with counterfactuals for text classification,” in Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November
    16-20, 2020, pp. 6695–6704, 2020.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (132) S. Choi, H. Park, J. Yeo, 和 S. Hwang, “少即是多：用于文本分类的对抗性注意监督”，发表于 2020 年自然语言处理实证方法会议，EMNLP
    2020, 在线, 2020年11月16-20日, 第6695–6704页, 2020年。
- en: (133) “A Tensorflow implementation of BERT.” [https://github.com/google-research/bert](https://github.com/google-research/bert),
    2019.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (133) “BERT 的 TensorFlow 实现。” [https://github.com/google-research/bert](https://github.com/google-research/bert),
    2019年。
- en: (134) I. Chalkidis, M. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos,
    “Large-scale multi-label text classification on EU legislation,” in Proc. ACL,
    2019, pp. 6314–6322, 2019.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (134) I. Chalkidis, M. Fergadiotis, P. Malakasiotis, 和 I. Androutsopoulos, “EU
    法规的大规模多标签文本分类”，发表于 Proc. ACL, 2019, 第6314–6322页, 2019年。
- en: (135) “A Tensorflow implementation of BERT-BASE.” [https://github.com/iliaschalkidis/lmtc-eurlex57k](https://github.com/iliaschalkidis/lmtc-eurlex57k),
    2019.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (135) “BERT-BASE 的 TensorFlow 实现。” [https://github.com/iliaschalkidis/lmtc-eurlex57k](https://github.com/iliaschalkidis/lmtc-eurlex57k),
    2019年。
- en: (136) C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to fine-tune BERT for text classification?,”
    in Proc. CCL, 2019, pp. 194–206, 2019.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (136) C. Sun, X. Qiu, Y. Xu, 和 X. Huang, “如何对 BERT 进行微调以进行文本分类？”，发表于 Proc. CCL,
    2019, 第194–206页, 2019年。
- en: (137) “A Tensorflow implementation of BERT4doc-Classification.” [https://github.com/xuyige/BERT4doc-Classification](https://github.com/xuyige/BERT4doc-Classification),
    2019.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (137) “BERT4doc-Classification 的 TensorFlow 实现。” [https://github.com/xuyige/BERT4doc-Classification](https://github.com/xuyige/BERT4doc-Classification),
    2019年。
- en: '(138) Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V.
    Le, “Xlnet: Generalized autoregressive pretraining for language understanding,”
    in Proc. NeurIPS, 2019, pp. 5754–5764, 2019.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (138) Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, 和 Q. V. Le,
    “XLNet：用于语言理解的广义自回归预训练”，发表于 Proc. NeurIPS, 2019, 第5754–5764页, 2019年。
- en: (139) “A Tensorflow implementation of XLNet.” [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet),
    2019.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (139) “XLNet 的 TensorFlow 实现。” [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet),
    2019年。
- en: '(140) Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized BERT pretraining
    approach,” CoRR, vol. abs/1907.11692, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (140) Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, 和 V. Stoyanov, “Roberta：一种强健优化的 BERT 预训练方法”，CoRR, 卷abs/1907.11692,
    2019年。
- en: (141) “A PyTorch implementation of RoBERTa.” [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq),
    2019.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (141) “RoBERTa 的 PyTorch 实现。” [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq),
    2019年。
- en: '(142) D. Croce, G. Castellucci, and R. Basili, “GAN-BERT: generative adversarial
    learning for robust text classification with a bunch of labeled examples,” in
    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
    ACL 2020, Online, July 5-10, 2020, pp. 2114–2119, 2020.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (142) D. Croce, G. Castellucci, 和 R. Basili, “GAN-BERT：用于鲁棒文本分类的生成对抗学习”，发表于
    第58届计算语言学协会年会，ACL 2020, 在线, 2020年7月5-10日, 第2114–2119页, 2020年。
- en: (143) “A pytorch implementation of GAN-BERT.” [https://github.com/crux82/ganbert](https://github.com/crux82/ganbert),
    2020.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (143) “GAN-BERT 的 PyTorch 实现。” [https://github.com/crux82/ganbert](https://github.com/crux82/ganbert),
    2020年。
- en: '(144) S. Garg and G. Ramakrishnan, “BAE: bert-based adversarial examples for
    text classification,” in Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6174–6181,
    2020.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(144) S. Garg 和 G. Ramakrishnan，“BAE: 基于 BERT 的文本分类对抗样本，”收录于《2020 年自然语言处理实证方法会议论文集》，EMNLP
    2020，在线，2020 年 11 月 16-20 日，第 6174–6181 页，2020。'
- en: (145) “An implementation of BAE.” [https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py](https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py),
    2020.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (145) “BAE 的实现。” [https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py](https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_2019.py)，2020。
- en: '(146) Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT:
    A lite BERT for self-supervised learning of language representations,” in Proc.
    ICLR, 2020, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(146) Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma 和 R. Soricut，“ALBERT:
    一种轻量 BERT 用于自监督语言表示学习，”收录于《ICLR 会议论文集》，2020，2020。'
- en: (147) “A Tensorflow implementation of ALBERT.” [https://github.com/google-research/ALBERT](https://github.com/google-research/ALBERT),
    2020.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (147) “ALBERT 的 Tensorflow 实现。” [https://github.com/google-research/ALBERT](https://github.com/google-research/ALBERT)，2020。
- en: (148) H. Zhang and J. Zhang, “Text graph transformer for document classification,”
    in Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 8322–8327, 2020.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (148) H. Zhang 和 J. Zhang，“用于文档分类的文本图转换器，”收录于《2020 年自然语言处理实证方法会议论文集》，EMNLP 2020，在线，2020
    年 11 月 16-20 日，第 8322–8327 页，2020。
- en: '(149) W. Chang, H. Yu, K. Zhong, Y. Yang, and I. S. Dhillon, “Taming pretrained
    transformers for extreme multi-label text classification,” in KDD ’20: The 26th
    ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA,
    USA, August 23-27, 2020, pp. 3163–3171, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(149) W. Chang, H. Yu, K. Zhong, Y. Yang 和 I. S. Dhillon，“驯化预训练 Transformer
    以进行极端多标签文本分类，”收录于 KDD ’20: 第 26 届 ACM SIGKDD 知识发现与数据挖掘大会，虚拟活动，加利福尼亚，美国，2020 年
    8 月 23-27 日，第 3163–3171 页，2020。'
- en: (150) “An implementation of X-Transformer.” [https://github.com/OctoberChang/X-Transformer](https://github.com/OctoberChang/X-Transformer),
    2020.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (150) “X-Transformer 的实现。” [https://github.com/OctoberChang/X-Transformer](https://github.com/OctoberChang/X-Transformer)，2020。
- en: '(151) T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang, “Lightxml:
    Transformer with dynamic negative sampling for high-performance extreme multi-label
    text classification,” CoRR, vol. abs/2101.03305, 2021.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(151) T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao 和 F. Zhuang，“Lightxml: 用于高性能极端多标签文本分类的动态负采样
    Transformer，” CoRR，第 abs/2101.03305 卷，2021。'
- en: (152) “An implementation of LightXML.” [https://github.com/kongds/LightXML](https://github.com/kongds/LightXML),
    2021.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (152) “LightXML 的实现。” [https://github.com/kongds/LightXML](https://github.com/kongds/LightXML)，2021。
- en: (153) H. Peng, J. Li, Y. He, Y. Liu, M. Bao, L. Wang, Y. Song, and Q. Yang,
    “Large-scale hierarchical text classification with recursively regularized deep
    graph-cnn,” in Proc. WWW, 2018, pp. 1063–1072, 2018.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (153) H. Peng, J. Li, Y. He, Y. Liu, M. Bao, L. Wang, Y. Song 和 Q. Yang，“使用递归正则化深度图卷积神经网络的大规模层次文本分类，”收录于《WWW
    会议论文集》，2018，第 1063–1072 页，2018。
- en: (154) “A Tensorflow implementation of DeepGraphCNNforTexts.” [https://github.com/HKUST-KnowComp/DeepGraphCNNforTexts](https://github.com/HKUST-KnowComp/DeepGraphCNNforTexts),
    2018.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (154) “DeepGraphCNNforTexts 的 Tensorflow 实现。” [https://github.com/HKUST-KnowComp/DeepGraphCNNforTexts](https://github.com/HKUST-KnowComp/DeepGraphCNNforTexts)，2018。
- en: (155) L. Yao, C. Mao, and Y. Luo, “Graph convolutional networks for text classification,”
    in Proc. AAAI, 2019, pp. 7370–7377, 2019.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (155) L. Yao, C. Mao 和 Y. Luo，“用于文本分类的图卷积网络，”收录于《AAAI 会议论文集》，2019，第 7370–7377
    页，2019。
- en: (156) “A Tensorflow implementation of TextGCN.” [https://github.com/yao8839836/text_gcn](https://github.com/yao8839836/text_gcn),
    2019.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (156) “TextGCN 的 Tensorflow 实现。” [https://github.com/yao8839836/text_gcn](https://github.com/yao8839836/text_gcn)，2019。
- en: (157) F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger,
    “Simplifying graph convolutional networks,” in Proc. ICML, 2019, pp. 6861–6871,
    2019.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (157) F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu 和 K. Q. Weinberger，“简化图卷积网络，”收录于《ICML
    会议论文集》，2019，第 6861–6871 页，2019。
- en: (158) “An implementation of SGC.” [https://github.com/Tiiiger/SGC](https://github.com/Tiiiger/SGC),
    2019.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (158) “SGC 的实现。” [https://github.com/Tiiiger/SGC](https://github.com/Tiiiger/SGC)，2019。
- en: (159) L. Huang, D. Ma, S. Li, X. Zhang, and H. Wang, “Text level graph neural
    network for text classification,” in Proc. EMNLP, 2019, pp. 3442–3448, 2019.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (159) L. Huang, D. Ma, S. Li, X. Zhang 和 H. Wang，“用于文本分类的文本级图神经网络，”收录于《EMNLP
    会议论文集》，2019，第 3442–3448 页，2019。
- en: (160) “An implementation of TextLevelGNN.” [https://github.com/LindgeW/TextLevelGNN](https://github.com/LindgeW/TextLevelGNN),
    2019.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (160) “TextLevelGNN 的实现。” [https://github.com/LindgeW/TextLevelGNN](https://github.com/LindgeW/TextLevelGNN)，2019
    年。
- en: (161) H. Peng, J. Li, S. Wang, L. Wang, Q. Gong, R. Yang, B. Li, P. Yu, and
    L. He, “Hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale
    multi-label text classification,” IEEE Transactions on Knowledge and Data Engineering,
    2019.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (161) H. Peng, J. Li, S. Wang, L. Wang, Q. Gong, R. Yang, B. Li, P. Yu, 和 L.
    He，“用于大规模多标签文本分类的层次化分类感知和注意力图胶囊 rcnns，”《IEEE 知识与数据工程汇刊》，2019 年。
- en: '(162) Y. Zhang, X. Yu, Z. Cui, S. Wu, Z. Wen, and L. Wang, “Every document
    owns its structure: Inductive text classification via graph neural networks,”
    in Proc. ACL, 2020, pp. 334–339, 2020.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (162) Y. Zhang, X. Yu, Z. Cui, S. Wu, Z. Wen, 和 L. Wang，“每个文档都有其结构：通过图神经网络进行归纳文本分类，”《ACL
    会议录》，2020 年，第 334–339 页，2020 年。
- en: (163) “A Tensorflow implementation of TextING.” [https://github.com/CRIPAC-DIG/TextING](https://github.com/CRIPAC-DIG/TextING),
    2019.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (163) “TextING 的 Tensorflow 实现。” [https://github.com/CRIPAC-DIG/TextING](https://github.com/CRIPAC-DIG/TextING)，2019
    年。
- en: (164) X. Liu, X. You, X. Zhang, J. Wu, and P. Lv, “Tensor graph convolutional
    networks for text classification,” in The Thirty-Fourth AAAI Conference on Artificial
    Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
    Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020,
    pp. 8409–8416, 2020.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (164) X. Liu, X. You, X. Zhang, J. Wu, 和 P. Lv，“用于文本分类的张量图卷积网络，”《第三十四届 AAAI
    人工智能会议，AAAI 2020》，《第三十二届人工智能创新应用会议，IAAI 2020》，《第十届 AAAI 教育进展研讨会，EAAI 2020》，美国纽约，2020
    年 2 月 7-12 日，第 8409–8416 页，2020 年。
- en: (165) “A Tensorflow implementation of TensorGCN.” [https://github.com/THUMLP/TensorGCN](https://github.com/THUMLP/TensorGCN),
    2019.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (165) “TensorGCN 的 Tensorflow 实现。” [https://github.com/THUMLP/TensorGCN](https://github.com/THUMLP/TensorGCN)，2019
    年。
- en: '(166) A. Pal, M. Selvakumar, and M. Sankarasubbu, “MAGNET: multi-label text
    classification using attention-based graph neural network,” in Proc. ICAART, 2020,
    pp. 494–505, 2020.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(166) A. Pal, M. Selvakumar, 和 M. Sankarasubbu，“MAGNET: 基于注意力的图神经网络的多标签文本分类，”《ICAART
    会议录》，2020 年，第 494–505 页，2020 年。'
- en: (167) “A repository of MAGNET.” [https://github.com/monk1337/MAGnet](https://github.com/monk1337/MAGnet),
    2020.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (167) “MAGNET 的代码库。” [https://github.com/monk1337/MAGnet](https://github.com/monk1337/MAGnet)，2020
    年。
- en: (168) “A Tensorflow implementation of Miyato et al..” [https://github.com/TobiasLee/Text-Classification](https://github.com/TobiasLee/Text-Classification),
    2017.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (168) “Miyato 等人的 Tensorflow 实现。” [https://github.com/TobiasLee/Text-Classification](https://github.com/TobiasLee/Text-Classification)，2017
    年。
- en: (169) J. Zeng, J. Li, Y. Song, C. Gao, M. R. Lyu, and I. King, “Topic memory
    networks for short text classification,” in Proc. EMNLP, 2018, pp. 3120–3131,
    2018.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (169) J. Zeng, J. Li, Y. Song, C. Gao, M. R. Lyu, 和 I. King，“用于短文本分类的主题记忆网络，”《EMNLP
    会议录》，2018 年，第 3120–3131 页，2018 年。
- en: (170) J. Zhang, P. Lertvittayakumjorn, and Y. Guo, “Integrating semantic knowledge
    to tackle zero-shot text classification,” in Proc. NAACL, 2019, pp. 1031–1040,
    2019.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (170) J. Zhang, P. Lertvittayakumjorn, 和 Y. Guo，“整合语义知识以应对零样本文本分类，”《NAACL 会议录》，2019
    年，第 1031–1040 页，2019 年。
- en: (171) “A Tensorflow implementation of KG4ZeroShotText.” [https://github.com/JingqingZ/KG4ZeroShotText](https://github.com/JingqingZ/KG4ZeroShotText),
    2019.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (171) “KG4ZeroShotText 的 Tensorflow 实现。” [https://github.com/JingqingZ/KG4ZeroShotText](https://github.com/JingqingZ/KG4ZeroShotText)，2019
    年。
- en: (172) M. k. Alsmadi, K. B. Omar, S. A. Noah, and I. Almarashdah, “Performance
    comparison of multi-layer perceptron (back propagation, delta rule and perceptron)
    algorithms in neural networks,” in 2009 IEEE International Advance Computing Conference,
    pp. 296–299, 2009.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (172) M. k. Alsmadi, K. B. Omar, S. A. Noah, 和 I. Almarashdah，“多层感知器（反向传播、德尔塔规则和感知器）算法在神经网络中的性能比较，”《2009
    IEEE 国际先进计算会议》，第 296–299 页，2009 年。
- en: '(173) S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. E. P. Reyes, M. Shyu,
    S. Chen, and S. S. Iyengar, “A survey on deep learning: Algorithms, techniques,
    and applications,” ACM Comput. Surv., vol. 51, no. 5, pp. 92:1–92:36, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (173) S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. E. P. Reyes, M. Shyu,
    S. Chen, 和 S. S. Iyengar，“深度学习综述：算法、技术和应用，”《ACM 计算机调查》，第 51 卷，第 5 期，第 92:1–92:36
    页，2019 年。
- en: '(174) L. Qin, W. Che, Y. Li, M. Ni, and T. Liu, “Dcr-net: A deep co-interactive
    relation network for joint dialog act recognition and sentiment classification,”
    in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8665–8672, 2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(174) L. Qin, W. Che, Y. Li, M. Ni, 和 T. Liu, “Dcr-net: 一种深度共交互关系网络用于联合对话行为识别和情感分类”，在第三十四届
    AAAI 人工智能大会，AAAI 2020，第二十三届人工智能创新应用大会，IAAI 2020，第十届人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日，第8665–8672页，2020年。'
- en: '(175) Z. Deng, H. Peng, D. He, J. Li, and P. S. Yu, “Htcinfomax: A global model
    for hierarchical text classification via information maximization,” in Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021
    (K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tür, I. Beltagy, S. Bethard,
    R. Cotterell, T. Chakraborty, and Y. Zhou, eds.), pp. 3259–3265, Association for
    Computational Linguistics, 2021.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(175) Z. Deng, H. Peng, D. He, J. Li, 和 P. S. Yu, “Htcinfomax: 通过信息最大化进行层次文本分类的全球模型”，在《2021年北美计算语言学协会会议：人类语言技术论文集》，NAACL-HLT
    2021，线上，2021年6月6-11日（K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tür,
    I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, 和 Y. Zhou 编辑），第3259–3265页，计算语言学协会，2021年。'
- en: (176) D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is BERT really robust?
    A strong baseline for natural language attack on text classification and entailment,”
    in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8018–8025, 2020.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (176) D. Jin, Z. Jin, J. T. Zhou, 和 P. Szolovits, “BERT 真的鲁棒吗？对文本分类和推理的自然语言攻击的强基准”，在第三十四届
    AAAI 人工智能大会，AAAI 2020，第二十三届人工智能创新应用大会，IAAI 2020，第十届人工智能教育进展研讨会，EAAI 2020，美国纽约，2020年2月7-12日，第8018–8025页，2020年。
- en: '(177) C. Li, X. Peng, H. Peng, J. Li, and L. Wang, “Textgtl: Graph-based transductive
    learning for semi-supervised textclassification via structure-sensitive interpolation,”
    in IJCAI 2021, ijcai.org, 2021.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(177) C. Li, X. Peng, H. Peng, J. Li, 和 L. Wang, “Textgtl: 基于图的传导学习用于半监督文本分类，通过结构敏感插值”，在
    IJCAI 2021，ijcai.org，2021年。'
- en: '(178) T. Miyato, S. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial training:
    a regularization method for supervised and semi-supervised learning,” CoRR, vol. abs/1704.03976,
    2017.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (178) T. Miyato, S. Maeda, M. Koyama, 和 S. Ishii, “虚拟对抗训练：监督和半监督学习的正则化方法”，CoRR，第abs/1704.03976卷，2017年。
- en: (179) G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-encoders,”
    in Proc. ICANN, 2011 (T. Honkela, W. Duch, M. Girolami, and S. Kaski, eds.), (Berlin,
    Heidelberg), pp. 44–51, Springer Berlin Heidelberg, 2011.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (179) G. E. Hinton, A. Krizhevsky, 和 S. D. Wang, “变换自编码器”，在 Proc. ICANN, 2011（T.
    Honkela, W. Duch, M. Girolami, 和 S. Kaski 编辑），（柏林，海德堡），第44–51页，施普林格·柏林·海德堡，2011年。
- en: (180) S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (180) S. Hochreiter 和 J. Schmidhuber, “长短期记忆”，《神经计算》，第9卷，第8期，第1735–1780页，1997年。
- en: (181) S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated
    corpus for learning natural language inference,” in Proc. EMNLP, 2015, pp. 632–642,
    2015.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (181) S. R. Bowman, G. Angeli, C. Potts, 和 C. D. Manning, “用于学习自然语言推断的大型注释语料库”，在
    Proc. EMNLP, 2015，第632–642页，2015年。
- en: (182) Z. Wang, W. Hamza, and R. Florian, “Bilateral multi-perspective matching
    for natural language sentences,” in Proc. IJCAI, 2017, pp. 4144–4150, 2017.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (182) Z. Wang, W. Hamza, 和 R. Florian, “自然语言句子的双向多视角匹配”，在 Proc. IJCAI, 2017，第4144–4150页，2017年。
- en: (183) R. Johnson and T. Zhang, “Semi-supervised convolutional neural networks
    for text categorization via region embedding,” in Proc. NeurIPS, 2015, pp. 919–927,
    2015.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (183) R. Johnson 和 T. Zhang, “通过区域嵌入的半监督卷积神经网络进行文本分类”，在 Proc. NeurIPS, 2015，第919–927页，2015年。
- en: (184) K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual
    networks,” in Proc. ECCV, 2016, pp. 630–645, 2016.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (184) K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差网络中的身份映射”，在 Proc. ECCV, 2016，第630–645页，2016年。
- en: (185) I. Bazzi, Modelling out-of-vocabulary words for robust speech recognition.
    PhD thesis, Massachusetts Institute of Technology, 2002.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (185) I. Bazzi, “为鲁棒语音识别建模词汇外单词”，博士论文，麻省理工学院，2002年。
- en: (186) H. Nguyen and M. Nguyen, “A deep neural architecture for sentence-level
    sentiment classification in twitter social networking,” in Proc. PACLING, 2017,
    pp. 15–27, 2017.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (186) H. Nguyen 和 M. Nguyen，“用于 Twitter 社交网络中句子级情感分类的深度神经网络架构，” 见 Proc. PACLING，2017，pp.
    15–27，2017。
- en: '(187) B. Adams and G. McKenzie, “Crowdsourcing the character of a place: Character-level
    convolutional networks for multilingual geographic text classification,” Trans.
    GIS, vol. 22, no. 2, pp. 394–408, 2018.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (187) B. Adams 和 G. McKenzie，“众包地方特征：用于多语言地理文本分类的字符级卷积网络，” Trans. GIS，vol. 22，第2期，pp.
    394–408，2018。
- en: (188) Z. Chen and T. Qian, “Transfer capsule network for aspect level sentiment
    classification,” in Proc. ACL, 2019, pp. 547–556, 2019.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (188) Z. Chen 和 T. Qian，“用于方面级情感分类的迁移胶囊网络，” 见 Proc. ACL，2019，pp. 547–556，2019。
- en: '(189) W. Xue, W. Zhou, T. Li, and Q. Wang, “MTNA: A neural multi-task model
    for aspect category classification and aspect term extraction on restaurant reviews,”
    in Proc. IJCNLP, 2017, pp. 151–156, 2017.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (189) W. Xue, W. Zhou, T. Li 和 Q. Wang，“MTNA：用于餐厅评论的方面类别分类和方面术语提取的神经多任务模型，”
    见 Proc. IJCNLP，2017，pp. 151–156，2017。
- en: (190) D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in Proc. ICLR, 2015, 2015.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (190) D. Bahdanau, K. Cho 和 Y. Bengio， “通过联合学习对齐和翻译的神经机器翻译，” 见 Proc. ICLR，2015，2015。
- en: (191) Z. Hu, X. Li, C. Tu, Z. Liu, and M. Sun, “Few-shot charge prediction with
    discriminative legal attributes,” in Proc. COLING, 2018, pp. 487–498, 2018.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (191) Z. Hu, X. Li, C. Tu, Z. Liu 和 M. Sun，“通过区分法律属性进行少样本费用预测，” 见 Proc. COLING，2018，pp.
    487–498，2018。
- en: (192) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. NeurIPS, 2017,
    pp. 5998–6008, 2017.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (192) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser 和 I. Polosukhin，“注意力机制即是你所需要的，” 见 Proc. NeurIPS，2017，pp. 5998–6008，2017。
- en: (193) Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis
    via embedding commonsense knowledge into an attentive LSTM,” in Proc. AAAI, 2018,
    pp. 5876–5883, 2018.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (193) Y. Ma, H. Peng 和 E. Cambria，“通过将常识知识嵌入注意力 LSTM 进行目标方面情感分析，” 见 Proc. AAAI，2018，pp.
    5876–5883，2018。
- en: (194) Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based LSTM for aspect-level
    sentiment classification,” in Proc. EMNLP, 2016, pp. 606–615, 2016.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (194) Y. Wang, M. Huang, X. Zhu 和 L. Zhao，“基于注意力的 LSTM 进行方面级情感分类，” 见 Proc. EMNLP，2016，pp.
    606–615，2016。
- en: (195) F. Fan, Y. Feng, and D. Zhao, “Multi-grained attention network for aspect-level
    sentiment classification,” in Proc. EMNLP, 2018, pp. 3433–3442, 2018.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (195) F. Fan, Y. Feng 和 D. Zhao，“用于方面级情感分类的多粒度注意力网络，” 见 Proc. EMNLP，2018，pp.
    3433–3442，2018。
- en: (196) M. Tan, C. dos Santos, B. Xiang, and B. Zhou, “Improved representation
    learning for question answer matching,” in Proc. ACL, 2016, (Berlin, Germany),
    pp. 464–473, Association for Computational Linguistics, Aug. 2016.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (196) M. Tan, C. dos Santos, B. Xiang 和 B. Zhou，“改进的表示学习用于问题答案匹配，” 见 Proc. ACL，2016，（德国柏林），pp.
    464–473，计算语言学协会，2016年8月。
- en: (197) C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou, “Attentive pooling networks,”
    CoRR, vol. abs/1602.03609, 2016.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (197) C. N. dos Santos, M. Tan, B. Xiang 和 B. Zhou，“注意力池化网络，” CoRR，vol. abs/1602.03609，2016。
- en: '(198) X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models
    for natural language processing: A survey,” CoRR, vol. abs/2003.08271, 2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (198) X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai 和 X. Huang，“自然语言处理的预训练模型：一项综述，”
    CoRR，vol. abs/2003.08271，2020。
- en: (199) A. Radford, “Improving language understanding by generative pre-training,”
    2018.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (199) A. Radford，“通过生成预训练提升语言理解，” 2018。
- en: '(200) Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov,
    “Transformer-xl: Attentive language models beyond a fixed-length context,” in
    Proc. ACL, 2019, pp. 2978–2988, 2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (200) Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le 和 R. Salakhutdinov，“Transformer-xl：超越固定长度上下文的注意力语言模型，”
    见 Proc. ACL，2019，pp. 2978–2988，2019。
- en: (201) G. Jawahar, B. Sagot, and D. Seddah, “What does BERT learn about the structure
    of language?,” in Proc. ACL, 2019, pp. 3651–3657, 2019.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (201) G. Jawahar, B. Sagot 和 D. Seddah，“BERT 对语言结构的学习是什么？，” 见 Proc. ACL，2019，pp.
    3651–3657，2019。
- en: '(202) M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “BART: denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” in Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,
    July 5-10, 2020, pp. 7871–7880, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (202) M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V.
    Stoyanov 和 L. Zettlemoyer，“BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练，” 见第58届计算语言学协会年会论文集，ACL
    2020，在线，2020年7月5-10日，pp. 7871–7880，2020。
- en: '(203) M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, “Spanbert:
    Improving pre-training by representing and predicting spans,” Trans. Assoc. Comput.
    Linguistics, vol. 8, pp. 64–77, 2020.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(203) M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, 和 O. Levy, “Spanbert:
    通过表示和预测跨度来改进预训练，” *Trans. Assoc. Comput. Linguistics*, 卷8, 页64–77, 2020。'
- en: '(204) Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu,
    H. Tian, and H. Wu, “ERNIE: enhanced representation through knowledge integration,”
    CoRR, vol. abs/1904.09223, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(204) Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu,
    H. Tian, 和 H. Wu, “ERNIE: 通过知识整合增强表示，” *CoRR*, 卷abs/1904.09223, 2019。'
- en: (205) M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural
    networks on graphs with fast localized spectral filtering,” in Proc. NeurIPS,
    2016, pp. 3837–3845, 2016.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (205) M. Defferrard, X. Bresson, 和 P. Vandergheynst, “在图上进行卷积神经网络的快速局部谱滤波，”
    在 Proc. *NeurIPS*, 2016, 页3837–3845, 2016。
- en: (206) H. Peng, R. Zhang, Y. Dou, R. Yang, J. Zhang, and P. S. Yu, “Reinforced
    neighborhood selection guided multi-relational graph neural networks,” arXiv preprint
    arXiv:2104.07886, 2021.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (206) H. Peng, R. Zhang, Y. Dou, R. Yang, J. Zhang, 和 P. S. Yu, “强化邻域选择引导的多关系图神经网络，”
    *arXiv preprint arXiv:2104.07886*, 2021。
- en: '(207) H. Peng, R. Yang, Z. Wang, J. Li, L. He, P. Yu, A. Zomaya, and R. Ranjan,
    “Lime: Low-cost incremental learning for dynamic heterogeneous information networks,”
    IEEE Transactions on Computers, 2021.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(207) H. Peng, R. Yang, Z. Wang, J. Li, L. He, P. Yu, A. Zomaya, 和 R. Ranjan,
    “Lime: 低成本增量学习用于动态异构信息网络，” *IEEE Transactions on Computers*, 2021。'
- en: (208) J. Li, H. Peng, Y. Cao, Y. Dou, H. Zhang, P. Yu, and L. He, “Higher-order
    attribute-enhancing heterogeneous graph neural networks,” IEEE Transactions on
    Knowledge and Data Engineering, 2021.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (208) J. Li, H. Peng, Y. Cao, Y. Dou, H. Zhang, P. Yu, 和 L. He, “高阶属性增强异构图神经网络，”
    *IEEE Transactions on Knowledge and Data Engineering*, 2021。
- en: (209) D. Marcheggiani and I. Titov, “Encoding sentences with graph convolutional
    networks for semantic role labeling,” in Proc. EMNLP, 2017, pp. 1506–1515, 2017.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (209) D. Marcheggiani 和 I. Titov, “使用图卷积网络对句子进行编码以进行语义角色标注，” 在 Proc. *EMNLP*,
    2017, 页1506–1515, 2017。
- en: (210) Y. Li, R. Jin, and Y. Luo, “Classifying relations in clinical narratives
    using segment graph convolutional and recurrent neural networks (seg-gcrns),”
    JAMIA, vol. 26, no. 3, pp. 262–268, 2019.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (210) Y. Li, R. Jin, 和 Y. Luo, “使用分段图卷积和递归神经网络（seg-gcrns）对临床叙述中的关系进行分类，” *JAMIA*,
    卷26, 第3期, 页262–268, 2019。
- en: (211) J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Sima’an, “Graph
    convolutional encoders for syntax-aware neural machine translation,” in Proc.
    EMNLP, 2017, pp. 1957–1967, 2017.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (211) J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, 和 K. Sima’an, “用于语法感知神经机器翻译的图卷积编码器，”
    在 Proc. *EMNLP*, 2017, 页1957–1967, 2017。
- en: (212) P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio,
    “Graph attention networks,” in Proc. ICLR, 2018, 2018.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (212) P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Liò, 和 Y. Bengio,
    “图注意力网络，” 在 Proc. *ICLR*, 2018, 2018。
- en: (213) L. Hu, T. Yang, C. Shi, H. Ji, and X. Li, “Heterogeneous graph attention
    networks for semi-supervised short text classification,” in Proc. EMNLP, 2019,
    pp. 4820–4829, 2019.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (213) L. Hu, T. Yang, C. Shi, H. Ji, 和 X. Li, “用于半监督短文本分类的异构图注意力网络，” 在 Proc.
    *EMNLP*, 2019, 页4820–4829, 2019。
- en: (214) Z. Li, X. Ding, and T. Liu, “Constructing narrative event evolutionary
    graph for script event prediction,” in Proc. IJCAI, 2018, pp. 4201–4207, 2018.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (214) Z. Li, X. Ding, 和 T. Liu, “构建叙事事件演变图用于脚本事件预测，” 在 Proc. *IJCAI*, 2018,
    页4201–4207, 2018。
- en: (215) J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, “Signature
    verification using a siamese time delay neural network,” in Proc. NeurIPS, 1993],
    pp. 737–744, 1993.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (215) J. Bromley, I. Guyon, Y. LeCun, E. Säckinger, 和 R. Shah, “使用孪生时间延迟神经网络进行签名验证，”
    在 Proc. *NeurIPS*, 1993, 页737–744, 1993。
- en: (216) J. Mueller and A. Thyagarajan, “Siamese recurrent architectures for learning
    sentence similarity,” in Proc. AAAI, 2016, pp. 2786–2792, 2016.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (216) J. Mueller 和 A. Thyagarajan, “用于学习句子相似性的孪生递归架构，” 在 Proc. *AAAI*, 2016,
    页2786–2792, 2016。
- en: (217) Jayadeva, H. Pant, M. Sharma, and S. Soman, “Twin neural networks for
    the classification of large unbalanced datasets,” Neurocomputing, vol. 343, pp. 34
    – 49, 2019. Learning in the Presence of Class Imbalance and Concept Drift.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (217) Jayadeva, H. Pant, M. Sharma, 和 S. Soman, “双重神经网络用于大规模不平衡数据集的分类，” *Neurocomputing*,
    卷343, 页34 – 49, 2019。处理类别不平衡和概念漂移中的学习。
- en: (218) T. Miyato, S. ichi Maeda, M. Koyama, K. Nakae, and S. Ishii, “Distributional
    smoothing with virtual adversarial training,” 2015.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (218) T. Miyato, S. ichi Maeda, M. Koyama, K. Nakae, 和 S. Ishii, “通过虚拟对抗训练进行分布平滑，”
    2015。
- en: (219) T. Zhang, M. Huang, and L. Zhao, “Learning structured representation for
    text classification via reinforcement learning,” in Proc. AAAI, 2018, pp. 6053–6060,
    2018.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (219) T. 张、M. 黄 和 L. 赵，“通过强化学习进行文本分类的结构化表示学习，”发表于《AAAI会议论文集》，2018年，第6053–6060页，2018年。
- en: (220) J. Weston, S. Chopra, and A. Bordes, “Memory networks,” 2015.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (220) J. 韦斯顿、S. 乔普拉 和 A. 博尔德斯，“记忆网络，”2015年。
- en: (221) X. Li and W. Lam, “Deep multi-task learning for aspect term extraction
    with memory interaction,” in Proc. EMNLP, 2017, pp. 2886–2892, 2017.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (221) X. 李 和 W. 蓝，“具有记忆交互的深度多任务学习用于方面术语提取，”发表于《EMNLP会议论文集》，2017年，第2886–2892页，2017年。
- en: (222) C. Shen, C. Sun, J. Wang, Y. Kang, S. Li, X. Liu, L. Si, M. Zhang, and
    G. Zhou, “Sentiment classification towards question-answering with hierarchical
    matching network,” in Proc. EMNLP, 2018, pp. 3654–3663, 2018.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (222) C. 沈、C. 孙、J. 王、Y. 康、S. 李、X. 刘、L. 司、M. 张 和 G. 周，“面向问答的情感分类与层次匹配网络，”发表于《EMNLP会议论文集》，2018年，第3654–3663页，2018年。
- en: (223) X. Ding, K. Liao, T. Liu, Z. Li, and J. Duan, “Event representation learning
    enhanced with external commonsense knowledge,” in Proc. EMNLP, 2019, pp. 4893–4902,
    2019.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (223) X. 丁、K. 廖、T. 刘、Z. 李 和 J. 段，“增强外部常识知识的事件表示学习，”发表于《EMNLP会议论文集》，2019年，第4893–4902页，2019年。
- en: (224) Y. Zhang, D. Song, P. Zhang, X. Li, and P. Wang, “A quantum-inspired sentiment
    representation model for twitter sentiment analysis,” Applied Intelligence, 2019.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (224) Y. 张、D. 宋、P. 张、X. 李 和 P. 王，“一种量子启发的情感表示模型用于Twitter情感分析，”《应用智能》，2019年。
- en: '(225) O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, “Q8BERT: quantized
    8bit BERT,” in Fifth Workshop on Energy Efficient Machine Learning and Cognitive
    Computing - NeurIPS Edition, EMC2@NeurIPS 2019, Vancouver, Canada, December 13,
    2019, pp. 36–39, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(225) O. 扎夫里尔、G. 布杜赫、P. 伊萨克 和 M. 瓦瑟布拉特，“Q8BERT: 量化的8位BERT，”发表于第五届节能机器学习与认知计算研讨会
    - NeurIPS版，EMC2@NeurIPS 2019，加拿大温哥华，2019年12月13日，第36–39页，2019年。'
- en: (226) “MR Corpus.” [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/),
    2002.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (226) “MR 语料库。” [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)，2002年。
- en: (227) “SST Corpus.” [http://nlp.stanford.edu/sentiment](http://nlp.stanford.edu/sentiment),
    2013.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (227) “SST 语料库。” [http://nlp.stanford.edu/sentiment](http://nlp.stanford.edu/sentiment)，2013年。
- en: (228) R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts,
    “Recursive deep models for semantic compositionality over a sentiment treebank,”
    in Proc. EMNLP, 2013, (Seattle, Washington, USA), pp. 1631–1642, Association for
    Computational Linguistics, Oct. 2013.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (228) R. 索彻、A. 佩雷利金、J. 吴、J. 庄、C. D. 曼宁、A. 吴 和 C. 波茨，“用于情感树库的递归深度模型的语义组合性，”发表于《EMNLP会议论文集》，2013年，（美国华盛顿州西雅图），第1631–1642页，计算语言学协会，2013年10月。
- en: (229) “MPQA Corpus.” [http://www.cs.pitt.edu/mpqa/](http://www.cs.pitt.edu/mpqa/),
    2005.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (229) “MPQA 语料库。” [http://www.cs.pitt.edu/mpqa/](http://www.cs.pitt.edu/mpqa/)，2005年。
- en: (230) Q. Diao, M. Qiu, C. Wu, A. J. Smola, J. Jiang, and C. Wang, “Jointly modeling
    aspects, ratings and sentiments for movie recommendation (JMARS),” in Proc. ACM
    SIGKDD, 2014, pp. 193–202, 2014.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (230) Q. 刁、M. 邱、C. 吴、A. J. 斯莫拉、J. 姜 和 C. 王，“联合建模电影推荐的方面、评分和情感（JMARS），”发表于《ACM
    SIGKDD会议论文集》，2014年，第193–202页，2014年。
- en: (231) D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent neural
    network for sentiment classification,” in Proc. EMNLP, 2015, pp. 1422–1432, 2015.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (231) D. 唐、B. 秦 和 T. 刘，“基于门控递归神经网络的文档建模用于情感分类，”发表于《EMNLP会议论文集》，2015年，第1422–1432页，2015年。
- en: (232) “Amazon review Corpus.” [https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products),
    2015.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (232) “Amazon 评论语料库。” [https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products)，2015年。
- en: (233) “Twitter Corpus.” [https://www.cs.york.ac.uk/semeval-2013/task2/](https://www.cs.york.ac.uk/semeval-2013/task2/),
    2013.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (233) “Twitter 语料库。” [https://www.cs.york.ac.uk/semeval-2013/task2/](https://www.cs.york.ac.uk/semeval-2013/task2/)，2013年。
- en: (234) “AG Corpus.” [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html),
    2004.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (234) “AG 语料库。” [http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)，2004年。
- en: (235) “Reuters Corpus.” [https://www.cs.umb.edu/~smimarog/textmining/datasets/](https://www.cs.umb.edu/~smimarog/textmining/datasets/),
    2007.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (235) “Reuters 语料库。” [https://www.cs.umb.edu/~smimarog/textmining/datasets/](https://www.cs.umb.edu/~smimarog/textmining/datasets/)，2007年。
- en: (236) C. Wang, M. Zhang, S. Ma, and L. Ru, “Automatic online news issue construction
    in web environment,” in Proc. WWW, 2008, pp. 457–466, 2008.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (236) C. 王、M. 张、S. 马 和 L. 如，“在网页环境中自动构建在线新闻议题，”发表于《WWW会议论文集》，2008年，第457–466页，2008年。
- en: '(237) X. Li, C. Li, J. Chi, J. Ouyang, and C. Li, “Dataless text classification:
    A topic modeling approach with document manifold,” in Proceedings of the 27th
    ACM International Conference on Information and Knowledge Management, CIKM 2018,
    Torino, Italy, October 22-26, 2018, pp. 973–982, 2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (237) X. Li, C. Li, J. Chi, J. Ouyang, 和 C. Li, “无数据文本分类：一种基于文档流形的主题建模方法,” 见于第27届ACM国际信息与知识管理会议（CIKM
    2018）论文集，意大利都灵，2018年10月22-26日, 页码 973–982, 2018年。
- en: (238) J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer, “Dbpedia - A large-scale,
    multilingual knowledge base extracted from wikipedia,” Semantic Web, vol. 6, no. 2,
    pp. 167–195, 2015.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (238) J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes,
    S. Hellmann, M. Morsey, P. van Kleef, S. Auer, 和 C. Bizer, “DBpedia - 从维基百科提取的大规模多语言知识库,”
    Semantic Web, 第6卷，第2期, 页码 167–195, 2015年。
- en: (239) “Ohsumed Corpus.” [http://davis.wpi.edu/xmdv/datasets/ohsumed.html](http://davis.wpi.edu/xmdv/datasets/ohsumed.html),
    2015.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (239) “Ohsumed语料库。” [http://davis.wpi.edu/xmdv/datasets/ohsumed.html](http://davis.wpi.edu/xmdv/datasets/ohsumed.html),
    2015年。
- en: (240) “EUR-Lex Corpus.” [http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html](http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html),
    2019.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (240) “EUR-Lex语料库。” [http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html](http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html),
    2019年。
- en: (241) “Amazon670K Corpus.” [http://manikvarma.org/downloads/XC/XMLRepository.html](http://manikvarma.org/downloads/XC/XMLRepository.html),
    2016.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (241) “Amazon670K语料库。” [http://manikvarma.org/downloads/XC/XMLRepository.html](http://manikvarma.org/downloads/XC/XMLRepository.html),
    2016年。
- en: (242) J. Yin and J. Wang, “A dirichlet multinomial mixture model-based approach
    for short text clustering,” in The 20th ACM SIGKDD International Conference on
    Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 -
    27, 2014, pp. 233–242, 2014.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (242) J. Yin 和 J. Wang, “基于Dirichlet多项混合模型的短文本聚类方法,” 见于第20届ACM SIGKDD国际知识发现与数据挖掘会议（KDD
    ’14），美国纽约，2014年8月24-27日, 页码 233–242, 2014年。
- en: '(243) J. Chen, Z. Gong, W. Wang, W. Liu, M. Yang, and C. Wang, “Tam: Targeted
    analysis model with reinforcement learning on short texts,” IEEE Transactions
    on Neural Networks and Learning Systems, pp. 1–10, 2020.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(243) J. Chen, Z. Gong, W. Wang, W. Liu, M. Yang, 和 C. Wang, “Tam: 基于强化学习的短文本目标分析模型,”
    IEEE Transactions on Neural Networks and Learning Systems, 页码 1–10, 2020年。'
- en: (244) F. Wang, Z. Wang, Z. Li, and J. Wen, “Concept-based short text classification
    and ranking,” in Proc. CIKM, 2014, pp. 1069–1078, 2014.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (244) F. Wang, Z. Wang, Z. Li, 和 J. Wen, “基于概念的短文本分类与排序,” 见于CIKM会议论文集, 2014年,
    页码 1069–1078, 2014年。
- en: (245) “Fudan Corpus.” [www.datatang.com/data/44139and43543](www.datatang.com/data/44139and43543),
    2015.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (245) “复旦语料库。” [www.datatang.com/data/44139and43543](www.datatang.com/data/44139and43543),
    2015年。
- en: '(246) P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100, 000+ questions
    for machine comprehension of text,” in Proc. EMNLP, 2016, pp. 2383–2392, 2016.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(246) P. Rajpurkar, J. Zhang, K. Lopyrev, 和 P. Liang, “Squad: 100,000+ 用于机器理解文本的问题,”
    见于EMNLP会议论文集, 2016年, 页码 2383–2392, 2016年。'
- en: (247) X. Yao, B. V. Durme, C. Callison-Burch, and P. Clark, “Answer extraction
    as sequence tagging with tree edit distance,” in Proc. NAACL, 2013, pp. 858–867,
    2013.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (247) X. Yao, B. V. Durme, C. Callison-Burch, 和 P. Clark, “作为序列标记的答案提取与树编辑距离,”
    见于NAACL会议论文集, 2013年, 页码 858–867, 2013年。
- en: (248) “TREC Corpus.” [https://cogcomp.seas.upenn.edu/Data/QA/QC/](https://cogcomp.seas.upenn.edu/Data/QA/QC/),
    2002.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (248) “TREC语料库。” [https://cogcomp.seas.upenn.edu/Data/QA/QC/](https://cogcomp.seas.upenn.edu/Data/QA/QC/),
    2002年。
- en: '(249) Y. Yang, W. Yih, and C. Meek, “Wikiqa: A challenge dataset for open-domain
    question answering,” in Proc. EMNLP, 2015, pp. 2013–2018, 2015.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(249) Y. Yang, W. Yih, 和 C. Meek, “Wikiqa: 用于开放领域问答的挑战数据集,” 见于EMNLP会议论文集, 2015年,
    页码 2013–2018, 2015年。'
- en: '(250) B. Pang and L. Lee, “A sentimental education: Sentiment analysis using
    subjectivity summarization based on minimum cuts,” in Proc. ACL, 2004, (Barcelona,
    Spain), pp. 271–278, July 2004.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (250) B. Pang 和 L. Lee, “情感教育：基于最小割的主观性总结情感分析,” 见于ACL会议论文集, 2004年（西班牙巴塞罗那）,
    页码 271–278, 2004年7月。
- en: (251) M. Hu and B. Liu, “Mining and summarizing customer reviews,” in Proc.
    ACM SIGKDD, 2004, pp. 168–177, 2004.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (251) M. Hu 和 B. Liu, “挖掘和总结客户评论,” 见于ACM SIGKDD会议论文集, 2004年, 页码 168–177, 2004年。
- en: (252) “Reuters Corpus.” [https://martin-thoma.com/nlp-reuters](https://martin-thoma.com/nlp-reuters),
    2017.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (252) “路透社语料库。” [https://martin-thoma.com/nlp-reuters](https://martin-thoma.com/nlp-reuters),
    2017年。
- en: (253) J. Kim, S. Jang, E. L. Park, and S. Choi, “Text classification using capsules,”
    Neurocomputing, vol. 376, pp. 214–221, 2020.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (253) J. Kim, S. Jang, E. L. Park, 和 S. Choi, “使用胶囊进行文本分类,” Neurocomputing,
    第376卷, 页码 214–221, 2020年。
- en: (254) “Reuters10 Corpus.” [http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html),
    2020.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (254) “Reuters10语料库。” [http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html),
    2020年。
- en: '(255) D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, “RCV1: A new benchmark collection
    for text categorization research,” J. Mach. Learn. Res., vol. 5, pp. 361–397,
    2004.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(255) D. D. Lewis, Y. Yang, T. G. Rose, 和 F. Li，“RCV1: 一个用于文本分类研究的新基准集合，”机器学习研究杂志，卷
    5，页 361–397，2004年。'
- en: (256) “RCV1-V2 Corpus.” [http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm](http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm),
    2004.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (256) “RCV1-V2 语料库。” [http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm](http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm)，2004年。
- en: '(257) B. Pang and L. Lee, “Seeing stars: Exploiting class relationships for
    sentiment categorization with respect to rating scales,” in Proc. ACL, 2005, pp. 115–124,
    2005.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (257) B. Pang 和 L. Lee，“看到星星：利用类别关系进行情感分类与评级尺度相关的研究，”发表于 ACL 会议，2005年，页 115–124，2005年。
- en: (258) J. Wiebe, T. Wilson, and C. Cardie, “Annotating expressions of opinions
    and emotions in language,” Language Resources and Evaluation, vol. 39, no. 2-3,
    pp. 165–210, 2005.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (258) J. Wiebe, T. Wilson, 和 C. Cardie，“在语言中标注意见和情感的表达，”语言资源与评估，卷 39，第 2-3 期，页
    165–210，2005年。
- en: (259) M. Thelwall, K. Buckley, and G. Paltoglou, “Sentiment strength detection
    for the social web,” J. Assoc. Inf. Sci. Technol., vol. 63, no. 1, pp. 163–173,
    2012.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (259) M. Thelwall, K. Buckley, 和 G. Paltoglou，“社交网络的情感强度检测，”信息科学与技术协会杂志，卷 63，第
    1 期，页 163–173，2012年。
- en: '(260) P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V. Stoyanov, “Semeval-2016
    task 4: Sentiment analysis in twitter,” in Proc. SemEval, 2016), 2016.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (260) P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, 和 V. Stoyanov，“Semeval-2016任务4：Twitter中的情感分析，”发表于
    SemEval 会议，2016年，2016年。
- en: '(261) Z. Lu, “Pubmed and beyond: a survey of web tools for searching biomedical
    literature,” Database, vol. 2011, 2011.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (261) Z. Lu，“Pubmed及其他：搜索生物医学文献的网络工具概述，”数据库，卷 2011，2011年。
- en: '(262) T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and
    L. Deng, “MS MARCO: A human generated machine reading comprehension dataset,”
    in Proc. NeurIPS, 2016, 2016.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(262) T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, 和 L. Deng，“MS
    MARCO: 一个由人类生成的机器阅读理解数据集，”发表于 NeurIPS 会议，2016年，2016年。'
- en: (263) [https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs](https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs).
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (263) [https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs](https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs)。
- en: '(264) P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know: Unanswerable
    questions for squad,” in Proc. ACL, 2018, pp. 784–789, 2018.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (264) P. Rajpurkar, R. Jia, 和 P. Liang，“了解你不知道的：SQuAD的无法回答问题，”发表于 ACL 会议，2018年，页
    784–789，2018年。
- en: (265) A. Williams, N. Nangia, and S. R. Bowman, “A broad-coverage challenge
    corpus for sentence understanding through inference,” in Proc. NAACL, 2018, pp. 1112–1122,
    2018.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (265) A. Williams, N. Nangia, 和 S. R. Bowman，“一个广覆盖的挑战语料库，用于通过推理理解句子，”发表于 NAACL
    会议，2018年，页 1112–1122，2018年。
- en: '(266) M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and R. Zamparelli,
    “Semeval-2014 task 1: Evaluation of compositional distributional semantic models
    on full sentences through semantic relatedness and textual entailment,” in Proc.
    SemEval, 2014, pp. 1–8, 2014.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (266) M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, 和 R. Zamparelli，“Semeval-2014任务1：通过语义相关性和文本蕴含评估组合分布语义模型，”发表于
    SemEval 会议，2014年，页 1–8，2014年。
- en: '(267) B. Dolan, C. Quirk, and C. Brockett, “Unsupervised construction of large
    paraphrase corpora: Exploiting massively parallel news sources,” in Proc. COLING,
    2004, 2004.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (267) B. Dolan, C. Quirk, 和 C. Brockett，“无监督构建大规模同义句语料库：利用大规模并行新闻来源，”发表于 COLING
    会议，2004年，2004年。
- en: '(268) D. M. Cer, M. T. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, “Semeval-2017
    task 1: Semantic textual similarity - multilingual and cross-lingual focused evaluation,”
    CoRR, vol. abs/1708.00055, 2017.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (268) D. M. Cer, M. T. Diab, E. Agirre, I. Lopez-Gazpio, 和 L. Specia，“Semeval-2017任务1：语义文本相似度
    - 多语言和跨语言的重点评估，”CoRR，卷 abs/1708.00055，2017年。
- en: (269) I. Dagan, O. Glickman, and B. Magnini, “The PASCAL recognising textual
    entailment challenge,” in Machine Learning Challenges, Evaluating Predictive Uncertainty,
    Visual Object Classification and Recognizing Textual Entailment, First PASCAL
    Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13,
    2005, Revised Selected Papers, pp. 177–190, 2005.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (269) I. Dagan, O. Glickman, 和 B. Magnini，“PASCAL 识别文本蕴含挑战，”发表于机器学习挑战：评估预测不确定性、视觉对象分类和识别文本蕴含，首届
    PASCAL 机器学习挑战研讨会，MLCW 2005，英国南安普顿，2005年4月11-13日，修订精选论文，页 177–190，2005年。
- en: '(270) T. Khot, A. Sabharwal, and P. Clark, “Scitail: A textual entailment dataset
    from science question answering,” in Proceedings of the Thirty-Second AAAI Conference
    on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
    Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in
    Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
    2018, pp. 5189–5197, 2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(270) T. Khot、A. Sabharwal 和 P. Clark，“Scitail: 来自科学问答的文本蕴涵数据集，”发表于第三十二届 AAAI
    人工智能会议（AAAI-18）、第30届人工智能创新应用会议（IAAI-18）和第八届 AAAI 教育进展会议（EAAI-18），美国路易斯安那州新奥尔良，2018年2月2-7日，pp.
    5189–5197, 2018。'
- en: '(271) K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber,
    and L. E. Barnes, “Hdltex: Hierarchical deep learning for text classification,”
    in Proc. ICMLA, 2017, pp. 364–371, 2017.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(271) K. Kowsari、D. E. Brown、M. Heidarysafa、K. J. Meimandi、M. S. Gerber 和 L.
    E. Barnes，“Hdltex: 层次深度学习用于文本分类，”发表于 Proc. ICMLA, 2017, pp. 364–371, 2017。'
- en: (272) “AmazonCat-13K Corpus.” [https://drive.google.com/open?id=1VwHAbri6y6oh8lkpZ6sSY_b1FRNnCLFL](https://drive.google.com/open?id=1VwHAbri6y6oh8lkpZ6sSY_b1FRNnCLFL),
    2018.
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (272) “AmazonCat-13K 数据集。” [https://drive.google.com/open?id=1VwHAbri6y6oh8lkpZ6sSY_b1FRNnCLFL](https://drive.google.com/open?id=1VwHAbri6y6oh8lkpZ6sSY_b1FRNnCLFL),
    2018。
- en: (273) “BlurbGenreCollection-EN Corpus.” [https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html](https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html),
    2017.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (273) “BlurbGenreCollection-EN 数据集。” [https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html](https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-collection.html),
    2017。
- en: '(274) I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. Ó. Séaghdha, S. Padó,
    M. Pennacchiotti, L. Romano, and S. Szpakowicz, “Semeval-2010 task 8: Multi-way
    classification of semantic relations between pairs of nominals,” in Proc. NAACL,
    2009, pp. 94–99, 2009.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (274) I. Hendrickx、S. N. Kim、Z. Kozareva、P. Nakov、D. Ó. Séaghdha、S. Padó、M.
    Pennacchiotti、L. Romano 和 S. Szpakowicz，“Semeval-2010 任务8：名词对之间的语义关系的多类别分类，”发表于
    Proc. NAACL, 2009, pp. 94–99, 2009。
- en: (275) S. M. Strassel, M. A. Przybocki, K. Peterson, Z. Song, and K. Maeda, “Linguistic
    resources and evaluation techniques for evaluation of cross-document automatic
    content extraction,” in Proc. LREC, 2008, 2008.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (275) S. M. Strassel、M. A. Przybocki、K. Peterson、Z. Song 和 K. Maeda，“跨文档自动内容提取的语言资源和评估技术，”发表于
    Proc. LREC, 2008, 2008。
- en: (276) Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning, “Position-aware
    attention and supervised data improve slot filling,” in Proc. EMNLP, 2017, pp. 35–45,
    2017.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (276) Y. Zhang、V. Zhong、D. Chen、G. Angeli 和 C. D. Manning，“位置感知注意力和监督数据改进槽填充，”发表于
    Proc. EMNLP, 2017, pp. 35–45, 2017。
- en: (277) S. Riedel, L. Yao, and A. McCallum, “Modeling relations and their mentions
    without labeled text,” in Proc. ECML PKDD, 2010, pp. 148–163, 2010.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (277) S. Riedel、L. Yao 和 A. McCallum，“在没有标记文本的情况下建模关系及其提及，”发表于 Proc. ECML PKDD,
    2010, pp. 148–163, 2010。
- en: (278) “FewRel Corpus.” [https://github.com/thunlp/FewRel](https://github.com/thunlp/FewRel),
    2019.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (278) “FewRel 数据集。” [https://github.com/thunlp/FewRel](https://github.com/thunlp/FewRel),
    2019。
- en: (279) S. Kim, L. F. D’Haro, R. E. Banchs, J. D. Williams, and M. Henderson,
    “The fourth dialog state tracking challenge,” in Proc. IWSDS, 2016, pp. 435–449,
    2016.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (279) S. Kim、L. F. D’Haro、R. E. Banchs、J. D. Williams 和 M. Henderson，“第四届对话状态追踪挑战赛，”发表于
    Proc. IWSDS, 2016, pp. 435–449, 2016。
- en: (280) J. Ang, Y. Liu, and E. Shriberg, “Automatic dialog act segmentation and
    classification in multiparty meetings,” in Proc. ICASSP, 2005, pp. 1061–1064,
    2005.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (280) J. Ang、Y. Liu 和 E. Shriberg，“在多人会议中自动对话行为分段和分类，”发表于 Proc. ICASSP, 2005,
    pp. 1061–1064, 2005。
- en: (281) D. Jurafsky and E. Shriberg, “Switchboard swbd-damsl shallow-discourse-function
    annotation coders manual,” 01 1997.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (281) D. Jurafsky 和 E. Shriberg，“Switchboard swbd-damsl 浅层话语功能注释编码器手册，”1997年01月。
- en: (282) A. Severyn and A. Moschitti, “Learning to rank short text pairs with convolutional
    deep neural networks,” in Proceedings of the 38th International ACM SIGIR Conference
    on Research and Development in Information Retrieval, Santiago, Chile, August
    9-13, 2015 (R. Baeza-Yates, M. Lalmas, A. Moffat, and B. A. Ribeiro-Neto, eds.),
    pp. 373–382, ACM, 2015.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (282) A. Severyn 和 A. Moschitti，“使用卷积深度神经网络学习对短文本对的排序，”发表于第38届国际 ACM SIGIR 信息检索研究与开发会议，智利圣地亚哥，2015年8月9-13日（R.
    Baeza-Yates、M. Lalmas、A. Moffat 和 B. A. Ribeiro-Neto 编），pp. 373–382, ACM, 2015。
- en: (283) C. D. Manning, P. Raghavan, and H. Schütze, Introduction to information
    retrieval. Cambridge University Press, 2008.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (283) C. D. Manning、P. Raghavan 和 H. Schütze，《信息检索导论》。剑桥大学出版社，2008。
- en: '(284) T. Nakagawa, K. Inui, and S. Kurohashi, “Dependency tree-based sentiment
    classification using crfs with hidden variables,” in Human Language Technologies:
    Conference of the North American Chapter of the Association of Computational Linguistics,
    Proceedings, June 2-4, 2010, Los Angeles, California, USA, pp. 786–794, 2010.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (284) T. Nakagawa, K. Inui, 和 S. Kurohashi，“基于依赖树的情感分类使用带有隐藏变量的条件随机场”，发表于《人类语言技术：计算语言学协会北美分会会议论文集》，2010年6月2-4日，加州洛杉矶,
    页码 786–794, 2010年。
- en: (285) J. Howard and S. Ruder, “Universal language model fine-tuning for text
    classification,” in Proc. ACL, 2018, pp. 328–339, 2018.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (285) J. Howard 和 S. Ruder，“用于文本分类的通用语言模型微调”，发表于《ACL会议论文集》，2018年, 页码 328–339,
    2018年。
- en: (286) G. Wang, C. Li, W. Wang, Y. Zhang, D. Shen, X. Zhang, R. Henao, and L. Carin,
    “Joint embedding of words and labels for text classification,” in Proc. ACL, 2018,
    pp. 2321–2331, 2018.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (286) G. Wang, C. Li, W. Wang, Y. Zhang, D. Shen, X. Zhang, R. Henao, 和 L. Carin，“用于文本分类的词汇和标签联合嵌入”，发表于《ACL会议论文集》，2018年,
    页码 2321–2331, 2018年。
- en: (287) X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks for
    natural language understanding,” in Proc. ACL, 2019, pp. 4487–4496, 2019.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (287) X. Liu, P. He, W. Chen, 和 J. Gao，“用于自然语言理解的多任务深度神经网络”，发表于《ACL会议论文集》，2019年,
    页码 4487–4496, 2019年。
- en: '(288) P. K. Pushp and M. M. Srivastava, “Train once, test anywhere: Zero-shot
    learning for text classification,” CoRR, vol. abs/1712.05972, 2017.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (288) P. K. Pushp 和 M. M. Srivastava，“训练一次，随处测试：文本分类的零样本学习”，《CoRR》，卷号 abs/1712.05972,
    2017年。
- en: (289) C. Song, S. Zhang, N. Sadoughi, P. Xie, and E. P. Xing, “Generalized zero-shot
    text classification for ICD coding,” in Proc. IJCAI, 2020, pp. 4018–4024, 2020.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (289) C. Song, S. Zhang, N. Sadoughi, P. Xie, 和 E. P. Xing，“用于ICD编码的广义零样本文本分类”，发表于《IJCAI会议论文集》，2020年,
    页码 4018–4024, 2020年。
- en: (290) R. Geng, B. Li, Y. Li, X. Zhu, P. Jian, and J. Sun, “Induction networks
    for few-shot text classification,” in Proc. EMNLP, 2019, pp. 3902–3911, 2019.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (290) R. Geng, B. Li, Y. Li, X. Zhu, P. Jian, 和 J. Sun，“用于少样本文本分类的引导网络”，发表于《EMNLP会议论文集》，2019年,
    页码 3902–3911, 2019年。
- en: '(291) S. Deng, N. Zhang, Z. Sun, J. Chen, and H. Chen, “When low resource NLP
    meets unsupervised language model: Meta-pretraining then meta-learning for few-shot
    text classification (student abstract),” in Proc. AAAI, 2020, pp. 13773–13774,
    2020.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (291) S. Deng, N. Zhang, Z. Sun, J. Chen, 和 H. Chen，“当低资源自然语言处理遇上无监督语言模型：元预训练然后元学习用于少样本文本分类（学生摘要）”，发表于《AAAI会议论文集》，2020年,
    页码 13773–13774, 2020年。
- en: (292) R. Geng, B. Li, Y. Li, J. Sun, and X. Zhu, “Dynamic memory induction networks
    for few-shot text classification,” in Proc. ACL, 2020, pp. 1087–1094, 2020.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (292) R. Geng, B. Li, Y. Li, J. Sun, 和 X. Zhu，“用于少样本文本分类的动态记忆引导网络”，发表于《ACL会议论文集》，2020年,
    页码 1087–1094, 2020年。
- en: '(293) K. R. Rojas, G. Bustamante, A. Oncevay, and M. A. S. Cabezudo, “Efficient
    strategies for hierarchical text classification: External knowledge and auxiliary
    tasks,” in Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 2252–2257, 2020.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (293) K. R. Rojas, G. Bustamante, A. Oncevay, 和 M. A. S. Cabezudo，“层次文本分类的高效策略：外部知识和辅助任务”，发表于《第58届计算语言学协会年会》，ACL
    2020, 在线, 2020年7月5-10日, 页码 2252–2257, 2020年。
- en: (294) N. Shanavas, H. Wang, Z. Lin, and G. I. Hawe, “Knowledge-driven graph
    similarity for text classification,” Int. J. Mach. Learn. Cybern., vol. 12, no. 4,
    pp. 1067–1081, 2021.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (294) N. Shanavas, H. Wang, Z. Lin, 和 G. I. Hawe，“用于文本分类的知识驱动图相似度”，《国际机器学习与网络杂志》，第12卷，第4期，页码
    1067–1081, 2021年。
- en: (295) Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu, and J. Zhao, “An end-to-end
    model for question answering over knowledge base with cross-attention combining
    global knowledge,” in Proc. ACL, 2017, pp. 221–231, 2017.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (295) Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu, 和 J. Zhao，“结合全球知识的跨注意力知识库问答的端到端模型”，发表于《ACL会议论文集》，2017年,
    页码 221–231, 2017年。
- en: '(296) R. Türker, L. Zhang, M. Koutraki, and H. Sack, “TECNE: knowledge based
    text classification using network embeddings,” in Proc. EKAW, 2018, pp. 53–56,
    2018.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(296) R. Türker, L. Zhang, M. Koutraki, 和 H. Sack，“TECNE: 使用网络嵌入的基于知识的文本分类”，发表于《EKAW会议论文集》，2018年,
    页码 53–56, 2018年。'
- en: '(297) X. Liang, D. Cheng, F. Yang, Y. Luo, W. Qian, and A. Zhou, “F-HMTC: detecting
    financial events for investment decisions based on neural hierarchical multi-label
    text classification,” in Proceedings of the Twenty-Ninth International Joint Conference
    on Artificial Intelligence, IJCAI 2020, pp. 4490–4496, 2020.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(297) X. Liang, D. Cheng, F. Yang, Y. Luo, W. Qian, 和 A. Zhou，“F-HMTC: 基于神经层次多标签文本分类的投资决策财务事件检测”，发表于《第二十九届国际人工智能联合会议》,
    IJCAI 2020, 页码 4490–4496, 2020年。'
- en: (298) S. P. B., S. Modi, K. S. Hareesha, and S. Kumar, “Classification and comparison
    of malignancy detection of cervical cells based on nucleus and textural features
    in microscopic images of uterine cervix,” Int. J. Medical Eng. Informatics, vol. 13,
    no. 1, pp. 1–13, 2021.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (298) S. P. B., S. Modi, K. S. Hareesha, 和 S. Kumar, “基于细胞核和纹理特征在宫颈显微图像中恶性检测的分类与比较，”《国际医学工程与信息学杂志》，第13卷，第1期，第1–13页，2021年。
- en: (299) T. Wang, L. Liu, N. Liu, H. Zhang, L. Zhang, and S. Feng, “A multi-label
    text classification method via dynamic semantic representation model and deep
    neural network,” Appl. Intell., vol. 50, no. 8, pp. 2339–2351, 2020.
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (299) T. Wang, L. Liu, N. Liu, H. Zhang, L. Zhang, 和 S. Feng, “一种通过动态语义表示模型和深度神经网络的多标签文本分类方法，”《应用智能》，第50卷，第8期，第2339–2351页，2020年。
- en: (300) B. Wang, X. Hu, P. Li, and P. S. Yu, “Cognitive structure learning model
    for hierarchical multi-label text classification,” Knowl. Based Syst., vol. 218,
    p. 106876, 2021.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (300) B. Wang, X. Hu, P. Li, 和 P. S. Yu, “用于分层多标签文本分类的认知结构学习模型，”《知识基系统》，第218卷，第106876页，2021年。
- en: '(301) J. Du, Y. Huang, and K. Moilanen, “Pointing to select: A fast pointer-lstm
    for long text classification,” in Proceedings of the 28th International Conference
    on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December
    8-13, 2020, pp. 6184–6193, 2020.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (301) J. Du, Y. Huang, 和 K. Moilanen, “指向选择：一种快速的pointer-lstm用于长文本分类，” 载于第28届国际计算语言学大会论文集，COLING
    2020，西班牙巴塞罗那（在线），2020年12月8-13日，第6184–6193页，2020年。
- en: '(302) J. Du, C. Vong, and C. L. P. Chen, “Novel efficient RNN and lstm-like
    architectures: Recurrent and gated broad learning systems and their applications
    for text classification,” IEEE Trans. Cybern., vol. 51, no. 3, pp. 1586–1597,
    2021.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (302) J. Du, C. Vong, 和 C. L. P. Chen, “新型高效的RNN和类似LSTM的架构：递归和门控的广泛学习系统及其在文本分类中的应用，”《IEEE网络信息学汇刊》，第51卷，第3期，第1586–1597页，2021年。
- en: (303) T. Kanchinadam, Q. You, K. Westpfahl, J. Kim, S. Gunda, S. Seith, and
    G. Fung, “A simple yet brisk and efficient active learning platform for text classification,”
    CoRR, vol. abs/2102.00426, 2021.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (303) T. Kanchinadam, Q. You, K. Westpfahl, J. Kim, S. Gunda, S. Seith, 和 G.
    Fung, “一个简单而迅速且高效的文本分类主动学习平台，” CoRR，第abs/2102.00426卷，2021年。
- en: (304) Y. Zhou, J. Jiang, K. Chang, and W. Wang, “Learning to discriminate perturbations
    for blocking adversarial attacks in text classification,” in Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019,
    Hong Kong, China, November 3-7, 2019, pp. 4903–4912, 2019.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (304) Y. Zhou, J. Jiang, K. Chang, 和 W. Wang, “学习辨别扰动以阻挡文本分类中的对抗攻击，” 载于2019年自然语言处理实证方法大会和第9届国际联合自然语言处理会议论文集，EMNLP-IJCNLP
    2019，香港，中国，2019年11月3-7日，第4903–4912页，2019年。
- en: '(305) A. Azizi, I. A. Tahmid, A. Waheed, N. Mangaokar, J. Pu, M. Javed, C. K.
    Reddy, and B. Viswanath, “T-miner: A generative approach to defend against trojan
    attacks on dnn-based text classification,” CoRR, vol. abs/2103.04264, 2021.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (305) A. Azizi, I. A. Tahmid, A. Waheed, N. Mangaokar, J. Pu, M. Javed, C. K.
    Reddy, 和 B. Viswanath, “T-miner：一种生成性方法以防御针对基于DNN的文本分类的木马攻击，” CoRR，第abs/2103.04264卷，2021年。
