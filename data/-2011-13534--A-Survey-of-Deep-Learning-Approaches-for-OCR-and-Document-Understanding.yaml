- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2011.13534] A Survey of Deep Learning Approaches for OCR and Document Understanding'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2011.13534] 深度学习在 OCR 和文档理解中的应用调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.13534](https://ar5iv.labs.arxiv.org/html/2011.13534)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2011.13534](https://ar5iv.labs.arxiv.org/html/2011.13534)
- en: A Survey of Deep Learning Approaches for OCR and Document Understanding
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在 OCR 和文档理解中的应用调查
- en: Nishant Subramani
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Nishant Subramani
- en: Intel Labs
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Intel Labs
- en: Masakhane
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Masakhane
- en: nishant.subramani23@gmail.com Alexandre Matton
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: nishant.subramani23@gmail.com Alexandre Matton
- en: Scale AI
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Scale AI
- en: alexandre.matton@scale.com Malcolm Greaves
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: alexandre.matton@scale.com Malcolm Greaves
- en: Scale AI
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Scale AI
- en: malcolm.greaves@scale.com Adrian Lam
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: malcolm.greaves@scale.com Adrian Lam
- en: Scale AI
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Scale AI
- en: adrian.lam@scale.com
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: adrian.lam@scale.com
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Documents are a core part of many businesses in many fields such as law, finance,
    and technology among others. Automatic understanding of documents such as invoices,
    contracts, and resumes is lucrative, opening up many new avenues of business.
    The fields of natural language processing and computer vision have seen tremendous
    progress through the development of deep learning such that these methods have
    started to become infused in contemporary document understanding systems. In this
    survey paper, we review different techniques for document understanding for documents
    written in English and consolidate methodologies present in literature to act
    as a jumping-off point for researchers exploring this area.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文档是许多行业（如法律、金融和技术等）核心的一部分。自动理解文档（如发票、合同和简历）具有很高的价值，开辟了许多新的商业机会。自然语言处理和计算机视觉领域通过深度学习取得了巨大的进展，这些方法已开始融入当代文档理解系统。在这篇调查论文中，我们回顾了用于理解英文文档的不同技术，并整合了文献中存在的方法，作为研究人员探索这一领域的起点。
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Humans compose documents to record and preserve information. As information
    carrying vehicles, documents are written using different layouts to represent
    diverse sets of information for a variety of different consumers. In this work,
    we look at the problem of document understanding for documents written in English.
    Here, we take the term document understanding to mean the automated process of
    reading, interpreting, and extracting information from the written text and illustrated
    figures contained within a document’s pages. From the perspective as practitioners
    of machine learning, this survey covers the methods by which we build models to
    automatically understand documents that were originally composed for human consumption.
    Document understanding models take in documents and segment pages of documents
    into useful parts (i.e. regions corresponding to a specific table or property),
    often using optical character recognition (OCR) (Mori et al., [1999](#bib.bib73))
    with some level of document layout analysis. These models use this information
    to understand the contents of the document at large, e.g. that this region or
    bounding box corresponds to an address. In this survey, we focus on these aspects
    of document understanding at a more granular level and discuss popular methods
    for these tasks. Our goal is to summarize the approaches present in modern document
    understanding and highlight current trends and limitations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人类编写文档以记录和保存信息。作为信息载体，文档采用不同的布局来表示多样的信息，以满足各种不同的需求。在这项工作中，我们关注的是英文文档的理解问题。这里，我们将文档理解定义为从文档页面中的书面文本和插图中自动读取、解释和提取信息的过程。从机器学习实践者的角度来看，这项调查涵盖了我们如何构建模型来自动理解最初为人类使用而编写的文档。文档理解模型接收文档并将文档页面分割成有用的部分（即对应于特定表格或属性的区域），通常使用光学字符识别（OCR）（Mori
    等人，[1999](#bib.bib73)）和一定程度的文档布局分析。这些模型利用这些信息来理解文档的整体内容，例如该区域或边界框对应于一个地址。在这项调查中，我们更详细地关注文档理解的这些方面，并讨论这些任务的常见方法。我们的目标是总结现代文档理解中的方法，并突出当前的趋势和局限性。
- en: 'In Section [2](#S2 "2 Document Processing & Understanding ‣ A Survey of Deep
    Learning Approaches for OCR and Document Understanding"), we discuss some general
    themes in modern NLP and document understanding and provide a framework for building
    end-to-end automated document understanding systems. Next, in Section [3](#S3
    "3 Optical Character Recognition ‣ A Survey of Deep Learning Approaches for OCR
    and Document Understanding"), we look at the best methods for OCR encompassing
    both text detection (Section [3.1](#S3.SS1 "3.1 Text Detection ‣ 3 Optical Character
    Recognition ‣ A Survey of Deep Learning Approaches for OCR and Document Understanding"))
    and text transcription (Section [3.3](#S3.SS3 "3.3 Text Transcription ‣ 3 Optical
    Character Recognition ‣ A Survey of Deep Learning Approaches for OCR and Document
    Understanding")). We take a broader view of the document understanding problem
    in section [4](#S4 "4 Document Layout Analysis ‣ A Survey of Deep Learning Approaches
    for OCR and Document Understanding"), presenting multiple approaches to document
    layout analysis: the problem of locating relevant information on each page. Following
    this, we discuss popular approaches for information extraction (Section [5](#S5
    "5 Information Extraction ‣ A Survey of Deep Learning Approaches for OCR and Document
    Understanding")).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2节](#S2 "2 Document Processing & Understanding ‣ A Survey of Deep Learning
    Approaches for OCR and Document Understanding")中，我们讨论了现代NLP和文档理解中的一些一般主题，并提供了构建端到端自动化文档理解系统的框架。接下来，在第[3节](#S3
    "3 Optical Character Recognition ‣ A Survey of Deep Learning Approaches for OCR
    and Document Understanding")中，我们查看了最佳的OCR方法，包括文本检测（第[3.1节](#S3.SS1 "3.1 Text Detection
    ‣ 3 Optical Character Recognition ‣ A Survey of Deep Learning Approaches for OCR
    and Document Understanding)")和文本转录（第[3.3节](#S3.SS3 "3.3 Text Transcription ‣ 3
    Optical Character Recognition ‣ A Survey of Deep Learning Approaches for OCR and
    Document Understanding")）。在第[4节](#S4 "4 Document Layout Analysis ‣ A Survey of
    Deep Learning Approaches for OCR and Document Understanding")中，我们从更广泛的视角看待文档理解问题，介绍了文档布局分析的多种方法：即在每页上定位相关信息的问题。接下来，我们讨论了信息提取的流行方法（第[5节](#S5
    "5 Information Extraction ‣ A Survey of Deep Learning Approaches for OCR and Document
    Understanding")）。
- en: 2 Document Processing & Understanding
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文档处理与理解
- en: Document processing historically involved handcrafted rule-based algorithms (Lebourgeois
    et al., [1992](#bib.bib53); Ha et al., [1995](#bib.bib32); Amin and Shiu, [2001](#bib.bib2)),
    but with the widespread success of deep learning (Collobert and Weston, [2008](#bib.bib17);
    Krizhevsky et al., [2012](#bib.bib52); Sutskever et al., [2014](#bib.bib90)),
    computer vision (CV) and natural language processing (NLP) based methods have
    come to the fore. Advancements in object detection and image segmentation have
    led to systems that edge close to human performance on a variety of tasks (Redmon
    et al., [2016](#bib.bib79); Lin et al., [2017](#bib.bib62)). As a result, these
    methods have been applied to a variety of other domains including NLP and speech (Gehring
    et al., [2017](#bib.bib27); Wu et al., [2018](#bib.bib96); Subramani and Rao,
    [2020](#bib.bib88)). Since documents can be read and viewed as a visual information
    medium, many practitioners leverage computer vision techniques as well and use
    them for text detection and instance segmentation (Long et al., [2018](#bib.bib69);
    Katti et al., [2018](#bib.bib49)). We cover specific methods to do these in Sections [3.1](#S3.SS1
    "3.1 Text Detection ‣ 3 Optical Character Recognition ‣ A Survey of Deep Learning
    Approaches for OCR and Document Understanding") and [4.1](#S4.SS1 "4.1 Instance
    Segmentation for Layout Analysis ‣ 4 Document Layout Analysis ‣ A Survey of Deep
    Learning Approaches for OCR and Document Understanding").
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文档处理在历史上涉及手工制作的基于规则的算法（Lebourgeois et al., [1992](#bib.bib53); Ha et al., [1995](#bib.bib32);
    Amin and Shiu, [2001](#bib.bib2)），但随着深度学习的广泛成功（Collobert and Weston, [2008](#bib.bib17);
    Krizhevsky et al., [2012](#bib.bib52); Sutskever et al., [2014](#bib.bib90)），计算机视觉（CV）和自然语言处理（NLP）方法已经成为主流。对象检测和图像分割的进展导致了接近人类表现的系统（Redmon
    et al., [2016](#bib.bib79); Lin et al., [2017](#bib.bib62)）。因此，这些方法已被应用于包括NLP和语音在内的各种其他领域（Gehring
    et al., [2017](#bib.bib27); Wu et al., [2018](#bib.bib96); Subramani and Rao,
    [2020](#bib.bib88)）。由于文档可以被视为一种视觉信息媒介，许多从业者也利用计算机视觉技术，用于文本检测和实例分割（Long et al.,
    [2018](#bib.bib69); Katti et al., [2018](#bib.bib49)）。我们在第[3.1节](#S3.SS1 "3.1
    Text Detection ‣ 3 Optical Character Recognition ‣ A Survey of Deep Learning Approaches
    for OCR and Document Understanding")和第[4.1节](#S4.SS1 "4.1 Instance Segmentation
    for Layout Analysis ‣ 4 Document Layout Analysis ‣ A Survey of Deep Learning Approaches
    for OCR and Document Understanding")中详细介绍了这些具体方法。
- en: The widespread success and popularity of large pretrained language models such
    as ELMo and BERT have caused document understanding to shift towards using deep
    learning based models (Peters et al., [2018](#bib.bib75); Devlin et al., [2019](#bib.bib20)).
    These models can be fine-tuned for a variety of tasks and have replaced word vectors
    as the de-facto standard for pretraining for natural language tasks. However,
    language models, both recurrent neural network based and transformer based (Vaswani
    et al., [2017](#bib.bib92)), struggle with long sequences (Cho et al., [2014a](#bib.bib13);
    Subramani et al., [2019](#bib.bib87); Subramani and Suresh, [2020](#bib.bib89)).
    Given that texts can be very dense and long in business documents, model architecture
    modifications are necessary. The most simple approach is to truncate documents
    into smaller sequences of 512 tokens such that pretrained language models can
    be used off-the-shelf (Xie et al., [2019](#bib.bib97); Joshi et al., [2019](#bib.bib44)).
    Another approach that has gained traction recently is based on reducing the complexity
    of the self-attention component of transformer-based language models (Child et al.,
    [2019](#bib.bib11); Beltagy et al., [2020](#bib.bib7); Katharopoulos et al., [2020](#bib.bib48);
    Kitaev et al., [2020](#bib.bib51); Choromanski et al., [2020](#bib.bib15)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大型预训练语言模型如ELMo和BERT的广泛成功和流行使得文档理解转向使用基于深度学习的模型（Peters等， [2018](#bib.bib75)；Devlin等，
    [2019](#bib.bib20)）。这些模型可以针对各种任务进行微调，并已取代词向量成为自然语言任务预训练的事实标准。然而，语言模型，无论是基于递归神经网络还是基于变换器（Vaswani等，
    [2017](#bib.bib92)），在处理长序列时仍然存在困难（Cho等， [2014a](#bib.bib13)；Subramani等， [2019](#bib.bib87)；Subramani和Suresh，
    [2020](#bib.bib89)）。鉴于商业文档中的文本可能非常密集和冗长，因此需要对模型架构进行修改。最简单的方法是将文档截断为较小的512个标记的序列，以便可以直接使用预训练语言模型（Xie等，
    [2019](#bib.bib97)；Joshi等， [2019](#bib.bib44)）。另一个最近获得关注的方法是基于减少变换器模型中自注意力组件的复杂性（Child等，
    [2019](#bib.bib11)；Beltagy等， [2020](#bib.bib7)；Katharopoulos等， [2020](#bib.bib48)；Kitaev等，
    [2020](#bib.bib51)；Choromanski等， [2020](#bib.bib15)）。
- en: 'All effective, modern, end-to-end document understanding systems present in
    the literature integrate multiple deep neural network architectures for both reading
    and comprehending a document’s content. Since documents are made for humans, not
    machines, practitioners must combine CV as well as NLP architectures into a unified
    solution. While specific use cases will dictate the exact techniques used, a full
    end-to-end system employs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中现有的所有有效、现代的端到端文档理解系统都整合了多种深度神经网络架构，以便读取和理解文档内容。由于文档是为人类而非机器制作的，因此从业者必须将计算机视觉（CV）和自然语言处理（NLP）架构结合成一个统一的解决方案。尽管具体的使用案例会决定使用的确切技术，但完整的端到端系统包括：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A computer-vision based document layout analysis module, which partitions each
    document page into distinct content regions. This model not only delineates between
    relevant and irrelevant regions, but also serves to categorize the type of content
    it identifies.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于计算机视觉的文档布局分析模块，将每个文档页面分割成不同的内容区域。该模型不仅区分了相关区域和不相关区域，还用于分类它所识别的内容类型。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An optical character recognition (OCR) model, whose purpose is to locate and
    faithfully transcribe all written text present in the document. Straddling the
    boundary between CV and NLP, OCR models may either use document layout analysis
    directly or solve the problem in an independent fashion.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 光学字符识别（OCR）模型的目的是定位和准确转录文档中所有的书面文本。OCR模型横跨计算机视觉（CV）和自然语言处理（NLP）的边界，它们可以直接使用文档布局分析，也可以以独立的方式解决问题。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Information extraction models that use the output of OCR or document layout
    analysis to comprehend and identify relationships between the information that
    is being conveyed in the document. Usually specialized to a particular domain
    and task, these models provide the structure necessary to make a document machine
    readable, providing utility in document understanding.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息提取模型利用OCR或文档布局分析的输出，以理解和识别文档中所传达的信息之间的关系。这些模型通常针对特定领域和任务进行专业化，提供了使文档机器可读所需的结构，在文档理解中提供了实用性。
- en: In the following sections, we expand upon these concepts that constitute an
    end-to-end document understanding solution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将详细阐述这些构成端到端文档理解解决方案的概念。
- en: 3 Optical Character Recognition
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 光学字符识别
- en: 'OCR has two primary components: text detection and text transcription. Generally,
    these two components are separate and employ different models for each task. Below,
    we discuss state-of-the-art methods for each of these components and show how
    a document can be processed through different generic OCR systems. See Figure [1](#S3.F1
    "Figure 1 ‣ 3.1 Text Detection ‣ 3 Optical Character Recognition ‣ A Survey of
    Deep Learning Approaches for OCR and Document Understanding") for details.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OCR 主要包括两个组件：文本检测和文本转录。通常，这两个组件是分开的，并为每个任务使用不同的模型。下面，我们将讨论每个组件的最新方法，并展示如何通过不同的通用
    OCR 系统处理文档。详细信息请参见图 [1](#S3.F1 "Figure 1 ‣ 3.1 Text Detection ‣ 3 Optical Character
    Recognition ‣ A Survey of Deep Learning Approaches for OCR and Document Understanding")。
- en: 3.1 Text Detection
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 文本检测
- en: 'Text detection is the task of finding text present in a page or image. The
    input, an image, is often represented by a three dimensional tensor, $C\times
    H\times W$, where $C$ is the number of channels (often three, for red, green and
    blue), $H$ is the height, and $W$ is the width of the image. Text detection is
    a challenging problem because text comes in a variety of shapes and orientations
    and can often be distorted. We explore two common ways researchers pose the text
    detection problem: as an object detection task and as a instance segmentation
    task. A text detection model must either learn to output coordinates of bounding
    boxes around text (object detection), or a mask, where pixels with text are marked
    and pixels without are not (instance segmentation).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文本检测是识别页面或图像中存在的文本的任务。输入，即图像，通常由三维张量表示，$C\times H\times W$，其中 $C$ 是通道数（通常为三，分别对应红色、绿色和蓝色），$H$
    是高度，$W$ 是图像的宽度。文本检测是一个具有挑战性的问题，因为文本的形状和方向多种多样，并且往往会发生扭曲。我们探讨了研究人员提出文本检测问题的两种常见方式：作为物体检测任务和作为实例分割任务。文本检测模型必须学习输出围绕文本的边界框的坐标（物体检测），或一个掩膜，其中文本像素被标记，而没有文本的像素未被标记（实例分割）。
- en: '![Refer to caption](img/e1a40dcc9be2ed3a707c5ec58caf778b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e1a40dcc9be2ed3a707c5ec58caf778b.png)'
- en: 'Figure 1: Here, we show the general OCR process. A document can take the left
    path and go through an object detection model, which outputs bounding boxes, and
    a transcription model that transcribes the text in each of those bounding boxes.
    If the document takes the middle path, the object passes through a generic text
    instance segmentation model that colors pixels black if they contain text and
    a text transcription model that transcribes the regions of text the instance segmentation
    model identifies. If the document takes the right path, the model goes through
    a character-specific instance segmentation model, which outputs which character
    a pixel corresponds to. All paths produce the same structured output. The document
    comes from FUNSD (Jaume et al., [2019](#bib.bib43)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：这里展示了通用的 OCR 过程。文档可以沿着左侧路径，通过物体检测模型，该模型输出边界框，以及转录模型，该模型转录每个边界框中的文本。如果文档走中间路径，则物体经过通用文本实例分割模型，该模型将包含文本的像素标记为黑色，并且文本转录模型转录实例分割模型识别的文本区域。如果文档走右侧路径，则模型经过特定字符的实例分割模型，该模型输出每个像素对应的字符。所有路径产生相同的结构化输出。文档来源于
    FUNSD（Jaume 等，[2019](#bib.bib43)）。
- en: 3.1.1 Text Detection as Object Detection
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 将文本检测视为物体检测
- en: Traditionally, text detection revolved around hand-crafting features to detect
    characters (Matas et al., [2004](#bib.bib71); Lowe, [2004](#bib.bib70)). Advances
    in deep learning, especially in object detection and semantic segmentation, have
    led to a change in how text detection is tackled. Using these well-performing
    object detectors from the traditional computer vision literature, such as the
    Single-Shot MultiBox Detector (SSD) and Faster R-CNN models (Liu et al., [2016](#bib.bib63);
    Ren et al., [2015](#bib.bib81)), practitioners build efficient text detectors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，文本检测依赖于手工设计特征来检测字符（Matas 等，[2004](#bib.bib71)；Lowe，[2004](#bib.bib70)）。深度学习的进步，特别是在物体检测和语义分割方面，已经改变了文本检测的处理方式。利用传统计算机视觉文献中的高性能物体检测器，如单次检测多框检测器（SSD）和
    Faster R-CNN 模型（Liu 等，[2016](#bib.bib63)；Ren 等，[2015](#bib.bib81)），从业人员构建了高效的文本检测器。
- en: One of the first papers applying a regression-based detector for text is TextBoxes (Liao
    et al., [2016](#bib.bib60), [2018a](#bib.bib59)). They added long default boxes
    that have large aspect ratios to SSD, in order to adapt the object detector to
    text. Several papers built on this work to make regression-based models resilient
    to orientations, like the Deep Matching Prior Network (DMPNet) and the Rotation-Sensitive
    Regression Detector (RRD) (Liu and Jin, [2017](#bib.bib67); Liao et al., [2018b](#bib.bib61)).
    Other papers have a similar approach to the problem, but develop their own proposal
    network that is tuned towards text rather than towards natural images. For instance,
    Tian et al. ([2016](#bib.bib91)) combine convolutional networks with recurrent
    networks using a vertical anchor mechanism in their Connectionist Text Proposal
    Network to improve accuracy for horizontal text.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个将回归基础的检测器应用于文本的论文之一是 TextBoxes (Liao 等，[2016](#bib.bib60)，[2018a](#bib.bib59))。他们在
    SSD 中添加了具有大长宽比的长默认框，以便将目标检测器适应于文本。几篇论文在此基础上进行了扩展，使回归基础的模型对方向具有韧性，如 Deep Matching
    Prior Network (DMPNet) 和 Rotation-Sensitive Regression Detector (RRD) (Liu 和 Jin，[2017](#bib.bib67)；Liao
    等，[2018b](#bib.bib61))。其他论文采用类似的方法，但开发了自己的提议网络，该网络针对文本而非自然图像进行调优。例如，Tian 等人 ([2016](#bib.bib91))
    在其 Connectionist Text Proposal Network 中结合了卷积网络和递归网络，使用垂直锚点机制以提高对横向文本的准确性。
- en: Object detection models are generally evaluated via an intersection over union
    (IoU) metric and an F1 score. The metric computes how much of a candidate bounding
    box overlaps with the ground truth bounding box (the intersection) divided by
    the total space occupied by both the candidate and ground truth bounding boxes
    (the union). Next, an IoU threshold $\tau$ is chosen to determine which predicted
    boxes count as true positives (IoU $\geq\tau$). The remainder are classified as
    false positives. Any box that the model fails to detect is classified as a false
    negative. Using those definitions, an F1 score is computed to evaluate the object
    detection model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测模型通常通过交并比 (IoU) 指标和 F1 分数来评估。该指标计算候选边界框与真实边界框（交集）重叠的部分除以候选和真实边界框占据的总空间（并集）。接下来，选择
    IoU 阈值 $\tau$ 来确定哪些预测框算作真正的正例（IoU $\geq\tau$）。其余的被归类为假正例。模型未能检测到的任何框都被归类为假负例。使用这些定义，计算
    F1 分数来评估目标检测模型。
- en: 3.1.2 Text Detection as Instance Segmentation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 文本检测作为实例分割
- en: 'Text detection in documents has its own unique set of challenges: notably,
    the text is usually dense and documents contain a lot more text than what is usually
    present in natural images. To combat this density problem, text detection can
    be posed as an ultra-dense instance segmentation task. Instance segmentation is
    the task of classifying each pixel of an image as specific, pre-defined categories.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的文本检测面临着独特的挑战：尤其是文本通常非常密集，而且文档中的文本量远超过自然图像中通常出现的文本量。为了应对这种密度问题，文本检测可以被视为一个超密集的实例分割任务。实例分割的任务是将图像的每一个像素分类为特定的、预定义的类别。
- en: Segmentation-based text detectors work at the pixel level to identify regions
    of text. These per-pixel predictions are often used to estimate probabilities
    of text regions, characters, and their relationships among adjacent characters
    in a unified framework. Practitioners use popular segmentation methods like Fully
    Convolutional Networks (FCN) to detect text (Long et al., [2015](#bib.bib68)),
    improving upon object detection models, especially when text is misaligned or
    distorted. Several papers build on this segmentation foundation to output word
    bounding areas by extracting bounding areas directly from the segmentation output (Yao
    et al., [2016](#bib.bib100); He et al., [2017a](#bib.bib33); Deng et al., [2018](#bib.bib18)).
    TextSnake extends this further by predicting the text region, center line, direction
    of text, and candidate radius from an FCN (Long et al., [2018](#bib.bib69)). These
    features are then combined with a striding algorithm to extract the central axis
    points to reconstruct the text instance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于分割的文本检测器在像素级别工作，以识别文本区域。这些每像素预测通常用于估计文本区域、字符及其相邻字符之间关系的概率。实践者使用流行的分割方法，如全卷积网络（FCN），来检测文本（Long
    et al., [2015](#bib.bib68)），改进了对象检测模型，尤其是在文本错位或扭曲时。几篇论文在这个分割基础上建立，通过直接从分割输出中提取边界区域来输出单词边界区域（Yao
    et al., [2016](#bib.bib100); He et al., [2017a](#bib.bib33); Deng et al., [2018](#bib.bib18)）。TextSnake进一步扩展了这一点，通过从FCN（Long
    et al., [2018](#bib.bib69)）中预测文本区域、中心线、文本方向和候选半径。这些特征然后与步幅算法结合，提取中央轴点以重建文本实例。
- en: 3.2 Word-level versus character-level
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 单词级与字符级
- en: While most papers cited above try to directly detect words or even lines of
    words, some papers argue that character-level detection is an easier problem than
    general text detection because characters are less ambiguous than text lines or
    words. CRAFT uses an FCN model to output a two-dimensional Gaussian heatmap for
    each character (Baek et al., [2019](#bib.bib4)). Characters that are close together
    are then grouped together in a rotated rectangle that has the smallest area possible
    to still encapsulate the set of characters. More recently, Ye et al. ([2020](#bib.bib101))
    combine global, word-level, and character-level features obtained using Region
    Proposal Networks (RPN) to great success.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上面提到的大多数论文尝试直接检测单词甚至单词行，但一些论文认为字符级检测比一般的文本检测更容易，因为字符比文本行或单词更不容易产生歧义。CRAFT使用FCN模型为每个字符输出一个二维高斯热图（Baek
    et al., [2019](#bib.bib4)）。然后，将相互靠近的字符组合在一个旋转的矩形中，该矩形具有最小的面积，以便仍能包含这些字符集合。最近，Ye等人（[2020](#bib.bib101)）结合了通过区域提议网络（RPN）获得的全局、单词级和字符级特征，取得了巨大的成功。
- en: Most of the models described above were mainly developed for text scene detection,
    but can be easily adapted to document text detection to handle difficult cases
    like distorted text. We expect less distortion in documents than in natural images,
    but poorly scanned documents or documents with certain fonts could still pose
    these problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述的大多数模型主要用于文本场景检测，但可以很容易地调整用于文档文本检测，以处理诸如扭曲文本等困难情况。我们预期文档中的扭曲程度会比自然图像中的少，但扫描质量差的文档或具有某些字体的文档仍可能存在这些问题。
- en: 3.3 Text Transcription
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 文本转录
- en: Text transcription is the task of transcribing the text present in an image.
    The input, an image, is often a crop corresponding to either a character, word,
    or sequence of words, and has dimension $C\times H^{\prime}\times W^{\prime}$.
    A text transcription model must learn to ingest this cropped image and output
    a sequence of tokens belonging to some pre-specified vocabulary $V$. $V$ often
    corresponds to a set of characters. For digit recognition for instance, this is
    the most intuitive approach (Goodfellow et al., [2013](#bib.bib28)). Otherwise,
    $V$ can also correspond to a set of words, similarly to a word-level language
    modeling problem (Jaderberg et al., [2014](#bib.bib42)). In both cases, the problem
    can be framed as a multi-class classification problem with the number of classes
    equal to the size of the vocabulary $V$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 文本转录是将图像中存在的文本转录的任务。输入图像通常是对应于字符、单词或单词序列的裁剪图像，维度为 $C\times H^{\prime}\times
    W^{\prime}$。文本转录模型必须学习接收这个裁剪图像，并输出一个属于某些预先指定词汇表 $V$ 的令牌序列。$V$ 通常对应于一组字符。例如，对于数字识别，这是一种最直观的方法（Goodfellow
    et al., [2013](#bib.bib28)）。否则，$V$ 也可以对应于一组单词，类似于单词级语言建模问题（Jaderberg et al., [2014](#bib.bib42)）。在这两种情况下，问题都可以被表述为一个多类分类问题，类的数量等于词汇表
    $V$ 的大小。
- en: Word-level text transcription models require more data as the number of classes
    in the multi-class classification problem is much larger than for character-level.
    On one hand, predicting words instead of characters decreases the probability
    of making small typos (like replacing an "a" by an "o" in a word like "elephant").
    On the other, limiting oneself to a word-level vocabulary means that it is not
    possible to transcribe words which are not part of this vocabulary. This problem
    doesn’t exist at the character-level, as the number of characters is limited.
    As long as we know the language of the document, it is straightforward to build
    a vocabulary which contains all the possible characters. Subword units are a viable
    alternative (Sennrich et al., [2016](#bib.bib83)), as they alleviate the issues
    present in both word and character level transcription.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 词级别的文本转录模型需要更多的数据，因为多分类问题中的类别数量比字符级别要大得多。一方面，预测词语而不是字符可以减少出现小错误的概率（例如，将“elephant”中的“a”替换为“o”）。另一方面，限制自己使用词汇表意味着无法转录不在词汇表中的词汇。这种问题在字符级别不存在，因为字符的数量是有限的。只要我们知道文档的语言，建立一个包含所有可能字符的词汇表是直接的。子词单元是一个可行的替代方案（Sennrich
    等，[2016](#bib.bib83)），因为它们缓解了词级和字符级转录中存在的问题。
- en: Recently the research community has moved towards using recurrent neural networks,
    specifically recurrent models with LSTM or GRU units on top of a convolutional
    image feature extractor (Hochreiter and Schmidhuber, [1997](#bib.bib37); Cho et al.,
    [2014b](#bib.bib14); Wang and Hu, [2017](#bib.bib93)). To transcribe a token,
    two different decoding mechanisms are often used. One is standard greedy decoding
    or beam search using an attention-based sequence decoder with cross entropy loss (Bahdanau
    et al., [2014](#bib.bib5)), exactly like decoding with a conditional language
    model. Sometimes images are poorly oriented or misaligned, reducing the effectiveness
    of standard sequence attention. To overcome this, He et al. ([2018](#bib.bib36))
    uses attention alignment, encoding spatial information of characters directly,
    while Shi et al. ([2016](#bib.bib85)) use spatial attention mechanisms directly.
    The second way in which transcription decoding is often done is with connectionist
    temporal classification (CTC) loss (Graves et al., [2006](#bib.bib29)), a common
    loss function in speech which models repeated characters in sequence outputs well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究社区已经转向使用递归神经网络，特别是将 LSTM 或 GRU 单元叠加在卷积图像特征提取器上（Hochreiter 和 Schmidhuber，[1997](#bib.bib37)；Cho
    等，[2014b](#bib.bib14)；Wang 和 Hu，[2017](#bib.bib93)）。在转录一个标记时，通常使用两种不同的解码机制。一种是标准的贪心解码或基于注意力的序列解码器使用的束搜索，带有交叉熵损失（Bahdanau
    等，[2014](#bib.bib5)），与使用条件语言模型的解码完全一样。有时图像的方向不佳或对齐不准确，这降低了标准序列注意力的有效性。为了解决这个问题，He
    等人（[2018](#bib.bib36)）使用了注意力对齐，直接编码字符的空间信息，而 Shi 等人（[2016](#bib.bib85)）直接使用了空间注意力机制。第二种常用的转录解码方式是使用连接时序分类（CTC）损失（Graves
    等，[2006](#bib.bib29)），这是一种在语音中常见的损失函数，能够很好地建模序列输出中的重复字符。
- en: The majority of text transcription models borrow from advances in sequence modeling
    for both text and speech and often can utilize these advancements well with only
    minor adjustments. As a result, practitioners seldom directly tackle this aspect
    relative to the other components of the document understanding task.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文本转录模型借鉴了文本和语音序列建模的进展，并且通常只需少量调整即可很好地利用这些进展。因此，实践者很少直接处理相对于文档理解任务其他组件的这一方面。
- en: 3.4 End-to-end models
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 端到端模型
- en: End-to-end approaches combine text detection and text transcription in order
    to improve both components jointly (Li et al., [2017](#bib.bib55)). For instance,
    if the text prediction has a very low probability, it means the detected box either
    did not capture the entire word or captured something that is not text. An end-to-end
    approach may be very effective in this case. Combining these two methods is fairly
    common and both Fast Oriented Text Spotting (FOTS) and TextSpotter with Explicit
    Alignment and Attention sequentially combine these models to train end-to-end (Liu
    et al., [2018](#bib.bib65); He et al., [2018](#bib.bib36)). These approaches use
    shared convolutions as features to both text detection and recognition, and implement
    methods for complex orientations of text. Feng et al. ([2019](#bib.bib24)) introduce
    TextDragon, an end-to-end model that performs well on distorted text by utilizing
    a differentiable region of interest slide operator, which specializes in correcting
    distortions in regions of interest. Mask TextSpotter is another end-to-end model
    that combines region proposal networks for bounding boxes with text and character
    segmentation (Liao et al., [2020](#bib.bib58)). These recent works show the power
    of end-to-end OCR solutions in reducing errors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端方法结合了文本检测和文本转录，以便共同改进这两个组件（Li et al., [2017](#bib.bib55)）。例如，如果文本预测的概率非常低，这意味着检测到的框要么没有捕捉到整个单词，要么捕捉到的是非文本内容。在这种情况下，端到端的方法可能非常有效。结合这两种方法是相当常见的，Fast
    Oriented Text Spotting（FOTS）和TextSpotter with Explicit Alignment and Attention依次结合这些模型以进行端到端训练（Liu
    et al., [2018](#bib.bib65); He et al., [2018](#bib.bib36)）。这些方法使用共享卷积作为文本检测和识别的特征，并实施文本复杂方向的方法。Feng
    et al.（[2019](#bib.bib24)）介绍了TextDragon，这是一种端到端模型，通过利用可微分的兴趣区域滑动操作符，在扭曲文本上表现良好，该操作符专注于纠正兴趣区域中的扭曲。Mask
    TextSpotter是另一种端到端模型，结合了边界框的区域建议网络与文本和字符分割（Liao et al., [2020](#bib.bib58)）。这些近期工作展示了端到端OCR解决方案在减少错误方面的强大能力。
- en: Yet, having separate text detection and text recognition models offers more
    flexibility. First, the two models can be trained separately. In the case where
    only a small dataset is available to train the whole OCR module, but a lot of
    text recognition data is easily accessible, it makes sense to leverage this big
    amount of data in the training of the recognition model. Moreover, with two separate
    models, it is easy to compute two separate sets of metrics and have a more complete
    understanding of where the bottleneck might be.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有独立的文本检测和文本识别模型提供了更多的灵活性。首先，这两个模型可以分别训练。在仅有一个小数据集用于训练整个OCR模块，但有大量文本识别数据容易获取的情况下，利用这大量数据进行识别模型的训练是有意义的。此外，通过两个独立的模型，可以计算两个独立的指标集，从而更全面地了解瓶颈可能在哪里。
- en: Hence, both two-model and end-to-end approaches are viable. Whether one is better
    than the other mainly depends on the data available and what one wants to achieve.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，双模型和端到端方法都是可行的。哪个方法更好主要取决于可用的数据和想要达到的目标。
- en: 3.5 Datasets for Text Detection & Transcription
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 文本检测和转录的数据集
- en: Most of the literature revolves around scene text detection, rather than document
    text detection, and report results on those datasets. Some of the major ones are
    ICDAR (Karatzas et al., [2013](#bib.bib47), [2015](#bib.bib46)), Total-Text (Ch’ng
    and Chan, [2017](#bib.bib12)), CTW1500 (Yuliang et al., [2017](#bib.bib103)),
    and SynthText (Gupta et al., [2016](#bib.bib30)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分文献围绕场景文本检测展开，而不是文档文本检测，并在这些数据集上报告结果。一些主要的数据集包括ICDAR（Karatzas et al., [2013](#bib.bib47),
    [2015](#bib.bib46)），Total-Text（Ch’ng and Chan, [2017](#bib.bib12)），CTW1500（Yuliang
    et al., [2017](#bib.bib103)），以及SynthText（Gupta et al., [2016](#bib.bib30)）。
- en: Jaume et al. ([2019](#bib.bib43)) present FUNSD, a dataset for text detection,
    transcription, and document understanding with 199 fully annotated forms comprising
    of 31k word level bounding boxes. Another recent document understanding dataset
    comes from the ICDAR 2019 Robust Reading Challenge on Scanned Receipts OCR and
    Information Extraction (SROIE). It contains 1000 whole scanned receipt images,
    with line-level annotations for text detection/transcription, and labels for Key
    Information Extraction. The website contains a ranking of the solutions proposed
    to address this problem. As solutions are still posted after the end of the competition,
    it is a good way to keep track of the most recent methods.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Jaume 等（[2019](#bib.bib43)）提出了FUNSD，一个用于文本检测、转录和文档理解的数据集，包含199个完全标注的表单，共计31k词级边界框。另一个近期的文档理解数据集来自ICDAR
    2019年扫描收据OCR和信息提取（SROIE）的鲁棒阅读挑战。它包含1000个完整的扫描收据图像，具有文本检测/转录的行级注释和关键信息提取的标签。该网站包含了针对这个问题提出的解决方案的排名。由于比赛结束后仍会有解决方案发布，这是一种跟踪最新方法的好方法。
- en: 4 Document Layout Analysis
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 文档布局分析
- en: Document layout analysis is the process of locating and categorizing regions
    of interest on a picture or scanned image of a page. Broadly, most approaches
    can be distilled into page segmentation and logical structural analysis (Binmakhashen
    and Mahmoud, [2019](#bib.bib8); Okun et al., [1999](#bib.bib74)). Page segmentation
    methods focus on appearance and use visual cues to partition pages into distinct
    regions; the most common are text, figures, images, and tables. In contrast, logical
    structural analysis focuses on providing finer-grained semantic classifications
    for these regions, i.e. identifying a region of text that is a paragraph and distinguishing
    that from a caption or document title.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 文档布局分析是定位和分类图片或扫描图像中感兴趣区域的过程。大体上，大多数方法可以提炼为页面分割和逻辑结构分析（Binmakhashen 和 Mahmoud,
    [2019](#bib.bib8); Okun 等, [1999](#bib.bib74)）。页面分割方法关注于外观，利用视觉线索将页面划分为不同区域；最常见的有文本、图形、图像和表格。相反，逻辑结构分析专注于对这些区域提供更细粒度的语义分类，即识别一个文本区域是否为段落，并将其与标题或文档标题区分开来。
- en: 'Research in methods for document layout analysis has a long history, both in
    academia and industry. ¹¹1The first ISO standard that defined aspects of modern-day
    document layout analysis was drafted four decades ago: ISO 8613-1:1989 From the
    first pioneering heuristic approaches (Lebourgeois et al., [1992](#bib.bib53);
    Okun et al., [1999](#bib.bib74); Liang et al., [1997](#bib.bib57)), to multi-stage
    classical machine learning systems (Qin et al., [2018](#bib.bib78); Wei et al.,
    [2013](#bib.bib95); Eskenazi et al., [2017](#bib.bib23)), the evolution of document
    layout analysis methods is now dominated by end-to-end differentiable methods (Yang
    et al., [2017](#bib.bib99); Binmakhashen and Mahmoud, [2019](#bib.bib8); Ares Oliveira
    et al., [2018](#bib.bib3); Agarwal et al., [2020](#bib.bib1); Monnier, [2020](#bib.bib72);
    Pramanik et al., [2020](#bib.bib76)).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 文档布局分析方法的研究有着悠久的历史，无论是在学术界还是工业界。¹¹1第一部定义现代文档布局分析方面的ISO标准起草于四十年前：ISO 8613-1:1989。从最早的启发式方法（Lebourgeois
    等, [1992](#bib.bib53); Okun 等, [1999](#bib.bib74); Liang 等, [1997](#bib.bib57)），到多阶段经典机器学习系统（Qin
    等, [2018](#bib.bib78); Wei 等, [2013](#bib.bib95); Eskenazi 等, [2017](#bib.bib23)），文档布局分析方法的演变现在主要由端到端的可微方法主导（Yang
    等, [2017](#bib.bib99); Binmakhashen 和 Mahmoud, [2019](#bib.bib8); Ares Oliveira
    等, [2018](#bib.bib3); Agarwal 等, [2020](#bib.bib1); Monnier, [2020](#bib.bib72);
    Pramanik 等, [2020](#bib.bib76)）。
- en: 4.1 Instance Segmentation for Layout Analysis
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 布局分析的实例分割
- en: When applied to the problem of layout analysis in business documents, instance
    segmentation methods predict per-pixel labels to categorize regions of interest.
    Such methods are flexible and easily adapt to the courser-grained task of page
    segmentation or the more-specific task of logical structural analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于商业文档的布局分析问题时，实例分割方法预测每个像素的标签以分类感兴趣区域。这些方法灵活且易于适应粗粒度的页面分割任务或更具体的逻辑结构分析任务。
- en: '![Refer to caption](img/04bc533e61a87e879db37f655aac1009.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04bc533e61a87e879db37f655aac1009.png)'
- en: 'Figure 2: A document is passed through a generic layout analysis model, resulting
    in a layout segmentation mask with the following classes: figure (green), figure
    caption (orange), heading (purple), paragraph (red), and algorithm (blue). The
    document has been reproduced with permission (Subramani, [2016](#bib.bib86)).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：文档通过一个通用的布局分析模型处理，结果生成一个布局分割掩模，包含以下类别：图像（绿色）、图像说明（橙色）、标题（紫色）、段落（红色）和算法（蓝色）。该文档已获得许可转载（Subramani，[2016](#bib.bib86)）。
- en: In Yang et al. ([2017](#bib.bib99)), the authors describe an end-to-end neural
    network that combines both text and visual features in a encoder-decoder architecture
    that also incorporates an unsupervised pretraining network. During inference,
    their approach uses a downsampling cascade of pooling layers to encode visual
    information, which is fed into a symmetrical upsampling cascade for decoding.
    At each cascade level, the produced encoding is also directly passed into the
    respective decoding block, concatenating the down- and up-sampled representations.
    This architecture ensures that visual feature information at different levels
    of resolution is considered during the encoding and decoding process (Burt and
    Adelson, [1983](#bib.bib9)). For the final decoding layer, localized text embeddings
    are supplied alongside the computed visual representation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Yang 等人（[2017](#bib.bib99)）的研究中，作者描述了一种端到端的神经网络，该网络结合了文本和视觉特征，采用编码器-解码器架构，并融合了无监督预训练网络。在推断过程中，他们的方法使用下采样级联的池化层来编码视觉信息，然后输入到对称的上采样级联中进行解码。在每个级联层次中，生成的编码也直接传递到相应的解码块，连接下采样和上采样的表示。这种架构确保在编码和解码过程中考虑不同分辨率层次的视觉特征信息（Burt
    和 Adelson，[1983](#bib.bib9)）。对于最终的解码层，局部文本嵌入与计算出的视觉表示一起提供。
- en: This U-Net inspired encoding-decoding architecture has been adopted for document
    layout analysis in several different approaches (Ronneberger et al., [2015](#bib.bib82)).
    The method in Ares Oliveira et al. ([2018](#bib.bib3)), and later extended by
    Barman et al. ([2020](#bib.bib6)) via additional text embeddings, use convolution
    maxpooling layers with large filter sizes to feed the document image through a
    ResNet bottleneck (He et al., [2015](#bib.bib35)). The representation is then
    processed by bilinear upsampling layers and smaller 1x1 and 3x3 convolution layers.
    Both works are used to perform layout analysis on historical documents and newspapers
    from multiple European languages, respectively. In Lee et al. ([2019](#bib.bib54)),
    the authors combine the U-Net architecture pattern with trainable multiplication
    layers. This layer type is specialized for extracting co-occurrence texture features
    from the network’s convolution feature maps, which are effective for locating
    regions that have periodically repeating information, such as tables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种受 U-Net 启发的编码-解码架构已被采用于几种不同的文档布局分析方法（Ronneberger 等人，[2015](#bib.bib82)）。Ares
    Oliveira 等人（[2018](#bib.bib3)）的方法，随后由 Barman 等人（[2020](#bib.bib6)）通过额外的文本嵌入进行扩展，使用大滤波器尺寸的卷积最大池化层将文档图像传递通过
    ResNet 瓶颈（He 等人，[2015](#bib.bib35)）。然后，表示通过双线性上采样层和较小的 1x1 和 3x3 卷积层进行处理。这两项工作分别用于对来自多种欧洲语言的历史文档和报纸进行布局分析。在
    Lee 等人（[2019](#bib.bib54)）的研究中，作者将 U-Net 架构模式与可训练的乘法层结合。这种层类型专门用于从网络的卷积特征图中提取共现纹理特征，这些特征对于定位周期性重复信息的区域（如表格）非常有效。
- en: 4.2 Addressing Data Scarcity and Alternative Approaches
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 解决数据稀缺和替代方法
- en: Obtaining high quality training data for layout analysis is a labor intensive
    task that requires both mechanical precision and an understanding of the document’s
    contents. As a consequence of the difficulties in layout annotation of documents
    from brand new domains, several approaches exist to either leverage structure
    in unlabeled data or use well-defined rule sets to generate synthetic labeled
    documents to further improve generalizability and performance of document layout
    analysis systems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 获取高质量的布局分析训练数据是一项劳动密集型任务，需要机械精度和对文档内容的理解。由于对全新领域文档进行布局注释的困难，存在几种方法可以利用未标记数据中的结构或使用明确的规则集生成合成标记文档，从而进一步提高文档布局分析系统的普适性和性能。
- en: Masked language models such as BERT and RoBERTa have shown effective empirical
    performance on many downstream NLP tasks (Devlin et al., [2019](#bib.bib20); Liu
    et al., [2019b](#bib.bib66)). Inspired by the pretraining strategy in BERT and
    RoBERTa, Xu et al. ([2020](#bib.bib98)) define a Masked Visual-Language Model,
    which randomly masks input tokens and uses the model to predict the masked tokens.
    Unlike BERT, their method provides the 2-D positional embedding of the token during
    this masked prediction task, which enables the model to combine both semantic
    and spatial relationships between textual elements. Mentioned earlier in section [4.1](#S4.SS1
    "4.1 Instance Segmentation for Layout Analysis ‣ 4 Document Layout Analysis ‣
    A Survey of Deep Learning Approaches for OCR and Document Understanding"), Yang
    et al. ([2017](#bib.bib99)) introduce an auxiliary document image reconstruction
    task in their broader instance segmentation-based network. During training, this
    auxiliary module uses a separate upsampling decoder that, without the aid of skip
    connections, predicts the original pixel values from the encoded representation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 像 BERT 和 RoBERTa 这样的掩码语言模型在许多下游 NLP 任务中展示了有效的实证性能（Devlin et al., [2019](#bib.bib20);
    Liu et al., [2019b](#bib.bib66)）。受到 BERT 和 RoBERTa 中预训练策略的启发，Xu 等（[2020](#bib.bib98)）定义了一种掩码视觉语言模型，该模型随机掩盖输入标记并使用模型预测被掩盖的标记。与
    BERT 不同，他们的方法在这一掩码预测任务中提供了标记的 2-D 位置嵌入，这使得模型能够结合文本元素之间的语义和空间关系。如第 [4.1](#S4.SS1
    "4.1 Instance Segmentation for Layout Analysis ‣ 4 Document Layout Analysis ‣
    A Survey of Deep Learning Approaches for OCR and Document Understanding") 节所提到的，Yang
    等（[2017](#bib.bib99)）在他们更广泛的基于实例分割的网络中引入了一种辅助文档图像重建任务。在训练过程中，这个辅助模块使用一个独立的上采样解码器，在没有跳跃连接的帮助下，从编码表示中预测原始像素值。
- en: While pretraining lets practitioners gain more value from their unlabeled documents,
    this technique alone is not always sufficient to effectively surmount data scarcity
    concerns. Relying on the intuition that many business and academic documents have
    repeated patterns in both content as well as page-level organization, several
    approaches have emerged to manufacture synthetic, labeled data in order to provide
    data suitable for a pretraining-like routine (Zhong et al., [2019b](#bib.bib106)).
    In Monnier ([2020](#bib.bib72)), the authors propose a three-stage method for
    synthesizing new labeled documents. First, they generate the document by randomly
    choosing the a document background from a set of nearly 200 known document backgrounds.
    Second, they use a grid based layout method to define both individual document
    element content and their respective sizes. Third, their process introduces corruptions,
    such as Gaussian blur and random image crops. This modular, rule-based synthetic
    document generation approach creates a heterogeneous dataset to make pretraining
    of layout analysis models more robust.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预训练使从业者能够从未标记文档中获得更多价值，但仅靠这种技术往往不足以有效解决数据稀缺的问题。基于许多商业和学术文档在内容和页面级组织中都有重复模式的直觉，已经出现了几种方法来生成合成标记数据，以提供适合预训练类似流程的数据（Zhong
    et al., [2019b](#bib.bib106)）。在 Monnier（[2020](#bib.bib72)）中，作者提出了一种三阶段生成新标记文档的方法。首先，他们从近200个已知文档背景中随机选择一个文档背景来生成文档。其次，他们使用基于网格的布局方法来定义每个文档元素的内容及其各自的大小。第三，他们的过程引入了诸如高斯模糊和随机图像裁剪等破坏。这个模块化、基于规则的合成文档生成方法创建了一个异质数据集，使得布局分析模型的预训练更加稳健。
- en: Alternatively, instead of defining rules to generate a heterogeneous set of
    documents, several synthesizing procedures take cues from data augmentation methods.
    Capobianco and Marinai ([2017](#bib.bib10)) and Journet et al. ([2017](#bib.bib45))
    describe general-purpose toolkits that use an existing set of labeled documents
    to introduce deformations and perturbations in source images. Importantly, such
    changes to the training data are balanced so as to preserve the original semantic
    content while still exposing the model training to realistic errors that it must
    account for during inference on unseen data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为替代，除了定义规则生成异质文档集之外，一些合成过程借鉴了数据增强方法。Capobianco 和 Marinai（[2017](#bib.bib10)）以及
    Journet 等（[2017](#bib.bib45)）描述了使用现有标记文档集引入变形和扰动的通用工具包。重要的是，这些对训练数据的更改经过平衡，以保持原始语义内容，同时让模型训练暴露于必须在对未见数据进行推理时考虑的现实错误中。
- en: 4.3 Datasets for Layout Analysis
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 布局分析的数据集
- en: Recently, there has been a deluge of datasets specifically targeting the document
    layout analysis problem. The International Conference on Document Analysis and
    Recognition (ICDAR) has produced several datasets from their various annual competitions;
    the most recent from 2017 and 2019 provide gold-standard data for document layout
    analysis and other document processing tasks (Gao et al., [2017](#bib.bib26);
    Clausner et al., [2019](#bib.bib16); Huang et al., [2019](#bib.bib39); Gao et al.,
    [2019](#bib.bib25)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，针对文档布局分析问题的数据集激增。国际文档分析与识别会议（ICDAR）通过他们的各种年度比赛产生了多个数据集；最近的2017年和2019年数据集提供了文档布局分析及其他文档处理任务的金标准数据（Gao
    et al., [2017](#bib.bib26); Clausner et al., [2019](#bib.bib16); Huang et al.,
    [2019](#bib.bib39); Gao et al., [2019](#bib.bib25)）。
- en: On the larger side, DocBank is a a collection of half a million document pages
    with token-level annotations suitable for training and evaluating document layout
    analysis systems (Li et al., [2020](#bib.bib56)). The authors constructed this
    dataset using weak supervision (Hoffmann et al., [2011](#bib.bib38)), matching
    data from the LaTeX source of known PDFs to form annotations. Similarly, Zhong
    et al. ([2019b](#bib.bib106)) created PubLayNet by automatically matching XML
    content representations for over one million PDFs on PubMed Central™, consiting
    of approximately 360 thousand document images. While not full document layout,
    Zhong et al. ([2019a](#bib.bib105)) have created PubTabNet from PubMed Central
    as well. Their data consists of 568 thousand table images alongside an HTML representations
    of content.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从更大的角度来看，DocBank是一个包含50万页文档的集合，具有适合训练和评估文档布局分析系统的标记级注释（Li et al., [2020](#bib.bib56)）。作者使用弱监督（Hoffmann
    et al., [2011](#bib.bib38)）构建了这个数据集，将已知PDF的LaTeX源数据匹配形成注释。类似地，Zhong et al. ([2019b](#bib.bib106))通过自动匹配PubMed
    Central™上的一百万多个PDF的XML内容表示创建了PubLayNet，包含约36万张文档图像。虽然不是完整的文档布局，Zhong et al. ([2019a](#bib.bib105))也从PubMed
    Central创建了PubTabNet。他们的数据包括56.8万张表格图像和内容的HTML表示。
- en: 5 Information Extraction
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 信息提取
- en: The goal of information extraction for document understanding is to take documents
    that may have diverse layouts and extract information into a structured format.
    Examples include receipt understanding to identify item names, quantities, and
    prices and form understanding to identify different key-value pairs. Document
    extraction of information by humans goes beyond simply reading text on a page
    as it is often necessary to learn page layouts for complete understanding. As
    such, recent enhancements have extended text encoding strategies for documents
    by additionally encoding structural and visual information of text in a variety
    of ways.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文档理解的信息提取的目标是将可能具有多样化布局的文档提取成结构化格式。示例包括收据理解以识别项目名称、数量和价格，以及表单理解以识别不同的键值对。人类对信息的提取超越了简单地阅读页面上的文本，因为通常需要学习页面布局以获得完整的理解。因此，近期的改进通过以多种方式额外编码文本的结构和视觉信息，扩展了文档的文本编码策略。
- en: 5.1 2D Positional Embeddings
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 2D位置嵌入
- en: Multiple sequence tagging approaches have been proposed which augment current
    named entity recognition (NER) methods by embedding attributes of 2D bounding
    boxes and merging them with text embeddings to create models which are simultaneously
    aware of both context and spatial positioning when extracting information. Xu
    et al. ([2020](#bib.bib98)) embeds the pair of $x,y$ coordinates that define a
    bounding box using two different embedding tables and pretrain a masked language
    model (LM). During pretraining, text is randomly masked but the 2D positional
    embeddings are retained. This model can then be fine-tuned on a downstream task.
    Alternatively, the bounding box coordinates can also be embedded using $sin$ and
    $cos$ functions like positional encoding methods (Vaswani et al., [2017](#bib.bib92);
    Hwang et al., [2020](#bib.bib41)). Other features can also be embedded such as
    the line or sequence number (Hwang et al., [2019](#bib.bib40)). In this scenario,
    the document is preprocessed to assign a line number to each individual token.
    Each token is then ordered from left to right and given a sequential position.
    Finally, both the line and sequential positions are embedded.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出多种序列标记方法，这些方法通过嵌入 2D 边界框的属性并将其与文本嵌入合并，增强了当前的命名实体识别（NER）方法，以创建同时关注上下文和空间位置的模型，从而在提取信息时能够感知这两者。Xu
    et al. ([2020](#bib.bib98)) 使用两个不同的嵌入表嵌入定义边界框的 $x,y$ 坐标，并预训练一个掩码语言模型（LM）。在预训练过程中，文本被随机掩码，但
    2D 位置嵌入被保留。然后，该模型可以在下游任务上进行微调。或者，边界框坐标也可以使用 $sin$ 和 $cos$ 函数进行嵌入，如位置编码方法（Vaswani
    et al., [2017](#bib.bib92); Hwang et al., [2020](#bib.bib41)）。还可以嵌入其他特征，如行号或序列号（Hwang
    et al., [2019](#bib.bib40)）。在这种情况下，文档会被预处理以为每个单独的标记分配行号。每个标记从左到右排序，并赋予一个序列位置。最后，同时嵌入行位置和序列位置。
- en: While these strategies have seen success, relying solely on the line number
    or bounding box coordinates can be misleading when the document has been scanned
    on an uneven surface, leading to curved text. Additionally, bounding box based
    embeddings still miss critical visual information such as typographical emphases
    (bold, italics) and images such as logos. To overcome these, a crop of the image
    corresponding to the token of interest can be embedded using a Faster R-CNN model
    to create token image embeddings which are combined with the 2D positional embeddings (Xu
    et al., [2020](#bib.bib98)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些策略取得了成功，但仅依赖行号或边界框坐标在文档在不平整的表面上扫描时可能会导致误导，从而导致文本弯曲。此外，基于边界框的嵌入仍然缺少关键的视觉信息，如排版强调（粗体、斜体）和图像（如徽标）。为了克服这些问题，可以使用
    Faster R-CNN 模型嵌入与感兴趣标记对应的图像裁剪，从而创建标记图像嵌入，并与 2D 位置嵌入结合（Xu et al., [2020](#bib.bib98)）。
- en: 5.2 Image Embeddings
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 图像嵌入
- en: 'Information extraction for documents can also be framed as a computer vision
    challenge wherein the goal of the model is to semantically segment information
    or regress bounding boxes over the areas of interest. This strategy helps preserve
    the 2D layout of the document and allows models to take advantage of 2D correlations.
    While it is theoretically possible to learn strictly from the document image,
    directly embedding textual information into the image simplifies the task for
    models to understand the 2D textual relationships. In these cases, an encoding
    function is applied onto a proposed textual level (i.e. character, token, word)
    to create individual embedding vectors. These vectors are transposed into each
    pixel that comprises the bounding box corresponding to the embedded text, ultimately
    creating an image of $W\times H\times D$ where $W$ is the width, $H$ is the height,
    and $D$ is the embedding dimension. Proposed variants are listed as following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的信息提取也可以被框架化为计算机视觉挑战，其中模型的目标是语义分割信息或对感兴趣区域进行边界框回归。这一策略有助于保留文档的 2D 布局，并允许模型利用
    2D 相关性。虽然从理论上讲，可以严格从文档图像中学习，但直接将文本信息嵌入图像可以简化模型理解 2D 文本关系的任务。在这些情况下，将对建议的文本级别（即字符、标记、词）应用编码函数，以创建单个嵌入向量。这些向量被转置到组成边界框的每个像素中，最终创建一个
    $W\times H\times D$ 的图像，其中 $W$ 是宽度，$H$ 是高度，$D$ 是嵌入维度。提出的变体列举如下：
- en: '1.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: CharGrid embeds characters with a one-hot encoding into the image (Katti et al.,
    [2018](#bib.bib49))
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CharGrid 使用一热编码将字符嵌入图像中（Katti et al., [2018](#bib.bib49)）。
- en: '2.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: WordGrid embeds individual words using word2vec or FastText (Kerroumi et al.,
    [2020](#bib.bib50))
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WordGrid 使用 word2vec 或 FastText 嵌入单个词汇（Kerroumi et al., [2020](#bib.bib50)）。
- en: '3.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: BERTgrid finetunes BERT on task-specific documents and is used obtain contextual
    wordpiece vectors (Denk and Reisswig, [2019](#bib.bib19))
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BERTgrid 在任务特定文档上对 BERT 进行微调，用于获得上下文相关的词片向量 （Denk 和 Reisswig，[2019](#bib.bib19)）。
- en: '4.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4。
- en: C+BERTgrid, combines context-specific and character vectors (Denk and Reisswig,
    [2019](#bib.bib19))
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C+BERTgrid，结合了上下文特定和字符向量 （Denk 和 Reisswig，[2019](#bib.bib19)）。
- en: When comparing the grid methods, C+BERTgrid has shown the best performance,
    likely due to its contextualized word vectors combined with a degree of resiliency
    to OCR errors. Zhao et al. ([2019](#bib.bib104)) proposes an alternative approach
    to directly apply text embeddings to the image. A grid is projected on top of
    the image and a mapping function assigns each token to a unique cell in the grid.
    Models then learn to assign each cell in the grid to a class. This method significantly
    reduces the dimensionality due to its grid system, while still retaining the majority
    of the 2D spatial relationships.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较网格方法时，C+BERTgrid 展现了最佳性能，这可能归因于其上下文化的词向量结合了对 OCR 错误的一定抵抗力。赵等人（[2019](#bib.bib104)）提出了一种替代方法，直接将文本嵌入应用到图像上。网格被投影到图像上，并且映射函数将每个标记分配给网格中的唯一单元格。模型然后学习将每个单元格分配给一个类别。由于其网格系统，该方法显著减少了维度，同时仍保留了大部分
    2D 空间关系。
- en: 5.3 Documents as Graphs
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 文档作为图形
- en: Unstructured text on documents can also be represented as graph networks, where
    the nodes in a graph represent different textual segments. Two nodes are connected
    with an edge if they are cardinally adjacent to each other, allowing the relationship
    between words to be modeled directly (Qian et al., [2019](#bib.bib77)). An encoder
    such as a BiLSTM encodes text segments into nodes (Qian et al., [2019](#bib.bib77)).
    Edges can be represented as a binary adjacency matrix or a richer matrix, encoding
    additional visual information such as the distance between segments or shape of
    the source and target nodes (Liu et al., [2019a](#bib.bib64)). A graph convolutional
    network is then applied at different receptive fields in a similar fashion to
    dilated convolutions (Yu and Koltun, [2015](#bib.bib102)) to ensure that both
    local and global information can be learned (Qian et al., [2019](#bib.bib77)).
    After this, the representation is passed to a sequence tagging decoder.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的非结构化文本也可以表示为图网络，其中图中的节点代表不同的文本片段。如果两个节点在空间上相邻，则它们之间有一条边，这允许直接建模词之间的关系 （Qian
    等，[2019](#bib.bib77)）。如 BiLSTM 这样的编码器将文本片段编码为节点 （Qian 等，[2019](#bib.bib77)）。边缘可以表示为二进制邻接矩阵或更丰富的矩阵，编码额外的视觉信息，如片段之间的距离或源节点和目标节点的形状 （Liu
    等，[2019a](#bib.bib64)）。然后，图卷积网络以类似于扩张卷积的方式在不同的感受野上应用 （Yu 和 Koltun，[2015](#bib.bib102)），以确保可以学习到局部和全局信息 （Qian
    等，[2019](#bib.bib77)）。之后，将表示传递给序列标记解码器。
- en: 'Documents can also be represented as a directed graph and a spatial dependency
    parser (Hwang et al., [2020](#bib.bib41)). In this representation, nodes are represented
    by textual segments, but field nodes denoting the node type are used to initialize
    each DAG. In addition, two kinds of edges are defined:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 文档也可以表示为有向图和空间依赖解析器 （Hwang 等，[2020](#bib.bib41)）。在这种表示中，节点由文本片段表示，但用于初始化每个 DAG
    的字段节点表示节点类型。此外，定义了两种边：
- en: '1.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1。
- en: Edges that group together segments belonging to the same category (STORENAME
    $\rightarrow$ Peet’s $\rightarrow$ Coffee; a field node followed by two nodes
    representing a store name)
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将属于同一类别的片段分组在一起的边（STORENAME $\rightarrow$ Peet’s $\rightarrow$ Coffee；一个字段节点后跟两个表示商店名称的节点）
- en: '2.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2。
- en: Edges that connect relationships between different groups (Peet’s $\rightarrow$
    94107; a zipcode).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接不同组之间关系的边（Peet’s $\rightarrow$ 94107；一个邮政编码）。
- en: A transformer with an additional 2D positional embedding is used to spatially
    encode the text. After this, the task becomes to predict the relationship matrix
    for each edge type. This method can represent arbitrarily deep hierarchies and
    can be applied towards complicated document layouts.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有额外 2D 位置信息嵌入的变换器对文本进行空间编码。之后的任务是预测每种边缘类型的关系矩阵。这种方法能够表示任意深度的层级结构，并且可以应用于复杂的文档布局。
- en: 5.4 Tables
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 表格
- en: Tabular data extraction remains a challenging aspect of information extraction
    due to their wide variety of formats and complex hierarchies. Table datasets typically
    have multiple tasks to perform (Shahab et al., [2010](#bib.bib84); Göbel et al.,
    [2013](#bib.bib31); Gao et al., [2019](#bib.bib25); Zhong et al., [2019a](#bib.bib105)).
    The first task is table detection which involves localizing the bounding box containing
    the table(s) inside the document. The next task is table structure recognition,
    which requires extracting the row, column, and cell information into a common
    format. This can be taken one step further to table recognition, which requires
    understanding both the structural information as well as the content by classifying
    cells within the table itself (Zhong et al., [2019a](#bib.bib105)). As textual
    and visual features are equally important to properly extracting and understanding
    tables, many diverse methods have been proposed to perform this task.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据提取仍然是信息提取中的一个挑战性方面，因其格式多样且层次复杂。表格数据集通常需要执行多个任务（Shahab 等，[2010](#bib.bib84)；Göbel
    等，[2013](#bib.bib31)；Gao 等，[2019](#bib.bib25)；Zhong 等，[2019a](#bib.bib105)）。第一个任务是表格检测，即在文档中定位包含表格的边界框。接下来的任务是表格结构识别，这需要将行、列和单元格信息提取为一种通用格式。这一步可以进一步发展为表格识别，要求理解结构信息以及通过分类表格中的单元格来理解内容（Zhong
    等，[2019a](#bib.bib105)）。由于文本特征和视觉特征对正确提取和理解表格同等重要，已经提出了许多不同的方法来完成这一任务。
- en: 'One such proposal named TableSense performs both table detection and structure
    recognition (Dong et al., [2019a](#bib.bib21)). TableSense uses a three stage
    approach: cell featurization, object detection with convolutional models, and
    an uncertainty-based active learning sampling mechanism. TableSense’s proposed
    architecture for table detection performs significantly better than traditional
    methods in computer vision such as YOLO-v3 or Mask R-CNN (Redmon and Farhadi,
    [2018](#bib.bib80); He et al., [2017b](#bib.bib34)). Since this approach does
    not work well for general spreadsheets,  Dong et al. ([2019b](#bib.bib22)) extend
    upon the previous work by using a multitask framework to jointly learn table regions,
    structural components of spreadsheets, and cell types. They add an additional
    stage, which leverages language models to learn the semantic contents of table
    cells in order to flatten complex tables into a single standard format.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为 TableSense 的提议同时执行表格检测和结构识别（Dong 等，[2019a](#bib.bib21)）。TableSense 使用三阶段方法：单元格特征化、使用卷积模型的对象检测，以及基于不确定性的主动学习采样机制。TableSense
    提出的表格检测架构在计算机视觉领域中的传统方法如 YOLO-v3 或 Mask R-CNN（Redmon 和 Farhadi，[2018](#bib.bib80)；He
    等，[2017b](#bib.bib34)）中表现显著更好。由于这种方法在通用电子表格中的效果不佳，Dong 等（[2019b](#bib.bib22)）通过使用多任务框架来扩展先前的工作，以共同学习表格区域、电子表格的结构组件和单元格类型。他们增加了一个额外的阶段，利用语言模型来学习表格单元格的语义内容，以便将复杂表格扁平化为单一标准格式。
- en: Wang et al. ([2020](#bib.bib94)) propose TUTA, which focuses on understanding
    the content within tables after the structure has been determined. The authors
    present three new objectives for language model pretraining for table understanding
    by using tree-based transformers. The objectives introduced for pretraining are
    designed to help the model understand tables at the token, cell, and table level.
    The authors mask a proportion of tokens depending on the table cell for the model
    to predict, randomly mask particular cell headers for the model to predict the
    header string based on its location, and provide the table with context such as
    table titles or descriptions that may or may not be associated for the model to
    identify which contextual elements are positively associated with the table. The
    transformer architecture is modified to reduce distractions from attention by
    limiting the attention connections to items based on a cell’s hierarchical distance
    to another cell. Fine-tuning TUTA has demonstrated state of the art performance
    on multiple datasets for cell type classification.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Wang等（[2020](#bib.bib94)）提出了TUTA，重点是在结构确定后理解表格中的内容。作者通过使用基于树的变压器，提出了用于表格理解的语言模型预训练的三个新目标。为预训练引入的目标旨在帮助模型在标记、单元格和表格级别理解表格。作者根据表格单元格的不同遮盖一部分标记，让模型预测，随机遮盖特定的单元格标题，让模型根据其位置预测标题字符串，并提供表格上下文，如表格标题或描述，以便模型识别与表格正相关的上下文元素。变压器架构被修改，以通过将注意力连接限制为基于单元格的层次距离来减少来自注意力的干扰。TUTA的微调在多个数据集中显示了最先进的细胞类型分类性能。
- en: 6 Conclusion
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Document understanding is a hot topic in industry and has immense monetary value.
    Most documents are private data corresponding to private contracts, invoices,
    and records. As a result, openly available datasets are hard to come by and have
    not been a focus for academia with respect to other application areas. The academic
    literature on methodologies to tackle document understanding is similarly sparse
    relative to areas with an abundance of publicly available data such as image classification
    and translation. However, the most effective approaches for document understanding
    make use of recent advancements in deep neural network modeling. End-to-end document
    understanding is achievable by creating an integrated system that performs layout
    analysis, optical character recognition, and domain-specific information extraction.
    In this survey, we attempt to consolidate and organize the methodologies which
    are present in literature in order to be a jumping-off point for scholars and
    practitioners alike who want to explore document understanding.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 文档理解在工业界是一个热门话题，并具有巨大的经济价值。大多数文档是与私人合同、发票和记录相关的私人数据。因此，公开可用的数据集很难获得，并且在学术界相对于其他应用领域并未成为重点。相对于图像分类和翻译等数据丰富的领域，处理文档理解的方法学的学术文献也同样稀少。然而，最有效的文档理解方法利用了深度神经网络建模的最新进展。通过创建一个执行布局分析、光学字符识别和领域特定信息提取的集成系统，可以实现端到端的文档理解。在这项综述中，我们尝试整合和组织现有文献中的方法，以便为那些希望探索文档理解的学者和从业者提供一个起点。
- en: References
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2020) M. Agarwal, Ajoy Mondal, and C. Jawahar. 2020. Cdec-net:
    Composite deformable cascade network for table detection in document images. *ArXiv*.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等（2020）M. Agarwal、Ajoy Mondal和C. Jawahar。2020年。Cdec-net：用于文档图像中的表格检测的复合变形级联网络。*ArXiv*。
- en: Amin and Shiu (2001) A. Amin and R. Shiu. 2001. Page segmentation and classification
    utilizing bottom-up approach. *Int. J. Image Graph.*
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin和Shiu（2001）A. Amin和R. Shiu。2001年。利用自下而上的方法进行页面分割和分类。*Int. J. Image Graph.*
- en: 'Ares Oliveira et al. (2018) Sofia Ares Oliveira, Benoit Seguin, and Frederic
    Kaplan. 2018. dhsegment: A generic deep-learning approach for document segmentation.
    *ICFHR*.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ares Oliveira等（2018）Sofia Ares Oliveira、Benoit Seguin和Frederic Kaplan。2018年。dhsegment：一种通用深度学习文档分割方法。*ICFHR*。
- en: Baek et al. (2019) Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk
    Lee. 2019. Character region awareness for text detection. In *CVPR*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baek等（2019）Youngmin Baek、Bado Lee、Dongyoon Han、Sangdoo Yun和Hwalsuk Lee。2019年。用于文本检测的字符区域感知。发表于*CVPR*。
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. In *ICLR*.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau等（2014）Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio。2014年。通过联合学习对齐和翻译的神经机器翻译。发表于*ICLR*。
- en: Barman et al. (2020) Raphaël Barman, Maud Ehrmann, Simon Clematide, Sofia Ares
    Oliveira, and Frédéric Kaplan. 2020. Combining visual and textual features for
    semantic segmentation of historical newspapers. *arXiv*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barman 等（2020）Raphaël Barman、Maud Ehrmann、Simon Clematide、Sofia Ares Oliveira
    和 Frédéric Kaplan。2020。结合视觉和文本特征进行历史报纸的语义分割。*arXiv*。
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *ArXiv*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltagy 等（2020）Iz Beltagy、Matthew E. Peters 和 Arman Cohan。2020。Longformer：长文档变换器。*ArXiv*。
- en: 'Binmakhashen and Mahmoud (2019) Galal M. Binmakhashen and Sabri A. Mahmoud.
    2019. Document layout analysis: A comprehensive survey. *ACM Comput. Surv.*'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binmakhashen 和 Mahmoud（2019）Galal M. Binmakhashen 和 Sabri A. Mahmoud。2019。文档布局分析：综合调查。*ACM
    Comput. Surv.*。
- en: Burt and Adelson (1983) P. Burt and E. Adelson. 1983. The laplacian pyramid
    as a compact image code. *IEEE Transactions on Communications*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burt 和 Adelson（1983）P. Burt 和 E. Adelson。1983。拉普拉斯金字塔作为紧凑的图像编码。*IEEE Transactions
    on Communications*。
- en: 'Capobianco and Marinai (2017) S. Capobianco and S. Marinai. 2017. Docemul:
    A toolkit to generate structured historical documents. In *ICDAR*.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Capobianco 和 Marinai（2017）S. Capobianco 和 S. Marinai。2017。Docemul：生成结构化历史文档的工具包。在
    *ICDAR*。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    2019. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等（2019）Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。2019。用稀疏变换器生成长序列。*arXiv
    preprint arXiv:1904.10509*。
- en: 'Ch’ng and Chan (2017) Chee Kheng Ch’ng and Chee Seng Chan. 2017. Total-text:
    A comprehensive dataset for scene text detection and recognition. In *2017 14th
    IAPR International Conference on Document Analysis and Recognition (ICDAR)*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ch’ng 和 Chan（2017）Chee Kheng Ch’ng 和 Chee Seng Chan。2017。Total-text：一个用于场景文本检测和识别的综合数据集。在
    *2017 14th IAPR International Conference on Document Analysis and Recognition
    (ICDAR)*。
- en: 'Cho et al. (2014a) Kyunghyun Cho, B. V. Merrienboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014a. On the properties of neural machine translation: Encoder-decoder
    approaches. In *SSST@EMNLP*.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014a）Kyunghyun Cho、B. V. Merrienboer、Dzmitry Bahdanau 和 Yoshua Bengio。2014a。神经机器翻译的属性：编码器–解码器方法。在
    *SSST@EMNLP*。
- en: Cho et al. (2014b) Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014b. Learning phrase
    representations using rnn encoder–decoder for statistical machine translation.
    In *EMNLP*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014b）Kyunghyun Cho、Bart van Merriënboer、Caglar Gulcehre、Dzmitry Bahdanau、Fethi
    Bougares、Holger Schwenk 和 Yoshua Bengio。2014b。使用 RNN 编码器–解码器学习短语表示用于统计机器翻译。在 *EMNLP*。
- en: Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David
    Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz
    Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. *arXiv
    preprint arXiv:2009.14794*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski 等（2020）Krzysztof Choromanski、Valerii Likhosherstov、David Dohan、Xingyou
    Song、Andreea Gane、Tamas Sarlos、Peter Hawkins、Jared Davis、Afroz Mohiuddin、Lukasz
    Kaiser 等。2020。重新思考带有表演者的注意力。*arXiv preprint arXiv:2009.14794*。
- en: Clausner et al. (2019) C. Clausner, A. Antonacopoulos, and S. Pletschacher.
    2019. Icdar2019 competition on recognition of documents with complex layouts -
    rdcl2019. *ICDAR*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clausner 等（2019）C. Clausner、A. Antonacopoulos 和 S. Pletschacher。2019。ICDAR2019
    复杂布局文档识别竞赛 - RDCL2019。*ICDAR*。
- en: 'Collobert and Weston (2008) Ronan Collobert and Jason Weston. 2008. A unified
    architecture for natural language processing: Deep neural networks with multitask
    learning. In *ICML*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert 和 Weston（2008）Ronan Collobert 和 Jason Weston。2008。自然语言处理的统一架构：具有多任务学习的深度神经网络。在
    *ICML*。
- en: 'Deng et al. (2018) Dan Deng, Haifeng Liu, Xuelong Li, and Deng Cai. 2018. Pixellink:
    Detecting scene text via instance segmentation. In *AAAI*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2018）Dan Deng、Haifeng Liu、Xuelong Li 和 Deng Cai。2018。Pixellink：通过实例分割检测场景文本。在
    *AAAI*。
- en: 'Denk and Reisswig (2019) Timo I. Denk and C. Reisswig. 2019. Bertgrid: Contextualized
    embedding for 2d document representation and understanding. *ArXiv*, abs/1909.04948.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denk 和 Reisswig（2019）Timo I. Denk 和 C. Reisswig。2019。Bertgrid：用于 2D 文档表示和理解的上下文化嵌入。*ArXiv*,
    abs/1909.04948。
- en: 'Devlin et al. (2019) J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    2019. Bert: Pre-training of deep bidirectional transformers for language understanding.
    In *NAACL-HLT*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2019）J. Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2019。Bert：用于语言理解的深度双向变换器预训练。在
    *NAACL-HLT*。
- en: 'Dong et al. (2019a) Haoyu Dong, S. Liu, S. Han, Z. Fu, and D. Zhang. 2019a.
    Tablesense: Spreadsheet table detection with convolutional neural networks. In
    *AAAI*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2019a）Haoyu Dong、S. Liu、S. Han、Z. Fu 和 D. Zhang。2019a。Tablesense：使用卷积神经网络检测电子表格表格。在
    *AAAI*。
- en: Dong et al. (2019b) Haoyu Dong, Shijie Liu, Zhouyu Fu, Shi Han, and Dongmei
    Zhang. 2019b. Semantic structure extraction for spreadsheet tables with a multi-task
    learning architecture. In *Workshop on Document Intelligence at NeurIPS 2019*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019b) Haoyu Dong, Shijie Liu, Zhouyu Fu, Shi Han, 和 Dongmei Zhang.
    2019b. 基于多任务学习架构的电子表格表格语义结构提取。载于 *Workshop on Document Intelligence at NeurIPS
    2019*。
- en: Eskenazi et al. (2017) Sébastien Eskenazi, Petra Gomez-Krämer, and Jean-Marc
    Ogier. 2017. A comprehensive survey of mostly textual document segmentation algorithms
    since 2008. *Pattern Recognition*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eskenazi et al. (2017) Sébastien Eskenazi, Petra Gomez-Krämer, 和 Jean-Marc Ogier.
    2017. 自2008年以来的主要文本文档分割算法的综合调查。*Pattern Recognition*。
- en: 'Feng et al. (2019) Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, and Cheng-Lin
    Liu. 2019. Textdragon: An end-to-end framework for arbitrary shaped text spotting.
    In *CVPR*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng et al. (2019) Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, 和 Cheng-Lin
    Liu. 2019. Textdragon: 一种用于任意形状文本检测的端到端框架。载于 *CVPR*。'
- en: Gao et al. (2019) L. Gao, Yilun Huang, H. Déjean, Jean-Luc Meunier, Q. Yan,
    Y. Fang, F. Kleber, and E. M. Lang. 2019. Icdar 2019 competition on table detection
    and recognition (ctdar). *ICDAR*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2019) L. Gao, Yilun Huang, H. Déjean, Jean-Luc Meunier, Q. Yan,
    Y. Fang, F. Kleber, 和 E. M. Lang. 2019. ICDAR 2019 表格检测与识别竞赛（CTDAR）。*ICDAR*。
- en: Gao et al. (2017) L. Gao, X. Yi, Z. Jiang, L. Hao, and Z. Tang. 2017. Icdar2017
    competition on page object detection. In *2017 14th IAPR International Conference
    on Document Analysis and Recognition (ICDAR)*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2017) L. Gao, X. Yi, Z. Jiang, L. Hao, 和 Z. Tang. 2017. ICDAR 2017
    页面对象检测竞赛。载于 *2017第十四届IAPR国际文档分析与识别大会（ICDAR）*。
- en: Gehring et al. (2017) Jonas Gehring, M. Auli, David Grangier, Denis Yarats,
    and Yann Dauphin. 2017. Convolutional sequence to sequence learning. In *ICML*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring et al. (2017) Jonas Gehring, M. Auli, David Grangier, Denis Yarats,
    和 Yann Dauphin. 2017. 卷积序列到序列学习。载于 *ICML*。
- en: Goodfellow et al. (2013) Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha
    Arnoud, and Vinay Shet. 2013. Multi-digit number recognition from street view
    imagery using deep convolutional neural networks. In *ICLR*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2013) Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha
    Arnoud, 和 Vinay Shet. 2013. 利用深度卷积神经网络从街景图像中识别多位数数字。载于 *ICLR*。
- en: 'Graves et al. (2006) Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen
    Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented
    sequence data with recurrent neural networks. In *ICML*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves et al. (2006) Alex Graves, Santiago Fernández, Faustino Gomez, 和 Jürgen
    Schmidhuber. 2006. 连接主义时间分类：用递归神经网络标注未分段的序列数据。载于 *ICML*。
- en: Gupta et al. (2016) Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. 2016.
    Synthetic data for text localisation in natural images. In *CVPR*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. (2016) Ankush Gupta, Andrea Vedaldi, 和 Andrew Zisserman. 2016.
    用于自然图像中文字定位的合成数据。载于 *CVPR*。
- en: Göbel et al. (2013) M. Göbel, T. Hassan, E. Oro, and G. Orsi. 2013. Icdar 2013
    table competition. In *2013 12th International Conference on Document Analysis
    and Recognition*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Göbel et al. (2013) M. Göbel, T. Hassan, E. Oro, 和 G. Orsi. 2013. ICDAR 2013
    表格竞赛。载于 *2013第十二届国际文档分析与识别大会*。
- en: Ha et al. (1995) J. Ha, R. Haralick, and I. Phillips. 1995. Document page decomposition
    by the bounding-box project. *ICDAR*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ha et al. (1995) J. Ha, R. Haralick, 和 I. Phillips. 1995. 通过边界框投影进行文档页面分解。*ICDAR*。
- en: He et al. (2017a) Dafang He, Xiao Yang, Chen Liang, Zihan Zhou, Alexander G
    Ororbi, Daniel Kifer, and C Lee Giles. 2017a. Multi-scale fcn with cascaded instance
    aware segmentation for arbitrary oriented word spotting in the wild. In *CVPR*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017a) Dafang He, Xiao Yang, Chen Liang, Zihan Zhou, Alexander G
    Ororbi, Daniel Kifer, 和 C Lee Giles. 2017a. 多尺度FCN与级联实例感知分割用于自然场景中任意方向的单词检测。载于
    *CVPR*。
- en: He et al. (2017b) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017b. Mask r-cnn. In *ICCV*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017b) Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick.
    2017b. Mask R-CNN。载于 *ICCV*。
- en: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
    Deep residual learning for image recognition. In *CVPR*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2015.
    用于图像识别的深度残差学习。载于 *CVPR*。
- en: He et al. (2018) Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, and
    Changming Sun. 2018. An end-to-end textspotter with explicit alignment and attention.
    In *CVPR*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2018) Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, 和 Changming
    Sun. 2018. 一种具有显式对齐和注意机制的端到端文本定位器。载于 *CVPR*。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆。*Neural computation*。
- en: Hoffmann et al. (2011) Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer,
    and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction
    of overlapping relations. In *ACL*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2011) Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer,
    and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction
    of overlapping relations. In *ACL*.
- en: Huang et al. (2019) Z. Huang, Ke-Han Chen, Jianhua He, X. Bai, Dimosthenis Karatzas,
    S. Lu, and C. Jawahar. 2019. Icdar2019 competition on scanned receipt ocr and
    information extraction. *ICDAR*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2019) Z. Huang, Ke-Han Chen, Jianhua He, X. Bai, Dimosthenis Karatzas,
    S. Lu, and C. Jawahar. 2019. Icdar2019 competition on scanned receipt ocr and
    information extraction. *ICDAR*.
- en: 'Hwang et al. (2019) Wonseok Hwang, Seonghyeon Kim, Minjoon Seo, Jinyeong Yim,
    Seunghyun Park, Sungrae Park, Junyeop Lee, Bado Lee, and Hwalsuk Lee. 2019. Post-{ocr}
    parsing: building simple and robust parser via {bio} tagging. In *Workshop on
    Document Intelligence at NeurIPS 2019*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hwang et al. (2019) Wonseok Hwang, Seonghyeon Kim, Minjoon Seo, Jinyeong Yim,
    Seunghyun Park, Sungrae Park, Junyeop Lee, Bado Lee, and Hwalsuk Lee. 2019. Post-{ocr}
    parsing: building simple and robust parser via {bio} tagging. In *Workshop on
    Document Intelligence at NeurIPS 2019*.'
- en: Hwang et al. (2020) Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang,
    and Minjoon Seo. 2020. Spatial dependency parsing for semi-structured document
    information extraction. *arXiv*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang et al. (2020) Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang,
    and Minjoon Seo. 2020. Spatial dependency parsing for semi-structured document
    information extraction. *arXiv*.
- en: Jaderberg et al. (2014) Max Jaderberg, K. Simonyan, A. Vedaldi, and Andrew Zisserman.
    2014. Synthetic data and artificial neural networks for natural scene text recognition.
    *ArXiv*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg et al. (2014) Max Jaderberg, K. Simonyan, A. Vedaldi, and Andrew Zisserman.
    2014. Synthetic data and artificial neural networks for natural scene text recognition.
    *ArXiv*.
- en: 'Jaume et al. (2019) Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
    Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents.
    In *ICDARW*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jaume et al. (2019) Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
    Thiran. 2019. Funsd: A dataset for form understanding in noisy scanned documents.
    In *ICDARW*.'
- en: 'Joshi et al. (2019) Mandar Joshi, Omer Levy, Daniel S. Weld, and Luke Zettlemoyer.
    2019. Bert for coreference resolution: Baselines and analysis. In *EMNLP/IJCNLP*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi et al. (2019) Mandar Joshi, Omer Levy, Daniel S. Weld, and Luke Zettlemoyer.
    2019. Bert for coreference resolution: Baselines and analysis. In *EMNLP/IJCNLP*.'
- en: 'Journet et al. (2017) Nicholas Journet, Muriel Visani, Boris Mansencal, Kieu
    Van-Cuong, and Antoine Billy. 2017. Doccreator: A new software for creating synthetic
    ground-truthed document images. *Journal of Imaging*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Journet et al. (2017) Nicholas Journet, Muriel Visani, Boris Mansencal, Kieu
    Van-Cuong, and Antoine Billy. 2017. Doccreator: A new software for creating synthetic
    ground-truthed document images. *Journal of Imaging*.'
- en: Karatzas et al. (2015) Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou,
    Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan
    Chandrasekhar, Shijian Lu, et al. 2015. Icdar 2015 competition on robust reading.
    In *ICDAR*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karatzas et al. (2015) Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou,
    Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay
    Ramaseshan Chandrasekhar, Shijian Lu, et al. 2015. Icdar 2015 competition on robust
    reading. In *ICDAR*.
- en: Karatzas et al. (2013) Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
    Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez
    Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. 2013. Icdar 2013 robust
    reading competition. In *CVPR*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karatzas et al. (2013) Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,
    Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David
    Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. 2013. Icdar
    2013 robust reading competition. In *CVPR*.
- en: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *ICML*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
    and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers
    with linear attention. In *ICML*.'
- en: 'Katti et al. (2018) A. R. Katti, C. Reisswig, Cordula Guder, Sebastian Brarda,
    S. Bickel, J. Höhne, and Jean Baptiste Faddoul. 2018. Chargrid: Towards understanding
    2d documents. In *EMNLP*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Katti et al. (2018) A. R. Katti, C. Reisswig, Cordula Guder, Sebastian Brarda,
    S. Bickel, J. Höhne, and Jean Baptiste Faddoul. 2018. Chargrid: Towards understanding
    2d documents. In *EMNLP*.'
- en: 'Kerroumi et al. (2020) Mohamed Kerroumi, Othmane Sayem, and Aymen Shabou. 2020.
    Visualwordgrid: Information extraction from scanned documents using a multimodal
    approach. *ArXiv*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kerroumi et al. (2020) Mohamed Kerroumi, Othmane Sayem, and Aymen Shabou. 2020.
    Visualwordgrid: Information extraction from scanned documents using a multimodal
    approach. *ArXiv*.'
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The efficient transformer. In *ICLR*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.
    Reformer: The efficient transformer. In *ICLR*.'
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *NeurIPS*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等人（2012）Alex Krizhevsky、Ilya Sutskever和Geoffrey E Hinton。2012年。使用深度卷积神经网络的ImageNet分类。发表于*NeurIPS*。
- en: Lebourgeois et al. (1992) F. Lebourgeois, Z. Bublinski, and H. Emptoz. 1992.
    A fast and efficient method for extracting text paragraphs and graphics from unconstrained
    documents. In *ICPR*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lebourgeois等人（1992）F. Lebourgeois、Z. Bublinski和H. Emptoz。1992年。一个快速高效的从非约束文档中提取文本段落和图形的方法。发表于*ICPR*。
- en: Lee et al. (2019) Joonho Lee, Hideaki Hayashi, Wataru Ohyama, and Seiichi Uchida.
    2019. Page segmentation using a convolutional neural network with trainable co-occurrence
    features. In *ICDAR*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等人（2019）Joonho Lee、Hideaki Hayashi、Wataru Ohyama和Seiichi Uchida。2019年。使用可训练共现特征的卷积神经网络进行页面分割。发表于*ICDAR*。
- en: Li et al. (2017) Hui Li, Peng Wang, and Chunhua Shen. 2017. Towards end-to-end
    text spotting with convolutional recurrent neural networks. In *CVPR*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2017）Hui Li、Peng Wang和Chunhua Shen。2017年。朝着端到端文本识别的卷积递归神经网络。发表于*CVPR*。
- en: 'Li et al. (2020) Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun
    Li, and Ming Zhou. 2020. Docbank: A benchmark dataset for document layout analysis.
    *arXiv*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2020）Minghao Li、Yiheng Xu、Lei Cui、Shaohan Huang、Furu Wei、Zhoujun Li和Ming
    Zhou。2020年。Docbank：一个用于文档布局分析的基准数据集。发表于*arXiv*。
- en: 'Liang et al. (1997) Jisheng Liang, Richard Rogers, Robert M Haralick, and Ihsin T
    Phillips. 1997. Uw-isl document image analysis toolbox: An experimental environment.
    In *ICDAR*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（1997）Jisheng Liang、Richard Rogers、Robert M Haralick和Ihsin T Phillips。1997年。Uw-isl文档图像分析工具箱：一个实验环境。发表于*ICDAR*。
- en: 'Liao et al. (2020) Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang
    Bai. 2020. Mask textspotter v3: Segmentation proposal network for robust scene
    text spotting. *arXiv*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等人（2020）Minghui Liao、Guan Pang、Jing Huang、Tal Hassner和Xiang Bai。2020年。Mask
    TextSpotter V3：用于鲁棒场景文本识别的分割建议网络。发表于*arXiv*。
- en: 'Liao et al. (2018a) Minghui Liao, Baoguang Shi, and Xiang Bai. 2018a. Textboxes++:
    A single-shot oriented scene text detector. *IEEE Transactions on Image Processing*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等人（2018a）Minghui Liao、Baoguang Shi和Xiang Bai。2018a年。Textboxes++：一个单次定向场景文本检测器。发表于*IEEE
    Transactions on Image Processing*。
- en: 'Liao et al. (2016) Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and
    Wenyu Liu. 2016. Textboxes: A fast text detector with a single deep neural network.
    In *AAAI*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等人（2016）Minghui Liao、Baoguang Shi、Xiang Bai、Xinggang Wang和Wenyu Liu。2016年。Textboxes：一个使用单个深度神经网络的快速文本检测器。发表于*AAAI*。
- en: Liao et al. (2018b) Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-song Xia, and
    Xiang Bai. 2018b. Rotation-sensitive regression for oriented scene text detection.
    In *CVPR*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao等人（2018b）Minghui Liao、Zhen Zhu、Baoguang Shi、Gui-song Xia和Xiang Bai。2018b年。面向旋转的回归用于定向场景文本检测。发表于*CVPR*。
- en: Lin et al. (2017) Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath
    Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection.
    In *CVPR*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人（2017）Tsung-Yi Lin、Piotr Dollár、Ross Girshick、Kaiming He、Bharath Hariharan和Serge
    Belongie。2017年。用于对象检测的特征金字塔网络。发表于*CVPR*。
- en: 'Liu et al. (2016) Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy,
    Scott Reed, Cheng-Yang Fu, and Alexander C Berg. 2016. Ssd: Single shot multibox
    detector. In *ECCV*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2016）Wei Liu、Dragomir Anguelov、Dumitru Erhan、Christian Szegedy、Scott Reed、Cheng-Yang
    Fu和Alexander C Berg。2016年。SSD：单次多框检测器。发表于*ECCV*。
- en: Liu et al. (2019a) Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha Zhao. 2019a.
    Graph convolution for multimodal information extraction from visually rich documents.
    In *NAACL*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2019a）Xiaojing Liu、Feiyu Gao、Qiong Zhang和Huasha Zhao。2019a年。图卷积用于从视觉丰富文档中提取多模态信息。发表于*NAACL*。
- en: 'Liu et al. (2018) Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and
    Junjie Yan. 2018. Fots: Fast oriented text spotting with a unified network. In
    *CVPR*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2018）Xuebo Liu、Ding Liang、Shi Yan、Dagui Chen、Yu Qiao和Junjie Yan。2018年。FOTS：使用统一网络的快速定向文本识别。发表于*CVPR*。
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    Roberta: A robustly optimized bert pretraining approach. *arXiv*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2019b）Yinhan Liu、Myle Ott、Naman Goyal、Jingfei Du、Mandar Joshi、Danqi Chen、Omer
    Levy、Mike Lewis、Luke Zettlemoyer和Veselin Stoyanov。2019b年。Roberta：一种稳健优化的BERT预训练方法。发表于*arXiv*。
- en: 'Liu and Jin (2017) Yuliang Liu and Lianwen Jin. 2017. Deep matching prior network:
    Toward tighter multi-oriented text detection. In *CVPR*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu和Jin（2017）Yuliang Liu和Lianwen Jin。2017年。深度匹配先验网络：朝着更紧密的多定向文本检测。发表于*CVPR*。
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. In *CVPR*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long等人（2015）Jonathan Long、Evan Shelhamer和Trevor Darrell。2015年。用于语义分割的全卷积网络。发表于*CVPR*。
- en: 'Long et al. (2018) Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao
    Wu, and Cong Yao. 2018. Textsnake: A flexible representation for detecting text
    of arbitrary shapes. In *ECCV*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人（2018）Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu,
    和 Cong Yao. 2018. Textsnake：一种用于检测任意形状文本的灵活表示。在 *ECCV*。
- en: Lowe (2004) David G Lowe. 2004. Distinctive image features from scale-invariant
    keypoints. *IJCV*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe（2004）David G Lowe. 2004. 从尺度不变关键点中提取的独特图像特征。*IJCV*。
- en: Matas et al. (2004) Jiri Matas, Ondrej Chum, Martin Urban, and Tomás Pajdla.
    2004. Robust wide-baseline stereo from maximally stable extremal regions. *Image
    and vision computing*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matas 等人（2004）Jiri Matas, Ondrej Chum, Martin Urban, 和 Tomás Pajdla. 2004. 从最大稳定极值区域获得的鲁棒宽基线立体视觉。*Image
    and vision computing*。
- en: 'Monnier (2020) Tom Monnier. 2020. docextractor: An off-the-shelf historical
    document element extraction. In *Semantic Scholar*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monnier（2020）Tom Monnier. 2020. docextractor：一款现成的历史文档元素提取工具。在 *Semantic Scholar*。
- en: Mori et al. (1999) Shunji Mori, Hirobumi Nishida, and Hiromitsu Yamada. 1999.
    *Optical character recognition*. John Wiley & Sons, Inc.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mori 等人（1999）Shunji Mori, Hirobumi Nishida, 和 Hiromitsu Yamada. 1999. *光学字符识别*。John
    Wiley & Sons, Inc.
- en: 'Okun et al. (1999) Oleg Okun, David Doermann, and Matti Pietikainen. 1999.
    Page segmentation and zone classification: The state of the art. Technical report,
    OULU UNIV (FINLAND) DEPT OF ELECTRICAL ENGINEERING.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Okun 等人（1999）Oleg Okun, David Doermann, 和 Matti Pietikainen. 1999. 页面分割和区域分类：技术现状。技术报告，OULU
    UNIV（芬兰）电气工程系。
- en: Peters et al. (2018) Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,
    Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized
    word representations. In *NAACL-HLT*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等人（2018）Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
    Clark, Kenton Lee, 和 Luke Zettlemoyer. 2018. 深度上下文化词表示。在 *NAACL-HLT*。
- en: Pramanik et al. (2020) Subhojeet Pramanik, Shashank Mujumdar, and Hima Patel.
    2020. Towards a multi-modal, multi-task learning based pre-training framework
    for document representation learning. *arXiv*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pramanik 等人（2020）Subhojeet Pramanik, Shashank Mujumdar, 和 Hima Patel. 2020.
    面向文档表示学习的多模态、多任务学习预训练框架。*arXiv*。
- en: 'Qian et al. (2019) Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, and Regina
    Barzilay. 2019. GraphIE: A graph-based framework for information extraction. In
    *NAACL*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人（2019）Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, 和 Regina Barzilay.
    2019. GraphIE：一种基于图的框架用于信息提取。在 *NAACL*。
- en: 'Qin et al. (2018) W. Qin, R. Elanwar, and M. Betke. 2018. Laba: Logical layout
    analysis of book page images in arabic using multiple support vector machines.
    In *ASAR*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2018）W. Qin, R. Elanwar, 和 M. Betke. 2018. Laba：使用多个支持向量机对阿拉伯语书页图像进行逻辑布局分析。在
    *ASAR*。
- en: 'Redmon et al. (2016) Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
    Farhadi. 2016. You only look once: Unified, real-time object detection. In *CVPR*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 等人（2016）Joseph Redmon, Santosh Divvala, Ross Girshick, 和 Ali Farhadi.
    2016. 你只看一次：统一的实时目标检测。在 *CVPR*。
- en: 'Redmon and Farhadi (2018) Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental
    improvement. *ArXiv*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redmon 和 Farhadi（2018）Joseph Redmon 和 Ali Farhadi. 2018. Yolov3：增量改进。*ArXiv*。
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster r-cnn: Towards real-time object detection with region proposal networks.
    In *NeurIPS*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等人（2015）Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. 2015. Faster
    R-CNN：面向实时目标检测的区域提议网络。在 *NeurIPS*。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *International
    Conference on Medical image computing and computer-assisted intervention*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger 等人（2015）Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox. 2015.
    U-net：用于生物医学图像分割的卷积网络。在 *International Conference on Medical image computing and
    computer-assisted intervention*。
- en: Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
    Neural machine translation of rare words with subword units. In *ACL*.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等人（2016）Rico Sennrich, Barry Haddow, 和 Alexandra Birch. 2016. 使用子词单元的稀有词神经机器翻译。在
    *ACL*。
- en: Shahab et al. (2010) Asif Shahab, F. Shafait, T. Kieninger, and A. Dengel. 2010.
    An open approach towards the benchmarking of table structure recognition systems.
    In *DAS ’10*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shahab 等人（2010）Asif Shahab, F. Shafait, T. Kieninger, 和 A. Dengel. 2010. 面向表格结构识别系统的开放基准测试方法。在
    *DAS ’10*。
- en: Shi et al. (2016) Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and X. Bai.
    2016. Robust scene text recognition with automatic rectification. *CVPR*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2016）Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, 和 X. Bai. 2016.
    具有自动矫正的鲁棒场景文本识别。*CVPR*。
- en: 'Subramani (2016) Nishant Subramani. 2016. Pag2admg: An algorithm for the complete
    causal enumeration of a markov equivalence class. *arXiv preprint arXiv:1612.00099*.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Subramani (2016) Nishant Subramani. 2016. Pag2admg: 用于马尔可夫等价类的完全因果枚举的算法。*arXiv
    预印本 arXiv:1612.00099*。'
- en: Subramani et al. (2019) Nishant Subramani, Samuel R. Bowman, and Kyunghyun Cho.
    2019. Can unconditional language models recover arbitrary sentences? In *NeurIPS*.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramani 等 (2019) Nishant Subramani, Samuel R. Bowman, 和 Kyunghyun Cho. 2019.
    无条件语言模型能恢复任意句子吗？发表于*NeurIPS*。
- en: Subramani and Rao (2020) Nishant Subramani and D. Rao. 2020. Learning efficient
    representations for fake speech detection. In *AAAI*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramani 和 Rao (2020) Nishant Subramani 和 D. Rao. 2020. 高效表示学习用于虚假语音检测。发表于*AAAI*。
- en: Subramani and Suresh (2020) Nishant Subramani and Nivedita Suresh. 2020. Discovering
    useful sentence representations from large pretrained language models. *ArXiv*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Subramani 和 Suresh (2020) Nishant Subramani 和 Nivedita Suresh. 2020. 从大型预训练语言模型中发现有用的句子表示。*ArXiv*。
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. In *NeurIPS*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等 (2014) Ilya Sutskever, Oriol Vinyals, 和 Quoc V Le. 2014. 使用神经网络的序列到序列学习。发表于*NeurIPS*。
- en: Tian et al. (2016) Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao. 2016.
    Detecting text in natural image with connectionist text proposal network. In *ECCV*.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2016) Zhi Tian, Weilin Huang, Tong He, Pan He, 和 Yu Qiao. 2016. 使用连接主义文本提议网络检测自然图像中的文本。发表于*ECCV*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *NeurIPS*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力即一切。发表于*NeurIPS*。
- en: Wang and Hu (2017) J. Wang and Xiaolin Hu. 2017. Gated recurrent convolution
    neural network for ocr. In *NeurIPS*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Hu (2017) J. Wang 和 Xiaolin Hu. 2017. 用于光学字符识别的门控递归卷积神经网络。发表于*NeurIPS*。
- en: Wang et al. (2020) Zhiruo Wang, Haoyu Dong, Ran Jia, Jiugang Li, Zhiyi Fu, Shi
    Han, and Dongmei Zhang. 2020. Structure-aware pre-training for table understanding
    with tree-based transformers. *ArXiv*.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2020) Zhiruo Wang, Haoyu Dong, Ran Jia, Jiugang Li, Zhiyi Fu, Shi Han,
    和 Dongmei Zhang. 2020. 基于结构的预训练用于表格理解的树基变换器。*ArXiv*。
- en: Wei et al. (2013) Hao Wei, Micheal Baechler, Fouad Slimane, and Rolf Ingold.
    2013. Evaluation of svm, mlp and gmm classifiers for layout analysis of historical
    documents. In *ICDAR*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2013) Hao Wei, Micheal Baechler, Fouad Slimane, 和 Rolf Ingold. 2013.
    用于历史文档布局分析的支持向量机、多层感知机和高斯混合模型分类器评估。发表于*ICDAR*。
- en: Wu et al. (2018) Xiang Wu, R. He, Z. Sun, and T. Tan. 2018. A light cnn for
    deep face representation with noisy labels. *IEEE Transactions on Information
    Forensics and Security*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2018) Xiang Wu, R. He, Z. Sun, 和 T. Tan. 2018. 一种轻量级卷积神经网络用于带有噪声标签的深度人脸表示。*IEEE
    信息取证与安全学报*。
- en: Xie et al. (2019) Qizhe Xie, Zihang Dai, E. Hovy, Minh-Thang Luong, and Quoc V.
    Le. 2019. Unsupervised data augmentation for consistency training. *ArXiv*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 (2019) Qizhe Xie, Zihang Dai, E. Hovy, Minh-Thang Luong, 和 Quoc V. Le.
    2019. 一致性训练的无监督数据增强。*ArXiv*。
- en: Xu et al. (2020) Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and
    Ming Zhou. 2020. Layoutlm pre-training of text and layout for document image understanding.
    *KDD*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2020) Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, 和 Ming
    Zhou. 2020. Layoutlm 预训练用于文档图像理解的文本和布局。*KDD*。
- en: Yang et al. (2017) Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel
    Kifer, and C Lee Giles. 2017. Learning to extract semantic structure from documents
    using multimodal fully convolutional neural networks. In *CVPR*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2017) Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer,
    和 C Lee Giles. 2017. 学习从文档中提取语义结构使用多模态全卷积神经网络。发表于*CVPR*。
- en: Yao et al. (2016) Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang Zhou,
    and Zhimin Cao. 2016. Scene text detection via holistic, multi-channel prediction.
    *arXiv*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2016) Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang Zhou, 和 Zhimin
    Cao. 2016. 通过整体多通道预测进行场景文本检测。*arXiv*。
- en: 'Ye et al. (2020) Jian Ye, Z. Chen, J. Liu, and Bo Du. 2020. Textfusenet: Scene
    text detection with richer fused features. In *IJCAI*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等 (2020) Jian Ye, Z. Chen, J. Liu, 和 Bo Du. 2020. Textfusenet: 使用更丰富的融合特征进行场景文本检测。发表于*IJCAI*。'
- en: Yu and Koltun (2015) Fisher Yu and Vladlen Koltun. 2015. Multi-scale context
    aggregation by dilated convolutions. In *ICLR*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Koltun (2015) Fisher Yu 和 Vladlen Koltun. 2015. 通过扩张卷积进行多尺度上下文聚合。发表于*ICLR*。
- en: 'Yuliang et al. (2017) Liu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang Sheng.
    2017. Detecting curve text in the wild: New dataset and new solution. *arXiv*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuliang 等 (2017) Liu Yuliang, Jin Lianwen, Zhang Shuaitao, 和 Zhang Sheng. 2017.
    森林中检测曲线文本: 新数据集和新解决方案。*arXiv*。'
- en: 'Zhao et al. (2019) Xiaohui Zhao, Endi Niu, Zhuo Wu, and Xiaoguang Wang. 2019.
    Cutie: Learning to understand documents with convolutional universal text information
    extractor. *ArXiv*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2019）肖辉 Zhao、Endi Niu、Zhuo Wu 和 Xiaoguang Wang。2019。Cutie：学习通过卷积通用文本信息提取器理解文档。*ArXiv*。
- en: 'Zhong et al. (2019a) Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno-Yepes.
    2019a. Image-based table recognition: data, model, and evaluation. In *ECCV*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等（2019a）徐中、Elaheh ShafieiBavani 和 Antonio Jimeno-Yepes。2019a。基于图像的表格识别：数据、模型和评估。见于
    *ECCV*。
- en: 'Zhong et al. (2019b) Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019b.
    Publaynet: largest dataset ever for document layout analysis. In *ICDAR*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等（2019b）徐中、简斌 Tang 和 Antonio Jimeno Yepes。2019b。Publaynet：有史以来最大的文档布局分析数据集。见于
    *ICDAR*。
