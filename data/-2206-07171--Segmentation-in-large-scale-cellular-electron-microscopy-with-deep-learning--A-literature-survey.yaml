- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:45:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:45:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2206.07171] Segmentation in large-scale cellular electron microscopy with
    deep learning: A literature survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2206.07171] 大规模细胞电子显微镜中的深度学习分割：文献综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2206.07171](https://ar5iv.labs.arxiv.org/html/2206.07171)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2206.07171](https://ar5iv.labs.arxiv.org/html/2206.07171)
- en: 'Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模细胞电子显微镜中的深度学习分割：文献综述
- en: Anusha Aswath [a.aswath@rug.nl](mailto:a.aswath@rug.nl) Ahmad Alsahaf Ben N.
    G. Giepmans George Azzopardi Bernoulli Institute of Mathematics, Computer Science
    and Artificial Intelligence, University Groningen, Groningen, The Netherlands
    Dept. Biomedical Sciences of Cells and Systems, University Groningen, University
    Medical Center Groningen, Groningen, The Netherlands
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Anusha Aswath [a.aswath@rug.nl](mailto:a.aswath@rug.nl) Ahmad Alsahaf Ben N.
    G. Giepmans George Azzopardi 伯努利数学、计算机科学与人工智能研究所，格罗宁根大学，荷兰格罗宁根 生物医学科学与系统系，格罗宁根大学，格罗宁根大学医学中心，荷兰格罗宁根
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Automated and semi-automated techniques in biomedical electron microscopy (EM)
    enable the acquisition of large datasets at a high rate. Segmentation methods
    are therefore essential to analyze and interpret these large volumes of data,
    which can no longer completely be labeled manually. In recent years, deep learning
    algorithms achieved impressive results in both pixel-level labeling (semantic
    segmentation) and the labeling of separate instances of the same class (instance
    segmentation). In this review, we examine how these algorithms were adapted to
    the task of segmenting cellular and sub-cellular structures in EM images. The
    special challenges posed by such images and the network architectures that overcame
    some of them are described. Moreover, a thorough overview is also provided on
    the notable datasets that contributed to the proliferation of deep learning in
    EM. Finally, an outlook of current trends and future prospects of EM segmentation
    is given, especially in the area of label-free learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生物医学电子显微镜（EM）中的自动化和半自动化技术使得能够以高速获取大数据集。因此，分割方法对于分析和解释这些大体积数据至关重要，这些数据已无法完全手动标记。近年来，深度学习算法在像素级标记（语义分割）和同一类别的单独实例标记（实例分割）方面取得了显著成果。在这篇综述中，我们探讨了这些算法如何适应于分割EM图像中的细胞和亚细胞结构。描述了这些图像带来的特殊挑战以及克服一些挑战的网络架构。此外，还提供了对促进深度学习在EM中普及的重要数据集的详细概述。最后，给出了对EM分割当前趋势和未来前景的展望，特别是在无标签学习领域。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Electron microscopy, segmentation, supervised, unsupervised, deep learning,
    semantic, instance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 电子显微镜，分割，监督，非监督，深度学习，语义，实例
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Electron microscopy (EM) is widely used in life sciences to study tissues, cells,
    subcellular components and (macro) molecular complexes at nanometer scale. Two-dimensional
    (2D) EM aids in diagnosis of diseases, but routinely it still depends upon biased
    snapshots of areas of interest. Automated pipelines for collection, stitching
    and open access publishing of 2D EM have been pioneered for transmission EM (TEM)
    images [[45](#bib.bib45)] as well as scanning TEM (STEM) [[109](#bib.bib109)]
    for acquisition of areas up to 1mm² at nanometer-range resolution. Nowadays, imaging
    of large areas at high resolution is entering the field as a routine method and
    is provided by most EM manufacturers. We term this nanotomy, for nano-anatomy[[102](#bib.bib102),
    [15](#bib.bib15), [38](#bib.bib38)]. The large-scale images allow for open access
    world-wide data sharing; see nanotomy.org¹¹1[www.nanotomy.org](www.nanotomy.org)
    for more than 50 published studies and the accessible nanotomy data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 电子显微镜（EM）广泛应用于生命科学，以纳米尺度研究组织、细胞、亚细胞组分和（宏观）分子复合物。二维（2D）EM有助于疾病诊断，但通常仍依赖于感兴趣区域的偏倚快照。自动化管道已开创了2D
    EM图像的收集、拼接和开放获取出版[[45](#bib.bib45)]，以及扫描透射电子显微镜（STEM）[[109](#bib.bib109)]，用于获取高达1mm²的纳米级分辨率区域。如今，大范围高分辨率成像作为一种常规方法进入领域，并由大多数EM制造商提供。我们将其称为纳米解剖学[[102](#bib.bib102),
    [15](#bib.bib15), [38](#bib.bib38)]。大规模图像允许全球开放数据共享；有关50多项已发表研究和可访问的纳米解剖学数据，请参见nanotomy.org¹¹1[www.nanotomy.org](www.nanotomy.org)。
- en: '![Refer to caption](img/6547b71a65e77bb935a9be7a3d753a85.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6547b71a65e77bb935a9be7a3d753a85.png)'
- en: 'Figure 1: Large-scale EM (‘nanotomy’) of a section of human pancreas. Overview
    of a single large-scale EM (top-left) and snapshots from this total map at higher
    zoom showing several cellular, subcellular and macromolecular structures as indicated
    and annotated. Note the information density of these maps: millions of subcellular
    structures of a kind can be present per dataset [[15](#bib.bib15)]. Full access
    to digital zoomable data at full resolution is via [http://www.nanotomy.org](http://www.nanotomy.org).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：人胰腺切片的大规模电子显微镜（‘nanotomy’）图像。单个大规模电子显微镜图像的概览（左上方）以及从此总图中高倍放大显示的几个细胞、亚细胞和大分子结构的快照。注意这些地图的信息密度：每个数据集中可能存在数百万种亚细胞结构[[15](#bib.bib15)]。通过[http://www.nanotomy.org](http://www.nanotomy.org)可以完全访问数字可缩放的全分辨率数据。
- en: 'A typical nanotomy dataset has a size of 5-25GB at 2.5nm pixel size. Nanotomy
    allows scientists to pan and zoom through different tissues or cellular structures,
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Segmentation in large-scale cellular
    electron microscopy with deep learning: A literature survey"). Large-scale 2D
    EM provides unbiased data recording to discover events such as pathogenesis of
    diseases and morphological (shape and texture) changes at the subcellular level.
    Moreover, nanotomy allows for the quantification of subcellular hallmarks. With
    state-of-the-art 2D EM technology, such as multibeam scanning EM [[43](#bib.bib43),
    [104](#bib.bib104)], up to 100 times faster acquisition and higher throughput
    allows for imaging of tissue-wide sections in the range of hours instead of days.
    For a side-by-side example of single beam versus multibeam nanotomy, see de Boer
    and Giepmans [[13](#bib.bib13)]. Given the automated and faster image acquisition
    in 2D EM a data avalanche (petabyte range per microscope/month) is becoming a
    reality.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '一个典型的nanotomy数据集的大小为5-25GB，像素大小为2.5nm。Nanotomy允许科学家在不同组织或细胞结构之间进行平移和缩放，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Segmentation in large-scale cellular electron microscopy
    with deep learning: A literature survey")所示。大规模2D EM提供了无偏的数据记录，用于发现如疾病发病机制和亚细胞水平的形态（形状和纹理）变化等事件。此外，nanotomy还允许对亚细胞标志物进行量化。利用先进的2D
    EM技术，如多束扫描电子显微镜[[43](#bib.bib43), [104](#bib.bib104)]，每秒可采集速度提高至100倍，允许在几小时内而不是几天内对组织范围的切片进行成像。有关单束与多束nanotomy的并排示例，请参见de
    Boer和Giepmans[[13](#bib.bib13)]。鉴于2D EM的自动化和更快的图像采集，数据雪崩（每台显微镜每月PB级别）正成为现实。'
- en: 'Automated large-scale three-dimensional (3D) or volume EM (vEM), which creates
    stacks of images, is also booming [[95](#bib.bib95), [114](#bib.bib114), [96](#bib.bib96)]. The
    faster acquisition of 3D EM for serial-sectioning transmission EM (ssTEM) and
    serial block-face scanning EM (SBF-SEM) technologies can also lead to accumulation
    of petabytes of data. For instance, a complete brain volume of an adult fruit
    fly was imaged by ssTEM [[130](#bib.bib130)], which covered a single neuron cell
    in a volume of 1mm³ or $10,000$ voxels and required 100 TB. Additionally, manual
    annotation is not practical due to the size of 3D EM datasets. An example by Heinrich
    et al. [[59](#bib.bib59)] shows that one person needed two weeks to manually label
    a fraction (1 $\mu m^{3}$) of a whole-cell volume containing tons of instances
    of various types of organelles, whereas the whole cell could take 60 person-years.
    Whole-cell cryo-electron tomography (cryo-ET) has also advanced the capabilities
    of 3D EM to investigate the structure of cellular architecture and macromolecular
    assemblies in their native environment. The core data acquisition techniques of
    such 2D and 3D EM technologies are listed in Table [3](#footnote3 "footnote 3
    ‣ Table 1 ‣ 1 Introduction ‣ Segmentation in large-scale cellular electron microscopy
    with deep learning: A literature survey").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '自动化的大规模三维（3D）或体积电子显微镜（vEM），它创建图像堆叠，也正在蓬勃发展[[95](#bib.bib95), [114](#bib.bib114),
    [96](#bib.bib96)]。串联切片透射电子显微镜（ssTEM）和串联块面扫描电子显微镜（SBF-SEM）技术更快的3D EM采集也可能导致数据累积达到PB级。例如，使用ssTEM对成年果蝇的完整大脑体积进行了成像[[130](#bib.bib130)]，其覆盖了体积为1mm³或$10,000$体素的单个神经元细胞，所需数据量为100
    TB。此外，由于3D EM数据集的规模，手动标注是不切实际的。Heinrich等人[[59](#bib.bib59)]的一个例子表明，一个人需要两周时间来手动标注一个包含大量不同类型细胞器的全细胞体积的一个部分（1
    $\mu m^{3}$），而整个细胞可能需要60个人年。全细胞冷冻电子断层扫描（cryo-ET）也提升了3D EM的能力，以调查细胞结构和大分子组装体在其自然环境中的结构。这些2D和3D
    EM技术的核心数据采集技术列于表[3](#footnote3 "footnote 3 ‣ Table 1 ‣ 1 Introduction ‣ Segmentation
    in large-scale cellular electron microscopy with deep learning: A literature survey")中。'
- en: This increase in the scale and acquisition speeds of EM data accelerated the
    development of compatible methods for automatic analysis, especially in the areas
    of semantic and instance segmentation. Semantic segmentation classifies the pixels
    of an image into semantically meaningful categories, e.g. nuclei and background,
    while instance segmentation focuses on separating individual instances within
    the same class; e.g. the delineation of apposed mitochondria.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: EM数据的规模和获取速度的增加加速了兼容自动分析方法的发展，特别是在语义分割和实例分割领域。语义分割将图像的像素分类为具有语义意义的类别，例如核和背景，而实例分割则专注于分离同一类别中的各个实例；例如区分靠近的线粒体。
- en: In the past, traditional image analysis methods as well as shallow learning
    algorithms²²2Shallow learning in this context refers to supervised machine learning
    with hand-crafted features, or traditional unsupervised techniques such as PCA
    and clustering. have been used for the segmentation of EM images, for instance
    using statistical analysis of pixel neighborhoods [[73](#bib.bib73)], eigenvector
    analysis [[48](#bib.bib48)], watershed and hierarchical region merging [[83](#bib.bib83),
    [82](#bib.bib82)], superpixel analysis and shape modeling [[65](#bib.bib65)],
    and random forest [[17](#bib.bib17)]. However, the past few years marked a dominance
    of deep learning (DL) in this domain, similarly to the trends of segmentation
    in light microscopy and other medical imagining modalities [[84](#bib.bib84),
    [79](#bib.bib79)]. Compared to traditional image analysis and machine learning
    with handcrafted features, deep learning segmentation reduces or removes the need
    for domain knowledge of the specific imaged sample to extract relevant features
    [[84](#bib.bib84)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，传统图像分析方法以及浅层学习算法²²2在此上下文中，浅层学习指的是利用手工特征的监督机器学习，或传统的无监督技术，如PCA和聚类。已被用于EM图像的分割，例如使用像素邻域的统计分析
    [[73](#bib.bib73)]、特征向量分析 [[48](#bib.bib48)]、分水岭和层次区域合并 [[83](#bib.bib83), [82](#bib.bib82)]、超像素分析和形状建模
    [[65](#bib.bib65)]、以及随机森林 [[17](#bib.bib17)]。然而，近年来深度学习（DL）在这一领域的主导地位显著提升，类似于光学显微镜和其他医学成像方式中的分割趋势
    [[84](#bib.bib84), [79](#bib.bib79)]。与传统图像分析和利用手工特征的机器学习相比，深度学习分割减少或消除了对特定成像样本的领域知识以提取相关特征的需求
    [[84](#bib.bib84)]。
- en: The popularity of DL segmentation led to the development of DL plug-ins for
    many of the routinely used biomedical image analysis software tools like CellProfiler
    [[19](#bib.bib19)], ImageJ [[106](#bib.bib106)], Weka [[3](#bib.bib3)], and Ilastik
    [[9](#bib.bib9)], which had previously been limited to traditional image processing
    methods or shallow learning. Moreover, it led to the development of specialized
    tools that enable biologists to train and use DL networks with the aid of graphical
    user interfaces [[22](#bib.bib22), [8](#bib.bib8)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DL分割的流行促使了许多常用生物医学图像分析软件工具如CellProfiler [[19](#bib.bib19)]、ImageJ [[106](#bib.bib106)]、Weka
    [[3](#bib.bib3)] 和Ilastik [[9](#bib.bib9)] 开发了DL插件，这些工具之前仅限于传统的图像处理方法或浅层学习。此外，这也催生了专门的工具，使生物学家能够借助图形用户界面训练和使用DL网络
    [[22](#bib.bib22), [8](#bib.bib8)]。
- en: We review the recent progress of automatic image segmentation in EM, with a
    focus on the last six years that marked significant progress in both DL-based
    semantic and instance segmentation, while also giving an overview of the main
    DL architectures that enabled this progress.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了EM自动图像分割的近期进展，重点关注过去六年在DL基础的语义分割和实例分割方面取得的显著进展，同时概述了实现这一进展的主要DL架构。
- en: 'The manuscript is organized as follows: Section [2](#S2 "2 Strategy of literature
    search ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") describes the literature search strategy used for this review. Section [3](#S3
    "3 Collections of key EM datasets ‣ Segmentation in large-scale cellular electron
    microscopy with deep learning: A literature survey") presents the benchmark datasets,
    which have been key for the progress of the segmentation methods. Section [4](#S4
    "4 Background of backbone deep learning networks for EM semantic and instance
    segmentation ‣ Segmentation in large-scale cellular electron microscopy with deep
    learning: A literature survey") lays the background about the main neural network
    architectures for 2D and 3D segmentation of EM datasets. Sections [5](#S5 "5 Fully
    supervised methods ‣ Segmentation in large-scale cellular electron microscopy
    with deep learning: A literature survey") and [6](#S6 "6 Semi-, un- and self-supervised
    methods ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") review the papers that propose new methodologies for semantic
    and instance segmentation with different DL approaches. These are followed by
    Section [7](#S7 "7 Segmentation evaluation metrics ‣ Segmentation in large-scale
    cellular electron microscopy with deep learning: A literature survey"), which
    describes the evaluation metrics used in the reviewed papers. Finally, Section
    [8](#S8 "8 Discussion and open challenges ‣ Segmentation in large-scale cellular
    electron microscopy with deep learning: A literature survey") provides an outlook
    of the overall progress of this field along with a discussion on future prospects.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 手稿的组织结构如下：第[2](#S2 "2 文献检索策略 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节描述了本综述使用的文献检索策略。第[3](#S3
    "3 关键EM数据集的集合 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节介绍了基准数据集，这些数据集对于分割方法的进展至关重要。第[4](#S4
    "4 EM语义和实例分割的主干深度学习网络背景 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节提供了2D和3D EM数据集分割的主要神经网络架构背景。第[5](#S5
    "5 完全监督的方法 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节和第[6](#S6 "6 半监督、无监督和自监督的方法 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节回顾了提出新方法的论文，这些方法使用不同的深度学习（DL）方法进行语义和实例分割。接下来是第[7](#S7
    "7 分割评估指标 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节，描述了在审阅的论文中使用的评估指标。最后，第[8](#S8 "8 讨论与开放挑战
    ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")节提供了对该领域整体进展的展望，并讨论了未来的前景。
- en: 'Table 1: Main large-scale EM techniques. More information is given in the MyScope
    website³³3[https://myscope.training/](https://myscope.training/)and the reviews
    by [[95](#bib.bib95)], [[114](#bib.bib114)] and [[69](#bib.bib69)]. The last row
    shows example 2D images and 3D stacks of such technologies except STEM, an example
    of which is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Segmentation
    in large-scale cellular electron microscopy with deep learning: A literature survey").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主要的大规模EM技术。更多信息请参考MyScope网站³³3[https://myscope.training/](https://myscope.training/)以及[[95](#bib.bib95)]、[[114](#bib.bib114)]和[[69](#bib.bib69)]的综述。最后一行展示了这些技术的2D图像和3D堆栈示例，除了STEM，STEM的一个示例见图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ 基于深度学习的大规模细胞电子显微镜分割：文献综述")。
- en: 2D EM Data acquisition technique Transmission Electron Microscopy (TEM) A widefield
    electron beam illuminates an ultra-thin specimen and transmitted electrons are
    detected on the other side of the sample. The structure that is electron dense
    appears dark and others appear lighter depending on their (lack of) scattering.
    Scanning Electron Microscopy (SEM) The raster scanning beam interacts with the
    material and can result in backscattering or the formation of secondary electrons.
    Their intensity reveals sample information. Scanning Transmission Electron Microscopy
    (STEM) SEM on ultrathin sections and using a detector for the transmitted electrons.
    3D EM Serial section TEM (ssTEM) or SEM (ssSEM) Volume EM technique for examining
    3D ultrastructure by scanning adjacent ultrathin (typical 60-80nm) sections using
    TEM or SEM, respectively. Serial Block-Face scanning EM (SBF-SEM) The block face
    is scanned followed by removal of the top layer by a diamond knife (typical 20-60nm)
    and the newly exposed block face is scanned. This can be repeated thousands of
    times. Focused Ion Beam SEM (FIB-SEM) Block face imaging as above, but sections
    are repeatedly removed by a focused ion beam that has higher precision than a
    knife (typically down to 4nm), making it suitable for smaller samples. Cryo-electron
    tomography (Cryo-ET) It captures a series of 2D projection images of a flash-frozen
    specimen from different angles, and then uses computational reconstruction methods
    to generate a 3D model or tomogram.  ![[Uncaptioned image]](img/0d1b62da82b1d0cdf0963430631a098f.png)
    ![[Uncaptioned image]](img/ff71e273967b2d34a71327429dbf1bf1.png) ![[Uncaptioned
    image]](img/192a75ca804f08033a186b0b2ee1ed8c.png) ![[Uncaptioned image]](img/ff07f8d4cd43633eb1dbe654e63abec9.png)
    ![[Uncaptioned image]](img/044c17796528827cdfb1826cfb15ca5b.png) ![[Uncaptioned
    image]](img/1460edc1b723595de59f5485e562d249.png)  TEM 2D Cryo-ET 2D SEM 2D ssSEM
    volume - 2D sections SBF-SEM volume FIB-SEM  [[33](#bib.bib33)] [[29](#bib.bib29)]
    [[66](#bib.bib66)] [[1](#bib.bib1)] [[87](#bib.bib87)]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 2D EM 数据采集技术 透射电子显微镜（TEM）一个宽场电子束照射超薄标本，透射电子在样品的另一侧被检测。电子密集的结构呈现为暗色，其他结构则因其（缺乏）散射而显得较亮。
    扫描电子显微镜（SEM）光栅扫描束与材料相互作用，可能导致反向散射或二次电子的形成。其强度揭示样品信息。 扫描透射电子显微镜（STEM）在超薄切片上进行SEM，并使用探测器检测透射电子。
    3D EM 序列切片 TEM（ssTEM）或 SEM（ssSEM）体积 EM 技术，通过使用 TEM 或 SEM 扫描相邻的超薄（典型60-80nm）切片来检查
    3D 超微结构。 序列块面扫描 EM（SBF-SEM）扫描块面，随后用金刚石刀（典型20-60nm）去除顶部层，新暴露的块面被扫描。这可以重复进行数千次。
    聚焦离子束 SEM（FIB-SEM）如上所述进行块面成像，但使用聚焦离子束以比刀片更高的精度（典型4nm），适用于较小的样品。 冷冻电子断层扫描（Cryo-ET）捕捉来自不同角度的闪冻标本的系列2D投影图像，然后使用计算重建方法生成3D模型或断层图。
    ![[未标注的图像]](img/0d1b62da82b1d0cdf0963430631a098f.png) ![[未标注的图像]](img/ff71e273967b2d34a71327429dbf1bf1.png)
    ![[未标注的图像]](img/192a75ca804f08033a186b0b2ee1ed8c.png) ![[未标注的图像]](img/ff07f8d4cd43633eb1dbe654e63abec9.png)
    ![[未标注的图像]](img/044c17796528827cdfb1826cfb15ca5b.png) ![[未标注的图像]](img/1460edc1b723595de59f5485e562d249.png)
    TEM 2D Cryo-ET 2D SEM 2D ssSEM 体积 - 2D 切片 SBF-SEM 体积 FIB-SEM  [[33](#bib.bib33)]
    [[29](#bib.bib29)] [[66](#bib.bib66)] [[1](#bib.bib1)] [[87](#bib.bib87)]
- en: 2 Strategy of literature search
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文献检索策略
- en: 'Our survey strategy is motivated by the following questions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查策略受到以下问题的激励：
- en: '1.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Which datasets are accessible for EM analysis, what are their challenges and
    what role do they play in DL research?
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些数据集可用于 EM 分析，它们面临什么挑战，并且它们在深度学习研究中扮演什么角色？
- en: '2.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: How is EM image (semantic and instance) segmentation being addressed by fully/semi/un/self-supervised
    DL pipelines?
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如何通过完全/半监督/无监督深度学习管道处理EM图像（语义和实例）分割？
- en: 'To answer these questions, the following search query was used in Pubmed, Web
    of Science, and Google Scholar on words in titles (TI) only, restricted to 2017-2022:
    TI=((electron microscopy OR EM) AND (segmentation OR semantic OR instance OR supervised
    OR unsupervised OR self-supervised OR semi-supervised)), and title or abstracts
    containing (deep learning, segmentation, electron microscopy) on Google Scholar.
    Results from the query that were outside the scope of this study, such as deep
    learning in material sciences and methods based on traditional image processing
    (pre-DL era), were excluded. The forward and backward snowballing technique was
    then used to compile the final list of 38 papers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，我们在 Pubmed、Web of Science 和 Google Scholar 上使用了以下搜索查询，仅限于标题（TI），时间范围为
    2017-2022：TI=((电子显微镜 OR EM) AND (分割 OR 语义 OR 实例 OR 监督 OR 无监督 OR 自监督 OR 半监督))，并在
    Google Scholar 上搜索标题或摘要中包含（深度学习，分割，电子显微镜）的文献。排除了与本研究范围无关的结果，如材料科学中的深度学习和基于传统图像处理的方法（前深度学习时代）。随后使用了前向和后向雪球技术来编制最终的38篇论文清单。
- en: 'Fig. [2](#S2.F2 "Figure 2 ‣ 2 Strategy of literature search ‣ Segmentation
    in large-scale cellular electron microscopy with deep learning: A literature survey")
    summarizes this collection of 38 papers in terms of learning technique (fully
    supervised or not), segmentation type (semantic or instance), application (2D
    or 3D) and the underlying modeling backbone. Before reviewing these papers, we
    discuss the key EM datasets and describe the evolution of DL architectures, which
    are two crucial components that have been permitting the progress of EM segmentation
    analysis.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S2.F2 "图 2 ‣ 2 文献检索策略 ‣ 使用深度学习的规模化细胞电子显微镜分割：文献综述")总结了这38篇论文的学习技术（是否完全监督）、分割类型（语义或实例）、应用（2D
    或 3D）及其基础建模骨干。在审查这些论文之前，我们讨论了关键的电子显微镜数据集，并描述了深度学习架构的演变，这两个关键组件促进了电子显微镜分割分析的进展。
- en: '![Refer to caption](img/c2604e17cca18640ef59f68946dddbee.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c2604e17cca18640ef59f68946dddbee.png)'
- en: 'Figure 2: Categorization of the 38 papers reviewed in this survey. The papers
    are first categorized on the learning paradigm (fully vs. semi/un/self-supervised)
    and on the segmentation type (semantic vs. instance). Each quadrant shows the
    distributions of applications (2D vs. 3D) and DL backbones (U-Net vs. FCN vs.
    Other) of the papers that use the corresponding learning and segmentation approaches.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：本调查中回顾的38篇论文的分类。论文首先按学习范式（完全监督 vs. 半监督/无监督/自监督）和分割类型（语义 vs. 实例）进行分类。每个象限展示了应用（2D
    vs. 3D）和深度学习骨干（U-Net vs. FCN vs. 其他）的分布情况，这些论文使用了相应的学习和分割方法。
- en: 3 Collections of key EM datasets
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 关键 EM 数据集的集合
- en: 'Collections of labeled and unlabeled EM images have played a significant role
    in advancing DL research for EM segmentation, and some were associated with notable
    segmentation competitions and challenges. This section provides the details of
    all collections used by the 38 papers in this survey. Table [2](#S3.T2 "Table
    2 ‣ 3.1 Serial section TEM and SEM datasets ‣ 3 Collections of key EM datasets
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") reports the main properties of these datasets and below
    is an in-depth discussion of their characteristics and the challenges they address. The
    discussion is categorized according to the EM modality used to acquire the datasets.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和未标记的电子显微镜图像在推动电子显微镜分割的深度学习研究中发挥了重要作用，其中一些与著名的分割竞赛和挑战相关。本节提供了本调查中38篇论文所使用的所有数据集的详细信息。表 [2](#S3.T2
    "表 2 ‣ 3.1 Serial section TEM and SEM datasets ‣ 3 Collections of key EM datasets
    ‣ 使用深度学习的规模化细胞电子显微镜分割：文献综述")列出了这些数据集的主要属性，下面是对其特征和所解决挑战的深入讨论。讨论按照用于获取数据集的电子显微镜模式进行分类。
- en: 3.1 Serial section TEM and SEM datasets
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 串行切片 TEM 和 SEM 数据集
- en: Serial-section transmission or scanning EM (ssTEM or ssSEM) is used for studying
    synaptic junctions and highly-resolved membranes in neural tissues. Advances in
    microscopy techniques in serial section EM have enabled the study of neurons with
    increased connectivity in complex mammalian tissues (such as mice and humans)
    and even whole brain tissues of smaller animal models, like the fruit fly and
    zebrafish. This imaging approach visualizes the generated volumes in a highly
    anisotropic manner, i.e. the $x$- and $y$-directions have a high resolution, however,
    the $z$-direction has a lower resolution, as it is reliant on serial cutting precision.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 序列切片传输或扫描电子显微镜（ssTEM 或 ssSEM）用于研究神经组织中的突触连接和高分辨率膜。序列切片电子显微镜技术的进步使得可以在复杂哺乳动物组织（如小鼠和人类）中研究具有更高连接性的神经元，甚至在较小动物模型（如果蝇和斑马鱼）的整个大脑组织中进行研究。这种成像方法以高度各向异性的方式可视化生成的体积，即
    $x$ 和 $y$ 方向具有高分辨率，而 $z$ 方向的分辨率较低，因为它依赖于序列切割精度。
- en: The Drosophila larve dataset (#1)⁴⁴4$\#n$ refers to the entry $n$ in Table 2.
    of the ISBI 2012 challenge was the first notable EM dataset for automatic neuronal
    segmentation, featuring two volumes with 30 sections each. The main challenge
    of that dataset is to develop algorithms that can accurately segment the neural
    structures present in the EM images. The success of deep neural networks as pixel
    classifiers in the ISBI 2012 challenge [[33](#bib.bib33)] paved the way for deep
    learning in serial section EM segmentation. Recently, a connectome of an entire
    brain of a Drosophila fruit fly has been published by Winding et al. [[120](#bib.bib120)],
    and will serve as a new resource for various follow-up works.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 果蝇幼虫数据集 (#1)⁴⁴4$\#n$ 指的是 ISBI 2012 挑战中表 2 的第 $n$ 项，这是第一个显著的用于自动神经元分割的电子显微镜数据集，包含两个体积，每个体积有
    30 个切片。该数据集的主要挑战是开发能够准确分割电子显微镜图像中神经结构的算法。深度神经网络作为像素分类器在 ISBI 2012 挑战中的成功[[33](#bib.bib33)]
    为序列切片电子显微镜分割中的深度学习铺平了道路。最近，Winding 等人发布了整个果蝇大脑的连通组图谱[[120](#bib.bib120)]，这将作为各种后续工作的新资源。
- en: The CREMI3D dataset (#2) consists of three large and diverse sub-volumes of
    neural tissue along with ground truth annotations for training and evaluation
    purposes, and was part of a competition at the MICCAI 2016 conference. The dataset
    comes from a full adult fly brain (FAFB) volume and contains 213 teravoxels. It
    was imaged at the synaptic resolution to understand the functioning of brain circuits
    (connectomics) and its goal was to segment neurons, synapses, and their pre-post
    synaptic partners. The CREMI3D dataset is part of the FlyEM project and since
    its inception, it has been used to evaluate various image analysis methods for
    neural circuit reconstruction, including DL approaches such as convolutional neural
    networks (CNNs) and recurrent neural networks (RNNs).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CREMI3D 数据集 (#2) 包含三个大型且多样化的神经组织子体积，并附有用于训练和评估的真实标注，是 MICCAI 2016 会议上的一个竞赛的一部分。该数据集来自一个完整的成体果蝇大脑（FAFB）体积，包含
    213 太伏特。它以突触分辨率成像，旨在了解大脑电路（连通组学）的功能，其目标是分割神经元、突触及其前后突触伙伴。CREMI3D 数据集是 FlyEM 项目的一部分，自其成立以来，它已被用来评估各种神经电路重建的图像分析方法，包括卷积神经网络（CNN）和递归神经网络（RNN）等深度学习方法。
- en: The SNEMI3D dataset (#3) consists of a volume of 100 ssSEM images of the neural
    tissue from a mouse cortex. It is a subset of the largest mouse neocortex dataset
    imaged by Kasthuri et al. [[66](#bib.bib66)] using an automated ssSEM technique
    and hence is also known as the Kasthuri dataset. The dataset was created as part
    of the ISBI 2013 challenge on segmentation of neural structures in EM images.
    The main challenge of this dataset is to develop algorithms that can accurately
    segment the neuronal membranes present in the EM images and reconstruct a 3D model
    of the tissue. This is a difficult task due to the large size of the dataset and
    the complexity of the neural structures, namely axons, dendrites, synapses, and
    glial cells.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: SNEMI3D 数据集 (#3) 包含 100 张小鼠皮层神经组织的 ssSEM 图像。它是 Kasthuri 等人[[66](#bib.bib66)]
    使用自动 ssSEM 技术拍摄的最大小鼠新皮质数据集的一个子集，因此也被称为 Kasthuri 数据集。该数据集是 ISBI 2013 挑战的一部分，挑战内容是分割电子显微镜图像中的神经结构。该数据集的主要挑战是开发能够准确分割电子显微镜图像中神经膜的算法，并重建组织的
    3D 模型。由于数据集的巨大规模和神经结构的复杂性（如轴突、树突、突触和胶质细胞），这是一项困难的任务。
- en: 'The Kasthuri++ and Lucchi++ (#4, #9) datasets were introduced by Casser et al.
    [[21](#bib.bib21)] with corrected annotations of Kasthuri and Lucchi. The Kasthuri
    dataset, which is used for dense reconstructions of the neuronal cells was corrected
    for the jaggedness between inter-slice components as they were not accurate. The
    Lucchi dataset is a FIB-SEM dataset used for the segmentation of mitochondria
    in the mouse neocortex. It was corrected for consistency of all annotations related
    to mitochondrial membranes, as well as to rectify any categorization errors in
    the ground truth.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kasthuri++ 和 Lucchi++ (#4, #9) 数据集由 Casser 等人引入 [[21](#bib.bib21)]，修正了 Kasthuri
    和 Lucchi 的注释。Kasthuri 数据集用于神经细胞的密集重建，已修正了切片间组件的锯齿状，因为这些组件不准确。Lucchi 数据集是一个 FIB-SEM
    数据集，用于小鼠新皮层中线粒体的分割。它已修正了与线粒体膜相关的所有注释的一致性，并纠正了真实数据中的任何分类错误。'
- en: 'Table 2: Key datasets from studies that perform high-resolution automated (volume)
    EM segmentation using deep learning. The abbreviations of the (sub) cellular structures
    are defined in the legend.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用深度学习进行高分辨率自动（体积）电子显微镜分割的关键数据集。这些（亚）细胞结构的缩写在图例中定义。
- en: '[t] # Dataset Acquisition Region Pixel/Voxel size ($nm$) Pixels Labeled (sub)
    cellular structures Public repository 1 ISBI 2012/ Drosophila VNC ssTEM Nervous
    cord (Drosophila) $4\times 4\times 50$ $512\times 512\times 30$ NM [https://imagej.net/events/isbi-2012-segmentation-challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)
    2 MICCAI 2016/ CREMI3D ssTEM Adult fly brain (Drosophila) $4\times 4\times 40$
    $1250\times 1250\times 125$ NM, S, SP [https://cremi.org](https://cremi.org) 3
    ISBI 2013/ SNEMI3D / Kasthuri ssSEM Neocortex (Mouse) $3\times 3\times 30$ $1024\times
    1024\times 100$ NM [https://snemi3d.grand-challenge.org/](https://snemi3d.grand-challenge.org/)
    4 Kasthuri++ ssSEM Neocortex (Mouse) $3\times 3\times 30$ $1643\times 1613\times
    85$ M, NM [https://casser.io/connectomics](https://casser.io/connectomics) 5 Xiao
    ssSEM Cortex (Rat) $2\times 2\times 50$ $8624\times 8416\times 20$ M [http://95.163.198.142/MiRA/mitochondria31/](http://95.163.198.142/MiRA/mitochondria31/)
    6 MitoEM ssSEM Cortex(Human, rat) $8\times 8\times 30$ $4096\times 4096\times
    1000$ M [https://mitoem.grand-challenge.org/](https://mitoem.grand-challenge.org/)
    7 NucMM ssSEM Whole brain (Zebrafish) $4\times 4\times 30$ $1450\times 2000\times
    397$ N [https://nucmm.grand-challenge.org/](https://nucmm.grand-challenge.org/)
    8 Lucchi / EPFL Hippocampus FIB-SEM Hippocampus (Mouse) $5\times 5\times 5$ $1024\times
    768\times 165$ M [https://www.epfl.ch/labs/cvlab/data/data-em/](https://www.epfl.ch/labs/cvlab/data/data-em/)
    9 Lucchi++ FIB-SEM Hippocampus (Mouse) $5\times 5\times 5$ $1024\times 768\times
    165$ M [https://casser.io/connectomics](https://casser.io/connectomics) 10 FIB-25
    FIB-SEM Optic lobe (Drosophila) $8\times 8\times 8$ $520\times 520\times 520$
    N, S [http://research.janelia.org/FIB-25/FIB-25.tar.bz2](http://research.janelia.org/FIB-25/FIB-25.tar.bz2)
    11 OpenOrganelle FIB-SEM Interphase HeLa, Macrophage, T-cells $8\times 8\times
    8$ Varying sizes CN, CH, EN, ER, ERN, ERES, G, LP, L, MT, NE, NP, Nu, N, PM, R,
    V [https://openorganelle.janelia.org](https://openorganelle.janelia.org) 12 Cardiac
    mitochondria FIB-SEM Heart muscle (Mouse) $15\times 15\times 15$ $1728\times 2022\times
    100$ M [http://labalaban.nhlbi.nih.gov/files/SuppDataset.tif](http://labalaban.nhlbi.nih.gov/files/SuppDataset.tif)
    13 UroCell FIB-SEM Urothelial cells (Mouse) $16\times 16\times 15$ 5 subvolumes
    of $256\times 256\times 256$ G, L, M, V [https://github.com/MancaZerovnikMekuc/UroCell](https://github.com/MancaZerovnikMekuc/UroCell)
    14 Perez SBF-SEM Brain (Mouse) $7.8\times 7.8\times 30$ $16000\times 12000\times
    1283$ L, M, Nu, N [https://www.sci.utah.edu/releases/chm_v2.1.367/](https://www.sci.utah.edu/releases/chm_v2.1.367/)
    15 SegEM SBF-SEM Mouse cortex $11\times 11\times 26$ 279 volumes of $100\times
    100\times 100$ NM [https://segem.rzg.mpg.de/webdav/SegEM_challenge/](https://segem.rzg.mpg.de/webdav/SegEM_challenge/)
    16 Guay SBF-SEM Platelets (Human) $10\times 10\times 50$ $800\times 800\times
    50$ Cell, CC, CP, GN, M [https://leapmanlab.github.io/dense-cell/](https://leapmanlab.github.io/dense-cell/)
    17 Axon SBF-SEM White matter (Mouse) $50\times 50\times 50$ $1000\times 1000\times
    3250$ A, M, My, N [http://segem.brain.mpg.de/challenge/](http://segem.brain.mpg.de/challenge/)
    18 CDeep3M-S SBF-SEM Brain (Mouse) $2.4\times 2.4\times 24$ $16000\times 10000\times
    400$ M, NM, Nu, V [https://github.com/CRBS/cdeep3m](https://github.com/CRBS/cdeep3m)
    19 EMPIAR-10094 SBF-SEM HeLa cells $10\times 10\times 50$ $8192\times 8192\times
    517$ Unlabeled [http://dx.doi.org/10.6019/EMPIAR-10094](http://dx.doi.org/10.6019/EMPIAR-10094)
    20 CEM500K All of the above 20 regions (10 organisms) $2\times 2\times 2$ to $20\times
    20\times 20$ $224\times 224\times 496544$ Unlabeled [https://www.ebi.ac.uk/empiar/EMPIAR-10592/](https://www.ebi.ac.uk/empiar/EMPIAR-10592/)
    21 CDeep3M-C Cryo-ET Brain (Mouse) $1.6\times 1.6\times 1.6$ $938\times 938\times
    938$ NM, V [https://github.com/CRBS/cdeep3m](https://github.com/CRBS/cdeep3m)
    22 Cellular Cryo-ET Cryo-ET PC12 cells $2.8\times 2.8\times 2.8$ $938\times 938\times
    938$ L, M, PM, V [https://www.ebi.ac.uk/emdb/EMD-8594](https://www.ebi.ac.uk/emdb/EMD-8594)
    A - Axons, CC - Canalicular channel, CH - Chromatin, CN - Centrosome, CP - Cytoplasm,
    D - Dendrites, EN - Endoplasmic Reticulum, ERES - Endoplasmic Reticulum Exit Site,
    G - Golgi, GC - Glial cells, GN - Granules, L - Lysosome, LD - Lipid Droplet,
    M - Mitochondria, MT - Microtubule, My - Myelin. N - Nucleus, NE - Nuclear Envelope,
    NM - Neuronal membrane, NP - Nuclear Pore, Nu - Nucleolus, PM - Plasma Membrane,
    R - Ribosome, S -Synapse, SP - Synaptic partners, V - Vesicle.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[t] # 数据集获取区域 像素/体素大小（$nm$） 像素 标记的（子）细胞结构 公开仓库 1 ISBI 2012/ 果蝇 VNC ssTEM 神经索（果蝇）
    $4\times 4\times 50$ $512\times 512\times 30$ NM [https://imagej.net/events/isbi-2012-segmentation-challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)
    2 MICCAI 2016/ CREMI3D ssTEM 成年果蝇脑 $4\times 4\times 40$ $1250\times 1250\times
    125$ NM, S, SP [https://cremi.org](https://cremi.org) 3 ISBI 2013/ SNEMI3D / Kasthuri
    ssSEM 新皮层（小鼠） $3\times 3\times 30$ $1024\times 1024\times 100$ NM [https://snemi3d.grand-challenge.org/](https://snemi3d.grand-challenge.org/)
    4 Kasthuri++ ssSEM 新皮层（小鼠） $3\times 3\times 30$ $1643\times 1613\times 85$ M,
    NM [https://casser.io/connectomics](https://casser.io/connectomics) 5 Xiao ssSEM
    皮层（大鼠） $2\times 2\times 50$ $8624\times 8416\times 20$ M [http://95.163.198.142/MiRA/mitochondria31/](http://95.163.198.142/MiRA/mitochondria31/)
    6 MitoEM ssSEM 皮层（人类、大鼠） $8\times 8\times 30$ $4096\times 4096\times 1000$ M [https://mitoem.grand-challenge.org/](https://mitoem.grand-challenge.org/)
    7 NucMM ssSEM 全脑（斑马鱼） $4\times 4\times 30$ $1450\times 2000\times 397$ N [https://nucmm.grand-challenge.org/](https://nucmm.grand-challenge.org/)
    8 Lucchi / EPFL 海马 FIB-SEM 海马（小鼠） $5\times 5\times 5$ $1024\times 768\times 165$
    M [https://www.epfl.ch/labs/cvlab/data/data-em/](https://www.epfl.ch/labs/cvlab/data/data-em/)
    9 Lucchi++ FIB-SEM 海马（小鼠） $5\times 5\times 5$ $1024\times 768\times 165$ M [https://casser.io/connectomics](https://casser.io/connectomics)
    10 FIB-25 FIB-SEM 视叶（果蝇） $8\times 8\times 8$ $520\times 520\times 520$ N, S [http://research.janelia.org/FIB-25/FIB-25.tar.bz2](http://research.janelia.org/FIB-25/FIB-25.tar.bz2)
    11 OpenOrganelle FIB-SEM 间期 HeLa、巨噬细胞、T细胞 $8\times 8\times 8$ 不同尺寸 CN, CH, EN,
    ER, ERN, ERES, G, LP, L, MT, NE, NP, Nu, N, PM, R, V [https://openorganelle.janelia.org](https://openorganelle.janelia.org)
    12 心脏线粒体 FIB-SEM 心肌（小鼠） $15\times 15\times 15$ $1728\times 2022\times 100$ M [http://labalaban.nhlbi.nih.gov/files/SuppDataset.tif](http://labalaban.nhlbi.nih.gov/files/SuppDataset.tif)
    13 UroCell FIB-SEM 尿路上皮细胞（小鼠） $16\times 16\times 15$ 5 个子体积 $256\times 256\times
    256$ G, L, M, V [https://github.com/MancaZerovnikMekuc/UroCell](https://github.com/MancaZerovnikMekuc/UroCell)
    14 Perez SBF-SEM 大脑（小鼠） $7.8\times 7.8\times 30$ $16000\times 12000\times 1283$
    L, M, Nu, N [https://www.sci.utah.edu/releases/chm_v2.1.367/](https://www.sci.utah.edu/releases/chm_v2.1.367/)
    15 SegEM SBF-SEM 小鼠皮层 $11\times 11\times 26$ 279 个体积 $100\times 100\times 100$
    NM [https://segem.rzg.mpg.de/webdav/SegEM_challenge/](https://segem.rzg.mpg.de/webdav/SegEM_challenge/)
    16 Guay SBF-SEM 血小板（人类） $10\times 10\times 50$ $800\times 800\times 50$ Cell,
    CC, CP, GN, M [https://leapmanlab.github.io/dense-cell/](https://leapmanlab.github.io/dense-cell/)
    17 Axon SBF-SEM 白质（小鼠） $50\times 50\times 50$ $1000\times 1000\times 3250$ A,
    M, My, N [http://segem.brain.mpg.de/challenge/](http://segem.brain.mpg.de/challenge/)
    18 CDeep3M-S SBF-SEM 大脑（小鼠） $2.4\times 2.4\times 24$ $16000\times 10000\times
    400$ M, NM, Nu, V [https://github.com/CRBS/cdeep3m](https://github.com/CRBS/cdeep3m)
    19 EMPIAR-10094 SBF-SEM HeLa 细胞 $10\times 10\times 50$ $8192\times 8192\times
    517$ 未标记 [http://dx.doi.org/10.6019/EMPIAR-10094](http://dx.doi.org/10.6019/EMPIAR-10094)
    20 CEM500K 上述所有 20 个区域（10 种生物） $2\times 2\times 2$ 到 $20\times 20\times 20$ $224\times
    224\times 496544$ 未标记 [https://www.ebi.ac.uk/empiar/EMPIAR-10592/](https://www.ebi.ac.uk/empiar/EMPIAR-10592/)
    21 CDeep3M-C 冷冻电子断层扫描 大脑（小鼠） $1.6\times 1.6\times 1.6$ $938\times 938\times 938$
    NM, V [https://github.com/CRBS/cdeep3m](https://github.com/CRBS/cdeep3m) 22 Cellular
    冷冻电子断层扫描 冷冻电子断层扫描 PC12 细胞 $2.8\times 2.8\times 2.8$ $938\times 938\times 938$
    L, M, PM, V [https://www.ebi.ac.uk/emdb/EMD-8594](https://www.ebi.ac.uk/emdb/EMD-8594)
    A - 轴突，CC - 管道通道，CH - 染色质，CN - 中心体，CP - 细胞质，D - 树突，EN - 内质网，ERES - 内质网出口位点，G -
    高尔基体，GC - 胶质细胞，GN - 小颗粒，L - 溶酶体，LD - 脂滴，M - 线粒体，MT - 微管，My - 髓磷脂。N - 细胞核，NE -
    核膜，NM - 神经膜，NP - 核孔，Nu - 核仁，PM - 质膜，R - 核糖体，S - 突触，SP - 突触伙伴，V - 小泡。'
- en: The Xiao (#5) dataset for mitochondria segmentation was collected from a rat
    brain by Xiao et al. [[122](#bib.bib122)] using advanced ssSEM technology. Automated
    cutting was used to produce 31 sections, each with an approximate thickness of
    50 nm for segmenting mitochondria. The ground truth dataset was prepared through
    2D manual annotation and image registration of serial-section images, which was
    made publicly available for accelerating neuroscience analysis.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao (#5) 数据集用于线粒体分割，由Xiao等人[[122](#bib.bib122)] 使用先进的ssSEM技术从大鼠脑中收集。使用自动切割生成了31个切片，每个切片的厚度约为50
    nm，用于分割线粒体。通过2D手动注释和序列切片图像的图像配准准备了真实数据集，并公开以加速神经科学分析。
- en: Mito-EM (#6) [[119](#bib.bib119)] introduced the largest mammalian mitochondria
    dataset from humans (MitoEM-H) and adult rats (MitoEM-R). It is about 3600 times
    larger than the standard dataset for mitochondria segmentation (Lucchi) containing
    mitochondria instances of at least 2000 voxels in size. Complex morphology such
    as mitochondria on a string (MOAS) connected by thin microtubules or instances
    entangled in 3D were captured using ssSEM. The MitoEM dataset was created to provide
    a comprehensive view of the ultrastructure of mitochondria and to facilitate a
    comparative study of mitochondrial morphology and function in rats and humans.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Mito-EM (#6) [[119](#bib.bib119)] 引入了来自人类（MitoEM-H）和成年大鼠（MitoEM-R）的最大哺乳动物线粒体数据集。它比用于线粒体分割的标准数据集（Lucchi）大约大3600倍，后者包含至少2000个体素大小的线粒体实例。使用ssSEM捕获了复杂的形态，如由细微管连接的线粒体串（MOAS）或在3D中缠绕的实例。MitoEM数据集的创建旨在提供线粒体超微结构的全面视角，并促进对大鼠和人类线粒体形态和功能的比较研究。
- en: The NucMM dataset (#7) [[78](#bib.bib78)] contains two fully annotated volumes;
    one that contains almost a whole zebrafish brain with around 170,000 nuclei imaged
    using ssTEM; and another that contains part of a mouse visual cortex with about
    7,000 nuclei imaged using micro-CT. Micro-CT or micro-computed tomography uses
    X-rays to produce 3D images of objects at low resolution and hence is not a part
    of this review. The large-scale nuclei instance segmentation dataset from ssTEM
    covers 0.14${mm}^{3}$ of the entire volume of the zebrafish brain at $4\times
    4\times 30$ nm/voxel. As most of the nuclei segmentation datasets are from light
    microscopy at the $\mu m$ scale, the dataset was downsampled to $512\times 512\times
    480$ nm/voxel.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NucMM数据集 (#7) [[78](#bib.bib78)] 包含两个完全注释的体积；一个是使用ssTEM成像的几乎整个斑马鱼脑，包含约170,000个细胞核；另一个是使用微CT成像的部分小鼠视觉皮层，约包含7,000个细胞核。微CT或微计算机断层扫描使用X射线生成低分辨率的物体3D图像，因此不在本综述范围内。来自ssTEM的大规模细胞核实例分割数据集覆盖了斑马鱼脑整个体积的0.14${mm}^{3}$，分辨率为$4\times
    4\times 30$ nm/体素。由于大多数细胞核分割数据集来自光学显微镜，且在$\mu m$尺度下，数据集被下采样到$512\times 512\times
    480$ nm/体素。
- en: 3.2 FIB-SEM datasets
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 FIB-SEM数据集
- en: FIB-SEM generates datasets that are ideal for automated connectome tracing and
    for examining brain tissue at resolutions lower than $10\times 10\times 10$ nm.
    The method can produce sections with a thickness of $4$ nm, but the volumes are
    typically smaller in comparison to other techniques, due to their high $z$-resolutions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: FIB-SEM生成的数据集非常适合自动化连通组追踪和以低于$10\times 10\times 10$ nm的分辨率检查脑组织。该方法可以生成厚度为$4$
    nm的切片，但由于其高$z$分辨率，相比其他技术，体积通常较小。
- en: The Lucchi dataset (#8) is an isotropic FIB-SEM volume imaged from the hippocampus
    of a mouse brain, and it has the same spatial resolution along all three axes.
    This dataset has now become the de facto standard for evaluating mitochondria
    segmentation performance. Efforts to expand FIB-SEM to larger volumes were made
    by Takemura et al. [[112](#bib.bib112)] who compiled the FIB-25 (#10) dataset
    by reconstructing the synaptic circuits of seven columns in the eye region of
    a Drosophila’s brain. FIB-25 contains over 10,000 annotated neurons, including
    their synaptic connections, and is one of the most comprehensive EM datasets of
    the Drosophila brain to date. It was created to provide a detailed map of the
    neural circuits in the Drosophila brain and to facilitate the study of neural
    connectivity and information processing. The dataset is publicly available and
    can be accessed through the FlyEM project website.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Lucchi 数据集（#8）是一个从小鼠大脑的海马体成像的各向同性 FIB-SEM 体积数据集，具有沿所有三个轴相同的空间分辨率。该数据集现已成为评估线粒体分割性能的事实标准。Takemura
    等人曾努力将 FIB-SEM 扩展到更大的体积，[[112](#bib.bib112)] 他们通过重建果蝇大脑眼区七个柱状结构的突触电路，编制了 FIB-25（#10）数据集。FIB-25
    包含超过 10,000 个标注的神经元，包括它们的突触连接，是迄今为止最全面的果蝇大脑 EM 数据集之一。该数据集的创建旨在提供果蝇大脑神经电路的详细图谱，并促进神经连通性和信息处理的研究。该数据集是公开的，可以通过
    FlyEM 项目网站访问。
- en: Enhanced FIB-SEM techniques have also enabled high-throughput and reliable long-term
    imaging for large-scale EM ($10^{3}$ to $3\times 10^{7}\mu m^{3}$), such as the
    OpenOrganelle atlas (#11) of 3D whole cells and tissues of Xu et al. [[125](#bib.bib125)].
    The datasets for the 3D reconstruction of cells were made open-source under the
    OpenOrganelle repository for exploring local cellular interactions and their intricate
    arrangements.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的 FIB-SEM 技术还实现了大规模 EM 的高通量和可靠的长期成像（$10^{3}$ 到 $3\times 10^{7}\mu m^{3}$），例如
    Xu 等人（#11）的 3D 全细胞和组织的 OpenOrganelle 图谱 [[125](#bib.bib125)]。这些细胞 3D 重建数据集在 OpenOrganelle
    资源库下公开，以便探索局部细胞相互作用及其复杂的排列。
- en: Other FIB-SEM datasets include ones requiring a high-resolution analysis of
    3D organelles in important tissues of the heart muscle and urinary bladder. Cardiac
    mitochondria (#12) is a FIB-SEM dataset introduced to segment mitochondria in
    cardiomyocytes [[68](#bib.bib68)]. The FIB-SEM technique was needed to better
    characterize diffusion channels in mitochondria-rich muscle fibers. Isotropic
    voxels at 15 nm resolution were imaged according to the set of experiments performed
    by Glancy et al. [[51](#bib.bib51)]. The UroCell (#13) from FIB-SEM was imaged
    by Mekuč et al. [[90](#bib.bib90)] to focus on mitochondria and endolysosomes
    and was further extended to Golgi apparatus and fusiform vesicles. The dataset
    is unique as it is publicly available for further analysis of the epithelium cells
    of the urinary bladder, where the organelles form an important component in maintaining
    the barrier between the membrane of the bladder and the surrounding blood tissues.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 FIB-SEM 数据集包括需要对心肌和尿囊重要组织中的 3D 细胞器进行高分辨率分析的数据集。心脏线粒体（#12）是一个 FIB-SEM 数据集，旨在对心肌细胞中的线粒体进行分割
    [[68](#bib.bib68)]。需要使用 FIB-SEM 技术来更好地表征线粒体丰富的肌纤维中的扩散通道。根据 Glancy 等人进行的实验，15 nm
    分辨率的各向同性体素被成像 [[51](#bib.bib51)]。Mekuč 等人通过 FIB-SEM 成像的 UroCell（#13） [[90](#bib.bib90)]
    主要关注线粒体和内溶酶体，并进一步扩展到高尔基体和梭形囊泡。该数据集独特之处在于它是公开的，可供进一步分析尿囊的上皮细胞，其中细胞器在维持膀胱膜与周围血组织之间的屏障方面发挥着重要作用。
- en: 3.3 SBF-SEM datasets
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 SBF-SEM 数据集
- en: Connectomics research was also based on popular datasets imaged using SBF-SEM
    [[61](#bib.bib61), [16](#bib.bib16)]. Imaging using SBF-SEM produces anisotropic
    sections but does not need image registration and avoids missing sections in comparison
    to serial-sectioning TEM/SEM, as the technique images the sample intact on a block
    surface. Such a technique also enabled imaging large volumes for studying the
    organization of neural circuits and cells across hundreds of microns through millimeters
    of neurons in a $z$-stack.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 连接组学研究也基于使用 SBF-SEM 成像的热门数据集 [[61](#bib.bib61), [16](#bib.bib16)]。使用 SBF-SEM
    的成像产生各向异性切片，但不需要图像配准，并且避免了与串行切片 TEM/SEM 相比丢失切片的问题，因为该技术在块面上成像样本保持完整。这种技术还使得在 $z$
    轴堆叠中成像大体积以研究神经电路和细胞的组织成为可能，涵盖了数百微米到毫米的神经元。
- en: The Perez dataset (#14) [[98](#bib.bib98)] involved the acquisition of 1283
    serial images from the hypothalamus’s suprachiasmatic nucleus (SCN), a small part
    of the mouse brain, to produce an image stack with tissue dimensions approximately
    measuring 450,000 $\mu m^{3}$. The large acquired volume was downsampled from
    3.8 to 7.8 nm/pixel in the $x-y$ resolution to scale up the processing of these
    tetra-voxel-sized SBF-SEM images. It was introduced for the automatic segmentation
    of mitochondria, lysosomes, nuclei, and nucleoli in brain tissues.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Perez 数据集（#14） [[98](#bib.bib98)] 涉及从小鼠脑部的下丘脑上交叉核（SCN）获取 1283 张序列图像，以生成一个约 450,000
    $\mu m^{3}$ 的组织图像堆栈。大体积的图像被从 3.8 降采样到 7.8 nm/像素，以提升处理这些四体素大小 SBF-SEM 图像的能力。该数据集用于自动分割脑组织中的线粒体、溶酶体、细胞核和核仁。
- en: SegEM (#15) introduced an EM dataset acquired using SBF-SEM from the mouse somatosensory
    cortex [[12](#bib.bib12)]. The images in the SegEM dataset are provided with corresponding
    segmentation labels for dendrites, axons, and synapses. The labels were generated
    using a semi-automated approach which involved a combination of skeleton annotations
    and machine learning algorithms to trace long neurites accurately. Since then,
    SegEM has been used for benchmarking popular models like flood-filling networks
    that test the efficiency of algorithms on volume-spanning neurites.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SegEM（#15）引入了一个使用 SBF-SEM 从小鼠体感皮层获取的 EM 数据集 [[12](#bib.bib12)]。SegEM 数据集中的图像附有对应的分割标签，标注了树突、轴突和突触。标签是通过半自动化的方法生成的，该方法结合了骨架注释和机器学习算法，以准确追踪长神经突。此后，SegEM
    被用于基准测试流行模型，如测试算法在跨体积神经突上的效率的洪水填充网络。
- en: The Guay dataset (#16) is a fully annotated dataset of platelet cells from two
    human subjects and was designed for dense cellular segmentation [[52](#bib.bib52)].
    It has also been used for large-volume cell reconstruction along with mitochondria,
    nuclei, lysosomes, and various granules inside the cells.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Guay 数据集（#16）是一个完整标注的血小板细胞数据集，来自两名人类受试者，旨在进行密集的细胞分割 [[52](#bib.bib52)]。它也被用于大体积细胞重建，包括细胞内的线粒体、细胞核、溶酶体和各种颗粒。
- en: The Axon dataset (#17) is a collection of SBF-SEM images of white matter tissue
    from rats, captured at a lower resolution of 50 nm/pixel [[1](#bib.bib1)]. The
    low-resolution image stack of 130000 $\mu m^{3}$ was enough to resolve structures
    like myelin, myelinated axons, mitochondria, and cell nuclei. A wide field of
    view employing low-resolution SBF-SEM stacks was considered important for quantifying
    metrics such as myelinated axon tortuosity, inter-mitochondrial distance, and
    cell density.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Axon 数据集（#17）是来自大鼠的白质组织的 SBF-SEM 图像集合，分辨率为 50 nm/像素 [[1](#bib.bib1)]。这组低分辨率的
    130000 $\mu m^{3}$ 图像堆栈足以解析出髓鞘、髓鞘轴突、线粒体和细胞核等结构。采用低分辨率 SBF-SEM 堆栈的广视场被认为对于量化指标如髓鞘轴突的弯曲度、线粒体间距离和细胞密度是重要的。
- en: 'CDeep3M proposed two new datasets from SBF-SEM and cryo electron tomography
    (cryo-ET) for automatic segmentation. The first one, CDeep3M-S (#18), is a large
    SBF-SEM dataset for membrane, mitochondria, and synapse identification from the
    cerebellum and lateral habenula of mice. Imaged at $2.4$ nm pixel size, a cloud
    implementation of the latest architecture for anisotropic datasets was used to
    segment structures such as the neuronal membrane, synaptic vesicles mitochondria,
    and nucleus in brain tissues. The second dataset, CDeep3M-C (#21), was from cryo-ET
    and is explained further in subsection [3.4](#S3.SS4 "3.4 Cryo-ET datasets ‣ 3
    Collections of key EM datasets ‣ Segmentation in large-scale cellular electron
    microscopy with deep learning: A literature survey").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 'CDeep3M 提出了两个来自 SBF-SEM 和冷冻电子断层扫描（cryo-ET）的新数据集，用于自动分割。第一个数据集，CDeep3M-S（#18），是一个大型的
    SBF-SEM 数据集，用于识别来自小鼠小脑和侧脑岛的膜、线粒体和突触。图像分辨率为 $2.4$ nm 像素，使用了最新架构的云实现来分割神经膜、突触囊泡、线粒体和脑组织中的细胞核。第二个数据集，CDeep3M-C（#21），来自
    cryo-ET，进一步在小节 [3.4](#S3.SS4 "3.4 Cryo-ET datasets ‣ 3 Collections of key EM datasets
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") 中进行了解释。'
- en: The EMPIAR-10094 dataset (#19) consists of EM images of cervical cancer “HeLa”
    cells imaged using SBF-SEM. The dataset is imaged at $8192\times 8192$ pixels
    over a total of 518 slices, and consists of different HeLa cells distributed in
    the background of the embedding resin. The dataset has been made publicly available
    with no labels and has mostly been used for delineating structures such as plasma
    membranes and nuclear envelopes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: EMPIAR-10094 数据集（#19）包含了使用 SBF-SEM 成像的宫颈癌 “HeLa” 细胞的 EM 图像。该数据集的成像分辨率为 $8192\times
    8192$ 像素，总共 518 切片，包含分布在嵌入树脂背景中的不同 HeLa 细胞。该数据集已公开提供，无标签，主要用于 delineating 结构，如
    plasma membranes 和 nuclear envelopes。
- en: Unlabeled datasets, such as CEM500K, from various unrelated experiments and
    EM modalities for solving the segmentation of a particular structure seem promising.
    The CEM500k (#20) is an EM unlabeled dataset containing around 500,000 images
    from various unrelated experiments and different EM modalities for cellular EM.
    The images from different experiments were standardized to 2D images of size $512\times
    512$ pixels with pixel resolutions ranging from 2 nm in datasets from serial section
    EM and ${\sim}20$ nm for SBF-SEM. The dataset was further filtered by removing
    duplicates and low-quality images in order to provide robustness to changes in
    image contrast and making it suitable for training modeling techniques.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 各种无标签的数据集，如 CEM500K，来自于不同无关实验和 EM 模态，解决特定结构的分割问题似乎很有前景。CEM500k（#20）是一个 EM 无标签数据集，包含大约
    500,000 张来自不同无关实验和不同 EM 模态的细胞 EM 图像。这些来自不同实验的图像被标准化为大小为 $512\times 512$ 像素的 2D
    图像，像素分辨率范围从序列切片 EM 数据集中的 2 nm 到 SBF-SEM 的 ${\sim}20$ nm。该数据集经过进一步过滤，去除了重复和低质量的图像，以增强对图像对比度变化的鲁棒性，并使其适合用于训练建模技术。
- en: 3.4 Cryo-ET datasets
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 冷冻电镜-电子断层扫描数据集
- en: Electron tomography (ET) is used to obtain 3D structures of EM sections using
    the tilt-series acquisition technique. Cryo-ET does so at cryogenic temperatures
    to image vitrified biological samples. Attempts for segmentation on cryo-ET can
    be found by Moussavi et al. [[92](#bib.bib92)] and in the review of Carvalho et al.
    [[20](#bib.bib20)]. The identification of macromoleular structures is beyond the
    scope of this review. Cryo-ET presents challenges in visualizing and interpreting
    tomographic datasets due to two main factors. Firstly, sample thickness increases
    as the tilt angle increases, leading to an artifact known as the “missing wedge”
    and reduced resolution in the $z$-direction. Secondly, vitrified biological samples
    are sensitive to electron dose, resulting in a low signal-to-noise ratio and difficulties
    in distinguishing features of interest from background noise. As the resolution
    capacity of TEM decreases with the increase in sample thickness, focused ion beam
    (FIB) milling can be used to obtain a high-resolution tomogram. Cryo-FIB SEM is
    an evolving technology for cellular imaging that is rapidly being used in recent
    years. This is mainly attributable to its ability to image larger specimens that
    may be too thick for cryo-ET, such as whole cells or tissues.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 电子断层扫描（ET）用于通过倾斜系列采集技术获取 EM 切片的 3D 结构。冷冻 ET 在低温下成像玻璃化的生物样本。关于冷冻 ET 的分割尝试可以在
    Moussavi 等人的研究 [[92](#bib.bib92)] 和 Carvalho 等人的综述 [[20](#bib.bib20)] 中找到。大分子结构的识别超出了本综述的范围。冷冻
    ET 在可视化和解释断层扫描数据集时面临两大挑战。首先，随着倾斜角度的增加，样本厚度增加，导致所谓的“缺失楔”伪影和 $z$ 方向上的分辨率降低。其次，玻璃化生物样本对电子剂量敏感，导致信噪比低，难以从背景噪声中区分出感兴趣的特征。由于样本厚度增加导致
    TEM 分辨率能力下降，可以使用聚焦离子束（FIB）铣削来获得高分辨率的断层扫描图像。冷冻 FIB SEM 是一种用于细胞成像的不断发展的技术，近年来被迅速采用。这主要归功于其成像较大样本的能力，这些样本可能对冷冻
    ET 过厚，如整个细胞或组织。
- en: CDeep3M-C (#21) is a cryo-ET dataset for the segmentation of vesicles and membranes
    from the mouse brain [[53](#bib.bib53)]. At a voxel size of 1.6 nm, it was used
    to digitally recreate a tiny section (approximately $1.5\times 1.5\times 1.5$$\mu
    m^{3}$) of a high-pressure frozen tissue. The final volume was built from 7 sequential
    tomograms (serial sections), each created by tilting a sample every $0.5^{\circ}$
    in an electron beam from $-60^{\circ}to+60^{\circ}$. The cellular cryo-ET dataset
    (#22) was acquired at low magnification for annotation and qualitative cellular
    analysis of organelles like mitochondria, vesicles, microtubules, and plasma membrane
    [[29](#bib.bib29)]. The PC12 cell line was reconstructed using 30 serial sections
    imaged at $850\times 850\times 81$ pixel size at 2.8 nm resolution. The tomograms
    of platelets and cyanobacteria utilized in that work are from previously published
    datasets [[118](#bib.bib118), [35](#bib.bib35)].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CDeep3M-C（#21）是一个用于从小鼠脑中分割囊泡和膜的冷冻电子断层扫描（cryo-ET）数据集[[53](#bib.bib53)]。在1.6纳米的体素尺寸下，该数据集用于数字重建一个高压冷冻组织的微小部分（约$1.5\times
    1.5\times 1.5$$\mu m^{3}$）。最终的体积由7个连续的断层图（串行切片）构建，每个断层图是通过将样品在电子束中每$0.5^{\circ}$倾斜一次从$-60^{\circ}到+60^{\circ}$创建的。细胞冷冻电子断层扫描数据集（#22）是在低放大倍率下获得的，用于注释和定性分析线粒体、囊泡、微管和细胞膜等细胞器[[29](#bib.bib29)]。PC12细胞系使用30个在$850\times
    850\times 81$像素尺寸和2.8纳米分辨率下成像的串行切片进行重建。用于该工作的血小板和蓝细菌的断层图来自之前发布的数据集[[118](#bib.bib118),
    [35](#bib.bib35)]。
- en: 4 Background of backbone deep learning networks for EM semantic and instance
    segmentation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 背景：用于电子显微镜语义和实例分割的骨干深度学习网络
- en: The rapid progress of DL methods, in particular CNNs, has had a great impact
    on advancing segmentation of EM images, as well as other medical images of various
    modalities [[79](#bib.bib79), [108](#bib.bib108)], including light microscopy
    [[124](#bib.bib124), [84](#bib.bib84)]. Deep learning in EM analysis has also
    been addressed in the reviews by Treder et al. [[115](#bib.bib115)] and Ede [[44](#bib.bib44)].
    The former gives a broad overview of different EM applications in both physical
    and life sciences and the latter provides a practitioner’s perspective focused
    on the hardware and software packages to perform DL-based EM analysis. In contrast,
    this review provides an in-depth view of fully/semi/self/un-supervised deep learning
    methods for the semantic and instance segmentation in (sub)cellular EM. This section
    covers the main milestones in the progression of network architectures and their
    key attributes, which are necessary to put in context the 38 papers that are reviewed
    in this work.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法的快速进展，特别是卷积神经网络（CNNs），对推动电子显微镜图像的分割以及其他各种医学图像的进步产生了重大影响[[79](#bib.bib79),
    [108](#bib.bib108)]，包括光学显微镜[[124](#bib.bib124), [84](#bib.bib84)]。电子显微镜分析中的深度学习也在Treder等人的综述[[115](#bib.bib115)]和Ede[[44](#bib.bib44)]中有所涉及。前者对物理科学和生命科学中不同的电子显微镜应用进行了广泛的概述，后者则提供了聚焦于进行基于深度学习的电子显微镜分析的硬件和软件包的实践者视角。相比之下，本综述提供了对（子）细胞电子显微镜中用于语义和实例分割的完全/半监督/自监督/无监督深度学习方法的深入探讨。本节涵盖了网络架构进展中的主要里程碑及其关键特性，这些特性是将本文评审的38篇论文放在背景中进行理解所必需的。
- en: Semantic segmentation in EM images is the identification of objects or subcellular
    organelles in such a way that each pixel is mapped to a specific class. This is
    different than instance segmentation, which refers to the process of dividing
    an image into multiple segments, each corresponding to a unique object or instance.
    Instance segmentation is particularly important in the study of cellular structures
    and their interactions, as it allows for the identification and quantification
    of individual objects in large-scale datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子显微镜图像中的语义分割是指以每个像素映射到特定类别的方式识别对象或亚细胞器。这与实例分割不同，后者是指将图像分割成多个段，每个段对应一个唯一的对象或实例。实例分割在研究细胞结构及其相互作用中尤为重要，因为它允许在大规模数据集中识别和量化单个对象。
- en: '![Refer to caption](img/d410d6fcac583ee1641a6d016b2a45ca.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d410d6fcac583ee1641a6d016b2a45ca.png)'
- en: 'Figure 3: Encoder-decoder networks of the original works on FCN [[85](#bib.bib85)]
    and U-Net [[105](#bib.bib105)]. Each of the fully connected (fc) layers in FCN
    and convolutional layers in U-Net are followed by the nonlinear activation function
    ReLU and max pooling. In FCN, the fully connected layers are then converted to
    convolutional layers via the ‘fc to conv’ component. The last layer uses a softmax
    function to assign a probability class score to each pixel. The FCN decoder includes
    an upsampling component that is linearly combined with the low-level feature maps
    in the third convolutional layer of the encoder. The sizes of these feature maps
    are four times less than the size of the input image $I$ (denoted by $I/4$). Finally,
    there is a direct upsampling from $I/4$ to the original size of $I$ followed by
    softmax for classification. The symmetrical U-Net architecture shares the features
    maps in the encoder with the decoder path together with skip connections.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：原始工作中的编码器-解码器网络 FCN [[85](#bib.bib85)] 和 U-Net [[105](#bib.bib105)]。在 FCN
    中，每个全连接（fc）层和 U-Net 中的卷积层后面都跟有非线性激活函数 ReLU 和最大池化。在 FCN 中，全连接层通过“fc to conv”组件转换为卷积层。最后一层使用
    softmax 函数为每个像素分配一个概率类得分。FCN 解码器包括一个上采样组件，该组件与编码器第三个卷积层中的低级特征图线性组合。这些特征图的尺寸是输入图像
    $I$ 尺寸的四分之一（表示为 $I/4$）。最后，从 $I/4$ 直接上采样到原始尺寸 $I$，然后进行 softmax 分类。对称的 U-Net 结构在编码器和解码器路径中共享特征图，并带有跳跃连接。
- en: The CNN designed by Ciresan et al. [[33](#bib.bib33)], for instance, was used
    for the semantic segmentation of neuronal membranes in stacks of EM images. The
    images were segmented by predicting the label of each local region or patch covered
    by a convolutional filter in a sliding window approach and introduced max-pooling
    layers instead of sub-sampling layers. As indicated by Arganda-Carreras et al.
    [[4](#bib.bib4)], it led to winning the ISBI 2012 neuronal segmentation challenge⁵⁵5[https://imagej.net/events/isbi-2012-segmentation-challenge](https://imagej.net/events/isbi-2012-segmentation-challenge).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Ciresan 等人设计的 CNN [[33](#bib.bib33)] 被用于堆叠 EM 图像中神经膜的语义分割。这些图像通过预测每个局部区域或块的标签来进行分割，该区域或块由滑动窗口方法中的卷积滤波器覆盖，并引入了最大池化层而不是子采样层。正如
    Arganda-Carreras 等人 [[4](#bib.bib4)] 所指出的，这一方法赢得了 ISBI 2012 神经分割挑战赛⁵⁵5[https://imagej.net/events/isbi-2012-segmentation-challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)。
- en: Despite its success, the method suffered from two major limitations - firstly,
    the sliding window approach was slow due to the redundancy of processing large
    overlaps between adjacent patches, and secondly, there was a trade-off between
    the size of the patches (context) and localization accuracy. Since the network’s
    depth is an important factor for a larger receptive field (the size of the viewing
    field from which the network receives information), larger patches require deeper
    networks. Localization ability, however, decreases with deeper networks due to
    downsampling by the many max pooling layers and the use of smaller patches allows
    the network to see only a little context.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该方法取得了成功，但仍存在两个主要限制 - 首先，滑动窗口方法由于处理相邻块之间的大重叠部分的冗余而较慢，其次，块的大小（上下文）与定位精度之间存在权衡。由于网络的深度是获得更大感受野（网络接收信息的视野大小）的重要因素，因此更大的块需要更深的网络。然而，由于许多最大池化层的下采样，随着网络深度的增加，定位能力降低，而使用较小的块允许网络仅看到很少的上下文。
- en: 'Improvements in the semantic segmentation of EM images continued with the development
    of the Fully Convolutional Network (FCN) [[85](#bib.bib85)] and the U-Net architecture
    [[105](#bib.bib105)], Fig. [3](#S4.F3 "Figure 3 ‣ 4 Background of backbone deep
    learning networks for EM semantic and instance segmentation ‣ Segmentation in
    large-scale cellular electron microscopy with deep learning: A literature survey").
    The concept of expanding a CNN to handle inputs of any size using fully convolutional
    layers instead of fully connected ones helped evolve dense predictions for segmentation.
    A skip architecture was introduced to make use of a feature spectrum that merges
    deep, coarse, semantic information with shallow, fine, appearance information.
     The U-Net extended an FCN network with a U-shaped topology to optimize the tradeoff
    between localization and context. The contracting path (encoder) captures a larger
    context using the downsampled features and the expanding path (decoder) upsamples
    features to their original size with the same number of layers making it a symmetric
    or U-shaped network. The skip connections between the encoder-decoder layers bypass
    some of the neural network layers and as a result, an alternative and shorter
    path is provided for backpropagating the error of the loss function, which contributed
    to avoiding the vanishing gradient problem [[71](#bib.bib71)]. Increased connectivity
    in the upsampling path within FCNs and the consideration of multi-level contexts
    were key to improving semantic segmentation [[5](#bib.bib5), [42](#bib.bib42)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'EM 图像的语义分割技术随着全卷积网络（FCN）[[85](#bib.bib85)]和 U-Net 架构的开发而不断改进[[105](#bib.bib105)]，如图
    [3](#S4.F3 "Figure 3 ‣ 4 Background of backbone deep learning networks for EM
    semantic and instance segmentation ‣ Segmentation in large-scale cellular electron
    microscopy with deep learning: A literature survey") 所示。将 CNN 扩展为处理任意大小输入的全卷积层而不是全连接层的概念，推动了用于分割的密集预测的发展。引入了跳跃架构以利用特征谱，将深层、粗糙的语义信息与浅层、细致的外观信息进行融合。U-Net
    将 FCN 网络扩展为 U 形拓扑，以优化定位与上下文之间的权衡。收缩路径（编码器）使用下采样特征捕获更大上下文，而扩展路径（解码器）则将特征上采样到原始大小，并保持相同数量的层，从而使其成为对称或
    U 形网络。编码器-解码器层之间的跳跃连接绕过了一些神经网络层，因此为误差的反向传播提供了替代且较短的路径，这有助于避免梯度消失问题[[71](#bib.bib71)]。在
    FCNs 中增加了上采样路径的连通性并考虑了多层级上下文，这些都是提高语义分割精度的关键[[5](#bib.bib5), [42](#bib.bib42)]。'
- en: DeepLab is another family of semantic segmentation networks, which have the
    ability to achieve robustness for different scales without increasing computational
    complexity [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)].
    DeepLab architectures are based on FCNs but extended with the use of dilated (or
    atrous) convolutions, which were originally proposed by [[126](#bib.bib126)],
    and image-level features. The atrous dilations are used within Atrous Spatial
    Pyramid Modules (ASPP), which perform multi-scale feature extraction by using
    multiple atrous convolutions with different dilation rates. As a backbone network,
    the latest DeepLab architecture, namely DeepLab v3+, uses the Residual Neural
    Network (ResNet) to produce image-level feature maps. The module performs parallel
    convolution on the feature map obtained from the ResNet backbone and outputs multiple
    feature maps, which are then concatenated and fed into the next layer. This allows
    the network to capture features of multiple scales, which is crucial for tasks
    like semantic segmentation. ResNet is notable for its ability to overcome the
    vanishing gradient problem and the degradation issue, simultaneously [[58](#bib.bib58)].
    This breakthrough was attributable to the introduction of residual connections,
    which allow the network to learn residual functions, or the difference between
    the desired output and the current output, rather than the full function. This
    helps the network to learn more effectively and avoid overfitting.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLab是另一类语义分割网络，它们具有在不增加计算复杂度的情况下实现对不同尺度的鲁棒性的能力[[25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]。DeepLab架构基于FCNs，但扩展使用了膨胀（或空洞）卷积，这些卷积最初由[[126](#bib.bib126)]提出，并使用了图像级特征。空洞膨胀用于空洞空间金字塔模块（ASPP）中，通过使用不同膨胀率的多个空洞卷积进行多尺度特征提取。作为骨干网络，最新的DeepLab架构，即DeepLab
    v3+，使用残差神经网络（ResNet）生成图像级特征图。该模块对从ResNet骨干网络获得的特征图进行并行卷积，输出多个特征图，然后将这些特征图连接在一起并输入到下一层。这使得网络能够捕捉多个尺度的特征，这对于语义分割等任务至关重要。ResNet因其克服梯度消失问题和降级问题的能力而受到关注[[58](#bib.bib58)]。这一突破归功于引入了残差连接，使网络能够学习残差函数，或期望输出与当前输出之间的差异，而不是整个函数。这有助于网络更有效地学习并避免过拟合。
- en: The 3D segmentation of neuronal stacks was set as a challenge in ISBI 2013\.
    The major challenges in analyzing volume EM datasets are misalignments or missing
    sections due to serial sectioning, and volume anisotropy due to different resolutions
    in different directions. Specifically, it refers to the situation where the resolution
    in the $z$-axis (the depth dimension) is lower than the resolution in the $x-y$
    plane.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元堆叠的3D分割被设定为ISBI 2013的一个挑战。分析体积电子显微镜数据集的主要挑战是由于串联切片而导致的错位或缺失部分，以及由于不同方向的分辨率不同而产生的体积各向异性。具体而言，它指的是在$z$轴（深度维度）上的分辨率低于$x-y$平面上的分辨率的情况。
- en: There are three typical approaches for the analysis of 3D volumes. The first
    involves 2D segmentation of each image in the stack, followed by 3D reconstructions
    based on clustering techniques, that may range from basic watershed to complex
    graph cuts algorithms. The second approach is based on 3D CNNs, which can learn
    representations of volumetric data that include 3D spatial context. One example
    of such 3D CNNs is the 3D U-Net by Çiçek et al. [[32](#bib.bib32)], which was
    inspired by the original U-Net that uses local and larger contextual information.
    It was then extended into the V-Net model by Milletari et al. [[91](#bib.bib91)]
    by adding residual stages. The HighRes3DNet is another 3D CNN based on the FCN
    architecture, with dilated and residual convolutions, and has been successful
    in obtaining accurate segmentations of neuronal mitochondria [[77](#bib.bib77)].
    In terms of performance, both HighRes3DNet and V-Net have achieved state-of-the-art
    results on several medical image segmentation benchmarks. However, HighRes3DNet
    has been shown to have better performance on tasks involving high-resolution and
    multi-modal medical images, while V-Net has been shown to be more efficient in
    terms of computational resources and memory usage. A variant of the 3D network
    is the hybrid 2D-3D methodology as proposed by Lee et al. [[74](#bib.bib74)] for
    the segmentation of anisotropic volumes. They utilize only 2D convolutions in
    the initial layers that downsample the input feature maps with high $x-y$ resolution
    (independent of the $z$-axis) until they are roughly isotropic to be efficiently
    processed by 3D convolutions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 3D 体积的分析，有三种典型方法。第一种方法涉及对每张堆叠图像进行 2D 分割，然后基于聚类技术进行 3D 重建，这些技术可能从基本的水shed算法到复杂的图切割算法。第二种方法基于
    3D CNNs，它们可以学习包括 3D 空间上下文的体积数据表示。一个例子是Çiçek等人提出的 3D U-Net [[32](#bib.bib32)]，它受到原始
    U-Net 的启发，后者使用局部和更大的上下文信息。随后，Milletari 等人 [[91](#bib.bib91)] 通过添加残差阶段将其扩展为 V-Net
    模型。HighRes3DNet 是另一个基于 FCN 架构的 3D CNN，具有扩张和残差卷积，并且在获得神经线粒体的准确分割方面取得了成功 [[77](#bib.bib77)]。就性能而言，HighRes3DNet
    和 V-Net 在多个医学图像分割基准测试中都取得了最先进的结果。然而，HighRes3DNet 在涉及高分辨率和多模态医学图像的任务中表现更佳，而 V-Net
    在计算资源和内存使用方面则更为高效。3D 网络的一种变体是 Lee 等人 [[74](#bib.bib74)] 提出的混合 2D-3D 方法，用于各向异性体积的分割。他们在初始层中仅使用
    2D 卷积，这些卷积将具有高 $x-y$ 分辨率（与 $z$ 轴无关）的输入特征图下采样，直到它们大致各向同性，以便能够高效地通过 3D 卷积处理。
- en: Graph analysis is the third approach for 3D segmentation. Graph-based methods
    typically involve partitioning a graph into regions or clusters based on properties
    such as color or intensity values, edge strength, or other image features such
    as shape. These methods often use graph theory algorithms, like graph cuts or
    minimum spanning trees, to identify regions that are distinct from one another.
    This may be coupled with structure-based analysis that uses certain geometrical
    properties to identify boundaries between objects. Global shape descriptors were
    used to learn the connectivity of 3D super voxels by Lucchi et al. [[86](#bib.bib86)]
    for segmentation using graph-cuts, addressing issues with local statistics and
    distracting membranes. Turaga et al. [[116](#bib.bib116)] suggested how CNNs can
    be used for directly predicting 3D graph affinities based on a structured loss
    function for neuronal boundary segmentation. The proposed loss function assigned
    scores to the edges between adjacent pixels based on their likelihood of belonging
    to same or different regions and also penalized their assignment for achieving
    incorrect predictions that violate the underlying structure of the image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图分析是 3D 分割的第三种方法。基于图的方法通常涉及将图划分为基于颜色或强度值、边缘强度或其他图像特征（如形状）的区域或簇。这些方法通常使用图论算法，如图切割或最小生成树，来识别彼此不同的区域。这可能与结构基础分析结合使用，利用某些几何属性来识别对象之间的边界。Lucchi
    等人 [[86](#bib.bib86)] 使用全局形状描述符来学习 3D 超体素的连通性，用于基于图切割的分割，解决了局部统计数据和干扰膜的问题。Turaga
    等人 [[116](#bib.bib116)] 提出了如何使用 CNNs 直接预测基于结构化损失函数的 3D 图亲和性，用于神经边界分割。所提出的损失函数根据相邻像素之间属于相同或不同区域的可能性为边分配分数，同时惩罚其分配，以实现违背图像底层结构的错误预测。
- en: Instance segmentation involves classifying each pixel/voxel of a given image/volume
    to a particular class along with assigning a unique identity to pixels/voxels
    of individual objects. Instance segmentation using deep learning can be divided
    into proposal-based (top-down) and proposal-free (bottom-up) approaches. Proposal-based
    approaches such as RCNN, FastRCNN, and FasterRCNN are two-stage detection networks
    that use a deep neural network for feature extraction (encoder) and region proposals
    for the segmentation of objects of interest, followed by bounding box regression
    and classification to obtain instance segmentation [[81](#bib.bib81)]. Mask-RCNN
    [[56](#bib.bib56)] is a popular choice for generic object instance segmentation
    built upon FasterRCNN, which uses a branch of the network to predict a binary
    mask for each object instance. Top-down instance segmentation has also been accomplished
    using recurrent networks with attention mechanisms, either by extracting visual
    characteristics and producing instance labels one item at a time or by guiding
    the formation of bounding boxes followed by a segmentation network [[103](#bib.bib103),
    [50](#bib.bib50)]. The Flood Filling Network (FFN) uses this concept to obtain
    individual object masks directly from raw image pixels [[63](#bib.bib63)] and
    has also been used for EM segmentation as reviewed below.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割涉及将给定图像/体积的每个像素/体素分类到特定类别，并为每个物体的像素/体素分配唯一身份。使用深度学习的实例分割可以分为基于提议（自上而下）和无提议（自下而上）的方法。基于提议的方法，如RCNN、FastRCNN和FasterRCNN，是两阶段检测网络，使用深度神经网络进行特征提取（编码器）和区域提议进行目标物体的分割，随后进行边界框回归和分类以获得实例分割[[81](#bib.bib81)]。Mask-RCNN[[56](#bib.bib56)]是基于FasterRCNN的一种流行的通用物体实例分割选择，它利用网络的一个分支来预测每个物体实例的二进制掩膜。自上而下的实例分割也可以通过带有注意力机制的递归网络实现，要么通过提取视觉特征并逐项生成实例标签，要么通过引导边界框的形成，随后由分割网络完成[[103](#bib.bib103),
    [50](#bib.bib50)]。Flood Filling Network（FFN）利用这一概念直接从原始图像像素中获取单独的物体掩膜[[63](#bib.bib63)]，并且也被用于EM分割，如下文所述。
- en: The other approach is known as proposal-free, which aims to combine semantic
    and instance segmentation in a bottom-up approach. This was the strategy taken
    by Chen et al. [[24](#bib.bib24)], where the prediction of contours/edges of objects
    along with semantic masks were incorporated into FCNs in a multi-task learning
    approach. Both contour/edge maps and semantic masks were then fused to obtain
    the instance segmentation maps. Other approaches use boundary-aware instance information
    (e.g. the distance between object boundaries or the amount of overlap between
    objects) to fuse edge features with intermediate layers of the network [[6](#bib.bib6),
    [93](#bib.bib93)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法被称为无提议（proposal-free），旨在通过自下而上的方法结合语义和实例分割。这是陈等人[[24](#bib.bib24)]采用的策略，其中物体的轮廓/边缘预测与语义掩膜一起被整合到FCNs中，形成多任务学习的方法。然后，将轮廓/边缘图和语义掩膜融合以获得实例分割图。其他方法使用边界感知实例信息（例如，物体边界之间的距离或物体之间的重叠量）来将边缘特征与网络的中间层融合[[6](#bib.bib6),
    [93](#bib.bib93)]。
- en: 'Semantic instance segmentation is another family of techniques that addresses
    instance segmentation with semantic-based approaches. Instead of inferring for
    each pixel the probability of belonging to a certain class, they infer the probability
    of belonging to a certain instance of a class. In fact, De Brabandere et al. [[36](#bib.bib36)]
    proposed a discriminative loss function in this regard and demonstrated that it
    is superior than the cross-entropy and Dice loss function for instance segmentation.
    The discriminative loss function consists of three terms: a segmentation term,
    which penalizes incorrect class predictions; a boundary term, which penalizes
    incorrect boundary predictions; and a regularization term, which encourages smoothness
    in the predicted masks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语义实例分割是另一类技术，采用基于语义的方法来解决实例分割问题。它们不是为每个像素推断属于某个类别的概率，而是推断属于某个类别的实例的概率。事实上，De
    Brabandere等人[[36](#bib.bib36)]提出了一种判别损失函数，并证明其优于交叉熵和Dice损失函数在实例分割中的表现。判别损失函数由三项组成：分割项，用于惩罚错误的类别预测；边界项，用于惩罚错误的边界预测；以及正则化项，用于鼓励预测掩膜的平滑性。
- en: 5 Fully supervised methods
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 完全监督的方法
- en: 'Fully supervised methods use annotated images (training data) to learn computational
    models that can segment structures in unseen images from similar distributions
    (test data). The training set is used by the algorithm to determine the model’s
    parameters in such a way as to maximize the model’s generalization ability. Table [3](#S5.T3
    "Table 3 ‣ 5.1.1 Approaches based on 2D CNNs ‣ 5.1 End-to-end learning - semantic
    segmentation ‣ 5 Fully supervised methods ‣ Segmentation in large-scale cellular
    electron microscopy with deep learning: A literature survey") summarizes the 33
    papers (of the 38) that have used supervised learning for the semantic and instance
    segmentation of (sub) cellular structures.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '完全监督的方法使用标注的图像（训练数据）来学习可以在未见过的类似分布的图像（测试数据）中分割结构的计算模型。训练集被算法用来确定模型的参数，以最大化模型的泛化能力。表 [3](#S5.T3
    "Table 3 ‣ 5.1.1 Approaches based on 2D CNNs ‣ 5.1 End-to-end learning - semantic
    segmentation ‣ 5 Fully supervised methods ‣ Segmentation in large-scale cellular
    electron microscopy with deep learning: A literature survey") 总结了33篇（共38篇）使用监督学习进行（子）细胞结构的语义和实例分割的论文。'
- en: 5.1 End-to-end learning - semantic segmentation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 端到端学习 - 语义分割
- en: End-to-end learning is a machine learning approach where a single model learns
    to perform a task without relying on pre-defined intermediate steps or features.
    Instead, the model is trained to map the input data directly to the desired output,
    in a single end-to-end process. End-to-end learning has become increasingly popular
    in recent years due to advances in deep learning, which allow the creation of
    models with large numbers of layers that can learn complex representations of
    data. These models are trained using backpropagation, a method for updating the
    weights of the model based on the error generated by a given loss function between
    the predicted output and the true output, which allows the model to improve its
    performance over time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端学习是一种机器学习方法，其中一个模型学习执行一个任务，而不依赖于预定义的中间步骤或特征。相反，模型被训练来将输入数据直接映射到期望的输出，这在一个端到端的过程中完成。由于深度学习的进步，端到端学习在近年来变得越来越受欢迎，这些进步允许创建具有大量层的模型，能够学习数据的复杂表示。这些模型通过反向传播进行训练，反向传播是一种基于预测输出与真实输出之间的损失函数产生的误差来更新模型权重的方法，从而使模型能够随着时间的推移改善其性能。
- en: The 16 papers that fall within this category are focused on the semantic segmentation
    of two main cellular structures, namely NM - neuronal membranes (8 papers) and
    M - mitochondria (5 papers). Other structures include N - nuclei, NE - nuclear
    envelopes, and L - lysosome.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别中的16篇论文集中于两种主要的细胞结构的语义分割，即NM - 神经膜（8篇论文）和M - 线粒体（5篇论文）。其他结构包括N - 细胞核，NE
    - 核膜和L - 溶酶体。
- en: Neuronal membrane segmentation refers to the process of identifying and separating
    the neuronal membrane from other structures in an EM image. Segmenting neuronal
    membranes in EM volumes helps partition an image into distinct regions that represent
    different neuronal cells and processes. It is essential for studying the function
    of neurons along with their synaptic connections for understanding the different
    signaling pathways in the brain. Digital reconstruction or tracing of 3D neurons
    depends on the accuracy of neuronal membrane segmentation as discontinuities could
    lead to merge and split errors which in turn affect the reconstruction.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 神经膜分割是指在电子显微镜图像中识别和分离神经膜与其他结构的过程。在电子显微镜体积中分割神经膜有助于将图像划分为表示不同神经细胞和过程的区域。这对于研究神经元的功能以及它们的突触连接以理解大脑中的不同信号传导路径至关重要。3D神经元的数字重建或追踪依赖于神经膜分割的准确性，因为断裂可能导致合并和分裂错误，从而影响重建的质量。
- en: Similarly, mitochondria segmentation is the process of identifying and separating
    mitochondria, a type of organelle found in eukaryotic cells, from other structures
    in an EM image. Mitochondria segmentation is a challenging task due to the variability
    in their size, shape, and distribution within cells. Accurately segmenting mitochondria
    in 2D and 3D is important for studying the structure and function of these organelles,
    as well as investigating their role in various diseases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，线粒体分割是指在电子显微镜图像中识别和分离线粒体这一类真核细胞中的细胞器，与其他结构进行区分。由于线粒体的大小、形状和分布在细胞内存在变异，因此线粒体分割是一项具有挑战性的任务。准确分割2D和3D中的线粒体对于研究这些细胞器的结构和功能以及探究它们在各种疾病中的作用至关重要。
- en: Below we categorize the proposed approaches based on their underlying 2D or
    3D CNN architectures.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们根据其基础2D或3D CNN架构对提出的方法进行分类。
- en: 5.1.1 Approaches based on 2D CNNs
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 基于2D CNN的方法
- en: 'Table 3: The list of 33 (out of 38) papers reviewed in this work that are based
    on fully supervised learning frameworks with 2D and 3D CNN architectures applied
    to both semantic and instance segmentation. The abbreviation Org. stands for the
    studied organelle/s. The Type (2D and/or 3D) column indicates the type of methods
    used and problems addressed. The studies that are marked as both 2D and 3D use
    a 2D backbone method coupled with some post-processing operations for 3D reconstruction.
    The other studies that are flagged as 2D or 3D only, use 2D or 3D only backbones
    to address 2D or 3D problems, respectively. The numbers in the Datasets column
    serve as correspondences to the identifiers in Table [2](#S3.T2 "Table 2 ‣ 3.1
    Serial section TEM and SEM datasets ‣ 3 Collections of key EM datasets ‣ Segmentation
    in large-scale cellular electron microscopy with deep learning: A literature survey"),
    and the definitions of the performance metrics are presented in Section [7](#S7
    "7 Segmentation evaluation metrics ‣ Segmentation in large-scale cellular electron
    microscopy with deep learning: A literature survey").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：本研究中回顾的33篇（共38篇）基于完全监督学习框架的文献列表，这些文献使用了2D和3D CNN架构应用于语义分割和实例分割。缩写Org.代表研究的细胞器。Type（2D和/或3D）列表示所使用的方法类型和解决的问题。标记为2D和3D的研究使用了2D主干方法，并结合了一些后处理操作进行3D重建。标记为2D或3D的研究仅使用2D或3D主干来解决2D或3D问题。Datasets列中的数字对应于表[2](#S3.T2
    "表2 ‣ 3.1 序列TEM和SEM数据集 ‣ 3 个关键EM数据集 ‣ 大规模细胞电子显微镜中的分割：文献综述")中的标识符，性能指标的定义见第[7](#S7
    "7 分割评估指标 ‣ 大规模细胞电子显微镜中的分割：文献综述")节。
- en: '| Citation | Org. | Type | Datasets | Performance | Backbone | Main methodological
    components |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 组织 | 类型 | 数据集 | 性能 | 主干 | 主要方法组件 |'
- en: '|  |  | 2D | 3D |  | metrics |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2D | 3D |  | 指标 |  |  |'
- en: '| End-to-end learning - semantic segmentation |  |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: 端到端学习 - 语义分割 |  |  |
- en: '| [[46](#bib.bib46)] | NM |  |  | 1, 3 | RE, WE, PE | 2D U-Net | Residual blocks,
    deconvolutions |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [[46](#bib.bib46)] | NM |  |  | 1, 3 | RE, WE, PE | 2D U-Net | 残差块、反卷积 |'
- en: '| [[94](#bib.bib94)] | M |  |  | 1 | Acc, P, R, F1, JI | 2D FCN | Block processing,
    Z-filtering |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | M |  |  | 1 | Acc, P, R, F1, JI | 2D FCN | 块处理、Z滤波 |'
- en: '| [[29](#bib.bib29)] | MT, M, PM, V |  |  | 22 | No evaluation | 2D FCN | A
    CNN architecture with four layers |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] | MT, M, PM, V |  |  | 22 | 无评估 | 2D FCN | 四层CNN架构 |'
- en: '| [[123](#bib.bib123)] | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | 2D FCN | Residual
    blocks, multi-level features |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#bib.bib123)] | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | 2D FCN | 残差块、多级特征
    |'
- en: '| [[21](#bib.bib21)] | M |  |  | 4, 9 | Acc, P, R, JI | 2D U-Net | Few parameters,
    light-weight model |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [[21](#bib.bib21)] | M |  |  | 4, 9 | Acc, P, R, JI | 2D U-Net | 参数较少，轻量级模型
    |'
- en: '| [[64](#bib.bib64)] | N |  |  | Private^∗ | JI, Acc | 2D FCN | Residual, atrous,
    multi-level fusion |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] | N |  |  | 私有^∗ | JI, Acc | 2D FCN | 残差、空洞、多级融合 |'
- en: '| [[18](#bib.bib18)] | NM |  |  | 1 | $V_{rand}$ | 2D U-Net | Dense blocks,
    summation-skip |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [[18](#bib.bib18)] | NM |  |  | 1 | $V_{rand}$ | 2D U-Net | 密集块、求和跳跃 |'
- en: '| [[101](#bib.bib101)] | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | 2D U-Net |
    Residual, summation-skip, multi-stage |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | 2D U-Net |
    残差、求和跳跃、多阶段 |'
- en: '| [[110](#bib.bib110)] | NE |  |  | 19 | P, R, F1 | 2D U-Net | Tri-axis prediction
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | NE |  |  | 19 | P, R, F1 | 2D U-Net | 三轴预测 |'
- en: '| [[30](#bib.bib30)] | M |  |  | 8 | P, R, JI | 3D U-Net | Factorised convolutions
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [[30](#bib.bib30)] | M |  |  | 8 | P, R, JI | 3D U-Net | 因式分解卷积 |'
- en: '| [[75](#bib.bib75)] | NM |  |  | 3 | RE | 3D U-Net | 3D graph affinity, hybrid
    2D-3D, residual |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [[75](#bib.bib75)] | NM |  |  | 3 | RE | 3D U-Net | 3D图谱亲和、混合2D-3D、残差 |'
- en: '| [[122](#bib.bib122)] | M |  |  | 5, 8 | JI, DSC | 3D U-Net | Hybrid 2D-3D,
    residual, auxiliary supervision |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bib122)] | M |  |  | 5, 8 | JI, DSC | 3D U-Net | 混合2D-3D、残差、辅助监督
    |'
- en: '| [[49](#bib.bib49)] | NM |  |  | 2, 10, 15 | $V_{info}$, CREMI | 3D U-Net
    | 3D graph affinity prediction |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [[49](#bib.bib49)] | NM |  |  | 2, 10, 15 | $V_{info}$, CREMI | 3D U-Net
    | 3D图谱亲和预测 |'
- en: '| [[60](#bib.bib60)] | NM |  |  | 2 | CREMI | 3D U-Net | Signed distance regression
    map, hybrid 2D-3D |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | NM |  |  | 2 | CREMI | 3D U-Net | 签名距离回归图、混合2D-3D |'
- en: '| [[90](#bib.bib90)] | M, L |  |  | 13 | TNR, R, DSC | 3D FCN | HighRes3DZMNet,
    zero-mean, residual/atrous |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] | M, L |  |  | 13 | TNR, R, DSC | 3D FCN | HighRes3DZMNet、零均值、残差/空洞
    |'
- en: '| [[59](#bib.bib59)] | Many |  |  | 10 | DSC | 3D U-Net | Multi-class segmentation
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | 多 |  |  | 10 | DSC | 3D U-Net | 多类别分割 |'
- en: '| [[7](#bib.bib7)] | NM |  |  | 2 | ARAND | 3D U-Net | Signed 3D graph affinity
    prediction |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [[7](#bib.bib7)] | NM |  |  | 2 | ARAND | 3D U-Net | 带符号的3D图形亲和力预测 |'
- en: '| End-to-end learning - instance segmentation |  |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: 端到端学习 - 实例分割 |  |  |
- en: '| [[80](#bib.bib80)] | M |  |  | 8 | Acc, P, R, JI, DSC | Mask-RCNN | Recursive
    network, multiple bounding boxes |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [[80](#bib.bib80)] | M |  |  | 8 | 准确率，P，R，JI，DSC | Mask-RCNN | 递归网络，多重边界框
    |'
- en: '| [[127](#bib.bib127)] | M |  |  | 4, 8 | JI, DSC, AJI, PQ | 2D U-Net | Hierarchical
    view ensemble module, multi-task |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [[127](#bib.bib127)] | M |  |  | 4, 8 | JI, DSC, AJI, PQ | 2D U-Net | 分层视图集成模块，多任务
    |'
- en: '| [[88](#bib.bib88)] | M |  |  | 4, 8 | JI, DSC, AJI, PQ | 2D U-Net | Residual
    blocks, two-stage, shape soft-labels |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | M |  |  | 4, 8 | JI, DSC, AJI, PQ | 2D U-Net | 残差块，两阶段，形状软标签
    |'
- en: '| [[119](#bib.bib119)] | M |  |  | 6, 8 | JI, AP-75 | 3D U-Net | Mask, contour
    prediction, watershed |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | M |  |  | 6, 8 | JI, AP-75 | 3D U-Net | 掩膜，轮廓预测，分水岭
    |'
- en: '| [[1](#bib.bib1)] | A, N |  |  | 17 | $V_{info}$, ARAND | 3D U-Net | Shape-based
    postprocessing |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [[1](#bib.bib1)] | A, N |  |  | 17 | $V_{info}$, ARAND | 3D U-Net | 基于形状的后处理
    |'
- en: '| [[78](#bib.bib78)] | N |  |  | 7 | AP-50, AP-75, AP | 3D U-Net | Hybrid 2D-3D
    module, residual blocks |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [[78](#bib.bib78)] | N |  |  | 7 | AP-50, AP-75, AP | 3D U-Net | 混合2D-3D模块，残差块
    |'
- en: '| [[76](#bib.bib76)] | M |  |  | 6 | JI, DSC, AP | 3D FCN | Hybrid 2D-3D module,
    multi-scale |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] | M |  |  | 6 | JI, DSC, AP | 3D FCN | 混合2D-3D模块，多尺度 |'
- en: '| [[89](#bib.bib89)] | M |  |  | 13 | TPR, TNR, JI, DSC | 3D U-Net | HighRes3DzNet,
    geodesic active contours |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] | M |  |  | 13 | TPR, TNR, JI, DSC | 3D U-Net | HighRes3DzNet，地质主动轮廓
    |'
- en: '| Ensemble learning - semantic segmentation |  |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: 集成学习 - 语义分割 |  |  |
- en: '| [[128](#bib.bib128)] | NM |  |  | 3 | RE | 3D U-Net | Hybrid 3D-2D, residual/inception/atrous
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | NM |  |  | 3 | RE | 3D U-Net | 混合3D-2D，残差/卷积/空洞 |'
- en: '| [[54](#bib.bib54)] | NM, M, N, V |  |  | 18, 21 | A, P, R, F-1 | 3D U-Net
    | Hybrid 3D-2D, residual/inception/atrous |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [[54](#bib.bib54)] | NM, M, N, V |  |  | 18, 21 | A, P, R, F-1 | 3D U-Net
    | 混合3D-2D，残差/卷积/空洞 |'
- en: '| [[52](#bib.bib52)] | C, M, GN |  |  | 16 | Mean JI | 3D U-Net | Hybrid 2D-3D,
    spatial pyramids |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] | C, M, GN |  |  | 16 | 平均 JI | 3D U-Net | 混合2D-3D，空间金字塔
    |'
- en: '| [[68](#bib.bib68)] | M |  |  | 12, 18 | Acc, TPR, TNR, F1, JI, $V_{rand}$,
    $V_{info}$ | 2D U-Net | Ensemble of different networks |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[68](#bib.bib68)] | M |  |  | 12, 18 | 准确率，TPR，TNR，F1，JI，$V_{rand}$，$V_{info}$
    | 2D U-Net | 不同网络的集成 |'
- en: '| Transfer learning - semantic segmentation |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: 转移学习 - 语义分割 |  |  |
- en: '| [[37](#bib.bib37)] | M |  |  | 1 | Acc, P, F1 | VGG | Few shot, hypercolumn
    features, boosting |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bib37)] | M |  |  | 1 | 准确率，P，F1 | VGG | 少量样本，超列特征，增强 |'
- en: '| [[11](#bib.bib11)] | M |  |  | Private^∗ | JI | 2D U-Net | Deep domain adaptation,
    two-stream U-Net |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[11](#bib.bib11)] | M |  |  | Private^∗ | JI | 2D U-Net | 深度领域适应，两流U-Net
    |'
- en: '| Configurable networks - semantic segmentation |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 可配置网络 - 语义分割 |  |  |'
- en: '| [[62](#bib.bib62)] | NM |  |  | 2 | Acc, P, F1 | 2D, 3D U-Net | nn U-Net,
    self-configuring method |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bib62)] | NM |  |  | 2 | 准确率，P，F1 | 2D，3D U-Net | nn U-Net，自配置方法
    |'
- en: '| [[47](#bib.bib47)] | M |  |  | 4, 8 | JI | 2D, 3D U-Net | Stable networks,
    blended output, $z$-filtering |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bib47)] | M |  |  | 4, 8 | JI | 2D, 3D U-Net | 稳定网络，混合输出，$z$-滤波
    |'
- en: ^∗Private indicates that the dataset used is not publicly available.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗“Private” 表示所使用的数据集不可公开获取。
- en: Successes of DL networks for segmentation in EM were achieved using 2D architectures
    with deep contextual networks. Those networks typically had FCN or U-Net as backbones.
    Deeper contextual networks have generally produced better 2D segmentations that
    mostly allowed doing away with multi-step post-processing for obtaining 2D segmentations
    and 3D reconstructions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络在电子显微镜分割中的成功是使用了具有深度上下文网络的2D架构。这些网络通常以FCN或U-Net作为骨干网。更深的上下文网络通常产生了更好的2D分割，这主要使得能够摆脱多步骤后处理以获得2D分割和3D重建。
- en: Residual Deconvolutional Networks (RDN) by Fakhry et al. [[46](#bib.bib46)]
    are based on a combination of residual connections, which allow for the efficient
    training of deep networks, and deconvolutional layers in the decoder of 2D U-Net,
    which help to recover spatial information lost during downsampling. The proposed
    method was evaluated on the ISBI 2012 (Drosophila VNC) and 2013 (SNEMI3D) benchmark
    datasets and compared to several state-of-the-art segmentation methods. The results
    demonstrated that RDNs were superior in terms of segmentation accuracy and required
    a simple post-processing step such as watershed to segment/reconstruct neural
    circuits.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Fakhry 等人 [[46](#bib.bib46)] 提出的残差反卷积网络 (RDN) 基于残差连接的组合，这使得深度网络的高效训练成为可能，以及
    2D U-Net 解码器中的反卷积层，这有助于恢复在下采样过程中丢失的空间信息。所提出的方法在 ISBI 2012（果蝇 VNC）和 2013（SNEMI3D）基准数据集上进行了评估，并与几种最先进的分割方法进行了比较。结果表明，RDN
    在分割准确性方面优于其他方法，并且只需要一个简单的后处理步骤，如 watershed，来分割/重建神经回路。
- en: Oztel et al. [[94](#bib.bib94)] proposed using a median filtering approach to
    incorporate 3D context for the reconstruction of mitochondria from the output
    of 2D segmentations. An FCN was used for delineating mitochondria from the background
    followed by median filtering along the $z$ direction in the volume of images.
    This $z$-filtering allows the removal of spurious strokes and the recovery of
    regions of interest when sufficient adjacent slices contain the missed component.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Oztel 等人 [[94](#bib.bib94)] 提出了使用中值过滤方法来结合 3D 上下文，以从 2D 分割的输出中重建线粒体。使用 FCN 从背景中
    delineate 线粒体，然后在图像体积中的 $z$ 方向上进行中值过滤。这种 $z$-过滤允许去除伪影，并在相邻切片中存在遗漏成分时恢复感兴趣区域。
- en: The deep contextual residual network (DCR) by Xiao et al. [[123](#bib.bib123)]
    is an extension of FCN with residual blocks and multi-scale feature fusion. They
    used the summation based skip connections which fuse high-level details from output
    of deconvolutions in the decoder and low-level information from ResNet encoder.
    The proposed post-processing method with a multi-cut approach and 3D contextual
    features proved important to reduce discontinuities (boundary splits or merges),
    which in turn helped to reduce false positives and false negatives in various
    2D sections. DCR outperformed several state-of-the-art segmentation methods on
    the ISBI 2012 dataset.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao 等人 [[123](#bib.bib123)] 提出的深度上下文残差网络 (DCR) 是 FCN 的一种扩展，结合了残差块和多尺度特征融合。他们使用了基于求和的跳跃连接，将解卷积输出中的高级细节与
    ResNet 编码器中的低级信息进行融合。所提出的多切割方法和 3D 上下文特征的后处理方法对于减少不连续性（边界分裂或合并）至关重要，这反过来帮助减少了各种
    2D 截面的假阳性和假阴性。DCR 在 ISBI 2012 数据集上的表现超越了几种最先进的分割方法。
- en: Advanced networks for different tasks may be too computationally demanding to
    run on affordable hardware, leading users to modify macro-level design aspects.
    Examples of such modifications include downsampling input images and reducing
    network size or depth to ensure compatibility with computer hardware constraints.
    Casser et al. [[21](#bib.bib21)] introduced a fast mitochondria segmentation method
    using a reduced number of layers and lightweight bilinear upsampling instead of
    transposed convolutions in the decoder of U-Net. Moreover, they introduced a novel
    data augmentation method that generates training samples on the fly by randomly
    applying spatial transformations to the original images, which leads to increased
    training efficiency and robustness to variations in image quality. The authors
    also incorporate a post-processing step based on $z$-filtering to reconstruct
    3D mitochondria. The proposed approach was evaluated on several EM datasets and
    achieved state-of-the-art performance in terms of segmentation accuracy and speed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同任务的高级网络可能会对经济实用的硬件要求过高，导致用户需修改宏观设计方面的内容。此类修改的例子包括下采样输入图像以及减少网络的大小或深度，以确保与计算机硬件限制的兼容性。Casser
    等人 [[21](#bib.bib21)] 提出了一个快速的线粒体分割方法，该方法使用了较少的层数和轻量级的双线性上采样，而不是 U-Net 解码器中的转置卷积。此外，他们还引入了一种新颖的数据增强方法，通过随机对原始图像应用空间变换来实时生成训练样本，从而提高了训练效率和对图像质量变化的鲁棒性。作者还结合了基于
    $z$-过滤的后处理步骤来重建 3D 线粒体。所提出的方法在多个 EM 数据集上进行了评估，并在分割准确性和速度方面达到了最先进的性能。
- en: Data augmentation is a technique that is mostly used in machine learning and
    computer vision to increase the size and diversity of a training set. This is
    the case with most of the papers that are reviewed here. The process involves
    applying various transformations or modifications to the existing data in order
    to create new, but similar, instances of the data. It is particularly useful in
    cases where the size of the available dataset is limited, as it allows the model
    to learn from a larger and more diverse set of data without requiring additional
    data collection efforts. It can also help prevent overfitting and improve the
    robustness of the model by exposing it to a wider range of data variations. Moreover,
    test-time augmentation has also been proven effective to average out noise in
    predictions but at the cost of time complexity [[75](#bib.bib75), [128](#bib.bib128),
    [122](#bib.bib122), [127](#bib.bib127)].
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种主要用于机器学习和计算机视觉中的技术，用于增加训练集的规模和多样性。这在这里审阅的大多数论文中都有应用。该过程涉及对现有数据进行各种变换或修改，以创建新的但相似的数据实例。在可用数据集的规模有限的情况下特别有用，因为它允许模型从更大且更具多样性的数据集中学习，而无需额外的数据收集工作。它还可以帮助防止过拟合，通过将模型暴露于更广泛的数据变异范围来提高模型的鲁棒性。此外，测试时增强也被证明对平均预测噪声有效，但代价是时间复杂性[[75](#bib.bib75),
    [128](#bib.bib128), [122](#bib.bib122), [127](#bib.bib127)]。
- en: A residual encoder module with ASPP for multi-scale contextual feature integration
    was investigated by Jiang et al. [[64](#bib.bib64)]. The decoder module included
    the fusion of previous low-level features and high-level features from the output
    of ASPP, followed by bi-linear upsampling to obtain the segmentation map. They
    achieved better performance compared to the baseline, U-Net, and Deeplab v3+ for
    the segmentation of cell bodies and cell nuclei.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Jiang等人[[64](#bib.bib64)]研究了具有ASPP的残差编码器模块，用于多尺度上下文特征集成。解码器模块包括将之前的低级特征与ASPP输出的高级特征融合，随后进行双线性上采样以获得分割图。与基线模型U-Net和Deeplab
    v3+相比，他们在细胞体和细胞核的分割中取得了更好的性能。
- en: The Dense-UNet model was proposed by Cao et al. [[18](#bib.bib18)] as an extension
    of the popular U-Net architecture that incorporates densely connected blocks within
    the U-Net’s skip connections. The densely connected blocks help to improve gradient
    flow and feature reuse, which leads to better feature representation and higher
    segmentation accuracy. Besides its outstanding results on the ISBI 2012 challenge,
    the model turned out to be highly robust to variations in noises and artifacts
    of neuronal membrane images, requiring no further post-processing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Dense-UNet模型由Cao等人提出[[18](#bib.bib18)]，作为流行的U-Net架构的扩展，结合了U-Net跳跃连接中的密集连接块。这些密集连接块有助于改善梯度流动和特征重用，从而提高特征表示和分割精度。除了在ISBI
    2012挑战赛上表现出色外，该模型对神经膜图像中的噪声和伪影变化表现出高度的鲁棒性，无需进一步的后处理。
- en: FusionNet is a fully residual U-Net architecture that combines different levels
    of feature representations by fusing the output of multiple sub-networks with
    different receptive fields. It includes a residual learning framework along with
    deconvolutional layers to improve the training convergence and segmentation accuracy.
    The study by Quan et al. [[101](#bib.bib101)] showed that an integrated multi-stage
    refinement process using four concatenated FusionNet units can effectively eliminate
    the requirement for any proofreading⁶⁶6Proofreading refers to the manual validation
    of segmented (manual or automatic) image data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: FusionNet是一种完全残差的U-Net架构，通过融合多个具有不同感受野的子网络的输出，结合了不同层次的特征表示。它包括一个残差学习框架以及反卷积层，以提高训练收敛性和分割精度。Quan等人[[101](#bib.bib101)]的研究表明，使用四个级联FusionNet单元的集成多阶段精炼过程可以有效地消除任何校对需求⁶⁶6校对指的是对分割（手动或自动）图像数据的人工验证。
- en: A novel data augmentation strategy was also proposed by Spiers et al. [[110](#bib.bib110)],
    which simulates realistic variations in the EM images to improve the robustness
    of their 2D CNN for the semantic segmentation of nuclear envelopes. The proposed
    approach based on 2D U-Net achieved high segmentation accuracy and can be used
    to extract meaningful biological information from the segmented nuclear envelope,
    such as the distribution of nuclear pores. Their model was run on each axis after
    transposing the stack, and the resulting three orthogonal predictions were merged
    to produce the ultimate segmentation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spiers 等人[[110](#bib.bib110)] 还提出了一种新颖的数据增强策略，模拟了电子显微镜图像中的现实变化，以提高其2D CNN在核膜语义分割中的鲁棒性。基于2D
    U-Net 的方法实现了高分割准确度，并可用于提取分割核膜中的有意义的生物信息，如核孔的分布。他们的模型在转置堆栈后对每个轴运行，并将结果合并成最终的分割。
- en: Chen et al. [[29](#bib.bib29)] used a 2D CNN with only four layers for the segmentation
    of membranes, mitochondria, vesicles, and microtubules in cryo-ET. The architecture
    of the CNN layers was optimized to capture a large context by utilizing $15\times
    15$ pixel kernels in the first two layers. This design allowed for the use of
    a single max-pooling layer to downsample the output to half the input resolution,
    which aids in distinguishing intricate details of structures such as single (vesicle,
    microtubule) or double membrane (plasma membrane, mitochondria). A CNN for each
    of the four structures was trained with a few sections of the tomogram containing
    structures of interest. Automated segmentation was required for subsequent sub-tomogram
    classification and averaging for the determination of in-situ structures for the
    molecular components of interest.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人[[29](#bib.bib29)] 使用了一个只有四层的2D CNN来对膜、线粒体、囊泡和微管进行分割。CNN层的架构经过优化，通过在前两层中使用$15\times
    15$像素的卷积核来捕捉大范围的上下文。这一设计允许使用单个最大池化层将输出下采样到输入分辨率的一半，有助于区分诸如单层（囊泡、微管）或双层膜（质膜、线粒体）的细节。针对这四种结构的每一种CNN都用包含感兴趣结构的几节断层扫描图进行训练。后续的子断层扫描分类和平均需要自动分割，以确定感兴趣的分子成分的原位结构。
- en: 5.1.2 Approaches based on 3D CNNs
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于3D CNN的方法
- en: Similar to 2D deep architectures, a 3D CNN consists of multiple layers of filters,
    including convolutional, pooling, and activation layers, to learn spatial features
    from the input data. The filters scan the input volume at different locations
    and orientations to identify features that are relevant for segmentation. The
    key difference between 2D and 3D CNNs is the inclusion of an additional depth
    dimension in the input data. This allows the network to capture the spatial and
    depth relationships between adjacent slices in the volume. Due to the large amount
    of data and computational resources required for training 3D CNNs, such methods
    are typically used in high-end computing environments, such as specialized workstations
    or cloud computing platforms. Hybrid 2D-3D architectures have also been investigated
    that try to find the right trade-off between high computational demand and effectiveness.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于2D深度架构，3D卷积神经网络（CNN）由多个滤波层组成，包括卷积层、池化层和激活层，用于从输入数据中学习空间特征。这些滤波器在不同的位置和方向扫描输入体积，以识别对分割有意义的特征。2D和3D
    CNN的主要区别在于输入数据中包含了额外的深度维度。这使得网络能够捕捉相邻切片之间的空间和深度关系。由于训练3D CNN所需的大量数据和计算资源，这些方法通常用于高端计算环境，如专业工作站或云计算平台。还研究了混合2D-3D架构，试图在高计算需求和效果之间找到适当的权衡。
- en: In this review, there are three approaches that adopted complete 3D CNN architectures
    in a fully supervised way. The first is the work by Cheng and Varshney [[30](#bib.bib30)]
    who proposed a 3D CNN for the segmentation of mitochondria in volumetric data.
    The authors also propose a novel data augmentation technique that uses stochastic
    sampling in the pooling layers to generate realistic variations in the feature
    space. In their thorough investigation, they conclude that the 3D CNNs outperform
    their 2D counterparts with a high statistical significance. The improvement was
    mainly attributable to the introduced augmentations as well as to the factorized
    convolutions which also permitted high efficiency, which was also proven useful
    in FIB-SEM (isotropic) volumes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综述中，有三种方法采用了完全监督的完整 3D CNN 架构。第一种是 Cheng 和 Varshney [[30](#bib.bib30)] 的工作，他们提出了一种用于体积数据中线粒体分割的
    3D CNN。作者们还提出了一种新颖的数据增强技术，该技术在池化层中使用随机采样来生成特征空间中的现实变化。在他们的深入研究中，他们得出结论，3D CNN
    在统计学上显著优于 2D 对应物。该改进主要归因于引入的增强技术以及因子卷积，这些技术也带来了高效性，这在 FIB-SEM（各向同性）体积中也被证明是有用的。
- en: '[[90](#bib.bib90)] also presented a 3D CNN-based method for the segmentation
    of mitochondria and endolysosomes in volumetric EM. The proposed method is based
    on the HighRes3DNet architecture, but it has the filters in the first layer constrained
    to having zero mean, and called it HighRes3DZMNet. The zero mean layer made the
    neural network robust to changes in the brightness of the volume inputs. The network
    is trained using the UroCell dataset for jointly segmenting mitochondria and endolysosomes
    due to similar morphologies of these biological structures. The method was also
    applied to segment mitochondria in the Lucchi++ dataset to achieve state-of-the-art
    segmentation results for FIB-SEM volumes.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[[90](#bib.bib90)] 还提出了一种基于 3D CNN 的方法，用于体积 EM 中线粒体和内溶酶体的分割。该方法基于 HighRes3DNet
    架构，但其第一层的滤波器被限制为零均值，并称之为 HighRes3DZMNet。零均值层使神经网络对体积输入亮度的变化具有鲁棒性。该网络使用 UroCell
    数据集进行训练，以联合分割线粒体和内溶酶体，因为这些生物结构具有相似的形态。该方法还应用于 Lucchi++ 数据集中的线粒体分割，达到了 FIB-SEM
    体积的最先进分割结果。'
- en: Heinrich et al. [[59](#bib.bib59)] also relied on a 3D CNN for the segmentation
    of 35 organelle classes in cells from FIB-SEM volumes. The multi-channel 3D U-Net
    was trained on 28 volumes from the open-source OpenOrganelle collection covering
    four different cell types. They investigated how one segmentation model that is
    trained with samples of all 35 organelles compares with more specific models that
    are trained with subsets of semantically-related organelle classes, such as the
    endoplasmic reticulum (ER) and its associated structures, namely ER exit sites,
    ER membrane, and ER lumen. It turned out, that the single model that is trained
    by all classes outperforms the more specific ones. This is attributable to the
    richer diversity in the training set which resulted in a model with better generalization
    abilities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Heinrich 等人 [[59](#bib.bib59)] 也依靠 3D CNN 对 FIB-SEM 体积中的 35 种细胞器类别进行分割。多通道 3D
    U-Net 在来自开放源代码 OpenOrganelle 集合的 28 个体积上进行了训练，涵盖了四种不同的细胞类型。他们研究了一个经过所有 35 种细胞器样本训练的分割模型，与针对语义相关细胞器类别子集（如内质网（ER）及其相关结构，即
    ER 出口部位、ER 膜和 ER 腔）的更具体模型进行比较的情况。结果表明，所有类别训练的单一模型优于更具体的模型。这归因于训练集中丰富的多样性，从而导致了具有更好泛化能力的模型。
- en: Hybrid 2D-3D approaches were adopted for the segmentation of volume datasets
    in order to reduce the computational cost of 3D convolutions in certain layers
    and achieve better convergence. Their main application lies in the ability to
    segment anisotropic volumes for efficiently processing their 3D context. For instance,
    both anisotropic and isotropic EM volumes could be processed using hybrid 2D-3D
    network architectures that include $3\times 3\times 1$ convolutions instead of
    $3\times 3\times 3$ to modify them to 2D ones. Xiao et al. [[122](#bib.bib122)]
    was the first to introduce a fully residual hybrid 2D-3D network with deep supervision
    to improve mitochondria segmentation. For reducing the number of parameters, 3D
    convolutions were used only in the first and last layers of a 3D U-Net. A deeply
    supervised strategy was proposed by injecting auxiliary branches into the initial
    layers of the decoder for avoiding the vanishing gradients problem. The complexity
    of the network allowed it to use a simple connected component analysis method
    for 3D reconstruction across both anisotropic and isotropic volume EM datasets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少某些层中3D卷积的计算成本并实现更好的收敛性，采用了混合2D-3D方法来对体积数据集进行分割。这些方法的主要应用在于能够对各向异性体积进行分割，以有效处理其3D上下文。例如，可以使用包括$3\times
    3\times 1$卷积的混合2D-3D网络架构处理各向异性和各向同性EM体积，而不是$3\times 3\times 3$卷积，以将其修改为2D卷积。Xiao等人[[122](#bib.bib122)]首次引入了一个完全残差的混合2D-3D网络，并通过深度监督来改进线粒体分割。为了减少参数数量，3D卷积仅在3D
    U-Net的第一层和最后一层使用。提出了一种深度监督策略，通过在解码器的初始层注入辅助分支，以避免梯度消失问题。网络的复杂性使其能够使用简单的连通组件分析方法进行3D重建，适用于各向异性和各向同性体积EM数据集。
- en: 'Lee et al. [[75](#bib.bib75)] adapted the hybrid 2D-3D model of Turaga et al.
    [[116](#bib.bib116)] to predict 3D affinity maps for the segmentation of neuronal
    membranes in 3D volumes. The proposed CNN model incorporated multi-slice inputs
    along with long-range affinity-based auxiliary supervision in both the $z$- and
    $x-y$ directions. They utilized a hybrid 2D-3D U-Net for segmenting anisotropic
    volumes and post-processing with a simple mean-affinity agglomeration strategy
    for segmenting neuronal regions. The proposed affinity supervision simulates the
    use of boundary maps with different thicknesses in the DeepEM3D (Section  [5.3](#S5.SS3
    "5.3 Ensemble learning ‣ 5 Fully supervised methods ‣ Segmentation in large-scale
    cellular electron microscopy with deep learning: A literature survey")), outperforming
    it in the SNEMI3D competition.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lee等人[[75](#bib.bib75)]将Turaga等人[[116](#bib.bib116)]的混合2D-3D模型进行了改编，以预测3D体积中神经膜的3D亲和力图。所提出的CNN模型结合了多切片输入以及在$z$-方向和$x-y$方向上的长距离亲和力辅助监督。他们利用混合2D-3D
    U-Net对各向异性体积进行分割，并使用简单的均值亲和力聚合策略进行后处理，以分割神经区域。提出的亲和力监督模拟了DeepEM3D中的边界图（第[5.3](#S5.SS3
    "5.3 Ensemble learning ‣ 5 Fully supervised methods ‣ Segmentation in large-scale
    cellular electron microscopy with deep learning: A literature survey")节），在SNEMI3D竞赛中表现优异。'
- en: A structured loss that favors high affinities between 3D voxels was used to
    obtain topologically correct segmentations by Funke et al. [[49](#bib.bib49)].
    The affinity predictions were accurate enough to be used with a simple agglomeration
    to efficiently segment both isotropic and anisotropic (CREMI, FIB, and SegEM)
    data, outperforming methods with more elaborate post-processing pipelines. Bailoni
    et al. [[7](#bib.bib7)] used signed graphs to anticipate both attractive and repulsive
    forces among 3D voxels, enabling graph prediction through a 3D U-Net, in a manner
    similar to the method proposed by Funke et al. [[49](#bib.bib49)].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Funke等人使用了一种结构化的损失函数，该函数偏好3D体素之间的高亲和力，从而获得拓扑正确的分割[[49](#bib.bib49)]。亲和力预测足够准确，可以通过简单的聚合方法有效地分割各向同性和各向异性（CREMI、FIB和SegEM）数据，表现优于具有更复杂后处理流程的方法。Bailoni等人[[7](#bib.bib7)]使用签名图来预测3D体素之间的吸引力和排斥力，使得通过3D
    U-Net进行图预测成为可能，这种方法与Funke等人[[49](#bib.bib49)]提出的方法类似。
- en: Building on the concept of long-range affinities for boundary detection, Heinrich
    et al. [[60](#bib.bib60)] used neighboring context to predict voxel-wise distance
    maps through regression loss instead of probabilities. Those distance predictions,
    when thresholded, generated precise binary segmentations for synapses. Such distance
    prediction maps with simple thresholding allowed scaling the prediction at high-throughput
    speeds (3 megavoxels per second) for a full adult fly brain volume of 50 teravoxels
    in size.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Heinrich等人基于边界检测的长程关联概念，[[60](#bib.bib60)] 使用邻近上下文通过回归损失而非概率预测体素距离图。这些距离预测经过阈值处理后，生成了精确的二值分割图像，用于突触的分割。这种距离预测图像经过简单阈值处理，允许在高通量速度（每秒3兆体素）下进行预测，适用于大小为50太体素的完整成年果蝇脑体积。
- en: 5.2 End-to-end learning - instance segmentation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 端到端学习 - 实例分割
- en: End-to-end learning approaches are also the most popular ones for instance segmentation,
    which requires the delineation of each instance within the same class of structures.
    This is particularly important for classes of structures that tend to be apposed
    with each other, such as mitochondria.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端学习方法也是实例分割中最受欢迎的方法，这要求对同一类别的结构进行逐一标定。这对于那些彼此相接的结构类别尤为重要，如线粒体。
- en: 'CNN-based methods for instance segmentation were grouped into two categories
    by Wei et al. [[119](#bib.bib119)]: top-down and bottom-up. Top-down methods typically
    utilize region proposal networks followed by precise delineation in each region.
    Conversely, bottom-up approaches aim to predict a binary segmentation mask, an
    affinity map, or a binary mask with instance boundary followed by several post-processing
    steps to distinguish instances. Due to the undefined scale of bounding boxes in
    EM images, bottom-up approaches have been the preferred methodology for 2D and
    3D instance segmentation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Wei等人将基于CNN的方法分为两类[[119](#bib.bib119)]：自上而下和自下而上。自上而下的方法通常利用区域提议网络，然后在每个区域进行精确标定。相反，自下而上的方法旨在预测二值分割掩模、亲和图或带实例边界的二值掩模，然后经过几个后处理步骤以区分实例。由于EM图像中边界框的尺度不确定，自下而上的方法已成为2D和3D实例分割的首选方法。
- en: 'The delineation of neuronal membrane does not require binary labels to distinguish
    one type of neuron from the other. There is no interesting semantics involved,
    unlike distinguishing a sub-cellular structure from other irrelevant structures
    or backgrounds followed by delineation to obtain individual instances. This type
    of segmentation is also referred to as image partitioning, as it divides the entire
    image into different neuronal parts based on its membranes. Such partitioning
    allows for the reconstruction of individual neuronal structures using post-processing.
    Figure [7](#footnote7 "footnote 7 ‣ Figure 4 ‣ 5.2.1 Approaches based on 2D CNNs
    ‣ 5.2 End-to-end learning - instance segmentation ‣ 5 Fully supervised methods
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") shows examples of semantic and instance segmentation of
    mitochondria along with an illustration of neuronal 3D reconstruction after image
    partitioning.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '神经膜的标定不需要二值标签来区分不同类型的神经元。与将亚细胞结构与其他无关结构或背景区分开并进行标定以获得单个实例不同，这里没有有趣的语义。这种分割类型也被称为图像分割，因为它根据膜将整个图像划分为不同的神经部分。这种分割允许通过后处理重建单个神经结构。图 [7](#footnote7
    "footnote 7 ‣ Figure 4 ‣ 5.2.1 Approaches based on 2D CNNs ‣ 5.2 End-to-end learning
    - instance segmentation ‣ 5 Fully supervised methods ‣ Segmentation in large-scale
    cellular electron microscopy with deep learning: A literature survey") 显示了线粒体的语义和实例分割示例，以及图像分割后的神经元3D重建插图。'
- en: 5.2.1 Approaches based on 2D CNNs
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于2D CNN的方法
- en: The only top-down approach from the reviewed works in this paper is the one
    proposed by Liu et al. [[80](#bib.bib80)]. They introduced a pipeline that complements
    Mask-RCNN. In particular, they proposed a mechanism that refines undersegmented
    mitochondria in the output of Mask-RCNN, by iteratively enhancing the field of
    view that preserves the previous segmentation states. They systematically demonstrated
    that their approach outperformed competing methods that rely on U-Net, FFN, and
    Mask-RCNN in instance segmentation of mitochondria.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾的工作中唯一的自上而下的方法是刘等人提出的[[80](#bib.bib80)]。他们引入了一个补充 Mask-RCNN 的流程。特别地，他们提出了一种机制，通过迭代增强视野来精炼
    Mask-RCNN 输出中的欠分割线粒体，同时保持之前的分割状态。他们系统地展示了他们的方法在线粒体实例分割中优于依赖 U-Net、FFN 和 Mask-RCNN
    的竞争方法。
- en: '| ![Refer to caption](img/bb1ba7105389c6b8ce8a398056badf31.png) | ![Refer to
    caption](img/c520aa8ba7d68acaef09b35da5fd3845.png) | ![Refer to caption](img/866e70929462f039233ddd1680f4c575.png)
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/bb1ba7105389c6b8ce8a398056badf31.png) | ![参见说明](img/c520aa8ba7d68acaef09b35da5fd3845.png)
    | ![参见说明](img/866e70929462f039233ddd1680f4c575.png) |'
- en: '| (a) | (b) | (c) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) | (c) |'
- en: '| ![Refer to caption](img/6a3fac257475a1fdf8b2b2822c9c8a6d.png) | ![Refer to
    caption](img/c6007e9103aee846165b9ea1b052e907.png) | ![Refer to caption](img/1848a6f9110b9ae3bd7a3609a1e3539e.png)
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/6a3fac257475a1fdf8b2b2822c9c8a6d.png) | ![参见说明](img/c6007e9103aee846165b9ea1b052e907.png)
    | ![参见说明](img/1848a6f9110b9ae3bd7a3609a1e3539e.png) |'
- en: '| (d) | (e) | (f) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| (d) | (e) | (f) |'
- en: 'Figure 4: Example of (top row) semantic and instance segmentation of mitochondria
    and (bottom row) neuronal membrane segmentation followed by 3D reconstruction
    of neuronal objects from a volumetric EM image. (a) Raw EM 2D section extracted
    from a FIB-SEM volume of a mouse kidney from the OpenOrganelle jrc_mus-kidney
    dataset⁷⁷7[https://open.quiltdata.com/b/janelia-cosem-datasets/tree/jrc_mus-kidney/](https://open.quiltdata.com/b/janelia-cosem-datasets/tree/jrc_mus-kidney/).
    (b, c) Ground truth labels for semantic and instance segmentation. The instance
    segmentation map identifies each individual mitochondria with a unique color.
    (d) Raw EM 2D section extracted from the SNEMI3D (#3) dataset for the task of
    neuronal membrane segmentation and reconstruction. (e) The ground truth map of
    the neuronal membrane segmentation, which is used to partition the image completely.
    (f) 3D reconstruction of selected neuronal structures that pass through the given
    2D section from adjacent sections of the EM volume. The information from multiple
    images is used to create a 3D reconstruction through various post-processing methods,
    such as clustering, watershed, or graph-based methods.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: (上排) 线粒体的语义和实例分割示例，(下排) 神经膜分割示例，随后从体积电子显微镜图像中重建神经对象。 (a) 从 OpenOrganelle
    jrc_mus-kidney 数据集中提取的来自小鼠肾脏的原始电子显微镜 2D 截面 [https://open.quiltdata.com/b/janelia-cosem-datasets/tree/jrc_mus-kidney/](https://open.quiltdata.com/b/janelia-cosem-datasets/tree/jrc_mus-kidney/)。
    (b, c) 语义和实例分割的真实标签。实例分割图识别每个独立的线粒体，并使用独特的颜色标记。 (d) 从 SNEMI3D (#3) 数据集中提取的用于神经膜分割和重建的原始电子显微镜
    2D 截面。 (e) 神经膜分割的真实标签图，用于完全分割图像。 (f) 从电子显微镜体积的相邻截面中选取的神经结构的 3D 重建。通过多张图像的信息，利用聚类、分水岭或图形方法等各种后处理方法创建
    3D 重建。'
- en: Shape prior turned out to be important for some techniques to improve the quality
    of instance segmentation. Shape prior refers to the incorporation of prior knowledge
    about the expected shape or structure of an object of interest into segmentation
    algorithms. For example, Yuan et al. [[127](#bib.bib127)] proposed the Hive-Net
    CNN, which was designed to overcome the challenges posed by the high variability
    in mitochondria shapes and sizes, as well as the presence of other cellular structures
    in the images. The network consists of multiple view-specific sub-networks that
    process different views of the image, and a centerline-aware hierarchical ensemble
    module that combines the outputs of the sub-networks to generate the final segmentation
    result. The centerline-aware module uses a new type of loss function that encourages
    the network to learn the morphology of mitochondria and to segment them along
    their centerlines. The proposed network was evaluated on two publicly available
    datasets, and an ablation study concluded that the centerline-aware module and
    the view-specific sub-networks were critical for achieving high segmentation accuracy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 形状先验对提高实例分割质量的一些技术来说变得非常重要。形状先验是指将关于目标对象预期形状或结构的先验知识融入分割算法中。例如，Yuan 等人[[127](#bib.bib127)]
    提出了 Hive-Net CNN，该网络旨在克服线粒体形状和大小的高变异性以及图像中其他细胞结构的挑战。该网络由多个视图特定的子网络组成，这些子网络处理图像的不同视图，并且还有一个中心线感知的分层集成模块，该模块结合子网络的输出生成最终的分割结果。中心线感知模块使用了一种新的损失函数，该函数鼓励网络学习线粒体的形态并沿其中心线进行分割。所提出的网络在两个公开数据集上进行了评估，消融研究表明，中心线感知模块和视图特定子网络对于实现高分割精度至关重要。
- en: Shape information has also been exploited by the hierarchical encoder-decoder
    network (HED-Net) for the instance segmentation of mitochondria [[88](#bib.bib88)].
    That strategy leveraged the shape information available in the manual labels to
    train the model more effectively. Instead of relying solely on the ground truth
    label maps for model training, an additional subcategory-aware supervision was
    introduced. That was achieved by decomposing each manual label map into two complementary
    label maps based on the ovality of the mitochondria. The resulting three-label
    maps were used to supervise the training of the HED-Net. The original label map
    was used to guide the network to segment all mitochondria of varying shapes, while
    the auxiliary label maps guided the network to segment subcategories of mitochondria
    with circular and elliptic shapes, respectively. The experiments conducted on
    two publicly available benchmarks show that the proposed HED-Net outperforms state-of-the-art
    methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 形状信息还被分层编码器-解码器网络（HED-Net）用于线粒体的实例分割[[88](#bib.bib88)]。该策略利用手动标签中可用的形状信息来更有效地训练模型。为了避免仅依赖真实标签图进行模型训练，引入了附加的子类别感知监督。这是通过将每个手动标签图分解为两个基于线粒体椭圆度的互补标签图来实现的。得到的三个标签图用于监督
    HED-Net 的训练。原始标签图用于引导网络分割所有形状各异的线粒体，而辅助标签图分别引导网络分割具有圆形和椭圆形的线粒体子类别。对两个公开基准数据集进行的实验表明，所提出的
    HED-Net 超越了最先进的方法。
- en: The inclusion of apriori knowledge about shape in segmentation algorithms contributes
    to increased specificity as they become more selective in delineating the structures
    of interest and keep false positives to a minimum. They can also improve generalization
    ability especially when the training data is limited. Methods that use shape priors,
    however, are more structure-specific, and therefore different methods may need
    to be designed for the segmentation of distinct organelles.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割算法中加入形状的先验知识有助于提高特异性，因为这些算法在划定感兴趣结构时变得更加选择性，并将假阳性保持在最低限度。它们还可以改善泛化能力，特别是当训练数据有限时。然而，使用形状先验的方法通常更具结构特异性，因此可能需要为不同的细胞器设计不同的方法。
- en: 5.2.2 Approaches based on 3D CNNs
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 基于 3D CNN 的方法
- en: The largest instance segmentation dataset for mitochondria (MitoEM) proposed
    by Wei et al. [[119](#bib.bib119)] benchmarks the dataset by proposing a 3D U-Net.
    It is trained with binary masks and contours using two separate decoders, followed
    by a marker-controlled watershed to obtain instance segmentations, and is called
    U3D-BC +MW for short. Wei et al. [[119](#bib.bib119)] introduced two networks,
    MitoEM-R and MitoEM-H, citing variations in sizes, shapes, and noise content for
    serial sections from rat and human samples. The MitoEM-R network can generalize
    on the human dataset as the rat samples have complex mitochondrial morphologies.
    The simpler U3D-BC +MW method was shown to be more effective than FFNs, as they
    were not able to capture the fine geometry of mitochondria with complex shapes
    or in close contact to each other.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Wei 等人提出的最大线粒体实例分割数据集（MitoEM）[[119](#bib.bib119)] 通过提出一个 3D U-Net 对数据集进行了基准测试。该方法使用两个独立的解码器进行二进制掩码和轮廓的训练，然后通过标记控制的分水岭算法获得实例分割，简称为
    U3D-BC +MW。Wei 等人[[119](#bib.bib119)] 引入了两个网络，MitoEM-R 和 MitoEM-H，考虑到来自大鼠和人类样本的序列切片在大小、形状和噪声内容上的差异。由于大鼠样本具有复杂的线粒体形态，MitoEM-R
    网络可以在人体数据集上进行泛化。与 FFNs 相比，更简单的 U3D-BC +MW 方法被证明更有效，因为 FFNs 无法捕捉复杂形状或紧密接触的线粒体的细微几何特征。
- en: The DeepACSON approach by Abdollahzadeh et al. [[1](#bib.bib1)], which was proposed
    for the instance segmentation of axons and nuclei in 3D volumes, is supported
    by a postprocessing method that relies on shape features. To correct for topological
    errors, a cylindrical shape decomposition algorithm is used as a postprocessing
    step to identify any erroneously detected axons and to correct under-segmented
    ones at their cross-overs. The circularity of the cell nucleus is corrected using
    the level-set based geometric deformable model, which approximates the initial
    shape of the object with a curve. This is then adjusted to minimize an energy
    function associated with the curve when it fits perfectly to the object’s boundaries.
    Energy functions enable the inclusion of shape information, whether it is a vague
    concept like smoothness constraints or a precise idea like shape constraints (strict
    adherence to a particular shape).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Abdollahzadeh 等人提出的 DeepACSON 方法[[1](#bib.bib1)]，用于 3D 体积中的轴突和细胞核实例分割，得到了依赖于形状特征的后处理方法的支持。为了纠正拓扑错误，使用了圆柱形状分解算法作为后处理步骤，以识别任何错误检测的轴突，并纠正其交叉点处的欠分割。细胞核的圆形度通过基于水平集的几何可变形模型进行修正，该模型用曲线近似对象的初始形状。然后调整曲线以最小化与曲线相关的能量函数，当它完美地拟合对象的边界时。能量函数使得可以包含形状信息，无论是模糊的概念如平滑约束，还是具体的概念如形状约束（严格遵守特定形状）。
- en: Nuclei instance segmentation on a large-scale EM dataset was proposed by Lin
    et al. [[78](#bib.bib78)]. Their network, U3D-BCD, was inspired by the U3D-BC
    above but involved the additional learning of a signed Euclidean distance map
    along with foreground masks and instance contours to capture the structure of
    the background for segmentation. To locate the seeds for object centers, their
    methods starts by thresholding the predictions to identify markers with high foreground
    probability and distance value, but low contour probability. Next, the marker-controlled
    watershed transform algorithm is applied with the predicted distance map and seeds
    to generate masks. This approach has two advantages over the U3D-BC model [[119](#bib.bib119)],
    which also utilizes marker-controlled watershed transform for decoding. Firstly,
    the consistency among the three representations is leveraged to locate the seeds,
    which is more robust than the U3D-BC method which relies on only two predictions.
    Secondly, it uses the smooth signed distance map in the watershed decoding process,
    which is more effective in capturing instance structure than the foreground probability
    map used in U3D-BC.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Lin 等人[[78](#bib.bib78)] 提出了在大规模 EM 数据集上的细胞核实例分割。他们的网络 U3D-BCD 受到了上述 U3D-BC
    的启发，但涉及了额外的学习签名欧几里得距离图，以及前景掩码和实例轮廓，以捕捉背景结构进行分割。为了定位对象中心的种子，他们的方法首先通过阈值化预测来识别具有高前景概率和距离值但轮廓概率低的标记。接着，应用标记控制的分水岭变换算法与预测的距离图和种子生成掩码。这种方法相比于也利用标记控制分水岭变换进行解码的
    U3D-BC 模型[[119](#bib.bib119)] 有两个优势。首先，利用三种表示之间的一致性来定位种子，这比依赖于仅两个预测的 U3D-BC 方法更具鲁棒性。其次，它在分水岭解码过程中使用平滑的签名距离图，这比
    U3D-BC 使用的前景概率图更有效地捕捉实例结构。
- en: Li et al. [[76](#bib.bib76)] addressed 3D mitochondria instance segmentation
    with two supervised deep neural networks, namely ResUNet-H and ResU-Net-R, for
    the rat and human samples on the MitoEM dataset, respectively. Both networks produce
    outputs in the form of a semantic and instance boundary masks. Due to the increased
    difficulty of the human sample, Res-UNet-H has an additional decoder path to separately
    predict the semantic mask and instance boundary, while Res-UNet-R has only one
    path. Once the semantic mask and instance boundary are obtained, a seed map is
    synthesized, and the mitochondria instances are obtained using connected component
    labeling. To enhance the networks’ segmentation performance, a simple but effective
    anisotropic convolution block is designed, and a multi-scale training strategy
    is deployed. The MitoEM dataset has sparsely distributed imaging noise, with the
    human sample having a stronger subjective noise level than the rat sample. To
    reduce the influence of noise on segmentation, an interpolation network was utilized
    to restore the regions with noise, which were coarsely marked by humans. Besides
    mitochondria instance segmentation, the proposed method was demonstrated to have
    superior performance for mitochondria semantic segmentation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[76](#bib.bib76)] 使用两个监督深度神经网络，即 ResUNet-H 和 ResU-Net-R，分别处理 MitoEM 数据集上的大鼠和人类样本的
    3D 线粒体实例分割。这两个网络生成的输出是语义和实例边界掩码。由于人类样本的难度增加，Res-UNet-H 具有额外的解码器路径，用于单独预测语义掩码和实例边界，而
    Res-UNet-R 只有一条路径。一旦获得了语义掩码和实例边界，就会合成一个种子图，并使用连通组件标记法获得线粒体实例。为了提高网络的分割性能，设计了一种简单但有效的各向异性卷积块，并采用了多尺度训练策略。MitoEM
    数据集存在稀疏分布的成像噪声，人类样本的主观噪声水平比大鼠样本更强。为了减少噪声对分割的影响，利用了插值网络来恢复被粗略标记的噪声区域。除了线粒体实例分割，所提方法在线粒体语义分割方面也显示出优越的性能。
- en: Mekuč et al. [[89](#bib.bib89)] extended their previous approach based on the
    HighRes3DZMNet by post-processing steps with active contours to separate apposing
    mitochondria and thus achieving instance segmentation. By means of experiments
    on the extended UroCell dataset they demonstrated that this new approach is more
    effective than the U3D-BC +MW method.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Mekuč 等人 [[89](#bib.bib89)] 在基于 HighRes3DZMNet 的先前方法的基础上，通过活性轮廓的后处理步骤来分离相对的线粒体，从而实现实例分割。他们通过对扩展的
    UroCell 数据集的实验表明，这种新方法比 U3D-BC +MW 方法更有效。
- en: 5.3 Ensemble learning
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 集成学习
- en: Ensemble learning methods combine outputs of multiple algorithms or models to
    obtain better predictive performance in terms of accuracy and generalization.
    Pixel- or voxel-wise averaging and the majority or median voting are among the
    main aggregation methods.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习方法结合了多个算法或模型的输出，以获得更好的预测性能，特别是在准确性和泛化方面。像素或体素级的平均以及多数或中位数投票是主要的聚合方法之一。
- en: An ensemble technique was in fact investigated by Zeng et al. [[128](#bib.bib128)]
    for the segmentation of neuronal membranes in the brain volumes. They trained
    several variations of their DeepEM3D network, which could process different numbers
    of input slices and inputs with varying thicknesses of object boundaries. The
    DeepEM3D network extended the FCN architecture by introducing a hybrid network
    with 3D convolutions in the first two layers to enable integrating anisotropic
    information in the early stages, and 2D layers afterwards. DeepEM3D employed inception
    and residual modules, multiple dilated convolutions, and combined the result of
    three models that integrated one, three, and five consecutive serial sections.
    Employing an ensemble strategy for enhancing boundaries (by maximum superposition)
    within the probability maps generated by these models proved essential for performing
    with near-human accuracy in the SNEMI3D challenge.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Zeng 等人 [[128](#bib.bib128)] 研究了一种集成技术，用于脑体积中神经膜的分割。他们训练了几种不同变体的 DeepEM3D
    网络，这些网络能够处理不同数量的输入切片和具有不同厚度物体边界的输入。DeepEM3D 网络通过在前两层引入具有 3D 卷积的混合网络来扩展 FCN 架构，以便在早期阶段集成各向异性信息，然后是
    2D 层。DeepEM3D 采用了 inception 和 residual 模块，多层扩张卷积，并结合了三个模型的结果，这些模型整合了一个、三个和五个连续的序列切片。通过这些模型生成的概率图中，使用集成策略（通过最大叠加）来增强边界对于在
    SNEMI3D 挑战中实现接近人类的准确度至关重要。
- en: CDeep3M is a cloud implementation of DeepEM3D to segment various anisotropic
    SBF-SEM and cryo-ET datasets [[54](#bib.bib54)]. Trained by a few sub-volumes
    of the cryo-ET tomogram, the resulting network was able to segment vesicles and
    membranes with high accuracy in other tomograms. The network implementation proved
    efficient for segmenting large-volume EM datasets such as SBF-SEM making it easier
    to analyze enormous amounts of imaging data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: CDeep3M 是 DeepEM3D 的云实现，用于分割各种各样的各向异性 SBF-SEM 和冷冻电子断层扫描 (cryo-ET) 数据集 [[54](#bib.bib54)]。通过训练少量冷冻电子断层扫描的子体积，得到的网络能够在其他断层扫描中高精度地分割囊泡和膜。这种网络实现证明了它在分割大体积电子显微镜数据集（如
    SBF-SEM）方面的高效性，使得分析大量影像数据变得更容易。
- en: The strengths of the ensemble paradigm was also confirmed by Guay et al. [[52](#bib.bib52)]
    for the segmentation of cytoplasm, mitochondria, and four types of granules in
    platelet cells. They demonstrated that the best segmentation performance (in terms
    of intersection over union) was achieved by combining the output of the top $k$
    performing weak classifiers, with each such classifier learned by a small portion
    of the training data. Similar to above, each model was a hybrid 2D-3D network
    used to segment anisotropic SBF-SEM volumes. They also highlighted that besides
    its effectiveness, their ensemble paradigm ensured better reproducibility of the
    results in comparison to individual models that were sensitive to initialization.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Guay 等人 [[52](#bib.bib52)] 也证实了集成范式的优势，用于分割血小板细胞中的细胞质、线粒体和四种类型的颗粒。他们展示了，通过结合排名前
    $k$ 的弱分类器的输出，获得了最佳的分割性能（按交并比衡量），每个分类器都是通过一小部分训练数据学习的。与上述类似，每个模型都是一种混合的 2D-3D 网络，用于分割各向异性
    SBF-SEM 体积。他们还强调，除了其有效性之外，他们的集成范式在结果的可重复性方面优于对初始化敏感的单个模型。
- en: Multiple network outputs were also combined with a workflow for binary EM segmentation
    provided by the EM-stellar platform [[68](#bib.bib68)]. Unlike the above two approaches,
    Khadangi et al. [[68](#bib.bib68)] used the ensemble paradigm to aggregate the
    output of different types of networks, namely CDeep3EM [[53](#bib.bib53)], EM-Net
    [[67](#bib.bib67)], PReLU-Net [[57](#bib.bib57)], ResNet, SegNet, U-Net, and VGG-16\.
    A cross-evaluation using a heatmap of different evaluation metrics revealed that
    no single deep architecture performs consistently well across all segmentation
    metrics. This is why ensemble approaches have an edge over individual methods
    as they leverage the strengths of each underlying model as was demonstrated in
    the evaluation of two different datasets for mitochondria segmentation in cardiac
    and brain tissue.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多个网络输出还与 EM-stellar 平台提供的二进制电子显微镜分割工作流结合 [[68](#bib.bib68)]。与上述两种方法不同，Khadangi
    等人 [[68](#bib.bib68)] 使用集成范式来聚合不同类型网络的输出，即 CDeep3EM [[53](#bib.bib53)]、EM-Net
    [[67](#bib.bib67)]、PReLU-Net [[57](#bib.bib57)]、ResNet、SegNet、U-Net 和 VGG-16。通过使用不同评估指标的热图进行交叉评估，揭示了没有单一的深度架构在所有分割指标上都表现一致良好。这就是为什么集成方法相比单一方法具有优势，因为它们利用了每个基础模型的优点，正如在对心脏和脑组织的线粒体分割的两个不同数据集的评估中所示。
- en: 5.4 Transfer learning
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 迁移学习
- en: Transfer learning is a framework that adapts the knowledge acquired from one
    dataset to another, and is generally used when an application has an insufficient
    amount of training samples. A pre-trained model is fine-tuned, usually in the
    final layers, with the training samples of a new dataset. This technique was used
    by Mekuč et al. [[90](#bib.bib90)] for the segmentation of mitochondria and endolysosomes
    from the background in EM images. Since mitochondria and endolysosomes share similar
    texture and mitochondria are more in abundance a binary segmentation model was
    first learned to segment mitochondria from the background. Subsequently, transfer
    learning was used to adapt the learned model for the segmentation of endolysosomes
    too. This was achieved by freezing all layers of the network except for the last
    one, which was fine-tuned by a smaller training set that included examples of
    endolysosomes. This approach is a demonstration how transfer learning can be used
    when the availability of a certain structure is limited.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一个将从一个数据集获得的知识适应到另一个数据集的框架，通常在应用程序缺乏足够的训练样本时使用。一个预训练模型通常在最后几层中进行微调，使用新数据集的训练样本。这一技术被
    Mekuč 等人 [[90](#bib.bib90)] 用于从电子显微镜图像的背景中分割线粒体和内溶酶体。由于线粒体和内溶酶体具有相似的纹理且线粒体更为丰富，因此首先学习了一个二分类分割模型来从背景中分割线粒体。随后，迁移学习被用来将已学模型适应于内溶酶体的分割。这是通过冻结网络的所有层，除了最后一层，该层通过包含内溶酶体示例的小型训练集进行微调。这种方法展示了当特定结构的可用性有限时，如何使用迁移学习。
- en: Fine-tuning a pre-trained network comes with the risk of overfitting to the
    few labeled training examples of the new dataset or application. This challenge
    has opened up new research avenues, namely few-shot learning and domain adaptation.
    The former can be a meta-learning approach that “learns to learn” from a given
    pre-trained model when conditioned on a few training examples (referred to as
    the support set) to perform well on new queries passed through a fixed feature
    extractor [[107](#bib.bib107)].
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练网络存在过拟合少量标记训练示例或新数据集的风险。这一挑战开辟了新的研究领域，即少样本学习和领域适应。前者可以是一个元学习方法，即在给定预训练模型的条件下“学习如何学习”，从少量训练示例（称为支持集）中进行学习，以便在通过固定特征提取器传递的新查询上表现良好
    [[107](#bib.bib107)]。
- en: Few-shot learning was the focus of the work by Dietlmeier et al. [[37](#bib.bib37)],
    who proposed a few-shot hypercolumn-based approach for mitochondria segmentation
    in cardiac and outer hair cells. The idea behind hypercolumn feature extraction
    was to extract features from different levels of a pre-trained CNN and combine
    them to form a single, high-dimensional feature representation for each pixel.
    The VGG-16 model pre-trained on the ImageNet dataset was used to extract hypercolumns,
    which were then passed through a linear regressor for actively selecting features.
    Only 20 labeled patches (2 $\%$- 98$\%$ train-test split) were used from a FIB-SEM
    stack for training a gradient-based boosting classifier (XGBoost). They showed
    how high segmentation accuracy on the Drosophila VNC dataset could be achieved
    by actively selecting features and learning using far less training data and even
    by using a single training sample (single-shot).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Few-shot 学习是 Dietlmeier 等人 [[37](#bib.bib37)] 研究的重点，他们提出了一种基于少量样本的超柱（hypercolumn）方法用于心脏和外毛细胞的线粒体分割。超柱特征提取的理念是从预训练的卷积神经网络（CNN）的不同层提取特征，并将它们组合成一个单一的高维特征表示，用于每一个像素。使用了在
    ImageNet 数据集上预训练的 VGG-16 模型来提取超柱特征，然后通过线性回归器进行主动特征选择。仅使用了 20 个标记补丁（2 $\%$-98$\%$
    训练-测试分割）用于训练基于梯度的提升分类器（XGBoost）。他们展示了通过主动特征选择和利用远少于训练数据的方式，甚至使用单个训练样本（单次学习）也能在果蝇
    VNC 数据集上实现高分割准确率。
- en: Domain adaptation is another form of transfer learning, where the source to
    target datasets share the same labels (classes) but have a different data distribution.
    Changes in data distribution can be due to slightly different experimental parameters
    during EM imaging or due to the imaging of different tissue types or body locations.
    Bermúdez-Chacón et al. [[11](#bib.bib11)] proposed the two-stream U-Net architecture,
    where the weights are related, yet different for each of the two domains, for
    supervised training on a few target labels. Only $10\%$ of labeled target data
    was required for domain adaptation to achieve state-of-the-art performance when
    compared to a U-Net trained on a fully annotated dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 域适应是另一种形式的迁移学习，其中源数据集和目标数据集共享相同的标签（类别），但数据分布不同。数据分布的变化可能是由于在EM成像过程中实验参数的略微不同，或者由于成像的组织类型或身体部位不同。Bermúdez-Chacón
    等人 [[11](#bib.bib11)] 提出了两流U-Net架构，其中权重相关但在两个领域中不同，用于对少量目标标签进行监督训练。与在完全注释数据集上训练的U-Net相比，仅需$10\%$标记的目标数据即可实现最先进的性能。
- en: 5.5 Configurability and Reproducability
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 可配置性和可重复性
- en: A key challenge in designing CNNs is the determination of the right architecture
    for the problem at hand. This has motivated research effort in what are known
    self-configurable networks that can automatically determine certain design choices.
    A self-configurable network is thus a type of artificial neural network that is
    capable of dynamically adapting its structure and parameters based on the input
    data and task concerned. This concept was used by Isensee et al. [[62](#bib.bib62)],
    who proposed the no-new-Net (nnU-Net) framework that consists of a 2D U-Net, 3D
    U-Net and a cascade of two 3D U-Nets. Self-configuration based on cross-validation
    was used to automatically determine some hyperparameters, such as the patch size,
    batch size and number of pooling operations. While it was shown to be very effective
    in various semantic segmentation problems in medical image benchmark datasets,
    its generalization ability in EM datasets has yet to be evaluated comprehensively.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 设计CNN的一个关键挑战是确定适合手头问题的正确架构。这激励了研究被称为自配置网络的努力，这些网络可以自动确定某些设计选择。因此，自配置网络是一种能够根据输入数据和相关任务动态调整其结构和参数的人工神经网络。这一概念被
    Isensee 等人 [[62](#bib.bib62)] 使用，他们提出了 no-new-Net (nnU-Net) 框架，该框架由 2D U-Net、3D
    U-Net 和两个 3D U-Net 的级联组成。基于交叉验证的自配置用于自动确定一些超参数，如补丁大小、批处理大小和池化操作数量。虽然它在各种医学图像基准数据集的语义分割问题中显示出非常有效，但其在电子显微镜数据集中的泛化能力尚未全面评估。
- en: 'Table 4: The list of 5 (out of 38) papers reviewed in this work and that are
    based on semi-, un- and self-supervised learning frameworks along with co-relative
    light and electron microscopy (CLEM) as discussed in Section 4.2). The abbreviation
    Org. stands for the studied organelle/s. The Type (2D and/or 3D) column indicates
    the type of methods used and problems addressed. The studies that are marked as
    both 2D and 3D use a 2D backbone method coupled with some post-processing operations
    for 3D reconstruction. The other studies that are flagged as 2D or 3D only, use
    2D or 3D only backbones to address 2D or 3D problems, respectively. The numbers
    in the Datasets column serve as correspondences to the identifiers in Table [2](#S3.T2
    "Table 2 ‣ 3.1 Serial section TEM and SEM datasets ‣ 3 Collections of key EM datasets
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey"), and the definitions of the performance metrics are presented
    in Section [7](#S7 "7 Segmentation evaluation metrics ‣ Segmentation in large-scale
    cellular electron microscopy with deep learning: A literature survey").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：本研究中回顾的 38 篇论文中的 5 篇，基于半监督、无监督和自监督学习框架，以及相关的光学和电子显微镜（CLEM），如第 4.2 节所讨论。缩写
    Org. 代表研究的细胞器。Type (2D 和/或 3D) 列指示所使用的方法类型和解决的问题。标记为 2D 和 3D 的研究使用 2D 主干方法，并结合一些后处理操作进行
    3D 重建。标记为仅 2D 或 3D 的其他研究，则仅使用 2D 或 3D 主干来解决 2D 或 3D 问题。Datasets 列中的数字对应于表 [2](#S3.T2
    "Table 2 ‣ 3.1 Serial section TEM and SEM datasets ‣ 3 Collections of key EM datasets
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") 中的标识符，性能指标的定义在第 [7](#S7 "7 Segmentation evaluation metrics
    ‣ Segmentation in large-scale cellular electron microscopy with deep learning:
    A literature survey") 节中呈现。'
- en: '| Citation | Org. | Type | Datasets | Performance | Backbone | Main methodological
    components |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 引用 | 组织 | 类型 | 数据集 | 性能 | 主干 | 主要方法学组件 |'
- en: '|  |  | 2D | 3D |  | metrics |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 2D | 3D |  | 指标 |  |  |'
- en: '| Semi-supervised learning - The superscripts $S$ and $I$ indicate semantic
    and instance segmentation |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 半监督学习 - 上标 $S$ 和 $I$ 表示语义和实例分割 |'
- en: '| [[111](#bib.bib111)]^S | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | FCN | Sequential
    semi-supervised learning |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| [[111](#bib.bib111)]^S | NM |  |  | 1 | $V_{rand}$, $V_{info}$ | FCN | 顺序半监督学习
    |'
- en: '| [[121](#bib.bib121)]^I | M |  |  | 1,6 | AP-50, AP | 2D U-Net | Positive
    unlabeled, momentum encoder |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)]^I | M |  |  | 1,6 | AP-50, AP | 2D U-Net | 正样本无标签，动量编码器
    |'
- en: '| Unsupervised learning - Semantic segmentation |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 无监督学习 - 语义分割 |  |  |'
- en: '| [[10](#bib.bib10)] | M, S |  |  | 1, 3, 8 | JI | 2D U-Net | Two stream U-Net,
    domain adaptation |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| [[10](#bib.bib10)] | M, S |  |  | 1, 3, 8 | JI | 2D U-Net | 双流 U-Net，领域适应
    |'
- en: '| [[97](#bib.bib97)] | M |  |  | 3, 8 | JI, DSC | 2D U-Net | Domain discriminators
    for adversarial loss |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| [[97](#bib.bib97)] | M |  |  | 3, 8 | JI, DSC | 2D U-Net | 对抗损失的领域鉴别器 |'
- en: '| Self-supervised learning - Semantic segmentation |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 自监督学习 - 语义分割 |  |  |'
- en: '| [[34](#bib.bib34)] | M |  |  | 2, 4, 8, 10,13, 18 | JI | 3D U-Net | Self-supervised
    learning, fine-tuning |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bib34)] | M |  |  | 2, 4, 8, 10,13, 18 | JI | 3D U-Net | 自监督学习，微调
    |'
- en: An experimental study by Franco-Barranco et al. [[47](#bib.bib47)] uncovered
    substantial reproducibility issues of different networks proposed for mitochondria
    segmentation in EM data. Additionally, it distinguished the impact of innovative
    architectures from that of training choices (such as pre-processing, data augmentation,
    output reconstruction, and post-processing strategies) by conducting multiple
    executions of the same configurations. Their systematic analysis enabled the identification
    of stable and lightweight models that consistently deliver state-of-the-art performance
    on publicly available datasets.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Franco-Barranco 等人[[47](#bib.bib47)] 的实验研究揭示了不同网络在电子显微镜数据中用于线粒体分割的显著重现性问题。此外，他们通过多次执行相同的配置，区分了创新架构和训练选择（如预处理、数据增强、输出重建和后处理策略）的影响。他们的系统分析使得能够识别出在公开数据集上始终如一地提供最先进性能的稳定且轻量级的模型。
- en: 6 Semi-, un- and self-supervised methods
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 半监督、无监督和自监督方法
- en: Semi-supervised and unsupervised learning are two types of machine learning
    methods, whose main difference between them is the amount of labeled data they
    use to train the model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督和无监督学习是两种机器学习方法，其主要区别在于它们用于训练模型的标记数据量。
- en: Unsupervised learning is a type of machine learning that deals with finding
    patterns and relationships in unlabeled training data. In this case, the algorithm
    learns to identify patterns and relationships in the data by clustering or grouping
    similar data points together. Semi-supervised learning, on the other hand, is
    a combination of supervised and unsupervised learning. It uses both labeled and
    unlabeled data to train the model. The labeled data is used to train the model
    on specific tasks, while the unlabeled data is used to help the algorithm learn
    patterns and relationships in the data [[131](#bib.bib131)]. In self-supervised
    learning, a model is trained on a dataset with labels that are automatically generated
    from the data itself. The goal is to learn useful representations of the data
    that can be used for downstream tasks, such as segmentation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种处理未标记训练数据中模式和关系发现的机器学习类型。在这种情况下，算法通过对数据点进行聚类或分组来学习识别模式和关系。另一方面，半监督学习是监督学习和无监督学习的结合。它使用标记和未标记的数据来训练模型。标记的数据用于训练模型进行特定任务，而未标记的数据用于帮助算法学习数据中的模式和关系[[131](#bib.bib131)]。在自监督学习中，模型在从数据本身自动生成标签的数据集上进行训练。目标是学习有用的数据表示，这些表示可以用于下游任务，例如分割。
- en: A common strategy for semi-supervised learning is to use label propagation through
    self-training. The process begins by training a classifier on labeled samples
    and then classifying the unlabeled samples. A selection of these samples based
    on an active selection strategy or learned classifier is then added to the training
    set and the process is repeated multiple times [[31](#bib.bib31)]. This can be
    performed either inductively or transductively. The former refers to training
    a model on unseen targets to add new information to the previously trained model
    so that it can generalize on new unseen data, and the latter to training a model
    based on a select subset of labeled and unlabeled data to be able to predict correctly
    on a limited set of seen targets.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习的一个常见策略是通过自训练使用标签传播。这个过程开始于对标记样本进行分类器训练，然后对未标记样本进行分类。基于主动选择策略或学习分类器选择的一些样本随后被添加到训练集中，且该过程会重复多次
    [[31](#bib.bib31)]。这可以是归纳进行，也可以是转导进行。前者指的是在未见目标上训练模型，以向之前训练的模型添加新信息，使其能够对新的未见数据进行泛化；后者指的是基于一部分标记和未标记数据训练模型，以便能够在有限的已见目标上正确预测。
- en: A semi-supervised approach was proposed by Takaya et al. [[111](#bib.bib111)]
    for the segmentation of neuronal membranes. They called their approach 4S that
    stands for sequential semi-supervised segmentation. It was based on the fact that
    adjacent images in a volume are strongly correlated. The goal of their method
    is to have a model that can only generalize to the next few slices instead of
    to the whole volume. This was achieved by starting with a few labeled slices that
    are used to train the first model. Then, in an iterative approach the model was
    used to infer the segmentation maps of a small set of subsequent images and the
    resulting segmentation maps were used as pseudolabels to retrain the model. Label
    propagation from labeled to the available unlabeled data was performed by predicting
    pseudo labels on the subsequent sections which represent the same targets and
    whose predictions could be included in the next round of model training as ground
    truth labels. It allowed the training to weigh the most recent inputs heavily
    unlike transfer learning where the goal is to generalize well on all use cases
    of the unlabeled dataset.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Takaya 等人提出了一种半监督方法 [[111](#bib.bib111)] 用于神经膜的分割。他们将这种方法称为4S，即序列半监督分割。该方法基于一个事实，即体积中的相邻图像具有很强的相关性。他们方法的目标是使模型仅对接下来的几个切片进行泛化，而不是对整个体积进行泛化。这通过从少量标记切片开始，这些切片用于训练第一个模型来实现。然后，通过迭代的方法，模型用于推断一小组后续图像的分割图，并将得到的分割图作为伪标签来重新训练模型。从标记数据到可用未标记数据的标签传播是通过在表示相同目标的后续部分上预测伪标签来完成的，这些预测可以作为真实标签包含在下一轮模型训练中。这使得训练能够更重视最近的输入，而不是像迁移学习那样，迁移学习的目标是在未标记数据集的所有用例上实现良好的泛化。
- en: Another semi-supervised method was introduced by Wolny et al. [[121](#bib.bib121)]
    for the segmentation of mitochondria and neuronal membranes. In contrast to the
    above, their goal was to train a model with a few manually annotated images, which
    can generalize for the whole dataset. In particular, they used a training set
    with a combination of positive labeled data and unlabeled data of positive and
    negative instances. As there is no direct supervision on the unlabeled part of
    the image, an embedding consistency term was introduced by training two networks
    on different data-augmented versions of each pixel. This was coupled with a push-pull
    loss function that they proposed to enforce constraints between different instances.
    It was realized by using anchor projections in the embedding space of a point
    in each instance to derive a soft label based on the set of surrounding pixels
    in the projected space. The instance segmentation was then achieved by grouping
    the pixel embeddings. This semi-supervised method is notable for a good tradeoff
    between segmentation performance and effort in manual annotation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种半监督方法由Wolny等人[[121](#bib.bib121)]提出，用于线粒体和神经膜的分割。与上述方法不同，他们的目标是用少量手动标注的图像训练一个可以推广到整个数据集的模型。特别是，他们使用了一个由正标注数据和正负实例的未标注数据组合的训练集。由于对未标注部分的图像没有直接监督，通过在每个像素的不同数据增强版本上训练两个网络引入了嵌入一致性项。这与他们提出的推拉损失函数相结合，以强制不同实例之间的约束。通过在每个实例的嵌入空间中使用锚投影来实现，根据投影空间中周围像素的集合推导出软标签。然后通过将像素嵌入分组来实现实例分割。这种半监督方法在分割性能和手动标注工作量之间取得了良好的平衡。
- en: 'Unsupervised learning was explored by Bermúdez-Chacón et al. [[10](#bib.bib10)],
    who investigated the unsupervised domain adaptation strategy for mitochondria
    segmentation to demonstrate how a model trained on one brain structure (source:
    mouse striatum) could be adapted to another brain structure (target: mouse hippocampus).
    Labeled data was only available to train the model on the source dataset (striatum).
    Visual correspondences were then used to determine pivot locations in the target
    dataset to characterize regions of mitochondria or synapses. These locations were
    then aggregated through a voting scheme to construct a consensus heatmap, which
    guided their model adaptation in two ways: a) optimizing model parameters to ensure
    agreement between predictions and their sets of correspondences, or b) incorporating
    high-scoring regions of the heatmap as soft labels in other domain adaptation
    pipelines. These unsupervised techniques yielded high-quality segmentations on
    unannotated volumes for mitochondria and synapses, consistent with results obtained
    under full supervision, without the need for new annotation effort.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习由Bermúdez-Chacón等人[[10](#bib.bib10)]进行了探讨，他们研究了线粒体分割的无监督领域自适应策略，以展示如何将一个在某脑结构（源：小鼠纹状体）上训练的模型适应到另一个脑结构（目标：小鼠海马）上。仅在源数据集（纹状体）上提供了标注数据来训练模型。然后使用视觉对应关系来确定目标数据集中枢点位置，以描述线粒体或突触的区域。这些位置通过投票机制汇总，构建了一个共识热图，该热图以两种方式指导了模型自适应：a）优化模型参数以确保预测与对应关系集之间的一致性，或b）将热图中的高分区域作为其他领域自适应管道中的软标签。这些无监督技术在未标注的线粒体和突触体积上产生了高质量的分割，与完全监督下获得的结果一致，无需新的标注工作。
- en: In the case of severe domain shifts such as from a FIB-SEM to an ssSEM dataset
    as investigated by Peng et al. [[97](#bib.bib97)], adversarial learning may be
    used for domain adaptation in different tissues of various species. Adversarial
    learning is a machine learning paradigm that trains a model with an adversarial
    loss function that encourages the model to learn domain-invariant features. Peng
    et al. [[97](#bib.bib97)] combined the geometrical cues from annotated labels
    with visual cues latent in images of both the source and target domains using
    adversarial domain adaptive multi-task learning. Instead of manually-defined shape
    priors, they learned geometrical cues from the source domain through adversarial
    learning, while jointly learning domain-invariant and discriminative features.
    By doing so, the model learned features that were useful for both source and target
    domains, and could perform well on the target domain despite having only labeled
    data in the source domain. The method was evaluated extensively on three benchmarks
    under various settings through ablations, parameter analysis, and comparisons,
    demonstrating its superior performance in segmentation accuracy and visual quality
    compared to state-of-the-art methods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在严重的领域转移情况下，比如从FIB-SEM到ssSEM数据集（如Peng等人[[97](#bib.bib97)]所研究的），可以使用对抗性学习进行不同物种组织的领域适应。对抗性学习是一种机器学习范式，通过对抗损失函数训练模型，促使模型学习领域不变特征。Peng等人[[97](#bib.bib97)]将注释标签中的几何线索与源域和目标域图像中潜在的视觉线索结合起来，使用对抗领域适应多任务学习。与手动定义的形状先验不同，他们通过对抗学习从源域中学习几何线索，同时共同学习领域不变和区分性特征。通过这样做，模型学到了对源域和目标域都有用的特征，并且尽管源域中只有标记数据，仍能在目标域上表现良好。该方法在三个基准数据集上经过广泛评估，包括消融实验、参数分析和比较，展示了在分割准确性和视觉质量方面优于最先进的方法。
- en: Contrastive learning is a self-supervised paradigm where a model is trained
    to learn useful representations of input data by contrasting similar and dissimilar
    samples. The basic idea is to take a set of positive pairs (e.g., two different
    augmentations of the same image) and a set of negative pairs (e.g., two images
    containing different types of objects), and train the model to assign higher similarity
    scores to positive pairs and lower similarity scores to negative pairs. This results
    in a model that captures the underlying structure of the data and can be used
    for downstream tasks like classification, object detection, and semantic segmentation.
    Conrad and Narayan [[34](#bib.bib34)] used contrastive learning, specifically
    moment contrast, [[55](#bib.bib55)], to learn useful feature representation from
    the unlabeled CEM500K dataset followed by transfer learning on given datasets.
    The heterogeneity of CEM500k coupled with the unsupervised initialization of a
    segmentation model contributed to achieving state-of-the-art results on six benchmark
    datasets that concern different types of organelles.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习是一种自监督范式，其中模型通过对比相似和不相似的样本来学习输入数据的有用表示。基本思路是取一组正对（例如，同一图像的两个不同增强）和一组负对（例如，包含不同类型对象的两张图像），训练模型将更高的相似度分数分配给正对，将较低的相似度分数分配给负对。这会导致模型捕捉到数据的潜在结构，可用于分类、目标检测和语义分割等下游任务。Conrad和Narayan[[34](#bib.bib34)]使用了对比学习，特别是moment
    contrast[[55](#bib.bib55)]，从未标记的CEM500K数据集中学习有用的特征表示，然后在给定的数据集上进行迁移学习。CEM500k的异质性以及分割模型的无监督初始化有助于在涉及不同类型细胞器的六个基准数据集上实现最先进的结果。
- en: 7 Segmentation evaluation metrics
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 分割评估指标
- en: Segmentation methods are evaluated by measuring the extent of overlap between
    the ground truth (GT) and prediction (PR) segmentation maps.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 分割方法通过测量地面真实值（GT）与预测（PR）分割图之间的重叠程度来评估。
- en: 'For semantic segmentation, all GT connected components are considered as one
    object, and similarly all PR connected components are treated as one object. This
    reduces the problem to binary classification. Typical performance measures include
    Accuracy, Precision and Recall and their harmonic mean (also called F1-score or
    Dice similarity coefficient (DSC)), the Jaccard Index (JI), also known as the
    Intersection over Union (IoU), and the Conformity coefficient Chang et al. [[23](#bib.bib23)],
    Fig. [5](#S7.F5 "Figure 5 ‣ 7 Segmentation evaluation metrics ‣ Segmentation in
    large-scale cellular electron microscopy with deep learning: A literature survey").'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '对于语义分割，所有GT连接组件被视为一个对象，类似地，所有PR连接组件也被视为一个对象。这将问题简化为二分类。典型的性能度量包括准确率（`Accuracy`）、精确率（`Precision`）和召回率（`Recall`）及其调和平均值（也称为F1得分或Dice相似系数（`DSC`））、Jaccard指数（`JI`），也称为交集比（`IoU`），以及一致性系数（Chang等人，[[23](#bib.bib23)]），图 [5](#S7.F5
    "Figure 5 ‣ 7 Segmentation evaluation metrics ‣ Segmentation in large-scale cellular
    electron microscopy with deep learning: A literature survey")。'
- en: '|  | <math   alttext="\begin{split}Accuracy&amp;=(TP+TN)/(TP+FP+FN+TN)\\ Precision~{}(P)&amp;=TP/(TP+FP)\\'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}准确率（`Accuracy`）&amp;=(TP+TN)/(TP+FP+FN+TN)\\
    精确率（`Precision~{}(P)`）&amp;=TP/(TP+FP)\\'
- en: Recall~{}(R)&amp;=TP/(TP+FN)\\
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率（`Recall~{}(R)`）&amp;=TP/(TP+FN)\\
- en: F_{1}~{}(or~{}DSC)&amp;=2PR/(P+R)\\
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: F_{1}（或`DSC`）&amp;=2PR/(P+R)\\
- en: JI~{}(or~{}IoU)&amp;=TP/(TP+FP+FN)\\
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: JI（或`IoU`）&amp;=TP/(TP+FP+FN)\\
- en: Conformity&amp;=1-(FN+FP)/TP\\
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性（`Conformity`）&amp;=1-(FN+FP)/TP\\
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000"
    >A</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >u</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >y</mi></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mo mathcolor="#000000" stretchy="false"
    >(</mo><mrow ><mrow ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow
    ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000"
    >N</mi></mrow></mrow><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow><mo
    mathcolor="#000000"  >/</mo><mrow ><mo mathcolor="#000000" stretchy="false"  >(</mo><mrow
    ><mrow ><mi mathcolor="#000000"  >T</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >P</mi></mrow><mo mathcolor="#000000"  >+</mo><mrow ><mi
    mathcolor="#000000"  >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >P</mi></mrow><mo
    mathcolor="#000000"  >+</mo><mrow ><mi mathcolor="#000000"  >F</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >N</mi></mrow><mo mathcolor="#000000"  >+</mo><mrow
    ><mi mathcolor="#000000"  >T</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >N</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000" >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >n</mi><mo lspace="0.330em"
    rspace="0em"  >​</mo><mrow ><mo mathcolor="#000000" stretchy="false"  >(</mo><mi
    mathcolor="#000000"  >P</mi><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mi mathcolor="#000000"
    >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo
    mathcolor="#000000" >/</mo><mrow ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow
    ><mrow ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000"
    >F</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000" >R</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >c</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >l</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >l</mi><mo lspace="0.330em" rspace="0em"  >​</mo><mrow
    ><mo mathcolor="#000000" stretchy="false"  >(</mo><mi mathcolor="#000000"  >R</mi><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mi mathcolor="#000000" >T</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000"
    >/</mo><mrow ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow ><mrow ><mi
    mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000"
    >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000" >F</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >N</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi mathcolor="#000000" >F</mi><mn mathcolor="#000000"
    >1</mn></msub><mo lspace="0.330em" rspace="0em" >​</mo><mrow ><mo mathcolor="#000000"
    stretchy="false"  >(</mo><mrow ><mi mathcolor="#000000" >o</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo lspace="0.330em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >D</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >S</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >C</mi></mrow><mo mathcolor="#000000"
    stretchy="false"  >)</mo></mrow></mrow></mtd><mtd columnalign="left"  ><mrow ><mo
    mathcolor="#000000" >=</mo><mrow ><mrow ><mn mathcolor="#000000" >2</mn><mo lspace="0em"
    rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi mathcolor="#000000" >R</mi></mrow><mo mathcolor="#000000" >/</mo><mrow
    ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow ><mi mathcolor="#000000"
    >P</mi><mo mathcolor="#000000" >+</mo><mi mathcolor="#000000" >R</mi></mrow><mo
    mathcolor="#000000" stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000" >J</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >I</mi><mo lspace="0.330em" rspace="0em"  >​</mo><mrow
    ><mo mathcolor="#000000" stretchy="false"  >(</mo><mrow ><mi mathcolor="#000000"
    >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo
    lspace="0.330em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >I</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >U</mi></mrow><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mi mathcolor="#000000"
    >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo
    mathcolor="#000000" >/</mo><mrow ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow
    ><mrow ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000"
    >F</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo
    mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000"  >F</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >N</mi></mrow></mrow><mo mathcolor="#000000"
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow
    ><mi mathcolor="#000000" >C</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >n</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >f</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >m</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >t</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >y</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo mathcolor="#000000" >=</mo><mrow ><mn mathcolor="#000000"
    >1</mn><mo mathcolor="#000000"  >−</mo><mrow ><mrow ><mrow ><mo mathcolor="#000000"
    stretchy="false" >(</mo><mrow ><mrow ><mi mathcolor="#000000" >F</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi mathcolor="#000000" >N</mi></mrow><mo mathcolor="#000000"
    >+</mo><mrow ><mi mathcolor="#000000" >F</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathcolor="#000000" >P</mi></mrow></mrow><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow><mo
    mathcolor="#000000"  >/</mo><mi mathcolor="#000000"  >T</mi></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >P</mi></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝐴</ci><ci  >𝑐</ci><ci
    >𝑐</ci><ci >𝑢</ci><ci  >𝑟</ci><ci >𝑎</ci><ci >𝑐</ci><ci  >𝑦</ci></apply><apply
    ><apply ><apply  ><apply ><ci >𝑇</ci><ci  >𝑃</ci></apply><apply ><ci >𝑇</ci><ci  >𝑁</ci></apply></apply><apply
    ><apply ><ci  >𝑇</ci><ci >𝑃</ci></apply><apply ><ci >𝐹</ci><ci >𝑃</ci></apply><apply
    ><ci  >𝐹</ci><ci >𝑁</ci></apply><apply ><ci >𝑇</ci><ci >𝑁</ci></apply></apply></apply><ci
    >𝑃</ci><ci  >𝑟</ci><ci >𝑒</ci><ci >𝑐</ci><ci  >𝑖</ci><ci >𝑠</ci><ci >𝑖</ci><ci  >𝑜</ci><ci
    >𝑛</ci><ci >𝑃</ci></apply></apply><apply ><apply  ><apply ><apply ><ci  >𝑇</ci><ci
    >𝑃</ci></apply><apply ><apply ><ci  >𝑇</ci><ci >𝑃</ci></apply><apply ><ci >𝐹</ci><ci
    >𝑃</ci></apply></apply></apply><ci >𝑅</ci><ci  >𝑒</ci><ci >𝑐</ci><ci >𝑎</ci><ci  >𝑙</ci><ci
    >𝑙</ci><ci >𝑅</ci></apply></apply><apply ><apply  ><apply ><apply ><ci  >𝑇</ci><ci
    >𝑃</ci></apply><apply ><apply ><ci  >𝑇</ci><ci >𝑃</ci></apply><apply ><ci >𝐹</ci><ci
    >𝑁</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐹</ci><cn type="integer" >1</cn></apply><apply ><ci  >𝑜</ci><ci >𝑟</ci><ci >𝐷</ci><ci  >𝑆</ci><ci
    >𝐶</ci></apply></apply></apply><apply ><apply ><apply  ><apply ><cn type="integer"
    >2</cn><ci >𝑃</ci><ci  >𝑅</ci></apply><apply ><ci >𝑃</ci><ci  >𝑅</ci></apply></apply><ci
    >𝐽</ci><ci >𝐼</ci><apply ><ci  >𝑜</ci><ci >𝑟</ci><ci >𝐼</ci><ci  >𝑜</ci><ci >𝑈</ci></apply></apply></apply><apply
    ><apply ><apply  ><apply ><ci >𝑇</ci><ci >𝑃</ci></apply><apply ><apply  ><ci >𝑇</ci><ci
    >𝑃</ci></apply><apply ><ci >𝐹</ci><ci >𝑃</ci></apply><apply ><ci  >𝐹</ci><ci >𝑁</ci></apply></apply></apply><ci
    >𝐶</ci><ci >𝑜</ci><ci >𝑛</ci><ci >𝑓</ci><ci >𝑜</ci><ci >𝑟</ci><ci >𝑚</ci><ci >𝑖</ci><ci
    >𝑡</ci><ci >𝑦</ci></apply></apply><apply ><apply ><cn type="integer" >1</cn><apply
    ><apply  ><apply ><apply ><ci  >𝐹</ci><ci >𝑁</ci></apply><apply ><ci >𝐹</ci><ci
    >𝑃</ci></apply></apply><ci >𝑇</ci></apply><ci >𝑃</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}Accuracy&=(TP+TN)/(TP+FP+FN+TN)\\ Precision~{}(P)&=TP/(TP+FP)\\
    Recall~{}(R)&=TP/(TP+FN)\\ F_{1}~{}(or~{}DSC)&=2PR/(P+R)\\ JI~{}(or~{}IoU)&=TP/(TP+FP+FN)\\
    Conformity&=1-(FN+FP)/TP\\ \end{split}</annotation></semantics></math> |  | (1)
    |
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000"
    >A</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >u</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >y</mi></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mo mathcolor="#000000" stretchy="false"
    >(</mo><mrow ><mrow ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow
    ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000"
    >N</mi></mrow></mrow><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow><mo
    mathcolor="#000000"  >/</mo><mrow ><mo mathcolor="#000000" stretchy="false"  >(</mo><mrow
    ><mrow ><mi mathcolor="#000000"  >T</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >P</mi></mrow><mo mathcolor="#000000"  >+</mo><mrow ><mi
    mathcolor="#000000"  >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >P</mi></mrow><mo
    mathcolor="#000000"  >+</mo><mrow ><mi mathcolor="#000000"  >F</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >N</mi></mrow><mo mathcolor="#000000"  >+</mo><mrow
    ><mi mathcolor="#000000"  >T</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >N</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000" >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >c</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >o</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >n</mi><mo lspace="0.330em"
    rspace="0em"  >​</mo><mrow ><mo mathcolor="#000000" stretchy="false"  >(</mo><mi
    mathcolor="#000000"  >P</mi><mo mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mi mathcolor="#000000"
    >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo
    mathcolor="#000000" >/</mo><mrow ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow
    ><mrow ><mi mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000"
    >F</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi mathcolor="#000000" >R</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >e</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    mathcolor="#000000"  >c</mi><mo lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi mathcolor="#000000"  >l</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >l</mi><mo lspace="0.330em" rspace="0em"  >​</mo><mrow
    ><mo mathcolor="#000000" stretchy="false"  >(</mo><mi mathcolor="#000000"  >R</mi><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mo mathcolor="#000000" >=</mo><mrow ><mrow ><mi mathcolor="#000000" >T</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >P</mi></mrow><mo mathcolor="#000000"
    >/</mo><mrow ><mo mathcolor="#000000" stretchy="false" >(</mo><mrow ><mrow ><mi
    mathcolor="#000000" >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000"
    >P</mi></mrow><mo mathcolor="#000000" >+</mo><mrow ><mi mathcolor="#000000" >F</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi mathcolor="#000000" >N</mi></mrow></mrow><mo
    mathcolor="#000000" stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi mathcolor="#000000" >F</mi><mn mathcolor="#000000"
    >1</mn></msub><mo lspace="0.330em" rspace="0em"  >​</mo><mrow ><mo mathcolor="#000000"
    stretchy="false"  >(</mo><mrow ><mi mathcolor="#000000" >o</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi mathcolor="#000000"  >r</mi><mo lspace="0.330em" rspace="0em"  >​</
- en: where TP, FP, FN, and TN are the number of true positives, false positives,
    false negatives, and true negatives at pixel level. The Accuracy measure is a
    ratio of all correctly classified pixels to all pixels, Precision is the ratio
    of all true positive pixels to the number of positive predictions made by the
    algorithm, and Recall (Sensitivity or True Positive Rate) is the ratio of all
    true positive pixels to the number of all positive pixels in the ground truth.
    The JI (or IoU) and DSC measure the similarity between the predicted class labels
    and the true class labels, while the Conformity coefficient measures the ratio
    of the number of misclassified pixels to the number of true positive pixels subtracted
    from 1\. A negative Conformity value indicates that the number of misclassified
    pixels is higher than the true positive ones, and vice-versa. Each of these metrics
    has its own strengths and weaknesses, and the choice of metric depends on the
    specific requirements of the classification task and the goals of the analysis.
    For example, accuracy is a simple and a good global measure but it is only suitable
    when the class distribution is balanced.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: TP、FP、FN 和 TN 分别是像素级别上的真正例、假正例、假负例和真负例的数量。准确度测量是所有正确分类像素与所有像素的比率，精度是所有真正例像素与算法做出的正预测数量的比率，召回率（灵敏度或真正例率）是所有真正例像素与地面真值中所有正像素数量的比率。JI（或
    IoU）和 DSC 衡量预测类别标签与真实类别标签之间的相似性，而一致性系数测量的是错误分类像素数量与真正例像素数量的比率，减去1。负的一致性值表示错误分类的像素数量高于真正例像素，反之亦然。这些指标各有优缺点，指标的选择取决于分类任务的具体要求和分析目标。例如，准确度是一个简单且有效的全局测量指标，但仅在类别分布平衡时适用。
- en: '![Refer to caption](img/42b29187d78fff8056a700c32c30068e.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42b29187d78fff8056a700c32c30068e.png)'
- en: 'Figure 5: Common performance metrics for segmentation methods. For semantic
    segmentation, the overall overlap of the ground truth (GT) mask with the prediction
    (PR) is compared without differentiating between objects of the foreground class.
    As to instance segmentation, each GT component is matched with only one PR component,
    the one with which it has the largest intersection. In the above example, the
    GT component ‘c’ overlaps with two PR components, ‘A’ and ‘B’, but is matched
    only with ‘B’ due to a larger overlap. The Aggregated Jaccard Index (AJI) is the
    ratio of the sum of all intersections of the matched pairs of GT and PR components
    to the sum of the unions of such pairs plus the sum of all pixels in the unmatched
    GT and PR components. The Panoptic Quality (PQ) captures both semantic and instance
    segmentation performance. The former is the sum of all IoUs between the matched
    GT and PR components divided by the number of matched components (TPs), and the
    latter is the number of TPs divided by the number of TPs plus half of the FPs
    and FNs together. The symbol $|.|$ indicates the cardinality of the set concerned.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：分割方法的常见性能指标。对于语义分割，地面真值（GT）掩码与预测（PR）的整体重叠度被比较，而不区分前景类别的对象。对于实例分割，每个 GT 组件与唯一一个
    PR 组件匹配，即与其交集最大的那个。在上述示例中，GT 组件‘c’与两个 PR 组件‘A’和‘B’重叠，但由于与‘B’的重叠较大，因此仅与‘B’匹配。聚合杰卡德指数（AJI）是匹配的
    GT 和 PR 组件对的所有交集之和与这些对的并集之和加上所有未匹配的 GT 和 PR 组件中的像素总和的比率。全景质量（PQ）同时捕捉语义和实例分割性能。前者是匹配的
    GT 和 PR 组件之间所有 IoU 的总和除以匹配组件（TPs）的数量，后者是 TPs 的数量除以 TPs 数量加上 FPs 和 FNs 数量的一半的总和。符号
    $|.|$ 表示相关集合的基数。
- en: For the segmentation of partitions, such as neuronal structures, the preservation
    of the topology is more important than the pixel-based accuracy. For instance,
    a prediction that oversegments (e.g. splitting the delineation of a neuron in
    two or more partitions) should be penalized more than a prediction of displaced,
    shrunk or expanded segments. Metrics such as the Rand Index (RI), Warping error
    (WE), and variation of information (VOI) take into account the topological errors
    in neuronal membrane segmentation. RI measures the similarity between the PR and
    GT segmentation maps, by calculating the sum of pairs of pixels that are both
    in the same object and both in different objects out of the total combination
    of pixel pairs in both GT and PR maps [[117](#bib.bib117), [2](#bib.bib2)]. The
    complement of the RI (i.e. 1 - RI) is known as the Rand Error (RE). The adapted
    rand error (ARAND) was the evaluation metric used in the SNEMI 3D challenge and
    is given as 1 - the maximal $F$-score of the RI. The maximal $F$-score is achieved
    when the precision and recall are at their optimal trade-off. The RI provides
    a score ranging from 0 to 1, with a value of 1 indicating perfect matching between
    two objects.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像神经结构这样的分割任务，拓扑的保留比基于像素的准确性更为重要。例如，过度分割的预测（例如，将一个神经元的轮廓分割成两个或多个部分）应比预测位移、收缩或扩展的分割受到更多惩罚。像Rand指数（RI）、变形误差（WE）和信息变异（VOI）这样的指标考虑了神经膜分割中的拓扑错误。RI通过计算PR和GT分割图中相同对象和不同对象的像素对的总和来衡量PR和GT分割图之间的相似性[[117](#bib.bib117),
    [2](#bib.bib2)]。RI的补集（即1 - RI）被称为Rand误差（RE）。调整后的Rand误差（ARAND）是SNEMI 3D挑战赛中使用的评估指标，定义为1
    - RI的最大$F$-分数。最大$F$-分数在精确度和召回率达到最佳折衷时获得。RI提供一个从0到1的分数，其中值为1表示两个对象之间的完美匹配。
- en: 'Other metrics that were part of the ISBI 2012 challenge are the WE and pixel
    error (PE). The WE is a segmentation metric that penalizes topological disagreements,
    i.e: the number of splits and mergers required to obtain the desired segmentation.
    On the other hand, the PE is defined as the ratio of the number of pixel locations
    at which the GT and PR labelings disagree. While expanding, shrinking, or translating
    a boundary between two neurons does not affect the WE, they incur a large PE.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ISBI 2012挑战赛中的其他指标包括**WE**和**像素误差（PE）**。WE是一种分割指标，用于惩罚拓扑不一致，即获取所需分割所需的拆分和合并数量。另一方面，PE定义为GT和PR标注不一致的像素位置的比例。尽管扩展、收缩或平移两个神经元之间的边界不会影响WE，但会导致较大的PE。
- en: 'The variation of information (VOI) quantifies the distance between PR and GT
    objects by measuring the amount of information that is lost or gained when one
    segmentation is transformed into the other [[2](#bib.bib2)]. The VOI between the
    GT and PR components is the sum of two conditional entropies: the first one, $H(PR|GT)$,
    is a measure of over-segmentation, the second one, $H(GT|PR)$, a measure of under-segmentation.
    These measures are referred to as the VOI split or merge error, respectively.
    The VOI and ARAND were also combined to form the CREMI score by first taking the
    sum of the VOI split and VOI merge and then combining the result with ARAND using
    geometric mean.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 信息变异（VOI）通过测量将一个分割转换为另一个分割时丢失或获得的信息量来量化PR和GT对象之间的距离[[2](#bib.bib2)]。GT和PR组件之间的VOI是两个条件熵的总和：第一个$H(PR|GT)$是过度分割的度量，第二个$H(GT|PR)$是欠分割的度量。这些度量分别称为VOI拆分或合并误差。VOI和ARAND还被结合形成CREMI评分，首先计算VOI拆分和VOI合并的总和，然后将结果与ARAND通过几何平均结合。
- en: Evaluations of segmentation quality were most accurately reflected by the normalized
    versions of the RI and VOI, which are denoted by $V_{rand}$ and $V_{info}$, respectively
    [[4](#bib.bib4)]. Given a pair of GT and PR segmentation maps, $V_{rand}$ provides
    a weighted harmonic mean of the split and merge errors, where the split error
    is the probability of two selected pixels belonging to the same segment in PR
    given that they belong to the same segment in GT, and vice versa for the merge
    error. In pixel pair classification, the split and merge scores can be seen as
    representing precision and recall, respectively, for identifying whether the pixels
    belong to the same object (true positives) or different objects (true negatives).
    When the split and merge errors are weighed equally it is known as the Rand $F$-score.
    Similarly, $V_{info}$ is given by the mutual harmonic mean of the information-theoretic
    split and merge scores, which defines how much information in PR is provided by
    GT and vice versa.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 分割质量的评估最准确地反映在RI和VOI的归一化版本中，它们分别表示为$V_{rand}$和$V_{info}$ [[4](#bib.bib4)]。给定一对GT和PR分割图，$V_{rand}$提供了分裂和合并误差的加权调和均值，其中分裂误差是指在GT中属于同一分段的两个选定像素在PR中属于同一分段的概率，合并误差则相反。在像素对分类中，分裂和合并分数可以视为分别代表识别像素是否属于同一对象（真正例）或不同对象（真正负例）的精度和召回率。当分裂和合并误差被同等权重时，称为Rand
    $F$-score。类似地，$V_{info}$由信息理论分裂和合并分数的互调和均值给出，定义了GT提供了多少PR中的信息，反之亦然。
- en: For the evaluation of object detection, where different connected components
    are treated as different objects, the above measures are also applicable. The
    main difference is the way a true positive is considered. In object detection
    a PR region is considered a TP if it overlaps with more than a given threshold
    (e.g. 50%) a GT component in terms of IoU, otherwise it is a FP. The unmatched
    GT components are then considered as FNs. A popular metric in object detection
    is average precision (AP), which is essentially the area under the precision-recall
    curve that is determined by systematically changing the detection threshold. The
    default AP measure uses a 50% IoU overlap threshold, but other variations of the
    AP can be used depending on how strict the evaluation must be. The term AP-$\alpha$
    denotes the average precision at a given IoU threshold $\alpha$. The higher the
    $\alpha$ the stricter the evaluation is. In problems with more than two classes,
    the mean AP (mAP) can be used to aggregate all the APs of all the classes involved
    by taking their average.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于物体检测的评估，其中不同的连通组件被视为不同的对象，上述测量方法同样适用。主要区别在于真正的正例（true positive）的定义。在物体检测中，如果一个PR区域与一个GT组件在IoU（交并比）方面的重叠超过给定的阈值（例如50%），则该PR区域被视为TP，否则为FP。未匹配的GT组件则被视为FN。物体检测中的一个常用指标是平均精度（AP），它本质上是通过系统地改变检测阈值来确定的精度-召回曲线下的面积。默认的AP测量使用50%的IoU重叠阈值，但根据评估的严格程度，可以使用AP的其他变体。术语AP-$\alpha$表示在给定IoU阈值$\alpha$下的平均精度。$\alpha$越高，评估越严格。在具有两个以上类别的问题中，可以使用平均AP（mAP）来通过取所有涉及类别的AP的平均值来汇总所有AP。
- en: 'Instance segmentation requires more detailed measures to quantify the segmentation
    mask accuracy along with the detection performance. Metrics such as the aggregated
    Jaccard index (AJI) and the Panoptic Quality (PQ), which were originally proposed
    by [[72](#bib.bib72)] and [[70](#bib.bib70)], respectively, have also been used
    in EM [[88](#bib.bib88), [127](#bib.bib127)] to evaluate instance segmentation
    algorithms more comprehensively. See Fig. [5](#S7.F5 "Figure 5 ‣ 7 Segmentation
    evaluation metrics ‣ Segmentation in large-scale cellular electron microscopy
    with deep learning: A literature survey") for an example.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '实例分割需要更详细的指标来量化分割掩模的准确性以及检测性能。诸如聚合Jaccard指数（AJI）和全景质量（PQ）等指标，分别由[[72](#bib.bib72)]和[[70](#bib.bib70)]最初提出，也在EM
    [[88](#bib.bib88), [127](#bib.bib127)]中被用来更全面地评估实例分割算法。请参见图[5](#S7.F5 "Figure
    5 ‣ 7 Segmentation evaluation metrics ‣ Segmentation in large-scale cellular electron
    microscopy with deep learning: A literature survey")中的示例。'
- en: '|  | <math   alttext="\begin{split}AJI&amp;=\frac{\sum_{j=1}^{N}&#124;GT^{j}\cap
    PR^{j^{*}}&#124;}{\sum_{j=1}^{N}&#124;GT^{j}\cup PR^{j^{*}}&#124;+\sum_{i\in FP}&#124;PR^{i}&#124;}\\
    PQ&amp;=\frac{\sum_{j\in TP}JI(GT^{j},PR^{j^{*}})}{&#124;TP&#124;}\times\frac{&#124;TP&#124;}{&#124;TP&#124;+\frac{1}{2}&#124;FP&#124;+\frac{1}{2}&#124;FN&#124;}\\'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}AJI&amp;=\frac{\sum_{j=1}^{N}&#124;GT^{j}\cap
    PR^{j^{*}}&#124;}{\sum_{j=1}^{N}&#124;GT^{j}\cup PR^{j^{*}}&#124;+\sum_{i\in FP}&#124;PR^{i}&#124;}\\
    PQ&amp;=\frac{\sum_{j\in TP}JI(GT^{j},PR^{j^{*}})}{&#124;TP&#124;}\times\frac{&#124;TP&#124;}{&#124;TP&#124;+\frac{1}{2}&#124;FP&#124;+\frac{1}{2}&#124;FN&#124;}\\'
- en: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi  >A</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >J</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >I</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mfrac  ><mrow ><msubsup ><mo  >∑</mo><mrow
    ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup><mrow ><mo lspace="0em"
    stretchy="false"  >&#124;</mo><mrow ><mrow ><mi >G</mi><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >T</mi><mi >j</mi></msup></mrow><mo >∩</mo><mrow ><mi >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mrow ><mrow ><msubsup ><mo >∑</mo><mrow
    ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup><mrow ><mo lspace="0em"
    stretchy="false" >&#124;</mo><mrow ><mrow ><mi >G</mi><mo lspace="0em" rspace="0em"
    >​</mo><msup ><mi >T</mi><mi >j</mi></msup></mrow><mo >∪</mo><mrow ><mi >P</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mo rspace="0.055em"  >+</mo><mrow
    ><msub ><mo rspace="0em" >∑</mo><mrow ><mi >i</mi><mo >∈</mo><mrow ><mi >F</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow></mrow></msub><mrow ><mo stretchy="false"  >&#124;</mo><mrow
    ><mi >P</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi >R</mi><mi >i</mi></msup></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow></mrow></mfrac></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi  >P</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >Q</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow  ><mfrac
    ><mrow ><msub  ><mo >∑</mo><mrow ><mi >j</mi><mo >∈</mo><mrow ><mi >T</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow></mrow></msub><mrow ><mi >J</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >I</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mi >G</mi><mo lspace="0em" rspace="0em" >​</mo><msup
    ><mi >T</mi><mi >j</mi></msup></mrow><mo >,</mo><mrow ><mi >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mo stretchy="false" >&#124;</mo><mrow
    ><mi >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow><mo stretchy="false"
    >&#124;</mo></mrow></mfrac><mo lspace="0.222em" rspace="0.222em"  >×</mo><mfrac
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mi >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mrow ><mi >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mo >+</mo><mrow
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >&#124;</mo><mrow ><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow></mrow><mo >+</mo><mrow
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >&#124;</mo><mrow ><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >N</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝐴</ci><ci  >𝐽</ci><ci
    >𝐼</ci></apply><apply ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑗</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝑁</ci></apply><apply ><apply ><apply ><ci >𝐺</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑇</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑃</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑗</ci></apply></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑗</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝑁</ci></apply><apply ><apply ><apply ><ci >𝐺</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑇</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑃</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑗</ci></apply></apply></apply></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci >𝑖</ci><apply
    ><ci >𝐹</ci><ci >𝑃</ci></apply></apply></apply><apply ><apply ><ci >𝑃</ci><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑅</ci><ci >𝑖</ci></apply></apply></apply></apply></apply></apply><ci
    >𝑃</ci><ci >𝑄</ci></apply></apply><apply ><apply  ><apply ><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑗</ci><apply ><ci >𝑇</ci><ci
    >𝑃</ci></apply></apply></apply><apply ><ci  >𝐽</ci><ci >𝐼</ci><interval closure="open"  ><apply
    ><ci >𝐺</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑇</ci><ci
    >𝑗</ci></apply></apply><apply ><ci >𝑃</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑗</ci></apply></apply></apply></interval></apply></apply><apply
    ><apply ><ci  >𝑇</ci><ci >𝑃</ci></apply></apply></apply><apply ><apply ><apply
    ><ci >𝑇</ci><ci >𝑃</ci></apply></apply><apply ><apply ><apply ><ci >𝑇</ci><ci
    >𝑃</ci></apply></apply><apply ><apply ><cn type="integer" >1</cn><cn type="integer"
    >2</cn></apply><apply ><apply ><ci >𝐹</ci><ci >𝑃</ci></apply></apply></apply><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply><apply
    ><apply ><ci >𝐹</ci><ci >𝑁</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}AJI&=\frac{\sum_{j=1}^{N}&#124;GT^{j}\cap
    PR^{j^{*}}&#124;}{\sum_{j=1}^{N}&#124;GT^{j}\cup PR^{j^{*}}&#124;+\sum_{i\in FP}&#124;PR^{i}&#124;}\\
    PQ&=\frac{\sum_{j\in TP}JI(GT^{j},PR^{j^{*}})}{&#124;TP&#124;}\times\frac{&#124;TP&#124;}{&#124;TP&#124;+\frac{1}{2}&#124;FP&#124;+\frac{1}{2}&#124;FN&#124;}\\
    \end{split}</annotation></semantics></math> |  | (2) |
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi  >A</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >J</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >I</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mo >=</mo><mfrac  ><mrow ><msubsup ><mo  >∑</mo><mrow
    ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup><mrow ><mo lspace="0em"
    stretchy="false"  >&#124;</mo><mrow ><mrow ><mi >G</mi><mo lspace="0em" rspace="0em"  >​</mo><msup
    ><mi >T</mi><mi >j</mi></msup></mrow><mo >∩</mo><mrow ><mi >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mrow ><mrow ><msubsup ><mo >∑</mo><mrow
    ><mi >j</mi><mo >=</mo><mn >1</mn></mrow><mi >N</mi></msubsup><mrow ><mo lspace="0em"
    stretchy="false" >&#124;</mo><mrow ><mrow ><mi >G</mi><mo lspace="0em" rspace="0em"
    >​</mo><msup ><mi >T</mi><mi >j</mi></msup></mrow><mo >∪</mo><mrow ><mi >P</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow><mo rspace="0.055em"  >+</mo><mrow
    ><msub ><mo rspace="0em" >∑</mo><mrow ><mi >i</mi><mo >∈</mo><mrow ><mi >F</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow></mrow></msub><mrow ><mo stretchy="false"  >&#124;</mo><mrow
    ><mi >P</mi><mo lspace="0em" rspace="0em"  >​</mo><msup ><mi >R</mi><mi >i</mi></msup></mrow><mo
    stretchy="false"  >&#124;</mo></mrow></mrow></mrow></mfrac></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><mi  >P</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >Q</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mo >=</mo><mrow  ><mfrac
    ><mrow ><msub  ><mo >∑</mo><mrow ><mi >j</mi><mo >∈</mo><mrow ><mi >T</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow></mrow></msub><mrow ><mi >J</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >I</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mi >G</mi><mo lspace="0em" rspace="0em" >​</mo><msup
    ><mi >T</mi><mi >j</mi></msup></mrow><mo >,</mo><mrow ><mi >P</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msup ><mi >R</mi><msup ><mi >j</mi><mo >∗</mo></msup></msup></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow ><mo stretchy="false" >&#124;</mo><mrow
    ><mi >T</mi><mo lspace="0em" rspace="0em" >​</mo><mi >P</mi></mrow><mo stretchy="false"
    >&#124;</mo></mrow></mfrac><mo lspace="0.222em" rspace="0.222em"  >×</mo><mfrac
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mi >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mrow ><mi >T</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mo >+</mo><mrow
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >&#124;</mo><mrow ><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >P</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow></mrow><mo >+</mo><mrow
    ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >&#124;</mo><mrow ><mi >F</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >N</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><ci >𝐴</ci><ci  >𝐽</ci><ci
    >𝐼</ci></apply><apply ><apply ><apply  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑗</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝑁</ci></apply><apply ><apply ><apply ><ci >𝐺</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑇</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑃</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑗</ci></apply></apply></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><ci >𝑗</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝑁</ci></apply><apply ><apply ><apply ><ci >𝐺</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑇</ci><ci >𝑗</ci></apply></apply><apply ><ci >𝑃</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝑗</ci></apply></apply></apply></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><ci >𝑖</ci><apply
    ><ci >𝐹</ci
- en: where $N$ is the number of GT regions, and $j^{\ast}$ is the index of the connected
    region in $PR$ that is matched with the largest overlapping region (in terms of
    JI) with ground truth segment $GT^{j}$; FP is the set of false positive segments
    in $PR$ without the corresponding ground truth regions in $GT$, FN is the set
    of false negative segments in $GT$ that have been left unmatched with any regions
    in $PR$ and TP is the set of all matched regions in $GT$ and $PR$ with at least
    50% overlap in JI. The symbol $|.|$ indicates the cardinality of a given set.
    A GT component can only be used once to match with a PR component. In case there
    are multiple PR components overlapping the same GT component, the GT component
    will only be matched with the PR component having the largest IoU. The AJI is
    an object-level performance metric which measures the ability of a segmentation
    algorithm to accurately identify and delineate individual objects within an image.
    It takes into account both the segmentation quality and the accuracy of object
    identification. For problems where many GT regions are apposing or in very close
    proximity with each other (e.g. mitochondria in 2D EM), there is a high risk that
    one PR region overlaps multiple GT regions. Such cases are overpenalised by the
    AJI measure. Overpenalization is prevented to happen with PQ because the matching
    of PR and GT regions are only valid when they overlap with more than 50% in JI.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是GT区域的数量，$j^{\ast}$ 是在 $PR$ 中与地面实况分割 $GT^{j}$ 具有最大重叠区域（以JI为标准）匹配的连通区域的索引；FP
    是在 $PR$ 中没有对应地面实况区域的假阳性分割集合，FN 是在 $GT$ 中未与 $PR$ 中的任何区域匹配的假阴性分割集合，而TP 是 $GT$ 和
    $PR$ 中所有匹配区域的集合，并且在JI中至少有50%的重叠。符号 $|.|$ 表示给定集合的基数。一个GT组件只能使用一次来匹配一个PR组件。如果有多个PR组件与同一GT组件重叠，则GT组件仅与具有最大IoU的PR组件匹配。AJI是一个物体级性能指标，用于测量分割算法准确识别和勾画图像中个体物体的能力。它考虑了分割质量和物体识别的准确性。对于许多GT区域彼此相邻或非常接近的情况（例如2D
    EM中的线粒体），存在一个PR区域重叠多个GT区域的高风险。这种情况会被AJI测量过度惩罚。PQ可以防止过度惩罚，因为PR和GT区域的匹配仅在它们在JI中重叠超过50%时才有效。
- en: 8 Discussion and open challenges
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与开放挑战
- en: Convolutional neural networks have become the standard choice for automatic
    feature extraction and segmentation in EM data. The most notable backbone networks
    are FCN and U-Net. Their use of deeper contextual network architectures is essential
    for accurate 2D prediction and by extension for 3D reconstruction. To produce
    dense predictions, early methods used a stack of successive convolutions followed
    by spatial pooling. Consecutive methods upsample high-level feature maps and combine
    them with low-level feature maps to restore crisp object boundaries and global
    information during decoding. To extend the receptive field of convolutions in
    the initial layers, numerous techniques have advocated the use of dilated or atrous
    convolutions. Recent works use spatial pyramid pooling to gather multi-scale contextual
    information in order to acquire global information in upper levels. More specialized
    architectures came into prominence to solve certain problems of anisotropy using
    hybrid 2D-3D networks and have now become the de facto for anisotropic EM datasets.
    The extension of 3D networks for graph-based affinity labeling proved useful as
    they can efficiently model structures across volume stacks to avoid several postprocessing
    steps for 3D reconstruction.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络已成为EM数据自动特征提取和分割的标准选择。最著名的骨干网络是FCN和U-Net。它们采用更深层次的上下文网络架构对于准确的2D预测和进而的3D重建至关重要。为了生成密集的预测，早期方法使用了一系列连续的卷积和空间池化。连续的方法通过上采样高级特征图并将其与低级特征图结合，以在解码过程中恢复清晰的物体边界和全局信息。为了扩展初始层中卷积的感受野，许多技术提倡使用扩张卷积或空洞卷积。近期的工作使用空间金字塔池化来收集多尺度的上下文信息，以便在上层获得全局信息。更专业的架构开始受到关注，用以解决某些各向异性问题，通过混合2D-3D网络，现在已成为各向异性EM数据集的事实标准。3D网络在图基亲和标记中的扩展证明了其有效性，因为它们可以有效地建模体积堆栈中的结构，避免多个3D重建的后处理步骤。
- en: The advent of state-of-the-art deep neural networks led to a saturation of segmentation
    performance on small datasets, such as the ones from Ciresan et al. [[33](#bib.bib33)]
    and Lucchi et al. [[87](#bib.bib87)]. Despite the focus in many studies on improving
    network architectures, a lot of the differences in performance can in fact be
    attributed to variations in preprocessing, data augmentation, and postprocessing
    [[62](#bib.bib62), [47](#bib.bib47)]. It is therefore likely that a focus on these
    areas in the near future will lead to new milestones in EM segmentation performance.
    Many of these developments, whether in network design or in postproceesing, have
    been in fully supervised segmentation, a technique that is greatly limited by
    the availability and quality of annotated data. While the number of large-scale
    annotated EM datasets has dramatically risen in recent years, the published datasets
    are not always precisely annotated, and are often composed of crude masks built
    semi-automatically using pre-trained networks and proofreading.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的深度神经网络的出现导致了在小型数据集上的分割性能饱和，例如来自Ciresan等人[[33](#bib.bib33)]和Lucchi等人[[87](#bib.bib87)]的数据集。尽管许多研究专注于改进网络架构，但性能差异在很大程度上可以归因于预处理、数据增强和后处理的变化[[62](#bib.bib62),
    [47](#bib.bib47)]。因此，未来对这些领域的关注很可能会在EM分割性能上带来新的突破。许多这些进展，无论是在网络设计还是后处理方面，都集中在完全监督的分割上，这种技术在标注数据的可用性和质量上受到很大限制。尽管近年来大规模标注EM数据集的数量急剧增加，但发布的数据集并不总是准确标注的，且通常由使用预训练网络和校对构建的粗略掩模组成。
- en: This limiting scarcity of annotation in EM is due to the complexity of the produced
    images, their large scale, and the ways in which the annotations are obtained.
    Manual annotations can either be performed by one or a few domain experts, or
    they could be performed collaboratively by large groups in a crowd-sourced manner.
    Expert annotations are more accurate and time consuming. For instance, the large-scale
    connectomics project required extensive labeling and proofreading [[100](#bib.bib100)].
    Crowd-sourced approaches on the other hand require additional organizational efforts,
    specialized software, and instruction of the participants [[110](#bib.bib110)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: EM中标注稀缺的限制是由于所产生图像的复杂性、其大规模以及标注获取的方式。手动标注可以由一个或几个领域专家进行，也可以通过众包方式由大型团队合作完成。专家标注更为准确但耗时。例如，大规模的连接组学项目需要广泛的标注和校对[[100](#bib.bib100)]。另一方面，众包方法则需要额外的组织工作、专业软件和参与者的指导[[110](#bib.bib110)]。
- en: In addition to manual annotation, EM images can be labeled using specialized
    imaging modalities that target specific structures in the sample. For instance,
    CLEM (Correlative light electron microscopy) is used to label structures targeted
    with fluorescent probes at (sub)cellular scales [[14](#bib.bib14), [41](#bib.bib41),
    [59](#bib.bib59)]. Other EM modalities that could assist in annotation include
    energy dispersive X-ray spectroscopy (EDX), electron energy loss spectroscopy
    (EELS), cathodoluminescence (CL), and secondary ion mass spectroscopy at the nanoscale
    (NanoSIMS) [[99](#bib.bib99)]. These methods reduce the bias in human annotation,
    but may require longer sample preparation, specialized equipment, or additional
    image processing to produce segmentations.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了手动标注，EM图像还可以使用专门的成像模式来标记样本中特定结构。例如，CLEM（相关光电子显微镜）用于标记用荧光探针靶向的（亚）细胞尺度的结构[[14](#bib.bib14),
    [41](#bib.bib41), [59](#bib.bib59)]。其他可能有助于标注的EM模式包括能量色散X射线光谱（EDX）、电子能量损失光谱（EELS）、阴极发光（CL）和纳米尺度的二次离子质谱（NanoSIMS）[[99](#bib.bib99)]。这些方法减少了人工标注的偏差，但可能需要更长的样本准备时间、专用设备或额外的图像处理以生成分割结果。
- en: The scarcity of annotated EM data could also be addressed algorithmically by
    relying on semi-supervised and unsupervised learning techniques. These techniques
    are able to segment EM images with minimal or no annotations, that can scale to
    larger datasets with varied structures. Few-shot learning for segmentation has
    shown promising results in natural images [[39](#bib.bib39), [113](#bib.bib113)],
    and the use of transformers in segmentation could prove useful for large-scale
    EM data in the future [[40](#bib.bib40), [129](#bib.bib129)].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过依赖半监督和无监督学习技术，稀缺的标注 EM 数据也可以通过算法方式得到解决。这些技术能够对 EM 图像进行分割，几乎不需要或完全不需要标注，并且可以扩展到具有不同结构的大型数据集。针对自然图像的少量样本学习在分割方面显示了有前景的结果[[39](#bib.bib39),
    [113](#bib.bib113)]，而变压器在分割中的应用对于未来的大规模 EM 数据可能会证明有用[[40](#bib.bib40), [129](#bib.bib129)]。
- en: These prospects of label-free segmentation highlight the importance of collecting
    unlabeled yet relevant segmentation datasets, like the curation of unlabeled heterogeneous
    mitochondria images in CEM500K. Such datasets have shown how unlabeled pre-training
    using self-supervision could pave the way for breakthroughs in EM segmentation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 无标签分割的前景突显了收集无标签但相关的分割数据集的重要性，例如 CEM500K 中无标签异质线粒体图像的策划。这些数据集展示了使用自监督的无标签预训练如何为电子显微镜（EM）分割的突破奠定基础。
- en: Complex datasets with challenges such as MitoEM, NucMM, or even unlabeled datasets
    such as CEM500K led to deep generalist models rather than specialized networks
    but still have a long way to go as sub-cellular image segmentation using large-scale
    EM is yet to explore both challenges in biological research and computer vision.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 面临挑战的复杂数据集如 MitoEM、NucMM，甚至无标签数据集如 CEM500K，导致了深度通用模型的出现，而非专业化网络，但在使用大规模 EM 进行亚细胞图像分割时仍需克服长远的挑战，这一领域尚待探索生物研究和计算机视觉中的挑战。
- en: 9 Acknowledgement
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: 'This project has received funding from the Centre for Data Science and Systems
    Complexity at the University of Groningen⁸⁸8[www.rug.nl/research/fse/themes/dssc/](www.rug.nl/research/fse/themes/dssc/).
    Part of the work has been sponsored by ZonMW grant 91111.006; the Netherlands
    Electron Microscopy Infrastructure (NEMI), NWO National Roadmap for Large-Scale
    Research Infrastructure of the Dutch Research Council (NWO 184.034.014); the Network
    for Pancreatic Organ donors with Diabetes (nPOD; RRID:SCR${}_{0}14641$), a collaborative
    T1D research project sponsored by JDRF (nPOD: $5-SRA-2018-557-Q-R$) and The Leona
    M. & Harry B. Helmsley Charitable Trust (Grant $2018PG-T1D053$). The content and
    views expressed are the responsibility of the authors and do not necessarily reflect
    the official view of nPOD. Organ Procurement Organizations (OPO) partnering with
    nPOD to provide research resources are listed in [http://www.jdrfnpod.org/for-partners/npod-partners/](http://www.jdrfnpod.org/for-partners/npod-partners/).
    Thanks are also due to Kim Kats for her assistance in preparing Fig. 1.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '本项目获得了格罗宁根大学数据科学与系统复杂性中心的资助⁸⁸8[www.rug.nl/research/fse/themes/dssc/](www.rug.nl/research/fse/themes/dssc/)。部分工作由
    ZonMW 资助 91111.006；荷兰电子显微镜基础设施（NEMI），荷兰研究委员会（NWO 184.034.014）的 NWO 大规模研究基础设施国家路线图；糖尿病胰腺器官捐赠者网络（nPOD；RRID:SCR${}_{0}14641$），由
    JDRF 赞助的协作 T1D 研究项目（nPOD: $5-SRA-2018-557-Q-R$）以及 Leona M. 和 Harry B. Helmsley
    慈善信托（资助 $2018PG-T1D053$）。内容和观点仅代表作者，未必反映 nPOD 的官方观点。与 nPOD 合作提供研究资源的器官采购组织（OPO）名单见
    [http://www.jdrfnpod.org/for-partners/npod-partners/](http://www.jdrfnpod.org/for-partners/npod-partners/)。还要感谢
    Kim Kats 协助准备图 1。'
- en: References
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdollahzadeh et al. [2021] Abdollahzadeh, A., Belevich, I., Jokitalo, E., Sierra,
    A., Tohka, J., 2021. Deepacson automated segmentation of white matter in 3D electron
    microscopy. Communications biology 4, 1–14.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdollahzadeh 等人 [2021] Abdollahzadeh, A., Belevich, I., Jokitalo, E., Sierra,
    A., Tohka, J., 2021. Deepacson 自动分割白质的 3D 电子显微镜。生物学通讯 4, 1–14。
- en: Arbelaez et al. [2010] Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., 2010.
    Contour detection and hierarchical image segmentation. IEEE transactions on pattern
    analysis and machine intelligence 33, 898–916.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbelaez 等人 [2010] Arbelaez, P., Maire, M., Fowlkes, C., Malik, J., 2010. 轮廓检测和层次图像分割。IEEE
    模式分析与机器智能交易 33, 898–916。
- en: 'Arganda-Carreras et al. [2017] Arganda-Carreras, I., Kaynig, V., Rueden, C.,
    Eliceiri, K.W., Schindelin, J., Cardona, A., Sebastian Seung, H., 2017. Trainable
    Weka Segmentation: a machine learning tool for microscopy pixel classification.
    Bioinformatics 33, 2424–2426.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arganda-Carreras 等人 [2017] Arganda-Carreras, I., Kaynig, V., Rueden, C., Eliceiri,
    K.W., Schindelin, J., Cardona, A., Sebastian Seung, H., 2017. Trainable Weka 分割：一种用于显微镜像素分类的机器学习工具。生物信息学
    33, 2424–2426。
- en: Arganda-Carreras et al. [2015] Arganda-Carreras, I., Turaga, S.C., Berger, D.R.,
    Cireşan, D., Giusti, A., Gambardella, L.M., Schmidhuber, J., Laptev, D., Dwivedi,
    S., Buhmann, J.M., et al., 2015. Crowdsourcing the creation of image segmentation
    algorithms for connectomics. Frontiers in neuroanatomy , 142.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arganda-Carreras 等人 [2015] Arganda-Carreras, I., Turaga, S.C., Berger, D.R.,
    Cireşan, D., Giusti, A., Gambardella, L.M., Schmidhuber, J., Laptev, D., Dwivedi,
    S., Buhmann, J.M., 等人, 2015. 众包创建连接组学图像分割算法。神经解剖学前沿 , 142。
- en: 'Badrinarayanan et al. [2017] Badrinarayanan, V., Kendall, A., Cipolla, R.,
    2017. Segnet: A deep convolutional encoder-decoder architecture for image segmentation.
    IEEE transactions on pattern analysis and machine intelligence 39, 2481–2495.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Badrinarayanan 等人 [2017] Badrinarayanan, V., Kendall, A., Cipolla, R., 2017.
    Segnet：一种用于图像分割的深度卷积编码器-解码器架构。IEEE 模式分析与机器智能事务 39, 2481–2495。
- en: 'Bai and Urtasun [2017] Bai, M., Urtasun, R., 2017. Deep watershed transform
    for instance segmentation, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 5221–5229.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 和 Urtasun [2017] Bai, M., Urtasun, R., 2017. 实例分割的深度分水岭变换，发表于：IEEE 计算机视觉与模式识别会议论文集,
    页码：5221–5229。
- en: 'Bailoni et al. [2022] Bailoni, A., Pape, C., Hütsch, N., Wolf, S., Beier, T.,
    Kreshuk, A., Hamprecht, F.A., 2022. Gasp, a generalized framework for agglomerative
    clustering of signed graphs and its application to instance segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 11645–11655.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bailoni 等人 [2022] Bailoni, A., Pape, C., Hütsch, N., Wolf, S., Beier, T., Kreshuk,
    A., Hamprecht, F.A., 2022. Gasp，一种用于有符号图的聚类的广义框架及其在实例分割中的应用，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集,
    页码：11645–11655。
- en: 'Belevich and Jokitalo [2021] Belevich, I., Jokitalo, E., 2021. Deepmib: user-friendly
    and open-source software for training of deep learning network for biological
    image segmentation. PLoS computational biology 17, e1008374.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belevich 和 Jokitalo [2021] Belevich, I., Jokitalo, E., 2021. Deepmib：用于生物图像分割的深度学习网络训练的用户友好且开源的软件。PLoS
    计算生物学 17, e1008374。
- en: 'Berg et al. [2019] Berg, S., Kutra, D., Kroeger, T., Straehle, C.N., Kausler,
    B.X., Haubold, C., Schiegg, M., Ales, J., Beier, T., Rudy, M., et al., 2019. Ilastik:
    interactive machine learning for (bio) image analysis. Nature Methods 16, 1226–1232.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berg 等人 [2019] Berg, S., Kutra, D., Kroeger, T., Straehle, C.N., Kausler, B.X.,
    Haubold, C., Schiegg, M., Ales, J., Beier, T., Rudy, M., 等人, 2019. Ilastik：用于（生物）图像分析的交互式机器学习。自然方法
    16, 1226–1232。
- en: Bermúdez-Chacón et al. [2019] Bermúdez-Chacón, R., Altingövde, O., Becker, C.,
    Salzmann, M., Fua, P., 2019. Visual correspondences for unsupervised domain adaptation
    on electron microscopy images. IEEE transactions on medical imaging 39, 1256–1267.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bermúdez-Chacón 等人 [2019] Bermúdez-Chacón, R., Altingövde, O., Becker, C., Salzmann,
    M., Fua, P., 2019. 用于电子显微镜图像的无监督领域适应的视觉对应。IEEE 医学影像学事务 39, 1256–1267。
- en: 'Bermúdez-Chacón et al. [2018] Bermúdez-Chacón, R., Márquez-Neila, P., Salzmann,
    M., Fua, P., 2018. A domain-adaptive two-stream U-Net for electron microscopy
    image segmentation, in: 2018 IEEE 15th International Symposium on Biomedical Imaging
    (ISBI 2018), IEEE. pp. 400–404.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bermúdez-Chacón 等人 [2018] Bermúdez-Chacón, R., Márquez-Neila, P., Salzmann,
    M., Fua, P., 2018. 用于电子显微镜图像分割的领域自适应双流 U-Net，发表于：2018 IEEE 第15届国际生物医学成像研讨会 (ISBI
    2018)，IEEE. 页码：400–404。
- en: 'Berning et al. [2015] Berning, M., Boergens, K.M., Helmstaedter, M., 2015.
    Segem: efficient image analysis for high-resolution connectomics. Neuron 87, 1193–1206.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berning 等人 [2015] Berning, M., Boergens, K.M., Helmstaedter, M., 2015. Segem：高分辨率连接组学的高效图像分析。神经元
    87, 1193–1206。
- en: 'de Boer and Giepmans [2021] de Boer, P., Giepmans, B.N., 2021. State-of-the-art
    microscopy to understand islets of langerhans: what to expect next? Immunology
    and Cell Biology 99, 509–520.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Boer 和 Giepmans [2021] de Boer, P., Giepmans, B.N., 2021. 了解朗格汉斯岛的最先进显微镜技术：接下来会有什么？免疫学与细胞生物学
    99, 509–520。
- en: 'de Boer et al. [2015] de Boer, P., Hoogenboom, J., Giepmans, B., 2015. Correlated
    light and electron microscopy: ultrastructure lights up! Nature Methods 12, 503–513.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Boer 等人 [2015] de Boer, P., Hoogenboom, J., Giepmans, B., 2015. 相关光学与电子显微镜：超结构亮起来了！自然方法
    12, 503–513。
- en: de Boer et al. [2020] de Boer, P., Pirozzi, N.M., Wolters, A.H., Kuipers, J.,
    Kusmartseva, I., Atkinson, M.A., Campbell-Thompson, M., Giepmans, B.N., 2020.
    Large-scale electron microscopy database for human type 1 diabetes. Nature communications
    11, 1–9.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Boer 等人 [2020] de Boer, P., Pirozzi, N.M., Wolters, A.H., Kuipers, J., Kusmartseva,
    I., Atkinson, M.A., Campbell-Thompson, M., Giepmans, B.N., 2020. 大规模电子显微镜数据库用于人类
    1 型糖尿病。Nature communications 11, 1–9。
- en: Briggman et al. [2011] Briggman, K.L., Helmstaedter, M., Denk, W., 2011. Wiring
    specificity in the direction-selectivity circuit of the retina. Nature 471, 183–188.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Briggman 等人 [2011] Briggman, K.L., Helmstaedter, M., Denk, W., 2011. 视网膜方向选择电路中的连接特异性。Nature
    471, 183–188。
- en: Cao et al. [2019] Cao, L., Lu, Y., Li, C., Yang, W., 2019. Automatic segmentation
    of pathological glomerular basement membrane in transmission electron microscopy
    images with random forest stacks. Computational and mathematical methods in medicine
    2019.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曹等人 [2019] 曹, L., 陆, Y., 李, C., 杨, W., 2019. 使用随机森林堆栈自动分割透射电子显微镜图像中的病理性肾小管基膜。Computational
    and mathematical methods in medicine 2019。
- en: 'Cao et al. [2020] Cao, Y., Liu, S., Peng, Y., Li, J., 2020. Denseunet: densely
    connected unet for electron microscopy image segmentation. IET Image Processing
    14, 2682–2689.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '曹等人 [2020] 曹, Y., 刘, S., 彭, Y., 李, J., 2020. Denseunet: 用于电子显微镜图像分割的密集连接 Unet。IET
    Image Processing 14, 2682–2689。'
- en: 'Carpenter et al. [2006] Carpenter, A.E., Jones, T.R., Lamprecht, M.R., Clarke,
    C., Kang, I.H., Friman, O., Guertin, D.A., Chang, J.H., Lindquist, R.A., Moffat,
    J., et al., 2006. Cellprofiler: image analysis software for identifying and quantifying
    cell phenotypes. Genome biology 7, 1–11.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carpenter 等人 [2006] Carpenter, A.E., Jones, T.R., Lamprecht, M.R., Clarke,
    C., Kang, I.H., Friman, O., Guertin, D.A., Chang, J.H., Lindquist, R.A., Moffat,
    J., 等人, 2006. Cellprofiler: 用于识别和量化细胞表型的图像分析软件。Genome biology 7, 1–11。'
- en: 'Carvalho et al. [2018] Carvalho, L., Sobieranski, A.C., von Wangenheim, A.,
    2018. 3d segmentation algorithms for computerized tomographic imaging: a systematic
    literature review. Journal of digital imaging 31, 799–850.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carvalho 等人 [2018] Carvalho, L., Sobieranski, A.C., von Wangenheim, A., 2018.
    计算机断层扫描成像的 3D 分割算法：系统文献综述。Journal of digital imaging 31, 799–850。
- en: Casser et al. [2018] Casser, V., Kang, K., Pfister, H., Haehn, D., 2018. Fast
    mitochondria segmentation for connectomics.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casser 等人 [2018] Casser, V., Kang, K., Pfister, H., Haehn, D., 2018. 用于连接组学的快速线粒体分割。
- en: von Chamier et al. [2021] von Chamier, L., Laine, R.F., Jukkala, J., Spahn,
    C., Krentzel, D., Nehme, E., Lerche, M., Hernández-Pérez, S., Mattila, P.K., Karinou,
    E., et al., 2021. Democratising deep learning for microscopy with zerocostdl4mic.
    Nature communications 12, 1–18.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: von Chamier 等人 [2021] von Chamier, L., Laine, R.F., Jukkala, J., Spahn, C.,
    Krentzel, D., Nehme, E., Lerche, M., Hernández-Pérez, S., Mattila, P.K., Karinou,
    E., 等人, 2021. 使用 zerocostdl4mic 实现显微镜深度学习的民主化。Nature communications 12, 1–18。
- en: Chang et al. [2009] Chang, H.H., Zhuang, A.H., Valentino, D.J., Chu, W.C., 2009.
    Performance measure characterization for evaluating neuroimage segmentation algorithms.
    Neuroimage 47, 122–135.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常等人 [2009] 常, H.H., 庄, A.H., Valentino, D.J., 朱, W.C., 2009. 评估神经图像分割算法的性能测量特征。Neuroimage
    47, 122–135。
- en: 'Chen et al. [2017a] Chen, H., Qi, X., Yu, L., Dou, Q., Qin, J., Heng, P.A.,
    2017a. Dcan: Deep contour-aware networks for object instance segmentation from
    histology images. Medical image analysis 36, 135–146.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2017a] 陈, H., 齐, X., 于, L., 陶, Q., 秦, J., 横, P.A., 2017a. DCAN: 从组织学图像中进行对象实例分割的深度轮廓感知网络。Medical
    image analysis 36, 135–146。'
- en: Chen et al. [2014] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,
    A.L., 2014. Semantic image segmentation with deep convolutional nets and fully
    connected crfs. arXiv preprint arXiv:1412.7062 .
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2014] 陈, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.,
    2014. 使用深度卷积网络和全连接 CRFs 的语义图像分割。arXiv 预印本 arXiv:1412.7062。
- en: 'Chen et al. [2017b] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,
    A.L., 2017b. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis
    and machine intelligence 40, 834–848.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2017b] 陈, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.,
    2017b. Deeplab: 使用深度卷积网络、空洞卷积和全连接 CRFs 的语义图像分割。IEEE transactions on pattern analysis
    and machine intelligence 40, 834–848。'
- en: Chen et al. [2017c] Chen, L.C., Papandreou, G., Schroff, F., Adam, H., 2017c.
    Rethinking atrous convolution for semantic image segmentation. arXiv preprint
    arXiv:1706.05587 .
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2017c] 陈, L.C., Papandreou, G., Schroff, F., Adam, H., 2017c. 重新思考用于语义图像分割的空洞卷积。arXiv
    预印本 arXiv:1706.05587。
- en: 'Chen et al. [2018] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam,
    H., 2018. Encoder-decoder with atrous separable convolution for semantic image
    segmentation, in: Proceedings of the European conference on computer vision (ECCV),
    pp. 801–818.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等 [2018] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018.
    使用空洞可分离卷积的编码器-解码器进行语义图像分割，见：欧洲计算机视觉会议（ECCV）论文集，页码 801–818。
- en: Chen et al. [2017d] Chen, M., Dai, W., Sun, S.Y., Jonasch, D., He, C.Y., Schmid,
    M.F., Chiu, W., Ludtke, S.J., 2017d. Convolutional neural networks for automated
    annotation of cellular cryo-electron tomograms. Nature methods 14, 983–985.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等 [2017d] Chen, M., Dai, W., Sun, S.Y., Jonasch, D., He, C.Y., Schmid, M.F.,
    Chiu, W., Ludtke, S.J., 2017d. 用于自动标注细胞冷冻电子断层扫描的卷积神经网络。自然方法 14, 983–985。
- en: 'Cheng and Varshney [2017] Cheng, H.C., Varshney, A., 2017. Volume segmentation
    using convolutional neural networks with limited training data, in: 2017 IEEE
    international conference on image processing (ICIP), IEEE. pp. 590–594.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng和Varshney [2017] Cheng, H.C., Varshney, A., 2017. 使用有限训练数据的卷积神经网络进行体积分割，见：2017
    IEEE国际图像处理会议（ICIP），IEEE，页码 590–594。
- en: 'Cheplygina et al. [2019] Cheplygina, V., de Bruijne, M., Pluim, J.P., 2019.
    Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning
    in medical image analysis. Medical Image Analysis 54, 280–296.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheplygina等 [2019] Cheplygina, V., de Bruijne, M., Pluim, J.P., 2019. 非完全监督：医学图像分析中的半监督、多实例和迁移学习综述。医学图像分析
    54, 280–296。
- en: 'Çiçek et al. [2016] Çiçek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger,
    O., 2016. 3d u-net: learning dense volumetric segmentation from sparse annotation,
    in: International conference on medical image computing and computer-assisted
    intervention, Springer. pp. 424–432.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Çiçek等 [2016] Çiçek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger,
    O., 2016. 3d u-net：从稀疏标注中学习密集体积分割，见：医学图像计算与计算机辅助干预国际会议，Springer，页码 424–432。
- en: Ciresan et al. [2012] Ciresan, D., Giusti, A., Gambardella, L., Schmidhuber,
    J., 2012. Deep neural networks segment neuronal membranes in electron microscopy
    images. Advances in neural information processing systems 25, 2843–2851.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciresan等 [2012] Ciresan, D., Giusti, A., Gambardella, L., Schmidhuber, J., 2012.
    深度神经网络在电子显微镜图像中分割神经膜。神经信息处理系统进展 25, 2843–2851。
- en: Conrad and Narayan [2021] Conrad, R., Narayan, K., 2021. Cem500k, a large-scale
    heterogeneous unlabeled cellular electron microscopy image dataset for deep learning.
    Elife 10, e65894.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conrad和Narayan [2021] Conrad, R., Narayan, K., 2021. Cem500k，一个用于深度学习的大规模异质未标记细胞电子显微镜图像数据集。Elife
    10, e65894。
- en: Dai et al. [2013] Dai, W., Fu, C., Raytcheva, D., Flanagan, J., Khant, H.A.,
    Liu, X., Rochat, R.H., Haase-Pettingell, C., Piret, J., Ludtke, S.J., et al.,
    2013. Visualizing virus assembly intermediates inside marine cyanobacteria. Nature
    502, 707–710.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai等 [2013] Dai, W., Fu, C., Raytcheva, D., Flanagan, J., Khant, H.A., Liu,
    X., Rochat, R.H., Haase-Pettingell, C., Piret, J., Ludtke, S.J., 等, 2013. 可视化海洋蓝藻内部的病毒组装中间体。自然
    502, 707–710。
- en: De Brabandere et al. [2017] De Brabandere, B., Neven, D., Gool, L.V., 2017.
    Semantic instance segmentation with a discriminative loss function. [arXiv:1708.02551](http://arxiv.org/abs/1708.02551).
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Brabandere等 [2017] De Brabandere, B., Neven, D., Gool, L.V., 2017. 使用判别损失函数进行语义实例分割。
    [arXiv:1708.02551](http://arxiv.org/abs/1708.02551)。
- en: Dietlmeier et al. [2019] Dietlmeier, J., McGuinness, K., Rugonyi, S., Wilson,
    T., Nuttall, A., O’Connor, N.E., 2019. Few-shot hypercolumn-based mitochondria
    segmentation in cardiac and outer hair cells in focused ion beam-scanning electron
    microscopy FIB-SEM data. Pattern recognition letters 128, 521--528.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dietlmeier等 [2019] Dietlmeier, J., McGuinness, K., Rugonyi, S., Wilson, T.,
    Nuttall, A., O’Connor, N.E., 2019. 基于少量样本的超柱状线粒体分割在聚焦离子束扫描电子显微镜FIB-SEM数据中。模式识别快报
    128, 521--528。
- en: Dittmayer et al. [2021] Dittmayer, C., Goebel, H.H., Heppner, F.L., Stenzel,
    W., Bachmann, S., 2021. Preparation of samples for large-scale automated electron
    microscopy of tissue and cell ultrastructure. Microscopy and Microanalysis 27,
    815--827.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dittmayer等 [2021] Dittmayer, C., Goebel, H.H., Heppner, F.L., Stenzel, W., Bachmann,
    S., 2021. 大规模自动化电子显微镜样品准备，用于组织和细胞超微结构。显微学与微分析 27, 815--827。
- en: 'Dong and Xing [2018] Dong, N., Xing, E.P., 2018. Few-shot semantic segmentation
    with prototype learning., in: BMVC.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong和Xing [2018] Dong, N., Xing, E.P., 2018. 基于原型学习的少样本语义分割，见：BMVC。
- en: 'Dosovitskiy et al. [2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Transformers
    for image recognition at scale. [arXiv:2010.11929](http://arxiv.org/abs/2010.11929).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等 [2021] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., Houlsby, N., 2021. 一张图像价值 16x16 个词：用于大规模图像识别的 Transformers。[arXiv:2010.11929](http://arxiv.org/abs/2010.11929)。
- en: Drawitsch et al. [2018] Drawitsch, F., Karimi, A., Boergens, K.M., Helmstaedter,
    M., 2018. Fluoem, virtual labeling of axons in three-dimensional electron microscopy
    data for long-range connectomics. Elife 7, e38976.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drawitsch 等 [2018] Drawitsch, F., Karimi, A., Boergens, K.M., Helmstaedter,
    M., 2018. Fluoem，在三维电子显微镜数据中虚拟标记轴突以进行长程连接组学。Elife 7, e38976。
- en: 'Drozdzal et al. [2016] Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury,
    S., Pal, C., 2016. The importance of skip connections in biomedical image segmentation,
    in: Deep learning and data labeling for medical applications. Springer, pp. 179--187.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drozdzal 等 [2016] Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal,
    C., 2016. 在生物医学图像分割中跳跃连接的重要性，见：深度学习和医学应用的数据标注。Springer, 第179--187页。
- en: Eberle et al. [2015] Eberle, A., Mikula, S., Schalek, R., Lichtman, J., Tate,
    M.K., Zeidler, D., 2015. High-resolution, high-throughput imaging with a multibeam
    scanning electron microscope. Journal of microscopy 259, 114--120.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eberle 等 [2015] Eberle, A., Mikula, S., Schalek, R., Lichtman, J., Tate, M.K.,
    Zeidler, D., 2015. 使用多束扫描电子显微镜进行高分辨率、高通量成像。显微镜学杂志 259, 114--120。
- en: 'Ede [2021] Ede, J.M., 2021. Deep learning in electron microscopy. Machine Learning:
    Science and Technology 2, 011004.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ede [2021] Ede, J.M., 2021. 电子显微镜中的深度学习。机器学习：科学与技术 2, 011004。
- en: 'Faas et al. [2012] Faas, F.G., Avramut, M.C., M. van den Berg, B., Mommaas,
    A.M., Koster, A.J., Ravelli, R.B., 2012. Virtual nanoscopy: generation of ultra-large
    high resolution electron microscopy maps. Journal of Cell Biology 198, 457--469.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faas 等 [2012] Faas, F.G., Avramut, M.C., M. van den Berg, B., Mommaas, A.M.,
    Koster, A.J., Ravelli, R.B., 2012. 虚拟纳米显微术：生成超大高分辨率电子显微镜图谱。细胞生物学杂志 198, 457--469。
- en: Fakhry et al. [2017] Fakhry, A., Zeng, T., Ji, S., 2017. Residual deconvolutional
    networks for brain electron microscopy image segmentation. IEEE transactions on
    medical imaging 36, 447--456.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fakhry 等 [2017] Fakhry, A., Zeng, T., Ji, S., 2017. 用于脑部电子显微镜图像分割的残差反卷积网络。IEEE
    医学成像交易 36, 447--456。
- en: Franco-Barranco et al. [2022] Franco-Barranco, D., Muñoz-Barrutia, A., Arganda-Carreras,
    I., 2022. Stable deep neural network architectures for mitochondria segmentation
    on electron microscopy volumes. Neuroinformatics 20, 437--450.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franco-Barranco 等 [2022] Franco-Barranco, D., Muñoz-Barrutia, A., Arganda-Carreras,
    I., 2022. 用于线粒体分割的稳定深度神经网络架构，基于电子显微镜体积。神经信息学 20, 437--450。
- en: Frangakis and Hegerl [2002] Frangakis, A.S., Hegerl, R., 2002. Segmentation
    of two-and three-dimensional data from electron microscopy using eigenvector analysis.
    Journal of Structural Biology 138, 105--113.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frangakis 和 Hegerl [2002] Frangakis, A.S., Hegerl, R., 2002. 使用特征向量分析对电子显微镜的二维和三维数据进行分割。结构生物学杂志
    138, 105--113。
- en: Funke et al. [2018] Funke, J., Tschopp, F., Grisaitis, W., Sheridan, A., Singh,
    C., Saalfeld, S., Turaga, S.C., 2018. Large scale image segmentation with structured
    loss based deep learning for connectome reconstruction. IEEE transactions on pattern
    analysis and machine intelligence 41, 1669--1680.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Funke 等 [2018] Funke, J., Tschopp, F., Grisaitis, W., Sheridan, A., Singh, C.,
    Saalfeld, S., Turaga, S.C., 2018. 使用基于结构化损失的深度学习进行大规模图像分割以重建连通组。IEEE 模式分析与机器智能交易
    41, 1669--1680。
- en: Ghosh et al. [2019] Ghosh, S., Das, N., Das, I., Maulik, U., 2019. Understanding
    deep learning techniques for image segmentation. ACM Computing Surveys (CSUR)
    52, 1--35.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh 等 [2019] Ghosh, S., Das, N., Das, I., Maulik, U., 2019. 理解图像分割的深度学习技术。ACM
    计算调查 (CSUR) 52, 1--35。
- en: Glancy et al. [2015] Glancy, B., Hartnell, L.M., Malide, D., Yu, Z.X., Combs,
    C.A., Connelly, P.S., Subramaniam, S., Balaban, R.S., 2015. Mitochondrial reticulum
    for cellular energy distribution in muscle. Nature 523, 617--620.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glancy 等 [2015] Glancy, B., Hartnell, L.M., Malide, D., Yu, Z.X., Combs, C.A.,
    Connelly, P.S., Subramaniam, S., Balaban, R.S., 2015. 用于肌肉中细胞能量分布的线粒体网络。自然 523,
    617--620。
- en: Guay et al. [2021] Guay, M.D., Emam, Z.A., Anderson, A.B., Aronova, M.A., Pokrovskaya,
    I.D., Storrie, B., Leapman, R.D., 2021. Dense cellular segmentation for em using
    2d--3d neural network ensembles. Scientific reports 11, 1--11.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guay 等 [2021] Guay, M.D., Emam, Z.A., Anderson, A.B., Aronova, M.A., Pokrovskaya,
    I.D., Storrie, B., Leapman, R.D., 2021. 利用 2d--3d 神经网络集成进行电子显微镜的密集细胞分割。科学报告 11,
    1--11。
- en: Haberl et al. [2018a] Haberl, M.G., Churas, C., Tindall, L., Boassa, D., Phan,
    S., Bushong, E.A., Madany, M., Akay, R., Deerinck, T.J., Peltier, S.T., et al.,
    2018a. Cdeep3m—plug-and-play cloud-based deep learning for image segmentation.
    Nature methods 15, 677--680.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haberl 等人 [2018a] Haberl, M.G., Churas, C., Tindall, L., Boassa, D., Phan, S.,
    Bushong, E.A., Madany, M., Akay, R., Deerinck, T.J., Peltier, S.T., 等，2018a. Cdeep3m—即插即用的基于云的深度学习图像分割。Nature
    methods 15, 677--680。
- en: Haberl et al. [2018b] Haberl, M.G., Churas, C., Tindall, L., Boassa, D., Phan,
    S., Bushong, E.A., Madany, M., Akay, R., Deerinck, T.J., Peltier, S.T., et al.,
    2018b. Cdeep3m—plug-and-play cloud-based deep learning for image segmentation.
    Nature methods 15, 677--680.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haberl 等人 [2018b] Haberl, M.G., Churas, C., Tindall, L., Boassa, D., Phan, S.,
    Bushong, E.A., Madany, M., Akay, R., Deerinck, T.J., Peltier, S.T., 等，2018b. Cdeep3m—即插即用的基于云的深度学习图像分割。Nature
    methods 15, 677--680。
- en: 'He et al. [2020] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020. Momentum
    contrast for unsupervised visual representation learning, in: Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pp. 9729--9738.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2020] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R., 2020. 动量对比用于无监督视觉表示学习，见于：IEEE/CVF
    计算机视觉与模式识别会议论文集，页 9729--9738。
- en: 'He et al. [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask
    R-CNN, in: Proceedings of the IEEE international conference on computer vision,
    pp. 2961--2969.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2017] He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask R-CNN，见于：IEEE
    国际计算机视觉会议论文集，页 2961--2969。
- en: 'He et al. [2015] He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving deep into
    rectifiers: Surpassing human-level performance on imagenet classification, in:
    Proceedings of the IEEE international conference on computer vision, pp. 1026--1034.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2015] He, K., Zhang, X., Ren, S., Sun, J., 2015. 深入研究整流器：在 imagenet 分类中超越人类水平，见于：IEEE
    国际计算机视觉会议论文集，页 1026--1034。
- en: 'He et al. [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
    for image recognition, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 770--778.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] He, K., Zhang, X., Ren, S., Sun, J., 2016. 深度残差学习用于图像识别，见于：IEEE
    计算机视觉与模式识别会议论文集，页 770--778。
- en: Heinrich et al. [2021] Heinrich, L., Bennett, D., Ackerman, D., Park, W., Bogovic,
    J., Eckstein, N., Petruncio, A., Clements, J., Pang, S., Xu, C.S., et al., 2021.
    Whole-cell organelle segmentation in volume electron microscopy. Nature , 1--6.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heinrich 等人 [2021] Heinrich, L., Bennett, D., Ackerman, D., Park, W., Bogovic,
    J., Eckstein, N., Petruncio, A., Clements, J., Pang, S., Xu, C.S., 等，2021. 体积电子显微镜中的全细胞细胞器分割。Nature
    , 1--6。
- en: 'Heinrich et al. [2018] Heinrich, L., Funke, J., Pape, C., Nunez-Iglesias, J.,
    Saalfeld, S., 2018. Synaptic cleft segmentation in non-isotropic volume electron
    microscopy of the complete drosophila brain, in: International Conference on Medical
    Image Computing and Computer-Assisted Intervention, Springer. pp. 317--325.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heinrich 等人 [2018] Heinrich, L., Funke, J., Pape, C., Nunez-Iglesias, J., Saalfeld,
    S., 2018. 完整果蝇脑的非各向异性体积电子显微镜中的突触裂缝分割，见于：医学图像计算与计算机辅助手术国际会议，Springer。页 317--325。
- en: Helmstaedter et al. [2013] Helmstaedter, M., Briggman, K.L., Turaga, S.C., Jain,
    V., Seung, H.S., Denk, W., 2013. Connectomic reconstruction of the inner plexiform
    layer in the mouse retina. Nature 500, 168--174.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helmstaedter 等人 [2013] Helmstaedter, M., Briggman, K.L., Turaga, S.C., Jain,
    V., Seung, H.S., Denk, W., 2013. 小鼠视网膜内层复合体的连通体重建。Nature 500, 168--174。
- en: 'Isensee et al. [2019] Isensee, F., Petersen, J., Kohl, S.A., Jäger, P.F., Maier-Hein,
    K.H., 2019. nnu-net: Breaking the spell on successful medical image segmentation.
    arXiv preprint arXiv:1904.08128 1, 2.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isensee 等人 [2019] Isensee, F., Petersen, J., Kohl, S.A., Jäger, P.F., Maier-Hein,
    K.H., 2019. nnu-net：破解成功医学图像分割的魔咒。arXiv 预印本 arXiv:1904.08128 1, 2。
- en: Januszewski et al. [2018] Januszewski, M., Kornfeld, J., Li, P.H., Pope, A.,
    Blakely, T., Lindsey, L., Maitin-Shepard, J., Tyka, M., Denk, W., Jain, V., 2018.
    High-precision automated reconstruction of neurons with flood-filling networks.
    Nature methods 15, 605--610.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Januszewski 等人 [2018] Januszewski, M., Kornfeld, J., Li, P.H., Pope, A., Blakely,
    T., Lindsey, L., Maitin-Shepard, J., Tyka, M., Denk, W., Jain, V., 2018. 使用洪水填充网络进行高精度自动神经元重建。Nature
    methods 15, 605--610。
- en: 'Jiang et al. [2019] Jiang, Y., Xiao, C., Li, L., Chen, X., Shen, L., Han, H.,
    2019. An effective encoder-decoder network for neural cell bodies and cell nucleus
    segmentation of em images, in: 2019 41st Annual International Conference of the
    IEEE Engineering in Medicine and Biology Society (EMBC), IEEE. pp. 6302--6305.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2019] Jiang, Y., Xiao, C., Li, L., Chen, X., Shen, L., Han, H., 2019.
    一种有效的编码解码网络用于神经细胞体和细胞核分割，见于：2019年第41届IEEE医学与生物工程年会（EMBC），IEEE。页 6302--6305。
- en: Karabağ et al. [2019] Karabağ, C., Jones, M.L., Peddie, C.J., Weston, A.E.,
    Collinson, L.M., Reyes-Aldasoro, C.C., 2019. Segmentation and modelling of the
    nuclear envelope of hela cells imaged with serial block face scanning electron
    microscopy. Journal of Imaging 5, 75.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karabağ等人 [2019] Karabağ, C., Jones, M.L., Peddie, C.J., Weston, A.E., Collinson,
    L.M., Reyes-Aldasoro, C.C., 2019. 使用串联断面扫描电子显微镜成像对hela细胞核膜的分割与建模。《成像杂志》5, 75。
- en: Kasthuri et al. [2015] Kasthuri, N., Hayworth, K.J., Berger, D.R., Schalek,
    R.L., Conchello, J.A., Knowles-Barley, S., Lee, D., Vázquez-Reina, A., Kaynig,
    V., Jones, T.R., et al., 2015. Saturated reconstruction of a volume of neocortex.
    Cell 162, 648--661.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasthuri等人 [2015] Kasthuri, N., Hayworth, K.J., Berger, D.R., Schalek, R.L.,
    Conchello, J.A., Knowles-Barley, S., Lee, D., Vázquez-Reina, A., Kaynig, V., Jones,
    T.R., et al., 2015. 大脑新皮质体积的饱和重建。《细胞》162, 648--661.
- en: 'Khadangi et al. [2021a] Khadangi, A., Boudier, T., Rajagopal, V., 2021a. Em-net:
    Deep learning for electron microscopy image segmentation, in: 2020 25th International
    Conference on Pattern Recognition (ICPR), IEEE. pp. 31--38.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khadangi等人 [2021a] Khadangi, A., Boudier, T., Rajagopal, V., 2021a. Em-net:
    用于电子显微镜图像分割的深度学习, in:《2020年第25届国际模式识别大会（ICPR）》, IEEE. pp. 31--38.'
- en: 'Khadangi et al. [2021b] Khadangi, A., Boudier, T., Rajagopal, V., 2021b. Em-stellar:
    benchmarking deep learning for electron microscopy image segmentation. Bioinformatics
    37, 97--106.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khadangi等人 [2021b] Khadangi, A., Boudier, T., Rajagopal, V., 2021b. Em-stellar:
    基于电子显微镜图像分割的深度学习基准。《生物信息学》37, 97--106.'
- en: Kievits et al. [2022] Kievits, A.J., Lane, R., Carroll, E.C., Hoogenboom, J.P.,
    2022. How innovations in methodology offer new prospects for volume electron microscopy.
    Journal of Microscopy 287, 114--137.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kievits等人 [2022] Kievits, A.J., Lane, R., Carroll, E.C., Hoogenboom, J.P., 2022.
    方法论创新为体积电子显微镜带来新的前景。《显微镜杂志》287, 114--137.
- en: 'Kirillov et al. [2019] Kirillov, A., He, K., Girshick, R., Rother, C., Dollar,
    P., 2019. Panoptic segmentation, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kirillov等人 [2019] Kirillov, A., He, K., Girshick, R., Rother, C., Dollar, P.,
    2019. 全景分割, in: 《IEEE/CVF计算机视觉与模式识别（CVPR）会议论文集》。'
- en: Krizhevsky et al. [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012.
    Imagenet classification with deep convolutional neural networks. Advances in neural
    information processing systems 25, 1097--1105.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等人 [2012] Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. 使用深度卷积神经网络进行imagenet分类。《神经信息处理系统进展》25,
    1097--1105.
- en: Kumar et al. [2017] Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane,
    A., Sethi, A., 2017. A dataset and a technique for generalized nuclear segmentation
    for computational pathology. IEEE transactions on medical imaging 36, 1550--1560.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等人 [2017] Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A.,
    Sethi, A., 2017. 用于计算病理学的广义核分割的数据集和技术。《医学成像IEEE交易》36, 1550--1560.
- en: Kylberg et al. [2012] Kylberg, G., Uppström, M., Hedlund, K.O., Borgefors, G.,
    Sintorn, I.M., 2012. Segmentation of virus particle candidates in transmission
    electron microscopy images. Journal of microscopy 245, 140--147.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kylberg等人 [2012] Kylberg, G., Uppström, M., Hedlund, K.O., Borgefors, G., Sintorn,
    I.M., 2012. 在透射电子显微镜图像中分割病毒颗粒候选者。《显微镜杂志》245, 140--147.
- en: Lee et al. [2015] Lee, K., Zlateski, A., Ashwin, V., Seung, H.S., 2015. Recursive
    training of 2d-3d convolutional networks for neuronal boundary prediction. Advances
    in Neural Information Processing Systems 28.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等人 [2015] Lee, K., Zlateski, A., Ashwin, V., Seung, H.S., 2015. 递归训练2d-3d卷积网络进行神经元边界预测.
    《第28届神经信息处理系统进展》。
- en: Lee et al. [2017] Lee, K., Zung, J., Li, P., Jain, V., Seung, H.S., 2017. Superhuman
    accuracy on the snemi3d connectomics challenge. [arXiv:1706.00120](http://arxiv.org/abs/1706.00120).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等人 [2017] Lee, K., Zung, J., Li, P., Jain, V., Seung, H.S., 2017. 在snemi3d突触连通性挑战赛上的超人准确性。[arXiv:1706.00120](http://arxiv.org/abs/1706.00120).
- en: 'Li et al. [2022] Li, M., Chen, C., Liu, X., Huang, W., Zhang, Y., Xiong, Z.,
    2022. Advanced deep networks for 3d mitochondria instance segmentation, in: 2022
    IEEE 19th International Symposium on Biomedical Imaging (ISBI), IEEE. pp. 1--5.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人 [2022] Li, M., Chen, C., Liu, X., Huang, W., Zhang, Y., Xiong, Z., 2022.
    用于3d线粒体实例分割的先进深度网络，在：《2022年IEEE第19届生物医学成像研讨会（ISBI）》, IEEE. pp. 1--5.
- en: 'Li et al. [2017] Li, W., Wang, G., Fidon, L., Ourselin, S., Cardoso, M.J.,
    Vercauteren, T., 2017. On the compactness, efficiency, and representation of 3d
    convolutional networks: brain parcellation as a pretext task, in: International
    conference on information processing in medical imaging, Springer. pp. 348--360.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人 [2017] Li, W., Wang, G., Fidon, L., Ourselin, S., Cardoso, M.J., Vercauteren,
    T., 2017. 论3d卷积网络的紧凑性、效率和表示：以大脑分区为假设任务, in: 《医学影像信息处理国际会议》, Springer. pp. 348--360.'
- en: 'Lin et al. [2021] Lin, Z., Wei, D., Petkova, M.D., Wu, Y., Ahmed, Z., Zou,
    S., Wendt, N., Boulanger-Weill, J., Wang, X., Dhanyasi, N., et al., 2021. NucMM
    dataset: 3d neuronal nuclei instance segmentation at sub-cubic millimeter scale,
    in: International Conference on Medical Image Computing and Computer-Assisted
    Intervention, Springer. pp. 164--174.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2021] Lin, Z., Wei, D., Petkova, M.D., Wu, Y., Ahmed, Z., Zou, S., Wendt,
    N., Boulanger-Weill, J., Wang, X., Dhanyasi, N., 等，2021. NucMM 数据集：亚立方毫米尺度下的三维神经核实例分割，收录于：医学图像计算与计算机辅助干预国际会议，Springer出版社。第164--174页。
- en: Litjens et al. [2017] Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A.,
    Ciompi, F., Ghafoorian, M., Van Der Laak, J.A., Van Ginneken, B., Sánchez, C.I.,
    2017. A survey on deep learning in medical image analysis. Medical image analysis
    42, 60--88.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litjens 等人 [2017] Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi,
    F., Ghafoorian, M., Van Der Laak, J.A., Van Ginneken, B., Sánchez, C.I., 2017.
    医学图像分析中深度学习的综述。医学图像分析 42, 60--88。
- en: Liu et al. [2020a] Liu, J., Li, L., Yang, Y., Hong, B., Chen, X., Xie, Q., Han,
    H., 2020a. Automatic reconstruction of mitochondria and endoplasmic reticulum
    in electron microscopy volumes by deep learning. Frontiers in neuroscience 14,
    599.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2020a] Liu, J., Li, L., Yang, Y., Hong, B., Chen, X., Xie, Q., Han,
    H., 2020a. 通过深度学习在电子显微镜体积中自动重建线粒体和内质网。神经科学前沿 14, 599。
- en: 'Liu et al. [2020b] Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu,
    X., Pietikäinen, M., 2020b. Deep learning for generic object detection: A survey.
    International journal of computer vision 128, 261--318.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2020b] Liu, L., Ouyang, W., Wang, X., Fieguth, P., Chen, J., Liu, X.,
    Pietikäinen, M., 2020b. 深度学习在通用物体检测中的应用：综述。国际计算机视觉杂志 128, 261--318。
- en: Liu et al. [2014] Liu, T., Jones, C., Seyedhosseini, M., Tasdizen, T., 2014.
    A modular hierarchical approach to 3d electron microscopy image segmentation.
    Journal of neuroscience methods 226, 88--102.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2014] Liu, T., Jones, C., Seyedhosseini, M., Tasdizen, T., 2014. 用于三维电子显微镜图像分割的模块化层次方法。神经科学方法杂志
    226, 88--102。
- en: 'Liu et al. [2012] Liu, T., Jurrus, E., Seyedhosseini, M., Ellisman, M., Tasdizen,
    T., 2012. Watershed merge tree classification for electron microscopy image segmentation,
    in: Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012),
    IEEE. pp. 133--137.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2012] Liu, T., Jurrus, E., Seyedhosseini, M., Ellisman, M., Tasdizen,
    T., 2012. 用于电子显微镜图像分割的分水岭合并树分类，收录于：第21届国际模式识别大会（ICPR2012）论文集，IEEE出版社。第133--137页。
- en: Liu et al. [2021] Liu, Z., Jin, L., Chen, J., Fang, Q., Ablameyko, S., Yin,
    Z., Xu, Y., 2021. A survey on applications of deep learning in microscopy image
    analysis. Computers in Biology and Medicine 134, 104523.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021] Liu, Z., Jin, L., Chen, J., Fang, Q., Ablameyko, S., Yin, Z.,
    Xu, Y., 2021. 深度学习在显微图像分析中的应用综述。生物医学计算机 134, 104523。
- en: 'Long et al. [2015] Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional
    networks for semantic segmentation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, pp. 3431--3440.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人 [2015] Long, J., Shelhamer, E., Darrell, T., 2015. 用于语义分割的全卷积网络，收录于：IEEE计算机视觉与模式识别会议论文集，第3431--3440页。
- en: 'Lucchi et al. [2013] Lucchi, A., Li, Y., Fua, P., 2013. Learning for structured
    prediction using approximate subgradient descent with working sets, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1987--1994.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucchi 等人 [2013] Lucchi, A., Li, Y., Fua, P., 2013. 使用近似子梯度下降和工作集进行结构化预测学习，收录于：IEEE计算机视觉与模式识别会议论文集，第1987--1994页。
- en: Lucchi et al. [2011] Lucchi, A., Smith, K., Achanta, R., Knott, G., Fua, P.,
    2011. Supervoxel-based segmentation of mitochondria in em image stacks with learned
    shape features. IEEE transactions on medical imaging 31, 474--486.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lucchi 等人 [2011] Lucchi, A., Smith, K., Achanta, R., Knott, G., Fua, P., 2011.
    基于超体素的线粒体分割，通过学习的形状特征在电子显微镜图像堆栈中。IEEE医学成像学报 31, 474--486。
- en: Luo et al. [2021] Luo, Z., Wang, Y., Liu, S., Peng, J., 2021. Hierarchical encoder-decoder
    with soft label-decomposition for mitochondria segmentation in EM images. Frontiers
    in Neuroscience 15.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 [2021] Luo, Z., Wang, Y., Liu, S., Peng, J., 2021. 带软标签分解的层次编码器-解码器用于电子显微镜图像中的线粒体分割。神经科学前沿
    15。
- en: Mekuč et al. [2022] Mekuč, M.Ž., Bohak, C., Boneš, E., Hudoklin, S., Marolt,
    M., et al., 2022. Automatic segmentation and reconstruction of intracellular compartments
    in volumetric electron microscopy data. Computer methods and programs in biomedicine
    223, 106959.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mekuč 等人 [2022] Mekuč, M.Ž., Bohak, C., Boneš, E., Hudoklin, S., Marolt, M.,
    等，2022. 在体积电子显微镜数据中进行细胞内区室的自动分割和重建。计算机方法与生物医学程序 223, 106959。
- en: Mekuč et al. [2020] Mekuč, M.Ž., Bohak, C., Hudoklin, S., Kim, B.H., Kim, M.Y.,
    Marolt, M., et al., 2020. Automatic segmentation of mitochondria and endolysosomes
    in volumetric electron microscopy data. Computers in biology and medicine 119,
    103693.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mekuč 等 [2020] Mekuč, M.Ž., Bohak, C., Hudoklin, S., Kim, B.H., Kim, M.Y., Marolt,
    M., 等，2020. 在体积电子显微镜数据中自动分割线粒体和内泡体。生物医学计算机 119, 103693。
- en: 'Milletari et al. [2016] Milletari, F., Navab, N., Ahmadi, S.A., 2016. V-net:
    Fully convolutional neural networks for volumetric medical image segmentation,
    in: 2016 fourth international conference on 3D vision (3DV), IEEE. pp. 565--571.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milletari 等 [2016] Milletari, F., Navab, N., Ahmadi, S.A., 2016. V-net：用于体积医学图像分割的全卷积神经网络，见：2016年第四届国际3D视觉会议（3DV），IEEE。第565--571页。
- en: Moussavi et al. [2010] Moussavi, F., Heitz, G., Amat, F., Comolli, L.R., Koller,
    D., Horowitz, M., 2010. 3D segmentation of cell boundaries from whole cell cryogenic
    electron tomography volumes. Journal of Structural Biology 170, 134--145.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moussavi 等 [2010] Moussavi, F., Heitz, G., Amat, F., Comolli, L.R., Koller,
    D., Horowitz, M., 2010. 从整个细胞冷冻电子层析体积中进行 3D 细胞边界分割。结构生物学杂志 170, 134--145。
- en: 'Oda et al. [2018] Oda, H., Roth, H.R., Chiba, K., Sokolić, J., Kitasaka, T.,
    Oda, M., Hinoki, A., Uchida, H., Schnabel, J.A., Mori, K., 2018. Besnet: boundary-enhanced
    segmentation of cells in histopathological images, in: Medical Image Computing
    and Computer Assisted Intervention--MICCAI 2018: 21st International Conference,
    Granada, Spain, September 16-20, 2018, Proceedings, Part II 11, Springer. pp.
    228--236.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oda 等 [2018] Oda, H., Roth, H.R., Chiba, K., Sokolić, J., Kitasaka, T., Oda,
    M., Hinoki, A., Uchida, H., Schnabel, J.A., Mori, K., 2018. Besnet：在组织病理图像中增强边界的细胞分割，见：医学图像计算与计算机辅助干预--MICCAI
    2018：第21届国际会议，西班牙格拉纳达，2018年9月16-20日，论文集，第II部分 11, Springer。第228--236页。
- en: 'Oztel et al. [2017] Oztel, I., Yolcu, G., Ersoy, I., White, T., Bunyak, F.,
    2017. Mitochondria segmentation in electron microscopy volumes using deep convolutional
    neural network, in: 2017 IEEE International Conference on Bioinformatics and Biomedicine
    (BIBM), IEEE. pp. 1195--1200.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oztel 等 [2017] Oztel, I., Yolcu, G., Ersoy, I., White, T., Bunyak, F., 2017.
    使用深度卷积神经网络在电子显微镜体积数据中分割线粒体，见：2017 IEEE 国际生物信息学与生物医学会议（BIBM），IEEE。第1195--1200页。
- en: 'Peddie and Collinson [2014] Peddie, C.J., Collinson, L.M., 2014. Exploring
    the third dimension: volume electron microscopy comes of age. Micron 61, 9--19.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peddie 和 Collinson [2014] Peddie, C.J., Collinson, L.M., 2014. 探索第三维度：体积电子显微镜的成熟。Micron
    61, 9--19。
- en: Peddie et al. [2022] Peddie, C.J., Genoud, C., Kreshuk, A., Meechan, K., Micheva,
    K.D., Narayan, K., Pape, C., Parton, R.G., Schieber, N.L., Schwab, Y., et al.,
    2022. Volume electron microscopy. Nature Reviews Methods Primers 2, 1--23.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peddie 等 [2022] Peddie, C.J., Genoud, C., Kreshuk, A., Meechan, K., Micheva,
    K.D., Narayan, K., Pape, C., Parton, R.G., Schieber, N.L., Schwab, Y., 等，2022.
    体积电子显微镜。自然评论方法初探 2, 1--23。
- en: Peng et al. [2020] Peng, J., Yi, J., Yuan, Z., 2020. Unsupervised mitochondria
    segmentation in em images via domain adaptive multi-task learning. IEEE Journal
    of Selected Topics in Signal Processing 14, 1199--1209.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等 [2020] Peng, J., Yi, J., Yuan, Z., 2020. 通过领域自适应多任务学习对 EM 图像进行无监督线粒体分割。IEEE
    选择主题信号处理期刊 14, 1199--1209。
- en: Perez et al. [2014] Perez, A.J., Seyedhosseini, M., Deerinck, T.J., Bushong,
    E.A., Panda, S., Tasdizen, T., Ellisman, M.H., 2014. A workflow for the automatic
    segmentation of organelles in electron microscopy image stacks. Frontiers in neuroanatomy
    8, 126.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等 [2014] Perez, A.J., Seyedhosseini, M., Deerinck, T.J., Bushong, E.A.,
    Panda, S., Tasdizen, T., Ellisman, M.H., 2014. 用于自动分割电子显微镜图像堆栈中细胞器的工作流程。神经解剖学前沿
    8, 126。
- en: 'Pirozzi et al. [2018] Pirozzi, N.M., Hoogenboom, J.P., Giepmans, B.N., 2018.
    Colorem: analytical electron microscopy for element-guided identification and
    imaging of the building blocks of life. Histochemistry and cell biology 150, 509--520.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pirozzi 等 [2018] Pirozzi, N.M., Hoogenboom, J.P., Giepmans, B.N., 2018. Colorem：用于元素引导识别和生命基本结构成分成像的分析电子显微镜。组织化学与细胞生物学
    150, 509--520。
- en: Plaza and Funke [2018] Plaza, S.M., Funke, J., 2018. Analyzing image segmentation
    for connectomics. Frontiers in neural circuits 12, 102.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plaza 和 Funke [2018] Plaza, S.M., Funke, J., 2018. 分析连接组学的图像分割。神经回路前沿 12, 102。
- en: 'Quan et al. [2021] Quan, T.M., Hildebrand, D.G.C., Jeong, W.K., 2021. Fusionnet:
    A deep fully residual convolutional neural network for image segmentation in connectomics.
    Frontiers in Computer Science , 34.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quan 等 [2021] Quan, T.M., Hildebrand, D.G.C., Jeong, W.K., 2021. Fusionnet：用于连接组学图像分割的深度全残差卷积神经网络。计算机科学前沿，34。
- en: Ravelli et al. [2013] Ravelli, R.B., Kalicharan, R.D., Avramut, M.C., Sjollema,
    K.A., Pronk, J.W., Dijk, F., Koster, A.J., Visser, J.T., Faas, F.G., Giepmans,
    B.N., 2013. Destruction of tissue, cells and organelles in type 1 diabetic rats
    presented at macromolecular resolution. Scientific reports 3, 1--6.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravelli 等 [2013] Ravelli, R.B., Kalicharan, R.D., Avramut, M.C., Sjollema, K.A.,
    Pronk, J.W., Dijk, F., Koster, A.J., Visser, J.T., Faas, F.G., Giepmans, B.N.,
    2013. 1 型糖尿病大鼠组织、细胞和细胞器的破坏，呈现于大分子分辨率。科学报告 3, 1--6。
- en: 'Ren and Zemel [2017] Ren, M., Zemel, R.S., 2017. End-to-end instance segmentation
    with recurrent attention, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 6656--6664.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 和 Zemel [2017] Ren, M., Zemel, R.S., 2017. 基于递归注意的端到端实例分割，发表于：IEEE 计算机视觉与模式识别会议论文集，页码
    6656--6664。
- en: 'Ren and Kruit [2016] Ren, Y., Kruit, P., 2016. Transmission electron imaging
    in the delft multibeam scanning electron microscope 1. Journal of Vacuum Science
    & Technology B, Nanotechnology and Microelectronics: Materials, Processing, Measurement,
    and Phenomena 34, 06KF02.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 和 Kruit [2016] Ren, Y., Kruit, P., 2016. 在代尔夫特多束扫描电子显微镜中的透射电子成像 1. 真空科学与技术
    B 杂志，纳米技术与微电子学：材料、加工、测量与现象 34, 06KF02。
- en: 'Ronneberger et al. [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
    Convolutional networks for biomedical image segmentation, in: International Conference
    on Medical image computing and computer-assisted intervention, Springer. pp. 234--241.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger 等 [2015] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: 用于生物医学图像分割的卷积网络，发表于：医学图像计算与计算机辅助干预国际会议，Springer.
    页码 234--241。'
- en: 'Schindelin et al. [2012] Schindelin, J., Arganda-Carreras, I., Frise, E., Kaynig,
    V., Longair, M., Pietzsch, T., Preibisch, S., Rueden, C., Saalfeld, S., Schmid,
    B., et al., 2012. Fiji: an open-source platform for biological-image analysis.
    Nature methods 9, 676--682.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schindelin 等 [2012] Schindelin, J., Arganda-Carreras, I., Frise, E., Kaynig,
    V., Longair, M., Pietzsch, T., Preibisch, S., Rueden, C., Saalfeld, S., Schmid,
    B., 等，2012. Fiji: 一个开源生物图像分析平台。自然方法 9, 676--682。'
- en: Shaban et al. [2017] Shaban, A., Bansal, S., Liu, Z., Essa, I., Boots, B., 2017.
    One-shot learning for semantic segmentation. [arXiv:1709.03410](http://arxiv.org/abs/1709.03410).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaban 等 [2017] Shaban, A., Bansal, S., Liu, Z., Essa, I., Boots, B., 2017.
    用于语义分割的一次性学习。 [arXiv:1709.03410](http://arxiv.org/abs/1709.03410)。
- en: Shen et al. [2017] Shen, D., Wu, G., Suk, H.I., 2017. Deep learning in medical
    image analysis. Annual review of biomedical engineering 19, 221--248.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 [2017] Shen, D., Wu, G., Suk, H.I., 2017. 医学图像分析中的深度学习。生物医学工程年度综述 19,
    221--248。
- en: Sokol et al. [2015] Sokol, E., Kramer, D., Diercks, G.F., Kuipers, J., Jonkman,
    M.F., Pas, H.H., Giepmans, B.N., 2015. Large-scale electron microscopy maps of
    patient skin and mucosa provide insight into pathogenesis of blistering diseases.
    Journal of Investigative Dermatology 135, 1763--1770.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sokol 等 [2015] Sokol, E., Kramer, D., Diercks, G.F., Kuipers, J., Jonkman, M.F.,
    Pas, H.H., Giepmans, B.N., 2015. 患者皮肤和黏膜的大规模电子显微镜图谱提供了对水疱性疾病发病机制的见解。皮肤病学研究杂志 135,
    1763--1770。
- en: Spiers et al. [2021] Spiers, H., Songhurst, H., Nightingale, L., de Folter,
    J., Community, Z.V., Hutchings, R., Peddie, C.J., Weston, A., Strange, A., Hindmarsh,
    S., et al., 2021. Deep learning for automatic segmentation of the nuclear envelope
    in electron microscopy data, trained with volunteer segmentations. Traffic .
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spiers 等 [2021] Spiers, H., Songhurst, H., Nightingale, L., de Folter, J., Community,
    Z.V., Hutchings, R., Peddie, C.J., Weston, A., Strange, A., Hindmarsh, S., 等，2021.
    基于深度学习的电子显微镜数据中核膜的自动分割，经过志愿者分割训练。Traffic。
- en: Takaya et al. [2021] Takaya, E., Takeichi, Y., Ozaki, M., Kurihara, S., 2021.
    Sequential semi-supervised segmentation for serial electron microscopy image with
    small number of labels. Journal of Neuroscience Methods 351, 109066.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takaya 等 [2021] Takaya, E., Takeichi, Y., Ozaki, M., Kurihara, S., 2021. 针对序列电子显微镜图像的顺序半监督分割，标注数量少。神经科学方法杂志
    351, 109066。
- en: Takemura et al. [2015] Takemura, S.y., Xu, C.S., Lu, Z., Rivlin, P.K., Parag,
    T., Olbris, D.J., Plaza, S., Zhao, T., Katz, W.T., Umayam, L., et al., 2015. Synaptic
    circuits and their variations within different columns in the visual system of
    drosophila. Proceedings of the National Academy of Sciences 112, 13711--13716.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takemura 等 [2015] Takemura, S.y., Xu, C.S., Lu, Z., Rivlin, P.K., Parag, T.,
    Olbris, D.J., Plaza, S., Zhao, T., Katz, W.T., Umayam, L., 等，2015. 果蝇视觉系统中不同柱内的突触回路及其变异。美国国家科学院院刊
    112, 13711--13716。
- en: 'Tao et al. [2020] Tao, X., Hong, X., Chang, X., Dong, S., Wei, X., Gong, Y.,
    2020. Few-shot class-incremental learning, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 12183--12192.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao等人 [2020] Tao, X., Hong, X., Chang, X., Dong, S., Wei, X., Gong, Y., 2020.
    少样本类别增量学习，载于：IEEE/CVF计算机视觉与模式识别会议论文集，第12183--12192页。
- en: Titze and Genoud [2016] Titze, B., Genoud, C., 2016. Volume scanning electron
    microscopy for imaging biological ultrastructure. Biology of the Cell 108, 307--323.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Titze和Genoud [2016] Titze, B., Genoud, C., 2016. 用于成像生物超微结构的体积扫描电子显微镜。细胞生物学
    108, 307--323。
- en: Treder et al. [2022] Treder, K.P., Huang, C., Kim, J.S., Kirkland, A.I., 2022.
    Applications of deep learning in electron microscopy. Microscopy 71, i100--i115.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Treder等人 [2022] Treder, K.P., Huang, C., Kim, J.S., Kirkland, A.I., 2022. 深度学习在电子显微镜中的应用。显微学
    71, i100--i115。
- en: Turaga et al. [2010] Turaga, S.C., Murray, J.F., Jain, V., Roth, F., Helmstaedter,
    M., Briggman, K., Denk, W., Seung, H.S., 2010. Convolutional networks can learn
    to generate affinity graphs for image segmentation. Neural computation 22, 511--538.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turaga等人 [2010] Turaga, S.C., Murray, J.F., Jain, V., Roth, F., Helmstaedter,
    M., Briggman, K., Denk, W., Seung, H.S., 2010. 卷积网络可以学习生成用于图像分割的亲和图。神经计算 22, 511--538。
- en: Unnikrishnan et al. [2007] Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007.
    Toward objective evaluation of image segmentation algorithms. IEEE transactions
    on pattern analysis and machine intelligence 29, 929--944.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unnikrishnan等人 [2007] Unnikrishnan, R., Pantofaru, C., Hebert, M., 2007. 迈向图像分割算法的客观评估。IEEE模式分析与机器智能学报
    29, 929--944。
- en: Wang et al. [2015] Wang, R., Stone, R.L., Kaelber, J.T., Rochat, R.H., Nick,
    A.M., Vijayan, K.V., Afshar-Kharghan, V., Schmid, M.F., Dong, J.F., Sood, A.K.,
    et al., 2015. Electron cryotomography reveals ultrastructure alterations in platelets
    from patients with ovarian cancer. Proceedings of the National Academy of Sciences
    112, 14266--14271.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2015] Wang, R., Stone, R.L., Kaelber, J.T., Rochat, R.H., Nick, A.M.,
    Vijayan, K.V., Afshar-Kharghan, V., Schmid, M.F., Dong, J.F., Sood, A.K., 等人,
    2015. 电子冷冻断层摄影揭示了卵巢癌患者血小板中的超微结构变化。美国国家科学院院刊 112, 14266--14271。
- en: 'Wei et al. [2020] Wei, D., Lin, Z., Barranco, D., Wendt, N., Liu, X., Yin,
    W., Huang, X., Gupta, A., Jang, W., Wang, X., Arganda-Carreras, I., Lichtman,
    J., Pfister, H., 2020. Mitoem dataset: Large-scale 3d mitochondria instance segmentation
    from em images, in: International Conference on Medical Image Computing and Computer
    Assisted Intervention.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人 [2020] Wei, D., Lin, Z., Barranco, D., Wendt, N., Liu, X., Yin, W., Huang,
    X., Gupta, A., Jang, W., Wang, X., Arganda-Carreras, I., Lichtman, J., Pfister,
    H., 2020. Mitoem数据集：基于EM图像的大规模3D线粒体实例分割，载于：国际医学图像计算与计算机辅助干预会议。
- en: Winding et al. [2023] Winding, M., Pedigo, B.D., Barnes, C.L., Patsolic, H.G.,
    Park, Y., Kazimiers, T., Fushiki, A., Andrade, I.V., Khandelwal, A., Valdes-Aleman,
    J., Li, F., Randel, N., Barsotti, E., Correia, A., Fetter, R.D., Hartenstein,
    V., Priebe, C.E., Vogelstein, J.T., Cardona, A., Zlatic, M., 2023. The connectome
    of an insect brain. Science 379, eadd9330.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Winding等人 [2023] Winding, M., Pedigo, B.D., Barnes, C.L., Patsolic, H.G., Park,
    Y., Kazimiers, T., Fushiki, A., Andrade, I.V., Khandelwal, A., Valdes-Aleman,
    J., Li, F., Randel, N., Barsotti, E., Correia, A., Fetter, R.D., Hartenstein,
    V., Priebe, C.E., Vogelstein, J.T., Cardona, A., Zlatic, M., 2023. 一种昆虫大脑的连通组图谱。科学
    379, eadd9330。
- en: 'Wolny et al. [2022] Wolny, A., Yu, Q., Pape, C., Kreshuk, A., 2022. Sparse
    object-level supervision for instance segmentation with pixel embeddings, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 4402--4411.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolny等人 [2022] Wolny, A., Yu, Q., Pape, C., Kreshuk, A., 2022. 基于像素嵌入的稀疏对象级监督用于实例分割，载于：IEEE/CVF计算机视觉与模式识别会议论文集，第4402--4411页。
- en: Xiao et al. [2018a] Xiao, C., Chen, X., Li, W., Li, L., Wang, L., Xie, Q., Han,
    H., 2018a. Automatic mitochondria segmentation for em data using a 3d supervised
    convolutional network. Frontiers in neuroanatomy 12, 92.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等人 [2018a] Xiao, C., Chen, X., Li, W., Li, L., Wang, L., Xie, Q., Han, H.,
    2018a. 使用3D监督卷积网络进行EM数据的自动线粒体分割。神经解剖学前沿 12, 92。
- en: 'Xiao et al. [2018b] Xiao, C., Liu, J., Chen, X., Han, H., Shu, C., Xie, Q.,
    2018b. Deep contextual residual network for electron microscopy image segmentation
    in connectomics, in: 2018 IEEE 15th International Symposium on Biomedical Imaging
    (ISBI 2018), IEEE. pp. 378--381.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等人 [2018b] Xiao, C., Liu, J., Chen, X., Han, H., Shu, C., Xie, Q., 2018b.
    用于连接组学的电子显微镜图像分割的深度上下文残差网络，载于：2018 IEEE第15届生物医学成像国际研讨会（ISBI 2018），IEEE，第378--381页。
- en: 'Xing et al. [2017] Xing, F., Xie, Y., Su, H., Liu, F., Yang, L., 2017. Deep
    learning in microscopy image analysis: A survey. IEEE transactions on neural networks
    and learning systems 29, 4550--4568.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing et al. [2017] Xing, F., Xie, Y., Su, H., Liu, F., Yang, L., 2017. 显微镜图像分析中的深度学习：一项综述。IEEE神经网络与学习系统汇刊
    29, 4550--4568。
- en: Xu et al. [2021] Xu, C.S., Pang, S., Shtengel, G., Müller, A., Ritter, A.T.,
    Hoffman, H.K., Takemura, S.y., Lu, Z., Pasolli, H.A., Iyer, N., et al., 2021.
    An open-access volume electron microscopy atlas of whole cells and tissues. Nature
    599, 147--151.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2021] Xu, C.S., Pang, S., Shtengel, G., Müller, A., Ritter, A.T.,
    Hoffman, H.K., Takemura, S.y., Lu, Z., Pasolli, H.A., Iyer, N., 等, 2021. 一个开放获取的体积电子显微镜全细胞和组织图谱。自然
    599, 147--151。
- en: 'Yu and Koltun [2016] Yu, F., Koltun, V., 2016. Multi-scale context aggregation
    by dilated convolutions, in: Bengio, Y., LeCun, Y. (Eds.), 4th International Conference
    on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
    Conference Track Proceedings. URL: [http://arxiv.org/abs/1511.07122](http://arxiv.org/abs/1511.07122).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu and Koltun [2016] Yu, F., Koltun, V., 2016. 通过膨胀卷积进行多尺度上下文聚合，收录于: Bengio,
    Y., LeCun, Y. (编), 第四届国际学习表征大会, ICLR 2016, 圣胡安, 波多黎各, 2016年5月2-4日, 会议论文集。网址：[http://arxiv.org/abs/1511.07122](http://arxiv.org/abs/1511.07122)。'
- en: 'Yuan et al. [2021] Yuan, Z., Ma, X., Yi, J., Luo, Z., Peng, J., 2021. Hive-net:
    Centerline-aware hierarchical view-ensemble convolutional network for mitochondria
    segmentation in em images. Computer Methods and Programs in Biomedicine 200, 105925.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan et al. [2021] Yuan, Z., Ma, X., Yi, J., Luo, Z., Peng, J., 2021. Hive-net:
    以中心线为导向的层次视图集成卷积网络用于线粒体在电子显微镜图像中的分割。生物医学计算方法与程序 200, 105925。'
- en: 'Zeng et al. [2017] Zeng, T., Wu, B., Ji, S., 2017. DeepEM3D: approaching human-level
    performance on 3d anisotropic em image segmentation. Bioinformatics 33, 2555--2562.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. [2017] Zeng, T., Wu, B., Ji, S., 2017. DeepEM3D: 在三维各向异性电子显微镜图像分割上接近人类水平的表现。生物信息学
    33, 2555--2562。'
- en: 'Zheng et al. [2021] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
    Fu, Y., Feng, J., Xiang, T., Torr, P.H., et al., 2021. Rethinking semantic segmentation
    from a sequence-to-sequence perspective with transformers, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, pp. 6881--6890.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. [2021] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y.,
    Fu, Y., Feng, J., Xiang, T., Torr, P.H., 等, 2021. 从序列到序列的视角重新思考语义分割，收录于: IEEE/CVF计算机视觉与模式识别大会论文集,
    页 6881--6890。'
- en: Zheng et al. [2018] Zheng, Z., Lauritzen, J.S., Perlman, E., Robinson, C.G.,
    Nichols, M., Milkie, D., Torrens, O., Price, J., Fisher, C.B., Sharifi, N., et al.,
    2018. A complete electron microscopy volume of the brain of adult drosophila melanogaster.
    Cell 174, 730--743.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2018] Zheng, Z., Lauritzen, J.S., Perlman, E., Robinson, C.G.,
    Nichols, M., Milkie, D., Torrens, O., Price, J., Fisher, C.B., Sharifi, N., 等,
    2018. 一个完整的成年果蝇脑部电子显微镜体积图像。细胞 174, 730--743。
- en: Zhu and Goldberg [2009] Zhu, X., Goldberg, A.B., 2009. Introduction to semi-supervised
    learning. Synthesis lectures on artificial intelligence and machine learning 3,
    1--130.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu and Goldberg [2009] Zhu, X., Goldberg, A.B., 2009. 半监督学习导论。人工智能与机器学习合成讲座
    3, 1--130。
