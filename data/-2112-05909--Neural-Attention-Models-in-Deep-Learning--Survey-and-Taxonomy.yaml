- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:49:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.05909] Neural Attention Models in Deep Learning: Survey and Taxonomy'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.05909] 神经注意模型在深度学习中的调查与分类'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.05909](https://ar5iv.labs.arxiv.org/html/2112.05909)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.05909](https://ar5iv.labs.arxiv.org/html/2112.05909)
- en: \DeclareUnicodeCharacter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \DeclareUnicodeCharacter
- en: 2212-
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2212-
- en: 'Neural Attention Models in Deep Learning: Survey and Taxonomy'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经注意模型在深度学习中的调查与分类
- en: 'Alana de Santana Correia, and Esther Luna Colombini Laboratory of Robotics
    and Cogntive Systems (LaRoCS) Institute of Computing, University of Campinas,
    Av. Albert Einstein, 1251 - Campinas, SP - Brazil e-mail: {alana.correia, esther}@ic.unicamp.br.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Alana de Santana Correia 和 Esther Luna Colombini 机器人与认知系统实验室（LaRoCS）计算机学院，坎皮纳斯大学，阿尔伯特·爱因斯坦大道1251号
    - 坎皮纳斯，SP - 巴西 电子邮件：{alana.correia, esther}@ic.unicamp.br。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Attention is a state of arousal capable of dealing with limited processing bottlenecks
    in human beings by focusing selectively on one piece of information while ignoring
    other perceptible information [[1](#bib.bib1)]. For decades, concepts and functions
    of attention have been studied in philosophy, psychology, neuroscience, and computing.
    Currently, this property has been widely explored in deep neural networks. Many
    different neural attention models are now available and have been a very active
    research area over the past six years. From the theoretical standpoint of attention,
    this survey provides a critical analysis of major neural attention models. Here
    we propose a taxonomy that corroborates with theoretical aspects that predate
    Deep Learning. Our taxonomy provides an organizational structure that asks new
    questions and structures the understanding of existing attentional mechanisms.
    In particular, 17 criteria derived from psychology and neuroscience classic studies
    are formulated for qualitative comparison and critical analysis on the 51 main
    models found on a set of more than 650 papers analyzed. Also, we highlight several
    theoretical issues that have not yet been explored, including discussions about
    biological plausibility, highlight current research trends, and provide insights
    for the future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种兴奋状态，能够通过选择性地关注一条信息而忽略其他感知信息来应对人类处理瓶颈[[1](#bib.bib1)]。几十年来，注意力的概念和功能在哲学、心理学、神经科学和计算领域得到了研究。目前，这一特性在深度神经网络中得到了广泛探索。许多不同的神经注意模型现在可用，并且在过去六年中一直是一个非常活跃的研究领域。从注意力的理论角度来看，本调查对主要的神经注意模型进行了关键分析。我们提出了一种与深度学习之前的理论方面相符的分类方法。我们的分类提供了一个组织结构，提出了新问题并构建了对现有注意机制的理解。特别地，我们基于心理学和神经科学经典研究提出了17个标准，用于对分析的超过650篇论文中的51个主要模型进行定性比较和关键分析。此外，我们还突出了尚未探索的几个理论问题，包括对生物学合理性的讨论，突出当前的研究趋势，并为未来提供了见解。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Survey, Taxonomy, Attention Mechanism, Neural Networks, Deep Learning, Attention
    Models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 调查、分类学、注意机制、神经网络、深度学习、注意模型。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Attention is a state of arousal capable of dealing with limited processing bottlenecks
    by focusing selectively on one piece of information while ignoring other perceptible
    information [[1](#bib.bib1)]. According to James [[2](#bib.bib2)] attention can
    be considered as an internal force that spontaneously or voluntarily creates a
    mental expectation of a sensory or motor nature, favoring the perception of stimuli
    and the production of responses. This internal force can also be understood as
    a cognitive need, given that at any moment, the environment presents more perceptual
    information than can be supported, and it is impossible to perform all motor actions
    simultaneously in response to all external stimuli. In nature, attention is an
    essential activity concerning the survival of all forms of life, resulting from
    a long process of cognitive evolution of living beings. Among the beings that
    occupy the lowest evolutionary scale positions, attention acts mainly on perception,
    selecting, and modulating relevant stimuli from the environment. This mechanism
    is decisive for the perpetuation and evolution of species, as it is characterized
    as the ability to settle in points of interest in the environment and recognize
    possible prey, predators, or rivals. In humans, attention is intrinsically present
    in the brain throughout the cognitive cycle, acting from the perception of stimuli,
    organization of complex mental processes to decision making.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是一种能够通过选择性地专注于一条信息而忽略其他可感知信息，从而处理有限处理瓶颈的唤醒状态 [[1](#bib.bib1)]。根据詹姆斯的观点 [[2](#bib.bib2)]，注意力可以被认为是一种内在力量，它自发或主动地创造对感官或运动性质的心理预期，促进刺激的感知和反应的产生。这种内在力量也可以被理解为一种认知需求，因为在任何时刻，环境提供的感知信息总是超出可处理的范围，无法对所有外部刺激同时作出反应。在自然界中，注意力是一项与所有生命形式的生存密切相关的基本活动，源自生物长期的认知进化过程。在处于最低进化等级的生物中，注意力主要作用于感知，选择和调节来自环境的相关刺激。这个机制对于物种的延续和进化至关重要，因为它被定义为在环境中集中于感兴趣的点，识别可能的猎物、捕食者或竞争对手的能力。在人类中，注意力在整个认知周期中固有地存在于大脑中，从刺激的感知、复杂心理过程的组织到决策制定。
- en: For decades, several areas of science have been concerned with understanding
    the role of attention. In psychology, studies dating back to 1890, looking for
    behavioral correlates that reflect the performance of attentional processes in
    the human brain, such as surveillance time [[3](#bib.bib3)], inattentional blindness
    [[4](#bib.bib4)], attentional blink [[5](#bib.bib5)], reaction time in cognitive
    processing [[6](#bib.bib6)], and the selective ability to filter external stimuli
    [[7](#bib.bib7)]. Cognitive neuroscience studies have employed invasive and non-invasive
    approaches, such as neuroanatomical/neurophysiological techniques, electroencephalography,
    positron emission tomography (PET), and functional magnetic resonance imaging
    (fMRI), to capture insights about attentional disorders [[8](#bib.bib8)]. Neurophysiologists
    seek to study how neurons respond to represent external stimuli of interest [[9](#bib.bib9)].
    Finally, computational neuroscientists capture all the insights from the different
    perspectives experienced and support realistic computational models to simulate
    and explain attentional behaviors, seeking to understand how, where and when attention
    processes occur or are needed [[10](#bib.bib10)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，科学的多个领域一直关注注意力的作用。在心理学中，从1890年开始的研究寻找反映人脑注意力过程表现的行为相关因素，例如监视时间 [[3](#bib.bib3)]、无意识盲点
    [[4](#bib.bib4)]、注意力闪烁 [[5](#bib.bib5)]、认知处理中的反应时间 [[6](#bib.bib6)] 以及选择性过滤外部刺激的能力
    [[7](#bib.bib7)]。认知神经科学研究采用了侵入性和非侵入性的方法，如神经解剖/神经生理技术、电生理学、正电子发射断层扫描（PET）和功能性磁共振成像（fMRI），以捕捉有关注意力障碍的见解
    [[8](#bib.bib8)]。神经生理学家致力于研究神经元如何响应代表感兴趣的外部刺激 [[9](#bib.bib9)]。最后，计算神经科学家整合来自不同视角的所有见解，并支持现实的计算模型，以模拟和解释注意力行为，力求理解注意力过程发生或需要的方式、位置和时机
    [[10](#bib.bib10)]。
- en: Inspired by these studies, computer scientists in the 1990s proposed the first
    attentional mechanisms for computer systems to resolve performance limitations
    inherent in the high computational complexity of the algorithms existing at the
    time. Initially, several attentional vision models for object recognition [[11](#bib.bib11)],
    image compression [[12](#bib.bib12)], image matching [[13](#bib.bib13)], image
    segmentation [[12](#bib.bib12)], object tracking [walther2004detection], active
    vision [[14](#bib.bib14)], and recognition [[11](#bib.bib11)] emerged inspired
    by Feature Integration Theory - one of the first theories to formalize visual
    attention to perception - in which a set of simple features is extracted from
    a scene observed by the system separately, and in subsequent steps the integration
    of the stimuli occurs supporting the identification of relevant objects in the
    environment. Subsequently, visual attention emerged as a tool capable of providing
    essential information on the environment for robotic agents’ decision-making in
    the world. So, several robotic navigation systems [[15](#bib.bib15)], SLAM[[16](#bib.bib16)],
    and and human-robot interaction [[17](#bib.bib17)] integrated attention to improve
    the performance of these autonomous agents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 受这些研究的启发，计算机科学家在1990年代提出了第一个注意力机制，用于解决当时算法固有的高计算复杂性所带来的性能限制。最初，受到特征整合理论启发的几个注意力视觉模型应运而生，这些模型用于目标识别[[11](#bib.bib11)]、图像压缩[[12](#bib.bib12)]、图像匹配[[13](#bib.bib13)]、图像分割[[12](#bib.bib12)]、目标跟踪[walther2004detection]、主动视觉[[14](#bib.bib14)]和识别[[11](#bib.bib11)]。特征整合理论是最早将视觉注意力形式化为感知的理论之一，其中从系统观察到的场景中提取一组简单的特征，然后在随后的步骤中进行刺激的整合，从而支持识别环境中的相关对象。随后，视觉注意力作为一种工具，能够为机器人代理在世界中进行决策提供环境的基本信息。因此，若干机器人导航系统[[15](#bib.bib15)]、SLAM[[16](#bib.bib16)]以及人机交互[[17](#bib.bib17)]集成了注意力机制，以提高这些自主代理的性能。
- en: Artificial intelligence scientists have noticed attention as a fundamental concept
    for improving deep neural networks’ performance in the last decade. In Deep Learning,
    Attention introduces a new form of computing inspired by the human brain quite
    different from what neural networks do today. Attentional mechanisms make networks
    more scalable, simpler, facilitate multimodality, and reduce information bottlenecks
    from long spatial and temporal dependencies. Attentional interfaces currently
    focus on two major fronts of development and research as small modules that can
    be easily plugged into classic DL architectures and end-to-end attention networks
    where attention is intrinsically present throughout the architecture. The attentional
    interfaces usually complement convolutional and recurrence operations allowing
    the control of the dynamic flow of resources and internal or external information,
    coming from specific parts of the neural structure or other external cognitive
    elements (e.g., external memories, pre-trained layers). End-to-end attention networks
    represent major advances in Deep Learning. State-of-art approaches in Natural
    Language Processing [[18](#bib.bib18)] [[19](#bib.bib19)], multimodal learning,
    and unstructured data learning via graph neural networks use end-to-end attention
    approaches [[20](#bib.bib20)][[21](#bib.bib21)]. Currently, much of the research
    aimed at DL uses attentional structures in the most diverse application domains,
    so that we have been able to map more than 6,000 works published in the area since
    2014 in the main publication repositories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年里，人工智能科学家注意到注意力机制是提升深度神经网络性能的一个基本概念。在深度学习中，注意力引入了一种新型计算形式，灵感来自于人脑，与神经网络目前的操作方式大相径庭。注意力机制使得网络更加可扩展、更简洁，促进了多模态的处理，并减少了来自长时间和空间依赖的信息瓶颈。注意力接口目前主要集中在两个主要的发展和研究方向：作为可以轻松插入经典深度学习架构的小模块，以及在整个架构中固有存在注意力的端到端注意力网络。注意力接口通常补充了卷积和递归操作，允许控制资源和内部或外部信息的动态流，这些信息来自神经结构的特定部分或其他外部认知元素（例如，外部记忆、预训练层）。端到端注意力网络代表了深度学习的重大进展。在自然语言处理[[18](#bib.bib18)]
    [[19](#bib.bib19)]、多模态学习以及通过图神经网络对非结构化数据的学习中，最先进的方法都使用了端到端注意力方法[[20](#bib.bib20)][[21](#bib.bib21)]。目前，许多针对深度学习的研究使用了注意力结构，在各种应用领域都有涉及，因此我们能够在主要出版库中绘制出自2014年以来超过6000篇相关工作的地图。
- en: Despite the high extent of research in various areas of computing, psychology,
    neuroscience, and even philosophy, the historical problem is that attention is
    omnipresent in the brain, as there is no single attention center, which makes
    concepts and aspects of study quite abstract and difficult to validate. When a
    group of related concepts and ideas becomes challenging to manage, a taxonomy
    is useful. Through taxonomy, it is possible to group different aspects and systematically
    study them. In psychology and neuroscience, this problem is still present, but
    there are already several theories and taxonomies widely accepted by several cognitive-behavioral
    researchers [[22](#bib.bib22)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在计算机科学、心理学、神经科学甚至哲学的各个领域进行了大量研究，但历史性的问题在于注意力在大脑中无处不在，因为没有单一的注意力中心，这使得概念和研究方面相当抽象且难以验证。当一组相关概念和想法变得难以管理时，分类法就显得很有用。通过分类法，可以将不同方面归类并系统地研究它们。在心理学和神经科学中，这个问题依然存在，但已经有几个理论和分类法被多个认知行为研究者广泛接受[[22](#bib.bib22)]。
- en: 'Specifically, in Deep Learning, there is no a taxonomy based on theoretical
    concepts of attention, given that the few that exist are far from theoretical
    concepts and are quite specific in a given scope [[23](#bib.bib23)][[24](#bib.bib24)].
    In this sense, a unified attention framework supported by a taxonomy with concepts
    founded on psychology and neuroscience is necessary to elucidate how the different
    attentional mechanisms act in DL, facilitating the visualization of new research
    opportunities. Thus, we aim in this work to present the reader with a taxonomy
    of attention for neural networks based on various theoretical insights on the
    attention and several relevant researches that precedes Deep Learning [[1](#bib.bib1)].
    We formulated a very broad and generic taxonomy around five main dimensions: components
    (Section [IV-A](#S4.SS1 "IV-A Components: Selective, Divided, Oriented and Sustained
    ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")), function of the process (Section [IV-B](#S4.SS2 "IV-B
    Selective Perception versus Selective Cognition ‣ IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")), stimulus’
    nature (Section [IV-C](#S4.SS3 "IV-C Stimulus’ Nature ‣ IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")), process’
    nature according to the stimulus (Section [IV-D](#S4.SS4 "IV-D Bottom-Up versus
    Top-Down Models ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models
    in Deep Learning: Survey and Taxonomy")), and continuity (Section [IV-E](#S4.SS5
    "IV-E Continuity: Soft versus Hard ‣ IV A Taxonomy for Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy")).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，在深度学习中，没有基于注意力理论概念的分类法，因为现有的少数分类法远离理论概念，并且在特定范围内较为具体[[23](#bib.bib23)][[24](#bib.bib24)]。在这种情况下，基于心理学和神经科学概念的统一注意力框架是必要的，以阐明不同的注意力机制在深度学习中的作用，便于发现新的研究机会。因此，我们的目标是为读者呈现一个基于多种理论见解和深度学习之前的相关研究的神经网络注意力分类法[[1](#bib.bib1)]。我们围绕五个主要维度制定了一个非常广泛和通用的分类法：组件（第
    [IV-A](#S4.SS1 "IV-A Components: Selective, Divided, Oriented and Sustained ‣
    IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy") 节），过程的功能（第 [IV-B](#S4.SS2 "IV-B Selective Perception versus
    Selective Cognition ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models
    in Deep Learning: Survey and Taxonomy") 节），刺激的性质（第 [IV-C](#S4.SS3 "IV-C Stimulus’
    Nature ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy") 节），根据刺激的过程性质（第 [IV-D](#S4.SS4 "IV-D Bottom-Up
    versus Top-Down Models ‣ IV A Taxonomy for Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy") 节），以及连续性（第 [IV-E](#S4.SS5 "IV-E
    Continuity: Soft versus Hard ‣ IV A Taxonomy for Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy") 节）。'
- en: I-A Contributions
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 贡献
- en: This survey presents a taxonomy that corroborates theoretical aspects of attention
    that predate Deep Learning. Based on our taxonomy, we present and discuss the
    main neural attention models in the field.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查呈现了一种与深度学习之前的注意力理论方面相符合的分类法。基于我们的分类法，我们展示并讨论了该领域主要的神经注意力模型。
- en: 'As the main contributions of our work, we highlight:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们工作的主要贡献，我们强调：
- en: '1.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We proposed a unified attention framework;
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个统一的注意力框架；
- en: '2.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduced the first survey with a taxonomy based on theoretical concepts
    of attention;
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首次提出了基于注意力理论概念的分类调查；
- en: '3.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We systematically review the main neural attention models and provide all of
    our search and filtering tools to help researchers in future searches on the topic
    ¹¹1Download link: [https://github.com/larocs/attention_dl](https://github.com/larocs/attention_dl);'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们系统回顾了主要的神经注意力模型，并提供了所有的搜索和过滤工具，帮助研究人员在未来的主题搜索中¹¹1下载链接：[https://github.com/larocs/attention_dl](https://github.com/larocs/attention_dl)；
- en: '4.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We select the main works through exhaustive search and filtering techniques
    and choose the most relevant among more than 650 papers critically analyzed;
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过详尽的搜索和过滤技术选择了主要的研究成果，并在分析的650多篇论文中挑选出最相关的；
- en: '5.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Based on our taxonomy, we discuss the biological plausibility of the main attentional
    mechanisms;
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于我们的分类法，我们讨论了主要注意力机制的生物学可能性；
- en: '6.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Based on the concepts presented in our taxonomy, we describe in detail the main
    attentional systems of the field;
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于我们分类法中提出的概念，我们详细描述了该领域的主要注意力系统；
- en: '7.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Finally, we present a broad description of trends and research opportunities.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们提供了对趋势和研究机会的广泛描述。
- en: I-B Organization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 组织结构
- en: 'This survey is structured as follows. In Section  [II](#S2 "II The concept
    of attention ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")
    we present the concept of attention. Section [III](#S3 "III An unified Attention
    Framework ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") contains
    an unified attention model. In section  [IV](#S4 "IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") we introduce
    and discuss our taxonomy. Section [V](#S5 "V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy"), we present the main
    architectures and discuss the central models from our taxonomy perspective. Finally,
    in Section  [VI](#S6 "VI Discussion ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy") we discuss limitations, open challenges, current trends,
    and future directions in the area, concluding our work in section  [VII](#S7 "VII
    Conclusions ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的结构如下。在第[II](#S2 "II The concept of attention ‣ Neural Attention Models
    in Deep Learning: Survey and Taxonomy")节中，我们介绍了注意力的概念。第[III](#S3 "III An unified
    Attention Framework ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")节包含了一个统一的注意力模型。在第[IV](#S4
    "IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")节中，我们介绍并讨论了我们的分类法。在第[V](#S5 "V Neural Attention Models ‣
    Neural Attention Models in Deep Learning: Survey and Taxonomy")节中，我们展示了主要的架构，并从我们的分类法角度讨论了中心模型。最后，在第[VI](#S6
    "VI Discussion ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")节中，我们讨论了该领域的局限性、开放挑战、当前趋势和未来方向，并在第[VII](#S7
    "VII Conclusions ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")节中总结了我们的工作。'
- en: II The concept of attention
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 注意力的概念
- en: 'Attention is difficult to define formally and universally. In psychology, Attention
    is the act or state of attending, primarily through applying the mind to an object
    of sense or thought or even as a condition of readiness that selectively narrows
    or focuses consciousness and receptivity [[1](#bib.bib1)]. However, from a Deep
    Learning standpoint, a clear definition of attention is needed. In our definition,
    Attention is a system composed of one or multiple modules, which allocate structural
    or temporal resources, select or modulate signals to perform a task. Each module
    consists of a function or multiple non-linear functions trained in conjunction
    with the neural network. Specifically, each module outputs a selective or modulating
    mask for an input signal. The structural resources allocated are elements of the
    architecture (e.g., number of neurons, number of layers), time resources refer
    to computation per step, number of time steps, processing time in modules of the
    architectures or frameworks. The task is the goal application (e.g., classification,
    regression, segmentation, object recognition, control, among others), and signals
    are given at any abstraction level (e.g., features, visual information, audio,
    text, memories, latent space vectors). Our definition is supported by the following
    sections of this work, mainly for our taxonomy (Section  [IV](#S4 "IV A Taxonomy
    for Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力很难正式和普遍地定义。在心理学中，注意力是指注意的行为或状态，主要是通过将思维集中在感官或思维的对象上，或甚至作为一种准备状态，这种状态选择性地缩小或集中意识和接受能力[[1](#bib.bib1)]。然而，从深度学习的角度来看，需要对注意力有一个明确的定义。在我们的定义中，注意力是由一个或多个模块组成的系统，这些模块分配结构或时间资源，选择或调节信号以执行任务。每个模块由一个或多个非线性函数组成，这些函数与神经网络一起训练。具体来说，每个模块输出一个选择性或调节性的掩膜，用于处理输入信号。分配的结构资源是体系结构的元素（例如，神经元数量、层数），时间资源指的是每一步的计算量、时间步数、体系结构或框架中模块的处理时间。任务是目标应用（例如，分类、回归、分割、目标识别、控制等），信号可以在任何抽象级别提供（例如，特征、视觉信息、音频、文本、记忆、潜在空间向量）。我们的定义得到了本文以下部分的支持，主要是我们的分类法（第[IV](#S4
    "IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")节）。'
- en: III An unified Attention Framework
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 统一的注意力框架
- en: 'In this section, we define a general and unified model of attention. Our model
    corroborates with theoretical aspects and is independent of the architecture and
    the application domain. Specifically, we consider that an attentional system contains
    a set of attentional subsystems - even in a recursive manner - to allocate resources
    for processes. An attentional subsystem, at each time step $t$, receives as input
    a contextual input $c_{t}$, a focus target $\tau_{t}$, and past inner state $i_{t-1}$.
    And produces as output a current inner state $i_{t}$, and current focus output
    $a_{t}$, as shown Figure [1](#S3.F1 "Figure 1 ‣ III An unified Attention Framework
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy"). The focused
    output is the main element of the subsystem because it assigns targets an importance
    score. Together, several attentional subsystems always perform actions to provide
    selection capabilities. The subsystem profile depends on the data structure and
    the desired output. We propose a general structure with some additional components
    that, although not universally present, are still found in most models in the
    literature. In Figure [2](#S3.F2 "Figure 2 ‣ III An unified Attention Framework
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy"), we list all
    key components.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们定义了一个通用且统一的注意力模型。我们的模型与理论方面相符，并且独立于架构和应用领域。具体来说，我们认为一个注意力系统包含一组注意力子系统——甚至以递归方式——用于分配资源以进行处理。一个注意力子系统在每个时间步$t$，接收作为输入的上下文输入$c_{t}$、焦点目标$\tau_{t}$和过去的内部状态$i_{t-1}$。它的输出是当前的内部状态$i_{t}$和当前的焦点输出$a_{t}$，如图[1](#S3.F1
    "Figure 1 ‣ III An unified Attention Framework ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy")所示。焦点输出是子系统的主要元素，因为它为目标分配重要性评分。多个注意力子系统共同作用，提供选择能力。子系统的特征取决于数据结构和所需的输出。我们提出了一个通用结构，并添加了一些附加组件，虽然这些组件并非普遍存在，但在文献中的大多数模型中仍然可以找到。在图[2](#S3.F2
    "Figure 2 ‣ III An unified Attention Framework ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy")中，我们列出了所有关键组件。'
- en: '![Refer to caption](img/2f5f6d2b3be96072b297be1d833fef0f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2f5f6d2b3be96072b297be1d833fef0f.png)'
- en: 'Figure 1: Illustration of our attentional framework for DL, in which several
    attentional subsystems are coupled in the neural networks sequentially or recurrently.
    Each subsystem has a different profile based on the input data’s structure and
    sensory modality. A single subsystem receives as the primary input the focus target
    (i.e., the stimulus to be filtered), and sometimes auxiliary inputs (e.g., contextual
    information and subsystem’s previous internal state) to help the mechanism guide
    the focus in time.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的注意力框架示意图，其中多个注意力子系统在神经网络中按顺序或递归地耦合。每个子系统根据输入数据的结构和感官模态具有不同的特征。单个子系统将关注目标（即要过滤的刺激）作为主要输入，有时还会接收辅助输入（例如，上下文信息和子系统的前一个内部状态），以帮助机制在时间上引导关注。
- en: '| Symbol | Description |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Context |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 |'
- en: '| --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| $k$ | Sensory modality index. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $k$ | 感官模态索引。 |'
- en: '| $C$ |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $C$ |'
- en: '&#124; Contextual input set, $C=\{c_{t-1},\ldots,c_{t}\}$, $C\in\mathbb{R}$,
    (e.g., hidden states, memory data, sensory data). &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文输入集，$C=\{c_{t-1},\ldots,c_{t}\}$，$C\in\mathbb{R}$，（例如，隐藏状态、记忆数据、感官数据）。
    &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $c_{t}$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $c_{t}$ |'
- en: '&#124; Contextual input at time $t$, $c_{t}=\{c_{t}^{1},\ldots,c_{t}^{k}\}$,
    $c_{t}\in C$. &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间$t$的上下文输入，$c_{t}=\{c_{t}^{1},\ldots,c_{t}^{k}\}$，$c_{t}\in C$。 &#124;'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $c^{k}_{t}$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| $c^{k}_{t}$ |'
- en: '&#124; Contextual input from sensory modality $k$ at time $t$, $c_{t}^{k}=\{c_{t,1}^{k},\ldots,c_{t,n_{ck}}^{k}\}$,
    $c_{t,j}^{k}\in\mathbb{R}^{F_{c}}$, where $F_{c}$ is &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间$t$的感官模态$k$的上下文输入，$c_{t}^{k}=\{c_{t,1}^{k},\ldots,c_{t,n_{ck}}^{k}\}$，$c_{t,j}^{k}\in\mathbb{R}^{F_{c}}$，其中$F_{c}$是
    &#124;'
- en: '&#124; amount of features. &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征的数量。 &#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Focus target |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 关注目标 |'
- en: '| $T$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| $T$ |'
- en: '&#124; Focus target set, $T=\{\tau_{t-1},\ldots,\tau_{t}\}$, $T\in\mathbb{R}$.
    &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标关注集，$T=\{\tau_{t-1},\ldots,\tau_{t}\}$，$T\in\mathbb{R}$。 &#124;'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $\tau_{t}$ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| $\tau_{t}$ |'
- en: '&#124; Focus target at time $t$, $\tau_{t}=\{\tau_{t}^{1},\ldots,\tau_{t}^{k}\}$,
    $\tau_{t}\in T$. &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间$t$的关注目标，$\tau_{t}=\{\tau_{t}^{1},\ldots,\tau_{t}^{k}\}$，$\tau_{t}\in
    T$。 &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $\tau_{t}^{k}$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| $\tau_{t}^{k}$ |'
- en: '&#124; Focus target from sensory modality $k$ &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 感官模态$k$的关注目标 &#124;'
- en: '&#124; Features for $n_{\tau k}$ elements, if $\tau_{t}^{k}$ is a data, Hyperparameters
    or index, if $\tau_{t}^{k}$ is a program. &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于$n_{\tau k}$个元素的特征，如果$\tau_{t}^{k}$是数据，则为超参数或索引，如果$\tau_{t}^{k}$是程序。
    &#124;'
- en: '&#124; $\tau_{t}^{k}=\{\tau_{t,1}^{k},\ldots,\tau_{t,n_{\tau k}}^{k}\}$, $\tau_{t,j}^{k}\in$  $\mathbb{R}^{F_{\tau
    k}}$, where $F_{\tau k}$ is amount of features. &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\tau_{t}^{k}=\{\tau_{t,1}^{k},\ldots,\tau_{t,n_{\tau k}}^{k}\}$，$\tau_{t,j}^{k}\in$
    $\mathbb{R}^{F_{\tau k}}$，其中$F_{\tau k}$是特征的数量。 &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Inner state |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 内部状态 |'
- en: '| $I$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| $I$ |'
- en: '&#124; Inner state set, $I=\{i_{t-1},\ldots,i_{t}\}$, $I\in\mathbb{R}$. &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 内部状态集，$I=\{i_{t-1},\ldots,i_{t}\}$，$I\in\mathbb{R}$。 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $i_{t}$ | Inner state at time $t$, $\iota_{t}\in I$. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| $i_{t}$ | 时间$t$的内部状态，$\iota_{t}\in I$。 |'
- en: '| $i_{t}-1$ | Past inner state at time $t-1$, $\iota_{t-1}\in I$. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $i_{t}-1$ | 时间$t-1$的过去内部状态，$\iota_{t-1}\in I$。 |'
- en: '| Focus output |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 关注输出 |'
- en: '| $A$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| $A$ |'
- en: '&#124; Focus output set, $A=\{a_{t-1},\ldots,a_{t}\}$, $A=\left\{x\in\mathbb{R}:0<x<1\right\}$
    or $A=\left\{x\in\mathbb{Z}:0\leq x\leq 1\right\}$. &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关注输出集，$A=\{a_{t-1},\ldots,a_{t}\}$，$A=\left\{x\in\mathbb{R}:0<x<1\right\}$或$A=\left\{x\in\mathbb{Z}:0\leq
    x\leq 1\right\}$。 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $a_{t}$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| $a_{t}$ |'
- en: '&#124; Focus output at time $t$, $a_{t}=\{a_{t}^{1},\ldots,a_{t}^{k}\}\in A$.
    &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间$t$的关注输出，$a_{t}=\{a_{t}^{1},\ldots,a_{t}^{k}\}\in A$。 &#124;'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $a_{t}^{k}$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| $a_{t}^{k}$ |'
- en: '&#124; Focus output from sensory modality $k$ at time $t$, $a_{t}^{k}=\{a_{t,1}^{k},\ldots,a_{t,n_{\tau
    k}}^{k}\}$ are attention scores, &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时间$t$的感官模态$k$的关注输出，$a_{t}^{k}=\{a_{t,1}^{k},\ldots,a_{t,n_{\tau k}}^{k}\}$是注意力分数，
    &#124;'
- en: '&#124; $a_{t,j}^{k}\in\mathbb{R}^{F_{\tau k}}$ or $a_{t,j}^{k}\in\mathbb{R}$,
    $a_{t}^{k}\in\mathbb{R}^{n_{\tau k}\times F_{\tau k}}$ or $a_{t}^{k}\in\mathbb{R}^{n_{\tau
    k}}$. &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $a_{t,j}^{k}\in\mathbb{R}^{F_{\tau k}}$或$a_{t,j}^{k}\in\mathbb{R}$，$a_{t}^{k}\in\mathbb{R}^{n_{\tau
    k}\times F_{\tau k}}$或$a_{t}^{k}\in\mathbb{R}^{n_{\tau k}}$。 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 2: Notation for unified attention model. Note the notation supports
    recurrence and multimodality.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：统一注意力模型的符号说明。注意符号支持递归和多模态。
- en: IV A Taxonomy for Attention Models
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 注意力模型的分类
- en: 'This section introduces our taxonomy around 16 factors that will be used to
    categorize and discuss the main neural attention models summarized in Figure [3](#S4.F3
    "Figure 3 ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy"). These factors have their origins in behavioral
    and computational studies of attention. In section [IV-A](#S4.SS1 "IV-A Components:
    Selective, Divided, Oriented and Sustained ‣ IV A Taxonomy for Attention Models
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") we present the
    models of attention from the component perspective, whereas in section [IV-B](#S4.SS2
    "IV-B Selective Perception versus Selective Cognition ‣ IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") we discuss
    the process function presenting mechanisms of perceptual and cognitive selection.
    Section [IV-C](#S4.SS3 "IV-C Stimulus’ Nature ‣ IV A Taxonomy for Attention Models
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") presents the
    mechanisms according to the nature of the stimulus, while in section [IV-D](#S4.SS4
    "IV-D Bottom-Up versus Top-Down Models ‣ IV A Taxonomy for Attention Models ‣
    Neural Attention Models in Deep Learning: Survey and Taxonomy") we discuss the
    mechanisms analyzing the nature of the process according to the stimulus. Finally,
    in section [IV-E](#S4.SS5 "IV-E Continuity: Soft versus Hard ‣ IV A Taxonomy for
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")
    we present the mechanisms from the continuity standpoint.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了我们围绕16个因素的分类法，这些因素将用于分类和讨论在图[3](#S4.F3 "图3 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")中总结的主要神经注意力模型。这些因素起源于对注意力的行为和计算研究。在[IV-A](#S4.SS1
    "IV-A 组成部分：选择性、分裂性、定向性和持续性 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们从组成部分的角度展示注意力模型，而在[IV-B](#S4.SS2
    "IV-B 选择性知觉与选择性认知 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们讨论了过程功能，呈现了感知和认知选择机制。[IV-C](#S4.SS3
    "IV-C 刺激的性质 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")节根据刺激的性质展示了机制，而在[IV-D](#S4.SS4
    "IV-D 自下而上与自上而下模型 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们讨论了根据刺激分析过程性质的机制。最后，在[IV-E](#S4.SS5
    "IV-E 连续性：软性与硬性 ‣ IV A 注意力模型的分类法 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们从连续性的角度展示了机制。
- en: '| No | Model | Year | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 | f11
    | f12 | f13 | f14 | f15 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 编号 | 模型 | 年份 | f1 | f2 | f3 | f4 | f5 | f6 | f7 | f8 | f9 | f10 | f11 | f12
    | f13 | f14 | f15 |'
- en: '| Bottom-up |  |  |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 自下而上 |  |  |  |'
- en: '| 1 | STN [[25](#bib.bib25)] | 2015 | + | - | - | - | - | + | FM | - | - |
    - | - | + | - | + | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | STN [[25](#bib.bib25)] | 2015 | + | - | - | - | - | + | FM | - | - |
    - | - | + | - | + | - |'
- en: '| 2 | Chen et al. [[26](#bib.bib26)] | 2016 | - | + | - | - | - | + | - | -
    | L | - | - | - | + | - | + |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Chen 等 [[26](#bib.bib26)] | 2016 | - | + | - | - | - | + | - | - | L
    | - | - | - | + | - | + |'
- en: '| 3 | AT [[27](#bib.bib27)] | 2016 | - | + | - | - | - | + | FM | - | - | -
    | - | + | - | - | + |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 3 | AT [[27](#bib.bib27)] | 2016 | - | + | - | - | - | + | FM | - | - | -
    | - | + | - | - | + |'
- en: '| 4 | SNAIL [[28](#bib.bib28)] | 2017 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 4 | SNAIL [[28](#bib.bib28)] | 2017 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
- en: '| 5 | SENet [[29](#bib.bib29)] | 2018 | - | + | - | - | - | + | - | - | V |
    - | - | + | - | - | + |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 5 | SENet [[29](#bib.bib29)] | 2018 | - | + | - | - | - | + | - | - | V |
    - | - | + | - | - | + |'
- en: '| 6 | GAT [[20](#bib.bib20)] | 2018 | - | + | - | - | - | + | H | - | H | -
    | - | + | + | - | + |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 6 | GAT [[20](#bib.bib20)] | 2018 | - | + | - | - | - | + | H | - | H | -
    | - | + | + | - | + |'
- en: '| 7 | $A^{2}-Nets$ [[30](#bib.bib30)] | 2018 | - | + | - | - | - | + | FM |
    - | - | - | - | - | + | - | + |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 7 | $A^{2}-Nets$ [[30](#bib.bib30)] | 2018 | - | + | - | - | - | + | FM |
    - | - | - | - | - | + | - | + |'
- en: '| 8 | DANet [[31](#bib.bib31)] | 2019 | - | + | + | - | - | + | FM | - | V
    | - | - | + | - | - | + |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 8 | DANet [[31](#bib.bib31)] | 2019 | - | + | + | - | - | + | FM | - | V
    | - | - | + | - | - | + |'
- en: '| 9 | HAN [[32](#bib.bib32)] | 2019 | - | + | - | - | - | + | H | - | H | -
    | - | + | + | - | + |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 9 | HAN [[32](#bib.bib32)] | 2019 | - | + | - | - | - | + | H | - | H | -
    | - | + | + | - | + |'
- en: '| 10 | TIM [[33](#bib.bib33)] | 2021 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 10 | TIM [[33](#bib.bib33)] | 2021 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
- en: '| Top-down |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 自上而下 |  |  |  |'
- en: '| 11 | RNNSearch [[34](#bib.bib34)] | 2014 | - | + | + | - | - | + | H | -
    | - | - | - | - | + | - | + |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 11 | RNNSearch [[34](#bib.bib34)] | 2014 | - | + | + | - | - | + | H | -
    | - | - | - | - | + | - | + |'
- en: '| 12 | Tang et al. [[35](#bib.bib35)] | 2014 | + | - | - | + | + | - | - |
    + | - | - | - | - | + | + | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 12 | Tang 等 [[35](#bib.bib35)] | 2014 | + | - | - | + | + | - | - | + | -
    | - | - | - | + | + | - |'
- en: '| 13 | aNN [[36](#bib.bib36)] | 2014 | - | + | + | - | - | + | - | - | + |
    - | - | - | + | - | + |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 13 | aNN [[36](#bib.bib36)] | 2014 | - | + | + | - | - | + | - | - | + |
    - | - | - | + | - | + |'
- en: '| 14 | NTM [[37](#bib.bib37)] | 2014 | - | + | + | - | - | + | EM | - | O |
    - | - | - | + | + | + |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 14 | NTM [[37](#bib.bib37)] | 2014 | - | + | + | - | - | + | EM | - | O |
    - | - | - | + | + | + |'
- en: '| 15 | RAM [[38](#bib.bib38)] | 2014 | + | - | + | - | + | - | I | - | - |
    - | - | - | + | + | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 15 | RAM [[38](#bib.bib38)] | 2014 | + | - | + | - | + | - | I | - | - |
    - | - | - | + | + | - |'
- en: '| 16 | dasNet [[39](#bib.bib39)] | 2014 | - | + | + | - | - | + | - | - | V
    | - | - | - | + | - | + |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 16 | dasNet [[39](#bib.bib39)] | 2014 | - | + | + | - | - | + | - | - | V
    | - | - | - | + | - | + |'
- en: '| 17 | EMNet [[19](#bib.bib19)] | 2015 | - | + | - | - | - | + | EM | - | -
    | - | - | - | + | - | + |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 17 | EMNet [[19](#bib.bib19)] | 2015 | - | + | - | - | - | + | EM | - | -
    | - | - | - | + | - | + |'
- en: '| 18 | DRAW [[40](#bib.bib40)] | 2015 | + | + | + | - | + | + | I/H | - | -
    | - | - | - | + | - | + |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 18 | DRAW [[40](#bib.bib40)] | 2015 | + | + | + | - | + | + | I/H | - | -
    | - | - | - | + | - | + |'
- en: '| 19 | Xu et al. [[41](#bib.bib41)] | 2015 | - | + | + | - | - | + | H | -
    | - | - | - | - | + | + | + |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 19 | Xu 等人 [[41](#bib.bib41)] | 2015 | - | + | + | - | - | + | H | - | -
    | - | - | - | + | + | + |'
- en: '| 20 | Ptr-Net [[42](#bib.bib42)] | 2015 | + | + | + | - | - | + | H | - |
    - | - | - | - | + | + | + |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 20 | Ptr-Net [[42](#bib.bib42)] | 2015 | + | + | + | - | - | + | H | - |
    - | - | - | - | + | + | + |'
- en: '| 21 | Rocktäschel et al. [[43](#bib.bib43)] | 2015 | - | + | + | - | - | +
    | H | - | - | - | - | - | + | - | + |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 21 | Rocktäschel 等人 [[43](#bib.bib43)] | 2015 | - | + | + | - | - | + | H
    | - | - | - | - | - | + | - | + |'
- en: '| 22 | Luong et al. [[44](#bib.bib44)] | 2015 | - | + | + | - | - | + | H |
    - | - | - | - | - | + | - | + |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 22 | Luong 等人 [[44](#bib.bib44)] | 2015 | - | + | + | - | - | + | H | - |
    - | - | - | - | + | - | + |'
- en: '| 23 | Hermann et al. [[45](#bib.bib45)] | 2015 | - | + | + | - | - | + | H
    | - | - | - | - | - | + | - | + |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 23 | Hermann 等人 [[45](#bib.bib45)] | 2015 | - | + | + | - | - | + | H | -
    | - | - | - | - | + | - | + |'
- en: '| 24 | DMN [[46](#bib.bib46)] | 2015 | + | - | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 24 | DMN [[46](#bib.bib46)] | 2015 | + | - | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
- en: '| 25 | BiDAF [[47](#bib.bib47)] | 2016 | + | + | - | - | - | + | H | - | -
    | - | - | - | + | + | + |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 25 | BiDAF [[47](#bib.bib47)] | 2016 | + | + | - | - | - | + | H | - | -
    | - | - | - | + | + | + |'
- en: '| 26 | STRAW [[48](#bib.bib48)] | 2016 | + | + | + | - | - | + | EM | - | -
    | + | - | - | + | + | + |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 26 | STRAW [[48](#bib.bib48)] | 2016 | + | + | + | - | - | + | EM | - | -
    | + | - | - | + | + | + |'
- en: '| 27 | Allamanis et al. [[49](#bib.bib49)] | 2016 | - | + | + | - | - | + |
    H | - | - | - | - | - | + | - | + |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 27 | Allamanis 等人 [[49](#bib.bib49)] | 2016 | - | + | + | - | - | + | H |
    - | - | - | - | - | + | - | + |'
- en: '| 28 | Lu et al. [[50](#bib.bib50)] | 2016 | - | + | - | - | - | + | H | -
    | - | - | - | - | + | - | + |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 28 | Lu 等人 [[50](#bib.bib50)] | 2016 | - | + | - | - | - | + | H | - | -
    | - | - | - | + | - | + |'
- en: '| 29 | ACT [[51](#bib.bib51)] | 2016 | + | + | + | - | - | + | - | - | - |
    - | + | - | + | - | + |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 29 | ACT [[51](#bib.bib51)] | 2016 | + | + | + | - | - | + | - | - | - |
    - | + | - | + | - | + |'
- en: '| 30 | Lu et al. [[52](#bib.bib52)] | 2016 | - | + | + | - | - | + | FM/H |
    - | MC | - | - | - | + | - | + |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 30 | Lu 等人 [[52](#bib.bib52)] | 2016 | - | + | + | - | - | + | FM/H | - |
    MC | - | - | - | + | - | + |'
- en: '| 31 | HAN [[53](#bib.bib53)] | 2016 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 31 | HAN [[53](#bib.bib53)] | 2016 | - | + | + | - | - | + | H | - | - |
    - | - | - | + | - | + |'
- en: '| 32 | Excitation Backprop [[54](#bib.bib54)] | 2016 | - | + | - | - | - |
    + | N | - | - | - | - | - | + | - | + |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 32 | Excitation Backprop [[54](#bib.bib54)] | 2016 | - | + | - | - | - |
    + | N | - | - | - | - | - | + | - | + |'
- en: '| 33 | DCN [[55](#bib.bib55)] | 2016 | - | + | - | - | - | + | H | - | - |
    - | - | - | + | + | + |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 33 | DCN [[55](#bib.bib55)] | 2016 | - | + | - | - | - | + | H | - | - |
    - | - | - | + | + | + |'
- en: '| 34 | GCA-LSTM [[56](#bib.bib56)] | 2017 | - | + | + | - | - | + | H | - |
    - | - | - | - | + | - | + |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 34 | GCA-LSTM [[56](#bib.bib56)] | 2017 | - | + | + | - | - | + | H | - |
    - | - | - | - | + | - | + |'
- en: '| 35 | Reed et al. [[57](#bib.bib57)] | 2017 | - | + | + | - | - | + | H |
    - | - | - | - | - | + | - | + |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 35 | Reed 等人 [[57](#bib.bib57)] | 2017 | - | + | + | - | - | + | H | - |
    - | - | - | - | + | - | + |'
- en: '| 36 | Seo et al. [[58](#bib.bib58)] | 2017 | - | + | + | - | - | + | FM |
    - | - | - | - | - | + | - | + |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 36 | Seo 等人 [[58](#bib.bib58)] | 2017 | - | + | + | - | - | + | FM | - |
    - | - | - | - | + | - | + |'
- en: '| 37 | SAB [[59](#bib.bib59)][[60](#bib.bib60)] | 2017 | - | + | + | - | -
    | + | H | - | - | - | - | - | + | - | + |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 37 | SAB [[59](#bib.bib59)][[60](#bib.bib60)] | 2017 | - | + | + | - | -
    | + | H | - | - | - | - | - | + | - | + |'
- en: '| 38 | ACF Network [[61](#bib.bib61)] | 2017 | + | - | + | - | - | + | - |
    - | - | + | - | - | + | + | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 38 | ACF Network [[61](#bib.bib61)] | 2017 | + | - | + | - | - | + | - |
    - | - | + | - | - | + | + | - |'
- en: '| 39 | Kim et al. [[62](#bib.bib62)] | 2017 | - | + | - | - | - | + | H | -
    | - | - | - | - | + | - | + |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 39 | Kim 等人 [[62](#bib.bib62)] | 2017 | - | + | - | - | - | + | H | - | -
    | - | - | - | + | - | + |'
- en: '| 40 | BAN [[63](#bib.bib63)] | 2018 | - | + | - | - | - | + | - | - | V |
    - | - | - | + | - | + |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 40 | BAN [[63](#bib.bib63)] | 2018 | - | + | - | - | - | + | - | - | V |
    - | - | - | + | - | + |'
- en: '| 41 | AG [[64](#bib.bib64)] | 2018 | - | + | - | - | - | + | FM | - | - |
    - | - | - | + | - | + |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 41 | AG [[64](#bib.bib64)] | 2018 | - | + | - | - | - | + | FM | - | - |
    - | - | - | + | - | + |'
- en: '| 42 | Perera et al. [[65](#bib.bib65)] | 2018 | - | + | + | - | - | + | H
    | - | MC | - | - | - | + | - | + |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 42 | Perera et al. [[65](#bib.bib65)] | 2018 | - | + | + | - | - | + | H
    | - | MC | - | - | - | + | - | + |'
- en: '| 43 | Deng et al. [[66](#bib.bib66)] | 2018 | - | + | - | - | - | + | H |
    - | - | - | - | - | + | - | + |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 43 | Deng et al. [[66](#bib.bib66)] | 2018 | - | + | - | - | - | + | H |
    - | - | - | - | - | + | - | + |'
- en: '| 44 | HAN [[67](#bib.bib67)] | 2020 | - | + | - | - | - | + | M | - | - |
    - | - | - | + | - | + |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 44 | HAN [[67](#bib.bib67)] | 2020 | - | + | - | - | - | + | M | - | - |
    - | - | - | + | - | + |'
- en: '| 45 | IMRAM [[68](#bib.bib68)] | 2020 | - | + | - | - | - | + | H | - | -
    | - | - | - | + | - | + |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 45 | IMRAM [[68](#bib.bib68)] | 2020 | - | + | - | - | - | + | H | - | -
    | - | - | - | + | - | + |'
- en: '| 46 | Lekkala et al. [[69](#bib.bib69)] | 2020 | - | + | - | - | - | + | -
    | - | V | - | - | - | + | - | + |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 46 | Lekkala et al. [[69](#bib.bib69)] | 2020 | - | + | - | - | - | + | -
    | - | V | - | - | - | + | - | + |'
- en: '| Hybrid |  |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 混合 |  |  |  |'
- en: '| 47 | Transformer [[18](#bib.bib18)] | 2017 | - | + | + | - | - | + | H |
    - | - | - | - | - | + | - | + |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 47 | Transformer [[18](#bib.bib18)] | 2017 | - | + | + | - | - | + | H |
    - | - | - | - | - | + | - | + |'
- en: '| 48 | DiSAN [[70](#bib.bib70)] | 2018 | - | + | - | - | - | + | H | - | L
    | - | - | - | + | - | + |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 48 | DiSAN [[70](#bib.bib70)] | 2018 | - | + | - | - | - | + | H | - | L
    | - | - | - | + | - | + |'
- en: '| 49 | ANP [[71](#bib.bib71)] | 2019 | - | + | - | - | + | + | O | - | - |
    - | - | - | + | - | + |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 49 | ANP [[71](#bib.bib71)] | 2019 | - | + | - | - | + | + | O | - | - |
    - | - | - | + | - | + |'
- en: '| 50 | BRIMs [[72](#bib.bib72)] | 2020 | - | + | + | - | - | + | H | - | -
    | - | - | - | + | - | + |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 50 | BRIMs [[72](#bib.bib72)] | 2020 | - | + | + | - | - | + | H | - | -
    | - | - | - | + | - | + |'
- en: '| 51 | MSAN [[73](#bib.bib73)] | 2020 | + | + | + | - | - | + | H | - | - |
    + | - | - | + | + | + |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 51 | MSAN [[73](#bib.bib73)] | 2020 | + | + | + | - | - | + | H | - | - |
    + | - | - | + | + | + |'
- en: 'Figure 3: Summary of main neural attention models. Factors in order are: Selective
    (f1), divided (f2), oriented (f3), sustained (f4), selective perception (f5),
    selective cognition (f6), location-based (f7), object-based (f8), feature-based
    (f9), task-oriented (f10), time-oriented (f11), stateless (f12), stateful (f13),
    hard (f14), soft (f15). In the location-based column (f7) column: hidden states/data
    embeddings (H), external memory cells (EM), feature maps (FM), input data (I),
    and others (O). In the feature-based column (f9) column: visual (V), linguistic
    (L), memory cell (MC), hidden states (H), and others (O). In the other columns,
    the presence of the feature is indicated by the + symbol and absence by the -
    symbol.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：主要神经注意力模型的总结。因素的顺序是：选择性（f1），分裂性（f2），定向性（f3），持续性（f4），选择性感知（f5），选择性认知（f6），基于位置（f7），基于对象（f8），基于特征（f9），任务导向（f10），时间导向（f11），无状态（f12），有状态（f13），硬性（f14），软性（f15）。在基于位置（f7）列中：隐藏状态/数据嵌入（H），外部记忆单元（EM），特征图（FM），输入数据（I），和其他（O）。在基于特征（f9）列中：视觉（V），语言（L），记忆单元（MC），隐藏状态（H），和其他（O）。在其他列中，特征的存在用
    + 符号表示，缺失用 - 符号表示。
- en: 'IV-A Components: Selective, Divided, Oriented and Sustained'
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 组件：选择性、分裂性、定向性和持续性
- en: Each attention subsystem can have a selective, divided, oriented, or sustained
    property. Regarding the number of elements that we pay attention to simultaneously,
    we can classify subsystems as selective or divided attention. Regarding attention
    between instants of time, attention can be oriented or sustained. Selective attention
    chooses only one stimulus among all the others. In contrast, divided attention
    is the highest level of attention and refers to responding simultaneously to multiple
    stimuli or tasks. Biologically, divided attention can only operate two tasks simultaneously
    if one of them is mediated by automatic processes and the other by a cognitive
    [[74](#bib.bib74)] so that only one task requires much intellectual effort.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力子系统可以具有选择性、分裂性、定向性或持续性属性。关于我们同时关注的元素数量，我们可以将子系统分类为选择性注意力或分裂注意力。关于时间瞬间之间的注意力，注意力可以是定向的或持续的。选择性注意力只选择一个刺激，而分裂注意力则是最高级别的注意力，指的是同时响应多个刺激或任务。从生物学角度看，如果一个任务由自动过程介导，另一个由认知过程介导，则分裂注意力只能同时操作两个任务，以便只有一个任务需要大量智力努力。
- en: 'Oriented attention can shift the focus by leaving a stimulus for another through
    three processes: 1) leave the current focus, 2) change the focus to the expected
    stimulus, and 3) locate the target and maintain attention. Such a component represents
    the ability to coordinate simultaneous tasks to be interrupted and resumed temporarily.
    This function is usually linked to the central executive who coordinates and manages
    the information processing activities of the brain [[75](#bib.bib75)]. At working
    memory (WM), oriented attention mechanisms are extremely useful in determining
    which perceptual and long-term memory information is loaded and deleted at each
    time step $t$ among all existing information. Oriented components often select
    a single item or set of items from all the information available to feed some
    mental process input. Sustained attention or vigilance allows the maintenance
    of the goal over time through directly focusing on specific stimuli to complete
    a planned activity. This component plays a fundamental role in learning, performing
    daily tasks, maintaining dialogues and social relationships, among many other
    skills that affect mental health [[76](#bib.bib76)]. Despite this, sustained attention
    is generally less studied than transient aspects of attention, such as shifting,
    dividing, and attentional selection. What distinguishes sustained attention is
    the focus on performance on a single task over time. There are fluctuations within
    the individual’s overall ability to maintain stable performance on the task, with
    a trade-off between exposure to stimuli and a recovery period.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定向注意力可以通过三个过程将焦点从一个刺激转移到另一个刺激：1) 离开当前焦点，2) 将焦点转移到预期的刺激上，3) 确定目标并维持注意力。这一组件代表了协调同时进行的任务的能力，以便在被打断后能够暂时恢复。这一功能通常与中央执行系统相关联，中央执行系统协调和管理大脑的信息处理活动[[75](#bib.bib75)]。在工作记忆（WM）中，定向注意力机制在确定每个时间步$t$中加载和删除的感知信息及长期记忆信息时极为有用。定向组件通常从所有可用的信息中选择一个或一组项目，以提供某些心理过程的输入。持续注意力或警觉性通过直接集中注意于特定刺激以完成计划活动，从而在时间上维持目标。这个组件在学习、执行日常任务、维持对话和社会关系等众多影响心理健康的技能中扮演着基础性角色[[76](#bib.bib76)]。尽管如此，持续注意力通常比注意力的瞬态方面（如转移、分配和选择）研究得少。持续注意力的区别在于专注于单一任务的表现持续时间。个体在任务上保持稳定表现的总体能力存在波动，并在暴露于刺激和恢复期间之间存在权衡。
- en: In our framework, selective attention can be understood as the ability of the
    subsystem to select only one stimulus from the focus target among all stimuli
    or select only a subset of stimuli from the target with the same attentional weight
    and completely inhibit the response of the unselected stimuli. On the other hand,
    in divided attention the attentional mask is distributed over the entire input
    focus target so that no stimulus is completely inhibited, only modulated with
    weights greater than or less than its original values. In oriented attention,
    at time $t_{1}$, the attentional focus is on a non-empty subset of elements of
    the focus target $\tau_{t}$, at time $t_{2}$ the focus is shifted to a new subset
    of elements from the same target or another target, at time $t_{3}$ the focus
    may be a different target or initial target. However, if the target does not change
    over time and the attentional mask is always the same, or if the target changes
    over time but the attentional mask remains on the same semantic elements as the
    target, the system is sustained attention. For example, if the target is a sequence
    of images and the attentional mask remains centered around the same object throughout
    the sequence, or if the mask remains on the same features as the image, the system
    is sustained attention. If the attentional system can choose to change its focus
    in time or remain in the same focus, the system is also considered oriented attention
    because, at some point, there is the possibility of switching.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的框架中，选择性注意可以理解为子系统从所有刺激中仅选择一个来自焦点目标的刺激，或者从目标中选择仅具有相同注意权重的刺激子集，并完全抑制未选择刺激的反应。另一方面，在分散注意力中，注意力面罩分布在整个输入焦点目标上，因此没有刺激会被完全抑制，只是被调节为比其原始值更大或更小的权重。在定向注意中，在时间
    $t_{1}$，注意力集中在焦点目标 $\tau_{t}$ 的一个非空元素子集上；在时间 $t_{2}$，注意力转移到来自相同目标或其他目标的新元素子集上；在时间
    $t_{3}$，注意力可能集中在一个不同的目标或初始目标上。然而，如果目标随时间不变且注意力面罩始终相同，或者如果目标随时间变化但注意力面罩保持在与目标相同的语义元素上，则系统为持续注意。例如，如果目标是一系列图像，且注意力面罩在整个序列中始终围绕同一对象，或者面罩始终保持在图像的相同特征上，则系统为持续注意。如果注意力系统可以选择在时间上改变其焦点或保持在相同焦点上，则系统也被认为是定向注意，因为在某些时候，存在切换的可能性。
- en: Oriented or sustained systems are present only in sequential/recurring architectures.
    The first oriented systems emerged through visual search engines in images and
    encoder-decoder frameworks. Visual search architectures based on RAM [[38](#bib.bib38)]
    use the oriented attention to control the perceptual sensors’ focus. At each time
    step $t$, the sensors act as retina selecting only a portion of the input image
    for subsequent processing. The selected image portion receives the same attentional
    weight while the other regions are completely inhibited from further processing,
    presenting selective characteristics of attention. A DRAW [[40](#bib.bib40)] uses
    a very similar oriented component for image generation. At each time step $t$,
    two attentional components shift the focus from a reading head and a writing head
    to small portions of the image, allowing sequential generation and refinement
    of the portions previously generated. However, at the same time $t$, attentional
    masks explicitly divide attention between all pixels in the image. However, the
    masking computation process on targets occurs to select only desired patches,
    excluding other regions in subsequent processing, such as a selective mechanism.
    Spatial Transformer [[25](#bib.bib25)] features a selected visual search engine
    that samples only one region of interest in the image, undoes spatial deformations,
    and passes the result on to subsequent processing. This mechanism is very flexible
    regarding the temporal aspect, and its characteristics depend on the architecture
    to which it is attached. If it is in non-sequential architectures, it is only
    selected, but if it is in sequential architectures, it can act as an oriented
    or sustained attention mechanism. If the mechanism can track the same region of
    interest, it is sustained. Otherwise, it is oriented.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 面向或持续的系统仅存在于顺序/重复架构中。第一个面向系统通过图像中的视觉搜索引擎和编码器-解码器框架出现。基于RAM的视觉搜索架构[[38](#bib.bib38)]使用面向注意力来控制感知传感器的焦点。在每个时间步$t$，传感器充当视网膜，只选择输入图像的一部分进行后续处理。被选中的图像部分接收相同的注意力权重，而其他区域则完全被抑制，呈现出注意力的选择性特征。一个DRAW
    [[40](#bib.bib40)]使用了非常相似的面向组件来生成图像。在每个时间步$t$，两个注意力组件将焦点从读取头和写入头转移到图像的较小部分，从而实现对先前生成部分的顺序生成和细化。然而，在同一时间$t$，注意力掩膜明确地将注意力在图像的所有像素之间进行划分。然而，目标上的掩膜计算过程会选择仅所需的区域，排除其他区域进行后续处理，如选择性机制。空间变换器[[25](#bib.bib25)]具有一个选择性的视觉搜索引擎，它只采样图像中的一个感兴趣区域，撤销空间变形，并将结果传递给后续处理。这个机制在时间方面非常灵活，其特征取决于它附加的架构。如果它在非顺序架构中，它仅仅是被选择的，但如果它在顺序架构中，它可以作为面向或持续的注意力机制。如果机制能够跟踪相同的兴趣区域，它就是持续的。否则，它是面向的。
- en: Most of the encoder-decoder-attention frameworks for reasoning [[77](#bib.bib77)]
    [[43](#bib.bib43)][[78](#bib.bib78)], machine comprehension [[79](#bib.bib79)],
    neuralne and machine translation [[34](#bib.bib34)] are divided and oriented attention.
    At each time step $t$, the attentional mask can simultaneously hide hidden states
    from the encoder to compose a dynamic context vector with the weighting of all
    information, such as a reasoning structure in a working memory that selects and
    modulates memories to meet some mental process. The same hidden states remain
    the target in the next time step, but the system can assign attentional weights
    in a completely different way, featuring an oriented attention system. Some attentional
    systems that operate on external memories are also divided and oriented. The classic
    Neural Turing Machine [[37](#bib.bib37)] [[80](#bib.bib80)] divides attention
    on all external memory cells to retrieve a modulated representation of the stored
    memories. At each time step, oriented attention mechanisms are guided by a different
    contextual input that defines how the new distribution of attention on memories
    will be. STRAW [[48](#bib.bib48)] uses a divided and oriented read/write heads
    system to find and update regions with a greater focus on the action plan of a
    trained virtual agent via reinforcement learning. Another selected and oriented
    attentional system decides when the heads should act or if only advances in the
    time of the action-plan and commitment-plan should be made. Perera et al. [[65](#bib.bib65)]
    uses two divided and oriented mechanisms attention in LSTMs cells. The first is
    external to the LSTM cell to aggregate historical information from the previous
    hidden states in just one memory vector $h_{A}^{t-1}$. The second acts inside
    the cell as a gate layer to update the current cell vector proportional to the
    importance of each portion of $h_{A}^{t-1}$. The mechanism receives the memory
    vector $h_{A}^{t-1}$ as the target, dividing attention over the entire vector,
    giving more weight to the most relevant portions and less weight to less relevant
    ones. The current cell vector is then updated.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用于推理的编码器-解码器-注意力框架[[77](#bib.bib77)] [[43](#bib.bib43)][[78](#bib.bib78)]、机器理解[[79](#bib.bib79)]、神经网络和机器翻译[[34](#bib.bib34)]都是分离和定向注意力的。在每一个时间步
    $t$，注意力掩码可以同时隐藏来自编码器的隐藏状态，以组合一个动态上下文向量，权衡所有信息，如工作记忆中的推理结构，选择和调节记忆以满足某些心理过程。相同的隐藏状态仍然是下一个时间步的目标，但系统可以以完全不同的方式分配注意力权重，具有定向注意力系统。一些在外部记忆上操作的注意力系统也是分离和定向的。经典的神经图灵机[[37](#bib.bib37)]
    [[80](#bib.bib80)] 将注意力分配到所有外部记忆单元，以检索存储记忆的调制表示。在每一个时间步，定向注意力机制由不同的上下文输入引导，以定义对记忆的新注意力分布。STRAW[[48](#bib.bib48)]
    使用分离和定向的读/写头系统，通过强化学习找到并更新区域，更多地关注训练虚拟代理的行动计划。另一个选择的定向注意力系统决定何时行动，或是否仅应在行动计划和承诺计划的时间上前进。Perera
    等人[[65](#bib.bib65)] 在 LSTM 单元中使用了两个分离和定向的注意力机制。第一个是 LSTM 单元外部的，将前一隐藏状态的历史信息聚合成一个记忆向量
    $h_{A}^{t-1}$。第二个在单元内部作为门控层，更新当前单元向量，与 $h_{A}^{t-1}$ 的每个部分的重要性成比例。该机制将记忆向量 $h_{A}^{t-1}$
    作为目标，对整个向量进行注意力分配，对最相关的部分给予更多权重，对较不相关的部分给予较少权重。然后更新当前单元向量。
- en: The GCA-LSTM Network [[56](#bib.bib56)] uses a divided and oriented attentional
    mechanism to update, at each time step $t$, the cell state in LSTM units. The
    mechanism generates an attentional mask for the $i_{j,t}$input gate and LSTM cell’s
    spatial / temporal contextual information, based on previous hidden states. At
    dasNet [[39](#bib.bib39)] attentional mechanisms coupled in CNNs communicate the
    static convolutional structures with each other generating a sequential processing
    structure, in which oriented and divided attention systems receive as a context
    a set of feature maps from the previous time step. It then generates attentional
    masks on the feature maps of the current time step, choosing between maintaining
    or not the same attentional weights on the new feature maps. However, most convolutional
    structures are only divided attention, mainly targeting feature maps. Attention
    gated networks [[64](#bib.bib64)] presents a classic example of purely divided
    mechanisms. They receive a set of feature maps as input and return a spatial attentional
    mask over all the maps simultaneously. In this case, the attentional system is
    focused on spatial characteristics, weighting the same regions of different maps
    with the same weight, but on each map, the mask weighs each pixel with different
    weights. Some mechanisms are also quite flexible concerning architecture and temporal
    characteristics. For example, Structured Attention Networks [[62](#bib.bib62)]
    features divided mechanisms that can be coupled in sequential architectures operating
    as oriented or sustained attention.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: GCA-LSTM 网络 [[56](#bib.bib56)] 使用了一个分层且定向的注意力机制，在每个时间步 $t$ 更新 LSTM 单元中的细胞状态。该机制根据之前的隐藏状态生成
    $i_{j,t}$ 输入门和 LSTM 单元的空间/时间上下文信息的注意力掩码。在 dasNet [[39](#bib.bib39)] 中，CNNs 中的注意力机制相互通信静态卷积结构，生成一个序列处理结构，其中定向和分层注意力系统以先前时间步的特征图集作为上下文。然后，它在当前时间步的特征图上生成注意力掩码，在新的特征图上选择保持或不保持相同的注意力权重。然而，大多数卷积结构仅为分层注意力，主要针对特征图。注意力门控网络
    [[64](#bib.bib64)] 提供了纯粹分层机制的经典示例。它们接收一组特征图作为输入，并在所有图上同时返回空间注意力掩码。在这种情况下，注意力系统集中于空间特征，以相同的权重加权不同图的相同区域，但在每张图上，掩码对每个像素的权重不同。一些机制在架构和时间特征方面也相当灵活。例如，结构化注意力网络
    [[62](#bib.bib62)] 具有可以耦合在序列架构中的分层机制，作为定向或持续注意力运行。
- en: Some approaches use the oriented component to shift the focus on neural structures
    and not directly on the data. The Attentional Correlation Filter Network [[61](#bib.bib61)]
    uses an oriented and select component to choose each time step a different set
    of filtering validation strategies for the input image. Modality Shifting Attention
    [[73](#bib.bib73)] uses an oriented and select component to decide between using
    the visual modality or the linguistic modality. Few models have sustained attention.
    To our knowledge, only Tang et al. [[35](#bib.bib35)] proposed an attentional
    system selected and sustained capable of tracking a specific face in a scene amid
    occlusion situations, sudden changes in rotation, scale, and perspective.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法使用定向组件将注意力转移到神经结构上，而不是直接在数据上。注意力相关滤波网络 [[61](#bib.bib61)] 使用定向和选择组件，每个时间步选择不同的过滤验证策略来处理输入图像。模态转换注意力
    [[73](#bib.bib73)] 使用定向和选择组件来决定使用视觉模态还是语言模态。很少有模型具有持续注意力。据我们所知，只有 Tang 等人 [[35](#bib.bib35)]
    提出了一个能够在遮挡、旋转、缩放和视角突然变化的情况下追踪特定面孔的注意力系统。
- en: IV-B Selective Perception versus Selective Cognition
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 选择性感知与选择性认知
- en: 'Attention can focus on things other than the sensory stimuli that come through
    the senses. It can address mental processes, such as memories, thoughts, mental
    calculations, etc. When the focus is on the external environment, it can also
    be called selective perception, and when focused on the internal environment,
    it can be called selective cognition. We consider that perceptual selection occurs
    when the attentional subsystem receives external sensory stimuli. In this case,
    the attention acts between the raw data and the neural network reinforcing the
    perception. Selective cognition, the set $\tau_{t}$is information in the latent
    space (i.e., memory data, data embeddings, feature data, or hidden states). Despite
    the classic studies of attention in sensory perception, Deep Learning approaches
    focus on cognitive selection on hidden states/embedding vectors, external memory,
    and feature maps, as shown in Figure [3](#S4.F3 "Figure 3 ‣ IV A Taxonomy for
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")
    in f4 and f5\. The first attentional mechanism in the area, proposed for RNNSearch
    [[34](#bib.bib34)], is cognitive selection on encoder hidden states. The goal
    is to weight a dynamic context vector based on the words previously generated.
    Following this line, countless other cognitive mechanisms have been developed
    to deal with long-distance dependencies between internal memory structures in
    the encoder-decoder frameworks and even access memories external to the network.
    Subsequently, mechanisms for hierarchical alignment appeared [[53](#bib.bib53)],
    multimodal alignment [[41](#bib.bib41)][[67](#bib.bib67)], boost features [[29](#bib.bib29)][[30](#bib.bib30)],
    feature embedding [[26](#bib.bib26)] , and fusion information [[56](#bib.bib56)]
    all using internal information from the neural network. The main existing perceptual
    selection mechanisms are focused on computer vision tasks, bringing some inspirations
    from the theories of human visual attention [[38](#bib.bib38)].'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力不仅可以集中在通过感官传递的感觉刺激上，还可以关注心理过程，例如记忆、思维、心理计算等。当注意力集中在外部环境时，这也可以称为选择性感知，而当集中在内部环境时，则可以称为选择性认知。我们认为，当注意力子系统接收外部感觉刺激时，感知选择会发生。在这种情况下，注意力在原始数据和神经网络之间起作用，从而增强感知。选择性认知，即集合
    $\tau_{t}$，是潜在空间中的信息（即记忆数据、数据嵌入、特征数据或隐藏状态）。尽管经典的注意力研究集中在感觉感知上，但深度学习方法则侧重于隐藏状态/嵌入向量、外部记忆和特征图上的认知选择，如图[3](#S4.F3
    "Figure 3 ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy")中所示的f4和f5。该领域中的第一个注意力机制是为RNNSearch [[34](#bib.bib34)]提出的，侧重于编码器隐藏状态上的认知选择。其目标是基于之前生成的词来加权动态上下文向量。沿着这条路线，已经开发出无数其他认知机制，以处理编码器-解码器框架中内部记忆结构之间的长距离依赖关系，甚至访问网络外部的记忆。随后，出现了层次对齐
    [[53](#bib.bib53)]、多模态对齐 [[41](#bib.bib41)][[67](#bib.bib67)]、特征增强 [[29](#bib.bib29)][[30](#bib.bib30)]、特征嵌入
    [[26](#bib.bib26)] 和信息融合 [[56](#bib.bib56)] 机制，这些机制都使用了神经网络中的内部信息。现有的主要感知选择机制集中在计算机视觉任务上，借鉴了人类视觉注意力理论的一些启示
    [[38](#bib.bib38)]。'
- en: IV-C Stimulus’ Nature
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 刺激的性质
- en: According to the nature of the stimulus, attention can be task-oriented or time-oriented
    if the target is a program (i.e., neural network), and it can be space-based versus
    object-based if the target is a data set.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 根据刺激的性质，注意力可以是面向任务的或时间导向的，如果目标是程序（即神经网络），也可以是基于空间的或基于对象的，如果目标是数据集。
- en: IV-C1 Space-Based versus Object-Based Models
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 基于空间与基于对象的模型
- en: 'There is no consensus on the perceptual scale served by attention: Do we attend
    to stimulus locations, features, or objects? In the last 50 years, behavioral
    studies have broadly demonstrated the modulation of attention in several perceptual
    domains, including space, features, objects, and sensory modalities. The current
    belief is that attention can be deployed for each of these units, implying that
    there is no single attentional unit. However, classical studies’ main domain is
    spatial (i.e., location-based attention), which has been the focus of intense
    research since 1970\. This focus is not accidental - vision is an inherently spatial
    sense, and the first cortical stages of visual representation are spatially organized.
    Many important studies have documented spatial attention in modulating neural
    activities in the extra-striated cortex [[81](#bib.bib81)].'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 关于注意在感知尺度上的共识尚不存在：我们是否关注刺激位置、特征或对象？在过去的50年中，行为研究广泛证明了注意力在几个知觉领域的调节，包括空间、特征、对象和感觉模态。当前的观点是注意力可以用于这些单元中的每个单元，这意味着没有单一的注意单元。然而，经典研究的主要领域是空间（即基于位置的注意力），自1970年以来一直是研究的重点。这个重点并非偶然
    - 视觉是一种本质上是空间感知的感觉，而视觉表征的第一个皮质阶段是在空间上组织的。许多重要的研究已经记录了空间注意力在额外条带皮质中调节神经活动的情况。
- en: Subsequently, other attentional selection domains that are not strictly spatial
    - feature-based and object-based became the focus of investigation in classical
    literature. Feature-based attention refers to selecting stimuli based on the values
    expressed within a specific feature dimension (e.g., yellow in the color dimension
    and left in the movement dimension). Saenz et al. [[82](#bib.bib82)] did experiments
    on humans using functional magnetic resonance imaging (fMRI) and observed that
    the magnitude of neural responses depends on the stimulus features in conjunction
    with spatial aspects to select spatial and non-spatial sensory information relevant
    to the task. While location and feature-based attention are widely studied, object-based
    has been the focus of behavioral research only for the past 25 years [[81](#bib.bib81)].
    Studies have revealed that attention can be directed to one or two spatially overlapping
    objects. O’Craven et al. [[83](#bib.bib83)] showed observers a spatial overlap
    between house and face. At any time, the test subjects should look at the house
    or face. The authors observed brain activity in selective cortical regions of
    faces and selective at home that depended on which of the two stimuli was attended
    to. They found that the magnitude of the motion-triggered signal in the MT area
    also depended on whether the assisted object was moving or not, suggesting that
    all assisted objects’ features were selected.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，经典文献中的其他非严格空间的注意选择领域 - 基于特征和基于对象的注意成为研究的焦点。基于特征的注意是指根据特定特征维度中表达的值选择刺激（例如，颜色维度中的黄色和移动维度中的左侧）。Saenz等人使用功能磁共振成像（fMRI）对人体进行了实验，并观察到神经反应的幅度取决于刺激特征与选择与任务相关的空间和非空间感觉信息的视觉刺激特征的结合。虽然位置和基于特征的注意已广泛研究，但基于对象的注意仅在过去25年的行为研究中成为关注的焦点。研究发现，注意力可以指向一个或两个空间重叠的对象。O'Craven等人展示了观察者之间房屋和面孔之间的空间重叠。任何时候，测试对象都应该看着房子或面孔。作者在脸部的选择性皮质区域以及依赖于被注意到的两个刺激之一的房屋的选择性查看家中观察到了大脑活动。他们发现MT区运动触发信号的幅度也取决于辅助对象是否移动，这表明选择了所有辅助对象的特征。
- en: In our framework, location-based attention is an subsystem focused on stimulus
    localization existindo um peso atencional para cada estímulo do focus target,
    ou seja $a_{t,j}^{k}\in\mathbb{R}$. The feature-based attention is a subsystem
    focused on the target features, ou seja the focus output $a_{t,j}^{k}\in\mathbb{R}^{F_{\tau
    k}}$. In object-based attention an subsystem capable of focusing on the focus
    target’s semantic elements. For example, the focus target $\tau_{t}^{k}$ at time
    t maybe represent the set of $n_{\tau^{k}}$ pixels $px$, and attention subsystem
    select only the object. The focus output set $A=\left\{x\in\mathbb{Z}:0\leq x\leq
    1\right\}$, $a_{t}^{k}\in\mathbb{R}^{n_{\tau k}}$, and each position of $a_{t,j}^{k}$
    is 1 only if the pixels are inside object.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的框架中，基于位置的注意力是一个子系统，专注于刺激定位，对每个焦点目标的刺激存在一个注意力权重，即 $a_{t,j}^{k}\in\mathbb{R}$。基于特征的注意力是一个子系统，专注于目标特征，即焦点输出
    $a_{t,j}^{k}\in\mathbb{R}^{F_{\tau k}}$。在基于对象的注意力中，一个子系统能够专注于焦点目标的语义元素。例如，时间 t
    的焦点目标 $\tau_{t}^{k}$ 可能代表 $n_{\tau^{k}}$ 个像素 $px$ 的集合，注意力子系统仅选择对象。焦点输出集 $A=\left\{x\in\mathbb{Z}:0\leq
    x\leq 1\right\}$，$a_{t}^{k}\in\mathbb{R}^{n_{\tau k}}$，并且 $a_{t,j}^{k}$ 的每个位置仅在像素在对象内部时为
    1。
- en: 'In this context, most models are data-driven and are exclusively location-based,
    as shown in Figure [3](#S4.F3 "Figure 3 ‣ IV A Taxonomy for Attention Models ‣
    Neural Attention Models in Deep Learning: Survey and Taxonomy") in f6\. Its main
    targets are the hidden states/embedding vectors, feature maps, external memory
    cells, and raw data from the neural network input stimuli. When the mechanisms
    are focused on hidden states or embedding vectors, the attentional weights assist
    in the construction of dynamic context vectors minimizing information bottleneck
    problems in multimodal approaches [[41](#bib.bib41)] [[67](#bib.bib67)], encoder-decoder
    structures [[34](#bib.bib34)], and embedding representation problems [[26](#bib.bib26)].
    Similarly, when applied to external memory cells, the mechanisms were able to
    different memory locations to build a dynamic vector of summarizing past experiences
    from the task’s current context. When location-based mechanisms are applied on
    feature-maps, they adjust the output of feature extractors to make target regions
    stand out in the presence of disturbing backgrounds seeking to imitate mechanisms
    in some regions of the human visual cortex. However, few location-based mechanisms
    focus directly on input stimuli from the neural network. Although there is a wide
    range of research in psychology and neuroscience focusing on spatial aspects of
    human vision, only RAM [[38](#bib.bib38)] (Section [V-C](#S5.SS3 "V-C Recurrent
    Attention Model (RAM): A visual attention system for image classification ‣ V
    Neural Attention Models ‣ Neural Attention Models in Deep Learning: Survey and
    Taxonomy")), DRAW [[40](#bib.bib40)] (Section [V-F](#S5.SS6 "V-F Deep Recurrent
    Attentive Writer (DRAW) ‣ V Neural Attention Models ‣ Neural Attention Models
    in Deep Learning: Survey and Taxonomy")), Spatial Transformer [[25](#bib.bib25)]
    and similar approaches explore location-based attention models on raw input image
    data. Spatial Transformer presents a particularly interesting approach, based
    on feature maps, the localization network determines the transformation parameters,
    which act as a context for the attentional system to select a local grid on the
    feature maps and apply the learned transformation minimizing the deformations
    of the focus region for the convolutional network in a classification task.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个背景下，大多数模型是数据驱动的，并且完全基于位置，如图[3](#S4.F3 "Figure 3 ‣ IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")所示。它们的主要目标是神经网络输入刺激的隐藏状态/嵌入向量、特征图、外部记忆单元和原始数据。当机制专注于隐藏状态或嵌入向量时，注意权重有助于构建动态上下文向量，减少多模态方法[[41](#bib.bib41)]
    [[67](#bib.bib67)]、编码器-解码器结构[[34](#bib.bib34)]和嵌入表示问题[[26](#bib.bib26)]中的信息瓶颈问题。同样，当应用于外部记忆单元时，这些机制能够不同的记忆位置来构建一个总结任务当前上下文的动态向量。当位置基础机制应用于特征图时，它们调整特征提取器的输出，以使目标区域在存在干扰背景时突出，试图模拟人类视觉皮层某些区域的机制。然而，很少有位置基础机制直接关注神经网络的输入刺激。尽管在心理学和神经科学领域有广泛的研究关注人类视觉的空间方面，但只有RAM
    [[38](#bib.bib38)]（第[V-C](#S5.SS3 "V-C Recurrent Attention Model (RAM): A visual
    attention system for image classification ‣ V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy")节）、DRAW [[40](#bib.bib40)]（第[V-F](#S5.SS6
    "V-F Deep Recurrent Attentive Writer (DRAW) ‣ V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy")节）、空间变换器[[25](#bib.bib25)]及类似方法探讨了对原始输入图像数据的基于位置的注意力模型。空间变换器提出了一种特别有趣的方法，基于特征图，定位网络确定变换参数，这些参数作为上下文使注意系统选择特征图上的局部网格，并应用学习到的变换，从而最小化焦点区域的变形，适用于卷积网络中的分类任务。'
- en: 'There are few purely feature-based, object-based, or hybrid approaches. Purely
    feature-based mechanisms are more common on feature maps in convolutional neural
    networks [[29](#bib.bib29)] [[30](#bib.bib30)] and in graph neural networks [[20](#bib.bib20)]
    [[84](#bib.bib84)] seeking to adjust the properties of features from global information
    or the vicinity of the target stimuli, in an attempt to highlight those that are
    most relevant to the target task. There are still few hybrid approaches with location
    and feature-based mechanisms in two stages of processing: 1) In the first stage,
    the features are weighted on the target, building useful context vectors for the
    second stage; 2) In the second stage, location-based mechanisms use the iteration
    over the features to guide the attentional focus on the location of the stimuli.
    There are also very few object-based approaches in the field. To the best of our
    knowledge, only Tang et al. [[35](#bib.bib35)] proposed an object-centered visual
    attention approach to generative models inspired by Shifter Circuit Model [[85](#bib.bib85)].
    It is a biologically plausible visual attention model to form invariant representations
    of scale and position of objects in the world. By controlling neurons driven by
    memory, it controls synaptic forces that guide the flow of spatially organized
    information in the primary visual cortex (V1) to upper cortical regions, allowing
    assisted objects to be represented in an invariant way in position and scale.
    Similarly, the model uses an attentional system with a retina-like representation
    to search for faces in a scene guided by signals present in associative memory.
    A series of canonical transformations learned during training help guide the focus
    on the image, selecting the patch of the face of interest. In a second stage,
    auxiliary mechanisms propagate the resulting signals to an associative memory
    represented by a Gaussian Deep Belief Networks (DBN) [[86](#bib.bib86)].'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎没有完全基于特征、基于对象或混合方法的研究。完全基于特征的机制在卷积神经网络的特征图[[29](#bib.bib29)] [[30](#bib.bib30)]和图神经网络[[20](#bib.bib20)]
    [[84](#bib.bib84)]中更为常见，这些机制试图通过全局信息或目标刺激的附近调整特征的属性，以突出那些与目标任务最相关的特征。目前仍然很少有混合方法在处理的两个阶段中结合位置和特征机制：1）在第一阶段，特征在目标上加权，为第二阶段构建有用的上下文向量；2）在第二阶段，基于位置的机制使用对特征的迭代来引导注意力集中在刺激的位置。在该领域中，也很少有基于对象的方法。据我们所知，只有
    Tang 等人[[35](#bib.bib35)] 提出了一个以对象为中心的视觉注意力方法，该方法受 Shifter Circuit Model [[85](#bib.bib85)]
    启发。这是一种生物学上合理的视觉注意力模型，用于形成对象在世界上的尺度和位置的不变表示。通过控制由记忆驱动的神经元，它控制突触力量，从而引导空间组织信息在初级视觉皮层（V1）向上层皮层区域的流动，使辅助对象在位置和尺度上以不变的方式进行表示。类似地，该模型使用具有视网膜样表示的注意力系统来搜索场景中的面孔，并由联想记忆中存在的信号引导。训练过程中学到的一系列标准变换帮助引导图像上的注意力，选择感兴趣的面孔区域。在第二阶段，辅助机制将结果信号传播到由高斯深度信念网络（DBN）[[86](#bib.bib86)]
    表示的联想记忆中。
- en: IV-C2 Task-oriented versus Time-oriented Models
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 任务导向与时间导向模型
- en: 'When the target is a program, the attentional mechanism intuitively seeks to
    answer the following questions: Among all these networks, which should be chosen
    to perform an answer/task? How much computing time should be spent for each neural
    structure ?. The task-oriented selection subsystem chooses one (or more) programs
    to be executed next. We can consider the target set $T$ to be the set of possible
    $N$ programs and attention to select the most appropriate for the task in time
    $t$. Time-oriented selection attention chooses how much computing time to allocate
    to each program given a computational time budget. For example, the framework
    contains several neural networks $\tau_{t}=\{\tau_{t}^{1},\tau_{t}^{2},\tau_{t}^{3}\}$to
    be executed. The attention subsystem must decide how much computation to spend,
    from the budget $B$, on each neural network $\tau_{t}^{k}$. The focus output is
    $a_{t}=\{a_{t}^{1},a_{t}^{2},a_{t}^{3}\}$, $a_{t}^{1}+a_{t}^{2}+a_{t}^{3}=1$,
    and the amount of computation for each program can be calculated as $a_{t}^{k}B$.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是一个程序时，注意力机制直观地寻求回答以下问题：在所有这些网络中，应该选择哪个来执行答案/任务？每个神经结构应花费多少计算时间？任务导向的选择子系统选择一个（或多个）程序进行下一步执行。我们可以认为目标集
    $T$ 是可能的 $N$ 个程序的集合，注意力选择最适合任务的程序，在时间 $t$ 内。时间导向选择注意力决定在给定计算时间预算的情况下，为每个程序分配多少计算时间。例如，框架包含几个神经网络
    $\tau_{t}=\{\tau_{t}^{1},\tau_{t}^{2},\tau_{t}^{3}\}$ 需要执行。注意力子系统必须决定从预算 $B$ 中花费多少计算量在每个神经网络
    $\tau_{t}^{k}$ 上。焦点输出为 $a_{t}=\{a_{t}^{1},a_{t}^{2},a_{t}^{3}\}$，$a_{t}^{1}+a_{t}^{2}+a_{t}^{3}=1$，每个程序的计算量可以计算为
    $a_{t}^{k}B$。
- en: 'Few architectures in the area target a neural network (Figure [3](#S4.F3 "Figure
    3 ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy") in f9 and f10). Adaptive Computation Time (ACT) [[51](#bib.bib51)]
    chooses how many auxiliary computing substeps will be performed by a recurring
    structure in a $t$time frame. The structure decides how a computing budget will
    be allocated, controlling when the recurring structure should stop and generate
    the final output $y_{t}$based on the auxiliary outputs. The Attentional Correlation
    Filter Network [[61](#bib.bib61)] directs the attentional focus on a different
    set of feature extractors based on previous deep regression network validation
    scores, determining which set of extractors each time step $t$ must be activated
    to receive the input image stream. The previous validation scores work as context
    information giving feedback to the attentional system about the performance of
    the general system in the task, from that feedback attention can regulate the
    focus points for the next iteration. Modality Shifting Attention [[73](#bib.bib73)]
    has a task-oriented mechanism responsible for shifting attention between the neural
    network that captures visual sensory stimuli and the neural network that captures
    linguistic stimuli. The system is guided by a context represented by the question
    about a sequence of images and captions for question-answering tasks. In STRAW
    [[48](#bib.bib48)] a task-oriented attentional mechanism controls the activation
    of read, write and the time-advance structure over the action-plan and the commitment-plan
    controlling when the data-oriented attentional mechanisms must on updating a trained
    agent’s plans via Reinforcement Learning.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在该领域中，少数架构针对神经网络（见图 [3](#S4.F3 "图 3 ‣ IV A 注意力模型的分类 ‣ 深度学习中的神经注意力模型：调查与分类")，位于
    f9 和 f10）。自适应计算时间（ACT）[[51](#bib.bib51)] 选择在 $t$ 时间框架内由递归结构执行多少个辅助计算子步骤。该结构决定计算预算的分配方式，控制递归结构何时停止并基于辅助输出生成最终输出
    $y_{t}$。注意力相关滤波网络 [[61](#bib.bib61)] 根据之前深度回归网络的验证分数，将注意力集中在不同的特征提取器集上，确定每个时间步
    $t$ 必须激活哪个特征提取器集以接收输入图像流。之前的验证分数作为上下文信息反馈给注意力系统，关于任务中一般系统的表现，从这些反馈中，注意力可以调节下一次迭代的关注点。模态转换注意力
    [[73](#bib.bib73)] 具有一个任务导向的机制，负责在捕捉视觉感官刺激的神经网络和捕捉语言刺激的神经网络之间转移注意力。该系统由一个上下文指导，该上下文通过图像序列和问题回答任务的标题表示。在
    STRAW [[48](#bib.bib48)] 中，任务导向的注意力机制控制对行动计划和承诺计划的读、写以及时间推进结构的激活，控制数据导向的注意力机制何时需要通过强化学习来更新训练代理的计划。
- en: IV-D Bottom-Up versus Top-Down Models
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 自下而上 versus 自上而下模型
- en: A big difference between the models is whether they depend on bottom-up influences,
    top-down influences, or a combination of both. Bottom-up or involutory attention
    is determined by the characteristics of the input stimuli (ie, stimulus-driven),
    while top-down cues are determined by cognitive phenomena such as knowledge, expectation,
    reward and current goals (ie, goal-driven). Stimuli that attract attention in
    a bottom-up manner are sufficiently distinct concerning the surrounding characteristics.
    As a result, attention is exogenous, automatic, reflective, and feed-forward.
    A typical example of bottom-up attention is to look at a scene with just one horizontal
    bar between several vertical bars where attention it is immediately directed to
    the horizontal bar. On the other hand, top-down attention to be a voluntary process
    deliberated by the individual, in which a particular location, feature, or object
    is relevant to current behavioral goals. Such a process is guided by elements
    of a high semantic level, such as motivation, expectation, private interests,
    rewards, and social motivations [[1](#bib.bib1)]. In our framework, bottom-up
    attention is guided by the focus target’s discrepancies, whether or not there
    is a contextual entry $c_{t}$. However, the context comes from information from
    the target’s own set of stimuli. If the attention is guided by contextual information
    from other sensory sources, external or previous memories, the attention is top-down.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 模型之间的一个重大区别在于它们是否依赖于自下而上的影响、 自上而下的影响，或两者的结合。自下而上或内在注意是由输入刺激的特征决定的（即，刺激驱动），而自上而下的线索由认知现象决定，如知识、期望、奖励和当前目标（即，目标驱动）。以自下而上的方式吸引注意的刺激在周围特征上具有足够的独特性。因此，注意是外源性的、自动的、反射性的和前馈的。自下而上注意的一个典型例子是看一个场景，其中有一根水平条在几根垂直条之间，注意力会立即被引导到水平条上。另一方面，自上而下的注意是一个由个人自行决定的自愿过程，其中某个特定的位置、特征或对象与当前的行为目标相关。这样的过程受到高语义水平的元素的指导，如动机、期望、个人兴趣、奖励和社会动机
    [[1](#bib.bib1)]。在我们的框架中，自下而上的注意由焦点目标的差异指导，无论是否存在上下文条目 $c_{t}$。然而，上下文来自于目标自身的刺激集的信息。如果注意是由来自其他感觉来源的上下文信息、外部信息或之前的记忆所引导，那么注意就是自上而下的。
- en: Additionally, we consider an additional classification regarding the presence
    of context or the previous inner state of attention. In this sense, the bottom-up
    and top-down mechanisms can still be stateful or stateless. In stateful the attentional
    subsystem considers context and inner information as part of the input set (ie,
    $i_{t-1}$$\neq$$\emptyset$or $c_{t}$$\neq$$\emptyset$), otherwise the subsystem
    implements the stateless selection. The bottom-up stateless mechanisms have no
    previous context or inner states of attention as part of the input, so the attentional
    focus is assigned only through the internal extraction of the target’s discrepancies.
    The bottom-up stateful mechanisms can have contextual and previous inner state-input
    simultaneously, or just one of the options. However, the context extracted externally
    comes from the current target. In the inner state, it represents the previous
    state of attention on the same target. That is, the target does not change over
    time. There are no top-down stateless mechanisms since a condition for the existence
    of top-down influences is the presence of a context external to the current target.
    For the existence of top-down stateful mechanisms, the presence of context as
    input is mandatory, with the presence of the previous inner state being optional.
    Besides, the context refers to previous memories, external memories, or elements
    from other sensory sources other than the target.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还考虑了关于上下文存在与之前内在状态的分类。在这种意义上，自下而上和自上而下机制仍然可以是有状态的或无状态的。在有状态的情况下，注意子系统将上下文和内在信息视为输入集的一部分（即，$i_{t-1}$$\neq$$\emptyset$
    或 $c_{t}$$\neq$$\emptyset$），否则子系统实现无状态选择。自下而上的无状态机制没有以前的上下文或内在注意状态作为输入的一部分，因此注意焦点仅通过目标差异的内部提取来分配。自下而上的有状态机制可以同时具有上下文和之前的内在状态输入，或者仅具有其中之一。然而，从外部提取的上下文来自当前目标。在内在状态中，它代表相同目标上的先前注意状态。也就是说，目标不会随时间改变。没有自上而下的无状态机制，因为存在自上而下影响的条件是存在当前目标之外的上下文。对于自上而下的有状态机制，输入中必须存在上下文，而之前的内在状态是可选的。此外，上下文指的是之前的记忆、外部记忆或来自其他感觉来源的元素，而不是目标。
- en: 'The mechanisms explore top-down and bottom-up influences seeking to answer
    one of two questions: 1 intuitively) Where to look at the target given the alignment
    observed between it and the context? 2) In the absence of context, where should
    I look, given the discrepancies and similarities between the target elements?
    Some models answer the first question by extracting the context directly from
    the target, that is, using bottom-up influences. The encoder stack of Neural Transformer
    [[18](#bib.bib18)] (Section [V-H](#S5.SS8 "V-H Neural Transformer ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")) models
    a bottom-up attentional system completely guided by low-level contexts extracted
    directly from the target of the attentional system. In each encoder, an input
    $I$ is decomposed into a set of queries, keys, and values extracted in parallel.
    In the initial stage of attention, queries act as a low-level context and keys
    as a target, producing attentional masks. In the second stage, these masks and
    the values make up the context vector to guide the attentional focus on the original
    $I$ input that will be input to a new attentional encoder, resulting in a chain
    of bottom-up attentional systems with a low-level context.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制探索了自上而下和自下而上的影响，旨在回答以下两个问题之一：1) 直观地看，在观察到目标与上下文之间的对齐关系的情况下，应将目标置于何处？2) 在没有上下文的情况下，鉴于目标元素之间的差异和相似性，我应该看向何处？一些模型通过直接从目标中提取上下文来回答第一个问题，也就是说，使用自下而上的影响。神经变换器的编码器堆栈[[18](#bib.bib18)]（第[V-H](#S5.SS8
    "V-H 神经变换器 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类"）节）建模了一个完全由直接从注意力系统目标中提取的低级上下文指导的自下而上的注意力系统。在每个编码器中，输入$I$被分解为一组并行提取的查询、键和值。在注意力的初始阶段，查询作为低级上下文，键作为目标，产生注意力掩码。在第二阶段，这些掩码和值组成上下文向量，以指导对原始$I$输入的注意力焦点，该输入将被输入到新的注意力编码器中，从而形成一个具有低级上下文的自下而上的注意力系统链。
- en: 'Typically, attention-based graph neural networks use a combination of bottom-up
    mechanisms to guide attentional focus with and without context. Graph Attention
    Networks [[20](#bib.bib20)] (Section [V-I](#S5.SS9 "V-I Graph Attention Networks
    (GATs) ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")) targets a set of neighboring nodes and from the discrepancies
    between that set defines different attentional weights for each feature. In the
    second stage, this answer is used as a context by another bottom-up and location-based
    subsystem on the set of neighboring input nodes. The final composition between
    the attentional map and all nodes generates an embedding with the neighborhood’s
    representativeness for a specific node. Similarly, Heterogeneous Graph Attention
    Network [[84](#bib.bib84)] uses two bottom-up attention mechanisms at the node
    level and bottom-up mechanisms at the semantic level to capture various types
    of semantic information between heterogeneous graphs.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，基于注意力的图神经网络使用自下而上的机制组合来指导有上下文和没有上下文的注意力焦点。图注意力网络[[20](#bib.bib20)]（第[V-I](#S5.SS9
    "V-I 图注意力网络（GATs） ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类"）节）针对一组邻近节点，并根据该组之间的差异为每个特征定义不同的注意力权重。在第二阶段，这个答案作为上下文被另一个自下而上和基于位置的子系统使用，作用于邻近输入节点集。注意力图与所有节点之间的最终组合生成一个具有特定节点邻域代表性的嵌入。同样，异构图注意力网络[[84](#bib.bib84)]在节点级别使用两种自下而上的注意力机制，在语义级别使用自下而上的机制，以捕捉异构图之间的各种语义信息。
- en: In convolutional networks, the bottom-up mechanisms are present when the target
    is a set of feature maps. Usually, only inter-channel or intra-channel discrepancies
    guide attention by promoting boost features or recalibrating channels. The attentional
    mechanism of Squeeze-and-Excitation Networks [[29](#bib.bib29)], a pioneering
    approach in the area, receives as a focus target a set of feature maps, generates
    channel-wise statistics using global average pooling. Based on these statistics,
    it captures channel-wise dependencies capable of learning non-linear iterations
    between channels. Finally, re-scale each input channel in a feature-based approach.
    Double Attention Networks [[30](#bib.bib30)], in a similar but location-based
    approach, uses as feature context maps from the same convolutional layer (i.e.,
    bottom-up) or different layers (i.e., top-down) as a context for defining relations
    between the elements, whose main objective is based on the extracted relations
    to ponder the regions of the feature maps of a convolutional layer. Finally, the
    SNAIL [[28](#bib.bib28)] via bottom-up captures temporal relationships between
    feature maps in a meta-learner approach using self-attention mechanisms similar
    to Transformer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积网络中，当目标是一组特征图时，底向上的机制存在。通常，只有通道间或通道内的差异通过提升特征或重新校准通道来引导注意力。Squeeze-and-Excitation
    Networks [[29](#bib.bib29)] 的注意力机制，作为该领域的开创性方法，以一组特征图作为关注目标，使用全局平均池化生成通道级统计数据。基于这些统计数据，它捕捉通道级依赖关系，能够学习通道之间的非线性迭代。最后，以基于特征的方法重新缩放每个输入通道。Double
    Attention Networks [[30](#bib.bib30)] 采用类似但基于位置的方法，使用来自同一卷积层（即底向上）或不同层（即顶向下）的特征上下文图来定义元素之间的关系，其主要目标是基于提取的关系来调整卷积层特征图的区域。最后，SNAIL
    [[28](#bib.bib28)] 通过底向上的方法，利用类似 Transformer 的自注意力机制在元学习者方法中捕捉特征图之间的时间关系。
- en: 'However, most area approaches are top-down stateful, as shown in Figure [3](#S4.F3
    "Figure 3 ‣ IV A Taxonomy for Attention Models ‣ Neural Attention Models in Deep
    Learning: Survey and Taxonomy"). The first mechanism was proposed for RNNSearch
    [[34](#bib.bib34)] in mid-2014, in which the context received as input is directly
    responsible for the dynamic change of the context vector $c_{t}$received by the
    decoder at each time step $t$. Similarly, Xu et al. [[41](#bib.bib41)] used information
    from the decoder’s previous hidden state to guide a location-based mechanism through
    regions of the input image in a multimodal question-answering approach. Networks
    with internal and external memory are usually top-down structures. End-to-End
    Memory Networks [[19](#bib.bib19)] uses an external query for a question as a
    context to search and focus on memory cells more related to the question’s content.
    Neural Turing Machine [[37](#bib.bib37)] uses as context parameters issued by
    the network controller that define content-based and location-based addressing
    in external memory. Sparse Attentive Backtracking [[59](#bib.bib59)][[60](#bib.bib60)]
    applies an attentional sparse memory recovery method to build the next hidden
    state of an RNN, using the provisional hidden state $\widehat{h}^{(t)}$ and target
    all previously processed memories. The mechanism acts as a method capable of blaming
    or giving credit to previous memories, similarly to what human beings do, without
    the need to repeat all events from the present until the credited event, managing
    to capture long-distance dependencies between states efficiently. Zhang et al.
    [[54](#bib.bib54)] used top-down attention to propose Excitation Backprop - a
    new backpropagation scheme based on biological evidence from Winner-Take-All (WTA)
    competition [[87](#bib.bib87)] among visual filters and the Selective Tuning Model
    [[88](#bib.bib88)] selectively adjust a visual processing system through a top-down
    hierarchy of winner-take-all processes embedded in the visual processing pyramid.
    Similarly, Excitation Backprop uses a probabilistic WTA on CNNs to promote excitatory
    or inhibitory connections between neighboring neurons from top-down influences
    from previously visited neurons.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，大多数区域方法是自上而下的有状态的，如图 [3](#S4.F3 "Figure 3 ‣ IV A Taxonomy for Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") 所示。第一个机制是在2014年中期为RNNSearch
    [[34](#bib.bib34)] 提出的，其中接收的上下文作为输入直接负责解码器在每个时间步 $t$ 接收的上下文向量 $c_{t}$ 的动态变化。同样，Xu
    等人 [[41](#bib.bib41)] 使用解码器的先前隐藏状态中的信息，通过输入图像的区域引导基于位置的机制，以进行多模态问答。具有内部和外部记忆的网络通常是自上而下的结构。端到端记忆网络
    [[19](#bib.bib19)] 使用作为上下文的问题外部查询来搜索并关注与问题内容更相关的记忆单元。神经图灵机 [[37](#bib.bib37)]
    使用由网络控制器发出的参数作为上下文，这些参数定义了外部记忆中的基于内容和位置的寻址。稀疏注意回溯 [[59](#bib.bib59)][[60](#bib.bib60)]
    应用了一种注意力稀疏记忆恢复方法来构建RNN的下一个隐藏状态，使用临时隐藏状态 $\widehat{h}^{(t)}$ 和目标所有先前处理过的记忆。该机制作为一种能够归咎于或给予先前记忆赞扬的方法，类似于人类的做法，而无需重复从当前事件到已被认可事件的所有事件，有效地捕捉状态之间的长距离依赖。Zhang
    等人 [[54](#bib.bib54)] 使用自上而下的注意力提出了Excitation Backprop - 一种基于Winner-Take-All (WTA)
    竞争 [[87](#bib.bib87)] 和选择性调节模型 [[88](#bib.bib88)] 的生物学证据的新反向传播方案，该方案通过自上而下的WTA过程层次结构选择性地调整视觉处理系统。类似地，Excitation
    Backprop 在CNNs上使用概率WTA，以促进来自先前访问的神经元的自上而下影响之间的兴奋性或抑制性连接。'
- en: 'Co-attention structures are also typically top-down. There are attentional
    mechanisms propagating attention in this structure in two ways: from the query
    to the context and from the context to the query. Dynamic Coattention Networks
    [[55](#bib.bib55)] computes attentional scores in this way. The mechanisms produce
    weights for each word in the question based on document words as context, just
    as they produce weights for each word in the document based on question words
    as context. Similarly, Hypergraph Attention Networks [[67](#bib.bib67)] computes
    attention between two hypergraphs comparing the semantics between two symbolic
    representations of different sensory sources.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 共同注意结构通常也是自上而下的。在这种结构中，有两种方式传播注意力机制：从查询到上下文，以及从上下文到查询。动态共同注意网络 [[55](#bib.bib55)]
    就是通过这种方式计算注意力分数。机制为问题中的每个词基于文档词作为上下文生成权重，就如同它们为文档中的每个词基于问题词作为上下文生成权重一样。类似地，超图注意网络
    [[67](#bib.bib67)] 计算两个超图之间的注意力，比较不同感官来源的两个符号表示之间的语义。
- en: 'Some structures are hybrid and use both levels of influence to guide attention.
    Neural Transformer [[18](#bib.bib18)] (Section [V-H](#S5.SS8 "V-H Neural Transformer
    ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning: Survey
    and Taxonomy")) stands out as the main hybrid structure in the area. It has a
    completely bottom-up encoder and a hybrid decoder so that the first attentional
    system is bottom-up and processes the translated words. In contrast, the second
    is top-down for using the last encoder’s attentional information as a context
    to guide attention on the target, represented by the words previously translated.
    Following the structure of Transformer, recently BRIMs [[72](#bib.bib72)] presented
    hybrid mechanisms to carry out communication between RIMs modules. Bottom-up attentional
    subsystems communicate between modules of the same layer, as well as the composition
    of hidden states in initial layers using the entry $x_{t}$ as the target, and
    via top-down attention modules in different layers communicate with each other
    requesting information about hidden states of previous and posterior layers to
    compose the current hidden state.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '一些结构是混合型的，使用两种影响级别来引导注意力。神经转换器 [[18](#bib.bib18)]（第 [V-H](#S5.SS8 "V-H Neural
    Transformer ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")节）在这一领域中作为主要的混合结构脱颖而出。它具有完全的自下而上的编码器和混合解码器，使得第一个注意力系统是自下而上的，处理翻译后的单词。相比之下，第二个注意力系统是自上而下的，利用最后一个编码器的注意力信息作为上下文来引导对目标的注意力，目标由之前翻译的单词表示。沿用转换器的结构，最近
    BRIMs [[72](#bib.bib72)] 提出了混合机制以实现 RIMs 模块之间的通信。自下而上的注意力子系统在同一层的模块之间进行通信，以及在初始层中使用输入
    $x_{t}$ 作为目标的隐藏状态的组合，而通过不同层的自上而下注意力模块之间相互通信，请求关于前后层隐藏状态的信息，以组成当前隐藏状态。'
- en: 'IV-E Continuity: Soft versus Hard'
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 连续性：软选择与硬选择
- en: The selection may occur either by choosing a discrete subset of the possible
    choices or by performing a soft (or continuous) giving real-valued scores to the
    possible choices. The different types of selection can be implemented with modules
    by appropriately choosing the focus output set$A$. If $A=\left\{x\in\mathbb{R}:0<x<1\right\}$
    the selection is soft, and$A=\left\{x\in\mathbb{Z}:0\leq x\leq 1\right\}$ the
    selection is hard.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 选择可能通过选择可能选择的离散子集来进行，也可能通过对可能选择给予真实值分数的软（或连续）方式来进行。如果$A=\left\{x\in\mathbb{R}:0<x<1\right\}$，则选择是软选择，而$A=\left\{x\in\mathbb{Z}:0\leq
    x\leq 1\right\}$，则选择是硬选择。
- en: In Deep Learning, the mechanisms are mainly divided into the following categories:1)
    hard attention determines whether a part of the mechanism’s input should be considered
    or not, reflecting the interdependence between the input of the mechanism and
    the target of the deep neural network. The weight assigned to an input port is
    either 0 or 1; 2) soft attention divides attentional weights between 0 and 1 for
    each input element so that the sum of all weights is equal to 1\. It decides how
    much attention should be focused on each element, considering the interdependence
    between the input of the deep neural network’s mechanism and target; 3) self-attention
    quantifies the interdependence between the input elements of the mechanism. This
    mechanism allows the inputs to interact with each other ”self” and determine what
    they should pay more attention to. There are also some secondary categories:1)
    global attention is a simplification of the classic soft attention proposed for
    encoder-decoder frameworks; 2) local attention is a tradeoff between hard and
    soft attention; 3) co-attention assigns attention from both the context to the
    target and from the target to the context, and 4) hierarchical attention presents
    mechanisms adapted to deal with hierarchical structures at different levels of
    granularity. Concerning continuity, the mechanisms classified as hard attention
    have hard continuity according to our taxonomy, and all other mechanisms in the
    area have soft continuity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，这些机制主要分为以下几类：1) 硬注意力决定机制的某一部分输入是否应被考虑，反映了机制输入和深度神经网络目标之间的相互依赖关系。分配给输入端口的权重要么是0，要么是1；2)
    软注意力为每个输入元素分配介于0和1之间的注意力权重，使所有权重的总和等于1。它决定了应将多少注意力集中在每个元素上，考虑了深度神经网络机制和目标之间的相互依赖关系；3)
    自注意力量化机制输入元素之间的相互依赖关系。该机制允许输入相互“自”交互并确定它们应更关注什么。还有一些次要类别：1) 全局注意力是为编码器-解码器框架提出的经典软注意力的简化版；2)
    局部注意力是在硬注意力和软注意力之间的折衷；3) 共同注意力从上下文到目标以及从目标到上下文分配注意力；4) 分层注意力展示了适用于处理不同层次粒度的层次结构的机制。关于连续性，根据我们的分类，硬注意力机制具有硬连续性，而该领域的所有其他机制具有软连续性。
- en: 'There is still no systematic study to determine the advantages and disadvantages
    of hard and soft continuity mechanisms. However, there is a wide range of soft
    continuity mechanisms, as shown in the Figure [V](#S5 "V Neural Attention Models
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") in f13 and f14\.
    One justification is that the hard mechanisms, in most cases, make the architecture
    non-differentiable, requiring more elaborate training strategies, such as Reinforcement
    Learning (RL) or even hybrid supervised and RL [[38](#bib.bib38)] approaches.
    Usually, these strategies are still little explored in computer vision and natural
    language processing - where the main neural attention models are - because they
    require the well-designed design of the reward functions, which is not always
    intuitive or necessary when there is ground truth, available for training via
    supervised learning. Currently, few architectures in the area use both mechanisms
    simultaneously. Pointer Networks [[42](#bib.bib42)] features a soft mechanism
    for distributing attention over all input elements, followed by a hard mechanism
    for choosing one as an output at each stage of the decoder. Modality Shifting
    Attention [[73](#bib.bib73)] uses a hard mechanism to switch between different
    sensory modalities and soft continuity mechanisms to reason about the final prediction
    of the network.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '目前尚无系统研究来确定硬连续性和软连续性机制的优缺点。然而，如图 [V](#S5 "V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy") 的 f13 和 f14 所示，软连续性机制的范围很广。一个解释是，硬机制在大多数情况下使架构不可微分，要求更复杂的训练策略，如强化学习（RL）或甚至混合监督和
    RL [[38](#bib.bib38)] 方法。通常，这些策略在计算机视觉和自然语言处理领域 - 主要的神经注意力模型所在的领域 - 仍然很少被探索，因为它们需要精心设计的奖励函数，当有地面真值可用于监督学习训练时，这往往并不直观或必要。目前，该领域很少有架构同时使用这两种机制。指针网络
    [[42](#bib.bib42)] 特点是一个软机制用于在所有输入元素上分配注意力，接着在解码器的每个阶段使用硬机制选择一个作为输出。模态转移注意力 [[73](#bib.bib73)]
    使用硬机制在不同感官模态之间切换，并使用软连续性机制对网络的最终预测进行推理。'
- en: V Neural Attention Models
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 神经注意力模型
- en: 'In this section, we discuss some of the main neural attention models from the
    theoretical perspective of attention. A timeline summarizing the main developments
    and their main contributions is shown in Figure [4](#S5.F4 "Figure 4 ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy").
    In the [V-A](#S5.SS1 "V-A RNN Search: the beginning ‣ V Neural Attention Models
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") section we discuss
    RNNSearch [[34](#bib.bib34)], in the [V-B](#S5.SS2 "V-B Neural Turing Machine:
    An attention-augmented memory approach ‣ V Neural Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy") section we discuss Neural Turing
    Machine [[37](#bib.bib37)], in the [V-C](#S5.SS3 "V-C Recurrent Attention Model
    (RAM): A visual attention system for image classification ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") we discussed
    RAM [[38](#bib.bib38)], in the [V-D](#S5.SS4 "V-D End-To-End Memory Networks (EMNet):
    A memory-based end-to-end attention system ‣ V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy") section we discussed
    the End-to-End Memory Networks [[19](#bib.bib19)], in the [V-E](#S5.SS5 "V-E Show,
    Attend and Tell: A multimodal approach ‣ V Neural Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy") we discussed the Show, Attend and
    Tell [[41](#bib.bib41)], in the [V-F](#S5.SS6 "V-F Deep Recurrent Attentive Writer
    (DRAW) ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy") section we discussed the DRAW [[40](#bib.bib40)], in the
    [V-G](#S5.SS7 "V-G Bi-Directional Attention Flow (BiDAF) ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") section
    we discussed the BiDAF [[47](#bib.bib47)], in the [V-H](#S5.SS8 "V-H Neural Transformer
    ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning: Survey
    and Taxonomy") section a Neural Transformer [[18](#bib.bib18)], finally in the
    section [V-I](#S5.SS9 "V-I Graph Attention Networks (GATs) ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") we discussed
    a GATs [[20](#bib.bib20)].'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从注意力的理论角度讨论了一些主要的神经注意力模型。图[4](#S5.F4 "图 4 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")总结了主要的发展历程及其主要贡献。在[V-A](#S5.SS1
    "V-A RNN 搜索：起点 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们讨论了RNNSearch [[34](#bib.bib34)]，在[V-B](#S5.SS2
    "V-B 神经图灵机：一种增强注意力的记忆方法 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中，我们讨论了神经图灵机 [[37](#bib.bib37)]，在[V-C](#S5.SS3
    "V-C 循环注意力模型 (RAM)：一种用于图像分类的视觉注意力系统 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了RAM
    [[38](#bib.bib38)]，在[V-D](#S5.SS4 "V-D 端到端记忆网络 (EMNet)：一种基于记忆的端到端注意力系统 ‣ V 神经注意力模型
    ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了端到端记忆网络 [[19](#bib.bib19)]，在[V-E](#S5.SS5 "V-E 显示、关注和讲述：一种多模态方法
    ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了显示、关注和讲述 [[41](#bib.bib41)]，在[V-F](#S5.SS6
    "V-F 深度递归注意力写作器 (DRAW) ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了DRAW [[40](#bib.bib40)]，在[V-G](#S5.SS7
    "V-G 双向注意力流 (BiDAF) ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了BiDAF [[47](#bib.bib47)]，在[V-H](#S5.SS8
    "V-H 神经变换器 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了神经变换器 [[18](#bib.bib18)]，最后在[V-I](#S5.SS9
    "V-I 图注意力网络 (GATs) ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")节中我们讨论了GATs [[20](#bib.bib20)]。
- en: '![Refer to caption](img/026fb3a1283ec9be8791c6b39bb4e7f3.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/026fb3a1283ec9be8791c6b39bb4e7f3.png)'
- en: 'Figure 4: Timeline illustrating the main key developments from 2014 to the
    present day. RNNSearch [[34](#bib.bib34)] presented the first attention mechanism.
    Neural Turing Machine [[37](#bib.bib37)] and Memory Networks [[89](#bib.bib89)]
    introduced memory and dynamic flow control. RAM [[38](#bib.bib38)] and DRAW [[40](#bib.bib40)]
    learned to combine multi-glimpse, visual attention, and sequential processing.
    Spatial Transformer [[25](#bib.bib25)] introduced a module to increase the robustness
    of CNNs to variations. Show, attend and tell [[41](#bib.bib41)] created attention
    for multimodality. Pointer Networks [[42](#bib.bib42)] presented attention as
    a pointer. BiDAF [[47](#bib.bib47)], HAN [[53](#bib.bib53)], and DCN [[55](#bib.bib55)]
    presented attentional techniques to align data with different hierarchical levels.
    ACT [[51](#bib.bib51)] introduced the computation time topic. Neural Transformer
    [[18](#bib.bib18)] was the first self-attentive neural network with an end-to-end
    attention approach. GATs [[20](#bib.bib20)] introduced attention in GNNs. BERT
    [[90](#bib.bib90)], GPT-2 [[91](#bib.bib91)], GPT-3 [[92](#bib.bib92)], and DALLE
    [[93](#bib.bib93)] are state of the art in language models and text-to-image generation.
    Finally, BRIMs [[72](#bib.bib72)] learned to combine bottom-up and top-down signals.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：时间轴展示了从 2014 年到现在的主要关键发展。RNNSearch [[34](#bib.bib34)] 提出了第一个注意力机制。Neural
    Turing Machine [[37](#bib.bib37)] 和 Memory Networks [[89](#bib.bib89)] 引入了记忆和动态流控制。RAM
    [[38](#bib.bib38)] 和 DRAW [[40](#bib.bib40)] 学会了结合多视角、视觉注意力和顺序处理。Spatial Transformer
    [[25](#bib.bib25)] 引入了一个模块来增强 CNN 对变化的鲁棒性。Show, attend and tell [[41](#bib.bib41)]
    为多模态创建了注意力。Pointer Networks [[42](#bib.bib42)] 将注意力呈现为指针。BiDAF [[47](#bib.bib47)]、HAN
    [[53](#bib.bib53)] 和 DCN [[55](#bib.bib55)] 提出了注意力技术，用于对齐具有不同层次的数据。ACT [[51](#bib.bib51)]
    引入了计算时间主题。Neural Transformer [[18](#bib.bib18)] 是第一个具有端到端注意力方法的自注意神经网络。GATs [[20](#bib.bib20)]
    在 GNN 中引入了注意力。BERT [[90](#bib.bib90)]、GPT-2 [[91](#bib.bib91)]、GPT-3 [[92](#bib.bib92)]
    和 DALLE [[93](#bib.bib93)] 是语言模型和文本到图像生成领域的最新技术。最后，BRIMs [[72](#bib.bib72)] 学会了结合自下而上和自上而下的信号。
- en: 'V-A RNN Search: the beginning'
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A RNN 搜索：开始
- en: 'RNNSearch [[34](#bib.bib34)] uses attention for machine translation. The purpose
    is compute an output sequence y that is translation from input sequence x. The
    architecture consists of an encoder followed by a decoder, as shown in Figure
    [2](#footnote2 "footnote 2 ‣ Figure 5 ‣ V-A RNN Search: the beginning ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy").
    The encoder is a bidirectional RNN (BiRNN) that consist of forward and backward
    RNN’s for compute an annotation term $h_{j}$. The forward RNN $\overrightarrow{f}$
    reads the input sequence in the order of $x_{1}$ to $x_{N}$ and calculates the
    forward hidden state sequence ($\overrightarrow{h_{1}},...,\overrightarrow{h_{N}}$).
    The backward RNN reads the sequence in the reverse order $\overleftarrow{f}$,
    (from $x_{N}$ to $x_{1}$), resulting in the backward hidden states sequence ($\overleftarrow{h_{1}},...,\overleftarrow{h_{N}}$).
    The annotation $h_{j}$, for each word $x_{j}$, is the concatenation of the $\overrightarrow{h_{j}}$
    and $\overleftarrow{h_{j}}$ as follows'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 'RNNSearch [[34](#bib.bib34)] 使用注意力机制进行机器翻译。其目的是计算一个从输入序列 x 翻译过来的输出序列 y。该架构包括一个编码器和一个解码器，如图
    [2](#footnote2 "footnote 2 ‣ Figure 5 ‣ V-A RNN Search: the beginning ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")
    所示。编码器是一个双向 RNN（BiRNN），由前向和后向 RNN 组成，用于计算注释项 $h_{j}$。前向 RNN $\overrightarrow{f}$
    按照 $x_{1}$ 到 $x_{N}$ 的顺序读取输入序列，并计算前向隐状态序列 ($\overrightarrow{h_{1}},...,\overrightarrow{h_{N}}$)。后向
    RNN 以相反的顺序 $\overleftarrow{f}$（从 $x_{N}$ 到 $x_{1}$）读取序列，得到后向隐状态序列 ($\overleftarrow{h_{1}},...,\overleftarrow{h_{N}}$)。每个词
    $x_{j}$ 的注释 $h_{j}$ 是 $\overrightarrow{h_{j}}$ 和 $\overleftarrow{h_{j}}$ 的拼接，如下所示：'
- en: '|  | $(h_{1},...,h_{N})=Encoder(x_{1},...,x_{N})$ |  | (1) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $(h_{1},...,h_{N})=Encoder(x_{1},...,x_{N})$ |  | (1) |'
- en: '|  | $h_{j}=[\overrightarrow{h_{j}};\overleftarrow{h_{j}}]^{T}$ |  | (2) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{j}=[\overrightarrow{h_{j}};\overleftarrow{h_{j}}]^{T}$ |  | (2) |'
- en: 'The decoder consists of classic RNN and attention system. The classic RNN calculates
    from a context vector $c_{t_{decoder}}$ a probability distribution for all possible
    output symbols:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由经典 RNN 和注意力系统组成。经典 RNN 从上下文向量 $c_{t_{decoder}}$ 计算所有可能输出符号的概率分布：
- en: '|  | $p(y_{t}&#124;y_{1},...,y_{t-1},x)=RNN_{decoder}(c_{t_{decoder}})$ |  |
    (3) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y_{t}&#124;y_{1},...,y_{t-1},x)=RNN_{decoder}(c_{t_{decoder}})$ |  |
    (3) |'
- en: 'The attentional system has only one subsystem (Figure [2](#footnote2 "footnote
    2 ‣ Figure 5 ‣ V-A RNN Search: the beginning ‣ V Neural Attention Models ‣ Neural
    Attention Models in Deep Learning: Survey and Taxonomy")) that receives information
    from a single sensory modality (i.e., textual). At each time step t, the subsystem
    takes as input a contextual $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{s_{t-1}\right\}$,
    a focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{h_{1},...,h_{N}\right\}$,
    and produces attention weights $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}=\left\{a_{1},...,a_{N}\right\}$
    as output, where $s_{t-1}\in\mathbb{R}^{1\times d}$ is the decoder’s previous
    hidden state, $h_{j}\in\mathbb{R}^{1\times d}$ is encoder annotation vector, $a_{t}^{1}\in\mathbb{R}^{1\times
    N}$ are the weights of attention over all the encoder’s annotation vectors. The
    focus target is processed by alignment function $e_{t,j}=a(s_{t-1},h_{j})$ to
    obtain a set scores $e_{t,j}$ that reflects the importance of $h_{j}$ with respect
    $s_{t-1}$ in deciding the next state $s_{t}$ and generating $y_{t}$. The alignment
    function $a$ is a feedforward neural network which is jointly trained with the
    framework. The scores are normalized through a softmax function to obtain attention
    weights $a_{j}=\frac{e^{e_{t,j}}}{\sum_{j=1}^{N}e^{e_{t,j}}}$. Finally, a weighted
    sum over enconder’s hidden states generates the dynamic context vector $c_{t_{decoder}}=\sum_{j=1}^{N}a_{j}h_{j}$
    $\in\mathbb{R}^{1\times d}$.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力系统只有一个子系统（图 [2](#footnote2 "footnote 2 ‣ Figure 5 ‣ V-A RNN Search: the
    beginning ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")），接收来自单一感官模态（即文本）的信息。在每个时间步 t，子系统以上下文 $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{s_{t-1}\right\}$、一个焦点目标
    $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{h_{1},...,h_{N}\right\}$
    为输入，并产生注意力权重 $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}=\left\{a_{1},...,a_{N}\right\}$
    作为输出，其中 $s_{t-1}\in\mathbb{R}^{1\times d}$ 是解码器的上一个隐藏状态，$h_{j}\in\mathbb{R}^{1\times
    d}$ 是编码器的注释向量，$a_{t}^{1}\in\mathbb{R}^{1\times N}$ 是对所有编码器注释向量的注意力权重。焦点目标通过对齐函数
    $e_{t,j}=a(s_{t-1},h_{j})$ 处理，以获得一组分数 $e_{t,j}$，这些分数反映了 $h_{j}$ 相对于 $s_{t-1}$
    在决定下一个状态 $s_{t}$ 和生成 $y_{t}$ 时的重要性。对齐函数 $a$ 是一个前馈神经网络，与框架共同训练。这些分数通过 softmax 函数进行归一化，以获得注意力权重
    $a_{j}=\frac{e^{e_{t,j}}}{\sum_{j=1}^{N}e^{e_{t,j}}}$。最后，对编码器的隐藏状态进行加权求和，生成动态上下文向量
    $c_{t_{decoder}}=\sum_{j=1}^{N}a_{j}h_{j}$ $\in\mathbb{R}^{1\times d}$。'
- en: Intuitively, attention decides which parts of the source sentence pay attention.
    Allowing the decoder to have this mechanism is unnecessary to encode a context
    vector of fixed size. Instead, the information is spread throughout the annotation
    sequence and can be selectively retrieved by the decoder as needed. By assigning
    attention simultaneously and continuously to each $h_{j}$, the selection is soft
    and divided. Top-down stateful since a previous decoder’s state represents a context.
    Location-based since the purpose of attention is to weigh the stimuli $h_{j}$
    by assigning the same weight to all features. Finally, the the system is cognitive
    and oriented for differently selecting the same focus target in latent space at
    each time step $t$.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，注意力决定了源句子中哪些部分受到关注。允许解码器拥有这一机制是不必要的，因为它不需要对固定大小的上下文向量进行编码。相反，信息在注释序列中传播，可以根据需要由解码器选择性地检索。通过同时不断地分配注意力给每个
    $h_{j}$，选择是柔性和分散的。自上而下的有状态，因为前一个解码器的状态代表了一个上下文。基于位置的，因为注意力的目的是通过为所有特征分配相同的权重来权衡刺激
    $h_{j}$。最后，系统是认知的，并且针对每个时间步 $t$ 在潜在空间中以不同的方式选择相同的焦点目标。
- en: '![Refer to caption](img/c49ae2f6ee478b572c82aa3c0db98543.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c49ae2f6ee478b572c82aa3c0db98543.png)'
- en: 'Figure 5: RNNSearch [[34](#bib.bib34)] architecture illustration²²2[https://github.com/larocs/attention_dl/blob/master/imgs](https://github.com/larocs/attention_dl/blob/master/imgs).
    The encoder generates a set of hidden states, which are the input for the only
    attention system. The divided attention shares the focus between the different
    stimuli, and in a top-down way, it generates a dynamic context vector for a decoder.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: RNNSearch [[34](#bib.bib34)] 架构示意²²2[https://github.com/larocs/attention_dl/blob/master/imgs](https://github.com/larocs/attention_dl/blob/master/imgs)。编码器生成一组隐藏状态，这些状态作为唯一注意力系统的输入。分配的注意力在不同的刺激之间共享焦点，并以自上而下的方式，为解码器生成一个动态上下文向量。'
- en: 'V-B Neural Turing Machine: An attention-augmented memory approach'
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 神经图灵机：一种增强注意力的记忆方法
- en: 'Neural Turing Machine [[37](#bib.bib37)] uses attention for algorithmic tasks.
    The architecture consists of a controller, heads, external memory, and attention
    system, as shown in Figure [6](#S5.F6 "Figure 6 ‣ V-B Neural Turing Machine: An
    attention-augmented memory approach ‣ V Neural Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy"). The controller is a feedforward
    or recurrent network, which interacts with the outside world and the attentional
    system to manipulate memory through reading and writing heads. The memory $M_{t}=\left\{M_{t,0},...,M_{t,N-1}\right\}$
    $\in$ $\mathbb{R}^{N\times M}$ is an matrix, where $N$ is amount number of memory
    cells, and $M$ is amount number of memory cell’s features. The attentional system
    uses the controller parameters to define each reading/writing operation’s focus
    determining the importance degree at each location. So, a single head can attend
    an individual cell or weakly in several cells.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图灵机 [[37](#bib.bib37)] 使用注意力机制来执行算法任务。其架构包括一个控制器、多个头部、外部内存和注意力系统，如图[6](#S5.F6
    "图 6 ‣ V-B 神经图灵机：一种增强注意力的记忆方法 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：综述与分类")所示。控制器是一个前馈或递归网络，它与外界和注意力系统交互，通过读写头操作内存。内存
    $M_{t}=\left\{M_{t,0},...,M_{t,N-1}\right\}$ $\in$ $\mathbb{R}^{N\times M}$ 是一个矩阵，其中
    $N$ 是内存单元的数量，$M$ 是内存单元特征的数量。注意力系统使用控制器参数来定义每个读写操作的焦点，从而确定每个位置的重要程度。因此，一个单一的头可以关注单个内存单元或在多个单元中进行弱关注。
- en: '![Refer to caption](img/c3e83dc7b7352d2ebc043e282cbe9288.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c3e83dc7b7352d2ebc043e282cbe9288.png)'
- en: 'Figure 6: Neural Turing Machine [[37](#bib.bib37)] architecture illustration.
    The architecture has a controller, an external memory, read/write heads, and an
    attentional system. At each time step $t$, the attentional system, guided by the
    controller’s parameters, defines the memory locations that will be read and written.
    The system is oriented, divided, location-based, top-down stateful, cognitive
    and soft.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：神经图灵机 [[37](#bib.bib37)] 架构示意图。该架构包含一个控制器、一个外部内存、读写头和一个注意力系统。在每个时间步 $t$，注意力系统在控制器参数的指导下，定义将被读取和写入的内存位置。该系统具有方向性、分割性、基于位置、从上到下的有状态、认知和柔性特征。
- en: At each time step $t$, the system receives as input a focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{M_{t,0},...,M_{t,N-1}\right\}$
    with all memory content, and a contextual input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1},c_{t,4}^{1},c_{t,5}^{1}\right\}=\left\{k_{t},\beta_{t},g_{t},s_{t},\gamma_{t}\right\}$
    with controller outputs, where $k_{t}\in\mathbb{R}^{M}$ is the key vector, $\beta_{t}\in\mathbb{R}$
    is the key strength, $g_{t}\in\mathbb{R}$ is the interpolation gate, $s_{t}\in\mathbb{R}^{2k+1}$
    is the shift weight, and $\gamma_{t}\geq 1$ is the sharpening weight. Unlike most
    of the attentional systems, this system receives as input the past inner state
    $i_{t-1}=\left\{a_{t-1}\right\}$, which is the attentional mask in the previous
    time. The attentional system acts as an interface between the controller and read/write
    heads. It has two addressing steps - the first focuses on content, and the second
    is focusing on location. This structure is very similar to VOCUS [[94](#bib.bib94)],
    an classic visual attention model proposed by Fintrop in 2006\. Content addressing
    resembles the bottom-up step, and the subsequent processing is similar VOCUS top-down
    step.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步 $t$，系统接收作为输入的焦点目标 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{M_{t,0},...,M_{t,N-1}\right\}$，其中包含所有内存内容，以及上下文输入
    $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1},c_{t,4}^{1},c_{t,5}^{1}\right\}=\left\{k_{t},\beta_{t},g_{t},s_{t},\gamma_{t}\right\}$，其中包括控制器的输出，其中
    $k_{t}\in\mathbb{R}^{M}$ 是关键向量，$\beta_{t}\in\mathbb{R}$ 是关键强度，$g_{t}\in\mathbb{R}$
    是插值门，$s_{t}\in\mathbb{R}^{2k+1}$ 是位移权重，$\gamma_{t}\geq 1$ 是锐化权重。与大多数注意力系统不同，该系统接收过去的内部状态
    $i_{t-1}=\left\{a_{t-1}\right\}$，即前一个时间的注意力掩码。注意力系统充当控制器和读写头之间的接口。它有两个寻址步骤 - 第一个步骤关注内容，第二个步骤关注位置。这种结构与2006年Fintrop提出的经典视觉注意力模型VOCUS
    [[94](#bib.bib94)] 非常相似。内容寻址类似于自下而上的步骤，随后的处理则类似于VOCUS的自上而下步骤。
- en: Content addressing is inspired by the Hopfield Network, but with a simple retrieval
    mechanism. It is based on similarity between the memories and an approximation
    vector issued by the controller. Specifically, the key vector $k_{t}$ is compared
    to each vector $M_{t,i}$ by a similarity function $K\left(k_{t},M_{t,i}\right)=\frac{k_{t}M_{t,i}}{\left\|k_{t}\right\|\left\|M_{t,i}\right\|}$,
    producing content addressing weights $w_{t}^{c}\in\mathbb{R}^{N}$, composed by
    $w_{t,i}^{c}=\frac{e^{\beta_{t}K\left(k_{t},M_{t,i}\right)}}{\sum_{j=0}^{N-1}e^{\beta_{t}K\left(k_{t},M_{t,j}\right)}}$,
    where $K$ is the cosine similarity.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 内容寻址受到 Hopfield 网络的启发，但具有简单的检索机制。它基于记忆之间的相似性和由控制器发出的近似向量。具体而言，键向量 $k_{t}$ 与每个向量
    $M_{t,i}$ 通过相似性函数 $K\left(k_{t},M_{t,i}\right)=\frac{k_{t}M_{t,i}}{\left\|k_{t}\right\|\left\|M_{t,i}\right\|}$
    进行比较，产生内容寻址权重 $w_{t}^{c}\in\mathbb{R}^{N}$，其组成部分为 $w_{t,i}^{c}=\frac{e^{\beta_{t}K\left(k_{t},M_{t,i}\right)}}{\sum_{j=0}^{N-1}e^{\beta_{t}K\left(k_{t},M_{t,j}\right)}}$，其中
    $K$ 是余弦相似度。
- en: 'Content addressing is very efficient, but in some tasks, it needs a recognizable
    spatial address. Location-based addressing facilitates simple iteration with three
    main steps - interpolation, convolutional shift, and sharpening. Interpolation
    controls the use of the content-based addressing mask. The gate $g_{t}\in\left[0,1\right]$
    combines the past inner state $a_{t-1}$ with the $w_{t}^{c}$. If the gate is zero,
    the content weighting is ignored. If the gate is one, the previous attentional
    mask is ignored, and the system uses only content-based addressing. After interpolation,
    the convolutional shift allows the current focus to change and serve adjacent
    memory locations. This mechanism, is a one-dimensional circular convolution, where
    the shifting weight $s_{t}$ is the kernel to be convolved on the output’s interpolation:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 内容寻址非常高效，但在某些任务中，它需要一个可识别的空间地址。基于位置的寻址通过三个主要步骤 - 插值、卷积位移和锐化 - 使简单的迭代成为可能。插值控制内容基寻址掩膜的使用。门
    $g_{t}\in\left[0,1\right]$ 将过去的内部状态 $a_{t-1}$ 与 $w_{t}^{c}$ 结合。如果门值为零，则忽略内容加权。如果门值为一，则忽略先前的注意力掩膜，系统仅使用内容基寻址。插值后，卷积位移允许当前焦点改变并服务于相邻的记忆位置。这个机制是一维循环卷积，其中位移权重
    $s_{t}$ 是在输出插值上卷积的核：
- en: '|  | $w_{t}^{g}=g_{t}w_{t}^{c}+\left(1-g_{t}\right)a_{t-1}$ |  | (4) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{t}^{g}=g_{t}w_{t}^{c}+\left(1-g_{t}\right)a_{t-1}$ |  | (4) |'
- en: '|  | $\tilde{w_{t,i}}=\sum_{j=0}^{N-1}w_{t}^{g}\left(j\right)s_{t}\left(i-j\right)$
    |  | (5) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{w_{t,i}}=\sum_{j=0}^{N-1}w_{t}^{g}\left(j\right)s_{t}\left(i-j\right)$
    |  | (5) |'
- en: where $g_{t}\in\mathbb{R}$ is the interpolation gate, $a_{t-1}\in\mathbb{R}^{N}$
    are the attention weights of the previous time step, $s_{t}$ is a normalized distribution
    over the allowed integer displacements.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g_{t}\in\mathbb{R}$ 是插值门，$a_{t-1}\in\mathbb{R}^{N}$ 是前一时间步的注意力权重，$s_{t}$
    是允许的整数位移的归一化分布。
- en: Intuitively, $s_{t}$ represents the displacement instructions. If only one offset
    position is allowed (i.e, $k=1$), $s_{t}$ will be a vector consisting of 3 elements,
    which can be interpreted by following instructions $\left\{\right.$ shift 1 forward,
    maintain focus, shift 1 backward $\left.\right\}$. In the general case, $s_{t}$
    will have $2k+1$ elements, where $k$ is the highest absolute displacement value.
    To avoid very high dispersions, the sharpening step takes $\tilde{w_{t}}$ and
    $\gamma_{t}$ to adjust the sharpness of the weights generating the final attention
    mask $a_{t}\in\mathbb{R}^{N}$, composed by weights $a_{t,i}=\frac{\tilde{w_{t,i}}^{\gamma^{t}}}{\sum_{j=0}^{N-1}\tilde{w_{t,j}}^{\gamma^{t}}}$
    for each memory cell $i$.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，$s_{t}$ 表示位移指令。如果只允许一个偏移位置（即 $k=1$），$s_{t}$ 将是一个由 3 个元素组成的向量，可以通过以下指令解释
    $\left\{\right.$ 向前移动 1，保持关注，向后移动 1 $\left.\right\}$。在一般情况下，$s_{t}$ 将有 $2k+1$
    个元素，其中 $k$ 是最大绝对位移值。为了避免非常高的分散，锐化步骤采用 $\tilde{w_{t}}$ 和 $\gamma_{t}$ 来调整生成最终注意力掩膜
    $a_{t}\in\mathbb{R}^{N}$ 的权重的锐度，最终掩膜由每个记忆单元 $i$ 的权重 $a_{t,i}=\frac{\tilde{w_{t,i}}^{\gamma^{t}}}{\sum_{j=0}^{N-1}\tilde{w_{t,j}}^{\gamma^{t}}}$
    组成。
- en: The reading head takes the attentional reading mask and the memory for generate
    as output $r_{t}\leftarrow\sum_{i}^{N}a_{t,i}M_{t,i}\in\mathbb{R}^{M}$, defined
    as a convex combination of memory cells. Similarly, the writing head takes the
    attentional writing mask to erases and adds data in memory. The elements in memory
    cell $i$ are reset to zero if $a_{t,i}$ and the erase element are one; if $a_{t,i}$
    or the erase is zero, the memory will not be changed. When multiple heads are
    present, erasures can be performed in any order, as multiplication is commutative.
    The combined erase and add operations of all writing heads produce the final memory
    contents in time $t$. Specifically,
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读头部将注意力阅读掩码和内存用于生成输出 $r_{t}\leftarrow\sum_{i}^{N}a_{t,i}M_{t,i}\in\mathbb{R}^{M}$，定义为内存单元的凸组合。同样，写入头部使用注意力写入掩码来擦除和添加内存中的数据。如果
    $a_{t,i}$ 和擦除元素为一，则内存单元 $i$ 的元素被重置为零；如果 $a_{t,i}$ 或擦除元素为零，则内存不会改变。当存在多个头部时，擦除操作可以按任何顺序执行，因为乘法是交换的。所有写入头部的组合擦除和添加操作在时间
    $t$ 生成最终的内存内容。具体来说，
- en: '|  | $\tilde{M}_{t,i}\leftarrow M_{t-1,i}\left[1-a_{t,i}e_{t}\right]$ |  |
    (6) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{M}_{t,i}\leftarrow M_{t-1,i}\left[1-a_{t,i}e_{t}\right]$ |  |
    (6) |'
- en: '|  | $M_{t,i}\leftarrow\tilde{M}_{t,i}+a_{t,i}add_{t}$ |  | (7) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{t,i}\leftarrow\tilde{M}_{t,i}+a_{t,i}add_{t}$ |  | (7) |'
- en: where $M_{t,i}\in\mathbb{R}^{M}$ is the memory cell $i$, $\tilde{M}_{t,i}\in\mathbb{R}^{M}$
    is the memory cell $i$ with the content deleted, $a_{t,i}\in\mathbb{R}$ is the
    attentional weight for memory cell $i$, $e_{t}\in\mathbb{R}^{M}$, such that $e_{t}=\left\{x\in\mathbb{R}:0\leq
    x\leq 1\right\}$ is the erasure vector, $add_{t}\in\mathbb{R}^{M}$ is the vector
    with content to be added in memory, 1 is a row-vector of all 1-s, and the multiplication
    against the memory cell is point-wise. Note that both the delete and add vectors
    have independent $M$ components, allowing for refined control over which elements
    in each cell location are modified.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{t,i}\in\mathbb{R}^{M}$ 是内存单元 $i$，$\tilde{M}_{t,i}\in\mathbb{R}^{M}$ 是内容被删除的内存单元
    $i$，$a_{t,i}\in\mathbb{R}$ 是内存单元 $i$ 的注意力权重，$e_{t}\in\mathbb{R}^{M}$，使得 $e_{t}=\left\{x\in\mathbb{R}:0\leq
    x\leq 1\right\}$ 是擦除向量，$add_{t}\in\mathbb{R}^{M}$ 是要添加到内存中的内容向量，1 是全为 1 的行向量，且与内存单元的乘法是逐点的。请注意，删除和添加向量都有独立的
    $M$ 组件，允许对每个单元位置中修改哪些元素进行精细控制。
- en: 'NTM’s attentional system has location, soft, oriented and divided properties,
    since weights attentional mask respect the constraint $\sum_{i=1}^{N-1}a_{t,i}=1$
    e suas intensidades em cada local se alteram no tempo. The system is top-down
    stateful por ser influenciado por parâmetros estimados pelo controlador, and cognitive
    for atuar sobre os heads. Este mecanismo introduziu duas características importantes
    da cognição humana: estruturas de ligação variável e o processamento procedural.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: NTM 的注意力系统具有位置、软性、定向和分隔特性，因为权重注意力掩码尊重约束 $\sum_{i=1}^{N-1}a_{t,i}=1$，且每个位置的强度随时间变化。该系统是自顶向下的状态驱动的，因为它受到控制器估计参数的影响，并且具有认知特性以作用于头部。这个机制引入了人类认知的两个重要特性：可变连接结构和过程处理。
- en: 'V-C Recurrent Attention Model (RAM): A visual attention system for image classification'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 递归注意力模型（RAM）：用于图像分类的视觉注意力系统
- en: 'RAM [[38](#bib.bib38)] uses attention for image classification. The architecture
    consists of the attention system, glimpse sensor, glimpse network, location network,
    core network, and action network (Figure [7](#S5.F7 "Figure 7 ‣ V-C Recurrent
    Attention Model (RAM): A visual attention system for image classification ‣ V
    Neural Attention Models ‣ Neural Attention Models in Deep Learning: Survey and
    Taxonomy")). First, the glimpse sensor extracts a retina-like representation $\rho_{t}$
    around localization $l_{t-1}$. It encodes the region around $l_{t-1}$ in high-resolution,
    and progressively uses a low-resolution representation for the farthest pixels
    of $l_{t-1}$. At each time step $t$, the attention system inside glimpse sensor
    selects $N_{s}$ square patches centered in $l_{t-1}$. The first path being $g_{w}\times
    g_{w}$ pixels in size, and each subsystem produces successive patches having twice
    the width of the previous.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'RAM [[38](#bib.bib38)] 使用注意力机制进行图像分类。该架构包括注意力系统、瞥见传感器、瞥见网络、位置网络、核心网络和动作网络（图
    [7](#S5.F7 "Figure 7 ‣ V-C Recurrent Attention Model (RAM): A visual attention
    system for image classification ‣ V Neural Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy")）。首先，瞥见传感器提取一个类似视网膜的表示 $\rho_{t}$，围绕定位点
    $l_{t-1}$。它对 $l_{t-1}$ 周围区域进行高分辨率编码，并逐渐对 $l_{t-1}$ 最远像素使用低分辨率表示。在每个时间步 $t$，瞥见传感器内的注意力系统选择以
    $l_{t-1}$ 为中心的 $N_{s}$ 个方形补丁。第一个路径为 $g_{w}\times g_{w}$ 像素大小，每个子系统产生的后续补丁宽度是前一个的两倍。'
- en: The attention system is similar classic visual attention approches with programmed
    microsaccades designed in the 1990s [[95](#bib.bib95)]. Itti et al. [[95](#bib.bib95)]
    presented the first practical approache to microsaccades based on a competitive
    structure – Winner-takes-all (WTA) mechanisms jointly inhibition and return strategy
    [[95](#bib.bib95)]. Differently, RAM uses reinforcement learning in a sequential
    structure to determine the best policy for microsaccades, and uses only top-down
    stateful attention to modulate the focus, while the classic systems have even
    explored bottom-up, top-down and hybrid approaches. The RAM is an exception with
    selective perception, hard, selective and oriented attention simultaneously. It
    adopts a location-based system while hybrid approches with feature-based and location-based
    are biologically plausible and widely used in historical models. In most classic
    visual attention systems was common an feature-based approach to modulating low-level
    features (i.e., color, intensity, and orientation), and after merging all stimulus
    an location-based attention finds the regions to be attended.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系统类似于20世纪90年代设计的经典视觉注意力方法，具有编程微跳动 [[95](#bib.bib95)]。Itti 等人 [[95](#bib.bib95)]
    提出了基于竞争结构的微跳动的第一个实际方法——赢家通吃（WTA）机制联合抑制和回归策略 [[95](#bib.bib95)]。不同的是，RAM 使用强化学习的顺序结构来确定微跳动的最佳策略，并且仅使用自上而下的状态注意力来调节焦点，而经典系统甚至探索了自下而上、自上而下和混合的方法。RAM
    是一个例外，具有选择性知觉、硬性、选择性和定向注意力的同时特征。它采用基于位置的系统，而基于特征和基于位置的混合方法在生物学上是可信的，并且在历史模型中被广泛使用。在大多数经典视觉注意力系统中，常见的是基于特征的方法来调节低级特征（即颜色、强度和方向），然后合并所有刺激后，基于位置的注意力会找到需要关注的区域。
- en: With some divergences from the classic visual attention models, RAM presents
    several attentional subsystems in parallel, where each takes the same focus target
    as input $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{x_{t}\right\}$,
    where $x_{t}$ is input image, and a different scale factor in each contextual
    input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1}\right\}=\left\{l_{t-1},s_{i},bd\right\}$,
    where $l_{t-1}\in\mathbb{R}^{2}$, $s_{i}\in\mathbb{R}$ is the scale factor $i$,
    $bd\in\mathbb{R}$ is the sensor bandwidth. And produces as output an attention
    mask $a_{t}$ = $\left\{a_{t}^{1}\right\}$ = $\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}$
    = $\left\{a_{1},...,a_{N}\right\}$, where $a_{i}$ is one if the pixel inside in
    focus patch and zero otherwise. The masks $a_{t}$ select the patches and then
    they are scaled to the sensor bandwidth dimensions and stacked to produce retina-like
    representation $\rho_{t}$. The glimpse network $f_{g}$ combines $\rho_{t}$ and
    $l_{t-1}$ to produce the glimpse feature vector $g_{t}=Rect\left(Linear\left(h_{g}\right),Linear\left(h_{l}\right)\right)$,
    where $h_{g}$, $h_{l}\in\mathbb{R}^{128}$, $g_{t}\in\mathbb{R}^{256}$, $h_{l}=Rect\left(Linear\left(l_{t-1}\right)\right)$,
    and $h_{g}=Rect\left(Linear\left(\rho\left(x_{t},l_{t-1}\right)\right)\right)$.
    Let $Linear\left(x\right)=Wx+b$ for some weight matrix $W$ and bias vector $b$,
    $Rect\left(x\right)=max\left(x,0\right)$ be the rectifier nonlinearity.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典视觉注意力模型有所不同，RAM 在并行处理的多个注意力子系统中展示了几个注意力子系统，其中每个子系统接收相同的焦点目标作为输入 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{x_{t}\right\}$，其中
    $x_{t}$ 是输入图像，每个上下文输入 $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1}\right\}=\left\{l_{t-1},s_{i},bd\right\}$
    具有不同的尺度因子，其中 $l_{t-1}\in\mathbb{R}^{2}$，$s_{i}\in\mathbb{R}$ 是尺度因子 $i$，$bd\in\mathbb{R}$
    是传感器带宽。输出一个注意力掩码 $a_{t}$ = $\left\{a_{t}^{1}\right\}$ = $\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}$
    = $\left\{a_{1},...,a_{N}\right\}$，其中 $a_{i}$ 如果像素在聚焦区域内则为1，否则为0。掩码 $a_{t}$ 选择区域，然后将其缩放到传感器带宽的尺寸并堆叠，以生成类似视网膜的表示
    $\rho_{t}$。瞥见网络 $f_{g}$ 结合 $\rho_{t}$ 和 $l_{t-1}$ 生成瞥见特征向量 $g_{t}=Rect\left(Linear\left(h_{g}\right),Linear\left(h_{l}\right)\right)$，其中
    $h_{g}$，$h_{l}\in\mathbb{R}^{128}$，$g_{t}\in\mathbb{R}^{256}$，$h_{l}=Rect\left(Linear\left(l_{t-1}\right)\right)$，$h_{g}=Rect\left(Linear\left(\rho\left(x_{t},l_{t-1}\right)\right)\right)$。让
    $Linear\left(x\right)=Wx+b$，其中 $W$ 是权重矩阵，$b$ 是偏置向量，$Rect\left(x\right)=max\left(x,0\right)$
    是整流非线性函数。
- en: '![Refer to caption](img/dd713e79887fd485e0c38e9313466a95.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dd713e79887fd485e0c38e9313466a95.png)'
- en: 'Figure 7: RAM [[38](#bib.bib38)] architecture illustration. The architecture
    presents a perceptual selection system within the glimpse sensor to generate a
    retina-like representation. The core network captures this representation and
    synthesizes a historical composition between the current and previous steps. The
    action and location network uses this historical summarization to determine the
    next action and focus via the reinforcement learning paradigm.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：RAM [[38](#bib.bib38)] 架构示意图。该架构展示了在视图传感器内的感知选择系统，以生成类似视网膜的表示。核心网络捕获此表示，并综合当前步骤和先前步骤之间的历史组成。动作和位置网络利用这一历史总结，通过强化学习范式来确定下一个动作和焦点。
- en: The core network $f_{h}$ receives as input the glimpse encoding $g_{t}$, the
    previous internal state $h_{t-1}$, and outputs the current internal state $h_{t}=f_{h}\left(h_{t-1},g_{t},\theta_{h}\right)$.
    The internal state $h_{t}$ summarizes the history of the information seen to decide
    how to act and to deploy the sensor. The location network $f_{l}\left(h_{t},\theta_{l}\right)$
    and action network $f_{a}\left(h_{t},\theta_{a}\right)$ uses $h_{t}$ to generate
    next location $l_{t}$, and the classification $action_{t}$, respectively. For
    simple classification experiments by Graves [[38](#bib.bib38)], $f_{h}$ was a
    network of rectifier units defined as $h_{t}=f_{h}\left(h_{t-1},g_{t}\right)=Rect\left(Linear\left(h_{t-1}\right)+Linear\left(g_{t}\right)\right)$,
    and on a dynamic environment was used LSTM units. The location networks generate
    as output the average’s location policy, given by $f_{l}\left(h_{t},\theta_{l}\right)=Linear(h_{t})$.
    The location $l_{t}$ is chosen stochastically from a distribution parameterized
    by the location network $f_{l}(h_{t},\theta_{l})$, ie, $l_{t}\sim p(\cdot|f_{l}(h_{t},\theta_{l}))$,
    where $p$ is two-component Gaussian with a fixed variance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 核心网络 $f_{h}$ 接收输入为视图编码 $g_{t}$、先前的内部状态 $h_{t-1}$，并输出当前内部状态 $h_{t}=f_{h}\left(h_{t-1},g_{t},\theta_{h}\right)$。内部状态
    $h_{t}$ 总结了所见信息的历史，以决定如何行动和部署传感器。位置网络 $f_{l}\left(h_{t},\theta_{l}\right)$ 和动作网络
    $f_{a}\left(h_{t},\theta_{a}\right)$ 使用 $h_{t}$ 生成下一个位置 $l_{t}$ 和分类 $action_{t}$。在Graves
    [[38](#bib.bib38)] 的简单分类实验中，$f_{h}$ 是一个定义为 $h_{t}=f_{h}\left(h_{t-1},g_{t}\right)=Rect\left(Linear\left(h_{t-1}\right)+Linear\left(g_{t}\right)\right)$
    的整流单元网络，而在动态环境中使用了 LSTM 单元。位置网络生成的输出为平均位置策略，给定 $f_{l}\left(h_{t},\theta_{l}\right)=Linear(h_{t})$。位置
    $l_{t}$ 从由位置网络 $f_{l}(h_{t},\theta_{l})$ 参数化的分布中随机选择，即 $l_{t}\sim p(\cdot|f_{l}(h_{t},\theta_{l}))$，其中
    $p$ 是具有固定方差的双分量高斯分布。
- en: For classification decisions, the action network $f_{a}\left(h_{t},\theta_{a}\right)=\frac{e^{Linear\left(h_{t}\right)}}{Z}$,
    conditions a distribution to generate the output $action_{t}\sim p\left(\cdot|f_{a}\left(h_{t},\theta_{a}\right)\right)$,
    where $p$ is a softmax. After executing an action, the agent receives a new visual
    observation of the environment $x_{t+1}$ and reward signal $r_{t+1}$. The goal
    is to maximize the sum $R=\sum_{t=1}^{T}r_{t}$ of the reward signal. For image
    classification, for $r_{T}=1$ if the object is classified correctly after $T$
    steps and $0$ otherwise. This setup is instance of a Partially Observable Markov
    Decision Process (POMDP). The true state of the environment is unobserved, and
    the model needs to learning a stochastic policy $\pi\left(\left(l_{t},a_{t}\right)|s_{1:t};\theta\right)$
    with parameters $\theta=\left\{\theta_{g},\theta_{a},\theta_{h}\right\}$, that,
    at each time step $t$, maps the the environment’s history $s_{1:t}=x_{1},l_{1},a_{1},...,x_{t−1},l_{t−1},a_{t−1},x_{t}$
    to a distribution over actions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类决策，动作网络 $f_{a}\left(h_{t},\theta_{a}\right)=\frac{e^{Linear\left(h_{t}\right)}}{Z}$，对分布施加条件以生成输出
    $action_{t}\sim p\left(\cdot|f_{a}\left(h_{t},\theta_{a}\right)\right)$，其中 $p$
    是 softmax。执行动作后，代理接收环境的新视觉观察 $x_{t+1}$ 和奖励信号 $r_{t+1}$。目标是最大化奖励信号的总和 $R=\sum_{t=1}^{T}r_{t}$。对于图像分类，如果对象在
    $T$ 步后被正确分类，则 $r_{T}=1$，否则为 $0$。该设置是部分可观察马尔可夫决策过程 (POMDP) 的一个实例。环境的真实状态是不可观察的，模型需要学习一个随机策略
    $\pi\left(\left(l_{t},a_{t}\right)|s_{1:t};\theta\right)$，其参数为 $\theta=\left\{\theta_{g},\theta_{a},\theta_{h}\right\}$，该策略在每个时间步
    $t$ 将环境历史 $s_{1:t}=x_{1},l_{1},a_{1},...,x_{t−1},l_{t−1},a_{t−1},x_{t}$ 映射到动作分布上。
- en: 'V-D End-To-End Memory Networks (EMNet): A memory-based end-to-end attention
    system'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 端到端记忆网络 (EMNet)：基于记忆的端到端注意系统
- en: 'End-To-End Memory Networks [[19](#bib.bib19)] uses attention for question answering,
    and language modeling. The architecture is a form of Memory Networks [[89](#bib.bib89)]
    but unlike, it is trained end-to-end. EMNet consists of a memory and a stack of
    identical attentional systems, as shown in Figure [8](#S5.F8 "Figure 8 ‣ V-D End-To-End
    Memory Networks (EMNet): A memory-based end-to-end attention system ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy").
    Each layer $i$ takes as input set $\left\{x_{1},...,x_{N}\right\}$ to be stored
    in memory. The input set is converted in memory vectors $\left\{m_{1},...,m_{N}\right\}$
    and $\left\{h_{1},...,h_{N}\right\}$ using the embedding matrix $A^{i}\in\mathbb{R}^{d\times
    V}$ to generate each $m_{i}\in\mathbb{R}^{d}$, and the matrix $C^{i}\in\mathbb{R}^{d\times
    V}$ to generate each $h_{i}\in\mathbb{R}^{d}$. In the first layer, the question
    $q$ is also embedded by $B^{1}$ to obtain an internal state $u^{1}$. From the
    second layer, the internal state $u^{i+1}=u^{i}+o^{i}\in\mathbb{R}^{d}$ is the
    sum of the $i$ layer output and the internal state $u^{i}$. Finally, last layer
    generates the prediction $\hat{a}=\frac{e^{Wu^{i+1}}}{\sum_{j=1}^{N}e^{Wu^{i+1}}}$,
    where $\hat{a}\in\mathbb{R}^{V}$ is the predicted label, and $W\in\mathbb{R}^{V\times
    d}$ is the matrix of weights.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '端到端记忆网络 [[19](#bib.bib19)] 使用注意力机制进行问答和语言建模。该架构是一种记忆网络 [[89](#bib.bib89)] 的形式，但与之不同的是，它是端到端训练的。EMNet
    由一个记忆体和一系列相同的注意力系统堆叠组成，如图 [8](#S5.F8 "Figure 8 ‣ V-D End-To-End Memory Networks
    (EMNet): A memory-based end-to-end attention system ‣ V Neural Attention Models
    ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy") 所示。每一层 $i$ 将输入集合
    $\left\{x_{1},...,x_{N}\right\}$ 存储到记忆中。输入集合被转换为记忆向量 $\left\{m_{1},...,m_{N}\right\}$
    和 $\left\{h_{1},...,h_{N}\right\}$，使用嵌入矩阵 $A^{i}\in\mathbb{R}^{d\times V}$ 生成每个
    $m_{i}\in\mathbb{R}^{d}$，以及矩阵 $C^{i}\in\mathbb{R}^{d\times V}$ 生成每个 $h_{i}\in\mathbb{R}^{d}$。在第一层中，问题
    $q$ 也通过 $B^{1}$ 嵌入以获得内部状态 $u^{1}$。从第二层开始，内部状态 $u^{i+1}=u^{i}+o^{i}\in\mathbb{R}^{d}$
    是第 $i$ 层输出和内部状态 $u^{i}$ 的和。最后，最后一层生成预测值 $\hat{a}=\frac{e^{Wu^{i+1}}}{\sum_{j=1}^{N}e^{Wu^{i+1}}}$，其中
    $\hat{a}\in\mathbb{R}^{V}$ 是预测标签，$W\in\mathbb{R}^{V\times d}$ 是权重矩阵。'
- en: '![Refer to caption](img/ee563366336ee340a0fe267e1e4dd296.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee563366336ee340a0fe267e1e4dd296.png)'
- en: 'Figure 8: End-to-End Memory Networks [[19](#bib.bib19)] architecture illustration.
    The network is a stack of attentional systems interconnected with each other and
    with external memory. All subsystems are a top-down and cognitive selection. This
    architecture shows the attention distributed throughout the network, in which
    the selection of inferior stimuli guides selection at higher levels through the
    interconnection between the modules. Such a structure is closer to the biological
    mechanisms since there is not only an attentional center in the brain.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：端到端记忆网络 [[19](#bib.bib19)] 架构示意图。网络是一个相互连接的注意力系统的堆叠，并与外部记忆相连。所有子系统都是自上而下的认知选择。该架构展示了网络中注意力的分布，其中下层刺激的选择通过模块之间的互连指导更高层次的选择。这种结构更接近生物机制，因为大脑中不仅仅存在一个注意力中心。
- en: The attention system is the architecture’s core. In each layer $i$, it takes
    as focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{h_{1},...,h_{N}\right\}$
    memories’ embeddings, and contextual input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},..,c_{t,N+1}^{1}\right\}=\left\{u^{i},m_{1},...,m_{N}\right\}$.
    Through an alignment function $e_{i,j}=(u^{i})^{T}m_{j}$ the attentional system
    computes the match between $u^{i}$ and each memory $m_{i}$, generating as output
    a mask of importance $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}=\left\{a_{1},...,a_{N}\right\}$
    for each $h_{i}$, where $a_{j}=\frac{e^{e_{i,j}}}{\sum_{j=1}^{N}e^{e_{i,j}}}\in\mathbb{R}$.
    The output $o^{i}$ is a sum over the transformed inputs $h_{i}$, weighted by the
    attention mask. Intuitively, attention looks for the memory elements most related
    to question $q$ using a simple alignment function dispensing traditional RNNs.
    This system can also be seen as version of attention in RNNSearch [[34](#bib.bib34)]
    with mutiple computational steps per output symbol, and with similar selection
    characteristics – soft, divided, top-down stateful, cognitive, and location-based.
    Note that the alignment function is differentiable. Therefore, during training,
    all architecture elements are jointly learned by minimizing a standard cross-entropy
    loss between $\hat{a}$ and the true label $a$ using stochastic gradient descent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系统是该架构的核心。在每一层 $i$ 中，它以 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{h_{1},...,h_{N}\right\}$
    的记忆嵌入作为关注目标，并使用上下文输入 $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},..,c_{t,N+1}^{1}\right\}=\left\{u^{i},m_{1},...,m_{N}\right\}$。通过对齐函数
    $e_{i,j}=(u^{i})^{T}m_{j}$，注意力系统计算 $u^{i}$ 与每个记忆 $m_{i}$ 之间的匹配度，生成每个 $h_{i}$ 的重要性掩码
    $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N}^{1}\right\}=\left\{a_{1},...,a_{N}\right\}$，其中
    $a_{j}=\frac{e^{e_{i,j}}}{\sum_{j=1}^{N}e^{e_{i,j}}}\in\mathbb{R}$。输出 $o^{i}$
    是经过注意力掩码加权的变换输入 $h_{i}$ 的总和。直观地，注意力通过简单的对齐函数寻找与问题 $q$ 最相关的记忆元素，摆脱了传统的 RNN。这种系统也可以看作是
    RNNSearch [[34](#bib.bib34)] 的一个版本，具有每个输出符号的多个计算步骤，并具有类似的选择特征——软、分段、自上而下的状态依赖、认知和基于位置。注意对齐函数是可微分的。因此，在训练过程中，所有架构元素通过最小化
    $\hat{a}$ 和真实标签 $a$ 之间的标准交叉熵损失，使用随机梯度下降进行联合学习。
- en: 'V-E Show, Attend and Tell: A multimodal approach'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'V-E Show, Attend and Tell: 一种多模态方法'
- en: 'Show, Attend and Tell [[41](#bib.bib41)] uses attention for image caption.
    The architecture consists of an encoder, decoder, and an attention system (Figure
    [9](#S5.F9 "Figure 9 ‣ V-E Show, Attend and Tell: A multimodal approach ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")).
    The encoder is a CNN to extract features from image $I$. And at time step $t$,
    the decoder uses LSTM units for generating one word $y$ for a caption. Similar
    to RNNSearch [[34](#bib.bib34)], the decoder calculates from a context vector
    $z_{t}$ a probability distribution for all possible caption symbols, as follows'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 'Show, Attend and Tell [[41](#bib.bib41)] 使用注意力机制进行图像字幕生成。该架构包括一个编码器、解码器和一个注意力系统（图
    [9](#S5.F9 "图 9 ‣ V-E Show, Attend and Tell: 一种多模态方法 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")）。编码器是一个
    CNN，用于从图像 $I$ 中提取特征。在时间步 $t$，解码器使用 LSTM 单元生成一个字幕词 $y$。类似于 RNNSearch [[34](#bib.bib34)]，解码器根据上下文向量
    $z_{t}$ 计算所有可能字幕符号的概率分布，如下所示'
- en: '|  | $(f_{1},...,f_{N})=Encoder(I)$ |  | (8) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $(f_{1},...,f_{N})=Encoder(I)$ |  | (8) |'
- en: '|  | $p(y_{t}&#124;y_{1},...,y_{C},I)=Decoder(z_{t})$ |  | (9) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y_{t}&#124;y_{1},...,y_{C},I)=Decoder(z_{t})$ |  | (9) |'
- en: where $C$ is caption size.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C$ 是标题大小。
- en: '![Refer to caption](img/815cd501b59fa1d69f0a6bd571fe1619.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/815cd501b59fa1d69f0a6bd571fe1619.png)'
- en: 'Figure 9: Show, Attend and Tell [[41](#bib.bib41)] architecture illustration.
    Attention unifies different sensory experiences to decide a task. A single attentional
    subsystem, which can be hard or soft, aligns high-level image and text representations
    to determine the next word in the decoder.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：Show, Attend and Tell [[41](#bib.bib41)] 架构示意图。注意力统一了不同的感官体验以决定任务。一个单一的注意力子系统，可以是硬性或软性，调整高层次的图像和文本表示，以确定解码器中的下一个词。
- en: 'The attention system has a subsystem (Figure [9](#S5.F9 "Figure 9 ‣ V-E Show,
    Attend and Tell: A multimodal approach ‣ V Neural Attention Models ‣ Neural Attention
    Models in Deep Learning: Survey and Taxonomy")) that generates the context vector
    $z_{t}$ for the decoder. It receives a contextual input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{h_{t-1}\right\}$
    with language content, a focus target $\tau_{t}=\left\{\tau_{t}^{2}\right\}=\left\{\tau_{t,1}^{2},...,\tau_{t,N}^{2}\right\}=\left\{f_{1},...,f_{N}\right\}$
    with visual content, and produces attention weigths $a_{t}=\left\{a_{t}^{2}\right\}=\left\{a_{t,1}^{2},...,a_{t,N}^{2}\right\}=\left\{a_{1},...,a_{N}\right\}$
    as output, where $h_{t-1}\in\mathbb{R}^{1\times d}$ is a previous decoder state,
    $f_{j}\in\mathbb{R}^{1\times d_{I}}$ is encoder annotation vector, $a_{t}^{2}\in\mathbb{R}^{1\times
    N}$ are attention weights for all annotation vectors. This structure receives
    as input different sensory modalities (i.e, visual in focus target and language
    in context), inspired by perceptual theories [[96](#bib.bib96)]. Nos seres vivos,
    a experiência perceptual não é desarticulada e fragmentada, mas está intimimente
    ligada à uma unidade objectual comum. Por exemplo, para agarrar uma bola, uma
    pessoa precisa vê-la se aproximando, ou para decidir o gosto de uma comida, elementos
    do tato e do cheiro ponderam a decisão. Similarmente, esta arquitetura implementa
    atenção entre duas fontes sensoriais diferentes para decidir uma tarefa. The attention
    system is equal to the RNNSearch [[34](#bib.bib34)], and has same selection features
    – soft, divided, location-based, top-down stateful, oriented, and cognitive –
    if implements soft attention. In contrast, if the mechanism is hard attention
    has an additional sampling block $a_{t}\sim Multinoulli_{N}(\alpha_{t,j})$ parameterized
    by the scores $\alpha_{t,j}=\frac{e^{e_{t,j}}}{\sum_{j=1}^{N}e^{e_{t,j}}}$, where
    $e_{t,j}=a(h_{t-1},f_{j})$. The sampling mechanism makes the system stochastic
    with hard continuity.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力系统有一个子系统（图 [9](#S5.F9 "Figure 9 ‣ V-E Show, Attend and Tell: A multimodal
    approach ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")），用于生成解码器的上下文向量 $z_{t}$。它接收语言内容的上下文输入 $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{h_{t-1}\right\}$，具有视觉内容的焦点目标
    $\tau_{t}=\left\{\tau_{t}^{2}\right\}=\left\{\tau_{t,1}^{2},...,\tau_{t,N}^{2}\right\}=\left\{f_{1},...,f_{N}\right\}$，并生成作为输出的注意力权重
    $a_{t}=\left\{a_{t}^{2}\right\}=\left\{a_{t,1}^{2},...,a_{t,N}^{2}\right\}=\left\{a_{1},...,a_{N}\right\}$，其中
    $h_{t-1}\in\mathbb{R}^{1\times d}$ 是之前的解码器状态，$f_{j}\in\mathbb{R}^{1\times d_{I}}$
    是编码器标注向量，$a_{t}^{2}\in\mathbb{R}^{1\times N}$ 是所有标注向量的注意力权重。这个结构接收不同的感官模态作为输入（即，焦点目标中的视觉和上下文中的语言），灵感来自感知理论
    [[96](#bib.bib96)]。我们生物体的感知经验不是脱节和碎片化的，而是紧密联系在一个共同的对象单位中。例如，为了抓住一个球，一个人需要看到它靠近，或者为了决定食物的味道，触觉和嗅觉元素会影响决策。类似地，这种架构实现了两种不同感官来源之间的注意力，以决定任务。注意力系统等同于
    RNNSearch [[34](#bib.bib34)]，并具有相同的选择特性——软的、分段的、基于位置的、从上而下的、有状态的、定向的和认知的——如果实现软注意力。相比之下，如果机制是硬注意力，则具有一个额外的采样块
    $a_{t}\sim Multinoulli_{N}(\alpha_{t,j})$，其参数化为分数 $\alpha_{t,j}=\frac{e^{e_{t,j}}}{\sum_{j=1}^{N}e^{e_{t,j}}}$，其中
    $e_{t,j}=a(h_{t-1},f_{j})$。采样机制使得系统具有硬性连续性的随机性。'
- en: V-F Deep Recurrent Attentive Writer (DRAW)
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 深度递归注意力写入器（DRAW）
- en: 'DRAW [[40](#bib.bib40)] uses attention for image generation. The architecture
    is similar variational autoencoders (VAEs) [[97](#bib.bib97)] with some differences
    (Figure [10](#S5.F10 "Figure 10 ‣ V-F Deep Recurrent Attentive Writer (DRAW) ‣
    V Neural Attention Models ‣ Neural Attention Models in Deep Learning: Survey and
    Taxonomy")). Firstly, the encoder/decoder are recurrent neural networks, and the
    encoder receives the previous outputs from the decoder. Secondly, the decoder
    outputs are added successively to the distribution generating the data, instead
    of generating that distribution in a single step. And thirdly, the attention system
    dynamically updates the network, restricting the input region observed by the
    encoder and the output region modified by the decoder. The attention make decisions
    about which regions are input for the network, which regions are modified in the
    generated image, and also what needs to be modified.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'DRAW [[40](#bib.bib40)] 使用注意力机制进行图像生成。该架构类似于变分自编码器（VAEs）[[97](#bib.bib97)]，但有一些不同之处（图
    [10](#S5.F10 "Figure 10 ‣ V-F Deep Recurrent Attentive Writer (DRAW) ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")）。首先，编码器/解码器是递归神经网络，编码器接收来自解码器的先前输出。其次，解码器的输出被连续地添加到生成数据的分布中，而不是在单一步骤中生成该分布。第三，注意力系统动态更新网络，限制编码器观察的输入区域和解码器修改的输出区域。注意力机制决定了网络输入哪些区域、生成的图像中修改哪些区域，以及需要修改什么。'
- en: '![Refer to caption](img/e0f54d8def2a5e464946b50236ae9539.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e0f54d8def2a5e464946b50236ae9539.png)'
- en: 'Figure 10: DRAW [[40](#bib.bib40)] architecture illustration. The architecture
    has an encoder, a decoder, and attention-controlled read/write heads. Read generates
    a superimposed filter grid on an image and extract an patch $N\times N$. Similarly,
    write head defines which image patch will be drawn. Both systems are divided,
    selective, oriented, location-based, top-down stateful, soft and hard.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：DRAW [[40](#bib.bib40)] 架构图。该架构包含一个编码器、一个解码器和一个受注意力控制的读/写头。读头在图像上生成一个叠加的滤波器网格，并提取一个
    $N\times N$ 的补丁。类似地，写头定义了将绘制哪个图像补丁。两个系统都是分离的、选择性的、定向的、基于位置的、从上到下的、有状态的、软的和硬的。
- en: 'At each time-step $t$, the encoder receives as input the read vector $r_{t}=read(x_{t},\hat{x_{t}},h_{t-1}^{dec})$,
    and the previous decoder’s state $h_{t-1}^{dec}$ for generate current state $h_{t}^{enc}=RNN^{enc}(h_{t-1}^{enc},[r_{t},h_{t-1}^{dec}])$,
    where $x_{t}$ is input image, $\hat{x_{t}}=x-\sigma(wt_{t-1})$ is error image,
    and $\sigma$ is the sigmoid function. The $h_{t}^{enc}$ parameterize the $Q$ distribution
    over the latent space vector $z_{t}\sim Q(Z_{t}|h_{t}^{enc})$, which is input
    to the decoder. The $Q\left(Z_{t}|h_{t}^{enc}\right)$ distribution is a diagonal
    Gaussian $N\left(Z_{t}|\mu_{t},\sigma_{t}\right)$ in latent space, where $\mu_{t}=W(h_{t}^{enc})$,
    $\sigma_{t}=e^{W(h_{t}^{enc})}$, W are weight matrices of a linear transformation
    layer. The decoder output $h_{t}^{dec}=RNN^{dec}(h_{t-1}^{dec},z_{t})$ is added
    by the write function for reconstruct the image through the cumulative canvas
    matrix $wt_{t}=wt_{t-1}+write(h_{t}^{dec})$. After T iterations the canvas matrix
    $wt_{T}$ parameterize $D(X|wt_{T})$ for generate image $\tilde{x}$:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步 $t$，编码器接收读向量 $r_{t}=read(x_{t},\hat{x_{t}},h_{t-1}^{dec})$ 作为输入，并接收上一个解码器的状态
    $h_{t-1}^{dec}$ 以生成当前状态 $h_{t}^{enc}=RNN^{enc}(h_{t-1}^{enc},[r_{t},h_{t-1}^{dec}])$，其中
    $x_{t}$ 是输入图像，$\hat{x_{t}}=x-\sigma(wt_{t-1})$ 是误差图像，$\sigma$ 是 sigmoid 函数。$h_{t}^{enc}$
    参数化 $Q$ 分布，该分布定义在潜在空间向量 $z_{t}\sim Q(Z_{t}|h_{t}^{enc})$ 上，作为解码器的输入。$Q\left(Z_{t}|h_{t}^{enc}\right)$
    分布是潜在空间中的对角高斯 $N\left(Z_{t}|\mu_{t},\sigma_{t}\right)$，其中 $\mu_{t}=W(h_{t}^{enc})$，$\sigma_{t}=e^{W(h_{t}^{enc})}$，W
    是线性变换层的权重矩阵。解码器输出 $h_{t}^{dec}=RNN^{dec}(h_{t-1}^{dec},z_{t})$ 通过写函数加到累积画布矩阵 $wt_{t}=wt_{t-1}+write(h_{t}^{dec})$
    上，以重建图像。经过 T 次迭代后，画布矩阵 $wt_{T}$ 参数化 $D(X|wt_{T})$ 以生成图像 $\tilde{x}$：
- en: '|  | $\tilde{z_{t}}\sim P(Z_{t})$ |  | (10) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{z_{t}}\sim P(Z_{t})$ |  | (10) |'
- en: '|  | $\tilde{h}_{t}^{dec}=RNN^{dec}(\tilde{h}_{t-1}^{dec},\tilde{z}_{t})$ |  |
    (11) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{h}_{t}^{dec}=RNN^{dec}(\tilde{h}_{t-1}^{dec},\tilde{z}_{t})$ |  |
    (11) |'
- en: '|  | $\tilde{wt}_{t}=\tilde{wt}_{t-1}+write(\tilde{h}_{t}^{dec})$ |  | (12)
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{wt}_{t}=\tilde{wt}_{t-1}+write(\tilde{h}_{t}^{dec})$ |  | (12)
    |'
- en: '|  | $\tilde{x}\sim D(X&#124;\tilde{wt}_{T})$ |  | (13) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{x}\sim D(X&#124;\tilde{wt}_{T})$ |  | (13) |'
- en: where P is a prior, $\tilde{z_{t}}$ is a sample of latent space. D is a probability
    distribution, if the input is binary it is a Bernoulli distribution with a mean
    given by $\sigma(wt_{T})$.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 P 是先验分布，$\tilde{z_{t}}$ 是潜在空间的样本。D 是一个概率分布，如果输入是二进制的，则是一个伯努利分布，其均值由 $\sigma(wt_{T})$
    给出。
- en: 'The read and write functions are controlled by the attention systems for decide
    which image path will be processed, and which image region will be modified in
    the output (Figure [10](#S5.F10 "Figure 10 ‣ V-F Deep Recurrent Attentive Writer
    (DRAW) ‣ V Neural Attention Models ‣ Neural Attention Models in Deep Learning:
    Survey and Taxonomy")). The attention system has only one subsystem and a single
    sensory modality (i.e, visual). At each time step t, the subsystem takes as input
    a contextual $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1},c_{t,4}^{1}\right\}=\left\{g_{X},g_{Y},\delta,\sigma^{2}\right\}$
    with grid properties, a focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,2N}^{1}\right\}=\left\{x,\hat{x}\right\}$,
    and produces attention weights $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,2N}^{1}\right\}=\left\{F_{x},F_{y}\right\}$.
    The pair $(g_{X},g_{Y})$ is grid center coordinates, $\delta$ is width of the
    step, $\sigma^{2}$ is isotropic variance of the Gaussian filters, $x$ $\in\mathbb{R}^{B\times
    A}$ is a input image, $\hat{x}\in\mathbb{R}^{A\times B}$ is the error image, $a_{t}^{1}\in\mathbb{R}^{1\times
    2N}$ is the weight of attention to $\tau_{t}$. The parameters $g_{X}$, $g_{Y}$,
    $\delta$, $\sigma^{2}$, $\gamma$ are determined dynamically using a linear transformation
    of the $h^{dec}$, as'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和写入功能由注意力系统控制，以决定处理哪条图像路径以及输出中将修改哪个图像区域（图 [10](#S5.F10 "图 10 ‣ V-F 深度递归注意力写入器
    (DRAW) ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")）。注意力系统只有一个子系统和一个感官模态（即视觉）。在每个时间步$t$，子系统将上下文输入$c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},c_{t,2}^{1},c_{t,3}^{1},c_{t,4}^{1}\right\}=\left\{g_{X},g_{Y},\delta,\sigma^{2}\right\}$（具有网格属性）、一个焦点目标$\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,2N}^{1}\right\}=\left\{x,\hat{x}\right\}$，并生成注意力权重$a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,2N}^{1}\right\}=\left\{F_{x},F_{y}\right\}$。对
    $(g_{X},g_{Y})$ 是网格中心坐标，$\delta$ 是步长的宽度，$\sigma^{2}$ 是高斯滤波器的各向同性方差，$x \in \mathbb{R}^{B\times
    A}$ 是输入图像，$\hat{x} \in \mathbb{R}^{A\times B}$ 是误差图像，$a_{t}^{1} \in \mathbb{R}^{1\times
    2N}$ 是对$\tau_{t}$的注意力权重。参数$g_{X}$、$g_{Y}$、$\delta$、$\sigma^{2}$、$\gamma$ 是使用 $h^{dec}$
    的线性变换动态确定的，如
- en: '|  | $(\tilde{g_{X}},\tilde{g_{Y}},log\sigma^{2},log\tilde{\delta},log\gamma)=W(h^{dec})$
    |  | (14) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\tilde{g_{X}},\tilde{g_{Y}},log\sigma^{2},log\tilde{\delta},log\gamma)=W(h^{dec})$
    |  | (14) |'
- en: '|  | $g_{X}=\frac{A+1}{2}(\tilde{g}_{X}+1)$ |  | (15) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{X}=\frac{A+1}{2}(\tilde{g}_{X}+1)$ |  | (15) |'
- en: '|  | $g_{y}=\frac{B+1}{2}(\tilde{g}_{Y}+1)$ |  | (16) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{y}=\frac{B+1}{2}(\tilde{g}_{Y}+1)$ |  | (16) |'
- en: '|  | $\delta=\frac{max(A,B)-1}{N-1}\tilde{\delta}$ |  | (17) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta=\frac{max(A,B)-1}{N-1}\tilde{\delta}$ |  | (17) |'
- en: where variance, step, and intensity are given on a log scale to ensure positive
    values. The scale of $g_{X},g_{Y},\delta$ is chosen to ensure that the initial
    path, with an initialized network, covers approximately the entire input image.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 方差、步长和强度在对数尺度上给出，以确保正值。$g_{X}$、$g_{Y}$ 和 $\delta$ 的尺度选择是为了确保具有初始化网络的初始路径大致覆盖整个输入图像。
- en: 'The attention system in read is divided, selective, oriented, perceptive, location-based,
    top-down stateful, soft, and hard. Despite being completely differentiable, this
    system applies the soft mask over the entire image as a divided selection. However,
    unlike other approaches in the literature, targeting mask computation only generates
    patches from one region, such as hard and selective attention. In the write, the
    features are similar, but the attention is cognitive. Specifically, the attention
    functions generate as output $a_{t}$ matrices of the horizontal and vertical Gaussian
    filter bank $F_{X}[i,a]=\frac{1}{Z_{X}}e^{\left(-\frac{(a-\mu_{X}^{i})^{2}}{2\sigma^{2}}\right)}$,
    and $F_{Y}[j,b]=\frac{1}{Z_{Y}}e^{\left(-\frac{(b-\mu_{Y}^{i})^{2}}{2\sigma^{2}}\right)}$,
    where $F_{X}\in\mathbb{R}^{N\times A}$, $F_{Y}$ $\in\mathbb{R}^{N\times B}$, $(i,j)$
    is a point of the attention mask, $\left(a,b\right)$ is a point in the input image,
    $Z_{X}$ and $Z_{Y}$ are normalization constants for $\sum_{a}F_{X}[i,a]=1$ and
    $\sum_{b}F_{Y}[j,b]=1$. Finally, read operation returns concatenation of two patches
    $N\times N$ of the input image and the error image. For the write operation, a
    specific set of parameters $\hat{\delta}$, $\hat{F_{X}}$, $\hat{F_{Y}}$ are output
    from $h_{t}^{dec}$:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取操作中，注意力系统被分为选择性、定向、感知、基于位置、从上到下状态保持、软性和硬性。尽管完全可微分，但该系统将软性掩膜应用于整个图像作为分离选择。然而，与文献中的其他方法不同，目标掩膜计算只生成来自一个区域的补丁，如硬性和选择性注意力。在写入操作中，特征类似，但注意力是认知性的。具体来说，注意力函数生成的输出为
    $a_{t}$ 矩阵，横向和纵向高斯滤波器组 $F_{X}[i,a]=\frac{1}{Z_{X}}e^{\left(-\frac{(a-\mu_{X}^{i})^{2}}{2\sigma^{2}}\right)}$
    和 $F_{Y}[j,b]=\frac{1}{Z_{Y}}e^{\left(-\frac{(b-\mu_{Y}^{i})^{2}}{2\sigma^{2}}\right)}$，其中
    $F_{X}\in\mathbb{R}^{N\times A}$，$F_{Y}$ $\in\mathbb{R}^{N\times B}$，$(i,j)$ 是注意力掩膜的一个点，$\left(a,b\right)$
    是输入图像中的一个点，$Z_{X}$ 和 $Z_{Y}$ 是用于 $\sum_{a}F_{X}[i,a]=1$ 和 $\sum_{b}F_{Y}[j,b]=1$
    的归一化常数。最后，读取操作返回输入图像和错误图像的两个补丁 $N\times N$ 的串联。对于写入操作，一组特定的参数 $\hat{\delta}$，$\hat{F_{X}}$，$\hat{F_{Y}}$
    从 $h_{t}^{dec}$ 输出：
- en: '|  | $read(x,\hat{x}_{t},h_{t-1}^{dec})=\gamma[F_{Y}xF_{X}^{T},F_{Y}\hat{x}F_{X}^{T}]$
    |  | (18) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $read(x,\hat{x}_{t},h_{t-1}^{dec})=\gamma[F_{Y}xF_{X}^{T},F_{Y}\hat{x}F_{X}^{T}]$
    |  | (18) |'
- en: '|  | $w_{t}=W(h_{t}^{dec})$ |  | (19) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{t}=W(h_{t}^{dec})$ |  | (19) |'
- en: '|  | $write(h_{t}^{dec})=\frac{1}{\hat{\gamma}}\hat{F_{Y}^{T}}w_{t}\hat{F_{X}}$
    |  | (20) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $write(h_{t}^{dec})=\frac{1}{\hat{\gamma}}\hat{F_{Y}^{T}}w_{t}\hat{F_{X}}$
    |  | (20) |'
- en: where $w_{t}$ is $\mathbb{R}^{N\times N}$ writing patch emited by $h_{t}^{dec}$.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{t}$ 是 $\mathbb{R}^{N\times N}$ 的写入补丁，由 $h_{t}^{dec}$ 发出。
- en: V-G Bi-Directional Attention Flow (BiDAF)
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-G 双向注意力流（BiDAF）
- en: 'BiDAF [[47](#bib.bib47)] uses attention for machine comprehension. The architecture
    consists of eight main components – character embedding layer, word embedding
    layer, contextual embedding layer, attention system, context-to-query, query-to-context,
    modeling layer and output layer (Figure [11](#S5.F11 "Figure 11 ‣ V-G Bi-Directional
    Attention Flow (BiDAF) ‣ V Neural Attention Models ‣ Neural Attention Models in
    Deep Learning: Survey and Taxonomy")). The character embedding layer maps each
    word to a high-dimensional vector space. Let $\left\{wc_{1},...,wc_{N_{c}}\right\}$
    and $\left\{wq_{1},...,wq_{N_{q}}\right\}$ represent the words in the input paragraph
    and the question, respectively. Characters are embedded into vectors using Convolutional
    Neural Network (CNN). The word embedding layer maps each word to a high-dimensional
    vector space using pre-trained Glove [[98](#bib.bib98)]. The concatenation of
    the character and word embedding vectors is passed through two-layer of the Highway
    Network [[99](#bib.bib99)], and generate context set $X=\left\{x_{1},...,x_{N_{c}}\right\}$,
    $x_{i}\in\mathbb{R}^{d}$, and query set $Q=\left\{q_{1},...,q_{N_{c}}\right\}$,
    $q_{i}\in\mathbb{R}^{d}$.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: BiDAF [[47](#bib.bib47)] 使用注意力机制进行机器理解。该架构包括八个主要组件——字符嵌入层、词嵌入层、上下文嵌入层、注意力系统、上下文到查询、查询到上下文、建模层和输出层（图
    [11](#S5.F11 "图 11 ‣ V-G 双向注意力流（BiDAF） ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：综述与分类")）。字符嵌入层将每个词映射到高维向量空间。设
    $\left\{wc_{1},...,wc_{N_{c}}\right\}$ 和 $\left\{wq_{1},...,wq_{N_{q}}\right\}$
    分别表示输入段落和问题中的词。字符通过卷积神经网络（CNN）嵌入到向量中。词嵌入层使用预训练的 Glove [[98](#bib.bib98)] 将每个词映射到高维向量空间。字符和词嵌入向量的串联通过两层高速公路网络
    [[99](#bib.bib99)]，生成上下文集 $X=\left\{x_{1},...,x_{N_{c}}\right\}$，$x_{i}\in\mathbb{R}^{d}$，和查询集
    $Q=\left\{q_{1},...,q_{N_{c}}\right\}$，$q_{i}\in\mathbb{R}^{d}$。
- en: '![Refer to caption](img/470dcf50c1d0080b216a3aa7a3c9b773.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/470dcf50c1d0080b216a3aa7a3c9b773.png)'
- en: 'Figure 11: BiDAF [[47](#bib.bib47)] architecture illustration. The attention
    system has small independent modules that communicate different queries with the
    target. In sequence, a hierarchically superior subsystem recalibrates previous
    attentional signals changing original attention focus.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：BiDAF [[47](#bib.bib47)]架构示意图。注意力系统具有小型独立模块，这些模块与目标进行不同查询的通信。接着，一个层次上优越的子系统重新校准先前的注意力信号，改变原始注意力的焦点。
- en: The contextual embedding layer takes as input a set $X$ and $Q$ and uses an
    bidirectional LSTM [[100](#bib.bib100)] to model temporal iterations between words,
    generating as output a set of context vectors $H=\left\{h_{1},...,h_{N_{c}}\right\}$,
    $h_{i}\in\mathbb{R}^{2d}$, and query vectors $U=\left\{u_{1},...,u_{N_{q}}\right\}$,
    $u_{i}\in\mathbb{R}^{2d}$. Each vector of $H$ and $U$ is 2d dimensional due to
    the concatenation of the forward and backward LSTMs outputs. Note that the first
    three layers are feature extractors for context and queries in different granularity
    levels, similarly CNNs. The next layer is the attentional system, whose function
    is to link and disseminate information from the context and query. Unlike the
    classic mechanisms, the attentional system does not summarize the query and the
    context in vectors of unique features. Instead, the attention vector and the embedding’s
    previous layers can flow to the subsequent modeling layer to reduce information
    loss by the early summary.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文嵌入层以一组$X$和$Q$作为输入，使用双向LSTM [[100](#bib.bib100)] 来建模词语之间的时间迭代，生成一组上下文向量$H=\left\{h_{1},...,h_{N_{c}}\right\}$，$h_{i}\in\mathbb{R}^{2d}$，以及查询向量$U=\left\{u_{1},...,u_{N_{q}}\right\}$，$u_{i}\in\mathbb{R}^{2d}$。由于前向和后向LSTM输出的拼接，每个$H$和$U$的向量都是2d维度的。注意，前三层是用于不同粒度层次的上下文和查询的特征提取器，类似于CNN。下一层是注意力系统，其功能是链接和传播来自上下文和查询的信息。与经典机制不同，注意力系统不会将查询和上下文总结成具有唯一特征的向量。相反，注意力向量和嵌入的前几层可以流向后续建模层，以减少早期总结带来的信息丢失。
- en: The attentional system has several subsystems in parallel, which receive as
    input the same focus target and different contextual information, that is, $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{c}}^{1}\right\}=\left\{h_{1},...,h_{N_{c}}\right\}$,
    $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{u_{j}\right\}$.
    Each subsystem has an alignment function that relates all context vectors to a
    given query $u_{j}$, and outputs masks that together make up an attention matrix
    $S\in\mathbb{R}^{N_{c}\times N_{q}}$, ie, $a_{t}=\left\{a_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{c}}^{1}\right\}=\left\{S_{1,1},...,S_{N_{c},j}\right\}$.
    Each subsystem presents the same selection features – soft, divided, location-based,
    top-down stateful, and cognitive. Specifically,
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系统具有多个并行的子系统，这些子系统接收相同的焦点目标和不同的上下文信息，即$\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{c}}^{1}\right\}=\left\{h_{1},...,h_{N_{c}}\right\}$，$c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{u_{j}\right\}$。每个子系统都有一个对齐函数，该函数将所有上下文向量与给定查询$u_{j}$相关联，并输出掩码，这些掩码共同构成一个注意力矩阵$S\in\mathbb{R}^{N_{c}\times
    N_{q}}$，即$a_{t}=\left\{a_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{c}}^{1}\right\}=\left\{S_{1,1},...,S_{N_{c},j}\right\}$。每个子系统呈现相同的选择特征——软性、分段、基于位置、从上到下有状态和认知特征。具体来说，
- en: '|  | $S_{i,j}=a(h_{i},u_{j})$ |  | (21) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{i,j}=a(h_{i},u_{j})$ |  | (21) |'
- en: '|  | $a(h_{i},u_{j})=w_{(S)}^{T}[h;u;h\odot u]$ |  | (22) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $a(h_{i},u_{j})=w_{(S)}^{T}[h;u;h\odot u]$ |  | (22) |'
- en: where $S_{i,j}\in\mathbb{R}$ indicates the similarity between the $i-th$ word
    of the context vector and the $j-th$ query word, $h_{i}$ is the $i-th$ context
    vector, and $u_{j}$ is the $j-th$ query vector, and $a$ is alignment function
    that encodes the similarity between two input vectors, $w_{(S)}\in\mathbb{R}^{6D}$
    is a trainable weight vector, $\odot$ is point-to-point multiplication, [;] is
    vector concatenation across row.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$S_{i,j}\in\mathbb{R}$表示上下文向量第$i$个词和第$j$个查询词之间的相似度，$h_{i}$是第$i$个上下文向量，$u_{j}$是第$j$个查询向量，$a$是对齐函数，用于编码两个输入向量之间的相似度，$w_{(S)}\in\mathbb{R}^{6D}$是一个可训练的权重向量，$\odot$是逐点乘法，[;]是跨行向量拼接。
- en: The $S$ matrix is input to the context-to-query layer (C2Q), which normalizes
    the weights and applies them to each query vector $u_{j}$ to produce the set $\widetilde{U}=\left\{\widetilde{u_{1}},...,\widetilde{u}_{N_{c}}\right\}$
    with the most relevant query words for each context word. $S$ is also input to
    the query-to-context layer (Q2C), which has an attention subsystem hard, selective,
    location-based, top-down stateful, and cognitive. The attention subsystem uses
    a row’s $S$ matrix as contextual information and the $H$ set as focus target $\tau_{t}$.
    It uses a hard selection to change the focus of attention produced in previous
    layers using the $max_{col}$ function. The output’s attention mask searches for
    which words in the context are essential to answer the query generating a dynamic
    context vector $\widetilde{h}$. Specifically,
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: $S$ 矩阵被输入到上下文到查询层（C2Q），该层规范化权重并将其应用于每个查询向量 $u_{j}$ 以生成集合 $\widetilde{U}=\left\{\widetilde{u_{1}},...,\widetilde{u}_{N_{c}}\right\}$，其中包含每个上下文词最相关的查询词。$S$
    也被输入到查询到上下文层（Q2C），该层具有硬选择、选择性、基于位置、从上到下的状态和认知的注意力子系统。注意力子系统使用行的 $S$ 矩阵作为上下文信息，并将
    $H$ 集合作为焦点目标 $\tau_{t}$。它使用硬选择来改变之前层产生的注意力焦点，使用 $max_{col}$ 函数。输出的注意力掩码搜索上下文中哪些词对回答查询至关重要，生成动态上下文向量
    $\widetilde{h}$。具体而言，
- en: '|  | $\alpha_{t}=softmax(S_{t,1:N_{q}})$ |  | (23) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{t}=softmax(S_{t,1:N_{q}})$ |  | (23) |'
- en: '|  | $\widetilde{u_{t}}=\sum_{j=1}^{N_{q}}\alpha_{t,j}u_{j}$ |  | (24) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{u_{t}}=\sum_{j=1}^{N_{q}}\alpha_{t,j}u_{j}$ |  | (24) |'
- en: '|  | $b=softmax(max_{col}(S))$ |  | (25) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | $b=softmax(max_{col}(S))$ |  | (25) |'
- en: '|  | $\widetilde{h}=\sum_{j=1}^{N_{c}}b_{j}h_{j}$ |  | (26) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widetilde{h}=\sum_{j=1}^{N_{c}}b_{j}h_{j}$ |  | (26) |'
- en: where $\alpha_{t}\in\mathbb{R}^{N_{q}}$ and $\sum\alpha_{t}=1$ $\forall t$,
    $\widetilde{u_{t}}\in\mathbb{R}^{2d}$ is an array containing the query vectors
    served by the entire context, $b\in\mathbb{T}$, and the $max_{col}$ is performed
    across the column, and $\widetilde{h}\in\mathbb{R}^{2d}$.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha_{t}\in\mathbb{R}^{N_{q}}$ 并且 $\sum\alpha_{t}=1$ $\forall t$，$\widetilde{u_{t}}\in\mathbb{R}^{2d}$
    是一个包含由整个上下文提供的查询向量的数组，$b\in\mathbb{T}$，$max_{col}$ 在列上进行，$\widetilde{h}\in\mathbb{R}^{2d}$。
- en: Finally, the contextual embeddings $h_{i}$, $\widetilde{u}_{i}$, and $\widetilde{h}_{i}$
    are combined to generate $G=\left\{g_{1},...,g_{N_{c}}\right\}$, where each vector
    can be considered as the query-aware representation of each context word. Next,
    $G$ is input for the modeling layer, which captures the interaction among the
    context words conditioned on the query. This layer uses two bidirectional LSTMs
    [[100](#bib.bib100)], with output dimension d for each direction, getting the
    matrix $\beta$ which is passed to the output layer to predict the response. The
    output layer is application-specific, predicting the initial and final indices
    of the paragraph sentence. For this, the probability distributions of the initial
    and final index are obtained throughout the entire paragraph, as follows
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，上下文嵌入 $h_{i}$、$\widetilde{u}_{i}$ 和 $\widetilde{h}_{i}$ 被结合生成 $G=\left\{g_{1},...,g_{N_{c}}\right\}$，其中每个向量可以被视为每个上下文词的查询感知表示。接下来，$G$
    被输入到建模层，该层捕捉上下文词之间基于查询的交互。该层使用两个双向LSTM [[100](#bib.bib100)]，每个方向的输出维度为d，获得矩阵 $\beta$，然后传递给输出层以预测响应。输出层是特定应用的，预测段落句子的初始和最终索引。为此，在整个段落中获得初始和最终索引的概率分布，如下所示：
- en: '|  | $g_{i}=\beta(h_{i},\widetilde{u_{i}},\widetilde{h_{i}})$ |  | (27) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $g_{i}=\beta(h_{i},\widetilde{u_{i}},\widetilde{h_{i}})$ |  | (27) |'
- en: '|  | $\beta(h_{i},\widetilde{u_{i}},\widetilde{h_{i}})=[h_{i};\widetilde{u_{i}};h_{i}\odot\widetilde{u_{i}};h_{i}\odot\widetilde{h_{i}}]$
    |  | (28) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta(h_{i},\widetilde{u_{i}},\widetilde{h_{i}})=[h_{i};\widetilde{u_{i}};h_{i}\odot\widetilde{u_{i}};h_{i}\odot\widetilde{h_{i}}]$
    |  | (28) |'
- en: '|  | $p^{1}=Softmax(w_{(p^{1})}^{T}[G;M])$ |  | (29) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{1}=Softmax(w_{(p^{1})}^{T}[G;M])$ |  | (29) |'
- en: '|  | $p^{2}=Softmax(w_{(p^{2})}^{T}[G;M_{2}])$ |  | (30) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{2}=Softmax(w_{(p^{2})}^{T}[G;M_{2}])$ |  | (30) |'
- en: where $g_{i}\in\mathbb{R}^{d_{G}}$ corresponding to the i-th context word, $\beta\in\mathbb{R}^{d_{G}}(d_{G}=8d)$
    can be an trainable neural network but in this example is a function that merges
    three vectors, and $d_{G}$ is the output dimension of the $\beta$ function, $w_{(p^{1})}\in\mathbb{R}^{10d}$
    are vectors of trainable weights, $M$ is the output of the first modeling layer,
    $M_{2}\in\mathbb{R}^{2d\times N_{c}}$ is the output of M after going through another
    layer of bidirectional LSTM.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g_{i}\in\mathbb{R}^{d_{G}}$ 对应于第 i 个上下文词，$\beta\in\mathbb{R}^{d_{G}}(d_{G}=8d)$
    可以是一个可训练的神经网络，但在这个例子中是一个合并三个向量的函数，$d_{G}$ 是 $\beta$ 函数的输出维度，$w_{(p^{1})}\in\mathbb{R}^{10d}$
    是可训练权重的向量，$M$ 是第一个建模层的输出，$M_{2}\in\mathbb{R}^{2d\times N_{c}}$ 是经过另一个双向 LSTM 层处理后的
    M 的输出。
- en: V-H Neural Transformer
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-H 神经 Transformer
- en: 'Neural Transformer [[18](#bib.bib18)] uses attention for machine translation.
    The architecture consists of an arbitrary amount of stacked encoders/decoders,
    as shown in Figure [12](#S5.F12 "Figure 12 ‣ V-H Neural Transformer ‣ V Neural
    Attention Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy").
    Each encoder has linear layers, an attention system, feed-forward neural networks,
    and normalization steps. The attention system is architecture’s core. It has several
    parallel heads. Each head has $N$ attention subsystems that perform the same task,
    but with different contextual inputs. In the first layer, the encoder receives
    as input an embedding matrix $I=I_{emb}+T$ $\in\mathbb{R}^{N\times d_{emb}}$ for
    $N$ words, where $I_{emb}=\left\{e_{1},e_{2},e_{3},...,e_{N}\right\},e_{i}\in\mathbb{R}^{1\times
    d_{emb}}$ is an embedding matrix, and $T=\left\{PE_{0},PE_{1},PE_{2},PE_{3},...,PE_{N}\right\}$
    is composed by positional encodings $PE_{i}=\left\{pe_{0},pe_{1},p_{2},...,p_{d_{emb}}\right\}$:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 神经 Transformer [[18](#bib.bib18)] 使用注意力机制进行机器翻译。该架构由任意数量的堆叠编码器/解码器组成，如图 [12](#S5.F12
    "图 12 ‣ V-H 神经 Transformer ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：综述与分类") 所示。每个编码器具有线性层、注意力系统、前馈神经网络和归一化步骤。注意力系统是架构的核心。它有多个并行的头部。每个头部有
    $N$ 个注意力子系统，这些子系统执行相同的任务，但具有不同的上下文输入。在第一层中，编码器接收一个嵌入矩阵 $I=I_{emb}+T$ $\in\mathbb{R}^{N\times
    d_{emb}}$ 作为输入，其中 $I_{emb}=\left\{e_{1},e_{2},e_{3},...,e_{N}\right\},e_{i}\in\mathbb{R}^{1\times
    d_{emb}}$ 是一个嵌入矩阵，$T=\left\{PE_{0},PE_{1},PE_{2},PE_{3},...,PE_{N}\right\}$ 由位置编码
    $PE_{i}=\left\{pe_{0},pe_{1},p_{2},...,p_{d_{emb}}\right\}$ 组成：
- en: '|  | $pe_{i}\in\left\{\begin{matrix}sin(\frac{pos}{10000^{\frac{2i}{d_{emb}}}})&amp;i=0,2,...,d_{emb}\\
    cos(\frac{pos}{10000^{\frac{2i}{d_{emb}}}})&amp;i=1,3,...,d_{emb-1}\end{matrix}\right.$
    |  | (31) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $pe_{i}\in\left\{\begin{matrix}sin(\frac{pos}{10000^{\frac{2i}{d_{emb}}}})&amp;i=0,2,...,d_{emb}\\
    cos(\frac{pos}{10000^{\frac{2i}{d_{emb}}}})&amp;i=1,3,...,d_{emb-1}\end{matrix}\right.$
    |  | (31) |'
- en: where $d_{emb}=512$ is the word embedding dimension. $T$ is the matrix of positions
    encoders. $PE_{i}\in\mathbb{R}^{1\times d_{emb}}$ is a position encoder vector
    for a word, pos is the position of the word in the sequence, i $\in[0,255]$, and
    refers to the dimension value.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{emb}=512$ 是词嵌入维度。$T$ 是位置编码器的矩阵。$PE_{i}\in\mathbb{R}^{1\times d_{emb}}$
    是一个词的位置编码向量，pos 是词在序列中的位置，i $\in[0,255]$，并表示维度值。
- en: '![Refer to caption](img/ad386963f8e86b99f5246c752e7fced2.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ad386963f8e86b99f5246c752e7fced2.png)'
- en: 'Figure 12: The Neural Transformer [[18](#bib.bib18)] architecture illustration.
    The attention system has several parallel heads. Each head has $N$ attention subsystems
    that perform the same task but have different contextual inputs. The encoder is
    purely bottom-up stateful attention. Each query communicates each stimulus in
    focus target with the other. The decoder is hybrid, in which the first system
    is bottom-up and the second is top-down.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 神经 Transformer [[18](#bib.bib18)] 架构示意图。注意力系统具有多个并行的头部。每个头部有 $N$ 个注意力子系统，这些子系统执行相同的任务，但具有不同的上下文输入。编码器是纯粹的自下而上的有状态注意力。每个查询在专注目标中与其他刺激进行通信。解码器是混合型的，其中第一个系统是自下而上的，第二个系统是自上而下的。'
- en: The input $I$ goes through linear layers and generates, for each word, a query
    vector ($q_{i}$), a key vector ($k_{i}$), and a value vector ($v_{i}$), as follows
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 $I$ 经过线性层生成每个词的查询向量 ($q_{i}$)、键向量 ($k_{i}$) 和值向量 ($v_{i}$)，如下所示
- en: '|  | $Q_{i}=X\times W^{Q}_{i},Q_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (32)
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q_{i}=X\times W^{Q}_{i},Q_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (32)
    |'
- en: '|  | $K_{i}=X\times W^{K}_{i},K_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (33)
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{i}=X\times W^{K}_{i},K_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (33)
    |'
- en: '|  | $V_{i}=X\times W^{V}_{i},V_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (34)
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i}=X\times W^{V}_{i},V_{i}\in\mathbb{R}^{N\times d_{k}}$ |  | (34)
    |'
- en: where i is the head index, $W^{Q}_{i}$, $W^{K}_{i}$, and $W^{V}_{i}$ $\in\mathbb{R}^{d_{emb}\times
    d_{k}}$ are trainable weights. The value $d_{k}$ is $d_{emb}/h=64$, and $h=8$
    is the number of parallel heads. $Q_{i}=\left\{q_{1},q_{2},...,q_{N}\right\},q_{i}\in\mathbb{R}^{1\times
    d_{k}}$ is the queries matrix, $K_{i}=\left\{k_{1},k_{2},...,k_{N}\right\},k_{i}\in\mathbb{R}^{1\times
    d_{k}}$ is the keys matrix, and $V_{i}=\left\{v_{1},v_{2},...,v_{N}\right\},v_{i}\in\mathbb{R}^{1\times
    d_{k}}$ is the values matrix.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 i 是头索引，$W^{Q}_{i}$、$W^{K}_{i}$ 和 $W^{V}_{i}$ $\in\mathbb{R}^{d_{emb}\times
    d_{k}}$ 是可训练权重。值 $d_{k}$ 是 $d_{emb}/h=64$，而 $h=8$ 是并行头的数量。$Q_{i}=\left\{q_{1},q_{2},...,q_{N}\right\},q_{i}\in\mathbb{R}^{1\times
    d_{k}}$ 是查询矩阵，$K_{i}=\left\{k_{1},k_{2},...,k_{N}\right\},k_{i}\in\mathbb{R}^{1\times
    d_{k}}$ 是键矩阵，而 $V_{i}=\left\{v_{1},v_{2},...,v_{N}\right\},v_{i}\in\mathbb{R}^{1\times
    d_{k}}$ 是值矩阵。
- en: 'The attention system receives as input all $Q$, $K$, and $V$ arrays in several
    parallel attention heads. The multi-head structure explores multiple subspaces,
    getting different projections of the data. Having multiple heads on the Transformer
    is similar to having multiple filters on CNNs. Each head has $N$ attention subsystems
    which receive as input the same focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{k_{1},...,k_{N}\right\}$,
    different contextual inputs $c_{t_{att_{i}}}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{q_{i}\right\}$,
    and outputs an attention mask that relates all keys to specific query $q_{i}$.
    Mathematically, the operations performed by a head (Figure [12](#S5.F12 "Figure
    12 ‣ V-H Neural Transformer ‣ V Neural Attention Models ‣ Neural Attention Models
    in Deep Learning: Survey and Taxonomy")) are represented by matrix multiplication
    between all queries and keys, as follows'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力系统接收所有 $Q$、$K$ 和 $V$ 数组作为多个并行注意力头的输入。多头结构探索多个子空间，获得数据的不同投影。在 Transformer
    上拥有多个头类似于在 CNN 上拥有多个滤波器。每个头有 $N$ 个注意力子系统，这些子系统接收相同的关注目标 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{k_{1},...,k_{N}\right\}$、不同的上下文输入
    $c_{t_{att_{i}}}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1}\right\}=\left\{q_{i}\right\}$，并输出一个注意力掩码，将所有键与特定的查询
    $q_{i}$ 相关联。从数学上讲，一个头执行的操作（见图 [12](#S5.F12 "图 12 ‣ V-H 神经变换器 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：综述与分类")）由所有查询和键之间的矩阵乘法表示，如下所示。
- en: '|  | $S_{i}=softmax(\frac{Q_{i}*K_{i}^{T}}{\sqrt{d_{k}}})$ |  | (35) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{i}=softmax(\frac{Q_{i}*K_{i}^{T}}{\sqrt{d_{k}}})$ |  | (35) |'
- en: where $S_{i}\in\mathbb{R}^{N\times N}$ is self-attention matrix, i is the head
    index, Q $\in\mathbb{R}^{N\times d_{k}}$, $K^{T}$ $\in\mathbb{R}^{d_{k}\times
    N}$. For gradient stability, $d_{k}$ is the normalization factor, and an softmax
    function turns the attention scores into probabilities.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{i}\in\mathbb{R}^{N\times N}$ 是自注意力矩阵，i 是头索引，Q $\in\mathbb{R}^{N\times
    d_{k}}$，$K^{T}$ $\in\mathbb{R}^{d_{k}\times N}$。为了梯度稳定性，$d_{k}$ 是归一化因子，softmax
    函数将注意力分数转换为概率。
- en: 'The similarity between $q_{i}$ and $k_{i}$ is a score of importance that the
    elements of the sequence have among themselves. This structure characterizes the
    most important bottom-up stateful system in the field, in which contexts come
    simultaneously from the discrepancies between the different stimuli of the focus
    target itself. Intuitively, queries act as small units that allow parallel communication
    between all the target’s stimuli. In sequence, other bottom-up stateful attention
    receives as context input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{S_{1},...,S_{h},V_{1},...,V_{h}\right\}$
    all self-attention matrices $S_{i}$ and values $V_{i}$, and as focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{x_{1},...,x_{N}\right\}$
    the original input $I$ composed by $x_{i}\in\mathbb{R}^{d_{emb}}$ vectors. This
    system create final attention mask $a_{t}$ for original encoder input $I$:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: $q_{i}$ 和 $k_{i}$ 之间的相似性是序列元素之间重要性的评分。这一结构表征了该领域中最重要的自下而上的有状态系统，其中上下文同时来自于不同刺激之间的差异。直观地说，查询作为小单元允许所有目标的刺激之间的并行通信。接下来，其他自下而上的有状态注意力将上下文输入
    $c_{t}=\left\{c_{t}^{1}\right\}=\left\{S_{1},...,S_{h},V_{1},...,V_{h}\right\}$
    所有自注意力矩阵 $S_{i}$ 和值 $V_{i}$，以及作为关注目标的 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N}^{1}\right\}=\left\{x_{1},...,x_{N}\right\}$
    原始输入 $I$ 由 $x_{i}\in\mathbb{R}^{d_{emb}}$ 向量组成。该系统为原始编码器输入 $I$ 创建最终的注意力掩码 $a_{t}$：
- en: '|  | $Z_{i}=S_{i}*V_{i}$ |  | (36) |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z_{i}=S_{i}*V_{i}$ |  | (36) |'
- en: '|  | $a_{t}=Concat(Z_{1},Z_{2},...,Z_{h})*W^{O}$ |  | (37) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{t}=Concat(Z_{1},Z_{2},...,Z_{h})*W^{O}$ |  | (37) |'
- en: where $a_{t}=\left\{a_{t,1},...,a_{t,N}\right\}$ $\in\mathbb{R}^{N\times d_{emb}}$
    is a linear combination from each head, and $W^{O}$ $\in\mathbb{R}^{h*d_{k}\times
    d_{emb}}$ are weights learned during training.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{t}=\left\{a_{t,1},...,a_{t,N}\right\}$ $\in\mathbb{R}^{N\times d_{emb}}$
    是每个头的线性组合，$W^{O}$ $\in\mathbb{R}^{h*d_{k}\times d_{emb}}$ 是在训练过程中学习到的权重。
- en: 'The mask $a_{t}$ modulates focus target $I$ through simple sum and layer-normalization
    [[101](#bib.bib101)] step, resulting updated $I=\left\{x_{1},...,x_{N}\right\}$.
    After, each updated $x_{i}$ goes through feedfoward neural network composed by
    linear transformations and ReLU activation function. Finally, residual input $I$,
    and feedfoward results are input for last layer-normalization step:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码 $a_{t}$ 通过简单的求和和层归一化 [[101](#bib.bib101)] 步骤调节关注目标 $I$，结果是更新后的 $I=\left\{x_{1},...,x_{N}\right\}$。然后，每个更新后的
    $x_{i}$ 通过由线性变换和 ReLU 激活函数组成的前馈神经网络。最后，残差输入 $I$ 和前馈结果作为最后层归一化步骤的输入：
- en: '|  | $I=LayerNorm(I+a_{t})$ |  | (38) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | $I=LayerNorm(I+a_{t})$ |  | (38) |'
- en: '|  | $f_{i}=FFN(x_{i})=max(0,x_{i}W_{1}+b_{1})W_{2}+b_{2}$ |  | (39) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{i}=FFN(x_{i})=max(0,x_{i}W_{1}+b_{1})W_{2}+b_{2}$ |  | (39) |'
- en: '|  | $I=LayerNorm(I+F)$ |  | (40) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | $I=LayerNorm(I+F)$ |  | (40) |'
- en: where the hidden layer has a dimensionality $d_{ff}=2048$, and $F$ $\in\mathbb{R}^{N\times
    d_{emb}}$ is feedfoward matrix, and updated $I$ $\in\mathbb{R}^{N\times d_{emb}}$
    is output encoder.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 其中隐藏层的维度为 $d_{ff}=2048$，$F$ $\in\mathbb{R}^{N\times d_{emb}}$ 是前馈矩阵，更新后的 $I$
    $\in\mathbb{R}^{N\times d_{emb}}$ 是输出编码器。
- en: 'The last encoder output are transformed into the contextual inputs $K_{encdec}$,
    $V_{encdec}$ for new attention systems in decoder levels. This data can also be
    understood as short-term memories that help the decoder focus on the input sequences’
    appropriate information. All decoders are similar to a single encoder structure,
    but with some important differences: 1) Step-by-step processing; and 2) Two attention
    systems (Figure [12](#S5.F12 "Figure 12 ‣ V-H Neural Transformer ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")). The
    first has masked multi-head structure to mask future translate words with $-\infty$
    values. The second receives as focus target the previously translated word embeddings
    and as contextual input $K_{encdec}$, $V_{encdec}$ and queries from the layer
    below it. This system operates as top-down stateful attention, given keys/values
    and queries/target from different sources. This structure acts as oriented attention,
    given change the focus target e attention masks at each time step $t$. After all
    attention systems, a linear layer followed by the softmax function transform the
    last decoder output stack into a probability vector to predict the correct word.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的编码器输出被转换为解码器层中新注意力系统的上下文输入 $K_{encdec}$ 和 $V_{encdec}$。这些数据也可以理解为短期记忆，帮助解码器集中在输入序列的相关信息上。所有解码器结构与单个编码器结构类似，但有一些重要的区别：1)
    逐步处理；2) 两个注意力系统（图 [12](#S5.F12 "图 12 ‣ V-H 神经网络变换器 ‣ V 神经注意力模型 ‣ 深度学习中的神经注意力模型：调查与分类")）。第一个系统具有掩码多头结构，用
    $-\infty$ 值掩盖未来的翻译词。第二个系统将先前翻译的词嵌入作为关注目标，同时接收 $K_{encdec}$、$V_{encdec}$ 作为上下文输入，并从下层接收查询。该系统作为自上而下的有状态注意力运行，根据不同来源提供的键/值和查询/目标。这个结构作为定向注意力，随着每个时间步
    $t$ 改变关注目标和注意力掩码。在所有注意力系统之后，一个线性层和 softmax 函数将最后的解码器输出堆栈转换为概率向量，以预测正确的词。
- en: Neural Transformer represents the primary end-to-end attention approach. Communication
    between bottom-up and top-down structures makes it the first hybrid attention
    model. The simultaneously location-based and feature-based attention is also innovative,
    given the most models are only location-based. In the first stage of attention,
    parallel heads are location-based, while the second stage, composed of a single
    attention subsystem, is feature-based. Besides, the sequence of multiple stacked
    attention modules eliminates the need for recurrence in tasks where RNNs are typically
    used.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 神经变换器代表了主要的端到端注意力方法。底向上和自上而下结构之间的通信使其成为第一个混合注意力模型。与大多数仅基于位置的模型不同，同时具备位置和特征基础的注意力也具有创新性。在注意力的第一阶段，平行头是基于位置的，而第二阶段由单个注意力子系统组成，基于特征。此外，多层堆叠的注意力模块序列消除了在
    RNN 通常用于的任务中对递归的需求。
- en: V-I Graph Attention Networks (GATs)
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-I 图注意力网络（GATs）
- en: 'GATs [[20](#bib.bib20)] uses attention to operate on graph-structured data.
    The architecture consists of stacking several graph attentional layers (GAL) (Figure
    [13](#S5.F13 "Figure 13 ‣ V-I Graph Attention Networks (GATs) ‣ V Neural Attention
    Models ‣ Neural Attention Models in Deep Learning: Survey and Taxonomy")). The
    GAL input is the nodes’ features $h=\left\{\overrightarrow{h_{1}},\overrightarrow{h_{2}},\overrightarrow{h_{3}},...,\overrightarrow{h_{N_{i}}}\right\}$,
    where $\overrightarrow{h_{i}}\in\mathbb{R}^{1\times F}$, $N$ is the number of
    nodes, and $F$ is the number of features in each node. The outputs are a new set
    of features $h^{{}^{\prime}}=\left\{\overrightarrow{h_{1}^{{}^{\prime}}},\overrightarrow{h_{2}^{{}^{\prime}}},\overrightarrow{h_{3}^{{}^{\prime}}},...,\overrightarrow{h_{N}^{{}^{\prime}}}\right\}$
    with a potentially different cardinality $F^{{}^{\prime}}$. This embeddings are
    combination of small bottom-up subsystems. First, $N_{h}$ heads with $N_{i}$ bottom-up
    stateless subsystems operating in parallel to modulate all stimulus’ features
    based on neighborhood. In sequence, $N_{h}$ bottom-up stateful subsystems transform
    the heads outputs into the attention masks for $h$.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 'GATs [[20](#bib.bib20)] 使用注意力机制处理图结构数据。其架构包括堆叠多个图注意力层（GAL）（见图 [13](#S5.F13
    "Figure 13 ‣ V-I Graph Attention Networks (GATs) ‣ V Neural Attention Models ‣
    Neural Attention Models in Deep Learning: Survey and Taxonomy")）。GAL 的输入是节点特征
    $h=\left\{\overrightarrow{h_{1}},\overrightarrow{h_{2}},\overrightarrow{h_{3}},...,\overrightarrow{h_{N_{i}}}\right\}$，其中
    $\overrightarrow{h_{i}}\in\mathbb{R}^{1\times F}$，$N$ 是节点数量，$F$ 是每个节点的特征数量。输出是一个新的特征集
    $h^{{}^{\prime}}=\left\{\overrightarrow{h_{1}^{{}^{\prime}}},\overrightarrow{h_{2}^{{}^{\prime}}},\overrightarrow{h_{3}^{{}^{\prime}}},...,\overrightarrow{h_{N}^{{}^{\prime}}}\right\}$，具有可能不同的基数
    $F^{{}^{\prime}}$。这些嵌入是由多个自下而上的小型子系统组合而成。首先，$N_{h}$ 个头与 $N_{i}$ 个自下而上的无状态子系统并行工作，根据邻域调整所有刺激的特征。接下来，$N_{h}$
    个自下而上的有状态子系统将头部输出转换为 $h$ 的注意力掩码。'
- en: '![Refer to caption](img/ec5ce69fe68782871fc5723373b72b23.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec5ce69fe68782871fc5723373b72b23.png)'
- en: 'Figure 13: Graph Attentional Layers (GAL) [[20](#bib.bib20)] architecture illustration.
    The bottom-up and feature-based attention captures the discrepancies between the
    focus target features and generates embedding vectors for each node as output.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：图注意力层（GAL） [[20](#bib.bib20)] 架构示意图。自下而上的特征注意力捕捉焦点目标特征之间的差异，并为每个节点生成嵌入向量作为输出。
- en: 'Each bottom-up stateless subsystem takes as input a focus target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},\tau_{t,2}^{1}\right\}=\left\{W\overrightarrow{h_{i}},W\overrightarrow{h_{j}}\right\}$,
    $\tau_{t}\in\mathbb{R}^{2F}$. An alignment function $a$ generates the attentional
    weights for each input feature, ie, $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},a_{t,2}^{1}\right\}=\left\{a^{T}\right\}$,
    where $a_{t}\in\mathbb{R}^{2F}$, and $a$ is a single-layer feedforward neural
    network, parametrized by a weight vector $a^{T}$. Each subsystem has the same
    selection characteristics – divided, cognitive, feature-based, and soft. The result
    of attention mask over $\tau_{t}$ is contextual input $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},...,c_{t,N_{i}}^{1}\right\}$
    for bottom-up stateful attention subsystems:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 每个自下而上的无状态子系统以焦点目标 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},\tau_{t,2}^{1}\right\}=\left\{W\overrightarrow{h_{i}},W\overrightarrow{h_{j}}\right\}$
    作为输入，$\tau_{t}\in\mathbb{R}^{2F}$。对齐函数 $a$ 生成每个输入特征的注意力权重，即 $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},a_{t,2}^{1}\right\}=\left\{a^{T}\right\}$，其中
    $a_{t}\in\mathbb{R}^{2F}$，$a$ 是一个由权重向量 $a^{T}$ 参数化的单层前馈神经网络。每个子系统具有相同的选择特性——分割的、认知的、基于特征的和软的。对
    $\tau_{t}$ 的注意力掩码的结果是自下而上的有状态注意力子系统的上下文输入 $c_{t}=\left\{c_{t}^{1}\right\}=\left\{c_{t,1}^{1},...,c_{t,N_{i}}^{1}\right\}$：
- en: '|  | $c_{t,j}^{1}=\overrightarrow{a^{T}}[W\overrightarrow{h_{i}}&#124;&#124;W\overrightarrow{h_{j}}]$
    |  | (41) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{t,j}^{1}=\overrightarrow{a^{T}}[W\overrightarrow{h_{i}}&#124;&#124;W\overrightarrow{h_{j}}]$
    |  | (41) |'
- en: where $.^{T}$ represents transposition and $\left|\right|$ is the concatenation
    operation.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $.^{T}$ 表示转置，而 $\left|\right|$ 是连接操作。
- en: The bottom-up stateful attention subsystems based on heads output, and focus
    target $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{i}}^{1}\right\}=\left\{W\overrightarrow{h_{i}},W\overrightarrow{h_{N_{i}}}\right\}$
    generates attention mask $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N_{i}}^{1}\right\}=\left\{a_{i,1},...,a_{i,N_{i}}\right\}$,
    where $a_{t}\in\mathbb{R}^{N_{i}}$. These subsystems apply, rectification and
    normalization layers over $c_{t}$ to produce divided, cognitive, location-based,
    and soft selection over all nodes. This subsystems’ objective is to generate a
    embedding for each node $i$ resulting from a combination of the all neighborhood,
    as follows
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 基于头部输出的自下而上的有状态注意力子系统，和焦点目标 $\tau_{t}=\left\{\tau_{t}^{1}\right\}=\left\{\tau_{t,1}^{1},...,\tau_{t,N_{i}}^{1}\right\}=\left\{W\overrightarrow{h_{i}},W\overrightarrow{h_{N_{i}}}\right\}$
    生成注意力掩码 $a_{t}=\left\{a_{t}^{1}\right\}=\left\{a_{t,1}^{1},...,a_{t,N_{i}}^{1}\right\}=\left\{a_{i,1},...,a_{i,N_{i}}\right\}$，其中
    $a_{t}\in\mathbb{R}^{N_{i}}$。这些子系统对 $c_{t}$ 应用修正和归一化层，以产生对所有节点的划分、认知、基于位置的和软选择。这些子系统的目标是为每个节点
    $i$ 生成一个嵌入，这个嵌入是所有邻域的组合结果，如下所示
- en: '|  | $\alpha_{i,j}=LeakyReLU(c_{t})$ |  | (42) |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{i,j}=LeakyReLU(c_{t})$ |  | (42) |'
- en: '|  | $a_{i,j}=\frac{\alpha_{i,j}}{\sum_{k\in N_{i}}\alpha_{i,j}}$ |  | (43)
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $a_{i,j}=\frac{\alpha_{i,j}}{\sum_{k\in N_{i}}\alpha_{i,j}}$ |  | (43)
    |'
- en: '|  | $\overrightarrow{h_{i}^{{}^{\prime}}}=\sigma(\frac{1}{N_{h}}\sum_{k=1}^{N_{h}}\sum_{j\in
    N_{i}}a_{i,j}^{k}W^{k}\overrightarrow{h_{j}})$ |  | (44) |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overrightarrow{h_{i}^{{}^{\prime}}}=\sigma(\frac{1}{N_{h}}\sum_{k=1}^{N_{h}}\sum_{j\in
    N_{i}}a_{i,j}^{k}W^{k}\overrightarrow{h_{j}})$ |  | (44) |'
- en: where $N_{h}$ is amount of heads, $N_{i}$ is the set of nodes belonging to a
    certain neighborhood of the $i$ node, and LeakyReLU is non-linear function with
    negative input slope $\alpha=0.2$. $\overrightarrow{h_{i}^{{}^{\prime}}}\in\mathbb{R}^{F^{{}^{\prime}}}$
    is the output from the GAT layer to node $i$.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{h}$ 是头部的数量，$N_{i}$ 是属于 $i$ 节点某个特定邻域的节点集合，LeakyReLU 是具有负输入斜率 $\alpha=0.2$
    的非线性函数。$\overrightarrow{h_{i}^{{}^{\prime}}}\in\mathbb{R}^{F^{{}^{\prime}}}$ 是来自
    GAT 层到节点 $i$ 的输出。
- en: VI Discussion
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论
- en: In this section, we discuss outstanding issues with attention models around
    some main topics.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了注意力模型中的一些主要问题。
- en: VI-A Perception, multimodality, and sensors sharing
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 感知、多模态和传感器共享
- en: Historically, several aspects and characteristics of attention have been mapped
    by observing external stimuli’ selection, resulting in a wide range of theories
    to address attention’s perceptual selection. Some researchers consider attention
    as a fundamental process for the existence of perception. In classical theories,
    sensory memories store information about the input stimuli for a period.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，通过观察外部刺激的选择，已经对注意力的几个方面和特征进行了映射，形成了各种理论来解决注意力的感知选择问题。一些研究者认为注意力是感知存在的基本过程。在经典理论中，感觉记忆在一段时间内存储有关输入刺激的信息。
- en: The stimuli have activation and inhibition dynamics within the system. Such
    dynamics are entirely dependent on exposure to the same input stimulus (i.e.,
    inhibition and return), and the construction of the internal representation of
    the world occurs sequentially through the exploration of stimuli that fire with
    greater intensity, guided by top-down and bottom-up influences. However, few attentional
    systems in Deep Learning have been explored from this perspective. In CNNs, the
    main focus of the systems is feature maps instead of the raw input image. In RNNs
    and generative models, the models are mainly cognitive selection over previous
    memories. Usually, the few existing perceptual selection systems present all the
    input stimuli to the network. In parallel, computation is made over the data in
    one pass, without the sequential dynamics and without considering the same stimulus’s
    activation degree depending on the time exposure.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 刺激在系统内具有激活和抑制动态。这些动态完全依赖于对相同输入刺激的暴露（即，抑制和返回），并且对世界的内部表征的构建是通过探索那些以更大强度触发的刺激逐步完成的，受到自上而下和自下而上的影响。然而，从这个角度探讨深度学习中的注意力系统的研究仍然较少。在卷积神经网络（CNN）中，系统的主要关注点是特征图，而不是原始输入图像。在递归神经网络（RNN）和生成模型中，模型主要是对之前的记忆进行认知选择。通常，现有的感知选择系统将所有输入刺激呈现给网络。同时，计算是在一次传递中完成的，没有序列动态，也没有考虑相同刺激的激活程度随时间暴露的变化。
- en: Similarly, attentional systems also explore few aspects of multimodality. Multiple
    inter and intra modal attentional stages for sensory fusion are still poorly explored.
    Typically, most of the current systems relate two different modalities without
    sensory fusion. One modality is only supported for the selection of stimuli of
    the other already in latent space. However, classical theories indicate that many
    areas of the brain share the processing of information from different senses.
    Some evidence indicates that large parts of the visual cortex are multisensory,
    with different fusion stages ranging from perception to higher levels of processing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，注意力系统也探讨了多模态的若干方面。用于感官融合的多个跨模态和内模态注意力阶段仍然探索不足。通常，现有的大多数系统在没有感官融合的情况下将两种不同的模态关联起来。一种模态仅用于选择另一种模态中的刺激，这些刺激已经存在于潜在空间中。然而，经典理论指出，大脑的许多区域共享来自不同感觉的信息处理。一些证据表明，视觉皮层的大部分是多感官的，不同的融合阶段从感知到更高水平的处理都有涉及。
- en: VI-B Attention over data
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 数据上的注意力
- en: Most attentional systems occur over the data that flows through neural networks.
    Systems are almost exclusively location-based, disregarding features and object
    properties. However, this approach does not fully corroborate theories and cognitive
    models of the 90s. The theory of integration of characteristics by Treisman [[102](#bib.bib102)]
    points to the initial stages of perception as purely feature-based, to the point
    where the merging of the different features occurs, and a WTA process determines
    the location of the most prominent stimulus by inhibiting the missing stimuli.
    Backer et al. [[103](#bib.bib103)] presented a computational model in three stages,
    the first feature-based stage, the second location normally detecting four protruding
    regions, and finally, an object-based inhibition of return. Besides, there is
    strong experimental evidence showing that these properties are not multi-exclusive,
    present mainly in the human view [[104](#bib.bib104)]. We believe that attentional
    systems with all three aspects can greatly benefit computer vision applications.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数注意力系统发生在流经神经网络的数据上。这些系统几乎完全基于位置，忽略了特征和对象属性。然而，这种方法并未完全验证90年代的理论和认知模型。特雷斯曼（Treisman）的特征整合理论[[102](#bib.bib102)]指出，感知的初始阶段完全基于特征，直到不同特征的融合发生，一个WTA过程通过抑制缺失的刺激来确定最显著刺激的位置。Backer等人[[103](#bib.bib103)]提出了一个三阶段的计算模型，第一阶段是基于特征的，第二阶段通常检测四个突出的区域，最后是基于对象的回避抑制。此外，有强有力的实验证据表明，这些属性并非多重排斥，主要存在于人类视野中[[104](#bib.bib104)]。我们认为，具备所有三个方面的注意力系统可以大大有利于计算机视觉应用。
- en: VI-C Attention over program
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 程序上的注意力
- en: Most attentional systems are oriented towards data selection. So far, task-oriented
    or time-oriented systems do not exist. Task-oriented structures are important
    to increase the modularity of classical neural networks and facilitate interpretability,
    given that the behavior of the modules can be indirectly investigated through
    the decisions made by the attentional system. Similarly, time-oriented structures
    are important for the consistent distribution of computation times between different
    structures. Such a structure is particularly interesting for deciding between
    the exposure time of stimuli in perceptual stages.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数注意力系统面向数据选择。到目前为止，尚不存在面向任务或时间的系统。面向任务的结构对增加经典神经网络的模块化和提高可解释性非常重要，因为可以通过注意力系统做出的决策间接调查模块的行为。同样，面向时间的结构对在不同结构之间一致分配计算时间也很重要。这种结构对于在感知阶段决定刺激暴露时间特别有趣。
- en: 'VI-D Dual-Process Theory: Connectionism versus Symbolism'
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 双重过程理论：联结主义与符号主义
- en: 'According to the dual-process theory [[105](#bib.bib105)], human reasoning
    has two distinct systems. The first is unconscious, fast, intuitive, automatic,
    non-linguistic, and deals only with implicit knowledge. The second is conscious,
    evolutionarily recent, exclusive to humans, slow, conscious, linguistic, algorithmic,
    incorporating rules-based reasoning and explicit knowledge forms. Semantic variables
    in conscious thinking are often causal, controllable, and relate to thoughts so
    that concepts can be recombined to form new and unknown concepts. For years, in
    AI, these two types of reasoning have been studied separately, creating two distinct
    research branches: connectionist models represented mainly by neural networks
    and symbolic models represented by highly declarative and recursive algorithms.
    Although different, these two systems are complementary in complex reasoning.
    Currently, the fusion between the symbolic and the connectionist (i.e., neuro-symbolic
    approaches) is one of the main AI challenges for the coming years. In this context,
    attention is a key element as an interface between the two types of representation.
    One of the main issues to be solved by attention is how to link two systems with
    such different representations of knowledge. Few systems have acted in this perspective.
    Some initiatives have emerged through external memory networks [[89](#bib.bib89)]
    [[37](#bib.bib37)], in RL [[48](#bib.bib48)] and recently, in Hypergraph Attention
    Networks [[67](#bib.bib67)], in which attentional systems reason about symbolic
    graphs in a same semantic space and build representative embeddings for a connectionist
    model to make the prediction.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 根据双过程理论[[105](#bib.bib105)]，人类推理有两个不同的系统。第一个系统是无意识的、快速的、直观的、自动的、非语言的，只处理隐性知识。第二个系统是有意识的、进化上较新的、仅人类特有的、缓慢的、有意识的、语言的、算法性的，包含基于规则的推理和显性知识形式。在有意识的思维中，语义变量通常是因果的、可控的，并与思想相关，以便概念可以重新组合形成新的和未知的概念。多年来，在人工智能领域，这两种推理类型一直被分开研究，形成了两个不同的研究分支：主要由神经网络表示的连接主义模型和由高度声明性和递归算法表示的符号模型。尽管不同，这两种系统在复杂推理中是互补的。目前，符号模型和连接主义模型（即神经符号方法）之间的融合是未来几年人工智能的主要挑战之一。在这个背景下，注意力是两种表示之间的关键接口。注意力需要解决的主要问题之一是如何将这两种具有如此不同知识表示的系统连接起来。只有少数系统从这个角度出发进行研究。一些举措通过外部记忆网络[[89](#bib.bib89)]
    [[37](#bib.bib37)]、强化学习[[48](#bib.bib48)]以及最近的超图注意力网络[[67](#bib.bib67)]中出现，其中注意力系统在同一语义空间中对符号图进行推理，并构建代表性嵌入，以便连接主义模型进行预测。
- en: VI-E Human Memories
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 人类记忆
- en: One of the main questions of connectionist models is how to represent, organize
    and manage knowledge external to the prediction structure to guarantee reusability
    of the knowledge acquired previously. In psychology and neuroscience, the relationship
    between attention and memory is intrinsic and fundamental to our cognitive system.
    However, in Deep Learning, most attentional systems focus on selecting and representing
    data from the current input. Few systems are directed to external memories or
    internal storage structures with specific characteristics. Research, still in
    its initial stages, focuses on the management of working memories [[37](#bib.bib37)]
    or episodic [[106](#bib.bib106)], with the absence of sensory, motor, procedural,
    and semantic memories.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 连接主义模型的主要问题之一是如何表示、组织和管理预测结构外的知识，以确保之前获得的知识可以重用。在心理学和神经科学中，注意力和记忆之间的关系是我们认知系统内在且基础的。然而，在深度学习中，大多数注意力系统专注于从当前输入中选择和表示数据。很少有系统针对具有特定特征的外部记忆或内部存储结构。研究仍处于初期阶段，主要关注工作记忆[[37](#bib.bib37)]或情景记忆[[106](#bib.bib106)]，而感官、运动、程序性和语义记忆则缺乏。
- en: VI-F Hybrid Models
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-F 混合模型
- en: Most attentional systems receive only top-down influences, coming from memories
    of the previous internal states of neural networks. High-level contextual elements
    such as rewards, motivation, emotional and sensory status are still absent. Besides,
    few systems have bottom-up influences, although there are numerous theories and
    models discussed in the classical literature, mainly focused on sensory perception.
    We believe that bottom-up attention can bring significant learning benefits in
    the early stages of perception or unsupervised/self-supervised approaches. Likewise,
    the area still lacks hybrid neural attention models that address both levels of
    attention. Only Neural Transformer [[18](#bib.bib18)], and recently BRIMs [[72](#bib.bib72)]
    have brought some significant insights to the area. However, a major research
    question still little explored is how to iterate between the two levels of attention
    and how to associate the two modulations in attentional processes.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数注意力系统仅接收来自神经网络先前内部状态的自上而下的影响。高层次的上下文元素，如奖励、动机、情感和感觉状态仍然缺失。此外，尽管经典文献中讨论了许多理论和模型，主要集中于感官知觉，但几乎没有系统具有自下而上的影响。我们认为，自下而上的注意力在感知早期阶段或无监督/自监督方法中可以带来显著的学习收益。同样，该领域仍缺乏同时处理两个注意力层次的混合神经注意力模型。只有神经变换器
    [[18](#bib.bib18)] 和最近的 BRIMs [[72](#bib.bib72)] 为该领域带来了一些重要见解。然而，一个尚未充分探索的主要研究问题是如何在两个注意力层次之间进行迭代，以及如何将这两种调节关联到注意力过程中的问题。
- en: According to some neurophysiological evidence these mechanisms are concentrated
    in different brain areas, but they interact with each other constantly and are
    simultaneously present from the initial stages of perception. At some stages,
    the bottom-up mechanisms can completely overlap top-down influences in a competitive
    process, or both can be aligned, and their effects are combined in some way. For
    example, when reading this article, if your senses notice an imminent danger,
    you will stop reading for safety, where bottom-up influences have completely suppressed
    top-down influences. On the other hand, you may be invited to search for a specific
    target in an image, and discrepant features not related to the target can interfere
    with the visual search process but do not fully influence the point of stopping
    the search for the desired target. In contrast, there is still little iteration
    between these mechanisms in neural networks so that they act separately at different
    time stages. At Neural Transformer, for example, only bottom-up attention acts
    on perception in the first stage of time. The perceptual stage is over, and the
    top-down attention helps generate the translated words, with still weak iterations
    between the two mechanisms.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一些神经生理学证据，这些机制集中在不同的脑区，但它们彼此间不断互动，并在感知的初始阶段同时存在。在某些阶段，自下而上的机制可以在竞争过程中完全覆盖自上而下的影响，或者两者可以对齐，并以某种方式结合其效果。例如，当你阅读本文时，如果你的感官注意到即将发生的危险，你会为了安全停止阅读，此时自下而上的影响完全抑制了自上而下的影响。另一方面，你可能会被要求在图像中寻找特定目标，而与目标无关的差异特征可能会干扰视觉搜索过程，但不会完全影响停止寻找所需目标的点。相比之下，在神经网络中，这些机制之间的迭代仍然很少，以至于它们在不同的时间阶段独立作用。例如，在神经变换器中，仅在时间的第一阶段，自下而上的注意力作用于感知阶段。感知阶段结束后，自上而下的注意力有助于生成翻译后的词语，两个机制之间的迭代仍然很弱。
- en: VII Conclusions
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: In this survey, we presented a review of attention in Deep Learning from a theoretical
    point of view. We propose a general framework of attention and taxonomy based
    on theoretical concepts that date back to the pre-Deep Learning era. From a set
    of more than 650 papers critically analyzed via systematic review, we identified
    the main neural attention models in the literature, discussed, and classified
    their main aspects using the different perspectives of attention presented in
    our taxonomy. We have identified models that corroborate classical theories and
    biological evidence discussed for years by psychologists and neuroscientists.
    Finally, we present the key developments in the area in detail, formulating and
    explaining their attention systems from our framework and taxonomy perspective.
    Finally, we present a critical discussion of the models analyzed, presenting some
    relevant research opportunities. We hope that this survey will provide a better
    understanding of how attentional mechanisms work in neural networks from the theoretical
    point of view of care and that our taxonomy will help systematize the mechanisms
    facilitating their understanding and provide a better understanding of different
    directions of attention helping to guide the future development of the area.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们从理论角度对深度学习中的注意力进行了综述。我们提出了一个基于理论概念的一般注意力框架和分类法，这些概念可以追溯到深度学习之前的时代。通过系统评审分析了650多篇论文，我们识别了文献中的主要神经注意力模型，讨论并使用我们分类法中的不同注意力视角对其主要方面进行了分类。我们发现了一些支持经典理论和生物学证据的模型，这些理论和证据多年来被心理学家和神经科学家讨论。最后，我们详细介绍了该领域的关键进展，从我们的框架和分类法视角对它们的注意力系统进行了解释。最后，我们对分析的模型进行了批判性讨论，提出了一些相关的研究机会。我们希望这项调查能提供对神经网络中注意机制如何运作的更好理解，并且我们的分类法将有助于系统化机制，促进对不同注意力方向的理解，指导该领域未来的发展。
- en: Acknowledgments
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The research was supported by the Coordination for the Improvement of Higher
    Education Personnel (CAPES). This work was carried out within the scope of PPI-Softex
    with support from the MCTI, through the Technical Cooperation Agreement [01245.013778/2020-21].
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究得到了协调提高高等教育人员（CAPES）的支持。此项工作在PPI-Softex的范围内进行，并获得了MCTI的支持，通过技术合作协议[01245.013778/2020-21]。
- en: References
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] E. L. Colombini, A. da Silva Simoes, and C. Ribeiro, “An attentional model
    for intelligent robotics agents.” Ph.D. dissertation, Instituto Tecnológico de
    Aeronáutica, São José dos Campos, Brazil, 2014.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] E. L. Colombini, A. da Silva Simoes 和 C. Ribeiro, “智能机器人代理的注意力模型。” 博士学位论文，航空技术学院，圣若泽·杜斯坎普斯，巴西，2014年。'
- en: '[2] W. James, *The Principles of Psychology*.   Dover Publications, 1890.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] W. James, *心理学原理*。多佛出版公司，1890年。'
- en: '[3] N. H. Mackworth, “The breakdown of vigilance during prolonged visual search,”
    *Quarterly Journal of Experimental Psychology*, vol. 1, no. 1, pp. 6–21, 1948.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. H. Mackworth, “在长期视觉搜索中警觉性的崩溃，” *实验心理学季刊*，第1卷，第1期，第6–21页，1948年。'
- en: '[4] D. J. Simons and C. F. Chabris, “Gorillas in our midst: Sustained inattentional
    blindness for dynamic events,” *perception*, vol. 28, no. 9, pp. 1059–1074, 1999.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. J. Simons 和 C. F. Chabris, “我们之间的猩猩：对动态事件的持续性忽视盲点，” *知觉*，第28卷，第9期，第1059–1074页，1999年。'
- en: '[5] J. E. Raymond, K. L. Shapiro, and K. M. Arnell, “Temporary suppression
    of visual processing in an rsvp task: An attentional blink?” *Journal of experimental
    psychology: Human perception and performance*, vol. 18, no. 3, p. 849, 1992.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. E. Raymond, K. L. Shapiro 和 K. M. Arnell, “在RSVP任务中的视觉处理暂时抑制：注意力闪烁？”
    *实验心理学杂志：人类感知与表现*，第18卷，第3期，第849页，1992年。'
- en: '[6] A. F. Sanders, *Elements of human performance: Reaction processes and attention
    in human skill*.   Psychology Press, 1998.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. F. Sanders, *人类表现要素：反应过程和人类技能中的注意力*。心理学出版社，1998年。'
- en: '[7] D. E. Broadbent, *Perception and communication*.   Elsevier, 2013.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. E. Broadbent, *知觉与沟通*。爱思唯尔，2013年。'
- en: '[8] J. Driver and R. S. Frackowiak, “Neurobiological measures of human selective
    attention,” *Neuropsychologia*, vol. 39, no. 12, pp. 1257–1262, 2001.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Driver 和 R. S. Frackowiak, “人类选择性注意的神经生物学测量，” *神经心理学*，第39卷，第12期，第1257–1262页，2001年。'
- en: '[9] S. Treue, “Neural correlates of attention in primate visual cortex,” *Trends
    in neurosciences*, vol. 24, no. 5, pp. 295–300, 2001.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Treue, “灵长类动物视觉皮层中的注意力神经相关性，” *神经科学趋势*，第24卷，第5期，第295–300页，2001年。'
- en: '[10] E. T. Rolls and G. Deco, “Attention in natural scenes: neurophysiological
    and computational bases,” *Neural networks*, vol. 19, no. 9, pp. 1383–1394, 2006.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] E. T. Rolls 和 G. Deco，《自然场景中的注意力：神经生理学和计算基础》，*Neural networks*，第19卷，第9期，页码1383–1394，2006年。'
- en: '[11] A. A. Salah, E. Alpaydin, and L. Akarun, “A selective attention-based
    method for visual pattern recognition with application to handwritten digit recognition
    and face recognition,” *IEEE PAMI*, vol. 24, no. 3, pp. 420–425, 2002.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. A. Salah, E. Alpaydin, 和 L. Akarun，《基于选择性注意的方法用于视觉模式识别及其在手写数字识别和面部识别中的应用》，*IEEE
    PAMI*，第24卷，第3期，页码420–425，2002年。'
- en: '[12] N. Ouerhani, “Visual attention: from bio-inspired modeling to real-time
    implementation,” Ph.D. dissertation, Université de Neuchâtel, 2003.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Ouerhani，《视觉注意力：从生物启发建模到实时实现》，大学博士学位论文，Université de Neuchâtel，2003年。'
- en: '[13] D. Walther, “Interactions of visual attention and object recognition:
    computational modeling, algorithms, and psychophysics,” Ph.D. dissertation, California
    Institute of Technology, 2006.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. Walther，《视觉注意力与物体识别的交互：计算建模、算法与心理物理学》，加州理工学院博士学位论文，2006年。'
- en: '[14] J. J. Clark and N. J. Ferrier, “Modal control of an attentive vision system,”
    in *IEEE ICCV*.   IEEE, 1988, pp. 514–523.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. J. Clark 和 N. J. Ferrier，《注意视觉系统的模态控制》，*IEEE ICCV*，IEEE，1988年，页码514–523。'
- en: '[15] S. Baluja and D. A. Pomerleau, “Expectation-based selective attention
    for visual monitoring and control of a robot vehicle,” *Robotics and autonomous
    systems*, vol. 22, no. 3-4, pp. 329–344, 1997.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. Baluja 和 D. A. Pomerleau，《基于期望的选择性注意用于视觉监控和机器人控制》，*Robotics and autonomous
    systems*，第22卷，第3-4期，页码329–344，1997年。'
- en: '[16] S. Frintrop and P. Jensfelt, “Attentional landmarks and active gaze control
    for visual slam,” *IEEE Transactions on Robotics*, vol. 24, no. 5, pp. 1054–1065,
    2008.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Frintrop 和 P. Jensfelt，《视觉SLAM的注意力标志物与主动视线控制》，*IEEE Transactions on
    Robotics*，第24卷，第5期，页码1054–1065，2008年。'
- en: '[17] C. Breazeal and B. Scassellati, “A context-dependent attention system
    for a social robot,” *rn*, vol. 255, p. 3, 1999.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] C. Breazeal 和 B. Scassellati，《用于社交机器人依赖于上下文的注意力系统》，*rn*，第255卷，第3页，1999年。'
- en: '[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin, “Attention is all you need,” *arXiv:1706.03762 [cs]*,
    June 2017, arXiv: 1706.03762\. [Online]. Available: [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, 和 I. Polosukhin，《注意力即一切》，*arXiv:1706.03762 [cs]*，2017年6月，arXiv: 1706.03762。[在线]
    可用： [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762)'
- en: '[19] S. Sukhbaatar, J. Weston, R. Fergus *et al.*, “End-to-end memory networks,”
    in *Advances in neural information processing systems*, 2015, pp. 2440–2448.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Sukhbaatar, J. Weston, R. Fergus *等*，《端到端记忆网络》，*Advances in neural
    information processing systems*，2015年，页码2440–2448。'
- en: '[20] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” *arXiv preprint arXiv:1710.10903*, 2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, 和 Y. Bengio，《图注意力网络》，*arXiv
    preprint arXiv:1710.10903*，2017年。'
- en: '[21] Y. Zhang, X. Wang, X. Jiang, C. Shi, and Y. Ye, “Hyperbolic graph attention
    network,” *arXiv preprint arXiv:1912.03046*, 2019.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Zhang, X. Wang, X. Jiang, C. Shi, 和 Y. Ye，《双曲图注意力网络》，*arXiv preprint
    arXiv:1912.03046*，2019年。'
- en: '[22] M. M. Chun, J. D. Golomb, and N. B. Turk-Browne, “A taxonomy of external
    and internal attention,” *Annual review of psychology*, vol. 62, pp. 73–101, 2011.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. M. Chun, J. D. Golomb, 和 N. B. Turk-Browne，《外部和内部注意力的分类》，*Annual review
    of psychology*，第62卷，页码73–101，2011年。'
- en: '[23] S. Chaudhari, G. Polatkan, R. Ramanath, and V. Mithal, “An attentive survey
    of attention models,” *arXiv preprint arXiv:1904.02874*, 2019.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Chaudhari, G. Polatkan, R. Ramanath, 和 V. Mithal，《注意力模型的详尽调查》，*arXiv
    preprint arXiv:1904.02874*，2019年。'
- en: '[24] A. Galassi, M. Lippi, and P. Torroni, “Attention in natural language processing,”
    *IEEE Transactions on Neural Networks and Learning Systems*, 2020.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Galassi, M. Lippi, 和 P. Torroni，《自然语言处理中的注意力》，*IEEE Transactions on
    Neural Networks and Learning Systems*，2020年。'
- en: '[25] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spatial
    transformer networks,” *arXiv preprint arXiv:1506.02025*, 2015.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Jaderberg, K. Simonyan, A. Zisserman, 和 K. Kavukcuoglu，《空间变换网络》，*arXiv
    preprint arXiv:1506.02025*，2015年。'
- en: '[26] X. Chen, C. Liu, R. Shin, D. Song, and M. Chen, “Latent attention for
    if-then program synthesis,” *arXiv preprint arXiv:1611.01867*, 2016.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] X. Chen, C. Liu, R. Shin, D. Song, 和 M. Chen，《用于if-then程序合成的潜在注意力》，*arXiv
    preprint arXiv:1611.01867*，2016年。'
- en: '[27] S. Zagoruyko and N. Komodakis, “Paying more attention to attention: Improving
    the performance of convolutional neural networks via attention transfer,” *arXiv
    preprint arXiv:1612.03928*, 2016.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. Zagoruyko 和 N. Komodakis, “更多关注注意力：通过注意力转移提高卷积神经网络的性能，” *arXiv 预印本
    arXiv:1612.03928*，2016。'
- en: '[28] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, “A simple neural attentive
    meta-learner,” *arXiv:1707.03141 [cs, stat]*, July 2017, arXiv: 1707.03141\. [Online].
    Available: [http://arxiv.org/abs/1707.03141](http://arxiv.org/abs/1707.03141)'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] N. Mishra, M. Rohaninejad, X. Chen, 和 P. Abbeel, “一个简单的神经注意力元学习者，” *arXiv:1707.03141
    [cs, stat]*，2017年7月，arXiv: 1707.03141\. [在线]. 可用： [http://arxiv.org/abs/1707.03141](http://arxiv.org/abs/1707.03141)'
- en: '[29] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, “Squeeze-and-excitation
    networks,” *arXiv:1709.01507 [cs]*, September 2017, arXiv: 1709.01507. [Online].
    Available: [http://arxiv.org/abs/1709.01507](http://arxiv.org/abs/1709.01507)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Hu, L. Shen, S. Albanie, G. Sun, 和 E. Wu, “压缩与激励网络，” *arXiv:1709.01507
    [cs]*，2017年9月，arXiv: 1709.01507。 [在线]. 可用： [http://arxiv.org/abs/1709.01507](http://arxiv.org/abs/1709.01507)'
- en: '[30] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng, “A2̂-nets: Double
    attention networks,” in *Advances in Neural Information Processing Systems*, 2018,
    pp. 352–361.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Chen, Y. Kalantidis, J. Li, S. Yan, 和 J. Feng, “A2̂-nets：双重注意力网络，”
    在 *神经信息处理系统的进展*，2018，第352–361页。'
- en: '[31] J. Fu, J. Liu, H. Tian, Z. Fang, and H. Lu, “Dual attention network for
    scene segmentation,” *arXiv:1809.02983 [cs]*, September 2018, arXiv: 1809.02983\.
    [Online]. Available: [http://arxiv.org/abs/1809.02983](http://arxiv.org/abs/1809.02983)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Fu, J. Liu, H. Tian, Z. Fang, 和 H. Lu, “用于场景分割的双重注意力网络，” *arXiv:1809.02983
    [cs]*，2018年9月，arXiv: 1809.02983\. [在线]. 可用： [http://arxiv.org/abs/1809.02983](http://arxiv.org/abs/1809.02983)'
- en: '[32] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu, “Heterogeneous
    graph attention network,” in *The World Wide Web Conference*, 2019, pp. 2022–2032.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, 和 P. S. Yu, “异质图注意力网络，”
    在 *全球网络会议*，2019，第2022–2032页。'
- en: '[33] A. Lamb, D. He, A. Goyal, G. Ke, C.-F. Liao, M. Ravanelli, and Y. Bengio,
    “Transformers with competitive ensembles of independent mechanisms,” *arXiv preprint
    arXiv:2103.00336*, 2021.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Lamb, D. He, A. Goyal, G. Ke, C.-F. Liao, M. Ravanelli, 和 Y. Bengio,
    “具有独立机制竞争性集成的变换器，” *arXiv 预印本 arXiv:2103.00336*，2021。'
- en: '[34] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” September 2014. [Online]. Available: [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Bahdanau, K. Cho, 和 Y. Bengio, “通过联合学习对齐和翻译的神经机器翻译，” 2014年9月。 [在线].
    可用： [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)'
- en: '[35] Y. Tang, N. Srivastava, and R. R. Salakhutdinov, “Learning generative
    models with visual attention,” 2014, pp. 1808–1816\. [Online]. Available: [http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention](http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Tang, N. Srivastava, 和 R. R. Salakhutdinov, “学习具有视觉注意力的生成模型，” 2014，第1808–1816页\.
    [在线]. 可用： [http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention](http://papers.nips.cc/paper/5345-learning-generative-models-with-visual-attention)'
- en: '[36] Q. Wang, J. Zhang, S. Song, and Z. Zhang, “Attentional neural network:
    Feature selection using cognitive feedback,” *arXiv preprint arXiv:1411.5140*,
    2014.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Q. Wang, J. Zhang, S. Song, 和 Z. Zhang, “注意力神经网络：使用认知反馈的特征选择，” *arXiv
    预印本 arXiv:1411.5140*，2014。'
- en: '[37] A. Graves, G. Wayne, and I. Danihelka, “Neural turing machines,” *arXiv:1410.5401
    [cs]*, October 2014, arXiv: 1410.5401\. [Online]. Available: [http://arxiv.org/abs/1410.5401](http://arxiv.org/abs/1410.5401)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Graves, G. Wayne, 和 I. Danihelka, “神经图灵机，” *arXiv:1410.5401 [cs]*，2014年10月，arXiv:
    1410.5401\. [在线]. 可用： [http://arxiv.org/abs/1410.5401](http://arxiv.org/abs/1410.5401)'
- en: '[38] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models of
    visual attention,” 2014, pp. 2204–2212\. [Online]. Available: [http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] V. Mnih, N. Heess, A. Graves, 和 K. Kavukcuoglu, “视觉注意力的递归模型，” 2014，第2204–2212页\.
    [在线]. 可用： [http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention)'
- en: '[39] M. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber, “Deep networks with
    internal selective attention through feedback connections,” *arXiv preprint arXiv:1407.3068*,
    2014.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] M. Stollenga, J. Masci, F. Gomez, 和 J. Schmidhuber, “通过反馈连接进行内部选择性注意力的深度网络，”
    *arXiv 预印本 arXiv:1407.3068*，2014。'
- en: '[40] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra, “Draw:
    A recurrent neural network for image generation,” *arXiv preprint arXiv:1502.04623*,
    2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, 和 D. Wierstra, “Draw：用于图像生成的递归神经网络，”
    *arXiv 预印本 arXiv:1502.04623*，2015。'
- en: '[41] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
    attention,” in *International Conference on Machine Learning*, June 2015, pp.
    2048–2057\. [Online]. Available: [http://proceedings.mlr.press/v37/xuc15.html](http://proceedings.mlr.press/v37/xuc15.html)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    和 Y. Bengio，“展示、关注和讲述：具有视觉注意力的神经图像描述生成”，发表于*国际机器学习大会*，2015年6月，第2048–2057页。 [在线].
    网址：[http://proceedings.mlr.press/v37/xuc15.html](http://proceedings.mlr.press/v37/xuc15.html)'
- en: '[42] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in *Advances
    in neural information processing systems*, 2015, pp. 2692–2700.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] O. Vinyals, M. Fortunato, 和 N. Jaitly，“指针网络”，发表于*神经信息处理系统进展*，2015年，第2692–2700页。'
- en: '[43] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kočiskỳ, and P. Blunsom,
    “Reasoning about entailment with neural attention,” *arXiv preprint arXiv:1509.06664*,
    2015.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T. Rocktäschel, E. Grefenstette, K. M. Hermann, T. Kočiskỳ, 和 P. Blunsom，“通过神经注意力推理关于蕴含的问题”，*arXiv
    预印本 arXiv:1509.06664*，2015年。'
- en: '[44] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based
    neural machine translation,” in *Proceedings of the 2015 Conference on Empirical
    Methods in Natural Language Processing*.   Lisbon, Portugal: Association for Computational
    Linguistics, 2015, pp. 1412–1421\. [Online]. Available: [http://aclweb.org/anthology/D15-1166](http://aclweb.org/anthology/D15-1166)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. Luong, H. Pham, 和 C. D. Manning，“基于注意力的神经机器翻译的有效方法”，发表于*2015年自然语言处理实证方法会议论文集*。   葡萄牙里斯本：计算语言学协会，2015年，第1412–1421页。
    [在线]. 网址：[http://aclweb.org/anthology/D15-1166](http://aclweb.org/anthology/D15-1166)'
- en: '[45] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    and P. Blunsom, “Teaching machines to read and comprehend,” in *Advances in neural
    information processing systems*, 2015, pp. 1693–1701.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    和 P. Blunsom，“教机器阅读和理解”，发表于*神经信息处理系统进展*，2015年，第1693–1701页。'
- en: '[46] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,
    V. Zhong, R. Paulus, and R. Socher, “Ask me anything: Dynamic memory networks
    for natural language processing,” June 2015\. [Online]. Available: [https://arxiv.org/abs/1506.07285](https://arxiv.org/abs/1506.07285)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Kumar, O. Irsoy, P. Ondruska, M. Iyyer, J. Bradbury, I. Gulrajani,
    V. Zhong, R. Paulus, 和 R. Socher，“问我任何事：用于自然语言处理的动态记忆网络”，2015年6月。 [在线]. 网址：[https://arxiv.org/abs/1506.07285](https://arxiv.org/abs/1506.07285)'
- en: '[47] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, “Bidirectional attention
    flow for machine comprehension,” *arXiv:1611.01603 [cs]*, November 2016, arXiv:
    1611.01603\. [Online]. Available: [http://arxiv.org/abs/1611.01603](http://arxiv.org/abs/1611.01603)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Seo, A. Kembhavi, A. Farhadi, 和 H. Hajishirzi，“用于机器理解的双向注意力流”，*arXiv:1611.01603
    [cs]*，2016年11月，arXiv: 1611.01603。 [在线]. 网址：[http://arxiv.org/abs/1611.01603](http://arxiv.org/abs/1611.01603)'
- en: '[48] V. Mnih, J. Agapiou, S. Osindero, A. Graves, O. Vinyals, K. Kavukcuoglu
    *et al.*, “Strategic attentive writer for learning macro-actions,” *arXiv preprint
    arXiv:1606.04695*, 2016.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] V. Mnih, J. Agapiou, S. Osindero, A. Graves, O. Vinyals, K. Kavukcuoglu
    *等*，“学习宏动作的战略性注意力写作者”，*arXiv 预印本 arXiv:1606.04695*，2016年。'
- en: '[49] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention network
    for extreme summarization of source code,” in *International conference on machine
    learning*.   PMLR, 2016, pp. 2091–2100.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Allamanis, H. Peng, 和 C. Sutton，“用于极端源代码总结的卷积注意力网络”，发表于*国际机器学习会议*。   PMLR，2016年，第2091–2100页。'
- en: '[50] J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
    co-attention for visual question answering,” *arXiv preprint arXiv:1606.00061*,
    2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Lu, J. Yang, D. Batra, 和 D. Parikh，“层次化问题-图像共同注意力用于视觉问答”，*arXiv 预印本
    arXiv:1606.00061*，2016年。'
- en: '[51] A. Graves, “Adaptive computation time for recurrent neural networks,”
    *arXiv:1603.08983 [cs]*, March 2016, arXiv: 1603.08983\. [Online]. Available:
    [http://arxiv.org/abs/1603.08983](http://arxiv.org/abs/1603.08983)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Graves，“递归神经网络的自适应计算时间”，*arXiv:1603.08983 [cs]*，2016年3月，arXiv: 1603.08983。
    [在线]. 网址：[http://arxiv.org/abs/1603.08983](http://arxiv.org/abs/1603.08983)'
- en: '[52] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing when to look: Adaptive
    attention via a visual sentinel for image captioning,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2017, pp. 375–383.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Lu, C. Xiong, D. Parikh, 和 R. Socher，“知道何时查看：通过视觉哨兵进行自适应注意力用于图像描述”，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第375–383页。'
- en: '[53] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical
    attention networks for document classification,” in *Proceedings of the 2016 conference
    of the North American chapter of the association for computational linguistics:
    human language technologies*, 2016, pp. 1480–1489.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, 和 E. Hovy，“用于文档分类的层次注意力网络，”
    在 *2016年北美计算语言学协会：人类语言技术会议论文集* 中，2016年，页码1480–1489。'
- en: '[54] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff, “Top-down
    neural attention by excitation backprop,” *International Journal of Computer Vision*,
    vol. 126, no. 10, pp. 1084–1102, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] J. Zhang, S. A. Bargal, Z. Lin, J. Brandt, X. Shen, 和 S. Sclaroff，“通过激励反向传播的自上而下神经注意力，”
    *国际计算机视觉杂志*，第126卷，第10期，页码1084–1102，2018年。'
- en: '[55] C. Xiong, V. Zhong, and R. Socher, “Dynamic coattention networks for question
    answering,” *arXiv preprint arXiv:1611.01604*, 2016.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. Xiong, V. Zhong, 和 R. Socher，“用于问答的动态共同注意力网络，” *arXiv 预印本 arXiv:1611.01604*，2016年。'
- en: '[56] J. Liu, G. Wang, P. Hu, L.-Y. Duan, and A. C. Kot, “Global context-aware
    attention lstm networks for 3d action recognition,” in *The IEEE Conference on
    Computer Vision and Pattern Recognition (CVPR)*, July 2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Liu, G. Wang, P. Hu, L.-Y. Duan, 和 A. C. Kot，“用于3D动作识别的全局上下文感知注意力LSTM网络，”
    在 *IEEE计算机视觉与模式识别会议（CVPR）* 中，2017年7月。'
- en: '[57] S. Reed, Y. Chen, T. Paine, A. v. d. Oord, S. M. A. Eslami, D. Rezende,
    O. Vinyals, and N. de Freitas, “Few-shot autoregressive density estimation: Towards
    learning to learn distributions,” *arXiv:1710.10304 [cs]*, October 2017, arXiv:
    1710.10304\. [Online]. Available: [http://arxiv.org/abs/1710.10304](http://arxiv.org/abs/1710.10304)'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Reed, Y. Chen, T. Paine, A. v. d. Oord, S. M. A. Eslami, D. Rezende,
    O. Vinyals, 和 N. de Freitas，“少量样本自回归密度估计：朝向学习分布的学习，” *arXiv:1710.10304 [cs]*，2017年10月，arXiv:
    1710.10304。 [在线]. 可用: [http://arxiv.org/abs/1710.10304](http://arxiv.org/abs/1710.10304)'
- en: '[58] P. H. Seo, A. Lehrmann, B. Han, and L. Sigal, “Visual reference resolution
    using attention memory for visual dialog,” *arXiv preprint arXiv:1709.07992*,
    2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] P. H. Seo, A. Lehrmann, B. Han, 和 L. Sigal，“使用注意力记忆进行视觉对话中的视觉参考解析，” *arXiv
    预印本 arXiv:1709.07992*，2017年。'
- en: '[59] N. R. Ke, A. Goyal, O. Bilaniuk, J. Binas, M. C. Mozer, C. Pal, and Y. Bengio,
    “Sparse attentive backtracking: Temporal creditassignment through reminding,”
    *arXiv preprint arXiv:1809.03702*, 2018.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. R. Ke, A. Goyal, O. Bilaniuk, J. Binas, M. C. Mozer, C. Pal, 和 Y. Bengio，“稀疏注意力回溯：通过提醒进行时间信用分配，”
    *arXiv 预印本 arXiv:1809.03702*，2018年。'
- en: '[60] N. R. Ke, A. Goyal, O. Bilaniuk, J. Binas, L. Charlin, C. Pal, and Y. Bengio,
    “Sparse attentive backtracking: Long-range credit assignment in recurrent networks,”
    *arXiv preprint arXiv:1711.02326*, 2017.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] N. R. Ke, A. Goyal, O. Bilaniuk, J. Binas, L. Charlin, C. Pal, 和 Y. Bengio，“稀疏注意力回溯：递归网络中的长距离信用分配，”
    *arXiv 预印本 arXiv:1711.02326*，2017年。'
- en: '[61] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, and J. Y. Choi,
    “Attentional correlation filter network for adaptive visual tracking,” in *2017
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, July 2017,
    pp. 4828–4837.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, 和 J. Y. Choi，“用于自适应视觉跟踪的注意力相关滤波网络，”
    在 *2017 IEEE计算机视觉与模式识别会议（CVPR）* 中，2017年7月，页码4828–4837。'
- en: '[62] Y. Kim, C. Denton, L. Hoang, and A. M. Rush, “Structured attention networks,”
    *arXiv:1702.00887 [cs]*, February 2017, arXiv: 1702.00887\. [Online]. Available:
    [http://arxiv.org/abs/1702.00887](http://arxiv.org/abs/1702.00887)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Y. Kim, C. Denton, L. Hoang, 和 A. M. Rush，“结构化注意力网络，” *arXiv:1702.00887
    [cs]*，2017年2月，arXiv: 1702.00887。 [在线]. 可用: [http://arxiv.org/abs/1702.00887](http://arxiv.org/abs/1702.00887)'
- en: '[63] J.-H. Kim, J. Jun, and B.-T. Zhang, “Bilinear attention networks,” in
    *Advances in Neural Information Processing Systems 31*, S. Bengio, H. Wallach,
    H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.   Curran Associates,
    Inc., 2018, pp. 1564–1574\. [Online]. Available: [http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf](http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf)'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J.-H. Kim, J. Jun, 和 B.-T. Zhang，“双线性注意力网络，” 在 *神经信息处理系统进展 31* 中，S. Bengio,
    H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, 和 R. Garnett 编，Curran
    Associates, Inc.，2018年，页码1564–1574。 [在线]. 可用: [http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf](http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf)'
- en: '[64] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker,
    and D. Rueckert, “Attention gated networks: Learning to leverage salient regions
    in medical images,” *Medical image analysis*, vol. 53, pp. 197–207, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker,
    和 D. Rueckert，“注意力门控网络：学习利用医学图像中的显著区域，” *医学图像分析*，第53卷，页码197–207，2019年。'
- en: '[65] D. Perera and R. Zimmermann, “Lstm networks for online cross-network recommendations,”
    *arXiv preprint arXiv:2008.10849*, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] D. Perera 和 R. Zimmermann，"在线跨网络推荐的LSTM网络"，*arXiv 预印本 arXiv:2008.10849*，2020年。'
- en: '[66] Y. Deng, Y. Kim, J. Chiu, D. Guo, and A. Rush, “Latent alignment and variational
    attention,” in *Advances in Neural Information Processing Systems 31*, S. Bengio,
    H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.   Curran
    Associates, Inc., 2018, pp. 9712–9724\. [Online]. Available: [http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf](http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Y. Deng, Y. Kim, J. Chiu, D. Guo 和 A. Rush，"潜在对齐和变分注意力"，发表于*神经信息处理系统进展31*，S.
    Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi 和 R. Garnett 编辑。Curran
    Associates, Inc.，2018年，页9712–9724。[在线]. 可用：[http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf](http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf)'
- en: '[67] E.-S. Kim, W. Y. Kang, K.-W. On, Y.-J. Heo, and B.-T. Zhang, “Hypergraph
    attention networks for multimodal learning,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 14 581–14 590.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] E.-S. Kim, W. Y. Kang, K.-W. On, Y.-J. Heo 和 B.-T. Zhang，"用于多模态学习的超图注意力网络"，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页14,581–14,590。'
- en: '[68] H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu, and J. Han, “Imram: Iterative
    matching with recurrent attention memory for cross-modal image-text retrieval,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 12 655–12 663.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu 和 J. Han，"IMRAM：用于跨模态图像-文本检索的递归注意力记忆迭代匹配"，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页12,655–12,663。'
- en: '[69] K. Lekkala and L. Itti, “Attentive feature reuse for multi task meta learning,”
    *arXiv preprint arXiv:2006.07438*, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] K. Lekkala 和 L. Itti，"多任务元学习中的注意力特征重用"，*arXiv 预印本 arXiv:2006.07438*，2020年。'
- en: '[70] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, “Disan: Directional
    self-attention network for rnn/cnn-free language understanding,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 32, no. 1, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan 和 C. Zhang，"Disan：面向RNN/CNN的方向性自注意力网络"，发表于*AAAI人工智能会议论文集*，第32卷，第1期，2018年。'
- en: '[71] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals,
    and Y. W. Teh, “Attentive neural processes,” *arXiv preprint arXiv:1901.05761*,
    2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals
    和 Y. W. Teh，"关注神经过程"，*arXiv 预印本 arXiv:1901.05761*，2019年。'
- en: '[72] S. Mittal, A. Lamb, A. Goyal, V. Voleti, M. Shanahan, G. Lajoie, M. Mozer,
    and Y. Bengio, “Learning to combine top-down and bottom-up signals in recurrent
    neural networks with attention over modules,” in *International Conference on
    Machine Learning*.   PMLR, 2020, pp. 6972–6986.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Mittal, A. Lamb, A. Goyal, V. Voleti, M. Shanahan, G. Lajoie, M. Mozer
    和 Y. Bengio，"学习在递归神经网络中结合自上而下和自下而上的信号，利用模块注意力"，发表于*国际机器学习会议*。PMLR，2020年，页6972–6986。'
- en: '[73] J. Kim, M. Ma, T. Pham, K. Kim, and C. D. Yoo, “Modality shifting attention
    network for multi-modal video question answering,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2020, pp. 10 106–10 115.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. Kim, M. Ma, T. Pham, K. Kim 和 C. D. Yoo，"用于多模态视频问答的模态转换注意力网络"，发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，页10,106–10,115。'
- en: '[74] M. W. Eysenck, *Psychology: An international perspective*.   Taylor &
    Francis, 2004.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. W. Eysenck，*心理学：国际视角*。Taylor & Francis，2004年。'
- en: '[75] K. Oberauer, “Working memory and attention–a conceptual analysis and review,”
    *Journal of cognition*, vol. 2, no. 1, 2019.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Oberauer，"工作记忆与注意力——概念分析与回顾"，*认知期刊*，第2卷，第1期，2019年。'
- en: '[76] M. Esterman and D. Rothlein, “Models of sustained attention,” *Current
    opinion in psychology*, vol. 29, pp. 174–180, 2019.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Esterman 和 D. Rothlein，"持续注意力模型"，*心理学当前观点*，第29卷，页174–180，2019年。'
- en: '[77] D. A. Hudson and C. D. Manning, “Compositional attention networks for
    machine reasoning,” *arXiv preprint arXiv:1803.03067*, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] D. A. Hudson 和 C. D. Manning，"用于机器推理的组合注意力网络"，*arXiv 预印本 arXiv:1803.03067*，2018年。'
- en: '[78] H. Nam, J.-W. Ha, and J. Kim, “Dual attention networks for multimodal
    reasoning and matching,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 299–307.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] H. Nam, J.-W. Ha 和 J. Kim，"用于多模态推理和匹配的双重注意力网络"，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，页299–307。'
- en: '[79] K. M. Hermann, T. Kočiský, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    and P. Blunsom, “Teaching machines to read and comprehend,” *arXiv:1506.03340
    [cs]*, June 2015, arXiv: 1506.03340\. [Online]. Available: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] K. M. Hermann, T. Kočiský, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman,
    和 P. Blunsom，“教机器阅读和理解，” *arXiv:1506.03340 [cs]*，2015年6月，arXiv: 1506.03340\. [在线].
    可用链接: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)'
- en: '[80] Q. Mao, J. Li, S. Wang, Y. Zhang, H. Peng, M. He, and L. Wang, “Aspect-based
    sentiment classification with attentive neural turing machines.” in *IJCAI*, 2019,
    pp. 5139–5145.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Q. Mao, J. Li, S. Wang, Y. Zhang, H. Peng, M. He, 和 L. Wang，“基于方面的情感分类与注意神经图灵机。”
    收录于 *IJCAI*，2019年，页码5139–5145。'
- en: '[81] S. Yantis, “The neural basis of selective attention: Cortical sources
    and targets of attentional modulation,” *Current directions in psychological science*,
    vol. 17, no. 2, pp. 86–90, 2008.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Yantis，“选择性注意的神经基础：注意调节的皮层来源和目标，” *《心理科学的当前方向》*，第17卷，第2期，页码86–90，2008年。'
- en: '[82] M. Saenz, G. T. Buracas, and G. M. Boynton, “Global effects of feature-based
    attention in human visual cortex,” *Nature neuroscience*, vol. 5, no. 7, pp. 631–632,
    2002.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. Saenz, G. T. Buracas, 和 G. M. Boynton，“基于特征的注意力在人体视觉皮层中的全局效应，” *《自然神经科学》*，第5卷，第7期，页码631–632，2002年。'
- en: '[83] K. M. O’Craven, P. E. Downing, and N. Kanwisher, “fmri evidence for objects
    as the units of attentional selection,” *Nature*, vol. 401, no. 6753, pp. 584–587,
    1999.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] K. M. O’Craven, P. E. Downing, 和 N. Kanwisher，“fMRI证据表明对象是注意选择的单元，” *《自然》*，第401卷，第6753期，页码584–587，1999年。'
- en: '[84] X. Wang, H. Ji, C. Shi, B. Wang, P. Cui, P. Yu, and Y. Ye, “Heterogeneous
    graph attention network,” *arXiv:1903.07293 [cs]*, March 2019, arXiv: 1903.07293\.
    [Online]. Available: [http://arxiv.org/abs/1903.07293](http://arxiv.org/abs/1903.07293)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Wang, H. Ji, C. Shi, B. Wang, P. Cui, P. Yu, 和 Y. Ye，“异构图注意网络，” *arXiv:1903.07293
    [cs]*，2019年3月，arXiv: 1903.07293\. [在线]. 可用链接: [http://arxiv.org/abs/1903.07293](http://arxiv.org/abs/1903.07293)'
- en: '[85] B. A. Olshausen, C. H. Anderson, and D. C. Van Essen, “A neurobiological
    model of visual attention and invariant pattern recognition based on dynamic routing
    of information,” *Journal of Neuroscience*, vol. 13, no. 11, pp. 4700–4719, 1993.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] B. A. Olshausen, C. H. Anderson, 和 D. C. Van Essen，“基于动态信息路由的视觉注意力和不变模式识别的神经生物学模型，”
    *《神经科学杂志》*，第13卷，第11期，页码4700–4719，1993年。'
- en: '[86] G. E. Hinton, “Deep belief networks,” *Scholarpedia*, vol. 4, no. 5, p.
    5947, 2009.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] G. E. Hinton，“深度置信网络，” *Scholarpedia*，第4卷，第5期，页码5947，2009年。'
- en: '[87] D. K. Lee, L. Itti, C. Koch, and J. Braun, “Attention activates winner-take-all
    competition among visual filters,” *Nature neuroscience*, vol. 2, no. 4, pp. 375–381,
    1999.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. K. Lee, L. Itti, C. Koch, 和 J. Braun，“注意力激活了视觉过滤器之间的赢家通吃竞争，” *《自然神经科学》*，第2卷，第4期，页码375–381，1999年。'
- en: '[88] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis, and F. Nuflo,
    “Modeling visual attention via selective tuning,” *Artificial intelligence*, vol. 78,
    no. 1-2, pp. 507–545, 1995.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] J. K. Tsotsos, S. M. Culhane, W. Y. K. Wai, Y. Lai, N. Davis, 和 F. Nuflo，“通过选择性调节建模视觉注意力，”
    *《人工智能》*，第78卷，第1-2期，页码507–545，1995年。'
- en: '[89] J. Weston, S. Chopra, and A. Bordes, “Memory networks,” *arXiv preprint
    arXiv:1410.3916*, 2014.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Weston, S. Chopra, 和 A. Bordes，“记忆网络，” *arXiv预印本 arXiv:1410.3916*，2014年。'
- en: '[90] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv:1810.04805
    [cs]*, October 2018, arXiv: 1810.04805\. [Online]. Available: [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert: 用于语言理解的深度双向变换器的预训练，”
    *arXiv:1810.04805 [cs]*，2018年10月，arXiv: 1810.04805\. [在线]. 可用链接: [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805)'
- en: '[91] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
    models are unsupervised multitask learners,” *OpenAI blog*, vol. 1, no. 8, p. 9,
    2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和 I. Sutskever，“语言模型是无监督的多任务学习者，”
    *OpenAI博客*，第1卷，第8期，页码9，2019年。'
- en: '[92] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *arXiv preprint arXiv:2005.14165*, 2020.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell *等*，“语言模型是少量学习者，” *arXiv预印本 arXiv:2005.14165*，2020年。'
- en: '[93] M. D. M. Reddy, M. S. M. Basha, M. M. C. Hari, and M. N. Penchalaiah,
    “Dall-e: Creating images from text,” 2021.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] M. D. M. Reddy, M. S. M. Basha, M. M. C. Hari, 和 M. N. Penchalaiah，“Dall-e:
    从文本创建图像，” 2021年。'
- en: '[94] S. Frintrop, *VOCUS: A visual attention system for object detection and
    goal-directed search*.   Springer, 2006, vol. 3899.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] S. Frintrop, *VOCUS: 一种用于物体检测和目标导向搜索的视觉注意系统*。Springer, 2006年，第3899卷。'
- en: '[95] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
    for rapid scene analysis,” *IEEE PAMI*, vol. 20, no. 11, pp. 1254–1259, 1998.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] L. Itti、C. Koch 和 E. Niebur，“基于显著性的视觉注意力模型用于快速场景分析”，*IEEE PAMI*，第20卷，第11期，第1254–1259页，1998年。'
- en: '[96] C. O’Callaghan, “Perception and multimodality,” *The*, 2012.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. O’Callaghan，“感知与多模态性”，*The*，2012年。'
- en: '[97] C. Doersch, “Tutorial on variational autoencoders,” *arXiv preprint arXiv:1606.05908*,
    2016.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] C. Doersch，“变分自编码器教程”，*arXiv预印本 arXiv:1606.05908*，2016年。'
- en: '[98] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in *Proceedings of the 2014 conference on empirical methods
    in natural language processing (EMNLP)*, 2014, pp. 1532–1543.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Pennington、R. Socher 和 C. D. Manning，“Glove: 全球词向量表示”，发表于 *2014年自然语言处理经验方法会议（EMNLP）论文集*，2014年，第1532–1543页。'
- en: '[99] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,” *arXiv
    preprint arXiv:1505.00387*, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] R. K. Srivastava、K. Greff 和 J. Schmidhuber，“高速公路网络”，*arXiv预印本 arXiv:1505.00387*，2015年。'
- en: '[100] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
    *IEEE Transactions on Signal Processing*, vol. 45, no. 11, pp. 2673–2681, 1997.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Schuster 和 K. K. Paliwal，“双向递归神经网络”，*IEEE信号处理汇刊*，第45卷，第11期，第2673–2681页，1997年。'
- en: '[101] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” *arXiv
    preprint arXiv:1607.06450*, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] J. L. Ba、J. R. Kiros 和 G. E. Hinton，“层归一化”，*arXiv预印本 arXiv:1607.06450*，2016年。'
- en: '[102] A. M. Treisman and G. Gelade, “A feature-integration theory of attention,”
    *Cognitive psychology*, vol. 12, no. 1, pp. 97–136, 1980.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. M. Treisman 和 G. Gelade，“注意力的特征整合理论”，*认知心理学*，第12卷，第1期，第97–136页，1980年。'
- en: '[103] G. Backer, B. Mertsching, and M. Bollmann, “Data-and model-driven gaze
    control for an active-vision system,” *IEEE Transactions on Pattern Analysis and
    Machine Intelligence*, vol. 23, no. 12, pp. 1415–1429, 2001.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] G. Backer、B. Mertsching 和 M. Bollmann，“数据驱动和模型驱动的注视控制用于主动视觉系统”，*IEEE模式分析与机器智能汇刊*，第23卷，第12期，第1415–1429页，2001年。'
- en: '[104] S. Frintrop, E. Rome, and H. I. Christensen, “Computational visual attention
    systems and their cognitive foundations: A survey,” *ACM Transactions on Applied
    Perception (TAP)*, vol. 7, no. 1, pp. 1–39, 2010.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Frintrop、E. Rome 和 H. I. Christensen，“计算视觉注意系统及其认知基础：综述”，*ACM应用感知汇刊（TAP）*，第7卷，第1期，第1–39页，2010年。'
- en: '[105] D. Kahneman, “Maps of bounded rationality: Psychology for behavioral
    economics,” *American economic review*, vol. 93, no. 5, pp. 1449–1475, 2003.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] D. Kahneman，“有限理性地图：行为经济学的心理学”，*美国经济评论*，第93卷，第5期，第1449–1475页，2003年。'
- en: '[106] C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual
    and textual question answering,” *arXiv:1603.01417 [cs]*, March 2016, arXiv: 1603.01417\.
    [Online]. Available: [http://arxiv.org/abs/1603.01417](http://arxiv.org/abs/1603.01417)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] C. Xiong、S. Merity 和 R. Socher，“用于视觉和文本问题回答的动态记忆网络”，*arXiv:1603.01417
    [cs]*，2016年3月，arXiv: 1603.01417\. [在线]. 可用链接: [http://arxiv.org/abs/1603.01417](http://arxiv.org/abs/1603.01417)'
