- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1803.04818] A Survey on Deep Learning Toolkits and Libraries for Intelligent
    User Interfaces'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1803.04818] 关于智能用户界面的深度学习工具包和库的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1803.04818](https://ar5iv.labs.arxiv.org/html/1803.04818)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1803.04818](https://ar5iv.labs.arxiv.org/html/1803.04818)
- en: \toappear
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \toappear
- en: A Survey on Deep Learning Toolkits and Libraries
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度学习工具包和库的调查
- en: for Intelligent User Interfaces
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 用于智能用户界面
- en: Jan Zacharias
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Jan Zacharias
- en: Michael Barz
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Barz
- en: Daniel Sonntag German Research Center for Artificial Intelligence (DFKI) Saarbrücken,
    Germany [jan.zacharias@dfki.de](mailto:jan.zacharias@dfki.de) German Research
    Center for Artificial Intelligence (DFKI) Saarbrücken, Germany [michael.barz@dfki.de](mailto:michael.barz@dfki.de)
    German Research Center for Artificial Intelligence (DFKI) Saarbrücken, Germany
    [daniel.sonntag@dfki.de](mailto:daniel.sonntag@dfki.de)(2018)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel Sonntag 德国人工智能研究中心（DFKI）萨尔布吕肯，德国 [jan.zacharias@dfki.de](mailto:jan.zacharias@dfki.de)
    德国人工智能研究中心（DFKI）萨尔布吕肯，德国 [michael.barz@dfki.de](mailto:michael.barz@dfki.de) 德国人工智能研究中心（DFKI）萨尔布吕肯，德国
    [daniel.sonntag@dfki.de](mailto:daniel.sonntag@dfki.de)(2018)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper provides an overview of prominent deep learning toolkits and, in
    particular, reports on recent publications that contributed open source software
    for implementing tasks that are common in intelligent user interfaces (IUI). We
    provide a scientific reference for researchers and software engineers who plan
    to utilise deep learning techniques within their IUI research and development
    projects.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了主要的深度学习工具包，特别是报告了最近的出版物，这些出版物贡献了用于实现智能用户界面（IUI）中常见任务的开源软件。我们为计划在其 IUI 研究和开发项目中利用深度学习技术的研究人员和软件工程师提供了科学参考。
- en: 'category:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 分类：
- en: 'H.5.2 Information Interfaces and Presentation (e.g. HCI): User Interfaces'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: H.5.2 信息界面和展示（例如HCI）：用户界面
- en: 'keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Artificial intelligence; Machine learning; Deep learning; Interactive machine
    learning; Hyper-parameter tuning; Convolutional neural networks
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能；机器学习；深度学习；互动机器学习；超参数调优；卷积神经网络
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Intelligent user interfaces (IUIs) aim to incorporate intelligent automated
    capabilities in human-computer interaction (HCI), where the net impact is an interaction
    that improves performance or usability in critical ways. Deep learning techniques
    can be used in an IUI to implement artificial intelligence (AI) components that
    effectively leverage human skills and capabilities, so that human performance
    with an application excels [[55](#bib.bib55)].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 智能用户界面（IUI）旨在在人机交互（HCI）中融入智能自动化能力，其净影响是提升性能或可用性的交互方式。深度学习技术可以用于 IUI 中，实施有效利用人类技能和能力的人工智能（AI）组件，从而使应用程序中的人类表现突出[[55](#bib.bib55)]。
- en: Many IUIs, especially in the smartphone domain, use multiple input and output
    modalities for more efficient, flexible and robust user interaction [[45](#bib.bib45)].
    They allow users to select a suitable input mode, or to shift among modalities
    as needed during the changing physical contexts and demands of continuous mobile
    use.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多智能用户界面，特别是在智能手机领域，使用多种输入和输出模式以实现更高效、灵活和可靠的用户交互[[45](#bib.bib45)]。它们允许用户选择合适的输入模式，或在物理环境和连续移动使用的需求变化时在模式之间切换。
- en: Deep learning has the potential to increase this flexibility with user and adaptation
    models that support speech, pen, (multi-) touch, gestures and gaze as modalities
    and can learn the appropriate alignment of them for mutual disambiguation. This
    ensures a higher precision in understanding the user input and to overcome the
    limitations of individual signal or interaction modalities [[42](#bib.bib42)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有可能通过支持语音、笔、（多点）触摸、手势和注视等模式的用户和适应模型来增加这种灵活性，并可以学习这些模式的适当对齐，以实现相互消歧。这确保了更高的理解用户输入的精度，并克服了单一信号或交互模式的局限[[42](#bib.bib42)]。
- en: Deep learning systems are able to process very complex real-world input data
    by using a nested hierarchy of concepts with increasing complexity for its representation [[26](#bib.bib26)].
    Multimodal IUIs can greatly benefit from deep learning techniques because the
    complexity inherent in high-level event detection and multimodal signal processing
    can be modelled by likewise complex deep network structures and efficiently trained
    with nowadays available GPU-infrastructures. Especially recurrent model architectures
    are suitable for processing sequential multimodal signal streams, e.g., for natural
    dialogues and long-term autonomous agents [[53](#bib.bib53)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统能够通过使用逐层概念层次结构来处理非常复杂的现实世界输入数据[[26](#bib.bib26)]。多模态智能用户界面可以从深度学习技术中大大受益，因为高层事件检测和多模态信号处理的复杂性可以通过同样复杂的深度网络结构建模，并利用现今可用的GPU基础设施进行高效训练。特别是递归模型架构适合处理连续的多模态信号流，例如自然对话和长期自主代理[[53](#bib.bib53)]。
- en: This paper provides IUI researchers and practitioners with an overview of deep
    learning toolkits and libraries that are available under an open source license
    and describes how they can be applied for intelligent multimodal interaction (considering
    the modules of the architecture in figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning Toolkits and Libraries for Intelligent User Interfaces")
    for classification). Related deep learning surveys are in the medical domain[[22](#bib.bib22)]
    or focus on the techniques [[46](#bib.bib46), [5](#bib.bib5), [51](#bib.bib51)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文为IUI研究人员和从业者提供了一个关于可在开源许可下使用的深度学习工具包和库的概述，并描述了它们如何应用于智能多模态交互（考虑图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Toolkits and Libraries
    for Intelligent User Interfaces")中的架构模块进行分类）。相关的深度学习综述集中在医疗领域[[22](#bib.bib22)]或技术[[46](#bib.bib46),
    [5](#bib.bib5), [51](#bib.bib51)]。
- en: 'A major challenge common to all AI systems is to move from closed to open world
    settings. Superhuman performance in one environment can lead to unexpected behaviour
    and dangerous situations in another:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所有AI系统面临的主要挑战是从封闭环境转向开放世界环境。在一个环境中达到超人性能可能在另一个环境中导致意外行为和危险情况。
- en: '>>The ’intelligence’ of an artificial intelligence system'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '>>人工智能系统的“智能”'
- en: can be deep but narrow.<< [[18](#bib.bib18)]
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可能是深度但狭窄。<< [[18](#bib.bib18)]
- en: An implication is the need for efficient learning methods and adaptive models
    for long-term autonomous systems. Promising techniques include active learning [[49](#bib.bib49)],
    reinforcement learning [[28](#bib.bib28)], interactive machine learning [[4](#bib.bib4)]
    and machine teaching [[52](#bib.bib52)]. We motivate the use of interactive training
    approaches enabling efficient and continuous updates to machine learning models.
    This is particularly useful for enhancing user interaction because it enables
    robust processing of versatile signal streams and joint analysis of data from
    multiple modalities and sensors to understand complex user behaviour.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点暗示了对长期自主系统的高效学习方法和适应性模型的需求。 promising技术包括主动学习[[49](#bib.bib49)]、强化学习[[28](#bib.bib28)]、交互式机器学习[[4](#bib.bib4)]和机器教学[[52](#bib.bib52)]。我们推动使用交互式培训方法，实现对机器学习模型的高效和持续更新。这对于增强用户交互尤为重要，因为它能有效处理多样化的信号流，并对来自多个模态和传感器的数据进行联合分析，以理解复杂的用户行为。
- en: '![Refer to caption](img/1f85f0e9a9dab8af00bae01a553dad82.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1f85f0e9a9dab8af00bae01a553dad82.png)'
- en: 'Figure 1: Categorization of intelligent user interface key components, based
    on the conceptual architecture [[60](#bib.bib60)] and DFKI’s Smartweb system [[56](#bib.bib56)].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：智能用户界面关键组件的分类，基于概念架构[[60](#bib.bib60)]和DFKI的Smartweb系统[[56](#bib.bib56)]。
- en: 2 Toolkits and Libraries
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 工具包和库
- en: We briefly introduce the most popular frameworks used for implementing deep
    learning algorithms (summarised in table [1](#S2.T1 "Table 1 ‣ 2.1 Deep Learning
    Frameworks ‣ 2 Toolkits and Libraries ‣ A Survey on Deep Learning Toolkits and
    Libraries for Intelligent User Interfaces")). We include information about licenses
    and supported programming languages for quick compatibility or preference checks.
    Then, we present and qualitatively evaluate open source contributions that are
    based on these frameworks and that implement key elements of an IUI. Works are
    grouped into categories in alignment to an adapted version of the high-level IUI
    architecture as depicted in figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    on Deep Learning Toolkits and Libraries for Intelligent User Interfaces"). Other
    ways of applying the described systems are certainly possible and appropriate
    depending on the use case at hand.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了用于实现深度学习算法的最流行框架（总结在表格 [1](#S2.T1 "表 1 ‣ 2.1 深度学习框架 ‣ 2 工具包和库 ‣ 智能用户界面深度学习工具包和库的调查")）。我们包含了有关许可证和支持的编程语言的信息，以便快速进行兼容性或偏好检查。然后，我们展示并定性评估了基于这些框架并实现
    IUI 关键元素的开源贡献。工作根据适应版的高层次 IUI 架构进行分类，如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 智能用户界面深度学习工具包和库的调查")所示。根据具体使用案例，应用所描述的系统的其他方式当然也是可能和适当的。
- en: 2.1 Deep Learning Frameworks
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 深度学习框架
- en: The core of most deep learning contributions are sophisticated machine learning
    frameworks like TensorFlow [[2](#bib.bib2)] and Caffe [[31](#bib.bib31)]. Most
    of these software packages are published as open source, allowing independent
    review, verification, and democratised usage in IUI projects. This does not imply
    that there are no errors in current implementations, however, the open character
    enables everybody to search for and eventually identify the root cause of wrong
    results or poor performance. Similarly, new features can be proposed and contributed.
    Table [1](#S2.T1 "Table 1 ‣ 2.1 Deep Learning Frameworks ‣ 2 Toolkits and Libraries
    ‣ A Survey on Deep Learning Toolkits and Libraries for Intelligent User Interfaces")
    lists toolkits and libraries that are a representative selection of available
    open source solutions. The table is sorted by a popularity rating proposed by
    François Chollet, the author of the Keras deep learning toolkit. GitHub metrics
    are used and weighted by coefficients such that the relative correlation of each
    metric reflects the number of users. To model the factors, Chollet informally
    took into account Google Analytics data for the Keras project website, PyPI download
    data, ArXiv mentions data as well as Google Trends data among other sources. The
    rating is calculated as the sum of the GitHub Contributions$\times 30$, Issues$\times
    20$, Forks$\times 3$ and the Stars, scaled to $100\%$ defined by the top-scorer
    TensorFlow as a benchmark. While the exact numbers have been chosen manually,
    like in an ensemble model the relative order of magnitude of the coefficients
    matter beside the fact that multiple data sources are used.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习贡献的核心是如 TensorFlow [[2](#bib.bib2)] 和 Caffe [[31](#bib.bib31)] 等复杂的机器学习框架。这些软件包大多数以开源形式发布，允许独立审查、验证以及在
    IUI 项目中的民主使用。这并不意味着当前实现中没有错误，但开放性特征使每个人都能搜索并最终识别错误结果或性能差的根本原因。同样，新的功能也可以被提议和贡献。表
    [1](#S2.T1 "表 1 ‣ 2.1 深度学习框架 ‣ 2 工具包和库 ‣ 智能用户界面深度学习工具包和库的调查") 列出了工具包和库，这些是代表性选择的开源解决方案。该表按
    Keras 深度学习工具包作者 François Chollet 提出的流行度评分排序。GitHub 指标被使用并按系数加权，以使每个指标的相对关联性反映用户数量。为建模这些因素，Chollet
    非正式地考虑了 Keras 项目网站的 Google Analytics 数据、PyPI 下载数据、ArXiv 提及数据以及 Google Trends 数据等其他来源。评分计算为
    GitHub Contributions$\times 30$、Issues$\times 20$、Forks$\times 3$ 和 Stars 的总和，按以
    TensorFlow 为基准的 $100\%$ 进行缩放。虽然确切的数字是手动选择的，但如同在集成模型中，系数的相对量级也很重要，除了使用多个数据源的事实之外。
- en: '| Name | Website | GitHub URL | License | Language | APIs | Rating [%] |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 网站 | GitHub 网址 | 许可证 | 语言 | APIs | 评分 [%] |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| TensorFlow [[2](#bib.bib2)] | [http://tensorflow.org](http://tensorflow.org)
    | [tensorflow/ tensorflow](http://github.com/tensorflow/tensorflow) | Apache-2.0
    | C++, Python | Python, C++, Java, Go | 100 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow [[2](#bib.bib2)] | [http://tensorflow.org](http://tensorflow.org)
    | [tensorflow/ tensorflow](http://github.com/tensorflow/tensorflow) | Apache-2.0
    | C++，Python | Python，C++，Java，Go | 100 |'
- en: '| Keras [[15](#bib.bib15)] | [http://keras.io/](http://keras.io/) | [fchollet/keras](http://github.com/fchollet/keras)
    | MIT | Python | Python, R | 46.1 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Keras [[15](#bib.bib15)] | [http://keras.io/](http://keras.io/) | [fchollet/keras](http://github.com/fchollet/keras)
    | MIT | Python | Python, R | 46.1 |'
- en: '| Caffe [[31](#bib.bib31)] | [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org)
    | [BVLC/caffe](http://github.com/BVLC/caffe) | BSD | C++ | Python, MATLAB | 38.1
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Caffe [[31](#bib.bib31)] | [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org)
    | [BVLC/caffe](http://github.com/BVLC/caffe) | BSD | C++ | Python, MATLAB | 38.1
    |'
- en: '| MXNet [[14](#bib.bib14)] | [http://mxnet.io](http://mxnet.io) | [apache/incubator-mxnet](http://github.com/apache/incubator-mxnet)
    | Apache-2.0 | C++ | Python, Scala, R, JavaScript, Julia, MATLAB, Go, C++, Perl
    | 34 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| MXNet [[14](#bib.bib14)] | [http://mxnet.io](http://mxnet.io) | [apache/incubator-mxnet](http://github.com/apache/incubator-mxnet)
    | Apache-2.0 | C++ | Python, Scala, R, JavaScript, Julia, MATLAB, Go, C++, Perl
    | 34 |'
- en: '| Theano [[3](#bib.bib3)] | [http://deeplearning.net/ software/theano](http://deeplearning.net/software/theano)
    | [Theano/Theano](http://github.com/Theano/Theano) | BSD | Python | Python | 19.3
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Theano [[3](#bib.bib3)] | [http://deeplearning.net/ software/theano](http://deeplearning.net/software/theano)
    | [Theano/Theano](http://github.com/Theano/Theano) | BSD | Python | Python | 19.3
    |'
- en: '| CNTK [[62](#bib.bib62)] | [https://docs.microsoft.com/ en-us/cognitive-toolkit](https://docs.microsoft.com/en-us/cognitive-toolkit)
    | [Microsoft/CNTK](http://github.com/Microsoft/CNTK) | MIT | C++ | Python, C++,
    C#, Java | 18.4 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| CNTK [[62](#bib.bib62)] | [https://docs.microsoft.com/ en-us/cognitive-toolkit](https://docs.microsoft.com/en-us/cognitive-toolkit)
    | [Microsoft/CNTK](http://github.com/Microsoft/CNTK) | MIT | C++ | Python, C++,
    C#, Java | 18.4 |'
- en: '| DeepLearning4J [[21](#bib.bib21)] | [https://deeplearning4j.org](https://deeplearning4j.org)
    | [deeplearning4j/ deeplearning4j](http://github.com/deeplearning4j/deeplearning4j)
    | Apache-2.0 | Java, Scala | Java, Scala, Clojure, Kotlin | 17.8 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| DeepLearning4J [[21](#bib.bib21)] | [https://deeplearning4j.org](https://deeplearning4j.org)
    | [deeplearning4j/ deeplearning4j](http://github.com/deeplearning4j/deeplearning4j)
    | Apache-2.0 | Java, Scala | Java, Scala, Clojure, Kotlin | 17.8 |'
- en: '| PaddlePaddle | [http://www.paddlepaddle.org](http://www.paddlepaddle.org)
    | [baidu/paddle](http://github.com/baidu/paddle) | Apache-2.0 | C++ | C++ | 16.3
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| PaddlePaddle | [http://www.paddlepaddle.org](http://www.paddlepaddle.org)
    | [baidu/paddle](http://github.com/baidu/paddle) | Apache-2.0 | C++ | C++ | 16.3
    |'
- en: '| PyTorch | [http://pytorch.org](http://pytorch.org) | [pytorch/pytorch](http://github.com/pytorch/pytorch)
    | BSD | C++, Python | Python | 14.3 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch | [http://pytorch.org](http://pytorch.org) | [pytorch/pytorch](http://github.com/pytorch/pytorch)
    | BSD | C++, Python | Python | 14.3 |'
- en: '| Chainer [[59](#bib.bib59)] | [https://chainer.org](https://chainer.org) |
    [pfnet/chainer](http://github.com/pfnet/chainer) | MIT | Python | Python | 7.9
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Chainer [[59](#bib.bib59)] | [https://chainer.org](https://chainer.org) |
    [pfnet/chainer](http://github.com/pfnet/chainer) | MIT | Python | Python | 7.9
    |'
- en: '| Torch7 [[17](#bib.bib17)] | [http://torch.ch/](http://torch.ch) | [torch/torch7](http://github.com/torch/torch7)
    | BSD | C, Lua | C, Lua, LuaJIT | 7.8 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Torch7 [[17](#bib.bib17)] | [http://torch.ch/](http://torch.ch) | [torch/torch7](http://github.com/torch/torch7)
    | BSD | C, Lua | C, Lua, LuaJIT | 7.8 |'
- en: '| DIGITS [[61](#bib.bib61)] | [https://developer.nvidia.com/ digits](https://developer.nvidia.com/digits)
    | [NVIDIA/DIGITS](http://github.com/NVIDIA/DIGITS) | BSD | Python | REST/Json
    | 7.8 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| DIGITS [[61](#bib.bib61)] | [https://developer.nvidia.com/ digits](https://developer.nvidia.com/digits)
    | [NVIDIA/DIGITS](http://github.com/NVIDIA/DIGITS) | BSD | Python | REST/Json
    | 7.8 |'
- en: '| TFLearn [[19](#bib.bib19)] | [http://tflearn.org](http://tflearn.org) | [tflearn/tflearn](http://github.com/tflearn/tflearn)
    | MIT | Python | Python | 7.5 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| TFLearn [[19](#bib.bib19)] | [http://tflearn.org](http://tflearn.org) | [tflearn/tflearn](http://github.com/tflearn/tflearn)
    | MIT | Python | Python | 7.5 |'
- en: '| Caffe2 | [https://caffe2.ai](https://caffe2.ai) | [caffe2/caffe2](http://github.com/caffe2/caffe2)
    | Apache-2.0 | C++, Python | Python, C++ | 7.4 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Caffe2 | [https://caffe2.ai](https://caffe2.ai) | [caffe2/caffe2](http://github.com/caffe2/caffe2)
    | Apache-2.0 | C++, Python | Python, C++ | 7.4 |'
- en: '| dlib [[36](#bib.bib36)] | [http://dlib.net](http://dlib.net) | [davisking/dlib](http://github.com/davisking/dlib)
    | Boost | C++ | C++, Python | 5.7 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| dlib [[36](#bib.bib36)] | [http://dlib.net](http://dlib.net) | [davisking/dlib](http://github.com/davisking/dlib)
    | Boost | C++ | C++, Python | 5.7 |'
- en: 'Table 1: Open source software overview with rating based on GitHub metrics'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：基于GitHub指标的开源软件概览及评级
- en: 2.2 Deep Learning in IUIs
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度学习在交互用户界面中的应用
- en: One can find a multitude of deep learning software on the web and it is unclear
    whether these packages are useful and easy to setup in an IUI project at a first
    glance. With this work, we want to shed light on selected open source contributions
    by providing an overview and sharing our experiences in terms of utility and ease
    of use. We group related works based on the long acknowledged conceptual architecture
    by Wahlster and Maybury [[60](#bib.bib60)] and its reference implementation [[56](#bib.bib56)]
    which defines essential parts and modules of IUIs (see figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Learning Toolkits and Libraries for Intelligent
    User Interfaces")).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 网页上可以找到大量的深度学习软件，初看这些包是否在 IUI 项目中有用且易于设置尚不清楚。通过这项工作，我们希望通过提供概述并分享我们在实用性和易用性方面的经验，来阐明一些精选的开源贡献。我们根据
    Wahlster 和 Maybury 长期认可的概念架构[[60](#bib.bib60)]及其参考实现[[56](#bib.bib56)]对相关工作进行分组，该架构定义了
    IUI 的基本部分和模块（见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning
    Toolkits and Libraries for Intelligent User Interfaces")）。
- en: We consider the functional coherent elements *Input Processing & Media Analysis*;
    *Interaction Management, Output Rendering & Media Design* and *Backend* services
    as the main building blocks. Many works can be applied at different stages or
    implement multiple roles at once, particularly deep learning systems that are
    trained end-to-end. Inference mechanisms and the representation of the current
    "information state" and histories of meta models are shared across all IUI components.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将功能上连贯的元素 *输入处理与媒体分析*；*互动管理、输出渲染与媒体设计* 和 *后端* 服务视为主要构建块。许多工作可以在不同阶段应用或一次性实现多个角色，特别是经过端到端训练的深度学习系统。推断机制和当前“信息状态”以及元模型历史的表示在所有
    IUI 组件中是共享的。
- en: 2.2.1 Input Processing & Media Analysis
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 输入处理与媒体分析
- en: Components that implement this role help in analysing and understanding the
    user by including available modalities and fusing them, if appropriate. Examples
    are IUIs that require a natural language understanding (NLU) component to extract
    structured semantic information from unstructured text, e.g., for classifying
    the user’s intent and extracting entities.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此角色的组件通过包括可用的模式并在适当时将它们融合，帮助分析和理解用户。例如，需要自然语言理解 (NLU) 组件的 IUI 用于从非结构化文本中提取结构化语义信息，例如，用于分类用户意图和提取实体。
- en: 'Hauswald et al. [[29](#bib.bib29)] implemented the Tonic Suite that offers
    different classification services as a service based on DjiNN, an infrastructure
    for deep neural networks (DNN). It can process visual stimuli for image classification
    based on AlexNet [[37](#bib.bib37)] and face recognition by replicating Facebook’s
    DeepFace [[58](#bib.bib58)]. Natural language input is supported in terms of digit
    recognition based on MNIST [[38](#bib.bib38)] and automated speech recognition
    (ASR) based on the Kaldi¹¹1https://github.com/kaldi-asr/kaldi ASR toolkit [[47](#bib.bib47)]
    trained on the VoxForge²²2http://www.voxforge.org open-source large scale speech
    corpora. Further, several natural language processing techniques are available:
    part-of-speech tagging, named entity recognition and word chunking—all based on
    neural networks first introduced by NEC’s SENNA³³3http://ml.nec-labs.com/senna
    project. The Tonic suite relies on the Caffe framework.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Hauswald 等人[[29](#bib.bib29)] 实现了 Tonic Suite，提供基于 DjiNN 的不同分类服务，DjiNN 是一个用于深度神经网络
    (DNN) 的基础设施。它可以处理图像分类的视觉刺激，基于 AlexNet [[37](#bib.bib37)] 和通过复制 Facebook 的 DeepFace
    [[58](#bib.bib58)] 进行面部识别。自然语言输入方面，支持基于 MNIST [[38](#bib.bib38)] 的数字识别和基于 Kaldi¹¹1https://github.com/kaldi-asr/kaldi
    ASR 工具包 [[47](#bib.bib47)] 的自动语音识别（ASR），该工具包在 VoxForge²²2http://www.voxforge.org
    开源的大规模语音语料库上进行训练。此外，还提供了几种自然语言处理技术：词性标注、命名实体识别和词块划分——这些技术均基于 NEC 的 SENNA³³3http://ml.nec-labs.com/senna
    项目首次引入的神经网络。Tonic Suite 依赖于 Caffe 框架。
- en: The BSD 3-Clause licensed C++ source code⁴⁴4https://github.com/claritylab/djinn
    is however inconsistently annotated and TODO’s to improve the documentation quality
    remain unresolved. Sufficient installation instructions for DjiNN and Tonic are
    separately hosted⁵⁵5http://djinn.clarity-lab.org/djinn/^,⁶⁶6http://djinn.clarity-lab.org/tonic-suite/.
    Both software packages have not been updated since 2015, rely on an outdated Caffe
    version and have many other legacy dependencies, thus extending installation time
    considerably.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BSD 3-Clause 许可证的 C++ 源代码⁴⁴4https://github.com/claritylab/djinn 的注释不一致，提升文档质量的
    TODO 尚未解决。DjiNN 和 Tonic 的详细安装说明分别托管在⁵⁵5http://djinn.clarity-lab.org/djinn/^,⁶⁶6http://djinn.clarity-lab.org/tonic-suite/。这两个软件包自
    2015 年以来没有更新，依赖于过时的 Caffe 版本，并且有许多其他遗留依赖，因此安装时间大大延长。
- en: 'Relation extraction is an important part of natural language processing (NLP):
    relational facts are extracted from plain text. For instance, this task is required
    in the automated population and extension of a knowledge base from user input.
    Lin et al. [[41](#bib.bib41)] released a neural relation extraction implementation⁷⁷7https://github.com/thunlp/NRE
    under the MIT license. The same repository hosts the re-implementation of the
    relation extractors of Zeng et al. [[64](#bib.bib64), [63](#bib.bib63)]. The C++
    code is completely undocumented, the ReadMe file lists comparative results whereas
    required resources are not directly accessible and dependencies are not listed.
    More recently the authors published a better documented relation extractor ⁸⁸8https://github.com/thunlp/TensorFlow-NRE
    inspired by [[41](#bib.bib41), [65](#bib.bib65)] which uses TensorFlow and is
    written in Python—the code is annotated and dependencies are listed appropriately.
    Similarly, Nguyen and Grishman [[43](#bib.bib43)] proposed the combination of
    convolutional and recurrent neural networks in hybrid models via majority voting
    for relation extraction. The authors published their source code⁹⁹9https://github.com/anoperson/DeepIE,
    which is based on the Theano framework, to allow others to verify and potentially
    improve on their method concerning the ACE 2005 Corpus^(10)^(10)10https://catalog.ldc.upenn.edu/ldc2006t06.
    The Python source does not contain licensing information and is not annotated.
    The ReadMe file outlines how the evaluation can be performed and states that the
    dataset needs to be procured separately, incurring a $$4000$ fee.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取是自然语言处理（NLP）的一个重要部分：从普通文本中提取关系事实。例如，这项任务在从用户输入中自动填充和扩展知识库时是必需的。Lin 等人 [[41](#bib.bib41)]
    发布了一个基于 MIT 许可证的神经关系抽取实现⁷⁷7https://github.com/thunlp/NRE。该仓库还托管了 Zeng 等人 [[64](#bib.bib64),
    [63](#bib.bib63)] 的关系抽取器的重实现。C++ 代码完全没有文档，ReadMe 文件列出了比较结果，而所需资源无法直接访问，依赖项也没有列出。最近，作者发布了一个文档更完善的关系抽取器⁸⁸8https://github.com/thunlp/TensorFlow-NRE，灵感来源于
    [[41](#bib.bib41), [65](#bib.bib65)]，该工具使用 TensorFlow 并用 Python 编写——代码有注释，并且依赖项被适当列出。同样，Nguyen
    和 Grishman [[43](#bib.bib43)] 提出了通过多数投票将卷积神经网络和递归神经网络结合在混合模型中进行关系抽取。作者发布了他们的源代码⁹⁹9https://github.com/anoperson/DeepIE，该代码基于
    Theano 框架，允许他人验证和潜在地改进他们的方法，涉及 ACE 2005 语料库^(10)^(10)10https://catalog.ldc.upenn.edu/ldc2006t06。Python
    源代码没有包含许可信息，并且没有注释。ReadMe 文件概述了如何进行评估，并指出需要单独获取数据集，费用为 $$4000$。
- en: Chen and Manning proposed a dependency parser using neural networks that analyses
    the grammatical structure of sentences and tries to establish relationships between
    "head" words and words which modify those heads [[13](#bib.bib13)]. The software
    is part of the Stanford Parser^(11)^(11)11https://nlp.stanford.edu/software/lex-parser.html
    and CoreNLP,^(12)^(12)12https://github.com/stanfordnlp/CoreNLP a Java toolkit
    for NLP which allows the computer to analyse, understand, alter or generate natural
    language. Consequently, CoreNLP relates also to the media design element of an
    IUI. Building instructions are included in the ReadMe file and a well-written
    HTML documentation is available^(13)^(13)13https://stanfordnlp.github.io/CoreNLP/.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 和 Manning 提出了一个使用神经网络的依赖解析器，该解析器分析句子的语法结构，并试图建立“头”词与修饰这些头词的词之间的关系 [[13](#bib.bib13)]。该软件是
    Stanford Parser^(11)^(11)11https://nlp.stanford.edu/software/lex-parser.html 和
    CoreNLP^(12)^(12)12https://github.com/stanfordnlp/CoreNLP 的一部分，CoreNLP 是一个 Java
    工具包，用于 NLP，使计算机能够分析、理解、修改或生成自然语言。因此，CoreNLP 也与 IUI 的媒体设计元素相关。构建说明包含在 ReadMe 文件中，并且提供了编写良好的
    HTML 文档^(13)^(13)13https://stanfordnlp.github.io/CoreNLP/。
- en: 'The open source contribution RASA_NLU^(14)^(14)14https://github.com/RasaHQ/rasa_nlu [[9](#bib.bib9)],
    written in Python and published under the Apache-2.0 license, performs natural
    language understanding with intent classification and entity extraction. Several
    machine learning backends can be employed: spaCy^(15)^(15)15https://github.com/explosion/spaCy
    which uses thinc^(16)^(16)16https://github.com/explosion/thinc, a deep learning
    capable library, MITIE^(17)^(17)17https://github.com/mit-nlp/MITIE with dlib as
    backend and scikit-learn^(18)^(18)18https://github.com/scikit-learn/scikit-learn.
    When using docker^(19)^(19)19https://www.docker.com/ the installation is noteworthy
    simple. A single command downloads all required components and dependencies and
    starts the container with the NLU service in minutes (depending on the internet
    connection). A complete installation and getting started guide is available as
    ReadMe. As the source code is consistently annotated, an automatically generated
    Sphinx^(20)^(20)20http://www.sphinx-doc.org HTML documentation is accessible^(21)^(21)21https://rasahq.github.io/rasa_nlu/master/.
    Short questions can be asked in a gitter chat^(22)^(22)22https://gitter.im/RasaHQ/rasa_nlu.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 开源项目 RASA_NLU^(14)^(14)14https://github.com/RasaHQ/rasa_nlu [[9](#bib.bib9)]，由
    Python 编写并在 Apache-2.0 许可证下发布，执行自然语言理解，包括意图分类和实体提取。可以使用几种机器学习后端：spaCy^(15)^(15)15https://github.com/explosion/spaCy，它使用
    thinc^(16)^(16)16https://github.com/explosion/thinc，一个深度学习能力的库；MITIE^(17)^(17)17https://github.com/mit-nlp/MITIE，使用
    dlib 作为后端；以及 scikit-learn^(18)^(18)18https://github.com/scikit-learn/scikit-learn。使用
    docker^(19)^(19)19https://www.docker.com/ 时，安装过程非常简单。一个命令可以下载所有所需的组件和依赖项，并在几分钟内启动带有
    NLU 服务的容器（具体时间取决于互联网连接）。完整的安装和入门指南可以在 ReadMe 中找到。由于源代码始终被注释，自动生成的 Sphinx^(20)^(20)20http://www.sphinx-doc.org
    HTML 文档可以访问^(21)^(21)21https://rasahq.github.io/rasa_nlu/master/。简短的问题可以在 gitter
    chat^(22)^(22)22https://gitter.im/RasaHQ/rasa_nlu 中提出。
- en: In pervasive computing, understanding how the user interacts with the environment
    is essential. Bertasius et al. [[7](#bib.bib7)] designed a deep neural network
    model with Caffe^(23)^(23)23https://github.com/gberta/EgoNet that processes the
    user’s visual and tactile interactions for identifying the action object. The
    authors provide a pre-trained model and the Python code that predicts the area
    of the action object in an RGB(D) image, i.e., depth information is optional and
    can be used to improve the accuracy. Unfortunately, the annotated dataset that
    was created by the authors to train and test the model remains unpublished, thus
    preventing complete verification of the results and model enhancements by third
    parties. The published source is thoroughly annotated but lacks licensing information
    and detailed dependency information. In a follow-up publication, Bertasius et
    al. [[8](#bib.bib8)] demonstrated that the supervised creation of the training
    dataset can be omitted by using segmentation and recognition agents, implemented
    as cross-pathway architecture in a visual-spatial network (VSN). Unsupervised
    learning is desirable, because the training of the action-object detection model
    requires pixelwise annotation of captured images by humans, which is time-intensive
    and costly. The implemented VSN learns to detect likely action-objects from unlabelled
    egocentrically captured image data. The GitHub repository^(24)^(24)24https://github.com/gberta/Visual-Spatial-Network
    contains pre-trained models, the Python code to perform predictions and matlab
    sources for the multiscale combinatorial grouping that is required for the segmentation
    agent. Similarly to the first contribution, the code lacks a license while code
    annotations are sufficient. The ReadMe file contains general training instructions,
    the actual setup of the training toolchain is very cumbersome as merely pointers
    are given.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在普适计算中，理解用户如何与环境互动至关重要。Bertasius 等人[[7](#bib.bib7)] 使用 Caffe^(23)^(23)23https://github.com/gberta/EgoNet
    设计了一个深度神经网络模型，处理用户的视觉和触觉互动以识别行动对象。作者提供了一个预训练模型和Python代码，预测RGB(D)图像中的动作对象区域，即深度信息是可选的，并可以用于提高准确性。不幸的是，作者创建的用于训练和测试模型的标注数据集尚未公开，从而阻碍了第三方对结果和模型改进的完全验证。发布的来源已经充分注释，但缺乏许可信息和详细的依赖信息。在随后的出版物中，Bertasius
    等人[[8](#bib.bib8)] 证明通过使用分割和识别代理来创建训练数据集的监督过程可以省略，这些代理作为视觉空间网络（VSN）中的交叉通路架构实现。无监督学习是可取的，因为动作对象检测模型的训练需要人类对捕获图像进行逐像素标注，这既耗时又昂贵。实现的VSN从未标记的自我中心捕获图像数据中学习检测可能的动作对象。GitHub仓库^(24)^(24)24https://github.com/gberta/Visual-Spatial-Network
    包含预训练模型、进行预测的Python代码以及用于分割代理所需的多尺度组合分组的matlab源代码。与第一个贡献类似，代码缺乏许可证，而代码注释足够。ReadMe文件包含一般训练说明，实际的训练工具链设置非常繁琐，仅给出了指针。
- en: In [[6](#bib.bib6)] Barz and Sonntag used Caffe to combine gaze and egocentric
    camera data with GPU based object classification and attention detection for the
    construction of episodic memories of egocentric events in real-time. The code
    is not yet available, but will be published as a plug-in for the Pupil Labs eye
    tracking suite [[35](#bib.bib35)]^(25)^(25)25https://github.com/pupil-labs/pupil
    in mid of 2018.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[6](#bib.bib6)]中，Barz 和 Sonntag 使用 Caffe 将注视点和自我中心摄像头数据与基于 GPU 的物体分类和注意力检测结合起来，以实时构建自我中心事件的情节记忆。代码尚未公开，但计划于2018年中作为Pupil
    Labs眼动追踪套件的插件发布[[35](#bib.bib35)]^(25)^(25)25https://github.com/pupil-labs/pupil。
- en: 2.2.2 Interaction Management, Output Rendering & Media Design
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 互动管理、输出渲染与媒体设计
- en: User input is interpreted with regard to the current state of considered models,
    e.g., the discourse context, in order to identify and plan future actions and
    design appropriate IUI output. Components described here implement a central dialogue
    management functionality and generate outputs for the users in terms of multimodal
    natural language generation (NLG).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入根据当前模型的状态进行解释，例如话语背景，以便识别和计划未来的行动，并设计适当的IUI输出。这里描述的组件实现了一个核心对话管理功能，并为用户生成多模态自然语言生成（NLG）的输出。
- en: RASA_CORE^(26)^(26)26https://github.com/RasaHQ/rasa_core [[9](#bib.bib9)] is
    an open source discourse/dialogue manager framework which is written in Python
    and uses Keras’ LSTM implementation in order to allow contextual, layered conversations.
    While no docker image is available, the documentation^(27)^(27)27https://core.rasa.ai/
    quality is on par with RASA_NLU, a chat is analogously available^(28)^(28)28https://gitter.im/RasaHQ/rasa_core.
    Four example chatbots are provided^(29)^(29)29https://github.com/RasaHQ/rasa_core/tree/master/examples
    for getting started easily as well as a number of tutorials^(30)^(30)30https://core.rasa.ai/tutorial_basics.html.
    In an evaluation, Braun et al. [[10](#bib.bib10)] found the RASA ensemble to score
    second best against commercial closed source systems. Note that RASA_CORE also
    fits the media design module as it can generate responses and clarification questions
    in a conversational system on its own. The capabilities can be extended by using
    more sophisticated NLG techniques (see Gatt and Kramer [[24](#bib.bib24)] for
    a recent overview).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RASA_CORE^(26)^(26)26https://github.com/RasaHQ/rasa_core [[9](#bib.bib9)] 是一个开源话语/对话管理框架，使用Python编写，利用Keras的LSTM实现来支持上下文相关的分层对话。虽然没有提供docker镜像，但文档质量^(27)^(27)27https://core.rasa.ai/
    与RASA_NLU相当，类似地可以进行聊天^(28)^(28)28https://gitter.im/RasaHQ/rasa_core。提供了四个示例聊天机器人^(29)^(29)29https://github.com/RasaHQ/rasa_core/tree/master/examples，以便轻松入门，并有多个教程^(30)^(30)30https://core.rasa.ai/tutorial_basics.html。在一项评估中，Braun等人[[10](#bib.bib10)]发现RASA集成系统在与商业闭源系统比较时得分第二。请注意，RASA_CORE也适用于媒体设计模块，因为它可以在对话系统中独立生成响应和澄清问题。通过使用更复杂的NLG技术，可以扩展其功能（参见Gatt和Kramer
    [[24](#bib.bib24)]的最新概述）。
- en: 'In [[20](#bib.bib20), [57](#bib.bib57)] a deep reinforcement learning system
    for optimising a visually grounded goal-directed dialogue system was implemented
    using TensorFlow. GuessWhat?!^(31)^(31)31https://github.com/GuessWhatGame/guesswhat/,
    the corresponding open source contribution, is moderately annotated and contains
    a ReadMe file that allows verification of the published results by outlining required
    steps for their reproduction. Pre-trained models are available for download and
    basic installation instructions are contained as well. Unfortunately, the authors
    omitted correct Python dependency version information which leads to some trial-and-error
    during installation. This contribution uses the Apache-2.0 license.^(32)^(32)32Update
    in future revisions: https://github.com/voicy-ai/DialogStateTracking'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[20](#bib.bib20), [57](#bib.bib57)]中，使用TensorFlow实现了一个深度强化学习系统，以优化视觉基础的目标导向对话系统。GuessWhat?!^(31)^(31)31https://github.com/GuessWhatGame/guesswhat/，其对应的开源贡献，注释适中，并包含一个ReadMe文件，通过概述重现结果所需的步骤来验证已发布的结果。预训练模型可以下载，基本的安装说明也包含其中。不幸的是，作者省略了正确的Python依赖版本信息，这导致安装过程中需要一些试错。这一贡献使用Apache-2.0许可证。^(32)^(32)32未来修订更新：
    https://github.com/voicy-ai/DialogStateTracking
- en: 2.2.3 Backend
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 后端
- en: Visual scene understanding is an important property of an IUI which needs to
    process image or video input. From a technical perspective, this requires image
    classification and accurate region proposals so that a meaningful segmentation
    of a scene with multiple objects is possible. Sonntag et al. contributed the py-faster-rcnn-ft^(33)^(33)33https://github.com/DFKI-Interactive-Machine-Learning/py-faster-rcnn-ft
    Python library that allows convenient fine-tuning of deep learning models that
    offer this functionality. E.g., the VGG_CNN_M_1024 model [[12](#bib.bib12)] can
    be fine-tuned on specific categories of the MS COCO [[40](#bib.bib40)] image dataset,
    hence improving classification accuracy [[25](#bib.bib25)]. While the library
    can work entirely in the background, it also comes with an user interface allowing
    to select categories and inspect the results graphically. The software uses the
    Caffe framework and is GPL-3 licensed. It features an extensive ReadMe file containing
    an installation and getting started guide with example listings and optional pre-trained
    model resources for quick evaluation. This work is based on py-faster-rcnn^(34)^(34)34https://github.com/rbgirshick/py-faster-rcnn [[48](#bib.bib48)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉场景理解是交互式用户界面（IUI）的一个重要特性，需要处理图像或视频输入。从技术角度来看，这要求图像分类和准确的区域建议，以便对具有多个对象的场景进行有意义的分割。Sonntag
    等人贡献了 py-faster-rcnn-ft^(33)^(33)33https://github.com/DFKI-Interactive-Machine-Learning/py-faster-rcnn-ft
    Python 库，使得深度学习模型的便捷微调成为可能，从而提供这种功能。例如，VGG_CNN_M_1024 模型 [[12](#bib.bib12)] 可以在
    MS COCO [[40](#bib.bib40)] 图像数据集的特定类别上进行微调，从而提高分类准确性 [[25](#bib.bib25)]。虽然该库可以完全在后台运行，但它也提供了一个用户界面，允许选择类别并以图形方式检查结果。该软件使用
    Caffe 框架，并采用 GPL-3 许可证。它包含一个详细的 ReadMe 文件，其中包含安装和入门指南、示例列表以及用于快速评估的可选预训练模型资源。这项工作基于
    py-faster-rcnn^(34)^(34)34https://github.com/rbgirshick/py-faster-rcnn [[48](#bib.bib48)]。
- en: Nvidia’s DIGITS^(35)^(35)35https://github.com/NVIDIA/DIGITS enables deep learning
    beginners to design and train models to solve image classification problems and
    put these models to use. The Python software features an intelligent web (HTML/JS)
    interface that allows highly interactive modification of the neural network by
    the user as well as data management and fine-tuning of existing models. The DIGITS
    interface displays performance statistics in real time so that the user can quickly
    identify and select the best performing model for deployment. On the other hand,
    DIGITS can also be used as a backend component of an IUI via its REST API to perform
    inference on trained models. The software is BSD-3 licensed and can use Caffe,
    Torch, and Tensorflow as deep learning framework. The installation via docker
    requires little effort and the accompanying ReadMe file links to multiple well-grounded
    howto guides. A graphically enriched documentation and introduction to deep learning
    is available from Nvidia^(36)^(36)36https://devblogs.nvidia.com/parallelforall/digits-deep-learning-gpu-training-system/.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Nvidia 的 DIGITS^(35)^(35)35https://github.com/NVIDIA/DIGITS 使深度学习初学者能够设计和训练模型以解决图像分类问题，并将这些模型投入使用。该
    Python 软件具有智能的 Web (HTML/JS) 界面，允许用户对神经网络进行高度交互式的修改，并进行数据管理和现有模型的微调。DIGITS 界面实时显示性能统计信息，以便用户可以快速识别和选择最佳的模型进行部署。另一方面，DIGITS
    还可以通过其 REST API 作为 IUI 的后端组件，用于对训练好的模型进行推断。该软件采用 BSD-3 许可证，可以使用 Caffe、Torch 和
    Tensorflow 作为深度学习框架。通过 docker 安装几乎不需要额外努力，随附的 ReadMe 文件链接到多个成熟的教程指南。Nvidia 提供了图形丰富的文档和深度学习入门介绍^(36)^(36)36https://devblogs.nvidia.com/parallelforall/digits-deep-learning-gpu-training-system/。
- en: '3 Outlook: Interactive Machine Learning'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 展望：交互式机器学习
- en: 'To develop the positive aspects of artificial intelligence, manage its risks
    and challenges, and ensure that everyone has the opportunity to help in building
    an AI-enhanced society and to participate in its benefits, we suggest using a
    methodology where human intelligences & machine learning take the center stage:
    Interactive Machine Learning is the design and implementation of algorithms and
    intelligent user interface frameworks that facilitate machine learning with the
    help of human interaction.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发人工智能的积极方面，管理其风险和挑战，并确保每个人都有机会参与建设 AI 增强社会并分享其成果，我们建议采用一种方法论，其中人类智能与机器学习处于中心位置：交互式机器学习是设计和实现算法和智能用户界面框架，以便通过人类互动促进机器学习。
- en: This field of research explores the possibilities of helping AI systems to achieve
    their full potential, based on interaction with humans. The current misconception
    is that artificial intelligence is more likely to be performance oriented than
    learning oriented. By machine teaching [[52](#bib.bib52)] we can "assist" AI systems
    in becoming self-sustaining, "lifelong" learners [[39](#bib.bib39), [55](#bib.bib55)]
    as a domain expert trains complex models by encapsulating the required mechanics
    of machine learning. This resource oriented methodology contributes in closing
    the gap between the demand for machine learning models and relatively low amount
    of experts capable of creating them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这一研究领域探讨了帮助AI系统发挥其全部潜力的可能性，基于与人类的互动。目前的误解是，人工智能更倾向于以性能为导向而非以学习为导向。通过机器教学[[52](#bib.bib52)]，我们可以“辅助”AI系统成为自我维持的“终身”学习者[[39](#bib.bib39),
    [55](#bib.bib55)]，正如领域专家通过封装机器学习所需的机制来训练复杂模型。这种资源导向的方法有助于弥合对机器学习模型的需求与能够创建这些模型的专家数量之间的差距。
- en: 'Interactive machine learning (IML) includes feedback in real-time, allowing
    fast-paced iterative model improvements through intelligent interactions with
    a user [[4](#bib.bib4)]. As shown by Sonntag, when using artificial intelligence
    (AI) to implement intelligent automated capabilities in an IUI, effects on HCI
    must be considered in order to prevent negative side-effects like diminished predictability
    and lost controllability which ultimately impact the usability of the IUI [[54](#bib.bib54)].
    This adverse effect can be diminished by adopting the concept of the binocular
    view when building IUIs: both AI and HCI aspects are simultaneously addressed
    so that the systems intelligence, and how the user should be able to interact
    with it, are optimally synchronised [[30](#bib.bib30)].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 互动机器学习（IML）包括实时反馈，通过与用户的智能互动，实现快速的迭代模型改进[[4](#bib.bib4)]。正如Sonntag所示，当在IUI中使用人工智能（AI）实施智能自动化功能时，必须考虑对HCI的影响，以防止如预测能力下降和可控性丧失等负面副作用，这些最终会影响IUI的可用性[[54](#bib.bib54)]。通过在构建IUI时采纳双目视角的概念，这种不利影响可以得到缓解：同时解决AI和HCI方面的问题，从而使系统智能和用户与之交互的方式得到最佳同步[[30](#bib.bib30)]。
- en: Many principles used in active learning are adopted, for example, query strategies
    for selecting most influential samples that shall be labelled [[23](#bib.bib23),
    [50](#bib.bib50)] and semi-supervised learning for the automatic propagation of
    labels under confidence constraints [[50](#bib.bib50)]. IML benefits from IUIs
    that support the user in training/teaching a model and is, at the same time, essential
    for building and maintaining models for intelligent and multimodal interaction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 许多在主动学习中使用的原则被采纳，例如用于选择最具影响力样本的查询策略，这些样本将被标注[[23](#bib.bib23), [50](#bib.bib50)]，以及在置信约束下标签自动传播的半监督学习[[50](#bib.bib50)]。IML从支持用户训练/教学模型的IUI中受益，同时，这对于构建和维护智能和多模态交互模型是必不可少的。
- en: 'Recent works use deep learning in combination with active or passive user input
    to improve the model training or performance: Green et al. [[27](#bib.bib27)]
    applied the IML concept for a language translation task that benefits from human
    users and machine agents: the human in the loop can produce higher quality translations
    while the suggested machine translation is continuously improved as the human
    corrects its suggestions. Venkitasubramanian et al. [[44](#bib.bib44)] present
    a model that learns to recognise animals by watching documentaries. They implicitly
    involve humans by incorporating their gaze signal together with the subtitles
    of a movie as weak supervision signal. Their classifiers learns using image representations
    from pre-trained convolutional neural networks (CNN). Jiang et al. [[33](#bib.bib33)]
    present an algorithm for learning new object classes and corresponding relations
    in a human-robot dialogue using CNN-based features. The relations are used to
    reason about future scenarios where known faces and objects are recognised. Cognolato
    et al. [[16](#bib.bib16)] use human gaze and hand movements to sample images of
    objects that get manipulated and fine-tune a CNN with that data. In the context
    of active learning, Käding et al. [[34](#bib.bib34)] investigate the trade-off
    between model quality and the computational effort of fine-tuning for continuously
    changing models. Jiang et al. [[32](#bib.bib32)] present a GPU-accelerated framework
    for interactive machine learning that allows easy model adoption and provides
    several result visualisations to support users.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究结合深度学习和主动或被动用户输入，以改进模型训练或性能：Green等人[[27](#bib.bib27)]将IML概念应用于语言翻译任务，该任务受益于人类用户和机器代理：人类在环中可以产生更高质量的翻译，同时机器翻译的建议会随着人类纠正建议而不断改进。Venkitasubramanian等人[[44](#bib.bib44)]提出了一种通过观看纪录片学习识别动物的模型。他们通过将人类的注视信号与电影字幕结合作为弱监督信号来隐式地涉及人类。其分类器使用来自预训练卷积神经网络（CNN）的图像表示进行学习。姜等人[[33](#bib.bib33)]提出了一种算法，用于在基于CNN特征的人机对话中学习新的对象类别及其相应的关系。这些关系用于推理未来场景，其中已知的面孔和对象被识别。Cognolato等人[[16](#bib.bib16)]利用人类的注视和手部动作来采样被操作的物体图像，并使用这些数据对CNN进行微调。在主动学习的背景下，Käding等人[[34](#bib.bib34)]研究了模型质量与对不断变化的模型进行微调的计算成本之间的权衡。姜等人[[32](#bib.bib32)]提出了一个GPU加速的互动式机器学习框架，允许轻松地采纳模型，并提供多种结果可视化以支持用户。
- en: In [[32](#bib.bib32)] techniques for customised and interactive model optimisation
    are proposed. Jiang and Canny use the BIDMach framework [[11](#bib.bib11)] which
    builds upon Caffe to provide a machine learning architecture which is modular
    and supports primary and secondary loss functions. The users of this system are
    able to directly manipulate deep learning model parameters during training. The
    user can perform model optimisations with the help of interactive visualisation
    tools and controls via a web interface. It however remains challenging to transfer
    the concepts of interactive machine learning to deep learning algorithms and to
    investigate their impact on model performance and usability, particularly in the
    context of IUIs and multimodal settings.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[32](#bib.bib32)]中提出了定制化和互动式模型优化的技术。姜和坎尼使用了基于Caffe构建的BIDMach框架[[11](#bib.bib11)]，该框架提供了一个模块化的机器学习架构，支持主损失函数和次损失函数。该系统的用户能够在训练过程中直接调整深度学习模型参数。用户可以通过网络接口的互动可视化工具和控制进行模型优化。然而，将互动式机器学习的概念转移到深度学习算法上，并研究其对模型性能和可用性的影响，尤其是在IUIs和多模态设置中的影响，仍然具有挑战性。
- en: References
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1]'
- en: '[2] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
    Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz,
    Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike
    Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
    Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine
    Learning on Heterogeneous Distributed Systems. (2015). [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
    Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz,
    Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike
    Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,
    Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu, 和 Xiaoqiang Zheng。2015年。《TensorFlow: Large-Scale Machine
    Learning on Heterogeneous Distributed Systems》。 (2015). [https://www.tensorflow.org/](https://www.tensorflow.org/)'
- en: '[3] Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
    Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov,
    Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin
    Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski,
    Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier,
    Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté,
    Myriam Côté, Aaron Courville, Yann N. Dauphin, Olivier Delalleau, Julien Demouth,
    Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent
    Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu
    Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe
    Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun
    Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal
    Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux,
    Nicholas Léonard, Zhouhan Lin, Jesse A. Livezey, Cory Lorenz, Jeremiah Lowin,
    Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T. McGibbon,
    Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto
    Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel
    Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier,
    François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,
    Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S. Ramana
    Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian,
    Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley,
    Dustin J. Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang,
    and Ying Zhang. 2016. Theano: A Python framework for fast computation of mathematical
    expressions. (2016), 1–19. [http://arxiv.org/abs/1605.02688](http://arxiv.org/abs/1605.02688)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
    Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov,
    Alexander Belopolsky, Yoshua Bengio, Arnaud Bergeron, James Bergstra, Valentin
    Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski,
    Xavier Bouthillier, Alexandre de Brébisson, Olivier Breuleux, Pierre-Luc Carrier,
    Kyunghyun Cho, Jan Chorowski, Paul Christiano, Tim Cooijmans, Marc-Alexandre Côté,
    Myriam Côté, Aaron Courville, Yann N. Dauphin, Olivier Delalleau, Julien Demouth,
    Guillaume Desjardins, Sander Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent
    Dumoulin, Samira Ebrahimi Kahou, Dumitru Erhan, Ziye Fan, Orhan Firat, Mathieu
    Germain, Xavier Glorot, Ian Goodfellow, Matt Graham, Caglar Gulcehre, Philippe
    Hamel, Iban Harlouchet, Jean-Philippe Heng, Balázs Hidasi, Sina Honari, Arjun
    Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal
    Lamblin, Eric Larsen, César Laurent, Sean Lee, Simon Lefrancois, Simon Lemieux,
    Nicholas Léonard, Zhouhan Lin, Jesse A. Livezey, Cory Lorenz, Jeremiah Lowin,
    Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, Robert T. McGibbon,
    Roland Memisevic, Bart van Merriënboer, Vincent Michalski, Mehdi Mirza, Alberto
    Orlandi, Christopher Pal, Razvan Pascanu, Mohammad Pezeshki, Colin Raffel, Daniel
    Renshaw, Matthew Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, John Salvatier,
    François Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Vlad Serban,
    Dmitriy Serdyuk, Samira Shabanian, Étienne Simon, Sigurd Spieckermann, S. Ramana
    Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph Turian,
    Sebastian Urban, Pascal Vincent, Francesco Visin, Harm de Vries, David Warde-Farley,
    Dustin J. Webb, Matthew Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang,
    和 Ying Zhang。2016年。《Theano: A Python framework for fast computation of mathematical
    expressions》。 (2016), 1–19. [http://arxiv.org/abs/1605.02688](http://arxiv.org/abs/1605.02688)'
- en: '[4] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014.
    Power to the People: The Role of Humans in Interactive Machine Learning. AI Magazine
    35, 4 (dec 2014), 105. DOI:[http://dx.doi.org/10.1609/aimag.v35i4.2513](http://dx.doi.org/10.1609/aimag.v35i4.2513)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Saleema Amershi、Maya Cakmak、William Bradley Knox 和 Todd Kulesza。2014。赋权于人民：人类在互动机器学习中的作用。AI杂志35，第4期（2014年12月），105。DOI：[http://dx.doi.org/10.1609/aimag.v35i4.2513](http://dx.doi.org/10.1609/aimag.v35i4.2513)'
- en: '[5] Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, and Mohak Shah. 2016.
    Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning. (2016),
    1–11.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Soheil Bahrampour、Naveen Ramakrishnan、Lukas Schott 和 Mohak Shah。2016。Caffe、Neon、Theano
    和 Torch 的深度学习比较研究。（2016），1–11。'
- en: '[6] Michael Barz and Daniel Sonntag. 2016. Gaze-guided object classification
    using deep neural networks for attention-based computing. In Proceedings of the
    2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing
    Adjunct - UbiComp ’16. ACM Press, New York, New York, USA, 253–256. DOI:[http://dx.doi.org/10.1145/2968219.2971389](http://dx.doi.org/10.1145/2968219.2971389)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Michael Barz 和 Daniel Sonntag。2016。利用深度神经网络进行注视引导的对象分类。发表于2016年ACM国际联合会议论文集
    - UbiComp ’16。ACM Press，纽约，美国，253–256。DOI：[http://dx.doi.org/10.1145/2968219.2971389](http://dx.doi.org/10.1145/2968219.2971389)'
- en: '[7] Gedas Bertasius, Hyun Soo Park, Stella X. Yu, and Jianbo Shi. 2017a. First
    Person Action-Object Detection with EgoNet. In Proceedings of Robotics: Science
    and Systems. [http://arxiv.org/abs/1603.04908](http://arxiv.org/abs/1603.04908)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gedas Bertasius、Hyun Soo Park、Stella X. Yu 和 Jianbo Shi。2017a。使用EgoNet的第一人称动作-对象检测。发表于机器人：科学与系统会议论文集。[http://arxiv.org/abs/1603.04908](http://arxiv.org/abs/1603.04908)'
- en: '[8] Gedas Bertasius, Hyun Soo Park, Stella X. Yu, and Jianbo Shi. 2017b. Unsupervised
    Learning of Important Objects from First-Person Videos. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition. 1956–1964. DOI:[http://dx.doi.org/10.1109/ICCV.2017.216](http://dx.doi.org/10.1109/ICCV.2017.216)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Gedas Bertasius、Hyun Soo Park、Stella X. Yu 和 Jianbo Shi。2017b。无监督学习从第一人称视频中提取重要对象。发表于IEEE计算机视觉与模式识别会议论文集。1956–1964。DOI：[http://dx.doi.org/10.1109/ICCV.2017.216](http://dx.doi.org/10.1109/ICCV.2017.216)'
- en: '[9] Tom Bocklisch, Joey Faulkner, Nick Pawlowski, and Alan Nichol. 2017. Rasa:
    Open Source Language Understanding and Dialogue Management. (dec 2017). [http://arxiv.org/abs/1712.05181](http://arxiv.org/abs/1712.05181)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tom Bocklisch、Joey Faulkner、Nick Pawlowski 和 Alan Nichol。2017。Rasa：开源语言理解与对话管理。（2017年12月）。[http://arxiv.org/abs/1712.05181](http://arxiv.org/abs/1712.05181)'
- en: '[10] Daniel Braun and Manfred Langen. 2017. Evaluating Natural Language Understanding
    Services for Conversational Question Answering Systems. August (2017), 174–185.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Daniel Braun 和 Manfred Langen。2017。评估对话问答系统的自然语言理解服务。8月（2017），174–185。'
- en: '[11] John Canny and Huasha Zhao. 2013. Big Data Analytics with Small Footprint
    : Squaring the Cloud. Proceedings of the 19th ACM SIGKDD international conference
    on Knowledge discovery and data mining. ACM (2013), 95–103. DOI:[http://dx.doi.org/10.1145/2487575.2487677](http://dx.doi.org/10.1145/2487575.2487677)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] John Canny 和 Huasha Zhao。2013。小脚印的大数据分析：平方云。发表于第19届ACM SIGKDD国际知识发现与数据挖掘会议论文集。ACM（2013），95–103。DOI：[http://dx.doi.org/10.1145/2487575.2487677](http://dx.doi.org/10.1145/2487575.2487677)'
- en: '[12] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014.
    Return of the Devil in the Details: Delving Deep into Convolutional Nets. British
    Machine Vision Conference (2014). DOI:[http://dx.doi.org/10.5244/C.28.6](http://dx.doi.org/10.5244/C.28.6)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Ken Chatfield、Karen Simonyan、Andrea Vedaldi 和 Andrew Zisserman。2014。细节中的魔鬼归来：深入探讨卷积网络。英国机器视觉会议（2014）。DOI：[http://dx.doi.org/10.5244/C.28.6](http://dx.doi.org/10.5244/C.28.6)'
- en: '[13] Danqi Chen and Christopher Manning. 2014. A Fast and Accurate Dependency
    Parser using Neural Networks. Proceedings of the 2014 Conference on Empirical
    Methods in Natural Language Processing (EMNLP) i (2014), 740–750. DOI:[http://dx.doi.org/10.3115/v1/D14-1082](http://dx.doi.org/10.3115/v1/D14-1082)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Danqi Chen 和 Christopher Manning。2014。使用神经网络的快速准确依存解析器。发表于2014年自然语言处理实证方法会议（EMNLP）论文集，740–750。DOI：[http://dx.doi.org/10.3115/v1/D14-1082](http://dx.doi.org/10.3115/v1/D14-1082)'
- en: '[14] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
    Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible and Efficient
    Machine Learning Library for Heterogeneous Distributed Systems. (2015), 1–6. DOI:[http://dx.doi.org/10.1145/2532637](http://dx.doi.org/10.1145/2532637)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tianqi Chen、Mu Li、Yutian Li、Min Lin、Naiyan Wang、Minjie Wang、Tianjun Xiao、Bing
    Xu、Chiyuan Zhang 和 Zheng Zhang。2015。MXNet：一个灵活高效的异构分布式系统机器学习库。（2015），1–6。DOI：[http://dx.doi.org/10.1145/2532637](http://dx.doi.org/10.1145/2532637)'
- en: '[15] François Chollet and Others. 2015. Keras. (2015). [https://github.com/fchollet/keras](https://github.com/fchollet/keras)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] François Chollet 等人。2015。Keras。（2015）。[https://github.com/fchollet/keras](https://github.com/fchollet/keras)'
- en: '[16] Matteo Cognolato, Mara Graziani, Francesca Giordaniello, Gianluca Saetta,
    Franco Bassetto, Peter Brugger, Barbara Caputo, Henning Müller, and Manfredo Atzori.
    2017. Semi-automatic training of an object recognition system in scene camera
    data using gaze tracking and accelerometers. In International Conference on Computer
    Vision Systems (ICVS).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Matteo Cognolato, Mara Graziani, Francesca Giordaniello, Gianluca Saetta,
    Franco Bassetto, Peter Brugger, Barbara Caputo, Henning Müller 和 Manfredo Atzori。2017。使用凝视追踪和加速度计在场景相机数据中半自动训练物体识别系统。在国际计算机视觉系统会议（ICVS）。'
- en: '[17] Ronan Collobert, Koray Kavukcuoglu, and Clément Farabet. 2011. Torch7:
    A matlab-like environment for machine learning. BigLearn, NIPS Workshop (2011),
    1–6. [http://infoscience.epfl.ch/record/192376/files/Collobert](http://infoscience.epfl.ch/record/192376/files/Collobert)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Ronan Collobert, Koray Kavukcuoglu 和 Clément Farabet。2011。Torch7: 类似 Matlab
    的机器学习环境。BigLearn, NIPS 研讨会（2011），1–6。[http://infoscience.epfl.ch/record/192376/files/Collobert](http://infoscience.epfl.ch/record/192376/files/Collobert)'
- en: '[18] Committee on Technology. 2016. Preparing for the Future of Artificial
    Intelligence. Technical Report. Executive Office of the President of the United
    States, National Science and Technology Council, Committee on Technology. [https://obamawhitehouse.archives.gov/sites/default/files/whitehouse](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 技术委员会。2016。为人工智能的未来做准备。技术报告。美国总统执行办公室，国家科学技术委员会，技术委员会。[https://obamawhitehouse.archives.gov/sites/default/files/whitehouse](https://obamawhitehouse.archives.gov/sites/default/files/whitehouse)'
- en: '[19] Aymeric Damien and Others. 2016. TFLearn. (2016). [https://github.com/tflearn/tflearn](https://github.com/tflearn/tflearn)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Aymeric Damien 等人。2016。TFLearn。（2016）。[https://github.com/tflearn/tflearn](https://github.com/tflearn/tflearn)'
- en: '[20] Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle,
    and Aaron Courville. 2017. GuessWhat?! Visual object discovery through multi-modal
    dialogue. Conference on Computer Vision and Pattern Recognition (2017). DOI:[http://dx.doi.org/10.1109/CVPR.2017.475](http://dx.doi.org/10.1109/CVPR.2017.475)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle
    和 Aaron Courville。2017。GuessWhat?! 通过多模态对话发现视觉对象。计算机视觉与模式识别会议（2017）。DOI:[http://dx.doi.org/10.1109/CVPR.2017.475](http://dx.doi.org/10.1109/CVPR.2017.475)'
- en: '[21] Deeplearning4j Development Team. 2018. Deeplearning4j: Open-source distributed
    deep learning for the JVM. (2018). [http://deeplearning4j.org](http://deeplearning4j.org)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Deeplearning4j 开发团队。2018。Deeplearning4j: JVM 上的开源分布式深度学习。（2018）。[http://deeplearning4j.org](http://deeplearning4j.org)'
- en: '[22] Bradley J. Erickson, Panagiotis Korfiatis, Zeynettin Akkus, Timothy Kline,
    and Kenneth Philbrick. 2017. Toolkits and Libraries for Deep Learning. Journal
    of Digital Imaging 30, 4 (2017), 400–405. DOI:[http://dx.doi.org/10.1007/s10278-017-9965-6](http://dx.doi.org/10.1007/s10278-017-9965-6)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Bradley J. Erickson, Panagiotis Korfiatis, Zeynettin Akkus, Timothy Kline
    和 Kenneth Philbrick。2017。深度学习的工具包和库。数字成像期刊 30, 4（2017），400–405。DOI:[http://dx.doi.org/10.1007/s10278-017-9965-6](http://dx.doi.org/10.1007/s10278-017-9965-6)'
- en: '[23] James Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik:
    Interactive Concept Learning in Image Search. In Proceeding of the twenty-sixth
    annual CHI conference on Human factors in computing systems - CHI ’08. ACM Press,
    New York, New York, USA, 29. DOI:[http://dx.doi.org/10.1145/1357054.1357061](http://dx.doi.org/10.1145/1357054.1357061)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] James Fogarty, Desney Tan, Ashish Kapoor 和 Simon Winder。2008。CueFlik:
    图像搜索中的互动概念学习。在第二十六届年度 CHI 会议论文集 - CHI ’08。ACM Press，纽约，美国，29。DOI:[http://dx.doi.org/10.1145/1357054.1357061](http://dx.doi.org/10.1145/1357054.1357061)'
- en: '[24] Albert Gatt and Emiel Krahmer. 2018. Survey of the State of the Art in
    Natural Language Generation: Core tasks, applications and evaluation. Journal
    of Artificial Intelligence Research 61 (mar 2018), 65–170. [http://dx.doi.org/10.1613/jair.5477http://arxiv.org/abs/1703.09902](http://dx.doi.org/10.1613/jair.5477http://arxiv.org/abs/1703.09902)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Albert Gatt 和 Emiel Krahmer。2018。自然语言生成的现状调查：核心任务、应用和评估。人工智能研究期刊 61（2018年3月），65–170。[http://dx.doi.org/10.1613/jair.5477http://arxiv.org/abs/1703.09902](http://dx.doi.org/10.1613/jair.5477http://arxiv.org/abs/1703.09902)'
- en: '[25] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014.
    Rich feature hierarchies for accurate object detection and semantic segmentation.
    Proceedings of the IEEE conference on computer vision and pattern recognition
    (2014), 580–587. DOI:[http://dx.doi.org/10.1109/CVPR.2014.81](http://dx.doi.org/10.1109/CVPR.2014.81)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Ross Girshick、Jeff Donahue、Trevor Darrell 和 Jitendra Malik. 2014. 用于准确目标检测和语义分割的丰富特征层次.
    《IEEE 计算机视觉与模式识别会议论文集》（2014），580–587。DOI:[http://dx.doi.org/10.1109/CVPR.2014.81](http://dx.doi.org/10.1109/CVPR.2014.81)'
- en: '[26] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning.
    MIT Press.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ian Goodfellow、Yoshua Bengio 和 Aaron Courville. 2016. 深度学习. MIT Press.'
- en: '[27] Spence Green, Jeffrey Heer, and Christopher D. Manning. 2015. Natural
    Language Translation at the Intersection of AI and HCI. Queue 13, 6 (2015), 1–13.
    DOI:[http://dx.doi.org/10.1145/2791301.2798086](http://dx.doi.org/10.1145/2791301.2798086)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Spence Green、Jeffrey Heer 和 Christopher D. Manning. 2015. 在 AI 和 HCI 交汇处的自然语言翻译.
    《Queue》13，6（2015），1–13。DOI:[http://dx.doi.org/10.1145/2791301.2798086](http://dx.doi.org/10.1145/2791301.2798086)'
- en: '[28] Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan.
    2016. Cooperative Inverse Reinforcement Learning. In Advances in Neural Information
    Processing Systems 29, D D Lee, M Sugiyama, U V Luxburg, I Guyon, and R Garnett
    (Eds.). Curran Associates, Inc., 3909–3917. [http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf](http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Dylan Hadfield-Menell、Stuart J Russell、Pieter Abbeel 和 Anca Dragan. 2016.
    合作逆强化学习. 收录于《神经信息处理系统进展 29》，由 D D Lee、M Sugiyama、U V Luxburg、I Guyon 和 R Garnett
    编集。Curran Associates, Inc.，3909–3917。[http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf](http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf)'
- en: '[29] Johann Hauswald, Yiping Kang, Michael A. Laurenzano, Quan Chen, Cheng
    Li, Trevor Mudge, Ronald G. Dreslinski, Jason Mars, and Lingjia Tang. 2015. DjiNN
    and Tonic: DNN as a service and its implications for future warehouse scale computers.
    In Proceedings of the 42nd Annual International Symposium on Computer Architecture
    - ISCA ’15. ACM, 27–40. DOI:[http://dx.doi.org/10.1145/2749469.2749472](http://dx.doi.org/10.1145/2749469.2749472)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Johann Hauswald、Yiping Kang、Michael A. Laurenzano、Quan Chen、Cheng Li、Trevor
    Mudge、Ronald G. Dreslinski、Jason Mars 和 Lingjia Tang. 2015. DjiNN 和 Tonic: 将 DNN
    作为服务及其对未来仓库级计算机的影响. 收录于《第42届国际计算机架构年会 - ISCA ’15 论文集》。ACM，27–40。DOI:[http://dx.doi.org/10.1145/2749469.2749472](http://dx.doi.org/10.1145/2749469.2749472)'
- en: '[30] Anthony Jameson, Aaron Spaulding, and Neil Yorke-Smith. 2009. Introduction
    to the Special Issue on “Usable AI”. AI Magazine 30, 4 (2009), 11–15.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Anthony Jameson、Aaron Spaulding 和 Neil Yorke-Smith. 2009. “可用 AI” 特刊导言.
    《AI 杂志》30，4（2009），11–15.'
- en: '[31] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
    Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional
    Architecture for Fast Feature Embedding. (2014). DOI:[http://dx.doi.org/10.1145/2647868.2654889](http://dx.doi.org/10.1145/2647868.2654889)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Yangqing Jia、Evan Shelhamer、Jeff Donahue、Sergey Karayev、Jonathan Long、Ross
    Girshick、Sergio Guadarrama 和 Trevor Darrell. 2014. Caffe: 快速特征嵌入的卷积架构.（2014）。DOI:[http://dx.doi.org/10.1145/2647868.2654889](http://dx.doi.org/10.1145/2647868.2654889)'
- en: '[32] Biye Jiang and John Canny. 2017. Interactive Machine Learning via a GPU-accelerated
    Toolkit. In Proceedings of the 22nd International Conference on Intelligent User
    Interfaces - IUI ’17. ACM Press, New York, New York, USA, 535–546. DOI:[http://dx.doi.org/10.1145/3025171.3025172](http://dx.doi.org/10.1145/3025171.3025172)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Biye Jiang 和 John Canny. 2017. 通过 GPU 加速工具包进行交互式机器学习. 收录于《第22届智能用户界面国际会议论文集
    - IUI ’17》。ACM Press, New York, New York, USA，535–546。DOI:[http://dx.doi.org/10.1145/3025171.3025172](http://dx.doi.org/10.1145/3025171.3025172)'
- en: '[33] Shuqiang Jiang, Weiqing Min, Xue Li, Huayang Wang, Jian Sun, and Jiaqi
    Zhou. 2017. Dual Track Multimodal Automatic Learning through Human-Robot Interaction.
    In Proceedings of the Twenty-Sixth International Joint Conference on Artificial
    Intelligence. International Joint Conferences on Artificial Intelligence Organization,
    4485–4491. DOI:[http://dx.doi.org/10.24963/ijcai.2017/626](http://dx.doi.org/10.24963/ijcai.2017/626)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Shuqiang Jiang、Weiqing Min、Xue Li、Huayang Wang、Jian Sun 和 Jiaqi Zhou.
    2017. 通过人机交互的双轨多模态自动学习. 收录于《第二十六届国际联合人工智能会议论文集》。国际联合人工智能组织，4485–4491。DOI:[http://dx.doi.org/10.24963/ijcai.2017/626](http://dx.doi.org/10.24963/ijcai.2017/626)'
- en: '[34] Christoph Käding, Erik Rodner, Alexander Freytag, and Joachim Denzler.
    2017. Fine-Tuning Deep Neural Networks in Continuous Learning Scenarios. In Computer
    Vision – ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan,
    November 20-24, 2016, Revised Selected Papers, Part III, Chu-Song Chen, Jiwen
    Lu, and Kai-Kuang Ma (Eds.). Springer International Publishing, Cham, 588–605.
    DOI:[http://dx.doi.org/10.1007/978-3-319-54526-4_43](http://dx.doi.org/10.1007/978-3-319-54526-4_43)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Christoph Käding, Erik Rodner, Alexander Freytag, and Joachim Denzler.
    2017. Fine-Tuning Deep Neural Networks in Continuous Learning Scenarios. In Computer
    Vision – ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan,
    November 20-24, 2016, Revised Selected Papers, Part III, Chu-Song Chen, Jiwen
    Lu, and Kai-Kuang Ma (Eds.). Springer International Publishing, Cham, 588–605.
    DOI:[http://dx.doi.org/10.1007/978-3-319-54526-4_43](http://dx.doi.org/10.1007/978-3-319-54526-4_43)'
- en: '[35] Moritz Kassner, William Patera, and Andreas Bulling. 2014. Pupil: An Open
    Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction.
    In Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive
    and Ubiquitous Computing (UbiComp ’14 Adjunct). ACM, New York, NY, USA, 1151–1160.
    DOI:[http://dx.doi.org/10.1145/2638728.2641695](http://dx.doi.org/10.1145/2638728.2641695)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Moritz Kassner, William Patera, and Andreas Bulling. 2014. Pupil: An Open
    Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction.
    In Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive
    and Ubiquitous Computing (UbiComp ’14 Adjunct). ACM, New York, NY, USA, 1151–1160.
    DOI:[http://dx.doi.org/10.1145/2638728.2641695](http://dx.doi.org/10.1145/2638728.2641695)'
- en: '[36] Davis. E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of
    Machine Learning Research 10 (2009), 1755–1758. DOI:[http://dx.doi.org/10.1145/1577069.1755843](http://dx.doi.org/10.1145/1577069.1755843)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Davis. E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of
    Machine Learning Research 10 (2009), 1755–1758. DOI:[http://dx.doi.org/10.1145/1577069.1755843](http://dx.doi.org/10.1145/1577069.1755843)'
- en: '[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet
    Classification with Deep Convolutional Neural Networks. Advances In Neural Information
    Processing Systems (2012), 1–9. DOI:[http://dx.doi.org/10.1016/j.protcy.2014.09.007](http://dx.doi.org/10.1016/j.protcy.2014.09.007)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet
    Classification with Deep Convolutional Neural Networks. Advances In Neural Information
    Processing Systems (2012), 1–9. DOI:[http://dx.doi.org/10.1016/j.protcy.2014.09.007](http://dx.doi.org/10.1016/j.protcy.2014.09.007)'
- en: '[38] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based
    learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based
    learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.'
- en: '[39] Henry Lieberman, Bonnie A Nardi, and David J Wright. 2001. Chapter 12
    - Training Agents to Recognize Text by Example. In Your Wish is My Command, Henry
    Lieberman (Ed.). Morgan Kaufmann, San Francisco, 227 – XII. DOI:[http://dx.doi.org/https://doi.org/10.1016/B978-155860688-3/50013-0](http://dx.doi.org/https://doi.org/10.1016/B978-155860688-3/50013-0)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Henry Lieberman, Bonnie A Nardi, and David J Wright. 2001. Chapter 12
    - Training Agents to Recognize Text by Example. In Your Wish is My Command, Henry
    Lieberman (Ed.). Morgan Kaufmann, San Francisco, 227 – XII. DOI:[http://dx.doi.org/https://doi.org/10.1016/B978-155860688-3/50013-0](http://dx.doi.org/https://doi.org/10.1016/B978-155860688-3/50013-0)'
- en: '[40] Tsung Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common
    objects in context. European conference on computer vision (2014), 740–755. DOI:[http://dx.doi.org/10.1007/978-3-319-10602-1_48](http://dx.doi.org/10.1007/978-3-319-10602-1_48)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Tsung Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common
    objects in context. European conference on computer vision (2014), 740–755. DOI:[http://dx.doi.org/10.1007/978-3-319-10602-1_48](http://dx.doi.org/10.1007/978-3-319-10602-1_48)'
- en: '[41] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016.
    Neural Relation Extraction with Selective Attention over Instances. Proceedings
    of ACL (2016), 2124–2133. DOI:[http://dx.doi.org/10.18653/v1/P16-1200](http://dx.doi.org/10.18653/v1/P16-1200)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016.
    Neural Relation Extraction with Selective Attention over Instances. Proceedings
    of ACL (2016), 2124–2133. DOI:[http://dx.doi.org/10.18653/v1/P16-1200](http://dx.doi.org/10.18653/v1/P16-1200)'
- en: '[42] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y
    Ng. 2011. Multimodal Deep Learning. In Proceedings of the 28th International Conference
    on International Conference on Machine Learning (ICML’11). Omnipress, USA, 689–696.
    [http://dl.acm.org/citation.cfm?id=3104482.3104569](http://dl.acm.org/citation.cfm?id=3104482.3104569)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y
    Ng. 2011. Multimodal Deep Learning. In Proceedings of the 28th International Conference
    on International Conference on Machine Learning (ICML’11). Omnipress, USA, 689–696.
    [http://dl.acm.org/citation.cfm?id=3104482.3104569](http://dl.acm.org/citation.cfm?id=3104482.3104569)'
- en: '[43] Thien Huu Nguyen and Ralph Grishman. 2015. Combining Neural Networks and
    Log-linear Models to Improve Relation Extraction. arXiv preprint arXiv:1511.05926
    (2015). [http://arxiv.org/abs/1511.05926](http://arxiv.org/abs/1511.05926)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Thien Huu Nguyen 和 Ralph Grishman。2015。结合神经网络和对数线性模型以改进关系提取。arXiv 预印本
    arXiv:1511.05926（2015）。[http://arxiv.org/abs/1511.05926](http://arxiv.org/abs/1511.05926)'
- en: '[44] Aparna Nurani Venkitasubramanian, Tinne Tuytelaars, and Marie-Francine
    Moens. 2017. Learning to recognize animals by watching documentaries: using subtitles
    as weak supervision. In Proceedings of the 6th Workshop on Vision and Language
    (VL’17) at EACL 2016.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Aparna Nurani Venkitasubramanian, Tinne Tuytelaars, 和 Marie-Francine Moens。2017。通过观看纪录片学习识别动物：使用字幕作为弱监督。在EACL
    2016第6届视觉与语言研讨会（VL''17）上。'
- en: '[45] Sharon Oviatt, Björn Schuller, Philip R Cohen, Daniel Sonntag, Gerasimos
    Potamianos, and Antonio Krüger. 2017. The Handbook of Multimodal-Multisensor Interfaces.
    Association for Computing Machinery and Morgan & Claypool, New York, NY, USA,
    Chapter Intro, 1–15. DOI:[http://dx.doi.org/10.1145/3015783.3015785](http://dx.doi.org/10.1145/3015783.3015785)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Sharon Oviatt, Björn Schuller, Philip R Cohen, Daniel Sonntag, Gerasimos
    Potamianos, 和 Antonio Krüger。2017。《多模态多传感器接口手册》。计算机协会和Morgan & Claypool，纽约，美国，第1章，1–15。DOI:[http://dx.doi.org/10.1145/3015783.3015785](http://dx.doi.org/10.1145/3015783.3015785)'
- en: '[46] Aniruddha Parvat, Jai Chavan, Siddhesh Kadam, Souradeep Dev, and Vidhi
    Pathak. 2017. A Survey of Deep-learning Frameworks. International Conference on
    Inventive Systems and Control (ICISC) (2017), 7. DOI:[http://dx.doi.org/10.1109/ICISC.2017.8068684](http://dx.doi.org/10.1109/ICISC.2017.8068684)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Aniruddha Parvat, Jai Chavan, Siddhesh Kadam, Souradeep Dev, 和 Vidhi Pathak。2017。深度学习框架调查。发明系统与控制国际会议（ICISC）（2017），7。DOI:[http://dx.doi.org/10.1109/ICISC.2017.8068684](http://dx.doi.org/10.1109/ICISC.2017.8068684)'
- en: '[47] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek,
    Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan
    Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition
    Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding.
    IEEE Signal Processing Society.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek,
    Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan
    Silovsky, Georg Stemmer, 和 Karel Vesely。2011。Kaldi语音识别工具包。在IEEE 2011自动语音识别与理解研讨会。IEEE信号处理学会。'
- en: '[48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN:
    Towards Real-Time Object Detection with Region Proposal Networks. In Advances
    in Neural Information Processing Systems (NIPS).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun。2015。Faster R-CNN：使用区域提议网络实现实时目标检测。在神经信息处理系统（NIPS）进展中。'
- en: '[49] Burr Settles. 2010. Active learning literature survey. University of Wisconsin,
    Madison 52, 55-66 (2010), 11.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Burr Settles。2010。主动学习文献综述。威斯康星大学，麦迪逊52，55-66（2010），11。'
- en: '[50] Burr Settles. 2011. Closing the loop: fast, interactive semi-supervised
    annotation with queries on features and instances. In Proceedings of the Conference
    on Empirical Methods in Natural Language Processing. Association for Computational
    Linguistics, 1467–1478. [http://dl.acm.org/citation.cfm?id=2145588](http://dl.acm.org/citation.cfm?id=2145588)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Burr Settles。2011。闭环：快速、互动的半监督注释，带有特征和实例查询。在自然语言处理经验方法会议论文集中。计算语言学协会，1467–1478。[http://dl.acm.org/citation.cfm?id=2145588](http://dl.acm.org/citation.cfm?id=2145588)'
- en: '[51] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmarking
    State-of-the-Art Deep Learning Software Tools. (2016). DOI:[http://dx.doi.org/10.1109/CCBD.2016.029](http://dx.doi.org/10.1109/CCBD.2016.029)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Shaohuai Shi, Qiang Wang, Pengfei Xu, 和 Xiaowen Chu。2016。最先进深度学习软件工具的基准测试。（2016）。DOI:[http://dx.doi.org/10.1109/CCBD.2016.029](http://dx.doi.org/10.1109/CCBD.2016.029)'
- en: '[52] Patrice Simard, Saleema Amershi, Max Chickering, Alicia Edelman Pelton,
    Soroush Ghorashi, Chris Meek, Gonzalo Ramos, Jina Suh, Johan Verwey, Mo Wang,
    and John Wernsing. 2017. Machine Teaching: A New Paradigm for Building Machine
    Learning Systems. (2017). [https://arxiv.org/abs/1707.06742](https://arxiv.org/abs/1707.06742)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Patrice Simard, Saleema Amershi, Max Chickering, Alicia Edelman Pelton,
    Soroush Ghorashi, Chris Meek, Gonzalo Ramos, Jina Suh, Johan Verwey, Mo Wang,
    和 John Wernsing。2017。机器教学：构建机器学习系统的新范式。（2017）。[https://arxiv.org/abs/1707.06742](https://arxiv.org/abs/1707.06742)'
- en: '[53] Daniel Sonntag. 2009. Introspection and Adaptable Model Integration for
    Dialogue-based Question Answering. In IJCAI. 1549–1554.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Daniel Sonntag。2009。内省与可适应模型集成用于基于对话的问答系统。在IJCAI。1549–1554。'
- en: '[54] Daniel Sonntag. 2012. Collaborative Multimodality. KI - Künstliche Intelligenz
    26, May 2012 (2012), 161–168. DOI:[http://dx.doi.org/10.1007/s13218-012-0169-4](http://dx.doi.org/10.1007/s13218-012-0169-4)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Daniel Sonntag。2012年。《Collaborative Multimodality》。KI - Künstliche Intelligenz
    26, 2012年5月（2012），161–168。DOI：[http://dx.doi.org/10.1007/s13218-012-0169-4](http://dx.doi.org/10.1007/s13218-012-0169-4)'
- en: '[55] Daniel Sonntag. 2017. Intelligent User Interfaces - A Tutorial. CoRR abs/1702.0
    (2017). [http://arxiv.org/abs/1702.05250](http://arxiv.org/abs/1702.05250)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Daniel Sonntag。2017年。《Intelligent User Interfaces - A Tutorial》。CoRR abs/1702.0（2017）。[http://arxiv.org/abs/1702.05250](http://arxiv.org/abs/1702.05250)'
- en: '[56] Daniel Sonntag, Ralf Engel, Gerd Herzog, Alexander Pfalzgraf, Norbert
    Pfleger, Massimo Romanelli, and Norbert Reithinger. 2007. SmartWeb Handheld -
    Multimodal Interaction with Ontological Knowledge Bases and Semantic Web Services.
    In Artifical Intelligence for Human Computing, ICMI 2006 and IJCAI 2007 International
    Workshops, Banff, Canada, November 3, 2006, Hyderabad, India, January 6, 2007,
    Revised Seleced and Invited Papers (Lecture Notes in Computer Science), Thomas S.
    Huang, Anton Nijholt, Maja Pantic, and Alex Pentland (Eds.), Vol. 4451\. Springer,
    272–295. DOI:[http://dx.doi.org/10.1007/978-3-540-72348-6_14](http://dx.doi.org/10.1007/978-3-540-72348-6_14)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Daniel Sonntag, Ralf Engel, Gerd Herzog, Alexander Pfalzgraf, Norbert
    Pfleger, Massimo Romanelli, 和 Norbert Reithinger。2007年。《SmartWeb Handheld - Multimodal
    Interaction with Ontological Knowledge Bases and Semantic Web Services》。在人工智能与人类计算，ICMI
    2006和IJCAI 2007国际研讨会，Banff, Canada，2006年11月3日，Hyderabad, India，2007年1月6日，修订版和邀请论文（Lecture
    Notes in Computer Science），Thomas S. Huang, Anton Nijholt, Maja Pantic, 和 Alex
    Pentland（编辑），第4451卷。Springer, 272–295。DOI：[http://dx.doi.org/10.1007/978-3-540-72348-6_14](http://dx.doi.org/10.1007/978-3-540-72348-6_14)'
- en: '[57] Florian Strub, Harm de Vries, Jérémie Mary, Bilal Piot, Aaron Courville,
    and Olivier Pietquin. 2017. End-to-end optimization of goal-driven and visually
    grounded dialogue systems. In Proceedings of the Twenty-Sixth International Joint
    Conference on Artificial Intelligence. International Joint Conferences on Artificial
    Intelligence Organization, 2765–2771. DOI:[http://dx.doi.org/10.24963/ijcai.2017/385](http://dx.doi.org/10.24963/ijcai.2017/385)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Florian Strub, Harm de Vries, Jérémie Mary, Bilal Piot, Aaron Courville,
    和 Olivier Pietquin。2017年。《End-to-end optimization of goal-driven and visually
    grounded dialogue systems》。在第二十六届国际联合人工智能会议论文集。国际联合人工智能组织，2765–2771。DOI：[http://dx.doi.org/10.24963/ijcai.2017/385](http://dx.doi.org/10.24963/ijcai.2017/385)'
- en: '[58] Yaniv Taigman, Marc Aurelio Ranzato, Tel Aviv, and Menlo Park. 2014. DeepFace:
    Closing the Gap to Human-Level Performance in Face Verification. Cvpr (2014).
    DOI:[http://dx.doi.org/10.1109/CVPR.2014.220](http://dx.doi.org/10.1109/CVPR.2014.220)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Yaniv Taigman, Marc Aurelio Ranzato, 特拉维夫和门洛帕克。2014年。《DeepFace: Closing
    the Gap to Human-Level Performance in Face Verification》。Cvpr (2014)。DOI：[http://dx.doi.org/10.1109/CVPR.2014.220](http://dx.doi.org/10.1109/CVPR.2014.220)'
- en: '[59] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer:
    a Next-Generation Open Source Framework for Deep Learning. Proceedings of Workshop
    on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference
    on Neural Information Processing Systems (NIPS) (2015), 1–6.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Seiya Tokui, Kenta Oono, Shohei Hido, 和 Justin Clayton。2015年。《Chainer:
    a Next-Generation Open Source Framework for Deep Learning》。在第二十九届神经信息处理系统年会（NIPS）机器学习系统研讨会（LearningSys）论文集（2015），1–6。'
- en: '[60] Wolfgang Wahlster and Mark Maybury. 1998. Intelligent User Interfaces:
    An Introduction. RUIU (1998), 1–13.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Wolfgang Wahlster 和 Mark Maybury。1998年。《Intelligent User Interfaces: An
    Introduction》。RUIU（1998），1–13。'
- en: '[61] Luke Yeager. 2015. DIGITS : the Deep learning GPU Training System. ICML
    AutoML Workshop (2015).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Luke Yeager。2015年。《DIGITS : the Deep learning GPU Training System》。ICML
    AutoML Workshop（2015）。'
- en: '[62] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian
    Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, Jasha Droppo,
    Geoffrey Zweig, Chris Rossbach, Jon Currey, Jie Gao, Avner May, Baolin Peng, Andreas
    Stolcke, and Malcolm Slaney. 2015. An Introduction to Computational Networks and
    the Computational Network Toolkit. Microsoft Technical Report 112, MSR-TR-2014-112
    (2015).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian
    Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, Jasha Droppo,
    Geoffrey Zweig, Chris Rossbach, Jon Currey, Jie Gao, Avner May, Baolin Peng, Andreas
    Stolcke, 和 Malcolm Slaney。2015年。《An Introduction to Computational Networks and
    the Computational Network Toolkit》。微软技术报告112，MSR-TR-2014-112（2015）。'
- en: '[63] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant Supervision
    for Relation Extraction via Piecewise Convolutional Neural Networks. Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing September
    (2015), 1753–1762. DOI:[http://dx.doi.org/10.18653/v1/D15-1203](http://dx.doi.org/10.18653/v1/D15-1203)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. 使用分段卷积神经网络进行关系抽取的远程监督。2015年自然语言处理会议论文集（2015年9月），1753–1762。DOI:[http://dx.doi.org/10.18653/v1/D15-1203](http://dx.doi.org/10.18653/v1/D15-1203)'
- en: '[64] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014.
    Relation Classification via Convolutional Deep Neural Network. Coling 2011 (2014),
    2335–2344. [http://www.nlpr.ia.ac.cn/cip/liukang.files/camera](http://www.nlpr.ia.ac.cn/cip/liukang.files/camera)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014.
    使用卷积深度神经网络进行关系分类。Coling 2011（2014），2335–2344。[http://www.nlpr.ia.ac.cn/cip/liukang.files/camera](http://www.nlpr.ia.ac.cn/cip/liukang.files/camera)'
- en: '[65] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and
    Bo Xu. 2016. Attention-Based Bidirectional Long Short-Term Memory Networks for
    Relation Classification. Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers) (2016), 207–212. [http://anthology.aclweb.org/P16-2034](http://anthology.aclweb.org/P16-2034)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and
    Bo Xu. 2016. 基于注意力的双向长短期记忆网络进行关系分类。第54届计算语言学年会短论集（2016），207–212。[http://anthology.aclweb.org/P16-2034](http://anthology.aclweb.org/P16-2034)'
