- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1812.09449] A Survey on Deep Learning for Named Entity Recognition'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1812.09449] 深度学习在命名实体识别中的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1812.09449](https://ar5iv.labs.arxiv.org/html/1812.09449)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1812.09449](https://ar5iv.labs.arxiv.org/html/1812.09449)
- en: A Survey on Deep Learning for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在命名实体识别中的综述
- en: Named Entity Recognition
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li J. Li is with the Inception
    Institute of Artificial Intelligence, United Arab Emirates. This work was done
    when the author was with Nanyang Technological University, Singapore. E-mail:
    jli030@e.ntu.edu.sg. A. Sun is with School of Computer Science and Engineering,
    Nanyang Technological University, Singapore. E-mail: axsun@ntu.edu.sg. J. Han
    is with SAP, Singapore. E-mail: ray.han@sap.com. C. Li is with School of Cyber
    Science and Engineering, Wuhan University, China. E-mail:cllee@whu.edu.cn.Accepted
    in IEEE TKDE.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Jing Li, Aixin Sun, Jianglei Han 和 Chenliang Li J. Li 在阿联酋的人工智能创新院工作。本研究是在作者在新加坡南洋理工大学工作期间完成的。电子邮件：jli030@e.ntu.edu.sg。A.
    Sun 在新加坡南洋理工大学计算机科学与工程学院工作。电子邮件：axsun@ntu.edu.sg。J. Han 在新加坡 SAP 工作。电子邮件：ray.han@sap.com。C.
    Li 在中国武汉大学网络科学与工程学院工作。电子邮件：cllee@whu.edu.cn。被 IEEE TKDE 接受。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Named entity recognition (NER) is the task to identify mentions of rigid designators
    from text belonging to predefined semantic types such as person, location, organization
    etc. NER always serves as the foundation for many natural language applications
    such as question answering, text summarization, and machine translation. Early
    NER systems got a huge success in achieving good performance with the cost of
    human engineering in designing domain-specific features and rules. In recent years,
    deep learning, empowered by continuous real-valued vector representations and
    semantic composition through nonlinear processing, has been employed in NER systems,
    yielding stat-of-the-art performance. In this paper, we provide a comprehensive
    review on existing deep learning techniques for NER. We first introduce NER resources,
    including tagged NER corpora and off-the-shelf NER tools. Then, we systematically
    categorize existing works based on a taxonomy along three axes: distributed representations
    for input, context encoder, and tag decoder. Next, we survey the most representative
    methods for recent applied techniques of deep learning in new NER problem settings
    and applications. Finally, we present readers with the challenges faced by NER
    systems and outline future directions in this area.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是从文本中识别属于预定义语义类型（如人、地点、组织等）的固定指代符的任务。NER始终作为许多自然语言应用的基础，如问答系统、文本摘要和机器翻译。早期的NER系统在通过设计领域特定特征和规则来实现良好性能方面取得了巨大成功，但代价是需要人工工程。近年来，深度学习通过连续的实值向量表示和非线性处理的语义组合，已被应用于NER系统中，取得了最先进的性能。在本文中，我们对现有的深度学习技术在NER中的应用进行了全面回顾。我们首先介绍NER资源，包括标记的NER语料库和现成的NER工具。然后，我们根据三个维度对现有工作进行系统分类：输入的分布式表示、上下文编码器和标签解码器。接下来，我们调查了深度学习在新的NER问题设置和应用中的最具代表性的方法。最后，我们向读者介绍了NER系统面临的挑战，并概述了该领域的未来发展方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Natural language processing, named entity recognition, deep learning, survey
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理，命名实体识别，深度学习，综述
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Named Entity Recognition (NER) aims to recognize mentions of rigid designators
    from text belonging to predefined semantic types such as person, location, organization
    etc [[1](#bib.bib1)]. NER not only acts as a standalone tool for information extraction
    (IE), but also plays an essential role in a variety of natural language processing
    (NLP) applications such as text understanding [[2](#bib.bib2), [3](#bib.bib3)],
    information retrieval [[4](#bib.bib4), [5](#bib.bib5)], automatic text summarization [[6](#bib.bib6)],
    question answering [[7](#bib.bib7)], machine translation [[8](#bib.bib8)], and
    knowledge base construction [[9](#bib.bib9)] etc.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）旨在识别文本中属于预定义语义类型的固定指代符，如人、地点、组织等[[1](#bib.bib1)]。NER不仅作为信息提取（IE）的独立工具，还在许多自然语言处理（NLP）应用中发挥着重要作用，如文本理解[[2](#bib.bib2),
    [3](#bib.bib3)]，信息检索[[4](#bib.bib4), [5](#bib.bib5)]，自动文本摘要[[6](#bib.bib6)]，问答系统[[7](#bib.bib7)]，机器翻译[[8](#bib.bib8)]，和知识库构建[[9](#bib.bib9)]等。
- en: Evolution of NER. The term “Named Entity” (NE) was first used at the sixth Message
    Understanding Conference (MUC-6) [[10](#bib.bib10)], as the task of identifying
    names of organizations, people and geographic locations in text, as well as currency,
    time and percentage expressions. Since MUC-6 there has been increasing interest
    in NER, and various scientific events (e.g., CoNLL03 [[11](#bib.bib11)], ACE [[12](#bib.bib12)],
    IREX [[13](#bib.bib13)], and TREC Entity Track [[14](#bib.bib14)]) devote much
    effort to this topic.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NER的发展。术语“命名实体”（NE）首次在第六届消息理解会议（MUC-6）[[10](#bib.bib10)]上使用，指的是识别文本中的组织、人员和地理位置的名称，以及货币、时间和百分比表达。从MUC-6以来，对NER的兴趣不断增加，许多科学事件（例如CoNLL03[[11](#bib.bib11)]、ACE[[12](#bib.bib12)]、IREX[[13](#bib.bib13)]和TREC实体跟踪[[14](#bib.bib14)]）对这一主题投入了大量精力。
- en: 'Regarding the problem definition, Petasis et al. [[15](#bib.bib15)] restricted
    the definition of named entities: “A NE is a proper noun, serving as a name for
    something or someone”. This restriction is justified by the significant percentage
    of proper nouns present in a corpus. Nadeau and Sekine [[1](#bib.bib1)] claimed
    that the word “Named” restricted the task to only those entities for which one
    or many rigid designators stands for the referent. Rigid designator, defined in [[16](#bib.bib16)],
    include proper names and natural kind terms like biological species and substances.
    Despite the various definitions of NEs, researchers have reached common consensus
    on the types of NEs to recognize. We generally divide NEs into two categories:
    generic NEs (e.g., person and location) and domain-specific NEs (e.g., proteins,
    enzymes, and genes). In this paper, we mainly focus on generic NEs in English
    language. We do not claim this article to be exhaustive or representative of all
    NER works on all languages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于问题定义，Petasis等人[[15](#bib.bib15)]限制了命名实体的定义：“一个命名实体是一个专有名词，作为某物或某人的名称。”这一限制是因为语料库中专有名词的比例很高。Nadeau和Sekine[[1](#bib.bib1)]声称，“Named”一词将任务限制为仅包括那些有一个或多个固定指称符代指的实体。固定指称符在[[16](#bib.bib16)]中定义，包括专有名词和自然类别术语，如生物物种和物质。尽管对命名实体的定义各不相同，但研究人员对需要识别的命名实体类型达成了一致意见。我们通常将命名实体分为两类：通用命名实体（例如，人和地点）和领域特定命名实体（例如，蛋白质、酶和基因）。在本文中，我们主要关注英语中的通用命名实体。我们不声称这篇文章涵盖了所有语言中的所有NER工作。
- en: 'As to the techniques applied in NER, there are four main streams: 1) Rule-based
    approaches, which do not need annotated data as they rely on hand-crafted rules;
    2) Unsupervised learning approaches, which rely on unsupervised algorithms without
    hand-labeled training examples; 3) Feature-based supervised learning approaches,
    which rely on supervised learning algorithms with careful feature engineering;
    4) Deep-learning based approaches, which automatically discover representations
    needed for the classification and/or detection from raw input in an end-to-end
    manner. We brief 1), 2) and 3), and review 4) in detail.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于命名实体识别（NER）中应用的技术，主要有四种流派：1）基于规则的方法，它们不需要标注数据，因为依赖于手工制定的规则；2）无监督学习方法，它们依赖于无监督算法而不需要手工标注的训练示例；3）基于特征的监督学习方法，它们依赖于监督学习算法并且需要仔细的特征工程；4）基于深度学习的方法，它们以端到端的方式自动发现分类和/或检测所需的表示。我们简要介绍1）、2）和3），并详细回顾4）。
- en: Motivations for conducting this survey. In recent years, deep learning (DL,
    also named deep neural network) has attracted significant attention due to its
    success in various domains. Starting with Collobert et al. [[17](#bib.bib17)],
    DL-based NER systems with minimal feature engineering have been flourishing. Over
    the past few years, a considerable number of studies have applied deep learning
    to NER and successively advanced the state-of-the-art performance [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]. This
    trend motivates us to conduct a survey to report the current status of deep learning
    techniques in NER research. By comparing the choices of DL architectures, we aim
    to identify factors affecting NER performance as well as issues and challenges.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这项调查的动机。近年来，深度学习（DL，也称为深度神经网络）由于在各种领域的成功而引起了广泛关注。从Collobert等人[[17](#bib.bib17)]开始，基于深度学习的NER系统在极少的特征工程下蓬勃发展。在过去几年中，相当多的研究将深度学习应用于NER，并逐步推动了最先进的性能[[17](#bib.bib17)、[18](#bib.bib18)、[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)]。这一趋势激励我们进行调查，以报告深度学习技术在NER研究中的现状。通过比较DL架构的选择，我们旨在识别影响NER性能的因素以及存在的问题和挑战。
- en: On the other hand, although NER studies have been thriving for a few decades,
    to the best of our knowledge, there are few reviews in this field so far. Arguably
    the most established one was published by Nadeau and Sekine [[1](#bib.bib1)] in
    2007. This survey presents an overview of the technique trend from hand-crafted
    rules towards machine learning. Marrero et al. [[22](#bib.bib22)] summarized NER
    works from the perspectives of fallacies, challenges and opportunities in 2013.
    Then Patawar and Potey [[23](#bib.bib23)] provided a short review in 2015. The
    two recent short surveys are on new domains [[24](#bib.bib24)] and complex entity
    mentions [[25](#bib.bib25)], respectively. In summary, existing surveys mainly
    cover feature-based machine learning models, but not the modern DL-based NER systems.
    More germane to this work are the two recent surveys [[26](#bib.bib26), [27](#bib.bib27)]
    in 2018. Goyal et al. [[27](#bib.bib27)] surveyed developments and progresses
    made in NER. However, they did not include recent advances of deep learning techniques.
    Yadav and Bethard [[26](#bib.bib26)] presented a short survey of recent advances
    in NER based on representations of words in sentence. This survey focuses more
    on the distributed representations for input (e.g., char- and word-level embeddings)
    and do not review the context encoders and tag decoders. The recent trend of applied
    deep learning on NER tasks (e.g., multi-task learning, transfer learning, reinforcement
    leanring and adversarial learning) are not in their servery as well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管NER研究已经蓬勃发展了几十年，尽我们所知，目前在这一领域的综述文章仍然较少。可以说，最为成熟的一篇综述文章是由Nadeau和Sekine [[1](#bib.bib1)]于2007年发表的。这篇综述介绍了从手工规则到机器学习的技术趋势概况。Marrero等人 [[22](#bib.bib22)]在2013年从谬误、挑战和机遇的角度总结了NER工作。随后，Patawar和Potey [[23](#bib.bib23)]在2015年提供了一篇简短的综述。最近的两篇简短综述分别涉及新领域 [[24](#bib.bib24)]和复杂实体提及 [[25](#bib.bib25)]。总之，现有的综述主要覆盖了基于特征的机器学习模型，而不是现代基于深度学习的NER系统。本工作更相关的是2018年的两篇近期综述 [[26](#bib.bib26),
    [27](#bib.bib27)]。Goyal等人 [[27](#bib.bib27)]调查了NER的最新进展。然而，他们并未包含深度学习技术的最新进展。Yadav和Bethard [[26](#bib.bib26)]对基于句子中单词表示的NER最新进展进行了简要综述。这篇综述更多地关注于输入的分布式表示（例如，字符和词级嵌入），而未对上下文编码器和标签解码器进行评审。近年来应用深度学习于NER任务的趋势（如多任务学习、迁移学习、强化学习和对抗学习）也未在他们的综述中涉及。
- en: 'Contributions of this survey. We intensely review applications of deep learning
    techniques in NER, to enlighten and guide researchers and practitioners in this
    area. Specifically, we consolidate NER corpora, off-the-shelf NER systems (from
    both academia and industry) in a tabular form, to provide useful resources for
    NER research community. We then present a comprehensive survey on deep learning
    techniques for NER. To this end, we propose a new taxonomy, which systematically
    organizes DL-based NER approaches along three axes: distributed representations
    for input, context encoder (for capturing contextual dependencies for tag decoder),
    and tag decoder (for predicting labels of words in the given sequence). In addition,
    we also survey the most representative methods for recent applied deep learning
    techniques in new NER problem settings and applications. Finally, we present readers
    with the challenges faced by NER systems and outline future directions in this
    area.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述的贡献。我们深入回顾了深度学习技术在NER中的应用，以启发和指导该领域的研究人员和从业者。具体而言，我们将NER语料库、现成的NER系统（来自学术界和工业界）以表格形式汇总，提供有用的资源给NER研究社区。然后，我们对深度学习技术在NER中的应用进行了全面的综述。为此，我们提出了一个新的分类法，该分类法系统地沿三个轴线组织基于深度学习的NER方法：输入的分布式表示、上下文编码器（用于捕获上下文依赖关系以进行标签解码）和标签解码器（用于预测给定序列中单词的标签）。此外，我们还综述了最近在新的NER问题设置和应用中的代表性深度学习方法。最后，我们向读者展示了NER系统面临的挑战，并概述了该领域的未来方向。
- en: 2 Background
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: We first give a formal formulation of the NER problem. We then introduce the
    widely-used NER datasets and tools. Next, we detail the evaluation metrics and
    summarize the traditional approaches to NER.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先给出NER问题的正式表述。接着，我们介绍了广泛使用的NER数据集和工具。然后，我们详细说明了评估指标，并总结了传统的NER方法。
- en: 2.1 What is NER?
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 什么是NER？
- en: A named entity is a word or a phrase that clearly identifies one item from a
    set of other items that have similar attributes [[28](#bib.bib28)]. Examples of
    named entities are organization, person, and location names in general domain;
    gene, protein, drug and disease names in biomedical domain. NER is the process
    of locating and classifying named entities in text into predefined entity categories.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体是指从一组具有相似属性的其他项中明确识别出某一项的词语或短语[[28](#bib.bib28)]。命名实体的示例包括一般领域中的组织、个人和地点名称；生物医学领域中的基因、蛋白质、药物和疾病名称。NER
    是在文本中定位和分类命名实体的过程，将其划分为预定义的实体类别。
- en: '![Refer to caption](img/94633de98196353e3e83b1866db27dd0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94633de98196353e3e83b1866db27dd0.png)'
- en: 'Figure 1: An illustration of the named entity recognition task.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：命名实体识别任务的示意图。
- en: Formally, given a sequence of tokens $s=\langle w_{1},w_{2},...,w_{N}\rangle$,
    NER is to output a list of tuples $\langle I_{s},I_{e},t\rangle$, each of which
    is a named entity mentioned in $s$. Here, $I_{s}\in[1,N]$ and $I_{e}\in[1,N]$
    are the start and the end indexes of a named entity mention; $t$ is the entity
    type from a predefined category set. Figure [1](#S2.F1 "Figure 1 ‣ 2.1 What is
    NER? ‣ 2 Background ‣ A Survey on Deep Learning for Named Entity Recognition")
    shows an example where a NER system recognizes three named entities from the given
    sentence. When NER was first defined in MUC-6 [[10](#bib.bib10)], the task is
    to recognize names of people, organizations, locations, and time, currency, percentage
    expressions in text. Note that the task focuses on a small set of coarse entity
    types and one type per named entity. We call this kind of NER tasks as coarse-grained
    NER [[10](#bib.bib10), [11](#bib.bib11)]. Recently, some fine-grained NER tasks [[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)] focus
    on a much larger set of entity types where a mention may be assigned multiple
    fine-grained types.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，给定一个标记序列 $s=\langle w_{1},w_{2},...,w_{N}\rangle$，NER 的目标是输出一个元组列表 $\langle
    I_{s},I_{e},t\rangle$，每个元组都是 $s$ 中提到的命名实体。这里，$I_{s}\in[1,N]$ 和 $I_{e}\in[1,N]$
    是命名实体提及的起始和结束索引；$t$ 是预定义类别集中的实体类型。图 [1](#S2.F1 "图 1 ‣ 2.1 什么是 NER? ‣ 2 背景 ‣ 深度学习命名实体识别综述")
    显示了一个例子，其中 NER 系统从给定的句子中识别出三个命名实体。当 NER 在 MUC-6[[10](#bib.bib10)] 中首次定义时，该任务是识别文本中的人名、组织名、地点名以及时间、货币、百分比表达。请注意，该任务集中在一小部分粗粒度实体类型和每个命名实体一个类型上。我们将这种
    NER 任务称为粗粒度 NER[[10](#bib.bib10), [11](#bib.bib11)]。最近，一些细粒度 NER 任务[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33)] 专注于更大范围的实体类型，其中一个提及可能被分配多个细粒度类型。
- en: NER acts as an important pre-processing step for a variety of downstream applications
    such as information retrieval, question answering, machine translation, etc. Here,
    we use semantic search as an example to illustrate the importance of NER in supporting
    various applications. Semantic search refers to a collection of techniques, which
    enable search engines to understand the concepts, meaning, and intent behind the
    queries from users [[34](#bib.bib34)]. According to [[4](#bib.bib4)], about 71%
    of search queries contain at least one named entity. Recognizing named entities
    in search queries would help us to better understand user intents, hence to provide
    better search results. To incorporate named entities in search, entity-based language
    models [[34](#bib.bib34)], which consider individual terms as well as term sequences
    that have been annotated as entities (both in documents and in queries), have
    been proposed by Raviv et al. [[35](#bib.bib35)]. There are also studies utilizing
    named entities for an enhanced user experience, such as query recommendation [[36](#bib.bib36)],
    query auto-completion [[37](#bib.bib37), [38](#bib.bib38)] and entity cards [[39](#bib.bib39),
    [40](#bib.bib40)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NER 作为多种下游应用的重要预处理步骤，如信息检索、问答系统、机器翻译等。这里，我们以语义搜索为例，说明 NER 在支持各种应用中的重要性。语义搜索指一组技术，使搜索引擎能够理解用户查询背后的概念、意义和意图[[34](#bib.bib34)]。根据[[4](#bib.bib4)]，大约
    71% 的搜索查询包含至少一个命名实体。在搜索查询中识别命名实体有助于更好地理解用户意图，从而提供更好的搜索结果。为了在搜索中纳入命名实体，Raviv 等人[[35](#bib.bib35)]
    提出了基于实体的语言模型[[34](#bib.bib34)]，该模型考虑了作为实体标注的单个术语及术语序列（无论是在文档还是查询中）。还有一些研究利用命名实体来增强用户体验，如查询推荐[[36](#bib.bib36)]、查询自动补全[[37](#bib.bib37),
    [38](#bib.bib38)]和实体卡片[[39](#bib.bib39), [40](#bib.bib40)]。
- en: 'TABLE I: List of annotated datasets for English NER. “#Tags” refers to the
    number of entity types.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：英语命名实体识别（NER）标注数据集列表。“#Tags”指实体类型的数量。
- en: '| Corpus | Year | Text Source | #Tags | URL |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Corpus | 年份 | 文本来源 | 标签数 | URL |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MUC-6 | 1995 | Wall Street Journal | 7 | [https://catalog.ldc.upenn.edu/LDC2003T13](https://catalog.ldc.upenn.edu/LDC2003T13)
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MUC-6 | 1995 | 华尔街日报 | 7 | [https://catalog.ldc.upenn.edu/LDC2003T13](https://catalog.ldc.upenn.edu/LDC2003T13)
    |'
- en: '| MUC-6 Plus | 1995 | Additional news to MUC-6 | 7 | [https://catalog.ldc.upenn.edu/LDC96T10](https://catalog.ldc.upenn.edu/LDC96T10)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MUC-6 Plus | 1995 | MUC-6 的附加新闻 | 7 | [https://catalog.ldc.upenn.edu/LDC96T10](https://catalog.ldc.upenn.edu/LDC96T10)
    |'
- en: '| MUC-7 | 1997 | New York Times news | 7 | [https://catalog.ldc.upenn.edu/LDC2001T02](https://catalog.ldc.upenn.edu/LDC2001T02)
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| MUC-7 | 1997 | 纽约时报新闻 | 7 | [https://catalog.ldc.upenn.edu/LDC2001T02](https://catalog.ldc.upenn.edu/LDC2001T02)
    |'
- en: '| CoNLL03 | 2003 | Reuters news | 4 | [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| CoNLL03 | 2003 | 路透社新闻 | 4 | [https://www.clips.uantwerpen.be/conll2003/ner/](https://www.clips.uantwerpen.be/conll2003/ner/)
    |'
- en: '| ACE | 2000 - 2008 | Transcripts, news | 7 | [https://www.ldc.upenn.edu/collaborations/past-projects/ace](https://www.ldc.upenn.edu/collaborations/past-projects/ace)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ACE | 2000 - 2008 | 转录文本、新闻 | 7 | [https://www.ldc.upenn.edu/collaborations/past-projects/ace](https://www.ldc.upenn.edu/collaborations/past-projects/ace)
    |'
- en: '| OntoNotes | 2007 - 2012 | Magazine, news, web, etc. | 18 | [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| OntoNotes | 2007 - 2012 | 杂志、新闻、网络等 | 18 | [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)
    |'
- en: '| W-NUT | 2015 - 2018 | User-generated text | 6/10 | [http://noisy-text.github.io](http://noisy-text.github.io)
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| W-NUT | 2015 - 2018 | 用户生成文本 | 6/10 | [http://noisy-text.github.io](http://noisy-text.github.io)
    |'
- en: '| BBN | 2005 | Wall Street Journal | 64 | [https://catalog.ldc.upenn.edu/LDC2005T33](https://catalog.ldc.upenn.edu/LDC2005T33)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| BBN | 2005 | 华尔街日报 | 64 | [https://catalog.ldc.upenn.edu/LDC2005T33](https://catalog.ldc.upenn.edu/LDC2005T33)
    |'
- en: '| WikiGold | 2009 | Wikipedia | 4 | https://figshare.com/articles/Learning_multilingual_named_entity
    _recognition_from_Wikipedia/5462500 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| WikiGold | 2009 | 维基百科 | 4 | https://figshare.com/articles/Learning_multilingual_named_entity
    _recognition_from_Wikipedia/5462500 |'
- en: '| WiNER | 2012 | Wikipedia | 4 | [http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner](http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner)
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| WiNER | 2012 | 维基百科 | 4 | [http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner](http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner)
    |'
- en: '| WikiFiger | 2012 | Wikipedia | 112 | [https://github.com/xiaoling/figer](https://github.com/xiaoling/figer)
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| WikiFiger | 2012 | 维基百科 | 112 | [https://github.com/xiaoling/figer](https://github.com/xiaoling/figer)
    |'
- en: '| HYENA | 2012 | Wikipedia | 505 | https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/hyena
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| HYENA | 2012 | 维基百科 | 505 | https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/hyena
    |'
- en: '| N³ | 2014 | News | 3 | [http://aksw.org/Projects/N3NERNEDNIF.html](http://aksw.org/Projects/N3NERNEDNIF.html)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| N³ | 2014 | 新闻 | 3 | [http://aksw.org/Projects/N3NERNEDNIF.html](http://aksw.org/Projects/N3NERNEDNIF.html)
    |'
- en: '| Gillick | 2016 | Magazine, news, web, etc. | 89 | [https://arxiv.org/e-print/1412.1820v2](https://arxiv.org/e-print/1412.1820v2)
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Gillick | 2016 | 杂志、新闻、网络等 | 89 | [https://arxiv.org/e-print/1412.1820v2](https://arxiv.org/e-print/1412.1820v2)
    |'
- en: '| FG-NER | 2018 | Various | 200 | [https://fgner.alt.ai/](https://fgner.alt.ai/)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| FG-NER | 2018 | 各种 | 200 | [https://fgner.alt.ai/](https://fgner.alt.ai/)
    |'
- en: '| NNE | 2019 | Newswire | 114 | [https://github.com/nickyringland/nested_named_entities](https://github.com/nickyringland/nested_named_entities)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| NNE | 2019 | 新闻稿 | 114 | [https://github.com/nickyringland/nested_named_entities](https://github.com/nickyringland/nested_named_entities)
    |'
- en: '| GENIA | 2004 | Biology and clinical text | 36 | [http://www.geniaproject.org/home](http://www.geniaproject.org/home)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| GENIA | 2004 | 生物学和临床文本 | 36 | [http://www.geniaproject.org/home](http://www.geniaproject.org/home)
    |'
- en: '| GENETAG | 2005 | MEDLINE | 2 | [https://sourceforge.net/projects/bioc/files/](https://sourceforge.net/projects/bioc/files/)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| GENETAG | 2005 | MEDLINE | 2 | [https://sourceforge.net/projects/bioc/files/](https://sourceforge.net/projects/bioc/files/)
    |'
- en: '| FSU-PRGE | 2010 | PubMed and MEDLINE | 5 | [https://julielab.de/Resources/FSU_PRGE.html](https://julielab.de/Resources/FSU_PRGE.html)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| FSU-PRGE | 2010 | PubMed 和 MEDLINE | 5 | [https://julielab.de/Resources/FSU_PRGE.html](https://julielab.de/Resources/FSU_PRGE.html)
    |'
- en: '| NCBI-Disease | 2014 | PubMed | 1 | [https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| NCBI-Disease | 2014 | PubMed | 1 | [https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/)
    |'
- en: '| BC5CDR | 2015 | PubMed | 3 | [http://bioc.sourceforge.net/](http://bioc.sourceforge.net/)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| BC5CDR | 2015 | PubMed | 3 | [http://bioc.sourceforge.net/](http://bioc.sourceforge.net/)
    |'
- en: '| DFKI | 2018 | Business news and social media | 7 | [https://dfki-lt-re-group.bitbucket.io/product-corpus/](https://dfki-lt-re-group.bitbucket.io/product-corpus/)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| DFKI | 2018 | 商业新闻和社交媒体 | 7 | [https://dfki-lt-re-group.bitbucket.io/product-corpus/](https://dfki-lt-re-group.bitbucket.io/product-corpus/)
    |'
- en: '2.2 NER Resources: Datasets and Tools'
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 NER 资源：数据集和工具
- en: High quality annotations are critical for both model learning and evaluation.
    In the following, we summarize widely-used datasets and off-the-shelf tools for
    English NER.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的注释对模型学习和评估至关重要。接下来，我们总结了广泛使用的数据集和现成的工具，用于英语 NER。
- en: A tagged corpus is a collection of documents that contain annotations of one
    or more entity types. Table [I](#S2.T1 "TABLE I ‣ 2.1 What is NER? ‣ 2 Background
    ‣ A Survey on Deep Learning for Named Entity Recognition") lists some widely-used
    datasets with their data sources and number of entity types (also known as tag
    types). Summarized in Table [I](#S2.T1 "TABLE I ‣ 2.1 What is NER? ‣ 2 Background
    ‣ A Survey on Deep Learning for Named Entity Recognition"), before 2005, datasets
    were mainly developed by annotating news articles with a small number of entity
    types, suitable for coarse-grained NER tasks. After that, more datasets were developed
    on various kinds of text sources including Wikipedia articles, conversation, and
    user-generated text (e.g., tweets and YouTube comments and StackExchange posts
    in W-NUT). The number of tag types becomes significantly larger, e.g., 505 in
    HYENA. We also list a number of domain specific datasets, particularly developed
    on PubMed and MEDLINE texts. The number of entity types ranges from 1 in NCBI-Disease
    to 36 in GENIA.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 标注语料库是包含一种或多种实体类型注释的文档集合。表格 [I](#S2.T1 "TABLE I ‣ 2.1 What is NER? ‣ 2 Background
    ‣ A Survey on Deep Learning for Named Entity Recognition") 列出了某些广泛使用的数据集及其数据源和实体类型数量（也称为标签类型）。在表格
    [I](#S2.T1 "TABLE I ‣ 2.1 What is NER? ‣ 2 Background ‣ A Survey on Deep Learning
    for Named Entity Recognition") 中总结了，2005 年之前，数据集主要通过标注新闻文章来开发，实体类型数量较少，适合粗粒度的
    NER 任务。之后，更多的数据集在各种文本源上开发，包括维基百科文章、对话和用户生成的文本（例如，推文、YouTube 评论和 W-NUT 中的 StackExchange
    帖子）。标签类型的数量显著增加，例如，HYENA 中有 505 种。我们还列出了许多特定领域的数据集，特别是在 PubMed 和 MEDLINE 文本上开发的。实体类型的数量从
    NCBI-Disease 中的 1 到 GENIA 中的 36 不等。
- en: 'We note that many recent NER works report their performance on CoNLL03 and
    OntoNotes datasets (see Table [III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random
    Fields ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣
    A Survey on Deep Learning for Named Entity Recognition")). CoNLL03 contains annotations
    for Reuters news in two languages: English and German. The English dataset has
    a large portion of sports news with annotations in four entity types (Person,
    Location, Organization, and Miscellaneous) [[11](#bib.bib11)]. The goal of the
    OntoNotes project was to annotate a large corpus, comprising of various genres
    (weblogs, news, talk shows, broadcast, usenet newsgroups, and conversational telephone
    speech) with structural information (syntax and predicate argument structure)
    and shallow semantics (word sense linked to an ontology and coreference). There
    are 5 versions, from Release 1.0 to Release 5.0\. The texts are annotated with
    18 entity types. We also note two Github repositores¹¹1[https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets)
    and [https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data](https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data)
    which host some NER corpora.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到许多近期的命名实体识别（NER）研究报告了它们在 CoNLL03 和 OntoNotes 数据集上的表现（参见表格 [III](#S3.T3
    "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder Architectures ‣
    3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning for Named Entity
    Recognition")）。CoNLL03 包含了两种语言的路透新闻注释：英语和德语。英语数据集包含了大量的体育新闻，标注了四种实体类型（Person、Location、Organization
    和 Miscellaneous）[[11](#bib.bib11)]。OntoNotes 项目的目标是对一个大语料库进行注释，包括各种类型（博客、新闻、谈话节目、广播、Usenet
    新闻组和对话电话语音）的结构信息（语法和谓词参数结构）和浅层语义（与本体链接的词义和共指）。共有 5 个版本，从 Release 1.0 到 Release
    5.0。文本被注释为 18 种实体类型。我们还注意到两个 Github 仓库¹¹1[https://github.com/juand-r/entity-recognition-datasets](https://github.com/juand-r/entity-recognition-datasets)
    和 [https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data](https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data)
    托管了一些 NER 语料库。
- en: 'TABLE II: Off-the-shelf NER tools offered by academia and industry projects.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 II：学术界和工业项目提供的现成 NER 工具。
- en: '| NER System | URL |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| NER 系统 | URL |'
- en: '| --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| StanfordCoreNLP | [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| StanfordCoreNLP | [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)
    |'
- en: '| OSU Twitter NLP | [https://github.com/aritter/twitter_nlp](https://github.com/aritter/twitter_nlp)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| OSU Twitter NLP | [https://github.com/aritter/twitter_nlp](https://github.com/aritter/twitter_nlp)
    |'
- en: '| Illinois NLP | [http://cogcomp.org/page/software/](http://cogcomp.org/page/software/)
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Illinois NLP | [http://cogcomp.org/page/software/](http://cogcomp.org/page/software/)
    |'
- en: '| NeuroNER | [http://neuroner.com/](http://neuroner.com/) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| NeuroNER | [http://neuroner.com/](http://neuroner.com/) |'
- en: '| NERsuite | [http://nersuite.nlplab.org/](http://nersuite.nlplab.org/) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| NERsuite | [http://nersuite.nlplab.org/](http://nersuite.nlplab.org/) |'
- en: '| Polyglot | [https://polyglot.readthedocs.io](https://polyglot.readthedocs.io)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Polyglot | [https://polyglot.readthedocs.io](https://polyglot.readthedocs.io)
    |'
- en: '| Gimli | [http://bioinformatics.ua.pt/gimli](http://bioinformatics.ua.pt/gimli)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Gimli | [http://bioinformatics.ua.pt/gimli](http://bioinformatics.ua.pt/gimli)
    |'
- en: '| spaCy | [https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| spaCy | [https://spacy.io/api/entityrecognizer](https://spacy.io/api/entityrecognizer)
    |'
- en: '| NLTK | [https://www.nltk.org](https://www.nltk.org) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| NLTK | [https://www.nltk.org](https://www.nltk.org) |'
- en: '| OpenNLP | [https://opennlp.apache.org/](https://opennlp.apache.org/) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| OpenNLP | [https://opennlp.apache.org/](https://opennlp.apache.org/) |'
- en: '| LingPipe | [http://alias-i.com/lingpipe-3.9.3/](http://alias-i.com/lingpipe-3.9.3/)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LingPipe | [http://alias-i.com/lingpipe-3.9.3/](http://alias-i.com/lingpipe-3.9.3/)
    |'
- en: '| AllenNLP | [https://demo.allennlp.org/](https://demo.allennlp.org/) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| AllenNLP | [https://demo.allennlp.org/](https://demo.allennlp.org/) |'
- en: '| IBM Watson | https://natural-language-understanding-demo.ng.bluemix.net |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| IBM Watson | https://natural-language-understanding-demo.ng.bluemix.net |'
- en: '| FG-NER | [https://fgner.alt.ai/extractor/](https://fgner.alt.ai/extractor/)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FG-NER | [https://fgner.alt.ai/extractor/](https://fgner.alt.ai/extractor/)
    |'
- en: '| Intellexer | [http://demo.intellexer.com/](http://demo.intellexer.com/) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Intellexer | [http://demo.intellexer.com/](http://demo.intellexer.com/) |'
- en: '| Repustate | https://repustate.com/named-entity-recognition-api-demo |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Repustate | https://repustate.com/named-entity-recognition-api-demo |'
- en: '| AYLIEN | [https://developer.aylien.com/text-api-demo](https://developer.aylien.com/text-api-demo)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| AYLIEN | [https://developer.aylien.com/text-api-demo](https://developer.aylien.com/text-api-demo)
    |'
- en: '| Dandelion API | https://dandelion.eu/semantic-text/entity-extraction-demo
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Dandelion API | https://dandelion.eu/semantic-text/entity-extraction-demo
    |'
- en: '| displaCy | [https://explosion.ai/demos/displacy-ent](https://explosion.ai/demos/displacy-ent)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| displaCy | [https://explosion.ai/demos/displacy-ent](https://explosion.ai/demos/displacy-ent)
    |'
- en: '| ParallelDots | https://www.paralleldots.com/named-entity-recognition |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ParallelDots | https://www.paralleldots.com/named-entity-recognition |'
- en: '| TextRazor | https://www.textrazor.com/named _entity_recognition |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| TextRazor | https://www.textrazor.com/named_entity_recognition |'
- en: 'There are many NER tools available online with pre-trained models. Table [II](#S2.T2
    "TABLE II ‣ 2.2 NER Resources: Datasets and Tools ‣ 2 Background ‣ A Survey on
    Deep Learning for Named Entity Recognition") summarizes popular ones for English
    NER by academia (top) and industry (bottom).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在线有许多具有预训练模型的NER工具。表[II](#S2.T2 "TABLE II ‣ 2.2 NER Resources: Datasets and
    Tools ‣ 2 Background ‣ A Survey on Deep Learning for Named Entity Recognition")总结了学术界（上）和工业界（下）对英语NER的流行工具。'
- en: 2.3 NER Evaluation Metrics
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 NER 评估指标
- en: NER systems are usually evaluated by comparing their outputs against human annotations.
    The comparison can be quantified by either exact-match or relaxed match.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: NER系统通常通过将其输出与人工注释进行比较来进行评估。比较可以通过精确匹配或宽松匹配来量化。
- en: 2.3.1 Exact-match Evaluation
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 精确匹配评估
- en: 'NER essentially involves two subtasks: boundary detection and type identification.
    In “exact-match evaluation” [[11](#bib.bib11), [41](#bib.bib41), [42](#bib.bib42)],
    a correctly recognized instance requires a system to correctly identify its boundary
    and type, simultaneously. More specifically, the numbers of False positives (FP),
    False negatives (FN) and True positives (TP) are used to compute Precision, Recall,
    and F-score.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: NER 本质上涉及两个子任务：边界检测和类型识别。在“精确匹配评估”[[11](#bib.bib11), [41](#bib.bib41), [42](#bib.bib42)]中，正确识别的实例要求系统同时正确识别其边界和类型。更具体地说，利用假阳性（FP）、假阴性（FN）和真阳性（TP）的数量来计算精度、召回率和F值。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False Positive (FP): entity that is returned by a NER system but does not appear
    in the ground truth.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假阳性（FP）：由NER系统返回但未出现在真实数据中的实体。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False Negative (FN): entity that is not returned by a NER system but appears
    in the ground truth.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假阴性（FN）：由NER系统未返回但出现在真实数据中的实体。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'True Positive (TP): entity that is returned by a NER system and also appears
    in the ground truth.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真阳性（TP）：由NER系统返回并且也出现在真实数据中的实体。
- en: Precision refers to the percentage of your system results which are correctly
    recognized. Recall refers to the percentage of total entities correctly recognized
    by your system.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 精度指的是系统结果中正确识别的百分比。召回率指的是系统正确识别的总实体的百分比。
- en: '|  | $\text{Precision}=\frac{\#TP}{\#(TP+FP)}\quad\quad\text{Recall}=\frac{\#TP}{\#(TP+FN)}$
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Precision}=\frac{\#TP}{\#(TP+FP)}\quad\quad\text{Recall}=\frac{\#TP}{\#(TP+FN)}$
    |  |'
- en: 'A measure that combines precision and recall is the harmonic mean of precision
    and recall, the traditional F-measure or balanced F-score:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一种结合精确度和召回率的度量是精确度和召回率的调和平均数，即传统的F-measure或平衡F-score：
- en: '|  | $\text{F-score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$
    |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{F-score}=2\times\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$
    |  |'
- en: In addition, the macro-averaged F-score and micro-averaged F-score both consider
    the performance across multiple entity types. Macro-averaged F-score independently
    calculates the F-score on different entity types, then takes the average of the
    F-scores. Micro-averaged F-score sums up the individual false negatives, false
    positives and true positives across all entity types then applies them to get
    the statistics. The latter can be heavily affected by the quality of recognizing
    entities in large classes in the corpus.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，宏平均F-score和微平均F-score都考虑了多个实体类型的表现。宏平均F-score独立计算不同实体类型的F-score，然后取这些F-scores的平均值。微平均F-score则将所有实体类型的假阴性、假阳性和真正例相加，然后应用于统计。这种方法可能会受到大类别实体识别质量的较大影响。
- en: 2.3.2 Relaxed-match Evaluation
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 放宽匹配评估
- en: 'MUC-6 [[10](#bib.bib10)] defines a relaxed-match evaluation: a correct type
    is credited if an entity is assigned its correct type regardless its boundaries
    as long as there is an overlap with ground truth boundaries; a correct boundary
    is credited regardless an entity’s type assignment. Then ACE [[12](#bib.bib12)]
    proposes a more complex evaluation procedure. It resolves a few issues like partial
    match and wrong type, and considers subtypes of named entities. However, it is
    problematic because the final scores are comparable only when parameters are fixed [[1](#bib.bib1),
    [23](#bib.bib23), [22](#bib.bib22)]. Complex evaluation methods are not intuitive
    and make error analysis difficult. Thus, complex evaluation methods are not widely
    used in recent studies.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MUC-6[[10](#bib.bib10)]定义了放宽匹配评估：如果一个实体被分配到正确的类型，而其边界与真实边界有重叠，即使其边界不完全正确，也会被记为正确类型；如果一个实体的边界正确，不论其类型分配如何，也会被记为正确边界。然后ACE[[12](#bib.bib12)]提出了更复杂的评估程序。它解决了部分匹配和错误类型等问题，并考虑了命名实体的子类型。然而，这种方法存在问题，因为最终得分仅在参数固定时才具有可比性[[1](#bib.bib1),
    [23](#bib.bib23), [22](#bib.bib22)]。复杂的评估方法不直观，且使得错误分析变得困难。因此，复杂的评估方法在最近的研究中不被广泛使用。
- en: 2.4 Traditional Approaches to NER
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 传统NER方法
- en: 'Traditional approaches to NER are broadly classified into three main streams:
    rule-based, unsupervised learning, and feature-based supervised learning approaches [[1](#bib.bib1),
    [26](#bib.bib26)].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的NER方法大致分为三大类：基于规则的方法、无监督学习方法和基于特征的监督学习方法[[1](#bib.bib1), [26](#bib.bib26)]。
- en: 2.4.1 Rule-based Approaches
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 基于规则的方法
- en: Rule-based NER systems rely on hand-crafted rules. Rules can be designed based
    on domain-specific gazetteers [[43](#bib.bib43), [9](#bib.bib9)] and syntactic-lexical
    patterns [[44](#bib.bib44)]. Kim [[45](#bib.bib45)] proposed to use Brill rule
    inference approach for speech input. This system generates rules automatically
    based on Brill’s part-of-speech tagger. In biomedical domain, Hanisch et al. [[46](#bib.bib46)]
    proposed ProMiner, which leverages a pre-processed synonym dictionary to identify
    protein mentions and potential gene in biomedical text. Quimbaya et al. [[47](#bib.bib47)]
    proposed a dictionary-based approach for NER in electronic health records. Experimental
    results show the approach improves recall while having limited impact on precision.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的NER系统依赖于手工制定的规则。规则可以根据领域特定的词典[[43](#bib.bib43), [9](#bib.bib9)]和句法-词汇模式[[44](#bib.bib44)]来设计。Kim[[45](#bib.bib45)]提议使用Brill规则推断方法处理语音输入。该系统基于Brill的词性标注器自动生成规则。在生物医学领域，Hanisch等人[[46](#bib.bib46)]提出了ProMiner，该系统利用预处理的同义词词典来识别生物医学文本中的蛋白质提及和潜在基因。Quimbaya等人[[47](#bib.bib47)]提出了一种基于词典的方法，用于电子健康记录中的NER。实验结果显示该方法提高了召回率，同时对精确度影响有限。
- en: Some other well-known rule-based NER systems include LaSIE-II [[48](#bib.bib48)],
    NetOwl [[49](#bib.bib49)], Facile [[50](#bib.bib50)], SAR [[51](#bib.bib51)],
    FASTUS [[52](#bib.bib52)], and LTG [[53](#bib.bib53)] systems. These systems are
    mainly based on hand-crafted semantic and syntactic rules to recognize entities.
    Rule-based systems work very well when lexicon is exhaustive. Due to domain-specific
    rules and incomplete dictionaries, high precision and low recall are often observed
    from such systems, and the systems cannot be transferred to other domains.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他著名的基于规则的NER系统包括 LaSIE-II [[48](#bib.bib48)]、NetOwl [[49](#bib.bib49)]、Facile
    [[50](#bib.bib50)]、SAR [[51](#bib.bib51)]、FASTUS [[52](#bib.bib52)] 和 LTG [[53](#bib.bib53)]
    系统。这些系统主要基于手工制作的语义和句法规则来识别实体。基于规则的系统在词汇表完整时效果很好。由于领域特定规则和不完整的词典，这些系统通常表现出高精度和低召回率，并且这些系统无法转移到其他领域。
- en: 2.4.2 Unsupervised Learning Approaches
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 无监督学习方法
- en: A typical approach of unsupervised learning is clustering [[1](#bib.bib1)].
    Clustering-based NER systems extract named entities from the clustered groups
    based on context similarity. The key idea is that lexical resources, lexical patterns,
    and statistics computed on a large corpus can be used to infer mentions of named
    entities. Collins et al. [[54](#bib.bib54)] observed that use of unlabeled data
    reduces the requirements for supervision to just 7 simple “seed” rules. The authors
    then presented two unsupervised algorithms for named entity classification. Similarly,
    KNOWITALL [[9](#bib.bib9)] leveraged a set of predicate names as input and bootstraps
    its recognition process from a small set of generic extraction patterns.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的典型方法是聚类 [[1](#bib.bib1)]。基于聚类的NER系统根据上下文相似性从聚类组中提取命名实体。关键思想是，词汇资源、词汇模式和在大型语料库上计算的统计数据可以用来推断命名实体的提及。Collins
    等 [[54](#bib.bib54)] 观察到使用未标记的数据将监督要求减少到仅仅 7 条简单的“种子”规则。作者随后提出了两种用于命名实体分类的无监督算法。类似地，KNOWITALL
    [[9](#bib.bib9)] 利用一组谓词名称作为输入，并从一小组通用提取模式中引导其识别过程。
- en: Nadeau et al. [[55](#bib.bib55)] proposed an unsupervised system for gazetteer
    building and named entity ambiguity resolution. This system combines entity extraction
    and disambiguation based on simple yet highly effective heuristics. In addition,
    Zhang and Elhadad [[44](#bib.bib44)] proposed an unsupervised approach to extracting
    named entities from biomedical text. Instead of supervision, their model resorts
    to terminologies, corpus statistics (e.g., inverse document frequency and context
    vectors) and shallow syntactic knowledge (e.g., noun phrase chunking). Experiments
    on two mainstream biomedical datasets demonstrate the effectiveness and generalizability
    of their unsupervised approach.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Nadeau 等 [[55](#bib.bib55)] 提出了一个用于地名库构建和命名实体歧义解决的无监督系统。该系统结合了基于简单但高度有效的启发式方法的实体提取和消歧。此外，Zhang
    和 Elhadad [[44](#bib.bib44)] 提出了从生物医学文本中提取命名实体的无监督方法。他们的模型依靠术语、语料库统计（例如逆文档频率和上下文向量）以及浅层句法知识（例如名词短语切分）来代替监督。对两个主流生物医学数据集的实验展示了他们的无监督方法的有效性和泛化能力。
- en: 2.4.3 Feature-based Supervised Learning Approaches
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 基于特征的监督学习方法
- en: Applying supervised learning, NER is cast to a multi-class classification or
    sequence labeling task. Given annotated data samples, features are carefully designed
    to represent each training example. Machine learning algorithms are then utilized
    to learn a model to recognize similar patterns from unseen data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 采用监督学习，命名实体识别（NER）被转化为多类分类或序列标注任务。在给定的标注数据样本中，特征被精心设计以表示每个训练实例。然后，使用机器学习算法来学习一个模型，以从未见过的数据中识别类似的模式。
- en: Feature engineering is critical in supervised NER systems. Feature vector representation
    is an abstraction over text where a word is represented by one or many Boolean,
    numeric, or nominal values [[56](#bib.bib56), [1](#bib.bib1)]. Word-level features
    (e.g., case, morphology, and part-of-speech tag) [[57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59)], list lookup features (e.g., Wikipedia gazetteer and DBpedia
    gazetteer) [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)],
    and document and corpus features (e.g., local syntax and multiple occurrences) [[64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)] have been widely used in
    various supervised NER systems. More feature designs are discussed in [[1](#bib.bib1),
    [68](#bib.bib68), [28](#bib.bib28)]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程在监督NER系统中至关重要。特征向量表示是对文本的抽象，其中一个词可以由一个或多个布尔值、数值或名义值[[56](#bib.bib56)、[1](#bib.bib1)]表示。词级特征（例如，大小写、形态学和词性标签）[[57](#bib.bib57)、[58](#bib.bib58)、[59](#bib.bib59)]，列表查找特征（例如，维基百科地名和DBpedia地名）[[60](#bib.bib60)、[61](#bib.bib61)、[62](#bib.bib62)、[63](#bib.bib63)]，以及文档和语料库特征（例如，本地语法和多次出现）[[64](#bib.bib64)、[65](#bib.bib65)、[66](#bib.bib66)、[67](#bib.bib67)]在各种监督NER系统中被广泛使用。更多特征设计的讨论见[[1](#bib.bib1)、[68](#bib.bib68)、[28](#bib.bib28)]。
- en: Based on these features, many machine learning algorithms have been applied
    in supervised NER, including Hidden Markov Models (HMM) [[69](#bib.bib69)], Decision
    Trees [[70](#bib.bib70)], Maximum Entropy Models [[71](#bib.bib71)], Support Vector
    Machines (SVM) [[72](#bib.bib72)], and Conditional Random Fields (CRF) [[73](#bib.bib73)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些特征，许多机器学习算法已被应用于监督命名实体识别（NER），包括隐马尔可夫模型（HMM）[[69](#bib.bib69)]、决策树[[70](#bib.bib70)]、最大熵模型[[71](#bib.bib71)]、支持向量机（SVM）[[72](#bib.bib72)]和条件随机场（CRF）[[73](#bib.bib73)]。
- en: Bikel et al. [[74](#bib.bib74), [75](#bib.bib75)] proposed the first HMM-based
    NER system, named IdentiFinder, to identify and classify names, dates, time expressions,
    and numerical quantities. In addition, Szarvas et al. [[76](#bib.bib76)] developed
    a multilingual NER system by using C4.5 decision tree and AdaBoostM1 learning
    algorithm. A major merit is that it provides an opportunity to train several independent
    decision tree classifiers through different subsets of features then combine their
    decisions through a majority voting scheme. Borthwick et al. [[77](#bib.bib77)]
    proposed “maximum entropy named entity” (MENE) by applying the maximum entropy
    theory. MENE is able to make use of an extraordinarily diverse range of knowledge
    sources in making its tagging decisions. Other systems using maximum entropy can
    be found in [[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)].
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Bikel等人[[74](#bib.bib74)、[75](#bib.bib75)]提出了第一个基于HMM的NER系统，名为IdentiFinder，用于识别和分类名称、日期、时间表达式和数值量。此外，Szarvas等人[[76](#bib.bib76)]通过使用C4.5决策树和AdaBoostM1学习算法开发了一个多语言NER系统。其主要优点是提供了通过不同特征子集训练多个独立的决策树分类器的机会，然后通过多数投票机制结合它们的决策。Borthwick等人[[77](#bib.bib77)]提出了通过应用最大熵理论的“最大熵命名实体”（MENE）。MENE能够利用极其多样化的知识源来做出标记决策。其他使用最大熵的系统可以在[[78](#bib.bib78)、[79](#bib.bib79)、[80](#bib.bib80)]中找到。
- en: McNamee and Mayfield [[81](#bib.bib81)] used 1000 language-related and 258 orthography
    and punctuation features to train SVM classifiers. Each classifier makes binary
    decision whether the current token belongs to one of the eight classes, i.e.,
    B- (Beginning), I- (Inside) for PERSON, ORGANIZATION, LOCATION, and MIS tags.
    SVM does not consider “neighboring” words when predicting an entity label. CRFs
    takes context into account. McCallum and Li [[82](#bib.bib82)] proposed a feature
    induction method for CRFs in NER. Experiments were performed on CoNLL03, and achieved
    F-score of 84.04% for English. Krishnan and Manning [[67](#bib.bib67)] proposed
    a two-stage approach based on two coupled CRF classifiers. The second CRF makes
    use of the latent representations derived from the output of the first CRF. We
    note that CRF-based NER has been widely applied to texts in various domains, including
    biomedical text [[58](#bib.bib58), [83](#bib.bib83)], tweets [[84](#bib.bib84),
    [85](#bib.bib85)] and chemical text [[86](#bib.bib86)].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: McNamee和Mayfield[[81](#bib.bib81)]使用了1000个与语言相关的特征和258个正字法及标点特征来训练SVM分类器。每个分类器做出二元决策，判断当前的标记是否属于八个类别之一，即B-（开始）、I-（内部）用于PERSON、ORGANIZATION、LOCATION和MIS标签。SVM在预测实体标签时不会考虑“邻近”的词语。CRF则考虑了上下文。McCallum和Li[[82](#bib.bib82)]提出了一种用于NER的CRF特征诱导方法。实验在CoNLL03上进行，取得了84.04%的F-score。Krishnan和Manning[[67](#bib.bib67)]提出了一种基于两个耦合CRF分类器的两阶段方法。第二个CRF利用从第一个CRF输出中得到的潜在表示。我们注意到，基于CRF的NER已广泛应用于各个领域的文本，包括生物医学文本[[58](#bib.bib58),
    [83](#bib.bib83)]、推文[[84](#bib.bib84), [85](#bib.bib85)]和化学文本[[86](#bib.bib86)]。
- en: 3 Deep Learning Techniques for NER
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习技术用于NER
- en: In recent years, DL-based NER models become dominant and achieve state-of-the-art
    results. Compared to feature-based approaches, deep learning is beneficial in
    discovering hidden features automatically. Next, we first briefly introduce what
    deep learning is, and why deep learning for NER. We then survey DL-based NER approaches.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于深度学习（DL）的命名实体识别（NER）模型变得主导，并取得了最先进的结果。与基于特征的方法相比，深度学习在自动发现隐藏特征方面具有优势。接下来，我们首先简要介绍深度学习是什么，以及为什么使用深度学习进行NER。然后，我们将调查基于DL的NER方法。
- en: 3.1 Why Deep Learning for NER?
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 为什么选择深度学习用于NER？
- en: Deep learning is a field of machine learning that is composed of multiple processing
    layers to learn representations of data with multiple levels of abstraction [[87](#bib.bib87)].
    The typical layers are artificial neural networks which consists of the forward
    pass and backward pass. The forward pass computes a weighted sum of their inputs
    from the previous layer and pass the result through a non-linear function. The
    backward pass is to compute the gradient of an objective function with respect
    to the weights of a multilayer stack of modules via the chain rule of derivatives.
    The key advantage of deep learning is the capability of representation learning
    and the semantic composition empowered by both the vector representation and neural
    processing. This allows a machine to be fed with raw data and to automatically
    discover latent representations and processing needed for classification or detection
    [[87](#bib.bib87)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个领域，由多个处理层组成，以学习具有多层次抽象的数据表示[[87](#bib.bib87)]。典型的层是人工神经网络，包括前向传播和反向传播。前向传播计算来自前一层的输入的加权和，并通过非线性函数传递结果。反向传播是通过导数链式法则计算目标函数相对于多层模块堆叠的权重的梯度。深度学习的关键优势在于表示学习和由向量表示及神经处理赋能的语义组合能力。这使得机器可以接收原始数据，自动发现分类或检测所需的潜在表示和处理[[87](#bib.bib87)]。
- en: There are three core strengths of applying deep learning techniques to NER.
    First, NER benefits from the non-linear transformation, which generates non-linear
    mappings from input to output. Compared with linear models (e.g., log-linear HMM
    and linear chain CRF), DL-based models are able to learn complex and intricate
    features from data via non-linear activation functions. Second, deep learning
    saves significant effort on designing NER features. The traditional feature-based
    approaches require considerable amount of engineering skill and domain expertise.
    DL-based models, on the other hand, are effective in automatically learning useful
    representations and underlying factors from raw data. Third, deep neural NER models
    can be trained in an end-to-end paradigm, by gradient descent. This property enables
    us to design possibly complex NER systems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习技术应用于NER有三个核心优势。首先，NER受益于非线性变换，这生成了从输入到输出的非线性映射。与线性模型（例如，log-linear HMM和线性链CRF）相比，基于深度学习的模型能够通过非线性激活函数从数据中学习复杂和细致的特征。其次，深度学习显著减少了设计NER特征的工作量。传统的基于特征的方法需要大量的工程技能和领域专业知识。而基于深度学习的模型则能够有效地从原始数据中自动学习有用的表示和潜在因素。第三，深度神经NER模型可以通过梯度下降以端到端的方式进行训练。这一特性使我们能够设计可能复杂的NER系统。
- en: '![Refer to caption](img/8bdcc12aeba5eb1f77bc010b45c25ecd.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8bdcc12aeba5eb1f77bc010b45c25ecd.png)'
- en: 'Figure 2: The taxonomy of DL-based NER. From input sequence to predicted tags,
    a DL-based NER model consists of distributed representations for input, context
    encoder, and tag decoder.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的命名实体识别（NER）分类。从输入序列到预测标签，基于深度学习的NER模型包括输入的分布式表示、上下文编码器和标签解码器。
- en: '![Refer to caption](img/b2e70a8829d9fd6b502ec95e49a38811.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b2e70a8829d9fd6b502ec95e49a38811.png)'
- en: (a) CNN-based character-level representation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基于CNN的字符级表示。
- en: '![Refer to caption](img/78fefe397d97d4238c123fc0f2bd1960.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/78fefe397d97d4238c123fc0f2bd1960.png)'
- en: (b) RNN-based character-level representation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于RNN的字符级表示。
- en: 'Figure 3: CNN-based and RNN-based models for extracting character-level representation
    for a word.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：用于提取单词字符级表示的CNN-based和RNN-based模型。
- en: 'Why we use a new taxonomy in this survey? Existing taxonomy [[88](#bib.bib88),
    [26](#bib.bib26)] is based on character-level encoder, word-level encoder, and
    tag decoder. We argue that the description of “word-level encoder” is inaccurate
    because word-level information is used twice in a typical DL-based NER model:
    1) word-level representations are used as raw features, and 2) word-level representations
    (together with character-level representations) are used to capture context dependence
    for tag decoding. In this survey, we summarize recent advances in NER with the
    general architecture presented in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Why Deep Learning
    for NER? ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning for
    Named Entity Recognition"). Distributed representations for input consider word-
    and character-level embeddings as well as incorporation of additional features
    like POS tag and gazetteer that have been effective in feature-based based approaches.
    Context encoder is to capture the context dependencies using CNN, RNN, or other
    networks. Tag decoder predict tags for tokens in the input sequence. For instance,
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Why Deep Learning for NER? ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition")
    each token is predicted with a tag indicated by B-(begin), I-(inside), E-(end),
    S-(singleton) of a named entity with its type, or O-(outside) of named entities.
    Note that there are other tag schemes or tag notations, e.g., BIO. Tag decoder
    may also be trained to detect entity boundaries and then the detected text spans
    are classified to the entity types.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本调查中为何使用新的分类法？现有的分类法[[88](#bib.bib88), [26](#bib.bib26)] 基于字符级编码器、词级编码器和标签解码器。我们认为“词级编码器”的描述不准确，因为在典型的基于深度学习的命名实体识别模型中，词级信息使用了两次：1）词级表示作为原始特征使用，2）词级表示（与字符级表示一起）用于捕捉上下文依赖以进行标签解码。在本调查中，我们总结了命名实体识别领域的最新进展，通用架构如图[2](#S3.F2
    "Figure 2 ‣ 3.1 Why Deep Learning for NER? ‣ 3 Deep Learning Techniques for NER
    ‣ A Survey on Deep Learning for Named Entity Recognition")所示。输入的分布式表示考虑了词级和字符级嵌入以及其他有效的附加特征，如POS标签和词典，这些特征在基于特征的方法中效果显著。上下文编码器使用CNN、RNN或其他网络来捕捉上下文依赖。标签解码器预测输入序列中每个标记的标签。例如，在图[2](#S3.F2
    "Figure 2 ‣ 3.1 Why Deep Learning for NER? ‣ 3 Deep Learning Techniques for NER
    ‣ A Survey on Deep Learning for Named Entity Recognition")中，每个标记都预测一个由B-(begin)、I-(inside)、E-(end)、S-(singleton)表示的命名实体类型的标签，或O-(outside)表示非命名实体。请注意，还有其他标签方案或标签符号，例如BIO。标签解码器也可以被训练来检测实体边界，然后将检测到的文本跨度分类到实体类型。
- en: 3.2 Distributed Representations for Input
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 输入的分布式表示
- en: 'A straightforward option of representing a word is one-hot vector representation.
    In one-hot vector space, two distinct words have completely different representations
    and are orthogonal. Distributed representation represents words in low dimensional
    real-valued dense vectors where each dimension represents a latent feature. Automatically
    learned from text, distributed representation captures semantic and syntactic
    properties of word, which do not explicitly present in the input to NER. Next,
    we review three types of distributed representations that have been used in NER
    models: word-level, character-level, and hybrid representations.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表示一个词的直接选择是独热向量表示。在独热向量空间中，两个不同的词具有完全不同的表示，并且是正交的。分布式表示以低维实值密集向量表示词汇，其中每个维度代表一个潜在特征。自动从文本中学习的分布式表示捕捉了词的语义和句法特性，这些特性在NER输入中并没有明确呈现。接下来，我们回顾了在NER模型中使用的三种类型的分布式表示：词级、字符级和混合表示。
- en: 3.2.1 Word-level Representation
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 词级表示
- en: Some studies [[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)] employed
    word-level representation, which is typically pre-trained over large collections
    of text through unsupervised algorithms such as continuous bag-of-words (CBOW)
    and continuous skip-gram models [[92](#bib.bib92)]. Recent studies [[88](#bib.bib88),
    [93](#bib.bib93)] have shown the importance of such pre-trained word embeddings.
    Using as the input, the pre-trained word embeddings can be either fixed or further
    fine-tuned during NER model training. Commonly used word embeddings include Google
    Word2Vec, Stanford GloVe, Facebook fastText and SENNA.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究 [[89](#bib.bib89), [90](#bib.bib90), [91](#bib.bib91)] 使用了词级表示，这通常是通过无监督算法在大规模文本集合上预训练的，例如连续词袋模型（CBOW）和连续
    skip-gram 模型 [[92](#bib.bib92)]。最近的研究 [[88](#bib.bib88), [93](#bib.bib93)] 显示了这些预训练词嵌入的重要性。作为输入时，预训练词嵌入可以是固定的，也可以在
    NER 模型训练过程中进一步微调。常用的词嵌入包括 Google Word2Vec、Stanford GloVe、Facebook fastText 和 SENNA。
- en: 'Yao et al. [[94](#bib.bib94)] proposed Bio-NER, a biomedical NER model based
    on deep neural network architecture. The word representation in Bio-NER is trained
    on PubMed database using skip-gram model. The dictionary contains 205,924 words
    in 600 dimensional vectors. Nguyen et al. [[89](#bib.bib89)] used word2vec toolkit
    to learn word embeddings for English from the Gigaword corpus augmented with newsgroups
    data from BOLT (Broad Operational Language Technologies). Zhai et al. [[95](#bib.bib95)]
    designed a neural model for sequence chunking, which consists of two sub-tasks:
    segmentation and labeling. The neural model can be fed with SENNA embeddings or
    randomly initialized embeddings.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Yao 等人 [[94](#bib.bib94)] 提出了 Bio-NER，这是一种基于深度神经网络架构的生物医学命名实体识别（NER）模型。Bio-NER
    中的词表示是在 PubMed 数据库上使用 skip-gram 模型进行训练的。词典包含 205,924 个词，使用 600 维向量表示。Nguyen 等人
    [[89](#bib.bib89)] 使用 word2vec 工具包从 Gigaword 语料库中学习英语词嵌入，并结合了来自 BOLT（Broad Operational
    Language Technologies）的新闻组数据。Zhai 等人 [[95](#bib.bib95)] 设计了一种用于序列切块的神经模型，该模型包括两个子任务：分割和标注。该神经模型可以使用
    SENNA 嵌入或随机初始化的嵌入。
- en: Zheng et al. [[90](#bib.bib90)] jointly extracted entities and relations using
    a single model. This end-to-end model uses word embeddings learned on NYT corpus
    by word2vec tookit. Strubell et al. [[91](#bib.bib91)] proposed a tagging scheme
    based on Iterated Dilated Convolutional Neural Networks (ID-CNNs). The lookup
    table in their model are initialized by 100-dimensional embeddings trained on
    SENNA corpus by skip-n-gram. In their proposed neural model for extracting entities
    and their relations, Zhou et al. [[96](#bib.bib96)] used the pre-trained 300-dimensional
    word vectors from Google. In addition, GloVe [[97](#bib.bib97), [98](#bib.bib98)]
    and fastText [[99](#bib.bib99)] are also widely used in NER tasks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng 等人 [[90](#bib.bib90)] 使用单一模型共同提取实体和关系。该端到端模型使用在 NYT 语料库上通过 word2vec 工具包学习的词嵌入。Strubell
    等人 [[91](#bib.bib91)] 提出了基于迭代膨胀卷积神经网络（ID-CNNs）的标注方案。他们模型中的查找表由在 SENNA 语料库上通过 skip-n-gram
    训练的 100 维嵌入初始化。在他们提出的用于提取实体及其关系的神经模型中，Zhou 等人 [[96](#bib.bib96)] 使用了从 Google 获得的预训练
    300 维词向量。此外，GloVe [[97](#bib.bib97), [98](#bib.bib98)] 和 fastText [[99](#bib.bib99)]
    也在 NER 任务中得到了广泛应用。
- en: 3.2.2 Character-level Representation
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 字符级表示
- en: 'Instead of only considering word-level representations as the basic input,
    several studies [[100](#bib.bib100), [101](#bib.bib101)] incorporated character-based
    word representations learned from an end-to-end neural model. Character-level
    representation has been found useful for exploiting explicit sub-word-level information
    such as prefix and suffix. Another advantage of character-level representation
    is that it naturally handles out-of-vocabulary. Thus character-based model is
    able to infer representations for unseen words and share information of morpheme-level
    regularities. There are two widely-used architectures for extracting character-level
    representation: CNN-based and RNN-based models. Figures [3(a)](#S3.F3.sf1 "In
    Figure 3 ‣ 3.1 Why Deep Learning for NER? ‣ 3 Deep Learning Techniques for NER
    ‣ A Survey on Deep Learning for Named Entity Recognition") and [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3.1 Why Deep Learning for NER? ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition") illustrate the
    two architectures.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除了仅将词级表示作为基本输入外，几项研究 [[100](#bib.bib100), [101](#bib.bib101)] 还结合了从端到端神经模型中学习的基于字符的词表示。字符级表示被发现对利用显式的子词级信息（如前缀和后缀）非常有用。字符级表示的另一个优点是它自然地处理了词汇外的情况。因此，基于字符的模型能够推断未见词的表示并共享词素级的规律信息。提取字符级表示有两种广泛使用的架构：基于
    CNN 的和基于 RNN 的模型。图 [3(a)](#S3.F3.sf1 "图 3 ‣ 3.1 为什么使用深度学习进行 NER? ‣ 3 NER 的深度学习技术
    ‣ 关于命名实体识别的深度学习综述") 和 [3(b)](#S3.F3.sf2 "图 3 ‣ 3.1 为什么使用深度学习进行 NER? ‣ 3 NER 的深度学习技术
    ‣ 关于命名实体识别的深度学习综述") 说明了这两种架构。
- en: Ma et al. [[97](#bib.bib97)] utilized a CNN for extracting character-level representations
    of words. Then the character representation vector is concatenated with the word
    embedding before feeding into a RNN context encoder. Likewise, Li et al. [[98](#bib.bib98)]
    applied a series of convolutional and highway layers to generate character-level
    representations for words. The final embeddings of words are fed into a bidirectional
    recursive network. Yang et al. [[102](#bib.bib102)] proposed a neural reranking
    model for NER, where a convolutional layer with a fixed window-size is used on
    top of a character embedding layer. Recently, Peters et al. [[103](#bib.bib103)]
    proposed ELMo word representation, which are computed on top of two-layer bidirectional
    language models with character convolutions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 等人 [[97](#bib.bib97)] 利用 CNN 提取单词的字符级表示。然后，将字符表示向量与词嵌入连接，再输入到 RNN 上下文编码器中。同样，Li
    等人 [[98](#bib.bib98)] 应用了一系列卷积层和高速公路层来生成单词的字符级表示。单词的最终嵌入被输入到一个双向递归网络中。Yang 等人
    [[102](#bib.bib102)] 提出了一个用于命名实体识别（NER）的神经重排序模型，其中在字符嵌入层之上使用了一个具有固定窗口大小的卷积层。最近，Peters
    等人 [[103](#bib.bib103)] 提出了 ELMo 词表示，该表示是在两个层的双向语言模型与字符卷积的基础上计算的。
- en: For RNN-based models, Long Short-Term Memory (LSTM) and Gated Recurrent Unit
    (GRU) are two typical choices of the basic units. Kuru et al. [[100](#bib.bib100)]
    proposed CharNER, a character-level tagger for language-independent NER. CharNER
    considers a sentence as a sequence of characters and utilizes LSTMs to extract
    character-level representations. It outputs a tag distribution for each character
    instead of each word. Then word-level tags are obtained from the character-level
    tags. Their results show that taking characters as the primary representation
    is superior to words as the basic input unit. Lample et al. [[19](#bib.bib19)]
    utilized a bidirectional LSTM to extract character-level representations of words.
    Similar to [[97](#bib.bib97)], character-level representation is concatenated
    with pre-trained word-level embedding from a word lookup table. Gridach [[104](#bib.bib104)]
    investigated word embeddings and character-level representation in identifying
    biomedical named entities. Rei et al. [[105](#bib.bib105)] combined character-level
    representations with word embeddings using a gating mechanism. In this way, Rei’s
    model dynamically decides how much information to use from a character- or word-level
    component. Tran et al. [[101](#bib.bib101)] introduced a neural NER model with
    stack residual LSTM and trainable bias decoding, where word features are extracted
    from word embeddings and character-level RNN. Yang et al. [[106](#bib.bib106)]
    developed a model to handle both cross-lingual and multi-task joint training in
    a unified manner. They employed a deep bidirectional GRU to learn informative
    morphological representation from the character sequence of a word. Then character-level
    representation and word embedding are concatenated to produce the final representation
    for a word.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 RNN 的模型，长短期记忆网络（LSTM）和门控递归单元（GRU）是两个典型的基本单元选择。Kuru 等人[[100](#bib.bib100)]
    提出了 CharNER，一个用于语言无关 NER 的字符级标注器。CharNER 将句子视为字符序列，并利用 LSTM 提取字符级表示。它为每个字符输出一个标签分布，而不是每个单词。然后，从字符级标签中获得词级标签。他们的结果表明，将字符作为主要表示优于将词作为基本输入单元。Lample
    等人[[19](#bib.bib19)] 使用双向 LSTM 提取单词的字符级表示。类似于[[97](#bib.bib97)]，字符级表示与来自词查找表的预训练词级嵌入连接在一起。Gridach
    [[104](#bib.bib104)] 研究了在识别生物医学命名实体时的词嵌入和字符级表示。Rei 等人[[105](#bib.bib105)] 结合了字符级表示和词嵌入，使用了门控机制。通过这种方式，Rei
    的模型动态决定从字符或词级组件中使用多少信息。Tran 等人[[101](#bib.bib101)] 引入了一种具有堆叠残差 LSTM 和可训练偏置解码的神经
    NER 模型，其中从词嵌入和字符级 RNN 中提取词特征。Yang 等人[[106](#bib.bib106)] 开发了一种以统一方式处理跨语言和多任务联合训练的模型。他们采用深度双向
    GRU 从单词的字符序列中学习信息丰富的形态学表示。然后，将字符级表示和词嵌入连接在一起，以产生单词的最终表示。
- en: Recent advances in language modeling using recurrent neural networks made it
    viable to model language as distributions over characters. The contextual string
    embeddings by Akbik et al. [[107](#bib.bib107)], uses character-level neural language
    model to generate a contextualized embedding for a string of characters in a sentential
    context. An important property is that the embeddings are contextualized by their
    surrounding text, meaning that the same word has different embeddings depending
    on its contextual use. Figure [4](#S3.F4 "Figure 4 ‣ 3.2.2 Character-level Representation
    ‣ 3.2 Distributed Representations for Input ‣ 3 Deep Learning Techniques for NER
    ‣ A Survey on Deep Learning for Named Entity Recognition") illustrates the architecture
    of extracting a contextual string embedding for word “Washington” in a sentential
    context.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，利用递归神经网络进行的语言建模进展使得将语言建模为字符分布变得可行。Akbik 等人提出的上下文字符串嵌入[[107](#bib.bib107)]使用字符级神经语言模型在句子上下文中生成字符字符串的上下文嵌入。一个重要的特性是，这些嵌入是由其周围文本上下文化的，这意味着相同的单词在不同的上下文中具有不同的嵌入。图
    [4](#S3.F4 "Figure 4 ‣ 3.2.2 Character-level Representation ‣ 3.2 Distributed
    Representations for Input ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition") 说明了如何在句子上下文中提取单词“Washington”的上下文字符串嵌入的架构。
- en: '![Refer to caption](img/627b09c3d93b9887c08207fb271d2b37.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/627b09c3d93b9887c08207fb271d2b37.png)'
- en: 'Figure 4: Extraction of a contextual string embedding for word “Washington”
    in a sentential context [[107](#bib.bib107)]. From the forward language model
    (shown in red), the model extracts the output hidden state after the last character
    in the word. From the backward language model (shown in blue), the model extracts
    the output hidden state before the first character in the word. Both output hidden
    states are concatenated to form the final embedding of a word.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在句子上下文中提取“Washington”一词的上下文字符串嵌入[[107](#bib.bib107)]。从前向语言模型（以红色显示）中，模型提取了单词最后一个字符后的输出隐藏状态。从后向语言模型（以蓝色显示）中，模型提取了单词第一个字符前的输出隐藏状态。两个输出隐藏状态被串联形成词的最终嵌入。
- en: 3.2.3 Hybrid Representation
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 混合表示
- en: Besides word-level and character-level representations, some studies also incorporate
    additional information (e.g., gazetteers [[18](#bib.bib18), [108](#bib.bib108)],
    lexical similarity [[109](#bib.bib109)], linguistic dependency [[110](#bib.bib110)]
    and visual features [[111](#bib.bib111)]) into the final representations of words,
    before feeding into context encoding layers. In other words, the DL-based representation
    is combined with feature-based approach in a hybrid manner. Adding additional
    information may lead to improvements in NER performance, with the price of hurting
    generality of these systems.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了词级和字符级表示，一些研究还将额外信息（例如，地名词典[[18](#bib.bib18), [108](#bib.bib108)]，词汇相似性[[109](#bib.bib109)]，语言依赖[[110](#bib.bib110)]和视觉特征[[111](#bib.bib111)]）融入到最终的词表示中，然后再输入到上下文编码层。换句话说，基于DL的表示与基于特征的方法以混合方式结合。添加额外信息可能会提高命名实体识别（NER）性能，但也可能影响这些系统的普适性。
- en: 'The use of neural models for NER was pioneered by [[17](#bib.bib17)], where
    an architecture based on temporal convolutional neural networks over word sequence
    was proposed. When incorporating common priori knowledge (e.g., gazetteers and
    POS), the resulting system outperforms the baseline using only word-level representations.
    In the BiLSTM-CRF model by Huang et al. [[18](#bib.bib18)], four types of features
    are used for the NER task: spelling features, context features, word embeddings,
    and gazetteer features. Their experimental results show that the extra features
    (i.e., gazetteers) boost tagging accuracy. The BiLSTM-CNN model by Chiu and Nichols [[20](#bib.bib20)]
    incorporates a bidirectional LSTM and a character-level CNN. Besides word embeddings,
    the model uses additional word-level features (capitalization, lexicons) and character-level
    features (4-dimensional vector representing the type of a character: upper case,
    lower case, punctuation, other).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 神经模型在NER中的使用由[[17](#bib.bib17)]开创，其中提出了一种基于时间卷积神经网络的词序列架构。在融合常见先验知识（例如，地名词典和词性标注）时，所得系统优于仅使用词级表示的基线模型。在黄等人提出的BiLSTM-CRF模型[[18](#bib.bib18)]中，NER任务使用了四种类型的特征：拼写特征、上下文特征、词嵌入和地名词典特征。他们的实验结果显示，额外特征（即地名词典）提高了标注准确性。Chiu和Nichols提出的BiLSTM-CNN模型[[20](#bib.bib20)]结合了双向LSTM和字符级CNN。除了词嵌入外，该模型还使用了额外的词级特征（大小写，词典）和字符级特征（4维向量表示字符类型：大写、小写、标点符号、其他）。
- en: Wei et al. [[112](#bib.bib112)] presented a CRF-based neural system for recognizing
    and normalizing disease names. This system employs rich features in addition to
    word embeddings, including words, POS tags, chunking, and word shape features
    (e.g., dictionary and morphological features). Strubell et al. [[91](#bib.bib91)]
    concatenated 100-dimensional embeddings with a 5-dimensional word shape vector
    (e.g., all capitalized, not capitalized, first-letter capitalized or contains
    a capital letter). Lin et al. [[113](#bib.bib113)] concatenated character-level
    representation, word-level representation, and syntactical word representation
    (i.e., POS tags, dependency roles, word positions, head positions) to form a comprehensive
    word representation. A multi-task approach for NER was proposed by Aguilar et
    al. [[114](#bib.bib114)]. This approach utilizes a CNN to capture orthographic
    features and word shapes at character level. For syntactical and contextual information
    at word level, e.g., POS and word embeddings, the model implements a LSTM architecture.
    Jansson and Liu [[115](#bib.bib115)] proposed to combine Latent Dirichlet Allocation
    (LDA) with deep learning on character-level and word-level embeddings.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Wei 等人 [[112](#bib.bib112)] 提出了一个基于 CRF 的神经系统，用于识别和标准化疾病名称。该系统除了词嵌入外，还采用了丰富的特征，包括词语、词性标签、分块以及词形特征（例如，词典和形态特征）。Strubell
    等人 [[91](#bib.bib91)] 将 100 维嵌入与 5 维词形向量（例如，全大写、非大写、首字母大写或包含大写字母）连接起来。Lin 等人 [[113](#bib.bib113)]
    将字符级表示、词级表示和语法词表示（即词性标签、依赖关系角色、词位置、主语位置）连接形成了全面的词表示。Aguilar 等人 [[114](#bib.bib114)]
    提出了一个用于 NER 的多任务方法。该方法利用 CNN 捕捉字符级的正字法特征和词形特征。对于词级的语法和上下文信息，例如，词性和词嵌入，模型实现了 LSTM
    架构。Jansson 和 Liu [[115](#bib.bib115)] 提出了将潜在狄利克雷分配（LDA）与深度学习结合应用于字符级和词级嵌入。
- en: Xu et al. [[116](#bib.bib116)] proposed a local detection approach for NER based
    on fixed-size ordinally forgetting encoding (FOFE) [[117](#bib.bib117)], FOFE
    explores both character-level and word-level representations for each fragment
    and its contexts. In the multi-modal NER system by Moon et al. [[118](#bib.bib118)],
    for noisy user-generated data like tweets and Snapchat captions, word embeddings,
    character embeddings, and visual features are merged with modality attention.
    Ghaddar and Langlais [[109](#bib.bib109)] found that it was unfair that lexical
    features had been mostly discarded in neural NER systems. They proposed an alternative
    lexical representation which is trained offline and can be added to any neural
    NER system. The lexical representation is computed for each word with a 120-dimensional
    vector, where each element encodes the similarity of the word with an entity type.
    Recently, Devlin et al. [[119](#bib.bib119)] proposed a new language representation
    model called BERT, bidirectional encoder representations from transformers. BERT
    uses masked language models to enable pre-trained deep bidirectional representations.
    For a given token, its input representation is comprised by summing the corresponding
    position, segment and token embeddings. Note that pre-trained language model embeddings
    often require large-scale corpora for training, and intrinsically incorporate
    auxiliary embeddings (e.g., position and segment embeddings). For this reason,
    we category these contextualized language-model embeddings as hybrid representations
    in this survey.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[116](#bib.bib116)] 提出了基于固定大小有序遗忘编码（FOFE）[[117](#bib.bib117)] 的局部检测方法，FOFE
    探索了每个片段及其上下文的字符级和词级表示。在 Moon 等人 [[118](#bib.bib118)] 的多模态 NER 系统中，对于噪声较大的用户生成数据如推文和
    Snapchat 字幕，词嵌入、字符嵌入和视觉特征通过模态注意力进行融合。Ghaddar 和 Langlais [[109](#bib.bib109)] 发现，词汇特征在神经
    NER 系统中大多被忽视是不公平的。他们提出了一种替代的词汇表示，该表示在离线训练后可以添加到任何神经 NER 系统中。词汇表示通过 120 维向量计算，每个元素编码词与实体类型的相似性。最近，Devlin
    等人 [[119](#bib.bib119)] 提出了一个新的语言表示模型，称为 BERT，即来自变换器的双向编码表示。BERT 使用掩蔽语言模型来实现预训练的深度双向表示。对于给定的标记，其输入表示由相应的位置、段落和标记嵌入的总和构成。请注意，预训练语言模型嵌入通常需要大规模的语料库进行训练，并且本质上包含辅助嵌入（例如，位置和段落嵌入）。因此，我们在本次调查中将这些上下文化的语言模型嵌入归类为混合表示。
- en: '![Refer to caption](img/c638f9b23a217795bb18a8884d193189.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c638f9b23a217795bb18a8884d193189.png)'
- en: 'Figure 5: Sentence approach network based on CNN [[17](#bib.bib17)]. The convolution
    layer extracts features from the whole sentence, treating it as a sequence with
    global structure.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 CNN 的句子处理网络 [[17](#bib.bib17)]。卷积层从整个句子中提取特征，将其视为具有全局结构的序列。
- en: 3.3 Context Encoder Architectures
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 上下文编码器架构
- en: 'Here, we now review widely-used context encoder architectures: convolutional
    neural networks, recurrent neural networks, recursive neural networks, and deep
    transformer.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们现在回顾一些广泛使用的上下文编码器架构：卷积神经网络、递归神经网络、递归神经网络和深度变换器。
- en: 3.3.1 Convolutional Neural Networks
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 卷积神经网络
- en: 'Collobert et al. [[17](#bib.bib17)] proposed a sentence approach network where
    a word is tagged with the consideration of whole sentence, shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.2.3 Hybrid Representation ‣ 3.2 Distributed Representations for
    Input ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning for Named
    Entity Recognition"). Each word in the input sequence is embedded to an $N$-dimensional
    vector after the stage of input representation. Then a convolutional layer is
    used to produce local features around each word, and the size of the output of
    the convolutional layers depends on the number of words in the sentence. The global
    feature vector is constructed by combining local feature vectors extracted by
    the convolutional layers. The dimension of the global feature vector is fixed,
    independent of the sentence length, in order to apply subsequent standard affine
    layers. Two approaches are widely used to extract global features: a max or an
    averaging operation over the position (i.e., “time” step) in the sentence. Finally,
    these fixed-size global features are fed into tag decoder to compute distribution
    scores for all possible tags for the words in the network input. Following Collobert’s
    work, Yao et al. [[94](#bib.bib94)] proposed Bio-NER for biomedical NER. Wu et
    al. [[120](#bib.bib120)] utilized a convolutional layer to generate global features
    represented by a number of global hidden nodes. Both local features and global
    features are then fed into a standard affine network to recognize named entities
    in clinical text.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Collobert 等人 [[17](#bib.bib17)] 提出了一个句子处理网络，其中单词的标记考虑了整个句子的上下文，如图 [5](#S3.F5
    "图 5 ‣ 3.2.3 混合表示 ‣ 3.2 输入的分布式表示 ‣ 3 深度学习技术用于命名实体识别 ‣ 深度学习在命名实体识别中的调查") 所示。输入序列中的每个单词在输入表示阶段后被嵌入到一个
    $N$ 维向量中。然后，使用卷积层生成围绕每个单词的局部特征，卷积层输出的大小取决于句子中的单词数量。通过结合卷积层提取的局部特征向量来构建全局特征向量。全局特征向量的维度是固定的，不依赖于句子的长度，以便应用后续的标准仿射层。提取全局特征的两种广泛使用的方法是：对句子中的位置（即“时间”步）的最大操作或平均操作。最后，这些固定大小的全局特征被输入到标签解码器中，以计算网络输入中所有可能标签的分布得分。继
    Collobert 的工作之后，Yao 等人 [[94](#bib.bib94)] 提出了用于生物医学命名实体识别的 Bio-NER。Wu 等人 [[120](#bib.bib120)]
    利用卷积层生成由若干全局隐藏节点表示的全局特征。然后，将局部特征和全局特征输入到标准仿射网络中，以识别临床文本中的命名实体。
- en: Zhou et al. [[96](#bib.bib96)] observed that with RNN latter words influence
    the final sentence representation more than former words. However, important words
    may appear anywhere in a sentence. In their proposed model, named BLSTM-RE, BLSTM
    is used to capture long-term dependencies and obtain the whole representation
    of an input sequence. CNN is then utilized to learn a high-level representation,
    which is then fed into a sigmoid classifier. Finally, the whole sentence representation
    (generated by BLSTM) and the relation presentation (generated by the sigmoid classifier)
    are fed into another LSTM to predict entities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Zhou 等人 [[96](#bib.bib96)] 观察到，在 RNN 中，后面的单词对最终句子表示的影响大于前面的单词。然而，重要的单词可能出现在句子中的任何位置。在他们提出的模型
    BLSTM-RE 中，BLSTM 用于捕捉长期依赖关系并获得输入序列的整体表示。然后使用 CNN 学习高层表示，之后将其输入到 sigmoid 分类器中。最后，将由
    BLSTM 生成的整体句子表示和由 sigmoid 分类器生成的关系表示输入到另一个 LSTM 中以预测实体。
- en: Traditionally, the time complexity of LSTMs for a sequence of length $N$ is
    $\mathcal{O(N)}$ in a parallelism manner. Strubell et al. [[91](#bib.bib91)] proposed
    ID-CNNs, referred to Iterated Dilated Convolutional Neural Networks, which is
    more computationally efficient due to the capacity of handling larger context
    and structured prediction. Figure [6](#S3.F6 "Figure 6 ‣ 3.3.1 Convolutional Neural
    Networks ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition") shows the architecture
    of a dilated CNN block, where four stacked dilated convolutions of width 3 produce
    token representations. Experimental results show that ID-CNNs achieves 14-20x
    test-time speedups compared to Bi-LSTM-CRF while retaining comparable accuracy.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，LSTM在长度为$N$的序列上的时间复杂度为$\mathcal{O(N)}$，以并行的方式进行计算。Strubell等人[[91](#bib.bib91)]提出了ID-CNNs，即迭代膨胀卷积神经网络，这种方法在处理更大上下文和结构化预测的能力上更加高效。图[6](#S3.F6
    "图 6 ‣ 3.3.1 卷积神经网络 ‣ 3.3 上下文编码器架构 ‣ 3 命名实体识别的深度学习技术 ‣ 关于命名实体识别的深度学习综述")展示了一个膨胀CNN块的架构，其中四个堆叠的宽度为3的膨胀卷积生成了令牌表示。实验结果表明，相比于Bi-LSTM-CRF，ID-CNNs在测试时间上获得了14-20倍的加速，同时保持了类似的准确性。
- en: '![Refer to caption](img/0a597071eea04655c5f2254f1227e733.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/0a597071eea04655c5f2254f1227e733.png)'
- en: 'Figure 6: The architecture of ID-CNNs with filter width 3 and maximum dilation
    width 4. [[91](#bib.bib91)].'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：ID-CNNs的架构，滤波器宽度为3，最大膨胀宽度为4[[91](#bib.bib91)]。
- en: 3.3.2 Recurrent Neural Networks
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 循环神经网络
- en: Recurrent neural networks, together with its variants such as gated recurrent
    unit (GRU) and long-short term memory (LSTM), have demonstrated remarkable achievements
    in modeling sequential data. In particular, bidirectional RNNs efficiently make
    use of past information (via forward states) and future information (via backward
    states) for a specific time frame [[18](#bib.bib18)]. Thus, a token encoded by
    a bidirectional RNN will contain evidence from the whole input sentence. Bidirectional
    RNNs therefore become de facto standard for composing deep context-dependent representations
    of text [[91](#bib.bib91), [97](#bib.bib97)]. A typical architecture of RNN-based
    context encoder is shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.3.2 Recurrent Neural
    Networks ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition").
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络及其变体，如门控循环单元（GRU）和长短期记忆（LSTM），在建模序列数据方面表现出了显著的成就。特别是，双向RNN有效利用了特定时间框架中的过去信息（通过前向状态）和未来信息（通过后向状态）[[18](#bib.bib18)]。因此，由双向RNN编码的令牌将包含来自整个输入句子的证据。因此，双向RNN成为了构建深度上下文依赖文本表示的事实标准[[91](#bib.bib91),
    [97](#bib.bib97)]。基于RNN的上下文编码器的典型架构如图[7](#S3.F7 "图 7 ‣ 3.3.2 循环神经网络 ‣ 3.3 上下文编码器架构
    ‣ 3 命名实体识别的深度学习技术 ‣ 关于命名实体识别的深度学习综述")所示。
- en: '![Refer to caption](img/2a7aa0da7bc5b207f3ec1b43f6419794.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/2a7aa0da7bc5b207f3ec1b43f6419794.png)'
- en: 'Figure 7: The architecture of RNN-based context encoder.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：基于RNN的上下文编码器架构。
- en: The work by Huang et al. [[18](#bib.bib18)] is among the first to utilize a
    bidirectional LSTM CRF architecture to sequence tagging tasks (POS, chunking and
    NER). Following [[18](#bib.bib18)], a body of works [[97](#bib.bib97), [20](#bib.bib20),
    [105](#bib.bib105), [112](#bib.bib112), [19](#bib.bib19), [89](#bib.bib89), [95](#bib.bib95),
    [90](#bib.bib90), [101](#bib.bib101), [96](#bib.bib96), [113](#bib.bib113)] applied
    BiLSTM as the basic architecture to encode sequence context information. Yang
    et al. [[106](#bib.bib106)] employed deep GRUs on both character and word levels
    to encode morphology and context information. They further extended their model
    to cross-lingual and multi-task joint trained by sharing the architecture and
    parameters.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Huang等人[[18](#bib.bib18)]的工作是最早利用双向LSTM CRF架构进行序列标注任务（POS、分块和NER）的研究之一。继[[18](#bib.bib18)]之后，许多研究[[97](#bib.bib97),
    [20](#bib.bib20), [105](#bib.bib105), [112](#bib.bib112), [19](#bib.bib19), [89](#bib.bib89),
    [95](#bib.bib95), [90](#bib.bib90), [101](#bib.bib101), [96](#bib.bib96), [113](#bib.bib113)]采用BiLSTM作为基础架构来编码序列上下文信息。Yang等人[[106](#bib.bib106)]在字符和词汇层面上使用深度GRUs来编码形态和上下文信息。他们进一步扩展了他们的模型，通过共享架构和参数，扩展到跨语言和多任务联合训练。
- en: Gregoric et al. [[121](#bib.bib121)] employed multiple independent bidirectional
    LSTM units across the same input. Their model promotes diversity among the LSTM
    units by employing an inter-model regularization term. By distributing computation
    across multiple smaller LSTMs, they found a reduction in total number of parameters.
    Recently, some studies [[122](#bib.bib122), [123](#bib.bib123)] designed LSTM-based
    neural networks for nested named entity recognition. Katiyar and Cardie [[122](#bib.bib122)]
    presented a modification to standard LSTM-based sequence labeling model to handle
    nested named entity recognition. Ju et al. [[123](#bib.bib123)] proposed a neural
    model to identify nested entities by dynamically stacking flat NER layers until
    no outer entities are extracted. Each flat NER layer employs bidirectional LSTM
    to capture sequential context. The model merges the outputs of the LSTM layer
    in the current flat NER layer to construct new representations for the detected
    entities and then feeds them into the next flat NER layer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Gregoric 等人[[121](#bib.bib121)] 在相同输入上使用了多个独立的双向 LSTM 单元。他们的模型通过引入跨模型正则化项来促进
    LSTM 单元之间的多样性。通过将计算分配到多个较小的 LSTM 中，他们发现参数总数有所减少。最近，一些研究[[122](#bib.bib122), [123](#bib.bib123)]
    设计了基于 LSTM 的神经网络用于嵌套命名实体识别。Katiyar 和 Cardie [[122](#bib.bib122)] 对标准的基于 LSTM 的序列标注模型进行了修改，以处理嵌套命名实体识别。Ju
    等人 [[123](#bib.bib123)] 提出了一个神经模型，通过动态堆叠平面 NER 层来识别嵌套实体，直到不再提取外部实体。每个平面 NER 层使用双向
    LSTM 捕捉序列上下文。该模型将当前平面 NER 层的 LSTM 层的输出合并，以构造检测到的实体的新表示，然后将其输入到下一个平面 NER 层。
- en: 3.3.3 Recursive Neural Networks
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 递归神经网络
- en: '![Refer to caption](img/86f4e6818f253d5bffefc2206a7e44a1.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86f4e6818f253d5bffefc2206a7e44a1.png)'
- en: 'Figure 8: Bidirectional recursive neural networks for NER [[98](#bib.bib98)].
    The computations are done recursively in two directions. The bottom-up direction
    computes the semantic composition of the subtree of each node, and the top-down
    counterpart propagates to that node the linguistic structures which contain the
    subtree.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：用于 NER 的双向递归神经网络[[98](#bib.bib98)]。计算在两个方向上递归进行。自下而上的方向计算每个节点子树的语义组合，而自上而下的对应部分则向该节点传播包含子树的语言结构。
- en: Recursive neural networks are non-linear adaptive models that are able to learn
    deep structured information, by traversing a given structure in topological order.
    Named entities are highly related to linguistic constituents, e.g., noun phrases [[98](#bib.bib98)].
    However, typical sequential labeling approaches take little into consideration
    about phrase structures of sentences. To this end, Li et al. [[98](#bib.bib98)]
    proposed to classify every node in a constituency structure for NER. This model
    recursively calculates hidden state vectors of every node and classifies each
    node by these hidden vectors. Figure [8](#S3.F8 "Figure 8 ‣ 3.3.3 Recursive Neural
    Networks ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition") shows how to recursively
    compute two hidden state features for every node. The bottom-up direction calculates
    the semantic composition of the subtree of each node, and the top-down counterpart
    propagates to that node the linguistic structures which contain the subtree. Given
    hidden vectors for every node, the network calculates a probability distribution
    of entity types plus a special non-entity type.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络是非线性自适应模型，能够通过按拓扑顺序遍历给定结构来学习深层次的结构信息。命名实体与语言成分高度相关，例如名词短语[[98](#bib.bib98)]。然而，典型的序列标注方法对句子的短语结构考虑较少。为此，Li
    等人 [[98](#bib.bib98)] 提出了对句法结构中的每个节点进行分类的方法。这种模型递归地计算每个节点的隐藏状态向量，并根据这些隐藏向量对每个节点进行分类。图
    [8](#S3.F8 "图 8 ‣ 3.3.3 递归神经网络 ‣ 3.3 上下文编码器架构 ‣ 3 用于命名实体识别的深度学习技术 ‣ 命名实体识别的深度学习综述")
    显示了如何递归地计算每个节点的两个隐藏状态特征。自下而上的方向计算每个节点子树的语义组合，而自上而下的对应部分则向该节点传播包含子树的语言结构。给定每个节点的隐藏向量，网络计算实体类型的概率分布加上一个特殊的非实体类型。
- en: '![Refer to caption](img/8a056b31c8635a5fddd86f631cf1655f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a056b31c8635a5fddd86f631cf1655f.png)'
- en: 'Figure 9: A sequence labeling model with an additional language modeling objective [[124](#bib.bib124)],
    performing NER on the sentence “Fischler proposes measures”. At each token position
    (e.g., “proposes”), the network is optimised to predict the previous word (“Fischler”),
    the current label (“O”), and the next word (“measures”) in the sequence.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 一个附加语言建模目标的序列标注模型 [[124](#bib.bib124)]，对句子“Fischler proposes measures”进行NER。在每个标记位置（例如，“proposes”），网络被优化为预测前一个词（“Fischler”）、当前标签（“O”）和序列中的下一个词（“measures”）。'
- en: 3.3.4 Neural Language Models
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 神经语言模型
- en: 'Language model is a family of models describing the generation of sequences.
    Given a token sequence, $(t_{1},t_{2},\ldots,t_{N})$, a forward language model
    computes the probability of the sequence by modeling the probability of token
    $t_{k}$ given its history $(t_{1},\ldots,t_{k-1})$ [[21](#bib.bib21)]:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是一类描述序列生成的模型。给定一个标记序列 $(t_{1},t_{2},\ldots,t_{N})$，前向语言模型通过建模标记 $t_{k}$
    在其历史 $(t_{1},\ldots,t_{k-1})$ 的条件下的概率来计算序列的概率 [[21](#bib.bib21)]。
- en: '|  | $p(t_{1},t_{2},\ldots,t_{N})=\prod\limits_{k=1}^{N}{p(t_{k}&#124;t_{1},t_{2},\ldots,t_{k-1})}$
    |  | (1) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(t_{1},t_{2},\ldots,t_{N})=\prod\limits_{k=1}^{N}{p(t_{k}&#124;t_{1},t_{2},\ldots,t_{k-1})}$
    |  | (1) |'
- en: 'A backward language model is similar to a forward language model, except it
    runs over the sequence in reverse order, predicting the previous token given its
    future context:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 反向语言模型类似于前向语言模型，只是它以相反的顺序运行序列，预测给定其未来上下文的前一个标记：
- en: '|  | $p(t_{1},t_{2},\ldots,t_{N})=\prod\limits_{k=1}^{N}{p({t_{k}}&#124;{t_{k+1},t_{k+2},...,t_{N}})}$
    |  | (2) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(t_{1},t_{2},\ldots,t_{N})=\prod\limits_{k=1}^{N}{p({t_{k}}&#124;{t_{k+1},t_{k+2},...,t_{N}})}$
    |  | (2) |'
- en: For neural language models, probability of token $t_{k}$ can be computed by
    the output of recurrent neural networks. At each position $k$, we can obtain two
    context-dependent representations (forward and backward) and then combine them
    as the final language model embedding for token $t_{k}$. Such language-model-augmented
    knowledge has been empirically verified to be helpful in numerous sequence labeling
    tasks [[124](#bib.bib124), [21](#bib.bib21), [125](#bib.bib125), [126](#bib.bib126),
    [103](#bib.bib103), [127](#bib.bib127)].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经语言模型，标记 $t_{k}$ 的概率可以通过递归神经网络的输出计算。在每个位置 $k$，我们可以获得两个依赖上下文的表示（前向和后向），然后将它们结合起来作为标记
    $t_{k}$ 的最终语言模型嵌入。这样的语言模型增强知识在许多序列标注任务中已被经验验证为有帮助的 [[124](#bib.bib124), [21](#bib.bib21),
    [125](#bib.bib125), [126](#bib.bib126), [103](#bib.bib103), [127](#bib.bib127)]。
- en: Rei [[124](#bib.bib124)] proposed a framework with a secondary objective – learning
    to predict surrounding words for each word in the dataset. Figure [9](#S3.F9 "Figure
    9 ‣ 3.3.3 Recursive Neural Networks ‣ 3.3 Context Encoder Architectures ‣ 3 Deep
    Learning Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition")
    illustrates the architecture with a short sentence on the NER task. At each time
    step (i.e., token position), the network is optimised to predict the previous
    token, the current tag, and the next token in the sequence. The added language
    modeling objective encourages the system to learn richer feature representations
    which are then reused for sequence labeling.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Rei [[124](#bib.bib124)] 提出了一个具有次要目标的框架——学习预测数据集中每个单词周围的单词。图 [9](#S3.F9 "Figure
    9 ‣ 3.3.3 Recursive Neural Networks ‣ 3.3 Context Encoder Architectures ‣ 3 Deep
    Learning Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition")
    展示了一个在命名实体识别（NER）任务中使用的短句的架构。在每个时间步（即，标记位置），网络被优化为预测序列中的前一个标记、当前标签和下一个标记。增加的语言建模目标鼓励系统学习更丰富的特征表示，然后将这些表示用于序列标注。
- en: Peters et al. [[21](#bib.bib21)] proposed TagLM, a language model augmented
    sequence tagger. This tagger considers both pre-trained word embeddings and bidirectional
    language model embeddings for every token in the input sequence for sequence labeling
    task. Figure [10](#S3.F10 "Figure 10 ‣ 3.3.4 Neural Language Models ‣ 3.3 Context
    Encoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition") shows the architecture of LM-LSTM-CRF
    model  [[126](#bib.bib126), [125](#bib.bib125)]. The language model and sequence
    tagging model share the same character-level layer in a multi-task learning manner.
    The vectors from character-level embeddings, pre-trained word embeddings, and
    language model representations, are concatenated and fed into the word-level LSTMs.
    Experimental results demonstrate that multi-task learning is an effective approach
    to guide the language model to learn task-specific knowledge.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Peters 等人 [[21](#bib.bib21)] 提出了 TagLM，一种增强语言模型的序列标注器。该标注器考虑了每个输入序列中的预训练词嵌入和双向语言模型嵌入用于序列标注任务。图 [10](#S3.F10
    "图 10 ‣ 3.3.4 神经语言模型 ‣ 3.3 上下文编码器架构 ‣ 3 NER 的深度学习技术 ‣ 深度学习命名实体识别综述") 显示了 LM-LSTM-CRF
    模型的架构 [[126](#bib.bib126), [125](#bib.bib125)]。语言模型和序列标注模型在多任务学习模式下共享相同的字符级层。字符级嵌入、预训练词嵌入和语言模型表示的向量被拼接并输入到词级
    LSTM 中。实验结果表明，多任务学习是一种有效的方法，可以指导语言模型学习任务特定的知识。
- en: '![Refer to caption](img/89e0ae4676fb2213876ee369a48f266e.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89e0ae4676fb2213876ee369a48f266e.png)'
- en: 'Figure 10: Sequence labeling architecture with contextualized representations [[125](#bib.bib125)].
    Character-level representation, pre-trained word embedding and contextualized
    representation from bidirectional language models are concatenated and further
    fed into context encoder.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 带有上下文表示的序列标注架构 [[125](#bib.bib125)]。字符级表示、预训练的词嵌入和来自双向语言模型的上下文表示被拼接并进一步输入到上下文编码器中。'
- en: '![Refer to caption](img/56dbad717f7b5e922feaa92bab983d07.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/56dbad717f7b5e922feaa92bab983d07.png)'
- en: (a) Google BERT
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Google BERT
- en: '![Refer to caption](img/18d61570dcbdb35f699f88e2047c70bc.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18d61570dcbdb35f699f88e2047c70bc.png)'
- en: (b) OpenAI GPT
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: (b) OpenAI GPT
- en: '![Refer to caption](img/3243b8f2bae9aa590239795214331ebb.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3243b8f2bae9aa590239795214331ebb.png)'
- en: (c) AllenNLP ELMo
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: (c) AllenNLP ELMo
- en: 'Figure 11: Differences in pre-training model architectures [[119](#bib.bib119)].
    Google BERT uses a bidirectional Transformer (abbreviated as “Trm”). OpenAI GPT
    uses a left-to-right Transformer. AllenNLP ELMo uses the concatenation of independently
    trained left-to-right and right-to-left LSTM to generate features for downstream
    tasks.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 预训练模型架构的差异 [[119](#bib.bib119)]。Google BERT 使用双向 Transformer（缩写为“Trm”）。OpenAI
    GPT 使用从左到右的 Transformer。AllenNLP ELMo 使用独立训练的从左到右和从右到左的 LSTM 的拼接来生成下游任务的特征。'
- en: Figure [4](#S3.F4 "Figure 4 ‣ 3.2.2 Character-level Representation ‣ 3.2 Distributed
    Representations for Input ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition") shows the contextual string embedding
    using neural character-level language modeling by Akbik et al. [[107](#bib.bib107)].
    They utilized the hidden states of a forward-backward recurrent neural network
    to create contextualized word embeddings. A major merit of this model is that
    character-level language model is independent of tokenization and a fixed vocabulary.
    Peters et al. [[103](#bib.bib103)] proposed ELMo representations, which are computed
    on top of two-layer bidirectional language models with character convolutions.
    This new type of deep contextualized word representation is capable of modeling
    both complex characteristics of word usage (e.g., semantics and syntax), and usage
    variations across linguistic contexts (e.g., polysemy).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S3.F4 "图 4 ‣ 3.2.2 字符级表示 ‣ 3.2 输入的分布式表示 ‣ 3 NER 的深度学习技术 ‣ 深度学习命名实体识别综述")
    显示了 Akbik 等人 [[107](#bib.bib107)] 使用神经字符级语言建模的上下文字符串嵌入。他们利用前向-后向递归神经网络的隐藏状态来创建上下文词嵌入。该模型的一个主要优点是字符级语言模型不依赖于分词和固定词汇表。Peters
    等人 [[103](#bib.bib103)] 提出了 ELMo 表示，这些表示是在具有字符卷积的双层双向语言模型之上计算的。这种新型深度上下文化的词表示能够建模词汇使用的复杂特征（例如，语义和句法）以及不同语言上下文中的使用变化（例如，多义词）。
- en: 3.3.5 Deep Transformer
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 深度 Transformer
- en: Neural sequence labeling models are typically based on complex convolutional
    or recurrent networks which consists of encoders and decoders. Transformer, proposed
    by Vaswani et al. [[128](#bib.bib128)], dispenses with recurrence and convolutions
    entirely. Transformer utilizes stacked self-attention and point-wise, fully connected
    layers to build basic blocks for encoder and decoder. Experiments on various tasks [[128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130)] show Transformers to be superior in quality
    while requiring significantly less time to train.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 神经序列标注模型通常基于复杂的卷积或递归网络，这些网络由编码器和解码器组成。Transformer 由 Vaswani 等人 [[128](#bib.bib128)]
    提出，完全摒弃了递归和卷积。Transformer 利用堆叠的自注意力和逐点全连接层来构建编码器和解码器的基本块。在各种任务上的实验 [[128](#bib.bib128)、[129](#bib.bib129)、[130](#bib.bib130)]
    显示，Transformer 在质量上表现优越，同时训练所需时间显著减少。
- en: Based on transformer, Radford et al. [[131](#bib.bib131)] proposed Generative
    Pre-trained Transformer (GPT) for language understanding tasks. GPT has a two-stage
    training procedure. First, they use a language modeling objective with Transformers
    on unlabeled data to learn the initial parameters. Then they adapt these parameters
    to a target task using the supervised objective, resulting in minimal changes
    to the pre-trained model. Unlike GPT (a left-to-right architecture), Bidirectional
    Encoder Representations from Transformers (BERT) is proposed to pre-train deep
    bidirectional Transformer by jointly conditioning on both left and right context
    in all layers [[119](#bib.bib119)]. Figure [11](#S3.F11 "Figure 11 ‣ 3.3.4 Neural
    Language Models ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques
    for NER ‣ A Survey on Deep Learning for Named Entity Recognition") summarizes
    BERT [[119](#bib.bib119)], GPT [[131](#bib.bib131)] and ELMo [[103](#bib.bib103)].
    In addition, Baevski et al. [[132](#bib.bib132)] proposed a novel cloze-driven
    pre-training regime based on a bi-directional Transformer, which is trained with
    a cloze-style objective and predicts the center word given all left and right
    context.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer，Radford 等人 [[131](#bib.bib131)] 提出了用于语言理解任务的生成预训练 Transformer（GPT）。GPT
    具有两个阶段的训练过程。首先，他们使用 Transformer 对未标记数据进行语言建模目标以学习初始参数。然后，他们将这些参数调整到目标任务上，使用监督目标，结果是对预训练模型的更改最小。与
    GPT（左到右结构）不同，双向编码器表示从 Transformer（BERT）被提出以通过在所有层中共同考虑左侧和右侧上下文来预训练深度双向 Transformer
    [[119](#bib.bib119)]。图 [11](#S3.F11 "Figure 11 ‣ 3.3.4 Neural Language Models
    ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey
    on Deep Learning for Named Entity Recognition") 总结了 BERT [[119](#bib.bib119)]、GPT
    [[131](#bib.bib131)] 和 ELMo [[103](#bib.bib103)]。此外，Baevski 等人 [[132](#bib.bib132)]
    提出了基于双向 Transformer 的新型 cloze 驱动预训练机制，该机制通过 cloze 风格的目标进行训练，并在给定所有左侧和右侧上下文的情况下预测中心词。
- en: These language model embeddings pre-trained using Transformer are becoming a
    new paradigm of NER. First, these embeddings are contextualized and can be used
    to replace the traditional embeddings, such as Google Word2vec and Stanford GloVe.
    Some studies [[110](#bib.bib110), [108](#bib.bib108), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135), [136](#bib.bib136)] have achieved promising performance via
    leveraging the combination of traditional embeddings and language model embeddings.
    Second, these language model embeddings can be further fine-tuned with one additional
    output layer for a wide range of tasks including NER and chunking. Especially,
    Li et al. [[137](#bib.bib137), [138](#bib.bib138)] framed the NER task as a machine
    reading comprehension (MRC) problem, which can be solved by fine-tuning the BERT
    model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些使用 Transformer 预训练的语言模型嵌入正在成为命名实体识别（NER）的新范式。首先，这些嵌入是上下文化的，可以用来替代传统的嵌入，例如
    Google Word2vec 和 Stanford GloVe。一些研究 [[110](#bib.bib110)、[108](#bib.bib108)、[133](#bib.bib133)、[134](#bib.bib134)、[135](#bib.bib135)、[136](#bib.bib136)]
    通过结合传统嵌入和语言模型嵌入取得了令人满意的效果。其次，这些语言模型嵌入可以通过增加一个输出层进一步微调，以适用于包括 NER 和分块在内的广泛任务。特别是，Li
    等人 [[137](#bib.bib137)、[138](#bib.bib138)] 将 NER 任务构建为机器阅读理解（MRC）问题，可以通过微调 BERT
    模型来解决。
- en: 3.4 Tag Decoder Architectures
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 标签解码器架构
- en: 'Tag decoder is the final stage in a NER model. It takes context-dependent representations
    as input and produce a sequence of tags corresponding to the input sequence. Figure [12](#S3.F12
    "Figure 12 ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER
    ‣ A Survey on Deep Learning for Named Entity Recognition") summarizes four architectures
    of tag decoders: MLP + softmax layer, conditional random fields (CRFs), recurrent
    neural networks, and pointer networks.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 标签解码器是 NER 模型的最终阶段。它将上下文依赖的表示作为输入，并产生与输入序列相对应的标签序列。图[12](#S3.F12 "Figure 12
    ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey
    on Deep Learning for Named Entity Recognition") 概述了四种标签解码器架构：MLP + softmax 层、条件随机场（CRFs）、递归神经网络和指针网络。
- en: '![Refer to caption](img/bc95cf792aca22bb5bfadc0c373cb6f7.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/bc95cf792aca22bb5bfadc0c373cb6f7.png)'
- en: (a) MLP+Softmax
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MLP+Softmax
- en: '![Refer to caption](img/0fb9b328a2d097df4def5182e4c04957.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0fb9b328a2d097df4def5182e4c04957.png)'
- en: (b) CRF
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CRF
- en: '![Refer to caption](img/4b56b7617c19cd67c81bf631260f1093.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4b56b7617c19cd67c81bf631260f1093.png)'
- en: (c) RNN
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (c) RNN
- en: '![Refer to caption](img/ef953a46224e45cd84893c6af79f1da8.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ef953a46224e45cd84893c6af79f1da8.png)'
- en: (d) Pointer Network
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Pointer Network
- en: 'Figure 12: Differences in four tag decoders: MLP+Softmax, CRF, RNN, and Pointer
    network.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：四种标签解码器的差异：MLP+Softmax、CRF、RNN 和 Pointer 网络。
- en: 3.4.1 Multi-Layer Perceptron + Softmax
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 多层感知器 + Softmax
- en: NER is in general formulated as a sequence labeling problem. With a multi-layer
    Perceptron + Softmax layer as the tag decoder layer, the sequence labeling task
    is cast as a multi-class classification problem. Tag for each word is independently
    predicted based on the context-dependent representations without taking into account
    its neighbors.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）通常被表述为序列标注问题。使用多层感知器 + Softmax 层作为标签解码器层，序列标注任务被转化为多类分类问题。每个词的标签是基于上下文依赖的表示独立预测的，而不考虑其邻近词。
- en: A number of NER models [[116](#bib.bib116), [98](#bib.bib98), [91](#bib.bib91),
    [119](#bib.bib119), [139](#bib.bib139)] that have been introduced earlier use
    MLP + Softmax as the tag decoder. As a domain-specific NER task, Tomori et al. [[140](#bib.bib140)]
    used softmax as tag decoder to predict game states in Japanese chess game. Their
    model takes both input from text and input from chess board ($9\times 9$ squares
    with 40 pieces of 14 different types) and predict 21 named entities specific to
    this game. Text representations and game state embeddings are both fed to a softmax
    layer for prediction of named entities using BIO tag scheme.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 许多早期介绍的 NER 模型[[116](#bib.bib116), [98](#bib.bib98), [91](#bib.bib91), [119](#bib.bib119),
    [139](#bib.bib139)] 使用 MLP + Softmax 作为标签解码器。作为领域特定的 NER 任务，Tomori 等人[[140](#bib.bib140)]
    使用 softmax 作为标签解码器来预测日本象棋游戏中的游戏状态。他们的模型同时接受来自文本和棋盘的输入（$9\times 9$ 格子，含 40 个不同类型的
    14 个棋子），并预测 21 个特定于此游戏的命名实体。文本表示和游戏状态嵌入都被送入 softmax 层以使用 BIO 标签方案进行命名实体预测。
- en: 3.4.2 Conditional Random Fields
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 条件随机场
- en: A conditional random field (CRF) is a random field globally conditioned on the
    observation sequence [[73](#bib.bib73)]. CRFs have been widely used in feature-based
    supervised learning approaches (see Section [2.4.3](#S2.SS4.SSS3 "2.4.3 Feature-based
    Supervised Learning Approaches ‣ 2.4 Traditional Approaches to NER ‣ 2 Background
    ‣ A Survey on Deep Learning for Named Entity Recognition")). Many deep learning
    based NER models use a CRF layer as the tag decoder, e.g., on top of an bidirectional
    LSTM layer [[90](#bib.bib90), [103](#bib.bib103), [18](#bib.bib18), [141](#bib.bib141)],
    and on top of a CNN layer [[94](#bib.bib94), [17](#bib.bib17), [91](#bib.bib91)].
    Listed in Table [III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4
    Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition"), CRF is the most common choice for tag
    decoder, and the state-of-the-art performance on CoNLL03 and OntoNotes5.0 is achieved
    by [[107](#bib.bib107)] with a CRF tag decoder.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 条件随机场（CRF）是一个全球条件在观测序列上的随机场[[73](#bib.bib73)]。CRF 在基于特征的监督学习方法中被广泛使用（参见第[2.4.3节](#S2.SS4.SSS3
    "2.4.3 Feature-based Supervised Learning Approaches ‣ 2.4 Traditional Approaches
    to NER ‣ 2 Background ‣ A Survey on Deep Learning for Named Entity Recognition")）。许多基于深度学习的
    NER 模型使用 CRF 层作为标签解码器，例如，位于双向 LSTM 层[[90](#bib.bib90), [103](#bib.bib103), [18](#bib.bib18),
    [141](#bib.bib141)] 之上，以及位于 CNN 层[[94](#bib.bib94), [17](#bib.bib17), [91](#bib.bib91)]
    之上。表[III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition") 中列出的 CRF 是最常见的标签解码器选择，并且在 CoNLL03 和 OntoNotes5.0
    上的最先进性能由[[107](#bib.bib107)] 的 CRF 标签解码器实现。
- en: CRFs, however, cannot make full use of segment-level information because the
    inner properties of segments cannot be fully encoded with word-level representations.
    Zhuo et al. [[142](#bib.bib142)] then proposed gated recursive semi-markov CRFs,
    which directly model segments instead of words, and automatically extract segment-level
    features through a gated recursive convolutional neural network. Recently, Ye
    and Ling [[143](#bib.bib143)] proposed hybrid semi-Markov CRFs for neural sequence
    labeling. This approach adopts segments instead of words as the basic units for
    feature extraction and transition modeling. Word-level labels are utilized in
    deriving segment scores. Thus, this approach is able to leverage both word- and
    segment-level information for segment score calculation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CRFs无法充分利用段级信息，因为段的内部属性无法通过词级表示完全编码。Zhuo等人[[142](#bib.bib142)]随后提出了门控递归半马尔可夫CRFs，它们直接对段进行建模而不是单词，并通过门控递归卷积神经网络自动提取段级特征。最近，Ye和Ling[[143](#bib.bib143)]提出了用于神经序列标注的混合半马尔可夫CRFs。这种方法采用段而非单词作为特征提取和转移建模的基本单位。单词级标签用于推导段分数。因此，这种方法能够利用单词级和段级信息进行段分数计算。
- en: 'TABLE III: Summary of recent works on neural NER. LSTM: long short-term memory,
    CNN: convolutional neural network, GRU: gated recurrent unit, LM: language model,
    ID-CNN: iterated dilated convolutional neural network, BRNN: bidirectional recursive
    neural network, MLP: multi-layer perceptron, CRF: conditional random field, Semi-CRF:
    Semi-markov conditional random field, FOFE: fixed-size ordinally forgetting encoding.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：神经NER的最新工作总结。LSTM：长短期记忆，CNN：卷积神经网络，GRU：门控递归单元，LM：语言模型，ID-CNN：迭代膨胀卷积神经网络，BRNN：双向递归神经网络，MLP：多层感知机，CRF：条件随机场，Semi-CRF：半马尔可夫条件随机场，FOFE：固定大小有序遗忘编码。
- en: '| Work | Input representation | Context encoder | Tag decoder | Performance
    (F-score) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 输入表示 | 上下文编码器 | 标签解码器 | 性能（F值） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Character | Word | Hybrid |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 字符 | 单词 | 混合 |'
- en: '| --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [[94](#bib.bib94)] | - | Trained on PubMed | POS | CNN | CRF | GENIA: 71.01%
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | - | 在PubMed上训练 | 词性标注 | CNN | CRF | GENIA: 71.01% |'
- en: '| [[89](#bib.bib89)] | - | Trained on Gigaword | - | GRU | GRU | ACE 2005:
    80.00% |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] | - | 在Gigaword上训练 | - | GRU | GRU | ACE 2005: 80.00%
    |'
- en: '| [[95](#bib.bib95)] | - | Random | - | LSTM | Pointer Network | ATIS: 96.86%
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | - | 随机 | - | LSTM | Pointer Network | ATIS: 96.86% |'
- en: '| [[90](#bib.bib90)] | - | Trained on NYT | - | LSTM | LSTM | NYT: 49.50% |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] | - | 在NYT上训练 | - | LSTM | LSTM | NYT: 49.50% |'
- en: '| [[91](#bib.bib91)] | - | SENNA | Word shape | ID-CNN | CRF | CoNLL03: 90.65%;
    OntoNotes5.0: 86.84% |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] | - | SENNA | 单词形状 | ID-CNN | CRF | CoNLL03: 90.65%; OntoNotes5.0:
    86.84% |'
- en: '| [[96](#bib.bib96)] | - | Google word2vec | - | LSTM | LSTM | CoNLL04: 75.0%
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | - | Google word2vec | - | LSTM | LSTM | CoNLL04: 75.0%
    |'
- en: '| [[100](#bib.bib100)] | LSTM | - | - | LSTM | CRF | CoNLL03: 84.52% |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] | LSTM | - | - | LSTM | CRF | CoNLL03: 84.52% |'
- en: '| [[97](#bib.bib97)] | CNN | GloVe | - | LSTM | CRF | CoNLL03: 91.21% |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [[97](#bib.bib97)] | CNN | GloVe | - | LSTM | CRF | CoNLL03: 91.21% |'
- en: '| [[105](#bib.bib105)] | LSTM | Google word2vec | - | LSTM | CRF | CoNLL03:
    84.09% |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [[105](#bib.bib105)] | LSTM | Google word2vec | - | LSTM | CRF | CoNLL03:
    84.09% |'
- en: '| [[19](#bib.bib19)] | LSTM | SENNA | - | LSTM | CRF | CoNLL03: 90.94% |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| [[19](#bib.bib19)] | LSTM | SENNA | - | LSTM | CRF | CoNLL03: 90.94% |'
- en: '| [[106](#bib.bib106)] | GRU | SENNA | - | GRU | CRF | CoNLL03: 90.94% |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| [[106](#bib.bib106)] | GRU | SENNA | - | GRU | CRF | CoNLL03: 90.94% |'
- en: '| [[98](#bib.bib98)] | CNN | GloVe | POS | BRNN | Softmax | OntoNotes5.0: 87.21%
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [[98](#bib.bib98)] | CNN | GloVe | 词性标注 | BRNN | Softmax | OntoNotes5.0:
    87.21% |'
- en: '| [[107](#bib.bib107)] | LSTM-LM | - | - | LSTM | CRF | CoNLL03: 93.09%; OntoNotes5.0:
    89.71% |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [[107](#bib.bib107)] | LSTM-LM | - | - | LSTM | CRF | CoNLL03: 93.09%; OntoNotes5.0:
    89.71% |'
- en: '| [[103](#bib.bib103)] | CNN-LSTM-LM | - | - | LSTM | CRF | CoNLL03: 92.22%
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | CNN-LSTM-LM | - | - | LSTM | CRF | CoNLL03: 92.22%
    |'
- en: '| [[17](#bib.bib17)] | - | Random | POS | CNN | CRF | CoNLL03: 89.86% |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| [[17](#bib.bib17)] | - | 随机 | 词性标注 | CNN | CRF | CoNLL03: 89.86% |'
- en: '| [[18](#bib.bib18)] | - | SENNA | Spelling, n-gram, gazetteer | LSTM | CRF
    | CoNLL03: 90.10% |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| [[18](#bib.bib18)] | - | SENNA | 拼写，n-gram，地名词典 | LSTM | CRF | CoNLL03: 90.10%
    |'
- en: '| [[20](#bib.bib20)] | CNN | SENNA | capitalization, lexicons | LSTM | CRF
    | CoNLL03: 91.62%; OntoNotes5.0: 86.34% |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| [[20](#bib.bib20)] | CNN | SENNA | 大小写，词典 | LSTM | CRF | CoNLL03: 91.62%;
    OntoNotes5.0: 86.34% |'
- en: '| [[116](#bib.bib116)] | - | - | FOFE | MLP | CRF | CoNLL03: 91.17% |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | - | - | FOFE | MLP | CRF | CoNLL03: 91.17% |'
- en: '| [[101](#bib.bib101)] | LSTM | GloVe | - | LSTM | CRF | CoNLL03: 91.07% |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] | LSTM | GloVe | - | LSTM | CRF | CoNLL03: 91.07% |'
- en: '| [[113](#bib.bib113)] | LSTM | GloVe | Syntactic | LSTM | CRF | W-NUT17: 40.42%
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | LSTM | GloVe | 句法 | LSTM | CRF | W-NUT17: 40.42% |'
- en: '| [[102](#bib.bib102)] | CNN | SENNA | - | LSTM | Reranker | CoNLL03: 91.62%
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bib102)] | CNN | SENNA | - | LSTM | 重新排序器 | CoNLL03: 91.62% |'
- en: '| [[114](#bib.bib114)] | CNN | Twitter Word2vec | POS | LSTM | CRF | W-NUT17:
    41.86% |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bib114)] | CNN | Twitter Word2vec | 词性标注 | LSTM | CRF | W-NUT17:
    41.86% |'
- en: '| [[115](#bib.bib115)] | LSTM | GloVe | POS, topics | LSTM | CRF | W-NUT17:
    41.81% |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | LSTM | GloVe | 词性标注, 主题 | LSTM | CRF | W-NUT17: 41.81%
    |'
- en: '| [[118](#bib.bib118)] | LSTM | GloVe | Images | LSTM | CRF | SnapCaptions:
    52.4% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | LSTM | GloVe | 图像 | LSTM | CRF | SnapCaptions: 52.4%
    |'
- en: '| [[109](#bib.bib109)] | LSTM | SSKIP | Lexical | LSTM | CRF | CoNLL03: 91.73%;
    OntoNotes5.0: 87.95% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | LSTM | SSKIP | 词汇 | LSTM | CRF | CoNLL03: 91.73%;
    OntoNotes5.0: 87.95% |'
- en: '| [[119](#bib.bib119)] | - | WordPiece | Segment, position | Transformer |
    Softmax | CoNLL03: 92.8% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | - | WordPiece | 分段, 位置 | Transformer | Softmax | CoNLL03:
    92.8% |'
- en: '| [[121](#bib.bib121)] | LSTM | SENNA | - | LSTM | Softmax | CoNLL03: 91.48%
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | LSTM | SENNA | - | LSTM | Softmax | CoNLL03: 91.48%
    |'
- en: '| [[124](#bib.bib124)] | LSTM | Google Word2vec | - | LSTM | CRF | CoNLL03:
    86.26% |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | LSTM | Google Word2vec | - | LSTM | CRF | CoNLL03:
    86.26% |'
- en: '| [[21](#bib.bib21)] | GRU | SENNA | LM | GRU | CRF | CoNLL03: 91.93% |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [[21](#bib.bib21)] | GRU | SENNA | 语言模型 | GRU | CRF | CoNLL03: 91.93% |'
- en: '| [[126](#bib.bib126)] | LSTM | GloVe | - | LSTM | CRF | CoNLL03: 91.71% |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| [[126](#bib.bib126)] | LSTM | GloVe | - | LSTM | CRF | CoNLL03: 91.71% |'
- en: '| [[142](#bib.bib142)] | - | SENNA | POS, gazetteers | CNN | Semi-CRF | CoNLL03:
    90.87% |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bib142)] | - | SENNA | 词性标注, 地名词典 | CNN | 半CRF | CoNLL03: 90.87%
    |'
- en: '| [[143](#bib.bib143)] | LSTM | GloVe | - | LSTM | Semi-CRF | CoNLL03: 91.38%
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [[143](#bib.bib143)] | LSTM | GloVe | - | LSTM | 半CRF | CoNLL03: 91.38% |'
- en: '| [[88](#bib.bib88)] | CNN | Trained on Gigaword | - | LSTM | LSTM | CoNLL03:
    90.69%; OntoNotes5.0: 86.15% |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | CNN | 在Gigaword上训练 | - | LSTM | LSTM | CoNLL03: 90.69%;
    OntoNotes5.0: 86.15% |'
- en: '| [[110](#bib.bib110)] | - | GloVe | ELMo, dependency | LSTM | CRF | CoNLL03:
    92.4%; OntoNotes5.0: 89.88% |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | - | GloVe | ELMo, 依赖关系 | LSTM | CRF | CoNLL03: 92.4%;
    OntoNotes5.0: 89.88% |'
- en: '| [[108](#bib.bib108)] | CNN | GloVe | ELMo, gazetteers | LSTM | Semi-CRF |
    CoNLL03: 92.75%; OntoNotes5.0: 89.94% |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] | CNN | GloVe | ELMo, 地名词典 | LSTM | 半CRF | CoNLL03:
    92.75%; OntoNotes5.0: 89.94% |'
- en: '| [[133](#bib.bib133)] | LSTM | GloVe | ELMo, POS | LSTM | Softmax | CoNLL03:
    92.28% |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | LSTM | GloVe | ELMo, 词性标注 | LSTM | Softmax | CoNLL03:
    92.28% |'
- en: '| [[137](#bib.bib137)] | - | - | BERT | - | Softmax | CoNLL03: 93.04%; OntoNotes5.0:
    91.11% |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [[137](#bib.bib137)] | - | - | BERT | - | Softmax | CoNLL03: 93.04%; OntoNotes5.0:
    91.11% |'
- en: '| [[138](#bib.bib138)] | - | - | BERT | - | Softmax +Dice Loss | CoNLL03: 93.33%;
    OntoNotes5.0: 92.07% |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | - | - | BERT | - | Softmax + Dice损失 | CoNLL03: 93.33%;
    OntoNotes5.0: 92.07% |'
- en: '| [[134](#bib.bib134)] | LSTM | GloVe | BERT, document-level embeddings | LSTM
    | CRF | CoNLL03: 93.37%; OntoNotes5.0: 90.3% |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] | LSTM | GloVe | BERT, 文档级嵌入 | LSTM | CRF | CoNLL03:
    93.37%; OntoNotes5.0: 90.3% |'
- en: '| [[135](#bib.bib135)] | CNN | GloVe | BERT, global embeddings | GRU | GRU
    | CoNLL03: 93.47% |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [[135](#bib.bib135)] | CNN | GloVe | BERT, 全球嵌入 | GRU | GRU | CoNLL03: 93.47%
    |'
- en: '| [[132](#bib.bib132)] | CNN | - | Cloze-style LM embeddings | LSTM | CRF |
    CoNLL03: 93.5% |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | CNN | - | Cloze风格语言模型嵌入 | LSTM | CRF | CoNLL03: 93.5%
    |'
- en: '| [[136](#bib.bib136)] | - | GloVe | Plooled contextual embeddings | RNN |
    CRF | CoNLL03: 93.47% |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | - | GloVe | 池化上下文嵌入 | RNN | CRF | CoNLL03: 93.47%
    |'
- en: 3.4.3 Recurrent Neural Networks
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 循环神经网络
- en: A few studies [[90](#bib.bib90), [96](#bib.bib96), [144](#bib.bib144), [89](#bib.bib89),
    [88](#bib.bib88)] have explored RNN to decode tags. Shen et al. [[88](#bib.bib88)]
    reported that RNN tag decoders outperform CRF and are faster to train when the
    number of entity types is large. Figure [12(c)](#S3.F12.sf3 "In Figure 12 ‣ 3.4
    Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition") illustrates the workflow of RNN-based
    tag decoders, which serve as a language model to greedily produce a tag sequence.
    The [GO]-symbol at the first step is provided as $y_{1}$ to the RNN decoder. Subsequently,
    at each time step $i$, the RNN decoder computes current decoder hidden state $h_{i+1}^{Dec}$
    in terms of previous step tag $y_{i}$, previous step decoder hidden state $h_{i}^{Dec}$
    and current step encoder hidden state $h_{i+1}^{Enc}$; the current output tag
    $y_{i+1}$ is decoded by using a softmax loss function and is further fed as an
    input to the next time step. Finally, we obtain a tag sequence over all time steps.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究[[90](#bib.bib90), [96](#bib.bib96), [144](#bib.bib144), [89](#bib.bib89),
    [88](#bib.bib88)]探索了使用RNN解码标签。Shen等人[[88](#bib.bib88)]报告称，当实体类型数量较多时，RNN标签解码器的表现优于CRF，并且训练速度更快。图[12(c)](#S3.F12.sf3
    "在图12 ‣ 3.4 标签解码器架构 ‣ 3 命名实体识别的深度学习技术 ‣ 基于深度学习的命名实体识别综述")展示了基于RNN的标签解码器的工作流程，它们作为语言模型贪婪地生成标签序列。在第一步，符号[GO]作为$y_{1}$提供给RNN解码器。随后，在每个时间步骤$i$，RNN解码器计算当前解码器隐藏状态$h_{i+1}^{Dec}$，其基于上一步标签$y_{i}$、上一步解码器隐藏状态$h_{i}^{Dec}$和当前步骤编码器隐藏状态$h_{i+1}^{Enc}$；当前输出标签$y_{i+1}$通过softmax损失函数解码，并进一步作为输入提供给下一个时间步骤。最后，我们获得一个跨越所有时间步骤的标签序列。
- en: 3.4.4 Pointer Networks
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 指针网络
- en: Pointer networks apply RNNs to learn the conditional probability of an output
    sequence with elements that are discrete tokens corresponding to the positions
    in an input sequence [[145](#bib.bib145), [146](#bib.bib146)]. It represents variable
    length dictionaries by using a softmax probability distribution as a “pointer”.
    Zhai et al. [[95](#bib.bib95)] first applied pointer networks to produce sequence
    tags. Illustrated in Figure [12(d)](#S3.F12.sf4 "In Figure 12 ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition"), pointer networks first identify a chunk (or a
    segment), and then label it. This operation is repeated until all the words in
    input sequence are processed. In Figure [12(d)](#S3.F12.sf4 "In Figure 12 ‣ 3.4
    Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition"), given the start token “<s>”, the segment
    “Michael Jeffery Jordan” is first identified and then labeled as “PERSON”. The
    segmentation and labeling can be done by two separate neural networks in pointer
    networks. Next, “Michael Jeffery Jordan” is taken as input and fed into pointer
    networks. As a result, the segment “was” is identified and labeled as “O”.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 指针网络将RNN应用于学习输出序列的条件概率，其中元素是对应于输入序列位置的离散标记[[145](#bib.bib145), [146](#bib.bib146)]。它通过使用softmax概率分布作为“指针”来表示可变长度的字典。Zhai等人[[95](#bib.bib95)]首次将指针网络应用于生成序列标签。如图[12(d)](#S3.F12.sf4
    "在图12 ‣ 3.4 标签解码器架构 ‣ 3 命名实体识别的深度学习技术 ‣ 基于深度学习的命名实体识别综述")所示，指针网络首先识别一个块（或一个段落），然后对其进行标记。该操作重复进行，直到处理完输入序列中的所有单词。在图[12(d)](#S3.F12.sf4
    "在图12 ‣ 3.4 标签解码器架构 ‣ 3 命名实体识别的深度学习技术 ‣ 基于深度学习的命名实体识别综述")中，给定起始标记“<s>”，首先识别出段落“Michael
    Jeffery Jordan”，然后将其标记为“PERSON”。在指针网络中，分割和标记可以通过两个独立的神经网络完成。接下来，“Michael Jeffery
    Jordan”作为输入，输入到指针网络中。结果，段落“was”被识别并标记为“O”。
- en: 3.5 Summary of DL-based NER
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于深度学习的命名实体识别总结
- en: Architecture Summary. Table [III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random
    Fields ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣
    A Survey on Deep Learning for Named Entity Recognition") summarizes recent works
    on neural NER by their architecture choices. BiLSTM-CRF is the most common architecture
    for NER using deep learning. The method [[132](#bib.bib132)] which pre-trains
    a bi-directional Transformer model in a cloze-style manner, achieves the state-of-the-art
    performance ($93.5\%$) on CoNLL03. The work with BERT and dice loss [[138](#bib.bib138)]
    achieves the state-of-the-art performance ($92.07\%$) on OntoNotes5.0.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 架构总结。表[III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition")总结了近年来在神经NER领域中不同架构选择的工作。BiLSTM-CRF是使用深度学习进行NER的最常见架构。方法[[132](#bib.bib132)]通过以填空式的方式预训练双向Transformer模型，在CoNLL03数据集上达到了最先进的性能（$93.5\%$）。使用BERT和dice
    loss的工作[[138](#bib.bib138)]在OntoNotes5.0数据集上取得了最先进的性能（$92.07\%$）。
- en: The success of a NER system heavily relies on its input representation. Integrating
    or fine-tuning pre-trained language model embeddings is becoming a new paradigm
    for neural NER. When leveraging these language model embeddings, there are significant
    performance improvements [[107](#bib.bib107), [103](#bib.bib103), [108](#bib.bib108),
    [133](#bib.bib133), [137](#bib.bib137), [138](#bib.bib138), [134](#bib.bib134),
    [135](#bib.bib135), [132](#bib.bib132), [136](#bib.bib136)]. The last column in
    Table [III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition") lists the reported performance in F-score on a
    few benchmark datasets. While high F-scores have been reported on formal documents
    (e.g., CoNLL03 and OntoNotes5.0), NER on noisy data (e.g., W-NUT17) remains challenging.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: NER系统的成功在很大程度上依赖于其输入表示。集成或微调预训练语言模型嵌入正成为神经NER的新范式。在利用这些语言模型嵌入时，性能显著提升[[107](#bib.bib107),
    [103](#bib.bib103), [108](#bib.bib108), [133](#bib.bib133), [137](#bib.bib137),
    [138](#bib.bib138), [134](#bib.bib134), [135](#bib.bib135), [132](#bib.bib132),
    [136](#bib.bib136)]。表[III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields
    ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey
    on Deep Learning for Named Entity Recognition")的最后一列列出了在一些基准数据集上报告的F-score性能。虽然在正式文档（例如，CoNLL03和OntoNotes5.0）上报告了高F-score，但在嘈杂数据（例如，W-NUT17）上的NER仍然具有挑战性。
- en: 'Architecture Comparison. We discuss pros and cons from three perspectives:
    input, encoder, and decoder. First, no consensus has been reached about whether
    external knowledge should be or how to integrate into DL-based NER models. Some
    studies [[142](#bib.bib142), [108](#bib.bib108), [133](#bib.bib133), [110](#bib.bib110)]
    shows that NER performance can be boosted with external knowledge. However, the
    disadvantages are also apparent: 1) acquiring external knowledge is labor-intensive
    (e.g., gazetteers) or computationally expensive (e.g., dependency); 2) integrating
    external knowledge adversely affects end-to-end learning and hurts the generality
    of DL-based systems.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 架构比较。我们从三个角度讨论了优缺点：输入、编码器和解码器。首先，对于是否应该将外部知识集成到基于深度学习的命名实体识别（NER）模型中或如何集成尚无共识。一些研究[[142](#bib.bib142),
    [108](#bib.bib108), [133](#bib.bib133), [110](#bib.bib110)]表明，外部知识可以提升NER性能。然而，缺点也很明显：1）获取外部知识是劳动密集型的（例如，地名词典）或计算昂贵的（例如，依赖关系）；2）集成外部知识会对端到端学习产生负面影响，并损害基于深度学习系统的普遍性。
- en: 'Second, Transformer encoder is more effective than LSTM when Transformer is
    pre-trained on huge corpora. Transformers fail on NER task if they are not pre-trained
    and when the training data is limited [[147](#bib.bib147), [148](#bib.bib148)].
    On the other hand, Transformer encoder is faster than recursive layers when the
    length of the sequence $n$ is smaller than the dimensionality of the representation
    $d$ (complexities: self-attention $\mathcal{O}(n^{2}\cdot d)$ and recurrent $\mathcal{O}(n\cdot
    d^{2})$) [[128](#bib.bib128)].'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当Transformer在大规模语料上进行预训练时，其编码器比LSTM更有效。如果Transformer没有经过预训练且训练数据有限，则在NER任务上表现不佳[[147](#bib.bib147),
    [148](#bib.bib148)]。另一方面，当序列长度$n$小于表示维度$d$时，Transformer编码器比递归层更快（复杂度：自注意力 $\mathcal{O}(n^{2}\cdot
    d)$ 和递归 $\mathcal{O}(n\cdot d^{2})$）[[128](#bib.bib128)]。
- en: Third, a major disadvantage of RNN and Pointer Network decoders lies in greedily
    decoding, which means that the input of current step needs the output of previous
    step. This mechanism may have a significant impact on the speed and is an obstacle
    to parallelization. CRF is the most common choice for tag decoder. CRF is powerful
    to capture label transition dependencies when adopting non-language-model (i.e.,
    non-contextualized) embeddings such as Word2vec and GloVe. However, CRF could
    be computationally expensive when the number of entity types is large. More importantly,
    CRF does not always lead to better performance compared with softmax classification
    when adopting contextualized language model embeddings such as BERT and ELMo [[139](#bib.bib139),
    [137](#bib.bib137)].
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，RNN 和 Pointer Network 解码器的一个主要缺点在于贪婪解码，这意味着当前步骤的输入需要依赖于前一步骤的输出。这种机制可能会对速度产生显著影响，并且是并行化的障碍。CRF
    是最常用的标签解码器选择。CRF 在采用非语言模型（即非上下文化）嵌入，如 Word2vec 和 GloVe 时，能够强有力地捕捉标签转移依赖。然而，当实体类型数量较多时，CRF
    的计算开销可能较大。更重要的是，CRF 在采用上下文化语言模型嵌入，如 BERT 和 ELMo 时，不一定总能比 softmax 分类得到更好的性能 [[139](#bib.bib139),
    [137](#bib.bib137)]。
- en: For end users, what architecture to choose is data and domain task dependent.
    If data is abundant, training models with RNNs from scratch and fine-tuning contextualized
    language models could be considered. If data is scarce, adopting transfer strategies
    might be a better choice. For newswires domain, there are many pre-trained off-the-shelf
    models available. For specific domains (e.g., medical and social media), fine-tuning
    general-purpose contextualized language models with domain-specific data is often
    an effective way.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最终用户来说，选择何种架构取决于数据和领域任务。如果数据丰富，可以考虑从头开始训练 RNN 模型并微调上下文化语言模型。如果数据稀缺，采用迁移策略可能是更好的选择。对于新闻领域，有许多预训练的现成模型可用。对于特定领域（如医学和社交媒体），通常通过使用领域特定数据微调通用上下文化语言模型是一种有效的方法。
- en: NER for Different Languages. In this survey, we mainly focus on NER in English
    and in general domain. Apart from English language, there are many studies on
    other languages or cross-lingual settings. Wu et al. [[120](#bib.bib120)] and
    Wang et al. [[149](#bib.bib149)] investigated NER in Chinese clinical text. Zhang
    and Yang [[150](#bib.bib150)] proposed a lattice-structured LSTM model for Chinese
    NER, which encodes a sequence of input characters as well as all potential words
    that match a lexicon. Other than Chinese, many studies are conducted on other
    languages. Examples include Mongolian [[151](#bib.bib151)], Czech [[152](#bib.bib152)],
    Arabic [[153](#bib.bib153)], Urdu [[154](#bib.bib154)], Vietnamese [[155](#bib.bib155)],
    Indonesian [[156](#bib.bib156)], and Japanese [[157](#bib.bib157)]. Each language
    has its own characteristics for understanding the fundamentals of NER task on
    that language. There are also a number of studies [[158](#bib.bib158), [106](#bib.bib106),
    [159](#bib.bib159), [160](#bib.bib160)] aiming to solve the NER problem in a cross-lingual
    setting by transferring knowledge from a source language to a target language
    with few or no labels.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 针对不同语言的 NER。在这项调查中，我们主要关注英文及一般领域的 NER。除了英语，还有许多关于其他语言或跨语言设置的研究。吴等 [[120](#bib.bib120)]
    和王等 [[149](#bib.bib149)] 研究了中文临床文本中的 NER。张和杨 [[150](#bib.bib150)] 提出了一个用于中文 NER
    的格状 LSTM 模型，该模型对输入字符序列及所有与词汇表匹配的潜在词进行编码。除了中文，许多研究还涉及其他语言。例如蒙古语 [[151](#bib.bib151)]、捷克语
    [[152](#bib.bib152)]、阿拉伯语 [[153](#bib.bib153)]、乌尔都语 [[154](#bib.bib154)]、越南语 [[155](#bib.bib155)]、印尼语
    [[156](#bib.bib156)] 和日语 [[157](#bib.bib157)]。每种语言在理解该语言的 NER 任务基础时都有其自身特点。还有许多研究
    [[158](#bib.bib158), [106](#bib.bib106), [159](#bib.bib159), [160](#bib.bib160)]
    旨在通过将知识从源语言转移到目标语言（少量或没有标签）来解决跨语言环境中的 NER 问题。
- en: 4 Applied Deep Learning for NER
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 应用深度学习进行命名实体识别（NER）
- en: Sections [3.2](#S3.SS2 "3.2 Distributed Representations for Input ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition"),
    [3.3](#S3.SS3 "3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques
    for NER ‣ A Survey on Deep Learning for Named Entity Recognition"), and [3.4](#S3.SS4
    "3.4 Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey
    on Deep Learning for Named Entity Recognition") outline typical network architectures
    for NER. In this section, we survey recent applied deep learning techniques that
    are being explored for NER.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[3.2](#S3.SS2 "3.2 输入的分布式表示 ‣ 3 NER的深度学习技术 ‣ 对命名实体识别的深度学习综述")、[3.3](#S3.SS3
    "3.3 上下文编码器架构 ‣ 3 NER的深度学习技术 ‣ 对命名实体识别的深度学习综述")和[3.4](#S3.SS4 "3.4 标签解码器架构 ‣ 3
    NER的深度学习技术 ‣ 对命名实体识别的深度学习综述")节概述了NER的典型网络架构。在本节中，我们调查了正在探索的用于NER的最新应用深度学习技术。'
- en: 4.1 Deep Multi-task Learning for NER
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 深度多任务学习用于NER
- en: Multi-task learning [[161](#bib.bib161)] is an approach that learns a group
    of related tasks together. By considering the relation between different tasks,
    multi-task learning algorithms are expected to achieve better results than the
    ones that learn each task individually.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习[[161](#bib.bib161)]是一种同时学习一组相关任务的方法。通过考虑不同任务之间的关系，多任务学习算法预计能取得比单独学习每个任务更好的结果。
- en: Collobert et al. [[17](#bib.bib17)] trained a window/sentence approach network
    to jointly perform POS, Chunk, NER, and SRL tasks. This multi-task mechanism lets
    the training algorithm to discover internal representations that are useful for
    all the tasks of interest. Yang et al. [[106](#bib.bib106)] proposed a multi-task
    joint model, to learn language-specific regularities, jointly trained for POS,
    Chunk, and NER tasks. Rei [[124](#bib.bib124)] found that by including an unsupervised
    language modeling objective in the training process, the sequence labeling model
    achieves consistent performance improvement. Lin et al. [[160](#bib.bib160)] proposed
    a multi-lingual multi-task architecture for low-resource settings, which can effectively
    transfer different types of knowledge to improve the main model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Collobert等人[[17](#bib.bib17)]训练了一个窗口/句子方法网络，来共同执行POS、Chunk、NER和SRL任务。这种多任务机制使得训练算法能够发现对所有感兴趣任务都有用的内部表示。Yang等人[[106](#bib.bib106)]提出了一种多任务联合模型，用于学习语言特定的规律，联合训练POS、Chunk和NER任务。Rei[[124](#bib.bib124)]发现，通过在训练过程中包含无监督语言建模目标，序列标注模型能够实现一致的性能提升。Lin等人[[160](#bib.bib160)]提出了一种针对低资源环境的多语言多任务架构，该架构可以有效地转移不同类型的知识以改善主模型。
- en: 'Other than considering NER together with other sequence labeling tasks, multi-task
    learning framework can be applied for joint extraction of entities and relations [[90](#bib.bib90),
    [96](#bib.bib96)], or to model NER as two related subtasks: entity segmentation
    and entity category prediction [[162](#bib.bib162), [114](#bib.bib114)]. In biomedical
    domain, because of the differences in different datasets, NER on each dataset
    is considered as a task in a multi-task setting [[163](#bib.bib163), [164](#bib.bib164)].
    A main assumption here is that the different datasets share the same character-
    and word-level information. Then multi-task learning is applied to make more efficient
    use of the data and to encourage the models to learn more generalized representations.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将NER与其他序列标注任务一起考虑外，多任务学习框架还可以用于实体和关系的联合提取[[90](#bib.bib90), [96](#bib.bib96)]，或将NER建模为两个相关的子任务：实体分割和实体类别预测[[162](#bib.bib162),
    [114](#bib.bib114)]。在生物医学领域，由于不同数据集之间的差异，每个数据集上的NER被视为多任务设置中的一个任务[[163](#bib.bib163),
    [164](#bib.bib164)]。主要假设是不同数据集共享相同的字符和词级信息。然后应用多任务学习，以更有效地利用数据，并鼓励模型学习更通用的表示。
- en: 4.2 Deep Transfer Learning for NER
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 深度迁移学习用于NER
- en: Transfer learning aims to perform a machine learning task on a target domain
    by taking advantage of knowledge learned from a source domain [[165](#bib.bib165)].
    In NLP, transfer learning is also known as domain adaptation. On NER tasks, the
    traditional approach is through bootstrapping algorithms [[166](#bib.bib166),
    [167](#bib.bib167), [168](#bib.bib168)]. Recently, a few approaches [[169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171), [172](#bib.bib172), [127](#bib.bib127),
    [173](#bib.bib173)] have been proposed for low-resource and across-domain NER
    using deep neural networks.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习旨在通过利用从源领域学到的知识，在目标领域执行机器学习任务[[165](#bib.bib165)]。在自然语言处理（NLP）中，迁移学习也被称为领域适应。在命名实体识别（NER）任务中，传统的方法是通过引导算法[[166](#bib.bib166),
    [167](#bib.bib167), [168](#bib.bib168)]。最近，一些方法[[169](#bib.bib169), [170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [127](#bib.bib127), [173](#bib.bib173)]
    已经被提出，用于利用深度神经网络进行低资源和跨领域的NER。
- en: 'Pan et al. [[169](#bib.bib169)] proposed a transfer joint embedding (TJE) approach
    for cross-domain NER. TJE employs label embedding techniques to transform multi-class
    classification to regression in a low-dimensional latent space. Qu et al. [[174](#bib.bib174)]
    observed that related named entity types often share lexical and context features.
    Their approach learns the correlation between source and target named entity types
    using a two-layer neural network. Their approach is applicable to the setting
    that the source domain has similar (but not identical) named entity types with
    the target domain. Peng and Dredze [[162](#bib.bib162)] explored transfer learning
    in a multi-task learning setting, where they considered two domains: news and
    social media, for two tasks: word segmentation and NER.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Pan等人[[169](#bib.bib169)] 提出了用于跨领域NER的迁移联合嵌入（TJE）方法。TJE采用标签嵌入技术，将多类分类转化为低维潜在空间中的回归。Qu等人[[174](#bib.bib174)]
    观察到相关的命名实体类型通常共享词汇和上下文特征。他们的方法使用双层神经网络学习源领域和目标领域命名实体类型之间的关联。该方法适用于源领域与目标领域具有相似（但不完全相同）命名实体类型的设置。Peng和Dredze[[162](#bib.bib162)]
    探索了在多任务学习环境中进行迁移学习，他们考虑了两个领域：新闻和社交媒体，涉及两个任务：词语分割和NER。
- en: 'In the setting of transfer learning, different neural models commonly share
    different parts of model parameters between source task and target task. Yang
    et al. [[175](#bib.bib175)] first investigated the transferability of different
    layers of representations. Then they presented three different parameter-sharing
    architectures for cross-domain, cross-lingual, and cross-application scenarios.
    If two tasks have mappable label sets, there is a shared CRF layer, otherwise,
    each task learns a separate CRF layer. Experimental results show significant improvements
    on various datasets under low-resource conditions (i.e., fewer available annotations).
    Pius and Mark [[176](#bib.bib176)] extended Yang’s approach to allow joint training
    on informal corpus (e.g., WNUT 2017), and to incorporate sentence level feature
    representation. Their approach achieved the 2nd place at the WNUT 2017 shared
    task for NER, obtaining an F1-score of 40.78%. Zhao et al. [[177](#bib.bib177)]
    proposed a multi-task model with domain adaption, where the fully connection ayer
    are adapted to different datasets, and the CRF features are computed separately.
    A major merit of Zhao’s model is that the instances with different distribution
    and misaligned annotation guideline are filtered out in data selection procedure.
    Different from these parameter-sharing architectures, Lee et al. [[170](#bib.bib170)]
    applied transfer learning in NER by training a model on source task and using
    the trained model on target task for fine-tuning. Recently, Lin and Lu [[171](#bib.bib171)]
    also proposed a fine-tuning approach for NER by introducing three neural adaptation
    layers: word adaptation layer, sentence adaptation layer, and output adaptation
    layer. Beryozkin et al. [[178](#bib.bib178)] proposed a tag-hierarchy model for
    heterogeneous tag-sets NER setting, where the hierarchy is used during inference
    to map fine-grained tags onto a target tag-set. In addition, some studies [[164](#bib.bib164),
    [179](#bib.bib179), [180](#bib.bib180)] explored transfer learning in biomedical
    NER to reduce the amount of required labeled data.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习的设置中，不同的神经模型通常在源任务和目标任务之间共享模型参数的不同部分。杨等人[[175](#bib.bib175)]首次研究了不同表示层的迁移能力。随后，他们提出了三种不同的参数共享架构，适用于跨领域、跨语言和跨应用场景。如果两个任务具有可映射的标签集，则存在一个共享的CRF层，否则每个任务学习一个单独的CRF层。实验结果表明，在低资源条件下（即可用标注较少），在各种数据集上有显著改进。皮乌斯和马克[[176](#bib.bib176)]扩展了杨的方法，使其能够在非正式语料库（例如WNUT
    2017）上进行联合训练，并纳入句子级特征表示。他们的方法在WNUT 2017共享任务的NER中获得了第二名，F1得分为40.78%。赵等人[[177](#bib.bib177)]提出了一种具有领域适应的多任务模型，其中完全连接层被适应于不同的数据集，CRF特征则被单独计算。赵的模型的一大优点是，在数据选择过程中，具有不同分布和不一致标注指南的实例被过滤掉。与这些参数共享架构不同，李等人[[170](#bib.bib170)]通过在源任务上训练模型并使用该模型在目标任务上进行微调，应用了迁移学习于NER。最近，林和卢[[171](#bib.bib171)]也提出了一种微调方法，通过引入三个神经适应层：词汇适应层、句子适应层和输出适应层。贝里奥兹金等人[[178](#bib.bib178)]提出了一种用于异构标签集NER设置的标签层次模型，其中层次结构在推理过程中用于将细粒度标签映射到目标标签集。此外，一些研究[[164](#bib.bib164),
    [179](#bib.bib179), [180](#bib.bib180)]探讨了生物医学NER中的迁移学习，以减少所需标注数据的数量。
- en: 4.3 Deep Active Learning for NER
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 深度主动学习用于NER
- en: The key idea behind active learning is that a machine learning algorithm can
    perform better with substantially less data from training, if it is allowed to
    choose the data from which it learns [[181](#bib.bib181)]. Deep learning typically
    requires a large amount of training data which is costly to obtain. Thus, combining
    deep learning with active learning is expected to reduce data annotation effort.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的关键思想是，如果机器学习算法可以选择其学习的数据，它可以在训练中用显著更少的数据实现更好的表现[[181](#bib.bib181)]。深度学习通常需要大量的训练数据，而这些数据的获取成本很高。因此，将深度学习与主动学习相结合有望减少数据标注的工作量。
- en: Training with active learning proceeds in multiple rounds. However, traditional
    active learning schemes are expensive for deep learning because after each round
    they require complete retraining of the classifier with newly annotated samples.
    Because retraining from scratch is not practical for deep learning, Shen et al. [[88](#bib.bib88)]
    proposed to carry out incremental training for NER with each batch of new labels.
    They mix newly annotated samples with the existing ones, and update neural network
    weights for a small number of epochs, before querying for labels in a new round.
    Specifically, at the beginning of each round, the active learning algorithm chooses
    sentences to be annotated, to the predefined budget. The model parameters are
    updated by training on the augmented dataset, after receiving chose annotations.
    The active learning algorithm adopts uncertainty sampling strategy [[182](#bib.bib182)]
    in selecting sentences to be annotated. Experimental results show that active
    learning algorithms achieve 99% performance of the best deep learning model trained
    on full data using only 24.9% of the training data on the English dataset and
    30.1% on Chinese dataset. Moreover, 12.0% and 16.9% of training data were enough
    for deep active learning model to outperform shallow models learned on full training
    data [[183](#bib.bib183)].
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的训练在多个回合中进行。然而，传统的主动学习方案对于深度学习而言成本较高，因为每一回合后都需要用新标注的样本完全重新训练分类器。由于从头开始重新训练对于深度学习不切实际，Shen
    等人[[88](#bib.bib88)] 提出了对 NER 进行增量训练的方案，每批次的新标签进行训练。他们将新标注的样本与现有样本混合，并在进行新一轮标签查询之前，更新神经网络权重若干个训练周期。具体而言，在每轮开始时，主动学习算法会选择需要标注的句子，直到预定义的预算。模型参数通过在增强数据集上训练来更新，在收到选择的标注后。主动学习算法在选择需要标注的句子时采用不确定性采样策略[[182](#bib.bib182)]。实验结果表明，主动学习算法在英文数据集上仅用24.9%的训练数据就能达到最佳深度学习模型在全数据上训练的99%性能，在中文数据集上则为30.1%。此外，12.0%和16.9%的训练数据足以让深度主动学习模型优于在全训练数据上学习的浅层模型[[183](#bib.bib183)]。
- en: 4.4 Deep Reinforcement Learning for NER
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 深度强化学习在 NER 中的应用
- en: 'Reinforcement learning (RL) is a branch of machine learning inspired by behaviorist
    psychology, which is concerned with how software agents take actions in an environment
    so as to maximize some cumulative rewards [[184](#bib.bib184), [185](#bib.bib185)].
    The idea is that an agent will learn from the environment by interacting with
    it and receiving rewards for performing actions. Specifically, the RL problem
    can be formulated as follows [[186](#bib.bib186)]: the environment is modeled
    as a stochastic finite state machine with inputs (actions from agent) and outputs
    (observations and rewards to the agent). It consists of three key components:
    (i) state transition function, (ii) observation (i.e., output) function, and (iii)
    reward function. The agent is also modeled as a stochastic finite state machine
    with inputs (observations/rewards from the environment) and outputs (actions to
    the environment). It consists of two components: (i) state transition function,
    and (ii) policy/output function. The ultimate goal of an agent is to learn a good
    state-update function and policy by attempting to maximize the cumulative rewards.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是受行为主义心理学启发的机器学习的一个分支，关注于软件代理在环境中采取行动以最大化某些累积奖励[[184](#bib.bib184),
    [185](#bib.bib185)]。其理念是代理通过与环境互动并获得执行动作的奖励来学习环境。具体而言，RL 问题可以表述为[[186](#bib.bib186)]：环境被建模为具有输入（代理的动作）和输出（观察与奖励）的随机有限状态机。它包含三个关键组件：（i）状态转移函数，（ii）观察（即，输出）函数，以及（iii）奖励函数。代理也被建模为具有输入（来自环境的观察/奖励）和输出（对环境的动作）的随机有限状态机。它包含两个组件：（i）状态转移函数，以及（ii）策略/输出函数。代理的*终极目标*是通过尝试最大化累积奖励来学习一个好的状态更新函数和策略。
- en: Narasimhan et al. [[187](#bib.bib187)] modeled the task of information extraction
    as a Markov decision process (MDP), which dynamically incorporates entity predictions
    and provides flexibility to choose the next search query from a set of automatically
    generated alternatives. The process is comprised of issuing search queries, extraction
    from new sources, and reconciliation of extracted values, and the process repeats
    until sufficient evidence is obtained. In order to learn a good policy for an
    agent, they utilize a deep Q-network [[188](#bib.bib188)] as a function approximator,
    in which the state-action value function (i.e., Q-function) is approximated by
    using a deep neural network. Recently, Yang et al. [[189](#bib.bib189)] utilized
    the data generated by distant supervision to perform new type named entity recognition
    in new domains. The instance selector is based on reinforcement learning and obtains
    the feedback reward from the NE tagger, aiming at choosing positive sentences
    to reduce the effect of noisy annotation.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Narasimhan 等人 [[187](#bib.bib187)] 将信息提取任务建模为一个马尔可夫决策过程（MDP），该过程动态地结合了实体预测，并提供了从一组自动生成的备选查询中选择下一个搜索查询的灵活性。该过程包括发出搜索查询、从新源中提取信息以及协调提取值，直到获得足够的证据。为了为代理学习一个良好的策略，他们利用深度
    Q 网络 [[188](#bib.bib188)] 作为函数逼近器，其中状态-动作值函数（即 Q 函数）通过使用深度神经网络进行逼近。最近，Yang 等人
    [[189](#bib.bib189)] 利用通过远程监督生成的数据在新领域中执行新型命名实体识别。实例选择器基于强化学习，并从 NE 标注器处获得反馈奖励，旨在选择正样本以减少噪声标注的影响。
- en: 4.5 Deep Adversarial Learning for NER
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 深度对抗学习用于命名实体识别（NER）
- en: 'Adversarial learning [[190](#bib.bib190)] is the process of explicitly training
    a model on adversarial examples. The purpose is to make the model more robust
    to attack or to reduce its test error on clean inputs. Adversarial networks learn
    to generate from a training distribution through a 2-player game: one network
    generates candidates (generative network) and the other evaluates them (discriminative
    network). Typically, the generative network learns to map from a latent space
    to a particular data distribution of interest, while the discriminative network
    discriminates between candidates generated by the generator and instances from
    the real-world data distribution [[191](#bib.bib191)].'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗学习 [[190](#bib.bib190)] 是在对抗样本上显式训练模型的过程。其目的是使模型对攻击更加鲁棒或减少其在干净输入上的测试误差。对抗网络通过一个二人博弈来学习从训练分布中生成样本：一个网络生成候选样本（生成网络），另一个网络对其进行评估（判别网络）。通常，生成网络学习将从潜在空间映射到特定的数据分布，而判别网络则区分生成器生成的候选样本和来自真实世界数据分布的实例
    [[191](#bib.bib191)]。
- en: 'For NER, adversarial examples are often produced in two ways. Some studies
    [[192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194)] considered the instances
    in a source domain as adversarial examples for a target domain, and vice versa.
    For example, Li et al. [[193](#bib.bib193)] and Cao et al. [[194](#bib.bib194)]
    both incorporated adversarial examples from other domains to encourage domain-invariant
    features for cross-domain NER. Another option is to prepare an adversarial sample
    by adding an original sample with a perturbation. For example, dual adversarial
    transfer network (DATNet), proposed in [[195](#bib.bib195)], aims to deal with
    the problem of low-resource NER. An adversarial sample is produced by adding original
    sample with a perturbation bounded by a small norm $\epsilon$ to maximize the
    loss function as follows: ${\eta_{x}}=\arg\mathop{\max}\limits_{\eta:{{\left\|\eta\right\|}_{2}}\leq\epsilon}l(\Theta;x+\eta)$,
    where $\Theta$ is the current model parameters set, $\epsilon$ can be determined
    on validation set. An adversarial example is constructed by $x_{adv}=x+\eta_{x}$.
    The classifier is trained on the mixture of original and adversarial examples
    to improve generalization.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命名实体识别（NER），对抗样本通常通过两种方式生成。一些研究 [[192](#bib.bib192), [193](#bib.bib193), [194](#bib.bib194)]
    将源领域中的实例视为目标领域的对抗样本，反之亦然。例如，Li 等人 [[193](#bib.bib193)] 和 Cao 等人 [[194](#bib.bib194)]
    都将来自其他领域的对抗样本纳入其中，以鼓励跨领域 NER 的领域不变特征。另一种选择是通过添加扰动来准备对抗样本。例如，[[195](#bib.bib195)]
    中提出的双对抗迁移网络（DATNet）旨在解决低资源 NER 的问题。对抗样本通过向原始样本添加扰动（扰动的范数由小的 $\epsilon$ 界定）来生成，以最大化损失函数，如下所示：${\eta_{x}}=\arg\mathop{\max}\limits_{\eta:{{\left\|\eta\right\|}_{2}}\leq\epsilon}l(\Theta;x+\eta)$，其中
    $\Theta$ 是当前模型参数集，$\epsilon$ 可以在验证集上确定。对抗样本由 $x_{adv}=x+\eta_{x}$ 构造。分类器在原始样本和对抗样本的混合上进行训练，以提高泛化能力。
- en: 4.6 Neural Attention for NER
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 神经注意力在NER中的应用
- en: The attention mechanism is loosely based on the visual attention mechanism found
    in human [[196](#bib.bib196)]. For example, people usually focus on a certain
    region of an image with “high resolution” while perceiving the surrounding region
    with “low resolution”. Neural attention mechanism allows neural networks have
    the ability to focus on a subset of its inputs. By applying attention mechanism,
    a NER model could capture the most informative elements in the inputs. In particular,
    the Transformer architecture reviewed in Section [3.3.5](#S3.SS3.SSS5 "3.3.5 Deep
    Transformer ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition") relies entirely
    on attention mechanism to draw global dependencies between input and output.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在某种程度上基于人类视觉注意力机制[[196](#bib.bib196)]。例如，人们通常将注意力集中在图像的某个“高分辨率”区域，同时以“低分辨率”感知周围区域。神经注意力机制使得神经网络能够专注于其输入的一个子集。通过应用注意力机制，NER模型可以捕捉输入中的最重要元素。特别是，第[3.3.5节](#S3.SS3.SSS5
    "3.3.5 Deep Transformer ‣ 3.3 Context Encoder Architectures ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition")中回顾的Transformer架构完全依赖于注意力机制来建立输入和输出之间的全局依赖关系。
- en: There are many other ways of applying attention mechanism in NER tasks. Rei
    et al. [[105](#bib.bib105)] applied an attention mechanism to dynamically decide
    how much information to use from a character- or word-level component in an end-to-end
    NER model. Zukov-Gregoric et al. [[197](#bib.bib197)] explored the self-attention
    mechanism in NER, where the weights are dependent on a single sequence (rather
    than on the relation between two sequences). Xu et al. [[198](#bib.bib198)] proposed
    an attention-based neural NER architecture to leverage document-level global information.
    In particular, the document-level information is obtained from document represented
    by pre-trained bidirectional language model with neural attention. Zhang et al. [[199](#bib.bib199)]
    used an adaptive co-attention network for NER in tweets. This adaptive co-attention
    network is a multi-modal model using co-attention process. Co-attention includes
    visual attention and textual attention to capture the semantic interaction between
    different modalities.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在NER任务中应用注意力机制还有许多其他方式。Rei等人[[105](#bib.bib105)]在端到端的NER模型中应用了注意力机制，以动态决定从字符级或词级组件中使用多少信息。Zukov-Gregoric等人[[197](#bib.bib197)]探索了NER中的自注意力机制，其中权重依赖于单个序列（而不是两个序列之间的关系）。Xu等人[[198](#bib.bib198)]提出了一种基于注意力的神经NER架构，以利用文档级的全局信息。特别是，文档级信息来自由预训练的双向语言模型和神经注意力表示的文档。Zhang等人[[199](#bib.bib199)]在推文中使用了一种自适应共同注意力网络进行NER。这种自适应共同注意力网络是一种使用共同注意力过程的多模态模型。共同注意力包括视觉注意力和文本注意力，以捕捉不同模态之间的语义交互。
- en: 5 Challenges and Future Directions
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战和未来方向
- en: Discussed in Section [3.5](#S3.SS5 "3.5 Summary of DL-based NER ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition"),
    the choices tag decoders do not vary as much as the choices of input representations
    and context encoders. From Google Word2vec to the more recent BERT model, DL-based
    NER benefits significantly from the advances made in pre-trained embeddings in
    modeling languages. Without the need of complicated feature-engineering, we now
    have the opportunity to re-look the NER task for its challenges and potential
    future directions.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[3.5节](#S3.SS5 "3.5 Summary of DL-based NER ‣ 3 Deep Learning Techniques for
    NER ‣ A Survey on Deep Learning for Named Entity Recognition")讨论中，标签解码器的选择不像输入表示和上下文编码器的选择那样变化多端。从Google
    Word2vec到更近期的BERT模型，基于深度学习的NER显著受益于在建模语言中预训练嵌入所取得的进展。在无需复杂特征工程的情况下，我们现在有机会重新审视NER任务的挑战和潜在未来方向。
- en: 5.1 Challenges
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 挑战
- en: Data Annotation. Supervised NER systems, including DL-based NER, require big
    annotated data in training. However, data annotation remains time consuming and
    expensive. It is a big challenge for many resource-poor languages and specific
    domains as domain experts are needed to perform annotation tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 数据注释。监督的NER系统，包括基于深度学习的NER，需要大量标注数据进行训练。然而，数据注释仍然耗时且昂贵。这对许多资源贫乏的语言和特定领域是一个巨大挑战，因为需要领域专家来执行注释任务。
- en: Quality and consistency of the annotation are both major concerns because of
    the language ambiguity. For instance, a same named entity may be annotated with
    different types. As an example, “Baltimore” in the sentence “Baltimore defeated
    the Yankees”, is labeled as Location in MUC-7 and Organization in CoNLL03\. Both
    “Empire State” and “Empire State Building”, is labeled as Location in CoNLL03
    and ACE datasets, causing confusion in entity boundaries. Because of the inconsistency
    in data annotation, model trained on one dataset may not work well on another
    even if the documents in the two datasets are from the same domain.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注释的质量和一致性都是主要问题，因为语言的模糊性。例如，相同的命名实体可能被标注为不同的类型。例如，在句子“巴尔的摩击败了扬基”中，“Baltimore”在MUC-7中标注为地点，在CoNLL03中标注为组织。“Empire
    State”和“Empire State Building”在CoNLL03和ACE数据集中都标注为地点，导致实体边界的混淆。由于数据注释的不一致，即使两个数据集中的文档来自相同的领域，基于一个数据集训练的模型也可能在另一个数据集上表现不佳。
- en: 'To make data annotation even more complicated, Katiyar and Cardie [[122](#bib.bib122)]
    reported that nested entities are fairly common: 17% of the entities in the GENIA
    corpus are embedded within another entity; in the ACE corpora, 30% of sentences
    contain nested entities. There is a need to develop common annotation schemes
    to be applicable to both nested entities and fine-grained entities, where one
    named entity may be assigned multiple types.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据注释变得更加复杂，Katiyar和Cardie [[122](#bib.bib122)] 报告说嵌套实体相当常见：GENIA语料库中17%的实体嵌套在另一个实体中；在ACE语料库中，30%的句子包含嵌套实体。需要开发通用的注释方案，以适用于嵌套实体和细粒度实体，其中一个命名实体可能被分配多种类型。
- en: Informal Text and Unseen Entities. Listed in Table [III](#S3.T3 "TABLE III ‣
    3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition"),
    decent results are reported on datasets with formal documents (e.g., news articles).
    However, on user-generated text e.g., WUT-17 dataset, the best F-scores are slightly
    above 40%. NER on informal text (e.g., tweets, comments, user forums) is more
    challenging than on formal text due to the shortness and noisiness. Many user-generated
    texts are domain specific as well. In many application scenarios, a NER system
    has to deal with user-generated text such as customer support in e-commerce and
    banking.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式文本和未见实体。在表[III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4
    Tag Decoder Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep
    Learning for Named Entity Recognition")中列出，对于正式文档（例如新闻文章）的数据集，报告了不错的结果。然而，在用户生成的文本（例如WUT-17数据集）上，最佳的F值稍高于40%。由于非正式文本（例如推文、评论、用户论坛）的简短性和噪声性，其NER要比正式文本更具挑战性。许多用户生成的文本也是领域特定的。在许多应用场景中，NER系统必须处理用户生成的文本，例如电子商务和银行的客户支持。
- en: Another interesting dimension to evaluate the robustness and effectiveness of
    NER system is its capability of identifying unusual, previously-unseen entities
    in the context of emerging discussions. There exists a shared task²²2[https://noisy-text.github.io/2017/emerging-rare-entities.html](https://noisy-text.github.io/2017/emerging-rare-entities.html)
    for this direction of research on WUT-17 dataset [[200](#bib.bib200)].
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 评估NER系统的鲁棒性和有效性的另一个有趣维度是其在新兴讨论背景下识别不寻常、之前未见实体的能力。这个方向的研究在WUT-17数据集上有一个共享任务²²2[https://noisy-text.github.io/2017/emerging-rare-entities.html](https://noisy-text.github.io/2017/emerging-rare-entities.html)
    [[200](#bib.bib200)]。
- en: 5.2 Future Directions
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 未来方向
- en: With the advances in modeling languages and demand in real-world applications,
    we expect NER to receive more attention from researchers. On the other hand, NER
    is in general considered as a pre-processing component to downstream applications.
    That means a particular NER task is defined by the requirement of downstream application,
    e.g., the types of named entities and whether there is a need to detect nested
    entities [[201](#bib.bib201)]. Based on the studies in this survey, we list the
    following directions for further exploration in NER research.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 随着建模语言的进步和实际应用需求的增加，我们预计命名实体识别（NER）将受到研究者更多的关注。另一方面，NER通常被视为下游应用的预处理组件。这意味着特定的NER任务由下游应用的需求定义，例如，命名实体的类型以及是否需要检测嵌套实体[[201](#bib.bib201)]。根据本调查中的研究，我们列出了进一步探索NER研究的以下方向。
- en: Fine-grained NER and Boundary Detection. While many existing studies [[97](#bib.bib97),
    [19](#bib.bib19), [109](#bib.bib109)] focused on coarse-grained NER in general
    domain, we expect more research on fine-grained NER in domain-specific areas to
    support various real word applications [[202](#bib.bib202)]. The challenges in
    fine-grained NER are the significant increase in NE types and the complication
    introduced by allowing a named entity to have multiple NE types. This calls for
    a re-visit of the common NER approaches where the entity boundaries and the types
    are detected simultaneously e.g., by using B- I- E- S-(entity type) and O as the
    decoding tags. It is worth considering to define named entity boundary detection
    as a dedicated task to detect NE boundaries while ignoring the NE types. The decoupling
    of boundary detection and NE type classification enables common and robust solutions
    for boundary detection that can be shared across different domains, and dedicated
    domain-specific approaches for NE type classification. Correct entity boundaries
    also effectively alleviate error propagation in entity linking to knowledge bases.
    There has been some studies [[203](#bib.bib203), [95](#bib.bib95)] which consider
    entity boundary detection as an intermediate step (i.e., a subtask) in NER. To
    the best of our knowledge, no existing work separately focuses on entity boundary
    detection to provide a robust recognizer. We expect a breakout in this research
    direction in the future.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度NER和边界检测。虽然许多现有研究[[97](#bib.bib97), [19](#bib.bib19), [109](#bib.bib109)]集中在通用领域的粗粒度NER上，但我们期待在特定领域进行更多细粒度NER的研究，以支持各种实际应用[[202](#bib.bib202)]。细粒度NER中的挑战包括NE类型的显著增加以及允许一个命名实体具有多个NE类型所带来的复杂性。这要求重新审视常见的NER方法，其中实体边界和类型同时被检测，例如，通过使用B-
    I- E- S-（实体类型）和O作为解码标签。值得考虑将命名实体边界检测定义为一个专门的任务，以检测NE边界而忽略NE类型。边界检测和NE类型分类的解耦使得边界检测可以在不同领域中共享常见和稳健的解决方案，同时也为NE类型分类提供了专门的领域特定方法。正确的实体边界还有效地缓解了实体链接到知识库中的错误传播。已有一些研究[[203](#bib.bib203),
    [95](#bib.bib95)]将实体边界检测视为NER中的一个中间步骤（即子任务）。据我们所知，尚无现有工作单独关注实体边界检测以提供稳健的识别器。我们期待未来在这一研究方向上的突破。
- en: Joint NER and Entity Linking. Entity linking (EL) [[204](#bib.bib204)], also
    referred to as named entity normalization or disambiguation, aims at assigning
    a unique identity to entities mentioned in text with reference to a knowledge
    base, e.g., Wikipedia in general domain and the Unified Medical Language System
    (UMLS) in biomedical domain. Most existing works individually solve NER and EL
    as two separate tasks in a pipeline setting. We consider that the semantics carried
    by the successfully linked entities (e.g., through the related entities in the
    knowledge base) are significantly enriched [[66](#bib.bib66), [205](#bib.bib205)].
    That is, linked entities contributes to the successful detection of entity boundaries
    and correct classification of entity types. It is worth exploring approaches for
    jointly performing NER and EL, or even entity boundary detection, entity type
    classification, and entity linking, so that each subtask benefits from the partial
    output by other subtasks, and alleviate error propagations that are unavoidable
    in pipeline settings.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 联合命名实体识别（NER）和实体链接（EL）。实体链接（EL）[[204](#bib.bib204)]，也称为命名实体标准化或消歧义，旨在通过参考知识库（如通用领域的维基百科或生物医学领域的统一医学语言系统（UMLS））为文本中提到的实体分配唯一身份。大多数现有工作在管道设置中将NER和EL作为两个独立的任务来解决。我们认为，通过成功链接的实体（例如，通过知识库中的相关实体）所承载的语义显著丰富[[66](#bib.bib66),
    [205](#bib.bib205)]。也就是说，链接的实体有助于成功检测实体边界和正确分类实体类型。值得探索联合执行NER和EL，甚至是实体边界检测、实体类型分类和实体链接的方法，以便每个子任务都能从其他子任务的部分输出中获益，并缓解管道设置中不可避免的错误传播。
- en: DL-based NER on Informal Text with Auxiliary Resource. As discussed in Section [5.1](#S5.SS1
    "5.1 Challenges ‣ 5 Challenges and Future Directions ‣ A Survey on Deep Learning
    for Named Entity Recognition"), performance of DL-based NER on informal text or
    user-generated content remains low. This calls for more research in this area.
    In particular, we note that the performance of NER benefits significantly from
    the availability of auxiliary resources [[206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208)], e.g., a dictionary of location names in user language. While
    Table [III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition") does not provide strong evidence of involving gazetteer
    as additional features leads to performance increase to NER in general domain,
    we consider auxiliary resources are often necessary to better understand user-generated
    content. The question is how to obtain matching auxiliary resources for a NER
    task on user-generated content or domain-specific text, and how to effectively
    incorporate the auxiliary resources in DL-based NER.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的命名实体识别（NER）在非正式文本中的应用及辅助资源。正如在第[5.1节](#S5.SS1 "5.1 Challenges ‣ 5 Challenges
    and Future Directions ‣ A Survey on Deep Learning for Named Entity Recognition")中讨论的那样，基于深度学习的NER在非正式文本或用户生成内容上的表现仍然较低。这需要在这一领域进行更多研究。特别是，我们注意到NER的表现从辅助资源的可用性中显著受益[[206](#bib.bib206),
    [207](#bib.bib207), [208](#bib.bib208)]，例如用户语言中的地点名称词典。虽然表[III](#S3.T3 "TABLE
    III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep
    Learning Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition")没有提供强有力的证据表明涉及地名词典作为额外特征会在一般领域中提高NER的表现，但我们认为辅助资源通常是更好理解用户生成内容所必需的。问题在于如何获得适用于用户生成内容或特定领域文本的匹配辅助资源，以及如何有效地将这些辅助资源融入基于深度学习的NER中。
- en: Scalability of DL-based NER. Making neural NER models more scalable is still
    a challenge. Moreover, there is still a need for solutions on optimizing exponential
    growth of parameters when the size of data grows [[209](#bib.bib209)]. Some DL-based
    NER models have achieved good performance with the cost of massive computing power.
    For example, the ELMo representation represents each word with a $3\times 1024$-dimensional
    vector, and the model was trained for 5 weeks on 32 GPUs [[107](#bib.bib107)].
    Google BERT representations were trained on 64 cloud TPUs. However, end users
    are not able to fine-tune these models if they have no access to powerful computing
    resources. Developing approaches to balancing model complexity and scalability
    will be a promising direction. On the other hand, model compression and pruning
    techniques are also options to reduce the space and computation time required
    for model learning.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的NER的可扩展性。使神经NER模型更具可扩展性仍然是一个挑战。此外，当数据规模增长时，仍然需要解决参数指数增长的优化问题[[209](#bib.bib209)]。一些基于深度学习的NER模型在消耗大量计算资源的情况下取得了良好的表现。例如，ELMo表示将每个词表示为一个$3\times
    1024$维的向量，该模型在32个GPU上训练了5周[[107](#bib.bib107)]。Google BERT表示在64个云TPU上进行了训练。然而，如果最终用户没有强大的计算资源，他们无法对这些模型进行微调。开发平衡模型复杂性和可扩展性的方法将是一个有前途的方向。另一方面，模型压缩和剪枝技术也是减少模型学习所需空间和计算时间的选项。
- en: 'Deep Transfer Learning for NER. Many entity-focused applications resort to
    off-the-shelf NER systems to recognize named entities. However, model trained
    on one dataset may not work well on other texts due to the differences in characteristics
    of languages as well as the differences in annotations. Although there are some
    studies of applying deep transfer learning to NER (see Section [4.2](#S4.SS2 "4.2
    Deep Transfer Learning for NER ‣ 4 Applied Deep Learning for NER ‣ A Survey on
    Deep Learning for Named Entity Recognition")), this problem has not been fully
    explored. More future efforts should be dedicated on how to effectively transfer
    knowledge from one domain to another by exploring the following research problems:
    (a) developing a robust recognizer, which is able to work well across different
    domains; (b) exploring zero-shot, one-shot and few-shot learning in NER tasks;
    (c) providing solutions to address domain mismatch, and label mismatch in cross-domain
    settings.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习在命名实体识别中的应用。许多以实体为重点的应用依赖于现成的命名实体识别系统来识别命名实体。然而，基于一个数据集训练的模型在其他文本上可能效果不佳，原因在于语言特征的差异以及标注的差异。虽然有一些研究探讨了将深度迁移学习应用于命名实体识别的情况（参见第[4.2](#S4.SS2
    "4.2 Deep Transfer Learning for NER ‣ 4 Applied Deep Learning for NER ‣ A Survey
    on Deep Learning for Named Entity Recognition")节），但这个问题尚未被充分探索。未来更多的努力应集中在如何通过探讨以下研究问题有效地将知识从一个领域迁移到另一个领域：（a）开发一种鲁棒的识别器，能够在不同领域中良好工作；（b）探索在命名实体识别任务中的零-shot、one-shot和few-shot学习；（c）提供解决跨领域设置中的领域不匹配和标签不匹配问题的方案。
- en: 'An Easy-to-use Toolkit for DL-based NER. Recently, Röder et al. [[210](#bib.bib210)]
    developed GERBIL, which provides researchers, end users and developers with easy-to-use
    interfaces for benchmarking entity annotation tools with the aim of ensuring repeatable
    and archiveable experiments. However, it does not involve recent DL-based techniques.
    Ott [[211](#bib.bib211)] presented FAIRSEQ, a fast, extensible toolkit for sequence
    modeling, especially for machine translation and text stigmatization. Dernoncourt
    et al. [[212](#bib.bib212)] implemented a framework, named NeuroNER, which only
    relies on a variant of recurrent neural network. In recent years, many deep learning
    frameworks (e.g., TensorFlow, PyTorch, and Keras) have been designed to offer
    building blocks for designing, training and validating deep neural networks, through
    a high level programming interface.³³3[https://developer.nvidia.com/deep-learning-frameworks](https://developer.nvidia.com/deep-learning-frameworks)
    In order to re-implement the architectures in Table [III](#S3.T3 "TABLE III ‣
    3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder Architectures ‣ 3 Deep Learning
    Techniques for NER ‣ A Survey on Deep Learning for Named Entity Recognition"),
    developers may write codes from scratch with existing deep learning frameworks.
    We envision that an easy-to-use NER toolkit can guide developers to complete it
    with some standardized modules: data-processing, input representation, context
    encoder, tag decoder, and effectiveness measure. We believe that experts and non-experts
    can both benefit from such toolkits.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 易用的基于深度学习的命名实体识别工具包。最近，Röder等人[[210](#bib.bib210)]开发了GERBIL，为研究人员、最终用户和开发人员提供了易于使用的接口，用于基准测试实体标注工具，以确保实验的可重复性和可归档性。然而，它未涉及最新的基于深度学习的技术。Ott
    [[211](#bib.bib211)]提出了FAIRSEQ，这是一个快速、可扩展的序列建模工具包，特别适用于机器翻译和文本污名化。Dernoncourt等人[[212](#bib.bib212)]实现了一个名为NeuroNER的框架，该框架仅依赖于递归神经网络的一个变体。近年来，许多深度学习框架（例如TensorFlow、PyTorch和Keras）被设计用来提供构建、训练和验证深度神经网络的构建模块，通过高级编程接口。³³3[https://developer.nvidia.com/deep-learning-frameworks](https://developer.nvidia.com/deep-learning-frameworks)
    为了重新实现表[III](#S3.T3 "TABLE III ‣ 3.4.2 Conditional Random Fields ‣ 3.4 Tag Decoder
    Architectures ‣ 3 Deep Learning Techniques for NER ‣ A Survey on Deep Learning
    for Named Entity Recognition")中的架构，开发人员可能需要用现有的深度学习框架从头编写代码。我们设想一个易用的命名实体识别工具包可以引导开发人员使用一些标准化模块来完成它：数据处理、输入表示、上下文编码器、标签解码器和效果测量。我们相信专家和非专家都可以从这样的工具包中受益。
- en: 6 Conclusion
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This survey aims to review recent studies on deep learning-based NER solutions
    to help new researchers building a comprehensive understanding of this field.
    We include in this survey the background of the NER research, a brief of traditional
    approaches, current state-of-the-arts, and challenges and future research directions.
    First, we consolidate available NER resources, including tagged NER corpora and
    off-the-shelf NER systems, with focus on NER in general domain and NER in English.
    We present these resources in a tabular form and provide links to them for easy
    access. Second, we introduce preliminaries such as definition of NER task, evaluation
    metrics, traditional approaches to NER, and basic concepts in deep learning. Third,
    we review the literature based on varying models of deep learning and map these
    studies according to a new taxonomy. We further survey the most representative
    methods for recent applied deep learning techniques in new problem settings and
    applications. Finally, we summarize the applications of NER and present readers
    with challenges in NER and future directions. We hope that this survey can provide
    a good reference when designing DL-based NER models.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述旨在回顾基于深度学习的命名实体识别（NER）解决方案的最新研究，以帮助新研究人员建立对该领域的全面理解。我们在此综述中包括了NER研究的背景、传统方法的简要介绍、当前的最前沿技术、挑战以及未来的研究方向。首先，我们整合了现有的NER资源，包括标记的NER语料库和现成的NER系统，重点关注一般领域的NER和英文NER。我们以表格形式呈现这些资源，并提供链接以便于访问。其次，我们介绍了预备知识，如NER任务的定义、评估指标、传统的NER方法和深度学习的基本概念。第三，我们基于不同的深度学习模型回顾文献，并根据新的分类法对这些研究进行映射。我们进一步调查了最具代表性的方法在新的问题设置和应用中的深度学习技术。最后，我们总结了NER的应用，并向读者展示了NER中的挑战和未来方向。我们希望本综述能为设计基于深度学习的NER模型提供良好的参考。
- en: References
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. Nadeau and S. Sekine, “A survey of named entity recognition and classification,”
    *Lingvist. Investig.*, vol. 30, no. 1, pp. 3–26, 2007.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. Nadeau 和 S. Sekine，“命名实体识别与分类的综述”，*Lingvist. Investig.*，第30卷，第1期，pp.
    3–26，2007年。'
- en: '[2] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE: enhanced
    language representation with informative entities,” in *ACL*, 2019, pp. 1441–1451.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, 和 Q. Liu，“ERNIE：基于信息实体的增强语言表示”，发表于*ACL*，2019年，pp.
    1441–1451。'
- en: '[3] P. Cheng and K. Erk, “Attending to entities for better text understanding,”
    *arXiv preprint arXiv:1911.04361*, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] P. Cheng 和 K. Erk，“关注实体以更好地理解文本”，*arXiv preprint arXiv:1911.04361*，2019年。'
- en: '[4] J. Guo, G. Xu, X. Cheng, and H. Li, “Named entity recognition in query,”
    in *SIGIR*, 2009, pp. 267–274.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Guo, G. Xu, X. Cheng, 和 H. Li，“查询中的命名实体识别”，发表于*SIGIR*，2009年，pp. 267–274。'
- en: '[5] D. Petkova and W. B. Croft, “Proximity-based document representation for
    named entity retrieval,” in *CIKM*, 2007, pp. 731–740.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Petkova 和 W. B. Croft，“基于接近度的文档表示用于命名实体检索”，发表于*CIKM*，2007年，pp. 731–740。'
- en: '[6] C. Aone, M. E. Okurowski, and J. Gorlinsky, “A trainable summarizer with
    knowledge acquired from robust nlp techniques,” *Adv. Autom. Text Summ.*, vol. 71,
    1999.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Aone, M. E. Okurowski, 和 J. Gorlinsky，“一个可训练的摘要生成器，结合了从强健的自然语言处理技术中获得的知识”，*Adv.
    Autom. Text Summ.*，第71卷，1999年。'
- en: '[7] D. M. Aliod, M. van Zaanen, and D. Smith, “Named entity recognition for
    question answering,” in *ALTA*, 2006, pp. 51–58.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. M. Aliod, M. van Zaanen, 和 D. Smith，“用于问答的命名实体识别”，发表于*ALTA*，2006年，pp.
    51–58。'
- en: '[8] B. Babych and A. Hartley, “Improving machine translation quality with automatic
    named entity recognition,” in *EAMT*, 2003, pp. 1–8.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] B. Babych 和 A. Hartley，“通过自动命名实体识别提高机器翻译质量”，发表于*EAMT*，2003年，pp. 1–8。'
- en: '[9] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland,
    D. S. Weld, and A. Yates, “Unsupervised named-entity extraction from the web:
    An experimental study,” *Artif. Intell.*, vol. 165, no. 1, pp. 91–134, 2005.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland,
    D. S. Weld, 和 A. Yates，“从网络中无监督提取命名实体：一项实验研究”，*Artif. Intell.*，第165卷，第1期，pp. 91–134，2005年。'
- en: '[10] R. Grishman and B. Sundheim, “Message understanding conference-6: A brief
    history,” in *COLING*, vol. 1, 1996.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Grishman 和 B. Sundheim，“信息理解会议-6：简要历史”，发表于*COLING*，第1卷，1996年。'
- en: '[11] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the conll-2003
    shared task: Language-independent named entity recognition,” in *NAACL-HLT*, 2003,
    pp. 142–147.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] E. F. Tjong Kim Sang 和 F. De Meulder，“CONLL-2003共享任务简介：语言独立的命名实体识别”，发表于*NAACL-HLT*，2003年，pp.
    142–147。'
- en: '[12] G. R. Doddington, A. Mitchell, M. A. Przybocki, L. A. Ramshaw, S. Strassel,
    and R. M. Weischedel, “The automatic content extraction (ace) program-tasks, data,
    and evaluation.” in *LREC*, vol. 2, 2004, p. 1.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] G. R. Doddington, A. Mitchell, M. A. Przybocki, L. A. Ramshaw, S. Strassel,
    和 R. M. Weischedel, “自动内容提取（ace）程序——任务、数据和评估。”发表于*LREC*，第2卷，2004年，第1页。'
- en: '[13] G. Demartini, T. Iofciu, and A. P. De Vries, “Overview of the inex 2009
    entity ranking track,” in *INEX*, 2009, pp. 254–264.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Demartini, T. Iofciu, 和 A. P. De Vries, “INEX 2009实体排名跟踪概述，”发表于*INEX*，2009年，第254–264页。'
- en: '[14] K. Balog, P. Serdyukov, and A. P. De Vries, “Overview of the trec 2010
    entity track,” in *TREC*, 2010.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K. Balog, P. Serdyukov, 和 A. P. De Vries, “TREC 2010实体跟踪概述，”发表于*TREC*，2010年。'
- en: '[15] G. Petasis, A. Cucchiarelli, P. Velardi, G. Paliouras, V. Karkaletsis,
    and C. D. Spyropoulos, “Automatic adaptation of proper noun dictionaries through
    cooperation of machine learning and probabilistic methods,” in *SIGIR*, 2000,
    pp. 128–135.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] G. Petasis, A. Cucchiarelli, P. Velardi, G. Paliouras, V. Karkaletsis,
    和 C. D. Spyropoulos, “通过机器学习和概率方法的合作自动调整专有名词词典，”发表于*SIGIR*，2000年，第128–135页。'
- en: '[16] S. A. Kripke, “Naming and necessity,” in *Semantics of natural language*.   Springer,
    1972, pp. 253–355.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. A. Kripke, “命名与必然性，”发表于*自然语言语义学*，Springer，1972年，第253–355页。'
- en: '[17] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa,
    “Natural language processing (almost) from scratch,” *J. Mach. Learn. Res.*, vol. 12,
    no. Aug, pp. 2493–2537, 2011.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, 和 P. Kuksa,
    “从零开始的自然语言处理（几乎），”*J. Mach. Learn. Res.*，第12卷，第8期，第2493–2537页，2011年。'
- en: '[18] Z. Huang, W. Xu, and K. Yu, “Bidirectional lstm-crf models for sequence
    tagging,” *arXiv preprint arXiv:1508.01991*, 2015.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Huang, W. Xu, 和 K. Yu, “双向LSTM-CRF模型用于序列标注，”*arXiv预印本 arXiv:1508.01991*，2015年。'
- en: '[19] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural
    architectures for named entity recognition,” in *NAACL*, 2016, pp. 260–270.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, 和 C. Dyer, “用于命名实体识别的神经架构，”发表于*NAACL*，2016年，第260–270页。'
- en: '[20] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional
    lstm-cnns,” *Trans. Assoc. Comput. Linguist.*, pp. 357–370, 2016.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. P. Chiu 和 E. Nichols, “基于双向LSTM-CNN的命名实体识别，”*Trans. Assoc. Comput.
    Linguist.*，第357–370页，2016年。'
- en: '[21] M. E. Peters, W. Ammar, C. Bhagavatula, and R. Power, “Semi-supervised
    sequence tagging with bidirectional language models,” in *ACL*, 2017, pp. 1756–1765.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. E. Peters, W. Ammar, C. Bhagavatula, 和 R. Power, “使用双向语言模型进行半监督序列标注，”发表于*ACL*，2017年，第1756–1765页。'
- en: '[22] M. Marrero, J. Urbano, S. Sánchez-Cuadrado, J. Morato, and J. M. Gómez-Berbís,
    “Named entity recognition: fallacies, challenges and opportunities,” *Comput.
    Stand. Interfaces*, vol. 35, no. 5, pp. 482–489, 2013.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Marrero, J. Urbano, S. Sánchez-Cuadrado, J. Morato, 和 J. M. Gómez-Berbís,
    “命名实体识别：谬误、挑战与机遇，”*计算机标准与接口*，第35卷，第5期，第482–489页，2013年。'
- en: '[23] M. L. Patawar and M. Potey, “Approaches to named entity recognition: a
    survey,” *Int. J. Innov. Res. Comput. Commun. Eng.*, vol. 3, no. 12, pp. 12 201–12 208,
    2015.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. L. Patawar 和 M. Potey, “命名实体识别的方法：综述，”*国际创新研究计算机通信工程期刊*，第3卷，第12期，第12 201–12 208页，2015年。'
- en: '[24] C. J. Saju and A. Shaja, “A survey on efficient extraction of named entities
    from new domains using big data analytics,” in *ICRTCCM*, 2017, pp. 170–175.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. J. Saju 和 A. Shaja, “利用大数据分析从新领域高效提取命名实体的综述，”发表于*ICRTCCM*，2017年，第170–175页。'
- en: '[25] X. Dai, “Recognizing complex entity mentions: A review and future directions,”
    in *ACL*, 2018, pp. 37–44.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Dai, “识别复杂实体提及：综述与未来方向，”发表于*ACL*，2018年，第37–44页。'
- en: '[26] V. Yadav and S. Bethard, “A survey on recent advances in named entity
    recognition from deep learning models,” in *COLING*, 2018, pp. 2145–2158.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] V. Yadav 和 S. Bethard, “基于深度学习模型的命名实体识别最新进展综述，”发表于*COLING*，2018年，第2145–2158页。'
- en: '[27] A. Goyal, V. Gupta, and M. Kumar, “Recent named entity recognition and
    classification techniques: A systematic review,” *Comput. Sci. Rev.*, vol. 29,
    pp. 21–43, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Goyal, V. Gupta, 和 M. Kumar, “近期命名实体识别和分类技术：系统综述，”*计算机科学评论*，第29卷，第21–43页，2018年。'
- en: '[28] R. Sharnagat, “Named entity recognition: A literature survey,” *Center
    For Indian Language Technology*, 2014.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] R. Sharnagat, “命名实体识别：文献综述，”*印度语言技术中心*，2014年。'
- en: '[29] X. Ling and D. S. Weld, “Fine-grained entity recognition.” in *AAAI*,
    vol. 12, 2012, pp. 94–100.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] X. Ling 和 D. S. Weld, “细粒度实体识别。”发表于*AAAI*，第12卷，2012年，第94–100页。'
- en: '[30] X. Ren, W. He, M. Qu, L. Huang, H. Ji, and J. Han, “Afet: Automatic fine-grained
    entity typing by hierarchical partial-label embedding,” in *EMNLP*, 2016, pp.
    1369–1378.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Ren, W. He, M. Qu, L. Huang, H. Ji, 和 J. Han, “AFET：通过层次部分标签嵌入进行自动细粒度实体类型分类，”发表于*EMNLP*，2016年，第1369–1378页。'
- en: '[31] A. Abhishek, A. Anand, and A. Awekar, “Fine-grained entity type classification
    by jointly learning representations and label embeddings,” in *EACL*, 2017, pp.
    797–807.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Abhishek, A. Anand 和 A. Awekar，“通过联合学习表示和标签嵌入进行细粒度实体类型分类，” 在 *EACL*，2017年，第797–807页。'
- en: '[32] A. Lal, A. Tomer, and C. R. Chowdary, “Sane: System for fine grained named
    entity typing on textual data,” in *WWW*, 2017, pp. 227–230.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Lal, A. Tomer 和 C. R. Chowdary，“Sane: 用于文本数据的细粒度命名实体类型系统，” 在 *WWW*，2017年，第227–230页。'
- en: '[33] L. d. Corro, A. Abujabal, R. Gemulla, and G. Weikum, “Finet: Context-aware
    fine-grained named entity typing,” in *EMNLP*, 2015, pp. 868–878.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. d. Corro, A. Abujabal, R. Gemulla 和 G. Weikum，“Finet: 上下文感知的细粒度命名实体类型识别，”
    在 *EMNLP*，2015年，第868–878页。'
- en: '[34] K. Balog, *Entity-Oriented Search*.   Springer, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] K. Balog，*Entity-Oriented Search*。 施普林格，2018年。'
- en: '[35] H. Raviv, O. Kurland, and D. Carmel, “Document retrieval using entity-based
    language models,” in *SIGIR*, 2016, pp. 65–74.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] H. Raviv, O. Kurland 和 D. Carmel，“使用基于实体的语言模型进行文档检索，” 在 *SIGIR*，2016年，第65–74页。'
- en: '[36] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna,
    “The query-flow graph: model and applications,” in *CIKM*, 2008, pp. 609–618.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis 和 S. Vigna，“查询流图：模型和应用，”
    在 *CIKM*，2008年，第609–618页。'
- en: '[37] F. Cai, M. De Rijke *et al.*, “A survey of query auto completion in information
    retrieval,” *Found. Trends® in Inf. Retr.*, vol. 10, no. 4, pp. 273–363, 2016.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] F. Cai, M. De Rijke *等*，“信息检索中查询自动补全的调查，” *Found. Trends® in Inf. Retr.*,
    第10卷，第4期，第273–363页，2016年。'
- en: '[38] Z. Bar-Yossef and N. Kraus, “Context-sensitive query auto-completion,”
    in *WWW*, 2011, pp. 107–116.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Z. Bar-Yossef 和 N. Kraus，“上下文敏感的查询自动补全，” 在 *WWW*，2011年，第107–116页。'
- en: '[39] G. Saldanha, O. Biran, K. McKeown, and A. Gliozzo, “An entity-focused
    approach to generating company descriptions,” in *ACL*, vol. 2, 2016, pp. 243–248.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] G. Saldanha, O. Biran, K. McKeown 和 A. Gliozzo，“一种以实体为中心的公司描述生成方法，” 在
    *ACL*，第2卷，2016年，第243–248页。'
- en: '[40] F. Hasibi, K. Balog, and S. E. Bratsberg, “Dynamic factual summaries for
    entity cards,” in *SIGIR*, 2017, pp. 773–782.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] F. Hasibi, K. Balog 和 S. E. Bratsberg，“动态事实摘要用于实体卡片，” 在 *SIGIR*，2017年，第773–782页。'
- en: '[41] S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang, “Conll-2012
    shared task: Modeling multilingual unrestricted coreference in ontonotes,” in
    *EMNLP*, 2012, pp. 1–40.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Pradhan, A. Moschitti, N. Xue, O. Uryupina 和 Y. Zhang，“Conll-2012共享任务：在ontonotes中建模多语言不受限的共指，”
    在 *EMNLP*，2012年，第1–40页。'
- en: '[42] C. Dogan, A. Dutra, A. Gara, A. Gemma, L. Shi, M. Sigamani, and E. Walters,
    “Fine-grained named entity recognition using elmo and wikidata,” *CoRR*, vol.
    abs/1904.10503, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] C. Dogan, A. Dutra, A. Gara, A. Gemma, L. Shi, M. Sigamani 和 E. Walters，“使用elmo和wikidata进行细粒度命名实体识别，”
    *CoRR*, 第abs/1904.10503卷，2019年。'
- en: '[43] S. Sekine and C. Nobata, “Definition, dictionaries and tagger for extended
    named entity hierarchy.” in *LREC*, 2004, pp. 1977–1980.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Sekine 和 C. Nobata，“扩展命名实体层次的定义、字典和标注器。” 在 *LREC*，2004年，第1977–1980页。'
- en: '[44] S. Zhang and N. Elhadad, “Unsupervised biomedical named entity recognition:
    Experiments with clinical and biological texts,” *J. Biomed. Inform.*, vol. 46,
    no. 6, pp. 1088–1098, 2013.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Zhang 和 N. Elhadad，“无监督生物医学命名实体识别：临床和生物文本的实验，” *J. Biomed. Inform.*,
    第46卷，第6期，第1088–1098页，2013年。'
- en: '[45] J.-H. Kim and P. C. Woodland, “A rule-based named entity recognition system
    for speech input,” in *ICSLP*, 2000.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J.-H. Kim 和 P. C. Woodland，“用于语音输入的基于规则的命名实体识别系统，” 在 *ICSLP*，2000年。'
- en: '[46] D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and J. Fluck, “Prominer:
    rule-based protein and gene entity recognition,” *BMC Bioinform.*, vol. 6, no. 1,
    p. S14, 2005.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer 和 J. Fluck，“Prominer:
    基于规则的蛋白质和基因实体识别，” *BMC Bioinform.*, 第6卷，第1期，第S14页，2005年。'
- en: '[47] A. P. Quimbaya, A. S. Múnera, R. A. G. Rivera, J. C. D. Rodríguez, O. M. M.
    Velandia, A. A. G. Peña, and C. Labbé, “Named entity recognition over electronic
    health records through a combined dictionary-based approach,” *Procedia Comput.
    Sci.*, vol. 100, pp. 55–61, 2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. P. Quimbaya, A. S. Múnera, R. A. G. Rivera, J. C. D. Rodríguez, O.
    M. M. Velandia, A. A. G. Peña, 和 C. Labbé，“通过结合字典的方法进行电子健康记录中的命名实体识别，” *Procedia
    Comput. Sci.*, 第100卷，第55–61页，2016年。'
- en: '[48] K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. Cunningham,
    and Y. Wilks, “University of sheffield: Description of the lasie-ii system as
    used for muc-7,” in *MUC-7*, 1998.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. Cunningham
    和 Y. Wilks，“谢菲尔德大学：作为muc-7使用的lasie-ii系统的描述，” 在 *MUC-7*，1998年。'
- en: '[49] G. Krupka and K. IsoQuest, “Description of the nerowl extractor system
    as used for muc-7,” in *MUC-7*, 2005, pp. 21–28.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] G. Krupka 和 K. IsoQuest，“nerowl提取系统的描述，作为muc-7使用，” 在 *MUC-7*，2005年，第21–28页。'
- en: '[50] W. J. Black, F. Rinaldi, and D. Mowatt, “Facile: Description of the ne
    system used for muc-7,” in *MUC-7*, 1998.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] W. J. Black, F. Rinaldi, 和 D. Mowatt， “Facile：描述用于muc-7的ne系统”，见于*MUC-7*，1998年。'
- en: '[51] C. Aone, L. Halverson, T. Hampton, and M. Ramos-Santacruz, “Sra: Description
    of the ie2 system used for muc-7,” in *MUC-7*, 1998.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Aone, L. Halverson, T. Hampton, 和 M. Ramos-Santacruz， “Sra：描述用于muc-7的ie2系统”，见于*MUC-7*，1998年。'
- en: '[52] D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, M. Kameyama, D. Martin,
    K. Myers, and M. Tyson, “Sri international fastus system: Muc-6 test results and
    analysis,” in *MUC-6*, 1995, pp. 237–248.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, M. Kameyama, D. Martin,
    K. Myers, 和 M. Tyson， “Sri international fastus系统：Muc-6测试结果与分析”，见于*MUC-6*，1995年，第237–248页。'
- en: '[53] A. Mikheev, M. Moens, and C. Grover, “Named entity recognition without
    gazetteers,” in *EACL*, 1999, pp. 1–8.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] A. Mikheev, M. Moens, 和 C. Grover， “无需词典的命名实体识别”，见于*EACL*，1999年，第1–8页。'
- en: '[54] M. Collins and Y. Singer, “Unsupervised models for named entity classification,”
    in *EMNLP*, 1999, pp. 100–110.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Collins 和 Y. Singer， “用于命名实体分类的无监督模型”，见于*EMNLP*，1999年，第100–110页。'
- en: '[55] D. Nadeau, P. D. Turney, and S. Matwin, “Unsupervised named-entity recognition:
    Generating gazetteers and resolving ambiguity,” in *CSCSI*, 2006, pp. 266–277.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] D. Nadeau, P. D. Turney, 和 S. Matwin， “无监督命名实体识别：生成词典和解决歧义”，见于*CSCSI*，2006年，第266–277页。'
- en: '[56] S. Sekine and E. Ranchhod, *Named entities: recognition, classification
    and use*.   John Benjamins Publishing, 2009, vol. 19.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Sekine 和 E. Ranchhod， *命名实体：识别、分类及应用*。John Benjamins Publishing，2009年，第19卷。'
- en: '[57] G. Zhou and J. Su, “Named entity recognition using an hmm-based chunk
    tagger,” in *ACL*, 2002, pp. 473–480.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] G. Zhou 和 J. Su， “使用基于HMM的分块标记器进行命名实体识别”，见于*ACL*，2002年，第473–480页。'
- en: '[58] B. Settles, “Biomedical named entity recognition using conditional random
    fields and rich feature sets,” in *ACL*, 2004, pp. 104–107.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] B. Settles， “使用条件随机场和丰富特征集的生物医学命名实体识别”，见于*ACL*，2004年，第104–107页。'
- en: '[59] W. Liao and S. Veeramachaneni, “A simple semi-supervised algorithm for
    named entity recognition,” in *NAACL-HLT*, 2009, pp. 58–65.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] W. Liao 和 S. Veeramachaneni， “一种简单的半监督命名实体识别算法”，见于*NAACL-HLT*，2009年，第58–65页。'
- en: '[60] A. Mikheev, “A knowledge-free method for capitalized word disambiguation,”
    in *ACL*, 1999, pp. 159–166.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] A. Mikheev， “一种无知识的首字母词消歧方法”，见于*ACL*，1999年，第159–166页。'
- en: '[61] J. Kazama and K. Torisawa, “Exploiting wikipedia as external knowledge
    for named entity recognition,” in *EMNLP-CoNLL*, 2007.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Kazama 和 K. Torisawa， “利用维基百科作为命名实体识别的外部知识”，见于*EMNLP-CoNLL*，2007年。'
- en: '[62] A. Toral and R. Munoz, “A proposal to automatically build and maintain
    gazetteers for named entity recognition by using wikipedia,” in *Workshop on NEW
    TEXT Wikis and blogs and other dynamic text sources*, 2006.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Toral 和 R. Munoz， “一个通过使用维基百科自动构建和维护命名实体词典的提案”，见于*Workshop on NEW TEXT
    Wikis and blogs and other dynamic text sources*，2006年。'
- en: '[63] J. Hoffart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol,
    B. Taneva, S. Thater, and G. Weikum, “Robust disambiguation of named entities
    in text,” in *EMNLP*, 2011, pp. 782–792.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Hoffart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol,
    B. Taneva, S. Thater, 和 G. Weikum， “文本中命名实体的鲁棒消歧”，见于*EMNLP*，2011年，第782–792页。'
- en: '[64] Y. Ravin and N. Wacholder, *Extracting names from natural-language text*.   IBM
    Research Report RC 2033, 1997.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Ravin 和 N. Wacholder， *从自然语言文本中提取姓名*。IBM研究报告RC 2033，1997年。'
- en: '[65] J. Zhu, V. Uren, and E. Motta, “Espotter: Adaptive named entity recognition
    for web browsing,” in *WM*.   Springer, 2005, pp. 518–529.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] J. Zhu, V. Uren, 和 E. Motta， “Espotter：用于网页浏览的自适应命名实体识别”，见于*WM*，Springer，2005年，第518–529页。'
- en: '[66] Z. Ji, A. Sun, G. Cong, and J. Han, “Joint recognition and linking of
    fine-grained locations from tweets,” in *WWW*, 2016, pp. 1271–1281.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Z. Ji, A. Sun, G. Cong, 和 J. Han， “从推文中联合识别和链接细粒度位置”，见于*WWW*，2016年，第1271–1281页。'
- en: '[67] V. Krishnan and C. D. Manning, “An effective two-stage model for exploiting
    non-local dependencies in named entity recognition,” in *ACL*, 2006, pp. 1121–1128.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] V. Krishnan 和 C. D. Manning， “一种有效的两阶段模型，用于利用命名实体识别中的非本地依赖关系”，见于*ACL*，2006年，第1121–1128页。'
- en: '[68] D. Campos, S. Matos, and J. L. Oliveira, “Biomedical named entity recognition:
    a survey of machine-learning tools,” in *Theory Appl. Adv. Text Min.*, 2012.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. Campos, S. Matos, 和 J. L. Oliveira， “生物医学命名实体识别：机器学习工具的调查”，见于*Theory
    Appl. Adv. Text Min.*，2012年。'
- en: '[69] S. R. Eddy, “Hidden markov models,” *Curr. Opin. Struct. Biol.*, vol. 6,
    no. 3, pp. 361–365, 1996.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. R. Eddy， “隐马尔可夫模型”，*Curr. Opin. Struct. Biol.*，第6卷，第3期，第361–365页，1996年。'
- en: '[70] J. R. Quinlan, “Induction of decision trees,” *Mach. Learn.*, vol. 1,
    no. 1, pp. 81–106, 1986.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. R. Quinlan， “决策树的归纳”，*Mach. Learn.*，第1卷，第1期，第81–106页，1986年。'
- en: '[71] J. N. Kapur, *Maximum-entropy models in science and engineering*.   John
    Wiley & Sons, 1989.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. N. Kapur， *科学与工程中的最大熵模型*。 John Wiley & Sons, 1989 年。'
- en: '[72] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf, “Support
    vector machines,” *IEEE Intell. Syst. Their Appl.*, vol. 13, no. 4, pp. 18–28,
    1998.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, 和 B. Scholkopf， “支持向量机，”
    *IEEE 智能系统及其应用*，第 13 卷，第 4 期，页 18–28，1998 年。'
- en: '[73] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, “Conditional random
    fields: Probabilistic models for segmenting and labeling sequence data,” pp. 282–289,
    2001.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. D. Lafferty, A. McCallum, 和 F. C. N. Pereira， “条件随机场：用于分割和标记序列数据的概率模型，”
    页 282–289，2001 年。'
- en: '[74] D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel, “Nymble: a high-performance
    learning name-finder,” in *ANLC*, 1997, pp. 194–201.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] D. M. Bikel, S. Miller, R. Schwartz, 和 R. Weischedel， “Nymble: 一个高性能的学习型名字查找器，”
    见 *ANLC*，1997 年，页 194–201。'
- en: '[75] D. M. Bikel, R. Schwartz, and R. M. Weischedel, “An algorithm that learns
    what’s in a name,” *Mach. Learn.*, vol. 34, no. 1-3, pp. 211–231, 1999.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] D. M. Bikel, R. Schwartz, 和 R. M. Weischedel， “一个学习名字内容的算法，” *机器学习*，第
    34 卷，第 1-3 期，页 211–231，1999 年。'
- en: '[76] G. Szarvas, R. Farkas, and A. Kocsor, “A multilingual named entity recognition
    system using boosting and c4\. 5 decision tree learning algorithms,” in *DS*.   Springer,
    2006, pp. 267–278.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] G. Szarvas, R. Farkas, 和 A. Kocsor， “一个使用提升和 C4.5 决策树学习算法的多语言命名实体识别系统，”
    见 *DS*。 Springer, 2006 年，页 267–278。'
- en: '[77] A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman, “Nyu: Description
    of the mene named entity system as used in muc-7,” in *MUC-7*, 1998.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A. Borthwick, J. Sterling, E. Agichtein, 和 R. Grishman， “Nyu: MUC-7 使用的
    MENE 命名实体系统描述，” 见 *MUC-7*，1998 年。'
- en: '[78] O. Bender, F. J. Och, and H. Ney, “Maximum entropy models for named entity
    recognition,” in *HLT-NAACL*, 2003, pp. 148–151.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] O. Bender, F. J. Och, 和 H. Ney， “用于命名实体识别的最大熵模型，” 见 *HLT-NAACL*，2003 年，页
    148–151。'
- en: '[79] H. L. Chieu and H. T. Ng, “Named entity recognition: a maximum entropy
    approach using global information,” in *CoNLL*, 2002, pp. 1–7.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] H. L. Chieu 和 H. T. Ng， “命名实体识别：一种使用全局信息的最大熵方法，” 见 *CoNLL*，2002 年，页 1–7。'
- en: '[80] J. R. Curran and S. Clark, “Language independent ner using a maximum entropy
    tagger,” in *HLT-NAACL*, 2003, pp. 164–167.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] J. R. Curran 和 S. Clark， “使用最大熵标记器的语言无关命名实体识别，” 见 *HLT-NAACL*，2003 年，页
    164–167。'
- en: '[81] P. McNamee and J. Mayfield, “Entity extraction without language-specific
    resources,” in *CoNLL*, 2002, pp. 1–4.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] P. McNamee 和 J. Mayfield， “无需语言特定资源的实体提取，” 见 *CoNLL*，2002 年，页 1–4。'
- en: '[82] A. McCallum and W. Li, “Early results for named entity recognition with
    conditional random fields, feature induction and web-enhanced lexicons,” in *HLT-NAACL*,
    2003, pp. 188–191.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. McCallum 和 W. Li， “使用条件随机场、特征引导和网络增强词典进行命名实体识别的初步结果，” 见 *HLT-NAACL*，2003
    年，页 188–191。'
- en: '[83] S. Liu, Y. Sun, B. Li, W. Wang, and X. Zhao, “Hamner: Headword amplified
    multi-span distantly supervised method for domain specific named entity recognition,”
    *arXiv preprint arXiv:1912.01731*, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Liu, Y. Sun, B. Li, W. Wang, 和 X. Zhao， “Hamner: 头词增强多跨度远程监督方法用于特定领域的命名实体识别，”
    *arXiv 预印本 arXiv:1912.01731*，2019 年。'
- en: '[84] A. Ritter, S. Clark, O. Etzioni *et al.*, “Named entity recognition in
    tweets: an experimental study,” in *EMNLP*, 2011, pp. 1524–1534.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. Ritter, S. Clark, O. Etzioni *等*， “推文中的命名实体识别：一项实验研究，” 见 *EMNLP*，2011
    年，页 1524–1534。'
- en: '[85] X. Liu, S. Zhang, F. Wei, and M. Zhou, “Recognizing named entities in
    tweets,” in *ACL*, 2011, pp. 359–367.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] X. Liu, S. Zhang, F. Wei, 和 M. Zhou， “在推文中识别命名实体，” 见 *ACL*，2011 年，页 359–367。'
- en: '[86] T. Rocktäschel, M. Weidlich, and U. Leser, “Chemspot: a hybrid system
    for chemical named entity recognition,” *Bioinformatics*, vol. 28, no. 12, pp.
    1633–1640, 2012.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] T. Rocktäschel, M. Weidlich, 和 U. Leser， “Chemspot: 一个用于化学命名实体识别的混合系统，”
    *生物信息学*，第 28 卷，第 12 期，页 1633–1640，2012 年。'
- en: '[87] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, p. 436, 2015.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Y. LeCun, Y. Bengio, 和 G. Hinton， “深度学习，” *自然*，第 521 卷，第 7553 期，页 436，2015
    年。'
- en: '[88] Y. Shen, H. Yun, Z. C. Lipton, Y. Kronrod, and A. Anandkumar, “Deep active
    learning for named entity recognition,” in *ICLR*, 2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Shen, H. Yun, Z. C. Lipton, Y. Kronrod, 和 A. Anandkumar， “用于命名实体识别的深度主动学习，”
    见 *ICLR*，2017 年。'
- en: '[89] T. H. Nguyen, A. Sil, G. Dinu, and R. Florian, “Toward mention detection
    robustness with recurrent neural networks,” *arXiv preprint arXiv:1602.07749*,
    2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. H. Nguyen, A. Sil, G. Dinu, 和 R. Florian， “通过递归神经网络实现提及检测的鲁棒性，” *arXiv
    预印本 arXiv:1602.07749*，2016 年。'
- en: '[90] S. Zheng, F. Wang, H. Bao, Y. Hao, P. Zhou, and B. Xu, “Joint extraction
    of entities and relations based on a novel tagging scheme,” in *ACL*, 2017, pp.
    1227–1236.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] S. Zheng, F. Wang, H. Bao, Y. Hao, P. Zhou, 和 B. Xu， “基于新型标记方案的实体和关系联合提取，”
    见 *ACL*，2017 年，页 1227–1236。'
- en: '[91] E. Strubell, P. Verga, D. Belanger, and A. McCallum, “Fast and accurate
    entity recognition with iterated dilated convolutions,” in *ACL*, 2017, pp. 2670–2680.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] E. Strubell, P. Verga, D. Belanger, 和 A. McCallum，“通过迭代膨胀卷积实现快速准确的实体识别，”发表于*ACL*，2017年，第2670–2680页。'
- en: '[92] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” in *ICLR*, 2013.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Mikolov, K. Chen, G. Corrado, 和 J. Dean，“在向量空间中高效估计词表示，”发表于*ICLR*，2013年。'
- en: '[93] J. Yang, S. Liang, and Y. Zhang, “Design challenges and misconceptions
    in neural sequence labeling,” in *COLING*, 2018, pp. 3879–3889.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Yang, S. Liang, 和 Y. Zhang，“神经序列标注中的设计挑战与误解，”发表于*COLING*，2018年，第3879–3889页。'
- en: '[94] L. Yao, H. Liu, Y. Liu, X. Li, and M. W. Anwar, “Biomedical named entity
    recognition based on deep neutral network,” *Int. J. Hybrid Inf. Technol.*, vol. 8,
    no. 8, pp. 279–288, 2015.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] L. Yao, H. Liu, Y. Liu, X. Li, 和 M. W. Anwar，“基于深度神经网络的生物医学命名实体识别，”*Int.
    J. Hybrid Inf. Technol.*，第8卷，第8期，第279–288页，2015年。'
- en: '[95] F. Zhai, S. Potdar, B. Xiang, and B. Zhou, “Neural models for sequence
    chunking.” in *AAAI*, 2017, pp. 3365–3371.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] F. Zhai, S. Potdar, B. Xiang, 和 B. Zhou，“用于序列切块的神经模型，”发表于*AAAI*，2017年，第3365–3371页。'
- en: '[96] P. Zhou, S. Zheng, J. Xu, Z. Qi, H. Bao, and B. Xu, “Joint extraction
    of multiple relations and entities by using a hybrid neural network,” in *CCL-NLP-NABD*.   Springer,
    2017, pp. 135–146.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] P. Zhou, S. Zheng, J. Xu, Z. Qi, H. Bao, 和 B. Xu，“通过使用混合神经网络联合提取多个关系和实体，”发表于*CCL-NLP-NABD*。Springer，2017年，第135–146页。'
- en: '[97] X. Ma and E. Hovy, “End-to-end sequence labeling via bi-directional lstm-cnns-crf,”
    in *ACL*, 2016, pp. 1064–1074.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Ma 和 E. Hovy，“通过双向LSTM-CNNs-CRF的端到端序列标注，”发表于*ACL*，2016年，第1064–1074页。'
- en: '[98] P.-H. Li, R.-P. Dong, Y.-S. Wang, J.-C. Chou, and W.-Y. Ma, “Leveraging
    linguistic structures for named entity recognition with bidirectional recursive
    neural networks,” in *EMNLP*, 2017, pp. 2664–2669.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P.-H. Li, R.-P. Dong, Y.-S. Wang, J.-C. Chou, 和 W.-Y. Ma，“利用语言结构进行命名实体识别的双向递归神经网络，”发表于*EMNLP*，2017年，第2664–2669页。'
- en: '[99] C. Wang, K. Cho, and D. Kiela, “Code-switched named entity recognition
    with embedding attention,” in *CALCS*, 2018, pp. 154–158.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] C. Wang, K. Cho, 和 D. Kiela，“使用嵌入注意力的代码切换命名实体识别，”发表于*CALCS*，2018年，第154–158页。'
- en: '[100] O. Kuru, O. A. Can, and D. Yuret, “Charner: Character-level named entity
    recognition,” in *COLING*, 2016, pp. 911–921.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] O. Kuru, O. A. Can, 和 D. Yuret，“Charner: 字符级命名实体识别，”发表于*COLING*，2016年，第911–921页。'
- en: '[101] Q. Tran, A. MacKinlay, and A. J. Yepes, “Named entity recognition with
    stack residual lstm and trainable bias decoding,” in *IJCNLP*, 2017, pp. 566–575.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Q. Tran, A. MacKinlay, 和 A. J. Yepes，“具有堆叠残差LSTM和可训练偏置解码的命名实体识别，”发表于*IJCNLP*，2017年，第566–575页。'
- en: '[102] J. Yang, Y. Zhang, and F. Dong, “Neural reranking for named entity recognition,”
    in *RANLP*, 2017, pp. 784–792.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Yang, Y. Zhang, 和 F. Dong，“用于命名实体识别的神经重排序，”发表于*RANLP*，2017年，第784–792页。'
- en: '[103] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” in *NAACL-HLT*, 2018,
    pp. 2227–2237.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, 和 L.
    Zettlemoyer，“深度上下文化的词表示，”发表于*NAACL-HLT*，2018年，第2227–2237页。'
- en: '[104] M. Gridach, “Character-level neural network for biomedical named entity
    recognition,” *J. Biomed. Inform.*, vol. 70, pp. 85–91, 2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Gridach，“用于生物医学命名实体识别的字符级神经网络，”*J. Biomed. Inform.*，第70卷，第85–91页，2017年。'
- en: '[105] M. Rei, G. K. Crichton, and S. Pyysalo, “Attending to characters in neural
    sequence labeling models,” in *COLING*, 2016, pp. 309–318.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] M. Rei, G. K. Crichton, 和 S. Pyysalo，“在神经序列标注模型中关注字符，”发表于*COLING*，2016年，第309–318页。'
- en: '[106] Z. Yang, R. Salakhutdinov, and W. Cohen, “Multi-task cross-lingual sequence
    tagging from scratch,” *arXiv preprint arXiv:1603.06270*, 2016.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Yang, R. Salakhutdinov, 和 W. Cohen，“从零开始的多任务跨语言序列标记，”*arXiv preprint
    arXiv:1603.06270*，2016年。'
- en: '[107] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings for
    sequence labeling,” in *COLING*, 2018, pp. 1638–1649.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Akbik, D. Blythe, 和 R. Vollgraf，“用于序列标注的上下文化字符串嵌入，”发表于*COLING*，2018年，第1638–1649页。'
- en: '[108] T. Liu, J. Yao, and C. Lin, “Towards improving neural named entity recognition
    with gazetteers,” in *ACL*, 2019, pp. 5301–5307.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] T. Liu, J. Yao, 和 C. Lin，“通过地名词典改进神经命名实体识别，”发表于*ACL*，2019年，第5301–5307页。'
- en: '[109] A. Ghaddar and P. Langlais, “Robust lexical features for improved neural
    network named-entity recognition,” in *COLING*, 2018, pp. 1896–1907.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] A. Ghaddar 和 P. Langlais，“用于改进神经网络命名实体识别的鲁棒词汇特征，”发表于*COLING*，2018年，第1896–1907页。'
- en: '[110] Z. Jie and W. Lu, “Dependency-guided lstm-crf for named entity recognition,”
    in *EMNLP*, 2018, pp. 3860–3870.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Z. Jie 和 W. Lu，“用于命名实体识别的依赖指导LSTM-CRF，”发表于*EMNLP*，2018年，第3860–3870页。'
- en: '[111] D. Lu, L. Neves, V. Carvalho, N. Zhang, and H. Ji, “Visual attention
    model for name tagging in multimodal social media,” in *ACL*, 2018, pp. 1990–1999.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] D. 鲁, L. 内维斯, V. 卡瓦利奥, N. 张, and H. 吉, “多模式社交媒体中的命名实体标记的视觉关注模型,” in *ACL*,
    2018, pp. 1990–1999.'
- en: '[112] Q. Wei, T. Chen, R. Xu, Y. He, and L. Gui, “Disease named entity recognition
    by combining conditional random fields and bidirectional recurrent neural networks,”
    *Database*, vol. 2016, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Q. 魏, T. 陈, R. 徐, Y. 何, and L. 馈, “将条件随机场和双向递归神经网络相结合的疾病命名实体识别,” *Database*,
    vol. 2016, 2016.'
- en: '[113] B. Y. Lin, F. Xu, Z. Luo, and K. Zhu, “Multi-channel bilstm-crf model
    for emerging named entity recognition in social media,” in *W-NUT*, 2017, pp.
    160–165.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] B. Y. 林, F. 徐, Z. 罗, and K. 朱, “社交媒体中新兴命名实体识别的多通道bilstm-crf模型,” in *W-NUT*,
    2017, pp. 160–165.'
- en: '[114] G. Aguilar, S. Maharjan, A. P. L. Monroy, and T. Solorio, “A multi-task
    approach for named entity recognition in social media data,” in *W-NUT*, 2017,
    pp. 148–153.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] G. 阿吉拉尔, S. 马哈尔简, A. P. L. 莫尼罗伊, and T. 索洛里奥, “社交媒体数据中命名实体识别的多任务方法,”
    in *W-NUT*, 2017, pp. 148–153.'
- en: '[115] P. Jansson and S. Liu, “Distributed representation, lda topic modelling
    and deep learning for emerging named entity recognition from social media,” in
    *W-NUT*, 2017, pp. 154–159.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] P. 詹森 and S. 刘, “社交媒体新兴命名实体识别的分布式表示、LDA主题建模和深度学习,” in *W-NUT*, 2017,
    pp. 154–159.'
- en: '[116] M. Xu, H. Jiang, and S. Watcharawittayakul, “A local detection approach
    for named entity recognition and mention detection,” in *ACL*, vol. 1, 2017, pp.
    1237–1247.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. 徐, H. 江, and S. 瓦查拉维塔育尔, “用于命名实体识别和提及检测的本地检测方法,” in *ACL*, vol. 1,
    2017, pp. 1237–1247.'
- en: '[117] S. Zhang, H. Jiang, M. Xu, J. Hou, and L. Dai, “A fixed-size encoding
    method for variable-length sequences with its application to neural network language
    models,” *arXiv preprint arXiv:1505.01504*, 2015.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] S. 张, H. 江, M. 徐, J. 侯, and L. 戴, “可变长度序列的定长编码方法及其在神经网络语言模型中的应用,” *arXiv
    preprint arXiv:1505.01504*, 2015.'
- en: '[118] S. Moon, L. Neves, and V. Carvalho, “Multimodal named entity recognition
    for short social media posts,” in *NAACL*, 2018, pp. 852–860.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. 莫恩, L. 内维斯, and V. 卡瓦利奥, “短社交媒体帖子中的多模态命名实体识别,” in *NAACL*, 2018, pp.
    852–860.'
- en: '[119] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of
    deep bidirectional transformers for language understanding,” in *NAACL-HLT*, 2019,
    pp. 4171–4186.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] J. 德文, M. 常, K. 李, and K. 图坦诺娃, “Bert: 深度双向转换器的预训练用于语言理解,” in *NAACL-HLT*,
    2019, pp. 4171–4186.'
- en: '[120] Y. Wu, M. Jiang, J. Lei, and H. Xu, “Named entity recognition in chinese
    clinical text using deep neural network,” *MEDINFO*, pp. 624–628, 2015.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Y. 吴, M. 江, J. 雷, and H. 徐, “使用深度神经网络在中文临床文本中的命名实体识别,” *MEDINFO*, pp.
    624–628, 2015.'
- en: '[121] A. Z. Gregoric, Y. Bachrach, and S. Coope, “Named entity recognition
    with parallel recurrent neural networks,” in *ACL*, vol. 2, 2018, pp. 69–74.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. Z. 格雷戈瑞克, Y. 巴赫拉赫, and S. 库普, “使用并行递归神经网络的命名实体识别,” in *ACL*, vol. 2,
    2018, pp. 69–74.'
- en: '[122] A. Katiyar and C. Cardie, “Nested named entity recognition revisited,”
    in *ACL*, vol. 1, 2018, pp. 861–871.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] A. 卡蒂亚尔 and C. 卡迪, “再审嵌套命名实体识别,” in *ACL*, vol. 1, 2018, pp. 861–871.'
- en: '[123] M. Ju, M. Miwa, and S. Ananiadou, “A neural layered model for nested
    named entity recognition,” in *NAACL-HLT*, vol. 1, 2018, pp. 1446–1459.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. 居, M. 三轮, and S. 阿纳尼阿杜, “用于嵌套命名实体识别的神经分层模型,” in *NAACL-HLT*, vol. 1,
    2018, pp. 1446–1459.'
- en: '[124] M. Rei, “Semi-supervised multitask learning for sequence labeling,” in
    *ACL*, 2017, pp. 2121–2130.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] M. 雷伊, “用于序列标注的半监督多任务学习,” in *ACL*, 2017, pp. 2121–2130.'
- en: '[125] L. Liu, X. Ren, J. Shang, J. Peng, and J. Han, “Efficient contextualized
    representation: Language model pruning for sequence labeling,” in *EMNLP*, 2018,
    pp. 1215–1225.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] L. 刘, X. 任, J. 尚, J. 彭, and J. 韩, “高效上下文表示：面向序列标注的语言模型剪枝,” in *EMNLP*,
    2018, pp. 1215–1225.'
- en: '[126] L. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and J. Han, “Empower
    sequence labeling with task-aware neural language model,” in *AAAI*, 2017, pp.
    5253–5260.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] L. 刘, J. 尚, F. 徐, X. 任, H. 贵, J. 彭, and J. 韩, “使任务感知神经语言模型具备序列标注的权力,”
    in *AAAI*, 2017, pp. 5253–5260.'
- en: '[127] C. Jia, L. Xiao, and Y. Zhang, “Cross-domain NER using cross-domain language
    modeling,” in *ACL*, 2019, pp. 2464–2474.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] C. 贾, L. 肖, and Y. 张, “使用跨领域语言建模的跨领域NER,” in *ACL*, 2019, pp. 2464–2474.'
- en: '[128] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NIPS*, 2017, pp.
    5998–6008.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. 瓦斯瓦尼, N. 沙兹尔, N. 帕马尔, J. 尤斯科雷特, L. 琼斯, A. N. 戈麦斯, Ł. 凯撒, and I. 波洛苏克辛,
    “注意力就是一切,” in *NIPS*, 2017, pp. 5998–6008.'
- en: '[129] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, and
    N. Shazeer, “Generating wikipedia by summarizing long sequences,” *arXiv preprint
    arXiv:1801.10198*, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] P. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, L. Kaiser, 和 N.
    Shazeer，“通过总结长序列生成维基百科，” *arXiv预印本 arXiv:1801.10198*，2018年。'
- en: '[130] N. Kitaev and D. Klein, “Constituency parsing with a self-attentive encoder,”
    in *ACL*, 2018, pp. 2675–2685.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] N. Kitaev 和 D. Klein，“带自注意力编码器的成分分析，”发表于 *ACL*，2018年，页码2675–2685。'
- en: '[131] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving
    language understanding by generative pre-training,” *Technical Report, OpenAI*,
    2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever，“通过生成预训练提高语言理解，”
    *技术报告，OpenAI*，2018年。'
- en: '[132] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, and M. Auli, “Cloze-driven
    pretraining of self-attention networks,” *CoRR*, vol. abs/1903.07785, 2019.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. Baevski, S. Edunov, Y. Liu, L. Zettlemoyer, 和 M. Auli，“基于填空的自注意力网络预训练，”
    *CoRR*，第abs/1903.07785卷，2019年。'
- en: '[133] C. Xia, C. Zhang, T. Yang, Y. Li, N. Du, X. Wu, W. Fan, F. Ma, and P. S.
    Yu, “Multi-grained named entity recognition,” in *ACL*, 2019, pp. 1430–1440.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] C. Xia, C. Zhang, T. Yang, Y. Li, N. Du, X. Wu, W. Fan, F. Ma, 和 P. S.
    Yu，“多粒度命名实体识别，”发表于 *ACL*，2019年，页码1430–1440。'
- en: '[134] Y. Luo, F. Xiao, and H. Zhao, “Hierarchical contextualized representation
    for named entity recognition,” *CoRR*, vol. abs/1911.02257, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Y. Luo, F. Xiao, 和 H. Zhao，“用于命名实体识别的层次上下文化表示，” *CoRR*，第abs/1911.02257卷，2019年。'
- en: '[135] Y. Liu, F. Meng, J. Zhang, J. Xu, Y. Chen, and J. Zhou, “GCDT: A global
    context enhanced deep transition architecture for sequence labeling,” in *ACL*,
    2019, pp. 2431–2441.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Y. Liu, F. Meng, J. Zhang, J. Xu, Y. Chen, 和 J. Zhou，“GCDT：一种增强全球上下文的深度转移架构用于序列标注，”发表于
    *ACL*，2019年，页码2431–2441。'
- en: '[136] Y. Jiang, C. Hu, T. Xiao, C. Zhang, and J. Zhu, “Improved differentiable
    architecture search for language modeling and named entity recognition,” in *EMNLP*,
    2019, pp. 3576–3581.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Y. Jiang, C. Hu, T. Xiao, C. Zhang, 和 J. Zhu，“改进的可微分架构搜索用于语言建模和命名实体识别，”发表于
    *EMNLP*，2019年，页码3576–3581。'
- en: '[137] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A unified MRC framework
    for named entity recognition,” *CoRR*, vol. abs/1910.11476, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, 和 J. Li，“用于命名实体识别的统一MRC框架，” *CoRR*，第abs/1910.11476卷，2019年。'
- en: '[138] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, “Dice loss for data-imbalanced
    NLP tasks,” *CoRR*, vol. abs/1911.02855, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] X. Li, X. Sun, Y. Meng, J. Liang, F. Wu, 和 J. Li，“数据不平衡NLP任务的Dice损失，”
    *CoRR*，第abs/1911.02855卷，2019年。'
- en: '[139] L. Cui and Y. Zhang, “Hierarchically-refined label attention network
    for sequence labeling,” in *EMNLP*, 2019, pp. 4113–4126.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] L. Cui 和 Y. Zhang，“用于序列标注的层次化标签注意力网络，”发表于 *EMNLP*，2019年，页码4113–4126。'
- en: '[140] S. Tomori, T. Ninomiya, and S. Mori, “Domain specific named entity recognition
    referring to the real world by deep neural networks,” in *ACL*, vol. 2, 2016,
    pp. 236–242.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] S. Tomori, T. Ninomiya, 和 S. Mori，“通过深度神经网络参考现实世界的领域特定命名实体识别，”发表于 *ACL*，第2卷，2016年，页码236–242。'
- en: '[141] Y. Lin, L. Liu, H. Ji, D. Yu, and J. Han, “Reliability-aware dynamic
    feature composition for name tagging,” in *ACL*, 2019, pp. 165–174.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Lin, L. Liu, H. Ji, D. Yu, 和 J. Han，“面向命名标记的可靠性感知动态特征组合，”发表于 *ACL*，2019年，页码165–174。'
- en: '[142] J. Zhuo, Y. Cao, J. Zhu, B. Zhang, and Z. Nie, “Segment-level sequence
    modeling using gated recursive semi-markov conditional random fields,” in *ACL*,
    vol. 1, 2016, pp. 1413–1423.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Zhuo, Y. Cao, J. Zhu, B. Zhang, 和 Z. Nie，“使用门控递归半马尔可夫条件随机场的分段级序列建模，”发表于
    *ACL*，第1卷，2016年，页码1413–1423。'
- en: '[143] Z.-X. Ye and Z.-H. Ling, “Hybrid semi-markov crf for neural sequence
    labeling,” in *ACL*, 2018, pp. 235–240.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z.-X. Ye 和 Z.-H. Ling，“用于神经序列标注的混合半马尔可夫CRF，”发表于 *ACL*，2018年，页码235–240。'
- en: '[144] A. Vaswani, Y. Bisk, K. Sagae, and R. Musa, “Supertagging with lstms,”
    in *NAACL-HLT*, 2016, pp. 232–237.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] A. Vaswani, Y. Bisk, K. Sagae, 和 R. Musa，“使用LSTM的超标记，”发表于 *NAACL-HLT*，2016年，页码232–237。'
- en: '[145] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in *NIPS*,
    2015, pp. 2692–2700.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] O. Vinyals, M. Fortunato, 和 N. Jaitly，“指针网络，”发表于 *NIPS*，2015年，页码2692–2700。'
- en: '[146] J. Li, A. Sun, and S. Joty, “Segbot: A generic neural text segmentation
    model with pointer network,” in *IJCAI*, 2018, pp. 4166–4172.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] J. Li, A. Sun, 和 S. Joty，“Segbot：一种通用的神经文本分割模型与指针网络，”发表于 *IJCAI*，2018年，页码4166–4172。'
- en: '[147] Q. Guo, X. Qiu, P. Liu, Y. Shao, X. Xue, and Z. Zhang, “Star-transformer,”
    in *NAACL-HLT*, 2019, pp. 1315–1325.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Q. Guo, X. Qiu, P. Liu, Y. Shao, X. Xue, 和 Z. Zhang，“Star-transformer，”发表于
    *NAACL-HLT*，2019年，页码1315–1325。'
- en: '[148] H. Yan, B. Deng, X. Li, and X. Qiu, “Tener: Adapting transformer encoder
    for name entity recognition,” *arXiv preprint arXiv:1911.04474*, 2019.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] H. Yan, B. Deng, X. Li, 和 X. Qiu，“Tener：为命名实体识别适配的变换器编码器，” *arXiv预印本
    arXiv:1911.04474*，2019年。'
- en: '[149] Q. Wang, Y. Zhou, T. Ruan, D. Gao, Y. Xia, and P. He, “Incorporating
    dictionaries into deep neural networks for the chinese clinical named entity recognition,”
    *J. Biomed. Inform.*, vol. 92, 2019.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Q. Wang, Y. Zhou, T. Ruan, D. Gao, Y. Xia, 和 P. He，“将词典融入深度神经网络以进行中文临床命名实体识别”，*J.
    Biomed. Inform.*，第92卷，2019年。'
- en: '[150] Y. Zhang and J. Yang, “Chinese ner using lattice lstm,” in *ACL*, 2018,
    pp. 1554–1564.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Y. Zhang 和 J. Yang，“使用格子LSTM的中文命名实体识别”，见于*ACL*，2018年，第1554–1564页。'
- en: '[151] W. Wang, F. Bao, and G. Gao, “Mongolian named entity recognition with
    bidirectional recurrent neural networks,” in *ICTAI*, 2016, pp. 495–500.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] W. Wang, F. Bao, 和 G. Gao，“基于双向递归神经网络的蒙古语命名实体识别”，见于*ICTAI*，2016年，第495–500页。'
- en: '[152] J. Straková, M. Straka, and J. Hajič, “Neural networks for featureless
    named entity recognition in czech,” in *TSD*, 2016, pp. 173–181.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Straková, M. Straka, 和 J. Hajič，“用于捷克语无特征命名实体识别的神经网络”，见于*TSD*，2016年，第173–181页。'
- en: '[153] M. Gridach, “Character-aware neural networks for arabic named entity
    recognition for social media,” in *WSSANLP*, 2016, pp. 23–32.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] M. Gridach，“用于社交媒体的阿拉伯语命名实体识别的字符感知神经网络”，见于*WSSANLP*，2016年，第23–32页。'
- en: '[154] M. K. Malik, “Urdu named entity recognition and classification system
    using artificial neural network,” *ACM Trans. Asian Low-Resour. Lang. Inf. Process.*,
    vol. 17, no. 1, p. 2, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] M. K. Malik，“使用人工神经网络的乌尔都语命名实体识别与分类系统”，*ACM Trans. Asian Low-Resour.
    Lang. Inf. Process.*，第17卷，第1期，第2页，2017年。'
- en: '[155] T.-H. Pham and P. Le-Hong, “End-to-end recurrent neural network models
    for vietnamese named entity recognition: Word-level vs. character-level,” in *PACLING*,
    2017, pp. 219–232.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] T.-H. Pham 和 P. Le-Hong，“越南语命名实体识别的端到端递归神经网络模型：词级与字符级”，见于*PACLING*，2017年，第219–232页。'
- en: '[156] K. Kurniawan and S. Louvan, “Empirical evaluation of character-based
    model on neural named-entity recognition in indonesian conversational texts,”
    *arXiv preprint arXiv:1805.12291*, 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] K. Kurniawan 和 S. Louvan，“基于字符的模型在印尼语对话文本中的神经命名实体识别的实证评估”，*arXiv预印本 arXiv:1805.12291*，2018年。'
- en: '[157] K. Yano, “Neural disease named entity extraction with character-based
    bilstm+ crf in japanese medical text,” *arXiv preprint arXiv:1806.03648*, 2018.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] K. Yano，“使用基于字符的bilstm+crf在日语医学文本中进行神经疾病命名实体提取”，*arXiv预印本 arXiv:1806.03648*，2018年。'
- en: '[158] A. Bharadwaj, D. Mortensen, C. Dyer, and J. Carbonell, “Phonologically
    aware neural model for named entity recognition in low resource transfer settings,”
    in *EMNLP*, 2016, pp. 1462–1472.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] A. Bharadwaj, D. Mortensen, C. Dyer, 和 J. Carbonell，“在低资源迁移设置下的语音意识神经模型用于命名实体识别”，见于*EMNLP*，2016年，第1462–1472页。'
- en: '[159] J. Xie, Z. Yang, G. Neubig, N. A. Smith, and J. Carbonell, “Neural cross-lingual
    named entity recognition with minimal resources,” in *EMNLP*, 2018, pp. 369–379.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] J. Xie, Z. Yang, G. Neubig, N. A. Smith, 和 J. Carbonell，“使用最少资源的神经跨语言命名实体识别”，见于*EMNLP*，2018年，第369–379页。'
- en: '[160] Y. Lin, S. Yang, V. Stoyanov, and H. Ji, “A multi-lingual multi-task
    architecture for low-resource sequence labeling,” in *ACL*, 2018, pp. 799–809.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Y. Lin, S. Yang, V. Stoyanov, 和 H. Ji，“一种多语言多任务架构用于低资源序列标注”，见于*ACL*，2018年，第799–809页。'
- en: '[161] R. Caruana, “Multitask learning,” *Mach. learn.*, vol. 28, no. 1, pp.
    41–75, 1997.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] R. Caruana，“多任务学习”，*Mach. learn.*，第28卷，第1期，第41–75页，1997年。'
- en: '[162] N. Peng and M. Dredze, “Multi-task domain adaptation for sequence tagging,”
    in *RepL4NLP*, 2017, pp. 91–100.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] N. Peng 和 M. Dredze，“用于序列标注的多任务领域适应”，见于*RepL4NLP*，2017年，第91–100页。'
- en: '[163] G. Crichton, S. Pyysalo, B. Chiu, and A. Korhonen, “A neural network
    multi-task learning approach to biomedical named entity recognition,” *BMC Bioinform.*,
    vol. 18, no. 1, p. 368, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] G. Crichton, S. Pyysalo, B. Chiu, 和 A. Korhonen，“一种神经网络多任务学习方法用于生物医学命名实体识别”，*BMC
    Bioinform.*，第18卷，第1期，第368页，2017年。'
- en: '[164] X. Wang, Y. Zhang, X. Ren, Y. Zhang, M. Zitnik, J. Shang, C. Langlotz,
    and J. Han, “Cross-type biomedical named entity recognition with deep multi-task
    learning,” *arXiv preprint arXiv:1801.09851*, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] X. Wang, Y. Zhang, X. Ren, Y. Zhang, M. Zitnik, J. Shang, C. Langlotz,
    和 J. Han，“通过深度多任务学习进行跨类型生物医学命名实体识别”，*arXiv预印本 arXiv:1801.09851*，2018年。'
- en: '[165] S. J. Pan, Q. Yang *et al.*, “A survey on transfer learning,” *IEEE Trans.
    Knowl. Data Eng.*, vol. 22, no. 10, pp. 1345–1359, 2010.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] S. J. Pan, Q. Yang *et al.*，“迁移学习调查”，*IEEE Trans. Knowl. Data Eng.*，第22卷，第10期，第1345–1359页，2010年。'
- en: '[166] J. Jiang and C. Zhai, “Instance weighting for domain adaptation in nlp,”
    in *ACL*, 2007, pp. 264–271.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] J. Jiang 和 C. Zhai，“在NLP中的领域适应实例加权”，见于*ACL*，2007年，第264–271页。'
- en: '[167] D. Wu, W. S. Lee, N. Ye, and H. L. Chieu, “Domain adaptive bootstrapping
    for named entity recognition,” in *EMNLP*, 2009, pp. 1523–1532.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] D. Wu, W. S. Lee, N. Ye, 和 H. L. Chieu，“用于命名实体识别的领域自适应引导”，见于*EMNLP*，2009年，第1523–1532页。'
- en: '[168] A. Chaudhary, J. Xie, Z. Sheikh, G. Neubig, and J. G. Carbonell, “A little
    annotation does a lot of good: A study in bootstrapping low-resource named entity
    recognizers,” pp. 5163–5173, 2019.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Chaudhary, J. Xie, Z. Sheikh, G. Neubig, 和 J. G. Carbonell, “一点标注带来很大好处：关于引导低资源命名实体识别器的研究，”第5163–5173页，2019年。'
- en: '[169] S. J. Pan, Z. Toh, and J. Su, “Transfer joint embedding for cross-domain
    named entity recognition,” *ACM Trans. Inf. Syst.*, vol. 31, no. 2, p. 7, 2013.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] S. J. Pan, Z. Toh, 和 J. Su, “跨领域命名实体识别的迁移联合嵌入，”*ACM Trans. Inf. Syst.*，第31卷，第2期，第7页，2013年。'
- en: '[170] J. Y. Lee, F. Dernoncourt, and P. Szolovits, “Transfer learning for named-entity
    recognition with neural networks,” *arXiv preprint arXiv:1705.06273*, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] J. Y. Lee, F. Dernoncourt, 和 P. Szolovits, “使用神经网络进行命名实体识别的迁移学习，”*arXiv
    预印本 arXiv:1705.06273*，2017年。'
- en: '[171] B. Y. Lin and W. Lu, “Neural adaptation layers for cross-domain named
    entity recognition,” in *EMNLP*, 2018, pp. 2012–2022.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] B. Y. Lin 和 W. Lu, “跨领域命名实体识别的神经适应层，”发表于*EMNLP*，2018年，第2012–2022页。'
- en: '[172] Y. Cao, Z. Hu, T. Chua, Z. Liu, and H. Ji, “Low-resource name tagging
    learned with weakly labeled data,” in *EMNLP*, 2019, pp. 261–270.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Cao, Z. Hu, T. Chua, Z. Liu, 和 H. Ji, “利用弱标注数据学习低资源名称标注，”发表于*EMNLP*，2019年，第261–270页。'
- en: '[173] X. Huang, L. Dong, E. Boschee, and N. Peng, “Learning A unified named
    entity tagger from multiple partially annotated corpora for efficient adaptation,”
    in *CoNLL*, 2019, pp. 515–527.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] X. Huang, L. Dong, E. Boschee, 和 N. Peng, “从多个部分标注的语料库中学习统一的命名实体标注器以便高效适应，”发表于*CoNLL*，2019年，第515–527页。'
- en: '[174] L. Qu, G. Ferraro, L. Zhou, W. Hou, and T. Baldwin, “Named entity recognition
    for novel types by transfer learning,” in *EMNLP*, 2016, pp. 899–905.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] L. Qu, G. Ferraro, L. Zhou, W. Hou, 和 T. Baldwin, “通过迁移学习进行新类型的命名实体识别，”发表于*EMNLP*，2016年，第899–905页。'
- en: '[175] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for sequence
    tagging with hierarchical recurrent networks,” in *ICLR*, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Yang, R. Salakhutdinov, 和 W. W. Cohen, “使用层次递归网络进行序列标注的迁移学习，”发表于*ICLR*，2017年。'
- en: '[176] P. von Däniken and M. Cieliebak, “Transfer learning and sentence level
    features for named entity recognition on tweets,” in *W-NUT*, 2017, pp. 166–171.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] P. von Däniken 和 M. Cieliebak, “用于推特的命名实体识别的迁移学习和句子级特征，”发表于*W-NUT*，2017年，第166–171页。'
- en: '[177] H. Zhao, Y. Yang, Q. Zhang, and L. Si, “Improve neural entity recognition
    via multi-task data selection and constrained decoding,” in *NAACL-HLT*, vol. 2,
    2018, pp. 346–351.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. Zhao, Y. Yang, Q. Zhang, 和 L. Si, “通过多任务数据选择和受限解码提高神经实体识别，”发表于*NAACL-HLT*，第2卷，2018年，第346–351页。'
- en: '[178] G. Beryozkin, Y. Drori, O. Gilon, T. Hartman, and I. Szpektor, “A joint
    named-entity recognizer for heterogeneous tag-sets using a tag hierarchy,” in
    *ACL*, 2019, pp. 140–150.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] G. Beryozkin, Y. Drori, O. Gilon, T. Hartman, 和 I. Szpektor, “使用标签层次结构的联合命名实体识别器，”发表于*ACL*，2019年，第140–150页。'
- en: '[179] J. M. Giorgi and G. D. Bader, “Transfer learning for biomedical named
    entity recognition with neural networks,” *Bioinformatics*, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] J. M. Giorgi 和 G. D. Bader, “利用神经网络进行生物医学命名实体识别的迁移学习，”*Bioinformatics*，2018年。'
- en: '[180] Z. Wang, Y. Qu, L. Chen, J. Shen, W. Zhang, S. Zhang, Y. Gao, G. Gu,
    K. Chen, and Y. Yu, “Label-aware double transfer learning for cross-specialty
    medical named entity recognition,” in *NAACL-HLT*, 2018, pp. 1–15.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Z. Wang, Y. Qu, L. Chen, J. Shen, W. Zhang, S. Zhang, Y. Gao, G. Gu,
    K. Chen, 和 Y. Yu, “面向标签的双重迁移学习用于跨专业医疗命名实体识别，”发表于*NAACL-HLT*，2018年，第1–15页。'
- en: '[181] B. Settles, “Active learning,” *Synth. Lect. Artif. Intell. Mach. Learn.*,
    vol. 6, no. 1, pp. 1–114, 2012.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] B. Settles, “主动学习，”*Synth. Lect. Artif. Intell. Mach. Learn.*，第6卷，第1期，第1–114页，2012年。'
- en: '[182] D. D. Lewis and W. A. Gale, “A sequential algorithm for training text
    classifiers,” in *SIGIR*, 1994, pp. 3–12.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] D. D. Lewis 和 W. A. Gale, “训练文本分类器的序列算法，”发表于*SIGIR*，1994年，第3–12页。'
- en: '[183] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using ontonotes,”
    in *CoNLL*, 2013, pp. 143–152.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, 和 Z. Zhong, “通过ontologies进行鲁棒语言分析，”发表于*CoNLL*，2013年，第143–152页。'
- en: '[184] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning:
    A survey,” *J. Artif. Intell. Res.*, vol. 4, pp. 237–285, 1996.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] L. P. Kaelbling, M. L. Littman, 和 A. W. Moore, “强化学习：综述，”*J. Artif. Intell.
    Res.*，第4卷，第237–285页，1996年。'
- en: '[185] R. S. Sutton and A. G. Barto, *Introduction to reinforcement learning*.   MIT
    press Cambridge, 1998, vol. 135.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] R. S. Sutton 和 A. G. Barto, *强化学习导论*。MIT出版社，剑桥，1998年，第135卷。'
- en: '[186] S. C. Hoi, D. Sahoo, J. Lu, and P. Zhao, “Online learning: A comprehensive
    survey,” *arXiv preprint arXiv:1802.02871*, 2018.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] S. C. Hoi, D. Sahoo, J. Lu, 和 P. Zhao, “在线学习：综合调查，”*arXiv 预印本 arXiv:1802.02871*，2018年。'
- en: '[187] K. Narasimhan, A. Yala, and R. Barzilay, “Improving information extraction
    by acquiring external evidence with reinforcement learning,” in *EMNLP*, 2016,
    pp. 2355–2365.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] K. Narasimhan, A. Yala, 和 R. Barzilay，“通过强化学习获取外部证据以改善信息抽取”，发表于*EMNLP*，2016年，第2355–2365页。'
- en: '[188] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制”，*Nature*，第518卷，第7540期，第529页，2015年。'
- en: '[189] Y. Yang, W. Chen, Z. Li, Z. He, and M. Zhang, “Distantly supervised NER
    with partial annotation learning and reinforcement learning,” in *COLING*, 2018,
    pp. 2159–2169.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y. Yang, W. Chen, Z. Li, Z. He, 和 M. Zhang，“通过部分标注学习和强化学习进行远程监督的命名实体识别”，发表于*COLING*，2018年，第2159–2169页。'
- en: '[190] D. Lowd and C. Meek, “Adversarial learning,” in *SIGKDD*, 2005, pp. 641–647.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] D. Lowd 和 C. Meek，“对抗学习”，发表于*SIGKDD*，2005年，第641–647页。'
- en: '[191] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014, pp.
    2672–2680.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio，“生成对抗网络”，发表于*NIPS*，2014年，第2672–2680页。'
- en: '[192] L. Huang, H. Ji, and J. May, “Cross-lingual multi-level adversarial transfer
    to enhance low-resource name tagging,” in *NAACL-HLT*, 2019, pp. 3823–3833.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] L. Huang, H. Ji, 和 J. May，“跨语言多层对抗迁移以增强低资源名称标注”，发表于*NAACL-HLT*，2019年，第3823–3833页。'
- en: '[193] J. Li, D. Ye, and S. Shang, “Adversarial transfer for named entity boundary
    detection with pointer networks,” in *IJCAI*, 2019, pp. 5053–5059.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] J. Li, D. Ye, 和 S. Shang，“通过指针网络进行命名实体边界检测的对抗迁移”，发表于*IJCAI*，2019年，第5053–5059页。'
- en: '[194] P. Cao, Y. Chen, K. Liu, J. Zhao, and S. Liu, “Adversarial transfer learning
    for chinese named entity recognition with self-attention mechanism,” in *EMNLP*,
    2018, pp. 182–192.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] P. Cao, Y. Chen, K. Liu, J. Zhao, 和 S. Liu，“利用自注意力机制进行中文命名实体识别的对抗迁移学习”，发表于*EMNLP*，2018年，第182–192页。'
- en: '[195] J. T. Zhou, H. Zhang, D. Jin, H. Zhu, M. Fang, R. S. M. Goh, and K. Kwok,
    “Dual adversarial neural transfer for low-resource named entity recognition,”
    in *ACL*, 2019, pp. 3461–3471.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. T. Zhou, H. Zhang, D. Jin, H. Zhu, M. Fang, R. S. M. Goh, 和 K. Kwok，“低资源命名实体识别的双重对抗神经迁移”，发表于*ACL*，2019年，第3461–3471页。'
- en: '[196] D. Britz, “Attention and memory in deep learning and nlp,” *Online: http://www.
    wildml. com/2016/01/attention-and-memory-in-deeplearning-and-nlp*, 2016.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] D. Britz，“深度学习和自然语言处理中的注意力和记忆”，*在线： http://www.wildml.com/2016/01/attention-and-memory-in-deeplearning-and-nlp*，2016年。'
- en: '[197] A. Zukov-Gregoric, Y. Bachrach, P. Minkovsky, S. Coope, and B. Maksak,
    “Neural named entity recognition using a self-attention mechanism,” in *ICTAI*,
    2017, pp. 652–656.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. Zukov-Gregoric, Y. Bachrach, P. Minkovsky, S. Coope, 和 B. Maksak，“使用自注意力机制的神经命名实体识别”，发表于*ICTAI*，2017年，第652–656页。'
- en: '[198] G. Xu, C. Wang, and X. He, “Improving clinical named entity recognition
    with global neural attention,” in *APWeb-WAIM*, 2018, pp. 264–279.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] G. Xu, C. Wang, 和 X. He，“通过全局神经注意力改善临床命名实体识别”，发表于*APWeb-WAIM*，2018年，第264–279页。'
- en: '[199] Q. Zhang, J. Fu, X. Liu, and X. Huang, “Adaptive co-attention network
    for named entity recognition in tweets,” in *AAAI*, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Q. Zhang, J. Fu, X. Liu, 和 X. Huang，“适应性协注意力网络用于推文中的命名实体识别”，发表于*AAAI*，2018年。'
- en: '[200] L. Derczynski, E. Nichols, M. van Erp, and N. Limsopatham, “Results of
    the wnut2017 shared task on novel and emerging entity recognition,” in *W-NUT*,
    2017, pp. 140–147.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] L. Derczynski, E. Nichols, M. van Erp, 和 N. Limsopatham，“wnut2017共享任务在新颖和新兴实体识别中的结果”，发表于*W-NUT*，2017年，第140–147页。'
- en: '[201] J. Fisher and A. Vlachos, “Merge and label: A novel neural network architecture
    for nested NER,” in *ACL*, 2019, pp. 5840–5850.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] J. Fisher 和 A. Vlachos，“合并与标注：一种用于嵌套命名实体识别的新型神经网络架构”，发表于*ACL*，2019年，第5840–5850页。'
- en: '[202] D. Ye, Z. Xing, C. Y. Foo, Z. Q. Ang, J. Li, and N. Kapre, “Software-specific
    named entity recognition in software engineering social content,” in *SANER*,
    2016, pp. 90–101.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] D. Ye, Z. Xing, C. Y. Foo, Z. Q. Ang, J. Li, 和 N. Kapre，“软件工程社交内容中的软件特定命名实体识别”，发表于*SANER*，2016年，第90–101页。'
- en: '[203] I. Partalas, C. Lopez, N. Derbas, and R. Kalitvianski, “Learning to search
    for recognizing named entities in twitter,” in *W-NUT*, 2016, pp. 171–177.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] I. Partalas, C. Lopez, N. Derbas, 和 R. Kalitvianski，“学习搜索以识别Twitter中的命名实体”，发表于*W-NUT*，2016年，第171–177页。'
- en: '[204] W. Shen, J. Han, J. Wang, X. Yuan, and Z. Yang, “Shine+: A general framework
    for domain-specific entity linking with heterogeneous information networks,” *IEEE
    Trans. Knowl. Data Eng.*, vol. 30, no. 2, pp. 353–366, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] W. Shen, J. Han, J. Wang, X. Yuan 和 Z. Yang，“Shine+: 一种针对领域特定实体链接的通用框架，适用于异构信息网络，”
    *IEEE Trans. Knowl. Data Eng.*, 第30卷，第2期，第353–366页, 2018。'
- en: '[205] M. C. Phan, A. Sun, Y. Tay, J. Han, and C. Li, “Pair-linking for collective
    entity disambiguation: Two could be better than all,” *arXiv preprint arXiv:1802.01074*,
    2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. C. Phan, A. Sun, Y. Tay, J. Han 和 C. Li，“集体实体消歧中的配对链接：两个可能比所有更好，”
    *arXiv preprint arXiv:1802.01074*, 2018。'
- en: '[206] C. Li and A. Sun, “Extracting fine-grained location with temporal awareness
    in tweets: A two-stage approach,” *J. Assoc. Inf. Sci. Technol.*, vol. 68, no. 7,
    pp. 1652–1670, 2017.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] C. Li 和 A. Sun，“在推文中提取具有时间意识的细粒度位置：一种两阶段方法，” *J. Assoc. Inf. Sci. Technol.*,
    第68卷，第7期，第1652–1670页, 2017。'
- en: '[207] J. Han, A. Sun, G. Cong, W. X. Zhao, Z. Ji, and M. C. Phan, “Linking
    fine-grained locations in user comments,” *IEEE Trans. Knowl. Data Eng.*, vol. 30,
    no. 1, pp. 59–72, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] J. Han, A. Sun, G. Cong, W. X. Zhao, Z. Ji 和 M. C. Phan，“用户评论中的细粒度位置链接，”
    *IEEE Trans. Knowl. Data Eng.*, 第30卷，第1期，第59–72页, 2018。'
- en: '[208] M. C. Phan and A. Sun, “Collective named entity recognition in user comments
    via parameterized label propagation,” *J. Assoc. Inf. Sci. Technol.*, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] M. C. Phan 和 A. Sun，“通过参数化标签传播的用户评论中的集体命名实体识别，” *J. Assoc. Inf. Sci.
    Technol.*, 2019。'
- en: '[209] Z. Batmaz, A. Yurekli, A. Bilge, and C. Kaleli, “A review on deep learning
    for recommender systems: challenges and remedies,” *Artif. Intell. Rev.*, pp.
    1–37, 2018.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Z. Batmaz, A. Yurekli, A. Bilge 和 C. Kaleli，“深度学习在推荐系统中的综述：挑战与对策，” *Artif.
    Intell. Rev.*, 第1–37页, 2018。'
- en: '[210] M. Röder, R. Usbeck, and A. N. Ngomo, “GERBIL - benchmarking named entity
    recognition and linking consistently,” *Semantic Web*, vol. 9, no. 5, pp. 605–625,
    2018.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] M. Röder, R. Usbeck 和 A. N. Ngomo，“GERBIL - 一致性评估命名实体识别和链接，” *Semantic
    Web*, 第9卷，第5期，第605–625页, 2018。'
- en: '[211] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,
    and M. Auli, “fairseq: A fast, extensible toolkit for sequence modeling,” in *NAACL-HLT*,
    2019, pp. 48–53.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier 和
    M. Auli，“fairseq：一个快速、可扩展的序列建模工具包，” 见 *NAACL-HLT*, 2019，第48–53页。'
- en: '[212] F. Dernoncourt, J. Y. Lee, and P. Szolovits, “NeuroNER: an easy-to-use
    program for named-entity recognition based on neural networks,” in *EMNLP*, 2017,
    pp. 97–102.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] F. Dernoncourt, J. Y. Lee 和 P. Szolovits，“NeuroNER：一个易于使用的基于神经网络的命名实体识别程序，”
    见 *EMNLP*, 2017，第97–102页。'
