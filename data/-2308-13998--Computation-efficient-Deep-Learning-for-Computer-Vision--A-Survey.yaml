- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:37:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:37:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2308.13998] Computation-efficient Deep Learning for Computer Vision: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2308.13998] 计算高效的计算机视觉深度学习：一项调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13998](https://ar5iv.labs.arxiv.org/html/2308.13998)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2308.13998](https://ar5iv.labs.arxiv.org/html/2308.13998)
- en: 'Computation-efficient Deep Learning for Computer Vision:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算高效的计算机视觉深度学习：
- en: A Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一项调查
- en: 'Yulin Wang^(†∗) ^∗Yulin Wang and Yizeng Han contribute equally to this work.    ^§Corresponding
    author: Gao Huang. Yizeng Han^(†∗) Chaofei Wang^† Shiji Song^† Qi Tian^‡ Gao Huang^(†§)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王玉林^(†∗) ^∗王玉林和韩逸增对这项工作贡献相同。    ^§通讯作者：高辉。韩逸增^(†∗) 常飞^† 宋世基^† 田琦^‡ 高辉^(†§)
- en: ^†Department of Automation BNRist Tsinghua University     ^‡Huawei Inc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^†自动化系 BNRist 清华大学     ^‡华为公司
- en: wang-yl19@mails.tsinghua.edu.cn   gaohuang@tsinghua.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: wang-yl19@mails.tsinghua.edu.cn   gaohuang@tsinghua.edu.cn
- en: Abstract\\
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要\\
- en: 'Over the past decade, deep learning models have exhibited considerable advancements,
    reaching or even exceeding human-level performance in a range of visual perception
    tasks. This remarkable progress has sparked interest in applying deep networks
    to real-world applications, such as autonomous vehicles, mobile devices, robotics,
    and edge computing. However, the challenge remains that state-of-the-art models
    usually demand significant computational resources, leading to impractical power
    consumption, latency, or carbon emissions in real-world scenarios. This trade-off
    between effectiveness and efficiency has catalyzed the emergence of a new research
    focus: computationally efficient deep learning, which strives to achieve satisfactory
    performance while minimizing the computational cost during inference. This review
    offers an extensive analysis of this rapidly evolving field by examining four
    key areas: 1) the development of static or dynamic light-weighted backbone models
    for the efficient extraction of discriminative deep representations; 2) the specialized
    network architectures or algorithms tailored for specific computer vision tasks;
    3) the techniques employed for compressing deep learning models; and 4) the strategies
    for deploying efficient deep networks on hardware platforms. Additionally, we
    provide a systematic discussion on the critical challenges faced in this domain,
    such as network architecture design, training schemes, practical efficiency, and
    more realistic model compression approaches, as well as potential future research
    directions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，深度学习模型取得了显著进展，在一系列视觉感知任务中达到了甚至超越了人类水平的表现。这一显著进展引发了将深度网络应用于现实世界应用的兴趣，如自动驾驶车辆、移动设备、机器人和边缘计算。然而，挑战在于，最先进的模型通常需要大量计算资源，导致现实场景中功耗、延迟或碳排放不可行。效果与效率之间的这种权衡催生了一个新的研究焦点：计算高效深度学习，它努力在推理过程中实现令人满意的性能，同时最小化计算成本。这篇综述通过考察四个关键领域，对这一快速发展的领域进行了广泛的分析：1）静态或动态轻量级骨干模型的发展，用于高效提取区分性深度表征；2）针对特定计算机视觉任务的专业网络架构或算法；3）压缩深度学习模型所使用的技术；4）在硬件平台上部署高效深度网络的策略。此外，我们还系统地讨论了该领域面临的关键挑战，如网络架构设计、训练方案、实际效率和更现实的模型压缩方法，以及潜在的未来研究方向。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Over the past decade, the field of computer vision has experienced significant
    advancements in deep learning. Innovations in model architectures and learning
    algorithms [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)] have allowed deep networks to approach or even
    exceed human-level performance on benchmark competition datasets for a wide range
    of visual tasks, such as image recognition [[6](#bib.bib6), [7](#bib.bib7)], object
    detection [[8](#bib.bib8)], image segmentation [[9](#bib.bib9), [10](#bib.bib10)],
    video understanding [[11](#bib.bib11), [12](#bib.bib12)], and 3D perception [[13](#bib.bib13)].
    This considerable progress has stimulated interest in deploying deep models in
    practical applications, including self-driving cars, mobile devices, robotics,
    unmanned aerial vehicles, and internet of things devices [[14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年里，计算机视觉领域在深度学习方面取得了重大进展。模型架构和学习算法的创新[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)]使得深度网络在广泛的视觉任务上，如图像识别[[6](#bib.bib6),
    [7](#bib.bib7)]、目标检测[[8](#bib.bib8)]、图像分割[[9](#bib.bib9), [10](#bib.bib10)]、视频理解[[11](#bib.bib11),
    [12](#bib.bib12)]和3D感知[[13](#bib.bib13)]，在基准竞赛数据集上接近或甚至超越人类水平。这一显著进展激发了在实际应用中部署深度模型的兴趣，包括自动驾驶汽车、移动设备、机器人、无人机和物联网设备[[14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]。
- en: However, the demands of real-world applications are distinct from those of competitions.
    Models achieving state-of-the-art accuracy in competitions often exhibit computational
    intensity and resource requirements during inference. In contrast, computation
    is typically equivalent to practical latency, power consumption, and carbon emissions.
    Low-latency or real-time inference is crucial for ensuring security and enhancing
    user experience [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)].
    Deep learning systems must prioritize low power consumption to improve battery
    life or reduce energy costs [[21](#bib.bib21), [22](#bib.bib22), [22](#bib.bib22),
    [23](#bib.bib23)]. Minimizing carbon emissions is also essential for environmental
    considerations [[24](#bib.bib24), [25](#bib.bib25)]. Motivated by these practical
    challenges, a substantial portion of recent literature focuses on achieving a
    balance between effectiveness and computational efficiency. Ideally, deep learning
    models should yield accurate predictions while minimizing the computational cost
    during inference. This topic has given rise to numerous intriguing research questions
    and garnered significant attention from both academic and industrial sectors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实应用的需求与竞赛的需求是不同的。在竞赛中实现最先进准确度的模型往往在推理过程中表现出计算强度和资源需求。相比之下，计算通常等同于实际延迟、功耗和碳排放。低延迟或实时推理对于确保安全和提升用户体验至关重要[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]。深度学习系统必须优先考虑低功耗，以提高电池寿命或降低能源成本[[21](#bib.bib21),
    [22](#bib.bib22), [22](#bib.bib22), [23](#bib.bib23)]。最小化碳排放也是环境考量中的一个重要方面[[24](#bib.bib24),
    [25](#bib.bib25)]。受这些实际挑战的启发，近期大量文献集中于在效果与计算效率之间取得平衡。理想情况下，深度学习模型应在推理过程中提供准确的预测，同时最小化计算成本。这个话题催生了许多引人入胜的研究问题，并引起了学术界和工业界的广泛关注。
- en: 'In light of these developments, this survey presents a comprehensive and systematic
    review of the exploration towards computationally efficient deep learning. Our
    aim is to provide an overview of this rapidly evolving field, summarize recent
    advances, and identify important challenges and potential directions for future
    research. Specifically, we will discuss existing works from the perspective of
    the following five directions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些发展，本调查提供了对计算高效深度学习探索的全面和系统的回顾。我们的目标是概述这一快速发展的领域，总结近期的进展，并识别重要挑战和未来研究的潜在方向。具体而言，我们将从以下五个方向讨论现有工作：
- en: '![Refer to caption](img/879c442a36a83996a3a2d22b78821fd7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/879c442a36a83996a3a2d22b78821fd7.png)'
- en: 'Figure 1: An overview of the survey. We first review the design of backbone
    networks, which are divided into static and dynamic models. Then we discuss the
    design of task-specialized algorithms and network architectures. Finally, we summarize
    the model compression approaches and the efficient hardware deployment techniques.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：调查概述。我们首先回顾骨干网络的设计，这些网络被分为静态模型和动态模型。然后我们讨论任务专用算法和网络架构的设计。最后，我们总结模型压缩方法和高效硬件部署技术。
- en: 1) Efficient Backbone Models. Designing light-weighted backbone networks that
    effectively extract discriminative deep representations from images, videos or
    3D scenes with minimal computation by optimizing both efficient network micro-architectures
    (*e.g.*, operators, modules, and layers) [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] and improving the system-level organization of micro-architectures
    [[29](#bib.bib29), [30](#bib.bib30)]. Recent advances in neural architecture search
    (NAS) [[31](#bib.bib31), [32](#bib.bib32)] have further enabled the automatic
    design of backbones.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 高效骨干模型。设计轻量级骨干网络，以在最小计算量下有效提取图像、视频或3D场景的区分性深度表示，通过优化高效网络微架构（*例如*，操作符、模块和层）[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]，以及改进微架构的系统级组织[[29](#bib.bib29), [30](#bib.bib30)]。最近在神经网络结构搜索（NAS）[[31](#bib.bib31),
    [32](#bib.bib32)]方面的进展进一步实现了骨干网络的自动设计。
- en: 2) Dynamic Deep Networks. Developing dynamic networks is an important emerging
    research direction for improving computational efficiency. These networks break
    the limits of static computational graphs and propose adapting their structures
    or parameters to the input during inference [[33](#bib.bib33)]. For example, the
    model can selectively activate certain model components (*e.g.*, layers [[34](#bib.bib34)],
    channels [[35](#bib.bib35)], and sub-networks [[36](#bib.bib36)]) based on each
    test input or allocate less computation to less informative spatial/temporal regions
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)] of each input.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 动态深度网络。开发动态网络是提升计算效率的重要新兴研究方向。这些网络打破了静态计算图的限制，提出在推理过程中根据输入调整其结构或参数[[33](#bib.bib33)]。例如，模型可以根据每个测试输入选择性地激活某些模型组件（*例如*，层[[34](#bib.bib34)]，通道[[35](#bib.bib35)]，和子网络[[36](#bib.bib36)]），或者对每个输入的较少信息空间/时间区域分配较少的计算[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39)]。
- en: 3) Task-specialized Efficient Models. Numerous works focus on building task-specific
    heads on top of the features from light-weighted static/dynamic backbones to efficiently
    accomplish specific computer vision tasks. Examples include fast one-stage models
    for real-time object detection [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)], the efficient multi-branch architecture for semantic segmentation
    [[44](#bib.bib44)], and end-to-end instance segmentation frameworks [[45](#bib.bib45),
    [46](#bib.bib46)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 任务专用高效模型。许多工作集中于在轻量级静态/动态骨干的特征上构建任务特定的头部，以高效完成特定的计算机视觉任务。示例包括用于实时目标检测的快速一阶段模型[[40](#bib.bib40),
    [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]，用于语义分割的高效多分支架构[[44](#bib.bib44)]，以及端到端实例分割框架[[45](#bib.bib45),
    [46](#bib.bib46)]。
- en: 4) Model Compression Techniques. Orthogonal to network architecture design,
    many algorithms have been proposed to compress relatively large models with minimal
    accuracy loss. This can be achieved by pruning less important network components
    [[47](#bib.bib47), [48](#bib.bib48)], quantizing parameters [[49](#bib.bib49),
    [50](#bib.bib50)], or distilling knowledge from large models to smaller models
    of interest [[51](#bib.bib51), [52](#bib.bib52)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 模型压缩技术。与网络架构设计正交，许多算法已经被提出用于以最小的准确度损失压缩相对较大的模型。这可以通过剪枝不重要的网络组件[[47](#bib.bib47),
    [48](#bib.bib48)]，量化参数[[49](#bib.bib49), [50](#bib.bib50)]，或从大模型中提取知识到小模型中[[51](#bib.bib51),
    [52](#bib.bib52)]来实现。
- en: 5) Efficient Deployment on Hardware. To achieve high practical efficiency, it
    is necessary to consider hardware requirements when developing deep learning applications.
    Reducing latency on specific hardware devices is usually treated as an objective
    in network design [[53](#bib.bib53), [54](#bib.bib54)] or algorithm-hardware co-design
    [[55](#bib.bib55), [56](#bib.bib56)]. Additionally, several acceleration tools
    have been developed for efficient deployment of deep learning models [[57](#bib.bib57),
    [58](#bib.bib58), [59](#bib.bib59)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 硬件上的高效部署。为了实现高实际效率，开发深度学习应用时需要考虑硬件要求。减少特定硬件设备上的延迟通常被视为网络设计的目标 [[53](#bib.bib53),
    [54](#bib.bib54)] 或算法-硬件协同设计 [[55](#bib.bib55), [56](#bib.bib56)]。此外，已经开发了若干加速工具，以高效部署深度学习模型
    [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]。
- en: 'While some relevant surveys exist [[14](#bib.bib14), [60](#bib.bib60)], our
    survey is more up-to-date and comprehensive in several crucial aspects: 1) we
    systematically review model design techniques for images, videos, and 3D vision;
    2) we summarize the recent works on designing dynamic deep neural networks for
    efficient inference; and 3) we thoroughly discuss the specialized models for accomplishing
    the most common and challenging computer vision tasks, *e.g.*, object detection
    and image segmentation.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在一些相关的调查 [[14](#bib.bib14), [60](#bib.bib60)]，但我们的调查在几个关键方面更为现代和全面：1）我们系统地回顾了图像、视频和
    3D 视觉的模型设计技术；2）我们总结了最近在设计动态深度神经网络以实现高效推理方面的工作；3）我们详细讨论了用于完成最常见和最具挑战性的计算机视觉任务的专业模型，*例如*，目标检测和图像分割。
- en: 'Table 1: The “*split-transform-merge*” architecture in representative computationally
    efficient deep networks. These blocks are typically adopted as basic components
    to build models. Here “Conv” refers to a convolutional layer.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：代表性计算高效深度网络中的 “*拆分-变换-合并*” 架构。这些模块通常作为基本组件用于构建模型。这里的 “卷积” 指的是卷积层。
- en: '|  | ResNet-50 | ResNeXt | Res2Net | MobileNet V2/V3 | EfficientNet | ShuffleNet
    | ShuffleNet V2 | Vision Transformer |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | ResNet-50 | ResNeXt | Res2Net | MobileNet V2/V3 | EfficientNet | ShuffleNet
    | ShuffleNet V2 | 视觉变换器 |'
- en: '|  | [[4](#bib.bib4)] | [[61](#bib.bib61)] | [[62](#bib.bib62)] | [[27](#bib.bib27),
    [28](#bib.bib28)] | [[29](#bib.bib29)] | [[63](#bib.bib63)] | [[64](#bib.bib64)]
    | [[6](#bib.bib6), [65](#bib.bib65)] |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | [[4](#bib.bib4)] | [[61](#bib.bib61)] | [[62](#bib.bib62)] | [[27](#bib.bib27),
    [28](#bib.bib28)] | [[29](#bib.bib29)] | [[63](#bib.bib63)] | [[64](#bib.bib64)]
    | [[6](#bib.bib6), [65](#bib.bib65)] |'
- en: '| *Split* function | 1x1 Conv | 1x1 Conv | 1x1 Conv | 1x1 Conv | 1x1 Conv |
    1x1 Group Conv + Channel Shuffle | Channel Split + 1x1 Conv | Linear Projection
    (*i.e.*, 1x1 Conv) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| *拆分* 函数 | 1x1 卷积 | 1x1 卷积 | 1x1 卷积 | 1x1 卷积 | 1x1 卷积 | 1x1 分组卷积 + 通道重排 |
    通道拆分 + 1x1 卷积 | 线性投影 (*即*，1x1 卷积) |'
- en: '| *Transform* function | 3x3 Conv | 3x3 Group Conv | Cascade 3x3 Conv | 3x3
    or 5x5 Depth-wise Conv | 3x3 or 5x5 Depth-wise Conv | 3x3 Depth-wise Conv | Identity
    and 3x3 Depth-wise Conv | Multi-head Self-attention |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| *变换* 函数 | 3x3 卷积 | 3x3 分组卷积 | 级联 3x3 卷积 | 3x3 或 5x5 深度卷积 | 3x3 或 5x5 深度卷积
    | 3x3 深度卷积 | 恒等和 3x3 深度卷积 | 多头自注意力 |'
- en: '| *Merge* function | 1x1 Conv | Concat + 1x1 Conv | Concat + 1x1 Conv | Concat
    + 1x1 Conv | Concat + 1x1 Conv | 1x1 Group Conv | Concat [-0.2ex] + 1x1 Conv +
    Concat [-0.2ex] + Channel Shuffle | Concat + Linear Projection |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| *合并* 函数 | 1x1 卷积 | 拼接 + 1x1 卷积 | 拼接 + 1x1 卷积 | 拼接 + 1x1 卷积 | 拼接 + 1x1 卷积
    | 1x1 分组卷积 | 拼接 [-0.2ex] + 1x1 卷积 + 拼接 [-0.2ex] + 通道重排 | 拼接 + 线性投影 |'
- en: 'The rest of this survey is organized as follows (see Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Computation-efficient Deep Learning for Computer Vision:
    A Survey") for the overview). In Sec. [2](#S2 "2 Architecture Design of Backbone
    Networks ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")
    and [3](#S3 "3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey"), we introduce the design of efficient static and
    dynamic backbone networks, respectively. In Sec. [4](#S4 "4 Efficient Models for
    Downstream Computer Vision Tasks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey"), the methodology for designing task-specialized efficient models
    is reviewed. The techniques for compressing deep learning models are investigated
    in Sec. [5](#S5 "5 Model Compression Techniques ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey"). Efficient hardware deployment approaches are
    summarized in Sec. [6](#S6 "6 Efficient Deployment on Hardware ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey"). Lastly, we discuss existing challenges
    and future directions in Sec. [7](#S7 "7 Challenges and Future Directions ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的其余部分组织如下（请参见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey) 以了解概述）。在第 [2](#S2 "2 Architecture
    Design of Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey") 和第 [3](#S3 "3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey") 节中，我们分别介绍了高效静态和动态主干网络的设计。在第 [4](#S4
    "4 Efficient Models for Downstream Computer Vision Tasks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey") 节中，回顾了设计任务专用高效模型的方法。在第 [5](#S5 "5
    Model Compression Techniques ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey") 节中，探讨了压缩深度学习模型的技术。在第 [6](#S6 "6 Efficient Deployment on Hardware
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey") 节中，总结了高效硬件部署方法。最后，在第
    [7](#S7 "7 Challenges and Future Directions ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey") 节中，我们讨论了现有挑战和未来方向。'
- en: 2 Architecture Design of Backbone Networks
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 主干网络的架构设计
- en: Typically, deep learning models for computation vision tasks incorporate two
    components, *i.e.,* 1) a *backbone network* that extracts deep representations
    from the raw inputs (*e.g.*, images, video frames, and point clouds); and 2) a
    *task-specific head* that is designed specialized for the task of interest. The
    deep features obtained from backbone networks are fed into the head to accomplish
    the corresponding task. The outputs of backbones (*i.e.*, the inputs of the head)
    are usually assumed to have similar formats, while the outputs of the head are
    tailored for the tasks of interest.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，用于计算机视觉任务的深度学习模型包含两个组件，*即*：1) 一个 *主干网络*，用于从原始输入（*例如*，图像、视频帧和点云）中提取深度表示；2)
    一个 *任务专用头*，为感兴趣的任务专门设计。主干网络获得的深度特征被输入到头部，以完成相应的任务。主干网络的输出（*即*，头部的输入）通常假设具有类似的格式，而头部的输出则根据感兴趣的任务进行定制。
- en: 'In this section, we focus on how to design a computational-efficient general-purpose
    backbone network. Our discussions will start from processing the most fundamental
    data form, 2D images, where a light-weighted network may be obtained by either
    *manual design* (Sec. [2.1](#S2.SS1 "2.1 Efficient Models by Manual Design ‣ 2
    Architecture Design of Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey")) or *automatic searching approaches* (Sec. [2.2](#S2.SS2
    "2.2 Automatic Architecture Design ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")). Then we
    will discuss the backbones for processing *videos* (Sec. [2.3](#S2.SS3 "2.3 Efficient
    Backbones for Video Understanding ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")) and understanding
    *3D scenes* (Sec. [2.4](#S2.SS4 "2.4 Efficient Backbones for 3D Vision ‣ 2 Architecture
    Design of Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们重点讨论如何设计计算高效的通用骨干网络。我们的讨论将从处理最基本的数据形式——二维图像开始，在这种情况下，可以通过*手动设计*（第[2.1节](#S2.SS1
    "2.1 Efficient Models by Manual Design ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")）或*自动搜索方法*（第[2.2节](#S2.SS2
    "2.2 Automatic Architecture Design ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")）获得轻量化网络。接着，我们将讨论处理*视频*（第[2.3节](#S2.SS3
    "2.3 Efficient Backbones for Video Understanding ‣ 2 Architecture Design of Backbone
    Networks ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")）和理解*3D场景*（第[2.4节](#S2.SS4
    "2.4 Efficient Backbones for 3D Vision ‣ 2 Architecture Design of Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")）的骨干网络。'
- en: 2.1 Efficient Models by Manual Design
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 手动设计的高效模型
- en: 'A considerable number of efficient backbone networks are designed manually
    based on theoretical derivations, empirical observations, or heuristics. Existing
    works can be categorized into two levels according the granularity of modifying
    the network: *micro-architecture* (Sec. 2.1.1) and *macro-architecture* (Sec.
    2.1.2)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相当多的高效骨干网络是基于理论推导、经验观察或启发式方法手动设计的。现有的工作可以根据修改网络的粒度分为两个级别：*微观架构*（第2.1.1节）和*宏观架构*（第2.1.2节）。
- en: 2.1.1 Micro-architecture
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 微观架构
- en: The micro-architecture refers to the individual layers, modules, and neural
    operators of backbones. These basic components are the foundation for constructing
    deep networks. Many works seek to attain higher computational efficiency by improving
    them. Notably, these works usually serve as off-the-shelf plug-in components that
    can be employed together with other techniques.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 微观架构指的是骨干网络的各个层、模块和神经操作符。这些基本组件是构建深度网络的基础。许多工作通过改进这些组件来提高计算效率。值得注意的是，这些工作通常作为即插即用的组件，可以与其他技术一起使用。
- en: 1) Split-transform-merge Strategy. Typically, deep networks consist of multiple
    successively stacked layers with dense connection, where all the input neurons
    is connected to every output neuron. Formally, $\ell$-th layer $f^{\ell}$ with
    inputs $\mathbf{x}^{\ell-1}$ and outputs $\mathbf{x}^{\ell}$ can be expressed
    by
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 分割-变换-合并策略。通常，深度网络由多个层叠加的层组成，具有密集的连接，其中所有输入神经元都连接到每个输出神经元。形式上，第$\ell$层$f^{\ell}$的输入为$\mathbf{x}^{\ell-1}$，输出为$\mathbf{x}^{\ell}$，可以表示为
- en: '|  | $\mathbf{x}^{\ell}=f^{\ell}(\mathbf{x}^{\ell-1}).$ |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{\ell}=f^{\ell}(\mathbf{x}^{\ell-1}).$ |  | (1) |'
- en: 'However, such dense layers tend to computationally intensive. To address this
    issue, researchers have proposed to replace the dense connection with particularly
    designed topologies [[3](#bib.bib3), [1](#bib.bib1), [61](#bib.bib61)], which
    dramatically reduces the computational complexity, yet yields a competitive or
    stronger representation ability. Among existing works, one of the most popular
    designs is the *split-transform-merge* strategy, as shown in the following (as
    a fundamental component, a residual connection [[4](#bib.bib4)] is added here):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种密集层往往计算密集。为了解决这个问题，研究人员提出了用特别设计的拓扑结构来替代密集连接[[3](#bib.bib3), [1](#bib.bib1),
    [61](#bib.bib61)]，这显著减少了计算复杂度，同时保持了竞争力或更强的表示能力。在现有的研究中，最受欢迎的设计之一是*分割-变换-合并*策略，如下所示（作为基本组件，这里添加了残差连接[[4](#bib.bib4)]）：
- en: '|  | $\begin{split}&amp;\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}=f^{\ell}_{\textnormal{split}}(\mathbf{x}^{\ell-1}),\\
    \mathbf{x}^{\ell}&amp;=\mathbf{x}^{\ell-1}+f^{\ell}_{\textnormal{merge}}(f^{\ell}_{1}(\mathbf{x}^{\ell-1}_{1}),\ldots,f^{\ell}_{C}(\mathbf{x}^{\ell-1}_{C})),\end{split}$
    |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}&amp;\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}=f^{\ell}_{\textnormal{split}}(\mathbf{x}^{\ell-1}),\\
    \mathbf{x}^{\ell}&amp;=\mathbf{x}^{\ell-1}+f^{\ell}_{\textnormal{merge}}(f^{\ell}_{1}(\mathbf{x}^{\ell-1}_{1}),\ldots,f^{\ell}_{C}(\mathbf{x}^{\ell-1}_{C})),\end{split}$
    |  | (2) |'
- en: 'where $\mathbf{x}^{\ell-1}$ is *split* into $C$ embeddings $\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}$
    with lower dimensions by a cheap operator $f^{\ell}_{\textnormal{split}}(\cdot)$.
    The low-dimensional embeddings are processed by the *transform* functions $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$,
    whose input/output dimensions are the same. Notably, each of these functions corresponds
    to a dense layer, but this procedure is efficient due to the reduced input feature
    dimension. The processed embeddings are *merged* by $f^{\ell}_{\textnormal{merge}}(\cdot)$.
    In the following, we will first discuss the design of the transform and split/merge
    functions respectively, and then introduce recent improvements over the “split-transform-merge”
    paradigm. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Computation-efficient Deep
    Learning for Computer Vision: A Survey") summarizes some representative *split-transform-merge*
    architectures in popular computationally efficient deep networks.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathbf{x}^{\ell-1}$ 被*拆分*成 $C$ 个低维的嵌入 $\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}$，通过一个廉价的操作符
    $f^{\ell}_{\textnormal{split}}(\cdot)$。这些低维嵌入通过*变换*函数 $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$
    进行处理，这些函数的输入/输出维度是相同的。值得注意的是，这些函数对应于一个密集层，但由于输入特征维度的减少，这个过程是高效的。处理后的嵌入通过 $f^{\ell}_{\textnormal{merge}}(\cdot)$
    被*合并*。接下来，我们将分别讨论变换和拆分/合并函数的设计，然后介绍对“拆分-变换-合并”范式的最新改进。表 [1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")
    总结了在流行的计算高效深度网络中的一些代表性*拆分-变换-合并*架构。'
- en: '*a) “Transform” - homogeneous multi-branch architecture.* A straightforward
    choice is to let $f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$ have the same architecture,
    and differentiate each other only in the values of the learnable parameters. This
    design is named as *grouped convolution* [[1](#bib.bib1), [61](#bib.bib61)] when
    $f^{\ell}_{c}(\cdot)$ corresponds to a convolutional layer, and is adopted in
    many efficient ConvNets [[61](#bib.bib61), [66](#bib.bib66), [67](#bib.bib67)].
    IGCV [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)] further introduces
    a permutation operation to facilitate the interaction of different groups of embeddings
    (*i.e.*, $\mathbf{x}^{\ell-1}_{c},c\!=\!1,\dots,C$). In addition to convolution,
    this design is also widely adopted in the self-attention layers of vision Transformers
    (ViTs) [[6](#bib.bib6), [65](#bib.bib65)]. In ViTs, it is name as *multi-head
    self-attention*, where $f^{\ell}_{c}(\cdot)$ is the scaled dot-product attention
    [[71](#bib.bib71)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*a) “变换” - 同质多分支架构。* 一个直接的选择是让 $f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$ 具有相同的架构，仅在可学习参数的值上有所不同。当
    $f^{\ell}_{c}(\cdot)$ 对应于卷积层时，这种设计被称为*分组卷积* [[1](#bib.bib1), [61](#bib.bib61)]，并被许多高效的卷积网络
    [[61](#bib.bib61), [66](#bib.bib66), [67](#bib.bib67)] 采用。IGCV [[68](#bib.bib68),
    [69](#bib.bib69), [70](#bib.bib70)] 进一步引入了一个置换操作，以促进不同组嵌入的交互（*即*，$\mathbf{x}^{\ell-1}_{c},c\!=\!1,\dots,C$）。除了卷积，这种设计还被广泛应用于视觉Transformer（ViTs）的自注意力层
    [[6](#bib.bib6), [65](#bib.bib65)]。在ViTs中，这被称为*多头自注意力*，其中 $f^{\ell}_{c}(\cdot)$
    是缩放点积注意力 [[71](#bib.bib71)]。'
- en: In particular, the *grouped convolution* is named as *depth-wise separable convolution*
    when $C$ is equal to the channel number of $\mathbf{x}^{\ell-1}$. Here $f^{\ell}_{c}(\cdot)$
    typically corresponds to a single convolution operation, where a large kernel
    size with sufficient receptive fields can be used without dramatically increasing
    the computational cost. This highly efficient component is first proposed in MobileNet
    [[26](#bib.bib26)], and adopted adopted in a wide variety of follow-up models
    [[72](#bib.bib72), [27](#bib.bib27), [28](#bib.bib28), [73](#bib.bib73), [74](#bib.bib74),
    [63](#bib.bib63), [64](#bib.bib64), [29](#bib.bib29), [75](#bib.bib75), [76](#bib.bib76)].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，当$C$等于$\mathbf{x}^{\ell-1}$的通道数时，*分组卷积*被称为*深度可分卷积*。这里的$f^{\ell}_{c}(\cdot)$通常对应于单个卷积操作，可以使用大内核尺寸和足够的感受野而不会显著增加计算成本。这个高效的组件首次在MobileNet中提出[[26](#bib.bib26)]，并被广泛应用于各种后续模型[[72](#bib.bib72),
    [27](#bib.bib27), [28](#bib.bib28), [73](#bib.bib73), [74](#bib.bib74), [63](#bib.bib63),
    [64](#bib.bib64), [29](#bib.bib29), [75](#bib.bib75), [76](#bib.bib76)]。
- en: '*b) “Transform” - heterogenous multi-branch architecture.* Another line of
    works focus on developing nonequivalent branches, where each $f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$
    is assigned with a specialized architecture or task. For example, the Inception
    architectures [[3](#bib.bib3), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)] adopt a varying receptive fields for different branches (*e.g.*,
    by changing the convolution kernel size), aiming to aggregate the discriminative
    information at multiple levels. Recent works further extend this idea by feeding
    the outputs of $f^{\ell}_{c}(\cdot)$ to $f^{\ell}_{c+1}(\cdot)$ [[62](#bib.bib62),
    [73](#bib.bib73), [81](#bib.bib81)], and thus integrating multi-scale features
    into the outputs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*b) “变换” - 异质多分支架构。* 另一系列工作专注于开发不等效的分支，每个$f^{\ell}_{c}(\cdot),c\!=\!1,\dots,C$被分配一个专门的架构或任务。例如，Inception架构[[3](#bib.bib3),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]采用不同的感受野用于不同的分支（*例如*，通过改变卷积核尺寸），旨在聚合多个层次的判别信息。最近的工作进一步扩展了这一思路，将$f^{\ell}_{c}(\cdot)$的输出传递给$f^{\ell}_{c+1}(\cdot)$[[62](#bib.bib62),
    [73](#bib.bib73), [81](#bib.bib81)]，从而将多尺度特征整合到输出中。'
- en: '*c) “Split/merge” functions* $f^{\ell}_{\textnormal{split}}(\cdot)$ and $f^{\ell}_{\textnormal{merge}}(\cdot)$
    are designed to map the features into or back from low-dimension embeddings with
    minimal cost. Most works adopt similar architectures for these two components:
    $f^{\ell}_{\textnormal{split}}(\cdot)$ corresponds to $1\!\times\!1$ convolution,
    while $f^{\ell}_{\textnormal{merge}}(\cdot)$ is accomplished by concatenation
    or concatenation + $1\!\times\!1$ convolution. Representative examples include
    ResNeXt [[61](#bib.bib61)], MobileNets [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]
    and Inception networks [[3](#bib.bib3), [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)]. In particular, ShuffleNet [[63](#bib.bib63)] presents a more
    efficient design by combining $1\!\times\!1$ grouped convolution with channel
    shuffle.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*c) “分割/合并”* 函数$f^{\ell}_{\textnormal{split}}(\cdot)$和$f^{\ell}_{\textnormal{merge}}(\cdot)$旨在以最小的成本将特征映射到低维嵌入或从低维嵌入恢复。大多数工作采用类似的架构：$f^{\ell}_{\textnormal{split}}(\cdot)$对应于$1\!\times\!1$卷积，而$f^{\ell}_{\textnormal{merge}}(\cdot)$通过连接或连接
    + $1\!\times\!1$卷积来完成。代表性例子包括ResNeXt[[61](#bib.bib61)]，MobileNets[[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]和Inception网络[[3](#bib.bib3), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]。特别地，ShuffleNet[[63](#bib.bib63)]通过将$1\!\times\!1$分组卷积与通道重排相结合，提出了更高效的设计。'
- en: '*d) Improved paradigms over “split-transform-merge”.* More recently, some works
    start to rethink the limitations of the split-transform-merge paradigm, and find
    that more efficient deep networks can be obtained by breaking this design principle.
    For example, motivated by the success of ViTs, ConvNeXt [[74](#bib.bib74)] explicitly
    introduces an multilayer perceptron (MLP) at each layer by reverse $f^{\ell}_{\textnormal{split}}(\cdot)$
    and the depth-wise separable convolution (*i.e.*, $f^{\ell}_{c}(\cdot)$). EfficientNetV2
    [[82](#bib.bib82)] replace $f^{\ell}_{c}(\cdot)$ and $f^{\ell}_{\textnormal{split}}(\cdot)$
    with a regular dense convolutional layer at earlier layers, achieving higher practical
    efficiency on GPU devices. MobileNeXt [[83](#bib.bib83)] moves the depth-wise
    convolution layers to the two ends of the residual path to encode more expressive
    spatial information.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*d) 改进了“拆分-变换-合并”的范式。* 最近一些研究开始重新审视拆分-变换-合并范式的局限性，发现通过打破这一设计原则可以获得更高效的深度网络。例如，受到ViTs成功的启发，ConvNeXt
    [[74](#bib.bib74)] 在每一层显式引入了一个多层感知机（MLP），通过反向的 $f^{\ell}_{\textnormal{split}}(\cdot)$
    和深度可分离卷积 (*即*，$f^{\ell}_{c}(\cdot)$)。EfficientNetV2 [[82](#bib.bib82)] 用一个常规的稠密卷积层替代了
    $f^{\ell}_{c}(\cdot)$ 和 $f^{\ell}_{\textnormal{split}}(\cdot)$，在较早的层中实现了更高的GPU设备实际效率。MobileNeXt
    [[83](#bib.bib83)] 将深度卷积层移至残差路径的两端，以编码更具表现力的空间信息。'
- en: '2) Inverted Bottleneck. Bottleneck [[4](#bib.bib4)] is a widely-used efficient
    component in ConvNets. Its basic architecture can be understood on top of Eq.
    (LABEL:eq:split): the total channel number of $\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    will be reduced compared to $\mathbf{x}^{\ell-1}$ (*e.g.*, by $4\times$ in ResNet
    [[4](#bib.bib4)]). Consequently, the computationally intensive operations $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$
    are performed on the low-dimensional embeddings, and the overall cost is saved.
    The effectiveness of this bottleneck is validated in both dense layers ($C\!=\!1$)
    [[4](#bib.bib4)] and grouped convolution [[61](#bib.bib61)]. However, it may be
    sub-optimal in depth-wise separable convolution, where its low-dimensional transform
    results in information loss [[27](#bib.bib27)]. Inspired by this observation,
    MobileNetV2 [[27](#bib.bib27)] achieves an improved efficiency by proposing an
    inverted bottleneck, *i.e.*, $\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    have more dimensions than $\mathbf{x}^{\ell-1}$ (*e.g.*, $6\times$ [[27](#bib.bib27)]).
    This designed is further adopted by a number of recent works [[70](#bib.bib70),
    [83](#bib.bib83), [74](#bib.bib74)].'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 反向瓶颈。瓶颈 [[4](#bib.bib4)] 是ConvNets中广泛使用的高效组件。其基本结构可以理解为Eq. (LABEL:eq:split)之上：$\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    的总通道数将比 $\mathbf{x}^{\ell-1}$ 减少 (*例如*，在ResNet [[4](#bib.bib4)] 中减少 $4\times$)。因此，计算密集型操作
    $f^{\ell}_{1}(\cdot),\ldots,f^{\ell}_{C}(\cdot)$ 在低维嵌入上进行，从而节省了整体成本。该瓶颈的有效性在稠密层
    ($C\!=\!1$) [[4](#bib.bib4)] 和分组卷积 [[61](#bib.bib61)] 中得到了验证。然而，在深度可分离卷积中，它可能是次优的，因为其低维变换导致信息丢失
    [[27](#bib.bib27)]。受此观察的启发，MobileNetV2 [[27](#bib.bib27)] 通过提出反向瓶颈 *即*，$\{\mathbf{x}^{\ell-1}_{1},\ldots,\mathbf{x}^{\ell-1}_{C}\}$
    的维度比 $\mathbf{x}^{\ell-1}$ 更多 (*例如*，$6\times$ [[27](#bib.bib27)])，实现了改进的效率。这一设计在许多近期工作中进一步被采用
    [[70](#bib.bib70), [83](#bib.bib83), [74](#bib.bib74)]。
- en: 3) Feature Reusing. Conventionally, the successive linear connection is the
    dominant topology for network design. The inputs are fed into a layer and transformed
    to obtained the inputs of the next layer. Any feature will be utilized for only
    a single time. Although being straightforward, this design is usually sub-optimal
    from the lens of computational efficiency. An important idea for lighted-weighted
    models is to *reuse* the have-been-used features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 特征重用。传统上，连续的线性连接是网络设计的主要拓扑结构。输入被送入一层并进行变换，以获得下一层的输入。任何特征只会被利用一次。尽管这种设计直接，但从计算效率的角度来看，通常是次优的。轻量级模型的重要思想是*重用*已经使用过的特征。
- en: '*a) Inter-layer feature reusing.* A basic idea is to reuse the features from
    previous layers. The skip-layer residual connection [[84](#bib.bib84), [4](#bib.bib4)]
    adds the inputs of each layer to the outputs, contributing the effective training
    of very deep and computationally more efficient networks. A more general formulation
    is established by dense connection [[85](#bib.bib85), [5](#bib.bib5)], where all
    the previous features are fed into a next layer. CondenseNets [[86](#bib.bib86),
    [87](#bib.bib87)] extend this architecture by automatically learning the inter-layer
    connection topology. In contrast, other works like ShuffleNetV2 [[64](#bib.bib64)]
    and G-GhostNet [[88](#bib.bib88)] focus on manually designing inter-layer interaction
    mechanisms.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*a) 层间特征重用。* 一个基本的想法是重用来自前几层的特征。跳层残差连接 [[84](#bib.bib84), [4](#bib.bib4)] 将每一层的输入加到输出中，促进了非常深的网络的有效训练和计算效率。一个更一般的公式是通过密集连接
    [[85](#bib.bib85), [5](#bib.bib5)] 建立的，其中所有先前的特征被输入到下一层。CondenseNets [[86](#bib.bib86),
    [87](#bib.bib87)] 通过自动学习层间连接拓扑来扩展这一架构。相比之下，其他工作如 ShuffleNetV2 [[64](#bib.bib64)]
    和 G-GhostNet [[88](#bib.bib88)] 则专注于手动设计层间交互机制。'
- en: '*b) Intra-layer feature reusing.* The idea of feature reusing can also be leveraged
    within each network layers. For example, GhostNets [[89](#bib.bib89), [90](#bib.bib90)]
    demonstrate that there exist considerable redundancy in the outputs of each layer.
    They first obtain a small set of intrinsic output features, which are not only
    used as the inputs of the next layer, but also reused to generating other output
    features using cheap operations like linear transformations.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*b) 层内特征重用。* 特征重用的想法也可以在每个网络层内利用。例如，GhostNets [[89](#bib.bib89), [90](#bib.bib90)]
    证明了每一层的输出中存在相当大的冗余。他们首先获取一小组固有输出特征，这些特征不仅用于下一层的输入，还通过线性变换等廉价操作重用以生成其他输出特征。'
- en: 4) Feature Down-sampling. Extracting deep representations from image-based data
    typically yields feature maps, which inherently have spatial sizes (*i.e.*, height
    and weight). This property can be leveraged to reduce the computational cost of
    models *e.g.*, introducing properly configured feature down-sampling modules.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 特征下采样。从基于图像的数据中提取深度表示通常会产生特征图，这些特征图本身具有空间尺寸（*即*，高度和宽度）。这一特性可以用来减少模型的计算成本，例如，引入适当配置的特征下采样模块。
- en: '*a) Processing feature maps efficiently.* The cost of processing feature maps
    grows quadratically with respect to their height/weight. OctConv [[91](#bib.bib91)]
    finds that processing all the features with the same resolution is not an optimal
    design. They propose to process a group of features at a down-sampled scale to
    capture only the low-frequency information, while the remaining features are designed
    to recognize high-frequency patterns, and the two groups exchange information
    after each layer. Consequently, the overall computational cost is reduced. This
    idea is also effective in ViTs [[92](#bib.bib92)]. Similarly, HRNets [[93](#bib.bib93),
    [94](#bib.bib94)] and HRFormer [[95](#bib.bib95)] maintain multi-resolution features
    at each layer, aiming to efficiently extract multi-scale discriminative representations
    for various computer vision tasks in the meantime.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*a) 高效处理特征图。* 处理特征图的成本随着其高度/宽度的平方增长。OctConv [[91](#bib.bib91)] 发现处理所有特征图的相同分辨率并不是最优设计。他们建议在下采样的尺度下处理一组特征以捕捉低频信息，同时其余特征用于识别高频模式，并且两个组在每层之后交换信息。因此，整体计算成本得到降低。这个想法在
    ViTs [[92](#bib.bib92)] 中也有效。类似地，HRNets [[93](#bib.bib93), [94](#bib.bib94)] 和
    HRFormer [[95](#bib.bib95)] 在每一层维持多分辨率特征，旨在高效提取多尺度的判别性表示以应对各种计算机视觉任务。'
- en: '*b) Facilitating efficient self-attention.* Particularly, feature down-sampling
    can be embedded into self-attention operations in ViTs to improve its efficiency.
    For example, PVTs [[96](#bib.bib96), [97](#bib.bib97)] and ShuntedViT [[98](#bib.bib98)]
    propose to compute attention maps efficiently with down-sampled feature maps.
    Twins [[99](#bib.bib99)] perform self-attention on low-resolution features to
    aggregate global information efficiently.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*b) 促进高效的自注意力。* 特别地，特征下采样可以嵌入到 ViTs 的自注意力操作中，以提高其效率。例如，PVTs [[96](#bib.bib96),
    [97](#bib.bib97)] 和 ShuntedViT [[98](#bib.bib98)] 提出使用下采样特征图高效计算注意力图。Twins [[99](#bib.bib99)]
    在低分辨率特征上执行自注意力以高效汇聚全局信息。'
- en: 5) Efficient Self-attention. ViTs [[6](#bib.bib6)] have achieved remarkable
    success in the fields of computer vision. Their self-attention mechanisms enable
    adaptively aggregating information across the entire image, yielding excellent
    scalability with the growing dataset scale or model size. However, vanilla self-attention
    suffers from high computational cost. A considerable number of recent visual backbones
    focus on developing more efficient self-attention modules without sacrificing
    their performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 高效自注意力。ViTs [[6](#bib.bib6)]在计算机视觉领域取得了显著成功。它们的自注意力机制能够自适应地聚合整个图像的信息，随着数据集规模或模型大小的增长而具有出色的可扩展性。然而，普通自注意力存在计算成本高的问题。相当多的近期视觉骨干网络集中于开发更高效的自注意力模块，而不牺牲性能。
- en: '*a) Locality-inspired Self-attention.* In this direction, an important idea
    is drawn from the success of ConvNets: exploiting the locality of images, *i.e.*,
    encouraging the models to aggregate more information from adjacent spatial regions.
    Swin Transformers [[7](#bib.bib7), [100](#bib.bib100)] achieve this by performing
    self-attention only within a square windows. Some other works extending this idea
    by designing different shapes of attention windows [[101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106)]
    or introducing soft local constraints to attention maps [[107](#bib.bib107), [108](#bib.bib108)].
    An important challenge faced by these works is how to model the interaction of
    different windows effectively. Possible solutions to address this issue include
    changing window positions [[7](#bib.bib7), [100](#bib.bib100)], shuffling the
    channels [[109](#bib.bib109)], designing specialized window shapes [[101](#bib.bib101),
    [103](#bib.bib103), [105](#bib.bib105), [106](#bib.bib106)], or further introducing
    window-level global self-attention modules [[110](#bib.bib110), [111](#bib.bib111),
    [99](#bib.bib99)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*a) 受局部性启发的自注意力。* 在这一方向上，一个重要的思想源于ConvNets的成功：利用图像的局部性，*即*，鼓励模型从邻近的空间区域聚合更多信息。Swin
    Transformers [[7](#bib.bib7), [100](#bib.bib100)]通过仅在方形窗口内执行自注意力来实现这一点。一些其他工作通过设计不同形状的注意力窗口[[101](#bib.bib101),
    [102](#bib.bib102), [103](#bib.bib103), [104](#bib.bib104), [105](#bib.bib105),
    [106](#bib.bib106)]或引入对注意力图的软局部约束[[107](#bib.bib107), [108](#bib.bib108)]扩展了这一思想。这些工作面临的重要挑战是如何有效建模不同窗口之间的互动。解决这一问题的可能方案包括更改窗口位置[[7](#bib.bib7),
    [100](#bib.bib100)]、混洗通道[[109](#bib.bib109)]、设计专门的窗口形状[[101](#bib.bib101), [103](#bib.bib103),
    [105](#bib.bib105), [106](#bib.bib106)]，或进一步引入窗口级全局自注意力模块[[110](#bib.bib110),
    [111](#bib.bib111), [99](#bib.bib99)]。'
- en: '*b) SoftMax-free Self-attention.* To reduce the inherent high computation complexity
    of self-attention, another line of research proposes to replace the SoftMax function
    in self-attention with separate kernel functions, yielding linear attention [[112](#bib.bib112)].
    As representative examples, Performer [[113](#bib.bib113)] approximates SoftMax
    with orthogonal random features, while Nyströmformer [[114](#bib.bib114)] and
    SOFT [[115](#bib.bib115)] attain this goal through matrix decomposition. Castling-ViT
    [[116](#bib.bib116)] measures the spectral similarity between tokens with linear
    angular kernels. EfficientViT [[117](#bib.bib117)] further leverages depth-wise
    convolution to improve the local feature extraction ability of linear attention.
    FLatten Transformer proposes a focused linear attention module to achieve high
    expressiveness. [[118](#bib.bib118)].'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*b) 无SoftMax的自注意力。* 为了减少自注意力固有的高计算复杂度，另一项研究提出用独立的核函数替换自注意力中的SoftMax函数，从而实现线性注意力[[112](#bib.bib112)]。例如，Performer
    [[113](#bib.bib113)]通过正交随机特征来近似SoftMax，而Nyströmformer [[114](#bib.bib114)]和SOFT
    [[115](#bib.bib115)]通过矩阵分解实现这一目标。Castling-ViT [[116](#bib.bib116)]使用线性角度核来测量标记之间的谱相似性。EfficientViT
    [[117](#bib.bib117)]进一步利用深度卷积来提高线性注意力的局部特征提取能力。FLatten Transformer提出了一种集中的线性注意力模块以实现高表达性[[118](#bib.bib118)]。'
- en: 2.1.2 Macro-architecture
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 宏观架构
- en: The macro-architecture refers to the system-level methodology of organizing
    micro-architectures (*e.g.*, operators, modules and layers) and constructing the
    whole deep networks. Existing literature has revealed that, even with the same
    efficient micro-architectures, the approaches and configurations for combining
    them will significantly affect the computational efficiency of the resulting models.
    In the following, we will discuss the works and design principles relevant to
    this topic.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观架构指的是组织微观架构（*例如*，操作符、模块和层）并构建整个深度网络的系统级方法。现有文献已揭示，即使使用相同的高效微观架构，组合它们的方法和配置也会显著影响最终模型的计算效率。接下来，我们将讨论与此主题相关的工作和设计原则。
- en: 1) Marrying Convolution and Attention Modules. Convolution and self-attention
    are both important modules with their own strengths. A considerable amount of
    literature has been published to study how to combine them for a higher overall
    computational efficiency. At the per-layer level, convolution can be leveraged
    to generate the inputs of self-attention, *e.g.*, queries/keys/values [[75](#bib.bib75),
    [76](#bib.bib76)] or position embeddings [[119](#bib.bib119)]. In addition, some
    works simultaneously utilize self-attention and a convolutional layer, and fuse
    their outputs [[120](#bib.bib120), [121](#bib.bib121)], which facilitates the
    learning of local features. Another promising idea is to integrate convolution
    into the feed-forward network after the self-attention module [[122](#bib.bib122),
    [123](#bib.bib123), [76](#bib.bib76)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 结合卷积和注意力模块。卷积和自注意力都是重要的模块，各自具有优势。已有大量文献研究如何将它们结合以提高整体计算效率。在每层级别上，可以利用卷积生成自注意力的输入，如查询/键/值[[75](#bib.bib75),
    [76](#bib.bib76)]或位置嵌入[[119](#bib.bib119)]。此外，一些工作同时利用自注意力和卷积层，并融合它们的输出[[120](#bib.bib120),
    [121](#bib.bib121)]，这有助于学习局部特征。另一个有前景的想法是将卷积集成到自注意力模块之后的前馈网络中[[122](#bib.bib122),
    [123](#bib.bib123), [76](#bib.bib76)]。
- en: At the network level, many existing works focus on the placing order of self-attention
    and depth-wise convolution blocks. In particular, leveraging convolution at earlier
    layers is proven beneficial [[124](#bib.bib124), [125](#bib.bib125), [126](#bib.bib126),
    [127](#bib.bib127), [128](#bib.bib128)], which enables the efficient extraction
    of local representations. Besides, convolutional blocks are usually adopted as
    light-weighted down-sample layers [[127](#bib.bib127), [129](#bib.bib129), [103](#bib.bib103)].
    Another line of works parallelizes both a self-attention path and a convolution
    path in a single model [[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)], where the two paths
    typically interact in a layer-wise fashion.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络层面，许多现有工作关注自注意力和深度卷积块的放置顺序。特别是，利用早期层的卷积被证明是有益的[[124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128)]，这使得有效提取局部表示成为可能。此外，卷积块通常被作为轻量级下采样层[[127](#bib.bib127),
    [129](#bib.bib129), [103](#bib.bib103)]。另一类工作在单个模型中并行化自注意力路径和卷积路径[[130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135)]，这两条路径通常以逐层方式交互。
- en: '2) Depth-width Relationship. In the context of ConvNets and hierarchical ViTs,
    the backbone models consist of multiple stages with progressively reduced feature
    resolution. The layers within each stage usually have the same width, while later
    stages are wider. The stage-wise width growing rule is an important configuration,
    where it is popular to adopt an exponential growth with base two [[4](#bib.bib4),
    [5](#bib.bib5), [7](#bib.bib7)]. In contrast, RegNets [[136](#bib.bib136), [30](#bib.bib30)]
    further propose a more detailed principle: widths and depths of good networks
    can be explained by a quantized linear function.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 深度-宽度关系。在 ConvNets 和层次化 ViTs 的背景下，主干模型由多个阶段组成，每个阶段的特征分辨率逐渐降低。每个阶段中的层通常具有相同的宽度，而后期阶段则更宽。阶段性宽度增长规则是一个重要的配置，其中采用以二为底的指数增长是较为流行的做法[[4](#bib.bib4),
    [5](#bib.bib5), [7](#bib.bib7)]。相比之下，RegNets [[136](#bib.bib136), [30](#bib.bib30)]进一步提出了一个更详细的原则：良好网络的宽度和深度可以通过量化线性函数来解释。
- en: 3) Model Scaling. On top of designing a single efficient model, it is also important
    to obtain a family of models that can adapt to varying computational budgets.
    An important principle for addressing this issue is *compound scaling* [[29](#bib.bib29),
    [82](#bib.bib82)], which indicates that simultaneously increasing the depth, width
    and input resolution of a given base model will yield a family of efficient network
    architectures. Dollár *et al.* [[137](#bib.bib137)] further study how to design
    a proper model scaling rule in terms of the actual runtime. In addition, TinyNets
    [[138](#bib.bib138)] extend this idea to the shrinking of the model size.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 模型缩放。在设计单一高效模型的基础上，获得一系列能够适应不同计算预算的模型也是重要的。解决此问题的一个重要原则是*复合缩放*[[29](#bib.bib29),
    [82](#bib.bib82)]，它表明同时增加给定基础模型的深度、宽度和输入分辨率将产生一系列高效的网络架构。Dollár *等*[[137](#bib.bib137)]进一步研究了如何在实际运行时间方面设计适当的模型缩放规则。此外，TinyNets[[138](#bib.bib138)]将这一思想扩展到模型尺寸的缩小上。
- en: 2.2 Automatic Architecture Design
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自动架构设计
- en: Compared to manually designing backbones, another appealing idea is to find
    proper network architectures automatically, which is usually referred to as *neural
    architecture search (NAS)*. In recent years, a number of existing works have investigated
    this idea through the lens of computational efficiency. In the following, we will
    discuss the basic computation-aware formulation of NAS (Sec. 2.2.1) and how the
    practical speed is considered in NAS (Sec. 2.2.2).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动设计骨干网络相比，另一个吸引人的想法是自动寻找合适的网络架构，这通常被称为*神经架构搜索（NAS）*。近年来，许多现有研究通过计算效率的视角探讨了这一想法。接下来，我们将讨论NAS的基本计算感知公式（第2.2.1节）以及实际速度在NAS中的考虑（第2.2.2节）。
- en: 2.2.1 Computation-aware NAS
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 计算感知的神经架构搜索（NAS）
- en: 'Typically, NAS consists of two components: a searching space that contain a
    number of candidate architectures, and an algorithm to search for an optimal architecture.
    The computational cost for inferring the model is usually treated as a constraint,
    which is either inherently controlled by the searching space or strictly restricted
    by a pre-defined rule. The optimization objective is to maximize the validation
    accuracy.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，NAS包括两个组成部分：包含多个候选架构的搜索空间，以及一个用于搜索最佳架构的算法。推断模型的计算成本通常被视为一个约束条件，这要么是由搜索空间本身固有控制，要么是由预定义规则严格限制。优化目标是最大化验证准确率。
- en: 1) Early Works. Early NAS methods propose to formulate a discrete searching
    space [[31](#bib.bib31), [139](#bib.bib139), [140](#bib.bib140)]. The network
    is viewed as a graph with a number of nodes connected by edges, where each edge
    corresponds to an operation and one needs to find the optimal operation for each
    edge. Such a problem can be solved with discrete optimization algorithms. For
    example, by viewing the validation performance as the rewards, one can leveraged
    off-the-shelf reinforcement learning methods [[31](#bib.bib31), [139](#bib.bib139),
    [140](#bib.bib140)]. Moreover, evolutionary algorithms also achieve favorable
    performance for discrete NAS [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143)].
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 早期工作。早期的NAS方法提出了离散搜索空间[[31](#bib.bib31), [139](#bib.bib139), [140](#bib.bib140)]。网络被视为一个图，其中多个节点通过边连接，每条边对应一个操作，需要为每条边找到最佳操作。这类问题可以通过离散优化算法解决。例如，通过将验证性能视为奖励，可以利用现成的强化学习方法[[31](#bib.bib31),
    [139](#bib.bib139), [140](#bib.bib140)]。此外，进化算法在离散NAS中也取得了良好的效果[[141](#bib.bib141),
    [142](#bib.bib142), [143](#bib.bib143)]。
- en: 2) Efficient Searching Algorithms. The aforementioned NAS methods are able to
    find computationally more efficient network architectures than human design. However,
    their searching cost is a notable limitation, since their search procedure usually
    incorporates training many candidate networks from scratch to convergence to evaluate
    their validation accuracy. Motivated by this issue, a large number of works focus
    on developing low cost NAS algorithms. A basic idea in this direction is to reuse
    the previous candidates, *e.g.*, adding/deleting layers [[117](#bib.bib117), [144](#bib.bib144)]
    and paths [[145](#bib.bib145)] on top of currently found architectures or adopting
    existing architectures as network components [[146](#bib.bib146)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 高效的搜索算法。上述NAS方法能够找到比人工设计更高效的计算网络架构。然而，它们的搜索成本是一个显著的限制，因为它们的搜索过程通常涉及从头开始训练许多候选网络直到收敛，以评估其验证准确性。受到这一问题的激励，大量工作集中在开发低成本的NAS算法。这个方向的一个基本思想是重用之前的候选项，*例如*，在当前发现的架构上添加/删除层
    [[117](#bib.bib117), [144](#bib.bib144)] 和路径 [[145](#bib.bib145)]，或者将现有架构作为网络组件
    [[146](#bib.bib146)]。
- en: Driven by these preliminary explorations, ENAS [[147](#bib.bib147)] and DARTS
    [[32](#bib.bib32)] propose a parameter-sharing paradigm. They propose to construct
    a large computational graph that contains all possible connections and operations,
    such that each subgraph within it corresponds to a network architecture. The large
    graph is named as a *super-net*, while all possible candidate networks share the
    same super-net parameters. Hence, one can train the super-net, and directly sample
    architectures from it without retraining any specific candidate network. The network
    selection process is usually formulated to be differentiable and accomplished
    efficiently via gradient-based optimization methods [[148](#bib.bib148), [149](#bib.bib149),
    [150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)]. Besides, some recent
    works focus on improving this procedure by introducing progressive searching mechanisms
    [[153](#bib.bib153), [154](#bib.bib154)], introducing hyper-networks [[155](#bib.bib155),
    [156](#bib.bib156)] or training more proper super-nets for NAS [[157](#bib.bib157),
    [158](#bib.bib158)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些初步探索的推动下，ENAS [[147](#bib.bib147)] 和 DARTS [[32](#bib.bib32)] 提出了一个参数共享的范式。它们建议构建一个包含所有可能连接和操作的大型计算图，这样其中的每个子图都对应一个网络架构。这个大型图被称为*超网络*，所有可能的候选网络共享相同的超网络参数。因此，可以训练超网络，并直接从中采样架构而无需重新训练任何特定的候选网络。网络选择过程通常被形式化为可微分的，并通过基于梯度的优化方法高效完成
    [[148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152)]。此外，一些最新的工作通过引入渐进搜索机制 [[153](#bib.bib153), [154](#bib.bib154)]、引入超网络
    [[155](#bib.bib155), [156](#bib.bib156)] 或训练更合适的超网络进行NAS [[157](#bib.bib157),
    [158](#bib.bib158)] 来改善这一过程。
- en: 2.2.2 Latency-aware Neural Architecture Search
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 考虑延迟的神经架构搜索
- en: From the lens of practical efficiency, an important challenge faced by NAS is
    the inference speed on real hardware (*e.g.*, GPUs or CPUs). Since NAS usually
    leads to irregular network architectures, the obtained model with low theoretical
    computational cost may not be efficient in practice. To address this issue, recent
    NAS methods explicitly incorporate real latency into the optimization objective
    to achieve a good trade-off between real speed and accuracy [[54](#bib.bib54),
    [53](#bib.bib53), [159](#bib.bib159)]. As representative examples, MobileNetV3
    [[28](#bib.bib28)] leverages hardware-aware NAS to obtain the basic architecture,
    and modifies it manually. Once-for-all [[24](#bib.bib24)] proposes to train a
    shared general super-nets, and perform NAS on top of it conditioned on the specific
    hardwares, yielding a state-of-the-art efficiency.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际效率的角度来看，NAS面临的一个重要挑战是对真实硬件（*例如*，GPU或CPU）的推理速度。由于NAS通常会导致不规则的网络架构，所得到的模型可能在实践中效率不高，尽管其理论计算成本较低。为了解决这个问题，最近的NAS方法明确将真实延迟纳入优化目标，以在真实速度和准确性之间实现良好的权衡
    [[54](#bib.bib54), [53](#bib.bib53), [159](#bib.bib159)]。作为代表性的例子，MobileNetV3
    [[28](#bib.bib28)] 利用硬件感知NAS获得基础架构，并进行手动修改。Once-for-all [[24](#bib.bib24)] 提出了训练一个共享的通用超网络，并在其基础上根据具体硬件进行NAS，从而获得最先进的效率。
- en: 2.3 Efficient Backbones for Video Understanding
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 视频理解的高效骨干网
- en: In this subsection, we will focus on the efficient backbones for processing
    videos. Notably, videos consist of a series of frames, each of which is an image.
    In general, the aforementioned techniques for processing images are typically
    compatible with videos. Hence, here we mainly review the efficient modeling of
    the temporal relationships of video frames, including *ConvNet-based* (Sec. 2.3.1)
    and *Transformer-based* (Sec. 2.3.2) approaches.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将重点关注处理视频的高效骨干网络。值得注意的是，视频由一系列帧组成，每一帧都是一幅图像。一般来说，上述处理图像的技术通常也适用于视频。因此，这里我们主要回顾视频帧时间关系的高效建模，包括*基于卷积神经网络*（第2.3.1节）和*基于变换器*（第2.3.2节）的方法。
- en: 2.3.1 Efficient 3D ConvNets
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 高效的3D卷积神经网络
- en: The most straightforward approach to modeling temporal relationships may be
    introducing 3D convolutional layers [[160](#bib.bib160), [161](#bib.bib161)],
    such that one can directly perform convolution in the space formed by frame height,
    width, and video duration. However, 3D convolution is computationally expensive,
    and many efficient backbones have been proposed to alleviate this problem.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 建模时间关系的最直接方法可能是引入3D卷积层 [[160](#bib.bib160), [161](#bib.bib161)]，这样可以在由帧的高度、宽度和视频时长形成的空间中直接进行卷积。然而，3D卷积计算开销大，已经提出了许多高效的骨干网络来缓解这个问题。
- en: 1) Marrying 2D and 3D Convolution. A basic idea is to avoid designing a pure
    3D ConvNets, *i.e.*, most of the feature extraction process may be accomplished
    by the efficient 2D convolution, while 3D convolution is only introduced at several
    particular positions. From the lens of macro-architecture, this goal can be attained
    by sequentially mixing 2D and 3D blocks, either first using 3D and later 2D or
    first 2D and later 3D [[162](#bib.bib162), [163](#bib.bib163)]. At the micro-architecture
    level, the group-wise or depth-width 3D convolution can be integrated in to the
    *transform* module of 2D split-transform-merge architecture (Eq. (LABEL:eq:split))
    [[164](#bib.bib164), [165](#bib.bib165)].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 结合2D和3D卷积。一个基本思路是避免设计纯3D卷积神经网络，即，大部分特征提取过程可以通过高效的2D卷积来完成，而3D卷积仅在几个特定位置引入。从宏观架构的角度来看，这个目标可以通过顺序混合2D和3D块来实现，可以先使用3D，然后再使用2D，或者先使用2D，再使用3D
    [[162](#bib.bib162), [163](#bib.bib163)]。在微观架构层面，组卷积或深度宽度3D卷积可以集成到2D分裂-变换-合并架构的*变换*模块中（方程式（LABEL:eq:split））[[164](#bib.bib164),
    [165](#bib.bib165)]。
- en: '2) (2+1)D Networks. Another elegant idea is to decompose 3D convolution into
    two components: a 2D convolution that extract representation from video frames,
    and a temporal operation that only focuses on learning the temporal relationships.
    The former can directly adopt 2D neural operators, while the latter can be implemented
    using 1D temporal convolution [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)],
    adaptive 1D convolution [[169](#bib.bib169)], and MLPs [[170](#bib.bib170)].'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2) (2+1)D网络。另一个优雅的想法是将3D卷积分解为两个组件：一个从视频帧中提取表示的2D卷积，以及一个仅关注学习时间关系的时间操作。前者可以直接采用2D神经操作，而后者可以使用1D时间卷积
    [[166](#bib.bib166), [167](#bib.bib167), [168](#bib.bib168)]，自适应1D卷积 [[169](#bib.bib169)]
    和MLPs [[170](#bib.bib170)] 来实现。
- en: 3) 2D Networks. In addition to the aforementioned approaches, the models with
    only 2D convolution may also be able to model temporal relationships. This is
    typically achieved by designing zero-parameter operations. For example, subtracting
    the features of adjacent frames to extract the motion information [[171](#bib.bib171),
    [172](#bib.bib172)]. The temporal-shift-based models [[173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175)] propose to shift part of the channels of 2D features along
    the temporal dimension, performing information exchange among neighboring frames
    efficiently.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 2D网络。除了上述方法，只有2D卷积的模型也可能能够建模时间关系。这通常通过设计零参数操作来实现。例如，通过减去相邻帧的特征来提取运动信息 [[171](#bib.bib171),
    [172](#bib.bib172)]。基于时间位移的模型 [[173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175)]
    提出将2D特征的一部分通道在时间维度上进行位移，从而有效地在相邻帧之间进行信息交换。
- en: 4) Long/Short-term Separable Networks. Another important idea is modeling long/short-term
    temporal dynamics with separate network architectures. An representative work
    in this direction is SlowFast [[176](#bib.bib176)], which incorporate a lower
    temporal resolution slow pathway and a higher temporal resolution fast pathway.
    Many recent works [[172](#bib.bib172), [177](#bib.bib177)] further extend this
    idea.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 长/短期可分离网络。另一个重要的想法是通过分离的网络架构建模长/短期时间动态。在这方面，一个代表性的工作是SlowFast [[176](#bib.bib176)]，它结合了低时间分辨率的慢通道和高时间分辨率的快通道。许多近期的工作
    [[172](#bib.bib172), [177](#bib.bib177)] 进一步扩展了这个理念。
- en: 2.3.2 Transformer-based Video Backbones
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 基于Transformer的视频骨干网络
- en: Driven by the success of ViTs [[6](#bib.bib6)], a considerable number of recent
    works focus on facilitating efficient video understanding with self-attention-based
    models. In general, most of these works extend the aforementioned design ideas
    (including both image-based and video-based backbones) in the context of ViTs,
    *e.g.*, performing spatial-temporal local self-attention [[178](#bib.bib178),
    [179](#bib.bib179), [180](#bib.bib180)], combining self-attention and convolution
    [[181](#bib.bib181)], and performing 1D temporal attention in (2+1)D designs [[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 受到ViTs [[6](#bib.bib6)] 成功的驱动，许多近期的工作集中在利用基于自注意力的模型促进高效的视频理解。一般来说，这些工作大多数在ViTs的背景下扩展了前述设计理念（包括图像基础和视频基础的骨干网络），*例如*，执行空间-时间局部自注意力
    [[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)]，结合自注意力和卷积 [[181](#bib.bib181)]，以及在（2+1）D设计中执行1D时间注意力
    [[182](#bib.bib182), [183](#bib.bib183), [184](#bib.bib184), [185](#bib.bib185)]。
- en: 2.4 Efficient Backbones for 3D Vision
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 高效的3D视觉骨干网络
- en: The perception and understanding of 3D scenes is not only a key ability of human
    intelligence, but also an important task for computer vision which are ubiquitous
    in real-world applications. In this subsection, we will review the backbones designed
    for processing 3D information efficiently. In general, the works in this direction
    can be categorized by the forms of model inputs, *i.e.*, *3D point clouds* (Sec.
    2.4.1), *3D voxels* (Sec. 2.4.2) and *multi-view images* (Sec. 2.4.3).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对3D场景的感知和理解不仅是人类智能的关键能力，也是计算机视觉中的重要任务，在现实世界应用中无处不在。本小节将回顾为高效处理3D信息而设计的骨干网络。一般来说，这方面的工作可以按照模型输入的形式进行分类，*即*，*3D点云*（第2.4.1节）、*3D体素*（第2.4.2节）和*多视角图像*（第2.4.3节）。
- en: 2.4.1 Point-based Models
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 基于点的模型
- en: A fundamental type of 3D geometric data structure is the cloud of 3D points,
    where each point is represented by its three coordinates. PointNet [[186](#bib.bib186)]
    is the pioneering work that leveraging deep learning to process 3D point clouds.
    It adopts point-wise feature extraction with shared MLPs to maintain the permutation
    invariance. PointNet++ [[187](#bib.bib187)] improves PointNet by facilitating
    capturing local geometric structures. On top of them, a number of works focus
    on how to aggregating local information effectively without increasing computational
    cost significantly. Representative approaches include introducing graph neural
    networks [[188](#bib.bib188), [189](#bib.bib189)], projecting 3D points to regular
    grids to perform convolution [[190](#bib.bib190), [191](#bib.bib191), [192](#bib.bib192),
    [193](#bib.bib193)], aggregating the features of adjacent points using the weights
    determined by the local geometric structure [[194](#bib.bib194), [195](#bib.bib195),
    [196](#bib.bib196)], and self-attention [[197](#bib.bib197), [198](#bib.bib198)].
    In particular, recent works have revealed that point-based models can achieve
    state-of-the-art computational efficiency with proper training and model scaling
    techniques [[199](#bib.bib199)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基本的3D几何数据结构是3D点云，其中每个点由其三个坐标表示。PointNet [[186](#bib.bib186)] 是利用深度学习处理3D点云的开创性工作。它采用点级特征提取和共享MLPs来保持排列不变性。PointNet++
    [[187](#bib.bib187)] 通过促进捕捉局部几何结构来改进PointNet。在这些基础上，许多工作关注如何在不显著增加计算成本的情况下有效地聚合局部信息。代表性的方法包括引入图神经网络
    [[188](#bib.bib188), [189](#bib.bib189)]，将3D点投影到规则网格上进行卷积 [[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192), [193](#bib.bib193)]，使用由局部几何结构确定的权重聚合相邻点的特征
    [[194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196)]，以及自注意力 [[197](#bib.bib197),
    [198](#bib.bib198)]。特别是，近期的研究表明，基于点的模型在适当的训练和模型扩展技术下可以实现最先进的计算效率 [[199](#bib.bib199)]。
- en: 2.4.2 Voxel-based Models
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 基于体素的模型
- en: The 3D point clouds can be further transformed to voxels, which are regular
    and can be directly processed with 3D convolution [[200](#bib.bib200)]. Typically,
    the 3D space is divided into cubic voxel grids, while the features of the points
    in each grid will be averaged. The side length of the grid is named as the voxel
    resolution. An important technique for processing voxels efficiently is sparse
    convolution [[201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)], *i.e.*,
    only performing convolution on the voxels with 3D points in them. Many works design
    backbone networks with this mechanism conditioned on the vision task of interest
    [[204](#bib.bib204), [205](#bib.bib205), [206](#bib.bib206)] for an optimal efficiency-accuracy
    trade-off. In addition, the point-based and voxel-based models can be combined
    to reduce the memory and computational cost [[207](#bib.bib207)]. Some recent
    works have explored the automatic backbone design using NAS [[208](#bib.bib208)]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 3D点云可以进一步转换为体素，这些体素是规则的，可以直接通过3D卷积[[200](#bib.bib200)]进行处理。通常，3D空间被划分为立方体体素网格，而每个网格中点的特征将被平均。网格的边长称为体素分辨率。有效处理体素的一个重要技术是稀疏卷积[[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203)]，*即*，仅对包含3D点的体素执行卷积。许多工作根据感兴趣的视觉任务设计具有这种机制的主干网络[[204](#bib.bib204),
    [205](#bib.bib205), [206](#bib.bib206)]，以实现最佳的效率-准确度权衡。此外，点基模型和体素基模型可以结合起来以减少内存和计算成本[[207](#bib.bib207)]。一些最近的工作探索了使用NAS[[208](#bib.bib208)]进行自动主干设计。
- en: 2.4.3 Multi-view-based Models
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 基于多视角的模型
- en: Multi-view projective analysis is another effective solution for understanding
    3D shapes, where the 3D objects are projected into 2D images from varying visual
    angles and processed by 2D backbone networks [[209](#bib.bib209)]. This idea can
    be implemented for recognition [[210](#bib.bib210), [211](#bib.bib211)], retrieval
    [[212](#bib.bib212), [213](#bib.bib213)] and pose estimation [[214](#bib.bib214)].
    An important challenge for these methods is how to fuse the multi-view features.
    Existing works have proposed to leverage LSTM [[213](#bib.bib213)] or graph convolutional
    network [[210](#bib.bib210)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多视角投影分析是理解3D形状的另一种有效方法，其中3D对象从不同的视觉角度投影到2D图像中，并通过2D主干网络[[209](#bib.bib209)]进行处理。这个想法可以应用于识别[[210](#bib.bib210),
    [211](#bib.bib211)]、检索[[212](#bib.bib212), [213](#bib.bib213)]和姿态估计[[214](#bib.bib214)]。这些方法面临的一个重要挑战是如何融合多视角特征。现有工作提出利用LSTM[[213](#bib.bib213)]或图卷积网络[[210](#bib.bib210)]。
- en: 3 Dynamic Backbone Networks
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动态主干网络
- en: 'Although the advanced architectures introduced in Sec. [2](#S2 "2 Architecture
    Design of Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey") have achieved significant progress in improving the inference
    efficiency of deep models, they generally have an intrinsic limitation: the computational
    graphs are kept the same during inference when processing different inputs with
    varying complexity. Such a *static* inference paradigm inevitably brings redundant
    computation on some “easy” samples. To address this issue, dynamic neural networks
    [[33](#bib.bib33)] have attracted great research interest in recent years due
    to their favorable efficiency, representation power, and adaptiveness [[33](#bib.bib33)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管第[2](#S2 "2 Architecture Design of Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")节中介绍的高级架构在提高深度模型的推理效率方面取得了显著进展，但它们通常存在一个内在的限制：在处理不同复杂度的输入时，计算图在推理过程中保持不变。这种*静态*的推理范式不可避免地会在一些“简单”样本上带来冗余计算。为了解决这个问题，动态神经网络[[33](#bib.bib33)]近年来因其良好的效率、表示能力和适应性[[33](#bib.bib33)]而引起了极大的研究兴趣。'
- en: 'Researchers have proposed various types of dynamic networks which can adapt
    their architectures/parameters to different inputs. Based on the granularity of
    adaptive inference, we categorize related works into *sample-wise* (Sec. [3.1](#S3.SS1
    "3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), *spatial-wise* (Sec. [3.2](#S3.SS2
    "3.2 Spatial-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), and *temporal-wise* (Sec. [3.3](#S3.SS3
    "3.3 Temporal-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) dynamic networks. Compared to the
    previous work [[33](#bib.bib33)] which contains both vision and language models,
    we mainly focus on the computational efficient models for vision tasks in this
    survey. Moreover, more up-to-date works are included.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '研究人员提出了多种类型的动态网络，它们可以根据不同的输入调整其架构/参数。根据自适应推理的粒度，我们将相关工作分类为*样本级*（第[3.1节](#S3.SS1
    "3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")）、*空间级*（第[3.2节](#S3.SS2 "3.2 Spatial-wise
    Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey")）和*时间级*（第[3.3节](#S3.SS3 "3.3 Temporal-wise Dynamic
    Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for
    Computer Vision: A Survey")）动态网络。与包含视觉和语言模型的先前工作[[33](#bib.bib33)]相比，本次调查主要关注视觉任务中的计算高效模型。此外，还包括了更多最新的研究成果。'
- en: 3.1 Sample-wise Dynamic Networks
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 样本级动态网络
- en: 'The most common adaptive inference paradigm is processing each input sample
    (*e.g.* an image) dynamically. There are mainly two lines of work in this direction:
    one aims at reducing the computation with decent network performance via dynamic
    *architectures*, and the other adjusts network *parameters* to boost the representation
    power with minor computational overhead. In this survey, we focus on the former
    line which typically reduces redundant computation for improving efficiency. Popular
    approaches include three types: 1) dynamic depth, 2) dynamic width, and 3) dynamic
    routing in a super network (SuperNet).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的自适应推理范式是动态处理每个输入样本（*例如* 图像）。在这一方向上主要有两条研究路线：一种旨在通过动态*架构*在保持良好网络性能的同时减少计算，另一种则调整网络*参数*以提高表示能力，同时计算开销较小。在本次调查中，我们重点关注前者，该方法通常通过减少冗余计算来提高效率。流行的方法包括三种类型：1）动态深度，2）动态宽度，3）在超网络（SuperNet）中的动态路由。
- en: '![Refer to caption](img/a42d226b13123857b6481412fa1a44f4.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a42d226b13123857b6481412fa1a44f4.png)'
- en: 'Figure 2: Dynamic early exiting. When the prediction at an early exit satisfies
    some criterion (the green tick), the inference procedure terminates, and the later
    computation will be skipped.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：动态早期退出。当早期退出的预测满足某些标准（绿色勾号）时，推理过程终止，后续计算将被跳过。
- en: 3.1.1 Dynamic Depth
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 动态深度
- en: The inference procedure of a traditional (static) network can be written as
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 传统（静态）网络的推理过程可以写作
- en: '|  | $\mathbf{y}=f(\mathbf{x})=f^{L}\circ f^{L-1}\circ\dots\circ f^{1}(\mathbf{x}),$
    |  | (3) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y}=f(\mathbf{x})=f^{L}\circ f^{L-1}\circ\dots\circ f^{1}(\mathbf{x}),$
    |  | (3) |'
- en: 'where $f^{\ell},\ell=1,2,\dots,L$ is the $\ell$-th layer, and $L$ is the network
    depth. In contrast, networks with dynamic depth process each sample $\mathbf{x}_{i}$
    with an adaptive number of layers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f^{\ell},\ell=1,2,\dots,L$ 是第 $\ell$ 层，$L$ 是网络深度。相比之下，具有动态深度的网络用适应性层数处理每个样本
    $\mathbf{x}_{i}$：
- en: '|  | $\mathbf{y}_{i}=f(\mathbf{x})=f^{L_{i}}\circ f^{L_{i}-1}\circ\dots\circ
    f^{1}(\mathbf{x}),$ |  | (4) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y}_{i}=f(\mathbf{x})=f^{L_{i}}\circ f^{L_{i}-1}\circ\dots\circ
    f^{1}(\mathbf{x}),$ |  | (4) |'
- en: where $1\leq L_{i}\leq L$ is decided based on $\mathbf{x}_{i}$ itself.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $1\leq L_{i}\leq L$ 是根据 $\mathbf{x}_{i}$ 本身决定的。
- en: 'There are mainly two common implementations to realize dynamic depth. The first
    is *early exiting*, which means that the network predictions for some “easy” samples
    can be output at an intermediate layer without activating the deeper layers [[215](#bib.bib215),
    [216](#bib.bib216)] (Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")). Researchers have found that multiple classifiers in a deep
    model may interfere with each other and degrade the performance by forcing early
    layers to capture semantic-level features [[34](#bib.bib34)]. To address this
    issue, multi-scale feature representation is adopted [[34](#bib.bib34), [217](#bib.bib217)]
    to quickly produce coarse-scale features with rich semantic information. Instead
    of constructing intermediate exits in convolutional networks, the recent Dynamic
    Vision Transformer (DVT) [[218](#bib.bib218)] realizes early exiting in cascaded
    vision Transformers which process images with different token numbers. Dynamic
    Perceiver [[219](#bib.bib219)] proposes to integrate intermediate features and
    perform early-exit by introducing an addition attention-based path. Apart from
    architectural design, researchers have also proposed specialized techniques [[220](#bib.bib220),
    [221](#bib.bib221)] for training early-exiting models.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 实现动态深度主要有两种常见方法。第一种是*早期退出*，意味着网络对一些“简单”样本的预测可以在中间层输出，而无需激活更深层次 [[215](#bib.bib215),
    [216](#bib.bib216)]（图[2](#S3.F2 "图2 ‣ 3.1 样本级动态网络 ‣ 3 动态主干网络 ‣ 计算高效的计算机视觉深度学习综述")）。研究人员发现，深层模型中的多个分类器可能会相互干扰，并通过强迫早期层捕捉语义级别特征来降低性能
    [[34](#bib.bib34)]。为了解决这一问题，采用了多尺度特征表示 [[34](#bib.bib34), [217](#bib.bib217)]，以快速生成具有丰富语义信息的粗尺度特征。与其在卷积网络中构建中间退出，最近的动态视觉Transformer
    (DVT) [[218](#bib.bib218)] 实现了在处理不同token数量的级联视觉Transformer中的早期退出。动态Perceiver [[219](#bib.bib219)]
    提出了通过引入额外的基于注意力的路径来集成中间特征并进行早期退出。除了架构设计，研究人员还提出了用于训练早期退出模型的专业技术 [[220](#bib.bib220),
    [221](#bib.bib221)]。
- en: '![Refer to caption](img/8c9975d0efb635e6a5d6ba14e2604679.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8c9975d0efb635e6a5d6ba14e2604679.png)'
- en: 'Figure 3: Dynamic layer skipping. A gating module is used to decide whether
    to execute the block.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：动态层跳过。使用门控模块决定是否执行该块。
- en: 'The aforementioned early-exiting methods dynamically terminate the forward
    propagation at a certain layer. An alternative approach to dynamic depth is *skipping
    intermediate layers* in models with skip connection such as ResNets [[4](#bib.bib4)]
    and vision Transformers [[6](#bib.bib6)] (Figure [3](#S3.F3 "Figure 3 ‣ 3.1.1
    Dynamic Depth ‣ 3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks
    ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")). Let $\mathbf{x}^{\ell}$
    and $f^{\ell}$ denote the feature and the computational unit at layer-$\ell$,
    a typical implementation of layer skipping is using a gating module $g^{\ell}(\cdot)$
    to dynamically decide whether to execute $f^{\ell}$ [[222](#bib.bib222), [223](#bib.bib223),
    [224](#bib.bib224)]:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的早期退出方法在某一层动态终止前向传播。动态深度的另一种方法是*跳过中间层*，应用于具有跳跃连接的模型如ResNets [[4](#bib.bib4)]
    和视觉Transformer [[6](#bib.bib6)]（图[3](#S3.F3 "图3 ‣ 3.1.1 动态深度 ‣ 3.1 样本级动态网络 ‣ 3
    动态主干网络 ‣ 计算高效的计算机视觉深度学习综述")）。设 $\mathbf{x}^{\ell}$ 和 $f^{\ell}$ 分别表示第$\ell$层的特征和计算单元，典型的层跳过实现是使用门控模块
    $g^{\ell}(\cdot)$ 动态决定是否执行 $f^{\ell}$ [[222](#bib.bib222), [223](#bib.bib223),
    [224](#bib.bib224)]：
- en: '|  | $\mathbf{x}^{\ell+1}=\mathbf{x}^{\ell}+g^{\ell}(\mathbf{x})\cdot f^{\ell}(\mathbf{x}),g^{\ell}(\mathbf{x})\in{0,1}.$
    |  | (5) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}^{\ell+1}=\mathbf{x}^{\ell}+g^{\ell}(\mathbf{x})\cdot f^{\ell}(\mathbf{x}),g^{\ell}(\mathbf{x})\in{0,1}.$
    |  | (5) |'
- en: '![Refer to caption](img/39e08f3ac1c730a72080b38f4727296c.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39e08f3ac1c730a72080b38f4727296c.png)'
- en: 'Figure 4: Dynamic channel skipping, which uses a gating module to decide the
    computation of convolution channels.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：动态通道跳过，使用门控模块决定卷积通道的计算。
- en: 3.1.2 Dynamic Width
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 动态宽度
- en: 'Instead of skipping an entire layer, a less aggressive approach is adjusting
    the network *width* to different inputs. In this direction, the most popular implementation
    is dynamically *skipping the channels* in convolutional blocks via a gating module
    [[35](#bib.bib35), [225](#bib.bib225), [226](#bib.bib226), [227](#bib.bib227)]
    (Figure [4](#S3.F4 "Figure 4 ‣ 3.1.1 Dynamic Depth ‣ 3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")). Specifically, a gating module is first executed before conducting
    a convolution operation. The output of this gating module is a $C$-dimensional
    binary vector that decides whether to compute each channel, where $C$ is the output
    channel number. This implementation is similar to that in the aforementioned layer-skipping
    scheme. The most prominent difference is that the output of the gating module
    in layer skipping is a scalar, and the gating module in channel-skipping is required
    to output a vector controlling the computation of different channels. Apart from
    convolution layers, the same idea can also be applied in vision Transformers to
    dynamically skip channels in multi-layer perceptron (MLP) blocks [[224](#bib.bib224)].'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与跳过整个层的做法不同，较少激进的方法是根据不同的输入调整网络的*宽度*。在这个方向上，最流行的实现是通过门控模块动态*跳过通道* [[35](#bib.bib35),
    [225](#bib.bib225), [226](#bib.bib226), [227](#bib.bib227)]（图 [4](#S3.F4 "图 4
    ‣ 3.1.1 动态深度 ‣ 3.1 样本级动态网络 ‣ 3 动态骨干网络 ‣ 面向计算效率的深度学习：计算机视觉的综述")）。具体而言，首先执行一个门控模块，然后进行卷积操作。这个门控模块的输出是一个
    $C$ 维的二进制向量，用于决定是否计算每个通道，其中 $C$ 是输出通道数量。这个实现类似于上述的层跳过方案。最明显的区别在于，层跳过中的门控模块输出的是一个标量，而通道跳过中的门控模块需要输出一个控制不同通道计算的向量。除了卷积层，类似的想法也可以应用于视觉变换器中，以动态跳过多层感知机（MLP）块中的通道
    [[224](#bib.bib224)]。
- en: 3.1.3 Dynamic Routing in SuperNets
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 超级网络中的动态路由
- en: Instead of skipping the computation of layers or channels in conventional network
    architectures, one can also realize data-dependent inference via *dynamic routing*
    in super networks (SuperNets). A SuperNet usually contains various inference paths,
    and routing nodes are responsible for allocating each sample to the appropriate
    path. Let $\mathbf{x}_{i}^{\ell}$ denote the $i$-th node in layer-$\ell$, a general
    formulation of the computation for obtaining node-$j$ in the next layer can be
    written as
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统网络架构中跳过层或通道的计算不同，可以通过在超级网络（SuperNets）中使用*动态路由*实现数据依赖的推断。一个超级网络通常包含各种推断路径，路由节点负责将每个样本分配到合适的路径。设
    $\mathbf{x}_{i}^{\ell}$ 表示第 $i$ 个节点在层 $\ell$ 中，获取下一个层中节点 $j$ 的计算的一般公式可以写为
- en: '|  | $\mathbf{x}_{j}^{\ell+1}=\sum_{i:\alpha_{i\rightarrow j}^{\ell}>0}\alpha_{i\rightarrow
    j}^{\ell}f_{i\rightarrow j}^{\ell}(\mathbf{x}_{i}^{\ell}),$ |  | (6) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{x}_{j}^{\ell+1}=\sum_{i:\alpha_{i\rightarrow j}^{\ell}>0}\alpha_{i\rightarrow
    j}^{\ell}f_{i\rightarrow j}^{\ell}(\mathbf{x}_{i}^{\ell}),$ |  | (6) |'
- en: where $f_{i\rightarrow j}^{\ell}$ is the transformation between node $i$ and
    $j$, and $\alpha_{i\rightarrow j}^{\ell}$ is the weight for this path which is
    calculated based on $\mathbf{x}_{i}^{\ell}$. If $\alpha_{i\rightarrow j}^{\ell}=0$,
    the transformation $f_{i\rightarrow j}^{\ell}$ can be skipped.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f_{i\rightarrow j}^{\ell}$ 是节点 $i$ 和 $j$ 之间的变换，而 $\alpha_{i\rightarrow j}^{\ell}$
    是为该路径计算的权重，基于 $\mathbf{x}_{i}^{\ell}$ 计算。如果 $\alpha_{i\rightarrow j}^{\ell}=0$，则可以跳过变换
    $f_{i\rightarrow j}^{\ell}$。
- en: Extensive works have proposed different forms of SuperNets, such as tree structures
    [[36](#bib.bib36), [228](#bib.bib228), [229](#bib.bib229)], dynamic mixture-of-experts
    [[230](#bib.bib230), [231](#bib.bib231)], and more general architectures [[232](#bib.bib232),
    [233](#bib.bib233)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 大量工作提出了不同形式的超级网络，例如树状结构 [[36](#bib.bib36), [228](#bib.bib228), [229](#bib.bib229)]，动态专家混合
    [[230](#bib.bib230), [231](#bib.bib231)]，以及更通用的架构 [[232](#bib.bib232), [233](#bib.bib233)]。
- en: 3.2 Spatial-wise Dynamic Networks
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 空间级动态网络
- en: It has been found that different spatial locations in an image contribute unequally
    to the performance of vision tasks [[234](#bib.bib234)]. However, most existing
    deep models process different spatial locations with the same computation, leading
    to redundant computation on less important regions. To this end, spatial-wise
    dynamic networks are proposed to exploit the spatial redundancy in image data
    to achieve an improved efficiency. Based on the granularity of adaptive inference,
    we categorize relative works into pixel level, region level, and resolution level.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 已发现图像中的不同空间位置对视觉任务的性能贡献不均[[234](#bib.bib234)]。然而，大多数现有深度模型对不同的空间位置进行相同的计算，导致对不重要区域的冗余计算。为此，提出了空间自适应动态网络，以利用图像数据中的空间冗余来提高效率。根据自适应推理的粒度，我们将相关工作分为像素级、区域级和分辨率级。
- en: 3.2.1 Pixel-level Dynamic Networks
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 像素级动态网络
- en: 'A typical approach to spatial-wise adaptive inference is dynamically deciding
    whether to compute each pixel in a convolution block based on a binary mask [[235](#bib.bib235),
    [236](#bib.bib236), [237](#bib.bib237)]. This form is similar to that in layer
    skipping and channel skipping (Sec. [3.1](#S3.SS1 "3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")), except that the gating module is required to output a spatial
    mask. Each element of this spatial mask determines the computation of a feature
    pixel. In this way, the mask generators learn to locate the most discriminative
    regions in image features, and redundant computation on less informative pixels
    can be skipped.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '一种典型的空间自适应推理方法是根据二进制掩码动态决定是否计算卷积块中的每个像素[[235](#bib.bib235), [236](#bib.bib236),
    [237](#bib.bib237)]。这种形式类似于层跳过和通道跳过（第[3.1](#S3.SS1 "3.1 Sample-wise Dynamic Networks
    ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for Computer
    Vision: A Survey")节），不同之处在于门控模块需要输出空间掩码。这个空间掩码的每个元素决定了特征像素的计算。通过这种方式，掩码生成器学习定位图像特征中最具判别性的区域，并且可以跳过对信息量较少的像素的冗余计算。'
- en: The limitation of such pixel-level dynamic computation is that the acceleration
    is currently not supported by most deep learning libraries. The memory access
    cost can be heavier than static convolutions, and the computation parallelism
    is reduced due to sparse convolution. As a result, although the computation can
    be significantly reduced, the practical efficiency of these methods usually lags
    behind their theoretical efficiency. To this end, researchers have also proposed
    “coarse-grained” spatial-wise dynamic networks [[238](#bib.bib238), [39](#bib.bib39)],
    which means that an element of a spatial mask can decide a patch rather than a
    pixel. In this way, more contiguous memory access is realized for realistic speedup.
    Moreover, the scheduling strategies are also proven to have a considerable effect
    on the inference latency [[39](#bib.bib39)]. It is also promising to co-design
    algorithm, scheduling, and hardware devices to better harvest the theoretical
    efficiency of spatial-wise dynamic networks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种像素级动态计算的局限性在于，目前大多数深度学习库不支持加速。内存访问成本可能比静态卷积更高，并且由于稀疏卷积，计算并行性降低。因此，虽然计算可以显著减少，但这些方法的实际效率通常落后于理论效率。为此，研究人员还提出了“粗粒度”空间自适应动态网络[[238](#bib.bib238),
    [39](#bib.bib39)]，这意味着空间掩码的一个元素可以决定一个区域而不是一个像素。这样可以实现更连续的内存访问，从而加速实际速度。此外，调度策略也被证明对推理延迟有显著影响[[39](#bib.bib39)]。共同设计算法、调度和硬件设备，以更好地利用空间自适应动态网络的理论效率也是很有前景的。
- en: Apart from skipping the computation of certain pixels, another line of work
    breaks the static reception field of traditional convolution and proposes deformable
    convolution [[239](#bib.bib239), [240](#bib.bib240), [241](#bib.bib241)]. Specifically,
    a lightweight module is used to learn the offsets for each feature pixel, and
    the convolution neighbors are sampled from arbitrary locations based on the predicted
    offsets. This idea has also been implemented in vision Transformers to enhance
    the performance of the local attention mechanism [[242](#bib.bib242)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了跳过某些像素的计算，另一类工作打破了传统卷积的静态接收场，提出了可变形卷积[[239](#bib.bib239), [240](#bib.bib240),
    [241](#bib.bib241)]。具体而言，使用轻量级模块来学习每个特征像素的偏移量，并根据预测的偏移量从任意位置采样卷积邻域。这一思想也已在视觉Transformer中实现，以增强局部注意机制的性能[[242](#bib.bib242)]。
- en: 3.2.2 Region-level Dynamic Networks
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 区域级动态网络
- en: Instead of flexibly deciding which feature pixels to compute, another line of
    work aims at locating important regions (patches) in input images and cropping
    these patches for recognition tasks. For example, image recognition can be formulated
    as a sequential decision problem, in which an RNN is adopted to make predictions
    based on the cropped image patches [[243](#bib.bib243), [244](#bib.bib244)]. A
    multi-scale CNN with multiple sub-networks could also be used to perform the classification
    task based on cropped salient image patches [[245](#bib.bib245)]. A lightweight
    module is placed between every two sub-networks to decide the coordinate and size
    of the salient patch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了灵活地决定计算哪些特征像素，另一种方法旨在定位输入图像中的重要区域（块）并裁剪这些块以进行识别任务。例如，图像识别可以被表述为一个顺序决策问题，在其中采用RNN基于裁剪的图像块进行预测[[243](#bib.bib243),
    [244](#bib.bib244)]。也可以使用多尺度CNN与多个子网络来执行基于裁剪显著图像块的分类任务[[245](#bib.bib245)]。在每两个子网络之间放置一个轻量级模块来决定显著块的坐标和大小。
- en: 'Along this direction, the recent glance-and-focus network (GFNet) [[246](#bib.bib246),
    [247](#bib.bib247)] proposes a general framework for region-level dynamic inference
    which is compatible with various visual backbones. It first “glances” a low-resolution
    input image, and then repeatedly “focus” on salient regions using reinforcement
    learning (RL) [[248](#bib.bib248)]. Moreover, early exiting (Sec. [3.1](#S3.SS1
    "3.1 Sample-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) is allowed, which means that the
    step number of “focus” can be dynamically adjusted for different input images.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，近期的瞥视-聚焦网络（GFNet）[[246](#bib.bib246), [247](#bib.bib247)]提出了一个适用于各种视觉骨干网的区域级动态推断的通用框架。它首先“瞥视”一个低分辨率的输入图像，然后使用强化学习（RL）[[248](#bib.bib248)]重复地“聚焦”于显著区域。此外，允许早期退出（参见[3.1](#S3.SS1
    "3.1 样本级动态网络 ‣ 3 动态骨干网络 ‣ 计算高效的计算机视觉深度学习：综述")），这意味着“聚焦”的步骤数量可以根据不同输入图像动态调整。
- en: 3.2.3 Resolution-level Dynamic Networks
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 分辨率级动态网络
- en: Most existing vision models process different images with the same resolution.
    However, the input complexity could vary, and not all images require a high-resolution
    representation. Ideally, low-resolution representations should be sufficient for
    those “easy” samples with large objects and canonical features. The early work
    [[249](#bib.bib249)] proposes to adaptively zoom input images in the face detection
    task. The recent resolution adaptive network (RANet) [[217](#bib.bib217)] builds
    a multi-scale architecture, in which inputs are first processed with a low resolution
    and a small sub-network. Large sub-networks and high-resolution representations
    are conditionally activated based on early predictions. Instead of using a specialized
    structure, dynamic resolution network [[250](#bib.bib250)] rescales each image
    with the resolution predicted by a small model and feeds the rescaled image to
    common CNNs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的视觉模型处理具有相同分辨率的不同图像。然而，输入复杂性可能有所不同，并非所有图像都需要高分辨率表示。理想情况下，低分辨率表示对于那些具有大物体和标准特征的“简单”样本应该足够。早期的工作[[249](#bib.bib249)]提出在面部检测任务中自适应地缩放输入图像。近期的分辨率自适应网络（RANet）[[217](#bib.bib217)]建立了一个多尺度架构，其中输入首先以低分辨率和小子网络进行处理。根据早期预测，有条件地激活大子网络和高分辨率表示。不同于使用专门结构，动态分辨率网络[[250](#bib.bib250)]通过小模型预测的分辨率来重新调整每张图像的大小，并将调整后的图像输入到通用CNN中。
- en: Note that different spatial locations are still processed equally in the aforementioned
    methods. We categorize the relative works in this section since they mainly utilize
    the spatial redundancy of image inputs for efficient inference.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上述方法中，不同的空间位置仍然以相同的方式处理。我们在本节中对相关工作进行分类，因为它们主要利用图像输入的空间冗余进行高效推断。
- en: 3.3 Temporal-wise Dynamic Networks
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 时间动态网络
- en: 'As video data can be viewed as a sequence of image data, adaptive computation
    could also be performed along the temporal dimension due to the considerable redundancy
    in video recognition tasks. Representative works can generally be divided into
    two lines: one processes video with recurrent models and dynamically save computation
    at certain time steps; the other aims at sampling key frames/clips and allocating
    the computation to these sampled frames.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视频数据可以视为一系列图像数据，因此由于视频识别任务中的显著冗余，也可以在时间维度上执行自适应计算。代表性的工作通常可以分为两类：一类处理具有递归模型的视频，并在某些时间步动态节省计算；另一类则旨在采样关键帧/剪辑，并将计算分配给这些采样帧。
- en: 3.3.1 Dynamic Recurrent Models
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 动态递归模型
- en: Different video frames are unequally informative. To this end, extensive studies
    propose to dynamically activate computation when updating the hidden state in
    recurrent models. For example, LiteEval [[251](#bib.bib251)] establishes two different
    sized LSTM [[252](#bib.bib252)]. In each time step, a gating module is used to
    decide which LSTM should be executed for processing the current frame. AdaFuse
    [[253](#bib.bib253)] dynamically skips the computation of some convolution channels,
    and these channels are filled with the hidden state from the previous step. Moreover,
    the numerical precision [[254](#bib.bib254)] and image resolution [[255](#bib.bib255)]
    of different frames can also be dynamically decided.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的视频帧在信息量上不均等。为此，广泛的研究提出了在递归模型中动态激活计算来更新隐藏状态。例如，LiteEval [[251](#bib.bib251)]
    建立了两个不同尺寸的 LSTM [[252](#bib.bib252)]。在每个时间步中，使用一个门控模块来决定应执行哪个 LSTM 以处理当前帧。AdaFuse
    [[253](#bib.bib253)] 动态跳过某些卷积通道的计算，这些通道用来自前一个步骤的隐藏状态填充。此外，不同帧的数值精度 [[254](#bib.bib254)]
    和图像分辨率 [[255](#bib.bib255)] 也可以动态决定。
- en: The aforementioned works generally require a ConvNet for encoding each input
    frame before updating the hidden state. A more flexible solution is allowing the
    network to learn “where to see”. In other words, networks can directly jump to
    an arbitrary temporal location in the video [[256](#bib.bib256), [257](#bib.bib257),
    [37](#bib.bib37)] or perform early exiting [[258](#bib.bib258), [259](#bib.bib259),
    [260](#bib.bib260)] instead of “watch” the entire video frame by frame.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作通常需要一个卷积网络来编码每个输入帧，然后更新隐藏状态。一个更灵活的解决方案是允许网络学习“看哪里”。换句话说，网络可以直接跳到视频中的任意时间位置
    [[256](#bib.bib256), [257](#bib.bib257), [37](#bib.bib37)] 或执行早期退出 [[258](#bib.bib258),
    [259](#bib.bib259), [260](#bib.bib260)]，而不是逐帧“观看”整个视频。
- en: 3.3.2 Dynamic Key Frame Sampling
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 动态关键帧采样
- en: An alternative to skipping computation in recurrent networks is sampling key
    frames and then feeding the sampled frames rather than the whole video to a standard
    model. Reinforcement learning is a popular technique for training frame samplers
    [[261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263)].
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过递归网络中的计算的替代方法是采样关键帧，然后将采样的帧而非整个视频输入到标准模型中。强化学习是训练帧采样器的流行技术 [[261](#bib.bib261),
    [262](#bib.bib262), [263](#bib.bib263)]。
- en: A recent trend is simultaneously achieving dynamic inference from multiple perspectives.
    For example, AdaFocus and its variants [[38](#bib.bib38), [264](#bib.bib264),
    [265](#bib.bib265), [266](#bib.bib266)] makes use of both spatial and temporal
    redundancy in video data. Dynamic architecture with 3D convolution [[267](#bib.bib267)]
    is also an interesting topic.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的趋势是从多个角度同时实现动态推断。例如，AdaFocus 及其变体 [[38](#bib.bib38), [264](#bib.bib264),
    [265](#bib.bib265), [266](#bib.bib266)] 利用视频数据中的空间和时间冗余。具有 3D 卷积的动态架构 [[267](#bib.bib267)]
    也是一个有趣的话题。
- en: 4 Efficient Models for Downstream Computer Vision Tasks
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 下游计算机视觉任务的高效模型
- en: 'In this section, we assume that a light-weighted backbone network has already
    been obtained, and discuss how to design task-specific heads or algorithms on
    top of them. The general aim is to facilitate accomplishing real-world computer
    vision tasks efficiently or even in real time. To this end, we will focus on three
    representative tasks, namely *object detection* (Sec. [4.1](#S4.SS1 "4.1 Object
    Detection ‣ 4 Efficient Models for Downstream Computer Vision Tasks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), *semantic segmentation* (Sec.
    [4.2](#S4.SS2 "4.2 Semantic Segmentation ‣ 4 Efficient Models for Downstream Computer
    Vision Tasks ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")),
    and *instance segmentation* (Sec. [4.3](#S4.SS3 "4.3 Instance Segmentation ‣ 4
    Efficient Models for Downstream Computer Vision Tasks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")), all of which have a strong need
    for accurate and real-time applications. Note that most of other more complex
    computer vision tasks (*e.g.* visual object tracking) are mainly based on the
    three tasks we consider.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们假设已经获得了一个轻量级的骨干网络，并讨论如何在其上设计特定任务的头部或算法。总体目标是实现高效甚至实时的真实世界计算机视觉任务。为此，我们将重点讨论三项代表性任务，即*目标检测*（第[4.1节](#S4.SS1
    "4.1 目标检测 ‣ 4 高效的下游计算机视觉任务模型 ‣ 计算效率高的计算机视觉深度学习综述")）、*语义分割*（第[4.2节](#S4.SS2 "4.2
    语义分割 ‣ 4 高效的下游计算机视觉任务模型 ‣ 计算效率高的计算机视觉深度学习综述")）和*实例分割*（第[4.3节](#S4.SS3 "4.3 实例分割
    ‣ 4 高效的下游计算机视觉任务模型 ‣ 计算效率高的计算机视觉深度学习综述")），这些任务都对准确性和实时性有强烈需求。请注意，大多数其他更复杂的计算机视觉任务（例如，视觉目标跟踪）主要基于我们考虑的这三项任务。
- en: 4.1 Object Detection
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 目标检测
- en: 'Object detection aims to answer two fundamental questions in computer vision:
    what visual objects are contained in the images, and where are them [[8](#bib.bib8)]?
    The classification and localization results obtained by object detection usually
    serve as the basis of other vision tasks, *e.g.*, instance segmentation, image
    captioning, and object tracking. The algorithms for object detection can be roughly
    categorized into *two-stage* (Sec. 4.1.1) and *one-stage* (Sec. 4.1.2). In the
    following, we will discuss them respectively from the lens of computational efficiency.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测旨在回答计算机视觉中的两个基本问题：图像中包含了什么视觉对象，以及它们的具体位置在哪里[[8](#bib.bib8)]？目标检测所获得的分类和定位结果通常作为其他视觉任务的基础，例如，实例分割、图像描述和目标跟踪。目标检测的算法大致可以分为*两阶段*（第4.1.1节）和*单阶段*（第4.1.2节）。接下来，我们将从计算效率的角度分别探讨这两种方法。
- en: 4.1.1 Two-stage Detectors
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 两阶段检测器
- en: Object detection with deep learning starts from the two stage paradigm. The
    pioneer work, RCNN [[268](#bib.bib268), [269](#bib.bib269)], proposes to first
    crop a set of object proposals from the images, and classify them with deep networks.
    On top of it, SPPNet [[270](#bib.bib270)] avoids repeatedly inferring the backbones
    by adaptively pooling the features of the regions of interest. Fast RCNN [[271](#bib.bib271)]
    simultaneously train a detector and a bounding box regressor in the same network,
    leading to more than 200 times of speedup than RCNN. Faster R-CNN [[272](#bib.bib272),
    [273](#bib.bib273)] and its improvements [[274](#bib.bib274), [275](#bib.bib275)]
    introduce a region proposal network that cheaply generates object proposals from
    the features, yielding the first nearly real-time deep learning detector. The
    feature pyramid networks further propose to leverage the feature maps at varying
    scales to detect the object with different sizes respectively, which improves
    the detection accuracy significantly without sacrificing the efficiency [[276](#bib.bib276)].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的目标检测起始于两阶段范式。开创性的工作，RCNN [[268](#bib.bib268), [269](#bib.bib269)]，提出首先从图像中裁剪出一组对象建议，并用深度网络对其进行分类。在此基础上，SPPNet
    [[270](#bib.bib270)]通过自适应地池化感兴趣区域的特征，避免了重复推断骨干网络。Fast RCNN [[271](#bib.bib271)]在同一网络中同时训练检测器和边界框回归器，使速度比RCNN提高了200倍以上。Faster
    R-CNN [[272](#bib.bib272), [273](#bib.bib273)]及其改进[[274](#bib.bib274), [275](#bib.bib275)]引入了区域提议网络，从特征中便宜地生成对象提议，创造了第一个几乎实时的深度学习检测器。特征金字塔网络进一步提出利用不同尺度的特征图分别检测不同尺寸的目标，从而显著提高检测精度而不牺牲效率[[276](#bib.bib276)]。
- en: 4.1.2 One-stage Detectors
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 单阶段检测器
- en: The major motivation behind the two-stage detects is the “coarse-to-fine” refining,
    *i.e.*, first obtaining the coarse proposals, and then refining the localization
    and discrimination results on top of these proposals, such that an excellent detection
    performance can be achieved. Despite the aforementioned techniques proposed to
    improve the efficiency of this procedure, the speed and the complexity of two-stage
    detectors are usually not applicable to real-time applications. In contrast, the
    one-stage detectors directly output the detection results in a single step, yielding
    much faster inference speed with a decent accuracy.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段检测的主要动机是“粗到细”的精炼，*即*，首先获得粗略的建议，然后在这些建议上精炼定位和分类结果，以便实现卓越的检测性能。尽管有上述技术被提出以提高该过程的效率，两阶段检测器的速度和复杂性通常不适用于实时应用。相比之下，单阶段检测器在一个步骤中直接输出检测结果，具有更快的推断速度和相当的准确性。
- en: 1) Bounding-box-based Methods. The first deep-learning-based one-stage detector
    is YOLO [[40](#bib.bib40)]. YOLO divides the image into grid regions and simultaneously
    predicts the bounding boxes and the classification results conditioned on each
    region. The subsequent works of YOLO [[277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279), [280](#bib.bib280), [43](#bib.bib43)] focus on further improving
    the localization performance or classification accuracy without affecting the
    practical speed. The latest version, YOLOv7 [[43](#bib.bib43)], achieves a state-of-the-art
    effectiveness-efficiency trade-off.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于边界框的方法。第一个基于深度学习的单阶段检测器是YOLO [[40](#bib.bib40)]。YOLO 将图像分成网格区域，并同时预测每个区域的边界框和分类结果。YOLO
    的后续工作 [[277](#bib.bib277), [278](#bib.bib278), [279](#bib.bib279), [280](#bib.bib280),
    [43](#bib.bib43)] 侧重于进一步提高定位性能或分类准确性，同时不影响实际速度。最新版本YOLOv7 [[43](#bib.bib43)] 实现了最先进的效能-效率权衡。
- en: In addition to YOLO, SSD [[41](#bib.bib41)] improves the accuracy of one-stage
    detectors by detecting the objects at different scales on different layers of
    the network. RetinaNet [[281](#bib.bib281)] proposes a focal loss to encourage
    the model to focus more on the difficult, misclassified examples, which boosting
    the accuracy of one-stage detectors effectively.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了YOLO，SSD [[41](#bib.bib41)] 通过在网络的不同层上检测不同尺度的物体来提高单阶段检测器的准确性。RetinaNet [[281](#bib.bib281)]
    提出了焦点损失，鼓励模型更加关注困难的、被误分类的样本，从而有效地提升了单阶段检测器的准确性。
- en: 2) Point-based Methods. The aforementioned detection methods mostly learn to
    produce the ground-truth bounding boxes on top of pre-defined anchor boxes. Despite
    the effectiveness, this paradigm suffers from a lot of design hyper-parameters
    and an imbalance between positive/negative boxes during training. To address this
    issue, CornerNet [[282](#bib.bib282)] proposes to directly predict the top-left
    corner and bottom-right corner of candidate boxes. Many subsequent works extend
    this point-based setting. For example, FCOS [[42](#bib.bib42)] predicts the distances
    from each location in feature maps to the four sides of the bounding box. ExtremeNet
    [[283](#bib.bib283)] learns to detect the extreme points the center of bounding
    boxes. CenterNet [[284](#bib.bib284)] further considers each object to be a single
    center point and regresses all the attributes (2D/3D size, orientation, depth,
    locations, etc.) based on this point.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基于点的方法。上述检测方法大多学习在预定义的锚点框上生成真实边界框。尽管有效，但这种范式在训练过程中存在许多设计超参数和正负样本框的不平衡。为了解决这个问题，CornerNet
    [[282](#bib.bib282)] 提出了直接预测候选框的左上角和右下角。许多后续工作扩展了这种基于点的设置。例如，FCOS [[42](#bib.bib42)]
    预测特征图中每个位置到边界框四个边的距离。ExtremeNet [[283](#bib.bib283)] 学习检测边界框的极端点。CenterNet [[284](#bib.bib284)]
    进一步考虑每个物体为一个单一的中心点，并基于此点回归所有属性（2D/3D大小、方向、深度、位置等）。
- en: 3) Transformer-based Methods. In recent years, N. Carion *et al.* propose an
    end-to-end Transformer-based detection network, DETR [[285](#bib.bib285)]. DETR
    views detection as a set prediction problem, where the results are obtained based
    on several object queries. Deformable DETR [[286](#bib.bib286)] addresses the
    long convergence issue of DETR by introducing a deformable mechanism to self-attention.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 基于变换器的方法。近年来，N. Carion *et al.* 提出了一个端到端的基于变换器的检测网络，DETR [[285](#bib.bib285)]。DETR
    将检测视为一个集合预测问题，结果是基于多个目标查询获得的。Deformable DETR [[286](#bib.bib286)] 通过引入变形机制来处理DETR的长期收敛问题。
- en: 4.2 Semantic Segmentation
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语义分割
- en: The aim of semantic segmentation is to predict the semantic label of each pixels
    [[9](#bib.bib9), [287](#bib.bib287)], *e.g.*, if a pixel belongs to a car, a bike,
    etc. Here we summarize existing efficient semantic segmentation methodologies
    based on their paradigms *i.e.*, *encoder-decoder* (Sec. 4.2.1), *multi-branch*
    (Sec. 4.2.2) and others (Sec. 4.2.3).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割的目标是预测每个像素的语义标签[[9](#bib.bib9), [287](#bib.bib287)]，*例如*，一个像素是否属于汽车、摩托车等。在这里，我们总结了基于不同范式的现有高效语义分割方法，*即*，*编码器-解码器*（第4.2.1节）、*多分支*（第4.2.2节）及其他（第4.2.3节）。
- en: 4.2.1 Encoder-decoder
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 编码器-解码器
- en: A popular approach is to first extract the low-resolution discriminative representations
    with a multi-stage backbone network, up-sample the deep features to the input
    resolution with a decoder, and then produce the pixel-wise predictions. This procedure
    is named as “encoder-decoder” [[288](#bib.bib288), [289](#bib.bib289)]. To improve
    the efficiency of this paradigm, many works propose to design light-weighted decoders.
    Representative methods include introducing split-transform-merge architectures
    [[290](#bib.bib290), [291](#bib.bib291), [292](#bib.bib292), [73](#bib.bib73),
    [293](#bib.bib293)] (Eq. (LABEL:eq:split)), developing efficient approximations
    of the computationally intensive dilated convolution [[294](#bib.bib294), [295](#bib.bib295),
    [296](#bib.bib296)], and introducing dense connections [[296](#bib.bib296), [297](#bib.bib297)].
    In addition, it is efficient to simultaneously feed the low-level and high-level
    features into the decoder, *i.e.*, comprehensively leveraging both of them improves
    the accuracy without introducing notable computational overhead [[298](#bib.bib298),
    [299](#bib.bib299), [300](#bib.bib300), [301](#bib.bib301), [297](#bib.bib297)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的方法是首先使用多阶段骨干网络提取低分辨率的判别性表示，通过解码器将深层特征上采样到输入分辨率，然后生成像素级预测。这个过程被称为“编码器-解码器”[[288](#bib.bib288),
    [289](#bib.bib289)]。为了提高这一范式的效率，许多工作提出了设计轻量级解码器。代表性的方法包括引入分割-变换-合并架构[[290](#bib.bib290),
    [291](#bib.bib291), [292](#bib.bib292), [73](#bib.bib73), [293](#bib.bib293)]（方程式（LABEL:eq:split）），开发计算密集型膨胀卷积的高效近似[[294](#bib.bib294),
    [295](#bib.bib295), [296](#bib.bib296)]，以及引入密集连接[[296](#bib.bib296), [297](#bib.bib297)]。此外，同时将低级和高级特征输入到解码器中是高效的，*即*，全面利用这两者可以提高准确性而不会引入显著的计算开销[[298](#bib.bib298),
    [299](#bib.bib299), [300](#bib.bib300), [301](#bib.bib301), [297](#bib.bib297)]。
- en: 4.2.2 Multi-branch Models
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 多分支模型
- en: 'Another popular efficient paradigm is designing multi-branch architectures.
    Typically, the model consist of two types of paths: 1) context paths with low-resolution
    feature maps and large receptive fields, aiming to extract discriminative information;
    and 2) spatial paths that preserve the low-level spatial information. These paths
    are fused in a parallel [[44](#bib.bib44), [298](#bib.bib298), [302](#bib.bib302),
    [303](#bib.bib303), [304](#bib.bib304)] or cascade [[305](#bib.bib305)] fashion,
    yielding high-resolution but semantically rich deep representations for segmentation.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的高效范式是设计多分支架构。通常，模型包括两种类型的路径：1）具有低分辨率特征图和大接收域的上下文路径，旨在提取判别性信息；2）保留低级空间信息的空间路径。这些路径以并行[[44](#bib.bib44),
    [298](#bib.bib298), [302](#bib.bib302), [303](#bib.bib303), [304](#bib.bib304)]或级联[[305](#bib.bib305)]的方式融合，从而生成高分辨率但语义丰富的深层表示用于分割。
- en: 4.2.3 Others
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 其他
- en: In recent years, some new ideas have been proposed to facilitate efficient semantic
    segmentation. For example, processing deep features with self-attention layers
    [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)], designing segmentation
    models with NAS [[309](#bib.bib309), [310](#bib.bib310), [311](#bib.bib311)],
    adjusting the architecture of the decoder conditioned on the inputs [[233](#bib.bib233)].
    More recently, a considerable number of papers seek to design efficient semantic
    segmentation models on top of ViTs [[312](#bib.bib312), [313](#bib.bib313), [314](#bib.bib314),
    [315](#bib.bib315), [316](#bib.bib316)]. These works mainly focus on achieving
    a state-of-the-art performance with as less computational cost as possible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，提出了一些新思路来促进高效的语义分割。例如，使用自注意力层处理深层特征[[306](#bib.bib306), [307](#bib.bib307),
    [308](#bib.bib308)]，设计带有NAS的分割模型[[309](#bib.bib309), [310](#bib.bib310), [311](#bib.bib311)]，以及根据输入调整解码器的架构[[233](#bib.bib233)]。最近，相当多的论文尝试在ViTs之上设计高效的语义分割模型[[312](#bib.bib312),
    [313](#bib.bib313), [314](#bib.bib314), [315](#bib.bib315), [316](#bib.bib316)]。这些工作主要关注于以尽可能低的计算成本实现最先进的性能。
- en: 4.3 Instance Segmentation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实例分割
- en: Instance segmentation can be seen as a combination of object detection and semantic
    segmentation, where the model needs to detect the instances of objects, demarcate
    their boundaries and recognize their categories [[9](#bib.bib9), [317](#bib.bib317)].
    Existing works in this direction can be categorized into *two-stage* (Sec. 4.3.1)
    and *End-to-end* (Sec. 4.3.2).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割可以看作是目标检测和语义分割的结合，其中模型需要检测物体的实例，标定其边界并识别其类别 [[9](#bib.bib9), [317](#bib.bib317)]。这一方向的现有工作可以分为*两阶段*（第4.3.1节）和*端到端*（第4.3.2节）。
- en: 4.3.1 Two-stage Approaches
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 两阶段方法
- en: From the lens of efficiency, a notable milestone of deep-learning-based instance
    segmentation is the proposing of Mask R-CNN [[318](#bib.bib318)]. Mask R-CNN is
    developed by introducing mask segmentation branches on the basis of Faster R-CNN
    [[272](#bib.bib272)]. It enjoys high computational efficiency by directly obtaining
    the regions of interest from the feature maps. In contrast, MaskLab [[319](#bib.bib319)]
    improved Faster R-CNN by adding the semantic segmentation and direction prediction
    paths. To improve the accuracy of Mask R-CNN, MS R-CNN [[320](#bib.bib320)] predicts
    the quality of the predicted instance masks and prioritizes more accurate mask
    predictions during validation. PANet [[321](#bib.bib321)] introduces a path augmentation
    mechanism to facilitate the bottom-up information interaction of feature maps.
    HTC [[322](#bib.bib322)] proposes a hybrid task cascade framework to learn more
    discriminative features progressively while integrating complementary features
    in the meantime.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从效率的角度来看，基于深度学习的实例分割的一个重要里程碑是Mask R-CNN [[318](#bib.bib318)]的提出。Mask R-CNN是在Faster
    R-CNN [[272](#bib.bib272)]的基础上引入了掩码分割分支。它通过直接从特征图中获取兴趣区域来实现高计算效率。相比之下，MaskLab
    [[319](#bib.bib319)] 通过添加语义分割和方向预测路径来改进Faster R-CNN。为了提高Mask R-CNN的准确性，MS R-CNN
    [[320](#bib.bib320)] 预测预测的实例掩码的质量，并在验证期间优先考虑更准确的掩码预测。PANet [[321](#bib.bib321)]
    引入了一种路径增强机制，以促进特征图的自下而上的信息交互。HTC [[322](#bib.bib322)] 提出了一个混合任务级联框架，以逐步学习更具辨别性的特征，同时整合互补特征。
- en: 4.3.2 End-to-end Approaches
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 端到端方法
- en: Another liner of works focus on realizing efficient end-to-end instance segmentation.
    SOLO [[45](#bib.bib45), [46](#bib.bib46)] achieves this by introducing the “instance
    categories”, which assigns categories to each pixel within an instance according
    to the instance’s location and size, thus converting instance segmentation into
    a pure dense classification problem. YOLACT [[323](#bib.bib323)] and BlendMask
    [[324](#bib.bib324)] propose to first generate a set of prototype masks, and then
    combines them with per-instance mask coefficients or attention scores. Inspired
    by SSD [[41](#bib.bib41)] and RetinaNet [[281](#bib.bib281)], TensorMask [[325](#bib.bib325)]
    build an efficient sliding-window-based instance segmentation framework.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个工作方向集中于实现高效的端到端实例分割。SOLO [[45](#bib.bib45), [46](#bib.bib46)] 通过引入“实例类别”来实现这一点，该方法根据实例的位置和大小为实例内的每个像素分配类别，从而将实例分割转换为纯粹的密集分类问题。YOLACT
    [[323](#bib.bib323)] 和 BlendMask [[324](#bib.bib324)] 提出了首先生成一组原型掩码，然后将它们与每个实例的掩码系数或注意力分数相结合。受SSD
    [[41](#bib.bib41)] 和 RetinaNet [[281](#bib.bib281)] 启发，TensorMask [[325](#bib.bib325)]
    建立了一个高效的基于滑动窗口的实例分割框架。
- en: 5 Model Compression Techniques
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 模型压缩技术
- en: Deep networks necessitate substantial resources, including energy, processing
    capacity, and storage. These resource requirements diminish the suitability of
    deep networks for resource-constrained devices [[326](#bib.bib326)]. Furthermore,
    the extensive resource requirements of deep networks become a bottleneck for real-time
    inference and executing deep networks on browser-based applications. To address
    these drawbacks of deep networks, various model compression techniques have been
    proposed in existing literature. Several comprehensive reviews on model compression
    techniques exist [[327](#bib.bib327), [328](#bib.bib328)]. These reviews categorize
    model compression techniques, discuss challenges, provide overviews, solutions,
    and future directions of model compression techniques. We adopt their classification
    structure but place a greater emphasis on vision-related works. Specifically,
    we categorize existing research into network pruning [[329](#bib.bib329), [330](#bib.bib330)],
    network quantization [[331](#bib.bib331), [330](#bib.bib330)], low-rank decomposition
    [[332](#bib.bib332), [333](#bib.bib333)], knowledge distillation [[51](#bib.bib51),
    [334](#bib.bib334), [335](#bib.bib335)], and other techniques [[336](#bib.bib336),
    [337](#bib.bib337)]. For readers interested in a particular category, we recommend
    consulting these more targeted reviews [[329](#bib.bib329), [330](#bib.bib330),
    [331](#bib.bib331), [52](#bib.bib52)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络需要大量资源，包括能源、处理能力和存储。这些资源需求减少了深度网络在资源受限设备上的适用性 [[326](#bib.bib326)]。此外，深度网络的广泛资源需求成为实时推理和在浏览器应用中执行深度网络的瓶颈。为了解决这些深度网络的缺陷，现有文献中提出了各种模型压缩技术。现有有几篇关于模型压缩技术的综合评审
    [[327](#bib.bib327), [328](#bib.bib328)]。这些评审对模型压缩技术进行了分类，讨论了挑战，提供了概述、解决方案和未来方向。我们采用了他们的分类结构，但更侧重于与视觉相关的工作。具体而言，我们将现有研究分为网络剪枝
    [[329](#bib.bib329), [330](#bib.bib330)]、网络量化 [[331](#bib.bib331), [330](#bib.bib330)]、低秩分解
    [[332](#bib.bib332), [333](#bib.bib333)]、知识蒸馏 [[51](#bib.bib51), [334](#bib.bib334),
    [335](#bib.bib335)] 和其他技术 [[336](#bib.bib336), [337](#bib.bib337)]。对于对特定类别感兴趣的读者，我们建议查阅这些更有针对性的评审
    [[329](#bib.bib329), [330](#bib.bib330), [331](#bib.bib331), [52](#bib.bib52)]。
- en: '![Refer to caption](img/0aaa8618fecc579b6e81920b46a8dee2.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0aaa8618fecc579b6e81920b46a8dee2.png)'
- en: 'Figure 5: The steps of network pruning.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 网络剪枝的步骤。'
- en: 5.1 Network Pruning
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 网络剪枝
- en: 'Network pruning is one of the most prevalent techniques for reducing the size
    of a deep learning model by eliminating inadequate components, such as channels,
    filters, neurons, or layers, resulting in a light-weighted model. Network pruning
    techniques can be categorized into four types: channel pruning, filter pruning,
    connection pruning, and layer pruning. These techniques help decrease the storage
    and computation requirements of deep networks. A typical pruning algorithm consists
    of two stages: evaluating and pruning unimportant parameters, followed by fine-tuning
    the pruned model to restore accuracy. The steps and categories are illustrated
    in Figure [5](#S5.F5 "Figure 5 ‣ 5 Model Compression Techniques ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey").'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '网络剪枝是通过去除不必要的组件（如通道、滤波器、神经元或层）来减少深度学习模型大小的最常见技术之一，从而得到一个轻量级模型。网络剪枝技术可以分为四种类型：通道剪枝、滤波器剪枝、连接剪枝和层剪枝。这些技术有助于减少深度网络的存储和计算需求。一个典型的剪枝算法包括两个阶段：评估和剪除不重要的参数，然后对剪枝后的模型进行微调以恢复准确性。步骤和类别在图
    [5](#S5.F5 "Figure 5 ‣ 5 Model Compression Techniques ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey") 中进行了说明。'
- en: In deep networks, the inputs provided to each layer are channeled. Channel pruning
    involves removing unimportant channels to reduce computation and storage requirements.
    Various channel pruning schemes have been proposed [[338](#bib.bib338), [47](#bib.bib47),
    [339](#bib.bib339)]. Convolutional operations in ConvNets incorporate a large
    number of filters to enhance performance. Increases in filter quantities result
    in a significant growth in the number of floating-point operations. Filter pruning
    eliminates unimportant filters, thus reducing computation [[340](#bib.bib340),
    [341](#bib.bib341), [332](#bib.bib332)]. The number of input and output connections
    to a layer in deep networks determines the number of parameters. These parameters
    can be used to estimate the storage and computation requirements of deep networks.
    Connection pruning is a direct approach to reduce parameters by removing unimportant
    connections [[342](#bib.bib342), [343](#bib.bib343), [344](#bib.bib344)]. Layer
    pruning involves selecting and deleting certain unimportant layers from the network,
    leading to ultra-high compression of the deep network. This is particularly useful
    for deploying deep networks on resource-constrained computing devices, where ultra-high
    compression is necessary. Some layer pruning approaches have been proposed to
    substantially reduce both storage and computation requirements [[53](#bib.bib53),
    [345](#bib.bib345)]. However, layer pruning may result in a higher accuracy compromise
    due to the structural deterioration of deep networks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度网络中，输入会被分配到每一层。通道剪枝涉及移除不重要的通道，以减少计算和存储需求。已经提出了各种通道剪枝方案[[338](#bib.bib338),
    [47](#bib.bib47), [339](#bib.bib339)]。ConvNets中的卷积操作包含大量滤波器，以提高性能。滤波器数量的增加会导致浮点操作数量的显著增长。滤波器剪枝通过去除不重要的滤波器来减少计算[[340](#bib.bib340),
    [341](#bib.bib341), [332](#bib.bib332)]。深度网络中每层的输入和输出连接的数量决定了参数的数量。这些参数可以用来估计深度网络的存储和计算需求。连接剪枝是一种直接减少参数的方法，通过去除不重要的连接[[342](#bib.bib342),
    [343](#bib.bib343), [344](#bib.bib344)]。层剪枝涉及选择和删除网络中某些不重要的层，从而实现深度网络的超高压缩。这对于在资源受限的计算设备上部署深度网络尤其有用，在这些情况下需要超高压缩。一些层剪枝方法已经提出，能够显著减少存储和计算需求[[53](#bib.bib53),
    [345](#bib.bib345)]。然而，层剪枝可能导致由于深度网络结构的退化而造成更高的精度折中。
- en: 5.2 Network Quantization
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 网络量化
- en: Network quantization aims to compress the original network by reducing the storage
    requirements of weights. It can be categorized into linear quantization and nonlinear
    quantization. Linear quantization focuses on minimizing the number of bits needed
    to represent each weight, while nonlinear quantization involves dividing weights
    into several groups, with each group sharing a single weight.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 网络量化旨在通过减少权重的存储需求来压缩原始网络。它可以分为线性量化和非线性量化。线性量化侧重于最小化表示每个权重所需的位数，而非线性量化则涉及将权重划分为多个组，每组共享一个权重。
- en: 5.2.1 Linear Quantization
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 线性量化
- en: Utilizing 32-bit floating-point numbers to represent weights consumes a substantial
    amount of resources. Consequently, linear quantization employs low-bit number
    representation to approximate each weight. Suyog *et al.* contend that the weights
    of deep networks can be represented by 16-bit fixed-point numbers without significantly
    reducing classification accuracy [[346](#bib.bib346)]. Some studies further compress
    ConvNets to 8-bit [[347](#bib.bib347), [348](#bib.bib348)]. In the extreme case
    of a 1-bit representation for each weight, binary weight neural networks emerge.
    The primary concept is to directly learn binary weights or activation during model
    training. Several works directly train ConvNets with binary weights, including
    BinaryConnect [[49](#bib.bib49)], BinaryNet[[349](#bib.bib349)], and XNOR [[50](#bib.bib50)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 利用32位浮点数表示权重会消耗大量资源。因此，线性量化采用低位数表示来逼近每个权重。Suyog *等人*认为深度网络的权重可以用16位定点数表示，而不会显著降低分类精度[[346](#bib.bib346)]。一些研究进一步将ConvNets压缩到8位[[347](#bib.bib347),
    [348](#bib.bib348)]。在每个权重1位表示的极端情况下，会出现二值权重神经网络。主要概念是在模型训练过程中直接学习二值权重或激活。几个研究直接训练具有二值权重的ConvNets，包括BinaryConnect[[49](#bib.bib49)]、BinaryNet[[349](#bib.bib349)]和XNOR[[50](#bib.bib50)]。
- en: 5.2.2 Nonlinear Quantization
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 非线性量化
- en: Nonlinear quantization entails dividing weights into several groups, with each
    group sharing a single weight. Gong *et al.* initially employ the k-means algorithm
    to cluster weight parameters and replace the parameter values with the clustering
    center values, substantially reducing the network’s storage space [[350](#bib.bib350)].
    Wu *et al.* further quantize convolution filters, fully connected layers, and
    other parameters [[351](#bib.bib351)]. Chen *et al.* randomly assign weights to
    hash buckets, with each hash bucket sharing a single weight [[352](#bib.bib352)].
    Han *et al.* combine network pruning, parameter quantization, and Huffman coding
    to achieve significant reductions in storage and memory [[353](#bib.bib353)].
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性量化涉及将权重划分为多个组，每组共享一个权重。Gong *等* 最初使用 k-means 算法对权重参数进行聚类，并用聚类中心值替代参数值，从而显著减少网络的存储空间[[350](#bib.bib350)]。Wu
    *等* 进一步量化卷积滤波器、全连接层和其他参数[[351](#bib.bib351)]。Chen *等* 随机将权重分配到哈希桶中，每个哈希桶共享一个权重[[352](#bib.bib352)]。Han
    *等* 结合网络剪枝、参数量化和霍夫曼编码，显著降低了存储和内存需求[[353](#bib.bib353)]。
- en: 5.3 Knowledge Distillation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 知识蒸馏
- en: 'Knowledge distillation (KD) [[51](#bib.bib51)] is a widely adopted technique
    for transferring “dark knowledge” from a high-capacity model (teacher) to a more
    compact model (student) in order to achieve various types of efficiency. The two
    primary aspects of KD are knowledge representation and distillation schemes. In
    this section, we concentrate on existing research in these two technical areas
    and further summarize the theoretical exploration and application progress of
    KD in computer vision, as illustrated in Figure [6](#S5.F6 "Figure 6 ‣ 5.3 Knowledge
    Distillation ‣ 5 Model Compression Techniques ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）[[51](#bib.bib51)]是一种广泛采用的技术，用于将高容量模型（教师）中的“黑暗知识”转移到更紧凑的模型（学生）中，以实现各种效率目标。KD
    的两个主要方面是知识表示和蒸馏方案。在这一部分，我们集中讨论了这两个技术领域的现有研究，并进一步总结了 KD 在计算机视觉中的理论探索和应用进展，如图 [6](#S5.F6
    "图 6 ‣ 5.3 知识蒸馏 ‣ 5 模型压缩技术 ‣ 计算高效的深度学习：计算机视觉的调查") 所示。
- en: '![Refer to caption](img/42a002b2cc4b48a6adb3a2d2b186ec3b.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42a002b2cc4b48a6adb3a2d2b186ec3b.png)'
- en: 'Figure 6: Knowledge distillation. The section mainly contains knowledge representation,
    distillation schemes, theory and applications.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 知识蒸馏。该部分主要包含知识表示、蒸馏方案、理论和应用。'
- en: 5.3.1 Knowledge Representation
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 知识表示
- en: 'Drawing on [[52](#bib.bib52)], we examine different forms of knowledge in the
    following categories: response-based knowledge, feature-based knowledge, and relation-based
    knowledge. Response-based knowledge typically refers to the neural response of
    the teacher model’s final output layer, with the main idea being to directly emulate
    the teacher model’s final prediction. The most prevalent response-based knowledge
    for image classification is soft targets [[51](#bib.bib51)]. In object detection
    tasks, the response may include logits along with the bounding box offset [[354](#bib.bib354)].
    For semantic landmark localization tasks, such as human pose estimation, the teacher
    model’s response may consist of a heatmap for each landmark [[355](#bib.bib355)].'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[52](#bib.bib52)]，我们研究了以下几类知识的不同形式：基于响应的知识、基于特征的知识和基于关系的知识。基于响应的知识通常指的是教师模型最终输出层的神经响应，主要思想是直接模拟教师模型的最终预测。图像分类中最常见的基于响应的知识是软目标[[51](#bib.bib51)]。在目标检测任务中，响应可能包括
    logits 以及边界框偏移[[354](#bib.bib354)]。对于语义标志点定位任务，如人体姿态估计，教师模型的响应可能包括每个标志点的热图[[355](#bib.bib355)]。
- en: Feature-based knowledge pertains to the feature representation derived from
    intermediate layers. Fitnets [[334](#bib.bib334)] are the first to introduce intermediate
    representations, which subsequently inspire the development of various methods
    [[356](#bib.bib356), [357](#bib.bib357), [358](#bib.bib358), [359](#bib.bib359),
    [360](#bib.bib360), [361](#bib.bib361), [362](#bib.bib362), [363](#bib.bib363)].
    Relation-based knowledge further investigates the relationships between different
    feature layers [[364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366)] or
    data samples [[367](#bib.bib367), [368](#bib.bib368), [369](#bib.bib369), [335](#bib.bib335)].
    For instance, Yim *et al.* [[364](#bib.bib364)] propose calculating the relations
    between pairs of feature maps using the Gram matrix, while Liu *et al.* [[367](#bib.bib367)]
    suggest transferring the instance relationship graph, which defines instance features
    and relationships as vertices and edges, respectively.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的知识涉及从中间层提取的特征表示。Fitnets [[334](#bib.bib334)]首次引入了中间表示，这随后激发了各种方法的发展[[356](#bib.bib356),
    [357](#bib.bib357), [358](#bib.bib358), [359](#bib.bib359), [360](#bib.bib360),
    [361](#bib.bib361), [362](#bib.bib362), [363](#bib.bib363)]。基于关系的知识进一步研究了不同特征层[[364](#bib.bib364),
    [365](#bib.bib365), [366](#bib.bib366)]或数据样本[[367](#bib.bib367), [368](#bib.bib368),
    [369](#bib.bib369), [335](#bib.bib335)]之间的关系。例如，Yim *et al.* [[364](#bib.bib364)]建议使用Gram矩阵计算特征图对之间的关系，而Liu
    *et al.* [[367](#bib.bib367)]则建议转移实例关系图，将实例特征和关系分别定义为顶点和边。
- en: 5.3.2 Distillation Schemes
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 蒸馏方案
- en: 'The learning schemes of knowledge distillation can be classified into three
    main categories based on the synchronization of the teacher model’s update with
    the student model: offline distillation, online distillation, and self-distillation.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的学习方案可以根据教师模型更新与学生模型同步的方式分为三大类：离线蒸馏、在线蒸馏和自我蒸馏。
- en: In offline distillation, the teacher model is usually assumed to be pre-trained.
    The primary focus of offline methods is to enhance various aspects of knowledge
    transfer, including knowledge representation and the design of loss functions.
    Vanilla knowledge distillation [[51](#bib.bib51)] serves as a classic example
    of offline distillation methods. Most prior knowledge distillation methods operate
    in an offline manner.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线蒸馏中，通常假设教师模型已经预训练。离线方法的主要关注点是提升知识转移的各个方面，包括知识表示和损失函数的设计。Vanilla知识蒸馏[[51](#bib.bib51)]是离线蒸馏方法的经典示例。大多数先前的知识蒸馏方法都是以离线方式进行的。
- en: In cases where a high-capacity, high-performance teacher model is unavailable,
    online distillation provides an alternative. In this approach, both the teacher
    model and the student model are updated simultaneously, allowing for an end-to-end
    trainable knowledge distillation framework. Deep mutual learning [[370](#bib.bib370)]
    introduced a method for training multiple neural networks collaboratively, where
    any given network can serve as the student model while the others act as teachers.
    Numerous online knowledge distillation methods have been proposed [[371](#bib.bib371),
    [372](#bib.bib372), [373](#bib.bib373)], with multi-branch architecture [[371](#bib.bib371)]
    and ensemble techniques [[372](#bib.bib372), [374](#bib.bib374)] being widely
    adopted.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当高容量、高性能的教师模型不可用时，在线蒸馏提供了一种替代方案。在这种方法中，教师模型和学生模型会同时更新，从而形成一个端到端可训练的知识蒸馏框架。深度互学习[[370](#bib.bib370)]提出了一种训练多个神经网络协同工作的办法，其中任何给定的网络都可以作为学生模型，而其他网络则充当教师。已经提出了许多在线知识蒸馏方法[[371](#bib.bib371),
    [372](#bib.bib372), [373](#bib.bib373)]，其中多分支架构[[371](#bib.bib371)]和集成技术[[372](#bib.bib372),
    [374](#bib.bib374)]被广泛采用。
- en: Self-distillation refers to a learning process in which the student model acquires
    knowledge independently, without the presence of teacher models, whether pre-trained
    or virtual. Several studies have explored this idea in various contexts. For instance,
    Zhang *et al.* [[375](#bib.bib375)] propose a method for distilling knowledge
    from deeper layers to shallower ones for image classification tasks. Similarly,
    Hou *et al.* [[376](#bib.bib376)] employ attention maps from deeper layers as
    distillation targets for lower layers in object detection tasks. In contrast,
    Yang *et al.* [[377](#bib.bib377)] introduce snapshot distillation, where checkpoints
    from earlier epochs are considered as teachers to distill knowledge for the models
    in later epochs. Additionally, Wang *et al.* [[362](#bib.bib362)] suggest constraining
    the outputs of the backbone network using target class activation maps.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 自我蒸馏指的是一种学习过程，其中学生模型独立地获取知识，而没有教师模型的存在，无论是预训练的还是虚拟的。一些研究在不同的背景下探索了这一思想。例如，张*等*[[375](#bib.bib375)]提出了一种将知识从深层传递到浅层的图像分类任务方法。类似地，侯*等*[[376](#bib.bib376)]在目标检测任务中使用来自深层的注意力图作为低层的蒸馏目标。相比之下，杨*等*[[377](#bib.bib377)]引入了快照蒸馏，其中早期时期的检查点被视为教师，用于对后期时期的模型进行知识蒸馏。此外，王*等*[[362](#bib.bib362)]建议使用目标类激活图来约束骨干网络的输出。
- en: 5.3.3 Theory and Applications
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 理论与应用
- en: A wide range of knowledge distillation methods has been extensively employed
    in vision applications. Initially, most knowledge distillation methods were developed
    for image classification [[51](#bib.bib51), [364](#bib.bib364), [378](#bib.bib378),
    [379](#bib.bib379), [380](#bib.bib380)] and later extended to other vision tasks,
    including face recognition [[381](#bib.bib381), [361](#bib.bib361)], action recognition
    [[382](#bib.bib382), [383](#bib.bib383)], object detection [[354](#bib.bib354),
    [384](#bib.bib384), [385](#bib.bib385)], semantic segmentation [[386](#bib.bib386),
    [387](#bib.bib387), [388](#bib.bib388), [389](#bib.bib389)], depth estimation
    [[390](#bib.bib390), [391](#bib.bib391), [392](#bib.bib392), [393](#bib.bib393),
    [394](#bib.bib394), [395](#bib.bib395)], image retrieval [[396](#bib.bib396),
    [397](#bib.bib397)], video captioning [[398](#bib.bib398), [399](#bib.bib399),
    [400](#bib.bib400)], and video classification [[401](#bib.bib401), [402](#bib.bib402)],
    among others.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 大范围的知识蒸馏方法已被广泛应用于视觉任务中。最初，大多数知识蒸馏方法是为图像分类[[51](#bib.bib51), [364](#bib.bib364),
    [378](#bib.bib378), [379](#bib.bib379), [380](#bib.bib380)]开发的，后来扩展到其他视觉任务，包括人脸识别[[381](#bib.bib381),
    [361](#bib.bib361)]、动作识别[[382](#bib.bib382), [383](#bib.bib383)]、目标检测[[354](#bib.bib354),
    [384](#bib.bib384), [385](#bib.bib385)]、语义分割[[386](#bib.bib386), [387](#bib.bib387),
    [388](#bib.bib388), [389](#bib.bib389)]、深度估计[[390](#bib.bib390), [391](#bib.bib391),
    [392](#bib.bib392), [393](#bib.bib393), [394](#bib.bib394), [395](#bib.bib395)]、图像检索[[396](#bib.bib396),
    [397](#bib.bib397)]、视频字幕生成[[398](#bib.bib398), [399](#bib.bib399), [400](#bib.bib400)]和视频分类[[401](#bib.bib401),
    [402](#bib.bib402)]等。
- en: Despite the significant practical success, relatively few works have focused
    on the theoretical or empirical understanding of knowledge distillation [[403](#bib.bib403),
    [404](#bib.bib404), [405](#bib.bib405), [379](#bib.bib379)]. Hinton *et al.* [[51](#bib.bib51)]
    suggest that the success of KD could be attributed to learning similarities between
    categories. Yuan *et al.* [[403](#bib.bib403)] posited that dark knowledge not
    only encompasses category similarities but also imposes regularization on student
    training. They indicate that KD is a learned label smoothing regularization (LSR).
    Tang *et al.* [[404](#bib.bib404)] propose approach where, in addition to regularization
    and class relationships, another type of knowledge, instance-specific knowledge,
    is also used by the teacher to rescale the student model’s per-instance gradients.
    Chen *et al.* [[405](#bib.bib405)] quantify the extraction of visual concepts
    from the intermediate layers of a deep learning model to explain knowledge distillation.
    Wang *et al.* [[379](#bib.bib379)] connect KD with the information bottleneck
    and empirically validate that preserving more mutual information between feature
    representation and input is more important than improving the teacher model’s
    accuracy. Overall, theoretical research remains limited compared to the diverse
    and numerous applications.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了显著的实际成功，但相对较少的工作关注于知识蒸馏的理论或实证理解 [[403](#bib.bib403), [404](#bib.bib404),
    [405](#bib.bib405), [379](#bib.bib379)]。Hinton *等人* [[51](#bib.bib51)] 提出KD的成功可以归因于学习类别之间的相似性。Yuan
    *等人* [[403](#bib.bib403)] 假设暗知识不仅涵盖了类别相似性，还对学生训练施加了正则化。他们指出KD是一种学习的标签平滑正则化（LSR）。Tang
    *等人* [[404](#bib.bib404)] 提出了在正则化和类别关系之外，教师还使用另一种类型的知识，即特定实例知识，以重新调整学生模型的每实例梯度。Chen
    *等人* [[405](#bib.bib405)] 量化了从深度学习模型的中间层提取视觉概念，以解释知识蒸馏。Wang *等人* [[379](#bib.bib379)]
    将KD与信息瓶颈联系起来，并实证验证了在特征表示与输入之间保留更多互信息比提高教师模型的准确性更为重要。总体而言，相比于多样且众多的应用，理论研究仍然有限。
- en: 5.4 Low-rank Factorization
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 低秩分解
- en: Convolution kernels can be viewed as 3D tensors. Ideas based on tensor decomposition
    are derived from the intuition that there is structural sparsity in the 3D tensor.
    In the case of fully connected layers, they can be viewed as 2D matrices (or 3D
    tensors), and low-rankness can also be helpful. The key idea of low-rank factorization
    is to find an approximate low-rank tensor that is close to the real tensor and
    easy to decompose. Low-rank factorization is beneficial for both tensors and matrices.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核可以被视为3D张量。基于张量分解的思想源于这样一种直觉，即3D张量中存在结构性稀疏。在全连接层的情况下，它们可以被视为2D矩阵（或3D张量），低秩性也同样有帮助。低秩分解的关键思想是找到一个接近真实张量且易于分解的近似低秩张量。低秩分解对张量和矩阵都是有益的。
- en: There are several typical low-rank methods for compressing 3D convolutional
    layers. Lebedev *et al.* [[406](#bib.bib406)] propose Canonical Polyadic (CP)
    decomposition for kernel tensors. They use nonlinear least squares to compute
    the CP decomposition for a better low-rank approximation. Since low-rank tensor
    decomposition is a non-convex problem and generally difficult to compute, Jaderberg
    *et al.* use iterative schemes to obtain an approximate local solution [[333](#bib.bib333)].
    Then, Tai *et al.* find that the particular form of low-rank decomposition in
    [[333](#bib.bib333)] has an exact closed-form solution, which is the global optimum,
    and present a method for training low-rank constrained ConvNets from scratch [[407](#bib.bib407)].
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩3D卷积层有几种典型的低秩方法。Lebedev *等人* [[406](#bib.bib406)] 提出了用于核张量的典型多项式（CP）分解。他们使用非线性最小二乘法计算CP分解，以获得更好的低秩近似。由于低秩张量分解是一个非凸问题，通常难以计算，Jaderberg
    *等人* 使用迭代方案来获得近似的局部解 [[333](#bib.bib333)]。然后，Tai *等人* 发现 [[333](#bib.bib333)]
    中低秩分解的特定形式有一个精确的闭式解，这就是全局最优解，并提出了一种从头开始训练低秩约束ConvNets的方法 [[407](#bib.bib407)]。
- en: Many classical works have exploited low-rankness in fully connected layers.
    Denil *et al.* reduce the number of dynamic parameters in deep models using the
    low-rank method [[408](#bib.bib408)]. Zhang *et al.* introduce a Tucker decomposition
    model to compress weight tensors in fully connected layers [[409](#bib.bib409)].
    Lu *et al.* adopt truncated singular value decomposition to decompose the fully
    connected layer for designing compact multi-task deep learning architectures [[410](#bib.bib410)].
    Sainath *et al.* explore a low-rank matrix factorization of the final weight layer
    in deep networks for acoustic modeling [[411](#bib.bib411)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 许多经典工作利用了全连接层中的低秩性。Denil *等人* 使用低秩方法减少深度模型中的动态参数[[408](#bib.bib408)]。Zhang *等人*
    引入了Tucker分解模型来压缩全连接层中的权重张量[[409](#bib.bib409)]。Lu *等人* 采用截断奇异值分解来分解全连接层，以设计紧凑的多任务深度学习架构[[410](#bib.bib410)]。Sainath
    *等人* 探索了深度网络中最终权重层的低秩矩阵分解，用于声学建模[[411](#bib.bib411)]。
- en: 5.5 Hybrid Techniques
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 混合技术
- en: 'Apart from the four categories of mainstream techniques mentioned above, there
    are other techniques for network compression. Some studies have attempted to integrate
    orthogonal techniques to achieve more significant performance [[353](#bib.bib353),
    [412](#bib.bib412), [413](#bib.bib413)]. Some works have designed compact networks
    [[26](#bib.bib26), [64](#bib.bib64), [89](#bib.bib89)] or efficient convolutions
    [[86](#bib.bib86), [72](#bib.bib72)], which have been discussed in Sec. [2](#S2
    "2 Architecture Design of Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey").'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '除了上述提到的四类主流技术，还有其他网络压缩技术。一些研究尝试整合正交技术以实现更显著的性能[[353](#bib.bib353)、[412](#bib.bib412)、[413](#bib.bib413)]。一些工作设计了紧凑的网络[[26](#bib.bib26)、[64](#bib.bib64)、[89](#bib.bib89)]或高效的卷积[[86](#bib.bib86)、[72](#bib.bib72)]，这些内容在第[2](#S2
    "2 Architecture Design of Backbone Networks ‣ Computation-efficient Deep Learning
    for Computer Vision: A Survey")节中讨论过。'
- en: 6 Efficient Deployment on Hardware
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 硬件上的高效部署
- en: 'The aforementioned works mostly design network architectures based on their
    theoretical computation (*e.g.* floating operations, FLOPs). However, there is
    often a gap between theoretical computation and practical latency on hardware
    devices [[27](#bib.bib27), [64](#bib.bib64)]. Realistic efficiency can be influenced
    by other factors such as hardware properties and scheduling strategies. Along
    this direction, we review relative works from the following perspectives: 1) hardware-aware
    neural architecture search (Sec. [6.1](#S6.SS1 "6.1 Hardware-aware Model Design
    ‣ 6 Efficient Deployment on Hardware ‣ Computation-efficient Deep Learning for
    Computer Vision: A Survey")); 2) acceleration software libraries and hardware
    design (Sec. [6.2](#S6.SS2 "6.2 Acceleration Tools ‣ 6 Efficient Deployment on
    Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey"));
    and 3) algorithm-software codesign techniques.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '上述工作大多基于理论计算（*例如* 浮点运算，FLOPs）设计网络架构。然而，理论计算与硬件设备上的实际延迟之间往往存在差距[[27](#bib.bib27)、[64](#bib.bib64)]。现实中的效率可以受到其他因素的影响，如硬件属性和调度策略。在这个方向上，我们从以下几个方面回顾相关工作：1)
    硬件感知的神经架构搜索（第[6.1](#S6.SS1 "6.1 Hardware-aware Model Design ‣ 6 Efficient Deployment
    on Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")节）；2)
    加速软件库和硬件设计（第[6.2](#S6.SS2 "6.2 Acceleration Tools ‣ 6 Efficient Deployment on
    Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")节）；3)
    算法-软件共设计技术。'
- en: 6.1 Hardware-aware Model Design
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 硬件感知的模型设计
- en: As the practical latency of models can be influenced by many factors other than
    theoretical computation, the commonly used FLOPs is an inaccurate proxy for network
    efficiency. Ideally, one should develop efficient models based on specific hardware
    properties. However, hand-designing networks for different hardware devices can
    be laborious. Therefore, automatically *searching* for efficient architectures
    is emerging as a promising direction. Compared to the traditional NAS methods
    [[31](#bib.bib31), [414](#bib.bib414)], this line of works can generate appropriate
    models which satisfy different hardware constraints and gain realistic efficiency
    in practice. For example, ProxylessNAS [[54](#bib.bib54)] establishes a latency
    prediction function based on realistic tests on targeted hardware, and the predicted
    latency is then directly used as a regularization item in the NAS objective. A
    similar idea is also implemented by MnasNet [[53](#bib.bib53)] to search for efficient
    models on mobile devices. The following works FBNet [[159](#bib.bib159)], FBNet-v2
    [[415](#bib.bib415)] and OFA [[416](#bib.bib416)] have improved NAS techniques.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的实际延迟可能受到理论计算之外的许多因素的影响，常用的 FLOPs 并不准确地反映网络效率。理想情况下，应根据特定硬件属性开发高效的模型。然而，为不同硬件设备手动设计网络可能是费力的。因此，自动*搜索*高效架构正成为一个有前景的方向。与传统的
    NAS 方法 [[31](#bib.bib31), [414](#bib.bib414)] 相比，这类研究可以生成符合不同硬件约束条件的适当模型，并在实践中获得实际的效率。例如，ProxylessNAS
    [[54](#bib.bib54)] 基于对目标硬件的实际测试建立了一个延迟预测函数，然后将预测的延迟直接作为 NAS 目标中的正则化项。MnasNet [[53](#bib.bib53)]
    也实现了类似的思想，以在移动设备上搜索高效模型。以下工作 FBNet [[159](#bib.bib159)]、FBNet-v2 [[415](#bib.bib415)]
    和 OFA [[416](#bib.bib416)] 改进了 NAS 技术。
- en: 'Apart from the traditional static models, the hardware-aware design paradigm
    has also been applied to develop *spatial-wise dynamic networks* (Sec. [3.2](#S3.SS2
    "3.2 Spatial-wise Dynamic Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient
    Deep Learning for Computer Vision: A Survey")) [[39](#bib.bib39)].'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '除了传统的静态模型，硬件感知设计范式也已应用于开发*空间动态网络*（第 [3.2](#S3.SS2 "3.2 Spatial-wise Dynamic
    Networks ‣ 3 Dynamic Backbone Networks ‣ Computation-efficient Deep Learning for
    Computer Vision: A Survey")节） [[39](#bib.bib39)]。'
- en: Note that we mainly give a brief introduction of basic ideas in this work due
    to the page limit. For more detailed techniques we refer the readers to the survey
    [[417](#bib.bib417)] which specifically focuses on this topic.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于篇幅限制，我们主要简要介绍了这项工作的基本思想。有关更详细的技术，请参阅专门关注该主题的综述 [[417](#bib.bib417)]。
- en: 6.2 Acceleration Tools
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 加速工具
- en: In addition to architectural design, the efficient deployment of algorithms
    on hardwares also requires acceleration software libraries or specific hardware
    accelerators.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 除了架构设计，算法在硬件上的高效部署还需要加速软件库或特定的硬件加速器。
- en: 1) Software Libraries. Extensive efforts have been made to accelerate model
    inference on different hardware platforms. For example, NVIDIA TensorRT [[57](#bib.bib57)]
    is widely used to deploy models for optimized inference on GPUs. NNPACK (https://github.com/Maratyszcza/NNPACK.),
    CoreML [[58](#bib.bib58)] and TinyEngine [[418](#bib.bib418)] are representative
    tools on multi-core CPUs, Apple silicons, and microcontrollers (MCUs), respectively.
    Cross-platform tools such as Tencent TNN (https://github.com/Tencent/TNN). and
    Apache TVM [[59](#bib.bib59)] have also emerged as popular development tools.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 软件库。已经做了大量工作来加速不同硬件平台上的模型推理。例如，NVIDIA TensorRT [[57](#bib.bib57)] 广泛用于在 GPU
    上部署优化推理的模型。NNPACK (https://github.com/Maratyszcza/NNPACK.)、CoreML [[58](#bib.bib58)]
    和 TinyEngine [[418](#bib.bib418)] 分别是多核 CPU、苹果芯片和微控制器（MCUs）上的代表性工具。跨平台工具如 Tencent
    TNN (https://github.com/Tencent/TNN). 和 Apache TVM [[59](#bib.bib59)] 也成为了流行的开发工具。
- en: 2) Hardware Accelerators. Apart from adapting neural architectures to given
    hardware devices, another line of works studies accelerators from the hardware
    perspective to enable fast inference of deep models. For example, DianNao [[419](#bib.bib419)]
    focuses on memory behavior and proposes an accelerator that simultaneously improves
    the inference speed and energy consumption of deep models. An FPGA-based accelerator
    is proposed quantitatively analyze the throughput of CNNs with the help of the
    classical roofline model [[420](#bib.bib420)]. In addition to the regular deep
    networks, researchers have also proposed accelerators to improve the inference
    efficiency of spatially sparse convolution [[421](#bib.bib421), [422](#bib.bib422)].
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 硬件加速器。除了将神经网络架构适配到给定的硬件设备上，另一类研究从硬件角度研究加速器，以实现深度模型的快速推理。例如，DianNao [[419](#bib.bib419)]
    关注内存行为，并提出了一种加速器，该加速器能够同时提高深度模型的推理速度和能耗。基于 FPGA 的加速器被提出，用于定量分析 CNN 的吞吐量，借助经典的屋顶线模型
    [[420](#bib.bib420)]。除了常规的深度网络外，研究人员还提出了加速器，以提高空间稀疏卷积的推理效率 [[421](#bib.bib421),
    [422](#bib.bib422)]。
- en: 6.3 Algorithm-Hardware Co-design
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 算法-硬件协同设计
- en: 'The aforementioned methods typically improve the inference efficiency from
    the perspective of either algorithm or hardware. Ideally, one should expect algorithms
    and hardware can “cooperate” with each other to further push forward the Pareto
    frontier between accuracy and efficiency trade-off. Along this direction, extensive
    efforts have been made based on the highly flexible and versatile Field Programmable
    Gate Arrays (FPGA) platform, and NAS techniques (Sec. [6.1](#S6.SS1 "6.1 Hardware-aware
    Model Design ‣ 6 Efficient Deployment on Hardware ‣ Computation-efficient Deep
    Learning for Computer Vision: A Survey")) are widely used to search for hardware-friendly
    network structures [[423](#bib.bib423), [55](#bib.bib55), [424](#bib.bib424),
    [56](#bib.bib56), [425](#bib.bib425)]. The recent MCUNet series [[418](#bib.bib418),
    [426](#bib.bib426), [427](#bib.bib427)] has enabled both inference and training
    on MCUs based on algorithm-hardware co-design with the help of their proposed
    tiny-Engine tool (Sec. [6.2](#S6.SS2 "6.2 Acceleration Tools ‣ 6 Efficient Deployment
    on Hardware ‣ Computation-efficient Deep Learning for Computer Vision: A Survey")).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法通常从算法或硬件的角度提高推理效率。理想情况下，应该期望算法和硬件能够“协作”以进一步推动准确性和效率权衡之间的帕累托前沿。在这方面，基于高度灵活和多功能的现场可编程门阵列（FPGA）平台已进行了广泛的工作，并且
    NAS 技术（第 [6.1](#S6.SS1 "6.1 硬件感知模型设计 ‣ 6 硬件上的高效部署 ‣ 计算高效的计算机视觉深度学习：综述") 节）被广泛用于搜索硬件友好的网络结构
    [[423](#bib.bib423), [55](#bib.bib55), [424](#bib.bib424), [56](#bib.bib56), [425](#bib.bib425)]。最近的
    MCUNet 系列 [[418](#bib.bib418), [426](#bib.bib426), [427](#bib.bib427)] 通过其提出的
    tiny-Engine 工具（第 [6.2](#S6.SS2 "6.2 加速工具 ‣ 6 硬件上的高效部署 ‣ 计算高效的计算机视觉深度学习：综述") 节）实现了基于算法-硬件协同设计的
    MCU 上的推理和训练。
- en: The co-designing method has also been applied to the field of dynamic neural
    networks, especially for efficient spatially adaptive convolution [[428](#bib.bib428),
    [429](#bib.bib429), [430](#bib.bib430)] and attention [[431](#bib.bib431)] operations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 协同设计方法也已应用于动态神经网络领域，尤其是高效的空间自适应卷积 [[428](#bib.bib428), [429](#bib.bib429), [430](#bib.bib430)]
    和注意力 [[431](#bib.bib431)] 操作。
- en: 7 Challenges and Future Directions
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 挑战与未来方向
- en: Despite the significant advances in the field of computationally efficient deep
    learning in recent years, numerous open challenges warrant further research. In
    this section, we summarize these challenges and discuss potential future directions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来在计算高效深度学习领域取得了显著进展，但仍有许多开放性挑战需要进一步研究。在本节中，我们总结了这些挑战并讨论了潜在的未来方向。
- en: 7.1 Designing General-purpose Backbones
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 设计通用骨干网
- en: The efficient extraction of discriminative representations from raw inputs has
    been established as a critical cornerstone for practical deep learning applications,
    as demonstrated in the existing literature. Light-weighted backbone networks are
    commonly employed to achieve this goal. As a result, a significant challenge lies
    in the design of efficient, general-purpose backbones. Potential avenues of investigation
    in this area encompass enhancing current convolution and self-attention operators
    via manual design [[26](#bib.bib26), [7](#bib.bib7)], employing automated architecture
    search methodologies [[24](#bib.bib24)], and amalgamating these approaches to
    create comprehensive efficient modules [[90](#bib.bib90)]. Specifically, the exploration
    of innovative information aggregation methods beyond convolution and self-attention
    appears promising, for instance, clustering algorithms [[432](#bib.bib432)], LSTM
    [[433](#bib.bib433)], and graph convolution [[434](#bib.bib434)]. Moreover, an
    emerging area of interest involves enabling backbone networks to accommodate multi-modal
    inputs (*e.g.*, text, images, and videos) and execute multiple visual tasks (*e.g.*,
    retrieval, classification, and visual question answering) [[435](#bib.bib435),
    [436](#bib.bib436)]. Consequently, the development of mobile-level multi-modal
    and multi-task visual foundation models could present an intriguing direction
    for future research.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始输入中高效提取区分性表示已被确立为实际深度学习应用的关键基石，如现有文献所示。通常使用轻量级主干网络来实现这一目标。因此，设计高效通用主干网络是一个重大挑战。在这个领域的潜在研究方向包括通过手动设计
    [[26](#bib.bib26), [7](#bib.bib7)] 改进当前的卷积和自注意力操作，采用自动化架构搜索方法 [[24](#bib.bib24)]，以及将这些方法结合起来创建综合的高效模块
    [[90](#bib.bib90)]。特别是，探索超越卷积和自注意力的创新信息聚合方法，如聚类算法 [[432](#bib.bib432)]、LSTM [[433](#bib.bib433)]
    和图卷积 [[434](#bib.bib434)]，显示出良好的前景。此外，一个新兴的兴趣领域是使主干网络能够处理多模态输入（*例如*，文本、图像和视频）并执行多个视觉任务（*例如*，检索、分类和视觉问答）
    [[435](#bib.bib435), [436](#bib.bib436)]。因此，开发移动级多模态和多任务视觉基础模型可能是未来研究的一个有趣方向。
- en: 7.2 Developing Task-specialized Models
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 开发任务专用模型
- en: In addition to the architectural advancements in backbone models, tailoring
    deep learning methodologies to specific computer vision tasks of interest has
    been demonstrated as crucial. Two research challenges of particular significance
    in this domain can be identified. Firstly, the exploitation of representations
    extracted by backbones to efficiently obtain task-specific features is essential,
    for example, multi-scale features for object detection and multi-path fused features
    for semantic segmentation. A potential solution to this challenge could involve
    designing specialized, efficient decoders (*e.g.*, utilizing NAS [[437](#bib.bib437),
    [311](#bib.bib311)]). Secondly, it is important to streamline the multi-stage
    design of visual tasks (*e.g.*, two-stage object detection [[273](#bib.bib273)]
    and instance segmentation [[318](#bib.bib318)] algorithms) to achieve end-to-end
    paradigms with minimal performance compromises. Additionally, the removal of time-consuming
    components, such as non-maximum suppression (NMS) [[8](#bib.bib8)], is crucial.
    A promising area for future research may involve the development of an efficient,
    unified, and end-to-end learnable interface for a majority of prevalent computer
    vision tasks [[438](#bib.bib438)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主干模型的架构进展之外，将深度学习方法量身定制为特定计算机视觉任务已经被证明至关重要。在这一领域，可以识别出两个特别重要的研究挑战。首先，利用主干模型提取的表示来高效地获得任务特定特征是必要的，例如，用于目标检测的多尺度特征和用于语义分割的多路径融合特征。应对这一挑战的一个潜在解决方案可能涉及设计专用的高效解码器（*例如*，使用
    NAS [[437](#bib.bib437), [311](#bib.bib311)]）。其次，简化视觉任务的多阶段设计（*例如*，两阶段目标检测 [[273](#bib.bib273)]
    和实例分割 [[318](#bib.bib318)] 算法）以实现端到端范式，并尽量减少性能损失是重要的。此外，去除耗时的组件，如非最大抑制（NMS） [[8](#bib.bib8)]，也是至关重要的。未来研究的一个有前景的领域可能涉及开发一个高效、统一的、端到端可学习的接口，适用于大多数流行的计算机视觉任务
    [[438](#bib.bib438)]。
- en: 7.3 Deep Networks for Edge Computing
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 边缘计算中的深度网络
- en: In practical applications, extant research predominantly focuses on conventional
    hardware, such as GPUs and CPUs. However, within the realm of edge computing,
    there is an increasing demand for the deployment of deep learning models on Internet
    of Things (IoT) devices and microcontrollers. These diminutive devices are characterized
    by their minimal size, low power consumption, affordability, and ubiquity [[418](#bib.bib418)].
    The development of deep learning algorithms specifically adapted for such devices
    represents an exigent research direction. MCUNets [[418](#bib.bib418), [426](#bib.bib426),
    [427](#bib.bib427)] have provided an initial exploration by optimizing the design,
    inference, and training of ConvNets for these devices. Another prospective concept
    involves the creation of spiking neural networks [[439](#bib.bib439)], which,
    when co-designed with hardware, can yield energy-efficient solutions.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，现有研究主要集中在传统硬件上，如GPU和CPU。然而，在边缘计算领域，越来越需要在物联网（IoT）设备和微控制器上部署深度学习模型。这些微小的设备具有体积小、功耗低、成本低和普及性强的特点[[418](#bib.bib418)]。专门为这些设备开发的深度学习算法是一个迫切的研究方向。MCUNets[[418](#bib.bib418),
    [426](#bib.bib426), [427](#bib.bib427)]通过优化ConvNets的设计、推理和训练，提供了初步探索。另一个有前景的概念是创建脉冲神经网络[[439](#bib.bib439)]，这种网络在与硬件共同设计时，可以实现节能解决方案。
- en: 7.4 Leveraging Large-scale Training Data
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 利用大规模训练数据
- en: Contemporary large visual backbone models have exhibited remarkable scalability
    in response to the increasing volumes of training data [[6](#bib.bib6)], that
    is, the model’s performance consistently enhances as more training data becomes
    accessible. However, it is generally arduous for computationally efficient models
    with a reduced number of parameters to capitalize on this high-data regime to
    the same extent as their larger counterparts. For example, the improvements attained
    by pre-training light-weighted models on expansive ImageNet-22K/JFT datasets are
    typically inferior to those observed in larger models [[6](#bib.bib6), [7](#bib.bib7),
    [74](#bib.bib74)]. This challenge is similarly experienced by self-supervised
    learning algorithms, where the methods effective for larger models frequently
    produce limited gains for smaller models [[440](#bib.bib440), [441](#bib.bib441)].
    As a result, a propitious avenue of research involves the exploration of effective
    scalable supervised and unsupervised learning algorithms for light-weighted models,
    allowing them to reap the benefits of an unlimited amount of data without incurring
    the expense of acquiring annotations. Some recent works on novel training algorithms
    have started to preliminarily explore this direction [[442](#bib.bib442), [82](#bib.bib82),
    [443](#bib.bib443), [444](#bib.bib444), [445](#bib.bib445)].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当代大型视觉骨干模型在应对不断增加的训练数据量方面表现出了显著的扩展性[[6](#bib.bib6)]，即随着更多训练数据的获取，模型的性能持续提高。然而，对于计算效率较高、参数较少的模型来说，充分利用这种高数据量的能力通常不如更大模型。举例来说，轻量级模型在广泛的ImageNet-22K/JFT数据集上进行预训练所获得的改进通常不及更大模型[[6](#bib.bib6),
    [7](#bib.bib7), [74](#bib.bib74)]。自监督学习算法也面临类似挑战，对于较大模型有效的方法往往在较小模型中获得的增益有限[[440](#bib.bib440),
    [441](#bib.bib441)]。因此，一个有前途的研究方向是探索有效的可扩展监督和无监督学习算法，以便轻量级模型在不需获得标注的情况下，也能从海量数据中获益。一些关于新型训练算法的近期研究已开始初步探索这一方向[[442](#bib.bib442),
    [82](#bib.bib82), [443](#bib.bib443), [444](#bib.bib444), [445](#bib.bib445)]。
- en: 7.5 Practical Efficiency
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 实际效率
- en: While numerous extant studies have attained low theoretical computational costs,
    they may be hindered by the restricted practical efficiency. For example, certain
    irregular network architectures discovered through NAS may display considerable
    latency on GPUs/CPUs, and the models employing group or depth-wise convolution
    may exhibit reduced gains in actual speedup relative to their theoretical computational
    efficiency. To tackle this challenge, researchers might consider integrating the
    speed on practical hardwares as an objective in architecture design [[53](#bib.bib53),
    [24](#bib.bib24)] or utilizing efficient implementation software [[57](#bib.bib57),
    [59](#bib.bib59)]. From a hardware design standpoint, one potential direction
    involves the creation of model-specialized hardware platforms [[423](#bib.bib423),
    [55](#bib.bib55), [424](#bib.bib424), [56](#bib.bib56)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大量现有研究已经实现了低理论计算成本，但它们可能受到实际效率受限的阻碍。例如，通过 NAS 发现的某些不规则网络架构可能在 GPUs/CPUs 上显示出显著的延迟，而采用组卷积或深度卷积的模型在实际加速方面的增益可能低于其理论计算效率。为了解决这一挑战，研究人员可能会考虑将实际硬件上的速度集成到架构设计中
    [[53](#bib.bib53), [24](#bib.bib24)] 或利用高效的实现软件 [[57](#bib.bib57), [59](#bib.bib59)]。从硬件设计的角度来看，一个潜在的方向是创建模型专用的硬件平台
    [[423](#bib.bib423), [55](#bib.bib55), [424](#bib.bib424), [56](#bib.bib56)]。
- en: 7.6 Model Compression Approaches
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6 模型压缩方法
- en: Network compression algorithms, encompassing network pruning, quantization,
    and knowledge distillation, have exhibited a robust capacity to diminish the inference
    costs associated with deep networks. However, several avenues of investigation
    remain unexplored. For instance, while the overarching concept of model compression
    is not confined to a particular vision task, a majority of algorithms predominantly
    concentrate on image classification, rendering their extension to other tasks
    non-trivial. A significant research direction entails the development of general-purpose,
    task-agnostic model compression techniques. Furthermore, strategies such as network
    pruning may yield irregular architectural topologies, potentially impairing the
    practical efficiency of deep learning models. Consequently, the examination of
    practically efficient compression methodologies constitutes a propitious area
    for future research.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 网络压缩算法，包括网络剪枝、量化和知识蒸馏，已显示出显著降低深度网络推理成本的能力。然而，仍有一些研究方向尚未探索。例如，尽管模型压缩的总体概念并不局限于特定的视觉任务，但大多数算法主要集中在图像分类上，使得它们向其他任务的扩展变得不那么简单。一个重要的研究方向是开发通用的、任务无关的模型压缩技术。此外，诸如网络剪枝之类的策略可能会导致不规则的架构拓扑，从而可能影响深度学习模型的实际效率。因此，研究实用的压缩方法是未来研究的一个有前景的领域。
- en: Acknowledgments
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported in part by the National Key R&D Program of China (2021ZD0140407),
    the National Natural Science Foundation of China (62022048, 62276150), Guoqiang
    Institute of Tsinghua University and Beijing Academy of Artificial Intelligence.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了中国国家重点研发计划（2021ZD0140407）、中国国家自然科学基金（62022048, 62276150）、清华大学国强研究院以及北京人工智能学院的支持。
- en: References
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,
    2017.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton. 用深度卷积神经网络进行 Imagenet
    分类。ACM 通讯, 60(6):84–90, 2017。'
- en: '[2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for
    large-scale image recognition. In ICLR, 2015.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Karen Simonyan 和 Andrew Zisserman. 用于大规模图像识别的非常深层卷积网络。发表于 ICLR, 2015。'
- en: '[3] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In CVPR, pages 1–9, 2015.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, 和 Andrew Rabinovich. 深入卷积。发表于
    CVPR, 页码 1–9, 2015。'
- en: '[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In CVPR, pages 770–778, 2016.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 深度残差学习用于图像识别。发表于 CVPR,
    页码 770–778, 2016。'
- en: '[5] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Convolutional networks with dense connectivity. TPAMI, 44(12):8704–8716,
    2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, 和 Kilian Q
    Weinberger. 具有密集连接的卷积网络。TPAMI, 44(12):8704–8716, 2019。'
- en: '[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. In ICLR, 2021.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Alexey Dosovitskiy、Lucas Beyer、Alexander Kolesnikov、Dirk Weissenborn、Xiaohua
    Zhai、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain
    Gelly 等。一张图像胜过 16x16 个词：用于大规模图像识别的 transformer。In ICLR，2021。'
- en: '[7] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
    and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted
    windows. In ICCV, pages 10012–10022, 2021.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ze Liu、Yutong Lin、Yue Cao、Han Hu、Yixuan Wei、Zheng Zhang、Stephen Lin 和 Baining
    Guo。Swin transformer：使用移动窗口的层次化视觉 transformer。In ICCV，页码 10012–10022，2021。'
- en: '[8] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object
    detection in 20 years: A survey. Proceedings of the IEEE, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Zhengxia Zou、Keyan Chen、Zhenwei Shi、Yuhong Guo 和 Jieping Ye。20年来的目标检测：综述。Proceedings
    of the IEEE，2023。'
- en: '[9] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz,
    and Demetri Terzopoulos. Image segmentation using deep learning: A survey. TPAMI,
    2021.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Shervin Minaee、Yuri Y Boykov、Fatih Porikli、Antonio J Plaza、Nasser Kehtarnavaz
    和 Demetri Terzopoulos。使用深度学习的图像分割：综述。TPAMI，2021。'
- en: '[10] Tianfei Zhou, Fatih Porikli, David J Crandall, Luc Van Gool, and Wenguan
    Wang. A survey on deep learning technique for video segmentation. TPAMI, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tianfei Zhou、Fatih Porikli、David J Crandall、Luc Van Gool 和 Wenguan Wang。深度学习技术在视频分割中的综述。TPAMI，2023。'
- en: '[11] Yu Kong and Yun Fu. Human action recognition and prediction: A survey.
    IJCV, 130(5):1366–1401, 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Yu Kong 和 Yun Fu。人类动作识别与预测：综述。IJCV，130(5)：1366–1401，2022。'
- en: '[12] Zehua Sun, Qiuhong Ke, Hossein Rahmani, Mohammed Bennamoun, Gang Wang,
    and Jun Liu. Human action recognition from various data modalities: A review.
    TPAMI, 2022.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Zehua Sun、Qiuhong Ke、Hossein Rahmani、Mohammed Bennamoun、Gang Wang 和 Jun
    Liu。从各种数据模态中进行人类动作识别：综述。TPAMI，2022。'
- en: '[13] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun.
    Deep learning for 3d point clouds: A survey. TPAMI, 43(12):4338–4364, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Yulan Guo、Hanyun Wang、Qingyong Hu、Hao Liu、Li Liu 和 Mohammed Bennamoun。3D
    点云的深度学习：综述。TPAMI，43(12)：4338–4364，2020。'
- en: '[14] Jiasi Chen and Xukan Ran. Deep learning with edge computing: A review.
    Proceedings of the IEEE, 107(8):1655–1674, 2019.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jiasi Chen 和 Xukan Ran。边缘计算中的深度学习：综述。Proceedings of the IEEE，107(8)：1655–1674，2019。'
- en: '[15] Yunbin Deng. Deep learning on mobile devices: a review. In Mobile Multimedia/Image
    Processing, Security, and Applications 2019, volume 10993, pages 52–66\. SPIE,
    2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Yunbin Deng。移动设备上的深度学习：综述。In Mobile Multimedia/Image Processing, Security,
    and Applications 2019，卷 10993，页码 52–66。SPIE，2019。'
- en: '[16] Chunlei Chen, Peng Zhang, Huixiang Zhang, Jiangyan Dai, Yugen Yi, Huihui
    Zhang, and Yonghui Zhang. Deep learning on computational-resource-limited platforms:
    a survey. Mobile Information Systems, 2020(4):1–19, 2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Chunlei Chen、Peng Zhang、Huixiang Zhang、Jiangyan Dai、Yugen Yi、Huihui Zhang
    和 Yonghui Zhang。计算资源受限平台上的深度学习：综述。Mobile Information Systems，2020(4)：1–19，2020。'
- en: '[17] Khan Muhammad, Amin Ullah, Jaime Lloret, Javier Del Ser, and Victor Hugo C
    de Albuquerque. Deep learning for safe autonomous driving: Current challenges
    and future directions. IEEE Transactions on Intelligent Transportation Systems,
    22(7):4316–4336, 2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Khan Muhammad、Amin Ullah、Jaime Lloret、Javier Del Ser 和 Victor Hugo C de
    Albuquerque。安全自动驾驶中的深度学习：当前挑战与未来方向。IEEE Transactions on Intelligent Transportation
    Systems，22(7)：4316–4336，2020。'
- en: '[18] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner,
    Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
    Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316,
    2016.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Mariusz Bojarski、Davide Del Testa、Daniel Dworakowski、Bernhard Firner、Beat
    Flepp、Prasoon Goyal、Lawrence D Jackel、Mathew Monfort、Urs Muller、Jiakai Zhang 等。端到端学习自驾车。arXiv
    预印本 arXiv:1604.07316，2016。'
- en: '[19] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.
    A survey of deep learning techniques for autonomous driving. Journal of Field
    Robotics, 37(3):362–386, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Sorin Grigorescu、Bogdan Trasnea、Tiberiu Cocias 和 Gigel Macesanu。自动驾驶深度学习技术的综述。Journal
    of Field Robotics，37(3)：362–386，2020。'
- en: '[20] Chaoyun Zhang, Paul Patras, and Hamed Haddadi. Deep learning in mobile
    and wireless networking: A survey. IEEE Communications surveys & tutorials, 21(3):2224–2287,
    2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Chaoyun Zhang、Paul Patras 和 Hamed Haddadi。移动和无线网络中的深度学习：综述。IEEE Communications
    Surveys & Tutorials，21(3)：2224–2287，2019。'
- en: '[21] Mehdi Mohammadi, Ala Al-Fuqaha, Sameh Sorour, and Mohsen Guizani. Deep
    learning for iot big data and streaming analytics: A survey. IEEE Communications
    Surveys & Tutorials, 20(4):2923–2960, 2018.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Mehdi Mohammadi、Ala Al-Fuqaha、Sameh Sorour 和 Mohsen Guizani。深度学习在物联网大数据与流分析中的应用：综述。IEEE
    Communications Surveys & Tutorials，20(4)：2923–2960，2018。'
- en: '[22] He Li, Kaoru Ota, and Mianxiong Dong. Learning iot in edge: Deep learning
    for the internet of things with edge computing. IEEE network, 32(1):96–101, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] He Li、Kaoru Ota 和 Mianxiong Dong。在边缘学习物联网：边缘计算下的物联网深度学习。IEEE 网络，32(1)：96–101，2018
    年。'
- en: '[23] Adrian Carrio, Carlos Sampedro, Alejandro Rodriguez-Ramos, and Pascual
    Campoy. A review of deep learning methods and applications for unmanned aerial
    vehicles. Journal of Sensors, 2017, 2017.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Adrian Carrio、Carlos Sampedro、Alejandro Rodriguez-Ramos 和 Pascual Campoy。无人机深度学习方法及应用的综述。传感器期刊，2017
    年，2017 年。'
- en: '[24] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR, 2020.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Han Cai、Chuang Gan、Tianzhe Wang、Zhekai Zhang 和 Song Han。一次性训练：训练一个网络并使其专门化以实现高效部署。发表于
    ICLR，2020 年。'
- en: '[25] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey
    on green deep learning. arXiv preprint arXiv:2111.05193, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Jingjing Xu、Wangchunshu Zhou、Zhiyi Fu、Hao Zhou 和 Lei Li。关于绿色深度学习的综述。arXiv
    预印本 arXiv:2111.05193，2021 年。'
- en: '[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861,
    2017.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Andrew G Howard、Menglong Zhu、Bo Chen、Dmitry Kalenichenko、Weijun Wang、Tobias
    Weyand、Marco Andreetto 和 Hartwig Adam。Mobilenets：用于移动视觉应用的高效卷积神经网络。arXiv 预印本 arXiv:1704.04861，2017
    年。'
- en: '[27] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, pages 4510–4520,
    2018.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Mark Sandler、Andrew Howard、Menglong Zhu、Andrey Zhmoginov 和 Liang-Chieh
    Chen。Mobilenetv2：倒置残差和线性瓶颈。发表于 CVPR，页码 4510–4520，2018 年。'
- en: '[28] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
    Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for
    mobilenetv3. In CVPR, pages 1314–1324, 2019.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Andrew Howard、Mark Sandler、Grace Chu、Liang-Chieh Chen、Bo Chen、Mingxing
    Tan、Weijun Wang、Yukun Zhu、Ruoming Pang、Vijay Vasudevan 等。寻找 mobilenetv3。发表于 CVPR，页码
    1314–1324，2019 年。'
- en: '[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In ICML, pages 6105–6114\. PMLR, 2019.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Mingxing Tan 和 Quoc Le。Efficientnet：重新思考卷积神经网络的模型缩放。发表于 ICML，页码 6105–6114。PMLR，2019
    年。'
- en: '[30] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and
    Piotr Dollár. Designing network design spaces. In CVPR, pages 10428–10436, 2020.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ilija Radosavovic、Raj Prateek Kosaraju、Ross Girshick、Kaiming He 和 Piotr
    Dollár。设计网络设计空间。发表于 CVPR，页码 10428–10436，2020 年。'
- en: '[31] Barret Zoph and Quoc Le. Neural architecture search with reinforcement
    learning. In ICLR, 2017.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Barret Zoph 和 Quoc Le。使用强化学习进行神经架构搜索。发表于 ICLR，2017 年。'
- en: '[32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture
    search. In ICLR, 2019.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Hanxiao Liu、Karen Simonyan 和 Yiming Yang。Darts：可微分架构搜索。发表于 ICLR，2019 年。'
- en: '[33] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang.
    Dynamic neural networks: A survey. TPAMI, 2021.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yizeng Han、Gao Huang、Shiji Song、Le Yang、Honghui Wang 和 Yulin Wang。动态神经网络：综述。TPAMI，2021
    年。'
- en: '[34] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten,
    and Kilian Q Weinberger. Multi-scale dense networks for resource efficient image
    classification. In ICLR, 2017.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Gao Huang、Danlu Chen、Tianhong Li、Felix Wu、Laurens Van Der Maaten 和 Kilian
    Q Weinberger。资源高效图像分类的多尺度密集网络。发表于 ICLR，2017 年。'
- en: '[35] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning.
    NeurIPS, 2017.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ji Lin、Yongming Rao、Jiwen Lu 和 Jie Zhou。运行时神经剪枝。NeurIPS，2017 年。'
- en: '[36] Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis
    DeCoste, Wei Di, and Yizhou Yu. Hd-cnn: hierarchical deep convolutional neural
    networks for large scale visual recognition. In ICCV, 2015.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Zhicheng Yan、Hao Zhang、Robinson Piramuthu、Vignesh Jagadeesh、Dennis DeCoste、Wei
    Di 和 Yizhou Yu。Hd-cnn：用于大规模视觉识别的层次化深度卷积神经网络。发表于 ICCV，2015 年。'
- en: '[37] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry S Davis.
    Adaframe: Adaptive frame selection for fast video recognition. In CVPR, 2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zuxuan Wu、Caiming Xiong、Chih-Yao Ma、Richard Socher 和 Larry S Davis。Adaframe：快速视频识别的自适应帧选择。发表于
    CVPR，2019 年。'
- en: '[38] Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, and Gao
    Huang. Adaptive focus for efficient video recognition. In ICCV, 2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Yulin Wang、Zhaoxi Chen、Haojun Jiang、Shiji Song、Yizeng Han 和 Gao Huang。用于高效视频识别的自适应聚焦。发表于
    ICCV，2021 年。'
- en: '[39] Yizeng Han, Zhihang Yuan, Yifan Pu, Chenhao Xue, Shiji Song, Guangyu Sun,
    and Gao Huang. Latency-aware spatial-wise dynamic networks. In NeurIPS, 2022.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yizeng Han、Zhihang Yuan、Yifan Pu、Chenhao Xue、Shiji Song、Guangyu Sun 和
    Gao Huang。延迟感知的空间动态网络。发表于 NeurIPS，2022 年。'
- en: '[40] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In CVPR, pages 779–788, 2016.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Joseph Redmon, Santosh Divvala, Ross Girshick 和 Ali Farhadi. 你只需看一次：统一的实时目标检测。发表于
    CVPR，第 779–788 页，2016 年。'
- en: '[41] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In ECCV,
    pages 21–37\. Springer, 2016.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu 和 Alexander C Berg. SSD：单次多框检测器。发表于 ECCV，第 21–37 页。Springer，2016
    年。'
- en: '[42] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional
    one-stage object detection. In ICCV, pages 9627–9636, 2019.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zhi Tian, Chunhua Shen, Hao Chen 和 Tong He. Fcos：全卷积单阶段目标检测。发表于 ICCV，第
    9627–9636 页，2019 年。'
- en: '[43] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable
    bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv
    preprint arXiv:2207.02696, 2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Chien-Yao Wang, Alexey Bochkovskiy 和 Hong-Yuan Mark Liao. Yolov7：可训练的免费配件集为实时目标检测器设立了新的最先进水平。arXiv
    预印本 arXiv:2207.02696，2022 年。'
- en: '[44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong
    Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation.
    In ECCV, pages 325–341, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu 和 Nong Sang.
    Bisenet：用于实时语义分割的双边分割网络。发表于 ECCV，第 325–341 页，2018 年。'
- en: '[45] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo:
    Segmenting objects by locations. In ECCV, pages 649–665\. Springer, 2020.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang 和 Lei Li. Solo：按位置分割目标。发表于
    ECCV，第 649–665 页。Springer，2020 年。'
- en: '[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2:
    Dynamic and fast instance segmentation. In NeurIPS, volume 33, pages 17721–17732,
    2020.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li 和 Chunhua Shen. Solov2：动态和快速的实例分割。发表于
    NeurIPS，第 33 卷，第 17721–17732 页，2020 年。'
- en: '[47] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
    Zhang. Learning efficient convolutional networks through network slimming. In
    ICCV, pages 2736–2744, 2017.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan 和 Changshui
    Zhang. 通过网络精简学习高效的卷积网络。发表于 ICCV，第 2736–2744 页，2017 年。'
- en: '[48] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
    sparse, trainable neural networks. In ICLR, 2019.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Jonathan Frankle 和 Michael Carbin. 彩票票据假设：寻找稀疏的、可训练的神经网络。发表于 ICLR，2019
    年。'
- en: '[49] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect:
    Training deep neural networks with binary weights during propagations. NeurIPS,
    2015.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Matthieu Courbariaux, Yoshua Bengio 和 Jean-Pierre David. Binaryconnect：在传播过程中使用二进制权重训练深度神经网络。NeurIPS，2015
    年。'
- en: '[50] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net:
    Imagenet classification using binary convolutional neural networks. In ECCV, pages
    525–542\. Springer, 2016.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon 和 Ali Farhadi. Xnor-net：使用二进制卷积神经网络进行
    ImageNet 分类。发表于 ECCV，第 525–542 页。Springer，2016 年。'
- en: '[51] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge
    in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Geoffrey Hinton, Oriol Vinyals, Jeff Dean 等. 提取神经网络中的知识。arXiv 预印本 arXiv:1503.02531，第
    2(7) 卷，2015 年。'
- en: '[52] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge
    distillation: A survey. IJCV, 129(6):1789–1819, 2021.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Jianping Gou, Baosheng Yu, Stephen J Maybank 和 Dacheng Tao. 知识蒸馏：综述。IJCV，第
    129(6) 卷，第 1789–1819 页，2021 年。'
- en: '[53] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
    Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for
    mobile. In CVPR, pages 2820–2828, 2019.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
    Howard 和 Quoc V Le. Mnasnet：面向平台的移动神经架构搜索。发表于 CVPR，第 2820–2828 页，2019 年。'
- en: '[54] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture
    search on target task and hardware. In ICLR, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Han Cai, Ligeng Zhu 和 Song Han. Proxylessnas：针对目标任务和硬件的直接神经架构搜索。发表于 ICLR，2019
    年。'
- en: '[55] Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow,
    Wen-mei Hwu, and Deming Chen. Fpga/dnn co-design: An efficient design methodology
    for iot intelligence on the edge. In DAC, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Cong Hao, Xiaofan Zhang, Yuhong Li, Sitao Huang, Jinjun Xiong, Kyle Rupnow,
    Wen-mei Hwu 和 Deming Chen. FPGA/DNN 协同设计：面向边缘的 IoT 智能高效设计方法。发表于 DAC，2019 年。'
- en: '[56] Weiwen Jiang, Lei Yang, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Shouzhen
    Gu, Sakyasingha Dasgupta, Yiyu Shi, and Jingtong Hu. Hardware/software co-exploration
    of neural architectures. IEEE Transactions on Computer-Aided Design of Integrated
    Circuits and Systems, 2020.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Weiwen Jiang, Lei Yang, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Shouzhen
    Gu, Sakyasingha Dasgupta, Yiyu Shi 和 Jingtong Hu. 神经架构的硬件/软件协同探索。IEEE 集成电路与系统计算机辅助设计学报，2020
    年。'
- en: '[57] Han Vanholder. Efficient inference with tensorrt. In GPU Technology Conference,
    2016.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Han Vanholder。使用 tensorrt 高效推断。在 GPU 技术大会，2016 年。'
- en: '[58] Mohit Thakkar and Mohit Thakkar. Introduction to core ml framework. Beginning
    Machine Learning in iOS: CoreML Framework, 2019.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Mohit Thakkar 和 Mohit Thakkar。Core ML 框架介绍。《iOS 中的机器学习入门: CoreML 框架》，2019
    年。'
- en: '[59] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q Yan,
    Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. Tvm:
    end-to-end optimization stack for deep learning. arXiv preprint arXiv:1802.04799,
    2018.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Haichen Shen, Eddie Q Yan,
    Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin 和 Arvind Krishnamurthy。Tvm:
    深度学习的端到端优化栈。arXiv 预印本 arXiv:1802.04799，2018 年。'
- en: '[60] Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen, and Qian
    Zhang. Deep learning on mobile and embedded devices: State-of-the-art, challenges,
    and future directions. ACM Computing Surveys (CSUR), 53(4):1–37, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yanjiao Chen, Baolin Zheng, Zihan Zhang, Qian Wang, Chao Shen 和 Qian Zhang。移动和嵌入式设备上的深度学习:
    现状、挑战与未来方向。ACM Computing Surveys (CSUR)，53(4):1–37，2020 年。'
- en: '[61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
    Aggregated residual transformations for deep neural networks. In CVPR, pages 1492–1500,
    2017.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu 和 Kaiming He。深度神经网络的聚合残差变换。在
    CVPR，页码 1492–1500，2017 年。'
- en: '[62] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang,
    and Philip Torr. Res2net: A new multi-scale backbone architecture. TPAMI, 43(2):652–662,
    2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang
    和 Philip Torr。Res2net: 一种新的多尺度主干架构。TPAMI，43(2):652–662，2019 年。'
- en: '[63] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An
    extremely efficient convolutional neural network for mobile devices. In CVPR,
    pages 6848–6856, 2018.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin 和 Jian Sun。Shufflenet: 一种极其高效的卷积神经网络，适用于移动设备。在
    CVPR，页码 6848–6856，2018 年。'
- en: '[64] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2:
    Practical guidelines for efficient cnn architecture design. In ECCV, pages 116–131,
    2018.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng 和 Jian Sun。Shufflenet v2: 高效
    CNN 架构设计的实用指南。在 ECCV，页码 116–131，2018 年。'
- en: '[65] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
    Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation
    through attention. In ICML, pages 10347–10357, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
    Sablayrolles 和 Hervé Jégou。数据高效的图像变换器训练与通过注意力机制的蒸馏。在 ICML，页码 10347–10357，2021
    年。'
- en: '[66] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.
    In CVPR, pages 510–519, 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Xiang Li, Wenhai Wang, Xiaolin Hu 和 Jian Yang。选择性卷积核网络。在 CVPR，页码 510–519，2019
    年。'
- en: '[67] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang,
    Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks.
    In CVPR, pages 2736–2746, 2022.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang,
    Yue Sun, Tong He, Jonas Mueller, R Manmatha 等人。Resnest: 分割注意力网络。在 CVPR 会议上，页码
    2736–2746，2022 年。'
- en: '[68] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. Interleaved group
    convolutions. In ICCV, pages 4373–4382, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Ting Zhang, Guo-Jun Qi, Bin Xiao 和 Jingdong Wang。交错组卷积。在 ICCV，页码 4373–4382，2017
    年。'
- en: '[69] Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong, and
    Guo-Jun Qi. Interleaved structured sparse convolutional neural networks. In CVPR,
    pages 8847–8856, 2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong 和
    Guo-Jun Qi。交错结构稀疏卷积神经网络。在 CVPR，页码 8847–8856，2018 年。'
- en: '[70] Ke Sun, Mingjie Li, Dong Liu, and Jingdong Wang. Igcv3: Interleaved low-rank
    group convolutions for efficient deep neural networks. In BMVC, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Ke Sun, Mingjie Li, Dong Liu 和 Jingdong Wang。Igcv3: 为高效深度神经网络设计的交错低秩组卷积。在
    BMVC，2018 年。'
- en: '[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    NeurIPS, 30, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。注意力机制即是你所需的。在 NeurIPS，30，2017
    年。'
- en: '[72] François Chollet. Xception: Deep learning with depthwise separable convolutions.
    In CVPR, pages 1251–1258, 2017.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] François Chollet。Xception: 深度学习中的深度可分离卷积。在 CVPR，页码 1251–1258，2017 年。'
- en: '[73] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and Hannaneh Hajishirzi.
    Espnetv2: A light-weight, power efficient, and general purpose convolutional neural
    network. In CVPR, pages 9190–9200, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Sachin Mehta, Mohammad Rastegari, Linda Shapiro 和 Hannaneh Hajishirzi。Espnetv2:
    一种轻量级、节能且通用的卷积神经网络。在 CVPR，页码 9190–9200，2019 年。'
- en: '[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
    and Saining Xie. A convnet for the 2020s. In CVPR, pages 11976–11986, 2022.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
    和 Saining Xie. 面向2020年代的卷积网络。发表于 CVPR, 页码 11976–11986, 2022。'
- en: '[75] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan,
    and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In ICCV,
    pages 22–31, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan,
    和 Lei Zhang. CVT：将卷积引入视觉变换器。发表于 ICCV, 页码 22–31, 2021。'
- en: '[76] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and
    Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In CVPR,
    pages 12175–12185, 2022.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, 和
    Chang Xu. CMT：卷积神经网络遇见视觉变换器。发表于 CVPR, 页码 12175–12185, 2022。'
- en: '[77] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In ICML, pages 448–456\.
    PMLR, 2015.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Sergey Ioffe 和 Christian Szegedy. 批归一化：通过减少内部协变量偏移加速深度网络训练。发表于 ICML, 页码
    448–456\. PMLR, 2015。'
- en: '[78] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
    Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages
    2818–2826, 2016.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, 和 Zbigniew
    Wojna. 重新思考计算机视觉中的 Inception 架构。发表于 CVPR, 页码 2818–2826, 2016。'
- en: '[79] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In AAAI, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, 和 Alexander A Alemi.
    Inception-v4, inception-resnet 以及残差连接对学习的影响。发表于 AAAI, 2017。'
- en: '[80] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, and Shuicheng
    YAN. Inception transformer. In NeurIPS, 2022.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang, 和 Shuicheng
    YAN. Inception transformer。发表于 NeurIPS, 2022。'
- en: '[81] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and
    Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated
    convolutions. In NeurIPS, 2022.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, 和 Jiwen
    Lu. Hornet：高效的高阶空间交互，通过递归门控卷积实现。发表于 NeurIPS, 2022。'
- en: '[82] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training.
    In ICML, pages 10096–10106\. PMLR, 2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Mingxing Tan 和 Quoc Le. EfficientNetV2：更小的模型和更快的训练。发表于 ICML, 页码 10096–10106\.
    PMLR, 2021。'
- en: '[83] Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
    Rethinking bottleneck structure for efficient mobile network design. In ECCV,
    pages 680–697\. Springer, 2020.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, 和 Shuicheng Yan. 重新思考瓶颈结构以实现高效的移动网络设计。发表于
    ECCV, 页码 680–697\. Springer, 2020。'
- en: '[84] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very
    deep networks. NeurIPS, 28, 2015.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Rupesh K Srivastava, Klaus Greff, 和 Jürgen Schmidhuber. 训练非常深的网络。发表于 NeurIPS,
    28, 2015。'
- en: '[85] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In CVPR, pages 4700–4708, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, 和 Kilian Q Weinberger.
    密集连接卷积网络。发表于 CVPR, 页码 4700–4708, 2017。'
- en: '[86] Gao Huang, Shichen Liu, Laurens Van der Maaten, and Kilian Q Weinberger.
    Condensenet: An efficient densenet using learned group convolutions. In CVPR,
    pages 2752–2761, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Gao Huang, Shichen Liu, Laurens Van der Maaten, 和 Kilian Q Weinberger.
    Condensenet：使用学习的组卷积的高效 DenseNet。发表于 CVPR, 页码 2752–2761, 2018。'
- en: '[87] Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang,
    and Qi Tian. Condensenet v2: Sparse feature reactivation for deep networks. In
    CVPR, pages 3569–3578, 2021.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Le Yang, Haojun Jiang, Ruojin Cai, Yulin Wang, Shiji Song, Gao Huang,
    和 Qi Tian. Condensenet v2：深度网络的稀疏特征再激活。发表于 CVPR, 页码 3569–3578, 2021。'
- en: '[88] Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, and
    Qi Tian. Ghostnets on heterogeneous devices via cheap operations. IJCV, 130(4):1050–1069,
    2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chunjing Xu, Enhua Wu, 和
    Qi Tian. 通过廉价操作在异构设备上使用 Ghostnets。发表于 IJCV, 130(4):1050–1069, 2022。'
- en: '[89] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
    Ghostnet: More features from cheap operations. In CVPR, pages 1580–1589, 2020.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, 和 Chang Xu. Ghostnet：通过廉价操作获得更多特征。发表于
    CVPR, 页码 1580–1589, 2020。'
- en: '[90] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, and Yunhe Wang.
    Ghostnetv2: Enhance cheap operation with long-range attention. In NeurIPS, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, 和 Yunhe Wang. GhostNetV2：通过长程注意力增强廉价操作。发表于
    NeurIPS, 2022。'
- en: '[91] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
    Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy
    in convolutional neural networks with octave convolution. In ICCV, pages 3435–3444,
    2019.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus
    Rohrbach, Shuicheng Yan 和 Jiashi Feng. 落 octave：通过 octave 卷积减少卷积神经网络中的空间冗余。在 ICCV，页面
    3435–3444，2019年。'
- en: '[92] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with
    hilo attention. In NeurIPS, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Zizheng Pan, Jianfei Cai 和 Bohan Zhuang. 具有 hilo 注意力的快速视觉变换器。在 NeurIPS，2022年。'
- en: '[93] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation
    learning for human pose estimation. In CVPR, pages 5693–5703, 2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Ke Sun, Bin Xiao, Dong Liu 和 Jingdong Wang. 用于人体姿态估计的深度高分辨率表征学习。在 CVPR，页面
    5693–5703，2019年。'
- en: '[94] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang
    Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution
    representation learning for visual recognition. TPAMI, 43(10):3349–3364, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang
    Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang 等. 用于视觉识别的深度高分辨率表征学习。TPAMI，43(10):3349–3364，2020年。'
- en: '[95] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and
    Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict.
    NeurIPS, 34:7281–7293, 2021.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen 和
    Jingdong Wang. Hrformer：用于密集预测的高分辨率视觉变换器。NeurIPS，34:7281–7293，2021年。'
- en: '[96] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone
    for dense prediction without convolutions. In ICCV, pages 568–578, 2021.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo 和 Ling Shao. Pyramid vision transformer：一种多用途骨干网络，无需卷积即可进行密集预测。在
    ICCV，页面 568–578，2021年。'
- en: '[97] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision
    transformer. Computational Visual Media, 8(3):415–424, 2022.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang,
    Tong Lu, Ping Luo 和 Ling Shao. Pvt v2：使用 Pyramid Vision Transformer 改进的基准。Computational
    Visual Media，8(3):415–424，2022年。'
- en: '[98] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng, and Xinchao Wang.
    Shunted self-attention via multi-scale token aggregation. In CVPR, pages 10853–10862,
    2022.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng 和 Xinchao Wang. 通过多尺度令牌聚合的
    Shunted 自注意力。在 CVPR，页面 10853–10862，2022年。'
- en: '[99] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin
    Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention
    in vision transformers. NeurIPS, 34:9355–9366, 2021.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin
    Wei, Huaxia Xia 和 Chunhua Shen. Twins：重新审视视觉变换器中空间注意力的设计。NeurIPS，34:9355–9366，2021年。'
- en: '[100] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia
    Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity
    and resolution. In CVPR, pages 12009–12019, 2022.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia
    Ning, Yue Cao, Zheng Zhang, Li Dong 等. Swin transformer v2：扩展容量和分辨率。在 CVPR，页面
    12009–12019，2022年。'
- en: '[101] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
    Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient
    visual backbones. In CVPR, pages 12894–12904, 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake
    Hechtman 和 Jonathon Shlens. 为参数高效视觉骨干网络扩展局部自注意力。在 CVPR，页面 12894–12904，2021年。'
- en: '[102] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan,
    and Jianfeng Gao. Focal self-attention for local-global interactions in vision
    transformers. arXiv preprint arXiv:2107.00641, 2021.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu
    Yuan 和 Jianfeng Gao. 用于视觉变换器中局部-全球交互的焦点自注意力。arXiv 预印本 arXiv:2107.00641，2021年。'
- en: '[103] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan,
    Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone
    with cross-shaped windows. In CVPR, pages 12124–12134, 2022.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu
    Yuan, Dong Chen 和 Baining Guo. Cswin transformer：一种具有交叉形窗口的一般视觉变换器骨干网络。在 CVPR，页面
    12124–12134，2022年。'
- en: '[104] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo:
    Vision outlooker for visual recognition. TPAMI, 2022.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng 和 Shuicheng Yan. Volo：用于视觉识别的视觉展望者。TPAMI，2022年。'
- en: '[105] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, and
    Jiaqi Ma. V2x-vit: Vehicle-to-everything cooperative perception with vision transformer.
    In ECCV, pages 107–124\. Springer, 2022.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang 和 Jiaqi
    Ma. V2x-vit：基于视觉变换器的车联网合作感知。在 ECCV，页面 107–124。Springer，2022年。'
- en: '[106] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar,
    Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In ECCV, pages
    459–479\. Springer, 2022.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar,
    Alan Bovik, 和 Yinxiao Li. Maxvit：多轴视觉变换器。在 ECCV, 页码 459–479\. Springer, 2022.'
- en: '[107] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio
    Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional
    inductive biases. In ICML, pages 2286–2296\. PMLR, 2021.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio
    Biroli, 和 Levent Sagun. Convit：通过软卷积归纳偏置改进视觉变换器。在 ICML, 页码 2286–2296\. PMLR, 2021.'
- en: '[108] Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, and Jie Chen.
    Locality guidance for improving vision transformers on tiny datasets. In ECCV,
    pages 110–127\. Springer, 2022.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Kehan Li, Runyi Yu, Zhennan Wang, Li Yuan, Guoli Song, 和 Jie Chen. 局部性指导：在小型数据集上改进视觉变换器。在
    ECCV, 页码 110–127\. Springer, 2022.'
- en: '[109] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and
    Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating
    messenger tokens. In CVPR, pages 12063–12072, 2022.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, 和
    Qi Tian. Msg-transformer：通过操控信使令牌交换局部空间信息。在 CVPR, 页码 12063–12072, 2022.'
- en: '[110] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
    Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training
    vision transformers from scratch on imagenet. In ICCV, pages 558–567, 2021.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
    Francis EH Tay, Jiashi Feng, 和 Shuicheng Yan. Tokens-to-token vit：在 imagenet 上从头训练视觉变换器。在
    ICCV, 页码 558–567, 2021.'
- en: '[111] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
    Transformer in transformer. NeurIPS, 34:15908–15919, 2021.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, 和 Yunhe Wang.
    变换器中的变换器。在 NeurIPS, 34:15908–15919, 2021.'
- en: '[112] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    In ICML, pages 5156–5165\. PMLR, 2020.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, 和 François Fleuret.
    变换器是 RNN：具有线性注意力的快速自回归变换器。在 ICML, 页码 5156–5165\. PMLR, 2020.'
- en: '[113] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking
    attention with performers. In ICLR, 2021.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou
    Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
    Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, 和 Adrian Weller. 重新思考与执行者的注意力。在
    ICLR, 2021.'
- en: '[114] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
    Fung, Yin Li, and Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating
    self-attention. In AAAI, 2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
    Fung, Yin Li, 和 Vikas Singh. Nyströmformer：一种基于 Nyström 的自注意力近似算法。在 AAAI, 2021.'
- en: '[115] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao,
    Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear
    complexity. In NeurIPS, 2021.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao,
    Chunjing Xu, Tao Xiang, 和 Li Zhang. Soft：具有线性复杂度的无 Softmax 变换器。在 NeurIPS, 2021.'
- en: '[116] Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi
    Fan, Peter Vajda, and Yingyan Lin. Castling-vit: Compressing self-attention via
    switching towards linear-angular attention during vision transformer inference.
    In CVPR, 2023.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Haoran You, Yunyang Xiong, Xiaoliang Dai, Bichen Wu, Peizhao Zhang, Haoqi
    Fan, Peter Vajda, 和 Yingyan Lin. Castling-vit：通过在视觉变换器推理过程中切换到线性-角度注意力来压缩自注意力。在
    CVPR, 2023.'
- en: '[117] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient
    architecture search by network transformation. In AAAI, 2018.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, 和 Jun Wang. 通过网络变换进行高效架构搜索。在
    AAAI, 2018.'
- en: '[118] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten
    transformer: Vision transformer using focused linear attention. In ICCV, 2023.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, 和 Gao Huang. 扁平变换器：使用集中的线性注意力的视觉变换器。在
    ICCV, 2023.'
- en: '[119] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional
    image transformers. In ICCV, pages 9981–9990, 2021.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Weijian Xu, Yifan Xu, Tyler Chang, 和 Zhuowen Tu. Co-scale conv-attentional
    图像变换器。在 ICCV, 页码 9981–9990, 2021.'
- en: '[120] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang. Mpvit:
    Multi-path vision transformer for dense prediction. In CVPR, pages 7287–7296,
    2022.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Youngwan Lee, Jonghee Kim, Jeffrey Willette, 和 Sung Ju Hwang. Mpvit：用于密集预测的多路径视觉变换器。在
    CVPR, 页码 7287–7296, 2022.'
- en: '[121] Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan L Yuille, and Wei
    Shen. Glance-and-gaze vision transformer. NeurIPS, 34:12992–13003, 2021.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Qihang Yu、Yingda Xia、Yutong Bai、Yongyi Lu、Alan L Yuille 和 Wei Shen。Glance-and-gaze
    视觉变换器。NeurIPS，34：12992–13003，2021 年。'
- en: '[122] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
    Incorporating convolution designs into visual transformers. In ICCV, pages 579–588,
    2021.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Kun Yuan、Shaopeng Guo、Ziwei Liu、Aojun Zhou、Fengwei Yu 和 Wei Wu。将卷积设计融入视觉变换器。发表于
    ICCV，第 579–588 页，2021 年。'
- en: '[123] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit:
    Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Yawei Li、Kai Zhang、Jiezhang Cao、Radu Timofte 和 Luc Van Gool。Localvit：将局部性引入视觉变换器。arXiv
    预印本 arXiv:2104.05707，2021 年。'
- en: '[124] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying
    convolution and attention for all data sizes. In NeurIPS, pages 3965–3977, 2021.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Zihang Dai、Hanxiao Liu、Quoc V Le 和 Mingxing Tan。Coatnet：将卷积和注意力结合以适应所有数据尺寸。发表于
    NeurIPS，第 3965–3977 页，2021 年。'
- en: '[125] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and
    Ross Girshick. Early convolutions help transformers see better. In NeurIPS, pages
    30392–30400, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Tete Xiao、Mannat Singh、Eric Mintun、Trevor Darrell、Piotr Dollář 和 Ross
    Girshick。早期卷积帮助变换器更好地“看见”。发表于 NeurIPS，第 30392–30400 页，2021 年。'
- en: '[126] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand
    Joulin, Hervé Jégou, and Matthijs Douze. Levit: a vision transformer in convnet’s
    clothing for faster inference. In ICCV, pages 12259–12269, 2021.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Benjamin Graham、Alaaeldin El-Nouby、Hugo Touvron、Pierre Stock、Armand Joulin、Hervé
    Jégou 和 Matthijs Douze。Levit：一款在卷积网络“外衣”下的视觉变换器，用于更快的推理。发表于 ICCV，第 12259–12269
    页，2021 年。'
- en: '[127] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose,
    and mobile-friendly vision transformer. In ICLR, 2022.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Sachin Mehta 和 Mohammad Rastegari。Mobilevit：轻量级、通用且移动友好的视觉变换器。发表于 ICLR，2022
    年。'
- en: '[128] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and
    Qi Tian. Visformer: The vision-friendly transformer. In ICCV, pages 589–598, 2021.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Zhengsu Chen、Lingxi Xie、Jianwei Niu、Xuefeng Liu、Longhui Wei 和 Qi Tian。Visformer：视觉友好的变换器。发表于
    ICCV，第 589–598 页，2021 年。'
- en: '[129] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe,
    and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In ICCV,
    pages 11936–11945, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Byeongho Heo、Sangdoo Yun、Dongyoon Han、Sanghyuk Chun、Junsuk Choe 和 Seong
    Joon Oh。重新思考视觉变换器的空间维度。发表于 ICCV，第 11936–11945 页，2021 年。'
- en: '[130] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer
    advanced by exploring intrinsic inductive bias. In NeurIPS, volume 34, pages 28522–28535,
    2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Yufei Xu、Qiming Zhang、Jing Zhang 和 Dacheng Tao。Vitae：通过探索内在归纳偏差提升的视觉变换器。发表于
    NeurIPS，第 34 卷，第 28522–28535 页，2021 年。'
- en: '[131] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao. Vitaev2: Vision
    transformer advanced by exploring inductive bias for image recognition and beyond.
    IJCV, pages 1–22, 2023.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Qiming Zhang、Yufei Xu、Jing Zhang 和 Dacheng Tao。Vitaev2：通过探索图像识别及其他方面的归纳偏差提升的视觉变换器。IJCV，第
    1–22 页，2023 年。'
- en: '[132] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,
    and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In CVPR, pages
    5270–5279, 2022.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Yinpeng Chen、Xiyang Dai、Dongdong Chen、Mengchen Liu、Xiaoyi Dong、Lu Yuan
    和 Zicheng Liu。Mobile-former：桥接 mobilenet 和变换器。发表于 CVPR，第 5270–5279 页，2022 年。'
- en: '[133] Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian
    Cheng, and Jingdong Wang. Mixformer: Mixing features across windows and dimensions.
    In CVPR, pages 5249–5259, 2022.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Qiang Chen、Qiman Wu、Jian Wang、Qinghao Hu、Tao Hu、Errui Ding、Jian Cheng
    和 Jingdong Wang。Mixformer：跨窗口和维度混合特征。发表于 CVPR，第 5249–5259 页，2022 年。'
- en: '[134] Zhiliang Peng, Zonghao Guo, Wei Huang, Yaowei Wang, Lingxi Xie, Jianbin
    Jiao, Qi Tian, and Qixiang Ye. Conformer: Local features coupling global representations
    for recognition and detection. TPAMI, 2023.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Zhiliang Peng、Zonghao Guo、Wei Huang、Yaowei Wang、Lingxi Xie、Jianbin Jiao、Qi
    Tian 和 Qixiang Ye。Conformer：局部特征结合全球表示用于识别和检测。TPAMI，2023 年。'
- en: '[135] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin
    Jiao, and Qixiang Ye. Conformer: Local features coupling global representations
    for visual recognition. In ICCV, pages 367–376, 2021.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Zhiliang Peng、Wei Huang、Shanzhi Gu、Lingxi Xie、Yaowei Wang、Jianbin Jiao
    和 Qixiang Ye。Conformer：局部特征结合全球表示用于视觉识别。发表于 ICCV，第 367–376 页，2021 年。'
- en: '[136] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr
    Dollár. On network design spaces for visual recognition. In ICCV, pages 1882–1890,
    2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Ilija Radosavovic、Justin Johnson、Saining Xie、Wan-Yen Lo 和 Piotr Dollář。关于视觉识别的网络设计空间。发表于
    ICCV，第 1882–1890 页，2019 年。'
- en: '[137] Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and accurate model
    scaling. In CVPR, pages 924–932, 2021.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Piotr Dollár、Mannat Singh 和 Ross Girshick。快速而准确的模型缩放。发表于 CVPR，第 924–932
    页，2021 年。'
- en: '[138] Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, and Tong Zhang.
    Model rubik’s cube: Twisting resolution, depth and width for tinynets. NeurIPS,
    33:19353–19364, 2020.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Kai Han, Yunhe Wang, Qiulin Zhang, Wei Zhang, Chunjing Xu, 和 Tong Zhang.
    模型魔方：扭曲解决方案，深度与宽度用于 TinyNets。在 NeurIPS 会议中，33:19353–19364，2020年。'
- en: '[139] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing
    neural network architectures using reinforcement learning. In ICLR, 2017.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Bowen Baker, Otkrist Gupta, Nikhil Naik, 和 Ramesh Raskar. 使用强化学习设计神经网络架构。在
    ICLR 会议中，2017年。'
- en: '[140] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning
    transferable architectures for scalable image recognition. In CVPR, pages 8697–8710,
    2018.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, 和 Quoc V Le. 学习可转移的架构以进行可扩展图像识别。在
    CVPR 会议中，页码 8697–8710，2018年。'
- en: '[141] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon
    Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image
    classifiers. In ICML, pages 2902–2911\. PMLR, 2017.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon
    Suematsu, Jie Tan, Quoc V Le, 和 Alexey Kurakin. 大规模图像分类器进化。在 ICML 会议中，页码 2902–2911。PMLR，2017年。'
- en: '[142] Lingxi Xie and Alan Yuille. Genetic cnn. In ICCV, pages 1379–1388, 2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Lingxi Xie 和 Alan Yuille. 遗传 CNN。在 ICCV 会议中，页码 1379–1388，2017年。'
- en: '[143] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized
    evolution for image classifier architecture search. In AAAI, pages 4780–4789,
    2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Esteban Real, Alok Aggarwal, Yanping Huang, 和 Quoc V Le. 图像分类器架构搜索的正则化进化。在
    AAAI 会议中，页码 4780–4789，2019年。'
- en: '[144] Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient
    architecture search for convolutional neural networks. In ICLR, 2018.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Thomas Elsken, Jan-Hendrik Metzen, 和 Frank Hutter. 简单高效的卷积神经网络架构搜索。在
    ICLR 会议中，2018年。'
- en: '[145] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level
    network transformation for efficient architecture search. In ICML, pages 678–687\.
    PMLR, 2018.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, 和 Yong Yu. 针对高效架构搜索的路径级网络转换。在
    ICML 会议中，页码 678–687。PMLR，2018年。'
- en: '[146] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and
    Koray Kavukcuoglu. Hierarchical representations for efficient architecture search.
    In ICLR, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, 和 Koray
    Kavukcuoglu. 用于高效架构搜索的层次化表示。在 ICLR 会议中，2018年。'
- en: '[147] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient
    neural architecture search via parameters sharing. In ICML, pages 4095–4104\.
    PMLR, 2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, 和 Jeff Dean. 通过参数共享进行高效的神经架构搜索。在
    ICML 会议中，页码 4095–4104。PMLR，2018年。'
- en: '[148] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan,
    and Quoc Le. Understanding and simplifying one-shot architecture search. In ICML,
    pages 550–559\. PMLR, 2018.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan,
    和 Quoc Le. 理解和简化一次性架构搜索。在 ICML 会议中，页码 550–559。PMLR，2018年。'
- en: '[149] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic
    neural architecture search. In ICLR, 2019.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Sirui Xie, Hehui Zheng, Chunxiao Liu, 和 Liang Lin. Snas：随机神经架构搜索。在 ICLR
    会议中，2019年。'
- en: '[150] Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in
    four gpu hours. In CVPR, pages 1761–1770, 2019.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Xuanyi Dong 和 Yi Yang. 在四个 GPU 小时内搜索稳健的神经架构。在 CVPR 会议中，页码 1761–1770，2019年。'
- en: '[151] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox,
    and Frank Hutter. Understanding and robustifying differentiable architecture search.
    In ICLR, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox,
    和 Frank Hutter. 理解和增强可微架构搜索。在 ICLR 会议中，2020年。'
- en: '[152] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and
    Qi Tian. Network adjustment: Channel and block search guided by resource utilization
    ratio. IJCV, 130(3):820–835, 2022.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, 和 Qi
    Tian. 网络调整：由资源利用率引导的通道和块搜索。在 IJCV 期刊中，130(3):820–835，2022年。'
- en: '[153] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable
    architecture search: Bridging the depth gap between search and evaluation. In
    ICCV, pages 1294–1303, 2019.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Xin Chen, Lingxi Xie, Jun Wu, 和 Qi Tian. 渐进式可微架构搜索：弥合搜索与评估之间的深度差距。在 ICCV
    会议中，页码 1294–1303，2019年。'
- en: '[154] Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, and Changshui
    Zhang. Greedynas: Towards fast one-shot nas with greedy supernet. In CVPR, pages
    1999–2008, 2020.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, 和 Changshui Zhang.
    Greedynas：基于贪婪超级网络的快速一次性 NAS。在 CVPR 会议中，页码 1999–2008，2020年。'
- en: '[155] Andrew Brock, Theo Lim, JM Ritchie, and Nick Weston. Smash: One-shot
    model architecture search through hypernetworks. In ICLR, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Andrew Brock, Theo Lim, JM Ritchie, 和 Nick Weston. Smash：通过超网络进行的一次性模型架构搜索。在
    ICLR 会议中，2018年。'
- en: '[156] Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for
    neural architecture search. In ICLR, 2019.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Chris Zhang, Mengye Ren 和 Raquel Urtasun。用于神经网络架构搜索的图超网络。发表于 ICLR，2019年。'
- en: '[157] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei,
    and Jian Sun. Single path one-shot neural architecture search with uniform sampling.
    In ECCV, pages 544–560\. Springer, 2020.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei
    和 Jian Sun。使用均匀采样的单路径一次性神经网络架构搜索。发表于 ECCV，页码 544–560。Springer，2020年。'
- en: '[158] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans,
    Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling
    up neural architecture search with big single-stage models. In ECCV, pages 702–717\.
    Springer, 2020.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans,
    Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang 和 Quoc Le。Bignas: 扩展神经网络架构搜索的大型单阶段模型。发表于
    ECCV，页码 702–717。Springer，2020年。'
- en: '[159] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming
    Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware
    efficient convnet design via differentiable neural architecture search. In CVPR,
    pages 10734–10742, 2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming
    Wu, Yuandong Tian, Peter Vajda, Yangqing Jia 和 Kurt Keutzer。Fbnet: 通过可微神经架构搜索进行硬件感知高效卷积网络设计。发表于
    CVPR，页码 10734–10742，2019年。'
- en: '[160] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar
    Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV,
    pages 4489–4497, 2015.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani 和 Manohar Paluri。使用
    3D 卷积网络学习时空特征。发表于 ICCV，页码 4489–4497，2015年。'
- en: '[161] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a
    new model and the kinetics dataset. In CVPR, pages 6299–6308, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Joao Carreira 和 Andrew Zisserman。行动识别的未来如何？一种新模型及其动力学数据集。发表于 CVPR，页码
    6299–6308，2017年。'
- en: '[162] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient
    convolutional network for online video understanding. In ECCV, pages 695–712,
    2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Mohammadreza Zolfaghari, Kamaljeet Singh 和 Thomas Brox。Eco: 用于在线视频理解的高效卷积网络。发表于
    ECCV，页码 695–712，2018年。'
- en: '[163] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
    Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video
    classification. In ECCV, pages 305–321, 2018.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu 和 Kevin Murphy。重新思考时空特征学习：视频分类中的速度与准确性权衡。发表于
    ECCV，页码 305–321，2018年。'
- en: '[164] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification
    with channel-separated convolutional networks. In ICCV, pages 5552–5561, 2019.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Du Tran, Heng Wang, Lorenzo Torresani 和 Matt Feiszli。使用通道分离卷积网络的视频分类。发表于
    ICCV，页码 5552–5561，2019年。'
- en: '[165] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video
    recognition. In CVPR, pages 203–213, 2020.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Christoph Feichtenhofer。X3d: 扩展架构以提高视频识别效率。发表于 CVPR，页码 203–213，2020年。'
- en: '[166] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
    Paluri. A closer look at spatiotemporal convolutions for action recognition. In
    CVPR, pages 6450–6459, 2018.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun 和 Manohar
    Paluri。深入探讨用于动作识别的时空卷积。发表于 CVPR，页码 6450–6459，2018年。'
- en: '[167] Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human action recognition
    using factorized spatio-temporal convolutional networks. In ICCV, pages 4597–4605,
    2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Lin Sun, Kui Jia, Dit-Yan Yeung 和 Bertram E Shi。使用分解时空卷积网络的人类动作识别。发表于
    ICCV，页码 4597–4605，2015年。'
- en: '[168] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation
    with pseudo-3d residual networks. In ICCV, pages 5533–5541, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Zhaofan Qiu, Ting Yao 和 Tao Mei。通过伪 3D 残差网络学习时空表示。发表于 ICCV，页码 5533–5541，2017年。'
- en: '[169] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal
    adaptive module for video recognition. In ICCV, pages 13708–13718, 2021.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian 和 Tong Lu。Tam: 用于视频识别的时间自适应模块。发表于
    ICCV，页码 13708–13718，2021年。'
- en: '[170] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal
    relational reasoning in videos. In ECCV, pages 803–818, 2018.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Bolei Zhou, Alex Andonian, Aude Oliva 和 Antonio Torralba。视频中的时间关系推理。发表于
    ECCV，页码 803–818，2018年。'
- en: '[171] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm:
    Spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000–2009,
    2019.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu 和 Junjie Yan。Stm: 用于动作识别的时空和运动编码。发表于
    ICCV，页码 2000–2009，2019年。'
- en: '[172] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang.
    Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages
    909–918, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang 和 Limin Wang。Tea:
    用于动作识别的时间激励和聚合。发表于 CVPR，页码 909–918，2020年。'
- en: '[173] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient
    video understanding. In ICCV, pages 7083–7093, 2019.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Ji Lin、Chuang Gan 和 Song Han。TSM：用于高效视频理解的时间偏移模块。在 ICCV 上，页面 7083–7093，2019
    年。'
- en: '[174] Hao Zhang, Yanbin Hao, and Chong-Wah Ngo. Token shift transformer for
    video classification. In ACM MM, pages 917–925, 2021.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Hao Zhang、Yanbin Hao 和 Chong-Wah Ngo。用于视频分类的 Token Shift Transformer。在
    ACM MM 上，页面 917–925，2021 年。'
- en: '[175] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift
    networks for video action recognition. In CVPR, pages 1102–1111, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Swathikiran Sudhakaran、Sergio Escalera 和 Oswald Lanz。用于视频动作识别的 Gate-Shift
    网络。在 CVPR 上，页面 1102–1111，2020 年。'
- en: '[176] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast
    networks for video recognition. In ICCV, pages 6202–6211, 2019.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Christoph Feichtenhofer、Haoqi Fan、Jitendra Malik 和 Kaiming He。Slowfast
    网络用于视频识别。在 ICCV 上，页面 6202–6211，2019 年。'
- en: '[177] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference
    networks for efficient action recognition. In CVPR, pages 1895–1904, 2021.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Limin Wang、Zhan Tong、Bin Ji 和 Gangshan Wu。Tdn：用于高效动作识别的时间差网络。在 CVPR 上，页面
    1895–1904，2021 年。'
- en: '[178] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić,
    and Cordelia Schmid. Vivit: A video vision transformer. In ICCV, pages 6836–6846,
    2021.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Anurag Arnab、Mostafa Dehghani、Georg Heigold、Chen Sun、Mario Lučić 和 Cordelia
    Schmid。Vivit：一种视频视觉 transformer。在 ICCV 上，页面 6836–6846，2021 年。'
- en: '[179] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez,
    and Georgios Tzimiropoulos. Space-time mixing attention for video transformer.
    NeurIPS, 34:19594–19607, 2021.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Adrian Bulat、Juan Manuel Perez Rua、Swathikiran Sudhakaran、Brais Martinez
    和 Georgios Tzimiropoulos。视频 transformer 的时空混合注意力。NeurIPS，34：19594–19607，2021 年。'
- en: '[180] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and
    Han Hu. Video swin transformer. In CVPR, pages 3202–3211, 2022.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Ze Liu、Jia Ning、Yue Cao、Yixuan Wei、Zheng Zhang、Stephen Lin 和 Han Hu。视频
    swin transformer。在 CVPR 上，页面 3202–3211，2022 年。'
- en: '[181] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li,
    and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation
    learning. In ICLR, 2022.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Kunchang Li、Yali Wang、Gao Peng、Guanglu Song、Yu Liu、Hongsheng Li 和 Yu
    Qiao。Uniformer：用于高效时空表示学习的统一 transformer。在 ICLR 上，2022 年。'
- en: '[182] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun,
    and Cordelia Schmid. Multiview transformers for video recognition. In CVPR, pages
    3333–3343, 2022.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Shen Yan、Xuehan Xiong、Anurag Arnab、Zhichao Lu、Mi Zhang、Chen Sun 和 Cordelia
    Schmid。用于视频识别的多视图 transformers。在 CVPR 上，页面 3333–3343，2022 年。'
- en: '[183] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze,
    Christoph Feichtenhofer, Andrea Vedaldi, and João F Henriques. Keeping your eye
    on the ball: Trajectory attention in video transformers. NeurIPS, 34:12493–12506,
    2021.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Mandela Patrick、Dylan Campbell、Yuki Asano、Ishan Misra、Florian Metze、Christoph
    Feichtenhofer、Andrea Vedaldi 和 João F Henriques。保持关注：视频 transformer 中的轨迹注意力。NeurIPS，34：12493–12506，2021
    年。'
- en: '[184] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention
    all you need for video understanding? In ICML, 2021.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Gedas Bertasius、Heng Wang 和 Lorenzo Torresani。时空注意力是否是视频理解的终极需求？在 ICML
    上，2021 年。'
- en: '[185] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer
    network. In ICCV, pages 3163–3172, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Daniel Neimark、Omri Bar、Maya Zohar 和 Dotan Asselmann。视频 transformer 网络。在
    ICCV 上，页面 3163–3172，2021 年。'
- en: '[186] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep
    learning on point sets for 3d classification and segmentation. In CVPR, pages
    652–660, 2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Charles R Qi、Hao Su、Kaichun Mo 和 Leonidas J Guibas。Pointnet：用于 3D 分类和分割的深度学习点集。在
    CVPR 上，页面 652–660，2017 年。'
- en: '[187] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 30,
    2017.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Charles Ruizhongtai Qi、Li Yi、Hao Su 和 Leonidas J Guibas。Pointnet++：在度量空间中对点集的深度层次特征学习。NeurIPS，30，2017
    年。'
- en: '[188] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph
    attention convolution for point cloud semantic segmentation. In CVPR, pages 10296–10305,
    2019.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Lei Wang、Yuchun Huang、Yaolin Hou、Shenman Zhang 和 Jie Shan。图注意力卷积用于点云语义分割。在
    CVPR 上，页面 10296–10305，2019 年。'
- en: '[189] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein,
    and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions
    On Graphics, 38(5):1–12, 2019.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Yue Wang、Yongbin Sun、Ziwei Liu、Sanjay E Sarma、Michael M Bronstein 和 Justin
    M Solomon。用于点云学习的动态图 CNN。ACM Transactions On Graphics，38(5)：1–12，2019 年。'
- en: '[190] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep
    learning on point sets with parameterized convolutional filters. In ECCV, pages
    87–102, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Yifan Xu、Tianqi Fan、Mingye Xu、Long Zeng 和 Yu Qiao。Spidercnn：使用参数化卷积滤波器的点集深度学习。在
    ECCV 上，页面 87–102，2018 年。'
- en: '[191] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and Qian-Yi Zhou. Tangent
    convolutions for dense prediction in 3d. In CVPR, pages 3887–3896, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] 马克西姆·塔塔尔琴科、朴在锡、弗拉德伦·科尔顿和周乾益。《用于3d的切线卷积》。在CVPR会议上，第3887–3896页，2018年。'
- en: '[192] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui,
    François Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution
    for point clouds. In ICCV, pages 6411–6420, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] 于格·托马斯、查尔斯·R·齐、让-埃马纽埃尔·德沙乌、比阿特丽斯·马尔科特基和利昂尼达斯·J·吉巴斯。《Kpconv：用于点云的灵活且可变形卷积》。在ICCV会议上，第6411–6420页，2019年。'
- en: '[193] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen.
    Pointcnn: Convolution on x-transformed points. NeurIPS, 31, 2018.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] 李杨艳、布瑞、孙铭超、吴伟、狄欣寒和陈宝全。《Pointcnn：在x变换点上的卷积》。NeurIPS，第31卷，2018年。'
- en: '[194] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape
    convolutional neural network for point cloud analysis. In CVPR, pages 8895–8904,
    2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] 刘永成、范斌、向世明和潘春红。《用于点云分析的关系-形状卷积神经网络》。在CVPR会议上，第8895–8904页，2019年。'
- en: '[195] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional
    networks on 3d point clouds. In CVPR, pages 9621–9630, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] 吴文轩、齐中昂和李富新。《Pointconv：在3d点云上的深度卷积网络》。在CVPR会议上，第9621–9630页，2019年。'
- en: '[196] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at
    local aggregation operators in point cloud analysis. In ECCV, pages 326–342\.
    Springer, 2020.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] 刘泽、胡寒、曹岳、张征和童欣。《对点云分析中的局部聚合算子的深入探讨》。在ECCV会议上，第326–342页。斯普林格，2020年。'
- en: '[197] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu,
    Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3d point cloud segmentation.
    In CVPR, pages 8500–8509, 2022.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] 莱鑫、刘建辉、李江、王立伟、赵恒爽、刘舒、齐晓娟和贾佳雅。《用于3d点云分割的分层变换器》。在CVPR会议上，第8500–8509页，2022年。'
- en: '[198] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun.
    Point transformer. In ICCV, pages 16259–16268, 2021.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] 赵恒爽、李江、贾佳雅、菲利普·H·S·托尔和弗拉德伦·科尔顿。《点变换器》。在ICCV会议上，第16259–16268页，2021年。'
- en: '[199] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader
    Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++
    with improved training and scaling strategies. In NeurIPS, 2022.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] 钱国成、李宇辰、彭浩文、麦金杰、哈桑·阿贝德·阿尔·卡德尔·哈穆德、穆罕默德·艾尔霍塞尼和伯纳德·加内姆。《Pointnext：通过改进的训练和扩展策略重新审视Pointnet++》。在NeurIPS会议上，2022年。'
- en: '[200] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural
    network for real-time object recognition. In IROS, pages 922–928\. IEEE, 2015.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] 丹尼尔·马图拉纳和塞巴斯蒂安·谢雷。《Voxnet：用于实时物体识别的3d卷积神经网络》。在IROS会议上，第922–928页。IEEE，2015年。'
- en: '[201] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic
    segmentation with submanifold sparse convolutional networks. In CVPR, pages 9224–9232,
    2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] 本杰明·格雷厄姆、马丁·恩格尔克和劳伦斯·范德马滕。《使用子流形稀疏卷积网络的3d语义分割》。在CVPR会议上，第9224–9232页，2018年。'
- en: '[202] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal
    convnets: Minkowski convolutional neural networks. In CVPR, pages 3075–3084, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] 崔斯托弗·崔、郭俊勇和西尔维奥·萨瓦雷斯。《4d时空卷积网络：明科斯基卷积神经网络》。在CVPR会议上，第3075–3084页，2019年。'
- en: '[203] Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Focal
    sparse convolutional networks for 3d object detection. In CVPR, pages 5428–5437,
    2022.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] 陈玉康、李燕伟、张向宇、孙剑和贾佳雅。《用于3d目标检测的焦点稀疏卷积网络》。在CVPR会议上，第5428–5437页，2022年。'
- en: '[204] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object
    detection. In CVPR, pages 10529–10538, 2020.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] 邵帅·施、郭超徐、李江、王喆、施建平、王晓刚和李鸿生。《Pv-rcnn：用于3d目标检测的点-体素特征集抽象》。在CVPR会议上，第10529–10538页，2020年。'
- en: '[205] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping
    Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction
    with local vector representation for 3d object detection. IJCV, pages 1–21, 2022.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] 邵帅·施、李江、邓佳俊、王喆、郭超徐、施建平、王晓刚和李鸿生。《Pv-rcnn++：带有局部向量表示的点-体素特征集抽象，用于3d目标检测》。IJCV，第1–21页，2022年。'
- en: '[206] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced
    grouping and sampling for point cloud 3d object detection. arXiv preprint arXiv:1908.09492,
    2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] 朱本晋、姜正凯、周祥鑫、李泽名和于刚。《用于点云3d目标检测的类别平衡分组与采样》。arXiv预印本 arXiv:1908.09492，2019年。'
- en: '[207] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for
    efficient 3d deep learning. In NeurIPS, volume 32, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] 刘志坚、唐浩天、林宇君和韩松。《用于高效3d深度学习的点-体素cnn》。在NeurIPS会议上，第32卷，2019年。'
- en: '[208] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang,
    and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution.
    In ECCV, pages 685–702\. Springer, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang,
    和 Song Han. 通过稀疏点-体素卷积搜索高效的3D架构。发表于ECCV, 页码 685–702, Springer, 2020。'
- en: '[209] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller.
    Multi-view convolutional neural networks for 3d shape recognition. In ICCV, pages
    945–953, 2015.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Hang Su, Subhransu Maji, Evangelos Kalogerakis, 和 Erik Learned-Miller.
    用于3D形状识别的多视图卷积神经网络。发表于ICCV, 页码 945–953, 2015。'
- en: '[210] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional
    network for 3d shape analysis. In CVPR, pages 1850–1859, 2020.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Xin Wei, Ruixuan Yu, 和 Jian Sun. View-gcn: 用于3D形状分析的视图基础图卷积网络。发表于CVPR,
    页码 1850–1859, 2020。'
- en: '[211] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view
    transformation network for 3d shape recognition. In ICCV, pages 1–11, 2021.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Abdullah Hamdi, Silvio Giancola, 和 Bernard Ghanem. Mvtn: 用于3D形状识别的多视图变换网络。发表于ICCV,
    页码 1–11, 2021。'
- en: '[212] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki.
    Gift: A real-time and scalable 3d shape search engine. In CVPR, pages 5023–5032,
    2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, 和 Longin Jan Latecki.
    Gift: 实时且可扩展的3D形状搜索引擎。发表于CVPR, 页码 5023–5032, 2016。'
- en: '[213] Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, and Yue Gao. Mlvcnn:
    Multi-loop-view convolutional neural network for 3d shape retrieval. In AAAI,
    pages 8513–8520, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, 和 Yue Gao. Mlvcnn: 用于3D形状检索的多循环视角卷积神经网络。发表于AAAI,
    页码 8513–8520, 2019。'
- en: '[214] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet:
    Joint object categorization and pose estimation using multiviews from unsupervised
    viewpoints. In CVPR, pages 5010–5019, 2018.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Asako Kanezaki, Yasuyuki Matsushita, 和 Yoshifumi Nishida. Rotationnet:
    使用来自无监督视角的多视图进行对象分类和姿态估计。发表于CVPR, 页码 5010–5019, 2018。'
- en: '[215] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet:
    Fast inference via early exiting from deep neural networks. In ICPR, 2016.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Surat Teerapittayanon, Bradley McDanel, 和 Hsiang-Tsung Kung. Branchynet:
    通过从深度神经网络的早期退出实现快速推理。发表于ICPR, 2016。'
- en: '[216] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive
    neural networks for efficient inference. In ICML, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, 和 Venkatesh Saligrama. 高效推理的自适应神经网络。发表于ICML,
    2017。'
- en: '[217] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang.
    Resolution adaptive networks for efficient inference. In CVPR, 2020.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, 和 Gao Huang. 分辨率自适应网络用于高效推理。发表于CVPR,
    2020。'
- en: '[218] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not all
    images are worth 16x16 words: Dynamic transformers for efficient image recognition.
    NeurIPS, 2021.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, 和 Gao Huang. 不是所有图像都值16x16字:
    动态变换器用于高效图像识别。发表于NeurIPS, 2021。'
- en: '[219] Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu,
    Chao Deng, Junlan Feng, Shiji Song, and Gao Huang. Dynamic perceiver for efficient
    visual recognition. In ICCV, 2023.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Yizeng Han, Dongchen Han, Zeyu Liu, Yulin Wang, Xuran Pan, Yifan Pu,
    Chao Deng, Junlan Feng, Shiji Song, 和 Gao Huang. 高效视觉识别的动态感知器。发表于ICCV, 2023。'
- en: '[220] Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, and Gao Huang. Improved
    techniques for training adaptive deep networks. In ICCV, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Hao Li, Hong Zhang, Xiaojuan Qi, Ruigang Yang, 和 Gao Huang. 用于训练自适应深度网络的改进技术。发表于ICCV,
    2019。'
- en: '[221] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao,
    Wenhui Huang, Chao Deng, and Gao Huang. Learning to weight samples for dynamic
    early-exiting networks. In ECCV, 2022.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao,
    Wenhui Huang, Chao Deng, 和 Gao Huang. 学习对动态早期退出网络的样本加权。发表于ECCV, 2022。'
- en: '[222] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez.
    Skipnet: Learning dynamic routing in convolutional networks. In ECCV, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, 和 Joseph E Gonzalez.
    Skipnet: 在卷积网络中学习动态路由。发表于ECCV, 2018。'
- en: '[223] Andreas Veit and Serge Belongie. Convolutional networks with adaptive
    inference graphs. In ECCV, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Andreas Veit 和 Serge Belongie. 具有自适应推理图的卷积网络。发表于ECCV, 2018。'
- en: '[224] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang
    Jiang, and Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image
    recognition. In CVPR, 2022.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang
    Jiang, 和 Ser-Nam Lim. Adavit: 用于高效图像识别的自适应视觉变换器。发表于CVPR, 2022。'
- en: '[225] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping
    for learning conditional channel gated networks. In ICLR, 2019.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Babak Ehteshami Bejnordi, Tijmen Blankevoort, 和 Max Welling. 批量塑形用于学习条件通道门控网络。发表于ICLR,
    2019。'
- en: '[226] Charles Herrmann, Richard Strong Bowen, and Ramin Zabih. Channel selection
    using gumbel softmax. In ECCV, 2020.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Charles Herrmann, Richard Strong Bowen 和 Ramin Zabih。使用 Gumbel Softmax
    的通道选择。发表于 ECCV，2020年。'
- en: '[227] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li, and
    Xiaojun Chang. Dynamic slimmable network. In CVPR, 2021.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang, Zhihui Li 和 Xiaojun
    Chang。动态可瘦网络。发表于 CVPR，2021年。'
- en: '[228] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi,
    and Aditya Nori. Adaptive neural trees. In ICML, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi 和
    Aditya Nori。自适应神经树。发表于 ICML，2019年。'
- en: '[229] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul
    Mazumder. The tree ensemble layer: Differentiability meets conditional computation.
    In ICML, 2020.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan 和 Rahul Mazumder。树集成层：可微分性遇上条件计算。发表于
    ICML，2020年。'
- en: '[230] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv:
    Conditionally parameterized convolutions for efficient inference. In NeurIPS,
    2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] Brandon Yang, Gabriel Bender, Quoc V Le 和 Jiquan Ngiam。Condconv：用于高效推理的条件参数化卷积。发表于
    NeurIPS，2019年。'
- en: '[231] Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan,
    Zidong Wang, Shiji Song, and Gao Huang. Adaptive rotated convolution for rotated
    object detection. In ICCV, 2023.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan,
    Zidong Wang, Shiji Song 和 Gao Huang。用于旋转物体检测的自适应旋转卷积。发表于 ICCV，2023年。'
- en: '[232] An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei, and Min Sun.
    Instanas: Instance-aware neural architecture search. In AAAI, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] An-Chieh Cheng, Chieh Hubert Lin, Da-Cheng Juan, Wei Wei 和 Min Sun。Instanas：实例感知的神经架构搜索。发表于
    AAAI，2020年。'
- en: '[233] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang,
    and Jian Sun. Learning dynamic routing for semantic segmentation. In CVPR, 2020.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang
    和 Jian Sun。学习动态路由用于语义分割。发表于 CVPR，2020年。'
- en: '[234] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.
    Learning deep features for discriminative localization. In CVPR, 2016.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva 和 Antonio Torralba。学习深度特征以实现辨别定位。发表于
    CVPR，2016年。'
- en: '[235] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less:
    A more complicated network with less inference complexity. In CVPR, 2017.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] Xuanyi Dong, Junshi Huang, Yi Yang 和 Shuicheng Yan。更多即更少：一个更复杂但推理复杂度更低的网络。发表于
    CVPR，2017年。'
- en: '[236] Thomas Verelst and Tinne Tuytelaars. Dynamic convolutions: Exploiting
    spatial sparsity for faster inference. In CVPR, 2020.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] Thomas Verelst 和 Tinne Tuytelaars。动态卷积：利用空间稀疏性加速推理。发表于 CVPR，2020年。'
- en: '[237] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially
    adaptive inference with stochastic feature sampling and interpolation. In ECCV,
    2020.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang 和 Stephen Lin。带有随机特征采样和插值的空间自适应推理。发表于
    ECCV，2020年。'
- en: '[238] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, and Haojun
    Jiang. Spatially adaptive feature refinement for efficient inference. TIP, 2021.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Yitian Zhang 和 Haojun Jiang。用于高效推理的空间自适应特征细化。发表于
    TIP，2021年。'
- en: '[239] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and
    Yichen Wei. Deformable convolutional networks. In ICCV, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu 和 Yichen
    Wei。可变形卷积网络。发表于 ICCV，2017年。'
- en: '[240] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets
    v2: More deformable, better results. In CVPR, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Xizhou Zhu, Han Hu, Stephen Lin 和 Jifeng Dai。Deformable convnets v2:
    更具可变形性，结果更佳。发表于 CVPR，2019年。'
- en: '[241] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu,
    Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale
    vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778,
    2022.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu,
    Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li 等。Internimage：探索大规模视觉基础模型与可变形卷积。arXiv
    预印本 arXiv:2211.05778，2022年。'
- en: '[242] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision
    transformer with deformable attention. In CVPR, 2022.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li 和 Gao Huang。具备可变形注意力的视觉变换器。发表于
    CVPR，2022年。'
- en: '[243] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of
    visual attention. NeurIPS, 2014.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] Volodymyr Mnih, Nicolas Heess, Alex Graves 等。视觉注意力的递归模型。发表于 NeurIPS，2014年。'
- en: '[244] Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and Wei Xu. Dynamic
    computational time for visual attention. In ICCVW, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Zhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen 和 Wei Xu。视觉注意力的动态计算时间。发表于
    ICCVW，2017年。'
- en: '[245] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    CVPR, 2017.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] Jianlong Fu, Heliang Zheng 和 Tao Mei。看得更近才能看得更好：用于细粒度图像识别的递归注意卷积神经网络。发表于
    CVPR，2017。'
- en: '[246] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang.
    Glance and focus: a dynamic approach to reducing spatial redundancy in image classification.
    In NeurIPS, 2020.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang 和 Gao Huang。瞥视与聚焦：减少图像分类中空间冗余的动态方法。发表于
    NeurIPS，2020。'
- en: '[247] Gao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei
    Qi, and Shiji Song. Glance and focus networks for dynamic visual recognition.
    TPAMI, 2022.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] Gao Huang, Yulin Wang, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei
    Qi 和 Shiji Song。用于动态视觉识别的瞥视与聚焦网络。TPAMI，2022。'
- en: '[248] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement
    learning: A survey. Journal of artificial intelligence research, 1996.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] Leslie Pack Kaelbling, Michael L Littman 和 Andrew W Moore。强化学习：综述。人工智能研究杂志，1996。'
- en: '[249] Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. Scale-aware
    face detection. In CVPR, 2017.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li 和 Xiaolin Hu。尺度感知的人脸检测。发表于
    CVPR，2017。'
- en: '[250] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan,
    and Yunhe Wang. Dynamic resolution network. NeurIPS, 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong Lan
    和 Yunhe Wang。动态分辨率网络。发表于 NeurIPS，2021。'
- en: '[251] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S Davis. Liteeval:
    A coarse-to-fine framework for resource efficient video recognition. NeurIPS,
    2019.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang 和 Larry S Davis。Liteeval：一个从粗到细的资源高效视频识别框架。发表于
    NeurIPS，2019。'
- en: '[252] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 1997.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] Sepp Hochreiter 和 Jürgen Schmidhuber。长短期记忆。神经计算，1997。'
- en: '[253] Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid
    Karlinsky, Kate Saenko, Aude Oliva, and Rogerio Feris. Adafuse: Adaptive temporal
    fusion network for efficient action recognition. In ICLR, 2021.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] Yue Meng, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid
    Karlinsky, Kate Saenko, Aude Oliva 和 Rogerio Feris。Adafuse：用于高效动作识别的自适应时序融合网络。发表于
    ICLR，2021。'
- en: '[254] Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Aude Oliva, Rogerio
    Feris, and Kate Saenko. Dynamic network quantization for efficient video inference.
    In ICCV, 2021.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] Ximeng Sun, Rameswar Panda, Chun-Fu Richard Chen, Aude Oliva, Rogerio
    Feris 和 Kate Saenko。动态网络量化用于高效视频推理。发表于 ICCV，2021。'
- en: '[255] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid
    Karlinsky, Aude Oliva, Kate Saenko, and Rogerio Feris. Ar-net: Adaptive frame
    resolution for efficient action recognition. In ECCV, 2020.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Yue Meng, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid
    Karlinsky, Aude Oliva, Kate Saenko 和 Rogerio Feris。Ar-net：用于高效动作识别的自适应帧分辨率。发表于
    ECCV，2020。'
- en: '[256] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end
    learning of action detection from frame glimpses in videos. In CVPR, 2016.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] Serena Yeung, Olga Russakovsky, Greg Mori 和 Li Fei-Fei。从视频中的帧瞥视进行动作检测的端到端学习。发表于
    CVPR，2016。'
- en: '[257] Humam Alwassel, Fabian Caba Heilbron, and Bernard Ghanem. Action search:
    Spotting actions in videos and its application to temporal action localization.
    In ECCV, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Humam Alwassel, Fabian Caba Heilbron 和 Bernard Ghanem。动作搜索：在视频中识别动作及其在时序动作定位中的应用。发表于
    ECCV，2018。'
- en: '[258] Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, and Yi Yang.
    Watching a small portion could be as good as watching all: Towards efficient video
    classification. In IJCAI, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge 和 Yi Yang。观看一小部分可能与观看全部效果相当：面向高效视频分类。发表于
    IJCAI，2018。'
- en: '[259] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, and Shilei
    Wen. Dynamic inference: A new approach toward efficient video action recognition.
    In CVPRW, 2020.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang 和 Shilei Wen。动态推理：朝向高效视频动作识别的新方法。发表于
    CVPRW，2020。'
- en: '[260] Amir Ghodrati, Babak Ehteshami Bejnordi, and Amirhossein Habibian. Frameexit:
    Conditional early exiting for efficient video recognition. In CVPR, 2021.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] Amir Ghodrati, Babak Ehteshami Bejnordi 和 Amirhossein Habibian。Frameexit：用于高效视频识别的条件提前退出。发表于
    CVPR，2021。'
- en: '[261] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li, and Jie Zhou. Deep progressive
    reinforcement learning for skeleton-based action recognition. In CVPR, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] Yansong Tang, Yi Tian, Jiwen Lu, Peiyang Li 和 Jie Zhou。基于骨架的动作识别的深度渐进强化学习。发表于
    CVPR，2018。'
- en: '[262] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, and Shilei Wen. Multi-agent
    reinforcement learning based frame sampling for effective untrimmed video recognition.
    In ICCV, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, 和 Shilei Wen. 基于多智能体强化学习的帧采样用于有效的非裁剪视频识别。在
    ICCV, 2019.'
- en: '[263] Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, and Limin Wang. Dynamic sampling
    networks for efficient action recognition in videos. TIP, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, 和 Limin Wang. 动态采样网络用于高效的视频动作识别。TIP,
    2020.'
- en: '[264] Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov,
    Nikita Orlov, Humphrey Shi, and Gao Huang. Adafocus v2: End-to-end training of
    spatial dynamic networks for video recognition. In CVPR, 2022.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Yulin Wang, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov,
    Nikita Orlov, Humphrey Shi, 和 Gao Huang. Adafocus v2: 端到端训练的空间动态网络用于视频识别。在 CVPR,
    2022.'
- en: '[265] Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita
    Orlov, Shiji Song, Humphrey Shi, and Gao Huang. Adafocusv3: On unified spatial-temporal
    dynamic video recognition. In ECCV, 2022.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Yulin Wang, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita
    Orlov, Shiji Song, Humphrey Shi, 和 Gao Huang. Adafocusv3: 统一空间-时间动态视频识别。在 ECCV,
    2022.'
- en: '[266] Ziwei Zheng, Le Yang, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, and
    Fan Li. Dynamic spatial focus for efficient compressed video action recognition.
    IEEE TCSVT, 2023.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] Ziwei Zheng, Le Yang, Yulin Wang, Miao Zhang, Lijun He, Gao Huang, 和
    Fan Li. 动态空间聚焦用于高效的压缩视频动作识别。IEEE TCSVT, 2023.'
- en: '[267] Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, and Larry S Davis. 2d or
    not 2d? adaptive 3d convolution selection for efficient video recognition. In
    CVPR, 2021.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, 和 Larry S Davis. 2D 还是非2D?
    自适应 3D 卷积选择用于高效视频识别。在 CVPR, 2021.'
- en: '[268] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
    feature hierarchies for accurate object detection and semantic segmentation. In
    CVPR, pages 580–587, 2014.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] Ross Girshick, Jeff Donahue, Trevor Darrell, 和 Jitendra Malik. 用于准确对象检测和语义分割的丰富特征层次。在
    CVPR, 页码 580–587, 2014.'
- en: '[269] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based
    convolutional networks for accurate object detection and segmentation. TPAMI,
    38(1):142–158, 2015.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Ross Girshick, Jeff Donahue, Trevor Darrell, 和 Jitendra Malik. 基于区域的卷积网络用于准确的对象检测和分割。TPAMI,
    38(1):142–158, 2015.'
- en: '[270] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid
    pooling in deep convolutional networks for visual recognition. TPAMI, 37(9):1904–1916,
    2015.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 深度卷积网络中的空间金字塔池化用于视觉识别。TPAMI,
    37(9):1904–1916, 2015.'
- en: '[271] Ross Girshick. Fast r-cnn. In ICCV, pages 1440–1448, 2015.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] Ross Girshick. Fast r-cnn。在 ICCV, 页码 1440–1448, 2015.'
- en: '[272] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. NeurIPS, 28,
    2015.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. Faster r-cnn: 通过区域提议网络实现实时对象检测。NeurIPS,
    28, 2015.'
- en: '[273] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn:
    Towards real-time object detection with region proposal networks. TPAMI, 39(06):1137–1149,
    2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. Faster r-cnn: 通过区域提议网络实现实时对象检测。TPAMI,
    39(06):1137–1149, 2017.'
- en: '[274] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection
    via region-based fully convolutional networks. NeurIPS, 29, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] Jifeng Dai, Yi Li, Kaiming He, 和 Jian Sun. R-fcn: 通过基于区域的全卷积网络进行对象检测。NeurIPS,
    29, 2016.'
- en: '[275] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian
    Sun. Light-head r-cnn: In defense of two-stage object detector. arXiv preprint
    arXiv:1711.07264, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, 和 Jian Sun.
    Light-head r-cnn: 为两阶段对象检测器辩护。arXiv 预印本 arXiv:1711.07264, 2017.'
- en: '[276] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
    and Serge Belongie. Feature pyramid networks for object detection. In CVPR, pages
    2117–2125, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan,
    和 Serge Belongie. 用于对象检测的特征金字塔网络。在 CVPR, 页码 2117–2125, 2017.'
- en: '[277] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Joseph Redmon 和 Ali Farhadi. Yolov3: 一项渐进的改进。arXiv 预印本 arXiv:1804.02767,
    2018.'
- en: '[278] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4:
    Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934,
    2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] Alexey Bochkovskiy, Chien-Yao Wang, 和 Hong-Yuan Mark Liao. Yolov4: 对象检测的最佳速度和准确性。arXiv
    预印本 arXiv:2004.10934, 2020.'
- en: '[279] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In
    CVPR, pages 7263–7271, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] Joseph Redmon 和 Ali Farhadi. Yolo9000: 更好、更快、更强。在 CVPR, 页码 7263–7271,
    2017.'
- en: '[280] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one
    representation: Unified network for multiple tasks. arXiv preprint arXiv:2105.04206,
    2021.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] 钱耀·王、易豪·叶和洪远·马克·廖。你只学习一个表示：多任务统一网络。arXiv 预印本 arXiv:2105.04206, 2021。'
- en: '[281] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In ICCV, pages 2980–2988, 2017.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] 林松懿、普里亚·戈亚尔、罗斯·吉尔希克、凯明·何和皮奥特·美元。用于密集目标检测的焦点损失。发表于 ICCV, 页码 2980–2988,
    2017。'
- en: '[282] Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints.
    In ECCV, pages 734–750, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] 黑·劳和贾·邓。Cornernet：将对象检测为成对关键点。发表于 ECCV, 页码 734–750, 2018。'
- en: '[283] Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl. Bottom-up object
    detection by grouping extreme and center points. In CVPR, pages 850–859, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] 兴义·周、佳成·卓和菲利普·克拉亨布赫尔。通过分组极值点和中心点进行自下而上的目标检测。发表于 CVPR, 页码 850–859, 2019。'
- en: '[284] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points.
    arXiv preprint arXiv:1904.07850, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] 兴义·周、德全·王和菲利普·克拉亨布赫尔。对象作为点。arXiv 预印本 arXiv:1904.07850, 2019。'
- en: '[285] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
    Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.
    In ECCV, pages 213–229\. Springer, 2020.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] 尼古拉斯·卡里昂、弗朗西斯科·马萨、加布里埃尔·辛奈夫、尼古拉斯·乌苏尼耶、亚历山大·基里洛夫和谢尔盖·扎戈鲁伊科。基于变压器的端到端目标检测。发表于
    ECCV, 页码 213–229\. Springer, 2020。'
- en: '[286] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
    Deformable detr: Deformable transformers for end-to-end object detection. In ICLR,
    2021.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] 习周·朱、韦杰·苏、乐伟·卢、彬·李、晓刚·王和季锋·戴。可变形 DETR：用于端到端目标检测的可变形变压器。发表于 ICLR, 2021。'
- en: '[287] Christopher J Holder and Muhammad Shafique. On efficient real-time semantic
    segmentation: a survey. arXiv preprint arXiv:2206.08605, 2022.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] 克里斯托弗·J·霍尔德和穆罕默德·沙菲克。关于高效实时语义分割的调查。arXiv 预印本 arXiv:2206.08605, 2022。'
- en: '[288] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya
    Jia. Pyramid scene parsing network. In CVPR, pages 2881–2890, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] 恒霜·赵、剑平·石、小娟·齐、晓刚·王和佳雅·贾。金字塔场景解析网络。发表于 CVPR, 页码 2881–2890, 2017。'
- en: '[289] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation. TPAMI, 39(12):2481–2495,
    2017.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] 维贾伊·巴德里纳亚南、亚历克斯·肯德尔和罗伯托·奇波拉。Segnet：用于图像分割的深度卷积编码器-解码器架构。TPAMI, 39(12):2481–2495,
    2017。'
- en: '[290] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello.
    Enet: A deep neural network architecture for real-time semantic segmentation.
    arXiv preprint arXiv:1606.02147, 2016.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] 亚当·帕兹克、阿比舍克·乔拉西亚、尚必尔·金和尤金尼奥·库尔西埃洛。Enet：用于实时语义分割的深度神经网络架构。arXiv 预印本 arXiv:1606.02147,
    2016。'
- en: '[291] Abhishek Chaurasia and Eugenio Culurciello. Linknet: Exploiting encoder
    representations for efficient semantic segmentation. In 2017 IEEE visual communications
    and image processing (VCIP), pages 1–4\. IEEE, 2017.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] 阿比舍克·乔拉西亚和尤金尼奥·库尔西埃洛。Linknet：利用编码器表示进行高效语义分割。发表于 2017 IEEE 视觉通信与图像处理（VCIP），页码
    1–4\. IEEE, 2017。'
- en: '[292] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh
    Hajishirzi. Espnet: Efficient spatial pyramid of dilated convolutions for semantic
    segmentation. In ECCV, pages 552–568, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] 萨钦·梅赫塔、穆罕默德·拉斯特加里、阿纳特·卡斯比、琳达·夏皮罗和哈纳赫·哈吉希尔齐。Espnet：用于语义分割的高效空间金字塔膨胀卷积。发表于
    ECCV, 页码 552–568, 2018。'
- en: '[293] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng,
    and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic
    segmentation. In NeurIPS, 2022.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[293] 孟豪·郭、成泽·陆、琦斌·侯、郑宁·刘、明铭·程和世敏·胡。Segnext：重新思考卷积注意力设计用于语义分割。发表于 NeurIPS,
    2022。'
- en: '[294] Eduardo Romera, José M Alvarez, Luis M Bergasa, and Roberto Arroyo. Erfnet:
    Efficient residual factorized convnet for real-time semantic segmentation. IEEE
    Transactions on Intelligent Transportation Systems, 19(1):263–272, 2017.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[294] 爱德华多·罗梅拉、何塞·M·阿尔瓦雷斯、路易斯·M·贝尔加萨和罗伯托·阿罗约。Erfnet：高效残差因式分解卷积网络用于实时语义分割。IEEE
    智能交通系统汇刊, 19(1):263–272, 2017。'
- en: '[295] Gen Li and Joongkyu Kim. Dabnet: Depth-wise asymmetric bottleneck for
    real-time semantic segmentation. In BMVC, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[295] 根·李和钟奎·金。Dabnet：用于实时语义分割的深度方向不对称瓶颈。发表于 BMVC, 2019。'
- en: '[296] Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, and Jing-Jhih Lin. Efficient
    dense modules of asymmetric convolution for real-time semantic segmentation. In
    ACM MM Asia, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[296] 邵元·罗、薛名·杭、盛伟·陈和靖智·林。用于实时语义分割的高效密集模块的非对称卷积。发表于 ACM MM Asia, 2019。'
- en: '[297] Ivan Krešo, Josip Krapac, and Siniša Šegvić. Efficient ladder-style densenets
    for semantic segmentation of large images. IEEE Transactions on Intelligent Transportation
    Systems, 22(8):4951–4961, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[297] Ivan Krešo、Josip Krapac 和 Siniša Šegvić. 高效的阶梯式 DenseNets 用于大图像的语义分割。IEEE
    智能交通系统汇刊，22(8):4951–4961，2020 年。'
- en: '[298] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and
    Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image
    segmentation. In ECCV, pages 801–818, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[298] Liang-Chieh Chen、Yukun Zhu、George Papandreou、Florian Schroff 和 Hartwig
    Adam. 使用膨胀可分离卷积的编码器-解码器用于语义图像分割。发表于 ECCV，页码 801–818，2018 年。'
- en: '[299] Rudra PK Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-scnn: Fast
    semantic segmentation network. In BMVC, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[299] Rudra PK Poudel、Stephan Liwicki 和 Roberto Cipolla. Fast-scnn：快速语义分割网络。发表于
    BMVC，2019 年。'
- en: '[300] Juntang Zhuang, Junlin Yang, Lin Gu, and Nicha Dvornek. Shelfnet for
    fast semantic segmentation. In ICCVW, pages 0–0, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[300] Juntang Zhuang、Junlin Yang、Lin Gu 和 Nicha Dvornek. Shelfnet 用于快速语义分割。发表于
    ICCVW，页码 0–0，2019 年。'
- en: '[301] Marin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense
    of pre-trained imagenet architectures for real-time semantic segmentation of road-driving
    images. In CVPR, pages 12607–12616, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[301] Marin Orsic、Ivan Kreso、Petra Bevandic 和 Sinisa Segvic. 为实时道路驾驶图像的语义分割辩护预训练的
    ImageNet 架构。发表于 CVPR，页码 12607–12616，2019 年。'
- en: '[302] Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach.
    Contextnet: Exploring context and detail for semantic segmentation in real-time.
    In BMVC, 2018.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[302] Rudra PK Poudel、Ujwal Bonde、Stephan Liwicki 和 Christopher Zach. Contextnet：探索实时语义分割中的上下文和细节。发表于
    BMVC，2018 年。'
- en: '[303] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong
    Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic
    segmentation. IJCV, 129:3051–3068, 2021.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[303] Changqian Yu、Changxin Gao、Jingbo Wang、Gang Yu、Chunhua Shen 和 Nong Sang.
    Bisenet v2：具有引导聚合的双边网络用于实时语义分割。IJCV，129:3051–3068，2021 年。'
- en: '[304] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng
    Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation.
    In CVPR, pages 9716–9725, 2021.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[304] Mingyuan Fan、Shenqi Lai、Junshi Huang、Xiaoming Wei、Zhenhua Chai、Junfeng
    Luo 和 Xiaolin Wei. 重新思考 bisenet 以实现实时语义分割。发表于 CVPR，页码 9716–9725，2021 年。'
- en: '[305] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya
    Jia. Icnet for real-time semantic segmentation on high-resolution images. In ECCV,
    pages 405–420, 2018.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[305] Hengshuang Zhao、Xiaojuan Qi、Xiaoyong Shen、Jianping Shi 和 Jiaya Jia. Icnet：用于高分辨率图像的实时语义分割。发表于
    ECCV，页码 405–420，2018 年。'
- en: '[306] Yu Wang, Quan Zhou, Jia Liu, Jian Xiong, Guangwei Gao, Xiaofu Wu, and
    Longin Jan Latecki. Lednet: A lightweight encoder-decoder network for real-time
    semantic segmentation. In ICIP, pages 1860–1864\. IEEE, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[306] Yu Wang、Quan Zhou、Jia Liu、Jian Xiong、Guangwei Gao、Xiaofu Wu 和 Longin
    Jan Latecki. Lednet：一种轻量级的编码器-解码器网络用于实时语义分割。发表于 ICIP，页码 1860–1864。IEEE，2019 年。'
- en: '[307] Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun. Dfanet: Deep feature
    aggregation for real-time semantic segmentation. In CVPR, pages 9522–9531, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[307] Hanchao Li、Pengfei Xiong、Haoqiang Fan 和 Jian Sun. Dfanet：用于实时语义分割的深度特征聚合。发表于
    CVPR，页码 9522–9531，2019 年。'
- en: '[308] Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin,
    Kate Saenko, and Stan Sclaroff. Real-time semantic segmentation with fast attention.
    IEEE Robotics and Automation Letters, 6(1):263–270, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[308] Ping Hu、Federico Perazzi、Fabian Caba Heilbron、Oliver Wang、Zhe Lin、Kate
    Saenko 和 Stan Sclaroff. 使用快速注意力的实时语义分割。IEEE 机器人与自动化快报，6(1):263–270，2020 年。'
- en: '[309] Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas:
    Fast neural architecture search for faster semantic segmentation. In ICCVW, pages
    0–0, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[309] Albert Shaw、Daniel Hunter、Forrest Landola 和 Sammy Sidhu. Squeezenas：快速神经架构搜索以实现更快的语义分割。发表于
    ICCVW，页码 0–0，2019 年。'
- en: '[310] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua,
    Alan L Yuille, and Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture
    search for semantic image segmentation. In CVPR, pages 82–92, 2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[310] Chenxi Liu、Liang-Chieh Chen、Florian Schroff、Hartwig Adam、Wei Hua、Alan
    L Yuille 和 Li Fei-Fei. Auto-deeplab：用于语义图像分割的层次神经架构搜索。发表于 CVPR，页码 82–92，2019 年。'
- en: '[311] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang
    Wang. Fasterseg: Searching for faster real-time semantic segmentation. In ICLR,
    2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311] Wuyang Chen、Xinyu Gong、Xianming Liu、Qian Zhang、Yuan Li 和 Zhangyang Wang.
    Fasterseg：寻求更快的实时语义分割。发表于 ICLR，2020 年。'
- en: '[312] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification
    is not all you need for semantic segmentation. NeurIPS, 34:17864–17875, 2021.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[312] Bowen Cheng、Alex Schwing 和 Alexander Kirillov. 每像素分类并非语义分割所需的全部。NeurIPS，34:17864–17875，2021
    年。'
- en: '[313] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao
    Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic
    segmentation from a sequence-to-sequence perspective with transformers. In CVPR,
    pages 6881–6890, 2021.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[313] Sixiao Zheng、Jiachen Lu、Hengshuang Zhao、Xiatian Zhu、Zekun Luo、Yabiao
    Wang、Yanwei Fu、Jianfeng Feng、Tao Xiang、Philip HS Torr 等人. 从序列到序列的视角重新思考语义分割，使用变换器。见
    CVPR，页码 6881–6890，2021 年。'
- en: '[314] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez,
    and Ping Luo. Segformer: Simple and efficient design for semantic segmentation
    with transformers. NeurIPS, 34:12077–12090, 2021.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[314] Enze Xie、Wenhai Wang、Zhiding Yu、Anima Anandkumar、Jose M Alvarez 和 Ping
    Luo. Segformer：具有变换器的语义分割的简单高效设计。NeurIPS，34:12077–12090，2021 年。'
- en: '[315] Yifan Zhang, Bo Pang, and Cewu Lu. Semantic segmentation by early region
    proxy. In CVPR, pages 1258–1268, 2022.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[315] Yifan Zhang、Bo Pang 和 Cewu Lu. 通过早期区域代理进行语义分割。见 CVPR，页码 1258–1268，2022
    年。'
- en: '[316] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and
    Rohit Girdhar. Masked-attention mask transformer for universal image segmentation.
    In CVPR, pages 1290–1299, 2022.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[316] Bowen Cheng、Ishan Misra、Alexander G Schwing、Alexander Kirillov 和 Rohit
    Girdhar. 用于通用图像分割的遮罩注意力掩模变换器。见 CVPR，页码 1290–1299，2022 年。'
- en: '[317] Huiyan Zhang, Hao Sun, Wengang Ao, and Georgi Dimirovski. A survey on
    instance segmentation: Recent advances and challenges. Int. J. Innov. Comput.
    Inf. Control, 17:1041–1053, 2021.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[317] Huiyan Zhang、Hao Sun、Wengang Ao 和 Georgi Dimirovski. 实例分割的调查：近期进展与挑战。Int.
    J. Innov. Comput. Inf. Control，17:1041–1053，2021 年。'
- en: '[318] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In ICCV, pages 2961–2969, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[318] Kaiming He、Georgia Gkioxari、Piotr Dollár 和 Ross Girshick. Mask R-CNN。见
    ICCV，页码 2961–2969，2017 年。'
- en: '[319] Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff,
    Peng Wang, and Hartwig Adam. Masklab: Instance segmentation by refining object
    detection with semantic and direction features. In CVPR, pages 4013–4022, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[319] Liang-Chieh Chen、Alexander Hermans、George Papandreou、Florian Schroff、Peng
    Wang 和 Hartwig Adam. Masklab：通过细化目标检测与语义和方向特征的实例分割。见 CVPR，页码 4013–4022，2018 年。'
- en: '[320] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang
    Wang. Mask scoring r-cnn. In CVPR, pages 6409–6418, 2019.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[320] Zhaojin Huang、Lichao Huang、Yongchao Gong、Chang Huang 和 Xinggang Wang.
    掩模评分 R-CNN。见 CVPR，页码 6409–6418，2019 年。'
- en: '[321] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation
    network for instance segmentation. In CVPR, pages 8759–8768, 2018.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[321] Shu Liu、Lu Qi、Haifang Qin、Jianping Shi 和 Jiaya Jia. 用于实例分割的路径聚合网络。见 CVPR，页码
    8759–8768，2018 年。'
- en: '[322] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang
    Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade
    for instance segmentation. In CVPR, pages 4974–4983, 2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[322] Kai Chen、Jiangmiao Pang、Jiaqi Wang、Yu Xiong、Xiaoxiao Li、Shuyang Sun、Wansen
    Feng、Ziwei Liu、Jianping Shi、Wanli Ouyang 等人。用于实例分割的混合任务级联。见 CVPR，页码 4974–4983，2019
    年。'
- en: '[323] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time
    instance segmentation. In ICCV, pages 9157–9166, 2019.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[323] Daniel Bolya、Chong Zhou、Fanyi Xiao 和 Yong Jae Lee. Yolact：实时实例分割。见 ICCV，页码
    9157–9166，2019 年。'
- en: '[324] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang
    Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In CVPR, pages
    8573–8581, 2020.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[324] Hao Chen、Kunyang Sun、Zhi Tian、Chunhua Shen、Yongming Huang 和 Youliang
    Yan. BlendMask：自上而下遇见自下而上的实例分割。见 CVPR，页码 8573–8581，2020 年。'
- en: '[325] Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Dollár. Tensormask:
    A foundation for dense object segmentation. In ICCV, pages 2061–2069, 2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[325] Xinlei Chen、Ross Girshick、Kaiming He 和 Piotr Dollář. TensorMask：稠密物体分割的基础。见
    ICCV，页码 2061–2069，2019 年。'
- en: '[326] Ayten Ozge Akmandor, YIN Hongxu, and Niraj K Jha. Smart, secure, yet
    energy-efficient, internet-of-things sensors. IEEE Transactions on Multi-Scale
    Computing Systems, 4(4):914–930, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[326] Ayten Ozge Akmandor、YIN Hongxu 和 Niraj K Jha. 智能、安全且高效的物联网传感器。IEEE 多尺度计算系统汇刊，4(4):914–930，2018
    年。'
- en: '[327] Rahul Mishra, Hari Prabhat Gupta, and Tanima Dutta. A survey on deep
    neural network compression: Challenges, overview, and solutions. arXiv preprint
    arXiv:2010.03954, 2020.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[327] Rahul Mishra、Hari Prabhat Gupta 和 Tanima Dutta. 深度神经网络压缩的调查：挑战、概述和解决方案。arXiv
    预印本 arXiv:2010.03954，2020 年。'
- en: '[328] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression
    and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[328] Yu Cheng、Duo Wang、Pan Zhou 和 Tao Zhang. 深度神经网络模型压缩与加速的调查。arXiv 预印本 arXiv:1710.09282，2017
    年。'
- en: '[329] Sheng Xu, Anran Huang, Lei Chen, and Baochang Zhang. Convolutional neural
    network pruning: A survey. In 2020 39th Chinese Control Conference (CCC), pages
    7458–7463\. IEEE, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[329] Sheng Xu, Anran Huang, Lei Chen, 和 Baochang Zhang. 卷积神经网络剪枝: 综述. 在 2020年第39届中国控制会议
    (CCC), 页码 7458–7463. IEEE, 2020.'
- en: '[330] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang.
    Pruning and quantization for deep neural network acceleration: A survey. Neurocomputing,
    461:370–403, 2021.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[330] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, 和 Xiaotong Zhang.
    深度神经网络加速的剪枝和量化: 综述. Neurocomputing, 461:370–403, 2021.'
- en: '[331] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    In Low-Power Computer Vision, pages 291–326\. Chapman and Hall/CRC.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[331] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, 和
    Kurt Keutzer. 高效神经网络推理的量化方法综述. 在《低功耗计算机视觉》中, 页码 291–326. Chapman and Hall/CRC.'
- en: '[332] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
    Exploiting linear structure within convolutional networks for efficient evaluation.
    NeurIPS, 27, 2014.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[332] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, 和 Rob Fergus.
    利用卷积网络中的线性结构进行高效评估. NeurIPS, 27, 2014.'
- en: '[333] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional
    neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[333] Max Jaderberg, Andrea Vedaldi, 和 Andrew Zisserman. 通过低秩扩展加速卷积神经网络. arXiv
    预印本 arXiv:1405.3866, 2014.'
- en: '[334] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
    Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[334] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,
    Carlo Gatta, 和 Yoshua Bengio. Fitnets: 为薄深网提供提示. 在 ICLR, 2015.'
- en: '[335] Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng
    Liu, Zhaoning Zhang, and Yu Liu. Correlation congruence for knowledge distillation.
    In ICCV, pages 5007–5016, 2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[335] Baoyun Peng, Xiao Jin, Dongsheng Li, Shunfeng Zhou, Yichao Wu, Jiaheng
    Liu, Zhaoning Zhang, 和 Yu Liu. 知识蒸馏的相关性一致性. 在 ICCV, 页码 5007–5016, 2019.'
- en: '[336] Jacob Søgaard Larsen and Line Clemmensen. Weight sharing and deep learning
    for spectral data. In ICASSP 2020-2020 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP), pages 4227–4231\. IEEE, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[336] Jacob Søgaard Larsen 和 Line Clemmensen. 权重共享和深度学习用于光谱数据. 在 ICASSP 2020-2020
    IEEE 国际声学、语音和信号处理会议 (ICASSP), 页码 4227–4231. IEEE, 2020.'
- en: '[337] Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, and Minje
    Park. Pvanet: Deep but lightweight neural networks for real-time object detection.
    arXiv preprint arXiv:1608.08021, 2016.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[337] Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, 和 Minje
    Park. Pvanet: 深度但轻量的实时目标检测神经网络. arXiv 预印本 arXiv:1608.08021, 2016.'
- en: '[338] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating
    very deep neural networks. In ICCV, pages 1389–1397, 2017.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[338] Yihui He, Xiangyu Zhang, 和 Jian Sun. 加速超深神经网络的通道剪枝. 在 ICCV, 页码 1389–1397,
    2017.'
- en: '[339] Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and
    Qi Tian. Variational convolutional neural network pruning. In CVPR, pages 2780–2789,
    2019.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[339] Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, 和
    Qi Tian. 变分卷积神经网络剪枝. 在 CVPR, 页码 2780–2789, 2019.'
- en: '[340] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning
    method for deep neural network compression. In ICCV, pages 5058–5066, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[340] Jian-Hao Luo, Jianxin Wu, 和 Weiyao Lin. Thinet: 一种用于深度神经网络压缩的滤波器级剪枝方法.
    在 ICCV, 页码 5058–5066, 2017.'
- en: '[341] Loc N Huynh, Youngki Lee, and Rajesh Krishna Balan. Deepmon: Mobile gpu-based
    deep learning framework for continuous vision applications. In Proceedings of
    the 15th Annual International Conference on Mobile Systems, Applications, and
    Services, pages 82–95, 2017.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[341] Loc N Huynh, Youngki Lee, 和 Rajesh Krishna Balan. Deepmon: 基于移动 GPU 的深度学习框架用于连续视觉应用.
    在第15届国际移动系统、应用和服务年会会议论文集中, 页码 82–95, 2017.'
- en: '[342] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz,
    and William J Dally. Eie: Efficient inference engine on compressed deep neural
    network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[342] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz,
    和 William J Dally. Eie: 高效的压缩深度神经网络推理引擎. ACM SIGARCH 计算机体系结构新闻, 44(3):243–254,
    2016.'
- en: '[343] Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression
    for deep learning. NeurIPS, 30, 2017.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[343] Christos Louizos, Karen Ullrich, 和 Max Welling. 深度学习的贝叶斯压缩. NeurIPS,
    30, 2017.'
- en: '[344] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. ACM
    SIGARCH computer architecture news, 45(2):27–40, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[344] 安格舒曼·帕拉沙, 闵洙·吕, 阿努拉格·穆卡拉, 安东尼奥·普利耶利, 兰哈拉詹·文卡特桑, 布鲁斯克·凯拉尼, 乔尔·埃默, 斯蒂芬·W·凯克勒,
    和 威廉·J·达利. SCNN：一种压缩稀疏卷积神经网络的加速器. ACM SIGARCH 计算机架构新闻, 45(2):27–40, 2017.'
- en: '[345] Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise
    neuron sharing. NeurIPS, 31, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[345] 何晓曦, 周自牧, 和 洛塔尔·提勒. 通过层级神经共享的多任务压缩. NeurIPS, 31, 2018.'
- en: '[346] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
    Deep learning with limited numerical precision. In ICML, pages 1737–1746\. PMLR,
    2015.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[346] 苏约格·古普塔, 安库尔·阿格拉瓦尔, 凯拉什·戈帕拉克里希南, 和 普里蒂什·纳拉扬. 使用有限数值精度的深度学习. 在 ICML 上,
    页码 1737–1746\. PMLR, 2015.'
- en: '[347] Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented
    approximation of convolutional neural networks. arXiv preprint arXiv:1604.03168,
    2016.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[347] 菲利普·吉塞尔, 穆罕默德·莫塔梅迪, 和 索赫尔·贾西亚. 硬件导向的卷积神经网络近似. arXiv 预印本 arXiv:1604.03168,
    2016.'
- en: '[348] Manu Mathew, Kumar Desappan, Pramod Kumar Swami, and Soyeb Nagori. Sparse,
    quantized, full frame cnn for low power embedded devices. In CVPR Workshops, pages
    11–19, 2017.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[348] 马努·马修, 库马尔·德萨潘, 普拉莫德·库马尔·斯瓦米, 和 索耶布·纳戈里. 稀疏、量化的全帧 CNN 用于低功耗嵌入设备. 在 CVPR
    工作坊上, 页码 11–19, 2017.'
- en: '[349] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Binarized neural networks: Training deep neural networks with weights
    and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[349] 马修·库尔巴利亚, 伊泰·胡巴拉, 丹尼尔·苏德里, 兰·艾尔-亚尼夫, 和 约书亚·本吉奥. 二值化神经网络：训练深度神经网络，权重和激活被限制为+1
    或 -1. arXiv 预印本 arXiv:1602.02830, 2016.'
- en: '[350] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep
    convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115,
    2014.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[350] 龚云超, 刘柳, 杨铭, 和 卢博米尔·布尔德夫. 使用向量量化压缩深度卷积网络. arXiv 预印本 arXiv:1412.6115,
    2014.'
- en: '[351] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized
    convolutional neural networks for mobile devices. In CVPR, pages 4820–4828, 2016.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[351] 吴佳祥, 冯聪, 王宇航, 胡青浩, 和 程健. 移动设备上的量化卷积神经网络. 在 CVPR 上, 页码 4820–4828, 2016.'
- en: '[352] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin
    Chen. Compressing neural networks with the hashing trick. In ICML, pages 2285–2294\.
    PMLR, 2015.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[352] 陈文林, 詹姆斯·威尔逊, 斯蒂芬·泰瑞, 基利安·温伯格, 和 颜心. 使用哈希技巧压缩神经网络. 在 ICML 上, 页码 2285–2294\.
    PMLR, 2015.'
- en: '[353] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. arXiv
    preprint arXiv:1510.00149, 2015.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[353] 韩松, 毛慧子, 和 威廉·J·达利. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络. arXiv 预印本 arXiv:1510.00149,
    2015.'
- en: '[354] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker.
    Learning efficient object detection models with knowledge distillation. NeurIPS,
    30, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[354] 陈国斌, 崔旺君, 余翔, 韩涛, 和 曼莫汉·钱德拉克. 通过知识蒸馏学习高效的目标检测模型. NeurIPS, 30, 2017.'
- en: '[355] Feng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In CVPR,
    pages 3517–3526, 2019.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[355] 张峰, 朱夏天, 和 叶茂. 快速人体姿态估计. 在 CVPR 上, 页码 3517–3526, 2019.'
- en: '[356] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention:
    Improving the performance of convolutional neural networks via attention transfer.
    In ICLR, 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[356] 谢尔盖·扎戈鲁伊科 和 尼科斯·科莫达基斯. 更多关注注意力：通过注意力转移提高卷积神经网络的性能. 在 ICLR 上, 2017.'
- en: '[357] Jonghwan Mun, Kimin Lee, Jinwoo Shin, and Bohyung Han. Learning to specialize
    with knowledge distillation for visual question answering. NeurIPS, 31, 2018.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[357] 文钟焕, 李基敏, 辛镇宇, 和 韩博雄. 通过知识蒸馏学习在视觉问答中的专业化. NeurIPS, 31, 2018.'
- en: '[358] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphrasing complex network:
    Network compression via factor transfer. NeurIPS, 31, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[358] 金长浩, 朴成旭, 和 郭诺俊. 复杂网络的同义改写：通过因子转移进行网络压缩. NeurIPS, 31, 2018.'
- en: '[359] Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie
    Yan, and Xiaolin Hu. Knowledge distillation via route constrained optimization.
    In ICCV, pages 1345–1354, 2019.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[359] 魏晓金, 彭宝云, 吴义超, 刘瑜, 刘家恒, 丁亮, 燕俊杰, 和 胡晓林. 通过路径约束优化进行知识蒸馏. 在 ICCV 上, 页码
    1345–1354, 2019.'
- en: '[360] Kunran Xu, Lai Rui, Yishi Li, and Lin Gu. Feature normalized knowledge
    distillation for image classification. In ECCV, pages 664–680\. Springer, 2020.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[360] 许昆然, 来瑞, 李义时, 和 顾林. 用于图像分类的特征归一化知识蒸馏. 在 ECCV 上, 页码 664–680\. Springer,
    2020.'
- en: '[361] Xiaobo Wang, Tianyu Fu, Shengcai Liao, Shuo Wang, Zhen Lei, and Tao Mei.
    Exclusivity-consistency regularized knowledge distillation for face recognition.
    In ECCV, pages 325–342\. Springer, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[361] Xiaobo Wang、Tianyu Fu、Shengcai Liao、Shuo Wang、Zhen Lei 和 Tao Mei. 面部识别的排他性一致性正则化知识蒸馏。发表于ECCV，325–342页。Springer，2020年。'
- en: '[362] Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, and Gao
    Huang. Towards learning spatially discriminative feature representations. In ICCV,
    pages 1326–1335, 2021.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[362] Chaofei Wang、Jiayu Xiao、Yizeng Han、Qisen Yang、Shiji Song 和 Gao Huang.
    迈向学习空间区分特征表示。发表于ICCV，1326–1335页，2021年。'
- en: '[363] Chaofei Wang, Shaowei Zhang, Shiji Song, and Gao Huang. Learn from the
    past: Experience ensemble knowledge distillation. In ICPR, 2022.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[363] Chaofei Wang、Shaowei Zhang、Shiji Song 和 Gao Huang. 从过去学习：经验集成知识蒸馏。发表于ICPR，2022年。'
- en: '[364] Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge
    distillation: Fast optimization, network minimization and transfer learning. In
    CVPR, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[364] Junho Yim、Donggyu Joo、Ji-Hoon Bae 和 Junmo Kim. 知识蒸馏的馈赠：快速优化、网络最小化和迁移学习。发表于CVPR，2017年。'
- en: '[365] Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-supervised knowledge
    distillation using singular value decomposition. In ECCV, pages 335–350, 2018.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[365] Seung Hyun Lee、Dae Ha Kim 和 Byung Cheol Song. 使用奇异值分解的自监督知识蒸馏。发表于ECCV，335–350页，2018年。'
- en: '[366] Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Heterogeneous
    knowledge distillation using information flow modeling. In CVPR, pages 2339–2348,
    2020.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[366] Nikolaos Passalis、Maria Tzelepi 和 Anastasios Tefas. 使用信息流建模的异质知识蒸馏。发表于CVPR，2339–2348页，2020年。'
- en: '[367] Yufan Liu, Jiajiong Cao, Bing Li, Chunfeng Yuan, Weiming Hu, Yangxi Li,
    and Yunqiang Duan. Knowledge distillation via instance relationship graph. In
    CVPR, pages 7096–7104, 2019.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[367] Yufan Liu、Jiajiong Cao、Bing Li、Chunfeng Yuan、Weiming Hu、Yangxi Li 和 Yunqiang
    Duan. 通过实例关系图的知识蒸馏。发表于CVPR，7096–7104页，2019年。'
- en: '[368] Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation.
    In ICCV, pages 1365–1374, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[368] Frederick Tung 和 Greg Mori. 相似性保持的知识蒸馏。发表于ICCV，1365–1374页，2019年。'
- en: '[369] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge
    distillation. In CVPR, pages 3967–3976, 2019.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[369] Wonpyo Park、Dongju Kim、Yan Lu 和 Minsu Cho. 关系知识蒸馏。发表于CVPR，3967–3976页，2019年。'
- en: '[370] Ying Zhang, Tao Xiang, Timothy M. Hospedales, and Huchuan Lu. Deep mutual
    learning. In CVPR, 2018.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[370] Ying Zhang、Tao Xiang、Timothy M. Hospedales 和 Huchuan Lu. 深度互学习。发表于CVPR，2018年。'
- en: '[371] Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation by on-the-fly
    native ensemble. NeurIPS, 31, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[371] Xiatian Zhu、Shaogang Gong 等。通过实时本地集成的知识蒸馏。发表于NeurIPS，第31届，2018年。'
- en: '[372] Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin
    Hu, and Ping Luo. Online knowledge distillation via collaborative learning. In
    CVPR, pages 11020–11029, 2020.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[372] Qiushan Guo、Xinjiang Wang、Yichao Wu、Zhipeng Yu、Ding Liang、Xiaolin Hu
    和 Ping Luo. 通过协作学习的在线知识蒸馏。发表于CVPR，11020–11029页，2020年。'
- en: '[373] Devesh Walawalkar, Zhiqiang Shen, and Marios Savvides. Online ensemble
    model compression using knowledge distillation. In ECCV, pages 18–35\. Springer,
    2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[373] Devesh Walawalkar、Zhiqiang Shen 和 Marios Savvides. 利用知识蒸馏的在线集成模型压缩。发表于ECCV，18–35页。Springer，2020年。'
- en: '[374] Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. Online
    knowledge distillation with diverse peers. In AAAI, 2020.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[374] Defang Chen、Jian-Ping Mei、Can Wang、Yan Feng 和 Chun Chen. 具有多样化同行的在线知识蒸馏。发表于AAAI，2020年。'
- en: '[375] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and
    Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural
    networks via self distillation. In ICCV, pages 3713–3722, 2019.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[375] Linfeng Zhang、Jiebo Song、Anni Gao、Jingwei Chen、Chenglong Bao 和 Kaisheng
    Ma. 成为自己的老师：通过自蒸馏提高卷积神经网络的性能。发表于ICCV，3713–3722页，2019年。'
- en: '[376] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change Loy. Learning lightweight
    lane detection cnns by self attention distillation. In ICCV, pages 1013–1021,
    2019.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[376] Yuenan Hou、Zheng Ma、Chunxiao Liu 和 Chen Change Loy. 通过自注意力蒸馏学习轻量级车道检测CNN。发表于ICCV，1013–1021页，2019年。'
- en: '[377] Chenglin Yang, Lingxi Xie, Chi Su, and Alan L Yuille. Snapshot distillation:
    Teacher-student optimization in one generation. In CVPR, pages 2859–2868, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[377] Chenglin Yang、Lingxi Xie、Chi Su 和 Alan L Yuille. 快照蒸馏：一代中的师生优化。发表于CVPR，2859–2868页，2019年。'
- en: '[378] Zhimao Peng, Zechao Li, Junge Zhang, Yan Li, Guo-Jun Qi, and Jinhui Tang.
    Few-shot image recognition with knowledge transfer. In ICCV, pages 441–449, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[378] Zhimao Peng、Zechao Li、Junge Zhang、Yan Li、Guo-Jun Qi 和 Jinhui Tang. 通过知识转移进行少样本图像识别。发表于ICCV，441–449页，2019年。'
- en: '[379] Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, and Gao Huang. Efficient
    knowledge distillation from model checkpoints. In NeurIPS, 2022.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[379] Chaofei Wang、Qisen Yang、Rui Huang、Shiji Song 和 Gao Huang。从模型检查点中高效地进行知识蒸馏。在NeurIPS会议，2022年。'
- en: '[380] Chaofei Wang, Ke Yang, Shaowei Zhang, Gao Huang, and Shiji Song. Tc3kd:
    Knowledge distillation via teacher-student cooperative curriculum customization.
    Neurocomputing, 508:284–292, 2022.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[380] Chaofei Wang、Ke Yang、Shaowei Zhang、Gao Huang 和 Shiji Song。Tc3kd：通过教师-学生合作课程定制的知识蒸馏。《神经计算》，508：284–292，2022年。'
- en: '[381] Yushu Feng, Huan Wang, Haoji Roland Hu, Lu Yu, Wei Wang, and Shiyan Wang.
    Triplet distillation for deep face recognition. In ICIP, pages 808–812\. IEEE,
    2020.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[381] Yushu Feng、Huan Wang、Haoji Roland Hu、Lu Yu、Wei Wang 和 Shiyan Wang。用于深度人脸识别的三重蒸馏。在ICIP会议，页面808–812。IEEE，2020年。'
- en: '[382] Nuno C Garcia, Pietro Morerio, and Vittorio Murino. Modality distillation
    with multiple stream networks for action recognition. In ECCV, pages 103–118,
    2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[382] Nuno C Garcia、Pietro Morerio 和 Vittorio Murino。用于动作识别的多流网络模态蒸馏。在ECCV会议，页面103–118，2018年。'
- en: '[383] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Sukthankar.
    D3d: Distilled 3d networks for video action recognition. In WACV, pages 625–634,
    2020.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[383] Jonathan Stroud、David Ross、Chen Sun、Jia Deng 和 Rahul Sukthankar。D3d：用于视频动作识别的蒸馏3d网络。在WACV会议，页面625–634，2020年。'
- en: '[384] Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao
    Mei. Relation distillation networks for video object detection. In ICCV, pages
    7023–7032, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[384] Jiajun Deng、Yingwei Pan、Ting Yao、Wengang Zhou、Houqiang Li 和 Tao Mei。用于视频对象检测的关系蒸馏网络。在ICCV会议，页面7023–7032，2019年。'
- en: '[385] Xing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, and
    Erjin Zhou. General instance distillation for object detection. In CVPR, pages
    7842–7851, 2021.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[385] Xing Dai、Zeren Jiang、Zhao Wu、Yiping Bao、Zhicheng Wang、Si Liu 和 Erjin
    Zhou。用于目标检测的通用实例蒸馏。在CVPR会议，页面7842–7851，2021年。'
- en: '[386] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong
    Wang. Structured knowledge distillation for semantic segmentation. In CVPR, pages
    2604–2613, 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[386] Yifan Liu、Ke Chen、Chris Liu、Zengchang Qin、Zhenbo Luo 和 Jingdong Wang。用于语义分割的结构化知识蒸馏。在CVPR会议，页面2604–2613，2019年。'
- en: '[387] Jianbo Jiao, Yunchao Wei, Zequn Jie, Honghui Shi, Rynson WH Lau, and
    Thomas S Huang. Geometry-aware distillation for indoor semantic segmentation.
    In CVPR, pages 2869–2878, 2019.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[387] Jianbo Jiao、Yunchao Wei、Zequn Jie、Honghui Shi、Rynson WH Lau 和 Thomas
    S Huang。用于室内语义分割的几何感知蒸馏。在CVPR会议，页面2869–2878，2019年。'
- en: '[388] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class
    feature variation distillation for semantic segmentation. In ECCV, pages 346–362\.
    Springer, 2020.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[388] Yukang Wang、Wei Zhou、Tao Jiang、Xiang Bai 和 Yongchao Xu。用于语义分割的类内特征变化蒸馏。在ECCV会议，页面346–362。Springer，2020年。'
- en: '[389] Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, and Chen Change Loy.
    Inter-region affinity distillation for road marking segmentation. In CVPR, pages
    12486–12495, 2020.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[389] Yuenan Hou、Zheng Ma、Chunxiao Liu、Tak-Wai Hui 和 Chen Change Loy。用于道路标记分割的区域间亲和力蒸馏。在CVPR会议，页面12486–12495，2020年。'
- en: '[390] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning
    monocular depth by distilling cross-domain stereo networks. In ECCV, pages 484–500,
    2018.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[390] Xiaoyang Guo、Hongsheng Li、Shuai Yi、Jimmy Ren 和 Xiaogang Wang。通过蒸馏跨领域立体网络来学习单目深度。在ECCV会议，页面484–500，2018年。'
- en: '[391] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks
    guided prediction-and-distillation network for simultaneous depth estimation and
    scene parsing. In CVPR, pages 675–684, 2018.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[391] Dan Xu、Wanli Ouyang、Xiaogang Wang 和 Nicu Sebe。Pad-net：用于同时深度估计和场景解析的多任务引导预测与蒸馏网络。在CVPR会议，页面675–684，2018年。'
- en: '[392] Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. Learning
    monocular depth estimation infusing traditional stereo knowledge. In CVPR, pages
    9799–9809, 2019.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[392] Fabio Tosi、Filippo Aleotti、Matteo Poggi 和 Stefano Mattoccia。通过注入传统立体知识来学习单目深度估计。在CVPR会议，页面9799–9809，2019年。'
- en: '[393] Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa Ricci. Refine
    and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised
    monocular depth estimation. In CVPR, pages 9768–9777, 2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[393] Andrea Pilzer、Stephane Lathuiliere、Nicu Sebe 和 Elisa Ricci。精炼与蒸馏：利用周期不一致性和知识蒸馏进行无监督单目深度估计。在CVPR会议，页面9768–9777，2019年。'
- en: '[394] Mihai Pirvu, Victor Robu, Vlad Licaret, Dragos Costea, Alina Marcu, Emil
    Slusanschi, Rahul Sukthankar, and Marius Leordeanu. Depth distillation: unsupervised
    metric depth estimation for uavs by finding consensus between kinematics, optical
    flow and deep learning. In CVPR, pages 3215–3223, 2021.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[394] Mihai Pirvu、Victor Robu、Vlad Licaret、Dragos Costea、Alina Marcu、Emil Slusanschi、Rahul
    Sukthankar 和 Marius Leordeanu。深度蒸馏：通过找到运动学、光流和深度学习之间的一致性来进行无人机的无监督度量深度估计。在CVPR会议，页面3215–3223，2021年。'
- en: '[395] Yiran Wang, Xingyi Li, Min Shi, Ke Xian, and Zhiguo Cao. Knowledge distillation
    for fast and accurate monocular depth estimation on mobile devices. In CVPR, pages
    2457–2465, 2021.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[395] 王怡然、李兴义、施敏、谢柯和曹志国。针对移动设备的快速准确单目深度估计的知识蒸馏。发表于 CVPR，第2457–2465页，2021年。'
- en: '[396] Wei Chen, Yu Liu, Nan Pu, Weiping Wang, Li Liu, and Michael S Lew. Feature
    estimations based correlation distillation for incremental image retrieval. TMM,
    24:1844–1856, 2021.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[396] 陈伟、刘宇、蒲楠、王伟平、刘磊和Michael S Lew。基于特征估计的相关性蒸馏用于增量图像检索。TMM，第24卷，第1844–1856页，2021年。'
- en: '[397] Young Kyun Jang, Geonmo Gu, Byungsoo Ko, Isaac Kang, and Nam Ik Cho.
    Deep hash distillation for image retrieval. In ECCV, pages 354–371\. Springer,
    2022.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[397] 姜永钧、具根模、许炳洙、康艾萨克和赵南熙。用于图像检索的深度哈希蒸馏。发表于 ECCV，第354–371页。Springer，2022年。'
- en: '[398] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu,
    and Zheng-Jun Zha. Object relational graph with teacher-recommended learning for
    video captioning. In CVPR, pages 13278–13288, 2020.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[398] 张子奇、石雅雅、袁春风、李冰、王佩锦、胡伟铭和查正军。带有教师推荐学习的视频字幕的对象关系图。发表于 CVPR，第13278–13288页，2020年。'
- en: '[399] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan
    Adeli, and Juan Carlos Niebles. Spatio-temporal graph for video captioning with
    knowledge distillation. In CVPR, pages 10870–10879, 2020.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[399] 潘博晓、蔡浩业、黄德安、李宽辉、Adrien Gaidon、Ehsan Adeli 和 Juan Carlos Niebles。用于视频字幕的时空图与知识蒸馏。发表于
    CVPR，第10870–10879页，2020年。'
- en: '[400] Jingjing Dong, Zhenzhen Hu, and Yuanen Zhou. Revisiting knowledge distillation
    for image captioning. In CAAI International Conference on Artificial Intelligence,
    pages 613–625\. Springer, 2021.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[400] 董晶晶、胡珍珍和周元恩。重新审视图像字幕的知识蒸馏。发表于 CAAI 国际人工智能会议，第613–625页。Springer，2021年。'
- en: '[401] Chenrui Zhang and Yuxin Peng. Better and faster: Knowledge transfer from
    multiple self-supervised learning tasks via graph distillation for video classification.
    In IJCAI, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[401] 陈瑞张和彭钰欣。更好更快：通过图蒸馏从多个自监督学习任务中迁移知识以进行视频分类。发表于 IJCAI，2018年。'
- en: '[402] Shweta Bhardwaj, Mukundhan Srinivasan, and Mitesh M Khapra. Efficient
    video classification using fewer frames. In CVPR, pages 354–363, 2019.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[402] Shweta Bhardwaj、Mukundhan Srinivasan 和 Mitesh M Khapra。使用更少的帧进行高效视频分类。发表于
    CVPR，第354–363页，2019年。'
- en: '[403] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisit
    knowledge distillation: a teacher-free framework. 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[403] 李媛、Francis EH Tay、李桂林、王涛和冯佳仕。重新审视知识蒸馏：一个无教师框架。2019年。'
- en: '[404] Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi,
    and Sagar Jain. Understanding and improving knowledge distillation. arXiv preprint
    arXiv:2002.03532, 2020.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[404] 唐佳熙、Rakesh Shivanna、赵哲、林东、Anima Singh、Ed H Chi 和 Sagar Jain。理解和改进知识蒸馏。arXiv
    预印本 arXiv:2002.03532，2020年。'
- en: '[405] Xu Cheng, Zhefan Rao, Yilan Chen, and Quanshi Zhang. Explaining knowledge
    distillation by quantifying the knowledge. In CVPR, pages 12925–12935, 2020.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[405] 徐成、饶哲凡、陈怡兰 和 张全世。通过量化知识来解释知识蒸馏。发表于 CVPR，第12925–12935页，2020年。'
- en: '[406] V Lebedev, Y Ganin, M Rakhuba, I Oseledets, and V Lempitsky. Speeding-up
    convolutional neural networks using fine-tuned cp-decomposition. In ICLR, 2015.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[406] V Lebedev、Y Ganin、M Rakhuba、I Oseledets和V Lempitsky。使用微调的 cp-分解加速卷积神经网络。发表于
    ICLR，2015年。'
- en: '[407] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, and E Weinan. Convolutional
    neural networks with low-rank regularization. In ICLR, 2016.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[407] 程泰、肖彤、张毅、王晓刚 和 E Weinan。具有低秩正则化的卷积神经网络。发表于 ICLR，2016年。'
- en: '[408] Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando
    De Freitas. Predicting parameters in deep learning. NeurIPS, 26, 2013.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[408] Misha Denil、Babak Shakibi、Laurent Dinh、Marc’Aurelio Ranzato 和 Nando De
    Freitas。深度学习中的参数预测。NeurIPS，第26卷，2013年。'
- en: '[409] Qingchen Zhang, Laurence T Yang, Xingang Liu, Zhikui Chen, and Peng Li.
    A tucker deep computation model for mobile multimedia feature learning. ACM Transactions
    on Multimedia Computing, Communications, and Applications (TOMM), 13(3s):1–18,
    2017.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[409] 张青晨、Laurence T Yang、刘兴刚、陈志奎和李鹏。用于移动多媒体特征学习的 Tucker 深度计算模型。ACM 多媒体计算、通讯与应用
    (TOMM)，第13卷第3期：第1–18页，2017年。'
- en: '[410] Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and
    Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications
    in person attribute classification. In CVPR, pages 5334–5343, 2017.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[410] 陆永熙、Abhishek Kumar、Shuangfei Zhai、程瑜、Tara Javidi 和 Rogerio Feris。在多任务网络中实现完全自适应特征共享，并应用于人属性分类。发表于
    CVPR，第5334–5343页，2017年。'
- en: '[411] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
    Ramabhadran. Low-rank matrix factorization for deep neural network training with
    high-dimensional output targets. In 2013 IEEE international conference on acoustics,
    speech and signal processing, pages 6655–6659\. IEEE, 2013.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[411] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, 和 Bhuvana
    Ramabhadran。高维输出目标的深度神经网络训练的低秩矩阵分解。在2013年IEEE国际声学、语音和信号处理会议，页码6655–6659。IEEE，2013。'
- en: '[412] Frederick Tung and Greg Mori. Clip-q: Deep network compression learning
    by in-parallel pruning-quantization. In CVPR, pages 7873–7882, 2018.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[412] Frederick Tung 和 Greg Mori。Clip-q：通过并行剪枝量化进行深度网络压缩学习。在CVPR，页码7873–7882，2018。'
- en: '[413] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via
    distillation and quantization. In ICLR, 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[413] Antonio Polino, Razvan Pascanu, 和 Dan Alistarh。通过蒸馏和量化进行模型压缩。在ICLR，2018。'
- en: '[414] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture
    search. In ICLR, 2019.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[414] Hanxiao Liu, Karen Simonyan, 和 Yiming Yang。DARTS：可微分架构搜索。在ICLR，2019。'
- en: '[415] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining
    Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, et al. Fbnetv2: Differentiable neural
    architecture search for spatial and channel dimensions. In CVPR, 2020.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[415] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining
    Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, 等人。Fbnetv2：用于空间和通道维度的可微分神经网络架构搜索。在CVPR，2020。'
- en: '[416] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all:
    Train one network and specialize it for efficient deployment. In ICLR, 2020.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[416] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, 和 Song Han。一次性完成：训练一个网络并使其专门化以实现高效部署。在ICLR，2020。'
- en: '[417] Krishna Teja Chitty-Venkata and Arun K Somani. Neural architecture search
    survey: A hardware perspective. ACM Computing Surveys, 2022.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[417] Krishna Teja Chitty-Venkata 和 Arun K Somani。神经网络架构搜索综述：硬件视角。ACM计算调查，2022。'
- en: '[418] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet:
    Tiny deep learning on iot devices. NeurIPS, 2020.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[418] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, 等人。Mcunet：在物联网设备上的微型深度学习。NeurIPS，2020。'
- en: '[419] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
    and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for
    ubiquitous machine-learning. ACM SIGARCH Computer Architecture News, 2014.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[419] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen,
    和 Olivier Temam。Diannao：一种小型高吞吐量加速器，用于普及的机器学习。ACM SIGARCH计算机架构新闻，2014。'
- en: '[420] Samuel Williams, Andrew Waterman, and David Patterson. Roofline: an insightful
    visual performance model for multicore architectures. Communications of the ACM,
    2009.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[420] Samuel Williams, Andrew Waterman, 和 David Patterson。Roofline：一种洞察多核架构的视觉性能模型。ACM通讯，2009。'
- en: '[421] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie Enright
    Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-neuron-free deep neural network
    computing. ACM SIGARCH Computer Architecture News, 2016.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[421] Jorge Albericio, Patrick Judd, Tayler Hetherington, Tor Aamodt, Natalie
    Enright Jerger, 和 Andreas Moshovos。Cnvlutin：无效神经元自由的深度神经网络计算。ACM SIGARCH计算机架构新闻，2016。'
- en: '[422] Liqiang Lu, Jiaming Xie, Ruirui Huang, Jiansong Zhang, Wei Lin, and Yun
    Liang. An efficient hardware accelerator for sparse convolutional neural networks
    on fpgas. In FCCM, 2019.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[422] Liqiang Lu, Jiaming Xie, Ruirui Huang, Jiansong Zhang, Wei Lin, 和 Yun
    Liang。针对FPGA的稀疏卷积神经网络的高效硬件加速器。在FCCM，2019。'
- en: '[423] Xinyi Zhang, Weiwen Jiang, Yiyu Shi, and Jingtong Hu. When neural architecture
    search meets hardware implementation: from hardware awareness to co-design. In
    ISVLSI, 2019.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[423] Xinyi Zhang, Weiwen Jiang, Yiyu Shi, 和 Jingtong Hu。当神经网络架构搜索遇上硬件实现：从硬件意识到共同设计。在ISVLSI，2019。'
- en: '[424] Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, and Yiyu Shi.
    Standing on the shoulders of giants: Hardware and neural architecture co-search
    with hot start. IEEE Transactions on Computer-Aided Design of Integrated Circuits
    and Systems, 2020.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[424] Weiwen Jiang, Lei Yang, Sakyasingha Dasgupta, Jingtong Hu, 和 Yiyu Shi。站在巨人的肩膀上：带热启动的硬件和神经网络架构共同搜索。IEEE计算机辅助设计集成电路与系统汇刊，2020。'
- en: '[425] Mohamed S Abdelfattah, Łukasz Dudziak, Thomas Chau, Royson Lee, Hyeji
    Kim, and Nicholas D Lane. Best of both worlds: Automl codesign of a cnn and its
    hardware accelerator. In DAC, 2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[425] Mohamed S Abdelfattah, Łukasz Dudziak, Thomas Chau, Royson Lee, Hyeji
    Kim, 和 Nicholas D Lane。两全其美：自动机器学习CNN及其硬件加速器的共同设计。在DAC，2020。'
- en: '[426] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song Han. Mcunetv2: Memory-efficient
    patch-based inference for tiny deep learning. In NeurIPS, 2021.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[426] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, 和 Song Han。Mcunetv2：用于微型深度学习的内存高效补丁推理。在NeurIPS，2021。'
- en: '[427] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song
    Han. On-device training under 256kb memory. In NeurIPS, 2022.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[427] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan 和 Song Han.
    在256KB内存下的设备端训练。发表于NeurIPS，2022年。'
- en: '[428] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, and G Edward Suh.
    Boosting the performance of cnn accelerators with dynamic fine-grained channel
    gating. In MICRO, 2019.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[428] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang 和 G Edward Suh.
    利用动态细粒度通道门控提升 CNN 加速器性能。发表于MICRO，2019年。'
- en: '[429] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng
    Jing, and Xiaoyao Liang. Drq: dynamic region-based quantization for deep neural
    network acceleration. In ISCA, 2020.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[429] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng
    Jing 和 Xiaoyao Liang. Drq: 基于动态区域的量化用于深度神经网络加速。发表于ISCA，2020年。'
- en: '[430] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars, and Marian
    Verhelst. Processor architecture optimization for spatially dynamic neural networks.
    In VLSI-SoC, 2021.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[430] Steven Colleman, Thomas Verelst, Linyan Mei, Tinne Tuytelaars 和 Marian
    Verhelst. 面向空间动态神经网络的处理器架构优化。发表于VLSI-SoC，2021年。'
- en: '[431] Zhe Zhou, Junlin Liu, Zhenyu Gu, and Guangyu Sun. Energon: Toward efficient
    acceleration of transformers using dynamic sparse attention. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, 2022.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[431] Zhe Zhou, Junlin Liu, Zhenyu Gu 和 Guangyu Sun. Energon: 朝着使用动态稀疏注意力的变换器高效加速迈进。IEEE计算机辅助设计集成电路与系统汇刊，2022年。'
- en: '[432] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, and Yun Fu.
    Image as set of points. In ICLR, 2023.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[432] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu 和 Yun Fu.
    图像作为点集。发表于ICLR，2023年。'
- en: '[433] Yuki Tatsunami and Masato Taki. Sequencer: Deep LSTM for image classification.
    In NeurIPS, 2022.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[433] Yuki Tatsunami 和 Masato Taki. Sequencer: 深度LSTM用于图像分类。发表于NeurIPS，2022年。'
- en: '[434] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and Enhua Wu. Vision GNN:
    An image is worth graph of nodes. In NeurIPS, 2022.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[434] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang 和 Enhua Wu. Vision GNN:
    一张图像即一图节点。发表于NeurIPS，2022年。'
- en: '[435] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang
    Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al.
    Image as a foreign language: Beit pretraining for all vision and vision-language
    tasks. arXiv preprint arXiv:2208.10442, 2022.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[435] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang
    Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som 等。图像作为外语：Beit
    预训练用于所有视觉和视觉语言任务。arXiv 预印本 arXiv:2208.10442，2022年。'
- en: '[436] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng
    Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new
    foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[436] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng
    Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li 等。Florence: 计算机视觉的新基础模型。arXiv
    预印本 arXiv:2111.11432，2021年。'
- en: '[437] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable
    feature pyramid architecture for object detection. In CVPR, pages 7036–7045, 2019.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[437] Golnaz Ghiasi, Tsung-Yi Lin 和 Quoc V Le. Nas-fpn: 学习可扩展的特征金字塔结构用于目标检测。发表于CVPR，页码7036–7045，2019年。'
- en: '[438] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and
    Geoffrey Hinton. A unified sequence interface for vision tasks. In NeurIPS, 2022.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[438] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet 和 Geoffrey
    Hinton. 视觉任务的统一序列接口。发表于NeurIPS，2022年。'
- en: '[439] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée
    Masquelier, and Anthony Maida. Deep learning in spiking neural networks. Neural
    networks, 111:47–63, 2019.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[439] Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, Timothée
    Masquelier 和 Anthony Maida. 在脉冲神经网络中的深度学习。神经网络，111:47–63，2019年。'
- en: '[440] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum
    contrast for unsupervised visual representation learning. In CVPR, pages 9729–9738,
    2020.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[440] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie 和 Ross Girshick. 用于无监督视觉表示学习的动量对比。发表于CVPR，页码9729–9738，2020年。'
- en: '[441] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
    Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000–16009,
    2022.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[441] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár 和 Ross
    Girshick. 掩码自编码器是可扩展的视觉学习者。发表于CVPR，页码16000–16009，2022年。'
- en: '[442] Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, and Gao Huang. Revisiting
    locally supervised learning: an alternative to end-to-end training. In ICLR, 2021.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[442] Yulin Wang, Zanlin Ni, Shiji Song, Le Yang 和 Gao Huang. 重访局部监督学习：一种替代端到端训练的方法。发表于ICLR，2021年。'
- en: '[443] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang,
    and Yi Yang. Automated progressive learning for efficient training of vision transformers.
    In CVPR, 2022.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[443] Changlin Li, Bohan Zhuang, Guangrun Wang, Xiaodan Liang, Xiaojun Chang
    和 Yi Yang. 自动化渐进学习用于高效训练视觉变换器。发表于CVPR，2022年。'
- en: '[444] Zanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang, Yue Cao, and Gao Huang.
    Deep incubation: Training large models by divide-and-conquering. In ICCV, 2022.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[444] 倪赞林、王宇霖、余江伟、姜浩君、曹跃和黄高。**Deep incubation**：通过分而治之训练大型模型。在 ICCV，2022。'
- en: '[445] Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, and
    Gao Huang. Efficienttrain: Exploring generalized curriculum learning for training
    visual backbones. In ICCV, 2023.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[445] 王宇霖、岳扬、陆睿、刘天娇、钟赵、宋世基和黄高。**Efficienttrain**：探索用于训练视觉骨干的广义课程学习。在 ICCV，2023。'
