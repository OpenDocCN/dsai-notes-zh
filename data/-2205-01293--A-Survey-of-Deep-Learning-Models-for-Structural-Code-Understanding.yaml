- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.01293] A Survey of Deep Learning Models for Structural Code Understanding'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.01293] 深度学习模型在结构化代码理解中的应用调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01293](https://ar5iv.labs.arxiv.org/html/2205.01293)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01293](https://ar5iv.labs.arxiv.org/html/2205.01293)
- en: A Survey of Deep Learning Models for Structural Code Understanding
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型在结构化代码理解中的应用调查
- en: Ruoting Wu [wurt8@mail2.sysu.edu.cn](mailto:wurt8@mail2.sysu.edu.cn) Sun Yat-sen
    University of China ,  Yuxin Zhang Sun Yat-sen University of China [zhangyx355@mail2.sysu.edu.cn](mailto:zhangyx355@mail2.sysu.edu.cn)
    ,  Qibiao Peng Sun Yat-sen University of China [pengqb3@mail2.sysu.edu.cn](mailto:pengqb3@mail2.sysu.edu.cn)
    ,  Liang Chen [chenliang6@mail.sysu.edu.cn](mailto:chenliang6@mail.sysu.edu.cn)
    Sun Yat-sen University of China  and  Zibin Zheng [zhzibin@mail.sysu.edu.cn](mailto:zhzibin@mail.sysu.edu.cn)
    Sun Yat-sen University of China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 吴若婷 [wurt8@mail2.sysu.edu.cn](mailto:wurt8@mail2.sysu.edu.cn) 中山大学，张玉欣 中山大学
    [zhangyx355@mail2.sysu.edu.cn](mailto:zhangyx355@mail2.sysu.edu.cn)，彭启彪 中山大学 [pengqb3@mail2.sysu.edu.cn](mailto:pengqb3@mail2.sysu.edu.cn)，陈亮
    [chenliang6@mail.sysu.edu.cn](mailto:chenliang6@mail.sysu.edu.cn) 中山大学以及郑子彬 [zhzibin@mail.sysu.edu.cn](mailto:zhzibin@mail.sysu.edu.cn)
    中山大学
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'In recent years, the rise of deep learning and automation requirements in the
    software industry has elevated Intelligent Software Engineering to new heights.
    The number of approaches and applications in code understanding is growing, with
    deep learning techniques being used in many of them to better capture the information
    in code data. In this survey, we present a comprehensive overview of the structures
    formed from code data. We categorize the models for understanding code in recent
    years into two groups: sequence-based and graph-based models, further make a summary
    and comparison of them¹¹1We provide a paper collection about deep learning models
    and datasets for structural code understanding in https://github.com/codingClaire/Structural-Code-Understanding.
    We also introduce metrics, datasets and the downstream tasks. Finally, we make
    some suggestions for future research in structural code understanding field.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习的兴起和软件行业自动化需求的增加将智能软件工程提升到了新的高度。代码理解中的方法和应用数量不断增加，许多应用中使用了深度学习技术，以更好地捕捉代码数据中的信息。在这项调查中，我们提供了从代码数据中形成的结构的全面概述。我们将近年来的代码理解模型分为两类：基于序列的模型和基于图的模型，并进一步总结和比较它们¹¹1我们在
    [https://github.com/codingClaire/Structural-Code-Understanding](https://github.com/codingClaire/Structural-Code-Understanding)
    提供了一些关于深度学习模型和数据集的论文集。我们还介绍了度量标准、数据集和下游任务。最后，我们对结构化代码理解领域的未来研究提出了一些建议。
- en: 'Code Representation,Intelligent Software Engineering, Graph Neural Networks,
    Deep Learning, Code Generation^†^†booktitle:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 代码表示，智能软件工程，图神经网络，深度学习，代码生成^†^†书名：
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: In the last several decades, deep learning has made remarkable achievements
    in various areas and permeated every aspect of human lives, especially in the
    domain of multi-media data processing such as image recognition, speech recognition,
    and natural language processing. With the booming development of deep learning
    techniques, as well as cooperatively increasing open-source code communities and
    automation requirements in the software industry, deep learning techniques began
    to be applied to more specific tasks in software engineering in recent years.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年里，深度学习在各个领域取得了显著成就，并渗透到了人类生活的各个方面，尤其是在图像识别、语音识别和自然语言处理等多媒体数据处理领域。随着深度学习技术的蓬勃发展，以及开源代码社区和软件行业自动化需求的共同增长，近年来深度学习技术开始被应用于软件工程中的更具体任务。
- en: 'Conventionally, source codes are considered as plain text sequences that may
    be understood using various existing approaches, such as deep learning approaches
    in neural language processing(NLP). However, when applied directly to source code,
    NLP approaches have the drawback of ignoring the code’s structural information.
    When the code is learned merely as a sequence of plain text, syntactic and semantic
    information that is crucial to understanding the code, as well as the many relationships
    between program entities, may be overlooked. Hence, a surge of works of understanding
    source code with structural information is proposed in recent years, which lead
    by the research of deep learning on sequences and graphs, such as Transformer(Bahdanau
    et al., [2014](#bib.bib20)), Graph Neural Networks(Wu et al., [2019](#bib.bib174)).
    These techniques and their variants are developed to cope with various tasks in
    source code understanding including code representation and other downstream tasks.
    Although these methods have achieved some improvements, structural code understanding
    is still facing many challenges, which are identified and summarized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，源代码被视为可以通过各种现有方法（如神经语言处理中的深度学习方法）理解的纯文本序列。然而，当直接应用于源代码时，NLP 方法的一个缺点是忽略了代码的结构信息。当代码仅作为纯文本序列进行学习时，理解代码所需的语法和语义信息以及程序实体之间的许多关系可能会被忽视。因此，近年来提出了大量利用结构信息理解源代码的工作，这些工作受到序列和图的深度学习研究的推动，如
    Transformer（Bahdanau 等，[2014](#bib.bib20)），图神经网络（Wu 等，[2019](#bib.bib174)）。这些技术及其变体旨在应对源代码理解中的各种任务，包括代码表示和其他下游任务。尽管这些方法取得了一些进展，但结构化代码理解仍面临许多挑战，这些挑战已被识别和总结如下：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code Structural Modeling: Since conventional language models feed the sequence
    of source code tokens as inputs, the structural information in the code is usually
    neglected. Therefore as result, a number of challenges arise about how to use
    structural information in code successfully, such as how to model structural information
    in code effectively and how to select effective structural information for specific
    downstream tasks.'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码结构建模：由于传统语言模型将源代码令牌的序列作为输入，代码中的结构信息通常被忽视。因此，如何成功利用代码中的结构信息成为了一系列挑战，例如如何有效地建模代码中的结构信息以及如何为特定的下游任务选择有效的结构信息。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code Generic Representation Learning: Much of the current research focuses
    on learning code representations for specific programming languages, making learning
    generic code representations a challenge. It’s about how to learn language-independent
    code representations that get beyond programming languages’ constraints.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码通用表示学习：目前的研究大多集中在为特定编程语言学习代码表示上，使得学习通用代码表示成为一个挑战。这涉及到如何学习超越编程语言限制的语言无关的代码表示。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code Task-specific Adaptation: The following adaptations remain a challenge:
    how to choose and design specific architectures for downstream applications such
    as code generation and program repair, how to process datasets for task specifications,
    and how to adapt models in few-shots learning, transfer learning, and cross-linguistic
    scenarios.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码任务特定适配：以下适配仍然是一个挑战：如何为下游应用（如代码生成和程序修复）选择和设计特定的架构，如何处理任务规范的数据集，以及如何在少量样本学习、迁移学习和跨语言场景中适配模型。
- en: 'In this survey, we present a comprehensive overview of structural learning
    techniques for code representation learning. In summary, our contributions can
    be listed as below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们对代码表示学习的结构学习技术进行了全面概述。总之，我们的贡献可以列举如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce the structures in code data as well as the generating procedure,
    then give a summary of the downstream tasks for structural code understanding.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了代码数据中的结构以及生成过程，然后总结了结构化代码理解的下游任务。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a new taxonomy of deep learning models for structural code understanding
    based on the structures, which are categorized into sequence-based models and
    graph-based models.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于结构的深度学习模型的新分类法，将其分为基于序列的模型和基于图的模型。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We outline the challenges, the open problems, and future directions for structural
    code understanding.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们概述了结构化代码理解的挑战、未解问题和未来方向。
- en: The survey is organized as follows. In Section [2](#S2 "2\. Preliminary ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"), we give some
    basic introduction of the structures in code and how they are extracted from code
    data. In Section [3](#S3 "3\. Sequence-based Models ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and Section [4](#S4 "4\. Graph-based
    Models ‣ A Survey of Deep Learning Models for Structural Code Understanding"),
    we separately introduce the models based on which structure they generally used,
    indeed, the sequence-based models and the graph-based models. We first give an
    overview of how they change the structures and then categorized them by the core
    models. In Section [5](#S5 "5\. Discussion and Comparison ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"), we offer a discussion and comparison
    between the sequence-based models and graph-based models. The downstream tasks
    after conducting code representation is introduced in Section [6](#S6 "6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). We then
    summarize the related metrics and datasets in Section [7](#S7 "7\. Metrics and
    Datasets ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    We try to discuss some open research questions in Section [8](#S8 "8\. Open Problems
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). Finally,
    we draw our conclusion in Section [9](#S9 "9\. Conclusion ‣ A Survey of Deep Learning
    Models for Structural Code Understanding").
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 调查内容安排如下。在第[2](#S2 "2\. 初步 ‣ 结构化代码理解的深度学习模型调查")节中，我们介绍了代码中的结构及其如何从代码数据中提取。在第[3](#S3
    "3\. 基于序列的模型 ‣ 结构化代码理解的深度学习模型调查")节和第[4](#S4 "4\. 基于图的模型 ‣ 结构化代码理解的深度学习模型调查")节中，我们分别介绍了基于序列模型和基于图模型的模型。我们首先概述了这些模型如何改变结构，然后按核心模型对它们进行分类。在第[5](#S5
    "5\. 讨论与比较 ‣ 结构化代码理解的深度学习模型调查")节中，我们对基于序列的模型和基于图的模型进行了讨论和比较。代码表示之后的下游任务在第[6](#S6
    "6\. 任务 ‣ 结构化代码理解的深度学习模型调查")节中介绍。接着，我们在第[7](#S7 "7\. 指标与数据集 ‣ 结构化代码理解的深度学习模型调查")节中总结了相关的指标和数据集。在第[8](#S8
    "8\. 开放问题 ‣ 结构化代码理解的深度学习模型调查")节中，我们尝试讨论一些开放的研究问题。最后，我们在第[9](#S9 "9\. 结论 ‣ 结构化代码理解的深度学习模型调查")节中得出结论。
- en: 2\. Preliminary
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步
- en: 2.1\. Structures in Code
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 代码中的结构
- en: 2.1.1\. Overview
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 概述
- en: First, we’ll go through the basic structures that programs can produce. Table
    [1](#S2.T1 "Table 1 ‣ 2.1.1\. Overview ‣ 2.1\. Structures in Code ‣ 2\. Preliminary
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") summarizes
    the most commonly used notations. The following is a piece of code snippet in
    Python we used to illustrate the structures in code data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍程序可以生成的基本结构。表[1](#S2.T1 "表1 ‣ 2.1.1\. 概述 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 结构化代码理解的深度学习模型调查")总结了最常用的符号。以下是我们用来说明代码数据中的结构的一个Python代码片段。
- en: '{python}'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '{python}'
- en: 'def add(a,b): x=0 if(a¿b): x=a-b; else: x=a+b; return x'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 'def add(a,b): x=0 if(a¿b): x=a-b; else: x=a+b; return x'
- en: res=add(1,2) print(res)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: res=add(1,2) print(res)
- en: Table 1\. Summary of notations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 符号总结。
- en: '| Symbol | Description | Symbol | Description | Symbol | Description |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 | 符号 | 描述 | 符号 | 描述 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $P$ &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $P$ &#124;'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Program &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 程序 &#124;'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $S$ &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $S$ &#124;'
- en: '|'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code Snippet &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码片段 &#124;'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $F$ &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $F$ &#124;'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; function &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 函数 &#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $A$ &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $A$ &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Abstract Syntax Tree of a code snippet &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码片段的抽象语法树 &#124;'
- en: '&#124; or a program &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或一个程序 &#124;'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $G_{c}$ &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $G_{c}$ &#124;'
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Control flow graph of a code snippet &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码片段的控制流图 &#124;'
- en: '&#124; or a program &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或一个程序 &#124;'
- en: '|'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $G_{d}$ &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $G_{d}$ &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Data flow graph of a code snippet &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码片段的数据流图 &#124;'
- en: '&#124; or a program &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或一个程序 &#124;'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $path$ &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $path$ &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sequence of nodes extracted &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提取的节点序列 &#124;'
- en: '&#124; from the AST &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从抽象语法树中 &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $n$ &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $n$ &#124;'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The length (token number) of a code snippet &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码片段的长度（令牌数） &#124;'
- en: '&#124; or a program &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或一个程序 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $y$ &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $y$ &#124;'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Labels of the code &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码标签 &#124;'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $D$ &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $D$ &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code description in natural language &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然语言中的代码描述 &#124;'
- en: '&#124; from the AST &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从AST中 &#124;'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $t$ &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $t$ &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; single token &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单一标记 &#124;'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $H$ &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $H$ &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; the intermediate representation &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中间表示 &#124;'
- en: '&#124; of the code &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码中的符号 &#124;'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The general techniques for generating structures from the source code snippet
    are shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1.1\. Overview ‣ 2.1\. Structures in
    Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"). The Lexical Analyzer first converts the code into a token-based
    sequence. Each token in the sequence has two attributes, type and value. Lexical
    Analysis of code is similar to the tokenization stage in natural language. Inspired
    by Hindle et al. (Hindle et al., [2012](#bib.bib65)), we refer to these unprocessed
    code sequences as Natural Code Sequence (NCS) in this article for the consistence
    of discussion, which may be called as token sequence or code sequence in other
    articles.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从源代码片段生成结构的一般技术如图[1](#S2.F1 "图1 ‣ 2.1.1. 概述 ‣ 2.1. 代码中的结构 ‣ 2. 初步 ‣ 深度学习模型在结构化代码理解中的调查")所示。词法分析器首先将代码转换为基于标记的序列。序列中的每个标记具有两个属性，类型和值。代码的词法分析类似于自然语言中的标记化阶段。受到Hindle等人（Hindle
    et al., [2012](#bib.bib65)）的启发，本文中我们将这些未经处理的代码序列称为自然代码序列（NCS），以保持讨论的一致性，在其他文章中可能称为标记序列或代码序列。
- en: The Syntax Analyzer, also known as a parser, takes the tokens and produces an
    Abstract Syntax Tree(AST) based on the code snippet’s grammar. The Abstract Syntax
    Tree is then utilized in the Semantic Analyzer to verify for semantic consistency,
    and the Intermediate Code Generator to construct the Intermediate Representation,
    which varies depending on the programming language. Control flow and Data flow
    are both graph-like Intermediate Representations known as Control Flow Graph(CFG)
    and Data Flow Graph(DFG), respectively.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 语法分析器，也称为解析器，接收标记并基于代码片段的语法生成抽象语法树（AST）。抽象语法树随后被用于语义分析器以验证语义一致性，并由中间代码生成器构建中间表示，这取决于编程语言。控制流和数据流都是类似图形的中间表示，分别称为控制流图（CFG）和数据流图（DFG）。
- en: Other structures established in software engineering, such as UML Graph and
    Program Dependency Graph, are in addition to the basic structures.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中建立的其他结构，如UML图和程序依赖图，除了基本结构之外。
- en: '![Refer to caption](img/aa52b4cd8439eb0757a7766f8682d3ac.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa52b4cd8439eb0757a7766f8682d3ac.png)'
- en: Figure 1\. The basic structures generated from code includes (a) Nature Code
    Sequence, (b) Abstract Syntax Tree, (c) Control Flow Graph and (d) Data Flow Graph.
    The front end of a compiler constitutes the four components (Lexical Analyzer,
    Semantic Analyzer, Syntax Analyzer, and Intermediate Code Generator)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 从代码生成的基本结构包括（a）自然代码序列，（b）抽象语法树，（c）控制流图和（d）数据流图。编译器的前端由四个组件（词法分析器、语义分析器、语法分析器和中间代码生成器）构成。
- en: 2.1.2\. Nature Code Sequence
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2. 自然代码序列
- en: Given a Program $P$ or a Code snippet $S$, the Nature Code Sequence(NCS) $S=\{t_{1},t_{2},...,t_{n}\}$
    is obtained by Lexical Analyzer, $t_{i}$ refers to the token in the code. Using
    NCS to represent the code is the simplest and most common approach. The position
    of the token in the sequence corresponds to the order in which it appears in the
    code snippet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 给定程序$P$或代码片段$S$，自然代码序列（NCS）$S=\{t_{1},t_{2},...,t_{n}\}$由词法分析器获得，$t_{i}$指代代码中的标记。使用NCS表示代码是最简单且最常见的方法。序列中标记的位置对应于其在代码片段中出现的顺序。
- en: 2.1.3\. Abstract Syntax Tree
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3. 抽象语法树
- en: Given the language grammar, the Abstract Syntax Tree (AST) of code is generated
    by Syntax Analyzer and marked as $A$, which hierarchically reflects the structural
    and syntax information of code. The root node of the syntax tree represents the
    start symbol. The interior nodes are the nonterminals in the grammar, while the
    leaf nodes are the terminals, which are usually the variables and identifiers
    defined by the programmers from the NCS. Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.3\. Abstract
    Syntax Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") is the AST of the above code snippet
    in Python.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据语言语法，代码的抽象语法树（AST）由语法分析器生成并标记为$A$，该树层次性地反映了代码的结构和语法信息。语法树的根节点表示起始符号。内部节点是语法中的非终结符，而叶节点是终结符，通常是由程序员从NCS定义的变量和标识符。图[2](#S2.F2
    "图 2 ‣ 2.1.3\. 抽象语法树 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 深度学习模型在结构化代码理解中的综述")是上述Python代码片段的AST。
- en: '![Refer to caption](img/608be885928af50420bfc8a0eb772b58.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/608be885928af50420bfc8a0eb772b58.png)'
- en: Figure 2\. An example of Abstract Syntax Tree
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 抽象语法树的示例
- en: 2.1.4\. Flow Graph
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 流图
- en: Flow Graphs are the graphs that cover the semantic information of source code.
    The two typical flows in flow graphs are control flow and data flow, which separately
    represent how the program executes and how the data flows. Fig.[3](#S2.F3 "Figure
    3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") shows the example
    of control flow and data flow graphs for the python code snippet mentioned.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 流图是覆盖源代码语义信息的图。流图中的两个典型流是控制流和数据流，分别表示程序的执行方式和数据的流动方式。图[3](#S2.F3 "图 3 ‣ 2.1.4\.
    流图 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 深度学习模型在结构化代码理解中的综述")展示了提到的Python代码片段的控制流和数据流图的示例。
- en: '![Refer to caption](img/68188532b7f5270b542f0d2811530daa.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68188532b7f5270b542f0d2811530daa.png)'
- en: Figure 3\. An example of Flow Graphs. (a) Control Flow Graph, (b) Data Flow
    Graph of expression $x=a-b$, (c)Data Flow Graph of expression $x1=a1+b1$.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 流图示例。(a) 控制流图，(b) 表达式$x=a-b$的数据流图，(c) 表达式$x1=a1+b1$的数据流图。
- en: Control Flow Graph
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制流图
- en: The Control Flow Graph (CFG) of a program marked as $G_{c}$, represents different
    execution paths of a program. Each node of CFG is a basic block that represents
    a behavior without branches. The edges of the graph represent how these basic
    blocks flow from one to another. In the example code snippet, the judgement of
    $a>b$ will cause the program into 2 branches, one is $x=a-b$, the other is $x=a+b$.
    As the Fig.[3](#S2.F3 "Figure 3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures in Code
    ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    (a) , the CFG has two edges from the decision node(if-statement) to the two downward
    nodes(two basic blocks).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的控制流图（CFG）标记为$G_{c}$，表示程序的不同执行路径。CFG的每个节点是一个基本块，表示没有分支的行为。图的边表示这些基本块如何相互流动。在示例代码片段中，$a>b$的判断将使程序分为两个分支，一个是$x=a-b$，另一个是$x=a+b$。如图[3](#S2.F3
    "图 3 ‣ 2.1.4\. 流图 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 深度学习模型在结构化代码理解中的综述") (a)所示，CFG从决策节点（if语句）到两个向下节点（两个基本块）有两条边。
- en: Data Flow Graph
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据流图
- en: 'The Data Flow Graph (DFG) of a program marked as $G_{d}$ represents the dependency
    relation between variables. DFG can represent code snippets without conditionals.
    In the example code snippet, we select two statements: $x=a-b$ and $x=a+b$ to
    draw their DFGs. To eliminate the repeated assignment to $x$, we rename variables
    in the second assignment, convert them to $x1=a1+b1$. Therefore, the two DFGs
    are shown in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures
    in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code
    Understanding") (b) and (c). Each data flow node represents the operation of variables
    and each edge represents how the value flows.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的数据流图（DFG）标记为$G_{d}$，表示变量之间的依赖关系。DFG可以表示没有条件的代码片段。在示例代码片段中，我们选择两个语句：$x=a-b$和$x=a+b$来绘制它们的DFG。为了消除对$x$的重复赋值，我们在第二个赋值中重命名变量，将其转换为$x1=a1+b1$。因此，两个DFG如图[3](#S2.F3
    "图 3 ‣ 2.1.4\. 流图 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 深度学习模型在结构化代码理解中的综述") (b)和(c)所示。每个数据流节点表示变量的操作，每条边表示值如何流动。
- en: Control/Data Flow Graph
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制/数据流图
- en: Because the DFG can only represent basic blocks without branches, it can be
    used to replace the basic blocks of a CFG, resulting in a Control/Data Flow Graph
    (CDFG). There are two types of nodes in a program’s CDFG, the decision node and
    the data-flow node.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DFG 只能表示没有分支的基本块，因此可以用它来替代 CFG 的基本块，从而得到控制/数据流图（CDFG）。程序的 CDFG 中有两种类型的节点，决策节点和数据流节点。
- en: 2.1.5\. Other Structures
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. 其他结构
- en: In addition to the above structures, there are some other code structures that
    are less common in code understanding.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述结构，还有一些在代码理解中不常见的其他代码结构。
- en: Other Intermediate Representation
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他中间表示
- en: Intermediate Representation(IR) is a data structure that can be obtained from
    a compiler such as LLVM Compiler Infrastructure(Lattner and Adve, [2004](#bib.bib86)).
    The frontend Compiler compiles the source code and generates an IR for the backend
    Compiler to optimize and translate. In LLVM infrastructure, the IR is in Static
    Single Assignment(SSA) form. The Broad definition of IR includes the flow graphs
    as well as other graph structures. Program Dependency Graph(PDG)(Ferrante et al.,
    [1987a](#bib.bib54)) is one of the intermediate representations that make explicit
    both data and control dependencies for each operation in a program. PDG are useful
    to perform optimizations through a single walk.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 中间表示（IR）是一种可以从编译器（如 LLVM 编译器基础设施（Lattner 和 Adve，[2004](#bib.bib86)））中获得的数据结构。前端编译器编译源代码并生成
    IR，供后端编译器优化和翻译。在 LLVM 基础设施中，IR 以静态单赋值（SSA）形式存在。IR 的广义定义包括流图以及其他图结构。程序依赖图（PDG）（Ferrante
    等，[1987a](#bib.bib54)）是中间表示之一，它明确表示程序中每个操作的数据和控制依赖关系。PDG 对通过单次遍历进行优化非常有用。
- en: The Unified Modeling Language
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统一建模语言
- en: The Unified Modeling Language (UML) is a widely-used language for specifying,
    visualizing, and documenting the artifacts of a software-intensive system. UML
    class diagram is a type of static structure diagram that describes the structure
    of a system by showing the system classes and their attributes, operations (or
    methods), and relationships.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 统一建模语言（UML）是一种广泛使用的语言，用于指定、可视化和记录软件密集型系统的文档。UML 类图是一种静态结构图，通过展示系统类及其属性、操作（或方法）和关系，描述系统的结构。
- en: 2.2\. Deep learning models
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 深度学习模型
- en: In this section, we focus on deep learning models commonly used in code understanding
    tasks that have shown to be effective in other domains and are now being used
    by a growing number of researchers in the code domain.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将深入探讨在代码理解任务中常用的深度学习模型，这些模型在其他领域中已被证明有效，现在越来越多的研究人员在代码领域中使用它们。
- en: 2.2.1\. Recurrent Neural Network
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 循环神经网络
- en: Recurrent Neural Networks (RNNs)(Rumelhart et al., [1986](#bib.bib135)) are
    the neural networks where each unit is connected by directed cycles. RNNs can
    use their hidden state to track the long-term information of the sequence data.
    Therefore, RNNs are a common choice for sequence modeling. Vanilla RNNs have gradient
    vanishing and gradient explosion problems, which can reduce the ability of a model
    to learn long-term information. Long Short-Term Memory(LSTM) and Gated Recurrent
    Units(GRU) are the two most used RNN models that can avoid the problem and achieve
    better results.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）（Rumelhart 等，[1986](#bib.bib135)）是每个单元通过有向循环连接的神经网络。RNNs 可以利用其隐藏状态跟踪序列数据的长期信息。因此，RNNs
    是序列建模的常见选择。Vanilla RNNs 存在梯度消失和梯度爆炸问题，这可能降低模型学习长期信息的能力。长短期记忆（LSTM）和门控循环单元（GRU）是两种最常用的
    RNN 模型，它们可以避免这些问题并取得更好的结果。
- en: Long Short-Term Memory(LSTM)(Hochreiter and Schmidhuber, [1997](#bib.bib66))
    has basic model from RNNs. Every unit of LSTM considers the hidden state, the
    current input, and the information from its memory cell. LSTM uses 3 gates, input
    gate, forget gate and output gate to control the learning and transfer of information.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）（Hochreiter 和 Schmidhuber，[1997](#bib.bib66)）具有来自 RNN 的基本模型。LSTM
    的每个单元考虑隐藏状态、当前输入以及来自其内存单元的信息。LSTM 使用 3 个门，输入门、遗忘门和输出门来控制信息的学习和传递。
- en: Gated Recurrent Units(GRU) (Cho et al., [2014](#bib.bib39)) combine the input
    gate and forget gate in LSTM into one gate, called the update gate, and the other
    gate is the reset gate. The reset gate and the update gate can control the degree
    of memory or forgetting of sequence information by the hidden state. Compared
    with LSTM, GRU has comparable performance but lower computational cost.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）（Cho et al., [2014](#bib.bib39)）将 LSTM 中的输入门和遗忘门合并为一个门，称为更新门，另一个门为重置门。重置门和更新门可以通过隐状态控制对序列信息的记忆或遗忘程度。与
    LSTM 相比，GRU 在性能上可比，但计算成本更低。
- en: 2.2.2\. Transformer
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. Transformer
- en: Bahdanau et al.(Bahdanau et al., [2014](#bib.bib20)) propose the Attention mechanism
    to solve the problems of excessive information length and information loss in
    machine translation tasks. It feeds all of the hidden states from the Encoder
    into the Decoder after linear weighting and assigns various attention weights
    to each input token, indicating which inputs are more significant to the output.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau 等人（Bahdanau et al., [2014](#bib.bib20)）提出了注意力机制，以解决机器翻译任务中信息长度过长和信息丢失的问题。它将编码器中的所有隐状态经过线性加权后输入到解码器中，并为每个输入令牌分配不同的注意力权重，指示哪些输入对输出更为重要。
- en: To enhance computational performance and better describe global relationships
    in sequences, Vaswani et al.(Vaswani et al., [2017](#bib.bib154)) propose self-attention.
    It is a special attention mechanism, so that information from any position in
    the sequence can directly affect the encoding of the other token. Based on self-attention
    they propose a new neural network model called Transformer which consists of multiple
    attention blocks made up by self-attention. Transfomer’s encoder uses the self-attention
    mechanism to associate the tokens in the input sequence with all other tokens
    as it learns the representation of the input. Also, the input of the Transfomer’s
    decoder is associated with the output of the encoder through self-attention. Transformer
    and its variants, such as Bert, GPT, etc., are capable of processing complex data
    and can process large amounts of sequence data. Therefore they are often used
    as pretraining models to capture the rich information from large amounts of complex
    data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强计算性能并更好地描述序列中的全局关系，Vaswani 等人（Vaswani et al., [2017](#bib.bib154)）提出了自注意力机制。这是一种特殊的注意力机制，使得序列中任何位置的信息可以直接影响其他令牌的编码。基于自注意力，他们提出了一种新的神经网络模型，称为
    Transformer，该模型由多个自注意力组成的注意力块构成。Transformer 的编码器使用自注意力机制将输入序列中的令牌与所有其他令牌关联，同时学习输入的表示。此外，Transformer
    的解码器的输入通过自注意力与编码器的输出相关联。Transformer 及其变体，如 Bert、GPT 等，能够处理复杂数据并处理大量序列数据，因此它们常被用作预训练模型，以从大量复杂数据中捕捉丰富的信息。
- en: 2.2.3\. Graph Neural Network
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. 图神经网络
- en: 'Graph Neural Networks(GNNs) are deep learning models using message passing
    between nodes to capture structural information and aggregate semantic information
    in graphs. GNN can be categorized into four groups according to the survey by
    Wu et al.(Wu et al., [2019](#bib.bib174)): Recursive GNNs, Convolutional GNNs,
    Graph autoencoders, and spatial-temporal GNNs. GNNs are widely used in node classification,
    edge prediction, and graph classification tasks.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）是使用节点之间的信息传递来捕捉结构信息和图中的语义信息的深度学习模型。根据 Wu 等人（Wu et al., [2019](#bib.bib174)）的调查，GNN
    可以分为四类：递归 GNN、卷积 GNN、图自编码器和时空 GNN。GNN 广泛用于节点分类、边预测和图分类任务。
- en: The following are the typical models used in code representation-related tasks.
    Gated graph Neural Network(GGNN) (Li et al., [2016](#bib.bib101)) uses gated recurrent
    units and unroll the propagation process for a fixed number of timesteps. The
    representations of nodes are the final step output. Graph Convolution Network(GCN)
    (Kipf and Welling, [2017](#bib.bib85)) is one of the convolution GNNs, which stack
    multiple graph convolutional layers to better extract the information from neighbors.
    Graph Attention Network(GAT)(Veličković et al., [2018](#bib.bib155)) uses the
    attention mechanism in the message-passing step to aggregate neighborhoods’ information
    with different weights and update the encoding of the node. The above Graph Neural
    Networks are typical deep learning models for learning the representation of graphs
    or nodes in code data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码表示相关任务中使用的典型模型。门控图神经网络（GGNN）（Li et al., [2016](#bib.bib101)）使用门控递归单元，并在固定的时间步数内展开传播过程。节点的表示是最终步骤的输出。图卷积网络（GCN）（Kipf
    和 Welling, [2017](#bib.bib85)）是卷积 GNN 的一种，它堆叠多个图卷积层以更好地从邻居中提取信息。图注意力网络（GAT）（Veličković
    et al., [2018](#bib.bib155)）在消息传递步骤中使用注意力机制，以不同的权重聚合邻域信息并更新节点的编码。上述图神经网络是用于学习代码数据中图或节点表示的典型深度学习模型。
- en: 2.2.4\. Encoder-Decoder Framework
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4. 编码器-解码器框架
- en: The Encoder-Decoder framework(Cho et al., [2014](#bib.bib39)) is presented as
    a solution to traditional machine translation difficulties. The input language
    is encoded in the encoder section to obtain the intermediate representation Context.
    And then in the decoder part, corresponding outputs are generated one by one based
    on Context and related inputs. Sutskever et al.(Sutskever et al., [2014](#bib.bib145))
    present the seq2seq model based on Encoder-Decoder framework to overcome the problem
    of indefinitely long input-output sequences, which aids sequence output with special
    markers such as ¡Eos¿. Different encoders and decoders can be selected according
    to particular tasks, such as RNN-based models, Transformer-based models, GNN-based
    models, etc. Encoder-decoder model architecture has become the mainstream approach
    to address the code generation issue and other tasks due to the naturalness and
    sequence of code. For example, Rabinovich et al.(Rabinovich et al., [2017](#bib.bib130))
    introduce a syntax network (ASN) that extends the encoder-decoder framework to
    generate AST.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器框架（Cho et al., [2014](#bib.bib39)）被提出作为解决传统机器翻译难题的方案。输入语言在编码器部分被编码以获得中间表示上下文（Context）。然后，在解码器部分，根据上下文和相关输入逐一生成相应的输出。Sutskever
    et al.（Sutskever et al., [2014](#bib.bib145)）提出了基于编码器-解码器框架的 seq2seq 模型，以克服无限长输入-输出序列的问题，这有助于使用特殊标记如¡Eos¿进行序列输出。根据具体任务可以选择不同的编码器和解码器，如基于
    RNN 的模型、基于 Transformer 的模型、基于 GNN 的模型等。由于代码的自然性和序列性，编码器-解码器模型架构已经成为解决代码生成问题和其他任务的主流方法。例如，Rabinovich
    et al.（Rabinovich et al., [2017](#bib.bib130)）引入了一种语法网络（ASN），它扩展了编码器-解码器框架以生成
    AST。
- en: 3\. Sequence-based Models
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 基于序列的模型
- en: Sequential models that perform well in sequence-related tasks, such as the Recurrent
    Neural Network family(Rumelhart et al., [1986](#bib.bib135); Hochreiter and Schmidhuber,
    [1997](#bib.bib66); Cho et al., [2014](#bib.bib39)) and Transformer(Vaswani et al.,
    [2017](#bib.bib154)), can be effectively applied to code-related tasks. They can
    be used to encoder-decoder architecture to fulfill downstream tasks such as code
    summarization and code generation, as well as learn to access code representation
    for downstream activities.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列相关任务中表现良好的序列模型，如递归神经网络系列（Rumelhart et al., [1986](#bib.bib135); Hochreiter
    和 Schmidhuber, [1997](#bib.bib66); Cho et al., [2014](#bib.bib39)）和 Transformer（Vaswani
    et al., [2017](#bib.bib154)），可以有效地应用于与代码相关的任务。它们可以用于编码器-解码器架构，以实现代码摘要和代码生成等下游任务，并且可以学习如何访问代码表示以进行下游活动。
- en: The frequently used term code sequence refers to natural code sequences created
    by the code itself, which is referring to NCS introduced by section [2.1.2](#S2.SS1.SSS2
    "2.1.2\. Nature Code Sequence ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"). Because the
    code is highly structured, there are sequences formed by pre-processing the code
    structure input such as AST introduced in section [2.1.3](#S2.SS1.SSS3 "2.1.3\.
    Abstract Syntax Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of
    Deep Learning Models for Structural Code Understanding"), which refer to the flattened
    sequence. In this section, we introduce the models for processing serialized code
    data mentioned above. We first show the structural transformation of code including
    the different methods of pre-processing the structures of code to get the flattened
    sequence, and then show the models for processing NCS and flattened sequences
    respectively.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 经常使用的术语代码序列指的是由代码本身生成的自然代码序列，指的是第 [2.1.2](#S2.SS1.SSS2 "2.1.2\. Nature Code
    Sequence ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") 节中介绍的NCS。由于代码结构高度结构化，因此有通过处理代码结构输入形成的序列，例如第
    [2.1.3](#S2.SS1.SSS3 "2.1.3\. Abstract Syntax Tree ‣ 2.1\. Structures in Code
    ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    节中介绍的AST，这些指的是扁平化序列。在本节中，我们介绍上述处理序列化代码数据的模型。我们首先展示代码的结构转换，包括获取扁平化序列的不同代码结构预处理方法，然后分别展示处理NCS和扁平化序列的模型。
- en: 3.1\. Structure Transformation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 结构转换
- en: 'The AST format is commonly used to express source code structural information.
    For the sequence model to make efficient use of the code’s structural information,
    some strategies for flattening the AST are offered. Flattening procedures are
    divided into four types, as shown in Fig [4](#S3.F4 "Figure 4 ‣ Type-4: AST partial
    retention ‣ 3.1\. Structure Transformation ‣ 3\. Sequence-based Models ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), which shows the varied
    sequences obtained by different types of flattening approaches.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'AST 格式通常用于表达源代码的结构信息。为了让序列模型高效利用代码的结构信息，提供了一些扁平化AST的策略。扁平化过程分为四种类型，如图 [4](#S3.F4
    "Figure 4 ‣ Type-4: AST partial retention ‣ 3.1\. Structure Transformation ‣ 3\.
    Sequence-based Models ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    所示，展示了不同扁平化方法获得的多样序列。'
- en: 'Type-1: Depth-first traversal'
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'Type-1: 深度优先遍历'
- en: Because the AST represents the code’s structural information as a tree, traversing
    the tree structure depth-first, so that the nodes on each subtree are adjacent
    in the sequence, is the simplest way to extract the flattened sequence. More models
    advocate preorder depth-first traversal because the root node (operator) of each
    subtree in the AST is frequently the subtree’s center. Type-1 refers to the Structure
    Transformation method that uses a depth-first traversal on the AST.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于抽象语法树（AST）以树的形式表示代码的结构信息，因此以深度优先的方式遍历树结构，使得每个子树上的节点在序列中相邻，是提取扁平化序列的最简单方法。更多模型提倡前序深度优先遍历，因为AST中每个子树的根节点（操作符）通常是子树的中心。Type-1
    指的是使用深度优先遍历AST的结构转换方法。
- en: 'Type-2: AST path'
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'Type-2: AST路径'
- en: The approach of serializing the AST via paths is recommended because every two
    nodes in the AST have a path between them. Methods for extracting paths from ASTs
    include paths between arbitrary nodes, paths between terminal nodes, paths from
    terminal nodes to root nodes, and so on. In code generation task, paths from terminal
    nodes to new nodes are also used. Type-2 refers to the approach of Structure Transformation
    that uses paths between nodes on the AST.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐通过路径序列化AST的方法，因为AST中的每两个节点之间都有一条路径。从AST中提取路径的方法包括任意节点之间的路径、终端节点之间的路径、从终端节点到根节点的路径等。在代码生成任务中，也使用从终端节点到新节点的路径。Type-2
    指的是使用AST中节点之间路径的结构转换方法。
- en: 'Type-3: Structure information addition'
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'Type-3: 结构信息添加'
- en: To better keep structural information in the AST and make the flattened sequence
    unique, Type-1/2-based structure information adding methods are proposed. The
    Structure-based traversal approach, for example, uses brackets to represent the
    AST structure, and the brackets in the created sequence may be used to detect
    the subtree of a certain node, allowing the generated sequence to be translated
    back to the AST. The use of ”¡” and ”¿” to enclose the non-terminal node’s subtree,
    which corresponds to the code block, is another technique to preserve structural
    information. As a result, we refer to the Structure Transformation method as Type-3
    because it incorporates code structure information in the obtained sequence.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地保留 AST 中的结构信息并使平展序列唯一，提出了基于 Type-1/2 的结构信息添加方法。例如，基于结构的遍历方法使用括号表示 AST 结构，创建的序列中的括号可用于检测某个节点的子树，从而允许生成的序列转换回
    AST。另一种保留结构信息的技术是使用“¡”和“¿”来括住非终结节点的子树，该子树对应于代码块。因此，我们将结构转换方法称为 Type-3，因为它在获得的序列中包含了代码结构信息。
- en: 'Type-4: AST partial retention'
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Type-4：AST 部分保留
- en: AST contains a lot of structural and syntactic information about the code, but
    it also contains a lot of useless data. As a result, numerous ways suggest filtering
    the ASTs, preserving the nodes of interest, and then flattening the sequence derived
    from the filtered ASTs. To generate flattened sequences in code defect detection
    jobs, only three types of AST nodes are generally kept, for example, method call
    and class instance creation nodes, declaration nodes, and control flow nodes.
    Type-4 refers to the Structure Transformation approach, which keeps only important
    nodes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AST 包含了大量关于代码的结构和语法信息，但也包含了大量无用的数据。因此，许多方法建议过滤 AST，保留感兴趣的节点，然后平展从过滤后的 AST 中得到的序列。在代码缺陷检测任务中，通常只保留三种类型的
    AST 节点，例如方法调用和类实例创建节点、声明节点和控制流节点。Type-4 指的是结构转换方法，它只保留重要的节点。
- en: '![Refer to caption](img/8e41cf0c77213096d47d079e50783536.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e41cf0c77213096d47d079e50783536.png)'
- en: Figure 4\. Examples of local AST using four types of structure transformation
    to obtain flattened sequence, (a) using depth first traversal to convert AST into
    sequence, (b) showing three paths in the leaf to leaf path set, (c) showing SBT(Hu
    et al., [2018](#bib.bib68)) using parentheses to retain structure information,
    (d) only retaining the type information of nodes in AST, and the obtained sequence
    is the same as (a).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 使用四种结构转换类型获得平展序列的局部 AST 示例，（a）使用深度优先遍历将 AST 转换为序列，（b）显示叶子到叶子路径集中的三条路径，（c）显示
    SBT (Hu et al., [2018](#bib.bib68)) 使用括号保留结构信息，（d）仅保留 AST 中节点的类型信息，获得的序列与（a）相同。
- en: 3.2\. Natural code sequence
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 自然代码序列
- en: NCS is a natural way to represent whole code fragments because code is made
    up of individual tokens. The approaches and models for comprehending code using
    NCS are described in this section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: NCS 是一种自然表示整个代码片段的方式，因为代码由独立的标记组成。使用 NCS 理解代码的方法和模型在本节中进行描述。
- en: N-gram Models
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: N-gram 模型
- en: N-gram models, widely used as early language processing approaches, assume a
    Markov property that the probability of the current word is only affected by its
    prefixed N-1 words, which can capture the statistical characteristics of a sequence
    to some extent. In order to take advantage of the statistical properties of NCS,
    some early models using N-Gram models to accomplish code representation tasks
    were proposed. Hindle et al. (Hindle et al., [2012](#bib.bib65)) firstly adopt
    N-gram models on NCS and find that the language models are conducive to extracting
    local statistics by exploring the naturalness rather than the syntax or semantics.
    Tu et al. (Tu et al., [2014](#bib.bib151)) cooperate N-gram model with a cache
    component to further capture the localness of source codes. Karaivanov et al. (Karaivanov
    et al., [2014](#bib.bib81)) exploit the N-gram model for phrase-based programming
    language translation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 模型，广泛用作早期语言处理方法，假设马尔可夫性质，即当前词的概率仅受其前 N-1 个词的影响，这在一定程度上可以捕捉序列的统计特征。为了利用
    NCS 的统计特性，提出了一些早期模型使用 N-Gram 模型来完成代码表示任务。Hindle 等人 (Hindle et al., [2012](#bib.bib65))
    首次在 NCS 上采用 N-gram 模型，发现语言模型有助于通过探索自然性而不是语法或语义来提取局部统计特征。Tu 等人 (Tu et al., [2014](#bib.bib151))
    将 N-gram 模型与缓存组件结合，进一步捕捉源代码的局部性。Karaivanov 等人 (Karaivanov et al., [2014](#bib.bib81))
    利用 N-gram 模型进行基于短语的编程语言翻译。
- en: The N-gram model takes all of the information from the N-1 tokens before the
    token to learn its representation, but it cannot use remote token information
    (such as the reuse of a variable in the code), and it also cannot build identical
    vector representations for tokens with similar meanings. As a result, the N-Gram
    model was gradually replaced by the more learning model introduced later in the
    code representation task.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 模型利用标记前的所有 N-1 个标记的信息来学习其表示，但它不能使用远程标记信息（如代码中变量的重复使用），也无法为具有相似意义的标记构建相同的向量表示。因此，N-Gram
    模型逐渐被后来在代码表示任务中引入的更先进的学习模型所替代。
- en: Convolutional Neural Network
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional neural network (CNN) is first proposed for processing images to
    learn image representations by capturing features of local images through convolutional
    kernels. Also, It can extract significant n-gram features from input sentences
    to create a semantic representation of the potential of sentence information for
    downstream tasks, while effectively capturing rich structural patterns in the
    code, so some work on learning code representations with CNN was proposed. To
    summarize the code snippet, Allamanis et al. ([2016](#bib.bib8)) employ an attentional
    neural network that uses convolution on the input tokens to detect local time-invariant
    and long-range topical attention features in a context-dependent way. For better
    code searching, CARLCS-CNN(Shuai et al., [2020](#bib.bib143)) first embeds code
    and query respectively using CNN since CNN can capture the informative keywords
    in query and code, then learns interdependent representations for the embedded
    code and query by a co-attention mechanism. CNN is unable to model long-range
    dependencies in code sequences, there are local limitations on the sensitivity
    to word order, and since code sequences are often large, CNN models are less often
    used in practical applications to learn to understand code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）最初是为了处理图像而提出的，通过卷积核捕捉局部图像的特征来学习图像表示。同时，它也可以从输入句子中提取重要的 n-gram 特征，以创建潜在句子信息的语义表示，以便用于下游任务，同时有效捕捉代码中的丰富结构模式，因此有些工作使用
    CNN 学习代码表示。为了总结代码片段，Allamanis 等人（[2016](#bib.bib8)）使用了一种注意力神经网络，该网络对输入标记进行卷积，以上下文相关的方式检测局部时间不变和长范围的主题注意力特征。为了更好的代码搜索，CARLCS-CNN（Shuai
    等人，[2020](#bib.bib143)）首先分别使用 CNN 嵌入代码和查询，因为 CNN 能够捕捉查询和代码中的信息关键词，然后通过共同注意机制学习嵌入代码和查询的相互依赖表示。CNN
    无法建模代码序列中的长范围依赖，局部对词序的敏感性有限，而且由于代码序列通常较大，CNN 模型在实际应用中不太常用于学习理解代码。
- en: Recurrent Neural Network
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: As previously section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural Network
    ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models
    for Structural Code Understanding") stated, RNN and their variants perform exceptionally
    well on sequential tasks, resulting in a huge number of RNN-based models for code-related
    tasks to handle NCS. Veselin et al. (RaychevVeselin et al., [2014](#bib.bib133))
    have shown that the well-trained RNN can outperform N-gram models (Hindle et al.,
    [2012](#bib.bib65)) when processing NCS. Dam et al. (Dam et al., [2016](#bib.bib43))
    propose to use LSTM to predict the next token in order to address the inability
    of n-gram models to capture token dependencies in sequences. CodeNN (Iyer et al.,
    [2016](#bib.bib73)) uses LSTM with attention to produce sentences that describe
    code snippets. Liu et al.(Liu et al., [2016](#bib.bib107)) employ latent attention
    over outputs of a Bi-LSTM to better translate natural language descriptions into
    If-Then program. Bhoopchand et al.(Bhoopchand et al., [2016](#bib.bib28)) enhance
    LSTM with a pointer network specialized in referring to predefined classes of
    identifiers to well capture long-range dependencies in the code, thus giving better
    suggestions for the next token input. CODEnn (Gu et al., [2018](#bib.bib58)) provides
    a deep architecture composed of a code embedding network, description embedding
    network, and a similarity module to align the embeddings of code-description pairs.
    The code embedding network and description embedding network are both use LSTM.
    Tal et al. (Ben-Nun et al., [2018a](#bib.bib24)) utilize RNNs to learn a language-agnostic
    intermediate representation that is generated from code syntactical structures.
    Vasic et al.(Vasic et al., [2019](#bib.bib153)) present a solution to the general
    variable-misuse problem in which enumerative search is replaced by a neural network
    containing LSTM that jointly localizes and repairs faults. CodeGRU (Hussain et al.,
    [2020](#bib.bib72)) further applies GRU in code sequence processing to capture
    contextual dependencies. Compared with CNN, RNN family can handle arbitrary length
    input and has more flexible code sequence modeling ability, but vanilla RNN has
    the problem of gradient disappearance and gradient explosion. LSTM and GRU, as
    variants of vanilla RNN, can learn long-term dependency and solve the problem
    of gradient explosion and disappearance to a certain extent, and gradually become
    the current RNN family Core.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面第[2.2.1节](#S2.SS2.SSS1 "2.2.1\. 循环神经网络 ‣ 2.2\. 深度学习模型 ‣ 2\. 初步 ‣ 结构代码理解的深度学习模型综述")所述，RNN及其变体在序列任务上表现卓越，因此有大量基于RNN的模型用于处理NCS。Veselin等人（RaychevVeselin等人，[2014](#bib.bib133)）已经证明，经过良好训练的RNN在处理NCS时可以超越N-gram模型（Hindle等人，[2012](#bib.bib65)）。Dam等人（Dam等人，[2016](#bib.bib43)）提出使用LSTM来预测下一个token，以解决n-gram模型无法捕捉序列中token依赖的问题。CodeNN（Iyer等人，[2016](#bib.bib73)）使用带有注意力机制的LSTM来生成描述代码片段的句子。Liu等人（Liu等人，[2016](#bib.bib107)）在Bi-LSTM的输出上应用潜在注意力，以更好地将自然语言描述翻译为If-Then程序。Bhoopchand等人（Bhoopchand等人，[2016](#bib.bib28)）通过增强LSTM的指针网络，专门用于引用预定义的标识符类别，以更好地捕捉代码中的长程依赖，从而为下一个token输入提供更好的建议。CODEnn（Gu等人，[2018](#bib.bib58)）提供了一个深度架构，由代码嵌入网络、描述嵌入网络和相似性模块组成，以对齐代码-描述对的嵌入。代码嵌入网络和描述嵌入网络都使用LSTM。Tal等人（Ben-Nun等人，[2018a](#bib.bib24)）利用RNN学习一种语言无关的中间表示，该表示由代码语法结构生成。Vasic等人（Vasic等人，[2019](#bib.bib153)）提出了一种解决一般变量误用问题的方案，其中枚举搜索被包含LSTM的神经网络所取代，该网络共同定位和修复故障。CodeGRU（Hussain等人，[2020](#bib.bib72)）进一步在代码序列处理过程中应用了GRU，以捕捉上下文依赖。与CNN相比，RNN家族可以处理任意长度的输入，并且具有更灵活的代码序列建模能力，但传统RNN存在梯度消失和梯度爆炸的问题。LSTM和GRU作为传统RNN的变体，在一定程度上可以学习长期依赖，并解决梯度爆炸和消失的问题，逐渐成为当前RNN家族的核心。
- en: Transformer
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Transformer
- en: Transformer can successfully handle the distant dependence problem, overcome
    the limitation that RNNs cannot be computed in parallel, and employ self-attention
    to generate more explanatory models, as described in section [2.2.2](#S2.SS2.SSS2
    "2.2.2\. Transformer ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"). Transformer is being
    used for a growing number of sequence-related work, and NCS is no exception. Ahmad
    et al. (Ahmad et al., [2020](#bib.bib3)) first employ the Transformer for code
    summarization to handle the ubiquitous long-range dependencies in source code
    from natural code sequence. TFix(Berabi et al., [2021](#bib.bib27)) works directly
    on program text and phrases the problem of code fixing as a text-to-text task,
    so it can leverage a powerful Transformer based model pre-trained on natural language
    and fine-tuned to generate code fixes. CodeBERT (Feng et al., [2020](#bib.bib52)),CuBert(Kanade
    et al., [2020](#bib.bib80)), GPT-C(Svyatkovskiy et al., [2020](#bib.bib146)) and
    CodeT5(Wang et al., [2021d](#bib.bib168)) use both NCS and related natural language
    to pre-train the Transformer architectures for downstream tasks, such as code
    search, code clone detection and code summarization. OSCAR(Peng et al., [2021](#bib.bib127))
    and GraphCodeBERT(Guo et al., [2020](#bib.bib59)) are also pre-trained models
    based on transformer using NCS while exploiting the semantic information of flow
    graph. OSCAR adds GCF information to the model training using positional encoding.
    GraphCodeBERT takes DFG as part of the input while exploiting the node and edge
    relationships for Graph-Guided Masked Attention to better understand the code.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 能够成功处理远程依赖问题，克服 RNN 无法并行计算的限制，并采用自注意力机制生成更具解释性的模型，正如在[2.2.2](#S2.SS2.SSS2
    "2.2.2\. Transformer ‣ 2.2\. 深度学习模型 ‣ 2\. 初步 ‣ 深度学习模型在结构代码理解中的调查")节中所描述的。Transformer
    正在被用于越来越多的序列相关工作，NCS 也不例外。Ahmad 等人（Ahmad et al., [2020](#bib.bib3)）首次使用 Transformer
    进行代码总结，以处理源代码中普遍存在的长距离依赖。TFix（Berabi et al., [2021](#bib.bib27)）直接处理程序文本，并将代码修复问题表述为文本到文本的任务，因此可以利用在自然语言上预训练的强大
    Transformer 模型，并对其进行微调以生成代码修复。CodeBERT（Feng et al., [2020](#bib.bib52)）、CuBert（Kanade
    et al., [2020](#bib.bib80)）、GPT-C（Svyatkovskiy et al., [2020](#bib.bib146)）和 CodeT5（Wang
    et al., [2021d](#bib.bib168)）利用 NCS 和相关自然语言对 Transformer 架构进行预训练，用于下游任务，如代码搜索、代码克隆检测和代码总结。OSCAR（Peng
    et al., [2021](#bib.bib127)）和 GraphCodeBERT（Guo et al., [2020](#bib.bib59)）也是基于
    Transformer 的预训练模型，使用 NCS，同时利用流图的语义信息。OSCAR 通过位置编码将 GCF 信息添加到模型训练中。GraphCodeBERT
    将 DFG 作为输入的一部分，同时利用节点和边关系进行图引导的掩码注意力，以更好地理解代码。
- en: Transformer can effectively learn a big quantity of data, but its memory and
    processing requirements are enormous when compared to models like RNN. Following
    that, work should be measured in terms of resource consumption and performance
    improvement, and a suitable model should be chosen by taking into account the
    current circumstances.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 能够有效地学习大量数据，但与 RNN 等模型相比，其内存和处理要求非常高。因此，后续工作应考虑资源消耗和性能提升，并根据当前情况选择合适的模型。
- en: Others
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他
- en: Other models have been incorporated in related work in addition to CNN, RNN,
    and other models that are extensively utilized in NCS related tasks. Sachdev et
    al.(Sachdev et al., [2018](#bib.bib137)) create a continuous vector embedding
    of each code fragment at method–level granularity, map the given natural language
    query to the same vector space, and use vector distance to simulate relevance
    of code fragments to a given query. CCLearner(Li et al., [2017a](#bib.bib97))
    extracts token sequence from known method-level code clones and non-clones to
    train a deep Neural Network(classifier) and then uses the classifier to detect
    clones in a given codebase. SCC(Alreshedy et al., [2018](#bib.bib16)) is a classifier
    that can identify the programming language of code snippets written in 21 different
    programming languages, it employs a Multinomial Naive Bayes(MNB) classifier trained
    using Stack Overflow posts. Sachdev et al.(Sachdev et al., [2018](#bib.bib137))
    propose a simple yet effective unsupervised model that combines word2vec(Mikolov
    et al., [2013](#bib.bib118)) and information retrieval methods for code search.
    UNIF (Cambronero et al., [2019a](#bib.bib32)) first uses word2vec to embeds code/quey
    and then combines code embedding with attention. These models show better results
    in specific tasks, and therefore, subsequent work dealing with NCS should not
    be limited to the adoption of mainstream models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在NCS相关任务中广泛使用的CNN、RNN等模型外，相关工作中还引入了其他模型。Sachdev等人（Sachdev et al., [2018](#bib.bib137)）创建了每个代码片段在方法级别粒度上的连续向量嵌入，将给定的自然语言查询映射到相同的向量空间，并使用向量距离来模拟代码片段与给定查询的相关性。CCLearner（Li
    et al., [2017a](#bib.bib97)）从已知的方法级代码克隆和非克隆中提取令牌序列来训练深度神经网络（分类器），然后使用分类器在给定的代码库中检测克隆。SCC（Alreshedy
    et al., [2018](#bib.bib16)）是一个能够识别21种不同编程语言中代码片段编程语言的分类器，它使用了一个通过Stack Overflow帖子训练的多项式朴素贝叶斯（MNB）分类器。Sachdev等人（Sachdev
    et al., [2018](#bib.bib137)）提出了一种简单而有效的无监督模型，结合了**word2vec**（Mikolov et al., [2013](#bib.bib118)）和信息检索方法用于代码搜索。UNIF（Cambronero
    et al., [2019a](#bib.bib32)）首先使用**word2vec**对代码/查询进行嵌入，然后将代码嵌入与注意力机制结合。这些模型在特定任务中表现更好，因此，处理NCS的后续工作不应仅限于采用主流模型。
- en: 3.3\. Flattened Sequence
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 扁平化序列
- en: The four structural transformations indicated in [3.1](#S3.SS1 "3.1\. Structure
    Transformation ‣ 3\. Sequence-based Models ‣ A Survey of Deep Learning Models
    for Structural Code Understanding") can be used to obtain flattened sequences.
    Although the flattening procedure consumes more resources, the flattened sequences
    preserve some structural information about the code, and the applicable models
    can learn more about the code from the flattened sequences than NCS. The sequences
    obtained by different structural transformations may be suitable for different
    models. The models for processing the flattened sequence are described in the
    following and these models also show good performance in the natural language
    processing field. We make a finer segmentation of the RNN family in this section,
    including Vanilla LSTM, Bi-direction LSTM, and GRU, because of the enormous variety
    of applications of RNNs and their variants on flattened sequences.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.1](#S3.SS1 "3.1\. 结构变换 ‣ 3\. 基于序列的模型 ‣ 结构代码理解的深度学习模型综述")中指出的四种结构变换可以用来获得扁平化序列。尽管扁平化过程消耗更多资源，但扁平化序列保留了一些关于代码的结构信息，适用的模型可以从扁平化序列中比NCS学到更多关于代码的信息。通过不同结构变换获得的序列可能适用于不同的模型。处理扁平化序列的模型在以下内容中进行了描述，这些模型在自然语言处理领域也表现良好。我们在本节中对RNN家族进行了更精细的细分，包括**Vanilla
    LSTM**、**双向LSTM**和**GRU**，因为RNN及其变体在扁平化序列中的应用种类繁多。
- en: Word2vec
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Word2vec
- en: word2vec is a method of converting an input token into a vector representation,
    where the converted vector contains, to some extent, the contextual information
    of the token. Word2vec contains two training models, CBOW (Continuous Bag-of-Words
    Model) and Skip-gram (Continuous Skip-gram Model), which have strong generality
    and are therefore used in early work on flattened sequence for learning code representation.
    API2Vec(Nguyen et al., [2017](#bib.bib123)) traverses the AST using Type-4 to
    build an annotation sequence according to the syntactic units related to APIs.
    These sequences are then used to train CBOW to generate API embeddings that may
    be utilized to migrate equivalent API usage from Java to C#. Alon et al.(Alon
    et al., [2018c](#bib.bib13)) first use Type-3 to obtain flattened sequence by
    adding up and down momentum information to the paths between nodes and then use
    word2vec to complete the prediction of method names. Because Word2vec is unable
    to learn the representation of polysemous words and cannot successfully capture
    long-range dependencies, further work is being done to combine Word2vec with other
    models to learn flattened sequences better.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec是一种将输入token转换为向量表示的方法，转换后的向量在一定程度上包含了token的上下文信息。Word2vec包含两种训练模型，CBOW（连续词袋模型）和Skip-gram（连续Skip-gram模型），它们具有较强的通用性，因此在早期的展平序列代码表示学习中得到了应用。API2Vec（Nguyen
    et al., [2017](#bib.bib123)）利用Type-4遍历AST，根据与API相关的语法单位构建注释序列。这些序列随后用于训练CBOW生成API嵌入，这些嵌入可能用于将等效API用法从Java迁移到C#。Alon等（Alon
    et al., [2018c](#bib.bib13)）首先使用Type-3通过增加节点间路径的上升和下降动量信息来获得展平序列，然后使用word2vec完成方法名称的预测。由于Word2vec无法学习多义词的表示并且无法成功捕捉长距离依赖，因此正在进行进一步的工作，将Word2vec与其他模型结合，以更好地学习展平序列。
- en: Deep Belief Network
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度置信网络（Deep Belief Network）
- en: 'Deep Belief Network(DBN)(Bengio, [2009](#bib.bib26)) is a generative model
    which uses a multi-level neural network to learn a representation from training
    data that could reconstruct the semantic and content of input data with Maximum
    probability. DBN can be used to identify features, classify data, generate data,
    and do other tasks. Wang et al.(Wang et al., [2016](#bib.bib162), [2020a](#bib.bib161))
    use Type-4 and then use DBN to complete the defect prediction. They produce flattened
    sequences from only three sorts of AST nodes: method invocation and class instance
    creation nodes, declaration nodes, and control flow nodes. Because the names of
    methods, classes, and types are usually project-specific, there are very few methods
    with the same name across multiple projects. To get better detection results,
    they extract all three classes of AST nodes for cross-project defect prediction(CPDP),
    but instead of utilizing their names, it uses their AST node type, such as method
    declaration and method invocation, for declaration nodes and control flow nodes.
    DBN can automatically learn semantic features from token vectors extracted from
    ASTs and is one of the first non-convolutional models to be successfully trained
    by applying deep architectures. However, DBN has mostly lost favor and is rarely
    used compared to other unsupervised or generative learning algorithms nowadays.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 深度置信网络（DBN）（Bengio, [2009](#bib.bib26)）是一种生成模型，利用多层神经网络从训练数据中学习表示，以最大概率重建输入数据的语义和内容。DBN可用于特征识别、数据分类、数据生成等任务。Wang等（Wang
    et al., [2016](#bib.bib162), [2020a](#bib.bib161)）使用Type-4，然后利用DBN完成缺陷预测。他们仅从三种AST节点生成展平序列：方法调用和类实例创建节点、声明节点和控制流节点。由于方法、类和类型的名称通常是项目特定的，跨多个项目的相同名称的方法非常少。为了获得更好的检测结果，他们提取了所有三类AST节点用于跨项目缺陷预测（CPDP），但不是利用其名称，而是使用其AST节点类型，如方法声明和方法调用，用于声明节点和控制流节点。DBN可以从从AST中提取的token向量中自动学习语义特征，是第一个通过应用深度架构成功训练的非卷积模型之一。然而，与其他无监督或生成学习算法相比，DBN如今已经不再受青睐，并且使用较少。
- en: Vanilla Long Short-Term Memory
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 香草长短期记忆（Vanilla Long Short-Term Memory）
- en: 'Vanilla LSTM is the LSTM introduced in section [2.2.1](#S2.SS2.SSS1 "2.2.1\.
    Recurrent Neural Network ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), which is capable
    of learning long-range dependencies in some extent and is heavily used in work
    dealing with flattened sequences. Liu et al.(Liu et al., [2017](#bib.bib108))
    use Type-1 and explore several variants of simple LSTM architecture for different
    variants of the code completion problem. Li et al.(Li et al., [2017b](#bib.bib94))
    utilize Type-3, which allows storing two extra bits of information about whether
    the AST has children and/or right siblings in the type node. And the pointer mixture
    network proposed consists of two main components: a global RNN component(LSTM)
    and a local pointer component, which utilizes the pointer network to point to
    the previous position in the local context according to the learned position weights
    to solve the OoV problem. DeepCom (Hu et al., [2018](#bib.bib68)) uses Type-3
    and design a new structure-based traversal(SBT) method to better preserve structural
    information in the code. The SBT traversal method uses parentheses to indicate
    the structure of the AST, and the parenthesis in the created sequence may be utilized
    to determine the subtree of a given node, allowing the generated sequence to be
    transformed back to AST. DeepCom employs the seq2seq model which uses LSTM as
    encoder and decoder to generate code fragment summaries. code2vec (Alon et al.,
    [2019c](#bib.bib15)) employs Type-2 to represent code fragments with the set of
    paths between all terminal node pairs in the code to complete the prediction of
    method names of the code and learns the representation of the sequence of internal
    non-terminal nodes using LSTM. For better program classification, Compton et al.(Compton
    et al., [2020](#bib.bib40)) investigate the effect of obfuscating variable names
    during the training of a code2vec model to force it to rely on the structure of
    the code rather than specific names and consider a simple approach to creating
    class-level embeddings by aggregating sets of method embeddings. Seml(Liang et al.,
    [[n.d.]](#bib.bib103)) uses Type-4 and sends the sequence obtained after filtering
    the AST to the LSTM to complete the defect detection of the code. Type-3 is used
    by SA-LSTM(Liu et al., [2020d](#bib.bib111)), which surrounds the sub-tree of
    each non-leaf node, which corresponds to code blocks, with ¡ and ¿. SA-LSTM enhances
    the LSTM network with a stack to store and recover contextual information based
    on the code’s structure for modeling the hierarchical structure of code.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Vanilla LSTM 是在第 [2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural Network ‣ 2.2\.
    Deep learning models ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for
    Structural Code Understanding") 节中介绍的 LSTM，它能够在一定程度上学习长期依赖，并且在处理平展序列的工作中被广泛使用。刘等（刘等，
    [2017](#bib.bib108)）使用 Type-1，并探索了几种简单 LSTM 架构的变体，以解决不同变体的代码补全问题。李等（李等， [2017b](#bib.bib94)）利用
    Type-3，这允许在类型节点中存储关于 AST 是否有子节点和/或右兄弟的两个额外信息位。提议的指针混合网络包括两个主要组件：全局 RNN 组件（LSTM）和本地指针组件，后者利用指针网络根据学习到的位置权重指向本地上下文中的先前位置，以解决
    OoV 问题。DeepCom（胡等， [2018](#bib.bib68)）使用 Type-3 并设计了一种基于结构的遍历（SBT）方法，以更好地保留代码中的结构信息。SBT
    遍历方法使用括号来表示 AST 的结构，并且创建序列中的括号可能被用来确定给定节点的子树，从而允许生成的序列被转换回 AST。DeepCom 采用 seq2seq
    模型，其中 LSTM 作为编码器和解码器生成代码片段摘要。code2vec（阿隆等， [2019c](#bib.bib15)）采用 Type-2，通过代码中所有终端节点对之间的路径集合来表示代码片段，以完成对代码方法名称的预测，并使用
    LSTM 学习内部非终端节点序列的表示。为了更好地进行程序分类，Compton 等（Compton 等， [2020](#bib.bib40)）研究了在训练
    code2vec 模型过程中混淆变量名称的效果，以迫使其依赖于代码的结构而非具体名称，并考虑了一种通过聚合方法嵌入创建类级别嵌入的简单方法。Seml（梁等，[[n.d.]](#bib.bib103)）使用
    Type-4 并将过滤 AST 后获得的序列发送到 LSTM 以完成代码的缺陷检测。SA-LSTM（刘等， [2020d](#bib.bib111)）使用
    Type-3，它用 ¡ 和 ¿ 包围每个非叶节点的子树，这对应于代码块。SA-LSTM 通过一个栈来增强 LSTM 网络，以存储和恢复基于代码结构的上下文信息，从而对代码的层次结构进行建模。
- en: Different works will modify the LSTM to suit their own tasks, but the LSTM has
    a disadvantage in parallel processing and cannot fully solve the gradient problem,
    as well as cannot do anything for a very large order of magnitude sequences, and
    Bi-LSTM and GRU introduced later also face the same problem. The latest effort
    will use LSTM or other RNN variants as a component of the model and mix it with
    other models to accomplish the task, in order to better exploit the advantages
    of RNN and its variants on sequence processing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的工作将修改LSTM以适应自己的任务，但LSTM在并行处理方面存在劣势，无法完全解决梯度问题，也不能处理非常大规模的序列，后来引入的Bi-LSTM和GRU也面临相同的问题。最新的努力将LSTM或其他RNN变体作为模型的一个组件，并与其他模型混合，以更好地利用RNN及其变体在序列处理上的优势。
- en: Bi-direction Long Short-Term Memory
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双向长短期记忆
- en: Compared with the traditional LSTM which only retains the previous information,
    the bidirectional Long short Memory (Bi-LSTM) can also use the later information,
    which can better capture the semantic dependencies in both directions. code2seq (Alon
    et al., [2018a](#bib.bib9)) employs the same Type-2 as code2vec to obtain the
    flattened sequence, and Bi-LSTM was used to learn the representation of the internal
    non-terminal nodes sequence. DeepCPDP(Chen et al., [2019a](#bib.bib35)) uses Type-4,
    using simplified Abstract Syntax Tree(SimAST) to represent the source code of
    each extracted program. DeepCPDP uses SimASTToken2Vec, an unsupervised-based embedding
    approach, and will classify the code inputted as defective or non-defective using
    Bi-LSTM with attention mechanism and Logistic regression. Pythia (Svyatkovskiy
    et al., [2019](#bib.bib147)) uses Type-1 to predict the method names and API calls
    that the developer wants to use in programming. Pythia aggregates the initial(obtained
    by word2vec) and intermediate(learned by Bi-LSTM) vector representations through
    a fully connected layer to obtain the final vector for prediction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的仅保留先前信息的LSTM相比，双向长短期记忆（Bi-LSTM）还可以利用后续信息，从而更好地捕捉双向的语义依赖。`code2seq`（Alon
    等， [2018a](#bib.bib9)）采用与`code2vec`相同的Type-2方法来获得扁平化序列，并使用Bi-LSTM学习内部非终结节点序列的表示。`DeepCPDP`（Chen
    等， [2019a](#bib.bib35)）使用Type-4，采用简化的抽象语法树（SimAST）来表示每个提取程序的源代码。`DeepCPDP`使用SimASTToken2Vec，一种基于无监督的嵌入方法，并将代码输入分类为缺陷或非缺陷，使用带有注意力机制的Bi-LSTM和逻辑回归。`Pythia`（Svyatkovskiy
    等， [2019](#bib.bib147)）使用Type-1来预测开发者在编程中想要使用的方法名称和API调用。`Pythia`通过一个全连接层将初始（通过word2vec获得）和中间（由Bi-LSTM学习）向量表示进行汇总，以获得最终的预测向量。
- en: Gated Recurrent Units
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 门控递归单元
- en: The GRU introduced in section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural
    Network ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") is a simplified version of LSTM with
    a reduced number of gates and therefore easier to converge with relatively few
    parameters, so some work will choose to use GRU to learn the representation of
    the flattened sequence. ast-attendgru(LeClair et al., [2019](#bib.bib90)) uses
    Type-3 and proposes SBT-AO which modifies the SBT AST Flastting procedure to simulate
    the case when only an AST can be extracted. SBT-AO replaces all words (except
    official Java API class names) in the code to a special ¡OTHER¿ token, remaining
    all the code structure in SBT. It uses two GRU with attention mechanism to process
    NCS and SBT-AO respectively for getting context vector and then predicts the summary
    one word at a time from the context vector, following what is typical in seq2seq
    models. Hybrid-Deepcom(Hu et al., [2020a](#bib.bib69)) uses the same Type-4 as
    Deepcom to obtain the flattened sequence. It also employs the seq2seq model for
    code summarization and utilizes two GRU as encoders for NCS and flattened sequence,
    respectively, to acquire both lexical and structural information of code fragments.
    GRU has one less gate and relatively fewer parameters than LSTM, so it is easier
    to converge and less computationally expensive, while having similar results in
    most tasks, but LSTM performs better with larger data sets, so the choice of LSTM
    and GRU in code sequence tasks needs to consider both data set size, training
    effect and training time.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural Network ‣ 2.2\. Deep learning
    models ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code
    Understanding")节中介绍的GRU是LSTM的简化版本，具有较少的门控，因此更易于收敛，并且参数较少，因此一些工作会选择使用GRU来学习展平序列的表示。ast-attendgru（LeClair等，[2019](#bib.bib90)）使用Type-3，并提出了SBT-AO，该方法修改了SBT
    AST Flastting过程，以模拟只能提取AST的情况。SBT-AO将代码中的所有单词（除了官方Java API类名）替换为特殊的¡OTHER¿标记，保持了SBT中的所有代码结构。它使用两个带有注意力机制的GRU分别处理NCS和SBT-AO，以获取上下文向量，然后从上下文向量中逐字预测摘要，这符合seq2seq模型中的典型做法。Hybrid-Deepcom（Hu等，[2020a](#bib.bib69)）使用与Deepcom相同的Type-4来获得展平序列。它还采用seq2seq模型进行代码摘要，并分别利用两个GRU作为编码器来处理NCS和展平序列，从而获取代码片段的词汇和结构信息。GRU的门控较少，参数相对较少，因此更易于收敛，计算开销也较低，而在大多数任务中结果相似，但LSTM在较大的数据集上表现更好，因此在代码序列任务中选择LSTM和GRU时需要考虑数据集大小、训练效果和训练时间。
- en: Transformer
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Transformer
- en: 'As previously described, Transformer is a more complex and powerful model compared
    to RNN, and more and more work has been done in recent years using transform to
    learn flattened sequences to accomplish related tasks. SLM(Alon et al., [2020](#bib.bib11))
    uses Type-2 to represent the code as a path from the root node and all leaf nodes
    to the target node. SLM uses LSTM to obtain the representation of all paths separately
    and then uses Transformer contextualize the path representation of all leaf nodes
    to the target node, and at the same time adds the position index in the parent
    node to the path representation from the root node to the target node, and passes
    the attention mechanism. The final vector is obtained by combining the path representation
    to make predictions on the target node. Kim et al.(Kim et al., [2020](#bib.bib83))
    propose three methods based on Transformer to better predict the token that the
    developer is about to input: 1\. pathTrans, uses Type-2 to serialize the AST using
    the path from the leaf node to the root node; 2\. TravTrans, uses Type-1, and
    uses the method of preorder first traversal to obtain sequences from the AST;
    3\. TravTrans+, uses Type-3, adding a matrix that saves the unique path between
    two nodes in TravTrans to enhance the Transformer’s self Note the block. And the
    experiment proves that TravTrans+ works better. Liu et al.(Liu et al., [2020c](#bib.bib110))
    uses both Type-1 and Type-2 to complete the prediction of the code. It uses Transformer-XL
    to encode the sequence obtained by preorder traversal and uses Bi-LSTM to encode
    the path from the target node to the root node, preserving the hierarchical information
    of the target node. TreeBERT(Xue et al., [2021](#bib.bib180)) employs Type-2 to
    represent the entire code fragment using multiple paths from the root node to
    the leaf nodes in the AST and then completes the pre-training using the modified
    Transformer architecture. The position embedding of each node is generated from
    its hierarchical information in the AST and the position information of its parent
    node to make better use of the structural information of the code. Since Transformer
    requires a lot of data and a lot of computational resources, subsequent work should
    consider more than just model performance when using Transformer.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，相较于RNN，Transformer是一个更复杂、更强大的模型，近年来越来越多的工作使用transform来学习扁平化的序列以完成相关任务。SLM（Alon
    et al., [2020](#bib.bib11)）使用Type-2将代码表示为从根节点到目标节点及所有叶子节点的路径。SLM使用LSTM分别获取所有路径的表示，然后使用Transformer对所有叶子节点到目标节点的路径表示进行上下文化，同时将父节点中的位置索引添加到从根节点到目标节点的路径表示中，并通过注意力机制进行传递。最终向量通过组合路径表示来对目标节点进行预测。Kim
    et al.（Kim et al., [2020](#bib.bib83)）提出了三种基于Transformer的方法来更好地预测开发者即将输入的token：1.
    pathTrans，使用Type-2通过从叶子节点到根节点的路径对AST进行序列化；2. TravTrans，使用Type-1，并采用先序遍历的方法从AST中获得序列；3.
    TravTrans+，使用Type-3，添加一个矩阵来保存TravTrans中两个节点之间的唯一路径，以增强Transformer的自注意力块。实验证明，TravTrans+效果更佳。Liu
    et al.（Liu et al., [2020c](#bib.bib110)）同时使用Type-1和Type-2来完成代码预测。它使用Transformer-XL对先序遍历获得的序列进行编码，并使用Bi-LSTM对从目标节点到根节点的路径进行编码，保留目标节点的层次信息。TreeBERT（Xue
    et al., [2021](#bib.bib180)）采用Type-2，通过从根节点到AST中叶子节点的多个路径来表示整个代码片段，然后使用修改后的Transformer架构完成预训练。每个节点的位置嵌入是根据其在AST中的层次信息和父节点的位置信息生成的，以更好地利用代码的结构信息。由于Transformer需要大量的数据和计算资源，后续工作在使用Transformer时应考虑的不仅仅是模型性能。
- en: 4\. Graph-based Models
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 基于图的模型
- en: While the sequential model with serialized structure as input is simple and
    visible in many works, the linear order of code snippets is inevitably losing
    hierarchical syntactic information. Therefore, recent works pay more attention
    to capturing the syntactic information of the code. Graph-based models for code
    understanding are described and categorized in this section, based on the structures
    used in the methods without serialization.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用序列化结构作为输入的顺序模型在许多工作中是简单且可见的，但代码片段的线性顺序不可避免地丢失了层次语法信息。因此，近年来的工作更加关注捕捉代码的语法信息。本节描述并分类了用于代码理解的基于图的模型，基于方法中使用的无序列化结构。
- en: The structures used in graph-based models are usually AST and Flow Graph, including
    CFG and DFG, introduced in section [2.1.3](#S2.SS1.SSS3 "2.1.3\. Abstract Syntax
    Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and [2.1.4](#S2.SS1.SSS4 "2.1.4\. Flow
    Graph ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"). We also introduce models that use
    other structures in section [2.1.5](#S2.SS1.SSS5 "2.1.5\. Other Structures ‣ 2.1\.
    Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural
    Code Understanding"). Unlike the sequence-based models, these structures of a
    program are considered as a tree structure or graph structure. In this section,
    we first introduce the transformation conducted in these structures and then categorized
    different models in AST, flow graphs, the combination of them, and other rarely
    seen structures generated from source code.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在图基模型中使用的结构通常是 AST 和 Flow Graph，包括在章节 [2.1.3](#S2.SS1.SSS3 "2.1.3\. 抽象语法树 ‣
    2.1\. 代码结构 ‣ 2\. 初步 ‣ 结构化代码理解的深度学习模型综述") 和 [2.1.4](#S2.SS1.SSS4 "2.1.4\. 流图 ‣
    2.1\. 代码结构 ‣ 2\. 初步 ‣ 结构化代码理解的深度学习模型综述") 中介绍的 CFG 和 DFG。我们还在章节 [2.1.5](#S2.SS1.SSS5
    "2.1.5\. 其他结构 ‣ 2.1\. 代码结构 ‣ 2\. 初步 ‣ 结构化代码理解的深度学习模型综述") 中介绍了使用其他结构的模型。与基于序列的模型不同，这些程序的结构被视为树结构或图结构。在本节中，我们首先介绍这些结构中的变换，然后将不同的模型分为
    AST、流图、它们的组合以及从源代码生成的其他较少见的结构。
- en: 4.1\. Structure Transformation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 结构变换
- en: Different from the transformation of sequences, the modifications of graph structures
    tend to add nodes or edges based on one typical graph structure. We summarize
    the structure’s transformation of the graph into three categories, the first two
    transformations are proposed on the basis of preserving a graph structure for
    further learning the representation of the graph, while the last transformation
    is extracting the information of graph and constructing a new mechanism or DSL(Domain-specific
    language) without keeping the program graph.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与序列的变换不同，图结构的修改往往基于一个典型的图结构来添加节点或边。我们将图的结构变换总结为三类，前两类变换是在保留图结构的基础上进行的，以便进一步学习图的表示，而最后一种变换则是提取图的信息并构建一个新的机制或
    DSL（领域特定语言），而不保留程序图。
- en: 'Type-1: Adding Edges'
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类型-1：添加边
- en: In AST-based structure, methods treat the structure from two different perspectives
    for better using the structural information. One takes the structure as a ”tree”,
    which means that the structure has directed edges with the hierarchy preserved.
    The other extends the AST structure by adding various types of edges and eventually
    makes the edge bidirectional, which makes the original AST structure a ”graph”.
    In two perspectives, adding edges are the most common transformation to preserve
    more information of structures. Allamanis et al. (Allamanis et al., [2018b](#bib.bib6))
    use AST as the backbone and add additional edges for capturing data-flow information.
    The edges contain types derived from AST(e.g. Child and NextToken) and from semantic(e.g.
    LastUse, LastWrite). Wang et al. (Wang and Li, [2021](#bib.bib166)) extend the
    AST with parent-child edges. Dinella(Dinella et al., [2020](#bib.bib46)) add SuccToken
    edges between the leaf nodes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 AST 的结构中，方法从两个不同的角度处理结构，以更好地利用结构信息。一种将结构视为“树”，这意味着结构具有有向边且层次结构得以保留。另一种通过添加各种类型的边来扩展
    AST 结构，最终使边双向，从而使原始的 AST 结构成为“图”。在这两种视角中，添加边是最常见的变换，以保留更多的结构信息。Allamanis 等人（Allamanis
    et al., [2018b](#bib.bib6)）使用 AST 作为基础，并添加额外的边以捕获数据流信息。这些边包含从 AST 衍生的类型（例如 Child
    和 NextToken）和语义（例如 LastUse, LastWrite）。Wang 等人（Wang and Li, [2021](#bib.bib166)）通过添加父子边扩展了
    AST。Dinella（Dinella et al., [2020](#bib.bib46)）在叶节点之间添加了 SuccToken 边。
- en: 'Type-2: Combination'
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类型-2：组合
- en: Some methods use AST-based and Flow-graph-based structures in combination, such
    as the structure CDFG, which is a combination of CFG and DFG. Other structures
    for graph-based models have been proposed, as well as some new structures based
    on the three basic structures. The combination will be discussed and introduced
    in Section [4.4](#S4.SS4 "4.4\. Combined Structures ‣ 4\. Graph-based Models ‣
    A Survey of Deep Learning Models for Structural Code Understanding").
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法结合了基于 AST 和流图的结构，如 CDFG 结构，它是 CFG 和 DFG 的结合。还提出了其他图基模型结构以及一些基于这三种基本结构的新结构。这些组合将在第
    [4.4](#S4.SS4 "4.4\. Combined Structures ‣ 4\. Graph-based Models ‣ A Survey of
    Deep Learning Models for Structural Code Understanding")节中讨论和介绍。
- en: 'Type-3: Information Extracting'
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 类型 3：信息提取
- en: Some methods choose not to preserve the graph structure of AST, Flow graphs,
    or others. Instead, they propose new structures, mechanisms, or DSL based on the
    syntactic and semantic information extracted from these graph structures. It is
    not the priority of our introductions in graph-based models, but still an important
    kind of structures transformation from graph structures. For example, Raychev
    et al. (Raychev et al., [2016](#bib.bib132)) propose a method to build probabilistic
    models of code and generate DSL called TGEN, for traversing AST and accumulating
    a conditioning context. Cvitkovic et al. (Cvitkovic et al., [2019](#bib.bib41))
    propose a Graph-Structured Cache for the out-of-vocabulary problem. An edit DSL
    called Tocopo(Tarlow et al., [2020b](#bib.bib150)) is created for code editing
    in bug fixing problems, which contains token expressions, copy expressions, and
    pointer expressions. Tocopo is a sequence of edit operations that represents the
    modification of an AST.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法选择不保留 AST、流图或其他图结构的结构，而是提出了基于从这些图结构中提取的语法和语义信息的新结构、机制或 DSL。这不是我们介绍图基模型的重点，但仍然是一种从图结构变换的结构。例如，Raychev
    等人（Raychev et al., [2016](#bib.bib132)）提出了一种构建代码概率模型的方法，并生成了一个名为 TGEN 的 DSL，用于遍历
    AST 和积累条件上下文。Cvitkovic 等人（Cvitkovic et al., [2019](#bib.bib41)）提出了一种图结构缓存，用于解决词汇外问题。一个名为
    Tocopo（Tarlow et al., [2020b](#bib.bib150)）的编辑 DSL 被创建用于在修复 bug 问题时进行代码编辑，它包含了
    token 表达式、复制表达式和指针表达式。Tocopo 是一系列编辑操作，表示对 AST 的修改。
- en: 4.2\. AST-based Structures
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 基于 AST 的结构
- en: As previously mentioned, the following models are considerably varied as a result
    of the differences between the perspectives of the AST structure.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，由于 AST 结构视角的不同，以下模型有很大的差异。
- en: 4.2.1\. Tree perspective
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 树视角
- en: Due to the large and deep structure of the tree structure, certain methods tend
    to use recursion of the tree structure to reduce the complexity of programs, especially
    passing the information from sub-trees to the full tree. Shi et al.(Shi et al.,
    [2021](#bib.bib139)) propose a method to split and reconstruct the whole AST of
    a program into subtrees hierarchically to get the representation of the code snippets.
    ASTNN (Zhang et al., [2019a](#bib.bib194)) uses a preorder traversal algorithm
    to split an AST into a set of statement subtrees, recursively encodes them to
    vectors, and eventually learns the representation of source code through the captured
    naturalness by BiGRU and RvNN. The method can reduce the difficulty in training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于树结构的庞大和深度结构，某些方法往往使用树结构的递归来减少程序的复杂性，特别是在将信息从子树传递到完整树时。Shi 等人（Shi et al., [2021](#bib.bib139)）提出了一种方法，将程序的整个
    AST 分层分解和重建为子树，以获得代码片段的表示。ASTNN（Zhang et al., [2019a](#bib.bib194)）使用前序遍历算法将 AST
    划分为一组语句子树，递归地将它们编码为向量，并最终通过 BiGRU 和 RvNN 捕捉的自然性学习源代码的表示。这种方法可以降低训练的难度。
- en: Most methods tend to modify the Recursive Neural Networks or Seq2seq model based
    on the structure of AST, such as AST-based LSTM (Wan et al., [2018](#bib.bib158))
    and tree-based Seq2seq model (Chakraborty et al., [2020](#bib.bib34)). The Tree-LSTM
    is one of the most often modified methods in models of code understanding. Tree-LSTM
    (Tai et al., [2015](#bib.bib148)) is a generalization of the standard LSTM for
    tree structures that composes the state from an input vector and hidden states
    of arbitrarily multiple children units. However, when the tree has ordered nodes,
    such as AST, the original Tree-LSTM is unable to manage the circumstance. To address
    the problem, a Multi-way Tree-LSTM model is developed, which extends the Tree-LSTM
    model. The model captures the interaction information between nodes by adding
    bidirectional LSTM to each gate before linear transformation to encapsulate the
    information of ordered children. In the code summarization task, the Multi-way
    Tree-LSTM learns the information in ASTs more effectively than a sequence model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法倾向于在 AST 结构的基础上修改递归神经网络或 Seq2seq 模型，例如基于 AST 的 LSTM（Wan 等，[2018](#bib.bib158)）和基于树的
    Seq2seq 模型（Chakraborty 等，[2020](#bib.bib34)）。Tree-LSTM 是代码理解模型中最常被修改的方法之一。Tree-LSTM（Tai
    等，[2015](#bib.bib148)）是标准 LSTM 在树结构上的一种推广，它通过输入向量和任意多个子单元的隐藏状态来组合状态。然而，当树具有有序节点（如
    AST）时，原始的 Tree-LSTM 无法处理这种情况。为了解决这个问题，开发了一种 Multi-way Tree-LSTM 模型，它扩展了 Tree-LSTM
    模型。该模型通过在每个门之前添加双向 LSTM 来捕捉节点之间的交互信息，从而封装有序子节点的信息。在代码摘要任务中，Multi-way Tree-LSTM
    比序列模型更有效地学习 AST 中的信息。
- en: Convolutional Neural Network is able to train easier and requires less time
    with the parallel computing mechanism compared to Recursive Neural Networks. Meanwhile,
    the adaptations to the vanilla CNN, such as tree-based CNN, have demonstrated
    their efficacy on code comprehensive tasks. Mou et al. (Mou et al., [2014](#bib.bib119))
    propose a Tree-Based Convolutional Neural Network (TBCNN) for the program classification
    task. The convolution layer can capture the information from AST. Chen et al.
    (Chen et al., [2019b](#bib.bib36)) propose a tree-based LSTM over API-enhanced
    AST for clone detection. The original AST is modified by adding a new node type
    to identify the API name. The model called TBCAA learns the representation of
    code using tree-based CNN, where each convolution kernel has a triangle shape.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 相比递归神经网络，卷积神经网络（CNN）能够更容易地进行训练，并且在并行计算机制下需要的时间更少。同时，对原始 CNN 的适配，如基于树的 CNN，已在代码综合任务中证明了其有效性。Mou
    等（Mou 等，[2014](#bib.bib119)）提出了一种用于程序分类任务的基于树的卷积神经网络（TBCNN）。卷积层可以捕捉来自 AST 的信息。Chen
    等（Chen 等，[2019b](#bib.bib36)）提出了一种基于树的 LSTM，用于 API 增强的 AST 进行克隆检测。通过添加新的节点类型来标识
    API 名称，对原始 AST 进行了修改。该模型称为 TBCAA，使用基于树的 CNN 学习代码的表示，其中每个卷积核具有三角形的形状。
- en: 4.2.2\. Graph perspective
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 图视角
- en: To encode the AST structure, most methods consider the structure as a graph
    and use a graph neural network. Two types of GNN are the most used model in the
    following graph-based models of AST, which are the Gated Graph Neural Networks(GGNN)
    and the Convolutional Graph Neural Networks. We also introduce other graph neural
    networks such as Graph Attention Networks that are used particularly for the AST
    structures.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编码 AST 结构，大多数方法将其视为图，并使用图神经网络。两种类型的 GNN 是以下基于图的 AST 模型中最常用的模型，分别是门控图神经网络（GGNN）和卷积图神经网络。我们还介绍了其他图神经网络，如图注意力网络，这些网络特别用于
    AST 结构。
- en: Gated Graph Neural Network
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 门控图神经网络
- en: Gated Graph Neural Network, which is a kind of Recurrent Graph Neural Networks,
    is the first developed graph neural network for code-related tasks when Li et
    al.(Li et al., [2015](#bib.bib99)) proposed Gated Graph Neural Network to infer
    formulas for program verification. GGNN updates the state of a node with a GRU
    cell, with the information from neighboring nodes and the node state in previous
    timestamp. Fernandes et al. (Fernandes et al., [2019](#bib.bib53)) modify GGNN
    to a framework to extend existing sequence encoders and conduct the experiments
    on three summarization tasks. Graph-based Grammar Fix (GGF) (Wu et al., [2020](#bib.bib173))
    use a mixture of GRU and GGNN as an encoder to encode the sub-AST (created by
    the erroneous code) and a token replacement mechanism as a decoder to generate
    the code fixing actions. Graph2diff(Tarlow et al., [2020b](#bib.bib150)) uses
    an encoder-decoder framework to predict the diff, also described as edit operations.
    The model takes the source code, bug configures files and compiler diagnostic
    messages as the graph input. The GGNN is used in the encoder stage to explicit
    the structure information of code. Although GGNN has shown its ability to learn
    the syntactic information from the AST of the graph, as a kind of Graph Neural
    network, it can only aggregate local information rather than global information.
    Therefore, some works also combine it with sequential models, such as Graph Sandwiches
    (Hellendoorn et al., [2019](#bib.bib63)). Furthermore, since GGNN employs RNN,
    it requires more memory to retain the hidden state of all nodes in the graph,
    despite the fact that it eliminates the need to constrain parameters to ensure
    convergence.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 门控图神经网络（Gated Graph Neural Network），作为一种递归图神经网络，是李等人（Li et al.，[2015](#bib.bib99)）在提出门控图神经网络用于推断程序验证公式时开发的第一个用于代码相关任务的图神经网络。GGNN
    使用 GRU 单元来更新节点的状态，结合来自邻近节点的信息和前一时间戳的节点状态。Fernandes 等人（Fernandes et al.，[2019](#bib.bib53)）将
    GGNN 修改为一个框架，以扩展现有的序列编码器，并在三个摘要任务上进行了实验。图基语法修正（Graph-based Grammar Fix，GGF）（Wu
    et al.，[2020](#bib.bib173)）使用 GRU 和 GGNN 的混合体作为编码器来编码由错误代码创建的子抽象语法树（sub-AST），并使用令牌替换机制作为解码器生成代码修复操作。Graph2diff（Tarlow
    et al.，[2020b](#bib.bib150)）使用编码器-解码器框架来预测差异，也称为编辑操作。该模型将源代码、错误配置文件和编译器诊断消息作为图输入。在编码器阶段使用
    GGNN 来显式表示代码的结构信息。尽管 GGNN 展示了从图的抽象语法树中学习语法信息的能力，但作为一种图神经网络，它只能聚合局部信息而非全局信息。因此，一些工作还将其与序列模型结合使用，例如
    Graph Sandwiches（Hellendoorn et al.，[2019](#bib.bib63)）。此外，由于 GGNN 使用 RNN，它需要更多的内存来保留图中所有节点的隐藏状态，尽管它消除了需要限制参数以确保收敛的需求。
- en: Convolutional Graph Neural Network
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积图神经网络
- en: Convolutional Graph Neural Network(convGNN) learns the representations of nodes
    based on the node vector and the neighbors of the node in the graph. The information
    from neighbors is combined through the aggregation process. Compare to Recurrent
    Graph Neural Networks such as GGNN, Convolutional Graph Neural Networks can stack
    multiple graph convolutional layers to better propagate the information across
    nodes, which can combine the information from neighbors. LeClair et al.(LeClair
    et al., [2020](#bib.bib89)) use convolution GNN to encode the AST nodes and edges
    for code summarization. The ConvGNN layer’s input is the AST token embedding and
    edges. after the ConvGNN layer, the model uses an attention mechanism to learn
    the important tokens in the source code and AST. Ji et al.(Ji et al., [2021b](#bib.bib75))
    also use GCN to encode AST for the code clone task. Liu et al.(Liu et al., [2021c](#bib.bib113))
    propose a task of code documentation generation for Jupyter notebooks. When generating
    documentation, the model HAConvGNN considers the relevant code cells and code
    token information. Convolutional GNN layers and a GRU layer are included in the
    encoder for code cells’ AST. The output of the GRU layer will be the input of
    a hierarchical attention mechanism to better preserve the graph structure.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积图神经网络（convGNN）基于节点向量和图中节点的邻居学习节点的表示。从邻居处获得的信息通过聚合过程进行结合。与如 GGNN 这样的递归图神经网络相比，卷积图神经网络可以堆叠多个图卷积层，以更好地在节点之间传播信息，从而结合来自邻居的信息。LeClair
    等（LeClair 等，[2020](#bib.bib89)）使用卷积 GNN 对 AST 节点和边进行编码，以进行代码摘要。ConvGNN 层的输入是 AST
    标记嵌入和边。经过 ConvGNN 层后，模型使用注意力机制来学习源代码和 AST 中的重要标记。Ji 等（Ji 等，[2021b](#bib.bib75)）也使用
    GCN 对 AST 进行编码以执行代码克隆任务。Liu 等（Liu 等，[2021c](#bib.bib113)）提出了一个 Jupyter 笔记本的代码文档生成任务。在生成文档时，模型
    HAConvGNN 考虑了相关的代码单元和代码标记信息。编码器中包含了卷积 GNN 层和 GRU 层，用于代码单元的 AST。GRU 层的输出将作为层次注意力机制的输入，以更好地保留图结构。
- en: Other Graph Neural Networks
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他图神经网络
- en: There are some methods using other Graph Neural Networks. Different from GCN,
    Graph Attention Network(GAT) (Veličković et al., [2018](#bib.bib155)) performs
    self-attention mechanism on message passing stage to learn the graph representation.
    Wang et al. (Wang and Li, [2021](#bib.bib166)) models the flattened sequence of
    a partial AST as an AST graph. To reduce the information loss, the parent-child
    relation and the positional information are recorded. Further, three types of
    AST Graph Attention Blocks are proposed to capture the structural information
    for learning the representation of the graph. Compare to GCN and GGNN, the GAT
    model improves the explainability of code completion or code summary tasks due
    to the utilization of the attention mechanism. Hoppity(Dinella et al., [2020](#bib.bib46))
    uses another type of GNN, which is Graph Isomorphism Network(GIN), as external
    memory to encode the AST of a buggy program, further using a central controller
    implemented by LSTM to predict the sequence of actions to fix bugs. The controller
    will expand or decline the memory when the graph structure is changed.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法使用其他图神经网络。不同于 GCN，图注意力网络（GAT）（Veličković 等，[2018](#bib.bib155)）在消息传递阶段执行自注意力机制以学习图表示。Wang
    等（Wang 和 Li，[2021](#bib.bib166)）将部分 AST 的展平序列建模为 AST 图。为了减少信息损失，记录了父子关系和位置相关信息。此外，提出了三种类型的
    AST 图注意力块，以捕捉结构信息以学习图的表示。与 GCN 和 GGNN 相比，GAT 模型通过利用注意力机制提高了代码补全或代码摘要任务的可解释性。Hoppity（Dinella
    等，[2020](#bib.bib46)）使用了另一种类型的 GNN，即图同构网络（GIN），作为外部存储器来编码有缺陷程序的 AST，进一步使用由 LSTM
    实现的中央控制器来预测修复错误的动作序列。当图结构发生变化时，控制器将扩展或缩减记忆。
- en: 4.3\. Flow-Graph-based Structures
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 基于流图的结构
- en: 'As previously introduced, flow graphs are separated into two types: control
    flow graphs (CFG) and data flow graphs (DFG). Although a flow graph is more likely
    to be seen as a graph, there are few works that treat flow graphs as trees. For
    example, BAST (Lin et al., [2021](#bib.bib104)) splits the code of a method according
    to the blocks in the dominator tree of CFG and generates the corresponding AST
    for the split code. The split ASTs’ representations are used in the pre-trained
    stage by predicting the next split AST in the dominator tree. The CFG is only
    used in splitting AST but can make the model more efficient and scalable for large
    programs.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，流图分为两种类型：控制流图（CFG）和数据流图（DFG）。虽然流图更可能被视为图，但很少有工作将流图视为树。例如，BAST (Lin et al.,
    [2021](#bib.bib104)) 根据CFG的支配树中的块拆分方法的代码，并为拆分的代码生成相应的AST。拆分AST的表示在预训练阶段用于预测支配树中的下一个拆分AST。CFG仅用于拆分AST，但可以使模型在处理大型程序时更高效和可扩展。
- en: The works that consider the CFG from a graph perspective and apply deep learning
    methods to represent the CFG are described in the following. The attributed Control
    Flow Graph (ACFG) is a common pre-processing step in some of the following works,
    especially in binary code similarity detection, such as Genius(Feng et al., [2016a](#bib.bib50)),
    Gemini (Xu et al., [2017](#bib.bib179)) and BugGraph (Ji et al., [2021a](#bib.bib76)).
    There are also some typical modifications for CFG, for example, lazy-binding CFG
    (Nguyen et al., [2018](#bib.bib122)), inter-procedural CFGs(ICFG) (Duan et al.,
    [2020](#bib.bib48)).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从图的角度考虑CFG并应用深度学习方法来表示CFG的工作如下所述。归因控制流图（ACFG）是以下一些工作中的常见预处理步骤，特别是在二进制代码相似性检测中，例如
    Genius(Feng et al., [2016a](#bib.bib50))、Gemini (Xu et al., [2017](#bib.bib179))
    和 BugGraph (Ji et al., [2021a](#bib.bib76))。还有一些典型的CFG修改，例如，懒绑定CFG (Nguyen et
    al., [2018](#bib.bib122))、过程间CFG (ICFG) (Duan et al., [2020](#bib.bib48))。
- en: Convolutional Neural Network
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional neural network (CNN) has translation invariance in many training
    data, therefore, it can not only capture the semantic information of code but
    is also suitable for order-aware modeling. Nguyen et al. (Nguyen et al., [2018](#bib.bib122))
    use CNN on the adjacency matrices converted by the lazy-binding CFG. The CFG will
    be converted into a pixel image, and later with a CNN model to recognize whether
    the target object is appears in the image. The method can be applied for malware
    detection. Yu et al. (Yu et al., [2020](#bib.bib191)) use Bert and CNN to learn
    CFG graph embedding, which can include semantic, structural, and order information.
    During the adjacent node prediction task, the Bert model is used to pre-train
    tokens and block embeddings. The order information of CFGs is extracted using
    CNN models. As indicated previously, CNN models are employed as a component for
    learning order information or for downstream tasks rather than directly for constructing
    the representation of the graph of code data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）在许多训练数据中具有平移不变性，因此，它不仅可以捕捉代码的语义信息，还适用于顺序感知建模。Nguyen et al. (Nguyen
    et al., [2018](#bib.bib122)) 使用CNN对由懒绑定CFG转换的邻接矩阵进行操作。CFG将被转换为像素图像，然后使用CNN模型识别目标对象是否出现在图像中。该方法可以应用于恶意软件检测。Yu
    et al. (Yu et al., [2020](#bib.bib191)) 使用Bert和CNN学习CFG图嵌入，可以包括语义、结构和顺序信息。在邻接节点预测任务中，Bert模型用于预训练令牌和块嵌入。CFG的顺序信息使用CNN模型提取。如前所述，CNN模型被作为学习顺序信息或下游任务的组件使用，而不是直接用于构建代码数据图的表示。
- en: Convolutional Graph Neural Network
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 卷积图神经网络
- en: DGCNN(Zhang et al., [2018](#bib.bib196)), as one of the ConvGNN, is proposed
    with a similar pooling strategy SortPooling. The approaches can allow attributed
    information to be aggregated quickly through neighborhood message passing, therefore,
    it is suitable for embedding structural information into vectors for further classification.
    To solve the malware classification challenge, Yan et al.(Yan et al., [2019](#bib.bib182))
    use DGCNN to embed CFGs. The CFG will first be converted to an attributed CFG,
    with the code characteristics defining the attributes. The DGCNN is used to aggregate
    these attributes with an adaptive max pooling to concatenate the layer outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: DGCNN（Zhang et al., [2018](#bib.bib196)），作为ConvGNN之一，提出了一种类似于SortPooling的池化策略。这些方法允许通过邻域消息传递快速聚合属性信息，因此，它适合将结构信息嵌入向量以进行进一步的分类。为了应对恶意软件分类挑战，Yan等人（Yan
    et al., [2019](#bib.bib182)）使用DGCNN嵌入CFG。CFG首先将被转换为带有属性的CFG，代码特征定义了这些属性。DGCNN用于使用自适应最大池化来聚合这些属性，以连接层输出。
- en: Graph Attention Network
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图注意力网络
- en: 'Li et al.(Li et al., [2020](#bib.bib92)) propose an event-based method CSEM
    for clone detection. GAT extracts the context information for each statement,
    which is modeled by the events that are embedded to capture execution semantics.
    BugGraph(Ji et al., [2021a](#bib.bib76)) compares the source-binary code similarity
    in two steps: source binary canonicalization and code similarity computation.
    In the code similarity computation step, BugGraph computes the similarity between
    the target and the comparing binary code. After disassembling both codes, each
    function will construct its ACFG and use GAT with the triplet loss as the output
    of the model to generate the embedding of each graph.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人（Li et al., [2020](#bib.bib92)）提出了一种基于事件的方法CSEM用于克隆检测。GAT提取每个语句的上下文信息，这些信息通过嵌入的事件建模以捕获执行语义。BugGraph（Ji
    et al., [2021a](#bib.bib76)）在两个步骤中比较源代码和二进制代码的相似性：源二进制规范化和代码相似性计算。在代码相似性计算步骤中，BugGraph计算目标代码和比较二进制代码之间的相似性。在对两个代码进行反汇编后，每个函数将构建其ACFG，并使用GAT与三元组损失作为模型输出，生成每个图的嵌入。
- en: Other Graph Neural Networks
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他图神经网络
- en: As previously mentioned, Yu et al. (Yu et al., [2020](#bib.bib191)) also uses
    MPNN(Gilmer et al., [2017](#bib.bib57)) to compute the graph embedding of a control-flow
    graph in order-aware modeling. BugLab (Allamanis et al., [2021](#bib.bib7)) uses
    standard message-passing graph neural networks to represent the graph of code
    entities. The graph includes syntactic entities and relations about the intraprocedural
    data and control flow, types, and documentation. BugLab trains two models, a selector
    model and a detector model to predict the rewrite on code snippet. The select
    model introduces buggy code and the detector model detects and repairs the bugs.
    Wang et al.(Wang et al., [2020b](#bib.bib167)) propose a new graph neural architecture
    called Graph Interval Neural Network(GINN) to learn the code embeddings. GINN
    takes the program’s control flow graph as input and abstracts it using three operators.
    The CFG is partitioned into a series of intervals using the partitioning operator.
    Messages between intervals passing are restricted. Then the heightening operator
    is applied to replace each activate interval with single created nodes until the
    sufficient propagation point is reached. The lowering operator will finally recover
    the original CFG. GINN model use only looping construct to learn feature representation,
    and the method shows improvement across program embeddings.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Yu等人（Yu et al., [2020](#bib.bib191)）也使用MPNN（Gilmer et al., [2017](#bib.bib57)）来计算控制流图的图嵌入，以进行有序建模。BugLab（Allamanis
    et al., [2021](#bib.bib7)）使用标准消息传递图神经网络来表示代码实体的图。该图包括关于程序内部数据和控制流、类型以及文档的语法实体和关系。BugLab训练两个模型，一个选择模型和一个检测模型，用于预测代码片段的重写。选择模型引入有缺陷的代码，而检测模型则检测并修复这些缺陷。Wang等人（Wang
    et al., [2020b](#bib.bib167)）提出了一种新的图神经架构，称为图间隔神经网络（GINN），用于学习代码嵌入。GINN以程序的控制流图为输入，并使用三个操作符对其进行抽象。使用分区操作符将CFG划分为一系列间隔。间隔之间传递的消息受到限制。然后应用提升操作符，将每个激活的间隔替换为单个创建的节点，直到达到足够的传播点。最后，降低操作符将恢复原始的CFG。GINN模型仅使用循环结构来学习特征表示，并且该方法在程序嵌入上表现出改进。
- en: 4.4\. Combined Structures
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 组合结构
- en: In some cases, the two types of flow graphs do not appear separately as well
    as AST. The following part will focus on how these structures are combined and
    what information can be retrieved from them.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，两种类型的流图与 AST 可能不会单独出现。以下部分将重点探讨这些结构如何组合以及可以从中获取哪些信息。
- en: Control Flow and Data Flow
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制流与数据流
- en: 'For the combination of two types of graphs, some novel value graph is presented
    such as the program’s Interprocedural Value-Flow Graph (IVFG). The IVFG is created
    using LLVM-IR, which combines code control-flow and alias-aware data-flows. IVFG
    is a directed graph with multiple edges. Flow2Vec(Sui et al., [2020](#bib.bib144))
    is a novel approach for embedding the code with IVFG. Pre-embedding, value-flow
    reachability via matrix multiplication, and high order proximity approximation
    are the three steps in the method. Brauckmann et al.(Brauckmann et al., [2020](#bib.bib29))
    use AST or control-data flow graphs (CDFGs) as the input of the predictive model
    to learn the code representation. The core of the predictive model is the GGNN
    in the embedding propagation layer. The model has shown its effectiveness in two
    complex tasks on OpenCL kernels, the CPU/GPU mapping, and Thread Coarsening. Deepsim
    (Zhao and Huang, [2018](#bib.bib197)) encodes both code control flow graph and
    data flow graph into a semantic matrix and uses a deep learning model to measure
    function similarity. The semantic matrix contains three features: variable features,
    basic block features, and relationship features between variables and basic blocks.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两种图的组合，提出了一些新颖的值图，例如程序的跨过程值流图（IVFG）。IVFG 是使用 LLVM-IR 创建的，它结合了代码控制流和别名感知的数据流。IVFG
    是一个具有多条边的有向图。Flow2Vec（Sui 等，[2020](#bib.bib144)）是一种将代码与 IVFG 嵌入的创新方法。该方法包括三个步骤：预嵌入、通过矩阵乘法的值流可达性以及高阶接近度近似。Brauckmann
    等人（Brauckmann 等，[2020](#bib.bib29)）使用 AST 或控制数据流图（CDFGs）作为预测模型的输入以学习代码表示。预测模型的核心是嵌入传播层中的
    GGNN。该模型在 OpenCL 内核的两个复杂任务上显示了其有效性，包括 CPU/GPU 映射和线程粗化。Deepsim（Zhao 和 Huang，[2018](#bib.bib197)）将代码控制流图和数据流图编码成语义矩阵，并使用深度学习模型来衡量函数相似性。语义矩阵包含三个特征：变量特征、基本块特征以及变量和基本块之间的关系特征。
- en: AST and Flow Graphs
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: AST 和流图
- en: Allamanis et al. (Allamanis et al., [2018b](#bib.bib6)) propose a method based
    on GGNN to construct graphs for source code, naming variables, and detecting variable
    misuse. The program graph combines both AST and the data flow graph, which contains
    both syntactic and semantic structure information of source code. Some works combine
    AST and CFG for multi-modal learning for generating the hybrid representation
    of code, such as (Wan et al., [2019a](#bib.bib156)). Devign (Zhou et al., [2019](#bib.bib198))
    utilizes GGNN with a Conv module for graph-level classification. The graph representation
    is based on AST structure and added multiple types of edges from CFG, DFG, and
    NCS. The Conv module learns the hierarchy information of representation of node
    features. Multi-modal learning can reduce the limitation of the approaches only
    using AST to represent the code. The combination of AST and Flow Graphs can cover
    extra semantic information of source code.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Allamanis 等人（Allamanis 等，[2018b](#bib.bib6)）提出了一种基于 GGNN 的方法，用于构建源代码图、命名变量和检测变量误用。程序图结合了
    AST 和数据流图，包含源代码的语法和语义结构信息。一些工作将 AST 和 CFG 结合用于多模态学习，以生成代码的混合表示，例如（Wan 等，[2019a](#bib.bib156)）。Devign（Zhou
    等，[2019](#bib.bib198)）利用 GGNN 和 Conv 模块进行图级分类。图表示基于 AST 结构，并添加了来自 CFG、DFG 和 NCS
    的多种边。Conv 模块学习节点特征的层次信息。多模态学习可以减少仅使用 AST 表示代码的限制。AST 和流图的组合可以涵盖源代码的额外语义信息。
- en: 4.5\. Other Structures
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 其他结构
- en: This section will discuss other structures that differ significantly from the
    AST and flow graphs. Although the AST and flow-graphs have been employed in most
    previous works, a program project still contains other graphs. Some are traditional
    graphs, such as the Program Dependency Graph and UML Diagrams presented in [2.1.5](#S2.SS1.SSS5
    "2.1.5\. Other Structures ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), while others are
    recently proposed structures for better semantic-aware or structure-aware code
    understanding.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论与AST和流图显著不同的其他结构。尽管AST和流图已在大多数先前工作中使用，但一个程序项目仍包含其他图形。一些是传统图形，如第[2.1.5](#S2.SS1.SSS5
    "2.1.5\. 其他结构 ‣ 2.1\. 代码中的结构 ‣ 2\. 初步 ‣ 结构代码理解深度学习模型综述")节中介绍的程序依赖图和UML图，而其他则是最近提出的结构，旨在实现更好的语义感知或结构感知代码理解。
- en: Program Dependence Graph
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 程序依赖图
- en: Li et al.(Li et al., [2019b](#bib.bib100)) propose an attention-based neural
    network to learn code representation for the bug detection task. The global context
    is extracted by the Program Dependence Graph and Data Flow Graph using Node2Vec,
    while the local context is extracted by previous bug fixes and AST paths. The
    global context and local context will be unified as the path vector for further
    analysis.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人（Li et al.，[2019b](#bib.bib100)）提出了一种基于注意力的神经网络来学习用于错误检测任务的代码表示。全局上下文通过程序依赖图和数据流图使用Node2Vec提取，而局部上下文通过之前的错误修复和AST路径提取。全局上下文和局部上下文将统一为路径向量进行进一步分析。
- en: UML Diagrams
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: UML图
- en: CoCoSUM(Wang et al., [2021a](#bib.bib165)) model the UML diagrams for code summarization
    task. The framework encodes the class names with a transformer-based model as
    the intra-class context and the UML diagrams with a Multi-Relational Graph Neural
    Network (MRGNN) as the inter-class context. The two kinds of embeddings together
    with the embeddings of the token and AST will be passed to an attention-based
    decoder to generate code summaries.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: CoCoSUM（Wang et al.，[2021a](#bib.bib165)）对代码总结任务进行了UML图建模。该框架使用基于变换器的模型对类名进行编码作为类内上下文，并使用多关系图神经网络（MRGNN）对UML图进行编码作为类间上下文。这两种嵌入与标记和AST的嵌入一起传递给基于注意力的解码器，以生成代码摘要。
- en: Code Property Graph
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码属性图
- en: 'Yamaguchi et al.(Yamaguchi et al., [2014](#bib.bib181)) model three types of
    code-related structures: ASTs, CFGs, and Program Dependency Graph (Ferrante et al.,
    [1987b](#bib.bib55)) as property graphs and combine them into code property graph.
    The newly proposed data structure enables characterizing the vulnerability type
    through graph traversals. Liu et al.(Liu et al., [2020a](#bib.bib112)) use the
    code property graph and propose a Hybrid GNN framework in the code summarization
    task. The framework fuse the static graph and dynamic graph to capture the global
    information of graphs.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Yamaguchi等人（Yamaguchi et al.，[2014](#bib.bib181)）对三种代码相关结构进行了建模：AST、CFG和程序依赖图（Ferrante
    et al.，[1987b](#bib.bib55)），并将它们组合成代码属性图。新提出的数据结构能够通过图遍历来表征漏洞类型。Liu等人（Liu et al.，[2020a](#bib.bib112)）使用代码属性图，并在代码总结任务中提出了一种混合GNN框架。该框架融合了静态图和动态图，以捕捉图的全局信息。
- en: Program Feedback Graph
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 程序反馈图
- en: Yasunaga et al.(Yasunaga and Liang, [2020a](#bib.bib186)) introduce a program
    feedback graph to model the reasoning process in program repair task. The nodes
    in the program feedback graph consist of tokens in the diagnostic arguments, the
    occurrences in the source code, and the remaining identifiers in the code. The
    framework DrRepair uses LSTM to encode the source code initially and GAT to further
    reason over the graph.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Yasunaga等人（Yasunaga和Liang，[2020a](#bib.bib186)）引入了一种程序反馈图来建模程序修复任务中的推理过程。程序反馈图中的节点包括诊断参数中的标记、源代码中的出现位置以及代码中的剩余标识符。框架DrRepair最初使用LSTM对源代码进行编码，然后使用GAT进一步对图进行推理。
- en: 5\. Discussion and Comparison
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 讨论与比较
- en: 'We have introduced two types of models in code understanding: sequence-based
    and graph-based models in section [3](#S3 "3\. Sequence-based Models ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") and Section [4](#S4
    "4\. Graph-based Models ‣ A Survey of Deep Learning Models for Structural Code
    Understanding") respectively.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码理解中介绍了两种模型：基于序列的模型和基于图的模型，分别在第[3](#S3 "3\. 基于序列的模型 ‣ 结构代码理解深度学习模型综述")节和第[4](#S4
    "4\. 基于图的模型 ‣ 结构代码理解深度学习模型综述")节中进行了讨论。
- en: Sequence-based models are models for processing code sequences such as NCS and
    flatten sequences obtained from AST by several transformation methods introduced
    in Section [3.1](#S3.SS1 "3.1\. Structure Transformation ‣ 3\. Sequence-based
    Models ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    Traditional statistical language models such as N-gram models are used in a large
    amount of early works. With the development of deep learning, word2vec and DBN
    models have also been used for code representation and downstream tasks. Furthermore,
    CNN which has revolutionized the computer vision field, can effectively capture
    rich structural patterns in sequences and are therefore naturally adopted for
    code sequence tasks. However, the most dominant models among sequence-based models
    are vanilla RNN models with their variants, such as LSTM, GRU, and Bi-LSTM, which
    are tailored for sequence modeling tasks. In recent years, transformer-based models
    that incorporate self-attention blocks have made major contributions to code sequence-based
    models.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 基于序列的模型是用于处理代码序列的模型，如NCS和通过第[3.1](#S3.SS1 "3.1\. 结构转换 ‣ 3\. 基于序列的模型 ‣ 深度学习模型在结构代码理解中的综述")节介绍的AST经过多种转换方法获得的扁平序列。传统的统计语言模型，如N-gram模型，在早期工作中被广泛使用。随着深度学习的发展，word2vec和DBN模型也被用于代码表示和下游任务。此外，CNN在计算机视觉领域引发了革命，能够有效捕捉序列中的丰富结构模式，因此自然地被用于代码序列任务。然而，在基于序列的模型中，最主流的模型是原始的RNN模型及其变体，如LSTM、GRU和Bi-LSTM，它们专门针对序列建模任务。近年来，结合自注意力块的transformer模型对基于序列的代码模型做出了重大贡献。
- en: Graph-based models are models that use the graph structures generated by codes
    such as AST and Flow Graphs to capture structural information. The transformation
    of graph structures has also been introduced in Section [4.1](#S4.SS1 "4.1\. Structure
    Transformation ‣ 4\. Graph-based Models ‣ A Survey of Deep Learning Models for
    Structural Code Understanding"). Some of these structures, especially AST can
    be seen as a tree or a graph, which leads to a different way to process the structures.
    From a tree perspective, these models use RNN models designed for tree structures,
    such as Tree-LSTM. From the perspective of the graph, CNN is also one of the most
    used models, while graph neural networks play an important role in the pipeline
    to learn the representation of nodes or graphs, such as GGNN, GCN, GAT, or MPNN
    with both AST and flow graphs. Above AST and Flow Graphs, there are also some
    combination structures and other rarely seen structures, for example, UML diagrams,
    Program Dependency Graph and so on used in graph-based models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的模型是利用代码生成的图结构，如AST和流图，来捕捉结构信息的模型。图结构的转换也在第[4.1](#S4.SS1 "4.1\. 结构转换 ‣ 4\.
    基于图的模型 ‣ 深度学习模型在结构代码理解中的综述")节中介绍。部分这些结构，尤其是AST，可以被视为树或图，这导致了处理这些结构的不同方式。从树的角度来看，这些模型使用为树结构设计的RNN模型，如Tree-LSTM。从图的角度来看，CNN也是最常用的模型之一，而图神经网络在学习节点或图的表示方面发挥了重要作用，例如GGNN、GCN、GAT或MPNN，适用于AST和流图。在AST和流图之上，还有一些组合结构和其他罕见结构，例如UML图、程序依赖图等，用于基于图的模型。
- en: 'The two categories of models have both shown the effectiveness on code representation
    and downstream tasks which we will introduce in Section [6](#S6 "6\. Tasks ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"). In this section,
    we emphasize three differences between the two types of models:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这两类模型在代码表示和下游任务上都显示了有效性，我们将在第[6](#S6 "6\. 任务 ‣ 深度学习模型在结构代码理解中的综述")节中介绍。在这一节中，我们强调两种模型之间的三个差异：
- en: First, sequence-based models and graph-based models view data in a different
    perspective. Because sequence-based models treat code as a collection of sequences,
    they must properly capture the relationships between sequences that correspond
    to semantic and syntactic information in the source code. Graph-based models view
    code data as a tree or a graph. Since nodes and edges in code graphs are rich
    in structural information, these models learn the representation in graphs to
    better understand the code. However, the models that use only sequential data
    are neglecting the syntactic or semantic information in structures, while the
    models learn solely on graph structures ignore the sequential information. As
    previously introduced, the boundary of the two kinds of models is not clear if
    we divide them with the information they used. There are sequences flattened from
    the AST structure that automatically combine the structural information in AST.
    In Graph-based models, some methods also use sequential information in code.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基于序列的模型和基于图的模型从不同的角度来看待数据。由于基于序列的模型将代码视为一系列序列，它们必须准确捕捉序列之间对应于源代码中的语义和句法信息的关系。基于图的模型将代码数据视为树或图。由于代码图中的节点和边富含结构信息，这些模型通过学习图中的表示来更好地理解代码。然而，仅使用序列数据的模型忽略了结构中的句法或语义信息，而仅依赖图结构的模型则忽略了序列信息。如前所述，如果按照使用的信息来划分这两种模型的边界并不明确。AST
    结构中展开的序列自动结合了 AST 中的结构信息。在基于图的模型中，一些方法也使用了代码中的序列信息。
- en: Second, sequence-based and graph-based models are both use serial models but
    with distinct proposes and scenarios. RNN and Transformer are the most commonly
    used serial models in deep learning models in code understanding, but the two
    types of models use RNN and Transformer with different proposes. Sequence-based
    models use RNN and Transformer as the core component of the models to learn relationships
    in code sequences. The RNN(RaychevVeselin et al., [2014](#bib.bib133); Ben-Nun
    et al., [2018a](#bib.bib24)) is used in the sequence-based models to tackle the
    problem of information passing in the code sequence. LSTM(Dam et al., [2016](#bib.bib43);
    Iyer et al., [2016](#bib.bib73)) and GRU(LeClair et al., [2019](#bib.bib90); Hu
    et al., [2020a](#bib.bib69)) which are the variant of RNN use the gating mechanism
    to better transfer useful information and solve the problem of gradient explosion
    and disappearance in code sequences. Transformer-based models(Ahmad et al., [2020](#bib.bib3);
    Alon et al., [2020](#bib.bib11)) are more commonly employed to address the issue
    of global reliance and use positional embedding to maintain the order information
    of code sequence, allowing for better learning of the entire code information.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，基于序列的模型和基于图的模型都使用序列模型，但它们的目的和应用场景不同。RNN 和 Transformer 是深度学习模型中用于代码理解的最常用的序列模型，但这两种模型在目的上有所不同。基于序列的模型将
    RNN 和 Transformer 作为模型的核心组件来学习代码序列中的关系。RNN（RaychevVeselin 等，[2014](#bib.bib133)；Ben-Nun
    等，[2018a](#bib.bib24)）在基于序列的模型中用于解决代码序列中信息传递的问题。LSTM（Dam 等，[2016](#bib.bib43)；Iyer
    等，[2016](#bib.bib73)）和 GRU（LeClair 等，[2019](#bib.bib90)；Hu 等，[2020a](#bib.bib69)）是
    RNN 的变体，使用门控机制来更好地传递有用信息，并解决代码序列中的梯度爆炸和消失问题。基于 Transformer 的模型（Ahmad 等，[2020](#bib.bib3)；Alon
    等，[2020](#bib.bib11)）更常用于解决全局依赖问题，并使用位置嵌入来保持代码序列的顺序信息，从而更好地学习整个代码信息。
- en: 'Although RNN-based model designed for serial data, they are widely used in
    graph-based models as sequences are the original form of code that contains the
    context information. Furthermore, although Graph Neural Network can capture the
    structural information of structures, the context information cannot be preserved
    once the source code is converted to a graph structure. Therefore, approaches
    in graph-based models will also use RNN such as LSTM or GRU to gather long-distance
    information. Some works use RNN for tree structures such as RvNN(Shi et al., [2021](#bib.bib139)),
    or tree-LSTM(Wan et al., [2019a](#bib.bib156)). Most of the works combined the
    RNN with GNN as the whole model frameworks, mainly categorized as the following
    three usages: 1) Conducting bidirectional GRU(Zhang et al., [2019a](#bib.bib194))
    or bidirectional LSTM as the encoder of the models, 2) using GRU to fuse resulting
    vectors after GNN components(Liu et al., [2020a](#bib.bib112)), 3) using RNN such
    as LSTM as decoder(Wang et al., [2020b](#bib.bib167)). Graph Sandwich Structure(Hellendoorn
    et al., [2019](#bib.bib63)) is one of the most typical works that combine GNN
    with RNN and Transformer structure to combine both local and global information.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RNN基于序列数据设计，但它们在基于图的模型中也被广泛使用，因为序列是包含上下文信息的代码的原始形式。此外，尽管图神经网络可以捕获结构信息，但一旦源代码转换为图结构，上下文信息就无法保留。因此，基于图的模型中的方法还会使用RNN，如LSTM或GRU，以收集长距离信息。一些工作使用RNN处理树结构，如RvNN(Shi
    et al., [2021](#bib.bib139))，或tree-LSTM(Wan et al., [2019a](#bib.bib156))。大多数工作将RNN与GNN结合作为整个模型框架，主要分为以下三种用途：1)
    作为模型的编码器使用双向GRU(Zhang et al., [2019a](#bib.bib194))或双向LSTM，2) 使用GRU融合GNN组件后的结果向量(Liu
    et al., [2020a](#bib.bib112))，3) 使用RNN如LSTM作为解码器(Wang et al., [2020b](#bib.bib167))。Graph
    Sandwich Structure(Hellendoorn et al., [2019](#bib.bib63))是结合GNN与RNN和Transformer结构以融合局部和全局信息的典型工作之一。
- en: Third, Attention mechanism are used in sequence-based and graph-based models
    but with various modifications.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，注意力机制在基于序列的模型和基于图的模型中都有使用，但有所不同。
- en: The attention mechanism is widely used in neural machine translation, especially
    in encoder-decoder frameworks. The attention mechanism can learn which words are
    important and further make predictions according to the importance of words. In
    code representation-related works, attention mechanisms are conducted in both
    sequence-based models and graph-based models.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在神经机器翻译中被广泛使用，特别是在编码器-解码器框架中。注意力机制可以学习哪些词重要，并根据词的重要性进行进一步的预测。在代码表示相关的工作中，注意力机制在基于序列的模型和基于图的模型中都有应用。
- en: In sequence-based models, the attention mechanism is primarily used to pay attention
    to the relationships between tokens in code sequences, including the relationships
    between input sequence tokens, between output sequence tokens and input sequence
    tokens, or between output sequence tokens, in order to facilitate more efficient
    information transmission. The Transformer-based models use self-attention to focus
    on the three relationships mentioned above while the other models pay more attention
    to the relationship between output sequence tokens and input sequence tokens.
    Attention mechanisms are added to RNN-based models by CodeNN(Iyer et al., [2016](#bib.bib73))
    and other models, allowing the model to make greater use of crucial token information
    while creating code summary or predicting the next token.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于序列的模型中，注意力机制主要用于关注代码序列中标记之间的关系，包括输入序列标记之间的关系、输出序列标记与输入序列标记之间的关系，或者输出序列标记之间的关系，以促进更高效的信息传递。基于Transformer的模型使用自注意力机制来关注上述三种关系，而其他模型则更多关注输出序列标记与输入序列标记之间的关系。注意力机制被CodeNN(Iyer
    et al., [2016](#bib.bib73))和其他模型添加到基于RNN的模型中，使模型在生成代码摘要或预测下一个标记时能更充分地利用关键标记信息。
- en: Some work in graph-based models conduct attention mechanism before output layer
    and create a context vector to predict the next token in the sequence, such as
    LeClair et al.(LeClair et al., [2020](#bib.bib89)). Others make modifications
    on vanilla deep learning models with attention mechanism, for example, using attention
    mechanism in message propagation process of GCN to hierarchically update the embeddings(Ji
    et al., [2021b](#bib.bib75)), combining attention mechanism in GRU layer and Convolutional
    layer to encode the order of the nodes in a path (Li et al., [2019b](#bib.bib100)).
    Different from sequence-based models, some works modify the original attention
    mechanism to better suit the hierarchical structure. Liu et al(Liu et al., [2021c](#bib.bib113))
    propose low-level attention and high-level attention. The former attention module
    attends the token in a sequence while the latter attends the code cell in the
    AST tree.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些图基模型在输出层之前进行注意力机制处理，并创建一个上下文向量来预测序列中的下一个标记，例如 LeClair 等人 (LeClair et al.,
    [2020](#bib.bib89))。其他模型则对普通深度学习模型进行修改，添加注意力机制，例如，在 GCN 的消息传播过程中使用注意力机制来分层更新嵌入
    (Ji et al., [2021b](#bib.bib75))，将注意力机制与 GRU 层和卷积层结合，以对路径中的节点顺序进行编码 (Li et al.,
    [2019b](#bib.bib100))。与基于序列的模型不同，一些工作对原始注意力机制进行了修改，以更好地适应层次结构。Liu 等人 (Liu et al.,
    [2021c](#bib.bib113)) 提出了低级注意力和高级注意力。前者的注意力模块关注序列中的标记，而后者则关注 AST 树中的代码单元。
- en: 6\. Tasks
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 任务
- en: We categorized the tasks used in code understanding into the following downstream
    tasks and summarized the sequence-based models and graph-based models on each
    downstream tasks.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于代码理解的任务分类为以下下游任务，并总结了每个下游任务中的基于序列的模型和图基模型。
- en: 6.1\. Code Generation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 代码生成
- en: Code Generation can provide varying levels of granularity of code output depending
    on the input. Code prediction in IDE that anticipates blank areas of input code
    snippets, such as method name prediction, next token prediction, and so on, is
    a typical code generation task. It’s also a kind of code generation task to generate
    snippets of code from natural language. Code generation considerably enhances
    developers’ efficiency and has been extensively researched in both industry and
    academia. The relevant approaches are summarized in Table [2](#S6.T2 "Table 2
    ‣ 6.1.4\. Function generation ‣ 6.1\. Code Generation ‣ 6\. Tasks ‣ A Survey of
    Deep Learning Models for Structural Code Understanding"), which is based mostly
    on the structure used by the model in the code generation task.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成可以根据输入提供不同层次的代码输出。IDE 中的代码预测任务，例如方法名称预测、下一个标记预测等，是典型的代码生成任务。从自然语言生成代码片段也是一种代码生成任务。代码生成显著提升了开发者的效率，并在工业界和学术界得到了广泛研究。相关方法在表
    [2](#S6.T2 "Table 2 ‣ 6.1.4\. 函数生成 ‣ 6.1\. 代码生成 ‣ 6\. 任务 ‣ 深度学习模型在结构化代码理解中的应用综述")
    中进行了总结，该表主要基于模型在代码生成任务中的使用结构。
- en: 6.1.1\. Method name generation
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 方法名称生成
- en: 'This task generates a summary method name based on the given method code body,
    which can help the code be more understandable, maintainable, and invocable. The
    formalization of the method name generation task is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务根据给定的方法代码体生成一个概要方法名称，这有助于代码变得更加易于理解、维护和调用。方法名称生成任务的形式化如下：
- en: Given a code body of a method with n tokens $S={t_{1},t_{2},...t_{n}}$, the
    generative model can output the method name.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 n 个标记的代码体 $S={t_{1},t_{2},...t_{n}}$，生成模型可以输出方法名称。
- en: 6.1.2\. Next token generation
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 下一个标记生成
- en: 'This task predicts the tokens including API calls that will be inputted by
    the developer. The models of the next token generation typically provide a list
    of tokens in order of probability depending on the developer’s previous input.
    It can substantially speed up development. The formalization of the next token
    generation task is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务预测开发者将输入的包括 API 调用在内的标记。下一个标记生成的模型通常根据开发者之前的输入提供一个按概率排序的标记列表。它可以大幅提高开发速度。下一个标记生成任务的形式化如下：
- en: Given a partial code snippet with $n-1$ tokens $S={t_{1},t_{2},...t_{n-1}}$,
    the generative model can generate a token list of $t_{n}$ sorted by probability.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 $n-1$ 个标记的部分代码片段 $S={t_{1},t_{2},...t_{n-1}}$，生成模型可以生成一个按概率排序的 $t_{n}$
    标记列表。
- en: 6.1.3\. Expression generation
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 表达式生成
- en: 'Compared to the next token generation, this task is a more granular code prediction
    task based on existing code fragments. It generates complete code expressions
    with certain functions such as conditional statements, loops, etc. It also includes
    the task of predicting missing code statements. The formalization of the next
    expression generation task is as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 与下一个token生成相比，这个任务是基于现有代码片段的更细粒度的代码预测任务。它生成具有特定功能（如条件语句、循环等）的完整代码表达式。它还包括预测缺失代码语句的任务。下一个表达式生成任务的形式化如下：
- en: Given a partial code snippet with $n$ tokens $S={t_{1},t_{2},...,t_{n-1}}$,
    the generative model can generate a whole code expression $S_{m}={t_{n},t_{n+1}...,t_{n+m-1}}$
    with a specific function. Or given a code with a missing piece $S={t_{1},t_{2},...,missingpiece,...,t_{n}}$,
    the generative model generates the missing piece code.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含$n$个token的部分代码片段$S={t_{1},t_{2},...,t_{n-1}}$，生成模型可以生成一个具有特定功能的完整代码表达式$S_{m}={t_{n},t_{n+1}...,t_{n+m-1}}$。或者给定一个缺失部分的代码$S={t_{1},t_{2},...,missingpiece,...,t_{n}}$，生成模型生成缺失的代码部分。
- en: 6.1.4\. Function generation
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4\. 函数生成
- en: 'This task can generate corresponding code snippets based on the user’s description
    of the code’s functionality, which can be seen as a natural language to the code
    translation process. Excellent natural language to code translation techniques
    can enable non-specialists to develop accordingly, but work in this area is still
    immature and needs to be further developed. The formalization of the function
    generation task is as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务可以根据用户对代码功能的描述生成相应的代码片段，这可以看作是自然语言到代码的翻译过程。优秀的自然语言到代码翻译技术可以使非专业人员也能相应地进行开发，但这一领域的工作仍然不成熟，需要进一步发展。函数生成任务的形式化如下：
- en: Given a natural language $D$ for the functional description of the code $S$,
    the generative model can generate the $S={t_{1},t_{2},...,t_{n}}$ according to
    $D$.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 给定自然语言$D$对代码$S$的功能描述，生成模型可以根据$D$生成$S={t_{1},t_{2},...,t_{n}}$。
- en: Table 2\. Code Generation
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 代码生成
- en: '| Type | Reference | Model | Generation | Description |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 生成 | 描述 |'
- en: '| Sequence | Code2seq(Alon et al., [2018a](#bib.bib9)) | BiLSTM | method name
    | Represents code by a collection of paths between terminal nodes n the AST |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | Code2seq(Alon et al., [2018a](#bib.bib9)) | BiLSTM | method name | 通过AST中终端节点之间的一组路径表示代码
    |'
- en: '|  | Code2vec(Alon et al., [2019c](#bib.bib15)) | LSTM | method name | Generates
    code’s representation by AST path |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | Code2vec(Alon et al., [2019c](#bib.bib15)) | LSTM | method name | 通过AST路径生成代码的表示
    |'
- en: '|  | SLM(Alon et al., [2020](#bib.bib11)) | LSTM | expression | Generates missing
    code using all the paths to it |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | SLM(Alon et al., [2020](#bib.bib11)) | LSTM | expression | 使用所有路径生成缺失代码
    |'
- en: '|  | Pythia(Svyatkovskiy et al., [2019](#bib.bib147)) | LSTM | next token |
    Predicts the methods and API calls by flattened AST |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | Pythia(Svyatkovskiy et al., [2019](#bib.bib147)) | LSTM | next token |
    通过展平的AST预测方法和API调用 |'
- en: '|  | (Liu et al., [2017](#bib.bib108)) | LSTM | next token | Uses the sequence
    obtained by traversing the AST to predict the next possible node |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | (Liu et al., [2017](#bib.bib108)) | LSTM | next token | 使用通过遍历AST获得的序列来预测下一个可能的节点
    |'
- en: '|  | (Li et al., [2017b](#bib.bib94)) | LSTM, Pointer Network | next token
    | Generates code through LSTM or pointer network |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | (Li et al., [2017b](#bib.bib94)) | LSTM, Pointer Network | next token
    | 通过LSTM或指针网络生成代码 |'
- en: '|  | (Kim et al., [2020](#bib.bib83)) | Transformer | next token | Feeds different
    paths to Transformer |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | (Kim et al., [2020](#bib.bib83)) | Transformer | next token | 将不同的路径输入到Transformer中
    |'
- en: '|  | (Liu et al., [2020c](#bib.bib110)) | Transformer-XL | next token | Feeds
    different paths to Transformer-XL with multi-task learning |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | (Liu et al., [2020c](#bib.bib110)) | Transformer-XL | next token | 将不同的路径输入到具有多任务学习的Transformer-XL中
    |'
- en: '|  | (Alon et al., [2018c](#bib.bib13)) | Word2vec | method name | Feeds different
    paths with up/down momentum to Word2vec |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | (Alon et al., [2018c](#bib.bib13)) | Word2vec | method name | 将具有上/下动量的不同路径输入到Word2vec中
    |'
- en: '|  | API2Vec(Nguyen et al., [2017](#bib.bib123)) | Word2vec | next token |
    Feeds different API paths to Word2vec |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | API2Vec(Nguyen et al., [2017](#bib.bib123)) | Word2vec | next token |
    将不同的API路径输入到Word2vec中 |'
- en: '| Graph | DEEP3 (Raychev et al., [2016](#bib.bib132)) | Decision tree | next
    token | Represents code in a DSL called TGEN and build probabilistic models with
    decision trees |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 图 | DEEP3 (Raychev et al., [2016](#bib.bib132)) | 决策树 | next token | 使用名为TGEN的DSL表示代码，并利用决策树构建概率模型
    |'
- en: '|  | (Brockschmidt et al., [2019](#bib.bib30)) | GRU,GGNN | expression | A
    generated model on ExprGen task which first obtains a graph by attribute grammars
    and later compute the attribute representations with GGNN and GRU |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | (Brockschmidt et al., [2019](#bib.bib30)) | GRU,GGNN | 表达式 | 一个在ExprGen任务上生成的模型，首先通过属性文法获取图，然后用GGNN和GRU计算属性表示
    |'
- en: '|  | CCAG(Wang and Li, [2021](#bib.bib166)) | GAT | next token | Uses AST Graph
    Attention Block(ASTGab) to model the flattened sequence of AST to capture different
    dependencies |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | CCAG(Wang and Li, [2021](#bib.bib166)) | GAT | 下一个标记 | 使用AST图注意力块（ASTGab）来建模AST的展平序列，以捕获不同的依赖关系
    |'
- en: '|  | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | method name | Constructs
    graphs by adding different types of edges and use GGNN to learn the representation
    of the graph |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | 方法名称 | 通过添加不同类型的边构建图，并使用GGNN学习图的表示
    |'
- en: '|  | Graph-Structured Cache(Cvitkovic et al., [2019](#bib.bib41)) | MPNN,CharCNN
    | method name | Introduces a Graph-Structured Cache representing vocabulary words
    as additional nodes, uses MPNN and CharCNN to generate outputs to further address
    open vocabulary issue |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | Graph-Structured Cache(Cvitkovic et al., [2019](#bib.bib41)) | MPNN,CharCNN
    | 方法名称 | 引入了一个图结构缓存，将词汇表词汇表示为额外的节点，使用MPNN和CharCNN生成输出，以进一步解决开放词汇问题 |'
- en: 6.2\. Code Summarization
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 代码摘要
- en: Code summarization is the work of using natural language to provide a concise
    explanation of the code’s functioning. It can help enhance the code’s readability,
    as well as the developer’s efficiency in understanding the program. The code summarization
    task can be thought of as a translation of the input code into natural language,
    hence a seq2seq model architecture is commonly used. In the encoder phase, (Hu
    et al., [2018](#bib.bib68), [2020a](#bib.bib69); LeClair et al., [2019](#bib.bib90))
    convert the input NSC or Flattened sequence into a context vector, and then in
    the decoder phase, they construct the words in the summarization one by one based
    on the context vector. To increase the effectiveness of code summarization, techniques
    such as the attention mechanism are applied to the task. The relevant techniques
    are summarized in Table [3](#S6.T3 "Table 3 ‣ 6.2\. Code Summarization ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") based on
    the model’s relevant code structures.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 代码摘要是利用自然语言对代码功能进行简洁说明的工作。它可以帮助提高代码的可读性，以及开发人员理解程序的效率。代码摘要任务可以被看作是将输入代码翻译成自然语言，因此通常使用seq2seq模型架构。在编码阶段，（Hu
    et al., [2018](#bib.bib68), [2020a](#bib.bib69); LeClair et al., [2019](#bib.bib90)）将输入的NSC或展平序列转换为上下文向量，然后在解码阶段，他们根据上下文向量逐个构造摘要中的词汇。为了提高代码摘要的效果，任务中应用了注意力机制等技术。相关技术在表[3](#S6.T3
    "Table 3 ‣ 6.2\. Code Summarization ‣ 6\. Tasks ‣ A Survey of Deep Learning Models
    for Structural Code Understanding")中根据模型的相关代码结构进行了总结。
- en: Table 3\. Code Summarization
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 代码摘要
- en: '| Type | Reference | Model | Description |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sequence | DeepCom (Hu et al., [2018](#bib.bib68)) | LSTM | Employs the seq2seq
    model which uses LSTM as encoder and decoder to generate code fragment summaries
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | DeepCom (Hu et al., [2018](#bib.bib68)) | LSTM | 采用seq2seq模型，其中LSTM作为编码器和解码器生成代码片段摘要
    |'
- en: '|  | Hybrid-Deepcom (Hu et al., [2020a](#bib.bib69)) | LSTM | Utilizes two
    GRU as encoders for NCS and flattened sequence |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | Hybrid-Deepcom (Hu et al., [2020a](#bib.bib69)) | LSTM | 利用两个GRU作为NCS和展平序列的编码器
    |'
- en: '|  | Ast-attendgru (LeClair et al., [2019](#bib.bib90)) | GRU | Uses two GRU
    with attention mechanism to process NCS and flattened sequence respectively for
    getting context vector |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | Ast-attendgru (LeClair et al., [2019](#bib.bib90)) | GRU | 使用两个带注意力机制的GRU分别处理NCS和展平序列，以获取上下文向量
    |'
- en: '|  | (Allamanis et al., [2016](#bib.bib8)) | CNN | Uses Convolutional Attention
    Network to summarize the code |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | (Allamanis et al., [2016](#bib.bib8)) | CNN | 使用卷积注意力网络来总结代码 |'
- en: '|  | (Iyer et al., [2016](#bib.bib73)) | LSTM | Uses LSTM with attention to
    produce sentences that describe code snippets |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | (Iyer et al., [2016](#bib.bib73)) | LSTM | 使用带注意力机制的LSTM生成描述代码片段的句子 |'
- en: '|  | (Zhang et al., [2020b](#bib.bib193)) | search engine | Retrieves flattened
    sequence on the search engine to obtain syntactically similar code fragments |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | (Zhang et al., [2020b](#bib.bib193)) | 搜索引擎 | 在搜索引擎上检索展平序列，以获取语法上相似的代码片段
    |'
- en: '|  | (Ahmad et al., [2020](#bib.bib3)) | Transformer | Employs transformer
    for code summarization |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  | (Ahmad et al., [2020](#bib.bib3)) | Transformer | 采用Transformer进行代码摘要
    |'
- en: '| Graph | (Wan et al., [2018](#bib.bib158)) | RNN, Tree-RNN | Uses a attention
    layer to fuse two representations, one from the structure of source code with
    AST-based LSTM, the other from the sequence with LSTM |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Graph | (Wan et al., [2018](#bib.bib158)) | RNN，树形 RNN | 使用注意力层融合两种表示，一种来自于
    AST 基于 LSTM 的源代码结构，另一种来自于 LSTM 的序列 |'
- en: '|  | (Fernandes et al., [2019](#bib.bib53)) | GGNN | Uses sequential encoder
    and a GGNN to generate the representations |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | (Fernandes et al., [2019](#bib.bib53)) | GGNN | 使用顺序编码器和 GGNN 生成表示 |'
- en: '|  | TBCAA(Chen et al., [2019b](#bib.bib36)) | Tree-based LSTM | Uses Tree-based
    convolution for API-enhanced AST |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | TBCAA (Chen et al., [2019b](#bib.bib36)) | 基于树的 LSTM | 使用基于树的卷积来增强 API
    的 AST |'
- en: '|  | (LeClair et al., [2020](#bib.bib89)) | ConvGNN | Encodes the node token
    embedding with recurrent layer and a ConvGNN, later uses an attention layer to
    learn important tokens |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | (LeClair et al., [2020](#bib.bib89)) | ConvGNN | 使用递归层和 ConvGNN 编码节点令牌嵌入，然后使用注意力层学习重要的令牌
    |'
- en: '|  | Flow2Vec (Sui et al., [2020](#bib.bib144)) | Flow2Vec | Pre-embeds the
    interprocedural value-flow graph, considers the reachability via matrix multiplication
    problem and uses it to approximate the high-order proximity embedding |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | Flow2Vec (Sui et al., [2020](#bib.bib144)) | Flow2Vec | 预嵌入过程间值流图，考虑通过矩阵乘法问题的可达性，并用其近似高阶邻近嵌入
    |'
- en: '|  | BASTS (Lin et al., [2021](#bib.bib104)) | Tree-LSTM | Uses Tree-LSTM and
    Transformer architecture to combine the representations of split AST and source
    code |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | BASTS (Lin et al., [2021](#bib.bib104)) | Tree-LSTM | 使用 Tree-LSTM 和 Transformer
    架构结合分割 AST 和源代码的表示 |'
- en: '|  | CoCoSum (Wang et al., [2021a](#bib.bib165)) | Transformer, Multi-Relational
    GNN | The global encoder contains an MRGNN to embed the UML class diagrams and
    a transformer-based model for embedding the class names, while the local encoder
    uses the GRU |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  | CoCoSum (Wang et al., [2021a](#bib.bib165)) | Transformer，多关系 GNN | 全球编码器包含一个
    MRGNN 来嵌入 UML 类图，以及一个基于 Transformer 的模型来嵌入类名，而本地编码器使用 GRU |'
- en: '|  | HybridGNN (Liu et al., [2020a](#bib.bib112)) | GNN | A hybrid message
    passing GNN based which fuse the static and dynamic graph |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | HybridGNN (Liu et al., [2020a](#bib.bib112)) | GNN | 基于消息传递的混合 GNN，融合了静态和动态图
    |'
- en: '|  | (Shi et al., [2021](#bib.bib139)) | RNN, attention | A convolutional attention
    network for extreme summarization of source code |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | (Shi et al., [2021](#bib.bib139)) | RNN，注意力 | 用于源代码极端总结的卷积注意力网络 |'
- en: '|  | (Liu et al., [2021c](#bib.bib113)) | HAConvGNN | Hierarchically splits
    the AST into subtrees, learns the representation of split AST, and reconstructs
    them to combine the structural and semantic information with RvNN |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | (Liu et al., [2021c](#bib.bib113)) | HAConvGNN | 层次分割 AST 为子树，学习分割 AST
    的表示，并重建它们以结合结构和语义信息与 RvNN |'
- en: 6.3\. Code Search
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 代码搜索
- en: Code search is a information retrieval task. The input of code search is the
    query which is natural language description or a code snippet. And the output
    is the best matching code snippets for input. The goal of Code Search is to retrieve
    code snippets from a large code corpus that most closely match a developer’s intent(Cambronero
    et al., [2019b](#bib.bib33)). Being able to explore and reuse existing code that
    is match to the developer’s intent is a fundamental productivity tool. Some online
    sites such as Stack Overflow are popular because of the convenience that searching
    for code relevant to a user’s question expressed in natural language.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 代码搜索是一个信息检索任务。代码搜索的输入是自然语言描述或代码片段的查询。输出是与输入最匹配的代码片段。代码搜索的目标是从大型代码库中检索最接近开发者意图的代码片段
    (Cambronero et al., [2019b](#bib.bib33))。能够探索和重用与开发者意图匹配的现有代码是一个基本的生产力工具。一些在线网站如
    Stack Overflow 由于提供了根据自然语言表达的用户问题搜索相关代码的便利而受到欢迎。
- en: Some articles refer to this task as semantic code search or code recommendation,
    these names emphasize characteristics of code search. It is based on the semantics
    of the input, and the semantic alignment between input and code snippets is crucial.
    And the information retrieval nature of this task is that all outputs are retrieved
    from the code corpus as they originally are. The retrieval nature distinguishes
    code search and code generation, where the generation task is aiming to synthesize
    and write codes not only already in the code corpus.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文章将此任务称为语义代码搜索或代码推荐，这些名称强调了代码搜索的特征。它基于输入的语义，输入与代码片段之间的语义对齐至关重要。该任务的信息检索特性在于所有输出均从代码库中检索出来，保持原样。检索特性区分了代码搜索和代码生成，其中生成任务的最终目标是合成和编写不仅仅是已经存在于代码库中的代码。
- en: The code search models are summarized in Table [4](#S6.T4 "Table 4 ‣ 6.3\. Code
    Search ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding").
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 代码搜索模型总结如表 [4](#S6.T4 "Table 4 ‣ 6.3\. Code Search ‣ 6\. Tasks ‣ A Survey of
    Deep Learning Models for Structural Code Understanding") 所示。
- en: Table 4\. Code Search
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 代码搜索
- en: '| Type | Reference | Model | Description |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sequence | (Sachdev et al., [2018](#bib.bib137)) | word2vec | Combines natural
    language techniques and information retrieval methods |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | （Sachdev 等，[2018](#bib.bib137)） | word2vec | 结合自然语言技术和信息检索方法 |'
- en: '|  | CODEnn (Gu et al., [2018](#bib.bib58)) | RNN | Jointly embeds code snippets
    and natural language descriptions into a high-dimensional vector space |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | CODEnn（Gu 等，[2018](#bib.bib58)） | RNN | 将代码片段和自然语言描述共同嵌入到高维向量空间中 |'
- en: '|  | UNIF(Cambronero et al., [2019b](#bib.bib33)) | word2vec | Uses attention
    to combine per-token embeddings and produce the code sentence embedding |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | UNIF（Cambronero 等，[2019b](#bib.bib33)） | word2vec | 使用注意力结合每个标记的嵌入并生成代码句子嵌入
    |'
- en: '|  | CARLCS-CNN (Shuai et al., [2020](#bib.bib143)) | CNN | Uses CNN to embed
    code and query respectively |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | CARLCS-CNN（Shuai 等，[2020](#bib.bib143)） | CNN | 使用 CNN 分别嵌入代码和查询 |'
- en: '| Graph | TBCAA(Chen et al., [2019b](#bib.bib36)) | tree-based LSTM | Tree-based
    convolution for API-enhanced AST |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 图 | TBCAA（Chen 等，[2019b](#bib.bib36)） | 基于树的 LSTM | 基于树的卷积用于增强 API 的 AST
    |'
- en: '|  | MMAN(Wan et al., [2019a](#bib.bib156)) | GGNN, Tree-LSTM | A multi-modal
    Attention Network that uses attention mechanism to capture the information from
    an LSTM for embedding sequential tokens, a Tree-LSTM for embedding AST, and a
    GGNN for representing CFG |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  | MMAN（Wan 等，[2019a](#bib.bib156)） | GGNN, Tree-LSTM | 一种多模态注意力网络，使用注意力机制从
    LSTM 捕获信息以嵌入序列标记，使用 Tree-LSTM 嵌入 AST，使用 GGNN 表示 CFG |'
- en: 6.4\. Clone Detection
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 克隆检测
- en: 'Clone detection task indicates that there are two or multiple similar code
    snippets in the same software or system, which is also known as code clones. The
    code clones can support the modifications by the developers for better reusing
    them. Code clones can be described as the following 4 types (Bellon et al., [2007](#bib.bib23)):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆检测任务表示在同一软件或系统中存在两个或多个相似的代码片段，这也称为代码克隆。代码克隆可以支持开发人员对其进行修改以便更好地重用它们。代码克隆可以描述为以下
    4 种类型（Bellon 等，[2007](#bib.bib23)）：
- en: Type-1 clones are identical code fragments, which may contain slight differences
    in white-space, layouts, or comments.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Type-1 克隆是相同的代码片段，可能在空格、布局或注释中存在轻微差异。
- en: Type-2 clones are identical code fragments, which may contain the differences
    of variable names, constants, function names, identifiers, literals, types, layouts,
    white-space, or comments.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Type-2 克隆是相同的代码片段，可能包含变量名、常量、函数名、标识符、字面量、类型、布局、空格或注释的差异。
- en: Type-3 clones are syntactically similar code fragments with added, deleted,
    or modified statements.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Type-3 克隆是语法上相似的代码片段，具有添加、删除或修改的语句。
- en: Type-4 clones are semantic similar code fragments that may use different lexical
    and syntax to express the equivalent semantic.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Type-4 克隆是语义上相似的代码片段，可能使用不同的词汇和语法来表达等效的语义。
- en: As the similarities decrease in the four types, the difficulty of detecting
    the clones increases. The approaches to solve the clone detection is shown in
    Table [5](#S6.T5 "Table 5 ‣ 6.4\. Clone Detection ‣ 6\. Tasks ‣ A Survey of Deep
    Learning Models for Structural Code Understanding").
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 随着四种类型的相似性减少，检测克隆的难度增加。解决克隆检测的方法如表 [5](#S6.T5 "Table 5 ‣ 6.4\. Clone Detection
    ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    所示。
- en: Table 5\. Clone Detection
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 克隆检测
- en: '| Type | Reference | Model | Description |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sequence | CCLearner (Li et al., [2017a](#bib.bib97)) | DNN | Extracts tokens
    from known method-level code clones and non-clones to train a classifier |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | CCLearner（Li 等，[2017a](#bib.bib97)） | DNN | 从已知的函数级代码克隆和非克隆中提取标记以训练分类器
    |'
- en: '|  | (White et al., [2016](#bib.bib171)) | RNN | Uses RNN modeling sequences
    of terms in a source code corpus |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | （White 等，[2016](#bib.bib171)） | RNN | 使用 RNN 建模源代码语料库中的术语序列 |'
- en: '| Graph | DeepSim (Zhao and Huang, [2018](#bib.bib197)) | Feed-forward neural
    network | An approach for measuring code functional similarity that uses two flow
    graphs as the basis and encodes them with Feed-forward neural network |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 图 | DeepSim（Zhao 和 Huang，[2018](#bib.bib197)） | 前馈神经网络 | 一种测量代码功能相似性的方法，使用两个流图作为基础，并用前馈神经网络对它们进行编码
    |'
- en: '|  | ASTNN (Zhang et al., [2019a](#bib.bib194)) | bidirectional RNN | Encodes
    the statement subtree and uses Bi-GRU to model the naturalness of the statements
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  | ASTNN（Zhang 等，[2019a](#bib.bib194)） | 双向 RNN | 对语句子树进行编码，并使用 Bi-GRU 模型化语句的自然性
    |'
- en: '|  | TBCAA (Chen et al., [2019b](#bib.bib36)) | tree-based LSTM | Tree-based
    convolution for API-enhanced AST |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | TBCAA（Chen 等，[2019b](#bib.bib36)） | 基于树的 LSTM | 基于树的卷积，用于 API 增强的 AST
    |'
- en: '|  | DEEPBINDIFF (Duan et al., [2020](#bib.bib48)) | Text-associated DeepWalk
    | Learns basic block embeddings with Text-associated DeepWalk algorithm, and match
    them with the k-hop greedy matching algorithm |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | DEEPBINDIFF（Duan 等，[2020](#bib.bib48)） | 文本关联的 DeepWalk | 使用文本关联的 DeepWalk
    算法学习基本块嵌入，并通过 k-hop 贪婪匹配算法进行匹配 |'
- en: '|  | (Yu et al., [2020](#bib.bib191)) | MPNN, CNN | Use BERT to pre-train token
    and block embeddings on an MLM task, and fine-tune them on 2 graph-level tasks
    with MPNN, GRU, and CNN |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  |（Yu 等，[2020](#bib.bib191)） | MPNN, CNN | 使用 BERT 预训练标记和块嵌入在 MLM 任务上，并在
    2 个图级任务上用 MPNN、GRU 和 CNN 进行微调 |'
- en: '|  | CSEM (Li et al., [2020](#bib.bib92)) | Transformer, GAT, CNN | Converts
    source code to intermediate representation, generates Node vector matrix and inputs
    it into GAT later and CNN layer to obtain embedding of code fragment |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  | CSEM（Li 等，[2020](#bib.bib92)） | Transformer, GAT, CNN | 将源代码转换为中间表示，生成节点向量矩阵，并将其输入到
    GAT 和 CNN 层，以获得代码片段的嵌入 |'
- en: '|  | OSCAR (Peng et al., [2021](#bib.bib127)) | Transformer | A hierarchical
    multi-layer Transformer pre-trained model with a novel positional encoding, contrastive
    learning with optimization techniques |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | OSCAR（Peng 等，[2021](#bib.bib127)） | Transformer | 一种层次化的多层 Transformer
    预训练模型，具有新颖的位置编码、对比学习与优化技术 |'
- en: '|  | HAG (Ji et al., [2021b](#bib.bib75)) | GCN | Uses GCN with layer-wise
    propagation and attention mechanism |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  | HAG（Ji 等，[2021b](#bib.bib75)） | GCN | 使用带有层级传播和注意力机制的 GCN |'
- en: 6.5\. Safety Analysis
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 安全分析
- en: 'Humans are relying more and more on programs and codes to handle various problems
    in life as computer technology advances, and defects and vulnerabilities in codes
    can result in significant losses. As a result, it is vital to examine the code’s
    reliability and security. Defects in code can cause programs to fail to run properly,
    and vulnerability can pose a potential threat to the safe operation of computer
    systems. Furthermore, Malware is Malicious software which is designed to attack
    the device. Therefore, in Safety analysis, we categorized three safety-related
    tasks: defect prediction, vulnerability prediction, and malware classification
    and further summarize the models for these three categories in Table [6](#S6.T6
    "Table 6 ‣ 6.5.3\. Malware Classification ‣ 6.5\. Safety Analysis ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding").'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算机技术的进步，人们越来越依赖程序和代码来处理生活中的各种问题，代码中的缺陷和漏洞可能导致重大损失。因此，检查代码的可靠性和安全性至关重要。代码中的缺陷可能导致程序无法正常运行，而漏洞可能对计算机系统的安全运行构成潜在威胁。此外，恶意软件是旨在攻击设备的恶意软件。因此，在安全分析中，我们将安全相关任务分为三个类别：缺陷预测、漏洞预测和恶意软件分类，并在表格
    [6](#S6.T6 "Table 6 ‣ 6.5.3\. Malware Classification ‣ 6.5\. Safety Analysis ‣
    6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    中进一步总结了这三类模型。
- en: 6.5.1\. Defect Prediction
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1\. 缺陷预测
- en: 'Defect prediction helps developers test more effectively while lowering the
    cost of software development by predicting areas of problematic code. The two
    types of defect prediction tasks now accessible are within-project defect prediction
    (WPDP), in which the training and test sets are from the same project, and cross-project
    defect prediction (CPDP), in which the test set is different from the training
    set. The formalization of the Defect prediction task is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 缺陷预测有助于开发人员更有效地测试，同时通过预测问题代码区域来降低软件开发成本。目前可用的两种缺陷预测任务是项目内缺陷预测（WPDP），其中训练集和测试集来自同一项目，以及跨项目缺陷预测（CPDP），其中测试集与训练集不同。缺陷预测任务的形式化如下：
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model can output a label $y$ which means the code snippet with defects(Buggy)
    or without defects(clean).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 $n$ 个标记的代码片段 $S=\{t_{1},t_{2},...t_{n}\}$，预测模型可以输出一个标签 $y$，表示代码片段是否存在缺陷（Buggy）或没有缺陷（clean）。
- en: 6.5.2\. Vulnerability Detection
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 漏洞检测
- en: 'The Vulnerability detection task, including vulnerability detection based on
    code similarity and code patterns, can prevent code from being attacked and improve
    the security of code. Some approaches use deep learning for vulnerability detection
    ,for example, VulDeePecker (Li et al., [2018](#bib.bib102)). The formalization
    of the Vulnerability detection task is as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 漏洞检测任务，包括基于代码相似性和代码模式的漏洞检测，可以防止代码受到攻击，提高代码的安全性。一些方法使用深度学习进行漏洞检测，例如VulDeePecker
    (Li et al., [2018](#bib.bib102))。漏洞检测任务的形式化如下：
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model can output a label $y$ which means the code snippet with or without vulnerability.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含$n$个标记的代码片段$S=\{t_{1},t_{2},...t_{n}\}$，预测模型可以输出一个标签$y$，表示代码片段是否存在漏洞。
- en: 6.5.3\. Malware Classification
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.3\. 恶意软件分类
- en: 'Malware classification is one kind of malware detection, which is a binary
    classification problem. Malware classification can be formalized as: Given a program
    P, classify P as a normal program or malware.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件分类是一种恶意软件检测问题，属于二分类问题。恶意软件分类可以被形式化为：给定程序P，将P分类为正常程序或恶意软件。
- en: Table 6\. Safety analysis
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 表6\. 安全分析
- en: '| Type | Reference | Model | Kind | Description |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考 | 模型 | 类型 | 描述 |'
- en: '| Sequence | (Wang et al., [2016](#bib.bib162)) | DBN | Defect | Uses DBN to
    automatically learn the semantic expression of code |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | (Wang et al., [2016](#bib.bib162)) | DBN | 缺陷 | 使用DBN自动学习代码的语义表达 |'
- en: '|  | Seml(Liang et al., [[n.d.]](#bib.bib103)) | LSTM | Defect | Predicts software
    defect with the help with LSTM |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | Seml(Liang et al., [[n.d.]](#bib.bib103)) | LSTM | 缺陷 | 借助LSTM预测软件缺陷 |'
- en: '|  | DeepCPDP(Chen et al., [2019a](#bib.bib35)) | Bi-LSTM | Defect | Proposes
    SimAST and SimAST2Vec |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | DeepCPDP(Chen et al., [2019a](#bib.bib35)) | Bi-LSTM | 缺陷 | 提出了SimAST和SimAST2Vec
    |'
- en: '|  | (Wang et al., [2020a](#bib.bib161)) | DBN | Defect | Utilizes the DBN
    to learn the higher-level semantic characteristics of code AST token |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | (Wang et al., [2020a](#bib.bib161)) | DBN | 缺陷 | 利用DBN学习代码AST标记的更高层次语义特征
    |'
- en: '|  | (Le et al., [2018](#bib.bib87)) | CNN-BiLSTM | Malware | Proposes an approach
    to enable malware classification by malware analysis non-experts |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | (Le et al., [2018](#bib.bib87)) | CNN-BiLSTM | 恶意软件 | 提出了一种使恶意软件分类能够由非专家进行恶意软件分析的方法
    |'
- en: '|  | (Gibert et al., [2019](#bib.bib56)) | CNN | Malware | Proposes a file
    agnostic deep learning approach for malware categorization to efficiently group
    malicious software into families based on a set of discriminant patterns extracted
    from their visualization as images |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | (Gibert et al., [2019](#bib.bib56)) | CNN | 恶意软件 | 提出了一种文件无关的深度学习方法用于恶意软件分类，以基于从图像中提取的一组判别模式高效地将恶意软件分组
    |'
- en: '| Graph | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | Defect | Constructs
    graphs by adding different types of edges and uses GGNN to learn the representation
    of the graph |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 图形 | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | 缺陷 | 通过添加不同类型的边构建图形，并使用GGNN学习图形的表示
    |'
- en: '|  | MAGIC (Yan et al., [2019](#bib.bib182)) | DGCNN | Defect | Extends the
    standard DGCNN on Weighted Vertices layer and Adaptive Max Pooling to aggregate
    attributes from graph structures |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  | MAGIC (Yan et al., [2019](#bib.bib182)) | DGCNN | 缺陷 | 扩展了标准DGCNN在加权顶点层和自适应最大池化上的功能，以从图结构中聚合属性
    |'
- en: '|  | (Li et al., [2019b](#bib.bib100)) | GRU, CNN, Attention mechanism | Defect
    | Extracts the global (from PDG and DFG) and local (from AST) context with Attention-Based
    GRU, CNN. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | (Li et al., [2019b](#bib.bib100)) | GRU, CNN, 注意力机制 | 缺陷 | 提取全局（来自PDG和DFG）和局部（来自AST）上下文，使用基于注意力的GRU和CNN。
    |'
- en: '|  | Devign (Zhou et al., [2019](#bib.bib198)) | GGNN, GRU, CNN | Vulnerability
    | Encodes the code into a joint graph structure and uses GGNN with the Conv module
    to learn the embedding. |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | Devign (Zhou et al., [2019](#bib.bib198)) | GGNN, GRU, CNN | 漏洞 | 将代码编码为联合图结构，并使用GGNN与卷积模块学习嵌入。
    |'
- en: '|  | BugGraph (Ji et al., [2021a](#bib.bib76)) | GTN | Vulnerability | A Graph
    Triplet-loss Network on the attributed CFG to learn similarity ranking. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | BugGraph (Ji et al., [2021a](#bib.bib76)) | GTN | 漏洞 | 在属性CFG上使用图三元组损失网络学习相似性排序。
    |'
- en: '|  | (Brauckmann et al., [2020](#bib.bib29)) | GGNN | Malware | Uses GGNN for
    learning predictive compiler tasks on AST and CDFGs |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | (Brauckmann et al., [2020](#bib.bib29)) | GGNN | 恶意软件 | 使用GGNN在AST和CDFGs上学习预测编译器任务
    |'
- en: 6.6\. Bug Localization
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 错误定位
- en: 'Bug localization is a task that localizes the position of bugs in a buggy code
    snippet. The bug localization can also be seen as the previous step of program
    repair. Bugs fall into two categories based on how they are discovered: static
    and dynamic. The static bug location is determined by the control and data dependencies,
    whereas the dynamic bug location is determined by the program execution. We provide
    works of bug localization task in Table [7](#S6.T7 "Table 7 ‣ 6.6\. Bug Localization
    ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    The formalization of bug localization task is as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 错误定位是一个将错误在代码片段中定位的任务。错误定位也可以看作是程序修复的前一步。根据发现方式，错误分为静态和动态两类。静态错误位置由控制和数据依赖关系确定，而动态错误位置则由程序执行确定。我们在表
    [7](#S6.T7 "表 7 ‣ 6.6\. 错误定位 ‣ 6\. 任务 ‣ 结构化代码理解的深度学习模型综述") 中提供了错误定位任务的相关工作。错误定位任务的形式化定义如下：
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model will predict the position $\lambda$ of buggy token $t_{\lambda}$ such as
    the misused variables or operators. The place where the correct token is needed
    to predict is also called a slot.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含 $n$ 个标记的代码片段 $S=\{t_{1},t_{2},...t_{n}\}$，预测模型将预测错误标记 $t_{\lambda}$ 的位置
    $\lambda$，例如误用的变量或操作符。需要预测正确标记的位置也称为槽位。
- en: Table 7\. Bug Localization
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 错误定位
- en: '| Type | Reference | Model | Description |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sequence | (Vasic et al., [2019](#bib.bib153)) | LSTM | Presents multi-headed
    pointer networks for training a model that jointly and directly localizes and
    repairs variable-misuse bugs |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Sequence | (Vasic et al., [2019](#bib.bib153)) | LSTM | 提出了多头指针网络，用于训练一个模型，该模型直接且共同定位和修复变量误用错误
    |'
- en: '|  | BULNER(Barbosa et al., [2019](#bib.bib22)) | Word2vec | Proposes a method
    for Bug Localization with word embeddings and Network Regularization |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | BULNER (Barbosa et al., [2019](#bib.bib22)) | Word2vec | 提出了一个基于词嵌入和网络正则化的错误定位方法
    |'
- en: '| Graph | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | Uses GRU and GGNN as
    encoder and a token replacement mechanism as decoder to encode the token information
    and generate the fixing actions |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Graph | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | 使用 GRU 和 GGNN 作为编码器，使用标记替换机制作为解码器，以编码标记信息并生成修复动作
    |'
- en: '|  | (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | Introduces a program
    feedback graph and apply GNN to model the reasoning process. |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | 引入了程序反馈图，并应用 GNN 来建模推理过程。
    |'
- en: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | Proposes Graph Interval
    Neural Network, which includes the heightening and lowering operator to learn
    the representation of CFG |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | 提出了图间隔神经网络（Graph Interval
    Neural Network），包括升高和降低操作符，用于学习 CFG 的表示 |'
- en: 6.7\. Program Repair
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 程序修复
- en: Program Repair, also known as bug fix, code refinement, aims at fixing the localized
    bugs. Some works perform the bug localization and program repair tasks jointly,
    for example, BugLab (Allamanis et al., [2021](#bib.bib7)). After detecting the
    bugs, the repair is conducted on the typical line of the program. Some works combine
    bug detection and program repair, which predict the location and fix action at
    the same time with a sequence combining two of them(Vasic et al., [2019](#bib.bib153)).
    Other repair tasks will only fix bugs assuming the bug locations already exist,
    such as CODIT(Chakraborty et al., [2020](#bib.bib34)).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 程序修复，也称为错误修复、代码精炼，旨在修复定位的错误。一些工作将错误定位和程序修复任务联合执行，例如 BugLab (Allamanis et al.,
    [2021](#bib.bib7))。在检测到错误后，会在程序的典型行上进行修复。一些工作将错误检测和程序修复结合起来，同时预测位置和修复动作，使用序列将两者结合起来
    (Vasic et al., [2019](#bib.bib153))。其他修复任务则仅修复假设错误位置已经存在的错误，如 CODIT (Chakraborty
    et al., [2020](#bib.bib34))。
- en: One of the most popular tasks in automated program repair is the VARMISUSE task
    proposed by Allamanis(Allamanis et al., [2018b](#bib.bib6)). The VARMISUSE task
    is to automatically detect the variable misuse mistakes in source code and repair
    it with the correct variable. In other words, program repair task can be seen
    as a kind of code generation in the slot where the mistake is detected. However,
    due to the difference of input and the proposal of generating the code, we consider
    program repair as a new task and summarize the models in Table [8](#S6.T8 "Table
    8 ‣ 6.7\. Program Repair ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural
    Code Understanding").
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化程序修复中最流行的任务之一是由Allamanis(Allamanis et al., [2018b](#bib.bib6))提出的VARMISUSE任务。VARMISUSE任务是自动检测源代码中的变量误用错误，并用正确的变量修复它。换句话说，程序修复任务可以视为在检测到错误的槽位中生成代码。然而，由于输入的差异以及生成代码的提议，我们将程序修复视为一种新任务，并在表[8](#S6.T8
    "Table 8 ‣ 6.7\. 程序修复 ‣ 6\. 任务 ‣ 结构化代码理解深度学习模型调查")中总结了模型。
- en: 'Because of the diverse datasets, languages, and granularities of the output,
    different works have distinct definitions of program repair. For example, the
    output may be a single word to repair a misused variable or a full sentence to
    repair a new code snippet line. The formalization of the repair of a token is
    similar to the formalization in [6.6](#S6.SS6 "6.6\. Bug Localization ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). After
    detecting the positions, the program repair task will generate the correct token
    $t_{\lambda}^{\prime}$. Another of the formalizations of the program repair task
    is as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集、语言和输出粒度的多样性，不同的工作对程序修复有不同的定义。例如，输出可能是修复误用变量的单个词，也可能是修复新代码片段行的完整句子。令牌修复的形式化类似于[6.6](#S6.SS6
    "6.6\. 错误定位 ‣ 6\. 任务 ‣ 结构化代码理解深度学习模型调查")中的形式化。在检测到位置后，程序修复任务将生成正确的令牌 $t_{\lambda}^{\prime}$。另一种程序修复任务的形式化如下：
- en: Given a broken code snippet $S=\{l_{1},l_{2},...l_{k}\}$ (with $k$ line) the
    diagnostic feedback from compiler(the feedback usually contains line number and
    error message) $I$ , the program repair task is to localize the erroneous line
    index $n$ and generate a repaired code version $l_{n}^{{}^{\prime}}$ replacing
    the wrong code of $l_{n}$.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个破损的代码片段 $S=\{l_{1},l_{2},...l_{k}\}$（具有 $k$ 行）以及编译器的诊断反馈（反馈通常包含行号和错误信息）$I$，程序修复任务是定位错误的行索引
    $n$ 并生成修复的代码版本 $l_{n}^{{}^{\prime}}$ 替换错误的 $l_{n}$。
- en: Table 8\. Program Repair
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 程序修复
- en: '| Type | Reference | Model | Description |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 参考文献 | 模型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sequence | DeepFix(Gupta et al., [2017](#bib.bib60)) | GRU | Fixes multiple
    errors by iteratively invoking a trained neural network |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Sequence | DeepFix(Gupta et al., [2017](#bib.bib60)) | GRU | 通过反复调用训练过的神经网络来修复多个错误
    |'
- en: '|  | TFix(Berabi et al., [2021](#bib.bib27)) | Transformer | Uses T5(Raffel
    et al., [2019](#bib.bib131)) to accurately synthesize fixes to a wide range of
    errors |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | TFix(Berabi et al., [2021](#bib.bib27)) | Transformer | 使用T5(Raffel et
    al., [2019](#bib.bib131))准确合成对各种错误的修复 |'
- en: '| Graph | CODIT (Chakraborty et al., [2020](#bib.bib34)) | LSTM | An encoder-decoder
    model which first learns the structural changes in AST modifications with tree-to-tree
    model, then predicts the token conditioned on the AST |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Graph | CODIT (Chakraborty et al., [2020](#bib.bib34)) | LSTM | 一种编码器-解码器模型，首先通过树对树模型学习AST修改中的结构变化，然后根据AST预测令牌
    |'
- en: '|  | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | Uses GRU and GGNN as encoder
    and a token replacement mechanism as decoder to encode the token information and
    generate the fixing actions |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | 使用GRU和GGNN作为编码器，使用令牌替换机制作为解码器来编码令牌信息并生成修复动作
    |'
- en: '|  | Hoppity (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | Introduces
    a program feedback graph and applies GNN to model the reasoning process |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | Hoppity (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | 引入程序反馈图，并应用GNN来建模推理过程
    |'
- en: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | Proposes Graph Interval
    Neural Network, which includes the heightening and lowering operator to learn
    the representation of CFG |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | 提出了图区间神经网络，包括升高和降低算子以学习CFG的表示
    |'
- en: 6.8\. Pre-training Task
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8\. 预训练任务
- en: Pre-training is the process of training a model on a large amount of pre-training
    data to extract as many features as possible from the data so that the model can
    better solve the aiming task after fine-tuning in specific dataset. With the proliferation
    of open source corpora, pre-trained models have emerged for large amounts of code
    data, such as CodeBERT (Feng et al., [2020](#bib.bib52)),CuBert(Kanade et al.,
    [2020](#bib.bib80)), GPT-C(Svyatkovskiy et al., [2020](#bib.bib146)), CodeT5(Wang
    et al., [2021d](#bib.bib168))and GraphCodeBERT(Guo et al., [2020](#bib.bib59)),
    which can capture semantic information in code and be quickly and effectively
    applied to various downstream tasks. The pre-training models are becoming mainstream
    models for code-related tasks. A series of pre-training models have been proposed
    by large companies such as Microsoft and Facebook. Some models such as Alphacode²²2https://alphacode.deepmind.com/
    and Codex³³3https://openai.com/blog/openai-codex/ have been applied in practice.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练是对大量预训练数据进行模型训练的过程，以从数据中提取尽可能多的特征，使得模型在特定数据集的微调后能更好地解决目标任务。随着开源语料库的普及，已经出现了大量代码数据的预训练模型，如
    CodeBERT （Feng et al., [2020](#bib.bib52)）、CuBert（Kanade et al., [2020](#bib.bib80)）、GPT-C（Svyatkovskiy
    et al., [2020](#bib.bib146)）、CodeT5（Wang et al., [2021d](#bib.bib168)）和 GraphCodeBERT（Guo
    et al., [2020](#bib.bib59)），这些模型能够捕捉代码中的语义信息，并快速有效地应用于各种下游任务。预训练模型正成为代码相关任务的主流模型。微软和
    Facebook 等大型公司提出了一系列预训练模型。一些模型如 Alphacode²²2https://alphacode.deepmind.com/ 和
    Codex³³3https://openai.com/blog/openai-codex/ 已经在实际应用中得到应用。
- en: 7\. Metrics and Datasets
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 评价指标和数据集
- en: We describe the metrics and datasets associated with code-related work in this
    section to serve as a reference for future work.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中描述了与代码相关工作相关的指标和数据集，以供未来工作参考。
- en: 7.1\. Metrics
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 指标
- en: 'The relevant metrics in the code representation field are divided into two
    categories: NLP-related metrics and information retrieval-related metrics.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 代码表示领域的相关评价指标分为两类：与自然语言处理（NLP）相关的指标和与信息检索相关的指标。
- en: 7.1.1\. NLP-related Metrics
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 与 NLP 相关的指标
- en: Due to the similarity between code and natural language, many metrics employed
    in the field of code are generated from the natural language field, particularly
    the generation tasks such as code summary.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码与自然语言之间的相似性，许多在代码领域使用的指标源自自然语言领域，特别是代码摘要等生成任务。
- en: •
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Accuracy, Precision, Recall and F1-score One of the most intuitive performance
    metrics is accuracy, which is defined as the ratio of correct predictions to total
    predictions. Precision refers to the percentage of correct predictions of true
    samples among all predictions predicted as true samples whereas Recall refers
    to the percentage of correctly predictions of true samples among all true samples.
    The weighted average of Precision and Recall is the F1 Score. As a result, this
    score considers both false positives and false negatives. It is more useful, especially
    when the distribution of classes is uneven. The three metrics are commonly used
    in classification or prediction tasks.
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率、精确率、召回率和 F1 分数 其中一个最直观的性能指标是准确率，它定义为正确预测与总预测的比例。精确率指的是所有被预测为正样本的预测中真正样本的百分比，而召回率指的是所有真实样本中被正确预测为正样本的百分比。精确率和召回率的加权平均值即为
    F1 分数。因此，这个分数考虑了假阳性和假阴性。特别是当类别分布不均时，这个指标更加有用。这三种指标通常用于分类或预测任务。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BLEU BiLingual Evaluation Understudy(BLEU)(Papineni et al., [2002](#bib.bib126))
    is designed for automated evaluation of statistical machine translation and can
    be used to measure the performance of code summarization and generation tasks.
    The score is computed as Equation [1](#S7.E1 "In 2nd item ‣ 7.1.1\. NLP-related
    Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and [2](#S7.E2 "In 2nd item ‣ 7.1.1\.
    NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep
    Learning Models for Structural Code Understanding"), where the former is the Brevity
    Penalty(BP) with the length of the candidate translation $c$ and the effective
    reference sequence length $r$. TBP $p_{n}$ is the ratio of length n subsequences
    in the candidate that is also in the reference. And the $N$ in Equation [2](#S7.E2
    "In 2nd item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") is usually
    set to 4(BLEU-4)(Hu et al., [2018](#bib.bib68)).
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BLEU 双语评估基准（BLEU）（Papineni 等，[2002](#bib.bib126)）旨在用于自动评估统计机器翻译，并可用于衡量代码总结和生成任务的性能。评分计算如方程[1](#S7.E1
    "在第2项 ‣ 7.1.1. NLP相关指标 ‣ 7.1. 指标 ‣ 7. 指标和数据集 ‣ 结构代码理解的深度学习模型综述")和[2](#S7.E2 "在第2项
    ‣ 7.1.1. NLP相关指标 ‣ 7.1. 指标 ‣ 7. 指标和数据集 ‣ 结构代码理解的深度学习模型综述")所示，其中前者是具有候选翻译长度$c$和有效参考序列长度$r$的简洁惩罚（BP）。TBP
    $p_{n}$ 是候选中长度为$n$的子序列在参考中也存在的比例。方程[2](#S7.E2 "在第2项 ‣ 7.1.1. NLP相关指标 ‣ 7.1. 指标
    ‣ 7. 指标和数据集 ‣ 结构代码理解的深度学习模型综述")中的$N$通常设为4（BLEU-4）（Hu 等，[2018](#bib.bib68)）。
- en: '| (1) |  | $\mathrm{BP}=\left\{\begin{array}[]{ll}1&amp;\text{ if }c>r\\ e^{(1-r/c)}&amp;\text{
    if }c\leq r\end{array}\right.$ |  |'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (1) |  | $\mathrm{BP}=\left\{\begin{array}[]{ll}1\text{ if }c>r\\ e^{(1-r/c)}\text{
    if }c\leq r\end{array}\right.$ |  |'
- en: '| (2) |  | $\mathrm{BLEU}=\mathrm{BP}\cdot\exp\left(\sum_{n=1}^{N}w_{n}\log
    p_{n}\right)$ |  |'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (2) |  | $\mathrm{BLEU}=\mathrm{BP}\cdot\exp\left(\sum_{n=1}^{N}w_{n}\log
    p_{n}\right)$ |  |'
- en: •
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perplexity (PPL) Perplexity is a great probabilistic measure used to evaluate
    exactly how confused the NLP models are. It’s typically used to evaluate language
    models, as well as the dialog generation tasks. PPL is defined as Equation [3](#S7.E3
    "In 3rd item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"),where $x_{i}$
    is the truth label and $P(x_{i})$ is the model output. A model with lower perplexity
    assigns higher probabilities to the true tokens and is expected to perform better.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 困惑度（PPL）困惑度是一个极好的概率测量，用于精确评估NLP模型的混乱程度。它通常用于评估语言模型以及对话生成任务。PPL定义如方程[3](#S7.E3
    "在第3项 ‣ 7.1.1. NLP相关指标 ‣ 7.1. 指标 ‣ 7. 指标和数据集 ‣ 结构代码理解的深度学习模型综述")，其中$x_{i}$是实际标签，$P(x_{i})$是模型输出。困惑度较低的模型对真实标记赋予更高的概率，预期表现更好。
- en: '| (3) |  | $PPL=\exp\left(-\sum_{i}^{T}P\left(x_{i}\right)\log P\left(x_{i}\right)\right),\forall
    i\in 0\ldots T.$ |  |'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (3) |  | $PPL=\exp\left(-\sum_{i}^{T}P\left(x_{i}\right)\log P\left(x_{i}\right)\right),\forall
    i\in 0\ldots T.$ |  |'
- en: •
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ROUGE As opposed to the BLEU score, the Recall-Oriented Understudy for Gisting
    Evaluation (ROUGE) evaluation metric(Lin, [2004](#bib.bib105)) measures the recall.It
    is commonly used in machine translation tasks to assess the quality of generated
    text. However, because it assesses recall, it is mostly employed in summarization
    tasks, where evaluating the amount of words the model can recall is more significant.
    (Lin, [2004](#bib.bib105)) proposes four Rouge methods: 1) ROUGE-N: calculate
    the recall rate on n-gram, 2) ROUGE-L: consider the longest common subsequence
    between the generated sequence $C$ and the target sequence $S$, 3) ROUGE-W: improve
    ROUGE-L and calculate the longest common subsequence by weighting method, and
    4) calculate the recall rate on n-gram that allows word skipping. The calculation
    method of ROUGE-L is shown in Equation [4](#S7.E4 "In 4th item ‣ 7.1.1\. NLP-related
    Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"),where $F_{LCS}$ is ROUGE-L and $\beta$
    is a constant.'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ROUGE 与 BLEU 得分不同，召回导向的摘要评价指标（ROUGE）（Lin, [2004](#bib.bib105)）用于衡量召回率。它通常用于机器翻译任务中评估生成文本的质量。然而，由于它评估的是召回率，因此主要用于摘要任务，在这些任务中，评估模型能召回多少词更为重要。（Lin,
    [2004](#bib.bib105)）提出了四种 ROUGE 方法：1) ROUGE-N：计算 n-gram 的召回率，2) ROUGE-L：考虑生成序列
    $C$ 与目标序列 $S$ 之间的最长公共子序列，3) ROUGE-W：改进 ROUGE-L 并通过加权方法计算最长公共子序列，4) 计算允许跳词的 n-gram
    的召回率。ROUGE-L 的计算方法如方程式 [4](#S7.E4 "在第4项 ‣ 7.1.1\. NLP相关指标 ‣ 7.1\. 指标 ‣ 7\. 指标和数据集
    ‣ 深度学习模型结构代码理解的综述") 所示，其中 $F_{LCS}$ 为 ROUGE-L，$\beta$ 为常数。
- en: '| (4) |  | $\displaystyle Recall_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(S)}$
    |  |'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle Recall_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(S)}$
    |  |'
- en: '|  | $\displaystyle Precision_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(C)}$
    |  |'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle Precision_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(C)}$
    |  |'
- en: '|  | $\displaystyle F_{LCS}$ | $\displaystyle=\frac{\left(1+\beta^{2}\right)Recall_{LCS}Precision_{LCS}}{Recall_{LCS}+\beta^{2}Precision_{LCS}}$
    |  |'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{LCS}$ | $\displaystyle=\frac{\left(1+\beta^{2}\right)Recall_{LCS}Precision_{LCS}}{Recall_{LCS}+\beta^{2}Precision_{LCS}}$
    |  |'
- en: •
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: METEOR The Metric for Evaluation of Translation with Explicit ORdering (METEOR)(Banerjee
    and Lavie, [2005](#bib.bib21)) is a precision-based metric for the evaluation
    of machine-translation output. It overcomes some of the pitfalls of the BLEU score,
    such as exact word matching whilst calculating precision. The METEOR score allows
    synonyms and stemmed words to be matched with a reference word. The most important
    thing in meteor is to use WordNet thesaurus to align the generated sequence with
    the target sequence.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: METEOR 用于显式排序翻译评价的度量（METEOR）（Banerjee 和 Lavie, [2005](#bib.bib21)）是一种基于精确度的机器翻译输出评价指标。它克服了
    BLEU 得分的一些缺陷，例如在计算精确度时需要精确的词匹配。METEOR 得分允许将同义词和词干词与参考词进行匹配。METEOR 的关键在于使用 WordNet
    词典将生成序列与目标序列对齐。
- en: •
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Word Error Rate (WER) It is important to substitute, delete, or insert some
    words into the generated sequence during the generation task in order to keep
    the generated sequence consistent with the target sequence. WER, which is defined
    as Equation [5](#S7.E5 "In 6th item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics
    ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"), can be used to assess the quality of the generated sequence.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字词错误率（WER）在生成任务中，重要的是在生成序列中替换、删除或插入一些词，以保持生成序列与目标序列的一致性。WER，如方程式 [5](#S7.E5
    "在第6项 ‣ 7.1.1\. NLP相关指标 ‣ 7.1\. 指标 ‣ 7\. 指标和数据集 ‣ 深度学习模型结构代码理解的综述") 所定义的，可以用来评估生成序列的质量。
- en: '| (5) |  | $\mathrm{WER}=100\cdot\frac{Substitution+Deletion+Insertion}{N}\%$
    |  |'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (5) |  | $\mathrm{WER}=100\cdot\frac{Substitution+Deletion+Insertion}{N}\%$
    |  |'
- en: •
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CodeBLEU CodeBLEU(Ren et al., [2020](#bib.bib134)) is a metric designed for
    code based on BLEU, which can pay attention to the keywords, leverage the tree
    structure and consider the semantic logic. It is defined as the weighted combination
    of $\mathrm{BLEU}$, $\mathrm{BLEU}_{\text{weight }}$,$\mathrm{Match}_{\text{ast}}$
    and $\mathrm{Match}_{\text{df}}$, which is shown in Equation [6](#S7.E6 "In 7th
    item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). The first
    term refers to the standard $\mathrm{BLEU}$. $\mathrm{BLEU}_{\text{weight }}$
    is the weighted n-gram match, obtained by comparing the hypothesis code and the
    reference code tokens with different weights. $\mathrm{Match}_{\text{ast}}$ is
    the syntactic AST match, exploring the syntactic information of code. $\mathrm{Match}_{\text{df}}$
    is the semantic data-flow match, considering the semantic similarity between the
    prediction and the reference. The weighted n-gram match and the syntactic AST
    match are used to measure grammatical correctness, whereas the semantic data-flow
    match is used to calculate logic correctness.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeBLEU CodeBLEU（Ren et al., [2020](#bib.bib134)）是基于BLEU的代码评估指标，它可以关注关键字，利用树结构并考虑语义逻辑。它被定义为$\mathrm{BLEU}$、$\mathrm{BLEU}_{\text{weight
    }}$、$\mathrm{Match}_{\text{ast}}$和$\mathrm{Match}_{\text{df}}$的加权组合，如方程[6](#S7.E6
    "In 7th item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding")所示。第一个项指的是标准的$\mathrm{BLEU}$。$\mathrm{BLEU}_{\text{weight
    }}$是加权的n-gram匹配，通过比较假设代码和参考代码的令牌与不同的权重来获得。$\mathrm{Match}_{\text{ast}}$是语法AST匹配，探索代码的语法信息。$\mathrm{Match}_{\text{df}}$是语义数据流匹配，考虑预测与参考之间的语义相似性。加权n-gram匹配和语法AST匹配用于测量语法正确性，而语义数据流匹配用于计算逻辑正确性。
- en: '| (6) |  | CodeBLEU | $\displaystyle=\alpha\cdot\mathrm{BLEU}+\beta\cdot\mathrm{BLEU}_{\text{weightt
    }}$ |  |'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (6) |  | CodeBLEU | $\displaystyle=\alpha\cdot\mathrm{BLEU}+\beta\cdot\mathrm{BLEU}_{\text{weightt
    }}$ |  |'
- en: '|  |  | $\displaystyle+\gamma\cdot\mathrm{Match}_{\text{ast}}+\delta\cdot\mathrm{Match}_{\text{df}}$
    |  |'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\gamma\cdot\mathrm{Match}_{\text{ast}}+\delta\cdot\mathrm{Match}_{\text{df}}$
    |  |'
- en: 7.1.2\. Information Retrieval related Metrics
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 信息检索相关指标
- en: Because some code-related tasks, such as Code Search, are similar to information
    retrieval, many code-related metrics are associated to information retrieval field.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些与代码相关的任务，例如代码搜索，与信息检索相似，因此许多与代码相关的指标与信息检索领域相关联。
- en: •
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SuccessRate(SR) If the information matching the input is in the top-k of the
    search information sorting list, the search is successful. SR@k Calculate the
    proportion of successful searches in all searches. The calculation method is shown
    in Equation [7](#S7.E7 "In 1st item ‣ 7.1.2\. Information Retrieval related Metrics
    ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models
    for Structural Code Understanding"), where $\delta$ is a constant, $Frank_{k}$
    is the rank of matched information in the search information sorting list and
    $Frank_{q}\leq K$ means successful retrieval.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成功率（SR） 如果与输入匹配的信息位于搜索信息排序列表的前k名，则搜索成功。SR@k 计算所有搜索中成功搜索的比例。计算方法见方程[7](#S7.E7
    "In 1st item ‣ 7.1.2\. Information Retrieval related Metrics ‣ 7.1\. Metrics ‣
    7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding")，其中$\delta$是常数，$Frank_{k}$是匹配信息在搜索信息排序列表中的排名，$Frank_{q}\leq K$表示检索成功。
- en: '| (7) |  | $\mathrm{SR@K}=\frac{1}{&#124;Q&#124;}\sum_{q=1}^{Q}\delta\left(\operatorname{FRank}_{q}\leq
    K\right)$ |  |'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (7) |  | $\mathrm{SR@K}=\frac{1}{|Q|}\sum_{q=1}^{Q}\delta\left(\operatorname{FRank}_{q}\leq
    K\right)$ |  |'
- en: •
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mean Reciprocal Rank (MRR) In the retrieval information sorting list, MRR considers
    the ranking of the retrieved matching information. The score is $\frac{1}{N}$
    if the nth information in the list fits the input, and $0$ if there is no matching
    sentence. The sum of all scores is the final score. Equation [8](#S7.E8 "In 2nd
    item ‣ 7.1.2\. Information Retrieval related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics
    and Datasets ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    shows how to calculate MRR.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 互惠排序（MRR） 在信息检索排序列表中，MRR考虑检索到的匹配信息的排名。如果列表中的第n条信息符合输入，则得分为$\frac{1}{N}$，如果没有匹配的句子，则得分为$0$。所有得分的总和为最终得分。方程[8](#S7.E8
    "In 2nd item ‣ 7.1.2\. Information Retrieval related Metrics ‣ 7.1\. Metrics ‣
    7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding")展示了如何计算MRR。
- en: '| (8) |  | $\mathrm{MRR}=\frac{1}{&#124;Q&#124;}\sum_{q=1}^{&#124;Q&#124;}\frac{1}{\operatorname{FRank}_{q}}$
    |  |'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (8) |  | $\mathrm{MRR}=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{\operatorname{FRank}_{q}}$
    |  |'
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Best Hit Rank Best Hit Rank is the highest rank of the hit snippets for the
    query. A highest best hit implies lower user effort to inspect the desired hit
    snippet.
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳命中排名（Best Hit Rank）是查询结果中命中片段的最高排名。最高的最佳命中意味着用户检查所需命中片段的努力更少。
- en: 7.2\. Datasets
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 数据集
- en: We summarize various datasets in the table [9](#S7.T9 "Table 9 ‣ 7.2\. Datasets
    ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"). Some works collect and generate their own datasets for study,
    which may cause the difficulty on comparing different works on the same tasks.
    Table [9](#S7.T9 "Table 9 ‣ 7.2\. Datasets ‣ 7\. Metrics and Datasets ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") does not contain any
    datasets that are not open-source.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[9](#S7.T9 "Table 9 ‣ 7.2\. Datasets ‣ 7\. Metrics and Datasets ‣ A Survey
    of Deep Learning Models for Structural Code Understanding")中总结了各种数据集。一些工作收集和生成自己的数据集进行研究，这可能会导致在相同任务上比较不同工作的困难。表[9](#S7.T9
    "Table 9 ‣ 7.2\. Datasets ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning
    Models for Structural Code Understanding")不包含任何非开源的数据集。
- en: Table 9\. Datasets
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 表9\. 数据集
- en: '| Name | Study | Description |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 研究 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [Genius Dataset](https://github.com/xiaojunxu/dnn-binary-code-similarity)
    | (Feng et al., [2016b](#bib.bib51); Xu et al., [2017](#bib.bib179)) | A real-world
    dataset of 33,045 devices which was collected from public sources and our system
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| [Genius数据集](https://github.com/xiaojunxu/dnn-binary-code-similarity) | (Feng
    et al., [2016b](#bib.bib51); Xu et al., [2017](#bib.bib179)) | 一个包含33,045个设备的现实世界数据集，来自公共来源和我们的系统
    |'
- en: '| [notebookcdg](https://github.com/dakuo/haconvgnn) | (Liu et al., [2021c](#bib.bib113))
    | Has 28,625 code–documentation pairs |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| [notebookcdg](https://github.com/dakuo/haconvgnn) | (Liu et al., [2021c](#bib.bib113))
    | 拥有28,625对代码–文档 |'
- en: '| [py150](http://www.srl.inf.ethz.ch/py150) | (Raychev et al., [2016](#bib.bib132);
    Kanade et al., [2019](#bib.bib79); Kim et al., [2020](#bib.bib83)) | Consists
    of parsed ASTs collected from GitHub python repositories by removing duplicate
    files |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| [py150](http://www.srl.inf.ethz.ch/py150) | (Raychev et al., [2016](#bib.bib132);
    Kanade et al., [2019](#bib.bib79); Kim et al., [2020](#bib.bib83)) | 包含从GitHub
    Python仓库中收集的解析AST，去除重复文件 |'
- en: '| [js150](http://www.srl.inf.ethz.ch/js150) | (Raychev et al., [2016](#bib.bib132);
    Wang and Li, [2021](#bib.bib166)) | Consists of 150’000 JavaScript files and their
    corresponding parsed ASTs |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| [js150](http://www.srl.inf.ethz.ch/js150) | (Raychev et al., [2016](#bib.bib132);
    Wang and Li, [2021](#bib.bib166)) | 包含15万JavaScript文件及其对应的解析AST |'
- en: '| [HGNN](https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization)
    | (Liu et al., [2020a](#bib.bib112)) | Crawled from diversified large-scale open-source
    C projects (total 95k+ unique functions in the dataset) |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| [HGNN](https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization)
    | (Liu et al., [2020a](#bib.bib112)) | 从多样化的大规模开源C项目中爬取（数据集中总共95k+独特函数） |'
- en: '| [CodeSearchNet](https://github.com/github/CodeSearchNet) | (Husain et al.,
    [2019](#bib.bib71); Feng et al., [2020](#bib.bib52); Ugner et al., [[n.d.]](#bib.bib152))
    | Consists of 2 million (comment, code) pairs from open source libraries |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| [CodeSearchNet](https://github.com/github/CodeSearchNet) | (Husain et al.,
    [2019](#bib.bib71); Feng et al., [2020](#bib.bib52); Ugner et al., [[n.d.]](#bib.bib152))
    | 包含来自开源库的200万（注释，代码）对 |'
- en: '| [CONCODE](https://github.com/sriniiyer/concode) | (Iyer et al., [2018](#bib.bib74);
    Allamanis, [2019](#bib.bib4)) | Consists over 100,000 examples consisting of Java
    classes from online code repositories |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| [CONCODE](https://github.com/sriniiyer/concode) | (Iyer et al., [2018](#bib.bib74);
    Allamanis, [2019](#bib.bib4)) | 包含超过100,000个示例，包括来自在线代码库的Java类 |'
- en: '| [CodeXCLUE](https://github.com/microsoft/CodeXGLUE) | (Lu et al., [2021](#bib.bib116);
    Puri et al., [2021](#bib.bib129); Wang et al., [2021d](#bib.bib168)) | Includes
    a collection of 10 tasks across 14 datasets |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| [CodeXCLUE](https://github.com/microsoft/CodeXGLUE) | (Lu et al., [2021](#bib.bib116);
    Puri et al., [2021](#bib.bib129); Wang et al., [2021d](#bib.bib168)) | 包含跨越14个数据集的10个任务的集合
    |'
- en: '| [TFix’s Code Patches Data](https://github.com/eth-sri/TFix) | (Berabi et al.,
    [2021](#bib.bib27)) | Contains more than 100k code patch pairs extracted from
    open source projects on GitHub |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| [TFix的代码补丁数据](https://github.com/eth-sri/TFix) | (Berabi et al., [2021](#bib.bib27))
    | 包含从GitHub开源项目中提取的10万对代码补丁 |'
- en: '| [CoDesc](https://github.com/csebuetnlp/CoDesc) | (Hasan et al., [2021](#bib.bib61))
    | A large dataset of 4.2m Java source code and parallel data of their description
    from code search, and code summarization studies |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| [CoDesc](https://github.com/csebuetnlp/CoDesc) | (Hasan et al., [2021](#bib.bib61))
    | 包含420万Java源代码及其描述的并行数据，来源于代码搜索和代码总结研究 |'
- en: '| [DeepFix](https://bitbucket.org/iiscseal/deepfix) | (Gupta et al., [2017](#bib.bib60);
    Yasunaga and Liang, [2020a](#bib.bib186); Chen et al., [2021](#bib.bib37)) | Consists
    of a program repair dataset (fix compiler errors in C programs) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| [DeepFix](https://bitbucket.org/iiscseal/deepfix) | (Gupta et al., [2017](#bib.bib60);
    Yasunaga and Liang, [2020a](#bib.bib186); Chen et al., [2021](#bib.bib37)) | 包含一个程序修复数据集（修复
    C 程序中的编译错误） |'
- en: '| [SPoC](https://github.com/michiyasunaga/DrRepair) | (Yasunaga and Liang,
    [2020a](#bib.bib186)) | A program synthesis dataset, containing 18,356 programs
    with human-authored pseudocode and test cases |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| [SPoC](https://github.com/michiyasunaga/DrRepair) | (Yasunaga and Liang,
    [2020a](#bib.bib186)) | 一个程序合成数据集，包含 18,356 个程序及其人类编写的伪代码和测试用例 |'
- en: '| [Defects4J](https://git.io/JJGwU) | (Chakraborty et al., [2020](#bib.bib34))
    | A large dataset of 32k real code change |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| [Defects4J](https://git.io/JJGwU) | (Chakraborty et al., [2020](#bib.bib34))
    | 一个包含 32k 个真实代码变更的大型数据集 |'
- en: '| [FunCom](http://leclair.tech/data/funcom/) | (LeClair and McMillan, [2019](#bib.bib91);
    Shrivastava, [2021](#bib.bib142); Wei et al., [2019](#bib.bib169); Mahmud et al.,
    [2021](#bib.bib117)) | A collection of  2.1 million Java methods and their associated
    Javadoc comments |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| [FunCom](http://leclair.tech/data/funcom/) | (LeClair and McMillan, [2019](#bib.bib91);
    Shrivastava, [2021](#bib.bib142); Wei et al., [2019](#bib.bib169); Mahmud et al.,
    [2021](#bib.bib117)) | 收集了 2.1 百万 Java 方法及其相关的 Javadoc 注释 |'
- en: '| [CoSQA](https://github.com/Jun-jie-Huang/CoCLR) | (Huang et al., [2021](#bib.bib70);
    Li et al., [2022](#bib.bib98)) | Includes 20,604 labels for pairs of natural language
    queries and codes, each annotated by at least 3 human annotators |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| [CoSQA](https://github.com/Jun-jie-Huang/CoCLR) | (Huang et al., [2021](#bib.bib70);
    Li et al., [2022](#bib.bib98)) | 包含 20,604 个自然语言查询与代码对的标签，每个标签至少由 3 名人工标注者标注 |'
- en: '| [CoNaLa](https://conala-corpus.github.io/) | (Yin et al., [2018b](#bib.bib189);
    Yin and Neubig, [2018](#bib.bib190)) | Consists 2379 training and 500 test examples
    that were manually annotated |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| [CoNaLa](https://conala-corpus.github.io/) | (Yin et al., [2018b](#bib.bib189);
    Yin and Neubig, [2018](#bib.bib190)) | 包含 2379 个训练和 500 个测试示例，这些示例经过人工标注 |'
- en: '| [Django](https://github.com/odashi/ase15-django-dataset) | (Oda et al., [2015](#bib.bib125);
    Lin et al., [2018](#bib.bib106)) | Comprises of 16000 training, 1000 development
    and 1805 test annotations |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| [Django](https://github.com/odashi/ase15-django-dataset) | (Oda et al., [2015](#bib.bib125);
    Lin et al., [2018](#bib.bib106)) | 包含 16000 个训练、1000 个开发和 1805 个测试注释 |'
- en: '| [BLANCA](https://github.com/wala/blanca) | (Abdelaziz et al., [2021](#bib.bib2))
    | A collection of benchmarks that assess code understanding based on tasks |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| [BLANCA](https://github.com/wala/blanca) | (Abdelaziz et al., [2021](#bib.bib2))
    | 一系列基于任务评估代码理解的基准测试集 |'
- en: '| [IndoNLG](https://github.com/indobenchmark/indonlg) | (Cahyawijaya et al.,
    [2021](#bib.bib31)) | A collection of Natural Language Generation (NLG) resources
    for Bahasa Indonesia with 6 kind of downstream tasks |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| [IndoNLG](https://github.com/indobenchmark/indonlg) | (Cahyawijaya et al.,
    [2021](#bib.bib31)) | 一个包含 6 种下游任务的印尼语自然语言生成 (NLG) 资源集 |'
- en: '| [Neural-Code-Search-Evaluation-Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset)
    | (Li et al., [2019a](#bib.bib93)) | An evaluation dataset consisting of natural
    language query and code snippet pairs for code search |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| [Neural-Code-Search-Evaluation-Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset)
    | (Li et al., [2019a](#bib.bib93)) | 一个评估数据集，由自然语言查询和代码片段对组成，用于代码搜索 |'
- en: 8\. Open Problems
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. 未解问题
- en: It is difficult to fully capture the information of the structures and semantics
    of codes with the existing technology. Most deep-learning models are designed
    for specific tasks and a single language, which are lack flexibility. The following
    open problems can be considered as the future work direction.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现有技术难以全面捕捉代码的结构和语义信息。 大多数深度学习模型针对特定任务和单一语言设计，缺乏灵活性。 以下未解问题可以作为未来的研究方向。
- en: Information capturing
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息捕捉
- en: Many approaches use structural information in code, however the majority of
    them just use structure information such as AST to capture syntax information.
    There are only a few approaches to learning the whole representation of code that
    combine structure and semantic information (such as DFG), and they are only applied
    to specific tasks, not all tasks. As a result, one of the goals of future study
    could be to improve the model’s ability to use the structure and semantic information
    of codes in various tasks.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法利用代码中的结构信息，但大多数方法仅使用结构信息（如 AST）来捕捉语法信息。 只有少数方法将结构和语义信息（如 DFG）结合来学习代码的整体表示，并且它们仅适用于特定任务，而不是所有任务。
    因此，未来研究的目标之一可能是提高模型在各种任务中使用代码结构和语义信息的能力。
- en: Flexibility
  id: totrans-453
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 灵活性
- en: The methods we described above are suitable to a certain task or dataset and
    lack flexibility. The model’s flexibility implies it may be utilized in a range
    of scenarios, including those using datasets from shorter programs (with small
    graph structures) or smaller sample sizes, as well as scenarios involving several
    downstream jobs or programming languages. As a result, future study could concentrate
    on how to train a model that can accommodate a variety of circumstances.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上述描述的方法适用于特定任务或数据集，但缺乏灵活性。模型的灵活性意味着它可以在多种场景中使用，包括那些使用较短程序的数据集（具有小的图结构）或样本较小的场景，以及涉及多个下游任务或编程语言的场景。因此，未来的研究可以集中于如何训练能够适应各种情况的模型。
- en: Model Simplicity
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型简洁性
- en: Recent models, particularly those based on Transformer and its derivatives,
    have improved performance, but they often need more effort and machine capability.
    Therefore, it is necessary to propose lightweight model under the premise of ensuring
    accuracy.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的模型，特别是基于 Transformer 及其衍生模型的模型，性能有所提升，但通常需要更多的努力和机器能力。因此，在确保准确性的前提下，提出轻量级模型是必要的。
- en: Explainability
  id: totrans-457
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 可解释性
- en: Deep learning approaches have always had explainability issues, and the approaches
    for code are no exception. While the current usage of attentional mechanisms in
    code generation tasks can explain the origins of token generation, there are still
    no good explanations for other tasks like safety analysis. Simultaneously, the
    model’s explainability is very useful in determining structural information and
    producing superior metrics in the code. As a result, additional research into
    the explainability of code models is still worthwhile.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法一直存在可解释性问题，而代码的情况也不例外。虽然当前在代码生成任务中使用的注意机制可以解释标记生成的来源，但对于其他任务，如安全分析，仍然没有很好的解释。同时，模型的可解释性在确定结构信息和生成更优指标方面非常有用。因此，对代码模型可解释性的进一步研究仍然值得进行。
- en: Robustness
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 鲁棒性
- en: The robustness of code-domain models has not yet been studied, but as model
    performance improves, the robustness of code-domain models is sure to become a
    hot study area in the future. In the code domain, both sequence-based and graph-based
    models rely on the representation of code tokens to some extent, which leads to
    model performance reduction when test code fragments are not represented in the
    same way as training code fragments (e.g., different representations of API calls
    in the python language). Furthermore, while code graph-based models can better
    capture the information of code fragments, graph structures are sensitive to attacks.
    There have been numerous techniques to explore the robustness of graph models,
    and how to convert them to work on code graphs is an important research field.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 代码领域模型的鲁棒性尚未被研究，但随着模型性能的提高，代码领域模型的鲁棒性肯定会成为未来的热门研究领域。在代码领域中，无论是基于序列的模型还是基于图的模型，都在一定程度上依赖于代码标记的表示，这导致当测试代码片段的表示方式与训练代码片段不同时（例如，Python语言中API调用的不同表示），模型性能下降。此外，虽然代码图模型可以更好地捕捉代码片段的信息，但图结构对攻击很敏感。已经有很多技术探讨图模型的鲁棒性，以及如何将其转换以适用于代码图是一个重要的研究领域。
- en: Metrics
  id: totrans-461
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指标
- en: 'In the previous section, the metrics for evaluate the effectiveness of the
    code representation are based on the the Natural Language Processing area and
    Information Retrieval area. Although there are metrics designed specifically for
    code representation, there is a small amount of number of metrics proposed to
    suit the code data and downstream tasks. The following are the potential directions
    that can be studied:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，用于评估代码表示效果的指标基于自然语言处理领域和信息检索领域。尽管有些指标是专门为代码表示设计的，但针对代码数据和下游任务提出的指标数量较少。以下是可能的研究方向：
- en: 1) Measure of information The requirement for appropriate measures for the information
    used in deep learning models is growing as the variety of structures employed
    in learning the representation for codes increases. Recent metrics that measure
    how models utilise this information in these structures are performed after the
    downstream tasks, however measures directly during the code representation stage
    have never been provided.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 信息度量 随着用于学习代码表示的结构多样性增加，对适当的信息度量的需求也在增长。最近的度量方法在下游任务之后测量模型如何利用这些结构中的信息，但在代码表示阶段直接提供的度量方法尚未出现。
- en: 2) Measure of Explainability As previous mentioned, generating better metrics
    for measuring model efficacy on downstream tasks is vital for the explainability
    of models, which can better qualify how the model works.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 可解释性的度量 如前所述，为下游任务生成更好的模型效能度量对于模型的可解释性至关重要，这可以更好地定量模型的工作方式。
- en: 3) Measure for Bias Problem The code data will inevitably contains repeat and
    duplication, which might contribute to a bias problem. As far as we know, there
    have been few studies and discussions on the bias problem in code representation.
    Therefore, it is a new future direction to consider the bias in code data, while
    reasonable metrics for measuring the bias of the models in code are required.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 偏见问题的度量 代码数据不可避免地包含重复和冗余，这可能导致偏见问题。就我们所知，关于代码表示中的偏见问题的研究和讨论还很少。因此，考虑代码数据中的偏见是一个新的未来方向，同时需要合理的度量标准来衡量代码模型的偏见。
- en: 9\. Conclusion
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: 'It is critical to understand the structural and semantic meaning of codes when
    working on intelligent software. In this survey, We give a comprehensive overview
    of structure-based methods to code representation learning in recent years, which
    we divide into two groups: sequence-based models and graph-based models, then
    summarize and compare the methods in each group. The downstream tasks, as well
    as metrics and datasets, are also introduced here. It is shown that deep learning
    models are useful in code understanding, and further multiple downstream tasks.
    However, the field of code comprehension is still in its infancy, with numerous
    obstacles and unsolved issues. Finally, as future directions for code understanding,
    we offer four open questions.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在从事智能软件开发时，理解代码的结构和语义含义至关重要。在这项调查中，我们对近年来结构化方法进行代码表示学习进行了全面概述，将其分为两组：基于序列的模型和基于图的模型，然后总结和比较每组中的方法。下游任务以及度量标准和数据集也在这里介绍。研究表明，深度学习模型在代码理解和进一步的多个下游任务中是有用的。然而，代码理解领域仍处于起步阶段，面临着众多障碍和未解决的问题。最后，作为未来代码理解的方向，我们提出了四个开放性问题。
- en: Acknowledgements.
  id: totrans-468
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: References
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abdelaziz et al. (2021) Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, and
    Kavitha Srinivas. 2021. Can Machines Read Coding Manuals Yet? – A Benchmark for
    Building Better Language Models for Code Understanding. *arXiv: Computation and
    Language* (2021).'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdelaziz et al. (2021) Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, 和
    Kavitha Srinivas. 2021. 机器能否阅读编码手册？ – 构建更好的代码理解语言模型的基准。*arXiv: 计算与语言* (2021)。'
- en: Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2020. A transformer-based approach for source code summarization.
    *arXiv preprint arXiv:2005.00653* (2020).
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, 和 Kai-Wei
    Chang. 2020. 基于变换器的源代码总结方法。*arXiv预印本 arXiv:2005.00653* (2020)。
- en: Allamanis (2019) Miltiadis Allamanis. 2019. The adverse effects of code duplication
    in machine learning models of code. In *Proceedings of the 2019 ACM SIGPLAN International
    Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software*.
    143–153.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis (2019) Miltiadis Allamanis. 2019. 代码重复对机器学习模型的负面影响。在*2019年ACM SIGPLAN国际新思想、新范式和编程与软件反思研讨会论文集*中，143–153。
- en: Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    and Charles Sutton. 2018a. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)* 51, 4 (2018), 1–37.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    和 Charles Sutton. 2018a. 大代码和自然性机器学习的调查。*ACM计算调查（CSUR）* 51, 4 (2018), 1–37。
- en: Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud
    Khademi. 2018b. Learning to represent programs with graphs. In *6th International
    Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings*.
    arXiv:1711.00740 [https://github.com/Microsoft/gated-graph-neural-network-samples](https://github.com/Microsoft/gated-graph-neural-network-samples)
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, 和 Mahmoud Khademi.
    2018b. 学习用图表示程序。在*第六届国际学习表征会议，ICLR 2018 - 会议论文集*中。arXiv:1711.00740 [https://github.com/Microsoft/gated-graph-neural-network-samples](https://github.com/Microsoft/gated-graph-neural-network-samples)
- en: Allamanis et al. (2021) Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt.
    2021. Self-Supervised Bug Detection and Repair. *Advances in Neural Information
    Processing Systems* 34 (2021).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis et al. (2021) Miltiadis Allamanis, Henry Jackson-Flux, 和 Marc Brockschmidt.
    2021. 自监督错误检测与修复。*神经信息处理系统进展* 34 (2021)。
- en: 'Allamanis et al. (2016) Miltiadis Allamanis, Hao Peng, and Charles Sutton.
    2016. A Convolutional Attention Network for Extreme Summarization of Source Code.
    *arXiv: Learning* (2016).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allamanis 等 (2016) Miltiadis Allamanis, Hao Peng 和 Charles Sutton。2016. 用于极端总结源代码的卷积注意力网络。*arXiv:
    学习* (2016)。'
- en: 'Alon et al. (2018a) Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018a.
    code2seq: Generating Sequences from Structured Representations of Code. *arXiv:
    Learning* (2018).'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alon 等 (2018a) Uri Alon, Shaked Brody, Omer Levy 和 Eran Yahav。2018a. code2seq:
    从结构化表示生成序列。*arXiv: 学习* (2018)。'
- en: 'Alon et al. (2019a) Uri Alon, Omer Levy, Shaked Brody, and Eran Yahav. 2019a.
    Code2Seq: Generating sequences from structured representations of code. *7th International
    Conference on Learning Representations, ICLR 2019* 1 (2019), 1–22. arXiv:1808.01400'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alon 等 (2019a) Uri Alon, Omer Levy, Shaked Brody 和 Eran Yahav。2019a. Code2Seq:
    从结构化表示生成代码序列。*第七届国际学习表示会议，ICLR 2019* 1 (2019), 1–22. arXiv:1808.01400'
- en: Alon et al. (2020) Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural
    language models of code. In *International Conference on Machine Learning*. PMLR,
    245–256.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等 (2020) Uri Alon, Roy Sadaka, Omer Levy 和 Eran Yahav。2020. 代码的结构化语言模型。发表于
    *国际机器学习会议*。PMLR, 245–256.
- en: Alon et al. (2018b) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2018b. A General Path-Based Representation for Predicting Program Properties.
    *ACM SIGPLAN Notices* 53, 4 (mar 2018), 404–419. [https://doi.org/10.1145/3192366.3192412](https://doi.org/10.1145/3192366.3192412)
    arXiv:1803.09544
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等 (2018b) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2018b. 一种通用的基于路径的表示法，用于预测程序属性。*ACM
    SIGPLAN 通告* 53, 4 (2018年3月), 404–419. [https://doi.org/10.1145/3192366.3192412](https://doi.org/10.1145/3192366.3192412)
    arXiv:1803.09544
- en: Alon et al. (2018c) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2018c. A general path-based representation for predicting program properties.
    In *Programming Language Design and Implementation*.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alon 等 (2018c) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2018c. 一种通用的基于路径的表示法，用于预测程序属性。发表于
    *编程语言设计与实现*。
- en: 'Alon et al. (2019b) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019b. Code2Vec: Learning Distributed Representations of Code. *Proceedings of
    the ACM on Programming Languages* 3, POPL (2019), 1–29. [https://doi.org/10.1145/3290353](https://doi.org/10.1145/3290353)
    arXiv:1803.09473'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alon 等 (2019b) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2019b.
    Code2Vec: 学习代码的分布式表示。*ACM 编程语言学会会议录* 3, POPL (2019), 1–29. [https://doi.org/10.1145/3290353](https://doi.org/10.1145/3290353)
    arXiv:1803.09473'
- en: 'Alon et al. (2019c) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019c. code2vec: learning distributed representations of code.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alon 等 (2019c) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2019c.
    code2vec: 学习代码的分布式表示。'
- en: 'Alreshedy et al. (2018) Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German,
    Venkatesh Srinivasan, and T. Aaron Gulliver. 2018. SCC: Automatic Classification
    of Code Snippets. *arXiv: Software Engineering* (2018).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alreshedy 等 (2018) Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German,
    Venkatesh Srinivasan 和 T. Aaron Gulliver。2018. SCC: 自动分类代码片段。*arXiv: 软件工程* (2018)。'
- en: 'Amodio et al. (2017) Matthew Amodio, Swarat Chaudhuri, and Thomas Reps. 2017.
    Neural Attribute Machines for Program Generation. *arXiv: Artificial Intelligence*
    (2017).'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amodio 等 (2017) Matthew Amodio, Swarat Chaudhuri 和 Thomas Reps。2017. 用于程序生成的神经属性机器。*arXiv:
    人工智能* (2017)。'
- en: Arbaaz Qureshi et al. ([n.d.]) Syed Arbaaz Qureshi, Sonu Mehta, Ranjita Bhagwan,
    and Rahul Kumar. [n.d.]. Assessing the Effectiveness of Syntactic Structure to
    Learn Code Edit Representations. ([n. d.]). arXiv:2106.06110v1
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbaaz Qureshi 等 ([n.d.]) Syed Arbaaz Qureshi, Sonu Mehta, Ranjita Bhagwan 和
    Rahul Kumar。[n.d.]. 评估句法结构在学习代码编辑表示中的有效性。 ([n. d.]). arXiv:2106.06110v1
- en: Arkesteijn and Saldanha ([n.d.]) Youri Arkesteijn and Nikhil Saldanha. [n.d.].
    Code Completion using Neural AAention and Byte Pair Encoding. ([n. d.]). arXiv:2004.06343v1
    [www.sri.inf.ethz.ch/py150](www.sri.inf.ethz.ch/py150)
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arkesteijn 和 Saldanha ([n.d.]) Youri Arkesteijn 和 Nikhil Saldanha。[n.d.]. 使用神经注意力和字节对编码的代码完成。
    ([n. d.]). arXiv:2004.06343v1 [www.sri.inf.ethz.ch/py150](www.sri.inf.ethz.ch/py150)
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. *arXiv
    preprint arXiv:1409.0473* (2014).
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 (2014) Dzmitry Bahdanau, Kyunghyun Cho 和 Yoshua Bengio。2014. 通过共同学习对齐和翻译的神经机器翻译。*arXiv
    预印本 arXiv:1409.0473* (2014)。
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
    In *Meeting of the Association for Computational Linguistics*.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee 和 Lavie (2005) Satanjeev Banerjee 和 Alon Lavie。2005. METEOR: 一种改进与人工判断相关性的自动机器翻译评估指标。发表于
    *计算语言学协会会议*。'
- en: 'Barbosa et al. (2019) Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini,
    Ricardo Britto, Frederico Soares, Solange Oliveira Rezende, Auri Marcelo Rizzo
    Vincenzi, and Márcio Eduardo Delamaro. 2019. BULNER: BUg Localization with word
    embeddings and NEtwork Regularization. *arXiv: Software Engineering* (2019).'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Barbosa 等（2019）Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini, Ricardo
    Britto, Frederico Soares, Solange Oliveira Rezende, Auri Marcelo Rizzo Vincenzi
    和 Márcio Eduardo Delamaro. 2019. BULNER: 使用词嵌入和网络正则化进行BUG定位。*arXiv: 软件工程*（2019）。'
- en: Bellon et al. (2007) Stefan Bellon, Rainer Koschke, Giulio Antoniol, Jens Krinke,
    and Ettore Merlo. 2007. Comparison and Evaluation of Clone Detection Tools. *IEEE
    Transactions on Software Engineering* 33, 9 (2007), 577–591. [https://doi.org/10.1109/TSE.2007.70725](https://doi.org/10.1109/TSE.2007.70725)
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellon 等（2007）Stefan Bellon, Rainer Koschke, Giulio Antoniol, Jens Krinke 和
    Ettore Merlo. 2007. 克隆检测工具的比较与评估。*IEEE软件工程学报* 33, 9（2007），577–591。 [https://doi.org/10.1109/TSE.2007.70725](https://doi.org/10.1109/TSE.2007.70725)
- en: 'Ben-Nun et al. (2018a) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
    Hoefler. 2018a. Neural Code Comprehension: A Learnable Representation of Code
    Semantics. *arXiv: Learning* (2018).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben-Nun 等（2018a）Tal Ben-Nun, Alice Shoshana Jakobovits 和 Torsten Hoefler. 2018a.
    神经代码理解：可学习的代码语义表示。*arXiv: 学习*（2018）。'
- en: 'Ben-Nun et al. (2018b) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
    Hoefler. 2018b. Neural Code Comprehension: A Learnable Representation of Code
    Semantics. *Advances in Neural Information Processing Systems* 2018-December (jun
    2018), 3585–3597. arXiv:1806.07336 [https://arxiv.org/abs/1806.07336v3](https://arxiv.org/abs/1806.07336v3)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-Nun 等（2018b）Tal Ben-Nun, Alice Shoshana Jakobovits 和 Torsten Hoefler. 2018b.
    神经代码理解：可学习的代码语义表示。*神经信息处理系统进展* 2018年12月（2018年6月），3585–3597。arXiv:1806.07336 [https://arxiv.org/abs/1806.07336v3](https://arxiv.org/abs/1806.07336v3)
- en: Bengio (2009) Yoshua Bengio. 2009. Learning Deep Architectures for AI.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio（2009）Yoshua Bengio. 2009. 深度架构学习与人工智能。
- en: 'Berabi et al. (2021) Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin
    Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.
    In *International Conference on Machine Learning*.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Berabi 等（2021）Berkay Berabi, Jingxuan He, Veselin Raychev 和 Martin Vechev.
    2021. TFix: 使用文本到文本的变换器学习修复编码错误。在 *国际机器学习会议*。'
- en: 'Bhoopchand et al. (2016) Avishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr,
    and Sebastian Riedel. 2016. Learning Python Code Suggestion with a Sparse Pointer
    Network. *arXiv: Neural and Evolutionary Computing* (2016).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhoopchand 等（2016）Avishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr 和 Sebastian
    Riedel. 2016. 使用稀疏指针网络学习Python代码建议。*arXiv: 神经与进化计算*（2016）。'
- en: Brauckmann et al. (2020) Alexander Brauckmann, Andrés Goens, Sebastian Ertel,
    and Jeronimo Castrillon. 2020. Compiler-based graph representations for deep learning
    models of code. In *Proceedings of the 29th International Conference on Compiler
    Construction*. 201–211.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brauckmann 等（2020）Alexander Brauckmann, Andrés Goens, Sebastian Ertel 和 Jeronimo
    Castrillon. 2020. 基于编译器的图表示法用于代码的深度学习模型。在 *第29届国际编译器构造会议论文集*。201–211。
- en: Brockschmidt et al. (2019) Marc Brockschmidt, Miltiadis Allamanis, Alexander
    Gaunt, and Oleksandr Polozov. 2019. Generative code modeling with graphs. In *7th
    International Conference on Learning Representations, ICLR 2019*. International
    Conference on Learning Representations, ICLR. arXiv:1805.08490 [https://arxiv.org/abs/1805.08490v2](https://arxiv.org/abs/1805.08490v2)
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brockschmidt 等（2019）Marc Brockschmidt, Miltiadis Allamanis, Alexander Gaunt
    和 Oleksandr Polozov. 2019. 通过图进行生成式代码建模。在 *第七届国际学习表征会议，ICLR 2019*。国际学习表征会议，ICLR。arXiv:1805.08490
    [https://arxiv.org/abs/1805.08490v2](https://arxiv.org/abs/1805.08490v2)
- en: 'Cahyawijaya et al. (2021) Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,
    Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim,
    Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG:
    Benchmark and Resources for Evaluating Indonesian Natural Language Generation.
    *arXiv: Computation and Language* (2021).'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cahyawijaya 等（2021）Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa
    Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri
    Bahar, Masayu Leylia Khodra, Ayu Purwarianti 和 Pascale Fung. 2021. IndoNLG: 评估印尼自然语言生成的基准与资源。*arXiv:
    计算与语言*（2021）。'
- en: Cambronero et al. (2019a) José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik
    Sen, and Satish Chandra. 2019a. When deep learning met code search. In *FSE*.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cambronero 等（2019a）José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen
    和 Satish Chandra. 2019a. 当深度学习遇上代码搜索。在 *FSE*。
- en: Cambronero et al. (2019b) José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik
    Sen, and Satish Chandra. 2019b. When deep learning met code search. In *Foundations
    of Software Engineering*.
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cambronero 等（2019b）José Pablo Cambronero、Hongyu Li、Seohyun Kim、Koushik Sen 和
    Satish Chandra。2019b。当深度学习遇上代码搜索。在 *软件工程基础*。
- en: 'Chakraborty et al. (2020) Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis,
    and Baishakhi Ray. 2020. CODIT: Code Editing with Tree-Based Neural Models. *IEEE
    Transactions on Software Engineering* TBD (sep 2020), 1–1. [https://doi.org/10.1109/tse.2020.3020502](https://doi.org/10.1109/tse.2020.3020502)
    arXiv:1810.00314'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty 等（2020）Saikat Chakraborty、Yangruibo Ding、Miltiadis Allamanis 和 Baishakhi
    Ray。2020。CODIT：基于树的神经模型代码编辑。*IEEE 软件工程学报* TBD（2020年9月），1–1。 [https://doi.org/10.1109/tse.2020.3020502](https://doi.org/10.1109/tse.2020.3020502)
    arXiv:1810.00314
- en: 'Chen et al. (2019a) Deyu Chen, Xiang Chen, Hao Li, Junfeng Xie, and Yanzhou
    Mu. 2019a. DeepCPDP: Deep Learning Based Cross-Project Defect Prediction. *IEEE
    Access* 7 (2019), 184832–184848.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019a）陈德宇、陈翔、李浩、谢俊锋 和 穆燕洲。2019a。DeepCPDP：基于深度学习的跨项目缺陷预测。*IEEE Access* 7（2019），184832–184848。
- en: Chen et al. (2019b) Long Chen, Wei Ye, and Shikun Zhang. 2019b. Capturing source
    code semantics via tree-based convolution over API-enhanced AST. In *Proceedings
    of the 16th ACM International Conference on Computing Frontiers*. 174–182.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2019b）陈龙、叶伟和张世坤。2019b。通过基于树的卷积捕捉源代码语义，应用于增强 API 的 AST。在 *第16届 ACM 国际计算前沿会议论文集*。174–182。
- en: 'Chen et al. (2021) Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2021. SEQUENCER: Sequence-to-Sequence
    Learning for End-to-End Program Repair. *IEEE Transactions on Software Engineering*
    47 (2021), 1943–1959.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）陈子敏、Steve Kommrusch、Michele Tufano、Louis-Noël Pouchet、Denys Poshyvanyk
    和 Martin Monperrus。2021。SEQUENCER：端到端程序修复的序列到序列学习。*IEEE 软件工程学报* 47（2021），1943–1959。
- en: Chen and Monperrus (2019) Zimin Chen and Martin Monperrus. 2019. A Literature
    Study of Embeddings on Source Code. (2019), 1–8. arXiv:1904.03061 [http://arxiv.org/abs/1904.03061](http://arxiv.org/abs/1904.03061)
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈和 Monperrus（2019）陈子敏 和 Martin Monperrus。2019。源代码嵌入的文献研究。（2019），1–8。arXiv:1904.03061
    [http://arxiv.org/abs/1904.03061](http://arxiv.org/abs/1904.03061)
- en: Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
    representations using RNN encoder-decoder for statistical machine translation.
    *arXiv preprint arXiv:1406.1078* (2014).
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014）Kyung-hyun Cho、Bart Van Merriënboer、Caglar Gulcehre、Dzmitry Bahdanau、Fethi
    Bougares、Holger Schwenk 和 Yoshua Bengio。2014。使用 RNN 编码器-解码器学习短语表示，用于统计机器翻译。*arXiv
    预印本 arXiv:1406.1078*（2014）。
- en: 'Compton et al. (2020) Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay.
    2020. Embedding Java Classes with code2vec: Improvements from Variable Obfuscation.
    In *Mining Software Repositories*.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Compton 等（2020）Rhys Compton、Eibe Frank、Panos Patros 和 Abigail Koay。2020。使用 code2vec
    嵌入 Java 类：变量混淆带来的改进。在 *挖掘软件库*。
- en: Cvitkovic et al. (2019) Milan Cvitkovic, Badal Singh, and Anima Anandkumar.
    2019. Open vocabulary learning on source code with a graph-structured cache. In
    *36th International Conference on Machine Learning, ICML 2019*, Vol. 2019-June.
    International Machine Learning Society (IMLS), 2662–2674. arXiv:1810.08305 [https://arxiv.org/abs/1810.08305v2](https://arxiv.org/abs/1810.08305v2)
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cvitkovic 等（2019）Milan Cvitkovic、Badal Singh 和 Anima Anandkumar。2019。在源代码上进行开放词汇学习，使用图结构缓存。在
    *第36届国际机器学习会议，ICML 2019*，第2019年6月卷。国际机器学习学会（IMLS），2662–2674。arXiv:1810.08305 [https://arxiv.org/abs/1810.08305v2](https://arxiv.org/abs/1810.08305v2)
- en: 'Dai et al. (2020) Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. 2020.
    Funnel-transformer: Filtering out sequential redundancy for efficient language
    processing. *Advances in Neural Information Processing Systems* 2020-Decem (2020),
    1–19. arXiv:2006.03236'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2020）Zihang Dai、Guokun Lai、Yiming Yang 和 Quoc V. Le。2020。Funnel-transformer：过滤顺序冗余以提高语言处理效率。*神经信息处理系统进展*
    2020年12月（2020），1–19。arXiv:2006.03236
- en: Dam et al. (2016) Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A deep language
    model for software code. *arXiv preprint arXiv:1608.02715* (2016).
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dam 等（2016）Hoa Khanh Dam、Truyen Tran 和 Trang Pham。2016。一种深度语言模型用于软件代码。*arXiv
    预印本 arXiv:1608.02715*（2016）。
- en: 'de Rezende Martins and Gerosa (2020) Marcelo de Rezende Martins and Marco Aurélio
    Gerosa. 2020. CoNCRA: A Convolutional Neural Network Code Retrieval Approach.
    *arXiv: Learning* (2020).'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'de Rezende Martins 和 Gerosa（2020）Marcelo de Rezende Martins 和 Marco Aurélio
    Gerosa。2020。CoNCRA：一种卷积神经网络代码检索方法。*arXiv: 学习*（2020）。'
- en: 'Denil et al. (2014) Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom,
    and Nando de Freitas. 2014. Modelling‚ Visualising and Summarising Documents with
    a Single Convolutional Neural Network. *arXiv: Computation and Language* (2014).'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Denil 等人 (2014) 米沙·德尼尔、阿尔班·德米拉伊、纳尔·卡尔赫布伦纳、菲尔·布伦索姆、和南多·德·弗雷塔斯。2014年。用单个卷积神经网络建模、可视化和总结文档。*arXiv:
    计算与语言* (2014)。'
- en: 'Dinella et al. (2020) Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik,
    Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect
    and fix bugs in programs. In *International Conference on Learning Representations
    (ICLR)*.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dinella 等人 (2020) 伊丽莎白·迪内拉、韩军·戴、李子阳、梅尤尔·奈克、乐松、和柯·王。2020年。Hoppity: 学习图转换以检测和修复程序中的错误。发表于
    *国际学习表示会议 (ICLR)*。'
- en: 'Ding et al. (2019) Steven H.H. Ding, Benjamin C.M. Fung, and Philippe Charland.
    2019. Asm2Vec: Boosting static representation robustness for binary clone search
    against code obfuscation and compiler optimization. *Proceedings - IEEE Symposium
    on Security and Privacy* 2019-May (2019), 472–489. [https://doi.org/10.1109/SP.2019.00003](https://doi.org/10.1109/SP.2019.00003)'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding 等人 (2019) 史蒂文·H.H. 丁、本杰明·C.M. 丰、和菲利普·查兰德。2019年。Asm2Vec: 提升静态表示的鲁棒性以应对代码混淆和编译器优化。*IEEE安全与隐私研讨会论文集*
    2019年5月 (2019), 472–489。 [https://doi.org/10.1109/SP.2019.00003](https://doi.org/10.1109/SP.2019.00003)'
- en: 'Duan et al. (2020) Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. 2020.
    Deepbindiff: Learning program-wide code representations for binary diffing. In
    *Network and Distributed System Security Symposium*.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan 等人 (2020) 岳端、李雪玺、王晶汉、和尹恒。2020年。Deepbindiff: 学习程序级代码表示以进行二进制差异比较。发表于 *网络与分布式系统安全研讨会*。'
- en: Fang et al. (2021) Sen Fang, You-Shuai Tan, Tao Zhang, and Yepang Liu. 2021.
    Self-Attention Networks for Code Search. *Information & Software Technology* 134
    (2021), 106542.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等人 (2021) 冯森、尤帅·谭、陶张、和叶胖·刘。2021年。用于代码搜索的自注意力网络。*信息与软件技术* 134 (2021), 106542。
- en: Feng et al. (2016a) Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian
    Testa, and Heng Yin. 2016a. Scalable graph-based bug search for firmware images.
    In *Proceedings of the ACM Conference on Computer and Communications Security*,
    Vol. 24-28-Octo. 480–491. [https://doi.org/10.1145/2976749.2978370](https://doi.org/10.1145/2976749.2978370)
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 (2016a) 龚乾、周润东、徐成成、程耀、布赖恩·泰斯塔、和尹恒。2016a。针对固件镜像的可扩展图基错误搜索。发表于 *ACM计算机与通信安全会议论文集*，24-28十月卷。480–491。
    [https://doi.org/10.1145/2976749.2978370](https://doi.org/10.1145/2976749.2978370)
- en: Feng et al. (2016b) Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian
    Testa, and Heng Yin. 2016b. Scalable Graph-based Bug Search for Firmware Images.
    In *Computer and Communications Security*.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 (2016b) 龚乾、周润东、徐成成、程耀、布赖恩·泰斯塔、和尹恒。2016b。针对固件镜像的可扩展图基错误搜索。发表于 *计算机与通信安全*。
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
    2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. *arXiv:
    Computation and Language* (2020).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等人 (2020) 张银峰、达亚·郭、杜雨堂、段楠、肖成峰、明功、林君寿、秦冰、刘婷、姜达新、和明周。2020年。CodeBERT: 一种用于编程和自然语言的预训练模型。*arXiv:
    计算与语言* (2020)。'
- en: Fernandes et al. (2019) Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt.
    2019. Structured neural summarization. In *7th International Conference on Learning
    Representations, ICLR 2019*. International Conference on Learning Representations,
    ICLR. arXiv:1811.01824 [https://arxiv.org/abs/1811.01824v4](https://arxiv.org/abs/1811.01824v4)
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes 等人 (2019) 帕特里克·费尔南德斯、米尔提亚迪斯·阿拉曼尼斯、和马克·布罗克施密特。2019年。结构化神经摘要。发表于 *第七届国际学习表示会议,
    ICLR 2019*。国际学习表示会议，ICLR。arXiv:1811.01824 [https://arxiv.org/abs/1811.01824v4](https://arxiv.org/abs/1811.01824v4)
- en: Ferrante et al. (1987a) Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren.
    1987a. The program dependence graph and its use in optimization. *ACM Transactions
    on Programming Languages and Systems (TOPLAS)* 9, 3 (1987), 319–349.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrante 等人 (1987a) 简·费兰特、卡尔·J·奥滕斯坦、和乔·D·沃伦。1987a。程序依赖图及其在优化中的应用。*ACM编程语言与系统交易
    (TOPLAS)* 9, 3 (1987), 319–349。
- en: Ferrante et al. (1987b) Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren.
    1987b. The program dependence graph and its use in optimization. *ACM Transactions
    on Programming Languages and Systems (TOPLAS)* 9, 3 (jul 1987), 319–349. [https://doi.org/10.1145/24039.24041](https://doi.org/10.1145/24039.24041)
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrante 等人 (1987b) 简·费兰特、卡尔·J·奥滕斯坦、和乔·D·沃伦。1987b。程序依赖图及其在优化中的应用。*ACM编程语言与系统交易
    (TOPLAS)* 9, 3 (1987年7月), 319–349。 [https://doi.org/10.1145/24039.24041](https://doi.org/10.1145/24039.24041)
- en: Gibert et al. (2019) Daniel Gibert, Carles Mateu, Jordi Planes, and Ramon Vicens.
    2019. Using convolutional neural networks for classification of malware represented
    as images. *Journal of Computer Virology and Hacking Techniques* 15 (2019), 15–28.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gibert 等 (2019) Daniel Gibert, Carles Mateu, Jordi Planes, 和 Ramon Vicens. 2019.
    使用卷积神经网络对表示为图像的恶意软件进行分类。*计算机病毒与黑客技术杂志* 15 (2019), 15–28。
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *34th International Conference on Machine Learning, ICML 2017*, Vol. 3\. 2053–2070.
    arXiv:1704.01212
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer 等 (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals,
    和 George E Dahl. 2017. 用于量子化学的神经消息传递。载于 *第34届国际机器学习大会, ICML 2017*，第3卷 2053–2070。arXiv:1704.01212
- en: Gu et al. (2018) Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code
    search. In *International Conference on Software Engineering*.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 (2018) Xiaodong Gu, Hongyu Zhang, 和 Sunghun Kim. 2018. 深度代码搜索。载于 *国际软件工程大会*。
- en: 'Guo et al. (2020) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert:
    Pre-training code representations with data flow. *arXiv preprint arXiv:2009.08366*
    (2020).'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 (2020) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, 等. 2020. Graphcodebert:
    使用数据流进行代码表示的预训练。*arXiv 预印本 arXiv:2009.08366* (2020)。'
- en: 'Gupta et al. (2017) Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade.
    2017. DeepFix: Fixing Common C Language Errors by Deep Learning. In *National
    Conference on Artificial Intelligence*.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gupta 等 (2017) Rahul Gupta, Soham Pal, Aditya Kanade, 和 Shirish Shevade. 2017.
    DeepFix: 通过深度学习修复常见的 C 语言错误。载于 *全国人工智能大会*。'
- en: 'Hasan et al. (2021) Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed
    Mehrab, Md Haque, Mahim Anjum, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal,
    and Rifat Shahriyar. 2021. CoDesc: A Large Code-Description Parallel Dataset.
    *arXiv preprint arXiv:2105.14220* (2021).'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hasan 等 (2021) Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed
    Mehrab, Md Haque, Mahim Anjum, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal,
    和 Rifat Shahriyar. 2021. CoDesc: 一个大型代码-描述平行数据集。*arXiv 预印本 arXiv:2105.14220* (2021)。'
- en: He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
    2020. Momentum Contrast for Unsupervised Visual Representation Learning. *Proceedings
    of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition*
    (2020), 9726–9735. [https://doi.org/10.1109/CVPR42600.2020.00975](https://doi.org/10.1109/CVPR42600.2020.00975)
    arXiv:1911.05722
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, 和 Ross Girshick. 2020.
    动量对比用于无监督视觉表示学习。*IEEE计算机视觉与模式识别会议论文集* (2020), 9726–9735。 [https://doi.org/10.1109/CVPR42600.2020.00975](https://doi.org/10.1109/CVPR42600.2020.00975)
    arXiv:1911.05722
- en: Hellendoorn et al. (2019) Vincent J Hellendoorn, Charles Sutton, Rishabh Singh,
    Petros Maniatis, and David Bieber. 2019. Global relational models of source code.
    In *International conference on learning representations*.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellendoorn 等 (2019) Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros
    Maniatis, 和 David Bieber. 2019. 源代码的全局关系模型。载于 *国际学习表征会议*。
- en: Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines
    to read and comprehend. *Advances in neural information processing systems* 28
    (2015), 1693–1701.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermann 等 (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse
    Espeholt, Will Kay, Mustafa Suleyman, 和 Phil Blunsom. 2015. 教授机器阅读和理解。*神经信息处理系统进展*
    28 (2015), 1693–1701。
- en: Hindle et al. (2012) Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and
    Premkumar Devanbu. 2012. On the naturalness of software. In *ICSE*.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hindle 等 (2012) Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, 和 Premkumar
    Devanbu. 2012. 软件的自然性。载于 *国际软件工程大会*。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-Term Memory. *Neural Computation* 9, 8 (1997), 1735–1780. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997.
    长短期记忆。*神经计算* 9, 8 (1997), 1735–1780。 [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
- en: Hu et al. (2020b) Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, and Mengting
    Yuan. 2020b. Neural joint attention code search over structure embeddings for
    software Q&A sites. *Journal of Systems and Software* 170 (2020), 110773.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2020b) Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, 和 Mengting Yuan.
    2020b. 基于结构嵌入的神经联合注意力代码搜索用于软件问答网站。*系统与软件杂志* 170 (2020), 110773。
- en: Hu et al. (2018) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep
    code comment generation. In *International Conference on Program Comprehension*.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2018) Xing Hu, Ge Li, Xin Xia, David Lo, 和 Zhi Jin. 2018. 深度代码注释生成。在*国际程序理解会议*中。
- en: Hu et al. (2020a) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020a. Deep
    code comment generation with hybrid lexical and syntactical information. *Empirical
    Software Engineering* 25 (2020), 2179–2217.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020a) Xing Hu, Ge Li, Xin Xia, David Lo, 和 Zhi Jin. 2020a. 使用混合词汇和句法信息进行深度代码注释生成。*实证软件工程*
    25 (2020), 2179–2217。
- en: 'Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu,
    Daxin Jiang, Ming Zhou, and Nan Duan. 2021. CoSQA: 20,000+ Web Queries for Code
    Search and Question Answering. *arXiv: Computation and Language* (2021).'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu,
    Daxin Jiang, Ming Zhou, 和 Nan Duan. 2021. CoSQA: 20,000+ 个用于代码搜索和问答的网络查询。*arXiv:计算与语言*（2021年）。'
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv preprint arXiv:1909.09436* (2019).'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    和 Marc Brockschmidt. 2019. Codesearchnet挑战：评估语义代码搜索的现状。*arXiv预印本arXiv:1909.09436*（2019年）。
- en: 'Hussain et al. (2020) Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang.
    2020. CodeGRU: Context-aware deep learning with gated recurrent unit for source
    code modeling. *Information & Software Technology* 125 (2020), 106309.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hussain et al. (2020) Yasir Hussain, Zhiqiu Huang, Yu Zhou, 和 Senzhang Wang.
    2020. CodeGRU: 使用门控递归单元进行上下文感知的深度学习以建模源代码。*信息与软件技术* 125 (2020), 106309。'
- en: 'Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2016. Summarizing source code using a neural attention model. In
    *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*. 2073–2083.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, 和 Luke Zettlemoyer.
    2016. 使用神经注意模型总结源代码。在*第54届计算语言学协会年会论文集（第1卷：长论文）*中。2073–2083。
- en: Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. In *Empirical
    Methods in Natural Language Processing*.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, 和 Luke Zettlemoyer.
    2018. 在编程上下文中将语言映射到代码。在*自然语言处理的实证方法*中。
- en: Ji et al. (2021b) Xiujuan Ji, Lei Liu, and Jingwen Zhu. 2021b. Code Clone Detection
    with Hierarchical Attentive Graph Embedding. *International Journal of Software
    Engineering and Knowledge Engineering* 31, 06 (2021), 837–861.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2021b) Xiujuan Ji, Lei Liu, 和 Jingwen Zhu. 2021b. 使用层次化注意图嵌入进行代码克隆检测。*软件工程与知识工程国际期刊*
    31, 06 (2021), 837–861。
- en: 'Ji et al. (2021a) Yuede Ji, Lei Cui, and H. Howie Huang. 2021a. BugGraph: Differentiating
    Source-Binary Code Similarity with Graph Triplet-Loss Network. *ASIA CCS 2021
    - Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security*
    1, c (2021), 702–715. [https://doi.org/10.1145/3433210.3437533](https://doi.org/10.1145/3433210.3437533)'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ji et al. (2021a) Yuede Ji, Lei Cui, 和 H. Howie Huang. 2021a. BugGraph: 用图三元组损失网络区分源代码与二进制代码的相似性。*ASIA
    CCS 2021 - 2021年ACM亚洲计算机与通信安全会议论文集* 1, c (2021), 702–715。 [https://doi.org/10.1145/3433210.3437533](https://doi.org/10.1145/3433210.3437533)'
- en: 'Jiang et al. (2021) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu.
    2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (may
    2021). arXiv:2105.12485 [https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485](https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485)'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, 和 Lei Lyu.
    2021. TreeBERT: 一种基于树的预训练编程语言模型。（2021年5月）。arXiv:2105.12485 [https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485](https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485)'
- en: Kalchbrenner et al. (2014) Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.
    2014. A Convolutional Neural Network for Modelling Sentences. In *ACL*.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalchbrenner et al. (2014) Nal Kalchbrenner, Edward Grefenstette, 和 Phil Blunsom.
    2014. 用于建模句子的卷积神经网络。在*ACL*中。
- en: Kanade et al. (2019) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2019. Learning and Evaluating Contextual Embedding of Source Code.
    (dec 2019). arXiv:2001.00059 [http://arxiv.org/abs/2001.00059](http://arxiv.org/abs/2001.00059)
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanade et al. (2019) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, 和 Kensen
    Shi. 2019. 学习和评估源代码的上下文嵌入。（2019年12月）。arXiv:2001.00059 [http://arxiv.org/abs/2001.00059](http://arxiv.org/abs/2001.00059)
- en: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code.
    In *International Conference on Machine Learning*.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanade 等人 (2020) Aditya Kanade、Petros Maniatis、Gogul Balakrishnan 和 Kensen Shi。2020。《学习和评估源代码的上下文嵌入》。在
    *国际机器学习大会*。
- en: Karaivanov et al. (2014) Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev.
    2014. Phrase-Based Statistical Translation of Programming Languages. In *SIGPLAN
    symposium on New ideas, new paradigms, and reflections on programming and software*.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karaivanov 等人 (2014) Svetoslav Karaivanov、Veselin Raychev 和 Martin Vechev。2014。《基于短语的编程语言统计翻译》。在
    *SIGPLAN 研讨会：新的想法、新的范式以及对编程和软件的反思*。
- en: 'Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, and Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary
    Models for Source Code. *Proceedings - International Conference on Software Engineering*
    (mar 2020), 1073–1085. [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342)
    arXiv:2003.07914v1'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karampatsis 等人 (2020) Rafael-Michael Karampatsis、Hlib Babii、Romain Robbes、Charles
    Sutton 和 Andrea Janes。2020。《大代码 != 大词汇量：源代码的开放词汇模型》。*会议录 - 国际软件工程大会* (2020年3月)，1073–1085.
    [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342)
    arXiv:2003.07914v1
- en: 'Kim et al. (2020) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2020. Code Prediction by Feeding Trees to Transformers. *arXiv: Software Engineering*
    (2020).'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 (2020) Seohyun Kim、Jinman Zhao、Yuchi Tian 和 Satish Chandra。2020。《通过将树结构输入到变换器中的代码预测》。*arXiv:
    软件工程* (2020)。'
- en: Kim et al. (2021) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2021. Code prediction by feeding trees to transformers. *Proceedings - International
    Conference on Software Engineering* 1 (2021), 150–162. [https://doi.org/10.1109/ICSE43902.2021.00026](https://doi.org/10.1109/ICSE43902.2021.00026)
    arXiv:2003.13848
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 (2021) Seohyun Kim、Jinman Zhao、Yuchi Tian 和 Satish Chandra。2021。《通过将树结构输入到变换器中的代码预测》。*会议录
    - 国际软件工程大会* 1 (2021)，150–162. [https://doi.org/10.1109/ICSE43902.2021.00026](https://doi.org/10.1109/ICSE43902.2021.00026)
    arXiv:2003.13848
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. arXiv:cs.LG/1609.02907
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 和 Welling (2017) Thomas N. Kipf 和 Max Welling。2017。《使用图卷积网络的半监督分类》。arXiv:cs.LG/1609.02907
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *International Symposium
    on Code Generation and Optimization, 2004\. CGO 2004.* IEEE, 75–86.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lattner 和 Adve (2004) Chris Lattner 和 Vikram Adve。2004。《LLVM：一个用于终身程序分析与转换的编译框架》。在
    *国际代码生成与优化研讨会，2004\. CGO 2004.* IEEE，75–86。
- en: 'Le et al. (2018) Quan Le, Oisín Boydell, Brian Mac Namee, and Mark Scanlon.
    2018. Deep learning at the shallow end: Malware classification for non-domain
    experts. *Digital Investigation* 26 (2018).'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等人 (2018) Quan Le、Oisín Boydell、Brian Mac Namee 和 Mark Scanlon。2018。《浅层深度学习：非领域专家的恶意软件分类》。*数字调查*
    26 (2018)。
- en: 'Le et al. (2020) Triet H.M. Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep
    Learning for Source Code Modeling and Generation: Models, Applications, and Challenges.
    *Comput. Surveys* 53, 3 (2020), 1–37. [https://doi.org/10.1145/3383458](https://doi.org/10.1145/3383458)
    arXiv:2002.05442'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等人 (2020) Triet H.M. Le、Hao Chen 和 Muhammad Ali Babar。2020。《源代码建模与生成的深度学习：模型、应用和挑战》。*计算机调查*
    53, 3 (2020), 1–37. [https://doi.org/10.1145/3383458](https://doi.org/10.1145/3383458)
    arXiv:2002.05442
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *IEEE
    International Conference on Program Comprehension*, Vol. 12\. IEEE Computer Society,
    184–195. [https://doi.org/10.1145/3387904.3389268](https://doi.org/10.1145/3387904.3389268)
    arXiv:2004.02843
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair 等人 (2020) Alexander LeClair、Sakib Haque、Lingfei Wu 和 Collin McMillan。2020。《通过图神经网络改进代码摘要》。在
    *IEEE 国际程序理解大会*，第12卷。IEEE计算机学会，184–195. [https://doi.org/10.1145/3387904.3389268](https://doi.org/10.1145/3387904.3389268)
    arXiv:2004.02843
- en: LeClair et al. (2019) Alexander LeClair, Siyuan Jiang, and Collin McMillan.
    2019. A neural model for generating natural language summaries of program subroutines.
    In *International Conference on Software Engineering*.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair 等人 (2019) Alexander LeClair、Siyuan Jiang 和 Collin McMillan。2019。《用于生成程序子例程自然语言总结的神经模型》。在
    *国际软件工程大会*。
- en: LeClair and McMillan (2019) Alexander LeClair and Collin McMillan. 2019. Recommendations
    for Datasets for Source Code Summarization. In *North American Chapter of the
    Association for Computational Linguistics*.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeClair 和 McMillan (2019) Alexander LeClair 和 Collin McMillan。2019。《源代码摘要数据集的推荐》。在
    *北美计算语言学协会年会*。
- en: Li et al. (2020) Bingzhuo Li, Chunyang Ye, Shouyang Guan, and Hui Zhou. 2020.
    Semantic Code Clone Detection Via Event Embedding Tree and GAT Network. *Proceedings
    - 2020 IEEE 20th International Conference on Software Quality, Reliability, and
    Security, QRS 2020* 3 (2020), 382–393. [https://doi.org/10.1109/QRS51102.2020.00057](https://doi.org/10.1109/QRS51102.2020.00057)
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2020) Bingzhuo Li, Chunyang Ye, Shouyang Guan, 和 Hui Zhou. 2020. 基于事件嵌入树和
    GAT 网络的语义代码克隆检测. *2020 IEEE 第20届国际软件质量、可靠性和安全会议论文集, QRS 2020* 3 (2020), 382–393.
    [https://doi.org/10.1109/QRS51102.2020.00057](https://doi.org/10.1109/QRS51102.2020.00057)
- en: 'Li et al. (2019a) Hongyu Li, Seohyun Kim, and Satish Chandra. 2019a. Neural
    Code Search Evaluation Dataset. *arXiv: Software Engineering* (2019).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019a) Hongyu Li, Seohyun Kim, 和 Satish Chandra. 2019a. 神经代码搜索评估数据集.
    *arXiv: 软件工程* (2019).'
- en: 'Li et al. (2017b) Jian Li, Yue Wang, Irwin King, and Michael R. Lyu. 2017b.
    Code Completion with Neural Attention and Pointer Networks. *arXiv: Computation
    and Language* (2017).'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2017b) Jian Li, Yue Wang, Irwin King, 和 Michael R. Lyu. 2017b. 使用神经注意力和指针网络的代码补全.
    *arXiv: 计算与语言* (2017).'
- en: Li et al. (2017c) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017c.
    Code Completion with Neural Attention and Pointer Networks. *IJCAI International
    Joint Conference on Artificial Intelligence* 2018-July (nov 2017), 4159–4165.
    [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2017c) Jian Li, Yue Wang, Michael R. Lyu, 和 Irwin King. 2017c. 使用神经注意力和指针网络的代码补全.
    *IJCAI 国际人工智能联合会议* 2018年7月 (2017年11月), 4159–4165. [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573
- en: Li et al. (2017d) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017d.
    Code Completion with Neural Attention and Pointer Networks. *IJCAI International
    Joint Conference on Artificial Intelligence* 2018-July (nov 2017), 4159–4165.
    [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573v2
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2017d) Jian Li, Yue Wang, Michael R. Lyu, 和 Irwin King. 2017d. 使用神经注意力和指针网络的代码补全.
    *IJCAI 国际人工智能联合会议* 2018年7月 (2017年11月), 4159–4165. [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573v2
- en: 'Li et al. (2017a) Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara
    Ryder. 2017a. Cclearner: A deep learning-based clone detection approach. In *2017
    IEEE International Conference on Software Maintenance and Evolution (ICSME)*.
    IEEE, 249–260.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2017a) Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, 和 Barbara Ryder.
    2017a. Cclearner: 一种基于深度学习的克隆检测方法. 收录于 *2017 IEEE 国际软件维护与演化会议 (ICSME)*. IEEE,
    249–260.'
- en: 'Li et al. (2022) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang,
    Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022. CodeRetriever:
    Unimodal and Bimodal Contrastive Learning. *arXiv preprint arXiv:2201.10866* (2022).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun
    Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, 和 Nan Duan. 2022. CodeRetriever: 单模态和双模态对比学习.
    *arXiv 预印本 arXiv:2201.10866* (2022).'
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, 和 Richard Zemel. 2015.
    门控图序列神经网络. *arXiv 预印本 arXiv:1511.05493* (2015).
- en: Li et al. (2019b) Yi Li, Shaohua Wang, Tien N. Nguyen, and Son Van Nguyen. 2019b.
    Improving bug detection via context-based code representation learning and attention-based
    neural networks. *Proceedings of the ACM on Programming Languages* 3, OOPSLA (oct
    2019), 30. [https://doi.org/10.1145/3360588](https://doi.org/10.1145/3360588)
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2019b) Yi Li, Shaohua Wang, Tien N. Nguyen, 和 Son Van Nguyen. 2019b.
    通过基于上下文的代码表示学习和基于注意力的神经网络改进错误检测. *ACM 编程语言会议论文集* 3, OOPSLA (2019年10月), 30. [https://doi.org/10.1145/3360588](https://doi.org/10.1145/3360588)
- en: Li et al. (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow.
    2016. Gated graph sequence neural networks. *4th International Conference on Learning
    Representations, ICLR 2016 - Conference Track Proceedings* 1 (2016), 1–20. arXiv:1511.05493
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, 和 Daniel Tarlow. 2016.
    门控图序列神经网络. *第4届国际学习表征会议, ICLR 2016 - 会议论文集* 1 (2016), 1–20. arXiv:1511.05493
- en: 'Li et al. (2018) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan
    Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-Based System
    for Vulnerability Detection. (jan 2018). [https://doi.org/10.14722/ndss.2018.23158](https://doi.org/10.14722/ndss.2018.23158)
    arXiv:1801.01681v1'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2018) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang,
    Zhijun Deng, 和 Yuyi Zhong. 2018. VulDeePecker: 一种基于深度学习的漏洞检测系统. (2018年1月). [https://doi.org/10.14722/ndss.2018.23158](https://doi.org/10.14722/ndss.2018.23158)
    arXiv:1801.01681v1'
- en: 'Liang et al. ([n.d.]) Hongliang Liang, Yue Yu, Lin Jiang, and Zhuosi Xie. [n.d.].
    Seml: A semantic LSTM model for software defect prediction. *IEEE Access* 7 ([n. d.]),
    83812–83824.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（[无日期]）Hongliang Liang, Yue Yu, Lin Jiang, 和 Zhuosi Xie。[无日期]。Seml：一种用于软件缺陷预测的语义LSTM模型。*IEEE
    Access* 7（[无日期]），83812–83824。
- en: Lin et al. (2021) Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen,
    Hui Li, and Rongxin Wu. 2021. Improving Code Summarization with Block-wise Abstract
    Syntax Tree Splitting. In *IEEE International Conference on Program Comprehension*,
    Vol. 2021-May. IEEE Computer Society, 184–195. [https://doi.org/10.1109/ICPC52881.2021.00026](https://doi.org/10.1109/ICPC52881.2021.00026)
    arXiv:2103.07845
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人（2021）Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li,
    和 Rongxin Wu。2021。通过块级抽象语法树拆分来改进代码总结。在*IEEE国际程序理解会议*，2021年5月卷。IEEE计算机学会，184–195。
    [https://doi.org/10.1109/ICPC52881.2021.00026](https://doi.org/10.1109/ICPC52881.2021.00026)
    arXiv:2103.07845
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Meeting of the Association for Computational Linguistics*.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin。2004。ROUGE：一个自动评估总结的软件包。在*计算语言学协会年会*。
- en: 'Lin et al. (2018) Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D.
    Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for Natural Language Interface
    to the Linux Operating System.. In *Language Resources and Evaluation*.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人（2018）Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, 和 Michael D. Ernst。2018。NL2Bash：一个自然语言接口到Linux操作系统的语料库和语义解析器。在*语言资源与评估*。
- en: Liu et al. (2016) Chang Liu, Xinyun Chen, Eui Chul Richard Shin, Mingcheng Chen,
    and Dawn Song. 2016. Latent Attention For If-Then Program Synthesis. In *NIPS*.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2016）Chang Liu, Xinyun Chen, Eui Chul Richard Shin, Mingcheng Chen, 和
    Dawn Song。2016。用于If-Then程序合成的潜在注意力。在*NIPS*。
- en: Liu et al. (2017) Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, and
    Dawn Song. 2017. Neural Code Completion.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2017）Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, 和 Dawn Song。2017。神经代码完成。
- en: Liu et al. (2020b) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020b. A self-attentional neural architecture for code completion with multi-task
    learning. *IEEE International Conference on Program Comprehension* (2020), 37–47.
    [https://doi.org/10.1145/3387904.3389261](https://doi.org/10.1145/3387904.3389261)
    arXiv:1909.06983
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2020b）Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, 和 Zhi Jin。2020b。用于代码完成的自注意神经架构与多任务学习。*IEEE国际程序理解会议*（2020年），37–47。
    [https://doi.org/10.1145/3387904.3389261](https://doi.org/10.1145/3387904.3389261)
    arXiv:1909.06983
- en: Liu et al. (2020c) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020c. A Self-Attentional Neural Architecture for Code Completion with Multi-Task
    Learning. In *International Conference on Program Comprehension*.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2020c）Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, 和 Zhi Jin。2020c。用于代码完成的自注意神经架构与多任务学习。在*程序理解国际会议*。
- en: Liu et al. (2020d) Fang Liu, Lu Zhang, and Zhi Jin. 2020d. Modeling programs
    hierarchically with stack-augmented LSTM. *Journal of Systems and Software* 164
    (2020), 110547.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2020d）Fang Liu, Lu Zhang, 和 Zhi Jin。2020d。用堆栈增强LSTM分层建模程序。*系统与软件杂志* 164（2020年），110547。
- en: Liu et al. (2020a) Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang
    Liu. 2020a. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.
    (jun 2020). arXiv:2006.05405 [https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405](https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405)
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2020a）Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, 和 Yang Liu。2020a。通过混合GNN的代码总结检索增强生成。（2020年6月）。arXiv:2006.05405
    [https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405](https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405)
- en: 'Liu et al. (2021c) Xuye Liu, Dakuo Wang, April Wang, Yufang Hou, and Lingfei
    Wu. 2021c. HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural
    Network for Code Documentation Generation in Jupyter Notebooks. (mar 2021). [https://doi.org/10.18653/v1/2021.findings-emnlp.381](https://doi.org/10.18653/v1/2021.findings-emnlp.381)
    arXiv:2104.01002'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2021c）Xuye Liu, Dakuo Wang, April Wang, Yufang Hou, 和 Lingfei Wu。2021c。HAConvGNN：基于层次注意力的卷积图神经网络用于Jupyter笔记本中的代码文档生成。（2021年3月）。
    [https://doi.org/10.18653/v1/2021.findings-emnlp.381](https://doi.org/10.18653/v1/2021.findings-emnlp.381)
    arXiv:2104.01002
- en: Liu et al. (2021a) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021a. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *IEEE Transactions on Knowledge and
    Data Engineering* 01 (jul 2021), 1–1. [https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021a）Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu 和
    Xun Wang. 2021a. 将图神经网络与专家知识结合用于智能合约漏洞检测。*IEEE 知识与数据工程汇刊* 01（2021年7月），1–1。 [https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
- en: Liu et al. (2021b) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021b. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *IEEE Transactions on Knowledge and
    Data Engineering* (2021). [https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2021b）Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu 和
    Xun Wang. 2021b. 将图神经网络与专家知识结合用于智能合约漏洞检测。*IEEE 知识与数据工程汇刊*（2021）。[https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
    Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
    Duan, Neel Sundaresan, Shao Kun Deng, Fu Shengyu, and Shujie Liu. 2021. CodeXGLUE:
    A Machine Learning Benchmark Dataset for Code Understanding and Generation. *arXiv:
    Software Engineering* (2021).'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等人（2021）Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
    Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
    Duan, Neel Sundaresan, Shao Kun Deng, Fu Shengyu 和 Shujie Liu. 2021. CodeXGLUE:
    用于代码理解和生成的机器学习基准数据集。*arXiv: 软件工程*（2021）。'
- en: 'Mahmud et al. (2021) Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios
    Anastasopoulos, and Kevin Moran. 2021. Code to Comment Translation: A Comparative
    Study on Model Effectiveness & Errors.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmud 等人（2021）Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos
    和 Kevin Moran. 2021. 代码到评论翻译：模型有效性和错误的比较研究。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov 等人（2013）Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado 和 Jeff
    Dean. 2013. 单词和短语的分布式表示及其组合性。*神经信息处理系统进展* 26（2013）。
- en: 'Mou et al. (2014) Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN:
    A tree-based convolutional neural network for programming language processing.
    *arXiv preprint arXiv:1409.5718* (2014).'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mou 等人（2014）Lili Mou, Ge Li, Zhi Jin, Lu Zhang 和 Tao Wang. 2014. TBCNN: 一种用于编程语言处理的基于树的卷积神经网络。*arXiv
    预印本 arXiv:1409.5718*（2014）。'
- en: Mou et al. (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional
    neural networks over tree structures for programming language processing. In *Thirtieth
    AAAI Conference on Artificial Intelligence*.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou 等人（2016）Lili Mou, Ge Li, Lu Zhang, Tao Wang 和 Zhi Jin. 2016. 针对编程语言处理的树结构卷积神经网络。发表于
    *第三十届 AAAI 人工智能会议*。
- en: Nguyen et al. (2014) Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen.
    2014. Migrating code with statistical machine translation. In *International Conference
    on Software Engineering*.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2014）Anh Tuan Nguyen, Tung Thanh Nguyen 和 Tien N. Nguyen. 2014. 使用统计机器翻译迁移代码。发表于
    *国际软件工程会议*。
- en: Nguyen et al. (2018) Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen, and Tho Thanh
    Quan. 2018. Auto-detection of sophisticated malware using lazy-binding control
    flow graph and deep learning. *Computers & Security* 76 (jul 2018), 128–155. [https://doi.org/10.1016/J.COSE.2018.02.006](https://doi.org/10.1016/J.COSE.2018.02.006)
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2018）Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen 和 Tho Thanh
    Quan. 2018. 使用延迟绑定控制流图和深度学习的复杂恶意软件自动检测。*计算机与安全* 76（2018年7月），128–155。 [https://doi.org/10.1016/J.COSE.2018.02.006](https://doi.org/10.1016/J.COSE.2018.02.006)
- en: Nguyen et al. (2017) Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and
    Tien N. Nguyen. 2017. Exploring API embedding for API usages and applications.
    In *International Conference on Software Engineering*.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2017）Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan 和 Tien N. Nguyen.
    2017. 探索 API 嵌入用于 API 使用和应用。发表于 *国际软件工程会议*。
- en: Nguyen et al. (2013) Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and
    Tien N. Nguyen. 2013. A statistical semantic language model for source code. In
    *FSE*.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2013）Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen 和 Tien N.
    Nguyen. 2013. 用于源代码的统计语义语言模型。发表于 *FSE*。
- en: Oda et al. (2015) Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata,
    Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to Generate
    Pseudo-Code from Source Code Using Statistical Machine Translation (T). In *Automated
    Software Engineering*.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oda等（2015）小田由辅、藤波浩之、格雷厄姆·纽比格、旗田秀明、坂田沙基亚尼、戸田智树和中村聪。2015。使用统计机器翻译（T）从源代码生成伪代码。
    在*自动化软件工程*。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In
    *Meeting of the Association for Computational Linguistics*.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni等（2002）基肖尔·帕皮尼、萨利姆·鲁科斯、托德·沃德和魏京·朱。2002。Bleu：一种自动评估机器翻译的方法。在*计算语言学协会会议*。
- en: Peng et al. (2021) Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and
    Tie-Yan Liu. 2021. How could Neural Networks understand Programs? (2021). arXiv:2105.04297
    [http://arxiv.org/abs/2105.04297](http://arxiv.org/abs/2105.04297)
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2021）丁岚·彭、舒欣·郑、雅涛·李、郭林·柯、狄·何、和铁燕·刘。2021。神经网络如何理解程序？（2021）。arXiv:2105.04297
    [http://arxiv.org/abs/2105.04297](http://arxiv.org/abs/2105.04297)
- en: Prosser (1959) Reese T. Prosser. 1959. Applications of Boolean Matrices to the
    Analysis of Flow Diagrams. In *Papers Presented at the December 1-3, 1959, Eastern
    Joint IRE-AIEE-ACM Computer Conference* (Boston, Massachusetts) *(IRE-AIEE-ACM
    ’59 (Eastern))*. Association for Computing Machinery, New York, NY, USA, 133–138.
    [https://doi.org/10.1145/1460299.1460314](https://doi.org/10.1145/1460299.1460314)
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prosser（1959）Reese T. Prosser。1959。布尔矩阵在流程图分析中的应用。在*1959年12月1-3日东部联合IRE-AIEE-ACM计算机会议论文集*（波士顿，马萨诸塞州）*(IRE-AIEE-ACM
    ’59（东部）)*。计算机协会，纽约，NY，美国，133–138。 [https://doi.org/10.1145/1460299.1460314](https://doi.org/10.1145/1460299.1460314)
- en: 'Puri et al. (2021) Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo
    Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
    Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler,
    Susan Malaika, and Frederick Reiss. 2021. CodeNet: A Large-Scale AI for Code Dataset
    for Learning a Diversity of Coding Tasks.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puri等（2021）鲁奇尔·普里、大卫·S·孔、吉尔特·扬森、魏·张、贾科莫·多梅尼科尼、弗拉基米尔·佐洛托夫、朱利安·多尔比、杰·陈、米希尔·乔杜里、林赛·德克、维罗尼卡·托斯特、卢卡·布拉蒂、索拉布·普贾尔、夏姆·拉姆吉、乌尔里希·芬克勒、苏珊·马拉伊卡和弗雷德里克·赖斯。2021。CodeNet：一个大规模的AI代码数据集，用于学习多样的编码任务。
- en: Rabinovich et al. (2017) Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017.
    Abstract Syntax Networks for Code Generation and Semantic Parsing. In *Meeting
    of the Association for Computational Linguistics*.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabinovich等（2017）马克西姆·拉宾诺维奇、米切尔·斯特恩和丹·克莱因。2017。用于代码生成和语义解析的抽象语法网络。在*计算语言学协会会议*。
- en: 'Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *arXiv:
    Learning* (2019).'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raffel等（2019）科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马滕、闫琪·周、魏·李和彼得·J·刘。2019。使用统一的文本到文本变换器探索迁移学习的极限。*arXiv:
    学习*（2019）。'
- en: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
    Probabilistic model for code with decision trees. *ACM SIGPLAN Notices* 51, 10
    (2016), 731–747. [https://doi.org/10.1145/2983990.2984041](https://doi.org/10.1145/2983990.2984041)
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raychev等（2016）Veselin Raychev、Pavol Bielik和Martin Vechev。2016。使用决策树的代码概率模型。*ACM
    SIGPLAN Notices* 51, 10（2016），731–747。 [https://doi.org/10.1145/2983990.2984041](https://doi.org/10.1145/2983990.2984041)
- en: RaychevVeselin et al. (2014) RaychevVeselin, VechevMartin, and YahavEran. 2014.
    Code completion with statistical language models. *Sigplan Notices* (2014).
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RaychevVeselin等（2014）RaychevVeselin、VechevMartin和YahavEran。2014。使用统计语言模型进行代码补全。*Sigplan
    Notices*（2014）。
- en: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU:
    a Method for Automatic Evaluation of Code Synthesis. *arXiv: Software Engineering*
    (2020).'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren等（2020）邵然、大亚·郭、帅路、龙周、舒杰·刘、杜宇·唐、尼尔·苏达雷桑、明周、安布罗斯·布兰科和帅马。2020。CodeBLEU：一种自动评估代码合成的方法。*arXiv:
    软件工程*（2020）。'
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart等（1986）大卫·E·鲁梅尔哈特、杰弗里·E·辛顿和罗纳德·J·威廉姆斯。1986。通过反向传播误差学习表示。*自然* 323, 6088
    (1986), 533–536。
- en: Rush et al. (2015) Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015.
    A Neural Attention Model for Abstractive Sentence Summarization. In *EMNLP*.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rush等（2015）亚历山大·M·拉什、苏米特·乔普拉和杰森·韦斯顿。2015。用于抽象句子摘要的神经注意力模型。在*EMNLP*。
- en: 'Sachdev et al. (2018) Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim,
    Koushik Sen, and Satish Chandra. 2018. Retrieval on source code: a neural code
    search.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sachdev 等人（2018）Saksham Sachdev、Hongyu Li、Sifei Luan、Seohyun Kim、Koushik Sen
    和 Satish Chandra。2018。源代码检索：一种神经代码搜索。
- en: 'Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention
    with relative position representations. *NAACL HLT 2018 - 2018 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies - Proceedings of the Conference* 2 (2018), 464–468. [https://doi.org/10.18653/v1/n18-2074](https://doi.org/10.18653/v1/n18-2074)
    arXiv:1803.02155'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaw 等人（2018）Peter Shaw、Jakob Uszkoreit 和 Ashish Vaswani。2018。具有相对位置表示的自注意力。*NAACL
    HLT 2018 - 2018 年北美计算语言学协会：人类语言技术会议 - 会议论文集* 2（2018），464–468。 [https://doi.org/10.18653/v1/n18-2074](https://doi.org/10.18653/v1/n18-2074)
    arXiv:1803.02155
- en: 'Shi et al. (2021) Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han,
    Dongmei Zhang, and Hongbin Sun. 2021. CAST: Enhancing Code Summarization with
    Hierarchical Splitting and Reconstruction of Abstract Syntax Trees. (aug 2021).
    arXiv:2108.12987 [http://arxiv.org/abs/2108.12987](http://arxiv.org/abs/2108.12987)'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人（2021）Ensheng Shi、Yanlin Wang、Lun Du、Hongyu Zhang、Shi Han、Dongmei Zhang
    和 Hongbin Sun。2021。CAST: 通过层次分割和重建抽象语法树来增强代码摘要。（2021年8月）。arXiv:2108.12987 [http://arxiv.org/abs/2108.12987](http://arxiv.org/abs/2108.12987)'
- en: 'Shi et al. (2020) Ke Shi, Yang Lu, Jingfei Chang, and Zhen Wei. 2020. PathPair2Vec:
    An AST path pair-based code representation method for defect prediction.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人（2020）Ke Shi、Yang Lu、Jingfei Chang 和 Zhen Wei。2020。PathPair2Vec: 一种基于
    AST 路径对的代码表示方法用于缺陷预测。'
- en: Shido et al. (2019) Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi
    Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source Code Summarization with
    Extended Tree-LSTM. In *Proceedings of the International Joint Conference on Neural
    Networks*, Vol. 2019-July. Institute of Electrical and Electronics Engineers Inc.
    [https://doi.org/10.1109/IJCNN.2019.8851751](https://doi.org/10.1109/IJCNN.2019.8851751)
    arXiv:1906.08094
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shido 等人（2019）Yusuke Shido、Yasuaki Kobayashi、Akihiro Yamamoto、Atsushi Miyamoto
    和 Tadayuki Matsumura。2019。使用扩展 Tree-LSTM 的自动源代码摘要。载于 *国际联合神经网络大会论文集*，卷 2019-July。电气和电子工程师协会。
    [https://doi.org/10.1109/IJCNN.2019.8851751](https://doi.org/10.1109/IJCNN.2019.8851751)
    arXiv:1906.08094
- en: 'Shrivastava (2021) Piyush Shrivastava. 2021. Neural Code Summarization. *arXiv:
    Software Engineering* (2021).'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shrivastava（2021）Piyush Shrivastava。2021。神经代码摘要。*arXiv: 软件工程*（2021）。'
- en: Shuai et al. (2020) Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and
    Yan Lei. 2020. Improving Code Search with Co-Attentive Representation Learning.
    In *International Conference on Program Comprehension*.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuai 等人（2020）Jianhang Shuai、Ling Xu、Chao Liu、Meng Yan、Xin Xia 和 Yan Lei。2020。通过协同注意力表示学习改进代码搜索。载于
    *国际程序理解大会*。
- en: 'Sui et al. (2020) Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020.
    Flow2Vec: value-flow-based precise code embedding. *Proceedings of the ACM on
    Programming Languages* 4, OOPSLA (nov 2020), 27. [https://doi.org/10.1145/3428301](https://doi.org/10.1145/3428301)'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sui 等人（2020）Yulei Sui、Xiao Cheng、Guanqin Zhang 和 Haoyu Wang。2020。Flow2Vec:
    基于值流的精确代码嵌入。*ACM 编程语言论文集* 4，OOPSLA（2020年11月），27。 [https://doi.org/10.1145/3428301](https://doi.org/10.1145/3428301)'
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. In *Advances in neural information
    processing systems*. 3104–3112.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等人（2014）Ilya Sutskever、Oriol Vinyals 和 Quoc V Le。2014。基于神经网络的序列到序列学习。载于
    *神经信息处理系统进展*。3104–3112。
- en: 'Svyatkovskiy et al. (2020) Alexey Svyatkovskiy, Shao Kun Deng, Fu Shengyu,
    and Neel Sundaresan. 2020. IntelliCode compose: code generation using transformer.
    In *FSE*.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Svyatkovskiy 等人（2020）Alexey Svyatkovskiy、Shao Kun Deng、Fu Shengyu 和 Neel Sundaresan。2020。IntelliCode
    compose: 使用 transformer 进行代码生成。载于 *FSE*。'
- en: 'Svyatkovskiy et al. (2019) Alexey Svyatkovskiy, Ying Zhao, Fu Shengyu, and
    Neel Sundaresan. 2019. Pythia: AI-assisted Code Completion System. *arXiv: Software
    Engineering* (2019).'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Svyatkovskiy 等人（2019）Alexey Svyatkovskiy、Ying Zhao、Fu Shengyu 和 Neel Sundaresan。2019。Pythia:
    AI 辅助代码补全系统。*arXiv: 软件工程*（2019）。'
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D. Manning.
    2015. Improved semantic representations from tree-structured long short-Term memory
    networks. In *ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    of the Asian Federation of Natural Language Processing, Proceedings of the Conference*,
    Vol. 1\. Association for Computational Linguistics (ACL), 1556–1566. [https://doi.org/10.3115/v1/p15-1150](https://doi.org/10.3115/v1/p15-1150)
    arXiv:1503.00075
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tai 等（2015）Kai Sheng Tai、Richard Socher 和 Christopher D. Manning。2015。通过树结构的长短期记忆网络改进语义表示。见于
    *ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics
    and the 7th International Joint Conference on Natural Language Processing of the
    Asian Federation of Natural Language Processing, Proceedings of the Conference*，第
    1 卷。计算语言学协会（ACL），1556–1566。[https://doi.org/10.3115/v1/p15-1150](https://doi.org/10.3115/v1/p15-1150)
    arXiv:1503.00075
- en: Tarlow et al. (2020a) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020a. Learning
    to Fix Build Errors with Graph2Diff Neural Networks. In *Proceedings - 2020 IEEE/ACM
    42nd International Conference on Software Engineering Workshops, ICSEW 2020*.
    Association for Computing Machinery, Inc, 19–20. [https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarlow 等（2020a）Daniel Tarlow、Subhodeep Moitra、Andrew Rice、Zimin Chen、Pierre
    Antoine Manzagol、Charles Sutton 和 Edward Aftandilian。2020a。通过 Graph2Diff 神经网络学习修复构建错误。见于
    *Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering
    Workshops, ICSEW 2020*。计算机协会，19–20。[https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
- en: Tarlow et al. (2020b) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020b. Learning
    to Fix Build Errors with Graph2Diff Neural Networks. In *Proceedings - 2020 IEEE/ACM
    42nd International Conference on Software Engineering Workshops, ICSEW 2020*.
    19–20. [https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarlow 等（2020b）Daniel Tarlow、Subhodeep Moitra、Andrew Rice、Zimin Chen、Pierre
    Antoine Manzagol、Charles Sutton 和 Edward Aftandilian。2020b。通过 Graph2Diff 神经网络学习修复构建错误。见于
    *Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering
    Workshops, ICSEW 2020*。19–20。[https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
- en: Tu et al. (2014) Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the
    localness of software. In *FSE*.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等（2014）Zhaopeng Tu、Zhendong Su 和 Premkumar Devanbu。2014。关于软件的局部性。见于 *FSE*。
- en: Ugner et al. ([n.d.]) Daniel Z ¨ Ugner, Tobias Kirschstein, Michele Catasta,
    Jure Leskovec, and Stephan G ¨ Unnemann. [n.d.]. Language-agnostic Representation
    Learning of Source Code from Structure and Context. ([n. d.]). arXiv:2103.11318v1
    [www.daml.in.tum.de/code-transformer,](www.daml.in.tum.de/code-transformer,)
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ugner 等（[n.d.]）Daniel Z ¨ Ugner、Tobias Kirschstein、Michele Catasta、Jure Leskovec
    和 Stephan G ¨ Unnemann。[n.d.]。从结构和上下文中进行语言无关的源代码表示学习。（[n.d.]）。arXiv:2103.11318v1
    [www.daml.in.tum.de/code-transformer](www.daml.in.tum.de/code-transformer)
- en: 'Vasic et al. (2019) Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    and Rishabh Singh. 2019. Neural Program Repair by Jointly Learning to Localize
    and Repair. *arXiv: Learning* (2019).'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vasic 等（2019）Marko Vasic、Aditya Kanade、Petros Maniatis、David Bieber 和 Rishabh
    Singh。2019。通过联合学习定位和修复实现神经程序修复。*arXiv: Learning*（2019）。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in neural information processing systems*. 5998–6008.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion
    Jones、Aidan N Gomez、Łukasz Kaiser 和 Illia Polosukhin。2017。注意力机制就是你所需要的。见于 *Advances
    in neural information processing systems*。5998–6008。
- en: Veličković et al. (2018) Petar Veličković, Arantxa Casanova, Pietro Liò, Guillem
    Cucurull, Adriana Romero, and Yoshua Bengio. 2018. Graph attention networks. In
    *6th International Conference on Learning Representations, ICLR 2018 - Conference
    Track Proceedings*. International Conference on Learning Representations, ICLR.
    arXiv:1710.10903 [https://arxiv.org/abs/1710.10903v3](https://arxiv.org/abs/1710.10903v3)
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković 等（2018）Petar Veličković、Arantxa Casanova、Pietro Liò、Guillem Cucurull、Adriana
    Romero 和 Yoshua Bengio。2018。图注意力网络。见于 *6th International Conference on Learning
    Representations, ICLR 2018 - Conference Track Proceedings*。国际学习表征会议，ICLR。arXiv:1710.10903
    [https://arxiv.org/abs/1710.10903v3](https://arxiv.org/abs/1710.10903v3)
- en: Wan et al. (2019a) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip Yu. 2019a. Multi-modal attention network learning for semantic
    source code retrieval. In *Proceedings - 2019 34th IEEE/ACM International Conference
    on Automated Software Engineering, ASE 2019*. Institute of Electrical and Electronics
    Engineers Inc., 13–25. [https://doi.org/10.1109/ASE.2019.00012](https://doi.org/10.1109/ASE.2019.00012)
    arXiv:1909.13516
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等（2019a）姚万、舒景东、隋宇雷、许光东、赵周、吴建和菲利普·余。2019a。用于语义源代码检索的多模态注意力网络学习。见于 *2019年第34届IEEE/ACM国际自动化软件工程会议论文集，ASE
    2019*。电气与电子工程师协会，13–25。 [https://doi.org/10.1109/ASE.2019.00012](https://doi.org/10.1109/ASE.2019.00012)
    arXiv:1909.13516
- en: Wan et al. (2019b) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip S Yu. 2019b. Multi-modal attention network learning for semantic
    source code retrieval. *arXiv preprint arXiv:1909.13516* (2019).
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等（2019b）姚万、舒景东、隋宇雷、许光东、赵周、吴建和菲利普·S·余。2019b。用于语义源代码检索的多模态注意力网络学习。*arXiv 预印本
    arXiv:1909.13516*（2019）。
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via
    deep reinforcement learning. In *ASE 2018 - Proceedings of the 33rd ACM/IEEE International
    Conference on Automated Software Engineering*. Association for Computing Machinery,
    Inc, 397–407. [https://doi.org/10.1145/3238147.3238206](https://doi.org/10.1145/3238147.3238206)
    arXiv:1811.07234
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等（2018）姚万、赵周、杨敏、许光东、英浩超、吴建和菲利普·S·余。2018。通过深度强化学习改善自动源代码摘要。在 *ASE 2018 -
    第33届ACM/IEEE国际自动化软件工程会议论文集* 中。计算机协会，397–407。 [https://doi.org/10.1145/3238147.3238206](https://doi.org/10.1145/3238147.3238206)
    arXiv:1811.07234
- en: Wang et al. (2021b) D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, and X.
    Liao. 2021b. Bridging Pre-trained Models and Downstream Tasks for Source Code
    Understanding. (2021).
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021b）D. 王、Z. 贾、S. 李、Y. 于、Y. 熊、W. 董和X. 廖。2021b。桥接预训练模型和下游任务以理解源代码。（2021）。
- en: Wang (2019) Ke Wang. 2019. Learning Scalable and Precise Representation of Program
    Semantics. (may 2019). arXiv:1905.05251 [https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251](https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251)
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang（2019）柯王。2019。学习可扩展和精确的程序语义表示。（2019年5月）。arXiv:1905.05251 [https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251](https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251)
- en: Wang et al. (2020a) Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020a.
    Deep Semantic Feature Learning for Software Defect Prediction. *IEEE Transactions
    on Software Engineering* 46 (2020), 1267–1293.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020a）宋王、刘泰跃、南杰昌和谭林。2020a。用于软件缺陷预测的深度语义特征学习。*IEEE 软件工程学报* 46（2020），1267–1293。
- en: Wang et al. (2016) Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning
    semantic features for defect prediction. In *International Conference on Software
    Engineering*.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2016）宋王、刘泰跃和谭林。2016。自动学习缺陷预测的语义特征。见于 *软件工程国际会议*。
- en: Wang et al. (2021c) Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. 2021c. Towards
    Full-line Code Completion with Neural Language Models. (2021). arXiv:2009.08603v1
    [www.aaai.org](www.aaai.org)
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021c）汪文汉、沈思杰、李戈和金志。2021c。通过神经语言模型实现全行代码补全。（2021）。arXiv:2009.08603v1
    [www.aaai.org](www.aaai.org)
- en: Wang et al. (2020c) Wenhan Wang, Kechi Zhang, Ge Li, and Zhi Jin. 2020c. Learning
    to Represent Programs with Heterogeneous Graphs. (2020), 1–10. arXiv:2012.04188
    [http://arxiv.org/abs/2012.04188](http://arxiv.org/abs/2012.04188)
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020c）汪文汉、张克琪、李戈和金志。2020c。通过异质图表示程序的学习。（2020），1–10。arXiv:2012.04188 [http://arxiv.org/abs/2012.04188](http://arxiv.org/abs/2012.04188)
- en: 'Wang et al. (2021a) Yanlin Wang, Shi Han, Dongmei Zhang, Ensheng Shi, Lun Du,
    Xiaodi Yang, Yuxuan Hu, Shi Han, and Hongyu Zhang. 2021a. CoCoSum: Contextual
    Code Summarization with Multi-Relational Graph Neural Network. *J. ACM* 1, 1 (jul
    2021), 24. [https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)
    arXiv:2107.01933'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021a）王彦林、韩石、张冬梅、施恩生、杜伦、杨晓迪、胡宇轩、韩石和张洪宇。2021a。CoCoSum：使用多关系图神经网络的上下文代码摘要。*J.
    ACM* 1，1（2021年7月），24。 [https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)
    arXiv:2107.01933
- en: Wang and Li (2021) Yanlin Wang and Hui Li. 2021. Code Completion by Modeling
    Flattened Abstract Syntax Trees as Graphs. (2021). arXiv:2103.09499 [http://arxiv.org/abs/2103.09499](http://arxiv.org/abs/2103.09499)
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Li（2021）王彦林和李辉。2021。通过将展平的抽象语法树建模为图来完成代码。（2021）。arXiv:2103.09499 [http://arxiv.org/abs/2103.09499](http://arxiv.org/abs/2103.09499)
- en: Wang et al. (2020b) Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. 2020b.
    Learning semantic program embeddings with graph interval neural network. *Proceedings
    of the ACM on Programming Languages* 4, OOPSLA (may 2020). [https://doi.org/10.1145/3428205](https://doi.org/10.1145/3428205)
    arXiv:2005.09997
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2020b）于旺、柯旺、冯娟高 和 林章王。2020b。利用图间隔神经网络学习语义程序嵌入。*ACM程序语言会议录* 4，OOPSLA（2020年5月）。[https://doi.org/10.1145/3428205](https://doi.org/10.1145/3428205)
    arXiv:2005.09997
- en: 'Wang et al. (2021d) Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi.
    2021d. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for
    Code Understanding and Generation. *arXiv: Computation and Language* (2021).'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2021d）岳旺、韦施旺、沙菲克·乔提 和 Steven C. H. Hoi。2021d。CodeT5：面向标识符的统一预训练编码器-解码器模型，用于代码理解和生成。*arXiv:计算与语言*（2021）。
- en: 'Wei et al. (2019) Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2019.
    Retrieve and refine: exemplar-based neural comment generation. In *Automated Software
    Engineering*.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等人（2019）博林·魏、永敏李、葛力、辛霞 和 智金。2019。检索和精炼：基于示例的神经评论生成。在*自动化软件工程*。
- en: Wei and Li (2017) Hui Hui Wei and Ming Li. 2017. Supervised deep features for
    Software functional clone detection by exploiting lexical and syntactical information
    in source code. In *IJCAI International Joint Conference on Artificial Intelligence*.
    3034–3040. [https://doi.org/10.24963/ijcai.2017/423](https://doi.org/10.24963/ijcai.2017/423)
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 和 Li（2017）慧慧魏 和 明李。2017。通过利用源代码中的词汇和语法信息进行软件功能克隆检测的监督深度特征。在*IJCAI国际人工智能联合会议*。3034–3040。[https://doi.org/10.24963/ijcai.2017/423](https://doi.org/10.24963/ijcai.2017/423)
- en: White et al. (2016) Martin White, Michele Tufano, Christopher Vendome, and Denys
    Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In *2016
    31st IEEE/ACM International Conference on Automated Software Engineering (ASE)*.
    IEEE, 87–98.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White等人（2016）马丁·怀特、米歇尔·图法诺、克里斯托弗·文多姆 和 丹尼斯·波希瓦尼克。2016。深度学习代码片段用于代码克隆检测。在*2016年第31届IEEE/ACM自动化软件工程国际会议（ASE）*。IEEE，87–98。
- en: White et al. (2015) Martin White, Christopher Vendome, Mario Linares-Vasquez,
    and Denys Poshyvanyk. 2015. Toward deep learning software repositories. In *Mining
    Software Repositories*.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White等人（2015）马丁·怀特、克里斯托弗·文多姆、马里奥·利纳雷斯-瓦斯克斯 和 丹尼斯·波希瓦尼克。2015。迈向深度学习软件库。在*挖掘软件库*。
- en: 'Wu et al. (2020) Liwei Wu, Fei Li, Youhua Wu, and Tao Zheng. 2020. GGF: A graph-based
    method for programming language syntax error correction. *IEEE International Conference
    on Program Comprehension* (2020), 139–148. [https://doi.org/10.1145/3387904.3389252](https://doi.org/10.1145/3387904.3389252)'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2020）李伟、费力、游华武 和 郑涛。2020。GGF：一种基于图的编程语言语法错误修正方法。*IEEE国际程序理解会议*（2020），139–148。[https://doi.org/10.1145/3387904.3389252](https://doi.org/10.1145/3387904.3389252)
- en: Wu et al. (2019) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and Philip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks.
    *IEEE Transactions on Neural Networks and Learning Systems* 32, 1 (jan 2019),
    4–24. [https://doi.org/10.1109/TNNLS.2020.2978386](https://doi.org/10.1109/TNNLS.2020.2978386)
    arXiv:1901.00596v4
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2019）宗汉吴、时睿潘、冯文陈、国栋龙、成琪张 和 Philip S. Yu。2019。图神经网络的全面调查。*IEEE神经网络与学习系统汇刊*
    32, 1（2019年1月），4–24。[https://doi.org/10.1109/TNNLS.2020.2978386](https://doi.org/10.1109/TNNLS.2020.2978386)
    arXiv:1901.00596v4
- en: Xiao et al. (2017) Yan Xiao, Jacky Keung, Qing Mi, and Kwabena Ebo Bennin. 2017.
    Improving Bug Localization with an Enhanced Convolutional Neural Network. In *Asia-Pacific
    Software Engineering Conference*.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等人（2017）杨晓、Jacky Keung、Qing Mi 和 Kwabena Ebo Bennin。2017。通过增强卷积神经网络改进缺陷定位。发表于*亚太软件工程会议*。
- en: Xie et al. (2021) Binbin Xie, Jinsong Su, Yubin Ge, Xiang Li, Jianwei Cui, Junfeng
    Yao, and Bin Wang. 2021. Improving Tree-Structured Decoder Training for Code Generation
    via Mutual Learning. (2021). arXiv:2105.14796v1 [www.aaai.org](www.aaai.org)
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等人（2021） Binbin Xie、金松苏、于宾葛、项李、建伟崔、俊峰姚 和 Bin Wang。2021。通过互学提升树状解码器训练以生成代码。（2021）。arXiv:2105.14796v1
    [www.aaai.org](www.aaai.org)
- en: Xu et al. (2021) Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan,
    Yan Lei, and Zhou Xu. 2021. Two-Stage Attention-Based Model for Code Search with
    Textual and Structural Features. In *IEEE International Conference on Software
    Analysis, Evolution, and Reengineering*.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人（2021）徐玲、欢欢杨、超刘、建行帅、孟燕、燕雷 和 周旭。2021。基于两阶段注意力的代码搜索模型，结合文本和结构特征。在*IEEE国际软件分析、演变与重构会议*。
- en: Xu et al. (2019) Sihan Xu, Sen Zhang, Weijing Wang, Xinya Cao, Chenkai Guo,
    and Jing Xu. 2019. Method name suggestion with hierarchical attention networks.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2019）司涵许、森张、伟晶王、新雅曹、陈凯郭和晶许。2019年。具有层次注意力网络的方法名称建议。
- en: Xu et al. (2017) Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn
    Song. 2017. Neural network-based graph embedding for cross-platform binary code
    similarity detection. In *Proceedings of the ACM Conference on Computer and Communications
    Security*. Association for Computing Machinery, 363–376. [https://doi.org/10.1145/3133956.3134018](https://doi.org/10.1145/3133956.3134018)
    arXiv:1708.06525
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2017）肖军许、常刘、钱丰、恒尹、乐宋和晓歌。2017年。基于神经网络的图嵌入用于跨平台二进制代码相似性检测。在*ACM计算机与通信安全会议论文集*中。计算机协会，363–376。[https://doi.org/10.1145/3133956.3134018](https://doi.org/10.1145/3133956.3134018)
    arXiv:1708.06525
- en: 'Xue et al. (2021) Jiang Xue, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu.
    2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. In *Uncertainty
    in Artificial Intelligence*.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 薛等（2021）江薛、卓然郑、陈吕、梁李和雷吕。2021年。TreeBERT：一种基于树的编程语言预训练模型。在*人工智能中的不确定性*中。
- en: Yamaguchi et al. (2014) Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad
    Rieck. 2014. Modeling and discovering vulnerabilities with code property graphs.
    In *Proceedings - IEEE Symposium on Security and Privacy*. 590–604. [https://doi.org/10.1109/SP.2014.44](https://doi.org/10.1109/SP.2014.44)
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 山口等（2014）法比安·山口、尼科·戈尔德、丹尼尔·阿普和孔拉德·里克。2014年。使用代码属性图建模和发现漏洞。在*IEEE安全与隐私研讨会论文集*中，590–604。[https://doi.org/10.1109/SP.2014.44](https://doi.org/10.1109/SP.2014.44)
- en: Yan et al. (2019) Jiaqi Yan, Guanhua Yan, and Dong Jin. 2019. Classifying Malware
    Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network.
    In *Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable
    Systems and Networks, DSN 2019*. Institute of Electrical and Electronics Engineers
    Inc., 52–63. [https://doi.org/10.1109/DSN.2019.00020](https://doi.org/10.1109/DSN.2019.00020)
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 严等（2019）佳琪严、关华严和董金。2019年。使用深度图卷积神经网络对控制流图表示的恶意软件进行分类。在*第49届年度IEEE/IFIP国际可靠性系统与网络会议论文集，DSN
    2019*中。电气和电子工程师学会，52–63。[https://doi.org/10.1109/DSN.2019.00020](https://doi.org/10.1109/DSN.2019.00020)
- en: 'Yang and Kuang ([n.d.]) Hao Yang and Li Kuang. [n.d.]. CCMC: Code Completion
    with a Memory Mechanism and a Copy Mechanism; CCMC: Code Completion with a Memory
    Mechanism and a Copy Mechanism. ([n. d.]). [https://doi.org/10.1145/3463274.3463332](https://doi.org/10.1145/3463274.3463332)'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨和匡（[n.d.]）郝杨和李匡。[n.d.]。CCMC: 具有记忆机制和复制机制的代码补全；CCMC: 具有记忆机制和复制机制的代码补全。 ([n. d.])。[https://doi.org/10.1145/3463274.3463332](https://doi.org/10.1145/3463274.3463332)'
- en: Yang et al. (2020) Kang Yang, Huiqun Yu, Guisheng Fan, and Xingguang Yang. 2020.
    Graph embedding code prediction model integrating semantic features. *Computer
    Science and Information Systems* 17, 3 (2020), 907–926. [https://doi.org/10.2298/CSIS190908027Y](https://doi.org/10.2298/CSIS190908027Y)
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等（2020）康杨、辉群于、桂生范和兴光杨。2020年。集成语义特征的图嵌入代码预测模型。*计算机科学与信息系统* 17, 3（2020），907–926。[https://doi.org/10.2298/CSIS190908027Y](https://doi.org/10.2298/CSIS190908027Y)
- en: 'Yang et al. (2019) Ke Yang, MingXing Zhang, Kang Chen, Xiaosong Ma, Yang Bai,
    and Yong Jiang. 2019. KnightKing: a fast distributed graph random walk engine.
    In *Proceedings of the 27th ACM Symposium on Operating Systems Principles*. 524–537.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '杨等（2019）柯杨、明兴张、康陈、肖松马、杨白和永江。2019年。KnightKing: 一个快速的分布式图随机游走引擎。在*第27届ACM操作系统原理研讨会论文集*中，524–537。'
- en: Yasunaga and Liang (2020a) Michihiro Yasunaga and Percy Liang. 2020a. Graph-based,
    Self-Supervised Program Repair from Diagnostic Feedback. (2020).
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安永和梁（2020a）安永和梁。2020a。基于图的自监督程序修复来自诊断反馈。（2020年）。
- en: Yasunaga and Liang (2020b) Michihiro Yasunaga and Percy Liang. 2020b. Graph-based,
    Self-Supervised Program Repair from Diagnostic Feedback. In *International Conference
    on Machine Learning*.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安永和梁（2020b）安永和梁。2020b。基于图的自监督程序修复来自诊断反馈。在*国际机器学习大会*中。
- en: Yin et al. (2018a) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu,
    and Graham Neubig. 2018a. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Proceedings - International Conference on Software Engineering*.
    IEEE Computer Society, 476–486. [https://doi.org/10.1145/3196398.3196408](https://doi.org/10.1145/3196398.3196408)
    arXiv:1805.08949
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2018a) 彭城·尹、博文·邓、埃德加·陈、博格丹·瓦西莱斯库和格雷厄姆·纽比格。2018a. 从 Stack Overflow
    中学习挖掘对齐的代码和自然语言对。在*国际软件工程会议论文集*。IEEE计算机协会，476–486。 [https://doi.org/10.1145/3196398.3196408](https://doi.org/10.1145/3196398.3196408)
    arXiv:1805.08949
- en: Yin et al. (2018b) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu,
    and Graham Neubig. 2018b. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Mining Software Repositories*.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2018b) 彭城·尹、博文·邓、埃德加·陈、博格丹·瓦西莱斯库和格雷厄姆·纽比格。2018b. 从 Stack Overflow
    中学习挖掘对齐的代码和自然语言对。在*矿藏软件库*。
- en: 'Yin and Neubig (2018) Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based
    Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. In *Empirical
    Methods in Natural Language Processing*.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin and Neubig (2018) 彭城·尹和格雷厄姆·纽比格。2018. TRANX：一种基于转换的神经抽象语法解析器用于语义解析和代码生成。在*自然语言处理中的实证方法*。
- en: 'Yu et al. (2020) Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and
    Shi Wu. 2020. Order matters: Semantic-aware neural networks for binary code similarity
    detection. In *AAAI 2020 - 34th AAAI Conference on Artificial Intelligence*, Vol. 34.
    AAAI press, 1145–1152. [https://doi.org/10.1609/aaai.v34i01.5466](https://doi.org/10.1609/aaai.v34i01.5466)'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2020) 泽平·余、瑞·曹、启逸·唐、森·聂、俊州·黄和石·吴。2020. 顺序很重要：语义感知神经网络用于二进制代码相似性检测。在*AAAI
    2020 - 第34届AAAI人工智能会议*，第34卷。AAAI出版社，1145–1152。 [https://doi.org/10.1609/aaai.v34i01.5466](https://doi.org/10.1609/aaai.v34i01.5466)
- en: Zhang et al. (2020a) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020a. Retrieval-based neural source code summarization. *Proceedings - International
    Conference on Software Engineering* (2020), 1385–1397. [https://doi.org/10.1145/3377811.3380383](https://doi.org/10.1145/3377811.3380383)
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020a) 简·张、徐·王、洪宇·张、海龙·孙和旭东·刘。2020a. 基于检索的神经源代码摘要。*国际软件工程会议论文集*（2020），1385–1397。
    [https://doi.org/10.1145/3377811.3380383](https://doi.org/10.1145/3377811.3380383)
- en: Zhang et al. (2020b) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020b. Retrieval-based neural source code summarization. In *International
    Conference on Software Engineering*.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020b) 简·张、徐·王、洪宇·张、海龙·孙和旭东·刘。2020b. 基于检索的神经源代码摘要。在*国际软件工程会议*。
- en: Zhang et al. (2019a) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019a. A Novel Neural Source Code Representation Based on
    Abstract Syntax Tree. *Proceedings - International Conference on Software Engineering*
    2019-May (2019), 783–794. [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019a) 简·张、徐·王、洪宇·张、海龙·孙、凯轩·王和旭东·刘。2019a. 基于抽象语法树的新型神经源代码表示。*国际软件工程会议论文集*
    2019年5月（2019），783–794。 [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
- en: Zhang et al. (2019b) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019b. A Novel Neural Source Code Representation Based on
    Abstract Syntax Tree. In *2019 IEEE/ACM 41st International Conference on Software
    Engineering (ICSE)*. 783–794. [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2019b) 简·张、徐·王、洪宇·张、海龙·孙、凯轩·王和旭东·刘。2019b. 基于抽象语法树的新型神经源代码表示。在*2019
    IEEE/ACM 第41届国际软件工程会议（ICSE）*。783–794。 [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
- en: Zhang et al. (2018) Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.
    2018. An end-to-end deep learning architecture for graph classification. In *Thirty-second
    AAAI conference on artificial intelligence*.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) 穆汉·张、志成·崔、玛丽昂·诺伊曼和义欣·陈。2018. 用于图分类的端到端深度学习架构。在*第三十二届AAAI人工智能会议*。
- en: 'Zhao and Huang (2018) Gang Zhao and Jeff Huang. 2018. DeepSim: Deep Learning
    Code Functional Similarity. (2018). [https://doi.org/10.1145/3236024.3236068](https://doi.org/10.1145/3236024.3236068)'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao and Huang (2018) 刚·赵和杰夫·黄。2018. DeepSim：深度学习代码功能相似性。（2018）。 [https://doi.org/10.1145/3236024.3236068](https://doi.org/10.1145/3236024.3236068)
- en: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and
    Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive
    Program Semantics via Graph Neural Networks. *Advances in Neural Information Processing
    Systems* 32 (sep 2019). arXiv:1909.03496 [https://arxiv.org/abs/1909.03496v1](https://arxiv.org/abs/1909.03496v1)'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2019) 余勤·周、尚清·刘、晶凯·萧、晓宁·杜、杨·刘。2019。Devign：通过图神经网络学习全面程序语义的有效漏洞识别。*神经信息处理系统进展*
    32 (2019年9月)。arXiv:1909.03496 [https://arxiv.org/abs/1909.03496v1](https://arxiv.org/abs/1909.03496v1)
- en: 'Zhu et al. (2016) Xiaowei Zhu, Wenguang Chen, Weimin Zheng, and Xiaosong Ma.
    2016. Gemini: A computation-centric distributed graph processing system. In *12th
    $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$
    16)*. 301–316.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2016) 晓伟·朱、文光·陈、伟敏·郑、晓松·马。2016。Gemini：一个以计算为中心的分布式图处理系统。在*第12届$\{$USENIX$\}$操作系统设计与实现研讨会（$\{$OSDI$\}$
    16）*。301–316。
- en: Zuo et al. (2019) Fei Zuo, Xiaopeng Li, Patrick Young, Lannan Luo, Qiang Zeng,
    and Zhexin Zhang. 2019. Neural Machine Translation Inspired Binary Code Similarity
    Comparison beyond Function Pairs. Internet Society. [https://doi.org/10.14722/ndss.2019.23492](https://doi.org/10.14722/ndss.2019.23492)
    arXiv:1808.04706
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuo et al. (2019) 飞·左、小鹏·李、帕特里克·杨、兰南·罗、强·曾、哲欣·张。2019。超越函数对的神经机器翻译启发的二进制代码相似性比较。互联网社会。
    [https://doi.org/10.14722/ndss.2019.23492](https://doi.org/10.14722/ndss.2019.23492)
    arXiv:1808.04706
