- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:04:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1910.07738] A Survey of Deep Learning Techniques for Autonomous Driving'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1910.07738] 《自动驾驶深度学习技术调查》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1910.07738](https://ar5iv.labs.arxiv.org/html/1910.07738)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1910.07738](https://ar5iv.labs.arxiv.org/html/1910.07738)
- en: A Survey of Deep Learning Techniques for Autonomous Driving
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《自动驾驶深度学习技术调查》
- en: Sorin Grigorescu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Sorin Grigorescu
- en: Artificial Intelligence,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，
- en: Elektrobit Automotive.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Elektrobit Automotive.
- en: Robotics, Vision and Control Lab,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人、视觉与控制实验室，
- en: Transilvania University of Brasov.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 布拉索夫大学。
- en: Brasov, Romania
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 罗马尼亚布拉索夫
- en: Sorin.Grigorescu@elektrobit.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Sorin.Grigorescu@elektrobit.com
- en: '&Bogdan Trasnea'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&Bogdan Trasnea'
- en: Artificial Intelligence,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，
- en: Elektrobit Automotive.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Elektrobit Automotive.
- en: Robotics, Vision and Control Lab,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人、视觉与控制实验室，
- en: Transilvania University of Brasov.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 布拉索夫大学。
- en: Brasov, Romania
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 罗马尼亚布拉索夫
- en: Bogdan.Trasnea@elektrobit.com
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Bogdan.Trasnea@elektrobit.com
- en: \ANDTiberiu Cocias
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \ANDTiberiu Cocias
- en: Artificial Intelligence,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，
- en: Elektrobit Automotive.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Elektrobit Automotive.
- en: Robotics, Vision and Control Lab,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人、视觉与控制实验室，
- en: Transilvania University of Brasov.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 布拉索夫大学。
- en: Brasov, Romania
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 罗马尼亚布拉索夫
- en: Tiberiu.Cocias@elektrobit.com
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Tiberiu.Cocias@elektrobit.com
- en: '&Gigel Macesanu'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&Gigel Macesanu'
- en: Artificial Intelligence,
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，
- en: Elektrobit Automotive.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Elektrobit Automotive.
- en: Robotics, Vision and Control Lab,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人、视觉与控制实验室，
- en: Transilvania University of Brasov.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 布拉索夫大学。
- en: Brasov, Romania
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 罗马尼亚布拉索夫
- en: 'Gigel.Macesanu@elektrobit.com The authors are with Elektrobit Automotive and
    the Robotics, Vision and Control Laboratory (ROVIS Lab) at the Department of Automation
    and Information Technology, Transilvania University of Brasov, 500036 Romania.
    E-mail: (see [http://rovislab.com/sorin_grigorescu.html](http://rovislab.com/sorin_grigorescu.html)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Gigel.Macesanu@elektrobit.com 作者来自Electrobit Automotive和布拉索夫大学自动化与信息技术系的机器人、视觉与控制实验室（ROVIS
    Lab），邮寄地址：请见 [http://rovislab.com/sorin_grigorescu.html](http://rovislab.com/sorin_grigorescu.html)。
- en: Abstract
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The last decade witnessed increasingly rapid progress in self-driving vehicle
    technology, mainly backed up by advances in the area of deep learning and artificial
    intelligence. The objective of this paper is to survey the current state-of-the-art
    on deep learning technologies used in autonomous driving. We start by presenting
    AI-based self-driving architectures, convolutional and recurrent neural networks,
    as well as the deep reinforcement learning paradigm. These methodologies form
    a base for the surveyed driving scene perception, path planning, behavior arbitration
    and motion control algorithms. We investigate both the modular perception-planning-action
    pipeline, where each module is built using deep learning methods, as well as End2End
    systems, which directly map sensory information to steering commands. Additionally,
    we tackle current challenges encountered in designing AI architectures for autonomous
    driving, such as their safety, training data sources and computational hardware.
    The comparison presented in this survey helps to gain insight into the strengths
    and limitations of deep learning and AI approaches for autonomous driving and
    assist with design choices.¹¹1The articles referenced in this survey can be accessed
    at the web-page accompanying this paper, available at [http://rovislab.com/survey_DL_AD.html](http://rovislab.com/survey_DL_AD.html)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年，自驾车技术取得了越来越快速的进展，这主要得益于深度学习和人工智能领域的突破。本文的目的是调查在自动驾驶中使用的深度学习技术的最新进展。我们首先介绍了基于AI的自驾车架构、卷积和递归神经网络，以及深度强化学习范式。这些方法为调查的驾驶场景感知、路径规划、行为裁决和运动控制算法奠定了基础。我们探讨了模块化的感知-规划-行动流程，其中每个模块都使用深度学习方法构建，以及端到端系统，它直接将感官信息映射到转向命令。此外，我们还讨论了设计AI架构用于自动驾驶中遇到的当前挑战，如安全性、训练数据来源和计算硬件。本调查中的比较有助于深入了解深度学习和AI方法在自动驾驶中的优缺点，并辅助设计选择。¹¹1本文参考的文章可通过本论文附带的网页访问，网址为
    [http://rovislab.com/survey_DL_AD.html](http://rovislab.com/survey_DL_AD.html)
- en: Contents
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内容
- en: '[1 Introduction](#S1 "In A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](#S1 "在《自动驾驶深度学习技术调查》中")'
- en: '[2 Deep Learning based Decision-Making Architectures for Self-Driving Cars](#S2
    "In A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 基于深度学习的自驾车决策架构](#S2 "在《自动驾驶深度学习技术调查》中")'
- en: '[3 Overview of Deep Learning Technologies](#S3 "In A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 深度学习技术概述](#S3 "在 自主驾驶深度学习技术调查")'
- en: '[3.1 Deep Convolutional Neural Networks](#S3.SS1 "In 3 Overview of Deep Learning
    Technologies ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 深度卷积神经网络](#S3.SS1 "在 3 深度学习技术概述 ‣ 自主驾驶深度学习技术调查")'
- en: '[3.2 Recurrent Neural Networks](#S3.SS2 "In 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 循环神经网络](#S3.SS2 "在 3 深度学习技术概述 ‣ 自主驾驶深度学习技术调查")'
- en: '[3.3 Deep Reinforcement Learning](#S3.SS3 "In 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 深度强化学习](#S3.SS3 "在 3 深度学习技术概述 ‣ 自主驾驶深度学习技术调查")'
- en: '[4 Deep Learning for Driving Scene Perception and Localization](#S4 "In A Survey
    of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 深度学习用于驾驶场景感知和定位](#S4 "在 自主驾驶深度学习技术调查")'
- en: '[4.1 Sensing Hardware: Camera vs. LiDAR Debate](#S4.SS1 "In 4 Deep Learning
    for Driving Scene Perception and Localization ‣ A Survey of Deep Learning Techniques
    for Autonomous Driving")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 传感硬件：相机与 LiDAR 辩论](#S4.SS1 "在 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[4.2 Driving Scene Understanding](#S4.SS2 "In 4 Deep Learning for Driving Scene
    Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 驾驶场景理解](#S4.SS2 "在 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[4.2.1 Bounding-Box-Like Object Detectors](#S4.SS2.SSS1 "In 4.2 Driving Scene
    Understanding ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.1 类似于边界框的物体检测器](#S4.SS2.SSS1 "在 4.2 驾驶场景理解 ‣ 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[4.2.2 Semantic and Instance Segmentation](#S4.SS2.SSS2 "In 4.2 Driving Scene
    Understanding ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.2 语义和实例分割](#S4.SS2.SSS2 "在 4.2 驾驶场景理解 ‣ 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[4.2.3 Localization](#S4.SS2.SSS3 "In 4.2 Driving Scene Understanding ‣ 4 Deep
    Learning for Driving Scene Perception and Localization ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2.3 定位](#S4.SS2.SSS3 "在 4.2 驾驶场景理解 ‣ 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[4.3 Perception using Occupancy Maps](#S4.SS3 "In 4 Deep Learning for Driving
    Scene Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 使用占用图的感知](#S4.SS3 "在 4 深度学习用于驾驶场景感知和定位 ‣ 自主驾驶深度学习技术调查")'
- en: '[5 Deep Learning for Path Planning and Behavior Arbitration](#S5 "In A Survey
    of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 深度学习用于路径规划和行为仲裁](#S5 "在 自主驾驶深度学习技术调查")'
- en: '[6 Motion Controllers for AI-based Self-Driving Cars](#S6 "In A Survey of Deep
    Learning Techniques for Autonomous Driving")'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 基于 AI 的自动驾驶汽车运动控制器](#S6 "在 自主驾驶深度学习技术调查")'
- en: '[6.1 Learning Controllers](#S6.SS1 "In 6 Motion Controllers for AI-based Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 学习控制器](#S6.SS1 "在 6 基于 AI 的自动驾驶汽车运动控制器 ‣ 自主驾驶深度学习技术调查")'
- en: '[6.2 End2End Learning Control](#S6.SS2 "In 6 Motion Controllers for AI-based
    Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 端到端学习控制](#S6.SS2 "在 6 基于 AI 的自动驾驶汽车运动控制器 ‣ 自主驾驶深度学习技术调查")'
- en: '[7 Safety of Deep Learning in Autonomous Driving](#S7 "In A Survey of Deep
    Learning Techniques for Autonomous Driving")'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 深度学习在自主驾驶中的安全性](#S7 "在 自主驾驶深度学习技术调查")'
- en: '[8 Data Sources for Training Autonomous Driving Systems](#S8 "In A Survey of
    Deep Learning Techniques for Autonomous Driving")'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 用于训练自主驾驶系统的数据来源](#S8 "在 自主驾驶深度学习技术调查")'
- en: '[9 Computational Hardware and Deployment](#S9 "In A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[9 计算硬件与部署](#S9 "在 自主驾驶深度学习技术调查")'
- en: '[10 Discussion and Conclusions](#S10 "In A Survey of Deep Learning Techniques
    for Autonomous Driving")'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10 讨论与结论](#S10 "在 自主驾驶深度学习技术调查")'
- en: '[10.1 Final Notes](#S10.SS1 "In 10 Discussion and Conclusions ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving")'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[10.1 最终说明](#S10.SS1 "在 10 讨论与结论 ‣ 自动驾驶深度学习技术的调查")'
- en: 1 Introduction
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Over the course of the last decade, Deep Learning and Artificial Intelligence
    (AI) became the main technologies behind many breakthroughs in computer vision [[1](#bib.bib1)],
    robotics [[2](#bib.bib2)] and Natural Language Processing (NLP) [[3](#bib.bib3)].
    They also have a major impact in the autonomous driving revolution seen today
    both in academia and industry. Autonomous Vehicles (AVs) and self-driving cars
    began to migrate from laboratory development and testing conditions to driving
    on public roads. Their deployment in our environmental landscape offers a decrease
    in road accidents and traffic congestions, as well as an improvement of our mobility
    in overcrowded cities. The title of ”self-driving” may seem self-evident, but
    there are actually five SAE Levels used to define autonomous driving. The SAE
    J3016 standard [[4](#bib.bib4)] introduces a scale from 0 to 5 for grading vehicle
    automation. Lower SAE Levels feature basic driver assistance, whilst higher SAE
    Levels move towards vehicles requiring no human interaction whatsoever. Cars in
    the level 5 category require no human input and typically will not even feature
    steering wheels or foot pedals.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习和人工智能（AI）成为计算机视觉[[1](#bib.bib1)]、机器人技术[[2](#bib.bib2)]和自然语言处理（NLP）[[3](#bib.bib3)]许多突破的主要技术。它们也对今天学术界和工业界的自动驾驶革命产生了重大影响。自动驾驶汽车（AVs）和自驾车开始从实验室开发和测试条件迁移到公共道路驾驶。它们在我们环境中的部署减少了道路事故和交通拥堵，同时改善了我们在拥挤城市中的出行。虽然“自驾”一词似乎是不言而喻的，但实际上有五个SAE等级用于定义自动驾驶。SAE
    J3016标准[[4](#bib.bib4)]引入了一个从0到5的等级尺度来评估车辆自动化。较低的SAE等级具有基本的驾驶辅助功能，而较高的SAE等级则趋向于不需要任何人工交互的车辆。5级汽车不需要人工输入，通常甚至不具备方向盘或踏板。
- en: Although most driving scenarios can be relatively simply solved with classical
    perception, path planning and motion control methods, the remaining unsolved scenarios
    are corner cases in which traditional methods fail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数驾驶场景可以通过经典的感知、路径规划和运动控制方法相对简单地解决，但剩下的未解决场景则是传统方法失效的边界情况。
- en: One of the first autonomous cars was developed by Ernst Dickmanns [[5](#bib.bib5)]
    in the 1980s. This paved the way for new research projects, such as PROMETHEUS,
    which aimed to develop a fully functional autonomous car. In 1994, the VaMP driverless
    car managed to drive $1,600km$, out of which $95\%$ were driven autonomously.
    Similarly, in 1995, CMU NAVLAB demonstrated autonomous driving on $6,000km$, with
    $98\%$ driven autonomously. Another important milestone in autonomous driving
    were the DARPA Grand Challenges in 2004 and 2005, as well as the DARPA Urban Challenge
    in 2007\. The goal was for a driverless car to navigate an off-road course as
    fast as possible, without human intervention. In 2004, none of the 15 vehicles
    completed the race. Stanley, the winner of the 2005 race, leveraged Machine Learning
    techniques for navigating the unstructured environment. This was a turning point
    in self-driving cars development, acknowledging Machine Learning and AI as central
    components of autonomous driving. The turning point is also notable in this survey
    paper, since the majority of the surveyed work is dated after 2005.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 20世纪80年代，恩斯特·迪克曼斯[[5](#bib.bib5)]开发了其中一辆首批自动驾驶汽车。这为新的研究项目铺平了道路，如PROMETHEUS，旨在开发一辆完全功能的自动驾驶汽车。1994年，VaMP无人驾驶汽车成功行驶了$1,600km$，其中$95\%$是自动驾驶的。类似地，1995年，CMU
    NAVLAB在$6,000km$上展示了自动驾驶，$98\%$为自动驾驶。2004年和2005年的DARPA大奖挑战赛以及2007年的DARPA城市挑战赛是自动驾驶的另一个重要里程碑。目标是让无人驾驶汽车在没有人工干预的情况下尽可能快地导航越野路线。2004年，15辆参赛车辆中没有一辆完成比赛。2005年比赛的获胜者Stanley利用了机器学习技术来导航非结构化环境。这标志着自动驾驶汽车发展的一个转折点，承认机器学习和人工智能作为自动驾驶的核心组成部分。这个转折点在这篇调查论文中也很突出，因为大多数调查工作都在2005年之后。
- en: In this survey, we review the different artificial intelligence and deep learning
    technologies used in autonomous driving, and provide a survey on state-of-the-art
    deep learning and AI methods applied to self-driving cars. We also dedicate complete
    sections on tackling safety aspects, the challenge of training data sources and
    the required computational hardware.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次调查中，我们回顾了在自动驾驶中使用的不同人工智能和深度学习技术，并提供了关于应用于自动驾驶汽车的最先进的深度学习和 AI 方法的综述。我们还专门分配了完整的章节来处理安全方面的问题、训练数据源的挑战以及所需的计算硬件。
- en: 2 Deep Learning based Decision-Making Architectures for Self-Driving Cars
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于深度学习的自动驾驶汽车决策架构
- en: Self-driving cars are autonomous decision-making systems that process streams
    of observations coming from different on-board sources, such as cameras, radars,
    LiDARs, ultrasonic sensors, GPS units and/or inertial sensors. These observations
    are used by the car’s computer to make driving decisions. The basic block diagrams
    of an AI powered autonomous car are shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep
    Learning based Decision-Making Architectures for Self-Driving Cars ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving"). The driving decisions are
    computed either in a modular perception-planning-action pipeline (Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(a)), or in
    an End2End learning fashion (Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based
    Decision-Making Architectures for Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")(b)), where sensory information is directly
    mapped to control outputs. The components of the modular pipeline can be designed
    either based on AI and deep learning methodologies, or using classical non-learning
    approaches. Various permutations of learning and non-learning based components
    are possible (e.g. a deep learning based object detector provides input to a classical
    A-star path planning algorithm). A safety monitor is designed to assure the safety
    of each module.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车是自主决策系统，处理来自不同车载源的观察数据流，如摄像头、雷达、LiDAR、超声波传感器、GPS 单元和/或惯性传感器。这些观察数据被汽车的计算机用来做出驾驶决策。基于
    AI 的自动驾驶汽车的基本框图如图 [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making
    Architectures for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for
    Autonomous Driving") 所示。驾驶决策要么在模块化的感知-规划-行动流水线中计算（图 [1](#S2.F1 "Figure 1 ‣ 2 Deep
    Learning based Decision-Making Architectures for Self-Driving Cars ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving")(a)），要么以 End2End 学习的方式计算（图
    [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for
    Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(b)），其中传感器信息直接映射到控制输出。模块化流水线的组件可以基于
    AI 和深度学习方法设计，也可以使用经典的非学习方法。各种学习和非学习组件的组合是可能的（例如，基于深度学习的物体检测器向经典的 A-star 路径规划算法提供输入）。安全监控器的设计旨在确保每个模块的安全。
- en: '![Refer to caption](img/33a7e1b466baddacfdaef03824e9a545.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/33a7e1b466baddacfdaef03824e9a545.png)'
- en: 'Figure 1: Deep Learning based self-driving car. The architecture can be implemented
    either as a sequential perception-planing-action pipeline (a), or as an End2End
    system (b). In the sequential pipeline case, the components can be designed either
    using AI and deep learning methodologies, or based on classical non-learning approaches.
    End2End learning systems are mainly based on deep learning methods. A safety monitor
    is usually designed to ensure the safety of each module.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于深度学习的自动驾驶汽车。该架构可以实现为顺序感知-规划-行动流水线 (a)，或 End2End 系统 (b)。在顺序流水线的情况下，组件可以使用
    AI 和深度学习方法设计，也可以基于经典的非学习方法。End2End 学习系统主要基于深度学习方法。通常设计一个安全监控器以确保每个模块的安全。
- en: 'The modular pipeline in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making
    Architectures for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for
    Autonomous Driving")(a) is hierarchically decomposed into four components which
    can be designed using either deep learning and AI approaches, or classical methods.
    These components are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures
    for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(a)
    中的模块化流水线被分解为四个组件，这些组件可以使用深度学习和 AI 方法设计，也可以使用经典方法。这些组件包括：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perception and Localization,
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知与定位，
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: High-Level Path Planning,
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级路径规划，
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Behavior Arbitration, or low-level path planning,
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行为仲裁，或低级路径规划，
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Motion Controllers.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作控制器。
- en: Based on these four high-level components, we have grouped together relevant
    deep learning papers describing methods developed for autonomous driving systems.
    Additional to the reviewed algorithms, we have also grouped relevant articles
    covering the safety, data sources and hardware aspects encountered when designing
    deep learning modules for self-driving cars.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这四个高层次组件，我们将相关的深度学习论文进行了汇总，这些论文描述了为自动驾驶系统开发的方法。除了审查的算法外，我们还汇总了涉及设计深度学习模块用于自动驾驶汽车时遇到的安全性、数据源和硬件方面的相关文章。
- en: Given a route planned through the road network, the first task of an autonomous
    car is to understand and localize itself in the surrounding environment. Based
    on this representation, a continuous path is planned and the future actions of
    the car are determined by the behavior arbitration system. Finally, a motion control
    system reactively corrects errors generated in the execution of the planned motion.
    A review of classical non-AI design methodologies for these four components can
    be found in [[6](#bib.bib6)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 给定通过道路网络规划的路线，自动驾驶汽车的第一个任务是理解并定位自身在周围环境中的位置。基于这一表示，会规划出一条连续路径，并由行为仲裁系统确定汽车的未来动作。最后，运动控制系统会对规划运动执行过程中产生的错误进行反应性修正。有关这四个组件的经典非AI设计方法的综述可以在 [[6](#bib.bib6)]中找到。
- en: Following, we will give an introduction of deep learning and AI technologies
    used in autonomous driving, as well as surveying different methodologies used
    to design the hierarchical decision making process described above. Additionally,
    we provide an overview of End2End learning systems used to encode the hierarchical
    process into a single deep learning architecture which directly maps sensory observations
    to control outputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍用于自动驾驶的深度学习和人工智能技术，并调查用于设计上述分层决策过程的不同方法。此外，我们还提供了用于将分层过程编码到单一深度学习架构中的End2End学习系统的概述，该架构直接将传感器观测映射到控制输出。
- en: 3 Overview of Deep Learning Technologies
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习技术概述
- en: In this section, we describe the basis of deep learning technologies used in
    autonomous vehicles and comment on the capabilities of each paradigm. We focus
    on Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Deep
    Reinforcement Learning (DRL), which are the most common deep learning methodologies
    applied to autonomous driving.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了用于自动驾驶车辆的深度学习技术的基础，并评论了每种范式的能力。我们重点介绍卷积神经网络（CNN）、递归神经网络（RNN）和深度强化学习（DRL），这些是应用于自动驾驶的最常见的深度学习方法。
- en: Throughout the survey, we use the following notations to describe time dependent
    sequences. The value of a variable is defined either for a single discrete time
    step $t$, written as superscript $<t>$, or as a discrete sequence defined in the
    $<t,t+k>$ time interval, where $k$ denotes the length of the sequence. For example,
    the value of a state variable $\mathbf{z}$ is defined either at discrete time
    $t$, as $\mathbf{z}^{<t>}$, or within a sequence interval $\mathbf{z}^{<t,t+k>}$.
    Vectors and matrices are indicated by bold symbols.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个调查中，我们使用以下符号来描述时间依赖的序列。变量的值定义为单个离散时间步$t$，写作上标$<t>$，或定义在$<t,t+k>$时间区间内的离散序列，其中$k$表示序列的长度。例如，状态变量$\mathbf{z}$的值可以在离散时间$t$定义为$\mathbf{z}^{<t>}$，或在序列区间内定义为$\mathbf{z}^{<t,t+k>}$.
    向量和矩阵用粗体符号表示。
- en: 3.1 Deep Convolutional Neural Networks
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度卷积神经网络
- en: Convolutional Neural Networks (CNN) are mainly used for processing spatial information,
    such as images, and can be viewed as image features extractors and universal non-linear
    function approximators [[7](#bib.bib7)], [[8](#bib.bib8)]. Before the rise of
    deep learning, computer vision systems used to be implemented based on handcrafted
    features, such as HAAR [[9](#bib.bib9)], Local Binary Patterns (LBP) [[10](#bib.bib10)],
    or Histograms of Oriented Gradients (HoG) [[11](#bib.bib11)]. In comparison to
    these traditional handcrafted features, convolutional neural networks are able
    to automatically learn a representation of the feature space encoded in the training
    set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）主要用于处理空间信息，如图像，可以视为图像特征提取器和通用非线性函数逼近器 [[7](#bib.bib7)]、[[8](#bib.bib8)]。在深度学习兴起之前，计算机视觉系统通常基于手工设计的特征实现，如HAAR [[9](#bib.bib9)]、局部二值模式（LBP） [[10](#bib.bib10)]或方向梯度直方图（HoG） [[11](#bib.bib11)]。与这些传统的手工设计特征相比，卷积神经网络能够自动学习训练集中的特征空间表示。
- en: 'CNNs can be loosely understood as very approximate analogies to different parts
    of the mammalian visual cortex [[12](#bib.bib12)]. An image formed on the retina
    is sent to the visual cortex through the thalamus. Each brain hemisphere has its
    own visual cortex. The visual information is received by the visual cortex in
    a crossed manner: the left visual cortex receives information from the right eye,
    while the right visual cortex is fed with visual data from the left eye. The information
    is processed according to the dual flux theory [[13](#bib.bib13)], which states
    that the visual flow follows two main fluxes: a ventral flux, responsible for
    visual identification and object recognition, and a dorsal flux used for establishing
    spatial relations between objects. A CNN mimics the functioning of the ventral
    flux, in which different areas of the brain are sensible to specific features
    in the visual field. The earlier brain cells in the visual cortex are activated
    by sharp transitions in the visual field of view, in the same way in which an
    edge detector highlights sharp transitions between the neighboring pixels in an
    image. These edges are further used in the brain to approximate object parts and
    finally to estimate abstract representations of objects.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 可以粗略地理解为哺乳动物视觉皮层不同部分的非常近似类比 [[12](#bib.bib12)]。在视网膜上形成的图像通过丘脑传送到视觉皮层。每个脑半球都有自己的视觉皮层。视觉信息以交叉的方式被视觉皮层接收：左视觉皮层接收来自右眼的信息，而右视觉皮层接收来自左眼的视觉数据。信息的处理遵循双流理论
    [[13](#bib.bib13)]，该理论指出视觉流遵循两个主要流动：腹侧流负责视觉识别和对象识别，背侧流用于建立对象之间的空间关系。CNN 模仿腹侧流的功能，其中大脑的不同区域对视觉场中的特定特征敏感。视觉皮层中较早的脑细胞通过视觉场中的尖锐过渡被激活，就像边缘检测器突出显示图像中相邻像素之间的尖锐过渡一样。这些边缘在大脑中进一步用于近似对象部分，并最终估计对象的抽象表示。
- en: An CNN is parametrized by its weights vector $\theta=[\mathbf{W},\mathbf{b}]$,
    where $\mathbf{W}$ is the set of weights governing the inter-neural connections
    and $\mathbf{b}$ is the set of neuron bias values. The set of weights $\mathbf{W}$
    is organized as image filters, with coefficients learned during training. Convolutional
    layers within a CNN exploit local spatial correlations of image pixels to learn
    translation-invariant convolution filters, which capture discriminant image features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 通过其权重向量 $\theta=[\mathbf{W},\mathbf{b}]$ 参数化，其中 $\mathbf{W}$ 是控制神经元连接的权重集合，$\mathbf{b}$
    是神经元偏置值集合。权重集合 $\mathbf{W}$ 组织为图像滤波器，系数在训练期间学习得到。CNN 中的卷积层利用图像像素的局部空间相关性来学习平移不变的卷积滤波器，这些滤波器捕捉到图像的判别特征。
- en: 'Consider a multichannel signal representation $\mathbf{M}_{k}$ in layer $k$,
    which is a channel-wise integration of signal representations $\mathbf{M}_{k,c}$,
    where $c\in\mathbb{N}$. A signal representation can be generated in layer $k+1$
    as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑第 $k$ 层中的多通道信号表示 $\mathbf{M}_{k}$，这是信号表示 $\mathbf{M}_{k,c}$ 的通道级整合，其中 $c\in\mathbb{N}$。信号表示可以在第
    $k+1$ 层生成，如下所示：
- en: '|  | $\mathbf{M}_{k+1,l}=\varphi(\mathbf{M}_{k}*\mathbf{w}_{k,l}+\mathbf{b}_{k,l}),$
    |  | (1) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{M}_{k+1,l}=\varphi(\mathbf{M}_{k}*\mathbf{w}_{k,l}+\mathbf{b}_{k,l}),$
    |  | (1) |'
- en: where $\mathbf{w}_{k,l}\in\mathbf{W}$ is a convolutional filter with the same
    number of channels as $\mathbf{M}_{k}$, $\mathbf{b}_{k,l}\in\mathbf{b}$ represents
    the bias, $l$ is a channel index and $*$ denotes the convolution operation. $\varphi(\cdot)$
    is an activation function applied to each pixel in the input signal. Typically,
    the Rectified Linear Unit (ReLU) is the most commonly used activation function
    in computer vision applications [[1](#bib.bib1)]. The final layer of a CNN is
    usually a fully-connected layer which acts as an object discriminator on a high-level
    abstract representation of objects.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{w}_{k,l}\in\mathbf{W}$ 是一个具有与 $\mathbf{M}_{k}$ 相同通道数的卷积滤波器，$\mathbf{b}_{k,l}\in\mathbf{b}$
    表示偏置，$l$ 是通道索引，$*$ 表示卷积操作。$\varphi(\cdot)$ 是应用于输入信号每个像素的激活函数。通常，Rectified Linear
    Unit (ReLU) 是计算机视觉应用中最常用的激活函数 [[1](#bib.bib1)]。CNN 的最终层通常是全连接层，它作为对象判别器，对对象的高层抽象表示进行分类。
- en: 'In a supervised manner, the response $R(\cdot;\theta)$ of a CNN can be trained
    using a training database $\mathcal{D}=[(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{m},y_{m})]$,
    where $\mathbf{x}_{i}$ is a data sample, $y_{i}$ is the corresponding label and
    $m$ is the number of training examples. The optimal network parameters can be
    calculated using Maximum Likelihood Estimation (MLE). For the clarity of explanation,
    we take as example the simple least-squares error function, which can be used
    to drive the MLE process when training regression estimators:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，CNN 的响应 $R(\cdot;\theta)$ 可以使用训练数据库 $\mathcal{D}=[(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{m},y_{m})]$
    进行训练，其中 $\mathbf{x}_{i}$ 是数据样本，$y_{i}$ 是对应的标签，$m$ 是训练示例的数量。可以使用最大似然估计（MLE）来计算最优网络参数。为了说明的清晰，我们以简单的最小二乘误差函数作为例子，该函数可以用于在训练回归估计器时推动
    MLE 过程：
- en: '|  | $\mathbf{\hat{\theta}}=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})=\arg\min_{\theta}\sum^{m}_{i=1}(R(\mathbf{x}_{i};\theta)-y_{i})^{2}.$
    |  | (2) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\hat{\theta}}=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})=\arg\min_{\theta}\sum^{m}_{i=1}(R(\mathbf{x}_{i};\theta)-y_{i})^{2}.$
    |  | (2) |'
- en: For classification purposes, the least-squares error is usually replaced by
    the cross-entropy, or the negative log-likelihood loss functions. The optimization
    problem in Eq. [2](#S3.E2 "In 3.1 Deep Convolutional Neural Networks ‣ 3 Overview
    of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving") is typically solved with Stochastic Gradient Descent (SGD) and the backpropagation
    algorithm for gradient estimation [[14](#bib.bib14)]. In practice, different variants
    of SGD are used, such as Adam [[15](#bib.bib15)] or AdaGrad [[16](#bib.bib16)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类目的，最小二乘误差通常被交叉熵或负对数似然损失函数所替代。公式 [2](#S3.E2 "在 3.1 深度卷积神经网络 ‣ 3 深度学习技术概述
    ‣ 自主驾驶深度学习技术调查") 中的优化问题通常使用随机梯度下降（SGD）和反向传播算法进行梯度估计 [[14](#bib.bib14)]。在实践中，使用了不同的
    SGD 变体，例如 Adam [[15](#bib.bib15)] 或 AdaGrad [[16](#bib.bib16)]。
- en: 3.2 Recurrent Neural Networks
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 递归神经网络
- en: Among deep learning techniques, Recurrent Neural Networks (RNN) are especially
    good in processing temporal sequence data, such as text, or video streams. Different
    from conventional neural networks, a RNN contains a time dependent feedback loop
    in its memory cell. Given a time dependent input sequence $[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t>}]$
    and an output sequence $[\mathbf{z}^{<t+1>},...,\mathbf{z}^{<t+\tau_{o}>}]$, a
    RNN can be ”unfolded” $\tau_{i}+\tau_{o}$ times to generate a loop-less network
    architecture matching the input length, as illustrated in Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies ‣
    A Survey of Deep Learning Techniques for Autonomous Driving"). $t$ represents
    a temporal index, while $\tau_{i}$ and $\tau_{o}$ are the lengths of the input
    and output sequences, respectively. Such neural networks are also encountered
    under the name of sequence-to-sequence models. An unfolded network has $\tau_{i}+\tau_{o}+1$
    identical layers, that is, each layer shares the same learned weights. Once unfolded,
    a RNN can be trained using the backpropagation through time algorithm. When compared
    to a conventional neural network, the only difference is that the learned weights
    in each unfolded copy of the network are averaged, thus enabling the network to
    shared the same weights over time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习技术中，递归神经网络（RNN）特别擅长处理时间序列数据，如文本或视频流。与传统神经网络不同，RNN 在其记忆单元中包含一个时间依赖的反馈循环。给定一个时间依赖的输入序列
    $[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t>}]$ 和一个输出序列 $[\mathbf{z}^{<t+1>},...,\mathbf{z}^{<t+\tau_{o}>}]$，RNN
    可以“展开” $\tau_{i}+\tau_{o}$ 次以生成一个匹配输入长度的无环网络结构，如图 [2](#S3.F2 "图 2 ‣ 3.2 递归神经网络
    ‣ 3 深度学习技术概述 ‣ 自主驾驶深度学习技术调查") 所示。$t$ 代表时间索引，而 $\tau_{i}$ 和 $\tau_{o}$ 分别是输入和输出序列的长度。这样的神经网络也被称为序列到序列模型。展开的网络有
    $\tau_{i}+\tau_{o}+1$ 个相同的层，即每一层共享相同的学习权重。一旦展开，RNN 可以使用时间反向传播算法进行训练。与传统神经网络相比，唯一的不同是每个展开副本中的学习权重会被平均，从而使网络能够在时间上共享相同的权重。
- en: '![Refer to caption](img/8fa54620f461cb53e71a774ee2d1e31c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8fa54620f461cb53e71a774ee2d1e31c.png)'
- en: 'Figure 2: A folded (a) and unfolded (b) over time, many-to-many Recurrent Neural
    Network. Over time $t$, both the input $\mathbf{s}^{<t-\tau_{i},t>}$ and output
    $\mathbf{z}^{<t+1,t+\tau_{o}>}$ sequences share the same weights $\mathbf{h}^{<\cdot>}$.
    The architecture is also referred to as a sequence-to-sequence model.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：折叠的（a）和展开的（b）随时间变化的多对多递归神经网络。随着时间$t$的推移，输入$\mathbf{s}^{<t-\tau_{i},t>}$和输出$\mathbf{z}^{<t+1,t+\tau_{o}>$序列共享相同的权重$\mathbf{h}^{<\cdot>}
    $。该架构也称为序列到序列模型。
- en: The main challenge in using basic RNNs is the vanishing gradient encountered
    during training. The gradient signal can end up being multiplied a large number
    of times, as many as the number of time steps. Hence, a traditional RNN is not
    suitable for capturing long-term dependencies in sequence data. If a network is
    very deep, or processes long sequences, the gradient of the network’s output would
    have a hard time in propagating back to affect the weights of the earlier layers.
    Under gradient vanishing, the weights of the network will not be effectively updated,
    ending up with very small weight values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基本RNN的主要挑战是训练过程中遇到的梯度消失问题。梯度信号可能被乘以大量的次数，最多达到时间步的数量。因此，传统RNN不适合捕捉序列数据中的长期依赖性。如果网络非常深，或者处理长序列，网络输出的梯度将难以传播回去影响早期层的权重。在梯度消失的情况下，网络的权重将无法有效更新，最终得到非常小的权重值。
- en: Long Short-Term Memory (LSTM) [[17](#bib.bib17)] networks are non-linear function
    approximators for estimating temporal dependencies in sequence data. As opposed
    to traditional recurrent neural networks, LSTMs solve the vanishing gradient problem
    by incorporating three gates, which control the input, output and memory state.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）[[17](#bib.bib17)]网络是非线性函数逼近器，用于估计序列数据中的时间依赖性。与传统的递归神经网络不同，LSTM通过引入三个门来解决梯度消失问题，这三个门分别控制输入、输出和记忆状态。
- en: 'Recurrent layers exploit temporal correlations of sequence data to learn time
    dependent neural structures. Consider the memory state $\mathbf{c}^{<t-1>}$ and
    the output state $\mathbf{h}^{<t-1>}$ in an LSTM network, sampled at time step
    $t-1$, as well as the input data $\mathbf{s}^{<t>}$ at time $t$. The opening or
    closing of a gate is controlled by a sigmoid function $\sigma(\cdot)$ of the current
    input signal $\mathbf{s}^{<t>}$ and the output signal of the last time point $\mathbf{h}^{<t-1>}$,
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 循环层利用序列数据的时间相关性来学习时间依赖的神经结构。考虑在时间步$t-1$采样的LSTM网络中的记忆状态$\mathbf{c}^{<t-1>}$和输出状态$\mathbf{h}^{<t-1>}$，以及时间$t$的输入数据$\mathbf{s}^{<t>}
    $。门的开启或关闭由当前输入信号$\mathbf{s}^{<t>} $和上一个时间点的输出信号$\mathbf{h}^{<t-1>}$的sigmoid函数$\sigma(\cdot)$控制，如下所示：
- en: '|  | $\Gamma_{u}^{<t>}=\sigma(\mathbf{W}_{u}\mathbf{s}^{<t>}+\mathbf{U}_{u}\mathbf{h}^{<t-1>}+\mathbf{b}_{u}),$
    |  | (3) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma_{u}^{<t>}=\sigma(\mathbf{W}_{u}\mathbf{s}^{<t>}+\mathbf{U}_{u}\mathbf{h}^{<t-1>}+\mathbf{b}_{u}),$
    |  | (3) |'
- en: '|  | $\Gamma_{f}^{<t>}=\sigma(\mathbf{W}_{f}\mathbf{s}^{<t>}+\mathbf{U}_{f}\mathbf{h}^{<t-1>}+\mathbf{b}_{f}),$
    |  | (4) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma_{f}^{<t>}=\sigma(\mathbf{W}_{f}\mathbf{s}^{<t>}+\mathbf{U}_{f}\mathbf{h}^{<t-1>}+\mathbf{b}_{f}),$
    |  | (4) |'
- en: '|  | $\Gamma_{o}^{<t>}=\sigma(\mathbf{W}_{o}\mathbf{s}^{<t>}+\mathbf{U}_{o}\mathbf{h}^{<t-1>}+\mathbf{b}_{o}),$
    |  | (5) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Gamma_{o}^{<t>}=\sigma(\mathbf{W}_{o}\mathbf{s}^{<t>}+\mathbf{U}_{o}\mathbf{h}^{<t-1>}+\mathbf{b}_{o}),$
    |  | (5) |'
- en: 'where $\Gamma_{u}^{<t>}$, $\Gamma_{f}^{<t>}$ and $\Gamma_{o}^{<t>}$ are gate
    functions of the input gate, forget gate and output gate, respectively. Given
    current observation, the memory state $\mathbf{c}^{<t>}$ will be updated as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Gamma_{u}^{<t>} $、$\Gamma_{f}^{<t>} $和$\Gamma_{o}^{<t>} $分别是输入门、遗忘门和输出门的门函数。给定当前观察，记忆状态$\mathbf{c}^{<t>}
    $将更新为：
- en: '|  | $\mathbf{c}^{<t>}=\Gamma_{u}^{<t>}*\tanh(\mathbf{W}_{c}\mathbf{s}^{<t>}+\mathbf{U}_{c}\mathbf{h}^{<t-1>}+\mathbf{b}_{c})+\Gamma_{f}*\mathbf{c}^{<t-1>},$
    |  | (6) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{c}^{<t>}=\Gamma_{u}^{<t>}*\tanh(\mathbf{W}_{c}\mathbf{s}^{<t>}+\mathbf{U}_{c}\mathbf{h}^{<t-1>}+\mathbf{b}_{c})+\Gamma_{f}*\mathbf{c}^{<t-1>},$
    |  | (6) |'
- en: 'The new network output $\mathbf{h}^{<t>}$ is computed as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 新的网络输出$\mathbf{h}^{<t>} $的计算公式为：
- en: '|  | $\mathbf{h}^{<t>}=\Gamma_{o}^{<t>}*\tanh(\mathbf{c}^{<t>}).$ |  | (7)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{h}^{<t>}=\Gamma_{o}^{<t>}*\tanh(\mathbf{c}^{<t>}).$ |  | (7)
    |'
- en: An LSTM network $Q$ is parametrized by $\theta=[\mathbf{W}_{i},\mathbf{U}_{i},\mathbf{b}_{i}]$,
    where $\mathbf{W}_{i}$ represents the weights of the network’s gates and memory
    cell multiplied with the input state, $\mathbf{U}_{i}$ are the weights governing
    the activations and $\mathbf{b}_{i}$ denotes the set of neuron bias values. $*$
    symbolizes element-wise multiplication.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LSTM网络$Q$由$\theta=[\mathbf{W}_{i},\mathbf{U}_{i},\mathbf{b}_{i}]$参数化，其中$\mathbf{W}_{i}$表示网络门和记忆单元的权重与输入状态的乘积，$\mathbf{U}_{i}$是控制激活的权重，$\mathbf{b}_{i}$表示神经元偏置值的集合。$*$表示逐元素乘法。
- en: 'In a supervised learning setup, given a set of training sequences $\mathcal{D}=[(\mathbf{s}^{<t-\tau_{i},t>}_{1},\mathbf{z}^{<t+1,t+\tau_{o}>}_{1}),...,(\mathbf{s}^{<t-\tau_{i},t>}_{q},\mathbf{z}^{<t+1,t+\tau_{o}>}_{q})]$,
    that is, $q$ independent pairs of observed sequences with assignments $\mathbf{z}^{<t,t+\tau_{o}>}$,
    one can train the response of an LSTM network $Q(\cdot;\theta)$ using Maximum
    Likelihood Estimation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习设置中，给定一组训练序列$\mathcal{D}=[(\mathbf{s}^{<t-\tau_{i},t>}_{1},\mathbf{z}^{<t+1,t+\tau_{o}>}_{1}),...,(\mathbf{s}^{<t-\tau_{i},t>}_{q},\mathbf{z}^{<t+1,t+\tau_{o}>}_{q})]$，即$q$个独立的观测序列对及其分配$\mathbf{z}^{<t,t+\tau_{o}>}$，可以使用最大似然估计法训练LSTM网络$Q(\cdot;\theta)$的响应。
- en: '|  | <math   alttext="\begin{split}\hat{\theta}&amp;=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &amp;=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\hat{\theta}&amp;=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &amp;=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\'
- en: '&amp;=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mover accent="true" ><mi >θ</mi><mo >^</mo></mover></mtd><mtd
    columnalign="left" ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo lspace="0.167em"
    >⁡</mo><mrow ><munder ><mi >max</mi><mi >θ</mi></munder><mo lspace="0.167em" >⁡</mo><mi
    >ℒ</mi></mrow></mrow><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >θ</mi><mo >;</mo><mi >𝒟</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi >min</mi><mi >θ</mi></munder></mrow><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><munderover ><mo movablelimits="false" >∑</mo><mrow
    ><mi >i</mi><mo >=</mo><mn >1</mn></mrow><mi >m</mi></munderover><mrow ><msub
    ><mi >l</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mi >Q</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup ><mi >𝐬</mi><mi >i</mi><mrow ><mrow ><mo
    ><</mo><mrow ><mi >t</mi><mo >−</mo><msub ><mi >τ</mi><mi >i</mi></msub></mrow></mrow><mo
    >,</mo><mrow ><mi >t</mi><mo >></mo></mrow></mrow></msubsup><mo >;</mo><mi >θ</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup ><mi >𝐳</mi><mi >i</mi><mrow
    ><mrow ><mo ><</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></mrow><mo >,</mo><mrow
    ><mrow ><mi >t</mi><mo >+</mo><msub ><mi >τ</mi><mi >o</mi></msub></mrow><mo >></mo></mrow></mrow></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi >min</mi><mi >θ</mi></munder></mrow><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><munderover ><mo movablelimits="false" rspace="0em"
    >∑</mo><mrow ><mi >i</mi><mo >=</mo><mn >1</mn></mrow><mi >m</mi></munderover><mrow
    ><munderover ><mo movablelimits="false" >∑</mo><mrow ><mi >t</mi><mo >=</mo><mn
    >1</mn></mrow><msub ><mi >τ</mi><mi >o</mi></msub></munderover><mrow ><msubsup
    ><mi >l</mi><mi >i</mi><mrow ><mo fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msubsup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mi >Q</mi><mrow ><mo
    fence="true" rspace="0em" ><</mo><mi >t</mi><mo fence="true" lspace="0em" >></mo></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐬</mi><mi >i</mi><mrow ><mrow ><mo ><</mo><mrow ><mi >t</mi><mo >−</mo><msub
    ><mi >τ</mi><mi >i</mi></msub></mrow></mrow><mo >,</mo><mrow ><mi >t</mi><mo >></mo></mrow></mrow></msubsup><mo
    >;</mo><mi >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup
    ><mi >𝐳</mi><mi >i</mi><mrow ><mo fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" >\begin{split}\hat{\theta}&=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}</annotation></semantics></math>
    |  | (8) |'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mover accent="true" ><mi >θ</mi><mo >^</mo></mover></mtd><mtd
    columnalign="left" ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo lspace="0.167em"
    >⁡</mo><mrow ><munder ><mi >max</mi><mi >θ</mi></munder><mo lspace="0.167em" >⁡</mo><mi
    >ℒ</mi></mrow></mrow><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >θ</mi><mo >;</mo><mi >𝒟</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi >min</mi><mi >θ</mi></munder></mrow><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><munderover ><mo movablelimits="false" >∑</mo><mrow
    ><mi >i</mi><mo >=</mo><mn >1</mn></mrow><mi >m</mi></munderover><mrow ><msub
    ><mi >l</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mi >Q</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup ><mi >𝐬</mi><mi >i</mi><mrow ><mrow ><mo
    ><</mo><mrow ><mi >t</mi><mo >−</mo><msub ><mi >τ</mi><mi >i</mi></msub></mrow></mrow><mo
    >,</mo><mrow ><mi >t</mi><mo >></mo></mrow></mrow></msubsup><mo >;</mo><mi >θ</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup ><mi >𝐳</mi><mi >i</mi><mrow
    ><mrow ><mo ><</mo><mrow ><mi >t</mi><mo >+</mo><mn >1</mn></mrow></mrow><mo >,</mo><mrow
    ><mrow ><mi >t</mi><mo >+</mo><msub ><mi >τ</mi><mi >o</mi></msub></mrow><mo >></mo></mrow></mrow></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mo >=</mo><mrow ><mrow ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi >min</mi><mi >θ</mi></munder></mrow><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><munderover ><mo movablelimits="false" rspace="0em"
    >∑</mo><mrow ><mi >i</mi><mo >=</mo><mn >1</mn></mrow><mi >m</mi></munderover><mrow
    ><munderover ><mo movablelimits="false" >∑</mo><mrow ><mi >t</mi><mo >=</mo><mn
    >1</mn></mrow><msub ><mi >τ</mi><mi >o</mi></msub></munderover><mrow ><msubsup
    ><mi >l</mi><mi >i</mi><mrow ><mo fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msubsup><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msup ><mi >Q</mi><mrow ><mo
    fence="true" rspace="0em" ><</mo><mi >t</mi><mo fence="true" lspace="0em" >></mo></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >𝐬</mi><mi >i</mi><mrow ><mrow ><mo ><</mo><mrow ><mi >t</mi><mo >−</mo><msub
    ><mi >τ</mi><mi >i</mi></msub></mrow></mrow><mo >,</mo><mrow ><mi >t</mi><mo >></mo></mrow></mrow></msubsup><mo
    >;</mo><mi >θ</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup
    ><mi >𝐳</mi><mi >i</mi><mrow ><mo fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msubsup><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" >\begin{split}\hat{\theta}&=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}</annotation></semantics></math>
    |  | (8) |'
- en: where an input sequence of observations $\mathbf{s}^{<t-\tau_{i},t>}=[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t-1>},\mathbf{s}^{<t>}]$
    is composed of $\tau_{i}$ consecutive data samples, $l(\cdot,\cdot)$ is the logistic
    regression loss function and $t$ represents a temporal index.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列 $\mathbf{s}^{<t-\tau_{i},t>}=[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t-1>},\mathbf{s}^{<t>}]$
    由 $\tau_{i}$ 个连续的数据样本组成，$l(\cdot,\cdot)$ 是逻辑回归损失函数，$t$ 代表一个时间索引。
- en: In recurrent neural networks terminology, the optimization procedure in Eq. [8](#S3.E8
    "In 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies ‣
    A Survey of Deep Learning Techniques for Autonomous Driving") is typically used
    for training ”many-to-many” RNN architectures, such as the one in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving"), where the input
    and output states are represented by temporal sequences of $\tau_{i}$ and $\tau_{o}$
    data instances, respectively. This optimization problem is commonly solved using
    gradient based methods, like Stochastic Gradient Descent (SGD), together with
    the backpropagation through time algorithm for calculating the network’s gradients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归神经网络术语中，方程 [8](#S3.E8 "在 3.2 递归神经网络 ‣ 3 深度学习技术概述 ‣ 自动驾驶深度学习技术调研") 中的优化过程通常用于训练“多对多”
    RNN 架构，如图 [2](#S3.F2 "图 2 ‣ 3.2 递归神经网络 ‣ 3 深度学习技术概述 ‣ 自动驾驶深度学习技术调研") 中的架构，其中输入和输出状态分别由
    $\tau_{i}$ 和 $\tau_{o}$ 数据实例的时间序列表示。这个优化问题通常使用基于梯度的方法，如随机梯度下降（SGD），以及通过时间反向传播算法来计算网络的梯度。
- en: 3.3 Deep Reinforcement Learning
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度强化学习
- en: In the following, we review the Deep Reinforcement Learning (DRL) concept as
    an autonomous driving task, using the Partially Observable Markov Decision Process
    (POMDP) formalism.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，我们将使用部分可观测马尔可夫决策过程（POMDP）形式化方法回顾深度强化学习（DRL）概念作为自动驾驶任务。
- en: In a POMDP, an agent, which in our case is the self-driving car, senses the
    environment with observation $\mathbf{I}^{<t>}$, performs an action $a^{<t>}$
    in state $\mathbf{s}^{<t>}$, interacts with its environment through a received
    reward $R^{<t+1>}$, and transits to the next state $\mathbf{s}^{<t+1>}$ following
    a transition function $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在 POMDP 中，代理（在我们的例子中是自动驾驶汽车）通过观察 $\mathbf{I}^{<t>}$ 感知环境，在状态 $\mathbf{s}^{<t>}$
    中执行动作 $a^{<t>}$，通过接收奖励 $R^{<t+1>}$ 与环境互动，并按照转移函数 $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$
    转移到下一个状态 $\mathbf{s}^{<t+1>}$。
- en: In RL based autonomous driving, the task is to learn an optimal driving policy
    for navigating from state $\mathbf{s}^{<t>}_{start}$ to a destination state $\mathbf{s}^{<t+k>}_{dest}$,
    given an observation $\mathbf{I}^{<t>}$ at time $t$ and the system’s state $\mathbf{s}^{<t>}$.
    $\mathbf{I}^{<t>}$ represents the observed environment, while $k$ is the number
    of time steps required for reaching the destination state $\mathbf{s}^{<t+k>}_{dest}$.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 RL 的自动驾驶中，任务是学习一个从状态 $\mathbf{s}^{<t>}_{start}$ 导航到目标状态 $\mathbf{s}^{<t+k>}_{dest}$
    的最优驾驶策略，给定时间 $t$ 的观察 $\mathbf{I}^{<t>}$ 和系统状态 $\mathbf{s}^{<t>}$. $\mathbf{I}^{<t>}$
    代表观察到的环境，而 $k$ 是到达目标状态 $\mathbf{s}^{<t+k>}_{dest}$ 所需的时间步数。
- en: 'In reinforcement learning terminology, the above problem can be modeled as
    a POMDP $M:=(I,S,A,T,R,\gamma)$, where:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习术语中，上述问题可以建模为 POMDP $M:=(I,S,A,T,R,\gamma)$，其中：
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $I$ is the set of observations, with $\mathbf{I}^{<t>}\in I$ defined as an observation
    of the environment at time $t$.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $I$ 是观察集合，其中 $\mathbf{I}^{<t>}\in I$ 定义为时间 $t$ 的环境观察。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S$ represents a finite set of states, $\mathbf{s}^{<t>}\in S$ being the state
    of the agent at time $t$, commonly defined as the vehicle’s position, heading
    and velocity.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$ 代表一个有限的状态集合，$\mathbf{s}^{<t>}\in S$ 是时间 $t$ 的智能体状态，通常定义为车辆的位置、航向和速度。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A$ represents a finite set of actions allowing the agent to navigate through
    the environment defined by $\mathbf{I}^{<t>}$, where $a^{<t>}\in A$ is the action
    performed by the agent at time $t$.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A$ 代表一个有限的动作集合，允许智能体在由 $\mathbf{I}^{<t>}$ 定义的环境中导航，其中 $a^{<t>}\in A$ 是智能体在时间
    $t$ 执行的动作。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $T:S\times A\times S\rightarrow[0,1]$ is a stochastic transition function, where
    $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$ describes the probability
    of arriving in state $\mathbf{s}^{<t+1>}$, after performing action $a^{<t>}$ in
    state $\mathbf{s}^{<t>}$.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $T:S\times A\times S\rightarrow[0,1]$ 是一个随机转移函数，其中 $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$
    描述了在状态 $\mathbf{s}^{<t>}$ 执行动作 $a^{<t>}$ 后到达状态 $\mathbf{s}^{<t+1>}$ 的概率。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $R:S\times A\times S\rightarrow\mathbb{R}$ is a scalar reward function which
    controls the estimation of $a$, where $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}\in\mathbb{R}$.
    For a state transition $\mathbf{s}^{<t>}\rightarrow\mathbf{s}^{<t+1>}$ at time
    $t$, we define a scalar reward function $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$
    which quantifies how well did the agent perform in reaching the next state.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $R:S\times A\times S\rightarrow\mathbb{R}$ 是一个标量奖励函数，用于控制 $a$ 的估计，其中 $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}\in\mathbb{R}$。对于时间
    $t$ 的状态转移 $\mathbf{s}^{<t>}\rightarrow\mathbf{s}^{<t+1>}$，我们定义一个标量奖励函数 $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$，用于量化代理在到达下一个状态时的表现。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\gamma$ is the discount factor controlling the importance of future versus
    immediate rewards.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\gamma$ 是折扣因子，用于控制未来奖励与即时奖励的相对重要性。
- en: 'Considering the proposed reward function and an arbitrary state trajectory
    $[\mathbf{s}^{<0>},\mathbf{s}^{<1>},...,\mathbf{s}^{<k>}]$ in observation space,
    at any time $\hat{t}\in[0,1,...,k]$, the associated cumulative future discounted
    reward is defined as:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所提出的奖励函数以及观察空间中的任意状态轨迹 $[\mathbf{s}^{<0>},\mathbf{s}^{<1>},...,\mathbf{s}^{<k>}]$，在任何时间
    $\hat{t}\in[0,1,...,k]$，相关的累计未来折扣奖励定义为：
- en: '|  | $R^{<\hat{t}>}=\sum^{k}_{t=\hat{t}}\gamma^{<t-\hat{t}>}r^{<t>},$ |  |
    (9) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $R^{<\hat{t}>}=\sum^{k}_{t=\hat{t}}\gamma^{<t-\hat{t}>}r^{<t>},$ |  |
    (9) |'
- en: where the immediate reward at time $t$ is given by $r^{<t>}$. In RL theory,
    the statement in Eq. [9](#S3.E9 "In 3.3 Deep Reinforcement Learning ‣ 3 Overview
    of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving") is known as a finite horizon learning episode of sequence length $k$ [[18](#bib.bib18)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，时间 $t$ 的即时奖励由 $r^{<t>}$ 给出。在强化学习理论中，方程[9](#S3.E9 "In 3.3 Deep Reinforcement
    Learning ‣ 3 Overview of Deep Learning Technologies ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving") 中的陈述被称为有限时域学习情节，序列长度为 $k$ [[18](#bib.bib18)]。
- en: 'The objective in RL is to find the desired trajectory policy that maximizes
    the associated cumulative future reward. We define the optimal action-value function
    $Q^{*}(\cdot,\cdot)$ which estimates the maximal future discounted reward when
    starting in state $\mathbf{s}^{<t>}$ and performing actions $[a^{<t>},...,a^{<t+k>}]$:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，目标是找到一个期望的轨迹策略，以最大化相关的累计未来奖励。我们定义了最优动作值函数 $Q^{*}(\cdot,\cdot)$，它估计从状态
    $\mathbf{s}^{<t>}$ 开始并执行动作 $[a^{<t>},...,a^{<t+k>}]$ 时的最大未来折扣奖励：
- en: '|  | $Q^{*}(\mathbf{s},a)=\underset{\pi}{\max}\mathbb{E}\text{ }[R^{<\hat{t}>}&#124;\mathbf{s}^{<\hat{t}>}=\mathbf{s},\text{
    }a^{<\hat{t}>}=a,\text{ }\pi],$ |  | (10) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(\mathbf{s},a)=\underset{\pi}{\max}\mathbb{E}\text{ }[R^{<\hat{t}>}&#124;\mathbf{s}^{<\hat{t}>}=\mathbf{s},\text{
    }a^{<\hat{t}>}=a,\text{ }\pi],$ |  | (10) |'
- en: 'where $\pi$ is an action policy, viewed as a probability density function over
    a set of possible actions that can take place in a given state. The optimal action-value
    function $Q^{*}(\cdot,\cdot)$ maps a given state to the optimal action policy
    of the agent in any state:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\pi$ 是一个动作策略，视为在给定状态下可能发生的一组动作的概率密度函数。最优动作值函数 $Q^{*}(\cdot,\cdot)$ 将给定状态映射到代理在任何状态下的最优动作策略：
- en: '|  | $\forall\mathbf{s}\in S:\pi^{*}(\mathbf{s})=\underset{a\in A}{\arg\max}Q^{*}(\mathbf{s},a).$
    |  | (11) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall\mathbf{s}\in S:\pi^{*}(\mathbf{s})=\underset{a\in A}{\arg\max}Q^{*}(\mathbf{s},a).$
    |  | (11) |'
- en: 'The optimal action-value function $Q^{*}$ satisfies the Bellman optimality
    equation [[19](#bib.bib19)], which is a recursive formulation of Eq. [10](#S3.E10
    "In 3.3 Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving"):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最优动作值函数 $Q^{*}$ 满足 Bellman 最优性方程 [[19](#bib.bib19)]，这是方程[10](#S3.E10 "In 3.3
    Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving") 的递归形式：
- en: '|  | $\displaystyle Q^{*}(\mathbf{s},a)=\sum_{\mathbf{s}}T_{\mathbf{s},a}^{\mathbf{s}^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right)$
    |  | (12) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q^{*}(\mathbf{s},a)=\sum_{\mathbf{s}}T_{\mathbf{s},a}^{\mathbf{s}^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right)$
    |  | (12) |'
- en: '|  | $\displaystyle=\mathbb{E}_{a^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right),$
    |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}_{a^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right),$
    |  |'
- en: 'where $\mathbf{s}^{\prime}$ represents a possible state visited after $\mathbf{s}=\mathbf{s}^{<t>}$
    and $a^{\prime}$ is the corresponding action policy. The model-based policy iteration
    algorithm was introduced in [[18](#bib.bib18)], based on the proof that the Bellman
    equation is a contraction mapping [[20](#bib.bib20)] when written as an operator
    $\nu$:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{s}^{\prime}$ 代表在 $\mathbf{s}=\mathbf{s}^{<t>}$ 之后可能访问的状态，而 $a^{\prime}$
    是相应的动作策略。基于 Bellman 方程作为算子 $\nu$ 的收缩映射证明 [[18](#bib.bib18)]，引入了基于模型的策略迭代算法 [[20](#bib.bib20)]：
- en: '|  | $\forall Q,\lim_{n\rightarrow\infty}\nu^{(n)}(Q)=Q^{*}.$ |  | (13) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall Q,\lim_{n\rightarrow\infty}\nu^{(n)}(Q)=Q^{*}.$ |  | (13) |'
- en: 'However, the standard reinforcement learning method described above is not
    feasible in high dimensional state spaces. In autonomous driving applications,
    the observation space is mainly composed of sensory information made up of images,
    radar, LiDAR, etc. Instead of the traditional approach, a non-linear parametrization
    of $Q^{*}$ can be encoded in the layers of a deep neural network. In literature,
    such a non-linear approximator is called a Deep Q-Network (DQN) [[21](#bib.bib21)]
    and is used for estimating the approximate action-value function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述标准强化学习方法在高维状态空间中不可行。在自动驾驶应用中，观测空间主要由图像、雷达、LiDAR 等组成的传感器信息构成。不同于传统方法，$Q^{*}$
    的非线性参数化可以在深度神经网络的层中编码。在文献中，这种非线性近似器称为深度 Q 网络（DQN）[[21](#bib.bib21)]，用于估计近似的动作价值函数：
- en: '|  | $Q(\mathbf{s}^{<t>},a^{<t>};\Theta)\approx Q^{*}(\mathbf{s}^{<t>},a^{<t>}),$
    |  | (14) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(\mathbf{s}^{<t>},a^{<t>};\Theta)\approx Q^{*}(\mathbf{s}^{<t>},a^{<t>}),$
    |  | (14) |'
- en: where $\Theta$ represents the parameters of the Deep Q-Network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Theta$ 表示深度 Q 网络的参数。
- en: 'By taking into account the Bellman optimality equation [12](#S3.E12 "In 3.3
    Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving"), it is possible to train
    a deep Q-network in a reinforcement learning manner through the minimization of
    the mean squared error. The optimal expected Q value can be estimated within a
    training iteration $i$ based on a set of reference parameters $\bar{\Theta}_{i}$
    calculated in a previous iteration $i^{\prime}$:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 Bellman 最优方程 [12](#S3.E12 "在 3.3 深度强化学习 ‣ 3 深度学习技术概述 ‣ 自动驾驶的深度学习技术综述")，可以通过最小化均方误差以强化学习方式训练深度
    Q 网络。在训练迭代 $i$ 中，基于先前迭代 $i^{\prime}$ 计算的一组参考参数 $\bar{\Theta}_{i}$ 可以估计最优的期望 Q
    值：
- en: '|  | $y=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q(\mathbf{s}^{\prime},a^{\prime};\bar{\Theta}_{i}),$
    |  | (15) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $y=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q(\mathbf{s}^{\prime},a^{\prime};\bar{\Theta}_{i}),$
    |  | (15) |'
- en: 'where $\bar{\Theta}_{i}:=\Theta_{i^{\prime}}$. The new estimated network parameters
    at training step $i$ are evaluated using the following squared error function:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bar{\Theta}_{i}:=\Theta_{i^{\prime}}$。在训练步骤 $i$ 中，新的估计网络参数使用以下平方误差函数进行评估：
- en: '|  | $\nabla J_{\hat{\Theta}_{i}}=\underset{\Theta_{i}}{\min}\text{ }\mathbb{E}_{\mathbf{s},y,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)^{2}\right],$
    |  | (16) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla J_{\hat{\Theta}_{i}}=\underset{\Theta_{i}}{\min}\text{ }\mathbb{E}_{\mathbf{s},y,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)^{2}\right],$
    |  | (16) |'
- en: 'where $r=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}$. Based on [16](#S3.E16 "In
    3.3 Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies ‣ A
    Survey of Deep Learning Techniques for Autonomous Driving"), the maximum likelihood
    estimation function from Eq. [8](#S3.E8 "In 3.2 Recurrent Neural Networks ‣ 3
    Overview of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques
    for Autonomous Driving") can be applied for calculating the weights of the deep
    Q-network. The gradient is approximated with random samples and the backpropagation
    algorithm, which uses stochastic gradient descent for training:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}$。基于 [16](#S3.E16 "在 3.3 深度强化学习
    ‣ 3 深度学习技术概述 ‣ 自动驾驶的深度学习技术综述")，可以应用方程 [8](#S3.E8 "在 3.2 循环神经网络 ‣ 3 深度学习技术概述 ‣
    自动驾驶的深度学习技术综述") 中的最大似然估计函数来计算深度 Q 网络的权重。梯度通过随机样本和反向传播算法进行近似，该算法使用随机梯度下降进行训练：
- en: '|  | $\nabla_{\Theta_{i}}=\mathbb{E}_{\mathbf{s},a,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)\nabla_{\Theta_{i}}\left(Q(\mathbf{s},a;\Theta_{i})\right)\right].$
    |  | (17) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\Theta_{i}}=\mathbb{E}_{\mathbf{s},a,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)\nabla_{\Theta_{i}}\left(Q(\mathbf{s},a;\Theta_{i})\right)\right].$
    |  | (17) |'
- en: 'The deep reinforcement learning community has made several independent improvements
    to the original DQN algorithm [[21](#bib.bib21)]. A study on how to combine these
    improvements on deep reinforcement learning has been provided by DeepMind in [[22](#bib.bib22)],
    where the combined algorithm, entitled Rainbow, was able to outperform the independently
    competing methods. DeepMind [[22](#bib.bib22)] proposes six extensions to the
    base DQN, each addressing a distinct concern:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习社区对原始DQN算法进行了若干独立改进 [[21](#bib.bib21)]。DeepMind在 [[22](#bib.bib22)]中提供了如何将这些改进结合在深度强化学习中的研究，其中结合算法Rainbow能够超越独立竞争的方法。DeepMind [[22](#bib.bib22)]提出了六种对基础DQN的扩展，每种扩展都解决了一个不同的问题：
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Double Q Learning addresses the overestimation bias and decouples the selection
    of an action and its evaluation;
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双重Q学习（Double Q Learning）解决了过度估计偏差，并解耦了动作选择和评估；
- en: •
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prioritized replay samples more frequently from the data in which there is information
    to learn;
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优先级重放（Prioritized replay）从数据中更频繁地抽样，以获取学习信息；
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dueling Networks aim at enhancing value based RL;
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对战网络（Dueling Networks）旨在增强基于价值的强化学习；
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-step learning is used for training speed improvement;
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多步学习（Multi-step learning）用于提高训练速度；
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Distributional RL improves the target distribution in the Bellman equation;
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布式强化学习（Distributional RL）改善了贝尔曼方程中的目标分布；
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Noisy Nets improve the ability of the network to ignore noisy inputs and allows
    state-conditional exploration.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 噪声网络（Noisy Nets）提高了网络忽略噪声输入的能力，并允许基于状态的条件探索。
- en: All of the above complementary improvements have been tested on the Atari 2600
    challenge. A good implementation of DQN regarding autonomous vehicles should start
    by combining the stated DQN extensions with respect to a desired performance.
    Given the advancements in deep reinforcement learning, the direct application
    of the algorithm still needs a training pipeline in which one should simulate
    and model the desired self-driving car’s behavior.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有补充改进都在Atari 2600挑战中进行了测试。对于自动驾驶汽车来说，DQN的良好实现应从结合上述DQN扩展和期望性能开始。考虑到深度强化学习的进展，算法的直接应用仍然需要一个训练管道，在该管道中，应该模拟和建模期望的自动驾驶汽车行为。
- en: The simulated environment state is not directly accessible to the agent. Instead,
    sensor readings provide clues about the true state of the environment. In order
    to decode the true environment state, it is not sufficient to map a single snapshot
    of sensors readings. The temporal information should also be included in the network’s
    input, since the environment’s state is modified over time. An example of DQN
    applied to autonomous vehicles in a simulator can be found in [[23](#bib.bib23)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟环境状态对智能体并不直接可访问。相反，传感器读取提供了关于环境真实状态的线索。为了解码真实的环境状态，仅映射单一的传感器读取快照是不够的。时间信息也应包含在网络输入中，因为环境状态会随时间变化。在模拟器中应用DQN到自动驾驶汽车的一个示例可以在 [[23](#bib.bib23)]中找到。
- en: DQN has been developed to operate in discrete action spaces. In the case of
    an autonomous car, the discrete actions would translate to discrete commands,
    such as turn left, turn right, accelerate, or break. The DQN approach described
    above has been extended to continuous action spaces based on policy gradient estimation [[24](#bib.bib24)].
    The method in [[24](#bib.bib24)] describes a model-free actor-critic algorithm
    able to learn different continuous control tasks directly from raw pixel inputs.
    A model-based solution for continuous Q-learning is proposed in [[25](#bib.bib25)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: DQN已经被开发用于离散动作空间。在自动驾驶汽车的情况下，离散动作将转化为离散命令，如左转、右转、加速或刹车。上述DQN方法已扩展到基于策略梯度估计的连续动作空间 [[24](#bib.bib24)]。方法 [[24](#bib.bib24)]描述了一种无模型的演员-评论家算法，能够直接从原始像素输入中学习不同的连续控制任务。对于连续Q学习，提出了一种基于模型的解决方案 [[25](#bib.bib25)]。
- en: Although continuous control with DRL is possible, the most common strategy for
    DRL in autonomous driving is based on discrete control [[26](#bib.bib26)]. The
    main challenge here is the training, since the agent has to explore its environment,
    usually through learning from collisions. Such systems, trained solely on simulated
    data, tend to learn a biased version of the driving environment. A solution here
    is to use Imitation Learning methods, such as Inverse Reinforcement Learning (IRL) [[27](#bib.bib27)],
    to learn from human driving demonstrations without needing to explore unsafe actions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用深度强化学习（DRL）进行连续控制是可能的，但在自主驾驶中，DRL最常用的策略是基于离散控制 [[26](#bib.bib26)]。主要的挑战在于训练，因为代理需要通过学习碰撞来探索其环境。仅通过模拟数据进行训练的系统往往会学习到一个有偏差的驾驶环境。解决方案是使用模仿学习方法，如逆向强化学习（IRL） [[27](#bib.bib27)]，从人类驾驶示范中学习，而无需探索不安全的动作。
- en: 4 Deep Learning for Driving Scene Perception and Localization
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习用于驾驶场景感知与定位
- en: The self-driving technology enables a vehicle to operate autonomously by perceiving
    the environment and instrumenting a responsive answer. Following, we give an overview
    of the top methods used in driving scene understanding, considering camera based
    vs. LiDAR environment perception. We survey object detection and recognition,
    semantic segmentation and localization in autonomous driving, as well as scene
    understanding using occupancy maps. Surveys dedicated to autonomous vision and
    environment perception can be found in [[28](#bib.bib28)] and [[29](#bib.bib29)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自主驾驶技术使得车辆能够通过感知环境并作出相应的反应来自主操作。接下来，我们将概述在驾驶场景理解中使用的主要方法，考虑基于相机与基于LiDAR的环境感知。我们调查了自主驾驶中的目标检测与识别、语义分割和定位，以及使用占用地图的场景理解。专门研究自主视觉和环境感知的调查可以在 [[28](#bib.bib28)]和 [[29](#bib.bib29)]中找到。
- en: '4.1 Sensing Hardware: Camera vs. LiDAR Debate'
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 传感器硬件：相机与LiDAR的争论
- en: Deep learning methods are particularly well suited for detecting and recognizing
    objects in 2D images and 3D point clouds acquired from video cameras and LiDAR
    (Light Detection and Ranging) devices, respectively.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法特别适合于检测和识别来自视频摄像头和LiDAR（光探测与测距）设备的2D图像和3D点云中的目标。
- en: In the autonomous driving community, 3D perception is mainly based on LiDAR
    sensors, which provide a direct 3D representation of the surrounding environment
    in the form of 3D point clouds. The performance of a LiDAR is measured in terms
    of field of view, range, resolution and rotation/frame rate. 3D sensors, such
    as Velodyne^®, usually have a $360^{\circ}$ horizontal field of view. In order
    to operate at high speeds, an autonomous vehicle requires a minimum of $200m$
    range, allowing the vehicle to react to changes in road conditions in time. The
    3D object detection precision is dictated by the resolution of the sensor, with
    most advanced LiDARs being able to provide a $3cm$ accuracy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主驾驶领域，3D感知主要基于LiDAR传感器，它提供了周围环境的直接3D表示形式，即3D点云。LiDAR的性能通过视场、范围、分辨率和旋转/帧率来衡量。3D传感器，如Velodyne^®，通常具有$360^{\circ}$的水平视场。为了以高速度运行，自主车辆需要至少$200m$的范围，以便及时对道路条件的变化做出反应。3D目标检测的精度由传感器的分辨率决定，最先进的LiDAR能够提供$3cm$的准确度。
- en: '![Refer to caption](img/7fd891e18f62c81967a53a5cbebd571d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7fd891e18f62c81967a53a5cbebd571d.png)'
- en: 'Figure 3: Examples of scene perception results. (a) 2D object detection in
    images. (b) 3D bounding box detector applied on LiDAR data. (c) Semantic segmentation
    results on images.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：场景感知结果示例。 (a) 图像中的2D目标检测。 (b) 应用于LiDAR数据的3D边界框检测器。 (c) 图像上的语义分割结果。
- en: Recent debate sparked around camera vs. LiDAR (Light Detection and Ranging)
    sensing technologies. Tesla^® and Waymo^®, two of the companies leading the development
    of self-driving technology [[30](#bib.bib30)], have different philosophies with
    respect to their main perception sensor, as well as regarding the targeted SAE
    level [[4](#bib.bib4)]. Waymo^® is building their vehicles directly as Level 5
    systems, with currently more than 10 million miles driven autonomously²²2[https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/](https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/).
    On the other hand, Tesla^® deploys its AutoPilot as an ADAS (Advanced Driver Assistance
    System) component, which customers can turn on or off at their convenience. The
    advantage of Tesla^® resides in its large training database, consisting of more
    than 1 billion driven miles³³3[https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/](https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/).
    The database has been acquired by collecting data from customers-owned cars.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最近围绕摄像头与 LiDAR（光探测与测距）传感技术的辩论引发了关注。特斯拉^® 和 Waymo^®，这两家在自动驾驶技术开发方面领先的公司[[30](#bib.bib30)]，在其主要感知传感器以及目标
    SAE 级别[[4](#bib.bib4)]方面有着不同的理念。Waymo^® 正在将其车辆直接构建为 5 级系统，目前已经自主驾驶超过 1000 万英里²²2[https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/](https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/)。另一方面，特斯拉^®
    将其 AutoPilot 部署为 ADAS（高级驾驶辅助系统）组件，客户可以根据需要开启或关闭。特斯拉^® 的优势在于其庞大的训练数据库，其中包含超过 10
    亿英里³³3[https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/](https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/)。该数据库通过从客户拥有的车辆收集数据来获得。
- en: The main sensing technologies differ in both companies. Tesla^® tries to leverage
    on its camera systems, whereas Waymo’s driving technology relies more on Lidar
    sensors⁴⁴4[https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation](https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation).
    The sensing approaches have advantages and disadvantages. LiDARs have high resolution
    and precise perception even in the dark, but are vulnerable to bad weather conditions
    (e.g. heavy rain) [[31](#bib.bib31)] and involve moving parts. In contrast, cameras
    are cost efficient, but lack depth perception and cannot work in the dark. Cameras
    are also sensitive to bad weather, if the weather conditions are obstructing the
    field of view.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 两家公司在主要传感技术上有所不同。特斯拉^® 试图利用其摄像头系统，而 Waymo 的驾驶技术则更多依赖于 LiDAR 传感器⁴⁴4[https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation](https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation)。这些感知方法各有优缺点。LiDAR
    具有高分辨率和精确的感知能力，即使在黑暗中也能工作，但对恶劣天气条件（例如大雨）[[31](#bib.bib31)]较为敏感，并且涉及运动部件。相比之下，摄像头成本效益高，但缺乏深度感知，并且不能在黑暗中工作。如果天气条件阻碍视野，摄像头也会对恶劣天气敏感。
- en: Researchers at Cornell University tried to replicate LiDAR-like point clouds
    from visual depth estimation [[32](#bib.bib32)]. An estimated depth map is reprojected
    into 3D space, with respect to the left sensor’s coordinate of a stereo camera.
    The resulting point cloud is referred to as pseudo-LiDAR. The pseudo-LiDAR data
    can be further fed to 3D deep learning processing methods, such as PointNet [[33](#bib.bib33)]
    or AVOD [[34](#bib.bib34)]. The success of image based 3D estimation is of high
    importance to the large scale deployment of autonomous cars, since the LiDAR is
    arguably one of the most expensive hardware component in a self-driving vehicle.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学的研究人员尝试通过视觉深度估计来复制类似于 LiDAR 的点云[[32](#bib.bib32)]。一个估计的深度图被重新投影到 3D 空间中，相对于立体相机的左侧传感器坐标。生成的点云称为伪
    LiDAR。伪 LiDAR 数据可以进一步用于 3D 深度学习处理方法，例如 PointNet[[33](#bib.bib33)] 或 AVOD[[34](#bib.bib34)]。图像基础的
    3D 估计的成功对大规模部署自动驾驶汽车至关重要，因为 LiDAR 无疑是自动驾驶车辆中最昂贵的硬件组件之一。
- en: Apart from these sensing technologies, radar and ultrasonic sensors are used
    to enhance perception capabilities. For example, alongside three Lidar sensors,
    Waymo also makes use of five radars and eight cameras, while Tesla^® cars are
    equipped with eights cameras, 12 ultrasonic sensors and one forward-facing radar.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些传感技术外，雷达和超声波传感器也用于增强感知能力。例如，除了三个 LiDAR 传感器，Waymo 还使用了五个雷达和八个摄像头，而特斯拉^® 车辆配备了八个摄像头、12
    个超声波传感器和一个前向雷达。
- en: 4.2 Driving Scene Understanding
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 驾驶场景理解
- en: An autonomous car should be able to detect traffic participants and drivable
    areas, particularly in urban areas where a wide variety of object appearances
    and occlusions may appear. Deep learning based perception, in particular Convolutional
    Neural Networks (CNNs), became the de-facto standard in object detection and recognition,
    obtaining remarkable results in competitions such as the ImageNet Large Scale
    Visual Recognition Challenge [[35](#bib.bib35)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆自动驾驶汽车应能够检测交通参与者和可驾驶区域，特别是在城市区域，其中可能会出现各种物体外观和遮挡。基于深度学习的感知，特别是卷积神经网络（CNNs），已成为物体检测和识别的事实标准，在ImageNet大规模视觉识别挑战赛[[35](#bib.bib35)]等竞赛中取得了显著成绩。
- en: 'Different neural networks architectures are used to detect objects as 2D regions
    of interest [[36](#bib.bib36)] [[37](#bib.bib37)] [[38](#bib.bib38)] [[39](#bib.bib39)] [[40](#bib.bib40)] [[41](#bib.bib41)]
    or pixel-wise segmented areas in images [[42](#bib.bib42)] [[43](#bib.bib43)] [[44](#bib.bib44)] [[45](#bib.bib45)],
    3D bounding boxes in LiDAR point clouds [[33](#bib.bib33)] [[46](#bib.bib46)] [[47](#bib.bib47)],
    as well as 3D representations of objects in combined camera-LiDAR data [[48](#bib.bib48)] [[49](#bib.bib49)] [[34](#bib.bib34)].
    Examples of scene perception results are illustrated in Fig. [3](#S4.F3 "Figure
    3 ‣ 4.1 Sensing Hardware: Camera vs. LiDAR Debate ‣ 4 Deep Learning for Driving
    Scene Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving"). Being richer in information, image data is more suited for the object
    recognition task. However, the real-world 3D positions of the detected objects
    have to be estimated, since depth information is lost in the projection of the
    imaged scene onto the imaging sensor.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的神经网络架构被用于检测物体，例如在图像中的2D感兴趣区域[[36](#bib.bib36)] [[37](#bib.bib37)] [[38](#bib.bib38)]
    [[39](#bib.bib39)] [[40](#bib.bib40)] [[41](#bib.bib41)]或逐像素分割区域[[42](#bib.bib42)]
    [[43](#bib.bib43)] [[44](#bib.bib44)] [[45](#bib.bib45)]，LiDAR点云中的3D边界框[[33](#bib.bib33)]
    [[46](#bib.bib46)] [[47](#bib.bib47)]，以及结合摄像头和LiDAR数据的3D物体表示[[48](#bib.bib48)]
    [[49](#bib.bib49)] [[34](#bib.bib34)]。场景感知结果的示例如图[3](#S4.F3 "Figure 3 ‣ 4.1 Sensing
    Hardware: Camera vs. LiDAR Debate ‣ 4 Deep Learning for Driving Scene Perception
    and Localization ‣ A Survey of Deep Learning Techniques for Autonomous Driving")所示。由于图像数据信息更丰富，因此更适合物体识别任务。然而，由于在将图像场景投射到成像传感器上时丢失了深度信息，因此必须估计检测物体的实际3D位置。'
- en: 4.2.1 Bounding-Box-Like Object Detectors
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 边界框状物体检测器
- en: 'The most popular architectures for 2D object detection in images are single
    stage and double stage detectors. Popular single stage detectors are ”You Only
    Look Once” (Yolo) [[36](#bib.bib36)] [[50](#bib.bib50)] [[51](#bib.bib51)], the
    Single Shot multibox Detector (SSD) [[52](#bib.bib52)], CornerNet [[37](#bib.bib37)]
    and RefineNet [[38](#bib.bib38)]. Double stage detectors, such as RCNN [[53](#bib.bib53)],
    Faster-RCNN [[54](#bib.bib54)], or R-FCN [[41](#bib.bib41)], split the object
    detection process into two parts: region of interest candidates proposals and
    bounding boxes classification. In general, single stage detectors do not provide
    the same performances as double stage detectors, but are significantly faster.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中2D物体检测的最流行架构是单阶段和双阶段检测器。流行的单阶段检测器包括”You Only Look Once”（Yolo）[[36](#bib.bib36)]
    [[50](#bib.bib50)] [[51](#bib.bib51)]，单次多框检测器（SSD）[[52](#bib.bib52)]，CornerNet[[37](#bib.bib37)]和RefineNet[[38](#bib.bib38)]。双阶段检测器，如RCNN[[53](#bib.bib53)]，Faster-RCNN[[54](#bib.bib54)]，或R-FCN[[41](#bib.bib41)]，将物体检测过程分为两个部分：感兴趣区域候选提案和边界框分类。一般来说，单阶段检测器的性能不如双阶段检测器，但显著更快。
- en: If in-vehicle computation resources are scarce, one can use detectors such as
    SqueezeNet [[40](#bib.bib40)] or [[55](#bib.bib55)], which are optimized to run
    on embedded hardware. These detectors usually have a smaller neural network architecture,
    making it possible to detect objects using a reduced number of operations, at
    the cost of detection accuracy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果车载计算资源有限，可以使用例如SqueezeNet[[40](#bib.bib40)]或[[55](#bib.bib55)]等检测器，这些检测器经过优化以在嵌入式硬件上运行。这些检测器通常具有较小的神经网络架构，从而可以在减少操作数量的情况下检测物体，但会以检测准确度为代价。
- en: A comparison between the object detectors described above is given in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2.2 Semantic and Instance Segmentation ‣ 4.2 Driving Scene Understanding
    ‣ 4 Deep Learning for Driving Scene Perception and Localization ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving"), based on the Pascal VOC 2012
    dataset and their measured mean Average Precision (mAP) with an Intersection over
    Union (IoU) value equal to $50$ and $75$, respectively.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S4.F4 "Figure 4 ‣ 4.2.2 Semantic and Instance Segmentation ‣ 4.2 Driving
    Scene Understanding ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")展示了上述物体检测器的比较，基于Pascal
    VOC 2012数据集及其在Intersection over Union（IoU）值分别为$50$和$75$时的平均精度（mAP）测量。
- en: A number of publications showcased object detection on raw 3D sensory data,
    as well as for combined video and LiDAR information. PointNet [[33](#bib.bib33)]
    and VoxelNet [[46](#bib.bib46)] are designed to detect objects solely from 3D
    data, providing also the 3D positions of the objects. However, point clouds alone
    do not contain the rich visual information available in images. In order to overcome
    this, combined camera-LiDAR architectures are used, such as Frustum PointNet [[48](#bib.bib48)],
    Multi-View 3D networks (MV3D) [[49](#bib.bib49)], or RoarNet [[56](#bib.bib56)].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一些出版物展示了对原始3D传感数据以及视频和LiDAR信息的物体检测。PointNet [[33](#bib.bib33)]和VoxelNet [[46](#bib.bib46)]旨在仅通过3D数据来检测物体，同时提供物体的3D位置。然而，点云本身并不包含图像中的丰富视觉信息。为了解决这个问题，使用了结合摄像头和LiDAR的架构，例如Frustum
    PointNet [[48](#bib.bib48)]、Multi-View 3D网络（MV3D） [[49](#bib.bib49)]或RoarNet [[56](#bib.bib56)]。
- en: The main disadvantage in using a LiDAR in the sensory suite of a self-driving
    car is primarily its cost⁵⁵5[https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/](https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/).
    A solution here would be to use neural network architectures such as AVOD (Aggregate
    View Object Detection) [[34](#bib.bib34)], which leverage on LiDAR data only for
    training, while images are used during training and deployment. At deployment
    stage, AVOD is able to predict 3D bounding boxes of objects solely from image
    data. In such a system, a LiDAR sensor is necessary only for training data acquisition,
    much like the cars used today to gather road data for navigation maps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在自驾车的传感器套件中使用LiDAR的主要缺点是其成本⁵⁵5[https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/](https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/)。一种解决方案是使用如AVOD（Aggregate
    View Object Detection） [[34](#bib.bib34)]的神经网络架构，该架构仅利用LiDAR数据进行训练，而在训练和部署过程中使用图像。在部署阶段，AVOD能够仅通过图像数据预测物体的3D边界框。在这样的系统中，LiDAR传感器仅在训练数据获取时是必要的，就像今天的汽车用于收集导航地图的道路数据一样。
- en: 4.2.2 Semantic and Instance Segmentation
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 语义和实例分割
- en: Driving scene understanding can also be achieved using semantic segmentation,
    representing the categorical labeling of each pixel in an image. In the autonomous
    driving context, pixels can be marked with categorical labels representing drivable
    area, pedestrians, traffic participants, buildings, etc. It is one of the high-level
    tasks that paves the way towards complete scene understanding, being used in applications
    such as autonomous driving, indoor navigation, or virtual and augmented reality.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶场景理解也可以通过语义分割来实现，这种分割表示图像中每个像素的类别标签。在自动驾驶背景下，像素可以标记为可驾驶区域、行人、交通参与者、建筑物等类别标签。这是实现完整场景理解的高级任务之一，应用于自动驾驶、室内导航或虚拟和增强现实等应用中。
- en: Semantic segmentation networks like SegNet [[42](#bib.bib42)], ICNet [[43](#bib.bib43)],
    ENet [[57](#bib.bib57)], AdapNet [[58](#bib.bib58)], or Mask R-CNN [[45](#bib.bib45)]
    are mainly encoder-decoder architectures with a pixel-wise classification layer.
    These are based on building blocks from some common network topologies, such as
    AlexNet [[1](#bib.bib1)], VGG-16 [[59](#bib.bib59)], GoogLeNet [[60](#bib.bib60)],
    or ResNet [[61](#bib.bib61)].
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割网络如SegNet [[42](#bib.bib42)]、ICNet [[43](#bib.bib43)]、ENet [[57](#bib.bib57)]、AdapNet [[58](#bib.bib58)]或Mask
    R-CNN [[45](#bib.bib45)]主要是具有像素级分类层的编码器-解码器架构。这些网络基于一些常见网络拓扑结构的构建模块，例如AlexNet
    [[1](#bib.bib1)]、VGG-16 [[59](#bib.bib59)]、GoogLeNet [[60](#bib.bib60)]或ResNet
    [[61](#bib.bib61)]。
- en: As in the case of bounding-box detectors, efforts have been made to improve
    the computation time of these systems on embedded targets. In [[44](#bib.bib44)]
    and [[57](#bib.bib57)], the authors proposed approaches to speed up data processing
    and inference on embedded devices for autonomous driving. Both architectures are
    light networks providing similar results as SegNet, with a reduced computation
    cost.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与边界框检测器的情况类似，已经对这些系统在嵌入式目标上的计算时间进行了改进。在[[44](#bib.bib44)]和[[57](#bib.bib57)]中，作者提出了加速数据处理和推断的方法，这些方法在嵌入式设备上用于自动驾驶。这两种架构都是轻量级网络，提供类似于SegNet的结果，同时减少了计算成本。
- en: The robustness objective for semantic segmentation was tackled for optimization
    in AdapNet [[58](#bib.bib58)]. The model is capable of robust segmentation in
    various environments by adaptively learning features of expert networks based
    on scene conditions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割的鲁棒性目标在AdapNet中进行了优化[[58](#bib.bib58)]。该模型通过基于场景条件自适应地学习专家网络的特征，在各种环境中实现了鲁棒的分割。
- en: A combined bounding-box object detector and semantic segmentation result can
    be obtained using architectures such as Mask R-CNN [[45](#bib.bib45)]. The method
    extends the effectiveness of Faster-RCNN to instance segmentation by adding a
    branch for predicting an object mask in parallel with the existing branch for
    bounding box recognition.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用诸如Mask R-CNN[[45](#bib.bib45)]的架构可以获得结合边界框目标检测和语义分割的结果。该方法通过为预测对象掩码添加一个分支，扩展了Faster-RCNN在实例分割中的有效性，与现有的边界框识别分支并行工作。
- en: Figure [5](#S4.F5 "Figure 5 ‣ 4.2.3 Localization ‣ 4.2 Driving Scene Understanding
    ‣ 4 Deep Learning for Driving Scene Perception and Localization ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving") shows tests results performed
    on four key semantic segmentation networks, based on the CityScapes dataset. The
    per-class mean Intersection over Union (mIoU) refers to multi-class segmentation,
    where each pixel is labeled as belonging to a specific object class, while per-category
    mIoU refers to foreground (object) - background (non-object) segmentation. The
    input samples have a size of $480px\times 320px$.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S4.F5 "Figure 5 ‣ 4.2.3 Localization ‣ 4.2 Driving Scene Understanding
    ‣ 4 Deep Learning for Driving Scene Perception and Localization ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving")显示了在CityScapes数据集上对四个关键语义分割网络进行的测试结果。每类平均交并比（mIoU）指的是多类分割，其中每个像素被标记为属于特定对象类，而每类别mIoU指的是前景（对象）
    - 背景（非对象）分割。输入样本的大小为$480px\times 320px$。
- en: '![Refer to caption](img/5902577748aa951168dccaf68f1d9e51.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/5902577748aa951168dccaf68f1d9e51.png)'
- en: 'Figure 4: Object detection and recognition performance comparison. The evaluation
    has been performed on the Pascal VOC 2012 benchmarking database. The first four
    methods on the right represent single stage detectors, while the remaining six
    are double stage detectors. Due to their increased complexity, the runtime performance
    in Frames-per-Second (FPS) is lower for the case of double stage detectors.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：目标检测和识别性能比较。评估已在Pascal VOC 2012基准数据库上进行。右侧的前四种方法表示单阶段检测器，而其余六种为双阶段检测器。由于其复杂性增加，双阶段检测器的每秒帧数（FPS）性能较低。
- en: 4.2.3 Localization
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 定位
- en: Localization algorithms aim at calculating the pose (position and orientation)
    of the autonomous vehicle as it navigates. Although this can be achieved with
    systems such as GPS, in the followings we will focus on deep learning techniques
    for visual based localization.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 定位算法的目的是在自主车辆导航时计算其姿态（位置和方向）。虽然这可以通过GPS等系统实现，但接下来我们将专注于基于视觉的深度学习定位技术。
- en: Visual Localization, also known as Visual Odometry (VO), is typically determined
    by matching keypoint landmarks in consecutive video frames. Given the current
    frame, these keypoints are used as input to a perspective-$n$-point mapping algorithm
    for computing the pose of the vehicle with respect to the previous frame. Deep
    learning can be used to improve the accuracy of VO by directly influencing the
    precision of the keypoints detector. In [[62](#bib.bib62)], a deep neural network
    has been trained for learning keypoints distractors in monocular VO. The so-called
    learned ephemerality mask, acts a a rejection scheme for keypoints outliers which
    might decrease the vehicle localization’s accuracy. The structure of the environment
    can be mapped incrementally with the computation of the camera pose. These methods
    belong to the area of Simultaneous Localization and Mapping (SLAM). For a survey
    on classical SLAM techniques, we refer the reader to [[63](#bib.bib63)].
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉定位，也称为视觉里程计（VO），通常通过在连续的视频帧中匹配关键点地标来确定。给定当前帧，这些关键点作为输入传递到一个透视-$n$-点映射算法中，用于计算车辆相对于前一帧的姿态。深度学习可以通过直接影响关键点检测器的精度来提高VO的准确性。在 [[62](#bib.bib62)]中，一个深度神经网络被训练用于学习单目VO中的关键点干扰物。所谓的学习到的瞬态掩膜，作为一个拒绝机制，用于排除可能降低车辆定位精度的关键点离群值。环境的结构可以通过计算相机姿态进行增量映射。这些方法属于同时定位与地图构建（SLAM）领域。有关经典SLAM技术的综述，请参见 [[63](#bib.bib63)]。
- en: Neural networks such as PoseNet [[64](#bib.bib64)], VLocNet++ [[65](#bib.bib65)],
    or the approaches introduced in [[66](#bib.bib66)], [[67](#bib.bib67)], [[68](#bib.bib68)], [[69](#bib.bib69)],
    or [[70](#bib.bib70)] are using image data to estimate the 3D pose of a camera
    in an End2End fashion. Scene semantics can be derived together with the estimated
    pose [[65](#bib.bib65)].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 像 PoseNet [[64](#bib.bib64)]、VLocNet++ [[65](#bib.bib65)] 或在 [[66](#bib.bib66)]、[[67](#bib.bib67)]、[[68](#bib.bib68)]、[[69](#bib.bib69)]、[[70](#bib.bib70)]
    中介绍的方法，使用图像数据以端到端的方式估计相机的3D姿态。场景语义可以与估计的姿态一起推导出来 [[65](#bib.bib65)]。
- en: LiDAR intensity maps are also suited for learning a real-time, calibration-agnostic
    localization for autonomous cars [[71](#bib.bib71)]. The method uses a deep neural
    network to build a learned representation of the driving scene from LiDAR sweeps
    and intensity maps. The localization of the vehicle is obtained through convolutional
    matching. In [[72](#bib.bib72)], laser scans and a deep neural network are used
    to learn descriptors for localization in urban and natural environments.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR强度图也适用于学习实时的、与校准无关的自动驾驶汽车定位 [[71](#bib.bib71)]。该方法使用深度神经网络从LiDAR扫描和强度图中构建驾驶场景的学习表示。车辆的定位通过卷积匹配获得。在 [[72](#bib.bib72)]中，激光扫描和深度神经网络用于学习城市和自然环境中的定位描述符。
- en: In order to safely navigate the driving scene, an autonomous car should be able
    to estimate the motion of the surrounding environment, also known as scene flow.
    Previous LiDAR based scene flow estimation techniques mainly relied on manually
    designed features. In recent articles, we have noticed a tendency to replace these
    classical methods with deep learning architectures able to automatically learn
    the scene flow. In [[73](#bib.bib73)], an encoding deep network is trained on
    occupancy grids with the purpose of finding matching or non-matching locations
    between successive timesteps.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全地导航驾驶场景，自动驾驶汽车应该能够估计周围环境的运动，也称为场景流。以往基于LiDAR的场景流估计技术主要依赖于手动设计的特征。在最近的文章中，我们注意到一种趋势，即用能够自动学习场景流的深度学习架构取代这些经典方法。在 [[73](#bib.bib73)]中，一个编码深度网络在占用网格上进行训练，目的是在连续时间步之间找到匹配或不匹配的位置。
- en: Although much progress has been reported in the area of deep learning based
    localization, VO techniques are still dominated by classical keypoints matching
    algorithms, combined with acceleration data provided by inertial sensors. This
    is mainly due to the fact that keypoints detectors are computational efficient
    and can be easily deployed on embedded devices.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在基于深度学习的定位领域已有许多进展，但视觉里程计（VO）技术仍然主要依赖于经典的关键点匹配算法，并结合了由惯性传感器提供的加速度数据。这主要是因为关键点检测器计算高效且易于在嵌入式设备上部署。
- en: '![Refer to caption](img/c41e4ed19e7dc3a9f96326e93560fbd8.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c41e4ed19e7dc3a9f96326e93560fbd8.png)'
- en: 'Figure 5: Semantic segmentation performance comparison on the CityScapes dataset [[74](#bib.bib74)].
    The input samples are $480px\times 320px$ images of driving scenes.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 CityScapes 数据集上的语义分割性能比较 [[74](#bib.bib74)]。输入样本是 $480px\times 320px$
    的驾驶场景图像。
- en: 4.3 Perception using Occupancy Maps
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用占据图的感知
- en: An occupancy map, also known as Occupancy Grid (OG), is a representation of
    the environment which divides the driving space into a set of cells and calculates
    the occupancy probability for each cell. Popular in robotics [[72](#bib.bib72)], [[75](#bib.bib75)],
    the OG representation became a suitable solution for self-driving vehicles. A
    couple of OG data samples are shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3 Perception
    using Occupancy Maps ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving").
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 占据图，也称为占据网格（OG），是一种环境表示方法，它将驾驶空间划分为一组单元，并计算每个单元的占据概率。在机器人技术中很受欢迎 [[72](#bib.bib72)],
    [[75](#bib.bib75)]，OG 表示法成为自动驾驶车辆的合适解决方案。图 [6](#S4.F6 "Figure 6 ‣ 4.3 Perception
    using Occupancy Maps ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving") 显示了几个 OG 数据样本。
- en: Deep learning is used in the context of occupancy maps either for dynamic objects
    detection and tracking [[76](#bib.bib76)], probabilistic estimation of the occupancy
    map surrounding the vehicle [[77](#bib.bib77)],[[78](#bib.bib78)], or for deriving
    the driving scene context [[79](#bib.bib79)], [[80](#bib.bib80)]. In the latter
    case, the OG is constructed by accumulating data over time, while a deep neural
    net is used to label the environment into driving context classes, such as highway
    driving, parking area, or inner-city driving.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在占据图的背景下，深度学习用于动态对象检测和跟踪 [[76](#bib.bib76)]，对车辆周围占据图的概率估计 [[77](#bib.bib77)],
    [[78](#bib.bib78)]，或用于推导驾驶场景上下文 [[79](#bib.bib79)], [[80](#bib.bib80)]。在后一种情况下，OG
    通过随时间积累数据来构建，同时使用深度神经网络将环境标记为驾驶上下文类别，如高速公路驾驶、停车区域或城市内部驾驶。
- en: Occupancy maps represent an in-vehicle virtual environment, integrating perceptual
    information in a form better suited for path planning and motion control. Deep
    learning plays an important role in the estimation of OG, since the information
    used to populate the grid cells is inferred from processing image and LiDAR data
    using scene perception methods, as the ones described in this chapter of the survey.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 占据图表示车内虚拟环境，整合感知信息，形式上更适合路径规划和运动控制。深度学习在 OG 估计中扮演重要角色，因为填充网格单元的信息是通过使用场景感知方法处理图像和激光雷达数据推断出来的，如本章调查中所述。
- en: '![Refer to caption](img/dc80bb3ebfdd99845dd7539d6a6cd539.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dc80bb3ebfdd99845dd7539d6a6cd539.png)'
- en: 'Figure 6: Examples of Occupancy Grids (OG). The images show a snapshot of the
    driving environment together with its respective occupancy grid [[80](#bib.bib80)].'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：占据网格（OG）的示例。这些图像展示了驾驶环境的快照以及其相应的占据网格 [[80](#bib.bib80)]。
- en: 5 Deep Learning for Path Planning and Behavior Arbitration
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度学习在路径规划和行为仲裁中的应用
- en: The ability of an autonomous car to find a route between two points, that is,
    a start position and a desired location, represents path planning. According to
    the path planning process, a self-driving car should consider all possible obstacles
    that are present in the surrounding environment and calculate a trajectory along
    a collision-free route. As stated in [[81](#bib.bib81)], autonomous driving is
    a multi-agent setting where the host vehicle must apply sophisticated negotiation
    skills with other road users when overtaking, giving way, merging, taking left
    and right turns, all while navigating unstructured urban roadways. The literature
    findings point to a non trivial policy that should handle safety in driving. Considering
    a reward function $R(\bar{s})=-r$ for an accident event that should be avoided
    and $R(\bar{s})\in[-1,1]$ for the rest of the trajectories, the goal is to learn
    to perform difficult maneuvers smoothly and safe.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车在两个点之间找到路径的能力，即起始位置和期望位置，代表了路径规划。根据路径规划过程，自驾车应考虑周围环境中所有可能的障碍物，并计算沿无碰撞路径的轨迹。如
    [[81](#bib.bib81)] 所述，自动驾驶是一个多代理设置，其中主车在超车、让路、合并、左转和右转时必须与其他道路使用者进行复杂的谈判，同时导航于非结构化的城市道路。文献研究表明，这需要一个复杂的策略来处理驾驶中的安全性。考虑到一个奖励函数
    $R(\bar{s})=-r$ 以避免事故事件，以及 $R(\bar{s})\in[-1,1]$ 用于其余的轨迹，目标是学习平稳且安全地执行困难的机动操作。
- en: This emerging topic of optimal path planning for autonomous cars should operate
    at high computation speeds, in order to obtain short reaction times, while satisfying
    specific optimization criteria. The survey in [[82](#bib.bib82)] provides a general
    overview of path planning in the automotive context. It addresses the taxonomy
    aspects of path planning, namely the mission planner, behavior planner and motion
    planner. However, [[82](#bib.bib82)] does not include a review on deep learning
    technologies, although the state of the art literature has revealed an increased
    interest in using deep learning technologies for path planning and behavior arbitration.
    Following, we discuss two of the most representative deep learning paradigms for
    path planning, namely Imitation Learning (IL) [[83](#bib.bib83)], [[84](#bib.bib84)], [[85](#bib.bib85)]
    and Deep Reinforcement Learning (DRL) based planning [[86](#bib.bib86)] [[87](#bib.bib87)].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 自主车的最优路径规划这一新兴主题应该以高计算速度运作，以获得短的反应时间，同时满足特定的优化标准。[[82](#bib.bib82)]中的调查提供了汽车背景下路径规划的总体概述。它涉及路径规划的分类方面，即任务规划器、行为规划器和运动规划器。然而，[[82](#bib.bib82)]未包含对深度学习技术的评审，尽管最新的文献已揭示出对使用深度学习技术进行路径规划和行为仲裁的兴趣增加。接下来，我们将探讨两种最具代表性的深度学习范式，用于路径规划，即模仿学习（IL）[[83](#bib.bib83)],
    [[84](#bib.bib84)], [[85](#bib.bib85)]和基于深度强化学习（DRL）的规划[[86](#bib.bib86)] [[87](#bib.bib87)]。
- en: The goal in Imitation Learning [[83](#bib.bib83)], [[84](#bib.bib84)], [[85](#bib.bib85)]
    is to learn the behavior of a human driver from recorded driving experiences [[88](#bib.bib88)].
    The strategy implies a vehicle teaching process from human demonstration. Thus,
    the authors employ CNNs to learn planning from imitation. For example, NeuroTrajectory [[85](#bib.bib85)]
    is a perception-planning deep neural network that learns the desired state trajectory
    of the ego-vehicle over a finite prediction horizon. Imitation learning can also
    be framed as an Inverse Reinforcement Learning (IRL) problem, where the goal is
    to learn the reward function from a human driver [[89](#bib.bib89)], [[27](#bib.bib27)].
    Such methods use real drivers behaviors to learn reward-functions and to generate
    human-like driving trajectories.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习[[83](#bib.bib83)], [[84](#bib.bib84)], [[85](#bib.bib85)]的目标是从记录的驾驶经验中学习人类驾驶员的行为[[88](#bib.bib88)]。该策略意味着从人类演示中进行车辆教学。因此，作者使用CNN来从模仿中学习规划。例如，NeuroTrajectory[[85](#bib.bib85)]是一个感知-规划深度神经网络，学习自我车辆在有限预测范围内的期望状态轨迹。模仿学习也可以被框定为一个逆向强化学习（IRL）问题，其目标是从人类驾驶员[[89](#bib.bib89)],
    [[27](#bib.bib27)]那里学习奖励函数。这些方法使用真实驾驶员的行为来学习奖励函数，并生成类似人类的驾驶轨迹。
- en: DRL for path planning deals mainly with learning driving trajectories in a simulator [[81](#bib.bib81)], [[90](#bib.bib90)], [[86](#bib.bib86)] [[87](#bib.bib87)].
    The real environmental model is abstracted and transformed into a virtual environment,
    based on a transfer model. In [[81](#bib.bib81)], it is stated that the objective
    function cannot ensure functional safety without causing a serious variance problem.
    The proposed solution for this issue is to construct a policy function composed
    of learnable and non-learnable parts. The learnable policy tries to maximize a
    reward function (which includes comfort, safety, overtake opportunity, etc.).
    At the same time, the non-learnable policy follows the hard constraints of functional
    safety, while maintaining an acceptable level of comfort.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: DRL用于路径规划主要处理在模拟器中学习驾驶轨迹[[81](#bib.bib81)], [[90](#bib.bib90)], [[86](#bib.bib86)]
    [[87](#bib.bib87)]。真实环境模型被抽象并转化为基于传递模型的虚拟环境。在[[81](#bib.bib81)]中指出，目标函数不能确保功能安全，而不会引发严重的方差问题。针对这一问题，提出的解决方案是构建一个由可学习和不可学习部分组成的策略函数。可学习策略尝试最大化奖励函数（包括舒适性、安全性、超车机会等）。同时，不可学习策略遵循功能安全的硬约束，同时保持可接受的舒适度。
- en: Both IL and DRL for path planning have advantages and disadvantages. IL has
    the advantage that it can be trained with data collected from the real-world.
    Nevertheless, this data is scarce on corner cases (e.g. driving off-lanes, vehicle
    crashes, etc.), making the trained network’s response uncertain when confronted
    with unseen data. On the other hand, although DRL systems are able to explore
    different driving situations within a simulated world, these models tend to have
    a biased behavior when ported to the real-world.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 路径规划中的IL和DRL各有优缺点。IL的优点在于可以利用现实世界收集的数据进行训练。然而，这些数据在边缘案例（例如偏离车道、车辆碰撞等）上稀缺，这使得训练后的网络在面对未见数据时反应不确定。另一方面，尽管DRL系统能够在模拟世界中探索不同的驾驶情况，但这些模型在迁移到现实世界时往往表现出偏差行为。
- en: 6 Motion Controllers for AI-based Self-Driving Cars
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于AI的自动驾驶汽车运动控制器
- en: The motion controller is responsible for computing the longitudinal and lateral
    steering commands of the vehicle. Learning algorithms are used either as part
    of Learning Controllers, within the motion control module from Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(a), or as
    complete End2End Control Systems which directly map sensory data to steering commands,
    as shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making
    Architectures for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for
    Autonomous Driving")(b).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 运动控制器负责计算车辆的纵向和横向转向指令。学习算法被用作学习控制器的一部分，位于图[1](#S2.F1 "Figure 1 ‣ 2 Deep Learning
    based Decision-Making Architectures for Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")(a)中的运动控制模块，或作为完整的端到端控制系统，直接将传感器数据映射到转向指令，如图[1](#S2.F1
    "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(b)所示。
- en: 6.1 Learning Controllers
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 学习控制器
- en: 'Traditional controllers make use of an a priori model composed of fixed parameters.
    When robots or other autonomous systems are used in complex environments, such
    as driving, traditional controllers cannot foresee every possible situation that
    the system has to cope with. Unlike controllers with fixed parameters, learning
    controllers make use of training information to learn their models over time.
    With every gathered batch of training data, the approximation of the true system
    model becomes more accurate, thus enabling model flexibility, consistent uncertainty
    estimates and anticipation of repeatable effects and disturbances that cannot
    be modeled prior to deployment [[91](#bib.bib91)]. Consider the following nonlinear,
    state-space system:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 传统控制器利用由固定参数组成的先验模型。当机器人或其他自主系统在复杂环境中使用时，例如驾驶，传统控制器无法预见系统必须应对的所有可能情况。与固定参数的控制器不同，学习控制器利用训练信息随时间学习其模型。随着每批训练数据的收集，真实系统模型的近似变得更加准确，从而实现模型的灵活性、一致的不确定性估计以及对不可预测效应和干扰的预期，这些在部署之前无法建模[[91](#bib.bib91)]。考虑以下非线性状态空间系统：
- en: '|  | $\mathbf{z}^{<t+1>}=\mathbf{f}_{true}(\mathbf{z}^{<t>},\mathbf{u}^{<t>}),$
    |  | (18) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}^{<t+1>}=\mathbf{f}_{true}(\mathbf{z}^{<t>},\mathbf{u}^{<t>}),$
    |  | (18) |'
- en: 'with observable state $\mathbf{z}^{<t>}\in\mathbb{R}^{n}$ and control input
    $\mathbf{u}^{<t>}\in\mathbb{R}^{m}$, at discrete time $t$. The true system $\mathbf{f}_{true}$
    is not known exactly and is approximated by the sum of an a-priori model and a
    learned dynamics model:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散时间$t$下，具有可观察状态$\mathbf{z}^{<t>}\in\mathbb{R}^{n}$和控制输入$\mathbf{u}^{<t>}\in\mathbb{R}^{m}$。真实系统$\mathbf{f}_{true}$并不完全确定，通过先验模型和学习的动态模型的总和来近似：
- en: '|  | $\mathbf{z}^{<t+1>}=\underset{\text{{a-priori} model}}{\mathbf{f}(\mathbf{z}^{<t>},\mathbf{u}^{<t>})}+\underset{\text{learned
    model}}{\mathbf{h}(\mathbf{z}^{<t>})}.$ |  | (19) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{z}^{<t+1>}=\underset{\text{{先验}模型}}{\mathbf{f}(\mathbf{z}^{<t>},\mathbf{u}^{<t>})}+\underset{\text{学习}模型}{\mathbf{h}(\mathbf{z}^{<t>})}.$
    |  | (19) |'
- en: In previous works, learning controllers have been introduced based on simple
    function approximators, such as Gaussian Process (GP) modeling [[92](#bib.bib92)],
    [[93](#bib.bib93)], [[91](#bib.bib91)], [[94](#bib.bib94)], or Support Vector
    Regression [[95](#bib.bib95)].
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在以往的研究中，已经引入了基于简单函数逼近器的学习控制器，例如高斯过程（GP）建模[[92](#bib.bib92)]，[[93](#bib.bib93)]，[[91](#bib.bib91)]，[[94](#bib.bib94)]，或支持向量回归[[95](#bib.bib95)]。
- en: Learning techniques are commonly used to learn a dynamics model which in turn
    improves an a priori system model in Iterative Learning Control (ILC) [[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)], [[99](#bib.bib99)]
    and Model Predictive Control (MPC) [[100](#bib.bib100)] [[101](#bib.bib101)], [[91](#bib.bib91)], [[94](#bib.bib94)], [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)], [[105](#bib.bib105)], [[106](#bib.bib106)].
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 学习技术通常用于学习动态模型，从而改善迭代学习控制（ILC）[[96](#bib.bib96)]、[[97](#bib.bib97)]、[[98](#bib.bib98)]、[[99](#bib.bib99)]和模型预测控制（MPC）[[100](#bib.bib100)]、[[101](#bib.bib101)]、[[91](#bib.bib91)]、[[94](#bib.bib94)]、[[102](#bib.bib102)]、[[103](#bib.bib103)]、[[104](#bib.bib104)]、[[105](#bib.bib105)]、[[106](#bib.bib106)]中的先验系统模型。
- en: Iterative Learning Control (ILC) is a method for controlling systems which work
    in a repetitive mode, such as path tracking in self-driving cars. It has been
    successfully applied to navigation in off-road terrain [[96](#bib.bib96)], autonomous
    car parking [[97](#bib.bib97)] and modeling of steering dynamics in an autonomous
    race car [[98](#bib.bib98)]. Multiple benefits are highlighted, such as the usage
    of a simple and computationally light feedback controller, as well as a decreased
    controller design effort (achieved by predicting path disturbances and platform
    dynamics).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代学习控制（ILC）是一种用于控制在重复模式下工作的系统的方法，例如自动驾驶汽车中的路径跟踪。它已成功应用于越野地形导航[[96](#bib.bib96)]、自主停车[[97](#bib.bib97)]以及自主赛车转向动态建模[[98](#bib.bib98)]。突出特点包括使用简单且计算轻量的反馈控制器，以及降低控制器设计工作量（通过预测路径干扰和平台动态实现）。
- en: Model Predictive Control (MPC) [[107](#bib.bib107)] is a control strategy that
    computes control actions by solving an optimization problem. It received lots
    of attention in the last two decades due to its ability to handle complex nonlinear
    systems with state and input constraints. The central idea behind MPC is to calculate
    control actions at each sampling time by minimizing a cost function over a short
    time horizon, while considering observations, input-output constraints and the
    system’s dynamics given by a process model. A general review of MPC techniques
    for autonomous robots is given in [[108](#bib.bib108)].
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测控制（MPC）[[107](#bib.bib107)]是一种通过解决优化问题来计算控制动作的控制策略。由于其处理具有状态和输入约束的复杂非线性系统的能力，在过去的二十年中受到了广泛关注。MPC的核心思想是在每个采样时间点计算控制动作，通过在短时间范围内最小化成本函数，同时考虑观测值、输入输出约束以及由过程模型给出的系统动态。有关自主机器人MPC技术的综合评述见[[108](#bib.bib108)]。
- en: Learning has been used in conjunction with MPC to learn driving models [[100](#bib.bib100)], [[101](#bib.bib101)],
    driving dynamics for race cars operating at their handling limits [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)],
    as well as to improve path tracking accuracy [[109](#bib.bib109)], [[91](#bib.bib91)], [[94](#bib.bib94)].
    These methods use learning mechanisms to identify nonlinear dynamics that are
    used in the MPC’s trajectory cost function optimization. This enables one to better
    predict disturbances and the behavior of the vehicle, leading to optimal comfort
    and safety constraints applied to the control inputs. Training data is usually
    in the form of past vehicle states and observations. For example, CNNs can be
    used to compute a dense occupancy grid map in a local robot-centric coordinate
    system. The grid map is further passed to the MPC’s cost function for optimizing
    the trajectory of the vehicle over a finite prediction horizon.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 学习已与MPC结合用于学习驾驶模型[[100](#bib.bib100)]、[[101](#bib.bib101)]、在处理极限下运行的赛车动态[[102](#bib.bib102)]、[[103](#bib.bib103)]、[[104](#bib.bib104)]，以及改善路径跟踪精度[[109](#bib.bib109)]、[[91](#bib.bib91)]、[[94](#bib.bib94)]。这些方法利用学习机制识别非线性动态，这些动态被用于MPC的轨迹成本函数优化。这使得能够更好地预测干扰和车辆行为，从而在控制输入上施加最佳的舒适性和安全性约束。训练数据通常以过去的车辆状态和观测值的形式存在。例如，可以使用CNN计算局部机器人中心坐标系统中的密集占用网格图。网格图进一步传递到MPC的成本函数中，以优化车辆在有限预测范围内的轨迹。
- en: A major advantage of learning controllers is that they optimally combine traditional
    model-based control theory with learning algorithms. This makes it possible to
    still use established methodologies for controller design and stability analysis,
    together with a robust learning component applied at system identification and
    prediction levels.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 学习控制器的主要优点在于它们能够最佳地结合传统的基于模型的控制理论与学习算法。这使得仍然可以使用已建立的方法论进行控制器设计和稳定性分析，同时在系统识别和预测层面应用强大的学习组件。
- en: 6.2 End2End Learning Control
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 端到端学习控制
- en: In the context of autonomous driving, End2End Learning Control is defined as
    a direct mapping from sensory data to control commands. The inputs are usually
    from a high-dimensional features space (e.g. images or point clouds). As illustrated
    in Fig [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures
    for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(b),
    this is opposed to traditional processing pipelines, where at first objects are
    detected in the input image, after which a path is planned and finally the computed
    control values are executed. A summary of some of the most popular End2End learning
    systems is given in Table [1](#S6.T1 "Table 1 ‣ 6.2 End2End Learning Control ‣
    6 Motion Controllers for AI-based Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving").
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在自主驾驶的背景下，端到端学习控制被定义为从传感器数据到控制指令的直接映射。输入通常来自高维特征空间（例如图像或点云）。如图 [1](#S2.F1 "图
    1 ‣ 2 基于深度学习的自驾车决策架构 ‣ 自主驾驶的深度学习技术综述") 所示，这与传统处理流程相对立，后者首先在输入图像中检测物体，然后规划路径，最后执行计算出的控制值。表
    [1](#S6.T1 "表 1 ‣ 6.2 端到端学习控制 ‣ 6 AI 驱动的自驾车运动控制器 ‣ 自主驾驶的深度学习技术综述") 给出了其中一些最受欢迎的端到端学习系统的总结。
- en: '| Name | Problem Space |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 问题空间 |'
- en: '&#124; Neural network &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经网络 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构 &#124;'
- en: '|'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sensor &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传感器 &#124;'
- en: '&#124; input &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入 &#124;'
- en: '| Description |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 描述 |'
- en: '|'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ALVINN &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ALVINN &#124;'
- en: '&#124;  [[110](#bib.bib110)] &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[110](#bib.bib110)] &#124;'
- en: '| Road following |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 道路跟随 |'
- en: '&#124; 3-layer &#124;'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3层 &#124;'
- en: '&#124; back-prop. &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反向传播。 &#124;'
- en: '&#124; network &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络 &#124;'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Camera, laser &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头，激光 &#124;'
- en: '&#124; range finder &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 距离传感器 &#124;'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ALVINN stands for Autonomous Land Vehicle In a Neural &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ALVINN 代表自主陆地车辆在神经网络中的 &#124;'
- en: '&#124; Network). Training has been conducted using simulated &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络）。训练已在模拟环境中进行 &#124;'
- en: '&#124; road images. Successful tests on the Carnegie Mellon &#124;'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 道路图像。成功的测试在卡内基梅隆 &#124;'
- en: '&#124; autonomous navigation test vehicle indicate that the &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主导航测试车辆表明， &#124;'
- en: '&#124; network can effectively follow real roads. &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网络可以有效地跟随实际道路。 &#124;'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DAVE &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DAVE &#124;'
- en: '&#124;  [[111](#bib.bib111)] &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[111](#bib.bib111)] &#124;'
- en: '| DARPA challenge |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| DARPA 挑战 |'
- en: '&#124; 6-layer &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 6层 &#124;'
- en: '&#124; CNN &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN &#124;'
- en: '|'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Raw camera &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始摄像头 &#124;'
- en: '&#124; images &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A vision-based obstacle avoidance system for off-road &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一种基于视觉的越野障碍物避让系统 &#124;'
- en: '&#124; mobile robots. The robot is a 50cm off-road truck, with two &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动机器人。机器人是一个50厘米的越野卡车，配有两个 &#124;'
- en: '&#124; front color cameras. A remote computer processes the video &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 前置彩色摄像头。一台远程计算机处理视频 &#124;'
- en: '&#124; and controls the robot via radio. &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并通过无线电控制机器人。 &#124;'
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NVIDIA PilotNet &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NVIDIA PilotNet &#124;'
- en: '&#124;  [[112](#bib.bib112)] &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[112](#bib.bib112)] &#124;'
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Autonomous &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主 &#124;'
- en: '&#124; driving in real &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实际驾驶中的 &#124;'
- en: '&#124; traffic situations &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通情况 &#124;'
- en: '| CNN |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| CNN |'
- en: '&#124; Raw camera &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始摄像头 &#124;'
- en: '&#124; images &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The system automatically learns internal representations of &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统自动学习内部表示 &#124;'
- en: '&#124; the necessary processing steps such as detecting useful road &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 所需的处理步骤，如检测有用的道路 &#124;'
- en: '&#124; features with human steering angle as the training signal. &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征以人为转向角作为训练信号。 &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Novel FCN-LSTM &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新颖的 FCN-LSTM &#124;'
- en: '&#124;  [[113](#bib.bib113)] &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Ego-motion &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自我运动 &#124;'
- en: '&#124; prediction &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预测 &#124;'
- en: '| FCN-LSTM |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| FCN-LSTM |'
- en: '&#124; Large scale &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大规模 &#124;'
- en: '&#124; video data &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频数据 &#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A generic vehicle motion model from large scale crowd- &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个来自大规模人群的通用车辆运动模型 &#124;'
- en: '&#124; sourced video data is obtained, while developing an end-to &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 获取了源视频数据，同时开发了一个端到端的 &#124;'
- en: '&#124; -end trainable architecture (FCN-LSTM) for predicting a &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -end 可训练架构 (FCN-LSTM) 用于预测一个 &#124;'
- en: '&#124; distribution of future vehicle ego-motion data. &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未来车辆自我运动数据的分布。 &#124;'
- en: '|'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Novel C-LSTM &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新颖的 C-LSTM &#124;'
- en: '&#124;  [[114](#bib.bib114)] &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[114](#bib.bib114)] &#124;'
- en: '| Steering angle control | C-LSTM |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 转向角控制 | C-LSTM |'
- en: '&#124; Camera frames, &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头帧， &#124;'
- en: '&#124; steering wheel &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向轮 &#124;'
- en: '&#124; angle &#124;'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向角度 &#124;'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; C-LSTM is end-to-end trainable, learning both visual and &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; C-LSTM 是端到端可训练的，学习视觉和 &#124;'
- en: '&#124; dynamic temporal dependencies of driving. Additionally, the &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 驾驶的动态时间依赖性。此外， &#124;'
- en: '&#124; steering angle regression problem is considered classification &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向角度回归问题被视为分类 &#124;'
- en: '&#124; while imposing a spatial relationship between the output &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 同时对输出施加空间关系 &#124;'
- en: '&#124; layer neurons. &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层神经元。 &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Drive360 &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Drive360 &#124;'
- en: '&#124;  [[115](#bib.bib115)] &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[115](#bib.bib115)] &#124;'
- en: '|'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Steering angle and &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向角度和 &#124;'
- en: '&#124; velocity control &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 速度控制 &#124;'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN + Fully &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN + 完全 &#124;'
- en: '&#124; Connected + &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 连接 + &#124;'
- en: '&#124; LSTM &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSTM &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Surround-view &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环绕视图 &#124;'
- en: '&#124; cameras, CAN &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头，CAN &#124;'
- en: '&#124; bus reader &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总线读取器 &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The sensor setup provides data for a 360-degree view of &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传感器设置提供了 360 度的视图 &#124;'
- en: '&#124; the area surrounding the vehicle. A new driving dataset &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车辆周围的区域。收集了一个新的驾驶数据集 &#124;'
- en: '&#124; is collected, covering diverse scenarios. A novel driving &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 覆盖了多种场景。一个新颖的驾驶 &#124;'
- en: '&#124; model is developed by integrating the surround-view &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型是通过集成环绕视图 &#124;'
- en: '&#124; cameras with the route planner. &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相机与路线规划器。 &#124;'
- en: '|'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DNN policy &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DNN 策略 &#124;'
- en: '&#124;  [[116](#bib.bib116)] &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[116](#bib.bib116)] &#124;'
- en: '| Steering angle control | CNN + FC | Camera images |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 转向角度控制 | CNN + FC | 摄像头图像 |'
- en: '&#124; The trained neural net directly maps pixel data from a &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练后的神经网络直接映射来自 &#124;'
- en: '&#124; front-facing camera to steering commands and does not &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从前向摄像头到转向指令的计算复杂度，并且不 &#124;'
- en: '&#124; require any other sensors. We compare the controller &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不需要任何其他传感器。我们比较了控制器 &#124;'
- en: '&#124; performance with the steering behavior of a human driver. &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与人类驾驶员的转向行为进行性能比较。 &#124;'
- en: '|'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DeepPicar &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepPicar &#124;'
- en: '&#124;  [[117](#bib.bib117)] &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[117](#bib.bib117)] &#124;'
- en: '| Steering angle control | CNN | Camera images |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 转向角度控制 | CNN | 摄像头图像 |'
- en: '&#124; DeepPicar is a small scale replica of a real self-driving car &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DeepPicar 是一个真实自驾车的缩小版 &#124;'
- en: '&#124; called DAVE-2 by NVIDIA. It uses the same network &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NVIDIA 称之为 DAVE-2。它使用相同的网络 &#124;'
- en: '&#124; architecture and can drive itself in real-time using a web &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构，并且可以使用网络实时驾驶 &#124;'
- en: '&#124; camera and a Raspberry Pi 3. &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头和 Raspberry Pi 3。 &#124;'
- en: '|'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TORCS DRL &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TORCS DRL &#124;'
- en: '&#124;  [[23](#bib.bib23)] &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[23](#bib.bib23)] &#124;'
- en: '|'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lane keeping and &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 车道保持和 &#124;'
- en: '&#124; obstacle avoidance &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 障碍物避让 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DQN + RNN &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN + RNN &#124;'
- en: '&#124; + CNN &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + CNN &#124;'
- en: '|'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TORCS &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TORCS &#124;'
- en: '&#124; simulator &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟器 &#124;'
- en: '&#124; images &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; It incorporates Recurrent Neural Networks for information &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它结合了递归神经网络用于信息 &#124;'
- en: '&#124; integration, enabling the car to handle partially observable &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 集成，使汽车能够处理部分可观察的 &#124;'
- en: '&#124; scenarios. It also reduces the computational complexity for &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 场景。它还减少了 &#124;'
- en: '&#124; deployment on embedded hardware. &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 嵌入式硬件上的部署。 &#124;'
- en: '|'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TORCS E2E &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TORCS E2E &#124;'
- en: '&#124;  [[118](#bib.bib118)] &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[118](#bib.bib118)] &#124;'
- en: '|'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Steering angle control &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向角度控制 &#124;'
- en: '&#124; in a simulated &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在模拟的 &#124;'
- en: '&#124; env. (TORCS) &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环境 (TORCS) &#124;'
- en: '| CNN |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| CNN |'
- en: '&#124; TORCS &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TORCS &#124;'
- en: '&#124; simulator &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟器 &#124;'
- en: '&#124; images &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The image features are split into three categories (sky-related, &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像特征分为三类（天空相关， &#124;'
- en: '&#124; roadside-related, and roadrelated features). Two experimental &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路边相关和道路相关特征)。两个实验 &#124;'
- en: '&#124; frameworks are used to investigate the importance of each &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 框架用于研究每个 &#124;'
- en: '&#124; single feature for training a CNN controller. &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单一特征用于训练 CNN 控制器。 &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Agile Autonomous Driving &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 敏捷自动驾驶 &#124;'
- en: '&#124;  [[106](#bib.bib106)] &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[106](#bib.bib106)] &#124;'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Steering angle and &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 转向角度和 &#124;'
- en: '&#124; velocity control &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 速度控制 &#124;'
- en: '&#124; for aggressive driving &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于激进驾驶 &#124;'
- en: '| CNN |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| CNN |'
- en: '&#124; Raw camera &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始相机 &#124;'
- en: '&#124; images &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '|'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A CNN, refereed to as the learner, is trained with optimal &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个被称为学习者的 CNN 通过最优的 &#124;'
- en: '&#124; trajectory examples provided at training time by an MPC controller.
    &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练时由 MPC 控制器提供的轨迹示例。 &#124;'
- en: '&#124; The MPC acts as an expert, encoding the scene dynamics &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPC 作为专家，编码场景动态 &#124;'
- en: '&#124; into the layers of the neural network. &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 进入神经网络的层中。 &#124;'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WRC6 AD &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WRC6 AD &#124;'
- en: '&#124;  [[26](#bib.bib26)] &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[26](#bib.bib26)] &#124;'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Driving in a &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 驾驶于 &#124;'
- en: '&#124; racing game &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 赛车游戏 &#124;'
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN + LSTM &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN + LSTM &#124;'
- en: '&#124; Encoder &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码器 &#124;'
- en: '|'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WRC6 &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WRC6 &#124;'
- en: '&#124; Racing &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 赛车 &#124;'
- en: '&#124; Game &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 游戏 &#124;'
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; An Asynchronous ActorCritic (A3C) framework is used to &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用异步演员评论家（A3C）框架来 &#124;'
- en: '&#124; learn the car control in a physically and graphically realistic &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在物理和图形上逼真的 &#124;'
- en: '&#124; rally game, with the agents evolving simultaneously on &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比赛游戏，其中代理同时进化于 &#124;'
- en: '&#124; different tracks. &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不同赛道。 &#124;'
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 1: Summary of End2End learning methods.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：End2End 学习方法的总结。
- en: End2End learning can also be formulated as a back-propagation algorithm scaled
    up to complex models. The paradigm was first introduced in the 1990s, when the
    Autonomous Land Vehicle in a Neural Network (ALVINN) system was built [[110](#bib.bib110)].
    ALVINN was designed to follow a pre-defined road, steering according to the observed
    road’s curvature. The next milestone in End2End driving is considered to be in
    the mid 2000s, when DAVE (Darpa Autonomous VEhicle) managed to drive through an
    obstacle-filled road, after it has been trained on hours of human driving acquired
    in similar, but not identical, driving scenarios [[111](#bib.bib111)]. Over the
    last couple of years, the technological advances in computing hardware have facilitated
    the usage of End2End learning models. The back-propagation algorithm for gradient
    estimation in deep networks is now efficiently implemented on parallel Graphic
    Processing Units (GPUs). This kind of processing allows the training of large
    and complex network architectures, which in turn require huge amounts of training
    samples (see Section [8](#S8 "8 Data Sources for Training Autonomous Driving Systems
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: End2End 学习也可以被表述为一个扩展到复杂模型的反向传播算法。这个范式最早在 1990 年代被提出，当时构建了自主土地车辆神经网络系统（ALVINN）[[110](#bib.bib110)]。ALVINN
    被设计用于跟随预定义的道路，根据观察到的道路曲率进行转向。End2End 驾驶的下一个里程碑被认为是在 2000 年代中期，当时 DAVE（达尔帕自主车辆）成功驾驶通过了充满障碍的道路，经过了类似但不完全相同的驾驶场景中的人类驾驶训练[[111](#bib.bib111)]。在过去几年中，计算硬件的技术进步促进了
    End2End 学习模型的使用。现在，深度网络中的梯度估计反向传播算法已高效地在并行图形处理单元（GPUs）上实现。这种处理允许训练大型和复杂的网络架构，而这些架构又需要大量的训练样本（参见第[8](#S8
    "8 Data Sources for Training Autonomous Driving Systems ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")节）。
- en: End2End control papers mainly employ either deep neural networks trained offline
    on real-world and/or synthetic data [[119](#bib.bib119)], [[113](#bib.bib113)], [[114](#bib.bib114)], [[115](#bib.bib115)], [[120](#bib.bib120)], [[116](#bib.bib116)], [[117](#bib.bib117)], [[121](#bib.bib121)], [[118](#bib.bib118)],
    or Deep Reinforcement Learning (DRL) systems trained and evaluated in simulation [[23](#bib.bib23)] [[122](#bib.bib122)], [[26](#bib.bib26)].
    Methods for porting simulation trained DRL models to real-world driving have also
    been reported [[123](#bib.bib123)], as well as DRL systems trained directly on
    real-world image data [[105](#bib.bib105)], [[106](#bib.bib106)].
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: End2End 控制论文主要使用离线训练的深度神经网络，训练数据来自现实世界和/或合成数据 [[119](#bib.bib119)], [[113](#bib.bib113)],
    [[114](#bib.bib114)], [[115](#bib.bib115)], [[120](#bib.bib120)], [[116](#bib.bib116)],
    [[117](#bib.bib117)], [[121](#bib.bib121)], [[118](#bib.bib118)]，或在模拟中训练和评估的深度强化学习（DRL）系统
    [[23](#bib.bib23)] [[122](#bib.bib122)], [[26](#bib.bib26)]。也有报告了将模拟训练的 DRL 模型迁移到现实世界驾驶中的方法
    [[123](#bib.bib123)]，以及直接在现实世界图像数据上训练的 DRL 系统 [[105](#bib.bib105)], [[106](#bib.bib106)]。
- en: End2End methods have been popularized in the last couple of years by NVIDIA^®,
    as part of the PilotNet architecture. The approach is to train a CNN which maps
    raw pixels from a single front-facing camera directly to steering commands [[119](#bib.bib119)].
    The training data is composed of images and steering commands collected in driving
    scenarios performed in a diverse set of lighting and weather conditions, as well
    as on different road types. Prior to training, the data is enriched using augmentation,
    adding artificial shifts and rotations to the original data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: End2End 方法在过去几年里由 NVIDIA^® 推广，作为 PilotNet 架构的一部分。该方法的核心是训练一个卷积神经网络（CNN），将来自单个前视摄像头的原始像素直接映射到转向指令 [[119](#bib.bib119)]。训练数据包括在各种光照和天气条件下以及不同道路类型上的驾驶场景中收集的图像和转向指令。在训练之前，数据通过数据增强进行丰富，为原始数据添加人工位移和旋转。
- en: 'PilotNet has $250.000$ parameters and approx. $27mil.$ connections. The evaluation
    is performed in two stages: first in simulation and secondly in a test car. An
    autonomy performance metric represents the percentage of time when the neural
    network drives the car:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: PilotNet 具有 $250.000$ 个参数和约 $27mil.$ 个连接。评估分为两个阶段：首先在模拟环境中进行，其次在测试车上进行。一个自治性能指标表示神经网络驾驶汽车的时间百分比：
- en: '|  | $autonomy=(1-\frac{(no.\;of\;interventions)*6\;sec}{elapsed\;time\;[sec]})*100.$
    |  | (20) |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | $autonomy=(1-\frac{(no.\;of\;interventions)*6\;sec}{elapsed\;time\;[sec]})*100.$
    |  | (20) |'
- en: An intervention is considered to take place when the simulated vehicle departs
    from the center line by more than one meter, assuming that $6$ seconds is the
    time needed by a human to retake control of the vehicle and bring it back to the
    desired state. An autonomy of $98\%$ was reached on a $20km$ drive from Holmdel
    to Atlantic Highlands in NJ, USA. Through training, PilotNet learns how the steering
    commands are computed by a human driver [[112](#bib.bib112)]. The focus is on
    determining which elements in the input traffic image have the most influence
    on the network’s steering decision. A method for finding the salient object regions
    in the input image is described, while reaching the conclusion that the low-level
    features learned by PilotNet are similar to the ones that are relevant to a human
    driver.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当模拟车辆偏离中心线超过一米时，认为发生了干预，假设 $6$ 秒是人类重新控制车辆并将其恢复到期望状态所需的时间。在美国新泽西州 Holmdel 到 Atlantic
    Highlands 的 $20km$ 驾驶过程中达到了 $98\%$ 的自主性。通过训练，PilotNet 学会了人类驾驶员如何计算转向指令 [[112](#bib.bib112)]。重点是确定输入交通图像中的哪些元素对网络的转向决策影响最大。描述了找到输入图像中显著对象区域的方法，并得出结论，PilotNet
    学到的低级特征与对人类驾驶员相关的特征类似。
- en: End2End architectures similar to PilotNet, which map visual data to steering
    commands, have been reported in [[116](#bib.bib116)], [[117](#bib.bib117)], [[121](#bib.bib121)].
    In [[113](#bib.bib113)], autonomous driving is formulated as a future ego-motion
    prediction problem. The introduced FCN-LSTM (Fully Convolutional Network - Long-Short
    Term Memory) method is designed to jointly train pixel-level supervised tasks
    using a fully convolutional encoder, together with motion prediction through a
    temporal encoder. The combination between visual temporal dependencies of the
    input data has also been considered in [[114](#bib.bib114)], where the C-LSTM
    (Convolutional Long Short Term Memory) network has been proposed for steering
    control. In [[115](#bib.bib115)], surround-view cameras were used for End2End
    learning. The claim is that human drivers also use rear and side-view mirrors
    for driving, thus all the information from around the vehicle needs to be gathered
    and integrated into the network model in order to output a suitable control command.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 PilotNet 的 End2End 架构，将视觉数据映射到转向指令的，已在 [[116](#bib.bib116)]， [[117](#bib.bib117)]， [[121](#bib.bib121)]
    中报告。在 [[113](#bib.bib113)] 中，将自动驾驶定义为未来自我运动预测问题。引入的 FCN-LSTM（全卷积网络 - 长短期记忆）方法旨在通过一个全卷积编码器共同训练像素级监督任务，同时通过时间编码器进行运动预测。在 [[114](#bib.bib114)]
    中也考虑了输入数据的视觉时间依赖性，其中提出了用于转向控制的 C-LSTM（卷积长短期记忆）网络。在 [[115](#bib.bib115)] 中，环视摄像头用于
    End2End 学习。声称人类驾驶员也使用后视镜和侧视镜，因此需要从车辆周围收集并整合所有信息到网络模型中，以输出合适的控制指令。
- en: To carry out an evaluation of the Tesla^® Autopilot system, [[120](#bib.bib120)]
    proposed an End2End Convolutional Neural Network framework. It is designed to
    determine differences between Autopilot and its own output, taking into consideration
    edge cases. The network was trained using real data, collected from over $420$
    hours of real road driving. The comparison between Tesla^®’s Autopilot and the
    proposed framework was done in real-time on a Tesla^® car. The evaluation revealed
    an accuracy of $90.4\%$ in detecting differences between both systems and the
    control transfer of the car to a human driver.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对Tesla^® Autopilot系统进行评估，[[120](#bib.bib120)]提出了一个End2End卷积神经网络框架。该框架旨在确定Autopilot与其自身输出之间的差异，考虑边缘案例。网络使用从超过$420$小时的实际道路驾驶中收集的真实数据进行训练。对Tesla^®的Autopilot和提出的框架之间的比较是在Tesla^®汽车上实时进行的。评估结果显示，检测两个系统之间差异的准确率为$90.4\%$，以及车辆对人类驾驶员的控制转移。
- en: 'Another approach to design End2End driving systems is DRL. This is mainly performed
    in simulation, where an autonomous agent can safely explore different driving
    strategies. In [[23](#bib.bib23)], a DRL End2End system is used to compute steering
    command in the TORCS game simulation engine. Considering a more complex virtual
    environment, [[122](#bib.bib122)] proposed an asynchronous advantage Actor-Critic
    (A3C) method for training a CNN on images and vehicle velocity information. The
    same idea has been enhanced in [[26](#bib.bib26)], having a faster convergence
    and permissiveness for more generalization. Both articles rely on the following
    procedure: receiving the current state of the game, deciding on the next control
    commands and then getting a reward on the next iteration. The experimental setup
    benefited from a realistic car game, namely World Rally Championship 6, and also
    from other simulated environments, like TORCS.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 设计End2End驾驶系统的另一种方法是DRL。这主要在仿真中进行，其中自主代理可以安全地探索不同的驾驶策略。在[[23](#bib.bib23)]中，使用了一个DRL
    End2End系统来计算TORCS游戏仿真引擎中的转向指令。考虑到更复杂的虚拟环境，[[122](#bib.bib122)]提出了一种用于训练CNN处理图像和车辆速度信息的异步优势演员-评论家（A3C）方法。[[26](#bib.bib26)]进一步增强了这一思想，实现了更快的收敛和更广泛的泛化。两个文章都依赖于以下过程：接收游戏的当前状态，决定下一个控制指令，然后在下一个迭代中获得奖励。实验设置受益于一个真实的汽车游戏，即《世界拉力锦标赛6》，以及其他仿真环境，如TORCS。
- en: The next trend in DRL based control seems to be the inclusion of classical model-based
    control techniques, as the ones detailed in Section [6.1](#S6.SS1 "6.1 Learning
    Controllers ‣ 6 Motion Controllers for AI-based Self-Driving Cars ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving"). The classical controller provides
    a stable and deterministic model on top of which the policy of the neural network
    is estimated. In this way, the hard constraints of the modeled system are transfered
    into the neural network policy [[124](#bib.bib124)]. A DRL policy trained on real-world
    image data has been proposed in [[105](#bib.bib105)] and [[106](#bib.bib106)]
    for the task of aggressive driving. In this case, a CNN, refereed to as the learner,
    is trained with optimal trajectory examples provided at training time by a model
    predictive controller.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DRL的控制的下一个趋势似乎是包括经典的基于模型的控制技术，如第[6.1](#S6.SS1 "6.1 Learning Controllers ‣
    6 Motion Controllers for AI-based Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")节中详细介绍的那样。经典控制器提供了一个稳定且确定的模型，在其上估计神经网络的策略。这样，建模系统的硬约束被转移到神经网络策略中[[124](#bib.bib124)]。在[[105](#bib.bib105)]和[[106](#bib.bib106)]中提出了一种在现实世界图像数据上训练的DRL策略，用于激进驾驶任务。在这种情况下，CNN被称为学习者，通过模型预测控制器在训练时提供的最佳轨迹示例进行训练。
- en: 7 Safety of Deep Learning in Autonomous Driving
  id: totrans-438
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 深度学习在自主驾驶中的安全性
- en: 'Safety implies the absence of the conditions that cause a system to be dangerous [[125](#bib.bib125)].
    Demonstrating the safety of a system which is running deep learning techniques
    depends heavily on the type of technique and the application context. Thus, reasoning
    about the safety of deep learning techniques requires:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性意味着系统不具备导致危险的条件[[125](#bib.bib125)]。证明运行深度学习技术的系统的安全性在很大程度上依赖于技术的类型和应用背景。因此，推理深度学习技术的安全性需要：
- en: •
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: understanding the impact of the possible failures;
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解可能失败的影响；
- en: •
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: understanding the context within the wider system;
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解更广泛系统中的背景；
- en: •
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: defining the assumption regarding the system context and the environment in
    which it will likely be used;
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义关于系统背景和可能使用环境的假设；
- en: •
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: defining what a safe behavior means, including non-functional constraints.
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义安全行为的含义，包括非功能性约束。
- en: In [[126](#bib.bib126)], an example is mapped on the above requirements with
    respect to a deep learning component. The problem space for the component is pedestrian
    detection with convolutional neural networks. The top level task of the system
    is to locate an object of class person from a distance of 100 meters, with a lateral
    accuracy of +/- 20 cm, a false negative rate of 1% and false positive rate of
    5%. The assumptions is that the braking distance and speed are sufficient to react
    when detecting persons which are 100 meters ahead of the planned trajectory of
    the vehicle. Alternative sensing methods can be used in order to reduce the overall
    false negative and false positive rates of the system to an acceptable level.
    The context information is that the distance and the accuracy shall be mapped
    to the dimensions of the image frames presented to the CNN.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[126](#bib.bib126)]中，一个例子根据上述要求映射到深度学习组件。该组件的问题空间是使用卷积神经网络进行行人检测。系统的顶层任务是从100米的距离内定位一个人类目标，具有+/-
    20厘米的横向精度，1%的假阴性率和5%的假阳性率。假设制动距离和速度足以在检测到位于车辆计划轨迹100米前方的人员时做出反应。可以使用替代传感方法将系统的总体假阴性率和假阳性率降低到可接受的水平。上下文信息是距离和精度应映射到呈现给CNN的图像框的尺寸。
- en: There is no commonly agreed definition for the term safety in the context of
    machine learning or deep learning. In [[127](#bib.bib127)], Varshney defines safety
    in terms of risk, epistemic uncertainty and the harm incurred by unwanted outcomes.
    He then analyses the choice of cost function and the appropriateness of minimizing
    the empirical average training cost.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习或深度学习的上下文中，尚未达成对安全术语的共同定义。在[[127](#bib.bib127)]中，Varshney 从风险、认知不确定性和非预期结果造成的危害来定义安全。他随后分析了成本函数的选择以及最小化经验平均训练成本的适宜性。
- en: '[[128](#bib.bib128)] takes into consideration the problem of accidents in machine
    learning systems. Such accidents are defined as unintended and harmful behaviors
    that may emerge from a poor AI system design. The authors present a list of five
    practical research problems related to accident risk, categorized according to
    whether the problem originates from having the wrong objective function (avoiding
    side effects and avoiding reward hacking), an objective function that is too expensive
    to evaluate frequently (scalable supervision), or undesirable behavior during
    the learning process (safe exploration and distributional shift).'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[[128](#bib.bib128)] 考虑了机器学习系统中的事故问题。这样的事故被定义为可能因AI系统设计不佳而出现的非预期和有害行为。作者提出了一份涉及事故风险的五个实际研究问题的清单，这些问题根据问题是否源于目标函数错误（避免副作用和奖励黑客）、目标函数评估频繁时过于昂贵（可扩展监督），或学习过程中不良行为（安全探索和分布转移）进行分类。'
- en: 'Enlarging the scope of safety, [[129](#bib.bib129)] propose a decision-theoretic
    definition of safety that applies to a broad set of domains and systems. They
    define safety to be the reduction or minimization of risk and epistemic uncertainty
    associated with unwanted outcomes that are severe enough to be seen as harmful.
    The key points in this definition are: i) the cost of unwanted outcomes has to
    be sufficiently high in some human sense for events to be harmful, and ii) safety
    involves reducing both the probability of expected harms, as well as the possibility
    of unexpected harms.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 扩大安全范围，[[129](#bib.bib129)] 提出了一个适用于广泛领域和系统的决策理论安全定义。他们将安全定义为减少或最小化与严重到足以被视为有害的非预期结果相关的风险和认知不确定性。该定义的关键点是：i）非预期结果的成本在某种人类意义上必须足够高才能被认为是有害的；ii）安全涉及降低预期危害的概率以及意外危害的可能性。
- en: Regardless of the above empirical definitions and possible interpretations of
    safety, the use of deep learning components in safety critical systems is still
    an open question. The ISO 26262 standard for functional safety of road vehicles
    provides a comprehensive set of requirements for assuring safety, but does not
    address the unique characteristics of deep learning-based software.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有上述经验定义和对安全的可能解释，将深度学习组件应用于安全关键系统仍然是一个悬而未决的问题。ISO 26262道路车辆功能安全标准提供了一套全面的安全保障要求，但未涉及深度学习基础软件的独特特性。
- en: '[[130](#bib.bib130)] addresses this gap by analyzing the places where machine
    learning can impact the standard and provides recommendations on how to accommodate
    this impact. These recommendations are focused towards the direction of identifying
    the hazards, implementing tools and mechanism for fault and failure situations,
    but also ensuring complete training datasets and designing a multi-level architecture.
    The usage of specific techniques for various stages within the software development
    life-cycle is desired.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '[[130](#bib.bib130)]通过分析机器学习可能影响标准的地方来解决这一空白，并提供了如何适应这一影响的建议。这些建议主要集中在识别危险、实施故障和失败情况的工具和机制，以及确保完整的训练数据集和设计多级架构的方向。期望在软件开发生命周期的各个阶段使用特定的技术。'
- en: 'The standard ISO 26262 recommends the use of a Hazard Analysis and Risk Assessment
    (HARA) method to identify hazardous events in the system and to specify safety
    goals that mitigate the hazards. The standard has 10 parts. Our focus is on Part
    6: product development at the software level, the standard following the well-known
    V model for engineering. Automotive Safety Integrity Level (ASIL) refers to a
    risk classification scheme defined in ISO 26262 for an item (e.g. subsystem) in
    an automotive system.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 标准ISO 26262建议使用危险分析和风险评估（HARA）方法来识别系统中的危险事件，并指定减轻这些危险的安全目标。该标准共有10个部分。我们的重点是第6部分：软件级产品开发，该标准遵循工程领域中著名的V模型。汽车安全完整性等级（ASIL）是指ISO
    26262中为汽车系统中的项目（例如子系统）定义的风险分类方案。
- en: ASIL represents the degree of rigor required (e.g., testing techniques, types
    of documentation required, etc.) to reduce risk, where ASIL D represents the highest
    and ASIL A the lowest risk. If an element is assigned to QM (Quality Management),
    it does not require safety management. The ASIL assessed for a given hazard is
    at first assigned to the safety goal set to address the hazard and is then inherited
    by the safety requirements derived from that goal [[130](#bib.bib130)].
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ASIL代表了减少风险所需的严格程度（例如，测试技术、所需的文档类型等），其中ASIL D表示最高风险，ASIL A表示最低风险。如果一个元素被分配给QM（质量管理），则不需要安全管理。给定危险的ASIL最初分配给解决该危险的安全目标，然后由从该目标派生的安全要求继承[[130](#bib.bib130)]。
- en: According to ISO26226, a hazard is defined as ”potential source of harm caused
    by a malfunctioning behavior, where harm is a physical injury or damage to the
    health of a person” [[131](#bib.bib131)]. Nevertheless, a deep learning component
    can create new types of hazards. An example of such a hazard is usually happening
    because humans think that the automated driver assistance (often developed using
    learning techniques) is more reliable than it actually is [[132](#bib.bib132)].
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 根据ISO26226，危险被定义为“由故障行为引起的潜在伤害源，其中伤害是对人的身体伤害或健康损害”[[131](#bib.bib131)]。然而，深度学习组件可能会产生新的类型的危险。这样的危险的一个例子通常发生在因为人们认为自动驾驶辅助（通常使用学习技术开发）比实际情况更可靠[[132](#bib.bib132)]。
- en: Due to its complexity, a deep learning component can fail in unique ways. For
    example, in Deep Reinforcement Learning systems, faults in the reward function
    can negatively affect the trained model [[128](#bib.bib128)]. In such a case,
    the automated vehicle figures out that it can avoid getting penalized for driving
    too close to other vehicles by exploiting certain sensor vulnerabilities so that
    it can’t see how close it is getting. Although hazards such as these may be unique
    to deep reinforcement learning components, they can be traced to faults, thus
    fitting within the existing guidelines of ISO 26262.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其复杂性，深度学习组件可能以独特的方式失败。例如，在深度强化学习系统中，奖励函数中的故障可能会对训练的模型产生负面影响[[128](#bib.bib128)]。在这种情况下，自动驾驶车辆会发现它可以通过利用某些传感器的漏洞来避免因驾驶过近其他车辆而受到惩罚，这样它就无法看到自己距离其他车辆有多近。虽然这些危险可能对深度强化学习组件是独特的，但它们可以追溯到故障，从而符合ISO
    26262的现有指南。
- en: 'A key requirement for analyzing the safety of deep learning components is to
    examine whether immediate human costs of outcomes exceed some harm severity thresholds.
    Undesired outcomes are truly harmful in a human sense and their effect is felt
    in near real-time. These outcomes can be classified as safety issues. The cost
    of deep learning decisions is related to optimization formulations which explicitly
    include a loss function $L$. The loss function $L:X\times Y\times Y\textrightarrow\rightarrow
    R$ is defined as the measure of the error incurred by predicting the label of
    an observation $x$ as $f(x)$, instead of $y$. Statistical learning calls the risk
    of $f$ as the expected value of the loss of $f$ under $P$:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 分析深度学习组件安全性的关键要求是检查结果的即时人类成本是否超过某些伤害严重性阈值。不希望的结果在人类意义上确实是有害的，并且其影响在近实时中被感受到。这些结果可以被归类为安全问题。深度学习决策的成本与明确包含损失函数
    $L$ 的优化公式有关。损失函数 $L:X\times Y\times Y\textrightarrow\rightarrow R$ 被定义为预测观察值 $x$
    的标签为 $f(x)$ 而不是 $y$ 所产生的错误的度量。统计学习将 $f$ 的风险称为 $f$ 在 $P$ 下的损失的期望值：
- en: '|  | $R(f)=\int L(x,f(x),y)dP(x,y),$ |  | (21) |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(f)=\int L(x,f(x),y)dP(x,y),$ |  | (21) |'
- en: 'where, $X\times Y$ is a random example space of observations $x$ and labels
    $y$, distributed according to a probability distribution $P(X,Y)$. The statistical
    learning problem consists of finding the function $f$ that optimizes (i.e. minimizes)
    the risk $R$ [[133](#bib.bib133)]. For an algorithm’s hypothesis $h$ and loss
    function $L$, the expected loss on the training set is called the empirical risk
    of $h$:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$X\times Y$ 是一个观察值 $x$ 和标签 $y$ 的随机示例空间，按照概率分布 $P(X,Y)$ 分布。统计学习问题的核心是找到优化（即最小化）风险
    $R$ 的函数 $f$ [[133](#bib.bib133)]。对于一个算法的假设 $h$ 和损失函数 $L$，训练集上的期望损失称为 $h$ 的经验风险：
- en: '|  | $\mathbf{R}_{emp}(h)=\frac{1}{m}\sum\limits_{i=1}^{m}L(x^{(i)},h(x)^{(i)},y^{(i)}).$
    |  | (22) |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{R}_{emp}(h)=\frac{1}{m}\sum\limits_{i=1}^{m}L(x^{(i)},h(x)^{(i)},y^{(i)}).$
    |  | (22) |'
- en: A machine learning algorithm then optimizes the empirical risk on the expectation
    that the risk decreases significantly. However, this standard formulation does
    not consider the issues related to the uncertainty that is relevant for safety.
    The distribution of the training samples ${(x_{1},y_{1}),...,(x_{m},y_{m})}$ is
    drawn from the true underlying probability distribution of $(X,Y)$, which may
    not always be the case. Usually the probability distribution is unknown, precluding
    the use of domain adaptation techniques [[134](#bib.bib134)] [[135](#bib.bib135)].
    This is one of the epistemic uncertainty that is relevant for safety because training
    on a dataset of different distribution can cause much harm through bias.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，机器学习算法优化经验风险，以期望风险显著减少。然而，这一标准公式并未考虑与安全相关的不确定性问题。训练样本 ${(x_{1},y_{1}),...,(x_{m},y_{m})}$
    的分布来自 $(X,Y)$ 的真实基础概率分布，这可能并非总是如此。通常概率分布是未知的，排除了领域自适应技术的使用 [[134](#bib.bib134)]
    [[135](#bib.bib135)]。这是与安全相关的一个认识不确定性，因为在不同分布的数据集上进行训练可能通过偏差造成很大伤害。
- en: In reality, a machine learning system only encounters a finite number of test
    samples and an actual operational risk is an empirical quantity on the test set.
    The operational risk may be much larger than the actual risk for small cardinality
    test sets, even if $h$ is risk-optimal. This uncertainty caused by the instantiation
    of the test set can have large safety implications on individual test samples [[136](#bib.bib136)].
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，机器学习系统只遇到有限数量的测试样本，实际操作风险是测试集上的经验量。对于小基数测试集，即使 $h$ 是风险最优的，操作风险也可能远大于实际风险。测试集实例化所造成的不确定性可能对单个测试样本产生重大安全影响
    [[136](#bib.bib136)]。
- en: Faults and failures of a programmed component (e.g. one using a formal algorithm
    to solve a problem) are totally different from the ones of a deep learning component.
    Specific faults of a deep learning component can be caused by unreliable or noisy
    sensor signals (video signal due to bad weather, radar signal due to absorbing
    construction materials, GPS data, etc.), neural network topology, learning algorithm,
    training set or unexpected changes in the environment (e.g. unknown driving scenes
    or accidents on the road). We must mention the first autonomous driving accident,
    produced by a Tesla^® car, where, due to object misclassification errors, the
    AutoPilot function collided the vehicle into a truck [[137](#bib.bib137)]. Despite
    the 130 million miles of testing and evaluation, the accident was caused under
    extremely rare circumstances, also known as Black Swans, given the height of the
    truck, its white color under bright sky, combined with the positioning of the
    vehicle across the road.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 编程组件（例如使用正式算法解决问题的组件）的故障与深度学习组件的故障完全不同。深度学习组件的特定故障可能由不可靠或噪声传感器信号（由于恶劣天气的视频信号、由于吸收性建筑材料的雷达信号、GPS数据等）、神经网络拓扑、学习算法、训练集或环境的意外变化（例如未知的驾驶场景或路上的事故）引起。我们必须提到第一个自动驾驶事故，由特斯拉^®汽车造成，由于对象误分类错误，自动驾驶功能将车辆撞上了卡车[[137](#bib.bib137)]。尽管经过了1.3亿英里的测试和评估，但事故发生在极其罕见的情况下，也称为黑天鹅事件，这与卡车的高度、在明亮天空下的白色相结合，以及车辆在路上的位置有关。
- en: Self-driving vehicles must have fail-safe mechanisms, usually encountered under
    the name of Safety Monitors. These must stop the autonomous control software once
    a failure is detected [[138](#bib.bib138)]. Specific fault types and failures
    have been cataloged for neural networks in [[139](#bib.bib139)], [[140](#bib.bib140)]
    and [[141](#bib.bib141)]. This led to the development of specific and focused
    tools and techniques to help finding faults. [[142](#bib.bib142)] describes a
    technique for debugging misclassifications due to bad training data, while an
    approach for troubleshooting faults due to complex interactions between linked
    machine learning components is proposed in [[143](#bib.bib143)]. In [[144](#bib.bib144)],
    a white box technique is used to inject faults onto a neural network by breaking
    the links or randomly changing the weights.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 自驾车辆必须具备故障安全机制，通常被称为安全监控器。这些监控器必须在检测到故障时停止自动控制软件[[138](#bib.bib138)]。神经网络的特定故障类型和失败已在[[139](#bib.bib139)]、[[140](#bib.bib140)]和[[141](#bib.bib141)]中进行了分类。这导致了针对性强的工具和技术的开发，以帮助发现故障。[[142](#bib.bib142)]描述了一种调试由于训练数据不良导致的误分类的技术，而[[143](#bib.bib143)]中提出了一种解决由于链接的机器学习组件之间复杂交互引发的故障的方法。在[[144](#bib.bib144)]中，采用了一种白盒技术，通过断开连接或随机改变权重来注入故障。
- en: The training set plays a key role in the safety of the deep learning component.
    ISO 26262 standard states that the component behavior shall be fully specified
    and each refinement shall be verified with respect to its specification. This
    assumption is violated in the case of a deep learning system, where a training
    set is used instead of a specification. It is not clear how to ensure that the
    corresponding hazards are always mitigated. The training process is not a verification
    process since the trained model will be “correct by construction” with respect
    to the training set, up to the limits of the model and the learning algorithm [[130](#bib.bib130)].
    Effects of this considerations are visible in the commercial autonomous vehicle
    market, where Black Swan events caused by data not present in the training set
    may lead to fatalities [[141](#bib.bib141)].
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集在深度学习组件的安全性中发挥着关键作用。ISO 26262标准规定，组件行为应完全规范化，每一次细化都应根据其规范进行验证。在深度学习系统中，这一假设被违背，因为使用了训练集而不是规范。目前还不清楚如何确保相应的危险始终得到缓解。训练过程并不是验证过程，因为训练后的模型将会在训练集的范围内“按构造正确”，但这受到模型和学习算法的限制[[130](#bib.bib130)]。这种考虑的影响在商业自动驾驶车辆市场中显而易见，数据中未出现的黑天鹅事件可能导致致命事故[[141](#bib.bib141)]。
- en: Detailed requirements shall be formulated and traced to hazards. Such a requirement
    can specify how the training, validation and testing sets are obtained. Subsequently,
    the data gathered can be verified with respect to this specification. Furthermore,
    some specifications, for example the fact that a vehicle cannot be wider than
    3 meters, can be used to reject false positive detections. Such properties are
    used even directly during the training process to improve the accuracy of the
    model [[145](#bib.bib145)].
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 需要制定详细要求并追溯到潜在危险。这样的要求可以指定如何获取训练、验证和测试集。随后，可以根据这些规范验证收集的数据。此外，一些规范，例如车辆宽度不能超过3米，可以用来排除假阳性检测。这样的属性甚至在训练过程中直接用于提高模型的准确性
    [[145](#bib.bib145)]。
- en: Machine learning and deep learning techniques are starting to become effective
    and reliable even for safety critical systems, even if the complete safety assurance
    for this type of systems is still an open question. Current standards and regulation
    from the automotive industry cannot be fully mapped to such systems, requiring
    the development of new safety standards targeted for deep learning.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习技术开始变得有效和可靠，即使对于安全关键系统来说，完全的安全保证仍然是一个悬而未决的问题。当前的汽车行业标准和法规无法完全适用于此类系统，因此需要开发新的深度学习安全标准。
- en: 8 Data Sources for Training Autonomous Driving Systems
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 自驾系统的训练数据来源
- en: Undeniably, the usage of real world data is a key requirement for training and
    testing an autonomous driving component. The high amount of data needed in the
    development stage of such components made data collection on public roads a valuable
    activity. In order to obtain a comprehensive description of the driving scene,
    the vehicle used for data collection is equipped with a variety of sensors such
    as radar, LIDAR, GPS, cameras, Inertial Measurement Units (IMU) and ultrasonic
    sensors. The sensors setup differs from vehicle to vehicle, depending on how the
    data is planned to be used. A common sensor setup for an autonomous vehicle is
    presented in Fig. [7](#S8.F7 "Figure 7 ‣ 8 Data Sources for Training Autonomous
    Driving Systems ‣ A Survey of Deep Learning Techniques for Autonomous Driving").
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 不可否认，使用真实世界数据是训练和测试自动驾驶组件的关键要求。开发阶段所需的大量数据使得在公共道路上收集数据成为一项有价值的活动。为了获得对驾驶场景的全面描述，用于数据收集的车辆配备了各种传感器，如雷达、LIDAR、GPS、摄像头、惯性测量单元（IMU）和超声波传感器。传感器设置因车辆而异，具体取决于数据的使用方式。图[7](#S8.F7
    "图 7 ‣ 8 自驾系统的训练数据来源 ‣ 自动驾驶深度学习技术的调查")展示了自动驾驶车辆的常见传感器设置。
- en: '![Refer to caption](img/70619ce1a90cdcb899377e8dd73e5428.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70619ce1a90cdcb899377e8dd73e5428.png)'
- en: 'Figure 7: Sensor suite of the nuTonomy^® self-driving car [[146](#bib.bib146)].'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: nuTonomy^® 自驾车的传感器套件 [[146](#bib.bib146)]。'
- en: In the last years, mainly due to the large and increasing research interest
    in autonomous vehicles, many driving datasets were made public and documented.
    They vary in size, sensor setup and data format. The researchers need only to
    identify the proper dataset which best fits their problem space. [[29](#bib.bib29)]
    published a survey on a broad spectrum of datasets. These datasets address the
    computer vision field in general, but there are few of them which fit to the autonomous
    driving topic.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，主要由于对自动驾驶车辆的广泛和不断增加的研究兴趣，许多驾驶数据集已被公开并记录。它们在大小、传感器设置和数据格式上有所不同。研究人员只需识别最适合其问题空间的适当数据集。[[29](#bib.bib29)]
    发布了关于广泛数据集的调查。这些数据集通常涉及计算机视觉领域，但适用于自动驾驶主题的数据集较少。
- en: A most comprehensive survey on publicly available datasets for self-driving
    vehicles algorithms can be found in [[147](#bib.bib147)]. The paper presents 27
    available datasets containing data recorded on public roads. The datasets are
    compared from different perspectives, such that the reader can select the one
    best suited for his task.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自驾车辆算法的公共数据集的最全面调查可以在[[147](#bib.bib147)]中找到。该论文介绍了27个包含在公共道路上记录的数据的数据集。这些数据集从不同的角度进行比较，以便读者可以选择最适合其任务的数据集。
- en: Despite our extensive search, we are yet to find a master dataset that combines
    at least parts of the ones available. The reason may be that there are no standard
    requirements for the data format and sensor setup. Each dataset heavily depends
    on the objective of the algorithm for which the data was collected. Recently,
    the companies Scale^® and nuTonomy^® started to create one of the largest and
    most detailed self-driving dataset on the market to date⁶⁶6[https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/](https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/).
    This includes Berkeley DeepDrive [[148](#bib.bib148)], a dataset developed by
    researchers at Berkeley University. More relevant datasets from the literature
    are pending for merging⁷⁷7[https://scale.com/open-datasets](https://scale.com/open-datasets).
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们进行了广泛的搜索，但仍未找到一个结合至少部分现有数据集的主数据集。原因可能在于数据格式和传感器设置没有标准要求。每个数据集都严重依赖于数据收集的算法目标。最近，Scale^®
    和 nuTonomy^® 公司开始创建迄今为止市场上最大且最详细的自动驾驶数据集⁶⁶6[https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/](https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/)。这包括伯克利深度驾驶[[148](#bib.bib148)]，这是伯克利大学研究人员开发的数据集。文献中更多相关数据集正在等待合并⁷⁷7[https://scale.com/open-datasets](https://scale.com/open-datasets)。
- en: In [[120](#bib.bib120)], the authors present a study that seeks to collect and
    analyze large scale naturalistic data of semi-autonomous driving in order to better
    characterize the state of the art of the current technology. The study involved
    $99$ participants, $29$ vehicles, $405,807$ miles and approximatively $5.5$ billion
    video frames. Unfortunately, the data collected in this study is not available
    for the public.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[120](#bib.bib120)]中，作者介绍了一项旨在收集和分析大规模自然驾驶数据的研究，以更好地表征当前技术的最前沿。该研究涉及99名参与者，29辆车，405,807
    英里，和大约55亿帧视频。不幸的是，这项研究收集的数据对公众不可用。
- en: In the remaining of this section we will provide and highlight the distinctive
    characteristics of the most relevant datasets that are publicly available.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将提供并突出介绍最相关的公开数据集的独特特征。
- en: '| Dataset | Problem Space | Sensor setup | Size | Location |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 问题空间 | 传感器设置 | 大小 | 位置 |'
- en: '&#124; Traffic &#124;'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通 &#124;'
- en: '&#124; condition &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 条件 &#124;'
- en: '| License |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 许可 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; NuScenes &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NuScenes &#124;'
- en: '&#124; [[146](#bib.bib146)] &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[146](#bib.bib146)] &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D tracking, &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 跟踪, &#124;'
- en: '&#124; 3D object &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 目标 &#124;'
- en: '&#124; detection &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '|'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Radar, Lidar, &#124;'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 雷达, 激光雷达, &#124;'
- en: '&#124; EgoData, GPS, &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EgoData, GPS, &#124;'
- en: '&#124; IMU, Camera &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IMU, 摄像头 &#124;'
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 345 GB &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 345 GB &#124;'
- en: '&#124; (1000 scenes, clips of 20s) &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (1000 场景, 每段 20 秒) &#124;'
- en: '|'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Boston, &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 波士顿, &#124;'
- en: '&#124; Singapore &#124;'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新加坡 &#124;'
- en: '| Urban | CC BY-NC-SA 3.0 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | CC BY-NC-SA 3.0 |'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AMUSE &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AMUSE &#124;'
- en: '&#124; [[149](#bib.bib149)] &#124;'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[149](#bib.bib149)] &#124;'
- en: '| SLAM |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| SLAM |'
- en: '&#124; Omnidirectional &#124;'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全景 &#124;'
- en: '&#124; camera, IMU, &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头, IMU, &#124;'
- en: '&#124; EgoData, GPS &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EgoData, GPS &#124;'
- en: '|'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 1 TB &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1 TB &#124;'
- en: '&#124; (7 clips) &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (7 个片段) &#124;'
- en: '| Los Angeles | Urban | CC BY-NC-ND 3.0 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 洛杉矶 | 城市 | CC BY-NC-ND 3.0 |'
- en: '|'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Ford &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 福特 &#124;'
- en: '&#124; [[150](#bib.bib150)] &#124;'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[150](#bib.bib150)] &#124;'
- en: '|'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D tracking, &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 跟踪, &#124;'
- en: '&#124; 3D object detection &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 目标检测 &#124;'
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Omnidirectional &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全景 &#124;'
- en: '&#124; camera, IMU, &#124;'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头, IMU, &#124;'
- en: '&#124; Lidar, GPS &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激光雷达, GPS &#124;'
- en: '| 100 GB | Michigan | Urban | Not specified |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 100 GB | 密歇根州 | 城市 | 未指定 |'
- en: '|'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; KITTI &#124;'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; KITTI &#124;'
- en: '&#124; [[151](#bib.bib151)] &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[151](#bib.bib151)] &#124;'
- en: '|'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D tracking, &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 跟踪, &#124;'
- en: '&#124; 3D object detection, &#124;'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 目标检测, &#124;'
- en: '&#124; SLAM &#124;'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SLAM &#124;'
- en: '|'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Monocular &#124;'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; cameras, IMU &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头, IMU &#124;'
- en: '&#124; Lidar, GPS &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激光雷达, GPS &#124;'
- en: '| 180 GB | Karlsruhe |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| 180 GB | 卡尔斯鲁厄 |'
- en: '&#124; Urban &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 城市 &#124;'
- en: '&#124; Rural &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 乡村 &#124;'
- en: '| CC BY-NC-SA 3.0 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| CC BY-NC-SA 3.0 |'
- en: '|'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Udacity &#124;'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Udacity &#124;'
- en: '&#124; [[152](#bib.bib152)] &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[152](#bib.bib152)] &#124;'
- en: '|'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D tracking, &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 跟踪, &#124;'
- en: '&#124; 3D object detection &#124;'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 目标检测 &#124;'
- en: '|'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Monocular &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; cameras, IMU, &#124;'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头，IMU，&#124;'
- en: '&#124; Lidar, GPS, &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激光雷达，GPS，&#124;'
- en: '&#124; EgoData &#124;'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EgoData &#124;'
- en: '| 220 GB | Mountain View | Rural | MIT |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| 220 GB | Mountain View | 郊区 | MIT |'
- en: '|'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cityscapes &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Cityscapes &#124;'
- en: '&#124; [[74](#bib.bib74)] &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[74](#bib.bib74)] &#124;'
- en: '|'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Semantic &#124;'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语义 &#124;'
- en: '&#124; understanding &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理解 &#124;'
- en: '|'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Color stereo &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 彩色立体 &#124;'
- en: '&#124; cameras &#124;'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头 &#124;'
- en: '|'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 63 GB &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 63 GB &#124;'
- en: '&#124; (5 clips) &#124;'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (5 个剪辑) &#124;'
- en: '|'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Darmstadt, &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 达姆施塔特，&#124;'
- en: '&#124; Zurich, &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 苏黎世，&#124;'
- en: '&#124; Strasbourg &#124;'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 斯特拉斯堡 &#124;'
- en: '| Urban | CC BY-NC-SA 3.0 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | CC BY-NC-SA 3.0 |'
- en: '|'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Oxford &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 牛津 &#124;'
- en: '&#124; [[153](#bib.bib153)] &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[153](#bib.bib153)] &#124;'
- en: '|'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3D tracking, &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 跟踪，&#124;'
- en: '&#124; 3D object detection, &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3D 目标检测，&#124;'
- en: '&#124; SLAM &#124;'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SLAM &#124;'
- en: '|'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Stereo and &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立体和 &#124;'
- en: '&#124; monocular &#124;'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; cameras, GPS &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头，GPS &#124;'
- en: '&#124; Lidar, IMU &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 激光雷达，IMU &#124;'
- en: '|'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 23 TB &#124;'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 23 TB &#124;'
- en: '&#124; (133 clips) &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (133 个剪辑) &#124;'
- en: '| Oxford |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 牛津 |'
- en: '&#124; Urban, &#124;'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 城市，&#124;'
- en: '&#124; Highway &#124;'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高速公路 &#124;'
- en: '| CC BY-NC-SA 3.0 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| CC BY-NC-SA 3.0 |'
- en: '|'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CamVid &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CamVid &#124;'
- en: '&#124; [[154](#bib.bib154)] &#124;'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[154](#bib.bib154)] &#124;'
- en: '|'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Object detection, &#124;'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测，&#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Monocular &#124;'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; color &#124;'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 彩色 &#124;'
- en: '&#124; camera &#124;'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头 &#124;'
- en: '|'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 8 GB &#124;'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 8 GB &#124;'
- en: '&#124; (4 clips) &#124;'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (4 个剪辑) &#124;'
- en: '| Cambridge | Urban | N/A |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| 剑桥 | 城市 | 不适用 |'
- en: '|'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Daimler &#124;'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 戴姆勒 &#124;'
- en: '&#124; pedestrian &#124;'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行人 &#124;'
- en: '&#124; [[155](#bib.bib155)] &#124;'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[155](#bib.bib155)] &#124;'
- en: '|'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pedestrian detection, &#124;'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行人检测，&#124;'
- en: '&#124; Classification, &#124;'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类，&#124;'
- en: '&#124; Segmentation, &#124;'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割，&#124;'
- en: '&#124; Path prediction &#124;'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路径预测 &#124;'
- en: '|'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Stereo and &#124;'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立体和 &#124;'
- en: '&#124; monocular &#124;'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; cameras &#124;'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头 &#124;'
- en: '|'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 91 GB &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 91 GB &#124;'
- en: '&#124; (8 clips) &#124;'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (8 个剪辑) &#124;'
- en: '|'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Amsterdam, &#124;'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 阿姆斯特丹，&#124;'
- en: '&#124; Beijing &#124;'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 北京 &#124;'
- en: '| Urban | N/A |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | 不适用 |'
- en: '|'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Caltech &#124;'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 加州理工学院 &#124;'
- en: '&#124; [[156](#bib.bib156)] &#124;'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[156](#bib.bib156)] &#124;'
- en: '|'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Tracking, &#124;'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跟踪，&#124;'
- en: '&#124; Segmentation, &#124;'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割，&#124;'
- en: '&#124; Object detection &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目标检测 &#124;'
- en: '|'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Monocular &#124;'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 单目 &#124;'
- en: '&#124; camera &#124;'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 摄像头 &#124;'
- en: '| 11 GB |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| 11 GB |'
- en: '&#124; Los Angeles &#124;'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 洛杉矶 &#124;'
- en: '&#124; (USA) &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (美国) &#124;'
- en: '| Urban | N/A |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 城市 | 不适用 |'
- en: 'Table 2: Summary of datasets for training autonomous driving systems'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 自动驾驶系统训练数据集总结'
- en: KITTI Vision Benchmark dataset (KITTI) [[151](#bib.bib151)]. Provided by the
    Karlsruhe Institute of Technology (KIT) from Germany, this dataset fits the challenges
    of benchmarking stereo-vision, optical flow, 3D tracking, 3D object detection
    or SLAM algorithms. It is known as the most prestigious dataset in the self-driving
    vehicles domain. To this date it counts more than 2000 citations in the literature.
    The data collection vehicle is equipped with multiple high-resolution color and
    gray-scale stereo cameras, a Velodyne 3D LiDAR and high-precision GPS/IMU sensors.
    In total, it provides 6 hours of driving data collected in both rural and highway
    traffic scenarios around Karlsruhe. The dataset is provided under the Creative
    Commons Attribution-NonCommercial-ShareAlike 3.0 License.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI 视觉基准数据集 (KITTI) [[151](#bib.bib151)]。该数据集由德国卡尔斯鲁厄理工学院 (KIT) 提供，适用于立体视觉、光流、3D
    跟踪、3D 目标检测或 SLAM 算法的基准测试挑战。它被认为是自动驾驶领域中最有声望的数据集。至今，它在文献中已有超过 2000 次引用。数据采集车辆配备了多个高分辨率彩色和灰度立体摄像头、一个
    Velodyne 3D 激光雷达以及高精度 GPS/IMU 传感器。总计提供了 6 小时的驾驶数据，涵盖了卡尔斯鲁厄周边的乡村和高速公路交通场景。数据集以
    Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License 许可证提供。
- en: NuScenes dataset [[146](#bib.bib146)]. Constructed by nuTonomy, this dataset
    contains 1000 driving scenes collected from Boston and Singapore, two known for
    their dense traffic and highly challenging driving situations. In order to facilitate
    common computer vision tasks, such as object detection and tracking, the providers
    annotated 25 object classes with accurate 3D bounding boxes at 2Hz over the entire
    dataset. Collection of vehicle data is still in progress. The final dataset will
    include approximately 1,4 million camera images, 400.000 Lidar sweeps, 1,3 million
    RADAR sweeps and 1,1 million object bounding boxes in 40.000 keyframes. The dataset
    is provided under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0
    License license.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: NuScenes 数据集 [[146](#bib.bib146)]。由 nuTonomy 构建，该数据集包含从波士顿和新加坡收集的 1000 个驾驶场景，这两个城市以其密集的交通和高度具有挑战性的驾驶情况而闻名。为了促进常见的计算机视觉任务，如对象检测和跟踪，提供者在整个数据集上以
    2Hz 的频率对 25 类对象进行了准确的 3D 边界框标注。车辆数据的收集仍在进行中。最终数据集将包括约 140 万张摄像头图像、40 万次激光雷达扫描、130
    万次雷达扫描和 110 万个对象边界框，分布在 40,000 个关键帧中。该数据集根据创作共用署名-非商业性使用-相同方式共享 3.0 许可证提供。
- en: Automotive multi-sensor dataset (AMUSE) [[149](#bib.bib149)]. Provided by Linköping
    University of Sweden, it consists of sequences recorded in various environments
    from a car equipped with an omnidirectional multi-camera, height sensors, an IMU,
    a velocity sensor and a GPS. The API for reading these data sets is provided to
    the public, together with a collection of long multi-sensor and multi-camera data
    streams stored in the given format. The dataset is provided under the Creative
    Commons Attribution-NonCommercial-NoDerivs 3.0 Unsupported License.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车多传感器数据集（AMUSE） [[149](#bib.bib149)]。由瑞典 Linköping 大学提供，包含在各种环境中从配备全向多摄像头、高度传感器、IMU、速度传感器和
    GPS 的汽车上记录的序列。公共提供了读取这些数据集的 API，以及以给定格式存储的长时间多传感器和多摄像头数据流集合。该数据集根据创作共用署名-非商业性使用-禁止演绎
    3.0 未支持许可证提供。
- en: Ford campus vision and lidar dataset (Ford) [[150](#bib.bib150)]. Provided by
    University of Michigan, this dataset was collected using a Ford F250 pickup truck
    equipped with professional (Applanix POS-LV) and a consumer (Xsens MTi-G) inertial
    measurement units (IMU), a Velodyne Lidar scanner, two push-broom forward looking
    Riegl Lidars and a Point Grey Ladybug3 omnidirectional camera system. The approx.
    100 GB of data was recorded around the Ford Research campus and downtown Dearborn,
    Michigan in 2009\. The dataset is well suited to test various autonomous driving
    and simultaneous localization and mapping (SLAM) algorithms.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: Ford 校园视觉和激光雷达数据集（Ford） [[150](#bib.bib150)]。由密歇根大学提供，该数据集使用装备了专业（Applanix POS-LV）和消费级（Xsens
    MTi-G）惯性测量单元（IMU）、Velodyne Lidar 扫描仪、两个推扫式前视 Riegl Lidar 和一个 Point Grey Ladybug3
    全向摄像系统的 Ford F250 皮卡车收集。约 100 GB 的数据在 2009 年记录于 Ford Research 校园和密歇根州底特律市区。该数据集非常适合测试各种自动驾驶和同时定位与地图构建（SLAM）算法。
- en: Udacity dataset [[152](#bib.bib152)]. The vehicle sensor setup contains monocular
    color cameras, GPS and IMU sensors, as well as a Velodyne 3D Lidar. The size of
    the dataset is 223GB. The data is labeled and the user is provided with the corresponding
    steering angle that was recorded during the test runs by the human driver.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: Udacity 数据集 [[152](#bib.bib152)]。该车辆传感器配置包含单目彩色摄像头、GPS 和 IMU 传感器，以及一个 Velodyne
    3D Lidar。数据集的大小为 223GB。数据已被标注，并且用户可以获得在测试过程中由人类驾驶员记录的相应转向角度。
- en: 'Cityscapes dataset[[74](#bib.bib74)]. Provided by Daimler AG R&D, Germany;
    Max Planck Institute for Informatics (MPI-IS), Germany, TU Darmstadt Visual Inference
    Group, Germany, the Cityscapes Dataset focuses on semantic understanding of urban
    street scenes, this being the reason for which it contains only stereo vision
    color images. The diversity of the images is very large: 50 cities, different
    seasons (spring, summer, fall), various weather conditions and different scene
    dynamics. There are 5000 images with fine annotations and 20000 images with coarse
    annotations. Two important challenges have used this dataset for benchmarking
    the development of algorithms for semantic segmentation [[157](#bib.bib157)] and
    instance segmentation [[158](#bib.bib158)].'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: Cityscapes数据集[[74](#bib.bib74)]。由Daimler AG研发，德国；马克斯·普朗克计算机科学研究所（MPI-IS），德国，达姆施塔特工业大学视觉推理组，德国提供，Cityscapes数据集专注于城市街道场景的语义理解，这也是它仅包含立体视觉彩色图像的原因。图像的多样性非常大：50个城市、不同季节（春天、夏天、秋天）、各种天气条件和不同的场景动态。数据集中包含5000张精细标注的图像和20000张粗略标注的图像。两个重要挑战使用了该数据集来基准测试语义分割[[157](#bib.bib157)]和实例分割[[158](#bib.bib158)]算法的开发。
- en: The Oxford dataset [[153](#bib.bib153)]. Provided by Oxford University, UK,
    the dataset collection spanned over 1 year, resulting in over 1000 km of recorded
    driving with almost 20 million images collected from 6 cameras mounted to the
    vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all
    weather conditions, including heavy rain, night, direct sunlight and snow. One
    of the particularities of this dataset is that the vehicle frequently drove the
    same route over the period of a year to enable researchers to investigate long-term
    localization and mapping for autonomous vehicles in real-world, dynamic urban
    environments.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津数据集[[153](#bib.bib153)]。由牛津大学，英国提供，数据集的收集时间跨度超过1年，记录了超过1000公里的驾驶，并从6个安装在车辆上的摄像头收集了近2000万张图像，以及LIDAR、GPS和INS的真实数据。数据在各种天气条件下收集，包括暴雨、夜晚、直射阳光和雪。该数据集的一个特别之处是车辆在一年期间经常沿着相同的路线行驶，以便研究人员调查真实世界动态城市环境中的长期定位和映射。
- en: The Cambridge-driving Labeled Video Dataset (CamVid) [[154](#bib.bib154)]. Provided
    by the University of Cambridge, UK, it is one of the most cited dataset from the
    literature and the first released publicly, containing a collection of videos
    with object class semantic labels, along with metadata annotations. The database
    provides ground truth labels that associate each pixel with one of 32 semantic
    classes. The sensor setup is based on only one monocular camera mounted on the
    dashboard of the vehicle. The complexity of the scenes is quite low, the vehicle
    being driven only in urban areas with relatively low traffic and good weather
    conditions.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 剑桥驾驶标注视频数据集（CamVid）[[154](#bib.bib154)]。由剑桥大学，英国提供，它是文献中引用最多的数据集之一，也是首个公开发布的数据集，包含带有对象类别语义标签的视频集合，以及元数据注释。该数据库提供了将每个像素与32个语义类别之一关联的真实标签。传感器设置仅基于安装在车辆仪表盘上的单个单目摄像头。场景的复杂性较低，车辆仅在相对低交通量和良好天气条件的城市区域行驶。
- en: The Daimler pedestrian benchmark dataset [[155](#bib.bib155)]. Provided by Daimler
    AG R&D and University of Amsterdam, this dataset fits the topics of pedestrian
    detection, classification, segmentation and path prediction. Pedestrian data is
    observed from a traffic vehicle by using only on-board mono and stereo cameras.
    It is the first dataset with contains pedestrians. Recently, the dataset was extended
    with cyclist video samples captured with the same setup [[159](#bib.bib159)].
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: Daimler行人基准数据集[[155](#bib.bib155)]。由Daimler AG研发和阿姆斯特丹大学提供，这个数据集涉及行人检测、分类、分割和路径预测等主题。行人数据通过交通车辆上仅使用单目和立体摄像头进行观察。这是第一个包含行人的数据集。最近，该数据集增加了用相同设备捕捉的骑行者视频样本[[159](#bib.bib159)]。
- en: Caltech pedestrian detection dataset (Caltech) [[156](#bib.bib156)]. Provided
    by California Institute of Technology, US, the dataset contains richly annotated
    videos, recorded from a moving vehicle, with challenging images of low resolution
    and frequently occluded people. There are approx. 10 hours of driving scenarios
    cumulating about 250.000 frames with a total of 350 thousand bounding boxes and
    2.300 unique pedestrians annotations. The annotations include both temporal correspondences
    between bounding boxes and detailed occlusion labels.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 加州理工学院行人检测数据集（Caltech）[[156](#bib.bib156)]。由美国加州理工学院提供，该数据集包含丰富注释的视频，这些视频是在移动车辆上录制的，具有低分辨率和经常遮挡的挑战性图像。大约有
    10 小时的驾驶场景，累计约 250,000 帧，共有 350,000 个边界框和 2,300 个独特的行人注释。这些注释包括边界框之间的时间对应关系和详细的遮挡标签。
- en: Given the variety and complexity of the available databases, choosing one or
    more to develop and test an autonomous driving component may be difficult. As
    it can be observed, the sensor setup varies among all the available databases.
    For localization and vehicle motion, the Lidar and GPS/IMU sensors are necessary,
    with the most popular Lidar sensors used being Velodyne [[160](#bib.bib160)] and
    Sick [[161](#bib.bib161)]. Data recorded from a radar sensor is present only in
    the NuScenes dataset. The radar manufacturers adopt proprietary data formats which
    are not public. Almost all available datasets include images captured from a video
    camera, while there is a balance use of monocular and stereo cameras mainly configured
    to capture gray-scale images. AMUSE and Ford databases are the only ones that
    use omnidirectional cameras.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于可用数据库的多样性和复杂性，选择一个或多个来开发和测试自动驾驶组件可能会很困难。可以观察到，所有可用数据库中的传感器设置各不相同。对于定位和车辆运动，Lidar
    和 GPS/IMU 传感器是必要的，其中最受欢迎的 Lidar 传感器是 Velodyne [[160](#bib.bib160)] 和 Sick [[161](#bib.bib161)]。雷达传感器记录的数据仅出现在
    NuScenes 数据集中。雷达制造商采用的专有数据格式并不公开。几乎所有可用的数据集都包括从视频摄像机捕获的图像，而单目和立体相机的使用比较平衡，主要配置为捕获灰度图像。AMUSE
    和 Ford 数据库是唯一使用全景相机的数据集。
- en: 'Besides raw recorded data, the datasets usually contain miscellaneous files
    such as annotations, calibration files, labels, etc. In order to cope with this
    files, the dataset provider must offer tools and software that enable the user
    to read and post-process the data. Splitting of the datasets is also an important
    factor to consider, because some of the datasets (e.g. Caltech, Daimler, Cityscapes)
    already provide pre-processed data that is classified in different sets: training,
    testing and validation. This enables benchmarking of desired algorithms against
    similar approaches to be consistent.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始记录数据外，这些数据集通常还包含各种文件，如注释、校准文件、标签等。为了处理这些文件，数据集提供者必须提供能够读取和后处理数据的工具和软件。数据集的拆分也是一个重要因素，因为一些数据集（例如
    Caltech、Daimler、Cityscapes）已经提供了按不同集合分类的预处理数据：训练、测试和验证。这使得所需算法的基准测试与类似方法的一致性得以保持。
- en: Another aspect to consider is the license type. The most commonly used license
    is Creative Commons Attribution-NonCommercial-ShareAlike 3.0\. It allows the user
    to copy and redistribute in any medium or format and also to remix, transform,
    and build upon the material. KITTI and NuScenes databases are examples of such
    distribution license. The Oxford database uses a Creative Commons Attribution-Noncommercial
    4.0\. which, compared with the first license type, does not force the user to
    distribute his contributions under the same license as the database. Opposite
    to that, the AMUSE database is licensed under Creative Commons Attribution-Noncommercial-noDerivs
    3.0 which makes the database illegal to distribute if modification of the material
    are made.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的方面是许可证类型。最常用的许可证是 Creative Commons Attribution-NonCommercial-ShareAlike
    3.0\。它允许用户以任何媒介或格式复制和再分发，也允许对材料进行 remix、变换和扩展。KITTI 和 NuScenes 数据库就是这种分发许可证的例子。牛津数据库使用
    Creative Commons Attribution-Noncommercial 4.0\，与第一个许可证类型相比，它不强制用户在相同许可证下分发其贡献。相反，AMUSE
    数据库的许可证是 Creative Commons Attribution-Noncommercial-noDerivs 3.0，这使得如果对材料进行了修改，该数据库将无法分发。
- en: With very few exceptions, the datasets are collected from a single city, which
    is usually around university campuses or company locations in Europe, the US,
    or Asia. Germany is the most active country for driving recording vehicles. Unfortunately,
    all available datasets together cover a very small portion of the world map. One
    reason for this is the memory size of the data which is in direct relation with
    the sensor setup and the quality. For example, the Ford dataset takes around 30
    GB for each driven kilometer, which means that covering an entire city will take
    hundreds of TeraBytes of driving data. The majority of the available datasets
    consider sunny, daylight and urban conditions, these being ideal operating conditions
    for autonomous driving systems.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 除了极少数例外，数据集通常来自单一城市，这些城市通常位于欧洲、美国或亚洲的大学校园或公司所在地。德国是记录驾驶车辆最活跃的国家。不幸的是，所有可用的数据集加起来只覆盖了世界地图上的一小部分。一个原因是数据的内存大小与传感器设置和质量直接相关。例如，福特数据集每行驶一公里需要约30
    GB，这意味着覆盖整个城市将需要数百TB的驾驶数据。大多数可用数据集考虑了阳光明媚、白天和城市环境，这些都是自动驾驶系统的理想操作条件。
- en: 9 Computational Hardware and Deployment
  id: totrans-649
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 计算硬件与部署
- en: Deploying deep learning algorithms on target edge devices is not a trivial task.
    The main limitations when it comes to vehicles are the price, performance issues
    and power consumption. Therefore, embedded platforms are becoming essential for
    integration of AI algorithms inside vehicles due to their portability, versatility,
    and energy efficiency.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标边缘设备上部署深度学习算法并非易事。对于车辆来说，主要的限制因素是价格、性能问题和功耗。因此，嵌入式平台由于其便携性、多功能性和能源效率，正变得对车辆内的人工智能算法集成至关重要。
- en: The market leader in providing hardware solutions for deploying deep learning
    algorithms inside autonomous cars is NVIDIA^®. DRIVE PX [[162](#bib.bib162)] is
    an AI car computer which was designed to enable the auto-makers to focus directly
    on the software for autonomous vehicles.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供用于部署深度学习算法的硬件解决方案方面，市场领导者是 NVIDIA^®。DRIVE PX [[162](#bib.bib162)] 是一款人工智能车载计算机，旨在使汽车制造商能够直接专注于自动驾驶车辆的软件。
- en: The newest version of DrivePX architecture is based on two Tegra X2 [[163](#bib.bib163)]
    systems on a chip (SoCs). Each SoC contains two Denve [[164](#bib.bib164)] cores,
    4 ARM A57 cores and a graphical computeing unit (GPU) from the Pascal [[165](#bib.bib165)]
    generation. NVIDIA^® DRIVE PX is capable to perform real-time environment perception,
    path planning and localization. It combines deep learning, sensor fusion and surround
    vision to improve the driving experience.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: DrivePX 架构的最新版本基于两个 Tegra X2 [[163](#bib.bib163)] 系统芯片 (SoCs)。每个 SoC 包含两个 Denver
    [[164](#bib.bib164)] 核心、4 个 ARM A57 核心以及来自 Pascal [[165](#bib.bib165)] 代的图形计算单元
    (GPU)。NVIDIA^® DRIVE PX 能够进行实时环境感知、路径规划和定位。它结合了深度学习、传感器融合和全景视觉，以改善驾驶体验。
- en: Introduced in September 2018, NVIDIA^® DRIVE AGX developer kit platform was
    presented as the world’s most advanced self-driving car platform [[166](#bib.bib166)],
    being based on the Volta technology [[167](#bib.bib167)]. It is available in two
    different configurations, namely DRIVE AGX Xavier and DRIVE AGX Pegasus.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA^® DRIVE AGX 开发者套件平台于2018年9月推出，被呈现为世界上最先进的自动驾驶汽车平台 [[166](#bib.bib166)]，基于
    Volta 技术 [[167](#bib.bib167)]。它提供了两种不同的配置，即 DRIVE AGX Xavier 和 DRIVE AGX Pegasus。
- en: DRIVE AGX Xavier is a scalable open platform that can serve as an AI brain for
    self driving vehicles, and is an energy-efficient computing platform, with 30
    trillion operations per second, while meeting automotive standards like the ISO
    26262 functional safety specification. NVIDIA^® DRIVE AGX Pegasus improves the
    performance with an architecture which is built on two NVIDIA^® Xavier processors
    and two state of the art TensorCore GPUs.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: DRIVE AGX Xavier 是一个可扩展的开放平台，可以作为自动驾驶车辆的人工智能“大脑”，并且是一个节能的计算平台，具有每秒30万亿次操作，同时符合
    ISO 26262 功能安全规范等汽车标准。NVIDIA^® DRIVE AGX Pegasus 通过基于两个 NVIDIA^® Xavier 处理器和两个最先进的
    TensorCore GPU 的架构来提升性能。
- en: A hardware platform used by the car makers for Advanced Driver Assistance Systems
    (ADAS) is the R-Car V3H system-on-chip (SoC) platform from Renesas Autonomy [[168](#bib.bib168)].
    This SoC provides the possibility to implement high performance computer vision
    with low power consumption. R-Car V3H is optimized for applications that involve
    the usage of stereo cameras, containing dedicated hardware for convolutional neural
    networks, dense optical flow, stereo-vision, and object classification. The hardware
    features four 1.0 GHz Arm Cortex-A53 MPCore cores, which makes R-Car V3H a suitable
    hardware platform which can be used to deploy trained inference engines for solving
    specific deep learning tasks inside the automotive domain.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 由汽车制造商用于高级驾驶辅助系统（ADAS）的硬件平台是瑞萨自主的 R-Car V3H 系统单芯片（SoC）平台 [[168](#bib.bib168)]。这个
    SoC 提供了实现高性能计算机视觉的可能性，同时具有低功耗。R-Car V3H 针对使用立体摄像头的应用进行了优化，包含用于卷积神经网络、稠密光流、立体视觉和对象分类的专用硬件。该硬件具有四个
    1.0 GHz 的 Arm Cortex-A53 MPCore 核心，使 R-Car V3H 成为一个适合在汽车领域内部署训练好的推理引擎以解决特定深度学习任务的硬件平台。
- en: Renesas also provides a similar SoC, called R-Car H3 [[169](#bib.bib169)] which
    delivers improved computing capabilities and compliance with functional safety
    standards. Equipped with new CPU cores (Arm Cortex-A57), it can be used as an
    embedded platform for deploying various deep learning algorithms, compared with
    R-Car V3H, which is only optimized for CNNs.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 瑞萨还提供了一款类似的 SoC，称为 R-Car H3 [[169](#bib.bib169)]，它提供了更强的计算能力，并符合功能安全标准。配备了新的
    CPU 核心（Arm Cortex-A57），与仅针对 CNNs 进行优化的 R-Car V3H 相比，它可以作为部署各种深度学习算法的嵌入式平台。
- en: 'A Field-Programmable Gate Array (FPGA) is another viable solution, showing
    great improvements in both performance and power consumption in deep learning
    applications. The suitability of the FPGAs for running deep learning algorithms
    can be analyzed from four major perspectives: efficiency and power, raw computing
    power, flexibility and functional safety. Our study is based on the research published
    by Intel [[170](#bib.bib170)], Microsoft [[171](#bib.bib171)] and UCLA [[172](#bib.bib172)].'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 现场可编程门阵列（FPGA）是另一个可行的解决方案，在深度学习应用中表现出了性能和功耗的显著提升。对 FPGA 运行深度学习算法的适用性可以从效率和功耗、原始计算能力、灵活性和功能安全四个主要方面进行分析。我们的研究基于英特尔
    [[170](#bib.bib170)]、微软 [[171](#bib.bib171)] 和 UCLA [[172](#bib.bib172)] 发表的研究成果。
- en: By reducing the latency in deep learning applications, FPGAs provide additional
    raw computing power. The memory bottlenecks, associated with external memory accesses,
    are reduced or even eliminated by the high amount of chip cache memory. In addition,
    FPGAs have the advantages of supporting a full range of data types, together with
    custom user-defined types.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少深度学习应用中的延迟，FPGA 提供了额外的原始计算能力。由于大量的芯片缓存内存，与外部内存访问相关的内存瓶颈得到了减少甚至消除。此外，FPGA
    支持全范围的数据类型，以及自定义用户定义类型的优势。
- en: FPGAs are optimized when it comes to efficiency and power consumption. The studies
    presented by manufacturers like Microsoft and Xilinx show that GPUs can consume
    upon ten times more power than FPGAs when processing algorithms with the same
    computation complexity, demonstrating that FPGAs can be a much more suitable solution
    for deep learning applications in the automotive field.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 在效率和功耗方面，FPGA 表现得更为出色。微软和赛灵思等制造商提供的研究表明，当处理具有相同计算复杂度的算法时，GPU 的功耗可以是 FPGA 的十倍，证明
    FPGA 对于汽车领域的深度学习应用是一个更合适的解决方案。
- en: In terms of flexibility, FPGAs are built with multiple architectures, which
    are a mix of hardware programmable resources, digital signal processors and Processor
    Block RAM (BRAM) components. This architecture flexibility is suitable for deep
    and sparse neural networks, which are the state of the art for the current machine
    learning applications. Another advantage is the possibility of connecting to various
    input and output peripheral devices like sensors, network elements and storage
    devices.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 在灵活性方面，FPGA 采用了多种架构，这些架构混合了硬件可编程资源、数字信号处理器和处理器块 RAM（BRAM）组件。这种架构的灵活性适用于深度和稀疏神经网络，这些网络是当前机器学习应用的最前沿技术。另一个优势是可以连接各种输入和输出外设，如传感器、网络元素和存储设备。
- en: In the automotive field, functional safety is one of the most important challenges.
    FPGAs have been designed to meet the safety requirements for a wide range of applications,
    including ADAS. When compared to GPUs, which were originally built for graphics
    and high-performance computing systems, where functional safety is not necessary,
    FPGAs provide a significant advantage in developing driver assistance systems.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车领域，功能安全是最重要的挑战之一。FPGA已经被设计以满足广泛应用的安全要求，包括ADAS。与最初为图形和高性能计算系统构建的GPU相比，功能安全并非必需，FPGA在开发驾驶辅助系统方面具有显著优势。
- en: 10 Discussion and Conclusions
  id: totrans-662
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 讨论与结论
- en: 'We have identified seven major areas that form open challenges in the field
    of autonomous driving. We believe that Deep Learning and Artificial Intelligence
    will play a key role in overcoming these challenges:'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了在自动驾驶领域中存在的七个主要挑战。我们认为，深度学习和人工智能将在克服这些挑战中发挥关键作用：
- en: 'Perception: In order for an autonomous car to safely navigate the driving scene,
    it must be able to understand its surroundings. Deep learning is the main technology
    behind a large number of perception systems. Although great progress has been
    reported with respect to accuracy in object detection and recognition [[173](#bib.bib173)],
    current systems are mainly designed to calculate 2D or 3D bounding boxes for a
    couple of trained object classes, or to provide a segmented image of the driving
    environment. Future methods for perception should focus on increasing the levels
    of recognized details, making it possible to perceive and track more objects in
    real-time. Furthermore, additional work is required for bridging the gap between
    image- and LiDAR-based 3D perception [[32](#bib.bib32)], enabling the computer
    vision community to close the current debate on camera vs. LiDAR as main perception
    sensors.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 感知：为了使自动驾驶汽车能够安全地导航驾驶场景，它必须能够理解其周围环境。深度学习是许多感知系统的主要技术。尽管在物体检测和识别的准确性方面已取得了显著进展[[173](#bib.bib173)]，但当前系统主要设计用于计算几个训练物体类别的2D或3D边界框，或提供驾驶环境的分割图像。未来的感知方法应着重于提高识别细节的水平，使其能够实时感知和跟踪更多物体。此外，还需要进一步工作来弥合基于图像和LiDAR的3D感知之间的差距[[32](#bib.bib32)]，使计算机视觉领域能够解决当前关于相机与LiDAR作为主要感知传感器的争论。
- en: 'Short- to middle-term reasoning: Additional to a robust and accurate perception
    system, an autonomous vehicle should be able to reason its driving behavior over
    a short (milliseconds) to middle (seconds to minutes) time horizon [[82](#bib.bib82)].
    AI and deep learning are promising tools that can be used for the high- and low-level
    path path planning required for navigating the miriad of driving scenarios. Currently,
    the largest portion of papers in deep learning for self-driving cars are focused
    mainly on perception and End2End learning [[81](#bib.bib81), [124](#bib.bib124)].
    Over the next period, we expect deep learning to play a significant role in the
    area of local trajectory estimation and planning. We consider long-term reasoning
    as solved, as provided by navigation systems. These are standard methods for selecting
    a route through the road network, from the car’s current position to destination [[82](#bib.bib82)].'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 短期到中期推理：除了一个强大而准确的感知系统外，自动驾驶车辆还应能够在短期（毫秒）到中期（秒到分钟）时间范围内推理其驾驶行为[[82](#bib.bib82)]。人工智能和深度学习是可以用于高低层路径规划的有前景的工具，以应对各种驾驶场景。目前，大部分关于自动驾驶汽车的深度学习论文主要集中在感知和端到端学习[[81](#bib.bib81),
    [124](#bib.bib124)]。在接下来的阶段，我们预计深度学习将在局部轨迹估计和规划领域发挥重要作用。我们认为长期推理已经得到解决，由导航系统提供。这些是通过道路网络选择路线的标准方法，从汽车当前的位置到目的地[[82](#bib.bib82)]。
- en: 'Availability of training data: ”Data is the new oil” became lately one of the
    most popular quote in the automotive industry. The effectiveness of deep learning
    systems is directly tied to the availability of training data. As a rule of thumb,
    current deep learning methods are also evaluated based on the quality of training
    data [[29](#bib.bib29)]. The better the quality of the data is, the higher the
    accuracy of the algorithm. The daily data recorded by an autonomous vehicle is
    on the order of petabytes. This poses challenges on the parallelization of the
    training procedure, as well as on the storage infrastructure. Simulation environments
    have been used in the last couple of years for bridging the gap between scarce
    data and the deep learning’s hunger for training examples. There is still a gap
    to be filled between the accuracy of a simulated world and real-world driving.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的可用性：“数据是新的石油”最近成为汽车行业最流行的引用之一。深度学习系统的效力与训练数据的可用性直接相关。一般来说，目前的深度学习方法也是根据训练数据的质量来评估[[29](#bib.bib29)]。数据的质量越好，算法的准确性就越高。自动驾驶汽车每天记录的数据量达到几PB。这对于训练过程的并行化以及存储基础设施都提出了挑战。近几年来，模拟环境已经被用来弥合稀缺数据和深度学习对训练示例的需求之间的差距。模拟世界和真实世界驾驶的准确性之间仍然存在差距。
- en: 'Learning corner cases: Most driving scenarios are considered solvable with
    classical methodologies. However, the remaining unsolved scenarios are corner
    cases which, until now, required the reasoning and intelligence of a human driver.
    In order to overcome corner cases, the generalization power of deep learning algorithms
    should be increased. Generalization in deep learning is of special importance
    in learning hazardous situations that can lead to accidents, especially due to
    the fact that training data for such corner cases is scarce. This implies also
    the design of one-shot and low-shot learning methods, that can be trained a reduced
    number of training examples.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 学习极端情况：大多数驾驶场景都可以通过传统方法解决。然而，剩下的未解决场景都是极端情况，直到现在，都需要人类驾驶员的推理和智慧。为了克服极端情况，深度学习算法的泛化能力应该提高。深度学习中的泛化在学习可能导致事故的危险情况中尤为重要，特别是由于这类极端情况的训练数据稀缺。这也意味着需要设计一次性和低次学习方法，可以用较少的训练示例来训练。
- en: 'Learning-based control methods: Classical controllers make use of an a-priori
    model composed of fixed parameters. In a complex case, such as autonomous driving,
    these controllers cannot anticipate all driving situations. The effectiveness
    of deep learning components to adapt based on past experiences can also be used
    to learn the parameters of the car’s control system, thus better approximating
    the underlaying true system model [[174](#bib.bib174), [94](#bib.bib94)].'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的控制方法：传统控制器使用由固定参数组成的先验模型。在像自动驾驶这样复杂的情况下，这些控制器无法预料所有的驾驶情况。深度学习组件根据过去的经验适应的有效性也可以用来学习汽车控制系统的参数，从而更好地逼近底层真实系统模型[[174](#bib.bib174),
    [94](#bib.bib94)]。
- en: 'Functional safety: The usage of deep learning in safety-critical systems is
    still an open debate, efforts being made to bring the computational intelligence
    and functional safety communities closer to each other. Current safety standards,
    such as the ISO 26262, do not accommodate machine learning software [[130](#bib.bib130)].
    Although new data-driven design methodologies have been proposed, there are still
    opened issues on the explainability, stability, or classification robustness of
    deep neural networks.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 功能安全：在安全关键系统中使用深度学习仍然是一个悬而未决的辩论，人们正在努力使计算智能和功能安全社区更加紧密地联系在一起。当前的安全标准，例如ISO 26262，并不包容机器学习软件[[130](#bib.bib130)]。尽管提出了新的数据驱动设计方法，但深度神经网络的可解释性、稳定性或分类鲁棒性仍然存在问题。
- en: 'Real-time computing and communication: Finally, real-time requirements have
    to be fulfilled for processing the large amounts of data gathered from the car’s
    sensors suite, as well as for updating the parameters of deep learning systems
    over high-speed communication lines [[170](#bib.bib170)]. These real-time constraints
    can be backed up by advances in semiconductor chips dedicated for self-driving
    cars, as well as by the rise of 5G communication networks.'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 实时计算和通信：最后，必须满足实时要求，以处理从汽车传感器套件收集的大量数据，并通过高速通信线路更新深度学习系统的参数 [[170](#bib.bib170)]。这些实时约束可以通过专门用于自动驾驶汽车的半导体芯片的进展以及5G通信网络的兴起来支持。
- en: 10.1 Final Notes
  id: totrans-671
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 最终备注
- en: Autonomous vehicle technology has seen a rapid progress in the past decade,
    especially due to advances in the area of artificial intelligence and deep learning.
    Current AI methodologies are nowadays either used or taken into consideration
    when designing different components for a self-driving car. Deep learning approaches
    have influenced not only the design of traditional perception-planning-action
    pipelines, but have also enabled End2End learning systems, able do directly map
    sensory information to steering commands.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶技术在过去十年中取得了快速进展，特别是由于人工智能和深度学习领域的进步。目前的人工智能方法在设计自动驾驶汽车的不同组件时，要么被使用，要么被考虑。深度学习方法不仅影响了传统的感知-规划-行动管道的设计，还使得
    End2End 学习系统能够直接将感官信息映射到转向指令。
- en: Driverless cars are complex systems which have to safely drive passengers or
    cargo from a starting location to destination. Several challenges are encountered
    with the advent of AI based autonomous vehicles deployment on public roads. A
    major challenge is the difficulty in proving the functional safety of these vehicle,
    given the current formalism and explainability of neural networks. On top of this,
    deep learning systems rely on large training databases and require extensive computational
    hardware.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 无人驾驶汽车是复杂的系统，需要安全地将乘客或货物从起点送到目的地。随着基于人工智能的自动驾驶车辆在公共道路上的部署，面临着若干挑战。一个主要挑战是鉴于当前神经网络的形式化和可解释性，证明这些车辆的功能安全的难度。此外，深度学习系统依赖于大量的训练数据库，并且需要广泛的计算硬件。
- en: This paper has provided a survey on deep learning technologies used in autonomous
    driving. The survey of performance and computational requirements serves as a
    reference for system level design of AI based self-driving vehicles.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对用于自动驾驶的深度学习技术进行了综述。性能和计算要求的综述作为基于人工智能的自动驾驶车辆系统级设计的参考。
- en: Acknowledgment
  id: totrans-675
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Elektrobit Automotive for the infrastructure
    and research support.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢 Elektrobit Automotive 提供的基础设施和研究支持。
- en: References
  id: totrans-677
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” in *Advances in Neural Information Processing
    Systems 25*, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds.   Curran
    Associates, Inc., 2012, pp. 1097–1105.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “ImageNet Classification with
    Deep Convolutional Neural Networks,” 见 *Advances in Neural Information Processing
    Systems 25*, F. Pereira, C. J. C. Burges, L. Bottou, 和 K. Q. Weinberger, 编著。Curran
    Associates, Inc., 2012年，第1097–1105页。'
- en: '[2] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki,
    A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder,
    L. Weng, and W. Zaremba, “Learning Dexterous In-Hand Manipulation,” *CoRR*, vol.
    abs/1808.00177, August 2018\. [Online]. Available: [https://arxiv.org/abs/1808.00177](https://arxiv.org/abs/1808.00177)'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki,
    A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P.
    Welinder, L. Weng, 和 W. Zaremba, “Learning Dexterous In-Hand Manipulation,” *CoRR*,
    vol. abs/1808.00177, 2018年8月。 [在线]。可用: [https://arxiv.org/abs/1808.00177](https://arxiv.org/abs/1808.00177)'
- en: '[3] Y. Goldberg, *Neural Network Methods for Natural Language Processing*,
    ser. Synthesis Lectures on Human Language Technologies.   Morgan & Claypool, 2017,
    vol. 37.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Goldberg, *Neural Network Methods for Natural Language Processing*,
    系列: Synthesis Lectures on Human Language Technologies。摩根与克雷普，2017年，第37卷。'
- en: '[4] SAE Committee, “Taxonomy and Definitions for Terms Related to On-road Motor
    Vehicle Automated Driving Systems,” 2014.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] SAE委员会, “与道路机动车自动驾驶系统相关的术语的分类和定义,” 2014年。'
- en: '[5] E. Dickmanns and V. Graefe, “Dynamic Monocular Machine Vision,” *Machine
    vision and applications*, vol. 1, pp. 223––240, 1988.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] E. Dickmanns 和 V. Graefe, “Dynamic Monocular Machine Vision,” *Machine
    vision and applications*, vol. 1, 第223–240页，1988年。'
- en: '[6] B. Paden, M. Cáp, S. Z. Yong, D. S. Yershov, and E. Frazzoli, “A Survey
    of Motion Planning and Control Techniques for Self-Driving Urban Vehicles,” *IEEE
    Trans. Intelligent Vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] B. Paden, M. Cáp, S. Z. Yong, D. S. Yershov 和 E. Frazzoli, “自动驾驶城市车辆的运动规划和控制技术综述，”
    *IEEE智能车辆学报*，第1卷，第1期，第33–55页，2016年。'
- en: '[7] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based Learning
    Applied to Document Recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, Nov 1998.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Lecun, L. Bottou, Y. Bengio 和 P. Haffner, “基于梯度的学习应用于文档识别，” *IEEE汇刊*，第86卷，第11期，第2278–2324页，1998年11月。'
- en: '[8] Y. Bengio, A. Courville, and P. Vincent, “Representation Learning: A Review
    and New Perspectives,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 35, no. 8, pp. 1798–1828, Aug 2013.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Bengio, A. Courville 和 P. Vincent, “表示学习：综述与新视角，” *IEEE模式分析与机器智能学报*，第35卷，第8期，第1798–1828页，2013年8月。'
- en: '[9] P. A. Viola and M. J. Jones, “Rapid Object Detection using a Boosted Cascade
    of Simple Features,” in *2001 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition (CVPR 2001), with CD-ROM, 8-14 December 2001, Kauai, HI,
    USA*, 2001, pp. 511–518.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. A. Viola 和 M. J. Jones, “使用增强级联简单特征的快速对象检测，” 见 *2001年IEEE计算机协会计算机视觉与模式识别会议（CVPR
    2001），附CD-ROM，2001年12月8-14日，美国夏威夷考艾岛*，2001年，第511–518页。'
- en: '[10] T. Ojala, M. Pietikäinen, and D. Harwood, “A Comparative Study of Texture
    Measures with Classification Based on Featured Distributions,” *Pattern Recognition*,
    vol. 29, no. 1, pp. 51–59, Jan. 1996.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. Ojala, M. Pietikäinen 和 D. Harwood, “基于特征分布的纹理度量的比较研究，” *模式识别*，第29卷，第1期，第51–59页，1996年1月。'
- en: '[11] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,”
    in *In CVPR*, 2005, pp. 886–893.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] N. Dalal 和 B. Triggs, “用于人类检测的方向梯度直方图，” 见 *在 CVPR 会议上*，2005年，第886–893页。'
- en: '[12] D. H. Hubel and T. N.Wiesel, “Shape and Arrangement of Columns in Cat’s
    Striate Cortex,” *The Journal of Physiology*, vol. 165, no. 3, p. 559–568, 1963.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] D. H. Hubel 和 T. N. Wiesel, “猫的纹状皮层中柱状结构的形状和排列，” *生理学杂志*，第165卷，第3期，第559–568页，1963年。'
- en: '[13] M. A. Goodale and A. Milner, “Separate Visual Pathways for Perception
    and Action,” *Trends in Neurosciences*, vol. 15, no. 1, pp. 20 – 25, 1992.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. A. Goodale 和 A. Milner, “感知与动作的视觉路径分离，” *神经科学趋势*，第15卷，第1期，第20–25页，1992年。'
- en: '[14] D. E. Rumelhart, J. L. McClelland, and C. PDP Research Group, Eds., *Parallel
    Distributed Processing: Explorations in the Microstructure of Cognition, Vol.
    1: Foundations*.   Cambridge, MA, USA: MIT Press, 1986.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. E. Rumelhart, J. L. McClelland 和 C. PDP 研究组, 主编，*并行分布处理：认知微观结构探索，第1卷：基础*。
    美国麻省剑桥：MIT出版社，1986年。'
- en: '[15] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
    in *3rd Int. Conf. on Learning Representations, ICLR 2015*, San Diego, CA, USA,
    May 2015.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. P. Kingma 和 J. Ba, “Adam：一种随机优化方法，” 见 *第三届国际学习表示会议，ICLR 2015*，美国加利福尼亚州圣地亚哥，2015年5月。'
- en: '[16] E. H. J. Duchi and Y. Singer, “Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization,” *Journal of Machine Learning Research*,
    vol. 12, pp. 2121–2159, 2011.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] E. H. J. Duchi 和 Y. Singer, “用于在线学习和随机优化的自适应子梯度方法，” *机器学习研究期刊*，第12卷，第2121–2159页，2011年。'
- en: '[17] S. Hochreiter and J. Schmidhuber, “Long Short-term Memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Hochreiter 和 J. Schmidhuber, “长短期记忆，” *神经计算*，第9卷，第8期，第1735–1780页，1997年。'
- en: '[18] R. Sutton and A. Barto, *Introduction to Reinforcement Learning*.   MIT
    Press, 1998.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Sutton 和 A. Barto, *强化学习导论*。 MIT出版社，1998年。'
- en: '[19] R. Bellman, *Dynamic Programming*.   Princeton University Press, 1957.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] R. Bellman, *动态规划*。 普林斯顿大学出版社，1957年。'
- en: '[20] C. Watkins and P. Dayan, “Q-Learning,” *Machine Learning*, vol. 8, no. 3,
    p. 279–292, 1992.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] C. Watkins 和 P. Dayan, “Q-学习，” *机器学习*，第8卷，第3期，第279–292页，1992年。'
- en: '[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
    “Human-level Control Through Deep Reinforcement Learning,” *Nature*, vol. 518,
    no. 7540, pp. 529–533, Feb. 2015.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, 和 D. Hassabis,
    “通过深度强化学习实现人类级控制，” *自然*，第518卷，第7540期，第529–533页，2015年2月。'
- en: '[22] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining Improvements in
    Deep Reinforcement Learning,” 2017.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, 和 D. Silver, “Rainbow：在深度强化学习中结合改进”，2017年。'
- en: '[23] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep Reinforcement
    Learning framework for Autonomous Driving,” *CoRR*, vol. abs/1704.02532, 2017.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. E. Sallab, M. Abdou, E. Perot, 和 S. Yogamani, “用于自动驾驶的深度强化学习框架”，*CoRR*，第abs/1704.02532卷，2017年。'
- en: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous Control with Deep Reinforcement Learning,”
    2-4 May 2016.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, 和 D. Wierstra, “使用深度强化学习进行连续控制”，2016年5月2-4日。'
- en: '[25] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous Deep Q-Learning
    with Model-based Acceleration,” in *Int. Conf. on Machine Learning ICML 2016*,
    vol. 48, Jun. 2016, pp. 2829–2838.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Gu, T. Lillicrap, I. Sutskever, 和 S. Levine, “基于模型加速的连续深度 Q 学习”，在 *2016
    年国际机器学习大会 ICML*，第48卷，2016年6月，第2829–2838页。'
- en: '[26] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi,
    “End-to-End Race Driving with Deep Reinforcement Learning,” *2018 IEEE Int. Conf.
    on Robotics and Automation (ICRA)*, pp. 2070–2075, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, 和 F. Nashashibi, “使用深度强化学习进行端到端的赛车驾驶”，*2018
    IEEE 国际机器人与自动化大会（ICRA）*，第2070–2075页，2018年。'
- en: '[27] M. Wulfmeier, D. Z. Wang, and I. Posner, “Watch This: Scalable Cost-Function
    Learning for Path Planning in Urban Environments,” *2016 IEEE/RSJ Int. Conf. on
    Intelligent Robots and Systems (IROS)*, vol. abs/1607.02329, 2016. [Online]. Available:
    [http://arxiv.org/abs/1607.02329](http://arxiv.org/abs/1607.02329)'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Wulfmeier, D. Z. Wang, 和 I. Posner, “观看这个：在城市环境中用于路径规划的可扩展成本函数学习”，*2016
    IEEE/RSJ 国际智能机器人与系统大会（IROS）*，第abs/1607.02329卷，2016年。[在线]. 可用： [http://arxiv.org/abs/1607.02329](http://arxiv.org/abs/1607.02329)'
- en: '[28] H. Zhu, K.-V. Yuen, L. S. Mihaylova, and H. Leung, “Overview of Environment
    Perception for Intelligent Vehicles,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 18, pp. 2584–2601, 2017.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H. Zhu, K.-V. Yuen, L. S. Mihaylova, 和 H. Leung, “智能车辆环境感知概述”，*IEEE 智能交通系统汇刊*，第18卷，第2584–2601页，2017年。'
- en: '[29] J. Janai, F. Guney, A. Behl, and A. Geiger, “Computer Vision for Autonomous
    Vehicles: Problems, Datasets and State-of-the-Art,” 04 2017.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Janai, F. Guney, A. Behl, 和 A. Geiger, “自动驾驶车辆的计算机视觉：问题、数据集和最新技术”，2017年4月。'
- en: '[30] S. O’Kane, “How Tesla and Waymo are Tackling a Major Problem for Self-Driving
    Cars: Data,” *Transportation*, 2018.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. O’Kane, “特斯拉和 Waymo 如何解决自动驾驶汽车面临的重大问题：数据”，*交通运输*，2018年。'
- en: '[31] S. Hasirlioglu, A. Kamann, I. Doric, and T. Brandmeier, “Test Methodology
    for Rain Influence on Automotive Surround Sensors,” in *2016 IEEE 19th Int. Conf.
    on Intelligent Transportation Systems (ITSC)*, Nov 2016, pp. 2242–2247.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Hasirlioglu, A. Kamann, I. Doric, 和 T. Brandmeier, “汽车周边传感器雨水影响的测试方法”，在
    *2016 IEEE 第19届国际智能交通系统大会（ITSC）*，2016年11月，第2242–2247页。'
- en: '[32] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Weinberger,
    “Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection
    for Autonomous Driving,” in *IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR) 2019*, June 2019.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, 和 K. Weinberger,
    “伪 LiDAR 来自视觉深度估计：弥合自动驾驶 3D 物体检测的差距”，在 *2019 年 IEEE 计算机视觉与模式识别会议（CVPR）*，2019年6月。'
- en: '[33] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep Learning on
    Point Sets for 3D Classification and Segmentation,” in *IEEE Conf. on Computer
    Vision and Pattern Recognition (CVPR) 2017*, July 2017.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “PointNet：用于 3D 分类和分割的点集深度学习”，在
    *2017 年 IEEE 计算机视觉与模式识别会议（CVPR）*，2017年7月。'
- en: '[34] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3D
    Proposal Generation and Object Detection from View Aggregation,” in *IEEE/RSJ
    Int. Conf. on Intelligent Robots and Systems (IROS) 2018*.   IEEE, 2018.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Ku, M. Mozifian, J. Lee, A. Harakeh, 和 S. L. Waslander, “从视图聚合生成联合
    3D 提议和物体检测”，在 *2018 年 IEEE/RSJ 国际智能机器人与系统大会（IROS）*。IEEE，2018年。'
- en: '[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large
    Scale Visual Recognition Challenge,” *Int. Journal of Computer Vision (IJCV)*,
    vol. 115, no. 3, pp. 211–252, 2015.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, 和 L. Fei-Fei, “ImageNet 大规模视觉识别挑战”，*国际计算机视觉杂志（IJCV）*，第115卷，第3期，第211–252页，2015年。'
- en: '[36] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look Once:
    Unified, Real-time Object Detection,” in *Proceedings of the IEEE Conf. on computer
    vision and pattern recognition*, 2016, pp. 779–788.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Redmon, S. Divvala, R. Girshick 和 A. Farhadi，“你只看一次：统一的实时对象检测”，在 *IEEE
    计算机视觉与模式识别会议论文集*，2016年，页码 779–788。'
- en: '[37] H. Law and J. Deng, “Cornernet: Detecting Objects as Paired Keypoints,”
    in *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    734–750.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Law 和 J. Deng，“Cornernet: 将对象检测为配对关键点”，在 *欧洲计算机视觉会议 (ECCV)*，2018，页码
    734–750。'
- en: '[38] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot Refinement
    Neural Network for Object Detection,” *IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*, 2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Zhang, L. Wen, X. Bian, Z. Lei 和 S. Z. Li，“单次精化神经网络进行对象检测”，*IEEE 计算机视觉与模式识别会议
    (CVPR)*，2017年。'
- en: '[39] R. Girshick, “Fast R-CNN,” in *Proceedings of the IEEE Int. Conf. on computer
    vision*, 2015, pp. 1440–1448.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] R. Girshick，“Fast R-CNN”，在 *IEEE 国际计算机视觉会议论文集*，2015年，页码 1440–1448。'
- en: '[40] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer,
    “Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and¡ 0.5 Mb Model
    Size,” *arXiv preprint arXiv:1602.07360*, 2016.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally 和 K. Keutzer，“Squeezenet:
    Alexnet 级别的准确度，参数减少 50 倍，模型大小 0.5 Mb”，*arXiv 预印本 arXiv:1602.07360*，2016年。'
- en: '[41] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object Detection via Region-based
    Fully Convolutional Networks,” in *Advances in neural information processing systems*,
    2016, pp. 379–387.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Dai, Y. Li, K. He 和 J. Sun，“R-fcn: 基于区域的全卷积网络进行对象检测”，在 *神经信息处理系统进展*，2016年，页码
    379–387。'
- en: '[42] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep Convolutional
    Encoder-Decoder Architecture for Image Segmentation,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 39, 2017.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] V. Badrinarayanan, A. Kendall 和 R. Cipolla，“SegNet: 用于图像分割的深度卷积编码器-解码器架构”，*IEEE
    模式分析与机器智能汇刊*，第 39 卷，2017年。'
- en: '[43] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, “Icnet for Real-time Semantic
    Segmentation on High-resolution Images,” *European Conference on Computer Vision*,
    pp. 418–434, 2018.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Zhao, X. Qi, X. Shen, J. Shi 和 J. Jia，“Icnet 在高分辨率图像上的实时语义分割”，*欧洲计算机视觉会议*，页码
    418–434，2018年。'
- en: '[44] M. Treml, J. A. Arjona-Medina, T. Unterthiner, R. Durgesh, F. Friedmann,
    P. Schuberth, A. Mayr, M. Heusel, M. Hofmarcher, M. Widrich, B. Nessler, and S. Hochreiter,
    “Speeding up Semantic Segmentation for Autonomous Driving,” 2016.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Treml, J. A. Arjona-Medina, T. Unterthiner, R. Durgesh, F. Friedmann,
    P. Schuberth, A. Mayr, M. Heusel, M. Hofmarcher, M. Widrich, B. Nessler 和 S. Hochreiter，“加速自动驾驶的语义分割”，2016年。'
- en: '[45] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, “Mask R-CNN,” *2017
    IEEE Int. Conf. on Computer Vision (ICCV)*, pp. 2980–2988, 2017.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] K. He, G. Gkioxari, P. Dollar 和 R. B. Girshick，“Mask R-CNN”，*2017 IEEE
    国际计算机视觉会议 (ICCV)*，页码 2980–2988，2017年。'
- en: '[46] Y. Zhou and O. Tuzel, “VoxelNet: End-to-End Learning for Point Cloud Based
    3D Object Detection,” *IEEE Conf. on Computer Vision and Pattern Recognition 2018*,
    pp. 4490–4499, 2018.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Zhou 和 O. Tuzel，“VoxelNet: 基于点云的 3D 对象检测的端到端学习”，*IEEE 计算机视觉与模式识别会议
    2018*，页码 4490–4499，2018年。'
- en: '[47] W. Luo, B. Yang, and R. Urtasun, “Fast and Furious: Real Time End-to-End
    3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net,”
    in *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 2018*, June 2018.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Luo, B. Yang 和 R. Urtasun，“快速与激烈：基于单卷积网络的实时端到端 3D 检测、跟踪和运动预测”，在 *IEEE
    计算机视觉与模式识别会议 (CVPR) 2018*，2018年6月。'
- en: '[48] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum PointNets for
    3D Object Detection from RGB-D Data,” in *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR) 2018*, June 2018.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. R. Qi, W. Liu, C. Wu, H. Su 和 L. J. Guibas，“Frustum PointNets 从 RGB-D
    数据进行 3D 对象检测”，在 *IEEE 计算机视觉与模式识别会议 (CVPR) 2018*，2018年6月。'
- en: '[49] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-View 3D Object Detection
    Network for Autonomous Driving,” in *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR) 2017*, July 2017.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Chen, H. Ma, J. Wan, B. Li 和 T. Xia，“用于自动驾驶的多视角 3D 对象检测网络”，在 *IEEE
    计算机视觉与模式识别会议 (CVPR) 2017*，2017年7月。'
- en: '[50] J. Redmon and A. Farhadi, “YOLO9000: Better, Faster, Stronger,” *IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2017.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Redmon 和 A. Farhadi，“YOLO9000: 更好、更快、更强”，*IEEE 计算机视觉与模式识别会议 (CVPR)*，2017年。'
- en: '[51] ——, “Yolov3: An Incremental Improvement,” *arXiv preprint arXiv:1804.02767*,
    2018.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] ——，“Yolov3: 增量改进”，*arXiv 预印本 arXiv:1804.02767*，2018年。'
- en: '[52] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single Shot Multibox Detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, 和 A. C.
    Berg, “Ssd: 单次多框检测器，” 见 *欧洲计算机视觉会议*。施普林格，2016年，第21–37页。'
- en: '[53] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich Feature Hierarchies
    for Accurate Object Detection and Semantic Segmentation,” in *Proceedings of the
    2014 IEEE Conf. on Computer Vision and Pattern Recognition*, ser. CVPR ’14.   Washington,
    DC, USA: IEEE Computer Society, 2014, pp. 580–587.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] R. Girshick, J. Donahue, T. Darrell, 和 J. Malik, “准确目标检测和语义分割的丰富特征层次结构，”
    见 *2014 IEEE计算机视觉与模式识别会议论文集*，系列 CVPR ’14。华盛顿特区，美国：IEEE计算机学会，2014年，第580–587页。'
- en: '[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-time
    Object Detection with Region Proposal Networks,” *IEEE Transactions on Pattern
    Analysis & Machine Intelligence*, no. 6, pp. 1137–1149, 2017.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Ren, K. He, R. Girshick, 和 J. Sun, “Faster R-CNN: 朝着实时目标检测与区域提议网络迈进，”
    *IEEE模式分析与机器智能汇刊*，第6期，第1137–1149页，2017年。'
- en: '[55] J. Li, K. Peng, and C.-C. Chang, “An Efficient Object Detection Algorithm
    Based on Compressed Networks,” *Symmetry*, vol. 10, no. 7, p. 235, 2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. Li, K. Peng, 和 C.-C. Chang, “基于压缩网络的高效目标检测算法，” *对称性*，第10卷，第7期，第235页，2018年。'
- en: '[56] K. Shin, Y. P. Kwon, and M. Tomizuka, “RoarNet: A Robust 3D Object Detection
    based on RegiOn Approximation Refinement,” *CoRR*, vol. abs/1811.03818, 2018.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] K. Shin, Y. P. Kwon, 和 M. Tomizuka, “RoarNet: 基于区域近似优化的鲁棒3D目标检测，” *CoRR*，卷号
    abs/1811.03818，2018年。'
- en: '[57] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A Deep Neural
    Network Architecture for Real-time Semantic Segmentation,” *arXiv preprint arXiv:1606.02147*,
    2016.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Paszke, A. Chaurasia, S. Kim, 和 E. Culurciello, “Enet: 实时语义分割的深度神经网络架构，”
    *arXiv预印本 arXiv:1606.02147*，2016年。'
- en: '[58] A. Valada, J. Vertens, A. Dhall, and W. Burgard, “AdapNet: Adaptive Semantic
    Segmentation in Adverse Environmental Conditions,” *2017 IEEE Int. Conf. on Robotics
    and Automation (ICRA)*, pp. 4644–4651, 2017.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Valada, J. Vertens, A. Dhall, 和 W. Burgard, “AdapNet: 不利环境条件下的自适应语义分割，”
    *2017 IEEE国际机器人与自动化会议（ICRA）*，第4644–4651页，2017年。'
- en: '[59] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-scale
    Image Recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深卷积网络，” *arXiv预印本 arXiv:1409.1556*，2014年。'
- en: '[60] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,” *IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2015.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入探索，” *IEEE计算机视觉与模式识别会议（CVPR）*，2015年。'
- en: '[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
    Recognition,” in *Proceedings of the IEEE Conf. on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差学习用于图像识别，” 见 *IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[62] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to Distraction:
    Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban
    Environments,” in *2018 IEEE Int. Conf. on Robotics and Automation (ICRA)*.   IEEE,
    2018.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] D. Barnes, W. Maddern, G. Pascoe, 和 I. Posner, “分心驾驶：用于城市环境中鲁棒单目视觉里程计的自监督干扰学习，”
    见 *2018 IEEE国际机器人与自动化会议（ICRA）*。IEEE，2018年。'
- en: '[63] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous Localization
    and Mapping: A Survey of Current Trends in Autonomous Driving,” *IEEE Transactions
    on Intelligent Vehicles*, vol. 2, no. 3, pp. 194–220, Sep 2017.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] G. Bresson, Z. Alsayed, L. Yu, 和 S. Glaser, “同时定位与建图：自主驾驶中当前趋势的综述，” *IEEE智能车辆汇刊*，第2卷，第3期，第194–220页，2017年9月。'
- en: '[64] A. Kendall, M. Grimes, and R. Cipolla, “PoseNet: A Convolutional Network
    for Real-Time 6-DOF Camera Relocalization,” in *Proceedings of the 2015 IEEE Int.
    Conf. on Computer Vision (ICCV)*.   Washington, DC, USA: IEEE Computer Society,
    2015, pp. 2938–2946.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Kendall, M. Grimes, 和 R. Cipolla, “PoseNet: 实时6自由度相机重定位的卷积网络，” 见 *2015
    IEEE国际计算机视觉会议（ICCV）论文集*。华盛顿特区，美国：IEEE计算机学会，2015年，第2938–2946页。'
- en: '[65] N. Radwan, A. Valada, and W. Burgard, “VLocNet++: Deep Multitask Learning
    for Semantic Visual Localization and Odometry,” *IEEE Robotics and Automation
    Letters*, Sep 2018.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] N. Radwan, A. Valada, 和 W. Burgard, “VLocNet++: 语义视觉定位与里程计的深度多任务学习，” *IEEE机器人与自动化信函*，2018年9月。'
- en: '[66] F. Walch, C. Hazirbas, L. Leal-Taixé, T. Sattler, S. Hilsenbeck, and D. Cremers,
    “Image-Based Localization Using LSTMs for Structured Feature Correlation,” *2017
    IEEE Int. Conf. on Computer Vision (ICCV)*, pp. 627–637, 2017.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] F. Walch, C. Hazirbas, L. Leal-Taixé, T. Sattler, S. Hilsenbeck, 和 D. Cremers，“基于图像的定位使用
    LSTM 进行结构化特征相关，”*2017 年 IEEE 计算机视觉国际会议 (ICCV)*，第 627–637 页，2017年。'
- en: '[67] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-Based Localization
    Using Hourglass Networks,” *2017 IEEE Int. Conf. on Computer Vision Workshops
    (ICCVW)*, pp. 870–877, 2017.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] I. Melekhov, J. Ylioinas, J. Kannala, 和 E. Rahtu，“使用 Hourglass 网络的基于图像的定位，”*2017
    年 IEEE 计算机视觉国际会议研讨会 (ICCVW)*，第 870–877 页，2017年。'
- en: '[68] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera Relocalization
    by Computing Pairwise Relative Poses Using Convolutional Neural Network,” in *The
    IEEE Int. Conf. on Computer Vision (ICCV)*, Oct 2017.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Z. Laskar, I. Melekhov, S. Kalia, 和 J. Kannala，“通过计算对相对姿态来进行相机重新定位，使用卷积神经网络，”在*IEEE
    计算机视觉国际会议 (ICCV)*，2017年10月。'
- en: '[69] E. Brachmann and C. Rother, “Learning Less is More – 6D Camera Localization
    via 3D Surface Regression,” in *IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR) 2018*, June 2018.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] E. Brachmann 和 C. Rother，“学习少即是多——通过 3D 表面回归进行 6D 相机定位，”在*IEEE 计算机视觉与模式识别会议
    (CVPR) 2018*，2018年6月。'
- en: '[70] P. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C. Cadena, “Leveraging
    Deep Visual Descriptors for Hierarchical Efficient Localization,” in *Proc. of
    the 2nd Conf. on Robot Learning (CoRL)*, Oct 2018.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] P. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, 和 C. Cadena，“利用深度视觉描述符进行层次化高效定位，”在*第
    2 届机器人学习会议 (CoRL) 会议论文集*，2018年10月。'
- en: '[71] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to Localize
    Using a LiDAR Intensity Map,” in *Proc. of the 2nd Conf. on Robot Learning (CoRL)*,
    Oct 2018.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] I. A. Barsan, S. Wang, A. Pokrovsky, 和 R. Urtasun，“使用 LiDAR 强度图进行定位学习，”在*第
    2 届机器人学习会议 (CoRL) 会议论文集*，2018年10月。'
- en: '[72] O. Garcia-Favrot and M. Parent, “Laser Scanner Based SLAM in Real Road
    and Traffic Environment,” in *IEEE Int. Conf. Robotics and Automation (ICRA09).
    Workshop on Safe navigation in open and dynamic environments Application to autonomous
    vehicles*, 2009.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] O. Garcia-Favrot 和 M. Parent，“基于激光扫描仪的 SLAM 在实际道路和交通环境中的应用，”在*IEEE 国际机器人与自动化会议
    (ICRA09)*，开放和动态环境安全导航研讨会，2009年。'
- en: '[73] A. K. Ushani and R. M. Eustice, “Feature Learning for Scene Flow Estimation
    from LIDAR,” in *Proc. of the 2nd Conf. on Robot Learning (CoRL)*, vol. 87, Oct
    2018, pp. 283–292.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. K. Ushani 和 R. M. Eustice，“从 LIDAR 中进行场景流估计的特征学习，”在*第 2 届机器人学习会议 (CoRL)
    会议论文集*，第 87 卷，2018年10月，第 283–292 页。'
- en: '[74] Cityscapes, “Cityscapes Data Collection,” [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/),
    2018.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Cityscapes，“Cityscapes 数据收集，” [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)，2018年。'
- en: '[75] S. Thrun, W. Burgard, and D. Fox, “Probabilistic Robotics (Intelligent
    Robotics and Autonomous Agents),” in *Cambridge: The MIT Press*, 2005.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] S. Thrun, W. Burgard, 和 D. Fox，“概率机器人学（智能机器人与自主代理），”在*剑桥：麻省理工学院出版社*，2005年。'
- en: '[76] P. Ondruska, J. Dequaire, D. Z. Wang, and I. Posner, “End-to-End Tracking
    and Semantic Segmentation Using Recurrent Neural Networks,” *CoRR*, vol. abs/1604.05091,
    2016.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] P. Ondruska, J. Dequaire, D. Z. Wang, 和 I. Posner，“使用递归神经网络的端到端跟踪和语义分割，”*CoRR*，第
    abs/1604.05091 卷，2016年。'
- en: '[77] S. Hoermann, M. Bach, and K. Dietmayer, “Dynamic Occupancy Grid Prediction
    for Urban Autonomous Driving: Deep Learning Approach with Fully Automatic Labeling,”
    *IEEE Int. Conf. on Robotics and Automation (ICRA)*, 2017.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Hoermann, M. Bach, 和 K. Dietmayer，“城市自主驾驶的动态占用网格预测：深度学习方法与完全自动标签，”*IEEE
    国际机器人与自动化会议 (ICRA)*，2017年。'
- en: '[78] S. Ramos, S. K. Gehrig, P. Pinggera, U. Franke, and C. Rother, “Detecting
    Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric
    Modeling,” *IEEE Intelligent Vehicles Symposium*, vol. 4, 2016.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] S. Ramos, S. K. Gehrig, P. Pinggera, U. Franke, 和 C. Rother，“为自动驾驶汽车检测意外障碍物：融合深度学习和几何建模，”*IEEE
    智能车辆研讨会*，第 4 卷，2016年。'
- en: '[79] C. Seeger, A. Müller, and L. Schwarz, “Towards Road Type Classification
    with Occupancy Grids,” in *Intelligent Vehicles Symposium - Workshop: DeepDriving
    - Learning Representations for Intelligent Vehicles, IEEE*, Gothenburg, Sweden,
    July 2016.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Seeger, A. Müller, 和 L. Schwarz，“面向道路类型分类的占用网格，”在*智能车辆研讨会 - 研讨会：DeepDriving
    - 学习智能车辆的表示，IEEE*，瑞典哥德堡，2016年7月。'
- en: '[80] L. Marina, B. Trasnea, T. Cocias, A. Vasilcoi, F. Moldoveanu, and S. Grigorescu,
    “Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving Context Understanding,”
    in *Int. Conf. on Robotic Computing IRC 2019*, Naples, Italy, 25-27 February 2019.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] L. Marina, B. Trasnea, T. Cocias, A. Vasilcoi, F. Moldoveanu, 和 S. Grigorescu，“深度网格网络（DGN）：用于实时驾驶环境理解的深度学习系统，”
    在 *2019年机器人计算国际会议 IRC*，意大利那不勒斯，2019年2月25-27日。'
- en: '[81] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, Multi-Agent, Reinforcement
    Learning for Autonomous Driving,” 2016.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Shalev-Shwartz, S. Shammah, 和 A. Shashua，“安全的多智能体强化学习用于自主驾驶，” 2016年。'
- en: '[82] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. Eng,
    D. Rus, and M. H. Ang, “Perception, Planning, Control, and Coordination for Autonomous
    Vehicles,” *Machines*, vol. 5, no. 1, p. 6, 2017.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. Eng,
    D. Rus, 和 M. H. Ang，“自动驾驶车辆的感知、规划、控制和协调，” *Machines*，第 5 卷，第 1 期，第 6 页，2017年。'
- en: '[83] E. Rehder, J. Quehl, and C. Stiller, “Driving Like a Human: Imitation
    Learning for Path Planning using Convolutional Neural Networks,” in *Int. Conf.
    on Robotics and Automation Workshops*, 2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] E. Rehder, J. Quehl, 和 C. Stiller，“像人类一样驾驶：使用卷积神经网络的路径规划模仿学习，” 在 *机器人与自动化研讨会国际会议*，2017年。'
- en: '[84] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A Fast Integrated Planning
    and Control Framework for Autonomous Driving via Imitation Learning,” *ASME 2018
    Dynamic Systems and Control Conference*, vol. 3, 2018. [Online]. Available: [https://arxiv.org/pdf/1707.02515.pdf](https://arxiv.org/pdf/1707.02515.pdf)'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] L. Sun, C. Peng, W. Zhan, 和 M. Tomizuka，“通过模仿学习的自主驾驶快速集成规划与控制框架，” *ASME
    2018动态系统与控制会议*，第 3 卷，2018年。 [在线]。可用： [https://arxiv.org/pdf/1707.02515.pdf](https://arxiv.org/pdf/1707.02515.pdf)'
- en: '[85] S. Grigorescu, B. Trasnea, L. Marina, A. Vasilcoi, and T. Cocias, “NeuroTrajectory:
    A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous
    Vehicles,” *IEEE Robotics and Automation Letters*, vol. 4, no. 4, pp. 3441–3448,
    October 2019.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Grigorescu, B. Trasnea, L. Marina, A. Vasilcoi, 和 T. Cocias，“NeuroTrajectory：一种神经进化方法用于自主车辆的局部状态轨迹学习，”
    *IEEE Robotics and Automation Letters*，第 4 卷，第 4 期，第 3441–3448 页，2019年10月。'
- en: '[86] L. Yu, X. Shao, Y. Wei, and K. Zhou, “Intelligent Land-Vehicle Model Transfer
    Trajectory Planning Method Based on Deep Reinforcement Learning,” *Sensors (Basel,
    Switzerland)*, vol. 18, 09 2018.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] L. Yu, X. Shao, Y. Wei, 和 K. Zhou，“基于深度强化学习的智能陆地车辆模型转移轨迹规划方法，” *Sensors
    (Basel, Switzerland)*，第 18 卷，2018年9月。'
- en: '[87] C. Paxton, V. Raman, G. D. Hager, and M. Kobilarov, “Combining Neural
    Networks and Tree Search for Task and Motion Planning in Challenging Environments,”
    *2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)*, vol. abs/1703.07887,
    2017\. [Online]. Available: [http://arxiv.org/abs/1703.07887](http://arxiv.org/abs/1703.07887)'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] C. Paxton, V. Raman, G. D. Hager, 和 M. Kobilarov，“结合神经网络和树搜索在挑战性环境中的任务和运动规划，”
    *2017年IEEE/RSJ智能机器人与系统国际会议（IROS）*，第 abs/1703.07887 卷，2017年。 [在线]。可用： [http://arxiv.org/abs/1703.07887](http://arxiv.org/abs/1703.07887)'
- en: '[88] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and Decision-Making
    for Autonomous Vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, vol. 1, 05 2018.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] W. Schwarting, J. Alonso-Mora, 和 D. Rus，“自主车辆的规划与决策，” *控制、机器人与自主系统年鉴*，第
    1 卷，2018年5月。'
- en: '[89] T. Gu, J. M. Dolan, and J. Lee, “Human-like Planning of Swerve Maneuvers
    for Autonomous Vehicles,” in *2016 IEEE Intelligent Vehicles Symposium (IV)*,
    June 2016, pp. 716–721.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] T. Gu, J. M. Dolan, 和 J. Lee，“人类般的自主车辆急转弯动作规划，” 在 *2016年IEEE智能车辆研讨会（IV）*，2016年6月，第
    716–721 页。'
- en: '[90] A. I. Panov, K. S. Yakovlev, and R. Suvorov, “Grid Path Planning with
    Deep Reinforcement Learning: Preliminary Results,” *Procedia Computer Science*,
    vol. 123, pp. 347 – 353, 2018, 8th Annual Int. Conf. on Biologically Inspired
    Cognitive Architectures, BICA 2017\. [Online]. Available: [http://www.sciencedirect.com/science/article/pii/S1877050918300553](http://www.sciencedirect.com/science/article/pii/S1877050918300553)'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. I. Panov, K. S. Yakovlev, 和 R. Suvorov，“基于深度强化学习的网格路径规划：初步结果，” *Procedia
    Computer Science*，第 123 卷，第 347 – 353 页，2018年，第8届生物启发认知架构国际会议，BICA 2017。 [在线]。可用：
    [http://www.sciencedirect.com/science/article/pii/S1877050918300553](http://www.sciencedirect.com/science/article/pii/S1877050918300553)'
- en: '[91] C. J. Ostafew, J. Collier, A. P. Schoellig, and T. D. Barfoot, “Learning-based
    Nonlinear Model Predictive Control to Improve Vision-based Mobile Robot Path Tracking,”
    *Journal of Field Robotics*, vol. 33, no. 1, pp. 133–152, 2015.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] C. J. Ostafew, J. Collier, A. P. Schoellig, 和 T. D. Barfoot，“基于学习的非线性模型预测控制以改善基于视觉的移动机器人路径跟踪，”
    *Journal of Field Robotics*，第 33 卷，第 1 期，第 133–152 页，2015年。'
- en: '[92] P. J. Nguyen-Tuong D and S. M, “Local Gaussian Process Regression for
    Real Time Online Model Learning,” in *Proceedings of the neural information processing
    systems Conference*, 2008, pp. 1193––1200.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] P. J. Nguyen-Tuong D 和 S. M， “实时在线模型学习的局部高斯过程回归”，见 *神经信息处理系统会议论文集*，2008年，页码
    1193–1200。'
- en: '[93] H. P. Meier F and S. S, “Efficient Bayesian Local Model Learning for Control,”
    in *IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016*.   IEEE,
    2014, pp. 2244––2249.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] H. P. Meier F 和 S. S， “高效的贝叶斯局部模型学习用于控制”，见 *IEEE/RSJ 智能机器人与系统国际会议 (IROS)
    2016*。IEEE，2014年，页码 2244–2249。'
- en: '[94] C. J. Ostafew, A. P. Schoellig, and T. D. Barfoot, “Robust Constrained
    Learning-Based NMPC Enabling Reliable Mobile Robot Path Tracking,” *Int. Journal
    of Robotics Research*, vol. 35, no. 13, pp. 1547–1563, 2016.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. J. Ostafew、A. P. Schoellig 和 T. D. Barfoot， “基于鲁棒约束学习的 NMPC 实现可靠的移动机器人路径跟踪”，*国际机器人研究杂志*，第35卷，第13期，页码
    1547–1563，2016年。'
- en: '[95] O. Sigaud, C. Salaün, and V. Padois, “On-line Regression Algorithms for
    Learning Mechanical Models of Robots: A Survey,” *Robotics and Autonomous Systems*,
    vol. 59, no. 12, pp. 1115–1129, Dec. 2011.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] O. Sigaud、C. Salaün 和 V. Padois， “用于学习机器人机械模型的在线回归算法：一项综述”，*机器人与自主系统*，第59卷，第12期，页码
    1115–1129，2011年12月。'
- en: '[96] C. Ostafew, A. Schoellig, and T. D. Barfoot, “Visual Teach and Repeat,
    Repeat, Repeat: Iterative Learning Control to Improve Mobile Robot Path Tracking
    in Challenging Outdoor Environments,” 11 2013, pp. 176–181.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Ostafew、A. Schoellig 和 T. D. Barfoot， “视觉教学与重复：迭代学习控制以改善移动机器人在挑战性户外环境中的路径跟踪”，2013年11月，页码
    176–181。'
- en: '[97] B. Panomruttanarug, “Application of Iterative Learning Control in Tracking
    a Dubin’s Path in Parallel Parking,” *Int. Journal of Automotive Technology*,
    vol. 18, no. 6, pp. 1099–1107, Dec 2017.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] B. Panomruttanarug， “迭代学习控制在平行停车中跟踪 Dubin 路径的应用”，*国际汽车技术杂志*，第18卷，第6期，页码
    1099–1107，2017年12月。'
- en: '[98] N. R. Kapania and J. C. Gerdes, “Path Tracking of Highly Dynamic Autonomous
    Vehicle Trajectories via Iterative Learning Control,” in *2015 American Control
    Conference (ACC)*, July 2015, pp. 2753–2758.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] N. R. Kapania 和 J. C. Gerdes， “通过迭代学习控制对高度动态自主车辆轨迹进行路径跟踪”，见 *2015 美国控制会议
    (ACC)*，2015年7月，页码 2753–2758。'
- en: '[99] Z. Yang, F. Zhou, Y. Li, and Y. Wang, “A Novel Iterative Learning Path-tracking
    Control for Nonholonomic Mobile Robots Against Initial Shifts,” *Int. Journal
    of Advanced Robotic Systems*, vol. 14, p. 172988141771063, 05 2017.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Z. Yang、F. Zhou、Y. Li 和 Y. Wang， “一种新型迭代学习路径跟踪控制方法，针对非完整移动机器人初始偏移”，*国际先进机器人系统杂志*，第14卷，页码
    172988141771063，2017年5月。'
- en: '[100] S. Lefèvre, A. Carvalho, and F. Borrelli, “A Learning-Based Framework
    for Velocity Control in Autonomous Driving,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 13, no. 1, pp. 32–42, Jan 2016.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Lefèvre、A. Carvalho 和 F. Borrelli， “用于自主驾驶的基于学习的速度控制框架”，*IEEE 自动化科学与工程学报*，第13卷，第1期，页码
    32–42，2016年1月。'
- en: '[101] S. Lefevre, A. Carvalho, and F. Borrelli, “Autonomous Car Following:
    A Learning-based Approach,” in *2015 IEEE Intelligent Vehicles Symposium (IV)*,
    June 2015, pp. 920–926.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Lefevre、A. Carvalho 和 F. Borrelli， “自主车辆跟随：一种基于学习的方法”，见 *2015 IEEE
    智能车辆研讨会 (IV)*，2015年6月，页码 920–926。'
- en: '[102] P. Drews, G. Williams, B. Goldfain, E. A Theodorou, and J. M Rehg, “Aggressive
    Deep Driving: Combining Convolutional Neural Networks and Model Predictive Control,”
    01 2017, pp. 133–142.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Drews、G. Williams、B. Goldfain、E. A. Theodorou 和 J. M. Rehg， “激进的深度驾驶：结合卷积神经网络与模型预测控制”，2017年1月，页码
    133–142。'
- en: '[103] P. Drews, G. Williams, B. Goldfain, E. A. Theodorou, and J. M. Rehg,
    “Aggressive Deep Driving: Model Predictive Control with a CNN Cost Model,” *CoRR*,
    vol. abs/1707.05303, 2017.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] P. Drews、G. Williams、B. Goldfain、E. A. Theodorou 和 J. M. Rehg， “激进的深度驾驶：具有
    CNN 成本模型的模型预测控制”，*CoRR*，第 abs/1707.05303 号卷，2017年。'
- en: '[104] U. Rosolia, A. Carvalho, and F. Borrelli, “Autonomous Racing using Learning
    Model Predictive Control,” in *2017 American Control Conference (ACC)*, May 2017,
    pp. 5115–5120.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] U. Rosolia、A. Carvalho 和 F. Borrelli， “使用学习模型预测控制的自主赛车”，见 *2017 美国控制会议
    (ACC)*，2017年5月，页码 5115–5120。'
- en: '[105] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. A. Theodorou, and
    B. Boots, “Learning Deep Neural Network Control Policies for Agile Off-Road Autonomous
    Driving,” 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Pan、C.-A. Cheng、K. Saigol、K. Lee、X. Yan、E. A. Theodorou 和 B. Boots，
    “用于敏捷越野自主驾驶的深度神经网络控制策略学习”，2017年。'
- en: '[106] Y. Pan, C. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Agile Off-Road Autonomous Driving Using End-to-End Deep Imitation Learning,”
    *Robotics: Science and Systems 2018*, 2018.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Pan、C. Cheng、K. Saigol、K. Lee、X. Yan、E. Theodorou 和 B. Boots， “使用端到端深度模仿学习的敏捷越野自主驾驶”，*机器人：科学与系统
    2018*，2018年。'
- en: '[107] J. Rawlings and D. Mayne, *Model Predictive Control: Theory and Design*.   Nob
    Hill Pub., 2009.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] J. Rawlings 和 D. Mayne，*模型预测控制：理论与设计*。Nob Hill Pub.，2009年。'
- en: '[108] M. Kamel, A. Hafez, and X. Yu, “A Review on Motion Control of Unmanned
    Ground and Aerial Vehicles Based on Model Predictive Control Techniques,” *Engineering
    Science and Military Technologies*, vol. 2, pp. 10–23, 03 2018.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. Kamel, A. Hafez, 和 X. Yu，“基于模型预测控制技术的无人地面和空中车辆运动控制综述”，*工程科学与军事技术*，第2卷，第10–23页，2018年3月。'
- en: '[109] M. Brunner, U. Rosolia, J. Gonzales, and F. Borrelli, “Repetitive Learning
    Model Predictive Control: An Autonomous Racing Example,” in *2017 IEEE 56th Annual
    Conference on Decision and Control (CDC)*, Dec 2017, pp. 2545–2550.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Brunner, U. Rosolia, J. Gonzales, 和 F. Borrelli，“重复学习模型预测控制：一个自主赛车示例”，收录于*2017年IEEE第56届决策与控制年会
    (CDC)*，2017年12月，第2545–2550页。'
- en: '[110] D. A. Pomerleau, “Alvinn: An autonomous Land Vehicle in a Neural Network,”
    in *Advances in neural information processing systems*, 1989, pp. 305–313.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. A. Pomerleau，“Alvinn：一个神经网络中的自主陆地车辆”，收录于*神经信息处理系统进展*，1989年，第305–313页。'
- en: '[111] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road Obstacle
    Avoidance through End-to-End Learning,” in *Advances in neural information processing
    systems*, 2006, pp. 739–746.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] U. Muller, J. Ben, E. Cosatto, B. Flepp, 和 Y. L. Cun，“通过端到端学习进行越野障碍物避免”，收录于*神经信息处理系统进展*，2006年，第739–746页。'
- en: '[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    and U. Muller, “Explaining How a Deep Neural Network Trained with End-to-End Learning
    Steers a Car,” *arXiv preprint arXiv:1704.07911*, 2017.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    和 U. Muller，“解释一个通过端到端学习训练的深度神经网络如何操控汽车”，*arXiv 预印本 arXiv:1704.07911*，2017年。'
- en: '[113] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-End Learning of Driving
    Models from Large-scale Video Datasets,” *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR)*, 2017.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] H. Xu, Y. Gao, F. Yu, 和 T. Darrell，“从大规模视频数据集中学习驾驶模型的端到端学习”，*IEEE计算机视觉与模式识别会议
    (CVPR)*，2017年。'
- en: '[114] H. M. Eraqi, M. N. Moustafa, and J. Honer, “End-to-end Deep Learning
    for Steering Autonomous Vehicles Considering Temporal Dependencies,” *arXiv preprint
    arXiv:1710.03804*, 2017.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] H. M. Eraqi, M. N. Moustafa, 和 J. Honer，“考虑时间依赖的端到端深度学习用于自主车辆的转向”，*arXiv
    预印本 arXiv:1710.03804*，2017年。'
- en: '[115] S. Hecker, D. Dai, and L. Van Gool, “End-to-End Learning of Driving Models
    with Surround-view Cameras and Route Planners,” in *European Conference on Computer
    Vision (ECCV)*, 2018.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] S. Hecker, D. Dai, 和 L. Van Gool，“使用全景摄像头和路线规划器进行驾驶模型的端到端学习”，收录于*欧洲计算机视觉会议
    (ECCV)*，2018年。'
- en: '[116] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, and J. K. Hedrick,
    “Learning a Deep Neural Net Policy for End-to-End Control of Autonomous Vehicles,”
    in *2017 American Control Conference (ACC)*, May 2017, pp. 4914–4919.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, 和 J. K. Hedrick，“学习一个深度神经网络策略用于端到端控制自主车辆”，收录于*2017年美国控制会议
    (ACC)*，2017年5月，第4914–4919页。'
- en: '[117] M. G. Bechtel, E. McEllhiney, and H. Yun, “DeepPicar: A Low-cost Deep
    Neural Network-based Autonomous Car,” in *The 24th IEEE Inter. Conf. on Embedded
    and Real-Time Computing Systems and Applications (RTCSA)*, August 2018, pp. 1–12.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] M. G. Bechtel, E. McEllhiney, 和 H. Yun，“DeepPicar：一个低成本的基于深度神经网络的自主汽车”，收录于*第24届IEEE国际嵌入式和实时计算系统与应用会议
    (RTCSA)*，2018年8月，第1–12页。'
- en: '[118] S. Yang, W. Wang, C. Liu, K. Deng, and J. K. Hedrick, “Feature Analysis
    and Selection for Training an End-to-End Autonomous Vehicle Controller Using the
    Deep Learning Approach,” *2017 IEEE Intelligent Vehicles Symposium*, vol. 1, 2017\.
    [Online]. Available: [http://arxiv.org/abs/1703.09744](http://arxiv.org/abs/1703.09744)'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. Yang, W. Wang, C. Liu, K. Deng, 和 J. K. Hedrick，“基于深度学习方法训练端到端自主车辆控制器的特征分析与选择”，*2017年IEEE智能车辆研讨会*，第1卷，2017年。[在线]。可用：[http://arxiv.org/abs/1703.09744](http://arxiv.org/abs/1703.09744)'
- en: '[119] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba,
    “End to End Learning for Self-Driving Cars,” *CoRR*, vol. abs/1604.07316, 2016\.
    [Online]. Available: [http://arxiv.org/abs/1604.07316](http://arxiv.org/abs/1604.07316)'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, 和 K. Zieba，“用于自驾车的端到端学习”，*CoRR*，第abs/1604.07316卷，2016年。[在线]。可用：[http://arxiv.org/abs/1604.07316](http://arxiv.org/abs/1604.07316)'
- en: '[120] L. Fridman, D. E. Brown, M. Glazer, W. Angell, S. Dodd, B. Jenik, J. Terwilliger,
    J. Kindelsberger, L. Ding, S. Seaman, H. Abraham, A. Mehler, A. Sipperley, A. Pettinato,
    L. Angell, B. Mehler, and B. Reimer, “MIT Autonomous Vehicle Technology Study:
    Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with
    Automation,” *IEEE Access 2017*, 2017\. [Online]. Available: [https://arxiv.org/abs/1711.06976](https://arxiv.org/abs/1711.06976)'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] L. Fridman, D. E. Brown, M. Glazer, W. Angell, S. Dodd, B. Jenik, J.
    Terwilliger, J. Kindelsberger, L. Ding, S. Seaman, H. Abraham, A. Mehler, A. Sipperley,
    A. Pettinato, L. Angell, B. Mehler, 和 B. Reimer，“MIT 自动驾驶技术研究：基于大规模深度学习的驾驶行为与自动化交互分析”，*IEEE
    Access 2017*，2017\. [在线]. 可用：[https://arxiv.org/abs/1711.06976](https://arxiv.org/abs/1711.06976)'
- en: '[121] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “DeepDriving: Learning
    Affordance for Direct Perception in Autonomous Driving,” *2015 IEEE Int. Conf.
    on Computer Vision (ICCV)*, pp. 2722–2730, 2015.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] C. Chen, A. Seff, A. L. Kornhauser, 和 J. Xiao，“DeepDriving：在自动驾驶中学习直接感知的可用性”，*2015
    IEEE 计算机视觉国际会议 (ICCV)*，第 2722–2730 页，2015。'
- en: '[122] E. Perot, M. Jaritz, M. Toromanoff, and R. D. Charette, “End-to-End Driving
    in a Realistic Racing Game with Deep Reinforcement Learning,” in *2017 IEEE Conf.
    on Computer Vision and Pattern Recognition Workshops (CVPRW)*, July 2017, pp.
    474–475.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] E. Perot, M. Jaritz, M. Toromanoff, 和 R. D. Charette，“在现实赛车游戏中使用深度强化学习进行端到端驾驶”，在*2017
    IEEE 计算机视觉与模式识别研讨会 (CVPRW)*，2017 年 7 月，第 474–475 页。'
- en: '[123] Wayve. (2018) Learning to Drive in a Day. [Online]. Available: [https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning](https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning)'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Wayve.（2018）一天内学习驾驶。[在线]. 可用：[https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning](https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning)'
- en: '[124] T. Zhang, G. Kahn, S. Levine, and P. Abbeel, “Learning Deep Control Policies
    for Autonomous Aerial Vehicles with MPC-guided Policy Search,” *2016 IEEE Int.
    Conf. on Robotics and Automation (ICRA)*, May 2016\. [Online]. Available: [http://dx.doi.org/10.1109/ICRA.2016.7487175](http://dx.doi.org/10.1109/ICRA.2016.7487175)'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] T. Zhang, G. Kahn, S. Levine, 和 P. Abbeel，“通过 MPC 指导的策略搜索学习自主飞行器的深度控制策略”，*2016
    IEEE 国际机器人与自动化会议 (ICRA)*，2016 年 5 月\. [在线]. 可用：[http://dx.doi.org/10.1109/ICRA.2016.7487175](http://dx.doi.org/10.1109/ICRA.2016.7487175)'
- en: '[125] T. Ferrel, “Engineering Safety-critical Systems in the 21st Century,”
    2010.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] T. Ferrel，“21 世纪的安全关键系统工程”，2010。'
- en: '[126] H. C. Burton S., Gauerhof L., “Making the Case for Safety of Machine
    Learning in Highly Automated Driving,” *Lecture Notes in Computer Science*, vol.
    10489, 2017.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] H. C. Burton S., Gauerhof L.，“为高度自动化驾驶中的机器学习安全性辩护”，*计算机科学讲义*，卷 10489，2017。'
- en: '[127] K. R. Varshney, “Engineering Safety in Machine Learning,” in *2016 Information
    Theory and Applications Workshop (ITA)*, Jan 2016, pp. 1–5.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. R. Varshney，“机器学习中的工程安全”，在*2016 信息论与应用研讨会 (ITA)*，2016 年 1 月，第 1–5
    页。'
- en: '[128] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, and
    D. Mané, “Concrete Problems in AI Safety,” *CoRR*, vol. abs/1606.06565, 2016.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, 和 D.
    Mané，“AI 安全中的具体问题”，*CoRR*，卷 abs/1606.06565，2016。'
- en: '[129] N. Möller, *The Concepts of Risk and Safety*.   Springer Netherlands,
    2012.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] N. Möller，*风险与安全的概念*。 Springer Netherlands，2012。'
- en: '[130] R. Salay, R. Queiroz, and K. Czarnecki, “An Analysis of ISO 26262: Using
    Machine Learning Safely in Automotive Software,” *CoRR*, vol. abs/1709.02435,
    2017\. [Online]. Available: [http://arxiv.org/abs/1709.02435](http://arxiv.org/abs/1709.02435)'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] R. Salay, R. Queiroz, 和 K. Czarnecki，“ISO 26262 的分析：在汽车软件中安全使用机器学习”，*CoRR*，卷
    abs/1709.02435，2017\. [在线]. 可用：[http://arxiv.org/abs/1709.02435](http://arxiv.org/abs/1709.02435)'
- en: '[131] S. Bernd, R. Detlev, E. Susanne, W. Ulf, B. Wolfgang, Patz, and Carsten,
    “Challenges in Applying the ISO 26262 for Driver Assistance Systems,” in *Schwerpunkt
    Vernetzung, 5\. Tagung Fahrerassistenz*, 2012.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] S. Bernd, R. Detlev, E. Susanne, W. Ulf, B. Wolfgang, Patz, 和 Carsten，“应用
    ISO 26262 进行驾驶辅助系统的挑战”，在*Schwerpunkt Vernetzung，第 5 次驾驶辅助系统研讨会*，2012。'
- en: '[132] R. Parasuraman and V. Riley, “Humans and Automation: Use, Misuse, Disuse,
    Abuse,” *Human Factors*, vol. 39, no. 2, pp. 230–253, 1997.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] R. Parasuraman 和 V. Riley，“人类与自动化：使用、误用、废弃、滥用”，*人因学*，卷 39，第 2 期，第 230–253
    页，1997。'
- en: '[133] F. Jose, *Safety-Critical Systems*, 2018.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] F. Jose，*安全关键系统*，2018。'
- en: '[134] H. Daumé, III and D. Marcu, “Domain Adaptation for Statistical Classifiers,”
    *J. Artif. Int. Res.*, vol. 26, no. 1, pp. 101–126, May 2006.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] H. Daumé III 和 D. Marcu，“统计分类器的领域适应”，*人工智能研究杂志*，卷 26，第 1 期，第 101–126
    页，2006 年 5 月。'
- en: '[135] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad, “Intelligible
    Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission,”
    in *Proceedings of the 21th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data
    Mining*, 2015, pp. 1721–1730.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm 和 N. Elhadad，“透明模型用于医疗保健：预测肺炎风险和医院30日再入院”，收录于
    *第21届ACM SIGKDD 国际知识发现和数据挖掘会议文集*，2015年, 第1721-1730页.'
- en: '[136] K. R. Varshney and H. Alemzadeh, “On the Safety of Machine Learning:
    Cyber-Physical Systems, Decision Sciences, and Data Products,” *Big data*, vol. 5,
    10 2016.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. R. Varshney 和 H. Alemzadeh，“关于机器学习的安全性：网络实体系统、决策科学和数据产品”， *大数据(Big
    data)*，第5卷, 2016年10月.'
- en: '[137] S. Levin, “Tesla Fatal Crash: ’Autopilot’ Mode Sped up Car Before Driver
    Killed, Report Finds,” *The Guardian*, 2018.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] S. Levin，“特斯拉致命撞车：事故发生前''自动驾驶''模式加速汽车，报告称”， *卫报(The Guardian)*，2018年.'
- en: '[138] P. Koopman, “Challenges in Autonomous Vehicle Validation: Keynote Presentation
    Abstract,” in *Proceedings of the 1st Int. Workshop on Safe Control of Connected
    and Autonomous Vehicles*, 2017.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] P. Koopman，“自动驾驶汽车验证中的挑战：主题演讲摘要”，收录于 *第1届国际安全控制联网和自主车辆研讨会文集*，2017年.'
- en: '[139] Z. Kurd, T. Kelly, and J. Austin, “Developing Artificial Neural Networks
    for Safety Critical Systems,” *Neural Computing and Applications*, vol. 16, no. 1,
    pp. 11–19, Jan 2007.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Z. Kurd, T. Kelly 和 J. Austin，“发展用于安全关键系统的人工神经网络”， *神经计算与应用(Neural Computing
    and Applications)*，第16卷, 第1期, 2007年, 第11-19页.'
- en: '[140] M. Harris, “Google Reports Self-driving Car Mistakes: 272 Failures and
    13 Near Misses,” *The Guardian*, 2016.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] M. Harris，“谷歌报告自动驾驶汽车失误：272次失败和13次险情”， *卫报(The Guardian)*，2016年.'
- en: '[141] J. McPherson, “How Uber’s Self-Driving Technology Could Have Failed In
    The Fatal Tempe Crash,” *Forbes*, 2018.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. McPherson，“Uber自动驾驶技术在致命的Tempe撞车中可能发生故障”， *福布斯(Forbes)*，2018年.'
- en: '[142] A. Chakarov, A. Nori, S. Rajamani, S. Sen, and D. Vijaykeerthy, “Debugging
    Machine Learning Tasks,” *arXiv preprint arXiv:1603.07292*, 2018.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] A. Chakarov, A. Nori, S. Rajamani, S. Sen 和 D. Vijaykeerthy，“调试机器学习任务”，
    *arXiv preprint arXiv:1603.07292*, 2018年.'
- en: '[143] B. Nushi, E. Kamar, E. Horvitz, and D. Kossmann, “On Human Intellect
    and Machine Failures: Troubleshooting Integrative Machine Learning Systems,” in
    *AAAI*, 2017.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] B. Nushi, E. Kamar, E. Horvitz 和 D. Kossmann，“关于人类智能和机器失败：故障排除综合机器学习系统”，收录于
    *AAAI*，2017年.'
- en: '[144] I. Takanami, M. Sato, and Y. P. Yang, “A Fault-value Injection Approach
    for Multiple-weight-fault Tolerance of MNNs,” in *Proceedings of the IEEE-INNS-ENNS*,
    2000, pp. 515–520 vol.3.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] I. Takanami, M. Sato 和 Y. P. Yang，“一种多重权重故障容错MNNs的故障值注入方法”，收录于 *IEEE-INNS-ENNS文集*，2000年,
    第515-520卷, 第3页.'
- en: '[145] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer,
    “Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,” in *CAV*,
    2017.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] G. Katz, C. W. Barrett, D. L. Dill, K. Julian 和 M. J. Kochenderfer，“Reluplex：一种用于验证深度神经网络的高效SMT
    Solver”，收录于 *CAV*，2017年.'
- en: '[146] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A multimodal Dataset for Autonomous
    Driving,” *arXiv preprint arXiv:1903.11027*, 2019.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan 和 O. Beijbom，“nuScenes：一个用于自动驾驶的多模态数据集”， *arXiv preprint arXiv:1903.11027*,
    2019年.'
- en: '[147] H. Yin and C. Berger, “When to Use what Data Set for Your Self-driving
    Car Algorithm: An Overview of Publicly Available Driving Datasets,” in *2017 IEEE
    20th Int. Conf. on Intelligent Transportation Systems (ITSC)*, Oct 2017, pp. 1–8.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] H. Yin 和 C. Berger, “何时选择什么数据集用于你的自动驾驶车算法：公开可用驾驶数据集概述”，收录于*2017 IEEE
    第20届智能交通系统国际会议 (ITSC)*, 2017年10月, 第1-8页.'
- en: '[148] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
    “BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling,”
    *CoRR*, vol. abs/1805.04687, 2018.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan 和 T. Darrell，“BDD100K：一个具备可扩展注释工具的多样化驾驶视频数据库”，*CoRR*，第abs/1805.04687卷,
    2018年.'
- en: '[149] P. Koschorrek, T. Piccini, P. Öberg, M. Felsberg, L. Nielsen, and R. Mester,
    “A Multi-sensor Traffic Scene Dataset with Omnidirectional Video,” in *Ground
    Truth - What is a good dataset? CVPR Workshop 2013*, 2013.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] P. Koschorrek, T. Piccini, P. Öberg, M. Felsberg, L. Nielsen 和 R. Mester，“具有全向视觉视频的多传感器交通场景数据集”，收录于
    *Ground Truth - What is a good dataset? CVPR Workshop 2013*，2013年.'
- en: '[150] G. Pandey, J. R. McBride, and R. M. Eustice, “Ford Campus Vision and
    Lidar Data Set ,” *Int. Journal of Robotics Research*, vol. 30, no. 13, pp. 1543–1552,
    2011.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] G. Pandey, J. R. McBride 和 R. M. Eustice，“Ford 校园视觉和激光雷达数据集”， *国际机器人学研究期刊(Int.
    Journal of Robotics Research)*，第30卷, 第13期, 2011年, 第1543-1552页.'
- en: '[151] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision Meets Robotics:
    The KITTI Dataset,” *The Int. Journal of Robotics Research*, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun，“视觉遇见机器人：KITTI 数据集”，*国际机器人研究期刊*，第32卷，第11期，第1231–1237页，2013年。'
- en: '[152] Udacity, “Udacity Data Collection,” [http://academictorrents.com/collection/self-driving-cars](http://academictorrents.com/collection/self-driving-cars),
    2018.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Udacity，“Udacity 数据收集”，[http://academictorrents.com/collection/self-driving-cars](http://academictorrents.com/collection/self-driving-cars)，2018年。'
- en: '[153] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *The Int. Journal of Robotics Research (IJRR)*, vol. 36,
    no. 1, pp. 3–15, 2017.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] W. Maddern, G. Pascoe, C. Linegar, 和 P. Newman，“1年，1000公里：牛津机器人车数据集”，*国际机器人研究期刊
    (IJRR)*，第36卷，第1期，第3–15页，2017年。'
- en: '[154] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic Object Classes
    in Video: A High-definition Ground Truth Database,” *Pattern Recognition Letters*,
    vol. 30, pp. 88–97, 2009.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] G. J. Brostow, J. Fauqueur, 和 R. Cipolla，“视频中的语义对象类别：高清真值数据库”，*模式识别快报*，第30卷，第88–97页，2009年。'
- en: '[155] F. Flohr and D. M. Gavrila, “Daimler Pedestrian Segmentation Benchmark
    Dataset,” in *Proc. of the British Machine Vision Conference*, 2013.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] F. Flohr 和 D. M. Gavrila，“戴姆勒行人分割基准数据集”，发表于 *英国机器视觉会议论文集*，2013年。'
- en: '[156] P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection:
    A Benchmark,” in *2009 IEEE Conf. on Computer Vision and Pattern Recognition*,
    2009, pp. 304–311.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] P. Dollar, C. Wojek, B. Schiele, 和 P. Perona，“行人检测：一个基准”，发表于 *2009 IEEE
    计算机视觉与模式识别会议*，2009年，第304–311页。'
- en: '[157] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid Scene Parsing Network,”
    in *2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2017,
    pp. 6230–6239.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia，“金字塔场景解析网络”，发表于 *2017 IEEE
    计算机视觉与模式识别会议 (CVPR)*，2017年，第6230–6239页。'
- en: '[158] S. Liu, J. Jia, S. Fidler, and R. Urtasun, “SGN: Sequential Grouping
    Networks for Instance Segmentation,” pp. 3516–3524, 10 2017.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] S. Liu, J. Jia, S. Fidler, 和 R. Urtasun，“SGN：用于实例分割的序列分组网络”，第3516–3524页，2017年10月。'
- en: '[159] X. Li, F. Flohr, Y. Yang, H. Xiong, M. Braun, S. Pan, K. Li, and D. M.
    Gavrila, “A New Benchmark for Vision-based Cyclist Detection,” in *2016 IEEE Intelligent
    Vehicles Symposium (IV)*, 2016, pp. 1028–1033.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] X. Li, F. Flohr, Y. Yang, H. Xiong, M. Braun, S. Pan, K. Li, 和 D. M.
    Gavrila，“基于视觉的自行车检测的新基准”，发表于 *2016 IEEE 智能车辆研讨会 (IV)*，2016年，第1028–1033页。'
- en: '[160] Velodyne, “Velodyne LiDAR for Data Collection,” [https://velodynelidar.com/](https://velodynelidar.com/),
    2018.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Velodyne，“Velodyne LiDAR用于数据收集”，[https://velodynelidar.com/](https://velodynelidar.com/)，2018年。'
- en: '[161] Sick, “Sick LiDAR for Data Collection,” [https://www.sick.com/](https://www.sick.com/),
    2018.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Sick，“Sick LiDAR用于数据收集”，[https://www.sick.com/](https://www.sick.com/)，2018年。'
- en: '[162] NVIDIA, “NVIDIA AI Car Computer Drive PX,” https://www.nvidia.com/en-au/self-driving-cars/drive-px/.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] NVIDIA，“NVIDIA AI 车载计算机 Drive PX”，https://www.nvidia.com/en-au/self-driving-cars/drive-px/。'
- en: '[163] ——, “Tegra X2,” https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] ——，“Tegra X2”，https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/。'
- en: '[164] ——, “Denver Core,” https://en.wikichip.org/wiki/nvidia/microarchitectures/denver.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] ——，“Denver Core”，https://en.wikichip.org/wiki/nvidia/microarchitectures/denver。'
- en: '[165] ——, “Pascal Microarchitecture,” https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] ——，“Pascal 微架构”，https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/。'
- en: '[166] ——, “NVIDIA Drive AGX,” https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] ——，“NVIDIA Drive AGX”，https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/。'
- en: '[167] ——, “NVIDIA Volta,” https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] ——，“NVIDIA Volta”，https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/。'
- en: '[168] Renesas, “R-Car V3H,” https://www.renesas.com/eu/en/solutions/automotive/soc/r-car-v3h.html/.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Renesas，“R-Car V3H”，https://www.renesas.com/eu/en/solutions/automotive/soc/r-car-v3h.html/。'
- en: '[169] ——, “R-Car H3,” https://www.renesas.com/sg/en/solutions/automotive/soc/r-car-h3.html/.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] ——，“R-Car H3”，https://www.renesas.com/sg/en/solutions/automotive/soc/r-car-h3.html/。'
- en: '[170] E. Nurvitadhi, G. Venkatesh, J. Sim, D. Marr, R. Huang, J. Ong Gee Hock,
    Y. T. Liew, K. Srivatsan, D. Moss, S. Subhaschandra, and G. Boudoukh, “Can FPGAs
    Beat GPUs in Accelerating Next-Generation Deep Neural Networks?” in *Proceedings
    of the 2017 ACM/SIGDA Int. Symposium on Field-Programmable Gate Arrays*, ser.
    FPGA ’17.   New York, NY, USA: ACM, 2017, pp. 5–14\. [Online]. Available: [http://doi.acm.org/10.1145/3020078.3021740](http://doi.acm.org/10.1145/3020078.3021740)'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] E. Nurvitadhi, G. Venkatesh, J. Sim, D. Marr, R. Huang, J. Ong Gee Hock,
    Y. T. Liew, K. Srivatsan, D. Moss, S. Subhaschandra, 和 G. Boudoukh，“FPGA能否在加速下一代深度神经网络方面超过GPU？”发表于
    *2017年ACM/SIGDA国际现场可编程门阵列研讨会论文集*，系列FPGA ’17。纽约，NY，USA: ACM，2017年，第5–14页。 [在线].
    可用: [http://doi.acm.org/10.1145/3020078.3021740](http://doi.acm.org/10.1145/3020078.3021740)'
- en: '[171] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. Strauss, and E. Chung,
    “Accelerating Deep Convolutional Neural Networks Using Specialized Hardware,”
    February 2015.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. Strauss, 和 E. Chung，“利用专用硬件加速深度卷积神经网络”，2015年2月。'
- en: '[172] J. Cong, Z. Fang, M. Lo, H. Wang, J. Xu, and S. Zhang, “Understanding
    Performance Differences of FPGAs and GPUs: (Abtract Only),” in *Proceedings of
    the 2018 ACM/SIGDA Int. Symposium on Field-Programmable Gate Arrays*, ser. FPGA
    ’18.   New York, NY, USA: ACM, 2018, pp. 288–288\. [Online]. Available: [http://doi.acm.org/10.1145/3174243.3174970](http://doi.acm.org/10.1145/3174243.3174970)'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Cong, Z. Fang, M. Lo, H. Wang, J. Xu, 和 S. Zhang，“理解FPGA和GPU的性能差异：（仅摘要）”，发表于
    *2018年ACM/SIGDA国际现场可编程门阵列研讨会论文集*，系列FPGA ’18。纽约，NY，USA: ACM，2018年，第288–288页。 [在线].
    可用: [http://doi.acm.org/10.1145/3174243.3174970](http://doi.acm.org/10.1145/3174243.3174970)'
- en: '[173] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object Detection with Deep
    Learning: A Review,” *IEEE transactions on neural networks and learning systems*,
    2018.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z.-Q. Zhao, P. Zheng, S.-t. Xu, 和 X. Wu，“基于深度学习的目标检测：综述”，*IEEE神经网络与学习系统汇刊*，2018年。'
- en: '[174] C. J. Ostafew, “Learning-based Control for Autonomous Mobile Robots,”
    Ph.D. dissertation, University of Toronto, 2016.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] C. J. Ostafew, “基于学习的自主移动机器人控制”，博士学位论文，多伦多大学，2016年。'
