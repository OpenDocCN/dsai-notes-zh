<!--yml

分类：未分类

日期：2024-09-06 20:06:52

-->

# [1812.07127] 深度强化学习在搜索、推荐和在线广告中的应用：综述

> 来源：[`ar5iv.labs.arxiv.org/html/1812.07127`](https://ar5iv.labs.arxiv.org/html/1812.07127)

\newsletterQuarter

春季 \newsletterYear2019

# 深度强化学习在搜索、推荐和在线广告中的应用：综述

相宇赵    密歇根州立大学

龙夏    京东

季良唐    密歇根州立大学

大伟尹    京东

###### 摘要

搜索、推荐和在线广告是网络上最重要的三种信息提供机制。这些信息获取技术通过在适当的时间和地点向用户推荐个性化的对象（信息或服务）来满足用户的信息需求，在缓解信息过载问题中发挥了关键作用。随着深度强化学习（DRL）的巨大进展，基于 DRL 的信息获取技术受到越来越多的关注。这些基于 DRL 的技术有两个关键优势——（1）它们能够根据用户的实时反馈不断更新信息获取策略，和（2）它们可以最大化用户的期望累积长期奖励，其中奖励根据信息获取应用的不同定义，如点击率、收入、用户满意度和参与度。本文概述了深度强化学习在搜索、推荐和在线广告中的应用，从方法论到应用，回顾了代表性的算法，并讨论了一些有吸引力的研究方向。

## 1 简介

万维网的爆炸性增长产生了大量数据。因此，信息超载问题变得越来越严重[Chang et al. (2006)]。因此，如何在适当的时间和地点识别满足用户信息需求的对象变得越来越重要，这促使了三种代表性的信息获取机制——搜索、推荐和在线广告。搜索机制输出与查询匹配的对象，推荐机制生成一组匹配用户隐含偏好的项目，而在线广告机制类似于搜索和推荐，但所呈现的对象是广告[Garcia-Molina et al. (2011)]。在设计这些信息获取机制的智能方法上已经做了大量工作。然而，传统技术往往面临几个共同的挑战。首先，大多数现有方法将信息获取视为一个静态任务，并按照固定的贪婪策略生成对象。这可能无法捕捉用户偏好的动态特性（或环境）。其次，大多数传统方法旨在最大化短期奖励，而完全忽视了建议的对象是否会在长期奖励中贡献更多[Shani et al. (2005)]。注意，奖励在信息获取任务中有不同的定义，如点击率（CTR）、收入和停留时间。

近年来，强化学习（RL）技术迅速发展，并应用于各种领域。在 RL 框架下，我们通过与动态环境的互动来获取经验，进而解决复杂问题。结果是得到一个**最优策略**，可以在没有任何具体指示的情况下解决复杂任务[Kaelbling et al. (1996)]。采用 RL 进行信息获取可以自然地解决上述挑战。首先，将信息获取任务视为 RL 代理（系统）与用户（环境）之间的顺序互动，代理可以根据用户的实时反馈不断更新其策略，直到系统收敛到生成最符合用户动态偏好的对象的最优策略。其次，RL 框架的设计旨在最大化来自用户的长期累积奖励。因此，代理可以识别那些即时奖励小但对长期奖励贡献大的对象。

由于强化学习的优势，开发基于 RL 的信息检索技术引起了极大的兴趣。因此，从强化学习的角度提供信息检索技术的概述是及时且必要的。在本次调查中，我们提供了最先进的基于 RL 的信息检索技术的全面概述，并讨论了一些未来的研究方向。调查的其余部分组织如下。在第二部分中，我们介绍了基于强化学习的信息检索技术的技术基础。接着，我们回顾了三个关键的信息检索任务——搜索、推荐和在线广告——以及第 3 到第五部分中的代表性算法。最后，我们总结了几项未来的研究方向。

## 2 技术基础

强化学习是学习如何将情况映射到动作 [Sutton and Barto (1998)]。RL 的两个基本元素是制定情况（数学模型）和学习映射（策略学习）。

### 2.1 问题表述

在强化学习中，问题的表述有两个主要设置：多臂老虎机（没有状态转移）和马尔可夫决策过程（有状态转移）。

#### 2.1.1 多臂老虎机

多臂老虎机（MABs）问题是探索/利用权衡的一个简单模型 [Varaiya and Walrand (1983)]。正式地，一个 $K$-MAB 可以定义如下。

###### 定义 2.1.

一个 $K$-MAB 是一个三元组 $\langle A, R, \pi \rangle$，其中 $A$ 是动作（臂）的集合，$|A|=K$，$r=R(a)$ 是执行动作 $a$ 时的奖励分布，策略 $\pi$ 描述了对可能动作的概率分布。

具有最高期望奖励的臂称为最佳臂（记作 $a_{*}$），其期望奖励 $r_{*}$ 是最优奖励。MAB 的一个算法在每个时间步 $t$ 时，采样一个臂 $a_{t}$ 并接收一个奖励 $r_{t}$。在做出选择时，算法依赖于历史（即，动作和奖励）直到时间 $t$-$1$。上下文多臂老虎机模型（即关联老虎机或带有附加信息的老虎机）是 MAB 的一种扩展，考虑了额外的信息 [Auer et al. (2002), Lu et al. (2010)]。

#### 2.1.2 马尔可夫决策过程

马尔可夫决策过程（MDP）是顺序决策的经典形式化，是一种数学理想化的强化学习问题形式 [Bellman (2013)]。我们定义一个 MDP 如下。

###### 定义 2.2.

马尔可夫决策过程是一个 5 元组 $\langle S,A,T,R,\pi\rangle$，其中 $S$ 是状态集合，$A$ 是离散的动作集合，$T$ 是状态转移函数 $s_{t+1}=T(s_{t},a_{t})$，它指定了一个函数，将状态 $s_{t}$ 映射到新的状态 $s_{t+1}$，以响应选择的动作 $a_{t}$，$r=R(s,a)$ 是在状态 $s$ 执行动作 $a$ 时的奖励分布，而策略 $\pi(a|s)$ 描述了代理的行为，它是一个可能动作的概率分布。

代理和环境在每个离散时间步 $t=\{0,1,2,\dots\}$ 进行交互。因此，会生成一个序列或轨迹 $\{s_{0},a_{0},r_{1},\cdots,s_{t},a_{t},$

$r_{t+1},\cdots\}$。一般来说，我们寻求最大化期望折扣回报，其中回报 $G_{t}$ 被定义为：$G_{t}=\sum_{k=0}^{\infty}\gamma^{k}r_{r+k+1}$，其中 $\gamma$ ($0\leq\gamma\leq 1$) 是折扣率。部分可观测马尔可夫决策过程（POMDP）是对 MDP 的扩展，用于系统状态不一定可观察的情况[Åström (1965), Smallwood 和 Sondik (1973), Sondik (1978), Kaelbling 等 (1998)]。

#### 2.1.3 多代理设置

马尔可夫决策过程推广到多代理情况的泛化是随机游戏[Bowling 和 Veloso (2002), Shoham 等 (2003), Busoniu 等 (2008)]。

###### 定义 2.3。

多代理游戏是一个元组 $\langle S,A_{1},\dots,A_{n},T,R_{1},\dots,R_{n},\pi_{1},\dots,\pi_{n}\rangle$，其中 $n$ 是代理的数量，$S$ 是环境状态的离散集合，$A_{i}$ 是代理 $i$ 的离散动作集合，$T$ 是状态转移概率函数，$R_{i}$ 是代理 $i$ 的奖励函数，$\pi_{i}$ 是代理 $i$ 采用的策略。

在多代理游戏中，状态转移是所有代理联合动作的结果 $\mathbf{a}_{t}=[a_{1,t}^{T},\dots,a_{n,t}^{T}]^{T}$，其中 $a_{i,t}\in A_{i}$ 表示代理 $i$ 在时间步 $t$ 采取的动作。奖励 $r_{i,k+1}$ 也依赖于联合动作。如果 $\pi_{1}=\cdots=\pi_{n}$，即所有代理采用相同的策略以最大化相同的期望回报，则多代理游戏是完全合作的。如果 $n=2$ 且 $\pi_{1}=-\pi_{2}$，即两个代理有相反的策略，则游戏是完全竞争的。混合游戏是既非完全合作也非完全竞争的随机游戏。

### 2.2 策略学习

强化学习是一类学习问题，其中代理（或多代理）的目标是找到优化其长期性能某些度量的策略。强化学习的解决方案可以从不同的角度进行分类。在这里，我们从两个角度来研究它们：是否有完整的模型以及寻找最佳策略的方法。

#### 2.2.1 基于模型 vs. 无模型

强化学习算法中，那些明确学习系统模型并利用这些模型解决 MDP 问题的方法称为基于模型的方法。基于模型的 RL 受到控制理论的强烈影响，通常在不同学科中进行解释。这些方法包括流行的算法，如 Dyna [Sutton (1991)]、优先级扫掠 [Moore and Atkeson (1993)]、Q-迭代 [Busoniu et al. (2010)]、策略梯度（PG） [Williams (1992)] 和 PG 的变体 [Baxter and Bartlett (2001), Kakade (2001)]。无模型的方法忽略模型，直接专注于通过与环境的互动来确定价值函数。为了实现这一点，这些方法严重依赖于采样和观察；因此，它们不需要了解系统的内部工作。一些这些方法的例子包括 Q-learning [Kröse (1995)]、SARSA [Rummery and Niranjan (1994)]、LSPI [Lagoudakis and Parr (2003)] 和 Actor-Critic [Konda and Tsitsiklis (1999)]。

#### 2.2.2 价值函数与策略搜索

那些首先找到最优价值函数然后提取最优策略的算法是价值函数方法，例如 Dyna、Q-learning、SARSA 和 DQN [Mnih et al. (2015)]。另一类方法是策略搜索方法，它们通过直接在策略空间中搜索来解决 MDP 问题。一个重要的策略搜索方法是策略梯度（PG）算法 [Williams (1992), Baxter and Bartlett (2001), Kakade (2001), Deisenroth and Rasmussen (2011)]。这些方法旨在直接建模和优化策略。策略通常用一个相对于 $\pi_{\theta}(a|s)$ 的参数化函数来建模。奖励（目标）函数的值取决于这个策略，然后可以应用各种算法来优化 $\theta$ 以获得最佳奖励。有一系列算法使用 PG 在策略空间中进行搜索，同时估计价值函数。这些方法的重要类别是 Actor-Critic（AC）及其变体 [Konda and Tsitsiklis (1999), Peters et al. (2005), Peters and Schaal (2008), Bhatnagar et al. (2007), Bhatnagar et al. (2009)]。这些是双时间尺度算法，其中评论员使用线性近似架构的时间差分（TD）学习，演员则基于评论员提供的信息在近似梯度方向上进行更新。

## 3 强化学习在搜索中的应用

搜索的目的是根据用户查询 [(86)] 找到并排序一组对象（例如，文档、记录）。在本节中，我们回顾了 RL 在搜索关键主题中的应用。

### 3.1 查询理解

查询理解是搜索引擎了解用户信息需求的主要任务。这对提高一般搜索相关性、用户体验以及帮助用户完成任务有潜在的帮助[Croft et al. (2010)]。在[Nogueira and Cho (2017)]中，强化学习（RL）被用于解决查询重构任务：提出了一种基于神经网络的查询重构框架，该框架重写查询以最大化返回相关文档的数量。在提出的框架中，搜索引擎被视为一个黑箱，代理学习如何使用它以检索更多相关项目，这为训练代理用于原本不打算用于的任务提供了可能性。此外，还估计了在给定环境下基于 RL 的模型的上限性能。在[Nogueira et al. (2018)]中，介绍了一种基于多代理的方法来高效学习多样化的查询重构。认为训练多个子代理比训练一个通用代理更容易，因为每个子代理只需学习在一部分示例中表现良好的策略。在提出的框架中，一个代理由多个专门的子代理和一个学习整合子代理答案以生成最终答案的元代理组成。因此，该方法通过并行处理加快了学习速度。

### 3.2 排名

相关性排序是信息检索的核心问题[Yin et al. (2016)]，而学习排序（LTR）是相关性排序中的关键技术。在 LTR 中，直接优化排序评估指标的方法具有代表性，并且已被证明有效[Yue et al. (2007), Xu and Li (2007), Xu et al. (2008)]。这些方法通常只优化在预定义排序位置计算的评估指标，例如在[Xu and Li (2007)]中计算的 NDCG 在排名$K$的位置。排名$K$之后的文档信息被忽略。为了解决这个问题，在[Zeng et al. (2017)]中，提出了一种基于马尔可夫决策过程的 LTR 模型 MDPRank，该模型能够利用所有排名位置计算的指标。奖励函数是基于信息检索评估指标定义的，模型参数可以通过最大化所有决策的累计奖励来学习。隐式相关反馈指的是搜索引擎和用户之间的互动过程，已被证明对提高检索准确性非常有效[Lv and Zhai (2009)]。Bandits 和 MDPs 都能自然地建模这种互动过程[Vorobev et al. (2015), Katariya et al. (2016), Katariya et al. (2017)]。在[Kveton et al. (2015)]中，引入了级联 Bandits 来识别最吸引人的项目，代理的目标是最大化其相对于最吸引人项目列表的总奖励。通过保持状态转移，MDP 能够建模用户与搜索引擎互动中的用户状态。在[Zeng et al. (2018)]中，互动过程被形式化为 MDP，并应用了递归神经网络来处理反馈。

除了相关性排名之外，另一个重要目标是为查询提供涵盖广泛主题的搜索结果，即搜索结果的多样化[Santos et al. (2015), Xu et al. (2017)]。典型方法将构建多样化排名的问题形式化为贪心的顺序文档选择过程。为了为一个位置选择最佳文档，多样化排名模型需要捕捉用户从之前文档中感知到的信息的效用。为了明确建模用户感知的效用，多样化排名的构建被形式化为一个顺序决策过程，并将其建模为连续状态马尔可夫决策过程，称为 MDP-DIV[Xia et al. (2017)]。$M$个文档的排名被形式化为$M$个决策的序列，每个动作对应于从候选集中选择一个文档。在参数训练阶段，采用了 REINFORCE 的策略梯度算法，并最大化了在多样性评价指标下的期望长期折扣奖励。有关多样性排名的更多工作见[Feng et al. (2018), Kapoor et al. (2018)]。

### 3.3 整页优化

为了提升用户体验，现代搜索引擎从不同的垂直领域（如网页、新闻、图片、视频、购物、知识卡片、本地地图等）汇总多样化的结果。页面展示被广泛定义为在搜索结果页面（SERP）上呈现一组项目的策略，这比简单的排名列表要表达得丰富得多。为异质结果的画廊找到合适的展示方式对现代搜索引擎至关重要。有效学习优化大决策空间的一种方法是分数因子设计。然而，这种方法可能会导致在大搜索空间中出现组合爆炸问题。在[Hill et al. (2017)]中，采用了多臂老虎机（bandit）模型来高效探索布局空间，并使用爬山算法实时选择最佳内容。该模型通过仅考虑页面组件之间的成对交互，避免了模型复杂性的组合爆炸。这种方法是一种贪心的交替优化策略，可以在线实时运行。在[Wang et al. (2016), Wang et al. (2018)]中，提出了一个框架来学习最佳页面展示，将异质结果呈现到 SERP 上。该框架利用了 MDP 设置，并将代理设计为一个算法，用于确定每个搜索查询在 SERP 上的页面内容展示。为了解决关键的效率问题，提出了一种基于策略的学习方法，可以迅速从高维空间中选择动作。

### 3.4 会话搜索

任务导向的搜索包括一系列由查询重构触发的搜索迭代。会话搜索中观察到马尔可夫链的现象：用户在前一次迭代中的搜索结果判断会影响用户在下一次搜索迭代中的行为。会话搜索被建模为基于部分可观察马尔可夫决策过程（POMDP）的双代理随机博弈模型，见[Luo et al. (2014)]。他们将会话搜索中的动态数学建模为用户与搜索引擎之间的合作博弈，而用户与搜索引擎共同协作，以共同最大化长期累积奖励。基于日志的文档重排序是一种特殊类型的会话搜索，它根据历史搜索日志对文档进行重排序，其中包括目标用户的个性化查询日志和其他用户的搜索活动。重排序旨在提供初始检索文档的更好排序，见[Zhang et al. (2014)]。如今，深度强化学习技术已被应用于电子商务搜索引擎中[Hu et al. (2018), Feng et al. (2018)]。为了更好地利用不同排序步骤之间的相关性，强化学习被用于学习一种最优排序策略，该策略最大化搜索会话中的期望累积奖励[Hu et al. (2018)]。它正式将搜索会话中的多步骤排序问题定义为 MDP，记作 SSMDP，并提出了一种新颖的策略梯度算法，用于学习一种最优排序策略，该算法能够处理高奖励方差和不平衡奖励分布的问题。在[Feng et al. (2018)]中，多场景排序被形式化为一个完全合作、部分可观察的多代理序列决策问题，记作 MA-RDPG。MA-RDPG 具有一个用于传递信息的通信组件，若干个用于执行排序操作的私有代理，以及一个用于评估合作代理整体表现的集中式评论者。代理通过共享全局动作价值函数和传递编码跨场景历史信息的消息来相互协作。

## 4 强化学习在推荐中的应用

推荐系统的目标是根据用户的反馈（或行为，如评分和评论）捕捉用户的偏好，并推荐符合其偏好的项目。在本节中，我们简要回顾了强化学习如何在推荐的几个关键任务中得到应用。

### 4.1 利用/探索困境

传统的推荐系统面临利用-探索困境，其中利用是指推荐预测最符合用户偏好的项目，而探索是指随机推荐项目以收集更多用户反馈。上下文赌博机模型的代理尝试在竞争的利用和探索任务之间取得平衡，以最大化在考虑的时间段内的累计长期奖励。在赌博机设置中平衡利用和探索的传统策略包括$\epsilon$-贪婪 [Watkins (1989)]、EXP3 [Auer et al. (2002)] 和 UCB1 [Auer et al. (2002)]。在新闻推送场景中，个性化新闻推荐的探索/利用问题被建模为上下文赌博机问题 [Li et al. (2010)]，并提出了一种学习算法 LinUCB，该算法根据用户和文章的上下文信息，顺序选择文章以最大化总用户点击量。

### 4.2 时间动态

现有的大多数推荐系统，如协同过滤、基于内容的推荐和学习排序，已经在静态环境（奖励）假设下进行了广泛研究，其中用户的偏好被假定为静态。然而，这一假设在现实中通常不成立，因为用户的偏好是动态的，因此奖励分布通常会随时间变化。在多臂赌博机设置中，通常引入一个变量奖励函数来描绘环境的动态特性。例如，基于粒子的学习动态上下文漂移模型被提出用于建模多臂赌博机问题中的奖励映射函数的变化，其中奖励映射函数的漂移被学习为一组随机游走粒子，并动态选择拟合良好的粒子来描述映射函数[Zeng et al. (2016)]。提出了一种上下文多臂赌博机算法，通过奖励估计置信度来检测环境变化，并相应地更新臂选择策略[Wu et al. (2018)]。在[Liu et al. (2018)]中，提出了一种基于变化检测的框架，用于多臂赌博机问题的分段静态奖励假设，其中使用上置信界（UCB）策略主动检测变化点并重新启动 UCB 指标。另一种捕捉用户动态偏好的解决方案是引入 MDP 设置[Chen et al. (2018), Liu et al. (2018), (97), Zou et al. (2019)]。在 MDP 设置下，引入状态来表示用户的偏好，状态转移捕捉了用户偏好的动态特性。在[(97)]中，用户的动态偏好（代理状态）是从其浏览历史中学习得来的。每次推荐系统向用户推荐一个项目时，用户将浏览该项目并提供反馈（跳过、点击或购买），这揭示了用户对推荐项目的满意度。根据反馈，推荐系统将更新其状态以表示用户的新偏好[ (97)]。

### 4.3 长期用户参与

用户参与度在推荐系统中是评估用户对推荐的项目（如产品、服务或信息）所作出的期望（甚至必要）反应的过程[Lalmas et al. (2014)]。用户参与度不仅可以通过即时反应（例如点击和对推荐项目的评分）来衡量，更重要的是通过长期反应（例如用户的重复购买）来衡量[(58)]。在[Wu et al. (2017)]中，长期用户参与度优化的问题被表述为一个序列决策问题。在每次迭代中，代理需要根据用户对过去推荐的动态反应来估计失去用户的风险。然后，引入了一种基于赌博的的方法[Wu et al. (2017)]，以平衡用户的即时点击和用户重新访问推荐系统时的预期未来点击。在实际的推荐会话中，用户将依次访问多个场景，如入口页面和项目详细信息页面，每个场景都有其自身的推荐策略。在[Zhao et al. (2019)]中，提出了一种基于多代理强化学习的方法（DeepChain），它可以捕捉不同场景之间的序列相关性，并联合优化多个推荐策略。具体来说，引入了基于模型的强化学习技术，以减少训练数据需求并执行更精确的策略更新。在新闻推荐场景[Zheng et al. (2018)]中，为了融入更多的用户反馈信息，长期用户反应（即用户返回的频率）被作为对用户即时点击行为的补充，并提出了一种基于深度 Q 学习的框架来优化新闻推荐策略。

### 4.4 页面级推荐

在实际的推荐系统中，通常每次会向用户推荐一页项目。在这种情况下，推荐系统需要同时（1）从更大的候选项目集中选择一组互补且多样化的项目，以及（2）制定一个项目展示（布局配置）策略，将这些项目放置在一个二维网页中，以实现最大的奖励。考虑到大量的项目，如果我们将每整页推荐视为一个动作，则动作空间极其庞大。为了解决这个大动作空间的问题，提出了一种深度确定性策略梯度算法 [Dulac-Arnold et al. (2015)]，其中 Actor 根据当前状态生成一个确定性的最优动作，而 Critic 输出该状态-动作对的 Q 值。DDPG 降低了传统基于价值的强化学习方法的计算成本，因此它适用于整个页面推荐设置 [Cai et al. (2018a), Cai et al. (2018b)]。最近提出了几种方法来提高效率 [Choi et al. (2018), Chen et al. (2018)]。在 [(95), Zhao et al. (2017)]中，引入了 CNN 技术来捕捉每个页面项目的展示模式和用户反馈。为了表示每个项目，利用了项目嵌入、类别嵌入和反馈嵌入，这有助于生成互补且多样化的推荐，并捕捉页面内用户的兴趣。还利用了带有技术来进行整页推荐 [Wang et al. (2017), Lacerda (2017)]。例如，将整页推荐任务视为组合半带问题，其中系统从候选集$K$个动作中推荐$S$个动作，并将选定的项目展示在$S$（从$M$中选出）个位置中 [Wang et al. (2017)]。

## **在线广告中的强化学习**  

在线广告的目标是将合适的广告分配给合适的用户，以最大化广告活动的收入、点击率（CTR）或投资回报率（ROI）。在线广告的两种主要营销策略是保证投放（GD）和实时竞价（RTB）。

### **5.1 保证投放**

在保证交付（GD）中，共享单一创意和主题的广告被分组为广告系列，并按照预定的交付数量（点击或展示）按广告系列收费[萨洛马廷等（2012）]。最流行的保证交付（GD）解决方案基于离线优化算法，然后调整为在线设置。然而，推导出分配展示的最优策略是具有挑战性的，特别是当环境在实际应用中不稳定时。在[吴等（2018）]中，提出了一种多智能体强化学习（MARL）方法，以在不稳定环境中为发布者推导合作策略以最大化其目标。他们将展示分配问题建模为拍卖问题，其中每个合同可以为单独的展示提交虚拟竞标。通过这种建模方法，他们通过解决合同的最优竞标函数推导出了最优的展示分配策略。

### 5.2 实时竞价

实时竞价（RTB）允许广告主在非常短的时间内为每一个独立的展示提交竞标。广告选择任务通常被建模为多臂老虎机（MAB）问题，其设定为每个臂的样本是独立同分布的，反馈是即时的，奖励是稳定的[杨和陆（2016），努阿拉等（2018），加斯帕里尼等（2018），汤等（2013），许等（2013），袁等（2013），施瓦茨等（2017）]。MAB 的收益函数可以演变，但假设它们随着时间的推移变化缓慢。另一方面，展示广告在广告活动期间被定期创建和移除。在[(20)]中研究了具有预算约束和可变成本的多臂老虎机问题。在这种情况下，拉动老虎机的臂会得到具有随机成本的随机奖励，算法旨在通过在受限预算下拉动臂来最大化长期奖励。这种设定比之前的研究中拉动一个臂是无成本或有固定成本的情况更能精确地建模互联网广告。

在多臂赌博机（MAB）设置下，竞标决策被视为静态优化问题，可以选择独立处理每次展示的价值，或者为每个广告量段设置竞标价格。然而，给定广告活动的竞标将在其生命周期内重复进行，直到预算用尽。因此，马尔可夫决策过程（MDP）设置也得到了研究[Cai et al. (2017)、Tang (2017)、Wang et al. (2018)、Zhao et al. (2018)、Rohde et al. (2018)、Wu et al. (2018)、Jin et al. (2018)]。提出了一种基于模型的强化学习框架来学习实时竞标（RTB）广告中的竞标策略[Cai et al. (2017)]，其中使用神经网络来近似状态值，这可以更好地处理大规模拍卖量和有限广告预算的问题。提出了一种无模型深度强化学习方法来解决有限预算下的竞标问题[Wu et al. (2018)]：该问题被建模为一个$\lambda$-控制问题，并设计了 RewardNet 用于生成奖励以解决奖励设计陷阱，而不是使用即时奖励。提出了一种多智能体竞标模型，考虑了系统中其他广告商的竞标情况，并引入了聚类方法来应对大量广告商的挑战[Jin et al. (2018)]。

## 6 结论与未来方向

在本文中，我们从强化学习的角度对信息寻求进行概述。我们首先介绍了基于强化学习的信息寻求方法的数学基础。然后，我们回顾了三种代表性的信息寻求机制——搜索、推荐和广告——的最先进算法。接下来，我们讨论了强化学习中的一些有趣研究方向，这些方向可以将信息寻求研究带入新的前沿。

首先，大多数现有工作仅在单一场景下训练策略，而忽视了其他场景中用户的行为（偏好）[Feng 等人（2018）]。这将导致次优策略，需采用协作式强化学习框架，同时考虑搜索、推荐和广告场景。其次，不同计算任务中的奖励函数类型各异。应设计更复杂的奖励函数以实现更多的信息获取目标，例如增加推荐的监督程度。第三，可以将更多类型的用户-代理交互纳入强化学习框架，例如将商品添加到购物车、用户的重复购买行为、用户在系统中的停留时间，以及用户与客服代表或 AI 对话系统代理的聊天。第四，测试新算法成本高，因为在实际系统中部署算法需要大量工程工作，且若算法不成熟可能对用户体验产生负面影响。因此，在上线前需要在线环境模拟器或基于历史日志的离线评估方法来预训练和评估新算法。最后，对信息获取的开放在线强化学习环境的需求日益增加，这可以推动强化学习和信息获取社区的发展，并实现离线和在线性能之间更好的一致性。

## 致谢

Xiangyu Zhao 和 Jiliang Tang 得到了国家科学基金会（NSF）资助，资助编号 IIS-1714741、IIS-1715940 和 CNS-1815636，以及来自 Criteo Faculty Research Award 的资助。

## 参考文献

+   Åström（1965）Åström, K. J. 1965. 马尔可夫过程的最优控制，具有不完全状态信息。数学分析与应用杂志 10, 1, 174–205。

+   Auer 等人（2002）Auer, P., Cesa-Bianchi, N., 和 Fischer, P. 2002. 多臂赌博机问题的有限时间分析。机器学习 47, 2-3, 235–256。

+   Auer 等人（2002）Auer, P., Cesa-Bianchi, N., Freund, Y., 和 Schapire, R. E. 2002. 非随机多臂赌博机问题。SIAM 计算杂志 32, 1, 48–77。

+   Baxter 和 Bartlett（2001）Baxter, J. 和 Bartlett, P. L. 2001. 无限时间视野策略梯度估计。人工智能研究杂志 15, 319–350。

+   Bellman（2013）Bellman, R. 2013. 动态规划。Courier Corporation。

+   Bhatnagar 等人（2007）Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., 和 Lee, M. 2007. 增量自然演员-评论家算法。见 NIPS ’07。

+   Bhatnagar 等人（2009）Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., 和 Lee, M. 2009. 自然演员-评论家算法。自动化 45, 11, 2471–2482。

+   Bowling 和 Veloso（2002）Bowling, M. H. 和 Veloso, M. M. 2002. 使用可变学习率的多智能体学习。人工智能 136, 2, 215–250。

+   Busoniu et al. (2010) Busoniu, L., Babuska, R., De Schutter, B., 和 Ernst, D. 2010. 使用函数逼近器的强化学习和动态规划。CRC 出版社。

+   Busoniu et al. (2008) Busoniu, L., Babuska, R., 和 Schutter, B. D. 2008. 多智能体强化学习的全面调查。IEEE Trans. Systems, Man, and Cybernetics, Part C 38, 2, 156–172。

+   Cai et al. (2017) Cai, H., Ren, K., Zhang, W., Malialis, K., Wang, J., Yu, Y., 和 Guo, D. 2017. 基于强化学习的实时竞价广告。发表于 WSDM ’17。

+   Cai et al. (2018a) Cai, Q., Filos-Ratsikas, A., Tang, P., 和 Zhang, Y. 2018a. 电子商务中的强化机制设计。发表于 WWW ’18。

+   Cai et al. (2018b) Cai, Q., Filos-Ratsikas, A., Tang, P., 和 Zhang, Y. 2018b. 针对电子商务中欺诈行为的强化机制设计。发表于 AAAI ’18。

+   Chang et al. (2006) Chang, C., Kayed, M., Girgis, M. R., 和 Shaalan, K. F. 2006. 网络信息提取系统的调查。IEEE Trans. Knowl. Data Eng. 18, 10, 1411–1428。

+   Chen et al. (2018) Chen, H., Dai, X., Cai, H., Zhang, W., Wang, X., Tang, R., Zhang, Y., 和 Yu, Y. 2018. 基于树结构策略梯度的大规模互动推荐。CoRR abs/1811.05869。

+   Chen et al. (2018) Chen, S., Yu, Y., Da, Q., Tan, J., Huang, H., 和 Tang, H. 2018. 在动态环境中稳定强化学习并应用于在线推荐。发表于 SIGKDD ’18。

+   Choi et al. (2018) Choi, S., Ha, H., Hwang, U., Kim, C., Ha, J., 和 Yoon, S. 2018. 基于强化学习的推荐系统，使用双聚类技术。CoRR abs/1801.05532。

+   Croft et al. (2010) Croft, W. B., Bendersky, M., Li, H., 和 Xu, G. 2010. 查询表示与理解研讨会。SIGIR Forum 44, 2, 48–53。

+   Deisenroth and Rasmussen (2011) Deisenroth, M. P. 和 Rasmussen, C. E. 2011. PILCO：一种基于模型且数据高效的策略搜索方法。发表于 ICML ’11。

+   (20) Ding, W., Qin, T., Zhang, X., 和 Liu, T. 多臂赌博机与预算约束及变动成本。发表于 AAAI ’13。

+   Dulac-Arnold et al. (2015) Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt, J., Mann, T., Weber, T., Degris, T., 和 Coppin, B. 2015. 大规模离散动作空间中的深度强化学习。arXiv 预印本 arXiv:1512.07679。

+   Feng et al. (2018) Feng, J., Li, H., Huang, M., Liu, S., Ou, W., Wang, Z., 和 Zhu, X. 2018. 学会协作：通过多智能体强化学习进行多场景排序。发表于 WWW ’18。

+   Feng et al. (2018) Feng, Y., Xu, J., Lan, Y., Guo, J., Zeng, W., 和 Cheng, X. 2018. 从贪婪选择到探索性决策：具有策略价值网络的多样化排序。发表于 SIGIR ’18。

+   Garcia-Molina et al. (2011) Garcia-Molina, H., Koutrika, G., 和 Parameswaran, A. G. 2011. 信息检索：搜索、推荐和广告的融合。Commun. ACM 54, 11, 121–130。

+   Gasparini 等（2018）Gasparini, M., Nuara, A., Trovò, F., Gatti, N., and Restelli, M. 2018. 通过学习已记录的赌徒反馈进行互联网广告的目标优化。发表于 IJCNN ’18。

+   Hill 等（2017）Hill, D. N., Nassif, H., Liu, Y., Iyer, A., and Vishwanathan, S. V. N. 2017. 实时多变量优化的高效带式算法。发表于 SIGKDD ’17。

+   Hu 等（2018）Hu, Y., Da, Q., Zeng, A., Yu, Y., and Xu, Y. 2018. 电子商务搜索引擎中的强化学习排序：形式化、分析与应用。发表于 SIGKDD ’18。

+   Jin 等（2018）Jin, J., Song, C., Li, H., Gai, K., Wang, J., and Zhang, W. 2018. 在展示广告中使用多智能体强化学习进行实时竞标。发表于 CIKM ’18。

+   Kaelbling 等（1998）Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. 1998. 在部分可观察的随机领域中规划与行动。Artif. Intell. 101, 1-2, 99–134。

+   Kaelbling 等（1996）Kaelbling, L. P., Littman, M. L., and Moore, A. W. 1996. 强化学习：综述。J. Artif. Intell. Res. 4, 237–285。

+   Kakade（2001）Kakade, S. 2001. 自然策略梯度。发表于 NIPS ’01。

+   Kapoor 等（2018）Kapoor, S., Keswani, V., Vishnoi, N. K., and Celis, L. E. 2018. 使用约束的赌徒基础个性化实现平衡新闻。发表于 IJCAI ’18。

+   Katariya 等（2017）Katariya, S., Kveton, B., Szepesvári, C., Vernade, C., and Wen, Z. 2017. 伯努利 rank-1 赌徒用于点击反馈。发表于 IJCAI ’17。

+   Katariya 等（2016）Katariya, S., Kveton, B., Szepesvári, C., and Wen, Z. 2016. DCM 赌徒：通过多次点击进行排序学习。发表于 ICML ’16。

+   Konda 和 Tsitsiklis（1999）Konda, V. R. and Tsitsiklis, J. N. 1999. Actor-critic 算法。发表于 NIPS ’99。

+   Kröse（1995）Krésé, B. J. A. 1995. 从延迟奖励中学习。Robotics and Autonomous Systems 15, 4, 233–235。

+   Kveton 等（2015）Kveton, B., Szepesvári, C., Wen, Z., and Ashkan, A. 2015. 级联赌徒：在级联模型中学习排序。发表于 ICML ’15。

+   Lacerda（2017）Lacerda, A. 2017. 多目标排序赌徒在推荐系统中的应用。Neurocomputing 246, 12–24。

+   Lagoudakis 和 Parr（2003）Lagoudakis, M. G. and Parr, R. 2003. 最小二乘策略迭代。Journal of Machine Learning Research 4, 1107–1149。

+   Lalmas 等（2014）Lalmas, M., O’Brien, H., and Yom-Tov, E. 2014. 用户参与度测量。信息概念、检索与服务综合讲座。Morgan & Claypool Publishers。

+   Li 等（2010）Li, L., Chu, W., Langford, J., and Schapire, R. E. 2010. 上下文赌徒方法进行个性化新闻推荐。发表于 WWW ’10。

+   Liu 等（2018）Liu, F., Lee, J., and Shroff, N. B. 2018. 基于变化检测的分段静态多臂赌徒问题框架。发表于 AAAI ’18。

+   Liu 等（2018）Liu, F., Tang, R., Li, X., Ye, Y., Chen, H., Guo, H., and Zhang, Y. 2018. 基于深度强化学习的推荐系统，具有显式的用户-项目交互建模。CoRR abs/1810.12027。

+   Lu 等（2010）Lu, T., Pál, D., 和 Pal, M. 2010. 上下文多臂强盗。发表于 AISTATS ’10。

+   Luo 等（2014）Luo, J., Zhang, S., 和 Yang, H. 2014. 双赢搜索：会话搜索中的双代理随机游戏。发表于 SIGIR ’14。

+   Lv 和 Zhai（2009）Lv, Y. 和 Zhai, C. 2009. 信息检索中的自适应相关反馈。发表于 CIKM ’09。

+   Mnih 等（2015）Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., 和 Hassabis, D. 2015. 通过深度强化学习实现人类水平的控制。自然 518, 7540, 529–533。

+   Moore 和 Atkeson（1993）Moore, A. W. 和 Atkeson, C. G. 1993. 优先级扫描：用更少的数据和时间进行强化学习。机器学习 13, 103–130。

+   Nogueira 等（2018）Nogueira, R., Bulian, J., 和 Ciaramita, M. 2018. 学习协调多个强化学习代理以进行多样化查询重构。CoRR abs/1809.10658。

+   Nogueira 和 Cho（2017）Nogueira, R. 和 Cho, K. 2017. 任务导向的查询重构与强化学习。发表于 EMNLP ’17。

+   Nuara 等（2018）Nuara, A., Trovò, F., Gatti, N., 和 Restelli, M. 2018. 用于点击付费广告活动的在线联合出价/预算优化的组合-强盗算法。发表于 AAAI ’18。

+   Peters 和 Schaal（2008）Peters, J. 和 Schaal, S. 2008. 自然演员-评论家。神经计算 71, 7-9, 1180–1190。

+   Peters 等（2005）Peters, J., Vijayakumar, S., 和 Schaal, S. 2005. 自然演员-评论家。发表于 ECML ’05。

+   Rohde 等（2018）Rohde, D., Bonner, S., Dunlop, T., Vasile, F., 和 Karatzoglou, A. 2018. Recogym：用于在线广告中产品推荐问题的强化学习环境。CoRR abs/1808.00720。

+   Rummery 和 Niranjan（1994）Rummery, G. A. 和 Niranjan, M. 1994. 使用连接主义系统的在线 Q 学习。第 37 卷。剑桥大学，工程系，剑桥，英国。

+   Salomatin 等（2012）Salomatin, K., Liu, T., 和 Yang, Y. 2012. 在线广告中拍卖和保证交付的统一优化框架。发表于 CIKM ’12。

+   Santos 等（2015）Santos, R. L. T., MacDonald, C., 和 Ounis, I. 2015. 搜索结果多样化。信息检索基础与趋势 9, 1, 1–90。

+   (58) Schopfer, S. 和 Keller, T. 移动购物清单应用的长期推荐基准测试，使用马尔可夫链。发表于 RecSys ’14。

+   Schwartz 等（2017）Schwartz, E. M., Bradlow, E. T., 和 Fader, P. S. 2017. 通过展示广告进行客户获取，使用多臂强盗实验。市场营销科学 36, 4, 500–522。

+   Shani 等（2005）Shani, G., Heckerman, D., 和 Brafman, R. I. 2005. 基于 MDP 的推荐系统。机器学习研究杂志 6, 1265–1295。

+   Shoham 等人（2003）Shoham, Y., Powers, R., 和 Grenager, T. 2003. 多智能体强化学习：一项关键调查。技术报告，斯坦福大学。

+   Smallwood 和 Sondik（1973）Smallwood, R. D. 和 Sondik, E. J. 1973. 有限时域上的部分可观察马尔可夫过程的最优控制。运筹学 21, 5, 1071–1088。

+   Sondik（1978）Sondik, E. J. 1978. 无限时域上的部分可观察马尔可夫过程的最优控制：折现成本。运筹学 26, 2, 282–304。

+   Sutton（1991）Sutton, R. S. 1991. Dyna，一种集成的学习、规划和反应架构。SIGART 公报 2, 4, 160–163。

+   Sutton 和 Barto（1998）Sutton, R. S. 和 Barto, A. G. 1998. 强化学习导论。第 135 卷。MIT 出版社，剑桥。

+   Tang 等人（2013）Tang, L., Rosales, R., Singh, A., 和 Agarwal, D. 2013. 通过上下文赌博机进行自动广告格式选择。在 CIKM ’13。

+   Tang（2017）Tang, P. 2017. 强化机制设计。在 IJCAI ’17。

+   Varaiya 和 Walrand（1983）Varaiya, P. 和 Walrand, J. C. 1983. 多臂赌博机问题和资源共享系统。在计算机性能和可靠性，国际研讨会论文集，意大利比萨，1983 年 9 月 26-30 日。181–196。

+   Vorobev 等人（2015）Vorobev, A., Lefortier, D., Gusev, G., 和 Serdyukov, P. 2015. 通过多臂赌博机收集有关搜索结果的额外反馈，针对生产排名。在 WWW ’15。

+   Wang 等人（2018）Wang, W., Jin, J., Hao, J., Chen, C., Yu, C., Zhang, W., Wang, J., Wang, Y., Li, H., Xu, J., 和 Gai, K. 2018. 通过约束的两级强化学习学习广告曝光。CoRR abs/1809.03149。

+   Wang 等人（2017）Wang, Y., Ouyang, H., Wang, C., Chen, J., Asamov, T., 和 Chang, Y. 2017. 用于整页推荐的高效有序组合半带宽问题。在 AAAI ’17。

+   Wang 等人（2016）Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., 和 Mei, Q. 2016. 超越排名：优化整页展示。在 WSDM ’16。

+   Wang 等人（2018）Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., 和 Mei, Q. 2018. 针对网页搜索优化整页展示。TWEB 12, 3, 19:1–19:25。

+   Watkins（1989）Watkins, C. J. C. H. 1989. 从延迟奖励中学习。博士论文，剑桥大学国王学院。

+   Williams（1992）Williams, R. J. 1992. 用于连接主义强化学习的简单统计梯度跟随算法。机器学习 8, 229–256。

+   Wu 等人（2018）Wu, D., Chen, C., Yang, X., Chen, X., Tan, Q., Xu, J., 和 Gai, K. 2018. 一种用于在线展示广告中印象分配的多智能体强化学习方法。CoRR abs/1809.03152。

+   Wu 等人（2018）Wu, D., Chen, X., Yang, X., Wang, H., Tan, Q., Zhang, X., Xu, J., 和 Gai, K. 2018. 通过无模型强化学习进行预算约束竞标。在 CIKM ’18。

+   Wu 等 (2018) Wu, Q., Iyer, N., 和 Wang, H. 2018. 在非平稳环境中学习上下文赌博机。在 SIGIR ’18。

+   Wu 等 (2017) Wu, Q., Wang, H., Hong, L., 和 Shi, Y. 2017. 归还即信仰：优化推荐系统中的长期用户参与。在 CIKM ’17。

+   Xia 等 (2017) Xia, L., Xu, J., Lan, Y., Guo, J., Zeng, W., 和 Cheng, X. 2017. 适应马尔可夫决策过程以进行搜索结果多样化。在 SIGIR ’17。

+   Xu 和 Li (2007) Xu, J. 和 Li, H. 2007. Adarank: 一种用于信息检索的提升算法。在 SIGIR ’07。

+   Xu 等 (2008) Xu, J., Liu, T., Lu, M., Li, H., 和 Ma, W. 2008. 在学习排名中直接优化评估指标。在 SIGIR ’08。

+   Xu 等 (2017) Xu, J., Xia, L., Lan, Y., Guo, J., 和 Cheng, X. 2017. 直接优化多样性评估指标：搜索结果多样化的新方法。ACM TIST 8, 3, 41:1–41:26。

+   Xu 等 (2013) Xu, M., Qin, T., 和 Liu, T. 2013. 搜索广告中多臂赌博机算法的估计偏差。在 NIPS ’13。

+   Yang 和 Lu (2016) Yang, H. 和 Lu, Q. 2016. 动态上下文多臂赌博机在展示广告中的应用。在 ICDM ’16。

+   (86) Yin, D., Hu, Y., Tang, J., Daly, T., Zhou, M., Ouyang, H., Chen, J., Kang, C., Deng, H., Nobata, C., 等。 在 Yahoo 搜索中排名相关性。在 SIGKDD’16。

+   Yin 等 (2016) Yin, D., Hu, Y., Tang, J., Jr., T. D., Zhou, M., Ouyang, H., Chen, J., Kang, C., Deng, H., Nobata, C., Langlois, J., 和 Chang, Y. 2016. 在 Yahoo 搜索中排名相关性。在 SIGKDD ’16。

+   Yuan 等 (2013) Yuan, S., Wang, J., 和 van der Meer, M. 2013. 使用上下文赌博机的自适应关键词提取用于停放域广告。CoRR abs/1307.3573。

+   Yue 等 (2007) Yue, Y., Finley, T., Radlinski, F., 和 Joachims, T. 2007. 用于优化平均精度的支持向量方法。在 SIGIR ’07。

+   Zeng 等 (2016) Zeng, C., Wang, Q., Mokhtari, S., 和 Li, T. 2016. 具有时间变化多臂赌博机的在线上下文感知推荐。在 SIGKDD ’16。

+   Zeng 等 (2017) Zeng, W., Xu, J., Lan, Y., Guo, J., 和 Cheng, X. 2017. 使用马尔可夫决策过程进行排序的强化学习。在 SIGIR ’17。

+   Zeng 等 (2018) Zeng, W., Xu, J., Lan, Y., Guo, J., 和 Cheng, X. 2018. 使用强化学习进行多页面搜索。在 ICTIR ’18。

+   Zhang 等 (2014) Zhang, S., Luo, J., 和 Yang, H. 2014. 用于无内容文档重排序的 POMDP 模型。在 SIGIR ’14。

+   Zhao 等 (2018) Zhao, J., Qiu, G., Guan, Z., Zhao, W., 和 He, X. 2018. 用于赞助搜索实时竞价的深度强化学习。在 SIGKDD ’18。

+   (95) Zhao, X., Xia, L., Zhang, L., Ding, Z., Yin, D., 和 Tang, J. 深度强化学习用于页面级推荐。在 ResSys’18。

+   Zhao 等 (2019) Zhao, X., Xia, L., Zhao, Y., Tang, J., 和 Yin, D. 2019. 基于模型的强化学习用于全链推荐。arXiv 预印本 arXiv:1902.03987。

+   (97) Zhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D. 通过配对深度强化学习进行负反馈推荐。见 SIGKDD’18。

+   Zhao 等（2017）Zhao, X., Zhang, L., Ding, Z., Yin, D., Zhao, Y., and Tang, J. 2017. 用于列表推荐的深度强化学习。arXiv 预印本 arXiv:1801.00209。

+   Zheng 等（2018）Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie, X., and Li, Z. 2018. DRN：用于新闻推荐的深度强化学习框架。见 WWW ’18。

+   Zou 等（2019）Zou, L., Xia, L., Ding, Z., Yin, D., Song, J., and Liu, W. 2019. 强化学习用于多样化推荐。见 DASFAA ’19。

{biography}

**Xiangyu Zhao** 是密歇根州立大学（MSU）计算机科学与工程的博士生。他的导师是 Dr. Jiliang Tang。在加入 MSU 之前，他在 USTC 获得了硕士学位（2017）和在 UESTC 获得了学士学位（2014）。他是 IEEE、SIGIR 和 SIAM 的学生会员。他目前的研究兴趣包括数据挖掘和机器学习，特别是（1）电子商务的强化学习；（2）城市计算和时空数据分析。加入 MSU 后，他在顶级期刊（如 SIGKDD Explorations）和会议（如 KDD、ICDM、CIKM、RecSys）上发表了工作。他曾获得 RecSys’18、KDD’18、SDM’18 和 CIKM’17 学生旅行奖。

**Long Xia** 是京东数据科学实验室的研究科学家。他目前主要负责将先进技术应用于京东的电子商务推荐系统。在此之前，他在中国科学院计算技术研究所获得了计算机科学博士学位。他的研究兴趣包括数据挖掘、应用机器学习、信息检索和推荐系统。他在顶级期刊和会议上发表了研究成果，如 TIST、SIGIR、KDD、RecSys。

**Jiliang Tang** 是密歇根州立大学计算机科学与工程系的助理教授。在此之前，他曾在 Yahoo 研究担任研究科学家，并于 2015 年获得亚利桑那州立大学的博士学位。他在社会计算、数据挖掘和机器学习方面具有广泛兴趣。他曾获得 ASONAM 2018 最佳论文奖、WSDM 2018 最佳学生论文奖、KDD 2016 最佳论文奖、2015 年最佳 KDD 论文奖亚军、Dean’s Dissertation Award 及 WSDM 2013 最佳论文提名。他目前是 ACM TKDD、ICWSM 和 Neurocomputing 的副主编。他在高排名期刊和顶级会议论文集中发表了研究成果，获得了数千次引用和广泛的媒体报道。

殷大伟是京东的高级总监，负责推荐、搜索、指标和知识图谱的科学工作。在加入京东之前，他曾在 Yahoo Labs 担任高级研究经理，领导相关性科学团队，并负责 Yahoo 搜索的核心搜索相关性。他获得了莱海大学的博士学位（2013 年）、硕士学位（2010 年）和山东大学的学士学位（2006 年）。他的研究兴趣包括数据挖掘、应用机器学习、信息检索和推荐系统。他在顶级会议和期刊上发表了几十篇研究论文，并获得了 WSDM2016 最佳论文奖、KDD2016 最佳论文奖和 WSDM2018 最佳学生论文奖。
