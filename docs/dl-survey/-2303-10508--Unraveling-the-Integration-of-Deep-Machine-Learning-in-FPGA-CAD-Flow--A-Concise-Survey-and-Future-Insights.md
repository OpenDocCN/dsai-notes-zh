<!--yml

类别：未分类

日期：2024-09-06 19:40:38

-->

# [2303.10508] 揭示深度机器学习在 FPGA CAD 流程中的集成：简要调查与未来见解

> 来源：[`ar5iv.labs.arxiv.org/html/2303.10508`](https://ar5iv.labs.arxiv.org/html/2303.10508)

# 揭示深度机器学习在 FPGA CAD 流程中的集成：简要调查与未来见解

Behnam Ghavami, Lesley Shannon 西蒙弗雷泽大学，加拿大不列颠哥伦比亚省本拿比

邮件：{behnam_ghavami, lesley_shannon}@sfu.ca

###### 摘要

本文概述了深度机器学习（DL）在 FPGA CAD 设计流程中的集成，重点关注高层次和逻辑综合、布局和布线。我们的分析识别了 FPGA CAD 设计中需要更多关注的关键研究领域，包括开发优化的开源基准测试，以便进行端到端的机器学习体验，以及研究人员和行业从业者之间知识共享的潜力，以在 FPGA CAD 决策步骤中融入更多智能。通过提供对深度机器学习在 FPGA CAD 流程中集成的见解，本文旨在为这一令人兴奋且快速发展的领域的未来研究方向提供信息。

## 一、引言

现场可编程门阵列（FPGAs）已成为现代数字系统的重要组成部分，包括医疗设备、自动驾驶汽车和数据中心。这些系统的设计过程是一项复杂且耗时的任务，涉及大量时间和资源的投入。计算机辅助设计（CAD）工具在确保 FPGA 基础系统的质量和效率方面发挥着关键作用。

FPGA 设计中的 CAD 工具，包括高层次综合、逻辑综合、布局和布线算法，用于将高层次的硬件描述转换为比特流表示。这些算法的质量对结果数字系统的性能和功耗有着显著影响。然而，由于问题的复杂性和大量的设计变量，为 FPGA 设计开发高质量的 CAD 工具是具有挑战性的。

深度机器学习（ML）技术在提高 FPGA CAD 算法的效率和效果方面显示出巨大潜力。ML 算法可以优化设计参数、预测设计结果，并加速设计过程。将 DL 技术集成到 FPGA CAD 流程设计中，有可能彻底改变 FPGA 基础系统的设计和实现方式。

本文概述了最新的深度机器学习（DL）相关努力在各种 FPGA CAD 设计步骤中的应用，包括高层次综合和逻辑综合、放置和布线。重点关注基于机器学习的 CAD 任务，并识别出在 CAD 设计中需要更多关注的关键研究领域。特别是，我们强调了开发针对端到端机器学习体验优化的开源基准的需求。因此，本文为希望了解将机器学习集成到 FPGA CAD 流程设计中的好处和挑战的研究人员和行业专业人士提供了有价值的资源。

## II 传统 FPGA CAD 流程

FPGA CAD 流程的主要步骤包括：

+   •

    设计输入：这一步涉及将设计捕捉在 HDL 中，如 Verilog 或 VHDL，或在高层次表示中如 C。

+   •

    综合与优化：在这一步中，HDL 设计被转换为门和寄存器的网表。综合工具分析设计并生成执行相同功能的逻辑电路。合成电路经过优化以提高性能，减少面积，并最小化功耗。优化可以在不同的抽象层次上进行，如门级优化或高层次综合。

+   •

    打包与放置：打包步骤涉及将放置的逻辑元素分组到称为逻辑簇的更紧凑的组中。这些逻辑簇随后被映射到 FPGA 上的物理区域，称为逻辑块或可配置逻辑块（CLBs）。放置是将设计的逻辑元素分配到 FPGA 上的物理位置的过程。放置工具确保元素的放置满足设计的时间约束。

+   •

    布线：布线是通过使用如开关盒和布线通道等布线资源在 FPGA 上建立逻辑元素之间连接的过程。布线工具确定连接的最佳路径，并确保其满足设计的时间要求。

+   •

    位流生成：最终步骤是生成可以编程到 FPGA 上的位流。

### II-A 使用 DL 模型优化 FPGA CAD 流程：所需步骤

DL 使用具有多层的神经网络来学习数据中的模式和关系，可以应用于 FPGA CAD 流程。DL 算法可以识别大量数据中的复杂模式和关系，从而可能产生性能更好的设计，并缩短设计周期。为了利用 DL 模型优化 FPGA CAD 流程，需要以下步骤：

+   •

    数据集创建：必须创建一个包含 FPGA 设计及其对应的放置和布线结果的大型数据集。该数据集可以用于训练和验证 DL 模型。

+   •

    特征提取：必须从 FPGA 设计中提取相关特征，例如逻辑块的位置、布线资源、时序约束和功率约束。

+   •

    模型选择：可以在数据集上评估和比较各种深度学习模型，以确定适用于 FPGA 计算机辅助设计流程的最佳模型。

+   •

    模型训练：所选择的深度学习模型必须在数据集上使用适当的训练和验证技术进行训练，如交叉验证或提前停止。

+   •

    模型集成：训练好的深度学习模型必须集成到 FPGA 计算机辅助设计流程中，并在测试数据集上进行评估，以评估其准确性和速度。

将深度学习模型集成到 FPGA 计算机辅助设计流程中有可能显著提高 FPGA 设计的效率和质量，使其成为 FPGA 社区一个令人兴奋的研究领域。

## III 通过机器学习的 FPGA 设计流程：从 HDL 展开到比特流生成

在这一部分，我们将回顾基于深度学习的 FPGA 计算机辅助设计技术的最新进展，并讨论它们对 FPGA 设计流程的潜在影响。

### III-A HDL 生成

有研究探讨了使用深度学习进行 HDL 的分析和优化。其中一种方法是使用卷积神经网络（CNN）进行 HDL 代码分类和代码错误识别，如 [1][2] 所示。Xia 等人提出了一种结合 RNN 和 CNN 的混合方法 [3]，用于自动 HDL 代码生成。这些方法在提高 HDL 设计的效率和准确性方面显示出了有希望的结果，并有潜力增强整体 FPGA 设计过程。

### III-B 综合

FPGA 设计流程包括高级综合（HLS）和逻辑综合阶段，其中 HLS 综合从高级语言生成 RTL（寄存器传输级）描述，而逻辑综合将 RTL 设计转换为门级网表。最近，研究人员探索了使用深度学习技术来改进 HLS 和逻辑综合，取得了有希望的成果。

#### III-B1 RTL 综合

在 FPGA 综合中，运行的缓慢时序可能会成为一个重要挑战，现代设计需要几天的运行时间。为了应对这一挑战，杨华结合了多种分类算法的预测，从而提高了 InTime 的预测准确性，这是一种针对 Xilinx 和 Altera 计算机辅助设计工具的自动时序插件 [4]。

#### III-B2 高级综合

近年来，深度学习（DL）算法在高层次综合（HLS）中得到了广泛应用，以优化资源和时间使用方面的性能。一种方法是利用图神经网络（GNN）来整合数据流图中操作之间的结构信息，从而提高操作延迟估计的准确性[5]。拥堵估计是 HLS 优化的另一个重要方面，赵等人[6]使用了梯度提升回归树（GBRT）来预测 HLS 中的路由拥堵。此外，Makrani[7]将时间优化建模为回归问题，并利用 DL 评估 HLS 工具输出代码的时钟频率。在[8]中，作者提出了一种方法来解决用于训练和预测 DL 模型的开源 HLS 设计的可用性挑战。他们提出了一种从单一设计生成多样化设计的方法，从而生成了一组可综合的 FPGA HLS 设计数据集。

### III-C 布局

为了实现准确的拥塞和可路由性评估，已经使用了几种机器学习技术。[9] 使用了 CNN 模型来预测电路在其放置基础上的可路由性。Al-Hyari [10] 提出了一个包括两个机器学习模型的拥塞估计框架，用于放置预测。该模型用于确定是否可以在没有传统路由器开销的情况下路由一个放置解决方案。第一个模型 MLCong 识别关键特征，以准确估计放置过程中的拥塞。第二个模型 MLRoute 利用这些特征来根据 MLCong 生成的拥塞图预测已放置电路的可路由性。Martin [11] 提出了一个简单的深度学习模型和集成模型，用于准确预测放置解决方案的可路由性。引入了基于 Bagging、Boosting 和分类器堆叠的三种集成方法，以提高模型的准确性和鲁棒性。Esmaeili 等 [12] 提出了一种在 FPGA 设计流程的详细放置 (DP) 优化步骤中使用强化学习 (RL) 减少运行时间的方法，同时保持结果质量 (QoR)。DP 的目标是优化全局放置，以提高路由步骤的成功率。提出并评估了三种基于表格 Q 学习、深度 Q 学习和演员-评论家方法的 RL 模型，以减少 DP 运行时间。结果表明，在不牺牲 QoR 的情况下，有可能显著减少运行时间。Murray 等 [13] 提出了 RLPlace，一种新的基于模拟退火 (SA) 的 FPGA 放置器，结合了强化学习 (RL) 和有针对性的扰动，以优化线路长度和时序。通过使用定向移动，所提出的方法比传统的随机移动更有效地探索解决方案空间，同时防止了结果质量 (QoR) 的振荡。在优化过程中，RL 被用于动态选择最有效的移动类型。Rajarathnam 等 [14] 提出了 DREAMPlaceFPGA，一个开源 FPGA 放置框架，使用 PyTorch 深度学习工具包加速构建。它通过优化操作符处理 FPGA 资源异质性和架构特定的合法性约束，并提供了 Python 的高级编程接口。

### III-D 路由

Farooq [15] 提出了一种基于强化学习（RL）的路由问题解决方法，通过将经典的路由迭代过程转化为 RL 的训练过程。该方法采用贪婪策略和定制化奖励函数，以加速路由步骤，同时保持与传统基于协商的拥堵驱动路由解决方案相似或更好的结果质量（QoR）。Ghavami 等人 [16] 提出了 MAPLE，这是一种在路由后利用深度学习（DL）模型进行 FPGA 设计的老化感知静态时序分析的工具。MAPLE 使用深度神经网络（DNN）有效建模基本模块层面上因老化引起的延迟退化。该框架通过为每种 FPGA 块类型训练一个 DNN 模型，准确预测延迟退化与综合老化因素之间的关系。

## IV 未来路线图：

改革 FPGA 设计流程

深度学习（DL）在 FPGA 设计流程的各个阶段中应用越来越广泛，以提升性能和效率。DL 的一个理想应用是在从高级描述直接生成比特流，省去了传统的综合和布图工具。采用这种方法，设计师可以快速评估和优化设计，在高层次上显著减少设计迭代，加速整体设计过程。这种方法有潜力彻底改变 FPGA 设计流程，使其更快且对具有不同专业背景的设计师更为可及。通过在 FPGA CAD 流程中利用 DL 的强大功能，设计师可以解锁新的可能性，实现更高的性能和更低的功耗。

### IV-A 潜在的深度学习模型

有几种类型的深度学习（DL）模型可能适合未来的直接从高级描述生成比特流的路线图。其中一种方法是使用深度学习模型，如卷积神经网络（CNNs）、递归神经网络（RNNs）或变换器（transformers），这些模型在自然语言处理和图像识别任务中已经取得了成功。这些模型可以调整以处理 FPGA 设计的高级描述并生成相应的比特流。另一种方法是使用强化学习（RL）模型，这些模型通过与 FPGA 设计环境进行试错互动来生成最佳比特流。强化学习模型有可能适应不同的设计目标和约束，如性能、功耗和面积，并相应地优化设计。直接从高级描述生成比特流是一项具有挑战性的任务，因为它需要较高的抽象水平。这就是大型深度学习模型可以发挥作用的地方。大型深度学习模型能够从大量数据中学习复杂的模式和关系，使其非常适合直接从高级描述生成比特流。此外，联邦学习（FL）是一种跨多个设备进行协作学习的方法，可以应用于从高级描述直接生成 FPGA 比特流的过程。在这种方法中，单个设备生成候选比特流，并将其结果与中央服务器共享，以生成最终的优化 FPGA 比特流。然而，针对这一应用实施联邦学习面临一些挑战，包括确保设备与中央服务器之间共享数据的隐私和安全。

### IV-B 挑战

开发基于深度学习的 FPGA CAD 流程，以直接从高级描述生成比特流，面临着需要解决的若干挑战。其中一个重大挑战是需要大量高质量的训练数据来有效地训练机器学习模型。训练数据的质量和多样性可以直接影响模型的性能和准确性。数据可用性也是开发深度学习模型用于 FPGA CAD 流程中的一个重要挑战，因为收集和整理这些数据可能是一个耗时且昂贵的过程。确保深度学习模型的可解释性也是一个问题，因为设计人员需要了解模型如何生成比特流，并确保它们符合设计目标和约束。最后，需要研究人员和行业从业者之间的合作与知识共享，以开发和评估基于深度学习的 FPGA CAD 流程。解决这些挑战对于实现这一未来 FPGA 设计路线图的潜在收益至关重要。

## V 结论

FPGA EDA 设计中的人工智能被期待成为下一个主要趋势，主要工具供应商和半导体公司已经在努力利用这项技术。在 FPGA 设计流程的各个阶段集成深度机器学习（DL）技术，有可能显著提升 FPGA 设计的效率、性能和可及性。研究人员探讨了使用 DL 模型来改善 FPGA 设计流程，从 HDL elaboration 到 bitstream 生成。这些模型可以帮助优化设计参数，提高布置和路由的准确性和速度。

## 参考文献

+   [1] J. Yang, X. Zhang, J. He, 和 Y. Sun, “Hdl-cnn: 基于深度学习的可综合 Verilog 代码生成方法，” *IEEE 计算机辅助设计集成电路与系统汇刊*，第 37 卷，第 12 期，第 3127–3137 页，2018 年。

+   [2] L. Tan, X. Song, X. Wu, 和 X. Wang, “基于深度学习方法的 HDL 代码错误检测，” *IEEE 电路与系统 II: 快速简报*，第 66 卷，第 4 期，第 544–548 页，2019 年。

+   [3] Y. Xia, Y. Cai, Y. Zhu, H. Chen, 和 W. Liu, “Hdlnet: 一种用于自动 HDL 代码生成的混合方法，” *IEEE 计算机辅助设计集成电路与系统汇刊*，第 40 卷，第 9 期，第 1966–1978 页，2021 年。

+   [4] Q. Yanghua, C. Adaikkala Raj, H. Ng, K. Teo, 和 N. Kapre, “针对 FPGA 设计时序收敛的设计特定机器学习方法的案例研究，” 在 *2016 年 ACM/SIGDA 现场可编程门阵列国际研讨会论文集*，2016 年，第 169–172 页。

+   [5] E. Ustun, C. Deng, D. Pal, Z. Li, 和 Z. Zhang, “使用图神经网络进行 FPGA 高级综合的准确操作延迟预测，” 在 *第 39 届国际计算机辅助设计会议论文集*，2020 年，第 1–9 页。

+   [6] J. Zhao, T. Liang, S. Sinha, 和 W. Zhang, “基于机器学习的 FPGA 高级综合路由拥塞预测，” 在 *2019 欧洲设计、自动化与测试会议暨展览会（DATE）*，2019 年，第 1130–1135 页。

+   [7] H. M. Makrani, F. Farahmand, H. Sayadi, S. Bondi, S. M. P. Dinakarrao, H. Homayoun, 和 S. Rafatirad, “Pyramid: 估计高级综合设计的最佳时序和资源使用的机器学习框架，” 在 *2019 第 29 届国际现场可编程逻辑与应用会议（FPL）*。 IEEE, 2019 年，第 397–403 页。

+   [8] P. Goswami, M. Shahshahani, 和 D. Bhatia, “Mlsbench: 一个用于机器学习的 FPGA 高级综合设计流程的基准集合，” 在 *2022 IEEE 第 13 届拉丁美洲电路与系统研讨会（LASCAS）*，2022 年，第 1–4 页。

+   [9] A. Al-Hyari, H. Szentimrey, A. Shamli, T. Martin, G. Grewal, 和 S. Areibi, “一个预测 FPGA 电路布置可路由性的深度学习框架，” *ACM 可重构技术与系统汇刊（TRETS）*，第 14 卷，第 3 期，第 1–28 页，2021 年。

+   [10] A. Al-Hyari, Z. Abuowaimer, T. Martin, G. Gréwal, S. Areibi, 和 A. Vannelli, “基于机器学习的现代 FPGA 新型拥塞估计和可路由性预测方法，” *ACM 可重构技术与系统汇刊（TRETS）*，第 12 卷，第 3 期，第 1–25 页，2019 年。

+   [11] T. Martin, S. Areibi, 和 G. Gréwal, “用于预测 FPGA 放置过程中可路由性的有效机器学习模型，” 载于 *2021 年 ACM/IEEE 第三届计算机辅助设计（CAD）机器学习研讨会（MLCAD）*。IEEE，2021 年，第 1–6 页。

+   [12] P. Esmaeili, T. Martin, S. Areibi, 和 G. Grewal, “通过强化学习指导 FPGA 详细放置，” 载于 *2022 年 IFIP/IEEE 第 30 届国际超大规模集成电路会议（VLSI-SoC）*。IEEE，2022 年，第 1–6 页。

+   [13] M. A. Elgammal, K. E. Murray, 和 V. Betz, “Rlplace: 使用强化学习和智能扰动优化 FPGA 放置，” *IEEE 集成电路和系统计算机辅助设计汇刊*，第 41 卷，第 8 期，第 2532–2545 页，2021 年。

+   [14] R. S. Rajarathnam, M. B. Alawieh, Z. Jiang, M. Iyer, 和 D. Z. Pan, “Dreamplacefpga: 一个开源的分析放置工具，用于大规模异构 FPGA，基于深度学习工具包，” 载于 *2022 年第 27 届亚洲和南太平洋设计自动化会议（ASP-DAC）*。IEEE，2022 年，第 300–306 页。

+   [15] I. Baig 和 U. Farooq, “使用强化学习进行 FPGA 后端流程的高效详细路由，” *电子学*，第 11 卷，第 14 期，第 2240 页，2022 年。

+   [16] B. Ghavami, M. Ibrahimipour, Z. Fang, 和 L. Shannon, “Maple: 基于机器学习的老化感知 FPGA 架构探索框架，” 载于 *2021 年第 31 届国际现场可编程逻辑与应用会议（FPL）*，2021 年，第 369–373 页。
