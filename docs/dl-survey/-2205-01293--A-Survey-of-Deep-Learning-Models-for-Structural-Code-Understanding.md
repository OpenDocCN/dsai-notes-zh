<!--yml

类别：未分类

日期：2024-09-06 19:46:44

-->

# [2205.01293] 深度学习模型在结构化代码理解中的应用调查

> 来源：[`ar5iv.labs.arxiv.org/html/2205.01293`](https://ar5iv.labs.arxiv.org/html/2205.01293)

# 深度学习模型在结构化代码理解中的应用调查

吴若婷 wurt8@mail2.sysu.edu.cn 中山大学，张玉欣 中山大学 zhangyx355@mail2.sysu.edu.cn，彭启彪 中山大学 pengqb3@mail2.sysu.edu.cn，陈亮 chenliang6@mail.sysu.edu.cn 中山大学以及郑子彬 zhzibin@mail.sysu.edu.cn 中山大学

###### 摘要。

近年来，深度学习的兴起和软件行业自动化需求的增加将智能软件工程提升到了新的高度。代码理解中的方法和应用数量不断增加，许多应用中使用了深度学习技术，以更好地捕捉代码数据中的信息。在这项调查中，我们提供了从代码数据中形成的结构的全面概述。我们将近年来的代码理解模型分为两类：基于序列的模型和基于图的模型，并进一步总结和比较它们¹¹1 我们在 [`github.com/codingClaire/Structural-Code-Understanding`](https://github.com/codingClaire/Structural-Code-Understanding) 提供了一些关于深度学习模型和数据集的论文集。我们还介绍了度量标准、数据集和下游任务。最后，我们对结构化代码理解领域的未来研究提出了一些建议。

代码表示，智能软件工程，图神经网络，深度学习，代码生成^†^†书名：

## 1\. 介绍

在过去几十年里，深度学习在各个领域取得了显著成就，并渗透到了人类生活的各个方面，尤其是在图像识别、语音识别和自然语言处理等多媒体数据处理领域。随着深度学习技术的蓬勃发展，以及开源代码社区和软件行业自动化需求的共同增长，近年来深度学习技术开始被应用于软件工程中的更具体任务。

传统上，源代码被视为可以通过各种现有方法（如神经语言处理中的深度学习方法）理解的纯文本序列。然而，当直接应用于源代码时，NLP 方法的一个缺点是忽略了代码的结构信息。当代码仅作为纯文本序列进行学习时，理解代码所需的语法和语义信息以及程序实体之间的许多关系可能会被忽视。因此，近年来提出了大量利用结构信息理解源代码的工作，这些工作受到序列和图的深度学习研究的推动，如 Transformer（Bahdanau 等，2014），图神经网络（Wu 等，2019）。这些技术及其变体旨在应对源代码理解中的各种任务，包括代码表示和其他下游任务。尽管这些方法取得了一些进展，但结构化代码理解仍面临许多挑战，这些挑战已被识别和总结如下：

+   •

    代码结构建模：由于传统语言模型将源代码令牌的序列作为输入，代码中的结构信息通常被忽视。因此，如何成功利用代码中的结构信息成为了一系列挑战，例如如何有效地建模代码中的结构信息以及如何为特定的下游任务选择有效的结构信息。

+   •

    代码通用表示学习：目前的研究大多集中在为特定编程语言学习代码表示上，使得学习通用代码表示成为一个挑战。这涉及到如何学习超越编程语言限制的语言无关的代码表示。

+   •

    代码任务特定适配：以下适配仍然是一个挑战：如何为下游应用（如代码生成和程序修复）选择和设计特定的架构，如何处理任务规范的数据集，以及如何在少量样本学习、迁移学习和跨语言场景中适配模型。

在这项调查中，我们对代码表示学习的结构学习技术进行了全面概述。总之，我们的贡献可以列举如下：

+   •

    我们介绍了代码数据中的结构以及生成过程，然后总结了结构化代码理解的下游任务。

+   •

    我们提出了一种基于结构的深度学习模型的新分类法，将其分为基于序列的模型和基于图的模型。

+   •

    我们概述了结构化代码理解的挑战、未解问题和未来方向。

调查内容安排如下。在第二部分中，我们介绍了代码中的结构及其如何从代码数据中提取。在第三部分和第四部分中，我们分别介绍了基于序列模型和基于图模型的模型。我们首先概述了这些模型如何改变结构，然后按核心模型对它们进行分类。在第五部分中，我们对基于序列的模型和基于图的模型进行了讨论和比较。代码表示之后的下游任务在第六部分中介绍。接着，我们在第七部分中总结了相关的指标和数据集。在第八部分中，我们尝试讨论一些开放的研究问题。最后，我们在第九部分中得出结论。

## 2\. 初步

### 2.1\. 代码中的结构

#### 2.1.1\. 概述

首先，我们将介绍程序可以生成的基本结构。表 1 总结了最常用的符号。以下是我们用来说明代码数据中的结构的一个 Python 代码片段。

{python}

def add(a,b): x=0 if(a¿b): x=a-b; else: x=a+b; return x

res=add(1,2) print(res)

表 1\. 符号总结。

| 符号 | 描述 | 符号 | 描述 | 符号 | 描述 |
| --- | --- | --- | --- | --- | --- |

|

&#124; $P$ &#124;

|

&#124; 程序 &#124;

|

&#124; $S$ &#124;

|

&#124; 代码片段 &#124;

|

&#124; $F$ &#124;

|

&#124; 函数 &#124;

|

|

&#124; $A$ &#124;

|

&#124; 代码片段的抽象语法树 &#124;

&#124; 或一个程序 &#124;

|

&#124; $G_{c}$ &#124;

|

&#124; 代码片段的控制流图 &#124;

&#124; 或一个程序 &#124;

|

&#124; $G_{d}$ &#124;

|

&#124; 代码片段的数据流图 &#124;

&#124; 或一个程序 &#124;

|

|

&#124; $path$ &#124;

|

&#124; 提取的节点序列 &#124;

&#124; 从抽象语法树中 &#124;

|

&#124; $n$ &#124;

|

&#124; 代码片段的长度（令牌数） &#124;

&#124; 或一个程序 &#124;

|

&#124; $y$ &#124;

|

&#124; 代码标签 &#124;

|

|

&#124; $D$ &#124;

|

&#124; 自然语言中的代码描述 &#124;

&#124; 从 AST 中 &#124;

|

&#124; $t$ &#124;

|

&#124; 单一标记 &#124;

|

&#124; $H$ &#124;

|

&#124; 中间表示 &#124;

&#124; 代码中的符号 &#124;

|

从源代码片段生成结构的一般技术如图 1 所示。词法分析器首先将代码转换为基于标记的序列。序列中的每个标记具有两个属性，类型和值。代码的词法分析类似于自然语言中的标记化阶段。受到 Hindle 等人（Hindle et al., 2012）的启发，本文中我们将这些未经处理的代码序列称为自然代码序列（NCS），以保持讨论的一致性，在其他文章中可能称为标记序列或代码序列。

语法分析器，也称为解析器，接收标记并基于代码片段的语法生成抽象语法树（AST）。抽象语法树随后被用于语义分析器以验证语义一致性，并由中间代码生成器构建中间表示，这取决于编程语言。控制流和数据流都是类似图形的中间表示，分别称为控制流图（CFG）和数据流图（DFG）。

在软件工程中建立的其他结构，如 UML 图和程序依赖图，除了基本结构之外。

![参见说明](img/aa52b4cd8439eb0757a7766f8682d3ac.png)

图 1. 从代码生成的基本结构包括（a）自然代码序列，（b）抽象语法树，（c）控制流图和（d）数据流图。编译器的前端由四个组件（词法分析器、语义分析器、语法分析器和中间代码生成器）构成。

#### 2.1.2. 自然代码序列

给定程序$P$或代码片段$S$，自然代码序列（NCS）$S=\{t_{1},t_{2},...,t_{n}\}$由词法分析器获得，$t_{i}$指代代码中的标记。使用 NCS 表示代码是最简单且最常见的方法。序列中标记的位置对应于其在代码片段中出现的顺序。

#### 2.1.3. 抽象语法树

根据语言语法，代码的抽象语法树（AST）由语法分析器生成并标记为$A$，该树层次性地反映了代码的结构和语法信息。语法树的根节点表示起始符号。内部节点是语法中的非终结符，而叶节点是终结符，通常是由程序员从 NCS 定义的变量和标识符。图 2 是上述 Python 代码片段的 AST。

![参见说明](img/608be885928af50420bfc8a0eb772b58.png)

图 2\. 抽象语法树的示例

#### 2.1.4\. 流图

流图是覆盖源代码语义信息的图。流图中的两个典型流是控制流和数据流，分别表示程序的执行方式和数据的流动方式。图 3 展示了提到的 Python 代码片段的控制流和数据流图的示例。

![参见说明](img/68188532b7f5270b542f0d2811530daa.png)

图 3\. 流图示例。(a) 控制流图，(b) 表达式$x=a-b$的数据流图，(c) 表达式$x1=a1+b1$的数据流图。

##### 控制流图

程序的控制流图（CFG）标记为$G_{c}$，表示程序的不同执行路径。CFG 的每个节点是一个基本块，表示没有分支的行为。图的边表示这些基本块如何相互流动。在示例代码片段中，$a>b$的判断将使程序分为两个分支，一个是$x=a-b$，另一个是$x=a+b$。如图 3 (a)所示，CFG 从决策节点（if 语句）到两个向下节点（两个基本块）有两条边。

##### 数据流图

程序的数据流图（DFG）标记为$G_{d}$，表示变量之间的依赖关系。DFG 可以表示没有条件的代码片段。在示例代码片段中，我们选择两个语句：$x=a-b$和$x=a+b$来绘制它们的 DFG。为了消除对$x$的重复赋值，我们在第二个赋值中重命名变量，将其转换为$x1=a1+b1$。因此，两个 DFG 如图 3 (b)和(c)所示。每个数据流节点表示变量的操作，每条边表示值如何流动。

##### 控制/数据流图

由于 DFG 只能表示没有分支的基本块，因此可以用它来替代 CFG 的基本块，从而得到控制/数据流图（CDFG）。程序的 CDFG 中有两种类型的节点，决策节点和数据流节点。

#### 2.1.5\. 其他结构

除了上述结构，还有一些在代码理解中不常见的其他代码结构。

##### 其他中间表示

中间表示（IR）是一种可以从编译器（如 LLVM 编译器基础设施（Lattner 和 Adve，2004））中获得的数据结构。前端编译器编译源代码并生成 IR，供后端编译器优化和翻译。在 LLVM 基础设施中，IR 以静态单赋值（SSA）形式存在。IR 的广义定义包括流图以及其他图结构。程序依赖图（PDG）（Ferrante 等，1987a）是中间表示之一，它明确表示程序中每个操作的数据和控制依赖关系。PDG 对通过单次遍历进行优化非常有用。

##### 统一建模语言

统一建模语言（UML）是一种广泛使用的语言，用于指定、可视化和记录软件密集型系统的文档。UML 类图是一种静态结构图，通过展示系统类及其属性、操作（或方法）和关系，描述系统的结构。

### 2.2\. 深度学习模型

在这一部分，我们将深入探讨在代码理解任务中常用的深度学习模型，这些模型在其他领域中已被证明有效，现在越来越多的研究人员在代码领域中使用它们。

#### 2.2.1\. 循环神经网络

循环神经网络（RNNs）（Rumelhart 等，1986）是每个单元通过有向循环连接的神经网络。RNNs 可以利用其隐藏状态跟踪序列数据的长期信息。因此，RNNs 是序列建模的常见选择。Vanilla RNNs 存在梯度消失和梯度爆炸问题，这可能降低模型学习长期信息的能力。长短期记忆（LSTM）和门控循环单元（GRU）是两种最常用的 RNN 模型，它们可以避免这些问题并取得更好的结果。

长短期记忆（LSTM）（Hochreiter 和 Schmidhuber，1997）具有来自 RNN 的基本模型。LSTM 的每个单元考虑隐藏状态、当前输入以及来自其内存单元的信息。LSTM 使用 3 个门，输入门、遗忘门和输出门来控制信息的学习和传递。

门控循环单元（GRU）（Cho et al., 2014）将 LSTM 中的输入门和遗忘门合并为一个门，称为更新门，另一个门为重置门。重置门和更新门可以通过隐状态控制对序列信息的记忆或遗忘程度。与 LSTM 相比，GRU 在性能上可比，但计算成本更低。

#### 2.2.2\. Transformer

Bahdanau 等人（Bahdanau et al., 2014）提出了注意力机制，以解决机器翻译任务中信息长度过长和信息丢失的问题。它将编码器中的所有隐状态经过线性加权后输入到解码器中，并为每个输入令牌分配不同的注意力权重，指示哪些输入对输出更为重要。

为了增强计算性能并更好地描述序列中的全局关系，Vaswani 等人（Vaswani et al., 2017）提出了自注意力机制。这是一种特殊的注意力机制，使得序列中任何位置的信息可以直接影响其他令牌的编码。基于自注意力，他们提出了一种新的神经网络模型，称为 Transformer，该模型由多个自注意力组成的注意力块构成。Transformer 的编码器使用自注意力机制将输入序列中的令牌与所有其他令牌关联，同时学习输入的表示。此外，Transformer 的解码器的输入通过自注意力与编码器的输出相关联。Transformer 及其变体，如 Bert、GPT 等，能够处理复杂数据并处理大量序列数据，因此它们常被用作预训练模型，以从大量复杂数据中捕捉丰富的信息。

#### 2.2.3\. 图神经网络

图神经网络（GNNs）是使用节点之间的信息传递来捕捉结构信息和图中的语义信息的深度学习模型。根据 Wu 等人（Wu et al., 2019）的调查，GNN 可以分为四类：递归 GNN、卷积 GNN、图自编码器和时空 GNN。GNN 广泛用于节点分类、边预测和图分类任务。

以下是代码表示相关任务中使用的典型模型。门控图神经网络（GGNN）（Li et al., 2016）使用门控递归单元，并在固定的时间步数内展开传播过程。节点的表示是最终步骤的输出。图卷积网络（GCN）（Kipf 和 Welling, 2017）是卷积 GNN 的一种，它堆叠多个图卷积层以更好地从邻居中提取信息。图注意力网络（GAT）（Veličković et al., 2018）在消息传递步骤中使用注意力机制，以不同的权重聚合邻域信息并更新节点的编码。上述图神经网络是用于学习代码数据中图或节点表示的典型深度学习模型。

#### 2.2.4. 编码器-解码器框架

编码器-解码器框架（Cho et al., 2014）被提出作为解决传统机器翻译难题的方案。输入语言在编码器部分被编码以获得中间表示上下文（Context）。然后，在解码器部分，根据上下文和相关输入逐一生成相应的输出。Sutskever et al.（Sutskever et al., 2014）提出了基于编码器-解码器框架的 seq2seq 模型，以克服无限长输入-输出序列的问题，这有助于使用特殊标记如¡Eos¿进行序列输出。根据具体任务可以选择不同的编码器和解码器，如基于 RNN 的模型、基于 Transformer 的模型、基于 GNN 的模型等。由于代码的自然性和序列性，编码器-解码器模型架构已经成为解决代码生成问题和其他任务的主流方法。例如，Rabinovich et al.（Rabinovich et al., 2017）引入了一种语法网络（ASN），它扩展了编码器-解码器框架以生成 AST。

## 3. 基于序列的模型

在序列相关任务中表现良好的序列模型，如递归神经网络系列（Rumelhart et al., 1986; Hochreiter 和 Schmidhuber, 1997; Cho et al., 2014）和 Transformer（Vaswani et al., 2017），可以有效地应用于与代码相关的任务。它们可以用于编码器-解码器架构，以实现代码摘要和代码生成等下游任务，并且可以学习如何访问代码表示以进行下游活动。

经常使用的术语代码序列指的是由代码本身生成的自然代码序列，指的是第 2.1.2 节中介绍的 NCS。由于代码结构高度结构化，因此有通过处理代码结构输入形成的序列，例如第 2.1.3 节中介绍的 AST，这些指的是扁平化序列。在本节中，我们介绍上述处理序列化代码数据的模型。我们首先展示代码的结构转换，包括获取扁平化序列的不同代码结构预处理方法，然后分别展示处理 NCS 和扁平化序列的模型。

### 3.1\. 结构转换

AST 格式通常用于表达源代码的结构信息。为了让序列模型高效利用代码的结构信息，提供了一些扁平化 AST 的策略。扁平化过程分为四种类型，如图 4 所示，展示了不同扁平化方法获得的多样序列。

##### Type-1: 深度优先遍历

由于抽象语法树（AST）以树的形式表示代码的结构信息，因此以深度优先的方式遍历树结构，使得每个子树上的节点在序列中相邻，是提取扁平化序列的最简单方法。更多模型提倡前序深度优先遍历，因为 AST 中每个子树的根节点（操作符）通常是子树的中心。Type-1 指的是使用深度优先遍历 AST 的结构转换方法。

##### Type-2: AST 路径

推荐通过路径序列化 AST 的方法，因为 AST 中的每两个节点之间都有一条路径。从 AST 中提取路径的方法包括任意节点之间的路径、终端节点之间的路径、从终端节点到根节点的路径等。在代码生成任务中，也使用从终端节点到新节点的路径。Type-2 指的是使用 AST 中节点之间路径的结构转换方法。

##### Type-3: 结构信息添加

为了更好地保留 AST 中的结构信息并使平展序列唯一，提出了基于 Type-1/2 的结构信息添加方法。例如，基于结构的遍历方法使用括号表示 AST 结构，创建的序列中的括号可用于检测某个节点的子树，从而允许生成的序列转换回 AST。另一种保留结构信息的技术是使用“¡”和“¿”来括住非终结节点的子树，该子树对应于代码块。因此，我们将结构转换方法称为 Type-3，因为它在获得的序列中包含了代码结构信息。

##### Type-4：AST 部分保留

AST 包含了大量关于代码的结构和语法信息，但也包含了大量无用的数据。因此，许多方法建议过滤 AST，保留感兴趣的节点，然后平展从过滤后的 AST 中得到的序列。在代码缺陷检测任务中，通常只保留三种类型的 AST 节点，例如方法调用和类实例创建节点、声明节点和控制流节点。Type-4 指的是结构转换方法，它只保留重要的节点。

![参见说明](img/8e41cf0c77213096d47d079e50783536.png)

图 4\. 使用四种结构转换类型获得平展序列的局部 AST 示例，（a）使用深度优先遍历将 AST 转换为序列，（b）显示叶子到叶子路径集中的三条路径，（c）显示 SBT (Hu et al., 2018) 使用括号保留结构信息，（d）仅保留 AST 中节点的类型信息，获得的序列与（a）相同。

### 3.2\. 自然代码序列

NCS 是一种自然表示整个代码片段的方式，因为代码由独立的标记组成。使用 NCS 理解代码的方法和模型在本节中进行描述。

##### N-gram 模型

N-gram 模型，广泛用作早期语言处理方法，假设马尔可夫性质，即当前词的概率仅受其前 N-1 个词的影响，这在一定程度上可以捕捉序列的统计特征。为了利用 NCS 的统计特性，提出了一些早期模型使用 N-Gram 模型来完成代码表示任务。Hindle 等人 (Hindle et al., 2012) 首次在 NCS 上采用 N-gram 模型，发现语言模型有助于通过探索自然性而不是语法或语义来提取局部统计特征。Tu 等人 (Tu et al., 2014) 将 N-gram 模型与缓存组件结合，进一步捕捉源代码的局部性。Karaivanov 等人 (Karaivanov et al., 2014) 利用 N-gram 模型进行基于短语的编程语言翻译。

N-gram 模型利用标记前的所有 N-1 个标记的信息来学习其表示，但它不能使用远程标记信息（如代码中变量的重复使用），也无法为具有相似意义的标记构建相同的向量表示。因此，N-Gram 模型逐渐被后来在代码表示任务中引入的更先进的学习模型所替代。

##### 卷积神经网络

卷积神经网络（CNN）最初是为了处理图像而提出的，通过卷积核捕捉局部图像的特征来学习图像表示。同时，它也可以从输入句子中提取重要的 n-gram 特征，以创建潜在句子信息的语义表示，以便用于下游任务，同时有效捕捉代码中的丰富结构模式，因此有些工作使用 CNN 学习代码表示。为了总结代码片段，Allamanis 等人（2016）使用了一种注意力神经网络，该网络对输入标记进行卷积，以上下文相关的方式检测局部时间不变和长范围的主题注意力特征。为了更好的代码搜索，CARLCS-CNN（Shuai 等人，2020）首先分别使用 CNN 嵌入代码和查询，因为 CNN 能够捕捉查询和代码中的信息关键词，然后通过共同注意机制学习嵌入代码和查询的相互依赖表示。CNN 无法建模代码序列中的长范围依赖，局部对词序的敏感性有限，而且由于代码序列通常较大，CNN 模型在实际应用中不太常用于学习理解代码。

##### 循环神经网络

如前面第 2.2.1 节所述，RNN 及其变体在序列任务上表现卓越，因此有大量基于 RNN 的模型用于处理 NCS。Veselin 等人（RaychevVeselin 等人，2014）已经证明，经过良好训练的 RNN 在处理 NCS 时可以超越 N-gram 模型（Hindle 等人，2012）。Dam 等人（Dam 等人，2016）提出使用 LSTM 来预测下一个 token，以解决 n-gram 模型无法捕捉序列中 token 依赖的问题。CodeNN（Iyer 等人，2016）使用带有注意力机制的 LSTM 来生成描述代码片段的句子。Liu 等人（Liu 等人，2016）在 Bi-LSTM 的输出上应用潜在注意力，以更好地将自然语言描述翻译为 If-Then 程序。Bhoopchand 等人（Bhoopchand 等人，2016）通过增强 LSTM 的指针网络，专门用于引用预定义的标识符类别，以更好地捕捉代码中的长程依赖，从而为下一个 token 输入提供更好的建议。CODEnn（Gu 等人，2018）提供了一个深度架构，由代码嵌入网络、描述嵌入网络和相似性模块组成，以对齐代码-描述对的嵌入。代码嵌入网络和描述嵌入网络都使用 LSTM。Tal 等人（Ben-Nun 等人，2018a）利用 RNN 学习一种语言无关的中间表示，该表示由代码语法结构生成。Vasic 等人（Vasic 等人，2019）提出了一种解决一般变量误用问题的方案，其中枚举搜索被包含 LSTM 的神经网络所取代，该网络共同定位和修复故障。CodeGRU（Hussain 等人，2020）进一步在代码序列处理过程中应用了 GRU，以捕捉上下文依赖。与 CNN 相比，RNN 家族可以处理任意长度的输入，并且具有更灵活的代码序列建模能力，但传统 RNN 存在梯度消失和梯度爆炸的问题。LSTM 和 GRU 作为传统 RNN 的变体，在一定程度上可以学习长期依赖，并解决梯度爆炸和消失的问题，逐渐成为当前 RNN 家族的核心。

##### Transformer

Transformer 能够成功处理远程依赖问题，克服 RNN 无法并行计算的限制，并采用自注意力机制生成更具解释性的模型，正如在 2.2.2 节中所描述的。Transformer 正在被用于越来越多的序列相关工作，NCS 也不例外。Ahmad 等人（Ahmad et al., 2020）首次使用 Transformer 进行代码总结，以处理源代码中普遍存在的长距离依赖。TFix（Berabi et al., 2021）直接处理程序文本，并将代码修复问题表述为文本到文本的任务，因此可以利用在自然语言上预训练的强大 Transformer 模型，并对其进行微调以生成代码修复。CodeBERT（Feng et al., 2020）、CuBert（Kanade et al., 2020）、GPT-C（Svyatkovskiy et al., 2020）和 CodeT5（Wang et al., 2021d）利用 NCS 和相关自然语言对 Transformer 架构进行预训练，用于下游任务，如代码搜索、代码克隆检测和代码总结。OSCAR（Peng et al., 2021）和 GraphCodeBERT（Guo et al., 2020）也是基于 Transformer 的预训练模型，使用 NCS，同时利用流图的语义信息。OSCAR 通过位置编码将 GCF 信息添加到模型训练中。GraphCodeBERT 将 DFG 作为输入的一部分，同时利用节点和边关系进行图引导的掩码注意力，以更好地理解代码。

Transformer 能够有效地学习大量数据，但与 RNN 等模型相比，其内存和处理要求非常高。因此，后续工作应考虑资源消耗和性能提升，并根据当前情况选择合适的模型。

##### 其他

除了在 NCS 相关任务中广泛使用的 CNN、RNN 等模型外，相关工作中还引入了其他模型。Sachdev 等人（Sachdev et al., 2018）创建了每个代码片段在方法级别粒度上的连续向量嵌入，将给定的自然语言查询映射到相同的向量空间，并使用向量距离来模拟代码片段与给定查询的相关性。CCLearner（Li et al., 2017a）从已知的方法级代码克隆和非克隆中提取令牌序列来训练深度神经网络（分类器），然后使用分类器在给定的代码库中检测克隆。SCC（Alreshedy et al., 2018）是一个能够识别 21 种不同编程语言中代码片段编程语言的分类器，它使用了一个通过 Stack Overflow 帖子训练的多项式朴素贝叶斯（MNB）分类器。Sachdev 等人（Sachdev et al., 2018）提出了一种简单而有效的无监督模型，结合了**word2vec**（Mikolov et al., 2013）和信息检索方法用于代码搜索。UNIF（Cambronero et al., 2019a）首先使用**word2vec**对代码/查询进行嵌入，然后将代码嵌入与注意力机制结合。这些模型在特定任务中表现更好，因此，处理 NCS 的后续工作不应仅限于采用主流模型。

### 3.3. 扁平化序列

在 3.1 中指出的四种结构变换可以用来获得扁平化序列。尽管扁平化过程消耗更多资源，但扁平化序列保留了一些关于代码的结构信息，适用的模型可以从扁平化序列中比 NCS 学到更多关于代码的信息。通过不同结构变换获得的序列可能适用于不同的模型。处理扁平化序列的模型在以下内容中进行了描述，这些模型在自然语言处理领域也表现良好。我们在本节中对 RNN 家族进行了更精细的细分，包括**Vanilla LSTM**、**双向 LSTM**和**GRU**，因为 RNN 及其变体在扁平化序列中的应用种类繁多。

##### Word2vec

word2vec 是一种将输入 token 转换为向量表示的方法，转换后的向量在一定程度上包含了 token 的上下文信息。Word2vec 包含两种训练模型，CBOW（连续词袋模型）和 Skip-gram（连续 Skip-gram 模型），它们具有较强的通用性，因此在早期的展平序列代码表示学习中得到了应用。API2Vec（Nguyen et al., 2017）利用 Type-4 遍历 AST，根据与 API 相关的语法单位构建注释序列。这些序列随后用于训练 CBOW 生成 API 嵌入，这些嵌入可能用于将等效 API 用法从 Java 迁移到 C#。Alon 等（Alon et al., 2018c）首先使用 Type-3 通过增加节点间路径的上升和下降动量信息来获得展平序列，然后使用 word2vec 完成方法名称的预测。由于 Word2vec 无法学习多义词的表示并且无法成功捕捉长距离依赖，因此正在进行进一步的工作，将 Word2vec 与其他模型结合，以更好地学习展平序列。

##### 深度置信网络（Deep Belief Network）

深度置信网络（DBN）（Bengio, 2009）是一种生成模型，利用多层神经网络从训练数据中学习表示，以最大概率重建输入数据的语义和内容。DBN 可用于特征识别、数据分类、数据生成等任务。Wang 等（Wang et al., 2016, 2020a）使用 Type-4，然后利用 DBN 完成缺陷预测。他们仅从三种 AST 节点生成展平序列：方法调用和类实例创建节点、声明节点和控制流节点。由于方法、类和类型的名称通常是项目特定的，跨多个项目的相同名称的方法非常少。为了获得更好的检测结果，他们提取了所有三类 AST 节点用于跨项目缺陷预测（CPDP），但不是利用其名称，而是使用其 AST 节点类型，如方法声明和方法调用，用于声明节点和控制流节点。DBN 可以从从 AST 中提取的 token 向量中自动学习语义特征，是第一个通过应用深度架构成功训练的非卷积模型之一。然而，与其他无监督或生成学习算法相比，DBN 如今已经不再受青睐，并且使用较少。

##### 香草长短期记忆（Vanilla Long Short-Term Memory）

Vanilla LSTM 是在第 2.2.1 节中介绍的 LSTM，它能够在一定程度上学习长期依赖，并且在处理平展序列的工作中被广泛使用。刘等（刘等， 2017）使用 Type-1，并探索了几种简单 LSTM 架构的变体，以解决不同变体的代码补全问题。李等（李等， 2017b）利用 Type-3，这允许在类型节点中存储关于 AST 是否有子节点和/或右兄弟的两个额外信息位。提议的指针混合网络包括两个主要组件：全局 RNN 组件（LSTM）和本地指针组件，后者利用指针网络根据学习到的位置权重指向本地上下文中的先前位置，以解决 OoV 问题。DeepCom（胡等， 2018）使用 Type-3 并设计了一种基于结构的遍历（SBT）方法，以更好地保留代码中的结构信息。SBT 遍历方法使用括号来表示 AST 的结构，并且创建序列中的括号可能被用来确定给定节点的子树，从而允许生成的序列被转换回 AST。DeepCom 采用 seq2seq 模型，其中 LSTM 作为编码器和解码器生成代码片段摘要。code2vec（阿隆等， 2019c）采用 Type-2，通过代码中所有终端节点对之间的路径集合来表示代码片段，以完成对代码方法名称的预测，并使用 LSTM 学习内部非终端节点序列的表示。为了更好地进行程序分类，Compton 等（Compton 等， 2020）研究了在训练 code2vec 模型过程中混淆变量名称的效果，以迫使其依赖于代码的结构而非具体名称，并考虑了一种通过聚合方法嵌入创建类级别嵌入的简单方法。Seml（梁等，[[n.d.]](#bib.bib103)）使用 Type-4 并将过滤 AST 后获得的序列发送到 LSTM 以完成代码的缺陷检测。SA-LSTM（刘等， 2020d）使用 Type-3，它用 ¡ 和 ¿ 包围每个非叶节点的子树，这对应于代码块。SA-LSTM 通过一个栈来增强 LSTM 网络，以存储和恢复基于代码结构的上下文信息，从而对代码的层次结构进行建模。

不同的工作将修改 LSTM 以适应自己的任务，但 LSTM 在并行处理方面存在劣势，无法完全解决梯度问题，也不能处理非常大规模的序列，后来引入的 Bi-LSTM 和 GRU 也面临相同的问题。最新的努力将 LSTM 或其他 RNN 变体作为模型的一个组件，并与其他模型混合，以更好地利用 RNN 及其变体在序列处理上的优势。

##### 双向长短期记忆

与传统的仅保留先前信息的 LSTM 相比，双向长短期记忆（Bi-LSTM）还可以利用后续信息，从而更好地捕捉双向的语义依赖。`code2seq`（Alon 等， 2018a）采用与`code2vec`相同的 Type-2 方法来获得扁平化序列，并使用 Bi-LSTM 学习内部非终结节点序列的表示。`DeepCPDP`（Chen 等， 2019a）使用 Type-4，采用简化的抽象语法树（SimAST）来表示每个提取程序的源代码。`DeepCPDP`使用 SimASTToken2Vec，一种基于无监督的嵌入方法，并将代码输入分类为缺陷或非缺陷，使用带有注意力机制的 Bi-LSTM 和逻辑回归。`Pythia`（Svyatkovskiy 等， 2019）使用 Type-1 来预测开发者在编程中想要使用的方法名称和 API 调用。`Pythia`通过一个全连接层将初始（通过 word2vec 获得）和中间（由 Bi-LSTM 学习）向量表示进行汇总，以获得最终的预测向量。

##### 门控递归单元

在第 2.2.1 节中介绍的 GRU 是 LSTM 的简化版本，具有较少的门控，因此更易于收敛，并且参数较少，因此一些工作会选择使用 GRU 来学习展平序列的表示。ast-attendgru（LeClair 等，2019）使用 Type-3，并提出了 SBT-AO，该方法修改了 SBT AST Flastting 过程，以模拟只能提取 AST 的情况。SBT-AO 将代码中的所有单词（除了官方 Java API 类名）替换为特殊的¡OTHER¿标记，保持了 SBT 中的所有代码结构。它使用两个带有注意力机制的 GRU 分别处理 NCS 和 SBT-AO，以获取上下文向量，然后从上下文向量中逐字预测摘要，这符合 seq2seq 模型中的典型做法。Hybrid-Deepcom（Hu 等，2020a）使用与 Deepcom 相同的 Type-4 来获得展平序列。它还采用 seq2seq 模型进行代码摘要，并分别利用两个 GRU 作为编码器来处理 NCS 和展平序列，从而获取代码片段的词汇和结构信息。GRU 的门控较少，参数相对较少，因此更易于收敛，计算开销也较低，而在大多数任务中结果相似，但 LSTM 在较大的数据集上表现更好，因此在代码序列任务中选择 LSTM 和 GRU 时需要考虑数据集大小、训练效果和训练时间。

##### Transformer

如前所述，相较于 RNN，Transformer 是一个更复杂、更强大的模型，近年来越来越多的工作使用 transform 来学习扁平化的序列以完成相关任务。SLM（Alon et al., 2020）使用 Type-2 将代码表示为从根节点到目标节点及所有叶子节点的路径。SLM 使用 LSTM 分别获取所有路径的表示，然后使用 Transformer 对所有叶子节点到目标节点的路径表示进行上下文化，同时将父节点中的位置索引添加到从根节点到目标节点的路径表示中，并通过注意力机制进行传递。最终向量通过组合路径表示来对目标节点进行预测。Kim et al.（Kim et al., 2020）提出了三种基于 Transformer 的方法来更好地预测开发者即将输入的 token：1. pathTrans，使用 Type-2 通过从叶子节点到根节点的路径对 AST 进行序列化；2. TravTrans，使用 Type-1，并采用先序遍历的方法从 AST 中获得序列；3. TravTrans+，使用 Type-3，添加一个矩阵来保存 TravTrans 中两个节点之间的唯一路径，以增强 Transformer 的自注意力块。实验证明，TravTrans+效果更佳。Liu et al.（Liu et al., 2020c）同时使用 Type-1 和 Type-2 来完成代码预测。它使用 Transformer-XL 对先序遍历获得的序列进行编码，并使用 Bi-LSTM 对从目标节点到根节点的路径进行编码，保留目标节点的层次信息。TreeBERT（Xue et al., 2021）采用 Type-2，通过从根节点到 AST 中叶子节点的多个路径来表示整个代码片段，然后使用修改后的 Transformer 架构完成预训练。每个节点的位置嵌入是根据其在 AST 中的层次信息和父节点的位置信息生成的，以更好地利用代码的结构信息。由于 Transformer 需要大量的数据和计算资源，后续工作在使用 Transformer 时应考虑的不仅仅是模型性能。

## 4. 基于图的模型

尽管使用序列化结构作为输入的顺序模型在许多工作中是简单且可见的，但代码片段的线性顺序不可避免地丢失了层次语法信息。因此，近年来的工作更加关注捕捉代码的语法信息。本节描述并分类了用于代码理解的基于图的模型，基于方法中使用的无序列化结构。

在图基模型中使用的结构通常是 AST 和 Flow Graph，包括在章节 2.1.3 和 2.1.4 中介绍的 CFG 和 DFG。我们还在章节 2.1.5 中介绍了使用其他结构的模型。与基于序列的模型不同，这些程序的结构被视为树结构或图结构。在本节中，我们首先介绍这些结构中的变换，然后将不同的模型分为 AST、流图、它们的组合以及从源代码生成的其他较少见的结构。

### 4.1\. 结构变换

与序列的变换不同，图结构的修改往往基于一个典型的图结构来添加节点或边。我们将图的结构变换总结为三类，前两类变换是在保留图结构的基础上进行的，以便进一步学习图的表示，而最后一种变换则是提取图的信息并构建一个新的机制或 DSL（领域特定语言），而不保留程序图。

##### 类型-1：添加边

在基于 AST 的结构中，方法从两个不同的角度处理结构，以更好地利用结构信息。一种将结构视为“树”，这意味着结构具有有向边且层次结构得以保留。另一种通过添加各种类型的边来扩展 AST 结构，最终使边双向，从而使原始的 AST 结构成为“图”。在这两种视角中，添加边是最常见的变换，以保留更多的结构信息。Allamanis 等人（Allamanis et al., 2018b）使用 AST 作为基础，并添加额外的边以捕获数据流信息。这些边包含从 AST 衍生的类型（例如 Child 和 NextToken）和语义（例如 LastUse, LastWrite）。Wang 等人（Wang and Li, 2021）通过添加父子边扩展了 AST。Dinella（Dinella et al., 2020）在叶节点之间添加了 SuccToken 边。

##### 类型-2：组合

一些方法结合了基于 AST 和流图的结构，如 CDFG 结构，它是 CFG 和 DFG 的结合。还提出了其他图基模型结构以及一些基于这三种基本结构的新结构。这些组合将在第 4.4 节中讨论和介绍。

##### 类型 3：信息提取

一些方法选择不保留 AST、流图或其他图结构的结构，而是提出了基于从这些图结构中提取的语法和语义信息的新结构、机制或 DSL。这不是我们介绍图基模型的重点，但仍然是一种从图结构变换的结构。例如，Raychev 等人（Raychev et al., 2016）提出了一种构建代码概率模型的方法，并生成了一个名为 TGEN 的 DSL，用于遍历 AST 和积累条件上下文。Cvitkovic 等人（Cvitkovic et al., 2019）提出了一种图结构缓存，用于解决词汇外问题。一个名为 Tocopo（Tarlow et al., 2020b）的编辑 DSL 被创建用于在修复 bug 问题时进行代码编辑，它包含了 token 表达式、复制表达式和指针表达式。Tocopo 是一系列编辑操作，表示对 AST 的修改。

### 4.2\. 基于 AST 的结构

如前所述，由于 AST 结构视角的不同，以下模型有很大的差异。

#### 4.2.1\. 树视角

由于树结构的庞大和深度结构，某些方法往往使用树结构的递归来减少程序的复杂性，特别是在将信息从子树传递到完整树时。Shi 等人（Shi et al., 2021）提出了一种方法，将程序的整个 AST 分层分解和重建为子树，以获得代码片段的表示。ASTNN（Zhang et al., 2019a）使用前序遍历算法将 AST 划分为一组语句子树，递归地将它们编码为向量，并最终通过 BiGRU 和 RvNN 捕捉的自然性学习源代码的表示。这种方法可以降低训练的难度。

大多数方法倾向于在 AST 结构的基础上修改递归神经网络或 Seq2seq 模型，例如基于 AST 的 LSTM（Wan 等，2018）和基于树的 Seq2seq 模型（Chakraborty 等，2020）。Tree-LSTM 是代码理解模型中最常被修改的方法之一。Tree-LSTM（Tai 等，2015）是标准 LSTM 在树结构上的一种推广，它通过输入向量和任意多个子单元的隐藏状态来组合状态。然而，当树具有有序节点（如 AST）时，原始的 Tree-LSTM 无法处理这种情况。为了解决这个问题，开发了一种 Multi-way Tree-LSTM 模型，它扩展了 Tree-LSTM 模型。该模型通过在每个门之前添加双向 LSTM 来捕捉节点之间的交互信息，从而封装有序子节点的信息。在代码摘要任务中，Multi-way Tree-LSTM 比序列模型更有效地学习 AST 中的信息。

相比递归神经网络，卷积神经网络（CNN）能够更容易地进行训练，并且在并行计算机制下需要的时间更少。同时，对原始 CNN 的适配，如基于树的 CNN，已在代码综合任务中证明了其有效性。Mou 等（Mou 等，2014）提出了一种用于程序分类任务的基于树的卷积神经网络（TBCNN）。卷积层可以捕捉来自 AST 的信息。Chen 等（Chen 等，2019b）提出了一种基于树的 LSTM，用于 API 增强的 AST 进行克隆检测。通过添加新的节点类型来标识 API 名称，对原始 AST 进行了修改。该模型称为 TBCAA，使用基于树的 CNN 学习代码的表示，其中每个卷积核具有三角形的形状。

#### 4.2.2\. 图视角

为了编码 AST 结构，大多数方法将其视为图，并使用图神经网络。两种类型的 GNN 是以下基于图的 AST 模型中最常用的模型，分别是门控图神经网络（GGNN）和卷积图神经网络。我们还介绍了其他图神经网络，如图注意力网络，这些网络特别用于 AST 结构。

##### 门控图神经网络

门控图神经网络（Gated Graph Neural Network），作为一种递归图神经网络，是李等人（Li et al.，2015）在提出门控图神经网络用于推断程序验证公式时开发的第一个用于代码相关任务的图神经网络。GGNN 使用 GRU 单元来更新节点的状态，结合来自邻近节点的信息和前一时间戳的节点状态。Fernandes 等人（Fernandes et al.，2019）将 GGNN 修改为一个框架，以扩展现有的序列编码器，并在三个摘要任务上进行了实验。图基语法修正（Graph-based Grammar Fix，GGF）（Wu et al.，2020）使用 GRU 和 GGNN 的混合体作为编码器来编码由错误代码创建的子抽象语法树（sub-AST），并使用令牌替换机制作为解码器生成代码修复操作。Graph2diff（Tarlow et al.，2020b）使用编码器-解码器框架来预测差异，也称为编辑操作。该模型将源代码、错误配置文件和编译器诊断消息作为图输入。在编码器阶段使用 GGNN 来显式表示代码的结构信息。尽管 GGNN 展示了从图的抽象语法树中学习语法信息的能力，但作为一种图神经网络，它只能聚合局部信息而非全局信息。因此，一些工作还将其与序列模型结合使用，例如 Graph Sandwiches（Hellendoorn et al.，2019）。此外，由于 GGNN 使用 RNN，它需要更多的内存来保留图中所有节点的隐藏状态，尽管它消除了需要限制参数以确保收敛的需求。

##### 卷积图神经网络

卷积图神经网络（convGNN）基于节点向量和图中节点的邻居学习节点的表示。从邻居处获得的信息通过聚合过程进行结合。与如 GGNN 这样的递归图神经网络相比，卷积图神经网络可以堆叠多个图卷积层，以更好地在节点之间传播信息，从而结合来自邻居的信息。LeClair 等（LeClair 等，2020）使用卷积 GNN 对 AST 节点和边进行编码，以进行代码摘要。ConvGNN 层的输入是 AST 标记嵌入和边。经过 ConvGNN 层后，模型使用注意力机制来学习源代码和 AST 中的重要标记。Ji 等（Ji 等，2021b）也使用 GCN 对 AST 进行编码以执行代码克隆任务。Liu 等（Liu 等，2021c）提出了一个 Jupyter 笔记本的代码文档生成任务。在生成文档时，模型 HAConvGNN 考虑了相关的代码单元和代码标记信息。编码器中包含了卷积 GNN 层和 GRU 层，用于代码单元的 AST。GRU 层的输出将作为层次注意力机制的输入，以更好地保留图结构。

##### 其他图神经网络

有一些方法使用其他图神经网络。不同于 GCN，图注意力网络（GAT）（Veličković 等，2018）在消息传递阶段执行自注意力机制以学习图表示。Wang 等（Wang 和 Li，2021）将部分 AST 的展平序列建模为 AST 图。为了减少信息损失，记录了父子关系和位置相关信息。此外，提出了三种类型的 AST 图注意力块，以捕捉结构信息以学习图的表示。与 GCN 和 GGNN 相比，GAT 模型通过利用注意力机制提高了代码补全或代码摘要任务的可解释性。Hoppity（Dinella 等，2020）使用了另一种类型的 GNN，即图同构网络（GIN），作为外部存储器来编码有缺陷程序的 AST，进一步使用由 LSTM 实现的中央控制器来预测修复错误的动作序列。当图结构发生变化时，控制器将扩展或缩减记忆。

### 4.3\. 基于流图的结构

如前所述，流图分为两种类型：控制流图（CFG）和数据流图（DFG）。虽然流图更可能被视为图，但很少有工作将流图视为树。例如，BAST (Lin et al., 2021) 根据 CFG 的支配树中的块拆分方法的代码，并为拆分的代码生成相应的 AST。拆分 AST 的表示在预训练阶段用于预测支配树中的下一个拆分 AST。CFG 仅用于拆分 AST，但可以使模型在处理大型程序时更高效和可扩展。

从图的角度考虑 CFG 并应用深度学习方法来表示 CFG 的工作如下所述。归因控制流图（ACFG）是以下一些工作中的常见预处理步骤，特别是在二进制代码相似性检测中，例如 Genius(Feng et al., 2016a)、Gemini (Xu et al., 2017) 和 BugGraph (Ji et al., 2021a)。还有一些典型的 CFG 修改，例如，懒绑定 CFG (Nguyen et al., 2018)、过程间 CFG (ICFG) (Duan et al., 2020)。

##### 卷积神经网络

卷积神经网络（CNN）在许多训练数据中具有平移不变性，因此，它不仅可以捕捉代码的语义信息，还适用于顺序感知建模。Nguyen et al. (Nguyen et al., 2018) 使用 CNN 对由懒绑定 CFG 转换的邻接矩阵进行操作。CFG 将被转换为像素图像，然后使用 CNN 模型识别目标对象是否出现在图像中。该方法可以应用于恶意软件检测。Yu et al. (Yu et al., 2020) 使用 Bert 和 CNN 学习 CFG 图嵌入，可以包括语义、结构和顺序信息。在邻接节点预测任务中，Bert 模型用于预训练令牌和块嵌入。CFG 的顺序信息使用 CNN 模型提取。如前所述，CNN 模型被作为学习顺序信息或下游任务的组件使用，而不是直接用于构建代码数据图的表示。

##### 卷积图神经网络

DGCNN（Zhang et al., 2018），作为 ConvGNN 之一，提出了一种类似于 SortPooling 的池化策略。这些方法允许通过邻域消息传递快速聚合属性信息，因此，它适合将结构信息嵌入向量以进行进一步的分类。为了应对恶意软件分类挑战，Yan 等人（Yan et al., 2019）使用 DGCNN 嵌入 CFG。CFG 首先将被转换为带有属性的 CFG，代码特征定义了这些属性。DGCNN 用于使用自适应最大池化来聚合这些属性，以连接层输出。

##### 图注意力网络

Li 等人（Li et al., 2020）提出了一种基于事件的方法 CSEM 用于克隆检测。GAT 提取每个语句的上下文信息，这些信息通过嵌入的事件建模以捕获执行语义。BugGraph（Ji et al., 2021a）在两个步骤中比较源代码和二进制代码的相似性：源二进制规范化和代码相似性计算。在代码相似性计算步骤中，BugGraph 计算目标代码和比较二进制代码之间的相似性。在对两个代码进行反汇编后，每个函数将构建其 ACFG，并使用 GAT 与三元组损失作为模型输出，生成每个图的嵌入。

##### 其他图神经网络

如前所述，Yu 等人（Yu et al., 2020）也使用 MPNN（Gilmer et al., 2017）来计算控制流图的图嵌入，以进行有序建模。BugLab（Allamanis et al., 2021）使用标准消息传递图神经网络来表示代码实体的图。该图包括关于程序内部数据和控制流、类型以及文档的语法实体和关系。BugLab 训练两个模型，一个选择模型和一个检测模型，用于预测代码片段的重写。选择模型引入有缺陷的代码，而检测模型则检测并修复这些缺陷。Wang 等人（Wang et al., 2020b）提出了一种新的图神经架构，称为图间隔神经网络（GINN），用于学习代码嵌入。GINN 以程序的控制流图为输入，并使用三个操作符对其进行抽象。使用分区操作符将 CFG 划分为一系列间隔。间隔之间传递的消息受到限制。然后应用提升操作符，将每个激活的间隔替换为单个创建的节点，直到达到足够的传播点。最后，降低操作符将恢复原始的 CFG。GINN 模型仅使用循环结构来学习特征表示，并且该方法在程序嵌入上表现出改进。

### 4.4\. 组合结构

在某些情况下，两种类型的流图与 AST 可能不会单独出现。以下部分将重点探讨这些结构如何组合以及可以从中获取哪些信息。

##### 控制流与数据流

对于两种图的组合，提出了一些新颖的值图，例如程序的跨过程值流图（IVFG）。IVFG 是使用 LLVM-IR 创建的，它结合了代码控制流和别名感知的数据流。IVFG 是一个具有多条边的有向图。Flow2Vec（Sui 等，2020）是一种将代码与 IVFG 嵌入的创新方法。该方法包括三个步骤：预嵌入、通过矩阵乘法的值流可达性以及高阶接近度近似。Brauckmann 等人（Brauckmann 等，2020）使用 AST 或控制数据流图（CDFGs）作为预测模型的输入以学习代码表示。预测模型的核心是嵌入传播层中的 GGNN。该模型在 OpenCL 内核的两个复杂任务上显示了其有效性，包括 CPU/GPU 映射和线程粗化。Deepsim（Zhao 和 Huang，2018）将代码控制流图和数据流图编码成语义矩阵，并使用深度学习模型来衡量函数相似性。语义矩阵包含三个特征：变量特征、基本块特征以及变量和基本块之间的关系特征。

##### AST 和流图

Allamanis 等人（Allamanis 等，2018b）提出了一种基于 GGNN 的方法，用于构建源代码图、命名变量和检测变量误用。程序图结合了 AST 和数据流图，包含源代码的语法和语义结构信息。一些工作将 AST 和 CFG 结合用于多模态学习，以生成代码的混合表示，例如（Wan 等，2019a）。Devign（Zhou 等，2019）利用 GGNN 和 Conv 模块进行图级分类。图表示基于 AST 结构，并添加了来自 CFG、DFG 和 NCS 的多种边。Conv 模块学习节点特征的层次信息。多模态学习可以减少仅使用 AST 表示代码的限制。AST 和流图的组合可以涵盖源代码的额外语义信息。

### 4.5. 其他结构

本节将讨论与 AST 和流图显著不同的其他结构。尽管 AST 和流图已在大多数先前工作中使用，但一个程序项目仍包含其他图形。一些是传统图形，如第 2.1.5 节中介绍的程序依赖图和 UML 图，而其他则是最近提出的结构，旨在实现更好的语义感知或结构感知代码理解。

##### 程序依赖图

Li 等人（Li et al.，2019b）提出了一种基于注意力的神经网络来学习用于错误检测任务的代码表示。全局上下文通过程序依赖图和数据流图使用 Node2Vec 提取，而局部上下文通过之前的错误修复和 AST 路径提取。全局上下文和局部上下文将统一为路径向量进行进一步分析。

##### UML 图

CoCoSUM（Wang et al.，2021a）对代码总结任务进行了 UML 图建模。该框架使用基于变换器的模型对类名进行编码作为类内上下文，并使用多关系图神经网络（MRGNN）对 UML 图进行编码作为类间上下文。这两种嵌入与标记和 AST 的嵌入一起传递给基于注意力的解码器，以生成代码摘要。

##### 代码属性图

Yamaguchi 等人（Yamaguchi et al.，2014）对三种代码相关结构进行了建模：AST、CFG 和程序依赖图（Ferrante et al.，1987b），并将它们组合成代码属性图。新提出的数据结构能够通过图遍历来表征漏洞类型。Liu 等人（Liu et al.，2020a）使用代码属性图，并在代码总结任务中提出了一种混合 GNN 框架。该框架融合了静态图和动态图，以捕捉图的全局信息。

##### 程序反馈图

Yasunaga 等人（Yasunaga 和 Liang，2020a）引入了一种程序反馈图来建模程序修复任务中的推理过程。程序反馈图中的节点包括诊断参数中的标记、源代码中的出现位置以及代码中的剩余标识符。框架 DrRepair 最初使用 LSTM 对源代码进行编码，然后使用 GAT 进一步对图进行推理。

## 5\. 讨论与比较

我们在代码理解中介绍了两种模型：基于序列的模型和基于图的模型，分别在第三部分和第四部分中进行了讨论。

基于序列的模型是用于处理代码序列的模型，如 NCS 和通过第 3.1 节介绍的 AST 经过多种转换方法获得的扁平序列。传统的统计语言模型，如 N-gram 模型，在早期工作中被广泛使用。随着深度学习的发展，word2vec 和 DBN 模型也被用于代码表示和下游任务。此外，CNN 在计算机视觉领域引发了革命，能够有效捕捉序列中的丰富结构模式，因此自然地被用于代码序列任务。然而，在基于序列的模型中，最主流的模型是原始的 RNN 模型及其变体，如 LSTM、GRU 和 Bi-LSTM，它们专门针对序列建模任务。近年来，结合自注意力块的 transformer 模型对基于序列的代码模型做出了重大贡献。

基于图的模型是利用代码生成的图结构，如 AST 和流图，来捕捉结构信息的模型。图结构的转换也在第 4.1 节中介绍。部分这些结构，尤其是 AST，可以被视为树或图，这导致了处理这些结构的不同方式。从树的角度来看，这些模型使用为树结构设计的 RNN 模型，如 Tree-LSTM。从图的角度来看，CNN 也是最常用的模型之一，而图神经网络在学习节点或图的表示方面发挥了重要作用，例如 GGNN、GCN、GAT 或 MPNN，适用于 AST 和流图。在 AST 和流图之上，还有一些组合结构和其他罕见结构，例如 UML 图、程序依赖图等，用于基于图的模型。

这两类模型在代码表示和下游任务上都显示了有效性，我们将在第六部分中介绍。在这一节中，我们强调两种模型之间的三个差异：

首先，基于序列的模型和基于图的模型从不同的角度来看待数据。由于基于序列的模型将代码视为一系列序列，它们必须准确捕捉序列之间对应于源代码中的语义和句法信息的关系。基于图的模型将代码数据视为树或图。由于代码图中的节点和边富含结构信息，这些模型通过学习图中的表示来更好地理解代码。然而，仅使用序列数据的模型忽略了结构中的句法或语义信息，而仅依赖图结构的模型则忽略了序列信息。如前所述，如果按照使用的信息来划分这两种模型的边界并不明确。AST 结构中展开的序列自动结合了 AST 中的结构信息。在基于图的模型中，一些方法也使用了代码中的序列信息。

其次，基于序列的模型和基于图的模型都使用序列模型，但它们的目的和应用场景不同。RNN 和 Transformer 是深度学习模型中用于代码理解的最常用的序列模型，但这两种模型在目的上有所不同。基于序列的模型将 RNN 和 Transformer 作为模型的核心组件来学习代码序列中的关系。RNN（RaychevVeselin 等，2014；Ben-Nun 等，2018a）在基于序列的模型中用于解决代码序列中信息传递的问题。LSTM（Dam 等，2016；Iyer 等，2016）和 GRU（LeClair 等，2019；Hu 等，2020a）是 RNN 的变体，使用门控机制来更好地传递有用信息，并解决代码序列中的梯度爆炸和消失问题。基于 Transformer 的模型（Ahmad 等，2020；Alon 等，2020）更常用于解决全局依赖问题，并使用位置嵌入来保持代码序列的顺序信息，从而更好地学习整个代码信息。

尽管 RNN 基于序列数据设计，但它们在基于图的模型中也被广泛使用，因为序列是包含上下文信息的代码的原始形式。此外，尽管图神经网络可以捕获结构信息，但一旦源代码转换为图结构，上下文信息就无法保留。因此，基于图的模型中的方法还会使用 RNN，如 LSTM 或 GRU，以收集长距离信息。一些工作使用 RNN 处理树结构，如 RvNN(Shi et al., 2021)，或 tree-LSTM(Wan et al., 2019a)。大多数工作将 RNN 与 GNN 结合作为整个模型框架，主要分为以下三种用途：1) 作为模型的编码器使用双向 GRU(Zhang et al., 2019a)或双向 LSTM，2) 使用 GRU 融合 GNN 组件后的结果向量(Liu et al., 2020a)，3) 使用 RNN 如 LSTM 作为解码器(Wang et al., 2020b)。Graph Sandwich Structure(Hellendoorn et al., 2019)是结合 GNN 与 RNN 和 Transformer 结构以融合局部和全局信息的典型工作之一。

第三，注意力机制在基于序列的模型和基于图的模型中都有使用，但有所不同。

注意力机制在神经机器翻译中被广泛使用，特别是在编码器-解码器框架中。注意力机制可以学习哪些词重要，并根据词的重要性进行进一步的预测。在代码表示相关的工作中，注意力机制在基于序列的模型和基于图的模型中都有应用。

在基于序列的模型中，注意力机制主要用于关注代码序列中标记之间的关系，包括输入序列标记之间的关系、输出序列标记与输入序列标记之间的关系，或者输出序列标记之间的关系，以促进更高效的信息传递。基于 Transformer 的模型使用自注意力机制来关注上述三种关系，而其他模型则更多关注输出序列标记与输入序列标记之间的关系。注意力机制被 CodeNN(Iyer et al., 2016)和其他模型添加到基于 RNN 的模型中，使模型在生成代码摘要或预测下一个标记时能更充分地利用关键标记信息。

一些图基模型在输出层之前进行注意力机制处理，并创建一个上下文向量来预测序列中的下一个标记，例如 LeClair 等人 (LeClair et al., 2020)。其他模型则对普通深度学习模型进行修改，添加注意力机制，例如，在 GCN 的消息传播过程中使用注意力机制来分层更新嵌入 (Ji et al., 2021b)，将注意力机制与 GRU 层和卷积层结合，以对路径中的节点顺序进行编码 (Li et al., 2019b)。与基于序列的模型不同，一些工作对原始注意力机制进行了修改，以更好地适应层次结构。Liu 等人 (Liu et al., 2021c) 提出了低级注意力和高级注意力。前者的注意力模块关注序列中的标记，而后者则关注 AST 树中的代码单元。

## 6\. 任务

我们将用于代码理解的任务分类为以下下游任务，并总结了每个下游任务中的基于序列的模型和图基模型。

### 6.1\. 代码生成

代码生成可以根据输入提供不同层次的代码输出。IDE 中的代码预测任务，例如方法名称预测、下一个标记预测等，是典型的代码生成任务。从自然语言生成代码片段也是一种代码生成任务。代码生成显著提升了开发者的效率，并在工业界和学术界得到了广泛研究。相关方法在表 2 中进行了总结，该表主要基于模型在代码生成任务中的使用结构。

#### 6.1.1\. 方法名称生成

这个任务根据给定的方法代码体生成一个概要方法名称，这有助于代码变得更加易于理解、维护和调用。方法名称生成任务的形式化如下：

给定一个包含 n 个标记的代码体 $S={t_{1},t_{2},...t_{n}}$，生成模型可以输出方法名称。

#### 6.1.2\. 下一个标记生成

这个任务预测开发者将输入的包括 API 调用在内的标记。下一个标记生成的模型通常根据开发者之前的输入提供一个按概率排序的标记列表。它可以大幅提高开发速度。下一个标记生成任务的形式化如下：

给定一个包含 $n-1$ 个标记的部分代码片段 $S={t_{1},t_{2},...t_{n-1}}$，生成模型可以生成一个按概率排序的 $t_{n}$ 标记列表。

#### 6.1.3\. 表达式生成

与下一个 token 生成相比，这个任务是基于现有代码片段的更细粒度的代码预测任务。它生成具有特定功能（如条件语句、循环等）的完整代码表达式。它还包括预测缺失代码语句的任务。下一个表达式生成任务的形式化如下：

给定一个包含$n$个 token 的部分代码片段$S={t_{1},t_{2},...,t_{n-1}}$，生成模型可以生成一个具有特定功能的完整代码表达式$S_{m}={t_{n},t_{n+1}...,t_{n+m-1}}$。或者给定一个缺失部分的代码$S={t_{1},t_{2},...,missingpiece,...,t_{n}}$，生成模型生成缺失的代码部分。

#### 6.1.4\. 函数生成

这个任务可以根据用户对代码功能的描述生成相应的代码片段，这可以看作是自然语言到代码的翻译过程。优秀的自然语言到代码翻译技术可以使非专业人员也能相应地进行开发，但这一领域的工作仍然不成熟，需要进一步发展。函数生成任务的形式化如下：

给定自然语言$D$对代码$S$的功能描述，生成模型可以根据$D$生成$S={t_{1},t_{2},...,t_{n}}$。

表 2\. 代码生成

| 类型 | 参考文献 | 模型 | 生成 | 描述 |
| --- | --- | --- | --- | --- |
| 序列 | Code2seq(Alon et al., 2018a) | BiLSTM | method name | 通过 AST 中终端节点之间的一组路径表示代码 |
|  | Code2vec(Alon et al., 2019c) | LSTM | method name | 通过 AST 路径生成代码的表示 |
|  | SLM(Alon et al., 2020) | LSTM | expression | 使用所有路径生成缺失代码 |
|  | Pythia(Svyatkovskiy et al., 2019) | LSTM | next token | 通过展平的 AST 预测方法和 API 调用 |
|  | (Liu et al., 2017) | LSTM | next token | 使用通过遍历 AST 获得的序列来预测下一个可能的节点 |
|  | (Li et al., 2017b) | LSTM, Pointer Network | next token | 通过 LSTM 或指针网络生成代码 |
|  | (Kim et al., 2020) | Transformer | next token | 将不同的路径输入到 Transformer 中 |
|  | (Liu et al., 2020c) | Transformer-XL | next token | 将不同的路径输入到具有多任务学习的 Transformer-XL 中 |
|  | (Alon et al., 2018c) | Word2vec | method name | 将具有上/下动量的不同路径输入到 Word2vec 中 |
|  | API2Vec(Nguyen et al., 2017) | Word2vec | next token | 将不同的 API 路径输入到 Word2vec 中 |
| 图 | DEEP3 (Raychev et al., 2016) | 决策树 | next token | 使用名为 TGEN 的 DSL 表示代码，并利用决策树构建概率模型 |
|  | (Brockschmidt et al., 2019) | GRU,GGNN | 表达式 | 一个在 ExprGen 任务上生成的模型，首先通过属性文法获取图，然后用 GGNN 和 GRU 计算属性表示 |
|  | CCAG(Wang and Li, 2021) | GAT | 下一个标记 | 使用 AST 图注意力块（ASTGab）来建模 AST 的展平序列，以捕获不同的依赖关系 |
|  | (Allamanis et al., 2018b) | GGNN | 方法名称 | 通过添加不同类型的边构建图，并使用 GGNN 学习图的表示 |
|  | Graph-Structured Cache(Cvitkovic et al., 2019) | MPNN,CharCNN | 方法名称 | 引入了一个图结构缓存，将词汇表词汇表示为额外的节点，使用 MPNN 和 CharCNN 生成输出，以进一步解决开放词汇问题 |

### 6.2\. 代码摘要

代码摘要是利用自然语言对代码功能进行简洁说明的工作。它可以帮助提高代码的可读性，以及开发人员理解程序的效率。代码摘要任务可以被看作是将输入代码翻译成自然语言，因此通常使用 seq2seq 模型架构。在编码阶段，（Hu et al., 2018, 2020a; LeClair et al., 2019）将输入的 NSC 或展平序列转换为上下文向量，然后在解码阶段，他们根据上下文向量逐个构造摘要中的词汇。为了提高代码摘要的效果，任务中应用了注意力机制等技术。相关技术在表 3 中根据模型的相关代码结构进行了总结。

表 3\. 代码摘要

| 类型 | 参考文献 | 模型 | 描述 |
| --- | --- | --- | --- |
| 序列 | DeepCom (Hu et al., 2018) | LSTM | 采用 seq2seq 模型，其中 LSTM 作为编码器和解码器生成代码片段摘要 |
|  | Hybrid-Deepcom (Hu et al., 2020a) | LSTM | 利用两个 GRU 作为 NCS 和展平序列的编码器 |
|  | Ast-attendgru (LeClair et al., 2019) | GRU | 使用两个带注意力机制的 GRU 分别处理 NCS 和展平序列，以获取上下文向量 |
|  | (Allamanis et al., 2016) | CNN | 使用卷积注意力网络来总结代码 |
|  | (Iyer et al., 2016) | LSTM | 使用带注意力机制的 LSTM 生成描述代码片段的句子 |
|  | (Zhang et al., 2020b) | 搜索引擎 | 在搜索引擎上检索展平序列，以获取语法上相似的代码片段 |
|  | (Ahmad et al., 2020) | Transformer | 采用 Transformer 进行代码摘要 |
| Graph | (Wan et al., 2018) | RNN，树形 RNN | 使用注意力层融合两种表示，一种来自于 AST 基于 LSTM 的源代码结构，另一种来自于 LSTM 的序列 |
|  | (Fernandes et al., 2019) | GGNN | 使用顺序编码器和 GGNN 生成表示 |
|  | TBCAA (Chen et al., 2019b) | 基于树的 LSTM | 使用基于树的卷积来增强 API 的 AST |
|  | (LeClair et al., 2020) | ConvGNN | 使用递归层和 ConvGNN 编码节点令牌嵌入，然后使用注意力层学习重要的令牌 |
|  | Flow2Vec (Sui et al., 2020) | Flow2Vec | 预嵌入过程间值流图，考虑通过矩阵乘法问题的可达性，并用其近似高阶邻近嵌入 |
|  | BASTS (Lin et al., 2021) | Tree-LSTM | 使用 Tree-LSTM 和 Transformer 架构结合分割 AST 和源代码的表示 |
|  | CoCoSum (Wang et al., 2021a) | Transformer，多关系 GNN | 全球编码器包含一个 MRGNN 来嵌入 UML 类图，以及一个基于 Transformer 的模型来嵌入类名，而本地编码器使用 GRU |
|  | HybridGNN (Liu et al., 2020a) | GNN | 基于消息传递的混合 GNN，融合了静态和动态图 |
|  | (Shi et al., 2021) | RNN，注意力 | 用于源代码极端总结的卷积注意力网络 |
|  | (Liu et al., 2021c) | HAConvGNN | 层次分割 AST 为子树，学习分割 AST 的表示，并重建它们以结合结构和语义信息与 RvNN |

### 6.3. 代码搜索

代码搜索是一个信息检索任务。代码搜索的输入是自然语言描述或代码片段的查询。输出是与输入最匹配的代码片段。代码搜索的目标是从大型代码库中检索最接近开发者意图的代码片段 (Cambronero et al., 2019b)。能够探索和重用与开发者意图匹配的现有代码是一个基本的生产力工具。一些在线网站如 Stack Overflow 由于提供了根据自然语言表达的用户问题搜索相关代码的便利而受到欢迎。

一些文章将此任务称为语义代码搜索或代码推荐，这些名称强调了代码搜索的特征。它基于输入的语义，输入与代码片段之间的语义对齐至关重要。该任务的信息检索特性在于所有输出均从代码库中检索出来，保持原样。检索特性区分了代码搜索和代码生成，其中生成任务的最终目标是合成和编写不仅仅是已经存在于代码库中的代码。

代码搜索模型总结如表 4 所示。

表 4\. 代码搜索

| 类型 | 参考文献 | 模型 | 描述 |
| --- | --- | --- | --- |
| 序列 | （Sachdev 等，2018） | word2vec | 结合自然语言技术和信息检索方法 |
|  | CODEnn（Gu 等，2018） | RNN | 将代码片段和自然语言描述共同嵌入到高维向量空间中 |
|  | UNIF（Cambronero 等，2019b） | word2vec | 使用注意力结合每个标记的嵌入并生成代码句子嵌入 |
|  | CARLCS-CNN（Shuai 等，2020） | CNN | 使用 CNN 分别嵌入代码和查询 |
| 图 | TBCAA（Chen 等，2019b） | 基于树的 LSTM | 基于树的卷积用于增强 API 的 AST |
|  | MMAN（Wan 等，2019a） | GGNN, Tree-LSTM | 一种多模态注意力网络，使用注意力机制从 LSTM 捕获信息以嵌入序列标记，使用 Tree-LSTM 嵌入 AST，使用 GGNN 表示 CFG |

### 6.4\. 克隆检测

克隆检测任务表示在同一软件或系统中存在两个或多个相似的代码片段，这也称为代码克隆。代码克隆可以支持开发人员对其进行修改以便更好地重用它们。代码克隆可以描述为以下 4 种类型（Bellon 等，2007）：

Type-1 克隆是相同的代码片段，可能在空格、布局或注释中存在轻微差异。

Type-2 克隆是相同的代码片段，可能包含变量名、常量、函数名、标识符、字面量、类型、布局、空格或注释的差异。

Type-3 克隆是语法上相似的代码片段，具有添加、删除或修改的语句。

Type-4 克隆是语义上相似的代码片段，可能使用不同的词汇和语法来表达等效的语义。

随着四种类型的相似性减少，检测克隆的难度增加。解决克隆检测的方法如表 5 所示。

表 5\. 克隆检测

| 类型 | 参考文献 | 模型 | 描述 |
| --- | --- | --- | --- |
| 序列 | CCLearner（Li 等，2017a） | DNN | 从已知的函数级代码克隆和非克隆中提取标记以训练分类器 |
|  | （White 等，2016） | RNN | 使用 RNN 建模源代码语料库中的术语序列 |
| 图 | DeepSim（Zhao 和 Huang，2018） | 前馈神经网络 | 一种测量代码功能相似性的方法，使用两个流图作为基础，并用前馈神经网络对它们进行编码 |
|  | ASTNN（Zhang 等，2019a） | 双向 RNN | 对语句子树进行编码，并使用 Bi-GRU 模型化语句的自然性 |
|  | TBCAA（Chen 等，2019b） | 基于树的 LSTM | 基于树的卷积，用于 API 增强的 AST |
|  | DEEPBINDIFF（Duan 等，2020） | 文本关联的 DeepWalk | 使用文本关联的 DeepWalk 算法学习基本块嵌入，并通过 k-hop 贪婪匹配算法进行匹配 |
|  | （Yu 等，2020） | MPNN, CNN | 使用 BERT 预训练标记和块嵌入在 MLM 任务上，并在 2 个图级任务上用 MPNN、GRU 和 CNN 进行微调 |
|  | CSEM（Li 等，2020） | Transformer, GAT, CNN | 将源代码转换为中间表示，生成节点向量矩阵，并将其输入到 GAT 和 CNN 层，以获得代码片段的嵌入 |
|  | OSCAR（Peng 等，2021） | Transformer | 一种层次化的多层 Transformer 预训练模型，具有新颖的位置编码、对比学习与优化技术 |
|  | HAG（Ji 等，2021b） | GCN | 使用带有层级传播和注意力机制的 GCN |

### 6.5\. 安全分析

随着计算机技术的进步，人们越来越依赖程序和代码来处理生活中的各种问题，代码中的缺陷和漏洞可能导致重大损失。因此，检查代码的可靠性和安全性至关重要。代码中的缺陷可能导致程序无法正常运行，而漏洞可能对计算机系统的安全运行构成潜在威胁。此外，恶意软件是旨在攻击设备的恶意软件。因此，在安全分析中，我们将安全相关任务分为三个类别：缺陷预测、漏洞预测和恶意软件分类，并在表格 6 中进一步总结了这三类模型。

#### 6.5.1\. 缺陷预测

缺陷预测有助于开发人员更有效地测试，同时通过预测问题代码区域来降低软件开发成本。目前可用的两种缺陷预测任务是项目内缺陷预测（WPDP），其中训练集和测试集来自同一项目，以及跨项目缺陷预测（CPDP），其中测试集与训练集不同。缺陷预测任务的形式化如下：

给定一个包含 $n$ 个标记的代码片段 $S=\{t_{1},t_{2},...t_{n}\}$，预测模型可以输出一个标签 $y$，表示代码片段是否存在缺陷（Buggy）或没有缺陷（clean）。

#### 6.5.2\. 漏洞检测

漏洞检测任务，包括基于代码相似性和代码模式的漏洞检测，可以防止代码受到攻击，提高代码的安全性。一些方法使用深度学习进行漏洞检测，例如 VulDeePecker (Li et al., 2018)。漏洞检测任务的形式化如下：

给定一个包含$n$个标记的代码片段$S=\{t_{1},t_{2},...t_{n}\}$，预测模型可以输出一个标签$y$，表示代码片段是否存在漏洞。

#### 6.5.3\. 恶意软件分类

恶意软件分类是一种恶意软件检测问题，属于二分类问题。恶意软件分类可以被形式化为：给定程序 P，将 P 分类为正常程序或恶意软件。

表 6\. 安全分析

| 类型 | 参考 | 模型 | 类型 | 描述 |
| --- | --- | --- | --- | --- |
| 序列 | (Wang et al., 2016) | DBN | 缺陷 | 使用 DBN 自动学习代码的语义表达 |
|  | Seml(Liang et al., [[n.d.]](#bib.bib103)) | LSTM | 缺陷 | 借助 LSTM 预测软件缺陷 |
|  | DeepCPDP(Chen et al., 2019a) | Bi-LSTM | 缺陷 | 提出了 SimAST 和 SimAST2Vec |
|  | (Wang et al., 2020a) | DBN | 缺陷 | 利用 DBN 学习代码 AST 标记的更高层次语义特征 |
|  | (Le et al., 2018) | CNN-BiLSTM | 恶意软件 | 提出了一种使恶意软件分类能够由非专家进行恶意软件分析的方法 |
|  | (Gibert et al., 2019) | CNN | 恶意软件 | 提出了一种文件无关的深度学习方法用于恶意软件分类，以基于从图像中提取的一组判别模式高效地将恶意软件分组 |
| 图形 | (Allamanis et al., 2018b) | GGNN | 缺陷 | 通过添加不同类型的边构建图形，并使用 GGNN 学习图形的表示 |
|  | MAGIC (Yan et al., 2019) | DGCNN | 缺陷 | 扩展了标准 DGCNN 在加权顶点层和自适应最大池化上的功能，以从图结构中聚合属性 |
|  | (Li et al., 2019b) | GRU, CNN, 注意力机制 | 缺陷 | 提取全局（来自 PDG 和 DFG）和局部（来自 AST）上下文，使用基于注意力的 GRU 和 CNN。 |
|  | Devign (Zhou et al., 2019) | GGNN, GRU, CNN | 漏洞 | 将代码编码为联合图结构，并使用 GGNN 与卷积模块学习嵌入。 |
|  | BugGraph (Ji et al., 2021a) | GTN | 漏洞 | 在属性 CFG 上使用图三元组损失网络学习相似性排序。 |
|  | (Brauckmann et al., 2020) | GGNN | 恶意软件 | 使用 GGNN 在 AST 和 CDFGs 上学习预测编译器任务 |

### 6.6\. 错误定位

错误定位是一个将错误在代码片段中定位的任务。错误定位也可以看作是程序修复的前一步。根据发现方式，错误分为静态和动态两类。静态错误位置由控制和数据依赖关系确定，而动态错误位置则由程序执行确定。我们在表 7 中提供了错误定位任务的相关工作。错误定位任务的形式化定义如下：

给定一个包含 $n$ 个标记的代码片段 $S=\{t_{1},t_{2},...t_{n}\}$，预测模型将预测错误标记 $t_{\lambda}$ 的位置 $\lambda$，例如误用的变量或操作符。需要预测正确标记的位置也称为槽位。

表 7\. 错误定位

| 类型 | 参考文献 | 模型 | 描述 |
| --- | --- | --- | --- |
| Sequence | (Vasic et al., 2019) | LSTM | 提出了多头指针网络，用于训练一个模型，该模型直接且共同定位和修复变量误用错误 |
|  | BULNER (Barbosa et al., 2019) | Word2vec | 提出了一个基于词嵌入和网络正则化的错误定位方法 |
| Graph | GGF (Wu et al., 2020) | GGNN | 使用 GRU 和 GGNN 作为编码器，使用标记替换机制作为解码器，以编码标记信息并生成修复动作 |
|  | (Dinella et al., 2020) | GAT, LSTM | 引入了程序反馈图，并应用 GNN 来建模推理过程。 |
|  | GINN (Wang et al., 2020b) | GINN | 提出了图间隔神经网络（Graph Interval Neural Network），包括升高和降低操作符，用于学习 CFG 的表示 |

### 6.7\. 程序修复

程序修复，也称为错误修复、代码精炼，旨在修复定位的错误。一些工作将错误定位和程序修复任务联合执行，例如 BugLab (Allamanis et al., 2021)。在检测到错误后，会在程序的典型行上进行修复。一些工作将错误检测和程序修复结合起来，同时预测位置和修复动作，使用序列将两者结合起来 (Vasic et al., 2019)。其他修复任务则仅修复假设错误位置已经存在的错误，如 CODIT (Chakraborty et al., 2020)。

自动化程序修复中最流行的任务之一是由 Allamanis(Allamanis et al., 2018b)提出的 VARMISUSE 任务。VARMISUSE 任务是自动检测源代码中的变量误用错误，并用正确的变量修复它。换句话说，程序修复任务可以视为在检测到错误的槽位中生成代码。然而，由于输入的差异以及生成代码的提议，我们将程序修复视为一种新任务，并在表 8 中总结了模型。

由于数据集、语言和输出粒度的多样性，不同的工作对程序修复有不同的定义。例如，输出可能是修复误用变量的单个词，也可能是修复新代码片段行的完整句子。令牌修复的形式化类似于 6.6 中的形式化。在检测到位置后，程序修复任务将生成正确的令牌 $t_{\lambda}^{\prime}$。另一种程序修复任务的形式化如下：

给定一个破损的代码片段 $S=\{l_{1},l_{2},...l_{k}\}$（具有 $k$ 行）以及编译器的诊断反馈（反馈通常包含行号和错误信息）$I$，程序修复任务是定位错误的行索引 $n$ 并生成修复的代码版本 $l_{n}^{{}^{\prime}}$ 替换错误的 $l_{n}$。

表 8\. 程序修复

| 类型 | 参考文献 | 模型 | 描述 |
| --- | --- | --- | --- |
| Sequence | DeepFix(Gupta et al., 2017) | GRU | 通过反复调用训练过的神经网络来修复多个错误 |
|  | TFix(Berabi et al., 2021) | Transformer | 使用 T5(Raffel et al., 2019)准确合成对各种错误的修复 |
| Graph | CODIT (Chakraborty et al., 2020) | LSTM | 一种编码器-解码器模型，首先通过树对树模型学习 AST 修改中的结构变化，然后根据 AST 预测令牌 |
|  | GGF (Wu et al., 2020) | GGNN | 使用 GRU 和 GGNN 作为编码器，使用令牌替换机制作为解码器来编码令牌信息并生成修复动作 |
|  | Hoppity (Dinella et al., 2020) | GAT, LSTM | 引入程序反馈图，并应用 GNN 来建模推理过程 |
|  | GINN (Wang et al., 2020b) | GINN | 提出了图区间神经网络，包括升高和降低算子以学习 CFG 的表示 |

### 6.8\. 预训练任务

预训练是对大量预训练数据进行模型训练的过程，以从数据中提取尽可能多的特征，使得模型在特定数据集的微调后能更好地解决目标任务。随着开源语料库的普及，已经出现了大量代码数据的预训练模型，如 CodeBERT （Feng et al., 2020）、CuBert（Kanade et al., 2020）、GPT-C（Svyatkovskiy et al., 2020）、CodeT5（Wang et al., 2021d）和 GraphCodeBERT（Guo et al., 2020），这些模型能够捕捉代码中的语义信息，并快速有效地应用于各种下游任务。预训练模型正成为代码相关任务的主流模型。微软和 Facebook 等大型公司提出了一系列预训练模型。一些模型如 Alphacode²²2https://alphacode.deepmind.com/ 和 Codex³³3https://openai.com/blog/openai-codex/ 已经在实际应用中得到应用。

## 7\. 评价指标和数据集

我们在本节中描述了与代码相关工作相关的指标和数据集，以供未来工作参考。

### 7.1\. 指标

代码表示领域的相关评价指标分为两类：与自然语言处理（NLP）相关的指标和与信息检索相关的指标。

#### 7.1.1\. 与 NLP 相关的指标

由于代码与自然语言之间的相似性，许多在代码领域使用的指标源自自然语言领域，特别是代码摘要等生成任务。

+   •

    准确率、精确率、召回率和 F1 分数 其中一个最直观的性能指标是准确率，它定义为正确预测与总预测的比例。精确率指的是所有被预测为正样本的预测中真正样本的百分比，而召回率指的是所有真实样本中被正确预测为正样本的百分比。精确率和召回率的加权平均值即为 F1 分数。因此，这个分数考虑了假阳性和假阴性。特别是当类别分布不均时，这个指标更加有用。这三种指标通常用于分类或预测任务。

+   •

    BLEU 双语评估基准（BLEU）（Papineni 等，2002）旨在用于自动评估统计机器翻译，并可用于衡量代码总结和生成任务的性能。评分计算如方程 1 和 2 所示，其中前者是具有候选翻译长度$c$和有效参考序列长度$r$的简洁惩罚（BP）。TBP $p_{n}$ 是候选中长度为$n$的子序列在参考中也存在的比例。方程 2 中的$N$通常设为 4（BLEU-4）（Hu 等，2018）。

    | (1) |  | $\mathrm{BP}=\left\{\begin{array}[]{ll}1\text{ if }c>r\\ e^{(1-r/c)}\text{ if }c\leq r\end{array}\right.$ |  |
    | --- | --- | --- | --- |
    | (2) |  | $\mathrm{BLEU}=\mathrm{BP}\cdot\exp\left(\sum_{n=1}^{N}w_{n}\log p_{n}\right)$ |  |

+   •

    困惑度（PPL）困惑度是一个极好的概率测量，用于精确评估 NLP 模型的混乱程度。它通常用于评估语言模型以及对话生成任务。PPL 定义如方程 3，其中$x_{i}$是实际标签，$P(x_{i})$是模型输出。困惑度较低的模型对真实标记赋予更高的概率，预期表现更好。

    | (3) |  | $PPL=\exp\left(-\sum_{i}^{T}P\left(x_{i}\right)\log P\left(x_{i}\right)\right),\forall i\in 0\ldots T.$ |  |
    | --- | --- | --- | --- |

+   •

    ROUGE 与 BLEU 得分不同，召回导向的摘要评价指标（ROUGE）（Lin, 2004）用于衡量召回率。它通常用于机器翻译任务中评估生成文本的质量。然而，由于它评估的是召回率，因此主要用于摘要任务，在这些任务中，评估模型能召回多少词更为重要。（Lin, 2004）提出了四种 ROUGE 方法：1) ROUGE-N：计算 n-gram 的召回率，2) ROUGE-L：考虑生成序列 $C$ 与目标序列 $S$ 之间的最长公共子序列，3) ROUGE-W：改进 ROUGE-L 并通过加权方法计算最长公共子序列，4) 计算允许跳词的 n-gram 的召回率。ROUGE-L 的计算方法如方程式 4 所示，其中 $F_{LCS}$ 为 ROUGE-L，$\beta$ 为常数。

    | (4) |  | $\displaystyle Recall_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(S)}$ |  |
    | --- | --- | --- | --- | --- |
    |  | $\displaystyle Precision_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(C)}$ |  |
    |  | $\displaystyle F_{LCS}$ | $\displaystyle=\frac{\left(1+\beta^{2}\right)Recall_{LCS}Precision_{LCS}}{Recall_{LCS}+\beta^{2}Precision_{LCS}}$ |  |

+   •

    METEOR 用于显式排序翻译评价的度量（METEOR）（Banerjee 和 Lavie, 2005）是一种基于精确度的机器翻译输出评价指标。它克服了 BLEU 得分的一些缺陷，例如在计算精确度时需要精确的词匹配。METEOR 得分允许将同义词和词干词与参考词进行匹配。METEOR 的关键在于使用 WordNet 词典将生成序列与目标序列对齐。

+   •

    字词错误率（WER）在生成任务中，重要的是在生成序列中替换、删除或插入一些词，以保持生成序列与目标序列的一致性。WER，如方程式 5 所定义的，可以用来评估生成序列的质量。

    | (5) |  | $\mathrm{WER}=100\cdot\frac{Substitution+Deletion+Insertion}{N}\%$ |  |
    | --- | --- | --- | --- |

+   •

    CodeBLEU CodeBLEU（Ren et al., 2020）是基于 BLEU 的代码评估指标，它可以关注关键字，利用树结构并考虑语义逻辑。它被定义为$\mathrm{BLEU}$、$\mathrm{BLEU}_{\text{weight }}$、$\mathrm{Match}_{\text{ast}}$和$\mathrm{Match}_{\text{df}}$的加权组合，如方程 6 所示。第一个项指的是标准的$\mathrm{BLEU}$。$\mathrm{BLEU}_{\text{weight }}$是加权的 n-gram 匹配，通过比较假设代码和参考代码的令牌与不同的权重来获得。$\mathrm{Match}_{\text{ast}}$是语法 AST 匹配，探索代码的语法信息。$\mathrm{Match}_{\text{df}}$是语义数据流匹配，考虑预测与参考之间的语义相似性。加权 n-gram 匹配和语法 AST 匹配用于测量语法正确性，而语义数据流匹配用于计算逻辑正确性。

    | (6) |  | CodeBLEU | $\displaystyle=\alpha\cdot\mathrm{BLEU}+\beta\cdot\mathrm{BLEU}_{\text{weightt }}$ |  |
    | --- | --- | --- | --- | --- |
    |  |  | $\displaystyle+\gamma\cdot\mathrm{Match}_{\text{ast}}+\delta\cdot\mathrm{Match}_{\text{df}}$ |  |

#### 7.1.2\. 信息检索相关指标

由于一些与代码相关的任务，例如代码搜索，与信息检索相似，因此许多与代码相关的指标与信息检索领域相关联。

+   •

    成功率（SR） 如果与输入匹配的信息位于搜索信息排序列表的前 k 名，则搜索成功。SR@k 计算所有搜索中成功搜索的比例。计算方法见方程 7，其中$\delta$是常数，$Frank_{k}$是匹配信息在搜索信息排序列表中的排名，$Frank_{q}\leq K$表示检索成功。

    | (7) |  | $\mathrm{SR@K}=\frac{1}{ | Q | }\sum_{q=1}^{Q}\delta\left(\operatorname{FRank}_{q}\leq K\right)$ |  |
    | --- | --- | --- | --- | --- | --- |

+   •

    互惠排序（MRR） 在信息检索排序列表中，MRR 考虑检索到的匹配信息的排名。如果列表中的第 n 条信息符合输入，则得分为$\frac{1}{N}$，如果没有匹配的句子，则得分为$0$。所有得分的总和为最终得分。方程 8 展示了如何计算 MRR。

    | (8) |  | $\mathrm{MRR}=\frac{1}{ | Q | }\sum_{q=1}^{ | Q | }\frac{1}{\operatorname{FRank}_{q}}$ |  |
    | --- | --- | --- | --- | --- | --- | --- | --- |

+   •

    最佳命中排名（Best Hit Rank）是查询结果中命中片段的最高排名。最高的最佳命中意味着用户检查所需命中片段的努力更少。

### 7.2\. 数据集

我们在表 9 中总结了各种数据集。一些工作收集和生成自己的数据集进行研究，这可能会导致在相同任务上比较不同工作的困难。表 9 不包含任何非开源的数据集。

表 9\. 数据集

| 名称 | 研究 | 描述 |
| --- | --- | --- |
| [Genius 数据集](https://github.com/xiaojunxu/dnn-binary-code-similarity) | (Feng et al., 2016b; Xu et al., 2017) | 一个包含 33,045 个设备的现实世界数据集，来自公共来源和我们的系统 |
| [notebookcdg](https://github.com/dakuo/haconvgnn) | (Liu et al., 2021c) | 拥有 28,625 对代码–文档 |
| [py150](http://www.srl.inf.ethz.ch/py150) | (Raychev et al., 2016; Kanade et al., 2019; Kim et al., 2020) | 包含从 GitHub Python 仓库中收集的解析 AST，去除重复文件 |
| [js150](http://www.srl.inf.ethz.ch/js150) | (Raychev et al., 2016; Wang and Li, 2021) | 包含 15 万 JavaScript 文件及其对应的解析 AST |
| [HGNN](https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization) | (Liu et al., 2020a) | 从多样化的大规模开源 C 项目中爬取（数据集中总共 95k+独特函数） |
| [CodeSearchNet](https://github.com/github/CodeSearchNet) | (Husain et al., 2019; Feng et al., 2020; Ugner et al., [[n.d.]](#bib.bib152)) | 包含来自开源库的 200 万（注释，代码）对 |
| [CONCODE](https://github.com/sriniiyer/concode) | (Iyer et al., 2018; Allamanis, 2019) | 包含超过 100,000 个示例，包括来自在线代码库的 Java 类 |
| [CodeXCLUE](https://github.com/microsoft/CodeXGLUE) | (Lu et al., 2021; Puri et al., 2021; Wang et al., 2021d) | 包含跨越 14 个数据集的 10 个任务的集合 |
| [TFix 的代码补丁数据](https://github.com/eth-sri/TFix) | (Berabi et al., 2021) | 包含从 GitHub 开源项目中提取的 10 万对代码补丁 |
| [CoDesc](https://github.com/csebuetnlp/CoDesc) | (Hasan et al., 2021) | 包含 420 万 Java 源代码及其描述的并行数据，来源于代码搜索和代码总结研究 |
| [DeepFix](https://bitbucket.org/iiscseal/deepfix) | (Gupta et al., 2017; Yasunaga and Liang, 2020a; Chen et al., 2021) | 包含一个程序修复数据集（修复 C 程序中的编译错误） |
| [SPoC](https://github.com/michiyasunaga/DrRepair) | (Yasunaga and Liang, 2020a) | 一个程序合成数据集，包含 18,356 个程序及其人类编写的伪代码和测试用例 |
| [Defects4J](https://git.io/JJGwU) | (Chakraborty et al., 2020) | 一个包含 32k 个真实代码变更的大型数据集 |
| [FunCom](http://leclair.tech/data/funcom/) | (LeClair and McMillan, 2019; Shrivastava, 2021; Wei et al., 2019; Mahmud et al., 2021) | 收集了 2.1 百万 Java 方法及其相关的 Javadoc 注释 |
| [CoSQA](https://github.com/Jun-jie-Huang/CoCLR) | (Huang et al., 2021; Li et al., 2022) | 包含 20,604 个自然语言查询与代码对的标签，每个标签至少由 3 名人工标注者标注 |
| [CoNaLa](https://conala-corpus.github.io/) | (Yin et al., 2018b; Yin and Neubig, 2018) | 包含 2379 个训练和 500 个测试示例，这些示例经过人工标注 |
| [Django](https://github.com/odashi/ase15-django-dataset) | (Oda et al., 2015; Lin et al., 2018) | 包含 16000 个训练、1000 个开发和 1805 个测试注释 |
| [BLANCA](https://github.com/wala/blanca) | (Abdelaziz et al., 2021) | 一系列基于任务评估代码理解的基准测试集 |
| [IndoNLG](https://github.com/indobenchmark/indonlg) | (Cahyawijaya et al., 2021) | 一个包含 6 种下游任务的印尼语自然语言生成 (NLG) 资源集 |
| [Neural-Code-Search-Evaluation-Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset) | (Li et al., 2019a) | 一个评估数据集，由自然语言查询和代码片段对组成，用于代码搜索 |

## 8. 未解问题

现有技术难以全面捕捉代码的结构和语义信息。 大多数深度学习模型针对特定任务和单一语言设计，缺乏灵活性。 以下未解问题可以作为未来的研究方向。

##### 信息捕捉

许多方法利用代码中的结构信息，但大多数方法仅使用结构信息（如 AST）来捕捉语法信息。 只有少数方法将结构和语义信息（如 DFG）结合来学习代码的整体表示，并且它们仅适用于特定任务，而不是所有任务。 因此，未来研究的目标之一可能是提高模型在各种任务中使用代码结构和语义信息的能力。

##### 灵活性

我们上述描述的方法适用于特定任务或数据集，但缺乏灵活性。模型的灵活性意味着它可以在多种场景中使用，包括那些使用较短程序的数据集（具有小的图结构）或样本较小的场景，以及涉及多个下游任务或编程语言的场景。因此，未来的研究可以集中于如何训练能够适应各种情况的模型。

##### 模型简洁性

最近的模型，特别是基于 Transformer 及其衍生模型的模型，性能有所提升，但通常需要更多的努力和机器能力。因此，在确保准确性的前提下，提出轻量级模型是必要的。

##### 可解释性

深度学习方法一直存在可解释性问题，而代码的情况也不例外。虽然当前在代码生成任务中使用的注意机制可以解释标记生成的来源，但对于其他任务，如安全分析，仍然没有很好的解释。同时，模型的可解释性在确定结构信息和生成更优指标方面非常有用。因此，对代码模型可解释性的进一步研究仍然值得进行。

##### 鲁棒性

代码领域模型的鲁棒性尚未被研究，但随着模型性能的提高，代码领域模型的鲁棒性肯定会成为未来的热门研究领域。在代码领域中，无论是基于序列的模型还是基于图的模型，都在一定程度上依赖于代码标记的表示，这导致当测试代码片段的表示方式与训练代码片段不同时（例如，Python 语言中 API 调用的不同表示），模型性能下降。此外，虽然代码图模型可以更好地捕捉代码片段的信息，但图结构对攻击很敏感。已经有很多技术探讨图模型的鲁棒性，以及如何将其转换以适用于代码图是一个重要的研究领域。

##### 指标

在前一部分中，用于评估代码表示效果的指标基于自然语言处理领域和信息检索领域。尽管有些指标是专门为代码表示设计的，但针对代码数据和下游任务提出的指标数量较少。以下是可能的研究方向：

1) 信息度量 随着用于学习代码表示的结构多样性增加，对适当的信息度量的需求也在增长。最近的度量方法在下游任务之后测量模型如何利用这些结构中的信息，但在代码表示阶段直接提供的度量方法尚未出现。

2) 可解释性的度量 如前所述，为下游任务生成更好的模型效能度量对于模型的可解释性至关重要，这可以更好地定量模型的工作方式。

3) 偏见问题的度量 代码数据不可避免地包含重复和冗余，这可能导致偏见问题。就我们所知，关于代码表示中的偏见问题的研究和讨论还很少。因此，考虑代码数据中的偏见是一个新的未来方向，同时需要合理的度量标准来衡量代码模型的偏见。

## 9\. 结论

在从事智能软件开发时，理解代码的结构和语义含义至关重要。在这项调查中，我们对近年来结构化方法进行代码表示学习进行了全面概述，将其分为两组：基于序列的模型和基于图的模型，然后总结和比较每组中的方法。下游任务以及度量标准和数据集也在这里介绍。研究表明，深度学习模型在代码理解和进一步的多个下游任务中是有用的。然而，代码理解领域仍处于起步阶段，面临着众多障碍和未解决的问题。最后，作为未来代码理解的方向，我们提出了四个开放性问题。

###### 致谢。

## 参考文献

+   (1)

+   Abdelaziz et al. (2021) Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, 和 Kavitha Srinivas. 2021. 机器能否阅读编码手册？ – 构建更好的代码理解语言模型的基准。*arXiv: 计算与语言* (2021)。

+   Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, 和 Kai-Wei Chang. 2020. 基于变换器的源代码总结方法。*arXiv 预印本 arXiv:2005.00653* (2020)。

+   Allamanis (2019) Miltiadis Allamanis. 2019. 代码重复对机器学习模型的负面影响。在*2019 年 ACM SIGPLAN 国际新思想、新范式和编程与软件反思研讨会论文集*中，143–153。

+   Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, 和 Charles Sutton. 2018a. 大代码和自然性机器学习的调查。*ACM 计算调查（CSUR）* 51, 4 (2018), 1–37。

+   Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, 和 Mahmoud Khademi. 2018b. 学习用图表示程序。在*第六届国际学习表征会议，ICLR 2018 - 会议论文集*中。arXiv:1711.00740 [`github.com/Microsoft/gated-graph-neural-network-samples`](https://github.com/Microsoft/gated-graph-neural-network-samples)

+   Allamanis et al. (2021) Miltiadis Allamanis, Henry Jackson-Flux, 和 Marc Brockschmidt. 2021. 自监督错误检测与修复。*神经信息处理系统进展* 34 (2021)。

+   Allamanis 等 (2016) Miltiadis Allamanis, Hao Peng 和 Charles Sutton。2016. 用于极端总结源代码的卷积注意力网络。*arXiv: 学习* (2016)。

+   Alon 等 (2018a) Uri Alon, Shaked Brody, Omer Levy 和 Eran Yahav。2018a. code2seq: 从结构化表示生成序列。*arXiv: 学习* (2018)。

+   Alon 等 (2019a) Uri Alon, Omer Levy, Shaked Brody 和 Eran Yahav。2019a. Code2Seq: 从结构化表示生成代码序列。*第七届国际学习表示会议，ICLR 2019* 1 (2019), 1–22. arXiv:1808.01400

+   Alon 等 (2020) Uri Alon, Roy Sadaka, Omer Levy 和 Eran Yahav。2020. 代码的结构化语言模型。发表于 *国际机器学习会议*。PMLR, 245–256.

+   Alon 等 (2018b) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2018b. 一种通用的基于路径的表示法，用于预测程序属性。*ACM SIGPLAN 通告* 53, 4 (2018 年 3 月), 404–419. [`doi.org/10.1145/3192366.3192412`](https://doi.org/10.1145/3192366.3192412) arXiv:1803.09544

+   Alon 等 (2018c) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2018c. 一种通用的基于路径的表示法，用于预测程序属性。发表于 *编程语言设计与实现*。

+   Alon 等 (2019b) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2019b. Code2Vec: 学习代码的分布式表示。*ACM 编程语言学会会议录* 3, POPL (2019), 1–29. [`doi.org/10.1145/3290353`](https://doi.org/10.1145/3290353) arXiv:1803.09473

+   Alon 等 (2019c) Uri Alon, Meital Zilberstein, Omer Levy 和 Eran Yahav。2019c. code2vec: 学习代码的分布式表示。

+   Alreshedy 等 (2018) Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German, Venkatesh Srinivasan 和 T. Aaron Gulliver。2018. SCC: 自动分类代码片段。*arXiv: 软件工程* (2018)。

+   Amodio 等 (2017) Matthew Amodio, Swarat Chaudhuri 和 Thomas Reps。2017. 用于程序生成的神经属性机器。*arXiv: 人工智能* (2017)。

+   Arbaaz Qureshi 等 ([n.d.]) Syed Arbaaz Qureshi, Sonu Mehta, Ranjita Bhagwan 和 Rahul Kumar。[n.d.]. 评估句法结构在学习代码编辑表示中的有效性。 ([n. d.]). arXiv:2106.06110v1

+   Arkesteijn 和 Saldanha ([n.d.]) Youri Arkesteijn 和 Nikhil Saldanha。[n.d.]. 使用神经注意力和字节对编码的代码完成。 ([n. d.]). arXiv:2004.06343v1 www.sri.inf.ethz.ch/py150

+   Bahdanau 等 (2014) Dzmitry Bahdanau, Kyunghyun Cho 和 Yoshua Bengio。2014. 通过共同学习对齐和翻译的神经机器翻译。*arXiv 预印本 arXiv:1409.0473* (2014)。

+   Banerjee 和 Lavie (2005) Satanjeev Banerjee 和 Alon Lavie。2005. METEOR: 一种改进与人工判断相关性的自动机器翻译评估指标。发表于 *计算语言学协会会议*。

+   Barbosa 等（2019）Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini, Ricardo Britto, Frederico Soares, Solange Oliveira Rezende, Auri Marcelo Rizzo Vincenzi 和 Márcio Eduardo Delamaro. 2019. BULNER: 使用词嵌入和网络正则化进行 BUG 定位。*arXiv: 软件工程*（2019）。

+   Bellon 等（2007）Stefan Bellon, Rainer Koschke, Giulio Antoniol, Jens Krinke 和 Ettore Merlo. 2007. 克隆检测工具的比较与评估。*IEEE 软件工程学报* 33, 9（2007），577–591。 [`doi.org/10.1109/TSE.2007.70725`](https://doi.org/10.1109/TSE.2007.70725)

+   Ben-Nun 等（2018a）Tal Ben-Nun, Alice Shoshana Jakobovits 和 Torsten Hoefler. 2018a. 神经代码理解：可学习的代码语义表示。*arXiv: 学习*（2018）。

+   Ben-Nun 等（2018b）Tal Ben-Nun, Alice Shoshana Jakobovits 和 Torsten Hoefler. 2018b. 神经代码理解：可学习的代码语义表示。*神经信息处理系统进展* 2018 年 12 月（2018 年 6 月），3585–3597。arXiv:1806.07336 [`arxiv.org/abs/1806.07336v3`](https://arxiv.org/abs/1806.07336v3)

+   Bengio（2009）Yoshua Bengio. 2009. 深度架构学习与人工智能。

+   Berabi 等（2021）Berkay Berabi, Jingxuan He, Veselin Raychev 和 Martin Vechev. 2021. TFix: 使用文本到文本的变换器学习修复编码错误。在 *国际机器学习会议*。

+   Bhoopchand 等（2016）Avishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr 和 Sebastian Riedel. 2016. 使用稀疏指针网络学习 Python 代码建议。*arXiv: 神经与进化计算*（2016）。

+   Brauckmann 等（2020）Alexander Brauckmann, Andrés Goens, Sebastian Ertel 和 Jeronimo Castrillon. 2020. 基于编译器的图表示法用于代码的深度学习模型。在 *第 29 届国际编译器构造会议论文集*。201–211。

+   Brockschmidt 等（2019）Marc Brockschmidt, Miltiadis Allamanis, Alexander Gaunt 和 Oleksandr Polozov. 2019. 通过图进行生成式代码建模。在 *第七届国际学习表征会议，ICLR 2019*。国际学习表征会议，ICLR。arXiv:1805.08490 [`arxiv.org/abs/1805.08490v2`](https://arxiv.org/abs/1805.08490v2)

+   Cahyawijaya 等（2021）Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti 和 Pascale Fung. 2021. IndoNLG: 评估印尼自然语言生成的基准与资源。*arXiv: 计算与语言*（2021）。

+   Cambronero 等（2019a）José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen 和 Satish Chandra. 2019a. 当深度学习遇上代码搜索。在 *FSE*。

+   Cambronero 等（2019b）José Pablo Cambronero、Hongyu Li、Seohyun Kim、Koushik Sen 和 Satish Chandra。2019b。当深度学习遇上代码搜索。在 *软件工程基础*。

+   Chakraborty 等（2020）Saikat Chakraborty、Yangruibo Ding、Miltiadis Allamanis 和 Baishakhi Ray。2020。CODIT：基于树的神经模型代码编辑。*IEEE 软件工程学报* TBD（2020 年 9 月），1–1。 [`doi.org/10.1109/tse.2020.3020502`](https://doi.org/10.1109/tse.2020.3020502) arXiv:1810.00314

+   陈等（2019a）陈德宇、陈翔、李浩、谢俊锋 和 穆燕洲。2019a。DeepCPDP：基于深度学习的跨项目缺陷预测。*IEEE Access* 7（2019），184832–184848。

+   陈等（2019b）陈龙、叶伟和张世坤。2019b。通过基于树的卷积捕捉源代码语义，应用于增强 API 的 AST。在 *第 16 届 ACM 国际计算前沿会议论文集*。174–182。

+   陈等（2021）陈子敏、Steve Kommrusch、Michele Tufano、Louis-Noël Pouchet、Denys Poshyvanyk 和 Martin Monperrus。2021。SEQUENCER：端到端程序修复的序列到序列学习。*IEEE 软件工程学报* 47（2021），1943–1959。

+   陈和 Monperrus（2019）陈子敏 和 Martin Monperrus。2019。源代码嵌入的文献研究。（2019），1–8。arXiv:1904.03061 [`arxiv.org/abs/1904.03061`](http://arxiv.org/abs/1904.03061)

+   Cho 等（2014）Kyung-hyun Cho、Bart Van Merriënboer、Caglar Gulcehre、Dzmitry Bahdanau、Fethi Bougares、Holger Schwenk 和 Yoshua Bengio。2014。使用 RNN 编码器-解码器学习短语表示，用于统计机器翻译。*arXiv 预印本 arXiv:1406.1078*（2014）。

+   Compton 等（2020）Rhys Compton、Eibe Frank、Panos Patros 和 Abigail Koay。2020。使用 code2vec 嵌入 Java 类：变量混淆带来的改进。在 *挖掘软件库*。

+   Cvitkovic 等（2019）Milan Cvitkovic、Badal Singh 和 Anima Anandkumar。2019。在源代码上进行开放词汇学习，使用图结构缓存。在 *第 36 届国际机器学习会议，ICML 2019*，第 2019 年 6 月卷。国际机器学习学会（IMLS），2662–2674。arXiv:1810.08305 [`arxiv.org/abs/1810.08305v2`](https://arxiv.org/abs/1810.08305v2)

+   Dai 等（2020）Zihang Dai、Guokun Lai、Yiming Yang 和 Quoc V. Le。2020。Funnel-transformer：过滤顺序冗余以提高语言处理效率。*神经信息处理系统进展* 2020 年 12 月（2020），1–19。arXiv:2006.03236

+   Dam 等（2016）Hoa Khanh Dam、Truyen Tran 和 Trang Pham。2016。一种深度语言模型用于软件代码。*arXiv 预印本 arXiv:1608.02715*（2016）。

+   de Rezende Martins 和 Gerosa（2020）Marcelo de Rezende Martins 和 Marco Aurélio Gerosa。2020。CoNCRA：一种卷积神经网络代码检索方法。*arXiv: 学习*（2020）。

+   Denil 等人 (2014) 米沙·德尼尔、阿尔班·德米拉伊、纳尔·卡尔赫布伦纳、菲尔·布伦索姆、和南多·德·弗雷塔斯。2014 年。用单个卷积神经网络建模、可视化和总结文档。*arXiv: 计算与语言* (2014)。

+   Dinella 等人 (2020) 伊丽莎白·迪内拉、韩军·戴、李子阳、梅尤尔·奈克、乐松、和柯·王。2020 年。Hoppity: 学习图转换以检测和修复程序中的错误。发表于 *国际学习表示会议 (ICLR)*。

+   Ding 等人 (2019) 史蒂文·H.H. 丁、本杰明·C.M. 丰、和菲利普·查兰德。2019 年。Asm2Vec: 提升静态表示的鲁棒性以应对代码混淆和编译器优化。*IEEE 安全与隐私研讨会论文集* 2019 年 5 月 (2019), 472–489。 [`doi.org/10.1109/SP.2019.00003`](https://doi.org/10.1109/SP.2019.00003)

+   Duan 等人 (2020) 岳端、李雪玺、王晶汉、和尹恒。2020 年。Deepbindiff: 学习程序级代码表示以进行二进制差异比较。发表于 *网络与分布式系统安全研讨会*。

+   Fang 等人 (2021) 冯森、尤帅·谭、陶张、和叶胖·刘。2021 年。用于代码搜索的自注意力网络。*信息与软件技术* 134 (2021), 106542。

+   Feng 等人 (2016a) 龚乾、周润东、徐成成、程耀、布赖恩·泰斯塔、和尹恒。2016a。针对固件镜像的可扩展图基错误搜索。发表于 *ACM 计算机与通信安全会议论文集*，24-28 十月卷。480–491。 [`doi.org/10.1145/2976749.2978370`](https://doi.org/10.1145/2976749.2978370)

+   Feng 等人 (2016b) 龚乾、周润东、徐成成、程耀、布赖恩·泰斯塔、和尹恒。2016b。针对固件镜像的可扩展图基错误搜索。发表于 *计算机与通信安全*。

+   Feng 等人 (2020) 张银峰、达亚·郭、杜雨堂、段楠、肖成峰、明功、林君寿、秦冰、刘婷、姜达新、和明周。2020 年。CodeBERT: 一种用于编程和自然语言的预训练模型。*arXiv: 计算与语言* (2020)。

+   Fernandes 等人 (2019) 帕特里克·费尔南德斯、米尔提亚迪斯·阿拉曼尼斯、和马克·布罗克施密特。2019 年。结构化神经摘要。发表于 *第七届国际学习表示会议, ICLR 2019*。国际学习表示会议，ICLR。arXiv:1811.01824 [`arxiv.org/abs/1811.01824v4`](https://arxiv.org/abs/1811.01824v4)

+   Ferrante 等人 (1987a) 简·费兰特、卡尔·J·奥滕斯坦、和乔·D·沃伦。1987a。程序依赖图及其在优化中的应用。*ACM 编程语言与系统交易 (TOPLAS)* 9, 3 (1987), 319–349。

+   Ferrante 等人 (1987b) 简·费兰特、卡尔·J·奥滕斯坦、和乔·D·沃伦。1987b。程序依赖图及其在优化中的应用。*ACM 编程语言与系统交易 (TOPLAS)* 9, 3 (1987 年 7 月), 319–349。 [`doi.org/10.1145/24039.24041`](https://doi.org/10.1145/24039.24041)

+   Gibert 等 (2019) Daniel Gibert, Carles Mateu, Jordi Planes, 和 Ramon Vicens. 2019. 使用卷积神经网络对表示为图像的恶意软件进行分类。*计算机病毒与黑客技术杂志* 15 (2019), 15–28。

+   Gilmer 等 (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, 和 George E Dahl. 2017. 用于量子化学的神经消息传递。载于 *第 34 届国际机器学习大会, ICML 2017*，第 3 卷 2053–2070。arXiv:1704.01212

+   Gu 等 (2018) Xiaodong Gu, Hongyu Zhang, 和 Sunghun Kim. 2018. 深度代码搜索。载于 *国际软件工程大会*。

+   Guo 等 (2020) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, 等. 2020. Graphcodebert: 使用数据流进行代码表示的预训练。*arXiv 预印本 arXiv:2009.08366* (2020)。

+   Gupta 等 (2017) Rahul Gupta, Soham Pal, Aditya Kanade, 和 Shirish Shevade. 2017. DeepFix: 通过深度学习修复常见的 C 语言错误。载于 *全国人工智能大会*。

+   Hasan 等 (2021) Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed Mehrab, Md Haque, Mahim Anjum, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal, 和 Rifat Shahriyar. 2021. CoDesc: 一个大型代码-描述平行数据集。*arXiv 预印本 arXiv:2105.14220* (2021)。

+   He 等 (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, 和 Ross Girshick. 2020. 动量对比用于无监督视觉表示学习。*IEEE 计算机视觉与模式识别会议论文集* (2020), 9726–9735。 [`doi.org/10.1109/CVPR42600.2020.00975`](https://doi.org/10.1109/CVPR42600.2020.00975) arXiv:1911.05722

+   Hellendoorn 等 (2019) Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, 和 David Bieber. 2019. 源代码的全局关系模型。载于 *国际学习表征会议*。

+   Hermann 等 (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, 和 Phil Blunsom. 2015. 教授机器阅读和理解。*神经信息处理系统进展* 28 (2015), 1693–1701。

+   Hindle 等 (2012) Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, 和 Premkumar Devanbu. 2012. 软件的自然性。载于 *国际软件工程大会*。

+   Hochreiter 和 Schmidhuber (1997) Sepp Hochreiter 和 Jürgen Schmidhuber. 1997. 长短期记忆。*神经计算* 9, 8 (1997), 1735–1780。 [`doi.org/10.1162/neco.1997.9.8.1735`](https://doi.org/10.1162/neco.1997.9.8.1735)

+   Hu 等 (2020b) Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, 和 Mengting Yuan. 2020b. 基于结构嵌入的神经联合注意力代码搜索用于软件问答网站。*系统与软件杂志* 170 (2020), 110773。

+   Hu et al. (2018) Xing Hu, Ge Li, Xin Xia, David Lo, 和 Zhi Jin. 2018. 深度代码注释生成。在*国际程序理解会议*中。

+   Hu et al. (2020a) Xing Hu, Ge Li, Xin Xia, David Lo, 和 Zhi Jin. 2020a. 使用混合词汇和句法信息进行深度代码注释生成。*实证软件工程* 25 (2020), 2179–2217。

+   Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, 和 Nan Duan. 2021. CoSQA: 20,000+ 个用于代码搜索和问答的网络查询。*arXiv:计算与语言*（2021 年）。

+   Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, 和 Marc Brockschmidt. 2019. Codesearchnet 挑战：评估语义代码搜索的现状。*arXiv 预印本 arXiv:1909.09436*（2019 年）。

+   Hussain et al. (2020) Yasir Hussain, Zhiqiu Huang, Yu Zhou, 和 Senzhang Wang. 2020. CodeGRU: 使用门控递归单元进行上下文感知的深度学习以建模源代码。*信息与软件技术* 125 (2020), 106309。

+   Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, 和 Luke Zettlemoyer. 2016. 使用神经注意模型总结源代码。在*第 54 届计算语言学协会年会论文集（第 1 卷：长论文）*中。2073–2083。

+   Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, 和 Luke Zettlemoyer. 2018. 在编程上下文中将语言映射到代码。在*自然语言处理的实证方法*中。

+   Ji et al. (2021b) Xiujuan Ji, Lei Liu, 和 Jingwen Zhu. 2021b. 使用层次化注意图嵌入进行代码克隆检测。*软件工程与知识工程国际期刊* 31, 06 (2021), 837–861。

+   Ji et al. (2021a) Yuede Ji, Lei Cui, 和 H. Howie Huang. 2021a. BugGraph: 用图三元组损失网络区分源代码与二进制代码的相似性。*ASIA CCS 2021 - 2021 年 ACM 亚洲计算机与通信安全会议论文集* 1, c (2021), 702–715。 [`doi.org/10.1145/3433210.3437533`](https://doi.org/10.1145/3433210.3437533)

+   Jiang et al. (2021) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, 和 Lei Lyu. 2021. TreeBERT: 一种基于树的预训练编程语言模型。（2021 年 5 月）。arXiv:2105.12485 [`arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485`](https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485)

+   Kalchbrenner et al. (2014) Nal Kalchbrenner, Edward Grefenstette, 和 Phil Blunsom. 2014. 用于建模句子的卷积神经网络。在*ACL*中。

+   Kanade et al. (2019) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, 和 Kensen Shi. 2019. 学习和评估源代码的上下文嵌入。（2019 年 12 月）。arXiv:2001.00059 [`arxiv.org/abs/2001.00059`](http://arxiv.org/abs/2001.00059)

+   Kanade 等人 (2020) Aditya Kanade、Petros Maniatis、Gogul Balakrishnan 和 Kensen Shi。2020。《学习和评估源代码的上下文嵌入》。在 *国际机器学习大会*。

+   Karaivanov 等人 (2014) Svetoslav Karaivanov、Veselin Raychev 和 Martin Vechev。2014。《基于短语的编程语言统计翻译》。在 *SIGPLAN 研讨会：新的想法、新的范式以及对编程和软件的反思*。

+   Karampatsis 等人 (2020) Rafael-Michael Karampatsis、Hlib Babii、Romain Robbes、Charles Sutton 和 Andrea Janes。2020。《大代码 != 大词汇量：源代码的开放词汇模型》。*会议录 - 国际软件工程大会* (2020 年 3 月)，1073–1085. [`doi.org/10.1145/3377811.3380342`](https://doi.org/10.1145/3377811.3380342) arXiv:2003.07914v1

+   Kim 等人 (2020) Seohyun Kim、Jinman Zhao、Yuchi Tian 和 Satish Chandra。2020。《通过将树结构输入到变换器中的代码预测》。*arXiv: 软件工程* (2020)。

+   Kim 等人 (2021) Seohyun Kim、Jinman Zhao、Yuchi Tian 和 Satish Chandra。2021。《通过将树结构输入到变换器中的代码预测》。*会议录 - 国际软件工程大会* 1 (2021)，150–162. [`doi.org/10.1109/ICSE43902.2021.00026`](https://doi.org/10.1109/ICSE43902.2021.00026) arXiv:2003.13848

+   Kipf 和 Welling (2017) Thomas N. Kipf 和 Max Welling。2017。《使用图卷积网络的半监督分类》。arXiv:cs.LG/1609.02907

+   Lattner 和 Adve (2004) Chris Lattner 和 Vikram Adve。2004。《LLVM：一个用于终身程序分析与转换的编译框架》。在 *国际代码生成与优化研讨会，2004\. CGO 2004.* IEEE，75–86。

+   Le 等人 (2018) Quan Le、Oisín Boydell、Brian Mac Namee 和 Mark Scanlon。2018。《浅层深度学习：非领域专家的恶意软件分类》。*数字调查* 26 (2018)。

+   Le 等人 (2020) Triet H.M. Le、Hao Chen 和 Muhammad Ali Babar。2020。《源代码建模与生成的深度学习：模型、应用和挑战》。*计算机调查* 53, 3 (2020), 1–37. [`doi.org/10.1145/3383458`](https://doi.org/10.1145/3383458) arXiv:2002.05442

+   LeClair 等人 (2020) Alexander LeClair、Sakib Haque、Lingfei Wu 和 Collin McMillan。2020。《通过图神经网络改进代码摘要》。在 *IEEE 国际程序理解大会*，第 12 卷。IEEE 计算机学会，184–195. [`doi.org/10.1145/3387904.3389268`](https://doi.org/10.1145/3387904.3389268) arXiv:2004.02843

+   LeClair 等人 (2019) Alexander LeClair、Siyuan Jiang 和 Collin McMillan。2019。《用于生成程序子例程自然语言总结的神经模型》。在 *国际软件工程大会*。

+   LeClair 和 McMillan (2019) Alexander LeClair 和 Collin McMillan。2019。《源代码摘要数据集的推荐》。在 *北美计算语言学协会年会*。

+   Li 等人 (2020) Bingzhuo Li, Chunyang Ye, Shouyang Guan, 和 Hui Zhou. 2020. 基于事件嵌入树和 GAT 网络的语义代码克隆检测. *2020 IEEE 第 20 届国际软件质量、可靠性和安全会议论文集, QRS 2020* 3 (2020), 382–393. [`doi.org/10.1109/QRS51102.2020.00057`](https://doi.org/10.1109/QRS51102.2020.00057)

+   Li 等人 (2019a) Hongyu Li, Seohyun Kim, 和 Satish Chandra. 2019a. 神经代码搜索评估数据集. *arXiv: 软件工程* (2019).

+   Li 等人 (2017b) Jian Li, Yue Wang, Irwin King, 和 Michael R. Lyu. 2017b. 使用神经注意力和指针网络的代码补全. *arXiv: 计算与语言* (2017).

+   Li 等人 (2017c) Jian Li, Yue Wang, Michael R. Lyu, 和 Irwin King. 2017c. 使用神经注意力和指针网络的代码补全. *IJCAI 国际人工智能联合会议* 2018 年 7 月 (2017 年 11 月), 4159–4165. [`doi.org/10.24963/ijcai.2018/578`](https://doi.org/10.24963/ijcai.2018/578) arXiv:1711.09573

+   Li 等人 (2017d) Jian Li, Yue Wang, Michael R. Lyu, 和 Irwin King. 2017d. 使用神经注意力和指针网络的代码补全. *IJCAI 国际人工智能联合会议* 2018 年 7 月 (2017 年 11 月), 4159–4165. [`doi.org/10.24963/ijcai.2018/578`](https://doi.org/10.24963/ijcai.2018/578) arXiv:1711.09573v2

+   Li 等人 (2017a) Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, 和 Barbara Ryder. 2017a. Cclearner: 一种基于深度学习的克隆检测方法. 收录于 *2017 IEEE 国际软件维护与演化会议 (ICSME)*. IEEE, 249–260.

+   Li 等人 (2022) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, 和 Nan Duan. 2022. CodeRetriever: 单模态和双模态对比学习. *arXiv 预印本 arXiv:2201.10866* (2022).

+   Li 等人 (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, 和 Richard Zemel. 2015. 门控图序列神经网络. *arXiv 预印本 arXiv:1511.05493* (2015).

+   Li 等人 (2019b) Yi Li, Shaohua Wang, Tien N. Nguyen, 和 Son Van Nguyen. 2019b. 通过基于上下文的代码表示学习和基于注意力的神经网络改进错误检测. *ACM 编程语言会议论文集* 3, OOPSLA (2019 年 10 月), 30. [`doi.org/10.1145/3360588`](https://doi.org/10.1145/3360588)

+   Li 等人 (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, 和 Daniel Tarlow. 2016. 门控图序列神经网络. *第 4 届国际学习表征会议, ICLR 2016 - 会议论文集* 1 (2016), 1–20. arXiv:1511.05493

+   Li 等人 (2018) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun Deng, 和 Yuyi Zhong. 2018. VulDeePecker: 一种基于深度学习的漏洞检测系统. (2018 年 1 月). [`doi.org/10.14722/ndss.2018.23158`](https://doi.org/10.14722/ndss.2018.23158) arXiv:1801.01681v1

+   Liang 等人（[无日期]）Hongliang Liang, Yue Yu, Lin Jiang, 和 Zhuosi Xie。[无日期]。Seml：一种用于软件缺陷预测的语义 LSTM 模型。*IEEE Access* 7（[无日期]），83812–83824。

+   Lin 等人（2021）Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li, 和 Rongxin Wu。2021。通过块级抽象语法树拆分来改进代码总结。在*IEEE 国际程序理解会议*，2021 年 5 月卷。IEEE 计算机学会，184–195。 [`doi.org/10.1109/ICPC52881.2021.00026`](https://doi.org/10.1109/ICPC52881.2021.00026) arXiv:2103.07845

+   Lin（2004）Chin-Yew Lin。2004。ROUGE：一个自动评估总结的软件包。在*计算语言学协会年会*。

+   Lin 等人（2018）Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, 和 Michael D. Ernst。2018。NL2Bash：一个自然语言接口到 Linux 操作系统的语料库和语义解析器。在*语言资源与评估*。

+   Liu 等人（2016）Chang Liu, Xinyun Chen, Eui Chul Richard Shin, Mingcheng Chen, 和 Dawn Song。2016。用于 If-Then 程序合成的潜在注意力。在*NIPS*。

+   Liu 等人（2017）Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, 和 Dawn Song。2017。神经代码完成。

+   Liu 等人（2020b）Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, 和 Zhi Jin。2020b。用于代码完成的自注意神经架构与多任务学习。*IEEE 国际程序理解会议*（2020 年），37–47。 [`doi.org/10.1145/3387904.3389261`](https://doi.org/10.1145/3387904.3389261) arXiv:1909.06983

+   Liu 等人（2020c）Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, 和 Zhi Jin。2020c。用于代码完成的自注意神经架构与多任务学习。在*程序理解国际会议*。

+   Liu 等人（2020d）Fang Liu, Lu Zhang, 和 Zhi Jin。2020d。用堆栈增强 LSTM 分层建模程序。*系统与软件杂志* 164（2020 年），110547。

+   Liu 等人（2020a）Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, 和 Yang Liu。2020a。通过混合 GNN 的代码总结检索增强生成。（2020 年 6 月）。arXiv:2006.05405 [`arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405`](https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405)

+   Liu 等人（2021c）Xuye Liu, Dakuo Wang, April Wang, Yufang Hou, 和 Lingfei Wu。2021c。HAConvGNN：基于层次注意力的卷积图神经网络用于 Jupyter 笔记本中的代码文档生成。（2021 年 3 月）。 [`doi.org/10.18653/v1/2021.findings-emnlp.381`](https://doi.org/10.18653/v1/2021.findings-emnlp.381) arXiv:2104.01002

+   Liu 等人（2021a）Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu 和 Xun Wang. 2021a. 将图神经网络与专家知识结合用于智能合约漏洞检测。*IEEE 知识与数据工程汇刊* 01（2021 年 7 月），1–1。 [`doi.org/10.1109/TKDE.2021.3095196`](https://doi.org/10.1109/TKDE.2021.3095196) arXiv:2107.11598

+   Liu 等人（2021b）Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu 和 Xun Wang. 2021b. 将图神经网络与专家知识结合用于智能合约漏洞检测。*IEEE 知识与数据工程汇刊*（2021）。[`doi.org/10.1109/TKDE.2021.3095196`](https://doi.org/10.1109/TKDE.2021.3095196) arXiv:2107.11598

+   Lu 等人（2021）Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Fu Shengyu 和 Shujie Liu. 2021. CodeXGLUE: 用于代码理解和生成的机器学习基准数据集。*arXiv: 软件工程*（2021）。

+   Mahmud 等人（2021）Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos 和 Kevin Moran. 2021. 代码到评论翻译：模型有效性和错误的比较研究。

+   Mikolov 等人（2013）Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado 和 Jeff Dean. 2013. 单词和短语的分布式表示及其组合性。*神经信息处理系统进展* 26（2013）。

+   Mou 等人（2014）Lili Mou, Ge Li, Zhi Jin, Lu Zhang 和 Tao Wang. 2014. TBCNN: 一种用于编程语言处理的基于树的卷积神经网络。*arXiv 预印本 arXiv:1409.5718*（2014）。

+   Mou 等人（2016）Lili Mou, Ge Li, Lu Zhang, Tao Wang 和 Zhi Jin. 2016. 针对编程语言处理的树结构卷积神经网络。发表于 *第三十届 AAAI 人工智能会议*。

+   Nguyen 等人（2014）Anh Tuan Nguyen, Tung Thanh Nguyen 和 Tien N. Nguyen. 2014. 使用统计机器翻译迁移代码。发表于 *国际软件工程会议*。

+   Nguyen 等人（2018）Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen 和 Tho Thanh Quan. 2018. 使用延迟绑定控制流图和深度学习的复杂恶意软件自动检测。*计算机与安全* 76（2018 年 7 月），128–155。 [`doi.org/10.1016/J.COSE.2018.02.006`](https://doi.org/10.1016/J.COSE.2018.02.006)

+   Nguyen 等人（2017）Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan 和 Tien N. Nguyen. 2017. 探索 API 嵌入用于 API 使用和应用。发表于 *国际软件工程会议*。

+   Nguyen 等人（2013）Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen 和 Tien N. Nguyen. 2013. 用于源代码的统计语义语言模型。发表于 *FSE*。

+   Oda 等（2015）小田由辅、藤波浩之、格雷厄姆·纽比格、旗田秀明、坂田沙基亚尼、戸田智树和中村聪。2015。使用统计机器翻译（T）从源代码生成伪代码。 在*自动化软件工程*。

+   Papineni 等（2002）基肖尔·帕皮尼、萨利姆·鲁科斯、托德·沃德和魏京·朱。2002。Bleu：一种自动评估机器翻译的方法。在*计算语言学协会会议*。

+   彭等（2021）丁岚·彭、舒欣·郑、雅涛·李、郭林·柯、狄·何、和铁燕·刘。2021。神经网络如何理解程序？（2021）。arXiv:2105.04297 [`arxiv.org/abs/2105.04297`](http://arxiv.org/abs/2105.04297)

+   Prosser（1959）Reese T. Prosser。1959。布尔矩阵在流程图分析中的应用。在*1959 年 12 月 1-3 日东部联合 IRE-AIEE-ACM 计算机会议论文集*（波士顿，马萨诸塞州）*(IRE-AIEE-ACM ’59（东部）)*。计算机协会，纽约，NY，美国，133–138。 [`doi.org/10.1145/1460299.1460314`](https://doi.org/10.1145/1460299.1460314)

+   Puri 等（2021）鲁奇尔·普里、大卫·S·孔、吉尔特·扬森、魏·张、贾科莫·多梅尼科尼、弗拉基米尔·佐洛托夫、朱利安·多尔比、杰·陈、米希尔·乔杜里、林赛·德克、维罗尼卡·托斯特、卢卡·布拉蒂、索拉布·普贾尔、夏姆·拉姆吉、乌尔里希·芬克勒、苏珊·马拉伊卡和弗雷德里克·赖斯。2021。CodeNet：一个大规模的 AI 代码数据集，用于学习多样的编码任务。

+   Rabinovich 等（2017）马克西姆·拉宾诺维奇、米切尔·斯特恩和丹·克莱因。2017。用于代码生成和语义解析的抽象语法网络。在*计算语言学协会会议*。

+   Raffel 等（2019）科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马滕、闫琪·周、魏·李和彼得·J·刘。2019。使用统一的文本到文本变换器探索迁移学习的极限。*arXiv: 学习*（2019）。

+   Raychev 等（2016）Veselin Raychev、Pavol Bielik 和 Martin Vechev。2016。使用决策树的代码概率模型。*ACM SIGPLAN Notices* 51, 10（2016），731–747。 [`doi.org/10.1145/2983990.2984041`](https://doi.org/10.1145/2983990.2984041)

+   RaychevVeselin 等（2014）RaychevVeselin、VechevMartin 和 YahavEran。2014。使用统计语言模型进行代码补全。*Sigplan Notices*（2014）。

+   Ren 等（2020）邵然、大亚·郭、帅路、龙周、舒杰·刘、杜宇·唐、尼尔·苏达雷桑、明周、安布罗斯·布兰科和帅马。2020。CodeBLEU：一种自动评估代码合成的方法。*arXiv: 软件工程*（2020）。

+   Rumelhart 等（1986）大卫·E·鲁梅尔哈特、杰弗里·E·辛顿和罗纳德·J·威廉姆斯。1986。通过反向传播误差学习表示。*自然* 323, 6088 (1986), 533–536。

+   Rush 等（2015）亚历山大·M·拉什、苏米特·乔普拉和杰森·韦斯顿。2015。用于抽象句子摘要的神经注意力模型。在*EMNLP*。

+   Sachdev 等人（2018）Saksham Sachdev、Hongyu Li、Sifei Luan、Seohyun Kim、Koushik Sen 和 Satish Chandra。2018。源代码检索：一种神经代码搜索。

+   Shaw 等人（2018）Peter Shaw、Jakob Uszkoreit 和 Ashish Vaswani。2018。具有相对位置表示的自注意力。*NAACL HLT 2018 - 2018 年北美计算语言学协会：人类语言技术会议 - 会议论文集* 2（2018），464–468。 [`doi.org/10.18653/v1/n18-2074`](https://doi.org/10.18653/v1/n18-2074) arXiv:1803.02155

+   Shi 等人（2021）Ensheng Shi、Yanlin Wang、Lun Du、Hongyu Zhang、Shi Han、Dongmei Zhang 和 Hongbin Sun。2021。CAST: 通过层次分割和重建抽象语法树来增强代码摘要。（2021 年 8 月）。arXiv:2108.12987 [`arxiv.org/abs/2108.12987`](http://arxiv.org/abs/2108.12987)

+   Shi 等人（2020）Ke Shi、Yang Lu、Jingfei Chang 和 Zhen Wei。2020。PathPair2Vec: 一种基于 AST 路径对的代码表示方法用于缺陷预测。

+   Shido 等人（2019）Yusuke Shido、Yasuaki Kobayashi、Akihiro Yamamoto、Atsushi Miyamoto 和 Tadayuki Matsumura。2019。使用扩展 Tree-LSTM 的自动源代码摘要。载于 *国际联合神经网络大会论文集*，卷 2019-July。电气和电子工程师协会。 [`doi.org/10.1109/IJCNN.2019.8851751`](https://doi.org/10.1109/IJCNN.2019.8851751) arXiv:1906.08094

+   Shrivastava（2021）Piyush Shrivastava。2021。神经代码摘要。*arXiv: 软件工程*（2021）。

+   Shuai 等人（2020）Jianhang Shuai、Ling Xu、Chao Liu、Meng Yan、Xin Xia 和 Yan Lei。2020。通过协同注意力表示学习改进代码搜索。载于 *国际程序理解大会*。

+   Sui 等人（2020）Yulei Sui、Xiao Cheng、Guanqin Zhang 和 Haoyu Wang。2020。Flow2Vec: 基于值流的精确代码嵌入。*ACM 编程语言论文集* 4，OOPSLA（2020 年 11 月），27。 [`doi.org/10.1145/3428301`](https://doi.org/10.1145/3428301)

+   Sutskever 等人（2014）Ilya Sutskever、Oriol Vinyals 和 Quoc V Le。2014。基于神经网络的序列到序列学习。载于 *神经信息处理系统进展*。3104–3112。

+   Svyatkovskiy 等人（2020）Alexey Svyatkovskiy、Shao Kun Deng、Fu Shengyu 和 Neel Sundaresan。2020。IntelliCode compose: 使用 transformer 进行代码生成。载于 *FSE*。

+   Svyatkovskiy 等人（2019）Alexey Svyatkovskiy、Ying Zhao、Fu Shengyu 和 Neel Sundaresan。2019。Pythia: AI 辅助代码补全系统。*arXiv: 软件工程*（2019）。

+   Tai 等（2015）Kai Sheng Tai、Richard Socher 和 Christopher D. Manning。2015。通过树结构的长短期记忆网络改进语义表示。见于 *ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference*，第 1 卷。计算语言学协会（ACL），1556–1566。[`doi.org/10.3115/v1/p15-1150`](https://doi.org/10.3115/v1/p15-1150) arXiv:1503.00075

+   Tarlow 等（2020a）Daniel Tarlow、Subhodeep Moitra、Andrew Rice、Zimin Chen、Pierre Antoine Manzagol、Charles Sutton 和 Edward Aftandilian。2020a。通过 Graph2Diff 神经网络学习修复构建错误。见于 *Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020*。计算机协会，19–20。[`doi.org/10.1145/3387940.3392181`](https://doi.org/10.1145/3387940.3392181) arXiv:1911.01205

+   Tarlow 等（2020b）Daniel Tarlow、Subhodeep Moitra、Andrew Rice、Zimin Chen、Pierre Antoine Manzagol、Charles Sutton 和 Edward Aftandilian。2020b。通过 Graph2Diff 神经网络学习修复构建错误。见于 *Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020*。19–20。[`doi.org/10.1145/3387940.3392181`](https://doi.org/10.1145/3387940.3392181) arXiv:1911.01205

+   Tu 等（2014）Zhaopeng Tu、Zhendong Su 和 Premkumar Devanbu。2014。关于软件的局部性。见于 *FSE*。

+   Ugner 等（[n.d.]）Daniel Z ¨ Ugner、Tobias Kirschstein、Michele Catasta、Jure Leskovec 和 Stephan G ¨ Unnemann。[n.d.]。从结构和上下文中进行语言无关的源代码表示学习。（[n.d.]）。arXiv:2103.11318v1 www.daml.in.tum.de/code-transformer

+   Vasic 等（2019）Marko Vasic、Aditya Kanade、Petros Maniatis、David Bieber 和 Rishabh Singh。2019。通过联合学习定位和修复实现神经程序修复。*arXiv: Learning*（2019）。

+   Vaswani 等（2017）Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N Gomez、Łukasz Kaiser 和 Illia Polosukhin。2017。注意力机制就是你所需要的。见于 *Advances in neural information processing systems*。5998–6008。

+   Veličković 等（2018）Petar Veličković、Arantxa Casanova、Pietro Liò、Guillem Cucurull、Adriana Romero 和 Yoshua Bengio。2018。图注意力网络。见于 *6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings*。国际学习表征会议，ICLR。arXiv:1710.10903 [`arxiv.org/abs/1710.10903v3`](https://arxiv.org/abs/1710.10903v3)

+   Wan 等（2019a）姚万、舒景东、隋宇雷、许光东、赵周、吴建和菲利普·余。2019a。用于语义源代码检索的多模态注意力网络学习。见于 *2019 年第 34 届 IEEE/ACM 国际自动化软件工程会议论文集，ASE 2019*。电气与电子工程师协会，13–25。 [`doi.org/10.1109/ASE.2019.00012`](https://doi.org/10.1109/ASE.2019.00012) arXiv:1909.13516

+   Wan 等（2019b）姚万、舒景东、隋宇雷、许光东、赵周、吴建和菲利普·S·余。2019b。用于语义源代码检索的多模态注意力网络学习。*arXiv 预印本 arXiv:1909.13516*（2019）。

+   Wan 等（2018）姚万、赵周、杨敏、许光东、英浩超、吴建和菲利普·S·余。2018。通过深度强化学习改善自动源代码摘要。在 *ASE 2018 - 第 33 届 ACM/IEEE 国际自动化软件工程会议论文集* 中。计算机协会，397–407。 [`doi.org/10.1145/3238147.3238206`](https://doi.org/10.1145/3238147.3238206) arXiv:1811.07234

+   Wang 等（2021b）D. 王、Z. 贾、S. 李、Y. 于、Y. 熊、W. 董和 X. 廖。2021b。桥接预训练模型和下游任务以理解源代码。（2021）。

+   Wang（2019）柯王。2019。学习可扩展和精确的程序语义表示。（2019 年 5 月）。arXiv:1905.05251 [`arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251`](https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251)

+   Wang 等（2020a）宋王、刘泰跃、南杰昌和谭林。2020a。用于软件缺陷预测的深度语义特征学习。*IEEE 软件工程学报* 46（2020），1267–1293。

+   Wang 等（2016）宋王、刘泰跃和谭林。2016。自动学习缺陷预测的语义特征。见于 *软件工程国际会议*。

+   Wang 等（2021c）汪文汉、沈思杰、李戈和金志。2021c。通过神经语言模型实现全行代码补全。（2021）。arXiv:2009.08603v1 www.aaai.org

+   Wang 等（2020c）汪文汉、张克琪、李戈和金志。2020c。通过异质图表示程序的学习。（2020），1–10。arXiv:2012.04188 [`arxiv.org/abs/2012.04188`](http://arxiv.org/abs/2012.04188)

+   Wang 等（2021a）王彦林、韩石、张冬梅、施恩生、杜伦、杨晓迪、胡宇轩、韩石和张洪宇。2021a。CoCoSum：使用多关系图神经网络的上下文代码摘要。*J. ACM* 1，1（2021 年 7 月），24。 [`doi.org/10.1145/nnnnnnn.nnnnnnn`](https://doi.org/10.1145/nnnnnnn.nnnnnnn) arXiv:2107.01933

+   Wang 和 Li（2021）王彦林和李辉。2021。通过将展平的抽象语法树建模为图来完成代码。（2021）。arXiv:2103.09499 [`arxiv.org/abs/2103.09499`](http://arxiv.org/abs/2103.09499)

+   Wang 等人（2020b）于旺、柯旺、冯娟高 和 林章王。2020b。利用图间隔神经网络学习语义程序嵌入。*ACM 程序语言会议录* 4，OOPSLA（2020 年 5 月）。[`doi.org/10.1145/3428205`](https://doi.org/10.1145/3428205) arXiv:2005.09997

+   Wang 等人（2021d）岳旺、韦施旺、沙菲克·乔提 和 Steven C. H. Hoi。2021d。CodeT5：面向标识符的统一预训练编码器-解码器模型，用于代码理解和生成。*arXiv:计算与语言*（2021）。

+   Wei 等人（2019）博林·魏、永敏李、葛力、辛霞 和 智金。2019。检索和精炼：基于示例的神经评论生成。在*自动化软件工程*。

+   Wei 和 Li（2017）慧慧魏 和 明李。2017。通过利用源代码中的词汇和语法信息进行软件功能克隆检测的监督深度特征。在*IJCAI 国际人工智能联合会议*。3034–3040。[`doi.org/10.24963/ijcai.2017/423`](https://doi.org/10.24963/ijcai.2017/423)

+   White 等人（2016）马丁·怀特、米歇尔·图法诺、克里斯托弗·文多姆 和 丹尼斯·波希瓦尼克。2016。深度学习代码片段用于代码克隆检测。在*2016 年第 31 届 IEEE/ACM 自动化软件工程国际会议（ASE）*。IEEE，87–98。

+   White 等人（2015）马丁·怀特、克里斯托弗·文多姆、马里奥·利纳雷斯-瓦斯克斯 和 丹尼斯·波希瓦尼克。2015。迈向深度学习软件库。在*挖掘软件库*。

+   Wu 等人（2020）李伟、费力、游华武 和 郑涛。2020。GGF：一种基于图的编程语言语法错误修正方法。*IEEE 国际程序理解会议*（2020），139–148。[`doi.org/10.1145/3387904.3389252`](https://doi.org/10.1145/3387904.3389252)

+   Wu 等人（2019）宗汉吴、时睿潘、冯文陈、国栋龙、成琪张 和 Philip S. Yu。2019。图神经网络的全面调查。*IEEE 神经网络与学习系统汇刊* 32, 1（2019 年 1 月），4–24。[`doi.org/10.1109/TNNLS.2020.2978386`](https://doi.org/10.1109/TNNLS.2020.2978386) arXiv:1901.00596v4

+   Xiao 等人（2017）杨晓、Jacky Keung、Qing Mi 和 Kwabena Ebo Bennin。2017。通过增强卷积神经网络改进缺陷定位。发表于*亚太软件工程会议*。

+   Xie 等人（2021） Binbin Xie、金松苏、于宾葛、项李、建伟崔、俊峰姚 和 Bin Wang。2021。通过互学提升树状解码器训练以生成代码。（2021）。arXiv:2105.14796v1 www.aaai.org

+   Xu 等人（2021）徐玲、欢欢杨、超刘、建行帅、孟燕、燕雷 和 周旭。2021。基于两阶段注意力的代码搜索模型，结合文本和结构特征。在*IEEE 国际软件分析、演变与重构会议*。

+   许等（2019）司涵许、森张、伟晶王、新雅曹、陈凯郭和晶许。2019 年。具有层次注意力网络的方法名称建议。

+   许等（2017）肖军许、常刘、钱丰、恒尹、乐宋和晓歌。2017 年。基于神经网络的图嵌入用于跨平台二进制代码相似性检测。在*ACM 计算机与通信安全会议论文集*中。计算机协会，363–376。[`doi.org/10.1145/3133956.3134018`](https://doi.org/10.1145/3133956.3134018) arXiv:1708.06525

+   薛等（2021）江薛、卓然郑、陈吕、梁李和雷吕。2021 年。TreeBERT：一种基于树的编程语言预训练模型。在*人工智能中的不确定性*中。

+   山口等（2014）法比安·山口、尼科·戈尔德、丹尼尔·阿普和孔拉德·里克。2014 年。使用代码属性图建模和发现漏洞。在*IEEE 安全与隐私研讨会论文集*中，590–604。[`doi.org/10.1109/SP.2014.44`](https://doi.org/10.1109/SP.2014.44)

+   严等（2019）佳琪严、关华严和董金。2019 年。使用深度图卷积神经网络对控制流图表示的恶意软件进行分类。在*第 49 届年度 IEEE/IFIP 国际可靠性系统与网络会议论文集，DSN 2019*中。电气和电子工程师学会，52–63。[`doi.org/10.1109/DSN.2019.00020`](https://doi.org/10.1109/DSN.2019.00020)

+   杨和匡（[n.d.]）郝杨和李匡。[n.d.]。CCMC: 具有记忆机制和复制机制的代码补全；CCMC: 具有记忆机制和复制机制的代码补全。 ([n. d.])。[`doi.org/10.1145/3463274.3463332`](https://doi.org/10.1145/3463274.3463332)

+   杨等（2020）康杨、辉群于、桂生范和兴光杨。2020 年。集成语义特征的图嵌入代码预测模型。*计算机科学与信息系统* 17, 3（2020），907–926。[`doi.org/10.2298/CSIS190908027Y`](https://doi.org/10.2298/CSIS190908027Y)

+   杨等（2019）柯杨、明兴张、康陈、肖松马、杨白和永江。2019 年。KnightKing: 一个快速的分布式图随机游走引擎。在*第 27 届 ACM 操作系统原理研讨会论文集*中，524–537。

+   安永和梁（2020a）安永和梁。2020a。基于图的自监督程序修复来自诊断反馈。（2020 年）。

+   安永和梁（2020b）安永和梁。2020b。基于图的自监督程序修复来自诊断反馈。在*国际机器学习大会*中。

+   Yin et al. (2018a) 彭城·尹、博文·邓、埃德加·陈、博格丹·瓦西莱斯库和格雷厄姆·纽比格。2018a. 从 Stack Overflow 中学习挖掘对齐的代码和自然语言对。在*国际软件工程会议论文集*。IEEE 计算机协会，476–486。 [`doi.org/10.1145/3196398.3196408`](https://doi.org/10.1145/3196398.3196408) arXiv:1805.08949

+   Yin et al. (2018b) 彭城·尹、博文·邓、埃德加·陈、博格丹·瓦西莱斯库和格雷厄姆·纽比格。2018b. 从 Stack Overflow 中学习挖掘对齐的代码和自然语言对。在*矿藏软件库*。

+   Yin and Neubig (2018) 彭城·尹和格雷厄姆·纽比格。2018. TRANX：一种基于转换的神经抽象语法解析器用于语义解析和代码生成。在*自然语言处理中的实证方法*。

+   Yu et al. (2020) 泽平·余、瑞·曹、启逸·唐、森·聂、俊州·黄和石·吴。2020. 顺序很重要：语义感知神经网络用于二进制代码相似性检测。在*AAAI 2020 - 第 34 届 AAAI 人工智能会议*，第 34 卷。AAAI 出版社，1145–1152。 [`doi.org/10.1609/aaai.v34i01.5466`](https://doi.org/10.1609/aaai.v34i01.5466)

+   Zhang et al. (2020a) 简·张、徐·王、洪宇·张、海龙·孙和旭东·刘。2020a. 基于检索的神经源代码摘要。*国际软件工程会议论文集*（2020），1385–1397。 [`doi.org/10.1145/3377811.3380383`](https://doi.org/10.1145/3377811.3380383)

+   Zhang et al. (2020b) 简·张、徐·王、洪宇·张、海龙·孙和旭东·刘。2020b. 基于检索的神经源代码摘要。在*国际软件工程会议*。

+   Zhang et al. (2019a) 简·张、徐·王、洪宇·张、海龙·孙、凯轩·王和旭东·刘。2019a. 基于抽象语法树的新型神经源代码表示。*国际软件工程会议论文集* 2019 年 5 月（2019），783–794。 [`doi.org/10.1109/ICSE.2019.00086`](https://doi.org/10.1109/ICSE.2019.00086)

+   Zhang et al. (2019b) 简·张、徐·王、洪宇·张、海龙·孙、凯轩·王和旭东·刘。2019b. 基于抽象语法树的新型神经源代码表示。在*2019 IEEE/ACM 第 41 届国际软件工程会议（ICSE）*。783–794。 [`doi.org/10.1109/ICSE.2019.00086`](https://doi.org/10.1109/ICSE.2019.00086)

+   Zhang et al. (2018) 穆汉·张、志成·崔、玛丽昂·诺伊曼和义欣·陈。2018. 用于图分类的端到端深度学习架构。在*第三十二届 AAAI 人工智能会议*。

+   Zhao and Huang (2018) 刚·赵和杰夫·黄。2018. DeepSim：深度学习代码功能相似性。（2018）。 [`doi.org/10.1145/3236024.3236068`](https://doi.org/10.1145/3236024.3236068)

+   Zhou et al. (2019) 余勤·周、尚清·刘、晶凯·萧、晓宁·杜、杨·刘。2019。Devign：通过图神经网络学习全面程序语义的有效漏洞识别。*神经信息处理系统进展* 32 (2019 年 9 月)。arXiv:1909.03496 [`arxiv.org/abs/1909.03496v1`](https://arxiv.org/abs/1909.03496v1)

+   Zhu et al. (2016) 晓伟·朱、文光·陈、伟敏·郑、晓松·马。2016。Gemini：一个以计算为中心的分布式图处理系统。在*第 12 届$\{$USENIX$\}$操作系统设计与实现研讨会（$\{$OSDI$\}$ 16）*。301–316。

+   Zuo et al. (2019) 飞·左、小鹏·李、帕特里克·杨、兰南·罗、强·曾、哲欣·张。2019。超越函数对的神经机器翻译启发的二进制代码相似性比较。互联网社会。 [`doi.org/10.14722/ndss.2019.23492`](https://doi.org/10.14722/ndss.2019.23492) arXiv:1808.04706
