<!--yml

category: 未分类

date: 2024-09-06 19:33:02

-->

# [2404.18961] 2.1.1\. 特征选择

> 来源：[`ar5iv.labs.arxiv.org/html/2404.18961`](https://ar5iv.labs.arxiv.org/html/2404.18961)

| 模型名称 | 来源 | 年份 | 类型 | 矩阵正则化 | 向量形式 |
| --- | --- | --- | --- | --- | --- |
| 正则化多任务学习 | KDD | evgeniou2004regularized | 组正则化 | Frobenius 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}-\frac{1}{T}\sum_{t=1}^{T}\boldsymbol{w}^{t}\&#124;}^{2}_{2}+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2}$ |
| 使用核方法学习多任务 | JMLR | \citeyearevgeniou2005learning | 先验共享 | 自适应惩罚 | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$ $\boldsymbol{V}\in\boldsymbol{S}^{D}$ |
| 交替结构优化 | JMLR | \citeyearando2005framework | 分解 | Frobenius 范数 | $\min\limits_{\{\boldsymbol{W},\boldsymbol{V}\},\Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{w}^{t}+\Theta^{\top}\boldsymbol{v}^{t})-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$,  s.t. $\Theta\Theta^{\top}=\boldsymbol{I}_{h\times h}$ |
| 多任务特征选择 | Tech. Rep.¹ | \citeyearobozinski2006multi | 组稀疏学习 | $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}$ |
| 多任务 Lasso | Thesis² | \citeyearzhang2006a | 组稀疏学习 | $\ell_{\infty,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{\infty}$ |
| 多任务特征学习 | NeurIPS | \citeyearargyriou2006multi | 组稀疏学习，特征学习 | $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{U},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;({\boldsymbol{X}^{(t)}}\boldsymbol{U})\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda(\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2})^{2}$,  s.t. $\boldsymbol{U}\in\boldsymbol{O}^{D}$ |
| 凸多任务特征学习 | Mach. Lea. | \citeyearargyriou2008convex | 特征学习 | 自适应惩罚 | $\min\limits_{\boldsymbol{V}, \boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda\sum_{t=1}^{T}{\boldsymbol{w}^{t}}^{\top}\boldsymbol{V}^{+}\boldsymbol{w}^{t},$  s.t. $\boldsymbol{V}\in\boldsymbol{S}_{+}^{D},$ tr$(\boldsymbol{V})\leq 1$, col$(\boldsymbol{W})\subseteq$col$(\boldsymbol{V})$ |
| 低秩 MTL | ICML | \citeyearji2009accelerated | 低秩学习 | 跟踪范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda\lvert\boldsymbol{W}\rvert_{*}$ |
| 凸 ASO | ICML | \citeyearchen2009convex | — | — | $\min\limits_{\boldsymbol{U}, \Theta}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{u}^{t}-\boldsymbol{y}^{t}\rvert_{2}^{2}+\lambda\eta(1-\eta)\text{tr}(\boldsymbol{U}^{\top}(\eta\boldsymbol{I}+\Theta^{\top}\Theta)^{-1}\boldsymbol{U}),~{}~{}s.t.~{}\Theta\Theta^{\top}=\boldsymbol{I}_{h\times h}$ |
| 脏块稀疏模型 | NeurIPS | \citeyearjalali2010dirty | 组稀疏学习，分解 | $\ell_{\infty,1}$ 范数 $+$ $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}(\boldsymbol{s}^{t}+\boldsymbol{b}^{t})-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\lvert\boldsymbol{s}_{d}\rvert_{1}+\lambda_{2}\sum_{d=1}^{D}\lvert\boldsymbol{b}_{d}\rvert_{\infty}$,  s.t. $\boldsymbol{W}=\boldsymbol{S}+\boldsymbol{B}$ |
| 稀疏多任务 Lasso | NeurIPS | \citeyearlee2010adaptive | 组稀疏学习 | 加权 $\ell_{2,1}$ 范数 $+$ 加权 $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}\lvert\boldsymbol{w}_{d}\rvert_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}\lvert\boldsymbol{w}_{d}\rvert_{1}$ |
| \cdashline1-6 |  |  |  | 加权 $\ell_{2,1}$ 范数 $+$ 加权 $\ell_{1,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\lvert{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\rvert^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\rho_{d}\lvert\boldsymbol{w}_{d}\rvert_{2}+\lambda_{2}\sum_{d=1}^{D}\theta_{d}\lvert\boldsymbol{w}_{d}\rvert_{1}+\log Z(\boldsymbol{\rho}, \boldsymbol{\theta})$, |
| 自适应多任务 Lasso | NeurIPS | \citeyearlee2010adaptive | 组稀疏学习 | $+$ 自适应惩罚 | $P(\boldsymbol{W} \mid \boldsymbol{\rho}, \boldsymbol{\theta})=\frac{1}{Z(\boldsymbol{\rho}, \boldsymbol{\theta})}\prod_{d=1}^{D}\prod_{t=1}^{T}\exp(-\theta_{d}\lvert w_{n,t}\rvert)\times\prod_{d=1}^{D}\exp(-\rho_{d}\lvert \mathbf{w}_{d}\rvert_{2})$ |
|  |  |  |  |  | $\min\limits_{\mathbf{M}_{0},\ldots,\mathbf{M}_{T}}\gamma_{0}\&#124;\mathbf{M}_{0}-\mathbf{I}\&#124;_{F}^{2}+\sum\nolimits_{t=1}^{T}\left[\gamma_{t}\&#124;\mathbf{M}_{t}\&#124;_{F}^{2}+\sum\nolimits_{(i,j)\in J_{t},j\neq i}d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})+\sum\nolimits_{(i,j,k)\in S_{t}}\xi_{ijk}\right]$ |
| 大边际多任务度量学习 | NeurIPS | \citeyearparameswaran2010large | 先验共享 | Frobenius 范数 | s.t. $\forall t,\forall(i,j,k)\in S_{t}\colon\quad d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{k})-d_{t}^{2}(\mathbf{x}_{i},\mathbf{x}_{j})\geq 1-\xi_{ijk};\xi_{ijk}\geq 0;\mathbf{M}_{0},\mathbf{M}_{1},\ldots,\mathbf{M}_{T}\geq 0$ |
| 层次多任务结构化输出学习 | NeurIPS | \citeyeargornitz2011hierarchical | 先验共享 | Frobenius 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{1}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}&#124;&#124;_{2}^{2}-\lambda\boldsymbol{w}^{T}\boldsymbol{w}_{p}$，其中 $p$ 是父节点。 |
|  |  |  | 低秩学习 |  |  |
| 稳健的 MTL | KDD | \citeyearchen2011integrating | 分解，组稀疏学习 | 跟踪范数 + $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\&#124;{\boldsymbol{X}^{(t)}}(\boldsymbol{l}^{t}+\boldsymbol{s}^{t})-\boldsymbol{y}^{(t)}\&#124;_{2}^{2}+\lambda_{1}\&#124;\boldsymbol{L}\&#124;_{*}+\lambda_{2}\sum_{t=1}^{T}\&#124;\boldsymbol{s}_{t}\&#124;_{2}$,  s.t. $\boldsymbol{W}=\boldsymbol{L}+\boldsymbol{S}$ |
| 时间组 Lasso | KDD | \citeyearzhou2011multi | 组稀疏学习 | Frobenius 范数 + $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{2}^{2}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$ |
| 聚类 MTL | NeurIPS | \citeyearzhou2011clustered | 任务聚类 | 聚类惩罚 + $\ell_{2,2}$ 范数 | $\min\limits_{\boldsymbol{W},\boldsymbol{F}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\lambda_{1}(\text{tr}(\boldsymbol{W}^{\top}\boldsymbol{W})-\text{tr}(\boldsymbol{F}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{F}))+\lambda_{2}\sum_{t=1}^{T}{\&#124;\boldsymbol{w}^{t}\&#124;}^{2}_{2},$ |
| $~{}~{}\text{s.t.}~{}\boldsymbol{F}_{t,j}=1/\sqrt{n_{j}}~{}\text{if}~{}t\in\mathcal{C}_{j}~{}\text{otherwise}~{}0,$ $t=1,\cdots,T$, 其中 $n_{j}$ 是第 $j$ 个聚类 $\mathbf{\mathcal{C}}_{j}$ 中的任务数。 |
|  |  |  | 分解，稀疏学习 |  |  |
| 稀疏和低秩 MTL | TKDD | \citeyearchen2012learning | 低秩学习 | $\ell_{1,1}$ 范数 + 迹范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{1}$，满足 $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q},\&#124;\boldsymbol{Q}\&#124;_{*}\leq\tau$ |
| 凸融合稀疏组套索 | KDD | \citeyearzhou2012modeling | 组稀疏学习 | $\ell_{1,1}$ 范数 $+$ $\ell_{2,1}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{1}+\lambda_{2}\sum_{t=1}^{T-1}\&#124;\boldsymbol{w}^{t}-\boldsymbol{w}^{t+1}\&#124;_{1}+\lambda_{3}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}$ |
| 自适应多任务弹性网络 | SDM | \citeyearchen2012adaptive | 组稀疏学习 | $\ell_{2,1}$ 范数 $+$ Frobenius 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}{\&#124;\boldsymbol{w}_{d}\&#124;}_{2}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{w}_{d}\&#124;_{2}^{2}$ |
| 多层次套索 | ICML | \citeyearlozano2012multi | 分解，稀疏学习 | $\ell_{1,1}$ 范数 + 自适应惩罚 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\theta_{d}+\lambda_{2}\sum_{d=1}^{D}\&#124;\boldsymbol{\boldsymbol{\gamma}}_{d}\&#124;_{1}$，满足 $\boldsymbol{W}=\vec{\boldsymbol{\theta}}\boldsymbol{\Lambda}\boldsymbol{\Gamma},\vec{\boldsymbol{\theta}}\geq\boldsymbol{0}$ |
| 稳健的多任务特征学习 | KDD | \citeyeargong2012robust | 分解，组稀疏学习 | $\ell_{2,1}$ 范数 + $\ell_{1,2}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda_{1}\sum_{d=1}^{D}\&#124;\boldsymbol{p}_{d}\&#124;_{2}+\lambda_{2}\sqrt{\sum_{d=1}^{D}\&#124;\boldsymbol{q}_{d}\&#124;_{1}^{2}}$，满足 $\boldsymbol{W}=\boldsymbol{P}+\boldsymbol{Q}$ |
| 多阶段多任务特征学习 | NeurIPS | \citeyeargong2012multi | 稀疏学习 | 限制的 $\ell_{1}$ 范数 \citepzhang2010analysis | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{d=1}^{R}\min\{\&#124;\boldsymbol{w}_{d}\&#124;_{1},\tau\}$ |
| MTL 的凸优化形式 | IJCAI | \citeyearzhang2012convex | 先验共享 | 聚类惩罚 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda_{1}}{2}$tr$(\boldsymbol{W}\boldsymbol{W}^{T})+\frac{\lambda_{2}}{2}$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})$ 约束条件 $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$, tr$\boldsymbol{\Omega}=1$ |
| 多线性多任务学习 | ICML | \citeyearromera2013multilinear | 低秩学习 | 重叠张量迹范数 | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{k=1}^{N}\&#124;\boldsymbol{W}_{(k)}\&#124;_{*}$ 其中 $\boldsymbol{W}_{(k)}$ 是张量 $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times I_{2}\times\cdots\times I_{N}}$ 的模式-$k$ 展开。 |
| 学习 MTL 的正则化方法 | TKDD | \citeyearzhang2014regularization | 先验共享 | 聚类惩罚 + $\ell_{2,2}$ 范数 | $\min\limits_{\boldsymbol{V},\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\frac{\lambda}{2}\sum_{t=1}^{T}&#124;&#124;\boldsymbol{w}^{t}&#124;&#124;_{2}^{2}+$tr$(\boldsymbol{W}\boldsymbol{\Omega}^{-1}\boldsymbol{W}^{T})+d$ln$\boldsymbol{\Omega}$ 约束条件 $\boldsymbol{\Omega}\in\boldsymbol{S}_{+}^{D}$ |
| 多线性多任务学习 | NeurIPS | \citeyearwimalawarne2014multitask | 低秩学习 | 缩放的潜在张量迹范数 | $\min\limits_{\boldsymbol{\mathcal{W}}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\inf_{\boldsymbol{\mathcal{W}}^{(1)}+\cdots+\boldsymbol{\mathcal{W}}^{(N)}=\boldsymbol{\mathcal{W}}}\lambda\sum_{k=1}^{N}I_{k}^{-1/2}\&#124;\boldsymbol{W}_{(k)}^{(k)}\&#124;_{*}$ 其中 $\boldsymbol{\mathcal{W}}\in\mathbb{R}^{D\times I_{2}\times\cdots\times I_{N}}$ 是一个张量。 |
| 任务树模型 | KDD | \citeyearhan2015learning | 任务聚类 | $\ell_{2,2}$ 范数 | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\sum_{h=1}^{H}\boldsymbol{w}_{h}^{t}-\boldsymbol{y}^{t}\&#124;_{2}^{2}+\sum_{h=1}^{H}\lambda_{h}\sum_{i<j}^{T}\&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;^{2}_{2}, \text{s.t.} \&#124;\boldsymbol{w}_{h-1}^{i}-\boldsymbol{w}_{h-1}^{j}\&#124;\succeq \&#124;\boldsymbol{w}_{h}^{i}-\boldsymbol{w}_{h}^{j}\&#124;,\forall h\geq 2,\forall i<j$ |
| 降秩多阶段 MTL | AAAI | \citeyearhan2016multi | 低秩学习 | 截断迹范数 \citepsun2013robust | $\min\limits_{\boldsymbol{W}}\frac{1}{2}\sum_{t=1}^{T}\frac{1}{N_{t}}\&#124;{\boldsymbol{X}^{(t)}}\boldsymbol{w}^{t}-\boldsymbol{y}^{(t)}\&#124;^{2}_{2}+\lambda\sum_{r=1}^{R}\min\{\sigma_{r}(\boldsymbol{W}),\tau\}$ |

+   1

    这项工作发表于《技术报告》，统计学系，加州大学伯克利分校。

+   2

    这项工作发表于简·张的博士论文，CMU 技术报告 CMU-LTI-06-006，2006 年。

## 2.1.1\. 特征选择

高维缩放\citepnegahban2008joint，其中模型权重的数量远大于观测值/特征的数量，即$D\gg N$，在许多现实世界的问题中出现，使得寻求有效预测变量变得昂贵且艰难。稀疏学习使用$\ell_{1}$正则化器，旨在识别由减少的非零元素数量特征的结构。这种简约的解决方案确保了保留和选择最有效和高效的特征子集，以适应目标任务\citeptibshirani1996regression。在 MTL 中，假设 LABEL:assump:parameter 支撑了所有稀疏学习模型的发展。在稀疏学习的设置下，该假设认为模型参数中的相似稀疏模式暗示了任务之间的相关性。因此，稀疏模式微妙地表示任务相关性，突显了从这些有限样本中得出的共同特征子集。采用稀疏性在 MTL 中的更多好处和效果已被充分评估和讨论

转换为 HTML 时发生致命错误并突然退出。此文档可能被截断或损坏。
