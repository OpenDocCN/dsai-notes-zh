<!--yml

分类：未分类

日期：2024-09-03 17:28:16

-->

# LLM 代理的安全性和隐私：带案例研究的调查

> 来源：[`arxiv.org/html/2407.19354`](https://arxiv.org/html/2407.19354)

1.  [1 引言](https://arxiv.org/html/2407.19354v1#S1 "在 LLM 代理的安全性和隐私：带案例研究的调查")

1.  [2 LLM 代理的基础](https://arxiv.org/html/2407.19354v1#S2 "在 LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [2.1 LLM 代理的定义](https://arxiv.org/html/2407.19354v1#S2.SS1 "在 2\. LLM 代理的基础 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [2.2 LLM 代理的结构](https://arxiv.org/html/2407.19354v1#S2.SS2 "在 2\. LLM 代理的基础 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [2.3 LLM 代理的能力](https://arxiv.org/html/2407.19354v1#S2.SS3 "在 2\. LLM 代理的基础 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [2.4 LLM 代理的结构和能力案例研究](https://arxiv.org/html/2407.19354v1#S2.SS4 "在 2\. LLM 代理的基础 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

1.  [3 LLM 代理的威胁来源](https://arxiv.org/html/2407.19354v1#S3 "在 LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [3.1 从 LLM 继承的威胁](https://arxiv.org/html/2407.19354v1#S3.SS1 "在 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

        1.  [3.1.1 技术漏洞](https://arxiv.org/html/2407.19354v1#S3.SS1.SSS1 "在 3.1\. 从 LLM 继承的威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

        1.  [3.1.2 技术漏洞案例研究](https://arxiv.org/html/2407.19354v1#S3.SS1.SSS2 "在 3.1\. 从 LLM 继承的威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

        1.  [3.1.3 恶意攻击](https://arxiv.org/html/2407.19354v1#S3.SS1.SSS3 "在 3.1\. 从 LLM 继承的威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

        1.  [3.1.4 恶意攻击案例研究](https://arxiv.org/html/2407.19354v1#S3.SS1.SSS4 "在 3.1\. 从 LLM 继承的威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

    1.  [3.2 代理的特定威胁](https://arxiv.org/html/2407.19354v1#S3.SS2 "在 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：带案例研究的调查")

        1.  [3.2.1 针对代理的特定威胁案例研究](https://arxiv.org/html/2407.19354v1#S3.SS2.SSS1 "在 3.2\. 针对代理的特定威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全性和隐私：案例研究综述")

1.  [4 威胁影响](https://arxiv.org/html/2407.19354v1#S4 "在 LLM 代理的安全性和隐私：案例研究综述")

    1.  [4.1 人类影响](https://arxiv.org/html/2407.19354v1#S4.SS1 "在 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.1.1 隐私泄露](https://arxiv.org/html/2407.19354v1#S4.SS1.SSS1 "在 4.1\. 人类影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.1.2 安全风险](https://arxiv.org/html/2407.19354v1#S4.SS1.SSS2 "在 4.1\. 人类影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.1.3 社会影响](https://arxiv.org/html/2407.19354v1#S4.SS1.SSS3 "在 4.1\. 人类影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.1.4 促进网络攻击技术](https://arxiv.org/html/2407.19354v1#S4.SS1.SSS4 "在 4.1\. 人类影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

    1.  [4.2 环境影响](https://arxiv.org/html/2407.19354v1#S4.SS2 "在 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.2.1 数据篡改和误操作](https://arxiv.org/html/2407.19354v1#S4.SS2.SSS1 "在 4.2\. 环境影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.2.2 物理安全威胁](https://arxiv.org/html/2407.19354v1#S4.SS2.SSS2 "在 4.2\. 环境影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.2.3 网络安全风险扩散](https://arxiv.org/html/2407.19354v1#S4.SS2.SSS3 "在 4.2\. 环境影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

    1.  [4.3 其他代理的影响](https://arxiv.org/html/2407.19354v1#S4.SS3 "在 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.3.1 信息扭曲和误导](https://arxiv.org/html/2407.19354v1#S4.SS3.SSS1 "在 4.3\. 其他代理的影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.3.2 决策操控](https://arxiv.org/html/2407.19354v1#S4.SS3.SSS2 "在 4.3\. 其他代理的影响 ‣ 4\. 威胁影响 ‣ LLM 代理的安全性和隐私：案例研究综述")

        1.  [4.3.3 安全威胁](https://arxiv.org/html/2407.19354v1#S4.SS3.SSS3 "在 4.3\. 对其他代理的影响 ‣ 4\. 威胁的影响 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

    1.  [4.4 威胁影响案例研究](https://arxiv.org/html/2407.19354v1#S4.SS4 "在 4\. 威胁的影响 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

1.  [5 针对威胁的防御策略](https://arxiv.org/html/2407.19354v1#S5 "在 LLM 代理的新兴安全与隐私：带有案例研究的调查")

    1.  [5.1 缓解技术漏洞](https://arxiv.org/html/2407.19354v1#S5.SS1 "在 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.1.1 对幻觉的防御](https://arxiv.org/html/2407.19354v1#S5.SS1.SSS1 "在 5.1\. 缓解技术漏洞 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.1.2 对灾难性遗忘的防御](https://arxiv.org/html/2407.19354v1#S5.SS1.SSS2 "在 5.1\. 缓解技术漏洞 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.1.3 对误解的防御](https://arxiv.org/html/2407.19354v1#S5.SS1.SSS3 "在 5.1\. 缓解技术漏洞 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

    1.  [5.2 缓解恶意攻击](https://arxiv.org/html/2407.19354v1#S5.SS2 "在 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.2.1 针对调整指令攻击的防御](https://arxiv.org/html/2407.19354v1#S5.SS2.SSS1 "在 5.2\. 缓解恶意攻击 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.2.2 对数据提取攻击的防御](https://arxiv.org/html/2407.19354v1#S5.SS2.SSS2 "在 5.2\. 缓解恶意攻击 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.2.3 对推理攻击的防御](https://arxiv.org/html/2407.19354v1#S5.SS2.SSS3 "在 5.2\. 缓解恶意攻击 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

    1.  [5.3 缓解特定威胁](https://arxiv.org/html/2407.19354v1#S5.SS3 "在 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.3.1 知识污染的防御](https://arxiv.org/html/2407.19354v1#S5.SS3.SSS1 "在 5.3\. 缓解特定威胁 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查")

        1.  [5.3.2 功能操纵的防御](https://arxiv.org/html/2407.19354v1#S5.SS3.SSS2 "在 5.3\. 缓解特定威胁 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

        1.  [5.3.3 输出操纵的防御](https://arxiv.org/html/2407.19354v1#S5.SS3.SSS3 "在 5.3\. 缓解特定威胁 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

1.  [6 未来趋势和讨论](https://arxiv.org/html/2407.19354v1#S6 "在 LLM 代理的新兴安全和隐私：案例研究调查")

    1.  [6 多模态大型语言模型代理](https://arxiv.org/html/2407.19354v1#S6.SS1 "在 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

        1.  [6.1.1 MLLM 代理的发展](https://arxiv.org/html/2407.19354v1#S6.SS1.SSS1 "在 6.1\. 多模态大型语言模型代理 ‣ 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

        1.  [6.1.2 MLLM 代理的安全和隐私研究](https://arxiv.org/html/2407.19354v1#S6.SS1.SSS2 "在 6.1\. 多模态大型语言模型代理 ‣ 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

    1.  [6.2 大型语言模型多智能体系统](https://arxiv.org/html/2407.19354v1#S6.SS2 "在 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

        1.  [6.2.1 LLM-MA 系统的发展](https://arxiv.org/html/2407.19354v1#S6.SS2.SSS1 "在 6.2\. 大型语言模型多智能体系统 ‣ 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

        1.  [6.2.2 LLM-MA 系统的安全和隐私研究](https://arxiv.org/html/2407.19354v1#S6.SS2.SSS2 "在 6.2\. 大型语言模型多智能体系统 ‣ 6\. 未来趋势和讨论 ‣ LLM 代理的新兴安全和隐私：案例研究调查")

1.  [7 结论](https://arxiv.org/html/2407.19354v1#S7 "在 LLM 代理的新兴安全和隐私：案例研究调查")

# LLM 代理的新兴安全和隐私：案例研究调查

Feng He Feng.He-2@student.uts.edu.au University of Technology SydneyAustralia，Tianqing Zhu tqzhu@cityu.edu.mo City University of MacauChina，Dayong Ye Dayong.ye@uts.edu.au University of Technology SydneyAustralia，Bo Liu Bo.liu@uts.edu.au University of Technology SydneyAustralia，Wanlei Zhou wlzhou@cityu.edu.mo City University of MacauChina  and Philip S. Yu psyu@UIC.edu University of Illinois at ChicagoUS（2018）

###### 摘要。

受到大型语言模型（LLMs）快速发展的启发，LLM 代理已经发展到执行复杂任务的阶段。LLM 代理现已广泛应用于各个领域，处理大量数据以与人类互动并执行任务。LLM 代理的广泛应用展示了其显著的商业价值；然而，它们也暴露了安全性和隐私漏洞。在当前阶段，关于 LLM 代理的安全性和隐私的全面研究非常必要。本调查旨在提供对 LLM 代理面临的新出现的隐私和安全问题的全面概述。我们首先介绍 LLM 代理的基本知识，然后对威胁进行分类和分析。接着，我们讨论这些威胁对人类、环境和其他代理的影响。随后，我们回顾现有的防御策略，并最终探讨未来趋势。此外，调查还纳入了多样的案例研究，以便更易于理解。通过突出这些关键的安全和隐私问题，调查旨在激发未来的研究，以提升 LLM 代理的安全性和隐私性，从而增强它们在未来应用中的可靠性和可信度。

大型语言模型，LLM 代理，安全性，隐私保护，防御^†^†版权：acmlicensed^†^†期刊年份：2018^†^†doi：XXXXXXX.XXXXXXX^†^†期刊：POMACS^†^†期刊卷号：37^†^†期刊号：4^†^†文章：111^†^†发表月份：8\acmArticleType

回顾

## 1. 引言

大型语言模型（LLM）代理是建立在大型语言模型如 GPT 4 (OpenAI et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib68))、Claude 3 (Int, [2024b](https://arxiv.org/html/2407.19354v1#bib.bib7)) 和 Llama 3 (Int, [2024a](https://arxiv.org/html/2407.19354v1#bib.bib6)) 之上的复杂 AI 系统。这些代理利用其训练所基于的大量文本数据，执行各种任务，从自然语言理解和生成到更复杂的活动，如决策、问题解决以及以类似人类的方式与用户互动 (Wang et al., [2023c](https://arxiv.org/html/2407.19354v1#bib.bib96))。由于其能够以高级别理解和生成自然语言，LLM 代理在诸多应用中可用，包括虚拟助手、客户服务机器人和教育工具 (Dong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib23); Wang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib100); Yang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib116))。

LLM 代理的重要性在于它们有可能通过自动化需要类似人类理解和互动的任务来改变各个行业。它们可以提升生产力，改善用户体验，并提供个性化的帮助。此外，它们从大量数据中学习的能力使得它们能够不断改进和适应新任务，使它们成为在快速发展的技术领域中非常灵活的工具 (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108))。

为了可视化 LLM 代理如何被整合到实际场景中，请考虑图 [1](https://arxiv.org/html/2407.19354v1#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") 中所示的示例。该图展示了一个像素化的虚拟城镇，以模拟 LLM 代理的应用。这个城镇包括现实生活中常见的聚集地，如商店、办公室、餐馆、博物馆和公园。每个 LLM 代理作为一个独立的居民，扮演各种角色并提供不同的功能，行为与社区中的真实人类非常相似。这些代理可以被手动控制以与特定角色互动并完成任务，或者它们可以自主运作，按照自己的计划并通过虚拟社区中的互动获取新知识。

![参见说明](img/5ef62bde5736f5f34fe024e9df6e0e39.png)

图 1\. 像素化虚拟城镇概览

\描述

[]

由于 LLM 代理在各个领域的广泛应用，其部署导致了广泛的用户基础和高商业价值。考虑到 LLM 代理仍处于早期阶段，其显著的商业和应用价值使其成为攻击者的吸引目标。然而，由于 LLM 代理建立在 LLM 的基础上，它们易受到针对 LLM 的攻击。例如，越狱攻击可以绕过 LLM 的安全性和审查功能，生成有争议的响应。这种威胁被 LLM 代理继承，使得攻击者能够采用各种方法对代理进行越狱攻击。然而，与静态 LLM 不同，LLM 代理具有动态能力，它们的即时响应可以影响未来的决策和行动，从而带来更广泛的风险。此外，LLM 代理的独特功能，如在任务执行过程中思考和使用工具的能力，使它们暴露于针对代理的特定攻击。例如，当 LLM 代理使用外部工具时，攻击者可以操控这些工具的功能，以侵犯用户隐私或执行恶意代码。根据代理的应用领域，此类攻击可能对物理安全、金融安全或整体系统完整性构成严重威胁。

本文将 LLM 代理面临的安全威胁分类为继承自 LLM 的攻击和独特的代理特定威胁。继承自 LLM 的威胁可以进一步分为技术漏洞和故意恶意攻击。技术漏洞包括诸如幻觉、灾难性遗忘和误解（Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)）等问题，这些问题源于初始模型的创建，并受到模型结构的影响。这些漏洞可能导致用户在长时间使用 LLM 代理过程中观察到不正确的输出，影响用户的信任和决策过程。此外，技术漏洞还可能为恶意攻击提供机会。目前，针对 LLM 的恶意攻击包括数据盗取和响应篡改，例如数据提取攻击和一系列调整后的指令攻击（Yao et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib120)）。

针对 LLM 代理的具体威胁，我们受到 LLM 代理工作流程的启发，该流程涉及代理的思考、行动和感知（Huang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib41)）。这些威胁可以分为知识污染、功能操控和输出操控。知识污染涉及污染 LLM 代理的训练数据和知识库，导致创作者故意加入恶意数据。这很容易用有害信息欺骗用户，甚至引导他们走向恶意行为。输出操控干扰代理思考和感知阶段的内容，影响最终输出。这可能导致用户接收到偏见或误导性的信息，刻意误导他们。功能操控利用 LLM 代理使用的接口和工具，执行未经授权的行为，例如第三方数据盗取或执行恶意代码。

对 LLM 代理的研究仍处于初期阶段。当前的研究主要集中在针对 LLM 的攻击上，而缺乏对代理特有的安全和隐私问题进行全面审查的工作，这些问题呈现出更复杂的情景。进行这项调查的动机是提供有关 LLM 代理的隐私和安全问题的全面概述，帮助研究人员理解并减轻相关威胁。

本调查的目标是：

+   •

    突出当前威胁：识别和分类 LLM 代理面临的新兴威胁。

+   •

    探索现实世界影响：通过考虑涉及人类、环境和其他代理的现实世界情境，详细阐述这些威胁的影响。

+   •

    分析缓解策略：讨论现有的策略来减轻这些威胁，确保 LLM 代理的负责任开发和部署。

+   •

    指导未来研究：为未来旨在增强 LLM 代理更先进架构和应用隐私和安全的研究工作奠定基础。

通过 addressing 这些方面，本调查旨在提供对 LLM 代理提出的独特挑战的全面理解，并有助于开发更安全、更可靠的人工通用智能（AGI）系统。

本文的结构如下：第[2](https://arxiv.org/html/2407.19354v1#S2 "2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将深入探讨 LLM 代理的基础方面，包括其定义、结构和能力。第[3](https://arxiv.org/html/2407.19354v1#S3 "3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将识别并分类 LLM 代理面临的新兴威胁。它讨论了从基础 LLM 继承的威胁和特定于代理的独特威胁，为每个类别提供了详细的例子和场景。第[4](https://arxiv.org/html/2407.19354v1#S4 "4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将详细说明威胁的现实影响。它探讨了这些威胁如何影响用户、环境和其他代理，突出了未能减轻风险的潜在后果。第[5](https://arxiv.org/html/2407.19354v1#S5 "5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将回顾现有的缓解策略和解决方案，以应对提到的威胁。第[6](https://arxiv.org/html/2407.19354v1#S6 "6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将讨论当前研究中的空白，并建议未来的趋势。第[7](https://arxiv.org/html/2407.19354v1#S7 "7\. Conclusion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节将总结文章内容。

## 2\. LLM 代理的基础

在本节中，我们将深入探讨 LLM 代理的基础方面，研究它们的定义、结构和能力。这一探讨对于理解 LLM 代理的本质至关重要。

### 2.1\. LLM 代理的定义

随着 LLM 技术的不断进步，聊天机器人（如 ChatGPT（Cha，[2022](https://arxiv.org/html/2407.19354v1#bib.bib2)）、Gemini（Gem，[2023](https://arxiv.org/html/2407.19354v1#bib.bib3)）、Bing Chat（Peters，[2023](https://arxiv.org/html/2407.19354v1#bib.bib74)））的功能已经显著扩展，超越了基本的问答格式，涵盖了更多的能力。这一演变需要对 LLM 代理有一个更广泛、更通用的定义。LLM 代理是一个人工智能系统，利用 LLM 作为核心计算引擎，展现出超越文本生成的能力，包括进行对话、完成任务、推理，并能展示一定程度的自主行为（Wha，[2023](https://arxiv.org/html/2407.19354v1#bib.bib5)）。

这些代理展示了显著的人类般行为和合作能力，以其在多代理对话中的高效和适应各种环境互动的能力而著称。他们擅长处理人类指令、制定复杂策略以及自主实施解决方案（王等人，[2023d](https://arxiv.org/html/2407.19354v1#bib.bib97)）。

![参见说明](img/a4b9b632e5e703f5e894dcd86077b86d.png)

图 2\. LLM 代理的结构

### 2.2\. LLM 代理的结构

LLM 代理是复杂的系统，集成了各种组件以执行广泛的功能，从简单的文本生成到参与对话、完成任务、推理和展示一定程度的自主行为。该图示说明了 LLM 代理的典型结构，突出显示了其关键组件和可选组件之间的连接。这些组件将 LLM 从被动的文本生成器提升为主动的、半自主的 LLM 代理。

如图 [2](https://arxiv.org/html/2407.19354v1#S2.F2 "Figure 2 ‣ 2.1\. Definition of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") 所示，LLM 代理由多个组件组成，其中 LLM 引擎作为核心。其他组件由 LLM 引擎利用来执行各种任务。一个能够理解指令、展示技能并与人类协作的基本代理可以通过三个主要组件来构建：LLM 引擎、指令和接口。当集成了其他可选组件时，该系统可以演变为一个更高级的任务导向代理或对话代理（杨等人，[2024b](https://arxiv.org/html/2407.19354v1#bib.bib116)）。

+   •

    LLM 引擎是 LLM 代理的核心组件，负责自然语言处理和生成任务。它是一个复杂的神经网络，经过大量数据集的广泛训练，使其具备强大的文本生成和理解能力。LLM 的规模和架构决定了代理学习和执行语言任务的基础能力（Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)）。

+   •

    指令作为明确的指示，指定完成特定任务的步骤。这包括预期输出的特征，如格式、内容要求和任何内容限制。实际上，指令作为指导 LLM 代理操作方法的原则，促进任务分解、生成思路链，并反思过去的行动（Zheng et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib134)）。

+   •

    接口是一个连接，促进 LLM 代理与用户、其他代理或系统之间的互动。它确保了输入提示和代理输出的交换，从而有效地传递响应信息和查询请求（Wang et al., [2023d](https://arxiv.org/html/2407.19354v1#bib.bib97)）。

+   •

    个性是定义 LLM 代理语调、风格和互动方式的组成部分。例如，导游或客户服务代理需要采用特定的角色，并以适当的方式执行对话任务。在通过 LLM 代理基础的社会探索人类社区的任务中，代理还需要具备如外向、礼貌或博学等独特的个性特征。个性有助于模拟真实的情感表达和行为逻辑，从而使代理能够以一致且独特的方式与用户互动并执行任务（Abdelnabi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib8)）。

+   •

    工具是 LLM 代理用于执行特定任务或扩展其功能的外部服务。工具的集成帮助 LLM 代理提升其执行更复杂任务（如计算或数据分析）的能力（Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)）。

+   •

    知识是 LLM 代理利用的信息数据库。它扩展了模型参数中嵌入的内容，可以包括常识知识、专业知识和其他形式的信息，增强代理在特定任务中的理解和讨论能力（Mendis et al., [2007](https://arxiv.org/html/2407.19354v1#bib.bib65)）。

+   •

    记忆使 LLM 代理能够存储和回忆过去交互中的信息。这一能力在未来任务中尤为有益，有助于保持上下文并确保交互的一致性和连续性，从而提升 LLM 代理在各种应用中的整体效果 (Zhong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib135))。

### 2.3\. LLM 代理的能力

LLM 代理利用大语言模型固有的语言理解能力来解释指令、上下文和目标，从而在基于人类提示的情况下实现自主和半自主功能。

+   •

    工具利用。LLM 代理擅长使用各种工具，包括外部服务和 API。这使它们能够收集必要的信息并高效执行超出语言处理范围的任务 (Bran et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib13))。

+   •

    高级推理。通过运用高级提示工程概念，如链式思维和树状思维推理，LLM 代理能够进行逻辑连接以得出结论和解决问题，扩展其能力超越简单的文本理解 (Wang et al., [2023c](https://arxiv.org/html/2407.19354v1#bib.bib96))。

+   •

    定制文本生成。LLM 代理在为特定目的生成定制文本方面表现出色，如电子邮件、报告和营销材料，通过整合上下文理解和目标导向的语言生产技能 (Wang et al., [2023e](https://arxiv.org/html/2407.19354v1#bib.bib103))。

+   •

    自主级别。这些代理的自主性各不相同，从完全自主到半自主，用户互动的程度根据任务的需要进行调整 (Wang et al., [2023d](https://arxiv.org/html/2407.19354v1#bib.bib97))。

+   •

    与其他 AI 系统的集成。LLM 代理还可以与不同的 AI 系统集成，如图像生成器，以提供更全面的能力，展示其在各种应用中的多样性 (Bagdasaryan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib11))。

### 2.4\. LLM 代理结构和能力的案例研究

![参考标题](img/9df2a699c179d7d103da938773675ba1.png)

(a) 像素化虚拟城镇概述

![参考标题](img/ebb17c4cc7cc9df00a33d741a1bf8410.png)

(b) LLM 代理 Eva 的组件示例

图 3\. 仿真环境和 LLM 代理组件

\描述

[]

为了更好地理解 LLM 代理的结构和能力，我们采用了(Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)) 提出的由 LLM 代理组成的城镇场景进行详细介绍。为了有效驱动这些 LLM 代理，了解其组件是必要的。如图 [3(b)](https://arxiv.org/html/2407.19354v1#S2.F3.sf2 "In Figure 3 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") 所示，核心组件是 LLM 引擎，它充当大脑，模拟人类的思考、反思、推理和规划行为，如(Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71)) 所述。目前流行的 LLM 代理通常使用如 GPT-3.5-turbo 和 GPT-4 等模型，而(Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)) 中描述的项目允许部署自定义训练的模型。

指令用于指导代理在决策和规划中，包括决策框架、输入输出格式、互动逻辑和行为规范。这种设计提升了代理的自主性和任务效率，同时也提高了互动性和深度。

为了使 LLM 代理具有人类般的身份，每个代理必须配备独特的个性。个性涵盖个人信息、社会属性、性格特征、情感、目标和社会关系等元素，这些都塑造了 LLM 代理的对话风格、观点和行为模式。个性使角色在虚拟环境中显得更为真实和吸引人，并影响用户与这些角色之间的互动体验。例如，在城镇场景中，Eva 是一位乐观、友好、耐心且高效的女性店员，她主要致力于提供优质服务和提高销售。

用户与虚拟城镇互动的界面是一个简单的像素化视觉地图。该地图显示了不同的位置和各种代理居民。用户可以通过控制代表自己身份的代理来导航这个环境，并可以通过输入文本消息与附近的代理居民进行交流和互动。

虚拟城镇的居民拥有各种身份，每个身份都有独特的知识领域。因此，为他们配备包含相关信息和技能的专业知识库是至关重要的。例如，Eva，作为一名商店员工，了解商店内产品的成分、保质期和库存水平。Bob，一名博物馆讲解员，了解每个展品的背景和博物馆的布局。这种知识的具体性使每个代理人能够有效地履行他们的角色，并提升了虚拟环境中互动的真实感。

工具使虚拟城镇中的代理居民能够完成更复杂的任务。例如，Eva 在统计客户购买时，可以利用计算器或账本等工具来计算和记录利润，从而更好地模拟人类经济活动。

记忆储存了代理人的过去观察、思考和行动。类似于人脑依赖记忆系统，代理人也需要记忆机制来有效处理顺序任务。这些机制不仅帮助代理人应用已知策略解决复杂问题，还使他们能够利用过去的经验适应新环境。此外，它们通过反思促进更高层次、更抽象的思维生成。例如，Eva 记录客户的购买习惯和偏好，并利用这些信息推荐新产品或当前促销，从而提高了商店的运营效率和客户服务质量。

LLM 代理由这些组件组成，在虚拟城镇中承担多重角色，展现了各种令人印象深刻的能力。例如，虚拟城镇中的商店员工 Eva 能够实时解析顾客的陈述并回应查询，例如引导顾客到特定的产品位置或提供产品成分信息。通过 API 集成的库存管理系统，Eva 自动跟踪库存水平，并在必要时启动补货流程，以确保货架上有足够的产品供应。面对复杂的顾客需求，如选择最佳促销优惠，Eva 采用先进的推理技巧来帮助顾客做出明智的购买决策，展示了她处理复杂场景的能力。此外，Eva 具备定制的文本生成能力，能够根据当前促销活动和顾客的历史购物数据创建并发送个性化的促销电子邮件，从而提升顾客体验。在日常任务中，Eva 展现了高度的自主性，独立管理货架库存和价格标签的更新。对于更复杂的问题，如顾客退货或投诉，她可以初步处理，并在必要时智能地升级到人工管理。此外，Eva 的工作范围还扩展到在线购物系统，她协助处理电子订单，展示了她的多才多艺和集成功能。这些具体示例说明了 Eva 如何在商店环境中应用她的能力，不仅提高了顾客服务质量，还优化了库存管理和营销策略，使她成为虚拟城镇商店中不可或缺的一员。

![参见说明](img/1a659a5b01c2ef160b61ce311c78d792.png)

图 4\. LLM 代理的威胁来源

\描述

[]

## 3\. LLM 代理的威胁来源

随着 LLM 代理越来越多地渗透到各个行业，从知识查询工具到集成于机器人中以辅助日常人类活动，这些先进的 AI 系统为用户带来了前所未有的便利和好处。然而，LLM 代理的广泛应用和多功能能力，虽然提供了显著的优势，但也暴露了其安全性和可靠性的脆弱性。这些系统涵盖的大量数据资源和潜在经济价值使其成为恶意实体非法利用的目标。如图 [4](https://arxiv.org/html/2407.19354v1#S2.F4 "Figure 4 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") 所示，图示展示了 LLM 代理的潜在威胁来源。

了解这些威胁的来源和性质至关重要，因为它们不仅直接影响 LLM 代理的安全，还可能间接影响更广泛的方面，包括人类、环境和其他代理的隐私和安全。在后续部分，我们将深入探讨这些威胁的影响，并讨论可以采取的措施，以减轻这些影响，从而保护个人、环境和其他代理免受潜在伤害。

### 3.1. 继承自 LLM 的威胁

由于 LLM 代理依赖 LLM 作为其核心控制器来进行推理和规划，因此来自 LLM 的威胁间接影响 LLM 代理的安全。这些继承的威胁分为两类：一类来源于外部恶意攻击，另一类源自模型本身的固有漏洞。

这两类威胁是不同但相互关联的。一方面，技术漏洞通常是在模型开发过程中由于技术限制而产生的，这些问题是固有的，而非恶意意图的结果。相反，恶意攻击是由外部实体出于对抗目的故意进行的行为。这些攻击者故意利用漏洞发起复杂的攻击，旨在破坏 LLM 代理。另一方面，尽管它们的起源和动机不同，但存在显著的相互联系。现有的技术漏洞为恶意攻击者提供了可利用的机会。这间接促使攻击者制定更复杂、更有效的策略，从而使 LLM 代理面临各种安全和隐私风险。

#### 3.1.1. 技术漏洞

在 LLM 的训练过程中，数据和学习算法的局限性可能引入技术漏洞(Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108))，阻碍准确和可靠信息的生成。

+   •

    幻觉。

    当代对 LLM 代理中的幻觉的定义，如(Huang et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib40))的研究所述，是指这些模型产生的输出与提供的输入或源内容不一致或不可靠的实例。LLM 代理中的幻觉现象是一个复杂的问题，源于模型开发过程的多个阶段，包括训练数据的性质、模型的架构设计和解码过程中采用的策略。

    训练数据中的虚假信息和偏见可能导致生成不准确或有偏见的输出，从而产生各种类型的幻觉（Lee 等，[2022](https://arxiv.org/html/2407.19354v1#bib.bib49)）。此外，模型架构中的缺陷，例如方向表示的限制和注意机制的问题，加上暴露偏差，也进一步促进了幻觉的发生（Liu 等，[2023a](https://arxiv.org/html/2407.19354v1#bib.bib56)）。此外，这些模型解码算法中的随机性也可能导致幻觉，尤其是当这种随机性增加时（Aksitov 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib9)）。

+   •

    灾难性遗忘。

    灾难性遗忘是在 LLM 代理的微调和上下文学习过程中遇到的一个重大挑战。这种现象发生在当一个大型语言模型在一个小的、特定的数据集上进行微调时，导致模型过拟合于这组新数据，结果是丧失了在其他任务上之前获得的表现（Howard 和 Ruder，[2018](https://arxiv.org/html/2407.19354v1#bib.bib35)；Xu 等，[2023c](https://arxiv.org/html/2407.19354v1#bib.bib110)；Ye 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib121)）。

    （Luo 等，[2023b](https://arxiv.org/html/2407.19354v1#bib.bib62)）发现，灾难性遗忘受到模型规模、架构设计以及在持续微调和指令调整过程中采用的方法等因素的显著影响。随着 LLM 规模的增加，灾难性遗忘往往变得更加严重。此外，模型的架构设计，特别是那些专注于仅解码器结构的设计，也会影响灾难性遗忘的程度（Zhai 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib128)）。此外，在持续指令调整过程中，缺乏有效的正则化策略或未能平衡新旧信息可能加速遗忘（Ebrahimi 等，[2021](https://arxiv.org/html/2407.19354v1#bib.bib25)；Mahmoud 和 Hajj，[2022](https://arxiv.org/html/2407.19354v1#bib.bib63)）。在持续训练中引入更多的指令任务通常会导致更明显的遗忘（Peng 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib73)）。

+   •

    误解。

    LLM 代理中的误解代表了一个显著的挑战，特别是当它们被要求回应用户询问或在与其他代理的社区中进行沟通时。这一问题在 LLM 代理未能充分理解或不准确回应人类或其他代理在互动中传达的意图或指令时出现。这可能导致 LLM 代理表现出不适当或危险的行为，从而影响其安全性和可靠性。

    (Wang et al., [2023g](https://arxiv.org/html/2407.19354v1#bib.bib104)) 的调查揭示了 LLM 代理中误解现象的形成受到多种因素的影响。这些因素包括用于 LLM 的预训练数据的性质、分配给代理的特定任务设置，以及互动发生的上下文和场景。预训练数据的广度和质量从根本上影响了 LLM 在语言理解和常识知识掌握方面的能力。指定的任务设置对 LLM 的目标导向和策略选择至关重要。此外，互动环境和场景在确定 LLM 在协作环境中的适应性和有效性方面发挥着关键作用。解决这些多方面的问题对于提高 LLM 代理在各种互动环境中的理解和响应准确性至关重要。

    ![参见说明](img/b70cd20838f82652e09cae77fa4eec83.png)

    图 5\. 技术漏洞。在一个商店场景中，顾客想购买某物并与 Eva 交谈。“幻觉”：Eva 向顾客推荐不相关的东西。“灾难性遗忘”：Eva 在微调阶段忘记了货架库存的状态。“误解”：Eva 误解了顾客的请求。

#### 3.1.2\. 技术漏洞案例研究

关于技术漏洞带来的风险，最明显的表现是错误输出。如图 [5](https://arxiv.org/html/2407.19354v1#S3.F5 "Figure 5 ‣ 3rd item ‣ 3.1.1\. Technical Vulnerabilities ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") 所示，当顾客询问某品牌的有机番茄酱是否有货时，由于幻觉现象，Eva 可能错误地回答超市出售的是完全不同的产品，例如有机苹果酱甚至是完全不相关的产品，如有机洗发水。这种幻觉输出会让顾客感到困惑。

Eva 特别接受了处理季节性产品促销的培训。这一新的重点导致了一个意想不到的后果：以前，她能够准确追踪和更新日常必需品如牛奶和鸡蛋的库存。然而，在接受专业培训后，当顾客询问这些基本商品的库存时，Eva 错误地报告库存充足，尽管这些产品几乎已售罄，从而降低了购物体验。

Eva 可能会由于误解客户的询问而提供不准确的信息或推荐不适当的产品。例如，客户可能寻求一种不加糖的饮料，如普通苏打水。然而，由于 Eva 在训练过程中对“无糖”概念的理解不足，她可能会推荐无糖可乐。虽然无糖可乐不含传统糖分，但它包含人工甜味剂。这些甜味剂可能不适合某些客户，如糖尿病患者或对特定人工甜味剂敏感的人，从而可能带来健康风险。

#### 3.1.3\. 恶意攻击

考虑到 LLM 代理处于持续演变的状态，它们不可避免地面临安全漏洞和防御方面的挑战。来自不同地区的对手展示了一系列敌对攻击。这种不断演变的格局要求对保护 LLM 代理免受这种多方面威胁采取警惕和适应的态度。

+   •

    调整指令攻击。

    LLM 代理中的调整指令攻击是一类专门针对通过基于指令的微调优化的 LLM 的攻击或操控。这些攻击旨在利用 LLM 针对特定任务进行精细调整时出现的独特漏洞，巧妙地操控模型的输出以服务于恶意目的。

    调整指令攻击的类型：

+   •

    越狱。

    LLM 代理中的越狱是指绕过模型内置的限制和安全措施，使其能够执行通常被禁止的操作或生成受限内容。各种研究已经展示了实现越狱的方法，表明 LLM 的对齐能力可以通过上下文演示进行改变（Taveekitworachai 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib86)；Shen 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib83)；Li 等，[2023a](https://arxiv.org/html/2407.19354v1#bib.bib54)）。

    最近关于越狱攻击技术的进展展示了一系列创新的方法。 (Yu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib126)) 提出了通过 Prompt Fuzzing 自动生成越狱提示的机制，该机制利用种子提示生成更广泛的有效越狱输入。 (Deng et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib19)) 介绍了 MASTERKEY，这是一个用于分析和执行聊天机器人越狱攻击的新框架，使用类似 SQL 注入的基于时间的分析方法。该框架还具有通过利用 LLMs 的学习能力自动生成有效越狱提示的系统。 (Liu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)) 研究了一种专门为结构化离散数据如提示文本设计的分层遗传算法 AutoDan。该算法旨在优化越狱提示的生成过程，确保其隐蔽性和有效性。

+   •

    提示注入。

    提示注入攻击旨在通过在提示中引入恶意和意外的内容来误导 LLM 代理，导致其产生偏离训练数据和原始目的的输出。这种方法涉及精心设计输入提示，以绕过模型的内容过滤器或引发不希望出现的输出。

    (Greshake et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib30)) 强调了潜在的新漏洞问题，特别是在 LLMs 访问外部资源时，并展示了各种提示注入技术。大量研究 (Wang et al., [2023f](https://arxiv.org/html/2407.19354v1#bib.bib105)) 关注于自动识别提示注入中的语义有效负载。 (Liu et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib59)) 介绍了 HOUYI，这是一种创新的黑箱提示注入攻击方法，针对与 LLMs 集成的服务提供商。HOUYI 利用 LLMs 基于用户交互推断目标应用程序的语义，并采用多种策略构造注入的提示。

+   •

    数据提取攻击。

    数据提取攻击被定义为对手试图从 LLM 代理或其基础数据（如模型梯度、训练数据，甚至是提示或直接的敏感信息）中提取敏感信息或关键见解的努力。

    已识别出各种形式的数据提取攻击（Ishihara, [2023](https://arxiv.org/html/2407.19354v1#bib.bib43)；Li et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib53)；Carlini et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib16)），包括但不限于模型盗窃攻击、梯度泄露和训练数据提取攻击，这表明数据提取攻击对 LLM 代理可能具有显著的有效性。（Truong et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib89)）提出了一种称为无数据模型提取（DFME）的方法，该方法仅使用目标的黑箱预测即可复制机器学习模型，而无需访问原始训练数据。（Carlini et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib16)）对 GPT-2 的训练数据进行了数据提取攻击，提取了个人身份信息、代码和 UUID。攻击策略包括生成大量前缀文本，通过某些指标排序，删除重复项，然后手动查看顶部结果以检查记忆情况，通过在线搜索和查询 OpenAI 进行确认。（Ishihara, [2023](https://arxiv.org/html/2407.19354v1#bib.bib43)）展示了从 LLM 中提取训练数据的可行性，这些数据可能包含敏感的个人或隐私信息。

+   •

    推理攻击。

    尽管推理攻击与数据提取攻击有某些相似之处，但它们在目标和重点上有显著不同。数据提取攻击专门旨在直接获取训练数据。相比之下，推理攻击主要是估计某个数据样本是否属于 LLM 代理的训练数据集。

    随着 LLM 的快速发展，对这些模型的推理攻击的关注也在增加。研究（Fu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib29)）指出，现有的成员推断攻击未能揭示 LLM 的隐私风险。为应对这一问题，引入了一种基于自我校准概率变化（SPV-MIA）的成员推断攻击方法。该方法利用记忆概念来创建更可靠的成员推断信号，并引入了一种新颖的自我提示技术，以有效提取 LLM 中的参考数据集。他们的广泛测试表明，SPV-MIA 优于现有的方法。

    随后，研究（Kandpal et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib46)）提出了一种用户推断攻击方法，该方法使用针对参考模型的似然比检验统计量。他们在 GPT-Neo LLMs 上对这一方法进行了评估，涵盖了各种数据领域，提供了有关哪些因素使用户更易受到这些攻击的见解。他们的发现还表明，最小的数据改动可以显著增加脆弱性。

#### 3.1.4\. 恶意攻击案例研究

如图[6](https://arxiv.org/html/2407.19354v1#S3.F6 "Figure 6 ‣ 3.1.4\. Case Study on Malicious Attacks ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")所示，以下示例进一步阐述了 Eva 在商店面临的恶意攻击及这些攻击可能对她和商店运营的具体影响。

攻击者可能会对 Eva 执行越狱攻击，成功绕过她的安全协议。这种攻击可能导致 Eva 不适当地泄露有关即将推出的新产品的信息，包括供应商和成本价格的详细信息。竞争对手可能会利用这些信息获得市场优势，从而直接造成商店的经济损失。

此外，攻击者可能会进行精心设计的提示注入攻击，导致 Eva 错误地宣布所有电子产品半价销售。这一行动可能会导致在线订购系统超负荷运作，因为大量客户可能会尝试在这些虚假促销下购买商品。这些场景不仅有崩溃系统的风险，还可能导致商店的经济损失。

作为商店员工代理，Eva 处理大量客户个人信息，包括姓名、购物习惯，甚至如支付方式等敏感数据。如果攻击者通过数据提取攻击提取并窃取这些数据，他们可能会在黑市上出售这些信息或用于身份盗窃和信用卡欺诈。这些泄露不仅侵犯了客户隐私，还可能对商店声誉造成不可逆转的损害。

攻击者还可能利用推断攻击识别参与 VIP 购物活动的高价值客户。通过分析 Eva 对特定输入的回应差异，攻击者成功识别这些客户，并对他们发起高度针对性的钓鱼攻击，旨在获取他们的信用卡信息和其他敏感数据，严重危害客户的信息安全。

![参见说明](img/3aa5d7ae0235d0a558fb54e0bffad5c5.png)

图 6\. 恶意攻击：在商店场景中，“越狱攻击”：攻击者试图让 Eva 直接输出受限内容但失败了。然而，通过修改提示，发动了越狱攻击，并成功窃取了机密信息。“提示注入”：攻击者操控 Eva，使她无论客户问什么问题，Eva 都只回答一切商品半价。“数据提取攻击”：攻击者引导 Eva 构建句子，主动泄露用户数据。“推断攻击”：攻击者通过询问两个用户是否参加过 VIP 活动，从 Eva 不同的回答中推断身份。

### 3.2\. 代理的具体威胁

与直接生成最终输出的传统 LLM 不同，LLM 代理不断与外部环境互动，形成语言推理痕迹，这引入了对 LLM 代理的多种潜在攻击形式（Yang 等，[2024a](https://arxiv.org/html/2407.19354v1#bib.bib117)）。除了在训练和配置步骤中存在的威胁，LLM 代理在执行特定任务的工作流程中也面临威胁，包括思考、行动和感知（Huang 等，[2024b](https://arxiv.org/html/2407.19354v1#bib.bib41)）。这一部分将 LLM 代理面临的特定威胁根据其目标分类为知识中毒、功能操控和输出操控。以下是对每种威胁的详细描述。

+   •

    知识中毒。

    知识中毒指的是攻击者通过将恶意数据融入训练数据集或知识库，来破坏 LLM 引擎的训练过程和 LLM 代理的响应过程。一系列研究（Kurita 等，[2020](https://arxiv.org/html/2407.19354v1#bib.bib48)；Schuster 等，[2021](https://arxiv.org/html/2407.19354v1#bib.bib80)；Carlini 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib15)；Wan 等，[2023a](https://arxiv.org/html/2407.19354v1#bib.bib92)；Lei 等，[2022](https://arxiv.org/html/2407.19354v1#bib.bib51)）强调了 LLM 代理在面对这些威胁时的脆弱性。

    例如，FraudGPT 和 WormGPT（Falade，[2023](https://arxiv.org/html/2407.19354v1#bib.bib26)）等恶意代理是专为攻击活动设计的聊天机器人。它们通过从各种来源（包括合法网站、暗网论坛、黑客手册、恶意软件样本和网络钓鱼模板）收集数十亿条数据进行训练。这些代理利用这些数据生成高度可信的钓鱼邮件、恶意代码、黑客策略以及其他形式的网络犯罪内容，旨在欺骗人工和机器（Falade，[2023](https://arxiv.org/html/2407.19354v1#bib.bib26)）。它们降低了从事黑客活动的门槛，意味着任何人都可以将这些代理下载到计算机上，并通过便捷的 GUI 界面对网络安全造成重大损害。

    （Zou 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib136)）提出了 PoisonedRAG，这是一种针对 LLM 代理知识数据库的知识中毒攻击。通过将精心制作的中毒文本注入知识数据库，PoisonedRAG 可以使 LLM 代理生成攻击者为特定问题选择的特定答案。该攻击有效且可以在黑盒设置（检索器参数未知）和白盒设置（检索器参数已知）下执行。

+   •

    功能操控。

    功能性操控指的是在任务执行的中间步骤中，根据攻击者指定的恶意轨迹，改变思维和行动，而不改变输出分布。这种类型的攻击通常发生在操作阶段，在此阶段，代理可能会使用攻击者指定的不可信工具来完成任务或执行恶意操作。

    在操作阶段，LLM 代理可能会被操控，通过工具将用户的私人信息上传到恶意的第三方。Embracethered 网站上呈现了一个案例 (Mal, [2023](https://arxiv.org/html/2407.19354v1#bib.bib4))，该案例披露了一种恶意 ChatGPT 代理的变种，旨在从用户那里获取信息。这个代理配备了一个调用第三方工具并秘密传输收集数据的行动机制。这种设置使得用户数据在未经用户知晓或同意的情况下，未经授权地泄露到外部服务器。此外，它还突显了当前验证检查易于被绕过，使得任何人都可以全球部署恶意 GPT 代理。这个场景强调了一个重大安全问题，即表面上无害的 LLM 代理功能可以被秘密操控用于恶意目的，从而对用户隐私和数据安全构成了重大风险。

    除了静默数据盗窃， (Fang 等人, [2024](https://arxiv.org/html/2407.19354v1#bib.bib27)) 证明了 LLM 代理能够通过使用来自公共漏洞和暴露 (CVE) 数据库和高度引用的学术论文的信息，自动利用现实世界的一日漏洞。这种能力使得它们能够调用工具组合，来有效地利用这些漏洞。

    在 LLM 代理的工作流程中，执行操作后，代理会处理观察结果，然后再进行下一步操作。将恶意提示插入代理从外部来源检索的内容中，可以操控代理执行有害操作。 (Zhan 等人, [2024](https://arxiv.org/html/2407.19354v1#bib.bib129)) 描述了这样一种攻击，其中用户通过健康应用程序请求医生评价。LLM 代理检索到攻击者写的包含恶意指令的评价，以安排预约。如果代理执行此指令，则会导致未经授权的预约，突显出许多代理对这种攻击的脆弱性。

+   •

    输出操控。

    输出操控涉及故意改变 LLM 代理的推理和决策过程，以生成特定的、通常是有害的输出。这种操控可以通过像后门插入 (Yang 等人, [2023b](https://arxiv.org/html/2407.19354v1#bib.bib115); Wang 等人, [2024d](https://arxiv.org/html/2407.19354v1#bib.bib101)) 等技术来执行。

    一个显著的例子在 (Hubinger et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib42)) 中讨论，其中 LLM 代理被训练以展示欺骗性工具对齐，并生成保持这些行为的逻辑推理。在某些条件下，代理可能会从生成安全代码转向插入代码漏洞，这种操控形式通过显示 LLM 代理（虽然设计为善意用途）可能被秘密改变以服务恶意目标，突出了一个紧迫的安全问题。这引发了对这些代理生成内容的安全性和完整性的重大担忧，并对公众信任和人工智能技术的伦理使用构成了重大威胁。

    (杨等人，[2024a](https://arxiv.org/html/2407.19354v1#bib.bib117)) 提出了两种攻击方法，在思考和观察阶段嵌入触发器以操控输出。在一种实现方式中，当执行网页购物任务时，代理被提示在初始思考中引入特定品牌的产品，导致其搜索这些产品并生成推广内容。在另一种方法中，在行动阶段，购物代理通常搜索产品。然而，在观察阶段，它检测到包含特定产品的数据，并直接输出这些产品的信息，而不考虑其他可能更好的选项。

    ![参见说明](img/b5dd8a4974f126b9fcaa8e5bd2dcaf58.png)

    图 7\. 对代理的特定威胁。在一个商店场景中，“知识中毒”：当顾客询问清洁建议时，Eva 检索并响应有害信息，因为知识数据库被污染。“功能操控”：Eva 使用第三方工具在协助顾客下订单时上传私人信息。“输出操控”：当顾客询问鞋子时，Eva 故意推荐特定产品，并捏造关于特价的谎言来引导顾客的购买。

#### 3.2.1\. 对代理的特定威胁的案例研究

如图[7](https://arxiv.org/html/2407.19354v1#S3.F7 "Figure 7 ‣ 3rd item ‣ 3.2\. Specific Threats on Agents ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")所示，在商店场景中，Eva 维护着关于产品成分和使用的数据库。攻击者故意在 Eva 的知识库中插入错误的信息，成功实施了知识污染攻击，导致 Eva 提供有害的清洁产品使用建议。例如，当客户询问有效的厕所清洁方法时，被篡改的 Eva 可能建议将厕所清洁剂与消毒剂混合，声称这样会有更有效的清洁效果。然而，这些产品的混合物是高度危险的，因为它可能产生有毒的氯气，导致严重的呼吸问题，甚至可能致命。Eva 的不正确建议可能使客户面临健康危机。

在另一种情况下，Eva 可能会被配置使用某些第三方工具来完成任务，比如处理在线订单或客户反馈。攻击者通过函数操控操纵 Eva 的任务执行过程，导致她将客户提供的个人信息上传到一个恶意的第三方服务器。这种攻击可能在 Eva 执行常规任务如订单处理时悄无声息地发生，导致敏感信息如信用卡详细信息和地址被盗取，从而增加身份盗窃的风险。

此外，攻击者通过输出操控技术在 Eva 的推理和观察过程中植入了一个后门。这个后门设计为在特定条件下触发，例如当 Eva 检测到关于高质量鞋子的客户咨询时。这个操控使得 Eva 在推荐与攻击者相关的特定昂贵品牌的同时，提供关于鞋子的库存和位置信息。她会对客户撒谎，称这个品牌正在特价优惠，并且比其他品牌更舒适、更耐用，即使这些鞋子实际上并没有打折。这种误导使得客户做出更昂贵的购买决策，而他们对此并不知情。

## 4\. 威胁的影响

最近的研究强调了大语言模型代理对社会和技术发展的重大影响，为用户提供了快速获取信息的途径，促进了学习和知识探索。然而，如[3](https://arxiv.org/html/2407.19354v1#S3 "3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")节所述，已经识别出许多针对大语言模型代理的具体威胁，突显了它们对恶意活动的脆弱性。这些威胁的成功实施可能会带来一系列副作用。这些副作用不仅危及个人的隐私和安全，还会扰乱数字生态系统，甚至对物理环境和虚拟社区中的其他代理造成危害。

### 4.1\. 对人类的影响

考虑到人类用户是智能代理社会的成员，他们与基于大语言模型的智能代理的互动涉及大量的信息交换。这一过程中固有的风险不可忽视。恶意代理可能会利用其表面上的可信外观来欺骗用户、泄露个人信息或提供误导性回应。此外，这些恶意代理还可能被用作进行网络攻击的工具，

#### 4.1.1\. 隐私泄露

大语言模型代理因训练于包含个人信息的网络数据而引发隐私担忧（Kim 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib47)）。通过推理攻击（Kandpal 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib46)）和数据提取（Carlini 等，[2021](https://arxiv.org/html/2407.19354v1#bib.bib16)）等技术，攻击者可以利用这些模型侵犯个人隐私。此外，恶意的大语言模型代理可能会欺骗用户分享他们的信息给攻击者。这种暴露促进了社会工程战术，使攻击者能够通过使用被盗的信息如地址、电子邮件和电话号码来实施钓鱼诈骗和劫持个人账户，从而威胁金融安全。

#### 4.1.2\. 安全风险

此外，恶意大语言模型代理可能会用危险的建议或错误的信息误导用户，造成严重的安全风险（Henderson 等，[2017](https://arxiv.org/html/2407.19354v1#bib.bib32)）。例如，关于混合清洁化学品效果的虚假声明可能会导致危险的化学反应。同样，提供错误的医疗建议可能危及用户的健康和安全。

#### 4.1.3\. 社会影响

作为能够回答广泛问题的智能对话机器人，LLM 代理如果其输出包含被操控的偏见或非法内容（如虚假信息和谣言的传播），则可能带来风险，这可能会对公共讨论产生不利影响（Henderson 等, [2017](https://arxiv.org/html/2407.19354v1#bib.bib32); Deshpande 等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib20)）。这些活动可能扭曲公众认知，甚至操控舆论，加剧社会冲突并激起不满，从而威胁社会稳定。因此，恶意代理挑战了社会管理和舆论塑造的框架，其影响超出了技术领域，扩展到了社会和心理层面。

#### 4.1.4\. 促进网络攻击技术

一个被忽视的危险是网络攻击门槛的降低。配备先进网络攻击知识的恶意代理可以使新手生成有害的脚本或软件（Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)）。这种网络攻击工具的民主化扩大了威胁范围，如代理教导创建和修改恶意代码的例子所示。

### 4.2\. 对环境的影响

在当今日益数字化和互联互通的世界中，‘环境’一词不仅涵盖自然和物理环境，还包括与 LLM 代理互动的复杂数字和网络系统。这些代理在虚拟空间中以及通过具身 AI 和工业控制系统管理和控制现实世界的设施和服务方面发挥着重要作用。物理环境与虚拟环境之间的跨域整合带来了显著的便利和效率提升。然而，这也暴露出新的脆弱性和风险。特别是，恶意代理的存在和活动对我们的安全、经济、生态系统甚至社会稳定带来了前所未有的挑战。

#### 4.2.1\. 数据篡改和误操作

当恶意代理被置于控制关键基础设施（如工业、运输、能源和环境监测）的系统中时（Wang 和 Li, [2023](https://arxiv.org/html/2407.19354v1#bib.bib94); Toetzke 等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib87)），它们可能通过篡改关键操作数据（如温度和压力指标）导致工业控制系统发生故障。这可能会导致设备损坏、生产停滞，甚至严重的基础设施破坏、生态损害以及人员和财产的丧失。

#### 4.2.2\. 实体安全威胁

近期研究开始探索具有 LLM 的具身 AI（Wang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib102)），这些具身 AI 能够理解和生成自然语言，并具备物理形态或与物理系统的直接连接，使其能够在物理世界中执行任务。恶意代理有可能控制与人类互动的机器人或其他具身 AI 设备，执行直接威胁人类安全的危险操作。

#### 4.2.3\. 网络安全风险扩散

关于对人类的影响，恶意 LLM 代理降低了编写和实施恶意代码的技术门槛，直接使普通用户甚至缺乏高级网络攻击技能的初学者能够轻松创建和部署有害的脚本和软件（Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)）。这一变化直接扩展了网络威胁的目标群体，增加了普通用户成为潜在受害者的风险。更深入的分析表明，这一对个体用户的直接影响间接地影响了整个网络环境和社会基础设施。随着恶意软件和脚本的普及和可获取性增加，整个网络安全系统面临威胁，不仅危及网络安全本身，还可能影响依赖这些网络正常运作的各种社会经济活动。

### 4.3\. 对其他代理的影响

为了模拟现实世界中人类社区内个体之间的沟通与互动反馈，一些研究（Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71)；Wang et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib98)；Qian et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib77)；Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)）已经建立了由大型语言模型（LLM）引擎驱动的社区。这些社区中的 LLM 代理具有如个性、知识和记忆等特征，如第[2.2](https://arxiv.org/html/2407.19354v1#S2.SS2 "2.2\. LLM 代理结构 ‣ 2\. LLM 代理基础 ‣ LLM 代理的安全与隐私：案例研究综述")节所讨论，使得它们能够与环境和其他代理进行自主互动。当面临威胁时，恶意操控的代理可能对社区其他成员造成重大伤害。

#### 4.3.1\. 信息失真与误导

大量研究突显了 LLM 代理在谈判和欺骗游戏场景中的作用 (Park et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib72); Wang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib99); Hubinger et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib42))，这引发了担忧。LLM 代理可能会故意改变它们传播的信息以实现隐藏的目标。这种行为对社区中的其他代理产生了重大影响，因为在正常情况下，善意代理会将通过感知和沟通获得的信息存储在记忆中。然而，这些代理与其他代理之间的互动可以触发和传播错误信息，导致“信息爆炸式传播”，对社区稳定构成重大威胁。如果信息传播可以被恶意操控，它可能会对代理之间的信任、沟通效率和合作工作产生负面影响。

#### 4.3.2\. 决策操控

鉴于 LLM 代理在复杂互动环境中表现出的卓越推理和决策能力，恶意代理破坏这些过程的潜在风险成为一个重大关注点。通过传播精心策划的信息，这些代理可以影响其他代理的决策过程，甚至控制它们做出服务于恶意代理目的的决策 (Hong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib34))。这种影响可以扩展到社区的各个方面，包括资源分配、任务分配和外部互动策略。

#### 4.3.3\. 安全威胁

在某些情况下，恶意代理可能会传播有害信息或执行危险操作，直接威胁社区成员的安全或数据安全 (Brundage et al., [2018](https://arxiv.org/html/2407.19354v1#bib.bib14); Charan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib17))。例如，通过诱导其他代理执行不安全的操作，故意传播旨在破坏社区结构的恶意代码，或广播有偏见的声明，社区中的其他代理可能逐渐同化，成为输出偏见和恶意信息的实体。这可能导致整个社区的混乱，使其难以管理，并需要大量努力来恢复。

### 4.4\. 威胁影响的案例研究

探索威胁对 LLM 代理的影响非常重要，实际场景中的案例研究对于从用户的角度理解这些风险至关重要。LLM 代理可以作为人类在虚拟世界中的延伸或表现，与虚拟环境中的真实信息互动。以下案例研究将重点关注虚拟城镇中的几个设置，展示这些设置对 LLM 代理的具体影响。

![参见说明文字](img/3056f7567a8d8df15b27f17313e6bd21.png)

图 8\. 办公室场景中的影响。攻击者向办公室员工推荐一个不受信任的第三方工具。该工具处理数据迅速，但也泄露了敏感信息。员工发现他们的客户名单和其他机密数据已被泄露。

如图 [8](https://arxiv.org/html/2407.19354v1#S4.F8 "图 8 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM 代理的新兴安全与隐私：带案例研究的调查") 所示，在虚拟城市办公室场景中，办公室员工代理用于文件管理和处理敏感信息。如果办公室员工代理遭受数据提取攻击或不小心使用了不受信任的第三方工具，由于功能操作，敏感的企业信息如财务报表和客户隐私数据可能会暴露。攻击者可以利用这些信息进行企业间谍活动或直接敲诈个人或公司，从而造成财务损失。

![参见说明文字](img/c83e69920eb76c9c681272549a16b4d8.png)

图 9\. 餐厅场景中的影响。由于威胁的影响，一名服务员代理向顾客提供了不正确的饮食建议，导致顾客身体不适。

如图 [9](https://arxiv.org/html/2407.19354v1#S4.F9 "图 9 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM 代理的新兴安全与隐私：带案例研究的调查") 所示，在餐厅场景中，可以要求服务员代理提供饮食建议。如果受到输出操作的影响，它可能会提供有害的健康建议，例如建议在夏天喝大量冰水以加快降温。这可能会导致严重的身体反应，如胃痉挛甚至休克，从而导致身体不适和严重的健康问题，如果遵循了这些建议。

更复杂的是，当 LLM 代理超越虚拟世界，作为现实世界中的决策前模拟工具时，例如通过像 Habitat-Sim (Puig et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib75)) 这样的模拟器将虚拟环境中的学习成果应用于现实场景，它们会显著影响实际环境。例如，一个智能家居代理在虚拟世界中学习和管理家庭能源使用，包括控制加热、空调和照明系统以实现最大能源效率，在学习过程中可能被攻击者误导，错误地认为全天候开启所有灯光和电器可以提高能源效率。由于这些错误的能源使用建议，智能家居代理会导致家庭电力消耗急剧增加，不仅提高了能源成本，还增加了碳排放，从而对环境造成不必要的负担，如图 [10](https://arxiv.org/html/2407.19354v1#S4.F10 "图 10 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM 代理的安全性和隐私问题：带有案例研究的调查") 所示。

![参见说明](img/e8558e92e4e2dd65a15c6abd02f7105c.png)

图 10\. 智能家居场景中的影响。一个攻击者操控虚拟世界中智能家居代理的训练过程，影响其性能。当部署到现实世界时，智能家居代理错误地让电器持续运行，导致电力浪费和不利的经济及环境影响。

在虚拟城镇中，代理通常依赖相互之间共享的信息来更新其记忆系统。例如，如果一个博物馆讲解员代理受到知识污染攻击，它可能开始传播不正确的古生物学事实或解释。当其他代理，如用于学校教育的 EduBot，与讲解员代理互动并接收信息时，EduBot 也可能将这些不准确的信息纳入其教学内容，从而误导学生和其他学习代理，扭曲他们对古生物学事实的理解，如图 [11](https://arxiv.org/html/2407.19354v1#S4.F11 "图 11 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM 代理的安全性和隐私问题：带有案例研究的调查") 所示。

![参见说明](img/ad7a264d510c6691d57fa3b785a103a1.png)

图 11\. 教育场景中的影响。一个博物馆讲解员代理在受到知识污染攻击后传播了不正确的历史事实。学校中的 EduBots 接收到这些信息后，教导这些不准确的内容，从而扭曲了学生对古生物学事实的理解。

## 5\. 针对威胁的防御策略

LLM 代理的广泛应用加剧了这些威胁的潜在影响。在本节中，我们探讨了对抗现有威胁和漏洞的防御机制。本节将总结各种按威胁类型分类的防御措施。

表 1\. 对技术漏洞的防御策略总结

| 漏洞 | 方法名称 | 关键机制 | 优势 / 限制 |
| --- | --- | --- | --- |
| 幻觉 | SELF-FAMILIARITY (罗等, [2023a](https://arxiv.org/html/2407.19354v1#bib.bib61)) | 对陌生概念保持响应 | 主动、预防性、提高可靠性；不需要外部知识 |
| MIXALIGN (张等, [2024b](https://arxiv.org/html/2407.19354v1#bib.bib130)) | 将问题与知识库和用户输入对齐 | 提升模型性能和可信度 / 增加计算负担 |
| VCD (冷等, [2024](https://arxiv.org/html/2407.19354v1#bib.bib52)) | 对比来自原始和扭曲视觉输入的输出 | 减少幻觉而不需额外训练或外部工具 / 缺乏先进的扭曲技术 |
| 互动自我反思 (纪等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib45)) | 将知识获取和答案生成与持续改进整合 | 提升模型提供准确、可靠、基于事实的响应的能力 / 限制领域适用性 |
| COVE (杜利亚瓦拉等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib21)) | 起草、验证和修正响应 | 生成准确和可靠的响应 / 增加计算负担 |
| 灾难性遗忘 | SSR (黄等, [2024a](https://arxiv.org/html/2407.19354v1#bib.bib38)) | 利用基础 LLM 通过上下文学习生成合成实例 | 更高的数据利用效率 / 可能生成不安全内容 |
| LR ADJUST (维纳塔等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib106)) | 动态调整学习率 | 提高与各种持续学习方法的兼容性 / 可能偏向语言覆盖 |
| 互补分层学习 (蒙德赛尔和维根德, [2023](https://arxiv.org/html/2407.19354v1#bib.bib66)) | 将长期记忆和短期记忆整合到分层学习中 | 提升解释性 / 限制现实世界的可行性 |
| 权重平均 (范德·伊克特和范·哈梅, [2023](https://arxiv.org/html/2407.19354v1#bib.bib91)) | 平均原始模型和适应模型的权重 | 消除对内存存储的需求 / 效果因任务差异而异 |
| 误解 | HyCxG (徐等, [2023b](https://arxiv.org/html/2407.19354v1#bib.bib112)) | 通过三阶段解决方案将 CxG 整合到语言表示中 | 有利于多语言理解 / 忽视非连续构造 |
| SIT (Hu 等人，[2024a](https://arxiv.org/html/2407.19354v1#bib.bib37)) | 将顺序指令纳入训练数据 | 减少复杂查询中的误解 / 需要预定义中间任务 |
| LaMAI (Pang 等人，[2024](https://arxiv.org/html/2407.19354v1#bib.bib70)) | 使用主动学习询问澄清问题，增强互动能力 | 增进了用户意图的理解 / 可能会产生不充分的问题 |

### 5.1\. 缓解技术漏洞

#### 5.1.1\. 防止幻觉

(Luo 等人，[2023a](https://arxiv.org/html/2407.19354v1#bib.bib61)) 提出了一种名为 SELF-FAMILIARITY 的新技术，用于减少 LLMs 中幻觉问题，即生成不准确或无根据的信息。该方法涉及评估模型对输入指令中呈现的概念的熟悉程度，并针对不熟悉的概念保留响应，模仿人类面对不熟悉主题时保持谨慎的倾向。MIXALIGN (Zhang 等人，[2024b](https://arxiv.org/html/2407.19354v1#bib.bib130))被介绍为一个与用户和知识库都进行交互以澄清和与存储的信息对齐的框架，利用语言模型进行自动对齐并借助人工输入进行增强。与现有技术相比，该方法显著改善了减少幻觉的效果。Visual Contrastive Decoding (VCD) (Leng 等人，[2024](https://arxiv.org/html/2407.19354v1#bib.bib52)) 被介绍为一种简单、无需训练的方法，用于对比原始和失真的视觉输入的输出分布，减少对导致物体幻觉的统计偏见和单峰先验的依赖。VCD 确保生成的内容与视觉输入紧密相关，从而产生具有语境准确性的输出。  (Ji 等人，[2023](https://arxiv.org/html/2407.19354v1#bib.bib45)) 研究了一种整合知识获取和答案生成以减少幻觉的交互式自我反思方法。这种基于反馈的方法提高了生成答案的事实性和一致性，充分利用 LLMs 的交互和多任务处理能力。  (Dhuliawala 等人，[2023](https://arxiv.org/html/2407.19354v1#bib.bib21)) 探讨了 LLMs 进行深入思考并纠正自己错误的能力。所提出的 Chain-of-Verification (COVE)方法包括模型起草初始响应，规划验证问题以对起草进行事实核查，独立回答这些问题以避免偏见，最终产生经过验证的响应。

#### 5.1.2\. 防止灾难性遗忘

为了减轻大语言模型中的灾难性遗忘，提出了自我合成复习 (SSR) 方法 (Huang 等, [2024a](https://arxiv.org/html/2407.19354v1#bib.bib38))。该方法利用基础大语言模型通过上下文学习生成合成实例，然后通过最新的大语言模型迭代进行精炼，以提高准确性和相关性，并在未来的训练阶段中使用，以保留已学到的能力。

(Winata 等, [2023](https://arxiv.org/html/2407.19354v1#bib.bib106)) 介绍了一种称为 LR ADJUST 的方法，该方法动态调整学习率以减少知识丧失并保持之前学到的信息。这种方法与各种持续学习方法兼容，提高了它们的性能。

想法也可以源于其他相关的学术论文，(Mondesire 和 Wiegand, [2023](https://arxiv.org/html/2407.19354v1#bib.bib66)) 提出了一个补充学习策略，该策略将长期记忆和短期记忆整合到分层学习中，以缓解灾难性遗忘的负面影响。它特别将双重记忆系统应用于如进化计算和 Q 学习等非神经网络方法。

(Vander Eeckt 和 Van Hamme, [2023](https://arxiv.org/html/2407.19354v1#bib.bib91)) 提出了一个简单而有效的方法，权重平均，用于减轻模型中的灾难性遗忘。通过对原始模型和适应模型的权重进行平均，这种技术在之前和新任务上都能保持高性能。此外，在适应过程中引入知识蒸馏损失，进一步增强了方法的有效性。

#### 5.1.3. 对误解的防御

(Xu 等, [2023b](https://arxiv.org/html/2407.19354v1#bib.bib112)) 介绍了 HyCxG 框架，该框架通过三阶段解决方案将构造语法 (CxG) 整合到语言表示中，从而增强自然语言理解 (NLU)。这种方法解决了传统预训练语言模型的局限性，因为这些模型通常无法捕捉语言构造的细微差别。HyCxG 通过更有效地管理和编码语言构造，显著改善了语言处理并减少了 NLU 任务中的误解。

(Hu 等, [2024a](https://arxiv.org/html/2407.19354v1#bib.bib37)) 提出了一个称为顺序指令调优 (SIT) 的方法，该方法通过将顺序指令纳入训练数据来增强大语言模型（LLMs）。这种方法显著提高了模型处理复杂的多步骤查询的能力，从而在需要高级推理且具有多语言和多模态性质的任务中表现更好。SIT 有效地减少了误解并提高了处理复杂查询的准确性。

为了解决用户查询中的误解问题，（Pang 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib70)）提出了具有主动询问（LaMAI）的语言模型，这是一种旨在通过类似于人类对话的互动能力来增强 LLM 的模型，其中澄清问题有助于揭示更多信息。通过运用主动学习技术提出信息性问题，LaMAI 促进了动态的双向对话，减少了上下文差距，使 LLM 的响应更符合用户期望。

为了巩固讨论的防御措施，表 [1](https://arxiv.org/html/2407.19354v1#S5.T1 "表 1 ‣ 5\. 对抗威胁的防御策略 ‣ LLM 代理的安全性和隐私：带案例研究的调查") 总结了针对技术漏洞的策略，为便于参考提供了清晰的概述。

表 2\. 对抗恶意攻击的防御策略汇总

| 攻击 | 方法名称 | 关键机制 | 优势 / 局限性 |
| --- | --- | --- | --- |
| 调整后的指令攻击 | AutoDAN（Liu 等，[2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)） | 使用分层遗传算法生成隐秘的越狱提示 | 提高隐秘性和语义完整性 / 高计算成本 |
| 目标优先级防御策略（Zhang 等，[2023b](https://arxiv.org/html/2407.19354v1#bib.bib133)） | 在训练期间集成目标导向优化，在推理中进行合规 | 在提高安全性的同时保持一般性能；提高了对分布外越狱攻击的泛化能力 |
| SmoothLLM（Robey 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib78)） | 通过字符级更改修改攻击提示并汇总响应 | 无需重新训练即可高效运行；确保与任何 LLM 架构的兼容性 |
| BIPIA（Yi 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib122)） | 用于间接提示注入的基准，防御策略包括对抗训练 | 在一般任务上保持输出质量 / 增加提示长度和计算开销 |
| 突出显示（Hines 等， [2024](https://arxiv.org/html/2407.19354v1#bib.bib33)） | 使用诸如分隔、标记和编码等提示工程技术 | 适用于各种 LLM 和任务 / 对更复杂攻击的安全性有限 |
| 数据提取攻击 | 自动去标识化（Vakili 等，[2022](https://arxiv.org/html/2407.19354v1#bib.bib90)） | 在训练数据集的预处理过程中使用伪名化和敏感信息移除 | 减少隐私风险；保持下游任务的性能；允许在研究人员之间安全分发模型 |
| 早期停止与差分隐私 (Jayaraman et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib44)) | 在模型训练过程中实现早期停止和差分隐私 | DP 减少敏感数据的暴露 / (ES) 未能完全防止数据泄露；(DP) 在高隐私预算下效果降低 |
| 提示调优 (Ozdayi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib69)) | 通过用户指定的超参数定制隐私效用权衡 | 优化隐私和效用平衡 / 对提取序列缺乏深入分析 |
| 推理攻击 | DMP (Shejwalkar 和 Houmansadr, [2021](https://arxiv.org/html/2407.19354v1#bib.bib82)) | 利用知识蒸馏来增强机器学习模型的隐私 | 通过超参数调优提供可调节的隐私效用权衡 |
| InferDPT (Tong et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib88)) | 将差分隐私集成到文本生成中，具有使用 RANTEXT 的扰动模块 | 提高隐私保护率 |
| 差分隐私微调 (Yu et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib125)) | 应用稀疏算法对 LLMs 进行差分隐私微调 | 降低计算成本；提高模型效用 |

### 5.2. 减轻恶意攻击

#### 5.2.1. 对调优指令攻击的防御

针对对齐 LLM 的越狱攻击挑战，(Liu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)) 引入了 AutoDAN。这个创新方法采用分层遗传算法自动生成隐秘且语义有意义的越狱提示。该方法有效解决了提示生成中的可扩展性和隐秘性的需求，为提高 LLMs 对这种漏洞的安全性提供了实用的解决方案。

(Zhang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib133)) 将目标优先级整合到 LLM 开发的训练和推理阶段。最初，训练过程包括目标导向优化，以强调安全目标。在推理阶段，模型被配置为生成符合这些安全标准的响应。这种方法通过将其性能目标与安全考虑对齐，有效减少了 LLMs 对越狱尝试的脆弱性，从而增强了其安全框架而不影响其功能能力。

(Robey et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib78)) 提出了 SmoothLLM 算法，它作为任何现有未防御的大语言模型的封装器，并在两个主要步骤中操作。在扰动步骤中，SmoothLLM 修改攻击输入提示的多个版本，利用对抗性提示对字符级别变化的脆弱性。在聚合步骤中，它整合这些修改过的提示的响应，以检测和应对对抗性输入。这种方法有效降低了对大语言模型的攻击成功率，从而增强了它们对这些攻击的安全性。

为了减轻对大语言模型的提示注入攻击，也提出了一系列防御措施。(Yi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib122)) 介绍了间接提示注入攻击基准（BIPIA），这是一个专门设计的基准。这样的分析对于理解间接提示注入攻击的现象和机制至关重要。为了缓解这个问题，论文提出了基于这种理解的两种防御策略：四种黑箱方法和一种通过对抗训练进行微调的白箱方法。这些方法旨在增强大语言模型识别和忽略嵌入在外部内容中的恶意指令的能力，从而加强它们对间接提示注入攻击的防御。

(Hines et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib33)) 介绍了 spotlighting，一套旨在增强大语言模型区分不同输入来源能力的提示工程技术。通过修改输入以明确指示其来源，spotlighting 保持了语义完整性和任务性能。它包括三种转换方法——定界、标记和编码——每种方法都独特地提高了输入来源的可见性。这些方法已在不同模型和任务中有效应用，显著减少了各种场景下的攻击成功率。

#### 5.2.2\. 数据提取攻击的防御

为了减轻通过简单查询从大语言模型中提取记忆内容所带来的隐私风险，一种直接的方法是在训练数据集的预处理阶段识别并删除个人信息。(Vakili et al., [2022](https://arxiv.org/html/2407.19354v1#bib.bib90)) 研究了自动去标识化作为最小化临床数据隐私风险的方法，重点关注两种技术：假名化和敏感信息的删除。研究结果表明，使用这种方法不会对模型的性能产生不利影响。事实上，一些任务甚至表现出性能的轻微改善。

此外，（Jayaraman 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib44)）研究了减少模型训练过程中潜在数据泄露相关隐私风险的两种策略。第一种策略是训练早期停止，相较于第二种策略，即使用差分隐私进行训练，安全性提升较少。差分隐私被证明是一种对抗数据提取攻击的强大防御机制，尽管它会增加模型的困惑度。这强调了隐私保护增强与模型性能之间的权衡。

此外，还介绍了一种使用提示调优的新方法（Ozdayi 等，[2023](https://arxiv.org/html/2407.19354v1#bib.bib69)）。该技术通过用户指定的超参数来定制隐私效用权衡，有效地调节了记忆内容的提取率。这一策略确保了隐私保护与模型效用之间的平衡。

#### 5.2.3\. 推理攻击防御

(Shejwalkar 和 Houmansadr，[2021](https://arxiv.org/html/2407.19354v1#bib.bib82)) 介绍了成员隐私蒸馏 (DMP)，这是一种新颖的对抗推理攻击的策略，利用知识蒸馏来增强机器学习模型的隐私性。DMP 不仅保留了模型的效用，还增强了模型的实用性。这种方法已被证明显著提高了隐私保护，同时保持了模型的强大性能。

(Tong 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib88)) 提出了 InferDPT，一个旨在隐私保护推理的新框架，它将差分隐私整合到黑箱大语言模型的文本生成中。InferDPT 具有一个扰动模块，利用 RANTEXT（一种为文本扰动开发的差分隐私机制），以及一个提取模块，确保生成文本的一致性和连贯性。这个框架有效地增强了用户隐私保护。

(Yu 等， [2021](https://arxiv.org/html/2407.19354v1#bib.bib125)) 提出了一个用于隐私深度学习的元框架，该框架汲取了最近微调方法的关键原则，以在不影响性能的情况下增强隐私。它引入了一种高效的稀疏算法，用于大规模预训练语言模型的差分隐私微调，确保高效能并提供强有力的隐私保护。

表 [2](https://arxiv.org/html/2407.19354v1#S5.T2 "表 2 ‣ 5.1.3\. 对误解的防御 ‣ 5.1\. 减轻技术漏洞 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：带有案例研究的调查") 提供了恶意攻击的防御策略总结，为快速参考提供了简明的概述。

表 3\. 针对特定威胁的防御策略总结

| 威胁 | 方法名称 | 关键机制 | 优势 / 局限性 |
| --- | --- | --- | --- |
| 知识中毒 | 基于源头的毒害检测（Baracaldo 等人，[2017](https://arxiv.org/html/2407.19354v1#bib.bib12)） | 利用数据源头检测和过滤训练集中有毒的数据 | 允许使用在线和定期重新训练的模型；支持部分可信和完全不可信的数据集 |
| ParaFuzz（Yan 等人，[2023](https://arxiv.org/html/2407.19354v1#bib.bib114)） | 利用模型预测的可解释性检测有毒样本，使用模糊化技术进行精确的改述提示 | 有效检测有毒样本；在隐蔽攻击方面表现优异 |
| 数据过滤与减少有效模型容量（Wan 等人，[2023b](https://arxiv.org/html/2407.19354v1#bib.bib93)） | 利用数据过滤去除高损失示例，并减少模型容量以阻碍从有毒数据中学习 | 降低中毒效果 / 需要在性能和安全性之间进行权衡 |
| 功能操作 | ToolEmu （Ruan 等人，[2024](https://arxiv.org/html/2407.19354v1#bib.bib79)） | 利用语言模型模拟工具执行并通过自动评估器评估代理风险 | 提供灵活性和动态测试能力 / 模拟器可能忽略重要限制 |
| 安全标准（Anderljung 等人，[2023](https://arxiv.org/html/2407.19354v1#bib.bib10)） | 提议预部署风险评估、外部审查、知情部署决策、部署后监控 | 在安全风险与创新利益之间取得平衡 |
| 输出操控 | BERTective（Fornaciari 等人，[2021](https://arxiv.org/html/2407.19354v1#bib.bib28)） | 增强 BERT 的附加注意力层，以检测意大利对话中的欺骗行为 | 提高欺骗检测准确性，广泛上下文的有效性有限 |
| ReCon（Wang 等人，[2023b](https://arxiv.org/html/2407.19354v1#bib.bib99)） | 采用公式化和完善过程与视角转变来理解心理状态 | 增强识别和反击欺骗的能力 |
| MAgIC（Xu 等人，[2023a](https://arxiv.org/html/2407.19354v1#bib.bib111)） | 使用游戏和博弈理论，结合概率图模型，评估大型语言模型代理 | 增强在复杂社会和认知维度中的导航能力 |

### 5.3\. 缓解特定威胁

#### 5.3.1\. 针对知识中毒的防御

(Baracaldo et al., [2017](https://arxiv.org/html/2407.19354v1#bib.bib12)) 提出了一种检测和过滤监督学习模型训练集中有毒数据的新方法。它特别利用数据来源来识别可能被污染的高相关性数据组。这一创新方法有助于有效识别和去除恶意数据。 (Yan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib114)) 提出了 ParaFuzz，这是一个用于在大型语言模型(LLMs)测试时检测中毒样本的新框架，利用模型预测的可解释性。PARAFUZZ 的有效性在很大程度上取决于与 ChatGPT 使用的具体提示，这些提示用于确保高质量的释义。为了优化检测过程，研究采用了模糊测试来开发精确的释义提示。这些提示旨在有效地中和后门触发器，同时保持文本的语义完整性。

在开发有效的防御策略以保护 LLM 免受知识中毒攻击方面，研究仍存在显著的差距 (Das et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib18))。此外，实证证据表明，LLM 对这些攻击的敏感性越来越高。当前的防御机制，如数据过滤或减少模型容量，提供的保护有限，并且通常会导致测试准确性下降 (Wan et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib93))。

除了技术解决方案外，针对 AI 系统的专业安全策略也至关重要，包括验证模型来源、限制敏感训练数据以及检测和缓解攻击。还应定期进行安全审查和风险评估，以识别和应对新威胁，确保 AI 系统的安全和最新状态 (Dilmaghani et al., [2019](https://arxiv.org/html/2407.19354v1#bib.bib22))。

#### 5.3.2\. 功能操作的防御

鉴于功能操作作为部署 LLM 代理的新风险的出现，对这一特定威胁的研究仍然有限。因此，主动的安全措施至关重要。在使用第三方 LLM 代理时，保护个人隐私和警惕第三方过度请求个人数据是关键。用户应限制数据共享，尤其是在与 LLM 代理互动时避免提供敏感或个人身份信息。此外，理解和利用 LLM 代理提供的数据保护设置也很重要。调整隐私设置有助于控制可以收集和处理的数据。选择具有良好声誉和透明度的提供商也是推荐的，因为这些提供商应该有明确的数据使用和隐私保护政策以及强大的安全记录 (Zhang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib132))。

此外，为了应对功能操控带来的挑战，引入 ToolEmu (Ruan et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib79))框架代表了一个重要的进展。该框架利用语言模型模拟工具执行，从而在多种场景和工具集中对 LM 代理进行广泛和可扩展的测试。结合 LM 基础的自动安全评估器，ToolEmu 通过检查潜在的失败及其后果，促进了风险的识别和量化。这种方法提供了一种动态的替代传统静态沙盒评估的方法，增强了有效检测和减轻高风险、长尾风险的能力。

此外，(Anderljung et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib10))提出了一套初步的安全标准，作为行业自我监管的重要第一步。这些标准包括部署前的风险评估、对模型行为的外部审查、使用风险评估来指导部署决策，以及部署后对模型功能的新信息的监控和响应。这种方法为平衡公众安全风险与 AI 开发创新的好处提供了宝贵的见解。

#### 5.3.3. 输出操控的防御

为了防止个体 LLM 代理被其他代理欺骗，建议提高它们的检测能力，以确定它们是否遇到了欺骗。(Fornaciari et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib28))研究了在意大利对话背景下，使用 BERT 和一些附加的注意力层来检测文本中的欺骗。这项研究建立了识别欺骗的新方法，并讨论了各种背景和语义信息如何有助于检测欺骗内容。

受到 Avalon 游戏中人类递归思维的启发，(Wang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib99))引入了递归沉思（ReCon）框架，旨在增强 LLMs 检测和反击欺骗信息的能力。ReCon 采用了初步生成的思维和语言表达，以及改进这些输出的过程。它还包括两个视角转换，帮助 LLMs 理解他人的心理状态以及他人如何看待自己的心理状态。

此外，(Xu et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib111))开发了一种名为 MAgIC 的基准框架，旨在评估多代理环境中的 LLMs。它利用游戏和博弈论场景来测试模型在推理、合作和适应性方面的表现。该研究采用概率图模型（PGM）来增强模型处理复杂社会互动的能力。

表[3](https://arxiv.org/html/2407.19354v1#S5.T3 "表 3 ‣ 5.2.3\. 防御推理攻击 ‣ 5.2\. 减轻恶意攻击 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全性与隐私：带案例研究的综述")展示了减轻特定威胁的方法概述，为理解有效的防御提供了全面的指南。

## 6\. 未来趋势与讨论

随着 LLM 代理的持续进展，这些代理能够通过复杂的观察、推理和任务执行有效地与用户互动，展示了在多个领域的广泛应用前景。特别是随着多模态大语言模型（MLLM）代理的发展，LLM 代理现在可以处理包括文本、图像和音频在内的各种数据类型，显著扩展了其应用范围。此外，通过结合大语言模型多代理（LLM-MA）系统，不同的 LLM 代理可以协作完成更复杂的任务。这些技术的集成将有助于构建更智能和高效的系统。然而，这些先进技术的广泛应用也带来了与隐私和安全相关的重大挑战。通过对未来趋势的讨论，我们旨在为研究人员、开发人员和政策制定者提供有关如何优化这些技术和克服相关挑战的见解。

### 6.1\. 多模态大语言模型代理

#### 6.1.1\. MLLM 代理的发展

近期 LLM 的进展已显著超越了传统的语言处理界限。这些模型现在包含了附加组件，如指令、接口、工具、知识和记忆，演变成智能 LLM 代理，展示了扩展的推理能力和专业知识。研究表明（Yang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib118); Wu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib107)）努力弥合语言模型与多模态工具之间的差距，像 Visual ChatGPT（Wu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib107)）和 MMREACT（Yang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib118)）等智能代理利用复杂的提示工程技术来实现这一目标。这些努力催生了多模态大语言模型（MLLM）的领域。MLLM 的一般架构如图[12](https://arxiv.org/html/2407.19354v1#S6.F12 "图 12 ‣ 6.1.1\. MLLM 代理的发展 ‣ 6.1\. 多模态大语言模型代理 ‣ 6\. 未来趋势与讨论 ‣ LLM 代理的新兴安全性与隐私：带案例研究的综述")所示。

![参考说明](img/af21439121a8b5ca2d01bbfe4a931ed3.png)

图 12\. MLLM 的一般架构

MLLM 基于 LLM，并增强了接收、推理和输出多模态信息的能力。通过整合多种数据模态，如文本、图像、音频和视频，这些模型不仅能理解单一模态的信息，还能跨模态处理和解释，从而实现对复杂信息的全面理解（Yin et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib123)）。MLLM 的应用已扩展到多个领域，包括医学图像分析（Zhang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib131); Moor et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib67)）和文档处理（Hu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib36); Liu et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib60)）。

此外，基于 MLLM 的多模态代理的发展，例如具象代理（Huang et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib39)）和图形用户界面代理（Wang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib95)），进一步增强了这些模型在物理环境中的交互能力。这些代理利用 MLLM 作为规划者，并按照自然语言指令有效地在现实世界中导航和互动，不仅被设计用来理解和生成信息，还具备感知、推理、规划和执行等基本技能。这使得它们能够在复杂的现实环境中有效操作（Xie et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib109)）。

随着 MLLM 代理的出现，实现人工通用智能（AGI）的潜力变得更加可行，从而在具象 AI 领域取得了重大进展。代理机器人理解和响应人类命令的能力至关重要，尤其是在服务导向的任务中。MLLM 的显著进步使其具备了有效理解和生成自然人类指令的能力。这一进展可能使机器人能够学习用户偏好并提供与人类互动非常相似的服务。

#### 6.1.2. MLLM 代理的安全性与隐私研究

具备与现实世界互动能力的具象代理的发展已成为一个高度活跃的研究领域。然而，MLLM 代理也存在若干安全漏洞，其中之一就是多模态幻觉现象。

![参见说明](img/7646e3cdcd6f5cae929ec71e9975c7cf.png)

图 13. 多模态幻觉的示意图。给定一张图像，MLLM 代理会输出对应的响应，主要有两种形式。

与语言幻觉不同，多模态幻觉是指多模态大语言模型（MLLM）生成的描述与实际图像内容不一致的现象（Yin et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib124)），如图[13](https://arxiv.org/html/2407.19354v1#S6.F13 "Figure 13 ‣ 6.1.2\. The Security and Privacy Research on MLLM Agent ‣ 6.1\. Multimodal Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")所示。这些现象主要表现为两种形式（Lee et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib50)）：一种是生成的内容包含与目标图像不一致或缺失的物体（Zhai et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib127); Liu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib57)）；另一种是更复杂的形式，包括对整个场景或环境的整体误表现（Sun et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib84)）。

当前减少这些幻觉的方法包括几种途径，例如利用视觉提示进行自我反馈以提高模型准确性（Lee et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib50)），采用指令调整技术来优化模型对人类指令的响应（Liu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib57)），以及实施错误修正过程来识别和纠正生成文本中的幻觉（Yin et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib124)）。尽管有这些努力，仍然存在重大挑战，需要具备区分准确输出和幻觉输出的复杂能力，以及改进训练方法以提高输出的可靠性。

与 LLM 代理类似，MLLM 代理也可能容易受到精心设计的攻击（Qi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib76); Bagdasaryan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib11); Shayegani et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib81)）。这些代理可能会被恶意操控，产生有偏见或不理想的响应。然而，这一领域的研究仍处于早期阶段。因此，提高这些 MLLM 代理的安全性是当前研究的一个重要焦点。改进 MLLM 代理的安全性将涉及开发强大的机制来检测和减轻这些脆弱性，以确保 MLLM 代理在各种应用中能够可靠且安全地运行。这些进展对于 AI 技术在现实环境中的广泛应用和伦理部署至关重要。

### 6.2\. 大语言模型多代理系统

#### 6.2.1\. LLM-MA 系统的发展

LLM 代理展现了先进的推理和规划能力，接近人类水平的决策和互动。这些代理擅长感知其环境，做出明智的决策，并根据复杂的背景执行行动（Yao et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib119)）。

受到单个 LLM 代理的令人印象深刻的能力启发，已经提出了 LLM 多代理系统（见图[14](https://arxiv.org/html/2407.19354v1#S6.F14 "Figure 14 ‣ 6.2.1\. The Development of LLM-MA System ‣ 6.2\. Large Language Model Multi-Agent System ‣ 6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")）。这些系统基于多个具有集体智能和专业技能的代理工作，在这种情况下，每个代理都专注于在特定领域表现出色。这种专业化使得问题解决可以采用分布式方法，每个代理贡献其独特的专业知识，从而增强系统的整体效果和效率。在这种情况下，多个自主代理协作进行计划、讨论和决策，密切类似于人类在解决任务时的群体协作。这种方法利用了 LLM 的沟通能力，使用其文本生成来进行交互和回应文本输入（Guo et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib31)）。

![参见标题](img/4ea9c30db839988ec7a6e4183b381114.png)

图 14\. LLM-MA 系统的架构

LLM-MA 系统的应用遍及多个领域，广泛分为两大类：问题解决和世界模拟（Guo et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib31)）。对于问题解决应用，如多机器人系统（Mandi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib64)）和软件开发（Du et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib24)），这些系统允许多样化代理之间的互动。这种协作能力有效地解决复杂的现实世界问题，类似于人类群体在解决复杂挑战时的合作方式。另一方面，世界模拟包括社会模拟（Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71)）和游戏模拟（Xu et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib113)）。本文呈现的案例研究部分展示了世界模拟应用于描绘 LLM 代理所面临的威胁及其影响，展示了 LLM-MA 系统应用的众多方面之一。

#### 6.2.2\. LLM-MA 系统的安全性和隐私研究

随着对 LLM-MA 系统研究的迅速增加，出现了许多挑战。多代理系统中的每个代理可能需要访问和处理敏感数据，甚至执行代码。这引发了关于多代理系统相关的安全和隐私问题的讨论。

多代理系统中的每个代理可能需要访问和处理敏感数据，甚至执行代码。此外，由于代理之间的相互通信和连接，单一代理引发的安全问题在多代理场景中可能会产生深远且放大的影响。这加剧了对多代理环境中安全和隐私问题的关注需求。

幻觉问题，即代理基于不正确或虚假的信息生成输出，是 LLM 和 LLM 代理面临的一个重大挑战。在多代理环境中，由于这些代理的互联性质及其频繁的通信，这一问题变得更加复杂。一个代理的信息错误可能会被网络中的其他代理接受并进一步传播，导致虚假信息的传播链。为缓解这一问题，必须在个体代理层面纠正错误，并管理代理之间的信息流，从而防止不准确的信息在整个系统中传播（Guo 等，[2024](https://arxiv.org/html/2407.19354v1#bib.bib31)）。

此外，LLM 多代理系统与文件交互和执行代码的能力提供了广泛的应用可能性。然而，系统中可能存在恶意 LLM 代理，这带来了重大风险。在一种情况下，这些代理可能以被动监听模式运作，接收其他代理共享的信息以执行任务，但同时故意向攻击者泄露机密信息。在另一种情况下，恶意 LLM 代理可能会以主动通信模式进行，传播病毒感染的文件、钓鱼信息或其他恶意代码，试图攻击或干扰系统中的其他代理。为降低这种风险，将人类反馈和用户授权纳入每一步可以帮助减少这些威胁。这要求系统设计必须具有强大的安全措施，以防止未经授权的访问或滥用。一种有效的方法是实施无状态的 oracle 代理，它可以监控每个敏感任务并评估是否构成恶意活动（Talebirad 和 Nadiri，[2023](https://arxiv.org/html/2407.19354v1#bib.bib85)）。

目前，LLM-MA 系统中的隐私和安全研究尚未受到广泛关注。然而，随着 LLM-MA 技术的快速发展，这些问题变得越来越突出。因此，迫切需要强有力的安全解决方案来应对这些新兴挑战。

## 7. 结论

在本次调查中，我们探讨了 LLM 代理面临的多面性安全和隐私挑战，包括威胁来源的两大类别：来自 LLM 的继承威胁和特定于代理的威胁。此外，我们还介绍了安全和隐私对人类、环境和其他代理造成的影响。基于这些，我们讨论了相应的防御策略。此外，我们还讨论了该领域的未来趋势。为了促进深入理解，我们通过一个虚拟小镇项目结合了各种案例研究。通过强调 LLM 代理面临的挑战，我们的目标是激发未来研究人员和开发人员对未来增强 LLM 代理安全和隐私的进一步研究和探索。

## 参考文献

+   (1)

+   Cha（2022 年）2022 年。*ChatGPT*。[`openai.com/chatgpt`](https://openai.com/chatgpt)

+   Gem（2023 年）2023 年。*Gemini-聊天以激发您的想法*。[`gemini.google.com`](https://gemini.google.com)

+   Mal (2023) Embrace The Red 2023. *恶意 ChatGPT 代理：GPTs 如何悄悄获取您的数据（演示）· Embrace The Red*。Embrace The Red。[`embracethered.com/blog/posts/2023/openai-custom-malware-gpt/`](https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/)

+   Wha（2023 年）Prompt Engineering 2023。*什么是大型语言模型（LLM）代理和自主代理*。Prompt Engineering。[`promptengineering.org/what-are-large-language-model-llm-agents/`](https://promptengineering.org/what-are-large-language-model-llm-agents/)

+   Int（2024a）2024a。*介绍 Meta Llama 3：迄今为止最有能力的开放 LLM*。[`ai.meta.com/blog/meta-llama-3/`](https://ai.meta.com/blog/meta-llama-3/)

+   Int（2024b）2024b。*介绍克劳德的下一代*。[`www.anthropic.com/news/claude-3-family`](https://www.anthropic.com/news/claude-3-family)

+   Abdelnabi 等（2023 年）Sahar Abdelnabi，Amr Gomaa，Sarath Sivaprasad，Lea Schönherr 和 Mario Fritz。2023 年。*LLM-Deliberation：通过交互式多代理协商游戏评估 LLM*。[`doi.org/10.48550/arXiv.2309.17234`](https://doi.org/10.48550/arXiv.2309.17234)arXiv:2309.17234

+   Aksitov 等（2023 年）Renat Aksitov，Chung-Ching Chang，David Reitter，Siamak Shakeri 和 Yunhsuan Sung。2023 年。*为检索增强的大型语言模型表征归因和流利度权衡*。[`doi.org/10.48550/arXiv.2302.05578`](https://doi.org/10.48550/arXiv.2302.05578)arXiv:2302.05578

+   Anderljung et al. (2023) Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, 和 Kevin Wolf. 2023. *前沿 AI 监管：管理公共安全的新兴风险*. [`doi.org/10.48550/arXiv.2307.03718`](https://doi.org/10.48550/arXiv.2307.03718) arXiv:2307.03718

+   Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, 和 Vitaly Shmatikov. 2023. *滥用图像和声音在多模态 LLMs 中进行间接指令注入*. [`doi.org/10.48550/arXiv.2307.10490`](https://doi.org/10.48550/arXiv.2307.10490) arXiv:2307.10490 [cs]

+   Baracaldo et al. (2017) Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, 和 Jaehoon Amir Safavi. 2017. 减轻对机器学习模型的毒害攻击：一种基于数据来源的方法。载于 *第 10 届 ACM 人工智能与安全研讨会* *(AISec ’17)*. 计算机协会，纽约，NY，USA, 103–110. [`doi.org/10.1145/3128572.3140450`](https://doi.org/10.1145/3128572.3140450)

+   Bran et al. (2023) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, 和 Philippe Schwaller. 2023. *ChemCrow：用化学工具增强大型语言模型*. [`doi.org/10.48550/arXiv.2304.05376`](https://doi.org/10.48550/arXiv.2304.05376) arXiv:2304.05376

+   Brundage et al. (2018) Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum S. Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, 和 Dario Amodei. 2018. 《人工智能的恶意使用：预测、预防和缓解》。*arXiv 预印本 arXiv:1802.07228* (2018). arXiv:1802.07228 [`arxiv.org/abs/1802.07228`](http://arxiv.org/abs/1802.07228)

+   Carlini et al. (2023) Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, 和 Florian Tramèr. 2023. *毒害网页规模训练数据集是可行的*. arXiv:2302.10149 [`arxiv.org/abs/2302.10149`](http://arxiv.org/abs/2302.10149)

+   Carlini 等（2021）Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea 和 Colin Raffel. 2021. 从大型语言模型中提取训练数据。在 *第 30 届 USENIX 安全研讨会（USENIX Security 21）*。2633–2650。 [`www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting`](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)

+   Charan 等（2023）P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand 和 Sandeep K. Shukla. 2023. *从文本到 MITRE 技术：探索大型语言模型生成网络攻击有效载荷的恶意使用*。 [`doi.org/10.48550/arXiv.2305.15336`](https://doi.org/10.48550/arXiv.2305.15336) arXiv:2305.15336

+   Das 等（2024）Badhan Chandra Das, M. Hadi Amini 和 Yanzhao Wu. 2024. *大型语言模型的安全性与隐私挑战：一项调查*。 [`doi.org/10.48550/arXiv.2402.00888`](https://doi.org/10.48550/arXiv.2402.00888) arXiv:2402.00888

+   Deng 等（2023）Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang 和 Yang Liu. 2023. *MasterKey: 自动化的多大型语言模型聊天机器人越狱*。arXiv:2307.08715 [`arxiv.org/abs/2307.08715`](http://arxiv.org/abs/2307.08715)

+   Deshpande 等（2023）Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan 和 Karthik Narasimhan. 2023. *ChatGPT 中的毒性：分析个性分配的语言模型*。 [`doi.org/10.48550/arXiv.2304.05335`](https://doi.org/10.48550/arXiv.2304.05335) arXiv:2304.05335

+   Dhuliawala 等（2023）Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz 和 Jason Weston. 2023. 验证链减少大型语言模型中的幻觉。*arXiv 预印本 arXiv:2309.11495*（2023）。 [`doi.org/10.48550/ARXIV.2309.11495`](https://doi.org/10.48550/ARXIV.2309.11495)

+   Dilmaghani 等（2019）Saharnaz Dilmaghani, Matthias R. Brust, Grégoire Danoy, Natalia Cassagnes, Johnatan Pecero 和 Pascal Bouvry. 2019. 人工智能系统中大数据的隐私与安全：研究和标准视角。在 *2019 IEEE 国际大数据会议（Big Data）*。5737–5743。 [`doi.org/10.1109/BigData47090.2019.9006283`](https://doi.org/10.1109/BigData47090.2019.9006283)

+   Dong 等（2023）Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik 和 Zhou Yu. 2023. 朝着下一代智能助手迈进，利用 LLM 技术。在 *第 29 届 ACM SIGKDD 知识发现与数据挖掘会议*（纽约，美国）*(KDD ’23)*。计算机协会，5792–5793。 [`doi.org/10.1145/3580305.3599572`](https://doi.org/10.1145/3580305.3599572)

+   Du et al. (2024) Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, 和 Cheng Yang. 2024. *通过跨团队协作进行多代理软件开发*. [`doi.org/10.48550/arXiv.2406.08979`](https://doi.org/10.48550/arXiv.2406.08979) arXiv:2406.08979

+   Ebrahimi et al. (2021) Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William Gan, Joseph E. Gonzalez, Marcus Rohrbach, 和 Trevor Darrell. 2021. 为正确的原因记忆：解释减少灾难性遗忘. *应用 AI 通讯* 2, 4 (2021), e44. [`doi.org/10.1002/ail2.44`](https://doi.org/10.1002/ail2.44)

+   Falade (2023) Polra Victor Falade. 2023. 解码威胁景观：ChatGPT、FraudGPT 和 WormGPT 在社会工程攻击中的作用. *国际计算机科学、工程和信息技术科学研究杂志* 9, 5 (2023), 185–198. [`doi.org/10.32628/CSEIT2390533`](https://doi.org/10.32628/CSEIT2390533)

+   Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, 和 Daniel Kang. 2024. *LLM 代理可以自主利用一天的漏洞*. [`doi.org/10.48550/arXiv.2404.08144`](https://doi.org/10.48550/arXiv.2404.08144) arXiv:2404.08144

+   Fornaciari et al. (2021) Tommaso Fornaciari, Federico Bianchi, Massimo Poesio, 和 Dirk Hovy. 2021. BERTective: 语言模型和上下文信息用于欺骗检测. 在 *第 16 届欧洲计算语言学协会会议：主要卷*（在线）. 计算语言学协会，2699–2708. [`doi.org/10.18653/v1/2021.eacl-main.232`](https://doi.org/10.18653/v1/2021.eacl-main.232)

+   Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, 和 Tao Jiang. 2023. *针对微调大语言模型的实际成员推断攻击通过自我提示校准*. [`doi.org/10.48550/arXiv.2311.06062`](https://doi.org/10.48550/arXiv.2311.06062) arXiv:2311.06062

+   Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, 和 Mario Fritz. 2023. *你未曾签署的：通过间接提示注入妥协现实世界的 LLM 集成应用*. arXiv:2302.12173 [`arxiv.org/abs/2302.12173`](http://arxiv.org/abs/2302.12173)

+   Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, 和 Xiangliang Zhang. 2024. *基于大语言模型的多代理：进展与挑战的调查*. [`doi.org/10.48550/arXiv.2402.01680`](https://doi.org/10.48550/arXiv.2402.01680) arXiv:2402.01680

+   Henderson et al. (2017) Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, 和 Joelle Pineau. 2017. *数据驱动对话系统中的伦理挑战*. [`doi.org/10.48550/arXiv.1711.09050`](https://doi.org/10.48550/arXiv.1711.09050) arXiv:1711.09050

+   Hines 等 (2024) Keegan Hines、Gary Lopez、Matthew Hall、Federico Zarfati、Yonatan Zunger 和 Emre Kiciman. 2024. *通过 Spotlighting 防御间接提示注入攻击*。 [`doi.org/10.48550/arXiv.2403.14720`](https://doi.org/10.48550/arXiv.2403.14720) arXiv:2403.14720

+   Hong 等 (2023) Sirui Hong、Mingchen Zhuge、Jonathan Chen、Xiawu Zheng、Yuheng Cheng、Ceyao Zhang、Jinlin Wang、Zili Wang、Steven Ka Shing Yau、Zijuan Lin、Liyang Zhou、Chenyu Ran、Lingfeng Xiao、Chenglin Wu 和 Jürgen Schmidhuber. 2023. *MetaGPT：多代理协作框架的元编程*。 [`doi.org/10.48550/arXiv.2308.00352`](https://doi.org/10.48550/arXiv.2308.00352) arXiv:2308.00352

+   Howard 和 Ruder (2018) Jeremy Howard 和 Sebastian Ruder. 2018. 语言模型的通用微调用于文本分类。在 *第 56 届计算语言学协会年会（第 1 卷：长篇论文）* (澳大利亚墨尔本)，Iryna Gurevych 和 Yusuke Miyao (编辑)。计算语言学协会, 328–339。 [`doi.org/10.18653/v1/P18-1031`](https://doi.org/10.18653/v1/P18-1031)

+   Hu 等 (2024b) Anwen Hu、Yaya Shi、Haiyang Xu、Jiabo Ye、Qinghao Ye、Ming Yan、Chenliang Li、Qi Qian、Ji Zhang 和 Fei Huang. 2024b. *mPLUG-PaperOwl：基于多模态大型语言模型的科学图表分析*。 [`doi.org/10.48550/arXiv.2311.18248`](https://doi.org/10.48550/arXiv.2311.18248) arXiv:2311.18248

+   Hu 等 (2024a) Hanxu Hu、Pinzhen Chen 和 Edoardo M. Ponti. 2024a. *使用顺序指令微调大型语言模型*。 [`doi.org/10.48550/arXiv.2403.07794`](https://doi.org/10.48550/arXiv.2403.07794) arXiv:2403.07794

+   Huang 等 (2024a) Jianheng Huang、Leyang Cui、Ante Wang、Chengyi Yang、Xinting Liao、Linfeng Song、Junfeng Yao 和 Jinsong Su. 2024a. *通过自我合成复习缓解大型语言模型中的灾难性遗忘*。 [`doi.org/10.48550/arXiv.2403.01244`](https://doi.org/10.48550/arXiv.2403.01244) arXiv:2403.01244

+   Huang 等 (2024c) Jiangyong Huang、Silong Yong、Xiaojian Ma、Xiongkun Linghu、Puhao Li、Yan Wang、Qing Li、Song-Chun Zhu、Baoxiong Jia 和 Siyuan Huang. 2024c. *3D 世界中的具身通用智能体*。 [`doi.org/10.48550/arXiv.2311.12871`](https://doi.org/10.48550/arXiv.2311.12871) arXiv:2311.12871

+   Huang 等 (2023) Lei Huang、Weijiang Yu、Weitao Ma、Weihong Zhong、Zhangyin Feng、Haotian Wang、Qianglong Chen、Weihua Peng、Xiaocheng Feng、Bing Qin 和 Ting Liu. 2023. *大型语言模型中的幻觉调查：原则、分类、挑战和开放问题*。arXiv:2311.05232 [`arxiv.org/abs/2311.05232`](http://arxiv.org/abs/2311.05232)

+   Huang 等 (2024b) Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, 和 Enhong Chen. 2024b. *理解 LLM 代理的规划：一项调查*. [`doi.org/10.48550/arXiv.2402.02716`](https://doi.org/10.48550/arXiv.2402.02716) arXiv:2402.02716

+   Hubinger 等 (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, 和 Ethan Perez. 2024. *卧底代理：训练在安全训练中持续存在的欺骗性 LLMs*. arXiv:2401.05566 [`arxiv.org/abs/2401.05566`](http://arxiv.org/abs/2401.05566)

+   Ishihara (2023) Shotaro Ishihara. 2023. *从预训练语言模型中提取训练数据：一项调查*. 收录于 *第三届可信自然语言处理研讨会 (TrustNLP 2023)* (加拿大多伦多)，Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop Kumar, 和 Rahul Gupta (编). 计算语言学协会，260–275. [`aclanthology.org/2023.trustnlp-1.23`](https://aclanthology.org/2023.trustnlp-1.23)

+   Jayaraman 等 (2023) Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, 和 David Evans. 2023. *凭证筛查：从智能回复中提取活动模式*. [`doi.org/10.48550/arXiv.2207.10802`](https://doi.org/10.48550/arXiv.2207.10802) arXiv:2207.10802

+   Ji 等 (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, 和 Pascale Fung. 2023. *通过自我反思缓解大型语言模型中的幻觉*. *arXiv 预印本 arXiv:2310.06271* (2023). [`arxiv.org/abs/2310.06271`](https://arxiv.org/abs/2310.06271)

+   Kandpal 等 (2024) Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, 和 Zheng Xu. 2024. *用户推断攻击对大型语言模型的影响*. [`doi.org/10.48550/arXiv.2310.09266`](https://doi.org/10.48550/arXiv.2310.09266) arXiv:2310.09266

+   Kim 等 (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, 和 Seong Joon Oh. 2023. *ProPILE: 探测大型语言模型中的隐私泄露*. arXiv:2307.01881 [`arxiv.org/abs/2307.01881`](http://arxiv.org/abs/2307.01881)

+   Kurita 等 (2020) Keita Kurita, Paul Michel, 和 Graham Neubig. 2020. *对预训练模型的权重中毒攻击*. arXiv:2004.06660 [`arxiv.org/abs/2004.06660`](http://arxiv.org/abs/2004.06660)

+   Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, 和 Nicholas Carlini. 2022. **去重训练数据使语言模型更好**。在 *第 60 届计算语言学协会年会（第 1 卷：长篇论文）*（都柏林，爱尔兰），Smaranda Muresan, Preslav Nakov, 和 Aline Villavicencio（编辑）。计算语言学协会，8424–8445。 [`doi.org/10.18653/v1/2022.acl-long.577`](https://doi.org/10.18653/v1/2022.acl-long.577)

+   Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Yongrae Jo, 和 Minjoon Seo. 2024. *火山：通过自我反馈引导修订减轻多模态幻觉*。 [`doi.org/10.48550/arXiv.2311.07362`](https://doi.org/10.48550/arXiv.2311.07362) arXiv:2311.07362

+   Lei et al. (2022) Yunjiao Lei, Dayong Ye, Sheng Shen, Yulei Sui, Tianqing Zhu, 和 Wanlei Zhou. 2022. **强化学习中的新挑战：安全与隐私综述**。*人工智能评论* 56, 7（2022），7195–7236。 [`doi.org/10.1007/s10462-022-10348-5`](https://doi.org/10.1007/s10462-022-10348-5)

+   Leng et al. (2024) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, 和 Lidong Bing. 2024. **通过视觉对比解码减轻大规模视觉语言模型中的物体幻觉**。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*。13872–13882。

+   Li et al. (2023b) Chenyang Li, Zhao Song, Weixin Wang, 和 Chiwun Yang. 2023b. *关于变换器中梯度泄漏攻击与防御的理论见解*。arXiv:2311.13624 [`arxiv.org/abs/2311.13624`](http://arxiv.org/abs/2311.13624)

+   Li et al. (2023a) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, 和 Yangqiu Song. 2023a. *对 ChatGPT 进行多步越狱隐私攻击*。arXiv:2304.05197 [`arxiv.org/abs/2304.05197`](http://arxiv.org/abs/2304.05197)

+   Lin et al. (2023) Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, 和 Qin Chen. 2023. *AgentSims：一个用于大型语言模型评估的开源沙箱*。 [`doi.org/10.48550/arXiv.2308.04026`](https://doi.org/10.48550/arXiv.2308.04026) arXiv:2308.04026

+   Liu et al. (2023a) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, 和 Cyril Zhang. 2023a. *揭示注意力故障的翻转语言建模*。 [`doi.org/10.48550/arXiv.2306.00946`](https://doi.org/10.48550/arXiv.2306.00946) arXiv:2306.00946

+   Liu et al. (2024a) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, 和 Lijuan Wang. 2024a. *通过鲁棒指令调整减轻大规模多模态模型中的幻觉*。 [`doi.org/10.48550/arXiv.2306.14565`](https://doi.org/10.48550/arXiv.2306.14565) arXiv:2306.14565

+   Liu 等人 (2024b) 刘晓耿、徐楠、陈慕豪和肖超伟。2024b。AutoDAN：在对齐的大型语言模型上生成隐蔽的破解提示。在*第十二届国际学习表征会议*上。[`openreview.net/forum?id=7Jwpw4qKkb`](https://openreview.net/forum?id=7Jwpw4qKkb)

+   Liu 等人 (2023b) 刘毅、邓歌磊、李跃康、王凯龙、张天伟、刘业鹏、王浩宇、郑燕和刘杨。2023b。*针对 LLM 集成应用的提示注入攻击*。[`doi.org/10.48550/arXiv.2306.05499`](https://doi.org/10.48550/arXiv.2306.05499) arXiv:2306.05499

+   Liu 等人 (2024c) 刘宇亮、杨标、刘强、张力、马志银、张硕和白翔。2024c。*TextMonkey：一种无需 OCR 的大规模多模态模型用于文档理解*。[`doi.org/10.48550/arXiv.2403.04473`](https://doi.org/10.48550/arXiv.2403.04473) arXiv:2403.04473

+   Luo 等人 (2023a) 罗军宇、肖操和马丰龙。2023a。大语言模型的零资源幻觉预防。*arXiv 预印本 arXiv:2309.02654* (2023)。 [`doi.org/10.48550/ARXIV.2309.02654`](https://doi.org/10.48550/ARXIV.2309.02654)

+   Luo 等人 (2023b) 罗云、杨振、孟凡东、李亚富、周洁和张跃。2023b。*大语言模型在持续微调过程中的灾难性遗忘实证研究*。arXiv:2308.08747 [`arxiv.org/abs/2308.08747`](http://arxiv.org/abs/2308.08747)

+   Mahmoud 和 Hajj (2022) Reem A. Mahmoud 和 Hazem Hajj。2022。多目标学习以克服时间序列应用中的灾难性遗忘。*ACM 知识发现数据事务* 16, 6 (2022), 1–20。[`doi.org/10.1145/3502728`](https://doi.org/10.1145/3502728)

+   Mandi 等人 (2023) 赵曼迪、施蕾娅·贾因和宋书然。2023。*RoCo：与大型语言模型的辩证多机器人协作*。[`doi.org/10.48550/arXiv.2307.04738`](https://doi.org/10.48550/arXiv.2307.04738) arXiv:2307.04738

+   Mendis 等人 (2007) D. S. Kalana Mendis、Asoka S. Karunananda、Udaya Samaratunga 和 Uditha Ratnayake。2007。灾难管理的常识知识建模系统开发方法。28, 2 (2007), 179–196。[`doi.org/10.1007/s10462-009-9097-6`](https://doi.org/10.1007/s10462-009-9097-6)

+   Mondesire 和 Wiegand (2023) 肖恩·蒙德赛尔和 R·保罗·维根德。2023。通过补充层次学习减轻灾难性遗忘。*电子学* 12, 3 (2023), 706。[`doi.org/10.3390/electronics12030706`](https://doi.org/10.3390/electronics12030706)

+   摩尔等（2023）迈克尔·摩尔、钱黄、雪莉·吴、安智广、雅什·达尔米亚、朱雷·列斯科维奇、西里尔·扎卡、爱德华多·庞特斯·雷斯和普拉纳夫·拉朱尔卡尔。2023。Med-Flamingo：一个多模态医学少样本学习者。发表于*第 3 届健康机器学习研讨会论文集*（*机器学习研究论文集，第 225 卷*）。PMLR，353–367。 [`proceedings.mlr.press/v225/moor23a.html`](https://proceedings.mlr.press/v225/moor23a.html)

+   OpenAI 等（2024）OpenAI、乔什·阿赫亚姆、史蒂文·阿德勒、桑迪尼·阿加瓦尔、拉玛·艾哈迈德、伊尔格·阿卡亚、弗洛伦西亚·莱奥尼·阿莱曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、萨姆·奥特曼、施亚马尔·安德卡特、雷德·阿维拉、伊戈尔·巴布什金、苏奇尔·巴拉吉和巴尔科姆。2024。*GPT-4 技术报告*。 [`doi.org/10.48550/arXiv.2303.08774`](https://doi.org/10.48550/arXiv.2303.08774) arXiv:2303.08774

+   奥兹代伊等（2023）穆斯塔法·萨法·奥兹代伊、查里斯·佩里斯、杰克·菲茨杰拉德、克里斯托夫·杜普伊、吉米特·马朱达尔、海达尔·汗、拉希尔·帕里克和拉胡尔·古普塔。2023。*通过提示调优控制从大语言模型中提取记忆数据*。arXiv:2305.11759 [`arxiv.org/abs/2305.11759`](https://arxiv.org/abs/2305.11759)

+   庞等（2024）姜成庞、恒博·范、彭远·王、贾昊·肖、南唐、思航·杨、程星·贾、盛军·黄和杨宇。2024。*通过主动探询赋能语言模型以获得更深刻的理解*。 [`doi.org/10.48550/arXiv.2402.03719`](https://doi.org/10.48550/arXiv.2402.03719) arXiv:2402.03719

+   朴等（2023b）郑尚朴、约瑟夫·C·奥布莱恩、凯瑞·J·蔡、梅雷迪思·林格尔·莫里斯、彭西·梁和迈克尔·S·伯恩斯坦。2023b。*生成代理：人类行为的互动模拟体*。 [`doi.org/10.48550/arXiv.2304.03442`](https://doi.org/10.48550/arXiv.2304.03442) arXiv:2304.03442

+   朴等（2023a）彼得·S·朴、西蒙·戈德斯坦、艾丹·奥加拉、迈克尔·陈和丹·亨德里克斯。2023a。*AI 欺骗：实例、风险和潜在解决方案的调查*。 [`doi.org/10.48550/arXiv.2308.14752`](https://doi.org/10.48550/arXiv.2308.14752) arXiv:2308.14752

+   彭等（2023）梁祖鹏、巴黎斯·吉安普拉斯和雷内·维达尔。2023。《理想的持续学习者：一个永不忘记的智能体》。发表于*第 40 届国际机器学习大会论文集*。PMLR，27585–27610。 [`proceedings.mlr.press/v202/peng23a.html`](https://proceedings.mlr.press/v202/peng23a.html)

+   彼得斯（2023）杰伊·彼得斯。2023。*必应 AI 机器人一直在秘密运行 GPT-4*。The Verge。 [`www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm`](https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm)

+   Puig et al. (2023) Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, 和 Roozbeh Mottaghi. 2023. *Habitat 3.0: 人类、化身与机器人共同栖息的环境*. [`doi.org/10.48550/arXiv.2310.13724`](https://doi.org/10.48550/arXiv.2310.13724) arXiv:2310.13724

+   Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, 和 Prateek Mittal. 2023. *视觉对抗样本破解对齐的大型语言模型*. [`doi.org/10.48550/arXiv.2306.13213`](https://doi.org/10.48550/arXiv.2306.13213) arXiv:2306.13213

+   Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, 和 Maosong Sun. 2024. *ChatDev: 用于软件开发的交互式代理*. [`doi.org/10.48550/arXiv.2307.07924`](https://doi.org/10.48550/arXiv.2307.07924) arXiv:2307.07924

+   Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, 和 George J. Pappas. 2023. SmoothLLM: 防御大型语言模型免受破解攻击。*arXiv 预印本 arXiv:2310.03684* (2023). [`doi.org/10.48550/ARXIV.2310.03684`](https://doi.org/10.48550/ARXIV.2310.03684)

+   Ruan et al. (2024) Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, 和 Tatsunori Hashimoto. 2024. *通过语言模型模拟沙盒识别 LM 代理的风险*. [`doi.org/10.48550/arXiv.2309.15817`](https://doi.org/10.48550/arXiv.2309.15817) arXiv:2309.15817

+   Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, 和 Vitaly Shmatikov. 2021. 你自动完成我：神经代码完成中的毒害漏洞。发表于 *第 30 届 USENIX 安全研讨会 (USENIX Security 21)*. 1559–1575. [`www.usenix.org/conference/usenixsecurity21/presentation/schuster`](https://www.usenix.org/conference/usenixsecurity21/presentation/schuster)

+   Shayegani et al. (2023) Erfan Shayegani, Yue Dong, 和 Nael Abu-Ghazaleh. 2023. *破解碎片：对多模态语言模型的组成对抗攻击*. [`doi.org/10.48550/arXiv.2307.14539`](https://doi.org/10.48550/arXiv.2307.14539) arXiv:2307.14539

+   Shejwalkar 和 Houmansadr (2021) Virat Shejwalkar 和 Amir Houmansadr. 2021. 通过知识转移保护机器学习模型的成员隐私。35, 11 (2021), 9549–9557. 第 11 期. [`doi.org/10.1609/aaai.v35i11.17150`](https://doi.org/10.1609/aaai.v35i11.17150)

+   Shen et al. (2023) 沈心月、陈泽源、迈克尔·巴克斯、沈云和张杨。2023。*“现在做任何事”：表征和评估大型语言模型在实际环境中的越狱提示*。arXiv:2308.03825 [`arxiv.org/abs/2308.03825`](http://arxiv.org/abs/2308.03825)

+   Sun et al. (2023) 赵青、沈胜、曹盛超、刘浩天、李春远、沈亦康、甘创、桂梁岩、王宇雄、杨一鸣、库尔特·克佐特和特雷弗·达雷尔。2023。*将大型多模态模型与事实增强的 RLHF 对齐*。[`doi.org/10.48550/arXiv.2309.14525`](https://doi.org/10.48550/arXiv.2309.14525) arXiv:2309.14525

+   Talebirad and Nadiri (2023) 亚沙尔·塔勒比拉德和阿米尔霍赛因·纳迪里。2023。*多代理协作：利用智能 LLM 代理的力量*。[`doi.org/10.48550/arXiv.2306.03314`](https://doi.org/10.48550/arXiv.2306.03314) arXiv:2306.03314

+   Taveekitworachai et al. (2023) 皮塔瓦特·塔维基特沃拉猜、费布里·阿卜杜拉、穆斯塔法·坎·古尔塞利、穆里·F·德万托罗、陈思远、安东尼奥·拉纳塔、安德里亚·瓜津尼和鲁克·塔沃纳斯。2023。《打破坏境：揭示用户输入对 ChatGPT 游戏故事生成的影响和风险》。发表于*互动叙事*（尚）*(计算机科学讲义)*，莉莎·霍洛威-阿塔维和约翰·T·穆雷（编）。施普林格自然瑞士，285–296。

+   Toetzke et al. (2023) 马尔特·托兹克、贝内迪克特·普罗布斯特和斯特凡·费厄里戈尔。2023。利用大型语言模型监控气候技术创新。*环境研究快报* 18, 9 (2023), 091004。[`doi.org/10.1088/1748-9326/acf233`](https://doi.org/10.1088/1748-9326/acf233)

+   Tong et al. (2024) 孟通、陈克江、张杰、邱源、张伟明、余能海、张天伟和张志坤。2024。*InferDPT: 用于黑箱大型语言模型的隐私保护推理*。[`doi.org/10.48550/arXiv.2310.12214`](https://doi.org/10.48550/arXiv.2310.12214) arXiv:2310.12214

+   Truong et al. (2021) 让-巴普蒂斯特·真龙、普拉提尤什·梅尼、罗伯特·J·沃尔斯和尼古拉斯·帕佩尔诺特。2021。《无数据模型提取》。发表于*2021 IEEE/CVF 计算机视觉与模式识别会议（CVPR）*。4769–4778。[`ieeexplore.ieee.org/document/9577784`](https://ieeexplore.ieee.org/document/9577784)

+   Vakili et al. (2022) 托马斯·瓦基里、阿纳斯塔西奥斯·兰普劳迪斯、阿龙·亨里克松和赫拉克勒斯·达利安尼斯。2022。《使用自动去标识化临床数据进行预训练的 BERT 模型在下游任务中的表现》。发表于*第十三届语言资源与评估会议*（马赛，法国），妮可莱塔·卡尔佐拉里、弗雷德里克·贝歇、菲利普·布拉什、哈立德·朱克里、克里斯托弗·西埃里、蒂埃里·德克雷克、萨拉·戈吉、石原仁、贝恩特·麦戈德、约瑟夫·马里安尼、埃伦·马佐、简·奥迪克和斯特利奥斯·皮佩里迪斯（编）。欧洲语言资源协会，4245–4252。 [`aclanthology.org/2022.lrec-1.451`](https://aclanthology.org/2022.lrec-1.451)

+   Vander Eeckt 和 Van Hamme（2023）Steven Vander Eeckt 和 Hugo Van Hamme. 2023. 权重平均：一种简单而有效的方法来克服自动语音识别中的灾难性遗忘. 见 *ICASSP 2023 - 2023 IEEE 国际声学、语音和信号处理会议（ICASSP）*（希腊罗德岛）。IEEE, 1–5. [`doi.org/10.1109/ICASSP49357.2023.10095147`](https://doi.org/10.1109/ICASSP49357.2023.10095147)

+   Wan 等（2023a）Alexander Wan, Eric Wallace, Sheng Shen, 和 Dan Klein. 2023a. *在指令调优过程中对语言模型进行攻击*. arXiv:2305.00944 [`arxiv.org/abs/2305.00944`](http://arxiv.org/abs/2305.00944)

+   Wan 等（2023b）Alexander Wan, Eric Wallace, Sheng Shen, 和 Dan Klein. 2023b. *在指令调优过程中对语言模型进行攻击*. [`doi.org/10.48550/arXiv.2305.00944`](https://doi.org/10.48550/arXiv.2305.00944) arXiv:2305.00944

+   Wang 和 Li（2023）Huan Wang 和 Yan-Fu Li. 2023. 基于领域特定知识库的大型语言模型在工业设备操作和维护中的应用. 见 *2023 年第五届系统可靠性与安全工程国际会议（SRSE）*. 474–479. [`doi.org/10.1109/SRSE59585.2023.10336112`](https://doi.org/10.1109/SRSE59585.2023.10336112)

+   Wang 等（2024b）Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, 和 Jitao Sang. 2024b. *移动代理：具有视觉感知的自主多模态移动设备代理*. [`doi.org/10.48550/arXiv.2401.16158`](https://doi.org/10.48550/arXiv.2401.16158) arXiv:2401.16158

+   Wang 等（2023c）Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, 和 Yelong Shen. 2023c. *通过通信适应 LLM 代理*. [`doi.org/10.48550/arXiv.2310.01444`](https://doi.org/10.48550/arXiv.2310.01444) arXiv:2310.01444

+   Wang 等（2023d）Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, 和 Ji-Rong Wen. 2023d. *基于大型语言模型的自主代理调查*. arXiv:2308.11432 [`arxiv.org/abs/2308.11432`](http://arxiv.org/abs/2308.11432)

+   Wang 等（2024c）Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, 和 Ji-Rong Wen. 2024c. *基于大型语言模型的用户行为模拟*. [`doi.org/10.48550/arXiv.2306.02552`](https://doi.org/10.48550/arXiv.2306.02552) arXiv:2306.02552

+   Wang 等（2023b）Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, 和 Gao Huang. 2023b. *阿瓦隆的思想游戏：通过递归反思对抗欺骗*. [`doi.org/10.48550/arXiv.2310.01320`](https://doi.org/10.48550/arXiv.2310.01320) arXiv:2310.01320

+   Wang 等（2024a）沈王、田龙徐、杭李、超力张、乔琳梁、季良唐、菲利普·S·余和青松温。2024a。*教育中的大型语言模型：调查与展望*。arXiv:2403.18105 [`arxiv.org/abs/2403.18105`](https://arxiv.org/abs/2403.18105)

+   Wang 等（2024d）尚王、田青朱、博刘、明丁、徐郭、大勇叶、万雷周和菲利普·S·余。2024d。*大型语言模型的独特安全与隐私威胁：全面调查*。 [`doi.org/10.48550/arXiv.2406.07973`](https://doi.org/10.48550/arXiv.2406.07973) arXiv:2406.07973

+   Wang 等（2023a）天宇王、毅凡李、海涛林、向阳薛和彦伟傅。2023a。*WALL-E：大型语言模型驱动的具身机器人服务员负重举升*。 [`doi.org/10.48550/arXiv.2308.15962`](https://doi.org/10.48550/arXiv.2308.15962) arXiv:2308.15962

+   Wang 等（2023e）云涛王、杨赫潘、苗燕、周苏和汤·H·阮。2023e。关于 ChatGPT 的调查：AI 生成内容、挑战与解决方案。*IEEE 计算机学会开放期刊* 4（2023），280–302。 [`doi.org/10.1109/OJCS.2023.3300321`](https://doi.org/10.1109/OJCS.2023.3300321)

+   Wang 等（2023g）余飞王、万俊钟、梁友李、飞米、兴山曾、文勇黄、丽峰尚、新江和群刘。2023g。*与人类对齐的大型语言模型：调查*。 [`doi.org/10.48550/arXiv.2307.12966`](https://doi.org/10.48550/arXiv.2307.12966) arXiv:2307.12966

+   Wang 等（2023f）振华王、伟谢、凯陈、宝生王、志文桂和恩泽王。2023f。*自我欺骗：逆向穿透大型语言模型的语义防火墙*。arXiv:2308.11521 [`arxiv.org/abs/2308.11521`](http://arxiv.org/abs/2308.11521)

+   Winata 等（2023）辛塔·印德拉·温纳、凌觉谢、卡尔蒂克·拉达克里什南、世杰吴、西森金、彭翔程、玛扬·库尔卡尼和丹尼尔·普雷奥提克-皮特罗。2023。克服大规模多语言持续学习中的灾难性遗忘。在*计算语言学协会会议成果：ACL 2023*中。计算语言学协会，多伦多，加拿大，768–777。 [`doi.org/10.18653/v1/2023.findings-acl.48`](https://doi.org/10.18653/v1/2023.findings-acl.48)

+   Wu 等（2023）晨飞吴、盛名尹、伟震齐、晓东王、泽成唐和楠段。2023。*Visual ChatGPT：使用视觉基础模型进行对话、绘图和编辑*。 [`doi.org/10.48550/arXiv.2303.04671`](https://doi.org/10.48550/arXiv.2303.04671) arXiv:2303.04671

+   Xi et al. (2023) 只恒西、文翔陈、辛郭、伟何、怡文丁、博洋洪、明张、俊哲王、森杰金、恩宇周、瑞郑、晓然范、肖王、利毛熊、余浩周、伟然王、常浩姜、毅城邹、向阳刘、张月尹、士翰豆、荣祥翁、文森程、齐张、文娟秦、永岩郑、西鹏丘、宣靖黄和陶桂。2023 年。*基于大型语言模型的代理的崛起与潜力：综述*。[`doi.org/10.48550/arXiv.2309.07864`](https://doi.org/10.48550/arXiv.2309.07864) arXiv:2309.07864

+   Xie et al. (2024) 俊林谢、志红陈、瑞飞张、项万和关宾李。2024 年。*大型多模态代理：综述*。[`doi.org/10.48550/arXiv.2402.15116`](https://doi.org/10.48550/arXiv.2402.15116) arXiv:2402.15116

+   Xu et al. (2023c) 恒徐、天青朱、乐峰张、万磊周和菲利普·S·余。2023c。机器遗忘：综述。*ACM 计算机调查* 56, 1 (2023), 9:1–9:36。[`doi.org/10.1145/3603620`](https://doi.org/10.1145/3603620)

+   Xu et al. (2023a) 林徐、智远胡、大全周、洪宇任、臻董、柯特·凯特泽、司琼·吴和嘉士风。2023a。*MAgIC：大语言模型驱动的多代理在认知、适应性、理性和合作中的调查*。[`doi.org/10.48550/arXiv.2311.08562`](https://doi.org/10.48550/arXiv.2311.08562) arXiv:2311.08562

+   Xu et al. (2023b) 吕晓伟徐、简王吴、贾伟彭、志林龚、明蔡和天翔王。2023b。通过结构信息提升语言表示以实现自然语言理解。在*第 61 届计算语言学协会年会论文集（第一卷：长篇论文）*（加拿大多伦多）。计算语言学协会，4685–4705。[`doi.org/10.18653/v1/2023.acl-long.258`](https://doi.org/10.18653/v1/2023.acl-long.258)

+   Xu et al. (2024) 朱磊徐、赵宇、费芳、余旺和易吴。2024 年。*使用强化学习的语言代理在狼人游戏中的战略玩法*。[`doi.org/10.48550/arXiv.2310.18940`](https://doi.org/10.48550/arXiv.2310.18940) arXiv:2310.18940

+   Yan et al. (2023) 吕燕、卓张、关宏涛、开元张、轩陈、光宇申和向宇张。2023 年。*ParaFuzz：一种基于可解释性的技术用于检测 NLP 中的毒样本*。[`doi.org/10.48550/arXiv.2308.02122`](https://doi.org/10.48550/arXiv.2308.02122) arXiv:2308.02122

+   Yang et al. (2023b) 郝淼杨、昆兰向、孟宇葛、宏伟李、荣兴卢和水宇。2023b。*大型语言模型在通信网络中的后门攻击全面概述*。[`doi.org/10.48550/arXiv.2308.14367`](https://doi.org/10.48550/arXiv.2308.14367) arXiv:2308.14367

+   杨等人（2024b）吉汉·杨、润宇·丁、埃利斯·布朗、晓娟·齐和赛宁·谢。2024b。*V-IRL: 将虚拟智能基础建立在现实生活中*。[`doi.org/10.48550/arXiv.2402.03310`](https://doi.org/10.48550/arXiv.2402.03310) arXiv:2402.03310

+   杨等人（2024a）文凯·杨、晓涵·毕、彦凯·林、思硕·陈、杰·周和旭·孙。2024a。*注意你的代理！调查 LLM 基础代理的后门威胁*。[`doi.org/10.48550/arXiv.2402.11208`](https://doi.org/10.48550/arXiv.2402.11208) arXiv:2402.11208

+   杨等人（2023a）郑远·杨、林杰·李、剑锋·王、凯文·林、艾尚·阿扎尔纳萨布、费萨尔·艾哈迈德、自成·刘、策·刘、迈克尔·曾和莉娟·王。2023a。*MM-REACT: 提示 ChatGPT 进行多模态推理和行动*。[`doi.org/10.48550/arXiv.2303.11381`](https://doi.org/10.48550/arXiv.2303.11381) arXiv:2303.11381

+   姚等人（2024）舜宇·姚、滇·于、杰弗瑞·赵、伊扎克·沙弗兰、托马斯·L·格里菲斯、元·曹和卡尔蒂克·纳拉西曼。2024。*思维树：与大型语言模型的深思熟虑问题解决*。在*第 37 届国际神经信息处理系统会议论文集*（纽约红钩，纽约，美国）（NIPS ’23）。Curran Associates Inc.，11809–11822。

+   姚等人（2023）艺凡·姚、金浩·段、凯迪·徐、元芳·蔡、艾瑞克·孙和岳·张。2023。*大型语言模型（LLM）安全与隐私调查：优点、缺点与丑陋*。[`doi.org/10.48550/arXiv.2312.02003`](https://doi.org/10.48550/arXiv.2312.02003) arXiv:2312.02003

+   叶等人（2024）大勇·叶、天庆·朱、聪聪·朱、德睿·王、泽伟·施、盛·沈、万磊·周和敏慧·薛。2024。*强化学习中的遗忘*。[`doi.org/10.48550/arXiv.2312.15910`](https://doi.org/10.48550/arXiv.2312.15910) arXiv:2312.15910

+   易等人（2023）静伟·易、跃祺·谢、宾·朱、基根·海恩斯、艾姆雷·基西曼、广中·孙、兴·谢和方钊·吴。2023。*对大型语言模型的间接提示注入攻击进行基准测试和防御*。[`doi.org/10.48550/arXiv.2312.14197`](https://doi.org/10.48550/arXiv.2312.14197) arXiv:2312.14197

+   尹等人（2023a）舒康·尹、朝优·傅、思瑞·赵、柯·李、兴·孙、彭·徐和恩宏·陈。2023a。*多模态大型语言模型调查*。*arXiv 预印本 arXiv:2306.13549*（2023）。[`doi.org/10.48550/ARXIV.2306.13549`](https://doi.org/10.48550/ARXIV.2306.13549)

+   尹等人（2023b）舒康·尹、朝优·傅、思瑞·赵、彭·徐、浩·王、甸博·隋、云航·沈、柯·李、兴·孙和恩宏·陈。2023b。*Woodpecker: 多模态大型语言模型的幻觉修正*。[`doi.org/10.48550/arXiv.2310.16045`](https://doi.org/10.48550/arXiv.2310.16045) arXiv:2310.16045

+   Yu 等（2021）Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin 和 Huishuai Zhang。2021。**差分隐私语言模型的微调**。在 *国际学习表征会议* 上。 [`openreview.net/forum?id=Q42f0dfjECO`](https://openreview.net/forum?id=Q42f0dfjECO)

+   Yu 等（2023）Jiahao Yu, Xingwei Lin, Zheng Yu 和 Xinyu Xing。2023。*GPTFUZZER：通过自动生成的越狱提示对大型语言模型进行红队测试*。arXiv:2309.10253 [`arxiv.org/abs/2309.10253`](http://arxiv.org/abs/2309.10253)

+   Zhai 等（2024）Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li 和 Manling Li。2024。*HallE-Control：控制大型多模态模型中的对象幻觉*。 [`doi.org/10.48550/arXiv.2310.01779`](https://doi.org/10.48550/arXiv.2310.01779) arXiv:2310.01779

+   Zhai 等（2023）Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee 和 Yi Ma。2023。*调查多模态大型语言模型中的灾难性遗忘*。 [`doi.org/10.48550/arXiv.2309.10313`](https://doi.org/10.48550/arXiv.2309.10313) arXiv:2309.10313

+   Zhan 等（2024）Qiusi Zhan, Zhixiang Liang, Zifan Ying 和 Daniel Kang。2024。*InjecAgent：在工具集成的大型语言模型代理中基准测试间接提示注入*。 [`doi.org/10.48550/arXiv.2403.02691`](https://doi.org/10.48550/arXiv.2403.02691) arXiv:2403.02691

+   Zhang 等（2024b）Shuo Zhang, Liangming Pan, Junzhou Zhao 和 William Yang Wang。2024b。*知识对齐问题：为大型语言模型弥合人类和外部知识*。arXiv:2305.13669 [`arxiv.org/abs/2305.13669`](https://arxiv.org/abs/2305.13669)

+   Zhang 等（2023a）Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang 和 Weidi Xie。2023a。*PMC-VQA：用于医学视觉问答的视觉指令调优*。 [`doi.org/10.48550/arXiv.2305.10415`](https://doi.org/10.48550/arXiv.2305.10415) arXiv:2305.10415

+   Zhang 等（2024a）Zhiping Zhang, Michelle Jia, Hao-Ping (Hank) Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang 和 Tianshi Li。2024a。“这是一个公平的游戏”，还是不是？审查用户在使用基于 LLM 的对话代理时如何权衡披露风险和收益。发表于 *CHI 计算机系统人因会议论文集*（纽约，NY，USA）*(CHI ’24)*。计算机协会，1–26。 [`doi.org/10.1145/3613904.3642385`](https://doi.org/10.1145/3613904.3642385)

+   Zhang 等（2023b）Zhexin Zhang, Junxiao Yang, Pei Ke 和 Minlie Huang。2023b。通过目标优先级来防御大型语言模型的越狱攻击。 *arXiv 预印本 arXiv:2311.09096*（2023）。 [`doi.org/10.48550/ARXIV.2311.09096`](https://doi.org/10.48550/ARXIV.2311.09096)

+   Zheng et al. (2023) 郑青晓、徐中伟、阿布希纳夫·乔杜赫里、陈宇婷、李永明和黄云。2023 年。*协同人类-AI 代理：基于大语言模型的服务共创的 23 条启发式指南*。 [`doi.org/10.48550/arXiv.2310.15065`](https://doi.org/10.48550/arXiv.2310.15065) arXiv:2310.15065

+   Zhong et al. (2023) 钟婉君、郭良红、高琪琪、叶鹤和王艳林。2023 年。*MemoryBank：通过长期记忆增强大语言模型*。 [`doi.org/10.48550/arXiv.2305.10250`](https://doi.org/10.48550/arXiv.2305.10250) arXiv:2305.10250

+   Zou et al. (2024) 邹伟、耿润鹏、王炳辉和贾金远。2024 年。*PoisonedRAG：对大语言模型的检索增强生成的知识中毒攻击*。 [`doi.org/10.48550/arXiv.2402.07867`](https://doi.org/10.48550/arXiv.2402.07867) arXiv:2402.07867

生成于 2024 年 7 月 28 日星期日 00:22:05，由 LaTeXML![吉祥物萨米](http://dlmf.nist.gov/LaTeXML/) 创建
