<!--yml

分类: 未分类

日期: 2024-09-03 17:29:00

-->

# 有用 LLM 评估的调查

> 来源：[`arxiv.org/html/2406.00936`](https://arxiv.org/html/2406.00936)

1.  [1 介绍](https://arxiv.org/html/2406.00936v1#S1 "在 有用 LLM 评估的调查")

    1.  [1.1 人工智能与大语言模型](https://arxiv.org/html/2406.00936v1#S1.SS1 "在 1 介绍 ‣ 有用 LLM 评估的调查")

    1.  [1.2 为什么评估 LLM 很重要](https://arxiv.org/html/2406.00936v1#S1.SS2 "在 1 介绍 ‣ 有用 LLM 评估的调查")

    1.  [1.3 有用 LLM 的路线图](https://arxiv.org/html/2406.00936v1#S1.SS3 "在 1 介绍 ‣ 有用 LLM 评估的调查")

    1.  [1.4 研究概述](https://arxiv.org/html/2406.00936v1#S1.SS4 "在 1 介绍 ‣ 有用 LLM 评估的调查")

1.  [2 核心能力评估](https://arxiv.org/html/2406.00936v1#S2 "在 有用 LLM 评估的调查")

    1.  [2.1 推理](https://arxiv.org/html/2406.00936v1#S2.SS1 "在 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.1.1 逻辑推理](https://arxiv.org/html/2406.00936v1#S2.SS1.SSS1 "在 2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.1.2 数学推理](https://arxiv.org/html/2406.00936v1#S2.SS1.SSS2 "在 2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.1.3 常识推理](https://arxiv.org/html/2406.00936v1#S2.SS1.SSS3 "在 2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.1.4 多跳推理](https://arxiv.org/html/2406.00936v1#S2.SS1.SSS4 "在 2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.1.5 结构化数据推理](https://arxiv.org/html/2406.00936v1#S2.SS1.SSS5 "在 2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

    1.  [2.2 社会影响](https://arxiv.org/html/2406.00936v1#S2.SS2 "在 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.2.1 安全](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1 "在 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

            1.  [内容安全](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px1 "在 2.2.1 安全 ‣ 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

            1.  [安全性](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px2 "在 2.2.1 安全 ‣ 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

            1.  [伦理考虑](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px3 "在 2.2.1 安全 ‣ 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

        1.  [2.2.2 真实性](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2 "在 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

            1.  [幻觉](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2.Px1 "在 2.2.2 真实性 ‣ 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")

            1.  [偏见缓解](https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2.Px2 "在 2.2.2 真实度 ‣ 2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

    1.  [2.3 领域知识](https://arxiv.org/html/2406.00936v1#S2.SS3 "在 2 核心能力评估 ‣ 有用的 LLM 评估调查")

        1.  [2.3.1 财务](https://arxiv.org/html/2406.00936v1#S2.SS3.SSS1 "在 2.3 领域知识 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

        1.  [2.3.2 立法](https://arxiv.org/html/2406.00936v1#S2.SS3.SSS2 "在 2.3 领域知识 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

        1.  [2.3.3 心理学](https://arxiv.org/html/2406.00936v1#S2.SS3.SSS3 "在 2.3 领域知识 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

        1.  [2.3.4 医学](https://arxiv.org/html/2406.00936v1#S2.SS3.SSS4 "在 2.3 领域知识 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

        1.  [2.3.5 教育](https://arxiv.org/html/2406.00936v1#S2.SS3.SSS5 "在 2.3 领域知识 ‣ 2 核心能力评估 ‣ 有用的 LLM 评估调查")

1.  [3 代理评估](https://arxiv.org/html/2406.00936v1#S3 "在 有用的 LLM 评估调查")

    1.  [3.1 规划](https://arxiv.org/html/2406.00936v1#S3.SS1 "在 3 代理评估 ‣ 有用的 LLM 评估调查")

    1.  [3.2 应用场景](https://arxiv.org/html/2406.00936v1#S3.SS2 "在 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.1 网络基础](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

            1.  [搜索引擎](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1.Px1 "在 3.2.1 网络基础 ‣ 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

            1.  [在线购物](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1.Px2 "在 3.2.1 网络基础 ‣ 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.2 代码生成](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS2 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.3 数据库查询](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS3 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.4 API 调用](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS4 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.5 工具创建](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS5 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.6 机器人导航](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS6 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

        1.  [3.2.7 机器人操作](https://arxiv.org/html/2406.00936v1#S3.SS2.SSS7 "在 3.2 应用场景 ‣ 3 代理评估 ‣ 有用的 LLM 评估调查")

    1.  [3.3 基准测试](https://arxiv.org/html/2406.00936v1#S3.SS3 "在 3 代理评估 ‣ 有用的 LLM 评估调查")

1.  [4 未来方向](https://arxiv.org/html/2406.00936v1#S4 "在 有用的 LLM 评估调查")

    1.  [4.1 动态评估](https://arxiv.org/html/2406.00936v1#S4.SS1 "在 4 未来方向 ‣ 有用的 LLM 评估调查")

    1.  [4.2 LLMs 作为评估者](https://arxiv.org/html/2406.00936v1#S4.SS2 "在 4 未来方向 ‣ 有用的 LLM 评估调查")

    1.  [4.3 根本原因分析](https://arxiv.org/html/2406.00936v1#S4.SS3 "在 4 未来方向 ‣ 有用的 LLM 评估调查")

    1.  [4.4 细粒度 LLM 代理评估](https://arxiv.org/html/2406.00936v1#S4.SS4 "在 4 未来方向 ‣ 有用的 LLM 评估调查")

    1.  [4.5 机器人基准开发](https://arxiv.org/html/2406.00936v1#S4.SS5 "在 4 未来方向 ‣ 有用的 LLM 评估调查")

1.  [5 结论](https://arxiv.org/html/2406.00936v1#S5 "在 有用的 LLM 评估调查")

# 有用的 LLM 评估调查

彭吉伦^∗ 程思佳^∗ 邓艾吉^∗ 施永瑜^∗

陈博恒^∗ 林彦廷 陈云农

台湾大学，台北，台湾

{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw

{ytl, y.v.chen}ieee.org

###### 摘要

大型语言模型（LLMs）因其在各种复杂任务中的卓越表现而引起了各个研究领域的关注。因此，需要精细的方法来评估 LLMs 的能力，以确定它们应承担的任务和责任。我们的研究主要讨论了如何有效评估 LLMs 作为有用工具。我们提出了一个两阶段框架：从“核心能力”到“代理”，清楚地解释了 LLMs 如何基于其特定能力进行应用，并说明了每个阶段的评估方法。核心能力指的是 LLMs 为生成高质量自然语言文本所需的能力。在确认 LLMs 具备核心能力后，它们可以作为代理解决现实世界中的复杂任务。在“核心能力”阶段，我们讨论了 LLMs 的推理能力、社会影响和领域知识。在“代理”阶段，我们展示了 LLMs 代理应用的具体行动、规划和工具学习。最后，我们审视了当前评估 LLMs 方法面临的挑战以及未来的发展方向。¹¹1[`github.com/MiuLab/EvalLLM-Survey`](https://github.com/MiuLab/EvalLLM-Survey) ^*^*脚注：同等贡献。

有用的 LLM 评估调查

彭吉伦^∗ 程思佳^∗ 邓艾吉^∗ 施永瑜^∗ 陈博恒^∗ 林彦廷 陈云农 台湾大学，台北，台湾 {b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw {ytl, y.v.chen}ieee.org

## 1 引言

![参见说明](img/5ef62bde5736f5f34fe024e9df6e0e39.png)

图 1：我们 LLM 评估的两阶段框架。

### 1.1 人工智能与大型语言模型

人工智能（AI）模拟人类行为以完成多个需要人类智能的任务。最初的 AI 模型试图通过前馈和简单的输入输出函数模拟单个神经元的功能 Muthukrishnan et al. ([2020](https://arxiv.org/html/2406.00936v1#bib.bib95))。随着时间的推移，各种机器学习（ML）和深度学习（DL）模型已经被开发出来。它们不仅能够从大量数据中识别模式，还能够进行预测，甚至处理诸如文本、图像和音频等非结构化数据。最近，Transformer 架构 Vaswani et al. ([2017](https://arxiv.org/html/2406.00936v1#bib.bib144)) 的提出，使得词嵌入能够依赖于上下文，并且模型训练可以扩大规模 Min et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib93))。因此，研究人员逐渐增加了预训练语言模型中的参数，以期达到更好的性能。以生成预训练变换器（GPT）系列为例，模型复杂度和能力的进步标志着参数数量的显著增加：GPT-1 Radford et al. ([2018](https://arxiv.org/html/2406.00936v1#bib.bib110)) 具有 1.17 亿个参数，GPT-2 Radford et al. ([2019](https://arxiv.org/html/2406.00936v1#bib.bib111)) 将其扩展到 15 亿个参数，而 GPT-3 Mann et al. ([2020](https://arxiv.org/html/2406.00936v1#bib.bib87)) 更是增加到 1750 亿个参数。此外，OpenAI 发布的 GPT-4 模型具有更大的模型规模，可以接受图像和文本输入并产生文本输出，并在各种专业和学术基准测试中展现了接近人类的表现 Achiam et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib2))。上述模型由于其巨大的规模，被称为 LLMs。由于在广泛复杂任务上的卓越表现，它们在各种研究领域引起了关注。

### 1.2 为什么评估 LLMs 很重要

早期对模型智能进行测试的工作称为图灵测试，提出了机器是否能够模仿人类智能的问题，并使人们无法区分 Pinar Saygin 等人（[2000](https://arxiv.org/html/2406.00936v1#bib.bib105)）。评估人工智能至关重要，因为它帮助我们衡量人工智能系统的实际能力和限制。随着人工智能技术的进步，特别是在软件测试和结构工程等领域，它们有时能表现得比人类更好。然而，我们需要明确的基准，以确保这些技术既可靠又有效（Salehi 和 Burgueño [2018](https://arxiv.org/html/2406.00936v1#bib.bib117)）。随着 LLMs 的快速发展，需要细化的方法来评估 LLMs 的能力，以确定它们应该承担的任务和责任。因为 LLMs 表现出超越特定任务的广泛能力，例如预测人类编写文本的下一个单词（Nolfi [2023](https://arxiv.org/html/2406.00936v1#bib.bib98)），如正式的语言能力（Mahowald et al. [2023](https://arxiv.org/html/2406.00936v1#bib.bib84)），事实知识（Petroni et al. [2019](https://arxiv.org/html/2406.00936v1#bib.bib104)），甚至是心智理论技能（Kosinski [2023](https://arxiv.org/html/2406.00936v1#bib.bib62)），我们应该设计特定于每个任务或领域的基准或评估方法。在当前的基准中，LLMs 的综合能力通过涵盖多个领域的任务自动进行评估，例如 HELM（Liang et al. [2022](https://arxiv.org/html/2406.00936v1#bib.bib72)）和 BIG-Bench（Srivastava et al. [2022](https://arxiv.org/html/2406.00936v1#bib.bib131)），或者通过像 AlpacaFarm（Dubois et al. [2024](https://arxiv.org/html/2406.00936v1#bib.bib32)）和 MT-bench（Zheng et al. [2024](https://arxiv.org/html/2406.00936v1#bib.bib183)）这样的自动生成的人类反馈。然而，当 LLMs 被要求执行特定任务时，存在针对这些任务量身定制的评估方法的可能性。这允许在相同任务下比较不同模型的能力，以选择表现最佳的模型。在本研究中，我们对 LLMs 的不同能力进行了分类，系统地回顾了每个类别下现有的评估方法，并讨论了作为“有用”工具的 LLMs 应如何有效地进行评估。

### 1.3 有用 LLMs 的路线图

为了确定大语言模型（LLMs）是否能够成为有用的工具，我们应该将 LLMs 的能力分为“核心能力”和“代理”，并分别讨论它们。核心能力指的是 LLMs 生成高质量自然语言文本所需的能力，这是执行复杂行为的基础。

首先，LLMs 必须具备推理能力，因为在与人类互动时，它们需要逐步推导论点以进行有效讨论。此外，LLMs 的社会影响需要引起重大关注，因为 LLMs 必须被视为安全且值得信赖的，才能让人类相信并积极使用它们。最后，LLMs 应具备跨领域的知识，它们可以帮助人类解决各种领域的问题。

在确认 LLMs 具备这些核心能力后，我们可以利用 LLMs 执行复杂行为以处理现实世界中的问题，我们将其定义为代理。例如，LLMs 代理可以进行规划，生成明确的思考过程，通过预测预期结果来选择和组织行动 Ghallab et al. ([2004](https://arxiv.org/html/2406.00936v1#bib.bib38))。然后，LLMs 代理可以在各种场景中解决任务，如使用工具、创建工具、导航具身机器人等。

尽管 LLMs 可以展示上述能力，但仍需全面的评估方法以确保 LLMs 在执行每个任务时达到令人满意的水平。现有关于 LLMs 评估方法的文献，包括 Guo et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib40)) 和 Chang et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib17))，提供了对 LLMs 各个方面评估方法的全面回顾，但尚无研究提供阶段性框架来探讨 LLMs 的可用性。因此，本文提出了一个两阶段框架，以检验 LLMs 是否足够有用 ([Figure 1](https://arxiv.org/html/2406.00936v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Useful LLM Evaluation"))。

### 1.4 研究概述

在本研究中，我们首先介绍了 LLMs 核心能力的评估方法 ([Figure 2](https://arxiv.org/html/2406.00936v1#S2.F2 "Figure 2 ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"))，包括推理（5 个子章节）、社会影响（2 个子章节）和领域知识（5 个子章节）。然后，对于 LLMs 代理 ([Figure 3](https://arxiv.org/html/2406.00936v1#S3.F3 "Figure 3 ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"))，我们介绍了 LLMs 代理应用的评估方法，包括规划、应用场景（7 个子章节）和基准。在这些子章节中，我们介绍了 LLMs 的应用、评估方法和数据集。最后，我们提出了对 LLMs 可用性的观点，并建议了未来的方向和挑战。

本文的贡献如下：

1.  (1)

    我们提供了一个两阶段框架：从核心能力到代理，以检验 LLMs 是否足够有用。

1.  (2)

    在每一节中，我们阐明了 LLM 在特定能力方面的应用及其评估方法。此外，我们还提供了对 LLM 在这些领域当前表现水平的分析。

1.  (3)

    我们审视了当前在评估方法中面临的挑战，以及未来发展的方向。

## 2 核心能力评估

{森林}

对于树=生长=东，反转=true，锚点=基础西，父锚点=东，子锚点=西，基础=左，字体=，矩形，绘制，圆角，左对齐，内部 xsep=4pt，内部 ysep=1pt， ，其中级别=1 字体=，填充=粉色!50，其中级别=2 字体=，填充=绿色!10，其中级别=3 字体=，填充=灰色!20，[核心能力评估

(第[2 节](https://arxiv.org/html/2406.00936v1#S2 "2 核心能力评估 ‣ 有用 LLM 评估的调查")),填充=黄色!20,字体=[推理

(第[2.1 节](https://arxiv.org/html/2406.00936v1#S2.SS1 "2.1 推理 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")) [逻辑推理 [Weston et al. ([2015](https://arxiv.org/html/2406.00936v1#bib.bib155)), Bhagavatula et al. ([2019](https://arxiv.org/html/2406.00936v1#bib.bib10))] ] [数学推理 [Cobbe et al. ([2021](https://arxiv.org/html/2406.00936v1#bib.bib21)), Hendrycks et al. ([2021](https://arxiv.org/html/2406.00936v1#bib.bib42))] ] [常识推理 [Talmor et al. ([2018](https://arxiv.org/html/2406.00936v1#bib.bib138)), Mihaylov et al. ([2018](https://arxiv.org/html/2406.00936v1#bib.bib92))] ] [多跳推理 [Geva et al. ([2021](https://arxiv.org/html/2406.00936v1#bib.bib37)), Yang et al. ([2018](https://arxiv.org/html/2406.00936v1#bib.bib166))] ] [结构化数据推理 [Chen et al. ([2020](https://arxiv.org/html/2406.00936v1#bib.bib18)), Zhang et al. ([2018](https://arxiv.org/html/2406.00936v1#bib.bib180))] ] ] [社会影响

(第[2.2 节](https://arxiv.org/html/2406.00936v1#S2.SS2 "2.2 社会影响 ‣ 2 核心能力评估 ‣ 有用 LLM 评估的调查")) [安全性[Lin et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib77)), Kim et al. ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib61)), Yuan et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib174)), Scherrer et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib118))] ] [真实性 [Jiang et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib52)), Zhang et al. ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib178)), Hort et al. ([2021](https://arxiv.org/html/2406.00936v1#bib.bib44)), Zhang et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib176))] ] ] [领域知识

（第[2.3](https://arxiv.org/html/2406.00936v1#S2.SS3 "2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation")节）[金融 [Wu et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib157)), Xie et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib158)), Li et al. ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib69))] ] [立法 [Blair-Stanek et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib13)), Engel and Mcadams ([2024](https://arxiv.org/html/2406.00936v1#bib.bib33)), Liga and Robaldo ([2023](https://arxiv.org/html/2406.00936v1#bib.bib74)), Deroy et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib28)) ] ] [心理学 [Lu et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib83)), Demszky et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib25)), Demszky et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib25)) ] ] [医学 [Agrawal et al. ([2022](https://arxiv.org/html/2406.00936v1#bib.bib3)), Sharma and Thakur ([2023](https://arxiv.org/html/2406.00936v1#bib.bib124)), Benoit ([2023](https://arxiv.org/html/2406.00936v1#bib.bib9)), Kumar ([2023](https://arxiv.org/html/2406.00936v1#bib.bib63)), Thirunavukarasu et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib141)) ] ] [教育 [Abdelghani et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib1)), Jia et al. ([2021](https://arxiv.org/html/2406.00936v1#bib.bib51)), Menick et al. ([2022](https://arxiv.org/html/2406.00936v1#bib.bib90)), Dijkstra et al. ([2022](https://arxiv.org/html/2406.00936v1#bib.bib29)), Kasneci et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib57)) ] ] ] ]

图 2：核心能力评估概览。

对 LLMs 核心能力的评估彻底检查了其在三个基本维度上的语言能力：推理、社会影响和领域特定知识。这一基本评估强调了 LLMs 在[2.1](https://arxiv.org/html/2406.00936v1#S2.SS1 "2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation")部分复杂认知推理过程中的能力，强调其在[2.2](https://arxiv.org/html/2406.00936v1#S2.SS2 "2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation")部分对真实性和安全标准的承诺，以及在[2.3](https://arxiv.org/html/2406.00936v1#S2.SS3 "2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation")部分在应用广泛领域知识方面的娴熟程度。

通过确认大规模语言模型（LLMs）具备这些核心能力，我们认识到这些技能有可能发展成更复杂的行为。这一发展突显了 LLMs 作为先进应用工具的适应性和可扩展性，表明未来的重点将是进一步增强这些基础能力。

### 2.1 推理

推理能力使得人类和机器都能做出有根据的决策，得出逻辑结论，并熟练解决问题。近期的研究 (Huang and Chang, [2023](https://arxiv.org/html/2406.00936v1#bib.bib46); Sun et al., [2024](https://arxiv.org/html/2406.00936v1#bib.bib136)) 越来越强调在大语言模型中增强推理能力，旨在达到甚至超越人类水平的推理能力，特别是在专业领域。在本节中，我们将关注评估大语言模型的各种推理能力。推理任务可以分为以下几类：逻辑推理、数学推理、常识推理、多跳推理和结构化数据推理。

#### 2.1.1 逻辑推理

| 类型 | 示例来源 | 输入 | 答案 |
| --- | --- | --- | --- |
| 演绎推理 | bAbI-15 (Weston et al., [2015](https://arxiv.org/html/2406.00936v1#bib.bib155)) | 羊怕狼。猫怕狗。老鼠怕猫。格特鲁德是一只羊。格特鲁德怕什么？ | 狼 |
| 演绎推理 | bAbI-16 (Weston et al., [2015](https://arxiv.org/html/2406.00936v1#bib.bib155)) | 莉莉是一只天鹅。莉莉是白色的。伯恩哈德是绿色的。格雷格是一只天鹅。格雷格是什么颜色的？ | 白色 |
| 归纳推理 | $\alpha$-NLI (Bhagavatula et al., [2019](https://arxiv.org/html/2406.00936v1#bib.bib10)) | 观察 1：我走进了我的数学课。观察 2：我失败了。假设 1：我看到门旁的绳子。假设 2：我没有为考试复习。 | 假设 2 |

表 1：不同类型逻辑推理的示例。

基于哲学和逻辑学的概念，逻辑推理可以进一步分为三种类型：1) 归纳推理涉及基于特定实例中的观察模式或规律推断一般结论。bAbI-15 (Weston et al., [2015](https://arxiv.org/html/2406.00936v1#bib.bib155)) 和 EntailmentBank (Dalvi et al., [2021](https://arxiv.org/html/2406.00936v1#bib.bib22)) 是常见的归纳推理基准。2) 演绎推理是根据已知前提和逻辑规则得出必要结论的过程。bAbI-16 (Weston et al., [2015](https://arxiv.org/html/2406.00936v1#bib.bib155)) 是测试演绎推理的常见基准。3) 归纳推理是一种基于给定观察和已知信息推断可能解释或假设的推理形式。$\alpha$-NLI、$\alpha$-NLG (Bhagavatula et al., [2019](https://arxiv.org/html/2406.00936v1#bib.bib10)) 和 AbductiveRules (Young et al., [2022](https://arxiv.org/html/2406.00936v1#bib.bib171)) 是几个归纳推理的基准。表 [1](https://arxiv.org/html/2406.00936v1#S2.T1 "Table 1 ‣ 2.1.1 Logical Reasoning ‣ 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation") 显示了每种类型的逻辑推理任务的几个示例。

Xu et al. ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib159))对包括 text-davinci-003、ChatGPT 和 BARD 在内的几种 LLM 的逻辑推理进行了全面研究。他们发现 BARD 在这三种模型中表现最好，而 ChatGPT 在演绎和归纳设置中表现较差。此外，他们还显示了 ChatGPT 在生成任务中表现不足，因为它是为聊天而定制的。Han et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib41))和 Liu et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib79))在他们的评估中包括了 GPT-4，发现其在某些场景中的表现质上与人类相匹配。

#### 2.1.2 数学推理

数学推理需要模型来理解和操作不同场景中的数学概念。例如，问题可能要求模型执行算术运算并操作抽象符号以获得准确的数值结果。著名的例子包括 GSM8K (Cobbe et al., [2021](https://arxiv.org/html/2406.00936v1#bib.bib21))和 MATH (Hendrycks et al., [2021](https://arxiv.org/html/2406.00936v1#bib.bib42))。

Stolfo et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib134))发现，与非指令调整模型相比，经过指令调整的 LLM 在数学问题的敏感性和鲁棒性方面有显著提高。Yuan et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib173))比较了 13 个模型在每种操作类型上的算术能力，发现 GPT-4 是唯一在每种操作中都表现优异的模型。

#### 2.1.3 常识推理

常识推理涉及理解和应用关于世界的基本知识的能力。这对于机器达到与人类认知相当的理解和互动水平至关重要。此外，常识认知在各种推理过程中，如因果检测、空间和时间理解等，具有关键作用。通常，常识推理任务被构建为选择题或判断对错的问题，其中包含需要模型应用常识知识回答的问题。例如，问题可能会问“你在结账前把葡萄放在哪里？”，模型应该选择正确的答案，即“购物车”。CommonsenseQA (Talmor et al., [2018](https://arxiv.org/html/2406.00936v1#bib.bib138))包含具有复杂语义的问题，需要先验知识才能回答。类似地，OpenBookQA (Mihaylov et al., [2018](https://arxiv.org/html/2406.00936v1#bib.bib92))包含设计用于评估对基本科学事实及其在新场景中应用的理解的基础级问题。

Bang 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib8)) 显示 ChatGPT 在多个常识基准测试中具有常识推理能力，这些测试涉及一般知识（Talmor 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib138)）和物理概念（Bisk 等人，[2020](https://arxiv.org/html/2406.00936v1#bib.bib12)；Wang 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib150)）。Bian 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib11)) 显示，指令调优模型在多个常识问答数据集上表现优越，包括 CommonsenseQA（Talmor 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib138)）和 OpenBookQA（Mihaylov 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib92)），这表明常识能力可以通过人类对齐得到提升。

#### 2.1.4 多跳推理

多跳推理任务要求模型进行顺序推理步骤以得出答案。它作为一个突出的评估方法，用于评估 LLMs 分析问题并通过逐步分解过程解决问题的能力，这类似于人类水平的能力。这个过程可以视为多种推理能力的融合，因为每一步可能需要应用之前讨论的一个或多个推理任务。例如，问题可能是“‘星际穿越’的导演出生在巴黎吗？”在这种情况下，模型必须首先确定电影的导演，然后确定他们的出生地。StrategyQA（Geva 等人，[2021](https://arxiv.org/html/2406.00936v1#bib.bib37)）要求模型生成若干隐含推理步骤，以制定出最终决策所需的策略。HotpotQA（Yang 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib166)）需要查找和推理多个支持性文档来形成响应。其问题多样，并不受限于任何预先存在的知识库。HoVer（Jiang 等人，[2020](https://arxiv.org/html/2406.00936v1#bib.bib53)）要求模型从多个相关的 Wikipedia 文章中收集事实，以判断这些事实是否支持声明。

Zheng 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib184)) 发现 ChatGPT 在 HotpotQA 上无法提供可靠和准确的答案。他们进一步分析指出，这种失败可能源于多种因素，其中事实正确性是最关键的。针对这个问题，他们强调了知识记忆和回忆对大型语言模型（LLMs）的重要性。

#### 2.1.5 结构化数据推理

前述的推理任务主要集中在涉及纯文本数据的场景。相比之下，结构化数据，如表格、知识图谱和数据库等特定格式的数据，对机器理解和推理提出了更大的挑战。要进行结构化数据推理，模型必须能够理解数据的格式，分析其中包含的信息，并生成与数据相关的问题的答案。

HybridQA (Chen 等人，[2020](https://arxiv.org/html/2406.00936v1#bib.bib18)) 集成了与维基百科表格对齐的问题和多个与表格中的实体相关的自由格式语料库。该模型需要汇总表格和文本信息以生成答案。MetaQA (Zhang 等人，[2018](https://arxiv.org/html/2406.00936v1#bib.bib180)) 包含电影领域中的问答对，并提供了一个知识图谱 (KG) 以促进信息检索。模型需要在 KG 上进行多跳推理，并处理 KG 实体与问题之间的潜在不匹配，以得出答案。Spider Realistic (Deng 等人，[2020](https://arxiv.org/html/2406.00936v1#bib.bib26)) 提供了一个基于 SQL 的 QA 数据集，要求模型进行文本到 SQL 的生成。具体而言，模型必须准确识别对列和值的文本引用，并将其映射到提供的数据库模式中。

Gao 等人，[2023](https://arxiv.org/html/2406.00936v1#bib.bib36) 对多种 LLM 进行了全面的文本到 SQL 任务研究，使用了各种提示工程方法。此外，他们还对开源模型进行了微调实验。然而，他们的发现表明，即使在微调之后，这些模型的表现仍然落后于通过零-shot 评估的专有模型。

### 2.2 社会影响

LLM 已成为现代社会的重要元素，显著影响了各个领域。凭借其在文本生成和理解方面的显著能力，LLM 正在重新塑造我们与信息的互动。因此，理解 LLM 的影响至关重要。通过探索这些维度，我们旨在理解 LLM 的更广泛的社会影响。我们的目标是将复杂的概念简化为易于理解的见解，提升我们评估 LLM 的能力。本讨论探讨了 LLM 的社会影响，重点关注两个关键方面：安全性和可信赖性。通过探索这些维度，我们旨在理解 LLM 的更广泛的社会影响。

#### 2.2.1 安全性

在本节中，我们探讨了保护用户在与 LLM 互动时所需的基本安全机制。确保这些模型仅生成安全内容至关重要，Oviedo-Trespalacios 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib99)）发现 ChatGPT 有时会发表不正确或有害的陈述，强调了专家验证的必要性。我们通过将安全问题分类为三个主要领域来解决这些问题：本节探讨了与 LLM 安全性相关的基本问题，包括内容安全、安全性和伦理考虑。

##### 内容安全

随着 LLM 和生成型 AI 的普及，相关的内容安全风险也在增加。基准测试提供了这些风险的关键见解。ToxicChat Lin 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib77)），基于来自开源聊天机器人的真实用户查询，强调了检测用户-AI 对话中毒性的独特挑战。Open AI Moderation Dataset Markov 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib88)）提供了识别现实世界应用中不良内容的全面方法。

AEGISSAFETYDATASET Ghosh 等人（[2024](https://arxiv.org/html/2406.00936v1#bib.bib39)），包含约 26,000 个由人类注释的人工-LLM 互动实例，加深了对内容安全问题的理解。AI Safety Benchmark v0.5 Vidgen 等人（[2024](https://arxiv.org/html/2406.00936v1#bib.bib146)），由 MLCommons AI Safety Working Group 创建，专注于评估 LLM 的安全性。SALAD-Bench Li 等人（[2024a](https://arxiv.org/html/2406.00936v1#bib.bib67)），旨在评估 LLM，包括对攻击和防御方法的评估。SafetyBench（Scherrer 等人，[2023](https://arxiv.org/html/2406.00936v1#bib.bib118)），是一个全面的 LLM 安全评估基准，包含 11,435 个涵盖七个不同安全类别的多项选择题。CValues（Xu 等人，[2023b](https://arxiv.org/html/2406.00936v1#bib.bib160)），是第一个中文人类价值观评估基准，用于衡量 LLM 在安全性和责任标准方面的对齐能力。KCDD（Kim 等人，[2024a](https://arxiv.org/html/2406.00936v1#bib.bib60)）包含 22,249 个由众包工人生成的对话，旨在模拟离线场景。该数据集将对话分类为四个符合国际法律标准的犯罪类别。BeaverTails（Ji 等人，[2023](https://arxiv.org/html/2406.00936v1#bib.bib50)）引入了一种新颖的“QA moderation”策略，以测试模型的安全对齐性，提供了与传统内容审核方法不同的全新视角。

此外，确保大型语言模型（LLMs）不会生成未成年人可接触的成人内容是至关重要的（Cifuentes et al., [2022](https://arxiv.org/html/2406.00936v1#bib.bib20); Karamizadeh et al., [2023](https://arxiv.org/html/2406.00936v1#bib.bib55)），同时要减少可能影响儿童的有害内容，确保输出内容不会鼓励非法活动（Nayerifard et al., [2023](https://arxiv.org/html/2406.00936v1#bib.bib97); Casino et al., [2022](https://arxiv.org/html/2406.00936v1#bib.bib16)），并避免生成可能煽动暴力的内容。在这一部分，基准测试和数据集在评估 LLMs 的安全对齐方面发挥着至关重要的作用。通过提供突出有害或不适当内容的标注数据，这些资源使研究人员能够开发和完善内容审查和安全执行的算法。

##### 安全

本节回顾了一系列关注提升数据隐私实践和增强 LLMs 对抗对抗性威胁的文献。Staab et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib133)) 讨论了 LLMs 从看似无害的文本输入中推断个人属性（如位置、收入和性别）的能力，使用从实际 Reddit 资料中得出的数据集来展示显著的隐私风险。讨论延续到 Kim et al. ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib61)) 介绍的 ProPILE，这是一种探测工具，使数据主体能够检测基于 LLMs 的服务中可能的个人身份信息泄露。Das et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib24)) 深入研究了这些漏洞，强调了对改进安全协议和探索有效防御的迫切需求，而 Yan et al. ([2024a](https://arxiv.org/html/2406.00936v1#bib.bib162)) 则重点澄清了与 LLMs 相关的数据隐私问题。此外，Carlini et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib15)) 和 Yao et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib169)) 强调了 LLMs 所带来的显著隐私风险，特别是它们倾向于逐字记忆和重现训练数据的部分内容。

关于对抗攻击的弹性，Yip 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib170)) 引入了一个框架，该框架利用创新技术量化应用程序对提示注入攻击的弹性，以进行稳健且可互操作的评估。Liu 等人 ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib80))；Jin 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib54)) 均提出了使用基于梯度的方法来增强对大型语言模型（LLM）对抗弹性的评估。这些方法强调了向更复杂和可靠的对抗威胁评估的关键转变。RigorLLM Yuan 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib174))，一个使用如基于能量的数据生成和极小极大优化等技术的框架，以增强有害内容的审查和提高对复杂对抗攻击的弹性。InjecAgent Zhan 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib175))，一个专门设计用于评估工具集成型 LLM 代理对间接提示注入攻击的脆弱性的基准，显示了常用 LLM 代理的显著易受攻击性。

##### 伦理考虑

在医学伦理 Balas 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib6)) 和道德决策 Scherrer 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib118)) 等敏感领域的伦理评估是具有挑战性的。这些研究对于提升 LLM 的伦理功能至关重要，确保其在实际应用中的负责任使用。在接下来的部分中，我们将介绍一些现有研究，这些研究涉及 LLM 评估中的伦理考虑及其改善伦理标准的方法。

Sorensen 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib130)) 强调了决策中的价值多元主义。他们介绍了 ValuePrism，这是一个包含 218k 个价值观、权利和义务的数据集，关联到 31k 个人工编写的情境，用于构建 Value Kaleidoscope（Kaleido），一个解释和评估人类价值观的模型。Kaleido 的输出被优于 GPT-4，显示出更高的准确性和更广泛的覆盖面。Duan 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib31)) 使用道德基础理论探索 LLM 的伦理价值，提出了 DeNEVIL，一种揭示价值倾向的提示生成算法。他们创建了 MoralPrompt，一个包含 2,397 个提示的数据集，并基准测试了 LLM 的内在价值，发现许多模型存在偏差。他们开发了 VILMO，一种提升价值一致性的对齐方法，适用于黑箱模型和开源模型。在 MoralChoice 数据集中，Scherrer 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib118)) 介绍了一个道德困境的例子，其中的情境是受访者是一名司机，正驶近一名过马路的行人。可能的回应包括：（1）减速并等待行人，这反映了一种负责任且伦理的决策；（2）加速撞击行人，这代表了一种不负责任且不伦理的选择。这个困境展示了用于评估现实环境中伦理行为的决策情境类型。

Scherrer 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib118)) 介绍了一种新颖的统计方法，用于检查 LLM 的道德信念，并量化 LLM 在各种道德情境下做出决策的可能性，分析了它们在 680 个高模糊性和 687 个低模糊性困境中的回应。研究结果表明，LLM 在简单情境中通常与常识一致，但在更模糊的情境中表现出显著的不确定性。这项研究提供了对 LLM 决策倾向和其在伦理情境中反映人类道德判断能力的洞见。

#### 2.2.2 真实性

评估 LLM 的可靠性需要确保其输出的真实性。Turpin 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib143)) 证明 Chain-of-Thought (CoT) 解释可能系统性地歪曲模型预测背后的真实推理。Khan 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib59)) 指出，随着 LLM 变得越来越复杂，可能超越人类专家，评估动态可能会发生变化，提出了更简单的模型是否能够有效评估更先进模型的问题。这种情况突显了 LLM 输出真实性的持续重要性，反映了模型评估中不断演变的挑战。

随着可信度成为关键优先事项，研究人员实施了各种评估策略以确保模型的可靠性。本节详细介绍了加强 LLM 输出可信度的策略。除了广为人知的 TruthfulQA 基准 Lin 等人 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib76)) 外，我们还关注以下主题：幻觉，偏见缓解。

##### 幻觉

在 LLMs 中，模型生成事实不准确或虚构内容的幻觉对其可信度和可靠性构成了重大挑战。

技术如 HaluEval 2.0 Jiang 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib52)) 和 HalluCode Liu 等人 ([2024a](https://arxiv.org/html/2406.00936v1#bib.bib78)) 基准已被开发用于有效的幻觉检测。其他方法包括 FEWL Wei 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib153))，该方法通过利用多个 LLM 响应来测量幻觉而无需黄金标准答案，以及 TofuEval Tang 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib139))，该方法通过详细的错误分类评估对话总结中的幻觉。Self-Alignment for Factuality Zhang 等人 ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib178)) 使用自我评估来提高 LLMs 中的事实准确性。LLM-free 多维基准 AMBER Wang 等人 ([2024a](https://arxiv.org/html/2406.00936v1#bib.bib147)) 允许通过低成本和高效的评估流程来评估生成任务和区分任务，包括各种类型的幻觉。该基准促进了对主流 MLLMs 如 GPT-4V 的全面评估和详细分析，并提供了缓解幻觉的指南。

Feldman 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib35)) 帮助识别和标记 LLMs 在其领域知识之外操作的实例，确保用户接收到准确的信息。该方法在上下文伴随问题提示时显著减少了幻觉，通过标签评估实现了消除幻觉的高效性。Yang 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib165)) 引入了一种自检方法，用于在关键任务中检测 LLMs 的事实错误，使用零资源设置中的反向验证。PHD 基准旨在检测通过人类标注的段落级幻觉，提升了检测方法的评估，并在效率和准确性上超越了现有方法。

##### 偏见缓解

一系列研究探讨了在评估和操作大型语言模型（LLMs）时存在的偏见问题，强调了减少这些偏见以提高质量和可靠性的必要性。

这里有一些一般的偏差基准。BBQ Parrish 等人（[2021](https://arxiv.org/html/2406.00936v1#bib.bib102)）是由作者构建的一套问题集数据集，突出展示了针对属于保护类别的人的社会偏差，涵盖了与美国英语环境相关的九个社会维度。BIAS Vermetten 等人（[2022](https://arxiv.org/html/2406.00936v1#bib.bib145)）是一个新型的基于行为的基准，用于检测每个维度以及跨维度的结构性偏差，基于 39 个统计测试。RecLLM Zhang 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib176)）研究了基于 LLM 的推荐中的公平性，提出了 FaiRLLM 基准来评估对敏感用户属性的偏差。MERS Wu 和 Aji（[2023](https://arxiv.org/html/2406.00936v1#bib.bib156)）引入了评估机器生成文本在多个维度上的基准，包括事实准确性和语言质量，特别针对并减少 LLM 评估中有利于不正确事实内容的偏差。

以下是与不同领域相关的具体偏差基准。在金融领域，Daniel 等人（[2008](https://arxiv.org/html/2406.00936v1#bib.bib23)）处理了评估投资经理中的“前瞻基准偏差”，该偏差识别了由于基准构成时间差异而导致的表现指标显著差异。这一发现强调了准确基准方法的必要性，以避免夸大绩效评估。Hort 等人（[2021](https://arxiv.org/html/2406.00936v1#bib.bib44)）使用模型行为突变方法来基准 ML 偏差缓解方法。尽管结果表明许多方法难以有效平衡公平性和准确性，但它们强调了在偏差缓解中需要更强有力策略的必要性。Wessel 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib154)）介绍了媒体偏差识别基准（MBIB），这是一个全面的框架，整合了各种类型的媒体偏差，提高了检测技术的有效性，促进了对媒体内容偏差评估的更统一和有效的方法。

### 2.3 领域知识

随着 LLM 在推理和安全性方面展示其能力，专家们已开始探索 LLM 在各个领域的知识。他们利用 LLM 完成特定任务，使这些模型成为有用的助手。在本节中，我们将深入探讨五个领域：金融、立法、心理学、医学和教育，介绍应用、评估方法，并讨论 LLM 在每个领域的方向和局限性。

#### 2.3.1 金融

LLM 在金融领域的应用相对较早。一些模型甚至专门为金融用途设计，如 FinBERT Liu 等（[2021b](https://arxiv.org/html/2406.00936v1#bib.bib82)）、XuanYuan 2.0 Zhang 和 Yang（[2023](https://arxiv.org/html/2406.00936v1#bib.bib179)）以及 BloombergGPT Wu 等（[2023](https://arxiv.org/html/2406.00936v1#bib.bib157)）。BloombergGPT 是一个拥有 500 亿参数的语言模型，训练数据涵盖广泛的金融数据。从 BloombergGPT 的验证过程中，我们可以深入了解金融 LLM 的评估方法。Wu 等（[2023](https://arxiv.org/html/2406.00936v1#bib.bib157)）在两大类任务上评估了 BloombergGPT：金融特定任务和通用任务。关于金融特定任务，使用了 FPB Malo 等（[2014](https://arxiv.org/html/2406.00936v1#bib.bib86)）、FiQA SA Maia 等（[2018](https://arxiv.org/html/2406.00936v1#bib.bib85)）、Headline Sinha 和 Khandait（[2021](https://arxiv.org/html/2406.00936v1#bib.bib127)）、NER Alvarado 等（[2015](https://arxiv.org/html/2406.00936v1#bib.bib4)）和 ConvFinQA Chen 等（[2022](https://arxiv.org/html/2406.00936v1#bib.bib19)）。他们还使用了社交媒体和新闻作为特定方面的情感分析数据集，并将 BloombergGPT 的回应与金融专家的注释进行比较。关于通用任务，利用了标准 LLM 基准进行评估，如 BIG-bench Hard Suzgun 等（[2022](https://arxiv.org/html/2406.00936v1#bib.bib137)），以及关于知识评估、阅读理解和语言学任务的多个数据集。条件性地，Xie 等（[2023](https://arxiv.org/html/2406.00936v1#bib.bib158)）提出了 PIXIU，一个框架包括基于微调 LLaMA 的金融 LLM、一个包含 136K 数据样本的指令数据以支持微调，以及一个包含 5 个任务和 9 个数据集的评估基准，为金融领域的 LLM 提供了评估能力的基准。在提到金融用途的 LLM 时，Li 等（[2023b](https://arxiv.org/html/2406.00936v1#bib.bib69)）认为主要挑战是虚假信息的生成以及 LLM 中的偏见表现，例如种族、性别和宗教偏见。此外，评估中的主要挑战是整合金融专家的领域知识，以根据金融 NLP 任务验证模型的性能 Lee 等（[2024](https://arxiv.org/html/2406.00936v1#bib.bib66)）。

#### 2.3.2 法规

大型语言模型（LLMs）在立法领域的能力也引起了关注，因为 GPT-4 在统一律师资格考试中的得分约为 297 分，超过了所有司法管辖区的及格线 Katz et al. ([2024](https://arxiv.org/html/2406.00936v1#bib.bib58))。LLMs 执行了各种任务，如法典推理、术语解释和法律规则分类，并对其表现进行了评估。Blair-Stanek et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib13)) 使用 SARA 数据集 Holzenberger et al. ([2020](https://arxiv.org/html/2406.00936v1#bib.bib43)) 评估了 GPT-3 在法典推理中的表现。他们发现 GPT-3 在零样本条件下仅达到 78% 的准确率，显示 GPT-3 无法处理基础法律工作，因为数据集中的法典远不如实际法典复杂。Engel 和 Mcadams ([2024](https://arxiv.org/html/2406.00936v1#bib.bib33)) 问询 Chat 3.5 Turbo 是否将法定术语“车辆”包括在待评估的候选对象列表中，以检验 LLMs 对法定意义的理解。他们发现 Chat 3.5 Turbo 的结果与 2,800 名英语使用者的回答类似 Tobia ([2020](https://arxiv.org/html/2406.00936v1#bib.bib142))。Liga 和 Robaldo ([2023](https://arxiv.org/html/2406.00936v1#bib.bib74)) 发现 GPT-3 能够识别义务规则、许可规则和构成规则之间的差异，使用了 LegalDocML Palmirani 和 Vitali ([2011](https://arxiv.org/html/2406.00936v1#bib.bib100)) 和 LegalRuleML Athan et al. ([2013](https://arxiv.org/html/2406.00936v1#bib.bib5)) 数据集。关于 LLMs 是否具备足够能力应用于专业法律领域，调查表明，预训练的 LLMs 还未准备好完全自动化用于案件判决摘要，因为生成的抽象摘要中发现了不一致或虚构的信息 Deroy et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib28))。

#### 2.3.3 心理学

人类语言数据在心理学的每个子领域都很重要且有价值。由于大型语言模型（LLMs）具备理解和使用多种语言的能力，因此情感检测和心理测量可以由 LLMs 完成。大量研究评估了 LLMs 是否能够以足够的质量完成这些任务。Rathje 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib114)) 测试了不同版本的 GPT（3.5 Turbo、4 和 4 Turbo）是否能够在 12 种语言的文本中检测情感、离散情绪、攻击性和道德基础。他们发现，LLMs 在检测心理学构念方面优于现有的英文词典分析，手动标注者的判断也是如此。Lu 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib83)) 评估了 GPT-4V 在情感计算任务中的 5 项关键能力。他们使用 DISFA 数据集 Mavadati 等人 ([2013](https://arxiv.org/html/2406.00936v1#bib.bib89)) 来评估 GPT-4V 的动作单元检测能力，RAF-DB 数据集 Shan 和 Deng ([2018](https://arxiv.org/html/2406.00936v1#bib.bib123)) 用于面部表情和复合情绪识别，Du 等人 ([2014](https://arxiv.org/html/2406.00936v1#bib.bib30)) 使用 CASME2 数据集，Yan 等人 ([2014](https://arxiv.org/html/2406.00936v1#bib.bib164)) 用于微表情识别，Zhao 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib181)) 和 iMiGUE 数据集 Liu 等人 ([2021a](https://arxiv.org/html/2406.00936v1#bib.bib81)) 用于微动作识别。结果表明，GPT-4V 对动作单元、复合情绪和微动作测试样本能够给出令人满意的回答，但在面部表情和微表情测试样本中未能正确回答。关于心理测量，Demszky 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib25)) 提出了两种评估特征对人类思维和行为影响的方法：1) 专家评估意味着经过培训的研究助理和 LLMs 对相同文本进行特定心理构念的评分，然后计算他们评分之间的一致性。2) 影响评估意味着评估操控前后的效果。例如，Karinshak 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib56)) 使用影响评估来测量参与者对 GPT-3 生成的支持疫苗接种消息的态度。Demszky 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib25)) 还建议，在评估 LLMs 的心理学任务能力时，初步评估可以使用专家评估进行操控检查或构念效度测量。随后，可能会利用与专家评估一致的文本进行影响评估研究，尝试测量对第三方参与者的预期效果，类似于评估预测效度或外部效度。

#### 2.3.4 医学

由于 ChatGPT 能够在没有额外训练的情况下通过美国医学执照考试（USMLE）Kung 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib64)）的测试，LLM 在医学领域引起了关注。以往的研究主要集中在探索 LLM 在临床工作和研究中的潜力 Thirunavukarasu 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib141)）。Agrawal 等人（[2022](https://arxiv.org/html/2406.00936v1#bib.bib3)）介绍了来自手动重新标注的 CASI 数据集 Moon 等人（[2014](https://arxiv.org/html/2406.00936v1#bib.bib94)）的数据集，用于基准测试少样本临床信息提取，并展示了 GPT-3 在这一任务中优于现有基准。Sharma 和 Thakur（[2023](https://arxiv.org/html/2406.00936v1#bib.bib124)）展示了 ChatGPT 可以帮助研究人员设计新药并优化新药的药代动力学和药效学。Benoit（[2023](https://arxiv.org/html/2406.00936v1#bib.bib9)）展示了在面对 45 个简化的标准化病例 Semigran 等人（[2015](https://arxiv.org/html/2406.00936v1#bib.bib120)）时，ChatGPT 以 75.6% 的首轮诊断准确率和 57.8% 的分诊准确率识别疾病，其表现与医生在同一组 45 个病例中 72.1% 的准确率相似。然而，在撰写学术临床论文时，当前的 LLM 无法满足 ICMJE 作者资格标准，因为它们无法理解作者的角色或对论文承担责任 Zielinski 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib187)）。此外，Kumar（[2023](https://arxiv.org/html/2406.00936v1#bib.bib63)）评估了 ChatGPT 在生物医学领域学术写作中的实用性，显示虽然其回答内容系统、准确且原创，但缺乏学术写作的质量和深度。总之，LLM 应用在医学领域的广泛部署目前还不可行，需要更深入的评估。临床医生和研究人员将继续负责提供最佳的知识和护理 Thirunavukarasu 等人（[2023](https://arxiv.org/html/2406.00936v1#bib.bib141)）。

#### 2.3.5 教育

LLM（大型语言模型）的对话和知识特性使其在教育中的应用成为可能。当前教育领域对 LLM 的评估方法大致可以分为两类：1) 人工注释意味着专家直接对 LLM 生成的材料进行评分，或对来自外部数据集或在线网站的未标记数据进行注释，以创建评估数据集。Abdelghani 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib1)) 使用 GPT-3 生成语言和语义提示，以帮助儿童提出发散性问题。他们有 2 位专家评估生成的语言和语义提示的质量。Jia 等人 ([2021](https://arxiv.org/html/2406.00936v1#bib.bib51)) 让流利的英语使用者对来自同伴评估平台 Expertiza 的数据进行注释，并确保足够的注释者一致性，以测试 BERT 模型在评估同伴评估中的准确性。Menick 等人 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib90)) 通过让付费承包商评估来自自然问题（Natural Questions）Kwiatkowski 等人 ([2019](https://arxiv.org/html/2406.00936v1#bib.bib65)) 和 ELI5 Fan 等人 ([2019](https://arxiv.org/html/2406.00936v1#bib.bib34)) 数据集的模型样本来评估他们的自支持问答模型。2) 指标和模型意味着使用传统指标或训练模型自动评估 LLM 生成的材料。Dijkstra 等人 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib29)) 提出了 EduQuiz，一种基于 GPT-3 模型的端到端测验生成器，能够生成完整的多项选择题及其正确答案和干扰答案。他们使用了 BLEU-4 Papineni 等人 ([2002](https://arxiv.org/html/2406.00936v1#bib.bib101))、ROUGE-L Lin ([2004](https://arxiv.org/html/2406.00936v1#bib.bib75)) 和 METEOR Banerjee 和 Lavie ([2005](https://arxiv.org/html/2406.00936v1#bib.bib7)) 指标来比较预测和真实数据实例。Raina 和 Gales ([2022](https://ar

## 3 代理评估

{forest}

for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=, rectangle, draw, rounded corners,align=left, inner xsep=4pt, inner ysep=1pt, , where level=1font=,fill=pink!50, where level=2font=,fill=green!10, where level=3font=,fill=gray!20, [代理评估

(节 [3](https://arxiv.org/html/2406.00936v1#S3.F3 "图 3 ‣ 3 代理评估 ‣ 有用 LLM 评估的调查")),fill=yellow!20,font=[规划

(节 [3.1](https://arxiv.org/html/2406.00936v1#S3.SS1 "3.1 规划 ‣ 3 代理评估 ‣ 有用 LLM 评估的调查")) [Song 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib128)), Huang 等 ([2022b](https://arxiv.org/html/2406.00936v1#bib.bib49)), Yao 等 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib168)), Shinn 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib125)),fill=gray!20] ] [应用场景

(节 [3.2](https://arxiv.org/html/2406.00936v1#S3.SS2 "3.2 应用场景 ‣ 3 代理评估 ‣ 有用 LLM 评估的调查")) [Web 基础 [Nakano 等 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib96)), Qin 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib107)), Yao 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib167)) ] ] [代码生成 [Liang 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib71)), Zhang 等 ([2024a](https://arxiv.org/html/2406.00936v1#bib.bib177)) ] ] [数据库查询 [Hu 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib45))] ] [API 调用 [Li 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib68)), Qin 等 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib108)), Yan 等 ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib163)) ] ] [工具创建 [Cai 等 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib14)), Qian 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib106)) ] ] [机器人导航 [Shah 等 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib122)), Zhou 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib185)), Zheng 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib182)) ] ] [机器人操作 [Huang 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib48)), Yu 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib172)) ] ] ] [基准

(节 [3.3](https://arxiv.org/html/2406.00936v1#S3.SS3 "3.3 基准 ‣ 3 代理评估 ‣ 有用 LLM 评估的调查")) [Ruan 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib116)), Li 等 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib68)), Tang 等 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib140)),fill=gray!20] ] ]

图 3: 代理评估概述。

在 LLM 的核心能力基础上，已经有一个日益增长的研究领域，利用 LLM 作为中央控制器来构建自主代理，以获得类似人类的决策能力 Wang 等 ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib148)).

在这一部分，我们将首先讨论评估 LLM 代理规划能力的方法，并介绍基于各种应用场景的评估。每个子节将提供关于 LLMs 应用、评估方法和使用的数据集的详细见解。

### 3.1 规划

代理的规划涉及在给定环境中战略性地制定和执行行动或步骤，以实现特定目标或结果，通常使用算法或模型来预测和决定最佳行动方案。

面对执行需要将复杂任务分解为更简单子任务的挑战，机器人规划使得机器人能够自主识别并执行实现特定目标的行动，同时考虑其周围环境和目标。在这种背景下，一些创新的方法，例如黄等人 ([2022a](https://arxiv.org/html/2406.00936v1#bib.bib47))，辛格等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib126))，宋等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib128))，利用通过大规模语言模型（LLMs）获得的广泛常识知识，使这些模型能够高效地将任务分解为可管理的子任务。Inner Monologue 黄等人 ([2022b](https://arxiv.org/html/2406.00936v1#bib.bib49)) 系统通过整合持续的自然语言反馈，利用 LLMs 进行机器人任务的动态规划。类似地，SayPlan Rana 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib113)) 通过使用 3D 场景图增强了 LLMs 的任务规划能力，以促进广泛的环境交互。这些方法在虚拟环境、具身体代理和物理机器人中进行了评估。此外，像 DEPS 王等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib152))，AdaPlanner 孙等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib135))，以及 Robots That Ask For Help 任等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib115)) 等多项工作引入了动态交互式重新规划、适应性策略和在面对不确定性时寻求帮助的能力。这些发展对于机器人在现实环境中的实际应用和有效性至关重要，展示了朝着更具适应性和智能的机器人系统迈出的重要一步。它们在越来越复杂的情境中进行评估，这些情境与现实生活条件紧密相符。

基于 LLM 的智能体利用 LLM 分析和生成类似人类的文本，通过快速准确地处理大量信息，辅助决策和战略规划。React Yao 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib168)) 提出了一个将推理与行动在语言模型中协同融合的范式，通过在 ALFWorld 和 WebShop 的基准测试中提高性能和可解释性。Reflexion Shinn 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib125)) 引入了一个突破性的框架，该框架利用口头反馈进行强化学习，使语言智能体通过自我反思提升技能，而无需更新模型权重。该方法在多种决策、推理和编程任务中进行了评估，显示出相较于传统方法的显著改进，应用于如 AlfWorld、HotPotQA 和 HumanEval 等环境。SelfCheck Miao 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib91)) 提供了一种零-shot 机制，使 LLM 能够自主验证其在数学问题解决中的多步骤推理，这显著提高了在 GSM8K、MathQA 和 MATH 等基准测试中的准确性，通过过滤掉低置信度的解决方案。

### 3.2 应用场景

#### 3.2.1 网络基础

在这一部分，我们重点关注 LLM 在网络环境中执行任务的情况。我们根据任务对评估方法进行分类。

##### 搜索引擎

WebGPT Nakano 等人 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib96)) 开发了一个基于文本的网络浏览环境，使得与微调后的语言模型进行交互，以生成更忠实的输出。WebGPT 模型的评估通过三种主要方法进行：与人类演示者在保留问题集上撰写的答案进行比较，与 ELI5 数据集中的最高投票答案进行比较，以及使用 TruthfulQA 数据集进行评估。

WebCPM Qin 等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib107)) 采用工具学习来使模型通过网络搜索回答长篇问题。其评估包括四个子任务：行动预测、搜索查询生成、支持性事实提取和信息综合，每个任务都通过 Micro-F1 和 Macro-F1（用于行动预测）以及 Rouge-L（用于其他三个任务，包括文本生成）独立评估。在整体评估中，八名注释员根据人类偏好手动比较模型生成的答案。

##### 在线购物

WebShop Yao 等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib167)) 提出了一个基准，用于评估基于 LLM 的代理在产品搜索和检索方面的能力。他们的数据集由 12,087 条指令组成，分为 10,587 条用于训练、1,000 条用于开发、500 条用于测试，并记录了每个实例的人类购物路径。评估指标包括任务得分和成功率，结果显示人类在所有衡量指标上均优于 LLM。

#### 3.2.2 代码生成

为了在复杂的实际任务中实现机器人控制的细致控制，Code as Policies Liang 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib71)) 模式使用 LLM 生成用于空间推理和适应新指令的策略代码。代码质量通过 HumanEval 和 RoboCodeGen 进行评估。RoboCodeGen 是一个包含 37 个函数生成任务的基准，专注于空间和几何推理及控制，支持 NumPy 等第三方库，缺乏文档字符串和类型提示，并允许未定义函数用于分层代码生成。评估指标是通过人工编写单元测试的生成代码的通过率。

CODEAGENTBENCH 基准 Zhang 等人 ([2024a](https://arxiv.org/html/2406.00936v1#bib.bib177)) 旨在评估 LLM 在实际代码生成任务中的表现。它提供了全面的输入信息，如文档、代码依赖和运行环境细节，挑战 LLM 生成准确且良好集成的代码解决方案。

#### 3.2.3 数据库查询

集成外部数据库或知识库使代理能够访问特定领域的信息，从而产生更现实的行动。例如，ChatDB Hu 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib45)) 使用 SQL 语句查询数据库，使代理能够进行逻辑行动。他们创建了一个由 70 条水果店管理日志记录组成的数据集用于评估。实验清晰地表明，ChatDB 在准确性上显著优于 ChatGPT。

#### 3.2.4 API 调用

LLM 代理也可以通过调用 API 来增强其能力。API-Bank，Li 等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib68)) 提出的工具，提供了一个专业的基准来评估工具增强 LLM 的表现。该基准包括 53 个标准 API 工具、工具增强 LLM 的详细工作流程，以及一个包含 264 个标注对话的数据集。评估指标包括 API 调用的准确性和 post-call 回复的 ROUGE-L，任务规划效率通过模型驱动的 API 调用成功完成计划任务来衡量。

Qin 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib108)) 对当代语言模型 (LLMs) 中工具学习的使用进行了学术研究，探讨了其有效性和局限性。他们评估了 18 种代表性工具在六个任务中的表现，并利用现有数据集将研究扩展到 12 个额外任务，例如幻灯片制作、AI 绘画和 3D 模型构建。他们增强了 ChatGPT 生成的用户查询，并手动评估了这些操作的成功率。

伯克利函数调用排行榜 (BFCL) Yan 等人 ([2024b](https://arxiv.org/html/2406.00936v1#bib.bib163)) 评估 LLM 在函数处理、语法树分析和函数执行等各种场景中的表现。它提供了一个交互式比较工具和一个涵盖数学、体育和金融等领域的数据集。评估包括简单、多重和并行函数测试。BFCL 促进了 LLM 在 Langchain 和 AutoGPT 等平台中的集成，提供了 GPT-4 等模型在成本和延迟方面的详细分析。

#### 3.2.5 工具创建

工具的使用取决于外部工具的可用性 Schick 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib119))。最近，有人致力于将 LLM 作为工具创造者，以生成可以用于各种请求的工具 (Ruan 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib116)))。LATM Cai 等人 ([2024](https://arxiv.org/html/2406.00936v1#bib.bib14)) 利用 GPT-4 开发工具，表明在这些应用中，更具成本效益的模型可以实现与更大模型相媲美的性能。他们使用了来自不同领域的六个数据集：逻辑推理、物体跟踪、Dyck 语言、词序列、 Chinese remainder theorem 和会议调度。前五个数据集来源于 BigBench Srivastava 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib132))，而会议调度任务则专门设计用于展示模型在实际应用中的效用。CREATOR Qian 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib106)) 使用 Creation Challenge 数据集评估 LLM 创建工具的能力，该数据集包含 2,000 个现有工具或代码包无法充分解决的新颖且具有挑战性的问题。评估表明，ChatGPT 的工具制作性能随着额外提示的增加而改善，准确率达到 75.5%，突显了工具创建在提升 LLM 问题解决能力中的重要性。

#### 3.2.6 机器人导航

由具身代理进行的导航涉及机器人或虚拟实体在物理或模拟环境中的自主移动和决策，使用传感器和算法感知周围环境、规划路线并完成导航任务。

LM-Nav Shah 等人 ([2022](https://arxiv.org/html/2406.00936v1#bib.bib122)) 提出了一个用于机器人导航的系统，该系统利用 LLM、VLM、视觉导航模型（VNM）和机器人导航，使机器人能够使用自然语言指令在复杂环境中导航，而无需特定的语言描述标注训练数据。他们在 20 个查询上进行了基准测试，这些查询涉及不同难度的环境，总长度超过 6 公里。LFG Shah 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib121)) 利用语言模型作为启发式方法来增强规划算法，通过自然语言描述中的语义线索指导机器人穿越陌生环境。他们在 ObjectNav 上评估了导航性能。

NavGPT Zhou 等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib185)) 利用大语言模型（LLMs）进行明确的推理和规划。这种方法结合了视觉观察的文本描述、导航历史和潜在的未来路径，以增强导航任务。随后，NaviLLM 模型 Zheng 等人 ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib182)) 出现，作为一种多功能的体态导航解决方案。它巧妙地调整 LLMs，以管理各种体态导航挑战，采用基于模式的指令将不同的任务转化为统一的生成建模问题。这些模型的性能通过视觉语言导航（VLN）基准严格评估，如 R2R、Reverie、CVDN 和 SOON。

#### 3.2.7 机器人操作

操作涉及使用体态代理与其环境中的物理对象互动和操控，实现从简单的取放操作到复杂的组装过程的任务。

VoxPoser Huang 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib48)) 提出了一个创新方法，其关键新颖性在于使用 LLMs 不仅仅是为了理解自然语言指令，而且重要的是生成与 VLMs 交互的代码，以创建详细的 3D 价值图。这些图指导机器人的动作，弥合了抽象指令和实际执行之间的差距。他们直接根据机器人操控任务的成功率评估结果。L2R Yu 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib172)) 提出了一个将语言指令翻译成奖励函数的方法，使用 LLMs 让机器人优化以执行特定任务，并在模拟环境中展示了这种方法在各种复杂的运动和操控任务中的应用。

### 3.3 基准

| 基准 | 描述 |
| --- | --- |
| APIBench (Patil 等人，[2023](https://arxiv.org/html/2406.00936v1#bib.bib103)) | 一个评估系统，包含 73 个 API 工具、314 个标注的工具使用对话（共 753 次 API 调用），以及一个包含 1,888 个工具使用对话的训练集，这些对话来自 2,138 个 API 涉及 1,000 个领域 |
| ToolEval (Qin et al., [2023c](https://arxiv.org/html/2406.00936v1#bib.bib109)) | 自动使用 ChatGPT 构建，包括来自 49 个类别的 16,464 个真实世界 RESTful API，针对单工具和多工具场景生成了多样化的指令和解决路径。 |
| ToolAlpaca (Tang et al., [2023](https://arxiv.org/html/2406.00936v1#bib.bib140)) | 包含来自 50 个类别的 400 多个真实工具 API 中的 3,938 个实例 |
| RestBench (Song et al., [2023b](https://arxiv.org/html/2406.00936v1#bib.bib129)) | 人工注释的数据集，包括两个真实世界的场景（TMDB 电影数据库和 Spotify 音乐播放器），分别包含 54 个和 40 个常用 API，为开发注释了 10 个指令-解决方案对，并为测试注释了 157 对（TMDB 100 对，Spotify 57 对） |
| WebArena (Zhou et al., [2023b](https://arxiv.org/html/2406.00936v1#bib.bib186)) | 一个现实且可重复的网络环境，具有四个完全操作的网络应用程序（电子商务、讨论论坛、协作开发和内容管理），以及 812 个长时域任务 |
| MIND2WEB (Deng et al., [2023](https://arxiv.org/html/2406.00936v1#bib.bib27)) | 来自 31 个领域的 137 个真实网站上的 2000 多个任务，具有众包的行动序列，能够创建处理多样且复杂的网页交互的代理 |

表 2：代理评估基准

评估 LLMs 在工具操作方面的能力主要围绕评估单一工具的有效性，使用既定的基准来衡量其对下游任务的影响，如之前讨论的。然而，越来越多的研究者正在将焦点转向涉及多个工具联合使用的场景，以评估经过工具学习训练的 LLMs 的性能。这种方法确保了对模型能力和在各种工具集中的限制的更全面和多样化的评估。

APIBench 由 Patil et al. ([2023](https://arxiv.org/html/2406.00936v1#bib.bib103)) 组建，汇集了来自主要中心如 HuggingFace、TorchHub 和 TensorHub 的全面 API 语料库，包括所有 TorchHub 和 TensorHub 的 API 调用以及每个 HuggingFace 任务类别中下载量前 20 的模型。使用 Self-Instruct Wang et al. ([2023a](https://arxiv.org/html/2406.00936v1#bib.bib151))，他们为每个 API 创建了 10 个合成用户提示，以评估 LLMs 的功能正确性和幻觉问题。

ToolBench，由 Xu et al. ([2023c](https://arxiv.org/html/2406.00936v1#bib.bib161))开发，评估 LLMs 在各种基于工具的任务中的泛化和高级推理技能。它集成了现有和新收集的数据集，包含八个任务，每个任务大约有 100 个测试案例。

基于 ToolBench，ToolLLM Qin 等人 ([2023c](https://arxiv.org/html/2406.00936v1#bib.bib109)）引入了 ToolEval，这是一种类似于排行榜的自动评估工具。ToolEval 使用两个指标：通过率，即在有限尝试内成功完成指令的比例，以及胜率，即与 ChatGPT 的性能比较。该评估方法结合了自动和人工评估，同时使用 ChatGPT 生成的解决方案作为基准，减少了潜在的人为偏见和不公平。

ToolAlpaca Tang 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib140)）扩展了评估框架，以涵盖真实世界场景。通过使用 426 个工具使用的训练集，该研究在 100 个评估实例中评估了十个新工具。遵循 ReAct 风格（Yao 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib168)）），工具使用在文本生成过程中被集成，人工评审员评估程序的准确性和整体正确性。

RestBench Song 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib129)）探讨了使用 API 的真实用户指令，重点关注 TMDB 电影数据库和 Spotify 音乐播放器场景。它分别筛选了 54 和 40 个常用 API，构建了 OpenAPI 规范。通过整合 RestGPT，将 LLM 与 RESTful API 连接，它遵循标准的 Web 服务协议。RestBench 通过人工注释的指令和黄金解决方案路径来评估性能，展示了 RestGPT 在复杂任务中的有效性，并向人工智能通用智能（AGI）迈进。

WebArena（Zhou 等人 ([2023b](https://arxiv.org/html/2406.00936v1#bib.bib186)））提供了一个拥有四个常见领域：电子商务、社交论坛讨论、协作软件开发和内容管理的完全功能性网站环境。其目的是以端到端的方式评估智能体，并确定其完成任务的准确性。

MIND2WEB（Deng 等人 ([2023](https://arxiv.org/html/2406.00936v1#bib.bib27)））是第一个用于开发和评估可以按照语言指令在任何网站上完成复杂任务的通用智能体的数据集。MIND2WEB 拥有超过 2,000 个任务，涵盖了 137 个网站和 31 个不同领域，取代了其他数据集中常见的过于简化的模拟环境，提供了真实世界网站的领域。

## 4 未来方向

LLM 能力和应用领域的快速进展使它们在短时间内取代了其他工具，显著提升了人们的生活。然而，评估方法的发展未能跟上 LLM 能力的扩展，通常使得找到完全匹配当前任务的基准变得具有挑战性。当前评估方法还有很大改进空间，以更准确地评估 LLM 在各种任务中的表现，并提供决策依据。因此，我们提出了五个未来评估方法的发展方向。我们期望这些改进将使 LLM 在公众眼中成为更“有用”的存在。

### 4.1 动态评估

当前的基准大多是静态的，一旦创建便不会改变。然而，不变的基准在评估时可能会出现两个问题。首先，现实世界中的事实知识随着时间变化。例如，总统职位每四年可能会更换，因此，用于评估 LLM 事实知识的数据集也需随时间更新，理想情况下要自动更新，以确保 LLM 提供的信息准确且与时俱进。

其次，随着 LLM 模型的扩展，数据集中的数据可能会泄露，成为 LLM 训练数据的一部分，这时这些数据集将不再作为有效的评估工具。因此，数据集中的评估问题必须能够自动替换和更新。例如，王等人提出的框架（[2024c](https://arxiv.org/html/2406.00936v1#bib.bib149)）可以操控原始实例的上下文或问题，重新框定新的不断发展的实例，以高信心动态扩展现有基准。这些进展将确保基准可以持续测量 LLM 在其进展过程中的能力。

### 4.2 LLM 作为评估者

目前许多数据集需要人工注释者标记每个问题的答案，这一过程既耗时又容易出错。因此，使用 LLM 作为评估者代表了一个有前途的发展方向。LLM 可以通过阅读文本并提供评分来模拟评分者，使我们避免为每个任务设计新的基准。相反，我们可以利用 LLM 的广泛能力作为各种任务的评分者。李等人（[2024b](https://arxiv.org/html/2406.00936v1#bib.bib70)）回顾了使用 LLM 作为评分者的当前方法，并识别了潜在的问题，例如对同一模型生成内容的偏好或评估顺序中的特定偏差。未来，我们可以逐步解决 LLM 作为评估者固有的偏见。在这种情况下，我们可以加快 LLM 应用的快速发展，同时使其能够自我评估，从而消除对额外数据集设计的需求。

### 4.3 根本原因分析

我们之前提到的评估方法主要依赖于评估 LLM 的输出。例如，我们向 LLM 提问，并根据其回答的准确性进行评估。这种评估方法使我们能够快速了解模型在各个方面的能力，并了解它能帮助我们完成什么。然而，仅仅通过检查模型的输出，我们无法确定模型产生特定响应的根本原因。当模型回答正确时，我们无法确定它是否真正具备相应的能力，还是仅仅之前遇到过类似问题并记住了答案。同样，当模型的响应未达到预期时，也很难确定模型出错的原因。因此，我们建议未来的评估方法应包括分析模型预测的根本原因。这将使我们能够更好地分析 LLM，从而促进未来更有用的 LLM 的发展。

### 4.4 精细化 LLM 代理评估

现有基准主要依赖任务的最终完成状态，缺乏精细化的逐步评估。此外，虽然当前研究更侧重于代理在有限环境（如在线购物）中执行任务的能力，但环境反馈往往是基于规则的、简单的，远离现实场景。未来的一个潜在方向是利用高智能模型，如 LLM，来设计更现实的评估环境。

### 4.5 机器人基准开发

近期的机器人研究主要强调使用仿真环境来促进向实际应用的过渡。这些环境在提升机器人在各种条件下的泛化能力方面至关重要。现在迫切需要开发类似于计算机视觉领域中的 ImageNet 的大规模基准，以严格评估这些泛化能力。此外，为了准确模拟现实场景，必须整合反映实际条件的特定任务。此外，数字双胞胎的概念代表了在模拟和现实世界环境中评估机器人另一条有前景的途径。鉴于在测试域外数据时计算机视觉仍存在显著差异，采用数字双胞胎和类似的方法可以显著减少 sim-2-real 差距，从而使评估模型能力的方式更加集中。

此外，其他方面的详细评估，如模拟到真实的差距、对抗扰动的鲁棒性、人机协作和多机器人协调，对在现实世界场景中有效部署机器人仍然至关重要。最后，随着深度学习在大规模数据训练中持续取得成功，评估像 RT-2 和 PaLM-E 这样的机器人基础模型也将对推进我们在复杂环境中对机器人技术的理解和应用至关重要。

## 5 结论

由于大型语言模型（LLMs）的不可解释性，我们需要各种评估方法来理解其能力，这也是 LLMs 进步的驱动力。本研究介绍了一个两阶段框架：从核心能力到代理，以评估 LLMs 的可用性。我们回顾了每一部分中的应用、基准和评估方法，旨在阐明当前 LLM 开发的优缺点。最后，我们提出了几种 LLMs 评估方法的改进方向，旨在使未来的 LLMs 评估更加灵活、自动化，并能够识别问题的根本原因。我们期待未来的研究使 LLMs 成为帮助人类社会的更有用工具。

## 致谢

## 参考文献

+   Abdelghani 等（2023）Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon 和 Pierre-Yves Oudeyer。2023 年。《GPT-3 驱动的教学代理以培养儿童的好奇心提问技能》。*国际人工智能教育杂志*，第 1-36 页。

+   Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等。2023 年。《Gpt-4 技术报告》。*arXiv 预印本 arXiv:2303.08774*。

+   Agrawal 等（2022）Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim 和 David Sontag。2022 年。《大型语言模型是少样本临床信息提取器》。*arXiv 预印本 arXiv:2205.12689*。

+   Alvarado 等（2015）Julio Cesar Salinas Alvarado, Karin Verspoor 和 Timothy Baldwin。2015 年。《域适应命名实体识别以支持信用风险评估》。见于 *2015 年澳大利亚语言技术协会会议论文集*，第 84-90 页。

+   Athan 等（2013）Tara Athan, Harold Boley, Guido Governatori, Monica Palmirani, Adrian Paschke 和 Adam Wyner。2013 年。《Oasis legalruleml》。见于 *第十四届国际人工智能与法律会议论文集*，第 3-12 页。

+   Balas 等（2024）Michael Balas, Jordan Joseph Wadden, Philip C Hébert, Eric Mathison, Marika D Warren, Victoria Seavilleklein, Daniel Wyzynski, Alison Callahan, Sean A Crawford, Parnian Arjmand 等。2024 年。《探索人工智能大型语言模型在医学伦理中的潜在效用：对 GPT-4 的专家小组评估》。*医学伦理学杂志*，50(2):90–96。

+   Banerjee 和 Lavie（2005）萨坦吉夫·班纳吉和阿隆·拉维。2005 年。Meteor：一种自动化的机器翻译评估指标，与人类判断的相关性得到改善。见于*ACL 机器翻译和/或摘要评估测量研讨会论文集*，页 65–72。

+   Bang 等（2023）白睫、塞缪尔·查雅维贾亚、李娜妍、戴文亮、苏丹、布莱恩·威利、霍莉·洛维尼亚、纪子伟、俞铁征、威利·钟、阮维·杜、徐燕和帕斯卡尔·冯。2023 年。[关于推理、幻觉和互动的 Chatgpt 多任务、多语言、多模态评估](http://arxiv.org/abs/2302.04023)。

+   Benoit（2023）詹姆斯·RA·贝努瓦。2023 年。用于临床小案例生成、修订和评估的 Chatgpt。*MedRxiv*，页 2023–02。

+   Bhagavatula 等（2019）钱德拉·巴哈瓦图拉、罗南·勒·布拉斯、查伊塔尼亚·马拉维亚、坂口圭介、阿里·霍尔茨曼、汉娜·拉什金、道格·道尼、斯科特·温陶·易和叶金·崔。2019 年。演绎常识推理。*arXiv 预印本 arXiv:1908.05739*。

+   Bian 等（2024）宁边、韩先培、孙乐、林鸿宇、陆耀杰、贺本、姜珊珊、董斌。2024 年。[Chatgpt 是一个知识丰富但经验不足的解题者：对大型语言模型中常识问题的调查](http://arxiv.org/abs/2303.16421)。

+   Bisk 等（2020）约纳坦·比斯克、罗温·泽勒斯、蒋峰、高业金等。2020 年。Piqa：关于自然语言中的物理常识的推理。见于*AAAI 人工智能会议论文集*，第 34 卷，页 7432–7439。

+   Blair-Stanek 等（2023）安德鲁·布莱尔-斯坦克、尼尔斯·霍尔岑伯格和本杰明·范·杜尔梅。2023 年。GPT-3 能进行法定推理吗？见于*第十九届国际人工智能与法律会议论文集*，页 22–31。

+   Cai 等（2024）蔡天乐、王雪智、马腾宇、陈欣云和周登喜。2024 年。[大型语言模型作为工具制造者](http://arxiv.org/abs/2305.17126)。

+   Carlini 等（2023）尼古拉斯·卡林尼、达芙妮·伊波利托、马修·贾吉尔斯基、凯瑟琳·李、弗洛里安·特拉默和张淇源。2023 年。[量化神经语言模型中的记忆](http://arxiv.org/abs/2202.07646)。

+   Casino 等（2022）弗兰·卡西诺、托马斯·K·达萨克利斯、乔治奥斯·P·斯帕图拉斯、马里奥斯·安纳格诺斯托普洛斯、阿姆里塔·戈萨尔、伊什万·博罗茨、阿古斯提·索拉纳斯、毛罗·孔蒂和康斯坦丁诺斯·帕萨基斯。2022 年。数字取证中的研究趋势、挑战和新兴主题：综述。*IEEE Access*，10:25464–25493。

+   Chang 等（2023）常玉鹏、王旭、王进东、吴源、杨林仪、朱凯杰、陈浩、易晓源、王存翔、王义东等。2023 年。关于大型语言模型评估的调查。*ACM 智能系统与技术交易*。

+   Chen 等（2020）陈文虎、查汉文、陈志宇、熊文汉、王洪和王威廉。2020 年。Hybridqa：一个多跳问题回答的数据集，涵盖表格和文本数据。*arXiv 预印本 arXiv:2004.07347*。

+   Chen 等人（2022）Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, 和 William Yang Wang. 2022. Convfinqa：探索会话金融问答中的数字推理链。*arXiv 预印本 arXiv:2210.03849*。

+   Cifuentes 等人（2022）Jenny Cifuentes, Ana Lucila Sandoval Orozco, 和 Luis Javier Garcia Villalba. 2022. 自动检测色情视频的人工智能策略综述。*多媒体工具与应用*，81(3):3205–3222。

+   Cobbe 等人（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, 等人. 2021. 训练验证者解决数学文字问题。*arXiv 预印本 arXiv:2110.14168*。

+   Dalvi 等人（2021）Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, 和 Peter Clark. 2021. 使用蕴涵树解释答案。*arXiv 预印本 arXiv:2104.08661*。

+   Daniel 等人（2008）Gilles Daniel, Didier Sornette, 和 Peter Wohrmann. 2008. 投资组合绩效评估中的前瞻性基准偏差。*arXiv 预印本 arXiv:0810.1922*。

+   Das 等人（2024）Badhan Chandra Das, M Hadi Amini, 和 Yanzhao Wu. 2024. 大型语言模型的安全性和隐私挑战：一项调查。*arXiv 预印本 arXiv:2402.00888*。

+   Demszky 等人（2023）Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, 等人. 2023. 在心理学中使用大型语言模型。*自然评论心理学*，2(11):688–701。

+   Deng 等人（2020）Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, 和 Matthew Richardson. 2020. 结构驱动的文本到 SQL 预训练。*arXiv 预印本 arXiv:2010.12773*。

+   Deng 等人（2023）Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, 和 Yu Su. 2023. [Mind2web: 面向网络的通用智能体](http://arxiv.org/abs/2306.06070)。

+   Deroy 等人（2023）Aniket Deroy, Kripabandhu Ghosh, 和 Saptarshi Ghosh. 2023. 预训练的抽象模型和大型语言模型在法律案件判断总结中的准备情况如何？*arXiv 预印本 arXiv:2306.01248*。

+   Dijkstra 等人（2022）Ramon Dijkstra, Zülküf Genç, Subhradeep Kayal, Jaap Kamps, 等人. 2022. 使用生成预训练变换器生成阅读理解测验。在 *iTextbooks@ AIED*，第 4–17 页。

+   Du 等人（2014）Shichuan Du, Yong Tao, 和 Aleix M Martinez. 2014. 复合面部情绪表达。*美国国家科学院院刊*，111(15):E1454–E1462。

+   Duan 等人（2024）Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, 和 Ning Gu. 2024. [Denevil: 通过指令学习解码和导航大型语言模型的伦理价值](http://arxiv.org/abs/2310.11053)。

+   Dubois 等 (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, 和 Tatsunori B Hashimoto. 2024. Alpacafarm：一个从人类反馈中学习的方法的仿真框架。*神经信息处理系统进展*，36。

+   Engel 和 Mcadams (2024) Christoph Engel 和 Richard H Mcadams. 2024. 向 gpt 询问法定术语的普通含义。*MPI Collective Goods Discussion Paper*, (2024/5)。

+   Fan 等 (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, 和 Michael Auli. 2019. Eli5：长篇问答。*arXiv 预印本 arXiv:1907.09190*。

+   Feldman 等 (2023) Philip Feldman, James R. Foulds, 和 Shimei Pan. 2023. [利用标记上下文提示捕捉 llm 幻觉](http://arxiv.org/abs/2306.06085)。

+   Gao 等 (2023) Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, 和 Jingren Zhou. 2023. 利用大型语言模型的文本到 SQL：基准评估。*arXiv 预印本 arXiv:2308.15363*。

+   Geva 等 (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, 和 Jonathan Berant. 2021. 亚里士多德是否使用过笔记本电脑？一个包含隐含推理策略的问题回答基准。*计算语言学协会会刊*，9:346–361。

+   Ghallab 等 (2004) Malik Ghallab, Dana Nau, 和 Paolo Traverso. 2004. *自动规划：理论与实践*。Elsevier。

+   Ghosh 等 (2024) Shaona Ghosh, Prasoon Varshney, Erick Galinkin, 和 Christopher Parisien. 2024. [Aegis: 在线自适应 ai 内容安全审核与 llm 专家集成](http://arxiv.org/abs/2404.05993)。

+   Guo 等 (2023) Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong 等. 2023. 评估大型语言模型：一项全面调查。*arXiv 预印本 arXiv:2310.19736*。

+   Han 等 (2023) Simon J. Han, Keith Ransom, Andrew Perfors, 和 Charles Kemp. 2023. [人类与大型语言模型中的归纳推理](http://arxiv.org/abs/2306.06548)。

+   Hendrycks 等 (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 2021. 用数学数据集测量数学问题解决能力。*arXiv 预印本 arXiv:2103.03874*。

+   Holzenberger 等 (2020) Nils Holzenberger, Andrew Blair-Stanek, 和 Benjamin Van Durme. 2020. 用于税法推理和问题回答的数据集。*arXiv 预印本 arXiv:2005.05257*。

+   Hort 等 (2021) Max Hort, Jie M Zhang, Federica Sarro, 和 Mark Harman. 2021. Fairea：一种模型行为变异方法用于基准测试偏差缓解方法。在 *第 29 届 ACM 欧洲软件工程会议联合会议与软件工程基础研讨会*，第 994–1006 页。

+   Hu et al. (2023) Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, 和 Hang Zhao. 2023. [Chatdb: 用数据库作为符号记忆增强 LLM](http://arxiv.org/abs/2306.03901)。

+   Huang and Chang (2023) Jie Huang 和 Kevin Chen-Chuan Chang. 2023. [面向大语言模型的推理：一项调查](http://arxiv.org/abs/2212.10403)。

+   Huang et al. (2022a) Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch. 2022a. 语言模型作为零样本规划器：为具身代理提取可操作知识。见 *国际机器学习大会*, 页 9118–9147\. PMLR。

+   Huang et al. (2023) Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, 和 Li Fei-Fei. 2023. Voxposer: 用于机器人操控的可组合 3D 值图与语言模型。*arXiv 预印本 arXiv:2307.05973*。

+   Huang et al. (2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, 和 Brian Ichter. 2022b. 内在独白：通过规划与语言模型进行具身推理。见 *arXiv 预印本 arXiv:2207.05608*。

+   Ji et al. (2023) Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, 和 Yaodong Yang. 2023. [Beavertails: 通过人类偏好数据集提高 LLM 的安全对齐](http://arxiv.org/abs/2307.04657)。

+   Jia et al. (2021) Qinjin Jia, Jialin Cui, Yunkai Xiao, Chengyuan Liu, Parvez Rashid, 和 Edward F Gehringer. 2021. All-in-one: 多任务学习 BERT 模型用于评估同行评审。*arXiv 预印本 arXiv:2110.03895*。

+   Jiang et al. (2024) Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, 和 Shikun Zhang. 2024. Hal-eval: 一个通用且细粒度的幻觉评估框架用于大型视觉语言模型。*arXiv 预印本 arXiv:2402.15721*。

+   Jiang et al. (2020) Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, 和 Mohit Bansal. 2020. [Hover: 一个用于多跳事实提取和声明验证的数据集](http://arxiv.org/abs/2011.03088)。

+   Jin et al. (2024) Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang, 等. 2024. Attackeval: 如何评估对大型语言模型的越狱攻击效果。*arXiv 预印本 arXiv:2401.09002*。

+   Karamizadeh et al. (2023) Sasan Karamizadeh, Saman Shojae Chaeikar, 和 Alireza Jolfaei. 2023. 使用 Boltzmann 机和深度学习进行成人内容图像识别。*进化智能*, 16(4):1185–1194。

+   Karinshak et al. (2023) Elise Karinshak, Sunny Xun Liu, Joon Sung Park, 和 Jeffrey T Hancock. 2023. 与 AI 合作进行劝说：研究大型语言模型生成支持疫苗接种信息的能力。*ACM 人机交互会议录*, 7(CSCW1):1–29。

+   Kasneci 等人（2023）Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, 等人。2023。《ChatGPT 会带来好处吗？关于大型语言模型在教育中机遇与挑战的讨论》。*学习与个体差异*，103:102274。

+   Katz 等人（2024）Daniel Martin Katz, Michael James Bommarito, Shang Gao, 和 Pablo Arredondo。2024。《GPT-4 通过了律师资格考试》。*皇家学会 A 卷哲学交易*，382(2270):20230254。

+   Khan 等人（2024）Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, 和 Ethan Perez。2024。《与更具说服力的 LLMs 辩论会导致更真实的答案》。*arXiv 预印本 arXiv:2402.06782*。

+   Kim 等人（2024a）Minju Kim, Heuiyeen Yeen, 和 Myoung-Wan Koo。2024a。《基于上下文的暴力检测：一个韩国犯罪对话数据集》。在*计算语言学协会发现：EACL 2024*，第 603–623 页。

+   Kim 等人（2024b）Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, 和 Seong Joon Oh。2024b。《Propile: 探测大型语言模型中的隐私泄漏》。*神经信息处理系统进展*，36。

+   Kosinski（2023）Michal Kosinski。2023。《心智理论可能在大型语言模型中自发出现》。*arXiv 预印本 arXiv:2302.02083*，4:169。

+   Kumar（2023）Arun HS Kumar。2023。《分析 ChatGPT 工具以评估其在生物医学领域学术写作中的潜力》。*生物学、工程学、医学与科学报告*，9(1):24–30。

+   Kung 等人（2023）Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, 等人。2023。《ChatGPT 在 USMLE 中的表现：利用大型语言模型进行 AI 辅助医学教育的潜力》。*PLoS 数字健康*，2(2):e0000198。

+   Kwiatkowski 等人（2019）Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, 等人。2019。《自然问题：一个问答研究的基准》。*计算语言学协会会刊*，7:453–466。

+   Lee 等人（2024）Jean Lee, Nicholas Stevens, Soyeon Caren Han, 和 Minseok Song。2024。《金融领域大型语言模型的调查（FinLLMs）》。*arXiv 预印本 arXiv:2402.02315*。

+   Li 等人（2024a）Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, 和 Jing Shao。2024a。[Salad-bench: 大型语言模型的分层和综合安全基准](http://arxiv.org/abs/2402.05044)。

+   Li 等人（2023a）Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, 和 Yongbin Li。2023a。[Api-bank: 一个全面的工具增强型 LLMs 基准](http://arxiv.org/abs/2304.08244)。

+   Li 等（2023b）Yinheng Li, Shaofei Wang, Han Ding, 和 Hang Chen. 2023b. 金融领域的大型语言模型：一项调查。见于*第四届 ACM 国际金融 AI 会议论文集*，页 374–382。

+   Li 等（2024b）Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, 和 Chongyang Tao. 2024b. 利用大型语言模型进行自然语言生成评估：一项调查。*arXiv 预印本 arXiv:2401.07103*。

+   Liang 等（2023）Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, 和 Andy Zeng. 2023. 代码作为政策：用于体现控制的语言模型程序。见于*2023 IEEE 国际机器人与自动化大会（ICRA）*，页 9493–9500。IEEE。

+   Liang 等（2022）Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar 等. 2022. 语言模型的整体评估。*arXiv 预印本 arXiv:2211.09110*。

+   Liang 等（2019）Yichan Liang, Jianheng Li, 和 Jian Yin. 2019. 用于课程学习的新型多选阅读理解数据集。见于*亚洲机器学习会议*，页 742–757。PMLR。

+   Liga 和 Robaldo（2023）Davide Liga 和 Livio Robaldo. 2023. 微调 gpt-3 以进行法律规则分类。*计算机法律与安全评论*，51:105864。

+   Lin（2004）Chin-Yew Lin. 2004. Rouge：一个自动评估摘要的软件包。见于*文本摘要的分支*，页 74–81。

+   Lin 等（2022）Stephanie Lin, Jacob Hilton, 和 Owain Evans. 2022. [Truthfulqa：衡量模型如何模仿人类虚假信息](http://arxiv.org/abs/2109.07958)。

+   Lin 等（2023）Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, 和 Jingbo Shang. 2023. [Toxicchat：揭示现实世界用户-AI 对话中毒性检测的隐性挑战](http://arxiv.org/abs/2310.17389)。

+   Liu 等（2024a）Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, 和 Li Zhang. 2024a. 探索与评估 llm 驱动的代码生成中的幻觉。*arXiv 预印本 arXiv:2404.00971*。

+   Liu 等（2023）Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, 和 Yue Zhang. 2023. [评估 chatgpt 和 gpt-4 的逻辑推理能力](http://arxiv.org/abs/2304.03439)。

+   Liu 等（2024b）Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, 和 Chaowei Xiao. 2024b. 针对大型语言模型的自动化和普遍提示注入攻击。*arXiv 预印本 arXiv:2403.04957*。

+   Liu 等（2021a）Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, 和 Guoying Zhao. 2021a. imigue：一个无身份的视频数据集，用于微手势理解和情感分析。见于*IEEE/CVF 计算机视觉与模式识别会议论文集*，页 10631–10642。

+   Liu 等（2021b）Zhuang Liu、Degen Huang、Kaiyu Huang、Zhuang Li 和 Jun Zhao。2021b 年。Finbert：用于金融文本挖掘的预训练金融语言表示模型。在 *第二十九届国际人工智能联合会议论文集* 中，第 4513–4519 页。

+   Lu 等（2024）Hao Lu、Xuesong Niu、Jiyao Wang、Yin Wang、Qingyong Hu、Jiaqi Tang、Yuting Zhang、Kaishen Yuan、Bin Huang、Zitong Yu 等。2024 年。GPT 作为心理学家？对 GPT-4v 在视觉情感计算上的初步评估。*arXiv 预印本 arXiv:2403.05916*。

+   Mahowald 等（2023）Kyle Mahowald、Anna A Ivanova、Idan A Blank、Nancy Kanwisher、Joshua B Tenenbaum 和 Evelina Fedorenko。2023 年。大语言模型中语言与思维的解离：认知视角。*arXiv 预印本 arXiv:2301.06627*。

+   Maia 等（2018）Macedo Maia、Siegfried Handschuh、André Freitas、Brian Davis、Ross McDermott、Manel Zarrouk 和 Alexandra Balahur。2018 年。Www’18 开放挑战：金融情感分析和问答。在 *2018 年网络会议伴随论文* 中，第 1941–1942 页。

+   Malo 等（2014）Pekka Malo、Ankur Sinha、Pekka Korhonen、Jyrki Wallenius 和 Pyry Takala。2014 年。好债还是坏债：检测经济文本中的语义倾向。*信息科学与技术协会杂志*，65(4):782–796。

+   Mann 等（2020）Ben Mann、N Ryder、M Subbiah、J Kaplan、P Dhariwal、A Neelakantan、P Shyam、G Sastry、A Askell、S Agarwal 等。2020 年。语言模型是少量样本学习者。*arXiv 预印本 arXiv:2005.14165*。

+   Markov 等（2023）Todor Markov、Chong Zhang、Sandhini Agarwal、Tyna Eloundou、Teddy Lee、Steven Adler、Angela Jiang 和 Lilian Weng。2023 年。[一种对现实世界中不良内容检测的整体方法](http://arxiv.org/abs/2208.03274)。

+   Mavadati 等（2013）S Mohammad Mavadati、Mohammad H Mahoor、Kevin Bartlett、Philip Trinh 和 Jeffrey F Cohn。2013 年。Disfa：自发面部动作强度数据库。*IEEE 情感计算学报*，4(2):151–160。

+   Menick 等（2022）Jacob Menick、Maja Trebacz、Vladimir Mikulik、John Aslanides、Francis Song、Martin Chadwick、Mia Glaese、Susannah Young、Lucy Campbell-Gillingam、Geoffrey Irving 等。2022 年。教授语言模型通过已验证的引用来支持回答。arxiv。

+   Miao 等（2023）Ning Miao、Yee Whye Teh 和 Tom Rainforth。2023 年。Selfcheck：使用 LLMs 自检其逐步推理。*arXiv 预印本 arXiv:2308.00436*。

+   Mihaylov 等（2018）Todor Mihaylov、Peter Clark、Tushar Khot 和 Ashish Sabharwal。2018 年。盔甲能导电吗？用于开放式问题回答的新数据集。在 *EMNLP* 中。

+   Min 等（2023）Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, 和 Dan Roth. 2023. 通过大型预训练语言模型的自然语言处理的最新进展：综述。*ACM 计算机调查*, 56(2):1–40。

+   Moon 等（2014）Sungrim Moon, Serguei Pakhomov, Nathan Liu, James O Ryan, 和 Genevieve B Melton. 2014. 使用临床笔记和医学词典资源创建的临床缩写和首字母缩略词的感知清单。*美国医学信息学会期刊*, 21(2):299–307。

+   Muthukrishnan 等（2020）Nikesh Muthukrishnan, Farhad Maleki, Katie Ovens, Caroline Reinhold, Behzad Forghani, Reza Forghani 等. 2020. 人工智能简史。*北美神经影像学诊所*, 30(4):393–399。

+   Nakano 等（2022）Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, 和 John Schulman. 2022. [Webgpt：带有人工反馈的浏览器辅助问答](http://arxiv.org/abs/2112.09332)。

+   Nayerifard 等（2023）Tahereh Nayerifard, Haleh Amintoosi, Abbas Ghaemi Bafghi, 和 Ali Dehghantanha. 2023. 数字取证中的机器学习：系统文献综述。*arXiv 预印本 arXiv:2306.04965*。

+   Nolfi（2023）Stefano Nolfi. 2023. 大型语言模型的意外能力。*arXiv 预印本 arXiv:2308.09720*。

+   Oviedo-Trespalacios 等（2023）Oscar Oviedo-Trespalacios, Amy E Peden, Thomas Cole-Hunter, Arianna Costantini, Milad Haghani, JE Rod, Sage Kelly, Helma Torkamaan, Amina Tariq, James David Albert Newton 等. 2023. 使用 ChatGPT 获取常见安全相关信息和建议的风险。*安全科学*, 167:106244。

+   Palmirani 和 Vitali（2011）Monica Palmirani 和 Fabio Vitali. 2011. 法律文档的 Akoma-ntoso。*面向语义网的立法 XML：文档管理的原则、模型、标准*, 页 75–100。

+   Papineni 等（2002）Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing Zhu. 2002. Bleu：一种自动评估机器翻译的方法。见于 *第 40 届计算语言学协会年会论文集*, 页 311–318。

+   Parrish 等（2021）Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, 和 Samuel R Bowman. 2021. Bbq：一种手动构建的问答偏差基准。*arXiv 预印本 arXiv:2110.08193*。

+   Patil 等（2023）Shishir G. Patil, Tianjun Zhang, Xin Wang, 和 Joseph E. Gonzalez. 2023. [Gorilla: 大型语言模型连接大量 API](http://arxiv.org/abs/2305.15334)。

+   Petroni 等人（2019）Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, 和 Sebastian Riedel。2019。语言模型可以作为知识库吗？ *arXiv 预印本 arXiv:1909.01066*。

+   Pinar Saygin 等人（2000）Ayse Pinar Saygin, Ilyas Cicekli, 和 Varol Akman。2000。图灵测试：50 年后。 *思维与机器*，10(4):463–518。

+   Qian 等人（2023）Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, 和 Heng Ji。2023。 [Creator: 用于解开大型语言模型的抽象与具体推理的工具创建](http://arxiv.org/abs/2305.14318)。

+   Qin 等人（2023a）Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, 和 Jie Zhou。2023a。 [Webcpm：用于中文长篇问答的交互式网络搜索](http://arxiv.org/abs/2305.06849)。

+   Qin 等人（2023b）Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, 和 Maosong Sun。2023b。 [使用基础模型进行工具学习](http://arxiv.org/abs/2304.08354)。

+   Qin 等人（2023c）Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, 和 Maosong Sun。2023c。 [Toolllm: 促进大型语言模型掌握 16000+现实世界 API](http://arxiv.org/abs/2307.16789)。

+   Radford 等人（2018）Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 等。2018。通过生成性预训练改善语言理解。

+   Radford 等人（2019）Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 等。2019。语言模型是无监督的多任务学习者。 *OpenAI 博客*，1(8):9。

+   Raina 和 Gales（2022）Vatsal Raina 和 Mark Gales。2022。多选题生成：迈向自动化评估框架。 *arXiv 预印本 arXiv:2209.11830*。

+   Rana 等人（2023）Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, 和 Niko Suenderhauf。2023。 [Sayplan：使用 3D 场景图为可扩展任务规划奠定大型语言模型的基础](https://openreview.net/forum?id=wMpOMO0Ss7a)。在 *第七届机器人学习年会* 上。

+   Rathje 等人（2023）Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, 和 Jay J Van Bavel。2023。Gpt 是进行多语种心理文本分析的有效工具。

+   Ren 等人（2023）Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley 等。2023 年。寻求帮助的机器人：大型语言模型规划者的不确定性对齐。*arXiv 预印本 arXiv:2307.01928*。

+   Ruan 等人（2023）Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng 和 Rui Zhao。2023 年。[Tptu：基于大语言模型的任务规划和工具使用的 AI 代理](http://arxiv.org/abs/2308.03427)。

+   Salehi 和 Burgueño（2018）Hadi Salehi 和 Rigoberto Burgueño。2018 年。结构工程中的新兴人工智能方法。*Engineering structures*, 171:170–189。

+   Scherrer 等人（2023）Nino Scherrer, Claudia Shi, Amir Feder 和 David M. Blei。2023 年。[评估大语言模型中编码的道德信念](http://arxiv.org/abs/2307.14324)。

+   Schick 等人（2023）Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda 和 Thomas Scialom。2023 年。[Toolformer：语言模型可以自我学习使用工具](http://arxiv.org/abs/2302.04761)。

+   Semigran 等人（2015）Hannah L Semigran, Jeffrey A Linder, Courtney Gidengil 和 Ateev Mehrotra。2015 年。自我诊断和分诊的症状检查器评估：审计研究。*bmj*, 351。

+   Shah 等人（2023）Dhruv Shah, Michael Robert Equi, Błażej Osiński, Fei Xia, Brian Ichter 和 Sergey Levine。2023 年。使用大语言模型进行导航：语义推测作为规划启发式方法。在*机器人学习会议*上，页码 2683–2699。PMLR。

+   Shah 等人（2022）Dhruv Shah, Blazej Osinski, Brian Ichter 和 Sergey Levine。2022 年。[LM-nav: 使用大规模预训练的语言、视觉和行动模型进行机器人导航](https://openreview.net/forum?id=UW5A3SweAH)。在*第六届年度机器人学习会议*上。

+   Shan 和 Deng（2018）Li Shan 和 Weihong Deng。2018 年。可靠的众包和深度局部保持学习用于无约束的面部表情识别。*IEEE 图像处理汇刊*, 28(1):356–370。

+   Sharma 和 Thakur（2023）Gaurav Sharma 和 Abhishek Thakur。2023 年。Chatgpt 在药物发现中的应用。

+   Shinn 等人（2023）Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan 和 Shunyu Yao。2023 年。[Reflexion：具有语言强化学习的语言代理](http://arxiv.org/abs/2303.11366)。

+   Singh 等人（2023）Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason 和 Animesh Garg。2023 年。Progprompt：使用大语言模型生成场景化机器人任务计划。在*2023 IEEE 国际机器人与自动化会议（ICRA）*上，页码 11523–11530。IEEE。

+   Sinha and Khandait (2021) Ankur Sinha 和 Tanmay Khandait. 2021. 新闻对商品市场的影响：数据集和结果。在 *信息与通信的进展：2021 年信息与通信未来会议 (FICC) 会议录，第 2 卷* 中，第 589–601 页。Springer。

+   Song et al. (2023a) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, 和 Yu Su. 2023a. Llm-planner: 基于大型语言模型的少量示例基础规划用于具身智能体。在 *IEEE/CVF 国际计算机视觉会议 (ICCV) 会议录* 中。

+   Song et al. (2023b) Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, 和 Sujian Li. 2023b. [Restgpt: 连接大型语言模型与真实世界的休闲 API](http://arxiv.org/abs/2306.06624)。

+   Sorensen et al. (2024) Taylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, 等. 2024. 价值万花筒：与多元化人类价值观、权利和责任的 AI 互动。在 *AAAI 人工智能会议会议录* 中，第 38 卷，第 19937–19947 页。

+   Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, 等. 2022. 超越模仿游戏：量化和推断语言模型的能力。 *arXiv 预印本 arXiv:2206.04615*。

+   Srivastava 等人 (2023) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Tr

+   Staab et al. (2023) Robin Staab, Mark Vero, Mislav Balunović, 和 Martin Vechev。2023 年。超越记忆：通过推断大型语言模型侵犯隐私。*arXiv 预印本 arXiv:2310.07298*。

+   Stolfo et al. (2023) Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, 和 Mrinmaya Sachan。2023 年。[一个因果框架来量化语言模型的数学推理鲁棒性](http://arxiv.org/abs/2210.12023)。

+   Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, 和 Chao Zhang。2023 年。[Adaplanner：基于反馈的语言模型自适应规划](https://proceedings.neurips.cc/paper_files/paper/2023/file/b5c8c1c117618267944b2617add0a766-Paper-Conference.pdf)。在*Advances in Neural Information Processing Systems*，第 36 卷，第 58202–58245 页。Curran Associates, Inc.

+   Sun et al. (2024) Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, 和 Zhenguo Li。2024 年。[对基础模型推理的调查](http://arxiv.org/abs/2312.11562)。

+   Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, 等人。2022 年。挑战大型基准任务以及思维链是否能够解决这些任务。*arXiv 预印本 arXiv:2210.09261*。

+   Talmor et al. (2018) Alon Talmor, Jonathan Herzig, Nicholas Lourie, 和 Jonathan Berant。2018 年。Commonsenseqa：一个针对常识知识的问答挑战。*arXiv 预印本 arXiv:1811.00937*。

+   Tang et al. (2024) Liyan Tang, Igor Shalyminov, Amy Wing mei Wong, Jon Burnsky, Jake W. Vincent, Yu’an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, 和 Kathleen McKeown。2024 年。[Tofueval：评估大型语言模型在主题集中对话总结中的幻觉](http://arxiv.org/abs/2402.13249)。

+   Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, 和 Le Sun。2023 年。[Toolalpaca：用于语言模型的通用工具学习，基于 3000 个模拟案例](http://arxiv.org/abs/2306.05301)。

+   Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, 和 Daniel Shu Wei Ting。2023 年。医学中的大型语言模型。*Nature medicine*，29(8):1930–1940。

+   Tobia (2020) Kevin P Tobia。2020 年。测试普通意义。*Harv. L. Rev.*，134:726。

+   Turpin et al. (2023) Miles Turpin, Julian Michael, Ethan Perez, 和 Samuel R. Bowman。2023 年。[语言模型并不总是说出它们的真实想法：思维链提示中的不忠实解释](http://arxiv.org/abs/2305.04388)。

+   Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制才是你需要的一切。*神经信息处理系统进展*, 30.

+   Vermetten et al. (2022) Diederick Vermetten, Bas van Stein, Fabio Caraffini, Leandro L Minku, 和 Anna V Kononova. 2022. Bias: 用于在连续域中基准测试结构偏差的工具箱。*IEEE 进化计算学报*, 26(6):1380–1393.

+   Vidgen et al. (2024) Bertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Borhane Blili-Hamelin, Kurt Bollacker, Rishi Bomassani, Marisa Ferrara Boston, Siméon Campos, Kal Chakra, Canyu Chen, Cody Coleman, Zacharie Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg, James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott A. Hale, Wiebke Hutiri, Joseph Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu, Foutse Khomh, Bhavya Kailkhura, Hannah Rose Kirk, Kevin Klyman, Chris Knotz, Michael Kuchnik, Shachi H. Kumar, Chris Lengerich, Bo Li, Zeyi Liao, Eileen Peters Long, Victor Lu, Yifan Mai, Priyanka Mary Mammen, Kelvin Manyeki, Sean McGregor, Virendra Mehta, Shafee Mohammed, Emanuel Moss, Lama Nachman, Dinesh Jinenhally Naganna, Amin Nikanjam, Besmira Nushi, Luis Oala, Iftach Orr, Alicia Parrish, Cigdem Patlak, William Pietri, Forough Poursabzi-Sangdeh, Eleonora Presani, Fabrizio Puletti, Paul Röttger, Saurav Sahay, Tim Santos, Nino Scherrer, Alice Schoenauer Sebag, Patrick Schramowski, Abolfazl Shahbazi, Vin Sharma, Xudong Shen, Vamsi Sistla, Leonard Tang, Davide Testuggine, Vithursan Thangarasa, Elizabeth Anne Watkins, Rebecca Weiss, Chris Welty, Tyler Wilbers, Adina Williams, Carole-Jean Wu, Poonam Yadav, Xianjun Yang, Yi Zeng, Wenhui Zhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang, Peter Mattson, 和 Joaquin Vanschoren. 2024. [介绍 mlcommons 的 ai 安全基准 v0.5](http://arxiv.org/abs/2404.12241).

+   Wang et al. (2024a) Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, 和 Jitao Sang. 2024a. [Amber: 一个无需大语言模型的多维基准，用于评估多语言模型的幻觉](http://arxiv.org/abs/2311.07397).

+   Wang et al. (2024b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, 和 Jirong Wen. 2024b. [基于大型语言模型的自主代理调查](https://doi.org/10.1007/s11704-024-40231-1). *计算机科学前沿*, 18(6).

+   Wang et al. (2024c) Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, 和 Xuanjing Huang. 2024c. 基准自演变：用于动态大语言模型评估的多智能体框架。*arXiv 预印本 arXiv:2402.11443*.

+   Wang et al. (2018) Su Wang, Greg Durrett, 和 Katrin Erk. 2018. 通过注入世界知识建模语义合理性。 *arXiv 预印本 arXiv:1804.00619*。

+   Wang et al. (2023a) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. 2023a. [Self-instruct: 通过自生成指令对齐语言模型](http://arxiv.org/abs/2212.10560)。

+   Wang et al. (2023b) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, 和 Yitao Liang. 2023b. 描述、解释、规划和选择：与大型语言模型的互动规划实现开放世界多任务代理。 *arXiv 预印本 arXiv:2302.01560*。

+   Wei et al. (2024) Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, 和 Yang Liu. 2024. 通过专业加权测量和减少 LLM 幻觉，无需金标准答案。 *arXiv 预印本 arXiv:2402.10412*。

+   Wessel et al. (2023) Martin Wessel, Tomás Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, 和 Timo Spinde. 2023. 介绍 MBIB——首个媒体偏见识别基准任务和数据集收集。在 *第 46 届国际 ACM SIGIR 信息检索研究与开发会议论文集*，第 2765–2774 页。

+   Weston et al. (2015) Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, 和 Tomas Mikolov. 2015. 朝着 AI 完整的问答系统：一组先决条件玩具任务。 *arXiv 预印本 arXiv:1502.05698*。

+   Wu and Aji (2023) Minghao Wu 和 Alham Fikri Aji. 2023. [风格重于实质：大型语言模型的评估偏差](http://arxiv.org/abs/2307.03025)。

+   Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, 和 Gideon Mann. 2023. Bloomberggpt: 一个用于金融的大型语言模型。 *arXiv 预印本 arXiv:2303.17564*。

+   Xie et al. (2023) Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, 和 Jimin Huang. 2023. Pixiu: 一个金融领域的大型语言模型、指令数据和评估基准。 *arXiv 预印本 arXiv:2306.05443*。

+   Xu et al. (2023a) Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, 和 Erik Cambria. 2023a. [大型语言模型真的擅长逻辑推理吗？全面评估及其超越](http://arxiv.org/abs/2306.09841)。

+   Xu et al. (2023b) Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, 和 Jingren Zhou. 2023b. [Cvalues: 测量中国大型语言模型的价值，从安全到责任](http://arxiv.org/abs/2307.09705)。

+   Xu et al. (2023c) Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, 和 Jian Zhang. 2023c. [开源大型语言模型的工具操作能力](http://arxiv.org/abs/2305.16504)。

+   燕等（2024a）燕碧伟、李昆、徐名辉、董悦言、张跃、任兆春和程秀珍。2024a。[关于保护大型语言模型（llms）数据隐私的调查](http://arxiv.org/abs/2403.05156)。

+   燕等（2024b）燕凡佳、毛欢志、吉查理·程杰、张天君、帕蒂尔·G·希什尔、斯托伊卡和冈萨雷斯·约瑟夫。2024b。伯克利函数调用排行榜。

+   燕等（2014）燕文静、李晓白、王素晶、赵国英、刘勇金、陈玉欣和傅晓兰。2014 年。Casme ii：一个改进的自发微表情数据库及基线评估。*PloS one*，9(1)：e86041。

+   杨等（2023）杨世平、孙仁亮和万晓君。2023 年。[一种用于段级幻觉检测的新基准和反向验证方法](http://arxiv.org/abs/2310.06498)。

+   杨等（2018）杨智霖、齐鹏、张赛争、贝吉奥、威廉·W·科恩、萨拉赫图丁诺夫和克里斯托弗·D·曼宁。2018 年。Hotpotqa：一个多样化的、可解释的多跳问答数据集。*arXiv 预印本 arXiv:1809.09600*。

+   姚等（2023a）姚顺宇、陈霍华德、杨约翰和纳拉辛汉。2023a。[Webshop：面向可扩展的现实世界网页交互与基础语言代理](http://arxiv.org/abs/2207.01206)。

+   姚等（2023b）姚顺宇、赵杰弗里、俞典、杜楠、沙夫兰和纳拉辛汉。2023b。[React：在语言模型中协同推理与行动](http://arxiv.org/abs/2210.03629)。

+   姚等（2024）姚一凡、段金豪、徐凯迪、蔡元方、孙志博和张跃。2024 年。[关于大型语言模型（llm）安全性和隐私的调查：优点、缺点与不足](https://doi.org/10.1016/j.hcc.2024.100211)。*高信度计算*，第 100211 页。

+   叶等（2024）叶丹尼尔·温基特、艾桑·埃斯马拉迪和陈春辉。2024 年。[一种评估大型语言模型对提示注入攻击的韧性的新评价框架](http://arxiv.org/abs/2401.00991)。

+   杨等（2022）杨纳森、包启明、贝恩斯曼和维特布罗克。2022 年。Abductionrules：训练变换器解释意外输入。*arXiv 预印本 arXiv:2203.12186*。

+   于等（2023）于文浩、尼姆罗德·吉利亚迪、傅初源、克尔曼、李光辉、蒙特塞·冈萨雷斯·阿雷纳斯、赖昂·刘易斯·姜、汤姆·埃雷兹、伦纳德·哈森克莱弗、简·洪普利克、布莱恩·伊赫特、特德·肖、彭旭、安迪·曾、张廷楠、尼古拉斯·赫斯、多尔萨·萨迪赫、谭杰、尤瓦尔·塔萨和谢飞。2023 年。语言到奖励的机器人技能合成。*arXiv 预印本 arXiv:2306.08647*。

+   袁等（2023）袁正、袁洪毅、谭川琦、王伟和黄松芳。2023 年。[大型语言模型在算术任务中的表现如何？](http://arxiv.org/abs/2304.02015)

+   Yuan 等人 (2024) Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song 和 Bo Li. 2024. [Rigorllm: 针对不良内容的大型语言模型的弹性保护措施](http://arxiv.org/abs/2403.13031)。

+   Zhan 等人 (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying 和 Daniel Kang. 2024. Injecagent: 在工具集成的大型语言模型代理中基准测试间接提示注入。 *arXiv 预印本 arXiv:2403.02691*。

+   Zhang 等人 (2023) Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng 和 Xiangnan He. 2023. [ChatGPT 在推荐系统中公平吗？评估大型语言模型推荐中的公平性](https://doi.org/10.1145/3604915.3608860)。见于 *第 17 届 ACM 推荐系统会议论文集*，RecSys ’23\. ACM。

+   Zhang 等人 (2024a) Kechi Zhang, Jia Li, Ge Li, Xianjie Shi 和 Zhi Jin. 2024a. [Codeagent: 通过工具集成代理系统增强代码生成，用于真实世界的仓库级编码挑战](http://arxiv.org/abs/2401.07339)。

+   Zhang 等人 (2024b) Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi 和 Helen Meng. 2024b. 自我对齐以确保事实准确性: 通过自我评估减轻大型语言模型中的虚假信息。 *arXiv 预印本 arXiv:2402.09267*。

+   Zhang 和 Yang (2023) Xuanyu Zhang 和 Qing Yang. 2023. Xuanyuan 2.0: 一个拥有数百亿参数的大型中文金融聊天模型。见于 *第 32 届 ACM 国际信息与知识管理大会论文集*，第 4435–4439 页。

+   Zhang 等人 (2018) Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola 和 Le Song. 2018. 基于知识图谱的问答的变分推理。见于 *AAAI*。

+   Zhao 等人 (2023) Guoying Zhao, Xiaobai Li, Yante Li 和 Matti Pietikäinen. 2023. 面部微表情: 一览。 *IEEE 会议录*。

+   Zheng 等人 (2023a) Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong 和 Liwei Wang. 2023a. [朝向学习一个通用模型用于具身导航](http://arxiv.org/abs/2312.02010)。

+   Zheng 等人 (2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等人. 2024. 通过 mt-bench 和聊天机器人竞技场评估 LLM 作为评判者的能力。 *神经信息处理系统进展*，第 36 卷。

+   Zheng 等人 (2023b) Shen Zheng, Jie Huang 和 Kevin Chen-Chuan Chang. 2023b. [为什么 ChatGPT 在提供真实答案方面存在不足？](http://arxiv.org/abs/2304.10513)

+   Zhou 等人 (2023a) Gengze Zhou, Yicong Hong 和 Qi Wu. 2023a. Navgpt: 在视觉与语言导航中利用大型语言模型进行明确推理。 *arXiv 预印本 arXiv:2305.16986*。

+   Zhou 等人 (2023b) Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon 和 Graham Neubig. 2023b. [Webarena: 为构建自主代理提供的真实网站环境](http://arxiv.org/abs/2307.13854)。

+   Zielinski 等（2023）Chris Zielinski, Margaret Winker, Rakesh Aggarwal, Lorraine Ferris, Markus Heinemann, Jose Florencio Lapeña Jr, Sanjay Pai, Edsel Ing, Leslie Citrome 等。2023 年。关于 ChatGPT 和聊天机器人的 WAME 推荐，与学术出版物相关。

生成于 2024 年 6 月 3 日星期一 02:19:28，由 LaTeXML![吉祥物萨米](http://dlmf.nist.gov/LaTeXML/)
