<!--yml

category: 未分类

date: 2024-09-03 17:29:44

-->

# 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查

> 来源：[`arxiv.org/html/2404.16038`](https://arxiv.org/html/2404.16038)

1.  [1 介绍](https://arxiv.org/html/2404.16038v1#S1 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [2 方法论](https://arxiv.org/html/2404.16038v1#S2 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [3 概述](https://arxiv.org/html/2404.16038v1#S3 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [3.1 主要组件](https://arxiv.org/html/2404.16038v1#S3.SS1 "在 3 概述 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [4 技术](https://arxiv.org/html/2404.16038v1#S4 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [4.1 生成式人工智能用于视频内容生成](https://arxiv.org/html/2404.16038v1#S4.SS1 "在 4 技术 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [4.2 大型语言模型用于视频场景理解](https://arxiv.org/html/2404.16038v1#S4.SS2 "在 4 技术 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [4.3 大型语言模型用于视频流媒体](https://arxiv.org/html/2404.16038v1#S4.SS3 "在 4 技术 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [5 应用](https://arxiv.org/html/2404.16038v1#S5 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [5.1 生成](https://arxiv.org/html/2404.16038v1#S5.SS1 "在 5 应用 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [5.2 视频场景理解](https://arxiv.org/html/2404.16038v1#S5.SS2 "在 5 应用 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [5.3 流媒体](https://arxiv.org/html/2404.16038v1#S5.SS3 "在 5 应用 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [6 挑战](https://arxiv.org/html/2404.16038v1#S6 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [6.1 生成](https://arxiv.org/html/2404.16038v1#S6.SS1 "在 6 个挑战 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [6.2 理解](https://arxiv.org/html/2404.16038v1#S6.SS2 "在 6 个挑战 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

    1.  [6.3 流媒体](https://arxiv.org/html/2404.16038v1#S6.SS3 "在 6 个挑战 ‣ 关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [7 关注点](https://arxiv.org/html/2404.16038v1#S7 "在关于生成式人工智能和大型语言模型的视频生成、理解和流媒体调查")

1.  [8 结论](https://arxiv.org/html/2404.16038v1#S8 "在《生成式人工智能和大语言模型在视频生成、理解和流媒体中的应用调查》中")

# 生成式人工智能和大语言模型在视频生成、理解和流媒体中的应用调查

周鹏远    王林    刘志    郝艳斌    潘晖    塔科马    康佳舒尔朱 通讯作者：周鹏远 (zpymyyn@gmail.com)

###### 摘要

本论文深入考察了当前最前沿的 AI 技术，即生成式人工智能（Generative AI）和大语言模型（LLMs），如何重塑视频技术领域，包括视频生成、理解和流媒体。论文强调了这些技术在生成高度现实的视频方面的创新使用，这是缩小现实世界动态与数字创作之间差距的重大突破。研究还探讨了 LLMs 在视频理解中的高级能力，展示了它们在从视觉内容中提取有意义信息方面的有效性，从而增强了我们与视频的互动。在视频流媒体领域，论文讨论了 LLMs 如何促进更高效和以用户为中心的流媒体体验，根据个体观众的偏好调整内容传递。这一综合性回顾探讨了生成式人工智能和 LLMs 在视频相关任务中的当前成就、持续挑战和未来可能性，强调了这些技术在多媒体、网络和 AI 社区中推动视频技术领域发展的巨大潜力。

{IEEEImpStatement}

本论文通过考察生成式人工智能（Generative AI）和大语言模型（LLMs）在视频生成、理解和流媒体中的整合，贡献于视频技术领域。对这些技术的探索提供了对其在提升视频内容现实性和互动性方面潜力和局限性的基础理解。对 LLMs 在视频理解中的探索为无障碍和互动方面的进步奠定了基础，承诺提供更好的教育工具、改进的用户界面和先进的视频分析应用。此外，论文强调了 LLMs 在优化视频流媒体服务中的作用，带来了更加个性化和带宽高效的平台。这可能会为娱乐行业带来实质性好处，通过适应性的流媒体解决方案满足个体偏好。通过识别关键挑战和未来研究方向，论文指导了将 AI 与视频技术融合的持续努力，同时提升了对潜在伦理问题的关注。它的影响超越学术界，鼓励在视频技术领域负责任的 AI 开发和政策制定，在技术进步与伦理考量之间取得平衡。{IEEEkeywords} 生成式人工智能（AI），大语言模型（LLM），视频理解，视频生成，视频流媒体，GPT

## 1 引言

视频内容的创建、分析和交付近年来都经历了重大突破，这要归功于视频相关技术的激动人心的发展。学术界和工业界努力突破视频处理领域的可行极限，从创建逼真的视频到理解复杂的视觉环境，再到优化视频流媒体以改善用户体验。整合生成性 AI 和 LLM 可以在视频相关领域开启令人兴奋的可能性。

凭借创建逼真且上下文一致的视频的能力，视频创作已成为一个引人入胜的研究领域。研究人员通过利用深度学习方法，如生成对抗网络（GANs），在生成揭示细节并捕捉现实世界动态的电影片段方面取得了显著进展。然而，诸如长期视频合成一致性和对生成内容的细粒度控制等挑战仍在探索中。

在视频理解方面也取得了类似的发展，这涉及从视频片段中提取重要信息。传统技术依赖于手动创建的特征和显式建模的视频动态。语言和视觉的最新进展已经取得了显著进展。基于预训练的变换器架构，如 OpenAI 的 GPT 及其他 LLMs，通常在处理和生成文本数据方面表现出了令人印象深刻的能力。这些 LLMs 在视频理解任务中，如字幕生成、动作识别和时间定位，具有巨大的潜力。

此外，随着对高质量、高分辨率和低延迟视频服务需求的不断增长，提高视频传输质量变得越来越重要和具有挑战性。带宽限制、网络抖动和不同用户偏好显著阻碍了无缝和沉浸式流媒体体验的提供。通过提供上下文感知的视频分发、实时视频质量改进和根据用户偏好进行自适应流媒体，LLMs 提供了一种令人兴奋的方式来克服这些困难。

鉴于这些进展，本研究全面分析了生成性 AI 和 LLMs 在生成、理解和流媒体视频方面的潜力。我们回顾现有工作，尝试回答以下问题：

+   •

    什么技术已经被提议并正在革命性地改变上述视频研究领域？

+   •

    还有哪些技术挑战需要解决，以推动 GAI 和 LLM 方法在上述视频服务中的应用？

+   •

    由于使用 GAI 和 LLM 方法，提出了哪些独特的问题？

我们希望引起多媒体、网络和人工智能社区的关注，以鼓励在这一迷人且快速发展的领域进行未来的研究。

| 年份 | GenAI | LLM | 生成 | 理解 | 流媒体 | 总结 |
| --- | --- | --- | --- | --- | --- | --- |
| [[1](https://arxiv.org/html/2404.16038v1#bib.bib1)], 2020 | $\surd$ | X | $\surd$ | X | X | VAEs、GANs 和 Transformers 在视频生成中的概述。 |
| [[2](https://arxiv.org/html/2404.16038v1#bib.bib2)], 2023 | $\surd$ | X | $\surd$ | X | X | 研究文本到图像和文本到视频的人工智能生成器。 |
| [[3](https://arxiv.org/html/2404.16038v1#bib.bib3)], 2023 | $\surd$ | X | $\surd$ | X | X | 关注于生成说服性视频的人工智能方法。 |
| [[4](https://arxiv.org/html/2404.16038v1#bib.bib4)], 2022 | $\surd$ | X | $\surd$ | X | X | 关注于视频生成的 GAN 方法。 |
| [[5](https://arxiv.org/html/2404.16038v1#bib.bib5)], 2023 | X | X | X | $\surd$ | X | 关注于描述的深度学习方法。 |
| [[6](https://arxiv.org/html/2404.16038v1#bib.bib6)], 2020 | X | X | X | $\surd$ | X | 针对特定数据集的综述描述方法。 |
| [[7](https://arxiv.org/html/2404.16038v1#bib.bib7)], 2019 | X | X | X | $\surd$ | X | 针对基于人工智能的视频描述的方法、数据集和指标。 |
| 我们的, 2023 | $\surd$ | $\surd$ | $\surd$ | $\surd$ | $\surd$ | GenAI 和 LLM 在视频生成、理解和流媒体中的应用。 |

表 1：近年来相关的综述论文。

## 2 方法论

本综述旨在从广泛的视角探讨生成型人工智能与 LLMs 以及视频领域之间的互动。它涵盖了从 Google Scholar、IEEE Xplore、ACM Digital Library、Elsevier、ScienceDirect、DBLP 等收集的超过 100 篇论文。查询结合了以下关键词：生成型人工智能 / LLM $\&amp;$ 视频理解 / 分割 / 生成 / 流媒体，以及与第[3](https://arxiv.org/html/2404.16038v1#S3 "3 Overview ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")节讨论的关键技术相关的关键词。我们进一步通过添加在互联网上突出的研究来补充这些文章，以覆盖该领域的综合重要出版物。这个过程一直持续到没有发现新文章为止。我们仔细审查了这些论文，并挑选出最相关和最重要的文章，同时过滤掉了不那么相关的文章。所选论文构成了本综述的核心，我们在撰写过程中进行了持续更新，以涵盖我们开始过程以来发表的论文。注意，由于 2023 年相关领域的快速发展和大量出版物，可能有一些我们忽视的新论文；但我们已尽力而为。

## 3 概述

我们设想生成式人工智能和 LLMs 在视频的整个生命周期中起着关键作用，包括生成、理解和流媒体。该框架跨越了三个主要的计算机科学群体，即人工智能、多媒体和网络。人工智能社区正在见证一种前所未有的发展速度，从能够进行文本到图像生成的模型到能够进行文本到视频生成的模型，仅仅用了大约一年的时间，从 2021 年到 2022 年。现在甚至有演示显示出仅仅通过使用提示就能创建 3D 视频的能力。因此，我们可以想象生成式人工智能在视频生成行业中变得更加重要，超越甚至完全取代传统的生成方法。视频理解对许多情况非常有用，例如场景分割、活动监视、事件检测和视频标题，这是一个受到越来越多关注的不断上升的方向。自从 2023 年以来，LLMs 在理解图像和视频等多模态输入方面的能力也得到了像 GPT-4 和 Video-ChatGPT 这样最先进产品的显著提升 [[8](https://arxiv.org/html/2404.16038v1#bib.bib8)]。至于视频流，LLMs 也具有有趣的潜力来改善流媒体管道的几个关键步骤。例如，具有改进理解能力的模型可以理解视频场景的语义含义，并通过相应地改变编码率来优化传输。此外，像在 XR 游戏中广泛使用的点云这样的 3D 视频流，可以从 LLM 对环境的理解中受益，以预测用户在下一时刻的视场，并进行内容预取。

### 3.1 主要组成部分

生成式人工智能和 LLMs 之间的协同作用已经在视频生成领域开辟了新的前沿，制作出与现实越来越难以区分的视觉效果。这些技术共同致力于通过以下方式（第 [4.1](https://arxiv.org/html/2404.16038v1#S4.SS1 "4.1 生成式人工智能用于视频内容生成 ‣ 4 技术 ‣ 对生成式人工智能和 LLM 进行调查，了解和流媒体的视频生成")部分）丰富数字景观：

+   •

    GANs（生成对抗网络）利用生成和判别网络之间的创造性对抗过程来理解和复制复杂模式，从而产生逼真的视频样本。

+   •

    VAEs（变分自动编码器）生成连贯的视频序列，为帧无缝融合提供了一个结构化的概率框架，使得故事情节合情合理。

+   •

    自回归模型创建了一个连续的序列，其中每个视频帧逻辑上都是紧随上一个的，确保了叙事和视觉的连续性，令观众着迷。

+   •

    扩散模型将复杂的文本叙述转化为详细且高分辨率的视频，将文本到视频合成的边界推向更远。

接下来，LLMs 通过提供丰富的上下文解释和描述来提升视频理解，促进更深入的内容互动（第[4.2](https://arxiv.org/html/2404.16038v1#S4.SS2 "4.2 LLMs for Video Scene Understanding ‣ 4 Technologies ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")节）：

+   •

    视频字幕生成使用 LLMs 来生成富有洞察力和准确的描述，将视觉内容的本质用自然语言捕捉，使视频更具可搜索性和可访问性。

+   •

    视频问答利用 LLMs 的上下文理解能力来处理复杂的观众询问，提供增值和深度的回答，提升观看体验。

+   •

    LLMs 使视频检索和分割发生革命性变化，它们解析和分类视频内容为可理解的片段，从而简化了大规模视频库的搜索和导航。

图 1：视频生成、理解和流媒体的分类，结合 GAI 和 LLMs。

最后但同样重要的是，LLMs 可以通过优化带宽使用、个性化内容传递和提升观众互动，从以下几个角度重新定义流媒体环境（第[4.3](https://arxiv.org/html/2404.16038v1#S4.SS3 "4.3 LLM for Video Streaming ‣ 4 Technologies ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")节）：

+   •

    带宽预测通过 LLMs 得以精细化，这些模型分析过去和现在的网络数据，预测未来需求以主动分配资源，从而确保无缝流媒体。

+   •

    观点预测通过 LLMs 对内容和用户行为的理解得到增强，预测视频中的下一个关注区域，以提供量身定制和沉浸式的观看体验。

+   •

    视频推荐和资源分配通过 LLMs 的分析能力得到提升，将观众偏好与内容匹配，并管理网络资源以提供定制化和高效的流媒体服务。

## 4 技术

### 4.1 生成性 AI 用于视频内容生成

生成式人工智能作为一种强大的工具，已经出现，并能够创造各种内容，包括图像、文本、音乐和视频。在视频内容创建方面，生成式模型有潜力通过自动生成逼真且高质量的内容来彻底改变我们创建和消费视频的方式。生成式模型，特别是基于深度学习的生成式模型，如 GANs [[9](https://arxiv.org/html/2404.16038v1#bib.bib9)]、变分自编码器（VAEs）[[10](https://arxiv.org/html/2404.16038v1#bib.bib10)]、自回归模型[[11](https://arxiv.org/html/2404.16038v1#bib.bib11)]和基于扩散的模型[[12](https://arxiv.org/html/2404.16038v1#bib.bib12), [13](https://arxiv.org/html/2404.16038v1#bib.bib13), [14](https://arxiv.org/html/2404.16038v1#bib.bib14)]，在生成各个领域的逼真且多样化的内容方面取得了显著成功。这些模型通过在大规模数据集上训练来学习数据的潜在分布，从而生成与训练数据相似的样本。一些最先进的生成式模型列在表[2](https://arxiv.org/html/2404.16038v1#S4.T2 "表 2 ‣ 4.1 视频内容生成的生成式人工智能 ‣ 4 技术 ‣ 关于生成式人工智能和大语言模型的视频生成、理解和流媒体的调查")中。然而，由于视频的空间-时间属性、对照片级真实动态场景的需求以及处理视频数据的高昂成本，生成式人工智能模型在视频内容生成方面面临独特的挑战。尽管存在这些挑战，生成式模型在视频内容创建方面已经取得了显著进展。我们现在将详细讨论这些模型。

![参考说明](img/d0fea0e974cba9e938fb6609ce3dde4e.png)

图 2：先进的基于人工智能的视频生成技术概述。

表 2：用于视频内容生成的生成方法回顾。

| 方法 | 输入信息 | 任务 |
| --- | --- | --- |
| GAN 模型 |  |
| VideoGAN [[9](https://arxiv.org/html/2404.16038v1#bib.bib9)] | 视频 | 给定单张静态图像在封闭场景域中的视频生成和视频预测。 |
| EDN [[15](https://arxiv.org/html/2404.16038v1#bib.bib15)] | 视频 | 使用姿态作为中间表示的视频到视频翻译。 |
| VAE 模型 |  |
| SVG [[10](https://arxiv.org/html/2404.16038v1#bib.bib10)] | 视频 | 给定简单运动视频（如人类活动）的初始帧的视频预测 |
| SadTalker [[16](https://arxiv.org/html/2404.16038v1#bib.bib16)] | 图像, 音频 | 给定面部图像和一段语音音频的对话头像生成。 |
| 自回归模型 |  |
| Video Pixel Networks [[11](https://arxiv.org/html/2404.16038v1#bib.bib11)] | 视频 | 给定简单运动视频（如 MNIST 运动）的初始帧的视频预测。 |
| CogVideo [[17](https://arxiv.org/html/2404.16038v1#bib.bib17)] | 视频, 文本 | 文本到视频生成、视频预测和视频帧插值。 |
| 扩散模型 |  |
| VDM [[12](https://arxiv.org/html/2404.16038v1#bib.bib12)] | 视频, 文本/标签 | 基于文本或标签的视频生成和视频预测。 |
| Imagen-Video [[13](https://arxiv.org/html/2404.16038v1#bib.bib13)] | 视频, 文本 | 文本到视频生成、视频预测和视频帧插值。 |
| Make-a-Video [[14](https://arxiv.org/html/2404.16038v1#bib.bib14)] | 视频, 文本 | 文本到视频生成、视频预测和视频帧插值。 |
| Video LDM [[18](https://arxiv.org/html/2404.16038v1#bib.bib18)] | 视频, 文本 | 文本到视频生成，高分辨率视频合成。 |
| DreamTalk [[19](https://arxiv.org/html/2404.16038v1#bib.bib19)] | 图像, 音频 | 给定面部图像和语音音频生成对话头。 |
| Dancing Avatar [[20](https://arxiv.org/html/2404.16038v1#bib.bib20)] | 动作, 文本 | 通过文本描述和动作生成高质量的人类视频。 |
| Discro [[21](https://arxiv.org/html/2404.16038v1#bib.bib21)] | 动作, 文本 | 通过文本描述和动作生成高质量的人类视频。 |

GANs 由生成器和判别器组成，它们在一个二人对抗的博弈中进行训练。生成器学习生成逼真的样本，而判别器学习区分生成的样本（即假样本）和真实样本（即真样本）。在视频生成的应用中，GANs 已被扩展以建模时间一致性并生成逼真的视频帧。例如，VideoGAN [[9](https://arxiv.org/html/2404.16038v1#bib.bib9)] 引入了一种双流架构，分别建模视频中的外观和运动。生成器生成视频帧，而判别器评估单个帧的真实性以及连续帧之间的运动。这种方法在生成逼真的人类动作和场景视频方面取得了成功。

变分自编码器（VAEs）是一种生成模型，通过优化数据似然性的变分下界来学习数据空间与潜在空间之间的概率映射。在视频生成的背景下，VAEs 已被调整以建模视频的时间结构并生成视频序列。例如，Stochastic Video Generation (SVG) 框架 [[10](https://arxiv.org/html/2404.16038v1#bib.bib10)] 扩展了 VAEs 以建模基于过去帧的未来视频帧的分布。SVG 框架引入了潜在变量的层次结构，以捕捉视频数据的多尺度特性，从而实现生成多样化和逼真的视频序列。

自回归模型通过建模每个数据点在其前置数据点条件下的条件分布来生成数据。在视频生成的背景下，自回归模型可以用来顺序生成视频帧，每一帧都以之前生成的帧为条件。一个突出的例子是视频像素网络（Video Pixel Networks）[[11](https://arxiv.org/html/2404.16038v1#bib.bib11)]，这是一个自回归模型，它扩展了 PixelCNN[[22](https://arxiv.org/html/2404.16038v1#bib.bib22)]以建模视频数据。VPN 将视频编码为四维依赖链，其中时间依赖通过 LSTM 捕捉，空间和颜色依赖通过 PixelCNN 捕捉。另一方面，Transformer[[23](https://arxiv.org/html/2404.16038v1#bib.bib23)]建模序列数据，并在许多自然语言处理和视觉任务中表现良好。与基于 GAN 的方法相比，自回归模型能够处理连续数据和离散数据。

扩散模型将数据生成构建为去噪过程。近年来，扩散模型在视觉生成中取得了显著成功，并在大多数图像相关的合成或编辑任务中达到了显著的最先进性能。视频扩散模型（Video Diffusion Model, VDM）[[12](https://arxiv.org/html/2404.16038v1#bib.bib12)]是第一个通过将 U-net[[24](https://arxiv.org/html/2404.16038v1#bib.bib24)]扩展到 3D 版本，将扩散模型引入视频生成领域的工作。后来，Imagen-Video[[13](https://arxiv.org/html/2404.16038v1#bib.bib13)]凭借其强大的预训练文本-图像生成器 Imagen，在高分辨率文本-视频合成中展示了显著的能力。它在串行空间层中插入了时间注意力层，以捕捉运动信息。Make-a-Video[[14](https://arxiv.org/html/2404.16038v1#bib.bib14)]是另一个在文本-视频合成中强有力的竞争者，通过在 CLIP[[25](https://arxiv.org/html/2404.16038v1#bib.bib25)]语义空间上进行条件处理。它首先生成关键帧，基于文本先前的信息，然后通过几个插值和上采样扩散模型进行级联，以实现高一致性和保真度。然而，以上提到的先驱工作都面临高计算成本的问题，Video LDM[[18](https://arxiv.org/html/2404.16038v1#bib.bib18)]被提出以缓解这一问题，通过在语义压缩空间中生成运动感知的潜在表示。

### 4.2 LLMs 用于视频场景理解

视频场景理解是一个旨在从视频中提取有意义信息的任务。它包括识别视频中的对象、活动和事件，并理解它们之间的关系 [[26](https://arxiv.org/html/2404.16038v1#bib.bib26)]。生成式 AI 和 LLMs 由于能够从大量数据中学习并生成视频内容的自然语言描述，已经成为视频场景理解的有前途的方法 [[27](https://arxiv.org/html/2404.16038v1#bib.bib27)]。本文讨论了 LLMs 在视频场景理解中的应用，并回顾了一些近年来提出的技术。

视频场景理解涉及几个子任务，包括对象检测、动作识别和事件检测 [[28](https://arxiv.org/html/2404.16038v1#bib.bib28)]。对象检测旨在识别和定位视频中的对象，而动作识别则旨在识别诸如行走、跑步和跳跃的人类动作。事件检测旨在识别和分类事件，如事故、体育赛事和音乐会。这些子任务具有挑战性，因为视频复杂而动态，相同的对象或动作可以以不同的方式和背景出现。

LLMs 是神经网络模型，训练时使用大量文本数据来生成自然语言文本。这些模型在自然语言处理任务中取得了令人印象深刻的成果，如语言翻译、问答和文本生成。LLMs 还可以用于视频场景理解，通过生成自然语言描述来解析视频内容 [[27](https://arxiv.org/html/2404.16038v1#bib.bib27)]。这些描述可以帮助总结视频内容，并提供关于视频中对象、动作和事件的洞见。

图 3：LLMs 在视频场景理解任务中的概述。

已经提出了几种方法来利用 LLMs 执行视频场景理解中的不同任务。尽管不同任务对 LLMs 的使用方式有不同的要求，但我们发现它们共享一些共同组件，如从视频片段中提取时间和语义特征、语义和视频特征对齐等，如图 [3](https://arxiv.org/html/2404.16038v1#S4.F3 "Figure 3 ‣ 4.2 LLMs for Video Scene Understanding ‣ 4 Technologies ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming") 所示。接下来，我们讨论其中的一些技术及其优缺点。

视频字幕生成是一个涉及生成视频内容自然语言描述的任务 [[29](https://arxiv.org/html/2404.16038v1#bib.bib29), [30](https://arxiv.org/html/2404.16038v1#bib.bib30)]。这个任务可以通过使用 LLMs 来解决，通过在大规模的视频和相应字幕的数据集上训练它们。这个过程包括两个主要步骤。首先，提取的视觉和音频特征被编码为固定长度的向量表示，使用训练好的 LLM [[31](https://arxiv.org/html/2404.16038v1#bib.bib31), [32](https://arxiv.org/html/2404.16038v1#bib.bib32)]。这种编码捕捉了视频中的重要信息，并为生成准确的字幕提供了上下文提示。然后，LLM 生成视频的文本描述或字幕。这些字幕可以涵盖一系列细节，包括物体、动作、事件或任何其他有效描述视频内容的相关信息 [[33](https://arxiv.org/html/2404.16038v1#bib.bib33), [34](https://arxiv.org/html/2404.16038v1#bib.bib34)]。

使用 LLMs 进行视频字幕生成在多个领域中得到应用，包括提升听力障碍人士的可及性、促进视频搜索和检索、生成视频摘要以及改善对视频内容的总体理解 [[35](https://arxiv.org/html/2404.16038v1#bib.bib35)]。

视频问答是一个涉及回答关于视频内容自然语言问题的任务。这个任务可以通过使用 LLMs 来解决，通过在大规模的视频和相应问题及答案的数据集上训练它们 [[36](https://arxiv.org/html/2404.16038v1#bib.bib36), [37](https://arxiv.org/html/2404.16038v1#bib.bib37), [36](https://arxiv.org/html/2404.16038v1#bib.bib36)]。模型学习从视频内容中提取相关信息以回答问题。这种方法的优点是能够生成针对具体问题的特定答案。然而，这种方法的局限性在于它需要大量标注数据，并且可能无法捕捉视频内容的上下文和复杂性 [[38](https://arxiv.org/html/2404.16038v1#bib.bib38), [39](https://arxiv.org/html/2404.16038v1#bib.bib39), [40](https://arxiv.org/html/2404.16038v1#bib.bib40)]。

使用 LLMs 的视频检索是指使用先进的语言模型从大型视频数据库中搜索和检索相关视频的过程。LLMs 是强大的神经网络模型，能够根据大量训练数据理解和生成类似人类的文本[[41](https://arxiv.org/html/2404.16038v1#bib.bib41), [35](https://arxiv.org/html/2404.16038v1#bib.bib35)]。这一任务可以通过在大规模的视频数据集上进行训练来处理，该数据集包含相应的文本描述。代表性的方法[[33](https://arxiv.org/html/2404.16038v1#bib.bib33), [42](https://arxiv.org/html/2404.16038v1#bib.bib42)]学会将视频的视觉内容与相应的文本描述相关联，如图[3](https://arxiv.org/html/2404.16038v1#S4.F3 "Figure 3 ‣ 4.2 LLMs for Video Scene Understanding ‣ 4 Technologies ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")所示。借助 LLMs 的强大能力，可以实现更准确和高效的视频检索，提升用户体验，并增强视频数据库的实用性。然而，这种方法的局限性在于需要大量标记数据，并且可能无法捕捉视频内容的细微细节[[43](https://arxiv.org/html/2404.16038v1#bib.bib43), [44](https://arxiv.org/html/2404.16038v1#bib.bib44)]。

视频分割，即对视频中的对象或感兴趣区域进行分割的任务，可以从大型语言模型（LLMs）的应用中受益[[45](https://arxiv.org/html/2404.16038v1#bib.bib45)]。LLMs 通过利用其语言理解能力可以辅助语义视频分割。通过结合文本描述或提示，LLMs 可以指导分割过程，提供高层次的背景和语义理解。例如，LLMs 可以生成描述所需对象或区域的文本掩码或描述，帮助实现准确且与背景相关的分割[[31](https://arxiv.org/html/2404.16038v1#bib.bib31), [46](https://arxiv.org/html/2404.16038v1#bib.bib46)]。此外，视频分割通常需要时间推理，以准确分割随时间变化的对象或区域。LLMs 可以用来建模长期时间依赖性，并捕捉跨视频帧的背景信息。通过在语言提示中加入时间线索或用时间目标训练 LLMs，它们可以促进时间视频分割，实现更连贯、一致的分割[[35](https://arxiv.org/html/2404.16038v1#bib.bib35)]。

总而言之，由于 LLMs 能够从大量数据中学习并生成视频内容的自然语言描述，它们已经成为视频场景理解的一个有前途的方法。本文讨论的技术展示了 LLMs 在视频场景理解中的潜力。然而，这些技术也存在局限性，如对大量标注数据的需求以及无法捕捉视频内容的细粒度细节。需要进一步研究以提高 LLMs 在视频场景理解中的性能，并克服这些局限性。

### 4.3 LLM 用于视频流

接下来，我们从各个角度探讨 ChatGPT 类似的 LLMs 如何提升视频流体验。如图[4](https://arxiv.org/html/2404.16038v1#S4.F4 "图 4 ‣ 4.3 LLM 用于视频流 ‣ 4 技术 ‣ 关于生成 AI 和 LLM 在视频生成、理解和流媒体中的调查")¹¹1 请注意，有时特定工作中只考虑了系统的部分内容。一个典型的视频系统包括视频捕捉、视频编码（即压缩）、视频网络传输、视频解码和视频帧恢复。我们首先讨论具有挑战性的流行视频格式。然后总结 LLM 在视频流中的潜力，以应对这些挑战。

图 4：典型视频传输系统的示意图。感兴趣的场景由多个摄像头捕捉，压缩后的视频传输到服务器。这些视频通过骨干网分发，并从相应的无线基站直接接收给移动用户。

LLMs 用于带宽预测。未来的带宽预测是提高视频传输的一个根本问题。带宽数据是时间性的；目前，大量工作依赖于深度学习方法，如 LSTM 和 RNN。大规模预测模型在时间序列预测中提供了重要的优势，能够更好地预测未来的网络状况，并作为视频传输的基石。此外，在样本稀缺的新环境中，有效利用 LLMs 和迁移学习技术，即使在样本有限的情况下，也能产生有希望的结果。例如，Azmin 等人[[47](https://arxiv.org/html/2404.16038v1#bib.bib47)]提出了一种基于 transformer 的模型，专为 5G 数据集设计，相比于仅依赖 LSTM 的方案，展示了显著的改进。他们引入了新颖的特征分析技术，包括 LASSO 和更新超参数的随机森林，以及现有的 Informer 与随机森林。

LLMs 用于视口预测。VR/360°及其他沉浸式视频系统中的一个关键方面是视口预测，这涉及到准确预测用户在虚拟环境中的下一个视点 [[48](https://arxiv.org/html/2404.16038v1#bib.bib48), [49](https://arxiv.org/html/2404.16038v1#bib.bib49)]。这一预测对确保无缝且响应迅速的观看体验至关重要。为了增强视口预测，我们可以利用如 GPT-4 等 LLMs 的能力，这些模型在 NLP 和生成任务中表现出色。通过将这些语言模型适应处理视频相关数据，我们可以显著提高视角预测的准确性。该过程涉及在包含视频序列、用户交互模式和位置信息的大型数据集上训练 LLM，以学习用户行为中的复杂模式和依赖关系，从而改善用户下一个视角的预测。例如，[[50](https://arxiv.org/html/2404.16038v1#bib.bib50)] 的研究介绍了一种基于变换器的方法，用于预测 360°视频中的视口。该技术仅专注于分析过去的视口扫描路径，以实现精确的长期视口预测，同时保持低计算复杂性。在[[51](https://arxiv.org/html/2404.16038v1#bib.bib51)] 进行的研究中，变换器被纳入以评估其在注视估计中的有效性。通过保留卷积层并将 CNN 与变换器结合，变换器作为补充组件来提升 CNN 的整体性能，取得了卓越的表现。此外，[[52](https://arxiv.org/html/2404.16038v1#bib.bib52)] 通过时空变换器将注视特征与场景上下文及人-物对的视觉特征相结合，预测视频中的人-物交互。

视频压缩优化。LLMs 可以优化视频编码和压缩，减少文件大小，提高传输效率。例如，[[53](https://arxiv.org/html/2404.16038v1#bib.bib53)] 提出了一个针对深度视频压缩的掩蔽图像建模变换器。按照预训练语言/图像模型中的代理任务概念，该变换器经过训练以充分利用帧之间的时间相关性和空间标记，在少数自回归步骤中进行处理。与此同时，[[54](https://arxiv.org/html/2404.16038v1#bib.bib54)] 介绍了一种基于变换器的神经视频压缩方法，该方法优雅简单，超越了以前的方法，而不依赖于显式运动预测或扭曲等结构先验。

资源分配。在无线通信网络中，资源分配是一项关键任务，涉及高效地分配有限的网络资源，如带宽、功率和时间槽，给不同的用户和应用。视频流作为数据密集型且受欢迎的应用之一，需要仔细的资源分配，以确保用户能够获得流畅且高质量的视频传输。

大型语言模型（LLMs）可以处理和分析与视频流相关的各种文本输入，包括用户偏好、视频内容描述、网络状况和其他背景数据。利用这些信息，LLMs 可以更好地理解用户需求、视频特性和网络要求，从而提出优化的资源分配策略。这些策略旨在优先分配资源，以最大化视频流的质量，最小化缓冲或延迟问题，并提升整体用户体验。

此外，LLMs 可以从大量数据中持续学习，根据变化的网络条件和用户行为调整资源分配决策。这种适应性使资源分配过程能够动态响应实时变化，从而实现更高效和适应性的视频流服务。

## 5 应用

### 5.1 生成

图 5：视频生成应用。

视频合成。生成式 AI 模型可以用于合成新的视频内容，从而在无需人工干预的情况下创建逼真的场景和特效。由于 GAN 的固有训练不稳定性，基于 GAN 的跨模态视频合成的探索相对较少。TGAN [[55](https://arxiv.org/html/2404.16038v1#bib.bib55)]，作为早期的尝试，通过先生成一个潜在表示，然后使用图像生成器将其解码为像素，利用 GAN 进行视频生成。NUWA [[56](https://arxiv.org/html/2404.16038v1#bib.bib56)]，一个基于变换器的模型，提出了一个统一的跨模态生成模型，能够适应各种生成场景，如文本到视频、草图到视频、视频预测等。CogVideo [[17](https://arxiv.org/html/2404.16038v1#bib.bib17)] 通过实现多帧率层次化训练策略来扩展文本到图像模型 CogView [[57](https://arxiv.org/html/2404.16038v1#bib.bib57)]，以更好地对齐文本和视频片段。最近的基于扩散的模型，如 Imagen-Video [[13](https://arxiv.org/html/2404.16038v1#bib.bib13)] 和 Make-a-Video [[14](https://arxiv.org/html/2404.16038v1#bib.bib14)]，将视频生成的边界推向了一个新的水平。然而，这些扩散模型由于参数数量庞大和复杂的级联网络，极大地限制了社区进一步开发的能力。与其他方法相比，Video LDM [[18](https://arxiv.org/html/2404.16038v1#bib.bib18)] 展现了高效性和表现力。它通过使用来自 WebVid 数据集 [[58](https://arxiv.org/html/2404.16038v1#bib.bib58)] 的 1070 万视频-字幕对的数据集，微调了公开可用的 Stable Diffusion (SD) 图像 LDM 模型来实现这一点。Text2Video-Zero [[59](https://arxiv.org/html/2404.16038v1#bib.bib59)] 进一步提出了一种不依赖于视频数据的方法。相反，它采用预定义的全局转换参数来扭曲潜在代码，并利用与起始帧的跨注意力来获得一致且去噪的帧。Video LDM 和 Text2Video-Zero 也具有个性化视频生成的能力。用户可以通过 Dreambooth [[60](https://arxiv.org/html/2404.16038v1#bib.bib60)] 等方法自定义视频中的概念。

针对特定领域的视频合成任务也有相关研究，例如基于音频的视频生成和人类舞蹈视频生成[[20](https://arxiv.org/html/2404.16038v1#bib.bib20)]。SadTalker[[16](https://arxiv.org/html/2404.16038v1#bib.bib16)] 利用条件 VAE 合成头部动作，实现风格化的音频驱动面部动画。DreamTalk[[19](https://arxiv.org/html/2404.16038v1#bib.bib19)] 采用扩散模型根据提供的源音频或视频生成高度多样的谈话头部。对于人类舞蹈视频生成，基于 GAN 的姿势引导视频生成模型 EDN[[15](https://arxiv.org/html/2404.16038v1#bib.bib15)]，在从特定人类舞蹈视频中提取的图像-姿势对上进行了微调。它能够根据任何开放集姿势图像生成一个人的图像。然而，EDN 在没有广泛预训练的情况下，高效准确地重建人类属性细节面临挑战。Discro[[21](https://arxiv.org/html/2404.16038v1#bib.bib21)]通过利用当前最先进的预训练扩散模型和结构化条件技术解决了这个问题。为了在推理过程中增强属性细节，它采用了 Grounded-SAM[[61](https://arxiv.org/html/2404.16038v1#bib.bib61)]进行前景提取，并在一个广泛的人类属性数据集上预训练模型，从而在舞蹈合成中实现了更好的组合效果。

另一项研究线索集中在通过整合当前的大型语言模型（LLMs）来提高文本引导视频生成的流畅性。为了更好地将视觉分词与 LLMs 的学习过程对齐，提出了 MAGVIT-v2[[62](https://arxiv.org/html/2404.16038v1#bib.bib62)]，作为一种简洁且富有表现力的视频分词器。这使得 LLMs 在视频生成性能上相较于基于扩散的模型有所提升。VideoPoet[[63](https://arxiv.org/html/2404.16038v1#bib.bib63)]，作为一种多功能视频生成模型，利用包括 MAGVIT-v2 在内的各种模态输入分词器来促进视频分词。它能够处理各种视频生成场景，实现视频与文本、音频等其他模态之间的无缝转换。

视频编辑允许用户自定义特定视频的编辑。这些应用程序不限于有限合成模型的能力，使得模型可以专注于编辑特定场景，从而提高时间一致性。例如，DiffVideoAE [[64](https://arxiv.org/html/2404.16038v1#bib.bib64)] 通过修改面部属性或利用 CLIP 信号，实现了对基于面部的语音视频的细粒度编辑。Tune-a-Video [[65](https://arxiv.org/html/2404.16038v1#bib.bib65)] 扩展了图像扩散模型，仅对给定视频进行微调，从而实现基于文本的编辑。另一方面，Pix2Video [[66](https://arxiv.org/html/2404.16038v1#bib.bib66)] 通过将前一帧的自注意力特征注入到当前帧中，实现了无训练和一致的文本编辑视频，隐式地聚合了时间信息。分层神经表示 [[67](https://arxiv.org/html/2404.16038v1#bib.bib67), [68](https://arxiv.org/html/2404.16038v1#bib.bib68)] 是另一种有前途的视频编辑方法，旨在将视频分解成不同的层。Text2Live [[69](https://arxiv.org/html/2404.16038v1#bib.bib69)] 结合了这种表示和文本指导，展示了令人信服的视频编辑结果。

随着生成 AI 技术的不断进步，众多视频生成平台应运而生。一个显著的例子是著名的 Pika 平台²²2https://github.com/pika/pika，它作为一个创意到视频的平台，利用 AI 无缝地创建和编辑视频。

视频预测是指基于观察到的过去帧来预测视频序列中的未来帧。视频预测任务具有广泛的社会影响，包括提升娱乐性、改善安全性、帮助理解人类行为以及推动自动化系统的发展。例如，它可以部署到自动化系统中，以更有效地规划和导航环境。早期的基于递归的方法，如 FRNN [[70](https://arxiv.org/html/2404.16038v1#bib.bib70)]，通过递归地输入先前的预测来生成后续帧。为了应对 RNN 结果模糊的问题，Hier-vRNN [[71](https://arxiv.org/html/2404.16038v1#bib.bib71)] 使用潜在变量的层次结构来增加潜在分布的表达能力。最近，条件扩散模型在视频预测中也展现了令人印象深刻的结果。通过对先前帧进行条件处理，RaMViD [[72](https://arxiv.org/html/2404.16038v1#bib.bib72)] 引入了随机条件遮罩，使扩散模型能够同时执行预测、填充和预测任务。MVCD [[73](https://arxiv.org/html/2404.16038v1#bib.bib73)] 还发现，在训练中随机且独立地处理所有过去帧或所有未来帧，往往能生成高质量的预测帧。另一方面，FDM [[74](https://arxiv.org/html/2404.16038v1#bib.bib74)] 发现，对先前帧进行选择性稀疏和长距离条件处理对于生成长视频是有效的。

### 5.2 视频场景理解

人类动作和行为识别是视频场景理解中的核心任务之一，旨在估计在线视频中的人类动作和行为 [[75](https://arxiv.org/html/2404.16038v1#bib.bib75), [76](https://arxiv.org/html/2404.16038v1#bib.bib76), [77](https://arxiv.org/html/2404.16038v1#bib.bib77)]。在这个背景下，需要分析考虑到人体尺寸、姿势、视角、光照条件和相机运动等多样性因素的动作和行为。对于这项任务，主要挑战是如何利用预训练的 LLMs 从视频序列中学习到强大的动作表示 [[78](https://arxiv.org/html/2404.16038v1#bib.bib78)]。LLMs 最近被应用于各种人类动作和识别任务。图 [6](https://arxiv.org/html/2404.16038v1#S5.F6 "图 6 ‣ 5.2 视频场景理解 ‣ 5 应用 ‣ 关于生成 AI 和 LLM 在视频生成、理解和流媒体中的调查") 展示了 LLM 引导的动作识别的示例。例如，Kaneko 等人 [[79](https://arxiv.org/html/2404.16038v1#bib.bib79)] 提出了一个方法，通过设计文本提示来获取用于人类活动的新特征。Zhou 等人 [[80](https://arxiv.org/html/2404.16038v1#bib.bib80)] 提出了一个方法，将来自物联网（IoT）传感器的信号（如相机视频、Lidar 和 mmWave）与 LLMs 连接，以实现人类动作识别的目标。通过对齐视觉和语言表示空间，可以直接将视觉特征与语言特征进行映射。因此，学习到的模型具备了零-shot 学习的能力，通过模仿人类识别物体的方式来识别未见过的物体。Wu 等人 [[81](https://arxiv.org/html/2404.16038v1#bib.bib81)] 介绍了一个视频-文本识别框架，使用视觉-语言模型（VLMs）如 CLIP [[25](https://arxiv.org/html/2404.16038v1#bib.bib25)] 的自然语言来连接视频领域进行跨模态知识提取。

图 6: VLMs 在人类动作识别中的示例。输入示例取自 kinetics 人类动作视频数据集 [[82](https://arxiv.org/html/2404.16038v1#bib.bib82)]。

表 3: 视频场景理解的代表性方法。

| 方法 | 输入模态 | 亮点 |
| --- | --- | --- |
| 人类动作和行为识别 |  |
| Kaneko 等人 [[79](https://arxiv.org/html/2404.16038v1#bib.bib79)] | 文本, 视频 | 设计文本提示以获取新的特征。 |
| Zhou 等人 [[80](https://arxiv.org/html/2404.16038v1#bib.bib80)] | 文本, 视频, Lidar, mmWave | 对齐视觉和语言表示空间以进行人类动作识别。 |
| Kaneko 等人 [[79](https://arxiv.org/html/2404.16038v1#bib.bib79)] | 视频, 文本 | 使用 VLMs 连接视频领域进行跨模态知识提取。 |
| 基于视频的对话和对话 |  |
| Video-ChatGPT [[8](https://arxiv.org/html/2404.16038v1#bib.bib8)] | 文本、视频 | 捕捉视频帧之间的时空关系。 |
| VideoChat [[83](https://arxiv.org/html/2404.16038v1#bib.bib83)] | 文本、视频 | 基于视频基础模型和 LLM 的视频中心对话系统。 |
| Liu et al. [[41](https://arxiv.org/html/2404.16038v1#bib.bib41)] | 文本、视频 | 视频对话任务的时间建模。 |
| 人机/机器交互 |  |
| PaLM-E [[84](https://arxiv.org/html/2404.16038v1#bib.bib84)] | 文本、图像、视频 | 一个大型具身多模态模型，用于处理各种具身推理任务。 |
| LM-Nav [[85](https://arxiv.org/html/2404.16038v1#bib.bib85)] | 文本、视频 | 基于视频输入的无缝人机对话系统。 |

借助 LLMs 或 VLMs 的指导，人类动作和物体识别方法已广泛应用于视频监控[[86](https://arxiv.org/html/2404.16038v1#bib.bib86)]、机器人导航[[78](https://arxiv.org/html/2404.16038v1#bib.bib78)、[87](https://arxiv.org/html/2404.16038v1#bib.bib87)、[88](https://arxiv.org/html/2404.16038v1#bib.bib88)]、医学诊断和医疗保健[[89](https://arxiv.org/html/2404.16038v1#bib.bib89)]、体育[[86](https://arxiv.org/html/2404.16038v1#bib.bib86)]。例如，配备视觉传感器的 LLMs 使机器人能够根据视频序列具备更强的 NLP 能力。这通过模仿人类的推理和对话，促进了人机之间更紧密的集成。在体育方面，LLMs 的零样本识别能力和语义丰富性被用于指导多种体育活动的动作识别模型，如足球和篮球。

总结而言，LLMs 与视频融合用于人类动作和物体识别，预示着视频场景理解的一个激动人心的新时代。随着积极的研究进展，这一领域对更广泛的视频应用享有极大的利益。

基于视频的对话和交流。LLMs 能够提供语义信息并生成符号空间信号，这些信号可以作为视频场景理解的指导。最近，这一点在互动视频对话和交流中得到了展示[[42](https://arxiv.org/html/2404.16038v1#bib.bib42), [41](https://arxiv.org/html/2404.16038v1#bib.bib41), [83](https://arxiv.org/html/2404.16038v1#bib.bib83), [90](https://arxiv.org/html/2404.16038v1#bib.bib90), [46](https://arxiv.org/html/2404.16038v1#bib.bib46), [91](https://arxiv.org/html/2404.16038v1#bib.bib91)]。在这个背景下，Video-ChatGPT [[42](https://arxiv.org/html/2404.16038v1#bib.bib42)] 旨在通过基于 LLMs 捕捉视频帧之间的时空关系来实现视频理解和对话。它在各种基准数据集上展示了强大的对话和上下文理解能力。另一方面，VideoChat [[83](https://arxiv.org/html/2404.16038v1#bib.bib83)] 引入了一个以视频为中心的多模态对话系统，整合了视频基础模型和 LLMs。此外，Liu 等人[[41](https://arxiv.org/html/2404.16038v1#bib.bib41)] 将 LLMs 扩展到视频领域，并结合了一个时空模块，用于视频对话任务的时间建模，如图[7](https://arxiv.org/html/2404.16038v1#S5.F7 "Figure 7 ‣ 5.2 Video Scene Understanding ‣ 5 Applications ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")所示。

总结来说，视频基础的对话和交流的最新进展主要通过将视频/图像基础模型与 LLMs 集成来展示。通过 LLMs，可以通过探索视频中心对话建模的时间关系实现零样本对话。

![参考标题](img/2ae8d7ba6b9803a6e5d08f698abadcd3.png)

图 7：基于 LLMs 的视频对话的代表性流程[[41](https://arxiv.org/html/2404.16038v1#bib.bib41)]。

人机/机器互动。随着 LLMs 的普及，许多研究工作致力于 LLMs 在人机/机器互动领域的应用，如图[8](https://arxiv.org/html/2404.16038v1#S5.F8 "Figure 8 ‣ 5.2 Video Scene Understanding ‣ 5 Applications ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")所示的视觉说明。一方面，借助预训练的 LLMs，机器人被赋予理解人类需求和查询的能力[[84](https://arxiv.org/html/2404.16038v1#bib.bib84)]。另一方面，LLMs 使机器人能够通过与 LLMs 的互动来表达流畅和类人的自然语言[[85](https://arxiv.org/html/2404.16038v1#bib.bib85)]。然而，将 LLMs 应用于人机/机器互动需要处理 LLMs 提供的不准确推理。为此，开发了基于摄像头视频输入的机器人对话系统，以实现与人类的更无缝互动。

作为一个新兴领域，这一方向展示了巨大的潜力，并为机器人导航和人机互动提供了新的范式。LLMs 帮助提升学习效率和表现，同时增强了人类与机器人的互动。

![参见说明](img/3d8bccdf1636d7e921a6f55fbeb8a26c.png)

图 8：基于大语言模型（LLM）进行地标提取、基于视觉-语言模型（VLM）进行基础定位、以及基于视觉导航模型（VNM）进行执行的导航指令示例 [[85](https://arxiv.org/html/2404.16038v1#bib.bib85)]。

### 5.3 流媒体

尽管 LLMs 在视频流媒体中的应用仍处于起步阶段，但在用户观看角度预测、网络状况预测、视频内容编码和处理等领域的潜在应用表明了显著的发展机会。持续的研究和创新有望推动 LLMs 在视频流媒体中的应用，*最终*为用户提供更智能和个性化的观看体验。在这种背景下，我们*深入探讨*了基于变换器的 LLMs 在视频流媒体领域中的几个经典应用。

360°和体积视频流。360°通常是一个球形视频，将一组摄像机或镜头同时拍摄的不同角度的视频拼接在一起。一旦视频合并成一个，摄像机或视频编辑软件会在颜色和对比度上同步不同的镜头。为了使用标准编解码器（如 H.264 [[92](https://arxiv.org/html/2404.16038v1#bib.bib92)] 和 HEVC [[93](https://arxiv.org/html/2404.16038v1#bib.bib93)]）压缩 360°视频，视频会被投影到二维领域。由于其全景特性，360°视频在相同感知质量下比传统视频大得多（4$\times$至 6$\times$）。最终的 360°视频在单眼 8K 分辨率下需要达到多个千兆比特每秒（Gbps）的带宽，对网络提出了极大挑战，并对成本造成巨大负担[[94](https://arxiv.org/html/2404.16038v1#bib.bib94), [95](https://arxiv.org/html/2404.16038v1#bib.bib95)]。主流行业认为，运动到光子延迟（MTP）不应超过 20 毫秒³³3Huawei-iLab. 2018\. 云 VR 网络解决方案白皮书。取自 http://www.huawei.com/，否则会导致用户眩晕。

体积视频（或全息视频），作为在 VR/AR/MR 中表示自然内容的介质，可能是视频技术的下一代，并且是 5G 及未来无线通信的典型应用案例[[96](https://arxiv.org/html/2404.16038v1#bib.bib96), [97](https://arxiv.org/html/2404.16038v1#bib.bib97)]。体积视频为用户提供了六自由度（6DoF）的沉浸式观看体验，即用户可以自由前进/后退（冲击）、上下移动（升降）或左右摆动（摆动），以选择他们喜欢的 3D 场景视角，从而享受比 3DoF VR 视频用户多出三个自由度的体验。作为最受欢迎的体积媒体表示形式，点云由 3D 点组成，每个点具有多个属性，例如坐标和颜色。

对于 360°和体积视频，每次用户感知 360°场景的一部分，即视场（FoV）。随着用户旋转头部，相应的 360°场景的不同 FoV 会被渲染出来供观察。通过允许用户自由选择视频球体内的任何观看角度，360°和体积视频将沉浸式观看体验提升到一个新水平，相较于传统视频和多视角视频。

与传统视频流相比，360°和体积视频的技术挑战包括：

+   •

    视口预测：每个用户每次只观察 360°场景的一部分，并且在视频播放过程中可能会切换视场角（FoV）。此外，解决不可避免的错误视点预测对于保证视频服务的质量也很重要。

+   •

    严格的延迟要求：MTP 需要低于 20 毫秒。

+   •

    基于瓦片的资源分配：360°和体积视频流的资源分配是在瓦片级别进行的，需要考虑质量切换。

由第[4.3 节](https://arxiv.org/html/2404.16038v1#S4.SS3 "4.3 LLM for Video Streaming ‣ 4 Technologies ‣ A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming")提到的、由 LLM 支持的技术，包括视口预测、带宽预测、压缩和资源分配，可以共同优化 360°和体积视频的挑战性流媒体任务。

短视频推荐。近年来，短视频越来越受到欢迎，像 TikTok 和 YouTube Shorts 这样的平台为用户提供了创建和分享内容的平台。这些视频通常长度从几秒到一分钟不等，涵盖了广泛的主题。短视频的兴起彻底改变了我们消费和创建内容的方式，使任何人都可以更轻松地与世界分享他们的想法和创意。

从技术角度来看，这些视频的传输与常规视频的传输相差甚远[[98](https://arxiv.org/html/2404.16038v1#bib.bib98)]。通常，服务器会向用户推荐一组视频（例如 5 个），所有这些视频都会推送给用户。用户然后选择观看哪些视频，并丢弃他们不喜欢的视频，导致传输资源浪费。然而，如果不是所有视频都被传输，用户可能会遇到缓冲或视频质量下降，这会显著影响他们的观看体验。这个问题涉及如何向用户推荐视频，是否传输所有视频或部分视频，以及如何分配视频资源等挑战。此外，缺乏可用于研究的视频库是一个重大障碍。准确的推荐对于最小化带宽浪费至关重要。结合 LLM 的视频推荐系统可以更好地理解用户的偏好和上下文，从而提供更准确和个性化的视频推荐。LLM 可以分析用户查询、视频描述和其他与视频相关的文本信息，以把握语义含义、情感和其他影响用户偏好的重要因素。这种方法有潜力显著提升用户的满意度、参与度和留存率。随着这些语言模型不断从大量文本数据中学习，它们在理解用户意图和偏好方面变得越来越熟练，从而提供更相关和吸引人的视频推荐。最终，这种视频推荐的改进可以带来更愉快和沉浸的用户体验，惠及用户和视频内容提供商。

图 9：LLM 在视频流媒体中的应用示意图。

视频服务增强。基于 Transformer 的 LLMs 可以应用于图像超分辨率，通过预测和生成更高分辨率的图像来提升视频质量，或从有损压缩的视频中去除伪影，并通过对视频内容进行照片级真实恢复来改善视觉属性。例如，Liu 等人介绍了一种开创性的轨迹感知 Transformer [[99](https://arxiv.org/html/2404.16038v1#bib.bib99)]，这是将 Transformer 架构整合到视频超分辨率任务中的初步尝试之一。该模型展示了优异的性能。Geng 等人提出了一种统一的时空 Transformer，结合了时间插值和空间超分辨率模块用于时空视频超分辨率 [[100](https://arxiv.org/html/2404.16038v1#bib.bib100)]。这种创新方法使得网络规模显著小于现有方法，实现了实时推断而性能几乎没有妥协。[[101](https://arxiv.org/html/2404.16038v1#bib.bib101)] 提出了一个具有低延迟的实时在线视频增强 Transformer，利用空间和时间注意力机制。该模型在定量和定性上均有显著进步，推断时间最小化。

基于 LLMs 和生成 AI 的视频服务增强近期也有显著进展。[[102](https://arxiv.org/html/2404.16038v1#bib.bib102)] 提出了在《英雄联盟》比赛中自动生成流媒体评论的创新方法。该系统熟练地识别关键事件，并利用生成 AI 服务生成语音输出。此外，[[103](https://arxiv.org/html/2404.16038v1#bib.bib103)] 介绍了一个综合性的基于 transformer 的视频字幕生成模型，这在流媒体服务中具有重要作用。作者提出了稀疏注意力掩码作为一种正则化技术，以改善长序列视频建模。他们还提供了定量验证，确认了可学习的稀疏注意力掩码在字幕生成领域的有效性。

表 4：视频流的 LLM 方法综述。

| 方法 | 输入信息 | 任务 |
| --- | --- | --- |
| 视口预测 |  |
| 基于 transformer 的方法 [[50](https://arxiv.org/html/2404.16038v1#bib.bib50)] | 过去的视角扫描路径 | 长期视角预测 |
|  |  | 复杂度低。 |
| 基于 transformer 的方法 [[51](https://arxiv.org/html/2404.16038v1#bib.bib51)] | 面部图像 | 眼动信息。 |
| 时空 transformer [[52](https://arxiv.org/html/2404.16038v1#bib.bib52)] | 注视特征、场景上下文以及 | 预测视频中的人物–物体交互 |
|  | 人物–物体对的视觉特征。 |  |
| 带宽预测 |  |
| 基于 transformer 的模型 [[47](https://arxiv.org/html/2404.16038v1#bib.bib47)] | 之前的带宽信息。 | 未来的带宽条件。 |
| 基于 GAN 的解决方案 [[104](https://arxiv.org/html/2404.16038v1#bib.bib104)] | 实际视频痕迹 | 合成视频流数据，具有 |
|  |  | 关注 360°/普通视频分类。 |
| 视频压缩 |  |
| 遮罩图像建模变换器 [[53](https://arxiv.org/html/2404.16038v1#bib.bib53)] | 视频 | 深度视频压缩。 |
| 基于变换器的方法 [[54](https://arxiv.org/html/2404.16038v1#bib.bib54)] | 视频 | 神经视频压缩。 |
| 视频增强 |  |
| 视频增强变换器 [[101](https://arxiv.org/html/2404.16038v1#bib.bib101)] | 原始视频 | 提升质量的视频。 |
| 基于变换器的方法 [[99](https://arxiv.org/html/2404.16038v1#bib.bib99)] | 视频 | 视频超分辨率。 |
| 统一的时空变换器 [[100](https://arxiv.org/html/2404.16038v1#bib.bib100)] | 视频 | 时空视频超分辨率。 |
| GAN 模型 [[105](https://arxiv.org/html/2404.16038v1#bib.bib105)] | 视频 | 实时超分辨率。 |
| 基于变换器的模型 [[103](https://arxiv.org/html/2404.16038v1#bib.bib103)] | 待观看视频 | 视频字幕生成。 |

## 6 挑战

在这一部分，我们讨论了生成式 AI 和 LLM 在视频生成、理解和流媒体服务中面临的主要挑战。

### 6.1 生成

时间一致性。生成式 AI 在视频内容创作中的主要挑战之一是确保生成帧之间的时间一致性。生成的视频序列应展现平滑和现实的运动模式，保持这些模式在帧之间是一项具有挑战性的任务。此外，视频量以及训练策略的选择在一致性方面也起着关键作用。将视频生成建模为离散图像生成任务容易导致时间一致性差，并且会受到时间闪烁 [[106](https://arxiv.org/html/2404.16038v1#bib.bib106), [107](https://arxiv.org/html/2404.16038v1#bib.bib107)]。通过将时间轴视为连续信号的隐式神经表示（INRs）方法 [[108](https://arxiv.org/html/2404.16038v1#bib.bib108)] 可以轻松生成任意长的视频。TGANv2 [[109](https://arxiv.org/html/2404.16038v1#bib.bib109)] 通过引入层次判别器来保证从粗到细的平滑度来解决这个问题。最近的图像预训练模型 [[18](https://arxiv.org/html/2404.16038v1#bib.bib18)] 发现，插入多个时间注意力层并在视频数据集上完全微调也是一种有效的方法。

高计算需求。视频生成需要处理高维数据，这大大增加了训练和推理的计算需求。开发高效的算法和并行化技术仍然是一个持续的挑战。像 NUWA [[56](https://arxiv.org/html/2404.16038v1#bib.bib56)] 和 Imagen-Video [[13](https://arxiv.org/html/2404.16038v1#bib.bib13)] 这样的工作属于文本-视频生成器类别，它们在数百万个文本-视频对上进行训练，使得大多数研究小组很难复制。然而，某些基于编辑的视频生成方法通过利用少量的视频数据集甚至完全不使用视频数据集来解决计算负担，以实现特定任务。Tune-a-Video [[65](https://arxiv.org/html/2404.16038v1#bib.bib65)] 是这种方法的一个例子，其中通过利用图像生成器来完成针对性的编辑任务来微调视频。这些特定任务驱动的视频，由于其受限的样本空间和对模型时间建模能力的较低要求，构成了一个可以广泛探索的方向。

大规模视频数据集的缺乏。虽然大规模图像数据集广泛可用，但类似规模和多样性的视频数据集却很稀缺。大规模视频数据集的缺乏阻碍了视频内容生成的生成 AI 模型的发展，因为这些模型依赖大量数据来学习潜在的数据分布。标注视频数据集相对稀缺，但它们在可控视频生成中发挥了至关重要的作用。由于视频内容的高度冗余性，一些近期研究 [[13](https://arxiv.org/html/2404.16038v1#bib.bib13), [110](https://arxiv.org/html/2404.16038v1#bib.bib110), [18](https://arxiv.org/html/2404.16038v1#bib.bib18)] 利用强大的预训练文本-图像生成器来初始化空间建模网络层，从而提高了单帧生成的质量。这使得时间模块能够更多地集中于建模序列信号的动态。此外，某些方法 [[12](https://arxiv.org/html/2404.16038v1#bib.bib12), [73](https://arxiv.org/html/2404.16038v1#bib.bib73)] 通过采用图像-视频联合训练技术解决了数据稀缺的问题，这些技术在时间一致性和帧保真度之间表现出一种权衡。

### 6.2 理解

时间推理。视频场景理解涉及对时间信息进行推理，包括视频中的动态、动作和互动。然而，LLMs（大型语言模型）通常难以有效捕捉和建模长时间跨度的时间依赖关系。视频中的时间推理具有挑战性，因为视频的长度各不相同，并且需要随着时间的推移识别和情境化动作。开发能够有效推理长期依赖关系、捕捉时间背景和理解视频场景动态的 LLM 架构是一个重要的研究挑战。需要探索如时间卷积、递归神经网络或注意力机制等技术，以提高 LLMs 的时间推理能力。

多模态理解。视频由视觉和听觉信息组成，全面理解视频需要多模态理解[[33](https://arxiv.org/html/2404.16038v1#bib.bib33)]。LLMs 需要有效整合视觉和听觉模态，以捕捉视频场景的完整背景和意义。然而，视觉和听觉信息的对齐和连接是一项复杂的任务。因此，必须探索网络架构和方法，以有效建模音视频交互、捕捉跨模态依赖关系并在 LLMs 中融合多模态信息[[111](https://arxiv.org/html/2404.16038v1#bib.bib111)]。此外，开发针对大规模多模态视频数据集的 LLMs 训练方法，这些数据集覆盖广泛的场景和语言，对于提升 LLMs 的多模态理解能力至关重要。

实时视频处理。与 LLMs 一起实时处理视频是一个重大挑战。实时视频场景理解对于诸如自动驾驶汽车、监控系统和视频分析等各种应用至关重要[[112](https://arxiv.org/html/2404.16038v1#bib.bib112)]。然而，LLMs 的大模型尺寸和计算需求妨碍了其实时处理能力。因此，需要进一步研究开发高效的网络、模型压缩方法和硬件优化，以加速 LLMs 在视频场景理解中的推理能力。可以探索如知识蒸馏[[113](https://arxiv.org/html/2404.16038v1#bib.bib113)、[114](https://arxiv.org/html/2404.16038v1#bib.bib114)]、剪枝和量化等技术，以减少计算负担，实现 LLMs 的实时视频处理。此外，探索分布式计算和硬件加速器可以进一步增强 LLMs 在视频场景理解中的实时能力[[111](https://arxiv.org/html/2404.16038v1#bib.bib111)]。

零样本性能的限制。尽管 LLMs 在零样本学习能力上表现出色，但要使 LLM 引导的视频场景理解模型具备相同的能力几乎是不可能的。与视频生成类似，主要挑战在于缺乏大规模的配对视频-文本数据集，因为生成视频剪辑的丰富文本描述困难重重。因此，很难为目标任务学习到强大的表示。另一个原因是，对于长篇视频，文本注释要么稀疏，要么不足以说明发生的事件或活动。因此，未来的研究可能会探讨如何利用 LLMs 在有限或稀疏的文本描述下施加更有效的监督。另一个方向是如何利用 LLMs 进一步生成具有更多语义丰富度的高质量视频-文本对。

### 6.3 流媒体

多变的环境和需求。用户观看视频的设备在计算能力、分辨率和网络条件上存在显著差异。此外，视频传输方式（如实时流媒体和点播）和视频类型（如 VR 视频和短视频）的多样性对带宽、实验和计算要求提出了不同的要求。设计或学习一种适应这些异质场景的算法是一项艰巨的任务。LLMs 有能力涵盖这些情况并提供解决方案。然而，在使用 LLMs 进行视频传输调度时，如何在短时间内有效解决这些挑战并提供答案（考虑到视频对算法复杂性的强需求）是一个不容小觑的重大挑战，未来需要进一步研究。

统一框架或标准。传统的视频传输方法已达到较高的成熟度，催生了 YouTube 和 Zoom 等广泛使用的应用。在这一领域，一个重要的推动因素是 MPEG-DASH 视频传输标准的引入[[115](https://arxiv.org/html/2404.16038v1#bib.bib115)]，它为视频传输策略奠定了基础。公司和研究团队因此能够在这一框架下创新并建立新的应用。然而，在 LLM 基础的视频传输上下文中，目前尚无统一的视频传输框架或标准。技术方法的分歧阻碍了这一领域的发展。建立统一的视频传输框架或标准是一项具有挑战性的任务，需要众多实体的参与。

大规模视频数据集的缺乏。与前述生成和理解讨论类似，在利用大语言模型进行传输领域的优化和调度时，学习是必要的。这自然导致对数据集的需求。目前，公开的数据集主要涵盖网络带宽 [[116](https://arxiv.org/html/2404.16038v1#bib.bib116)]、视频数据以及虚拟现实视频的用户数据，如 MPEG 提供的那些 ⁴⁴4https://www.mpeg.org/standards/。然而，与大语言模型学习的需求相比，这些数据集相对较小，而大公司拥有的数据集并不开放。此外，标注通信状态、用户设备、用户观看数据、用户满意度等的综合数据集目前仍然缺乏。生成式 AI 可能有助于生成用于带宽预测模型训练的数据集。[[104](https://arxiv.org/html/2404.16038v1#bib.bib104)] 引入了一种创新的 GAN 解决方案，用于合成视频流数据，重点在于 360°/普通视频分类。与仅依赖实际踪迹相比，这种方法在准确性上有所提高。

## 7 个担忧

除了吸引人的潜力，生成式 AI 和大语言模型也引发了诸多需要妥善解决的担忧。显著的担忧包括通过视频伪造传播误导性信息和知识产权侵犯等。

![参见说明](img/126dea612a1ec9251abdce38a8347ff7.png)

图 10：生成式 AI 和大语言模型解决方案面临的担忧。

误信息。生成式 AI 提高了生成看似真实的视频镜头的能力，这可能被滥用来创建虚假叙事、传播假新闻、在未获授权的情况下冒充个人或操控公众舆论，对政治、安全和可信度等方面造成严重影响。相关事件的增加引发了社会的广泛关注 ⁵⁵5https://www.nbcnews.com/tech/tech-news/deepfake-scams-arrived-fake-videos-spread-facebook-tiktok-youtube-rcna101415。

知识产权侵权。生成式 AI 不断改进以编辑和修订现有视频的风格和细节，侵犯了版权并未经授权使用专有内容。

安全性。生成式 AI 可以制作深度伪造视频，以模拟来自受信任来源或个人的合法视频，从而促进诈骗和网络犯罪。近年来，相关案例也有所增加 ⁶⁶6https://www.bbc.com/news/technology-66993651。

隐私泄露。如果 LLM 被用于无处不在的监控系统中，不仅可以识别个人，还可以推测他们的活动和日常习惯。这可能导致严重的隐私问题，使人们不断受到监控，侵犯隐私权。此外，当与配备音频接收器的监视器一起使用时，LLM 可能会窃听私人对话。

内容审查。虽然 LLM 驱动的流媒体服务有可能改善用户体验，但也可能导致内容的过度过滤，可能等同于审查。没有明确指南的情况下确定哪些内容能到达观众，可能会导致任意的内容压制。

偏见。现有的偏见问题，如刻板印象，可能在生成式 AI 和 LLM 的使用下加剧。个性化的流媒体推荐可能会加强现有的偏见，将用户与多样化的观点隔离。这种风险同样适用于视频的生成阶段。

成瘾性内容设计。生成式 AI 可以用来生成某些类型的视频，以优化最大参与度，这可能导致利用人类心理学增加屏幕时间的成瘾性内容。

总体而言，将生成式 AI 和 LLM 整合到视频行业中引发了诸多关注，包括隐私、伦理和社会影响等。在视频生成中，创建超现实的深度伪造技术带来了虚假信息、隐私侵犯和知识产权侵权的重大风险。LLM 对视频的理解能力不断提高，引发了关于隐私入侵的警报，例如为个性化分析和行为预测而挖掘敏感数据，这可能被利用于针对性的操控。在流媒体中，不透明的推荐系统可能形成内容泡沫，并可能扭曲文化叙事。此外，内容的个性化也引发了关于数据隐私、成瘾性内容设计的心理影响以及资源分配公平性的伦理问题。

为了解决这些问题，需要采取积极而谨慎的措施。监管机构应制定强有力的隐私保护和透明度要求，迫使视频服务披露用户数据如何影响内容交付。应建立伦理 AI 框架，以指导视频服务算法的创建和使用，避免偏见，确保内容的多样性和公平性。视频平台必须通过实施数据处理最佳实践并提供关于用户数据的清晰选择，来优先考虑用户同意和数据安全。此外，还需要全行业致力于伦理内容设计，避免操控行为，并促进心理健康。最后，视频服务必须通过适应性 AI 系统确保遵守国际法规，满足地方标准，同时尊重全球规范。通过这些共同努力，行业可以在保护个人权利和社会价值的同时，利用 AI 技术为视频服务带来好处。

## 8 结论

在这篇论文中，我们全面审视了生成式人工智能（Generative AI）和大语言模型（LLMs）如何革新视频技术领域，重点关注视频生成、理解和流媒体。这些技术的创新整合带来了高度逼真的数字创作，通过提取视觉内容中的有意义信息增强了视频理解，并提供了更高效、个性化的流媒体体验，从而改善了用户与视频的互动和根据用户偏好提供体验。

论文探讨了在视频相关任务中应用生成式 AI 和大语言模型（LLMs）的当前成就、持续挑战和未来可能性。它强调了这些技术在推动多媒体、网络和 AI 社区视频技术方面的巨大潜力，同时也突出需要进一步探索的挑战和问题。

从已审阅的工作中观察，我们可以看到，总体而言，像 GAI 和 LLMs 这样的先进 AI 技术正在对视频相关研究领域的几个关键部门产生深远影响。基于 AI 的方法最大优势在于其自动化能力和较低的人工成本。然而，这也带来了 AI 独特的挑战，如缺乏大规模数据集、高计算成本、一致性问题以及虚假信息和安全等问题。因此，学术界和工业界在快速发展的过程中应保持谨慎，以确保市场的可持续性。

## 参考文献

+   [1] R. Bhagwatkar *等*，“视频生成方法综述”，发表于 *2020 国际电力、仪器、控制与计算会议（PICC）*。IEEE，2020，第 1–5 页。

+   [2] A. Singh，“AI 文本到图像和 AI 文本到视频生成器的综述，” 在 *2023 年第 4 届国际人工智能、机器人与控制会议（AIRC）*。 IEEE，2023 年，第 32–36 页。

+   [3] C. Liu *等*，“AI 驱动的说服性视频生成：综述，” *ACM 计算机调查*，第 55 卷，第 13 期，1–31 页，2023 年。

+   [4] N. Aldausari *等*，“视频生成对抗网络：综述，” *ACM 计算机调查（CSUR）*，第 55 卷，第 2 期，第 1–25 页，2022 年。

+   [5] G. Rafiq *等*，“视频描述：深度学习方法的全面综述，” *人工智能评论*，第 1–80 页，2023 年。

+   [6] A. Singh *等*，“关于视频描述的最新方法和挑战的全面综述，” *arXiv 预印本 arXiv:2011.14752*，2020 年。

+   [7] N. Aafaq *等*，“视频描述：方法、数据集和评估指标的综述，” *ACM 计算机调查（CSUR）*，第 52 卷，第 6 期，第 1–37 页，2019 年。

+   [8] S. K. Muhammad Maaz, Hanoona Rasheed *等*，“Video-chatgpt：通过大型视觉和语言模型实现详细的视频理解，” *ArXiv 2306.05424*，2023 年。

+   [9] C. Vondrick *等*，“生成具有场景动态的视频，” *神经信息处理系统进展*，第 29 卷，2016 年。

+   [10] E. Denton *等*，“具有学习先验的随机视频生成，” 在 *国际机器学习会议*。 PMLR，2018 年，第 1174–1183 页。

+   [11] N. Kalchbrenner *等*，“视频像素网络，” 在 *国际机器学习大会*。 PMLR，2017 年，第 1771–1779 页。

+   [12] J. Ho *等*，“视频扩散模型，” *arXiv 预印本 arXiv:2204.03458*，2022 年。

+   [13] ——，“Imagen 视频：使用扩散模型生成高清晰度视频，” *arXiv 预印本 arXiv:2210.02303*，2022 年。

+   [14] U. Singer *等*，“Make-a-video：无需文本-视频数据的文本到视频生成，” *arXiv 预印本 arXiv:2209.14792*，2022 年。

+   [15] C. Chan *等*，“现在大家跳舞，” 在 *IEEE/CVF 国际计算机视觉会议论文集*，2019 年，第 5933–5942 页。

+   [16] W. Zhang *等*，“Sadtalker：学习现实的 3D 运动系数用于风格化的音频驱动单图像谈话面部动画，” 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 8652–8661 页。

+   [17] W. Hong *等*，“Cogvideo：通过变换器的大规模预训练用于文本到视频生成，” *arXiv 预印本 arXiv:2205.15868*，2022 年。

+   [18] A. Blattmann *等*，“对齐你的潜变量：使用潜在扩散模型进行高分辨率视频合成，” 在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 22 563–22 575 页。

+   [19] Y. Ma *等*，“Dreamtalk：当富有表现力的谈话头像生成遇上扩散概率模型，” *arXiv 预印本 arXiv:2312.09767*，2023 年。

+   [20] B. Qin *等*，“舞蹈头像：通过图像扩散模型进行姿势和文本指导的人体运动视频合成，” *arXiv 预印本 arXiv:2308.07749*，2023 年。

+   [21] T. Wang *等*，“Disco：用于现实人类舞蹈生成的解耦控制，” 2023 年。

+   [22] A. Van den Oord *等*，“基于像素 CNN 解码器的条件图像生成，” *神经信息处理系统进展*，第 29 卷，2016 年。

+   [23] A. Vaswani *等*，“注意力机制是你所需要的一切，” *神经信息处理系统进展*，第 30 卷，2017 年。

+   [24] O. Ronneberger *等*，“U-net：用于生物医学图像分割的卷积网络，” 见 *医学图像计算与计算机辅助手术–MICCAI 2015：第 18 届国际会议，德国慕尼黑，2015 年 10 月 5-9 日，会议论文集，第三部分第 18 卷*。 Springer，2015 年，第 234–241 页。

+   [25] A. Radford *等*，“从自然语言监督中学习可转移的视觉模型，” 见 *国际机器学习会议*。 PMLR，2021 年，第 8748–8763 页。

+   [26] Y. Chang *等*，“对大型语言模型评估的调查，” *arXiv 预印本 arXiv:2307.03109*，2023 年。

+   [27] G. Chen *等*，“Videollm：使用大型语言模型对视频序列进行建模，” *arXiv 预印本 arXiv:2305.13292*，2023 年。

+   [28] Y. Zhu *等*，“深度视频动作识别的综合研究，” *arXiv 预印本 arXiv:2012.06567*，2020 年。

+   [29] M. Bain，“通过语言的视角理解视频，” 博士学位论文，牛津大学，2023 年。

+   [30] S. Wu *等*，“Next-gpt：任意到任意的多模态 llm，” *arXiv 预印本 arXiv:2309.05519*，2023 年。

+   [31] X. Lai *等*，“Lisa：通过大型语言模型进行推理分割，” *arXiv 预印本 arXiv:2308.00692*，2023 年。

+   [32] A. Yang *等*，“Vid2seq：大规模预训练的视觉语言模型用于密集视频字幕生成，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 10,714–10,726 页。

+   [33] Y. Zhao *等*，“从大型语言模型中学习视频表示，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 6586–6597 页。

+   [34] J. Dave *等*，“用于密集视频字幕的分层语言建模，” 见 *发明计算与信息技术：ICICIT 2021 会议论文集*。 Springer，2022 年，第 421–431 页。

+   [35] K. Ma *等*，“Llavilo：通过基于适配器的多模态建模提升视频时刻检索，” 见 *IEEE/CVF 国际计算机视觉会议论文集*，2023 年，第 2798–2803 页。

+   [36] Z. Shao *等*，“通过答案启发式对大型语言模型进行提示以进行知识基础的视觉问答，” 见 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 14,974–14,983 页。

+   [37] A. C. A. M. de Faria *等*，“视觉问答：近期文献中技术和常见趋势的综述，” *arXiv 预印本 arXiv:2305.11033*，2023 年。

+   [38] H. J. Singh *等人*，“视觉问答的发展、应用、数据集和机遇：前沿调查”，在 *2023 年国际可持续计算与数据通信系统会议 (ICSCDS)*。IEEE，2023，第 778–785 页。

+   [39] J. Guo *等人*，“从图像到文本提示：使用冻结的大型语言模型进行零样本视觉问答”，在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2023，第 10 867–10 877 页。

+   [40] A. Salaberria *等人*，“图像描述：有效利用语言模型进行基于知识的视觉问答”，*专家系统应用*，第 212 卷，第 118669 页，2023。

+   [41] R. Liu *等人*，“一体化：无需视频指导调优的视频对话是可行的”，*arXiv 预印本 arXiv:2309.15785*，2023。

+   [42] M. Maaz *等人*，“Video-chatgpt：通过大型视觉和语言模型实现详细的视频理解”，*arXiv 预印本 arXiv:2306.05424*，2023。

+   [43] Z. Hu *等人*，“Reveal：利用多源多模态知识记忆进行检索增强的视觉-语言预训练”，在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2023，第 23 369–23 379 页。

+   [44] M. Yuksekgonul *等人*，“视觉-语言模型何时以及为何表现得像词袋，以及如何应对？”在 *第十一届国际学习表征会议*，2022。

+   [45] M. Gao *等人*，“视频目标分割的深度学习：综述”，*人工智能综述*，第 56 卷，第 1 期，第 457–531 页，2023。

+   [46] H. Zhang *等人*，“Video-llama：一个经过指令调优的音视频语言模型用于视频理解”，*arXiv 预印本 arXiv:2306.02858*，2023。

+   [47] T. Azmin *等人*，“在 5G 移动网络中使用 Informer 进行带宽预测”，在 *2022 年第 13 届未来网络国际会议 (NoF)*。IEEE，2022，第 1–9 页。

+   [48] J. Li *等人*，“在 360 度视频多播中，利用球面卷积增强视口预测，结合有限视场反馈”，*ACM 多媒体计算、通信与应用期刊*，第 19 卷，第 1 期，第 1–23 页，2023。

+   [49] S. Van Damme *等人*，“基于机器学习的内容无关视口预测用于 360 度视频”，*ACM 多媒体计算、通信与应用期刊 (TOMM)*，第 18 卷，第 2 期，第 1–24 页，2022。

+   [50] F.-Y. Chao *等人*，“基于 Transformer 的 360°视频长期视口预测：你只需扫描路径。”在 *MMSP*，2021，第 1–6 页。

+   [51] Y. Cheng *等人*，“使用 Transformer 进行视线估计”，在 *2022 年第 26 届国际模式识别大会 (ICPR)*。IEEE，2022，第 3341–3347 页。

+   [52] Z. Ni *等人*，“通过视线跟踪进行视频中的人-物体交互预测”，*计算机视觉与图像理解*，第 103741 页，2023。

+   [53] J. Xiang *等人*，“Mimt: 用于视频压缩的掩码图像建模变换器，” 载于 *第十一届国际学习表示会议*，2022 年。

+   [54] F. Mentzer *等人*，“Vct: 视频压缩变换器，” *arXiv 预印本 arXiv:2206.07307*，2022 年。

+   [55] Z. Ding *等人*，“Tgan: 用于大规模图像生成的深度张量生成对抗网络，” *arXiv 预印本 arXiv:1901.09953*，2019 年。

+   [56] C. Wu *等人*，“Nuwa-infinity: 无限视觉合成的自回归生成，” *arXiv 预印本 arXiv:2207.09814*，2022 年。

+   [57] M. Ding *等人*，“Cogview: 通过变换器掌握文本到图像生成，” *神经信息处理系统进展*，第 34 卷，第 19,822–19,835 页，2021 年。

+   [58] M. Bain *等人*，“Frozen in time: 用于端到端检索的联合视频和图像编码器，” 载于 *IEEE/CVF 国际计算机视觉会议论文集*，2021 年，第 1728–1738 页。

+   [59] L. Khachatryan *等人*，“Text2video-zero: 文本到图像扩散模型是零-shot 视频生成器，” *arXiv 预印本 arXiv:2303.13439*，2023 年。

+   [60] N. Ruiz *等人*，“Dreambooth: 针对主题驱动生成的文本到图像扩散模型的微调，” 载于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 22,500–22,510 页。

+   [61] A. Kirillov *等人*，“Segment anything，” *arXiv 预印本 arXiv:2304.02643*，2023 年。

+   [62] L. Yu *等人*，“语言模型击败扩散——分词器是视觉生成的关键，” *arXiv 预印本 arXiv:2310.05737*，2023 年。

+   [63] D. Kondratyuk *等人*，“Videopoet: 用于零-shot 视频生成的大型语言模型，” *arXiv 预印本 arXiv:2312.14125*，2023 年。

+   [64] G. Kim *等人*，“扩散视频自动编码器：通过解缠视频编码实现时间一致的面部视频编辑，” 载于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，2023 年，第 6091–6100 页。

+   [65] J. Z. Wu *等人*，“Tune-a-video: 图像扩散模型的一次性调整用于文本到视频生成，” *arXiv 预印本 arXiv:2212.11565*，2022 年。

+   [66] D. Ceylan *等人*，“Pix2video: 使用图像扩散的视频编辑，” *arXiv 预印本 arXiv:2303.12688*，2023 年。

+   [67] Y. Kasten *等人*，“分层神经图谱用于一致的视频编辑，” *ACM 图形学交易（TOG）*，第 40 卷，第 6 期，第 1–12 页，2021 年。

+   [68] E. Lu *等人*，“分层神经渲染用于视频中人物的重定时，” *arXiv 预印本 arXiv:2009.07833*，2020 年。

+   [69] O. Bar-Tal *等人*，“Text2live: 基于文本的分层图像和视频编辑，” 载于 *欧洲计算机视觉会议*。  Springer，2022 年，第 707–723 页。

+   [70] M. Oliu *等人*，“折叠递归神经网络用于未来视频预测，” 载于 *欧洲计算机视觉会议论文集（ECCV）*，2018 年，第 716–731 页。

+   [71] L. Castrejon *等人*，“改进的条件 VRNN 用于视频预测，” 载于 *IEEE/CVF 国际计算机视觉会议论文集*，2019 年，第 7608–7617 页。

+   [72] T. Höppe *等*，“视频预测和填补的扩散模型”，*arXiv 预印本 arXiv:2206.07696*，2022 年。

+   [73] V. Voleti *等*，“Mcvd-掩码条件视频扩散用于预测、生成和插值”，*神经信息处理系统进展*，第 35 卷，第 23 371–23 385 页，2022 年。

+   [74] W. Harvey *等*，“长视频的灵活扩散建模”，*神经信息处理系统进展*，第 35 卷，第 27 953–27 965 页，2022 年。

+   [75] G. A. S. Surek *等*，“基于视频的人类活动识别：深度学习方法”，*传感器*，第 23 卷，第 14 期，第 6384 页，2023 年。

+   [76] X. Hu *等*，“视频中的在线人类动作检测与预测：综述”，*神经计算*，第 491 卷，第 395–413 页，2022 年。

+   [77] M. G. Morshed *等*，“人类动作识别：基于分类法的调查、更新和机会”，*传感器*，第 23 卷，第 4 期，第 2182 页，2023 年。

+   [78] C. Zhang *等*，“人机交互的大型语言模型：综述”，*仿生智能与机器人*，第 100131 页，2023 年。

+   [79] H. Kaneko *等*，“利用大型语言模型在人体活动识别中开创传感器和特征”，见于*2023 年 ACM 国际联合会议：普适计算与可穿戴计算大会附录论文集*，2023，第 475–479 页。

+   [80] Y. Zhou *等*，“Tent：将语言模型与 IoT 传感器连接以实现零样本活动识别”，*arXiv 预印本 arXiv:2311.08245*，2023 年。

+   [81] W. Wu *等*，“基于双向跨模态知识探索的视频识别与预训练视觉语言模型”，见于*IEEE/CVF 计算机视觉与模式识别会议论文集*，2023，第 6620–6630 页。

+   [82] W. Kay *等*，“The kinetics human action video dataset”，*arXiv 预印本 arXiv:1705.06950*，2017 年。

+   [83] K. Li *等*，“Videochat：以聊天为中心的视频理解”，*arXiv 预印本 arXiv:2305.06355*，2023 年。

+   [84] D. Driess *等*，“Palm-e：一种具身的多模态语言模型”，*arXiv 预印本 arXiv:2303.03378*，2023 年。

+   [85] D. Shah *等*，“Lm-nav：使用大型预训练语言、视觉和动作模型的机器人导航”，见于*机器人学习会议*。PMLR，2023，第 492–504 页。

+   [86] F. Wu *等*，“体育视频动作识别综述：数据集、方法和应用”，*IEEE 多媒体学报*，2022 年。

+   [87] I. Singh *等*，“Progprompt：使用大型语言模型生成具体的机器人任务计划”，见于*2023 IEEE 国际机器人与自动化会议（ICRA）*。IEEE，2023，第 11 523–11 530 页。

+   [88] A. Brohan *等*，“Rt-2：视觉-语言-动作模型将网络知识转移到机器人控制中”，*arXiv 预印本 arXiv:2307.15818*，2023 年。

+   [89] A. Deng *等*，“利用语言辅助深度学习模型识别自闭症儿童视频中的问题行为”，*arXiv 预印本 arXiv:2211.09310*，2022 年。

+   [90] 罗 R.等人，“Valley: 大型语言模型增强能力的视频助理，”*arXiv 预印本 arXiv:2306.07207*，2023 年。

+   [91] 李 K.等人，“无面具教师: 向训练高效的视频基础模型迈进，”*arXiv 预印本 arXiv:2303.16058*，2023 年。

+   [92] Wiegand T.等人，“H.264/AVC 视频编码标准概述，”*IEEE 交易视频技术电路与系统*，第 13 卷，第 7 号，第 560-576 页，2003 年。

+   [93] 沙利文 G.J.等人，“高效视频编码（hevc）标准概述，”*IEEE 交易视频技术电路与系统*，第 22 卷，第 12 号，第 1649-1668 页，2012 年。

+   [94] 华为，“云 vr 导向的承载网络白皮书，”*华为 iLab VR 技术白皮书*，2017 年。

+   [95] 艾 H.等人，“全向视深度学习: 调查和新视角，”*arXiv 预印本 arXiv:2205.10468*，2022 年。

+   [96] 万德胡夫 J.等人，“从捕捉到呈现: 具有六自由度的体积媒体传送，”*IEEE 通讯杂志*，第 58 卷，第 10 号，第 49-55 页，2020 年。

+   [97] 刘 Z.等人，“点云视频流: 挑战与解决方案，”*IEEE 网络*，第 35 卷，第 5 号，第 202-209 页，2021 年。

+   [98] 郭 J.等人，“短视频流中的视频质量驱动策略，”*第 24 届无线与移动系统建模、分析和仿真国际会议论文集*，2021 年，第 221-228 页。

+   [99] 刘 C.等人，“学习轨迹感知变压器用于视频超分辨率，”*IEEE/CVF 计算机视觉与模式识别会议论文集*，2022 年，第 5687-5696 页。

+   [100] 耿 Z.等人，“Rstt: 时空视频超分辨率的实时时空变压器，”*IEEE/CVF 计算机视觉与模式识别会议论文集*，2022 年，第 17 441-17 451 页。

+   [101] 瓦斯卢安 F.等人，“高效视频增强变压器，”*2022 年 IEEE 国际图像处理会议*，IEEE，2022 年，第 4068-4072 页。

+   [102] 雷内拉 N.等人，“利用生成 ai 实现自动化视频游戏评论，”2023 年。

+   [103] 林 K.等人，“Swinbert: 用于视频字幕的稀疏注意力端到端变压器，”*IEEE/CVF 计算机视觉与模式识别会议论文集*，2022 年，第 17 949-17 958 页。

+   [104] 卡塔迪格 C.等人，“Videotrain: 用于合成视频流量生成的生成对抗框架，”*2021 年 IEEE 第 22 届世界无线、移动和多媒体网络研讨会*，IEEE，2021 年，第 209-218 页。

+   [105] 安加拉诺 S.等人，“知识蒸馏边缘的生成对抗超分辨率，”*人工智能工程应用*，第 123 卷，第 106407 页，2023 年。

+   [106] 田 Y.等人，“高分辨率视频合成所需的良好图像生成器，”*arXiv 预印本 arXiv:2104.15069*，2021 年。

+   [107] R. Villegas *等*，“自然视频序列预测的运动和内容分解”，*arXiv 预印本 arXiv:1706.08033*，2017 年。

+   [108] S. Yu *等*，“使用动态感知隐式生成对抗网络生成视频”，*arXiv 预印本 arXiv:2202.10571*，2022 年。

+   [109] M. Saito *等*，“稀疏训练，密集生成：高分辨率时序 GAN 的内存高效无监督训练”，*国际计算机视觉杂志*，第 128 卷，第 10-11 期，页码 2586–2606，2020 年。

+   [110] I. Skorokhodov *等*，“Stylegan-v：一种具有 Stylegan2 价格、图像质量和优点的连续视频生成器”，在 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2022 年，页码 3626–3636。

+   [111] Z. Guo *等*，“评估大语言模型：综合综述”，*arXiv 预印本 arXiv:2310.19736*，2023 年。

+   [112] J. Huang *等*，“面向大语言模型推理：综述”，*arXiv 预印本 arXiv:2212.10403*，2022 年。

+   [113] L. Wang *等*，“知识蒸馏和学生-教师学习在视觉智能中的应用：综述与新展望”，*IEEE 模式分析与机器智能学报*，第 44 卷，第 6 期，页码 3048–3068，2021 年。

+   [114] J. Zhu *等*，“一个好的学生是合作和可靠的：CNN-Transformer 协作学习用于语义分割”，在 *IEEE/CVF 国际计算机视觉大会论文集*，2023 年，页码 11 720–11 730。

+   [115] I. Sodagar，“用于互联网多媒体流媒体的 MPEG-DASH 标准”，*IEEE 多媒体*，第 18 卷，第 4 期，页码 62–67，2011 年。

+   [116] J. van der Hooft *等*，“基于 HTTP/2 的 HEVC 视频在 4G/LTE 网络上的自适应流媒体传输”，*IEEE 通讯快报*，第 20 卷，第 11 期，页码 2177–2180，2016 年。

生成于 2024 年 4 月 30 日星期二 19:27:28，由 LaTeXML![吉祥物 Sammy](http://dlmf.nist.gov/LaTeXML/)
