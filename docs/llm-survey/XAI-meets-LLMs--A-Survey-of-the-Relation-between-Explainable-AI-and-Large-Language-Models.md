<!--yml

分类：未分类

日期：2024-09-03 17:28:19

-->

# XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查

> 来源：[`arxiv.org/html/2407.15248`](https://arxiv.org/html/2407.15248)

1.  [1 引言](https://arxiv.org/html/2407.15248v1#S1 "在 XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [1.1 贡献](https://arxiv.org/html/2407.15248v1#S1.SS1 "在 1 引言 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [1.2 研究问题](https://arxiv.org/html/2407.15248v1#S1.SS2 "在 1 引言 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

1.  [2 LLMs 中解释的需求](https://arxiv.org/html/2407.15248v1#S2 "在 XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [可见用户挑战](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px1 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [信任与透明度](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px2 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [误用与批判性思维影响](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px3 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [不可见用户挑战](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px4 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [伦理和隐私问题](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px5 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [不准确性和幻觉](https://arxiv.org/html/2407.15248v1#S2.SS0.SSS0.Px6 "在 2 LLMs 中解释的需求 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

1.  [3 方法论](https://arxiv.org/html/2407.15248v1#S3 "在 XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

    1.  [3.1 论文检索](https://arxiv.org/html/2407.15248v1#S3.SS1 "在 3 方法论 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

        1.  [概述](https://arxiv.org/html/2407.15248v1#S3.SS1.SSS0.Px1 "在 3.1 论文检索 ‣ 3 方法论 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

        1.  [同行评审论文](https://arxiv.org/html/2407.15248v1#S3.SS1.SSS0.Px2 "在 3.1 论文检索 ‣ 3 方法论 ‣ XAI 遇见 LLMs：可解释人工智能与大型语言模型关系的调查")

        1.  [预印本论文](https://arxiv.org/html/2407.15248v1#S3.SS1.SSS0.Px3 "在 3.1 论文检索 ‣ 3 方法论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [3.2 论文选择](https://arxiv.org/html/2407.15248v1#S3.SS2 "在 3 方法论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [3.3 处理假阳性](https://arxiv.org/html/2407.15248v1#S3.SS3 "在 3 方法论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

1.  [4 检索结果](https://arxiv.org/html/2407.15248v1#S4 "在 XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [4.1 应用论文](https://arxiv.org/html/2407.15248v1#S4.SS1 "在 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

        1.  [4.1.1 解释](https://arxiv.org/html/2407.15248v1#S4.SS1.SSS1 "在 4.1 应用论文 ‣ 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

        1.  [4.1.2 作为特征](https://arxiv.org/html/2407.15248v1#S4.SS1.SSS2 "在 4.1 应用论文 ‣ 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [4.2 讨论论文](https://arxiv.org/html/2407.15248v1#S4.SS2 "在 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

        1.  [4.2.1 问题](https://arxiv.org/html/2407.15248v1#S4.SS2.SSS1 "在 4.2 讨论论文 ‣ 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

        1.  [4.2.2 基准与指标](https://arxiv.org/html/2407.15248v1#S4.SS2.SSS2 "在 4.2 讨论论文 ‣ 4 检索结果 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

1.  [5 讨论](https://arxiv.org/html/2407.15248v1#S5 "在 XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [开源参与](https://arxiv.org/html/2407.15248v1#S5.SS0.SSS0.Px1 "在 5 讨论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [目标](https://arxiv.org/html/2407.15248v1#S5.SS0.SSS0.Px2 "在 5 讨论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

    1.  [目标](https://arxiv.org/html/2407.15248v1#S5.SS0.SSS0.Px3 "在 5 讨论 ‣ XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

1.  [6 结论](https://arxiv.org/html/2407.15248v1#S6 "在 XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研")

# XAI 遇见 LLMs：可解释 AI 与大型语言模型之间关系的调研

\IEEEauthorblockNErik Cambria

\IEEEauthorblockAS 计算机科学与工程学院

南洋理工大学，新加坡

cambria@ntu.edu.sg    \IEEEauthorblockNLorenzo Malandri

\IEEEauthorblockA 统计与定量方法系

米兰比可卡大学，意大利米兰

lorenzo.malandri@unimib.it    \IEEEauthorblockNFabio Mercorio

\IEEEauthorblockA 统计与定量方法系

米兰比可卡大学，意大利米兰

fabio.mercorio@unimib.it    \IEEEauthorblockNNavid Nobani

\IEEEauthorblockA 统计与定量方法系

米兰比可卡大学，意大利米兰

navid.nobani@unimib.it    \IEEEauthorblockNAndrea Seveso

\IEEEauthorblockA 统计与定量方法系

米兰比可卡大学，意大利米兰

andrea.seveso@unimib.it

###### 摘要

在这项调查中，我们探讨了大型语言模型（LLM）研究中的关键挑战，重点关注可解释性的重要性。在 AI 和商业领域日益增长的兴趣推动下，我们强调了 LLM 的透明性需求。我们考察了当前 LLM 研究和可解释人工智能（XAI）的双重路径：通过 XAI 提升性能以及对模型可解释性的日益关注。我们的论文主张采取一种平衡的方法，将可解释性与功能进展同等重视。鉴于 LLM 研究的快速发展，我们的调查包括了同行评审和预印本（arXiv）论文，提供了 XAI 在 LLM 研究中作用的全面概述。我们最后敦促研究界共同推动 LLM 和 XAI 领域的发展。

{IEEE 关键词}

可解释人工智能、可解释机器学习、大型语言模型、自然语言处理

## 1 引言

\IEEEPARstart

大型语言模型的出现显著影响了人工智能（AI），由于它们在多个自然语言处理（NLP）应用中的卓越表现。它们的多功能性减少了对手工特征的需求，使其能够应用于多个领域。它们在内容生成和上下文理解方面的高度创造力促进了创意写作和对话 AI 的发展。此外，广泛的预训练使大型语言模型在没有进一步领域特定数据的情况下展示了强大的泛化能力 Zhao 等人（[2023a](https://arxiv.org/html/2407.15248v1#bib.bib1)）；Amin 等人（[2023](https://arxiv.org/html/2407.15248v1#bib.bib2)）。因此，大型语言模型迅速成为主流工具，深度融入许多行业领域，如医学（例如，Thirunavukarasu 等人（[2023](https://arxiv.org/html/2407.15248v1#bib.bib3)））和金融（例如，Wu 等人（[2023a](https://arxiv.org/html/2407.15248v1#bib.bib4)）），仅举几例。

然而，它们的出现也引发了伦理问题，需要持续努力解决与偏见、虚假信息和负责任的 AI 部署相关的问题。LLMs 是一个臭名昭著的复杂“黑箱”系统。它们的内部工作机制不透明，其复杂性使得解释它们变得具有挑战性（Kaadoud et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib5)); Cambria et al. ([2023a](https://arxiv.org/html/2407.15248v1#bib.bib6))）。这种不透明性可能导致产生不适当的内容或误导性输出（Weidinger et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib7))）。最后，缺乏对其训练数据的可见性可能进一步阻碍在关键应用中的信任和问责（Liu ([2023](https://arxiv.org/html/2407.15248v1#bib.bib8))）。

在这种背景下，XAI 是复杂的基于 LLM 的系统与人类理解其行为之间的关键桥梁。为 LLMs 开发 XAI 框架对于建立用户信任、确保问责制以及促进这些模型的负责任和伦理使用至关重要。

在本文中，我们以结构化的方式回顾和分类了当前针对 LLMs 的 XAI，强调了清晰和真实解释的重要性，正如 Sevastjanova 和 El-Assady ([2022](https://arxiv.org/html/2407.15248v1#bib.bib9)) 所建议的，旨在指导未来研究，以提升 LLMs 的可解释性和在实际应用中的可信度。

### 1.1 贡献

我们工作的贡献有三方面：

1.  1.

    我们提出了一个新的分类框架，用于评估关于 LLMs 可解释性的研究成果。该框架提供了对前沿技术的清晰和有组织的概述。

1.  2.

    我们基于 ArXiv 和 DBLP 数据库，进行了一项全面的同行评审和预印本论文的调查，超越了常用的研究工具。

1.  3.

    我们批判性地评估了当前的做法，识别了研究空白和问题，并阐明了潜在的未来研究方向。

### 1.2 研究问题

在本次调查中，我们探讨了 XAI 方法与 LLMs 的共存以及这两个领域如何融合。具体而言，我们的调查围绕以下关键问题展开：

1.  Q1

    目前，XAI 技术如何与 LLMs 集成？

1.  Q2

    LLMs 与 XAI 方法学融合的最新趋势是什么？

1.  Q3

    当前相关文献中存在哪些空白，哪些领域需要进一步研究？

## 2 LLMs 中对解释的需求

在 XAI 领域，与 LLMs 的交集带来了独特的挑战和机遇。本文旨在剖析这些挑战，扩展对 XAI 目标的传统理解，即为各种利益相关者揭示不透明模型的内部机制，同时避免引入新的不确定性（例如，Cambria et al. ([2023b](https://arxiv.org/html/2407.15248v1#bib.bib10)); Burkart 和 Huber ([2021](https://arxiv.org/html/2407.15248v1#bib.bib11))）。

尽管有了进步，LLMs 在复杂性和不透明性方面仍存在困难，这引发了设计、部署和解释的问题。受到 Weidinger et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib7)) 的启发，本文将 LLMs 面临的挑战分为用户可见和不可见两类。

##### 用户可见的挑战

用户在没有专业工具的情况下直接感知的挑战。

##### 信任与透明度

在关键领域如医疗 Mercorio et al. ([2020](https://arxiv.org/html/2407.15248v1#bib.bib12)); Gozzi et al. ([2022](https://arxiv.org/html/2407.15248v1#bib.bib13)); Alimonda et al. ([2022](https://arxiv.org/html/2407.15248v1#bib.bib14)) 或金融 Xing et al. ([2020](https://arxiv.org/html/2407.15248v1#bib.bib15)); Castelnovo et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib16)); Yeo et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib17))，由于黑箱模型（包括 LLMs）的不透明性，信任问题日益突出。解释性人工智能（XAI）必须提供透明且符合伦理的解释，以获得更广泛的接受，尤其是在那些要求解释性的严格法规下（例如，欧盟的 GDPR Novelli et al. ([2024](https://arxiv.org/html/2407.15248v1#bib.bib18)))。这影响了法规合规性和公众信誉，例如欧洲技能智能项目要求 XAI 提供决策解释，参考 Malandri et al. ([2022a](https://arxiv.org/html/2407.15248v1#bib.bib19), [2024](https://arxiv.org/html/2407.15248v1#bib.bib20), [b](https://arxiv.org/html/2407.15248v1#bib.bib21), [c](https://arxiv.org/html/2407.15248v1#bib.bib22))。

##### 滥用与批判性思维影响

LLMs 的多功能性存在被滥用的风险，例如用于有害目的的内容创建和逃避监管，参考 Shen et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib23))。过度依赖 LLMs 也可能削弱批判性思维和独立分析，这在教育环境中表现尤为明显（参见，例如 Abd-Alrazaq et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib24))）。

##### 用户不可见的挑战

需要更深层次模型理解的挑战。

##### 伦理与隐私关注

LLM 使用中的伦理困境，例如公平性和仇恨言论问题，以及隐私风险如敏感数据泄露，需要采取积极措施和伦理指南，参考 Weidinger et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib7)); Yan et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib25)); Salimi and Saheb ([2023](https://arxiv.org/html/2407.15248v1#bib.bib26))。

##### 不准确性与幻觉

大型语言模型（LLMs）可能生成虚假信息，这在教育、新闻和医疗等多个领域中存在风险。解决这些问题需要提高 LLMs 的准确性、教育用户以及开发事实检查系统，参考 Rawte et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib27)); Azaria and Mitchell ([2023](https://arxiv.org/html/2407.15248v1#bib.bib28))。

## 3 方法论

系统化映射研究（SMSs）是全面的调查，分类和总结特定研究领域中发布的各种作品，识别文献空白、趋势和未来研究需求。在大型或未充分探索的领域中，系统化文献综述（SLR）可能不可行时，它们特别有用。

SMS 和 SLR 遵循三阶段方法（规划、执行、报告），但方法不同，因为 SMS 解决更广泛的问题，覆盖更多的文献，审查较少细致，旨在提供研究领域的概述。而 SLR 侧重于具体问题，彻底审查较少的文献，力求获得精确的、基于证据的结果，Barn 等人（[2017](https://arxiv.org/html/2407.15248v1#bib.bib29)）对此进行了详细探讨。

根据 Martínez-Gárate 等人（[2023](https://arxiv.org/html/2407.15248v1#bib.bib30)）的方法，我们为 XAI 和 LLMs 设计了我们的 SMS，包括同行评审和预印本论文。后者的选择是因为我们相信在计算机科学等快速发展的领域中，包括提供最新研究的预印本对于全面评审至关重要，Oikonomidi 等人（[2020](https://arxiv.org/html/2407.15248v1#bib.bib31)）也持相同观点。

我们按照以下步骤构建我们的 SMS：第 [1.2](https://arxiv.org/html/2407.15248v1#S1.SS2 "1.2 研究问题 ‣ 1 引言 ‣ XAI 与 LLMs 结合：解释性人工智能与大型语言模型关系的调查") 节提出并定义了研究问题，第 [3.1](https://arxiv.org/html/2407.15248v1#S3.SS1 "3.1 论文检索 ‣ 3 方法 ‣ XAI 与 LLMs 结合：解释性人工智能与大型语言模型关系的调查") 节描述了如何进行论文检索；第 [3.2](https://arxiv.org/html/2407.15248v1#S3.SS2 "3.2 论文选择 ‣ 3 方法 ‣ XAI 与 LLMs 结合：解释性人工智能与大型语言模型关系的调查") 节描述了基于定义标准的论文选择过程；第 [3.3](https://arxiv.org/html/2407.15248v1#S3.SS3 "3.3 处理假阳性 ‣ 3 方法 ‣ XAI 与 LLMs 结合：解释性人工智能与大型语言模型关系的调查") 节解释了如何处理假阳性结果，最后在第 [4](https://arxiv.org/html/2407.15248v1#S4 "4 检索结果 ‣ XAI 与 LLMs 结合：解释性人工智能与大型语言模型关系的调查") 节我们描述了获得的结果。

### 3.1 论文检索

##### 概述

我们没有利用诸如 Google Scholar 之类的常见科学搜索引擎，而是采用了以下部分描述的自定义搜索方法。通过审查所获得论文的标题和摘要，我们使用与 LLMs 和 XAI 相关的预定义关键词集进行了有针对性的搜索。这种手动和深思熟虑的搜索策略旨在最小化遗漏自动搜索算法可能忽视的相关研究的风险，并确保我们 SMS 数据集的准确性和相关性。通过这一严格的过程，我们构建了一个明确定义的文献语料库，准备进行深入分析和审查。图 [1](https://arxiv.org/html/2407.15248v1#S3.F1 "图 1 ‣ 同行评审论文 ‣ 3.1 论文检索 ‣ 3 方法论 ‣ XAI 遇见 LLMs：解释性人工智能与大型语言模型关系的综述") 概述了这一过程。

##### 同行评审论文

我们通过识别 2022 年（研究开始时的最后一年）“人工智能”类别中的顶级 Q1 期刊来启动了这一步骤，这为我们提供了 58 本期刊，以便从中提取相关出版物。

随后，我们利用来自 dblp 计算机科学文献的 XML 转储¹¹1[`dblp.org/xml/dblp.xml.gz`](https://dblp.org/xml/dblp.xml.gz)来获取所有在识别的 Q1 期刊中发布的论文标题，除了十本不被 dblp 涵盖的期刊。我们收集了这些论文标题后，接着查找它们的摘要。为此，我们最初使用了 AMiner 的最后一个可用引用网络²²2[`originalfileserver.aminer.cn/misc/dblp_v14.tar.gz`](https://originalfileserver.aminer.cn/misc/dblp_v14.tar.gz)，但由于该转储缺少大部分 2023 年的出版物，我们利用了 Scopus API，这是一个详细的科学摘要和引用数据库，以检索与收集的标题对应的缺失摘要。

![参见说明](img/5ef62bde5736f5f34fe024e9df6e0e39.png)

图 1：用于获取与我们关键词相关的论文的过程，包括研究问题的定义、论文检索、论文选择、消除假阳性和将论文分类到预定义的类别中。

##### 预印本论文

我们抓取了 2010 年到 2023 年 10 月在 Arxiv 数据库中呈现的所有计算机科学论文，共计 548,711 篇。因此，我们使用了 Arxiv API 来获取这些论文的摘要。

### 3.2 论文选择

我们使用了一套全面的关键词来筛选收集的论文，以确保其与 LLMs 和 XAI 的相关性。这些搜索词被仔细挑选，以涵盖每个领域常用的各种术语和短语。³³3 XAI 的关键词包括：[’xai’，’explain’，’explanation’，’interpret’，’black box’，’black-box’，’blackbox’，’transparent model understanding’，’feature importance’，’accountable ai’，’ethical ai’，’trustworthy ai’，’fairness’，’ai justification’，’causal inference’，’ai audit’]；而 LLMs 的关键词包括：[’llm’，’large language model’，’gpt-3’，’gpt-2’，’gpt3’，’gpt2’，’bert’，’language model pre-training’，’fine-tuning language models’，’generative pre-trained transformer’，’llama’，’bard’，’roberta’，’T5’，’xlnet’，’megatron’，’electra’，’deberta’，’ernie’，’albert’，’bart’，’blenderbot’，’open pre-trained transformer’，’mt-nlg’，’turing-nlg’，’pegasus’，’gpt-3.5’，’gpt-4’，’gpt3.5’，’gpt4’，’cohere’，’claude’，’jurassic-1’，’openllama’，’falcon’，’dolly’，’mpt’，’guanaco’，’bloom’，’alpaca’，’openchatkit’，’gpt4all’，’flan-t5’，’orca’]

在我们的搜索中，我们在每个列表的成员之间应用了逻辑或操作符，以捕获单一类别中的任何术语，而在两个列表之间使用了逻辑与操作符，以确保只检索包含两个类别术语的论文进行分析。

### 3.3 处理假阳性

完成初步检索阶段后，我们共识别了 1,030 篇手稿。由于一些研究关键词具有广泛的含义，例如’explain’和’interpret’可以在与 XAI 不同的上下文中使用，我们检索到了一些假阳性论文，即仅涉及 XAI 或 LLMs 其中之一的论文。我们排除了这些假阳性——即仅涉及 XAI 或 LLMs 独立的或两者都不涉及的出版物。为此，我们手动分析了每篇论文的标题和摘要。这一细致的审查过程最终筛选出了 233 篇与 XAI 和 LLMs 相关的论文。

由于将所有这些论文纳入我们的调查是不切实际的，我们根据每年平均引用次数选择了最相关的论文。整个研究过程共选出了 35 篇文章。

## 4 检索结果

我们将论文分为两个宏观类别：应用论文，即以某种方式生成解释的论文，无论是为了可解释性还是将其作为其他任务的特征；以及讨论论文，即那些不涉及解释生成，但讨论解释性 LLM 模型相关问题或研究空白的论文。

### 4.1 应用论文

第一个宏类包括在方法论、工具或任务中使用 LLMs 的论文。根据 LLMs 的使用方式，我们进一步将该类别分为以下两个子类别：“解释”，即尝试解释 LLMs 如何工作并提供这些模型不透明本质的见解的论文。第二个子类别称为“作为特征”，使用 LLMs 生成的解释和特征来改进各种任务的结果。以下部分将讨论这些子类别：

| 论文和工具 | Star | Fork | 更新 | 目标 | 无关 | 目标 |
| --- | --- | --- | --- | --- | --- | --- |
| Vig ([2019](https://arxiv.org/html/2407.15248v1#bib.bib32))  [BertViz](https://github.com/jessevig/bertviz) | 6.1k | 734 | 08/23 | Transformers | ✓ | C  E  IMP  INT  R |
| Swamy et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib33))  [Experiments](https://github.com/epfml/interpret-lm-knowledge) | 19 | 2 | 05/22 | 基于 BERT 的 LM | ✗ | C  E  IMP  INT  R |
| Wu et al. ([2021](https://arxiv.org/html/2407.15248v1#bib.bib34))  [Polyjuice](https://github.com/tongshuangwu/polyjuice) | 90 | 16 | 08/22 | - | ✓ | C  E  IMP  INT  R |
| Wang et al. ([2022](https://arxiv.org/html/2407.15248v1#bib.bib35))  [TransformerLens](https://github.com/redwoodresearch/Easy-Transformer) | 48 | 161 | 01/23 | GPT2-small | ✗ | C  E  IMP  INT  R |
| Menon and Vondrick ([2022](https://arxiv.org/html/2407.15248v1#bib.bib36)) - | - | - | - | Vision-LM | ✓ | C  E  IMP  INT  R |
| Gao et al. ([2023a](https://arxiv.org/html/2407.15248v1#bib.bib37))  [Experiments](https://github.com/ArrogantL/ChatGPT4CausalReasoning) | 17 | 0 | 10/23 | ChatGPT | ✗ | C  E  IMP  INT  R |
| Pan et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib38)) - | - | - | - | LLMs | ✓ | C  E  IMP  INT  R |
| Conmy et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib39))  [ACDC](https://github.com/ArthurConmy/Automatic-Circuit-Discovery) | 105 | 23 | 11/23 | Transformers | ✓ | C  E  IMP  INT  R |
| He et al. ([2022](https://arxiv.org/html/2407.15248v1#bib.bib40))  [RR](https://github.com/HornHehhf/RR) | 38 | 2 | 02/23 | LLMs | ✓ | C  E  IMP  INT  R |
| Yoran et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib41))  [MCR](https://github.com/oriyor/reasoning-on-cots) | 71 | 9 | 01/24 | LLMs | ✓ | C  E  IMP  INT  R |
| Sarti et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib42))  [Inseq](https://github.com/inseq-team/inseq) | 250 | 26 | 01/24 | SeqGen models | ✓ | C  E  IMP  INT  R |
| Wu et al. ([2023b](https://arxiv.org/html/2407.15248v1#bib.bib43))  [Boundless DAS](https://github.com/frankaging/align-transformers) | 0 | 17 | 01/24 | LLMs | ✓ | C  E  IMP  INT  R |
| Li et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib44))  [XICL](https://github.com/paihengxu/XICL) | 1 | 3 | 11/23 | LLMs | ✓ | C  E  IMP  INT  R |
| Chen et al. ([2023](https://arxiv.org/html/2407.15248v1#bib.bib45)) LMExplainer | - | - | - | LLMs | ✓ | C  E  IMP  INT  R |
| Gao 等 ([2023b](https://arxiv.org/html/2407.15248v1#bib.bib46)) Chat-REC | - | - | - | Rec. systems | ✗ | C  E  IMP  INT  R |
| Zhang 等 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib47))  [DSRLM](https://github.com/moqingyan/dsr-lm) | 9 | 1 | 07/23 | LLMs | ✓ | C  E  IMP  INT  R |
| Singh 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib48))  [SASC](https://github.com/csinva/imodelsX) | 61 | 14 | 01/24 | LLMs | ✓ | C  E  IMP  INT  R |
| Li 等 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib49)) - | - | - | - | LLMs | ✓ | C  E  IMP  INT  R |
| Ye 和 Durrett ([2022](https://arxiv.org/html/2407.15248v1#bib.bib50))  [TextualExplInContext](https://github.com/xiye17/TextualExplInContext) | 11 | 2 | 02/23 | LLMs | ✓ | C  E  IMP  INT  R |
| Turpin 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib51))  [Experiments](https://github.com/milesaturpin/cot-unfaithfulness) | 25 | 9 | 03/23 | LLMs | ✓ | C  E  IMP  INT  R |
| Kang 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib52)) AutoSD | - | - | - | Debugging models | ✗ | C  E  IMP  INT  R |
| Krishna 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib53)) AMPLIFY | - | - | - | LLMs | ✓ | C  E  IMP  INT  R |
| Yang 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib54))  [Labo](https://github.com/YueYANG1996/LaBo) | 51 | 4 | 12/23 | CBM | ✗ | C  E  IMP  INT  R |
| Bitton-Guetta 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib55))  [WHOOPS!](https://whoops-benchmark.github.io/) | - | - | - | LLMs | ✓ | C  E  IMP  INT  R |
| Shi 等 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib56))  [Chatgraph](https://github.com/sycny/ChatGraph) | 2 | 0 | 07/23 | LLMs | ✓ | C  E  IMP  INT  R |

表 1：近期应用论文的汇总，总结了截至 2024 年 1 月的参与指标、更新时间线、模型特异性以及每项研究的总体目标。在表格的第一部分，列出了“解释”论文，在第二部分列出了“特征”工作。对于缺少关联存储库的论文，星标、叉号和最后更新时间未报告（-）。目标是研究的具体重点，例如特定类型的语言模型。无关表示研究是否是模型无关的。目标代表每项研究的主要目的：模型比较（C）、解释（E）、改进（IMP）、可解释性（INT）和推理（R）。

#### 4.1.1 解释

大多数论文，即 35 篇中的 17 篇，符合这个子类别，大多数论文都关注于需要更具可解释性和透明度的 LLM。

例如，Vig ([2019](https://arxiv.org/html/2407.15248v1#bib.bib32)) 引入了一种可视化工具，用于理解像 BERT 和 GPT-2 这样的 Transformer 模型中的注意力机制。该工具在多个尺度上提供洞察，从单个神经元到整个模型层，帮助检测模型偏差，定位相关注意力头，并将神经元与模型行为联系起来。

Swamy 等人 ([2021](https://arxiv.org/html/2407.15248v1#bib.bib33)) 提出了通过从 BERT 基语言模型的不同训练阶段提取知识图谱来解释这些模型的知识获取和语言能力的方法。知识图谱常用于可解释的外推推理 Lin 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib57))。

吴等人 ([2021](https://arxiv.org/html/2407.15248v1#bib.bib34)) 提出了 Polyjuice，一个通用的反事实生成器。这个工具通过在多个数据集上微调 GPT-2 来生成多样且真实的反事实，从而实现对类型和位置的可控扰动。

王等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib35)) 研究了 GPT-2 small 的机制可解释性，特别是它识别句子中间接对象的能力。该研究涉及模型计算图的电路分析和逆向工程，识别特定的注意力头及其在这一任务中的角色。

Menon 和 Vondrick ([2022](https://arxiv.org/html/2407.15248v1#bib.bib36)) 介绍了一种使用 LLM 生成的描述进行视觉分类的新方法。他们称之为“描述分类”，该方法利用像 GPT-3 这样的 LLM 生成视觉类别的描述特征。然后，这些特征被用来更准确地分类图像，同时提供比仅依赖类别名称的传统方法更透明的结果。

高等人 ([2023a](https://arxiv.org/html/2407.15248v1#bib.bib37)) 检查了 ChatGPT 在因果推理中的能力，使用任务如事件因果关系识别 (ECI)、因果发现 (CD) 和因果解释生成 (CEG)。作者声称虽然 ChatGPT 在因果解释方面有效，但在因果推理方面表现欠佳，且常常出现因果幻觉。研究还探讨了上下文学习 (ICL) 和思维链 (CoT) 技术的影响，得出结论：ChatGPT 的因果推理能力对提示的结构和措辞非常敏感。

Pan 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib38)) 提出了一个框架，旨在通过从知识图谱 (KGs) 获取明确的结构化知识来增强 LLMs，解决如幻觉和缺乏可解释性的问题。该论文概述了三种主要方法：KG 增强的 LLMs、LLM 扩展的 KGs 和与 KGs 协同的 LLMs。这种统一提升了 AI 系统在各种应用中的性能和可解释性。

Conmy 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib39)) 专注于神经网络中机制可解释性工作流的自动化。作者使用如自动电路发现 (ACDC) 等算法，自动识别神经模型中与特定行为或功能对应的子图。

He 等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib40)) 提出了一个新颖的 LLM 后处理方法，该方法利用外部知识来增强解释的可信度和整体性能。该方法称为“重新思考与检索”（Rethinking with Retrieval），使用 CoT 提示生成经过相关外部知识优化的推理路径。作者声称，他们的方法通过生成更准确和可靠的解释，显著提高了 LLM 在复杂推理任务中的性能。

Yoran 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib41)) 提出的多链推理（MCR）通过促使语言模型在多个推理链上进行元推理，从而改进了问答性能。这种方法有助于选择相关事实、混合不同链的信息，并生成更好的答案解释。论文展示了 MCR 在多跳问答中的优越性能，相较于之前的方法。

Inseq Sarti 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib42)) 是一个 Python 库，用于促进序列生成模型的可解释性分析。该工具包专注于提取模型内部信息和特征重要性评分，特别是针对变换器架构。它集中访问各种特征归因方法，能够通过热图等可视化方式直观展示，如 Aminimehr 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib58)) 提出的，从而促进对序列生成模型的公平和可重复评估。

Wu 等人 ([2023b](https://arxiv.org/html/2407.15248v1#bib.bib43)) 提出的无界分布对齐搜索（Boundless DAS）是一种用于识别大型语言模型中可解释因果结构的方法。在他们的论文中，作者展示了 Alpaca 模型（一个 7B 参数的语言模型）通过实现简单的算法与可解释的布尔变量来解决数值推理问题。

Li 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib44)) 研究了各种示例如何影响语言模型中的 ICL，通过探索对比输入-标签示例对的影响，包括标签翻转、输入扰动和添加补充解释。该研究采用显著性图来定性和定量分析这些示例如何影响语言模型的预测。

LMExplainer 陈等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib45)) 是一种用于解释语言模型决策过程的方法。这种方法结合了知识图谱和图注意力神经网络，以解释语言模型预测背后的推理过程。

Gao 等人 ([2023b](https://arxiv.org/html/2407.15248v1#bib.bib46)) 提出了一个新颖的推荐系统框架 Chat-REC，该框架整合了 LLM 以生成更具互动性和可解释性的推荐。该系统将用户档案和互动历史转化为 LLM 的提示，通过 LLM 的 ICL 能力来增强推荐过程。

Zhang 等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib47)) 提出的 DSR-LM 是一个将可微分符号推理与预训练语言模型相结合的框架。作者声称，他们的框架通过一个执行演绎推理的符号模块来提高语言模型的逻辑推理能力，从而在演绎推理任务上提升了准确性。

#### 4.1.2 作为特征

该子类别中的论文并不直接旨在提供更透明的模型或解释基于 LLM 的模型。相反，它们使用 LLM 生成推理和描述，并将这些生成的内容作为输入用于二次任务。

例如，Li 等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib49)) 探讨了 LLM 的解释如何增强较小语言模型（SLM）的推理能力。他们提出了一种多任务学习框架，在该框架下，SLM 通过 LLM 的解释进行训练，从而提高了推理任务的表现。

Ye 和 Durrett ([2022](https://arxiv.org/html/2407.15248v1#bib.bib50)) 评估了 LLM 在少样本学习场景中生成的解释的可靠性。作者声称，LLM 的解释通常不会显著提高学习表现，并且可能在事实准确性上存在问题，突显了 LLM 推理与解释中的事实准确性之间的潜在不一致。

Turpin 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib51)) 研究了 CoT 推理的可靠性。作者声称，尽管 CoT 可以提高任务表现，但它也可能系统性地误代表模型预测的真实原因。他们通过实验演示了如何通过模型输入中的偏置特征（如重新排序多项选择选项）在 CoT 解释中产生重大影响，而这些影响在解释本身中未被承认。

Kang 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib52)) 介绍了一种称为自动化科学调试（AutoSD）的自动调试过程的方法。这种方法利用 LLM 生成有关代码中错误的假设，并使用调试器与有缺陷的代码进行互动。该方法导致了自动结论和补丁生成，并为调试决策提供了清晰的解释，可能使开发者做出更高效、更准确的决策。

Krishna 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib53)) 提出了一个框架，称为通过利用上下文学习与事后解释放大模型性能（AMPLIFY），旨在通过自动生成推理来提升 LLMs 在复杂推理和语言理解任务中的表现。该框架利用事后解释方法，输出指示每个输入特征对模型预测影响的归因分数，以构建自然语言推理。这些推理为 LLMs 提供纠正信号。

Yang 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib54)) 介绍了语言指导瓶颈（LaBo），一种构建高性能概念瓶颈模型（CBMs）的方法，无需手动指定概念。LaBo 利用 GPT-3 生成关于类别的事实性句子，从中形成 CBMs 的候选概念。这些概念随后使用 CLIP Radford 等人 ([2021](https://arxiv.org/html/2407.15248v1#bib.bib59)) 与图像对齐，形成瓶颈层。该方法利用子模块效用高效地搜索瓶颈，重点关注区分性和多样化的信息。作者声称他们的方法在 11 个不同的数据集上，在少样本分类任务中优于黑箱线性探测器，显示出在数据更多时具有可比或更好的性能。

Bitton-Guetta 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib55)) 介绍了 WHOOPS!，一个新的数据集和基准，用于测试 AI 模型的视觉常识推理能力。该数据集包括有意违背常识的图像，使用如 Midjourney 等图像生成工具创建。论文评估了 AI 模型在图像描述、跨模态匹配、视觉问答以及生成解释这一具有挑战性的任务中的表现，在该任务中，模型必须识别并解释图像的异常性。结果显示，即使是像 GPT3 和 BLIP2 这样的先进模型也在这些任务中表现不佳，突显了 AI 在视觉常识推理方面与人类表现之间的差距。

### 4.2 讨论论文

与应用论文不同，本类别包括那些通过大语言模型（LLMs）探讨 XAI（可解释人工智能）问题及其相互关系的论文，但不一定提供具体的方法论、框架或应用。本类别进一步分为两个子类别：问题（Issues），即提及关切问题的工作；基准和指标（Benchmark and Metrics），主要关注评估和考核 LLM 领域 XAI 方法的工作。

#### 4.2.1 问题

Bowman ([2023](https://arxiv.org/html/2407.15248v1#bib.bib60)) 批判性地审视了 LLMs，突显了它们的不可预测性以及其能力随着规模扩展而出现的特性。他们强调了引导和解释 LLMs 的挑战以及对其局限性和潜力的细致理解的必要性。

Liu 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib61)) 提供了一项关于评估大型语言模型（LLMs）与人类价值观和意图对齐的调查和指南。他们对 LLM 的可信度进行了分类和详细说明，包括可靠性、安全性、公平性、抗滥用性、可解释性、遵守社会规范和鲁棒性。

Liao 和 Vaughan ([2023](https://arxiv.org/html/2407.15248v1#bib.bib62)) 强调了从以人为本的视角看待 LLM 透明性的必要性。作者讨论了实现 LLM 透明性面临的独特挑战，将其与较小、更专业的模型区分开来。该论文提出了一条研究路线图，强调了理解和解决 LLM 生态系统中各种利益相关者的透明性需求的重要性。它倡导开发和设计考虑这些利益相关者需求、新型 LLM 应用及其各种使用模式和相关挑战的透明性方法。

最后，Xie 等人 ([2023](https://arxiv.org/html/2407.15248v1#bib.bib63)) 强调了 ChatGPT 在金融市场分析中的可解释性和稳定性方面的局限性，采用零-shot 分析。作者建议需要更多专门的训练或微调。

#### 4.2.2 基准测试与指标

Lu 等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib64)) 介绍了 SCIENCEQA，这是一个用于多模态科学问答的新数据集。该数据集包含约 21k 个具有多样科学主题和注释的问题，涵盖讲座和解释以帮助理解推理过程。作者展示了如何训练语言模型，特别是大型语言模型（LLMs），以生成这些讲座和解释作为 CoT 过程的一部分，从而提升其推理能力。研究表明 CoT 改善了问答表现，并提供了 LLMs 在复杂多模态领域模仿类人多步推理的潜力的见解。

Golovneva 等人 ([2022](https://arxiv.org/html/2407.15248v1#bib.bib65)) 介绍了 ROSCOE，这是一套用于评估语言模型逐步推理的指标，特别是在没有黄金参考的情况下。该工作包括推理错误的分类法以及对 ROSCOE 在各种推理任务中与基线指标的全面评估。作者展示了 ROSCOE 在评估模型生成的推理中的语义一致性、逻辑性、信息量、流畅性和事实性方面的有效性。

赵等人（[2023b](https://arxiv.org/html/2407.15248v1#bib.bib66)）提供了一项关于 LLMs 可解释性技术的全面调查，重点关注基于 Transformer 的模型。它根据传统的微调和提示范式对这些技术进行分类，详细描述了生成局部和全局解释的方法。论文讨论了可解释性面临的挑战和未来研究的潜在方向，突出了 LLMs 相较于传统深度学习模型的独特复杂性和能力。然而，调查主要集中于一般的 XAI，对 XAI 与 LLMs 之间关系的覆盖较少。

## 5 讨论

我们的分析表明，被审阅的文献中，只有有限的几篇直接解决了第[2](https://arxiv.org/html/2407.15248v1#S2 "2 The Need for Explanations in LLMs ‣ XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models")节中突出的问题。例如，刘等人（[2023](https://arxiv.org/html/2407.15248v1#bib.bib61)）的研究集中于 LLMs 中的信任相关问题，而高等人（[2023a](https://arxiv.org/html/2407.15248v1#bib.bib37)）则探讨了 LLMs 引发的信息传播问题。这些识别出的问题关注不足表明，XAI 社区需要有实质性的参与，以充分应对这些问题。

##### 开源参与

我们的调查研究显示，越来越多的研究超越了仅仅描述文本中的方法论的传统方式。相反，他们将这些方法发布为可触及的工具或开源代码，通常托管在如 GitHub 等平台上。这一进展是提高计算机科学研究透明度和可重复性的一个值得称赞的步骤。趋势表明，作者们越来越倾向于发布他们的代码和公开他们的工具，这与几年前的情况有显著变化。然而，我们也应提到社区参与这些资源的程度不一致。虽然一些代码库吸引了大量兴趣，促进了进一步的发展和改进，但其他一些则被少用。这种参与差异提出了有关影响社区与这些资源互动因素的重要问题。

##### 目标

大多数研究主要关注于大型语言模型（LLMs），而不是集中于 AI 系统中更为专业或狭窄的主题。这种广泛的方法与相对较少关注特定类别系统（如推荐系统）或专门研究变换器（Transformers）的研究形成了对比。对 LLMs 的广泛关注在 AI 社区中代表了一种积极而有影响的趋势。鉴于 LLM 系统在学术和实际应用中的快速发展和日益重要，这种更广泛的关注是及时且至关重要的，有助于推动我们对这一领域的理解和能力的提升。它确保了研究跟上领域内的进展，促进了一种全面且前瞻性的方法，这是 AI 技术持续增长和发展的关键。

##### 目标

我们的分析，如表[1](https://arxiv.org/html/2407.15248v1#S4.T1 "Table 1 ‣ 4.1 Application Papers ‣ 4 Retrieval Results ‣ XAI meets LLMs: A Survey of the Relation between Explainable AI and Large Language Models")所示，揭示了 LLM 研究目标的分歧。一方面，这些研究中的一部分主要致力于解释和增强这些“黑箱”模型的可解释性。另一方面，更大的一部分则更注重任务，专注于增强特定任务和模型，而可解释性则只是附带的成果。这种研究重点的二分法突出了一个关键趋势：迫切需要将更多的注意力转向揭示 LLMs 的内部机制。与其仅仅利用这些模型来提高任务性能，不应忽视它们固有的不透明性。追求性能改进必须与揭示和澄清 LLMs 的基本机制的努力相平衡。这种方法对于深入理解这些复杂系统至关重要，以确保其应用的有效性和透明性。这样一种平衡的关注对于技术领域的进步以及保持伦理和负责任的 AI 发展是必不可少的。

## 6 结论

我们的短信显示，只有少数几项工作致力于为基于大语言模型（LLM）系统开发解释方法。考虑到 LLM 在各种应用中的迅速崛起，这一发现尤为突出。因此，在这种背景下，我们的研究具有双重目的。首先，它作为 XAI（可解释人工智能）社区的导航灯塔，突显了在创建可解释和透明的 LLM 系统方面的肥沃领域，这些领域能够有效地应对更广泛的 AI 社区面临的挑战。其次，它是一个行动号召，敦促研究人员和从业者进入这一相对未被充分探索的领域。LLM 系统中解释方法的需求不仅仅是技术上的必要性，更是负责任的 AI 实践的一步。通过关注这一领域，XAI 社区可以在使 AI 系统更高效、值得信赖和负责任方面做出重要贡献。

我们的行动号召如下：首先，使用 LLM 模型的研究人员必须承认并解决这些系统的不透明性所带来的潜在长期挑战。解释性的重要性应从一个“可有可无”的特性提升为开发过程中的一个不可或缺的方面。这需要在 LLM 系统的设计和实施阶段采取积极主动的方法来融入解释性。这种视角的转变对于确保这些模型有效、透明和负责任至关重要。其次，我们敦促 XAI 领域的研究人员扩大他们的调查范围。重点不仅应放在制定能够处理 LLM 系统复杂性的理论方法上，还应在于增强这些解释的展示层。目前，提供的解释通常对非技术性利益相关者来说过于复杂。因此，开发能够使这些解释更易于理解和接受的方式是必要的。这种双重方法将使 LLM 更易于理解和用户友好，并弥合技术效率与 AI 开发中的伦理责任之间的差距。

## 参考文献

+   Zhao et al. [2023a] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 大语言模型综述。*arXiv:2303.18223*，2023a。

+   Amin et al. [2023] Mostafa Amin, Erik Cambria, 和 Björn Schuller. ChatGPT 的回应能否提升传统自然语言处理？*IEEE Intelligent Systems*，38(5):5–11，2023。

+   Thirunavukarasu et al. [2023] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, 和 Daniel Shu Wei Ting. 医学中的大语言模型。*Nature medicine*，第 1–11 页，2023。

+   Wu et al. [2023a] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, 和 Gideon Mann. BloombergGPT：一种用于金融的大型语言模型。*arXiv:2303.17564*，2023a 年。

+   Kaadoud et al. [2021] Ikram Chraibi Kaadoud, Lina Fahed, 和 Philippe Lenca. 可解释的人工智能：知识发现、知识表示和表示学习交叉点的叙述性综述。见于 *MRC*，第 2995 卷，第 28–40 页。ceur-ws.org，2021 年。

+   Cambria et al. [2023a] Erik Cambria, Rui Mao, Melvin Chen, Zhaoxia Wang, 和 Seng-Beng Ho. 人工智能未来的七大支柱。*IEEE 智能系统*，38(6):62–69，2023a 年。

+   Weidinger et al. [2021] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, 等等。语言模型的伦理和社会风险。*arXiv:2112.04359*，2021 年。

+   Liu [2023] Yang Liu. 人工标注数据在大型语言模型时代的重要性。见于 *第三十二届国际人工智能联合会议论文集*，第 7026–7032 页，2023 年。

+   Sevastjanova 和 El-Assady [2022] Rita Sevastjanova 和 Mennatallah El-Assady. 小心合理化陷阱！当语言模型的可解释性与我们对语言的心理模型偏离时，2022 年。

+   Cambria et al. [2023b] Erik Cambria, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, 和 Navid Nobani. 关于 XAI 和自然语言解释的综述。*信息处理与管理*，60(1):103111，2023b 年。

+   Burkart 和 Huber [2021] Nadia Burkart 和 Marco F Huber. 关于监督机器学习可解释性的综述。*人工智能研究杂志*，70:245–317，2021 年。

+   Mercorio et al. [2020] Fabio Mercorio, Mario Mezzanzanica, 和 Andrea Seveso. exdil：一种用于分类和解释医院出院信的工具。见于 *国际跨领域机器学习与知识提取会议*，第 159–172 页。Springer，2020 年。

+   Gozzi et al. [2022] Noemi Gozzi, Lorenzo Malandri, Fabio Mercorio, 和 Alessandra Pedrocchi. 用于肌电控制假肢的 XAI：解释用于手势分类的 EMG 数据。*基于知识的系统*，240:108053，2022 年。

+   Alimonda et al. [2022] Nicola Alimonda, Luca Guidotto, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, 和 Giovanni Tosi. 关于医学中的网络物理系统的 XAI 综述。见于 *2022 IEEE 国际计量会议：扩展现实、人工智能和神经工程（MetroXRAINE）*，第 265–270 页。IEEE，2022 年。

+   Xing et al. [2020] Frank Xing, Lorenzo Malandri, Yue Zhang, 和 Erik Cambria. 财务情感分析：对常见错误和解决办法的调查。见于 *第 28 届国际计算语言学会议论文集*，第 978–987 页，2020 年。

+   Castelnovo 等人 [2023] Alessandro Castelnovo, Nicole Inverardi, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica 和 Andrea Seveso。《利用群体对比解释处理公平性问题》。发表于 *World Conference on Explainable Artificial Intelligence*，第 332–345 页。Springer，2023 年。

+   Yeo 等人 [2023] Wei Jie Yeo, Wihan van der Heever, Rui Mao, Erik Cambria, Ranjan Satapathy 和 Gianmarco Mengaldo。《金融领域可解释人工智能的全面综述》。*arXiv preprint arXiv:2309.11960*，2023 年。

+   Novelli 等人 [2024] Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato 和 Luciano Floridi。《欧盟法律中的生成性人工智能：责任、隐私、知识产权和网络安全》。*EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity (2024 年 1 月 14 日)*，2024 年。

+   Malandri 等人 [2022a] Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani 和 Andrea Seveso。《ContrXT：从任何文本分类器生成对比解释》。*Inf. Fusion*，81:103–115，2022a。[10.1016/j.inffus.2021.11.016](https://doi.org/10.1016/j.inffus.2021.11.016)。URL [`doi.org/10.1016/j.inffus.2021.11.016`](https://doi.org/10.1016/j.inffus.2021.11.016)。

+   Malandri 等人 [2024] Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica 和 Andrea Seveso。《通过符号推理进行模型对比解释》。*Decision Support Systems*，176:114040，2024 年。

+   Malandri 等人 [2022b] Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani 和 Andrea Seveso。《作为服务的文本分类器对比解释》。发表于 *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations*，第 46–53 页，2022b 年。

+   Malandri 等人 [2022c] Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, Navid Nobani, Andrea Seveso 等人。《好、坏与解释者：一种对比性解释文本分类器的工具》。发表于 *IJCAI*，第 5936–5939 页。AAAI 出版社，2022c。

+   Shen 等人 [2023] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen 和 Yang Zhang。《“现在就做任何事”：大语言模型的现实世界越狱提示的特征和评估》。*arXiv:2308.03825*，2023 年。

+   Abd-Alrazaq 等人 [2023] Alaa Abd-Alrazaq, Rawan AlSaad, Dari Alhuwail, Arfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah Aziz, Rafat Damseh, Sadam Alabed Alrazak, Javaid Sheikh 等人。《医学教育中的大语言模型：机遇、挑战和未来方向》。*JMIR Medical Education*，9(1):e48291，2023 年。

+   Yan 等人 [2023] Lixiang Yan, Lele Sha, Linxuan Zhao, Yuheng Li, Roberto Martinez-Maldonado, Guanliang Chen, Xinyu Li, Yueqiao Jin 和 Dragan Gašević。《大语言模型在教育中的实际和伦理挑战：系统范围的综述》。*British Journal of Educational Technology*，2023 年。

+   Salimi and Saheb [2023] Ali Salimi 和 Hady Saheb. 大型语言模型在眼科科学写作中的应用：伦理考量模糊不清？*American Journal of Ophthalmology*，2023 年。

+   Rawte et al. [2023] Vipula Rawte, Amit Sheth, 和 Amitava Das. 大型基础模型中的幻觉调查，2023 年。

+   Azaria and Mitchell [2023] Amos Azaria 和 Tom Mitchell. 一个 LLM 的内部状态知道何时撒谎。*arXiv:2304.13734*，2023 年。

+   Barn et al. [2017] Balbir Barn, Souvik Barat, 和 Tony Clark. 进行系统文献综述和系统映射研究。在*Innovations in Software Engineering Conference*，第 212–213 页，2017 年。

+   Martínez-Gárate et al. [2023] Ángel Antonio Martínez-Gárate, José Alfonso Aguilar-Calderón, Carolina Tripp-Barba, 和 Aníbal Zaldívar-Colado. 面向对话代理开发的模型驱动方法：系统映射研究。*IEEE Access*，2023 年。

+   Oikonomidi et al. [2020] Theodora Oikonomidi, Isabelle Boutron, Olivier Pierre, Guillaume Cabanac, Philippe Ravaud, 和 Covid-19 Nma Consortium. 预印本中评估 COVID-19 介入研究的证据变化：元研究。*BMC medicine*，18:1–10，2020 年。

+   Vig [2019] Jesse Vig. 变压器模型中注意力的多尺度可视化。在*Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations*，第 37–42 页，2019 年。

+   Swamy et al. [2021] Vinitra Swamy, Angelika Romanou, 和 Martin Jaggi. 通过知识图谱提取来解释语言模型。发表于*NeurIPS*，2021 年。

+   Wu et al. [2021] T Wu, M Tulio Ribeiro, J Heer, 和 D Weld. Polyjuice：生成反事实以解释、评估和改进模型。在*ACL-IJCNLP*，2021 年。

+   Wang et al. [2022] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, 和 Jacob Steinhardt. 野外中的可解释性：GPT-2 small 中间接对象识别的电路。在*NeurIPS ML Safety Workshop*，2022 年。

+   Menon and Vondrick [2022] Sachit Menon 和 Carl Vondrick. 通过大型语言模型描述进行视觉分类。在*The Eleventh International Conference on Learning Representations*，2022 年。

+   Gao et al. [2023a] Jinglong Gao, Xiao Ding, Bing Qin, 和 Ting Liu. ChatGPT 是一个好的因果推理工具吗？全面评估。*arXiv:2305.07375*，2023a 年。

+   Pan et al. [2023] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, 和 Xindong Wu. 统一大型语言模型和知识图谱：路线图。*arXiv:2306.08302*，2023 年。

+   Conmy et al. [2023] Arthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, 和 Adrià Garriga-Alonso. 朝着机械解释性的自动电路发现。*arXiv:2304.14997*，2023 年。

+   He et al. [2022] Hangfeng He, Hongming Zhang, 和 Dan Roth. 重新思考检索：忠实的大型语言模型推理。*arXiv:2301.00303*，2022 年。

+   Yoran 等人[2023] Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch 和 Jonathan Berant. 通过对多个思维链进行元推理来回答问题。*arXiv:2304.13007*，2023。

+   Sarti 等人[2023] Gabriele Sarti, Nils Feldhus, Ludwig Sickert 和 Oskar van der Wal. Inseq：用于序列生成模型的可解释性工具包。*arXiv:2302.13942*，2023。

+   Wu 等人[2023b] Zhengxuan Wu, Atticus Geiger, Christopher Potts 和 Noah D Goodman. 大规模的可解释性：在 Alpaca 中识别因果机制。*arXiv:2305.08809*，2023b。

+   Li 等人[2023] Zongxia Li, Paiheng Xu, Fuxiao Liu 和 Hyemi Song. 通过对比示例和显著性图理解上下文学习。*arXiv:2307.05052*，2023。

+   Chen 等人[2023] Zichen Chen, Ambuj K Singh 和 Misha Sra. Lmexplainer：一个增强知识的语言模型解释器。*arXiv:2303.16537*，2023。

+   Gao 等人[2023b] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang 和 Jiawei Zhang. Chat-rec：朝着互动和可解释的 LLMs 增强推荐系统迈进。*arXiv:2303.14524*，2023b。

+   Zhang 等人[2022] Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik 和 Eric Xing. 通过可微分符号编程提高语言模型的逻辑推理能力。在*ICML 2022 的首届预训练研讨会：观点、陷阱和前进路径*，2022。

+   Singh 等人[2023] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu 和 Jianfeng Gao. 使用语言模型以自然语言解释黑箱文本模块。*arXiv:2305.09863*，2023。

+   Li 等人[2022] Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao 等人. 大型语言模型的解释使小型推理器更好。*arXiv:2210.06726*，2022。

+   Ye 和 Durrett[2022] Xi Ye 和 Greg Durrett. 少量示例提示中文本推理中的解释不可靠性。*NeurIPS*，35:30378–30392，2022。

+   Turpin 等人[2023] Miles Turpin, Julian Michael, Ethan Perez 和 Samuel R Bowman. 语言模型并不总是表达他们的想法：链式思维提示中的不忠实解释。*arXiv:2305.04388*，2023。

+   Kang 等人[2023] Sungmin Kang, Bei Chen, Shin Yoo 和 Jian-Guang Lou. 通过大型语言模型驱动的科学调试实现可解释的自动化调试。*arXiv:2304.02195*，2023。

+   Krishna 等人[2023] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh 和 Himabindu Lakkaraju. 后验解释可以改善语言模型。*arXiv:2305.11426*，2023。

+   Yang 等人[2023] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch 和 Mark Yatskar. 瓶中的语言：用于可解释图像分类的语言模型引导的概念瓶颈。在*IEEE/CVF 计算机视觉与模式识别会议论文集*，第 19187–19197 页，2023。

+   Bitton-Guetta 等人 [2023] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky 和 Roy Schwartz。打破常识：哎呀！一个合成和组合图像的视觉与语言基准。发表于*IEEE/CVF 国际计算机视觉会议论文集*，第 2616–2627 页，2023。

+   Shi 等人 [2023] Yucheng Shi, Hehuan Ma, Wenliang Zhong, Gengchen Mai, Xiang Li, Tianming Liu 和 Junzhou Huang。Chatgraph：通过将 ChatGPT 知识转换为图形实现可解释的文本分类。*arXiv:2305.03513*，2023。

+   Lin 等人 [2023] Qika Lin, Jun Liu, Rui Mao, Fangzhi Xu 和 Erik Cambria。Techs：用于可解释推断推理的时间逻辑图网络。发表于*第 61 届计算语言学协会年会论文集（第 1 卷：长篇论文）*，第 1281–1293 页，2023。

+   Aminimehr 等人 [2023] Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini 和 Erik Cambria。Tbexplain：一种基于文本的场景分类模型解释方法，结合统计预测校正。*arXiv 预印本 arXiv:2307.10003*，2023。

+   Radford 等人 [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark 等。 从自然语言监督中学习可转移的视觉模型。发表于*国际机器学习会议*，第 8748–8763 页。PMLR, 2021。

+   Bowman [2023] Samuel R Bowman。关于大语言模型的八件事。*arXiv:2304.00612*，2023。

+   Liu 等人 [2023] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq 和 Hang Li。值得信赖的大语言模型：评估大语言模型对齐的调查和指南。发表于*社会责任语言建模研究*，2023。

+   Liao 和 Vaughan [2023] Q Vera Liao 和 Jennifer Wortman Vaughan。大语言模型时代的人工智能透明性：以人为本的研究路线图。*arXiv:2306.01941*，2023。

+   Xie 等人 [2023] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng 和 Jimin Huang。华尔街新手：对多模态股票走势预测挑战的零样本分析。*arXiv:2304.05351*，2023。

+   Lu 等人 [2022] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark 和 Ashwin Kalyan。学会解释：通过思维链进行科学问题解答的多模态推理。*NeurIPS*，35:2507–2521，2022。

+   Golovneva 等人 [2022] Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi 和 Asli Celikyilmaz。Roscoe：一步步推理评分的度量套件。发表于*第十一届国际学习表征会议*，2022。

+   Zhao 等人 [2023b]  纪实了赵海燕、陈涵洁、杨帆、刘宁浩、邓辉琪、蔡恒益、王帅强、尹大伟和杜孟南。对大语言模型的可解释性：一项调查。*ACM TIST*，2023b。

生成于 2024 年 7 月 21 日 星期日 19:21:34，来自 LaTeXML![吉祥物 Sammy](http://dlmf.nist.gov/LaTeXML/)
