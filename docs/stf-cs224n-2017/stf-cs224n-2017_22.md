# CS224n 研究热点 4 词嵌入对传统方法的启发

![hankcs.com 2017-06-15 下午 9.15.54.png](img/827529c28997911580fea6263b2b1dad.jpg "hankcs.com 2017-06-15 下午 9.15.54.png")

主讲人是一位发音特别纯正的印度小哥，只有微量口音。这篇论文挑战了对神经网络的迷信，展示了传统模型的生命力以及调参的重要性。

## 词语表示方法

以前的课程中讲过两大类得到 dense 词语表示的方法，一般认为 NN 模型更好：

![hankcs.com 2017-06-15 下午 9.17.09.png](img/355bf8a76e3bdbebc89b849c90280828.jpg "hankcs.com 2017-06-15 下午 9.17.09.png")

这里的 PPMI 也是一种利用共现矩阵的方法。

但 Levy 指出，超参数和实现细节比算法本身更重要：

![hankcs.com 2017-06-15 下午 9.18.33.png](img/29bf09b51f9a51f8d41a6e18e5dac297.jpg "hankcs.com 2017-06-15 下午 9.18.33.png")

## Skip-Gram 中的超参数

有负例的采样个数，负采样算法中的平滑指数：

![hankcs.com 2017-06-15 下午 9.20.31.png](img/3f7235c33de2da1ebcb9d19118be091c.jpg "hankcs.com 2017-06-15 下午 9.20.31.png")

## 对 PMI 的启发

PMI 中也有个类似负采样中的平滑指数的超参数：

![hankcs.com 2017-06-15 下午 9.28.49.png](img/eb0b18dd10a6761e46571b76d2b1eb5a.jpg "hankcs.com 2017-06-15 下午 9.28.49.png")

试验证明，取![](img/2532d3de80a469a126b5c945f7e8a0dd.jpg)恰好能得到最好的效果。

另外在一种叫 Shifted PMI 的变种中，也有类似于负采样个数的超参数：

![hankcs.com 2017-06-15 下午 9.30.07.png](img/1ef8d4e03efff379730d60116eda78b1.jpg "hankcs.com 2017-06-15 下午 9.30.07.png")

## 可调超参数一览表

这些方法中，存在大量的超参数可供折腾：

![hankcs.com 2017-06-15 下午 9.31.59.png](img/f70c52ac3e0a092322f1030dc9dea039.jpg "hankcs.com 2017-06-15 下午 9.31.59.png")

## 调参结果

将每种方法能调的超参数调到最佳，得到如下结果：

![hankcs.com 2017-06-15 下午 9.32.23.png](img/fc06016e3fe5faf4a137e83a24d16c6c.jpg "hankcs.com 2017-06-15 下午 9.32.23.png")

结果表示，没有性能稳定的方法，时而是 count-based 方法胜出，时而是 NN 方法胜出。

## 结论

*   这篇文章挑战了人们对 NN 模型的迷信，展示了 NN 模型并不一定比传统模型好。

*   虽然模型设计很重要，要想拿到好的效果，调参也非常非常重要。

*   不要迷信，要勇于挑战流行的论点（小哥认为词的向量表示领域还有很大的探索空间）。

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 研究热点 4 词嵌入对传统方法的启发](http://www.hankcs.com/nlp/cs224n-improve-word-embeddings.html)