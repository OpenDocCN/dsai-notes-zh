# CS224n 笔记 5 反向传播与项目指导

![hankcs.com 2017-06-10 下午 8.26.24.png](img/b75e134d6ffe5a51275d0058c9448126.jpg "hankcs.com 2017-06-10 下午 8.26.24.png")![hankcs.com 2017-06-10 下午 8.52.07.png](img/0d22b60d41796d903974474821084621.jpg "hankcs.com 2017-06-10 下午 8.52.07.png")最后一次数学课，其实都是些很基础的推导而已。从四个不同的层面讲解反向传播，其中电路和有向图类比还是很直观新颖的解释。

## 任意层的通用公式

第![](img/4c55f62a52ee5572ab96494e9e0a2876.jpg)层的残差：

![](img/2b7f582590fbaf43ef84acc0d477f070.jpg)

其中![](img/dad139946fdd1363ecd86d20700a53a6.jpg)是激活函数：

![](img/a56abf8040e4cbe268103f4659ecdc1c.jpg)

对于顶层来讲，残差就是根据某个损失函数得到的误差。对于底层来讲，激活函数不存在，或相当于![](img/3a4a4c7132c32db555266aad36ee1865.jpg)。

![](img/d132b400654f0a1c0bf2cf921b391c8a.jpg)是线性函数：

![](img/6cd1321722d47a891f8ec9ecfd53770d.jpg)

而![](img/fe81115fbd068e43bcad3df1d839178f.jpg)是相同大小的向量之间的 element wise product(![](img/7e9c56e00b226a8fb1d3f0ca0a9c8836.jpg))

正则化的损失函数关于第![](img/4c55f62a52ee5572ab96494e9e0a2876.jpg)层的权值矩阵的梯度：

![](img/e51daca47abf4e7eaf0017e8dc574d6f.jpg)

关于第![](img/4c55f62a52ee5572ab96494e9e0a2876.jpg)层的偏置的梯度：

![](img/8ce234faf86d1af21bb38cb79dc91cac.jpg)

其中，![](img/070b1af5eca3a5c5d72884b536090f17.jpg)是激活值：

![](img/2826ad4cb70a1f416e6b761fff59429b.jpg)

这里偏置单元与普通神经元在数学上并无不同，只不过由于激活值![](img/1e15e127279b7c6298f2539ec047deba.jpg)，所以可以把激活值省略掉。

## 反向传播的电路解释

比如函数![](img/1111709f2a67446ac337dd5dacb80433.jpg)可视作如下加法器和乘法器电路：

![hankcs.com 2017-06-10 下午 4.35.30.png](img/c42b176dc6a47bc0dfcda404cb36a475.jpg "hankcs.com 2017-06-10 下午 4.35.30.png")

定义![](img/48e99f881177f3863636375ac6b4241d.jpg) 、 ![](img/d7d0b1657305cf06afd9103e5737f290.jpg)，于是有![](img/ae4a4e6963e1b39f69d441076906d87c.jpg) 和![](img/b5bcbfedbeaee28e52f393ae6cdfb593.jpg)。求![](img/72fceedc0b8b1d7004f615a73980238a.jpg)

我们可以从输出到输入反向计算，先得到输出关于输出自己的导数：

![hankcs.com 2017-06-10 下午 4.41.14.png](img/ef687f1608984ee678ea2a9047f4d538.jpg "hankcs.com 2017-06-10 下午 4.41.14.png")

然后得到![](img/dad139946fdd1363ecd86d20700a53a6.jpg)关于![](img/d132b400654f0a1c0bf2cf921b391c8a.jpg)的导数：

![hankcs.com 2017-06-10 下午 7.39.38.png](img/971de01ddabe3c2065794c83d9d31dc8.jpg "hankcs.com 2017-06-10 下午 7.39.38.png")

另一条路，![](img/dad139946fdd1363ecd86d20700a53a6.jpg)关于![](img/807ceeb0b2cebef0db0b731ff4d59f51.jpg)的导数：

![hankcs.com 2017-06-10 下午 7.41.50.png](img/786409af6dad9c0447f85d0361757f3b.jpg "hankcs.com 2017-06-10 下午 7.41.50.png")

![](img/dad139946fdd1363ecd86d20700a53a6.jpg)关于![](img/c592009395c2de830215c39f7bb6f97b.jpg)的导数：

![hankcs.com 2017-06-10 下午 7.43.21.png](img/3c74bd337c0de9f51bdd1e12314f5e99.jpg "hankcs.com 2017-06-10 下午 7.43.21.png")

![](img/dad139946fdd1363ecd86d20700a53a6.jpg)关于![](img/40779fc60a53ff2b70f832ec10cade09.jpg)的导数：

![hankcs.com 2017-06-10 下午 7.43.46.png](img/aef4932599804ab6da46c9f44fca6a14.jpg "hankcs.com 2017-06-10 下午 7.43.46.png")

这种反向回溯的过程放到神经元中就是反向传播了：

![hankcs.com 2017-06-10 下午 8.26.24.png](img/b75e134d6ffe5a51275d0058c9448126.jpg "hankcs.com 2017-06-10 下午 8.26.24.png")

反向传播时每通过一级，就用链式法则乘以这一级的导数。

另一个稍微复杂一点的例子：

![hankcs.com 2017-06-10 下午 8.33.36.png](img/8603bc1731dac8236a9474e524a9d2b1.jpg "hankcs.com 2017-06-10 下午 8.33.36.png")

其中，sigmoid 相关的元件可以合并为一个 sigmoid gate：

![hankcs.com 2017-06-10 下午 8.35.43.png](img/902dba37982f1d2cc2530e8ca2ad7a45.jpg "hankcs.com 2017-06-10 下午 8.35.43.png")

## 第三种理解：流程图

将上述电路视作有向无环流程图去理解链式法则，比如一条路径：

![hankcs.com 2017-06-10 下午 8.41.57.png](img/e339e89040ab0d70c66447a8b542cf84.jpg "hankcs.com 2017-06-10 下午 8.41.57.png")

2 条路径：

![hankcs.com 2017-06-10 下午 8.43.00.png](img/f1d09272c984c6c6a17b56f55d9dd236.jpg "hankcs.com 2017-06-10 下午 8.43.00.png")

推广到多条路径：

![hankcs.com 2017-06-10 下午 8.43.39.png](img/16ce5c86e1577d6454d3c6fe92651742.jpg "hankcs.com 2017-06-10 下午 8.43.39.png")

推广到更复杂的流程图：

![hankcs.com 2017-06-10 下午 8.44.47.png](img/1939cf797aac7775c471fa2320b60a19.jpg "hankcs.com 2017-06-10 下午 8.44.47.png")

只要找到 z 的所有父节点应用链式法则并求和即可。

神经网络可以视作流程图的一个实例：

![hankcs.com 2017-06-10 下午 8.47.22.png](img/8d97392f89470a5c2081debd05d50d7a.jpg "hankcs.com 2017-06-10 下午 8.47.22.png")

任意流程图都可以执行反向传播：

![hankcs.com 2017-06-10 下午 8.48.22.png](img/43ce77d5da0a740c65f2944394ae3087.jpg "hankcs.com 2017-06-10 下午 8.48.22.png")

现在有一些软件包（TensorFlow）可以自动从前向传播的 symbolic expression（符号表达式）推导梯度，适用于快速设计原型。（其实 matlab 里也可以）

![hankcs.com 2017-06-10 下午 8.52.07.png](img/0d22b60d41796d903974474821084621.jpg "hankcs.com 2017-06-10 下午 8.52.07.png")

## 第四种解释：实际神经网络中的误差信号

其实就是把上面这些解释综合起来的解释，对如下 2 层的网络来讲：

![hankcs.com 2017-06-10 下午 8.53.58.png](img/520090d85687b531f671a736280a8ca3.jpg "hankcs.com 2017-06-10 下午 8.53.58.png")

假设最后一层对![](img/6fa5b72c6007f330f425b87500dd43cf.jpg)的误差是![](img/8199c40a54bed13c9acd7b795a6059e7.jpg)：

![hankcs.com 2017-06-10 下午 8.58.47.png](img/3a6b7bbabb3786ffc7dd5f098ece22d1.jpg "hankcs.com 2017-06-10 下午 8.58.47.png")

于是对![](img/2f63125cebbbc0f229e5dac45713676a.jpg)的导数是![](img/90e4cf346a772296f35898f0cabf3dae.jpg)

通过线性乘法器，对![](img/9dc5a4b33b9c6697de9b2e5736656043.jpg)的导数是权值与![](img/8199c40a54bed13c9acd7b795a6059e7.jpg)的乘积：

![hankcs.com 2017-06-10 下午 9.01.31.png](img/4eee8d50c5606bdc40aba2cca22740e6.jpg "hankcs.com 2017-06-10 下午 9.01.31.png")

通过一个 sigmoid gate，对![](img/4c9a5e4a08cc42abdc6ccca6ab27ffdc.jpg)的导数是：

![hankcs.com 2017-06-10 下午 9.05.13.png](img/03e7190d56e42d892703c51df6de69d2.jpg "hankcs.com 2017-06-10 下午 9.05.13.png")

再通过一个线性乘法器，得到对![](img/4a3e7793cb709ed57804a1575e8aa7ee.jpg)的导数：

![hankcs.com 2017-06-10 下午 9.07.09.png](img/fcbdfad61a2411fd964d44ebf0a735ff.jpg "hankcs.com 2017-06-10 下午 9.07.09.png")

于是对![](img/ef25fcbb789748e27c05690c90e50e9e.jpg)的导数是 ![](img/116a7c5c3e4af85a3b00803be1b279c6.jpg)。

## 课程项目

接下来都是围绕着课程项目的指导与建议，就不啰嗦了。简单写写一些体会：

*   不要想着一上来就发明个新模型搞个大新闻

*   也不要浪费大部分时间在爬虫上面，本末倒置

*   把旧模型用于新领域\新数据也是不错的项目

*   先要按部就班地熟悉数据、熟悉评测标准、实现基线方法

*   再根据基线方法的不足之处思考深度学习如何能带来改进

*   再实现一个已有的较为前沿的模型

*   观察该模型犯的错误，思考如何改进

*   这时才能没准就福至心灵发明一个新方法

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 5 反向传播与项目指导](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html)