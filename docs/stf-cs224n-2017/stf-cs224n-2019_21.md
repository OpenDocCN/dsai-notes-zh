# CS224n 研究热点 3 高效文本分类的锦囊妙计

![hankcs.com 2017-06-10 下午 9.24.11.png](img/6b0c83c7d14fe07e31c4ef74bd6432cb.jpg "hankcs.com 2017-06-10 下午 9.24.11.png")

## Facebook 的 fastText 

文本分类是 NLP 中常见的任务，比如情感分析：

![hankcs.com 2017-06-10 下午 9.23.57.png](img/dcdb645377e5add9e0ebe10693248235.jpg "hankcs.com 2017-06-10 下午 9.23.57.png")

## 词袋模型

虽然词袋模型只是所有词向量的某种平均，但其维度可以做到很低：

![hankcs.com 2017-06-10 下午 9.25.16.png](img/b5260b14fcd2e60e966161560882d372.jpg "hankcs.com 2017-06-10 下午 9.25.16.png")

为了抵抗词序丢失带来的语义丢失问题，可以用 ngram 特征来代替。

## 简单的线性模型

这并不是神经网络，因为从输入到隐藏层只是一个 look-up table，而隐藏层到输出则是一个逻辑斯谛回归线性分类器。

![hankcs.com 2017-06-10 下午 9.28.56.png](img/72948529969408b080a9e2287208f2d7.jpg "hankcs.com 2017-06-10 下午 9.28.56.png")

## 训练

用交叉熵作为损失函数：

![hankcs.com 2017-06-10 下午 9.29.57.png](img/ff9ada3997ad399d4207c3fdf62253d6.jpg "hankcs.com 2017-06-10 下午 9.29.57.png")

## Hierarchical softmax

与其用一个超大的 softmax 层，不如用多个 Hierarchical softmax：

![hankcs.com 2017-06-10 下午 9.31.34.png](img/5ba1e1050478a76f910568ed13d2133e.jpg "hankcs.com 2017-06-10 下午 9.31.34.png")

类似于[`www.hankcs.com/nlp/word2vec.html#h2-3`](http://www.hankcs.com/nlp/word2vec.html#h2-3) ，可以提高效率。

## 效果与速度

效果与最好的神经网络模型相差无几，但训练速度非常快：

![hankcs.com 2017-06-10 下午 9.34.41.png](img/15e31f276b868715e24ceda4e18d200d.jpg "hankcs.com 2017-06-10 下午 9.34.41.png")

## 总结

*   fastText 常常可以跟深度神经网络分类器打平。

*   但训练速度只需几秒，而不是几天。

*   还可以学习多种语言的词向量（效果比 word2vec 还要好）。

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 研究热点 3 高效文本分类的锦囊妙计](http://www.hankcs.com/nlp/cs224n-bag-of-tricks-for-efficient-text-classification.html)