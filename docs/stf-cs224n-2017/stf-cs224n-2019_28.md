# CS224n 研究热点 10 Character-Aware 神经网络语言模型

![hankcs.com 2017-07-04 下午 1.10.49.png](img/dd0c152182b1f65d01fc46a83ca27008.jpg "hankcs.com 2017-07-04 下午 1.10.49.png")

## 动机

大多数神经网络语言模型其实并没有注意到结构类似的词语意义也类似这种语言现象，这使它们无法赋予低频词合适的表示。所以这个新模型的目标是：

*   编码词素相关性：eventful, eventfully, uneventful

*   解决低频词问题

*   用更少的参数得到 comparable 效果

## 架构

输入字符，但依然在词语级别做预测。

![hankcs.com 2017-07-04 上午 10.23.35.png](img/13e5dcead202891f2579ec5261130c06.jpg "hankcs.com 2017-07-04 上午 10.23.35.png")

### 卷积层

读入字符的 embedding，做不同大小的卷积（捕捉不同的 ngram 与 subword），max 池化后得到特征向量。

![hankcs.com 2017-07-04 上午 10.26.43.png](img/459f2c60816ba9d85dee0de64841a84b.jpg "hankcs.com 2017-07-04 上午 10.26.43.png")

### Highway Network

这种网络类似 LSTM 的 Memory Cell，对输入做一些变换，但同时保留原始信息。

![hankcs.com 2017-07-04 上午 10.29.33.png](img/739cc3a3a581541a364e3228ffce6ee2.jpg "hankcs.com 2017-07-04 上午 10.29.33.png")

### LSTM

将 Highway Network 的输出喂给 LSTM，应该类似 CBOW 预测中间词语：

![hankcs.com 2017-07-04 上午 10.38.25.png](img/bc0e911f57abfc4471f4a3573ee2678d.jpg "hankcs.com 2017-07-04 上午 10.38.25.png")

## 量化结果

与最佳结果非常接近，但参数更少：

![hankcs.com 2017-07-04 上午 10.39.38.png](img/320765c2d7fae1386790796701235a6e.jpg "hankcs.com 2017-07-04 上午 10.39.38.png")

## 直观效果

可以捕捉构词类似的词语：

![hankcs.com 2017-07-04 上午 11.26.16.png](img/2fcbdb534d57596a3caa8b14e4c42518.jpg "hankcs.com 2017-07-04 上午 11.26.16.png")

比如 Richard 这个名字含有 rich，hard，ar 等词素，在 highway 之前能关联到 hard、rich 这些词语上；highway 之后能关联到 Eduard、Carl 之类的名字上。不仅名字对名字，连发音、性别都有更大的相似性。

对于 OOV 的效果也是惊人地好：

![hankcs.com 2017-07-04 上午 11.27.05.png](img/4647aaa2112de96142843bc6f3b9fe68.jpg "hankcs.com 2017-07-04 上午 11.27.05.png")

## 结论

字符级别的 CNN+Highway Network 可以提取丰富的语义和结构信息。这个模型将其他网络当成积木一样，构建了更好的语言模型。

## 实现

[`github.com/mkroutikov/tf-lstm-char-cnn`](https://github.com/mkroutikov/tf-lstm-char-cnn) 

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 研究热点 10 Character-Aware 神经网络语言模型](http://www.hankcs.com/nlp/cs224n-character-aware-neural-language-models.html)