# CS224n 笔记 9 机器翻译和高级 LSTM 及 GRU

简单回顾了传统统计机器翻译中的难题，过渡到利用 GRU 和 LSTM 来救场，最后介绍了一些较新的改进工作。

## 机器翻译

对于情感分析这类还算简单的任务，你可以整理一个情感极性词典、编写一堆规则做出一个勉强能用的系统。但到了机器翻译这个高级应用，就无法完全依靠规则了。现代机器翻译手段都是基于统计的，在平行语料上学习语言知识。世界上第一个平行语料库是罗塞塔石碑：

![Rosetta_Stone.JPG](img/038af11db9f8e256664ed12fd2b12859.jpg "Rosetta_Stone.JPG")

图片来源：[wiki](https://zh.wikipedia.org/wiki/%E7%BE%85%E5%A1%9E%E5%A1%94%E7%9F%B3%E7%A2%91)

传统机器翻译系统非常复杂，因为不同阶段用到了不同的机器学习方法。![hankcs.com 2017-06-23 上午 11.04.04.png](img/a87198500956a3b5828ad28a9fed852d.jpg "hankcs.com 2017-06-23 上午 11.04.04.png")![hankcs.com 2017-06-23 上午 11.15.38.png](img/90539ecea203eef3cd2925696d9a7005.jpg "hankcs.com 2017-06-23 上午 11.15.38.png")![hankcs.com 2017-06-23 下午 4.03.58.png](img/724efd0149cd311de25bc7f19a02c771.jpg "hankcs.com 2017-06-23 下午 4.03.58.png")![GRU.png](img/3d35df9a876a1d94e0bd7c4076b842b4.jpg "GRU.png")![LSTM.png](img/c937d89b0b6d1608c1abbf20da119525.jpg "LSTM.png")![LSTM3-chain.png](img/2cb7f3ff8847b2a1c29e2d2c0e67a890.jpg "LSTM3-chain.png")![hankcs.com 2017-06-23 下午 5.41.49.png](img/77e3bb3086c37669fa9f973820d3c498.jpg "hankcs.com 2017-06-23 下午 5.41.49.png")![hankcs.com 2017-06-23 下午 5.50.03.png](img/eeeedb141bdc07725731610274b8fb9a.jpg "hankcs.com 2017-06-23 下午 5.50.03.png")

## 传统统计机器翻译系统

定义一些符号：

原文![](img/dad139946fdd1363ecd86d20700a53a6.jpg)

译文![](img/be8982d125e27260b5c793cf0d39d70a.jpg)

机器翻译定义为找到使如下条件概率最大的![](img/be8982d125e27260b5c793cf0d39d70a.jpg)：

![](img/6e076848f1478129da6acf52e84be30f.jpg)

这里利用了贝叶斯公式。翻译模型![](img/a4e4774ee941170bd0eab6b0db16f9cd.jpg)在平行语料上训练得到，语言模型![](img/2bdbe38003cea6f4b9e786b4a31d7b8c.jpg)在未对齐的原文语料上训练（是非常廉价的）。

公式描述的翻译过程如下：

![hankcs.com 2017-06-23 上午 10.18.59.png](img/ea7cd23c52c6c2b7df13b8d21b50fbf6.jpg "hankcs.com 2017-06-23 上午 10.18.59.png")

### 第一步：对齐

找到原文中的哪个句子或短语翻译到译文中的哪个句子或短语，[这个第一步已经够难了](http://www.hankcs.com/nlp/michael-collins-machine-translation.html)。

对齐时，原文中可能有部分词语没有对应的译文：

![hankcs.com 2017-06-23 上午 10.23.00.png](img/fb6893cda0e859f6ca7faf75f0b20485.jpg "hankcs.com 2017-06-23 上午 10.23.00.png")

也可能在译文中有部分词语没有对应的原文，根据模型不同，可能有一对多的对齐方式：

![hankcs.com 2017-06-23 上午 10.24.36.png](img/4c4a1b14a4cb98614f93e57c83720e0f.jpg "hankcs.com 2017-06-23 上午 10.24.36.png")

也可能有多对一的对齐方式：

![2017-06-23_10-28-01.png](img/ea5ed68783546f2a3e3599a593b06167.jpg "2017-06-23_10-28-01.png")

还可能有多对多的对齐方式：

![hankcs.com 2017-06-23 上午 10.28.49.png](img/9a4f53b883e5cb8fb0e6d205999965d6.jpg "hankcs.com 2017-06-23 上午 10.28.49.png")

所有的对齐方式数量是组合数级的。

有时候还要通过句法分析，来进行不同颗粒度的对齐：

![hankcs.com 2017-06-23 上午 10.31.09.png](img/a3cef704302c827c894c8bccfe4a53e1.jpg "hankcs.com 2017-06-23 上午 10.31.09.png")

### 对齐之后

原文中每个单词都有多个备选单词，导致了许多短语的组合方式：

![hankcs.com 2017-06-23 上午 10.34.13.png](img/3c11b0508f0ecbbea5ae2c3147a6f635.jpg "hankcs.com 2017-06-23 上午 10.34.13.png")

### 解码：在海量假设中搜索最佳选择

这是一个特别复杂的搜索问题，涉及到许多语言模型。

![hankcs.com 2017-06-23 上午 10.36.40.png](img/0de4321ac3429a157351ad5d3975a0b0.jpg "hankcs.com 2017-06-23 上午 10.36.40.png")

### 传统机器翻译

这还只是传统机器翻译系统的冰山一角，有许多细节没有涉及到，还需要大量的人肉特征工程，总之是非常复杂的系统。其中每个环节都是独立不同的机器学习问题。这些独立的模型各自为政，并不以一个统一的优化目标为最终目标。

而深度学习则提供了一个统一的模型，一个统一的最终目标函数。在优化目标函数的过程中，得到一个 end to end 的完整的 joint 模型。传统机器翻译系统与深度学习是截然相反的，对齐模型、词序模型、语言模型……一堆独立的模型无法联合训练。

## 深度学习来救场

也许可以直接用 RNN 来接受原文，预测译文“下一个单词”：

![hankcs.com 2017-06-23 上午 10.50.00.png](img/33d0c0b86d7a5024ab182288fe7cebda.jpg "hankcs.com 2017-06-23 上午 10.50.00.png")

红圈所示特征表示必须能捕捉整个原文短语的语义，但是 RNN 无法记住太久之前的事情，大概五六个单词就到极限了。所以这不是个实用的模型。

在这个最简单的模型中，

Encoder 是：

![](img/958f73094afdd62bfdbf4399c2f7480d.jpg)

Decoder 是：

![](img/04180fdb751bff5d527583e6e8d75683.jpg)

![](img/bd7c120f6230b4de4988d270a27107a5.jpg)

最小化所有训练实例上的交叉熵误差：

![](img/e41e2c1f8165b2194123c37953e331cc.jpg)

记号 ![](img/ca528d4aa367c10fe3a64d04600e2cde.jpg) 一般代表接受的两个向量的权值不同。

softmax 分类器中必须有个代表句子终止的“单词”，不然模型会无休止地输出下去。

但神经网络机器翻译模型没有这么简单，必须加一些拓展。

1、编码器和解码器训练不同的权值矩阵

![hankcs.com 2017-06-23 上午 11.04.04.png](img/a87198500956a3b5828ad28a9fed852d.jpg "hankcs.com 2017-06-23 上午 11.04.04.png")

红蓝代表不同的权值。

2、decoder 中的隐藏层的输入来自 3 个方面：

*   前一个时刻的隐藏层

*   encoder 的最后一个隐藏层(![](img/520cff0b26c88eae410167696dcbcacf.jpg))

*   前一个预测结果 ![](img/55340d76d20208c4cd542785e7ee6b6a.jpg)

    这样导致 decoder 函数变为：

    ![](img/856b67eba97b83d9ed32926598841c31.jpg)

![en_decoder.png](img/69a86c492f47b65a2ae1c3567d73ce57.jpg "en_decoder.png")

这可以辅助训练 softmax 的权值矩阵，防止模型重复生成同一个单词。

上图还有一个复杂版本，表达的是同一个意思：

![hankcs.com 2017-06-23 上午 11.15.38.png](img/90539ecea203eef3cd2925696d9a7005.jpg "hankcs.com 2017-06-23 上午 11.15.38.png")

其中，带黑点的表示离散的向量表示，否则表示连续的向量空间。

3、使用深度 RNN

![deepbirnn.png](img/69a3fec7e29c69598694e60080bf0b07.jpg "deepbirnn.png")

4、使用 bi-directional encoder

5、不再用![](img/75716f1742c4a5adfb3884568a0bf366.jpg)作为训练实例，而是逆转原文词序：![](img/c59aac65bf255b183d0e30ff7aae7e43.jpg)。因为 A 更可能翻译为 X，而梯度消失导致 A 无法影响输出，倒过来 A 离输出近一些。逆转词序不会带来“语法语义上的改变”，因为模型学习的就是如何从逆序的原文翻译顺序的译文。但相应的，C 就离 Y 更远了

## 主要改进：更好的单元

引入 Gated Recurrent Units (GRU)，这种单元可以让模型学习何时遗忘从而将记忆保持很久、允许误差根据输入的不同而不同。

标准的 RNN 直接计算隐藏层：

![](img/68512a3c9d3d29c13ff39c67f735c7da.jpg)

GRU 先根据当前输入的词向量和隐藏层计算一个 update gate（另一个层）：

![](img/de496c5a04ffd0bbaa10a24b4a5d2da8.jpg)

利用相同的方法不同的权值计算 reset gate

![](img/faee5ddb072168335e4e0dc10bf36e95.jpg)

然后利用 reset gate 计算新的记忆

![](img/70afc8b1612185155cf746f702e21147.jpg)

这里的意思是，之前的记忆由 reset gate 控制，如果 reset gate 元素都是 0，则遗忘之前的事情。比如电影评论的情感分析，“有个文艺的少男爱死了一个平凡的少女，这个平凡的少女也爱死了那个文艺的少男，可两个人就是无法相会巴拉巴拉，真是个无聊的电影”。无论前面说了多少话，起决定性作用的可能就是“无聊”这个词。那么一些 GRU 可能会说，我遇到了一个情感强烈的词语，我不想让它被之前的记忆冲淡（求和），所以我把 reset gate 设为 0，之前的记忆不起作用，把这个情感词汇写成新的记忆。

而 update gate 的作用是调节最后的更新，到底时刻![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)的记忆多少来自之前，多少来自当前：

![](img/326e2908e5ccfc4349cc47b7667954f2.jpg)

如果![](img/98ba4bd98c899c9f15a00fe76fe782b2.jpg)是单位向量的话，则隐藏层只是复制前一个时刻的隐藏层，梯度不发生变化（衰减）。

这些公式写在一起已经非常直观了：

![](img/ae1ace74ccf7fa1435978be7d3627db4.jpg)

有些人觉得示意图更直观：

![hankcs.com 2017-06-23 下午 4.03.58.png](img/724efd0149cd311de25bc7f19a02c771.jpg "hankcs.com 2017-06-23 下午 4.03.58.png")

上图中，虚线代表某个 gate 的调节作用。隐藏层取决于上一个时刻的隐藏层和新的记忆，而 update gate 负责调节它们的比例，reset gate 和输入共同决定新的记忆……

有人问这个 reset gate 是不是多余的，是不是可以直接让![](img/d14b30e5423cc2bba511fa5ecc9027fa.jpg)。做是的确可以这么做，但![](img/22c5ed7653e3fae804006a00210327fc.jpg)永远无法把之前的记忆![](img/6c55220e47309a34947d09d7afff6d0d.jpg)“挤出去”，因为![](img/98ba4bd98c899c9f15a00fe76fe782b2.jpg)是个求和的形式。

下面又是一张故弄玄虚的图：

![hankcs.com 2017-06-23 下午 4.21.03.png](img/6d00a2327db8334d415aff9ed6ee1200.jpg "hankcs.com 2017-06-23 下午 4.21.03.png")

由于 gate 的调控作用是连续的而不是 0 或 1，所以上图中的开关并不合理。

对于短距离的依存来讲，reset gate 经常是激活的。

## LSTM（Long-Short-Term-Memories）

LSTM 与 GRU 动机相似，只不过单元结构有点不同。GRU 的单元结构如下：

![GRU.png](img/3d35df9a876a1d94e0bd7c4076b842b4.jpg "GRU.png")

LSTM 单元结构如下：

![LSTM.png](img/c937d89b0b6d1608c1abbf20da119525.jpg "LSTM.png")

上图用公式描述如下：

![](img/fe62fabfbd863885e4644f13aad4c89b.jpg)

 LSTM 的计算可以分为如下步骤

1.  New memory generation 与 GRU 的 New memory generation 类似。使用当前词语![](img/22c5ed7653e3fae804006a00210327fc.jpg) 和之前的隐状态![](img/1f144da32262a3ac352574b75109c931.jpg)来生成新的记忆![](img/ba76670a3ea6fbbd5990f51a80d3f589.jpg)。于是新记忆里面就包含了当前词语![](img/fedcfa2869ac5659a86e70574d0c765f.jpg)的属性。

2.  Input Gate 使用当前词语和之前的隐状态决定当前词语是否值得保留来 gate 新记忆，这个“是否”是通过![](img/651115cb0ff71c2ca1c1427396f12963.jpg)来体现的

3.  Forget Gate 与 input gate 类似，只不过它不是用来衡量输入单词的有用与否，而是衡量过去的记忆对计算当前记忆有用与否。它接受输入单词和上一刻的隐状态产生输出 ![](img/c3f1d5d1eca6424a3291543c20a894c6.jpg)。

4.  Final memory generation 根据 forget gate 的建议![](img/c3f1d5d1eca6424a3291543c20a894c6.jpg)来遗忘过去的记忆![](img/e8afb12f1ef293b1aec35e280f3970ba.jpg)。类似地，根据 input gate 的建议![](img/651115cb0ff71c2ca1c1427396f12963.jpg)来 gate 信的记忆 ![](img/ba76670a3ea6fbbd5990f51a80d3f589.jpg)，然后把两者加起来得到最终记忆 ![](img/8d342c2dfa1ac44cc3fbc3dae50852a0.jpg)。

5.  Output/Exposure Gate 这是 GRU 中不显式存在的门，用处是将最终记忆与隐状态分离开来。记忆 ![](img/8d342c2dfa1ac44cc3fbc3dae50852a0.jpg) 中的信息不是全部都需要存放到隐状态中，隐状态是个很重要的使用很频繁的东西，LSTM 中每个 gate 都需要隐状态的参与。Output Gate 产生 ![](img/566c9725a70553533263d052de25edd2.jpg)，用来 gate 记忆的 tanh 激活值。

### 一些可视化

一张最清晰的图示：

![LSTM3-chain.png](img/2cb7f3ff8847b2a1c29e2d2c0e67a890.jpg "LSTM3-chain.png")

[图源](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

一些把简单问题复杂化的图示：

![img017.jpg](img/7ff7ec13ca8951e5d85ce1669cedd780.jpg "img017.jpg")

[图源](http://people.idsia.ch/~juergen/lstm/sld017.htm)

![lstm_memorycell.png](img/2c1d6a358d411da435063e078ecc6eb6.jpg "lstm_memorycell.png")

[图源](http://deeplearning.net/tutorial/lstm.html)

都没有公式一目了然

### LSTM 很潮

是序列标注、seq2seq 任务的首选模型，可以层叠起来形成更深的模型。在数据量特别大的时候特别有用。

比如与传统 MT 模型的比较：

![hankcs.com 2017-06-23 下午 5.23.45.png](img/6bdd524640951583b87a9e20fc8038f6.jpg "hankcs.com 2017-06-23 下午 5.23.45.png")

那时候的 NN 模型还是仅限于重新排序传统 MT 模型产生的结果，而最新的研究就是完全甩开了 MT 模型：

![hankcs.com 2017-06-23 下午 5.25.42.png](img/663f27df98f939abd681dd9c7ea186b0.jpg "hankcs.com 2017-06-23 下午 5.25.42.png")

前三用的都是 NN。

### 深度 LSTM 用于机器翻译

输入原文序列，将最后一个时刻的隐藏层向量 PCA 降维后可视化：

![hankcs.com 2017-06-23 下午 5.32.19.png](img/2224eb838f93a9981d95a3fa98c47a56.jpg "hankcs.com 2017-06-23 下午 5.32.19.png")

发现无论词序如何，意义相同的句子在向量空间中更接近。

### 进一步改进：更多门！

![hankcs.com 2017-06-23 下午 5.41.49.png](img/77e3bb3086c37669fa9f973820d3c498.jpg "hankcs.com 2017-06-23 下午 5.41.49.png")

我感觉跟小孩玩乐高似的，拿着现有的单元拼拼凑凑成一个系统，跑出更高的分数就发 paper 了。至于为什么要这么拼这么改，不像理论完备的概率图模型，很少有人说得清楚吧。

## RNN 的最新改进

### softmax 的问题：无法出新词

对分类问题来讲，你无法指望分类模型给你分出一个训练集中不存在的类。即便是训练集中存在的类，如果样本数很少，模型也很难预测出该类。

对于预测下一个单词的语言模型来讲，也是如此。比如某某女士巴拉巴拉，然后自我介绍说我叫某某。如果某某不存在于训练集中，则模型无法预测出某某。

虽然可以用字符级的模型，但代价实在太大。

### 用指针来解决问题

如果把某某替换为“向前数第 10 个单词”这样的指针，问题就迎刃而解了。

![hankcs.com 2017-06-23 下午 5.50.03.png](img/eeeedb141bdc07725731610274b8fb9a.jpg "hankcs.com 2017-06-23 下午 5.50.03.png")

具体做法是，以前 100 个时刻的隐藏层作为输入，用一个 softmax 去计算前 100 个单词是 pointer 的概率，与原来的词表上的分布混合。

![hankcs.com 2017-06-23 下午 7.35.16.png](img/90f1f4193a7d5be34496b12080a51bfe.jpg "hankcs.com 2017-06-23 下午 7.35.16.png")

使用了 pointer 之后，困惑度下降了零点几个百分点：

![hankcs.com 2017-06-23 下午 7.37.10.png](img/66c5824233b2f5593c3caa500597226a.jpg "hankcs.com 2017-06-23 下午 7.37.10.png")

## 总结

*   RNN 很强大

*   有很多进行中的工作

*   GRU 更强大

*   LSTM 又更强大

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 9 机器翻译和高级 LSTM 及 GRU](http://www.hankcs.com/nlp/cs224n-mt-lstm-gru.html)