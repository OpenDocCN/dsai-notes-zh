# CS224n 研究热点 15 Neural Turing Machines

![dnc.jpg](img/1ac77a8b2a88f5063ddbfdc3a9dbfaed.jpg "dnc.jpg")    ![](img/0d43f3d355b56c92d7dad7adffe00d74.jpg)![hankcs.com 2017-07-14 上午 11.43.28.png](img/ae19686eb06f890c59b7d171a8f99e31.jpg "hankcs.com 2017-07-14 上午 11.43.28.png")

这次讲座覆盖了来自 DeepMind 的两篇论文：

![hankcs.com 2017-07-14 上午 11.44.00.png](img/402c170331df4e3877c31b26f327bc99.jpg "hankcs.com 2017-07-14 上午 11.44.00.png")

第二篇是第一篇的轻微改进，这里只关注抽象思想。

## 问题 

![hankcs.com 2017-07-14 上午 11.49.57.png](img/182a97319f14c60337608e6448ed254e.jpg "hankcs.com 2017-07-14 上午 11.49.57.png")

目前的神经网络擅长模式识别和动态决策，但无法使用知识进行深思或推断。比如明明可以胜任电子游戏这么复杂的问题，却无法完成最短路径这样的简单问题。

任何 DFS 算法变种都要储存当前访问过的节点，是否神经网络的问题出在记忆（或说内存）上呢？

## 记忆是解决方案吗

可是 LSTM 不是已经有 Memory 吗？事实上那只能叫 cache，不能叫 RAM。

## Neural Turing Machines

这种模型引入了可长期持续、随机读写的 RAM：

![hankcs.com 2017-07-14 上午 11.53.40.png](img/e548d22a1f0bd2208b012d640b68d96a.jpg "hankcs.com 2017-07-14 上午 11.53.40.png")

由 RNN 充当 controller 的角色：

![hankcs.com 2017-07-14 上午 11.54.50.png](img/1a6223271dd361aa22b990c2d58c7cda.jpg "hankcs.com 2017-07-14 上午 11.54.50.png")

决定何时读写哪一部分：

![hankcs.com 2017-07-14 上午 11.55.10.png](img/f51ce10b41127f8ea462818c3394784e.jpg "hankcs.com 2017-07-14 上午 11.55.10.png")

## 如何读写

如果你曾经上过计算机体系课，你就会明白内存是离散的结构，无法关于要读写的地址求导，自然无法利用反向传播优化。

解决方法是对所有地址进行不同的连续的程度读写，比如 attention 机制。

### 读内存

就是 attention 向量与 memory 矩阵的乘积：

![hankcs.com 2017-07-14 上午 11.58.54.png](img/a01bc5b88b209fa2db4a140288c81e0c.jpg "hankcs.com 2017-07-14 上午 11.58.54.png")

### 写内存

类似地，将要写入的值利用 attention 得到向量，写入内存的所有位置：

![hankcs.com 2017-07-14 下午 12.01.19.png](img/0620fead234075118a7497a5c94800a4.jpg "hankcs.com 2017-07-14 下午 12.01.19.png")

这种更新也有点像 GRU。

## attention 更新

如何得到正确的 attention 向量呢？

### 第一步

RNN controller 生成 query vector，与 memory 的每个地址做点积，得到一个强度向量，softmax 一下得到 attention 向量：

![hankcs.com 2017-07-14 下午 12.05.21.png](img/fdb2a323cb92d9ffc6d705d04ecf4218.jpg "hankcs.com 2017-07-14 下午 12.05.21.png")

### 第二步

与前一个 attention vector 做混合：

![hankcs.com 2017-07-14 下午 12.05.52.png](img/0d0d98a509fa6b0280eaf63b2e2cba7e.jpg "hankcs.com 2017-07-14 下午 12.05.52.png")

### 第三步

shift filter（允许读取 attention 集中注意的地址的邻居）一下，sharpen（锐化，使强者越强）得到最终 attention 分布，用于读写：

![hankcs.com 2017-07-14 下午 12.10.37.png](img/0489c5ce6108c0616cbdafe3dcc6c2f8.jpg "hankcs.com 2017-07-14 下午 12.10.37.png")

## 效果

可以在家谱知识库上做准确的推断：

![](img/0d43f3d355b56c92d7dad7adffe00d74.jpg)

## References

[`deepmind.com/blog/differentiable-neural-computers/`](https://deepmind.com/blog/differentiable-neural-computers/) 

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 研究热点 15 Neural Turing Machines](http://www.hankcs.com/nlp/cs224n-neural-turing-machines.html)