# CS224n 笔记 4 Word Window 分类与神经网络

这节课介绍了根据上下文预测单词分类的问题，与常见神经网络课程套路不同，以间隔最大化为目标函数，推导了对权值矩阵和词向量的梯度；初步展示了与传统机器学习方法不一样的风格。

## 分类问题

给定训练集![](img/705d46dffa236c74c9993102133a615e.jpg)

其中 ![](img/c1d8d90366495ae585c564c78a3a5ed6.jpg) 是一个 ![](img/9566974d45a96737f7e0ecf302d877b8.jpg)-维向量，![](img/78dfbafd40963decc8e0945aa8d82231.jpg) 是一个 ![](img/6c8feca3b2da3d6cf371417edff4be4f.jpg)-维 one-hot 向量，![](img/9341d9048ac485106d2b2ee8de14876f.jpg)是总数。在传统的机器学习方法中，往往通过诸如逻辑斯谛回归和 SVM 找到分类决策边界：

![LinearBoundary.png](img/0881785a00b58f80c36288ed5375d374.jpg "LinearBoundary.png")

### softmax 详细

softmax 分类函数：

![](img/4606e03c41c9f1d9c9a557b197f6e3fe.jpg)

计算方法分为两个步骤：取权值矩阵的某一行乘上输入向量，归一化得到概率。

### softmax 与交叉熵误差

训练的时候，可以直接最小化正确类别的概率的负对数：

![](img/90984eb0e34100a8cba73a0415c1bb44.jpg)

其实这个损失函数等效于交叉熵：

![](img/04010155bdd510159479721255e53290.jpg)

这是因为类别是 one-hot 向量。

对 N 个数据点来讲有：

![](img/fe60e8882f619507fa4d3f56893da659.jpg)

加上正则化项有：

![](img/3d9dc9bfc814c30ca3214cc1fcc3202c.jpg)

![hankcs.com 2017-06-09 下午 4.23.18.png](img/b17ac2a065bcc85fed0004eb4edb79dc.jpg "hankcs.com 2017-06-09 下午 4.23.18.png")

红线是 test error，蓝线是 training error，横轴是模型复杂度或迭代次数。直线是方差偏差均衡点。

### 优化

一般的 ML 问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合：

![hankcs.com 2017-06-09 下午 4.29.16.png](img/80996023c9ab9a2e749d6327b3352b74.jpg "hankcs.com 2017-06-09 下午 4.29.16.png")

### re-training 词向量失去泛化效果

比如有一个给单词做情感分析的小任务，在预训练的词向量中，这三个表示电视的单词都是在一起的：

![pretraining.png](img/669f163e82644ca9d6209a087db0cd4e.jpg "pretraining.png")

但由于情感分析语料中，训练集只含有 TV 和 telly，导致 re-training 之后两者跑到别处去了：

![retraining.png](img/c72fde5589d1d26af7001cd3c001d3db.jpg "retraining.png")

于是在测试集上导致 television 被误分类。

这个例子说明，如果任务的语料非常小，则不必在任务语料上重新训练词向量，否则会导致词向量过拟合。

顺便介绍一下词向量相关的术语：

词向量矩阵 L 常被称作 lookup table：

![hankcs.com 2017-06-09 下午 4.44.02.png](img/95e855f042aa4f0263c80b1d40ccc63d.jpg "hankcs.com 2017-06-09 下午 4.44.02.png")

Word vectors = word embeddings = word representations (mostly)

可能有些人认为“词向量”是个很俗的翻译，但根据这门课的课件，其实与“词嵌入”是等价的。而且 Rocher 也几乎是一直在用 word vector，而不是其他术语。

## Window classification

这是一种根据上下文给单个单词分类的任务，可以用于消歧或命名实体分类。上下文 Window 的向量可以通过拼接所有窗口中的词向量得到：

![hankcs.com 2017-06-09 下午 4.50.57.png](img/99a9cad77e62762f28e950a165b0a91e.jpg "hankcs.com 2017-06-09 下午 4.50.57.png")

这是一个

![hankcs.com 2017-06-09 下午 4.52.02.png](img/92ec5f51402fef694d77e537bc8409d6.jpg "hankcs.com 2017-06-09 下午 4.52.02.png")

的列向量。

### 最简单的分类器：softmax

![hankcs.com 2017-06-09 下午 4.56.04.png](img/a924328be2e355147bb3e92fd84a366b.jpg "hankcs.com 2017-06-09 下午 4.56.04.png")

![](img/067f2cb081e08987770a43a11fe9e447.jpg)对![](img/40779fc60a53ff2b70f832ec10cade09.jpg)求导，注意这里的![](img/40779fc60a53ff2b70f832ec10cade09.jpg)指的是窗口所有单词的词向量拼接向量。

![](img/8362a5c791938ecf48151b575fcb963c.jpg)

于是就可以更新词向量了：

![](img/4a49d00847b67e62c9cd878b73803f4e.jpg)

另一方面，对![](img/90490a34512e9bd1843ed4da713d0813.jpg)求偏导数，将所有参数的偏导数写到一起有：

![](img/2e127af4a730afe731d1672cf8a6f9fa.jpg)

在更新过程中计算代价最高的有矩阵运算![](img/cd9f535afcb433f264aa60a4e903688d.jpg)和指数函数，但矩阵运算比循环要快多了（在 CPU 上快一个数量级）。

### softmax（等价于逻辑斯谛回归）效果有限

仅限于较小的数据集，能够提供一个勉强的线性分类决策边界。

![hankcs.com 2017-06-09 下午 9.27.31.png](img/61f0d73ee01eba3012d28ce62f1c14eb.jpg "hankcs.com 2017-06-09 下午 9.27.31.png")

## 使用神经网络

神经网络可以提供非线性的决策边界：

![NonlinearBoundary.png](img/c3d580ffc71fee1b3de2db178bbe0994.jpg "NonlinearBoundary.png")

### 从逻辑斯谛回归到神经网络

老生常谈了，一些术语：

![hankcs.com 2017-06-09 下午 9.31.25.png](img/2dd942eda6be5cd2a9c08a2ca709e2dc.jpg "hankcs.com 2017-06-09 下午 9.31.25.png")

每个神经元是一个二分类逻辑斯谛回归单元：

![hankcs.com 2017-06-09 下午 9.33.00.png](img/a8f088987d59596b60aee8422f86af2d.jpg "hankcs.com 2017-06-09 下午 9.33.00.png")

神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么：

![hankcs.com 2017-06-09 下午 9.33.54.png](img/e4885cc847530232c0d95572e64b3cc2.jpg "hankcs.com 2017-06-09 下午 9.33.54.png")

我们把预测结果喂给下一级逻辑斯谛回归单元，由损失函数自动决定它们预测什么：

![hankcs.com 2017-06-09 下午 9.34.54.png](img/7f1fa7e0abc6100056a8faf85ad255c6.jpg "hankcs.com 2017-06-09 下午 9.34.54.png")

于是就得到了一个多层网络：

![hankcs.com 2017-06-09 下午 9.35.51.png](img/d242760fce6d61cee93d3e4dddfe468e.jpg "hankcs.com 2017-06-09 下午 9.35.51.png")

### 为什么需要非线性

因为线性系统所有层等效于一层：

![](img/5b305c215c205eaa0285bc8778c262ff.jpg)

而非线性模型可以捕捉很复杂的数据：

![hankcs.com 2017-06-09 下午 9.38.43.png](img/18d608c6ccb3991b1c1b5922a32069db.jpg "hankcs.com 2017-06-09 下午 9.38.43.png")

### 前向传播网络

一个简单的网络：

![hankcs.com 2017-06-09 下午 9.40.38.png](img/1b2f4eaec578394fa4435471db0c86b8.jpg "hankcs.com 2017-06-09 下午 9.40.38.png")

这种红点图经常在论文里看到，大致代表单元数；中间的空格分隔开一组神经元，比如隐藏层单元数为![](img/ce0c2a729934ddb18a1322686abfb844.jpg)。![](img/921e0f0523608a65f324548e8dc504e4.jpg)是隐藏层到 class 的权值矩阵：

![hankcs.com 2017-06-09 下午 11.34.31.png](img/ecb2485a0ce55cadd3157c32e57907ba.jpg "hankcs.com 2017-06-09 下午 11.34.31.png")

其中![](img/070b1af5eca3a5c5d72884b536090f17.jpg)是激活函数：

![](img/d2734da9e555486fd9af3ef3154146e7.jpg)

或：

![](img/1a05b9b4f585ba88c453d9a67ac49019.jpg)

### 间隔最大化目标函数

怎么设计目标函数呢，记![](img/c63a94dde7f3dbcac7224221e5aa4484.jpg)代表误分类样本的得分，![](img/0492c0bfd615cb5e61c847ece512ff51.jpg)表示正确分类样本的得分。则朴素的思路是最大化![](img/9992ede908bdee3b4da432289b6267be.jpg) 或最小化 ![](img/ab275da456974fe4d704e368f556c501.jpg)。但有种方法只计算![](img/0ffa6eaef6a189fee4bc47e5b518f0a0.jpg)时的错误，也就是说我们只要求正确分类的得分高于错误分类的得分即可，并不要求错误分类的得分多么多么小。这得到间隔最大化目标函数：

![](img/9d29a71bca567b6ce1d605b50ed886cd.jpg)

但上述目标函数要求太低，风险太大了，没有留出足够的“缓冲区域”。可以指定该间隔的宽度![](img/3d8018c750655ac3408ef156095ccb9f.jpg) ，得到：

![](img/2c381e829e4bc18a191144e851d884b1.jpg)

可以调整其他参数使得该间隔为 1：

![](img/9a5dbc5828db818b9767e3d5f662f566.jpg)

这实际上是将函数间隔转换为几何间隔，参考 SVM：[`www.hankcs.com/ml/support-vector-machine.html#h3-3`](http://www.hankcs.com/ml/support-vector-machine.html#h3-3)

在这个分类问题中，这两个得分的计算方式为：![](img/0910c734eee9d95fb145b1399036f686.jpg) 和 ![](img/bffc0ffd62f4c538a03f91dc16465013.jpg)；通常通过负采样算法得到负例。

另外，这个目标函数的好处是，随着训练的进行，可以忽略越来越多的实例，而只专注于那些难分类的实例。

## 反向传播训练

依然是老生常谈，跳过链式法则推导直接贴结果：

![](img/f815fd601d4d602a033a2fdc7cc22854.jpg)

其中![](img/5fe104578ec4581463bbf805a7b8af4c.jpg)是第![](img/a1c2f8d5b1226e67bdb44b12a6ddf18b.jpg)层的误差：

![](img/d339c1a308c63a02b7110f72bb6d7792.jpg)

可见只要控制误差的计算方式，就可以 smoothly 地过渡到间隔最大化目标函数：

![hankcs.com 2017-06-09 下午 10.38.49.png](img/780424b436eb34352497127ae33a404a.jpg "hankcs.com 2017-06-09 下午 10.38.49.png")

另外，对偏置的偏导数是![](img/96b66a6895da3eaf73ec8456ae9fb44c.jpg)：

![hankcs.com 2017-06-09 下午 10.21.13.png](img/baafdf68a983fbba3fe81b5a370b7f1c.jpg "hankcs.com 2017-06-09 下午 10.21.13.png")

最后一片拼图是对词向量的偏导数，由于连接时每个输入单元连到了多个隐藏单元，所以对某个输入单元的偏导数是求和的形式（残差来自相连的隐藏单元）：

![hankcs.com 2017-06-09 下午 10.24.20.png](img/9bd73e119890bd2de4fc435bce08ec5d.jpg "hankcs.com 2017-06-09 下午 10.24.20.png")

其中，![](img/5baff1027d6a9938031a102ac9bd5066.jpg)是第![](img/d8fdd0e28cfb03738fc5227885ee035a.jpg)列，转置后得到行向量；红色部分是误差，相乘后得到一个标量，代表词向量第![](img/d8fdd0e28cfb03738fc5227885ee035a.jpg)维的导数。那么对整个词向量的偏导数就是：

![](img/ea5eca8e25e49c0e1336a421154cce8e.jpg)

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 4 Word Window 分类与神经网络](http://www.hankcs.com/nlp/cs224n-word-window-classification-and-neural-networks.html)