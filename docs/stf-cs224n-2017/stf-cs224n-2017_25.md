# CS224n 研究热点 7 迈向更好的语言模型

![hankcs.com 2017-06-23 上午 11.29.06.png](img/23505358b8a00225a2b5f6b599caf334.jpg "hankcs.com 2017-06-23 上午 11.29.06.png")

我们已经知道一些手段来改进语言模型，比如：

1.  更好的输入：词→词根→字符

2.  更好的正则化/预处理

3.  这些手段综合起来得到了更好的语言模型

## 更好的输入

文本的多种颗粒度：

![hankcs.com 2017-06-23 下午 2.36.49.png](img/145d2cc8a286ae1daa07346af36bc7e8.jpg "hankcs.com 2017-06-23 下午 2.36.49.png")

更细的颗粒度相当于减小了词表，让模型更容易做对选择。试验表明的确降低了 error：

![hankcs.com 2017-06-23 下午 2.39.19.png](img/77e6158909a7d194dae308600823164d.jpg "hankcs.com 2017-06-23 下午 2.39.19.png")

## 更好的正则化和预处理

正则化就不说了。

预处理指的是，随机地将句子中的一些单词替换成另外的单词（比如把一个地名替换为另一个），或者使用 BiGram 统计信息来生成替换。

这样会得到一个更加平滑的分布，高频词将一些出场机会匀给了低频词。

![hankcs.com 2017-06-23 下午 2.51.03.png](img/5a93557a7a148b43b132fdd5d44a9fc9.jpg "hankcs.com 2017-06-23 下午 2.51.03.png")

对错误率的降低效果如下（左边是正则化，右边是预处理）：

![hankcs.com 2017-06-23 下午 2.53.41.png](img/da854916b3ec28e07f27b216f24004d6.jpg "hankcs.com 2017-06-23 下午 2.53.41.png")

## 更好的模型？

![hankcs.com 2017-06-23 下午 3.09.10.png](img/633bfd06b5f80ab626d141ad31f52264.jpg "hankcs.com 2017-06-23 下午 3.09.10.png")

**Noise Contrastive Estimation（NCE）**

与其用昂贵的交叉熵损失函数，不如用一种叫 NCE 损失的近似，理论证明当 k 值足够大时，两者梯度是接近的。

**更大的 LSTM 单元数**

LSTM 单元数增加到 1024，k 值越大越好，直到吃满 GPU 显存。

用上了这些种种改进之后，总算是拿到如下成绩：

![hankcs.com 2017-06-23 下午 3.08.50.png](img/78522e15ad2beb1c86d71a30c1246a32.jpg "hankcs.com 2017-06-23 下午 3.08.50.png")

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 研究热点 7 迈向更好的语言模型](http://www.hankcs.com/nlp/cs224n-better-language-modeling.html)