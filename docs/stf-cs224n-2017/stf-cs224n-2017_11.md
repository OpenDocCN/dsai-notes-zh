# CS224n 笔记 11 GRU 和 NMT 的进一步话题

![2017-06-30_14-14-18.png](img/d9b2d55e87e3ebea1150453190f2c311.jpg "2017-06-30_14-14-18.png")![](img/749012c81bc30aac56fd9e268961607c.jpg)从动机层面直观地充实了 GRU 和 LSTM 的理解，介绍了 MT 的评测方法，讨论了 NMT 中棘手的大词表问题和一些常见与最新的解决办法。

## 深入 GRU 

把 GRU 再详细讲一讲。

![hankcs.com 2017-06-29 下午 5.13.08.png](img/e7e9bc433c3b9f086caf35f21cb5614a.jpg "hankcs.com 2017-06-29 下午 5.13.08.png")

RNN 的梯度消失就不赘述了，红线连乘多次下溢出。

而 GRU 额外添加了一些“捷径”红线，允许梯度直接流过去，而不是连乘的方式递减过去。

### Update Gate

![hankcs.com 2017-06-29 下午 5.28.17.png](img/4b79901f861316213e5735eab29ffc66.jpg "hankcs.com 2017-06-29 下午 5.28.17.png")

用来自适应学习应该把多少注意力放到前一个隐藏层状态上。

### Reset Gate

![hankcs.com 2017-06-30 上午 9.14.27.png](img/a4f41cc969fde3b4a46c1c0a31a6690e.jpg "hankcs.com 2017-06-30 上午 9.14.27.png")

自适应地删除不需要的连接。

RNN 寄存器

![hankcs.com 2017-06-30 上午 9.16.46.png](img/182d37389a2cb54e6cb71cd48fa7551b.jpg "hankcs.com 2017-06-30 上午 9.16.46.png")

朴素 RNN 读取所有寄存器，运算后存入所有寄存器，没有灵活性。

### GRU 寄存器

![hankcs.com 2017-06-30 上午 10.04.08.png](img/06b3adcf04ee4a5bb5b8431dafa367d1.jpg "hankcs.com 2017-06-30 上午 10.04.08.png")

门多了之后，就可以灵活地选择读取部分寄存器，执行运算，写入部分寄存器。

Reset Gate 起到决定要读哪些寄存器的目的，而 Update Gate 决定要写的寄存器。这里的“决定”其实是“强度”的意思，不是绝对的。

### GRU 和 LSTM 对比

GRU 中的隐藏状态![](img/a048a5bfcc0242b6427d15ed11ef7e23.jpg)类似于 LSTM 中的 cell![](img/a96fd1792ebb964c44e6a4802fe73a45.jpg)，而 LSTM 中的隐藏状态![](img/a048a5bfcc0242b6427d15ed11ef7e23.jpg)其实相当于一个暴露给外部世界的“显状态”。LSTM 通过给 cell 加一个![](img/0a586295614644f838c9b78e46b2c06b.jpg)获得非线性的灵活性。

另外，观察![](img/6c55220e47309a34947d09d7afff6d0d.jpg)之前的系数，这里的![](img/33becaceee3dd4f30f106b6a8605226f.jpg)其实是“don't reset gate”，![](img/39e0c3cfa9742216d02b21de5ed57650.jpg)其实是“don't forget gate”。

![2017-06-30_14-05-38.png](img/67e225aa5d3c570918c49aadf6b747c4.jpg "2017-06-30_14-05-38.png")

## 深入 LSTM

宏观上的 LSTM Cell：

![2017-06-30_14-14-18.png](img/d9b2d55e87e3ebea1150453190f2c311.jpg "2017-06-30_14-14-18.png")

将所有操作都 gate 起来，方便遗忘甚至忽略一些信息，而不是把所有东西都塞到一起。

![2017-06-30_14-18-01.png](img/95ed90e773283e56916909452a12c33a.jpg "2017-06-30_14-18-01.png")

New Memory Cell 的计算是一个非线性的过程，与朴素 RNN 一模一样：

![2017-06-30_14-22-41.png](img/69df61a9fc836faf1b4a38fbef3c1284.jpg "2017-06-30_14-22-41.png")

最关键之处在于，Memory Cell 的更新中有一个加法项直接来自上一刻的 Cell，也就是说建立了![](img/a96fd1792ebb964c44e6a4802fe73a45.jpg)和![](img/e8afb12f1ef293b1aec35e280f3970ba.jpg)的直接线性连接（与 ResNet 类似）：

![2017-06-30_14-30-49.png](img/b89bf2b2ea7a6a04a7141fff4ea8fd23.jpg "2017-06-30_14-30-49.png")

类似于 GRU 中的加法，在反向传播的时候允许原封不动地传递残差，也允许不传递残差，总之是自适应的。

有了这些改进，LSTM 的记忆可以比 RNN 持续更长的 step（大约 100）：

![](img/749012c81bc30aac56fd9e268961607c.jpg)

### 训练技巧

*   将递归权值矩阵初始化为正交

*   将其他矩阵初始化为较小的值

*   将 forget gate 偏置设为 1：默认为不遗忘

*   使用自适应的学习率算法：Adam、AdaDelta

*   裁剪梯度的长度为小于 1-5

*   在 Cell 中垂直应用 Dropout 而不是水平 Dropout

*   保持耐心，通常需要训练很长时间

### Ensemble

如果想获得额外的 2 个百分点的效果提升，可以训练多个模型，平均它们的预测。

![2017-06-30_15-08-31.png](img/92dc8a7e18fafcfb8b3c6b063de0bd00.jpg "2017-06-30_15-08-31.png")

## MT 评测

以前人们认为交给人类译员来打分是最好的，但这太主观了，10 个译员给出的翻译可能都不相同。

### BLEU

后来 IBM 发明了一种简单有效的评价策略，叫 BLEU。

![hankcs.com 2017-06-30 下午 5.56.06.png](img/96ea77cdcf808079017df07f55973255.jpg "hankcs.com 2017-06-30 下午 5.56.06.png")

通过比较标准译文与机翻译文中 NGram 的重叠比率（0 到 1 之间）来衡量机翻质量。

### Brevity Penalty

是否可以通过输出大量无意义的 the 之类来作弊？不，通过 Brevity Penalty 来防止机翻比译员译文短。

一般取 4-gram 之内参与评测，最终的分值是所有 ngram 分值的几何平均乘上一个 Brevity Penalty：

![hankcs.com 2017-06-30 下午 5.57.09.png](img/c92e255d5f7d8d91a485da820f41b5ab.jpg "hankcs.com 2017-06-30 下午 5.57.09.png")

一个具体的 BLEU 重叠例子：

![hankcs.com 2017-06-30 下午 6.01.00.png](img/729f2bb6ebadb585ce274a66a922908c.jpg "hankcs.com 2017-06-30 下午 6.01.00.png")

### Multiple Reference Translations

为了防止某篇机翻实际上很好，可就是跟人类译文用词行文不相似的情况，IBM 的论文建议多准备几篇标准答案，这样总会撞上一个：

![hankcs.com 2017-06-30 下午 6.03.04.png](img/671c68c281cac3f6ee8859c0a90c9f4b.jpg "hankcs.com 2017-06-30 下午 6.03.04.png")

当然了，虽然 IBM 建议准备 4 篇，实际测试中经常只有 1 篇。没有完美体现能力的考试，现实社会也是如此。

刚开始的时候，BLEU 和人工打分几乎是线性相关（一致）的：

![hankcs.com 2017-06-30 下午 7.56.45.png](img/e4a5aceb6e4e42677585ab36499bede0.jpg "hankcs.com 2017-06-30 下午 7.56.45.png")

而当人们为了 BLEU 分数费尽心机之后，两者就开始脱钩了。这也是为什么 Google 翻译的效果没有宣称的那么好的原因之一。

## 解决大词表问题

大词表问题指的是 softmax 的计算难度：

![2017-06-30_20-04-35.png](img/3737fe6ff6200ce5128cdd8f67cc50ec.jpg "2017-06-30_20-04-35.png")

早期的 MT 系统会使用较小的词表，但这并不是解决问题，而是逃避问题。

![2017-06-30_20-05-53.png](img/44ca92cf4a1a1bdd4dfd8b1f9f5a92bc.jpg "2017-06-30_20-05-53.png")

另一种思路是，hierarchical softmax，建立树形词表，但这类方法过于复杂，让模型对树形结构敏感而不是对词语本身敏感。

另外就是上几次课见过的廉价的 NCE，这些方法对 GPU 都不友好。

### Large-vocab NMT

最新的方法是训练时每次只在词表的一个小子集上训练，因为 40%的词语只出现一次，如果把训练数据均分为许多份，则每一份中的稀有词可能都不会在其他语料中出现。然后测试时加一些技巧，待会儿详谈。

#### 训练

![hankcs.com 2017-06-30 下午 8.18.31.png](img/7589154ed92739cf47e598654522dee3.jpg "hankcs.com 2017-06-30 下午 8.18.31.png")

如何选择小词表呢？在刚才的方法上更进一步，让用词相似的文章进入同一个子集，这样每个子集的词表就更小了。

![hankcs.com 2017-06-30 下午 8.19.38.png](img/7ce860f6eaf55138577ee56cf5a72390.jpg "hankcs.com 2017-06-30 下午 8.19.38.png")

#### 测试

测试的时候先雷打不动将前![](img/a5db490cd70a38a0bb9f3de58c51589f.jpg)个最常使用的单词加入备选词表，然后将原文中每个单词可能的前![](img/bf90b43d1c342c0ad95b1d551450d4a8.jpg)个翻译加进去。最后在备选词表上 softmax。

![hankcs.com 2017-06-30 下午 8.25.35.png](img/fd955df021697979f39941e16fab34bc.jpg "hankcs.com 2017-06-30 下午 8.25.35.png")

### 更多技巧

上上次小讲座中提到的 pointer、用词素级别来翻译 goooooood morning 之类。至此下课时间到了，后面全部是我的理解。

#### Byte Pair Encoding

这种方法试图用分词的思想去找出所有有意义的“词素”，其统计方法说来也简单，就是在词频词表里统计所有的 ngram 组合作为新的更长的 ngram：

![hankcs.com 2017-06-30 下午 8.32.28.png](img/bf1226533a209276d06fb00cd88780c1.jpg "hankcs.com 2017-06-30 下午 8.32.28.png")

该方法拿到了 WMT2016 的第一名，代码在：[`github.com/rsennrich/nematus`](https://github.com/rsennrich/nematus)

#### 其他

一些字符级别的 LSTM 之类。

另有一些混合动力的 NMT，大部分情况下在词语级别做翻译，只在需要的时候从字符级去翻译。这个系统的主体是词语级别的 LSTM：

![2017-06-30_20-37-02.png](img/26ea9e1b473eb41530c11bb0979a3b00.jpg "2017-06-30_20-37-02.png")

在词语级别上做常规的柱搜索：

![hankcs.com 2017-06-30 下午 8.38.22.png](img/7e9a50a33edb11e2e86e7d9e412b56c2.jpg "hankcs.com 2017-06-30 下午 8.38.22.png")

一旦产生了 unknown 词语，则在字符级别进行柱搜索：

![2017-06-30_20-39-44.png](img/0a9da0196456ec2a0f8d0026e5510181.jpg "2017-06-30_20-39-44.png")

然后提升了两个 BLEU 分值。

![hankcs.com 2017-06-30 下午 8.40.31.png](img/db2f36b3a41c52ae8897b44db2092a45.jpg "hankcs.com 2017-06-30 下午 8.40.31.png")

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 11 GRU 和 NMT 的进一步话题](http://www.hankcs.com/nlp/cs224n-gru-nmt.html)