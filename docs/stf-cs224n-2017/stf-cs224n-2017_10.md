# CS224n 笔记 10 NMT 与 Attention

![hankcs.com 2017-06-27 下午 3.45.05.png](img/d61665fc0babbd3e692a4b7e2cea62b2.jpg "hankcs.com 2017-06-27 下午 3.45.05.png")从 NMT 的历史谈到现代，讲解了 attention 机制的动机、原理及最新的拓展。通过实例展示谷歌翻译的变化，直言不讳指出其过度炒作。Manning 今天还换上了新 MBP，挺潮的。

## 机器翻译

*   传统衡量机器对语言理解的测试之一

*   同时涉及到语言分析与理解

*   一个每年 400 亿美金的产业

*   主要在欧洲，亚洲也在兴起

### 机器翻译的需求

*   Google 每天翻译 1000 亿单词

*   Facebook 研发了自己的翻译系统，因为通用的机器翻译系统无法适应社交领域

*   eBay 用机器翻译来促进跨境交易

## 什么是 NMT

用一个大型神经网络建模整个翻译过程的系统。

### 架构

抽象的架构就是一个 encoder 一个 decoder：

![hankcs.com 2017-06-27 下午 3.17.59.png](img/c794cfbdd0fce5a231d54477e7676996.jpg "hankcs.com 2017-06-27 下午 3.17.59.png")

### NMT:青铜时代

80 年代神经网络是个很边缘的领域，另外计算力也很有限。当时的 NMT 系统只是个玩具：词表四五十，固定的 50 个输入（二进制编码），固定的 66 个输出，一到三层隐藏层，150 个单元……

![hankcs.com 2017-06-27 下午 3.33.17.png](img/72aea2ddcf60e3561f039df990703b2e.jpg "hankcs.com 2017-06-27 下午 3.33.17.png")

90 年代出现了一种类似 RNN 的更复杂的框架：

![hankcs.com 2017-06-27 下午 3.35.58.png](img/27a0b855492fc2473250b07611f34b9a.jpg "hankcs.com 2017-06-27 下午 3.35.58.png")

### 现代 NMT 模型

之前课上也提到过，一个 RNN 做 encoder，另一个 RNN 做 decoder：

![2017-06-27_15-39-23.png](img/2afdbb6af53a7fd56fa7d937a5f08b62.jpg "2017-06-27_15-39-23.png")

实际使用的系统更加复杂：

![hankcs.com 2017-06-27 下午 3.45.05.png](img/d61665fc0babbd3e692a4b7e2cea62b2.jpg "hankcs.com 2017-06-27 下午 3.45.05.png")

这里的 RNN 可视作以原文为条件的 conditional 语言模型

![2017-06-27_15-47-58.png](img/7e08b4fb862a46a238399e1bf33b7847.jpg "2017-06-27_15-47-58.png")

### RNN Encoder

![hankcs.com 2017-06-27 下午 3.51.08.png](img/6c11a23715ceca0bf0ee61a2f9e5205b.jpg "hankcs.com 2017-06-27 下午 3.51.08.png")

最后一个隐藏层的状态 Y 是整个原文的总结。

常见的 encoder 实际上加了一些 extension，比如 GRU 或 LSTM。

### Decoder：递归语言模型

常见的做法是把 encoder 的最后一层（最后一个时刻）作为 decoder 的第一层，这样就必须用 LSTM 保持中期记忆。

另一种做法是将 encoder 最后一层喂给 decoder 的每一层，这样就不会有记忆丢失的后顾之忧了。

![hankcs.com 2017-06-27 下午 3.59.24.png](img/6241dfad85ece8a1856173a648b131a9.jpg "hankcs.com 2017-06-27 下午 3.59.24.png")

### MT 的发展

基于短语的 MT 就是 2016-11 之前的 Google 翻译所采用的系统，其发展是缓慢的。神经网络兴起之后，出现了一种基于 Syntax-based SMT（估计是换换词向量），发展也不快。但 NMT 的发展是最迅猛的：

![2017-06-27_16-07-32.png](img/9b81c27415547b799aa6222b4f704ecb.jpg "2017-06-27_16-07-32.png")

### NMT 的四大优势

1.  End-to-end training
    为优化同一个损失函数调整所有参数

2.  Distributed representation
    更好地利用词语、短语之间的相似性

3.  Better exploitation of context
    利用更多上下文——原文和部分译文的上下文

4.  生成的文本更流畅
    可能跟上述优势有关

NMT 还避免了传统 MT 中的黑盒子（reorder 之类）模型。NMT 也存在弱点

*   无法显式利用语义或语法结构（依存句法分析完全用不上了，有些工作正在展开）

*   无法显式利用指代相消之类的结果

可见太统一的闭环系统也有自己的烦恼啊。

## 统计/神经网络机器翻译

Manning 说除了英语之外，学生中第二大语种是中文，而且他亮出了简体中文的例子，真是让人激动啊。他还特意在不同年份测试了 google 翻译的效果：

![hankcs.com 2017-06-27 下午 4.31.44.png](img/0c4c3ae05b5fa55883795cafdd4c7a87.jpg "hankcs.com 2017-06-27 下午 4.31.44.png")

其中，13 年有所进步，14-16 年又退步了并且停滞了 3 年。直到 2017 年才有质的飞跃。

我觉得这个句子涉及到指代相消，MT 系统很难消解损兵的“他们”。MT 系统必须有这样的常识：兵力悬殊的两军对战，人数少的一方更容易失败。估计语料不足以给它这种知识。

### NMT 主要由工业界促进

2016-02 微软在 Android 和 iOS 上发布了离线 NMT 系统，这对境外旅游人士特别有帮助。

2016-08 Systran 发布了 NMT 模型

2016-09 Google 发布了 NMT 系统，大肆宣传了一番，并且 overclaim 比得上人工翻译质量。Manning 真是直言不讳啊。

## 介绍 Attention

朴素 encoder-decoder 的问题是，只能用固定维度的最后一刻的 encoder 隐藏层来表示源语言 Y，必须将此状态一直传递下去，这是个很麻烦的事情。事实上，早期的 NMT 在稍长一点的句子上效果就骤降。

### Attention 机制

![hankcs.com 2017-06-27 下午 5.43.49.png](img/44b43dafc4a17912fb19d3cadbebb4c1.jpg "hankcs.com 2017-06-27 下午 5.43.49.png")

解决方法是将 encoder 的历史状态视作随机读取内存，这样不仅增加了源语言的维度，而且增加了记忆的持续时间（LSTM 只是短时记忆）。

这种机制也与人类译员的工作流程类似：不是先把长长的一个句子暗记于心再开始闭着眼睛翻译，而是草草扫一眼全文，然后一边查看原文一边翻译。这种“一边……一边……”其实类似于语料对齐的过程，即找出哪部分原文对应哪部分译文。而 NMT 中的 attention 是隐式地做对齐的。

### 词语对齐

传统的 SMT 中需要显式地做双语对齐：

![hankcs.com 2017-06-27 下午 5.50.02.png](img/cbd3dc0ebd42f9856f108a500c170e1d.jpg "hankcs.com 2017-06-27 下午 5.50.02.png")

而 attention model 是在翻译的过程中隐式地对齐。

### 同时学习翻译和对齐

一个非常棒的可视化，显示 attention model 成功地对齐了法语和英语，其中一小段语序的调整也反应出来了：

![hankcs.com 2017-06-27 下午 5.54.28.png](img/62cfdd0916b24f58c00003dd5ea7a6f2.jpg "hankcs.com 2017-06-27 下午 5.54.28.png")

### 打分

在图示问号时刻，究竟应该关注哪些时刻的 encoder 状态呢？关注的强度是多少呢？

有一种打分机制，以前一刻的 decoder 状态和某个 encoder 状态为参数，输出得分：

![hankcs.com 2017-06-27 下午 5.58.27.png](img/fec8431cf854259b2e1ca0d17fc7e47c.jpg "hankcs.com 2017-06-27 下午 5.58.27.png")

![hankcs.com 2017-06-27 下午 5.58.52.png](img/9a45e2ff90fc8c08b0f7e2d7d73f2220.jpg "hankcs.com 2017-06-27 下午 5.58.52.png")

![hankcs.com 2017-06-27 下午 5.59.11.png](img/7adbd9dc83f61e13ea83b8cffce37f76.jpg "hankcs.com 2017-06-27 下午 5.59.11.png")

![hankcs.com 2017-06-27 下午 5.59.32.png](img/a595a0dd46486fd2a8677a3bb5695d7a.jpg "hankcs.com 2017-06-27 下午 5.59.32.png")

然后 softmax 归一化分值转化为概率，这个概率称为对齐权值（alignment weights）：

![hankcs.com-2017-06-27-下午 6.00.23.png](img/5000ec10567192e4291147aac8222858.jpg "hankcs.com-2017-06-27-下午 6.00.23.png")

这个概率也代表模型应该将多少比例的注意力放在一个历史状态上：

![hankcs.com 2017-06-28 上午 9.12.56.png](img/0aa045497384234581c35f22efafd0d4.jpg "hankcs.com 2017-06-28 上午 9.12.56.png")

加权和得到一个 context vector，作为条件之一生成 decoder 的当前状态：

![hankcs.com 2017-06-28 上午 9.14.58.png](img/781c55c88b8811c2d0b5f05c8c778f9c.jpg "hankcs.com 2017-06-28 上午 9.14.58.png")

而分数的获得，是通过 attention function 进行的。attention function 有多种选择，其中流行的是中间这种。![](img/e114b122e0ce8034b0548b4d4ee373ed.jpg)给了两个向量更复杂的 interaction，而最后一种根本没有 interaction。

![2017-06-28_09-20-16.png](img/2ecd36918a1de51b75d80fffb0b5b2b7.jpg "2017-06-28_09-20-16.png")

有一些观点认为模型不应该注意所有的事情，可能对长句子来讲比较有潜力：

![hankcs.com 2017-06-28 上午 9.24.03.png](img/a8ac3be5be5510bcf2c70b14cb236258.jpg "hankcs.com 2017-06-28 上午 9.24.03.png")

但这些观点并没有取得更好的成绩：

![2017-06-28_09-27-50.png](img/fbe484767b60b159f8b9d2b4c0ce5533.jpg "2017-06-28_09-27-50.png")

句子特短的时候，模型们的得分都不高。这纯粹是因为语料中的短句子本来就语义隐晦，比如某个专有名词作为标题。而有 attention 的模型在句子很长的时候，效果依然没有下降，说明了 attention 的重要性。

LSTM 非常擅长生成自然的文本，但有时候译文与原文相去甚远，没有把注意力放在原文上。比如下面红色的名字不知道从哪里冒出来的：

![hankcs.com 2017-06-28 上午 9.33.12.png](img/6998c211ff08b62f3689c4b6858681f7.jpg "hankcs.com 2017-06-28 上午 9.33.12.png")

加了 attention 好了很多，还是比不上人类，有时候会把同一个意思的词重复两遍：

![hankcs.com 2017-06-28 上午 9.33.12.png](img/e16585bddaf1f9ffcf145310914516de.jpg "hankcs.com 2017-06-28 上午 9.33.12.png")

### 更多 attention！覆盖范围

在图片标题生成研究中，模型通过对图片不同区域的 attention 生成了不同的词语：

![hankcs.com 2017-06-28 上午 9.38.26.png](img/ab361f722d5e13dfcca62ed9c83b4e96.jpg "hankcs.com 2017-06-28 上午 9.38.26.png")

如何保证不错过任何重要的区域呢？

### Doubly attention

一种思路是同时注意原文和译文：

![2017-06-28_09-42-10.png](img/7caccb3bd163f3a25ae6fe17dd2f848f.jpg "2017-06-28_09-42-10.png")

### 用旧模型的语言学思想拓展 attention

可以利用 IBM2 等模型中的位置或 fertility(丰富程度)，因为一般而言一个词最多翻译为两三个词，如果生成了五六个词，那么模型可能在重复生成。

![hankcs.com 2017-06-28 上午 9.50.27.png](img/48348c915910d0c90c2ce1ef541ace1c.jpg "hankcs.com 2017-06-28 上午 9.50.27.png")

## decoder

模型能够在给定原文![](img/e189ab8c296ade73c4fcd78a054a1960.jpg)的情况下计算译文![](img/9dd8842915743e0264f13a27a70fc448.jpg)的概率![](img/9576769caab70990d55fc245577387e6.jpg)之后，就来到传统的问题了，找出最可能的译文![](img/24107a3e84caf8ec79cdced0dd7b94e6.jpg)：

![](img/63153f1d4cf52385037dbdf1dccce8f7.jpg)

在 decoding 的时候，朴素想法是生成所有的翻译，用语言模型打分，然后挑最大的。但译文数量是词表大小的指数函数，无法实现。

![2017-06-28_10-03-00.png](img/0b3c750be343f3114a8ffdda0714deac.jpg "2017-06-28_10-03-00.png")

### Ancestral sampling

在时刻![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)，根据之前的词语生成当前词语![](img/22c5ed7653e3fae804006a00210327fc.jpg)：

![](img/b9c0ae12b9ed41c7885b1ea9a296ec90.jpg)

可以多次 sample 取最好的。

理论上完美无缺，但实践中只会产生高方差的差效果。你也不想同一个句子每次翻译结果都不一样。

### Greedy Search

![2017-06-28_21-54-06.png](img/f046cf75dc321fae6feefaf074c74ee7.jpg "2017-06-28_21-54-06.png")

不多想了，贪婪地选取当前最可能的那个单词：

![](img/d1d16397882c0ba7b455d324097892eb.jpg)

缺点显而易见，不是全局最优，走错一步会影响后面的部分。

### Beam search

![2017-06-28_21-54-54.png](img/e0c7eee9a68612e24036e4af9d5680d8.jpg "2017-06-28_21-54-54.png")

老生常谈了，从不搜索到贪婪搜索到柱搜索，随处可见。

每个时刻记录![](img/a1c2f8d5b1226e67bdb44b12a6ddf18b.jpg)个最可能的选项（剪枝），在其中进行搜索。

![](img/177a56117ebed7bc6c547264016a6598.jpg)

然后递推![](img/5e1a0b0edd2442aab4e304d9efdf40be.jpg)：

![](img/6c181ce267a3f9613496cf26bee23add.jpg)

其中

![](img/63e1524a159e4cffcabdd7143aa0102d.jpg)

也就是说把词表中的词丢进入计算概率取前![](img/a5db490cd70a38a0bb9f3de58c51589f.jpg)个。

### 效果对比

![hankcs.com 2017-06-28 下午 10.01.15.png](img/d94cc6f7b8fd78ce4dba0b34890d9067.jpg "hankcs.com 2017-06-28 下午 10.01.15.png")

采样要采 50 轮才得到比贪心搜索稍好的结果，但很小的柱搜索轻松超越了它们。另外，基于短语的 MT 常用的柱搜索大小是 100 到 150，可见 NMT 的优势。

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 10 NMT 与 Attention](http://www.hankcs.com/nlp/cs224n-9-nmt-models-with-attention.html)