# CS224n 笔记 12 语音识别的 end-to-end 模型

这次斯坦福请到了深度学习教父 Hinton 的弟子 Navdeep 来讲语音识别，他正在英伟达工作，怪不得 N 卡在深度学习中的地位如此之高。而他本人也在用 Dell 的搭载了 N 卡的 XPS 跑 Ubuntu，一改以往“讲台必定信仰灯”的局面。

## Automatic Speech Recognition（ASR）

ASR 就是将声学信号转化为文本的系统。

![hankcs.com 2017-07-01 上午 10.40.18.png](img/bc5f3d94a09b2e548910e9f1e850caeb.jpg "hankcs.com 2017-07-01 上午 10.40.18.png")

语音是一种自然的用户接口：

![2017-07-01_10-42-10.png](img/8325067d1d0a6211bc02633840392ff9.jpg "2017-07-01_10-42-10.png")

### 传统 ASR

传统做法的主体是生成式语言模型，建模声学信号与文本的发音特征的联合概率，但 pipeline 的不同部分掺杂了不同的机器学习模型：

![hankcs.com 2017-07-01 上午 10.49.02.png](img/7756799d42509655937c1f63ee900df7.jpg "hankcs.com 2017-07-01 上午 10.49.02.png")

### 近现代 ASR

神经网络兴起之后，人们发现传统 pipeline 中的每个模型都可以被一种对应的神经网络所替代，并且取得更好的效果：

![hankcs.com 2017-07-01 上午 10.52.23.png](img/6358fdd74c09183c2d424892176b35a4.jpg "hankcs.com 2017-07-01 上午 10.52.23.png")

但是这么多混乱的目标函数各自为政，难免有顾此失彼的情况。这构成一种动机，为什么不用一个统一的大模型来取代这盘散沙呢？

### end-to-end ASR

直接从音频到字符的概率模型![](img/ab1c6f2da357d0c72831fb682b6c36fe.jpg)：

![hankcs.com 2017-07-01 上午 10.58.05.png](img/7a47ac7da5b7ac863db614ac43169093.jpg "hankcs.com 2017-07-01 上午 10.58.05.png")

（上半部分与生成式模型作对比）

## Connectionist Temporal Classification

这是一种强大的概率模型，特别适用于语音识别。其主体是一个[Bidirectional RNN](http://www.hankcs.com/nlp/cs224n-rnn-and-language-models.html#h3-12)，上面是一层 softmax。

![hankcs.com 2017-07-01 上午 11.07.28.png](img/461f52e7680356769641a18ee2cefd4f.jpg "hankcs.com 2017-07-01 上午 11.07.28.png")

词表中还有一个空格<b>，这很重要。

由于语音片段（帧）切割时的随意性，可能导致一个字符 c 被切割为多个帧，每个帧都输出 c。为了区分字符与字符的界限，所以引入空格分隔符。在解码的时候还需要限制字符只能转移到相同的字符，或者空格。

![hankcs.com 2017-07-01 上午 11.15.15.png](img/b6db30c2f8afff2989ab2d3469b70842.jpg "hankcs.com 2017-07-01 上午 11.15.15.png")

解码时的直观演示：

![2017-07-01_11-24-13.png](img/c163fae3ba685449dbd97ff34bfe8d9c.jpg "2017-07-01_11-24-13.png")

灰色线条表示无输出的概率。

### 一些效果

![2017-07-01_11-28-56.png](img/a13ccd3dbb9697a8720e3d7a2941309b.jpg "2017-07-01_11-28-56.png")

![hankcs.com 2017-07-01 下午 4.06.20.png](img/c5df443f3d7a9e03ea7c9087be200e1f.jpg "hankcs.com 2017-07-01 下午 4.06.20.png")

![hankcs.com 2017-07-01 下午 4.07.03.png](img/8da20e15c4acd178e02b0a0eda91cdaa.jpg "hankcs.com 2017-07-01 下午 4.07.03.png")

可见识别结果听上去挺像那么回事，可拼写不正确。Google 通过在训练时集成语言模型进去修正了这些问题。而且不再使用字符级别，而是使用单词级别的大词表，识别出可能的单词后，用语言模型挑出最可能的句子。

![2017-07-01_16-15-12.png](img/d538f360def867f9c43f08948ea4fc29.jpg "2017-07-01_16-15-12.png")

虽然这是个 end-to-end 模型，但还是掺杂了一个语言模型。没有语言模型的帮助，该 CTC 模型无法根据已识别的单词做条件调整下次预测。

## sequence to sequence speech recognition with attention

让语言模型也成为模型天然的一部分，将音频视作 sequence，文本视作另一个 sequence，类似于 NMT 中的 encoder-decoder，LSTM 模型根据之前的 y 和全部 x 预测下一个 y：

![2017-07-01_16-21-00.png](img/1cdcefbc03969015b0701daa7ec78d72.jpg "2017-07-01_16-21-00.png")

一下子把 x 都喂进去后，对于很长的序列来讲，需要做 attention，在不同的时刻关注输入的不同部分：

![hankcs.com 2017-07-01 下午 4.25.45.png](img/a9caa59195112f0fa894cc2417ac1f33.jpg "hankcs.com 2017-07-01 下午 4.25.45.png")

由于是 RNN，所以输入 x 依然不是定长的。

### Listen Attend and Spell

定义 score 函数，接受每个历史时刻的 encoder 隐藏状态![](img/a048a5bfcc0242b6427d15ed11ef7e23.jpg)和 decoder 的当前状态![](img/0492c0bfd615cb5e61c847ece512ff51.jpg)，得到当前应当对每个历史时刻倾注多少注意力。softmax 归一化，加权和得到最终的 context vector，参与预测。

![hankcs.com 2017-07-01 下午 4.32.39.png](img/a19262fa931dd4a12472b2cfe5dedc90.jpg "hankcs.com 2017-07-01 下午 4.32.39.png")

这里的 encoder 是树形的，因为对于较长的语音来讲，要 softmax 的 timestep 实在太多，效率不高、模型注意力被分散。通过用 softmax 把相邻的 timestep 总结一下，提高了效率和效果。

这个模型是强大的，学习到了很多 pattern：

![hankcs.com 2017-07-01 下午 4.40.29.png](img/b014a6bf101ed6595c0720a4ba86ecd9.jpg "hankcs.com 2017-07-01 下午 4.40.29.png")

还可能产生一个读音的不同拼写（取决于早期的预测结果，然后导致不同的 attention）：

![2017-07-01_16-45-01.png](img/a3c73fbc26bb7e793c4fe6de11a815a4.jpg "2017-07-01_16-45-01.png")

![2017-07-01_16-45-28.png](img/f4ab00be821fb8b018b8ed6aa0cc4559.jpg "2017-07-01_16-45-28.png")

### 效果

得到的效果虽然没有超越多年优化的旧模型，但也是一个量级的：

![hankcs.com 2017-07-01 下午 4.47.51.png](img/6ba726f7b0e20785c1c44025685a1067.jpg "hankcs.com 2017-07-01 下午 4.47.51.png")

### LAS 的限制

*   必须等到用户说完话之后才能开始识别

*   attention 是计算瓶颈

*   输入的长度对准确率影响特别大

![hankcs.com 2017-07-01 下午 4.50.42.png](img/3a49ab833f65eac5d3d6e73a90c2337a.jpg "hankcs.com 2017-07-01 下午 4.50.42.png")

## 在线 seq2seq 模型

希望能够即时产生输出，并且不需要在整个 sequence 上分配 attention。

### Neural Transducer

根据一个定长的输入序列片段产生输出，不要要前一个输出，依然需要空白符，依然需要 alignment（哪些字母属于一个词）：

![hankcs.com 2017-07-01 下午 5.05.23.png](img/539868b84d8d55e93ceeb0413d76674b.jpg "hankcs.com 2017-07-01 下午 5.05.23.png")

用空白符隔开的区块只是一个字符，究竟哪些字符成词，又回到了老生常谈的“分词”问题上来。这里采用了柱搜索找出最可能的路径。

![hankcs.com 2017-07-01 下午 5.10.55.png](img/2a328b697281a1fe19e5a4f7533f7457.jpg "hankcs.com 2017-07-01 下午 5.10.55.png")

训练的时候理论上有一个非常复杂的对数似然的梯度，但实际上经常只取对齐，不做识别上的求和：

![hankcs.com 2017-07-01 下午 5.15.10.png](img/6226fcbb4a495fa17697e65852c1d011.jpg "hankcs.com 2017-07-01 下午 5.15.10.png")

对齐的过程类似 viterbi，但并不严格是，我们是在找最优路径，但路径与之前的每个选择都有关。柱搜索不太理想，如果记录到每个 block（字符）为止产生特定数量 token 的最大概率，则可以用动态规划解决：

![hankcs.com 2017-07-01 下午 5.22.56.png](img/9c4faa5d031297e0b09e08a5feda4584.jpg "hankcs.com 2017-07-01 下午 5.22.56.png")

### 结果

![hankcs.com 2017-07-01 下午 5.25.36.png](img/2248eae29b0edc92bb64b49078d99fb3.jpg "hankcs.com 2017-07-01 下午 5.25.36.png")

在有 attention 的情况下，窗口大小影响不大；而在无 attention 的情况下，窗口较小效果较好。

### Encoder 中的卷积

与其简单地层叠两个 timestep，不如喂给很深的卷积网络：

![2017-07-01_19-06-34.png](img/a827c85f95b0b6555adc4c84ab97048b.jpg "2017-07-01_19-06-34.png")

效果显著：

![hankcs.com 2017-07-01 下午 7.08.28.png](img/33eae5c9fff4251809b8f9a15ef2a683.jpg "hankcs.com 2017-07-01 下午 7.08.28.png")

### 目标颗粒度

有很多选择：

![hankcs.com 2017-07-01 下午 7.10.38.png](img/707a19d8f29ee61b644985a0111e198c.jpg "hankcs.com 2017-07-01 下午 7.10.38.png")

但对语音识别来讲，更有用的是字符的 ngram（相当于音节）：

![hankcs.com 2017-07-01 下午 7.11.43.png](img/7d57c272cf8e0d8a6e4f3b06de31c868.jpg "hankcs.com 2017-07-01 下午 7.11.43.png")

它们有指数级的组合可能，不清楚哪一种是最好的：

![hankcs.com 2017-07-01 下午 7.14.31.png](img/d5d2fc71d49dfe59b2bac6de56f7dd98.jpg "hankcs.com 2017-07-01 下午 7.14.31.png")

对于 end-to-end 模型来讲，常用的手法是由模型自动决定 ngram 的分割：

![hankcs.com 2017-07-01 下午 7.15.34.png](img/bd7952ed8c3c22ba900f56994a06cbb6.jpg "hankcs.com 2017-07-01 下午 7.15.34.png")

### 效果

![hankcs.com 2017-07-01 下午 7.18.36.png](img/e80d5ed34a983ed5baa3ee8c8d381eb0.jpg "hankcs.com 2017-07-01 下午 7.18.36.png")

上表的 ngram 代表“最大产生 ngram”的意思。

![hankcs.com 2017-07-01 下午 7.19.47.png](img/ce7ca726126a75f76e5573d2b9c2590f.jpg "hankcs.com 2017-07-01 下午 7.19.47.png")

### 模型缺点

在句子开头和人名地名处困惑度较高：

![hankcs.com 2017-07-01 下午 7.23.08.png](img/099593480a5bb6b281edbbde930db4d4.jpg "hankcs.com 2017-07-01 下午 7.23.08.png")

在词语分界处的错误自信会导致搜索时的错误，连语言模型也无力回天。

### 解决办法

通过惩罚 softmax 输出概率 1 来 Entropy Regularization 正则化模型，可以克服这个问题：

![hankcs.com 2017-07-01 下午 7.26.50.png](img/28533b96bb3f86c8d5d57f5c08d74448.jpg "hankcs.com 2017-07-01 下午 7.26.50.png")

与其直接 ER，不如让输出的分布尽量与 Unigram 的分布相似，这样效果更好了。

### 另一个缺点

另一个问题是，模型偏向于惩罚生成很长的输出，这对很长的输入来讲会出现提前终止输出的情况：

![hankcs.com 2017-07-01 下午 7.30.51.png](img/2ba9121929b0077e0ee9289b29de4f1e.jpg "hankcs.com 2017-07-01 下午 7.30.51.png")

比如：

![hankcs.com 2017-07-01 下午 7.32.21.png](img/4e110360e73595b2aadc4c1fed39a1af.jpg "hankcs.com 2017-07-01 下午 7.32.21.png")

解决办法是在预测时惩罚那些不看输入的情况，未处理的输入越多，惩罚越大。

这种粗暴的方法还是取得了一些效果提升：

![hankcs.com 2017-07-01 下午 7.36.12.png](img/7e594d8b8a7cadb4f5761a78c5df7686.jpg "hankcs.com 2017-07-01 下午 7.36.12.png")

### Better Language Model Blending

标注音频-字幕数据毕竟不如海量的未标注文本多，而 end-to-end 模型是一个自治的大模型，内部隐式地存在通过标注数据学习到的语言模型。在哪里如何与外部语言模型混合呢？

答案是在 decoder 的 softmax 预测结果的对数概率上线性混合：

![2017-07-01_19-41-02.png](img/bdebb595912e685f65663aa88e7eb06d.jpg "2017-07-01_19-41-02.png")

还有很多种混合手段，也是个新的前沿课题。

### Better Sequence Training

上节课提到的 ground truth 问题，除了 scheduled sampling 之外，还有一些拓展。比如 Reinforement Learning 之类（草草提了两句）。

## 机会

一些研究方向了。

### 多音源

鸡尾酒舞会上有很多人说话，能否都识别出来呢？以前的生成式模型心中有一个固定的模式去生成数据与输入对比，不适合这个任务。现在常用的判别式模型反过来，以输入特征预测结果，应该可以做出以前做不到的成绩。

![hankcs.com 2017-07-01 下午 7.51.52.png](img/eedf312fa7c4baf6861b25e27c22d845.jpg "hankcs.com 2017-07-01 下午 7.51.52.png")

### "同声传译"

接受法语音频，直接输出英文文本。相当于将上面提到的模型与 MT 模型 blend 到一起了：

![hankcs.com 2017-07-01 下午 7.53.43.png](img/2d531efcc326eb89a3e6d54f7215c82b.jpg "hankcs.com 2017-07-01 下午 7.53.43.png")

两者分别对原文和音频的对齐是非常类似的：

![hankcs.com 2017-07-01 下午 7.55.27.png](img/aa05906c067b763abaabd083b86d85e5.jpg "hankcs.com 2017-07-01 下午 7.55.27.png")

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 12 语音识别的 end-to-end 模型](http://www.hankcs.com/nlp/cs224n-end-to-end-asr.html)