# CS224n 笔记 8 RNN 和语言模型

![deepbirnn.png](img/69a3fec7e29c69598694e60080bf0b07.jpg "deepbirnn.png")这次课推导 RNN，介绍各种训练技巧和拓展变种。梯度消失的推导很详细，用 Python 演示很直观，也给出了用裁剪防止梯度爆炸的直观解释。笔记里还补充了用于机器翻译时的 5 项改进。![cliping.png](img/fc512d168a34fad2aca8550c9f930907.jpg "cliping.png")![en_decoder.png](img/69a86c492f47b65a2ae1c3567d73ce57.jpg "en_decoder.png")

## 语言模型

语言模型就是计算一个单词序列（句子）的概率（![](img/8ade04732b1c2ff7b49efcdeea876dfd.jpg)）的模型。听上去很简单，做起来很难；听上去没什么用处，但用处非常多。比如在机器翻译中，判断译文序列中一种词序的自然程度高于另一种，判断一种用词选择优于另一种。

### 传统语言模型

为了简化问题，必须引入马尔科夫假设，句子的概率通常是通过待预测单词之前长度为![](img/493731e423d5db62086d0b8705dda0c8.jpg)的窗口建立条件概率来预测：

![](img/98068b2769af1e9e710443e3fca2904d.jpg)

为了估计此条件概率，常用极大似然估计，比如对于 BiGram 和[TriGram](http://www.hankcs.com/nlp/segment/second-order-hidden-markov-model-trigram-chinese-participle.html)模型，有：

![](img/8e9440146ff94ad6cda50cc0ebd3165b.jpg)

课件中写的 unigrams and bigrams (conditioning on one/two previous word(s))，是错误的。

在数据量足够的情况下，n-gram 中的 n 越大，模型效果越好。但实际上，数据量总是不如人意，这时候一些平滑方法就不可或缺。另外，这些 ngram 可能会占用上 G 的内存，在最新的研究中，一个 1260 亿的语料在 140G 内存的单机上花了 2.8 天才得到结果。

Bengio et al 提出了第一个大规模深度学习自然语言处理模型，只不过是用前![](img/493731e423d5db62086d0b8705dda0c8.jpg)个单词的词向量来做同样的事情（上文建模）而已，其网络结构如下：

![bengio_03.png](img/835e84176efb070a9fd01fe1d4336da3.jpg "bengio_03.png")

公式如下：

![](img/3d75d37a02fc7e9d0fee37219e8f3bb7.jpg)

这里![](img/e0690120cec341b3d35b3addbad7a059.jpg)就是前![](img/493731e423d5db62086d0b8705dda0c8.jpg)个单词词向量的线性运算，虽然这种模型名字里有“Neural”，但依然属于传统模型。

## Recurrent Neural Networks

新的语言模型是利用 RNN 对序列建模，复用不同时刻的线性非线性单元及权值，理论上之前所有的单词都会影响到预测单词。

![rnn.png](img/3829cd604ec44eda3fe3443a5f0dd80b.jpg "rnn.png")

所需内存只与词表大小成正比，不取决于序列长度。

给定一个词向量序列： ![](img/6f6080c51a8ae8175a4c0fd98a82a524.jpg) ，在每个时间点上都有隐藏层的特征表示：

![](img/68512a3c9d3d29c13ff39c67f735c7da.jpg)

其中，

![](img/6e0753bdbbb994801d2f348e461c9ac4.jpg)：是时间![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)时输入的单词的词向量。

![](img/391ee77b1141953e64046a91f3425f9c.jpg)：用来 condition 输入词向量![](img/22c5ed7653e3fae804006a00210327fc.jpg)的的权值矩阵。

![](img/a03c3ab8cef13804ce509f502e12269c.jpg)：用来 condition 前一个时间节点隐藏层特征表示![](img/6c55220e47309a34947d09d7afff6d0d.jpg)的权值矩阵。

![](img/3712f0ae8c39f12487a8ff881ac1af2c.jpg): 前一个时间点 ![](img/fe941e7225a84fbe8bc02f88886a6fe1.jpg) 的非线性激活函数的输出， ![](img/c50851e263ffcf7ea41dea4cc1acb91f.jpg) 是时间点 ![](img/bf4e64a1556434de3b95763cb674cbaa.jpg) 时的隐藏层初始状态。

![](img/c820076a1af7da0bdf9a00e5091cda48.jpg): 非线性激活函数(sigmoid)

![](img/c81969e7bb0a69b40129d4b07f2e72f6.jpg)：在时刻![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)时输出的整个词表 ![](img/b460e90cd7a42b5c157177d8fb818db7.jpg) 上的概率分布，![](img/a2429568b26fecf6ff5ed8bfbc4767ba.jpg)是给定上文![](img/6c55220e47309a34947d09d7afff6d0d.jpg)和最近的单词 ![](img/fedcfa2869ac5659a86e70574d0c765f.jpg)预测的下一个单词。其中![](img/6a211c97484f90b36802494d636343c5.jpg) ， ![](img/b79ecaabee3090e6ad382395d6e374cf.jpg)。

另外，这里的 ![](img/b6bd995a7b4c0eea0dd8416b61293ab7.jpg) 与拼接两个向量乘以权值矩阵的拼接是一样的。

未 unroll 的网络：

![rnn_node.png](img/0f63f2b1028cb33651d1009cc106d5aa.jpg "rnn_node.png")

等效于

![rnn_loop.png](img/ee0f9fd001dc59f986a85d9cab4ebf92.jpg "rnn_loop.png")

### 损失函数

分类问题中常见的交叉熵损失函数：

![](img/f24b7623b75c1298881b9ca07eddb544.jpg)

在大小为![](img/5a047a5ca04e45726dba21b8302977da.jpg)的整个语料上的交叉熵误差为：

![](img/c0b197f3d2d804c0c057ceb8a6fca93a.jpg)

如果以![](img/734964e5d1e04eb0b061f9ddcb3cc5e1.jpg)为底数会得到“perplexity 困惑度”，代表模型下结论时的困惑程度，越小越好：

![](img/7b13b4b493c259289a7616e66930e579.jpg)

### 训练 RNN 很难

观察句子 1：

"Jane walked into the room. John walked in too. Jane said hi to ___"

以及句子 2：

"Jane walked into the room. John walked in too. It was late in the day, and everyone was walking home after a long day at work. Jane said hi to ___"

人类可以轻松地在两个空中填入“John”这个答案，但 RNN 却很难做对第二个。这是因为在前向传播的时候，前面的![](img/40779fc60a53ff2b70f832ec10cade09.jpg)反复乘上![](img/90490a34512e9bd1843ed4da713d0813.jpg)，导致对后面的影响很小。

反向传播时也是如此。

整个序列的预测误差是之前每个时刻的误差之和：

![](img/f8a394f9b4d4145a100c2d1fb3525e53.jpg)

而每个时刻![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)的误差又是之前每个时刻的误差之和，应用链式法则：

![](img/6169f522563dc3a4cec3c25c54488b91.jpg)

此处的![](img/7f8690a76373a1724cf782f70b7f9c35.jpg)指的是在![](img/2dcd14680bb8255cd44cf9d666435da6.jpg)时间区域上应用链式法则：

![](img/6c7881cc867c5e2a1d44a93ff1daf381.jpg)

由于![](img/d8db1afff92c41b61796b58d8a9c6088.jpg)，每个 ![](img/26360f0a1aa6804379c55916a84865ed.jpg) 就是![](img/7cb1807f05c90f9cdce1f91ba4cee58d.jpg)的[雅克比矩阵](https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)（导数矩阵）：

![](img/e364ed33e4553256b73c8407be6ff46c.jpg)

将这几个式子写在一起，得到：

![](img/6d20734615a42583303281e9268a52d7.jpg)

这个式子最中间的部分最值得关注，因为它是一个连乘的形式，长度为时间区域的长度。记![](img/c64d35b6662b9d71c5fd5f3c1f73f8b5.jpg) 和 ![](img/7e220a21772471c8f59a8c434042477d.jpg)分别为矩阵和向量的范数(L2)，则上述雅克比矩阵的范数满足：

![](img/bb724e7fff9ab0e5332525e9827a2b2e.jpg)

由于使用了 sigmoid 激活函数，所以![](img/4e98a11db6e3138e7258b30dc7cc0bda.jpg)的矩阵范数最大只能为 1，所以链式法则利用上式![](img/f0d474da300e6d25261fe8409d2f0e72.jpg)次后得到一个更松弛的上界：

![](img/2fac4520b5a8175c035de45e887e74c7.jpg)

指数项![](img/19b6a126826649756a62c1e3dc44e397.jpg)在![](img/ae55b9a4bc2086db5121d1206eb3912d.jpg)显著地大于或小于 1 的时候，经过足够多的![](img/f0d474da300e6d25261fe8409d2f0e72.jpg)次乘法之后就会趋近于 0 或无穷大。小于 1 更常见，会导致很长时间之前的词语无法影响对当前词语的预测。

而大于 1 时，浮点数运算会产生溢出（NaN），一般可以很快发现。这叫做梯度爆炸。小于 1，或者下溢出并不产生异常，难以发现，但会显著降低模型对较远单词的记忆效果，这叫做梯度消失。

矩阵范数可参考：![](img/b126cdf7458bf8e06dec75275f5faa59.jpg)[1.4 向量和矩阵的范数.pdf](http://www.hankcs.com/wp-content/uploads/2017/06/1.4-%E5%90%91%E9%87%8F%E5%92%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E8%8C%83%E6%95%B0.pdf "1.4 向量和矩阵的范数.pdf")

### 梯度消失实例

有个[IPython Notebook](http://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)专门演示梯度消失，对于如下数据：

![1.png](img/43d5477dc86b54cb6f0ca92e737203fe.jpg "1.png")

学习非线性的决策边界：

![boundry.png](img/ac9ada7fc6be44a7cc8bfa6df0c05c1f.jpg "boundry.png")

用经典的三层网络结构，得到蓝色的第一层梯度的长度和绿色的第二层梯度的长度，可视化：

sigmoid 激活函数下：

![2.png](img/6b2088a4fceffc3a33539902dbeda4d2.jpg "2.png")

ReLU 激活函数下：

![3.png](img/24a557553b4b08f2bd61845f58bf71d3.jpg "3.png")

在这个例子的反向传播中，相邻两层梯度是近乎减半地减小。

### 防止梯度爆炸

一种暴力的方法是，当梯度的长度大于某个阈值的时候，将其缩放到某个阈值。虽然在数学上非常丑陋，但实践效果挺好。

其直观解释是，在一个只有一个隐藏节点的网络中，损失函数和权值 w 偏置 b 构成 error surface，其中有一堵墙：

![cliping.png](img/fc512d168a34fad2aca8550c9f930907.jpg "cliping.png")

每次迭代梯度本来是正常的，一次一小步，但遇到这堵墙之后突然梯度爆炸到非常大，可能指向一个莫名其妙的地方（实线长箭头）。但缩放之后，能够把这种误导控制在可接受的范围内（虚线短箭头）。

但这种 trick 无法推广到梯度消失，因为你不想设置一个最低值硬性规定之前的单词都相同重要地影响当前单词。

### 减缓梯度消失

与其随机初始化参数矩阵，不如初始化为单位矩阵。这样初始效果就是上下文向量和词向量的平均。然后用 ReLU 激活函数。这样可以在 step 多了之后，依然使得模型可训练。

![hankcs.com 2017-06-22 下午 8.04.04.png](img/e6005a70cfa52eb0052609de7b58a5f6.jpg "hankcs.com 2017-06-22 下午 8.04.04.png")

### 困惑度结果

![hankcs.com 2017-06-22 下午 8.07.09.png](img/3db03f125bc95a48ae06574dc3d1484f.jpg "hankcs.com 2017-06-22 下午 8.07.09.png")

相较于 NGram，RNN 的困惑度要小一些。

### 问题：softmax 太大了

词表太大的话，softmax 很费力。一个技巧是，先预测词语的分类（比如按词频分），然后在分类中预测词语。分类越多，困惑度越小，但速度越慢。所以存在一个平衡点：

![hankcs.com 2017-06-22 下午 8.12.05.png](img/b69787c4c63cba30c3c8ba121045e268.jpg "hankcs.com 2017-06-22 下午 8.12.05.png")

### 最后的实现技巧

记录每个![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)的误差不要丢，反向传播的时候将其累加起来。

## 序列模型的应用

可以把每个词分类到 NER、实体级别的情感分析（饭菜味道不错，但环境不太卫生）、意见表达。

其中，意见挖掘任务就是将每个词语归类为：

DSE：直接主观描述（明确表达观点等）

ESE：间接主观描述（间接地表达情感等）

语料标注采用经典的 BIO 标注：

![hankcs.com 2017-06-22 下午 8.23.51.png](img/7216855aceb65f33a20fc32f283f7015.jpg "hankcs.com 2017-06-22 下午 8.23.51.png")

实现这个任务的朴素网络结构就是一个裸的 RNN：

![hankcs.com 2017-06-22 下午 8.29.08.png](img/9e3db9a0d39dd90d57919316bcab9316.jpg "hankcs.com 2017-06-22 下午 8.29.08.png")

但是这个网络无法利用当前词语的下文辅助分类决策，解决方法是使用一些更复杂的 RNN 变种。

### Bidirectional RNNs

![birnn.png](img/c55343aa575d2e28816ef02b29bab15a.jpg "birnn.png")

![](img/11d1e3d6350309cb58d9b19286d9ef1d.jpg)

![](img/7881dfe7a4ff75897dc54a16c2645f4e.jpg)

![](img/ccb4cf43b99ba05ce132073a03dc94ef.jpg)

这里箭头表示从左到右或从右到左前向传播，对于每个时刻![](img/654b00d1036ba7f7d93e02f57fc00a75.jpg)的预测，都需要来自双向的特征向量，拼接后进行分类。箭头虽然不同，但参数还是同一套参数（有些地方是两套参数[1]  G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural Architectures for Named Entity Recognition.,” HLT-NAACL, 2016.）。

### Deep Bidirectional RNNs

理解了上图之后，再加几个层，每个时刻不但接受上个时刻的特征向量，还接受来自下层的特征表示：

![deepbirnn.png](img/69a3fec7e29c69598694e60080bf0b07.jpg "deepbirnn.png")

![](img/60c54b172ddd7ec99428c8302ca5bddb.jpg)

![](img/9c0a928944a2304d67a7faedec7b3f2d.jpg)

![](img/8fcfda5e458dfc7747c8ed04e6b459c0.jpg)

### 评测

评测方法是标准的 F1（因为标签样本不均衡），在不同规模的语料上试验不同层数的影响：

![hankcs.com 2017-06-22 下午 8.51.29.png](img/6b954ca3b5dcc52d58da982ed512bf82.jpg "hankcs.com 2017-06-22 下午 8.51.29.png")

可见层数不是越多越好。

### 应用：RNN 机器翻译模型

传统机器翻译模型在不同的阶段用到大量不同的机器学习算法，这里讨论用 RNN 统一整个流水线。

比如将 3 个单词的德语翻译为 2 个单词的英语，用到如下 RNN：

![rnn_translate.png](img/7f0f5092961f5e3cb5dcb5e1686e8205.jpg "rnn_translate.png")

其中：

![](img/80ff5109d08c73995e3a033643abfd81.jpg)

![](img/3756d62618b4bbb2dea93bc762d53965.jpg)

![](img/bd7c120f6230b4de4988d270a27107a5.jpg)

用来处理输入的叫 encoder，生成输出的叫 decoder。训练时使用交叉熵损失函数，最大化对数似然：

![](img/e41e2c1f8165b2194123c37953e331cc.jpg)

但预测准确率不理想，有如下拓展方法：

1、encoder 和 decoder 使用不同的权值矩阵。也就是上述两个![](img/025666040cfc8ba128c1e6769879104f.jpg)不再相同。

2、decoder 中的隐藏层的输入来自 3 个方面：

*   前一个时刻的隐藏层

*   encoder 的最后一个隐藏层(![](img/520cff0b26c88eae410167696dcbcacf.jpg))

*   前一个预测结果 ![](img/55340d76d20208c4cd542785e7ee6b6a.jpg)

    这样导致 decoder 函数变为：

    ![](img/fec7cfcdb13145962a40e74209142b98.jpg)

*   ![en_decoder.png](img/69a86c492f47b65a2ae1c3567d73ce57.jpg "en_decoder.png")

3、使用深度 RNN

4、使用 bi-directional encoder

5、不再用![](img/75716f1742c4a5adfb3884568a0bf366.jpg)作为训练实例，而是逆转原文词序：![](img/c59aac65bf255b183d0e30ff7aae7e43.jpg)。因为 A 更可能翻译为 X，而梯度消失导致 A 无法影响输出，倒过来 A 离输出近一些。

## 回顾

*   RNN 是最好的 DeepNLP 模型之一

*   因为梯度消失和梯度爆炸，训练很难

*   可以用很多技巧来训练

*   下次课将介绍更强大的 RNN 拓展：LSTM 和 GRU

![知识共享许可协议](http://www.hankcs.com/license/) [知识共享署名-非商业性使用-相同方式共享](http://www.hankcs.com/license/)：[码农场](http://www.hankcs.com) » [CS224n 笔记 8 RNN 和语言模型](http://www.hankcs.com/nlp/cs224n-rnn-and-language-models.html)