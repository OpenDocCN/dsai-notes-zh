# 10 -- Logistic Regression

上一节课，我们介绍了Linear Regression线性回归，以及用平方错误来寻找最佳的权重向量w，获得最好的线性预测。本节课将介绍Logistic Regression逻辑回归问题。

### **一、Logistic Regression Problem**

一个心脏病预测的问题：根据患者的年龄、血压、体重等信息，来预测患者是否会有心脏病。很明显这是一个二分类问题，其输出y只有{-1,1}两种情况。

二元分类，一般情况下，理想的目标函数f(x)&gt;0.5，则判断为正类1；若f(x)&lt;0.5，则判断为负类-1。

![这里写图片描述](img/a62d3d608b956e0c0157b33f6d07b26c.jpg)

但是，如果我们想知道的不是患者有没有心脏病，而是到底患者有多大的几率是心脏病。这表示，我们更关心的是目标函数的值（分布在0,1之间），表示是正类的概率（正类表示是心脏病）。这跟我们原来讨论的二分类问题不太一样，我们把这个问题称为软性二分类问题（’soft’ binary classification）。这个值越接近1，表示正类的可能性越大；越接近0，表示负类的可能性越大。

![这里写图片描述](img/b968cb65873dcec5948cbd6423406623.jpg)

对于软性二分类问题，理想的数据是分布在[0,1]之间的具体值，但是实际中的数据只可能是0或者1，我们可以把实际中的数据看成是理想数据加上了噪声的影响。

![这里写图片描述](img/cab07af3b8e9b21b7e318e7e296bb02a.jpg)

如果目标函数是![](img/41d27a7fd83c6e26af7b6859364cb49f.jpg)的话，我们如何找到一个好的Hypothesis跟这个目标函数很接近呢？

首先，根据我们之前的做法，对所有的特征值进行加权处理。计算的结果s，我们称之为’risk score’：

![这里写图片描述](img/118b641a894ff6c2ec2c602f52cc6f18.jpg)

但是特征加权和![](img/5fac02bb7c0dc1f050159f40b254aa1f.jpg)，如何将s值限定在[0,1]之间呢？一个方法是使用sigmoid Function，记为![](img/70347051068937d1d9f55806c9c83c15.jpg)。那么我们的目标就是找到一个hypothesis：![](img/51c6c4253f728a91b431ee87201d2767.jpg)。

![这里写图片描述](img/3e60dc67cfcb397d1fe593241525edae.jpg)

Sigmoid Function函数记为![](img/70347051068937d1d9f55806c9c83c15.jpg)，满足![](img/2cb14c9b98853f1b19ea5e02d757ca1d.jpg)，![](img/bd65a58a1f5dd55dd89adc05726d6953.jpg)，![](img/b23f0deb6a9de9114d7955b2d2028da7.jpg)。这个函数是平滑的、单调的S型函数。则对于逻辑回归问题，hypothesis就是这样的形式：

那我们的目标就是求出这个预测函数h(x)，使它接近目标函数f(x)。

### **二、Logistic Regression Error**

现在我们将Logistic Regression与之前讲的Linear Classification、Linear Regression做个比较：

![这里写图片描述](img/bfe45f2f9daf6bfc0413537d98518b59.jpg)

这三个线性模型都会用到线性scoring function ![](img/1766437ebeef6f9f34934af4df70d78f.jpg)。linear classification的误差使用的是0/1 err；linear regression的误差使用的是squared err。那么logistic regression的误差该如何定义呢？

先介绍一下“似然性”的概念。目标函数![](img/417c444cdd27e3a8720f901d62f00d38.jpg)，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。

![这里写图片描述](img/8ccc2a7f1c30012797073931feeb93cc.jpg)

logistic function: ![](img/51c6c4253f728a91b431ee87201d2767.jpg)满足一个性质：![](img/76c1848fde1f18040a7b7b2a223f839a.jpg)。那么，似然性h:

因为![](img/af443a67434ce6e2fcb484ce50e43ac9.jpg)对所有的h来说，都是一样的，所以我们可以忽略它。那么我们可以得到logistic h正比于所有的![](img/c41d5001c6d9f2779d904b25ed3c1e3f.jpg)乘积。我们的目标就是让乘积值最大化。

![这里写图片描述](img/1808f84b0f3196d54f04c204bc5b2fa7.jpg)

如果将w代入的话：

![这里写图片描述](img/c161ef70b9c994e4cde7ec416b4f8bad.jpg)

为了把连乘问题简化计算，我们可以引入ln操作，让连乘转化为连加：

![这里写图片描述](img/53a1c6f6bc89d08860ac841fe1a0d5ee.jpg)

接着，我们将maximize问题转化为minimize问题，添加一个负号就行，并引入平均数操作![](img/7dc6d0af4e9649e880651a0164c15c03.jpg)：

![这里写图片描述](img/51473a82241562066bf7fafe5b414433.jpg)

将logistic function的表达式带入，那么minimize问题就会转化为如下形式：

![这里写图片描述](img/f037ab828c2bfdaf41bfede49640a1b2.jpg)

至此，我们得到了logistic regression的err function，称之为cross-entropy error交叉熵误差：

![这里写图片描述](img/89a22492d5cf7c0395f4d6a3e9ea49e7.jpg)

### **三、Gradient of Logistic Regression Error**

我们已经推导了![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)的表达式，那接下来的问题就是如何找到合适的向量w，让![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)最小。

![这里写图片描述](img/3d4fd7008e03b9f11ab5278f13e63579.jpg)

Logistic Regression的![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)是连续、可微、二次可微的凸曲线（开口向上），根据之前Linear Regression的思路，我们只要计算![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)的梯度为零时的w，即为最优解。

![这里写图片描述](img/29fe34b866aef9ce35e6615b46431af1.jpg)

对![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)计算梯度，学过微积分的都应该很容易计算出来：

![这里写图片描述](img/82807dd6db78e0a6a5fb81cc915661cc.jpg)

最终得到的梯度表达式为：

![这里写图片描述](img/86d524e26cf5109df12114bb22de7d00.jpg)

为了计算![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)最小值，我们就要找到让![](img/18bb20e9c4b9f13aa1bf1ae4c98f7902.jpg)等于0的位置。

![这里写图片描述](img/97b5bc63285565f3822573b315f1568b.jpg)

上式可以看成![](img/4cf91e9cef35d03bcbb859c9bc4b51c9.jpg)是![](img/3cca114c4b46e778743527e6ce4573b7.jpg)的线性加权。要求![](img/4cf91e9cef35d03bcbb859c9bc4b51c9.jpg)与![](img/3cca114c4b46e778743527e6ce4573b7.jpg)的线性加权和为0，那么一种情况是线性可分，如果所有的权重![](img/4cf91e9cef35d03bcbb859c9bc4b51c9.jpg)为0，那就能保证![](img/18bb20e9c4b9f13aa1bf1ae4c98f7902.jpg)为0。![](img/4cf91e9cef35d03bcbb859c9bc4b51c9.jpg)是sigmoid function，根据其特性，只要让![](img/3be5ae5c387d7b1e805a95159f3e28c3.jpg)，即![](img/2ff5dd10987b183197e3d1c1e6dfb707.jpg)。![](img/2ff5dd10987b183197e3d1c1e6dfb707.jpg)表示对于所有的点，![](img/3c9ea39d24b4cbca9a352ceebb099051.jpg)与![](img/97160f729af879f33f6c9376bcbc0ee6.jpg)都是同号的，这表示数据集D必须是全部线性可分的才能成立。

然而，保证所有的权重![](img/4cf91e9cef35d03bcbb859c9bc4b51c9.jpg)为0是不太现实的，总有不等于0的时候，那么另一种常见的情况是非线性可分，只能通过使加权和为零，来求解w。这种情况没有closed-form解，与Linear Regression不同，只能用迭代方法求解。

![这里写图片描述](img/b687706131bfc1a3143e7def6f62ca87.jpg)

之前所说的Linear Regression有closed-form解，可以说是“一步登天”的；但是PLA算法是一步一步修正迭代进行的，每次对错误点进行修正，不断更新w值。PLA的迭代优化过程表示如下：

![这里写图片描述](img/6f81fbbd2a583c71f5db98706430a67e.jpg)

w每次更新包含两个内容：一个是每次更新的方向![](img/3c9ea39d24b4cbca9a352ceebb099051.jpg)，用![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)表示，另一个是每次更新的步长![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)。参数![](img/956770eb609290b6d8d8751e49f8f959.jpg)和终止条件决定了我们的迭代优化算法。

![这里写图片描述](img/5a1c43ae8ce8c51043185cf99e819323.jpg)

### **四、Gradient Descent**

根据上一小节PLA的思想，迭代优化让每次w都有更新：

![这里写图片描述](img/17e2ae237e67f2c65350099c2802d72d.jpg)

我们把![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)曲线看做是一个山谷的话，要求![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)最小，即可比作下山的过程。整个下山过程由两个因素影响：一个是下山的单位方向![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)；另外一个是下山的步长![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)。

![这里写图片描述](img/453c0f9c00d390b413452f83e523bf68.jpg)

利用微分思想和线性近似，假设每次下山我们只前进一小步，即![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)很小，那么根据泰勒Taylor一阶展开，可以得到：

关于Taylor展开的介绍，可参考我另一篇博客：
[多元函数的泰勒(Taylor)展开式](http://blog.csdn.net/red_stone1/article/details/70260070)

迭代的目的是让![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)越来越小，即让![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)。![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)是标量，因为如果两个向量方向相反的话，那么他们的内积最小（为负），也就是说如果方向![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)与梯度![](img/d327be654d84911f4e0bcb1620b43767.jpg)反向的话，那么就能保证每次迭代![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)都成立。则，我们令下降方向![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)为：

![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)是单位向量，![](img/4518d62dd616c7117ed0867ca41d62b3.jpg)每次都是沿着梯度的反方向走，这种方法称为梯度下降（gradient descent）算法。那么每次迭代公式就可以写成：

下面讨论一下![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)的大小对迭代优化的影响：![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)如果太小的话，那么下降的速度就会很慢；![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)如果太大的话，那么之前利用Taylor展开的方法就不准了，造成下降很不稳定，甚至会上升。因此，![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)应该选择合适的值，一种方法是在梯度较小的时候，选择小的![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)，梯度较大的时候，选择大的![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)，即![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)正比于![](img/9efb63341adf28f6dc5d37e00a72aacc.jpg)。这样保证了能够快速、稳定地得到最小值![](img/d53537a9fd3d004184f3feaf07b3fd34.jpg)。

![这里写图片描述](img/c096a48b4ff6339c8afd52e46cd6621a.jpg)

对学习速率![](img/23487c6fb6f01b8e086be914c051fc2e.jpg)做个更修正，梯度下降算法的迭代公式可以写成：

其中：

总结一下基于梯度下降的Logistic Regression算法步骤如下：

*   **初始化![](img/62c094df0983581deb7e41b7217d92c1.jpg)**
*   **计算梯度![](img/d327be654d84911f4e0bcb1620b43767.jpg)**
*   **迭代跟新![](img/6f1bf6ee56e78117572316f1f21796da.jpg)**
*   **满足![](img/64d575651785ce9721297e5de4f67412.jpg)或者达到迭代次数，迭代结束**

### **五、总结**

我们今天介绍了Logistic Regression。首先，从逻辑回归的问题出发，将![](img/fbb32254bb8f2a57a0500bc1e957d45c.jpg)作为目标函数，将![](img/a9b8f85a60ddaea5d413ba1da012846c.jpg)作为hypothesis。接着，我们定义了logistic regression的err function，称之为cross-entropy error交叉熵误差。然后，我们计算logistic regression error的梯度，最后，通过梯度下降算法，计算![](img/d327be654d84911f4e0bcb1620b43767.jpg)时对应的![](img/5c934ee50e8ccde6112a6b81b8e76e5d.jpg)值。

**_注明：_**

文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程