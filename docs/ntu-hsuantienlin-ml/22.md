# 4 -- Soft-Margin Support Vector Machine


上节课我们主要介绍了Kernel SVM。先将特征转换和计算内积这两个步骤合并起来，简化计算、提高计算速度，再用Dual SVM的求解方法来解决。Kernel SVM不仅能解决简单的线性分类问题，也可以求解非常复杂甚至是无限多维的分类问题，关键在于核函数的选择，例如线性核函数、多项式核函数和高斯核函数等等。但是，我们之前讲的这些方法都是Hard-Margin SVM，即必须将所有的样本都分类正确才行。这往往需要更多更复杂的特征转换，甚至造成过拟合。本节课将介绍一种Soft-Margin SVM，目的是让分类错误的点越少越好，而不是必须将所有点分类正确，也就是允许有noise存在。这种做法很大程度上不会使模型过于复杂，不会造成过拟合，而且分类效果是令人满意的。

### **Motivation and Primal Problem**

上节课我们说明了一点，就是SVM同样可能会造成overfit。原因有两个，一个是由于我们的SVM模型（即kernel）过于复杂，转换的维度太多，过于powerful了；另外一个是由于我们坚持要将所有的样本都分类正确，即不允许错误存在，造成模型过于复杂。如下图所示，左边的图![](img/de6156a36da5593672299bfefa52b1e5.jpg)是线性的，虽然有几个点分类错误，但是大部分都能完全分开。右边的图![](img/e94099764c799580f1714afd62695462.jpg)是四次多项式，所有点都分类正确了，但是模型比较复杂，可能造成过拟合。直观上来说，左边的图是更合理的模型。

![这里写图片描述](img/cd6990173963357c58b4216bbe0dc59b.jpg)

如何避免过拟合？方法是允许有分类错误的点，即把某些点当作是noise，放弃这些noise点，但是尽量让这些noise个数越少越好。回顾一下我们在机器学习基石笔记中介绍的pocket算法，pocket的思想不是将所有点完全分开，而是找到一条分类线能让分类错误的点最少。而Hard-Margin SVM的目标是将所有点都完全分开，不允许有错误点存在。为了防止过拟合，我们可以借鉴pocket的思想，即允许有犯错误的点，目标是让这些点越少越好。

![这里写图片描述](img/318d64bf8b507135a5da6752c96ee49a.jpg)

为了引入允许犯错误的点，我们将Hard-Margin SVM的目标和条件做一些结合和修正，转换为如下形式：

![这里写图片描述](img/6a4a6ccde66c7955733850f785440cf1.jpg)

修正后的条件中，对于分类正确的点，仍需满足![](img/421c2ca1e48f63ab4e6aeda6aa7dd8e9.jpg)，而对于noise点，满足![](img/75e85e6ae32dc853554b0127bacfbd1c.jpg)，即没有限制。修正后的目标除了![](img/fe7bbafafbb55291e5ff44c29986ee21.jpg)项，还添加了![](img/04b322c7f26d36bf3300ae1606314940.jpg)，即noise点的个数。参数C的引入是为了权衡目标第一项和第二项的关系，即权衡large margin和noise tolerance的关系。

我们再对上述的条件做修正，将两个条件合并，得到：

![这里写图片描述](img/e74611e41c2ff40e124e0d4a052e5243.jpg)

这个式子存在两个不足的地方。首先，最小化目标中第二项是非线性的，不满足QP的条件，所以无法使用dual或者kernel SVM来计算。然后，对于犯错误的点，有的离边界很近，即error小，而有的离边界很远，error很大，上式的条件和目标没有区分small error和large error。这种分类效果是不完美的。

![这里写图片描述](img/66f91e1d0fe6390db2e2c865a2d1ef44.jpg)

为了改正这些不足，我们继续做如下修正：

![这里写图片描述](img/6784a16580e3a2963f8806dee12623a1.jpg)

修正后的表达式中，我们引入了新的参数![](img/40ba8a7a08940acca1b2724689177103.jpg)来表示每个点犯错误的程度值，![](img/1d32e4c7621717e43be3ebb7e4d5ae55.jpg)。通过使用error值的大小代替是否有error，让问题变得易于求解，满足QP形式要求。这种方法类似于我们在机器学习基石笔记中介绍的0/1 error和squared error。这种soft-margin SVM引入新的参数![](img/5c12c10e0a4ede5cb71a28124e734649.jpg)。

至此，最终的Soft-Margin SVM的目标为：

![](img/2794a621c994da4de33da5a82c0e0a3c.jpg)

条件是：

![](img/0b3af39c316106e9e38e58f5384131bd.jpg)

![](img/1d32e4c7621717e43be3ebb7e4d5ae55.jpg)

其中，![](img/40ba8a7a08940acca1b2724689177103.jpg)表示每个点犯错误的程度，![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)，表示没有错误，![](img/40ba8a7a08940acca1b2724689177103.jpg)越大，表示错误越大，即点距离边界（负的）越大。参数C表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡，因为边界宽了，往往犯错误的点会增加。large C表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；small C表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。

与之对应的QP问题中，由于新的参数![](img/40ba8a7a08940acca1b2724689177103.jpg)的引入，总共参数个数为![](img/911d742ee57cd09587062da7fe504fac.jpg)，限制条件添加了![](img/1d32e4c7621717e43be3ebb7e4d5ae55.jpg)，则总条件个数为2N。

![这里写图片描述](img/9f5aa1dd5d52f2bbb5159bc9f584b63d.jpg)

### **Dual Problem**

接下来，我们将推导Soft-Margin SVM的对偶dual形式，从而让QP计算更加简单，并便于引入kernel算法。首先，我们把Soft-Margin SVM的原始形式写出来：

![这里写图片描述](img/2d5e65c361f1f9d330a17d58e8c0739e.jpg)

然后，跟我们在第二节课中介绍的Hard-Margin SVM做法一样，构造一个拉格朗日函数。因为引入了![](img/40ba8a7a08940acca1b2724689177103.jpg)，原始问题有两类条件，所以包含了两个拉格朗日因子![](img/b445498ce7b354c165c2d823865602f9.jpg)和![](img/a5240d5e29bc269362ba121cb320f46f.jpg)。拉格朗日函数可表示为如下形式：

![这里写图片描述](img/0d2870419e389f3520c4e8ca0e22a680.jpg)

接下来，我们跟第二节课中的做法一样，利用Lagrange dual problem，将Soft-Margin SVM问题转换为如下形式：

![这里写图片描述](img/949be96d374573ca885e693f76608bf2.jpg)

根据之前介绍的KKT条件，我们对上式进行简化。上式括号里面的是对拉格朗日函数![](img/116c8d7be75881ba2b3045f13d49238f.jpg)计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。

我们先对![](img/40ba8a7a08940acca1b2724689177103.jpg)做偏微分：

![](img/992c5f80b6848443bd8f3a6a01d1fce7.jpg)

根据上式，得到![](img/3e31d5ab52b3e4109176cda3d1175fb3.jpg)，因为有![](img/dee8eafe087bc432f1b48132de301cdb.jpg)，所以限制![](img/2d5dc37ee169283a647fee95b6c18871.jpg)。将![](img/3e31d5ab52b3e4109176cda3d1175fb3.jpg)代入到dual形式中并化简，我们发现![](img/a5240d5e29bc269362ba121cb320f46f.jpg)和![](img/40ba8a7a08940acca1b2724689177103.jpg)都被消去了：

![这里写图片描述](img/68865f0442c203dba14a1c22b4ccb5bc.jpg)

这个形式跟Hard-Margin SVM中的dual形式是基本一致的，只是条件不同。那么，我们分别令拉个朗日函数L对b和w的偏导数为零，分别得到：

![](img/9c044a4161aafa0fe2d6ec7b962d666d.jpg)

![](img/69bdfefe744163ecac980fd18bac2712.jpg)

经过化简和推导，最终标准的Soft-Margin SVM的Dual形式如下图所示：

![这里写图片描述](img/686a8abb8a2ca7da7b9f4d6f3e5bd4cf.jpg)

Soft-Margin SVM Dual与Hard-Margin SVM Dual基本一致，只有一些条件不同。Hard-Margin SVM Dual中![](img/db6890730ce9a7d6545d124446e7da9e.jpg)，而Soft-Margin SVM Dual中![](img/2d5dc37ee169283a647fee95b6c18871.jpg)，且新的拉格朗日因子![](img/3e31d5ab52b3e4109176cda3d1175fb3.jpg)。在QP问题中，Soft-Margin SVM Dual的参数![](img/b445498ce7b354c165c2d823865602f9.jpg)同样是N个，但是，条件由Hard-Margin SVM Dual中的N+1个变成2N+1个，这是因为多了N个![](img/b445498ce7b354c165c2d823865602f9.jpg)的上界条件。

对于Soft-Margin SVM Dual这部分推导不太清楚的同学，可以看下第二节课的笔记：[2 – Dual Support Vector Machine](http://blog.csdn.net/red_stone1/article/details/73822768)

### **Messages behind Soft-Margin SVM**

推导完Soft-Margin SVM Dual的简化形式后，就可以利用QP，找到Q，p，A，c对应的值，用软件工具包得到![](img/b445498ce7b354c165c2d823865602f9.jpg)的值。或者利用核函数的方式，同样可以简化计算，优化分类效果。Soft-Margin SVM Dual计算![](img/b445498ce7b354c165c2d823865602f9.jpg)的方法过程与Hard-Margin SVM Dual的过程是相同的。

![这里写图片描述](img/865a25e0b5a251e7bb07a6f6c55ed4b1.jpg)

但是如何根据![](img/b445498ce7b354c165c2d823865602f9.jpg)的值计算b呢？在Hard-Margin SVM Dual中，有complementary slackness条件：![](img/143c7a016f60f17ee3ffa7de1c336026.jpg)，找到SV，即![](img/94d62be943cf1eac2b155bbc649d2be5.jpg)的点，计算得到![](img/9cdf066c61c38471de8bc9e7624097fc.jpg)。

那么，在Soft-Margin SVM Dual中，相应的complementary slackness条件有两个（因为两个拉格朗日因子![](img/b445498ce7b354c165c2d823865602f9.jpg)和![](img/a5240d5e29bc269362ba121cb320f46f.jpg)）：

![](img/24badfcf38664366b3a4fa6f8625e2ac.jpg)

![](img/42574ea1314f40bae9a4be3c0a751a8e.jpg)

找到SV，即![](img/94d62be943cf1eac2b155bbc649d2be5.jpg)的点，由于参数![](img/40ba8a7a08940acca1b2724689177103.jpg)的存在，还不能完全计算出b的值。根据第二个complementary slackness条件，如果令![](img/3bc0cb93f55f85cb9924778b3576f443.jpg)，即![](img/427162674635fac93f76073834721530.jpg)，则一定有![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)，代入到第一个complementary slackness条件，即可计算得到![](img/9cdf066c61c38471de8bc9e7624097fc.jpg)。我们把![](img/3288b7bc128237422e3b1610ce23fa59.jpg)的点称为free SV。引入核函数后，b的表达式为：

![](img/757e3f73deab6f426ab099f0acef4fbb.jpg)

上面求解b提到的一个假设是![](img/67609924d00a63fd28c0b6a7c0ddecc3.jpg)，这个假设是否一定满足呢？如果没有free SV，所有![](img/aa69dda237beb633f148bc405725f9bd.jpg)大于零的点都满足![](img/df3197b5d35e22401af3b98f192d2e15.jpg)怎么办？一般情况下，至少存在一组SV使![](img/67609924d00a63fd28c0b6a7c0ddecc3.jpg)的概率是很大的。如果出现没有free SV的情况，那么b通常会由许多不等式条件限制取值范围，值是不确定的，只要能找到其中满足KKT条件的任意一个b值就可以了。这部分细节比较复杂，不再赘述。

![这里写图片描述](img/64bdaf37016f49c3731278d0975fa47e.jpg)

接下来，我们看看C取不同的值对margin的影响。例如，对于Soft-Margin Gaussian SVM，C分别取1，10，100时，相应的margin如下图所示：

![这里写图片描述](img/7fb4b7e89f9d6cdebe896f4d533f403b.jpg)

从上图可以看出，C=1时，margin比较粗，但是分类错误的点也比较多，当C越来越大的时候，margin越来越细，分类错误的点也在减少。正如前面介绍的，C值反映了margin和分类正确的一个权衡。C越小，越倾向于得到粗的margin，宁可增加分类错误的点；C越大，越倾向于得到高的分类正确率，宁可margin很细。我们发现，当C值很大的时候，虽然分类正确率提高，但很可能把noise也进行了处理，从而可能造成过拟合。也就是说Soft-Margin Gaussian SVM同样可能会出现过拟合现象，所以参数![](img/f932b37b23db91b08d2392465c894dac.jpg)的选择非常重要。

我们再来看看![](img/b445498ce7b354c165c2d823865602f9.jpg)取不同值是对应的物理意义。已知![](img/2d5dc37ee169283a647fee95b6c18871.jpg)满足两个complementary slackness条件：

![](img/24badfcf38664366b3a4fa6f8625e2ac.jpg)

![](img/42574ea1314f40bae9a4be3c0a751a8e.jpg)

若![](img/f45cc32c1db76f070988752be8d270ba.jpg)，得![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)。![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)表示该点没有犯错，![](img/f45cc32c1db76f070988752be8d270ba.jpg)表示该点不是SV。所以对应的点在margin之外（或者在margin上），且均分类正确。

若![](img/8a8f150fd55b13c824f70d4ba384f3b3.jpg)，得![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)，且![](img/668d079f15a25b2346fa69e0ee7d687a.jpg)。![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)表示该点没有犯错，![](img/668d079f15a25b2346fa69e0ee7d687a.jpg)表示该点在margin上。这些点即free SV，确定了b的值。

若![](img/6b4b521cb9e0c249f3f44a2f3bcd7f32.jpg)，不能确定![](img/40ba8a7a08940acca1b2724689177103.jpg)是否为零，且得到![](img/c2ab2398f3a7fb3fd1b478ddcef5c222.jpg)，这个式表示该点偏离margin的程度，![](img/40ba8a7a08940acca1b2724689177103.jpg)越大，偏离margin的程度越大。只有当![](img/d969059c4983d25c5a8efdf9836ae4f7.jpg)时，该点落在margin上。所以这种情况对应的点在margin之内负方向（或者在margin上），有分类正确也有分类错误的。这些点称为bounded SV。

所以，在Soft-Margin SVM Dual中，根据![](img/b445498ce7b354c165c2d823865602f9.jpg)的取值，就可以推断数据点在空间的分布情况。

![这里写图片描述](img/62c8dfb45e5e3aa5c7aa32d8f512cf34.jpg)

### **Model Selection**

在Soft-Margin SVM Dual中，kernel的选择、C等参数的选择都非常重要，直接影响分类效果。例如，对于Gaussian SVM，不同的参数![](img/168c59a9de2f3609f916c95bf403ca13.jpg)，会得到不同的margin，如下图所示。

![这里写图片描述](img/f8ff44bdaccd7cca0ef513d4eebfe2cd.jpg)

其中横坐标是C逐渐增大的情况，纵坐标是![](img/cdab9437b701fd21fb3294cfba7c4bc2.jpg)逐渐增大的情况。不同的![](img/168c59a9de2f3609f916c95bf403ca13.jpg)组合，margin的差别很大。那么如何选择最好的![](img/168c59a9de2f3609f916c95bf403ca13.jpg)等参数呢？最简单最好用的工具就是validation。

validation我们在机器学习基石课程中已经介绍过，只需要将由不同![](img/168c59a9de2f3609f916c95bf403ca13.jpg)等参数得到的模型在验证集上进行cross validation，选取![](img/3ec70cba3e47fb34ae8101f333268c3a.jpg)最小的对应的模型就可以了。例如上图中各种![](img/168c59a9de2f3609f916c95bf403ca13.jpg)组合得到的![](img/3ec70cba3e47fb34ae8101f333268c3a.jpg)如下图所示：

![这里写图片描述](img/f23d1837e6cc9503eeeea5bb997e79ca.jpg)

因为左下角的![](img/98b5f675bd3684b307703a98fdb6bde8.jpg)最小，所以就选择该![](img/168c59a9de2f3609f916c95bf403ca13.jpg)对应的模型。通常来说，![](img/98b5f675bd3684b307703a98fdb6bde8.jpg)并不是![](img/168c59a9de2f3609f916c95bf403ca13.jpg)的连续函数，很难使用最优化选择（例如梯度下降）。一般做法是选取不同的离散的![](img/168c59a9de2f3609f916c95bf403ca13.jpg)值进行组合，得到最小的![](img/98b5f675bd3684b307703a98fdb6bde8.jpg)，其对应的模型即为最佳模型。这种算法就是我们之前在机器学习基石中介绍过的V-Fold cross validation，在SVM中使用非常广泛。

V-Fold cross validation的一种极限就是Leave-One-Out CV，也就是验证集只有一个样本。对于SVM问题，它的验证集Error满足：

![](img/30bcf38b5d542ff81622db28bdf66606.jpg)

也就是说留一法验证集Error大小不超过支持向量SV占所有样本的比例。下面做简单的证明。令样本总数为N，对这N个点进行SVM分类后得到margin，假设第N个点![](img/a1ae50efc4bd11027491db192d2c9492.jpg)的![](img/64e4aed9b13374d3c6e1491e186d6908.jpg)，不是SV，即远离margin（正距离）。这时候，如果我们只使用剩下的N-1个点来进行SVM分类，那么第N个点![](img/a1ae50efc4bd11027491db192d2c9492.jpg)必然是分类正确的点，所得的SVM margin跟使用N个点的到的是完全一致的。这是因为我们假设第N个点是non-SV，对SV没有贡献，不影响margin的位置和形状。所以前N-1个点和N个点得到的margin是一样的。

那么，对于non-SV的点，它的![](img/1e110f128239934ca2def200e986b871.jpg)，即对第N个点，它的Error必然为零：

![](img/fb747eae399ee488edbe1900ecaa70f9.jpg)

另一方面，假设第N个点![](img/9ee13b2408bbefa15e6bd967429b5079.jpg)，即对于SV的点，它的Error可能是0，也可能是1，必然有：

![](img/880110535b9ee3640f0002c7d74c6845.jpg)

综上所述，即证明了![](img/30bcf38b5d542ff81622db28bdf66606.jpg)。这符合我们之前得到的结论，即只有SV影响margin，non-SV对margin没有任何影响，可以舍弃。

SV的数量在SVM模型选择中也是很重要的。一般来说，SV越多，表示模型可能越复杂，越有可能会造成过拟合。所以，通常选择SV数量较少的模型，然后在剩下的模型中使用cross-validation，比较选择最佳模型。

### **总结**

本节课主要介绍了Soft-Margin SVM。我们的出发点是与Hard-Margin SVM不同，不一定要将所有的样本点都完全分开，允许有分类错误的点，而使margin比较宽。然后，我们增加了![](img/40ba8a7a08940acca1b2724689177103.jpg)作为分类错误的惩罚项，根据之前介绍的Dual SVM，推导出了Soft-Margin SVM的QP形式。得到的![](img/b445498ce7b354c165c2d823865602f9.jpg)除了要满足大于零，还有一个上界C。接着介绍了通过![](img/b445498ce7b354c165c2d823865602f9.jpg)值的大小，可以将数据点分为三种：non-SVs，free SVs，bounded SVs，这种更清晰的物理解释便于数据分析。最后介绍了如何选择合适的SVM模型，通常的办法是cross-validation和利用SV的数量进行筛选。

**_注明：_**

文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程
