# 14 -- Regularization

上节课我们介绍了过拟合发生的原因：excessive power, stochastic/deterministic noise 和limited data。并介绍了解决overfitting的简单方法。本节课，我们将介绍解决overfitting的另一种非常重要的方法：Regularization规则化。

### **一、Regularized Hypothesis Set**

先来看一个典型的overfitting的例子：

![这里写图片描述](img/0628638c90c433fa742af5d2a184acf1.jpg)

如图所示，在数据量不够大的情况下，如果我们使用一个高阶多项式（图中红色曲线所示），例如10阶，对目标函数（蓝色曲线）进行拟合。拟合曲线波动很大，虽然![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)很小，但是![](img/245882eb75d78bef2732bf34a68dbdd5.jpg)很大，也就造成了过拟合现象。

那么如何对过拟合现象进行修正，使hypothesis更接近于target function呢？一种方法就是regularized fit。

![这里写图片描述](img/6a542844bf67ab123e8b5d250148c429.jpg)

这种方法得到的红色fit曲线，要比overfit的红色曲线平滑很多，更接近与目标函数，它的阶数要更低一些。那么问题就变成了我们要把高阶（10阶）的hypothesis sets转换为低阶（2阶）的hypothesis sets。通过下图我们发现，不同阶数的hypothesis存在如下包含关系：

![这里写图片描述](img/ddf4b7683d609ae8e46bf7519c65cfc1.jpg)

我们发现10阶多项式hypothesis sets里包含了2阶多项式hypothesis sets的所有项，那么在![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)中加入一些限定条件，使它近似为![](img/4866d85ea0db0c723a655ede34a15bac.jpg)即可。这种函数近似曾被称之为不适定问题（ill-posed problem）。

如何从10阶转换为2阶呢？首先，![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)可表示为：

而![](img/4866d85ea0db0c723a655ede34a15bac.jpg)可表示为：

所以，如果限定条件是![](img/2c4e3e868326c65fa8336ad12e03250b.jpg)，那么就有![](img/4866d85ea0db0c723a655ede34a15bac.jpg)。也就是说，对于高阶的hypothesis，为了防止过拟合，我们可以将其高阶部分的权重w限制为0，这样，就相当于从高阶的形式转换为低阶，fit波形更加平滑，不容易发生过拟合。

![这里写图片描述](img/c21531648d050b73ef357391fcd3ae38.jpg)

那有一个问题，令![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)高阶权重w为0，为什么不直接使用![](img/4866d85ea0db0c723a655ede34a15bac.jpg)呢？这样做的目的是拓展我们的视野，为即将讨论的问题做准备。刚刚我们讨论的限制是![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)高阶部分的权重w限制为0，这是比较苛刻的一种限制。下面，我们把这个限制条件变得更宽松一点，即令任意8个权重w为0，并不非要限定![](img/2c4e3e868326c65fa8336ad12e03250b.jpg)，这个Looser Constraint可以写成：

也就只是限定了w不为0的个数，并不限定必须是高阶的w。这种hypothesis记为![](img/4866d85ea0db0c723a655ede34a15bac.jpg)，称为sparse hypothesis set，它与![](img/4866d85ea0db0c723a655ede34a15bac.jpg)和![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)的关系为：

![这里写图片描述](img/58d332e68011bd9a7587aa23e5af6e73.jpg)

Looser Constraint对应的hypothesis应该更好解一些，但事实是sparse hypothesis set ![](img/4866d85ea0db0c723a655ede34a15bac.jpg)被证明也是NP-hard，求解非常困难。所以，还要转换为另一种易于求解的限定条件。

那么，我们寻找一种更容易求解的宽松的限定条件Softer Constraint，即：

其中，C是常数，也就是说，所有的权重w的平方和的大小不超过C，我们把这种hypothesis sets记为![](img/76ba84f2965d55b7076999ff997e88bc.jpg)。

![](img/4866d85ea0db0c723a655ede34a15bac.jpg)与![](img/76ba84f2965d55b7076999ff997e88bc.jpg)的关系是，它们之间有重叠，有交集的部分，但是没有完全包含的关系，也不一定相等。对应![](img/76ba84f2965d55b7076999ff997e88bc.jpg)，C值越大，限定的范围越大，即越宽松：

当C无限大的时候，即限定条件非常宽松，相当于没有加上任何限制，就与![](img/487acc196899db05fb9c2c0bb2cdf37a.jpg)没有什么两样。![](img/76ba84f2965d55b7076999ff997e88bc.jpg)称为regularized hypothesis set，这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重w记为![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)。接下来就要探讨如何求解![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)。

### **二、Weight Decay Regularization**

现在，针对H(c)，即加上限定条件，我们的问题变成：

![这里写图片描述](img/b892f677aa4097a85a0e4b94db4ec9d9.jpg)

我们的目的是计算![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)的最小值，限定条件是![](img/c85f8597ddf70e009d31d40b2e717870.jpg)。这个限定条件从几何角度上的意思是，权重w被限定在半径为![](img/bf4f4dd5c7dee15c74e796844a782030.jpg)的圆内，而球外的w都不符合要求，即便它是靠近![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)梯度为零的w。

![这里写图片描述](img/57da8bcedfb8dbd0ee8f0b300461b013.jpg)

下面用一张图来解释在限定条件下，最小化![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)的过程：

![这里写图片描述](img/5d6299f58bb5b2c913a449372e182016.jpg)

如上图所示，假设在空间中的一点w，根据梯度下降算法，w会朝着![](img/e0c4f909b6260b7712228261220ea320.jpg)的方向移动（图中蓝色箭头指示的方向），在没有限定条件的情况下，w最终会取得最小值![](img/779c9fcfd1d162fd45bc520817c2d761.jpg)，即“谷底”的位置。现在，加上限定条件，即w被限定在半径为![](img/bf4f4dd5c7dee15c74e796844a782030.jpg)的圆内，w距离原点的距离不能超过圆的半径，球如图中红色圆圈所示![](img/d0e69aef985f70d39cca96be12c5ab08.jpg)。那么，这种情况下，w不能到达![](img/779c9fcfd1d162fd45bc520817c2d761.jpg)的位置，最大只能位于圆上，沿着圆的切线方向移动（图中绿色箭头指示的方向）。与绿色向量垂直的向量（图中红色箭头指示的方向）是圆切线的法向量，即w的方向，w不能靠近红色箭头方向移动。那么随着迭代优化过程，只要![](img/e0c4f909b6260b7712228261220ea320.jpg)与w点切线方向不垂直，那么根据向量知识，![](img/e0c4f909b6260b7712228261220ea320.jpg)一定在w点切线方向上有不为零的分量，即w点会继续移动。只有当![](img/e0c4f909b6260b7712228261220ea320.jpg)与绿色切线垂直，即与红色法向量平行的时候，![](img/e0c4f909b6260b7712228261220ea320.jpg)在切线方向上没有不为零的分量了，也就表示这时w达到了最优解的位置。

有了这个平行的概念，我们就得到了获得最优解需要满足的性质：

上面公式中的![](img/b62ab2bbd282a7834aef6436264a5685.jpg)称为Lagrange multiplier，是用来解有条件的最佳化问题常用的数学工具，![](img/e399133eb1cc1132331b957c38495a00.jpg)是方便后面公式推导。那么我们的目标就变成了求解满足上面公式的![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)。

之前我们推导过，线性回归的![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)的表达式为：

计算![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)梯度，并代入到平行条件中，得到：

这是一个线性方程式，直接得到![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)为：

上式中包含了求逆矩阵的过程，因为![](img/76af03082dab295bcef9cf5b63b515bd.jpg)是半正定矩阵，如果![](img/b62ab2bbd282a7834aef6436264a5685.jpg)大于零，那么![](img/76af03082dab295bcef9cf5b63b515bd.jpg)一定是正定矩阵，即一定可逆。另外提一下，统计学上把这叫做ridge regression，可以看成是linear regression的进阶版。

如果对于更一般的情况，例如逻辑回归问题中，![](img/6b4234f8f0d806ba68f82156c014a89b.jpg)不是线性的，那么将其代入平行条件中得到的就不是一个线性方程式，![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)不易求解。下面我们从另一个角度来看一下平行等式：

已知![](img/6b4234f8f0d806ba68f82156c014a89b.jpg)是![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)对![](img/fb11694f0944d2eb1758f4cc4c364ff4.jpg)的导数，而![](img/8df252ee5425a41bf9b81505e6f28d9e.jpg)也可以看成是![](img/3dfa9878ad8ec413def029068dbdde22.jpg)的导数。那么平行等式左边可以看成一个函数的导数，导数为零，即求该函数的最小值。也就是说，问题转换为最小化该函数：

该函数中第二项就是限定条件regularizer，也称为weight-decay regularization。我们把这个函数称为Augmented Error，即![](img/76e2d27174aa547bb043174899910745.jpg)。

如果![](img/b62ab2bbd282a7834aef6436264a5685.jpg)不为零，对应于加上了限定条件，若![](img/b62ab2bbd282a7834aef6436264a5685.jpg)等于零，则对应于没有任何限定条件，问题转换成之前的最小化![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)。

下面给出一个曲线拟合的例子，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)取不同的值时，得到的曲线也不相同：

![这里写图片描述](img/49c4ad53dd71f3006b345747775e309c.jpg)

从图中可以看出，当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)时，发生了过拟合；当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)时，拟合的效果很好；当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)和![](img/b62ab2bbd282a7834aef6436264a5685.jpg)时，发生了欠拟合。我们可以把![](img/b62ab2bbd282a7834aef6436264a5685.jpg)看成是一种penality，即对hypothesis复杂度的惩罚，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)越大，w就越小，对应于C值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。![](img/b62ab2bbd282a7834aef6436264a5685.jpg)一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。

![这里写图片描述](img/01ae691b56f8abcd1ae6029f8d50c118.jpg)

事实上，这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。

我们目前讨论的多项式是形如![](img/a19e53f3d0993b6262026c86ea7afffd.jpg)的形式，若x的范围限定在[-1,1]之间，那么可能导致![](img/fc2348cccc5b4caae74f9510440de722.jpg)相对于低阶的值要小得多，则其对于的w非常大，相当于要给高阶项设置很大的惩罚。为了避免出现这种数据大小差别很大的情况，可以使用Legendre Polynomials代替![](img/a19e53f3d0993b6262026c86ea7afffd.jpg)这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。关于Legendre Polynomials的概念这里不详细介绍，有兴趣的童鞋可以看一下[维基百科](https://en.wikipedia.org/wiki/Legendre_polynomials)。

### **三、Regularization and VC Theory**

下面我们研究一下Regularization与VC理论之间的关系。Augmented Error表达式如下：

VC Bound表示为：

其中![](img/d98819125c64e9033cba9be34ddf5d7e.jpg)表示的是单个hypothesis的复杂度，记为![](img/eae3474bffb2c48404d4a99814d8bb40.jpg)；而![](img/3b273b44626e7829a870a146ba0696c1.jpg)表示整个hypothesis set的复杂度。根据Augmented Error和VC Bound的表达式，![](img/eae3474bffb2c48404d4a99814d8bb40.jpg)包含于![](img/3b273b44626e7829a870a146ba0696c1.jpg)之内，所以，![](img/76e2d27174aa547bb043174899910745.jpg)比![](img/f9a04b9f5a6d023002e116c8c6854dff.jpg)更接近于![](img/245882eb75d78bef2732bf34a68dbdd5.jpg)，即更好地代表![](img/245882eb75d78bef2732bf34a68dbdd5.jpg)，![](img/76e2d27174aa547bb043174899910745.jpg)与![](img/245882eb75d78bef2732bf34a68dbdd5.jpg)之间的误差更小。

![这里写图片描述](img/536e79210fa3326d57c0f931e65a8cc5.jpg)

根据VC Dimension理论，整个hypothesis set的![](img/3ede32294fd7aa1a37b78d4be699aa01.jpg)，这是因为所有的w都考虑了，没有任何限制条件。而引入限定条件的![](img/228d730c8ab17399a7821674c0a3ca34.jpg)，即有效的VC dimension。也就是说，![](img/df8bee418e750c05eb4f3ea0ccac0302.jpg)比较大，因为它代表了整个hypothesis set，但是![](img/9e4c19054920bb696023541e1d6d618c.jpg)比较小，因为由于regularized的影响，限定了w只取一小部分。其中A表示regularized算法。当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)时，有：

这些与实际情况是相符的，比如对多项式拟合模型，当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)时，所有的w都给予考虑，相应的![](img/2ec47ee64765a58de35b14c3ba9951c4.jpg)很大，容易发生过拟合。当![](img/b62ab2bbd282a7834aef6436264a5685.jpg)且越来越大时，很多w将被舍弃，![](img/9e4c19054920bb696023541e1d6d618c.jpg)减小，拟合曲线越来越平滑，容易发生欠拟合。

### **四、General Regularizers**

那么通用的Regularizers，即![](img/eae3474bffb2c48404d4a99814d8bb40.jpg)，应该选择什么样的形式呢？一般地，我们会朝着目标函数的方向进行选取。有三种方式：

*   **target-dependent**

*   **plausible**

*   **friendly**

![这里写图片描述](img/275fd367724df24567a5ebb1880edb89.jpg)

其实这三种方法跟之前error measure类似，其也有三种方法：

*   **user-dependent**

*   **plausible**

*   **friendly**

regularizer与error measure是机器学习模型设计中的重要步骤。

![这里写图片描述](img/f6813d8320c1a435a3c6b47c0cf6d37d.jpg)

接下来，介绍两种Regularizer：L2和L1。L2 Regularizer一般比较通用，其形式如下：

这种形式的regularizer计算的是w的平方和，是凸函数，比较平滑，易于微分，容易进行最优化计算。

L1 Regularizer的表达式如下：

L1计算的不是w的平方和，而是绝对值和，即长度和，也是凸函数。已知![](img/d0e69aef985f70d39cca96be12c5ab08.jpg)围成的是圆形，而![](img/fa3a48f6803dde9d78f97e49d37ff44f.jpg)围成的是正方形，那么在正方形的四个顶点处，是不可微分的（不像圆形，处处可微分）。根据之前介绍的平行等式推导过程，对应这种正方形，它的解大都位于四个顶点处（不太理解，欢迎补充赐教），因为正方形边界处的w绝对值都不为零，若![](img/e0c4f909b6260b7712228261220ea320.jpg)不与其平行，那么w就会向顶点处移动，顶点处的许多w分量为零，所以，L1 Regularizer的解是稀疏的，称为sparsity。优点是计算速度快。

![这里写图片描述](img/635be90afff56a814305942a3e64b8c9.jpg)

下面来看一下![](img/b62ab2bbd282a7834aef6436264a5685.jpg)如何取值，首先，若stochastic noise不同，那么一般情况下，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)取值有如下特点：

![这里写图片描述](img/1b8802523a5d7e7ce17145f7366533a8.jpg)

从图中可以看出，stochastic noise越大，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)越大。

另一种情况，不同的deterministic noise，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)取值有如下特点：

![这里写图片描述](img/47fa19e6f0ca6f5502b098ed7a26f419.jpg)

从图中可以看出，deterministic noise越大，![](img/b62ab2bbd282a7834aef6436264a5685.jpg)越大。

以上两种noise的情况下，都是noise越大，相应的![](img/b62ab2bbd282a7834aef6436264a5685.jpg)也就越大。这也很好理解，如果在开车的情况下，路况也不好，即noise越多，那么就越会踩刹车，这里踩刹车指的就是regularization。但是大多数情况下，noise是不可知的，这种情况下如何选择![](img/b62ab2bbd282a7834aef6436264a5685.jpg)？这部分内容，我们下节课将会讨论。

### **五、总结**

本节课主要介绍了Regularization。首先，原来的hypothesis set加上一些限制条件，就成了Regularized Hypothesis Set。加上限制条件之后，我们就可以把问题转化为![](img/698eb7045506202d5ee9af60ce79dd7c.jpg)最小化问题，即把w的平方加进去。这种过程，实际上回降低VC Dimension。最后，介绍regularization是通用的机器学习工具，设计方法通常包括target-dependent，plausible，friendly等等。下节课将介绍如何选取合适的![](img/b62ab2bbd282a7834aef6436264a5685.jpg)来建立最佳拟合模型。

**_注明：_**

文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程