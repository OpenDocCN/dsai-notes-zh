# 2 -- Dual Support Vector Machine


上节课我们主要介绍了线性支持向量机（Linear Support Vector Machine）。Linear SVM的目标是找出最“胖”的分割线进行正负类的分离，方法是使用二次规划来求出分类线。本节课将从另一个方面入手，研究对偶支持向量机（Dual Support Vector Machine），尝试从新的角度计算得出分类线，推广SVM的应用范围。

### **Motivation of Dual SVM**

首先，我们回顾一下，对于非线性SVM，我们通常可以使用非线性变换将变量从x域转换到z域中。然后，在z域中，根据上一节课的内容，使用线性SVM解决问题即可。上一节课我们说过，使用SVM得到large-margin，减少了有效的VC Dimension，限制了模型复杂度；另一方面，使用特征转换，目的是让模型更复杂，减小![](img/a8b61417c614fe3bc2af079fa6ed96cd.jpg)。所以说，非线性SVM是把这两者目的结合起来，平衡这两者的关系。那么，特征转换下，求解QP问题在z域中的维度设为![](img/500653986413116be2a517679c22bfd2.jpg)，如果模型越复杂，则![](img/500653986413116be2a517679c22bfd2.jpg)越大，相应求解这个QP问题也变得很困难。当![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)无限大的时候，问题将会变得难以求解，那么有没有什么办法可以解决这个问题呢？一种方法就是使SVM的求解过程不依赖![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)，这就是我们本节课所要讨论的主要内容。

![这里写图片描述](img/1de4d5d2e0727f3a50445556d6ca2507.jpg)

比较一下，我们上一节课所讲的Original SVM二次规划问题的变量个数是![](img/500653986413116be2a517679c22bfd2.jpg)，有N个限制条件；而本节课，我们把问题转化为对偶问题（’Equivalent’ SVM），同样是二次规划，只不过变量个数变成N个，有N+1个限制条件。这种对偶SVM的好处就是问题只跟N有关，与![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)无关，这样就不存在上文提到的当![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)无限大时难以求解的情况。

![这里写图片描述](img/d4871f5d57735df2c2b1473d89b6ac09.jpg)

如何把问题转化为对偶问题（’Equivalent’ SVM），其中的数学推导非常复杂，本文不做详细数学论证，但是会从概念和原理上进行简单的推导。

还记得我们在《机器学习基石》课程中介绍的Regularization中，在最小化![](img/a8b61417c614fe3bc2af079fa6ed96cd.jpg)的过程中，也添加了限制条件：![](img/b6d200f980aaa3ee660d804911e853cc.jpg)。我们的求解方法是引入拉格朗日因子![](img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg)，将有条件的最小化问题转换为无条件的最小化问题：![](img/cb76a6ddbcdf1a80fb2c6f10493c9aca.jpg)，最终得到的w的最优化解为：

![](img/a9861b11eab2dcc8a5fc6a5b75c4dad1.jpg)

所以，在regularization问题中，![](img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg)是已知常量，求解过程变得容易。那么，对于dual SVM问题，同样可以引入![](img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg)，将条件问题转换为非条件问题，只不过![](img/5e8df2ba7e47a784c714d176ed8bbb7a.jpg)是未知参数，且个数是N，需要对其进行求解。

![这里写图片描述](img/6d2f47282052c59f7060210764ab7e4b.jpg)

如何将条件问题转换为非条件问题？上一节课我们介绍的SVM中，目标是：![](img/55bc745bc4b82cd76bc564bef2fed36f.jpg)，条件是：![](img/9918e5500a5fc07d013a823f98c8511c.jpg)。首先，我们令拉格朗日因子为![](img/b445498ce7b354c165c2d823865602f9.jpg)（区别于regularization），构造一个函数：

![](img/d4d4ec18fd6e4ad4578c6cc41f84d1b8.jpg)

这个函数右边第一项是SVM的目标，第二项是SVM的条件和拉格朗日因子![](img/b445498ce7b354c165c2d823865602f9.jpg)的乘积。我们把这个函数称为拉格朗日函数，其中包含三个参数：b，w，![](img/b445498ce7b354c165c2d823865602f9.jpg)。

![这里写图片描述](img/14ac8209334051d986a9e3c89c980a00.jpg)

下面，我们利用拉格朗日函数，把SVM构造成一个非条件问题：

![这里写图片描述](img/d7b7419f72b7b55c5a0eedecf012140f.jpg)

该最小化问题中包含了最大化问题，怎么解释呢？首先我们规定拉格朗日因子![](img/db6890730ce9a7d6545d124446e7da9e.jpg)，根据SVM的限定条件可得：![](img/fe7ea758f408d074ca0ab7454393bfcf.jpg)，如果没有达到最优解，即有不满足![](img/fe7ea758f408d074ca0ab7454393bfcf.jpg)的情况，因为![](img/db6890730ce9a7d6545d124446e7da9e.jpg)，那么必然有![](img/e7fe5217a259ec8a98f670f6c90b14c3.jpg)。对于这种大于零的情况，其最大值是无解的。如果对于所有的点，均满足![](img/fe7ea758f408d074ca0ab7454393bfcf.jpg)，那么必然有![](img/ce162b749eb9a94653d0639fd70125e5.jpg)，则当![](img/9a882899a91f289a9441a621381dc7dc.jpg)时，其有最大值，最大值就是我们SVM的目标：![](img/fe7bbafafbb55291e5ff44c29986ee21.jpg)。因此，这种转化为非条件的SVM构造函数的形式是可行的。

### **Lagrange Dual SVM**

现在，我们已经将SVM问题转化为与拉格朗日因子![](img/b445498ce7b354c165c2d823865602f9.jpg)有关的最大最小值形式。已知![](img/db6890730ce9a7d6545d124446e7da9e.jpg)，那么对于任何固定的![](img/a8029168dab11a1038d7a3788d9df743.jpg)，且![](img/3a9cac3d490765bedd2ca462e6cd52f5.jpg)，一定有如下不等式成立：

![这里写图片描述](img/16e7a81a6274531b8573a7e15f64ec3f.jpg)

对上述不等式右边取最大值，不等式同样成立：

![这里写图片描述](img/963b0fba8275b236ee890c88610c1942.jpg)

上述不等式表明，我们对SVM的min和max做了对调，满足这样的关系，这叫做Lagrange dual problem。不等式右边是SVM问题的下界，我们接下来的目的就是求出这个下界。

已知![](img/32f64b55ca4b4acc4f1bd066ac0b4040.jpg)是一种弱对偶关系，在二次规划QP问题中，如果满足以下三个条件：

*   **函数是凸的（convex primal）**

*   **函数有解（feasible primal）**

*   **条件是线性的（linear constraints）**

那么，上述不等式关系就变成强对偶关系，![](img/32f64b55ca4b4acc4f1bd066ac0b4040.jpg)变成=，即一定存在满足条件的解![](img/5833c1253e9b8e7e8feb90a716e70c2f.jpg)，使等式左边和右边都成立，SVM的解就转化为右边的形式。

经过推导，SVM对偶问题的解已经转化为无条件形式：

![这里写图片描述](img/6d9afb3fae232cc023e110d2cea1d38c.jpg)

其中，上式括号里面的是对拉格朗日函数![](img/b0340fd145f2058f1ee1a09d92497ba9.jpg)计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。首先，令![](img/b0340fd145f2058f1ee1a09d92497ba9.jpg)对参数b的梯度为零：

![](img/792768feef52385f2efd53979913ea0a.jpg)

也就是说，最优解一定满足![](img/9c044a4161aafa0fe2d6ec7b962d666d.jpg)。那么，我们把这个条件代入计算max条件中（与![](img/db6890730ce9a7d6545d124446e7da9e.jpg)同为条件），并进行化简：

![这里写图片描述](img/9a4a4f3435de037b3b29d88372cb1b3e.jpg)

这样，SVM表达式消去了b，问题化简了一些。然后，再根据最小值思想，令![](img/b0340fd145f2058f1ee1a09d92497ba9.jpg)对参数w的梯度为零：

![](img/3ac526e188de7dcefb4c6d80020a1a00.jpg)

即得到：

![](img/69bdfefe744163ecac980fd18bac2712.jpg)

也就是说，最优解一定满足![](img/69bdfefe744163ecac980fd18bac2712.jpg)。那么，同样我们把这个条件代入并进行化简：

![这里写图片描述](img/c63dfb9c0510fce4842c0b20c0296210.jpg)

这样，SVM表达式消去了w，问题更加简化了。这时候的条件有3个：

*   all ![](img/db6890730ce9a7d6545d124446e7da9e.jpg)

*   ![](img/9c044a4161aafa0fe2d6ec7b962d666d.jpg)

*   ![](img/69bdfefe744163ecac980fd18bac2712.jpg)

SVM简化为只有![](img/b445498ce7b354c165c2d823865602f9.jpg)的最佳化问题，即计算满足上述三个条件下，函数![](img/11165ada31a78fadd43a2f7abf2c02f4.jpg)最小值时对应的![](img/b445498ce7b354c165c2d823865602f9.jpg)是多少。

总结一下，SVM最佳化形式转化为只与![](img/b445498ce7b354c165c2d823865602f9.jpg)有关：

![这里写图片描述](img/a5bc66de0a6f6105434a0124eb0739cc.jpg)

其中，满足最佳化的条件称之为Karush-Kuhn-Tucker(KKT)：

![这里写图片描述](img/315386eb8d4d1c52b020d8456e910819.jpg)

在下一部分中，我们将利用KKT条件来计算最优化问题中的![](img/82005cc2e0087e2a52c7e43df4a19a00.jpg)，进而得到b和w。

### **Solving Dual SVM**

上面我们已经得到了dual SVM的简化版了，接下来，我们继续对它进行一些优化。首先，将max问题转化为min问题，再做一些条件整理和推导，得到：

![这里写图片描述](img/6a301804378e31fb171cebe230f64857.jpg)

显然，这是一个convex的QP问题，且有N个变量![](img/b445498ce7b354c165c2d823865602f9.jpg)，限制条件有N+1个。则根据上一节课讲的QP解法，找到Q，p，A，c对应的值，用软件工具包进行求解即可。

![这里写图片描述](img/f8fab6ad372676d546b99fb0095e0f23.jpg)

求解过程很清晰，但是值得注意的是，![](img/0ffcb522e53e75f523d75f82d7e9c935.jpg)，大部分值是非零的，称为dense。当N很大的时候，例如N=30000，那么对应的![](img/1fe7a902369b857be0aefda4594ddedc.jpg)的计算量将会很大，存储空间也很大。所以一般情况下，对dual SVM问题的矩阵![](img/1fe7a902369b857be0aefda4594ddedc.jpg)，需要使用一些特殊的方法，这部分内容就不再赘述了。

![这里写图片描述](img/f5c266c1fb3bef9ec893b136721a16d4.jpg)

得到![](img/b445498ce7b354c165c2d823865602f9.jpg)之后，再根据之前的KKT条件，就可以计算出w和b了。首先利用条件![](img/a444bd494aa5bbd18fb590a1173feaa6.jpg)得到w，然后利用条件![](img/143c7a016f60f17ee3ffa7de1c336026.jpg)，取任一![](img/50d9c11c51222fd482856e48eec5a134.jpg)即![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点，得到![](img/64fccb927aebbcac5e8b1962f58ec34f.jpg)，进而求得![](img/8d4251be40eabb2af74bafe5492c6d15.jpg)。

![这里写图片描述](img/d9e6b27ecbfa0f3d5444cfcc8770af09.jpg)

值得注意的是，计算b值，![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0时，有![](img/668d079f15a25b2346fa69e0ee7d687a.jpg)成立。![](img/668d079f15a25b2346fa69e0ee7d687a.jpg)正好表示的是该点在SVM分类线上，即fat boundary。也就是说，满足![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点一定落在fat boundary上，这些点就是Support Vector。这是一个非常有趣的特性。

### **Messages behind Dual SVM**

回忆一下，上一节课中，我们把位于分类线边界上的点称为support vector（candidates）。本节课前面介绍了![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点一定落在分类线边界上，这些点称之为support vector（注意没有candidates）。也就是说分类线上的点不一定都是支持向量，但是满足![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点，一定是支持向量。

![这里写图片描述](img/4bba91a3df7aeb6ec3a4dd152ea6f87f.jpg)

SV只由![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点决定，根据上一部分推导的w和b的计算公式，我们发现，w和b仅由SV即![](img/b445498ce7b354c165c2d823865602f9.jpg)&gt;0的点决定，简化了计算量。这跟我们上一节课介绍的分类线只由“胖”边界上的点所决定是一个道理。也就是说，样本点可以分成两类：一类是support vectors，通过support vectors可以求得fattest hyperplane；另一类不是support vectors，对我们求得fattest hyperplane没有影响。

![这里写图片描述](img/f2c058722c1f4343cb344bf06c74026a.jpg)

回过头来，我们来比较一下SVM和PLA的w公式：

![这里写图片描述](img/88ec3911c97e06c02e6aa47919a0e3ea.jpg)

我们发现，二者在形式上是相似的。![](img/870e196274fb87ef507b6bbebf697db9.jpg)由fattest hyperplane边界上所有的SV决定，![](img/2de33d1152bc9266f54828db82afac0a.jpg)由所有当前分类错误的点决定。![](img/870e196274fb87ef507b6bbebf697db9.jpg)和![](img/2de33d1152bc9266f54828db82afac0a.jpg)都是原始数据点![](img/d873c5e6c901dc85105463368e9c46cd.jpg)的线性组合形式，是原始数据的代表。

![这里写图片描述](img/24e51a68b0a840edee6615be101c1d12.jpg)

总结一下，本节课和上节课主要介绍了两种形式的SVM，一种是Primal Hard-Margin SVM，另一种是Dual Hard_Margin SVM。Primal Hard-Margin SVM有![](img/81f3529fe606d9313bd5b610f9bebc92.jpg)个参数，有N个限制条件。当![](img/81f3529fe606d9313bd5b610f9bebc92.jpg)很大时，求解困难。而Dual Hard_Margin SVM有N个参数，有N+1个限制条件。当数据量N很大时，也同样会增大计算难度。两种形式都能得到w和b，求得fattest hyperplane。通常情况下，如果N不是很大，一般使用Dual SVM来解决问题。

![这里写图片描述](img/8ebdb61b1791c5baf2a3f536444a8597.jpg)

这节课提出的Dual SVM的目的是为了避免计算过程中对![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)的依赖，而只与N有关。但是，Dual SVM是否真的消除了对![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)的依赖呢？其实并没有。因为在计算![](img/3d0de89d8cc6447a36dd93c6e7aef429.jpg)的过程中，由z向量引入了![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)，实际上复杂度已经隐藏在计算过程中了。所以，我们的目标并没有实现。下一节课我们将继续研究探讨如何消除对![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)的依赖。

![这里写图片描述](img/13aec28d3ee01fc80f48db45bc47b3c6.jpg)

### **总结**

本节课主要介绍了SVM的另一种形式：Dual SVM。我们这样做的出发点是为了移除计算过程对![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)的依赖。Dual SVM的推导过程是通过引入拉格朗日因子![](img/82005cc2e0087e2a52c7e43df4a19a00.jpg)，将SVM转化为新的非条件形式。然后，利用QP，得到最佳解的拉格朗日因子![](img/82005cc2e0087e2a52c7e43df4a19a00.jpg)。再通过KKT条件，计算得到对应的w和b。最终求得fattest hyperplane。下一节课，我们将解决Dual SVM计算过程中对![](img/43d11718af2e4efe2c95da32038c4b0c.jpg)的依赖问题。

**_注明：_**

文章中所有的图片均来自台湾大学林轩田《机器学习技法》课程
