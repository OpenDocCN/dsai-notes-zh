# 7 -- The VC Dimension

前几节课着重介绍了机器能够学习的条件并做了详细的推导和解释。机器能够学习必须满足两个条件：

*   **假设空间H的Size M是有限的，即当N足够大的时候，那么对于假设空间中任意一个假设g，![](img/f414f6c0d01c45a4656efaecfbcce3e9.jpg)**。
*   **利用算法A从假设空间H中，挑选一个g，使![](img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg)，则![](img/fabc6ef6eee191794e6e61f6b3fa8453.jpg)**。

这两个条件，正好对应着test和trian两个过程。train的目的是使损失期望![](img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg)；test的目的是使将算法用到新的样本时的损失期望也尽可能小，即![](img/fabc6ef6eee191794e6e61f6b3fa8453.jpg)。

正因为如此，上次课引入了break point，并推导出只要break point存在，则M有上界，一定存在![](img/f414f6c0d01c45a4656efaecfbcce3e9.jpg)。

本次笔记主要介绍VC Dimension的概念。同时也是总结VC Dimension与![](img/e6d085f2f5b3c5ac156b93b931cf1aaf.jpg)，![](img/fabc6ef6eee191794e6e61f6b3fa8453.jpg)，Model Complexity Penalty（下面会讲到）的关系。

### **一、Definition of VC Dimension**

首先，我们知道如果一个假设空间H有break point k，那么它的成长函数是有界的，它的上界称为Bound function。根据数学归纳法，Bound function也是有界的，且上界为![](img/1c802837e2ff8555d27b6c504fa4aaa0.jpg)。从下面的表格可以看出，![](img/d585e715f76b1770b71b39ec58406698.jpg)比B(N,k)松弛很多。

![这里写图片描述](img/65438447a6404dacb7adf69ce1b84c57.jpg)

则根据上一节课的推导，VC bound就可以转换为：

![这里写图片描述](img/1a241a5e97470e36b6e837ca1c6b7c8f.jpg)

这样，不等式只与k和N相关了，一般情况下样本N足够大，所以我们只考虑k值。有如下结论：

*   **若假设空间H有break point k，且N足够大，则根据VC bound理论，算法有良好的泛化能力**

*   **在假设空间中选择一个矩g，使![](img/d89ccd61d9eb513dd34ee152f11e6f1e.jpg)，则其在全集数据中的错误率会较低**

![这里写图片描述](img/a5ca7210e5d1a3867c442706075b3364.jpg)

下面介绍一个新的名词：VC Dimension。VC Dimension就是某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力。（注意，只要存在一种分布的inputs能够正确分类也满足）。

shatter的英文意思是“粉碎”，也就是说对于inputs的所有情况都能列举出来。例如对N个输入，如果能够将![](img/b2c6525e1b53f5de75304b2103b8f5a2.jpg)种情况都列出来，则称该N个输入能够被假设集H shatter。

根据之前break point的定义：假设集不能被shatter任何分布类型的inputs的最少个数。则VC Dimension等于break point的个数减一。

![这里写图片描述](img/d9c6888aa7b6a1aeab0b966821eaf7f7.jpg)

现在，我们回顾一下之前介绍的四种例子，它们对应的VC Dimension是多少：

![这里写图片描述](img/68d0bc0cf15ef728557a7a402eda4333.jpg)

用![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)代替k，那么VC bound的问题也就转换为与![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)和N相关了。同时，如果一个假设集H的![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)确定了，则就能满足机器能够学习的第一个条件![](img/f414f6c0d01c45a4656efaecfbcce3e9.jpg)，与算法、样本数据分布和目标函数都没有关系。

![这里写图片描述](img/494b1b5f42ccd206e34c5bf81d269b8e.jpg)

### **二、VC Dimension of Perceptrons**

回顾一下我们之前介绍的2D下的PLA算法，已知Perceptrons的k=4，即![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)。根据VC Bound理论，当N足够大的时候，![](img/0cb06285659684a121aa7e03b9d43134.jpg)。如果找到一个g，使![](img/5f7fa28afc832c40a19b20c56cd5497d.jpg)，那么就能证明PLA是可以学习的。

![这里写图片描述](img/c39a6ed2181b71cd40194ec64f9df985.jpg)

这是在2D情况下，那如果是多维的Perceptron，它对应的![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)又等于多少呢？

已知在1D Perceptron，![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，在2D Perceptrons，![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，那么我们有如下假设：![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，其中d为维数。

要证明的话，只需分两步证明：

*   ![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)
*   ![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)

![这里写图片描述](img/b77632a86d53ec6e0992ff34b5f47ac3.jpg)

首先证明第一个不等式：![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)。

在d维里，我们只要找到某一类的d+1个inputs可以被shatter的话，那么必然得到![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)。所以，我们有意构造一个d维的矩阵![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)能够被shatter就行。![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)是d维的，有d+1个inputs，每个inputs加上第零个维度的常数项1，得到![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)的矩阵：

![这里写图片描述](img/d343eaf7d931622ab9c5a18bcb2da7c3.jpg)

矩阵中，每一行代表一个inputs，每个inputs是d+1维的，共有d+1个inputs。这里构造的![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)很明显是可逆的。shatter的本质是假设空间H对![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)的所有情况的判断都是对的，即总能找到权重W，满足![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)，![](img/402b6f3e239916446e4ddb6dfb487d13.jpg)。由于这里我们构造的矩阵![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)的逆矩阵存在，那么d维的所有inputs都能被shatter，也就证明了第一个不等式。

![这里写图片描述](img/846ab1576112dba66499d6bbd45cddd7.jpg)

然后证明第二个不等式：![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)。

在d维里，如果对于任何的d+2个inputs，一定不能被shatter，则不等式成立。我们构造一个任意的矩阵![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)，其包含d+2个inputs，该矩阵有d+1列，d+2行。这d+2个向量的某一列一定可以被另外d+1个向量线性表示，例如对于向量![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)，可表示为：

其中，假设![](img/ed4df0177082a678ddda7cf9f1b763cd.jpg)，![](img/4a42f9a7dcd77596f77d07806401e190.jpg).

那么如果![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)是正类，![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)均为负类，则存在![](img/207b8020a44212d6583b9a9a508eaf27.jpg)，得到如下表达式：
![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)&lt;font color="#0000ff"&gt;![](img/87f48f3e65043ed2ce3c7be04fb1b2fa.jpg)&lt;/font&gt;+&lt;font color="#ff0000"&gt;![](img/d15fd89e8ee09367b46f5d67b8deafc2.jpg)&lt;/font&gt;+![](img/bfcaf05c391bf5e95b0123077b1793bb.jpg)+&lt;font color="#ff0000"&gt;![](img/d59543d0e11f3be964667fa1cd0aa6f1.jpg)&lt;/font&gt;![](img/e06d0d70da65d0d75d43554b0fcfd918.jpg)

因为其中蓝色项大于0，代表正类；红色项小于0，代表负类。所有对于这种情况，![](img/1147960c1c23d8dc45d182a5f97785a4.jpg)一定是正类，无法得到负类的情况。也就是说，d+2个inputs无法被shatter。证明完毕！

![这里写图片描述](img/7b63d1d73456e3b08d3d8528dd085985.jpg)

综上证明可得![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)。

### **三、Physical Intuition VC Dimension**

![这里写图片描述](img/11bfe1c44d49f8d52587556bd70a9dfb.jpg)

上节公式中![](img/207b8020a44212d6583b9a9a508eaf27.jpg)又名features，即自由度。自由度是可以任意调节的，如同上图中的旋钮一样，可以调节。VC Dimension代表了假设空间的分类能力，即反映了H的自由度，产生dichotomy的数量，也就等于features的个数，但也不是绝对的。

![这里写图片描述](img/79fb72999312ff07fbe285f50b3a39cc.jpg)

例如，对2D Perceptrons，线性分类，![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，则![](img/207b8020a44212d6583b9a9a508eaf27.jpg)，也就是说只要3个features就可以进行学习，自由度为3。

介绍到这，我们发现M与![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)是成正比的，从而得到如下结论：

![这里写图片描述](img/1a5f49dd0cd8d8428ba57e52fdc7e7e3.jpg)

### **四、Interpreting VC Dimension**

下面，我们将更深入地探讨VC Dimension的意义。首先，把VC Bound重新写到这里：

![这里写图片描述](img/659f73151c15a98ed0d96b332cc9b5b1.jpg)

根据之前的泛化不等式，如果![](img/1061e3cbc87053b56ae2b1573a2f6451.jpg)，即出现bad坏的情况的概率最大不超过![](img/e43bd121c6fef869baa12e9adf4a201a.jpg)。那么反过来，对于good好的情况发生的概率最小为![](img/b4330fb1bb4c0e475155f0d398d0ce3b.jpg)，则对上述不等式进行重新推导：

![这里写图片描述](img/baca48fe5828da7f116ce70f49419886.jpg)

![](img/a385c3bea2dcb00656993bbdebe340d8.jpg)表现了假设空间H的泛化能力，![](img/a385c3bea2dcb00656993bbdebe340d8.jpg)越小，泛化能力越大。

![这里写图片描述](img/7216f7a0afaea0c4553f33039295dce8.jpg)

至此，已经推导出泛化误差![](img/4707f2d200bb617863f7161e0de612c5.jpg)的边界，因为我们更关心其上界（![](img/4707f2d200bb617863f7161e0de612c5.jpg)可能的最大值），即：

![这里写图片描述](img/9fcc49a165eccfae45b7e9feb6800a83.jpg)

上述不等式的右边第二项称为模型复杂度，其模型复杂度与样本数量N、假设空间H(![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg))、![](img/a385c3bea2dcb00656993bbdebe340d8.jpg)有关。![](img/4707f2d200bb617863f7161e0de612c5.jpg)由![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)共同决定。下面绘出![](img/4707f2d200bb617863f7161e0de612c5.jpg)、model complexity、![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)随![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)变化的关系：

![这里写图片描述](img/ada572ce18dcc85587581450123083f2.jpg)

通过该图可以得出如下结论：

*   **![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)越大，![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)越小，![](img/bfd231c1490723e852948a84827e8b04.jpg)越大（复杂）**。

*   **![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)越小，![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)越大，![](img/bfd231c1490723e852948a84827e8b04.jpg)越小（简单）**。

*   **随着![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)增大，![](img/4707f2d200bb617863f7161e0de612c5.jpg)会先减小再增大**。

所以，为了得到最小的![](img/4707f2d200bb617863f7161e0de612c5.jpg)，不能一味地增大![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)以减小![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)，因为![](img/571ed3fcbee703f5e037a62366e1e1d8.jpg)太小的时候，模型复杂度会增加，造成![](img/4707f2d200bb617863f7161e0de612c5.jpg)变大。也就是说，选择合适的![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，选择的features个数要合适。

下面介绍一个概念：样本复杂度（Sample Complexity）。如果选定![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)，样本数据D选择多少合适呢？通过下面一个例子可以帮助我们理解：

![这里写图片描述](img/55ac247926db04a7937ad6bcc1740683.jpg)

通过计算得到N=29300，刚好满足![](img/e43bd121c6fef869baa12e9adf4a201a.jpg)的条件。N大约是![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)的10000倍。这个数值太大了，实际中往往不需要这么多的样本数量，大概只需要![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)的10倍就够了。N的理论值之所以这么大是因为VC Bound 过于宽松了，我们得到的是一个比实际大得多的上界。

![这里写图片描述](img/2d342e1b5628209372a217cdb60e4177.jpg)

值得一提的是，VC Bound是比较宽松的，而如何收紧它却不是那么容易，这也是机器学习的一大难题。但是，令人欣慰的一点是，VC Bound基本上对所有模型的宽松程度是基本一致的，所以，不同模型之间还是可以横向比较。从而，VC Bound宽松对机器学习的可行性还是没有太大影响。

### **五、总结**

本节课主要介绍了VC Dimension的概念就是最大的non-break point。然后，我们得到了Perceptrons在d维度下的VC Dimension是d+1。接着，我们在物理意义上，将![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)与自由度联系起来。最终得出结论![](img/25e9ecbb8e04e03e89d61c3f36457c57.jpg)不能过大也不能过小。选取合适的值，才能让![](img/4707f2d200bb617863f7161e0de612c5.jpg)足够小，使假设空间H具有良好的泛化能力。

**_注明：_**

文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程