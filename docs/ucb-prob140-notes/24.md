# 二十四、简单线性回归

> 原文：https://nbviewer.jupyter.org/github/prob140/textbook/tree/gh-pages/notebooks/Chapter_24/
>
> 译者：[ThomasCai](https://github.com/ThomasCai)
>
> 协议：[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)
>
> 自豪地采用[谷歌翻译](https://translate.google.cn/)

## 00 简单线性回归
在数据科学中，回归模型被广泛地应用于预测。本章从概率论的角度来研究线性最小二乘法。重点是简单回归，即基于一个数值属性的预测。

当属性$X$和响应$Y$的联合分布为二元正态分布时，$(X,Y)$的经验分布是橄榄球的形状，与数字 8 的非常相似。我们将从相关的几何解释开始，因为这有助于理解回归和二元正态分布。我们要推导的线性回归的方程，可以用几种方法来表示；在本章的末尾，我们将以最容易扩展到多元回归的方式来表示它。

## 01 二元正态分布

```py
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import warnings
import matplotlib.cbook
warnings.filterwarnings("ignore",category=matplotlib.cbook.mplDeprecation)
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```

```py
# HIDDEN

def sin(theta):
    return np.sin(theta * np.pi/180)
def cos(theta):
    return np.cos(theta * np.pi/180)
def tan(theta):
    return sin(theta)/cos(theta)

def projection_1_2(theta):
    x = 1
    z = 2
    y = x*cos(theta) + z*sin(theta)
    plt.figure(figsize=(6, 6))
    plt.scatter(x, z, s=40, color='green')
    plt.plot([-3, 3], [0, 0], color='grey', lw=2, label=r'$X$'+' axis')
    plt.plot([0, 0], [-3, 3], color='grey', lw=2)
    plt.plot([-3, 3], [tan(theta)*(-3), tan(theta)*3], color='gold', lw=2, label='New axis at positive angle '+r'$\theta$ to the '+r'$X$'+' axis')
    plt.plot([0, x], [0, 0], color='blue', lw=2)
    plt.plot([x, x], [0, z], color='green', linestyle='--', lw=2)
    plt.plot([x, cos(theta)*y], [z, sin(theta)*y], color='green', linestyle='--', lw=2)
    plt.plot([0, cos(theta)*y], [0, sin(theta)*y], color='red', lw=2)         
    plt.axes().set_aspect('equal')
    plt.legend(bbox_to_anchor=(1.92, 1.02))
    plt.xlabel('$X$')
    plt.ylabel('$Z$', rotation=0)
    plt.title('Projection of $(X, Z) = (1, 2)$ on Gold Axis')
    plt.xlim(-3, 3)
    plt.ylim(-3, 3)
    
def projection_trig():
    x = 1
    z = 2
    x1 = x*cos(theta)
    x2 = z*sin(theta)
    y = x1 + x2
    plt.figure(figsize=(8, 8))
    plt.scatter(x, z, s=40, color='green')
    plt.plot([-3, 3], [0, 0], color='grey', lw=2)
    plt.plot([0, 0], [-3, 3], color='grey', lw=2)
    plt.plot([-3, 3], [tan(theta)*(-3), tan(theta)*3], color='gold', lw=2)
    plt.plot([0, x], [0, 0], color='blue', lw=2)
    plt.plot([x, x], [0, z], color='green', linestyle='--', lw=2)
    plt.plot([x, cos(theta)*y], [z, sin(theta)*y], color='green', linestyle='--', lw=2)
    plt.plot([x, cos(theta)*x1], [0, sin(theta)*x1], color='k', linestyle='--', lw=2)
    plt.plot([cos(theta)*y, x+cos(theta)*x2], [sin(theta)*y, sin(theta)*x2], color='k', linestyle='--', lw=2)
    plt.plot([x, x+cos(theta)*x2], [0, sin(theta)*x2], color='k', linestyle='--', lw=2)
    plt.plot([0, cos(theta)*x1], [0, sin(theta)*x1], color='brown', lw=3, label='Length = '+r'$X\cos(\theta)$')
    plt.plot([cos(theta)*x1, cos(theta)*y], [sin(theta)*x1, sin(theta)*y], color='darkblue', lw=3, label='Length = '+r'$Z\sin(\theta)$')
    plt.text(0.3, 0.06, r'$\theta$', fontsize=20)
    plt.text(1.03, 1.6, r'$\theta$', fontsize=20)
    plt.text(0.8, 2.1, r'$(X, Z)$', fontsize=15)
    plt.legend(bbox_to_anchor=(1.35, 1))
    plt.axes().set_aspect('equal')
    plt.xlabel('$X$')
    plt.ylabel('$Z$', rotation=0)
    plt.title('$Y =$ '+r'$X\cos(\theta) + Z\sin(\theta)$')
    plt.xlim(-0.5, 3)
    plt.ylim(-0.5, 3)
```
### 二元正态分布
多元正态分布由均值向量和协方差矩阵定义。协方差的单位通常难以理解，因为它们是两个变量的单位的乘积。

将协方差归一化以使其更容易解释是个好主意。正如你们在练习中看到的，对于联合分布的随机变量$X$和$Y$, $X$和$Y$之间的相关性（译者注：相关系数）定义为：

![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y}=\frac{Cov(X,Y)}{\sigma{_X}\sigma{_Y}}=E(\frac{X-\mu{_X}}{\sigma{_X}}\cdot\frac{Y-\mu{_Y}}{\sigma{_Y}})=E(X{^*}Y{^*}))

其中![](http://latex.codecogs.com/gif.latex?X{^*})是标准单位的$X$，![](http://latex.codecogs.com/gif.latex?Y{^*})是标准单位的$Y$。

**相关性的性质**

你在练习中可以看到这些。

- ![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})只依赖于标准单位，因此它是一个没有单位的纯数
- ![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y}=\tau{_Y,_X})
- ![](http://latex.codecogs.com/gif.latex?-1\leq\tau{_X,_Y}\leq1)
- 如果$Y=aX+b$，然后![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})是 1 或-1，这根据$a$的符号是正还是负。

我们认为![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})衡量了$X$和$Y$的线性关系。

**和的方差**

重写一下相关性的公式

![](http://latex.codecogs.com/gif.latex?Cov(X,Y)=\tau{_X,_Y}\sigma{_X}\sigma{_Y})

所以$X+Y$的方差是

![](http://latex.codecogs.com/gif.latex?\sigma{^2_{X+Y}}=\sigma{^2_X}+\sigma{^2_Y}+2\tau{_{X,Y}\sigma_X\sigma_Y})

注意与两个向量之和的长度的公式并行，相关性扮演着两个向量夹角的余弦的角色。如果这个角是 90 度，那么 cos 值为 0。这对应于相关性也为零，因此随机变量是不相关的。

在$X$和$Y$的联合分布是二元正态分布的情况下，我们将可视化这个想法。

**标准二元正态分布**

令$X$和$Z$是独立的标准正态变量，即具有均值向量$0$和协方差矩阵等于单位矩阵的二元正态随机变量。 现在确定一个数$\rho$（即希腊字母 rho，小写 r），使![](http://latex.codecogs.com/gif.latex?-1<\rho<1)，并令

![](http://latex.codecogs.com/gif.latex?A=\left[\begin{matrix}1&0\\\rho&\sqrt{1-\rho{^2}}\\\end{matrix}\right])

定义一个新的随机变量![](http://latex.codecogs.com/gif.latex?Y=\rhoX+\sqrt{1-\rho{^2}}Z)，并注意到

![](http://latex.codecogs.com/gif.latex?\left[\begin{matrix}X\\Y\\\end{matrix}\right]=\left[\begin{matrix}1&0\\\rho&\sqrt{1-\rho{^2}}\\\end{matrix}\right]\left[\begin{matrix}X\\Z\\\end{matrix}\right]=A\left[\begin{matrix}X\\Z\\\end{matrix}\right])

所以$X$和$Y$是均值向量为$0$和协方差矩阵的二元正态分布。

![](http://latex.codecogs.com/gif.latex?AIA{^T}=\left[\begin{matrix}1&0\\\rho&\sqrt{1-\rho{^2}}\\\end{matrix}\right]\left[\begin{matrix}1&\rho\\0&\sqrt{1-\rho{^2}}\\\end{matrix}\right]=\left[\begin{matrix}1&\rho\\\rho&1\\\end{matrix}\right])

我们说$X$和$Y$有标准的二元正态分布，相关性![](http://latex.codecogs.com/gif.latex?\rho)。

下图显示了在$\rho= 0.6$的情况下 1000 个$(X,Y)$点的经验分布。 您可以改变$rho$的值(译者注：也就是![](http://latex.codecogs.com/gif.latex?\rho))并观察散点图是如何变化的。 它将使您想起数据 8 中的许多此类模拟。

```py
# Plotting parameters
plt.figure(figsize=(5, 5))
plt.axes().set_aspect('equal')
plt.xlabel('$X$')
plt.ylabel('$Y$', rotation=0)
plt.xticks(np.arange(-4, 4.1))
plt.yticks(np.arange(-4, 4.1))

# X, Z, and Y
x = stats.norm.rvs(0, 1, size=1000)
z = stats.norm.rvs(0, 1, size=1000)
rho = 0.6
y = rho*x + np.sqrt((1-rho**2))*z
plt.scatter(x, y, color='darkblue', s=10);
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190122175734322.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

**余弦相关性**

我们定义

![](http://latex.codecogs.com/gif.latex?Y=\rho{X}+\sqrt{1-\rho{^2}}Z)

其中$X$和$Z$是独立同分布的标准正态变量。
我们从几何上理解这个结构。一个好的开始是$X$和$Z$的联合密度，它具有圆的对称性。

```py
# NO CODE

Plot_bivariate_normal([0, 0], [[1, 0], [0, 1]])
plt.xlabel('$X$')
plt.ylabel('$Z$')
plt.gca().set_zlabel('$f(x, z)$')
plt.title('Standard Bivariate Normal Distribution, Correlation = 0');
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190123103555357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

$X$轴和$Z$轴是正交的。让我们看看如果我们扭转它们会发生什么。

取任何正角度θ度并绘制与原始 X 轴成角度θ的新轴，每个点(X,Z)在这个轴上都有一个投影。

下图所示，点(X,Z)=(1,2)到金色轴的投影，金色轴与$X$轴成θ角。蓝色的部分是$X$的值，通过把(1,2)点向横轴上做垂线得到，称为(1,2)点到横轴上的投影。

红色部分是(1,2)点在金色轴上的投影，通过把(1,2)点向金色轴做垂线得到。

在下面的单元格中改变θ的值，可以观察到投影在金色轴旋转时如何变化。

```py
theta = 20
projection_1_2(theta)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190123105456612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

设$Y$是红色段的长度，并记住$X$是蓝色段的长度。 当θ非常小时，$Y$几乎等于$X$。当θ接近 90 度时，Y 几乎等于 Z。

一点三角法就表明了这一点：![](http://latex.codecogs.com/gif.latex?Y=X\cos(\theta)+Z\sin(\theta))

```py
projection_trig()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190124193814982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

因此

![](http://latex.codecogs.com/gif.latex?Y=X\cos(\theta)+Z\sin(\theta)=\rho{X}+\sqrt{1-\rho{^2}}Z)

其中![](http://latex.codecogs.com/gif.latex?\rho=\cos(\theta))

下面的一系列图像说明了θ为 30 度的转换。

```py
theta = 30
projection_1_2(theta)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019012419504497.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

当原始点(X, Z)具有独立同分布的标准正态坐标时，二元正态分布是蓝色和红色长度 X 和 Y 的联合分布。此变换(X, Z)的联合密度表面的圆形轮廓成(X, Y)的联合密度表面的椭圆形轮廓。

```py
cos(theta), (3**0.5)/2
>>(0.8660254037844387, 0.8660254037844386)
```

```py
rho = cos(theta)
Plot_bivariate_normal([0, 0], [[1, rho], [rho, 1]])
plt.title('Standard Bivariate Normal Distribution, Correlation = '+str(round(rho, 2)));
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190124200025492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

**小的θ**

正如我们前面看到的，当θ很小，几乎对轴的位置没做任何改变时，X 和 Y 几乎相等。

```py
theta = 2
projection_1_2(theta)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190124200532501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

因此，X 和 Y 的二元正态密度本质上受限于 X=Y 线。cos(θ)的相关性很大因为θ很小；它的值超过 0.999。

从而可以看到绘图函数是很难表示这个联合密度面的。

```py
rho = cos(theta)
rho
>>0.99939082701909576
```

```py
Plot_bivariate_normal([0, 0], [[1, rho], [rho, 1]])
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190124201007761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

**正交性和独立性**

当θ是 90 度，金色轴正交于 X 轴，并且 Y = Z（其中 Z 与 X 是独立的）。

```py
theta = 90
projection_1_2(theta)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190124201516855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

当θ= 90°时，cos(θ)= 0。(X,Y)的联合密度面与(X,Z)的联合密度面相同，并具有圆的对称性。

如果你把![](http://latex.codecogs.com/gif.latex?\rho{X})当作“信号”，把![](http://latex.codecogs.com/gif.latex?\sqrt{1-\rho{^2}}Z)当作“噪音”，那么$Y$可以被认为是一个观察的值“信号加噪声”。在本章剩下的部分中，我们将看看是否能将信号与噪声分开。

**二元正态的表示**

当我们只处理两个变量 X 和 Y 时，矩阵表示通常是不必要的。我们将交替使用以下三种表示。

- ![](http://latex.codecogs.com/gif.latex?X{_1})和![](http://latex.codecogs.com/gif.latex?X{_2})是具有![](http://latex.codecogs.com/gif.latex?(\mu{_1},\mu{_2},\sigma{^2_1},\sigma{^2_2},\rho))参数的二元正态变量
- 标准化变量![](http://latex.codecogs.com/gif.latex?X{^*_1})和![](http://latex.codecogs.com/gif.latex?X{^*_2})是标准的二元正态且相关性为![](http://latex.codecogs.com/gif.latex?\rho)，那么![](http://latex.codecogs.com/gif.latex?X{^*_2}=\rho{X{^*_1}}+\sqrt{1-\rho{^2}}Z)（其中标准正态$Z$与![](http://latex.codecogs.com/gif.latex?X{^*_1})是相互独立的）。这是由多元正态的定义 2 得出的。
- ![](http://latex.codecogs.com/gif.latex?X{_1})和![](http://latex.codecogs.com/gif.latex?X{_2})有多元正态分布的均值向量![](http://latex.codecogs.com/gif.latex?[\mu{_1}\,\mu{_2}]{^T})和协方差矩阵
$$
\left[
 \begin{matrix}
   \sigma{^2_1} & \rho\sigma{_1}\sigma{_2} \\
   \rho\sigma{_1}\sigma{_2} & \sigma{^2_2} \\
  \end{matrix}
  \right]
$$

## 02 线性最小二乘法

```py
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```
### 最小二乘线性预测器
在这一节中，我们将远离二元正态分布，看看我们能否基于另一个数字变量的预测因子，从一个数字变量的所有线性预测因子中找出最好的，而不管这两个变量的联合分布如何。

对于联合分布随机变量$X$和$Y$，你知道$E(Y∣X)$是$Y$基于$X$函数的最小二乘预测器。我们现在将允许的函数限制为线性函数，并且看看我们能否在其中找到最好的一个。下一节我们将看到这个最好的线性预测器，所有预测器中最好的，和二元正态分布之间的联系。

**最小化均方误差**

令$h(X)=aX+b$其中$a$和$b$为常量，并且使$MSE(a,b)$表示$MSE(h)$
![](http://latex.codecogs.com/gif.latex?MSE(a,b)=E((Y-(aX+b)){^2}))
为了找到最小二乘线性预测器，我们必须在所有$a$和$b$上最小化这个 MSE。我们将使用微积分分两步完成：

- 固定$a$的值，并且在这个$a$下寻找$b{^*_a}$值以使得$MSE(a,b)$最小；
- 然后将这个最小化的值$b{^*_a}$代入$b$，并且最小化![](http://latex.codecogs.com/gif.latex?MSE(a,b{^*_a}))以求出$a$。

**步骤一**

固定$a$且最小化$MSE(a,b)$以求得$b$

![](http://latex.codecogs.com/gif.latex?MSE(a,b)=E(((Y-aX)-b){^2})=E((Y-aX){^2})-2bE(Y-aX)+b{^2})

对 b 求导得

![](http://latex.codecogs.com/gif.latex?\frac{d}{db}MSE(a,b)=-2E(Y-aX)+2b)

设置此值等于 0 并求解，可得对于 a 的固定值的最小化的 b 值

![](http://latex.codecogs.com/gif.latex?b{^*_a}=E(Y-aX)=E(Y)-aE(X))

**步骤二**

现在我们最小化以下函数并求得$a$

![](http://latex.codecogs.com/gif.latex?E((Y-(aX+b{^*_a})){^2})=E((Y-(aX+E(Y)-aE(X))){^2})=E\Big(\big((Y-E(Y))-a(X-E(X))\big){^2}\Big)=E\big((Y-E(Y)){^2}\big)-2aE\big((Y-E(Y))(X-E(X))\big)+a{^2}E\big((X-E(X)){^2}\big)=Var(Y)-2aCov(X,Y)+a{^2}Var(X))

对$a$求导得$-2Cov(X,Y)+2aVar(X)$。所以最小化的$a$是

![](http://latex.codecogs.com/gif.latex?a{^*}=\frac{Cov(X,Y)}{Var(X)})

在这一点上，我们应该检查我们所拥有的是最小值，而不是最大值，但是根据您的预测经验，您可能只愿意接受我们所拥有的最小值。如果不是，那么再次求导，看看得到的函数的符号。

**回归线的斜率和截距**

最小二乘直线称为回归线。最小二乘直线称为回归线。现在你可以从数据 8 中得到它的等式的证明。设![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})是$X$和$Y$之间的相关性。然后斜率和截距由下式给出：

![](http://latex.codecogs.com/gif.latex?SlopeOfRegression_line=\frac{Cov(X,Y)}{Var(X)}=\tau{_X,_Y}\frac{\sigma{_Y}}{\sigma{_X}})

![](http://latex.codecogs.com/gif.latex?InterceptOfRegression_line=E(Y)-slope\cdot{E(X)})

**标准单位的回归**

如果$X$和$Y$都是用标准单位测量的，那么回归线的斜率就是相关性![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})，截距为 0。

换句话说，已知$X=x$标准单位，Y 的预测值为![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y}x)标准单位。当![](http://latex.codecogs.com/gif.latex?\tau{_X,_Y})为正而不是 1 时，这个结果称为回归因子；Y 的预测值比 X 的给定值更接近 0。

**散点图的线和形状**

以上计算表明：

- 回归线经过这个点$(E(X), E(Y))$.
- 不管$X$和$Y$的联合分布如何，回归线的方程都成立。
- 无论$X$和$Y$之间的关系如何，在所有直线中始终存在最佳直线预测器。如果关系不是大致线性的，则不希望使用最佳直线进行预测，因为最佳直线仅仅是一类不好的预测结果中最好， 它总是存在。

## 03 回归和二元正态分布

```py
# HIDDEN
from datascience import *
from prob140 import *
import numpy as np
import warnings
import matplotlib.cbook
warnings.filterwarnings("ignore",category=matplotlib.cbook.mplDeprecation)
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```

```py
# HIDDEN 
def bivariate_normal_regression(rho, n):
    x = stats.norm.rvs(size=n)
    z = stats.norm.rvs(size=n)
    y = rho * x  +  (1 - rho**2)**0.5 * z
    plt.scatter(x, y, color='darkblue', s=10)
    if rho >= 0:
        plt.plot([-4, 4], [-4, 4], color='red', lw=2, label='y = x')
    else:
        plt.plot([-4, 4], [4, -4], color='red', lw=2, label='y = -x')
    
    plt.plot([-4, 4], [rho*(-4), rho*(4)], color='green', lw=2, label='Regression Line: y = '+str(rho)+'x')
    
    # Axes, labels, and titles
    plt.xlim(-4, 4)
    plt.ylim(-4, 4)
    plt.axes().set_aspect('equal')
    plt.legend(bbox_to_anchor=(2, 1.02))
    plt.xlabel('$X$')
    plt.ylabel('$Y$', rotation=0)
    plt.title('Standard Bivariate Normal, Correlation '+str(rho))
```
### 回归和二元正态
令$X$和$Y$为标准的二元正态变量。且他们的相关性为$ρ$。关系为：

![](http://latex.codecogs.com/gif.latex?Y=\rho{X}+\sqrt{1-\rho{^2}}Z)

其中$X$和$Z$是独立的正态分布变量，他们直接导致了基于$X$的所有函数的的$Y$的最佳预测值。显然，最佳的预测为条件期望值$E(Y|X)$。

![](http://latex.codecogs.com/gif.latex?E(Y|X)=\rho{X})

因为$Z$与$X$是独立的，因此$E(Z)=0$。

因为$E(Y|X)$是$X$的一个线性函数，所以我们可以知道：

**如果$X$和$Y$有一个标准的二元正态分布，那么基于$X$的$Y$的最佳预测值是线性的**，并且有上一节推导的回归方程。

每一个二元正态分布都可以由标准二元正态变量的线性变换来构造。因此：

**如果$X$和$Y$是二元正态的，那么基于 X 的 Y 的最佳线性预测因子也是基于$X$的$Y$的所有预测结果中最好的。**

函数`bivariate_normal_regression（代码中）`以ρ和 n 为参数并显示由相关性ρ的标准二元正态分布产生的 n 个点的散点图。它也显示了 45 度“相同标准单位”线（红色）和$E(Y∣X)=ρX$（绿色）。

您在数据 8 中看到过这样的图，但是无论如何，请运行单元格几次以刷新内存。您可以看到当回归因子ρ> 0 时；绿线比红线（“相同标准单位”45 度线）更平。

```py
bivariate_normal_regression(0.6, 1000)
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019012520091921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Rob21hc0NhaTAwMQ==,size_16,color_FFFFFF,t_70)

**预测误差**

根据定义，$Y$等于一个“信号”，它是一个$X$的线性函数，再加上一些等于![](http://latex.codecogs.com/gif.latex?\sqrt{1-\rho{^2}}Z)的噪声。基于$X$ 的$Y$的最佳预测为线性函数ρX。

这个预测的均方误差是

![](http://latex.codecogs.com/gif.latex?Var(Y|X)=(1-\rho{}^2)Var(Z)=1-\rho{^2})

它不依赖于$X$。这是有意义的，因为$Y$定义中的“噪声”项与$X$无关。

**垂直带状分布**

如果$X$和$Y$是相关性为ρ的标准二元正态变量，上面的计算可以表明，给定$X = x$的 Y 的条件分布是正态的，平均值为ρx 和方差为![](http://latex.codecogs.com/gif.latex?\sqrt{1-\rho{^2}})。

**预测排名**

假设一大批学生的口语和数学考试成绩的散点图大致为椭圆形，两个变量之间的相关性为 0.5。

给定一个随机挑选的口语成绩排在第 80 分位数的学生，您对这个学生的数学成绩的百分位的预测是什么?

回答这些问题的一种方法是做一些概率假设。对事实的粗糙估计，根据给定的信息，学生的标准化的数学分数$M$和标准化的口语分数$V$有相关性ρ= 0.5 的标准二元正态分布。

考虑到学生的口语成绩是在第 80 分位数上，我们知道他们正处于 Python 所谓的标准正常曲线的 80％点。所以他们的标准单位分数大约是 0.84。

```py
standard_units_x = stats.norm.ppf(0.8)
standard_units_x
>>>0.8416212335729143
```
数学分数标准单位的回归预测是$0.5×0.84 = 0.42$。

```py
rho = 0.5
standard_units_predicted_y = rho * standard_units_x
standard_units_predicted_y
>>>0.42081061678645715
```
标准正常曲线下左侧 0.42 的区域约为 66％，因此您的预测是学生将达到大约数学分数的第 66 分位数。

```py
stats.norm.cdf(standard_units_predicted_y)
>>>0.6630533107760167
```
在这种设置中不要担心小数点和高精度。 计算基于关于数据的概率模型；偏离该模型对预测质量的影响要大于您的答案是第 67 分位数而不是第 66 分位数。

但是，您应该注意到，在答案中可以清楚地看到回归因子。学生预测的数学成绩比他们的口语成绩更接近平均水平。

## 04 回归方程

```py
# HIDDEN
from datascience import *
from prob140 import *
import matplotlib.pyplot as plt
import numpy as np
plt.style.use('fivethirtyeight')
%matplotlib inline
from scipy import stats
```
### 回归方程
基于$X$的预测$Y$的回归方程可以用几种等价的方式表示。回归方程和回归估计中的误差最好用标准单位来表示。所有其他的表示都遵循简单的代数。

令$X$和$Y$是二元正态参数![](http://latex.codecogs.com/gif.latex?(\mu{X},\mu{Y},\sigma{^2}X,\sigma{^2}Y,\rho))。然后，正如我们所见，最佳预测值$E(Y∣X)$是$X$的线性函数，因此$E(Y∣X)$的公式也是回归直线的方程。

**标准单位**

设![](http://latex.codecogs.com/gif.latex?X^*)为标准单位的$X$， ![](http://latex.codecogs.com/gif.latex?Y^*)为标准单位$Y$。回归方程为

![](http://latex.codecogs.com/gif.latex?E(Y^*|X^*)=\rho{X^*})

且预测中的误差量通过下式计算

![](http://latex.codecogs.com/gif.latex?SD(Y^*|X^*)=\sqrt{1-\rho^2})

条件标准差与预测的单位相同。条件方差是：

![](http://latex.codecogs.com/gif.latex?Var(Y^*|X^*)=1-\rho^2)

我们知道的不仅仅是条件期望和条件方差。已知给定![](http://latex.codecogs.com/gif.latex?X^*)的![](http://latex.codecogs.com/gif.latex?Y^*)的条件分布是正态的。这使得我们可以通过常规的正态曲线方法，找到给定![](http://latex.codecogs.com/gif.latex?X^*)的条件概率。例如,

![](http://latex.codecogs.com/gif.latex?P(Y{^*}<y{^*}|X{^*}<x{^*})=\Phi(\frac{y{^*}-\rho{x{^*}}}{\sqrt{1-\rho{^2}}}))

在高尔顿的一个著名数据集中，父子对的身高分布大致是二元正态分布，其相关系数为 0.5。在身高比平均值高出两个标准差的父亲中，儿子身高比平均水平高出两个标准差的比例是多少?

通过回归效应，你知道答案必须小于 50%。如果![](http://latex.codecogs.com/gif.latex?Y^*)表示随机选取的儿子的身高(标准单位)，![](http://latex.codecogs.com/gif.latex?X^*)表示他的父亲的身高(标准单位)，则比例近似为

![](http://latex.codecogs.com/gif.latex?P(Y{^*}>2|X{^*}=2)=1-\Phi(\frac{2-0.5\times2}{\sqrt{1-0.5{^2}}}))

这个值大约是 12.4%。

```py
1 - stats.norm.cdf(2, 0.5*2, np.sqrt(1-0.5**2))
>>> 0.12410653949496186
```

**一种原始形式**

通常，您想要以测量数据的单位进行预测。在改变上述公式中的单位之前，请记住$X$上的条件等价于![](http://latex.codecogs.com/gif.latex?X^*)上的条件。如果你知道 X 或者![](http://latex.codecogs.com/gif.latex?X^*)的值，你也知道另一个。

回归方程为

![](http://latex.codecogs.com/gif.latex?E(Y|X)=E(\sigma{_Y}Y{^*}+\mu{_Y}|X)=\sigma{_Y}E(Y{^*}|X)+\mu{_Y}=\sigma{_Y}\rho(\frac{X-\mu{_X}}{\sigma{_X}})+\mu{_Y}=\rho{\frac{\sigma{_Y}}{\sigma{_X}}}X+(\mu{_Y}-\rho{\frac{\sigma{_Y}}{\sigma{_X}}}\mu{_X}))

这和我们之前推导的最小二乘直线方程是一样的，我们没有对$X$和$Y$的联合分布做任何假设。这证实了我们的观察，如果$X$和$Y$是二元正态的，那么最佳的线性预测器就是所有预测器中最好的。

预测中的误差量由以下两式衡量

![](http://latex.codecogs.com/gif.latex?SD(Y|X)=SD(\sigma{_Y}Y{^*}+\mu{_Y}|X)=\sigma{_Y}SD(Y{^*}|X)=\sqrt{1-\rho{^2}}\sigma{_Y})

和

![](http://latex.codecogs.com/gif.latex?Var(Y|X)=(1-\rho{^2})\sigma^{2}_{Y})

给定$X$下的$Y$的条件分布为正态分布，上面计算的是均值和方差。

**另一种形式**

当只有两个变量时，二元正态的矩阵形式几乎没有必要。但仅用多元正态分布的参数（均值向量和协方差矩阵）就可以写出回归估计和条件方差。这项工作将在下一章中得到反馈，因为完全类似的公式将适用于多元回归。

定义![](http://latex.codecogs.com/gif.latex?\rho{_X,_Y}=Cov(X,Y))。那么$X$和$Y$有多元正态分布，其平均向量为![](http://latex.codecogs.com/gif.latex?[\mu{X},\mu{Y}]{^T})和协方差矩阵

![](http://latex.codecogs.com/gif.latex?
\left[
 \begin{matrix}
   \sigma{^2_X} & \sigma{_X,_Y} \\
   \sigma{_X,_Y} & \sigma{^2_Y} \\
  \end{matrix}
  \right]
)

现在

![](http://latex.codecogs.com/gif.latex?\rho=\frac{\sigma{_X,_Y}}{\sigma{_X}\sigma{_Y}})

并且回归方程能被写为

![](http://latex.codecogs.com/gif.latex?E(X|Y)=\sigma{_Y}\rho(\frac{X-\mu{_X}}{\sigma{_X}})+\mu{_Y}=\frac{\sigma{_X,_Y}}{\sigma{^2_X}}(X-\mu{_X})+\mu{_Y}=\sigma{_Y,_X}(\sigma{^2_X}){^{-1}}(X-\mu{_X})+\mu{_Y})

同时

![](http://latex.codecogs.com/gif.latex?\rho{^2}=\frac{\sigma^{2}_{X,Y}}{\sigma{^2_X}\sigma{^2_Y}})

所以误差的方差为

![](http://latex.codecogs.com/gif.latex?Var(Y|X)=(1-\rho{^2})\sigma{^2_Y}=\sigma{^2_Y}-\sigma^{2}_{X,Y}(\sigma{^2_X}){^{-1}}=\sigma{^2_Y}-\sigma_{Y,X}(\sigma{^2_X}){^{-1}}\sigma_{X,Y})
