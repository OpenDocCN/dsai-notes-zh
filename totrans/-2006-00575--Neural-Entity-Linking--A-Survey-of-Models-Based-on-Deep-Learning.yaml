- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:01:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2006.00575] Neural Entity Linking: A Survey of Models Based on Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2006.00575] 神经实体链接：基于深度学习的模型调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2006.00575](https://ar5iv.labs.arxiv.org/html/2006.00575)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2006.00575](https://ar5iv.labs.arxiv.org/html/2006.00575)
- en: 'Neural Entity Linking: A Survey of Models Based on Deep Learning'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经实体链接：基于深度学习的模型调查
- en: Ö.Özge Sevgililabel=e1]oezge.sevgili.ergueven@studium.uni-hamburg.de Equal contribution.
    Corresponding author. .[    A.Artem Shelmanovlabel=e2]shelmanov@airi.net Equal
    contribution. Corresponding author. .[    M.Mikhail Arkhipovlabel=e3]arkhipov@yahoo.com
    [    A.Alexander Panchenkolabel=e4]a.panchenko@skoltech.ru [    C.Chris Biemannlabel=e5]christian.biemann@uni-hamburg.de
    [ Language Technology Group, \orgnameUniversität Hamburg, Informatikum, Vogt-Kölln-Straße
    30, 22527 Hamburg, \cnyGermanypresep=
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ö.Özge Sevgililabel=e1]oezge.sevgili.ergueven@studium.uni-hamburg.de 平等贡献。通讯作者。[
       A.Artem Shelmanovlabel=e2]shelmanov@airi.net 平等贡献。通讯作者。[    M.Mikhail Arkhipovlabel=e3]arkhipov@yahoo.com
    [    A.Alexander Panchenkolabel=e4]a.panchenko@skoltech.ru [    C.Chris Biemannlabel=e5]christian.biemann@uni-hamburg.de
    [ 语言技术组，\orgname汉堡大学信息学部，Vogt-Kölln-Straße 30, 22527 汉堡，\cny德国presep=
- en: ']e1,e5 Center for Artificial Intelligence Technologies, \orgnameSkolkovo Institute
    of Science and Technology, Bolshoy Boulevard 30, bld. 1, 121205, Moscow, \cnyRussiapresep='
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ']e1,e5 人工智能技术中心，\orgname斯科尔科沃科技学院，Bolshoy Boulevard 30，bld. 1，121205，莫斯科，\cny俄罗斯presep='
- en: ']e4 Research Computing Center, \orgnameLomonosov Moscow State University, GSP-1,
    Leninskie Gory, 119991, Moscow, \cnyRussia AIRI, Nizhny Susalny lane 5 p. 19,
    105064, Moscow, \cnyRussiapresep='
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ']e4 研究计算中心，\orgname罗蒙诺索夫莫斯科国立大学，GSP-1，列宁山，119991，莫斯科，\cny俄罗斯 AIRI，Nizhny Susalny
    lane 5 p. 19，105064，莫斯科，\cny俄罗斯presep='
- en: ']e2 Neural Networks and Deep Learning Laboratory, \orgnameMoscow Institute
    of Physics and Technology, 9 Institutskiy per., Dolgoprudny, 141701, Moscow, \cnyRussiapresep='
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ']e2 神经网络与深度学习实验室，\orgname莫斯科物理技术学院，9 Institutskiy per.，Dolgoprudny，141701，莫斯科，\cny俄罗斯presep='
- en: ']e3 \orgnameFIZ Karlsruhe - Leibniz Institute for Information Infrastructure,
    \cnyGermany \orgnameLIPN, Université Sorbonne Paris Nord, \cnyFrance \orgnameVrije
    University of Amsterdam, \cnythe Netherlands \orgnameKnowledge Media Institute,
    (KMi), The Open University, \cnyUK \orgnameUniversity of Cagliari, \cnyItaly \orgnameFIZ
    Karlsruhe - Leibniz Institute for Information Infrastructure, \cnyGermany \orgnameUniversity
    or Company name, \cnyCountry \orgnameUniversity or Company name, \cnyCountry \orgnameUniversity
    or Company name, \cnyCountry \orgnameUniversity or Company name, \cnyCountry \orgnameUniversity
    or Company name, \cnyCountry \orgnameUniversity or Company name, \cnyCountry \orgnameUniversity
    or Company name, \cnyCountry(0000)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ']e3 \orgnameFIZ Karlsruhe - 莱布尼茨信息基础设施研究所，\cny德国 \orgnameLIPN，巴黎第北大学，\cny法国
    \orgname阿姆斯特丹自由大学，\cny荷兰 \orgname知识媒体研究所 (KMi)，开放大学，\cny英国 \orgname卡利亚里大学，\cny意大利
    \orgnameFIZ Karlsruhe - 莱布尼茨信息基础设施研究所，\cny德国 \orgname大学或公司名称，\cny国家 \orgname大学或公司名称，\cny国家
    \orgname大学或公司名称，\cny国家 \orgname大学或公司名称，\cny国家 \orgname大学或公司名称，\cny国家 \orgname大学或公司名称，\cny国家
    \orgname大学或公司名称，\cny国家（0000）'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This survey presents a comprehensive description of recent neural entity linking
    (EL) systems developed since 2015 as a result of the “deep learning revolution”
    in natural language processing. Its goal is to systemize design features of neural
    entity linking systems and compare their performance to the remarkable classic
    methods on common benchmarks. This work distills a generic architecture of a neural
    EL system and discusses its components, such as candidate generation, mention-context
    encoding, and entity ranking, summarizing prominent methods for each of them.
    The vast variety of modifications of this general architecture are grouped by
    several common themes: joint entity mention detection and disambiguation, models
    for global linking, domain-independent techniques including zero-shot and distant
    supervision methods, and cross-lingual approaches. Since many neural models take
    advantage of entity and mention/context embeddings to represent their meaning,
    this work also overviews prominent entity embedding techniques. Finally, the survey
    touches on applications of entity linking, focusing on the recently emerged use-case
    of enhancing deep pre-trained masked language models based on the Transformer
    architecture.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述全面描述了自2015年以来在自然语言处理中的“深度学习革命”背景下开发的近期神经实体链接（EL）系统。其目标是系统化神经实体链接系统的设计特性，并将其性能与经典方法在常见基准上的表现进行比较。这项工作提炼出神经EL系统的通用架构，并讨论其组件，如候选生成、提及上下文编码和实体排名，总结了每个组件的主要方法。这种通用架构的广泛修改被归纳为几个共同主题：联合实体提及检测与消歧、全局链接模型、包括零样本和远程监督方法在内的领域无关技术，以及跨语言方法。由于许多神经模型利用实体和提及/上下文嵌入来表示其意义，这项工作还概述了突出的实体嵌入技术。最后，综述涉及实体链接的应用，重点讨论了最近出现的基于Transformer架构的深度预训练掩码语言模型的增强用例。
- en: Entity Linking,Deep Learning,Neural Networks,Natural Language Processing,Knowledge
    Graphs,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接、深度学习、神经网络、自然语言处理、知识图谱，
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: '^†^†volume: 0'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†卷号：0
- en: 'Disclaimer: ©  Özge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko,
    and Chris Biemann, 2022\. The definitive, peer reviewed and edited version of
    this article is published in the Semantic Web Journal, Special Issue on Deep Learning
    and Knowledge Graphs, 2022'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明：©  Özge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko,
    和 Chris Biemann, 2022。本文的最终版经过同行评审和编辑，已发表于《语义网杂志》，深度学习与知识图谱特刊，2022年
- en: ', , , ,'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ', , , ,'
- en: '{review}'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '{评审}'
- en: editor \reviewerMehwish Alam \reviewerDavide Buscaldi \reviewerMichael Cochez
    \reviewerFrancesco Osborne \reviewerDiego Reforgiato Recupero \reviewerHarald
    Sack
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑 \评审人梅赫维什·阿拉姆 \评审人大维德·布斯卡尔迪 \评审人迈克尔·科切兹 \评审人弗朗切斯科·奥斯本 \评审人迭戈·雷福尔贾托·雷库佩罗 \评审人哈拉德·萨克
- en: '{review}'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '{评审}'
- en: solicited \reviewerItalo Lopes Oliveira \reviewerSahar Vahdati \reviewerMojtaba
    Nayyeri \reviewerDaza Cruz \reviewerAnonymous {review}open \reviewerFirst Open
    Reviewer \reviewerSecond Open Reviewer
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 征稿 \评审人意大利·洛佩斯·奥利维拉 \评审人萨哈尔·瓦赫达提 \评审人莫伊塔巴·纳耶里 \评审人达扎·克鲁兹 \评审人匿名 {评审}开放 \评审人第一位开放评审人
    \评审人第二位开放评审人
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Knowledge Graphs (KGs), such as Freebase [[Bollacker et al. (2008)](#bib.bib14)],
    DBpedia [[Lehmann et al. (2015)](#bib.bib92)], and Wikidata [[Vrandečić and Krötzsch
    (2014)](#bib.bib184)], contain rich and precise information about entities of
    all kinds, such as persons, locations, organizations, movies, and scientific theories,
    just to name a few. Each entity has a set of carefully defined relations and attributes,
    e.g. “was born in” or “play for”. This wealth of structured information gives
    rise to and facilitates the development of semantic processing algorithms as they
    can directly operate on and benefit from such entity representations. For instance,
    imagine a search engine that is able to retrieve mentions in the news during the
    last month of all retired NBA players with a net income of more than 1 billion
    US dollars. The list of players together with their income and retirement information
    may be available in a knowledge graph. Equipped with this information, it appears
    to be straightforward to look up mentions of retired basketball players in the
    newswire. However, the main obstacle in this setup is the lexical ambiguity of
    entities. In the context of this application, one would want to only retrieve
    all mentions of “Michael Jordan (basketball player)”¹¹1[https://en.wikipedia.org/wiki/MichaelJordan](https://en.wikipedia.org/wiki/MichaelJordan)
    and exclude mentions of other persons with the same name such as “Michael Jordan
    (mathematician)”²²2[https://en.wikipedia.org/wiki/MichaelI.Jordan](https://en.wikipedia.org/wiki/MichaelI.Jordan).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KGs），如 Freebase [[Bollacker et al. (2008)](#bib.bib14)]、DBpedia [[Lehmann
    et al. (2015)](#bib.bib92)] 和 Wikidata [[Vrandečić and Krötzsch (2014)](#bib.bib184)]，包含了关于各种实体的丰富而精确的信息，例如人名、地点、组织、电影和科学理论，仅举几例。每个实体都有一组精心定义的关系和属性，例如“出生地”或“效力于”。这种结构化信息的丰富性促进了语义处理算法的发展，因为这些算法可以直接操作和利用这些实体表示。例如，假设有一个搜索引擎可以检索到上个月新闻中所有净收入超过10亿美元的退役NBA球员的提及。包含球员名单及其收入和退役信息的知识图谱可能会提供这些信息。有了这些信息，查找新闻中的退役篮球运动员的提及似乎是直接的。然而，这种设置的主要障碍是实体的词汇歧义。在这种应用的背景下，人们只希望检索到“迈克尔·乔丹（篮球运动员）”¹¹1[https://en.wikipedia.org/wiki/MichaelJordan](https://en.wikipedia.org/wiki/MichaelJordan)的提及，并排除其他同名人物如“迈克尔·乔丹（数学家）”²²2[https://en.wikipedia.org/wiki/MichaelI.Jordan](https://en.wikipedia.org/wiki/MichaelI.Jordan)的提及。
- en: This is why Entity Linking (EL) – the process of matching a mention, e.g. “Michael
    Jordan”, in a textual context to a KG record (e.g. “basketball player” or “mathematician”)
    fitting the context – is the key technology enabling various semantic applications.
    Thus, EL is the task of identifying an entity mention in the (unstructured) text
    and establishing a link to an entry in a (structured) knowledge graph.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么实体链接（EL）——将文本上下文中的提及，例如“迈克尔·乔丹”，与符合上下文的知识图谱记录（例如“篮球运动员”或“数学家”）匹配的过程——是实现各种语义应用的关键技术。因此，EL的任务是识别文本中的实体提及并将其链接到知识图谱中的条目。
- en: 'Entity linking is an essential component of many information extraction (IE)
    and natural language understanding (NLU) pipelines since it resolves the lexical
    ambiguity of entity mentions and determines their meanings in context. A link
    between a textual mention and an entity in a knowledge graph also allows us to
    take advantage of the information encompassed in a semantic graph, which is shown
    to be useful in such NLU tasks as information extraction, biomedical text processing,
    or semantic parsing and question answering (see Section [5](#S5 "5 Applications
    of Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")).
    This wide range of direct applications is the reason why entity linking is enjoying
    great interest from both academy and industry for more than two decades.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '实体链接是许多信息提取（IE）和自然语言理解（NLU）流程中的关键组件，因为它解决了实体提及的词汇歧义问题，并确定了它们在上下文中的意义。文本提及与知识图谱中实体之间的链接还使我们能够利用语义图中的信息，这在信息提取、生物医学文本处理、语义解析和问答等NLU任务中被证明是有用的（见第
    [5](#S5 "5 Applications of Entity Linking ‣ Neural Entity Linking: A Survey of
    Models Based on Deep Learning) 节）。这种广泛的直接应用是实体链接在学术界和工业界都受到极大关注的原因，已有二十多年。'
- en: 1.1 Goal and Scope of this Survey
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 本调查的目标和范围
- en: Recently, a new generation of approaches for entity linking based on neural
    models and deep learning emerged, pushing the state-of-the-art performance in
    this task to a new level. The goal of our survey is to provide an overview of
    this latest wave of models, emerging from 2015.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于神经模型和深度学习的实体链接新一代方法应运而生，将这一任务的最先进表现提升到一个新水平。我们调查的目标是概述这一最新波段的模型，这些模型自2015年开始涌现。
- en: 'Models based on neural networks have managed to excel in EL as in many other
    natural language processing tasks due to their ability to learn useful distributed
    semantic representations of linguistic data [[Collobert et al. (2011)](#bib.bib30),
    [Young et al. (2018)](#bib.bib203), [Bengio et al. (2003)](#bib.bib11)]. These
    current state-of-the-art neural entity linking models have shown significant improvements
    over “classical”³³3On classical ML vs deep learning: [https://towardsdatascience.com/deep-learning-vs-classical-machine-learning-9a42c6d48aa](https://towardsdatascience.com/deep-learning-vs-classical-machine-learning-9a42c6d48aa)
    machine learning approaches [[Lazic et al. (2015)](#bib.bib84), [Ratinov et al.
    (2011)](#bib.bib148), [Chisholm and Hachey (2015)](#bib.bib27)] to name a few
    that are based on shallow architectures, e.g. Support Vector Machines, and/or
    depend mostly on hand-crafted features. Such models often cannot capture all relevant
    statistical dependencies and interactions [[Ganea and Hofmann (2017)](#bib.bib53)].
    In contrast, deep neural networks are able to learn sophisticated representations
    within their deep layered architectures. This reduces the burden of manual feature
    engineering and enables significant improvements in EL and other tasks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的模型在实体链接（EL）以及许多其他自然语言处理任务中表现出色，这是由于它们能够学习语言数据的有用分布式语义表示[[Collobert et
    al. (2011)](#bib.bib30), [Young et al. (2018)](#bib.bib203), [Bengio et al. (2003)](#bib.bib11)]。这些当前最先进的神经实体链接模型相较于“经典”³³3关于经典机器学习与深度学习的对比：
    [https://towardsdatascience.com/deep-learning-vs-classical-machine-learning-9a42c6d48aa](https://towardsdatascience.com/deep-learning-vs-classical-machine-learning-9a42c6d48aa)
    机器学习方法[[Lazic et al. (2015)](#bib.bib84), [Ratinov et al. (2011)](#bib.bib148),
    [Chisholm and Hachey (2015)](#bib.bib27)]等基于浅层架构（例如支持向量机）和/或主要依赖手工特征的模型显示出显著的改进。这些模型往往无法捕捉所有相关的统计依赖性和交互[[Ganea
    and Hofmann (2017)](#bib.bib53)]。相比之下，深度神经网络能够在其深层架构中学习复杂的表示。这减少了手动特征工程的负担，并在实体链接及其他任务中实现了显著的改进。
- en: 'In this survey, we systemize recently proposed neural models, distilling one
    generic architecture used by the majority of neural EL models (illustrated in
    Figures [2](#S1.F2 "Figure 2 ‣ 1.4 Contributions ‣ 1 Introduction ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning") and [5](#S3.F5 "Figure 5
    ‣ 3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣
    Neural Entity Linking: A Survey of Models Based on Deep Learning")). We describe
    the models used in each component of this architecture, e.g. candidate generation,
    mention-context encoding, entity ranking. Prominent variations of this generic
    architecture, e.g. end-to-end EL or global models, are also discussed. To better
    structure the sheer amount of available models, various types of methods are illustrated
    in taxonomies (Figures [3](#S2.F3 "Figure 3 ‣ 2.3 Terminological Aspects ‣ 2 Task
    Description ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")
    and [6](#S3.F6 "Figure 6 ‣ 3.1.4 Entity Ranking ‣ 3.1 General Architecture ‣ 3
    Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning")), while notable features of each model are carefully assembled in a
    tabular form (Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")). We discuss
    the performance of the models on commonly used entity linking/disambiguation benchmarks
    and an entity relatedness dataset. Because of the sheer amount of work, it was
    not possible for us to try available software and to compare approaches on further
    parameters, such as computational complexity, run-time, and memory requirements.
    Nevertheless, we created a comprehensive collection of references to publicly
    available official implementations of EL models and systems discussed in this
    survey (see Table [7](#A1.T7 "Table 7 ‣ Appendix A Public Implementations of Neural
    Entity Linking Models ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning") in Appendix A).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在本次调查中，我们系统化了最近提出的神经模型，提炼出大多数神经实体链接模型使用的通用架构（如图[2](#S1.F2 "Figure 2 ‣ 1.4 Contributions
    ‣ 1 Introduction ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")和[5](#S3.F5
    "Figure 5 ‣ 3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")所示）。我们描述了该架构中每个组件使用的模型，例如候选生成、提及上下文编码、实体排名。还讨论了该通用架构的显著变体，如端到端实体链接或全局模型。为了更好地结构化大量可用模型，各种方法类型在分类法中进行了说明（图[3](#S2.F3
    "Figure 3 ‣ 2.3 Terminological Aspects ‣ 2 Task Description ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning")和[6](#S3.F6 "Figure 6 ‣ 3.1.4 Entity
    Ranking ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning")），而每个模型的显著特征则以表格形式仔细汇总（表[2](#S3.T2
    "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning")）。我们讨论了模型在常用实体链接/消歧 benchmark 和实体相关性数据集上的表现。由于工作量巨大，我们无法尝试可用的软件，并比较进一步的参数，如计算复杂性、运行时间和内存需求。然而，我们创建了一个全面的参考资料集合，包含本调查中讨论的实体链接模型和系统的公开官方实现（见附录
    A 的表[7](#A1.T7 "Table 7 ‣ Appendix A Public Implementations of Neural Entity Linking
    Models ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")）。'
- en: An important component of neural entity linking systems is distributed entity
    representations and entity encoding methods. It has been shown that encoding the
    KG structure (entity relationships), entity definitions, or word/entity co-occurrence
    statistics from large textual corpora in low-dimensional vectors improves the
    generalization capabilities of EL models [[Huang et al. (2015)](#bib.bib70), [Ganea
    and Hofmann (2017)](#bib.bib53)]. Therefore, we also summarize distributed entity
    representation models and novel methods for entity encoding.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 神经实体链接系统的一个重要组成部分是分布式实体表示和实体编码方法。研究表明，将知识图谱结构（实体关系）、实体定义或来自大规模文本语料库的词汇/实体共现统计信息编码为低维向量，可以提高实体链接模型的泛化能力[[Huang
    et al. (2015)](#bib.bib70)，[Ganea and Hofmann (2017)](#bib.bib53)]。因此，我们还总结了分布式实体表示模型和实体编码的新方法。
- en: Many natural language processing systems take advantage of deep pre-trained
    language models like ELMo [[Peters et al. (2018)](#bib.bib138)], BERT [[Devlin
    et al. (2019)](#bib.bib36)], and their modifications. EL made its path into these
    models as a way of introducing information stored in KGs, which helps to adapt
    word representations to some text processing tasks. We discuss this novel application
    of EL and its further development.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言处理系统利用了深度预训练语言模型，如 ELMo [[Peters et al. (2018)](#bib.bib138)]，BERT [[Devlin
    et al. (2019)](#bib.bib36)]，以及它们的变体。EL 已经融入这些模型中，作为引入存储在 KG 中的信息的一种方式，这有助于将词汇表示适应某些文本处理任务。我们讨论了
    EL 的这一新颖应用及其进一步发展。
- en: 1.2 Article Collection Methodology
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 文章收集方法
- en: We do not have a strict article collection algorithm for the review like e.g.,
    the one conducted by Oliveira et al. ([2021](#bib.bib130)). Our main goal is to
    provide and describe a conceptual framework that can be applied to the majority
    of recently presented neural approaches to EL. Nevertheless, as with all surveys,
    we had to draw the line somewhere. The main criteria for including papers into
    this survey was that they had been published during or after 2015, and they primarily
    address the task of EL, i.e. resolving textual mentions to entries in KGs, or
    discussing EL applications. We explicitly exclude related work e.g., on (fine-grained)
    entity typing (see Aly et al. ([2021](#bib.bib4)); Choi et al. ([2018](#bib.bib28))),
    which also encompasses a disambiguation task, and work that employs KGs for other
    tasks than EL. This survey also does not try to cover all EL methods designed
    for specific domains like biomedical texts or messages in social media. For the
    general-purpose EL models evaluated on well-established benchmarks, we try to
    be as comprehensive as possible with respect to recent-enough papers that fit
    into the conceptual framework, no matter where they have appeared (however, with
    a focus on top conferences and journals in the fields of natural language processing
    and Semantic Web).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有像 Oliveira 等人 ([2021](#bib.bib130)) 进行的那样的严格文章收集算法。我们的主要目标是提供并描述一个可以应用于大多数最近提出的神经
    EL 方法的概念框架。然而，与所有调查一样，我们不得不在某个点上做出取舍。纳入本次调查的主要标准是这些论文必须在 2015 年或之后发表，并且主要关注 EL
    任务，即将文本提及解析为 KG 中的条目，或讨论 EL 应用。我们明确排除了与 (细粒度) 实体类型（参见 Aly 等人 ([2021](#bib.bib4));
    Choi 等人 ([2018](#bib.bib28))) 相关的工作，这些工作也包括一个消歧任务，以及用于其他任务而非 EL 的 KG 的工作。该调查也不试图涵盖为特定领域（如生物医学文本或社交媒体消息）设计的所有
    EL 方法。对于在成熟基准上评估的通用 EL 模型，我们尽可能全面地涵盖适合概念框架的近期论文，无论它们出现在何处（但重点关注自然语言处理和语义网领域的顶级会议和期刊）。
- en: '![Refer to caption](img/b3a73f80d0fbf489d7378396c96dbf81.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b3a73f80d0fbf489d7378396c96dbf81.png)'
- en: 'Figure 1: The entity linking task. An Entity Linking (EL) model takes a raw
    textual input and enriches it with entity mentions linked to nodes in a Knowledge
    Graph (KG). The task is commonly split into entity mention detection and entity
    disambiguation sub-tasks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：实体链接任务。一个实体链接（EL）模型接收原始文本输入，并通过将实体提及链接到知识图谱（KG）中的节点来丰富它。该任务通常分为实体提及检测和实体消歧两个子任务。
- en: 1.3 Previous Surveys
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 以往的调查
- en: One of the first surveys on EL was prepared by Shen et al. ([2015](#bib.bib160))
    in 2015\. They cover the main approaches to entity linking (within the modules,
    e.g. candidate generation, ranking), its applications, evaluation methods, and
    future directions. In the same year, Ling et al. ([2015](#bib.bib97)) presented
    a work that aims to provide (1) a standard problem definition to reduce confusion
    that appears due to the existence of variant similar tasks related to EL (e.g.,
    Wikification Milne and Witten ([2008](#bib.bib112)) and named entity linking Hoffart
    et al. ([2011](#bib.bib67))), and (2) a clear comparison of models and their various
    aspects.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 EL 的首次调查之一是由 Shen 等人 ([2015](#bib.bib160)) 在 2015 年准备的。他们涵盖了实体链接的主要方法（例如候选生成、排名等模块）、其应用、评估方法以及未来方向。在同一年，Ling
    等人 ([2015](#bib.bib97)) 提出了旨在提供（1）一个标准问题定义，以减少因存在与 EL 相关的不同类似任务而产生的混淆（例如，Wikification
    Milne 和 Witten ([2008](#bib.bib112)) 和命名实体链接 Hoffart 等人 ([2011](#bib.bib67)))，以及（2）对模型及其各个方面的明确比较。
- en: There are also other surveys that address a wider scope. The work of Martínez-Rodríguez
    et al. ([2020](#bib.bib106)), published in 2020, involves information extraction
    models and semantic web technologies. Namely, they consider many tasks, like named
    entity recognition, entity linking, terminology extraction, keyphrase extraction,
    topic modeling, topic labeling, relation extraction tasks. In a similar vein,
    the work of Al-Moslmi et al. ([2020](#bib.bib3)), released in 2020, overviews
    the research in named entity recognition, named entity disambiguation, and entity
    linking published between 2014 and 2019.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他调查涉及更广泛的范围。 Martínez-Rodríguez 等人 ([2020](#bib.bib106)) 于 2020 年发表的工作涉及信息提取模型和语义网技术。具体来说，他们考虑了许多任务，如命名实体识别、实体链接、术语提取、关键词提取、主题建模、主题标注、关系提取任务。类似地，Al-Moslmi
    等人 ([2020](#bib.bib3)) 于 2020 年发布的工作概述了 2014 至 2019 年间在命名实体识别、命名实体消歧和实体链接领域的研究。
- en: 'Another recent survey paper by Oliveira et al. ([2021](#bib.bib130)), published
    in 2020, analyses and summarizes EL approaches that exhibit some holism. This
    viewpoint limits the survey to the works that exploit various peculiarities of
    the EL task: additional metadata stored in specific input like microblogs, specific
    features that can be extracted from this input like geographic coordinates in
    tweets, timestamps, interests of users posted these tweets, and specific disambiguation
    methods that take advantage of these additional features. In the concurrent work,
    Möller et al. ([2022](#bib.bib118)) overview models developed specifically for
    linking English entities to the Wikidata Vrandečić and Krötzsch ([2014](#bib.bib184))
    and discuss features of this KG that can be exploited for increasing the linking
    performance.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Oliveira 等人 ([2021](#bib.bib130)) 于 2020 年发表的另一篇近期调查论文分析并总结了表现出一定整体性的 EL 方法。这种观点将调查范围限制在利用
    EL 任务各种特殊性的工作上：存储在特定输入中的附加元数据（如微博）、从这些输入中提取的特定特征（如推文中的地理坐标、时间戳、发布这些推文的用户兴趣），以及利用这些附加特征的特定消歧方法。在同期的工作中，Möller
    等人 ([2022](#bib.bib118)) 概述了专门为将英语实体链接到 Wikidata Vrandečić 和 Krötzsch ([2014](#bib.bib184))
    开发的模型，并讨论了可以利用这些 KG 特征来提高链接性能的特性。
- en: Previous surveys on similar topics (a) do not cover many recent publications
    Ling et al. ([2015](#bib.bib97)); Shen et al. ([2015](#bib.bib160)), (b) broadly
    cover numerous topics Martínez-Rodríguez et al. ([2020](#bib.bib106)); Al-Moslmi
    et al. ([2020](#bib.bib3)), or (c) are focused on the specific types of methods
    Oliveira et al. ([2021](#bib.bib130)) or a knowledge graph Möller et al. ([2022](#bib.bib118)).
    There is not yet, to our knowledge, a detailed survey specifically devoted to
    recent neural entity linking models. The previous surveys also do not address
    the topics of entity and context/mention encoding, applications of EL to deep
    pre-trained language models, and cross-lingual EL. We are also the first to summarize
    the domain-independent approaches to EL, several of which are based on zero-shot
    techniques.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以往对类似主题的调查 (a) 未涵盖许多最近的出版物，如 Ling 等人 ([2015](#bib.bib97)) 和 Shen 等人 ([2015](#bib.bib160))；(b)
    广泛涵盖了多个主题，如 Martínez-Rodríguez 等人 ([2020](#bib.bib106)) 和 Al-Moslmi 等人 ([2020](#bib.bib3))；或
    (c) 仅专注于特定类型的方法，如 Oliveira 等人 ([2021](#bib.bib130)) 或知识图谱 Möller 等人 ([2022](#bib.bib118))。据我们所知，目前尚未有专门致力于近期神经实体链接模型的详细调查。以往的调查也未涉及实体和上下文/提及编码、EL
    应用于深度预训练语言模型以及跨语言 EL 的主题。我们也是首次总结领域独立的 EL 方法，其中几种方法基于零样本技术。
- en: 1.4 Contributions
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 贡献
- en: 'More specifically, this article makes the following contributions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本文做出了以下贡献：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a survey of state-of-the-art neural entity linking models;
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对最先进的神经实体链接模型的调查；
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a systematization of various features of neural EL methods and their evaluation
    results on popular benchmarks;
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对神经 EL 方法各种特征及其在流行基准上的评估结果的系统化；
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a summary of entity and context/mention embedding techniques;
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对实体和上下文/提及嵌入技术的总结；
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a discussion of recent domain-independent (zero-shot) and cross-lingual EL approaches;
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对最近的领域独立（零样本）和跨语言实体链接方法的讨论；
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a survey of EL applications to modeling word representations.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对 EL 应用于建模词表示的调查。
- en: 'The structure of this survey is the following. We start with defining the EL
    task in Section [2](#S2 "2 Task Description ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning"). In Section [3.1](#S3.SS1 "3.1 General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning"), the general architecture of neural entity linking systems is
    presented. Modifications and variations of this basic pipeline are discussed in
    Section [3.2](#S3.SS2 "3.2 Modifications of the General Architecture ‣ 3 Neural
    Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").
    In Section [4](#S4 "4 Evaluation ‣ Neural Entity Linking: A Survey of Models Based
    on Deep Learning"), we summarize the performance of EL models on standard benchmarks
    and present results of the entity relatedness evaluation. Section [5](#S5 "5 Applications
    of Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")
    is dedicated to applications of EL with a focus on recently emerged applications
    for improving neural language models. Finally, Section [6](#S6 "6 Conclusion ‣
    Neural Entity Linking: A Survey of Models Based on Deep Learning") concludes the
    survey and suggests promising directions of future work.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的结构如下。我们首先在第 [2](#S2 "2 Task Description ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning") 节定义 EL 任务。在第 [3.1](#S3.SS1 "3.1 General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning") 节，介绍了神经实体链接系统的一般架构。第 [3.2](#S3.SS2 "3.2 Modifications of the General
    Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning") 节讨论了该基本流程的修改和变体。在第 [4](#S4 "4 Evaluation ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning") 节，我们总结了 EL 模型在标准基准测试上的性能，并呈现了实体相关性评估的结果。第
    [5](#S5 "5 Applications of Entity Linking ‣ Neural Entity Linking: A Survey of
    Models Based on Deep Learning") 节专注于 EL 的应用，特别是针对近期出现的改进神经语言模型的应用。最后，第 [6](#S6
    "6 Conclusion ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")
    节总结了调查结果，并提出了未来工作的有希望方向。'
- en: '![Refer to caption](img/35c8bf1232cd4788d380dabcb918e09f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/35c8bf1232cd4788d380dabcb918e09f.png)'
- en: 'Figure 2: General architecture for neural entity linking. Entity Linking (EL)
    consists of two main steps: Mention Detection (MD), when entity mention boundaries
    in a text are identified, and Entity Disambiguation (ED), when a corresponding
    entity is predicted for the given mention. Entity disambiguation is further carried
    out in two steps: Candidate Generation, when possible candidate entities are selected
    for the mention, and Entity Ranking, when a correspondence score between context/mention
    and each candidate is computed through the comparison of their vector representations.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：神经实体链接的总体架构。实体链接（EL）包括两个主要步骤：提及检测（MD），在文本中识别实体提及边界，以及实体消歧（ED），为给定的提及预测相应的实体。实体消歧进一步分为两个步骤：候选生成，当选择可能的候选实体进行提及时，以及实体排序，当通过比较它们的向量表示计算上下文/提及与每个候选实体之间的对应分数。
- en: 2 Task Description
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 任务描述
- en: 2.1 Informal Definition
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 非正式定义
- en: 'Consider the example presented in Figure [1](#S1.F1 "Figure 1 ‣ 1.2 Article
    Collection Methodology ‣ 1 Introduction ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning") with an entity mention Scott Young in a soccer-game-related
    context. Literally, this common name can refer to at least three different people:
    the American football player, the Welsh football player, or the writer. The EL
    task is to (1) correctly detect the entity mention in the text, (2) resolve its
    ambiguity and ultimately provide a link to a corresponding entity entry in a KG,
    e.g. provide for the Scott Young mention in this context a link to the Welsh footballer⁴⁴4[https://en.wikipedia.org/wiki/ScottYoung(Welshfootballer)](https://en.wikipedia.org/wiki/ScottYoung(Welshfootballer))
    instead of the writer⁵⁵5[https://en.wikipedia.org/wiki/ScottYoung(writer)](https://en.wikipedia.org/wiki/ScottYoung(writer)).
    To achieve this goal, the task is usually decomposed into two sub-tasks, as illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1.2 Article Collection Methodology ‣ 1 Introduction
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning"): Mention
    Detection (MD) and Entity Disambiguation (ED).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以图 [1](#S1.F1 "图 1 ‣ 1.2 文章收集方法 ‣ 1 引言 ‣ 神经实体链接：基于深度学习的模型调查") 为例，其中提到的实体 Scott
    Young 出现在一个与足球比赛相关的上下文中。字面上，这个常见的名字可以指代至少三个人：美国橄榄球运动员、威尔士足球运动员或作家。实体链接任务是 (1)
    正确检测文本中的实体提及，(2) 解决其歧义并最终提供指向知识图谱中相应实体条目的链接，例如，在这种情况下，为 Scott Young 提供指向威尔士足球运动员的链接⁴⁴4[https://en.wikipedia.org/wiki/ScottYoung(Welshfootballer)](https://en.wikipedia.org/wiki/ScottYoung(Welshfootballer))
    而不是作家⁵⁵5[https://en.wikipedia.org/wiki/ScottYoung(writer)](https://en.wikipedia.org/wiki/ScottYoung(writer))。为了实现这个目标，任务通常被分解为两个子任务，如图
    [1](#S1.F1 "图 1 ‣ 1.2 文章收集方法 ‣ 1 引言 ‣ 神经实体链接：基于深度学习的模型调查") 中所示：提及检测（MD）和实体消歧（ED）。
- en: 2.2 Formal Definition
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 正式定义
- en: 2.2.1 Knowledge Graph (KG)
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 知识图谱（KG）
- en: 'A KG contains entities, relations, and facts, where facts are denoted as triples
    (i.e. head entity, relation, tail entity) as defined in Ji et al. ([2022](#bib.bib77)).
    Formally, as defined by Färber et al. ([2018](#bib.bib45)), a KG is a set of RDF
    triples where each triple $(s,p,o)$ is an ordered set of the following terms:
    a subject $s\in U\cup B$, a predicate $p\in U$, and an object $o\in U\cup B\cup
    L$. An RDF term is either a URI $u\in U$, a blank node $b\in B$, or a literal
    $l\in L$. URI (or IRI) nodes are for the global identification of entities on
    the Web; literal nodes are for strings and other datatype values (e.g. integers,
    dates); and the blank node is for anonymous nodes, which are not assigned an identifier,
    as explained in Hogan et al. ([2021](#bib.bib68)).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个知识图谱（KG）包含实体、关系和事实，其中事实表示为三元组（即头实体、关系、尾实体），如 Ji 等人（[2022](#bib.bib77)）所定义。正式地，如
    Färber 等人（[2018](#bib.bib45)）所定义，知识图谱是 RDF 三元组的集合，其中每个三元组 $(s,p,o)$ 是以下术语的有序集合：一个主体
    $s\in U\cup B$、一个谓词 $p\in U$ 和一个对象 $o\in U\cup B\cup L$。RDF 术语可以是 URI $u\in U$、空白节点
    $b\in B$ 或文字 $l\in L$。URI（或 IRI）节点用于全球识别 Web 上的实体；文字节点用于字符串和其他数据类型的值（例如整数、日期）；空白节点用于匿名节点，这些节点没有分配标识符，如
    Hogan 等人（[2021](#bib.bib68)）所解释。
- en: This RDF representation can be considered as a multi-relational graph $G=(E,\mathbb{A}=\{A_{\_}0,A_{\_}1,...,A_{\_}m\subseteq(E\times
    E)\})$, where $E$ is a set of all entities of a KG, and $\mathbb{A}$ is a family
    of typed edge sets of length $m$. For example, $A_{\_}0$ is the “occupation” predicate
    adjacency matrix, $A_{\_}1$ is the “founded” predicate adjacency matrix, etc.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 RDF 表示可以看作是一个多关系图 $G=(E,\mathbb{A}=\{A_{\_}0,A_{\_}1,...,A_{\_}m\subseteq(E\times
    E)\})$，其中 $E$ 是 KG 中所有实体的集合，$\mathbb{A}$ 是长度为 $m$ 的类型边集族。例如，$A_{\_}0$ 是“职业”谓词的邻接矩阵，$A_{\_}1$
    是“创立”谓词的邻接矩阵，等等。
- en: There is also an equivalent three-way tensor representation of a KG $\mathcal{A}\in\{0,1\}^{n\times
    m\times n}$, where
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个等效的三维张量表示的知识图谱 $\mathcal{A}\in\{0,1\}^{n\times m\times n}$，其中
- en: '|  | $\mathcal{A}_{\_}{i,k,j}=\begin{cases}1&amp;\text{if $(i,j)\in A_{\_}k:k\leq
    m$}\\ 0&amp;\text{otherwise.}\end{cases}$ |  | (1) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{\_}{i,k,j}=\begin{cases}1&amp;\text{if $(i,j)\in A_{\_}k:k\leq
    m$}\\ 0&amp;\text{otherwise.}\end{cases}$ |  | (1) |'
- en: 2.2.2 Mention Detection (MD)
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 提及检测（MD）
- en: 'The goal of mention detection is to identify an entity mention span, while
    entity disambiguation performs linking of found mentions to entries of a KG. We
    can consider this task as determining an $\mathsf{MD}$ function that takes as
    input a textual context $c_{\_}i\in C$ (e.g. a document in a document collection)
    and outputs a sequence of $n$ mentions $(m_{\_}1,\dots m_{\_}n)$ in this context
    $m_{\_}i\in M$, where $M$ is a set of all possible text spans in the context:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 提及检测的目标是识别实体提及范围，而实体消歧则是将找到的提及链接到知识图谱的条目。我们可以将这个任务视为确定一个 $\mathsf{MD}$ 函数，该函数以文本上下文
    $c_{\_}i\in C$（例如，一个文档集合中的文档）为输入，并输出该上下文中 $n$ 个提及的序列 $(m_{\_}1,\dots m_{\_}n)$，其中
    $m_{\_}i\in M$，$M$ 是上下文中所有可能的文本范围的集合。
- en: '|  | $\mathsf{MD}:C\xrightarrow{}M^{n}.$ |  | (2) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{MD}:C\xrightarrow{}M^{n}.$ |  | (2) |'
- en: In the majority of works on EL, it is assumed that the mentions are already
    given or detected, for example, using a named entity recognition (NER) system
    (sometimes called named entity recognition and classification (NERC) Aly et al.
    ([2021](#bib.bib4)); Nadeau and Sekine ([2007](#bib.bib119))). We should note
    that, usually, in addition to MD, NER systems also tag/classify mentions with
    a predefined types Li et al. ([2022](#bib.bib95)); van Hulst et al. ([2020](#bib.bib181));
    Oliveira et al. ([2021](#bib.bib130)); Martins et al. ([2019](#bib.bib107)) that
    also can be leveraged for disambiguation Martins et al. ([2019](#bib.bib107)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数关于 EL 的研究中，通常假设提及已经给出或被检测到，例如，使用命名实体识别（NER）系统（有时称为命名实体识别与分类（NERC） Aly 等（[2021](#bib.bib4)）；Nadeau
    和 Sekine（[2007](#bib.bib119)））。我们应该注意，通常除了 MD，NER 系统还会用预定义的类型标记/分类提及 Li 等（[2022](#bib.bib95)）；van
    Hulst 等（[2020](#bib.bib181)）；Oliveira 等（[2021](#bib.bib130)）；Martins 等（[2019](#bib.bib107)），这些类型也可以用于消歧
    Martins 等（[2019](#bib.bib107)）。
- en: 2.2.3 Entity Disambiguation (ED)
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 实体消歧（ED）
- en: 'The entity disambiguation task can be considered as determining a function
    $\mathsf{ED}$ that, given a sequence of $n$ mentions in a document and their contexts
    $(c_{\_}1,\dots,c_{\_}n)$, outputs an entity assignment $(e_{\_}1,\dots,e_{\_}n),e_{\_}i\in
    E$, where $E$ is a set of entities in a KG:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实体消歧任务可以被视为确定一个函数 $\mathsf{ED}$，该函数在给定文档中的 $n$ 个提及及其上下文 $(c_{\_}1,\dots,c_{\_}n)$
    的情况下，输出一个实体分配 $(e_{\_}1,\dots,e_{\_}n),e_{\_}i\in E$，其中 $E$ 是一个知识图谱中的实体集合。
- en: '|  | $\mathsf{ED}:(M,C)^{n}\xrightarrow{}E^{n}.$ |  | (3) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{ED}:(M,C)^{n}\xrightarrow{}E^{n}.$ |  | (3) |'
- en: To learn a mapping from entity mentions in a context to entity entries in a
    KG, EL models use supervision signals like manually annotated mention-entity pairs.
    The size of KGs varies; they can contain hundreds of thousands or even millions
    of entities. Due to their large size, training data for EL would be extremely
    unbalanced; training sets can lack even a single example for a particular entity
    or mention, e.g. as in the popular AIDA corpus Hoffart et al. ([2011](#bib.bib67)).
    To deal with this problem, EL models should have wide generalization capabilities.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习从上下文中的实体提及到知识图谱中的实体条目的映射，实体链接（EL）模型使用像手动标注的提及-实体对这样的监督信号。知识图谱的大小各异；它们可以包含数十万甚至数百万个实体。由于其庞大的规模，EL
    的训练数据会极度不平衡；训练集可能缺少某个特定实体或提及的一个示例，例如在流行的 AIDA 语料库中 Hoffart 等（[2011](#bib.bib67)）。为了解决这个问题，EL
    模型应具备广泛的泛化能力。
- en: 'Despite KGs being usually large, they are incomplete. Therefore, some mentions
    in a text cannot be correctly mapped to any KG entry. Determining such unlinkable
    mentions, which usually is designated as linking to a $\mathsf{NIL}$ entry, is
    one of the current EL challenges. Methods that address this problem provide a
    separate function for it or extend the set of entities in the disambiguation function
    with this special entry:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知识图谱通常很大，但它们是不完整的。因此，文本中的一些提及无法正确映射到任何知识图谱条目。确定这些无法链接的提及，通常被指定为链接到 $\mathsf{NIL}$
    条目，是当前 EL 面临的挑战之一。解决此问题的方法提供了一个单独的函数，或者在消歧函数中扩展实体集合以包括这个特殊条目：
- en: '|  | $\mathsf{ED}:(M,C)^{n}\xrightarrow{}(E\cup\mathsf{NIL})^{n}.$ |  | (4)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{ED}:(M,C)^{n}\xrightarrow{}(E\cup\mathsf{NIL})^{n}.$ |  | (4)
    |'
- en: 2.3 Terminological Aspects
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 术语方面
- en: More or less, the same technologies and models are sometimes called differently
    in the literature. Namely, Wikification Cheng and Roth ([2013](#bib.bib26)) and
    entity disambiguation are considered as subtypes of EL Moro et al. ([2014](#bib.bib115)).
    To be comprehensive in this survey, we assume that the entity linking task encompasses
    both entity mention detection and entity disambiguation. However, only a few studies
    suggest models that perform MD and ED jointly, while the majority of papers on
    EL focus exclusively on ED and assume that mention boundaries are given by an
    external entity recognizer Rizzo et al. ([2014](#bib.bib152)) (which may lead
    to some terminological confusions). Numerous techniques that perform MD (e.g.
    in the NER task) without entity disambiguation are considered in many previous
    surveys Nadeau and Sekine ([2007](#bib.bib119)); Sharnagat ([2014](#bib.bib159));
    Goyal et al. ([2018](#bib.bib57)); Yadav and Bethard ([2018](#bib.bib193)); Li
    et al. ([2022](#bib.bib95)) inter alia and are out of the scope of this work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这些技术和模型有时会被不同地称呼。即，Wikification Cheng 和 Roth ([2013](#bib.bib26)) 和实体消歧被认为是
    EL 的子类型 Moro et al. ([2014](#bib.bib115))。为了全面涵盖此调查，我们假设实体链接任务包括实体提及检测和实体消歧。然而，只有少数研究提出了同时执行
    MD 和 ED 的模型，而大多数关于 EL 的论文专注于 ED 并假设提及边界由外部实体识别器给出 Rizzo et al. ([2014](#bib.bib152))（这可能导致一些术语上的混淆）。许多只执行
    MD（例如在 NER 任务中）而不进行实体消歧的技术在许多先前的调查中被考虑 Nadeau 和 Sekine ([2007](#bib.bib119));
    Sharnagat ([2014](#bib.bib159)); Goyal et al. ([2018](#bib.bib57)); Yadav 和 Bethard
    ([2018](#bib.bib193)); Li et al. ([2022](#bib.bib95)) 等，并不在本文讨论范围之内。
- en: Entity linking in the general case is not restricted to linking mentions to
    graph nodes but rather to concepts in a knowledge base. However, most of the modern
    widely-used knowledge bases organize information in the form of a graph Lehmann
    et al. ([2015](#bib.bib92)); Bollacker et al. ([2008](#bib.bib14)); Vrandečić
    and Krötzsch ([2014](#bib.bib184)), even in particular domains, like e.g. the
    scholarly domain Dessì et al. ([2021](#bib.bib34)). A basic statement in a data/knowledge
    base usually can be represented as a subject-predicate-object tuple $(s,p,o)$,
    e.g. (John_Lennon, occupation, singer) or (New_York_City, founded, 1624), and
    a set of such tuples can be represented as a multi-relational graph. This formalism
    helps to efficiently organize knowledge for many applications ranging from search
    engines to question answering and recommendation systems Hogan et al. ([2021](#bib.bib68));
    Ji et al. ([2022](#bib.bib77)). Therefore, in this article, the terms Knowledge
    Graph (KG) and Knowledge Base (KB) are used interchangeably.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接在一般情况下并不限于将提及链接到图节点，而是链接到知识库中的概念。然而，大多数现代广泛使用的知识库以图的形式组织信息 Lehmann et al.
    ([2015](#bib.bib92)); Bollacker et al. ([2008](#bib.bib14)); Vrandečić 和 Krötzsch
    ([2014](#bib.bib184))，即使在特定领域，例如学术领域 Dessì et al. ([2021](#bib.bib34))。数据/知识库中的基本陈述通常可以表示为一个主题-谓词-对象元组
    $(s,p,o)$，例如 (John_Lennon, occupation, singer) 或 (New_York_City, founded, 1624)，一组这样的元组可以表示为一个多关系图。这种形式有助于高效组织知识，适用于从搜索引擎到问答系统和推荐系统的许多应用
    Hogan et al. ([2021](#bib.bib68)); Ji et al. ([2022](#bib.bib77))。因此，在本文中，知识图谱
    (KG) 和知识库 (KB) 这两个术语是可以互换使用的。
- en: <svg   height="197.67" overflow="visible" version="1.1" width="662.28"><g transform="translate(0,197.67)
    matrix(1 0 0 -1 0 0) translate(316.2,0) translate(0,187.94)"><g stroke="#000000"><g
    fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -78.74
    -4.84)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="157.48" height="9.69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1 - General
    Architecture <g stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -176.63 -49.6 L -307.62
    -49.6 C -312.21 -49.6 -315.92 -53.32 -315.92 -57.9 L -315.92 -60.21 C -315.92
    -64.79 -312.21 -68.51 -307.62 -68.51 L -176.63 -68.51 C -172.05 -68.51 -168.33
    -64.79 -168.33 -60.21 L -168.33 -57.9 C -168.33 -53.32 -172.05 -49.6 -176.63 -49.6
    Z M -315.92 -68.51" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -176.63 -49.6 L -307.62 -49.6 C -312.21 -49.6 -315.92 -53.32 -315.92 -57.9
    L -315.92 -60.21 C -315.92 -64.79 -312.21 -68.51 -307.62 -68.51 L -176.63 -68.51
    C -172.05 -68.51 -168.33 -64.79 -168.33 -60.21 L -168.33 -57.9 C -168.33 -53.32
    -172.05 -49.6 -176.63 -49.6 Z M -315.92 -68.51"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -311.31 -63.9)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.1
    - Candidate Generation</foreignobject></g></g> <g stroke="#999999" fill="#999999"
    stroke-width="0.8pt" color="#999999"><path d="M -39.89 -9.73 L -195.95 -47.79"
    style="fill:none"><g transform="matrix(-0.97153 -0.23692 0.23692 -0.97153 -195.95
    -47.79)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44
    -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt"><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M -15.21 -49.52 L -146.2 -49.52 C -150.79 -49.52
    -154.51 -53.24 -154.51 -57.83 L -154.51 -60.28 C -154.51 -64.87 -150.79 -68.59
    -146.2 -68.59 L -15.21 -68.59 C -10.63 -68.59 -6.91 -64.87 -6.91 -60.28 L -6.91
    -57.83 C -6.91 -53.24 -10.63 -49.52 -15.21 -49.52 Z M -154.51 -68.59" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#FFFFFF"><path d="M -15.21 -49.52 L -146.2 -49.52 C
    -150.79 -49.52 -154.51 -53.24 -154.51 -57.83 L -154.51 -60.28 C -154.51 -64.87
    -150.79 -68.59 -146.2 -68.59 L -15.21 -68.59 C -10.63 -68.59 -6.91 -64.87 -6.91
    -60.28 L -6.91 -57.83 C -6.91 -53.24 -10.63 -49.52 -15.21 -49.52 Z M -154.51 -68.59"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -149.89 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject
    width="138.37" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#EA6B66">3.1.2 - Context-Mention Encoding</foreignobject></g></g> <g stroke="#999999"
    fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M -13.3 -9.73 L -62.08
    -45.42" style="fill:none"><g transform="matrix(-0.80707 -0.59045 0.59045 -0.80707
    -62.08 -45.42)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7
    C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt"><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 146.2 -49.52 L 15.21 -49.52 C 10.63 -49.52 6.91
    -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63 -68.59 15.21 -68.59 L 146.2
    -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28 L 154.51 -57.83 C 154.51 -53.24
    150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#FFFFFF"><path d="M 146.2 -49.52 L 15.21 -49.52 C 10.63
    -49.52 6.91 -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63 -68.59 15.21
    -68.59 L 146.2 -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28 L 154.51 -57.83
    C 154.51 -53.24 150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 11.52 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.3
    - Entity Encoding</foreignobject></g></g> <g stroke="#999999" fill="#999999" stroke-width="0.8pt"
    color="#999999"><path d="M 13.3 -9.73 L 62.08 -45.42" style="fill:none"><g transform="matrix(0.80707
    -0.59045 0.59045 0.80707 62.08 -45.42)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44
    -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 307.62 -49.6 L 176.63
    -49.6 C 172.05 -49.6 168.33 -53.32 168.33 -57.9 L 168.33 -60.21 C 168.33 -64.79
    172.05 -68.51 176.63 -68.51 L 307.62 -68.51 C 312.21 -68.51 315.92 -64.79 315.92
    -60.21 L 315.92 -57.9 C 315.92 -53.32 312.21 -49.6 307.62 -49.6 Z M 168.33 -68.51"
    style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path d="M
    307.62 -49.6 L 176.63 -49.6 C 172.05 -49.6 168.33 -53.32 168.33 -57.9 L 168.33
    -60.21 C 168.33 -64.79 172.05 -68.51 176.63 -68.51 L 307.62 -68.51 C 312.21 -68.51
    315.92 -64.79 315.92 -60.21 L 315.92 -57.9 C 315.92 -53.32 312.21 -49.6 307.62
    -49.6 Z M 168.33 -68.51"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 172.94
    -63.9)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37" height="9.69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.5 -
    Unlinkable Mention Prediction</foreignobject></g></g> <g stroke="#999999" fill="#999999"
    stroke-width="0.8pt" color="#999999"><path d="M 39.89 -9.73 L 195.95 -47.79" style="fill:none"><g
    transform="matrix(0.97153 -0.23692 0.23692 0.97153 195.95 -47.79)"><path d="M
    6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48
    0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -293.78 -119.64 h 117.15 v 42.43 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -289.17 -91.51)"><foreignobject width="107.93" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">surface form matching Le and Titov ([2019b](#bib.bib87));
    Moreno et al. ([2017](#bib.bib114))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -293.78 -152.09 h 117.15 v 42.43 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -289.17 -123.96)"><foreignobject width="107.93" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">expansion using aliases Hoffart et al. ([2011](#bib.bib67));
    Pershina et al. ([2015](#bib.bib137))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -293.78 -176.24 h 117.15 v 25.83 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -289.17 -164.71)"><foreignobject width="107.93" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">prior probability Spitkovsky and Chang ([2012](#bib.bib169))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M -135.13 -119.64 h 117.15 v 42.43
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -130.52 -91.51)"><foreignobject
    width="107.93" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">recurrent
    architecture Gupta et al. ([2017](#bib.bib62)); Kolitsas et al. ([2018](#bib.bib82));
    Sil et al. ([2018](#bib.bib164))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -135.16 -143.79 h 186.39 v 25.83 h -186.39 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -130.55 -132.26)"><foreignobject width="177.17" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">self attention Logeswaran et al. ([2019](#bib.bib100));
    Wu et al. ([2020b](#bib.bib191)); Yamada et al. ([2021](#bib.bib198))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 26.26 -121.03 h 186.39 v 42.43
    h -186.39 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 30.87 -92.89)"><foreignobject
    width="177.17" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">unstructured
    text based
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="197.67" overflow="visible" version="1.1" width="662.28"><g transform="translate(0,197.67)
    matrix(1 0 0 -1 0 0) translate(316.2,0) translate(0,187.94)"><g stroke="#000000"><g
    fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -78.74
    -4.84)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="157.48" height="9.69"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1 - 一般架构
    <g stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -176.63 -49.6 L -307.62
    -49.6 C -312.21 -49.6 -315.92 -53.32 -315.92 -57.9 L -315.92 -60.21 C -315.92
    -64.79 -312.21 -68.51 -307.62 -68.51 L -176.63 -68.51 C -172.05 -68.51 -168.33
    -64.79 -168.33 -60.21 L -168.33 -57.9 C -168.33 -53.32 -172.05 -49.6 -176.63 -49.6
    Z M -315.92 -68.51" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -176.63 -49.6 L -307.62 -49.6 C -312.21 -49.6 -315.92 -53.32 -315.92 -57.9
    L -315.92 -60.21 C -315.92 -64.79 -312.21 -68.51 -307.62 -68.51 L -176.63 -68.51
    C -172.05 -68.51 -168.33 -64.79 -168.33 -60.21 L -168.33 -57.9 C -168.33 -53.32
    -172.05 -49.6 -176.63 -49.6 Z M -315.92 -68.51"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -311.31 -63.9)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.1
    - 候选生成</foreignobject></g></g> <g stroke="#999999" fill="#999999" stroke-width="0.8pt"
    color="#999999"><path d="M -39.89 -9.73 L -195.95 -47.79" style="fill:none"><g
    transform="matrix(-0.97153 -0.23692 0.23692 -0.97153 -195.95 -47.79)"><path d="M
    6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48
    0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt"><g stroke-opacity="0.5"
    fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path
    d="M -15.21 -49.52 L -146.2 -49.52 C -150.79 -49.52 -154.51 -53.24 -154.51 -57.83
    L -154.51 -60.28 C -154.51 -64.87 -150.79 -68.59 -146.2 -68.59 L -15.21 -68.59
    C -10.63 -68.59 -6.91 -64.87 -6.91 -60.28 L -6.91 -57.83 C -6.91 -53.24 -10.63
    -49.52 -15.21 -49.52 Z M -154.51 -68.59" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -15.21 -49.52 L -146.2 -49.52 C -150.79 -49.52 -154.51
    -53.24 -154.51 -57.83 L -154.51 -60.28 C -154.51 -64.87 -150.79 -68.59 -146.2
    -68.59 L -15.21 -68.59 C -10.63 -68.59 -6.91 -64.87 -6.91 -60.28 L -6.91 -57.83
    C -6.91 -53.24 -10.63 -49.52 -15.21 -49.52 Z M -154.51 -68.59"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -149.89 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.2
    - 上下文提及编码</foreignobject></g></g> <g stroke="#999999" fill="#999999" stroke-width="0.8pt"
    color="#999999"><path d="M -13.3 -9.73 L -62.08 -45.42" style="fill:none"><g transform="matrix(-0.80707
    -0.59045 0.59045 -0.80707 -62.08 -45.42)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44
    -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 146.2 -49.52 L 15.21
    -49.52 C 10.63 -49.52 6.91 -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63
    -68.59 15.21 -68.59 L 146.2 -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28
    L 154.51 -57.83 C 154.51 -53.24 150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#FFFFFF"><path d="M 146.2 -49.52 L 15.21 -49.52 C 10.63
    -49.52 6.91 -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63 -68.59 15.21
    -68.59 L 146.2 -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28 L 154.51 -57.83
    C 154.51 -53.24 150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 11.52 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.1.3
    - 实体编码</foreignobject></g></g> <g stroke="#999999" fill="#
- en: Ganea and Hofmann ([2017](#bib.bib53)); Newman-Griffis et al. ([2018](#bib.bib125))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 26.26 -150.71 h 186.39 v 42.43
    h -186.39 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 30.87 -122.57)"><foreignobject
    width="177.17" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">relational
    information based
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Ganea 和 Hofmann ([2017](#bib.bib53)); Newman-Griffis 等人 ([2018](#bib.bib125))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 26.26 -150.71 h 186.39 v 42.43
    h -186.39 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 30.87 -122.57)"><foreignobject
    width="177.17" height="33.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">关系信息基础
- en: Perozzi et al. ([2014](#bib.bib136)); Bordes et al. ([2013](#bib.bib15)); Yamada
    et al. ([2016](#bib.bib194))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 26.26 -184.54 h 186.39 v 42.43 h -186.39 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 30.87 -156.41)"><foreignobject width="177.17" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">other information based (e.g.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Perozzi 等人 ([2014](#bib.bib136)); Bordes 等人 ([2013](#bib.bib15)); Yamada 等人
    ([2016](#bib.bib194))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 26.26 -184.54 h 186.39 v 42.43 h -186.39 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 30.87 -156.41)"><foreignobject width="177.17" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">其他基于信息（例如。
- en: description pages) Gupta et al. ([2017](#bib.bib62)); Francis-Landau et al.
    ([2016](#bib.bib49)); Gillick et al. ([2019](#bib.bib55))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 199.06 -111.34 h 147.02 v 25.83
    h -147.02 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.67 -99.81)"><foreignobject
    width="137.8" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">no
    candidate Sil et al. ([2018](#bib.bib164)); Tsai and Roth ([2016](#bib.bib176))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 198.77 -136.87 h 117.15 v 25.83
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.38 -125.34)"><foreignobject
    width="107.93" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">threshold
    Lazic et al. ([2015](#bib.bib84)); Peters et al. ([2019](#bib.bib139))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 198.77 -162.41 h 117.15 v 25.83
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.38 -150.88)"><foreignobject
    width="107.93" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">NIL
    predictor Kolitsas et al. ([2018](#bib.bib82))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M 199.06 -187.94 h 147.02 v 25.83 h -147.02 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 203.67 -176.41)"><foreignobject width="137.8" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">separate model Moreno et al. ([2017](#bib.bib114));
    Martins et al. ([2019](#bib.bib107))</foreignobject></g><g stroke="#999999" fill="#999999"
    stroke-width="0.8pt" color="#999999"><path d="M -278.45 -68.79 L -278.45 -98.43
    L -287.58 -98.43" style="fill:none"><g transform="matrix(-1.0 0.0 0.0 -1.0 -287.58
    -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44
    -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g stroke="#999999"
    fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M -278.45 -68.79
    L -278.45 -130.88 L -287.58 -130.88" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -130.88)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -278.45 -68.79 L -278.45 -163.33 L -287.58 -163.33" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -163.33)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -117.32 -68.86 L -117.32 -98.43 L -128.93 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -128.93 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -117.32 -68.86 L -117.32 -130.88 L -128.96 -130.88" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -128.96 -130.88)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -99.81 L 32.46 -99.81" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -99.81)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -129.49 L 32.46 -129.49" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -129.49)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -163.33 L 32.46 -163.33" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -163.33)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    210.29 -68.79 L 210.29 -98.43 L 205.26 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 205.26 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    210.29 -68.79 L 210.29 -123.96 L 204.97 -123.96" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 204.97 -123.96)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    210.29 -68.79 L 210.29 -149.49 L 204.97 -149.49" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 204.97 -149.49)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    210.29 -68.79 L 210.29 -175.02 L 205.26 -175.02" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 205.26 -175.02)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 描述页面）Gupta 等人 ([2017](#bib.bib62))；Francis-Landau 等人 ([2016](#bib.bib49))；Gillick
    等人 ([2019](#bib.bib55))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 199.06 -111.34 h 147.02 v 25.83 h -147.02 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 203.67 -99.81)"><foreignobject width="137.8" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">无候选者 Sil 等人 ([2018](#bib.bib164))；Tsai 和 Roth
    ([2016](#bib.bib176))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 198.77 -136.87 h 117.15 v 25.83 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 203.38 -125.34)"><foreignobject width="107.93" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">阈值 Lazic 等人 ([2015](#bib.bib84))；Peters 等人
    ([2019](#bib.bib139))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 198.77 -162.41 h 117.15 v 25.83 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 203.38 -150.88)"><foreignobject width="107.93" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">NIL 预测器 Kolitsas 等人 ([2018](#bib.bib82))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 199.06 -187.94 h 147.02 v 25.83
    h -147.02 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 203.67 -176.41)"><foreignobject
    width="137.8" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">分离模型
    Moreno 等人 ([2017](#bib.bib114))；Martins 等人 ([2019](#bib.bib107))</foreignobject></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -278.45 -68.79 L -278.45 -98.43 L -287.58 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -278.45 -68.79 L -278.45 -130.88 L -287.58 -130.88" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -130.88)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -278.45 -68.79 L -278.45 -163.33 L -287.58 -163.33" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -163.33)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -117.32 -68.86 L -117.32 -98.43 L -128.93 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -128.93 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -117.32 -68.86 L -117.32 -130.88 L -128.96 -130.88" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -128.96 -130.88)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -99.81 L 32.46 -99.81" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -99.81)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -129.49 L 32.46 -129.49" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -129.49)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -163.33 L 32.46 -163.33" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.46 -163.33)"><path d="M 6.48 0 C 4.56 0.
- en: 'Figure 3: Reference map of the general architecture of neural EL systems. The
    categorization of each step in the general neural EL architecture with alternative
    design choices and example references illustrating each of the choices.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 神经实体链接系统通用架构的参考图。通用神经实体链接架构中每一步的分类，包含替代设计选择和示例参考，说明每个选择。'
- en: 3 Neural Entity Linking
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 神经实体链接
- en: We start the discussion of neural entity linking approaches from the most general
    architecture of EL pipelines and continue with various specific modifications
    like joint entity mention detection and linking, disambiguation techniques that
    leverage global context, domain-independent EL approaches including zero-shot
    methods, and cross-lingual models.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最通用的实体链接流水线架构开始讨论神经实体链接方法，并继续探讨各种特定的修改，如联合实体提及检测和链接、利用全局上下文的消歧技术、包括零样本方法在内的领域独立实体链接方法，以及跨语言模型。
- en: 3.1 General Architecture
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 通用架构
- en: 'Some of the attempts to EL based on neural networks treat it as a multi-class
    classification task in which entities correspond to classes. However, the straightforward
    approach results in a large number of classes, which leads to suboptimal performance
    without task-sharing Kar et al. ([2018](#bib.bib80)). The streamlined approach
    to EL is to treat it as a ranking problem. We present the generalized EL architecture
    in Figure [2](#S1.F2 "Figure 2 ‣ 1.4 Contributions ‣ 1 Introduction ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning"), which is applicable to the
    majority of neural approaches. Here, the mention detection model identifies the
    mention boundaries in text. The next step is to produce a shortlist of possible
    entities (candidates) for the mention, e.g. producing Scott_Young_(writer) as
    a candidate rather than a completely random entity. Then, the mention encoder
    produces a semantic vector representation of a mention in a context. The entity
    encoder produces a set of vector representations of candidates. Finally, the entity
    ranking model compares mention and entity representations and estimates mention-entity
    correspondence scores. An optional step is to determine unlinkable mentions, for
    which a KG does not contain a corresponding entity. The categorization of each
    step in the general neural EL architecture is summarized in Figure [3](#S2.F3
    "Figure 3 ‣ 2.3 Terminological Aspects ‣ 2 Task Description ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基于神经网络的实体链接尝试将其视为一个多类分类任务，其中实体对应于类别。然而，这种直接的方法会导致类别数量庞大，从而在没有任务共享的情况下表现不佳
    Kar 等人 ([2018](#bib.bib80))。精简的实体链接方法是将其视为一个排名问题。我们在图 [2](#S1.F2 "图 2 ‣ 1.4 贡献
    ‣ 1 引言 ‣ 神经实体链接：基于深度学习的模型综述") 中展示了通用的实体链接架构，这适用于大多数神经方法。在这里，提及检测模型识别文本中的提及边界。接下来的步骤是为提及生成一个可能的实体（候选实体）名单，例如，生成
    Scott_Young_(writer) 作为候选实体，而不是完全随机的实体。然后，提及编码器生成提及在上下文中的语义向量表示。实体编码器生成候选实体的一组向量表示。最后，实体排名模型比较提及和实体的表示，并估计提及-实体对应分数。一个可选的步骤是确定不可链接的提及，即知识图谱中没有对应实体的提及。通用神经实体链接架构中每一步的分类在图
    [3](#S2.F3 "图 3 ‣ 2.3 术语方面 ‣ 2 任务描述 ‣ 神经实体链接：基于深度学习的模型综述") 中进行了总结。
- en: 'Table 1: Candidate generation examples. Candidate entities for the example
    mention “Big Blue” obtained using several candidate generation methods. The highlighted
    candidates are “correct” entities assuming that the given mention refers to the
    IBM corporation and not a river, e.g. Big_Blue_River_(Kansas).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 候选生成示例。使用几种候选生成方法获得的示例提及 “Big Blue” 的候选实体。高亮的候选实体是“正确的”实体，假设给定的提及指的是 IBM
    公司而不是河流，例如 Big_Blue_River_(Kansas)。'
- en: '| Method | 5 candidate entities for the example mention “Big Blue” |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 示例提及 “Big Blue” 的 5 个候选实体 |'
- en: '| --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; surface form matching based &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于表面形式匹配的 &#124;'
- en: '&#124; on DBpedia names⁶⁶6Random matches from DBpedia labels dataset – [http://downloads.dbpedia.org/2016-10/core-i18n/en/labelsen.ttl.bz2](http://downloads.dbpedia.org/2016-10/core-i18n/en/labelsen.ttl.bz2)
    &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自 DBpedia 名称⁶⁶6 从 DBpedia 标签数据集中随机匹配 – [http://downloads.dbpedia.org/2016-10/core-i18n/en/labelsen.ttl.bz2](http://downloads.dbpedia.org/2016-10/core-i18n/en/labelsen.ttl.bz2)
    &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Big_Blue_Trail, Big_Bluegrass, Big_Blue_Spring_cave_crayfish, &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Big_Blue_Trail, Big_Bluegrass, Big_Blue_Spring_cave_crayfish, &#124;'
- en: '&#124; Dexter_Bexley_and_the_Big_Blue_Beastie, IBM_Big_Blue_(X-League) &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dexter_Bexley_and_the_Big_Blue_Beastie, IBM_Big_Blue_(X-League) &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; expansion using aliases &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用别名的扩展 &#124;'
- en: '&#124; from YAGO-means⁷⁷7YAGO-means dataset of Hoffart et al. ([2011](#bib.bib67))
    – [http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aidameans.tsv.bz2](http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aidameans.tsv.bz2)
    &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自 YAGO-means⁷⁷7YAGO-means 数据集（Hoffart 等，([2011](#bib.bib67)）– [http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aidameans.tsv.bz2](http://resources.mpi-inf.mpg.de/yago-naga/aida/download/aidameans.tsv.bz2)
    &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Big_Blue_River_(Indiana), Big_Blue_River_(Kansas), &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Big_Blue_River_(Indiana), Big_Blue_River_(Kansas), &#124;'
- en: '&#124; Big_Blue_(crane), BigRed(drink), IBM &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Big_Blue_(crane), BigRed(drink), IBM &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; probability + expansion using aliases &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 概率 + 使用别名扩展 &#124;'
- en: '&#124; from Ganea and Hofmann ([2017](#bib.bib53)): Anchor prob. + CrossWikis
    + YAGO &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自 Ganea 和 Hofmann ([2017](#bib.bib53))：Anchor prob. + CrossWikis +
    YAGO &#124;'
- en: ⁸⁸8We generated these examples using the source code of Peters et al. ([2019](#bib.bib139))
    – [https://github.com/allenai/kb](https://github.com/allenai/kb) |
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ⁸⁸8 我们使用 Peters 等 ([2019](#bib.bib139)) 的源代码生成了这些示例 – [https://github.com/allenai/kb](https://github.com/allenai/kb)
    |
- en: '&#124; IBM, Big_Blue_River_(Kansas), The_Big_Blue &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IBM, Big_Blue_River_(Kansas), The_Big_Blue &#124;'
- en: '&#124; Big_Blue_River_(Indiana), Big_Blue_(crane) &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Big_Blue_River_(Indiana), Big_Blue_(crane) &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.1.1 Candidate Generation
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 候选生成
- en: An essential part of EL is candidate generation. The goal of this step is given
    an ambiguous entity mention, such as “Scott Young”, to provide a list of its possible
    “senses” as specified by entities in a KG. EL is analogous to the Word Sense Disambiguation
    (WSD) task Moro et al. ([2014](#bib.bib115)); Navigli ([2009](#bib.bib121)) as
    it also resolves lexical ambiguity. Yet in WSD, each sense of a word can be clearly
    defined by WordNet Fellbaum ([1998](#bib.bib46)), while in EL, KGs do not provide
    such an exact mapping between mentions and entities Moro et al. ([2014](#bib.bib115));
    Navigli ([2009](#bib.bib121)); Chang et al. ([2016](#bib.bib22)). Therefore, a
    mention potentially can be linked to any entity in a KG, resulting in a large
    search space, e.g. “Big Blue” referring to IBM. In the candidate generation step,
    this issue is addressed by performing effective preliminary filtering of the entity
    list.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接（EL）的一个关键部分是候选生成。这个步骤的目标是，在给定一个模糊的实体提及，例如“Scott Young”，提供一个可能的“含义”列表，这些含义由知识图谱（KG）中的实体指定。EL
    类似于词义消歧（WSD）任务（Moro 等，([2014](#bib.bib115)）；Navigli ([2009](#bib.bib121)），因为它也解决了词汇歧义的问题。然而，在
    WSD 中，WordNet（Fellbaum ([1998](#bib.bib46)）可以清晰定义每个词义，而在 EL 中，KG 不提供提及与实体之间的精确映射（Moro
    等，([2014](#bib.bib115)）；Navigli ([2009](#bib.bib121)）；Chang 等，([2016](#bib.bib22)））。因此，一个提及可能与
    KG 中的任何实体相关联，导致搜索空间很大，例如“Big Blue”指的是 IBM。在候选生成步骤中，通过对实体列表进行有效的初步过滤来解决这个问题。
- en: Formally, given a mention $m_{\_}i$, a candidate generator provides a list of
    probable entities, $e_{\_}1,e_{\_}2,...,e_{\_}k$, for each entity mention in a
    document.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，给定一个提及 $m_{\_}i$，候选生成器为文档中的每个实体提及提供一个可能实体的列表 $e_{\_}1,e_{\_}2,...,e_{\_}k$。
- en: '|  | $\mathsf{CG}:M\xrightarrow{}(e_{\_}1,e_{\_}2,...,e_{\_}k).$ |  | (5) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{CG}:M\xrightarrow{}(e_{\_}1,e_{\_}2,...,e_{\_}k).$ |  | (5) |'
- en: 'Similar to Shen et al. ([2015](#bib.bib160)); Al-Moslmi et al. ([2020](#bib.bib3)),
    we distinguish three common candidate generation methods in neural EL: (1) based
    on surface form matching, (2) based on expansion with aliases, and (3) based on
    a prior matching probability computation. In the first approach, a candidate list
    is composed of entities that match various surface forms of mentions in the text
    Zwicklbauer et al. ([2016b](#bib.bib211)); Moreno et al. ([2017](#bib.bib114));
    Le and Titov ([2019b](#bib.bib87)). There are many heuristics for the generation
    of mention forms and matching criteria like the Levenshtein distance, n-grams,
    and normalization. For the example mention of “Big Blue”, this approach would
    not work well, as the referent entity “IBM” or its long-form “International Business
    Machines” does not contain a mention string. Examples of candidate entity sets
    are presented in Table [1](#S3.T1 "Table 1 ‣ 3.1 General Architecture ‣ 3 Neural
    Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning"),
    where we searched a name matching of the mention “Big Blue” in the titles of all
    Wikipedia articles present in DBpedia and presented random 5 matches.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于 Shen 等人 ([2015](#bib.bib160)) 和 Al-Moslmi 等人 ([2020](#bib.bib3))，我们在神经实体链接中区分了三种常见的候选生成方法：（1）基于表面形式匹配，（2）基于别名扩展，以及（3）基于先验匹配概率计算。在第一种方法中，候选列表由与文本中的提及词的各种表面形式匹配的实体组成
    Zwicklbauer 等人 ([2016b](#bib.bib211))；Moreno 等人 ([2017](#bib.bib114))；Le 和 Titov
    ([2019b](#bib.bib87))。对于提及形式和匹配标准的生成，有许多启发式方法，如 Levenshtein 距离、n-grams 和归一化。以“Big
    Blue”作为例子，这种方法效果不佳，因为指代实体“IBM”或其全称“International Business Machines”不包含提及字符串。候选实体集合的示例见表
    [1](#S3.T1 "Table 1 ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning")，在该表中我们搜索了 DBpedia
    中所有维基百科文章标题中“Big Blue”的名称匹配，并展示了随机的 5 个匹配。'
- en: 'In the second approach, a dictionary of additional aliases is constructed using
    KG metadata like disambiguation/redirect pages of Wikipedia Fang et al. ([2019](#bib.bib43));
    Zwicklbauer et al. ([2016b](#bib.bib211)) or using a dictionary of aliases and/or
    synonyms (e.g. “NYC” stands for “New York City”). This helps to improve the candidate
    generation recall as the surface form matching usually cannot catch such cases.
    Pershina et al. ([2015](#bib.bib137)) expand the given mention to the longest
    mention in a context found using coreference resolution. Then, an entity is selected
    as a candidate if its title matches the longest version of the mention, or it
    is present in disambiguation/redirect pages of this mention. This resource is
    used in many EL models, e.g. Yamada et al. ([2016](#bib.bib194)); Cao et al. ([2017](#bib.bib19));
    Newman-Griffis et al. ([2018](#bib.bib125)); Radhakrishnan et al. ([2018](#bib.bib144));
    Martins et al. ([2019](#bib.bib107)); Onoe and Durrett ([2020](#bib.bib131));
    Sil et al. ([2018](#bib.bib164)). Another well-known alternative is YAGO Suchanek
    et al. ([2007](#bib.bib170)) – an ontology automatically constructed from Wikipedia
    and WordNet. Among many other relations, it provides “means” relations, and this
    mapping is utilized for candidate generation like in Hoffart et al. ([2011](#bib.bib67));
    Yamada et al. ([2016](#bib.bib194)); Ganea and Hofmann ([2017](#bib.bib53)); Sil
    et al. ([2018](#bib.bib164)); Shahbazi et al. ([2018](#bib.bib157)). In this technique,
    the external information would help to disambiguate “Big Blue” as “IBM”. Table
    [1](#S3.T1 "Table 1 ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") shows examples of
    candidates generated with the help of the YAGO-means candidate mapping dataset
    used in Hoffart et al. ([2011](#bib.bib67)).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '在第二种方法中，使用知识图谱（KG）元数据（如维基百科的消歧义/重定向页面）Fang等 ([2019](#bib.bib43))；Zwicklbauer等
    ([2016b](#bib.bib211))，或使用别名和/或同义词的字典（例如，“NYC”代表“New York City”）来构建额外的别名字典。这有助于提高候选生成的召回率，因为表面形式匹配通常无法捕捉到这种情况。Pershina等
    ([2015](#bib.bib137)) 将给定的提及扩展为上下文中找到的最长提及，使用共指解析。然后，如果实体的标题与提及的最长版本匹配，或者该实体出现在此提及的消歧义/重定向页面中，则选择该实体作为候选实体。该资源在许多实体链接（EL）模型中被使用，例如Yamada等
    ([2016](#bib.bib194))；Cao等 ([2017](#bib.bib19))；Newman-Griffis等 ([2018](#bib.bib125))；Radhakrishnan等
    ([2018](#bib.bib144))；Martins等 ([2019](#bib.bib107))；Onoe和Durrett ([2020](#bib.bib131))；Sil等
    ([2018](#bib.bib164))。另一个著名的替代方案是YAGO Suchanek等 ([2007](#bib.bib170))——一个从维基百科和WordNet自动构建的本体。它提供了许多其他关系，其中包括“means”关系，这一映射被用于候选生成，如Hoffart等
    ([2011](#bib.bib67))；Yamada等 ([2016](#bib.bib194))；Ganea和Hofmann ([2017](#bib.bib53))；Sil等
    ([2018](#bib.bib164))；Shahbazi等 ([2018](#bib.bib157))。在这项技术中，外部信息将有助于将“Big Blue”消歧为“IBM”。表[1](#S3.T1
    "Table 1 ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning")展示了使用Hoffart等 ([2011](#bib.bib67))中使用的YAGO-means候选映射数据集生成的候选示例。'
- en: The third approach to candidate generation is based on pre-calculated prior
    probabilities of correspondence between certain mentions and entities, $p(e|m)$.
    Many studies rely on mention-entity priors computed based on Wikipedia entity
    hyperlinks. A URL of a hyperlink to an entity page of Wikipedia determines a candidate
    entity, and the anchor text of the hyperlink determines a mention. Another widely-used
    option is CrossWikis Spitkovsky and Chang ([2012](#bib.bib169)), which is an extensive
    resource that leverages the frequency of mention-entity links in web crawl data
    Ganea and Hofmann ([2017](#bib.bib53)); Gupta et al. ([2017](#bib.bib62)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种候选生成方法基于对特定提及与实体之间对应关系的预计算先验概率$p(e|m)$。许多研究依赖于基于维基百科实体超链接计算的提及-实体先验。维基百科实体页面的超链接网址决定了一个候选实体，而超链接的锚文本决定了一个提及。另一种广泛使用的选项是CrossWikis
    Spitkovsky和Chang ([2012](#bib.bib169))，这是一个利用网页抓取数据中提及-实体链接频率的广泛资源，Ganea和Hofmann
    ([2017](#bib.bib53))；Gupta等 ([2017](#bib.bib62))。
- en: 'It is common to apply multiple approaches to candidate generation at once.
    For example, the resource constructed by Ganea and Hofmann ([2017](#bib.bib53))
    and used in many other EL methods Kolitsas et al. ([2018](#bib.bib82)); Peters
    et al. ([2019](#bib.bib139)); Yamada et al. ([2021](#bib.bib198)); Shahbazi et al.
    ([2019](#bib.bib158)); Le and Titov ([2019a](#bib.bib86)) relies on prior probabilities
    obtained from entity hyperlink count statistics of CrossWikis Spitkovsky and Chang
    ([2012](#bib.bib169)) and Wikipedia, as well as on entity aliases obtained from
    the “means” relationship of the YAGO ontology Hoffart et al. ([2011](#bib.bib67)).
    The illustrative mention “Big Blue” can be linked to its referent entity “IBM”
    with this method, as shown in Table [1](#S3.T1 "Table 1 ‣ 3.1 General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning"). As another example, Fang et al. ([2020](#bib.bib44)) utilize
    surface form matching and aliases. They share candidates between abbreviations
    and their expanded versions in the local context. The aliases are obtained from
    Wikipedia redirect and disambiguation pages, the Wikipedia search engine, and
    synonyms from WordNet Fellbaum ([1998](#bib.bib46)). Additionally, they submit
    mentions that are misspelled or contain multiple words to Wikipedia and Google
    search engines and search for the corresponding Wikipedia articles. It is also
    worth noting that some works also employ a candidate pruning step to reduce the
    number of candidates.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '通常会同时应用多种方法进行候选生成。例如，由 Ganea 和 Hofmann ([2017](#bib.bib53)) 构建的资源被许多其他 EL 方法使用，如
    Kolitsas 等 ([2018](#bib.bib82))；Peters 等 ([2019](#bib.bib139))；Yamada 等 ([2021](#bib.bib198))；Shahbazi
    等 ([2019](#bib.bib158))；Le 和 Titov ([2019a](#bib.bib86)) 依赖于从 CrossWikis Spitkovsky
    和 Chang ([2012](#bib.bib169)) 和维基百科获得的实体超链接计数统计数据以及 YAGO 本体 Hoffart 等 ([2011](#bib.bib67))
    的“means”关系获得的实体别名。这种方法可以将示例提及 “Big Blue” 链接到其指代实体 “IBM”，如表 [1](#S3.T1 "Table 1
    ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning") 所示。另一个例子是 Fang 等 ([2020](#bib.bib44))
    利用表面形式匹配和别名。他们在本地上下文中共享缩写及其扩展版本之间的候选项。别名来自维基百科重定向和消歧义页面、维基百科搜索引擎以及 WordNet Fellbaum
    ([1998](#bib.bib46)) 的同义词。此外，他们将拼写错误或包含多个单词的提及提交到维基百科和谷歌搜索引擎，搜索相应的维基百科文章。值得注意的是，一些研究还采用候选剪枝步骤以减少候选数量。'
- en: 'Recent zero-shot models Logeswaran et al. ([2019](#bib.bib100)); Gillick et al.
    ([2019](#bib.bib55)); Wu et al. ([2020b](#bib.bib191)) perform candidate generation
    without external resources. Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Domain-Independent
    Architectures ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning") describes
    them in detail.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的零样本模型 Logeswaran 等 ([2019](#bib.bib100))；Gillick 等 ([2019](#bib.bib55))；Wu
    等 ([2020b](#bib.bib191)) 在没有外部资源的情况下进行候选生成。第 [3.2.3](#S3.SS2.SSS3 "3.2.3 Domain-Independent
    Architectures ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning") 节对此进行了详细描述。'
- en: 3.1.2 Context-mention Encoding
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 上下文-提及编码
- en: To correctly disambiguate an entity mention, it is crucial to thoroughly capture
    the information from its context. The current mainstream approach is to construct
    a dense contextualized vector representation of a mention $\boldsymbol{y}_{\_}m$
    using an encoder neural network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确消歧义一个实体提及，至关重要的是彻底捕捉其上下文中的信息。目前主流的方法是使用编码器神经网络构建提及 $\boldsymbol{y}_{\_}m$
    的密集上下文向量表示。
- en: '|  | $\mathsf{mENC}:(C,M)^{n}\xrightarrow{}(\boldsymbol{y}_{\_}{m_{\_}1},\boldsymbol{y}_{\_}{m_{\_}2},...,\boldsymbol{y}_{\_}{m_{\_}n}).$
    |  | (6) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{mENC}:(C,M)^{n}\xrightarrow{}(\boldsymbol{y}_{\_}{m_{\_}1},\boldsymbol{y}_{\_}{m_{\_}2},...,\boldsymbol{y}_{\_}{m_{\_}n}).$
    |  | (6) |'
- en: 'Several early techniques in neural EL utilize a convolutional encoder Sun et al.
    ([2015](#bib.bib171)); Francis-Landau et al. ([2016](#bib.bib49)); Nguyen et al.
    ([2016b](#bib.bib127)); Sorokin and Gurevych ([2018](#bib.bib168)), as well as
    attention between candidate entity embeddings and embeddings of words surrounding
    a mention Ganea and Hofmann ([2017](#bib.bib53)); Le and Titov ([2019a](#bib.bib86)).
    However, in recent models, two approaches prevail: recurrent networks and self-attention
    Vaswani et al. ([2017](#bib.bib182)).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的一些神经 EL 技术使用卷积编码器 Sun 等 ([2015](#bib.bib171))；Francis-Landau 等 ([2016](#bib.bib49))；Nguyen
    等 ([2016b](#bib.bib127))；Sorokin 和 Gurevych ([2018](#bib.bib168))，以及候选实体嵌入与提及周围单词的嵌入之间的注意力
    Ganea 和 Hofmann ([2017](#bib.bib53))；Le 和 Titov ([2019a](#bib.bib86))。然而，在近期模型中，两种方法占主导地位：递归网络和自注意力
    Vaswani 等 ([2017](#bib.bib182))。
- en: A recurrent architecture with LSTM cells Hochreiter and Schmidhuber ([1997](#bib.bib66))
    that has been a backbone model for many NLP applications, is adopted to EL in
    Kolitsas et al. ([2018](#bib.bib82)); Martins et al. ([2019](#bib.bib107)); Le
    and Titov ([2019b](#bib.bib87)); Fang et al. ([2019](#bib.bib43)); Nie et al.
    ([2018](#bib.bib129)); Gupta et al. ([2017](#bib.bib62)); Sil et al. ([2018](#bib.bib164))
    inter alia. Gupta et al. ([2017](#bib.bib62)) concatenate outputs of two LSTM
    networks that independently encode left and right contexts of a mention (including
    the mention itself). In the same vein, Sil et al. ([2018](#bib.bib164)) encode
    left and right local contexts via LSTMs but also pool the results across all mentions
    in a coreference chain and postprocess left and right representations with a tensor
    network. A modification of LSTM – GRU Chung et al. ([2014](#bib.bib29)) – is used
    by Eshel et al. ([2017](#bib.bib40)) in conjunction with an attention mechanism
    Bahdanau et al. ([2015](#bib.bib7)) to encode left and right context of a mention.
    Kolitsas et al. ([2018](#bib.bib82)) represent an entity mention as a combination
    of LSTM hidden states included in the mention span. Le and Titov ([2019b](#bib.bib87))
    simply run a bidirectional LSTM network on words complemented with embeddings
    of word positions relative to a target mention. Shahbazi et al. ([2019](#bib.bib158))
    adopt pre-trained ELMo Peters et al. ([2018](#bib.bib138)) for mention encoding
    by averaging mention word vectors.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 具有 LSTM 单元的递归结构由 Hochreiter 和 Schmidhuber ([1997](#bib.bib66)) 提出，该模型作为许多 NLP
    应用的基础模型，被 Kolitsas 等人 ([2018](#bib.bib82))；Martins 等人 ([2019](#bib.bib107))；Le
    和 Titov ([2019b](#bib.bib87))；Fang 等人 ([2019](#bib.bib43))；Nie 等人 ([2018](#bib.bib129))；Gupta
    等人 ([2017](#bib.bib62))；Sil 等人 ([2018](#bib.bib164)) 等人在实体链接中采用。Gupta 等人 ([2017](#bib.bib62))
    将两个独立编码提及的左右上下文（包括提及本身）的 LSTM 网络的输出进行拼接。同样，Sil 等人 ([2018](#bib.bib164)) 通过 LSTM
    编码左右局部上下文，但还跨所有提及在共指链中进行结果汇聚，并使用张量网络对左右表示进行后处理。LSTM 的一种变体 – GRU 由 Chung 等人 ([2014](#bib.bib29))
    提出 – 被 Eshel 等人 ([2017](#bib.bib40)) 与 Bahdanau 等人 ([2015](#bib.bib7)) 提出的注意力机制结合使用，以编码提及的左右上下文。Kolitsas
    等人 ([2018](#bib.bib82)) 将实体提及表示为包含在提及范围内的 LSTM 隐状态的组合。Le 和 Titov ([2019b](#bib.bib87))
    仅在单词上运行一个双向 LSTM 网络，并补充目标提及的单词位置嵌入。Shahbazi 等人 ([2019](#bib.bib158)) 通过平均提及单词向量采用了预训练的
    ELMo 由 Peters 等人 ([2018](#bib.bib138)) 提出。
- en: Encoding methods based on self-attention have recently become ubiquitous. The
    EL models presented in Wu et al. ([2020b](#bib.bib191)); Logeswaran et al. ([2019](#bib.bib100));
    Peters et al. ([2019](#bib.bib139)); Yamada et al. ([2021](#bib.bib198)); Chen
    et al. ([2020](#bib.bib25)) and others rely on the outputs from pre-trained BERT
    layers Devlin et al. ([2019](#bib.bib36)) for context and mention encoding. In
    Peters et al. ([2019](#bib.bib139)), a mention representation is modeled by pooling
    over word pieces in a mention span. The authors also put an additional self-attention
    block over all mention representations that encode interactions between several
    entities in a sentence. Another approach to modeling mentions is to insert special
    tags around them and perform a reduction of the whole encoded sequence. Wu et al.
    ([2020b](#bib.bib191)) reduce a sequence by keeping the representation of the
    special pooling symbol ‘[CLS]’ inserted at the beginning of a sequence. Logeswaran
    et al. ([2019](#bib.bib100)) mark positions of a mention span by summing embeddings
    of words within the span with a special vector and using the same reduction strategy
    as Wu et al. ([2020b](#bib.bib191)). Yamada et al. ([2021](#bib.bib198)) concatenate
    text with all mentions in it and jointly encode this sequence via a self-attention
    model based on pre-trained BERT. In addition to the simple attention-based encoder
    of Ganea and Hofmann ([2017](#bib.bib53)), Chen et al. ([2020](#bib.bib25)) leverage
    BERT for capturing type similarity between a mention and an entity candidate.
    They replace mention tokens with a special “[MASK]” token and extract the embedding
    generated for this token by BERT. A corresponding entity representation is generated
    by averaging multiple embeddings of mentions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自注意力的编码方法最近变得非常普遍。Wu 等人 ([2020b](#bib.bib191))；Logeswaran 等人 ([2019](#bib.bib100))；Peters
    等人 ([2019](#bib.bib139))；Yamada 等人 ([2021](#bib.bib198))；Chen 等人 ([2020](#bib.bib25))
    和其他人提出的 EL 模型依赖于从预训练 BERT 层 Devlin 等人 ([2019](#bib.bib36)) 获取的上下文和提及编码。在 Peters
    等人 ([2019](#bib.bib139)) 中，提及表示通过对提及跨度内的词片进行池化来建模。作者还在所有提及表示上添加了一个额外的自注意力块，以编码句子中多个实体之间的交互。建模提及的另一种方法是围绕提及插入特殊标签，并对整个编码序列进行简化。Wu
    等人 ([2020b](#bib.bib191)) 通过保留在序列开头插入的特殊池化符号‘[CLS]’的表示来简化序列。Logeswaran 等人 ([2019](#bib.bib100))
    通过将提及跨度内单词的嵌入与特殊向量相加来标记提及跨度的位置，并使用与 Wu 等人 ([2020b](#bib.bib191)) 相同的简化策略。Yamada
    等人 ([2021](#bib.bib198)) 将文本与其中所有提及连接，并通过基于预训练 BERT 的自注意力模型对该序列进行联合编码。除了 Ganea
    和 Hofmann ([2017](#bib.bib53)) 的简单基于注意力的编码器外，Chen 等人 ([2020](#bib.bib25)) 利用 BERT
    捕捉提及和实体候选之间的类型相似性。他们用特殊的“[MASK]”标记替换提及标记，并提取 BERT 为该标记生成的嵌入。通过对多个提及的嵌入取平均值来生成对应的实体表示。
- en: '![Refer to caption](img/bfdba7a188c060fc1b93f2b6741dc204.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bfdba7a188c060fc1b93f2b6741dc204.png)'
- en: 'Figure 4: Visualization of entity embeddings. Entity embedding space for entities
    related to the ambiguous entity mention “Scott Young”. Three candidate entities
    from Wikipedia are illustrated. For each entity, their most similar $5$ entities
    are shown in the same colors. Entity embeddings are visualized with PCA, which
    is utilized to reduce dimensionality (in this example, to 2D), using pre-trained
    embeddings provided by Yamada et al. ([2020b](#bib.bib197))^(10)^(10)10We used
    the English 100D embeddings from'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：实体嵌入的可视化。与模糊实体提及“Scott Young”相关的实体嵌入空间。展示了来自 Wikipedia 的三个候选实体。对于每个实体，其最相似的
    $5$ 个实体以相同颜色显示。实体嵌入通过 PCA 可视化，PCA 用于降维（在这个例子中为 2D），使用由 Yamada 等人 ([2020b](#bib.bib197))
    提供的预训练嵌入^(10)^(10)10 我们使用了英文 100D 嵌入
- en: '[https://wikipedia2vec.github.io/wikipedia2vec/pretrained](https://wikipedia2vec.github.io/wikipedia2vec/pretrained).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://wikipedia2vec.github.io/wikipedia2vec/pretrained](https://wikipedia2vec.github.io/wikipedia2vec/pretrained)。'
- en: 3.1.3 Entity Encoding
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 实体编码
- en: To make EL systems robust, it is essential to construct distributed vector representations
    of entity candidates $\boldsymbol{y}_{\_}e$ in such a way that they capture semantic
    relatedness between entities in various aspects.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 EL 系统更具鲁棒性，必须以捕捉实体在各种方面的语义相关性的方式构建实体候选的分布式向量表示。
- en: '|  | $\mathsf{eENC}:E^{k}\xrightarrow{}(\boldsymbol{y}_{\_}{e_{\_}1},\boldsymbol{y}_{\_}{e_{\_}2},...,\boldsymbol{y}_{\_}{e_{\_}k}).$
    |  | (7) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{eENC}:E^{k}\xrightarrow{}(\boldsymbol{y}_{\_}{e_{\_}1},\boldsymbol{y}_{\_}{e_{\_}2},...,\boldsymbol{y}_{\_}{e_{\_}k}).$
    |  | (7) |'
- en: 'For instance, in Figure [10](#footnote10 "footnote 10 ‣ Figure 4 ‣ 3.1.2 Context-mention
    Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning"), the most similar entities
    for Scott Young in the Scott_Young_(American_football) sense are related to American
    football, whereas the Scott_Young_(writer) sense is in the proximity of writer-related
    entities.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，在图 [10](#footnote10 "footnote 10 ‣ Figure 4 ‣ 3.1.2 Context-mention Encoding
    ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning") 中，Scott Young 在 Scott_Young_(American_football)
    意义上的最相似实体与美国橄榄球相关，而在 Scott_Young_(writer) 意义上的最相似实体则与作家相关。'
- en: 'There are three common approaches to entity encoding in EL: (1) entity representations
    learned using unstructured texts and algorithms like word2vec Mikolov et al. ([2013a](#bib.bib110))
    based on co-occurrence statistics and developed originally for embedding words;
    (2) entity representations constructed using relations between entities in KGs
    and various graph embedding methods; (3) training a full-fledged neural encoder
    to convert textual descriptions of entities and/or other information into embeddings.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在实体链接 (EL) 中，实体编码的三种常见方法是：（1）使用非结构化文本和像 word2vec Mikolov 等人 ([2013a](#bib.bib110))
    这样的算法学习的实体表示，这些算法基于共现统计数据，并最初用于嵌入词；（2）使用知识图谱 (KG) 中实体之间的关系和各种图嵌入方法构建的实体表示；（3）训练一个完整的神经编码器，将实体的文本描述和/或其他信息转换为嵌入。
- en: 'In the first category, Ganea and Hofmann ([2017](#bib.bib53)) collect entity-word
    co-occurrences statistics from two sources: entity description pages from Wikipedia;
    text surrounding anchors of hyperlinks to Wikipedia pages of corresponding entities.
    They train entity embeddings using the max-margin objective that exploits the
    negative sampling approach like in the word2vec model, so vectors of co-occurring
    words and entities lie closer to each other compared to vectors of random words
    and entities. Some other methods directly replace or extend mention annotations
    (usually anchor text of a hyperlink) with an entity identifier and straightforwardly
    train on the modified corpus a word representation model like word2vec Zwicklbauer
    et al. ([2016b](#bib.bib211), [a](#bib.bib210)); Moreno et al. ([2017](#bib.bib114));
    Tsai and Roth ([2016](#bib.bib176)); Yamada et al. ([2017](#bib.bib195)). In Moreno
    et al. ([2017](#bib.bib114)); Ganea and Hofmann ([2017](#bib.bib53)); Tsai and
    Roth ([2016](#bib.bib176)); Newman-Griffis et al. ([2018](#bib.bib125)), entity
    embeddings are trained in such a way that entities become embedded in the same
    semantic space as words (or texts i.e., sentences and paragraphs Yamada et al.
    ([2017](#bib.bib195))). For example, Newman-Griffis et al. ([2018](#bib.bib125))
    propose a distantly-supervised method that expands the word2vec objective to jointly
    learn words and entity representations in the shared space. The authors leverage
    distant supervision from terminologies that map entities to their surface forms
    (e.g. Wikipedia page titles and redirects or terminology from UMLS Bodenreider
    ([2004](#bib.bib12))).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一类中，Ganea 和 Hofmann ([2017](#bib.bib53)) 从两个来源收集实体-词语共现统计数据：来自维基百科的实体描述页面；以及指向维基百科页面的超链接周围的文本。他们使用最大边际目标训练实体嵌入，利用类似
    word2vec 模型中的负采样方法，因此共现词和实体的向量比随机词和实体的向量更接近。其他一些方法直接用实体标识符替换或扩展提及注释（通常是超链接的锚文本），并在修改后的语料库上直接训练类似
    word2vec 的词表示模型，例如 Zwicklbauer 等人 ([2016b](#bib.bib211), [a](#bib.bib210))；Moreno
    等人 ([2017](#bib.bib114))；Tsai 和 Roth ([2016](#bib.bib176))；Yamada 等人 ([2017](#bib.bib195))。在
    Moreno 等人 ([2017](#bib.bib114))；Ganea 和 Hofmann ([2017](#bib.bib53))；Tsai 和 Roth
    ([2016](#bib.bib176))；Newman-Griffis 等人 ([2018](#bib.bib125)) 中，实体嵌入以这样的方式进行训练，使得实体嵌入到与词（或文本，即句子和段落
    Yamada 等人 ([2017](#bib.bib195))）相同的语义空间中。例如，Newman-Griffis 等人 ([2018](#bib.bib125))
    提出了一个远程监督方法，将 word2vec 目标扩展到在共享空间中共同学习词语和实体表示。作者利用来自术语的远程监督，这些术语将实体映射到其表面形式（例如维基百科页面标题和重定向或来自
    UMLS Bodenreider ([2004](#bib.bib12)) 的术语）。
- en: 'In the second category of entity encoding methods that use relations between
    entities in a KG, Huang et al. ([2015](#bib.bib70)) train a model that generates
    dense entity representations from sparse entity features (e.g. entity relations,
    descriptions) based on the entity relatedness. Several works expand their entity
    relatedness objective with functions that align words (or mentions) and entities
    in a unified vector space Fang et al. ([2016](#bib.bib42)); Yamada et al. ([2016](#bib.bib194));
    Yamada et al. ([2020b](#bib.bib197)); Cao et al. ([2017](#bib.bib19)); Shi et al.
    ([2020](#bib.bib162)); Radhakrishnan et al. ([2018](#bib.bib144)), just like the
    methods from the first category. For example, Yamada et al. ([2016](#bib.bib194))
    jointly optimize three objectives to learn word and entity representations: prediction
    of neighbor words for the given target word, prediction of neighbor entities for
    the target entity based on the relationships in a KG, and prediction of neighbor
    words for the given entity.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二类使用 KG 中实体之间关系的实体编码方法中，Huang et al. ([2015](#bib.bib70)) 训练了一个模型，该模型基于实体相关性从稀疏实体特征（例如实体关系、描述）生成密集实体表示。几项工作通过对齐词汇（或提及）和实体在统一的向量空间中的函数，扩展了它们的实体相关性目标
    Fang et al. ([2016](#bib.bib42))；Yamada et al. ([2016](#bib.bib194))；Yamada et
    al. ([2020b](#bib.bib197))；Cao et al. ([2017](#bib.bib19))；Shi et al. ([2020](#bib.bib162))；Radhakrishnan
    et al. ([2018](#bib.bib144))，就像第一类方法一样。例如，Yamada et al. ([2016](#bib.bib194))
    通过共同优化三个目标来学习词和实体的表示：预测给定目标词的邻近词，基于 KG 中的关系预测目标实体的邻近实体，以及预测给定实体的邻近词。
- en: 'Recently, knowledge graph embedding has become a prominent technique and facilitated
    solving various NLP and data mining tasks Wang et al. ([2017](#bib.bib187)) from
    KG completion Nayyeri et al. ([2019](#bib.bib122)); Bordes et al. ([2013](#bib.bib15));
    Wang et al. ([2014](#bib.bib189)) to entity classification Nickel et al. ([2011](#bib.bib128)).
    For entity linking, two major graph embedding algorithms are widely adopted: DeepWalk
    Perozzi et al. ([2014](#bib.bib136)) and TransE Bordes et al. ([2013](#bib.bib15)).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，知识图谱嵌入已成为一种显著的技术，并促进了解决各种 NLP 和数据挖掘任务 Wang et al. ([2017](#bib.bib187))，从
    KG 完成 Nayyeri et al. ([2019](#bib.bib122))；Bordes et al. ([2013](#bib.bib15))；Wang
    et al. ([2014](#bib.bib189)) 到实体分类 Nickel et al. ([2011](#bib.bib128))。对于实体链接，两种主要的图嵌入算法被广泛采用：DeepWalk
    Perozzi et al. ([2014](#bib.bib136)) 和 TransE Bordes et al. ([2013](#bib.bib15))。
- en: The goal of the DeepWalk Perozzi et al. ([2014](#bib.bib136)) algorithm is to
    produce embeddings of vertices that preserve their proximity in a graph Goyal
    and Ferrara ([2018](#bib.bib58)). It first generates several random walks for
    each vertex in a graph. The generated walks are used as training data for the
    skip-gram algorithm. Like in word2vec for language modeling, given a vertex, the
    algorithm maximizes the probabilities of its neighbors in the generated walks.
    Parravicini et al. ([2019](#bib.bib135)); Sevgili et al. ([2019](#bib.bib156))
    leverage DeepWalk-based graph embeddings built from DBpedia Lehmann et al. ([2015](#bib.bib92))
    for entity linking. Parravicini et al. ([2019](#bib.bib135)) use entity embeddings
    to compute cosine similarity scores of candidate entities in global entity linking.
    Sevgili et al. ([2019](#bib.bib156)) show that combining graph and text-based
    embeddings can slightly improve the performance of neural entity disambiguation
    when compared to using only text-based embeddings.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk Perozzi et al. ([2014](#bib.bib136)) 算法的目标是生成保留图中顶点邻近关系的嵌入。它首先为图中的每个顶点生成若干随机游走。这些生成的游走被用作
    skip-gram 算法的训练数据。与语言建模中的 word2vec 类似，给定一个顶点，该算法最大化生成的游走中其邻居的概率。Parravicini et
    al. ([2019](#bib.bib135))；Sevgili et al. ([2019](#bib.bib156)) 利用基于 DeepWalk 的图嵌入，建立于
    DBpedia Lehmann et al. ([2015](#bib.bib92))，用于实体链接。Parravicini et al. ([2019](#bib.bib135))
    使用实体嵌入计算全球实体链接中候选实体的余弦相似度得分。Sevgili et al. ([2019](#bib.bib156)) 表明，与仅使用基于文本的嵌入相比，结合图和文本基嵌入可以略微提高神经实体消歧的性能。
- en: 'The goal of the TransE Bordes et al. ([2013](#bib.bib15)) algorithm is to construct
    embeddings of both vertices and relations in such a way that they are compatible
    with the facts in a KG Wang et al. ([2017](#bib.bib187)). Consider the facts in
    a KG are represented in the form of triples (i.e. head entity, relation, tail
    entity). If a fact is contained in a KG, the TransE margin-based ranking criterion
    facilitates the presence of the following correspondence between embeddings: $head+relation\approx
    tail$. This means that the relationship in a KG should be a linear translation
    in the embedding space of entities. At the same time, if there is no such fact
    in a KG, this functional relationship should not hold. The TransE-based entity
    representations constructed from Wikidata Vrandečić and Krötzsch ([2014](#bib.bib184))
    and Freebase Bollacker et al. ([2008](#bib.bib14)) have been used for entity representation
    in language modeling Zhang et al. ([2019](#bib.bib206)) and in several works on
    EL Banerjee et al. ([2020](#bib.bib9)); Sorokin and Gurevych ([2018](#bib.bib168));
    Nedelchev et al. ([2020](#bib.bib124)). Banerjee et al. ([2020](#bib.bib9)); Sorokin
    and Gurevych ([2018](#bib.bib168)) utilize Wikidata-based entity embeddings as
    an input component of neural models along with other types of information about
    entities. The ablation study conducted by Banerjee et al. ([2020](#bib.bib9))
    show that the TransE entity embeddings are the most important features for their
    entity linking model. They attribute this finding to the fact that graph embeddings
    contain rich information about the KG structure. Similarly, Sorokin and Gurevych
    ([2018](#bib.bib168)) find that without KG structure information, their entity
    linker experiences a big performance drop. Nedelchev et al. ([2020](#bib.bib124))
    integrate knowledge graph embeddings built from Freebase and word embeddings in
    a single end-to-end model that solves entity and relation linking tasks jointly.
    The quantitative analysis shows that their KG-embedding-based method helps to
    pick correct entity candidates. Recently, Wu et al. ([2020a](#bib.bib190)) also
    utilize TransE embeddings with other types of entity embeddings, like Ganea and
    Hofmann ([2017](#bib.bib53)) or dynamic representation, to compute pairwise entity
    relatedness scores.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: TransE Bordes et al. ([2013](#bib.bib15)) 算法的目标是构建顶点和关系的嵌入，使其与知识图谱中的事实兼容 Wang
    et al. ([2017](#bib.bib187))。考虑到知识图谱中的事实以三元组（即头实体、关系、尾实体）的形式表示。如果一个事实包含在知识图谱中，TransE
    基于边际的排序标准有助于嵌入之间存在如下对应关系：$head+relation\approx tail$。这意味着知识图谱中的关系应该是实体嵌入空间中的线性转换。同时，如果知识图谱中没有这样的事实，这种功能关系不应该成立。从
    Wikidata Vrandečić 和 Krötzsch ([2014](#bib.bib184)) 和 Freebase Bollacker et al.
    ([2008](#bib.bib14)) 构建的基于 TransE 的实体表示已被用于语言建模 Zhang et al. ([2019](#bib.bib206))
    和多个实体链接（EL）相关工作 Banerjee et al. ([2020](#bib.bib9)); Sorokin 和 Gurevych ([2018](#bib.bib168));
    Nedelchev et al. ([2020](#bib.bib124))。Banerjee et al. ([2020](#bib.bib9)); Sorokin
    和 Gurevych ([2018](#bib.bib168)) 将基于 Wikidata 的实体嵌入作为神经模型的输入组件，并结合其他实体信息。Banerjee
    et al. ([2020](#bib.bib9)) 进行的消融研究表明，TransE 实体嵌入是其实体链接模型中最重要的特征。他们将这一发现归因于图嵌入包含关于知识图谱结构的丰富信息。同样，Sorokin
    和 Gurevych ([2018](#bib.bib168)) 发现，如果没有知识图谱结构信息，他们的实体链接器会出现显著的性能下降。Nedelchev
    et al. ([2020](#bib.bib124)) 将从 Freebase 和词嵌入构建的知识图谱嵌入整合到一个端到端的模型中，联合解决实体和关系链接任务。定量分析显示，他们的基于知识图谱嵌入的方法有助于选择正确的实体候选。最近，Wu
    et al. ([2020a](#bib.bib190)) 也利用 TransE 嵌入与其他类型的实体嵌入，如 Ganea 和 Hofmann ([2017](#bib.bib53))
    或动态表示，来计算成对实体的相关性得分。
- en: 'There are many other techniques for KG embedding: Grover and Leskovec ([2016](#bib.bib59));
    Wang et al. ([2014](#bib.bib189)); Nickel et al. ([2011](#bib.bib128)); Trouillon
    et al. ([2016](#bib.bib175)); Yang et al. ([2015](#bib.bib199)); Dettmers et al.
    ([2018](#bib.bib35)) inter alia and very recent 5*E Nayyeri et al. ([2021](#bib.bib123)),
    which is designed to preserve complex graph structures in the embedding space.
    However, they are not widely used in entity linking right now. A detailed overview
    of all graph embedding algorithms is out of the scope of the current work. We
    refer the reader to the previous surveys on this topic Goyal and Ferrara ([2018](#bib.bib58));
    Cai et al. ([2018](#bib.bib18)); Wang et al. ([2017](#bib.bib187)); Ruffinelli
    et al. ([2020](#bib.bib154)) and consider integration of novel KG embedding techniques
    in EL models a promising research direction.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的知识图谱嵌入技术：Grover 和 Leskovec ([2016](#bib.bib59))；Wang 等人 ([2014](#bib.bib189))；Nickel
    等人 ([2011](#bib.bib128))；Trouillon 等人 ([2016](#bib.bib175))；Yang 等人 ([2015](#bib.bib199))；Dettmers
    等人 ([2018](#bib.bib35)) 等，以及最近的 5*E Nayyeri 等人 ([2021](#bib.bib123))，旨在在嵌入空间中保留复杂的图结构。然而，目前它们在实体链接中并不广泛使用。对所有图嵌入算法的详细概述超出了当前工作的范围。我们将读者参考之前关于此主题的调查文献：Goyal
    和 Ferrara ([2018](#bib.bib58))；Cai 等人 ([2018](#bib.bib18))；Wang 等人 ([2017](#bib.bib187))；Ruffinelli
    等人 ([2020](#bib.bib154))，并认为将新颖的知识图谱嵌入技术整合到实体链接模型中是一个有前景的研究方向。
- en: In the last category, we place methods that produce entity representations using
    other types of information like entity descriptions and entity types. Often, an
    entity encoder is a full-fledged neural network, which is a part of an entity
    linking architecture. Sun et al. ([2015](#bib.bib171)) use a neural tensor network
    to encode interactions between surface forms of entities and their category information
    from a KG. In the same vein, Francis-Landau et al. ([2016](#bib.bib49)) and Nguyen
    et al. ([2016b](#bib.bib127)) construct entity representations by encoding titles
    and entity description pages with convolutional neural networks. In addition to
    a convolutional encoder for entity descriptions, Gupta et al. ([2017](#bib.bib62))
    also include an encoder for fine-grained entity types by using the type set of
    FIGER Ling and Weld ([2012](#bib.bib96)). Gillick et al. ([2019](#bib.bib55))
    construct entity representations by encoding entity page titles, short entity
    descriptions, and entity category information with feed-forward networks. Le and
    Titov ([2019b](#bib.bib87)) use only entity type information from a KG and a simple
    feed-forward network for entity encoding. Hou et al. ([2020](#bib.bib69)) also
    leverage entity types. However, instead of relying on existing type sets like
    in Gupta et al. ([2017](#bib.bib62)), they construct custom fine-grained semantic
    types using words from starting sentences of Wikipedia pages. To represent entities,
    they first average the word vectors of entity types and then linearly aggregate
    them with embeddings of Ganea and Hofmann ([2017](#bib.bib53)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个类别中，我们将使用其他类型的信息（如实体描述和实体类型）来生成实体表示的方法归入其中。通常，实体编码器是一个完整的神经网络，它是实体链接架构的一部分。Sun
    等人 ([2015](#bib.bib171)) 使用神经张量网络来编码实体表面形式与其来自知识图谱的类别信息之间的交互。同样，Francis-Landau
    等人 ([2016](#bib.bib49)) 和 Nguyen 等人 ([2016b](#bib.bib127)) 通过使用卷积神经网络对标题和实体描述页面进行编码来构建实体表示。除了用于实体描述的卷积编码器，Gupta
    等人 ([2017](#bib.bib62)) 还通过使用 FIGER Ling 和 Weld ([2012](#bib.bib96)) 的类型集来包含对细粒度实体类型的编码。Gillick
    等人 ([2019](#bib.bib55)) 通过使用前馈网络对实体页面标题、简短实体描述和实体类别信息进行编码来构建实体表示。Le 和 Titov ([2019b](#bib.bib87))
    仅使用来自知识图谱的实体类型信息以及一个简单的前馈网络来进行实体编码。Hou 等人 ([2020](#bib.bib69)) 也利用实体类型。然而，与 Gupta
    等人 ([2017](#bib.bib62)) 依赖现有类型集的方法不同，他们使用来自维基百科页面起始句子的词语来构建自定义的细粒度语义类型。为了表示实体，他们首先平均实体类型的词向量，然后将其与
    Ganea 和 Hofmann ([2017](#bib.bib53)) 的嵌入进行线性聚合。
- en: 'Recent works leverage deep language models like BERT Devlin et al. ([2019](#bib.bib36))
    or ELMo Peters et al. ([2018](#bib.bib138)) for encoding entities. Nie et al.
    ([2018](#bib.bib129)) use an architecture based on a recurrent network for obtaining
    entity representations from Wikipedia entity description pages. Subsequently,
    several models adopt BERT for the same purpose Logeswaran et al. ([2019](#bib.bib100));
    Wu et al. ([2020b](#bib.bib191)) inter alia. Yamada et al. ([2021](#bib.bib198))
    propose a masked entity prediction task, where a model based on the BERT architecture
    learns to predict randomly masked input entities. This task makes the model learn
    also how to generate entity representations along with standard word representations.
    Shahbazi et al. ([2019](#bib.bib158)) introduce E-ELMo that extends the ELMo model
    Peters et al. ([2018](#bib.bib138)) with an additional objective. The model is
    trained in a multi-task fashion: to predict next/previous words, as in a standard
    bidirectional language model, and to predict the target entity when encountering
    its mentions. As a result, besides the model for mention encoding, entity representations
    are obtained. Mulang’ et al. ([2020](#bib.bib117)) use bidirectional Transformers
    to jointly encode context of a mention, a candidate entity name, and multiple
    relationships of a candidate entity from a KG verbalized into textual triples:
    “[subject] [predicate] [object]”. The input sequence of the encoder is composed
    simply by appending all these types of information delimited by a special separator
    token.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作利用了深度语言模型，如BERT Devlin et al. ([2019](#bib.bib36))或ELMo Peters et al. ([2018](#bib.bib138))来编码实体。Nie
    et al. ([2018](#bib.bib129))使用基于递归网络的架构从维基百科实体描述页面获取实体表示。随后，几个模型采用BERT来实现同样的目的，如Logeswaran
    et al. ([2019](#bib.bib100))；Wu et al. ([2020b](#bib.bib191))等。Yamada et al. ([2021](#bib.bib198))提出了一种掩蔽实体预测任务，其中基于BERT架构的模型学习预测随机掩蔽的输入实体。这项任务使模型学会了如何生成实体表示以及标准的词表示。Shahbazi
    et al. ([2019](#bib.bib158))引入了E-ELMo，这是一种扩展了ELMo模型Peters et al. ([2018](#bib.bib138))的额外目标的模型。该模型以多任务方式进行训练：预测下一词/前一词，如标准的双向语言模型，并在遇到其提及时预测目标实体。因此，除了用于提及编码的模型外，还获得了实体表示。Mulang’
    et al. ([2020](#bib.bib117))使用双向Transformer共同编码提及的上下文、候选实体名称以及从KG中提取的多个候选实体关系，这些关系以文本三元组的形式表达：“[主语]
    [谓语] [宾语]”。编码器的输入序列简单地通过添加所有这些类型的信息，并用特殊分隔符标记进行分隔。
- en: '![Refer to caption](img/2594bcb9087b26ef088dde7461a0bb01.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2594bcb9087b26ef088dde7461a0bb01.png)'
- en: 'Figure 5: Entity ranking. A generalized entity candidate ranking neural architecture:
    entity candidates are ranked according their appropriateness for a particular
    mention in the current context.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：实体排名。一个广义的实体候选排名神经架构：实体候选根据其在当前上下文中特定提及的适用性进行排名。
- en: 3.1.4 Entity Ranking
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 实体排名
- en: 'The goal of this stage is given a list of entity candidates $(e_{\_}1,e_{\_}2,...,e_{\_}k)$
    from a KG and a context $C$ with a mention $M$ to rank these entities assigning
    a score to each of them, as in Equation [8](#S3.E8 "In 3.1.4 Entity Ranking ‣
    3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A
    Survey of Models Based on Deep Learning"), where $n$ is a number of entity mentions
    in a document, $k$ is a number of candidate entities. Figure [5](#S3.F5 "Figure
    5 ‣ 3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning") depicts the
    typical architecture of the ranking component.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这一阶段的目标是给定一个来自知识图谱（KG）的实体候选列表$(e_{\_}1,e_{\_}2,...,e_{\_}k)$以及一个包含提及$M$的上下文$C$，对这些实体进行排名，为每个实体分配一个分数，如公式[8](#S3.E8
    "在 3.1.4 实体排名 ‣ 3.1 总体架构 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习的模型概述")所示，其中$n$是文档中实体提及的数量，$k$是候选实体的数量。图[5](#S3.F5
    "图 5 ‣ 3.1.3 实体编码 ‣ 3.1 总体架构 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习的模型概述")描绘了排名组件的典型架构。
- en: '|  | $\mathsf{RNK}:((e_{\_}1,e_{\_}2,...,e_{\_}k),C,M)^{n}\xrightarrow{}\mathbb{R}^{n\times
    k}.$ |  | (8) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{RNK}:((e_{\_}1,e_{\_}2,...,e_{\_}k),C,M)^{n}\xrightarrow{}\mathbb{R}^{n\times
    k}.$ |  | (8) |'
- en: 'The mention representation $\boldsymbol{y}_{\_}m$ generated in the mention
    encoding step is compared with candidate entity representations $\boldsymbol{y}_{\_}{e_{\_}i}(i=1,2,\dots,k)$
    according to the similarity measure $s(m,e_{\_}i)$. Entity representations can
    be pre-trained (see Section [3.1.3](#S3.SS1.SSS3 "3.1.3 Entity Encoding ‣ 3.1
    General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning")) or generated by another encoder as in some
    zero-shot approaches (see Section [3.2.3](#S3.SS2.SSS3 "3.2.3 Domain-Independent
    Architectures ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")).
    The BERT-based model of Yamada et al. ([2021](#bib.bib198)) simultaneously learns
    how to encode mentions and entity embeddings in the unified architecture.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '在提及编码步骤中生成的提及表示 $\boldsymbol{y}_{\_}{m}$ 会根据相似度度量 $s(m,e_{\_}{i})$ 与候选实体表示
    $\boldsymbol{y}_{\_}{e_{\_}{i}}(i=1,2,\dots,k)$ 进行比较。实体表示可以预训练（见第 [3.1.3](#S3.SS1.SSS3
    "3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity Linking ‣
    Neural Entity Linking: A Survey of Models Based on Deep Learning") 节）或由另一个编码器生成，如某些零样本方法中所述（见第
    [3.2.3](#S3.SS2.SSS3 "3.2.3 Domain-Independent Architectures ‣ 3.2 Modifications
    of the General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning") 节）。Yamada 等人（[2021](#bib.bib198)）的基于
    BERT 的模型在统一架构中同时学习如何编码提及和实体嵌入。'
- en: 'Most of the state-of-the-art studies compute similarity $s(m,e)$ between representations
    of a mention $m$ and an entity $e$ using a dot product as in Ganea and Hofmann
    ([2017](#bib.bib53)); Gupta et al. ([2017](#bib.bib62)); Kolitsas et al. ([2018](#bib.bib82));
    Peters et al. ([2019](#bib.bib139)); Wu et al. ([2020b](#bib.bib191)):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数最先进的研究使用点积计算提及 $m$ 和实体 $e$ 表示之间的相似度 $s(m,e)$，如 Ganea 和 Hofmann（[2017](#bib.bib53)）、Gupta
    等人（[2017](#bib.bib62)）、Kolitsas 等人（[2018](#bib.bib82)）、Peters 等人（[2019](#bib.bib139)）、Wu
    等人（[2020b](#bib.bib191)）所述。
- en: '|  | $s\left(m,e_{\_}{i}\right)=\boldsymbol{y}_{\_}{m}\cdot\boldsymbol{y}_{\_}{e_{\_}{i}};$
    |  | (9) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $s\left(m,e_{\_}{i}\right)=\boldsymbol{y}_{\_}{m}\cdot\boldsymbol{y}_{\_}{e_{\_}{i}};$
    |  | (9) |'
- en: 'or cosine similarity as in Sun et al. ([2015](#bib.bib171)); Francis-Landau
    et al. ([2016](#bib.bib49)); Gillick et al. ([2019](#bib.bib55)):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如 Sun 等人（[2015](#bib.bib171)）、Francis-Landau 等人（[2016](#bib.bib49)）、Gillick
    等人（[2019](#bib.bib55)）所述的余弦相似度：
- en: '|  | $s\left(m,e_{\_}{i}\right)=\cos(\boldsymbol{y}_{\_}{m},\boldsymbol{y}_{\_}{e_{\_}{i}})=\frac{\boldsymbol{y}_{\_}{m}\cdot\boldsymbol{y}_{\_}{e_{\_}{i}}}{\&#124;\boldsymbol{y}_{\_}{m}\&#124;\cdot\&#124;\boldsymbol{y}_{\_}{e_{\_}{i}}\&#124;}.$
    |  | (10) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $s\left(m,e_{\_}{i}\right)=\cos(\boldsymbol{y}_{\_}{m},\boldsymbol{y}_{\_}{e_{\_}{i}})=\frac{\boldsymbol{y}_{\_}{m}\cdot\boldsymbol{y}_{\_}{e_{\_}{i}}}{\&#124;\boldsymbol{y}_{\_}{m}\&#124;\cdot\&#124;\boldsymbol{y}_{\_}{e_{\_}{i}}\&#124;}.$
    |  | (10) |'
- en: 'The final disambiguation decision is inferred via a probability distribution
    $P(e_{\_}i|m)$, which is usually approximated by a softmax function over the candidates.
    The calculated similarity score or probability can be combined with mention-entity
    priors obtained during the candidate generation phase Francis-Landau et al. ([2016](#bib.bib49));
    Ganea and Hofmann ([2017](#bib.bib53)); Kolitsas et al. ([2018](#bib.bib82)) or
    other features $f(e_{\_}i,m)$ such as various similarities, a string matching
    indicator, and entity types or type similarity Francis-Landau et al. ([2016](#bib.bib49));
    Shahbazi et al. ([2018](#bib.bib157)); Sil et al. ([2018](#bib.bib164)); Shahbazi
    et al. ([2019](#bib.bib158)); Yang et al. ([2019](#bib.bib200)); Chen et al. ([2020](#bib.bib25)).
    One of the common techniques for that is to use an additional one or two-layer
    feedforward network $\phi(\cdot,\cdot)$ Francis-Landau et al. ([2016](#bib.bib49));
    Ganea and Hofmann ([2017](#bib.bib53)); Shahbazi et al. ([2019](#bib.bib158)).
    The obtained local similarity score $\Phi(e_{\_}i,m)$ or the probability distribution
    can be further utilized for global scoring (see Section [3.2.2](#S3.SS2.SSS2 "3.2.2
    Global Context Architectures ‣ 3.2 Modifications of the General Architecture ‣
    3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning")).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '最终的消歧决策是通过概率分布 $P(e_{\_}i\mid m)$ 推断的，这通常通过对候选者使用 softmax 函数来近似。计算出的相似性评分或概率可以与候选生成阶段获得的提及-实体先验结合使用
    Francis-Landau 等 ([2016](#bib.bib49))；Ganea 和 Hofmann ([2017](#bib.bib53))；Kolitsas
    等 ([2018](#bib.bib82))，或者其他特征 $f(e_{\_}i,m)$，例如各种相似性、字符串匹配指示符以及实体类型或类型相似性 Francis-Landau
    等 ([2016](#bib.bib49))；Shahbazi 等 ([2018](#bib.bib157))；Sil 等 ([2018](#bib.bib164))；Shahbazi
    等 ([2019](#bib.bib158))；Yang 等 ([2019](#bib.bib200))；Chen 等 ([2020](#bib.bib25))。一种常见的技术是使用一个或两个额外的前馈网络
    $\phi(\cdot,\cdot)$ Francis-Landau 等 ([2016](#bib.bib49))；Ganea 和 Hofmann ([2017](#bib.bib53))；Shahbazi
    等 ([2019](#bib.bib158))。获得的局部相似性评分 $\Phi(e_{\_}i,m)$ 或概率分布可以进一步用于全局评分（见第 [3.2.2](#S3.SS2.SSS2
    "3.2.2 Global Context Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning") 节）。'
- en: '|  | $P(e_{\_}i&#124;m)=\frac{\exp(s(m,e_{\_}i))}{\sum_{\_}{i=1}^{k}{\exp(s(m,e_{\_}i))}}.$
    |  | (11) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(e_{\_}i\mid m)=\frac{\exp(s(m,e_{\_}i))}{\sum_{\_}{i=1}^{k}{\exp(s(m,e_{\_}i))}}.$
    |  | (11) |'
- en: '|  | $\Phi(e_{\_}i,m)=\phi(P(e_{\_}i&#124;m),f(e_{\_}i,m)).$ |  | (12) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi(e_{\_}i,m)=\phi(P(e_{\_}i\mid m),f(e_{\_}i,m)).$ |  | (12) |'
- en: 'There are several approaches to framing a training objective in the literature
    on EL. Consider that we have $k$ candidates for the target mention $m$, one of
    which is a true entity $e_{\_}*$. In some works, the models are trained with the
    standard negative log-likelihood objective like in classification tasks Logeswaran
    et al. ([2019](#bib.bib100)); Wu et al. ([2020b](#bib.bib191)). However, instead
    of classes, negative candidates are used:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在实体链接（EL）的文献中，有几种方法用于制定训练目标。假设我们有 $k$ 个目标提及 $m$ 的候选者，其中之一是真正的实体 $e_{\_}*$。在一些研究中，模型使用标准的负对数似然目标进行训练，类似于分类任务
    Logeswaran 等 ([2019](#bib.bib100))；Wu 等 ([2020b](#bib.bib191))。然而，使用的不是类别，而是负候选者：
- en: '|  | $\mathcal{L}\left(m\right)=-s\left(m,e_{\_}*\right)+\log\sum_{\_}{i=1}^{k}\exp\left(s\left(m,e_{\_}{i}\right)\right).$
    |  | (13) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}\left(m\right)=-s\left(m,e_{\_}*\right)+\log\sum_{\_}{i=1}^{k}\exp\left(s\left(m,e_{\_}{i}\right)\right).$
    |  | (13) |'
- en: 'Instead of the the negative log-likelihood, some works use variants of a ranking
    loss. The idea behind such an approach is to enforce a positive margin $\gamma>0$
    between similarity scores of mentions to positive and negative candidates Ganea
    and Hofmann ([2017](#bib.bib53)); Kolitsas et al. ([2018](#bib.bib82)); Peters
    et al. ([2019](#bib.bib139)):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与负对数似然函数不同，一些研究使用了排序损失的变体。这种方法的基本思想是强制将正负候选者的相似性评分之间保持一个正的间隔 $\gamma>0$。例如，Ganea
    和 Hofmann ([2017](#bib.bib53))；Kolitsas 等 ([2018](#bib.bib82))；Peters 等 ([2019](#bib.bib139))：
- en: '|  | $\mathcal{L}(m)=\sum_{\_}i{\ell(e_{\_}i,m)},\text{ where }$ |  | (14)
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(m)=\sum_{\_}i{\ell(e_{\_}i,m)},\text{ 其中 }$ |  | (14) |'
- en: '|  | $\ell(e_{\_}i,m)=\left[\gamma-\Phi\left(e_{\_}*,m\right)+\Phi(e_{\_}i,m)\right]_{\_}{+}.$
    |  | (15) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\ell(e_{\_}i,m)=\left[\gamma-\Phi\left(e_{\_}*,m\right)+\Phi(e_{\_}i,m)\right]_{\_}{+}.$
    |  | (15) |'
- en: or
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '|  | <math   alttext="\begin{array}[]{l}\ell(e_{\_}i,m)=\\ \quad\left\{\begin{array}[]{ll}\left[\gamma-\Phi(e_{\_}i,m)\right]_{\_}{+},&amp;\text{
    if }e_{\_}i\text{ equal }e_{\_}*\\'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{array}[]{l}\ell(e_{\_}i,m)=\\ \quad\left\{\begin{array}[]{ll}\left[\gamma-\Phi(e_{\_}i,m)\right]_{\_}{+},&amp;\text{
    如果 }e_{\_}i\text{ 等于 }e_{\_}*\\'
- en: \left[\Phi(e_{\_}i,m)\right]_{\_}{+},&amp;\text{ otherwise. }\end{array}\right.\end{array}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="left" ><mrow ><mrow ><mi mathvariant="normal" >ℓ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >e</mi><mi
    mathvariant="normal" >_</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi></mrow><mo
    >,</mo><mi >m</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mo >{</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd columnalign="left" ><mrow ><mrow ><msub ><mrow ><mo
    >[</mo><mrow ><mi >γ</mi><mo >−</mo><mrow ><mi mathvariant="normal" >Φ</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msub
    ><mi >e</mi><mi mathvariant="normal" >_</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mi >i</mi></mrow><mo >,</mo><mi >m</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow><mo
    >]</mo></mrow><mi mathvariant="normal" >_</mi></msub><mo rspace="0em" >+</mo></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mrow ><mtext > if </mtext><msub
    ><mi >e</mi><mi mathvariant="normal" >_</mi></msub><mi >i</mi><mtext > equal </mtext><msub
    ><mi >e</mi><mi mathvariant="normal" >_</mi></msub><mo lspace="0.222em" >∗</mo></mrow></mtd></mtr><mtr
    ><mtd columnalign="left" ><mrow ><mrow ><msub ><mrow ><mo >[</mo><mrow ><mi mathvariant="normal"
    >Φ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><msub ><mi >e</mi><mi mathvariant="normal" >_</mi></msub><mo lspace="0em" rspace="0em"
    >​</mo><mi >i</mi></mrow><mo >,</mo><mi >m</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >]</mo></mrow><mi mathvariant="normal" >_</mi></msub><mo rspace="0em" >+</mo></mrow><mo
    >,</mo></mrow></mtd><mtd columnalign="left" ><mtext > otherwise. </mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{array}[]{l}\ell(e_{\_}i,m)=\\ \quad\left\{\begin{array}[]{ll}\left[\gamma-\Phi(e_{\_}i,m)\right]_{\_}{+},&\text{
    if }e_{\_}i\text{ equal }e_{\_}*\\ \left[\Phi(e_{\_}i,m)\right]_{\_}{+},&\text{
    otherwise. }\end{array}\right.\end{array}</annotation></semantics></math> |  |
    (16) |
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \(\ell(e_{\_}i,m)=\left\{\begin{array}{ll}\left[\gamma-\Phi(e_{\_}i,m)\right]_{\_}{+},&
    \text{如果 } e_{\_}i \text{ 等于 } e_{\_}*\\ \left[\Phi(e_{\_}i,m)\right]_{\_}{+},&
    \text{否则。}\end{array}\right.\)
- en: <svg   height="187.29" overflow="visible" version="1.1" width="632.4"><g transform="translate(0,187.29)
    matrix(1 0 0 -1 0 0) translate(316.2,0) translate(0,174.1)"><g stroke="#000000"><g
    fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -78.74
    -1.38)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="157.48" height="16.6"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.2 - Modifications
    of the General Architecture <g stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -176.63
    -49.52 L -307.62 -49.52 C -312.21 -49.52 -315.92 -53.24 -315.92 -57.83 L -315.92
    -60.28 C -315.92 -64.87 -312.21 -68.59 -307.62 -68.59 L -176.63 -68.59 C -172.05
    -68.59 -168.33 -64.87 -168.33 -60.28 L -168.33 -57.83 C -168.33 -53.24 -172.05
    -49.52 -176.63 -49.52 Z M -315.92 -68.59" style="stroke:none"></path></g><g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -176.63 -49.52 L -307.62 -49.52 C -312.21 -49.52 -315.92
    -53.24 -315.92 -57.83 L -315.92 -60.28 C -315.92 -64.87 -312.21 -68.59 -307.62
    -68.59 L -176.63 -68.59 C -172.05 -68.59 -168.33 -64.87 -168.33 -60.28 L -168.33
    -57.83 C -168.33 -53.24 -172.05 -49.52 -176.63 -49.52 Z M -315.92 -68.59"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -311.31 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject
    width="138.37" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"
    color="#EA6B66">3.2.1 - Joint Entity Mention Detection and Disambiguation Architecture</foreignobject></g></g>
    <g stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path
    d="M -54.07 -13.19 L -195.63 -47.71" style="fill:none"><g transform="matrix(-0.97153
    -0.23692 0.23692 -0.97153 -195.63 -47.71)"><path d="M 6.48 0 C 4.56 0.36 1.44
    1.44 -0.72 2.7 L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -15.21 -49.6 L -146.2
    -49.6 C -150.79 -49.6 -154.51 -53.32 -154.51 -57.9 L -154.51 -60.21 C -154.51
    -64.79 -150.79 -68.51 -146.2 -68.51 L -15.21 -68.51 C -10.63 -68.51 -6.91 -64.79
    -6.91 -60.21 L -6.91 -57.9 C -6.91 -53.32 -10.63 -49.6 -15.21 -49.6 Z M -154.51
    -68.51" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -15.21 -49.6 L -146.2 -49.6 C -150.79 -49.6 -154.51 -53.32 -154.51 -57.9
    L -154.51 -60.21 C -154.51 -64.79 -150.79 -68.51 -146.2 -68.51 L -15.21 -68.51
    C -10.63 -68.51 -6.91 -64.79 -6.91 -60.21 L -6.91 -57.9 C -6.91 -53.32 -10.63
    -49.6 -15.21 -49.6 Z M -154.51 -68.51"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 -149.89 -63.9)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.2.2
    - Global Context Architecture</foreignobject></g></g> <g stroke="#999999" fill="#999999"
    stroke-width="0.8pt" color="#999999"><path d="M -18.02 -13.19 L -62.19 -45.5"
    style="fill:none"><g transform="matrix(-0.80708 -0.59044 0.59044 -0.80708 -62.19
    -45.5)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44
    -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt"><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 146.2 -49.52 L 15.21 -49.52 C 10.63 -49.52 6.91
    -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63 -68.59 15.21 -68.59 L 146.2
    -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28 L 154.51 -57.83 C 154.51 -53.24
    150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#FFFFFF"><path d="M 146.2 -49.52 L 15.21 -49.52 C 10.63
    -49.52 6.91 -53.24 6.91 -57.83 L 6.91 -60.28 C 6.91 -64.87 10.63 -68.59 15.21
    -68.59 L 146.2 -68.59 C 150.79 -68.59 154.51 -64.87 154.51 -60.28 L 154.51 -57.83
    C 154.51 -53.24 150.79 -49.52 146.2 -49.52 Z M 6.91 -68.59"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 11.52 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.2.3
    - Domain Independent Architecture</foreignobject></g></g> <g stroke="#999999"
    fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M 18.02 -13.19 L
    62.08 -45.42" style="fill:none"><g transform="matrix(0.80707 -0.59045 0.59045
    0.80707 62.08 -45.42)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72
    -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke-width="0.4pt"><g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 307.62 -49.52 L 176.63
    -49.52 C 172.05 -49.52 168.33 -53.24 168.33 -57.83 L 168.33 -60.28 C 168.33 -64.87
    172.05 -68.59 176.63 -68.59 L 307.62 -68.59 C 312.21 -68.59 315.92 -64.87 315.92
    -60.28 L 315.92 -57.83 C 315.92 -53.24 312.21 -49.52 307.62 -49.52 Z M 168.33
    -68.59" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 307.62 -49.52 L 176.63 -49.52 C 172.05 -49.52 168.33 -53.24 168.33 -57.83
    L 168.33 -60.28 C 168.33 -64.87 172.05 -68.59 176.63 -68.59 L 307.62 -68.59 C
    312.21 -68.59 315.92 -64.87 315.92 -60.28 L 315.92 -57.83 C 315.92 -53.24 312.21
    -49.52 307.62 -49.52 Z M 168.33 -68.59"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 172.94 -63.82)" fill="#EA6B66" stroke="#EA6B66"><foreignobject width="138.37"
    height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#EA6B66">3.2.4
    - Cross-lingual Architecture</foreignobject></g></g> <g stroke="#999999" fill="#999999"
    stroke-width="0.8pt" color="#999999"><path d="M 54.07 -13.19 L 195.63 -47.71"
    style="fill:none"><g transform="matrix(0.97153 -0.23692 0.23692 0.97153 195.63
    -47.71)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7 L -0.72 -2.7 C 1.44
    -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -294.88 -111.34 h 147.02 v 25.83 h -147.02 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -290.27 -99.81)"><foreignobject width="137.8" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">candidate based Kolitsas et al. ([2018](#bib.bib82));
    Peters et al. ([2019](#bib.bib139))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -293.78 -136.87 h 117.15 v 25.83 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -289.17 -125.34)"><foreignobject width="107.93" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">multitask learning Martins et al. ([2019](#bib.bib107))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M -293.78 -162.41 h 117.15 v 25.83
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -289.17 -150.88)"><foreignobject
    width="107.93" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">sequence
    labeling Broscheit ([2019](#bib.bib17))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -135.27 -100.27 h 131.27 v 25.83 h -131.27 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -130.66 -88.74)"><foreignobject width="122.05" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">graph based Zwicklbauer et al. ([2016b](#bib.bib211));
    Cao et al. ([2018](#bib.bib20))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M -134.85 -136.87 h 147.02 v 42.43 h -147.02 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -130.23 -108.74)"><foreignobject width="137.8" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">maximization of CRF potentials Ganea and Hofmann
    ([2017](#bib.bib53)); Le and Titov ([2018](#bib.bib85))</foreignobject></g> <g
    stroke-width="0.4pt" fill="#FFFFFF"><path d="M -133.93 -155.49 h 206.07 v 25.83
    h -206.07 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -129.32 -143.96)"><foreignobject
    width="196.85" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">sequential
    decision task Fang et al. ([2019](#bib.bib43)); Yang et al. ([2019](#bib.bib200));
    Yamada et al. ([2021](#bib.bib198))</foreignobject></g> <g stroke-width="0.4pt"
    fill="#FFFFFF"><path d="M -133.46 -174.1 h 147.02 v 25.83 h -147.02 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 -128.85 -162.57)"><foreignobject width="137.8" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">others Kolitsas et al. ([2018](#bib.bib82));
    Fang et al. ([2016](#bib.bib42))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 25.34 -104.42 h 127.33 v 25.83 h -127.33 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 29.96 -92.89)"><foreignobject width="118.11" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">distant learning Le and Titov ([2019b](#bib.bib87),
    [a](#bib.bib86))</foreignobject></g> <g stroke-width="0.4pt" fill="#FFFFFF"><path
    d="M 26.28 -129.95 h 117.15 v 25.83 h -117.15 Z" style="stroke:none"></path></g><g
    stroke-width="0.4pt" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0
    0.0 1.0 30.9 -118.42)"><foreignobject width="107.93" height="16.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">zero-shot Logeswaran et al. ([2019](#bib.bib100));
    Gillick et al. ([2019](#bib.bib55)); Wu et al. ([2020b](#bib.bib191))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 189.08 -111.34 h 117.15 v 25.83
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 193.7 -99.81)"><foreignobject
    width="107.93" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">representation
    based Pan et al. ([2017](#bib.bib134)); Tsai and Roth ([2016](#bib.bib176))</foreignobject></g>
    <g stroke-width="0.4pt" fill="#FFFFFF"><path d="M 189.08 -143.79 h 117.15 v 25.83
    h -117.15 Z" style="stroke:none"></path></g><g stroke-width="0.4pt" fill="#000000"
    stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 193.7 -132.26)"><foreignobject
    width="107.93" height="16.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">zero-shot
    Upadhyay et al. ([2018a](#bib.bib179)); Sil et al. ([2018](#bib.bib164))</foreignobject></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -266.41 -68.86 L -266.41 -98.43 L -288.68 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -288.68 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -266.41 -68.86 L -266.41 -123.96 L -287.58 -123.96" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -123.96)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -266.41 -68.86 L -266.41 -149.49 L -287.58 -149.49" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -287.58 -149.49)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -119.76 -68.79 L -119.76 -87.36 L -129.08 -87.36" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -129.08 -87.36)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -119.76 -68.79 L -119.76 -115.66 L -128.65 -115.66" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -128.65 -115.66)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -119.76 -68.79 L -119.76 -142.57 L -127.73 -142.57" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -127.73 -142.57)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    -119.76 -68.79 L -119.76 -161.19 L -127.26 -161.19" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 -127.26 -161.19)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -91.51 L 31.54 -91.51" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 31.54 -91.51)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    44.1 -68.86 L 44.1 -117.04 L 32.48 -117.04" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 32.48 -117.04)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    205.52 -68.86 L 205.52 -98.43 L 195.28 -98.43" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 195.28 -98.43)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g><g
    stroke="#999999" fill="#999999" stroke-width="0.8pt" color="#999999"><path d="M
    205.52 -68.86 L 205.52 -130.88 L 195.28 -130.88" style="fill:none"><g transform="matrix(-1.0
    0.0 0.0 -1.0 195.28 -130.88)"><path d="M 6.48 0 C 4.56 0.36 1.44 1.44 -0.72 2.7
    L -0.72 -2.7 C 1.44 -1.44 4.56 -0.36 6.48 0" style="stroke:none"></path></g></path></g>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Reference map of the modifications of the general architecture for
    neural EL. The categorization of each modification with various design choices
    and example references illustrating each choice. Sections [3.2.3](#S3.SS2.SSS3
    "3.2.3 Domain-Independent Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning") and [3.2.4](#S3.SS2.SSS4 "3.2.4 Cross-lingual Architectures ‣
    3.2 Modifications of the General Architecture ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") are categorized based
    on their EL solutions, here.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6：神经 EL 通用架构修改的参考图。展示了每个修改的分类、各种设计选择以及说明每种选择的示例参考文献。第[3.2.3](#S3.SS2.SSS3
    "3.2.3 Domain-Independent Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning")和[3.2.4](#S3.SS2.SSS4 "3.2.4 Cross-lingual Architectures ‣ 3.2
    Modifications of the General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity
    Linking: A Survey of Models Based on Deep Learning")部分根据其 EL 解决方案进行了分类。'
- en: 3.1.5 Unlinkable Mention Prediction
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 不可链接提及预测
- en: 'The referent entities of some mentions can be absent in the KGs, e.g. there
    is no Wikipedia entry about Scott Young as a cricket player of the Stenhousemuir
    cricket club.^(11)^(11)11Information about Scott Young as a cricket player: [https://www.stenhousemuircricketclub.com/teams/171906/player/scott-young-1828009](https://www.stenhousemuircricketclub.com/teams/171906/player/scott-young-1828009)
    Therefore, an EL system should be able to predict the absence of a reference if
    a mention appears in specific contexts, which is known as the NIL prediction task:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 某些提及的参考实体可能在知识图谱（KGs）中缺失，例如没有关于 Scott Young 作为 Stenhousemuir 板球队员的维基百科条目。^(11)^(11)11关于
    Scott Young 作为板球队员的信息：[https://www.stenhousemuircricketclub.com/teams/171906/player/scott-young-1828009](https://www.stenhousemuircricketclub.com/teams/171906/player/scott-young-1828009)。因此，EL
    系统应该能够预测在特定上下文中出现提及时参考缺失，这被称为 NIL 预测任务：
- en: '|  | $\mathsf{NILp}:(C,M)^{n}\xrightarrow{}\{0,1\}^{n}.$ |  | (17) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{NILp}:(C,M)^{n}\xrightarrow{}\{0,1\}^{n}.$ |  | (17) |'
- en: The NIL prediction task is essentially a classification with a reject option Hellman
    ([1970](#bib.bib64)); Fumera et al. ([2000](#bib.bib51)); Herbei and Wegkamp ([2006](#bib.bib65)).
    There are four common ways to perform NIL prediction. Sometimes a candidate generator
    does not yield any corresponding entities for a mention; such mentions are trivially
    considered unlikable Tsai and Roth ([2016](#bib.bib176)); Sil et al. ([2018](#bib.bib164)).
    One can set a threshold for the best linking probability (or a score), below which
    a mention is considered unlinkable Peters et al. ([2019](#bib.bib139)); Lazic
    et al. ([2015](#bib.bib84)). Some models introduce an additional special “NIL”
    entity in the ranking phase, so models can predict it as the best match for the
    mention Kolitsas et al. ([2018](#bib.bib82)). It is also possible to train an
    additional binary classifier that accepts mention-entity pairs after the ranking
    phase, as well as several additional features (best linking score, whether mentions
    are also detected by a dedicated NER system, etc.), as input and makes the final
    decision about whether a mention is linkable or not Moreno et al. ([2017](#bib.bib114));
    Martins et al. ([2019](#bib.bib107)).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: NIL 预测任务本质上是一种带有拒绝选项的分类 Hellman ([1970](#bib.bib64)); Fumera et al. ([2000](#bib.bib51));
    Herbei 和 Wegkamp ([2006](#bib.bib65))。执行 NIL 预测有四种常见方法。有时候候选生成器不会为提及生成任何对应实体；这些提及被视为
    trivially 不可链接 Tsai 和 Roth ([2016](#bib.bib176)); Sil et al. ([2018](#bib.bib164))。可以为最佳链接概率（或分数）设置一个阈值，低于该阈值的提及被视为不可链接
    Peters et al. ([2019](#bib.bib139)); Lazic et al. ([2015](#bib.bib84))。一些模型在排名阶段引入了一个额外的特殊“NIL”实体，因此模型可以预测它作为提及的最佳匹配
    Kolitsas et al. ([2018](#bib.bib82))。还可以训练一个额外的二元分类器，该分类器在排名阶段后接受提及-实体对以及几个附加特征（最佳链接分数、是否由专用
    NER 系统检测到提及等）作为输入，并做出关于提及是否可链接的最终决定 Moreno et al. ([2017](#bib.bib114)); Martins
    et al. ([2019](#bib.bib107))。
- en: 3.2 Modifications of the General Architecture
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通用架构的修改
- en: 'This section presents the most notable modifications and improvements of the
    general architecture of neural entity linking models presented in Section [3.1](#S3.SS1
    "3.1 General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A
    Survey of Models Based on Deep Learning") and Figures [2](#S1.F2 "Figure 2 ‣ 1.4
    Contributions ‣ 1 Introduction ‣ Neural Entity Linking: A Survey of Models Based
    on Deep Learning") and [5](#S3.F5 "Figure 5 ‣ 3.1.3 Entity Encoding ‣ 3.1 General
    Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning"). The categorization of each modification is summarized
    in Figure [6](#S3.F6 "Figure 6 ‣ 3.1.4 Entity Ranking ‣ 3.1 General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning").'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了神经实体链接模型的一般架构在第 [3.1](#S3.SS1 "3.1 一般架构 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习模型的综述")
    节和图 [2](#S1.F2 "图 2 ‣ 1.4 贡献 ‣ 1 引言 ‣ 神经实体链接：基于深度学习模型的综述") 及 [5](#S3.F5 "图 5 ‣
    3.1.3 实体编码 ‣ 3.1 一般架构 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习模型的综述") 中提出的最显著修改和改进。每个修改的分类总结在图
    [6](#S3.F6 "图 6 ‣ 3.1.4 实体排序 ‣ 3.1 一般架构 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习模型的综述") 中。
- en: 3.2.1 Joint Entity Mention Detection and Disambiguation
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 联合实体提及检测与消歧
- en: 'While it is common to separate the mention detection (cf. Equation [2](#S2.E2
    "In 2.2.2 Mention Detection (MD) ‣ 2.2 Formal Definition ‣ 2 Task Description
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")) and entity
    disambiguation stages (cf. Equation [3](#S2.E3 "In 2.2.3 Entity Disambiguation
    (ED) ‣ 2.2 Formal Definition ‣ 2 Task Description ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning")), as illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1.2 Article Collection Methodology ‣ 1 Introduction ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning"), a few systems provide joint solutions
    for entity linking where entity mention detection and disambiguation are done
    at the same time by the same model. Formally, the task becomes to detect a mention
    $m_{\_}i\in M$ and predict an entity $e_{\_}i\in E$ for a given context $c_{\_}i\in
    C$, for all $n$ entity mentions in the context:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通常会将提及检测（参见公式 [2](#S2.E2 "在 2.2.2 提及检测 (MD) ‣ 2.2 正式定义 ‣ 2 任务描述 ‣ 神经实体链接：基于深度学习模型的综述")）和实体消歧阶段（参见公式
    [3](#S2.E3 "在 2.2.3 实体消歧 (ED) ‣ 2.2 正式定义 ‣ 2 任务描述 ‣ 神经实体链接：基于深度学习模型的综述")）分开，如图
    [1](#S1.F1 "图 1 ‣ 1.2 文章收集方法 ‣ 1 引言 ‣ 神经实体链接：基于深度学习模型的综述") 所示，少数系统提供了实体链接的联合解决方案，其中实体提及检测和消歧由同一模型同时完成。正式来说，该任务变为在给定上下文
    $c_{\_}i\in C$ 中检测提及 $m_{\_}i\in M$ 并预测实体 $e_{\_}i\in E$，对所有 $n$ 个实体提及进行处理：
- en: '|  | $\mathsf{EL}:C\xrightarrow{}(M,E)^{n}.$ |  | (18) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{EL}:C\xrightarrow{}(M,E)^{n}.$ |  | (18) |'
- en: Undoubtedly, solving these two problems simultaneously makes the task more challenging.
    However, the interaction between these steps can be beneficial for improving the
    quality of the overall pipeline due to their natural mutual dependency. While
    first competitive models that provide joint solutions were probabilistic graphical
    models Luo et al. ([2015](#bib.bib102)); Nguyen et al. ([2016a](#bib.bib126)),
    we focus on purely neural approaches proposed recently Sorokin and Gurevych ([2018](#bib.bib168));
    Kolitsas et al. ([2018](#bib.bib82)); Peters et al. ([2019](#bib.bib139)); Martins
    et al. ([2019](#bib.bib107)); Broscheit ([2019](#bib.bib17)); Chen et al. ([2020](#bib.bib23));
    Poerner et al. ([2020](#bib.bib142)); De Cao et al. ([2021](#bib.bib33)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，同时解决这两个问题使任务更加具有挑战性。然而，由于这些步骤之间的自然相互依赖，它们的互动可能对提升整体管道的质量有所帮助。尽管最早提供联合解决方案的竞争模型是概率图模型
    Luo 等人 ([2015](#bib.bib102))；Nguyen 等人 ([2016a](#bib.bib126))，我们关注的是最近提出的纯神经网络方法
    Sorokin 和 Gurevych ([2018](#bib.bib168))；Kolitsas 等人 ([2018](#bib.bib82))；Peters
    等人 ([2019](#bib.bib139))；Martins 等人 ([2019](#bib.bib107))；Broscheit ([2019](#bib.bib17))；Chen
    等人 ([2020](#bib.bib23))；Poerner 等人 ([2020](#bib.bib142))；De Cao 等人 ([2021](#bib.bib33))。
- en: 'The main difference of joint models is the necessity to produce also mention
    candidates. For this purpose, Kolitsas et al. ([2018](#bib.bib82)) and Peters
    et al. ([2019](#bib.bib139)) enumerate all spans in a sentence with a certain
    maximum width, filter them by several heuristics (remove mentions with stop words,
    punctuation, ellipses, quotes, and currencies), and try to match them to a pre-built
    index of entities used for the candidate generation. If a mention candidate has
    at least one corresponding entity candidate, it is further treated by a ranking
    neural network that can also discard it by considering it unlinkable to any entity
    in a KG (see Section [3.1.4](#S3.SS1.SSS4 "3.1.4 Entity Ranking ‣ 3.1 General
    Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning")). Therefore, the decision during the entity disambiguation
    phase affects mention detection. In a similar fashion, Sorokin and Gurevych ([2018](#bib.bib168))
    treat each token n-gram up to a certain length as a possible mention candidate.
    They use an additional binary classifier for filtering candidate spans, which
    is trained jointly with an entity linker. Banerjee et al. ([2020](#bib.bib9))
    also enumerates all possible n-grams and expands each of them with candidate entities,
    which results in a long sequence of points corresponding to a candidate entity
    for a particular mention n-gram. This sequence is further processed by a single-layer
    BiLSTM pointer network Vinyals et al. ([2015](#bib.bib183)) that generates index
    numbers of potential entities in the input sequence. Li et al. ([2020](#bib.bib94))
    consider various possible spans as mention candidates and introduce a loss component
    for boundary detection, which is optimized along with the loss for disambiguation.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '联合模型的主要区别在于需要同时生成提及候选项。为此，Kolitsas 等人 ([2018](#bib.bib82)) 和 Peters 等人 ([2019](#bib.bib139))
    列出了句子中所有一定最大宽度的跨度，通过几个启发式规则进行筛选（去除含有停用词、标点符号、省略号、引号和货币符号的提及），并尝试将它们与用于候选生成的预构建实体索引进行匹配。如果一个提及候选项至少有一个对应的实体候选项，则进一步由一个排名神经网络处理，该网络也可以通过考虑无法链接到知识图谱中的任何实体来丢弃该候选项（参见第
    [3.1.4 节](#S3.SS1.SSS4 "3.1.4 Entity Ranking ‣ 3.1 General Architecture ‣ 3 Neural
    Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")）。因此，在实体消歧阶段的决策会影响提及检测。类似地，Sorokin
    和 Gurevych ([2018](#bib.bib168)) 将每个令牌 n-gram（直到一定长度）视为可能的提及候选项。他们使用一个额外的二分类器来筛选候选跨度，该分类器与实体链接器共同训练。Banerjee
    等人 ([2020](#bib.bib9)) 也列出了所有可能的 n-grams，并用候选实体扩展每个 n-gram，这导致了一长串点序列，对应于特定提及
    n-gram 的候选实体。这个序列进一步由一个单层 BiLSTM 指针网络 Vinyals 等人 ([2015](#bib.bib183)) 处理，该网络生成输入序列中潜在实体的索引号。Li
    等人 ([2020](#bib.bib94)) 考虑各种可能的跨度作为提及候选项，并引入了一个边界检测的损失组件，该组件与消歧的损失一起优化。'
- en: Martins et al. ([2019](#bib.bib107)) describe the approach with tighter integration
    between detection and linking phases via multi-task learning. The authors propose
    a stack-based bidirectional LSTM network with a shift-reduce mechanism and attention
    for entity recognition that propagates its internal states to the linker network
    for candidate entity ranking. The linker is supplemented with a NIL predictor
    network. The networks are trained jointly by optimizing the sum of losses from
    all three components.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Martins 等人 ([2019](#bib.bib107)) 通过多任务学习描述了检测和链接阶段之间更紧密集成的方法。作者提出了一种基于堆叠的双向
    LSTM 网络，具有移位-归约机制和用于实体识别的注意力机制，它将其内部状态传播到链接网络中进行候选实体排名。链接器还附加了一个 NIL 预测网络。这些网络通过优化来自所有三个组件的损失总和进行联合训练。
- en: '![Refer to caption](img/711db0e158450a6a9acf4b0bdc295309.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/711db0e158450a6a9acf4b0bdc295309.png)'
- en: 'Figure 7: Global entity disambiguation. The global entity linking resolves
    all mentions simultaneously based on entity coherence. Bolder lines indicate expected
    higher degrees of entity-entity similarity.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：全球实体消歧。全球实体链接基于实体一致性同时解析所有提及。较粗的线条表示实体-实体相似度预期较高。
- en: 'Broscheit ([2019](#bib.bib17)) goes further by suggesting a completely end-to-end
    method that deals with mention detection and linking jointly without explicitly
    executing a candidate generation step. In this work, the EL task is formulated
    as a sequence labeling problem, where each token in the text is assigned an entity
    link or a NIL class. They leverage a sequence tagger based on pre-trained BERT
    for this purpose. This simplistic approach does not supersede Kolitsas et al.
    ([2018](#bib.bib82)) but outperforms the baseline, in which candidate generation,
    mention detection, and linking are performed independently. In the same vein,
    Chen et al. ([2020](#bib.bib23)) use a sequence tagging framework for joint entity
    mention detection and disambiguation. However, they experiment with both settings:
    when a candidate list is available and not, and demonstrate that it is possible
    to achieve high linking performance without candidate sets. Similar to Li et al.
    ([2020](#bib.bib94)), they optimize the joint loss for linking and mention boundary
    detection.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Broscheit ([2019](#bib.bib17)) 进一步提出了一种完全端到端的方法，该方法将提及检测和链接联合处理，而不需要显式地执行候选生成步骤。在这项工作中，实体链接任务被形式化为一个序列标注问题，其中文本中的每个标记被分配一个实体链接或
    NIL 类。他们利用基于预训练 BERT 的序列标注器来实现这一目的。这种简单的方法并未取代 Kolitsas 等人 ([2018](#bib.bib82))
    的方法，但优于基线方法，在基线方法中，候选生成、提及检测和链接是独立进行的。同样，Chen 等人 ([2020](#bib.bib23)) 使用序列标注框架来联合进行实体提及检测和消歧。但他们在有候选列表和没有候选列表的两种情况下进行实验，并证明即使没有候选集合，也可以实现高链接性能。类似于
    Li 等人 ([2020](#bib.bib94))，他们优化了链接和提及边界检测的联合损失。
- en: Poerner et al. ([2020](#bib.bib142)) propose a model E-BERT-MLM, in which they
    repurpose the masked language model (MLM) objective for the selection of entity
    candidates in an end-to-end EL pipeline. The candidate mention spans and candidate
    entity sets are generated in the same way as in Kolitsas et al. ([2018](#bib.bib82)).
    For candidate selection, E-BERT-MLM inserts a special “[E-MASK]” token into the
    text before the considered candidate mention span and tries to restore an entity
    representation for it. The model is trained by minimizing the cross-entropy between
    the generated entity distribution of the potential spans and gold entities. In
    addition to the standard BERT architecture, the model contains a linear transformation
    pre-trained to align entity embeddings with embeddings of word-piece tokens.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Poerner 等人 ([2020](#bib.bib142)) 提出了一个模型 E-BERT-MLM，在该模型中，他们重新利用了掩码语言模型 (MLM)
    的目标用于在端到端的 EL 流水线中选择实体候选。候选提及跨度和候选实体集合的生成方式与 Kolitsas 等人 ([2018](#bib.bib82))
    相同。对于候选选择，E-BERT-MLM 在考虑的候选提及跨度之前向文本中插入一个特殊的 “[E-MASK]” 标记，并尝试为其恢复一个实体表示。通过最小化潜在跨度生成的实体分布与真实实体之间的交叉熵来训练模型。除了标准
    BERT 架构外，该模型还包含一个线性变换，该变换经过预训练以将实体嵌入与词片令牌的嵌入对齐。
- en: De Cao et al. ([2021](#bib.bib33)) recently have proposed a generative approach
    to performing mention detection and disambiguation jointly. Their model, which
    is based on BART Lewis et al. ([2020](#bib.bib93)), performs a sequence-to-sequence
    autoregressive generation of text markup with information about mention spans
    and links to entities in a KG. The generation process is constrained by a markup
    format and a candidate set, which is retrieved from standard pre-built candidate
    resources. Most of the time, the network works in a copy-paste regime when it
    copies input tokens into the output. When it finds a beginning of a mention, the
    model marks it with a square bracket, copies all tokens of a mention, adds a finishing
    square bracket, and generates a link to an entity. Although this approach to EL,
    at the first glance, is counterintuitive and completely different from the solutions
    with a standard bi-encoder architecture, this model achieves near state-of-the-art
    results for joint MD and ED and competitive performances on ED-only benchmarks.
    However, as it is shown in the paper, to achieve such impressive results, the
    model had to be pre-trained on a large annotated Wikipedia-based dataset Wu et al.
    ([2020b](#bib.bib191)). The authors also note that the memory footprint of the
    proposed model is much smaller than that of models based on the standard architecture
    due to no need for storing entity embeddings.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: De Cao 等人 ([2021](#bib.bib33)) 最近提出了一种生成方法，用于联合执行提及检测和消歧义。他们的模型基于 BART Lewis
    等人 ([2020](#bib.bib93))，进行带有提及跨度信息和指向知识图谱实体链接的文本标记的序列到序列自回归生成。生成过程受标记格式和候选集的约束，候选集从标准的预构建资源中检索出来。大多数时候，网络在复制-粘贴模式下工作，当它复制输入标记到输出中时。当找到提及的开始时，模型用方括号标记，复制提及的所有标记，添加结束方括号，并生成指向实体的链接。尽管这种EL方法乍看之下反直觉且与标准双编码器架构的解决方案完全不同，但该模型在联合MD和ED的任务中达到了接近最先进的结果，并在仅ED基准上表现出竞争力。然而，正如论文中所示，为了实现如此令人印象深刻的结果，模型必须在一个大型带注释的基于维基百科的数据集上进行预训练
    Wu 等人 ([2020b](#bib.bib191))。作者还指出，所提模型的内存占用比基于标准架构的模型要小得多，因为不需要存储实体嵌入。
- en: 3.2.2 Global Context Architectures
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 全局上下文架构
- en: 'Two kinds of contextual information are available in entity disambiguation:
    local and global. In local approaches to ED, each mention is disambiguated independently
    based on the surrounding words, as in the following function:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 实体消歧义中有两种上下文信息可用：局部和全局。在局部ED方法中，每个提及是根据周围的词汇独立消歧的，如下函数所示：
- en: '|  | $\mathsf{LED}:(M,C)\xrightarrow{}E.$ |  | (19) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{LED}:(M,C)\xrightarrow{}E.$ |  | (19) |'
- en: 'Global approaches to ED take into account semantic consistency (coherence)
    across multiple entities in a context. In this case, all $q$ entity mentions in
    a group are disambiguated interdependently: a disambiguation decision for one
    entity is affected by decisions made for other entities in a context as illustrated
    in Figure [7](#S3.F7 "Figure 7 ‣ 3.2.1 Joint Entity Mention Detection and Disambiguation
    ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") and Equation [20](#S3.E20
    "In 3.2.2 Global Context Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning").'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 全局ED方法考虑了上下文中多个实体之间的语义一致性（连贯性）。在这种情况下，组中的所有 $q$ 个实体提及是相互依赖地消歧的：一个实体的消歧决策会受到上下文中其他实体的决策的影响，如图
    [7](#S3.F7 "图 7 ‣ 3.2.1 联合实体提及检测和消歧义 ‣ 3.2 一般架构的修改 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习模型的调查")
    和方程 [20](#S3.E20 "在 3.2.2 全局上下文架构 ‣ 3.2 一般架构的修改 ‣ 3 神经实体链接 ‣ 神经实体链接：基于深度学习模型的调查")
    中说明。
- en: '|  | $\mathsf{GED}:((m_{\_}1,m_{\_}2,...,m_{\_}q),C)\xrightarrow{}E^{q}.$ |  |
    (20) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathsf{GED}:((m_{\_}1,m_{\_}2,...,m_{\_}q),C)\xrightarrow{}E^{q}.$ |  |
    (20) |'
- en: 'In the example presented in Figure [7](#S3.F7 "Figure 7 ‣ 3.2.1 Joint Entity
    Mention Detection and Disambiguation ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning"), the consistency score between correct entity candidates: the
    national football team sense of Wales and the Welsh footballer sense of Scott
    Young and John Hartson, is expected to be higher than between incorrect ones.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [7](#S3.F7 "Figure 7 ‣ 3.2.1 Joint Entity Mention Detection and Disambiguation
    ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") 中展示的示例中，正确实体候选之间的一致性评分：威尔士国家足球队的意义与威尔士足球运动员
    Scott Young 和 John Hartson 的意义，预计会高于错误实体之间的一致性评分。'
- en: Besides involving consistency, the considered context of a mention in global
    methods is usually larger than in local ones or even extends to the whole document.
    Although modeling consistency between entities and the extra information of the
    global context improves the disambiguation accuracy, the number of possible entity
    assignments is combinatorial Ganea et al. ([2016](#bib.bib54)), which results
    in high time complexity of disambiguation Yang et al. ([2019](#bib.bib200)); Ganea
    and Hofmann ([2017](#bib.bib53)). Another difficulty is an attempt to assign an
    entity its consistency score since this score is not possible to compute in advance
    due to the simultaneous disambiguation Yamada et al. ([2016](#bib.bib194)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 除了涉及一致性外，全球方法中提到的上下文通常比本地方法中的更大，甚至扩展到整个文档。虽然建模实体间的一致性和全球上下文的额外信息可以提高消歧义准确性，但可能的实体分配数量是组合性的（Ganea
    等，[2016](#bib.bib54)），这导致了消歧义的高时间复杂度（Yang 等，[2019](#bib.bib200)；Ganea 和 Hofmann，[2017](#bib.bib53)）。另一个困难是尝试为实体分配一致性评分，因为由于同时消歧义，这个评分无法预先计算（Yamada
    等，[2016](#bib.bib194)）。
- en: The typical approach to global disambiguation is to generate a graph including
    candidate entities of mentions in a context and perform some graph algorithms,
    like random walk algorithms (e.g. PageRank Page et al. ([1999](#bib.bib133)))
    or graph neural networks, over it to select highly consistent entities Zwicklbauer
    et al. ([2016b](#bib.bib211), [a](#bib.bib210)); Pershina et al. ([2015](#bib.bib137));
    Guo and Barbosa ([2018](#bib.bib61)). Recently, Xue et al. ([2019](#bib.bib192))
    propose a neural recurrent random walk network learning algorithm based on the
    transition matrix of candidate entities containing relevance scores, which are
    created from hyperlinks information and cosine similarity of entities. Cao et al.
    ([2018](#bib.bib20)) construct a subgraph from the candidates of neighbor mentions,
    integrate local and global features of each candidate, and apply a graph convolutional
    network over this subgraph. In this approach, the graph is static, which would
    be problematic in such cases that two mentions would co-occur in different documents
    with different topics, however, the produced graphs will be the same, and so,
    could not catch the different information Wu et al. ([2020a](#bib.bib190)). To
    address it, Wu et al. ([2020a](#bib.bib190)) propose a dynamic graph convolution
    architecture, where entity relatedness scores are computed and updated in each
    layer based on the previous layer information (initialized with some features,
    including context scores) and entity similarity scores. Globerson et al. ([2016](#bib.bib56))
    introduce a model with an attention mechanism that takes into account only the
    subgraph of the target mention, rather than all interactions of all the mentions
    in a document and restrict the number of mentions with an attention.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 全球消歧义的典型方法是生成一个图，包括上下文中提及的候选实体，并对其执行一些图算法，如随机游走算法（例如 PageRank（Page 等，[1999](#bib.bib133)））或图神经网络，以选择高度一致的实体（Zwicklbauer
    等，[2016b](#bib.bib211)，[a](#bib.bib210)；Pershina 等，[2015](#bib.bib137)；Guo 和 Barbosa，[2018](#bib.bib61)）。最近，Xue
    等（[2019](#bib.bib192)）提出了一种基于候选实体转移矩阵的神经递归随机游走网络学习算法，该转移矩阵包含相关性评分，这些评分由超链接信息和实体的余弦相似性创建（Cao
    等，[2018](#bib.bib20)）。此外，Cao 等（[2018](#bib.bib20)）从邻近提及的候选实体中构建一个子图，整合每个候选的本地和全球特征，并在这个子图上应用图卷积网络。在这种方法中，图是静态的，这在两个提及会在不同主题的不同文档中共同出现的情况下会有问题，因为生成的图将是相同的，因此无法捕捉到不同的信息（Wu
    等，[2020a](#bib.bib190)）。为了解决这个问题，Wu 等（[2020a](#bib.bib190)）提出了一种动态图卷积架构，其中实体相关性评分在每一层中基于前一层的信息（初始化时包含一些特征，包括上下文评分）和实体相似性评分进行计算和更新。Globerson
    等（[2016](#bib.bib56)）引入了一种具有注意力机制的模型，该模型仅考虑目标提及的子图，而不是文档中所有提及的所有交互，并通过注意力限制提及的数量。
- en: 'Some works approach global ED by maximizing the Conditional Random Field (CRF)
    potentials, where the first component $\Psi$ represents a local entity-mention
    score, and the other component $\Phi$ measures coherence among selected candidates
    Ganea and Hofmann ([2017](#bib.bib53)); Ganea et al. ([2016](#bib.bib54)); Le
    and Titov ([2018](#bib.bib85), [2019a](#bib.bib86)), as defined in Ganea and Hofmann
    ([2017](#bib.bib53)):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究通过最大化条件随机场（CRF）潜力来接近全球实体识别（ED），其中第一个组件 $\Psi$ 代表局部实体提及得分，另一个组件 $\Phi$ 衡量选定候选项之间的一致性，如
    Ganea 和 Hofmann ([2017](#bib.bib53))；Ganea 等 ([2016](#bib.bib54))；Le 和 Titov ([2018](#bib.bib85)，[2019a](#bib.bib86))
    所定义：
- en: '|  | $g(e,m,c)=\sum_{\_}{i=1}^{n}\Psi(e_{\_}i,m_{\_}i,c_{\_}i)+\sum_{\_}{i<j}\Phi(e_{\_}i,e_{\_}j).$
    |  | (21) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $g(e,m,c)=\sum_{\_}{i=1}^{n}\Psi(e_{\_}i,m_{\_}i,c_{\_}i)+\sum_{\_}{i<j}\Phi(e_{\_}i,e_{\_}j).$
    |  | (21) |'
- en: 'However, model training and its exact inference are NP-hard. Ganea and Hofmann
    ([2017](#bib.bib53)) utilize truncated fitting of loopy belief propagation Ganea
    et al. ([2016](#bib.bib54)); Globerson et al. ([2016](#bib.bib56)) with differentiable
    and trainable message passing iterations using pairwise entity scores to reduce
    the complexity. Le and Titov ([2018](#bib.bib85)) expand it in a way that pairwise
    scores take into account relations of mentions (e.g. located_in, or coreference:
    the mentions are coreferent if they refer to the same entity) by modeling relations
    between mentions as latent variables. Shahbazi et al. ([2018](#bib.bib157)) develop
    a greedy beam search strategy, which starts from a locally optimal initial solution
    and is improved by searching for possible corrections with the focus on the least
    confident mentions.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型训练及其精确推理是 NP 难的。Ganea 和 Hofmann ([2017](#bib.bib53)) 利用截断的循环信念传播拟合方法，Ganea
    等 ([2016](#bib.bib54))；Globerson 等 ([2016](#bib.bib56)) 使用可微分和可训练的消息传递迭代，通过使用成对实体得分来降低复杂性。Le
    和 Titov ([2018](#bib.bib85)) 以一种方式扩展了这一方法，使得成对得分考虑到提及之间的关系（例如，located_in，或共指：如果提及指的是同一个实体，则这些提及是共指的），通过将提及之间的关系建模为潜在变量。Shahbazi
    等 ([2018](#bib.bib157)) 开发了一种贪婪的束搜索策略，该策略从一个局部最优的初始解开始，并通过搜索可能的修正来加以改进，重点是最不确定的提及。
- en: Despite the optimizations proposed like in some aforementioned works, taking
    into account coherence scores among candidates of all mentions at once can be
    prohibitively slow. It also can be malicious due to erroneous coherence among
    wrong entities Fang et al. ([2019](#bib.bib43)). For example, if two mentions
    have coherent erroneous candidates, this noisy information may mislead the final
    global scoring. To resolve this issue, some studies define the global ED problem
    as a sequential decision task, where the disambiguation of new entities is based
    on the already disambiguated ones with high confidence. Fang et al. ([2019](#bib.bib43))
    train a policy network for sequential selection of entities using reinforcement
    learning. The disambiguation of mentions is ordered according to the local score,
    so the mentions with high confident entities are resolved earlier. The policy
    network takes advantage of output from the LSTM global encoder that maintains
    the information about earlier disambiguation decisions. Yang et al. ([2019](#bib.bib200))
    also utilize reinforcement learning for mention disambiguation. They use an attention
    model to leverage knowledge from previously linked entities. The model dynamically
    selects the most relevant entities for the target mention and calculates the coherence
    scores. Yamada et al. ([2021](#bib.bib198)) iteratively predict entities for yet
    unresolved mentions with a BERT model, while attending on the previous most confident
    entity choices. Similarly, Gu et al. ([2021](#bib.bib60)) sort mentions based
    on their ambiguity degrees produced by their BERT-based local model and update
    query/context based on the linked entities so that the next prediction can leverage
    the previous knowledge. They also utilize a gate mechanism to control historical
    cues – representations of linked entities. Yamada et al. ([2016](#bib.bib194))
    and Radhakrishnan et al. ([2018](#bib.bib144)) measure the similarity first based
    on unambiguous mentions and then predict entities for complex cases. Nguyen et al.
    ([2016b](#bib.bib127)) use an RNN to implicitly store information about previously
    seen mentions and corresponding entities. They leverage the hidden states of the
    RNN to reach this information as a feature for the computation of the global score.
    Tsai and Roth ([2016](#bib.bib176)) directly use embeddings of previously linked
    entities as features for the disambiguation model. Recently, Fang et al. ([2020](#bib.bib44))
    combine sequential approaches with graph based methods, where the model dynamically
    changes the graph depending on the current state. The graph is constructed with
    previously resolved entities, current candidate entities, and subsequent mention’s
    candidates. The authors use a graph attention network over this graph to make
    a global scoring. As explained before, Wu et al. ([2020a](#bib.bib190)) also change
    the entity graph dynamically depending on the outputs from previous layers of
    a GCN. Zwicklbauer et al. ([2016b](#bib.bib211)) include to the candidates graph
    a topic node created from the set of already disambiguated entities.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如一些前述工作中提出了优化方法，但考虑到所有提及项候选者的连贯性得分可能会非常缓慢。此外，由于错误实体间的错误连贯性，这种方法也可能存在恶意。Fang
    等人 ([2019](#bib.bib43)) 例如，如果两个提及项有连贯的错误候选项，这些噪声信息可能会误导最终的全局评分。为了解决这个问题，一些研究将全局ED问题定义为一个序列决策任务，其中新实体的消歧基于已高置信度消歧的实体。Fang
    等人 ([2019](#bib.bib43)) 训练了一个用于序列选择实体的策略网络，采用强化学习。消歧按局部得分排序，因此高置信度实体的提及项会更早解决。策略网络利用LSTM全局编码器的输出，保持早期消歧决策的信息。Yang
    等人 ([2019](#bib.bib200)) 也利用强化学习进行提及消歧。他们使用注意力模型从之前链接的实体中获取知识。该模型动态选择目标提及项的最相关实体并计算连贯性得分。Yamada
    等人 ([2021](#bib.bib198)) 通过BERT模型迭代预测尚未解决的提及项的实体，同时关注之前最有信心的实体选择。类似地，Gu 等人 ([2021](#bib.bib60))
    根据BERT基础局部模型产生的歧义度对提及项进行排序，并基于已链接的实体更新查询/上下文，以便下一次预测可以利用之前的知识。他们还利用门机制来控制历史提示——链接实体的表示。Yamada
    等人 ([2016](#bib.bib194)) 和 Radhakrishnan 等人 ([2018](#bib.bib144)) 首先基于明确的提及项测量相似性，然后预测复杂案例的实体。Nguyen
    等人 ([2016b](#bib.bib127)) 使用RNN隐式存储先前见过的提及项及相应实体的信息。他们利用RNN的隐藏状态将这些信息作为全局得分计算的特征。Tsai
    和 Roth ([2016](#bib.bib176)) 直接使用之前链接实体的嵌入作为消歧模型的特征。最近，Fang 等人 ([2020](#bib.bib44))
    将序列方法与图基方法结合，其中模型根据当前状态动态变化图。图的构建包括之前解决的实体、当前候选实体和后续提及项的候选实体。作者在这个图上使用图注意力网络进行全局评分。如前所述，Wu
    等人 ([2020a](#bib.bib190)) 也根据GCN的前一层输出动态改变实体图。Zwicklbauer 等人 ([2016b](#bib.bib211))
    将候选图中包括一个由已消歧实体集合创建的主题节点。
- en: Some studies, for example, Kolitsas et al. ([2018](#bib.bib82)) model the coherence
    component as an additional feed-forward neural network that uses the similarity
    score between the target entity and an average embedding of the candidates with
    a high local score. Fang et al. ([2016](#bib.bib42)) use the similarity score
    between the target entity and its surrounding entity candidates in a specified
    window as a feature for the disambiguation model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究，例如 Kolitsas et al. ([2018](#bib.bib82))，将一致性组件建模为一个额外的前馈神经网络，该网络使用目标实体与具有高局部得分的候选实体的平均嵌入之间的相似度分数。Fang
    et al. ([2016](#bib.bib42)) 将目标实体与其周围实体候选在指定窗口中的相似度分数作为消歧模型的一个特征。
- en: Another approach that can be considered as global is to make use of a document-wide
    context, which usually contains more than one mention and helps to capture the
    coherence implicitly instead of explicitly designing an entity coherence component
    Peters et al. ([2019](#bib.bib139)); Gupta et al. ([2017](#bib.bib62)); Moreno
    et al. ([2017](#bib.bib114)); Francis-Landau et al. ([2016](#bib.bib49)).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以被视为全球的方法是利用文档范围的上下文，这通常包含多个提及，并帮助隐式捕获一致性，而不是显式设计实体一致性组件 Peters et al. ([2019](#bib.bib139));
    Gupta et al. ([2017](#bib.bib62)); Moreno et al. ([2017](#bib.bib114)); Francis-Landau
    et al. ([2016](#bib.bib49))。
- en: 3.2.3 Domain-Independent Architectures
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 域独立架构
- en: Domain independence is one of the most desired properties of EL systems. Annotated
    resources are very limited and exist only for a few domains. Obtaining labeled
    data in a new domain requires much labor. Earlier, this problem is tackled by
    few domain-independent approaches based on unsupervised Wang et al. ([2015](#bib.bib186));
    Cao et al. ([2017](#bib.bib19)); Newman-Griffis et al. ([2018](#bib.bib125)) and
    semi-supervised models Lazic et al. ([2015](#bib.bib84)). Recent studies provide
    solutions based on distant learning and zero-shot methods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 域独立性是 EL 系统最期望的特性之一。标注资源非常有限，仅存在于少数几个领域。在新领域中获得标注数据需要大量的劳动。早期，这一问题由一些基于无监督的方法解决
    Wang et al. ([2015](#bib.bib186)); Cao et al. ([2017](#bib.bib19)); Newman-Griffis
    et al. ([2018](#bib.bib125)) 和半监督模型 Lazic et al. ([2015](#bib.bib84))。最近的研究提供了基于远程学习和零样本方法的解决方案。
- en: Le and Titov ([2019a](#bib.bib86), [b](#bib.bib87)) propose distant learning
    techniques that use only unlabeled documents. They rely on the weak supervision
    coming from a surface matching heuristic, and the EL task is framed as binary
    multi-instance learning. The model learns to distinguish between a set of positive
    entities and a set of random negatives. The positive set is obtained by retrieving
    entities with a high word overlap with the mention and that have relations in
    a KG to candidates of other mentions in the sentence. While showing promising
    performance, which in some cases rivals results of fully supervised systems, these
    approaches require either a KG describing relations of entities Le and Titov ([2019b](#bib.bib87))
    or mention-entity priors computed from entity hyperlink statistics extracted from
    Wikipedia Le and Titov ([2019a](#bib.bib86)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Le 和 Titov ([2019a](#bib.bib86), [b](#bib.bib87)) 提出了仅使用未标记文档的远程学习技术。他们依赖于来自表面匹配启发式的弱监督，并将
    EL 任务框定为二元多实例学习。该模型学习区分一组正面实体和一组随机负面实体。正面集合是通过检索与提及词重叠度高且在知识图谱中与句子中的其他提及候选实体有关系的实体获得的。尽管表现出有前景的性能，在某些情况下与完全监督系统的结果相媲美，但这些方法要么需要描述实体关系的知识图谱
    Le 和 Titov ([2019b](#bib.bib87))，要么需要从维基百科提取的实体超链接统计计算的提及实体先验 Le 和 Titov ([2019a](#bib.bib86))。
- en: Recently proposed zero-shot techniques Logeswaran et al. ([2019](#bib.bib100));
    Wu et al. ([2020b](#bib.bib191)); Yao et al. ([2020](#bib.bib201)); Tang et al.
    ([2021](#bib.bib173)) tackle problems related to adapting EL systems to new domains.
    In the zero-shot setting, the only entity information available is its description.
    As well as in other settings, texts with mention-entity pairs are also available.
    The key idea of zero-shot methods is to train an EL system on a domain with rich
    labeled data resources and apply it to a new domain with only minimal available
    data like descriptions of domain-specific entities. One of the first studies that
    proposes such a technique is Gupta et al. ([2017](#bib.bib62)) (not purely zero-shot
    because they also use entity typings). Existing zero-shot systems do not require
    such information resources as surface form dictionaries, prior entity-mention
    probabilities, KG entity relations, and entity typing, which makes them particularly
    suited for building domain-independent solutions. However, the limitation of information
    sources raises several challenges.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的零样本技术 Logeswaran 等人 ([2019](#bib.bib100)); Wu 等人 ([2020b](#bib.bib191));
    Yao 等人 ([2020](#bib.bib201)); Tang 等人 ([2021](#bib.bib173)) 解决了将 EL 系统适应于新领域的问题。在零样本设置中，唯一可用的实体信息是其描述。与其他设置一样，也提供了包含提及-实体对的文本。零样本方法的关键思想是先在具有丰富标注数据资源的领域上训练
    EL 系统，然后将其应用于仅有最少可用数据（如领域特定实体的描述）的新领域。其中一个首次提出这种技术的研究是 Gupta 等人 ([2017](#bib.bib62))（不是纯粹的零样本，因为他们还使用实体类型）。现有的零样本系统不需要诸如表面形式词典、先验实体-提及概率、知识图谱实体关系和实体类型等信息资源，这使得它们特别适合构建领域独立的解决方案。然而，信息来源的限制带来了若干挑战。
- en: 'Since only textual descriptions of entities are available for the target domain,
    one cannot rely on pre-built dictionaries for candidate generation. All zero-shot
    works rely on the same strategy to tackle candidate generation: pre-compute representations
    of entity descriptions (sometimes referred to as caching), compute a representation
    of a mention, and calculate its similarity with all the description representations.
    Pre-computed representations of descriptions save a lot of time at the inference
    stage. Particularly, Logeswaran et al. ([2019](#bib.bib100)) use the BM25 information
    retrieval formula Jones et al. ([2000](#bib.bib78)), which is a similarity function
    for count-based representations.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标领域只有实体的文本描述，因此无法依赖预构建的词典来生成候选项。所有零样本方法都依赖于相同的策略来处理候选生成：预计算实体描述的表示（有时称为缓存），计算提及的表示，并计算其与所有描述表示的相似度。预计算的描述表示在推理阶段节省了大量时间。特别是，Logeswaran
    等人 ([2019](#bib.bib100)) 使用了 BM25 信息检索公式 Jones 等人 ([2000](#bib.bib78))，这是一种用于基于计数的表示的相似性函数。
- en: A natural extension of count-based approaches is embeddings. The method proposed
    by Gillick et al. ([2019](#bib.bib55)), which is a predecessor of zero-shot approaches,
    uses average unigram and bigram embeddings followed by dense layers to obtain
    representations of mentions and descriptions. The only aspect that separates this
    approach from pure zero-shot techniques is the usage of entity categories along
    with descriptions to build entity representations. Cosine similarity is used for
    the comparison of representations. Due to the computational simplicity of this
    approach, it can be used in a single stage fashion where candidate generation
    and ranking are identical. For further speedup, it is possible to make this algorithm
    two-staged. In the first stage, an approximate search can be used for candidate
    set retrieval. In the second stage, the retrieved smaller set can be used for
    exact similarity computation. Instead of simple embeddings, Wu et al. ([2020b](#bib.bib191))
    suggest using a BERT-based bi-encoder for candidate generation. Two separate encoders
    generate representations of mentions and entity descriptions. Similar to the previous
    work, the candidate selection is based on the score obtained via a dot-product
    of mention/entity representations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 计数基础方法的自然扩展是嵌入（embeddings）。Gillick 等人提出的方法（[2019](#bib.bib55)），作为零样本方法的前身，使用平均的
    unigram 和 bigram 嵌入，然后通过密集层获得提及和描述的表示。唯一将这种方法与纯零样本技术区分开来的方面是使用实体类别以及描述来构建实体表示。余弦相似度用于比较表示。由于这种方法的计算简单性，它可以以单阶段的方式使用，其中候选生成和排名是相同的。为了进一步加速，可以将该算法分为两个阶段。在第一阶段，可以使用近似搜索来检索候选集合。在第二阶段，可以使用检索到的较小集合进行精确相似度计算。Wu
    等人（[2020b](#bib.bib191)）建议使用基于 BERT 的双编码器来进行候选生成，而不是简单的嵌入。两个独立的编码器生成提及和实体描述的表示。与之前的工作类似，候选选择是基于通过提及/实体表示的点积获得的得分。
- en: For entity ranking, a very simple embedding-based approach of Gillick et al.
    ([2019](#bib.bib55)) described above shows very competitive scores on the TAC
    KBP-2010 benchmark, outperforming some complex neural architectures. The recent
    studies of Logeswaran et al. ([2019](#bib.bib100)) and Wu et al. ([2020b](#bib.bib191))
    utilize a BERT-based cross-encoder to perform joint encoding of mentions and entities.
    The cross-encoder takes a concatenation of a context with a mention and an entity
    description to produce a scalar score for each candidate. The cross-attention
    helps to leverage the semantic information from the context and the definition
    on each layer of the encoder network Humeau et al. ([2020](#bib.bib71)); Reimers
    and Gurevych ([2019](#bib.bib150)). In both studies, cross-encoders achieve superior
    results compared to bi-encoders and count-based approaches. For entity linking,
    cross-attention between mention context representations and entity descriptions
    is also used by Nie et al. ([2018](#bib.bib129)). However, they leverage recurrent
    architectures for encoding. Yao et al. ([2020](#bib.bib201)) introduce a small
    tweak of positional embeddings in the Logeswaran et al. ([2019](#bib.bib100))’s
    architecture aimed at better handling long contexts. Tang et al. ([2021](#bib.bib173))
    address the problem of the limited size of the mention context and the entity
    description that could be processed by the standard BERT model. They argue that
    the input size of 512 tokens is not enough to capture context and entity description
    relatedness since the evidence for linking could scatter in different paragraphs
    and suggest a novel architecture that resolves this problem. Roughly speaking,
    their model splits the context of a mention and entity description into multiple
    paragraphs, performs cross-attention between representations of these paragraphs,
    and aggregates the results for disambiguation. The experimental results show that
    their model substantially improves the zero-shot performance keeping the inference
    time in an acceptable range.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实体排名，Gillick 等人 ([2019](#bib.bib55)) 描述的一种非常简单的基于嵌入的方法在 TAC KBP-2010 基准测试中表现出非常具有竞争力的分数，超越了一些复杂的神经网络架构。Logeswaran
    等人 ([2019](#bib.bib100)) 和 Wu 等人 ([2020b](#bib.bib191)) 的最新研究利用基于 BERT 的交叉编码器来对提及和实体进行联合编码。交叉编码器将上下文与提及和实体描述进行拼接，以产生每个候选项的标量分数。交叉注意力有助于利用每层编码器网络
    Humeau 等人 ([2020](#bib.bib71)) 和 Reimers 和 Gurevych ([2019](#bib.bib150)) 上下文和定义中的语义信息。在这两项研究中，交叉编码器相比于双编码器和基于计数的方法取得了更优的结果。对于实体链接，Nie
    等人 ([2018](#bib.bib129)) 也使用了提及上下文表示与实体描述之间的交叉注意力。然而，他们利用递归架构进行编码。Yao 等人 ([2020](#bib.bib201))
    在 Logeswaran 等人 ([2019](#bib.bib100)) 的架构中引入了对位置嵌入的小调整，旨在更好地处理长上下文。Tang 等人 ([2021](#bib.bib173))
    解决了标准 BERT 模型能够处理的提及上下文和实体描述的有限大小问题。他们认为512个标记的输入大小不足以捕捉上下文和实体描述的相关性，因为链接的证据可能散布在不同的段落中，并建议了一种新的架构来解决这个问题。粗略来说，他们的模型将提及和实体描述的上下文拆分成多个段落，对这些段落的表示进行交叉注意力，并汇总结果以进行消歧。实验结果表明，他们的模型在保持推理时间在可接受范围内的同时，显著提高了零-shot
    性能。
- en: Evaluation of zero-shot systems requires data from different domains. Logeswaran
    et al. ([2019](#bib.bib100)) proposes the Zero-shot EL^(12)^(12)12[https://github.com/lajanugen/zeshel](https://github.com/lajanugen/zeshel)
    dataset, constructed from several Wikias^(13)^(13)13[https://www.wikia.com](https://www.wikia.com).
    In the proposed setting, training is performed on one set of Wikias while evaluation
    is performed on others. Gillick et al. ([2019](#bib.bib55)) construct the Wikinews
    dataset. This dataset can be used for evaluation after training on Wikipedia data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 评估零-shot 系统需要来自不同领域的数据。Logeswaran 等人 ([2019](#bib.bib100)) 提出了 Zero-shot EL^(12)^(12)12[https://github.com/lajanugen/zeshel](https://github.com/lajanugen/zeshel)
    数据集，该数据集由多个 Wikias^(13)^(13)13[https://www.wikia.com](https://www.wikia.com) 构建。在提出的设置中，训练在一个
    Wikias 数据集上进行，而评估在其他数据集上进行。Gillick 等人 ([2019](#bib.bib55)) 构建了 Wikinews 数据集。该数据集可以在训练完成后用于评估。
- en: Clearly, heavy neural architectures pre-trained on general-purpose open corpora
    substantially advance the performance of zero-shot techniques. As highlighted
    by Logeswaran et al. ([2019](#bib.bib100)) further unsupervised pre-training on
    source data, as well as on the target data is beneficial. The development of better
    approaches to the utilization of unlabeled data might be a fruitful research direction.
    Furthermore, closing the performance gap of entity ranking between a fast representation
    based bi-encoder and a computationally intensive cross-encoder is an open question.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，经过在通用开放语料上预训练的复杂神经网络架构大大提升了零-shot 技术的性能。Logeswaran 等人（[2019](#bib.bib100)）指出，在源数据和目标数据上进一步进行无监督预训练是有益的。开发更好的利用无标注数据的方法可能是一个有前景的研究方向。此外，快速表示基础的双编码器和计算密集型的交叉编码器之间的实体排名性能差距仍然是一个悬而未决的问题。
- en: 3.2.4 Cross-lingual Architectures
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 跨语言架构
- en: An abundance of labeled data for EL in English contrasts with the amount of
    data available in other languages. The cross-lingual EL (sometimes called XEL)
    methods Ji et al. ([2015](#bib.bib76)) aim at overcoming the lack of annotation
    for resource-poor languages by leveraging supervision coming from their resource-rich
    counterparts. Many of these methods are feasible due to the presence of a unique
    source of supervision for EL – Wikipedia, which is available for a variety of
    languages. The inter-language links in Wikipedia that map pages in one language
    to equivalent pages in another language also help to map corresponding entities
    in different languages.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 英语中的标注数据丰富，而其他语言中的标注数据则相对稀少。Ji 等人（[2015](#bib.bib76)）提出的跨语言实体链接（有时称为 XEL）方法旨在通过利用来自资源丰富语言的监督来克服资源贫乏语言的标注不足。许多这些方法之所以可行，是因为存在一种独特的实体链接监督来源——维基百科，提供了多种语言的内容。维基百科中的跨语言链接将一种语言的页面映射到另一种语言的等效页面，也有助于映射不同语言中的对应实体。
- en: Challenges in XEL start at candidate generation and mention detection steps
    since a resource-poor language can lack mappings between mention strings and entities.
    In addition to the standard mention-entity priors based on inter-language links
    Tsai and Roth ([2016](#bib.bib176)); Sil et al. ([2018](#bib.bib164)); Upadhyay
    et al. ([2018a](#bib.bib179)), candidate generation can be approached by mining
    a translation dictionary Pan et al. ([2017](#bib.bib134)), training a translation
    and alignment model Tsai and Roth ([2018](#bib.bib177)); Upadhyay et al. ([2018b](#bib.bib180)),
    or applying a neural character-level string matching model Rijhwani et al. ([2019](#bib.bib151));
    Zhou et al. ([2019](#bib.bib207)). In the latter approach, the model is trained
    to match strings from a high-resource pivot language to strings in English. If
    a high-resource pivot language is similar to the target low-resource one, such
    a model is able to produce reasonable candidates for the latter. The neural string
    matching approach can be further improved with simpler average n-gram encoding
    and extending entity-entity pairs with mention-entity examples Zhou et al. ([2020](#bib.bib208)).
    Such an approach can also be applied to entity recognition Cotterell and Duh ([2017](#bib.bib31)).
    Fu et al. ([2020](#bib.bib50)) criticize methods that solely rely on Wikipedia
    due to the lack of inter-language links for resource-poor languages. They propose
    a candidate generation method that leverages results from querying online search
    engines (Google and Google Maps) and show that due to its much higher recall compared
    to other methods, it is possible to substantially increase the performance of
    XEL.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: XEL 的挑战始于候选生成和提及检测步骤，因为资源贫乏语言可能缺乏提及字符串和实体之间的映射。除了基于跨语言链接的标准提及-实体先验 Tsai 和 Roth（[2016](#bib.bib176)）；Sil
    等人（[2018](#bib.bib164)）；Upadhyay 等人（[2018a](#bib.bib179)），候选生成还可以通过挖掘翻译词典 Pan
    等人（[2017](#bib.bib134)）、训练翻译和对齐模型 Tsai 和 Roth（[2018](#bib.bib177)）；Upadhyay 等人（[2018b](#bib.bib180)），或应用神经字符级字符串匹配模型
    Rijhwani 等人（[2019](#bib.bib151)）；Zhou 等人（[2019](#bib.bib207)）来实现。在后者的方法中，模型被训练以将高资源中转语言的字符串匹配到英语字符串。如果高资源中转语言与目标低资源语言相似，则该模型能够为后者生成合理的候选。神经字符串匹配方法可以通过更简单的平均
    n-gram 编码和扩展实体-实体对与提及-实体示例 Zhou 等人（[2020](#bib.bib208)）进一步改进。这种方法也可以应用于实体识别 Cotterell
    和 Duh（[2017](#bib.bib31)）。Fu 等人（[2020](#bib.bib50)）批评了仅依赖维基百科的方法，因为资源贫乏语言缺乏跨语言链接。他们提出了一种利用在线搜索引擎（Google
    和 Google Maps）查询结果的候选生成方法，并显示由于其较高的召回率，相比其他方法，它可以显著提高 XEL 的性能。
- en: There are several approaches to candidate ranking that take advantage of cross-lingual
    data for dealing with the lack of annotated examples. Pan et al. ([2017](#bib.bib134))
    use the Abstract Meaning Representation (AMR) Banarescu et al. ([2013](#bib.bib8))
    statistics in English Wikipedia and mention context for ranking. To train an AMR
    tagger, pseudo-labeling Lee ([2013](#bib.bib89)) is used. Tsai and Roth ([2016](#bib.bib176))
    train monolingual embeddings for words and entities jointly by replacing every
    entity mention with corresponding entity tokens. Using the inter-language links,
    they learn the projection functions from multiple languages into the English embedding
    space. For ranking, context embeddings are averaged, projected into the English
    space, and compared with entity embeddings. The authors demonstrate that this
    approach helps to build better entity representations and boosts the EL accuracy
    in the cross-lingual setting by more than 1% for Spanish and Chinese. Sil et al.
    ([2018](#bib.bib164)) propose a method for zero-shot transfer from a high-resource
    language. The authors extend the previous approach with the least squares objective
    for embedding projection learning, the CNN context encoder, and a trainable re-weighting
    of each dimension of context and entity representations. The proposed approach
    demonstrates improved performance as compared to previous non-zero-shot approaches.
    Upadhyay et al. ([2018a](#bib.bib179)) argues that the success of zero-shot cross-lingual
    approaches Tsai and Roth ([2016](#bib.bib176)); Sil et al. ([2018](#bib.bib164))
    might be largely originating from a better estimation of mention-entity prior
    probabilities. Their approach extends Sil et al. ([2018](#bib.bib164)) with global
    context information and incorporation of typing information into context and entity
    representations (the system learns to predict typing during the training). The
    authors report a significant drop in performance for zero-shot cross-lingual EL
    without mention-entity priors, while showing state-of-the-art results with priors.
    They also show that training on a resource-rich language might be very beneficial
    for low-resource settings.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned techniques of cross-lingual entity linking heavily rely on
    pre-trained multilingual embeddings for entity ranking. While being effective
    in settings with at least prior probabilities available, the performance in realistic
    zero-shot scenarios drops drastically. Along with the recent success of the zero-shot
    multilingual transfer of large pre-trained language models, this is a motivation
    to utilize powerful multilingual self-supervised models. Botha et al. ([2020](#bib.bib16))
    use the zeros-shot monolingual architecture of Wu et al. ([2020b](#bib.bib191));
    Logeswaran et al. ([2019](#bib.bib100)) and mBERT Pires et al. ([2019](#bib.bib141))
    to build a massively multilingual EL model for more than 100 languages. Their
    system effectively selects proper entities among almost 20 million of candidates
    using a bi-encoder, hard negative mining, and an additional cross-lingual entity
    description retrieval task. The biggest improvements over the baselines are achieved
    in the zero-shot and few-shot settings, which demonstrates the benefits of training
    on a large amount of multilingual data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Methods that do not Fit the General Architecture
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a few works that propose methods not fitting the general architecture
    presented in Figures [2](#S1.F2 "Figure 2 ‣ 1.4 Contributions ‣ 1 Introduction
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning") and [5](#S3.F5
    "Figure 5 ‣ 3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").
    Raiman and Raiman ([2018](#bib.bib146)) rely on the intermediate supplementary
    task of entity typing instead of directly performing entity disambiguation. They
    learn a type system in a KG and train an intermediate type classifier of mentions
    that significantly refines the number of candidates for the final linking model.
    Onoe and Durrett ([2020](#bib.bib131)) leverage distant supervision from Wikipedia
    pages and the Wikipedia category system to train a fine-grained entity typing
    model. At test time, they use the soft type predictions and the information about
    candidate types derived from Wikipedia to perform the final disambiguation. The
    authors claim that such an approach helps to improve the domain independence of
    their EL system. Kar et al. ([2018](#bib.bib80)) consider a classification approach,
    where each entity is considered as a separate class or a task. They show that
    the straightforward classification is difficult due to exceeding memory requirements.
    Therefore, they experiment with multitask learning, where parameter learning is
    decomposed into solving groups of tasks. Globerson et al. ([2016](#bib.bib56))
    do not have any encoder components; instead, they rely on contextual and pairwise
    feature-based scores. They have an attention mechanism for global ED with a non-linear
    optimization as described in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Global Context
    Architectures ‣ 3.2 Modifications of the General Architecture ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Summary
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 2: Features of neural EL models. Neural entity linking models compared
    according to their architectural features. The description of columns is presented
    in the [beginning of Section 3.4](#tablecolumns "Each column ‣ 3.4 Summary ‣ 3
    Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning"). The footnotes in the table are enumerated in the [end](#tablefootnotes
    "In addition ‣ 3.4 Summary ‣ 3 Neural Entity Linking ‣ Neural Entity Linking:
    A Survey of Models Based on Deep Learning") of Section [3.4](#S3.SS4 "3.4 Summary
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning").'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '&#124; Encoder Type &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '| Global |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '&#124; MD+ &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ED &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NIL &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pred. &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ent. Encoder &#124;'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Source based on &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Candidate &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generation &#124;'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning Type &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for Disam. &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cross- &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; lingual &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| [Sun et al.](#bib.bib171) ([2015](#bib.bib171)) Sun et al. ([2015](#bib.bib171))
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN+Tensor net. &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | ent. specific info. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '&#124; surface match+aliases &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| [Francis-Landau et al.](#bib.bib49) ([2016](#bib.bib49)) Francis-Landau et al.
    ([2016](#bib.bib49)) | CNN | ✘³ |  | ✘ | ent. specific info. | surface match+prior
    | supervised |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib42) ([2016](#bib.bib42)) Fang et al. ([2016](#bib.bib42))
    | word2vec-based | ✘ |  |  | relational info. | n/a | supervised |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib194) ([2016](#bib.bib194)) Yamada et al. ([2016](#bib.bib194))
    | word2vec-based | ✘ |  |  | relational info. | aliases | supervised |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| [Zwicklbauer et al.](#bib.bib211) ([2016b](#bib.bib211)) Zwicklbauer et al.
    ([2016b](#bib.bib211)) | word2vec-based | ✘ |  | ✘ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '&#124; unstructured text + &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ent. specific info. &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; surface match &#124;'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '| unsupervised⁵ |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| [Tsai and Roth](#bib.bib176) ([2016](#bib.bib176)) Tsai and Roth ([2016](#bib.bib176))
    | word2vec-based | ✘ |  | ✘ | unstructured text | prior | supervised | ✘ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| [Nguyen et al.](#bib.bib127) ([2016b](#bib.bib127)) Nguyen et al. ([2016b](#bib.bib127))
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '| ✘ |  | ✘ | ent. specific info. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: '&#124; surface match+prior &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
- en: '| [Globerson et al.](#bib.bib56) ([2016](#bib.bib56)) Globerson et al. ([2016](#bib.bib56))
    | n/a | ✘ |  |  | n/a | prior+aliases | supervised |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib19) ([2017](#bib.bib19)) Cao et al. ([2017](#bib.bib19))
    | word2vec-based | ✘ |  |  | relational info. | aliases |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '&#124; supervised or &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unsupervised &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| [Eshel et al.](#bib.bib40) ([2017](#bib.bib40)) Eshel et al. ([2017](#bib.bib40))
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '&#124; GRU+Atten. &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | unstructured text¹ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '&#124; aliases or surface match &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| [Ganea and Hofmann](#bib.bib53) ([2017](#bib.bib53)) Ganea and Hofmann ([2017](#bib.bib53))
    | Atten. | ✘ |  |  | unstructured text |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| [Moreno et al.](#bib.bib114) ([2017](#bib.bib114)) Moreno et al. ([2017](#bib.bib114))
    | word2vec-based | ✘³ |  | ✘ | unstructured text |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '&#124; surface match+aliases &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| [Gupta et al.](#bib.bib62) ([2017](#bib.bib62)) Gupta et al. ([2017](#bib.bib62))
    | LSTM | ✘³ |  |  | ent. specific info. | prior | supervised⁴ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| [Nie et al.](#bib.bib129) ([2018](#bib.bib129)) Nie et al. ([2018](#bib.bib129))
    | LSTM+CNN | ✘ |  |  | ent. specific info. | surface match+prior | supervised
    |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| [Sorokin and Gurevych](#bib.bib168) ([2018](#bib.bib168)) Sorokin and Gurevych
    ([2018](#bib.bib168)) | CNN | ✘ | ✘ |  | relational info. | surface match | supervised
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib157) ([2018](#bib.bib157)) Shahbazi et al. ([2018](#bib.bib157))
    | Atten. | ✘ |  |  | unstructured text | prior+aliases | supervised |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib85) ([2018](#bib.bib85)) Le and Titov ([2018](#bib.bib85))
    | Atten. | ✘ |  |  | unstructured text | prior+aliases | supervised |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| [Newman-Griffis et al.](#bib.bib125) ([2018](#bib.bib125)) Newman-Griffis
    et al. ([2018](#bib.bib125)) | word2vec-based |  |  |  | unstructured text | aliases
    | unsupervised |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| [Radhakrishnan et al.](#bib.bib144) ([2018](#bib.bib144)) Radhakrishnan et al.
    ([2018](#bib.bib144)) | n/a | ✘ |  |  | relational info. | aliases | supervised
    |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| [Kolitsas et al.](#bib.bib82) ([2018](#bib.bib82)) Kolitsas et al. ([2018](#bib.bib82))
    | LSTM | ✘ | ✘ |  | unstructured text |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| [Sil et al.](#bib.bib164) ([2018](#bib.bib164)) Sil et al. ([2018](#bib.bib164))
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '&#124; LSTM+Tensor net. &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | ✘ | ent. specific info. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '&#124; prior or &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prior+aliases &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; zero-shot &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '| ✘ |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| [Upadhyay et al.](#bib.bib179) ([2018a](#bib.bib179)) Upadhyay et al. ([2018a](#bib.bib179))
    | CNN | ✘³ |  |  | ent. specific info. | prior |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '&#124; zero-shot &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '| ✘ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib20) ([2018](#bib.bib20)) Cao et al. ([2018](#bib.bib20))
    | Atten. | ✘ |  |  | relational info. | prior+aliases | supervised |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| [Raiman and Raiman](#bib.bib146) ([2018](#bib.bib146)) Raiman and Raiman
    ([2018](#bib.bib146)) | n/a | ✘ |  |  | n/a |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+type classifier &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised | ✘ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| [Mueller and Durrett](#bib.bib116) ([2018](#bib.bib116)) Mueller and Durrett
    ([2018](#bib.bib116)) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '&#124; GRU+Atten.+CNN &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | unstructured text¹ | surface match | supervised |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib158) ([2019](#bib.bib158)) Shahbazi et al. ([2019](#bib.bib158))
    | ELMo |  |  |  | unstructured text |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or aliases &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| [Logeswaran et al.](#bib.bib100) ([2019](#bib.bib100)) Logeswaran et al.
    ([2019](#bib.bib100)) | BERT |  |  |  | ent. specific info. | BM25 | zero-shot
    |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| [Gillick et al.](#bib.bib55) ([2019](#bib.bib55)) Gillick et al. ([2019](#bib.bib55))
    | FFNN |  |  |  | ent. specific info. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '&#124; nearest neighbors &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised⁴ |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| [Peters et al.](#bib.bib139) ([2019](#bib.bib139)) Peters et al. ([2019](#bib.bib139))²
    | BERT | ✘³ | ✘ | ✘ | unstructured text |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib87) ([2019b](#bib.bib87)) Le and Titov ([2019b](#bib.bib87))
    | LSTM |  |  |  | ent. specific info. | surface match |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '&#124; weakly- &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; supervised &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib86) ([2019a](#bib.bib86)) Le and Titov ([2019a](#bib.bib86))
    | Atten. | ✘ |  |  | unstructured text |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; weakly- &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; supervised &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib43) ([2019](#bib.bib43)) Fang et al. ([2019](#bib.bib43))
    | LSTM | ✘ |  |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '&#124; unstructured text + &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ent. specific info. &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '| aliases | supervised |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| [Martins et al.](#bib.bib107) ([2019](#bib.bib107)) Martins et al. ([2019](#bib.bib107))
    | LSTM |  | ✘ | ✘ | unstructured text | aliases | supervised |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| [Yang et al.](#bib.bib200) ([2019](#bib.bib200)) Yang et al. ([2019](#bib.bib200))
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '&#124; Atten. or CNN &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '| ✘ |  |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '&#124; unstructured text or &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ent. specific. info. &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '| prior+aliases | supervised |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| [Xue et al.](#bib.bib192) ([2019](#bib.bib192)) Xue et al. ([2019](#bib.bib192))
    | CNN | ✘ |  |  | ent. specific info. | prior+aliases | supervised |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| [Zhou et al.](#bib.bib207) ([2019](#bib.bib207)) Zhou et al. ([2019](#bib.bib207))
    | n/a | ✘ |  |  | unstructured text |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+char.- &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; level model &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '| zero-shot | ✘ |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| [Broscheit](#bib.bib17) ([2019](#bib.bib17)) Broscheit ([2019](#bib.bib17))
    | BERT |  | ✘ | ✘ | n/a | n/a | supervised |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| [Hou et al.](#bib.bib69) ([2020](#bib.bib69)) Hou et al. ([2020](#bib.bib69))
    | Atten. | ✘ |  |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '&#124; ent. specific info.+ &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unstructured text &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '| prior+aliases | supervised |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| [Onoe and Durrett](#bib.bib131) ([2020](#bib.bib131)) Onoe and Durrett ([2020](#bib.bib131))
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '&#124; ELMo+Atten. &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +CNN+LSTM &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | n/a |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '&#124; prior or aliases &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised⁴ |  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib23) ([2020](#bib.bib23)) Chen et al. ([2020](#bib.bib23))
    | BERT |  | ✘ |  | relational info. | n/a or aliases | supervised |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib191) ([2020b](#bib.bib191)) Wu et al. ([2020b](#bib.bib191))
    | BERT |  |  |  | ent. specific info. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '&#124; nearest neighbors &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; zero-shot &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| [Banerjee et al.](#bib.bib9) ([2020](#bib.bib9)) Banerjee et al. ([2020](#bib.bib9))
    | fastText |  | ✘ |  | relational info. | surface match | supervised |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib190) ([2020a](#bib.bib190)) Wu et al. ([2020a](#bib.bib190))
    | ELMo | ✘ |  |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '&#124; unstructured text+ &#124;'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; relational info. &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prior+aliases &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib44) ([2020](#bib.bib44)) Fang et al. ([2020](#bib.bib44))
    | BERT | ✘ |  |  | ent. specific info. |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '&#124; surface match+aliases+ &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Google Search &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib25) ([2020](#bib.bib25)) Chen et al. ([2020](#bib.bib25))
    | Atten.+BERT | ✘ |  |  | unstructured text |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| [Botha et al.](#bib.bib16) ([2020](#bib.bib16)) Botha et al. ([2020](#bib.bib16))
    | BERT |  |  |  | ent. specific info. | nearest neighbors | zero-shot | ✘ |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| [Yao et al.](#bib.bib201) ([2020](#bib.bib201)) Yao et al. ([2020](#bib.bib201))
    | BERT |  |  |  | ent. specific info. | BM25 | zero-shot |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| [Li et al.](#bib.bib94) ([2020](#bib.bib94)) Li et al. ([2020](#bib.bib94))
    | BERT |  | ✘ |  | ent. specific info. | nearest neighbors | zero-shot |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| [Poerner et al.](#bib.bib142) ([2020](#bib.bib142)) Poerner et al. ([2020](#bib.bib142))²
    | BERT | ✘ | ✘ | ✘ | relational info. | prior+aliases | supervised |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| [Fu et al.](#bib.bib50) ([2020](#bib.bib50)) Fu et al. ([2020](#bib.bib50))
    | M-BERT |  |  |  | ent. specific info. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '&#124; Google Search &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Google Maps &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '| zero-shot | ✘ |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| [Mulang’ et al.](#bib.bib117) ([2020](#bib.bib117)) Mulang’ et al. ([2020](#bib.bib117))
    |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '&#124; Atten. or CNN or BERT &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '| ✘ |  |  | relational info. | prior+aliases | supervised |  |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib198) ([2021](#bib.bib198)) Yamada et al. ([2021](#bib.bib198))
    | BERT | ✘ |  |  | unstructured text |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '&#124; prior+aliases &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or aliases &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| [Gu et al.](#bib.bib60) ([2021](#bib.bib60)) Gu et al. ([2021](#bib.bib60))
    | BERT | ✘ |  | ✘ | ent. specific info. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '&#124; surface match+prior &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or aliases &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '| supervised |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| [Tang et al.](#bib.bib173) ([2021](#bib.bib173)) Tang et al. ([2021](#bib.bib173))
    | BERT |  |  |  | ent. specific info. | BM25 | zero-shot |  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| [De Cao et al.](#bib.bib33) ([2021](#bib.bib33)) De Cao et al. ([2021](#bib.bib33))
    | BART | ✘ | ✘ |  | n/a | prior+aliases | supervised |  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: 'We summarize design features for neural EL models in Table [2](#S3.T2 "Table
    2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of
    Models Based on Deep Learning") and also links to their publicly available implementations
    in Table [7](#A1.T7 "Table 7 ‣ Appendix A Public Implementations of Neural Entity
    Linking Models ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")
    in Appendix A. The mention encoders have made a shift to self-attention architectures
    and started using deep pre-trained models like BERT. The majority of studies still
    rely on external knowledge for the candidate generation step. There is a surge
    of models that tackle the domain adaptation problem in a zero-shot fashion. However,
    the task of zero-shot joint entity mention detection and linking has not been
    addressed yet. It is shown in several works that the cross-encoder architecture
    is superior compared to models with separate mention and entity encoders. The
    global context is widely used, but there are few recent studies that focus only
    on local EL.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Each column in Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning") corresponds
    to a model feature. The encoder type column presents the architecture of the mention
    encoder of the neural entity linking model. It contains the following options:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n/a – a model does not have a neural encoder for mentions / contexts.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN – an encoder based on convolutional layers (usually with pooling).
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor net. – an encoder that uses a tensor network.
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atten. – means that a context-mention encoder leverages an attention mechanism
    to highlight the part of the context using an entity candidate.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRU – an encoder based on a recurrent neural network and gated recurrent units
    Chung et al. ([2014](#bib.bib29)).
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM – an encoder based on a recurrent neural network and long short-term memory
    cells Hochreiter and Schmidhuber ([1997](#bib.bib66)) (might be also bidirectional).
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FFNN – an encoder based on a simple feedforward neural network.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELMo – an encoder based on a pre-trained ELMo model Peters et al. ([2018](#bib.bib138)).
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT – an encoder based on a pre-trained BERT model Devlin et al. ([2019](#bib.bib36)).
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastText – an encoder based on a pre-trained fastText model Bojanowski et al.
    ([2017](#bib.bib13)).
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: word2vec-based – an encoder that leverages principles of CBOW or skip-gram algorithms
    Le and Mikolov ([2014](#bib.bib88)); Mikolov et al. ([2013a](#bib.bib110)); Mikolov
    et al. ([2013b](#bib.bib111)).
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that the theoretical complexity of various types of encoders is different.
    As discussed by Vaswani et al. ([2017](#bib.bib182)), complexity per layer of
    self-attention is $O(n^{2}\cdot d)$, as compared to $O(n\cdot d^{2})$ for a recurrent
    layer, and $O(k\cdot n\cdot d^{2})$ for a convolutional layer, where $n$ is the
    length of an input sequence, $d$ is the dimensionality, and $k$ is the kernel
    size of convolutions. At the same time, the self-attention allows for a better
    parallelization than the recurrent networks as the number of sequentially executed
    operations for self-attention requires a constant number of sequentially executed
    operations of $O(1)$, while a recurrent layer requires $O(n)$ sequential operations.
    Overall, estimation of the computational complexity of training and inference
    of various neural networks is certainly beyond the scope of the goal of this survey.
    The interested reader may refer to Vaswani et al. ([2017](#bib.bib182)) and specialized
    literature on this topic, e.g. Orponen ([1994](#bib.bib132)); Šíma and Orponen
    ([2003](#bib.bib165)); Livni et al. ([2014](#bib.bib99)).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'The global column shows whether a system uses a global solution (see Section
    [3.2.2](#S3.SS2.SSS2 "3.2.2 Global Context Architectures ‣ 3.2 Modifications of
    the General Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A
    Survey of Models Based on Deep Learning")). The MD+ED column refers to joint entity
    mention detection and disambiguation models, where detection and disambiguation
    of entities are performed collectively (Section [3.2.1](#S3.SS2.SSS1 "3.2.1 Joint
    Entity Mention Detection and Disambiguation ‣ 3.2 Modifications of the General
    Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning")). The NIL prediction column points out models that also
    label unlinkable mentions. The entity embedding column presents which resource
    is used to train entity representations based on the categorization in Section
    [3.1.3](#S3.SS1.SSS3 "3.1.3 Entity Encoding ‣ 3.1 General Architecture ‣ 3 Neural
    Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning"),
    where'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n/a – a model does not have a neural encoder for entities.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unstructured text – entity representations are constructed from unstructured
    text using approaches based on co-occurrence statistics developed originally for
    word embeddings like word2vec Mikolov et al. ([2013a](#bib.bib110)).
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: relational info. – a model uses relations between entities in KGs.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ent. specific info. – an entity encoder uses other types of information, like
    entity descriptions, types, or categories.
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the candidate generation column, the candidate generation methods are specified
    (Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Candidate Generation ‣ 3.1 General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning")). It contains the following options:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n/a – the solution that does not have an explicit candidate generation step
    (e.g. the method presented by Broscheit ([2019](#bib.bib17))).
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: surface match – surface form matching heuristics.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: aliases – a supplementary aliases for entities in a KG.
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prior – filtering candidates with pre-calculated mention-entity prior probabilities
    or frequency counts.
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: type classifier – Raiman and Raiman ([2018](#bib.bib146)) filter candidates
    using a classifier for an automatically learned type system.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BM25 – a variant of TF-IDF to measure similarity between a mention and a candidate
    entity based on description pages.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nearest neighbors – the similarity between mention and entity representations
    is calculated, and entities that are nearest neighbors of mentions are retrieved
    as candidates. Wu et al. ([2020b](#bib.bib191)) train a supplementary model for
    this purpose.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google search – leveraging Google Search Engine to retrieve entity candidates.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: char.-level model – a neural character-level string matching model.
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The learning type for disambiguation column shows whether a model is ‘supervised’,
    ‘unsupervised’, ‘weakly-supervised’, or ‘zero-shot’. The cross-lingual column
    refers to models that provide cross-lingual EL solutions (Section [3.2.4](#S3.SS2.SSS4
    "3.2.4 Cross-lingual Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning")).'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the following superscript notations are used to denote specific
    features of methods shown as a note in the Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning"):'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These works use only entity description pages, however, they are labeled as
    the first category (unstructured text) since their training method is based on
    principals from word2vec.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors provide EL as a subsystem of language modeling.
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These solutions do not rely on global coherence but are marked as “global” because
    they use document-wide context or multiple mentions at once for resolving entity
    ambiguity.
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These studies are domain-independent as discussed in Section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Domain-Independent Architectures ‣ 3.2 Modifications of the General Architecture
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning").'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zwicklbauer et al. ([2016b](#bib.bib211)) may not be accepted as purely unsupervised
    since they have some threshold parameters in the disambiguation algorithm tuned
    on a labeled set.
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 3: Evaluation datasets. Descriptive statistics of the evaluation datasets
    used in this survey to compare the EL models. The values for MSNBC, AQUAINT, and
    ACE2004 datasets are based on the update by Guo and Barbosa ([2018](#bib.bib61)).
    The statistics for AIDA-B, MSNBC, AQUAINT, ACE2004, CWEB, and WW is reported according
    to Ganea and Hofmann ([2017](#bib.bib53)) ($\#$ of mentions takes into account
    only non-NIL entity references). The TAC KBP dataset statistics is reported according
    to Ji et al. ([2010](#bib.bib75)); Wu et al. ([2020b](#bib.bib191)); Ellis et al.
    ([2015](#bib.bib39)); Ji et al. ([2015](#bib.bib76)) ($\#$ of mentions takes into
    account also NIL entity references).'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '| Corpus | Text Genre | $\#$ of Documents | $\#$ of Mentions |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| AIDA-B Hoffart et al. ([2011](#bib.bib67)) | News | 231 | 4,485 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| MSNBC Cucerzan ([2007](#bib.bib32)) | News | 20 | 656 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| AQUAINT Milne and Witten ([2008](#bib.bib112)) | News | 50 | 727 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| ACE2004 Ratinov et al. ([2011](#bib.bib148)) | News | 36 | 257 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| CWEB Guo and Barbosa ([2018](#bib.bib61)); Gabrilovich et al. ([2013](#bib.bib52))
    | Web & Wikipedia | 320 | 11,154 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| WW Guo and Barbosa ([2018](#bib.bib61)) | Web & Wikipedia | 320 | 6,821 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| TAC KBP 2010 Ji et al. ([2010](#bib.bib75)) | News & Web | 2,231 | 2,250
    |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TAC KBP 2015 Chinese Ji et al. ([2015](#bib.bib76)) &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; News & Forums &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '| 166 | 11,066 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TAC KBP 2015 Spanish Ji et al. ([2015](#bib.bib76)) &#124;'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; News & Forums &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '| 167 | 5,822 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Entity disambiguation evaluation. Micro F1/Accuracy scores of neural
    entity disambiguation as compared to some classic models on common evaluation
    datasets.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | AIDA-B | KBP’10 | MSNBC | AQUAINT | ACE-2004 | CWEB | WW | KBP’15
    (es) | KBP’15 (zh) |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '|  | Accuracy | Accuracy | Micro F1 | Micro F1 | Micro F1 | Micro F1 | Micro
    F1 | Accuracy | Accuracy |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| Non-Neural Baseline Models |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| DBpedia Spotlight ([2011](#bib.bib108)) Mendes et al. ([2011](#bib.bib108))
    | 0.561 | - | 0.421 | 0.518 | 0.539 | - | - | - | - |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| AIDA ([2011](#bib.bib67)) Hoffart et al. ([2011](#bib.bib67)) | 0.770 | -
    | 0.746 | 0.571 | 0.798 | - | - | - | - |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| [Ratinov et al.](#bib.bib148) ([2011](#bib.bib148)) Ratinov et al. ([2011](#bib.bib148))
    | - | - | 0.750 | 0.830 | 0.820 | 0.562 | 0.672 | - | - |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| WAT ([2014](#bib.bib140)) Piccinno and Ferragina ([2014](#bib.bib140)) |
    0.805 | - | 0.788 | 0.754 | 0.796 | - | - | - | - |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| Babelfy ([2014](#bib.bib115)) Moro et al. ([2014](#bib.bib115)) | 0.758 |
    - | 0.762 | 0.704 | 0.619 | - | - | - | - |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| [Lazic et al.](#bib.bib84) ([2015](#bib.bib84)) Lazic et al. ([2015](#bib.bib84))
    | 0.864 | - | - | - | - | - | - | - | - |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| [Chisholm and Hachey](#bib.bib27) ([2015](#bib.bib27)) Chisholm and Hachey
    ([2015](#bib.bib27)) | 0.887 | - | - | - | - | - | - | - | - |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| PBOH ([2016](#bib.bib54)) Ganea et al. ([2016](#bib.bib54)) | 0.804 | - |
    0.861 | 0.841 | 0.832 | - | - | - | - |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| [Guo and Barbosa](#bib.bib61) ([2018](#bib.bib61)) Guo and Barbosa ([2018](#bib.bib61))
    | 0.890 | - | 0.920 | 0.870 | 0.880 | 0.770 | 0.845 | - | - |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| Neural Models |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| [Sun et al.](#bib.bib171) ([2015](#bib.bib171)) Sun et al. ([2015](#bib.bib171))
    | - | 0.839 | - | - | - | - | - | - | - |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '| [Francis-Landau et al.](#bib.bib49) ([2016](#bib.bib49)) Francis-Landau et al.
    ([2016](#bib.bib49)) | 0.855 | - | - | - | - | - | - | - | - |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib42) ([2016](#bib.bib42)) Fang et al. ([2016](#bib.bib42))
    | - | 0.889 | 0.755 | 0.852 | 0.808 | - | - | - | - |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib194) ([2016](#bib.bib194)) Yamada et al. ([2016](#bib.bib194))
    | 0.931 | 0.855 | - | - | - | - | - | - | - |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| [Zwicklbauer et al.](#bib.bib211) ([2016b](#bib.bib211)) Zwicklbauer et al.
    ([2016b](#bib.bib211)) | 0.784 | - | 0.911 | 0.842 | 0.907 | - | - | - | - |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| [Tsai and Roth](#bib.bib176) ([2016](#bib.bib176)) Tsai and Roth ([2016](#bib.bib176))
    | - | - | - | - | - | - | - | 0.824 | 0.851 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| [Nguyen et al.](#bib.bib127) ([2016b](#bib.bib127)) Nguyen et al. ([2016b](#bib.bib127))
    | 0.872 | - | - | - | - | - | - | - | - |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| [Globerson et al.](#bib.bib56) ([2016](#bib.bib56)) Globerson et al. ([2016](#bib.bib56))
    | 0.927 | 0.872 | - | - | - | - | - | - | - |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib19) ([2017](#bib.bib19)) Cao et al. ([2017](#bib.bib19))
    | 0.851 | - | - | - | - | - | - | - | - |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| [Eshel et al.](#bib.bib40) ([2017](#bib.bib40)) Eshel et al. ([2017](#bib.bib40))
    | 0.873 | - | - | - | - | - | - | - | - |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| [Ganea and Hofmann](#bib.bib53) ([2017](#bib.bib53)) Ganea and Hofmann ([2017](#bib.bib53))
    | 0.922 | - | 0.937 | 0.885 | 0.885 | 0.779 | 0.775 | - | - |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| [Gupta et al.](#bib.bib62) ([2017](#bib.bib62)) Gupta et al. ([2017](#bib.bib62))
    | 0.829 | - | - | - | 0.907 | - | - | - | - |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| [Nie et al.](#bib.bib129) ([2018](#bib.bib129)) Nie et al. ([2018](#bib.bib129))
    | 0.898 | 0.891 | - | - | - | - | - | - | - |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib157) ([2018](#bib.bib157)) Shahbazi et al. ([2018](#bib.bib157))
    | 0.944 | 0.879 | - | - | - | - | - | - | - |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib85) ([2018](#bib.bib85)) Le and Titov ([2018](#bib.bib85))
    | 0.931 | - | 0.939 | 0.884 | 0.900 | 0.775 | 0.780 | - | - |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| [Radhakrishnan et al.](#bib.bib144) ([2018](#bib.bib144)) Radhakrishnan et al.
    ([2018](#bib.bib144)) | 0.930 | 0.896 | - | - | - | - | - | - | - |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| [Kolitsas et al.](#bib.bib82) ([2018](#bib.bib82)) Kolitsas et al. ([2018](#bib.bib82))
    | 0.831 | - | 0.864 | 0.832 | 0.855 | - | - | - | - |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| [Sil et al.](#bib.bib164) ([2018](#bib.bib164)) Sil et al. ([2018](#bib.bib164))
    | 0.940 | 0.874 | - | - | - | - | - | 0.823 | 0.844 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| [Upadhyay et al.](#bib.bib179) ([2018a](#bib.bib179)) Upadhyay et al. ([2018a](#bib.bib179))
    | - | - | - | - | - | - | - | 0.844 | 0.860 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib20) ([2018](#bib.bib20)) Cao et al. ([2018](#bib.bib20))
    | 0.800 | 0.910 | - | 0.870 | 0.880 | - | 0.860 | - | - |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '| [Raiman and Raiman](#bib.bib146) ([2018](#bib.bib146)) Raiman and Raiman
    ([2018](#bib.bib146)) | 0.949 | 0.909 | - | - | - | - | - | - | - |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib158) ([2019](#bib.bib158)) Shahbazi et al. ([2019](#bib.bib158))
    | 0.962 | 0.883 | 0.923 | 0.901 | 0.887 | 0.784 | 0.798 | - | - |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| [Gillick et al.](#bib.bib55) ([2019](#bib.bib55)) Gillick et al. ([2019](#bib.bib55))
    | - | 0.870 | - | - | - | - | - | - | - |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib87) ([2019b](#bib.bib87)) Le and Titov ([2019b](#bib.bib87))
    | 0.815 | - | - | - | - | - | - | - | - |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib86) ([2019a](#bib.bib86)) Le and Titov ([2019a](#bib.bib86))
    | 0.897 | - | 0.922 | 0.907 | 0.881 | 0.782 | 0.817 | - | - |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib43) ([2019](#bib.bib43)) Fang et al. ([2019](#bib.bib43))
    | 0.943 | - | 0.928 | 0.875 | 0.912 | 0.785 | 0.828 | - | - |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| [Yang et al.](#bib.bib200) ([2019](#bib.bib200)) Yang et al. ([2019](#bib.bib200))
    | 0.946 | - | 0.946 | 0.885 | 0.901 | 0.756 | 0.788 | - | - |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| [Xue et al.](#bib.bib192) ([2019](#bib.bib192)) Xue et al. ([2019](#bib.bib192))
    | 0.924 |  | 0.944 | 0.919 | 0.911 | 0.801 | 0.855 | - | - |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| [Zhou et al.](#bib.bib207) ([2019](#bib.bib207)) Zhou et al. ([2019](#bib.bib207))
    | - | - | - | - | - | - | - | 0.829 | 0.855 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| [Hou et al.](#bib.bib69) ([2020](#bib.bib69)) Hou et al. ([2020](#bib.bib69))
    | 0.926 | - | 0.943 | 0.912 | 0.907 | 0.785 | 0.819 | - | - |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| [Onoe and Durrett](#bib.bib131) ([2020](#bib.bib131)) Onoe and Durrett ([2020](#bib.bib131))
    | 0.859 | - | - | - | - | - | - | - | - |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib191) ([2020b](#bib.bib191)) Wu et al. ([2020b](#bib.bib191))
    | - | 0.945 | - | - | - | - | - | - | - |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib190) ([2020a](#bib.bib190)) Wu et al. ([2020a](#bib.bib190))
    | 0.931 | - | 0.927 | 0.894 | 0.906 | 0.814 | 0.792 | - | - |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib44) ([2020](#bib.bib44)) Fang et al. ([2020](#bib.bib44))
    | 0.830 | - | 0.800 | 0.880 | 0.890 | - | - | - | - |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib25) ([2020](#bib.bib25)) Chen et al. ([2020](#bib.bib25))
    | 0.937 | - | 0.945 | 0.898 | 0.908 | 0.782 | 0.810 | - | - |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| [Mulang’ et al.](#bib.bib117) ([2020](#bib.bib117)) Mulang’ et al. ([2020](#bib.bib117))
    | 0.949 | - | - | - | - | - | - | - | - |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib198) ([2021](#bib.bib198)) Yamada et al. ([2021](#bib.bib198))
    | 0.971 | - | 0.963 | 0.935 | 0.919 | 0.789 | 0.892 | - | - |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| [De Cao et al.](#bib.bib33) ([2021](#bib.bib33)) De Cao et al. ([2021](#bib.bib33))
    | 0.933 | - | 0.943 | 0.909 | 0.911 | 0.773 | 0.879 | - | - |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: 4 Evaluation
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present evaluation results for the entity linking and entity
    relatedness tasks on the commonly used datasets.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Entity Linking
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1 Experimental Setup
  id: totrans-529
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The evaluation results are reported based on two different evaluation settings.
    The first setup is entity disambiguation (ED) where the systems have access to
    the mention boundaries. The second setup is entity mention detection and disambiguation
    (MD+ED) where the input for the systems that perform MD and ED jointly is only
    plain text. We presented their results in separate tables since the scores for
    the joint models accumulate the errors made during the mention detection phase.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  id: totrans-531
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We report the evaluation results of monolingual EL models on the English datasets
    widely-used in recent research publications: AIDA Hoffart et al. ([2011](#bib.bib67)),
    TAC KBP 2010 Ji et al. ([2010](#bib.bib75)), MSNBC Cucerzan ([2007](#bib.bib32)),
    AQUAINT Milne and Witten ([2008](#bib.bib112)), ACE2004 Ratinov et al. ([2011](#bib.bib148)),
    CWEB Guo and Barbosa ([2018](#bib.bib61)); Gabrilovich et al. ([2013](#bib.bib52)),
    and WW Guo and Barbosa ([2018](#bib.bib61)). AIDA is the most popular dataset
    for benchmarking EL systems. For AIDA, we report the results calculated for the
    test set (AIDA-B).'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-lingual EL results are reported for the TAC KBP 2015 Ji et al. ([2015](#bib.bib76))
    Spanish (es) and Chinese (zh) datasets. The descriptive statistics of the datasets
    and their text genres are presented in Table [3](#S3.T3 "Table 3 ‣ 3.4 Summary
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning") according to information reported in Ganea and Hofmann ([2017](#bib.bib53));
    Wu et al. ([2020b](#bib.bib191)); Ji et al. ([2010](#bib.bib75), [2015](#bib.bib76));
    Ellis et al. ([2015](#bib.bib39)).'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b993da5c908954d4209d6e76b43c9d78.png)'
  id: totrans-534
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Entity disambiguation progress. Performance of the classic entity
    linking models (green) with the more recent neural models (gray) on the AIDA test
    set shows an improvement (around 10 points of accuracy).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/045eba25189f9e05480819e9f66adfe0.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Mention/context encoder type for entity disambiguation. Performance
    of the entity disambiguation models on the AIDA test set with mention/context
    encoder displayed with different colors as defined in Table [2](#S3.T2 "Table
    2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of
    Models Based on Deep Learning"). The bars with multiple colors refer to the models
    that use different types of encoder models; the bars do not reflect any meaning
    on the percentage. Note: we assigned the “RNN” label for the models LSTM, GRU,
    and ELMo; the “Transformers” label for BERT and BART models.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac7b0d61efdcca39393b0462d105b6a5.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Local-Global entity disambiguation. Performance of the entity disambiguation
    models on the AIDA test set with local/global models displayed with different
    colors as defined in Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").
    Note, some models, like Francis-Landau et al. ([2016](#bib.bib49)), do not rely
    on global coherence, but they use document-wide context or multiple mentions at
    once, as explained in Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity
    Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the ED setting, we present micro F1 or accuracy scores reported by model
    authors. We note that, since mentions are provided as an input, the number of
    mentions predicted by the model is equal to the number of mentions in the ground
    truth Shen et al. ([2015](#bib.bib160)), so micro F1, precision, recall, and accuracy
    scores are equal in this setting as explained in Shen et al. ([2015](#bib.bib160)):'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F1=Acc=\frac{\#\ correctly\ disamb.\ mentions}{\#\ total\ mentions}.$
    |  | (22) |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: 'For the MD+ED setting, where joint models are evaluated, we report micro F1
    scores based on strong annotation matching. The formulas to compute F1 scores
    are shown below, as described in Shen et al. ([2015](#bib.bib160)) and Ganea et al.
    ([2016](#bib.bib54)):'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P=\frac{\#\ correctly\ detected\ and\ disamb.\ mentions}{\#\
    predicted\ mentions\ by\ model},$ |  | (23) |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle R=\frac{\#\ correctly\ detected\ and\ disamb.\ mentions}{\#\
    mentions\ in\ ground\ truth},$ |  | (24) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle F1=\frac{2\cdot P\cdot R}{P+R}.$ |  | (25) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: We note that results reported in multiple considered papers are usually obtained
    using GERBIL Röder et al. ([2018](#bib.bib153)) – a platform for benchmarking
    EL models. It implements various experimental setups, including entity disambiguation
    denoted as D2KB and a combination of mention detection and disambiguation denoted
    as A2KB. GERBIL encompasses many evaluation datasets in a standartized way along
    with annotations and provides the computation of evaluation metrics, i.e. micro-macro
    precision, recall, and F-measure.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: Baseline Models
  id: totrans-548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While our goal is to perform a survey of neural EL systems, we also report results
    of several indicative and prominent classic non-neural systems as baselines to
    underline the advances yielded by neural models. More specifically, we report
    results of DBpedia Spotlight ([2011](#bib.bib108)) Mendes et al. ([2011](#bib.bib108)),
    AIDA ([2011](#bib.bib67)) Hoffart et al. ([2011](#bib.bib67)), [Ratinov et al.](#bib.bib148) ([2011](#bib.bib148)) Ratinov
    et al. ([2011](#bib.bib148)), WAT ([2014](#bib.bib140)) Piccinno and Ferragina
    ([2014](#bib.bib140)), Babelfy ([2014](#bib.bib115)) Moro et al. ([2014](#bib.bib115)),
    [Lazic et al.](#bib.bib84) ([2015](#bib.bib84)) Lazic et al. ([2015](#bib.bib84)),
    Chisholm and Hachey (2015) Chisholm and Hachey ([2015](#bib.bib27)), and PBOH
    ([2016](#bib.bib54)) Ganea et al. ([2016](#bib.bib54)).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: For considered neural EL systems, we present the best scores reported by the
    authors. For the baseline systems, the results are reported according to Kolitsas
    et al. ([2018](#bib.bib82))^(14)^(14)14Some of the baseline scores are presented
    in the appendix of Kolitsas et al. ([2018](#bib.bib82)), which is available at
    [https://arxiv.org/pdf/1808.07699.pdf](https://arxiv.org/pdf/1808.07699.pdf) and
    Ganea and Hofmann ([2017](#bib.bib53)).
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Discussion of Results
  id: totrans-551
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Entity Disambiguation Results
  id: totrans-552
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We start our discussion of the results from the entity disambiguation (ED)
    models, for which mention boundaries are provided. Figure [8](#S4.F8 "Figure 8
    ‣ Datasets ‣ 4.1.1 Experimental Setup ‣ 4.1 Entity Linking ‣ 4 Evaluation ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") shows how the performance
    of the entity disambiguation models on the most widely-used dataset AIDA improved
    during the course of the last decade and how the best disambiguation models based
    on classical machine learning methods (denoted as “non-neural”) correspond to
    the recent state-of-the-art models based on deep neural networks (denoted as “neural”).
    As one may observe, the models based on deep learning substantially improve the
    EL performance pushing the state of the art by around 10 percentage points in
    terms of accuracy.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [4](#S3.T4 "Table 4 ‣ 3.4 Summary ‣ 3 Neural Entity Linking ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") presents the comparison
    of the ED models in detail on several datasets presented above. The model of Yamada
    et al. ([2021](#bib.bib198)) yields the best result on AIDA and appears to behave
    robustly across different datasets, getting top scores or near top scores for
    most of them. Here, we should also mention that none of the non-neural baselines
    reach the best results on any dataset.'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: Among local models for disambiguation, the best results are reported by Shahbazi
    et al. ([2019](#bib.bib158)) and Wu et al. ([2020b](#bib.bib191)). It is worth
    noting that the latter model can be used in the zero-shot setting. Shahbazi et al.
    ([2019](#bib.bib158)) has the best score on AIDA among other local models outperforming
    them by a substantial margin. However, this is due to the use of the less-ambiguous
    resource of Pershina et al. ([2015](#bib.bib137)) for candidate generation, while
    many other works use the YAGO-based resource provided by Ganea and Hofmann ([2017](#bib.bib53)),
    which typically yields lower results.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: The common trend is that the global models (those trying to disambiguate several
    entity occurrences at once) outperform the local ones (relying on a single mention
    and its context). The best considered ED model of Yamada et al. ([2021](#bib.bib198))
    is global. Its performance improvements over competitors are attributed by the
    authors to the novel masked entity prediction objective that helps to fine-tune
    pre-trained BERT for producing contextualized entity embeddings and to the multi-step
    global disambiguation algorithm.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as one could see from Table [4](#S3.T4 "Table 4 ‣ 3.4 Summary ‣ 3
    Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning"), the least number of experiments is reported on the non-English datasets
    (TAC KBP datasets for Chinese and Spanish). Among the four reported results, the
    approach of Upadhyay et al. ([2018a](#bib.bib179)) provides the best scores, yet
    outperforming the other three approaches only by a small margin.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: Mention/Context Encoder Type
  id: totrans-558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure [9](#S4.F9 "Figure 9 ‣ Datasets ‣ 4.1.1 Experimental Setup ‣ 4.1 Entity
    Linking ‣ 4 Evaluation ‣ Neural Entity Linking: A Survey of Models Based on Deep
    Learning") provides further analysis of the performance of entity disambiguation
    models presented above. The top performing model by Yamada et al. ([2021](#bib.bib198))
    is based on Transformers. It is followed by the model of Shahbazi et al. ([2019](#bib.bib158)),
    which relies on RNNs: more specifically, it relies on the ELMo encoder that is
    based on pre-trained bidirectional LSTM cells. Overall, RNN is a popular choice
    for the mention-context encoder. However, recently, self-attention-based encoders,
    and especially the ones based on pre-trained Transformer networks, have gained
    popularity.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: Several approaches, such as Yamada et al. ([2016](#bib.bib194)), rely on simpler
    encoders based on the word2vec models, yet none of them manage to outperform more
    complex deep architectures.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: Local-global models
  id: totrans-561
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure [10](#S4.F10 "Figure 10 ‣ Datasets ‣ 4.1.1 Experimental Setup ‣ 4.1
    Entity Linking ‣ 4 Evaluation ‣ Neural Entity Linking: A Survey of Models Based
    on Deep Learning") visualizes the usage of the local and global context in various
    models for entity disambiguation. As one can observe from the plot, the majority
    of models perform global entity disambiguation, including the top-performing model
    by Yamada et al. ([2021](#bib.bib198)). Although Shahbazi et al. ([2019](#bib.bib158))
    provide a local model, they also show a good performance.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Evaluation of joint MD-ED models. Micro F1 scores for joint entity
    mention detection and entity disambiguation evaluation on AIDA-B and MSNBC datasets.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | AIDA-B | MSNBC |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| Non-Neural Baseline Models |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| DBpedia Spotlight ([2011](#bib.bib108)) Mendes et al. ([2011](#bib.bib108))
    | 0.578 | 0.406 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| AIDA ([2011](#bib.bib67)) Hoffart et al. ([2011](#bib.bib67)) | 0.728 | 0.651
    |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| WAT ([2014](#bib.bib140)) Piccinno and Ferragina ([2014](#bib.bib140)) |
    0.730 | 0.645 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| Babelfy ([2014](#bib.bib115)) Moro et al. ([2014](#bib.bib115)) | 0.485 |
    0.397 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| Neural Models |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| [Kolitsas et al.](#bib.bib82) ([2018](#bib.bib82)) Kolitsas et al. ([2018](#bib.bib82))
    | 0.824 | 0.724 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| [Martins et al.](#bib.bib107) ([2019](#bib.bib107)) Martins et al. ([2019](#bib.bib107))
    | 0.819 | - |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| [Peters et al.](#bib.bib139) ([2019](#bib.bib139)) Peters et al. ([2019](#bib.bib139))
    | 0.744 | - |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| [Broscheit](#bib.bib17) ([2019](#bib.bib17)) Broscheit ([2019](#bib.bib17))
    | 0.793 | - |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib23) ([2020](#bib.bib23)) Chen et al. ([2020](#bib.bib23))
    | 0.877 | - |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| [Poerner et al.](#bib.bib142) ([2020](#bib.bib142)) Poerner et al. ([2020](#bib.bib142))
    | 0.850 | - |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| [De Cao et al.](#bib.bib33) ([2021](#bib.bib33)) De Cao et al. ([2021](#bib.bib33))
    | 0.837 | 0.737 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: Joint Entity Mention Detection and Disambiguation
  id: totrans-578
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Table [5](#S4.T5 "Table 5 ‣ Local-global models ‣ 4.1.2 Discussion of Results
    ‣ 4.1 Entity Linking ‣ 4 Evaluation ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning") presents results of the joint MD and ED models. Only
    a fraction of the models presented in Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary
    ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on
    Deep Learning") is capable of performing both entity mention detection and disambiguation;
    thus, the list of results is much shorter. Among the joint MD and ED solutions,
    the best results on the AIDA dataset are reported by Chen et al. ([2020](#bib.bib23)).
    However, Poerner et al. ([2020](#bib.bib142)) note that these results might not
    be directly comparable with others due to a different evaluation protocol. The
    best comparable results on the AIDA dataset are shown by E-BERT Poerner et al.
    ([2020](#bib.bib142)). On the MSNBC dataset, the top scores are achieved by De Cao
    et al. ([2021](#bib.bib33)) with an autoregressive model. The scores of the systems
    that solve both tasks at once fall behind the disambiguation-only systems since
    they rely on noisy mention boundaries produced by themselves. In the joint MD
    and ED setting, the neural models also substantially (up to around 10 points)
    outperform the classic models.'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: On Effect of Hyperparameter Search
  id: totrans-580
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As explained above, in Tables [4](#S3.T4 "Table 4 ‣ 3.4 Summary ‣ 3 Neural
    Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning")
    and [5](#S4.T5 "Table 5 ‣ Local-global models ‣ 4.1.2 Discussion of Results ‣
    4.1 Entity Linking ‣ 4 Evaluation ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning"), we present the best scores reported by the authors of
    the models. In principle, each neural model can be further tuned as shown by Reimers
    and Gurevych ([2017](#bib.bib149)), but also the variance of neural models is
    rather high in general. Therefore, it may be possible to further optimize meta-parameters
    of one (possibly simpler) neural model so that it outperforms a more complex (but
    tuned in a less optimal way) model. One common example of such a case is RoBERTa Liu
    et al. ([2020](#bib.bib98)), which is basically the original BERT model, which
    was carefully and robustly optimized. This model outperformed many successors
    of the BERT model, showing the new state-of-the-art results on various tasks while
    keeping the original architecture.'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Entity Relatedness
  id: totrans-582
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The quality of entity representations can be measured by how they capture semantic
    relatedness between entities Huang et al. ([2015](#bib.bib70)); Ganea and Hofmann
    ([2017](#bib.bib53)); Yamada et al. ([2016](#bib.bib194)); Cao et al. ([2017](#bib.bib19));
    Shi et al. ([2020](#bib.bib162)). Moreover, the semantic relatedness is an important
    feature in global EL El Vaigh et al. ([2019](#bib.bib38)); Ceccarelli et al. ([2013](#bib.bib21)).
    In this section, we present results of entity relatedness evaluation, which is
    different from evaluation of EL pipelines.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Entity relatedness evaluation. Reported results for entity relatedness
    evaluation on the test set of Ceccarelli et al. ([2013](#bib.bib21)) .'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | nDCG@1 | nDCG@5 | nDCG@10 | MAP |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '| [Milne and Witten](#bib.bib112) ([2008](#bib.bib112)) Milne and Witten ([2008](#bib.bib112))
    | 0.540 | 0.520 | 0.550 | 0.480 |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
- en: '| [Huang et al.](#bib.bib70) ([2015](#bib.bib70)) Huang et al. ([2015](#bib.bib70))
    | 0.810 | 0.730 | 0.740 | 0.680 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib194) ([2016](#bib.bib194)) Yamada et al. ([2016](#bib.bib194))
    | 0.590 | 0.560 | 0.590 | 0.520 |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| [Ganea and Hofmann](#bib.bib53) ([2017](#bib.bib53)) Ganea and Hofmann ([2017](#bib.bib53))
    | 0.632 | 0.609 | 0.641 | 0.578 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib19) ([2017](#bib.bib19)) Cao et al. ([2017](#bib.bib19))
    | 0.613 | 0.613 | 0.654 | 0.582 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| [El Vaigh et al.](#bib.bib38) ([2019](#bib.bib38)) El Vaigh et al. ([2019](#bib.bib38))
    | 0.690 | 0.640 | 0.580 | - |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: '| [Shi et al.](#bib.bib162) ([2020](#bib.bib162)) Shi et al. ([2020](#bib.bib162))
    | 0.680 | 0.814 | 0.820 | - |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
- en: 4.2.1 Experimental Setup
  id: totrans-594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We summarize results from several works obtained on a benchmark of Ceccarelli
    et al. ([2013](#bib.bib21)) for entity relatedness evaluation based on the dataset
    of Hoffart et al. ([2011](#bib.bib67)). Given a target entity and a list of candidate
    entities, the task is to rank candidates semantically related to the target higher
    than the others Ganea and Hofmann ([2017](#bib.bib53)). For the most of the considered
    works, the relatedness is measured by the cosine similarity of entity representations.
    For comparison, we also add results for two other approaches: a well-known Wikipedia
    hyperlink-based measure devised by Milne and Witten ([2008](#bib.bib112)) known
    as WLM and a KG-based measure of El Vaigh et al. ([2019](#bib.bib38)).'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation metrics are normalized discounted cumulative gain (nDCG) Järvelin
    and Kekäläinen ([2002](#bib.bib73)) and a mean average precision (MAP) Manning
    et al. ([2008](#bib.bib105)). nDCG is a commonly used metric in information retrieval.
    It discounts the correct answers, depending on their rank in predictions Manning
    et al. ([2008](#bib.bib105)):'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $nDCG(Q,k)=\frac{1}{&#124;Q&#124;}\sum_{\_}{j=1}^{&#124;Q&#124;}Z_{\_}{kj}\sum_{\_}{m=1}^{k}\frac{2^{R(j,m)}-1}{{\log_{\_}{2}(1+m)}},$
    |  | (26) |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: where $Q$ is the set of target entities (queries); $Z_{\_}{kj}$ is a normalization
    factor, which corresponds to ideal ranking; $k$ is a number of candidates for
    each query; $R(j,m)\in\{0,1\}$ is the gold-standard annotation of relatedness
    between the target entity $j$ and a candidate $m$.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: 'MAP is another common metric in information retrieval Manning et al. ([2008](#bib.bib105)):'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MAP(Q)=\frac{1}{&#124;Q&#124;}\sum_{\_}{j=1}^{&#124;Q&#124;}\frac{1}{m_{\_}j}\sum_{\_}{k=1}^{m_{\_}j}Precision@r_{\_}{jk},$
    |  | (27) |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: where $Q$ is a set of target entities (queries); $m_{\_}j$ is the number of
    related candidate entities for the target $j$, and $Precision@r_{\_}{jk}$ is a
    precision at rank $r_{\_}{jk}$, where $r_{\_}{jk}$ is a rank of each related candidate
    in the prediction $k=1..m_{\_}j$ Manning et al. ([2008](#bib.bib105)).
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Discussion of Results
  id: totrans-602
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.2 Entity Relatedness ‣ 4 Evaluation ‣ Neural
    Entity Linking: A Survey of Models Based on Deep Learning") summarizes the evaluation
    results in the entity relatedness task reported by the authors of the models.
    The scores of Milne and Witten ([2008](#bib.bib112)) are taken from Huang et al.
    ([2015](#bib.bib70)).'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: The highest scores of nDCG@1 and MAP are reported by Huang et al. ([2015](#bib.bib70)),
    and the best scores of nDCG@5 and nDCG@10 are reported by Shi et al. ([2020](#bib.bib162)).
    The high scores of Huang et al. ([2015](#bib.bib70)) can be attributed to the
    usage of different information sources for constructing entity representations,
    including entity types and entity relations Ganea and Hofmann ([2017](#bib.bib53)).
    Shi et al. ([2020](#bib.bib162)) also use various types of data sources for constructing
    entity representations, including textual and knowledge graph information, like
    the types provided by a category hierarchy of a knowledge graph.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that cosine similarity based measures perform better in terms of nDCG@10
    than the methods based on relations in KG (shown as italic in Table [6](#S4.T6
    "Table 6 ‣ 4.2 Entity Relatedness ‣ 4 Evaluation ‣ Neural Entity Linking: A Survey
    of Models Based on Deep Learning")).'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: 5 Applications of Entity Linking
  id: totrans-606
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first give a brief overview of established applications
    of the entity linking technology and then discuss recently emerged use-cases specific
    to neural entity linking based on injection of these models as a part of a larger
    neural network, e.g. in a neural language model.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Established Applications
  id: totrans-608
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text Mining
  id: totrans-609
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An EL tool is a typical building block for text mining systems. Extracting and
    resolving the ambiguity of entity mentions is one of the first steps in a common
    information extraction pipeline. The ambiguity problem is especially crucial for
    such domains as biomedical and clinical text processing due to variability of
    medical terms, the complexity of medical ontologies such as UMLS Bodenreider ([2004](#bib.bib12)),
    and scarcity of annotated resources. There is a long history of development of
    EL tools for biomedical literature and electronic health record mining applications
    Aronson and Lang ([2010](#bib.bib6)); Savova et al. ([2010](#bib.bib155)); Soldaini
    and Goharian ([2016](#bib.bib167)); Tutubalina et al. ([2020](#bib.bib178)); Zhu
    et al. ([2020](#bib.bib209)); Loureiro and Jorge ([2020](#bib.bib101)); Kraljevic
    et al. ([2021](#bib.bib83)); Miftahutdinov et al. ([2021](#bib.bib109)); Chen
    et al. ([2021](#bib.bib24)). These tools have been successfully applied for summarization
    of clinical reports MacAvaney et al. ([2019](#bib.bib104)), extraction of drug-disease
    treatment relationships Khare et al. ([2014](#bib.bib81)), mining chemical-induced
    disease relations Bansal et al. ([2020](#bib.bib10)), differential diagnosis Amiri
    et al. ([2021](#bib.bib5)), patient screening Eyre et al. ([(in press, n.d.)](#bib.bib41)),
    and many other tasks. Besides medical text processing, EL is widely used for mining
    social networks and news Moon et al. ([2018](#bib.bib113)); Adjali et al. ([2020](#bib.bib2)).
    For example, Twitcident Abel et al. ([2012](#bib.bib1)) uses the DBpedia Spotlight
    Mendes et al. ([2011](#bib.bib108)) EL system for mining Twitter messages for
    small scale incidents. Provatorova et al. ([2020](#bib.bib143)) leverage a recently
    proposed EL toolkit REL van Hulst et al. ([2020](#bib.bib181)) for mining historical
    newspapers for people, places, and other entities in the CLEF HIPE 2020 evaluation
    campaign Ehrmann et al. ([2020](#bib.bib37)). Luo et al. ([2021](#bib.bib103))
    automatically construct a large-scale dataset of images and text captions that
    describe real and out-of-context news. They leverage REL for linking entities
    in image captions, which helps to automatically measure inconsistency between
    images and their text captions.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph population
  id: totrans-611
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: EL is one of the necessary steps of knowledge graph population algorithms. Before
    populating a KG with new facts extracted from raw texts, we have to determine
    mentioned concepts in these texts and link them to the corresponding graph nodes.
    A series of evaluation workshops TAC^(15)^(15)15https://tac.nist.gov/2019/index.html
    provides a forum for KG population tools (TAC KBP), as well as benchmarks for
    various subsystems including EL. For example, Ji and Grishman ([2011](#bib.bib74))
    and Ellis et al. ([2015](#bib.bib39)) overview various successful systems for
    knowledge graph population participated in the TAC KBP 2010 and 2015 tasks. Shen
    et al. ([2018](#bib.bib161)) propose a knowledge graph population algorithm that
    not only uses the results of EL, but also helps to improve EL itself. It iteratively
    populates a KG, while the EL model benefits from added knowledge and continuously
    learns to disambiguate better.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: Information retrieval and question-answering
  id: totrans-613
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'EL is also widely used in information retrieval and question-answering systems.
    EL helps to complement search results with additional semantic information, to
    resolve query ambiguity, and to restrict the search space. For example, Lee et al.
    ([2016](#bib.bib91)) use EL to complement the results of a biomedical literature
    search engine with found entities: genes, diseases, drugs, etc. COVIDASK Lee et al.
    ([2020](#bib.bib90)), a real-time question answering system that helps researchers
    to retrieve information related to coronavirus, uses the BioSyn model Sung et al.
    ([2020](#bib.bib172)) for processing COVID-19 articles and linking mentions of
    drugs, symptoms, diseases to concepts in biomedical ontologies. Links to entity
    descriptions help users to navigate the search results, which enhances the usability
    of the system. Yih et al. ([2015](#bib.bib202)) apply EL for pruning the search
    space of a question answering system. For the query: “Who first voiced Meg on
    Family Guy?”, after linking “Meg” and “Family Guy” to entities in a KG, the task
    becomes to resolve the predicates to the “Family Guy (the TV show)” entry rather
    than all entries in the KG. Shnayderman et al. ([2019](#bib.bib163)) develop a
    fast EL algorithm for pre-processing large corpora for their autonomous debating
    system Slonim et al. ([2021](#bib.bib166)) with the goal to conduct an argumentative
    dialog with an opponent on some topic and to prove a predefined point of view.
    The system uses the results of entity linking for corpus-based argument retrieval.'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 Novel Applications: Neural Entity Linking for Training Better Neural Language
    Models'
  id: totrans-615
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural EL models have unlocked the new category of applications that have not
    been available for classical machine learning methods. Namely, neural models allow
    the integration of an entire entity linking system inside a larger neural network
    such as BERT. As they are both neural networks, such kind of integration becomes
    possible. After integrating an entity linker into another model’s architecture,
    we can also expand the training objective with an additional EL-related task and
    train parameters of all neural components jointly:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\_}{\text{JOINT }}=\mathcal{L}_{\_}{\text{BERT }}+\mathcal{L}_{\_}{\text{EL-related
    }}.$ |  | (28) |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: Neural entity linkers can be integrated in any other networks. The main novel
    trend is the use of EL information for representation learning. Several studies
    have shown that contextual word representations could benefit from information
    stored in KGs by incorporating EL into deep language models (LMs) for transfer
    learning.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: 'KnowBERT Peters et al. ([2019](#bib.bib139)) injects one or several entity
    linkers between top layers of the BERT architecture and optimizes the whole network
    for multiple tasks: the masked language model (MLM) task and next sentence prediction
    (NSP) from the original BERT model, as well as EL:'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\_}{\mathrm{BERT}}=\mathcal{L}_{\_}{\mathrm{NSP}}+\mathcal{L}_{\_}{\mathrm{MLM}}.$
    |  | (29) |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '|  | $\mathcal{L}_{\_}{\text{KnowBert }}=\mathcal{L}_{\_}{\mathrm{NSP}}+\mathcal{L}_{\_}{\mathrm{MLM}}+\mathcal{L}_{\_}{\text{EL
    }}.$ |  | (30) |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: The authors adopt the general end-to-end EL architecture of Kolitsas et al.
    ([2018](#bib.bib82)) but use only the local context for disambiguation and an
    encoder based on self-attention over the representations generated by underlying
    BERT layers. If the EL subsystem detects an entity mention in a given sentence,
    corresponding pre-built entity representations of candidates are utilized for
    calculating the updated contextual word representations generated on the current
    BERT layer. These representations are used as input in a subsequent layer and
    can also be modified by a subsequent EL subsystem. Experiments with two EL subsystems
    based on Wikidata and WordNet show that presented modifications in KnowBERT help
    it to slightly surpass other deep pre-trained language models in tasks of relationship
    extraction, WSD, and entity typing.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: 'ERNIE Zhang et al. ([2019](#bib.bib206)) expands the BERT Devlin et al. ([2019](#bib.bib36))
    architecture with a knowledgeable encoder (K-Encoder), which fuses contextualized
    word representations obtained from the underlying self-attention network with
    entity representations from a pre-trained TransE model Bordes et al. ([2013](#bib.bib15)).
    EL in this study is performed by an external tool TAGME Ferragina and Scaiella
    ([2010](#bib.bib47)). For model pre-training, in addition to the MLM task, the
    authors introduce the task of restoring randomly masked entities in a given sequence
    keeping the rest of the entities and tokens. They refer to this procedure as a
    denoising entity auto-encoder (dEA):'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\_}{\text{ERNIE }}=\mathcal{L}_{\_}{\mathrm{NSP}}+\mathcal{L}_{\_}{\mathrm{MLM}}+\mathcal{L}_{\_}{\text{dEA
    }}.$ |  | (31) |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: Using English Wikipedia and Wikidata as training data, the authors show that
    introduced modifications provide performance gains in entity typing, relation
    classification, and several GLUE tasks Wang et al. ([2018](#bib.bib185)).
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: 'Wang et al. ([2021](#bib.bib188)) train a disambiguation network named KEPLER
    using the composition of two losses: regular MLM and a Knowledge Embedding (KE)
    loss based on the TransE Bordes et al. ([2013](#bib.bib15)) objective for encoding
    graph structures:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L_{\_}{\mathrm{KEPLER}}}=\mathcal{L}_{\_}{\mathrm{MLM}}+\mathcal{L}_{\_}{\mathrm{KE}}.$
    |  | (32) |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
- en: In the KE loss, representations of entities are obtained from their textual
    descriptions encoded with a self-attention network Liu et al. ([2020](#bib.bib98)),
    and representations of relations are trainable vectors. The network is trained
    on a dataset of entity-relation-entity triplets with descriptions gathered from
    Wikipedia and Wikidata. Although the system exhibits a significant drop in performance
    on general NLP benchmarks such as GLUE Wang et al. ([2018](#bib.bib185)), it shows
    increased performance on a wide range of KB-related tasks such as TACRED Zhang
    et al. ([2017](#bib.bib205)), FewRel Han et al. ([2018](#bib.bib63)), and OpenEntity
    Choi et al. ([2018](#bib.bib28)).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: Yamada et al. ([2020a](#bib.bib196)) propose a deep pre-trained model called
    “Language Understanding with Knowledge-based Embeddings” (LUKE). They modify RoBERTa
    Liu et al. ([2020](#bib.bib98)) by introducing an additional pre-training objective
    and an entity-aware self-attention mechanism. The objective is a simple adoption
    of the MLM task to entities $\mathcal{L}_{\_}{MLMe}$, instead of tokens, the authors
    suggest restoring randomly masked entities in an entity-annotated corpus.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L_{\_}{\mathrm{LUKE}}}=\mathcal{L}_{\_}{\mathrm{MLM}}+\mathcal{L}_{\_}{\mathrm{MLMe}}.$
    |  | (33) |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
- en: Although the corpus used in this work is constructed from Wikipedia by considering
    hyperlinks to other Wikipedia pages as mentions of entities in a KG, alternatively,
    it can be generated using an external entity linker.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: 'The entity-aware attention mechanism helps LUKE differentiate between words
    and entities via introducing four different query matrices for matching words
    and entities: one for each pair of input types (entity-entity, entity-word, word-entity,
    and the standard word-word). The proposed modifications give LUKE exceptional
    performance improvements over previous models in five tasks: Open Entity (entity
    typing) Choi et al. ([2018](#bib.bib28)), TACRED (relation classification) Zhang
    et al. ([2017](#bib.bib205)), CoNLL-2003 (named entity recognition) Tjong Kim Sang
    and De Meulder ([2003](#bib.bib174)), ReCoRD (cloze-style question answering)
    Zhang et al. ([2018](#bib.bib204)), and SQuAD 1.1 (reading comprehension) Rajpurkar
    et al. ([2016](#bib.bib147)).'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: Févry et al. ([2020](#bib.bib48)) propose a method for training a language model
    and entity representations jointly, which they call Entities as Experts (EaE).
    The model is based on the Transformer architecture and is similar to KnowBERT
    Peters et al. ([2019](#bib.bib139)). However, in addition to the trainable word
    embedding matrix, EaE features a separate trainable matrix for entity embeddings
    referred to as “memory”. The standard Transformer is also extended with an “entity
    memory” layer, which takes the output from the preceding Transformer layer and
    populates it with entity embeddings of mentions in the text. The retrieved entity
    embeddings are integrated into token representations by summation before layer
    normalization. To avoid dependence at inference on an external mention detector,
    the model applies a classifier to the output of Transformer blocks as in a sequence
    labeling model.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogously to Yamada et al. ([2020a](#bib.bib196)), the EaE is trained on
    a corpus annotated with mentions and entity links. The final loss function sums
    up of three components: the standard MLM objective, mention boundary detection
    loss as in a sequence labeling model $\mathcal{L}_{\_}{NER}$, and an entity linking
    objective that facilitates entity representations generated in the model to be
    close to entity embedding of an annotated entity.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L_{\_}{\mathrm{EaE}}}=\mathcal{L}_{\_}{\mathrm{MLM}}+\mathcal{L}_{\_}{\mathrm{NER}}+\mathcal{L}_{\_}{\mathrm{EL}}.$
    |  | (34) |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
- en: This approach to integrating knowledge about entities into LMs provides a significant
    performance boost in open domain question answering. EaE, having only 367 million
    of parameters, outperforms the 11 billion parameter version of T5 Raffel et al.
    ([2020](#bib.bib145)) on the TriviaQA task Joshi et al. ([2017](#bib.bib79)).
    The authors also show that EAE contains more factual knowledge than a comparably-sized
    BERT model.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: Poerner et al. ([2020](#bib.bib142)) present an E-BERT language model that also
    takes advantage of entity representations. This model is close to Zhang et al.
    ([2019](#bib.bib206)) as it also injects entities directly into the text and mixes
    entity representations with word embeddings in a similar way. However, instead
    of updating the weights of the whole pre-trained language model, they train only
    a linear transformation for aligning pre-trained entity representations with representations
    of word piece tokens of BERT. Such a small modification helps this model to outperform
    baselines on unsupervised question answering, supervised relation classification,
    and end-to-end entity linking.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: 'The considered works demonstrate that the integration of structured KGs and
    LMs usually helps to solve knowledge-oriented tasks: question answering (including
    open-domain QA), entity typing, relation extraction, and others. A high-precision
    supervision signal from KGs either leads to notable performance improvements or
    allows to reduce the number of trainable parameters of an LM while keeping a similar
    performance. Entity linking acts as a bridge between highly structured knowledge
    graphs and more flexible language models. We expect this approach to be crucial
    for the construction of future foundation models.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  id: totrans-639
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we have analyzed recently proposed neural entity linking models,
    which generally solve the task with higher accuracy than classical methods. We
    provide a generic neural entity linking architecture, which is applicable for
    most of the neural EL systems, including the description of its components, e.g.
    candidate generation, entity ranking, mention and entity encoding. Various modifications
    of the general architecture are grouped into four common directions: (1) joint
    entity mention detection and linking models, (2) global entity linking models,
    (3) domain-independent approaches, including zero-shot and distant supervision
    methods, and (4) cross-lingual techniques. Taxonomy figures and feature tables
    are provided to explain the categorization and to show which prominent features
    are used in each method.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: The majority of studies still rely on external knowledge for the candidate generation
    step. The mention encoders have made a shift from convolutional and recurrent
    models to self-attention architectures and start using pre-trained contextual
    language models like BERT. There is a current surge of methods that tackle the
    problem of adapting a model trained on one domain to another domain in a zero-shot
    fashion. These approaches do not need any annotated data in the target domain,
    but only descriptions of entities from this domain to perform such adaptation.
    It is shown in several works that the cross-encoder architecture is superior as
    compared to models with separate mention and entity encoders. The global context
    is widely used, but there are few recent studies that focus only on local EL.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: Among the solutions that perform mention detection and entity disambiguation
    jointly, the leadership is owned by the entity-enhanced BERT model (E-BERT) of
    Poerner et al. ([2020](#bib.bib142)) and the autoregressive model of De Cao et al.
    ([2021](#bib.bib33)) based on BART. Among published local models for disambiguation,
    the best results are reported by Shahbazi et al. ([2019](#bib.bib158)) and Wu
    et al. ([2020b](#bib.bib191)). The former solution leverages entity-aware ELMo
    (E-ELMo) trained to additionally predict entities along with words as in language-modelling
    task. The latter solution is based on a BERT bi-/cross-encoder and can be used
    in the zero-shot setting. Yamada et al. ([2021](#bib.bib198)) report results that
    are consistently better in comparison to all other solutions. Their high scores
    are attributed to the masked entity prediction mechanism for entity embedding
    and the usage of the pre-trained model based on BERT with a multi-step global
    scoring function.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: 7 Future Directions
  id: totrans-643
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We identify five promising directions of future work in entity linking listed
    below:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'More end-to-end models without an explicit candidate generation step: The candidate
    generation step relies on pre-constructed external resources or heuristics, as
    discussed in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Candidate Generation ‣ 3.1 General
    Architecture ‣ 3 Neural Entity Linking ‣ Neural Entity Linking: A Survey of Models
    Based on Deep Learning"). Both the recall and precision of EL systems depend on
    their completeness and ambiguity. The necessity of building such resources is
    also an obvious obstacle for applying models in zero-shot / cross-lingual settings.
    Several recent works demonstrate that it is possible to achieve high EL performance
    without external pre-built resources Gillick et al. ([2019](#bib.bib55)); Wu et al.
    ([2020b](#bib.bib191)) or eliminate the candidate generation step Broscheit ([2019](#bib.bib17));
    Botha et al. ([2020](#bib.bib16)). There is also a line of works devoted to methods
    that perform mention detection and entity disambiguation jointly Kolitsas et al.
    ([2018](#bib.bib82)); De Cao et al. ([2021](#bib.bib33)), which helps to avoid
    error propagation through multiple independent processing steps in an EL pipeline.
    We believe that a possible further research direction would be the development
    of entirely end-to-end trainable EL pipelines similar in spirit to the system
    of Broscheit ([2019](#bib.bib17)).'
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Further development of zero-shot approaches to address emerging entities: We
    also expect that zero-shot EL will rapidly evolve, engaging other features like
    global coherence across all entities in a document, NIL prediction, joining MD
    and ED steps together, or providing completely end-to-end solutions. The latter
    would be an especially challenging task but also a fascinating research direction.
    To allow for a proper comparison, more standardized benchmarks and evaluation
    processes for zero-shot methods are dearly needed.'
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-649
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'More use-cases of EL-enriched language models: Some studies Peters et al. ([2019](#bib.bib139));
    Zhang et al. ([2019](#bib.bib206)); Wang et al. ([2021](#bib.bib188)); Poerner
    et al. ([2020](#bib.bib142)) have shown improvements over contextual language
    models by including knowledge stored in KGs. They incorporate entity linking into
    these deep models to use information in KGs. In future work, more use-cases are
    expected to enhance language models by using entity linking. The enriched representations
    would be used in downstream tasks, enabling improvements there.'
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Integration of EL loss in more neural models: It may be interesting to integrate
    EL loss in other neural models distinct from the language models, but in a similar
    fashion as the models described in Section [5.2](#S5.SS2 "5.2 Novel Applications:
    Neural Entity Linking for Training Better Neural Language Models ‣ 5 Applications
    of Entity Linking ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").
    Due to the fact that an end-to-end EL model is also just a neural network, such
    integration with other networks is technically straightforward. Some multi task
    learning methods have been already proposed, e.g. joint relation extraction and
    entity linking Bansal et al. ([2020](#bib.bib10)). Since entity linking is a key
    step in information extraction, injecting information about entities contained
    in an EL model and multitask learning are expected to be useful for solving other
    related tasks.'
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-653
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multimodal EL: We witness the rise of a fascinating information extraction
    research direction that aims to build models capable of processing not only text,
    but also data from other modalities like images. For example, Moon et al. ([2018](#bib.bib113))
    and Adjali et al. ([2020](#bib.bib2)) leverage both text and images in social
    media posts for entity linking. Without taking into account an additional modality
    it would be impossible to correctly disambiguate entities in a very noisy and
    limited textual context. Entity linking methods in the near future potentially
    could take advantage of multimodal cross-attention and a surge of other techniques
    recently developed to improve processing multiple types of data in a single architecture
    Nagrani et al. ([2021](#bib.bib120)); Jaegle et al. ([2021](#bib.bib72)). We consider
    that vice-versa is also possible: EL could be seamlessly integrated into models
    for processing data with multiple modalities. EL not only provides disambiguation
    of mentions in the text but also connects a data instance to a knowledge graph,
    which opens the possibility of using reasoning elements during the solution of
    the final task.'
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{acks}'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: The work was partially supported by a Deutscher Akademischer Austauschdienst
    (DAAD) doctoral stipend and the DFG-funded JOIN-T project BI 1544/4\. The work
    of Artem Shelmanov in the current study (preparation of sections related to application
    of entity linking to neural language models, entity ranking, context-mention encoding,
    and overall harmonization of the text and results) is supported by the Russian
    Science Foundation (project 20-11-20166). Finally, this work was partially supported
    by the joint MTS-Skoltech laboratory.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Public Implementations of Neural Entity Linking Models
  id: totrans-657
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 7: Publicly available implementations (either provided in the paper or
    available at [PapersWithCode.com](https://paperswithcode.com)) of the neural models
    presented in Table [2](#S3.T2 "Table 2 ‣ 3.4 Summary ‣ 3 Neural Entity Linking
    ‣ Neural Entity Linking: A Survey of Models Based on Deep Learning").'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Link for Source Code |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
- en: '| [Sun et al.](#bib.bib171) ([2015](#bib.bib171)) Sun et al. ([2015](#bib.bib171))
    | - |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
- en: '| [Francis-Landau et al.](#bib.bib49) ([2016](#bib.bib49)) Francis-Landau et al.
    ([2016](#bib.bib49)) | [https://github.com/matthewfl/nlp-entity-convnet](https://github.com/matthewfl/nlp-entity-convnet)
    |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib42) ([2016](#bib.bib42)) Fang et al. ([2016](#bib.bib42))
    | - |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib194) ([2016](#bib.bib194)) Yamada et al. ([2016](#bib.bib194))
    | [https://github.com/wikipedia2vec/wikipedia2vec](https://github.com/wikipedia2vec/wikipedia2vec)
    |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
- en: '| [Zwicklbauer et al.](#bib.bib211) ([2016b](#bib.bib211)) Zwicklbauer et al.
    ([2016b](#bib.bib211)) | [https://github.com/quhfus/DoSeR](https://github.com/quhfus/DoSeR)
    |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
- en: '| [Tsai and Roth](#bib.bib176) ([2016](#bib.bib176)) Tsai and Roth ([2016](#bib.bib176))
    | - |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
- en: '| [Nguyen et al.](#bib.bib127) ([2016b](#bib.bib127)) Nguyen et al. ([2016b](#bib.bib127))
    | - |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
- en: '| [Globerson et al.](#bib.bib56) ([2016](#bib.bib56)) Globerson et al. ([2016](#bib.bib56))
    | - |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib19) ([2017](#bib.bib19)) Cao et al. ([2017](#bib.bib19))
    | [https://github.com/TaoMiner/bridgeGap](https://github.com/TaoMiner/bridgeGap)
    |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
- en: '| [Eshel et al.](#bib.bib40) ([2017](#bib.bib40)) Eshel et al. ([2017](#bib.bib40))
    | [https://github.com/yotam-happy/NEDforNoisyText](https://github.com/yotam-happy/NEDforNoisyText)
    |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
- en: '| [Ganea and Hofmann](#bib.bib53) ([2017](#bib.bib53)) Ganea and Hofmann ([2017](#bib.bib53))
    | [https://github.com/dalab/deep-ed](https://github.com/dalab/deep-ed) |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
- en: '| [Moreno et al.](#bib.bib114) ([2017](#bib.bib114)) Moreno et al. ([2017](#bib.bib114))
    | - |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
- en: '| [Gupta et al.](#bib.bib62) ([2017](#bib.bib62)) Gupta et al. ([2017](#bib.bib62))
    | [https://github.com/nitishgupta/neural-el](https://github.com/nitishgupta/neural-el)
    |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
- en: '| [Nie et al.](#bib.bib129) ([2018](#bib.bib129)) Nie et al. ([2018](#bib.bib129))
    | - |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| [Sorokin and Gurevych](#bib.bib168) ([2018](#bib.bib168)) Sorokin and Gurevych
    ([2018](#bib.bib168)) | [https://github.com/UKPLab/starsem2018-entity-linking](https://github.com/UKPLab/starsem2018-entity-linking)
    |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib157) ([2018](#bib.bib157)) Shahbazi et al. ([2018](#bib.bib157))
    | - |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib85) ([2018](#bib.bib85)) Le and Titov ([2018](#bib.bib85))
    | [https://github.com/lephong/mulrel-nel](https://github.com/lephong/mulrel-nel)
    |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| [Newman-Griffis et al.](#bib.bib125) ([2018](#bib.bib125)) Newman-Griffis
    et al. ([2018](#bib.bib125)) | [https://github.com/OSU-slatelab/JET](https://github.com/OSU-slatelab/JET)
    |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| [Radhakrishnan et al.](#bib.bib144) ([2018](#bib.bib144)) Radhakrishnan et al.
    ([2018](#bib.bib144)) | [https://github.com/priyaradhakrishnan0/ELDEN](https://github.com/priyaradhakrishnan0/ELDEN)
    |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| [Kolitsas et al.](#bib.bib82) ([2018](#bib.bib82)) Kolitsas et al. ([2018](#bib.bib82))
    | [https://github.com/dalab/end2endneuralel](https://github.com/dalab/end2endneuralel)
    |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| [Sil et al.](#bib.bib164) ([2018](#bib.bib164)) Sil et al. ([2018](#bib.bib164))
    | - |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: '| [Upadhyay et al.](#bib.bib179) ([2018a](#bib.bib179)) Upadhyay et al. ([2018a](#bib.bib179))
    | [https://github.com/shyamupa/xelms](https://github.com/shyamupa/xelms) |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
- en: '| [Cao et al.](#bib.bib20) ([2018](#bib.bib20)) Cao et al. ([2018](#bib.bib20))
    | [https://github.com/TaoMiner/NCEL](https://github.com/TaoMiner/NCEL) |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
- en: '| [Raiman and Raiman](#bib.bib146) ([2018](#bib.bib146)) Raiman and Raiman
    ([2018](#bib.bib146)) | [https://github.com/openai/deeptype](https://github.com/openai/deeptype)
    |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| [Mueller and Durrett](#bib.bib116) ([2018](#bib.bib116)) Mueller and Durrett
    ([2018](#bib.bib116)) | [https://github.com/davidandym/wikilinks-ned](https://github.com/davidandym/wikilinks-ned)
    |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| [Shahbazi et al.](#bib.bib158) ([2019](#bib.bib158)) Shahbazi et al. ([2019](#bib.bib158))
    | - |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| [Logeswaran et al.](#bib.bib100) ([2019](#bib.bib100)) Logeswaran et al.
    ([2019](#bib.bib100)) | [https://github.com/lajanugen/zeshel](https://github.com/lajanugen/zeshel)
    |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| [Gillick et al.](#bib.bib55) ([2019](#bib.bib55)) Gillick et al. ([2019](#bib.bib55))
    | [https://github.com/google-research/google-research/tree/master/denserepresentationsforentityretrieval](https://github.com/google-research/google-research/tree/master/denserepresentationsforentityretrieval)
    |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| [Peters et al.](#bib.bib139) ([2019](#bib.bib139)) Peters et al. ([2019](#bib.bib139))
    | [https://github.com/allenai/kb](https://github.com/allenai/kb) |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib87) ([2019b](#bib.bib87)) Le and Titov ([2019b](#bib.bib87))
    | [https://github.com/lephong/dl4el](https://github.com/lephong/dl4el) |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| [Le and Titov](#bib.bib86) ([2019a](#bib.bib86)) Le and Titov ([2019a](#bib.bib86))
    | [https://github.com/lephong/wnel](https://github.com/lephong/wnel) |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib43) ([2019](#bib.bib43)) Fang et al. ([2019](#bib.bib43))
    | - |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| [Martins et al.](#bib.bib107) ([2019](#bib.bib107)) Martins et al. ([2019](#bib.bib107))
    | - |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| [Yang et al.](#bib.bib200) ([2019](#bib.bib200)) Yang et al. ([2019](#bib.bib200))
    | [https://github.com/YoungXiyuan/DCA](https://github.com/YoungXiyuan/DCA) |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| [Xue et al.](#bib.bib192) ([2019](#bib.bib192)) Xue et al. ([2019](#bib.bib192))
    | [https://github.com/DeepLearnXMU/RRWEL](https://github.com/DeepLearnXMU/RRWEL)
    |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| [Zhou et al.](#bib.bib207) ([2019](#bib.bib207)) Zhou et al. ([2019](#bib.bib207))
    | [https://github.com/shuyanzhou/burnxel](https://github.com/shuyanzhou/burnxel)
    |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| [Broscheit](#bib.bib17) ([2019](#bib.bib17)) Broscheit ([2019](#bib.bib17))
    | [https://github.com/samuelbroscheit/entityknowledgeinbert](https://github.com/samuelbroscheit/entityknowledgeinbert)
    |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| [Hou et al.](#bib.bib69) ([2020](#bib.bib69)) Hou et al. ([2020](#bib.bib69))
    | [https://github.com/fhou80/EntEmb](https://github.com/fhou80/EntEmb) |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: '| [Onoe and Durrett](#bib.bib131) ([2020](#bib.bib131)) Onoe and Durrett ([2020](#bib.bib131))
    | [https://github.com/yasumasaonoe/ET4EL](https://github.com/yasumasaonoe/ET4EL)
    |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib23) ([2020](#bib.bib23)) Chen et al. ([2020](#bib.bib23))
    | - |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib191) ([2020b](#bib.bib191)) Wu et al. ([2020b](#bib.bib191))
    | [https://github.com/facebookresearch/BLINK](https://github.com/facebookresearch/BLINK)
    |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
- en: '| [Banerjee et al.](#bib.bib9) ([2020](#bib.bib9)) Banerjee et al. ([2020](#bib.bib9))
    | [https://github.com/debayan/pnel](https://github.com/debayan/pnel) |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
- en: '| [Wu et al.](#bib.bib190) ([2020a](#bib.bib190)) Wu et al. ([2020a](#bib.bib190))
    | [https://github.com/wujsAct/DGCNEL](https://github.com/wujsAct/DGCNEL) |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
- en: '| [Fang et al.](#bib.bib44) ([2020](#bib.bib44)) Fang et al. ([2020](#bib.bib44))
    | [https://github.com/fangzheng123/SGEL](https://github.com/fangzheng123/SGEL)
    |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| [Chen et al.](#bib.bib25) ([2020](#bib.bib25)) Chen et al. ([2020](#bib.bib25))
    | - |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| [Botha et al.](#bib.bib16) ([2020](#bib.bib16)) Botha et al. ([2020](#bib.bib16))
    | [http://goo.gle/mewsli-dataset](http://goo.gle/mewsli-dataset) |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| [Yao et al.](#bib.bib201) ([2020](#bib.bib201)) Yao et al. ([2020](#bib.bib201))
    | [https://github.com/seasonyao/Zero-Shot-Entity-Linking](https://github.com/seasonyao/Zero-Shot-Entity-Linking)
    |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
- en: '| [Li et al.](#bib.bib94) ([2020](#bib.bib94)) Li et al. ([2020](#bib.bib94))
    | [https://github.com/facebookresearch/BLINK/tree/master/elq](https://github.com/facebookresearch/BLINK/tree/master/elq)
    |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
- en: '| [Poerner et al.](#bib.bib142) ([2020](#bib.bib142)) Poerner et al. ([2020](#bib.bib142))
    | [https://github.com/npoe/ebert](https://github.com/npoe/ebert) |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
- en: '| [Fu et al.](#bib.bib50) ([2020](#bib.bib50)) Fu et al. ([2020](#bib.bib50))
    | [http://cogcomp.org/page/publicationview/911](http://cogcomp.org/page/publicationview/911)
    |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
- en: '| [Mulang’ et al.](#bib.bib117) ([2020](#bib.bib117)) Mulang’ et al. ([2020](#bib.bib117))
    | [https://github.com/mulangonando/Impact-of-KG-Context-on-ED](https://github.com/mulangonando/Impact-of-KG-Context-on-ED)
    |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
- en: '| [Yamada et al.](#bib.bib198) ([2021](#bib.bib198)) Yamada et al. ([2021](#bib.bib198))
    | [https://github.com/studio-ousia/luke](https://github.com/studio-ousia/luke)
    |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
- en: '| [Gu et al.](#bib.bib60) ([2021](#bib.bib60)) Gu et al. ([2021](#bib.bib60))
    | - |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
- en: '| [Tang et al.](#bib.bib173) ([2021](#bib.bib173)) Tang et al. ([2021](#bib.bib173))
    | - |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
- en: '| [De Cao et al.](#bib.bib33) ([2021](#bib.bib33)) De Cao et al. ([2021](#bib.bib33))
    | [https://github.com/facebookresearch/GENRE](https://github.com/facebookresearch/GENRE)
    |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
- en: References
  id: totrans-716
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abel et al. (2012) F. Abel, C. Hauff, G.-J. Houben, R. Stronkman and K. Tao,
    Twitcident: Fighting Fire with Information from Social Web Streams, in: Proceedings
    of the 21st International Conference on World Wide Web, WWW ’12 Companion, Association
    for Computing Machinery, New York, NY, USA, 2012, pp. 305–308–. ISBN ISBN 9781450312301.
    doi:10.1145/2187980.2188035.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adjali et al. (2020) O. Adjali, R. Besançon, O. Ferret, H. Le Borgne and B. Grau,
    Multimodal Entity Linking for Tweets, in: Advances in Information Retrieval, J.M. Jose,
    E. Yilmaz, J. Magalhães, P. Castells, N. Ferro, M.J. Silva and F. Martins, eds,
    Springer International Publishing, Cham, 2020, pp. 463–478. ISBN ISBN 978-3-030-45439-5.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al-Moslmi et al. (2020) T. Al-Moslmi, M. Gallofré Ocaña, A.L. Opdahl and C. Veres,
    Named Entity Extraction for Knowledge Graphs: A Literature Overview, IEEE Access
    8 (2020), 32862–32881. doi:10.1109/ACCESS.2020.2973928.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aly et al. (2021) R. Aly, A. Vlachos and R. McDonald, Leveraging Type Descriptions
    for Zero-shot Named Entity Recognition and Classification, in: Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers), Association for Computational Linguistics, Online, 2021, pp. 1516–1528.
    doi:10.18653/v1/2021.acl-long.120. [https://aclanthology.org/2021.acl-long.120](https://aclanthology.org/2021.acl-long.120).'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amiri et al. (2021) H. Amiri, M. Mohtarami and I. Kohane, Attentive Multiview
    Text Representation for Differential Diagnosis, in: Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 2: Short Papers), Association
    for Computational Linguistics, Online, 2021, pp. 1012–1019. doi:10.18653/v1/2021.acl-short.128.
    [https://aclanthology.org/2021.acl-short.128](https://aclanthology.org/2021.acl-short.128).'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aronson and Lang (2010) A.R. Aronson and F.-M. Lang, An overview of MetaMap:
    historical perspective and recent advances, Journal of the American Medical Informatics
    Association 17(3) (2010), 229–236. doi:10.1136/jamia.2009.002733.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bahdanau et al. (2015) D. Bahdanau, K. Cho and Y. Bengio, Neural machine translation
    by jointly learning to align and translate, in: 3rd International Conference on
    Learning Representations, ICLR 2015, San-Diego, California, USA, 2015. [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473).'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banarescu et al. (2013) L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt,
    U. Hermjakob, K. Knight, P. Koehn, M. Palmer and N. Schneider, Abstract Meaning
    Representation for Sembanking, in: Proceedings of the 7th Linguistic Annotation
    Workshop and Interoperability with Discourse, Association for Computational Linguistics,
    Sofia, Bulgaria, 2013, pp. 178–186. [https://aclanthology.org/W13-2322](https://aclanthology.org/W13-2322).'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee et al. (2020) D. Banerjee, D. Chaudhuri, M. Dubey and J. Lehmann,
    PNEL: Pointer Network Based End-To-End Entity Linking over Knowledge Graphs, in:
    The Semantic Web – ISWC 2020 - 19th International Semantic Web Conference, Athens,
    Greece, November 2-6, 2020, Proceedings, Part I, Vol. 12506, J.Z. Pan, V. Tamma,
    C. d’Amato, K. Janowicz, B. Fu, A. Polleres, O. Seneviratne and L. Kagal, eds,
    Springer International Publishing, Cham, 2020, pp. 21–38. ISBN ISBN 978-3-030-62419-4.
    doi:10.1007/978-3-030-62419-42.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal et al. (2020) T. Bansal, P. Verga, N. Choudhary and A. McCallum, Simultaneously
    Linking Entities and Extracting Relations from Biomedical Text without Mention-Level
    Supervision, Proceedings of the AAAI Conference on Artificial Intelligence 34(05)
    (2020), 7407–7414. doi:10.1609/aaai.v34i05.6236. [https://ojs.aaai.org/index.php/AAAI/article/view/6236](https://ojs.aaai.org/index.php/AAAI/article/view/6236).
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2003) Y. Bengio, R. Ducharme, P. Vincent and C. Janvin, A Neural
    Probabilistic Language Model, J. Mach. Learn. Res. – Journal of Machine Learning
    Research 3(null) (2003), 1137–1155–.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bodenreider (2004) O. Bodenreider, The Unified Medical Language System (UMLS):
    integrating biomedical terminology, Nucleic Acids Research 32(suppl1) (2004),
    D267–D270. doi:10.1093/nar/gkh061.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bojanowski et al. (2017) P. Bojanowski, E. Grave, A. Joulin and T. Mikolov,
    Enriching Word Vectors with Subword Information, Transactions of the Association
    for Computational Linguistics 5 (2017), 135–146. doi:10.1162/tacla00051. [https://aclanthology.org/Q17-1010](https://aclanthology.org/Q17-1010).
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bollacker et al. (2008) K. Bollacker, C. Evans, P. Paritosh, T. Sturge and
    J. Taylor, Freebase: A Collaboratively Created Graph Database for Structuring
    Human Knowledge, in: Proceedings of the 2008 ACM SIGMOD International Conference
    on Management of Data, SIGMOD ’08, Association for Computing Machinery, New York,
    NY, USA, 2008, pp. 1247–1250–. ISBN ISBN 9781605581026. doi:10.1145/1376616.1376746.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bordes et al. (2013) A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston and
    O. Yakhnenko, Translating Embeddings for Modeling Multi-relational Data, in: Advances
    in neural information processing systems, Vol. 26, C.J.C. Burges, L. Bottou, M. Welling,
    Z. Ghahramani and K.Q. Weinberger, eds, Stateline, Nevada, USA, 2013, pp. 2787–2795.
    [https://papers.nips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf](https://papers.nips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf).'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Botha et al. (2020) J.A. Botha, Z. Shan and D. Gillick, Entity Linking in 100
    Languages, in: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), Association for Computational Linguistics, Online,
    2020, pp. 7833–7845. doi:10.18653/v1/2020.emnlp-main.630. [https://aclanthology.org/2020.emnlp-main.630](https://aclanthology.org/2020.emnlp-main.630).'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Broscheit (2019) S. Broscheit, Investigating Entity Knowledge in BERT with
    Simple Neural End-To-End Entity Linking, in: Proceedings of the 23rd Conference
    on Computational Natural Language Learning (CoNLL), Association for Computational
    Linguistics, Hong Kong, China, 2019, pp. 677–685. doi:10.18653/v1/K19-1063. [https://aclanthology.org/K19-1063](https://aclanthology.org/K19-1063).'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2018) H. Cai, V.W. Zheng and K. Chang, A Comprehensive Survey of
    Graph Embedding: Problems, Techniques, and Applications, IEEE Transactions on
    Knowledge & Data Engineering 30(09) (2018), 1616–1637. doi:10.1109/TKDE.2018.2807452.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2017) Y. Cao, L. Huang, H. Ji, X. Chen and J. Li, Bridge Text and
    Knowledge by Learning Multi-Prototype Entity Mention Embedding, in: Proceedings
    of the 55th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), Association for Computational Linguistics, Vancouver, Canada,
    2017, pp. 1623–1633. doi:10.18653/v1/P17-1149. [https://aclanthology.org/P17-1149](https://aclanthology.org/P17-1149).'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2018) Y. Cao, L. Hou, J. Li and Z. Liu, Neural Collective Entity
    Linking, in: Proceedings of the 27th International Conference on Computational
    Linguistics, Association for Computational Linguistics, Santa Fe, New Mexico,
    USA, 2018, pp. 675–686. [https://aclanthology.org/C18-1057](https://aclanthology.org/C18-1057).'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceccarelli et al. (2013) D. Ceccarelli, C. Lucchese, S. Orlando, R. Perego
    and S. Trani, Learning Relatedness Measures for Entity Linking, in: Proceedings
    of the 22nd ACM International Conference on Information & Knowledge Management,
    CIKM ’13, Association for Computing Machinery, New York, NY, USA, 2013, pp. 139–148.
    ISBN ISBN 9781450322638. doi:10.1145/2505515.2505711.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2016) A. Chang, V.I. Spitkovsky, C.D. Manning and E. Agirre,
    A comparison of Named-Entity Disambiguation and Word Sense Disambiguation, in:
    Proceedings of the Tenth International Conference on Language Resources and Evaluation
    (LREC’16), European Language Resources Association (ELRA), Portorož, Slovenia,
    2016, pp. 860–867. [https://aclanthology.org/L16-1139](https://aclanthology.org/L16-1139).'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) H. Chen, X. Li, A. Zukov Gregoric and S. Wadhwa, Contextualized
    End-to-End Neural Entity Linking, in: Proceedings of the 1st Conference of the
    Asia-Pacific Chapter of the Association for Computational Linguistics and the
    10th International Joint Conference on Natural Language Processing, Association
    for Computational Linguistics, Suzhou, China, 2020, pp. 637–642. [https://aclanthology.org/2020.aacl-main.64](https://aclanthology.org/2020.aacl-main.64).'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) L. Chen, G. Varoquaux and F.M. Suchanek, A Lightweight Neural
    Model for Biomedical Entity Linking, 2021, pp. 12657–12665. [https://ojs.aaai.org/index.php/AAAI/article/view/17499](https://ojs.aaai.org/index.php/AAAI/article/view/17499).
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) S. Chen, J. Wang, F. Jiang and C.-Y. Lin, Improving Entity
    Linking by Modeling Latent Entity Type Information, Proceedings of the AAAI Conference
    on Artificial Intelligence 34(05) (2020), 7529–7537. doi:10.1609/aaai.v34i05.6251.
    [https://ojs.aaai.org/index.php/AAAI/article/view/6251](https://ojs.aaai.org/index.php/AAAI/article/view/6251).
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng and Roth (2013) X. Cheng and D. Roth, Relational Inference for Wikification,
    in: Proceedings of the 2013 Conference on Empirical Methods in Natural Language
    Processing, Association for Computational Linguistics, Seattle, Washington, USA,
    2013, pp. 1787–1796. [https://aclanthology.org/D13-1184](https://aclanthology.org/D13-1184).'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chisholm and Hachey (2015) A. Chisholm and B. Hachey, Entity Disambiguation
    with Web Links, Transactions of the Association for Computational Linguistics
    3 (2015), 145–156. doi:10.1162/tacla00129. [https://aclanthology.org/Q15-1011](https://aclanthology.org/Q15-1011).
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2018) E. Choi, O. Levy, Y. Choi and L. Zettlemoyer, Ultra-Fine
    Entity Typing, in: Proceedings of the 56th Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), Association for Computational
    Linguistics, Melbourne, Australia, 2018, pp. 87–96. doi:10.18653/v1/P18-1009.
    [https://aclanthology.org/P18-1009](https://aclanthology.org/P18-1009).'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chung et al. (2014) J. Chung, C. Gulcehre, K. Cho and Y. Bengio, Empirical
    evaluation of gated recurrent neural networks on sequence modeling, in: NIPS 2014
    Workshop on Deep Learning, Montréal, Canada, 2014. [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555).'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert et al. (2011) R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu
    and P. Kuksa, Natural Language Processing (Almost) from Scratch, J. Mach. Learn.
    Res. – Journal of Machine Learning Research 12(null) (2011), 2493–2537–.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cotterell and Duh (2017) R. Cotterell and K. Duh, Low-Resource Named Entity
    Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields,
    in: Proceedings of the Eighth International Joint Conference on Natural Language
    Processing (Volume 2: Short Papers), Asian Federation of Natural Language Processing,
    Taipei, Taiwan, 2017, pp. 91–96. [https://aclanthology.org/I17-2016](https://aclanthology.org/I17-2016).'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cucerzan (2007) S. Cucerzan, Large-Scale Named Entity Disambiguation Based
    on Wikipedia Data, in: Proceedings of the 2007 Joint Conference on Empirical Methods
    in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),
    Association for Computational Linguistics, Prague, Czech Republic, 2007, pp. 708–716.
    [https://aclanthology.org/D07-1074](https://aclanthology.org/D07-1074).'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Cao et al. (2021) N. De Cao, G. Izacard, S. Riedel and F. Petroni, Autoregressive
    Entity Retrieval, in: International Conference on Learning Representations, 2021.
    [https://openreview.net/forum?id=5k8F6UU39V](https://openreview.net/forum?id=5k8F6UU39V).'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dessì et al. (2021) D. Dessì, F. Osborne, D. Reforgiato Recupero, D. Buscaldi
    and E. Motta, Generating knowledge graphs by employing Natural Language Processing
    and Machine Learning techniques within the scholarly domain, Future Generation
    Computer Systems 116 (2021), 253–264. doi:10.1016/j.future.2020.10.026. [https://www.sciencedirect.com/science/article/pii/S0167739X2033003X](https://www.sciencedirect.com/science/article/pii/S0167739X2033003X).
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dettmers et al. (2018) T. Dettmers, P. Minervini, P. Stenetorp and S. Riedel,
    Convolutional 2D Knowledge Graph Embeddings, Proceedings of the AAAI Conference
    on Artificial Intelligence 32(1) (2018). [https://ojs.aaai.org/index.php/AAAI/article/view/11573](https://ojs.aaai.org/index.php/AAAI/article/view/11573).
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding, in:
    Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota,
    2019, pp. 4171–4186. doi:10.18653/v1/N19-1423. [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ehrmann et al. (2020) M. Ehrmann, M. Romanello, A. Flückiger and S. Clematide,
    Overview of CLEF HIPE 2020: Named Entity Recognition and Linking on Historical
    Newspapers, in: Experimental IR Meets Multilinguality, Multimodality, and Interaction,
    A. Arampatzis, E. Kanoulas, T. Tsikrika, S. Vrochidis, H. Joho, C. Lioma, C. Eickhoff,
    A. Névéol, L. Cappellato and N. Ferro, eds, Springer International Publishing,
    Cham, 2020, pp. 288–310. ISBN ISBN 978-3-030-58219-7. doi:10.1007/978-3-030-58219-721.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'El Vaigh et al. (2019) C.B. El Vaigh, F. Goasdoué, G. Gravier and P. Sébillot,
    Using Knowledge Base Semantics in Context-Aware Entity Linking, in: Proceedings
    of the ACM Symposium on Document Engineering 2019, DocEng ’19, Association for
    Computing Machinery, New York, NY, USA, 2019. ISBN ISBN 9781450368872. doi:10.1145/3342558.3345393.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ellis et al. (2015) J. Ellis, J. Getman, D. Fore, N. Kuster, Z. Song, A. Bies
    and S.M. Strassel, Overview of Linguistic Resources for the TAC KBP 2015 Evaluations:
    Methodologies and Results, in: Proceedings of the 2015 Text Analysis Conference,
    TAC 2015, NIST, Gaithersburg, Maryland, USA, 2015. [https://tac.nist.gov/publications/2015/additional.papers/TAC2015.KBP_resources_overview.proceedings.pdf](https://tac.nist.gov/publications/2015/additional.papers/TAC2015.KBP_resources_overview.proceedings.pdf).'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eshel et al. (2017) Y. Eshel, N. Cohen, K. Radinsky, S. Markovitch, I. Yamada
    and O. Levy, Named Entity Disambiguation for Noisy Text, in: Proceedings of the
    21st Conference on Computational Natural Language Learning (CoNLL 2017), Association
    for Computational Linguistics, Vancouver, Canada, 2017, pp. 58–68. doi:10.18653/v1/K17-1008.
    [https://aclanthology.org/K17-1008](https://aclanthology.org/K17-1008).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eyre et al. ((in press, n.d.)) H. Eyre, A.B. Chapman, K.S. Peterson, J. Shi,
    P.R. Alba, M.M. Jones, T.L. Box, S.L. DuVall and O.V. Patterson, Launching into
    clinical space with medspaCy: a new clinical text processing toolkit in Python,
    arXiv preprint arXiv:2106.07799 ((in press, n.d.)).'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2016) W. Fang, J. Zhang, D. Wang, Z. Chen and M. Li, Entity Disambiguation
    by Knowledge and Text Jointly Embedding, in: Proceedings of The 20th SIGNLL Conference
    on Computational Natural Language Learning, Association for Computational Linguistics,
    Berlin, Germany, 2016, pp. 260–269. doi:10.18653/v1/K16-1026. [https://aclanthology.org/K16-1026](https://aclanthology.org/K16-1026).'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2019) Z. Fang, Y. Cao, Q. Li, D. Zhang, Z. Zhang and Y. Liu, Joint
    Entity Linking with Deep Reinforcement Learning, in: The World Wide Web Conference,
    WWW ’19, Association for Computing Machinery, New York, NY, USA, 2019, pp. 438–447–.
    ISBN ISBN 9781450366748. doi:10.1145/3308558.3313517.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2020) Z. Fang, Y. Cao, R. Li, Z. Zhang, Y. Liu and S. Wang, High
    Quality Candidate Generation and Sequential Graph Attention Network for Entity
    Linking, in: Proceedings of The Web Conference 2020, WWW ’20, Association for
    Computing Machinery, New York, NY, USA, 2020, pp. 640–650–. ISBN ISBN 9781450370233.
    doi:10.1145/3366423.3380146.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Färber et al. (2018) M. Färber, F. Bartscherer, C. Menne and A. Rettinger, Linked
    Data Quality of DBpedia, Freebase, OpenCyc, Wikidata, and YAGO, Semantic Web 9(1)
    (2018), 77–129. doi:10.3233/SW-170275. [https://content.iospress.com/articles/semantic-web/sw275](https://content.iospress.com/articles/semantic-web/sw275).
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fellbaum (1998) C. Fellbaum (ed.), WordNet: An Electronic Lexical Database,
    MIT Press, Cambridge, MA, 1998. ISBN ISBN 978-0-262-06197-1.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferragina and Scaiella (2010) P. Ferragina and U. Scaiella, TAGME: On-the-Fly
    Annotation of Short Text Fragments (by Wikipedia Entities), in: Proceedings of
    the 19th ACM International Conference on Information and Knowledge Management,
    CIKM ’10, Association for Computing Machinery, New York, NY, USA, 2010, pp. 1625–1628.
    ISBN ISBN 9781450300995. doi:10.1145/1871437.1871689.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Févry et al. (2020) T. Févry, L. Baldini Soares, N. FitzGerald, E. Choi and
    T. Kwiatkowski, Entities as Experts: Sparse Memory Access with Entity Supervision,
    in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), Association for Computational Linguistics, Online, 2020, pp. 4937–4951.
    doi:10.18653/v1/2020.emnlp-main.400. [https://aclanthology.org/2020.emnlp-main.400](https://aclanthology.org/2020.emnlp-main.400).'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Francis-Landau et al. (2016) M. Francis-Landau, G. Durrett and D. Klein, Capturing
    Semantic Similarity for Entity Linking with Convolutional Neural Networks, in:
    Proceedings of the 2016 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Association for Computational
    Linguistics, San Diego, California, 2016, pp. 1256–1261. doi:10.18653/v1/N16-1150.
    [https://aclanthology.org/N16-1150](https://aclanthology.org/N16-1150).'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2020) X. Fu, W. Shi, X. Yu, Z. Zhao and D. Roth, Design Challenges
    in Low-resource Cross-lingual Entity Linking, in: Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational
    Linguistics, Online, 2020, pp. 6418–6432. doi:10.18653/v1/2020.emnlp-main.521.
    [https://aclanthology.org/2020.emnlp-main.521](https://aclanthology.org/2020.emnlp-main.521).'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fumera et al. (2000) G. Fumera, F. Roli and G. Giacinto, Reject option with
    multiple thresholds, Pattern recognition 33(12) (2000), 2099–2101.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gabrilovich et al. (2013) E. Gabrilovich, M. Ringgaard and A. Subramanya, FACC1:
    Freebase annotation of ClueWeb corpora, version 1 (release date 2013-06-26, format
    version 1, correction level 0), 2013, Note: http://lemurproject.org/clueweb09/.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganea and Hofmann (2017) O.-E. Ganea and T. Hofmann, Deep Joint Entity Disambiguation
    with Local Neural Attention, in: Proceedings of the 2017 Conference on Empirical
    Methods in Natural Language Processing, Association for Computational Linguistics,
    Copenhagen, Denmark, 2017, pp. 2619–2629. doi:10.18653/v1/D17-1277. [https://aclanthology.org/D17-1277](https://aclanthology.org/D17-1277).'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganea et al. (2016) O.-E. Ganea, M. Ganea, A. Lucchi, C. Eickhoff and T. Hofmann,
    Probabilistic Bag-Of-Hyperlinks Model for Entity Linking, in: Proceedings of the
    25th International Conference on World Wide Web, WWW ’16, International World
    Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 2016,
    pp. 927–938. ISBN ISBN 9781450341431. doi:10.1145/2872427.2882988.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gillick et al. (2019) D. Gillick, S. Kulkarni, L. Lansing, A. Presta, J. Baldridge,
    E. Ie and D. Garcia-Olano, Learning Dense Representations for Entity Retrieval,
    in: Proceedings of the 23rd Conference on Computational Natural Language Learning
    (CoNLL), Association for Computational Linguistics, Hong Kong, China, 2019, pp. 528–537.
    doi:10.18653/v1/K19-1049. [https://aclanthology.org/K19-1049](https://aclanthology.org/K19-1049).'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Globerson et al. (2016) A. Globerson, N. Lazic, S. Chakrabarti, A. Subramanya,
    M. Ringgaard and F. Pereira, Collective Entity Resolution with Multi-Focal Attention,
    in: Proceedings of the 54th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), Association for Computational Linguistics,
    Berlin, Germany, 2016, pp. 621–631. doi:10.18653/v1/P16-1059. [https://aclanthology.org/P16-1059](https://aclanthology.org/P16-1059).'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. (2018) A. Goyal, V. Gupta and M. Kumar, Recent Named Entity Recognition
    and Classification techniques: A systematic review, Computer Science Review 29
    (2018), 21–43. doi:10.1016/j.cosrev.2018.06.001. [https://www.sciencedirect.com/science/article/pii/S1574013717302782](https://www.sciencedirect.com/science/article/pii/S1574013717302782).'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal and Ferrara (2018) P. Goyal and E. Ferrara, Graph embedding techniques,
    applications, and performance: A survey, Knowledge-Based Systems 151 (2018), 78–94.
    doi:10.1016/j.knosys.2018.03.022. [https://www.sciencedirect.com/science/article/pii/S0950705118301540](https://www.sciencedirect.com/science/article/pii/S0950705118301540).'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grover and Leskovec (2016) A. Grover and J. Leskovec, Node2vec: Scalable Feature
    Learning for Networks, in: Proceedings of the 22nd ACM SIGKDD International Conference
    on Knowledge Discovery and Data Mining, KDD ’16, Association for Computing Machinery,
    New York, NY, USA, 2016, pp. 855–864. ISBN ISBN 9781450342322. doi:10.1145/2939672.2939754.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021) Y. Gu, X. Qu, Z. Wang, B. Huai, N.J. Yuan and X. Gui, Read,
    Retrospect, Select: An MRC Framework to Short Text Entity Linking, Proceedings
    of the AAAI Conference on Artificial Intelligence 35(14) (2021), 12920–12928.
    [https://ojs.aaai.org/index.php/AAAI/article/view/17528](https://ojs.aaai.org/index.php/AAAI/article/view/17528).'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Barbosa (2018) Z. Guo and D. Barbosa, Robust named entity disambiguation
    with random walks, Semantic Web 9(4) (2018), 459–479. doi:10.3233/SW-170273. [https://content.iospress.com/articles/semantic-web/sw273](https://content.iospress.com/articles/semantic-web/sw273).
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2017) N. Gupta, S. Singh and D. Roth, Entity Linking via Joint
    Encoding of Types, Descriptions, and Context, in: Proceedings of the 2017 Conference
    on Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Copenhagen, Denmark, 2017, pp. 2681–2690. doi:10.18653/v1/D17-1284.
    [https://www.aclweb.org/anthology/D17-1284](https://www.aclweb.org/anthology/D17-1284).'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2018) X. Han, H. Zhu, P. Yu, Z. Wang, Y. Yao, Z. Liu and M. Sun,
    FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with
    State-of-the-Art Evaluation, in: Proceedings of the 2018 Conference on Empirical
    Methods in Natural Language Processing, Association for Computational Linguistics,
    Brussels, Belgium, 2018, pp. 4803–4809. doi:10.18653/v1/D18-1514. [https://aclanthology.org/D18-1514](https://aclanthology.org/D18-1514).'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hellman (1970) M.E. Hellman, The Nearest Neighbor Classification Rule with a
    Reject Option, IEEE Transactions on Systems Science and Cybernetics 6(3) (1970),
    179–185. doi:10.1109/TSSC.1970.300339.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Herbei and Wegkamp (2006) R. Herbei and M.H. Wegkamp, Classification with Reject
    Option, The Canadian Journal of Statistics / La Revue Canadienne de Statistique
    34(4) (2006), 709–721. [http://www.jstor.org/stable/20445230](http://www.jstor.org/stable/20445230).
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) S. Hochreiter and J. Schmidhuber, Long Short-Term
    Memory, Neural Computation 9(8) (1997), 1735–1780–. doi:10.1162/neco.1997.9.8.1735.
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoffart et al. (2011) J. Hoffart, M.A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal,
    M. Spaniol, B. Taneva, S. Thater and G. Weikum, Robust Disambiguation of Named
    Entities in Text, in: Proceedings of the 2011 Conference on Empirical Methods
    in Natural Language Processing, Association for Computational Linguistics, Edinburgh,
    Scotland, UK., 2011, pp. 782–792. [https://aclanthology.org/D11-1072](https://aclanthology.org/D11-1072).'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hogan et al. (2021) A. Hogan, E. Blomqvist, M. Cochez, C. D’amato, G.D. Melo,
    C. Gutierrez, S. Kirrane, J.E.L. Gayo, R. Navigli, S. Neumaier, A.-C.N. Ngomo,
    A. Polleres, S.M. Rashid, A. Rula, L. Schmelzeisen, J. Sequeda, S. Staab and A. Zimmermann,
    Knowledge Graphs, ACM Computing Surveys 54(4) (2021). doi:10.1145/3447772.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2020) F. Hou, R. Wang, J. He and Y. Zhou, Improving Entity Linking
    through Semantic Reinforced Entity Embeddings, in: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, Association for Computational
    Linguistics, Online, 2020, pp. 6843–6848. doi:10.18653/v1/2020.acl-main.612. [https://aclanthology.org/2020.acl-main.612](https://aclanthology.org/2020.acl-main.612).'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2015) H. Huang, L. Heck and H. Ji, Leveraging deep neural networks
    and knowledge graphs for entity disambiguation, arXiv preprint arXiv:1504.07678
    (2015). [https://arxiv.org/abs/1504.07678](https://arxiv.org/abs/1504.07678).
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Humeau et al. (2020) S. Humeau, K. Shuster, M.-A. Lachaux and J. Weston, Poly-encoders:
    Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence
    Scoring, in: International Conference on Learning Representations, 2020. [https://openreview.net/forum?id=SkxgnnNFvH](https://openreview.net/forum?id=SkxgnnNFvH).'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaegle et al. (2021) A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman
    and J. Carreira, Perceiver: General Perception with Iterative Attention, in: Proceedings
    of the 38th International Conference on Machine Learning, M. Meila and T. Zhang,
    eds, Proceedings of Machine Learning Research, Vol. 139, PMLR, 2021, pp. 4651–4664.
    [https://proceedings.mlr.press/v139/jaegle21a.html](https://proceedings.mlr.press/v139/jaegle21a.html).'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Järvelin and Kekäläinen (2002) K. Järvelin and J. Kekäläinen, Cumulated Gain-Based
    Evaluation of IR Techniques, ACM Transactions on Information Systems 20(4) (2002),
    422–446–. doi:10.1145/582415.582418.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji and Grishman (2011) H. Ji and R. Grishman, Knowledge Base Population: Successful
    Approaches and Challenges, in: Proceedings of the 49th Annual Meeting of the Association
    for Computational Linguistics: Human Language Technologies, Association for Computational
    Linguistics, Portland, Oregon, USA, 2011, pp. 1148–1158. [https://aclanthology.org/P11-1115](https://aclanthology.org/P11-1115).'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2010) H. Ji, R. Grishman, H.T. Dang, K. Griffitt and J. Ellis, Overview
    of the TAC 2010 knowledge base population track, in: Third Text Analysis Conference
    (TAC), Gaithersburg, Maryland, USA, 2010. [https://blender.cs.illinois.edu/paper/kbp2010overview.pdf](https://blender.cs.illinois.edu/paper/kbp2010overview.pdf).'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2015) H. Ji, J. Nothman, B. Hachey and R. Florian, Overview of TAC-KBP2015
    Tri-lingual Entity Discovery and Linking, in: Proceedings of the 2015 Text Analysis
    Conference, TAC 2015, NIST, Gaithersburg, Maryland, USA, 2015, pp. 16–17. [https://tac.nist.gov/publications/2015/additional.papers/TAC2015.KBP_Trilingual_Entity_Discovery_and_Linking_overview.proceedings.pdf](https://tac.nist.gov/publications/2015/additional.papers/TAC2015.KBP_Trilingual_Entity_Discovery_and_Linking_overview.proceedings.pdf).'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2022) S. Ji, S. Pan, E. Cambria, P. Marttinen and P.S. Yu, A Survey
    on Knowledge Graphs: Representation, Acquisition, and Applications, IEEE Transactions
    on Neural Networks and Learning Systems 33(2) (2022), 494–514. doi:10.1109/TNNLS.2021.3070843.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jones et al. (2000) K.S. Jones, S. Walker and S.E. Robertson, A Probabilistic
    Model of Information Retrieval: Development and Comparative Experiments Part 2,
    Information Processing & Management 36(6) (2000), 809–840–. doi:10.1016/S0306-4573(00)00016-9.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joshi et al. (2017) M. Joshi, E. Choi, D. Weld and L. Zettlemoyer, TriviaQA:
    A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,
    in: Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), Association for Computational Linguistics,
    Vancouver, Canada, 2017, pp. 1601–1611. doi:10.18653/v1/P17-1147. [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kar et al. (2018) R. Kar, S. Reddy, S. Bhattacharya, A. Dasgupta and S. Chakrabarti,
    Task-Specific Representation Learning for Web-Scale Entity Disambiguation, Proceedings
    of the AAAI Conference on Artificial Intelligence 32(1) (2018). [https://ojs.aaai.org/index.php/AAAI/article/view/12066](https://ojs.aaai.org/index.php/AAAI/article/view/12066).
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khare et al. (2014) R. Khare, J. Li and Z. Lu, LabeledIn: Cataloging labeled
    indications for human drugs, Journal of Biomedical Informatics 52 (2014), 448–456.
    doi:10.1016/j.jbi.2014.08.004. [https://www.sciencedirect.com/science/article/pii/S1532046414001853](https://www.sciencedirect.com/science/article/pii/S1532046414001853).'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kolitsas et al. (2018) N. Kolitsas, O.-E. Ganea and T. Hofmann, End-to-End
    Neural Entity Linking, in: Proceedings of the 22nd Conference on Computational
    Natural Language Learning, Association for Computational Linguistics, Brussels,
    Belgium, 2018, pp. 519–529. doi:10.18653/v1/K18-1050. [https://aclanthology.org/K18-1050](https://aclanthology.org/K18-1050).'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kraljevic et al. (2021) Z. Kraljevic, T. Searle, A. Shek, L. Roguski, K. Noor,
    D. Bean, A. Mascio, L. Zhu, A.A. Folarin, A. Roberts, R. Bendayan, M.P. Richardson,
    R. Stewart, A.D. Shah, W.K. Wong, Z. Ibrahim, J.T. Teo and R.J.B. Dobson, Multi-domain
    clinical natural language processing with MedCAT: The Medical Concept Annotation
    Toolkit, Artificial Intelligence in Medicine 117 (2021), 102083. doi:10.1016/j.artmed.2021.102083.
    [https://www.sciencedirect.com/science/article/pii/S0933365721000762](https://www.sciencedirect.com/science/article/pii/S0933365721000762).'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazic et al. (2015) N. Lazic, A. Subramanya, M. Ringgaard and F. Pereira, Plato:
    A Selective Context Model for Entity Resolution, Transactions of the Association
    for Computational Linguistics 3 (2015), 503–515. doi:10.1162/tacla00154. [https://aclanthology.org/Q15-1036](https://aclanthology.org/Q15-1036).'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Titov (2018) P. Le and I. Titov, Improving Entity Linking by Modeling
    Latent Relations between Mentions, in: Proceedings of the 56th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), Association
    for Computational Linguistics, Melbourne, Australia, 2018, pp. 1595–1604. doi:10.18653/v1/P18-1148.
    [https://aclanthology.org/P18-1148](https://aclanthology.org/P18-1148).'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Titov (2019a) P. Le and I. Titov, Boosting Entity Linking Performance
    by Leveraging Unlabeled Documents, in: Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, Association for Computational
    Linguistics, Florence, Italy, 2019a, pp. 1935–1945. doi:10.18653/v1/P19-1187.
    [https://aclanthology.org/P19-1187](https://aclanthology.org/P19-1187).'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Titov (2019b) P. Le and I. Titov, Distant Learning for Entity Linking
    with Automatic Noise Detection, in: Proceedings of the 57th Annual Meeting of
    the Association for Computational Linguistics, Association for Computational Linguistics,
    Florence, Italy, 2019b, pp. 4081–4090. doi:10.18653/v1/P19-1400. [https://aclanthology.org/P19-1400](https://aclanthology.org/P19-1400).'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Mikolov (2014) Q. Le and T. Mikolov, Distributed Representations of
    Sentences and Documents, in: Proceedings of the 31st International Conference
    on Machine Learning, E.P. Xing and T. Jebara, eds, Proceedings of Machine Learning
    Research, Vol. 32, PMLR, Bejing, China, 2014, pp. 1188–1196. [https://proceedings.mlr.press/v32/le14.html](https://proceedings.mlr.press/v32/le14.html).'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2013) D.-H. Lee, Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks, in: Workshop on challenges in representation
    learning, ICML, Vol. 3, JMLR, Atlanta, USA, 2013, p. 2. [http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf).'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2020) J. Lee, S.S. Yi, M. Jeong, M. Sung, W. Yoon, Y. Choi, M. Ko
    and J. Kang, Answering Questions on COVID-19 in Real-Time, in: Proceedings of
    the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Association for Computational
    Linguistics, Online, 2020. doi:10.18653/v1/2020.nlpcovid19-2.1. [https://aclanthology.org/2020.nlpcovid19-2.1](https://aclanthology.org/2020.nlpcovid19-2.1).'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2016) S. Lee, D. Kim, K. Lee, J. Choi, S. Kim, M. Jeon, S. Lim,
    D. Choi, S. Kim, A.-C. Tan and J. Kang, BEST: Next-Generation Biomedical Entity
    Search Tool for Knowledge Discovery from Biomedical Literature, PLOS ONE 11(10)
    (2016), 1–16. doi:10.1371/journal.pone.0164680.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lehmann et al. (2015) J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas,
    P.N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer and C. Bizer, DBpedia
    - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia, Semantic
    Web Journal 6(2) (2015), 167–195, doi:10.3233/SW-140134. [https://content.iospress.com/articles/semantic-web/sw134](https://content.iospress.com/articles/semantic-web/sw134).
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lewis et al. (2020) M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,
    O. Levy, V. Stoyanov and L. Zettlemoyer, BART: Denoising Sequence-to-Sequence
    Pre-training for Natural Language Generation, Translation, and Comprehension,
    in: Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, Association for Computational Linguistics, Online, 2020, pp. 7871–7880.
    doi:10.18653/v1/2020.acl-main.703. [https://aclanthology.org/2020.acl-main.703](https://aclanthology.org/2020.acl-main.703).'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) B.Z. Li, S. Min, S. Iyer, Y. Mehdad and W.-t. Yih, Efficient
    One-Pass End-to-End Entity Linking for Questions, in: Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP), Association
    for Computational Linguistics, Online, 2020, pp. 6433–6441. doi:10.18653/v1/2020.emnlp-main.522.
    [https://aclanthology.org/2020.emnlp-main.522](https://aclanthology.org/2020.emnlp-main.522).'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) J. Li, A. Sun, J. Han and C. Li, A Survey on Deep Learning
    for Named Entity Recognition, IEEE Transactions on Knowledge and Data Engineering
    34(1) (2022), 50–70. doi:10.1109/TKDE.2020.2981314.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling and Weld (2012) X. Ling and D.S. Weld, Fine-Grained Entity Recognition,
    in: Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,
    AAAI’12, AAAI Press, 2012, pp. 94–100–. [https://ojs.aaai.org/index.php/AAAI/article/view/8122](https://ojs.aaai.org/index.php/AAAI/article/view/8122).'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2015) X. Ling, S. Singh and D.S. Weld, Design Challenges for Entity
    Linking, Transactions of the Association for Computational Linguistics 3 (2015),
    315–328. doi:10.1162/tacla00141. [https://aclanthology.org/Q15-1023](https://aclanthology.org/Q15-1023).
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020) Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
    M. Lewis, L. Zettlemoyer and V. Stoyanov, Ro{BERT}a: A Robustly Optimized {BERT}
    Pretraining Approach, 2020. [https://openreview.net/forum?id=SyxS0T4tvS](https://openreview.net/forum?id=SyxS0T4tvS).'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Livni et al. (2014) R. Livni, S. Shalev-Shwartz and O. Shamir, On the Computational
    Efficiency of Training Neural Networks, in: Advances in Neural Information Processing
    Systems, Vol. 27, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence and K.Q. Weinberger,
    eds, Curran Associates, Inc., 2014, pp. 855–863–. [https://proceedings.neurips.cc/paper/2014/file/3a0772443a0739141292a5429b952fe6-Paper.pdf](https://proceedings.neurips.cc/paper/2014/file/3a0772443a0739141292a5429b952fe6-Paper.pdf).'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logeswaran et al. (2019) L. Logeswaran, M.-W. Chang, K. Lee, K. Toutanova,
    J. Devlin and H. Lee, Zero-Shot Entity Linking by Reading Entity Descriptions,
    in: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, Association for Computational Linguistics, Florence, Italy, 2019,
    pp. 3449–3460. doi:10.18653/v1/P19-1335. [https://aclanthology.org/P19-1335](https://aclanthology.org/P19-1335).'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loureiro and Jorge (2020) D. Loureiro and A.M. Jorge, MedLinker: Medical Entity
    Linking with Neural Representations and Dictionary Matching, in: Advances in Information
    Retrieval, J.M. Jose, E. Yilmaz, J. Magalhães, P. Castells, N. Ferro, M.J. Silva
    and F. Martins, eds, Springer International Publishing, Cham, 2020, pp. 230–237.
    ISBN ISBN 978-3-030-45442-5. doi:10.1007/978-3-030-45442-529.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2015) G. Luo, X. Huang, C.-Y. Lin and Z. Nie, Joint Entity Recognition
    and Disambiguation, in: Proceedings of the 2015 Conference on Empirical Methods
    in Natural Language Processing, Association for Computational Linguistics, Lisbon,
    Portugal, 2015, pp. 879–888. doi:10.18653/v1/D15-1104. [https://aclanthology.org/D15-1104](https://aclanthology.org/D15-1104).'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2021) G. Luo, T. Darrell and A. Rohrbach, NewsCLIPpings: Automatic
    Generation of Out-of-Context Multimodal Media, in: Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Online and Punta Cana, Dominican Republic, 2021, pp. 6801–6817. doi:10.18653/v1/2021.emnlp-main.545.
    [https://aclanthology.org/2021.emnlp-main.545](https://aclanthology.org/2021.emnlp-main.545).'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MacAvaney et al. (2019) S. MacAvaney, S. Sotudeh, A. Cohan, N. Goharian, I. Talati
    and R.W. Filice, Ontology-Aware Clinical Abstractive Summarization, in: Proceedings
    of the 42nd International ACM SIGIR Conference on Research and Development in
    Information Retrieval, SIGIR’19, Association for Computing Machinery, New York,
    NY, USA, 2019, pp. 1013–1016–. ISBN ISBN 9781450361729. doi:10.1145/3331184.3331319.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manning et al. (2008) C.D. Manning, P. Raghavan and H. Schütze, Introduction
    to Information Retrieval, Cambridge University Press, USA, 2008. ISBN ISBN 0521865719.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martínez-Rodríguez et al. (2020) J.L. Martínez-Rodríguez, A. Hogan and I. López-Arévalo,
    Information extraction meets the Semantic Web: A survey, Semantic Web 11(2) (2020),
    255–335. doi:10.3233/SW-180333. [https://content.iospress.com/articles/semantic-web/sw180333](https://content.iospress.com/articles/semantic-web/sw180333).'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Martins et al. (2019) P.H. Martins, Z. Marinho and A.F.T. Martins, Joint Learning
    of Named Entity Recognition and Entity Linking, in: Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics: Student Research Workshop,
    Association for Computational Linguistics, Florence, Italy, 2019, pp. 190–196.
    doi:10.18653/v1/P19-2026. [https://aclanthology.org/P19-2026](https://aclanthology.org/P19-2026).'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mendes et al. (2011) P.N. Mendes, M. Jakob, A. García-Silva and C. Bizer, DBpedia
    Spotlight: Shedding Light on the Web of Documents, in: Proceedings of the 7th
    International Conference on Semantic Systems, I-Semantics ’11, Association for
    Computing Machinery, New York, NY, USA, 2011, pp. 1–8–. ISBN ISBN 9781450306218.
    doi:10.1145/2063518.2063519.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miftahutdinov et al. (2021) Z. Miftahutdinov, A. Kadurin, R. Kudrin and E. Tutubalina,
    Drug and Disease Interpretation Learning with Biomedical Entity Representation
    Transformer, in: Advances in Information Retrieval, D. Hiemstra, M.-F. Moens,
    J. Mothe, R. Perego, M. Potthast and F. Sebastiani, eds, Springer International
    Publishing, Cham, 2021, pp. 451–466. ISBN ISBN 978-3-030-72113-8. doi:10.1007/978-3-030-72113-830.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mikolov et al. (2013a) T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado and
    J. Dean, Distributed Representations of Words and Phrases and Their Compositionality,
    in: Proceedings of the 26th International Conference on Neural Information Processing
    Systems - Volume 2, NIPS’13, Curran Associates Inc., Red Hook, NY, USA, 2013a,
    pp. 3111–3119–. [https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mikolov et al. (2013b) T. Mikolov, K. Chen, G.S. Corrado and J. Dean, Efficient
    Estimation of Word Representations in Vector Space, in: 1st International Conference
    on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, 2013b.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milne and Witten (2008) D. Milne and I.H. Witten, Learning to Link with Wikipedia,
    in: Proceedings of the 17th ACM Conference on Information and Knowledge Management,
    CIKM ’08, Association for Computing Machinery, New York, NY, USA, 2008, pp. 509–518–.
    ISBN ISBN 9781595939913. doi:10.1145/1458082.1458150.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moon et al. (2018) S. Moon, L. Neves and V. Carvalho, Multimodal Named Entity
    Disambiguation for Noisy Social Media Posts, in: Proceedings of the 56th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Melbourne, Australia, 2018, pp. 2000–2008.
    doi:10.18653/v1/P18-1186. [https://aclanthology.org/P18-1186](https://aclanthology.org/P18-1186).'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moreno et al. (2017) J.G. Moreno, R. Besançon, R. Beaumont, E. D’hondt, A.-L. Ligozat,
    S. Rosset, X. Tannier and B. Grau, Combining Word and Entity Embeddings for Entity
    Linking, in: The Semantic Web, E. Blomqvist, D. Maynard, A. Gangemi, R. Hoekstra,
    P. Hitzler and O. Hartig, eds, Springer International Publishing, Cham, 2017,
    pp. 337–352. ISBN ISBN 978-3-319-58068-5. doi:10.1007/978-3-319-58068-521.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moro et al. (2014) A. Moro, A. Raganato and R. Navigli, Entity Linking meets
    Word Sense Disambiguation: a Unified Approach, Transactions of the Association
    for Computational Linguistics 2 (2014), 231–244. doi:10.1162/tacla00179. [https://aclanthology.org/Q14-1019](https://aclanthology.org/Q14-1019).'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mueller and Durrett (2018) D. Mueller and G. Durrett, Effective Use of Context
    in Noisy Entity Linking, in: Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing, Association for Computational Linguistics, Brussels,
    Belgium, 2018, pp. 1024–1029. doi:10.18653/v1/D18-1126. [https://aclanthology.org/D18-1126](https://aclanthology.org/D18-1126).'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mulang’ et al. (2020) I.O. Mulang’, K. Singh, C. Prabhu, A. Nadgeri, J. Hoffart
    and J. Lehmann, Evaluating the Impact of Knowledge Graph Context on Entity Disambiguation
    Models, in: Proceedings of the 29th ACM International Conference on Information
    & Knowledge Management, CIKM ’20, Association for Computing Machinery, New York,
    NY, USA, 2020, pp. 2157–2160–. ISBN ISBN 9781450368599. doi:10.1145/3340531.3412159.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Möller et al. (2022) C. Möller, J. Lehmann and R. Usbeck, Survey on English
    Entity Linking on Wikidata: Datasets and approaches, Vol. Pre-press, IOS Press,
    2022, pp. 1–42. doi:10.3233/SW-212865. [https://content.iospress.com/articles/semantic-web/sw212865](https://content.iospress.com/articles/semantic-web/sw212865).'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nadeau and Sekine (2007) D. Nadeau and S. Sekine, A survey of named entity recognition
    and classification, Lingvisticæ Investigationes 30(1) (2007), 3–26. doi:10.1075/li.30.1.03nad.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nagrani et al. (2021) A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid and
    C. Sun, Attention Bottlenecks for Multimodal Fusion, in: Advances in Neural Information
    Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang and J.W. Vaughan, eds,
    2021. [https://openreview.net/forum?id=KJ5h-yfUHa](https://openreview.net/forum?id=KJ5h-yfUHa).'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigli (2009) R. Navigli, Word Sense Disambiguation: A Survey, ACM Computing
    Surveys 41(2) (2009). doi:10.1145/1459352.1459355.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayyeri et al. (2019) M. Nayyeri, S. Vahdati, J. Lehmann and H.S. Yazdi, Soft
    Marginal TransE for Scholarly Knowledge Graph Completion, CoRR abs/1904.12211
    (2019). [http://arxiv.org/abs/1904.12211](http://arxiv.org/abs/1904.12211).
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nayyeri et al. (2021) M. Nayyeri, S. Vahdati, C. Aykul and J. Lehmann, 5* Knowledge
    Graph Embeddings with Projective Transformations, Proceedings of the AAAI Conference
    on Artificial Intelligence 35(10) (2021), 9064–9072. [https://ojs.aaai.org/index.php/AAAI/article/view/17095](https://ojs.aaai.org/index.php/AAAI/article/view/17095).
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nedelchev et al. (2020) R. Nedelchev, D. Chaudhuri, J. Lehmann and A. Fischer,
    End-to-End Entity Linking and Disambiguation leveraging Word and Knowledge Graph
    Embeddings, CoRR abs/2002.11143 (2020). [https://arxiv.org/abs/2002.11143](https://arxiv.org/abs/2002.11143).
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Newman-Griffis et al. (2018) D. Newman-Griffis, A.M. Lai and E. Fosler-Lussier,
    Jointly Embedding Entities and Text with Distant Supervision, in: Proceedings
    of The Third Workshop on Representation Learning for NLP, Association for Computational
    Linguistics, Melbourne, Australia, 2018, pp. 195–206. doi:10.18653/v1/W18-3026.
    [https://aclanthology.org/W18-3026](https://aclanthology.org/W18-3026).'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2016a) D.B. Nguyen, M. Theobald and G. Weikum, J-NERD: Joint
    Named Entity Recognition and Disambiguation with Rich Linguistic Features, Transactions
    of the Association for Computational Linguistics 4 (2016a), 215–229. doi:10.1162/tacla00094.
    [https://aclanthology.org/Q16-1016](https://aclanthology.org/Q16-1016).'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. (2016b) T.H. Nguyen, N. Fauceglia, M. Rodriguez Muro, O. Hassanzadeh,
    A. Massimiliano Gliozzo and M. Sadoghi, Joint Learning of Local and Global Features
    for Entity Linking via Neural Networks, in: Proceedings of COLING 2016, the 26th
    International Conference on Computational Linguistics: Technical Papers, The COLING
    2016 Organizing Committee, Osaka, Japan, 2016b, pp. 2310–2320. [https://aclanthology.org/C16-1218](https://aclanthology.org/C16-1218).'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nickel et al. (2011) M. Nickel, V. Tresp and H.-P. Kriegel, A Three-Way Model
    for Collective Learning on Multi-Relational Data, in: Proceedings of the 28th
    International Conference on International Conference on Machine Learning, ICML’11,
    Omnipress, Madison, WI, USA, 2011, pp. 809–816–. ISBN ISBN 9781450306195.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2018) F. Nie, Y. Cao, J. Wang, C.-Y. Lin and R. Pan, Mention and
    Entity Description Co-Attention for Entity Disambiguation, Proceedings of the
    AAAI Conference on Artificial Intelligence 32(1) (2018). [https://ojs.aaai.org/index.php/AAAI/article/view/12043](https://ojs.aaai.org/index.php/AAAI/article/view/12043).
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oliveira et al. (2021) I.L. Oliveira, R. Fileto, R. Speck, L.P.F. Garcia, D. Moussallem
    and J. Lehmann, Towards holistic Entity Linking: Survey and directions, Information
    Systems 95 (2021), 101624. doi:10.1016/j.is.2020.101624. [https://www.sciencedirect.com/science/article/pii/S0306437920300958](https://www.sciencedirect.com/science/article/pii/S0306437920300958).'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Onoe and Durrett (2020) Y. Onoe and G. Durrett, Fine-Grained Entity Typing for
    Domain Independent Entity Linking, Proceedings of the AAAI Conference on Artificial
    Intelligence 34(05) (2020), 8576–8583. doi:10.1609/aaai.v34i05.6380. [https://ojs.aaai.org/index.php/AAAI/article/view/6380](https://ojs.aaai.org/index.php/AAAI/article/view/6380).
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orponen (1994) P. Orponen, Computational Complexity of Neural Networks: A Survey,
    Nordic Journal of Computing 1(1) (1994), 94–110–.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Page et al. (1999) L. Page, S. Brin, R. Motwani and T. Winograd, The PageRank
    Citation Ranking: Bringing Order to the Web., Technical Report, 1999-66, Stanford
    InfoLab, 1999, Previous number = SIDL-WP-1999-0120. [http://ilpubs.stanford.edu:8090/422/](http://ilpubs.stanford.edu:8090/422/).'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2017) X. Pan, B. Zhang, J. May, J. Nothman, K. Knight and H. Ji,
    Cross-lingual Name Tagging and Linking for 282 Languages, in: Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers), Association for Computational Linguistics, Vancouver, Canada, 2017,
    pp. 1946–1958. doi:10.18653/v1/P17-1178. [https://aclanthology.org/P17-1178](https://aclanthology.org/P17-1178).'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parravicini et al. (2019) A. Parravicini, R. Patra, D.B. Bartolini and M.D. Santambrogio,
    Fast and Accurate Entity Linking via Graph Embedding, in: Proceedings of the 2nd
    Joint International Workshop on Graph Data Management Experiences & Systems (GRADES)
    and Network Data Analytics (NDA), GRADES-NDA’19, Association for Computing Machinery,
    New York, NY, USA, 2019. ISBN ISBN 9781450367899. doi:10.1145/3327964.3328499.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perozzi et al. (2014) B. Perozzi, R. Al-Rfou and S. Skiena, DeepWalk: Online
    Learning of Social Representations, in: Proceedings of the 20th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, KDD ’14, Association for Computing
    Machinery, New York, NY, USA, 2014, pp. 701–710–. ISBN ISBN 9781450329569. doi:10.1145/2623330.2623732.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pershina et al. (2015) M. Pershina, Y. He and R. Grishman, Personalized Page
    Rank for Named Entity Disambiguation, in: Proceedings of the 2015 Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Association for Computational Linguistics, Denver, Colorado,
    2015, pp. 238–243. doi:10.3115/v1/N15-1026. [https://aclanthology.org/N15-1026](https://aclanthology.org/N15-1026).'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peters et al. (2018) M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,
    K. Lee and L. Zettlemoyer, Deep Contextualized Word Representations, in: Proceedings
    of the 2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association
    for Computational Linguistics, New Orleans, Louisiana, 2018, pp. 2227–2237. doi:10.18653/v1/N18-1202.
    [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202).'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peters et al. (2019) M.E. Peters, M. Neumann, R. Logan, R. Schwartz, V. Joshi,
    S. Singh and N.A. Smith, Knowledge Enhanced Contextual Word Representations, in:
    Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
    and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
    Association for Computational Linguistics, Hong Kong, China, 2019, pp. 43–54.
    doi:10.18653/v1/D19-1005. [https://aclanthology.org/D19-1005](https://aclanthology.org/D19-1005).'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Piccinno and Ferragina (2014) F. Piccinno and P. Ferragina, From TagME to WAT:
    A New Entity Annotator, in: Proceedings of the First International Workshop on
    Entity Recognition &; Disambiguation, ERD ’14, Association for Computing Machinery,
    New York, NY, USA, 2014, pp. 55–62–. ISBN ISBN 9781450330237. doi:10.1145/2633211.2634350.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pires et al. (2019) T. Pires, E. Schlinger and D. Garrette, How Multilingual
    is Multilingual BERT?, in: Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, Association for Computational Linguistics, Florence,
    Italy, 2019, pp. 4996–5001. doi:10.18653/v1/P19-1493. [https://aclanthology.org/P19-1493](https://aclanthology.org/P19-1493).'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poerner et al. (2020) N. Poerner, U. Waltinger and H. Schütze, E-BERT: Efficient-Yet-Effective
    Entity Embeddings for BERT, in: Findings of the Association for Computational
    Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020,
    pp. 803–818. doi:10.18653/v1/2020.findings-emnlp.71. [https://aclanthology.org/2020.findings-emnlp.71](https://aclanthology.org/2020.findings-emnlp.71).'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Provatorova et al. (2020) V. Provatorova, S. Vakulenko, E. Kanoulas, K. Dercksen
    and J.M. van Hulst, Named Entity Recognition and Linking on Historical Newspapers:
    UvA.ILPS & REL at CLEF HIPE 2020, in: Working Notes of CLEF 2020 - Conference
    and Labs of the Evaluation Forum, L. Cappellato, C. Eickhoff, N. Ferro and A. Névéol,
    eds, CEUR Workshop Proceedings, Vol. 2696, CEUR-WS.org, Thessaloniki, Greece,
    2020. [http://ceur-ws.org/Vol-2696/paper_209.pdf](http://ceur-ws.org/Vol-2696/paper_209.pdf).'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radhakrishnan et al. (2018) P. Radhakrishnan, P. Talukdar and V. Varma, ELDEN:
    Improved Entity Linking Using Densified Knowledge Graphs, in: Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association
    for Computational Linguistics, New Orleans, Louisiana, 2018, pp. 1844–1853. doi:10.18653/v1/N18-1167.
    [https://aclanthology.org/N18-1167](https://aclanthology.org/N18-1167).'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
    Y. Zhou, W. Li and P.J. Liu, Exploring the Limits of Transfer Learning with a
    Unified Text-to-Text Transformer, Journal of Machine Learning Research 21(140)
    (2020), 1–67. [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raiman and Raiman (2018) J. Raiman and O. Raiman, DeepType: Multilingual Entity
    Linking by Neural Type System Evolution, in: AAAI Conference on Artificial Intelligence,
    New Orleans, Louisiana, USA., 2018. [https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17148](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17148).'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2016) P. Rajpurkar, J. Zhang, K. Lopyrev and P. Liang, SQuAD:
    100,000+ Questions for Machine Comprehension of Text, in: Proceedings of the 2016
    Conference on Empirical Methods in Natural Language Processing, Association for
    Computational Linguistics, Austin, Texas, 2016, pp. 2383–2392. doi:10.18653/v1/D16-1264.
    [https://aclanthology.org/D16-1264](https://aclanthology.org/D16-1264).'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ratinov et al. (2011) L. Ratinov, D. Roth, D. Downey and M. Anderson, Local
    and Global Algorithms for Disambiguation to Wikipedia, in: Proceedings of the
    49th Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies - Volume 1, HLT ’11, Association for Computational Linguistics, USA,
    2011, pp. 1375–1384–. ISBN ISBN 9781932432879. [http://dl.acm.org/citation.cfm?id=2002472.2002642](http://dl.acm.org/citation.cfm?id=2002472.2002642).'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2017) N. Reimers and I. Gurevych, Reporting Score Distributions
    Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging, in:
    Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
    Association for Computational Linguistics, Copenhagen, Denmark, 2017, pp. 338–348.
    doi:10.18653/v1/D17-1035. [https://aclanthology.org/D17-1035](https://aclanthology.org/D17-1035).'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) N. Reimers and I. Gurevych, Sentence-BERT: Sentence
    Embeddings using Siamese BERT-Networks, in: Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for
    Computational Linguistics, Hong Kong, China, 2019, pp. 3982–3992. doi:10.18653/v1/D19-1410.
    [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rijhwani et al. (2019) S. Rijhwani, J. Xie, G. Neubig and J. Carbonell, Zero-Shot
    Neural Transfer for Cross-Lingual Entity Linking, Proceedings of the AAAI Conference
    on Artificial Intelligence 33(01) (2019), 6924–6931. doi:10.1609/aaai.v33i01.33016924.
    [https://ojs.aaai.org/index.php/AAAI/article/view/4670](https://ojs.aaai.org/index.php/AAAI/article/view/4670).
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rizzo et al. (2014) G. Rizzo, M. van Erp and R. Troncy, Benchmarking the Extraction
    and Disambiguation of Named Entities on the Semantic Web, in: Proceedings of the
    Ninth International Conference on Language Resources and Evaluation (LREC’14),
    European Language Resources Association (ELRA), Reykjavik, Iceland, 2014, pp. 4593–4600.
    [http://www.lrec-conf.org/proceedings/lrec2014/pdf/176_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/176_Paper.pdf).'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Röder et al. (2018) M. Röder, R. Usbeck and A.N. Ngomo, GERBIL - Benchmarking
    Named Entity Recognition and Linking consistently, Semantic Web 9(5) (2018), 605
    – 625–. doi:10.3233/SW-170286. [https://content.iospress.com/articles/semantic-web/sw286](https://content.iospress.com/articles/semantic-web/sw286).
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruffinelli et al. (2020) D. Ruffinelli, S. Broscheit and R. Gemulla, You CAN
    Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings, in: International
    Conference on Learning Representations, 2020. [https://openreview.net/forum?id=BkxSmlBFvr](https://openreview.net/forum?id=BkxSmlBFvr).'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Savova et al. (2010) G.K. Savova, J.J. Masanz, P.V. Ogren, J. Zheng, S. Sohn,
    K.C. Kipper-Schuler and C.G. Chute, Mayo clinical Text Analysis and Knowledge
    Extraction System (cTAKES): architecture, component evaluation and applications,
    Journal of the American Medical Informatics Association 17(5) (2010), 507–513.
    doi:10.1136/jamia.2009.001560.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sevgili et al. (2019) Ö. Sevgili, A. Panchenko and C. Biemann, Improving Neural
    Entity Disambiguation with Graph Embeddings, in: Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics: Student Research Workshop,
    Association for Computational Linguistics, Florence, Italy, 2019, pp. 315–322.
    doi:10.18653/v1/P19-2044. [https://aclanthology.org/P19-2044](https://aclanthology.org/P19-2044).'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahbazi et al. (2018) H. Shahbazi, X. Fern, R. Ghaeini, C. Ma, R.M. Obeidat
    and P. Tadepalli, Joint Neural Entity Disambiguation with Output Space Search,
    in: Proceedings of the 27th International Conference on Computational Linguistics,
    Association for Computational Linguistics, Santa Fe, New Mexico, USA, 2018, pp. 2170–2180.
    [https://aclanthology.org/C18-1184](https://aclanthology.org/C18-1184).'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shahbazi et al. (2019) H. Shahbazi, X.Z. Fern, R. Ghaeini, R. Obeidat and P. Tadepalli,
    Entity-aware ELMo: Learning Contextual Entity Representation for Entity Disambiguation,
    arXiv preprint arXiv:1908.05762 (2019). [https://arxiv.org/abs/1908.05762](https://arxiv.org/abs/1908.05762).'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sharnagat (2014) R. Sharnagat, Named entity recognition: A literature survey,
    Center For Indian Language Technology (2014). [http://www.cfilt.iitb.ac.in/resources/surveys/rahul-ner-survey.pdf](http://www.cfilt.iitb.ac.in/resources/surveys/rahul-ner-survey.pdf).'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2015) W. Shen, J. Wang and J. Han, Entity Linking with a Knowledge
    Base: Issues, Techniques, and Solutions, IEEE Transactions on Knowledge & Data
    Engineering 27(02) (2015), 443–460. doi:10.1109/TKDE.2014.2327028.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2018) W. Shen, J. Han, J. Wang, X. Yuan and Z. Yang, SHINE+: A
    General Framework for Domain-Specific Entity Linking with Heterogeneous Information
    Networks, IEEE Transactions on Knowledge and Data Engineering 30(2) (2018), 353–366.
    doi:10.1109/TKDE.2017.2730862.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2020) W. Shi, S. Zhang, Z. Zhang, H. Cheng and J.X. Yu, Joint Embedding
    in Named Entity Linking on Sentence Level, arXiv preprint arXiv:2002.04936 (2020).
    [https://arxiv.org/abs/2002.04936](https://arxiv.org/abs/2002.04936).
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shnayderman et al. (2019) I. Shnayderman, L. Ein-Dor, Y. Mass, A. Halfon, B. Sznajder,
    A. Spector, Y. Katz, D. Sheinwald, R. Aharonov and N. Slonim, Fast End-to-End
    Wikification, arXiv preprint arXiv:1908.06785 (2019).
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sil et al. (2018) A. Sil, G. Kundu, R. Florian and W. Hamza, Neural Cross-Lingual
    Entity Linking, Proceedings of the AAAI Conference on Artificial Intelligence
    32(1) (2018). [https://ojs.aaai.org/index.php/AAAI/article/view/11964](https://ojs.aaai.org/index.php/AAAI/article/view/11964).
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Šíma and Orponen (2003) J. Šíma and P. Orponen, General-Purpose Computation
    with Neural Networks: A Survey of Complexity Theoretic Results, Neural Computation
    15(12) (2003), 2727–2778. doi:10.1162/089976603322518731.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slonim et al. (2021) N. Slonim, Y. Bilu, C. Alzate, R. Bar-Haim, B. Bogin, F. Bonin,
    L. Choshen, E. Cohen-Karlik, L. Dankin, L. Edelstein et al., An autonomous debating
    system, Nature 591(7850) (2021), 379–384. doi:10.1038/s41586-021-03215-w.
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soldaini and Goharian (2016) L. Soldaini and N. Goharian, QuickUMLS: a fast,
    unsupervised approach for medical concept extraction, in: MedIR workshop, SIGIR,
    2016, pp. 1–4. [http://medir2016.imag.fr/data/MEDIR_2016_paper_16.pdf](http://medir2016.imag.fr/data/MEDIR_2016_paper_16.pdf).'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sorokin and Gurevych (2018) D. Sorokin and I. Gurevych, Mixing Context Granularities
    for Improved Entity Linking on Question Answering Data across Entity Categories,
    in: Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,
    Association for Computational Linguistics, New Orleans, Louisiana, 2018, pp. 65–75.
    doi:10.18653/v1/S18-2007. [https://aclanthology.org/S18-2007](https://aclanthology.org/S18-2007).'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spitkovsky and Chang (2012) V.I. Spitkovsky and A.X. Chang, A Cross-Lingual
    Dictionary for English Wikipedia Concepts, in: Proceedings of the Eighth International
    Conference on Language Resources and Evaluation (LREC’12), European Language Resources
    Association (ELRA), Istanbul, Turkey, 2012, pp. 3168–3175. [http://www.lrec-conf.org/proceedings/lrec2012/pdf/266_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/266_Paper.pdf).'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suchanek et al. (2007) F.M. Suchanek, G. Kasneci and G. Weikum, YAGO: A Core
    of Semantic Knowledge, in: Proceedings of the 16th International Conference on
    World Wide Web, WWW ’07, Association for Computing Machinery, New York, NY, USA,
    2007, pp. 697–706–. ISBN ISBN 9781595936547. doi:10.1145/1242572.1242667.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2015) Y. Sun, L. Lin, D. Tang, N. Yang, Z. Ji and X. Wang, Modeling
    Mention, Context and Entity with Neural Networks for Entity Disambiguation, in:
    Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI’15,
    AAAI Press, 2015, pp. 1333–1339–. ISBN ISBN 9781577357384.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sung et al. (2020) M. Sung, H. Jeon, J. Lee and J. Kang, Biomedical Entity
    Representations with Synonym Marginalization, in: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, Association for Computational
    Linguistics, Online, 2020, pp. 3641–3650. doi:10.18653/v1/2020.acl-main.335. [https://www.aclweb.org/anthology/2020.acl-main.335](https://www.aclweb.org/anthology/2020.acl-main.335).'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2021) H. Tang, X. Sun, B. Jin and F. Zhang, A Bidirectional Multi-paragraph
    Reading Model for Zero-shot Entity Linking, 2021, pp. 13889–13897. [https://ojs.aaai.org/index.php/AAAI/article/view/17636](https://ojs.aaai.org/index.php/AAAI/article/view/17636).
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjong Kim Sang and De Meulder (2003) E.F. Tjong Kim Sang and F. De Meulder,
    Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity
    Recognition, in: Proceedings of the Seventh Conference on Natural Language Learning
    at HLT-NAACL 2003, 2003, pp. 142–147. [https://aclanthology.org/W03-0419](https://aclanthology.org/W03-0419).'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trouillon et al. (2016) T. Trouillon, J. Welbl, S. Riedel, E. Gaussier and
    G. Bouchard, Complex Embeddings for Simple Link Prediction, in: Proceedings of
    The 33rd International Conference on Machine Learning, M.F. Balcan and K.Q. Weinberger,
    eds, Proceedings of Machine Learning Research, Vol. 48, PMLR, New York, New York,
    USA, 2016, pp. 2071–2080. [https://proceedings.mlr.press/v48/trouillon16.html](https://proceedings.mlr.press/v48/trouillon16.html).'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsai and Roth (2016) C.-T. Tsai and D. Roth, Cross-lingual Wikification Using
    Multilingual Embeddings, in: Proceedings of the 2016 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Association for Computational Linguistics, San Diego, California, 2016, pp. 589–598.
    doi:10.18653/v1/N16-1072. [https://aclanthology.org/N16-1072](https://aclanthology.org/N16-1072).'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsai and Roth (2018) C.-T. Tsai and D. Roth, Learning Better Name Translation
    for Cross-Lingual Wikification, Proceedings of the AAAI Conference on Artificial
    Intelligence 32(1) (2018). [https://ojs.aaai.org/index.php/AAAI/article/view/12018](https://ojs.aaai.org/index.php/AAAI/article/view/12018).
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutubalina et al. (2020) E. Tutubalina, A. Kadurin and Z. Miftahutdinov, Fair
    Evaluation in Concept Normalization: a Large-scale Comparative Analysis for BERT-based
    Models, in: Proceedings of the 28th International Conference on Computational
    Linguistics, International Committee on Computational Linguistics, Barcelona,
    Spain (Online), 2020, pp. 6710–6716. doi:10.18653/v1/2020.coling-main.588. [https://aclanthology.org/2020.coling-main.588](https://aclanthology.org/2020.coling-main.588).'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay et al. (2018a) S. Upadhyay, N. Gupta and D. Roth, Joint Multilingual
    Supervision for Cross-lingual Entity Linking, in: Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Brussels, Belgium, 2018a, pp. 2486–2495. doi:10.18653/v1/D18-1270.
    [https://aclanthology.org/D18-1270](https://aclanthology.org/D18-1270).'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upadhyay et al. (2018b) S. Upadhyay, J. Kodner and D. Roth, Bootstrapping Transliteration
    with Constrained Discovery for Low-Resource Languages, in: Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Brussels, Belgium, 2018b, pp. 501–511. doi:10.18653/v1/D18-1046.
    [https://aclanthology.org/D18-1046](https://aclanthology.org/D18-1046).'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'van Hulst et al. (2020) J.M. van Hulst, F. Hasibi, K. Dercksen, K. Balog and
    A.P. de Vries, REL: An Entity Linker Standing on the Shoulders of Giants, in:
    Proceedings of the 43rd International ACM SIGIR Conference on Research and Development
    in Information Retrieval, Association for Computing Machinery, New York, NY, USA,
    2020, pp. 2197–2200–. ISBN ISBN 9781450380164. [https://doi.org/10.1145/3397271.3401416](https://doi.org/10.1145/3397271.3401416).'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A.N. Gomez, L. Kaiser and I. Polosukhin, Attention is All You Need, in: Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    NIPS’17, Curran Associates Inc., Red Hook, NY, USA, 2017, pp. 6000–6010–. ISBN
    ISBN 9781510860964. [https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2015) O. Vinyals, M. Fortunato and N. Jaitly, Pointer Networks,
    in: Advances in Neural Information Processing Systems 28, C. Cortes, N.D. Lawrence,
    D.D. Lee, M. Sugiyama and R. Garnett, eds, Curran Associates, Inc., 2015, pp. 2692–2700.
    [http://papers.nips.cc/paper/5866-pointer-networks.pdf](http://papers.nips.cc/paper/5866-pointer-networks.pdf).'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vrandečić and Krötzsch (2014) D. Vrandečić and M. Krötzsch, Wikidata: A Free
    Collaborative Knowledgebase, Communications of the ACM 57(10) (2014), 78–85–.
    doi:10.1145/2629489.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) A. Wang, A. Singh, J. Michael, F. Hill, O. Levy and S. Bowman,
    GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,
    in: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
    Neural Networks for NLP, Association for Computational Linguistics, Brussels,
    Belgium, 2018, pp. 353–355. doi:10.18653/v1/W18-5446. [https://aclanthology.org/W18-5446](https://aclanthology.org/W18-5446).'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2015) H. Wang, J.G. Zheng, X. Ma, P. Fox and H. Ji, Language and
    Domain Independent Entity Linking with Quantified Collective Validation, in: Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Lisbon, Portugal, 2015, pp. 695–704. doi:10.18653/v1/D15-1081.
    [https://aclanthology.org/D15-1081](https://aclanthology.org/D15-1081).'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Q. Wang, Z. Mao, B. Wang and L. Guo, Knowledge Graph Embedding:
    A Survey of Approaches and Applications, IEEE Transactions on Knowledge and Data
    Engineering 29(12) (2017), 2724–2743. doi:10.1109/TKDE.2017.2754499.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li and J. Tang,
    KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,
    Transactions of the Association for Computational Linguistics 9 (2021), 176–194.
    doi:10.1162/tacla00360. [https://doi.org/10.1162/tacl_a_00360](https://doi.org/10.1162/tacl_a_00360).'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2014) Z. Wang, J. Zhang, J. Feng and Z. Chen, Knowledge Graph Embedding
    by Translating on Hyperplanes, Proceedings of the AAAI Conference on Artificial
    Intelligence 28(1) (2014). [https://ojs.aaai.org/index.php/AAAI/article/view/8870](https://ojs.aaai.org/index.php/AAAI/article/view/8870).
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020a) J. Wu, R. Zhang, Y. Mao, H. Guo, M. Soflaei and J. Huai,
    Dynamic Graph Convolutional Networks for Entity Linking, in: Proceedings of The
    Web Conference 2020, WWW ’20, Association for Computing Machinery, New York, NY,
    USA, 2020a, pp. 1149–1159–. ISBN ISBN 9781450370233. doi:10.1145/3366423.3380192.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020b) L. Wu, F. Petroni, M. Josifoski, S. Riedel and L. Zettlemoyer,
    Scalable Zero-shot Entity Linking with Dense Entity Retrieval, in: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    Association for Computational Linguistics, Online, 2020b, pp. 6397–6407. doi:10.18653/v1/2020.emnlp-main.519.
    [https://aclanthology.org/2020.emnlp-main.519](https://aclanthology.org/2020.emnlp-main.519).'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2019) M. Xue, W. Cai, J. Su, L. Song, Y. Ge, Y. Liu and B. Wang,
    Neural Collective Entity Linking Based on Recurrent Random Walk Network Learning,
    in: Proceedings of the Twenty-Eighth International Joint Conference on Artificial
    Intelligence, IJCAI-19, International Joint Conferences on Artificial Intelligence
    Organization, 2019, pp. 5327–5333. doi:10.24963/ijcai.2019/740.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yadav and Bethard (2018) V. Yadav and S. Bethard, A Survey on Recent Advances
    in Named Entity Recognition from Deep Learning models, in: Proceedings of the
    27th International Conference on Computational Linguistics, Association for Computational
    Linguistics, Santa Fe, New Mexico, NM, USA, 2018, pp. 2145–2158. [https://www.aclweb.org/anthology/C18-1182](https://www.aclweb.org/anthology/C18-1182).'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yamada et al. (2016) I. Yamada, H. Shindo, H. Takeda and Y. Takefuji, Joint
    Learning of the Embedding of Words and Entities for Named Entity Disambiguation,
    in: Proceedings of The 20th SIGNLL Conference on Computational Natural Language
    Learning, Association for Computational Linguistics, Berlin, Germany, 2016, pp. 250–259.
    doi:10.18653/v1/K16-1025. [https://aclanthology.org/K16-1025](https://aclanthology.org/K16-1025).'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamada et al. (2017) I. Yamada, H. Shindo, H. Takeda and Y. Takefuji, Learning
    Distributed Representations of Texts and Entities from Knowledge Base, Transactions
    of the Association for Computational Linguistics 5 (2017), 397–411. doi:10.1162/tacla00069.
    [https://aclanthology.org/Q17-1028](https://aclanthology.org/Q17-1028).
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yamada et al. (2020a) I. Yamada, A. Asai, H. Shindo, H. Takeda and Y. Matsumoto,
    LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,
    in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), Association for Computational Linguistics, Online, 2020a,
    pp. 6442–6454. doi:10.18653/v1/2020.emnlp-main.523. [https://aclanthology.org/2020.emnlp-main.523](https://aclanthology.org/2020.emnlp-main.523).'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yamada et al. (2020b) I. Yamada, A. Asai, J. Sakuma, H. Shindo, H. Takeda,
    Y. Takefuji and Y. Matsumoto, Wikipedia2Vec: An Efficient Toolkit for Learning
    and Visualizing the Embeddings of Words and Entities from Wikipedia, in: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations, Association for Computational Linguistics, Online, 2020b, pp. 23–30.
    doi:10.18653/v1/2020.emnlp-demos.4. [https://aclanthology.org/2020.emnlp-demos.4](https://aclanthology.org/2020.emnlp-demos.4).'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamada et al. (2021) I. Yamada, K. Washio, H. Shindo and Y. Matsumoto, Global
    Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities,
    arXiv preprint arXiv:1909.00426v3 (2021). [https://arxiv.org/abs/1909.00426v3](https://arxiv.org/abs/1909.00426v3).
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2015) B. Yang, S.W.-t. Yih, X. He, J. Gao and L. Deng, Embedding
    Entities and Relations for Learning and Inference in Knowledge Bases, in: Proceedings
    of the International Conference on Learning Representations (ICLR) 2015, Proceedings
    of the international conference on learning representations (iclr) 2015 edn, 2015.
    [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ICLR2015_updated.pdf).'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) X. Yang, X. Gu, S. Lin, S. Tang, Y. Zhuang, F. Wu, Z. Chen,
    G. Hu and X. Ren, Learning Dynamic Context Augmentation for Global Entity Linking,
    in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019,
    pp. 271–281. doi:10.18653/v1/D19-1026. [https://aclanthology.org/D19-1026](https://aclanthology.org/D19-1026).'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2020) Z. Yao, L. Cao and H. Pan, Zero-shot Entity Linking with
    Efficient Long Range Sequence Modeling, in: Findings of the Association for Computational
    Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020,
    pp. 2517–2522. doi:10.18653/v1/2020.findings-emnlp.228. [https://aclanthology.org/2020.findings-emnlp.228](https://aclanthology.org/2020.findings-emnlp.228).'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yih et al. (2015) W.-t. Yih, M.-W. Chang, X. He and J. Gao, Semantic Parsing
    via Staged Query Graph Generation: Question Answering with Knowledge Base, in:
    Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
    and the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), Association for Computational Linguistics, Beijing, China, 2015,
    pp. 1321–1331. doi:10.3115/v1/P15-1128. [https://aclanthology.org/P15-1128](https://aclanthology.org/P15-1128).'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2018) T. Young, D. Hazarika, S. Poria and E. Cambria, Recent Trends
    in Deep Learning Based Natural Language Processing [Review Article], IEEE Computational
    Intelligence Magazine 13(3) (2018), 55–75. doi:10.1109/MCI.2018.2840738.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh and B.V. Durme,
    ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,
    CoRR abs/1810.12885 (2018). [http://arxiv.org/abs/1810.12885](http://arxiv.org/abs/1810.12885).'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Y. Zhang, V. Zhong, D. Chen, G. Angeli and C.D. Manning,
    Position-aware Attention and Supervised Data Improve Slot Filling, in: Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Copenhagen, Denmark, 2017, pp. 35–45. doi:10.18653/v1/D17-1004.
    [https://aclanthology.org/D17-1004](https://aclanthology.org/D17-1004).'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019) Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun and Q. Liu,
    ERNIE: Enhanced Language Representation with Informative Entities, in: Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics, Association
    for Computational Linguistics, Florence, Italy, 2019, pp. 1441–1451. doi:10.18653/v1/P19-1139.
    [https://aclanthology.org/P19-1139](https://aclanthology.org/P19-1139).'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) S. Zhou, S. Rijhwani and G. Neubig, Towards Zero-resource
    Cross-lingual Entity Linking, in: Proceedings of the 2nd Workshop on Deep Learning
    Approaches for Low-Resource NLP (DeepLo 2019), Association for Computational Linguistics,
    Hong Kong, China, 2019, pp. 243–252. doi:10.18653/v1/D19-6127. [https://aclanthology.org/D19-6127](https://aclanthology.org/D19-6127).'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020) S. Zhou, S. Rijhwani, J. Wieting, J. Carbonell and G. Neubig,
    Improving Candidate Generation for Low-resource Cross-lingual Entity Linking,
    Transactions of the Association for Computational Linguistics 8 (2020), 109–124.
    doi:10.1162/tacla00303. [https://doi.org/10.1162/tacl_a_00303](https://doi.org/10.1162/tacl_a_00303).
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) M. Zhu, B. Celikkaya, P. Bhatia and C.K. Reddy, LATTE: Latent
    Type Modeling for Biomedical Entity Linking, Proceedings of the AAAI Conference
    on Artificial Intelligence 34(05) (2020), 9757–9764. doi:10.1609/aaai.v34i05.6526.
    [https://ojs.aaai.org/index.php/AAAI/article/view/6526](https://ojs.aaai.org/index.php/AAAI/article/view/6526).'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zwicklbauer et al. (2016a) S. Zwicklbauer, C. Seifert and M. Granitzer, DoSeR
    - A Knowledge-Base-Agnostic Framework for Entity Disambiguation Using Semantic
    Embeddings, in: The Semantic Web. Latest Advances and New Domains, H. Sack, E. Blomqvist,
    M. d’Aquin, C. Ghidini, S.P. Ponzetto and C. Lange, eds, Springer International
    Publishing, Cham, 2016a, pp. 182–198. ISBN ISBN 978-3-319-34129-3. doi:10.1007/978-3-319-34129-312.'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zwicklbauer et al. (2016b) S. Zwicklbauer, C. Seifert and M. Granitzer, Robust
    and Collective Entity Disambiguation through Semantic Embeddings, in: Proceedings
    of the 39th International ACM SIGIR Conference on Research and Development in
    Information Retrieval, SIGIR ’16, Association for Computing Machinery, New York,
    NY, USA, 2016b, pp. 425–434–. ISBN ISBN 9781450340694. doi:10.1145/2911451.2911535.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
