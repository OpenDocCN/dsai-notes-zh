- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1804.04577] Untitled Document'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1804.04577] 无标题文档'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1804.04577](https://ar5iv.labs.arxiv.org/html/1804.04577)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1804.04577](https://ar5iv.labs.arxiv.org/html/1804.04577)
- en: April 2018 (revised August 2018) MIT/LIDS Report
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年4月（2018年8月修订）麻省理工学院/LIDS报告
- en: 'Feature-Based Aggregation and Deep Reinforcement Learning:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的聚合与深度强化学习：
- en: A Survey and Some New Implementations
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 调查与一些新的实现
- en: Dimitri P. Bertsekas^†^†† Dimitri Bertsekas is with the Dept. of Electr. Engineering
    and Comp. Science, and the Laboratory for Information and Decision Systems, M.I.T.,
    Cambridge, Mass., 02139\. A version of this paper will appear in IEEE/CAA Journal
    of Automatica Sinica.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dimitri P. Bertsekas^†^†† Dimitri Bertsekas在麻省理工学院电气工程与计算机科学系以及信息与决策系统实验室工作，地址为剑桥，马萨诸塞州，02139。本文的一个版本将出现在IEEE/CAA自动化学报中。
- en: Abstract
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper we discuss policy iteration methods for approximate solution of
    a finite-state discounted Markov decision problem, with a focus on feature-based
    aggregation methods and their connection with deep reinforcement learning schemes.
    We introduce features of the states of the original problem, and we formulate
    a smaller “aggregate” Markov decision problem, whose states relate to the features.
    We discuss properties and possible implementations of this type of aggregation,
    including a new approach to approximate policy iteration. In this approach the
    policy improvement operation combines feature-based aggregation with feature construction
    using deep neural networks or other calculations. We argue that the cost function
    of a policy may be approximated much more accurately by the nonlinear function
    of the features provided by aggregation, than by the linear function of the features
    provided by neural network-based reinforcement learning, thereby potentially leading
    to more effective policy improvement.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了用于有限状态折扣马尔可夫决策问题的近似解的策略迭代方法，重点关注基于特征的聚合方法及其与深度强化学习方案的联系。我们引入了原始问题状态的特征，并制定了一个较小的“聚合”马尔可夫决策问题，其状态与这些特征相关。我们讨论了这种聚合类型的属性和可能的实现，包括一种新的近似策略迭代方法。在这种方法中，策略改进操作结合了基于特征的聚合与使用深度神经网络或其他计算方法进行的特征构建。我们认为，通过聚合提供的特征的非线性函数可以比通过基于神经网络的强化学习提供的特征的线性函数更准确地近似策略的成本函数，从而有可能导致更有效的策略改进。
- en: Contents
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目录
- en: 1. Introduction
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 介绍
- en: 1.1 Alternative Approximate Policy Iteration Methods
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1.1 替代的近似策略迭代方法
- en: 1.2 Terminology
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2 术语
- en: '2. Approximate Policy Iteration: An Overview'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 近似策略迭代：概述
- en: 2.1 Direct and Indirect Approximation Approaches for Policy Evaluation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1 策略评估的直接和间接近似方法
- en: 2.2 Indirect Methods Based on Projected Equations
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2 基于投影方程的间接方法
- en: 2.3 Indirect Methods Based on Aggregation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3 基于聚合的间接方法
- en: 2.4 Implementation Issues
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4 实施问题
- en: 3. Approximate Policy Evaluation Based on Neural Networks
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 基于神经网络的近似策略评估
- en: 4. Feature-Based Aggregation Framework
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 基于特征的聚合框架
- en: 4.1 The Aggregate Problem
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1 聚合问题
- en: 4.2 Solving the Aggregate Problem with Simulation-Based Methods
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2 使用基于模拟的方法解决聚合问题
- en: 4.3 Feature Formation by Using Scoring Functions
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3 通过评分函数形成特征
- en: 4.4 Using Heuristics to Generate Features - Deterministic Optimization and Rollout
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 4.4 使用启发式方法生成特征 - 确定性优化和回滚
- en: 4.5 Stochastic Shortest Path Problems - Illustrative Examples
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5 随机最短路径问题 - 说明性示例
- en: 4.6 Multistep Aggregation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6 多步聚合
- en: 5. Policy Iteration with Feature-Based Aggregation and a Neural Network
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 基于特征的聚合与神经网络的策略迭代
- en: 6. Concluding Remarks
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 总结
- en: 7. References
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 参考文献
- en: 1. INTRODUCTION
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 引言
- en: 'We consider a discounted infinite horizon dynamic programming (DP) problem
    with $n$ states, which we denote by $i=1,\ldots,n$. State transitions $(i,j)$
    under control $u$ occur at discrete times according to transition probabilities
    $p_{ij}(u)$, and generate a cost $\alpha^{k}g(i,u,j)$ at time $k$, where $\alpha\in(0,1)$
    is the discount factor. We consider deterministic stationary policies $\mu$ such
    that for each $i$, $\mu(i)$ is a control that belongs to a constraint set $U(i)$.
    We denote by $J_{\mu}(i)$ the total discounted expected cost of $\mu$ over an
    infinite number of stages starting from state $i$, and by $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$
    the minimal value of $J_{\mu}(i)$ over all $\mu$. We denote by $J_{\mu}$ and $J^{\raise
    0.04pt\hbox{\sevenrm*}}$ the $n$-dimensional vectors that have components $J_{\mu}(i)$
    and $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$, $i=1,\ldots,n$, respectively. As is
    well known, $J_{\mu}$ is the unique solution of the Bellman equation for policy
    $\mu$:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个折扣无限时域动态规划（DP）问题，包含 $n$ 个状态，我们用 $i=1,\ldots,n$ 表示。控制 $u$ 下的状态转移 $(i,j)$
    发生在离散时间，根据转移概率 $p_{ij}(u)$ 生成一个在时间 $k$ 的成本 $\alpha^{k}g(i,u,j)$，其中 $\alpha\in(0,1)$
    是折扣因子。我们考虑确定性平稳政策 $\mu$，对于每个 $i$，$\mu(i)$ 是属于约束集合 $U(i)$ 的控制。我们用 $J_{\mu}(i)$
    表示从状态 $i$ 开始的无限期内政策 $\mu$ 的总折扣期望成本，用 $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$ 表示所有
    $\mu$ 的 $J_{\mu}(i)$ 的最小值。我们用 $J_{\mu}$ 和 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    表示具有 $J_{\mu}(i)$ 和 $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$ 分量的 $n$ 维向量，$i=1,\ldots,n$。众所周知，$J_{\mu}$
    是政策 $\mu$ 的 Bellman 方程的唯一解：
- en: '|  | $J_{\mu}(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J_{\mu}(j)\Big{)},\qquad i=1,\ldots,n,$ |  | (1.1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{\mu}(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J_{\mu}(j)\Big{)},\qquad i=1,\ldots,n,$ |  | (1.1) |'
- en: while $J^{\raise 0.04pt\hbox{\sevenrm*}}$ is the unique solution of the Bellman
    equation
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同时 $J^{\raise 0.04pt\hbox{\sevenrm*}}$ 是 Bellman 方程的唯一解
- en: '|  | $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)=\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\big{(}g(i,u,j)+\alpha
    J^{\raise 0.04pt\hbox{\sevenrm*}}(j)\big{)},\qquad i=1,\ldots,n.$ |  | (1.2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $J^{\raise 0.04pt\hbox{\sevenrm*}}(i)=\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\big{(}g(i,u,j)+\alpha
    J^{\raise 0.04pt\hbox{\sevenrm*}}(j)\big{)},\qquad i=1,\ldots,n.$ |  | (1.2) |'
- en: In this paper, we survey several ideas from aggregation-based approximate DP
    and deep reinforcement learning, all of which have been essentially known for
    some time, but are combined here in a new way. We will focus on methods of approximate
    policy iteration (PI for short), whereby we evaluate approximately the cost vector
    $J_{\mu}$ of each generated policy $\mu$. Our cost approximations use a feature
    vector $F(i)$ of each state $i$, and replace $J_{\mu}(i)$ with a function that
    depends on $i$ through $F(i)$, i.e., a function of the form
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们调查了几种基于聚合的近似 DP 和深度强化学习的思想，这些思想已被基本了解，但在这里以一种新的方式组合。我们将关注近似政策迭代（简称 PI）的方法，通过这些方法，我们对每个生成的政策
    $\mu$ 的成本向量 $J_{\mu}$ 进行近似评估。我们的成本近似使用每个状态 $i$ 的特征向量 $F(i)$，并用一个依赖于 $F(i)$ 的函数替代
    $J_{\mu}(i)$，即一种形式的函数
- en: '|  | $\hat{J}_{\mu}\big{(}F(i)\big{)}\approx J_{\mu}(i),\qquad i=1,\ldots,n.$
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{J}_{\mu}\big{(}F(i)\big{)}\approx J_{\mu}(i),\qquad i=1,\ldots,n.$
    |  |'
- en: We refer to such $\hat{J}_{\mu}$ as a feature-based approximation architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种 $\hat{J}_{\mu}$ 称为基于特征的近似架构。
- en: 'At the typical iteration of our approximate PI methodology, the cost vector
    $J_{\mu}$ of the current policy $\mu$ is approximated using a feature-based architecture
    $\hat{J}_{\mu}$, and a new policy $\hat{\mu}$ is then generated using a policy
    “improvement” procedure; see Fig. 1.1\. The salient characteristics of our approach
    are two:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的方法的典型迭代中，当前政策 $\mu$ 的成本向量 $J_{\mu}$ 使用基于特征的架构 $\hat{J}_{\mu}$ 进行近似，然后通过政策“改进”过程生成新的政策
    $\hat{\mu}$；见图 1.1。我们方法的显著特征有两个：
- en: (a) The feature vector $F(i)$ may be obtained using a neural network or other
    calculation that automatically constructs features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 特征向量 $F(i)$ 可以通过神经网络或其他自动构建特征的计算方法获得。
- en: (b) The policy improvement, which generates $\hat{\mu}$ is based on a DP problem
    that involves feature-based aggregation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 政策改进生成的 $\hat{\mu}$ 基于涉及特征聚合的 DP 问题。
- en: By contrast, the standard policy improvement method is based on the one-step
    lookahead minimization
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，标准的政策改进方法基于一步前瞻最小化
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\Big{(}g(i,u,j)+\alpha\hat{J}_{\mu}\big{(}F(j)\big{)}\Big{)},\qquad
    i=1,\ldots,n,$ |  | (1.3) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\Big{(}g(i,u,j)+\alpha\hat{J}_{\mu}\big{(}F(j)\big{)}\Big{)},\qquad
    i=1,\ldots,n,$ |  | (1.3) |'
- en: or alternatively, on multistep lookahead, possibly combined with Monte-Carlo
    tree search. We will argue that our feature-based aggregation approach has the
    potential to generate far better policies at the expense of a more computation-intensive
    policy improvement phase.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以基于多步前瞻，可能结合蒙特卡洛树搜索。我们将论证我们的基于特征的聚合方法有潜力生成更好的策略，但代价是计算密集的策略改进阶段。
- en: '![[Uncaptioned image]](img/9d8541b575044a2fb233b79db9ce7e23.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图像]](img/9d8541b575044a2fb233b79db9ce7e23.png)'
- en: Figure 1.1  Schematic view of feature-based approximate PI. The cost $J_{\mu}(i)$
    of the current policy $\mu$ starting from state $i$ is replaced by an approximation
    ${\hat{J\mkern 5.0mu}\mkern-5.0mu}{}_{\mu}\big{(}F(i)\big{)}$ that depends on
    $i$ through its feature vector $F(i)$. The feature vector is assumed independent
    of the current policy $\mu$ in this figure, but in general could depend on $\mu$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 特征基础的近似PI的示意图。当前策略$\mu$从状态$i$开始的成本$J_{\mu}(i)$被一个依赖于状态$i$的近似值${\hat{J\mkern
    5.0mu}\mkern-5.0mu}{}_{\mu}\big{(}F(i)\big{)}$所替代，这个近似值通过其特征向量$F(i)$依赖于$i$。在此图中，假设特征向量与当前策略$\mu$无关，但通常可能依赖于$\mu$。
- en: 1.1    Alternative Approximate Policy Iteration Methods
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1.1 替代的近似策略迭代方法
- en: A survey of approximate PI methods was given in 2011 by the author [Ber11a],
    and focused on linear feature-based architectures. These are architectures where
    $F(i)$ is an $s$-dimensional vector
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年，作者[Ber11a]对近似PI方法进行了综述，重点是线性特征基础架构。这些架构中$F(i)$是一个$s$维向量
- en: '|  | $F(i)=\big{(}F_{1}(i),\ldots,F_{s}(i)\big{)},$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i)=\big{(}F_{1}(i),\ldots,F_{s}(i)\big{)},$ |  |'
- en: and $\hat{J}_{\mu}$ depends linearly on $F$, i.e.,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并且$\hat{J}_{\mu}$在线性上依赖于$F$，即，
- en: '|  | $\hat{J}_{\mu}\big{(}F(i)\big{)}=\sum_{\ell=1}^{s}F_{\ell}(i)r_{\ell},\qquad
    i=1,\ldots,n,$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{J}_{\mu}\big{(}F(i)\big{)}=\sum_{\ell=1}^{s}F_{\ell}(i)r_{\ell},\qquad
    i=1,\ldots,n,$ |  |'
- en: 'for some scalar weights $r_{1},\ldots,r_{s}$. We considered in [Ber11a] two
    types of methods:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些标量权重$r_{1},\ldots,r_{s}$。我们在[Ber11a]中考虑了两种方法：
- en: (a) Projected equation methods, including temporal difference methods, where
    policy evaluation is based on simulation-based matrix inversion methods such as
    LSTD($\lambda$), or stochastic iterative methods such as TD($\lambda$), or variants
    of $\lambda$-policy iteration such as LSPE($\lambda$)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 投影方程方法，包括时间差分方法，其中策略评估基于模拟矩阵反演方法如LSTD($\lambda$)，或基于TD($\lambda$)的随机迭代方法，或$\lambda$-策略迭代的变体，如LSPE($\lambda$)]。
- en: (b) General aggregation methods (not just the feature-based type considered
    here).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一般的聚合方法（不仅仅是这里考虑的基于特征的方法）。
- en: These methods will be briefly discussed in Section 2. The present paper is complementary
    to the survey [Ber11a], and deals with approximate PI with nonlinear feature-based
    architectures, including some where features are generated with the aid of neural
    networks or some other heuristic calculations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法将在第2节中简要讨论。本文与综述[Ber11a]互补，涉及基于特征的非线性架构的近似PI，包括一些通过神经网络或其他启发式计算生成特征的方法。
- en: An important advantage of linear feature-based architectures is that given the
    form of the feature vector $F(\cdot)$, they can be trained with linear least squares-type
    methods. However, determining good features may be a challenge in general. Neural
    networks resolve this challenge through training that constructs automatically
    features and simultaneously combines the components of the features linearly with
    weights. This is commonly done by cost fitting/nonlinear regression, using a large
    number of state-cost sample pairs, which are processed through a sequence of alternately
    linear and nonlinear layers (see Section 3). The outputs of the final nonlinear
    layer are the features, which are then processed by a final linear layer that
    provides a linear combination of the features as a cost function approximation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 线性特征基础架构的一个重要优势是，给定特征向量$F(\cdot)$的形式，它们可以通过线性最小二乘类型的方法进行训练。然而，确定良好的特征可能是一项挑战。神经网络通过训练解决了这一挑战，该训练自动构建特征并同时用权重线性组合特征的各个组件。这通常通过成本拟合/非线性回归完成，使用大量状态-成本样本对，这些样本对通过一系列交替线性和非线性层进行处理（参见第3节）。最终非线性层的输出是特征，然后由最终线性层处理，提供特征的线性组合作为成本函数的近似。
- en: The idea of representing cost functions in terms of features of the state in
    a context that we may now call “approximation in value space” or “approximate
    DP” goes back to the work of Shannon on chess [Sha50]. The work of Samuel [Sam59],
    [Sam67] on checkers extended some of Shannon’s algorithmic schemes and introduced
    temporal difference ideas that motivated much subsequent research. The use of
    neural networks to simultaneously extract features of the optimal or the policy
    cost functions, and construct an approximation to these cost functions was also
    investigated in the early days of reinforcement learning; some of the original
    contributions that served as motivation for much subsequent work are Werbos [Wer77],
    Barto, Sutton, and Anderson [BSA83], Christensen and Korf [ChK86], Holland [Hol86],
    and Sutton [Sut88]. The use of a neural network as a cost function approximator
    for a challenging DP problem was first demonstrated impressively in the context
    of the game of backgammon by Tesauro [Tes92], [Tes94], [Tes95], [Tes02]. In Tesauro’s
    work the parameters of the network were trained by using a form of temporal differences
    (TD) learning, and the features constructed by the neural network were supplemented
    by some handcrafted features.^†^†† Tesauro also constructed a different backgammon
    player, trained by a neural network, but with a supervised learning approach,
    which used examples from human expert play [Tes89a], [Tes89b] (he called this
    approach “comparison learning”). However, his TD-based algorithm performed substantially
    better, and its success has been replicated by others, in both research and commercial
    programs. Tesauro and Galperin [TeG96] proposed still another approach to backgammon,
    based on a rollout strategy, which resulted in an even better playing program
    (see [Ber17] for an extensive discussion of rollout as a general approximate DP
    approach). At present, rollout-based backgammon programs are viewed as the most
    powerful in terms of performance, but are too time-consuming for real-time play.
    They have been used in a limited diagnostic way to assess the quality of neural
    network-based programs. A list of articles on computer backgammon may be found
    at http://www.bkgm.com/articles/page07.html.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将成本函数表示为状态特征的想法，在我们现在可能称之为“价值空间中的近似”或“近似动态规划”的背景下，可以追溯到香农在国际象棋上的工作 [Sha50]。塞缪尔
    [Sam59]，[Sam67] 在跳棋上的工作扩展了香农的一些算法方案，并引入了时间差分的思想，这些思想激发了许多后续研究。早期强化学习中也研究了同时提取最优或策略成本函数特征并构造这些成本函数近似值的神经网络的使用；一些最初的贡献，包括
    Werbos [Wer77]、Barto、Sutton 和 Anderson [BSA83]、Christensen 和 Korf [ChK86]、Holland
    [Hol86] 和 Sutton [Sut88]，为许多后续工作提供了动机。神经网络作为挑战性动态规划问题的成本函数逼近器的使用，最早由 Tesauro [Tes92]、[Tes94]、[Tes95]、[Tes02]
    在掼蛋游戏的背景下引人注目地展示。在 Tesauro 的工作中，网络的参数通过使用时间差分（TD）学习的形式进行训练，神经网络构造的特征得到了某些手工特征的补充。^†^††
    Tesauro 还构造了一个不同的掼蛋玩家，该玩家由神经网络训练，但采用了监督学习方法，使用了来自人类专家游戏的示例 [Tes89a]、[Tes89b]（他称这种方法为“比较学习”）。然而，他的基于
    TD 的算法表现显著更好，而且它的成功已被其他研究和商业程序复制。Tesauro 和 Galperin [TeG96] 提出了另一种基于回合策略的掼蛋方法，结果得到一个更好的游戏程序（有关回合作为一般近似动态规划方法的详细讨论，请参见
    [Ber17]）。目前，基于回合的掼蛋程序在性能方面被视为最强大，但对于实时游戏来说太耗时。它们已被用作有限的诊断方式来评估基于神经网络的程序的质量。有关计算机掼蛋的文章列表可以在
    http://www.bkgm.com/articles/page07.html 找到。
- en: Following Tesauro’s work, the synergistic potential of approximations using
    neural network or other architectures, and DP techniques had become apparent,
    and it was laid out in an influential survey paper by Barto, Bradtke, and Singh
    [BBS95]. It was then systematically developed in the neuro-dynamic programming
    book by Bertsekas and Tsitsiklis [BeT96], and the reinforcement learning book
    by Sutton and Barto [SuB98]. Subsequent books on approximate DP and reinforcement
    learning, which discuss approximate PI, among other techniques, include Cao [Cao07],
    Busoniu et. al. [BBD10], Szepesvari [Sze10], Powell [Pow11], Chang, Fu, Hu, and
    Marcus [CFH13], Vrabie, Vamvoudakis, and Lewis [VVL13], and Gosavi [Gos15]. To
    these, we may add the edited collections by Si, Barto, Powell, and Wunsch [SBP04],
    Lewis, Liu, and Lendaris [LLL08], and Lewis and Liu [LeL12], which contain several
    survey papers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Tesauro 的研究工作，使用神经网络或其他架构的近似和动态规划技术的协同潜力变得显而易见，这在 Barto、Bradtke 和 Singh [BBS95]
    的一篇具有影响力的综述论文中进行了阐述。随后，这一领域在 Bertsekas 和 Tsitsiklis [BeT96] 的神经动态规划书籍以及 Sutton
    和 Barto [SuB98] 的强化学习书籍中得到了系统化的发展。随后关于近似动态规划和强化学习的书籍，包括 Cao [Cao07]，Busoniu 等人
    [BBD10]，Szepesvari [Sze10]，Powell [Pow11]，Chang、Fu、Hu 和 Marcus [CFH13]，Vrabie、Vamvoudakis
    和 Lewis [VVL13]，以及 Gosavi [Gos15]，讨论了近似 PI 和其他技术。除此之外，还有 Si、Barto、Powell 和 Wunsch
    [SBP04]，Lewis、Liu 和 Lendaris [LLL08]，以及 Lewis 和 Liu [LeL12] 编辑的集合，其中包含几篇综述论文。
- en: The original ideas on approximate PI were enriched by further research ideas
    such as rollout (Abramson [Abr90], Tesauro and Galperin [TeG96], Bertsekas, Tsitsiklis,
    and Wu [BTW97], Bertsekas and Castanon [BeC99]; see the surveys in [Ber13], [Ber17]),
    adaptive simulation and Monte Carlo tree search (Chang, Hu, Fu, and Marcus [CFH05],
    [CFH13], Coulom [Cou06]; see the survey by Browne et al. [BPW12]), and deep neural
    networks (which are neural networks with many and suitably specialized layers;
    see for the example the book by Goodfellow, Bengio, and Courville [GBC16], the
    textbook discussion in [Ber17], Ch. 6, and the recent surveys by Schmidhuber [Sch15],
    Arulkumaran et al. [ADB17], Liu et al. [LWL17], and Li [Li17]).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于近似 PI 的原始想法，通过进一步的研究思想得到了丰富，例如展开方法（Abramson [Abr90]，Tesauro 和 Galperin [TeG96]，Bertsekas、Tsitsiklis
    和 Wu [BTW97]，Bertsekas 和 Castanon [BeC99]；见 [Ber13]，[Ber17] 的综述），自适应模拟和蒙特卡洛树搜索（Chang、Hu、Fu
    和 Marcus [CFH05]，[CFH13]，Coulom [Cou06]；见 Browne 等人 [BPW12] 的综述），以及深度神经网络（即具有许多适当专业层的神经网络；例如，见
    Goodfellow、Bengio 和 Courville [GBC16] 的书，讨论 [Ber17] 第6章的教材，以及 Schmidhuber [Sch15]、Arulkumaran
    等人 [ADB17]、Liu 等人 [LWL17] 和 Li [Li17] 的最新综述）。
- en: A recent impressive success of the deep neural network-based approximate PI
    methodology is the AlphaZero program, which attained a superhuman level of play
    for the games of chess, Go, and others (see Silver et al. [SHS17]). A noteworthy
    characteristic of this program is that it does not use domain-specific knowledge
    (i.e., handcrafted features), but rather relies entirely on the deep neural network
    to construct features for cost function approximation (at least as reported in
    [SHS17]). Whether it is advisable to rely exclusively on the neural network to
    provide features is an open question, as other investigations, including the ones
    by Tesauro noted earlier, suggest that using additional problem-specific hand-crafted
    features can be very helpful in the context of approximate DP. Except for the
    use of deep rather than shallow neural networks (which are used in backgammon),
    the AlphaZero algorithm is similar to several other algorithms that have been
    proposed in the literature and/or have been developed in the past. It can be viewed
    as a conceptually straightforward implementation of approximate PI, using Monte
    Carlo tree search and a single neural network to construct a cost and policy approximation,
    and does not rely on any fundamentally new ideas or insightful theoretical analysis.
    Conceptually, it bears considerable similarity to Tesauro’s TD-Gammon program.
    Its spectacular success may be attributed to the skillful implementation of an
    effective mix of known ideas, coupled with great computational power.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络基于近似 PI 方法的一个令人印象深刻的最近成功是 AlphaZero 程序，该程序在国际象棋、围棋等游戏中达到了超越人类水平的表现（参见
    Silver 等人的 [SHS17]）。这个程序的一个显著特点是它不使用领域特定的知识（即手工制作的特征），而是完全依赖深度神经网络来构建成本函数近似的特征（至少据
    [SHS17] 报告）。是否仅仅依赖神经网络来提供特征是一个开放的问题，因为其他调查，包括 Tesauro 等人的调查，表明在近似 DP 的背景下使用额外的问题特定手工制作的特征可以非常有帮助。除了使用深度而不是浅层神经网络（后者用于黑白棋），AlphaZero
    算法与文献中提出和/或过去开发的几种其他算法类似。它可以被看作是近似 PI 的概念上直接的实现，使用蒙特卡洛树搜索和单个神经网络来构建成本和策略的近似，并且不依赖于任何根本上新的想法或深刻的理论分析。从概念上看，它与
    Tesauro 的 TD-Gammon 程序有相当大的相似性。它的惊人成功可能归因于对已知思想的有效混合的熟练实现，再加上强大的计算能力。
- en: We note that the ability to simultaneously extract features and optimize their
    linear combination is not unique to neural networks. Other approaches that use
    a multilayer architecture have been proposed (see the survey by Schmidhuber [Sch15]),
    including the Group Method for Data Handling (GMDH), which is principally based
    on the use of polynomial (rather than sigmoidal) nonlinearities. The GMDH method
    was investigated extensively in the Soviet Union starting with the work of Ivakhnenko
    in the late 60s; see e.g., [Iva68]. It has been used in a large variety of applications,
    and its similarities with the neural network methodology have been noted (see
    the survey by Ivakhnenko [Iva71], and the large literature summary at the web
    site http://www.gmdh.net). Most of the GMDH research relates to inference-type
    problems. We are unaware of any application of GMDH in the context of approximate
    DP, but we believe this to be a fruitful area of investigation. In any case, the
    feature-based PI ideas of the present paper apply equally well in conjunction
    with GMDH networks as with the neural networks described in Section 3.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到同时提取特征并优化它们的线性组合的能力并不局限于神经网络。已经提出了其他使用多层架构的方法（参见 Schmidhuber 的综述 [Sch15]），包括基于多项式（而不是
    Sigmoid 函数）非线性的 Group Method for Data Handling (GMDH)。GMDH 方法从 60 年代后期苏联的 Ivakhnenko
    的工作开始得到广泛研究；详见例如 [Iva68]。它已在大量应用中使用，并且已注意到它与神经网络方法的相似之处（参见 Ivakhnenko 的综述 [Iva71]
    和网站 http://www.gmdh.net 上的大量文献总结）。大多数 GMDH 研究涉及推理类型问题。我们不知道在近似 DP 的背景下是否有任何 GMDH
    的应用，但我们认为这是一个有成果的研究领域。无论如何，本文的基于特征的 PI 思想同样适用于 GMDH 网络以及第 3 节中描述的神经网络。
- en: While automatic feature extraction is a critically important aspect of neural
    network architectures, the linearity of the combination of the feature components
    at the final layer may be a limitation. A nonlinear alternative is based on aggregation,
    a dimensionality reduction approach to address large-scale problems. This approach
    has a long history in scientific computation and operations research (see for
    example Bean, Birge, and Smith [BBS87], Chatelin and Miranker [ChM82], Douglas
    and Douglas [DoD93], Mendelssohn [Men82], and Rogers et. al. [RPW91]). It was
    introduced in the simulation-based approximate DP context, mostly in the form
    of value iteration; see Singh, Jaakkola, and Jordan [SJJ95], Gordon [Gor95], Tsitsiklis
    and Van Roy [TsV96] (see also the book [BeT96], Sections 3.1.2 and 6.7). More
    recently, aggregation was discussed in a reinforcement learning context involving
    the notion of “options” by Ciosek and Silver [CiS15], and the notion of “bottleneck
    simulator” by Serban et. al. [SSP18]; in both cases encouraging computational
    results were presented. Aggregation architectures based on features were discussed
    in Section 3.1.2 of the neuro-dynamic programming book [BeT96], and in Section
    6.5 of the author’s DP book [Ber12] (and earlier editions), including the feature-based
    architecture that is the focus of the present paper. They have the capability
    to produce policy cost function approximations that are nonlinear functions of
    the feature components, thus yielding potentially more accurate approximations.
    Basically, in feature-based aggregation the original problem is approximated by
    a problem that involves a relatively small number of “feature states.”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然自动特征提取是神经网络架构中一个至关重要的方面，但最终层特征组件组合的线性可能是一个限制。非线性的替代方法基于聚合，这是解决大规模问题的降维方法。这种方法在科学计算和运筹学中有着悠久的历史（例如
    Bean、Birge 和 Smith [BBS87]、Chatelin 和 Miranker [ChM82]、Douglas 和 Douglas [DoD93]、Mendelssohn
    [Men82]、以及 Rogers 等 [RPW91]）。它最早在基于仿真的近似 DP 上下文中引入，主要以价值迭代的形式出现；参见 Singh、Jaakkola
    和 Jordan [SJJ95]、Gordon [Gor95]、Tsitsiklis 和 Van Roy [TsV96]（另见书籍 [BeT96]，第 3.1.2
    节和第 6.7 节）。最近，聚合在涉及“选项”概念的强化学习上下文中被 Ciosek 和 Silver [CiS15] 讨论，在涉及“瓶颈模拟器”概念的上下文中被
    Serban 等 [SSP18] 讨论；在这两种情况下，都展示了令人鼓舞的计算结果。基于特征的聚合架构在神经动态规划书籍 [BeT96] 的第 3.1.2
    节和作者的 DP 书籍 [Ber12]（及早期版本）的第 6.5 节中进行了讨论，包括本论文重点关注的基于特征的架构。它们能够生成特征组件的非线性函数的策略成本函数近似，从而提供潜在更准确的近似。基本上，在基于特征的聚合中，原始问题被近似为一个涉及相对较少的“特征状态”的问题。
- en: 'Feature-based aggregation assumes a given form of feature vector, so for problems
    where good features are not apparent, it needs to be modified or to be supplemented
    by a method that can construct features from training data. Motivated by the reported
    successes of deep reinforcement learning with neural networks, we propose a two-stage
    process: first use a neural network or other scheme to construct good features
    for cost approximation, and then use there features to construct a nonlinear feature-based
    aggregation architecture. In effect we are proposing a new way to implement approximate
    PI: retain the policy evaluation phase which uses a neural network or alternative
    scheme, but replace the policy improvement phase with the solution of an aggregate
    DP problem. This DP problem involves the features that are generated by a neural
    network or other scheme (possibly together with other handcrafted features). Its
    dimension may be reduced to a manageable level by sampling, while its cost function
    values are generalized to the entire feature space by linear interpolation. In
    summary, our suggested policy improvement phase may be more complicated, but may
    be far more powerful as it relies on the potentially more accurate function approximation
    provided by a nonlinear combination of features.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的聚合假设给定了特征向量的形式，因此对于那些特征不明显的问题，它需要进行修改或补充一种可以从训练数据中构建特征的方法。受到深度强化学习在神经网络中成功案例的启发，我们提出了一种两阶段的过程：首先使用神经网络或其他方案构建用于成本近似的良好特征，然后使用这些特征构建一个非线性的特征基础聚合架构。实际上，我们提出了一种实现近似
    PI 的新方法：保留使用神经网络或其他方案的策略评估阶段，但将策略改进阶段替换为解决一个聚合 DP 问题。这个 DP 问题涉及由神经网络或其他方案（可能还包括其他手工特征）生成的特征。其维度可以通过采样减少到可管理的水平，而其成本函数值通过线性插值推广到整个特征空间。总之，我们建议的策略改进阶段可能更复杂，但可能更强大，因为它依赖于由特征的非线性组合提供的潜在更准确的函数近似。
- en: 'Aside from the power brought to bear by nonlinearly combining features, let
    us also note some other advantages that are generic to aggregation. In particular:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了非线性组合特征所带来的强大能力外，还要注意一些对聚合来说通用的其他优势。特别是：
- en: '(a) Aggregation aims to solve an “aggregate” DP problem, itself an approximation
    of the original DP problem, in the spirit of coarse-grid discretization of large
    state space problems. As a result, aggregation methods enjoy the stability and
    policy convergence guarantee of exact PI. By contrast, temporal difference-based
    and other PI methods can suffer from convergence difficulties such as policy oscillations
    and chattering (see e.g., [BeT96], [Ber11a], [Ber12]). A corollary to this is
    that when an aggregation scheme performs poorly, it is easy to identify the cause:
    it is the quantization error due to approximating a large state space with a smaller
    “aggregate” space. The possible directions for improvement (at a computational
    cost of course) are then clear: introduce additional aggregate states, and increase/improve
    these features.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 聚合旨在解决一个“聚合”DP问题，该问题本身是原始DP问题的近似，符合大状态空间问题的粗网格离散化精神。因此，聚合方法享有与精确PI相同的稳定性和策略收敛保证。相比之下，基于时间差的和其他PI方法可能会遭遇收敛困难，例如策略振荡和抖动（参见例如，[BeT96]，[Ber11a]，[Ber12]）。由此推论，当聚合方案表现不佳时，很容易识别原因：是由于用较小的“聚合”空间来近似较大的状态空间造成的量化误差。改进的可能方向（当然会有计算成本）也很明确：引入额外的聚合状态，并增加/改进这些特征。
- en: (b) Aggregation methods are characterized by error bounds, which are generic
    to PI methods that guarantee the convergence of the generated policies. These
    error bounds are better by a factor $(1-\alpha)$ compared to the corresponding
    error bounds for methods where policies need not converge, such as generic temporal
    difference methods with linear cost function approximation [see Eqs. (2.2) and
    (2.3) in the next section].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 聚合方法的特点是误差界限，这些误差界限是对PI方法的通用保证，确保生成策略的收敛。这些误差界限比需要策略收敛的方法的相应误差界限好一个因子$(1-\alpha)$，例如线性成本函数近似的通用时间差方法[参见下一节的方程(2.2)和(2.3)]。
- en: Let us finally note that the idea of using a deep neural network to extract
    features for use in another approximation architecture has been used earlier.
    In particular, it is central in the Deepchess program by David, Netanyahu, and
    Wolf [DNW16], which was estimated to perform at the level of a strong grandmaster,
    and at the level of some of the strongest computer chess programs. In this work
    the features were used, in conjunction with supervised learning and human grandmaster
    play selections, to train a deep neural network to compare any pair of legal moves
    in a given chess position, in the spirit of Tesauro’s comparison training approach
    [Tes89b]. By contrast in our proposal the features are used to formulate an aggregate
    DP problem, which can be solved by exact methods, including some that are based
    on simulation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要注意的是，利用深度神经网络提取特征以用于其他近似架构的想法早已被使用。特别是，它在David、Netanyahu和Wolf [DNW16] 的Deepchess程序中发挥了核心作用，该程序估计达到强大大师级别，与一些最强的计算机棋类程序相当。在这项工作中，这些特征与监督学习和人类大师棋局选择相结合，用于训练深度神经网络，以比较给定棋局中的任意一对合法棋步，符合Tesauro的比较训练方法[Ts89b]。相比之下，在我们的提议中，这些特征被用来制定一个聚合DP问题，可以通过精确的方法来解决，包括一些基于模拟的方法。
- en: The paper is organized as follows. In Section 2, we provide context for the
    subsequent developments, and summarize some of the implementation issues in approximate
    PI methods. In Section 3, we review some of the central ideas of approximate PI
    based on neural networks. In Section 4, we discuss PI ideas based on feature-based
    aggregation, assuming good features are known. In this section, we also discuss
    how features may be constructed on one or more “scoring functions,” which are
    estimates of the cost function of a policy, provided by a neural network or a
    heuristic. We also pay special attention to deterministic discrete optimization
    problems. Finally, in Section 5, we describe some of the ways to combine the feature
    extraction capability of deep neural networks with the nonlinear approximation
    possibilities offered by aggregation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 论文组织如下。在第2节中，我们提供了后续发展的背景，并总结了近似策略迭代方法中的一些实施问题。在第3节中，我们回顾了基于神经网络的近似策略迭代的一些核心思想。在第4节中，我们讨论了基于特征聚合的策略迭代思想，假设已知良好的特征。在这一节中，我们还讨论了如何在一个或多个“评分函数”上构建特征，这些函数是由神经网络或启发式方法提供的策略的成本函数估计值。我们还特别关注确定性离散优化问题。最后，在第5节中，我们描述了将深度神经网络的特征提取能力与聚合提供的非线性近似可能性结合的一些方法。
- en: 1.2    Terminology
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 1.2 术语
- en: The success of approximate DP in addressing challenging large-scale applications
    owes much to an enormously beneficial cross-fertilization of ideas from decision
    and control, and from artificial intelligence. The boundaries between these fields
    are now diminished thanks to a deeper understanding of the foundational issues,
    and the associated methods and core applications. Unfortunately, however, there
    have been substantial discrepancies of notation and terminology between the artificial
    intelligence and the optimization/decision/control fields, including the typical
    use of maximization/value function/reward in the former field and the use of minimization/cost
    function/cost per stage in the latter field. The notation and terminology used
    in this paper is standard in DP and optimal control, and in an effort to forestall
    confusion of readers that are accustomed to either the reinforcement learning
    or the optimal control terminology, we provide a list of selected terms commonly
    used in reinforcement learning (for example in the popular book by Sutton and
    Barto [SuB98], and its 2018 on-line 2nd edition), and their optimal control counterparts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 近似动态规划在处理具有挑战性的规模应用方面的成功，很大程度上归功于决策与控制领域与人工智能领域的思想交叉融合。这些领域之间的界限因对基础问题以及相关方法和核心应用的更深入理解而减少。然而，不幸的是，人工智能与优化/决策/控制领域之间存在显著的符号和术语差异，包括前者领域中的最大化/价值函数/奖励的典型使用，以及后者领域中的最小化/成本函数/每阶段成本的使用。本文使用的符号和术语在动态规划和最优控制中是标准的，为了避免让习惯于强化学习或最优控制术语的读者产生混淆，我们提供了一些在强化学习中常用的术语（例如，Sutton
    和 Barto 的热门书籍 [SuB98] 及其 2018 年在线第二版）及其最优控制对等术语的列表。
- en: (a) Agent = Controller or decision maker.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 代理 = 控制器或决策者。
- en: (b) Action = Control.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 行动 = 控制。
- en: (c) Environment = System.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 环境 = 系统。
- en: (d) Reward of a stage = (Opposite of) Cost of a stage.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 阶段奖励 = （阶段的）成本的相反数。
- en: (e) State value = (Opposite of) Cost of a state.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 状态价值 = （状态的）成本的相反数。
- en: (f) Value (or state-value) function = (Opposite of) Cost function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 价值（或状态价值）函数 = （成本函数的）相反数。
- en: (g) Maximizing the value function = Minimizing the cost function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 最大化价值函数 = 最小化成本函数。
- en: (h) Action (or state-action) value = $Q$-factor of a state-control pair.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 行动（或状态-行动）价值 = 状态-控制对的 $Q$-因子。
- en: (i) Planning = Solving a DP problem with a known mathematical model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 规划 = 使用已知数学模型解决动态规划问题。
- en: (j) Learning = Solving a DP problem in model-free fashion.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (j) 学习 = 以无模型的方式解决动态规划问题。
- en: (k) Self-learning (or self-play in the context of games) = Solving a DP problem
    using policy iteration.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (k) 自学习（或在游戏背景下的自对弈） = 使用策略迭代解决动态规划问题。
- en: (l) Deep reinforcement learning = Approximate DP using value and/or policy approximation
    with deep neural networks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (l) 深度强化学习 = 使用深度神经网络进行价值和/或策略近似的近似动态规划。
- en: (m) Prediction = Policy evaluation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (m) 预测 = 策略评估。
- en: (n) Generalized policy iteration = Optimistic policy iteration.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: (n) 广义策略迭代 = 乐观策略迭代。
- en: (o) State abstraction = Aggregation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (o) 状态抽象 = 聚合。
- en: (p) Episodic task or episode = Finite-step system trajectory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (p) 情节任务或情节 = 有限步系统轨迹。
- en: (q) Continuing task = Infinite-step system trajectory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (q) Continuing task = Infinite-step system trajectory.
- en: (r) Afterstate = Post-decision state.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (r) Afterstate = Post-decision state.
- en: '2. APPROXIMATE POLICY ITERATION: AN OVERVIEW'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 近似策略迭代：概述
- en: 'Many approximate DP algorithms are based on the principles of PI: the policy
    evaluation/policy improvement structure of PI is maintained, but the policy evaluation
    is done approximately, using simulation and some approximation architecture. In
    the standard form of the method, at each iteration, we compute an approximation
    $\tilde{J}_{\mu}(\cdot,r)$ to the cost function $J_{\mu}$ of the current policy
    $\mu$, and we generate an “improved” policy $\hat{\mu}$ using^†^†† The minimization
    in the policy improvement phase may alternatively involve multistep lookahead,
    possibly combined with Monte-Carlo tree search. It may also be done approximately
    through $Q$-factor approximations. Our discussion extends straightfowardly to
    schemes that include multistep lookahead or approximate policy improvement.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 许多近似动态规划算法基于PI的原则：保持PI的策略评估/策略改进结构，但是策略评估是通过模拟和一些逼近结构来进行的。在方法的标准形式中，每次迭代时，我们计算当前策略$\mu$的成本函数$J_{\mu}$的逼近$\tilde{J}_{\mu}(\cdot,r)$，并生成一个“改进”的策略$\hat{\mu}$，使用^†^††
    策略改进阶段的最小化可能会涉及多步展望，可能与蒙特卡罗树搜索结合。也可以通过$Q$因子逼近来进行近似。我们的讨论直接延伸到包括多步展望或近似策略改进的方案。
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\bigl{(}g(i,u,j)+\alpha\tilde{J}_{\mu}(j,r)\bigr{)},\qquad
    i=1,\ldots,n.$ |  | (2.1) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\bigl{(}g(i,u,j)+\alpha\tilde{J}_{\mu}(j,r)\bigr{)},\qquad
    i=1,\ldots,n.$ |  | (2.1) |'
- en: Here $\tilde{J}_{\mu}$ is a function of some chosen form (the approximation
    architecture), which depends on the state and on a parameter vector $r=(r_{1},\ldots,r_{s})$
    of relatively small dimension $s$.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的$\tilde{J}_{\mu}$是某种选择形式的函数（逼近结构），它依赖于状态和一个维度相对较小的参数向量$r=(r_{1},\ldots,r_{s})$。
- en: The theoretical basis for the method was discussed in the neuro-dynamic programming
    book [BeT96], Prop. 6.2 (see also [Ber12], Section 2.5.6, or [Ber18a], Sections
    2.4.1 and 2.4.2). It was shown there that if the policy evaluation is accurate
    to within $\delta$ (in the sup-norm sense), then for an $\alpha$-discounted problem,
    the method, while not convergent, is stable in the sense that it will yield in
    the limit (after infinitely many policy evaluations) stationary policies that
    are optimal to within
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的理论基础在神经动态规划书籍[BeT96]，Prop. 6.2（另见[Ber12]，Section 2.5.6，或[Ber18a]，Sections
    2.4.1和2.4.2）中有所讨论。在那里指出，如果策略评估在$\delta$（在sup-范数意义上）内准确，则对于$\alpha$-折现问题，该方法虽然不是收敛的，但在稳定性上表现良好，极限情况下（经过无限次策略评估后）将产生最优的稳态策略，这些策略在某种程度上是最优的
- en: '|  | ${2\alpha\delta\over(1-\alpha)^{2}},$ |  | (2.2) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | ${2\alpha\delta\over(1-\alpha)^{2}},$ |  | (2.2) |'
- en: where $\alpha$ is the discount factor. Moreover, if the generated sequence of
    policies actually converges to some $\bar{\mu}$, then $\bar{\mu}$ is optimal to
    within
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$\alpha$是折现因子。此外，如果生成的策略序列实际上收敛到某个$\bar{\mu}$，那么$\bar{\mu}$在某种程度上是最优的
- en: '|  | ${2\alpha\delta\over 1-\alpha}$ |  | (2.3) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | ${2\alpha\delta\over 1-\alpha}$ |  | (2.3) |'
- en: (see [BeT96], Section 6.4.1); this is a significantly improved error bound.
    In general, policy convergence may not be guaranteed, although it is guaranteed
    for the aggregation methods of this paper. Experimental evidence indicates that
    these bounds are often conservative, with just a few policy iterations needed
    before most of the eventual cost improvement is achieved.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: （见[BeT96]，Section 6.4.1）；这是一个显著改进的误差界限。一般来说，策略收敛性可能无法保证，尽管本文的聚合方法保证了这一点。实验数据表明，这些界限通常是保守的，大部分最终成本改进只需少数策略迭代即可实现。
- en: 2.1    Direct and Indirect Approximation Approaches for Policy Evaluation
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2.1    直接和间接逼近方法用于策略评估
- en: Given a class of functions ${\cal J}$ that defines an approximation architecture,
    there are two general approaches for approximating the cost function $J_{\mu}$
    of a fixed policy $\mu$ within ${\cal J}$. The most straightforward approach,
    referred to as direct (or cost fitting), is to find a $\tilde{J}_{\mu}\in{\cal
    J}$ that matches $J_{\mu}$ in some least squares error sense, i.e.,^†^†† Nonquadratic
    optimization criteria may also be used, although in practice the simple quadratic
    cost function has been adopted most frequently.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个定义了近似架构的函数类${\cal J}$，有两种一般的方法来近似固定策略$\mu$在${\cal J}$中的成本函数$J_{\mu}$。最直接的方法称为直接（或成本拟合），是找到一个$\tilde{J}_{\mu}\in{\cal
    J}$，使其在某种最小二乘误差意义上与$J_{\mu}$匹配，即^†^†† 也可以使用非二次优化标准，但在实践中，最常采用的是简单的二次成本函数。
- en: '|  | $\tilde{J}_{\mu}\in\arg\min_{\tilde{J}\in{\cal J}}\&#124;\tilde{J}-J_{\mu}\&#124;^{2}.$
    |  | (2.4) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu}\in\arg\min_{\tilde{J}\in{\cal J}}\&#124;\tilde{J}-J_{\mu}\&#124;^{2}.$
    |  | (2.4) |'
- en: Typically $\|\cdot\|$ is some weighted Euclidean norm with positive weights
    $\xi_{i}$, $i=1,\ldots,n$, while ${\cal J}$ consists of a parametrized class of
    functions $\tilde{J}(i,r)$ where $r=(r_{1},\ldots,r_{s})\in\Re^{s}$ is the parameter
    vector, i.e.,^†^†‡ We use standard vector notation. In particular, $\Re^{s}$ denotes
    the Euclidean space of $s$-dimensional real vectors, and $\Re$ denotes the real
    line.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常$\|\cdot\|$是某种带有正权重$\xi_{i}$的加权欧几里得范数，$i=1,\ldots,n$，而${\cal J}$由一个参数化的函数类$\tilde{J}(i,r)$组成，其中$r=(r_{1},\ldots,r_{s})\in\Re^{s}$是参数向量，即^†^†‡
    我们使用标准的向量表示法。特别地，$\Re^{s}$表示$s$维实向量的欧几里得空间，而$\Re$表示实数线。
- en: '|  | ${\cal J}=\big{\{}\tilde{J}(\cdot,r)\mid r\in\Re^{s}\big{\}}.$ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\cal J}=\big{\{}\tilde{J}(\cdot,r)\mid r\in\Re^{s}\big{\}}.$ |  |'
- en: Then the minimization problem in Eq. (2.4) is written as
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，公式(2.4)中的最小化问题被写成
- en: '|  | $\min_{r\in\Re^{s}}\sum_{i=1}^{n}\xi_{i}\big{(}\tilde{J}(i,r)-J_{\mu}(i)\big{)}^{2},$
    |  | (2.5) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{r\in\Re^{s}}\sum_{i=1}^{n}\xi_{i}\big{(}\tilde{J}(i,r)-J_{\mu}(i)\big{)}^{2},$
    |  | (2.5) |'
- en: and can be viewed as an instance of nonlinear regression.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 并且可以视为非线性回归的一个实例。
- en: In simulation-based methods, the preceding minimization is usually approximated
    by a least squares minimization of the form
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于模拟的方法中，前述的最小化通常被近似为形式的最小二乘法
- en: '|  | $\min_{r\in\Re^{s}}\sum_{m=1}^{M}\big{(}\tilde{J}(i_{m},r)-\beta_{m}\big{)}^{2},$
    |  | (2.6) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{r\in\Re^{s}}\sum_{m=1}^{M}\big{(}\tilde{J}(i_{m},r)-\beta_{m}\big{)}^{2},$
    |  | (2.6) |'
- en: where $(i_{m},\beta_{m})$, $m=1,\ldots,M$, are a large number of state-cost
    sample pairs, i.e., for each $m$, $i_{m}$ is a sample state and $\beta_{m}$ is
    equal to $J_{\mu}(i_{m})$ plus some simulation noise. Under mild statistical assumptions
    on the sample collection process, the sample-based minimization (2.6) is equivalent
    in the limit to the exact minimization (2.5). Neural network-based approximation,
    as described in Section 3, is an important example of direct approximation that
    uses state-cost training pairs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$(i_{m},\beta_{m})$，$m=1,\ldots,M$，是一大批状态-成本样本对，即对于每个$m$，$i_{m}$是一个样本状态，$\beta_{m}$等于$J_{\mu}(i_{m})$加上一些模拟噪声。在对样本收集过程的轻微统计假设下，基于样本的最小化（2.6）在极限情况下等同于精确最小化（2.5）。如第3节所述的基于神经网络的近似是一个使用状态-成本训练对的直接近似的重要例子。
- en: A common choice is to take ${\cal J}$ to be the subspace $\{\Phi r\mid r\in\Re^{s}\}$
    that is spanned by the columns of an $n\times s$ matrix $\Phi$, which can be viewed
    as basis functions (see the left side of Fig. 2.1). Then the approximation problem
    (2.6) becomes the linear least squares problem
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的选择是将${\cal J}$取为由$n\times s$矩阵$\Phi$的列张成的子空间$\{\Phi r\mid r\in\Re^{s}\}$，这可以视为基函数（见图2.1左侧）。然后，近似问题（2.6）变成线性最小二乘问题
- en: '|  | $\min_{(r_{1},\ldots,r_{s})\in\Re^{s}}\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}\phi_{i_{m}\ell}r_{\ell}-\beta_{m}\right)^{2},$
    |  | (2.7) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{(r_{1},\ldots,r_{s})\in\Re^{s}}\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}\phi_{i_{m}\ell}r_{\ell}-\beta_{m}\right)^{2},$
    |  | (2.7) |'
- en: where $\phi_{i\ell}$ is the $i\ell$th entry of the matrix $\Phi$ and $r_{\ell}$
    is the $\ell$th component of $r$. The solution of this problem can be obtained
    analytically and can be written in closed form (see e.g., [BeT96], Section 3.2.2).
    Note that the $i$th row of $\Phi$ may be viewed as a feature vector of state $i$,
    and $\Phi r$ may be viewed as a linear feature-based architecture.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\phi_{i\ell}$是矩阵$\Phi$的$i\ell$项，$r_{\ell}$是$r$的$\ell$项。该问题的解可以通过解析方法获得，并且可以写成闭式形式（参见例如，[BeT96]，第3.2.2节）。注意，$\Phi$的第$i$行可以视为状态$i$的特征向量，$\Phi
    r$可以视为基于线性特征的架构。
- en: In Section 3, we will see that neural network-based policy evaluation combines
    elements of both a linear and a nonlinear architecture. The nonlinearity is embodied
    in the features that the neural network constructs through training, but once
    the features are given, the neural network can be viewed as a linear feature-based
    architecture.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3节中，我们将看到基于神经网络的策略评估结合了线性和非线性架构的元素。非线性体现在神经网络通过训练构造的特征中，但一旦特征给定，神经网络可以视为线性特征基础架构。
- en: '![[Uncaptioned image]](img/fbfddd4364cce54ae83896fcb1827bc5.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/fbfddd4364cce54ae83896fcb1827bc5.png)'
- en: Figure 2.1  Two methods for approximating the cost function $J_{\mu}$ as a linear
    combination of basis functions. The approximation architecture is the subspace
    ${\cal J}=\{\Phi r\mid r\in\Re^{s}\}$, where $\Phi$ is matrix whose columns are
    the basis functions. In the direct method (see the figure on the left), $J_{\mu}$
    is projected on ${\cal J}$. In an example of the indirect method, the approximation
    is obtained by solving the projected form of Bellman’s equation $\Phi r=\Pi T_{\mu}\Phi
    r$, where $T_{\mu}\Phi r$ is the vector with components (T_μΦr)(i)=∑_j=1^n p_ij(μ(i))(g(i,μ(i),j)
    + α(Φr)(j)),  i=1,…,n, and $(\Phi r)(j)$ is the $j$th component of the vector
    $\Phi r$ (see the figure on the right).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 两种将成本函数$J_{\mu}$近似为基函数的线性组合的方法。近似架构是子空间${\cal J}=\{\Phi r\mid r\in\Re^{s}\}$，其中$\Phi$是列为基函数的矩阵。在直接方法中（参见左侧图），$J_{\mu}$被投影到${\cal
    J}$上。在间接方法的一个示例中，通过解决贝尔曼方程的投影形式$\Phi r=\Pi T_{\mu}\Phi r$来获得近似，其中$T_{\mu}\Phi
    r$是具有分量的向量$(T_μΦr)(i)=∑_j=1^n p_ij(μ(i))(g(i,μ(i),j) + α(Φr)(j))，i=1,…,n$，而$(\Phi
    r)(j)$是向量$\Phi r$的第$j$个分量（参见右侧图）。
- en: An often cited weakness of simulation-based direct approximation is excessive
    simulation noise in the cost samples $\beta_{m}$ that are used in the least squares
    minimization (2.7) or (2.7). This has motivated alternative approaches for policy
    evaluation that inherently involve less noise. A major approach of this type,
    referred to as indirect (or equation fitting), is to approximate Bellman’s equation
    for the policy $\mu$,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟基础直接近似的一个常被提及的弱点是最小二乘法优化中使用的成本样本$\beta_{m}$的过度模拟噪声。这促使了固有地涉及较少噪声的策略评估的替代方法。这类方法的一种主要方式，称为间接（或方程拟合），是对策略$\mu$的贝尔曼方程进行近似。
- en: '|  | $J(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J(j)\Big{)},\qquad i=1,\ldots,n,$ |  | (2.8) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J(j)\Big{)},\qquad i=1,\ldots,n,$ |  | (2.8) |'
- en: with another equation that is defined on the set ${\cal J}$. The solution of
    the approximate equation is then used as an approximation of the solution of the
    original. The most common indirect methods assume a linear approximation architecture,
    i.e., ${\cal J}$ is the subspace ${\cal J}=\{\Phi r\mid r\in\Re^{s}\},$ and approximate
    Bellman’s equation with another equation with fewer variables, the $s$ parameters
    $r_{1},\ldots,r_{s}$. Two major examples of this approach are projected equation
    methods and aggregation methods, which we proceed to discuss.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用定义在集合${\cal J}$上的另一个方程。然后，近似方程的解被用作原始方程解的近似。最常见的间接方法假设线性近似架构，即${\cal J}$是子空间${\cal
    J}=\{\Phi r\mid r\in\Re^{s}\}$，并用另一个变量更少的方程来近似贝尔曼方程，$s$个参数$r_{1},\ldots,r_{s}$。这种方法的两个主要示例是投影方程方法和聚合方法，我们将继续讨论。
- en: 2.2    Indirect Methods Based on Projected Equations
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 2.2    基于投影方程的间接方法
- en: Approximation using projected equations has a long history in numerical computation
    (e.g., partial differential equations) where it is known as Galerkin approximation
    [see e.g., [KVZ72], [Fle84], [Saa03], [Kir11]]. The projected equation approach
    is a special case of the so called Bubnov-Galerkin method, as noted in the papers
    [Ber11a], [Ber11b], and [YuB10]. In the context of approximate DP it is connected
    with temporal difference methods, and it is discussed in detail in many sources
    (see e.g., [BeT96], [BBD10], [Ber12], [Gos15]).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用投影方程的近似在数值计算（例如，偏微分方程）中有着悠久的历史，其中被称为Galerkin近似[参见例如，[KVZ72]，[Fle84]，[Saa03]，[Kir11]]。投影方程方法是所谓的Bubnov-Galerkin方法的特例，正如在论文[Ber11a]，[Ber11b]和[YuB10]中所述。在近似DP的背景下，它与时间差分方法相关，并在许多来源中详细讨论（参见例如，[BeT96]，[BBD10]，[Ber12]，[Gos15]）。
- en: To state the projected equation, let us introduce the transformation $T_{\mu}$,
    which is defined by the right-hand side of the Bellman equation (2.8); i.e., for
    any $J\in\Re^{n}$, $T_{\mu}J$ is the vector of $\Re^{n}$ with components
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了陈述投影方程，引入变换 $T_{\mu}$，它由贝尔曼方程 (2.8) 的右侧定义；即，对于任何 $J\in\Re^{n}$，$T_{\mu}J$
    是 $\Re^{n}$ 的向量，其分量为
- en: '|  | $(T_{\mu}J)(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J(j)\Big{)},\qquad i=1,\ldots,n.$ |  | (2.9) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $(T_{\mu}J)(i)=\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}\Big{(}g\bigl{(}i,\mu(i),j\bigr{)}+\alpha
    J(j)\Big{)},\qquad i=1,\ldots,n.$ |  | (2.9) |'
- en: Note that $T_{\mu}$ is a linear transformation from $\Re^{n}$ to $\Re^{n}$,
    and in fact in compact vector-matrix notation, it is written as
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 $T_{\mu}$ 是从 $\Re^{n}$ 到 $\Re^{n}$ 的线性变换，实际上在紧凑的向量-矩阵符号中，它写作
- en: '|  | $T_{\mu}J=g_{\mu}+\alpha P_{\mu}J,\qquad J\in\Re^{n},$ |  | (2.10) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{\mu}J=g_{\mu}+\alpha P_{\mu}J,\qquad J\in\Re^{n},$ |  | (2.10) |'
- en: where $P_{\mu}$ is the transition probability matrix of $\mu$, and $g_{\mu}$
    is the expected cost vector of $\mu$, i.e., the vector with components
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{\mu}$ 是 $\mu$ 的转移概率矩阵，$g_{\mu}$ 是 $\mu$ 的期望成本向量，即包含以下分量的向量
- en: '|  | $\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}g\bigl{(}i,\mu(i),j\bigr{)},\qquad
    i=1,\ldots,n.$ |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{j=1}^{n}p_{ij}\bigl{(}\mu(i)\bigr{)}g\bigl{(}i,\mu(i),j\bigr{)},\qquad
    i=1,\ldots,n.$ |  |'
- en: Moreover the Bellman equation (2.8) is written as the fixed point equation
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，贝尔曼方程 (2.8) 可写为固定点方程
- en: '|  | $J=T_{\mu}J.$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $J=T_{\mu}J.$ |  |'
- en: Let us denote by $\Pi J$ the projection of a vector $J\in\Re^{n}$ onto ${\cal
    J}$ with respect to some weighted Euclidean norm, and consider $\Pi T_{\mu}\Phi
    r$, the projection of $T_{\mu}\Phi r$ (here $T_{\mu}\Phi r$ is viewed as a vector
    in $\Re^{n}$, and $\Pi$ is viewed as an $n\times n$ matrix multiplying this vector).
    The projected equation takes the form
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\Pi J$ 为向量 $J\in\Re^{n}$ 在某些加权欧几里得范数下的投影，并考虑 $\Pi T_{\mu}\Phi r$，即 $T_{\mu}\Phi
    r$ 的投影（这里 $T_{\mu}\Phi r$ 被视为 $\Re^{n}$ 中的一个向量，而 $\Pi$ 被视为一个 $n\times n$ 的矩阵对该向量进行乘法运算）。投影方程的形式为
- en: '|  | $\Phi r=\Pi T_{\mu}\Phi r;$ |  | (2.11) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi r=\Pi T_{\mu}\Phi r;$ |  | (2.11) |'
- en: see the right-hand side of Fig. 2.1\. With this equation we want to find a vector
    $\Phi r$ of ${\cal J}$, which when transformed by $T_{\mu}$ and then projected
    back onto ${\cal J}$, yields itself. This is an overdetermined system of linear
    equations ($n$ equations in the $s$ unknowns $r_{1},\ldots,r_{s}$), which is equivalently
    written as
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 见图 2.1 的右侧。通过这个方程，我们希望找到一个向量 $\Phi r$，它属于 ${\cal J}$，经过 $T_{\mu}$ 变换后再投影回 ${\cal
    J}$，得到自身。这是一个超定的线性方程组（$n$ 个方程，$s$ 个未知数 $r_{1},\ldots,r_{s}$），可以等效地写作
- en: '|  | $\sum_{\ell=1}^{s}\phi_{i\ell}r_{\ell}=\sum_{m=1}^{n}\pi_{im}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    i=1,\ldots,n;$ |  | (2.12) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{\ell=1}^{s}\phi_{i\ell}r_{\ell}=\sum_{m=1}^{n}\pi_{im}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    i=1,\ldots,n;$ |  | (2.12) |'
- en: here $\phi_{i\ell}$ is the $i\ell$th component of the matrix $\Phi$ and $\pi_{im}$
    is the ${im}$th component of the projection matrix $\Pi$. The system can be shown
    to have a unique solution under conditions that can be somewhat restrictive, e.g.,
    assuming that the Markov chain corresponding to the policy $\mu$ has a unique
    steady-state distribution with positive components, that the projection norm involves
    this distribution, and that $\Phi$ has linearly independent columns (see e.g.,
    [Ber12], Section 6.3).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\phi_{i\ell}$ 是矩阵 $\Phi$ 的 $i\ell$ 元素，$\pi_{im}$ 是投影矩阵 $\Pi$ 的 ${im}$ 元素。系统可以在一些可能较为严格的条件下证明具有唯一解，例如，假设与策略
    $\mu$ 相对应的马尔可夫链具有唯一的稳态分布且所有分量为正，该投影范数涉及该分布，并且 $\Phi$ 具有线性无关的列（参见例如 [Ber12]，第 6.3
    节）。
- en: An important extension is to replace the projected equation (2.11) with the
    equation
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的扩展是将投影方程 (2.11) 替换为方程
- en: '|  | $\Phi r=\Pi T_{\mu}^{(\lambda)}\Phi r,$ |  | (2.13) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi r=\Pi T_{\mu}^{(\lambda)}\Phi r,$ |  | (2.13) |'
- en: where $\lambda$ is a scalar with $0\leq\lambda<1$, and the transformation $T_{\mu}^{(\lambda)}$
    is defined by
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个标量，$0\leq\lambda<1$，变换 $T_{\mu}^{(\lambda)}$ 定义为
- en: '|  | $\big{(}T_{\mu}^{(\lambda)}J\big{)}(i)=(1-\lambda)\sum_{\ell=0}^{\infty}\lambda^{\ell}(T_{\mu}^{\ell+1}J)(i),\qquad
    i=1,\ldots,n,\ J\in\Re^{n},$ |  | (2.14) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $\big{(}T_{\mu}^{(\lambda)}J\big{)}(i)=(1-\lambda)\sum_{\ell=0}^{\infty}\lambda^{\ell}(T_{\mu}^{\ell+1}J)(i),\qquad
    i=1,\ldots,n,\ J\in\Re^{n},$ |  | (2.14) |'
- en: 'and $T_{\mu}^{\ell}J$ is the $\ell$-fold composition of $T_{\mu}$ applied to
    the vector $J$. This approach to the approximate solution of Bellman’s equation
    is supported by extensive theory and practical experience (see the textbooks noted
    earlier). In particular, the TD($\lambda$) algorithm, and other related temporal
    difference methods, such as LSTD($\lambda$) and LSPE($\lambda$), aim to solve
    by simulation the projected equation (2.13). The choice of $\lambda$ embodies
    the important bias-variance tradeoff: larger values of $\lambda$ lead to better
    approximation of $J_{\mu}$, but require a larger number of simulation samples
    because of increased simulation noise (see the discussion in Section 6.3.6 of
    [Ber12]). An important insight is that the operator $T_{\mu}^{(\lambda)}$ is closely
    related to the proximal operator of convex analysis (with $\lambda$ corresponding
    to the penalty parameter of the proximal operator), as shown in the author’s paper
    [Ber16a] (see also the monograph [Ber18a], Section 1.2.5, and the paper [Ber18b]).
    In particular, TD($\lambda$) can be viewed as a stochastic simulation-based version
    of the proximal algorithm.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 而 $T_{\mu}^{\ell}J$ 是对向量 $J$ 应用的 $T_{\mu}$ 的 $\ell$-次组合。对 Bellman 方程的近似解法得到了广泛的理论和实践经验的支持（参见前述教科书）。特别是，TD($\lambda$)
    算法以及其他相关的时间差分方法，如 LSTD($\lambda$) 和 LSPE($\lambda$)，旨在通过模拟来求解投影方程 (2.13)。$\lambda$
    的选择体现了重要的偏差-方差权衡：较大的 $\lambda$ 值可以更好地逼近 $J_{\mu}$，但由于增加的模拟噪声，需要更多的模拟样本（参见 [Ber12]
    第 6.3.6 节的讨论）。一个重要的见解是，操作符 $T_{\mu}^{(\lambda)}$ 与凸分析中的近端操作符密切相关（其中 $\lambda$
    对应于近端操作符的惩罚参数），如作者的论文 [Ber16a] 所示（另见专著 [Ber18a] 第 1.2.5 节以及论文 [Ber18b]）。特别是，TD($\lambda$)
    可以看作是基于随机模拟的近端算法版本。
- en: A major issue in projected equation methods is whether the linear transformation
    $\Pi T_{\mu}$ [or $\Pi T_{\mu}^{(\lambda)}$] is a contraction mapping, in which
    case Eq. (2.11) [or Eq. (2.13), respectively] has a unique solution, which may
    be obtained by iterative fixed point algorithms. This depends on the projection
    norm, and it turns out that there are special norms for which $\Pi T_{\mu}^{(\lambda)}$
    is a contraction (these are related to the steady-state distribution of the system’s
    Markov chain under $\mu$; see the discussion of [Ber11a] or Section 6.3 of [Ber12]).
    An important fact is that for any projection norm, $\Pi T_{\mu}^{(\lambda)}$ is
    a contraction provided $\lambda$ is sufficiently close to 1\. Still the contraction
    issue regarding $\Pi T_{\mu}^{(\lambda)}$ is significant and affects materially
    the implementation of the corresponding approximate PI methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 投影方程方法中的一个主要问题是线性变换 $\Pi T_{\mu}$ [或 $\Pi T_{\mu}^{(\lambda)}$] 是否是一个收缩映射，在这种情况下，方程
    (2.11) [或方程 (2.13)，分别] 有唯一解，这可能通过迭代不动点算法获得。这取决于投影范数，事实证明，存在一些特殊范数，使得 $\Pi T_{\mu}^{(\lambda)}$
    是收缩的（这些与系统在 $\mu$ 下的马尔科夫链的稳态分布有关；参见 [Ber11a] 或 [Ber12] 第 6.3 节的讨论）。一个重要的事实是，对于任何投影范数，只要
    $\lambda$ 足够接近 1\，$\Pi T_{\mu}^{(\lambda)}$ 就是收缩的。然而，关于 $\Pi T_{\mu}^{(\lambda)}$
    的收缩问题仍然很重要，并且在实际实施对应的近似 PI 方法时会产生显著影响。
- en: Another important concern is that the projection matrix $\Pi$ may have some
    negative entries [i.e., some of the components $\pi_{im}$ in Eq. (2.12) may be
    negative], and as a result the linear transformations $\Pi T_{\mu}$ and $\Pi T_{\mu}^{(\lambda)}$
    may lack the monotonicity property that is essential for the convergence of the
    corresponding approximate PI method. Indeed the lack of monotonicity (the possibility
    that we may not have $\Pi T_{\mu}J\geq\Pi T_{\mu}J^{\prime}$ for two vectors $J,J^{\prime}$
    with $J\geq J^{\prime}$) is the fundamental mathematical reason for policy oscillations
    in PI methods that are based on temporal differences (see [Ber11a], [Ber12]).
    We refer to the literature for further details and analysis regarding the projected
    equations (2.11) and (2.13), as our focus will be on aggregation methods, which
    we discuss next.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要问题是投影矩阵 $\Pi$ 可能包含一些负值条目 [即，方程 (2.12) 中的一些组件 $\pi_{im}$ 可能为负]，因此线性变换 $\Pi
    T_{\mu}$ 和 $\Pi T_{\mu}^{(\lambda)}$ 可能缺乏对于对应近似 PI 方法收敛至关重要的单调性。实际上，缺乏单调性（即可能存在
    $\Pi T_{\mu}J\geq\Pi T_{\mu}J^{\prime}$ 对于两个向量 $J,J^{\prime}$，其中 $J\geq J^{\prime}$）是基于时间差分的
    PI 方法中策略震荡的基本数学原因（参见 [Ber11a]，[Ber12]）。我们在文献中查阅关于投影方程 (2.11) 和 (2.13) 的更多细节和分析，因为我们将重点讨论聚合方法，接下来我们将讨论这些方法。
- en: 2.3    Indirect Methods Based on Aggregation
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 2.3    基于聚合的间接方法
- en: Aggregation is another major indirect approach, which has originated in numerical
    linear algebra. Simple examples of aggregation involve finite-dimensional approximations
    of infinite dimensional equations, coarse grid approximations of linear systems
    of equations defined over a dense grid, and other related methods for dimensionality
    reduction of high-dimensional systems. In the context of DP, the aggregation idea
    is implemented by replacing the Bellman equation $J=T_{\mu}J$ [cf. Eq. (2.8)]
    with a lower-dimensional “aggregate” equation, which is defined on an approximation
    subspace ${\cal J}=\{\Phi r\mid r\in\Re^{s}\}$. The aggregation counterpart of
    the projected equation $\Phi r=\Pi T_{\mu}\Phi r$ is
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合是另一种主要的间接方法，起源于数值线性代数。简单的聚合例子包括无限维方程的有限维近似、定义在密集网格上的线性方程组的粗网格近似以及其他相关的高维系统降维方法。在动态规划（DP）的背景下，聚合思想通过用一个定义在近似子空间${\cal
    J}=\{\Phi r\mid r\in\Re^{s}\}$上的低维“聚合”方程替代贝尔曼方程$J=T_{\mu}J$ [参见方程(2.8)]来实现。投影方程$\Phi
    r=\Pi T_{\mu}\Phi r$的聚合对应方程是
- en: '|  | $\Phi r=\Phi DT_{\mu}\Phi r,$ |  | (2.15) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi r=\Phi DT_{\mu}\Phi r,$ |  | (2.15) |'
- en: where $\Phi$ and $D$ are some matrices, and $T_{\mu}$ is the linear transformation
    given by Eq. (2.9).^†^†† It turns out that under some widely applicable conditions,
    including the assumptions of Section 4, the projected and aggregation equations
    are closely related. In particular, it can be proved under these conditions that
    the matrix $\Phi D$ that appears in the aggregation equation (2.15) is a projection
    with respect to a suitable weighted Euclidean seminorm (see [YuB12], Section 4,
    or the book [Ber12]; it is a norm projection in the case of hard aggregation).
    Aside from establishing the relation between the two major indirect approximation
    methods, projected equation and aggregation, this result provides the basis for
    transferring the rich methodology of temporal differences methods such as TD($\lambda$)
    to the aggregation context. This is a vector-matrix notation for the linear system
    of $n$ equations in the $s$ variables $r_{1},\ldots,r_{s}$
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Phi$和$D$是一些矩阵，$T_{\mu}$是由方程(2.9)给出的线性变换。^†^†† 结果表明，在一些广泛适用的条件下，包括第4节的假设，投影方程和聚合方程之间有紧密的关系。特别地，可以在这些条件下证明，出现在聚合方程(2.15)中的矩阵$\Phi
    D$是相对于适当加权的欧几里得半范数的投影（见[YuB12]，第4节，或书籍[Ber12]；在硬聚合的情况下是范数投影）。除了确立这两种主要间接近似方法之间的关系，投影方程和聚合外，这一结果还为将诸如TD($\lambda$)等时间差分方法的丰富方法学转移到聚合背景中提供了基础。这是一个针对$
    s $个变量$r_{1},\ldots,r_{s}$的$n$个方程的线性系统的向量-矩阵表示。
- en: '|  | $\sum_{k=1}^{s}\phi_{ik}r_{k}=\sum_{k=1}^{s}\phi_{ik}\sum_{m=1}^{n}d_{km}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{k=1}^{s}\phi_{ik}r_{k}=\sum_{k=1}^{s}\phi_{ik}\sum_{m=1}^{n}d_{km}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  |'
- en: where $\phi_{i\ell}$ is the $i\ell$th component of the matrix $\Phi$ and $d_{km}$
    is the ${km}$th component of the matrix $D$.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\phi_{i\ell}$是矩阵$\Phi$的第$i\ell$项，$d_{km}$是矩阵$D$的第${km}$项。
- en: A key restriction for aggregation methods as applied to DP is that the rows
    of $D$ and $\Phi$ should be probability distributions. These distributions usually
    have intuitive interpretations in the context of specific aggregation schemes;
    see [Ber12], Section 6.5 for a discussion. Assuming that $\Phi$ has linearly independent
    columns, which is true for the most common types of aggregation schemes, Eq. (2.15) can
    be seen to be equivalent to
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于应用于动态规划（DP）的聚合方法，一个关键限制是$D$和$\Phi$的行应当是概率分布。这些分布在具体的聚合方案中通常有直观的解释；有关讨论请参见[Ber12]，第6.5节。假设$\Phi$具有线性无关的列，这对于最常见的聚合方案类型是正确的，那么方程(2.15)可以视为等价于
- en: '|  | $r=DT_{\mu}\Phi r,$ |  | (2.16) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=DT_{\mu}\Phi r,$ |  | (2.16) |'
- en: or
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '|  | $r_{k}=\sum_{m=1}^{n}d_{km}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    k=1,\ldots,s.$ |  | (2.17) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{k}=\sum_{m=1}^{n}d_{km}\sum_{j=1}^{n}p_{mj}\big{(}\mu(m)\big{)}\left(g\big{(}m,\mu(m),j\big{)}+\alpha\sum_{\ell=1}^{s}\phi_{j\ell}r_{\ell}\right),\qquad
    k=1,\ldots,s.$ |  | (2.17) |'
- en: 'In most of the important aggregation methods, including the one of Section
    4, $D$ and $\Phi$ are chosen so that the product $D\Phi$ is the identity:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数重要的聚合方法中，包括第4节中的方法，$D$和$\Phi$被选择使得$D\Phi$是单位矩阵：
- en: '|  | $D\Phi=I.$ |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $D\Phi=I.$ |  |'
- en: 'Assuming that this is true, the operator $I-DT_{\mu}\Phi$ of the aggregation
    equation (2.16) is obtained by pre-multiplying and post-multiplying the operator
    $I-T_{\mu}$ of the Bellman equation with $D$ and $\Phi$, respectively. Mathematically,
    this can be interpreted as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这是真的，那么聚合方程 (2.16) 的算子 $I-DT_{\mu}\Phi$ 是通过用 $D$ 和 $\Phi$ 预乘和后乘贝尔曼方程 $I-T_{\mu}$
    的算子得到的。从数学上讲，这可以解释如下：
- en: '(a) Post-multiplying with $\Phi$: We replace the $n$ variables $J(j)$ of the
    Bellman equation $J=T_{\mu}J$ with convex combinations of the $s$ variables $r_{\ell}$
    of the system (2.15), using the rows $(\phi_{j1},\ldots,\phi_{js})$ of $\Phi$:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 通过 $\Phi$ 后乘：我们用系统 (2.15) 中 $s$ 个变量 $r_{\ell}$ 的凸组合来替代贝尔曼方程 $J=T_{\mu}J$
    中的 $n$ 个变量 $J(j)$，使用 $\Phi$ 的行 $(\phi_{j1},\ldots,\phi_{js})$：
- en: '|  | $J(j)\approx\sum_{\ell=1}^{s}\phi_{j\ell}\,r_{\ell}.$ |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(j)\approx\sum_{\ell=1}^{s}\phi_{j\ell}\,r_{\ell}.$ |  |'
- en: '(b) Pre-multiplying with $D$: We form the $s$ equations of the aggregate system
    by taking convex combinations of the $n$ components of the $n\times n$ Bellman
    equation using the rows of $D$.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通过 $D$ 预乘：我们通过使用 $D$ 的行将 $n\times n$ 贝尔曼方程的 $n$ 个分量的凸组合形成 $s$ 个聚合系统方程。
- en: We will now describe how the aggregate system of Eq. (2.17) can be associated
    with a discounted DP problem that has $s$ states, called the aggregate states
    in what follows. At an abstract level, the aggregate states may be viewed as entities
    associated with the $s$ rows of $D$ or the $s$ columns of $\Phi$. Indeed, since
    $T_{\mu}$ has the form $T_{\mu}J=g_{\mu}+\alpha P_{\mu}J$ [cf. Eq. (2.10)], the
    aggregate system (2.17) becomes
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将描述如何将方程 (2.17) 的聚合系统与具有 $s$ 个状态的折扣DP问题相关联，这些状态在下面将被称为聚合状态。在抽象层面上，聚合状态可以视为与
    $D$ 的 $s$ 行或 $\Phi$ 的 $s$ 列相关联的实体。事实上，由于 $T_{\mu}$ 的形式为 $T_{\mu}J=g_{\mu}+\alpha
    P_{\mu}J$ [参见方程 (2.10)]，因此聚合系统 (2.17) 变为
- en: '|  | $r=\hat{g}_{\mu}+\alpha\hat{P}_{\mu}r,$ |  | (2.18) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=\hat{g}_{\mu}+\alpha\hat{P}_{\mu}r,$ |  | (2.18) |'
- en: where
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\hat{g}_{\mu}=Dg_{\mu},\qquad\hat{P}_{\mu}=DP_{\mu}\Phi.$ |  | (2.19)
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{g}_{\mu}=Dg_{\mu},\qquad\hat{P}_{\mu}=DP_{\mu}\Phi.$ |  | (2.19)
    |'
- en: 'It is straightforward to verify that $\hat{P}_{\mu}$ is a transition probability
    matrix, since the rows of $D$ and $\Phi$ are probability distributions. This means
    that the aggregation equation (2.18) [or equivalently Eq. (2.17)] represents a
    policy evaluation/Bellman equation for the discounted problem with transition
    matrix $\hat{P}_{\mu}$ and cost vector $\hat{g}_{\mu}$. This problem will be called
    the aggregate DP problem associated with policy $\mu$ in what follows. The corresponding
    aggregate state costs are $r_{1},\ldots,r_{s}$. Some important consequences of
    this are:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 直接验证 $\hat{P}_{\mu}$ 是一个转移概率矩阵是很简单的，因为 $D$ 和 $\Phi$ 的行是概率分布。这意味着聚合方程 (2.18)
    [或等效的方程 (2.17)] 表示了一个带有转移矩阵 $\hat{P}_{\mu}$ 和成本向量 $\hat{g}_{\mu}$ 的折扣问题的策略评估/贝尔曼方程。这个问题在下面将被称为与策略
    $\mu$ 相关的聚合DP问题。相应的聚合状态成本是 $r_{1},\ldots,r_{s}$。这有一些重要的后果：
- en: (a) The aggregation equation (2.18)-(2.19) inherits the favorable characteristics
    of the Bellman equation $J=T_{\mu}J$, namely its monotonicity and contraction
    properties, and its uniqueness of solution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 聚合方程 (2.18)-(2.19) 继承了贝尔曼方程 $J=T_{\mu}J$ 的有利特性，即其单调性和收缩性，以及解的唯一性。
- en: (b) Exact DP methods may be used to solve the aggregate DP problem. These methods
    often have more regular behavior than their counterparts based on projected equations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 精确的动态规划（DP）方法可以用来解决聚合DP问题。这些方法通常比基于投影方程的对策表现得更加规整。
- en: (c) Approximate DP methods, such as variants of simulation-based PI, may also
    be used to solve approximately the aggregate DP problem.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 近似DP方法，如基于模拟的PI的变体，也可以用来近似解决聚合DP问题。
- en: The preceding characteristics of the aggregation approach may be turned to significant
    advantage, and may counterbalance the restriction on the structure of $D$ and
    $\Phi$ (their rows must be probability distributions, as stated earlier).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合方法的前述特性可能会带来显著的优势，并且可能会抵消对 $D$ 和 $\Phi$ 结构的限制（它们的行必须是概率分布，如前所述）。
- en: 2.4    Implementation Issues
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 2.4 实施问题
- en: The implementation of approximate PI methods involves several delicate issues,
    which have been extensively investigated but have not been fully resolved, and
    are the subject of continuing research. We will discuss briefly some of these
    issues in what follows in this section. We preface this discussion by noting that
    all of these issues are addressed more easily and effectively within the direct
    approximation and the aggregation frameworks, than within the temporal difference/projected
    equation framework, because of the deficiencies relating to the lack of monotonicity
    and contraction of the operator $\Pi T_{\mu}$, which we noted in Section 2.2.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 近似PI方法的实现涉及几个微妙的问题，这些问题已被广泛研究但尚未完全解决，并且仍在持续研究中。我们将在本节接下来的内容中简要讨论其中的一些问题。我们在讨论之前指出，所有这些问题在直接逼近和聚合框架中更容易和更有效地解决，而在时间差分/投影方程框架中则较难，因为与$\Pi
    T_{\mu}$的单调性和收缩性相关的缺陷，如我们在第2.2节中指出的。
- en: The Issue of Exploration
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 探索问题
- en: An important generic difficulty with simulation-based PI is that in order to
    evaluate a policy $\mu$, we may need to generate cost samples using that policy,
    but this may bias the simulation by underrepresenting states that are unlikely
    to occur under $\mu$. As a result, the cost-to-go estimates of these underrepresented
    states may be highly inaccurate, causing potentially serious errors in the calculation
    of the improved control policy $\hat{\mu}$ via the policy improvement equation
    (2.1).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟的PI的一个重要通用难点是，为了评估策略$\mu$，我们可能需要使用该策略生成成本样本，但这可能会通过低估在$\mu$下不太可能出现的状态来偏倚模拟。因此，这些低估的状态的成本预测可能非常不准确，可能导致通过策略改进方程（2.1）计算改进控制策略$\hat{\mu}$时出现严重错误。
- en: The situation just described is known as inadequate exploration of the system’s
    dynamics. It is a particularly acute difficulty when the system is deterministic
    [i.e., $p_{ij}(u)$ is equal to 1 for a single successor state $j$], or when the
    randomness embodied in the transition probabilities of the current policy is “relatively
    small,” since then few states may be reached from a given initial state when the
    current policy is simulated.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的情况被称为系统动态的探索不足。当系统是确定性的（即，$p_{ij}(u)$对于单一后继状态$j$等于1）或当前策略的转移概率中体现的随机性“相对较小”时，这种困难尤其严重，因为当当前策略被模拟时，从给定的初始状态可能到达的状态很少。
- en: One possibility to guarantee adequate exploration of the state space is to break
    down the simulation to multiple short trajectories (see [Ber11c], [Ber12], [YuB12])
    and to ensure that the initial states employed form a rich and representative
    subset. This is naturally done within the direct approximation and the aggregation
    frameworks, but less so in the temporal difference framework, where the theoretical
    convergence analysis relies on the generation of a single long trajectory.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 确保充分探索状态空间的一种可能性是将模拟分解为多个短轨迹（参见[Ber11c]，[Ber12]，[YuB12]），并确保使用的初始状态形成一个丰富且具有代表性的子集。这在直接逼近和聚合框架中自然完成，但在时间差分框架中则较少，因为理论收敛分析依赖于生成一个单一的长轨迹。
- en: Another possibility for exploration is to artificially introduce some extra
    randomization in the simulation of the current policy, by occasionally generating
    random transitions using some policy other than $\mu$ (this is called an off-policy
    approach and its implementation has been the subject of considerable discussion;
    see the books [SuB98], [Ber12]). A Monte Carlo tree search implementation may
    naturally provide some degree of such randomization, and has worked well in game
    playing contexts, such as the AlphaZero architecture for playing chess, Go, and
    other games (Silver et al., [SHS17]). Other related approaches to improve exploration
    based on generating multiple short trajectories are discussed in Sections 6.4.1
    and 6.4.2 of [Ber12].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个探索的可能性是通过偶尔生成使用与$\mu$不同的策略的随机转移，在当前策略的模拟中人为引入一些额外的随机性（这被称为脱策略方法，其实现已经成为广泛讨论的主题；参见书籍[SuB98]，[Ber12]）。蒙特卡洛树搜索实现自然可以提供一定程度的这种随机化，并且在游戏播放场景中表现良好，例如用于国际象棋、围棋和其他游戏的AlphaZero架构（Silver等，[SHS17]）。基于生成多个短轨迹以改进探索的其他相关方法在[Ber12]的6.4.1节和6.4.2节中讨论。
- en: Limited Sampling/Optimistic Policy Iteration
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 有限采样/乐观策略迭代
- en: In the approximate PI approach discussed so far, the evaluation of the current
    policy $\mu$ must be fully carried out. An alternative is optimistic PI, where
    relatively few simulation samples are processed between successive policy changes
    and corresponding parameter updates.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在迄今为止讨论的近似 PI 方法中，当前策略 $\mu$ 的评估必须完全进行。另一种选择是乐观 PI，在这种方法中，相邻策略更改和相应参数更新之间处理的模拟样本相对较少。
- en: Optimistic PI with cost function approximation is frequently used in practical
    applications. In particular, extreme optimistic schemes, including nonlinear architecture
    versions, and involving a single or very few $Q$-factor updates between parameter
    updates have been widely recommended; see e.g., the books [BeT96], [SuB98], [BBD10]
    (where they are referred to as SARSA, a shorthand for State-Action-Reward-State-Action).
    The behavior of such schemes is very complex, and their theoretical convergence
    properties are unclear. In particular, they can exhibit fascinating and counterintuitive
    behavior, including a natural tendency for policy oscillations. This tendency
    is common to both optimistic and nonoptimistic PI, as we will discuss shortly,
    but in extreme optimistic PI schemes, oscillations tend to manifest themselves
    in an unusual form whereby we may have convergence in parameter space and oscillation
    in policy space (see [BeT96], Section 6.4.2, or [Ber12], Section 6.4.3).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 带有成本函数近似的乐观 PI 在实际应用中经常被使用。特别是，包括非线性架构版本在内的极端乐观方案，以及在参数更新之间涉及单次或非常少的 $Q$-因子更新，已被广泛推荐；参见，例如，[BeT96]，[SuB98]，[BBD10]（在这些书中称为
    SARSA，即状态-动作-奖励-状态-动作的简写）。这些方案的行为非常复杂，其理论收敛性质尚不清楚。特别是，它们可能表现出迷人且反直觉的行为，包括自然倾向于策略振荡。这种倾向在乐观和非乐观
    PI 中都很常见，正如我们将要讨论的，但在极端乐观 PI 方案中，振荡往往以不寻常的形式显现，我们可能会在参数空间中收敛而在策略空间中振荡（参见 [BeT96]，第
    6.4.2 节，或 [Ber12]，第 6.4.3 节）。
- en: On the other hand optimistic PI may in some cases deal better with the problem
    of exploration discussed earlier. The reason is that with rapid changes of policy,
    there may be less tendency to bias the simulation towards particular states that
    are favored by any single policy.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，乐观 PI 在某些情况下可能更好地解决前面讨论的探索问题。原因在于，随着策略的快速变化，可能较少倾向于将模拟偏向于任何单一策略所偏爱的特定状态。
- en: Policy Oscillations and Chattering
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 策略振荡和颤动
- en: Contrary to exact PI, which converges to an optimal policy in a fairly regular
    manner, approximate PI may oscillate. By this we mean that after a few iterations,
    policies tend to repeat in cycles. The parameter vectors $r$ that correspond to
    the oscillating policies may also tend to oscillate, although it is possible,
    in optimistic approximate PI methods, that there is convergence in parameter space
    and oscillation in policy space, a peculiar phenomenon known as chattering.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确 PI 相对，精确 PI 以相当规则的方式收敛到最优策略，近似 PI 可能会发生振荡。这里的意思是，在经过几次迭代后，策略趋向于循环重复。对应于振荡策略的参数向量
    $r$ 也可能会振荡，尽管在乐观近似 PI 方法中，参数空间可能会收敛而策略空间中振荡，这是一种称为颤动的特殊现象。
- en: Oscillations and chattering have been explained with the use of the so-called
    “greedy partition” of the parameter space into subsets that correspond to the
    same improved policy (see [BeT96], Section 6.4.2, or [Ber12], Section 6.4.3).
    Policy oscillations occur when the generated parameter sequence straddles the
    boundaries that separate sets of the partition. Oscillations can be potentially
    very damaging, because there is no guarantee that the policies involved in the
    oscillation are “good” policies, and there is often no way to verify how well
    they compare to the optimal.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 振荡和颤动已通过将参数空间“贪婪划分”成对应于相同改进策略的子集来解释（参见 [BeT96]，第 6.4.2 节，或 [Ber12]，第 6.4.3 节）。当生成的参数序列跨越划分集合的边界时，会发生策略振荡。振荡可能非常有害，因为无法保证涉及振荡的策略是“好”的策略，而且通常无法验证这些策略与最优策略的比较效果。
- en: We note that oscillations are avoided and approximate PI can be shown to converge
    to a single policy under special conditions that arise in particular when aggregation
    is used for policy evaluation. These conditions involve certain monotonicity assumptions
    [e.g., the nonnegativity of the components $\pi_{im}$ of the projection matrix
    in Eq. (2.12)], which are fulfilled in the case of aggregation (see [Ber11a]).
    However, for temporal difference methods, policy oscillations tend to occur generically,
    and often for very simple problems, involving few states (a two-state example
    is given in [Ber11a], and in [Ber12], Section 6.4.3). This is a potentially important
    advantage of the aggregation approach.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，振荡现象被避免，并且在特定条件下，近似 PI 可以证明会收敛到单一策略，这些条件特别是在使用聚合进行策略评估时出现。这些条件涉及某些单调性假设
    [例如，方程 (2.12) 中投影矩阵的分量 $\pi_{im}$ 的非负性]，在聚合情况下得到满足（见 [Ber11a]）。然而，对于时间差分方法，策略振荡倾向于普遍发生，并且通常发生在非常简单的问题中，涉及少量状态（如
    [Ber11a] 中给出的两状态示例，以及 [Ber12] 第 6.4.3 节）。这是一种可能的重要的聚合方法的优势。
- en: Model-Free Implementations
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型实现
- en: In many problems a mathematical model [the transition probabilities $p_{ij}(u)$
    and the cost vector $g$] is unavailable or hard to construct, but instead the
    system and cost structure can be simulated far more easily. In particular, let
    us assume that there is a computer program that for any given state $i$ and control
    $u$, simulates sample transitions to a successor state $j$ according to $p_{ij}(u)$,
    and generates the transition cost $g(i,u,j)$.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多问题中，数学模型 [转移概率 $p_{ij}(u)$ 和成本向量 $g$] 不可用或难以构建，但系统和成本结构可以更容易地进行模拟。特别地，假设有一个计算机程序，它对任意给定的状态
    $i$ 和控制 $u$，根据 $p_{ij}(u)$ 模拟转移到后继状态 $j$，并生成转移成本 $g(i,u,j)$。
- en: As noted earlier, the direct and indirect approaches to approximate evaluation
    of a single policy may be implemented in model-free fashion, simply by generating
    the needed cost samples for the current policy by simulation. However, given the
    result $\tilde{J}_{\mu}(\cdot)$ of the approximate policy evaluation, the policy
    improvement minimization
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，直接和间接的方法可以通过模拟生成当前策略所需的成本样本，从而以无模型的方式近似评估单一策略。然而，给定近似策略评估的结果 $\tilde{J}_{\mu}(\cdot)$，策略改进的最小化
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\bigl{(}g(i,u,j)+\alpha\tilde{J}_{\mu}(j)\bigr{)},\qquad
    i=1,\ldots,n,$ |  | (2.20) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\bigl{(}g(i,u,j)+\alpha\tilde{J}_{\mu}(j)\bigr{)},\qquad
    i=1,\ldots,n,$ |  | (2.20) |'
- en: 'still requires the transition probabilities $p_{ij}(u)$, so it is not model-free.
    To provide a model-free version we may use a parametric regression approach. In
    particular, suppose that for any state $i$ and control $u$, state transitions
    $(i,j)$, and corresponding transition costs $g(i,u,j)$ and values of $\tilde{J}_{\mu}(j)$
    can be generated in a model-free fashion when needed, by using a simulator of
    the true system. Then we can introduce a parametric family/approximation architecture
    of $Q$-factor functions, $\tilde{Q}_{\mu}(i,u,\theta)$, where $\theta$ is the
    parameter vector, and use a regularized least squares fit/regression to approximate
    the expected value that is minimized in Eq. (2.20). The steps are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然需要转移概率 $p_{ij}(u)$，因此这不是无模型的。为了提供无模型版本，我们可以使用参数回归方法。特别地，假设对于任意状态 $i$ 和控制 $u$，状态转移
    $(i,j)$，以及相应的转移成本 $g(i,u,j)$ 和 $\tilde{J}_{\mu}(j)$ 的值，可以在需要时通过使用真实系统的模拟器以无模型的方式生成。那么我们可以引入一个
    $Q$-因子函数的参数家族/近似架构 $\tilde{Q}_{\mu}(i,u,\theta)$，其中 $\theta$ 是参数向量，并使用正则化最小二乘拟合/回归来近似在
    Eq. (2.20) 中最小化的期望值。步骤如下：
- en: (a) Use the simulator to collect a large number of “representative” sample state-control
    pairs $(i_{m},u_{m})$, and successor states $j_{m}$, $m=1,\ldots,M$, and corresponding
    sample $Q$-factors
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用模拟器收集大量“代表性”的样本状态-控制对 $(i_{m},u_{m})$，以及后继状态 $j_{m}$，$m=1,\ldots,M$，并获得相应的样本
    $Q$-因子
- en: '|  | $\beta_{m}=g(i_{m},u_{m},j_{m})+\alpha\tilde{J}_{\mu}(j_{m}),\qquad m=1,\ldots,M.$
    |  | (2.21) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{m}=g(i_{m},u_{m},j_{m})+\alpha\tilde{J}_{\mu}(j_{m}),\qquad m=1,\ldots,M.$
    |  | (2.21) |'
- en: (b) Determine the parameter vector $\tilde{\theta}$ with the least-squares minimization
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用最小二乘法确定参数向量 $\tilde{\theta}$
- en: '|  | $\tilde{\theta}\in\arg\min_{\theta}\sum_{m=1}^{M}\big{(}\tilde{Q}_{\mu}(i_{m},u_{m},\theta)-\beta_{m}\big{)}^{2}$
    |  | (2.22) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\theta}\in\arg\min_{\theta}\sum_{m=1}^{M}\big{(}\tilde{Q}_{\mu}(i_{m},u_{m},\theta)-\beta_{m}\big{)}^{2}$
    |  | (2.22) |'
- en: (or a regularized minimization whereby a quadratic regularization term is added
    to the above quadratic objective).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: （或者一个正则化的最小化，其中在上述二次目标中添加了一个二次正则化项）。
- en: (c) Use the policy
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 使用策略
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\tilde{Q}_{\mu}(i,u,\tilde{\theta}),\qquad
    i=1,\ldots,n.$ |  | (2.23) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\tilde{Q}_{\mu}(i,u,\tilde{\theta}),\qquad
    i=1,\ldots,n.$ |  | (2.23) |'
- en: This policy may be generated on-line when the control constraint set $U(i)$
    contains a reasonably small number of elements. Otherwise an approximation in
    policy space is needed to represent the policy $\hat{\mu}$ using a policy approximation
    architecture. Such an architecture could be based on a neural network, in which
    case it is commonly called an “action network” or “actor network” to distinguish
    from its cost function approximation counterpart, which is called a “value network”
    or “critic network.”
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当控制约束集 $U(i)$ 包含一个合理小的元素数量时，可以在线生成此策略。否则，需要在策略空间中进行近似，以使用策略近似架构表示策略 $\hat{\mu}$。这样的架构可以基于神经网络，在这种情况下，通常称为“动作网络”或“演员网络”，以区别于其成本函数近似对称的“价值网络”或“评论员网络”。
- en: 'Note some important points about the preceding approximation procedure:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意关于前述近似过程的一些重要点：
- en: (1) It does not need the transition probabilities $p_{ij}(u)$ to generate the
    policy $\hat{\mu}$ through the minimization (2.23). The simulator to collect the
    samples (2.21) suffices.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 通过最小化（2.23）生成策略 $\hat{\mu}$ 不需要转移概率 $p_{ij}(u)$。仅使用样本收集模拟器（2.21）就足够了。
- en: (2) The policy $\hat{\mu}$ obtained through the minimization (2.23) is not the
    same as the one obtained through the minimization (2.20). There are two reasons
    for this. One is the approximation error introduced by the $Q$-factor architecture
    $\tilde{Q}_{\mu}$, and the other is the simulation error introduced by the finite-sample
    regression (2.22). We have to accept these sources of error as the price to pay
    for the convenience of not requiring a mathematical model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 通过最小化（2.23）获得的策略 $\hat{\mu}$ 与通过最小化（2.20）获得的策略不同。原因有两个。一是由 $Q$-因子架构 $\tilde{Q}_{\mu}$
    引入的近似误差，另一是由有限样本回归（2.22）引入的模拟误差。我们必须接受这些误差源，作为不需要数学模型的便利代价。
- en: '(3) Two approximations are potentially required: One to compute $\tilde{J}_{\mu}$,
    which is needed for the samples $\beta_{m}$ [cf. Eq. (2.21)], and another to compute
    $\tilde{Q}_{\mu}$ through the least squares minimization (2.22), and the subsequent
    policy generation formula (2.23). The approximation methods to obtain $\tilde{J}_{\mu}$
    and $\tilde{Q}_{\mu}$ may not be the same and in fact may be unrelated (for example
    $\tilde{J}_{\mu}$ need not involve a parametric approximation, e.g., it may be
    obtained by some type of problem approximation approach).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 可能需要两个近似：一个用于计算 $\tilde{J}_{\mu}$，这对于样本 $\beta_{m}$ 是必要的 [参见 Eq. (2.21)]，另一个用于通过最小二乘法最小化（2.22）计算
    $\tilde{Q}_{\mu}$，以及随后的策略生成公式（2.23）。获得 $\tilde{J}_{\mu}$ 和 $\tilde{Q}_{\mu}$ 的近似方法可能不相同，实际上可能没有关联（例如
    $\tilde{J}_{\mu}$ 不一定涉及参数近似，例如，它可以通过某种问题近似方法获得）。
- en: An alternative to first computing $\tilde{J}_{\mu}(\cdot)$ and then computing
    subsequently $\tilde{Q}_{\mu}(\cdot,\cdot,\theta)$ via the procedure (2.21)-(2.23) is
    to forgo the computation of $\tilde{J}_{\mu}(\cdot)$, and use just the parametric
    approximation architecture for the policy $Q$-factor, $\tilde{Q}_{\mu}(i,u,\theta)$.
    We may then train this $Q$-factor architecture, using state-control $Q$-factor
    samples, and either the direct or the indirect approach. Generally, algorithms
    for approximating policy cost functions can be adapted to approximating policy
    $Q$-factor functions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是首先计算 $\tilde{J}_{\mu}(\cdot)$，然后通过过程（2.21）-（2.23）计算 $\tilde{Q}_{\mu}(\cdot,\cdot,\theta)$，是跳过计算
    $\tilde{J}_{\mu}(\cdot)$，仅使用参数近似架构来表示策略 $Q$-因子 $\tilde{Q}_{\mu}(i,u,\theta)$。然后我们可以使用状态-控制
    $Q$-因子样本来训练这个 $Q$-因子架构，无论是直接方法还是间接方法。一般来说，近似策略成本函数的算法可以调整为近似策略 $Q$-因子函数。
- en: 'As an example, a direct model-free approximate PI scheme can be defined by
    Eqs. (2.22)-(2.23), using $M$ state-control samples $(i_{m},u_{m})$, corresponding
    successor states $j_{m}$ generated according to the probabilities $p_{i_{m}j}(u_{m})$,
    and sample costs $\beta_{m}$ equal to the sum of:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，可以通过方程（2.22）-（2.23）定义一个直接的无模型近似 PI 方案，使用 $M$ 个状态-控制样本 $(i_{m},u_{m})$，根据概率
    $p_{i_{m}j}(u_{m})$ 生成的相应后继状态 $j_{m}$，以及样本成本 $\beta_{m}$，其等于以下之和：
- en: (a) The first stage cost $g(i_{m},u_{m},j_{m})$.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第一阶段成本 $g(i_{m},u_{m},j_{m})$。
- en: (b) A $\alpha$-discounted simulated sample of the infinite horizon cost of starting
    at $j_{m}$ and using $\mu$ [in place of the term $\alpha\tilde{J}_{\mu}(j_{m})$
    in Eq. (2.21)].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个$\alpha$-折扣的模拟样本，表示从$j_{m}$开始并使用$\mu$的无限期成本[代替方程(2.21)中的$\alpha\tilde{J}_{\mu}(j_{m})$]。
- en: A PI scheme of this type was suggested by Fern, Yoon, and Givan [FYG06], and
    has been discussed by several other authors; see [Ber17], Section 6.3.4\. In particular,
    a variant of the method was used to train a tetris playing computer program that
    performs impressively better than programs that are based on other variants of
    approximate PI, and various other methods; see Scherrer [Sch13], Scherrer et al. [SGG15],
    and Gabillon, Ghavamzadeh, and Scherrer [GGS13], who also provide an analysis.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的PI方案由Fern、Yoon和Givan [FYG06] 提出，并且被其他一些作者讨论；参见[Ber17]，第6.3.4节。特别是，该方法的一个变体用于训练一个玩俄罗斯方块的计算机程序，该程序的表现显著优于基于其他近似PI变体和各种其他方法的程序；参见Scherrer
    [Sch13]、Scherrer等[SGG15]和Gabillon、Ghavamzadeh和Scherrer [GGS13]，他们也提供了分析。
- en: 3. APPROXIMATE POLICY EVALUATION BASED ON NEURAL NETWORKS
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 基于神经网络的近似策略评估
- en: In this section we will describe some of the basic ideas of the neural network
    methodology as it applies to the approximation of the cost vector $J_{\mu}$ of
    a fixed policy $\mu$. Since $\mu$ is fixed throughout this section, we drop the
    subscript $\mu$ is what follows. A neural network provides an architecture of
    the form
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述神经网络方法的一些基本思想，它应用于固定策略$\mu$的成本向量$J_{\mu}$的近似。由于$\mu$在本节中是固定的，我们在以下内容中省略下标$\mu$。神经网络提供了一种形式的架构
- en: '|  | $\tilde{J}(i,v,r)=\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}$ |  | (3.1) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}(i,v,r)=\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}$ |  | (3.1) |'
- en: that depends on a parameter vector $v$ and a parameter vector $r=(r_{1},\ldots,r_{s})$.
    Here for each state $i$, $\tilde{J}(i,v,r)$ approximates $J_{\mu}(i)$, while the
    vector
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这依赖于参数向量$v$和参数向量$r=(r_{1},\ldots,r_{s})$。在这里，对于每个状态$i$，$\tilde{J}(i,v,r)$近似于$J_{\mu}(i)$，而向量
- en: '|  | $F(i,v)=\big{(}F_{1}(i,v),\ldots,F_{s}(i,v)\big{)}$ |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i,v)=\big{(}F_{1}(i,v),\ldots,F_{s}(i,v)\big{)}$ |  |'
- en: 'may be viewed as a feature vector of the state $i$. Notice the different roles
    of the two parameter vectors: $v$ parametrizes $F(i,v)$, and $r$ is a vector of
    weights that combine linearly the components of $F(i,v)$. The idea is to use training
    to obtain simultaneously both the features and the linear weights.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 可以被视为状态$i$的特征向量。注意两个参数向量的不同作用：$v$参数化$F(i,v)$，而$r$是一个权重向量，线性组合$F(i,v)$的各个分量。这个想法是通过训练同时获得特征和线性权重。
- en: Consistent with the direct approximation framework of Section 2.1, to train
    a neural network, we generate a training set that consists of a large number of
    state-cost pairs $(i_{m},\beta_{m})$, $m=1,\ldots,M$, and we find $(v,r)$ that
    minimizes
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与第2.1节的直接近似框架一致，为了训练神经网络，我们生成一个包含大量状态-成本对$(i_{m},\beta_{m})$的训练集，$m=1,\ldots,M$，并找到$(v,r)$以最小化
- en: '|  | $\sum_{m=1}^{M}\big{(}\tilde{J}(i_{m},v,r)-\beta_{m}\big{)}^{2}.$ |  |
    (3.2) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{m=1}^{M}\big{(}\tilde{J}(i_{m},v,r)-\beta_{m}\big{)}^{2}.$ |  |
    (3.2) |'
- en: The training pairs $(i_{m},\beta_{m})$ are generated by some kind of calculation
    or simulation, and they may contain noise, i.e., $\beta_{m}$ is the cost of the
    policy starting from state $i_{m}$ plus some error.^†^†† There are also neural
    network implementations of the indirect/projected equation approximation approach,
    which make use of temporal differences, such as for example nonlinear versions
    of TD($\lambda$). We refer to the textbook literature on the subject, e.g., [SuB98].
    In this paper, we will focus on neural network training that is based on minimization
    of the quadratic cost function (3.2).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练对$(i_{m},\beta_{m})$由某种计算或模拟生成，它们可能包含噪声，即$\beta_{m}$是从状态$i_{m}$开始的策略的成本加上一些误差。^†^††
    还有一些神经网络实现的间接/投影方程近似方法，它们利用时间差分，例如TD($\lambda$)的非线性版本。我们参考了相关教科书文献，例如，[SuB98]。在本文中，我们将重点讨论基于最小化二次成本函数(3.2)的神经网络训练。
- en: The simplest type of neural network is the single layer perceptron; see Fig. 3.1\.
    Here the state $i$ is encoded as a vector of numerical values $y(i)$ with components
    $y_{1}(i),\ldots,y_{k}(i)$, which is then transformed linearly as
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的神经网络类型是单层感知机；参见图3.1。这里状态$i$被编码为数值向量$y(i)$，其分量为$y_{1}(i),\ldots,y_{k}(i)$，然后线性变换为
- en: '|  | $Ay(i)+b,$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $Ay(i)+b,$ |  |'
- en: where $A$ is an $m\times k$ matrix and $b$ is a vector in $\Re^{m}$. Some of
    the components of $y(i)$ may be known interesting features of $i$ that can be
    designed based on problem-specific knowledge or prior training experience. This
    transformation will be referred to as the linear layer of the neural network.
    We view the components of $A$ and $b$ as parameters to be determined, and we group
    them together into the parameter vector $v=(A,b)$.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 是一个 $m\times k$ 矩阵，$b$ 是一个 $\Re^{m}$ 中的向量。$y(i)$ 的一些分量可能是已知的状态 $i$ 的有趣特征，这些特征可以基于特定问题的知识或先前的训练经验进行设计。这一变换将被称为神经网络的线性层。我们将
    $A$ 和 $b$ 的分量视为待确定的参数，并将它们组合成参数向量 $v=(A,b)$。
- en: Each of the $s$ scalar output components of the linear layer,
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层的每一个 $s$ 个标量输出分量，
- en: '|  | $\big{(}Ay(i)+b\big{)}_{\ell},\qquad\ell=1,\ldots,s,$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $\big{(}Ay(i)+b\big{)}_{\ell},\qquad\ell=1,\ldots,s,$ |  |'
- en: becomes the input to a nonlinear differentiable function $\sigma$ that maps
    scalars to scalars. Typically $\sigma$ is monotonically increasing. A simple and
    popular possibility is the rectified linear unit, which is simply the function
    $\max\{0,\xi\}$, “rectified” to a differentiable function by some form of smoothing
    operation; for example
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 变成一个将标量映射到标量的非线性可微函数 $\sigma$ 的输入。通常 $\sigma$ 是单调递增的。一种简单而流行的选择是整流线性单元，它只是函数
    $\max\{0,\xi\}$，通过某种形式的平滑操作“整流”成一个可微函数；例如
- en: '|  | $\sigma(\xi)=\ln(1+e^{\xi}).$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(\xi)=\ln(1+e^{\xi}).$ |  |'
- en: Other functions, used since the early days of neural networks, have the property
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经网络早期就开始使用的其他函数具有以下特性
- en: '|  | $-\infty<\lim_{\xi\to-\infty}\sigma(\xi)<\lim_{\xi\to\infty}\sigma(\xi)<\infty.$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $-\infty<\lim_{\xi\to-\infty}\sigma(\xi)<\lim_{\xi\to\infty}\sigma(\xi)<\infty.$
    |  |'
- en: Such functions are referred to as sigmoids, and some common choices are the
    hyperbolic tangent function
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数被称为 sigmoid 函数，一些常见的选择是双曲正切函数
- en: '|  | $\sigma(\xi)=\tanh(\xi)={e^{\xi}-e^{-\xi}\over e^{\xi}+e^{-\xi}},$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(\xi)=\tanh(\xi)={e^{\xi}-e^{-\xi}\over e^{\xi}+e^{-\xi}},$ |  |'
- en: and the logistic function
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以及逻辑函数
- en: '|  | $\sigma(\xi)={1\over 1+e^{-\xi}}.$ |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(\xi)={1\over 1+e^{-\xi}}.$ |  |'
- en: In what follows, we will ignore the character of the function $\sigma$ (except
    for the differentiability requirement), and simply refer to it as a “nonlinear
    unit” and to the corresponding layer as a “nonlinear layer.”
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将忽略函数 $\sigma$ 的特性（除了可微性要求），并将其称为“非线性单元”，相应的层称为“非线性层”。
- en: '![[Uncaptioned image]](img/072216ca0be42eac3db032db3e1d6c52.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/072216ca0be42eac3db032db3e1d6c52.png)'
- en: Figure 3.1  A perceptron consisting of a linear layer and a nonlinear layer.
    It provides a way to compute features of the state, which can be used for approximation
    of the cost function of a given policy. The state $i$ is encoded as a vector of
    numerical values $y(i)$, which is then transformed linearly as $Ay(i)+b$ in the
    linear layer. The scalar output components of the linear layer, become the inputs
    to single input-single output nonlinear functions that produce the $s$ scalars
    $F_{\ell}(i,v)=\sigma\big{(}(Ay(i)+b)_{\ell}\big{)},$ which can be viewed as feature
    components that are in turn linearly weighted with parameters $r_{\ell}$.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1  一个包含线性层和非线性层的感知器。它提供了一种计算状态特征的方法，这些特征可以用于近似给定策略的成本函数。状态 $i$ 被编码为数值向量
    $y(i)$，然后在线性层中线性变换为 $Ay(i)+b$。线性层的标量输出分量，成为单输入单输出非线性函数的输入，这些函数产生 $s$ 个标量 $F_{\ell}(i,v)=\sigma\big{(}(Ay(i)+b)_{\ell}\big{)},$
    这些标量可以被视为特征分量，这些特征分量又与参数 $r_{\ell}$ 线性加权。
- en: At the outputs of the nonlinear units, we obtain the scalars
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性单元的输出处，我们得到标量
- en: '|  | $F_{\ell}(i,v)=\sigma\big{(}(Ay(i)+b)_{\ell}\big{)},\qquad\ell=1,\ldots,s.$
    |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{\ell}(i,v)=\sigma\big{(}(Ay(i)+b)_{\ell}\big{)},\qquad\ell=1,\ldots,s.$
    |  |'
- en: One possible interpretation is to view these scalars as features of state $i$,
    which are linearly combined using weights $r_{\ell}$, $\ell=1,\ldots,s$, to produce
    the final output
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解释是将这些标量视为状态 $i$ 的特征，这些特征通过权重 $r_{\ell}$, $\ell=1,\ldots,s$ 线性组合，以产生最终输出。
- en: '|  | $\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}=\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i)+b\big{)}_{\ell}\Big{)}\,r_{\ell}.$
    |  | (3.3) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}=\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i)+b\big{)}_{\ell}\Big{)}\,r_{\ell}.$
    |  | (3.3) |'
- en: Note that each value $F_{\ell}(i,v)$ depends on just the $\ell$th row of $A$
    and the $\ell$th component of $b$, not on the entire vector $v$. In some cases
    this motivates placing some constraints on individual components of $A$ and $b$
    to achieve special problem-dependent “handcrafted” effects.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个值 $F_{\ell}(i,v)$ 仅依赖于 $A$ 的第 $\ell$ 行和 $b$ 的第 $\ell$ 个分量，而不是整个向量 $v$。在某些情况下，这会激励对
    $A$ 和 $b$ 的个别分量施加一些约束，以实现特殊问题依赖的“手工制作”效果。
- en: Given a set of state-cost training pairs $(i_{m},\beta_{m})$, $m=1,\ldots,M$,
    the parameters of the neural network $A$, $b$, and $r$ are obtained by solving
    the training problem (3.2), i.e.,
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组状态-成本训练对 $(i_{m},\beta_{m})$，$m=1,\ldots,M$，神经网络的参数 $A$、$b$ 和 $r$ 通过解决训练问题
    (3.2) 获得，即：
- en: '|  | $\min_{A,b,r}\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i_{m})+b\big{)}_{\ell}\Big{)}\,r_{\ell}-\beta_{m}\right)^{2}.$
    |  | (3.4) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{A,b,r}\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i_{m})+b\big{)}_{\ell}\Big{)}\,r_{\ell}-\beta_{m}\right)^{2}.$
    |  | (3.4) |'
- en: The cost function of this problem is generally nonconvex, so there may exist
    multiple local minima.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的成本函数通常是非凸的，因此可能存在多个局部最小值。
- en: 'It is common to augment the cost function of this problem with a regularization
    function, such as a quadratic in the parameters $A$, $b$, and $r$. This is customary
    in least squares problems in order to make the problem easier to solve algorithmically.
    However, in the context of neural network training, regularization is primarily
    important for a different reason: it helps to avoid overfitting, which refers
    to a situation where a neural network model matches the training data very well
    but does not do as well on new data. This is a well known difficulty in machine
    learning, which may occur when the number of parameters of the neural network
    is relatively large (roughly comparable to the size of the training set). We refer
    to machine learning and neural network textbooks for a discussion of algorithmic
    questions regarding regularization and other issues that relate to the practical
    implementation of the training process. In any case, the training problem (3.4) is
    an unconstrained nonconvex differentiable optimization problem that can in principle
    be addressed with standard gradient-type methods.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会将此问题的成本函数与正则化函数一起增加，例如对参数 $A$、$b$ 和 $r$ 的二次函数。这在最小二乘问题中是惯例，以使问题在算法上更容易解决。然而，在神经网络训练的背景下，正则化主要因另一个原因而重要：它有助于避免过拟合，过拟合指的是神经网络模型对训练数据的匹配非常好，但在新数据上表现不佳。这是机器学习中的一个著名困难，可能发生在神经网络的参数数量相对较大时（大致与训练集的大小相当）。我们参考机器学习和神经网络的教科书，以讨论有关正则化和其他与训练过程实际实现相关的算法问题。无论如何，训练问题
    (3.4) 是一个无约束的非凸可微优化问题，从原则上讲，可以通过标准的梯度型方法来解决。
- en: 'Let us now discuss briefly two issues regarding the neural network formulation
    and training process just described:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简要讨论刚才描述的神经网络公式和训练过程的两个问题：
- en: (a) A major question is how to solve the training problem (3.4). The salient
    characteristic of the cost function of this problem is its form as the sum of
    a potentially very large number $M$ of component functions. This structure can
    be exploited with a variant of the gradient method, called incremental,^†^†† Sometimes
    the more recent name “stochastic gradient descent” is used in reference to this
    method. However, once the training set has been generated, possibly by some deterministic
    process, the method need not have a stochastic character, and it also does not
    guarantee cost function descent at each iteration. which computes just the gradient
    of a single squared error component
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一个主要问题是如何解决训练问题 (3.4)。这个问题的成本函数的显著特征是其形式为可能非常大的数量 $M$ 的组件函数之和。这种结构可以通过梯度方法的变体来利用，称为增量法，有时最近的名称“随机梯度下降”也用来指代这种方法。然而，一旦训练集已经生成，可能是通过某些确定性过程生成的，该方法不一定具有随机特征，并且它也不保证每次迭代都能降低成本函数。该方法仅计算单个平方误差组件的梯度。
- en: '|  | $\left(\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i_{m})+b\big{)}_{\ell}\Big{)}\,r_{\ell}-\beta_{m}\right)^{2}$
    |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left(\sum_{\ell=1}^{s}\sigma\Big{(}\big{(}Ay(i_{m})+b\big{)}_{\ell}\Big{)}\,r_{\ell}-\beta_{m}\right)^{2}$
    |  |'
- en: of the sum in Eq. (3.4) at each iteration, and then changes the current iterate
    in the opposite direction of this gradient using some stepsize; the books [Ber15],
    [Ber16b] provide extensive accounts, and theoretical analyses including the connection
    with stochastic gradient methods are given in the book [BeT96] and the paper [BeT00].
    Experience has shown that the incremental gradient method can be vastly superior
    to the ordinary (nonincremental) gradient method in the context of neural network
    training, and in fact the methods most commonly used in practice are incremental.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中对公式(3.4)中的和进行计算，然后使用某个步长在梯度的相反方向上更新当前迭代值；书籍[Ber15]、[Ber16b]提供了广泛的描述，而理论分析包括与随机梯度方法的联系则在书籍[BeT96]和论文[BeT00]中给出。经验表明，在神经网络训练中，增量梯度方法可以远远优于普通（非增量）梯度方法，实际上在实践中最常用的方法是增量的。
- en: (b) Another important question is how well we can approximate the cost function
    of the policy with a neural network architecture, assuming we can choose the number
    of the nonlinear units $s$ to be as large as we want. The answer to this question
    is quite favorable and is provided by the so-called universal approximation theorem.
    Roughly, the theorem says that assuming that $i$ is an element of a Euclidean
    space $X$ and $y(i)\equiv i$, a neural network of the form described can approximate
    arbitrarily closely (in an appropriate mathematical sense), over a closed and
    bounded subset $S\subset X$, any piecewise continuous function $J:S\mapsto\Re$,
    provided the number $s$ of nonlinear units is sufficiently large. For proofs of
    the theorem at different levels of generality, we refer to Cybenko [Cyb89], Funahashi
    [Fun89], Hornik, Stinchcombe, and White [HSW89], and Leshno et al. [LLP93]. For
    intuitive explanations we refer to Bishop ([Bis95], pp. 129-130) and Jones [Jon90].
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 另一个重要的问题是，在假设我们可以将非线性单元的数量$s$选择得足够大时，我们能多好地用神经网络结构来逼近策略的成本函数。对此问题的答案相当乐观，由所谓的普适逼近定理提供。大致而言，该定理表示，假设$i$是欧几里得空间$X$的一个元素且$y(i)\equiv
    i$，那么形式上描述的神经网络可以在闭合有界子集$S\subset X$上任意接近（在适当的数学意义下）任何分段连续函数$J:S\mapsto\Re$，前提是非线性单元的数量$s$足够大。有关该定理在不同普遍性水平上的证明，请参见Cybenko
    [Cyb89]、Funahashi [Fun89]、Hornik、Stinchcombe和White [HSW89]，以及Leshno等人 [LLP93]。有关直观解释，请参见Bishop
    ([Bis95]，第129-130页) 和Jones [Jon90]。
- en: While the universal approximation theorem provides some assurance about the
    adequacy of the neural network structure, it does not predict the number of nonlinear
    units that we may need for “good” performance in a given problem. Unfortunately,
    this is a difficult question to even pose precisely, let alone to answer adequately.
    In practice, one is reduced to trying increasingly larger numbers of units until
    one is convinced that satisfactory performance has been obtained for the task
    at hand. Experience has shown that in many cases the number of required nonlinear
    units and corresponding dimension of $A$ can be very large, adding significantly
    to the difficulty of solving the training problem. This has motivated various
    suggestions for modifications of the neural network structure. One possibility
    is to concatenate multiple single layer perceptrons so that the output of the
    nonlinear layer of one perceptron becomes the input to the linear layer of the
    next, as we will now discuss.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然普适逼近定理提供了一些关于神经网络结构适用性的保障，但它并未预测我们可能需要的非线性单元的数量以获得“良好”的性能。不幸的是，这个问题甚至难以精确提出，更不用说充分回答。在实践中，人们往往需要尝试越来越多的单元数量，直到确信任务的性能达到满意为止。经验表明，在许多情况下，所需的非线性单元数量和相应的$A$维度可能非常大，显著增加了训练问题的解决难度。这促使了对神经网络结构的各种修改建议。其中一种可能性是将多个单层感知器串联起来，使一个感知器的非线性层的输出成为下一个感知器的线性层的输入，正如我们现在将讨论的那样。
- en: Multilayer and Deep Neural Networks
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 多层和深度神经网络
- en: An important generalization of the single layer perceptron architecture is deep
    neural networks, which involve multiple layers of linear and nonlinear functions.
    The number of layers can be quite large, hence the “deep” characterization. The
    outputs of each nonlinear layer become the inputs of the next linear layer; see
    Fig. 3.2\. In some cases it may make sense to add as additional inputs some of
    the components of the state $i$ or the state encoding $y(i)$.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 单层感知器架构的一个重要推广是深度神经网络，它涉及多个线性和非线性函数层。层数可以非常大，因此称之为“深度”。每个非线性层的输出成为下一个线性层的输入；见图3.2。在某些情况下，可能需要将状态
    $i$ 或状态编码 $y(i)$ 的某些组件作为额外输入。
- en: The training problem for multilayer networks has the form
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 多层网络的训练问题的形式是
- en: '|  | $\min_{v,r}\,\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}-\beta_{m}\right)^{2},$
    |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{v,r}\,\sum_{m=1}^{M}\left(\sum_{\ell=1}^{s}F_{\ell}(i,v)r_{\ell}-\beta_{m}\right)^{2},$
    |  |'
- en: where $v$ represents the collection of all the parameters of the linear layers,
    and $F_{\ell}(i,v)$ is the $\ell$th feature component produced at the output of
    the final nonlinear layer. Various types of incremental gradient methods can also
    be applied here, specially adapted to the multi-layer structure and they are the
    methods most commonly used in practice, in combination with techniques for finding
    good starting points, etc. An important fact is that the gradient with respect
    to $v$ of each feature component $F_{\ell}(i,v)$ can be efficiently calculated
    using a special procedure known as backpropagation, which is just a computationally
    efficient way to apply the chain rule of differentiation. We refer to the specialized
    literature for various accounts (see e.g., [Bis95], [BeT96], [HOT06], [Hay08],
    [LZX17]).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $v$ 表示线性层所有参数的集合，$F_{\ell}(i,v)$ 是最终非线性层输出的第 $\ell$ 个特征组件。各种类型的增量梯度方法也可以应用于此，特别适应于多层结构，它们是实践中最常用的方法，并结合了寻找良好起点等技术。一个重要的事实是，相对于
    $v$ 的每个特征组件 $F_{\ell}(i,v)$ 的梯度可以使用一种称为反向传播的特殊程序高效计算，这是一种计算上高效的链式法则应用方式。我们参考专业文献以获取更多详细信息（见例如，[Bis95]，[BeT96]，[HOT06]，[Hay08]，[LZX17]）。
- en: '![[Uncaptioned image]](img/951fb5968829e385ced7b3d864686df7.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/951fb5968829e385ced7b3d864686df7.png)'
- en: Figure 3.2  A neural network with multiple layers. Each nonlinear layer constructs
    a set of features as inputs of the next linear layer. The features are obtained
    at the output of the final nonlinear layer are linearly combined to yield a cost
    function approximation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 多层神经网络。每个非线性层构建一组特征作为下一个线性层的输入。最终非线性层输出的特征线性组合以得到成本函数的近似。
- en: In view of the universal approximation property, the reason for having multiple
    nonlinear layers is not immediately apparent. A commonly given explanation is
    that a multilayer network provides a hierarchical sequence of features, where
    each set of features in the sequence is a function of the preceding set of features
    in the sequence. In the context of specific applications, this hierarchical structure
    can be exploited in order to specialize the role of some of the layers and to
    enhance particular characteristics of the state. Another reason commonly given
    is that with multiple linear layers, one may consider the possibility of using
    matrices $A$ with a particular sparsity pattern, or other structure that embodies
    special linear operations such as convolution. When such structures are used,
    the training problem often becomes easier, because the number of parameters in
    the linear layers may be drastically decreased.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通用逼近性质，拥有多个非线性层的原因并不是立刻显而易见的。一个常见的解释是，多层网络提供了一种分层特征序列，其中序列中的每组特征都是前一组特征的函数。在具体应用的背景下，这种分层结构可以被利用以专门化某些层的角色，并增强状态的特定特性。另一个常见的原因是，使用多个线性层时，可以考虑使用具有特定稀疏模式的矩阵
    $A$，或其他体现特殊线性操作如卷积的结构。当使用这些结构时，训练问题通常变得更容易，因为线性层中的参数数量可以大幅减少。
- en: 'Deep neural networks also have another advantage, which is important for our
    aggregation-related purposes in this paper: the final features obtained as output
    of the last nonlinear layer tend to be more complex, so their number can be made
    smaller as the number of nonlinear layers increases. This tends to facilitate
    the implementation of the feature-based aggregation schemes that we will discuss
    in what follows.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络还有另一个优势，这对于本文中与聚合相关的目的非常重要：作为最后一个非线性层输出的最终特征趋向于更复杂，因此随着非线性层数量的增加，它们的数量可以变得更少。这有助于实现我们接下来将讨论的基于特征的聚合方案。
- en: 4. FEATURE-BASED AGGREGATION FRAMEWORK
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 基于特征的聚合框架
- en: In this section, we will specialize the general aggregation framework of Section
    2.3 by introducing features in the definition of the matrices $D$ and $\Phi$.
    The starting point is a given feature mapping, i.e., a function $F$ that maps
    a state $i$ into its feature vector $F(i)$. We assume that $F$ is constructed
    in some way (including hand-crafted, or neural network-based), but we leave its
    construction unspecified for the moment.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过在矩阵$D$和$\Phi$的定义中引入特征，来专门化第2.3节的一般聚合框架。起点是给定的特征映射，即一个将状态$i$映射到其特征向量$F(i)$的函数$F$。我们假设$F$是以某种方式构造的（包括手工设计或基于神经网络），但暂时不具体说明其构造方式。
- en: We will form a lower-dimensional DP approximation of the original problem, and
    to this end we introduce disjoint subsets $S_{1},\ldots,S_{q}$ of state-feature
    pairs $\big{(}i,F(i)\big{)}$, which we call aggregate states. The subset of original
    system states $I_{\ell}$ that corresponds to $S_{\ell}$,
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将形成原始问题的低维DP近似，为此我们引入不相交的状态特征对$S_{1},\ldots,S_{q}$，我们称之为聚合状态。原始系统状态的子集$I_{\ell}$与$S_{\ell}$对应，
- en: '|  | $I_{\ell}=\big{\{}i\mid(i,F(i))\in S_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  | (4.1) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\ell}=\big{\{}i\mid(i,F(i))\in S_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  | (4.1) |'
- en: is called the disaggregation set of $S_{\ell}$. An alternative and equivalent
    definition, given $F$, is to start with disjoint subsets of states $I_{\ell}$,
    $\ell=1,\ldots,q$, and define the aggregates states $S_{\ell}$ by
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 称为$S_{\ell}$的解聚合集合。另一种等效的定义是，给定$F$，从不相交的状态子集$I_{\ell}$（$\ell=1,\ldots,q$）开始，并通过以下方式定义聚合状态$S_{\ell}$：
- en: '|  | $S_{\ell}=\big{\{}(i,F(i))\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  | (4.2) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{\ell}=\big{\{}(i,F(i))\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  | (4.2) |'
- en: Mathematically, the aggregate states are the restrictions of the feature mapping
    on the disaggregation sets $I_{\ell}$. In simple terms, we may view the aggregate
    states $S_{\ell}$ as some “pieces” of the graph of the feature mapping $F$; see
    Fig. 4.1.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，聚合状态是特征映射在解聚合集合$I_{\ell}$上的限制。简单来说，我们可以将聚合状态$S_{\ell}$视为特征映射$F$的图的一些“片段”；见图4.1。
- en: To preview our framework, we will aim to construct an aggregate DP problem whose
    states will be the aggregate states $S_{1},\ldots,S_{q}$, and whose optimal costs,
    denoted $r^{*}_{1},\ldots,r^{*}_{q}$, will be used to construct a function approximation
    $\tilde{J}$ to the optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}$.
    This approximation will be constant over each disaggregation set; see Fig. 4.1\.
    Our ultimate objective is that $\tilde{J}$ approximates closely $J^{\raise 0.04pt\hbox{\sevenrm*}}$,
    which suggests as a general guideline that the aggregate states should be selected
    so that $J^{\raise 0.04pt\hbox{\sevenrm*}}$ is nearly constant over each of the
    disaggregation sets $I_{1},\ldots,I_{q}$. This will also be brought out by our
    subsequent analysis.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预览我们的框架，我们将构建一个聚合DP问题，其状态将是聚合状态$S_{1},\ldots,S_{q}$，其最优成本记为$r^{*}_{1},\ldots,r^{*}_{q}$，将用于构造函数近似$\tilde{J}$，以逼近最优成本函数$J^{\raise
    0.04pt\hbox{\sevenrm*}}$。这种近似将在每个解聚合集合上保持恒定；见图4.1。我们的最终目标是使$\tilde{J}$紧密地逼近$J^{\raise
    0.04pt\hbox{\sevenrm*}}$，这作为一般指导方针表明，聚合状态应选择使得$J^{\raise 0.04pt\hbox{\sevenrm*}}$在每个解聚合集合$I_{1},\ldots,I_{q}$上几乎保持不变。这也将通过我们随后的分析得以体现。
- en: '![[Uncaptioned image]](img/274d0c576b9156d3335d8e39f90bf01f.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/274d0c576b9156d3335d8e39f90bf01f.png)'
- en: Figure 4.1  Illustration of aggregate states and a corresponding cost approximation,
    which is constant over each disaggregation set. Here there are three aggregate
    states, with disaggregation sets denoted $I_{1},I_{2},I_{3}$.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 聚合状态及其对应的成本近似的示意图，这在每个解聚合集合上是恒定的。这里有三个聚合状态，解聚合集合分别标记为$I_{1},I_{2},I_{3}$。
- en: To formulate an aggregation model that falls within the framework of Section
    2.3, we need to specify the matrices $\Phi$ and $D$. We refer to the row of $D$
    that corresponds to aggregate state $S_{\ell}$ as the disaggregation distribution
    of $S_{\ell}$ and to its elements $d_{\ell 1},\ldots,d_{\ell n}$ as the disaggregation
    probabilities of $S_{\ell}$. Similarly, we refer to the row of $\Phi$ that corresponds
    to state $j$, $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$, as the aggregation distribution
    of $j$, and to its elements as the aggregation probabilities of $j$. We impose
    some restrictions on the components of $D$ and $\Phi$, which we describe next.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了制定符合第2.3节框架的聚合模型，我们需要指定矩阵 $\Phi$ 和 $D$。我们将与聚合状态 $S_{\ell}$ 对应的 $D$ 的行称为 $S_{\ell}$
    的去聚合分布，其元素 $d_{\ell 1},\ldots,d_{\ell n}$ 称为 $S_{\ell}$ 的去聚合概率。类似地，我们将与状态 $j$
    对应的 $\Phi$ 的行 $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$ 称为 $j$ 的聚合分布，其元素称为 $j$ 的聚合概率。我们对
    $D$ 和 $\Phi$ 的分量施加一些限制，接下来我们将描述这些限制。
- en: '|  |    Definition of a Feature-Based Aggregation Architecture: Given the collection
    of aggregate states $S_{1},\ldots,S_{q}$ and the corresponding disaggregation
    sets $I_{1},\ldots,I_{q}$, the aggregation and disaggregation probabilities satisfy
    the following: (a) The disaggregation probabilities map each aggregate state onto
    its disaggregation set. By this we mean that the row of the matrix $D$ that corresponds
    to an aggregate state $S_{\ell}$ is a probability distribution $(d_{\ell 1},\ldots,d_{\ell
    n})$ over the original system states that assigns zero probabilities to states
    that are outside the disaggregation set $I_{\ell}$:d_ℓi=0,  ∀ i∉I_ℓ, ℓ=1,…,q.(For
    example, in the absence of special problem-specific considerations, a reasonable
    and convenient choice would be to assign equal probability to all states in $I_{\ell}$,
    and zero probability to all other states.) (b) The aggregation probabilities map
    each original system state that belongs to a disaggregation set onto the aggregate
    state of that set. By this we mean that the row $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$
    of the matrix $\Phi$ that corresponds to an original system state $j$ is specified
    as follows: (i) If $j$ belongs to some disaggregation set, say $I_{\ell}$, then
    ϕ_jℓ=1, and $\phi_{j\ell{{}^{\prime}}}=0$ for all $\ell{{}^{\prime}}\neq\ell$.
    (ii) If $j$ does not belong to any disaggregation set, the row $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$
    is an arbitrary probability distribution.  |  | (4.3)(4.4) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |   特征基础聚合架构的定义：给定聚合状态集合 $S_{1},\ldots,S_{q}$ 和相应的去聚合集合 $I_{1},\ldots,I_{q}$，聚合和去聚合概率满足以下条件：（a）去聚合概率将每个聚合状态映射到其去聚合集合。我们所指的是，与聚合状态
    $S_{\ell}$ 对应的矩阵 $D$ 的行是一个概率分布 $(d_{\ell 1},\ldots,d_{\ell n})$，该分布对不在去聚合集合 $I_{\ell}$
    内的状态赋予零概率：d_ℓi=0,  ∀ i∉I_ℓ, ℓ=1,…,q。（例如，在没有特殊问题特定考虑的情况下，一个合理且方便的选择是对 $I_{\ell}$
    中的所有状态赋予相等的概率，而对其他状态赋予零概率。） （b）聚合概率将每个属于去聚合集合的原始系统状态映射到该集合的聚合状态。我们所指的是，与原始系统状态
    $j$ 对应的矩阵 $\Phi$ 的行 $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$ 如下规定：（i）如果 $j$ 属于某个去聚合集合，例如
    $I_{\ell}$，则 ϕ_jℓ=1，而 $\phi_{j\ell{{}^{\prime}}}=0$ 对于所有 $\ell{{}^{\prime}}\neq\ell$。
    （ii）如果 $j$ 不属于任何去聚合集合，则行 $\{\phi_{j\ell}\mid\ell=1,\ldots,q\}$ 是一个任意的概率分布。 |  |
    (4.3)(4.4) |'
- en: 'There are several possible methods to choose the aggregate states. Generally,
    as noted earlier, the idea will be to form disaggregation sets over which the
    cost function values [$J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$ or $J_{\mu}(i)$,
    depending on the situation] vary as little as possible. We list three general
    approaches below, and we illustrate these approaches later with examples:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 选择聚合状态的方法有几种可能性。一般来说，如前所述，目标是形成去聚合集合，以使成本函数值 [$J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$
    或 $J_{\mu}(i)$，具体情况而定] 变化尽可能小。我们在下文中列出三种一般方法，并将通过示例进一步说明这些方法：
- en: '(a) State and feature-based approach: Sample in some way the set of original
    system states $i$, compute the corresponding feature vectors $F(i)$, and divide
    the pairs $\big{(}i,F(i)\big{)}$ thus obtained into subsets $S_{1},\ldots,S_{q}$.
    Some problem-specific knowledge may be used to organize the state sampling, with
    proper consideration given to issues of sufficient exploration and adequate representation
    of what is viewed as important parts of the state space. This scheme is suitable
    for problems where states with similar feature vectors have similar cost function
    values, and is ordinarily the type of scheme that we would use in conjunction
    with neural network-constructed features (see Section 5).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 状态和特征基础方法：以某种方式抽样原始系统状态集 $i$，计算相应的特征向量 $F(i)$，并将得到的对 $\big{(}i,F(i)\big{)}$
    划分为子集 $S_{1},\ldots,S_{q}$。可以使用一些特定问题的知识来组织状态抽样，同时适当考虑足够的探索和对视为重要的状态空间部分的充分表示。此方案适用于具有类似特征向量的状态具有类似成本函数值的问题，并且通常是我们与神经网络构造的特征结合使用的方案（见第5节）。
- en: '(b) Feature-based approach: Start with a collection of disjoint subsets $F_{\ell}$,
    $\ell=1,\ldots,q$, of the set of all possible feature values'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 特征基础方法：从所有可能的特征值集合的若干不相交子集 $F_{\ell}$, $\ell=1,\ldots,q$ 开始。
- en: '|  | ${\cal F}=\big{\{}F(i)\mid i=1,\ldots,n\big{\}},$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\cal F}=\big{\{}F(i)\mid i=1,\ldots,n\big{\}},$ |  |'
- en: compute in some way disjoint state subsets $I_{1},\ldots,I_{q}$ such that
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以某种方式计算不相交的状态子集 $I_{1},\ldots,I_{q}$，使得
- en: '|  | $F(i)\in F_{\ell},\qquad\forall\ i\in I_{\ell},\ \ell=1,\ldots,q,$ |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i)\in F_{\ell},\qquad\forall\ i\in I_{\ell},\ \ell=1,\ldots,q,$ |  |'
- en: and obtain the aggregate states
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 并获得聚合状态。
- en: '|  | $S_{\ell}=\big{\{}(i,F(i))\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{\ell}=\big{\{}(i,F(i))\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  |'
- en: with corresponding disaggregation sets $I_{1},\ldots,I_{q}$. This scheme is
    appropriate for problems where it can be implemented so that each disaggregation
    set $I_{\ell}$ consists of states with similar cost function values; examples
    will be given in Section 4.3.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以及相应的拆解集 $I_{1},\ldots,I_{q}$。此方案适用于可以实施的情况下，其中每个拆解集 $I_{\ell}$ 由具有类似成本函数值的状态组成；将在第4.3节中给出示例。
- en: '(c) State-based approach: Start with a collection of disjoint subsets of states
    $I_{1},\ldots,I_{q}$, and introduce an artificial feature vector $F(i)$ that is
    equal to the index $\ell$ for the states $i\in I_{\ell}$, $\ell=1,\ldots,q$, and
    to some default index, say 0, for the states that do not belong to $\cup_{\ell=1}^{q}I_{\ell}$.
    Then use as aggregate states the subsets'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 状态基础方法：从状态 $I_{1},\ldots,I_{q}$ 的若干不相交子集开始，并引入一个人工特征向量 $F(i)$，对于状态 $i\in
    I_{\ell}$，$\ell=1,\ldots,q$ 其值等于索引 $\ell$，对于不属于 $\cup_{\ell=1}^{q}I_{\ell}$ 的状态，其值等于某个默认索引，例如
    0。然后使用作为聚合状态的子集
- en: '|  | $S_{\ell}=\big{\{}(i,\ell)\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{\ell}=\big{\{}(i,\ell)\mid i\in I_{\ell}\big{\}},\qquad\ell=1,\ldots,q,$
    |  |'
- en: with $I_{1},\ldots,I_{q}$ as the corresponding disaggregation sets. In this
    scheme, the feature vector plays a subsidiary role, but the idea of using disaggregation
    subsets with similar cost function values is still central, as we will discuss
    shortly. (The scheme where the aggregate states are identified with subsets $I_{1},\ldots,I_{q}$
    of original system states has been called “aggregation with representative features”
    in [Ber12], Section 6.5, where its connection with feature-based aggregation has
    been discussed.)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{1},\ldots,I_{q}$ 作为相应的拆解集。在此方案中，特征向量扮演了辅助角色，但使用具有类似成本函数值的拆解子集的思想仍然是核心，如我们即将讨论的那样。（在[Ber12]第6.5节中，将聚合状态识别为原系统状态子集
    $I_{1},\ldots,I_{q}$ 的方案称为“代表特征的聚合”，其中讨论了它与特征基础聚合的联系。）
- en: 'The approaches of forming aggregate states just described cover most of the
    aggregation schemes that have been used in practice. Two classical examples of
    the state-based approach are the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的形成聚合状态的方法涵盖了大多数实际应用中的聚合方案。状态基础方法的两个经典例子如下：
- en: '|  |    Hard Aggregation: The starting point here is a partition of the state
    space that consists of disjoint subsets $I_{1},\ldots,I_{q}$ of states with $I_{1}\cup\cdots\cup
    I_{q}=\{1,\ldots,n\}$. The feature vector $F(i)$ of a state $i$ identifies the
    set of the partition that $i$ belongs to:F(i)=ℓ,  ∀ i∈I_ℓ, ℓ=1,…,q.The aggregate
    states are the subsets S_ℓ={(i,ℓ)∣i∈I_ℓ},  ℓ=1,…,q,and their disaggregation sets
    are the subsets $I_{1},\ldots,I_{q}$. The disaggregation probabilities $d_{i\ell}$
    are positive only for states $i\in I_{\ell}$ [cf. Eq. (4.3)]. The aggregation
    probabilities are equal to either 0 or 1, according toϕ_jℓ={1if $j\in I_{\ell}$,0otherwise,  j=1,…,n, ℓ=1,…,q,
    [cf. Eq. (4.4)].  |  | (4.5)(4.6) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |    硬聚合：这里的起点是状态空间的一个划分，该划分由不相交的状态子集 $I_{1},\ldots,I_{q}$ 组成，其中 $I_{1}\cup\cdots\cup
    I_{q}=\{1,\ldots,n\}$。状态 $i$ 的特征向量 $F(i)$ 确定 $i$ 所属的划分集：F(i)=ℓ，  ∀ i∈I_ℓ， ℓ=1,…,q。聚合状态是子集
    S_ℓ={(i,ℓ)∣i∈I_ℓ}，  ℓ=1,…,q，及其解构集合是子集 $I_{1},\ldots,I_{q}$。解构概率 $d_{i\ell}$ 仅对状态
    $i\in I_{\ell}$ 为正值 [参见 Eq. (4.3)]。聚合概率等于 0 或 1，根据 ϕ_jℓ={1如果 $j\in I_{\ell}$,0其他情况，  j=1,…,n， ℓ=1,…,q，[参见 Eq. (4.4)]。
    |  | (4.5)(4.6) |'
- en: The following aggregation example is typical of a variety of schemes arising
    in discretization or coarse grid schemes, where a smaller problem is obtained
    by discarding some of the original system states. The essence of this scheme is
    to solve a reduced DP problem, obtained by approximating the discarded state costs
    by interpolation using the nondiscarded state costs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的聚合示例是离散化或粗网格方案中各种方案的典型例子，其中通过丢弃一些原始系统状态来获得较小的问题。这种方案的本质是解决一个减少的 DP 问题，通过使用未丢弃的状态成本的插值来近似丢弃状态的成本。
- en: '|  |    Aggregation with Representative States: The starting point here is
    a collection of states $i_{1},\ldots,i_{q}$ that we view as “representative.”
    The costs of the nonrepresentative states are approximated by interpolation of
    the costs of the representative states, using the aggregation probabilities. The
    feature mapping isF(i)={ℓif $i=i_{\ell},\ \ell=1,\ldots,q$,0otherwise.The aggregate
    states are $S_{\ell}=\big{\{}(i_{\ell},\ell)\big{\}}$, $\ell=1,\ldots,q$, the
    disaggregation sets are $I_{\ell}=\{i_{\ell}\}$, $\ell=1,\ldots,q$, and the disaggregation
    probabilities are equal to either 0 or 1, according tod_ℓi={1if $i=i_{\ell}$,0otherwise,  i=1,…,n, ℓ=1,…,q,
    |  | (4.7) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |    代表性状态的聚合：这里的起点是一个状态集合 $i_{1},\ldots,i_{q}$，我们视为“代表性”的状态。非代表性状态的成本通过对代表性状态成本的插值来进行近似，使用聚合概率。特征映射是F(i)={ℓ如果
    $i=i_{\ell},\ \ell=1,\ldots,q$,0其他情况。聚合状态是 $S_{\ell}=\big{\{}(i_{\ell},\ell)\big{\}}$，
    $\ell=1,\ldots,q$，解构集合是 $I_{\ell}=\{i_{\ell}\}$， $\ell=1,\ldots,q$，解构概率等于 0 或
    1，根据 d_ℓi={1如果 $i=i_{\ell}$,0其他情况，  i=1,…,n, ℓ=1,…,q， |  | (4.7) |'
- en: '|  |   [cf. Eq. (4.3)]. The aggregation probabilities must satisfy the constraint
    $\phi_{j\ell}=1$ if $j=i_{\ell}$, $\ell=1,\ldots,q$ [cf. Eq. (4.4)], and can be
    arbitrary for states $j\notin\{i_{1},\ldots,i_{q}\}$.   |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |   [参见 Eq. (4.3)]。聚合概率必须满足约束条件 $\phi_{j\ell}=1$ 如果 $j=i_{\ell}$， $\ell=1,\ldots,q$
    [参见 Eq. (4.4)]，且对于状态 $j\notin\{i_{1},\ldots,i_{q}\}$ 可以是任意值。   |  |'
- en: An important class of aggregation frameworks with representative states arises
    in partially observed Markovian decision problems (POMDP), where observations
    from a controlled Markov chain become available sequentially over time. Here the
    states of the original high-dimensional DP problem are either information vectors
    (groups of past measurements) or “belief states” (conditional probability distributions
    of the state of the Markov chain given the available information). Features may
    be state estimates (given the information) and possibly their variances, or a
    relatively small number of representative belief states (see e.g., Section 5.1
    of [Ber12] or the paper by Yu and Bertsekas [YuB04] and the references quoted
    there).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的代表性状态聚合框架的类别出现在部分观测的马尔可夫决策问题（POMDP）中，其中来自受控马尔可夫链的观察数据随着时间的推移逐步获得。在这里，原始高维
    DP 问题的状态要么是信息向量（过去测量的组），要么是“信念状态”（给定可用信息的马尔可夫链状态的条件概率分布）。特征可能是状态估计（给定信息）及其方差，或者是相对较少的代表性信念状态（参见例如
    [Ber12] 的第 5.1 节或 Yu 和 Bertsekas [YuB04] 的论文及其中引用的参考文献）。
- en: Choice of Disaggregation Probabilities
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**解构概率的选择**'
- en: In both of the preceding aggregation schemes, the requirement $d_{\ell i}=0$
    for all $i\notin I_{\ell}$, cf. Eq. (4.3), leaves a lot of room for choice of
    the disaggregation probabilities. Simple examples show that the values of these
    probabilities can affect significantly the quality of aggregation-based approximations;
    the paper by Van Roy [Van06] provides a relevant discussion. Thus, finding a good
    set of disaggregation probabilities is an interesting issue.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的两种聚合方案中，对于所有 $i\notin I_{\ell}$，要求 $d_{\ell i}=0$（参见 Eq. (4.3)），这为离散化概率的选择留下了很大的空间。简单的例子表明，这些概率的值可以显著影响基于聚合的近似质量；Van
    Roy 的论文 [Van06] 提供了相关讨论。因此，找到一组好的离散化概率是一个有趣的问题。
- en: Generally, problem-specific knowledge and intuition can be helpful in designing
    aggregation schemes, but more systematic methods may be desirable, based on some
    kind of gradient or random search optimization. In particular, for a given set
    of aggregate states and matrix $\Phi$, we may introduce a parameter vector $\theta$
    and a parametrized disaggregation matrix $D(\theta)$, which is differentiable
    with respect to $\theta$. Then for a given policy $\mu$, we may try to find $\theta$
    that minimizes some cost function $F\big{(}\Phi r(\theta)\big{)}$, where $r(\theta)$
    is defined as the unique solution of the corresponding aggregation equation $r=D(\theta)T_{\mu}\Phi
    r$. For example we may use as cost function $F$ the squared Bellman equation residual
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，特定问题的知识和直觉对于设计聚合方案可能是有帮助的，但基于某种梯度或随机搜索优化的更系统的方法可能是更为理想的。特别是，对于给定的聚合状态集和矩阵
    $\Phi$，我们可以引入一个参数向量 $\theta$ 和一个关于 $\theta$ 可微的参数化离散化矩阵 $D(\theta)$。然后，对于给定的策略
    $\mu$，我们可以尝试找到使某个成本函数 $F\big{(}\Phi r(\theta)\big{)}$ 最小化的 $\theta$，其中 $r(\theta)$
    定义为相应聚合方程 $r=D(\theta)T_{\mu}\Phi r$ 的唯一解。例如，我们可以使用 Bellman 方程残差的平方作为成本函数 $F$。
- en: '|  | $F\big{(}\Phi r(\theta)\big{)}=\big{\&#124;}\Phi r(\theta)-\Phi D(\theta)T_{\mu}\Phi
    r(\theta)\big{\&#124;}^{2}.$ |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $F\big{(}\Phi r(\theta)\big{)}=\big{\&#124;}\Phi r(\theta)-\Phi D(\theta)T_{\mu}\Phi
    r(\theta)\big{\&#124;}^{2}.$ |  |'
- en: The key point here is that we can calculate the gradient of $r(\theta)$ with
    respect to each component of $\theta$ by using simulation and low-dimensional
    calculations based on aggregation. We can then use the chain rule to compute the
    gradient of $F$ with respect to $\theta$ for use in some gradient-based optimization
    method. This methodology has been developed for a related projected equation context
    by Menache, Mannor, and Shimkin [MMS06], Yu and Bertsekas YuB09], and Di Castro
    and Mannor [DiM10], but has never been tried in the context of aggregation. The
    paper [MMS06] also suggests the use of random search algorithms, such as the cross
    entropy method, in the context of basis function optimization. A further discussion
    of parametric optimization of the disaggregation probabilities or other structural
    elements of the aggregation framework is beyond the scope of the present paper,
    but may be an interesting subject for investigation.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于，我们可以通过使用基于聚合的模拟和低维计算来计算 $r(\theta)$ 对 $\theta$ 中每个分量的梯度。然后，我们可以使用链式法则来计算
    $F$ 对 $\theta$ 的梯度，以用于一些基于梯度的优化方法。这种方法在 Menache、Mannor 和 Shimkin [MMS06]、Yu 和
    Bertsekas [YuB09] 以及 Di Castro 和 Mannor [DiM10] 的相关投影方程背景下已得到发展，但在聚合背景下尚未尝试过。论文
    [MMS06] 还建议在基函数优化背景下使用随机搜索算法，如交叉熵法。对离散化概率或聚合框架中其他结构元素的参数优化的进一步讨论超出了本文的范围，但可能是一个有趣的研究课题。
- en: 4.1    The Aggregate Problem
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 4.1    聚合问题
- en: Given a feature-based aggregation framework (i.e., the aggregate states $S_{1},\ldots,S_{q}$,
    the corresponding disaggregation sets $I_{1},\ldots,I_{q}$, and the aggregation
    and disaggregation distributions), we can consider an aggregate DP problem that
    involves transitions between aggregate states. In particular, the transition probabilities
    $p_{ij}(u)$, and the disaggregation and aggregation probabilities specify a controlled
    dynamic system involving both the original system states and the aggregate states
    (cf. Fig. 4.2).^†^†† We will consider the aggregate problem for the case where
    there are multiple possible controls at each state. However, it is also possible
    to consider the aggregate problem for the purpose of finding an approximation
    to the cost function $J_{\mu}$ of a given policy $\mu$; this is the special case
    where the control constraint set $U(i)$ consists of the single control $\mu(i)$
    for every state $i$.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个基于特征的聚合框架（即，聚合状态$S_{1},\ldots,S_{q}$、相应的解聚集合$I_{1},\ldots,I_{q}$以及聚合和解聚分布），我们可以考虑一个涉及聚合状态之间转换的聚合DP问题。特别地，转换概率$p_{ij}(u)$，以及解聚和聚合概率指定了一个控制动态系统，涉及原始系统状态和聚合状态（参见
    Fig. 4.2）。^†^†† 我们将考虑在每个状态有多个可能控制的情况的聚合问题。然而，也可以考虑为寻找给定策略$\mu$的成本函数$J_{\mu}$的近似而研究聚合问题；这是控制约束集$U(i)$在每个状态$i$由单一控制$\mu(i)$组成的特例。
- en: '![[Uncaptioned image]](img/3fe1ed359fcb9ec817d77106397a8109.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/3fe1ed359fcb9ec817d77106397a8109.png)'
- en: Figure 4.2.  Illustration of the transition mechanism and the costs per stage
    of the aggregate problem.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2. 聚合问题的转换机制及每阶段的成本示意图。
- en: (i) From aggregate state $S_{\ell}$, we generate a transition to original system
    state $i$ according to $d_{\ell i}$ (note that $i$ must belong to the disaggregation
    set $I_{\ell}$, because of the requirement that $d_{\ell i}>0$ only if $i\in I_{\ell}$).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 从聚合状态$S_{\ell}$，我们根据$d_{\ell i}$生成一个到原始系统状态$i$的转换（注意，$i$必须属于解聚集合$I_{\ell}$，因为要求$d_{\ell
    i}>0$仅当$i\in I_{\ell}$时）。
- en: (ii) From original system state $i$, we generate a transition to original system
    state $j$ according to $p_{ij}(u)$, with cost $g(i,u,j)$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 从原始系统状态$i$，我们根据$p_{ij}(u)$生成一个到原始系统状态$j$的转换，成本为$g(i,u,j)$。
- en: (iii) From original system state $j$, we generate a transition to aggregate
    state $S_{\ell}$ according to $\phi_{j\ell}$ [note here the requirement that $\phi_{j\ell}=1$
    if $j\in I_{\ell}$; cf. Eq. (4.4)].
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 从原始系统状态$j$，我们根据$\phi_{j\ell}$生成一个到聚合状态$S_{\ell}$的转换[注意这里的要求是如果$j\in I_{\ell}$，则$\phi_{j\ell}=1$；参见
    Eq. (4.4)]。
- en: 'This is a DP problem with an enlarged state space that consists of two copies
    of the original state space $\{1,\ldots,n\}$ plus the $q$ aggregate states. We
    introduce the corresponding optimal vectors $\tilde{J}_{0}$, $\tilde{J}_{1}$,
    and $r^{*}=\{r^{*}_{1},\ldots,r^{*}_{q}\}$ where:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有扩展状态空间的DP问题，该状态空间包括两个原始状态空间$\{1,\ldots,n\}$的副本以及$q$个聚合状态。我们引入相应的最优向量$\tilde{J}_{0}$、$\tilde{J}_{1}$，以及$r^{*}=\{r^{*}_{1},\ldots,r^{*}_{q}\}$，其中：
- en: $r^{*}_{\ell}$ is the optimal cost-to-go from aggregate state $S_{\ell}$.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: $r^{*}_{\ell}$是从聚合状态$S_{\ell}$开始的最优成本。
- en: $\tilde{J}_{0}(i)$ is the optimal cost-to-go from original system state $i$
    that has just been generated from an aggregate state (left side of Fig. 4.3).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{J}_{0}(i)$是从原始系统状态$i$开始的最优成本，这个状态刚刚从聚合状态生成（见图 4.3 的左侧）。
- en: $\tilde{J}_{1}(j)$ is the optimal cost-to-go from original system state $j$
    that has just been generated from an original system state (right side of Fig. 4.3).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{J}_{1}(j)$是从原始系统状态$j$开始的最优成本，这个状态刚刚从原始系统状态生成（见图 4.3 的右侧）。
- en: Note that because of the intermediate transitions to aggregate states, $\tilde{J}_{0}$
    and $\tilde{J}_{1}$ are different.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于中间的聚合状态转换，$\tilde{J}_{0}$和$\tilde{J}_{1}$是不同的。
- en: 'These three vectors satisfy the following three Bellman’s equations:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个向量满足以下三条贝尔曼方程：
- en: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\tilde{J}_{0}(i),\qquad\ell=1,\ldots,q,$
    |  | (4.8) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\tilde{J}_{0}(i),\qquad\ell=1,\ldots,q,$
    |  | (4.8) |'
- en: '|  | $\tilde{J}_{0}(i)=\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\big{(}g(i,u,j)+\alpha\tilde{J}_{1}(j)\big{)},\qquad
    i=1,\ldots,n,$ |  | (4.9) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{0}(i)=\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\big{(}g(i,u,j)+\alpha\tilde{J}_{1}(j)\big{)},\qquad
    i=1,\ldots,n,$ |  | (4.9) |'
- en: '|  | $\tilde{J}_{1}(j)=\sum_{m=1}^{q}\phi_{j\ell}r^{*}_{m},\qquad j=1,\dots,n.$
    |  | (4.10) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{1}(j)=\sum_{m=1}^{q}\phi_{j\ell}r^{*}_{m},\qquad j=1,\dots,n.$
    |  | (4.10) |'
- en: By combining these equations, we see that $r^{*}$ satisfies
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些方程，我们看到$r^{*}$满足
- en: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}\,r^{*}_{m}\right),\qquad\ell=1,\ldots,q,$
    |  | (4.11) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}\,r^{*}_{m}\right),\qquad\ell=1,\ldots,q,$
    |  | (4.11) |'
- en: or equivalently $r^{*}=Hr^{*}$, where $H$ is the mapping that maps the vector
    $r$ to the vector $Hr$ with components
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 或者等效地 $r^{*}=Hr^{*}$，其中 $H$ 是将向量 $r$ 映射到具有分量的向量 $Hr$ 的映射。
- en: '|  | $(Hr)(\ell)=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q.$
    |  | (4.12) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $(Hr)(\ell)=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q.$
    |  | (4.12) |'
- en: 'It can be shown that $H$ is a contraction mapping with respect to the sup-norm
    and thus has $r^{*}$ as its unique fixed point. This follows from standard contraction
    arguments, and the fact that $d_{\ell i}$, $p_{ij}(u)$, and $\phi_{j\ell}$ are
    probabilities. Note the nature of $r^{*}_{\ell}$: it is the optimal cost of the
    aggregate state $S_{\ell}$, which is the restriction of the feature mapping $F$
    on the disaggregation set $I_{\ell}$. Thus, roughly, $r^{*}_{\ell}$ is an approximate
    optimal cost associated with states in $I_{\ell}$.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明 $H$ 是一个关于 sup-范数的收缩映射，因此 $r^{*}$ 是其唯一的不动点。这是由于标准的收缩论证以及 $d_{\ell i}$、$p_{ij}(u)$
    和 $\phi_{j\ell}$ 是概率的事实。注意 $r^{*}_{\ell}$ 的性质：它是聚合状态 $S_{\ell}$ 的最优成本，$S_{\ell}$
    是特征映射 $F$ 在去聚合集 $I_{\ell}$ 上的限制。因此，大致来说，$r^{*}_{\ell}$ 是与 $I_{\ell}$ 中的状态相关的近似最优成本。
- en: '![[Uncaptioned image]](img/30c783b63492856884e4c028e8e4fefb.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/30c783b63492856884e4c028e8e4fefb.png)'
- en: Figure 4.3.  The transition mechanism and the cost functions of the aggregate
    problem.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3. 聚合问题的过渡机制和成本函数。
- en: Solution of the Aggregate Problem
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合问题的解决方案
- en: 'While the aggregate problem involves more states than the original DP problem,
    it is in fact easier in some important ways. The reason is that it can be solved
    with algorithms that execute over the smaller space of aggregate states. In particular,
    exact and approximate simulation-based algorithms, can be used to find the lower-dimensional
    vector $r^{*}$ without computing the higher-dimensional vectors $\tilde{J}_{0}$
    and $\tilde{J}_{1}$. We describe some of these methods in Section 4.2, and we
    refer to Chapter 6 of [Ber12] for a more detailed discussion of simulation-based
    methods for computing the vector $r_{\mu}$ of the costs of the aggregate states
    that correspond to a given policy $\mu$. The simulator used for these methods
    is based on Figs. 4.2 and 4.3: transitions to and from the aggregate states are
    generated using the aggregation and disaggregation probabilities, respectively,
    while transitions $(i,j)$ between original system states are generated using a
    simulator of the original system (which is assumed to be available).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管聚合问题涉及的状态比原始动态规划问题更多，但在一些重要方面实际上更简单。原因在于，它可以通过在较小的聚合状态空间上执行的算法来解决。特别是，可以使用精确和近似的基于仿真的算法来找到低维向量
    $r^{*}$，而不需要计算高维向量 $\tilde{J}_{0}$ 和 $\tilde{J}_{1}$。我们在第4.2节描述了一些这些方法，并参考[Ber12]第6章，以获得关于计算与给定策略
    $\mu$ 相对应的聚合状态成本向量 $r_{\mu}$ 的仿真方法的更详细讨论。这些方法使用的仿真器基于图4.2和4.3：状态的聚合和去聚合概率分别用于生成聚合状态之间的过渡，而原始系统状态之间的过渡
    $(i,j)$ 是通过原始系统的仿真器生成的（假设可以获得）。
- en: 'Once $r^{*}$ is found, the optimal-cost-to-go of the original problem may be
    approximated by the vector $\tilde{J}_{1}$ of Eq. (4.10). Note that $\tilde{J}_{1}$
    is a “piecewise linear” cost approximation of $J^{\raise 0.04pt\hbox{\sevenrm*}}$:
    it is constant over each of the disaggregation sets $I_{\ell}$, $\ell=1,\ldots,q$
    [and equal to the optimal cost $r^{*}_{\ell}$ of the aggregate state $S_{\ell}$;
    cf. Eqs. (4.4) and (4.10)], and it is interpolated/linear outside the disaggregation
    sets [cf. Eq. (4.10)]. In the case where $\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$
    (e.g., in hard aggregation), the disaggregation sets $I_{\ell}$ form a partition
    of the original system state space, and $\tilde{J}_{1}$ is piecewise constant.
    Figure 4.4 illustrates a simple example of approximate cost function $\tilde{J}_{1}$.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到 $r^{*}$，原始问题的最优成本函数可以通过方程 (4.10) 的向量 $\tilde{J}_{1}$ 进行近似。注意，$\tilde{J}_{1}$
    是 $J^{\raise 0.04pt\hbox{\sevenrm*}}$ 的“分段线性”成本近似：它在每个去聚合集合 $I_{\ell}$，$\ell=1,\ldots,q$
    上是常数 [并且等于聚合状态 $S_{\ell}$ 的最优成本 $r^{*}_{\ell}$；参见方程 (4.4) 和 (4.10)]，在去聚合集合外是插值/线性
    [参见方程 (4.10)]。在 $\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$（例如，在硬聚合中）的情况下，去聚合集合
    $I_{\ell}$ 形成原系统状态空间的一个划分，$\tilde{J}_{1}$ 是分段常数的。图 4.4 说明了近似成本函数 $\tilde{J}_{1}$
    的一个简单例子。
- en: '![[Uncaptioned image]](img/d86c51a0c3dabae99df128a156c54ec3.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/d86c51a0c3dabae99df128a156c54ec3.png)'
- en: Figure 4.4.  Schematic illustration of the approximate cost function ${\tilde{J\mkern
    5.0mu}\mkern-5.0mu}{}_{1}$. Here the original states are the integers between
    1 and 50\. In this figure there are three aggregate states numbered $1,2,3$. The
    corresponding disaggregation sets are $I_{1}=\{1,\ldots,10\}$, $I_{2}=\{20,\ldots,30\}$,
    $I_{3}=\{40,\ldots,50\}$ are shown in the figure. The values of the approximate
    cost function ${\tilde{J\mkern 5.0mu}\mkern-5.0mu}{}_{1}(i)$ are constant within
    each disaggregation set $I_{\ell}$, $\ell=1,2,3$, and are obtained by linear interpolation
    for states $i$ that do not belong to any one of the sets $I_{\ell}$. If the sets
    $I_{\ell}$, $\ell=1,2,3$, include all the states $1,\ldots,50$, we have a case
    of hard aggregation. If each of the sets $I_{\ell}$, $\ell=1,2,3$, consist of
    a single state, we have a case of aggregation with representative states.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4.  近似成本函数 ${\tilde{J\mkern 5.0mu}\mkern-5.0mu}{}_{1}$ 的示意图。这里原始状态是 1 到
    50 之间的整数。在此图中，有三个总状态编号为 $1,2,3$。相应的去聚合集合是 $I_{1}=\{1,\ldots,10\}$、$I_{2}=\{20,\ldots,30\}$、$I_{3}=\{40,\ldots,50\}$，如图所示。近似成本函数
    ${\tilde{J\mkern 5.0mu}\mkern-5.0mu}{}_{1}(i)$ 在每个去聚合集合 $I_{\ell}$ 内是常数，$\ell=1,2,3$，对于不属于任何一个集合
    $I_{\ell}$ 的状态 $i$ 通过线性插值获得。如果集合 $I_{\ell}$，$\ell=1,2,3$，包含所有状态 $1,\ldots,50$，则为硬聚合的情况。如果每个集合
    $I_{\ell}$，$\ell=1,2,3$，由单一状态组成，则为代表状态的聚合情况。
- en: 'Let us also note that for the purposes of using feature-based aggregation to
    improve a given policy $\mu$, it is not essential to solve the aggregate problem
    to completion. Instead, we may perform one or just a few PIs and adopt the final
    policy obtained as a new “improved” policy. The quality of such a policy depends
    on how well the aggregate problem approximates the original DP problem. While
    it is not easy to quantify the relevant approximation error, generally a small
    error can be achieved if:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应注意，对于使用基于特征的聚合来改进给定策略 $\mu$ 的目的，不必将总问题求解到完成。相反，我们可以执行一次或几次 PI，并将最终获得的策略作为新的“改进”策略。此类策略的质量取决于总问题对原始
    DP 问题的近似程度。虽然量化相关的近似误差并不容易，但通常可以实现较小的误差，如果：
- en: (a) The feature mapping $F$ “conforms” to the optimal cost function $J^{\raise
    0.04pt\hbox{\sevenrm*}}$ in the sense that $F$ varies little in regions of the
    state space where $J^{\raise 0.04pt\hbox{\sevenrm*}}$ also varies little.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 特征映射 $F$ “符合”最优成本函数 $J^{\raise 0.04pt\hbox{\sevenrm*}}$，即在 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    变化很小的状态空间区域中，$F$ 变化也很小。
- en: (b) The aggregate states are selected so that $F$ varies little over each of
    the disaggregation sets $I_{1},\ldots,I_{q}$.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 聚合状态的选择使得 $F$ 在每个去聚合集合 $I_{1},\ldots,I_{q}$ 上变化很小。
- en: This is intuitive and is supported by the subsequent discussion and analysis.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这是直观的，并且得到了后续讨论和分析的支持。
- en: Given the optimal aggregate costs $r^{*}_{\ell}$, $\ell=1,\ldots,q$, the corresponding
    optimal policy is defined implicitly, using the one-step lookahead minimization
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 给定最优的总成本 $r^{*}_{\ell}$，$\ell=1,\ldots,q$，相应的最优策略是隐式定义的，使用一步前瞻最小化。
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}\,r^{*}_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  | (4.13) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}\,r^{*}_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  | (4.13) |'
- en: '[cf. Eq. (4.9)] or a multistep lookahead variant. It is also possible to use
    a model-free implementation, as we describe next.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[参见方程（4.9）]或多步前瞻变体。也可以使用无模型实现，如下所述。'
- en: Model-Free Implementation of the Optimal Policy of the Aggregate Problem
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合问题最优策略的无模型实现
- en: The computation of the optimal policy of the aggregate problem via Eq. (4.13) requires
    knowledge of the transition probabilities $p_{ij}(u)$ and the cost function $g$.
    Alternatively, this policy may be implemented in model-free fashion using a $Q$-factor
    architecture $\tilde{Q}(i,u,\theta)$, as described in Section 2.4, i.e., compute
    sample approximate $Q$-factors
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过方程（4.13）计算聚合问题的最优策略需要知道转移概率$p_{ij}(u)$和成本函数$g$。另外，可以使用$Q$-因子架构$\tilde{Q}(i,u,\theta)$以无模型的方式实现该策略，如第2.4节所述，即计算样本近似$Q$-因子。
- en: '|  | $\beta_{m}=g(i_{m},u_{m},j_{m})+\alpha\sum_{\ell=1}^{q}\phi_{j_{m}\ell}\,r^{*}_{\ell},\qquad
    m=1,\ldots,M,$ |  | (4.14) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta_{m}=g(i_{m},u_{m},j_{m})+\alpha\sum_{\ell=1}^{q}\phi_{j_{m}\ell}\,r^{*}_{\ell},\qquad
    m=1,\ldots,M,$ |  | (4.14) |'
- en: cf. Eq. (2.21), compute $\tilde{\theta}$ via a least squares regression
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 参见方程（2.21），通过最小二乘回归计算$\tilde{\theta}$
- en: '|  | $\tilde{\theta}\in\arg\min_{\theta}\sum_{m=1}^{M}\big{(}\tilde{Q}(i_{m},u_{m},\theta)-\beta_{m}\big{)}^{2}$
    |  | (4.15) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{\theta}\in\arg\min_{\theta}\sum_{m=1}^{M}\big{(}\tilde{Q}(i_{m},u_{m},\theta)-\beta_{m}\big{)}^{2}$
    |  | (4.15) |'
- en: (or a regularized version thereof), cf. Eq. (2.22), and approximate the optimal
    policy of the aggregate problem via
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: （或其正则化版本），参见方程（2.22），并通过以下方式近似聚合问题的最优策略
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\tilde{Q}(i,u,\tilde{\theta}),\qquad
    i=1,\ldots,n,$ |  | (4.16) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\tilde{Q}(i,u,\tilde{\theta}),\qquad
    i=1,\ldots,n,$ |  | (4.16) |'
- en: cf. Eq. (2.23).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 参见方程（2.23）。
- en: Error Bounds
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 错误界限
- en: Intuitively, if the disaggregation sets nearly cover the entire state space
    (in the sense that $\cup_{\ell=1,\ldots,q}I_{\ell}$ contains “most” of the states
    $1,\ldots,n$) and $J^{\raise 0.04pt\hbox{\sevenrm*}}$ is nearly constant over
    each disaggregation set, then $\tilde{J}_{0}$ and $\tilde{J}_{1}$ should be close
    to $J^{\raise 0.04pt\hbox{\sevenrm*}}$. In particular, in the case of hard aggregation,
    we have the following error bound, due to Tsitsiklis and VanRoy [TsV96]. We adapt
    their proof to the notation and terminology of this paper.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，如果拆分集合几乎覆盖整个状态空间（即$\cup_{\ell=1,\ldots,q}I_{\ell}$包含“绝大多数”状态$1,\ldots,n$），并且$J^{\raise
    0.04pt\hbox{\sevenrm*}}$在每个拆分集合上几乎是常数，那么$\tilde{J}_{0}$和$\tilde{J}_{1}$应该接近$J^{\raise
    0.04pt\hbox{\sevenrm*}}$。特别地，在硬聚合的情况下，我们有如下错误界限，由Tsitsiklis和VanRoy [TsV96]提供。我们将他们的证明调整为本文的符号和术语。
- en: '|  |    Proposition 4.1: In the case of hard aggregation, where $\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$,
    and Eqs. (4.5), (4.6) hold, we have—J^* (i)-r^*_ℓ—≤ϵ1-α,  ∀ i such that i∈I_ℓ, ℓ=1,…,q,where
    ϵ=max_ℓ=1,…,q max_i,j∈I_ℓ—J^* (i)-J^* (j)—.  |  | (4.17)(4.18) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  |    命题 4.1：在硬聚合的情况下，其中$\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$，且方程（4.5）、（4.6）成立，我们有—$J^*
    (i)-r^*_{\ell}$—≤$\epsilon/(1-\alpha)$，  对所有$i$使得$i∈I_{\ell}$，$\ell=1,…,q$，其中$\epsilon=\max_{\ell=1,…,q}\max_{i,j∈I_{\ell}}—J^*
    (i)-J^* (j)—$。  |  | (4.17)(4.18) |'
- en: Proof: Consider the mapping $H$ defined by Eq. (4.12), and consider the vector
    $\overline{r}$ with components defined by
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 证明：考虑由方程（4.12）定义的映射$H$，并考虑由以下分量定义的向量$\overline{r}$
- en: '|  | $\overline{r}_{\ell}=\min_{i\in I_{\ell}}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\epsilon\over
    1-\alpha},\qquad\ell\in 1,\ldots,q.$ |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overline{r}_{\ell}=\min_{i\in I_{\ell}}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\epsilon\over
    1-\alpha},\qquad\ell\in 1,\ldots,q.$ |  |'
- en: Denoting by $\ell(j)$ the index of the disaggregation set to which $j$ belongs,
    i.e., $j\in I_{\ell(j)}$, we have for all $\ell$,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\ell(j)$为$j$所属拆分集合的索引，即$j\in I_{\ell(j)}$，我们对所有$\ell$有，
- en: '|  | $\eqalignno{(H\overline{r})(\ell)&amp;=\sum_{i=1}^{n}d_{\ell i}\min_{u\in
    U(i)}\sum_{j=1}^{n}p_{ij}(u)\Big{(}g(i,u,j)+\alpha\overline{r}_{\ell(j)}\Big{)}\cr&amp;\leq\sum_{i=1}^{n}d_{\ell
    i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha J^{\raise 0.04pt\hbox{\sevenrm*}}(j)+{\alpha\epsilon\over
    1-\alpha}\right)\cr&amp;=\sum_{i=1}^{n}d_{\ell i}\left(J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\alpha\epsilon\over
    1-\alpha}\right)\cr&amp;\leq\min_{i\in I_{\ell}}\bigl{(}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+\epsilon\bigr{)}+{\alpha\epsilon\over
    1-\alpha}\cr&amp;=\min_{i\in I_{\ell}}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\epsilon\over
    1-\alpha}\cr&amp;=\overline{r}_{\ell},\cr}$ |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eqalignno{(H\overline{r})(\ell)&amp;=\sum_{i=1}^{n}d_{\ell i}\min_{u\in
    U(i)}\sum_{j=1}^{n}p_{ij}(u)\Big{(}g(i,u,j)+\alpha\overline{r}_{\ell(j)}\Big{)}\cr&amp;\leq\sum_{i=1}^{n}d_{\ell
    i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha J^{\raise 0.04pt\hbox{\sevenrm*}}(j)+{\alpha\epsilon\over
    1-\alpha}\right)\cr&amp;=\sum_{i=1}^{n}d_{\ell i}\left(J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\alpha\epsilon\over
    1-\alpha}\right)\cr&amp;\leq\min_{i\in I_{\ell}}\bigl{(}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+\epsilon\bigr{)}+{\alpha\epsilon\over
    1-\alpha}\cr&amp;=\min_{i\in I_{\ell}}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)+{\epsilon\over
    1-\alpha}\cr&amp;=\overline{r}_{\ell},\cr}$ |  |'
- en: where for the second equality we used the Bellman equation for the original
    system, which is satisfied by $J^{\raise 0.04pt\hbox{\sevenrm*}}$, and for the
    second inequality we used Eq. (4.18). Thus we have $H\overline{r}\leq\overline{r}$,
    from which it follows that $r^{*}\leq\overline{r}$ (since $H$ is monotone, which
    implies that the sequence $\{H^{k}\overline{r}\}$ is monotonically nonincreasing,
    and we have
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个等式中，我们使用了原始系统的贝尔曼方程，该方程由$J^{\raise 0.04pt\hbox{\sevenrm*}}$满足；在第二个不等式中，我们使用了公式（4.18）。因此，我们有$H\overline{r}\leq\overline{r}$，由此可以推导出$r^{*}\leq\overline{r}$（因为$H$是单调的，这意味着序列$\{H^{k}\overline{r}\}$是单调递减的，我们有
- en: '|  | $r^{*}=\lim_{k\to\infty}H^{k}\overline{r}$ |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{*}=\lim_{k\to\infty}H^{k}\overline{r}$ |  |'
- en: since $H$ is a contraction). This proves one side of the desired error bound.
    The other side follows similarly.  Q.E.D.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$H$是一个收缩映射。这证明了期望误差界的一侧。另一侧类似地得到。 Q.E.D.
- en: The scalar $\epsilon$ of Eq. (4.18) is the maximum variation of optimal cost
    within the sets of the partition of the hard aggregation scheme. Thus the meaning
    of the preceding proposition is that if the optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    varies by at most $\epsilon$ within each set of the partition, the hard aggregation
    scheme yields a piecewise constant approximation to the optimal cost function
    that is within ${\epsilon/(1-\alpha)}$ of the optimal. We know that for every
    approximation $\tilde{J}$ of $J^{\raise 0.04pt\hbox{\sevenrm*}}$ that is constant
    within each disaggregation set, the error
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 标量$\epsilon$在公式（4.18）中表示硬聚合方案划分集合内最优成本的最大变化。因此，前述命题的含义是，如果最优成本函数$J^{\raise 0.04pt\hbox{\sevenrm*}}$在每个划分集合内变化不超过$\epsilon$，则硬聚合方案提供了一个分段常数的最优成本函数近似值，该近似值在${\epsilon/(1-\alpha)}$以内。我们知道，对于每个在每个分解集合内常数的$J^{\raise
    0.04pt\hbox{\sevenrm*}}$的近似值$\tilde{J}$，误差
- en: '|  | $\max_{i=1,\ldots,n}\big{&#124;}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)-\tilde{J}(i)\big{&#124;}$
    |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{i=1,\ldots,n}\big{&#124;}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)-\tilde{J}(i)\big{&#124;}$
    |  |'
- en: is at least equal to $\epsilon/2$. Based on the bound (4.17), the actual value
    of this error for the case where $\tilde{J}$ is obtained by hard aggregation involves
    an additional multiplicative factor that is at most equal to $2/(1-\alpha)$, and
    depends on the disaggregation probabilities. In practice the bound (4.17) is typically
    conservative, and no examples are known where it is tight. Moreover, even for
    hard aggregation, the manner in which the error $J^{\raise 0.04pt\hbox{\sevenrm*}}-\tilde{J}_{1}$
    depends on the disaggregation distributions is complicated and is an interesting
    subject for research.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 至少等于$\epsilon/2$。基于界限（4.17），对于通过硬聚合获得的情况，这个误差的实际值涉及一个额外的乘法因子，该因子最大为$2/(1-\alpha)$，并且依赖于分解概率。实际上，界限（4.17）通常是保守的，没有已知的例子表明它是紧的。此外，即使是硬聚合，误差$J^{\raise
    0.04pt\hbox{\sevenrm*}}-\tilde{J}_{1}$如何依赖于分解分布也是复杂的，这是一个有趣的研究课题。
- en: The following proposition extends the result of the preceding proposition to
    the case where the aggregation probabilities are all either 0 or 1, in which case
    the cost function $\tilde{J}_{1}$ obtained by aggregation is a piecewise constant
    function, but the disaggregation sets need not form a partition of the state space.
    Examples of this type of scheme include cases where the aggregation probabilities
    are generated by a “nearest neighbor” scheme, and the cost $\tilde{J}_{1}(j)$
    of a state $j\notin\cup_{\ell=1}^{q}I_{\ell}$ is taken to be equal to the cost
    of the “nearest” state within $\cup_{\ell=1}^{q}I_{\ell}$.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 下述命题将前述命题的结果扩展到聚合概率全部为 0 或 1 的情况，此时通过聚合得到的成本函数 $\tilde{J}_{1}$ 是一个分段常数函数，但解聚集合不一定形成状态空间的划分。这种类型的方案的例子包括那些由“最近邻”方案生成的聚合概率的情况，以及状态
    $j\notin\cup_{\ell=1}^{q}I_{\ell}$ 的成本 $\tilde{J}_{1}(j)$ 被认为等于在 $\cup_{\ell=1}^{q}I_{\ell}$
    内“最近”状态的成本。
- en: '|  |    Proposition 4.2: Assume that each aggregation probability $\phi_{j\ell}$,
    $j=1,\ldots,n$, $\ell=1,\ldots,q$, is equal to either 0 or 1, and consider the
    sets^I_ℓ={j∣ϕ_jℓ=1},  ℓ=1,…,q. |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |    命题 4.2： 假设每个聚合概率 $\phi_{j\ell}$, $j=1,\ldots,n$, $\ell=1,\ldots,q$，都等于
    0 或 1，并考虑集合 `^I_ℓ={j∣ϕ_jℓ=1}`，  ℓ=1,…,q。 |  |'
- en: '|  |   Then we have—J^* (i)-r^*_ℓ—≤ϵ1-α,  ∀ i∈^I_ℓ, ℓ=1,…,q,where ϵ=max_ℓ=1,…,q max_i,j∈^I_ℓ—J^*
    (i)-J^* (j)—.   |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  |   然后我们有—`J^* (i)-r^*_ℓ—≤ϵ1-α`，  ∀ i∈`^I_ℓ`， ℓ=1,…,q，其中 `ϵ=max_ℓ=1,…,q max_i,j∈^I_ℓ—J^*
    (i)-J^* (j)—`。 |  |'
- en: Proof: We first note that by the definition of a feature-based aggregation scheme,
    we have $I_{\ell}\subset\hat{I}_{\ell}$ for all $\ell=1,\ldots,q$, while the sets
    $\hat{I}_{\ell}$, $\ell=1,\ldots,q$, form a partition of the original state space,
    in view of our assumption on the aggregation probabilities. Let us replace the
    feature vector $F$ with another feature vector $\hat{F}$ of the form
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 证明： 我们首先注意到，根据基于特征的聚合方案的定义，我们有 $I_{\ell}\subset\hat{I}_{\ell}$ 对于所有的 $\ell=1,\ldots,q$，同时集合
    $\hat{I}_{\ell}$, $\ell=1,\ldots,q$ 形成了原始状态空间的一个划分，这是基于我们对聚合概率的假设。我们用另一个特征向量 $\hat{F}$
    替换特征向量 $F$，形式为
- en: '|  | $\hat{F}(i)=\ell,\qquad\forall\ i\in\hat{I}_{\ell},\ \ell=1,\ldots,q.$
    |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{F}(i)=\ell,\qquad\forall\ i\in\hat{I}_{\ell},\ \ell=1,\ldots,q.$
    |  |'
- en: Since the aggregation probabilities are all either 0 or 1, the resulting aggregation
    scheme with $I_{\ell}$ replaced by $\hat{I}_{\ell}$, and with the aggregation
    and disaggregation probabilities remaining unchanged, is a hard aggregation scheme.
    When the result of Prop. 4.1 is applied to this hard aggregation scheme, the result
    of the present proposition follows.  Q.E.D.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚合概率都为 0 或 1，将 $I_{\ell}$ 替换为 $\hat{I}_{\ell}$ 后，结果的聚合方案，且聚合和解聚概率保持不变，是一个硬聚合方案。当将命题
    4.1 应用于这个硬聚合方案时，本命题的结果随之得出。 Q.E.D.
- en: The preceding propositions suggest the principal guideline for a feature-based
    aggregation scheme. It should be designed so that states that belong to the same
    disaggregation set have nearly equal optimal costs. In Section 4.3 we will elaborate
    on schemes that are based on this idea. In the next section we discuss the solution
    of the aggregate problem by simulation-based methods.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命题提示了特征基聚合方案的主要指导原则。它应该被设计成使得属于同一解聚集合的状态具有几乎相等的最优成本。在第 4.3 节中，我们将详细讨论基于这一思想的方案。在下一节中，我们将讨论通过基于模拟的方法解决聚合问题。
- en: 4.2    Solving the Aggregate Problem with Simulation-Based Methods
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 4.2    用基于模拟的方法解决聚合问题
- en: We will now focus on methods to compute the optimal cost vector $r^{*}$ of the
    aggregate problem that corresponds to the aggregate states. This is the unique
    solution of Eq. (4.11). We first note that since $r^{*}$, together with the cost
    functions $\tilde{J}_{0}$ and $\tilde{J}_{1}$, form the solution of the Bellman
    equations (4.8)-(4.10), they can all be computed with the classical (exact) methods
    of policy and value iteration (PI and VI for short, respectively). However, in
    this section, we will discuss specialized versions of PI and VI that compute just
    $r^{*}$ (which has relatively low dimension), but not $\tilde{J}_{0}$ and $\tilde{J}_{1}$
    (which may have astronomical dimension). These methods are based on stochastic
    simulation as they involve the aggregate problem, which is stochastic because
    of the disaggregation and aggregation probabilities, even if the original problem
    is deterministic.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注于计算与聚合状态对应的聚合问题的**最优成本向量**$r^{*}$的方法。这是方程式 (4.11) 的唯一解。我们首先注意到，由于$r^{*}$与成本函数$\tilde{J}_{0}$和$\tilde{J}_{1}$一起，形成了贝尔曼方程
    (4.8)-(4.10) 的解，因此它们可以通过经典的（精确的）策略和价值迭代方法（简称 PI 和 VI）来计算。然而，在本节中，我们将讨论 PI 和 VI
    的专门版本，这些版本仅计算$r^{*}$（其维度相对较低），而不计算$\tilde{J}_{0}$和$\tilde{J}_{1}$（可能具有天文数字级别的维度）。这些方法基于随机模拟，因为它们涉及聚合问题，而聚合问题是随机的，因为存在分解和聚合概率，即使原始问题是确定性的。
- en: We start with simulation-based versions of PI, where policy evaluation is done
    with lookup table versions of classical methods such as LSTD(0), LSPE(0), and
    TD(0), applied to a reduced size DP problem whose states are just the aggregate
    states.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基于模拟的 PI 版本开始，其中策略评估是通过查找表版本的经典方法，如 LSTD(0)，LSPE(0)，和 TD(0) 完成的，这些方法应用于一个简化规模的
    DP 问题，其状态仅为聚合状态。
- en: Simulation-Based Policy Iteration
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟的策略迭代
- en: One possible way to compute $r^{*}$ is a PI-like algorithm, which generates
    sequences of policies $\{\mu^{k}\}$ for the original problem and vectors $\{r^{k}\}$,
    which converge to an optimal policy and $r^{*}$, respectively. The algorithm does
    not compute any intermediate estimates of the high-dimensional vectors $\tilde{J}_{0}$
    and $\tilde{J}_{1}$. It starts with a stationary policy $\mu^{0}$ for the original
    problem, and given $\mu^{k}$, it performs a policy evaluation step by finding
    the unique fixed point of the contraction mapping $H_{\mu^{k}}=DT_{\mu^{k}}\Phi$
    that maps the vector $r$ to the vector $H_{\mu^{k}}r$ with components
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 $r^{*}$ 的一种可能方法是 PI 类算法，它生成原始问题的策略序列 $\{\mu^{k}\}$ 和向量 $\{r^{k}\}$，这些序列和向量分别收敛到最优策略和
    $r^{*}$。该算法不计算任何高维向量 $\tilde{J}_{0}$ 和 $\tilde{J}_{1}$ 的中间估计值。它从原始问题的静态策略 $\mu^{0}$
    开始，给定 $\mu^{k}$ 后，通过找到收缩映射 $H_{\mu^{k}}=DT_{\mu^{k}}\Phi$ 的唯一不动点，执行策略评估步骤，该映射将向量
    $r$ 映射到向量 $H_{\mu^{k}}r$，其分量为
- en: '|  | $(H_{\mu^{k}}r)(\ell)=\sum_{i=1}^{n}d_{\ell i}\sum_{j=1}^{n}p_{ij}\bigl{(}\mu^{k}(i)\bigr{)}\left(g\bigl{(}i,\mu^{k}(i),j\bigr{)}+\alpha\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q,$
    |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  | $(H_{\mu^{k}}r)(\ell)=\sum_{i=1}^{n}d_{\ell i}\sum_{j=1}^{n}p_{ij}\bigl{(}\mu^{k}(i)\bigr{)}\left(g\bigl{(}i,\mu^{k}(i),j\bigr{)}+\alpha\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q,$
    |  |'
- en: cf. Eq. (4.12). Thus the policy evaluation step finds $r^{k}=\{r^{k}_{\ell}\mid\ell=1,\ldots,q\}$
    satisfying
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: cf. 方程式 (4.12) 生成 $\mu^{k+1}$
- en: '|  | $r^{k}=H_{\mu^{k}}r^{k}=DT_{\mu^{k}}\Phi r^{k}=D\big{(}g_{\mu^{k}}+\alpha
    P_{\mu^{k}}\Phi r^{k}\big{)},$ |  | (4.19) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{k}=H_{\mu^{k}}r^{k}=DT_{\mu^{k}}\Phi r^{k}=D\big{(}g_{\mu^{k}}+\alpha
    P_{\mu^{k}}\Phi r^{k}\big{)},$ |  | (4.19) |'
- en: where $P_{\mu^{k}}$ is the transition probability matrix corresponding to $\mu^{k}$,
    $g_{\mu^{k}}$ is the expected cost vector of $\mu^{k}$, i.e., the vector whose
    $i$th component is
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{\mu^{k}}$ 是对应于 $\mu^{k}$ 的转移概率矩阵，$g_{\mu^{k}}$ 是 $\mu^{k}$ 的期望成本向量，即，其第
    $i$ 个分量为
- en: '|  | $\sum_{j=1}^{n}p_{ij}\bigl{(}\mu^{k}(i)\bigr{)}g\bigl{(}i,\mu^{k}(i),j\bigr{)},\qquad
    i=1,\ldots,n,$ |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{j=1}^{n}p_{ij}\bigl{(}\mu^{k}(i)\bigr{)}g\bigl{(}i,\mu^{k}(i),j\bigr{)},\qquad
    i=1,\ldots,n,$ |  |'
- en: and $D$ and $\Phi$ are the matrices with rows the disaggregation and aggregation
    distributions, respectively. Following the policy evaluation step, the algorithm
    generates $\mu^{k+1}$ by
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: $D$ 和 $\Phi$ 是矩阵，其行分别为分解和聚合分布。根据策略评估步骤，算法通过
- en: '|  | $\mu^{k+1}(i)=\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}r^{k}_{m}\right),\qquad
    i=1,\ldots,n;$ |  | (4.20) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu^{k+1}(i)=\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{m=1}^{q}\phi_{jm}r^{k}_{m}\right),\qquad
    i=1,\ldots,n;$ |  | (4.20) |'
- en: this is the policy improvement step. In the preceding minimization we use one
    step lookahead, but a multistep lookahead or Monte Carlo tree search can also
    be used.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这是策略改进步骤。在前面的最小化中我们使用了一步前瞻，但也可以使用多步前瞻或蒙特卡罗树搜索。
- en: It can be shown that this algorithm converges finitely to the unique solution
    of Eq. (4.11) [equivalently the unique fixed point of the mapping $H$ of Eq. (4.12)].
    An indirect way to show this is to use the convergence of PI applied to the aggregate
    problem to generate a sequence $\{\mu^{k},r^{k},\tilde{J}_{0}^{k},\tilde{J}_{1}^{k}\}$.
    We provide a more direct proof, which is essentially a special case of a more
    general convergence proof for PI-type methods given in Prop. 3.1 of [Ber11a].
    The key fact here is that the linear mappings $DT_{\mu}\Phi$ and $\Phi DT_{\mu}$
    are sup-norm contractions, and also have the monotonicity property of DP mappings,
    which is used in an essential way in the standard convergence proof of ordinary
    PI.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，该算法有限地收敛于方程 (4.11) 的唯一解（等价地是映射 $H$ 的唯一不动点，见方程 (4.12)）。一种间接的证明方法是利用应用于整体问题的
    PI 收敛性来生成序列 $\{\mu^{k},r^{k},\tilde{J}_{0}^{k},\tilde{J}_{1}^{k}\}$。我们提供了一种更直接的证明，这实际上是
    [Ber11a] 中 Prop. 3.1 所给的一般收敛证明的特例。关键事实是线性映射 $DT_{\mu}\Phi$ 和 $\Phi DT_{\mu}$ 是上确界范数收缩映射，并且具有
    DP 映射的单调性，这在标准的普通 PI 收敛证明中以基本方式被使用。
- en: '|  |    Proposition 4.3:  Let $\mu^{0}$ be any policy and let $\{\mu^{k},r^{k}\}$
    be a sequence generated by the PI algorithm (4.19)-(4.20). Then the sequence $\{r^{k}\}$
    is monotonically nonincreasing (i.e., we have $r_{\ell}^{k}\geq r_{\ell}^{k+1}$
    for all $\ell$ and $k$) and there exists an index $\bar{k}$ such that $r^{\bar{k}}$
    is equal to $r^{*}$, the unique solution of Eq. (4.11).  |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | 命题 4.3：  设 $\mu^{0}$ 为任意策略，且 $\{\mu^{k},r^{k}\}$ 为由 PI 算法（4.19）-(4.20)
    生成的序列。那么序列 $\{r^{k}\}$ 是单调非递增的（即，对于所有的 $\ell$ 和 $k$，都有 $r_{\ell}^{k}\geq r_{\ell}^{k+1}$），并且存在一个索引
    $\bar{k}$ 使得 $r^{\bar{k}}$ 等于 $r^{*}$，这是方程 (4.11) 的唯一解。 |  |'
- en: Proof: For each policy $\mu$, we consider the linear mapping $\Phi DT_{\mu}:\Re^{n}\mapsto\Re^{n}$
    given by
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 证明： 对于每个策略 $\mu$，我们考虑线性映射 $\Phi DT_{\mu}:\Re^{n}\mapsto\Re^{n}$，其定义为
- en: '|  | $\Phi DT_{\mu}J=\Phi D(g_{\mu}+\alpha P_{\mu}J),\qquad J\in\Re^{n}.$ |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi DT_{\mu}J=\Phi D(g_{\mu}+\alpha P_{\mu}J),\qquad J\in\Re^{n}.$ |  |'
- en: This mapping is monotone in the sense that for all vectors $J$ and $J^{\prime}$
    with $J\geq J^{\prime}$, we have
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 该映射在所有向量 $J$ 和 $J^{\prime}$ 满足 $J\geq J^{\prime}$ 的情况下是单调的。
- en: '|  | $\Phi DT_{\mu}J\geq\Phi DT_{\mu}J^{\prime},$ |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi DT_{\mu}J\geq\Phi DT_{\mu}J^{\prime},$ |  |'
- en: 'since the matrix $\alpha\Phi DP_{\mu}$ of the mapping has nonnegative components.
    Moreover, the mapping is a contraction of modulus $\alpha$ with respect to the
    sup-norm. The reason is that the matrix $\Phi DP_{\mu}$ is a transition probability
    matrix, i.e., it has nonnegative components and its row sums are all equal to
    1\. This can be verified by a straightforward calculation, using the fact that
    the rows of $\Phi$ and $D$ are probability distributions while $P_{\mu}$ is a
    transition probability matrix. It can also be intuitively verified from the structure
    of the aggregate problem: $\Phi DP_{\mu}$ is the matrix of transition probabilities
    under policy $\mu$ for the Markov chain whose $n$ states are the states depicted
    in the top righthand side of Fig. 4.2.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 由于映射的矩阵 $\alpha\Phi DP_{\mu}$ 具有非负分量。此外，该映射在上确界范数下是一个模量为 $\alpha$ 的收缩映射。原因在于矩阵
    $\Phi DP_{\mu}$ 是一个转移概率矩阵，即，它具有非负分量且其行和均为 1。可以通过直接计算验证这一点，使用了 $\Phi$ 和 $D$ 的行是概率分布的事实，而
    $P_{\mu}$ 是转移概率矩阵。也可以从整体问题的结构直观验证：$\Phi DP_{\mu}$ 是在策略 $\mu$ 下，对于 Markov 链的状态矩阵，其
    $n$ 个状态在图 4.2 的右上角。
- en: Since the mapping $DT_{\mu^{k}}\Phi$ has $r^{k}$ as its unique fixed point [cf. Eq. (4.19)],
    we have $r^{k}=DT_{\mu^{k}}\Phi r^{k}$, so that the vector
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 由于映射 $DT_{\mu^{k}}\Phi$ 具有 $r^{k}$ 作为其唯一不动点 [参见方程 (4.19)]，我们有 $r^{k}=DT_{\mu^{k}}\Phi
    r^{k}$，因此向量
- en: '|  | $\tilde{J}_{\mu^{k}}=\Phi r^{k}$ |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu^{k}}=\Phi r^{k}$ |  |'
- en: satisfies
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 满足
- en: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT_{\mu^{k}}\tilde{J}_{\mu^{k}}.$ |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT_{\mu^{k}}\tilde{J}_{\mu^{k}}.$ |  |'
- en: It follows that $\tilde{J}_{\mu^{k}}$ is the unique fixed point of the contraction
    mapping $\Phi DT_{\mu^{k}}$. By using the definition
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\tilde{J}_{\mu^{k}}$ 是收缩映射 $\Phi DT_{\mu^{k}}$ 的唯一不动点。利用定义
- en: '|  | $T_{\mu^{k+1}}\tilde{J}_{\mu^{k}}=T\tilde{J}_{\mu^{k}}$ |  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{\mu^{k+1}}\tilde{J}_{\mu^{k}}=T\tilde{J}_{\mu^{k}}$ |  |'
- en: of $\mu^{k+1}$ [cf. Eq. (4.20)], we have
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 的 $\mu^{k+1}$ [参见方程 (4.20)]，我们有
- en: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT_{\mu^{k}}\tilde{J}_{\mu^{k}}\geq\Phi DT\tilde{J}_{\mu^{k}}=\Phi
    DT_{\mu^{k+1}}\tilde{J}_{\mu^{k}}.$ |  | (4.21) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT_{\mu^{k}}\tilde{J}_{\mu^{k}}\geq\Phi DT\tilde{J}_{\mu^{k}}=\Phi
    DT_{\mu^{k+1}}\tilde{J}_{\mu^{k}}.$ |  | (4.21) |'
- en: Applying repeatedly the monotone mapping $\Phi DT_{\mu^{k+1}}$ to this relation,
    we have for all $m\geq 1$,
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 将单调映射$\Phi DT_{\mu^{k+1}}$重复应用于这个关系，我们对于所有$m\geq 1$有，
- en: '|  | $\tilde{J}_{\mu^{k}}\geq(\Phi DT_{\mu^{k+1}})^{m}\tilde{J}_{\mu^{k}}\geq\lim_{m\to\infty}(\Phi
    DT_{\mu^{k+1}})^{m}\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}},$ |  | (4.22) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu^{k}}\geq(\Phi DT_{\mu^{k+1}})^{m}\tilde{J}_{\mu^{k}}\geq\lim_{m\to\infty}(\Phi
    DT_{\mu^{k+1}})^{m}\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}},$ |  | (4.22) |'
- en: where the equality follows from the fact that $\tilde{J}_{\mu^{k+1}}$ is the
    fixed point of the contraction mapping $\Phi DT_{\mu^{k+1}}$. It follows that
    $\tilde{J}_{\mu^{k}}\geq\tilde{J}_{\mu^{k+1}}$, or equivalently
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的等式来源于事实，即$\tilde{J}_{\mu^{k+1}}$是收缩映射$\Phi DT_{\mu^{k+1}}$的固定点。因此，$\tilde{J}_{\mu^{k}}\geq\tilde{J}_{\mu^{k+1}}$，或者等价地
- en: '|  | $\Phi r^{k}\geq\Phi r^{k+1},\qquad k=0,1,\ldots.$ |  |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi r^{k}\geq\Phi r^{k+1},\qquad k=0,1,\ldots.$ |  |'
- en: By the definition of the feature-based aggregation architecture [cf. Eq. (4.4)],
    each column of $\Phi$ has at least one component that is equal to 1\. Therefore
    we have for all $k$
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特征基础聚合架构的定义 [参见方程（4.4）]，$\Phi$的每一列至少有一个分量等于1。因此对于所有$k$我们有
- en: '|  | $r^{k}\geq r^{k+1},$ |  |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{k}\geq r^{k+1},$ |  |'
- en: and moreover, the equality $r^{k}=r^{k+1}$ holds if and only if $\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}}.$
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当且仅当$\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}}$时，等式$r^{k}=r^{k+1}$成立。
- en: The inequality $\tilde{J}_{\mu^{k}}\geq\tilde{J}_{\mu^{k+1}}$ implies that as
    long as $\tilde{J}_{\mu^{k}}\neq\tilde{J}_{\mu^{k+1}}$, a policy $\mu^{k}$ cannot
    be repeated. Since there is only a finite number of policies, it follows that
    we must eventually have $\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}}$. In view of
    Eqs. (4.21)-(4.22), we see that
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 不等式$\tilde{J}_{\mu^{k}}\geq\tilde{J}_{\mu^{k+1}}$意味着，只要$\tilde{J}_{\mu^{k}}\neq\tilde{J}_{\mu^{k+1}}$，策略$\mu^{k}$就不能重复。由于策略的数量是有限的，因此我们最终必须有$\tilde{J}_{\mu^{k}}=\tilde{J}_{\mu^{k+1}}$。根据方程（4.21）-（4.22），我们看到
- en: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT\tilde{J}_{\mu^{k}}$ |  |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu^{k}}=\Phi DT\tilde{J}_{\mu^{k}}$ |  |'
- en: or
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '|  | $\Phi r^{k}=\Phi DT\Phi r^{k}.$ |  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Phi r^{k}=\Phi DT\Phi r^{k}.$ |  |'
- en: Since each column of $\Phi$ has at least one component that is equal to 1, it
    follows that $r^{k}$ is a fixed point of the mapping $H=DT\Phi$ of Eq. (4.12),
    which is $r^{*}$ by Eq. (4.11).  Q.E.D.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$\Phi$的每一列至少有一个分量等于1，因此$r^{k}$是方程（4.12）中的映射$H=DT\Phi$的固定点，而根据方程（4.11），它是$r^{*}$。 Q.E.D.
- en: To avoid the $n$-dimensional calculations of the policy evaluation step in the
    PI algorithm (4.19)-(4.20), one may use simulation. In particular, the policy
    evaluation equation, $r=H_{\mu}r$, is linear of the form
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免PI算法（4.19）-（4.20）中的政策评估步骤的$n$维计算，可以使用模拟。特别地，政策评估方程$r=H_{\mu}r$是形式为
- en: '|  | $r=Dg_{\mu}+\alpha DP_{\mu}\Phi r,$ |  | (4.23) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  | $r=Dg_{\mu}+\alpha DP_{\mu}\Phi r,$ |  | (4.23) |'
- en: '[cf. Eq. (4.19)]. Let us write this equation as $Cr=b$, where'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[参见方程（4.19）]。我们将这个方程写作$Cr=b$，其中'
- en: '|  | $C=I-\alpha DP_{\mu}\Phi,\qquad b=Dg_{\mu},$ |  |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=I-\alpha DP_{\mu}\Phi,\qquad b=Dg_{\mu},$ |  |'
- en: and note that it is Bellman’s equation for a policy with cost per stage vector
    equal to $Dg_{\mu}$ and transition probability matrix equal to $DP_{\mu}\Phi$.
    This is the transition matrix under policy $\mu$ for the Markov chain whose states
    are the aggregate states. The solution $r_{\mu}$ of the policy evaluation Eq. (4.23) is
    the cost vector corresponding to this Markov chain, and can be found by using
    simulation-based methods with lookup table representation.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 并且注意这是对于策略的贝尔曼方程，其每阶段成本向量等于$Dg_{\mu}$，转移概率矩阵等于$DP_{\mu}\Phi$。这是策略$\mu$下的马尔可夫链的转移矩阵，其中状态是聚合状态。策略评估方程（4.23）的解$r_{\mu}$是对应于该马尔可夫链的成本向量，可以通过使用基于模拟的方法和查找表表示来找到。
- en: In particular, we may use model-free simulation to approximate $C$ and $b$,
    and then solve the system $Cr=b$ approximately. To this end, we obtain a sequence
    of sample transitions $\big{\{}(i_{1},j_{1}),(i_{2},j_{2}),\ldots\big{\}}$ by
    first generating a sequence of states $\{i_{1},i_{2},\ldots\}$ according to some
    distribution $\{\xi_{i}\mid i=1,\ldots,n\}$ (with $\xi_{i}>0$ for all $i$), and
    then generate for each $m\geq 1$ a sample transition $(i_{m},j_{m})$ according
    to the distribution $\{p_{i_{m}j}\mid j=1,\ldots,n\}$. Given the first $M$ samples,
    we form the matrix $\widehat{C}_{M}$ and vector $\hat{b}_{M}$ given by
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们可以使用无模型模拟来近似 $C$ 和 $b$，然后近似地求解系统 $Cr=b$。为此，我们首先生成一个状态序列 $\{i_{1},i_{2},\ldots\}$，按照某些分布
    $\{\xi_{i}\mid i=1,\ldots,n\}$（对所有 $i$，$\xi_{i}>0$），然后对于每个 $m\geq 1$，生成一个样本转移
    $(i_{m},j_{m})$，按照分布 $\{p_{i_{m}j}\mid j=1,\ldots,n\}$。给定前 $M$ 个样本，我们形成矩阵 $\widehat{C}_{M}$
    和向量 $\hat{b}_{M}$，由下列公式给出
- en: '|  | $\widehat{C}_{M}=I-{\alpha\over M}\sum_{m=1}^{M}{1\over\xi_{i_{m}}}d(i_{m})\phi(j_{m})^{\prime},\qquad\hat{b}_{M}={1\over
    M}\sum_{m=1}^{M}{1\over\xi_{i_{m}}}d(i_{m})g\big{(}i_{m},\mu(i_{m}),j_{m}\big{)},$
    |  | (4.24) |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{C}_{M}=I-{\alpha\over M}\sum_{m=1}^{M}{1\over\xi_{i_{m}}}d(i_{m})\phi(j_{m})^{\prime},\qquad\hat{b}_{M}={1\over
    M}\sum_{m=1}^{M}{1\over\xi_{i_{m}}}d(i_{m})g\big{(}i_{m},\mu(i_{m}),j_{m}\big{)},$
    |  | (4.24) |'
- en: where $d(i)$ is the $i$th column of $D$ and $\phi(j)^{\prime}$ is the $j$th
    row of $\Phi$. We can then show that $\widehat{C}_{M}\to C$ and $\hat{b}_{M}\to
    d$ by using law of large numbers arguments, i.e., writing
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d(i)$ 是 $D$ 的第 $i$ 列，$\phi(j)^{\prime}$ 是 $\Phi$ 的第 $j$ 行。然后，我们可以使用大数法则的论证来展示
    $\widehat{C}_{M}\to C$ 和 $\hat{b}_{M}\to d$，即写作
- en: '|  | $C=I-\alpha\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}\big{(}\mu(i)\big{)}d(i)\phi(j)^{\prime},\qquad
    b=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}\big{(}\mu(i)\big{)}d(i)g\bigl{(}i,\mu(i),j\bigr{)},$
    |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=I-\alpha\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}\big{(}\mu(i)\big{)}d(i)\phi(j)^{\prime},\qquad
    b=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij}\big{(}\mu(i)\big{)}d(i)g\bigl{(}i,\mu(i),j\bigr{)},$
    |  |'
- en: multiplying and dividing $p_{ij}\big{(}\mu(i)\big{)}$ by $\xi_{i}$ in order
    to properly view these expressions as expected values, and using the relation
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 $p_{ij}\big{(}\mu(i)\big{)}$ 乘以 $\xi_{i}$ 以适当地将这些表达式视为期望值，并使用关系
- en: '|  | $\lim_{M\to\infty}{\hbox{Number of occurrences of the $i$ to $j$ transition
    from time $m=1$ to $m=M$}\over M}=\xi_{i}\,p_{ij}\big{(}\mu(i)\big{)}.$ |  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lim_{M\to\infty}{\hbox{从时间 $m=1$ 到 $m=M$ 的 $i$ 到 $j$ 转移的次数}\over M}=\xi_{i}\,p_{ij}\big{(}\mu(i)\big{)}.$
    |  |'
- en: The corresponding estimates
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的估计
- en: '|  | $\hat{r}_{M}=\widehat{C}_{M}^{-1}\hat{b}_{M}$ |  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}_{M}=\widehat{C}_{M}^{-1}\hat{b}_{M}$ |  |'
- en: 'converge to the unique solution of the policy evaluation Eq. (4.23) as $M\to\infty$,
    and provide the estimates $\Phi\hat{r}_{M}$ of the cost vector $J_{\mu}$ of $\mu$:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛到政策评估方程 (4.23) 的唯一解，且提供 $\mu$ 的成本向量 $J_{\mu}$ 的估计 $\Phi\hat{r}_{M}$：
- en: '|  | $\tilde{J}_{\mu}=\Phi\hat{r}_{M}.$ |  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{\mu}=\Phi\hat{r}_{M}.$ |  |'
- en: This is the aggregation counterpart of the LSTD(0) method. One may also use
    an iterative simulation-based LSPE(0)-type method or a TD(0)-type method to solve
    the equation $Cr=b$; see [Ber12].
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 LSTD(0) 方法的聚合对等物。也可以使用基于迭代模拟的 LSPE(0)-型方法或 TD(0)-型方法来解决方程 $Cr=b$；见 [Ber12]。
- en: 'Note that instead of using the probabilities $\xi_{i}$ to sample directly original
    system states, we may alternatively sample the aggregate states $S_{\ell}$ according
    to some distribution $\{\zeta_{\ell}\mid\ell=1,\ldots,q\}$, generate a sequence
    of aggregate states $\{S_{\ell_{1}},S_{\ell_{2}},\ldots\}$, and then generate
    a state sequence $\{i_{1},i_{2},\ldots\}$ using the disaggregation probabilities.
    In this case Eq. (4.24) should be modified as follows:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以选择不使用概率 $\xi_{i}$ 直接抽样原系统状态，而是根据某些分布 $\{\zeta_{\ell}\mid\ell=1,\ldots,q\}$
    抽样聚合状态 $S_{\ell}$，生成一系列聚合状态 $\{S_{\ell_{1}},S_{\ell_{2}},\ldots\}$，然后使用分解概率生成状态序列
    $\{i_{1},i_{2},\ldots\}$。在这种情况下，方程 (4.24) 应修改如下：
- en: '|  | $\widehat{C}_{M}=I-{\alpha\over M}\sum_{m=1}^{M}{1\over\zeta_{\ell_{m}}d_{\ell_{m}i_{m}}}d(i_{m})\phi(j_{m})^{\prime},\qquad\hat{b}_{M}={1\over
    M}\sum_{m=1}^{M}{1\over\zeta_{\ell_{m}}d_{\ell_{m}i_{m}}}d(i_{m})g\big{(}i_{m},\mu(i_{m}),j_{m}\big{)}.$
    |  |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{C}_{M}=I-{\alpha\over M}\sum_{m=1}^{M}{1\over\zeta_{\ell_{m}}d_{\ell_{m}i_{m}}}d(i_{m})\phi(j_{m})^{\prime},\qquad\hat{b}_{M}={1\over
    M}\sum_{m=1}^{M}{1\over\zeta_{\ell_{m}}d_{\ell_{m}i_{m}}}d(i_{m})g\big{(}i_{m},\mu(i_{m}),j_{m}\big{)}.$
    |  |'
- en: The main difficulty with the policy improvement step at a given state $i$ is
    the need to compute the expected value in the $Q$-factor expression
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定状态 $i$ 的策略改进步骤中的主要困难是需要计算 $Q$-因子表达式中的期望值
- en: '|  | $\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}r^{k}_{\ell}\right)$
    |  |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}r^{k}_{\ell}\right)$
    |  |'
- en: that is minimized over $u\in U(i)$. If the transition probabilities $p_{ij}(u)$
    are available and the number of successor states [the states $j$ such that $p_{ij}(u)>0$]
    is small, this expected value may be easily calculated (an important case where
    this is so is when the system is deterministic). Otherwise, one may consider approximating
    this expected value using one of the model-free schemes described in Section 2.4.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这在 $u\in U(i)$ 上被最小化。如果转移概率 $p_{ij}(u)$ 可用，并且后继状态的数量 [即 $p_{ij}(u)>0$ 的状态 $j$]
    较小，则可以容易地计算这个期望值（一个重要的情况是当系统是确定性时）。否则，可以考虑使用第 2.4 节中描述的无模型方案来近似这个期望值。
- en: Simulation-Based Value Iteration and $Q$-Learning
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 基于仿真的价值迭代和 $Q$-学习
- en: An exact VI algorithm for obtaining $r^{*}$ is the fixed point iteration
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一个精确的 VI 算法用于获得 $r^{*}$ 是固定点迭代
- en: '|  | $r^{k+1}=Hr^{k},$ |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{k+1}=Hr^{k},$ |  |'
- en: starting from some initial guess $r^{0}$, where $H$ is the contraction mapping
    of Eq. (4.12). A stochastic approximation-type algorithm based on this fixed point
    iteration generates a sequence of aggregate states $\{S_{\ell_{0}},S_{\ell_{1}},\ldots\}$
    by some probabilistic mechanism, which ensures that all aggregate states are generated
    infinitely often. Given $r^{k}$ and $S_{\ell_{k}}$, it independently generates
    an original system state $i_{k}$ according to the probabilities $d_{\ell i}$,
    and updates the component $r_{\ell_{k}}$ according to
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 从某个初始猜测 $r^{0}$ 开始，其中 $H$ 是方程 (4.12) 的收缩映射。基于这种固定点迭代的随机逼近类型算法通过某种概率机制生成一系列汇总状态
    $\{S_{\ell_{0}},S_{\ell_{1}},\ldots\}$，这确保所有汇总状态被无限次生成。给定 $r^{k}$ 和 $S_{\ell_{k}}$，它根据概率
    $d_{\ell i}$ 独立生成一个原始系统状态 $i_{k}$，并根据
- en: '|  | $r^{k+1}_{\ell_{k}}=(1-\gamma_{k})r^{k}_{\ell_{k}}+\gamma_{k}\min_{u\in
    U(i)}\sum_{j=1}^{n}p_{i_{k}j}(u)\left(g(i_{k},u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}r^{k}_{\ell}\right),$
    |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{k+1}_{\ell_{k}}=(1-\gamma_{k})r^{k}_{\ell_{k}}+\gamma_{k}\min_{u\in
    U(i)}\sum_{j=1}^{n}p_{i_{k}j}(u)\left(g(i_{k},u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}r^{k}_{\ell}\right),$
    |  |'
- en: 'where $\gamma_{k}$ is a diminishing positive stepsize, and leaves all the other
    components unchanged:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma_{k}$ 是递减的正步长，并保持所有其他组件不变：
- en: '|  | $r^{k+1}_{\ell}=r^{k}_{\ell},\qquad\hbox{if }\ell\neq\ell_{k}.$ |  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{k+1}_{\ell}=r^{k}_{\ell},\qquad\hbox{如果 }\ell\neq\ell_{k}.$ |  |'
- en: This algorithm can be viewed as an asynchronous stochastic approximation version
    of VI. The stepsize $\gamma_{k}$ should be diminishing (typically at the rate
    of $1/k$), and its justification and convergence mechanism are very similar to
    the ones for the $Q$-learning algorithm. We refer to the paper by Tsitsiklis and
    VanRoy [TsV96] for further discussion and analysis (see also [BeT96], Section
    3.1.2 and 6.7).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法可以视为 VI 的异步随机逼近版本。步长 $\gamma_{k}$ 应该是递减的（通常为 $1/k$ 的速率），其证明和收敛机制与 $Q$-学习算法的类似。我们参考
    Tsitsiklis 和 VanRoy 的论文 [TsV96] 以获得进一步的讨论和分析（参见 [BeT96]，第 3.1.2 节和 6.7 节）。
- en: A somewhat different algorithm is possible in the case of hard aggregation,
    assuming that for every $\ell$, the set $U(i)$ is the same for all states $i$
    in the disaggregation set $I_{\ell}$. Then, as discussed in [BeT96], Section 6.7.7,
    we can introduce $Q$-factors that are constant within each set $I_{\ell}$ and
    have the form
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬聚合的情况下，如果对于每个 $\ell$，集合 $U(i)$ 对于分解集合 $I_{\ell}$ 中的所有状态 $i$ 都相同，则可能会有一种稍微不同的算法。然后，如
    [BeT96] 第 6.7.7 节中讨论的，我们可以引入在每个集合 $I_{\ell}$ 内是常数的 $Q$-因子，其形式为
- en: '|  | $\tilde{Q}(i,u)=Q(\ell,u),\qquad\ i\in I_{\ell},\ u\in U(i).$ |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{Q}(i,u)=Q(\ell,u),\qquad\ i\in I_{\ell},\ u\in U(i).$ |  |'
- en: We then obtain an algorithm that updates the $Q$-factors $Q(\ell,u)$ one at
    a time, using a $Q$-learning-type iteration of the form
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到一个更新 $Q$-因子 $Q(\ell,u)$ 的算法，采用类似 $Q$-学习的迭代形式
- en: '|  | $Q(\ell,u):=(1-\gamma)Q(\ell,u)+\gamma\Big{(}g(i,u,j)+\alpha\min_{v\in
    U(j)}Q\big{(}{m(j)},v\big{)}\Big{)},$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(\ell,u):=(1-\gamma)Q(\ell,u)+\gamma\Big{(}g(i,u,j)+\alpha\min_{v\in
    U(j)}Q\big{(}{m(j)},v\big{)}\Big{)},$ |  |'
- en: where $i$ is a state within $I_{\ell}$ that is chosen with probability $d_{\ell
    i}$, $j$ is the outcome of a transition simulated according to the transition
    probabilities $p_{ij}(u)$, the index $m(j)$ corresponds to the aggregate state
    $S_{m(j)}$ to which $j$ belongs, and $\gamma$ is the stepsize. It can be seen
    that this algorithm coincides with $Q$-learning with lookup table representation,
    applied to a lower dimensional aggregate DP problem that involves just the aggregate
    states. With a suitably decreasing stepsize $\gamma$ and assuming that each pair
    $(\ell,u)$ is simulated an infinite number of times, the standard convergence
    results for $Q$-learning [Tsi94] apply.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$i$是从$I_{\ell}$中选择的一个状态，其选择概率为$d_{\ell i}$，$j$是根据转移概率$p_{ij}(u)$模拟的转移结果，索引$m(j)$对应于$j$所属的聚合状态$S_{m(j)}$，$\gamma$是步长。可以看出，该算法与$Q$-学习的查找表表示一致，应用于一个仅涉及聚合状态的低维聚合动态规划问题。在适当递减的步长$\gamma$下，并且假设每对$(\ell,u)$被模拟无限次，则$Q$-学习的标准收敛结果[Tsi94]适用。
- en: We note, however, that the $Q$-learning algorithm just described has a substantial
    drawback. It solves an aggregate problem that differs from the aggregate problem
    described in Section 4.1, because implicit in the algorithm is the restriction
    that the same control is applied at all states $i$ that belong to the same disaggregation
    set. In effect, we are assigning controls to subsets of states (the disaggregation
    sets) and not to individual states of the original problem. Clearly this is a
    coarser form of control, which is inferior in terms of performance. However, the
    $Q$-learning algorithm may find some use in the context of initialization of another
    algorithm that aspires to better performance.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们注意到，刚刚描述的$Q$-学习算法有一个显著的缺陷。它解决的是一个与第4.1节描述的聚合问题不同的聚合问题，因为算法中隐含的限制是，在属于同一去聚合集合的所有状态$i$上应用相同的控制。实际上，我们是将控制分配给状态的子集（去聚合集合），而不是原问题的单个状态。显然，这是一种更粗糙的控制形式，在性能上较差。然而，$Q$-学习算法可能在初始化其他追求更好性能的算法时找到一些用处。
- en: 4.3    Feature Formation by Using Scoring Functions
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 4.3    使用评分函数进行特征形成
- en: The choice of the feature mapping $F$ and the method to obtain aggregate states
    are clearly critical for the success of feature-based aggregation. In the subsequent
    Section 5 we will discuss how deep neural network architectures can be used for
    this purpose. In what follows in this section we consider some simple forms of
    feature mappings that can be used when we already have a reasonable estimate of
    the optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}$ or the cost function
    $J_{\mu}$ of some policy $\mu$, which we can use to group together states with
    similar estimated optimal cost. Then the aggregation approach can provide an improved
    piecewise constant or piecewise linear cost approximation. We provide some simple
    illustrative examples of this approach in Section 4.5.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 特征映射$F$的选择和获得聚合状态的方法显然对于基于特征的聚合的成功至关重要。在随后的第5节中，我们将讨论如何利用深度神经网络架构来实现这一目的。在本节的其余部分，我们考虑一些简单的特征映射形式，这些形式可以在我们已经对最优成本函数$J^{\raise
    0.04pt\hbox{\sevenrm*}}$或某些策略$\mu$的成本函数$J_{\mu}$有合理估计时使用，这样我们可以将具有相似估计最优成本的状态组合在一起。然后，聚合方法可以提供改进的分段常数或分段线性成本近似。在第4.5节中，我们提供了这种方法的一些简单示例。
- en: In particular, suppose that we have obtained in some way a real-valued scoring
    function $V(i)$ of the state $i$, which serves as an index of undesirability of
    state $i$ as a starting state (smaller values of $V$ are assigned to more desirable
    states, consistent with the view of $V$ as some form of “cost” function). One
    possibility is to use as $V$ an approximation of the cost function of some “good”
    (e.g., near-optimal) policy. Another possibility is to obtain $V$ as the cost
    function of some reasonable policy applied to an approximation of the original
    problem (e.g., a related problem that can be solved more easily either analytically
    or computationally; see [Ber17], Section 6.2). Still another possibility is to
    obtain $V$ by training a neural network or other architecture using samples of
    state-cost pairs obtained by using a software or human expert, and some supervised
    learning technique, such as for example Tesauro’s comparison learning scheme [Tes89b],
    [Tes01]. Finally, one may compute $V$ using some form of policy evaluation algorithm
    like TD($\lambda$).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地，假设我们以某种方式获得了状态 $i$ 的实值评分函数 $V(i)$，该评分函数作为状态 $i$ 作为起始状态的不良指数（较小的 $V$ 值分配给更可取的状态，这与
    $V$ 作为某种“成本”函数的观点一致）。一种可能性是将 $V$ 用作某个“良好”（例如，接近最优）策略的成本函数的近似值。另一种可能性是将 $V$ 作为对原始问题的近似的某种合理策略的成本函数来获得（例如，解决更容易的相关问题，无论是通过解析还是计算方法；见
    [Ber17], 第 6.2 节）。还有一种可能性是通过使用软件或人工专家获得的状态-成本对样本，以及某种监督学习技术（例如 Tesauro 的比较学习方案
    [Tes89b], [Tes01]）来训练神经网络或其他架构来获得 $V$。最后，可以使用某种形式的策略评估算法，如 TD($\lambda$) 来计算 $V$。
- en: Given the scoring function $V$, we will construct a feature mapping that groups
    together states $i$ with roughly equal scores $V(i)$. In particular, we let $R_{1},\ldots,R_{q}$
    be $q$ disjoint intervals that form a partition of the set of possible values
    of $V$ [i.e., are such that for any state $i$, there is a unique interval $R_{\ell}$
    such that $V(i)\in R_{\ell}$]. We define a feature vector $F(i)$ of the state
    $i$ according to
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 给定评分函数 $V$，我们将构造一个特征映射，将状态 $i$ 按照大致相等的评分 $V(i)$ 进行分组。具体地，我们令 $R_{1},\ldots,R_{q}$
    为 $q$ 个不相交的区间，这些区间形成 $V$ 值可能范围的划分 [即，对于任何状态 $i$，存在一个唯一的区间 $R_{\ell}$ 使得 $V(i)\in
    R_{\ell}$]。我们根据如下定义状态 $i$ 的特征向量 $F(i)$
- en: '|  | $F(i)=\ell,\qquad\forall\ i\hbox{ such that }V(i)\in R_{\ell},\quad\ell=1,\ldots,q.$
    |  | (4.25) |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i)=\ell,\qquad\forall\ i\hbox{ such that }V(i)\in R_{\ell},\quad\ell=1,\ldots,q.$
    |  | (4.25) |'
- en: This feature vector in turn defines a partition of the state space into the
    sets
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征向量进而定义了状态空间的划分为这些集合
- en: '|  | $I_{\ell}=\big{\{}i\mid F(i)=\ell\big{\}}=\big{\{}i\mid V(i)\in R_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  | (4.26) |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\ell}=\big{\{}i\mid F(i)=\ell\big{\}}=\big{\{}i\mid V(i)\in R_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  | (4.26) |'
- en: Assuming that all the sets $I_{\ell}$ are nonempty, we thus obtain a hard aggregation
    scheme, with aggregation probabilities defined by Eq. (4.6); see Fig. 4.5.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有的集合 $I_{\ell}$ 都非空，因此我们得到一个硬聚合方案，其聚合概率由公式 (4.6) 定义；见图 4.5。
- en: '![[Uncaptioned image]](img/293e23cea932477ae3b6c55ac19cd4f5.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图像]](img/293e23cea932477ae3b6c55ac19cd4f5.png)'
- en: Figure 4.5.  Hard aggregation scheme based on a single scoring function. We
    introduce $q$ disjoint intervals $R_{1},\ldots,R_{q}$ that form a partition of
    the set of possible values of $V$, and we define a feature vector $F(i)$ of the
    state $i$ according to F(i)=ℓ,  ∀ i such that V(i)∈R_ℓ, ℓ=1,…,q. This feature
    vector in turn defines a partition of the state space into the sets I_ℓ={i∣F(i)=ℓ}={i∣V(i)∈R_ℓ},  ℓ=1,…,q.
    The solution of the aggregate problem provides a piecewise constant approximation
    of the optimal cost function of the original problem.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5. 基于单一评分函数的硬聚合方案。我们引入 $q$ 个不相交的区间 $R_{1},\ldots,R_{q}$，这些区间形成 $V$ 值的可能范围的划分，并且我们根据
    $F(i)=ℓ,\forall i \text{ 使得 } V(i)\in R_ℓ,\ell=1,…,q$ 定义状态 $i$ 的特征向量。这个特征向量进而定义了状态空间的划分为集合
    $I_ℓ={i\mid F(i)=ℓ}={i\mid V(i)∈R_ℓ},\qquadℓ=1,…,q$。聚合问题的解提供了对原始问题的最优成本函数的分段常数近似。
- en: A related scoring function scheme may be based on representative states. Here
    the aggregate states and the disaggregation probabilities are obtained by forming
    a fairly large sample set of states $\{i_{m}\mid m=1,\ldots,M\},$ by computing
    their corresponding scores
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的评分函数方案可以基于代表性状态。在这里，通过形成一个相当大的状态样本集 $\{i_{m}\mid m=1,\ldots,M\}$，计算它们对应的评分，从而获得聚合状态和拆分概率。
- en: '|  | $\big{\{}V(i_{m})\mid m=1,\ldots,M\big{\}},$ |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | $\big{\{}V(i_{m})\mid m=1,\ldots,M\big{\}},$ |  |'
- en: and by suitably dividing the range of these scores into disjoint intervals $R_{1},\ldots,R_{q}$
    to form the aggregate states, similar to Eqs. (4.25)-(4.26). Simultaneously we
    obtain subsets of sampled states $\hat{I}_{\ell}\subset I_{\ell}$ to which we
    can assign positive disaggregation probabilities. Figure 4.6 illustrates this
    idea for the case where each subset $\hat{I}_{\ell}$ consists of a single (representative)
    state. This is a form of “discretization” of the original state space based on
    the score values of the states. As the figure indicates, the role of the scoring
    function is to assist in forming a set of states that is small (to keep the aggregate
    DP problem computations manageable) but representative (to provide sufficient
    detail in the approximation of $J^{\raise 0.04pt\hbox{\sevenrm*}}$, i.e., be dense
    in the parts of the state space where $J^{\raise 0.04pt\hbox{\sevenrm*}}$ varies
    a lot, and sparse in other parts).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过适当地将这些分数的范围划分为不相交的区间 $R_{1},\ldots,R_{q}$ 以形成聚合状态，类似于方程 (4.25)-(4.26)。同时，我们获得采样状态的子集
    $\hat{I}_{\ell}\subset I_{\ell}$，对这些子集我们可以分配正的去聚合概率。图 4.6 展示了这一思想，其中每个子集 $\hat{I}_{\ell}$
    由一个单一（代表性）状态组成。这是一种基于状态分数值的原始状态空间的“离散化”形式。如图所示，评分函数的作用是帮助形成一个小的（以保持聚合 DP 问题计算可管理）但具有代表性的状态集合（以在
    $J^{\raise 0.04pt\hbox{\sevenrm*}}$ 的近似中提供足够的细节，即，在 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    变化剧烈的状态空间部分密集，而在其他部分稀疏）。
- en: '![[Uncaptioned image]](img/52d7d6a6ff0e4e895b621c60d9470909.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/52d7d6a6ff0e4e895b621c60d9470909.png)'
- en: Figure 4.6.  Schematic illustration of aggregation based on sampling states
    and using a scoring function $V$ to form a representative set $i_{1},\ldots,i_{q}$.
    A piecewise linear approximation of $J^{*}$ is obtained by using the corresponding
    aggregate costs $r^{*}_{1},\ldots,r^{*}_{q}$ and the aggregation probabilities.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6. 基于采样状态的聚合示意图，并使用评分函数 $V$ 形成代表性集合 $i_{1},\ldots,i_{q}$。通过使用相应的聚合成本 $r^{*}_{1},\ldots,r^{*}_{q}$
    和聚合概率，可以获得 $J^{*}$ 的分段线性近似。
- en: The following proposition illustrates the important role of the quantization
    error, defined as
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命题说明了量化误差的重要作用，其定义为
- en: '|  | $\delta=\max_{\ell=1,\ldots,q}\max_{i,j\in I_{\ell}}\big{&#124;}V(i)-V(j)\big{&#124;}.$
    |  | (4.27) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta=\max_{\ell=1,\ldots,q}\max_{i,j\in I_{\ell}}\big{&#124;}V(i)-V(j)\big{&#124;}.$
    |  | (4.27) |'
- en: It represents the maximum error that can be incurred by approximating $V$ within
    each set $I_{\ell}$ with a single value from its range within the subset.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示通过用子集内范围的单一值来近似 $V$ 时可能产生的最大误差。
- en: '|  |    Proposition 4.4: Consider the hard aggregation scheme defined by a
    scoring function $V$ as described above. Assume that the variations of $J^{\raise
    0.04pt\hbox{\sevenrm*}}$ and $V$ over the sets $I_{1},\ldots,I_{q}$ are within
    a factor $\beta\geq 0$ of each other, i.e., that—J^* (i)-J^* (j)—≤β  —V(i)-V(j)—,  ∀ i,j∈I_ℓ, ℓ=1,…,q.
    (a) We have —J^* (i)-r^*_ℓ—≤βδ1-α,  ∀ i∈I_ℓ, ℓ=1,…,q, where $\delta$ is the quantization
    error of Eq. (4.27). (b) Assume that there is no quantization error, i.e., $V$
    and $J^{\raise 0.04pt\hbox{\sevenrm*}}$ are constant within each set $I_{\ell}$.
    Then the aggregation scheme yields the optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    exactly, i.e., J^* (i)=r^*_ℓ,  ∀ i∈I_ℓ, ℓ=1,…,q.  |  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  |    命题 4.4: 考虑由上述评分函数 $V$ 定义的硬聚合方案。假设 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    和 $V$ 在集合 $I_{1},\ldots,I_{q}$ 上的变化在一个因子 $\beta\geq 0$ 之内，即——J^* (i)-J^* (j)—≤β 
    —V(i)-V(j)—,  ∀ i,j∈I_ℓ, ℓ=1,…,q。 (a) 我们有 —J^* (i)-r^*_ℓ—≤βδ1-α,  ∀ i∈I_ℓ, ℓ=1,…,q，其中
    $\delta$ 是方程 (4.27) 的量化误差。 (b) 假设没有量化误差，即 $V$ 和 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    在每个集合 $I_{\ell}$ 内是常数。则聚合方案准确地给出了最优成本函数 $J^{\raise 0.04pt\hbox{\sevenrm*}}$，即
    J^* (i)=r^*_ℓ,  ∀ i∈I_ℓ, ℓ=1,…,q.  |  |'
- en: Proof: (a) Since we are dealing with a hard aggregation scheme, the result of
    Prop. 4.1 applies. By our assumptions, the maximum variation of $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    over the disaggregation sets $I_{\ell}$ is bounded by $\epsilon=\beta\delta$,
    and the result of part (a) follows from Prop. 4.1.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 证明: (a) 由于我们处理的是硬聚合方案，因此 Prop. 4.1 的结果适用。根据我们的假设，$J^{\raise 0.04pt\hbox{\sevenrm*}}$
    在去聚合集合 $I_{\ell}$ 上的最大变化由 $\epsilon=\beta\delta$ 进行界定，部分 (a) 的结果来自 Prop. 4.1。
- en: (b) This is a special case of part (a) with $\delta=\epsilon=0$.  Q.E.D.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 这是 (a) 部分的一个特殊情况，其中 $\delta=\epsilon=0$。 Q.E.D.
- en: Examples of scoring functions that may be useful in various settings are cost
    functions of nearly optimal policies, or approximations to such cost functions,
    provided for example by a neural network or other approximation schemes. Another
    example, arising in the adaptive aggregation scheme proposed by Bertsekas and
    Castanon [BeC89], is to use as $V(i)$ the residual vector $(TJ)(i)-J(i)$, where
    $J$ is some approximation to the optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}$,
    or the residual vector $(T_{\mu}J)(i)-J(i)$, where $J$ is some approximation to
    the cost function of a policy $\mu$; see also Keller, Mannor, and Precup [KMP06].
    Note that it is not essential that $V$ approximates well $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    or $J_{\mu}$. What is important is that states with similar values of $J^{\raise
    0.04pt\hbox{\sevenrm*}}$ or $J_{\mu}$ also have similar values of $V$.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种设置中可能有用的评分函数的例子包括几乎最优策略的成本函数，或通过神经网络或其他近似方案提供的这些成本函数的近似。另一个例子是，Bertsekas
    和 Castanon [BeC89] 提出的自适应聚合方案中，使用作为 $V(i)$ 的残差向量 $(TJ)(i)-J(i)$，其中 $J$ 是对最优成本函数
    $J^{\raise 0.04pt\hbox{\sevenrm*}}$ 的某种近似，或者残差向量 $(T_{\mu}J)(i)-J(i)$，其中 $J$ 是对策略
    $\mu$ 的成本函数的某种近似；另见 Keller、Mannor 和 Precup [KMP06]。注意，$V$ 很好地近似 $J^{\raise 0.04pt\hbox{\sevenrm*}}$
    或 $J_{\mu}$ 并不是至关重要的。重要的是具有类似 $J^{\raise 0.04pt\hbox{\sevenrm*}}$ 或 $J_{\mu}$
    值的状态也具有类似的 $V$ 值。
- en: Scoring Function Scheme with a State Space Partition
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 基于状态空间分区的评分函数方案
- en: Another useful scheme is based on a scoring function $V$, which is defined separately
    on each one of a collection of disjoint subsets $C_{1},\ldots,C_{m}$ that form
    a partition of the state space. We define a feature vector $F(i)$ that depends
    not only on the value of $V(i)$ but also on the membership of $i$ in the subsets
    of the partition. In particular, for each $\theta=1,\ldots,m$, let $R_{1\theta},\ldots,R_{q\theta}$
    be $q$ disjoint intervals that form a partition of the set of possible values
    of $V$ over the set $C_{\theta}$. We then define
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的方案基于评分函数 $V$，该函数在构成状态空间分区的每个不相交子集 $C_{1},\ldots,C_{m}$ 上单独定义。我们定义一个特征向量
    $F(i)$，它不仅依赖于 $V(i)$ 的值，还依赖于 $i$ 在分区子集中的成员身份。特别地，对于每个 $\theta=1,\ldots,m$，令 $R_{1\theta},\ldots,R_{q\theta}$
    为 $q$ 个不相交的区间，这些区间形成了 $V$ 在集合 $C_{\theta}$ 上可能值的分区。然后我们定义
- en: '|  | $F(i)=(\ell,\theta),\qquad\forall\ i\in C_{\theta}\hbox{ such that }V(i)\in
    R_{\ell\theta}.$ |  | (4.28) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i)=(\ell,\theta),\qquad\forall\ i\in C_{\theta}\hbox{ such that }V(i)\in
    R_{\ell\theta}.$ |  | (4.28) |'
- en: This feature vector in turn defines a partition of the state space into the
    $qm$ sets
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 该特征向量进一步定义了状态空间的分区为 $qm$ 个集合
- en: '|  | $I_{\ell\theta}=\big{\{}i\mid F(i)=(\ell,\theta)\big{\}}=\big{\{}i\in
    C_{\theta}\mid V(i)\in R_{\ell\theta}\big{\}},\qquad\ell=1,\ldots,q,\ \theta=1,\ldots,m,$
    |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\ell\theta}=\big{\{}i\mid F(i)=(\ell,\theta)\big{\}}=\big{\{}i\in
    C_{\theta}\mid V(i)\in R_{\ell\theta}\big{\}},\qquad\ell=1,\ldots,q,\ \theta=1,\ldots,m,$
    |  |'
- en: which represent the disaggregation sets of the resulting hard aggregation scheme.
    In this scheme the aggregate states depend not only on the values of $V$ but also
    on the subset $C_{\theta}$ of the partition.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示最终硬聚合方案的去聚合集。在此方案中，聚合状态不仅依赖于 $V$ 的值，还依赖于分区的子集 $C_{\theta}$。
- en: Using Multiple Scoring Functions
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多重评分函数
- en: The approach of forming features using a single scoring function can be extended
    to the case where we have a vector of scoring functions $V(i)=\big{(}V_{1}(i),\ldots,V_{s}(i)\big{)}$.
    Then we can partition the set of possible values of $V(i)$ into $q$ disjoint subsets
    $R_{1},\ldots,R_{q}$ of the $s$-dimensional space $\Re^{s}$, define a feature
    vector $F(i)$ according to
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单一评分函数形成特征的方法可以扩展到我们拥有评分函数向量 $V(i)=\big{(}V_{1}(i),\ldots,V_{s}(i)\big{)}$
    的情况。然后我们可以将 $V(i)$ 的可能值集合分割成 $s$ 维空间 $\Re^{s}$ 的 $q$ 个不相交的子集 $R_{1},\ldots,R_{q}$，并根据定义特征向量
    $F(i)$。
- en: '|  | $F(i)=\ell,\qquad\forall\ i\hbox{ such that }V(i)\in R_{\ell},\ \ell=1,\ldots,q,$
    |  | (4.29) |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(i)=\ell,\qquad\forall\ i\hbox{ such that }V(i)\in R_{\ell},\ \ell=1,\ldots,q,$
    |  | (4.29) |'
- en: and proceed as in the case of a scalar scoring function, i.e., construct a hard
    aggregation scheme with disaggregation sets given by
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 并按标量评分函数的情况继续，即，构造具有由以下给出的去聚合集的硬聚合方案
- en: '|  | $I_{\ell}=\big{\{}i\mid F(i)=\ell\big{\}}=\big{\{}i\mid V(i)\in R_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\ell}=\big{\{}i\mid F(i)=\ell\big{\}}=\big{\{}i\mid V(i)\in R_{\ell}\big{\}},\qquad\ell=1,\ldots,q.$
    |  |'
- en: One possibility to obtain multiple scoring functions is to start with a single
    fairly simple scoring function, obtain aggregate states as described earlier,
    solve the corresponding aggregate problem, and use the optimal cost function of
    that problem as an additional scoring function. This is reminiscent of feature
    iteration, an idea that has been suggested in several approximate DP works.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 获取多个评分函数的一种可能性是从一个相当简单的评分函数开始，获得之前描述的聚合状态，解决相应的聚合问题，并使用该问题的最优成本函数作为额外的评分函数。这类似于特征迭代，这一思想在一些近似动态规划工作中曾被提出。
- en: A related and complementary possibility is to somehow construct multiple policies,
    evaluate each of these policies (perhaps approximately, using a neural network),
    and use the policy cost function evaluations as scoring functions. This possibility
    may be particularly interesting in the case of a deterministic discrete optimization
    problem. The reason is that the deterministic character of the problem may obviate
    the need for expensive simulation and neural network training, as we discuss in
    the next section.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相关且互补的可能性是以某种方式构造多个策略，评估这些策略中的每一个（可能使用神经网络进行近似），并将策略成本函数评估用作评分函数。对于确定性离散优化问题，这种可能性可能特别有趣。原因是问题的确定性特征可能消除了对昂贵的模拟和神经网络训练的需求，如我们在下一节中讨论的那样。
- en: 4.4    Using Heuristics to Generate Features - Deterministic Optimization and
    Rollout
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 4.4    使用启发式方法生成特征 - 确定性优化与回滚
- en: An important context where it is natural to use multiple scoring functions is
    general deterministic optimization problems with a finite search space. For such
    problems simple heuristics are often available to obtain suboptimal solutions
    from various starting conditions, e.g., greedy algorithms of various kinds. The
    cost of each heuristic can then be used as a scoring function after the problem
    is converted to a finite horizon DP problem. The formulation that we will use
    in this section is very general and for this reason the number of states of the
    DP problem may be very large. Alternative DP reformulations with fewer states
    may be obtained by exploiting the structure of the problem. For example shortest
    path-type problems and discrete-time finite-state deterministic optimal control
    control problems can be naturally posed as DP problems with a simpler and more
    economical formulation than the one given here. In such cases the methodology
    to be described can be suitably adapted to exploit the problem-specific structural
    characteristics.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个评分函数的一个重要背景是有限搜索空间的一般确定性优化问题。对于这些问题，通常可以使用简单的启发式方法从各种初始条件中获得次优解，例如各种贪婪算法。在将问题转换为有限视界的动态规划问题后，可以将每个启发式方法的成本作为评分函数。我们在本节中使用的形式非常通用，因此动态规划问题的状态数量可能非常大。通过利用问题的结构，可以获得具有较少状态的替代动态规划重构。例如，最短路径类型问题和离散时间有限状态确定性最优控制问题可以自然地表述为具有比这里给出的更简单、更经济的动态规划问题。在这种情况下，将描述的方法可以适当地调整以利用问题特定的结构特征。
- en: The general discrete optimization problem that we consider in this section is
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中考虑的一般离散优化问题是
- en: '|  | $\eqalign{&amp;\hbox{minimize\ \ }G(u)\cr&amp;\hbox{subject to\ \ }u\in
    U,\cr}$ |  | (4.30) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eqalign{&amp;\hbox{最小化\ \ }G(u)\cr&amp;\hbox{约束条件\ \ }u\in U,\cr}$ |  |
    (4.30) |'
- en: where $U$ is a finite set of feasible solutions and $G(u)$ is a cost function.
    We assume that each solution $u$ has $N$ components; i.e., it has the form $u=(u_{1},\ldots,u_{N})$,
    where $N$ is a positive integer. We can then view the problem as a sequential
    decision problem, where the components $u_{1},\ldots,u_{N}$ are selected one-at-a-time.
    An $m$-tuple $(u_{1},\ldots,u_{m})$ consisting of the first $m$ components of
    a solution is called an $m$-solution. We associate $m$-solutions with the $m$th
    stage of a finite horizon DP problem.^†^†† Our aggregation framework of Section
    4.1 extends in a straightforward manner to finite-state finite-horizon problems.
    The main difference is that optimal cost functions, feature vectors, and scoring
    functions are not only state-dependent but also stage-dependent. In effect the
    states are the $m$-solutions for all values of $m$. In particular, for $m=1,\ldots,N$,
    the states of the $m$th stage are of the form $(u_{1},\ldots,u_{m})$. The initial
    state is a dummy (artificial) state. From this state we may move to any state
    $(u_{1})$, with $u_{1}$ belonging to the set
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U$ 是一个有限的可行解集，$G(u)$ 是一个成本函数。我们假设每个解 $u$ 有 $N$ 个分量；即，它的形式为 $u=(u_{1},\ldots,u_{N})$，其中
    $N$ 是一个正整数。我们可以将问题视为一个序贯决策问题，其中分量 $u_{1},\ldots,u_{N}$ 是逐一选择的。由前 $m$ 个分量组成的 $m$-元组
    $(u_{1},\ldots,u_{m})$ 被称为 $m$-解。我们将 $m$-解与有限时域动态规划问题的第 $m$ 阶段关联起来。^†^†† 我们在第
    4.1 节的聚合框架可以直接扩展到有限状态有限时域问题。主要的不同之处在于，最优成本函数、特征向量和评分函数不仅依赖于状态，还依赖于阶段。实际上，状态是所有
    $m$ 值的 $m$-解。特别地，对于 $m=1,\ldots,N$，第 $m$ 阶段的状态形式为 $(u_{1},\ldots,u_{m})$。初始状态是一个虚拟（人工）状态。从这个状态我们可以转移到任何状态
    $(u_{1})$，其中 $u_{1}$ 属于该集合
- en: '|  | $U_{1}=\bigl{\{}\tilde{u}_{1}\mid\hbox{there exists a solution of the
    form }(\tilde{u}_{1},\tilde{u}_{2},\ldots,\tilde{u}_{N})\in U\bigr{\}}.$ |  |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{1}=\bigl{\{}\tilde{u}_{1}\mid\hbox{存在形如 }(\tilde{u}_{1},\tilde{u}_{2},\ldots,\tilde{u}_{N})\in
    U\text{ 的解}\bigr{\}}.$ |  |'
- en: Thus $U_{1}$ is the set of choices of $u_{1}$ that are consistent with feasibility.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 $U_{1}$ 是与可行性一致的 $u_{1}$ 的选择集。
- en: More generally, from a state $(u_{1},\ldots,u_{m}),$ we may move to any state
    of the form $(u_{1},\ldots,u_{m},u_{m+1}),$ with $u_{m+1}$ belonging to the set
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，从状态 $(u_{1},\ldots,u_{m})$，我们可以转移到任何形式为 $(u_{1},\ldots,u_{m},u_{m+1})$
    的状态，其中 $u_{m+1}$ 属于该集合
- en: '|  | ${U_{m+1}(u_{1},\ldots,u_{m})=\big{\{}\tilde{u}_{m+1}\mid\ \hbox{there
    exists a solution of the form }(u_{1},\ldots,u_{m},\tilde{u}_{m+1},\ldots,\tilde{u}_{N})\in
    U\big{\}}.}$ |  | (4.31) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '|  | ${U_{m+1}(u_{1},\ldots,u_{m})=\big{\{}\tilde{u}_{m+1}\mid\ \hbox{存在形如
    }(u_{1},\ldots,u_{m},\tilde{u}_{m+1},\ldots,\tilde{u}_{N})\in U\text{ 的解}\big{\}}.}$
    |  | (4.31) |'
- en: The choices available at state $(u_{1},\ldots,u_{m})$ are $u_{m+1}\in U_{{m+1}}(u_{1},\ldots,u_{m})$.
    These are the choices of $u_{m+1}$ that are consistent with the preceding choices
    $u_{1},\ldots,u_{m}$, and are also consistent with feasibility. The terminal states
    correspond to the $N$-solutions $u=(u_{1},\ldots,u_{N})$, and the only nonzero
    cost is the terminal cost $G(u)$. This terminal cost is incurred upon transition
    from $u$ to an artificial termination state; see Fig. 4.7.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态 $(u_{1},\ldots,u_{m})$ 下可用的选择是 $u_{m+1}\in U_{{m+1}}(u_{1},\ldots,u_{m})$。这些是与前面的选择
    $u_{1},\ldots,u_{m}$ 一致的 $u_{m+1}$ 的选择，并且也符合可行性。终态对应于 $N$-解 $u=(u_{1},\ldots,u_{N})$，且唯一的非零成本是终端成本
    $G(u)$。这一终端成本在从 $u$ 过渡到人工终止状态时产生；见图 4.7。
- en: '![[Uncaptioned image]](img/ec2cd8fa382325c9bab5e133e9f78152.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/ec2cd8fa382325c9bab5e133e9f78152.png)'
- en: Figure 4.7.  Formulation of a discrete optimization problem as a DP problem.
    There is a cost $G(u)$ only at the terminal stage on the arc connecting an $N$-solution
    $u=(u_{1},\ldots,u_{N})$ to the artificial terminal state. Alternative formulations
    may use fewer states by taking advantage of the problem’s structure.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7. 将离散优化问题表述为动态规划问题。只有在连接 $N$-解 $u=(u_{1},\ldots,u_{N})$ 与人工终止状态的弧的终端阶段才有成本
    $G(u)$。替代的表述可以利用问题的结构，使用更少的状态。
- en: Let $J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$ denote the optimal
    cost starting from the $m$-solution $(u_{1},\ldots,u_{m})$, i.e., the optimal
    cost of the problem over solutions whose first $m$ components are constrained
    to be equal to $u_{i}$, $i=1,\ldots,m$, respectively. If we knew the optimal cost-to-go
    functions $J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$, we could construct
    an optimal solution by a sequence of $N$ single component minimizations. In particular,
    an optimal solution $(u_{1}^{*},\ldots,u_{N}^{*})$ could be obtained sequentially,
    starting with $u_{1}^{*}$ and proceeding forward to $u_{N}^{*}$, through the algorithm
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 令$J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$表示从$m$-解$(u_{1},\ldots,u_{m})$开始的最优成本，即问题的最优成本，约束条件是前$m$个分量必须等于$u_{i}$，$i=1,\ldots,m$。如果我们知道最优的前瞻成本函数$J^{\raise
    0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$，我们可以通过一系列$N$次单组件最小化来构建最优解。特别地，可以通过算法从$u_{1}^{*}$开始，逐步前进到$u_{N}^{*}$，获得最优解$(u_{1}^{*},\ldots,u_{N}^{*})$。
- en: '|  | $u_{m+1}^{*}\in\arg\min_{u_{m+1}\in U_{m+1}(u_{1}^{*},\ldots,u_{m}^{*})}J^{\raise
    0.04pt\hbox{\sevenrm*}}(u_{1}^{*},\ldots,u_{m}^{*},u_{m+1}),\qquad m=0,\ldots,N-1.$
    |  |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  | $u_{m+1}^{*}\in\arg\min_{u_{m+1}\in U_{m+1}(u_{1}^{*},\ldots,u_{m}^{*})}J^{\raise
    0.04pt\hbox{\sevenrm*}}(u_{1}^{*},\ldots,u_{m}^{*},u_{m+1}),\qquad m=0,\ldots,N-1.$
    |  |'
- en: Unfortunately, this is seldom viable, because of the prohibitive computation
    required to obtain the functions $J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这通常不可行，因为获得函数$J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$所需的计算成本过于高昂。
- en: Suppose that we have $s$ different heuristic algorithms, which we can apply
    for suboptimal solution. We assume that each of these algorithms can start from
    any $m$-solution $(u_{1},\ldots,u_{m})$ and produce an $N$-solution $(u_{1},\ldots,u_{m},u_{m+1},\ldots,u_{N})$.
    The costs thus generated by the $s$ heuristic algorithms are denoted by $V_{1}(u_{1},\ldots,u_{m}),\ldots,V_{s}(u_{1},\ldots,u_{m}),$
    respectively, and the corresponding vector of heuristic costs is denoted by
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有$s$种不同的启发式算法，我们可以将其应用于次优解。我们假设这些算法中的每一个都可以从任何$m$-解$(u_{1},\ldots,u_{m})$开始，生成一个$N$-解$(u_{1},\ldots,u_{m},u_{m+1},\ldots,u_{N})$。由$s$种启发式算法生成的成本分别记为$V_{1}(u_{1},\ldots,u_{m}),\ldots,V_{s}(u_{1},\ldots,u_{m})$，相应的启发式成本向量记为
- en: '|  | $V(u_{1},\ldots,u_{m})=\big{(}V_{1}(u_{1},\ldots,u_{m}),\ldots,V_{s}(u_{1},\ldots,u_{m})\big{)}.$
    |  |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(u_{1},\ldots,u_{m})=\big{(}V_{1}(u_{1},\ldots,u_{m}),\ldots,V_{s}(u_{1},\ldots,u_{m})\big{)}.$
    |  |'
- en: Note that the heuristic algorithms can be quite sophisticated, and at a given
    partial solution $(u_{1},\ldots,u_{m})$, may involve multiple component choices
    from $(u_{m+1},\ldots,u_{N})$ and/or suboptimizations that may depend on the previous
    choices $u_{1},\ldots,u_{m}$ in complicated ways. In fact, the heuristic algorithms
    may require some preliminary experimentation and training, using for example,
    among others, neural networks.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，启发式算法可能非常复杂，在给定的部分解$(u_{1},\ldots,u_{m})$中，可能涉及从$(u_{m+1},\ldots,u_{N})$中选择多个组件和/或依赖于之前选择$u_{1},\ldots,u_{m}$的复杂子优化。实际上，启发式算法可能需要一些初步实验和训练，例如使用神经网络等。
- en: The main idea now is to use the heuristic cost functions as scoring functions
    to construct a feature-based hard aggregation framework.^†^†† There are several
    variants of this scheme, involving for example a state space partition as in Section
    4.3\. Moreover, the method of partitioning the decision vector $u$ into its components
    $u_{1},\ldots,u_{N}$ may be critically important in specific applications. In
    particular, for each $m=1,\ldots,N-1$, we partition the set of possible values
    of $V(u_{1},\ldots,u_{m})$ into $q$ disjoint subsets $R_{1}^{m},\ldots,R_{q}^{m}$,
    we define a feature vector $F(u_{1},\ldots,u_{m})$ according to
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想是使用启发式成本函数作为评分函数来构建基于特征的硬聚合框架。^†^†† 该方案有几种变体，例如涉及第4.3节中的状态空间划分。此外，将决策向量$u$划分为其组成部分$u_{1},\ldots,u_{N}$的方法在特定应用中可能至关重要。特别地，对于每个$m=1,\ldots,N-1$，我们将$V(u_{1},\ldots,u_{m})$的可能值集合划分为$q$个不相交的子集$R_{1}^{m},\ldots,R_{q}^{m}$，并根据
- en: '|  | $F(u_{1},\ldots,u_{m})=\ell,\qquad\forall\ (u_{1},\ldots,u_{m})\hbox{
    such that }V(u_{1},\ldots,u_{m})\in R_{\ell}^{m},\ \ell=1,\ldots,q,$ |  | (4.32)
    |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(u_{1},\ldots,u_{m})=\ell,\qquad\forall\ (u_{1},\ldots,u_{m})\hbox{
    such that }V(u_{1},\ldots,u_{m})\in R_{\ell}^{m},\ \ell=1,\ldots,q,$ |  | (4.32)
    |'
- en: and we construct a hard aggregation scheme with disaggregation sets for each
    $m=1,\ldots,N-1$, given by
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个针对每个$m=1,\ldots,N-1$的硬聚合方案，给出
- en: '|  | $I_{\ell}^{m}=\big{\{}(u_{1},\ldots,u_{m})\mid V(u_{1},\ldots,u_{m})\in
    R_{\ell}^{m}\big{\}},\qquad\ell=1,\ldots,q.$ |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{\ell}^{m}=\big{\{}(u_{1},\ldots,u_{m})\mid V(u_{1},\ldots,u_{m})\in
    R_{\ell}^{m}\big{\}},\qquad\ell=1,\ldots,q.$ |  |'
- en: Note that the number of aggregate states is roughly similar for each of the
    $N-1$ stages. By contrast the number of states of the original problem may increase
    very fast (exponentially) as $N$ increases; cf. Fig. 4.7.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个$N-1$阶段的聚合状态数量大致相似。相比之下，原始问题的状态数量可能随着$N$的增加而非常快速地增加（指数级增长）；参见图4.7。
- en: The aggregation scheme is illustrated in Fig. 4.8\. It involves $N-1$ successive
    transitions between $m$-solutions to $(m+1)$-solutions ($m=1,\ldots,N-1$), interleaved
    with transitions to and from the corresponding aggregate states. The aggregate
    problem is completely defined once the aggregate states and the disaggregation
    probabilities have been chosen. The transition mechanism of stage $m$ involves
    the following steps.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合方案如图4.8所示。它涉及$N-1$次连续过渡，从$m$-解到$(m+1)$-解（$m=1,\ldots,N-1$），并交替进行到对应的聚合状态。聚合问题在选择了聚合状态和解聚概率后完全定义。阶段$m$的过渡机制涉及以下步骤。
- en: (1) From an aggregate state $\ell$ at stage $m$, we generate some state $(u_{1},\ldots,u_{m})\in
    I_{\ell}^{m}$ according to the disaggregation probabilities.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: （1）从阶段$m$的聚合状态$\ell$，我们根据解聚概率生成一些状态$(u_{1},\ldots,u_{m})\in I_{\ell}^{m}$。
- en: (2) We transition to the next state $(u_{1},\ldots,u_{m},u_{m+1})$ by selecting
    the control $u_{m+1}$.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: （2）通过选择控制$u_{m+1}$，我们过渡到下一个状态$(u_{1},\ldots,u_{m},u_{m+1})$。
- en: (3) We run the $s$ heuristics from the $(m+1)$-solution $(u_{1},\ldots,u_{m},u_{m+1})$
    to determine the next aggregate state, which is the index of the set of the partition
    of stage $m+1$ to which the vector
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: （3）我们运行$s$启发式方法，从$(m+1)$-解$(u_{1},\ldots,u_{m},u_{m+1})$中确定下一个聚合状态，即阶段$m+1$的划分集合的索引，该向量
- en: '|  | $V(u_{1},\ldots,u_{m},u_{m+1})=\big{(}V_{1}(u_{1},\ldots,u_{m},u_{m+1}),\ldots,V_{s}(u_{1},\ldots,u_{m},u_{m+1})\big{)}$
    |  |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(u_{1},\ldots,u_{m},u_{m+1})=\big{(}V_{1}(u_{1},\ldots,u_{m},u_{m+1}),\ldots,V_{s}(u_{1},\ldots,u_{m},u_{m+1})\big{)}$
    |  |'
- en: belongs.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 所属的。
- en: A key issue is the selection of the disaggregation probabilities for each stage.
    This requires, for each value of $m$, the construction of a suitable sample of
    $m$-solutions, where the disaggregation sets $I_{\ell}^{m}$ are adequately represented.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键问题是每个阶段的解聚概率选择。这需要对每个$m$值构造一个合适的$m$-解样本，其中解聚集合$I_{\ell}^{m}$得到充分表示。
- en: '![[Uncaptioned image]](img/9ae6414fe3ee8360f6582727a28651a6.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/9ae6414fe3ee8360f6582727a28651a6.png)'
- en: Figure 4.8.  Schematic illustration of the heuristics-based aggregation scheme
    for discrete optimization. The aggregate states are defined by the scoring functions/heuristics,
    and the optimal aggregate costs are obtained by DP stating from the last stage
    and proceeding backwards.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8。基于启发式的离散优化聚合方案的示意图。聚合状态由评分函数/启发式方法定义，最优的聚合成本通过从最后一个阶段开始的动态规划获得，然后向后推导。
- en: The solution of the aggregate problem by DP starts at the last stage to compute
    the corresponding aggregate costs $r^{*}_{\ell(N-1)}$ for each of the aggregate
    states $\ell$, using $G(u)$ as terminal cost function. Then it proceeds with the
    next-to-last stage to compute the corresponding aggregate costs $r^{*}_{\ell(N-2)}$,
    using the previously computed aggregate costs $r^{*}_{\ell(N-1)}$, etc.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态规划求解聚合问题从最后一个阶段开始，以计算每个聚合状态$\ell$的相应聚合成本$r^{*}_{\ell(N-1)}$，使用$G(u)$作为终端成本函数。然后，它继续进行到倒数第二个阶段，以计算相应的聚合成本$r^{*}_{\ell(N-2)}$，使用先前计算的聚合成本$r^{*}_{\ell(N-1)}$，依此类推。
- en: The optimal cost function $J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$
    for stage $m$ is approximated by a piecewise constant function, which is derived
    by solving the aggregate problem. This is the function
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段$m$的最优成本函数$J^{\raise 0.04pt\hbox{\sevenrm*}}(u_{1},\ldots,u_{m})$被近似为一个分段常数函数，该函数通过解决聚合问题获得。这是该函数
- en: '|  | $\tilde{J}(u_{1},\ldots,u_{m})=r^{*}_{\ell m},\qquad\forall\ (u_{1},\ldots,u_{m})\hbox{
    with }V(u_{1},\ldots,u_{m})\in R_{\ell}^{m},$ |  | (4.33) |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}(u_{1},\ldots,u_{m})=r^{*}_{\ell m},\qquad\forall\ (u_{1},\ldots,u_{m})\hbox{
    with }V(u_{1},\ldots,u_{m})\in R_{\ell}^{m},$ |  | (4.33) |'
- en: where $r^{*}_{\ell m}$ is the optimal cost of aggregate state $\ell$ at stage
    $m$ of the aggregate problem.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$r^{*}_{\ell m}$是聚合问题中阶段$m$的聚合状态$\ell$的最优成本。
- en: '![[Uncaptioned image]](img/0341363c4d23a425baf7522072108a76.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图像]](img/0341363c4d23a425baf7522072108a76.png)'
- en: Figure 4.9.  Sequential construction of a suboptimal $N$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$
    for the original problem, after the aggregate problem has been solved. Given the
    $m$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$, we run the $s$ heuristics
    from each of the candidate $(m+1)$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$,
    and compute the aggregate state and aggregate cost of this candidate $(m+1)$-solution.
    We then select as $\tilde{u}_{m+1}$ the one that corresponds to the candidate
    $(m+1)$-solution with minimal aggregate cost.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9. 在解决了聚合问题之后，为原始问题顺序构造一个次优的$N$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{N})$。给定$m$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m})$，我们从每个候选$(m+1)$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$运行$s$个启发式算法，并计算该候选$(m+1)$-解的聚合状态和聚合成本。然后，我们选择$\tilde{u}_{m+1}$，即对应于具有最小聚合成本的候选$(m+1)$-解。
- en: Once the aggregate problem has been solved for the costs $r^{*}_{\ell m}$, a
    suboptimal $N$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$ for the original
    problem is obtained sequentially, starting from stage 1 and proceeding to stage
    $N$, through the minimizations
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为成本$r^{*}_{\ell m}$解决了聚合问题，就可以顺序地得到原始问题的次优$N$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{N})$，从阶段1开始并进行到阶段$N$，通过最小化。
- en: '|  | $\tilde{u}_{1}\in\arg\min_{u_{1}}\tilde{J}(u_{1}),$ |  | (4.34) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{u}_{1}\in\arg\min_{u_{1}}\tilde{J}(u_{1}),$ |  | (4.34) |'
- en: '|  | $\tilde{u}_{m+1}\in\arg\min_{u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\quad
    m=1,\ldots,N-1.$ |  | (4.35) |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{u}_{m+1}\in\arg\min_{u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\quad
    m=1,\ldots,N-1.$ |  | (4.35) |'
- en: 'Note that to evaluate each of the costs $\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$
    needed for this minimization, we need to do the following (see Fig. 4.9):'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了评估每个所需的成本$\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$以进行这次最小化，我们需要执行以下操作（见图4.9）：
- en: (1) Run the $s$ heuristics from the $(m+1)$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$
    to evaluate the scoring vector of heuristic costs
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 从$(m+1)$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$运行$s$个启发式算法，以评估启发式成本的评分向量。
- en: '|  | $V(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})=\big{(}V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\big{)}.$
    |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})=\big{(}V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\big{)}.$
    |  |'
- en: (2) Set $\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$ to the aggregate
    cost $r^{*}_{\ell(m+1)}$ of the aggregate state $S_{\ell(m+1)}$ corresponding
    to this scoring vector, i.e., to the set $R_{\ell}^{(m+1)}$ such that
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 将$\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$设置为与此评分向量对应的聚合状态$S_{\ell(m+1)}$的聚合成本$r^{*}_{\ell(m+1)}$，即集合$R_{\ell}^{(m+1)}$，满足
- en: '|  | $V(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\in R_{\ell}^{(m+1)}.$ |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\in R_{\ell}^{(m+1)}.$ |  |'
- en: Once $\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$ has been computed
    for all ${u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}$, we select
    $\tilde{u}_{m+1}$ via the minimization (4.35), and repeat starting from the $(m+1)$-solution
    $(\tilde{u}_{1},\ldots,\tilde{u}_{m},\tilde{u}_{m+1})$. Note that even if there
    is only one heuristic, $\tilde{u}_{m+1}$ minimizes the aggregate cost $r^{*}_{\ell(m+1)}$,
    which is not the same as the cost corresponding to the heuristic.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为所有${u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}$计算了$\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$，我们通过最小化(4.35)选择$\tilde{u}_{m+1}$，并从$(m+1)$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m},\tilde{u}_{m+1})$开始重复。请注意，即使只有一个启发式算法，$\tilde{u}_{m+1}$也会最小化聚合成本$r^{*}_{\ell(m+1)}$，这与对应于启发式算法的成本不同。
- en: We finally mention a simple improvement of the scheme just described for constructing
    an $N$-solution. In the course of the algorithm many other $N$-solutions are obtained,
    during the training and final solution selection processes. It is possible that
    some of these solutions are actually better [have lower cost $G(u)$] than the
    final $N$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$ that is constructed
    by using the aggregate problem formulation. This can happen because the aggregation
    scheme is subject to quantization error. Thus it makes sense to maintain the best
    of the $N$-solutions generated in the course of the algorithm, and compare it
    at the end with the $N$-solution obtained through the aggregation scheme. This
    is similar to the so-called “fortified” version of the rollout algorithm (see
    [BTW97] or [Ber17]).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提到一个关于构造$N$-解的方案的简单改进。在算法过程中，许多其他的$N$-解会在训练和最终解选择过程中获得。可能这些解中的一些实际上比通过使用聚合问题公式构造的最终$N$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{N})$更好[具有更低的成本$G(u)$]。这可能是因为聚合方案会受到量化误差的影响。因此，保持算法过程中生成的最佳$N$-解，并在最后与通过聚合方案获得的$N$-解进行比较是有意义的。这类似于所谓的“强化”版本的展开算法（参见[BTW97]或[Ber17]）。
- en: Relation to the Rollout Algorithm
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 与展开算法的关系
- en: The idea of using one or more heuristic algorithms as a starting point for generating
    an improved solution of a discrete optimization problem is shared by other suboptimal
    control approaches. A prime example is the rollout algorithm, which in some contexts
    can be viewed as a single policy iteration; see [BTW97] for an analysis of rollout
    for discrete optimization problems, and the textbook [Ber17] for an extensive
    discussion and many references to applications, including the important model
    predictive control methodology for control system design.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个或多个启发式算法作为生成改进的离散优化问题解的起点的想法是其他次优控制方法所共有的。一个主要的例子是展开算法，在某些背景下可以被视为单次策略迭代；有关离散优化问题展开的分析，请参见[BTW97]，以及教科书[Ber17]，其中对控制系统设计中重要的模型预测控制方法进行了广泛讨论，并提供了许多应用的参考文献。
- en: Basically the rollout algorithm uses the scheme of Fig. 4.9 to construct a suboptimal
    solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$ in $N$ steps, one component at
    a time, but adds a new decision $\tilde{u}_{m+1}$ to the current $m$-solution
    $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$ in a simpler way. It runs the $s$ heuristics
    from each candidate $(m+1)$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$
    and computes the corresponding heuristic costs
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，展开算法使用图4.9的方案，在$N$步内逐步构造一个次优解$(\tilde{u}_{1},\ldots,\tilde{u}_{N})$，每次添加一个组件，但以更简单的方式将新的决策$\tilde{u}_{m+1}$添加到当前的$m$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m})$中。它从每个候选的$(m+1)$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})$中运行$s$个启发式方法，并计算相应的启发式成本
- en: '|  | $V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}).$
    |  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}).$
    |  |'
- en: It then selects as the next decision $\tilde{u}_{m+1}$ the one that minimizes
    over ${u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}$ the best heuristic
    cost
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它选择作为下一个决策$\tilde{u}_{m+1}$的方案是最小化${u_{m+1}\in U_{m+1}(\tilde{u}_{1},\ldots,\tilde{u}_{m})}$的最佳启发式成本
- en: '|  | $\hat{V}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})=\min\big{\{}V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\big{\}},$
    |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{V}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})=\min\big{\{}V_{1}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1}),\ldots,V_{s}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1})\big{\}},$
    |  |'
- en: i.e., it uses $\hat{V}$ in place of $\tilde{J}$ in Eqs. (4.34)-(4.35). In practice,
    the rollout algorithm’s heuristics may involve sophisticated suboptimizations
    that may make sense in the context of the problem at hand.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 即，它在方程（4.34）-（4.35）中使用$\hat{V}$代替$\tilde{J}$。在实践中，展开算法的启发式方法可能涉及复杂的次优化，这在特定问题的背景下可能是合理的。
- en: Note that the construction of the final $N$-solution is similar and equally
    complicated in the rollout and the scoring vector-based aggregation approach.
    However, the aggregation approach requires an extra layer of computation prior
    to constructing the $N$-solution, namely the solution of an aggregate problem.
    This may be a formidable problem, because it is stochastic (due to the use of
    disaggregation probabilities) and must be solved exactly (at least in principle).
    Still, the number of states of the aggregate problem may be quite reasonable,
    and its solution is well suited for parallel computation.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最终$N$-解的构造在展开和基于评分向量的聚合方法中是类似且同样复杂的。然而，聚合方法在构造$N$-解之前需要额外的一层计算，即解决聚合问题。这可能是一个艰巨的问题，因为它是随机的（由于使用了去聚合概率）并且必须准确解决（至少在理论上）。不过，聚合问题的状态数可能是相当合理的，而且其解决方案非常适合并行计算。
- en: On the other hand, setting aside the issue of computational solution of the
    aggregate problem, the heuristics-based aggregation algorithm has the potential
    of being far superior to the rollout algorithm, for the same reason that approximate
    policy improvement based on aggregation can be far superior to policy improvement
    based on one-step lookahead. In particular, with sufficiently large number of
    aggregate states to eliminate the effects of the quantization error, feature-based
    aggregation will find an optimal solution, regardless of the quality of the heuristics
    used. By contrast, policy iteration and rollout can only aspire to produce a solution
    that is better than the one produced by the heuristics.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，撇开聚合问题的计算解决方案，基于启发式的聚合算法有潜力远远优于展开算法，这和基于聚合的近似策略改进可以远远优于基于一步前瞻的策略改进是同样的原因。特别是，当聚合状态的数量足够大以消除量化误差的影响时，基于特征的聚合将找到一个最优解，无论所用的启发式方法质量如何。相比之下，策略迭代和展开算法只能期望产生比启发式方法更好的解。
- en: Using Multistep Lookahead and Monte Carlo Tree Search
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多步前瞻和蒙特卡罗树搜索
- en: Once the aggregate problem that is based on multiple scoring functions has been
    solved, the final $N$-solution can be constructed in more sophisticated ways than
    the one described in Fig. 4.9\. It can be seen that the scheme of Eqs. (4.34)-(4.35) and
    Fig. 4.9 is based on one-step lookahead. It is possible instead to use multistep
    lookahead or randomized versions such as Monte Carlo tree search.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基于多个评分函数的聚合问题被解决，最终$N$-解可以以比图4.9所描述的更复杂的方式构造。可以看出，方程式(4.34)-(4.35)和图4.9的方案是基于一步前瞻的。可以改用多步前瞻或随机版本，如蒙特卡罗树搜索。
- en: '![[Uncaptioned image]](img/27d935ccbcb84c9ba2d0c362e90b77b6.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图片]](img/27d935ccbcb84c9ba2d0c362e90b77b6.png)'
- en: Figure 4.10.  Sequential construction of a suboptimal $N$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$
    by using two-step lookahead, after the aggregate problem has been solved. Given
    the $m$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$, we run the $s$ heuristics
    from all the candidate $(m+2)$-solutions $(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1},u_{m+2})$,
    and select as $\tilde{u}_{m+1}$ the first component of the two-step sequence that
    corresponds to minimal aggregate cost.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10。 在解决聚合问题后，通过使用两步前瞻 sequential 构造一个次优的$N$-解 $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$。给定$m$-解
    $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$，我们从所有候选的$(m+2)$-解 $(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1},u_{m+2})$中运行$s$个启发式算法，并选择作为$\tilde{u}_{m+1}$的两步序列中对应于最小聚合成本的第一个组件。
- en: As an example, in a two-step lookahead scheme, we again obtain a suboptimal
    solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$ for the original problem in $N$
    stages, starting from stage 1 and proceeding to stage $N$. At stage 1, we carry
    out the two-step minimization
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，在两步前瞻方案中，我们再次为原始问题获得一个次优解 $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$，分为$N$个阶段，从第1阶段开始，直到第$N$阶段。在第1阶段，我们进行两步最小化
- en: '|  | $(\tilde{u}_{1},\tilde{u}_{2})\in\arg\min_{u_{1},u_{2}}\tilde{J}(u_{1},u_{2}),$
    |  | (4.36) |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\tilde{u}_{1},\tilde{u}_{2})\in\arg\min_{u_{1},u_{2}}\tilde{J}(u_{1},u_{2}),$
    |  | (4.36) |'
- en: 'and fix the first component $\tilde{u}_{1}$ of the result, cf. Fig. 4.10\.
    We then proceed sequentially: for $m=1,\ldots,N-2$, given the current $m$-solution
    $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$, we carry out the two-step minimization'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 并固定结果的第一个组件$\tilde{u}_{1}$，参见图4.10。然后我们按顺序进行：对于$m=1,\ldots,N-2$，给定当前$m$-解 $(\tilde{u}_{1},\ldots,\tilde{u}_{m})$，我们进行两步最小化
- en: '|  | $(\tilde{u}_{m+1},\tilde{u}_{m+2})\in\arg\min_{u_{m+1},u_{m+2}}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1},u_{m+2}),\quad
    m=1,\ldots,N-2,$ |  | (4.37) |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\tilde{u}_{m+1},\tilde{u}_{m+2})\in\arg\min_{u_{m+1},u_{m+2}}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{m},u_{m+1},u_{m+2}),\quad
    m=1,\ldots,N-2,$ |  | (4.37) |'
- en: and fix the first component $\tilde{u}_{m+1}$ of the result, cf. Fig. 4.10\.
    At the final stage, given the $(N-1)$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N-1})$,
    we carry out the one-step minimization
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 并固定结果的第一个组件$\tilde{u}_{m+1}$，参见图4.10。在最终阶段，给定$(N-1)$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{N-1})$，我们进行一步最小化
- en: '|  | $\tilde{u}_{N}\in\arg\min_{u_{N}}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{N-1},u_{N}),$
    |  | (4.38) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{u}_{N}\in\arg\min_{u_{N}}\tilde{J}(\tilde{u}_{1},\ldots,\tilde{u}_{N-1},u_{N}),$
    |  | (4.38) |'
- en: and obtain the final $N$-solution $(\tilde{u}_{1},\ldots,\tilde{u}_{N})$.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 并获得最终的$N$-解$(\tilde{u}_{1},\ldots,\tilde{u}_{N})$。
- en: Multistep lookahead generates a tree of fixed depth that is rooted at the last
    node $\tilde{u}_{m}$ of the current $m$-solution, and then runs the heuristics
    from each of the leaf nodes of the tree. We can instead select only a subset of
    these leaf nodes from which to run the heuristics, thereby economizing on computation.
    The selection may be based on some heuristic criterion. Monte Carlo tree search
    similarly uses multistep lookahead but selects only a random sample of leaf nodes
    to search based on some criterion.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 多步前瞻生成了一个固定深度的树，该树以当前$m$-解的最后一个节点$\tilde{u}_{m}$为根，然后从树的每个叶节点运行启发式算法。我们可以仅从这些叶节点中选择一个子集来运行启发式算法，从而节省计算。选择可能基于某些启发式标准。蒙特卡罗树搜索类似地使用多步前瞻，但根据某些标准仅选择随机样本的叶节点进行搜索。
- en: In a more general version of Monte Carlo tree search, instead of a single partial
    solution, we maintain multiple partial solutions, possibly of varying length.
    At each step, a one-step or multistep lookahead tree is generated from the most
    “promising” of the current partial solutions, selected by using a randomization
    mechanism. The heuristics are run from the leafs of the lookahead trees similar
    to Fig. 4.10\. Then some of the current partial solutions are expanded with an
    additional component based on the results produced by the heuristics. This type
    of Monte Carlo tree search has been suggested for use in conjunction with rollout
    (see the paper [RSS12]), and it can be similarly used with feature-based aggregation.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在蒙特卡罗树搜索的更一般版本中，我们维持多个部分解，可能长度不同。在每一步中，从当前最“有前景”的部分解中生成一个一步或多步前瞻树，该部分解通过随机化机制进行选择。启发式算法从前瞻树的叶节点中运行，类似于图4.10。然后，根据启发式算法产生的结果，扩展当前的一些部分解。已建议将这种类型的蒙特卡罗树搜索与回滚一起使用（见论文[RSS12]），并且它也可以与基于特征的聚合类似使用。
- en: 4.5    Stochastic Shortest Path Problems - Illustrative Examples
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 4.5    随机最短路径问题 - 说明性示例
- en: Our aggregation framework extends straightforwardly to stochastic shortest path
    (SSP for short) problems, where there is no discounting and in addition to the
    states $1,\ldots,n$, there is an additional cost-free and absorbing termination
    state, denoted $0$ (the text references given earlier discuss in detail such problems).
    The principal change needed is to account for the termination state by introducing
    an additional aggregate state with corresponding disaggregation set $\{0\}$. As
    before there are also other aggregate states $S_{1},\ldots,S_{q}$ whose disaggregation
    sets $I_{1},\ldots,I_{q}$ are subsets of $\{1,\ldots,n\}$. With this special handling
    of the termination state, the aggregate problem becomes a standard SSP problem
    whose termination state is the aggregate state corresponding to $0$. The Bellman
    equation of the aggregate problem is given by
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的聚合框架可以直接扩展到随机最短路径（简称SSP）问题，其中没有折扣，并且除了状态$1,\ldots,n$之外，还有一个额外的无成本且吸收的终止状态，记为$0$（前文提到的文本详细讨论了此类问题）。主要的变化是通过引入一个额外的聚合状态及其相应的分解集合$\{0\}$来考虑终止状态。如之前所述，还有其他聚合状态$S_{1},\ldots,S_{q}$，其分解集合$I_{1},\ldots,I_{q}$是$\{1,\ldots,n\}$的子集。通过这种特殊处理终止状态，聚合问题变成了一个标准的SSP问题，其终止状态是对应于$0$的聚合状态。聚合问题的贝尔曼方程为
- en: '|  | $r_{\ell}=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q,$
    |  | (4.39) |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{\ell}=\sum_{i=1}^{n}d_{\ell i}\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\sum_{m=1}^{q}\phi_{jm}\,r_{m}\right),\qquad\ell=1,\ldots,q,$
    |  | (4.39) |'
- en: '[cf. Eq. (4.12)]. It has a unique solution under some well-known conditions
    that date to the paper by Bertsekas and Tsitsiklis [BeT91] (there exists at least
    one proper policy, i.e., a stationary policy that guarantees eventual termination
    from each initial state with probability 1; moreover all stationary policies that
    are not proper have infinite cost starting from some initial state). In particular,
    these conditions are satisfied if all stationary policies are proper.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[参见 Eq. (4.12)]。在一些著名的条件下，它有一个唯一的解，这些条件可以追溯到 Bertsekas 和 Tsitsiklis 的论文 [BeT91]（至少存在一个适当的策略，即一个从每个初始状态以概率
    1 保证最终终止的平稳策略；此外，所有不适当的平稳策略在某些初始状态下具有无限成本）。特别地，如果所有平稳策略都是适当的，那么这些条件就会得到满足。'
- en: We will now provide two simple illustrative SSP examples, which were presented
    in the author’s paper [Ber95] as instances of poor performance of TD($\lambda$)
    and other methods that are based on projected equations and temporal differences
    (see also the book [BeT96], Section 6.3.2). In these examples the cost function
    of a policy will be approximated by using feature-based aggregation and a scoring
    function obtained using either the TD(1) or the TD(0) algorithms. The approximate
    cost function computed by aggregation will be compared with the results of TD(1)
    and TD(0). We will show that aggregation provides a much better approximation,
    suggesting better policy improvement results in a PI context.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将提供两个简单的 SSP 示例，这些示例在作者的论文 [Ber95] 中作为 TD($\lambda$) 和其他基于投影方程和时间差的方法的表现不佳的实例。在这些示例中，策略的成本函数将通过使用特征基础聚合和使用
    TD(1) 或 TD(0) 算法获得的评分函数来进行近似。通过聚合计算的近似成本函数将与 TD(1) 和 TD(0) 的结果进行比较。我们将展示聚合提供了更好的近似，从而在
    PI 上下文中建议更好的策略改进结果。
- en: Our examples involve a problem with a single policy $\mu$ where the corresponding
    Markov chain is deterministic with $n$ states plus a termination state $0$. Under
    $\mu$, when at state $i=1,\ldots,n$, we move to state $i-1$ at a cost $g_{i}$.
    Thus starting at state $i$ we traverse each of the states $i-1,\ldots,1$ and terminate
    at state 0 at costs $g_{i},g_{i-1},\ldots,g_{1}$, respectively, while accumulating
    the total cost
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例涉及一个单一策略 $\mu$ 的问题，其中对应的马尔可夫链是具有 $n$ 个状态加一个终止状态 0 的确定性链。在 $\mu$ 下，当在状态
    $i=1,\ldots,n$ 时，我们以成本 $g_{i}$ 移动到状态 $i-1$。因此，从状态 $i$ 开始，我们遍历每个状态 $i-1,\ldots,1$
    并在状态 0 终止，成本分别为 $g_{i},g_{i-1},\ldots,g_{1}$，同时累计总成本
- en: '|  | $J_{\mu}(i)=g_{i}+\cdots+g_{1},\qquad i=1,\ldots,n,$ |  |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '|  | $J_{\mu}(i)=g_{i}+\cdots+g_{1},\qquad i=1,\ldots,n,$ |  |'
- en: 'with $J_{\mu}(0)=0$. We consider a linear approximation to this cost function,
    which we denote by $V$:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 并且 $J_{\mu}(0)=0$。我们考虑对这个成本函数的线性近似，我们用 $V$ 表示：
- en: '|  | $V(i)=ri,\qquad i=1,\ldots,n,$ |  |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '|  | $V(i)=ri,\qquad i=1,\ldots,n,$ |  |'
- en: where $r$ is a scalar parameter. This parameter may be obtained by using any
    of the simulation-based methods that are available for training linear architectures,
    including TD($\lambda$). In the subsequent discussion we will assume that TD($\lambda$)
    is applied in an idealized form where the simulation samples contain no noise.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r$ 是一个标量参数。这个参数可以通过使用任何可用于训练线性架构的基于仿真的方法获得，包括 TD($\lambda$)。在随后的讨论中，我们将假设
    TD($\lambda$) 以理想化形式应用，其中仿真样本不含噪声。
- en: The TD(1) algorithm is based on minimizing the sum of the squares of the differences
    between $J_{\mu}$ and $V$ over all states, yielding the approximation
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: TD(1) 算法基于最小化所有状态中 $J_{\mu}$ 和 $V$ 之间差异的平方和，得到近似值
- en: '|  | $\hat{V}_{1}(i)=\hat{r}_{1}i,\qquad i=0,1,\ldots,n,$ |  |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{V}_{1}(i)=\hat{r}_{1}i,\qquad i=0,1,\ldots,n,$ |  |'
- en: where
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\hat{r}_{1}\in\arg\min_{r\in\Re}\sum_{i=1}^{n}\big{(}J_{\mu}(i)-ri\big{)}^{2}.$
    |  | (4.40) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}_{1}\in\arg\min_{r\in\Re}\sum_{i=1}^{n}\big{(}J_{\mu}(i)-ri\big{)}^{2}.$
    |  | (4.40) |'
- en: Here, consistent with our idealized setting of noise-free simulation, we assume
    that $J_{\mu}(i)$ is computed exactly for all $i$. The TD(0) algorithm is based
    on minimizing the sum of the squares of the errors in satisfying the Bellman equation
    $V(i)=g_{i}+V(i-1)$ (or temporal differences) over all states, yielding the approximation
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与我们理想化的无噪声仿真设置一致，我们假设 $J_{\mu}(i)$ 对所有 $i$ 都是准确计算的。TD(0) 算法基于最小化所有状态中满足
    Bellman 方程 $V(i)=g_{i}+V(i-1)$（或时间差）的误差的平方和，得到近似值
- en: '|  | $\hat{V}_{0}(i)=\hat{r}_{0}i,\qquad i=0,1,\ldots,n,$ |  |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{V}_{0}(i)=\hat{r}_{0}i,\qquad i=0,1,\ldots,n,$ |  |'
- en: where
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\hat{r}_{0}=\in\arg\min_{r\in\Re}\sum_{i=1}^{n}\big{(}g_{i}+r(i-1)-ri\big{)}^{2}.$
    |  | (4.41) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}_{0}=\in\arg\min_{r\in\Re}\sum_{i=1}^{n}\big{(}g_{i}+r(i-1)-ri\big{)}^{2}.$
    |  | (4.41) |'
- en: Again, we assume that the temporal differences $\big{(}g_{i}+r(i-1)-ri\big{)}$
    are computed exactly for all $i$.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 再次假设时间差 $\big{(}g_{i}+r(i-1)-ri\big{)}$ 对于所有 $i$ 都是精确计算的。
- en: The straightforward solution of the minimization problems in Eqs. (4.40) and
    (4.41) yields
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化方程 (4.40) 和 (4.41) 中问题的直接解为
- en: '|  | $\hat{r}_{1}={n(g_{1}+\cdots+g_{n})+(n-1)(g_{1}+\cdots+g_{n-1})+\cdots+g_{1}\over
    n^{2}+(n-1)^{2}+\cdots+1},$ |  |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}_{1}={n(g_{1}+\cdots+g_{n})+(n-1)(g_{1}+\cdots+g_{n-1})+\cdots+g_{1}\over
    n^{2}+(n-1)^{2}+\cdots+1},$ |  |'
- en: and
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | $\hat{r}_{0}={ng_{n}+(n-1)g_{n-1}+\cdots+g_{1}\over n+(n-1)+\cdots+1}.$
    |  |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}_{0}={ng_{n}+(n-1)g_{n-1}+\cdots+g_{1}\over n+(n-1)+\cdots+1}.$
    |  |'
- en: 'Consider now two different choices of the one-stage costs $g_{i}$:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑两种不同的单阶段成本 $g_{i}$ 选择：
- en: (a) $g_{1}=1$, and $g_{i}=0$ for all $i\neq 1$.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $g_{1}=1$，对于所有 $i\neq 1$，$g_{i}=0$。
- en: (b) $g_{n}=-(n-1)$, and $g_{i}=1$ for all $i\neq n$.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $g_{n}=-(n-1)$，对于所有 $i\neq n$，$g_{i}=1$。
- en: Figures 4.11 and 4.12 provide plots of $J_{\mu}(i)$, and the approximations
    $\hat{V}_{1}(i)$ and $\hat{V}_{0}(i)$ for these two cases (these plots come from
    [Ber95] where the number of states used was $n=50$). It can be seen that $\hat{V}_{1}(i)$
    and particularly $\hat{V}_{0}(i)$ are poor approximations of $J_{\mu}(i)$, suggesting
    that if used for policy improvement, they may yield a poor successor policy.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 和 4.12 展示了 $J_{\mu}(i)$ 以及这两个案例中的近似 $\hat{V}_{1}(i)$ 和 $\hat{V}_{0}(i)$（这些图来自
    [Ber95]，其中使用的状态数量为 $n=50$）。可以看出，$\hat{V}_{1}(i)$ 和特别是 $\hat{V}_{0}(i)$ 对 $J_{\mu}(i)$
    的近似效果较差，这表明如果用于策略改进，可能会产生较差的继任策略。
- en: '![[Uncaptioned image]](img/6ae57a391a1ecfe3ded24ec79f29439f.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/6ae57a391a1ecfe3ded24ec79f29439f.png)'
- en: 'Figure 4.11.  Form of $J_{\mu}(i)$ and the linear approximations $\hat{V}_{1}(i)$
    and $\hat{V}_{0}(i)$ for case (a): $g_{1}=1$, and $g_{i}=0$ for all $i=2,\ldots,n$.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11. $J_{\mu}(i)$ 的形式以及案例 (a) 中 $\hat{V}_{1}(i)$ 和 $\hat{V}_{0}(i)$ 的线性近似：$g_{1}=1$，对于所有
    $i=2,\ldots,n$，$g_{i}=0$。
- en: '![[Uncaptioned image]](img/3c36748dc9b8774aeb25fefdb104bee2.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![[无标题图像]](img/3c36748dc9b8774aeb25fefdb104bee2.png)'
- en: 'Figure 4.12.  Form of $J_{\mu}(i)$ and the linear approximations $\hat{V}_{1}(i)$
    and $\hat{V}_{0}(i)$ for case (b): $g_{n}=-(1-n)$, and $g_{i}=1$ for all $i=1,\ldots,n-1$.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12. $J_{\mu}(i)$ 的形式以及案例 (b) 中 $\hat{V}_{1}(i)$ 和 $\hat{V}_{0}(i)$ 的线性近似：$g_{n}=-(1-n)$，对于所有
    $i=1,\ldots,n-1$，$g_{i}=1$。
- en: We will now consider a hard aggregation scheme based on using $\hat{V}_{1}$
    and $\hat{V}_{0}$ as scoring functions. The aggregate states of such a scheme
    in effect consist of disaggregation subsets $I_{1},\ldots,I_{q}$ with $\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$
    plus the subset $\{0\}$ that serves as the termination state of the aggregate
    problem. With either $\hat{V}_{1}$ or $\hat{V}_{0}$ as the scoring function, the
    subsets $I_{1},\ldots,I_{q}$ consist of contiguous states. In order to guarantee
    that the termination state is eventually reached in the aggregate problem, we
    assume that the disaggregation probability of the smallest state within each of
    the subsets $I_{1},\ldots,I_{q}$ is strictly positive; this is a mild restriction,
    which is naturally satisfied in typical schemes that assign equal probability
    to all the states in a disaggregation set.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考虑一个基于使用 $\hat{V}_{1}$ 和 $\hat{V}_{0}$ 作为评分函数的硬聚合方案。这样的方案的聚合状态实际上包括了离散化子集
    $I_{1},\ldots,I_{q}$，其中 $\cup_{\ell=1}^{q}I_{\ell}=\{1,\ldots,n\}$，加上作为聚合问题终止状态的子集
    $\{0\}$。无论是 $\hat{V}_{1}$ 还是 $\hat{V}_{0}$ 作为评分函数，这些子集 $I_{1},\ldots,I_{q}$ 由连续状态组成。为了保证在聚合问题中最终到达终止状态，我们假设每个子集
    $I_{1},\ldots,I_{q}$ 内最小状态的离散化概率是严格正的；这是一个温和的限制，通常在将所有状态分配相等概率的离散化方案中自然满足。
- en: Consider first case (a) (cf. Fig. 4.11). Then, because the policy cost function
    $J_{\mu}$ is constant within each of the subsets $I_{1},\ldots,I_{q}$, the scalar
    $\epsilon$ in Prop. 4.1 is equal to 0, implying that the hard aggregation scheme
    yields the optimal cost function, i.e., $r^{*}_{\ell}=J_{\mu}(i)$ for all $i\in
    I_{\ell}$. To summarize, in case (a) the TD(0) approach yields a very poor linear
    cost function approximation, the TD(1) approach yields a poor linear cost function
    approximation, but the aggregation scheme yields the nonlinear policy cost function
    $J_{\mu}$ exactly.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑案例 (a)（参见图 4.11）。然后，因为策略成本函数 $J_{\mu}$ 在每个子集 $I_{1},\ldots,I_{q}$ 内是常数，Prop.
    4.1 中的标量 $\epsilon$ 等于 0，这意味着硬聚合方案产生了最优成本函数，即 $r^{*}_{\ell}=J_{\mu}(i)$ 对于所有 $i\in
    I_{\ell}$。总而言之，在案例 (a) 中，TD(0) 方法产生了非常差的线性成本函数近似，TD(1) 方法产生了较差的线性成本函数近似，但聚合方案准确地产生了非线性策略成本函数
    $J_{\mu}$。
- en: Consider next case (b) (cf. Fig. 4.12). Then, the hard aggregation scheme yields
    a piecewise constant approximation to the optimal cost function. The quality of
    the approximation is degraded by quantization effects. In particular, as the variations
    of $J_{\mu}$, and $\hat{V}_{1}$ or $\hat{V}_{0}$ increase over the disaggregation
    sets $I_{1},\ldots,I_{q}$, the quality of the approximation deteriorates, as predicted
    by Prop. 4.4\. Similarly, as the number of states in the disaggregation sets $I_{1},\ldots,I_{q}$
    is reduced, the quality of the approximation improves, as illustrated in Fig. 4.13\.
    In the extreme case where there is only one state in each of the disaggregation
    sets, the aggregation scheme yields exactly $J_{\mu}$.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来考虑情况(b)（参见图4.12）。然后，硬聚合方案提供了对最优成本函数的分段常数近似。近似的质量受到量化效应的影响。特别是，当$J_{\mu}$、$\hat{V}_{1}$或$\hat{V}_{0}$在离散化集$I_{1},\ldots,I_{q}$上的变化增加时，近似的质量会恶化，正如Prop.
    4.4所预测的那样。类似地，当离散化集$I_{1},\ldots,I_{q}$中的状态数量减少时，近似质量会改善，如图4.13所示。在极端情况下，每个离散化集只有一个状态时，聚合方案正好给出$J_{\mu}$。
- en: To summarize, in case (b) the TD(0) approach yields a very poor linear cost
    function approximation, the TD(1) approach yields a reasonably good linear cost
    function approximation, while the aggregation scheme yields a piecewise constant
    approximation whose quality depends on the coarseness of the quantization that
    is implicit in the selection of the number $q$ of disaggregation subsets. The
    example of case (b) also illustrates how the quality of the scoring function affects
    the quality of the approximation provided by the aggregation scheme. Here both
    $\hat{V}_{1}$ and $\hat{V}_{0}$ work well as scoring functions, despite their
    very different form, because states with similar values of $J_{\mu}$ also have
    similar values of $\hat{V}_{1}$ as well as $\hat{V}_{0}$ (cf. Prop. 4.4).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在情况(b)中，TD(0)方法产生了非常差的线性成本函数近似，TD(1)方法产生了相当好的线性成本函数近似，而聚合方案产生了一个分段常数近似，其质量取决于量化的粗糙程度，这在选择离散化子集数量$q$时隐含。情况(b)的例子还说明了评分函数的质量如何影响聚合方案提供的近似质量。尽管$\hat{V}_{1}$和$\hat{V}_{0}$形式非常不同，但它们作为评分函数表现良好，因为具有相似$J_{\mu}$值的状态也具有相似的$\hat{V}_{1}$和$\hat{V}_{0}$值（参见Prop.
    4.4）。
- en: '![[Uncaptioned image]](img/9824ac338adf486c29efeac44ad75380.png)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/9824ac338adf486c29efeac44ad75380.png)'
- en: Figure 4.13.  Schematic illustration of the piecewise constant approximation
    of $J_{\mu}$ that is provided by hard aggregation based on the scoring functions
    $\hat{V}_{1}$ and $\hat{V}_{0}$ in case (b).
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13. 硬聚合基于评分函数$\hat{V}_{1}$和$\hat{V}_{0}$提供的$J_{\mu}$的分段常数近似的示意图。
- en: 4.6    Multistep Aggregation
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 4.6 多步聚合
- en: 'The aggregation methodology discussed so far is based on the aggregate problem
    Markov chain of Fig. 4.2, which returns to an aggregate state after a single transition
    of the original chain. We may obtain alternative aggregation frameworks by considering
    a different Markov chain, which starting from an aggregate state, involves multiple
    original system state transitions before return to an aggregate state. We discuss
    two possibilities:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止讨论的聚合方法基于图4.2中的聚合问题马尔可夫链，该链在原始链的单次转移后返回到聚合状态。我们可以通过考虑不同的马尔可夫链来获得替代的聚合框架，该链从一个聚合状态开始，在返回到聚合状态之前涉及多个原始系统状态的转移。我们讨论了两种可能性：
- en: '(a) $k$-Step Aggregation: Here we require a fixed number $k$ of transitions
    between original system states before returning to an aggregate state.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $k$-步聚合：这里我们要求在返回到聚合状态之前原始系统状态之间的固定转移次数$k$。
- en: '(b) $\lambda$-Aggregation: Here the number $k$ of transitions prior to returning
    to an aggregate state is controlled by some randomization mechanism. In the case
    where $k$ is geometrically distributed with parameter $\lambda\in(0,1)$, this
    method involves multistep mappings that arise in temporal difference contexts
    and facilitate the use of temporal differences methodology.'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $\lambda$-聚合：这里返回到聚合状态之前的转移次数$k$由某种随机机制控制。在$k$以参数$\lambda\in(0,1)$呈几何分布的情况下，该方法涉及在时间差分上下文中出现的多步映射，并促进了时间差分方法的使用。
- en: Another related possibility, which we do not discuss in this paper, is to introduce
    temporal abstractions (faster/multistep “macro-actions” and transitions between
    selected states with suitably computed transition costs) into the upper (original
    system) portion of the aggregate problem Markov chain of Fig. 4.2\. There have
    been many proposals of this type in the reinforcement learning literature, under
    various names; for some representative works, see Hauskrecht et al. [HMK98], Sutton,
    Precup, and Singh [SPS99], Parr and Russell [PaR98], Dietterich [Die00], Konidaris
    and Barto [KoB09], Ciosek and Silver [CiS15], Mann, Mannor, and Precup [MMP15],
    Serban et al. [SSP18], and the references cited there. It is likely that some
    of these proposals can be fruitfully adapted to our feature-based aggregation
    context, and this is an interesting subject for further research.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种相关的可能性（我们在本文中不讨论）是将时间抽象（更快的/多步骤“宏动作”和在选定状态之间的过渡，带有适当计算的过渡成本）引入图 4.2 的聚合问题
    Markov 链的上层（原始系统）部分。这类提议在强化学习文献中已有很多，以各种名称出现；代表性工作包括 Hauskrecht 等人 [HMK98]、Sutton、Precup
    和 Singh [SPS99]、Parr 和 Russell [PaR98]、Dietterich [Die00]、Konidaris 和 Barto [KoB09]、Ciosek
    和 Silver [CiS15]、Mann、Mannor 和 Precup [MMP15]、Serban 等人 [SSP18] 和其中引用的参考文献。这些提议中的一些可能会在我们基于特征的聚合背景下得到有效应用，这是一个有趣的进一步研究课题。
- en: $k$-Step Aggregation
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: $k$-步聚合
- en: 'This scheme, suggested in [Ber11a] and illustrated in Fig. 4.14, is specified
    by disaggregation and aggregation probabilities as before, but involves $k>1$
    transitions between original system states in between transitions from and to
    aggregate states. The aggregate DP problem for this scheme involves $k+1$ copies
    of the original state space, in addition to the aggregate states. We accordingly
    introduce vectors $\tilde{J}_{0},\tilde{J}_{1},\ldots,\tilde{J}_{k}$, and $r^{*}=\{r^{*}_{1},\ldots,r^{*}_{q}\}$
    where:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方案由[Ber11a] 提出并在图 4.14 中说明，依旧由之前的解聚和聚合概率指定，但涉及 $k>1$ 次原始系统状态之间的过渡。该方案的聚合 DP
    问题涉及 $k+1$ 个原始状态空间副本，以及聚合状态。我们因此引入向量 $\tilde{J}_{0},\tilde{J}_{1},\ldots,\tilde{J}_{k}$
    和 $r^{*}=\{r^{*}_{1},\ldots,r^{*}_{q}\}$，其中：
- en: $r^{*}_{\ell}$ is the optimal cost-to-go from aggregate state $S_{\ell}$.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: $r^{*}_{\ell}$ 是从聚合状态 $S_{\ell}$ 出发的最优前向成本。
- en: $\tilde{J}_{0}(i)$ is the optimal cost-to-go from original system state $i$
    that has just been generated from an aggregate state (left side of Fig. 4.14).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{J}_{0}(i)$ 是从原始系统状态 $i$ 出发的最优前向成本，该状态刚刚从聚合状态生成（图 4.14 左侧）。
- en: $\tilde{J}_{1}(j_{1})$ is the optimal cost-to-go from original system state
    $j_{1}$ that has just been generated from an original system state $i$.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{J}_{1}(j_{1})$ 是从原始系统状态 $j_{1}$ 出发的最优前向成本，该状态刚刚从原始系统状态 $i$ 生成。
- en: $\tilde{J}_{m}(j_{m})$, $m=2,\ldots,k$, is the optimal cost-to-go from original
    system state $j_{m}$ that has just been generated from an original system state
    $j_{m-1}$.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: $\tilde{J}_{m}(j_{m})$, $m=2,\ldots,k$，是从原始系统状态 $j_{m}$ 出发的最优前向成本，该状态刚刚从原始系统状态
    $j_{m-1}$ 生成。
- en: 'These vectors satisfy the following set of Bellman equations:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量满足以下一组 Bellman 方程：
- en: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\tilde{J}_{0}(i),\qquad\ell=1,\ldots,q,$
    |  |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{*}_{\ell}=\sum_{i=1}^{n}d_{\ell i}\tilde{J}_{0}(i),\qquad\ell=1,\ldots,q,$
    |  |'
- en: '|  | $\tilde{J}_{0}(i)=\min_{u\in U(i)}\sum_{j_{1}=1}^{n}p_{ij_{1}}(u)\big{(}g(i,u,j_{1})+\alpha\tilde{J}_{1}(j_{1})\big{)},\qquad
    i=1,\ldots,n,$ |  | (4.42) |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{0}(i)=\min_{u\in U(i)}\sum_{j_{1}=1}^{n}p_{ij_{1}}(u)\big{(}g(i,u,j_{1})+\alpha\tilde{J}_{1}(j_{1})\big{)},\qquad
    i=1,\ldots,n,$ |  | (4.42) |'
- en: '|  | $\eqalign{\tilde{J}_{m}(j_{m})=\min_{u\in U(j_{m})}\sum_{j_{m+1}=1}^{n}p_{j_{m}j_{m+1}}&amp;(u)\big{(}g(j_{m},u,j_{m+1})+\alpha\tilde{J}_{m+1}(j_{m+1})\big{)},\cr&amp;\
    \ \ \ \ \ \ \ \ \ \ \ \ j_{m}=1,\ldots,n,\ m=1,\ldots,k-1,\cr}$ |  | (4.43) |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eqalign{\tilde{J}_{m}(j_{m})=\min_{u\in U(j_{m})}\sum_{j_{m+1}=1}^{n}p_{j_{m}j_{m+1}}&(u)\big{(}g(j_{m},u,j_{m+1})+\alpha\tilde{J}_{m+1}(j_{m+1})\big{)},\cr&\
    \ \ \ \ \ \ \ \ \ \ \ \ j_{m}=1,\ldots,n,\ m=1,\ldots,k-1,\cr}$ |  | (4.43) |'
- en: '|  | $\tilde{J}_{k}(j_{k})=\sum_{\ell=1}^{q}\phi_{j_{k}\ell}r^{*}_{\ell},\qquad
    j_{k}=1,\ldots,n.$ |  | (4.44) |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{J}_{k}(j_{k})=\sum_{\ell=1}^{q}\phi_{j_{k}\ell}r^{*}_{\ell},\qquad
    j_{k}=1,\ldots,n.$ |  | (4.44) |'
- en: 'By combining these equations, we obtain an equation for $r^{*}$:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些方程，我们得到 $r^{*}$ 的方程：
- en: '|  | $r^{*}=DT^{k}(\Phi r^{*}),$ |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{*}=DT^{k}(\Phi r^{*}),$ |  |'
- en: where $T$ is the usual DP mapping of the original problem [the case $k=1$ corresponds
    to Eqs. (4.11)-(4.12)]. As earlier, it can be seen that the associated mapping
    $DT^{k}\Phi$ is a contraction mapping with respect to the sup-norm, but its contraction
    modulus is $\alpha^{k}$ rather than $\alpha$.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$T$是原问题的常规DP映射[案例$k=1$对应方程(4.11)-(4.12)]。如前所述，可以看出相关的映射$DT^{k}\Phi$是在上确界范数下的收缩映射，但其收缩模数是$\alpha^{k}$而不是$\alpha$。
- en: '![[Uncaptioned image]](img/806f44799a64da5e8f6e5cd4afdf85ba.png)'
  id: totrans-586
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/806f44799a64da5e8f6e5cd4afdf85ba.png)'
- en: Figure 4.14  The transition mechanism for multistep aggregation. It is based
    on a dynamical system involving $k$ transitions between original system states
    interleaved between transitions from and to aggregate states.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 多步聚合的过渡机制。它基于一个动态系统，该系统涉及原始系统状态之间的$k$次过渡，并在从和到聚合状态之间交替进行。
- en: 'There is a similar mapping corresponding to a fixed policy and it can be used
    to implement a PI algorithm, which evaluates a policy through calculation of a
    corresponding parameter vector $r$ and then improves it. However, there is a major
    difference from the single-step aggregation case: a policy involves a set of $k$
    control functions $\{\mu_{0},\ldots,\mu_{k-1}\}$, and while a known policy can
    be easily simulated, its improvement involves multistep lookahead using the minimizations
    of Eqs. (4.42)-(4.44), and may be costly. Thus the preceding implementation of
    multistep aggregation-based PI is a useful idea only for problems where the cost
    of this multistep lookahead minimization (for a single given starting state) is
    not prohibitive.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个类似的映射对应于固定策略，可以用来实现PI算法，该算法通过计算相应的参数向量$r$来评估策略，然后改进它。然而，与单步聚合情况有一个主要区别：一个策略涉及一组$k$个控制函数$\{\mu_{0},\ldots,\mu_{k-1}\}$，虽然已知策略可以很容易地进行模拟，但其改进涉及多步预见，使用方程(4.42)-(4.44)的最小化，可能是昂贵的。因此，前述基于多步聚合的PI实现仅对那些多步预见最小化（对于一个给定的初始状态）成本不高的情况是有用的。
- en: 'On the other hand, from a theoretical point of view, a multistep aggregation
    scheme provides a means of better approximation of the true optimal cost vector
    $J^{\raise 0.04pt\hbox{\sevenrm*}}$, independent of the use of a large number
    of aggregate states. This can be seen from Eqs. (4.42)-(4.44), which by classical
    value iteration convergence results, show that $\tilde{J}_{0}(i)\to J^{\raise
    0.04pt\hbox{\sevenrm*}}(i)$ as $k\to\infty$, regardless of the choice of aggregate
    states. Moreover, because the modulus of the underlying contraction is $\alpha^{k}$,
    we can verify an improved error bound in place of the bound (4.17) of Prop. 4.1,
    which corresponds to $k=1$:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，从理论角度来看，多步聚合方案提供了一种更好地逼近真实最优成本向量$J^{\raise 0.04pt\hbox{\sevenrm*}}$的方法，这与使用大量聚合状态无关。这可以从方程(4.42)-(4.44)中看出，通过经典的值迭代收敛结果，这些方程表明$\tilde{J}_{0}(i)\to
    J^{\raise 0.04pt\hbox{\sevenrm*}}(i)$当$k\to\infty$时，无论聚合状态的选择如何。此外，由于底层收缩的模数为$\alpha^{k}$，我们可以验证一个改进的误差界限，替代命题4.1的界限(4.17)，该界限对应于$k=1$：
- en: '|  | $\big{&#124;}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)-r^{*}_{\ell}\big{&#124;}\leq{\epsilon\over
    1-\alpha^{k}},\qquad\forall\ i\hbox{ such that }i\in I_{\ell},\ \ell=1,\ldots,q,$
    |  |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|  | $\big{&#124;}J^{\raise 0.04pt\hbox{\sevenrm*}}(i)-r^{*}_{\ell}\big{&#124;}\leq{\epsilon\over
    1-\alpha^{k}},\qquad\forall\ i\hbox{ such that }i\in I_{\ell},\ \ell=1,\ldots,q,$
    |  |'
- en: where $\epsilon$ is given by Eq. (4.18). The proof is very similar to the one
    of Prop. 4.1.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\epsilon$由方程(4.18)给出。证明过程与命题4.1的证明非常相似。
- en: $\lambda$-Aggregation
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda$-聚合
- en: Multistep aggregation need not involve sequences of a fixed number of transitions
    between original system states. The number of transitions may be state-dependent
    or may be controlled by some randomized mechanism. In one such possibility, called
    $\lambda$-aggregation, we introduce a parameter $\lambda\in(0,1)$ and consider
    a Markov chain that makes a transition with probability $1-\lambda$ from an original
    system state to an aggregate state at each step, rather than with certainty after
    $k$ steps as in Fig.  4.14\. Then it can be shown that the cost vector of a given
    stationary policy $\mu$, may be evaluated approximately by $\Phi r_{\mu}$, where
    $r_{\mu}$ is the solution of the equation
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 多步聚合不一定涉及在原始系统状态之间的固定数量的过渡序列。过渡的数量可能依赖于状态或可能由某些随机机制控制。在一种这样的可能性中，称为$\lambda$-聚合，我们引入一个参数$\lambda\in(0,1)$，并考虑一个马尔可夫链，它在每一步以概率$1-\lambda$从原始系统状态过渡到聚合状态，而不是像图4.14中那样在$k$步后以确定性进行过渡。然后可以证明，给定的固定策略$\mu$的成本向量可以通过$\Phi
    r_{\mu}$进行近似评估，其中$r_{\mu}$是方程的解
- en: '|  | $r=DT_{\mu}^{(\lambda)}(\Phi r),$ |  | (4.45) |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
- en: where $T_{\mu}^{(\lambda)}$ is the mapping given by Eq. (2.14). This equation
    has a unique solution because the mapping $DT_{\mu}^{(\lambda)}\Phi$ can be shown
    to be a contraction mapping with respect to the sup-norm.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: As noted earlier, the aggregation equation
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Phi r=\Phi DT_{\mu}(\Phi r)$ |  |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: is a projected equation because $\Phi D$ is a projection mapping with respect
    to a suitable weighted Euclidean seminorm (see [YuB12], Section 4; it is a norm
    projection in the case of hard aggregation). Similarly, the $\lambda$-aggregation
    equation
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Phi r=\Phi DT_{\mu}^{(\lambda)}(\Phi r)$ |  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: is a projected equation, which is related to the proximal algorithm [Ber16a],
    [Ber18b], and may be solved by using temporal differences. Thus we may use exploration-enhanced
    versions of the LSTD($\lambda$) and LSPE($\lambda$) methods in an approximate
    PI scheme to solve the $\lambda$-aggregation equation. We refer to [Ber12] for
    further discussion.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: 5. POLICY ITERATION WITH FEATURE-BASED AGGREGATION AND A NEURAL NETWORK
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: We noted in Section 3 that neural networks can be used to construct features
    at the output of the last nonlinear layer. The neural network training process
    also yields linear weighting parameters for the feature vector $F(i)$ at the output
    of the last layer, thus obtaining an approximation $\hat{J}_{\mu}\big{(}F(i)\big{)}$
    to the cost function of a given policy $\mu$. Thus given the current policy $\mu$,
    the typical PI produces the new policy $\hat{\mu}$ using the approximate policy
    improvement operation (1.3) or a multistep variant, as illustrated in Fig. 5.1.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar PI scheme can be constructed based on feature-based aggregation with
    features supplied by the same neural network; see Fig. 5.2\. The main idea is
    to replace the (approximate) policy improvement operation with the solution of
    an aggregate problem, which provides the (approximately) improved policy $\hat{\mu}$.
    This is a more complicated policy improvement operation, but computes the new
    policy $\hat{\mu}$ based on a more accurate cost function approximation: one that
    is a nonlinear function of the features rather than linear. Moreover, $\hat{\mu}$
    not only aspires to be an improved policy relative to $\mu$, but also to be an
    optimal policy based on the aggregate problem, an approximation itself of the
    original DP problem. In particular, suppose that the neural network approximates
    $J_{\mu}$ perfectly. Then the scheme of Fig. 5.1 will replicate a single step
    of the PI algorithm starting from $\mu$, while the aggregation scheme of Fig. 5.2,
    with sufficient number of aggregate states, will produce a policy that is arbitrarily
    close to optimal.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: Let us now explain each of the steps of the aggregation-based PI procedure of
    Fig. 5.2, starting with the current policy $\mu$.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Feature mapping construction: We train the neural network using a training
    set of state-cost pairs that are generated using the current policy $\mu$. This
    provides a feature vector $F(i)$ as described in Section 3.'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 特征映射构建：我们使用使用当前策略$\mu$生成的状态成本对训练集对神经网络进行训练。这提供了一个特征向量$F(i)$，如第3节所述。
- en: '(b) Sampling to obtain the disaggregation sets: We sample the state space,
    generating a subset of states $I\subset\{1,\ldots,n\}$. We partition the corresponding
    set of state-feature pairs'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 获得细分集合的抽样：我们对状态空间进行抽样，生成状态子集$I\subset\{1,\ldots,n\}$。我们对应的状态特征对集合进行划分
- en: '|  | $\big{\{}(i,F(i))\mid i\in I\big{\}}$ |  |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '|  | $\big{\{}(i,F(i))\mid i\in I\big{\}}$ |  |'
- en: into a collection of subsets $S_{1},\ldots,S_{q}$. We then consider the aggregation
    framework with $S_{1},\ldots,S_{q}$ as the aggregate states, and the corresponding
    aggregate problem as described in Section 4\. The sampling to obtain the set of
    states $I$ may be combined with exploration to ensure that a sufficiently representative
    set of states is included.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态集合$S_{1},\ldots,S_{q}$划分为子集合。我们接下来考虑以$S_{1},\ldots,S_{q}$作为聚合状态的聚合框架，并根据第4节中描述的相应聚合问题进行描述。为了确保包含足够代表性的状态集合，可以将获得状态集$I$的抽样与探索结合起来。
- en: '![[Uncaptioned image]](img/6853466fa2f15c0ba472e78c4696c9d0.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![未标题的图片](img/6853466fa2f15c0ba472e78c4696c9d0.png)'
- en: Figure 5.1.  Schematic illustration of PI using a neural network-based cost
    approximation. Starting with a training set of state-cost pairs generated using
    the current policy $\mu$, the neural network yields a set of features and an approximate
    cost evaluation ${\hat{J\mkern 5.0mu}\mkern-5.0mu}{}_{\mu}$ using a linear combination
    of the features. This is followed by policy improvement using ${\hat{J\mkern 5.0mu}\mkern-5.0mu}{}_{\mu}$
    to generate the new policy $\hat{\mu}$.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1. 使用基于神经网络的成本近似的PI的示意图。从使用当前策略$\mu$生成的状态成本对训练集开始，神经网络生成一组特征，并使用特征的线性组合进行近似成本评估${\hat{J\mkern
    5.0mu}\mkern-5.0mu}{}_{\mu}$。然后，使用${\hat{J\mkern 5.0mu}\mkern-5.0mu}{}_{\mu}$进行策略改进，生成新策略$\hat{\mu}$。
- en: '![[Uncaptioned image]](img/3c699f5d2284be7e8811554d48e5eed6.png)'
  id: totrans-611
  prefs: []
  type: TYPE_IMG
  zh: '![未标题的图片](img/3c699f5d2284be7e8811554d48e5eed6.png)'
- en: Figure 5.2.  Illustration of PI using feature-based aggregation with features
    supplied by a neural network. Starting with a training set of state-cost pairs
    generated using the current policy $\mu$, the neural network yields a set of features,
    which are used to construct a feature-based aggregation framework. The optimal
    policy of the corresponding aggregate problem is used as the new policy $\hat{\mu}$.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2. 使用神经网络提供的基于特征的聚合进行PI的示例。从使用当前策略$\mu$生成的状态成本对训练集开始，神经网络生成一组特征，这些特征用于构建基于特征的聚合框架。相应聚合问题的最优策略被用作新策略$\hat{\mu}$。
- en: '(c) Aggregate problem solution: The aggregate DP problem is solved by using
    a simulation-based method to yield (perhaps approximately) the aggregate state
    optimal costs $r^{*}_{\ell}$, $\ell=1,\ldots,q$ (cf. Section 4.2).'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 聚合问题解决方案：通过使用基于模拟的方法解决聚合DP问题，以产生（可能是近似的）聚合状态的最优成本$r^{*}_{\ell}$，$\ell=1,\ldots,q$（参见第4.2节）。
- en: '(d) Definition of the improved policy: The “improved” policy is simply the
    optimal policy of the aggregate problem (or an approximation thereof, obtained
    for example after a few iterations of approximate simulation-based PI). This policy
    is defined implicitly by the aggregate costs $r^{*}_{\ell}$, $\ell=1,\ldots,q$,
    using the one-step lookahead minimization'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: (d) “改进”策略的定义：“改进”策略就是聚合问题的最优策略（或其近似值，例如在大约几次近似模拟的PI迭代后获得）。这个策略通过聚合成本$r^{*}_{\ell}$，$\ell=1,\ldots,q$隐含地定义，使用一步展望最小化
- en: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}\,r^{*}_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mu}(i)\in\arg\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha\sum_{\ell=1}^{q}\phi_{j\ell}\,r^{*}_{\ell}\right),\qquad
    i=1,\ldots,n,$ |  |'
- en: '[cf. Eq. (4.9)] or a multistep lookahead variant. Alternatively, the “improved”
    policy can implemented in model-free fashion using a $Q$-factor architecture $\tilde{Q}(i,u,\theta)$,
    as described in Sections 2.4 and 4.1, cf. Eqs. (4.14)-(4.16).'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '[参见方程(4.9)]或多步展望的变体。或者，可以以无模型方式实现“改进”的策略，使用$Q$因子架构$\tilde{Q}(i,u,\theta)$，如2.4节和4.1节描述的那样，参见方程(4.14)-(4.16)。'
- en: Let us also note that there are several options for implementing the algorithmic
    ideas of this section.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，本节的算法思想有几种实现选项。
- en: (1) The neural network-based feature construction process may be performed any
    number of times, each time followed by an aggregate problem solution that constructs
    a new policy, which is then used to generate new training data for the neural
    network. Alternatively, the neural network training and feature construction process
    may be done only once, followed by the solution of the corresponding feature-based
    aggregate problem.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 可以执行任意次数的基于神经网络的特征构造过程，每次之后都进行一次聚合问题的求解，从而构建一个新的策略，然后用这个策略生成新的神经网络训练数据。或者，神经网络训练和特征构造过程也可以只进行一次，然后求解相应的基于特征的聚合问题。
- en: (2) Several deep neural network-based PI cycles may be performed, a subset of
    the features thus generated may be selected, and the corresponding aggregate problem
    is solved just once, as a way of improving the final policy generated by the deep
    reinforcement learning process.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 可以执行多个基于深度神经网络的 PI 循环，从中生成的特征子集可能被选择，并且仅解决一次相应的聚合问题，以改善由深度强化学习过程生成的最终策略。
- en: (3) Following each cycle of neural network-based feature evaluation, the generated
    features may be supplemented with additional problem-specific handcrafted features,
    and/or features from previous cycles. This is a form of feature iteration that
    was noted in the preceding section.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 在每个基于神经网络的特征评估循环之后，可以用额外的问题特定手工特征和/或来自之前循环的特征来补充生成的特征。这是一种在前一节中提到的特征迭代形式。
- en: 'Finally, let us mention a potential weakness of using the features obtained
    at the output of the last nonlinear layer of the neural network in the context
    of aggregation: the sheer number of these features may be so large that the resulting
    number of aggregate states may become excessive. To address this situation one
    may consider pruning some of the features, or reducing their number using some
    form of regression, at the potential loss of some approximation accuracy. In this
    connection let us also emphasize a point made earlier in connection with an advantage
    of deep (rather than shallow) neural networks: because with each additional layer,
    the generated features tend to be more complex, their number at the output of
    the final nonlinear layer of the network can be made smaller as the number of
    layers increases. An extreme case is to use the cost function approximation obtained
    at the output of the neural network as a single feature/scoring function, in the
    spirit of Section 4.3.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提到使用神经网络最后一层非线性层输出的特征在聚合背景下可能存在的潜在弱点：这些特征的数量可能非常大，以至于最终的聚合状态数量可能变得过多。为了解决这种情况，可以考虑剪枝某些特征，或使用某种形式的回归来减少特征数量，可能会损失一些近似精度。在这方面，我们还要强调之前提到的深度（而非浅层）神经网络的一个优点：因为每增加一层，生成的特征通常更复杂，所以随着层数增加，最终非线性层输出的特征数量可以变得更少。一个极端的情况是将神经网络输出的成本函数近似作为单一特征/评分函数，符合第
    4.3 节的精神。
- en: Using Neural Networks in Conjunction with Heuristics
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络与启发式方法结合使用
- en: 'We noted at the end of Section 4.3 another use of neural networks in conjunction
    with aggregation: somehow construct multiple policies, evaluate each of these
    policies using a neural network, and use the policy cost function evaluations
    as multiple scoring functions in a feature-based aggregation scheme. In Section
    4.4, we elaborated on this idea for the case of the deterministic discrete optimization
    problem'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 4.3 节末尾提到另一种将神经网络与聚合结合使用的方法：以某种方式构造多个策略，使用神经网络评估每一个策略，并将策略成本函数评估作为特征基聚合方案中的多个评分函数。在第
    4.4 节中，我们对这一思想在确定性离散优化问题中的应用进行了详细阐述。
- en: '|  | $\eqalign{&amp;\hbox{minimize\ \ }G(u_{1},\ldots,u_{N})\cr&amp;\hbox{subject
    to\ \ }(u_{1},\ldots,u_{N})\in U,\cr}$ |  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '|  | $\eqalign{&amp;\hbox{最小化\ \ }G(u_{1},\ldots,u_{N})\cr&amp;\hbox{满足\ \
    }(u_{1},\ldots,u_{N})\in U,\cr}$ |  |'
- en: where $U$ is a finite set of feasible solutions and $G$ is a cost function [cf. Eq. (4.30)].
    We described the use of multiple heuristics to construct corresponding scoring
    functions. At any given $m$-solution, the scoring function values are computed
    by running each of the heuristics. A potential time-saving alternative is to approximate
    these scoring functions using neural networks.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U$ 是一个有限的可行解集合，而 $G$ 是一个成本函数 [参见 Eq. (4.30)]。我们描述了使用多种启发式方法来构造相应的评分函数。在任何给定的
    $m$-解中，通过运行每一个启发式方法来计算评分函数值。一种潜在的节省时间的替代方案是使用神经网络来近似这些评分函数。
- en: In particular, for each of the heuristics, we may train a separate neural network
    by using a training set consisting of pairs of $m$-solutions and corresponding
    heuristic costs. In this way we can obtain approximate scoring functions
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，对于每个启发式方法，我们可以通过使用由$m$-解和相应启发式成本对组成的训练集来训练一个单独的神经网络。这样，我们可以获得近似评分函数。
- en: '|  | $\tilde{V}_{1}(u_{1},\ldots,u_{m};\theta_{1}),\ldots,\tilde{V}_{s}(u_{1},\ldots,u_{m};\theta_{s}),$
    |  |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{V}_{1}(u_{1},\ldots,u_{m};\theta_{1}),\ldots,\tilde{V}_{s}(u_{1},\ldots,u_{m};\theta_{s}),$
    |  |'
- en: where $\theta_{1},\ldots,\theta_{s}$ are the corresponding neural network weight
    vectors. We may then use the approximate scoring functions as features in place
    of the exact heuristic cost functions to construct an aggregate problem similar
    to the one described in Section 4.4\. The solution of the aggregate problem can
    be used in turn to define a new policy, which may optionally be added to the current
    set of heuristics, as discussed earlier.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta_{1},\ldots,\theta_{s}$是相应的神经网络权重向量。然后，我们可以使用近似评分函数作为特征来代替精确的启发式成本函数，从而构建一个类似于第4.4节所描述的聚合问题。聚合问题的解可以用于定义一个新的政策，该政策可以选择性地添加到当前的启发式方法集合中，如前所述。
- en: Note that a separate neural network is needed for each heuristic and stage,
    so assembling the training data together with the training itself can be quite
    time consuming. However, both the data collection and the training processes can
    benefit greatly from parallelization.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个启发式方法和阶段都需要一个单独的神经网络，因此组装训练数据和训练本身可能会非常耗时。然而，数据收集和训练过程可以通过并行化大大受益。
- en: Finally, let us note that the approach of using a neural network to obtain approximate
    scoring functions may also be used in conjunction with a rollout scheme that uses
    a limited horizon. In such a scheme, starting from an $m$-solution, we may evaluate
    all possible subsequent $(m+1)$-solutions by running each of the $s$ heuristics
    up to a certain horizon depth of $d$ steps [rather than the full depth of $(N-m-1)$
    steps], and then approximate the subsequent heuristic cost [from stage $(m+1+d)$
    to stage $N$] by using the neural network estimates.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到，使用神经网络获取近似评分函数的方法也可以与使用有限视野的回滚方案结合使用。在这种方案中，从一个$m$-解开始，我们可以通过将每个$s$启发式方法运行到深度为$d$步[而不是全深度为$(N-m-1)$步]来评估所有可能的后续$(m+1)$-解，然后通过使用神经网络估计来近似后续启发式成本[从阶段$(m+1+d)$到阶段$N$]。
- en: 6. CONCLUDING REMARKS
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 6. **结论**
- en: 'We have surveyed some aspects of approximate PI methods with a focus on a new
    idea for policy improvement: feature-based aggregation that uses features provided
    by a neural network or a heuristic scheme, perhaps in combination with additional
    handcrafted features. We have argued that this type of policy improvement, while
    more time-consuming, may yield more effective policies, owing to the DP character
    of the aggregate problem and the use of a nonlinear feature-based architecture.
    The algorithmic idea of this paper seems to work well on small examples. However,
    tests with challenging problems are needed to fully evaluate its merits, particularly
    since solving the aggregate DP problem is more time-consuming than the standard
    one-step lookahead policy improvement scheme of Eq. (2.20) or its multistep lookahead
    variants.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了近似PI方法的一些方面，重点关注一种新的政策改进思路：基于特征的聚合，使用由神经网络或启发式方案提供的特征，或许结合其他手工特征。我们认为，这种政策改进虽然更耗时，但可能会产生更有效的政策，这得益于聚合问题的动态规划特性和使用非线性基于特征的架构。本文的算法思路在小规模示例中似乎效果良好。然而，需要用具有挑战性的问题进行测试，以全面评估其优点，特别是因为解决聚合动态规划问题比标准的一步前瞻政策改进方案（如方程(2.20)）或其多步前瞻变体更耗时。
- en: In this paper we have focused on finite-state discounted Markov decision problems,
    but our approach clearly extends to other types of finite-state DP involving stochastic
    uncertainty, including finite horizon, stochastic shortest path, and semi-Markov
    decision problems. It is also worth considering extensions to infinite-state problems,
    including those arising in the context of continuous spaces optimal control, shortest
    path, and partially observed Markov decision problems. Generally, the construction
    of aggregation frameworks for continuous spaces problems is conceptually straightforward,
    and follows the pattern discussed in this paper for finite-state problems. For
    example a hard aggregation scheme involves a partition of the continuous state
    space into a finite number of subsets/aggregate states, while a representative
    states scheme involves discretization of the continuous state space using a finite
    number of states. Note, however, that from a mathematical point of view, there
    may be a substantial issue of consistency, i.e., whether the solution of the aggregate
    problem “converges” to the solution of the continuous spaces problem as the number
    of aggregate states increases. Part of the reason has to do with the fact that
    the Bellman equation of continuous spaces problems need not have a unique solution.
    The author’s monograph [Ber18a], Sections 4.5 and 4.6, provides an analysis of
    this question for shortest path and optimal control problems with a continuous
    state space, and identifies classes of problems that are more amenable to approximate
    DP solution approaches.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们重点关注了有限状态折扣马尔可夫决策问题，但我们的方法显然可以扩展到涉及随机不确定性的其他类型有限状态动态规划问题，包括有限时间、随机最短路径和半马尔可夫决策问题。也值得考虑扩展到无限状态问题，包括那些在连续空间最优控制、最短路径和部分观察马尔可夫决策问题中出现的问题。通常，构建连续空间问题的聚合框架在概念上是简单的，并遵循本文针对有限状态问题讨论的模式。例如，硬聚合方案涉及将连续状态空间划分为有限数量的子集/聚合状态，而代表状态方案则涉及使用有限数量的状态对连续状态空间进行离散化。然而，需要注意的是，从数学角度来看，可能存在一致性问题，即随着聚合状态数量的增加，聚合问题的解是否“收敛”到连续空间问题的解。部分原因与连续空间问题的贝尔曼方程可能没有唯一解有关。作者的专著[Ber18a]，第4.5节和第4.6节，对连续状态空间中的最短路径和最优控制问题进行了分析，并识别出更适合于近似动态规划解决方法的问题类别。
- en: Finally, we note that the key issue of feature construction can be addressed
    in a number of ways. In this paper we have focused on the use of deep neural networks
    and heuristics for approximating the optimal cost function or the cost functions
    of policies. However, we may use instead any methodology that automatically constructs
    good features at reasonable computational cost.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到特征构造的关键问题可以通过多种方式来解决。在本文中，我们重点关注了使用深度神经网络和启发式方法来近似最优成本函数或策略的成本函数。然而，我们也可以使用任何能够以合理计算成本自动构造良好特征的方法。
- en: 7. REFERENCES
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 参考文献
- en: '[ADB17] Arulkumaran, K., Deisenroth, M. P., Brundage, M. and Bharath, A. A.,
    2017\. “A Brief Survey of Deep Reinforcement Learning,” arXiv preprint arXiv:1708.05866.'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[ADB17] Arulkumaran, K., Deisenroth, M. P., Brundage, M. 和 Bharath, A. A.，2017年。“深度强化学习简要综述”，arXiv
    预印本 arXiv:1708.05866。'
- en: '[Abr90] Abramson, B., 1990. “Expected-Outcome: A General Model of Static Evaluation,”
    IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 12, pp. 182-193.'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '[Abr90] Abramson, B.，1990年。“期望结果：静态评估的一般模型”，IEEE模式分析与机器智能学报，第12卷，第182-193页。'
- en: '[BBD10] Busoniu, L., Babuska, R., De Schutter, B., and Ernst, D., 2010. Reinforcement
    Learning and Dynamic Programming Using Function Approximators, CRC Press, N. Y.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[BBD10] Busoniu, L., Babuska, R., De Schutter, B. 和 Ernst, D.，2010年。使用函数近似器的强化学习与动态规划，CRC出版社，纽约。'
- en: '[BBS87] Bean, J. C., Birge, J. R., and Smith, R. L., 1987. “Aggregation in
    Dynamic Programming,” Operations Research, Vol. 35, pp. 215-220.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '[BBS87] Bean, J. C., Birge, J. R.，和 Smith, R. L.，1987年。“动态规划中的聚合”，运筹学，第35卷，第215-220页。'
- en: '[BBS95] Barto, A.  G., Bradtke, S. J., and Singh, S. P., 1995. “Real-Time Learning
    and Control Using Asynchronous Dynamic Programming,” Artificial Intelligence,
    Vol. 72, pp. 81-138.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '[BBS95] Barto, A. G., Bradtke, S. J., 和 Singh, S. P.，1995年。“实时学习与控制使用异步动态规划”，人工智能，第72卷，第81-138页。'
- en: '[BPW12] Browne, C., Powley, E., Whitehouse, D., Lucas, L., Cowling, P. I.,
    Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S., 2012. “A
    Survey of Monte Carlo Tree Search Methods,” IEEE Trans. on Computational Intelligence
    and AI in Games, Vol. 4, pp. 1-43.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '[BPW12] Browne, C., Powley, E., Whitehouse, D., Lucas, L., Cowling, P. I.,
    Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., 和 Colton, S., 2012.
    “蒙特卡罗树搜索方法综述，”IEEE计算智能与游戏中的AI学报，第4卷，第1-43页。'
- en: '[BSA83] Barto, A. G., Sutton, R. S., and Anderson, C. W., 1983. “Neuronlike
    Elements that Can Solve Difficult Learning Control Problems,” IEEE Trans. on Systems,
    Man, and Cybernetics, Vol. 13, pp. 835-846.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '[BSA83] Barto, A. G., Sutton, R. S., 和 Anderson, C. W., 1983. “可解决复杂学习控制问题的类神经元元素，”IEEE系统、人工与控制论学报，第13卷，第835-846页。'
- en: '[BTW97] Bertsekas, D. P., Tsitsiklis, J. N., and Wu, C., 1997. “Rollout Algorithms
    for Combinatorial Optimization,” Heuristics, Vol. 3, pp. 245-262.'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '[BTW97] Bertsekas, D. P., Tsitsiklis, J. N., 和 Wu, C., 1997. “组合优化的回滚算法，”启发式，第3卷，第245-262页。'
- en: '[BeC89] Bertsekas, D. P., and Castanon, D. A., 1989. “Adaptive Aggregation
    Methods for Infinite Horizon Dynamic Programming,” IEEE Trans. on Aut.  Control,
    Vol. AC-34, pp. 589-598.'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeC89] Bertsekas, D. P., 和 Castanon, D. A., 1989. “无限时间视野动态规划的自适应聚合方法，”IEEE自动控制学报，第AC-34卷，第589-598页。'
- en: '[BeC99] Bertsekas, D. P., and Castanon, D. A., 1999. “Rollout Algorithms for
    Stochastic Scheduling Problems,” Heuristics, Vol. 5, pp. 89-108.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeC99] Bertsekas, D. P., 和 Castanon, D. A., 1999. “用于随机调度问题的回滚算法，”启发式，第5卷，第89-108页。'
- en: '[BeT91] Bertsekas, D. P., and Tsitsiklis, J. N., 1991. “An Analysis of Stochastic
    Shortest Path Problems,” Math. Operations Research, Vol. 16, pp. 580-595.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeT91] Bertsekas, D. P., 和 Tsitsiklis, J. N., 1991. “随机最短路径问题的分析，”数学运筹研究，第16卷，第580-595页。'
- en: '[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming,
    Athena Scientific, Belmont, MA.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeT96] Bertsekas, D. P., 和 Tsitsiklis, J. N., 1996. 神经动态规划，雅典娜科学出版社，贝尔蒙特，马萨诸塞州。'
- en: '[BeT00] Bertsekas, D. P., and Tsitsiklis, J. N., 2000. “Gradient Convergence
    in Gradient Methods,” SIAM J. on Optimization, Vol. 10, pp. 627-642.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeT00] Bertsekas, D. P., 和 Tsitsiklis, J. N., 2000. “梯度方法中的梯度收敛，”SIAM优化学报，第10卷，第627-642页。'
- en: '[Ber95] Bertsekas, D. P., 1995. “A Counterexample to Temporal Differences Learning,”
    Neural Computation, Vol. 7, pp. 270-279.'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber95] Bertsekas, D. P., 1995. “时间差分学习的反例，”神经计算，第7卷，第270-279页。'
- en: '[Ber11a] Bertsekas, D. P., 2011. “Approximate Policy Iteration: A Survey and
    Some New Methods,” J. of Control Theory and Applications, Vol. 9, pp. 310-335.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber11a] Bertsekas, D. P., 2011. “近似策略迭代：综述与一些新方法，”控制理论与应用杂志，第9卷，第310-335页。'
- en: '[Ber11b] Bertsekas, D. P., 2011. “Temporal Difference Methods for General Projected
    Equations,” IEEE Trans. on Aut.  Control, Vol. 56, pp. 2128-2139.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber11b] Bertsekas, D. P., 2011. “通用投影方程的时间差分方法，”IEEE自动控制学报，第56卷，第2128-2139页。'
- en: '[Ber11c] Bertsekas, D. P., 2011. “$\lambda$-Policy Iteration: A Review and
    a New Implementation,” Lab. for Information and Decision Systems Report LIDS-P-2874,
    MIT; in Reinforcement Learning and Approximate Dynamic Programming for Feedback
    Control, by F. Lewis and D. Liu (eds.), IEEE Press, Computational Intelligence
    Series, 2012.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber11c] Bertsekas, D. P., 2011. “$\lambda$-策略迭代：综述与新实现，”信息与决策系统实验室报告 LIDS-P-2874，麻省理工学院；见《强化学习与近似动态规划用于反馈控制》，由
    F. Lewis 和 D. Liu (编)，IEEE出版社，计算智能系列，2012年。'
- en: '[Ber12] Bertsekas, D. P., 2012. Dynamic Programming and Optimal Control, Vol. II:
    Approximate Dynamic Programming, 4th edition, Athena Scientific, Belmont, MA.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber12] Bertsekas, D. P., 2012. 动态规划与最优控制，第II卷：近似动态规划，第4版，雅典娜科学出版社，贝尔蒙特，马萨诸塞州。'
- en: '[Ber13] Bertsekas, D. P., 2013. “Rollout Algorithms for Discrete Optimization:
    A Survey,” Handbook of Combinatorial Optimization, Springer.'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber13] Bertsekas, D. P., 2013. “离散优化的回滚算法：综述，”组合优化手册，Springer出版社。'
- en: '[Ber15] Bertsekas, D. P., 2015. Convex Optimization Algorithms, Athena Scientific,
    Belmont, MA.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber15] Bertsekas, D. P., 2015. 凸优化算法，雅典娜科学出版社，贝尔蒙特，马萨诸塞州。'
- en: '[Ber16a] Bertsekas, D. P., 2016. “Proximal Algorithms and Temporal Differences
    for Large Linear Systems: Extrapolation, Approximation, and Simulation,” Report
    LIDS-P-3205, MIT; arXiv preprint arXiv:1610.05427.'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber16a] Bertsekas, D. P., 2016. “大规模线性系统的近端算法与时间差分：外推、近似与仿真，”报告 LIDS-P-3205，麻省理工学院；arXiv
    预印本 arXiv:1610.05427。'
- en: '[Ber16b] Bertsekas, D. P., 2016. Nonlinear Programming, 3rd edition, Athena
    Scientific, Belmont, MA.'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber16b] Bertsekas, D. P., 2016. 非线性编程，第3版，雅典娜科学出版社，贝尔蒙特，马萨诸塞州。'
- en: '[Ber17] Bertsekas, D. P., 2017. Dynamic Programming and Optimal Control, Vol. I,
    4th edition, Athena Scientific, Belmont, MA.'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber17] Bertsekas, D. P., 2017. 动态规划与最优控制，第I卷，第4版，雅典娜科学出版社，贝尔蒙特，马萨诸塞州。'
- en: '[Ber18a] Bertsekas, D. P., 2018. Abstract Dynamic Programming, Athena Scientific,
    Belmont, MA.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber18a] Bertsekas, D. P., 2018. 抽象动态规划, Athena Scientific, Belmont, MA。'
- en: '[Ber18b] Bertsekas, D. P., 2018. “Proximal Algorithms and Temporal Difference
    Methods for Solving Fixed Point Problems,” Computational Optimization and Applications
    J., Vol. 70, pp. 709-736.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ber18b] Bertsekas, D. P., 2018. “解决固定点问题的近端算法和时序差分方法”，计算优化与应用期刊, 第70卷, 页709-736。'
- en: '[Bis95] Bishop, C. M, 1995. Neural Networks for Pattern Recognition, Oxford
    University Press, N. Y.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bis95] Bishop, C. M., 1995. 模式识别中的神经网络, Oxford University Press, N. Y.'
- en: '[CFH05] Chang, H. S., Hu, J., Fu, M. C., and Marcus, S. I., 2005.  “An Adaptive
    Sampling Algorithm for Solving Markov Decision Processes,” Operations Research,
    Vol. 53, pp. 126-139.'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '[CFH05] Chang, H. S., Hu, J., Fu, M. C., 和 Marcus, S. I., 2005. “解决马尔可夫决策过程的自适应采样算法”，运筹学,
    第53卷, 页126-139。'
- en: '[CFH13] Chang, H. S., Hu, J., Fu, M. C., and Marcus, S. I., 2013.  Simulation-Based
    Algorithms for Markov Decision Processes, (2nd Ed.), Springer, N. Y.'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '[CFH13] Chang, H. S., Hu, J., Fu, M. C., 和 Marcus, S. I., 2013. 基于仿真的马尔可夫决策过程算法（第2版），Springer,
    N. Y.'
- en: '[Cao07] Cao, X. R., 2007. Stochastic Learning and Optimization: A Sensitivity-Based
    Approach, Springer, N. Y.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '[Cao07] Cao, X. R., 2007. 随机学习与优化：基于灵敏度的方法, Springer, N. Y.'
- en: '[ChK86] Christensen, J., and Korf, R. E., 1986. “A Unified Theory of Heuristic
    Evaluation Functions and its Application to Learning,” in Proceedings AAAI-86,
    pp. 148-152.'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '[ChK86] Christensen, J., 和 Korf, R. E., 1986. “启发式评价函数的统一理论及其在学习中的应用”，AAAI-86会议论文集，页148-152。'
- en: '[ChM82] Chatelin, F., and Miranker, W. L., 1982.  “Acceleration by Aggregation
    of Successive Approximation Methods,” Linear Algebra and its Applications, Vol. 43,
    pp. 17-47.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '[ChM82] Chatelin, F., 和 Miranker, W. L., 1982. “通过聚合的加速逐次逼近方法”，线性代数及其应用, 第43卷,
    页17-47。'
- en: '[CiS15] Ciosek, K., and Silver, D., 2015. “Value Iteration with Options and
    State Aggregation,” Report, Centre for Computational Statistics and Machine Learning
    University College London.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '[CiS15] Ciosek, K., 和 Silver, D., 2015. “具有选项和状态聚合的价值迭代”，报告，伦敦大学计算统计与机器学习中心。'
- en: '[Cou06] Coulom, R., 2006. “Efficient Selectivity and Backup Operators in Monte-Carlo
    Tree Search,” International Conference on Computers and Games, Springer, pp. 72-83.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '[Cou06] Coulom, R., 2006. “蒙特卡罗树搜索中的高效选择性和备份操作”，国际计算机与游戏会议，Springer，页72-83。'
- en: '[Cyb89] Cybenko, 1989. “Approximation by Superpositions of a Sigmoidal Function,”
    Math. of Control, Signals, and Systems, Vol. 2, pp. 303-314.'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[Cyb89] Cybenko, 1989. “通过超叠加的Sigmoidal函数的逼近”，Math. of Control, Signals, and
    Systems, 第2卷, 页303-314。'
- en: '[DNW16] David, O. E., Netanyahu, N. S., and Wolf, L., 2016. “Deepchess: End-to-End
    Deep Neural Network for Automatic Learning in Chess,” in International Conference
    on Artificial Neural Networks, pp. 88-96, Springer.'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '[DNW16] David, O. E., Netanyahu, N. S., 和 Wolf, L., 2016. “Deepchess：用于棋类自动学习的端到端深度神经网络”，国际人工神经网络会议，页88-96，Springer。'
- en: '[DiM10] Di Castro, D., and Mannor, S., 2010. “Adaptive Bases for Reinforcement
    Learning,” Machine Learning and Knowledge Discovery in Databases, Vol. 6321, pp. 312-327.'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '[DiM10] Di Castro, D., 和 Mannor, S., 2010. “强化学习的自适应基函数”，机器学习与数据库中的知识发现, 第6321卷,
    页312-327。'
- en: '[Die00] Dietterich, T., 2000. “Hierarchical Reinforcement Learning with the
    MAXQ Value Function Decomposition,” J. of Artificial Intelligence Research, Vol. 13,
    pp. 227-303.'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '[Die00] Dietterich, T., 2000. “MAXQ值函数分解的分层强化学习”，人工智能研究期刊, 第13卷, 页227-303。'
- en: '[FYG06] Fern, A., Yoon, S. and Givan, R., 2006. “Approximate Policy Iteration
    with a Policy Language Bias: Solving Relational Markov Decision Processes,” J. of
    Artificial Intelligence Research, Vol. 25, pp. 75-118.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '[FYG06] Fern, A., Yoon, S. 和 Givan, R., 2006. “具有策略语言偏差的近似策略迭代：解决关系马尔可夫决策过程”，人工智能研究期刊,
    第25卷, 页75-118。'
- en: '[DoD93] Douglas, C. C., and Douglas, J., 1993. “A Unified Convergence Theory
    for Abstract Multigrid or Multilevel Algorithms, Serial and Parallel,” SIAM J. Num. Anal.,
    Vol. 30, pp. 136-158.'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '[DoD93] Douglas, C. C. 和 Douglas, J., 1993. “统一收敛理论用于抽象多重网格或多级算法，串行和并行”，SIAM
    J. Num. Anal., 第30卷, 页136-158。'
- en: '[Fle84] Fletcher, C. A. J., 1984. Computational Galerkin Methods, Springer,
    N. Y.'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '[Fle84] Fletcher, C. A. J., 1984. 计算Galerkin方法, Springer, N. Y.'
- en: '[Fun89] Funahashi, K., 1989. “On the Approximate Realization of Continuous
    Mappings by Neural Networks,” Neural Networks, Vol. 2, pp. 183-192.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '[Fun89] Funahashi, K., 1989. “神经网络对连续映射的近似实现”，神经网络, 第2卷, 页183-192。'
- en: '[GBC16] Goodfellow, I., Bengio, J., and Courville, A., Deep Learning, MIT Press,
    Cambridge, MA.'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '[GBC16] Goodfellow, I., Bengio, J., 和 Courville, A., 深度学习, MIT Press, Cambridge,
    MA。'
- en: '[GGS13] Gabillon, V., Ghavamzadeh, M., and Scherrer, B., 2013. “Approximate
    Dynamic Programming Finally Performs Well in the Game of Tetris,” in Advances
    in Neural Information Processing Systems, pp. 1754-1762.'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '[GGS13] Gabillon, V., Ghavamzadeh, M., 和 Scherrer, B., 2013. “近似动态规划在俄罗斯方块游戏中终于表现良好,”
    见神经信息处理系统进展, 页1754-1762.'
- en: '[Gor95] Gordon, G. J., 1995. “Stable Function Approximation in Dynamic Programming,”
    in Machine Learning: Proceedings of the 12th International Conference, Morgan
    Kaufmann, San Francisco, CA.'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gor95] Gordon, G. J., 1995. “动态规划中的稳定函数逼近,” 见《机器学习：第12届国际会议论文集》，Morgan Kaufmann,
    San Francisco, CA.'
- en: '[Gos15] Gosavi, A., 2015. Simulation-Based Optimization: Parametric Optimization
    Techniques and Reinforcement Learning, 2nd Edition, Springer, N. Y.'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '[Gos15] Gosavi, A., 2015. 《基于仿真的优化：参数优化技术和强化学习》，第2版, Springer, N. Y.'
- en: '[HMK98] Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., and Boutilier,
    C., 1998. “Hierarchical Solution of Markov Decision Processes Using Macro-Actions,”
    in Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence,
    pp. 220-229.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '[HMK98] Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., 和 Boutilier,
    C., 1998. “使用宏动作的马尔可夫决策过程的层次解决方案,” 见第14届人工智能不确定性会议论文集, 页220-229.'
- en: '[HOT06] Hinton, G. E., Osindero, S., and Teh, Y. W., 2006\. “A Fast Learning
    Algorithm for Deep Belief Nets,” Neural Computation, Vol. 18, pp. 1527-1554.'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '[HOT06] Hinton, G. E., Osindero, S., 和 Teh, Y. W., 2006. “深度置信网络的快速学习算法,” 神经计算,
    第18卷, 页1527-1554.'
- en: '[HSW89] Hornick, K., Stinchcombe, M., and White, H., 1989. “Multilayer Feedforward
    Networks are Universal Approximators,” Neural Networks, Vol. 2, pp. 359-159.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '[HSW89] Hornick, K., Stinchcombe, M., 和 White, H., 1989. “多层前馈网络是通用逼近器,” 神经网络,
    第2卷, 页359-159.'
- en: '[Hay08] Haykin, S., 2008. Neural Networks and Learning Machines, (3rd Edition),
    Prentice-Hall, Englewood-Cliffs, N. J.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hay08] Haykin, S., 2008. 《神经网络与学习机器》，（第3版），Prentice-Hall, Englewood-Cliffs,
    N. J.'
- en: '[Hol86] Holland, J. H., 1986. “Escaping Brittleness: the Possibility of General-Purpose
    Learning Algorithms Applied to Rule-Based Systems,” in Machine Learning: An Artificial
    Intelligence Approach, Michalski, R. S., Carbonell, J. G., and Mitchell, T. M.,
    (eds.), Morgan Kaufmann, San Mateo, CA, pp. 593-623.'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hol86] Holland, J. H., 1986. “摆脱脆弱性：将通用学习算法应用于基于规则的系统的可能性,” 见《机器学习：人工智能方法》，Michalski,
    R. S., Carbonell, J. G., 和 Mitchell, T. M., （编），Morgan Kaufmann, San Mateo, CA,
    页593-623.'
- en: '[Iva68] Ivakhnenko, A. G., 1968. “The Group Method of Data Handling: A Rival
    of the Method of Stochastic Approximation,” Soviet Automatic Control, Vol. 13,
    pp. 43-55.'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '[Iva68] Ivakhnenko, A. G., 1968. “数据处理的群体方法：随机近似法的竞争者,” 苏联自动控制, 第13卷, 页43-55.'
- en: '[Iva71] Ivakhnenko, A. G., 1971. “Polynomial Theory of Complex Systems,” IEEE
    Transactions on Systems, Man and Cybernetics, Vol. 4, pp. 364-378.'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '[Iva71] Ivakhnenko, A. G., 1971. “复杂系统的多项式理论,” IEEE 系统、人类与控制论杂志, 第4卷, 页364-378.'
- en: '[Jon90] Jones, L. K., 1990. “Constructive Approximations for Neural Networks
    by Sigmoidal Functions,” Proceedings of the IEEE, Vol. 78, pp. 1586-1589.'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jon90] Jones, L. K., 1990. “神经网络的构造性近似，通过Sigmoidal函数,” IEEE 会议录, 第78卷, 页1586-1589.'
- en: '[KMP06] Keller, P. W., Mannor, S., and Precup, D., 2006. “Automatic Basis Function
    Construction for Approximate Dynamic Programming and Reinforcement Learning,”
    in Proc. of the 23rd International Conference on Machine Learning, ACM, pp. 449-456.'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '[KMP06] Keller, P. W., Mannor, S., 和 Precup, D., 2006. “用于近似动态规划和强化学习的自动基函数构建,”
    见第23届国际机器学习大会论文集, ACM, 页449-456.'
- en: '[KVZ72] Krasnoselskii, M. A., Vainikko, G. M., Zabreyko, R. P., and Ruticki,
    Ya. B., 1972. Approximate Solution of Operator Equations, Translated by D. Louvish,
    Wolters-Noordhoff Pub., Groningen.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '[KVZ72] Krasnoselskii, M. A., Vainikko, G. M., Zabreyko, R. P., 和 Ruticki,
    Ya. B., 1972. 《算子方程的近似解》，翻译：D. Louvish, Wolters-Noordhoff 出版公司, Groningen.'
- en: '[Kir11] Kirsch, A., 2011. An Introduction to the Mathematical Theory of Inverse
    Problems, (2nd Edition), Springer, N. Y.'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kir11] Kirsch, A., 2011. 《逆问题的数学理论导论》，（第2版），Springer, N. Y.'
- en: '[KoB09] Konidaris, G., and Barto, A., 2009. “Efficient Skill Learning Using
    Abstraction Selection,” in 21st International Joint Conference on Artificial Intelligence.'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '[KoB09] Konidaris, G., 和 Barto, A., 2009. “使用抽象选择的高效技能学习,” 见第21届国际人工智能联合会议.'
- en: '[LLL08] Lewis, F. L., Liu, D., and Lendaris, G. G., 2008. Special Issue on
    Adaptive Dynamic Programming and Reinforcement Learning in Feedback Control, IEEE
    Trans. on Systems, Man, and Cybernetics, Part B, Vol. 38, No. 4.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLL08] Lewis, F. L., Liu, D., 和 Lendaris, G. G., 2008. 自适应动态规划和反馈控制中的强化学习特刊,
    IEEE 系统、人类与控制论杂志B部分, 第38卷, 第4期.'
- en: '[LLP93] Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S., 1993. “Multilayer
    Feedforward Networks with a Nonpolynomial Activation Function can Approximate
    any Function,” Neural Networks, Vol. 6, pp. 861-867.'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLP93] Leshno, M.，Lin, V. Y.，Pinkus, A. 和 Schocken, S., 1993.《具有非多项式激活函数的多层前馈网络可以逼近任何函数》，《神经网络》，第6卷，第861-867页。'
- en: '[LWL17] Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., and Alsaadi, F. E.,
    2017\. “A Survey of Deep Neural Network Architectures and their Applications,”
    Neurocomputing, Vol. 234, pp. 11-26.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '[LWL17] Liu, W.，Wang, Z.，Liu, X.，Zeng, N.，Liu, Y. 和 Alsaadi, F. E., 2017.《深度神经网络架构及其应用的调查》，《神经计算》，第234卷，第11-26页。'
- en: '[LeL12] Lewis, F. L., and Liu, D., 2012. Reinforcement Learning and Approximate
    Dynamic Programming for Feedback Control, IEEE Press Computational Intelligence
    Series, N. Y.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '[LeL12] Lewis, F. L. 和 Liu, D., 2012.《反馈控制中的强化学习与近似动态规划》，IEEE Press 计算智能系列，纽约。'
- en: '[Li17] Li, Y., 2017. “Deep Reinforcement Learning: An Overview,” arXiv preprint
    ArXiv: 1701.07274v5.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '[Li17] Li, Y., 2017.《深度强化学习概述》，arXiv 预印本 ArXiv:1701.07274v5。'
- en: '[MMP15] Mann, T.A., Mannor, S. and Precup, D., 2015. “Approximate Value Iteration
    with Temporally Extended Actions,” J. of Artificial Intelligence Research, Vol. 53,
    pp. 375-438.'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '[MMP15] Mann, T.A.，Mannor, S. 和 Precup, D., 2015.《带时间扩展动作的近似值迭代》，《人工智能研究杂志》，第53卷，第375-438页。'
- en: '[MMS06] Menache, I., Mannor, S., and Shimkin, N., 2005. “Basis Function Adaptation
    in Temporal Difference Reinforcement Learning,” Ann. Oper. Res., Vol. 134, pp. 215-238.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '[MMS06] Menache, I.，Mannor, S. 和 Shimkin, N., 2005.《时间差分强化学习中的基函数适应》，《运筹学年刊》，第134卷，第215-238页。'
- en: '[Men82] Mendelssohn, R., 1982. “An Iterative Aggregation Procedure for Markov
    Decision Processes,” Operations Research, Vol. 30, pp. 62-73.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '[Men82] Mendelssohn, R., 1982.《Markov决策过程的迭代聚合程序》，《运筹学》，第30卷，第62-73页。'
- en: '[PaR98] Parr, R., and Russell, S. J., 1998. “Reinforcement Learning with Hierarchies
    of Machines,” in Advances in Neural Information Processing Systems, pp. 1043-1049.'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '[PaR98] Parr, R. 和 Russell, S. J., 1998.《具有机器层次的强化学习》，收录于《神经信息处理系统进展》，第1043-1049页。'
- en: '[Pow11] Powell, W. B., 2011.  Approximate Dynamic Programming: Solving the
    Curses of Dimensionality, 2nd Edition, J. Wiley and Sons, Hoboken, N. J.'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pow11] Powell, W. B., 2011.《近似动态规划：解决维度灾难》，第二版，J. Wiley and Sons，霍博肯，新泽西州。'
- en: '[RPW91] Rogers, D. F., Plante, R. D., Wong, R. T., and Evans, J. R., 1991. “Aggregation
    and Disaggregation Techniques and Methodology in Optimization,” Operations Research,
    Vol. 39, pp. 553-582.'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '[RPW91] Rogers, D. F.，Plante, R. D.，Wong, R. T. 和 Evans, J. R., 1991.《优化中的聚合与解聚技术及方法论》，《运筹学》，第39卷，第553-582页。'
- en: '[SBP04] Si, J., Barto, A., Powell, W., and Wunsch, D., (Eds.) 2004. Learning
    and Approximate Dynamic Programming, IEEE Press, N. Y.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '[SBP04] Si, J.，Barto, A.，Powell, W. 和 Wunsch, D. (编), 2004.《学习与近似动态规划》，IEEE
    Press，纽约。'
- en: '[SGG15] Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist,
    M., 2015. “Approximate Modified Policy Iteration and its Application to the Game
    of Tetris,” J. of Machine Learning Research, Vol. 16, pp. 1629-1676.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[SGG15] Scherrer, B.，Ghavamzadeh, M.，Gabillon, V.，Lesner, B. 和 Geist, M., 2015.《近似修改策略迭代及其在俄罗斯方块游戏中的应用》，《机器学习研究杂志》，第16卷，第1629-1676页。'
- en: '[SHS17] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M.,
    Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T. and Lillicrap, T.,
    2017. “Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning
    Algorithm,” arXiv preprint arXiv:1712.01815.'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '[SHS17] Silver, D.，Hubert, T.，Schrittwieser, J.，Antonoglou, I.，Lai, M.，Guez,
    A.，Lanctot, M.，Sifre, L.，Kumaran, D.，Graepel, T. 和 Lillicrap, T., 2017.《通过自我博弈与通用强化学习算法掌握国际象棋和将棋》，arXiv
    预印本 arXiv:1712.01815。'
- en: '[SJJ95] Singh, S. P., Jaakkola, T., and Jordan, M. I., 1995. “Reinforcement
    Learning with Soft State Aggregation,” in Advances in Neural Information Processing
    Systems 7, MIT Press, Cambridge, MA.'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '[SJJ95] Singh, S. P.，Jaakkola, T. 和 Jordan, M. I., 1995.《带软状态聚合的强化学习》，收录于《神经信息处理系统进展第7卷》，MIT
    Press，剑桥，马萨诸塞州。'
- en: '[SPS99] Sutton, R., Precup, D., and Singh, S., 1999. “Between MDPs and Semi-MDPs:
    A Framework for Temporal Abstraction in Reinforcement Learning,” Artificial Intelligence,
    Vol. 112, pp. 181-211.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '[SPS99] Sutton, R.，Precup, D. 和 Singh, S., 1999.《MDP与半MDP之间：强化学习中的时间抽象框架》，《人工智能》，第112卷，第181-211页。'
- en: '[SSP18] Serban, I. V., Sankar, C., Pieper, M., Pineau, J., Bengio., J., 2018. “The
    Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach,” arXiv
    preprint arXiv:1807.04723.v1.'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '[SSP18] Serban, I. V.，Sankar, C.，Pieper, M.，Pineau, J.，Bengio, J., 2018.《瓶颈模拟器：一种基于模型的深度强化学习方法》，arXiv
    预印本 arXiv:1807.04723.v1。'
- en: '[Saa03] Saad, Y., 2003. Iterative Methods for Sparse Linear Systems, SIAM,
    Phila., Pa.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '[Saa03] Saad, Y., 2003.《稀疏线性系统的迭代方法》，SIAM，费城，宾夕法尼亚州。'
- en: '[Sam59] Samuel, A. L., 1959. “Some Studies in Machine Learning Using the Game
    of Checkers,” IBM J. of Research and Development, pp. 210-229.'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sam59] Samuel, A. L., 1959. “使用跳棋游戏的机器学习研究，”《IBM研究与开发杂志》，第210-229页。'
- en: '[Sam67] Samuel, A. L., 1967. “Some Studies in Machine Learning Using the Game
    of Checkers. II – Recent Progress,” IBM J. of Research and Development, pp. 601-617.'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sam67] Samuel, A. L., 1967. “使用跳棋游戏的机器学习研究。II – 近期进展，”《IBM研究与开发杂志》，第601-617页。'
- en: '[Sch13] Scherrer, B., 2013. “Performance Bounds for Lambda Policy Iteration
    and Application to the Game of Tetris,” J. of Machine Learning Research, Vol. 14,
    pp. 1181-1227.'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sch13] Scherrer, B., 2013. “Lambda策略迭代的性能界限及其在俄罗斯方块游戏中的应用，”《机器学习研究杂志》，第14卷，第1181-1227页。'
- en: '[Sch15] Schmidhuber, J., 2015. “Deep Learning in Neural Networks: An Overview,”
    Neural Networks, Vol. 61, pp. 85-117.'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sch15] Schmidhuber, J., 2015. “神经网络中的深度学习：概述，”《神经网络》，第61卷，第85-117页。'
- en: '[Sha50] Shannon, C., 1950.  “Programming a Digital Computer for Playing Chess,”
    Phil. Mag., Vol. 41, pp. 356-375.'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sha50] Shannon, C., 1950. “为数字计算机编程以玩国际象棋，”《哲学杂志》，第41卷，第356-375页。'
- en: '[SuB98] Sutton, R.  S., and Barto, A. G., 1998. Reinforcement Learning, MIT
    Press, Cambridge, MA. (A draft 2nd edition is available on-line.)'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '[SuB98] Sutton, R. S., 和 Barto, A. G., 1998. 《强化学习》，麻省理工学院出版社，剑桥，马萨诸塞州。（第二版草稿可在线获取。）'
- en: '[Sut88] Sutton, R.  S., 1988. “Learning to Predict by the Methods of Temporal
    Differences,” Machine Learning, Vol. 3, pp. 9-44.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sut88] Sutton, R. S., 1988. “通过时间差方法学习预测，”《机器学习》，第3卷，第9-44页。'
- en: '[Sze10] Szepesvari, C., 2010. Algorithms for Reinforcement Learning, Morgan
    and Claypool Publishers, San Franscisco, CA.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sze10] Szepesvari, C., 2010. 《强化学习算法》，Morgan and Claypool Publishers，旧金山，加州。'
- en: '[TeG96] Tesauro, G., and Galperin, G. R., 1996. “On-Line Policy Improvement
    Using Monte Carlo Search,” presented at the 1996 Neural Information Processing
    Systems Conference, Denver, CO; also in M. Mozer et al. (eds.), Advances in Neural
    Information Processing Systems 9, MIT Press (1997).'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '[TeG96] Tesauro, G., 和 Galperin, G. R., 1996. “使用蒙特卡罗搜索的在线策略改进，”在1996年神经信息处理系统会议上展示，丹佛，科罗拉多州；亦见
    M. Mozer 等（编），《神经信息处理系统进展 9》，麻省理工学院出版社（1997年）。'
- en: '[Tes89a] Tesauro, G. J., 1989\. “Neurogammon Wins Computer Olympiad,” Neural
    Computation, Vol. 1, pp. 321-323.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes89a] Tesauro, G. J., 1989. “Neurogammon 赢得计算机奥林匹克，”《神经计算》，第1卷，第321-323页。'
- en: '[Tes89b] Tesauro, G. J., 1989. “Connectionist Learning of Expert Preferences
    by Comparison Training,” in Advances in Neural Information Processing Systems,
    pp. 99-106.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes89b] Tesauro, G. J., 1989. “通过比较训练的连接主义学习专家偏好，”见《神经信息处理系统进展》，第99-106页。'
- en: '[Tes92] Tesauro, G. J., 1992. “Practical Issues in Temporal Difference Learning,”
    Machine Learning, Vol. 8, pp. 257-277.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes92] Tesauro, G. J., 1992. “时间差学习中的实际问题，”《机器学习》，第8卷，第257-277页。'
- en: '[Tes94] Tesauro, G. J., 1994. “TD-Gammon, a Self-Teaching Backgammon Program,
    Achieves Master-Level Play,” Neural Computation, Vol. 6, pp. 215-219.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes94] Tesauro, G. J., 1994. “TD-Gammon，一种自我教学的掼蛋程序，实现了大师级游戏水平，”《神经计算》，第6卷，第215-219页。'
- en: '[Tes95] Tesauro, G. J., 1995. “Temporal Difference Learning and TD-Gammon,”
    Communications of the ACM, Vol. 38, pp. 58-68.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes95] Tesauro, G. J., 1995. “时间差学习与 TD-Gammon，”《ACM通讯》，第38卷，第58-68页。'
- en: '[Tes01] Tesauro, G. J., 2001. “Comparison Training of Chess Evaluation Functions,”
    in Machines that Learn to Play Games, Nova Science Publishers, pp. 117-130.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes01] Tesauro, G. J., 2001. “国际象棋评估函数的比较训练，”见《学习玩游戏的机器》，Nova Science Publishers，第117-130页。'
- en: '[Tes02] Tesauro, G. J., 2002. “Programming Backgammon Using Self-Teaching Neural
    Nets,” Artificial Intelligence, Vol. 134, pp. 181-199.'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tes02] Tesauro, G. J., 2002. “使用自我教学神经网络编程掼蛋，”《人工智能》，第134卷，第181-199页。'
- en: '[TsV96] Tsitsiklis, J. N., and Van Roy, B., 1996\. “Feature-Based Methods for
    Large-Scale Dynamic Programming,” Machine Learning, Vol. 22, pp. 59-94.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[TsV96] Tsitsiklis, J. N., 和 Van Roy, B., 1996. “用于大规模动态规划的特征基础方法，”《机器学习》，第22卷，第59-94页。'
- en: '[Tsi94] Tsitsiklis, J. N., 1994. “Asynchronous Stochastic Approximation and
    Q-Learning,” Machine Learning, Vol. 16, pp. 185-202.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tsi94] Tsitsiklis, J. N., 1994. “异步随机逼近与 Q 学习，”《机器学习》，第16卷，第185-202页。'
- en: '[VVL13] Vrabie, V., Vamvoudakis, K. G., and Lewis, F. L., 2013. Optimal Adaptive
    Control and Differential Games by Reinforcement Learning Principles, The Institution
    of Engineering and Technology, London.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[VVL13] Vrabie, V., Vamvoudakis, K. G., 和 Lewis, F. L., 2013. 《基于强化学习原则的最优自适应控制与微分游戏》，工程与技术学会，伦敦。'
- en: '[Van06] Van Roy, B., 2006. “Performance Loss Bounds for Approximate Value Iteration
    with State Aggregation,” Mathematics of Operations Research, Vol. 31, pp. 234-244.'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '[Van06] Van Roy, B., 2006. “带状态聚合的近似值迭代的性能损失界限，”《运筹学数学》，第31卷，第234-244页。'
- en: '[Wer77] Werbös, P. J., 1977. “Advanced Forecasting Methods for Global Crisis
    Warning and Models of Intelligence,” General Systems Yearbook, Vol. 22, pp. 25-38.'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '[Wer77] Werbös, P. J., 1977. “全球危机预警的高级预测方法和智能模型，”《系统年鉴》，第22卷，第25-38页。'
- en: '[YuB04] Yu, H., and Bertsekas, D. P., 2004. “Discretized Approximations for
    POMDP with Average Cost,” Proc. of the 20th Conference on Uncertainty in Artificial
    Intelligence, Banff, Canada.'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '[YuB04] Yu, H., 和 Bertsekas, D. P., 2004. “带平均成本的POMDP的离散化近似，”第20届人工智能不确定性会议论文集，班夫，加拿大。'
- en: '[YuB09] Yu, H., and Bertsekas, D. P., 2009. “Basis Function Adaptation Methods
    for Cost Approximation in MDP,” Proceedings of 2009 IEEE Symposium on Approximate
    Dynamic Programming and Reinforcement Learning (ADPRL 2009), Nashville, Tenn.'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '[YuB09] Yu, H., 和 Bertsekas, D. P., 2009. “MDP中成本近似的基函数自适应方法，”2009年IEEE近似动态规划与强化学习研讨会（ADPRL
    2009）论文集，纳什维尔，田纳西州。'
- en: '[YuB10] Yu, H., and Bertsekas, D. P., 2010. “Error Bounds for Approximations
    from Projected Linear Equations,” Mathematics of Operations Research, Vol. 35,
    pp. 306-329.'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '[YuB10] Yu, H., 和 Bertsekas, D. P., 2010. “基于投影线性方程的近似误差界限，”《运筹学数学》，第35卷，第306-329页。'
- en: '[YuB12] Yu, H., and Bertsekas, D. P., 2012. “Weighted Bellman Equations and
    their Applications in Dynamic Programming,” Lab. for Information and Decision
    Systems Report LIDS-P-2876, MIT.'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '[YuB12] Yu, H., 和 Bertsekas, D. P., 2012. “加权贝尔曼方程及其在动态规划中的应用，”信息与决策系统实验室报告
    LIDS-P-2876，麻省理工学院。'
