- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:05:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.08349] Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.08349] 边缘计算与深度学习的融合：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.08349](https://ar5iv.labs.arxiv.org/html/1907.08349)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.08349](https://ar5iv.labs.arxiv.org/html/1907.08349)
- en: 'Convergence of Edge Computing and Deep Learning: A Comprehensive Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 边缘计算与深度学习的融合：全面调查
- en: 'Xiaofei Wang,  Yiwen Han,  Victor C.M. Leung,  Dusit Niyato,  Xueqiang Yan,
    Xu Chen Xiaofei Wang and Yiwen Han are with the College of Intelligence and Computing,
    Tianjin University, Tianjin, China. E-mails: xiaofeiwang@tju.edu.cn, hanyiwen@tju.edu.cn.V.
    C. M. Leung is with the College of Computer Science and Software Engineering,
    Shenzhen University, Shenzhen, China, and also with the Department of Electrical
    and Computer Engineering, the University of British Columbia, Vancouver, Canada.
    E-mail: vleung@ieee.org.Dusit Niyato is with School of Computer Science and Engineering,
    Nanyang Technological University, Singapore. E-mail: dniyato@ntu.edu.sg.Xueqiang
    Yan is with 2012 Lab of Huawei Technologies, Shenzhen, China. Email: yanxueqiang1@huawei.com.Xu
    Chen is with the School of Data and Computer Science, Sun Yat-sen University,
    Guangzhou, China. E-mail: chenxu35@mail.sysu.edu.cn.Corresponding author: Yiwen
    Han (hanyiwen@tju.edu.cn)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xiaofei Wang**、**Yiwen Han**、**Victor C.M. Leung**、**Dusit Niyato**、**Xueqiang
    Yan** 和 **Xu Chen** 均来自中国天津大学智能与计算学院。电子邮件：xiaofeiwang@tju.edu.cn、hanyiwen@tju.edu.cn。**V.
    C. M. Leung** 来自中国深圳大学计算机科学与软件工程学院，同时也在加拿大英属哥伦比亚大学电气与计算机工程系。电子邮件：vleung@ieee.org。**Dusit
    Niyato** 来自新加坡南洋理工大学计算机科学与工程学院。电子邮件：dniyato@ntu.edu.sg。**Xueqiang Yan** 在中国深圳华为技术有限公司2012实验室工作。电子邮件：yanxueqiang1@huawei.com。**Xu
    Chen** 来自中国中山大学数据与计算机学院。电子邮件：chenxu35@mail.sysu.edu.cn。通讯作者：**Yiwen Han** (hanyiwen@tju.edu.cn)'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Ubiquitous sensors and smart devices from factories and communities are generating
    massive amounts of data, and ever-increasing computing power is driving the core
    of computation and services from the cloud to the edge of the network. As an important
    enabler broadly changing people’s lives, from face recognition to ambitious smart
    factories and cities, developments of artificial intelligence (especially deep
    learning, DL) based applications and services are thriving. However, due to efficiency
    and latency issues, the current cloud computing service architecture hinders the
    vision of “providing artificial intelligence for every person and every organization
    at everywhere”. Thus, unleashing DL services using resources at the network edge
    near the data sources has emerged as a desirable solution. Therefore, edge intelligence,
    aiming to facilitate the deployment of DL services by edge computing, has received
    significant attention. In addition, DL, as the representative technique of artificial
    intelligence, can be integrated into edge computing frameworks to build intelligent
    edge for dynamic, adaptive edge maintenance and management. With regard to mutually
    beneficial edge intelligence and intelligent edge, this paper introduces and discusses:
    1) the application scenarios of both; 2) the practical implementation methods
    and enabling technologies, namely DL training and inference in the customized
    edge computing framework; 3) challenges and future trends of more pervasive and
    fine-grained intelligence. We believe that by consolidating information scattered
    across the communication, networking, and DL areas, this survey can help readers
    to understand the connections between enabling technologies while promoting further
    discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge
    DL.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 工厂和社区中普遍存在的传感器和智能设备正在生成大量数据，而不断增长的计算能力将计算和服务的核心从云端推向网络边缘。作为一种重要的推动力，广泛地改变了人们的生活，从面部识别到雄心勃勃的智能工厂和城市，基于人工智能（尤其是深度学习DL）的应用和服务正在蓬勃发展。然而，由于效率和延迟问题，目前的云计算服务架构阻碍了“为每个人和每个组织在任何地方提供人工智能”的愿景。因此，利用网络边缘靠近数据源的资源来释放深度学习服务已成为一种理想的解决方案。因此，边缘智能旨在通过边缘计算促进深度学习服务的部署，受到了广泛关注。此外，作为人工智能的代表技术，深度学习可以集成到边缘计算框架中，以构建智能边缘，实现动态、适应性的边缘维护和管理。关于相互促进的边缘智能和智能边缘，本文介绍并讨论了：1）两者的应用场景；2）实际实施方法和使能技术，即在定制的边缘计算框架中的深度学习训练和推理；3）更普遍和更精细智能的挑战与未来趋势。我们相信，通过整合散布在通信、网络和深度学习领域的信息，本调查可以帮助读者理解使能技术之间的联系，同时促进对边缘智能和智能边缘融合的进一步讨论，即边缘深度学习（Edge
    DL）。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Edge computing, deep learning, wireless communication, computation offloading,
    artificial intelligence
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算，深度学习，无线通信，计算卸载，人工智能
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'With the proliferation of computing and storage devices, from server clusters
    in cloud data centers (the cloud) to personal computers and smartphones, further,
    to wearable and other Internet of Things (IoT) devices, we are now in an information-centric
    era in which computing is ubiquitous and computation services are overflowing
    from the cloud to the edge. According to a Cisco white paper [[1](#bib.bib1)],
    $50$ billion IoT devices will be connected to the Internet by 2020\. On the other
    hand, Cisco estimates that nearly $850$ Zettabytes (ZB) of data will be generated
    each year outside the cloud by 2021, while global data center traffic is only
    $20.6$ ZB [[2](#bib.bib2)]. This indicates that data sources for big data are
    also undergoing a transformation: from large-scale cloud data centers to an increasingly
    wide range of edge devices. However, existing cloud computing is gradually unable
    to manage these massively distributed computing power and analyze their data:
    1) a large number of computation tasks need to be delivered to the cloud for processing
    [[3](#bib.bib3)], which undoubtedly poses serious challenges on network capacity
    and the computing power of cloud computing infrastructures; 2) many new types
    of applications, e.g., cooperative autonomous driving, have strict or tight delay
    requirements that the cloud would have difficulty meeting since it may be far
    away from the users [[4](#bib.bib4)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算和存储设备的普及，从云数据中心的服务器集群（云端）到个人计算机和智能手机，再到可穿戴设备和其他物联网（IoT）设备，我们现在处于一个信息为中心的时代，其中计算无处不在，计算服务从云端溢出到边缘。根据思科的白皮书[[1](#bib.bib1)]，到2020年，将有$50$亿个IoT设备连接到互联网。另一方面，思科预计到2021年，每年在云端之外将产生近$850$泽字节（ZB）的数据，而全球数据中心流量仅为$20.6$
    ZB[[2](#bib.bib2)]。这表明大数据的数据来源也正在经历转型：从大规模的云数据中心到范围越来越广泛的边缘设备。然而，现有的云计算逐渐无法管理这些大规模分布的计算能力并分析其数据：1）大量计算任务需要传送到云端处理[[3](#bib.bib3)]，这无疑对网络容量和云计算基础设施的计算能力提出了严峻挑战；2）许多新类型的应用，例如协作自动驾驶，有严格或紧迫的延迟要求，云端可能因距离用户较远而难以满足这些要求[[4](#bib.bib4)]。
- en: 'Therefore, edge computing [[5](#bib.bib5), [6](#bib.bib6)] emerges as an attractive
    alternative, especially to host computation tasks as close as possible to the
    data sources and end users. Certainly, edge computing and cloud computing are
    not mutually exclusive [[7](#bib.bib7), [8](#bib.bib8)]. Instead, the edge complements
    and extends the cloud. Compared with cloud computing only, the main advantages
    of edge computing combined with cloud computing are three folds: 1) backbone network
    alleviation, distributed edge computing nodes can handle a large number of computation
    tasks without exchanging the corresponding data with the cloud, thus alleviating
    the traffic load of the network; 2) agile service response, services hosted at
    the edge can significantly reduce the delay of data transmissions and improve
    the response speed; 3) powerful cloud backup, the cloud can provide powerful processing
    capabilities and massive storage when the edge cannot afford.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，边缘计算[[5](#bib.bib5), [6](#bib.bib6)]作为一种有吸引力的替代方案出现，特别是为了将计算任务尽可能靠近数据源和最终用户。当然，边缘计算和云计算并不是相互排斥的[[7](#bib.bib7),
    [8](#bib.bib8)]。相反，边缘计算补充和扩展了云计算。与单独的云计算相比，边缘计算结合云计算的主要优势有三点：1）骨干网络缓解，分布式边缘计算节点可以处理大量计算任务而无需与云端交换相应的数据，从而缓解网络流量负载；2）灵活的服务响应，托管在边缘的服务可以显著减少数据传输延迟，提高响应速度；3）强大的云端备份，当边缘无法承担时，云端可以提供强大的处理能力和海量存储。
- en: As a typical and more widely used new form of applications [[9](#bib.bib9)],
    various deep learning-based intelligent services and applications have changed
    many aspects of people’s lives due to the great advantages of Deep Learning (DL)
    in the fields of Computer Vision (CV) and Natural Language Processing (NLP) [[10](#bib.bib10)].
    These achievements are not only derived from the evolution of DL but also inextricably
    linked to increasing data and computing power. Nevertheless, for a wider range
    of application scenarios, such as smart cities, Internet of Vehicles (IoVs), etc.,
    there are only a limited number of intelligent services offered due to the following
    factors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种典型且更广泛使用的新型应用 [[9](#bib.bib9)]，各种基于深度学习的智能服务和应用由于深度学习（DL）在计算机视觉（CV）和自然语言处理（NLP）领域的巨大优势，已经改变了人们生活的许多方面
    [[10](#bib.bib10)]。这些成就不仅源于DL的演进，还与数据和计算能力的增加密不可分。然而，对于更广泛的应用场景，如智能城市、车联网（IoVs）等，由于以下因素，提供的智能服务数量仍然有限。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Cost: training and inference of DL models in the cloud requires devices or
    users to transmit massive amounts of data to the cloud, thus consuming a large
    amount of network bandwidth;'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成本：在云中训练和推理深度学习（DL）模型需要设备或用户将大量数据传输到云端，从而消耗大量网络带宽；
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Latency: the delay to access cloud services is generally not guaranteed and
    might not be short enough to satisfy the requirements of many time-critical applications
    such as cooperative autonomous driving [[11](#bib.bib11)];'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 延迟：访问云服务的延迟通常无法保证，可能不足以满足许多时间关键型应用的要求，例如协同自动驾驶 [[11](#bib.bib11)];
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reliability: most cloud computing applications relies on wireless communications
    and backbone networks for connecting users to services, but for many industrial
    scenarios, intelligent services must be highly reliable, even when network connections
    are lost;'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可靠性：大多数云计算应用依赖于无线通信和骨干网络将用户与服务连接起来，但对于许多工业场景，智能服务必须具有很高的可靠性，即使在网络连接丢失的情况下也是如此；
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Privacy: the data required for DL might carry a lot of private information,
    and privacy issues are critical to areas such as smart home and cities.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私：深度学习所需的数据可能包含大量私人信息，隐私问题在智能家居和智能城市等领域至关重要。
- en: 'TABLE I: List of Important Abbreviations in Alphabetical Order'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：按字母顺序列出的重要缩写列表
- en: '| Abbr. | Definition | Abbr. | Definition | Abbr. | Definition |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 定义 | 缩写 | 定义 | 缩写 | 定义 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| A-LSH | Adaptive Locality Sensitive Hashing | DVFS | Dynamic Voltage and
    Frequency Scaling | NLP | Natural Language Processing |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| A-LSH | 自适应局部敏感哈希 | DVFS | 动态电压和频率缩放 | NLP | 自然语言处理 |'
- en: '| AC | Actor-Critic | ECSP | Edge Computing Service Provider | NN | Neural
    Network |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| AC | Actor-Critic | ECSP | 边缘计算服务提供商 | NN | 神经网络 |'
- en: '| A3C | Asynchronous Advantage Actor-Critic | EEoI | Early Exit of Inference
    | NPU | Neural Processing Unit |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| A3C | 异步优势演员-评论员 | EEoI | 推理早期退出 | NPU | 神经处理单元 |'
- en: '| AE | Auto-Encoder | EH | Energy Harvesting | PPO | Proximate Policy Optimization
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| AE | 自编码器 | EH | 能量采集 | PPO | 近端策略优化 |'
- en: '| AI | Artificial Intelligence | FAP | Fog radio Access Point | QoE | Quality
    of Experience |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| AI | 人工智能 | FAP | 雾计算无线接入点 | QoE | 体验质量 |'
- en: '| APU | AI Processing Unit | FCNN | Fully Connected Neural Network | QoS |
    Quality of Service |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| APU | AI处理单元 | FCNN | 全连接神经网络 | QoS | 服务质量 |'
- en: '| AR | Augmented Reality | FL | Federated Learning | RAM | Random Access Memory
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| AR | 增强现实 | FL | 联邦学习 | RAM | 随机存取存储器 |'
- en: '| ASIC | Application-Specific Integrated Circuit | FPGA | Field Programmable
    Gate Array | RNN | Recurrent Neural Network |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| ASIC | 应用特定集成电路 | FPGA | 现场可编程门阵列 | RNN | 循环神经网络 |'
- en: '| BS | Base Station | FTP | Fused Tile Partitioning | RoI | Region-of-Interest
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| BS | 基站 | FTP | 融合瓷砖分区 | RoI | 关注区域 |'
- en: '| C-RAN | Cloud-Radio Access Networks | GAN | Generative Adversarial Network
    | RRH | Remote Radio Head |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| C-RAN | 云无线接入网络 | GAN | 生成对抗网络 | RRH | 遥控无线电头 |'
- en: '| CDN | Content Delivery Network | GNN | Graph Neural Network | RSU | Road-Side
    Unit |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| CDN | 内容分发网络 | GNN | 图神经网络 | RSU | 道路边单元 |'
- en: '| CNN | Convolutional Neural Network | IID | Independent and Identically Distributed
    | SDN | Software-Defined Network |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 | IID | 独立同分布 | SDN | 软件定义网络 |'
- en: '| CV | Computer Vision | IoT | Internet of Things | SGD | Stochastic Gradient
    Descent |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| CV | 计算机视觉 | IoT | 物联网 | SGD | 随机梯度下降 |'
- en: '| DAG | Directed Acyclic Graph | IoV | Internet of Vehicles | SINR | Signal-to-Interference-plus-Noise
    Ratio |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| DAG | 有向无环图 | IoV | 车联网 | SINR | 信号干扰加噪声比 |'
- en: '| D2D | Device-to-Device | KD | Knowledge Distillation | SNPE | Snapdragon
    Neural Processing Engine |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| D2D | 设备到设备 | KD | 知识蒸馏 | SNPE | 骁龙神经处理引擎 |'
- en: '| DDoS | Distributed Denial of Service | $k$NN | $k$-Nearest Neighbor | TL
    | Transfer Learning |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| DDoS | 分布式拒绝服务 | $k$NN | $k$-最近邻 | TL | 迁移学习 |'
- en: '| DDPG | Deep Deterministic Policy Gradient | MAB | Multi-Armed Bandit | UE
    | User Equipment |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| DDPG | 深度确定性策略梯度 | MAB | 多臂赌博机 | UE | 用户设备 |'
- en: '| DL | Deep Learning | MEC | Mobile (Multi-access) Edge Computing | VM | Virtual
    Machine |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| DL | 深度学习 | MEC | 移动（多接入）边缘计算 | VM | 虚拟机 |'
- en: '| DNN | Deep Neural Networks | MDC | Micro Data Center | VNF | Virtual Network
    Function |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 | MDC | 微型数据中心 | VNF | 虚拟网络功能 |'
- en: '| DQL | Deep Q-Learning | MDP | Markov Decision Process | V2V | Vehicle-to-Vehicle
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| DQL | 深度Q学习 | MDP | 马尔可夫决策过程 | V2V | 车对车 |'
- en: '| DRL | Deep Reinforcement Learning | MLP | Multi-Layer Perceptron | WLAN |
    Wireless Local Area Network |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| DRL | 深度强化学习 | MLP | 多层感知器 | WLAN | 无线局域网 |'
- en: '| DSL | Domain-specific Language | NFV | Network Functions Virtualizatio |
    ZB | Zettabytes |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| DSL | 特定领域语言 | NFV | 网络功能虚拟化 | ZB | 硬盘字节 |'
- en: '![Refer to caption](img/b7244ece9821d1edd97eed575f5d6fb2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b7244ece9821d1edd97eed575f5d6fb2.png)'
- en: 'Figure 1: Edge intelligence and intelligent edge.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：边缘智能与智能边缘。
- en: 'Since the edge is closer to users than the cloud, edge computing is expected
    to solve many of these issues. In fact, edge computing is gradually being combined
    with Artificial Intelligence (AI), benefiting each other in terms of the realization
    of edge intelligence and intelligent edge as depicted in Fig. [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"). Edge intelligence and intelligent edge are not independent of each other.
    Edge intelligence is the goal, and the DL services in intelligent edge are also
    a part of edge intelligence. In turn, intelligent edge can provide higher service
    throughput and resource utilization for edge intelligence.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于边缘离用户比云更近，边缘计算有望解决这些问题。实际上，边缘计算正逐渐与人工智能（AI）结合，相互促进，如图[1](#S1.F1 "图1 ‣ I 介绍
    ‣ 边缘计算与深度学习的融合：综合调查")所示。边缘智能与智能边缘并不是彼此独立的。边缘智能是目标，而智能边缘中的深度学习服务也是边缘智能的一部分。反过来，智能边缘可以为边缘智能提供更高的服务吞吐量和资源利用率。
- en: 'To be specific, on one hand, edge intelligence is expected to push DL computations
    from the cloud to the edge as much as possible, thus enabling various distributed,
    low-latency and reliable intelligent services. As shown in Fig. [2](#S1.F2 "Figure
    2 ‣ I Introduction ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"), the advantages include: 1) DL services are deployed close to the requesting
    users, and the cloud only participates when additional processing is required
    [[12](#bib.bib12)], hence significantly reducing the latency and cost of sending
    data to the cloud for processing; 2) since the raw data required for DL services
    is stored locally on the edge or user devices themselves instead of the cloud,
    protection of user privacy is enhanced; 3) the hierarchical computing architecture
    provides more reliable DL computation; 4) with richer data and application scenarios,
    edge computing can promote the pervasive application of DL and realize the prospect
    of “providing AI for every person and every organization at everywhere” [[13](#bib.bib13)];
    5) diversified and valuable DL services can broaden the commercial value of edge
    computing and accelerate its deployment and growth.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，一方面，边缘智能有望将深度学习计算从云端尽可能推向边缘，从而实现各种分布式、低延迟和可靠的智能服务。如图[2](#S1.F2 "图2 ‣ I
    介绍 ‣ 边缘计算与深度学习的融合：综合调查")所示，优势包括：1）深度学习服务部署在靠近请求用户的地方，云端仅在需要额外处理时参与[[12](#bib.bib12)]，从而显著减少将数据发送到云端进行处理的延迟和成本；2）由于深度学习服务所需的原始数据存储在边缘或用户设备上而不是云端，用户隐私保护得到增强；3）分层计算架构提供了更可靠的深度学习计算；4）凭借更丰富的数据和应用场景，边缘计算可以促进深度学习的广泛应用，实现“为每个人和每个组织在任何地方提供AI”的前景[[13](#bib.bib13)]；5）多样化和有价值的深度学习服务可以拓宽边缘计算的商业价值，加速其部署和增长。
- en: '![Refer to caption](img/ee121dd0618fa521bce6c6e722ec7817.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ee121dd0618fa521bce6c6e722ec7817.png)'
- en: 'Figure 2: Capabilities comparison of cloud, on-device and edge intelligence.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：云、设备端和边缘智能的能力对比。
- en: 'On the other hand, intelligent edge aims to incorporate DL into the edge for
    dynamic, adaptive edge maintenance and management. With the development of communication
    technology, network access methods are becoming more diverse. At the same time,
    the edge computing infrastructure acts as an intermediate medium, making the connection
    between ubiquitous end devices and the cloud more reliable and persistent [[14](#bib.bib14)].
    Thus the end devices, edge, and cloud are gradually merging into a community of
    shared resources. However, the maintenance and management of such a large and
    complex overall architecture (community) involving wireless communication, networking,
    computing, storage, etc., is a major challenge [[15](#bib.bib15)]. Typical network
    optimization methodologies rely on fixed mathematical models; however, it is difficult
    to accurately model rapidly changing edge network environments and systems. DL
    is expected to deal with this problem: when faced with complex and cumbersome
    network information, DL can rely on its powerful learning and reasoning ability
    to extract valuable information from data and make adaptive decisions, achieving
    intelligent maintenance and management accordingly.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，智能边缘旨在将深度学习融入边缘，实现动态、自适应的边缘维护和管理。随着通信技术的发展，网络接入方式变得更加多样。同时，边缘计算基础设施充当了中介，使得无处不在的终端设备与云之间的连接更加可靠和持久[[14](#bib.bib14)]。因此，终端设备、边缘和云逐渐融合成一个共享资源的社区。然而，涉及无线通信、网络、计算、存储等的大规模复杂整体架构（社区）的维护和管理是一个重大挑战[[15](#bib.bib15)]。典型的网络优化方法依赖于固定的数学模型，但准确建模快速变化的边缘网络环境和系统是困难的。深度学习被期望解决这个问题：面对复杂繁琐的网络信息时，深度学习可以依靠其强大的学习和推理能力，从数据中提取有价值的信息，并做出自适应决策，从而实现智能维护和管理。
- en: 'Therefore, considering that edge intelligence and intelligent edge, i.e., Edge
    DL, together face some of the same challenges and practical issues in multiple
    aspects, we identify the following five technologies that are essential for Edge
    DL:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到边缘智能和智能边缘，即边缘深度学习，共同面临多个方面的一些相同挑战和实际问题，我们确定了以下五项对边缘深度学习至关重要的技术：
- en: 1)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1)
- en: DL applications on Edge, technical frameworks for systematically organizing
    edge computing and DL to provide intelligent services;
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边缘上的深度学习应用，系统化组织边缘计算和深度学习以提供智能服务的技术框架；
- en: 2)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2)
- en: DL inference in Edge, focusing on the practical deployment and inference of
    DL in the edge computing architecture to fulfill different requirements, such
    as accuracy and latency;
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边缘深度学习推理，专注于边缘计算架构中深度学习的实际部署和推理，以满足不同的需求，如准确性和延迟；
- en: 3)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3)
- en: Edge computing for DL, which adapts the edge computing platform in terms of
    network architecture, hardware and software to support DL computation;
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习的边缘计算，指的是在网络架构、硬件和软件方面调整边缘计算平台以支持深度学习计算；
- en: 4)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4)
- en: DL training at Edge, training DL models for edge intelligence at distributed
    edge devices under resource and privacy constraints;
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边缘深度学习训练，在资源和隐私约束下，在分布式边缘设备上训练边缘智能的深度学习模型；
- en: 5)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5)
- en: DL for optimizing Edge, application of DL for maintaining and managing different
    functions of edge computing networks (systems), e.g., edge caching [[16](#bib.bib16)],
    computation offloading[[17](#bib.bib17)].
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于优化边缘的深度学习（DL），即应用深度学习来维护和管理边缘计算网络（系统）的不同功能，例如边缘缓存[[16](#bib.bib16)]、计算卸载[[17](#bib.bib17)]。
- en: 'As illustrated in Fig. [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey"), “DL applications
    on Edge” and “DL for optimizing edge” correspond to the theoretical goals of edge
    intelligence and intelligent edge, respectively. To support them, various DL models
    should be trained by intensive computation at first. In this case, for the related
    works leveraging edge computing resources to train various DL models, we classify
    them as “DL training at Edge”. Second, to enable and speed up Edge DL services,
    we focus on a variety of techniques supporting the efficient inference of DL models
    in edge computing frameworks and networks, called “DL inference in Edge”. At last,
    we classify all techniques, which adapts edge computing frameworks and networks
    to better serve Edge DL, as “Edge computing for DL”.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S1.F3 "Figure 3 ‣ I Introduction ‣ Convergence of Edge Computing and
    Deep Learning: A Comprehensive Survey") 所示，“边缘上的 DL 应用”和“用于优化边缘的 DL”分别对应边缘智能和智能边缘的理论目标。为了支持这些目标，首先需要通过密集的计算训练各种
    DL 模型。在这种情况下，对于利用边缘计算资源训练各种 DL 模型的相关工作，我们将其归类为“边缘上的 DL 训练”。其次，为了实现并加速边缘 DL 服务，我们关注支持
    DL 模型在边缘计算框架和网络中高效推理的各种技术，称为“边缘中的 DL 推理”。最后，我们将所有调整边缘计算框架和网络以更好地服务于边缘 DL 的技术归类为“用于
    DL 的边缘计算”。'
- en: '![Refer to caption](img/94240842679c6f9cb97a4d8b299a33c5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/94240842679c6f9cb97a4d8b299a33c5.png)'
- en: 'Figure 3: Landscape of Edge DL according to the proposed taxonomy.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 根据提出的分类法，边缘 DL 的全景。'
- en: 'To the best of our knowledge, existing articles that are most related to our
    work include [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Different from our more extensive coverage of Edge DL, [[18](#bib.bib18)] is focussed
    on the use of machine learning (rather than DL) in edge intelligence for wireless
    communication perspective, i.e., training machine learning at the network edge
    to improve wireless communication. Besides, discussions about DL inference and
    training are the main contribution of [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Different from these works, this survey focuses on these respects: 1) comprehensively
    consider deployment issues of DL by edge computing, spanning networking, communication,
    and computation; 2) investigate the holistic technical spectrum about the convergence
    of DL and edge computing in terms of the five enablers; 3) point out that DL and
    edge computing are beneficial to each other and considering only deploying DL
    on the edge is incomplete.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，现有与我们工作最相关的文章包括 [[18](#bib.bib18)、[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)]。不同于我们对边缘
    DL 更广泛的覆盖，[[18](#bib.bib18)] 专注于机器学习（而非 DL）在无线通信视角下的边缘智能应用，即在网络边缘训练机器学习以提高无线通信。此外，关于
    DL 推理和训练的讨论是 [[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)] 的主要贡献。不同于这些工作，本综述关注以下方面：1)
    全面考虑边缘计算下 DL 的部署问题，涵盖网络、通信和计算；2) 研究 DL 与边缘计算的融合在五个促进因素方面的整体技术谱；3) 指出 DL 和边缘计算相互促进，仅在边缘上部署
    DL 是不完整的。
- en: 'This paper is organized as follows (as abstracted in Fig. [4](#S1.F4 "Figure
    4 ‣ I Introduction ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")). We have given the background and motivations of this survey in the
    current section. Next, we provide some fundamentals related to edge computing
    and DL in Section [II](#S2 "II Fundamentals of Edge Computing ‣ Convergence of
    Edge Computing and Deep Learning: A Comprehensive Survey") and Section [III](#S3
    "III Fundamentals of Deep Learning ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"), respectively. The following sections introduce the five
    enabling technologies, i.e., DL applications on edge (Section [IV](#S4 "IV Deep
    Learning Applications on Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")), DL inference in edge (Section [V](#S5 "V Deep Learning
    Inference in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")), edge computing for DL services (Section [VI](#S6 "VI Edge Computing
    for Deep Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")), DL training at edge (Section [VII](#S7 "VII Deep Learning Training
    at Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")),
    and DL for optimizing edge (Section [VIII](#S8 "VIII Deep Learning for Optimizing
    Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")).
    Finally, we present lessons learned and discuss open challenges in Section [IX](#S9
    "IX Lessons Learned and Open Challenges ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey") and conclude this paper in Section [X](#S10
    "X Conclusions ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"). All related acronyms are listed in Table [I](#S1.T1 "TABLE I ‣ I Introduction
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的组织结构如下（如图 [4](#S1.F4 "Figure 4 ‣ I Introduction ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey") 所示）。我们在当前章节中介绍了本次调查的背景和动机。接下来，我们在第
    [II](#S2 "II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and
    Deep Learning: A Comprehensive Survey") 节和第 [III](#S3 "III Fundamentals of Deep
    Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")
    节中分别提供了与边缘计算和深度学习相关的一些基础知识。接下来的章节介绍了五项关键技术，即边缘上的深度学习应用（第 [IV](#S4 "IV Deep Learning
    Applications on Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey") 节）、边缘中的深度学习推理（第 [V](#S5 "V Deep Learning Inference in Edge ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey") 节）、用于深度学习服务的边缘计算（第
    [VI](#S6 "VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey") 节）、边缘上的深度学习训练（第 [VII](#S7 "VII Deep
    Learning Training at Edge ‣ Convergence of Edge Computing and Deep Learning: A
    Comprehensive Survey") 节）以及用于优化边缘的深度学习（第 [VIII](#S8 "VIII Deep Learning for Optimizing
    Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")
    节）。最后，我们在第 [IX](#S9 "IX Lessons Learned and Open Challenges ‣ Convergence of Edge
    Computing and Deep Learning: A Comprehensive Survey") 节总结了所学到的经验和讨论了开放挑战，并在第 [X](#S10
    "X Conclusions ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey") 节中结束本文。所有相关的缩略词列在表 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey") 中。'
- en: '![Refer to caption](img/e5f9dbf7935687dc7c4d4e95bba49483.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e5f9dbf7935687dc7c4d4e95bba49483.png)'
- en: 'Figure 4: Conceptual relationships of edge intelligence and intelligent edge.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：边缘智能与智能边缘的概念关系。
- en: II Fundamentals of Edge Computing
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 边缘计算基础知识
- en: Edge computing has become an important solution to break the bottleneck of emerging
    technologies by virtue of its advantages of reducing data transmission, improving
    service latency and easing cloud computing pressure. The edge computing architecture
    will become an important complement to the cloud, even replacing the role of the
    cloud in some scenarios. More detailed information can be found in [[22](#bib.bib22),
    [8](#bib.bib8), [23](#bib.bib23)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算凭借其减少数据传输、提高服务延迟和减轻云计算压力的优势，已成为突破新兴技术瓶颈的重要解决方案。边缘计算架构将成为云计算的重要补充，甚至在某些场景中取代云计算的角色。更多详细信息请参见
    [[22](#bib.bib22), [8](#bib.bib8), [23](#bib.bib23)]。
- en: II-A Paradigms of Edge Computing
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 边缘计算的范式
- en: In the development of edge computing, there have been various new technologies
    aimed at working at the edge of the network, with the same principles but different
    focuses, such as Cloudlet [[24](#bib.bib24)], Micro Data Centers (MDCs) [[25](#bib.bib25)],
    Fog Computing [[26](#bib.bib26)][[27](#bib.bib27)] and Mobile Edge Computing [[5](#bib.bib5)]
    (viz., Multi-access Edge Computing [[28](#bib.bib28)] now). However, the edge
    computing community has not yet reached a consensus on the standardized definitions,
    architectures and protocols of edge computing [[23](#bib.bib23)]. We use a common
    term “edge computing” for this set of emerging technologies. In this section,
    different edge computing concepts are introduced and differentiated.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘计算的发展中，出现了各种新技术，旨在在网络边缘工作，虽然原则相同但重点不同，如云集群 [[24](#bib.bib24)]、微数据中心（MDCs）
    [[25](#bib.bib25)]、雾计算 [[26](#bib.bib26)][[27](#bib.bib27)] 和移动边缘计算 [[5](#bib.bib5)]（即现在的多接入边缘计算
    [[28](#bib.bib28)]）。然而，边缘计算社区尚未就边缘计算的标准定义、架构和协议达成共识 [[23](#bib.bib23)]。我们使用“边缘计算”这一通用术语来指代这一系列新兴技术。在本节中，将介绍并区分不同的边缘计算概念。
- en: II-A1 Cloudlet and Micro Data Centers
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 云集群和微数据中心
- en: Cloudlet is a network architecture element that combines mobile computing and
    cloud computing. It represents the middle layer of the three-tier architecture,
    i.e., mobile devices, the micro cloud, and the cloud. Its highlights are efforts
    to 1) define the system and create algorithms that support low-latency edge cloud
    computing, and 2) implement related functionality in open source code as an extension
    of Open Stack cloud management software [[24](#bib.bib24)]. Similar to Cloudlets,
    MDCs [[25](#bib.bib25)] are also designed to complement the cloud. The idea is
    to package all the computing, storage, and networking equipment needed to run
    customer applications in one enclosure, as a stand-alone secure computing environment,
    for applications that require lower latency or end devices with limited battery
    life or computing abilities.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 云集群是一个结合移动计算和云计算的网络架构元素。它代表三层架构的中间层，即移动设备、微云和云。它的亮点在于 1) 定义系统并创建支持低延迟边缘云计算的算法，以及
    2) 将相关功能实现为开源代码，作为 Open Stack 云管理软件的扩展 [[24](#bib.bib24)]。类似于云集群，MDCs [[25](#bib.bib25)]
    也旨在补充云计算。其理念是将所有运行客户应用所需的计算、存储和网络设备打包在一个封装中，作为一个独立的安全计算环境，用于需要较低延迟或具有有限电池寿命或计算能力的终端设备的应用。
- en: II-A2 Fog Computing
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 雾计算
- en: One of the highlights of fog computing is that it assumes a fully distributed
    multi-tier cloud computing architecture with billions of devices and large-scale
    cloud data centers [[26](#bib.bib26)][[27](#bib.bib27)]. While cloud and fog paradigms
    share a similar set of services, such as computing, storage, and networking, the
    deployment of fog is targeted to specific geographic areas. In addition, fog is
    designed for applications that require real-time responding with less latency,
    such as interactive and IoT applications. Unlike Cloudlet, MDCs and MEC, fog computing
    is more focused on IoTs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 雾计算的一个亮点是它假设一个完全分布式的多层云计算架构，包含数十亿个设备和大规模的云数据中心 [[26](#bib.bib26)][[27](#bib.bib27)]。尽管云计算和雾计算范式共享类似的服务，如计算、存储和网络，雾计算的部署则针对特定的地理区域。此外，雾计算设计用于需要实时响应且延迟较低的应用程序，如交互式和物联网应用。与云集群、MDCs
    和 MEC 不同，雾计算更加关注物联网。
- en: II-A3 Mobile (Multi-access) Edge Computing (MEC)
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 移动（多接入）边缘计算（MEC）
- en: Mobile Edge Computing places computing capabilities and service environments
    at the edge of cellular networks [[5](#bib.bib5)]. It is designed to provide lower
    latency, context and location awareness, and higher bandwidth. Deploying edge
    servers on cellular Base Stations (BSs) allows users to deploy new applications
    and services flexibly and quickly. The European Telecommunications Standards Institute
    (ETSI) further extends the terminology of MEC from Mobile Edge Computing to Multi-access
    Edge Computing by accommodating more wireless communication technologies, such
    as Wi-Fi [[28](#bib.bib28)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 移动边缘计算将计算能力和服务环境置于蜂窝网络的边缘 [[5](#bib.bib5)]。它旨在提供较低的延迟、上下文和位置感知，以及更高的带宽。在蜂窝基站（BSs）上部署边缘服务器允许用户灵活、快速地部署新的应用程序和服务。欧洲电信标准协会（ETSI）进一步将
    MEC 的术语从移动边缘计算扩展到多接入边缘计算，以适应更多的无线通信技术，如 Wi-Fi [[28](#bib.bib28)]。
- en: II-A4 Definition of Edge Computing Terminologies
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 边缘计算术语定义
- en: 'The definition and division of edge devices are ambiguous in most literature
    (the boundary between edge nodes and end devices is not clear). For this reason,
    as depicted in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Convergence of Edge
    Computing and Deep Learning: A Comprehensive Survey"), we further divide common
    edge devices into end devices and edge nodes: the “end devices” (end level) is
    used to refer to mobile edge devices (including smartphones, smart vehicles, etc.)
    and various IoT devices, and the “edge nodes” (edge level) include Cloudlets,
    Road-Side Units (RSUs), Fog nodes, edge servers, MEC servers and so on, namely
    servers deployed at the edge of the network.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘设备的定义和划分在大多数文献中模糊（边缘节点和终端设备之间的界限不清）。因此，如图 [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey") 所示，我们进一步将常见的边缘设备划分为终端设备和边缘节点：“终端设备”（终端层）指移动边缘设备（包括智能手机、智能车辆等）和各种IoT设备，“边缘节点”（边缘层）包括Cloudlets、路边单元（RSUs）、雾节点、边缘服务器、MEC服务器等，即部署在网络边缘的服务器。'
- en: '![Refer to caption](img/7efad9cd67c75ec67150e8045d3ccfae.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7efad9cd67c75ec67150e8045d3ccfae.png)'
- en: 'Figure 5: A sketch of collaborative end-edge-cloud DL computing.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：协作的端-边缘-云深度学习计算的示意图。
- en: '![Refer to caption](img/386b118464424e2d426e4d46c6aaf389.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/386b118464424e2d426e4d46c6aaf389.png)'
- en: 'Figure 6: Computation collaboration is becoming more important for DL with
    respect to both training and inference.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在训练和推理方面，计算协作对深度学习变得越来越重要。
- en: 'TABLE II: Summary of Edge Computing AI Hardwares and Systems'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：边缘计算AI硬件和系统汇总
- en: '|  | Owner | Production | Feature |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 供应商 | 产品 | 特性 |'
- en: '| Integrated Commodities | Microsoft | Data Box Edge [[29](#bib.bib29)] | Competitive
    in data preprocessing and data transmission |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 集成商品 | 微软 | Data Box Edge [[29](#bib.bib29)] | 在数据预处理和数据传输方面具有竞争力 |'
- en: '| Intel |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 英特尔 |'
- en: '&#124; Movidius Neural &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Movidius神经网络 &#124;'
- en: '&#124; Compute Stick [[30](#bib.bib30)] &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Compute Stick [[30](#bib.bib30)] &#124;'
- en: '| Prototype on any platform with plug-and-play simplicity |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 在任何平台上进行原型设计，具有即插即用的简便性 |'
- en: '| NVIDIA | Jetson [[31](#bib.bib31)] | Easy-to-use platforms that runs in as
    little as 5 Watts |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA | Jetson [[31](#bib.bib31)] | 易于使用的平台，运行功耗低至5瓦 |'
- en: '|  | Huawei | Atlas Series [[32](#bib.bib32)] | An all-scenario AI infrastructure
    solution that bridges “device, edge, and cloud” |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 华为 | Atlas系列 [[32](#bib.bib32)] | 一种全场景的AI基础设施解决方案，连接“设备、边缘和云” |'
- en: '| AI Hardware for Edge Computing | Qualcomm | Snapdragon 8 Series [[33](#bib.bib33)]
    | Powerful adaptability to major DL frameworks |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 边缘计算AI硬件 | 高通 | Snapdragon 8系列 [[33](#bib.bib33)] | 对主要深度学习框架具有强大的适应性 |'
- en: '| HiSilicon | Kirin 600/900 Series [[34](#bib.bib34)] | Independent NPU for
    DL computation |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| HiSilicon | Kirin 600/900系列 [[34](#bib.bib34)] | 独立的NPU用于深度学习计算 |'
- en: '| HiSilicon | Ascend Series [[35](#bib.bib35)] |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| HiSilicon | Ascend系列 [[35](#bib.bib35)] |'
- en: '&#124; Full coverage – from the ultimate low energy consumption scenario &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全面覆盖 – 从极低能耗场景 &#124;'
- en: '&#124; to high computing power scenario &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高计算能力场景 &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| MediaTek | Helio P60 [[36](#bib.bib36)] | Simultaneous use of GPU and NPU
    to accelerate neural network computing |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 联发科 | Helio P60 [[36](#bib.bib36)] | GPU和NPU的同时使用以加速神经网络计算 |'
- en: '| NVIDIA | Turing GPUs [[37](#bib.bib37)] | Powerful capabilities and compatibility
    but with high energy consumption |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA | Turing GPUs [[37](#bib.bib37)] | 强大的能力和兼容性但能耗较高 |'
- en: '| Google | TPU [[38](#bib.bib38)] | Stable in terms of performance and power
    consumption |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 谷歌 | TPU [[38](#bib.bib38)] | 性能和功耗方面稳定 |'
- en: '| Intel | Xeon D-2100 [[39](#bib.bib39)] | Optimized for power- and space-constrained
    cloud-edge solutions |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 英特尔 | Xeon D-2100 [[39](#bib.bib39)] | 针对电源和空间受限的云边解决方案进行了优化 |'
- en: '|  | Samsung | Exynos 9820 [[40](#bib.bib40)] | Mobile NPU for accelerating
    AI tasks |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 三星 | Exynos 9820 [[40](#bib.bib40)] | 用于加速AI任务的移动NPU |'
- en: '| Edge Computing Frameworks | Huawei | KubeEdge [[41](#bib.bib41)] | Native
    support for edge-cloud collaboration |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 边缘计算框架 | 华为 | KubeEdge [[41](#bib.bib41)] | 原生支持边缘-云协作 |'
- en: '| Baidu | OpenEdge [[42](#bib.bib42)] | Computing framework shielding and application
    production simplification |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 百度 | OpenEdge [[42](#bib.bib42)] | 计算框架屏蔽和应用程序生产简化 |'
- en: '| Microsoft | Azure IoT Edge [[43](#bib.bib43)] | Remotely edge management
    with zero-touch device provisioning |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 微软 | Azure IoT Edge [[43](#bib.bib43)] | 通过零触摸设备配置进行远程边缘管理 |'
- en: '| Linux Foundation | EdgeX [[44](#bib.bib44)] | IoT edge across the industrial
    and enterprise use cases |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Linux Foundation | EdgeX [[44](#bib.bib44)] | 适用于工业和企业用例的IoT边缘计算 |'
- en: '| Linux Foundation | Akraino Edge Stack [[45](#bib.bib45)] | Integrated distributed
    cloud edge platform |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Linux Foundation | Akraino Edge Stack [[45](#bib.bib45)] | 集成分布式云边缘平台 |'
- en: '| NVIDIA | NVIDIA EGX [[46](#bib.bib46)] | Real-time perception, understanding,
    and processing at the edge |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| NVIDIA | NVIDIA EGX [[46](#bib.bib46)] | 实时感知、理解和处理边缘数据 |'
- en: '|  | Amazon | AWS IoT Greengrass [[47](#bib.bib47)] | Tolerance to edge devices
    even with intermittent connectivity |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | Amazon | AWS IoT Greengrass [[47](#bib.bib47)] | 即使在间歇性连接的边缘设备上也具有容错性
    |'
- en: '|  | Google | Google Cloud IoT [[48](#bib.bib48)] | Compatible with Google
    AI products, such as TensorFlow Lite and Edge TPU |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | Google | Google Cloud IoT [[48](#bib.bib48)] | 兼容 Google AI 产品，如 TensorFlow
    Lite 和 Edge TPU |'
- en: II-A5 Collaborative End-Edge-Cloud Computing
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A5 协作的端-边缘-云计算
- en: 'While cloud computing is created for processing computation-intensive tasks,
    such as DL, it cannot guarantee the delay requirements throughout the whole process
    from data generation to transmission to execution. Moreover, independent processing
    on the end or edge devices is limited by their computing capability, power consumption,
    and cost bottleneck. Therefore, collaborative end-edge-cloud computing for DL
    [[12](#bib.bib12)], abstracted in Fig. [5](#S2.F5 "Figure 5 ‣ II-A4 Definition
    of Edge Computing Terminologies ‣ II-A Paradigms of Edge Computing ‣ II Fundamentals
    of Edge Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"), is emerging as an important trend as depicted in Fig. [6](#S2.F6 "Figure
    6 ‣ II-A4 Definition of Edge Computing Terminologies ‣ II-A Paradigms of Edge
    Computing ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey"). In this novel computing paradigm,
    computation tasks with lower computational intensities, generated by end devices,
    can be executed directly at the end devices or offloaded to the edge, thus avoiding
    the delay caused by sending data to the cloud. For a computation-intensive task,
    it will be reasonably segmented and dispatched separately to the end, edge and
    cloud for execution, reducing the execution delay of the task while ensuring the
    accuracy of the results [[12](#bib.bib12), [49](#bib.bib49), [50](#bib.bib50)].
    The focus of this collaborative paradigm is not only the successful completion
    of tasks but also achieving the optimal balance of equipment energy consumption,
    server loads, transmission and execution delays.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然云计算是为了处理计算密集型任务而创建的，例如深度学习，但它不能保证从数据生成到传输再到执行的整个过程中都满足延迟要求。此外，终端或边缘设备上的独立处理受到其计算能力、功耗和成本瓶颈的限制。因此，协作的端-边缘-云计算用于深度学习
    [[12](#bib.bib12)]，如图 [5](#S2.F5 "Figure 5 ‣ II-A4 Definition of Edge Computing
    Terminologies ‣ II-A Paradigms of Edge Computing ‣ II Fundamentals of Edge Computing
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey") 所示，正在成为一个重要趋势，如图
    [6](#S2.F6 "Figure 6 ‣ II-A4 Definition of Edge Computing Terminologies ‣ II-A
    Paradigms of Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey") 所示。在这一新的计算范式中，由终端设备生成的计算任务强度较低，可以直接在终端设备上执行或卸载到边缘，从而避免因将数据发送到云而导致的延迟。对于计算密集型任务，它将被合理分段并分别调度到终端、边缘和云进行执行，以减少任务的执行延迟，同时确保结果的准确性
    [[12](#bib.bib12), [49](#bib.bib49), [50](#bib.bib50)]。这一协作范式的重点不仅是任务的成功完成，还在于实现设备能耗、服务器负载、传输和执行延迟的最佳平衡。'
- en: II-B Hardware for Edge Computing
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 边缘计算的硬件
- en: 'In this section, we discuss potential enabling hardware of edge intelligence,
    i.e., customized AI chips and commodities for both end devices and edge nodes.
    Besides, edge-cloud systems for DL are introduced as well (listed in Table [II](#S2.T2
    "TABLE II ‣ II-A4 Definition of Edge Computing Terminologies ‣ II-A Paradigms
    of Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们讨论了边缘智能的潜在使能硬件，即定制的 AI 芯片以及面向终端设备和边缘节点的商品。此外，还介绍了用于深度学习的边缘云系统（见表格 [II](#S2.T2
    "TABLE II ‣ II-A4 Definition of Edge Computing Terminologies ‣ II-A Paradigms
    of Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")）。'
- en: II-B1 AI Hardware for Edge Computing
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 边缘计算的 AI 硬件
- en: 'Emerged edge AI hardware can be classified into three categories according
    to their technical architecture: 1) Graphics Processing Unit (GPU)-based hardware,
    which tend to have good compatibility and performance, but generally consume more
    energy, e.g., NVIDIA’ GPUs based on Turing architecture [[37](#bib.bib37)]; 2)
    Field Programmable Gate Array (FPGA)-based hardware [[51](#bib.bib51), [52](#bib.bib52)],
    which are energy-saving and require less computation resources, but with worse
    compatibility and limited programming capability compared to GPUs; 3) Application
    Specific Integrated Circuit (ASIC)-based hardware, such as Google’s TPU [[38](#bib.bib38)]
    and HiSilicon’s Ascend series [[35](#bib.bib35)], usually with a custom design
    that is more stable in terms of performance and power consumption.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的边缘 AI 硬件可以根据其技术架构分为三类：1）基于图形处理单元（GPU）的硬件，通常具有良好的兼容性和性能，但一般消耗更多的能量，例如基于图灵架构的
    NVIDIA GPU [[37](#bib.bib37)]；2）基于现场可编程门阵列（FPGA）的硬件 [[51](#bib.bib51), [52](#bib.bib52)]，这些硬件节能且计算资源需求较少，但与
    GPU 相比，兼容性较差且编程能力有限；3）基于应用特定集成电路（ASIC）的硬件，如 Google 的 TPU [[38](#bib.bib38)] 和海思的
    Ascend 系列 [[35](#bib.bib35)]，通常具有定制设计，在性能和功耗方面更为稳定。
- en: As smartphones represent the most widely-deployed edge devices, chips for smartphones
    have undergone rapid developments, and their capabilities have been extended to
    the acceleration of AI computing. To name a few, Qualcomm first applies AI hardware
    acceleration [[33](#bib.bib33)] in Snapdragon and releases Snapdragon Neural Processing
    Engine (SNPE) SDK [[53](#bib.bib53)], which supports almost all major DL frameworks.
    Compared to Qualcomm, HiSilicon’s 600 series and 900 series chips [[34](#bib.bib34)]
    do not depend on GPUs. Instead, they incorporate an additional Neural Processing
    Unit (NPU) to achieve fast calculation of vectors and matrices, which greatly
    improves the efficiency of DL. Compared to HiSilicon and Qualcomm, MediaTek’s
    Helio P60 not only uses GPUs but also introduces an AI Processing Unit (APU) to
    further accelerate neural network computing [[36](#bib.bib36)]. Performance comparison
    of most commodity chips with respect to DL can be found in [[54](#bib.bib54)],
    and more customized chips of edge devices will be discussed in detail later.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于智能手机是最广泛部署的边缘设备，智能手机芯片经历了快速的发展，其能力也扩展到了 AI 计算加速。例如，高通首次在 Snapdragon 中应用 AI
    硬件加速 [[33](#bib.bib33)]，并发布了支持几乎所有主要深度学习框架的 Snapdragon Neural Processing Engine
    (SNPE) SDK [[53](#bib.bib53)]。相比之下，海思的 600 系列和 900 系列芯片 [[34](#bib.bib34)] 不依赖
    GPU，而是集成了额外的神经处理单元（NPU），以实现快速的向量和矩阵计算，从而大大提高了深度学习的效率。与海思和高通相比，联发科的 Helio P60 不仅使用
    GPU，还引入了 AI 处理单元（APU）以进一步加速神经网络计算 [[36](#bib.bib36)]。有关大多数商品芯片在深度学习方面的性能比较可以参见
    [[54](#bib.bib54)]，更多关于边缘设备的定制芯片将在后续详细讨论。
- en: '![Refer to caption](img/cbb2e2a3365b76a6a308b9babf99a5a8.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cbb2e2a3365b76a6a308b9babf99a5a8.png)'
- en: 'Figure 7: Basic structures and functions of typical DNNs and DL.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：典型 DNN 和深度学习的基本结构与功能。
- en: II-B2 Integrated Commodities Potentially for Edge Nodes
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 可能用于边缘节点的集成商品
- en: Edge nodes are expected to have computing and caching capabilities and to provide
    high-quality network connection and computing services near end devices. Compared
    to most end devices, edge nodes have more powerful computing capability to process
    tasks. On the other side, edge nodes can respond to end devices more quickly than
    the cloud. Therefore, by deploying edge nodes to perform the computation task,
    the task processing can be accelerated while ensuring accuracy. In addition, edge
    nodes also have the ability to cache, which can improve the response time by caching
    popular contents. For example, practical solutions including Huawei’ Atlas modules
    [[32](#bib.bib32)] and Microsoft’s Data Box Edge [[29](#bib.bib29)] can carry
    out preliminary DL inference and then transfer to the cloud for further improvement.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘节点预计具备计算和缓存能力，并提供高质量的网络连接和接近终端设备的计算服务。与大多数终端设备相比，边缘节点具有更强的计算能力来处理任务。另一方面，边缘节点能够比云计算更快地响应终端设备。因此，通过部署边缘节点来执行计算任务，可以加速任务处理，同时确保准确性。此外，边缘节点还具备缓存能力，可以通过缓存热门内容来改善响应时间。例如，实际解决方案包括华为的
    Atlas 模块 [[32](#bib.bib32)] 和微软的数据盒边缘 [[29](#bib.bib29)]，可以进行初步的深度学习推理，然后转移到云端进行进一步优化。
- en: II-B3 Edge Computing Frameworks
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 边缘计算框架
- en: Solutions for edge computing systems are blooming. For DL services with complex
    configuration and intensive resource requirements, edge computing systems with
    advanced and excellent microservice architecture are the future development direction.
    Currently, Kubernetes is as a mainstream container-centric system for the deployment,
    maintenance, and scaling of applications in cloud computing [[55](#bib.bib55)].
    Based on Kubernetes, Huawei develops its edge computing solution “KubeEdge” [[41](#bib.bib41)]
    for networking, application deployment and metadata synchronization between the
    cloud and the edge (also supported in Akraino Edge Stack [[45](#bib.bib45)]).
    “OpenEdge” [[42](#bib.bib42)] focus on shielding computing framework and simplifying
    application production. For IoT, Azure IoT Edge [[43](#bib.bib43)] and EdgeX [[44](#bib.bib44)]
    are devised for delivering cloud intelligence to the edge by deploying and running
    AI on cross-platform IoT devices.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算系统的解决方案正在蓬勃发展。对于具有复杂配置和大量资源需求的 DL 服务，具有先进和优秀的微服务架构的边缘计算系统是未来的发展方向。目前，Kubernetes
    是用于在云计算中部署、维护和扩展应用程序的主流容器中心系统 [[55](#bib.bib55)]. 华为基于 Kubernetes 开发了其边缘计算解决方案
    "KubeEdge" [[41](#bib.bib41)]，用于云和边缘之间的网络、应用部署和元数据同步（也支持于 Akraino Edge Stack [[45](#bib.bib45)]
    中）。"OpenEdge" [[42](#bib.bib42)] 专注于屏蔽计算框架和简化应用程序生产。对于物联网，Azure IoT Edge [[43](#bib.bib43)]
    和 EdgeX [[44](#bib.bib44)] 是用于通过部署和运行 AI 在跨平台物联网设备上向边缘提供云智能的解决方案。
- en: 'TABLE III: Potential DL libraries for edge computing'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 III：边缘计算的潜在 DL 库
- en: '| Library |  CNTK [[56](#bib.bib56)]  |  Chainer [[57](#bib.bib57)]  |  TensorFlow
    [[58](#bib.bib58)]  |  DL4J [[59](#bib.bib59)]  |  TensorFlow Lite [[60](#bib.bib60)]  |  MXNet
    [[61](#bib.bib61)]  |  (Py)Torch [[62](#bib.bib62)]  |  CoreML [[63](#bib.bib63)]  |  SNPE
    [[53](#bib.bib53)]  |  NCNN [[64](#bib.bib64)]  |  MNN [[65](#bib.bib65)]  |  Paddle-Mobile
    [[66](#bib.bib66)]  |  MACE [[67](#bib.bib67)]  |  FANN [[68](#bib.bib68)]  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 库 |  CNTK [[56](#bib.bib56)]  |  Chainer [[57](#bib.bib57)]  |  TensorFlow
    [[58](#bib.bib58)]  |  DL4J [[59](#bib.bib59)]  |  TensorFlow Lite [[60](#bib.bib60)]  |  MXNet
    [[61](#bib.bib61)]  |  (Py)Torch [[62](#bib.bib62)]  |  CoreML [[63](#bib.bib63)]  |  SNPE
    [[53](#bib.bib53)]  |  NCNN [[64](#bib.bib64)]  |  MNN [[65](#bib.bib65)]  |  Paddle-Mobile
    [[66](#bib.bib66)]  |  MACE [[67](#bib.bib67)]  |  FANN [[68](#bib.bib68)]  |'
- en: '| Owner |  Microsoft  |  Preferred Networks  |  Google  |  Skymind  |  Google  |  Apache
    Incubator  |  Facebook  |  Apple  |  Qualcomm  |  Tencent  |  Alibaba  |  Baidu  |  XiaoMi  |  ETH
    Zürich  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 负责人 |  微软公司  |  Preferred Networks  |  谷歌  |  Skymind  |  谷歌  |  Apache 孵化器
    |  Facebook  |  苹果  |  Qualcomm  |  腾讯  |  阿里巴巴  |  百度  |  小米  |  ETH Zurich  |'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Edge &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 边缘 &#124;'
- en: '&#124; Support &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持 &#124;'
- en: '| $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: '| Android | $\times$ | $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ |
    ✓ | ✓ | ✓ | ✓ | $\times$ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Android | $\times$ | $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ |
    ✓ | ✓ | ✓ | ✓ | $\times$ |'
- en: '| iOS | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | ✓ | ✓ | ✓ |
    $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| iOS | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | ✓ | ✓ | ✓ |
    $\times$ | ✓ | ✓ | ✓ | ✓ | $\times$ |'
- en: '| Arm | $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | ✓ | ✓ | ✓
    | ✓ | ✓ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Arm | $\times$ | $\times$ | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | ✓ | ✓ | ✓
    | ✓ | ✓ |'
- en: '| FPGA | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |
    ✓ | $\times$ | $\times$ | $\times$ | $\times$ | ✓ | $\times$ | $\times$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |
    ✓ | $\times$ | $\times$ | $\times$ | $\times$ | ✓ | $\times$ | $\times$ |'
- en: '| DSP | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | ✓ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| DSP | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | ✓ | $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |'
- en: '| GPU | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| GPU | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ |'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Mobile &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动 &#124;'
- en: '&#124; GPU &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPU &#124;'
- en: '| $\times$ | $\times$ | $\times$ | $\times$ | ✓ | $\times$ | $\times$ | ✓ |
    ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| $\times$ | $\times$ | $\times$ | $\times$ | ✓ | $\times$ | $\times$ | ✓ |
    ✓ | ✓ | ✓ | ✓ | ✓ | $\times$ |'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Training &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; Support &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持 &#124;'
- en: '| ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | ✓ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | ✓ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | $\times$ | ✓ | ✓ | $\times$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | ✓ |'
- en: II-C Virtualizing the Edge
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 虚拟化边缘
- en: 'The requirements of virtualization technology for integrating edge computing
    and DL reflect in the following aspects: 1) The resource of edge computing is
    limited. Edge computing cannot provide that resources for DL services as the cloud
    does. Virtualization technologies should maximize resource utilization under the
    constraints of limited resources; 2) DL services rely heavily on complex software
    libraries. The versions and dependencies of these software libraries should be
    taken into account carefully. Therefore, virtualization catering to Edge DL services
    should be able to isolate different services. Specifically, the upgrade, shutdown,
    crash, and high resource consumption of a single service should not affect other
    services; 3) The service response speed is critical for Edge DL. Edge DL requires
    not only the computing power of edge devices but also the agile service response
    that the edge computing architecture can provide.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化技术在集成边缘计算和深度学习（DL）方面的要求体现在以下几个方面：1）边缘计算的资源有限。边缘计算无法像云计算那样提供资源给DL服务。虚拟化技术应在资源有限的约束下最大化资源利用；2）DL服务严重依赖复杂的软件库。这些软件库的版本和依赖关系需要仔细考虑。因此，针对边缘DL服务的虚拟化应能够隔离不同的服务。具体来说，单个服务的升级、关闭、崩溃和高资源消耗不应影响其他服务；3）服务响应速度对边缘DL至关重要。边缘DL不仅需要边缘设备的计算能力，还需要边缘计算架构能够提供的敏捷服务响应。
- en: '![Refer to caption](img/58adcebbbb3d87a29f6c4d572fca047d.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/58adcebbbb3d87a29f6c4d572fca047d.png)'
- en: 'Figure 8: Virtualizing edge computing infrastructure and networks.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：虚拟化边缘计算基础设施和网络。
- en: 'The combination of edge computing and DL to form high-performance Edge DL services
    requires the coordinated integration of computing, networking and communication
    resources, as depicted in Fig. [8](#S2.F8 "Figure 8 ‣ II-C Virtualizing the Edge
    ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"). Specifically, both the computation virtualization and
    the integration of network virtualization, and management technologies are necessary.
    In this section, we discuss potential virtualization technologies for the edge.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算和DL的结合形成高性能的边缘DL服务，需要计算、网络和通信资源的协调整合，如图[8](#S2.F8 "图 8 ‣ II-C 虚拟化边缘 ‣ II
    边缘计算基础 ‣ 边缘计算与深度学习的融合：全面调查")所示。具体来说，计算虚拟化以及网络虚拟化和管理技术的整合都是必要的。在这一部分，我们讨论了边缘的潜在虚拟化技术。
- en: II-C1 Virtualization Techniques
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 虚拟化技术
- en: 'Currently, there are two main virtualization strategies: Virtual Machine (VM)
    and container. In general, VM is better at isolating while container provides
    easier deployment of repetitive tasks [[69](#bib.bib69)]. With VM virtualization
    at operating system level, a VM hypervisor splits a physical server into one or
    multiple VMs, and can easily manage each VM to execute tasks in isolation. Besides,
    the VM hypervisor can allocate and use idle computing resources more efficiently
    by creating a scalable system that includes multiple independent virtual computing
    devices.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，主要有两种虚拟化策略：虚拟机（VM）和容器。一般来说，VM在隔离方面更为出色，而容器则提供了更便捷的重复任务部署[[69](#bib.bib69)]。通过操作系统级别的VM虚拟化，VM虚拟机监控程序将物理服务器拆分成一个或多个VM，并能够轻松管理每个VM以在隔离的环境中执行任务。此外，VM虚拟机监控程序可以通过创建一个包含多个独立虚拟计算设备的可扩展系统，更高效地分配和利用闲置的计算资源。
- en: In contrast to VM, container virtualization is a more flexible tool for packaging,
    delivering, and orchestrating software infrastructure services and applications.
    Container virtualization for edge computing can effectively reduce the workload
    execution time with high performance and storage requirements, and can also deploy
    a large number of services in a scalable and straightforward fashion [[70](#bib.bib70)].
    A container consists of a single file that includes an application and execution
    environment with all dependencies, which makes it enable efficient service handoff
    to cope with user mobility [[71](#bib.bib71)]. Owning to that the execution of
    applications in the container does not depend on additional virtualization layers
    as in VM virtualization, the processor consumption and the amount of memory required
    to execute the application are significantly reduced.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与虚拟机（VM）相比，容器虚拟化是打包、交付和编排软件基础设施服务和应用程序的更灵活工具。边缘计算的容器虚拟化可以有效地减少工作负载执行时间，满足高性能和存储要求，并且可以以可扩展和直接的方式部署大量服务[[70](#bib.bib70)]。容器由一个包含应用程序及其所有依赖项的执行环境的单一文件组成，这使得它能够有效地进行服务切换以应对用户的移动性[[71](#bib.bib71)]。由于容器中的应用程序执行不依赖于如虚拟机虚拟化中的额外虚拟化层，因此处理器消耗和执行应用程序所需的内存显著减少。
- en: II-C2 Network Virtualization
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 网络虚拟化
- en: 'Traditional networking functions, combined with specific hardware, is not flexible
    enough to manage edge computing networks in an on-demand fashion. In order to
    consolidate network device functions into industry-standard servers, switches
    and storage, Network Functions Virtualization (NFV) enables Virtual Network Functions
    (VNFs) to run in software, by separating network functions and services from dedicated
    network hardware. Further, Edge DL services typically require high bandwidth,
    low latency, and dynamic network configuration, while Software-defined Networking
    (SDN) allows rapid deployment of services, network programmability and multi-tenancy
    support, through three key innovations [[72](#bib.bib72)]: 1) Decoupling of control
    planes and data planes; 2) Centralized and programmable control planes; 3) Standardized
    application programming interface. With these advantages, it supports a highly
    customized network strategy that is well suited for the high bandwidth, dynamic
    nature of Edge DL services.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的网络功能结合特定硬件，无法以按需方式灵活管理边缘计算网络。为了将网络设备功能整合到行业标准的服务器、交换机和存储中，网络功能虚拟化（NFV）通过将网络功能和服务从专用网络硬件中分离，使虚拟网络功能（VNF）能够在软件中运行。此外，边缘深度学习服务通常需要高带宽、低延迟和动态网络配置，而软件定义网络（SDN）通过三个关键创新[[72](#bib.bib72)]：1)
    控制平面和数据平面的解耦；2) 集中和可编程的控制平面；3) 标准化的应用编程接口，允许快速部署服务、网络编程和多租户支持。凭借这些优势，它支持一种高度定制的网络策略，非常适合边缘深度学习服务的高带宽和动态特性。
- en: Network virtualization and edge computing benefit each other. On the one hand,
    NFV/SDN can enhance the interoperability of edge computing infrastructure. For
    example, with the support of NFV/SDN, edge nodes can be efficiently orchestrated
    and integrated with cloud data centers [[73](#bib.bib73)]. On the other hand,
    both VNFs and Edge DL services can be hosted on a lightweight NFV framework (deployed
    on the edge) [[74](#bib.bib74)], thus reusing the infrastructure and infrastructure
    management of NFV to the largest extent possible [[75](#bib.bib75)].
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 网络虚拟化和边缘计算相互受益。一方面，NFV/SDN可以增强边缘计算基础设施的互操作性。例如，借助NFV/SDN的支持，边缘节点可以高效地编排并与云数据中心集成[[73](#bib.bib73)]。另一方面，VNF和边缘深度学习服务都可以托管在轻量级NFV框架（部署在边缘）[[74](#bib.bib74)]上，从而在最大程度上重用NFV的基础设施和基础设施管理[[75](#bib.bib75)]。
- en: II-C3 Network Slicing
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 网络切片
- en: 'Network slicing is a form of agile and virtual network architecture, a high-level
    abstraction of the network that allows multiple network instances to be created
    on top of a common shared physical infrastructure, each of which optimized for
    specific services. With increasingly diverse service and QoS requirements, network
    slicing, implemented by NFV/SDN, is naturally compatible with distributed paradigms
    of edge computing. To meet these, network slicing can be coordinated with joint
    optimization of computing and communication resources in edge computing networks
    [[76](#bib.bib76)]. Fig. [8](#S2.F8 "Figure 8 ‣ II-C Virtualizing the Edge ‣ II
    Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey") depicts an example of network slicing based on edge virtualization.
    In order to implement service customization in network slicing, virtualization
    technologies and SDN must be together to support tight coordination of resource
    allocation and service provision on edge nodes while allowing flexible service
    control. With network slicing, customized and optimized resources can be provided
    for Edge DL services, which can help reduce latency caused by access networks
    and support dense access to these services [[77](#bib.bib77)].'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '网络切片是一种敏捷和虚拟的网络架构，是网络的高级抽象，允许在共享的物理基础设施上创建多个网络实例，每个实例针对特定服务进行优化。随着服务和 QoS 要求的日益多样化，通过
    NFV/SDN 实现的网络切片自然与边缘计算的分布式范式兼容。为满足这些要求，网络切片可以与边缘计算网络中计算和通信资源的联合优化进行协调[[76](#bib.bib76)]。图[8](#S2.F8
    "Figure 8 ‣ II-C Virtualizing the Edge ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")展示了基于边缘虚拟化的网络切片示例。为了在网络切片中实现服务定制，虚拟化技术和
    SDN 必须共同支持边缘节点上资源分配和服务提供的紧密协调，同时允许灵活的服务控制。通过网络切片，可以为边缘 DL 服务提供定制和优化的资源，这有助于减少由接入网络引起的延迟，并支持对这些服务的密集访问[[77](#bib.bib77)]。'
- en: III Fundamentals of Deep Learning
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习基础
- en: With respect to CV, NLP, and AI, DL is adopted in a myriad of applications and
    corroborates its superior performance [[78](#bib.bib78)]. Currently, a large number
    of GPUs, TPUs, or FPGAs are required to be deployed in the cloud to process DL
    service requests. Nonetheless, the edge computing architecture, on account of
    it covers a large number of distributed edge devices, can be utilized to better
    serve DL. Certainly, edge devices typically have limited computing power or power
    consumption compared to the cloud. Therefore, the combination of DL and edge computing
    is not straightforward and requires a comprehensive understanding of DL models
    and edge computing features for design and deployment. In this section, we compendiously
    introduce DL and related technical terms, paving the way for discussing the integration
    of DL and edge computing (more details can be found in [[79](#bib.bib79)]).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 CV、NLP 和 AI，DL 被广泛应用，并证实了其卓越的性能[[78](#bib.bib78)]。目前，需要大量的 GPUs、TPUs 或 FPGAs
    部署在云端以处理 DL 服务请求。然而，由于边缘计算架构覆盖了大量分布式边缘设备，它可以更好地服务于 DL。确实，边缘设备通常相比云端具有有限的计算能力或功耗。因此，DL
    与边缘计算的结合并非简单，需要全面理解 DL 模型和边缘计算特性的设计和部署。在本节中，我们简要介绍了 DL 及相关技术术语，为讨论 DL 与边缘计算的整合铺平了道路（更多细节见[[79](#bib.bib79)]）。
- en: III-A Neural Networks in Deep Learning
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 深度学习中的神经网络
- en: DL models consist of various types of Deep Neural Networks (DNNs) [[79](#bib.bib79)].
    Fundamentals of DNNs in terms of basic structures and functions are introduced
    as follows.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: DL 模型由各种类型的深度神经网络（DNNs）组成[[79](#bib.bib79)]。DNN 的基本结构和功能如下所示。
- en: III-A1 Fully Connected Neural Network (FCNN)
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 全连接神经网络（FCNN）
- en: 'The output of each layer of FCNN, i.e., Multi-Layer Perceptron (MLP), is fed
    forward to the next layer, as in Fig. [7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware
    for Edge Computing ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge
    Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")(a).
    Between contiguous FCNN layers, the output of a neuron (cell), either the input
    or hidden cell, is directly passed to and activated by neurons belong to the next
    layer [[80](#bib.bib80)]. FCNN can be used for feature extraction and function
    approximation, however with high complexity, modest performance, and slow convergence.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 'FCNN 的每一层的输出，即多层感知器 (MLP)，被前馈到下一层，如图 [7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware
    for Edge Computing ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge
    Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")(a)
    所示。在连续的 FCNN 层之间，神经元（单元）的输出，无论是输入单元还是隐藏单元，都被直接传递到下一层的神经元并被激活 [[80](#bib.bib80)]。FCNN
    可以用于特征提取和函数逼近，但具有高复杂性、适中性能和缓慢收敛。'
- en: III-A2 Auto-Encoder (AE)
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 自编码器 (AE)
- en: 'AE, as in Fig. [7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware for Edge Computing
    ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")(b), is actually
    a stack of two NNs that replicate input to its output in an unsupervised learning
    style. The first NN learns the representative characteristics of the input (encoding).
    The second NN takes these features as input and restores the approximation of
    the original input at the match input output cell, used to converge on the identity
    function from input to output, as the final output (decoding). Since AEs are able
    to learn the low-dimensional useful features of input data to recover input data,
    it is often used to classify and store high-dimensional data [[81](#bib.bib81)].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware
    for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")(b) 所示，AE 实际上是两个神经网络的堆叠，它们以无监督学习的方式将输入复制到输出。第一个神经网络学习输入的代表性特征（编码）。第二个神经网络以这些特征作为输入，并在匹配输入输出单元中恢复原始输入的近似值，用于从输入到输出的身份函数的收敛，作为最终输出（解码）。由于
    AE 能够学习输入数据的低维有用特征以恢复输入数据，因此它通常用于分类和存储高维数据 [[81](#bib.bib81)]。'
- en: III-A3 Convolutional Neural Network (CNN)
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 卷积神经网络 (CNN)
- en: 'By employing pooling operations and a set of distinct moving filters, CNNs
    seize correlations between adjacent data pieces, and then generate a successively
    higher level abstraction of the input data, as in Fig. [7](#S2.F7 "Figure 7 ‣
    II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣ II Fundamentals
    of Edge Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")(c). Compared to FCNNs, CNNs can extract features while reducing the model
    complexity, which mitigates the risk of overfitting [[82](#bib.bib82)]. These
    characteristics make CNNs achieve remarkable performance in image processing and
    also useful in processing structural data similar to images.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '通过使用池化操作和一组不同的移动滤波器，CNN 捕捉到相邻数据片段之间的关联，然后生成输入数据的逐层高层抽象，如图 [7](#S2.F7 "Figure
    7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣
    II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(c) 所示。与 FCNNs 相比，CNNs 能够在减少模型复杂性的同时提取特征，这降低了过拟合的风险 [[82](#bib.bib82)]。这些特性使
    CNNs 在图像处理方面表现出色，也适用于处理类似图像的结构化数据。'
- en: III-A4 Generative Adversarial Network (GAN)
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 生成对抗网络 (GAN)
- en: 'GAN originates from game theory. As illustrated in Fig. [7](#S2.F7 "Figure
    7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣
    II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(d), GAN is composed of generator and discriminator. The
    goal of the generator is to learn about the true data distribution as much as
    possible by deliberately introducing feedback at the back-fed input cell, while
    the discriminator is to correctly determine whether the input data is coming from
    the true data or the generator. These two participants need to constantly optimize
    their ability to generate and distinguish in the adversarial process until finding
    a Nash equilibrium [[83](#bib.bib83)]. According to the features learned from
    the real information, a well-trained generator can thus fabricate indistinguishable
    information.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAN（生成对抗网络）起源于博弈论。如图[7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware for Edge Computing
    ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")(d)所示，GAN由生成器和判别器组成。生成器的目标是通过故意在反向输入单元中引入反馈，尽可能多地了解真实数据分布，而判别器的目标是正确判断输入数据是否来自真实数据或生成器。这两个参与者需要在对抗过程中不断优化它们的生成和区分能力，直到找到纳什均衡[[83](#bib.bib83)]。根据从真实信息中学习到的特征，经过良好训练的生成器可以制造出无法区分的信息。'
- en: III-A5 Recurrent Neural Network (RNN)
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 递归神经网络（RNN）
- en: 'RNNs are designed for handling sequential data. As depicted in Fig. [7](#S2.F7
    "Figure 7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing
    ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(e), each neuron in RNNs not only receives information
    from the upper layer but also receives information from the previous channel of
    its own [[10](#bib.bib10)]. In general, RNNs are natural choices for predicting
    future information or restoring missing parts of sequential data. However, a serious
    problem with RNNs is the gradient explosion. LSTM, as in Fig. [7](#S2.F7 "Figure
    7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣
    II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(f), improving RNN with adding a gate structure and a
    well-defined memory cell, can overcome this issue by controlling (prohibiting
    or allowing) the flow of information [[84](#bib.bib84)].'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'RNNs（递归神经网络）旨在处理序列数据。如图[7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware for Edge Computing
    ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")(e)所示，RNNs中的每个神经元不仅接收来自上层的信息，还接收来自自身前一通道的信息[[10](#bib.bib10)]。一般来说，RNNs是预测未来信息或恢复序列数据丢失部分的自然选择。然而，RNNs的一个严重问题是梯度爆炸。LSTM，如图[7](#S2.F7
    "Figure 7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing
    ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(f)所示，通过增加一个门控结构和一个明确定义的记忆单元来改进RNN，可以通过控制（禁止或允许）信息流来克服这一问题[[84](#bib.bib84)]。'
- en: III-A6 Transfer Learning (TL)
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A6 转移学习（TL）
- en: 'TL can transfer knowledge, as shown in Fig. [7](#S2.F7 "Figure 7 ‣ II-B1 AI
    Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣ II Fundamentals
    of Edge Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")(g), from the source domain to the target domain so as to achieve better
    learning performance in the target domain [[85](#bib.bib85)]. By using TL, existing
    knowledge learned by a large number of computation resources can be transferred
    to a new scenario, and thus accelerating the training process and reducing model
    development costs. Recently, a novel form of TL emerges, viz., Knowledge Distillation
    (KD) [[86](#bib.bib86)] emerges. As indicated in Fig. [7](#S2.F7 "Figure 7 ‣ II-B1
    AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing ‣ II Fundamentals
    of Edge Computing ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")(h), KD can extract implicit knowledge from a well-trained model (teacher),
    inference of which possess excellent performance but requires high overhead. Then,
    by designing the structure and objective function of the target DL model, the
    knowledge is “transferred” to a smaller DL model (student), so that the significantly
    reduced (pruned or quantized) target DL model achieves high performance as possible.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 'TL可以将知识从源领域转移到目标领域，如图[7](#S2.F7 "Figure 7 ‣ II-B1 AI Hardware for Edge Computing
    ‣ II-B Hardware for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")(g)所示，从而在目标领域实现更好的学习表现[[85](#bib.bib85)]。通过使用TL，已有的知识可以由大量计算资源转移到新场景，从而加速训练过程并降低模型开发成本。最近，出现了一种新型的TL形式，即知识蒸馏（KD）[[86](#bib.bib86)]。如图[7](#S2.F7
    "Figure 7 ‣ II-B1 AI Hardware for Edge Computing ‣ II-B Hardware for Edge Computing
    ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")(h)所示，KD可以从经过良好训练的模型（教师）中提取隐含知识，该模型的推理表现出色，但需要较高的开销。然后，通过设计目标DL模型的结构和目标函数，这些知识被“转移”到一个较小的DL模型（学生）中，以便显著减少（剪枝或量化）的目标DL模型能够实现尽可能高的性能。'
- en: III-B Deep Reinforcement Learning (DRL)
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 深度强化学习（DRL）
- en: 'As depicted in Fig. [9](#S3.F9 "Figure 9 ‣ III-B Deep Reinforcement Learning
    (DRL) ‣ III Fundamentals of Deep Learning ‣ Convergence of Edge Computing and
    Deep Learning: A Comprehensive Survey"), the goal of RL is to enable an agent
    in the environment to take the best action in the current state to maximize long-term
    gains, where the interaction between the agent’s action and state through the
    environment is modeled as a Markov Decision Process (MDP). DRL is the combination
    of DL and RL, but it focuses more on RL and aims to solve decision-making problems.
    The role of DL is to use the powerful representation ability of DNNs to fit the
    value function or the direct strategy to solve the explosion of state-action space
    or continuous state-action space problem. By virtue of these characteristics,
    DRL becomes a powerful solution in robotics, finance, recommendation system, wireless
    communication, etc [[87](#bib.bib87), [18](#bib.bib18)].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[9](#S3.F9 "Figure 9 ‣ III-B Deep Reinforcement Learning (DRL) ‣ III Fundamentals
    of Deep Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")所示，RL的目标是使环境中的智能体在当前状态下采取最佳行动，以最大化长期收益，其中智能体通过环境与状态之间的交互被建模为马尔可夫决策过程（MDP）。DRL是DL和RL的结合，但它更侧重于RL，旨在解决决策问题。DL的作用是利用DNNs的强大表示能力来拟合价值函数或直接策略，以解决状态-动作空间爆炸或连续状态-动作空间问题。凭借这些特性，DRL在机器人技术、金融、推荐系统、无线通信等领域成为一种强大的解决方案[[87](#bib.bib87),
    [18](#bib.bib18)]。'
- en: '![Refer to caption](img/02ffdacc393245036c1c4929faa8690b.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02ffdacc393245036c1c4929faa8690b.png)'
- en: 'Figure 9: Value-based and policy-gradient-based DRL approaches.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：基于价值和基于策略梯度的DRL方法。
- en: III-B1 Value-based DRL
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 基于价值的DRL
- en: As a representative of value-based DRL, Deep $Q$-Learning (DQL) uses DNNs to
    fit action values, successfully mapping high-dimensional input data to actions
    [[88](#bib.bib88)]. In order to ensure stable convergence of training, experience
    replay method is adopted to break the correlation between transition information
    and a separate target network is set up to suppress instability. Besides, Double
    Deep $Q$-Learning (Double-DQL) can deal with that DQL generally overestimating
    action values [[89](#bib.bib89)], and Dueling Deep $Q$-Learning (Dueling-DQL)
    [[90](#bib.bib90)] can learn which states are (or are not) valuable without having
    to learn the effect of each action at each state.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于价值的深度强化学习（DRL）的代表，Deep $Q$-Learning（DQL）使用深度神经网络（DNNs）来拟合动作值，成功地将高维输入数据映射到动作[[88](#bib.bib88)]。为了确保训练的稳定收敛，采用了经验回放方法来打破过渡信息之间的关联，并设置了一个独立的目标网络来抑制不稳定性。此外，Double
    Deep $Q$-Learning（Double-DQL）可以解决DQL通常会高估动作值的问题[[89](#bib.bib89)]，而Dueling Deep
    $Q$-Learning（Dueling-DQL）[[90](#bib.bib90)]可以学习哪些状态是（或不是）有价值的，而无需学习每个状态下每个动作的效果。
- en: III-B2 Policy-gradient-based DRL
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 基于策略梯度的DRL
- en: Policy gradient is another common strategy optimization method, such as Deep
    Deterministic Policy Gradient (DDPG) [[91](#bib.bib91)], Asynchronous Advantage
    Actor-Critic (A3C) [[92](#bib.bib92)], Proximate Policy Optimization (PPO) [[93](#bib.bib93)],
    etc. It updates the policy parameters by continuously calculating the gradient
    of the policy expectation reward with respect to them, and finally converges to
    the optimal strategy [[94](#bib.bib94)]. Therefore, when solving the DRL problem,
    DNNs can be used to parameterize the policy, and then be optimized by the policy
    gradient method. Further, Actor-Critic (AC) framework is widely adopted in policy-gradient-based
    DRL, in which the policy DNN is used to update the policy, corresponding to the
    Actor; the value DNN is used to approximate the value function of the state action
    pair, and provides gradient information, corresponding to the Critic.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是另一种常见的策略优化方法，例如深度确定性策略梯度（DDPG）[[91](#bib.bib91)]、异步优势演员-评论家（A3C）[[92](#bib.bib92)]、近端策略优化（PPO）[[93](#bib.bib93)]等。它通过不断计算策略期望奖励的梯度来更新策略参数，最终收敛到最优策略[[94](#bib.bib94)]。因此，在解决DRL问题时，可以使用DNNs来参数化策略，然后通过策略梯度方法进行优化。此外，Actor-Critic（AC）框架在基于策略梯度的DRL中被广泛采用，其中策略DNN用于更新策略，对应于Actor；价值DNN用于近似状态动作对的价值函数，并提供梯度信息，对应于Critic。
- en: III-C Distributed DL Training
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 分布式深度学习训练
- en: 'At present, training DL models in a centralized manner consumes a lot of time
    and computation resources, hindering further improving the algorithm performance.
    Nonetheless, distributed training can facilitate the training process by taking
    full advantage of parallel servers. There are two common ways to perform distributed
    training, i.e., data parallelism and model parallelism [[95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)] as illustrated in Fig. [10](#S3.F10 "Figure
    10 ‣ III-C Distributed DL Training ‣ III Fundamentals of Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，集中式训练深度学习模型消耗大量时间和计算资源，阻碍了算法性能的进一步提升。然而，分布式训练可以通过充分利用并行服务器来促进训练过程。分布式训练有两种常见的方法，即数据并行性和模型并行性[[95](#bib.bib95),
    [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98)]，如图[10](#S3.F10 "Figure 10
    ‣ III-C Distributed DL Training ‣ III Fundamentals of Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")所示。'
- en: '![Refer to caption](img/d4f4c6a1e1d0b333f37b60fc21ebfd42.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4f4c6a1e1d0b333f37b60fc21ebfd42.png)'
- en: 'Figure 10: Distributed training in terms of data and model parallelism.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：数据和模型并行方面的分布式训练。
- en: Model parallelism first splits a large DL model into multiple parts and then
    feeds data samples for training these segmented models in parallel. This not only
    can improve the training speed but also deal with the circumstance that the model
    is larger than the device memory. Training a large DL model generally requires
    a lot of computation resources, even thousands of CPUs are required to train a
    large-scale DL model. In order to solve this problem, distributed GPUs can be
    utilized for model parallel training [[99](#bib.bib99)]. Data parallelism means
    dividing data into multiple partitions, and then respectively training copies
    of the model in parallel with their own allocated data samples. By this means,
    the training efficiency of model training can be improved [[100](#bib.bib100)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行性首先将一个大型深度学习模型分割成多个部分，然后并行训练这些分割后的模型。这样不仅可以提高训练速度，还能处理模型超出设备内存的情况。训练大型深度学习模型通常需要大量的计算资源，甚至需要数千个CPU来训练大规模的深度学习模型。为了解决这个问题，可以利用分布式GPU进行模型并行训练[[99](#bib.bib99)]。数据并行性指的是将数据划分为多个部分，然后分别在并行的模型副本中用各自分配的数据样本进行训练。通过这种方式，可以提高模型训练的效率[[100](#bib.bib100)]。
- en: Coincidentally, a large number of end devices, edge nodes, and cloud data centers,
    are scattered and envisioned to be connected by virtue of edge computing networks.
    These distributed devices can potentially be powerful contributors once the DL
    training jumps out of the cloud.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 恰巧的是，大量的终端设备、边缘节点和云数据中心被分散，并预计通过边缘计算网络连接起来。这些分布式设备一旦深度学习训练从云端转移，可能会成为强大的贡献者。
- en: III-D Potential DL Libraries for Edge
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 边缘计算的潜在深度学习库
- en: 'Development and deployment of DL models rely on the support of various DL libraries.
    However, different DL libraries have their own application scenarios. For deploying
    DL on and for the edge, efficient lightweight DL libraries are required. Features
    of DL frameworks potentially supporting future edge intelligence are listed in
    Table [III](#S2.T3 "TABLE III ‣ II-B3 Edge Computing Frameworks ‣ II-B Hardware
    for Edge Computing ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey") (excluding libraries unavailable for
    edge devices, such as Theano [[101](#bib.bib101)]).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习模型的开发和部署依赖于各种深度学习库的支持。然而，不同的深度学习库有各自的应用场景。对于边缘计算及其应用，需要高效的轻量级深度学习库。未来支持边缘智能的深度学习框架的特点列在表[III](#S2.T3
    "TABLE III ‣ II-B3 Edge Computing Frameworks ‣ II-B Hardware for Edge Computing
    ‣ II Fundamentals of Edge Computing ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")中（不包括不可用于边缘设备的库，例如Theano [[101](#bib.bib101)]）。'
- en: IV Deep Learning Applications on Edge
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 边缘上的深度学习应用
- en: In general, DL services are currently deployed in cloud data centers (the cloud)
    for handling requests, due to the fact that most DL models are complex and hard
    to compute their inference results on the side of resource-limited devices. However,
    such kind of “end-cloud” architecture cannot meet the needs of real-time DL services
    such as real-time analytics, smart manufacturing and etc. Thus, deploying DL applications
    on the edge can broaden the application scenarios of DL especially with respect
    to the low latency characteristic. In the following, we present edge DL applications
    and highlight their advantages over the comparing architectures without edge computing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，由于大多数深度学习模型复杂且难以在资源有限的设备上计算其推断结果，深度学习服务当前主要部署在云数据中心（云端）以处理请求。然而，这种“端-云”架构无法满足实时深度学习服务的需求，如实时分析、智能制造等。因此，将深度学习应用部署在边缘上可以拓宽深度学习的应用场景，特别是对于低延迟特性。接下来，我们将介绍边缘深度学习应用，并强调它们相对于没有边缘计算的比较架构的优势。
- en: IV-A Real-time Video Analytic
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 实时视频分析
- en: 'Real-time video analytic is important in various fields, such as automatic
    pilot, VR and Augmented Reality (AR), smart surveillance, etc. In general, applying
    DL for it requires high computation and storage resources. Unfortunately, executing
    these tasks in the cloud often incurs high bandwidth consumption, unexpected latency,
    and reliability issues. With the development of edge computing, those problems
    tend to be addressed by moving video analysis near to the data source, viz., end
    devices or edge nodes, as the complementary of the cloud. In this section, as
    depicted in Fig. [11](#S4.F11 "Figure 11 ‣ IV-A Real-time Video Analytic ‣ IV
    Deep Learning Applications on Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"), we summarize related works as a hybrid hierarchical
    architecture, which is divided into three levels: end, edge, and cloud.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '实时视频分析在各种领域中都很重要，如自动驾驶、虚拟现实（VR）和增强现实（AR）、智能监控等。一般来说，应用深度学习（DL）需要高计算和存储资源。不幸的是，在云端执行这些任务通常会带来高带宽消耗、意外延迟和可靠性问题。随着边缘计算的发展，这些问题往往通过将视频分析移至数据源附近，即终端设备或边缘节点，作为云的补充来解决。在这一部分，如图[11](#S4.F11
    "Figure 11 ‣ IV-A Real-time Video Analytic ‣ IV Deep Learning Applications on
    Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")所示，我们总结了作为混合层次结构的相关工作，分为三个层次：终端、边缘和云。'
- en: '![Refer to caption](img/5bfb2ad0bb45386ec698ffa2fef91232.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5bfb2ad0bb45386ec698ffa2fef91232.png)'
- en: 'Figure 11: The collaboration of the end, edge and cloud layer for performing
    real-time video analytic by deep learning.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：终端、边缘和云层协作进行深度学习实时视频分析。
- en: IV-A1 End Level
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 终端层
- en: At the end level, video capture devices, such as smartphones and surveillance
    cameras are responsible for video capture, media data compression [[102](#bib.bib102)],
    image pre-processing, and image segmentation [[103](#bib.bib103)]. By coordinating
    with these participated devices, collaboratively training a domain-aware adaptation
    model can lead to better object recognition accuracy when used together with a
    domain-constrained deep model [[104](#bib.bib104)]. Besides, in order to appropriately
    offload the DL computation to the end devices, the edge nodes or the cloud, end
    devices should comprehensively consider tradeoffs between video compression and
    key metrics, e.g., network condition, data usage, battery consumption, processing
    delay, frame rate and accuracy of analytics, and thus determine the optimal offloading
    strategy [[102](#bib.bib102)].
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端层，视频捕捉设备，如智能手机和监控摄像头，负责视频捕捉、媒体数据压缩 [[102](#bib.bib102)]、图像预处理和图像分割 [[103](#bib.bib103)]。通过与这些参与设备协调，协同训练领域感知适应模型可以在与领域约束深度模型
    [[104](#bib.bib104)] 一起使用时，提高物体识别的准确性。此外，为了适当地将深度学习计算卸载到终端设备、边缘节点或云端，终端设备应全面考虑视频压缩和关键指标之间的权衡，例如网络状况、数据使用、耗电量、处理延迟、帧率和分析准确性，从而确定最佳的卸载策略
    [[102](#bib.bib102)]。
- en: If various DL tasks are executed at the end level independently, enabling parallel
    analytics requires a solution that supports efficient multi-tenant DL. With the
    model pruning and recovery scheme, NestDNN [[105](#bib.bib105)] transforms the
    DL model into a set of descendant models, in which the descendant model with fewer
    resource requirements shares its model parameters with the descendant model requiring
    more resources, making itself nested inside the descendent model requiring more
    resources without taking extra memory space. In this way, the multi-capacity model
    provides variable resource-accuracy trade-offs with a compact memory footprint,
    hence ensuring efficient multi-tenant DL at the end level.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果各种深度学习任务在终端层独立执行，实现并行分析需要一个支持高效多租户深度学习的解决方案。通过模型修剪和恢复方案，NestDNN [[105](#bib.bib105)]
    将深度学习模型转化为一组后代模型，其中资源需求较少的后代模型与资源需求更多的后代模型共享其模型参数，使得自身嵌套在需要更多资源的后代模型内部，而不占用额外的内存空间。通过这种方式，多容量模型提供了具有紧凑内存占用的可变资源-准确度权衡，从而确保了终端层高效的多租户深度学习。
- en: IV-A2 Edge Level
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 边缘层
- en: Numerous distributed edge nodes at the edge level generally cooperate with each
    other to provide better services. For example, LAVEA [[106](#bib.bib106)] attaches
    edge nodes to the same access point or BS as well as the end devices, which ensure
    that services can be as ubiquitous as Internet access. In addition, compressing
    the DL model on the edge can improve holistic performance. The resource consumption
    of the edge layer can be greatly reduced while ensuring the analysis performance,
    by reducing the unnecessary filters in CNN layers [[107](#bib.bib107)]. Besides,
    in order to optimize performance and efficiency, [[108](#bib.bib108)] presents
    an edge service framework, i.e., EdgeEye, which realizes a high-level abstraction
    of real-time video analytic functions based on DL. To fully exploit the bond function
    of the edge, VideoEdge [[109](#bib.bib109)] implements an end-edge-cloud hierarchical
    architecture to help achieve load balancing concerning analytical tasks while
    maintaining high analysis accuracy.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘级的众多分布式边缘节点通常相互合作以提供更好的服务。例如，LAVEA [[106](#bib.bib106)] 将边缘节点连接到相同的接入点或基站以及终端设备，从而确保服务能够像互联网访问一样无处不在。此外，通过在边缘压缩
    DL 模型可以提高整体性能。通过减少 CNN 层中的不必要滤波器 [[107](#bib.bib107)]，可以在确保分析性能的同时大幅减少边缘层的资源消耗。此外，为了优化性能和效率，[[108](#bib.bib108)]
    提出了一个边缘服务框架，即 EdgeEye，基于 DL 实现实时视频分析功能的高层抽象。为了充分发挥边缘的功能，VideoEdge [[109](#bib.bib109)]
    实现了一个端-边缘-云分层架构，以帮助实现分析任务的负载均衡，同时保持高分析准确性。
- en: IV-A3 Cloud Level
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 云级
- en: At the cloud level, the cloud is responsible for the integration of DL models
    among the edge layer and updating parameters of distributed DL models on edge
    nodes [[102](#bib.bib102)]. Since the distributed model training performance on
    an edge node may be significantly impaired due to its local knowledge, the cloud
    needs to integrate different well-trained DL models to achieve global knowledge.
    When the edge is unable to provide the service confidently (e.g., detecting objects
    with low confidence), the cloud can use its powerful computing power and global
    knowledge for further processing and assist the edge nodes to update DL models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在云级，云负责整合边缘层之间的 DL 模型，并更新边缘节点上分布式 DL 模型的参数 [[102](#bib.bib102)]。由于边缘节点上的分布式模型训练性能可能因为本地知识的局限而显著下降，云需要整合不同的训练良好的
    DL 模型以实现全局知识。当边缘无法自信地提供服务（例如，以低置信度检测物体）时，云可以利用其强大的计算能力和全局知识进行进一步处理，并协助边缘节点更新 DL
    模型。
- en: IV-B Autonomous Internet of Vehicles (IoVs)
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 自主互联网汽车（IoVs）
- en: It is envisioned that vehicles can be connected to improve safety, enhance efficiency,
    reduce accidents, and decrease traffic congestion in transportation systems [[110](#bib.bib110)].
    There are many information and communication technologies such as networking,
    caching, edge computing which can be used for facilitating the IoVs, though usually
    studied respectively. On one hand, edge computing provides low-latency, high-speed
    communication and fast-response services for vehicles, making automatic driving
    possible. On the other hand, DL techniques are important in various smart vehicle
    applications. Further, they are expected to optimize complex IoVs systems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 设想中，车辆可以通过连接来提高安全性、增强效率、减少事故，并降低交通拥堵 [[110](#bib.bib110)]。有许多信息和通信技术，如网络、缓存、边缘计算，可以用于促进
    IoVs，尽管这些技术通常是分别研究的。一方面，边缘计算为车辆提供低延迟、高速通信和快速响应服务，使自动驾驶成为可能。另一方面，深度学习（DL）技术在各种智能车辆应用中非常重要。此外，预计它们将优化复杂的
    IoVs 系统。
- en: In [[110](#bib.bib110)], a framework which integrates these technologies is
    proposed. This integrated framework enables dynamic orchestration of networking,
    caching and computation resources to meet requirements of different vehicular
    applications [[110](#bib.bib110)]. Since this system involves multi-dimensional
    control, a DRL-based approach is first utilized to solve the optimization problem
    for enhancing the holistic system performance. Similarly, DRL is also used in
    [[111](#bib.bib111)] to obtain the optimal task offloading policy in vehicular
    edge computing. Besides, Vehicle-to-Vehicle (V2V) communication technology can
    be taken advantaged to further connect vehicles, either as an edge node or an
    end device managed by DRL-based control policies [[112](#bib.bib112)].
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[110](#bib.bib110)]中，提出了一种集成这些技术的框架。该集成框架使得网络、缓存和计算资源的动态调度成为可能，以满足不同车辆应用的需求[[110](#bib.bib110)]。由于该系统涉及多维控制，因此首先利用基于深度强化学习（DRL）的方法来解决优化问题，以提升整体系统性能。类似地，[[111](#bib.bib111)]中也使用DRL来获取车辆边缘计算中的最优任务卸载策略。此外，可以利用车与车（V2V）通信技术进一步连接车辆，无论是作为边缘节点还是由DRL控制策略管理的终端设备[[112](#bib.bib112)]。
- en: IV-C Intelligent Manufacturing
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 智能制造
- en: Two most important principles in the intelligent manufacturing era are automation
    and data analysis, the former one of which is the main target and the latter one
    is one of the most useful tools [[113](#bib.bib113)]. In order to follow these
    principles, intelligent manufacturing should first address response latency, risk
    control, and privacy protection, and hence requires DL and edge computing. In
    intelligent factories, edge computing is conducive to expand the computation resources,
    the network bandwidth, and the storage capacity of the cloud to the IoT edge,
    as well as realizing the resource scheduling and data processing during manufacturing
    and production [[114](#bib.bib114)]. For autonomous manufacturing inspection,
    DeepIns [[113](#bib.bib113)] uses DL and edge computing to guarantee performance
    and process delay respectively. The main idea of this system is partitioning the
    DL model, used for inspection, and deploying them on the end, edge and cloud layer
    separately for improving the inspection efficiency.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能制造时代，两个最重要的原则是自动化和数据分析，其中前者是主要目标，后者是最有用的工具之一[[113](#bib.bib113)]。为了遵循这些原则，智能制造首先需要解决响应延迟、风险控制和隐私保护，因此需要深度学习和边缘计算。在智能工厂中，边缘计算有助于将计算资源、网络带宽和云存储容量扩展到物联网边缘，并实现制造和生产过程中的资源调度和数据处理[[114](#bib.bib114)]。对于自主制造检查，DeepIns[[113](#bib.bib113)]利用深度学习和边缘计算分别保证性能和处理延迟。该系统的主要思想是将用于检查的深度学习模型进行分区，并将其分别部署在端、边缘和云层，以提高检查效率。
- en: Nonetheless, with the exponential growth of IoT edge devices, 1) how to remotely
    manage evolving DL models and 2) how to continuously evaluate these models for
    them are necessary. In [[115](#bib.bib115)], a framework, dealing with these challenges,
    is developed to support complex-event learning during intelligent manufacturing,
    thus facilitating the development of real-time application on IoT edge devices.
    Besides, the power, energy efficiency, memory footprint limitation of IoT edge
    devices [[116](#bib.bib116)] should also be considered. Therefore, caching, communication
    with heterogeneous IoT devices, and computation offloading can be integrated [[117](#bib.bib117)]
    to break the resource bottleneck.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，随着物联网边缘设备的指数级增长，1）如何远程管理不断发展的深度学习模型，以及2）如何持续评估这些模型是必需的。在[[115](#bib.bib115)]中，开发了一种框架来应对这些挑战，以支持智能制造过程中的复杂事件学习，从而促进实时应用在物联网边缘设备上的发展。此外，还应考虑物联网边缘设备的功率、能源效率和内存占用限制[[116](#bib.bib116)]。因此，可以整合缓存、与异构物联网设备的通信以及计算卸载[[117](#bib.bib117)]，以突破资源瓶颈。
- en: IV-D Smart Home and City
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 智能家居与城市
- en: The popularity of IoTs will bring more and more intelligent applications to
    home life, such as intelligent lighting control systems, smart televisions, and
    smart air conditioners. But at the same time, smart homes need to deploy numerous
    wireless IoT sensors and controllers in corners, floors, and walls. For the protection
    of sensitive home data, the data processing of smart home systems must rely on
    edge computing. Like use cases in [[118](#bib.bib118), [119](#bib.bib119)], edge
    computing is deployed to optimize indoor positioning systems and home intrusion
    monitoring so that they can get lower latency than using cloud computing as well
    as the better accuracy. Further, the combination of DL and edge computing can
    make these intelligent services become more various and powerful. For instance,
    it endows robots the ability of dynamic visual servicing [[120](#bib.bib120)]
    and enables efficient music cognition system [[121](#bib.bib121)].
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网的普及将带来越来越多的智能应用到家庭生活中，如智能照明控制系统、智能电视和智能空调。但同时，智能家居需要在角落、地板和墙壁上部署大量无线物联网传感器和控制器。为了保护敏感的家庭数据，智能家居系统的数据处理必须依赖边缘计算。如在[[118](#bib.bib118),
    [119](#bib.bib119)]中的使用案例所示，边缘计算被部署以优化室内定位系统和家庭入侵监控，从而比使用云计算获得更低的延迟和更好的准确性。此外，深度学习与边缘计算的结合可以使这些智能服务变得更加多样和强大。例如，它赋予机器人动态视觉服务能力[[120](#bib.bib120)]，并使高效的音乐认知系统得以实现[[121](#bib.bib121)]。
- en: If the smart home is enlarged to a community or city, public safety, health
    data, public facilities, transportation, and other fields can benefit. The original
    intention of applying edge computing in smart cities is more due to cost and efficiency
    considerations. The natural characteristic of geographically distributed data
    sources in cities requires an edge computing-based paradigm to offer location-awareness
    and latency-sensitive monitoring and intelligent control. For instance, the hierarchical
    distributed edge computing architecture in [[122](#bib.bib122)] can support the
    integration of massive infrastructure components and services in future smart
    cities. This architecture can not only support latency-sensitive applications
    on end devices but also perform slightly latency-tolerant tasks efficiently on
    edge nodes, while large-scale DL models responsible for deep analysis are hosted
    on the cloud. Besides, DL can be utilized to orchestrate and schedule infrastructures
    to achieve the holistic load balancing and optimal resource utilization among
    a region of a city (e.g., within a campus [[123](#bib.bib123)]) or the whole city.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能家居扩大到一个社区或城市，公共安全、健康数据、公共设施、交通等领域都能从中受益。将边缘计算应用于智能城市的初衷更多是出于成本和效率的考虑。城市中地理分布的数据源的自然特性要求基于边缘计算的模式提供位置感知和延迟敏感的监控与智能控制。例如，[[122](#bib.bib122)]中的分层分布式边缘计算架构可以支持未来智能城市中大量基础设施组件和服务的集成。这种架构不仅可以支持终端设备上的延迟敏感应用，还可以在边缘节点上高效执行稍微能容忍延迟的任务，而负责深度分析的大规模深度学习模型则托管在云端。此外，深度学习可以用来编排和调度基础设施，实现区域内（如校园内[[123](#bib.bib123)]）或整个城市的全面负载均衡和资源优化利用。
- en: V Deep Learning Inference in Edge
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 深度学习推断在边缘
- en: In order to further improve the accuracy, DNNs become deeper and require larger-scale
    dataset. By this means, dramatic computation costs are introduced. Certainly,
    the outstanding performance of DL models is inseparable from the support of high-level
    hardware, and it is difficult to deploy them in the edge with limited resources.
    Therefore, large-scale DL models are generally deployed in the cloud while end
    devices just send input data to the cloud and then wait for the DL inference results.
    However, the cloud-only inference limits the ubiquitous deployment of DL services.
    Specifically, it can not guarantee the delay requirement of real-time services,
    e.g., real-time detection with strict latency demands. Moreover, for important
    data sources, data safety and privacy protection should be addressed. To deal
    with these issues, DL services tend to resort to edge computing. Therefore, DL
    models should be further customized to fit in the resource-constrained edge, while
    carefully treating the trade-off between the inference accuracy and the execution
    latency of them.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高准确性，深度神经网络（DNN）变得更深，并需要更大规模的数据集。这会引入剧烈的计算成本。当然，深度学习（DL）模型的卓越性能离不开高端硬件的支持，但在资源有限的边缘环境中部署它们较为困难。因此，大规模的DL模型通常在云端部署，而终端设备只需将输入数据发送到云端，然后等待DL推断结果。然而，仅依赖云端推断限制了DL服务的普遍部署。具体来说，这不能保证实时服务的延迟要求，例如具有严格延迟要求的实时检测。此外，对于重要的数据源，数据安全和隐私保护也应得到解决。为了解决这些问题，DL服务往往依赖边缘计算。因此，DL模型应进一步定制以适应资源受限的边缘，同时仔细权衡推断准确性和执行延迟之间的权衡。
- en: V-A Optimization of DL Models in Edge
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 边缘计算中的深度学习模型优化
- en: 'DL tasks are usually computationally intensive and requires large memory footprints.
    But in the edge, there are not enough resources to support raw large-scale DL
    models. Optimizing DL models and quantize their weights can reduce resource costs.
    In fact, model redundancies are common in DNNs [[124](#bib.bib124), [125](#bib.bib125)]
    and can be utilized to make model optimization possible. The most important challenge
    is how to ensure that there is no significant loss in model accuracy after being
    optimized. In other words, the optimization approach should transform or re-design
    DL models and make them fit in edge devices, with as little loss of model performance
    as possible. In this section, optimization methods for different scenarios are
    discussed: 1) general optimization methods for edge nodes with relatively sufficient
    resources; 2) fine-grained optimization methods for end devices with tight resource
    budgets.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: DL任务通常计算密集并且需要大内存。但在边缘环境中，没有足够的资源来支持原始的大规模DL模型。优化DL模型和量化其权重可以减少资源成本。实际上，DNN中模型冗余是常见的[[124](#bib.bib124)，[125](#bib.bib125)]，这些冗余可以用来实现模型优化。最重要的挑战是如何确保在优化后模型准确性没有显著下降。换句话说，优化方法应该转化或重新设计DL模型，使其适应边缘设备，并尽可能少地损失模型性能。本节讨论了不同场景下的优化方法：1）针对相对资源充足的边缘节点的一般优化方法；2）针对资源预算紧张的终端设备的精细化优化方法。
- en: V-A1 General Methods for Model Optimization
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 模型优化的一般方法
- en: 'On one hand, increasing the depth and width of DL models with nearly constant
    computation overhead is one direction of optimization, such as inception [[126](#bib.bib126)]
    and deep residual networks [[127](#bib.bib127)] for CNNs. On the other hand, for
    more general neural network structures, existing optimization methods can be divided
    into four categories [[128](#bib.bib128)]: 1) parameter pruning and sharing [[129](#bib.bib129),
    [130](#bib.bib130)], including also weights quantization [[131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133)]; 2) low-rank factorization [[124](#bib.bib124)];
    3) transferred/compact convolution filters [[134](#bib.bib134), [135](#bib.bib135),
    [107](#bib.bib107)]; 4) knowledge distillation [[136](#bib.bib136)]. These approaches
    can be applied to different kinds of DNNs or be composed to optimize a complex
    DL model for the edge.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，增加DL模型的深度和宽度，同时计算开销几乎保持不变，是一种优化方向，如用于CNN的inception[[126](#bib.bib126)]和深度残差网络[[127](#bib.bib127)]。另一方面，对于更一般的神经网络结构，现有的优化方法可以分为四类[[128](#bib.bib128)]：1）参数剪枝和共享[[129](#bib.bib129)，[130](#bib.bib130)]，还包括权重量化[[131](#bib.bib131)，[132](#bib.bib132)，[133](#bib.bib133)];
    2）低秩分解[[124](#bib.bib124)]; 3）转移/紧凑卷积滤波器[[134](#bib.bib134)，[135](#bib.bib135)，[107](#bib.bib107)];
    4）知识蒸馏[[136](#bib.bib136)]。这些方法可以应用于不同类型的DNN或组合应用以优化边缘计算中的复杂DL模型。
- en: V-A2 Model Optimization for Edge Devices
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 边缘设备的模型优化
- en: In addition to limited computing and memory footprint, other factors such as
    network bandwidth and power consumption also need to be considered. In this section,
    efforts for running DL on edge devices are differentiated and discussed.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有限的计算和内存占用外，还需要考虑其他因素，如网络带宽和功耗。在本节中，对在边缘设备上运行DL的努力进行了区分和讨论。
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Input: Each application scenario has specific optimization spaces. Concerning
    object detection, FFS-VA uses two prepositive stream-specialized filters and a
    small full-function tiny-YOLO model to filter out vast but non-target-object frames
    [[137](#bib.bib137)]. In order to adjust the configuration of the input video
    stream (such as frame resolution and sampling rate) online with low cost, Chameleon
    [[138](#bib.bib138)] greatly saves the cost of searching the best model configuration
    by leveraging temporal and spatial correlations of the video inputs, and allows
    the cost to be amortized over time and across multiple video feeds. Besides, as
    depicted in Fig. [12](#S5.F12 "Figure 12 ‣ 1st item ‣ V-A2 Model Optimization
    for Edge Devices ‣ V-A Optimization of DL Models in Edge ‣ V Deep Learning Inference
    in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey"),
    narrowing down the classifier’s searching space [[139](#bib.bib139)] and dynamic
    Region-of-Interest (RoI) encoding [[140](#bib.bib140)] to focus on target objects
    in video frames can further reduce the bandwidth consumption and data transmission
    delay. Though this kind of methods can significantly compress the size of model
    inputs and hence reduce the computation overhead without altering the structure
    of DL models, it requires a deep understanding of the related application scenario
    to dig out the potential optimization space.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型输入：每个应用场景都有特定的优化空间。针对目标检测，FFS-VA使用两个前置流专用过滤器和一个小型全功能小型YOLO模型来筛选出大量但非目标物体帧[[137](#bib.bib137)]。为了以低成本在线调整输入视频流的配置（如帧分辨率和采样率），Chameleon
    [[138](#bib.bib138)]通过利用视频输入的时间和空间相关性大大节省了搜索最佳模型配置的成本，并允许成本随时间和跨多个视频源进行摊销。此外，如图[12](#S5.F12
    "图12 ‣ 1st item ‣ V-A2模型优化用于边缘设备 ‣ V-A边缘DL模型的优化 ‣ V边缘深度学习推断 ‣ 边缘计算和深度学习的融合：一项全面调查")所示，缩小分类器的搜索空间[[139](#bib.bib139)]和动态感兴趣区域（RoI）编码[[140](#bib.bib140)]以聚焦视频帧中的目标对象可以进一步减少带宽消耗和数据传输延迟。尽管这种方法可以显著压缩模型输入的大小，从而减少DL模型的计算开销，而不改变其结构，但这需要对相关应用场景有深刻的理解，以挖掘潜在的优化空间。
- en: '![Refer to caption](img/0a29b1689d2994bf1db8a7e54c60345e.png)'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考标题](img/0a29b1689d2994bf1db8a7e54c60345e.png)'
- en: 'Figure 12: Optimization for model inputs, e.g., narrowing down the searching
    space of DL models (pictures are with permission from [[141](#bib.bib141)]).'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12：模型输入的优化，例如缩小DL模型的搜索空间（图片取自[[141](#bib.bib141)]，经允许使用）。
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Structure: Not paying attention to specific applications, but focusing
    on the widely used DNNs’ structures is also feasible. For instance, point-wise
    group convolution and channel shuffle [[142](#bib.bib142)], paralleled convolution
    and pooling computation [[143](#bib.bib143)], depth-wise separable convolution
    [[107](#bib.bib107)] can greatly reduce computation cost while maintaining accuracy.
    NoScope [[144](#bib.bib144)] leverages two types of models rather than the standard
    model (such as YOLO [[9](#bib.bib9)]): specialized models that waive the generality
    of standard models in exchange for faster inference, and difference detectors
    that identify temporal differences across input data. After performing efficient
    cost-based optimization of the model architecture and thresholds for each model,
    NoScope can maximize the throughput of DL services and by cascading these models.
    Besides, as depicted in Fig. [13](#S5.F13 "Figure 13 ‣ 2nd item ‣ V-A2 Model Optimization
    for Edge Devices ‣ V-A Optimization of DL Models in Edge ‣ V Deep Learning Inference
    in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey"),
    parameters pruning can be applied adaptively in model structure optimization as
    well [[145](#bib.bib145), [146](#bib.bib146), [147](#bib.bib147)]. Furthermore,
    the optimization can be more efficient if across the boundary between algorithm,
    software and hardware. Specifically, general hardware is not ready for the irregular
    computation pattern introduced by model optimization. Therefore, hardware architectures
    should be designed to work directly for optimized models [[145](#bib.bib145)].'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型结构：不关注具体应用，而是专注于广泛使用的 DNN 结构也是可行的。例如，点对点组卷积和通道重排 [[142](#bib.bib142)]，并行卷积和池化计算
    [[143](#bib.bib143)]，深度可分离卷积 [[107](#bib.bib107)] 可以在保持准确性的同时大大降低计算成本。NoScope
    [[144](#bib.bib144)] 利用两种模型类型而不是标准模型（如 YOLO [[9](#bib.bib9)]）：专门模型通过放弃标准模型的通用性来换取更快的推理速度，以及识别输入数据中时间差异的差异检测器。经过对模型架构和每个模型的阈值进行高效的基于成本的优化后，NoScope
    可以最大化 DL 服务的吞吐量，并通过级联这些模型。此外，如图 [13](#S5.F13 "图 13 ‣ 第二项 ‣ V-A2 边缘设备模型优化 ‣ V-A
    DL 模型优化 ‣ V 边缘深度学习推理 ‣ 边缘计算与深度学习的融合：全面调查") 所示，参数剪枝也可以在模型结构优化中自适应地应用 [[145](#bib.bib145)，[146](#bib.bib146)，[147](#bib.bib147)]。此外，如果跨越算法、软件和硬件的边界进行优化，效率会更高。具体而言，一般硬件尚未准备好处理模型优化引入的不规则计算模式。因此，硬件架构应设计为直接支持优化后的模型
    [[145](#bib.bib145)]。
- en: '![Refer to caption](img/a2eee14f20f1368c6b59ffe904719ad2.png)'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/a2eee14f20f1368c6b59ffe904719ad2.png)'
- en: 'Figure 13: Adaptive parameters pruning in model structure optimization.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13：模型结构优化中的自适应参数剪枝。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Selection: With various DL models, choosing the best one from available
    DL models in the edge requires weighing both precision and inference time. In
    [[148](#bib.bib148)], the authors use $k$NN to automatically construct a predictor,
    composed of DL models arranged in sequence. Then, the model selection can be determined
    by that predictor along with a set of automatically tuned features of the model
    input. Besides, combining different compression techniques (such as model pruning),
    multiple compressed DL models with different tradeoffs between the performance
    and the resource requirement can be derived. AdaDeep [[149](#bib.bib149)] explores
    the desirable balance between performance and resource constraints, and based
    on DRL, automatically selects various compression techniques (such as model pruning)
    to form a compressed model according to current available resources, thus fully
    utilizing the advantages of them.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择：在各种 DL 模型中，从可用的 DL 模型中选择最佳模型需要权衡精度和推理时间。在 [[148](#bib.bib148)] 中，作者使用 $k$NN
    自动构建一个预测器，由按序排列的 DL 模型组成。然后，可以通过该预测器以及一组自动调节的模型输入特征来确定模型选择。此外，通过结合不同的压缩技术（如模型剪枝），可以得到多个压缩的
    DL 模型，这些模型在性能和资源需求之间具有不同的权衡。AdaDeep [[149](#bib.bib149)] 探索了性能和资源约束之间的理想平衡，并基于
    DRL 自动选择各种压缩技术（如模型剪枝），以根据当前可用资源形成压缩模型，从而充分利用这些优势。
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Framework: Given the high memory footprint and computational demands
    of DL, running them on edge devices requires expert-tailored software and hardware
    frameworks. A software framework is valuable if it 1) provides a library of optimized
    software kernels to enable deployment of DL [[150](#bib.bib150)]; 2) automatically
    compresses DL models into smaller dense matrices by finding the minimum number
    of non-redundant hidden elements [[151](#bib.bib151)]; 3) performs quantization
    and coding on all commonly used DL structures [[146](#bib.bib146), [152](#bib.bib152),
    [151](#bib.bib151)]; 4) specializes DL models to contexts and shares resources
    across multiple simultaneously executing DL models [[152](#bib.bib152)]. With
    respect to the hardware, running DL models on Static Random Access Memory (SRAM)
    achieves better energy savings compared to Dynamic RAM (DRAM) [[146](#bib.bib146)].
    Hence, DL performance can be benefited if underlying hardware directly supports
    running optimized DL models [[153](#bib.bib153)] on the on-chip SRAM.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型框架：鉴于深度学习的高内存占用和计算需求，在边缘设备上运行它们需要专家量身定制的软件和硬件框架。如果一个软件框架具有以下特点，则其价值更高：1) 提供优化的软件内核库以支持深度学习部署[[150](#bib.bib150)]；2)
    通过找到最少的非冗余隐藏元素，将深度学习模型自动压缩成更小的稠密矩阵[[151](#bib.bib151)]；3) 对所有常用的深度学习结构进行量化和编码[[146](#bib.bib146)、[152](#bib.bib152)、[151](#bib.bib151)]；4)
    专门为上下文定制深度学习模型，并在多个同时执行的深度学习模型之间共享资源[[152](#bib.bib152)]。在硬件方面，与动态随机存取内存（DRAM）相比，运行在静态随机存取内存（SRAM）上的深度学习模型可以实现更好的能量节省[[146](#bib.bib146)]。因此，如果底层硬件直接支持在片上SRAM上运行优化的深度学习模型[[153](#bib.bib153)]，则可以提高深度学习性能。
- en: V-B Segmentation of DL Models
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 深度学习模型的分段
- en: In [[12](#bib.bib12)], the delay and power consumption of the most advanced
    DL models are evaluated on the cloud and edge devices, finding that uploading
    data to the cloud is the bottleneck of current DL servicing methods (leading to
    a large overhead of transmitting). Dividing the DL model and performing distributed
    computation can achieve better end-to-end delay performance and energy efficiency.
    In addition, by pushing part of DL tasks from the cloud to the edge, the throughput
    of the cloud can be improved. Therefore, the DL model can be segmented into multiple
    partitions and then allocated to 1) heterogeneous local processors (e.g., GPUs,
    CPUs) on the end device [[154](#bib.bib154)], 2) distributed edge nodes [[155](#bib.bib155),
    [156](#bib.bib156)], or 3) collaborative “end-edge-cloud” architecture [[157](#bib.bib157),
    [158](#bib.bib158), [12](#bib.bib12), [49](#bib.bib49)].
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[12](#bib.bib12)]中，评估了最先进的深度学习模型在云端和边缘设备上的延迟和功耗，发现将数据上传到云端是当前深度学习服务方法的瓶颈（导致了大量的传输开销）。将深度学习模型划分并进行分布式计算可以实现更好的端到端延迟性能和能效。此外，通过将部分深度学习任务从云端推送到边缘，可以提高云端的吞吐量。因此，可以将深度学习模型分割成多个部分，然后分配到1)
    异构本地处理器（例如，GPU、CPU）[[154](#bib.bib154)]，2) 分布式边缘节点[[155](#bib.bib155)、[156](#bib.bib156)]，或3)
    协作的“端-边缘-云”架构[[157](#bib.bib157)、[158](#bib.bib158)、[12](#bib.bib12)、[49](#bib.bib49)]。
- en: 'Partitioning the DL model horizontally, i.e., along the end, edge and cloud,
    is the most common segmentation method. The challenge lies in how to intelligently
    select the partition points. As illustrated in Fig. [14](#S5.F14 "Figure 14 ‣
    V-B Segmentation of DL Models ‣ V Deep Learning Inference in Edge ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey"), a general process
    for determining the partition point can be divided into three steps [[157](#bib.bib157),
    [12](#bib.bib12)]: 1) measuring and modeling the resource cost of different DNN
    layers and the size of intermediate data between layers; 2) predicting the total
    cost by specific layer configurations and network bandwidth; 3) choosing the best
    one from candidate partition points according to delay, energy requirements, etc.
    Another kind of model segmentation is vertically partitioning particularly for
    CNNs [[156](#bib.bib156)]. In contrast to horizontal partition, vertical partition
    fuses layers and partitions them vertically in a grid fashion, and thus divides
    CNN layers into independently distributable computation tasks.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '对深度学习模型进行水平划分，即沿着端、边缘和云，是最常见的分割方法。挑战在于如何智能地选择划分点。如图[14](#S5.F14 "Figure 14
    ‣ V-B Segmentation of DL Models ‣ V Deep Learning Inference in Edge ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")所示，确定划分点的一般过程可以分为三个步骤[[157](#bib.bib157),
    [12](#bib.bib12)]：1) 测量和建模不同DNN层的资源成本以及层间中间数据的大小；2) 根据特定的层配置和网络带宽预测总成本；3) 根据延迟、能量需求等从候选划分点中选择最佳点。另一种模型分割方法是垂直划分，特别是对于卷积神经网络（CNN）[[156](#bib.bib156)]。与水平划分相比，垂直划分将层融合并以网格方式垂直划分，从而将CNN层分成可以独立分配的计算任务。'
- en: '![Refer to caption](img/77a69832a770696b509418feb23ad22a.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/77a69832a770696b509418feb23ad22a.png)'
- en: 'Figure 14: Segmentation of DL models in the edge.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：边缘中的深度学习模型分割。
- en: V-C Early Exit of Inference (EEoI)
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 推理的早期退出（EEoI）
- en: To reach the best trade-off between model accuracy and processing delay, multiple
    DL models with different model performance and resource cost can be maintained
    for each DL service. Then, by intelligently selecting the best model, the desired
    adaptive inference is achieved [[159](#bib.bib159)]. Nonetheless, this idea can
    be further improved by the emerged EEoI [[160](#bib.bib160)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在模型准确性和处理延迟之间达到最佳权衡，可以为每个深度学习服务维护多个具有不同模型性能和资源成本的深度学习模型。然后，通过智能选择最佳模型，实现所需的自适应推理[[159](#bib.bib159)]。然而，这一思路可以通过新出现的EEoI[[160](#bib.bib160)]进一步改进。
- en: The performance improvement of additional layers in DNNs is at the expense of
    increased latency and energy consumption in feedforward inference. As DNNs grow
    larger and deeper, these costs become more prohibitive for edge devices to run
    real-time and energy-sensitive DL applications. By additional side branch classifiers,
    for partial samples, EEoI allows inference to exit early via these branches if
    with high confidence. For more difficult samples, EEoI will use more or all DNN
    layers to provide the best predictions.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络（DNN）中，增加额外层数的性能提升是以增加前向推理的延迟和能耗为代价的。随着DNN规模的扩大和加深，这些成本变得更加难以承受，使得边缘设备运行实时和能量敏感的深度学习应用变得困难。通过额外的分支分类器，对于部分样本，EEoI允许通过这些分支在高信心的情况下提前退出推理。对于更困难的样本，EEoI将使用更多或所有DNN层来提供最佳预测。
- en: 'As depicted in Fig. [15](#S5.F15 "Figure 15 ‣ V-C Early Exit of Inference (EEoI)
    ‣ V Deep Learning Inference in Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"), by taking advantage of EEoI, fast and localized inference
    using shallow portions of DL models at edge devices can be enabled. By this means,
    the shallow model on the edge device can quickly perform initial feature extraction
    and, if confident, can directly give inference results. Otherwise, the additional
    large DL model deployed in the cloud performs further processing and final inference.
    Compared to directly offloading DL computation to the cloud, this approach has
    lower communication costs and can achieve higher inference accuracy than those
    of the pruned or quantized DL models on edge devices [[113](#bib.bib113), [161](#bib.bib161)].
    In addition, since only immediate features rather than the original data are sent
    to the cloud, it provides better privacy protection. Nevertheless, EEoI shall
    not be deemed independent to model optimization (Section [V-A2](#S5.SS1.SSS2 "V-A2
    Model Optimization for Edge Devices ‣ V-A Optimization of DL Models in Edge ‣
    V Deep Learning Inference in Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")) and segmentation (Section [V-B](#S5.SS2 "V-B Segmentation
    of DL Models ‣ V Deep Learning Inference in Edge ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")). The envision of distributed DL over
    the end, edge and cloud should take their collaboration into consideration, e.g.,
    developing a collaborative and on-demand co-inference framework [[162](#bib.bib162)]
    for adaptive DNN partitioning and EEoI.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[15](#S5.F15 "Figure 15 ‣ V-C Early Exit of Inference (EEoI) ‣ V Deep Learning
    Inference in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")所示，通过利用EEoI，可以在边缘设备上实现快速和本地化的推理，使用DL模型的浅层部分。通过这种方式，边缘设备上的浅层模型可以快速执行初步特征提取，如果有信心，可以直接给出推理结果。否则，部署在云端的大型DL模型将进行进一步处理和最终推理。与直接将DL计算卸载到云端相比，这种方法具有更低的通信成本，并且可以实现比边缘设备上修剪或量化DL模型更高的推理准确度[[113](#bib.bib113),
    [161](#bib.bib161)]。此外，由于仅将即时特征而非原始数据发送到云端，这提供了更好的隐私保护。然而，EEoI不能被视为与模型优化（第[V-A2](#S5.SS1.SSS2
    "V-A2 Model Optimization for Edge Devices ‣ V-A Optimization of DL Models in Edge
    ‣ V Deep Learning Inference in Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")节）和分段（第[V-B](#S5.SS2 "V-B Segmentation of DL Models ‣
    V Deep Learning Inference in Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")节）的模型优化独立。对端、边缘和云的分布式DL的设想应考虑它们的协作，例如，开发一个协作和按需的共同推理框架[[162](#bib.bib162)]，用于自适应DNN分区和EEoI。'
- en: '![Refer to caption](img/08bb6475b1066b0005d0c50a76f41009.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08bb6475b1066b0005d0c50a76f41009.png)'
- en: 'Figure 15: Early exit of inference for DL inference in the edge.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：边缘DL推理的早期退出。
- en: V-D Sharing of DL Computation
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D DL计算共享
- en: The requests from nearby users within the coverage of an edge node may exhibit
    spatiotemporal locality [[163](#bib.bib163)]. For instance, users within the same
    area might request recognition tasks for the same object of interest, and it may
    introduce redundant computation of DL inference. In this case, based on offline
    analysis of applications and online estimates of network conditions, Cachier [[163](#bib.bib163)]
    proposes to cache related DL models for recognition applications in the edge node
    and to minimize expected end-to-end latency by dynamically adjusting its cache
    size. Based on the similarity between consecutive frames in first-person-view
    videos, DeepMon [[164](#bib.bib164)] and DeepCache [[165](#bib.bib165)] utilize
    the internal processing structure of CNN layers to reuse the intermediate results
    of the previous frame to calculate the current frame, i.e., caching internally
    processed data within CNN layers, to reduce the processing latency of continuous
    vision applications.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘节点覆盖范围内附近用户的请求可能表现出时空局部性[[163](#bib.bib163)]。例如，同一地区的用户可能会请求相同对象的识别任务，这可能会引入冗余的DL推理计算。在这种情况下，根据对应用程序的离线分析和对网络条件的在线估计，Cachier
    [[163](#bib.bib163)]建议在边缘节点缓存与识别应用程序相关的DL模型，并通过动态调整缓存大小来最小化期望的端到端延迟。基于第一人称视角视频中连续帧之间的相似性，DeepMon
    [[164](#bib.bib164)]和DeepCache [[165](#bib.bib165)]利用CNN层的内部处理结构重用前一帧的中间结果来计算当前帧，即在CNN层内部缓存处理过的数据，以减少连续视觉应用的处理延迟。
- en: Nevertheless, to proceed with effective caching and results reusing, accurate
    lookup for reusable results shall be addressed, i.e., the cache framework must
    systematically tolerate the variations and evaluate key similarities. DeepCache
    [[165](#bib.bib165)] performs cache key lookup to solve this. Specifically, it
    divides each video frame into fine-grained regions and searches for similar regions
    from cached frames in a specific pattern of video motion heuristics. For the same
    challenge, FoggyCache [[166](#bib.bib166)] first embeds heterogeneous raw input
    data into feature vectors with generic representation. Then, Adaptive Locality
    Sensitive Hashing (A-LSH), a variant of LSH commonly used for indexing high-dimensional
    data, is proposed to index these vectors for fast and accurate lookup. At last,
    Homogenized $k$NN, which utilizes the cached values to remove outliers and ensure
    a dominant cluster among the $k$ records initially chosen, is implemented based
    on $k$NN to determine the reuse output from records looked up by A-LSH.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，为了有效进行缓存和结果重用，必须解决准确查找可重用结果的问题，即缓存框架必须系统地容忍变化并评估关键相似性。DeepCache [[165](#bib.bib165)]
    通过执行缓存键查找来解决这个问题。具体而言，它将每个视频帧划分为细粒度区域，并以特定的视频运动启发式模式从缓存帧中搜索相似区域。对于相同的挑战，FoggyCache
    [[166](#bib.bib166)] 首先将异构原始输入数据嵌入为具有通用表示的特征向量。然后，提出了自适应局部敏感哈希（A-LSH），这是LSH的一个变体，常用于高维数据的索引，以便快速准确地查找这些向量。最后，基于
    $k$NN 实现的同质化 $k$NN 利用缓存值来去除异常值并确保在初步选择的 $k$ 条记录中主导的集群，从而确定从A-LSH查找的记录中的重用输出。
- en: Differ from sharing inference results, Mainstream [[167](#bib.bib167)] proposes
    to adaptively orchestrate DNN stem-sharing (the common part of several specialized
    DL models) among concurrent video processing applications. By exploiting computation
    sharing of specialized models among applications trained through TL from a common
    DNN stem, aggregate per-frame compute time can be significantly decreased. Though
    more specialized DL models mean both higher model accuracy and less shared DNN
    stems, the model accuracy decreases slowly as less-specialized DL models are employed
    (unless the fraction of the model specialized is very small). This characteristic
    hence enables that large portions of the DL model can be shared with low accuracy
    loss in Mainstream.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与共享推理结果不同，Mainstream [[167](#bib.bib167)] 提议自适应地协调DNN stem-sharing（几个专用深度学习模型的共同部分）在并发的视频处理应用程序之间。通过利用应用程序之间通过TL从通用DNN
    stem训练的专用模型的计算共享，可以显著减少每帧的计算时间。尽管更多的专用深度学习模型意味着更高的模型准确性和更少的共享DNN stem，但模型准确性随着不那么专用的深度学习模型的使用而缓慢下降（除非模型的专用部分非常小）。因此，这一特性使得Mainstream能够在较低的准确性损失情况下共享大部分深度学习模型。
- en: VI Edge Computing for Deep Learning
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 边缘计算与深度学习
- en: Extensive deployment of DL services, especially mobile DL, requires the support
    of edge computing. This support is not just at the network architecture level,
    the design, adaptation, and optimization of edge hardware and software are equally
    important. Specifically, 1) customized edge hardware and corresponding optimized
    software frameworks and libraries can help DL execution more efficiently; 2) the
    edge computing architecture can enable the offloading of DL computation; 3) well-designed
    edge computing frameworks can better maintain DL services running on the edge;
    4) fair platforms for evaluating Edge DL performance help further evolve the above
    implementations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习服务的大规模部署，特别是移动深度学习，需要边缘计算的支持。这种支持不仅仅是网络架构层面的，边缘硬件和软件的设计、适配和优化同样重要。具体来说，1)
    定制的边缘硬件和相应优化的软件框架和库可以提高深度学习的执行效率；2) 边缘计算架构可以实现深度学习计算的卸载；3) 设计良好的边缘计算框架可以更好地维持在边缘运行的深度学习服务；4)
    公平的边缘深度学习性能评估平台有助于进一步发展上述实现。
- en: VI-A Edge Hardware for DL
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 深度学习的边缘硬件
- en: VI-A1 Mobile CPUs and GPUs
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 移动CPU和GPU
- en: DL applications are more valuable if directly enabled on lightweight edge devices,
    such as mobile phones, wearable devices, and surveillance cameras, near to the
    location of events. Low-power IoT edge devices can be used to undertake lightweight
    DL computation, and hence avoiding communication with the cloud, but it still
    needs to face limited computation resources, memory footprint, and energy consumption.
    To break through these bottlenecks, in [[143](#bib.bib143)], the authors focus
    on ARM Cortex-M micro-controllers and develop CMSIS-NN, a collection of efficient
    NN kernels. By CMSIS-NN, the memory footprint of NNs on ARM Cortex-M processor
    cores can be minimized, and then the DL model can be fitted into IoT devices,
    meantime achieving normal performance and energy efficiency.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果直接在轻量级边缘设备上启用 DL 应用程序，如手机、可穿戴设备和监控摄像头，这些应用程序将更有价值，因为它们更接近事件发生的位置。低功耗 IoT 边缘设备可以用于承担轻量级的
    DL 计算，从而避免与云端的通信，但仍需面对计算资源、内存占用和能源消耗的限制。为突破这些瓶颈，在 [[143](#bib.bib143)] 中，作者关注
    ARM Cortex-M 微控制器，并开发了 CMSIS-NN，这是一个高效 NN 内核的集合。通过 CMSIS-NN，可以最小化 ARM Cortex-M
    处理器核心上 NNs 的内存占用，从而将 DL 模型适配到 IoT 设备中，同时实现正常的性能和能源效率。
- en: With regard to the bottleneck when running CNN layers on mobile GPUs, DeepMon
    [[164](#bib.bib164)] decomposes the matrices used in the CNN layers to accelerate
    the multiplications between high-dimensional matrices. By this means, high-dimensional
    matrix operations (particularly multiplications) in CNN layers are available in
    mobile GPUs and can be accelerated. In view of this work, various mobile GPUs,
    already deployed in edge devices, can be potentially explored with specific DL
    models and play a more important role in enabling edge DL.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在移动 GPU 上运行 CNN 层时的瓶颈，DeepMon [[164](#bib.bib164)] 分解了 CNN 层中使用的矩阵，以加速高维矩阵之间的乘法。通过这种方式，CNN
    层中的高维矩阵操作（特别是乘法）可以在移动 GPU 上进行并加速。鉴于这项工作，已经在边缘设备中部署的各种移动 GPU 可以与特定的 DL 模型进行潜在探索，并在实现边缘
    DL 中发挥更重要的作用。
- en: Other than DL inference [[143](#bib.bib143), [164](#bib.bib164)], important
    factors that affect the performance of DL training on mobile CPUs and GPUs are
    discussed in [[168](#bib.bib168)]. Since commonly used DL models, such as VGG
    [[169](#bib.bib169)], are too large for the memory size of mainstream edge devices,
    a relatively small Mentee network [[170](#bib.bib170)] is adopted to evaluate
    DL training. Evaluation results point out that the size of DL models is crucial
    for training performance and the efficient fusion of mobile CPUs and GPUs is important
    for accelerating the training process.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 DL 推断 [[143](#bib.bib143), [164](#bib.bib164)]，影响移动 CPU 和 GPU 上 DL 训练性能的重要因素在
    [[168](#bib.bib168)] 中进行了讨论。由于常用的 DL 模型，如 VGG [[169](#bib.bib169)]，对于主流边缘设备的内存大小来说过于庞大，因此采用了相对较小的
    Mentee 网络 [[170](#bib.bib170)] 来评估 DL 训练。评估结果指出，DL 模型的大小对训练性能至关重要，并且移动 CPU 和 GPU
    的高效融合对加速训练过程非常重要。
- en: VI-A2 FPGA-based Solutions
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 基于 FPGA 的解决方案
- en: Though GPU solutions are widely adopted in the cloud for DL training and inference,
    however, restricted by the tough power and cost budget in the edge, these solutions
    may not be available. Besides, edge nodes should be able to serve multiple DL
    computation requests at a time, and it makes simply using lightweight CPUs and
    GPUs impractical. Therefore, edge hardware based on Field Programmable Gate Array
    (FPGA) is explored to study their feasibility for edge DL.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GPU 解决方案在云端广泛用于 DL 训练和推断，但由于边缘设备在功耗和成本预算上的限制，这些解决方案可能无法使用。此外，边缘节点应能够同时处理多个
    DL 计算请求，因此仅使用轻量级的 CPU 和 GPU 是不切实际的。因此，探索基于现场可编程门阵列（FPGA）的边缘硬件，以研究其在边缘 DL 中的可行性。
- en: FPGA-based edge devices can achieve CNN acceleration with arbitrarily sized
    convolution and reconfigurable pooling [[143](#bib.bib143)], and they perform
    faster than the state-of-the-art CPU and GPU implementations [[145](#bib.bib145)]
    with respect to RNN-based speech recognition applications while achieving higher
    energy efficiency. In [[52](#bib.bib52)], the design and setup of an FPGA-based
    edge platform are developed to admit DL computation offloading from mobile devices.
    On implementing the FPGA-based edge platform, a wireless router and an FPGA board
    are combined together. Testing this preliminary system with typical vision applications,
    the FPGA-based edge platform shows its advantages, in terms of both energy consumption
    and hardware cost, over the GPU (or CPU)-based one.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 FPGA 的边缘设备可以通过任意尺寸的卷积和可重构的池化实现 CNN 加速 [[143](#bib.bib143)]，并且在 RNN 基于的语音识别应用中，相比最先进的
    CPU 和 GPU 实现，它们的性能更快，同时能效更高 [[145](#bib.bib145)]。在 [[52](#bib.bib52)] 中，开发了一种
    FPGA 基于的边缘平台，以允许将深度学习计算从移动设备卸载。在实现 FPGA 基于的边缘平台时，将无线路由器和 FPGA 板结合在一起。对这一初步系统进行典型视觉应用测试时，FPGA
    基于的边缘平台在能耗和硬件成本方面显示出相对于 GPU（或 CPU）基础平台的优势。
- en: 'Nevertheless, it is still pended to determine whether FPGAs or GPUs/CPUs are
    more suitable for edge computing, as shown in Table [IV](#S6.T4 "TABLE IV ‣ VI-A2
    FPGA-based Solutions ‣ VI-A Edge Hardware for DL ‣ VI Edge Computing for Deep
    Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey").
    Elaborated experiments are performed in [[171](#bib.bib171)] to investigate the
    advantages of FPGAs over GPUs: 1) capable of providing workload insensitive throughput;
    2) guaranteeing consistently high performance for high-concurrency DL computation;
    3) better energy efficiency. However, the disadvantage of FPGAs lies in that developing
    efficient DL algorithms on FPGA is unfamiliar to most programmers. Although tools
    such as Xilinx SDSoC can greatly reduce the difficulty [[52](#bib.bib52)], at
    least for now, additional works are still required to transplant the state-of-the-art
    DL models, programmed for GPUs, into the FPGA platform.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，尚未确定 FPGA 还是 GPU/CPU 更适合边缘计算，如表 [IV](#S6.T4 "表 IV ‣ VI-A2 基于 FPGA 的解决方案
    ‣ VI-A 边缘硬件用于深度学习 ‣ VI 边缘计算用于深度学习 ‣ 边缘计算与深度学习的融合：综合调查") 所示。在 [[171](#bib.bib171)]
    中进行了详细的实验，以研究 FPGA 相比于 GPU 的优势：1) 能提供与工作负载无关的吞吐量；2) 保证高并发深度学习计算的一致性高性能；3) 更好的能源效率。然而，FPGA
    的劣势在于开发高效的深度学习算法对于大多数程序员来说是不熟悉的。尽管像 Xilinx SDSoC 这样的工具可以大大降低难度 [[52](#bib.bib52)]，但至少目前，仍需额外的工作将针对
    GPU 编写的最先进的深度学习模型移植到 FPGA 平台上。
- en: 'TABLE IV: Comparison of Solutions for Edge nodes'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 边缘节点解决方案比较'
- en: '| Metrics |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 指标 |'
- en: '&#124; Preferred &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 优选 &#124;'
- en: '&#124; Hardware &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬件 &#124;'
- en: '| Analysis |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 分析 |'
- en: '| --- | --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Resource &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 资源 &#124;'
- en: '&#124; overhead &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开销 &#124;'
- en: '| FPGA | FPGA can be optimized by customized designs. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | FPGA 可以通过定制化设计进行优化。 |'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DL &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习 &#124;'
- en: '&#124; training &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '| GPU | Floating point capabilities are better on GPU. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| GPU | GPU 的浮点运算能力更强。 |'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DL &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习 &#124;'
- en: '&#124; inference &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 推理 &#124;'
- en: '| FPGA | FPGA can be customized for specific DL models. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | FPGA 可以针对特定的深度学习模型进行定制。 |'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Interface &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 接口 &#124;'
- en: '&#124; scalability &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可扩展性 &#124;'
- en: '| FPGA | It is more free to implement interfaces on FPGAs. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | 在 FPGA 上实现接口更加自由。 |'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Space &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空间 &#124;'
- en: '&#124; occupation &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 占用 &#124;'
- en: '|'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CPU/ &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU/ &#124;'
- en: '&#124; FPGA &#124;'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FPGA &#124;'
- en: '| Lower power consumption of FPGA leads to smaller space occupation. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| FPGA 的低功耗导致空间占用更小。 |'
- en: '| Compatibility |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 兼容性 |'
- en: '&#124; CPU/ &#124;'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU/ &#124;'
- en: '&#124; GPU &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPU &#124;'
- en: '| CPUs and GPUs have more stable architecture. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| CPU 和 GPU 具有更稳定的架构。 |'
- en: '|'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Development &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发 &#124;'
- en: '&#124; efforts &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 努力 &#124;'
- en: '|'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CPU/ &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CPU/ &#124;'
- en: '&#124; GPU &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPU &#124;'
- en: '| Toolchains and software libraries facilitate the practical development. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 工具链和软件库有助于实际开发。 |'
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Energy &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能耗 &#124;'
- en: '&#124; efficiency &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 效率 &#124;'
- en: '| FPGA | Customized designs can be optimized. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | 定制化设计可以进行优化。 |'
- en: '|'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Concurrency &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并发性 &#124;'
- en: '&#124; support &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持 &#124;'
- en: '| FPGA | FPGAs are suitable for stream process. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | FPGA 适合流处理。 |'
- en: '|'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Timing &#124;'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时序 &#124;'
- en: '&#124; latency &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 延迟 &#124;'
- en: '| FPGA | Timing on FPGAs can be an order of magnitude faster than GPUs. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| FPGA | FPGA上的时序可以比GPU快一个数量级。 |'
- en: VI-B Communication and Computation Modes for Edge DL
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 边缘深度学习的通信和计算模式
- en: 'TABLE V: Details about Edge Communication and Computation Modes for DL'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：深度学习的边缘通信和计算模式详细信息
- en: '|  | Ref. | DL Model | End/Edge/Cloud | Network | Dependency | Objective |
    Performance |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | 参考文献 | 深度学习模型 | 终端/边缘/云 | 网络 | 依赖 | 目标 | 性能 |'
- en: '| Integral Offloading |  &#124; DeepDecision &#124; &#124; [[172](#bib.bib172)]
    &#124;  | YOLO | Samsung Galaxy S7 / Server with a quad-core CPU at 2.7GHz, GTX970
    and 8GB RAM / N/A | Simulated WLAN & LAN | TensorFlow, Darknet | Consider the
    complex interaction between model accuracy, video quality, battery constraints,
    network data usage, and network conditions to determine an optimal offloading
    strategy | Achieve about 15 FPS video analytic while possessing higher accuracy
    than that of the baseline approaches |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 完全卸载 |  &#124; DeepDecision &#124; &#124; [[172](#bib.bib172)] &#124;  |
    YOLO | Samsung Galaxy S7 / 配备2.7GHz四核CPU、GTX970和8GB RAM的服务器 / 无 | 模拟的WLAN & LAN
    | TensorFlow, Darknet | 考虑模型准确性、视频质量、电池约束、网络数据使用和网络条件之间的复杂互动，以确定最佳的卸载策略 | 实现约15
    FPS的视频分析，同时具有比基线方法更高的准确性 |'
- en: '|  |  &#124; MASM &#124; &#124; [[173](#bib.bib173)] &#124;  | $\backslash$
    | Simulated devices / Cloudlet / N/A | $\backslash$ | $\backslash$ | Optimize
    workload assignment weights and the computation capacities of the VMs hosted on
    the Cloudlet | $\backslash$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; MASM &#124; &#124; [[173](#bib.bib173)] &#124;  | $\backslash$
    | 模拟设备 / 云小站 / 无 | $\backslash$ | $\backslash$ | 优化工作负载分配权重和Cloudlet上托管的VM的计算能力
    | $\backslash$ |'
- en: '|  |  &#124; EdgeEye &#124; &#124; [[108](#bib.bib108)] &#124;  | DetectNet,
    FaceNet | Cameras / Server with Intel i7-6700, GTX 1060 and 24GB RAM / N/A | Wi-Fi
    | TensorRT, ParaDrop, Kurento | Offload the live video analytics tasks to the
    edge using EdgeEye API, instead of using DL framework specific APIs, to provide
    higher inference performance | $\backslash$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; EdgeEye &#124; &#124; [[108](#bib.bib108)] &#124;  | DetectNet,
    FaceNet | 摄像头 / 配备Intel i7-6700、GTX 1060和24GB RAM的服务器 / 无 | Wi-Fi | TensorRT,
    ParaDrop, Kurento | 使用EdgeEye API将实时视频分析任务卸载到边缘，而不是使用特定于DL框架的API，以提供更高的推理性能 |
    $\backslash$ |'
- en: '| Partial Offloading |  &#124; DeepWear &#124; &#124; [[174](#bib.bib174)]
    &#124;  | MobileNet, GoogLeNet, DeepSense, etc. | Commodity smartwatches running
    Android Wear OS / Commodity smartphone running Android / N/A | Bluetooth | TensorFlow
    | Provide context-aware offloading, strategic model partition, and pipelining
    support to efficiently utilize the processing capacity of the edge | Bring up
    to 5.08$\times$ and 23.0$\times$ execution speedup, as well as 53.5% and 85.5%
    energy saving against wearable-only and handheld-only strategies, respectively
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 部分卸载 |  &#124; DeepWear &#124; &#124; [[174](#bib.bib174)] &#124;  | MobileNet,
    GoogLeNet, DeepSense等 | 运行Android Wear OS的普通智能手表 / 运行Android的普通智能手机 / 无 | 蓝牙 |
    TensorFlow | 提供上下文感知卸载、战略性模型分区和管道支持，以高效利用边缘的处理能力 | 提供高达5.08$\times$和23.0$\times$的执行加速，并相对于仅穿戴和仅手持策略节省53.5%和85.5%的能源
    |'
- en: '|  |  &#124; IONN &#124; &#124; [[175](#bib.bib175)] &#124;  | AlexNet | Embedded
    board Odroid XU4 / Server with an quad-core CPU at 3.6GHz, GTX 1080 Ti and 32GB
    RAM / Unspecified | WLAN | Caffe | Partitions the DNN layers and incrementally
    uploads the partitions to allow collaborative execution by the end and the edge
    (or cloud) to improves both the query performance and the energy consumption |
    Maintain almost the same uploading latency as integral uploading while largely
    improving query execution time |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; IONN &#124; &#124; [[175](#bib.bib175)] &#124;  | AlexNet | 嵌入式板Odroid
    XU4 / 配备3.6GHz四核CPU、GTX 1080 Ti和32GB RAM的服务器 / 未指定 | WLAN | Caffe | 对DNN层进行分区，并逐步上传这些分区，以允许终端和边缘（或云）进行协作执行，从而提高查询性能和能耗
    | 维持几乎相同的上传延迟，同时大幅提升查询执行时间 |'
- en: '| Vertical Collaboration |  [[176](#bib.bib176)]  | CNN, LSTM | Google Nexus
    9 / Server with an quad-core CPU and 16GB RAM / 3 desktops, each with i7-6850K
    and 2$\times$GTX 1080 Ti | WLAN & LAN | Apache Spark, TensorFlow | Perform data
    pre-processing and preliminary learning at the edge to reduce the network traffic,
    so as to speed up the computation in the cloud | Achieve 90% accuracy while reducing
    the execution time and the data transmission |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 垂直协作 |  [[176](#bib.bib176)]  | CNN, LSTM | Google Nexus 9 / 配备四核CPU和16GB
    RAM的服务器 / 3台桌面电脑，每台配有i7-6850K和2$\times$GTX 1080 Ti | WLAN & LAN | Apache Spark,
    TensorFlow | 在边缘进行数据预处理和初步学习，以减少网络流量，从而加快云端计算 | 实现90%的准确率，同时减少执行时间和数据传输 |'
- en: '|  |  &#124; Neurosurgeon &#124; &#124; [[12](#bib.bib12)] &#124;  | AlexNet,
    VGG, Deepface, MNIST, Kaldi, SENNA | Jetson TK1 mobile platform / Server with
    Intel Xeon E5$\times$2, NVIDIA Tesla K40 GPU and 256GB RAM / Unspecified | Wi-Fi,
    LTE & 3G | Caffe | Adapt to various DNN architectures, hardware platforms, wireless
    connections, and server load levels, and choose the partition point for best latency
    and best mobile energy consumption | Improve end-to-end latency by 3.1$\times$
    on average and up to 40.7$\times$, reduce mobile energy consumption by 59.5% on
    average and up to 94.7%, and improve data-center throughput by 1.5$\times$ on
    average and up to 6.7$\times$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; Neurosurgeon &#124; &#124; [[12](#bib.bib12)] &#124;  | AlexNet,
    VGG, Deepface, MNIST, Kaldi, SENNA | Jetson TK1移动平台 / 配备Intel Xeon E5$\times$2、NVIDIA
    Tesla K40 GPU和256GB RAM的服务器 / 未指明 | Wi-Fi, LTE & 3G | Caffe | 适应各种DNN架构、硬件平台、无线连接和服务器负载水平，并选择最佳分区点以实现最佳延迟和最佳移动能效
    | 平均提高端到端延迟3.1$\times$，最高可达40.7$\times$，平均减少移动能耗59.5%，最高可达94.7%，并且数据中心吞吐量平均提高1.5$\times$，最高可达6.7$\times$
    |'
- en: '|  |  [[161](#bib.bib161)]  | BranchyNet | $\backslash$ | $\backslash$ | $\backslash$
    | Minimize communication and resource usage for devices while allowing low-latency
    classification via EEoI | Reduce the communication cost by a factor of over 20$\times$
    while achieving 95% overall accuracy |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  |  [[161](#bib.bib161)]  | BranchyNet | $\backslash$ | $\backslash$ | $\backslash$
    | 最小化设备的通信和资源使用，同时通过EEoI实现低延迟分类 | 通信成本减少超过20$\times$，同时实现95%的总体准确率 |'
- en: '|  |  [[102](#bib.bib102)]  | Faster R-CNN | Xiaomi 6 / Server with i7 6700,
    GTX 980Ti and 32GB RAM / Work station with E5-2683 V3, GTX TitanXp$\times$4 and
    128GB RAM | WLAN & LAN | $\backslash$ | Achieve efficient object detection via
    wireless communications by interactions between the end, the edge and the cloud
    | Lose only 2.5% detection accuracy under the image compression ratio of 60% while
    significantly improving image transmission efficiency |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  |  [[102](#bib.bib102)]  | Faster R-CNN | 小米6 / 配备i7 6700、GTX 980Ti和32GB
    RAM的服务器 / 配备E5-2683 V3、GTX TitanXp$\times$4和128GB RAM的工作站 | WLAN & LAN | $\backslash$
    | 通过终端、边缘和云之间的交互实现高效的对象检测 | 在图像压缩比为60%的情况下，仅损失2.5%的检测准确率，同时显著提高图像传输效率 |'
- en: '|  |  &#124; VideoEdge &#124; &#124; [[109](#bib.bib109)] &#124;  | AlexNet,
    DeepFace, VGG16 | 10 Azure nodes emulating Cameras / 2 Azure nodes / 12 Azure
    nodes | Emulated hierarchical networks | $\backslash$ | Introduce dominant demand
    to identify the best tradeoff between multiple resources and accuracy | Improve
    accuracy by 5.4$\times$ compared to VideoStorm and only lose 6% accuracy of the
    optimum |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; VideoEdge &#124; &#124; [[109](#bib.bib109)] &#124;  | AlexNet,
    DeepFace, VGG16 | 10台Azure节点模拟摄像头 / 2台Azure节点 / 12台Azure节点 | 模拟的层次网络 | $\backslash$
    | 引入主导需求以识别多个资源与准确性之间的最佳权衡 | 与VideoStorm相比，准确性提高了5.4$\times$，且仅损失了6%的最佳准确性 |'
- en: '| Horizontal Collaboration |  &#124; MoDNN &#124; &#124; [[177](#bib.bib177)]
    &#124;  | VGG-16 | Multiple LG Nexus 5 / N/A / N/A | WLAN | MXNet | Partition
    already trained DNN models onto several mobile devices to accelerate DNN computations
    by alleviating device-level computing cost and memory usage | When the number
    of worker nodes increases from 2 to 4, MoDNN can speedup the DNN computation by
    2.17-4.28$\times$ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Horizontal Collaboration |  &#124; MoDNN &#124; &#124; [[177](#bib.bib177)]
    &#124;  | VGG-16 | 多台LG Nexus 5 / 不适用 / 不适用 | WLAN | MXNet | 将已训练的DNN模型分配到多个移动设备上，以通过减轻设备级计算成本和内存使用来加速DNN计算
    | 当工作节点数量从2增加到4时，MoDNN可以使DNN计算速度提高2.17-4.28$\times$ |'
- en: '|  |  [[130](#bib.bib130)]  | VGGNet-E, AlexNet | Xilinx Virtex-7 FPGA simulating
    multiple end devices / N/A / N/A | On-chip simulation | Torch, Vivado HLS | Fuse
    the processing of multiple CNN layers and enable caching of intermediate data
    to save data transfer (bandwidth) | Reduce the total data transfer by 95%, from
    77MB down to 3.6MB per image |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  |  [[130](#bib.bib130)]  | VGGNet-E, AlexNet | Xilinx Virtex-7 FPGA模拟多个终端设备
    / 不适用 / 不适用 | 片上模拟 | Torch, Vivado HLS | 融合多个CNN层的处理并启用中间数据缓存以节省数据传输（带宽） | 将总数据传输量减少95%，从每张图像77MB降至3.6MB
    |'
- en: '|  |  &#124; DeepThings &#124; &#124; [[156](#bib.bib156)] &#124;  | YOLOv2
    | Perfromance-limited Raspberry Pi 3 Model B / Raspberry Pi 3 Model B as gateway
    / N/A | WLAN | Darknet | Employ a scalable Fused Tile Partitioning of CNN layers
    to minimize memory footprint while exposing parallelism and a novel work scheduling
    process to reduce overall execution latency | Reduce memory footprint by more
    than 68% without sacrificing accuracy, improve throughput by 1.7$\times$-2.2$\times$
    and speedup CNN inference by 1.7$\times$-3.5$\times$ |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; DeepThings &#124; &#124; [[156](#bib.bib156)] &#124;  | YOLOv2
    | 性能受限的 Raspberry Pi 3 Model B / 作为网关的 Raspberry Pi 3 Model B / 无 | WLAN | Darknet
    | 采用可扩展的 CNN 层融合切片分区，减少内存占用，同时暴露并行性，并采用新颖的工作调度流程，以减少整体执行延迟 | 在不牺牲准确性的情况下，减少内存占用超过
    68%，提高吞吐量 1.7$\times$-2.2$\times$，加速 CNN 推理 1.7$\times$-3.5$\times$ |'
- en: '|  |  &#124; DeepCham &#124; &#124; [[104](#bib.bib104)] &#124;  | AlexNet
    | Multiple LG G2 / Wi-Fi router connected with a Linux server / N/A | WLAN & LAN
    | Android Caffe, OpenCV, EdgeBoxes | Coordinate participating mobile users for
    collaboratively training a domain-aware adaptation model to improve object recognition
    accuracy | Improve the object recognition accuracy by 150% when compared to that
    achieved merely using a generic DL model |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; DeepCham &#124; &#124; [[104](#bib.bib104)] &#124;  | AlexNet
    | 多台 LG G2 / 与 Linux 服务器连接的 Wi-Fi 路由器 / 无 | WLAN & LAN | Android Caffe, OpenCV,
    EdgeBoxes | 协调参与的移动用户，共同训练领域感知的适应模型，以提高物体识别准确性 | 与仅使用通用 DL 模型相比，提高了150%的物体识别准确性
    |'
- en: '|  |  &#124; LAVEA &#124; &#124; [[106](#bib.bib106)] &#124;  | OpenALPR |
    Raspberry PI 2 & Raspberry PI 3 / Servers with quad-core CPU and 4GB RAM / N/A
    | WLAN & LAN | Docker, Redis | Design various task placement schemes that are
    tailed for inter-edge collaboration to minimize the service response time | Have
    a speedup ranging from 1.3$\times$ to 4$\times$ (1.2$\times$ to 1.7$\times$) against
    running in local (client-cloud confguration) |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '|  |  &#124; LAVEA &#124; &#124; [[106](#bib.bib106)] &#124;  | OpenALPR |
    Raspberry PI 2 和 Raspberry PI 3 / 具有四核 CPU 和 4GB RAM 的服务器 / 无 | WLAN & LAN | Docker,
    Redis | 设计多种任务分配方案，专门针对边缘协作，以最小化服务响应时间 | 相对于本地运行（客户端-云配置），加速范围为1.3$\times$到4$\times$（1.2$\times$到1.7$\times$）
    |'
- en: 'Though on-device DL computation, illustrated in Sec. [V](#S5 "V Deep Learning
    Inference in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"), can cater for lightweight DL services. Nevertheless, an independent
    end device still cannot afford intensive DL computation tasks. The concept of
    edge computing can potentially cope with this dilemma by offloading DL computation
    from end devices to edge or (and) the cloud. Accompanied by the edge architectures,
    DL-centric edge nodes can become the significant extension of cloud computing
    infrastructure to deal with massive DL tasks. In this section, we classify four
    modes for Edge DL computation, as exhibited in Fig. [16](#S6.F16 "Figure 16 ‣
    VI-B Communication and Computation Modes for Edge DL ‣ VI Edge Computing for Deep
    Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey").'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在设备上的 DL 计算，如第 [V](#S5 "V 深度学习推理在边缘 ‣ 边缘计算与深度学习的融合：综合调查") 节所述，可以满足轻量级 DL 服务的需求。然而，独立的终端设备仍无法承担高强度的
    DL 计算任务。边缘计算的概念可以通过将 DL 计算从终端设备卸载到边缘或（和）云来应对这一困境。借助边缘架构，专注于 DL 的边缘节点可以成为云计算基础设施的重要扩展，以处理大量
    DL 任务。在这一节中，我们将边缘 DL 计算分为四种模式，如图 [16](#S6.F16 "图 16 ‣ VI-B 边缘 DL 的通信与计算模式 ‣ VI
    边缘计算用于深度学习 ‣ 边缘计算与深度学习的融合：综合调查") 所示。
- en: '![Refer to caption](img/f71bd06635fe73731402fa36b95abf79.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f71bd06635fe73731402fa36b95abf79.png)'
- en: 'Figure 16: Communication and computation modes for Edge DL.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '图 16: 边缘 DL 的通信与计算模式。'
- en: VI-B1 Integral Offloading
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B1 整体卸载
- en: 'The most natural mode of DL computation offloading is similar to the existed
    “end-cloud” computing, i.e., the end device sends its computation requests to
    the cloud for DL inference results (as depicted in Fig. [16](#S6.F16 "Figure 16
    ‣ VI-B Communication and Computation Modes for Edge DL ‣ VI Edge Computing for
    Deep Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")(a)). This kind of offloading is straightforward by extricating itself
    from DL task decomposition and combinatorial problems of resource optimization,
    which may bring about additional computation cost and scheduling delay, and thus
    simple to implement. In [[172](#bib.bib172)], the proposed distributed infrastructure
    DeepDecision ties together powerful edge nodes with less powerful end devices.
    In DeepDecision, DL inference can be performed on the end or the edge, depending
    on the tradeoffs between the inference accuracy, the inference latency, the DL
    model size, the battery level, and network conditions. With regard to each DL
    task, the end device decides whether locally processing or offloading it to an
    edge node.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 'DL计算卸载的最自然模式类似于现有的“端-云”计算，即终端设备将计算请求发送到云端以获得DL推断结果（如图[16](#S6.F16 "Figure 16
    ‣ VI-B Communication and Computation Modes for Edge DL ‣ VI Edge Computing for
    Deep Learning ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey")(a)所示）。这种卸载方式通过摆脱DL任务分解和资源优化组合问题的额外计算成本和调度延迟，显得直截了当，且易于实现。在[[172](#bib.bib172)]中，提出的分布式基础设施DeepDecision将强大的边缘节点与较弱的终端设备结合起来。在DeepDecision中，DL推断可以在终端或边缘上进行，具体取决于推断准确性、推断延迟、DL模型大小、电池电量和网络条件之间的权衡。对于每个DL任务，终端设备决定是本地处理还是卸载到边缘节点。'
- en: Further, the workload optimization among edge nodes should not be ignored in
    the offloading problem, since edge nodes are commonly resource-restrained compared
    to the cloud. In order to satisfy the delay and energy requirements of accomplishing
    a DL task with limited edge resources, providing DL models with different model
    sizes and performance in the edge can be adopted to fulfill one kind of task.
    Hence, multiple VMs or containers, undertaking different DL models separately,
    can be deployed on the edge node to process DL requests. Specifically, when a
    DL model with lower complexity can meet the requirements, it is selected as the
    serving model. For instance, by optimizing the workload assignment weights and
    computing capacities of VMs, MASM [[173](#bib.bib173)] can reduce the energy cost
    and delay while guaranteeing the DL inference accuracy.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在卸载问题中，不应忽视边缘节点之间的工作负载优化，因为边缘节点通常相对于云资源有限。为了满足完成DL任务所需的延迟和能量要求，可以在边缘提供不同模型大小和性能的DL模型，以完成某种任务。因此，可以在边缘节点上部署多个虚拟机或容器，分别承担不同的DL模型来处理DL请求。具体来说，当较低复杂度的DL模型能够满足要求时，它将被选为服务模型。例如，通过优化虚拟机的工作负载分配权重和计算能力，MASM[[173](#bib.bib173)]可以在保证DL推断准确性的同时，减少能量消耗和延迟。
- en: VI-B2 Partial Offloading
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B2 部分卸载
- en: 'Partially offloading the DL task to the edge is also feasible (as depicted
    in Fig. [16](#S6.F16 "Figure 16 ‣ VI-B Communication and Computation Modes for
    Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")(b)). An offloading system can be developed
    to enable online fine-grained partition of a DL task, and determine how to allocate
    these divided tasks to the end device and the edge node. As exemplified in [[178](#bib.bib178)],
    MAUI, capable of adaptively partitioning general computer programs, can conserve
    an order of magnitude energy by optimizing the task allocation strategies, under
    the network constraints. More importantly, this solution can decompose the whole
    program at runtime instead of manually partitioning of programmers before program
    deploying.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '部分将深度学习（DL）任务卸载到边缘也是可行的（如图[16](#S6.F16 "Figure 16 ‣ VI-B Communication and
    Computation Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")(b)所示）。可以开发一个卸载系统，以实现在线精细划分DL任务，并确定如何将这些划分后的任务分配给终端设备和边缘节点。如[[178](#bib.bib178)]中的例子，MAUI能够自适应地划分通用计算程序，通过优化任务分配策略，在网络约束下节省了一个数量级的能量。更重要的是，这种解决方案可以在运行时对整个程序进行分解，而不是在程序部署前由程序员手动划分。'
- en: Further, particularly for DL computation, DeepWear [[174](#bib.bib174)] abstracts
    a DL model as a Directed Acyclic Graph (DAG), where each node represents a layer
    and each edge represents the data flow among those layers. To efficiently determine
    partial offloading decisions, DeepWear first prunes the DAG by keeping only the
    computation-intensive nodes, and then grouping the repeated sub-DAGs. In this
    manner, the complex DAG can be transformed into a linear and much simpler one,
    thus enabling a linear complexity solution for selecting the optimal partition
    to offload.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，特别针对DL计算，DeepWear [[174](#bib.bib174)] 将DL模型抽象为有向无环图（DAG），其中每个节点代表一个层，每个边代表这些层之间的数据流。为了高效确定部分卸载决策，DeepWear首先通过保留仅计算密集的节点来修剪DAG，然后对重复的子DAG进行分组。通过这种方式，复杂的DAG可以转化为线性且简单的DAG，从而实现选择最佳卸载分区的线性复杂度解决方案。
- en: Nevertheless, uploading a part of the DL model to the edge nodes may still seriously
    delay the whole process of offloading DL computation. To deal with this challenge,
    an incremental offloading system IONN is proposed in [[175](#bib.bib175)]. Differ
    from packing up the whole DL model for uploading, IONN divides a DL model, prepared
    for uploading, into multiple partitions, and uploads them to the edge node in
    sequential. The edge node, receiving the partitioned models, incrementally builds
    the DL model as each partitioned model arrives, while being able to execute the
    offloaded partial DL computation even before the entire DL model is uploaded.
    Therefore, the key lies in the determination concerning the best partitions of
    the DL model and the uploading order. Specifically, on the one hand, DNN layers,
    performance benefit and uploading overhead of which are high and low, respectively,
    are preferred to be uploaded first, and thus making the edge node quickly build
    a partial DNN to achieve the best-expected query performance. On the other hand,
    unnecessary DNN layers, which cannot bring in any performance increase, are not
    uploaded and hence avoiding the offloading.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将DL模型的一部分上传到边缘节点仍可能严重延迟整个DL计算的卸载过程。为应对这一挑战，提出了一种增量卸载系统IONN，如[[175](#bib.bib175)]所示。与打包整个DL模型进行上传不同，IONN将准备上传的DL模型分割成多个分区，并依次上传到边缘节点。接收分割模型的边缘节点会随着每个分割模型的到达逐步构建DL模型，同时能够在整个DL模型上传之前执行已卸载的部分DL计算。因此，关键在于确定DL模型的最佳分区及上传顺序。具体来说，一方面，优先上传DNN层，因为这些层的性能收益高而上传开销低，从而使边缘节点能够快速构建部分DNN以实现最佳期望查询性能。另一方面，不上传那些无法带来性能提升的DNN层，从而避免卸载。
- en: VI-B3 Vertical Collaboration
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B3 垂直协作
- en: 'Expected offloading strategies among “End-Edge” architecture, as discussed
    in Section [VI-B1](#S6.SS2.SSS1 "VI-B1 Integral Offloading ‣ VI-B Communication
    and Computation Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey") and [VI-B2](#S6.SS2.SSS2
    "VI-B2 Partial Offloading ‣ VI-B Communication and Computation Modes for Edge
    DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey"), are feasible for supporting less computation-intensive
    DL services and small-scale concurrent DL queries. However, when a large number
    of DL queries need to be processed at one time, a single edge node is certainly
    insufficient.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '在“端-边缘”架构中的预期卸载策略，如第[VI-B1](#S6.SS2.SSS1 "VI-B1 Integral Offloading ‣ VI-B
    Communication and Computation Modes for Edge DL ‣ VI Edge Computing for Deep Learning
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")节和[VI-B2](#S6.SS2.SSS2
    "VI-B2 Partial Offloading ‣ VI-B Communication and Computation Modes for Edge
    DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey")节所讨论的，适用于支持计算不那么密集的DL服务和小规模并发DL查询。然而，当需要同时处理大量DL查询时，单个边缘节点显然不够用。'
- en: A natural choice of collaboration is the edge performs data pre-processing and
    preliminary learning, when the DL tasks are offloaded. Then, the intermediate
    data, viz., the output of edge architectures, are transmitted to the cloud for
    further DL computation [[176](#bib.bib176)]. Nevertheless, the hierarchical structure
    of DNNs can be further excavated for fitting the vertical collaboration. In [[12](#bib.bib12)],
    all layers of a DNN are profiled on the end device and the edge node in terms
    of the data and computation characteristics, in order to generate performance
    prediction models. Based on these prediction models, wireless conditions and server
    load levels, the proposed Neurosurgeon evaluates each candidate point in terms
    of end-to-end latency or mobile energy consumption and partition the DNN at the
    best one. Then, it decides the allocation of DNN partitions, i.e., which part
    should be deployed on the end, the edge or the cloud, while achieving best latency
    and energy consumption of end devices.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 一种自然的协作选择是边缘进行数据预处理和初步学习，当深度学习任务被卸载时。然后，将中间数据，即边缘架构的输出，传输到云端进行进一步的深度学习计算[[176](#bib.bib176)]。然而，DNN
    的层次结构可以进一步挖掘以适应垂直协作。在[[12](#bib.bib12)]中，所有 DNN 层在终端设备和边缘节点上根据数据和计算特性进行分析，以生成性能预测模型。基于这些预测模型、无线条件和服务器负载水平，所提出的神经外科医生评估每个候选点的端到端延迟或移动能耗，并在最佳位置分割
    DNN。然后，它决定 DNN 分区的分配，即哪个部分应部署在终端、边缘或云端，同时实现最佳的延迟和终端设备的能耗。
- en: 'By taking advantages of EEoI (Section [V-C](#S5.SS3 "V-C Early Exit of Inference
    (EEoI) ‣ V Deep Learning Inference in Edge ‣ Convergence of Edge Computing and
    Deep Learning: A Comprehensive Survey")), vertical collaboration can be more adapted.
    Partitions of a DNN can be mapped onto a distributed computing hierarchy (i.e.,
    the end, the edge and the cloud) and can be trained with multiple early exit points
    [[161](#bib.bib161)]. Therefore, the end and the edge can perform a portion of
    DL inference on themselves rather than directly requesting the cloud. Using an
    exit point after inference, results of DL tasks, the local device is confident
    about, can be given without sending any information to the cloud. For providing
    more accurate DL inference, the intermediate DNN output will be sent to the cloud
    for further inference by using additional DNN layers. Nevertheless, the intermediate
    output, e.g., high-resolution surveillance video streams, should be carefully
    designed much smaller than the raw input, therefore drastically reducing the network
    traffic required between the end and the edge (or the edge and the cloud).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '通过利用 EEoI（第 [V-C](#S5.SS3 "V-C Early Exit of Inference (EEoI) ‣ V Deep Learning
    Inference in Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey") 节），垂直协作可以更好地适应。DNN 的分区可以映射到分布式计算层次结构（即终端、边缘和云端）并可以通过多个早期退出点进行训练[[161](#bib.bib161)]。因此，终端和边缘可以自行执行部分深度学习推理，而不是直接请求云端。利用推理后的退出点，可以将终端设备对深度学习任务结果有信心的结果提供给终端，而无需将任何信息发送到云端。为了提供更准确的深度学习推理，中间的
    DNN 输出将发送到云端，通过使用额外的 DNN 层进行进一步推理。然而，中间输出，例如高分辨率监控视频流，应该设计得比原始输入小得多，从而大幅减少终端和边缘（或边缘与云端）之间所需的网络流量。'
- en: Though vertical collaboration can be considered as an evolution of cloud computing,
    i.e., “end-cloud” strategy. Compared to the pure “end-edge” strategy, the process
    of vertical collaboration may possibly be delayed, due to it requires additional
    communication with the cloud. However, vertical collaboration has its own advantages.
    One side, when edge architectures cannot afford the flood of DL queries by themselves,
    the cloud architectures can share partial computation tasks and hence ensure servicing
    these queries. On the other hand, the raw data must be preprocessed at the edge
    before they are transmitted to the cloud. If these operations can largely reduce
    the size of intermediate data and hence reduce the network traffic, the pressure
    of backbone networks can be alleviated.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管垂直协作可以被视为云计算的一种演变，即“终端-云端”策略。与纯粹的“终端-边缘”策略相比，垂直协作的过程可能会有所延迟，因为它需要与云端进行额外的通信。然而，垂直协作有其自身的优势。一方面，当边缘架构无法自行处理大量的深度学习查询时，云端架构可以分担部分计算任务，从而确保服务这些查询。另一方面，原始数据必须在传输到云端之前在边缘进行预处理。如果这些操作能够大幅减少中间数据的大小，从而减少网络流量，那么可以缓解骨干网络的压力。
- en: VI-B4 Horizontal Collaboration
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-B4 水平协作
- en: 'In Section [VI-B3](#S6.SS2.SSS3 "VI-B3 Vertical Collaboration ‣ VI-B Communication
    and Computation Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey"), vertical collaboration
    is discussed. However, devices among the edge or the end can also be united without
    the cloud to process resource-hungry DL applications, i.e., horizontal collaboration.
    By this means, the trained DNN models or the whole DL task can be partitioned
    and allocated to multiple end devices or edge nodes to accelerate DL computation
    by alleviating the resource cost of each of them. MoDNN, proposed in [[177](#bib.bib177)],
    executes DL in a local distributed mobile computing system over a Wireless Local
    Area Network (WLAN). Each layer of DNNs is partitioned into slices to increase
    parallelism and to reduce memory footprint, and these slices are executed layer-by-layer.
    By the execution parallelism among multiple end devices, the DL computation can
    be significantly accelerated.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在[VI-B3](#S6.SS2.SSS3 "VI-B3 纵向协作 ‣ VI-B 边缘 DL 的通信与计算模式 ‣ VI 深度学习的边缘计算 ‣ 边缘计算与深度学习的融合：全面调查")部分中讨论了纵向协作。然而，边缘或终端设备之间也可以在没有云的情况下联合起来处理资源密集型的
    DL 应用，即横向协作。通过这种方式，训练后的 DNN 模型或整个 DL 任务可以被划分并分配给多个终端设备或边缘节点，以通过缓解每个设备的资源成本来加速
    DL 计算。[[177](#bib.bib177)]中提出的 MoDNN 在无线局域网（WLAN）上的本地分布式移动计算系统中执行 DL。每个 DNN 层被划分为切片以增加并行性并减少内存占用，这些切片按层执行。通过多个终端设备之间的执行并行性，DL
    计算可以显著加速。
- en: With regard to specific DNN structures, e.g., CNN, a finer grid partitioning
    can be applied to minimize communication, synchronization, and memory overhead
    [[130](#bib.bib130)]. In [[156](#bib.bib156)], a Fused Tile Partitioning (FTP)
    method, able to divide each CNN layer into independently distributable tasks,
    is proposed. In contrast to only partitioning the DNN by layers as in [[12](#bib.bib12)],
    FTP can fuse layers and partitions them vertically in a grid fashion, hence minimizing
    the required memory footprint of participated edge devices regardless of the number
    of partitions and devices, while reducing communication and task migration cost
    as well. Besides, to support FTP, a distributed work-stealing runtime system,
    viz., idle edge devices stealing tasks from other devices with active work items
    [[156](#bib.bib156)], can adaptively distribute FTP partitions for balancing the
    workload of collaborated edge devices.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 关于具体的 DNN 结构，例如 CNN，可以应用更精细的网格划分来最小化通信、同步和内存开销[[130](#bib.bib130)]。在[[156](#bib.bib156)]中，提出了一种融合瓦片划分（FTP）方法，该方法能够将每个
    CNN 层划分为独立可分配的任务。与仅按层划分 DNN 的[[12](#bib.bib12)]方法相比，FTP 可以融合层并以网格方式垂直划分它们，从而最小化参与边缘设备所需的内存占用，无论分区和设备的数量如何，同时减少通信和任务迁移成本。此外，为支持
    FTP，分布式工作窃取运行时系统（即闲置的边缘设备从其他具有活动工作项的设备上窃取任务[[156](#bib.bib156)]），可以自适应地分配 FTP
    分区，以平衡协作边缘设备的工作负载。
- en: VI-C Tailoring Edge Frameworks for DL
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 为 DL 定制边缘框架
- en: Though there are gaps between the computational complexity and energy efficiency
    required by DL and the capacity of edge hardware [[179](#bib.bib179)], customized
    edge DL frameworks can help efficiently 1) match edge platform and DL models;
    2) exploit underlying hardware in terms of performance and power; 3) orchestrate
    and maintain DL services automatically.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DL 所需的计算复杂度和能源效率与边缘硬件的能力之间存在差距[[179](#bib.bib179)]，但定制的边缘 DL 框架可以有效地 1) 匹配边缘平台和
    DL 模型；2) 发挥底层硬件的性能和功率；3) 自动协调和维护 DL 服务。
- en: First, where to deploy DL services in edge computing (cellular) networks should
    be determined. The RAN controllers deployed at edge nodes are introduced in [[180](#bib.bib180)]
    to collect the data and run DL services, while the network controller, placed
    in the cloud, orchestrates the operations of the RAN controllers. In this manner,
    after running and feeding analytics and extract relevant metrics to DL models,
    these controllers can provide DL services to the users at the network edge.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应确定在边缘计算（蜂窝）网络中部署 DL 服务的位置。[[180](#bib.bib180)]中介绍了部署在边缘节点的 RAN 控制器，用于收集数据和运行
    DL 服务，而网络控制器则放置在云端，负责协调 RAN 控制器的操作。通过这种方式，在运行和提供分析并提取相关指标到 DL 模型之后，这些控制器可以向网络边缘的用户提供
    DL 服务。
- en: 'Second, as the deployment environment and requirements of DL models can be
    substantially different from those during model development, customized operators,
    adopted in developing DL models with (Py)Torch, TensorFlow, etc., may not be directly
    executed with the DL framework at the edge. To bridge the gap between deployment
    and development, the authors of [[181](#bib.bib181)] propose to specify DL models
    in development using the deployment tool with an operator library from the DL
    framework deployed at the edge. Furthermore, to automate the selection and optimization
    of DL models, ALOHA [[182](#bib.bib182)] formulates a toolflow: 1) Automate the
    model design. It generates the optimal model configuration by taking into account
    the target task, the set of constraints and the target architecture; 2) Optimize
    the model configuration. It partitions the DL model and accordingly generates
    architecture-aware mapping information between different inference tasks and the
    available resources. 3) Automate the model porting. It translates the mapping
    information into adequate calls to computing and communication primitives exposed
    by the target architecture.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于深度学习（DL）模型的部署环境和要求可能与模型开发期间有很大不同，因此在使用（Py）Torch、TensorFlow 等开发 DL 模型时采用的自定义操作符可能无法直接在边缘的
    DL 框架中执行。为了弥补部署与开发之间的差距，[[181](#bib.bib181)] 的作者建议在开发中使用边缘部署的 DL 框架的操作符库来指定 DL
    模型。此外，为了自动化 DL 模型的选择和优化，ALOHA [[182](#bib.bib182)] 制定了一种工具流程：1）自动化模型设计。它通过考虑目标任务、约束集和目标架构生成最佳模型配置；2）优化模型配置。它将
    DL 模型进行分区，并生成不同推理任务与可用资源之间的架构感知映射信息；3）自动化模型迁移。它将映射信息转换为适当的计算和通信原语调用，以适配目标架构。
- en: Third, the orchestration of DL models deployed at the edge should be addressed.
    OpenEI [[183](#bib.bib183)] defines each DL algorithm as a four-element tuple
    ¡Accuracy, Latency, Energy, Memory Footprint¿ to evaluate the Edge DL capability
    of the target hardware platform. Based on such tuple, OpenEI can select a matched
    model for a specific edge platform based on different Edge DL capabilities in
    an online manner. Zoo [[184](#bib.bib184)] provides a concise Domain-specific
    Language (DSL) to enable easy and type-safe composition of DL services. Besides,
    to enable a wide range of geographically distributed topologies, analytic engines,
    and DL services, ECO [[185](#bib.bib185)] uses a graph-based overlay network approach
    to 1) model and track pipelines and dependencies and then 2) map them to geographically
    distributed analytic engines ranging from small edge-based engines to powerful
    multi-node cloud-based engines. By this means, DL computation can be distributed
    as needed to manage cost and performance, while also supporting other practical
    situations, such as engine heterogeneity and discontinuous operations.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，应解决边缘部署的 DL 模型的协调问题。OpenEI [[183](#bib.bib183)] 将每个 DL 算法定义为一个四元素元组¡准确性、延迟、能耗、内存占用¿，以评估目标硬件平台的边缘
    DL 能力。基于此元组，OpenEI 可以以在线方式为特定边缘平台选择匹配的模型，依据不同的边缘 DL 能力。Zoo [[184](#bib.bib184)]
    提供了一种简洁的领域特定语言（DSL），以实现 DL 服务的轻松和类型安全组合。此外，为了支持各种地理分布的拓扑结构、分析引擎和 DL 服务，ECO [[185](#bib.bib185)]
    采用基于图的覆盖网络方法：1）建模和跟踪管道和依赖关系，然后 2）将其映射到从小型边缘引擎到强大的多节点云引擎等地理分布的分析引擎。通过这种方式，DL 计算可以根据需要分布，以管理成本和性能，同时支持其他实际情况，如引擎异质性和不连续操作。
- en: 'Nevertheless, these pioneer works are not ready to natively support valuable
    and also challenging features discussed in Section [VI-B](#S6.SS2 "VI-B Communication
    and Computation Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey"), such as computation
    offloading and collaboration, which still calls for further development.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管如此，这些开创性工作尚未准备好原生支持第 [VI-B](#S6.SS2 "VI-B Communication and Computation Modes
    for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey") 节中讨论的有价值且具有挑战性的特性，如计算卸载和协作，这仍需要进一步发展。'
- en: VI-D Performance Evaluation for Edge DL
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 边缘深度学习的性能评估
- en: Throughout the process of selecting appropriate edge hardware and associated
    software stacks for deploying different kinds of Edge DL services, it is necessary
    to evaluate their performance. Impartial evaluation methodologies can point out
    possible directions to optimize software stacks for specific edge hardware. In
    [[186](#bib.bib186)], for the first time, the performance of DL libraries is evaluated
    by executing DL inference on resource-constrained edge devices, pertaining to
    metrics like latency, memory footprint, and energy. In addition, particularly
    for Android smartphones, as one kind of edge devices with mobile CPUs or GPUs,
    AI Benchmark [[54](#bib.bib54)] extensively evaluates DL computation capabilities
    over various device configurations. Experimental results show that no single DL
    library or hardware platform can entirely outperform others, and loading the DL
    model may take more time than that of executing it. These discoveries imply that
    there are still opportunities to further optimize the fusion of edge hardware,
    edge software stacks, and DL libraries.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择适当的边缘硬件和相关软件栈以部署不同类型的Edge DL服务的过程中，需要评估它们的性能。公正的评估方法可以指出优化特定边缘硬件的软件栈的可能方向。在
    [[186](#bib.bib186)] 中，首次通过在资源受限的边缘设备上执行DL推理来评估DL库的性能，涉及延迟、内存占用和能耗等指标。此外，特别是针对Android智能手机作为一种配备移动CPU或GPU的边缘设备，AI
    Benchmark [[54](#bib.bib54)] 广泛评估了各种设备配置下的DL计算能力。实验结果表明，没有单一的DL库或硬件平台能够完全超越其他平台，加载DL模型的时间可能比执行它的时间更长。这些发现暗示仍有进一步优化边缘硬件、边缘软件栈和DL库融合的机会。
- en: Nonetheless, a standard testbed for Edge DL is missing, which hinders the study
    of edge architectures for DL. To evaluate the end-to-end performance of Edge DL
    services, not only the edge computing architecture but also its combination with
    end devices and the cloud shall be established, such as openLEON [[187](#bib.bib187)]
    and CAVBench [[188](#bib.bib188)] particularly for vehicular scenarios. Furthermore,
    simulations of the control panel of managing DL services are still not dabbled.
    An integrated testbed, consisting of wireless links and networking models, service
    requesting simulation, edge computing platforms, cloud architectures, etc., is
    ponderable in facilitating the evolution of “Edge Computing for DL”.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，目前缺乏针对Edge DL的标准测试平台，这限制了边缘架构在DL中的研究。为了评估Edge DL服务的端到端性能，不仅需要建立边缘计算架构，还需要与终端设备和云端的结合，如openLEON
    [[187](#bib.bib187)]和CAVBench [[188](#bib.bib188)]，特别是在车辆场景中。此外，管理DL服务的控制面板模拟仍然未被涉及。一个集成的测试平台，包括无线链接和网络模型、服务请求模拟、边缘计算平台、云架构等，有助于推动“Edge
    Computing for DL”的发展。
- en: VII Deep Learning Training at Edge
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 边缘深度学习训练
- en: Present DL training (distributed or not) in the cloud data center, namely cloud
    training, or cloud-edge training [[50](#bib.bib50)], viz., training data are preprocessed
    at the edge and then transmitted to cloud, are not appropriate for all kind of
    DL services, especially for DL models requiring locality and persistent training.
    Besides, a significant amount of communication resources will be consumed, and
    hence aggravating wireless and backbone networks if massive data are required
    to be continually transmitted from distributed end devices or edge nodes to the
    cloud. For example, with respect to surveillance applications integrated with
    object detection and target tracking, if end devices directly send a huge amount
    of real-time monitoring data to the cloud for persistent training, it will bring
    about high networking costs. In addition, merging all data into the cloud might
    violate privacy issues. All these challenges put forward the need for a novel
    training scheme against existing cloud training.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的DL训练（无论是分布式还是非分布式）在云数据中心，即云训练，或云-边缘训练 [[50](#bib.bib50)]，即在边缘预处理训练数据然后传输到云端，并不适用于所有类型的DL服务，特别是对需要局部性和持续训练的DL模型。此外，大量的通信资源将被消耗，从而加重无线和骨干网络的负担，尤其是当需要从分布式终端设备或边缘节点不断地将海量数据传输到云端时。例如，对于集成了对象检测和目标跟踪的监控应用，如果终端设备直接将大量实时监控数据发送到云端进行持续训练，将带来高昂的网络成本。此外，将所有数据合并到云端可能会违反隐私问题。所有这些挑战提出了针对现有云训练的创新训练方案的需求。
- en: 'Naturally, the edge architecture, which consists of a large number of edge
    nodes with modest computing resources, can cater for alleviating the pressure
    of networks by processing the data or training at themselves. Training at the
    edge or potentially among “end-edge-cloud”, treating the edge as the core architecture
    of training, is called “DL Training at Edge”. Such kind of DL training may require
    significant resources to digest distributed data and exchange updates. Nonetheless,
    FL is emerging and is promised to address these issues. We summarize select works
    on FL in Table [VI](#S7.T6 "TABLE VI ‣ VII-C Communication-efficient FL ‣ VII
    Deep Learning Training at Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey").'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '自然地，由大量边缘节点组成的边缘架构，其计算资源适中，可以通过在自身处理数据或训练来缓解网络压力。边缘训练或潜在的“终端-边缘-云”训练，将边缘视为训练的核心架构，称为“边缘的深度学习训练”。这种深度学习训练可能需要大量资源来消化分布式数据并交换更新。然而，联邦学习（FL）正在兴起，并有望解决这些问题。我们在表格[VI](#S7.T6
    "TABLE VI ‣ VII-C Communication-efficient FL ‣ VII Deep Learning Training at Edge
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")中总结了关于FL的相关研究。'
- en: VII-A Distributed Training at Edge
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 边缘的分布式训练
- en: 'Distributed training at the edge can be traced back to the work of [[189](#bib.bib189)],
    where a decentralized Stochastic Gradient Descent (SGD) method is proposed for
    the edge computing network to solve a large linear regression problem. However,
    this proposed method is designed for seismic imaging application and can not be
    generalized for future DL training, since the communication cost for training
    large scale DL models is extremely high. In [[190](#bib.bib190)], two different
    distributed learning solutions for edge computing environments are proposed. As
    depicted in Fig. [17](#S7.F17 "Figure 17 ‣ VII-A Distributed Training at Edge
    ‣ VII Deep Learning Training at Edge ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey"), one solution is that each end device trains
    a model based on local data, and then these model updates are aggregated at edge
    nodes. Another one is edge nodes train their own local models, and their model
    updates are exchanged and refined for constructing a global model. Though large-scale
    distributed training at edge evades transmitting bulky raw dataset to the cloud,
    the communication cost for gradients exchanging between edge devices is inevitably
    introduced. Besides, in practical, edge devices may suffer from higher latency,
    lower transmission rate and intermittent connections, and therefore further hindering
    the gradients exchanging between DL models belong to different edge devices.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘的分布式训练可以追溯到[[189](#bib.bib189)]的工作，其中提出了一种用于边缘计算网络的去中心化随机梯度下降（SGD）方法，以解决大规模线性回归问题。然而，这种方法是为地震成像应用设计的，不能推广到未来的深度学习训练，因为大规模深度学习模型的训练通信成本极高。在[[190](#bib.bib190)]中，提出了两种不同的边缘计算环境分布式学习解决方案。如图[17](#S7.F17
    "Figure 17 ‣ VII-A Distributed Training at Edge ‣ VII Deep Learning Training at
    Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")所示，一种解决方案是每个终端设备基于本地数据训练模型，然后将这些模型更新聚合在边缘节点。另一种方案是边缘节点训练各自的本地模型，然后交换和精炼这些模型更新，以构建一个全球模型。尽管大规模分布式边缘训练可以避免将大数据集传输到云端，但边缘设备之间梯度交换的通信成本不可避免地引入。此外，在实际应用中，边缘设备可能会遭遇更高的延迟、更低的传输速率和间歇性的连接，从而进一步阻碍不同边缘设备间的梯度交换。'
- en: '![Refer to caption](img/d408fa2fdadb8ed26b70427e1a0b3893.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d408fa2fdadb8ed26b70427e1a0b3893.png)'
- en: 'Figure 17: Distributed DL training at edge environments.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：边缘环境中的分布式深度学习训练。
- en: Most of the gradient exchanges are redundant, and hence updated gradients can
    be compressed to cut down the communication cost while preserving the training
    accuracy (such as DGC in [[191](#bib.bib191)]). First, DGC stipulates that only
    important gradients are exchanged, i.e., only gradients larger than a heuristically
    given threshold are transmitted. In order to avoid the information losing, the
    rest of the gradients are accumulated locally until they exceed the threshold.
    To be noted, gradients whether being immediately transmitted or accumulated for
    later exchanging will be coded and compressed, and hence saving the communication
    cost. Second, considering the sparse update of gradients might harm the convergence
    of DL training, momentum correction and local gradient clipping are adopted to
    mitigate the potential risk. By momentum correction, the sparse updates can be
    approximately equivalent to the dense updates. Before adding the current gradient
    to previous accumulation on each edge device locally, gradient clipping is performed
    to avoid the exploding gradient problem possibly introduced by gradient accumulation.
    Certainly, since partial gradients are delayed for updating, it might slow down
    the convergence. Hence, finally, for preventing the stale momentum from jeopardizing
    the performance of training, the momentum for delayed gradients is stopped, and
    less aggressive learning rate and gradient sparsity are adopted at the start of
    training to reduce the number of extreme gradients being delayed.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数梯度交换是冗余的，因此更新的梯度可以被压缩，以降低通信成本，同时保持训练精度（例如[[191](#bib.bib191)]中的DGC）。首先，DGC规定仅交换重要的梯度，即仅传输大于启发式给定阈值的梯度。为了避免信息丢失，其余梯度会在本地累积，直到超过阈值。需要注意的是，无论是立即传输还是累积以便后续交换的梯度都将被编码和压缩，从而节省通信成本。其次，考虑到稀疏梯度更新可能会影响深度学习训练的收敛性，采用动量修正和本地梯度裁剪来减轻潜在风险。通过动量修正，稀疏更新可以近似等同于密集更新。在每个边缘设备本地将当前梯度添加到之前的累积之前，会进行梯度裁剪，以避免因梯度累积可能引发的梯度爆炸问题。当然，由于部分梯度的更新被延迟，这可能会减缓收敛速度。因此，最终，为了防止过时的动量损害训练性能，对于延迟的梯度会停止动量，并在训练开始时采用较不激进的学习率和梯度稀疏性，以减少被延迟的极端梯度数量。
- en: With the same purpose of reducing the communication cost of synchronizing gradients
    and parameters during distributed training, two mechanisms can be combined together
    [[192](#bib.bib192)]. The first is transmitting only important gradients by taking
    advantage of sparse training gradients [[193](#bib.bib193)]. Hidden weights are
    maintained to record times of a gradient coordinate participating in gradient
    synchronization, and gradient coordinates with large hidden weight value are deemed
    as important gradients and will be more likely be selected in the next round training.
    On the other hand, the training convergence will be greatly harmed if residual
    gradient coordinates (i.e., less important gradients) are directly ignored, hence,
    in each training round, small gradient values are accumulated. Then, in order
    to avoid that these outdated gradients only contribute little influence on the
    training, momentum correction, viz., setting a discount factor to correct residual
    gradient accumulation, is applied.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少分布式训练中同步梯度和参数的通信成本，可以将两种机制结合在一起[[192](#bib.bib192)]。第一种机制是通过利用稀疏训练梯度来仅传输重要的梯度[[193](#bib.bib193)]。隐藏权重用于记录梯度坐标参与梯度同步的次数，具有较大隐藏权重值的梯度坐标被认为是重要梯度，并且在下一轮训练中更有可能被选择。另一方面，如果直接忽略残余梯度坐标（即较不重要的梯度），训练收敛性会受到很大影响，因此在每轮训练中，较小的梯度值会被累积。然后，为了避免这些过时的梯度对训练的影响微乎其微，会应用动量修正，即设置折扣因子来修正残余梯度累积。
- en: Particularly, when training a large DL model, exchanging corresponded model
    updates may consume more resources. Using an online version of KD can reduce such
    kind of communication cost [[194](#bib.bib194)]. In other words, the model outputs
    rather the updated model parameters on each device are exchanged, making the training
    of large-sized local models possible. Besides communication cost, privacy issues
    should be concerned as well. For example, in [[195](#bib.bib195)], personal information
    can be purposely obtained from training data by making use of the privacy leaking
    of a trained classifier. The privacy protection of training dataset at the edge
    is investigated in [[196](#bib.bib196)]. Different from [[190](#bib.bib190), [191](#bib.bib191),
    [192](#bib.bib192)], in the scenario of [[196](#bib.bib196)], training data are
    trained at edge nodes as well as be uploaded to the cloud for further data analysis.
    Hence, Laplace noises [[197](#bib.bib197)] are added to these possibly exposed
    training data for enhancing the training data privacy assurance.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，当训练一个大型深度学习模型时，交换对应的模型更新可能消耗更多资源。使用在线版本的知识蒸馏可以减少这种通信成本[[194](#bib.bib194)]。换句话说，交换的是每个设备的模型输出而不是更新的模型参数，从而使大型本地模型的训练成为可能。除了通信成本，隐私问题也应引起关注。例如，在[[195](#bib.bib195)]中，可以通过利用训练分类器的隐私泄露从训练数据中故意获取个人信息。边缘处训练数据的隐私保护在[[196](#bib.bib196)]中进行了研究。不同于[[190](#bib.bib190),
    [191](#bib.bib191), [192](#bib.bib192)]，在[[196](#bib.bib196)]的场景中，训练数据在边缘节点进行训练并上传到云端进行进一步的数据分析。因此，向这些可能暴露的训练数据中添加了拉普拉斯噪声[[197](#bib.bib197)]以增强训练数据的隐私保障。
- en: VII-B Vanilla Federated Learning at Edge
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 边缘的基础联邦学习
- en: 'In Section [VII-A](#S7.SS1 "VII-A Distributed Training at Edge ‣ VII Deep Learning
    Training at Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"), the holistic network architecture is explicitly separated, specifically,
    training is limited at the end devices or the edge nodes independently instead
    of among both of them. Certainly, by this means, it is simple to orchestrate the
    training process since there is no need to deal with heterogeneous computing capabilities
    and networking environments between the end and the edge. Nonetheless, DL training
    should be ubiquitous as well as DL inference. Federated Learning (FL) [[198](#bib.bib198),
    [199](#bib.bib199)] is emerged as a practical DL training mechanism among the
    end, the edge and the cloud. Though in the framework of native FL, modern mobile
    devices are taken as the clients performing local training. Naturally, these devices
    can be extended more widely in edge computing [[200](#bib.bib200), [201](#bib.bib201)].
    End devices, edge nodes and servers in the cloud can be equivalently deemed as
    clients in FL. These clients are assumed capable of handling different levels
    of DL training tasks, and hence contribute their updates to the global DL model.
    In this section, fundamentals of FL are discussed.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第VII-A节](#S7.SS1 "VII-A 分布式训练在边缘 ‣ VII 深度学习训练在边缘 ‣ 边缘计算与深度学习的融合：综合调查")中，整体网络架构被明确地分开，具体而言，训练限制在终端设备或边缘节点上独立进行，而不是在两者之间进行。显然，通过这种方式，可以简单地协调训练过程，因为无需处理终端和边缘之间的异构计算能力和网络环境。尽管如此，深度学习训练也应当像深度学习推理一样普遍。联邦学习（FL）[[198](#bib.bib198),
    [199](#bib.bib199)]作为终端、边缘和云之间的一种实际深度学习训练机制应运而生。虽然在原生FL框架中，现代移动设备被视为执行本地训练的客户端。这些设备自然可以在边缘计算中更广泛地扩展[[200](#bib.bib200),
    [201](#bib.bib201)]。终端设备、边缘节点和云中的服务器可以等同于FL中的客户端。这些客户端被假定能够处理不同级别的深度学习训练任务，因此将其更新贡献给全球深度学习模型。本节讨论了FL的基础知识。
- en: 'Without requiring uploading data for central cloud training, FL [[198](#bib.bib198),
    [199](#bib.bib199)] can allow edge devices to train their local DL models with
    their own collected data and upload only the updated model instead. As depicted
    in Fig. [18](#S7.F18 "Figure 18 ‣ VII-B Vanilla Federated Learning at Edge ‣ VII
    Deep Learning Training at Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"), FL iteratively solicits a random set of edge devices
    to 1) download the global DL model from an aggregation server (use “server” in
    following), 2) train their local models on the downloaded global model with their
    own data, and 3) upload only the updated model to the server for model averaging.
    Privacy and security risks can be significantly reduced by restricting the training
    data to only the device side, and thus avoiding the privacy issues as in [[195](#bib.bib195)],
    incurred by uploading training data to the cloud. Besides, FL introduces FederatedAveraging
    to combine local SGD on each device with a server performing model averaging.
    Experimental results corroborate FederatedAveraging is robust to unbalanced and
    non-IID data and can facilitate the training process, viz., reducing the rounds
    of communication needed to train a DL model.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 无需上传数据进行中央云训练，FL [[198](#bib.bib198), [199](#bib.bib199)] 可以允许边缘设备使用其收集的数据训练本地DL模型，并仅上传更新后的模型。如图
    [18](#S7.F18 "图18 ‣ VII-B 基本联邦学习在边缘 ‣ VII 边缘的深度学习训练 ‣ 边缘计算与深度学习的融合：综合调查") 所示，FL迭代地请求一组随机的边缘设备
    1）从聚合服务器（以下简称“服务器”）下载全局DL模型，2）使用自己的数据在下载的全局模型上训练本地模型，并且 3）仅将更新后的模型上传到服务器进行模型平均。通过将训练数据限制在设备端，隐私和安全风险可以显著减少，从而避免了如[[195](#bib.bib195)]中上传训练数据到云端带来的隐私问题。此外，FL引入了FederatedAveraging，将每个设备上的本地SGD与执行模型平均的服务器结合起来。实验结果证实，FederatedAveraging对不平衡和非IID数据具有鲁棒性，并且可以促进训练过程，即减少训练DL模型所需的通信轮次。
- en: '![Refer to caption](img/52278b866b963331598f4bccf7eda2fa.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52278b866b963331598f4bccf7eda2fa.png)'
- en: 'Figure 18: Federated learning among hierarchical network architectures.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：层次网络架构中的联邦学习。
- en: 'To summarize, FL can deal with several key challenges in edge computing networks:
    1) Non-IID training data. Training data on each device is sensed and collected
    by itself. Hence, any individual training data of a device will not be able to
    represent the global one. In FL, this can be met by FederatedAveraging; 2) Limited
    communication. Devices might potentially off-line or located in a poor communication
    environment. Nevertheless, performing more training computation on resource-sufficient
    devices can cut down communication rounds needed for global model training. In
    addition, FL only selects a part of devices to upload their updates in one round,
    therefore successfully handling the circumstance where devices are unpredictably
    off-line; 3) Unbalanced contribution. It can be tackled by FederatedAveraging,
    specifically, some devices may have less free resources for FL, resulting in varying
    amounts of training data and training capability among devices; 4) Privacy and
    security. The data need to be uploaded in FL is only the updated DL model. Further,
    secure aggregation and differential privacy [[197](#bib.bib197)], which are useful
    in avoiding the disclosure of privacy-sensitive data contained in local updates,
    can be applied naturally.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，FL可以解决边缘计算网络中的几个关键挑战：1）非IID训练数据。每个设备上的训练数据是由设备自身感知和收集的。因此，任何设备的单独训练数据都无法代表全局数据。在FL中，这可以通过FederatedAveraging来解决；2）有限的通信。设备可能处于离线状态或位于通信环境较差的地方。然而，在资源充足的设备上进行更多的训练计算可以减少全局模型训练所需的通信轮次。此外，FL只选择部分设备在一个轮次中上传其更新，从而成功处理了设备不可预测地离线的情况；3）不平衡的贡献。可以通过FederatedAveraging来解决，具体而言，一些设备可能在FL中可用的资源较少，导致设备之间训练数据和训练能力的差异；4）隐私和安全。FL中需要上传的数据仅为更新后的DL模型。此外，可以自然地应用安全聚合和差分隐私[[197](#bib.bib197)]，以避免泄露包含在本地更新中的隐私敏感数据。
- en: VII-C Communication-efficient FL
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 通信高效的FL
- en: In FL, raw training data are not required to be uploaded, thus largely reducing
    the communication cost. However, FL still needs to transmit locally updated models
    to the central server. Supposing the DL model size is large enough, uploading
    updates, such as model weights, from edge devices to the central server may also
    consume nonnegligible communication resources. To meet this, we can let FL clients
    communicate with the central server periodically (rather continually) to seek
    consensus on the shared DL model [[202](#bib.bib202)]. In addition, structured
    update, sketched update can help enhance the communication efficiency when clients
    uploading updates to the server as well. Structured update means restricting the
    model updates to have a pre-specified structure, specifically, 1) low-rank matrix;
    or 2) sparse matrix [[203](#bib.bib203), [202](#bib.bib202)]. On the other hand,
    for sketched update, full model updates are maintained. But before uploading them
    for model aggregation, combined operations of subsampling, probabilistic quantization,
    and structured random rotations are performed to compress the full updates [[203](#bib.bib203)].
    FedPAQ [[204](#bib.bib204)] simultaneously incorporates these features and provides
    near-optimal theoretical guarantees for both strongly convex and non-convex loss
    functions, while empirically demonstrating the communication-computation tradeoff.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FL 中，不需要上传原始训练数据，从而大大减少了通信成本。然而，FL 仍需将本地更新的模型传输到中央服务器。假设 DL 模型的大小足够大，从边缘设备到中央服务器上传更新（如模型权重）也可能消耗不容忽视的通信资源。为此，我们可以让
    FL 客户端周期性（而非持续）地与中央服务器进行通信，以寻求对共享 DL 模型的共识[[202](#bib.bib202)]。此外，结构化更新和草图更新也可以帮助提高客户端上传更新到服务器时的通信效率。结构化更新是指限制模型更新具有预先指定的结构，具体来说，1）低秩矩阵；或
    2）稀疏矩阵[[203](#bib.bib203), [202](#bib.bib202)]。另一方面，对于草图更新，保持完整的模型更新。但在将其上传以进行模型聚合之前，会执行子采样、概率量化和结构化随机旋转的组合操作，以压缩完整的更新[[203](#bib.bib203)]。FedPAQ
    [[204](#bib.bib204)] 同时结合了这些特性，并为强凸和非凸损失函数提供了近似最优的理论保证，同时在实践中展示了通信-计算的权衡。
- en: Different from only investigating on reducing communication cost on the uplink,
    [[205](#bib.bib205)] takes both server-to-device (downlink) and device-to-server
    (uplink) communication into consideration. For the downlink, the weights of the
    global DL model are reshaped into a vector, and then subsampling and quantization
    are applied [[203](#bib.bib203)]. Naturally, such kind of model compression is
    lossy, and unlike on the uplink (multiple edge devices are uploading their models
    for averaging), the loss cannot be mitigated by averaging on the downlink. Kashin’s
    representation [[206](#bib.bib206)] can be utilized before subsampling as a basis
    transform to mitigate the error incurred by subsequent compression operations.
    Furthermore, for the uplink, each edge device is not required to train a model
    based on the whole global model locally, but only to train a smaller sub-model
    or pruned model [[207](#bib.bib207)] instead. Since sub-models and pruned models
    are more lightweight than the global model, the amount of data in updates uploading
    is reduced.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅调查减少上行链路通信成本不同，[[205](#bib.bib205)] 考虑了服务器到设备（下行链路）和设备到服务器（上行链路）的通信。对于下行链路，全局
    DL 模型的权重被重塑为向量，然后应用子采样和量化[[203](#bib.bib203)]。自然，这种模型压缩是有损的，与上行链路（多个边缘设备上传它们的模型以进行平均）不同，下行链路上的损失不能通过平均来减轻。Kashin
    的表示[[206](#bib.bib206)] 可以在子采样之前作为基础变换来减轻随后的压缩操作带来的误差。此外，对于上行链路，每个边缘设备不需要基于整个全局模型进行本地训练，而只需训练一个较小的子模型或剪枝模型[[207](#bib.bib207)]。由于子模型和剪枝模型比全局模型更轻量，因此上传的更新数据量减少了。
- en: 'Computation resources of edge devices are scarce compared to the cloud. Additional
    challenges should be considered to improve communication efficiencies: 1) Computation
    resources are heterogeneous and limited at edge devices; 2) Training data at edge
    devices may be distributed non-uniformly [[208](#bib.bib208), [209](#bib.bib209),
    [210](#bib.bib210)]. For more powerful edge devices, ADSP [[211](#bib.bib211)]
    lets them continue training while committing model aggregation at strategically
    decided intervals. For general cases, based on the deduced convergence bound for
    distributed learning with non-IID data distributions, the aggregation frequency
    under given resource budgets among all participating devices can be optimized
    with theoretical guarantees [[208](#bib.bib208)]. Astraea [[212](#bib.bib212)]
    reduces $92\%$ communication traffic by designing a mediator-based multi-client
    rescheduling strategy. On the one hand, Astraea leverages data augmentation [5]
    to alleviate the defect of non-uniformly distributed training data. On the other
    hand, Astraea designs a greedy strategy for mediator-based rescheduling, in order
    to assign clients to the mediators. Each mediator traverses the data distribution
    of all unassigned clients to select the appropriate participating clients, aiming
    to make the mediator’s data distribution closest to the uniform distribution,
    i.e., minimizing the KullbackLeibler divergence [[213](#bib.bib213)] between mediator’s
    data distribution and uniform distribution. When a mediator reaches the max assigned
    clients limitation, the central server will create a new mediator and repeat the
    process until all clients have been assigned with training tasks.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘设备的计算资源相较于云端资源稀缺。需要考虑额外的挑战以提高通信效率：1) 边缘设备上的计算资源是异质的且有限；2) 边缘设备上的训练数据可能分布不均匀
    [[208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210)]。对于更强大的边缘设备，ADSP [[211](#bib.bib211)]
    允许它们在战略性决定的时间间隔内继续训练并提交模型聚合。对于一般情况，根据对非IID数据分布的分布式学习收敛界限，所有参与设备在给定资源预算下的聚合频率可以在理论上进行优化
    [[208](#bib.bib208)]。Astraea [[212](#bib.bib212)] 通过设计基于中介的多客户端重调策略减少了 $92\%$
    的通信流量。一方面，Astraea 利用数据增强 [5] 来缓解训练数据不均匀分布的缺陷。另一方面，Astraea 设计了一种贪心策略进行基于中介的重调，以便将客户端分配给中介。每个中介遍历所有未分配客户端的数据分布，以选择适当的参与客户端，旨在使中介的数据分布最接近均匀分布，即最小化中介的数据分布与均匀分布之间的
    Kullback-Leibler 散度 [[213](#bib.bib213)]。当一个中介达到最大分配客户端限制时，中央服务器将创建一个新的中介，并重复该过程，直到所有客户端都分配了训练任务。
- en: Aiming to accelerate the global aggregation in FL, [[214](#bib.bib214)] takes
    advantage of over-the-air computation [[215](#bib.bib215), [216](#bib.bib216),
    [217](#bib.bib217)], of which the principle is to explore the superposition property
    of a wireless multiple-access channel to compute the desired function by the concurrent
    transmission of multiple edge devices. The interferences of wireless channels
    can be harnessed instead of merely overcoming them. During the transmission, concurrent
    analog signals from edge devices can be naturally weighed by channel coefficients.
    Then the server only needs to superpose these reshaped weights as the aggregation
    results, nonetheless, without other aggregation operations.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速FL中的全球聚合，[[214](#bib.bib214)] 利用空中计算 [[215](#bib.bib215), [216](#bib.bib216),
    [217](#bib.bib217)]，其原理是利用无线多址信道的叠加特性，通过多个边缘设备的并发传输来计算所需的函数。无线信道的干扰可以被利用而不仅仅是克服。在传输过程中，来自边缘设备的并发模拟信号可以自然地由信道系数加权。然后服务器只需将这些重新调整的权重叠加为聚合结果，无需其他聚合操作。
- en: 'TABLE VI: Summary of the Selected Works on FL'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 联邦学习相关工作的总结'
- en: '|  | Ref. | DL Model | Scale | Dependency | Main Idea | Key Metrics or Performance
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | 参考文献 | 深度学习模型 | 规模 | 依赖性 | 主要思想 | 关键指标或性能 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Vanilla FL | [[198](#bib.bib198)] | FCNN, CNN, LSTM | Up to $5\mathrm{e}{5}$
    clients | TensorFlow | Leave the training data distributed on the mobile devices,
    and learns a shared model by aggregating locally-training updates | Communication
    rounds reduction: 10-100$\times$ |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| Vanilla FL | [[198](#bib.bib198)] | FCNN, CNN, LSTM | 多达 $5\mathrm{e}{5}$
    客户端 | TensorFlow | 保持训练数据分布在移动设备上，通过聚合本地训练更新来学习共享模型 | 通信轮次减少：10-100$\times$ |'
- en: '| [[199](#bib.bib199)] | RNN | Up to $1.5\mathrm{e}{6}$ clients | TensorFlow
    | Pace steering for scalable FL | Scalability improvement: up to 1.5e6 clients
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| [[199](#bib.bib199)] | RNN | 最多 $1.5\mathrm{e}{6}$ 客户端 | TensorFlow | 可扩展
    FL 的步伐调整 | 可扩展性提升：最多 1.5e6 客户端 |'
- en: '| Communication-efficient FL | [[202](#bib.bib202)] | ResNet18 | 4 clients
    per cluster / 7 clusters | $\backslash$ | Gradient sparsification; Periodic averaging
    | Top 1 accuracy; Communication latency reduction |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 通信高效的 FL | [[202](#bib.bib202)] | ResNet18 | 每个集群 4 个客户端 / 7 个集群 | $\backslash$
    | 梯度稀疏化；周期性平均 | Top 1 准确率；通信延迟减少 |'
- en: '| [[203](#bib.bib203)] | CNN, LSTM | Up to $1\mathrm{e}{3}$ clients | $\backslash$
    | Sketched updates | Communication cost reduction: by two orders of magnitude
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| [[203](#bib.bib203)] | CNN, LSTM | 最多 $1\mathrm{e}{3}$ 客户端 | $\backslash$
    | 草图更新 | 通信成本减少：减少两个数量级 |'
- en: '| [[205](#bib.bib205)] | CNN | Up to 500 clients | TensorFlow | Lossy compression
    on the global model; Federated Dropout | Downlink reduction: 14$\times$; Uplink
    reduction: 28$\times$; Local computation reduction: 1.7$\times$ |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| [[205](#bib.bib205)] | CNN | 最多 500 个客户端 | TensorFlow | 对全局模型进行有损压缩；联邦 Dropout
    | 下行链路减少：14$\times$；上行链路减少：28$\times$；本地计算减少：1.7$\times$ |'
- en: '| [[211](#bib.bib211)] | CNN, RNN | Up to 37 clients | TensorFlow | Let faster
    clients continue with their mini-batch training to keep overall synchronization
    | Convergence acceleration: 62.4% |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| [[211](#bib.bib211)] | CNN, RNN | 最多 37 个客户端 | TensorFlow | 让较快的客户端继续其小批量训练，以保持整体同步
    | 收敛加速：62.4% |'
- en: '| [[208](#bib.bib208)] | CNN | 5-500 clients (simulation); 3 Raspberry Pi and
    2 laptops (testbed) | $\backslash$ | Design a control algorithm that determines
    the best trade-off between local update and global aggregation | Training accuracy
    under resource budget |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| [[208](#bib.bib208)] | CNN | 5-500 客户端（模拟）；3 个 Raspberry Pi 和 2 台笔记本电脑（测试平台）
    | $\backslash$ | 设计一个控制算法，确定局部更新和全局聚合之间的最佳折衷 | 在资源预算下的训练准确率 |'
- en: '| [[204](#bib.bib204)] | FCNN | 50 clients | $\backslash$ | Periodic averaging;
    Partial device participation; Quantized message-passing | Total training loss
    and time |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| [[204](#bib.bib204)] | FCNN | 50 个客户端 | $\backslash$ | 周期性平均；部分设备参与；量化消息传递
    | 总训练损失和时间 |'
- en: '| [[212](#bib.bib212)] | CNN | 500 clients | $\backslash$ | Global data distribution
    based data augmentation; Mediator based multi-client rescheduling | Top 1 accuracy
    imrpovement: 5.59%-5.89%; Communication traffic reduction: 92% |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| [[212](#bib.bib212)] | CNN | 500 个客户端 | $\backslash$ | 基于全局数据分布的数据增强；基于中介的多客户端重新调度
    | Top 1 准确率提升：5.59%-5.89%；通信流量减少：92% |'
- en: '| [[207](#bib.bib207)] | LeNet, CNN, VGG11 | 10 Raspberry Pi | Py(Torch) |
    Jointly trains and prunes the model in a federated manner | Communication and
    computation load reduction |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| [[207](#bib.bib207)] | LeNet, CNN, VGG11 | 10 个 Raspberry Pi | Py(Torch)
    | 以联邦方式共同训练和剪枝模型 | 通信和计算负载减少 |'
- en: '| Resource -optimized FL | [[218](#bib.bib218)] | AlexNet, LeNet | Multiple
    Nvidia Jetson Nano | $\backslash$ | Partially train the model by masking a particular
    number of resource-intensive neurons | Training acceleration: 2$\times$; Model
    accuracy improvement: 4% |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 资源优化的 FL | [[218](#bib.bib218)] | AlexNet, LeNet | 多个 Nvidia Jetson Nano
    | $\backslash$ | 通过屏蔽特定数量的资源密集型神经元来部分训练模型 | 训练加速：2$\times$；模型准确性提升：4% |'
- en: '| [[219](#bib.bib219)] | $\backslash$ | Up to 50 clients | TensorFlow | Jointly
    optimize FL parameters and resources of user equipments | Convergence rate; Test
    accuracy |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| [[219](#bib.bib219)] | $\backslash$ | 最多 50 个客户端 | TensorFlow | 共同优化 FL 参数和用户设备的资源
    | 收敛速度；测试准确率 |'
- en: '| [[220](#bib.bib220)] | $\backslash$ | 20 clients / 1 BS | $\backslash$ |
    Jointly optimize wireless resource allocation and client selection | Reduction
    of the FL loss function value: up to 16% |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| [[220](#bib.bib220)] | $\backslash$ | 20 客户端 / 1 基站 | $\backslash$ | 共同优化无线资源分配和客户端选择
    | FL 损失函数值减少：最多 16% |'
- en: '| [[221](#bib.bib221)] | LSTM | 23-1,101 clients | TensorFlow | Modify FL training
    objectives with $\alpha$-fairness | Fairness; Training accuracy |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| [[221](#bib.bib221)] | LSTM | 23-1,101 客户端 | TensorFlow | 用 $\alpha$-公平性修改
    FL 训练目标 | 公平性；训练准确率 |'
- en: '| Security -enhanced FL | [[201](#bib.bib201)] | CNN | 100 clients | MXNET
    | Use the trimmed mean as a robust aggregation | Top 1 accuracy against data poisoning
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 安全增强的 FL | [[201](#bib.bib201)] | CNN | 100 客户端 | MXNET | 使用修剪均值作为稳健的聚合方法
    | 针对数据污染的 Top 1 准确率 |'
- en: '| [[222](#bib.bib222)] | $\backslash$ | $2\mathrm{e}{10}$-$2\mathrm{e}{14}$
    clients | $\backslash$ | Use Secure Aggregation to protect the privacy of each
    client’s model gradient | Communication expansion: 1.73$\times$-1.98$\times$ |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| [[222](#bib.bib222)] | $\backslash$ | $2\mathrm{e}{10}$-$2\mathrm{e}{14}$
    客户端 | $\backslash$ | 使用安全聚合来保护每个客户端模型梯度的隐私 | 通信扩展：1.73$\times$-1.98$\times$ |'
- en: '| [[223](#bib.bib223)] | $\backslash$ | 10 clients | $\backslash$ | Leverage
    blockchain to exchange and verify model updates of local training | Learning completion
    latency |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| [[223](#bib.bib223)] | $\backslash$ | 10 clients | $\backslash$ | 利用区块链交换和验证本地训练的模型更新
    | 学习完成延迟 |'
- en: VII-D Resource-optimized FL
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 资源优化的 FL
- en: When FL deploys the same neural network model to heterogeneous edge devices,
    devices with weak computing power (stragglers) may greatly delay the global model
    aggregation. Although the training model can be optimized to accelerate the stragglers,
    due to the limited resources of heterogeneous equipment, the optimized model usually
    leads to diverged structures and severely defect the collaborative convergence.
    ELFISH [[218](#bib.bib218)] first analyzes the computation consumption of the
    model training in terms of the time cost, memory usage, and computation workload.
    Under the guidance of the model analysis, which neurons need to be masked in each
    layer to ensure that the computation consumption of model training meets specific
    resource constraints can be determined. Second, unlike generating a deterministically
    optimized model with diverged structures, different sets of neurons will be dynamically
    masked in each training period and recovered and updated during the subsequent
    aggregation period, thereby ensuring comprehensive model updates overtime. It
    is worth noting that although ELFISH improves the training speed by 2$\times$
    through resource optimization, the idea of ELFISH is to make all stragglers work
    synchronously, the synchronous aggregation of which may not able to handle extreme
    situations.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 当 FL 部署相同的神经网络模型到异质边缘设备时，计算能力较弱的设备（滞后者）可能会大大延迟全球模型的聚合。尽管可以通过优化训练模型来加速滞后者，但由于异质设备的资源有限，优化后的模型通常会导致结构分歧，并严重影响协作收敛。ELFISH
    [[218](#bib.bib218)] 首先分析了模型训练的计算消耗，包括时间成本、内存使用和计算工作量。在模型分析的指导下，可以确定每一层中需要屏蔽的神经元，以确保模型训练的计算消耗符合特定的资源约束。其次，与生成具有分歧结构的确定性优化模型不同，在每个训练期间将动态屏蔽不同的神经元，并在随后的聚合期间恢复和更新，从而确保模型的全面更新。值得注意的是，尽管
    ELFISH 通过资源优化将训练速度提高了 2$\times$，但 ELFISH 的理念是使所有滞后者同步工作，而同步聚合可能无法处理极端情况。
- en: When FL is deployed in a mobile edge computing scenario, the wall-clock time
    of FL will mainly depend on the number of clients and their computing capabilities.
    Specifically, the total wall-clock time of FL includes not only the computation
    time but also the communication time of all clients. On the one hand, the computation
    time of a client depends on the computing capability of the clients and local
    data sizes. On the other hand, the communication time correlates to clients’ channel
    gains, transmission power, and local data sizes. Therefore, to minimize the wall-clock
    training time of the FL, appropriate resource allocation for the FL needs to consider
    not only FL parameters, such as accuracy level for computation-communication trade-off,
    but also the resources allocation on the client side, such as power and CPU cycles.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 当 FL 在移动边缘计算场景中部署时，FL 的壁钟时间主要取决于客户端的数量及其计算能力。具体来说，FL 的总壁钟时间不仅包括计算时间，还包括所有客户端的通信时间。一方面，客户端的计算时间依赖于客户端的计算能力和本地数据大小。另一方面，通信时间与客户端的信道增益、传输功率和本地数据大小相关。因此，为了最小化
    FL 的壁钟训练时间，适当的 FL 资源分配需要考虑 FL 参数，如计算-通信权衡的准确度水平，同时也需要考虑客户端侧的资源分配，如功率和 CPU 周期。
- en: However, minimizing the energy consumption of the client and the FL wall-clock
    time are conflicting. For example, the client can save energy by always maintain
    its CPU at low frequency, but this will definitely increase training time. Therefore,
    in order to strike a balance between energy cost and training time, the authors
    of [[219](#bib.bib219)] first design a new FL algorithm FEDL for each client to
    solve its local problem approximately till a local accuracy level achieved. Then,
    by using Pareto efficiency model [[224](#bib.bib224)], they formulate a non-convex
    resource allocation problem for FEDL over wireless networks to capture the trade-off
    between the clients’ energy cost and the FL wall-clock time). Finally, by exploiting
    the special structure of that problem, they decompose it into three sub-problems,
    and accordingly derive closed-form solutions and characterize the impact of the
    Pareto-efficient controlling knob to the optimal.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最小化客户端的能耗和FL的实际时间是相互冲突的。例如，客户端可以通过始终保持其CPU在低频率下以节省能量，但这将肯定会增加训练时间。因此，为了在能耗和训练时间之间找到平衡，[[219](#bib.bib219)]的作者首先为每个客户端设计了一个新的FL算法FEDL，以大致解决其本地问题直到达到本地准确性水平。然后，使用帕累托效率模型[[224](#bib.bib224)]，他们将FEDL在无线网络中的非凸资源分配问题形式化，以捕捉客户端的能耗和FL实际时间之间的权衡。最后，通过利用该问题的特殊结构，他们将其分解为三个子问题，并相应地推导出封闭形式解，并表征帕累托效率控制因子对最优解的影响。
- en: 'Since the uplink bandwidth for transmitting model updates is limited, the BS
    must optimize its resource allocation while the user must optimize its transmit
    power allocation to reduce the packet error rates of each user, thereby improving
    FL performance. To this end, the authors of [[220](#bib.bib220)] formulate resource
    allocation and user selection of FL into a joint optimization problem, the goal
    of which is to minimize the value of the FL loss function while meeting the delay
    and energy consumption requirements. To solve this problem, they first derive
    a closed-form expression for the expected convergence rate of the FL in order
    to establish an explicit relationship between the packet error rates and the FL
    performance. Based on this relationship, the optimization problem can be reduced
    to a mixed-integer nonlinear programming problem, and then solved as follows:
    First, find the optimal transmit power under a given user selection and resource
    block allocation; Then, transform the original optimization problem into a binary
    matching problem; Finally, using Hungarian algorithm [[225](#bib.bib225)] to find
    the best user selection and resource block allocation strategy.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传输模型更新的上行带宽有限，基站（BS）必须优化其资源分配，而用户则必须优化其传输功率分配，以减少每个用户的数据包错误率，从而提高联邦学习（FL）的性能。为此，[[220](#bib.bib220)]的作者将FL的资源分配和用户选择形式化为一个联合优化问题，其目标是在满足延迟和能耗要求的同时，最小化FL损失函数的值。为了解决这个问题，他们首先推导出FL的期望收敛速率的封闭形式表达式，以建立数据包错误率与FL性能之间的明确关系。基于这种关系，优化问题可以简化为一个混合整数非线性规划问题，然后按以下步骤解决：首先，找到在给定用户选择和资源块分配下的最优传输功率；然后，将原始优化问题转换为一个二分匹配问题；最后，使用匈牙利算法[[225](#bib.bib225)]找到最佳的用户选择和资源块分配策略。
- en: The number of devices involved in FL is usually large, ranging from hundreds
    to millions. Simply minimizing the average loss in such a large network may be
    not suited for the required model performance on some devices. In fact, although
    the average accuracy under vanilla FL is high, the model accuracy required for
    individual devices may not be guaranteed. To this end, based on the utility function
    $\alpha$-fairness [[226](#bib.bib226)] used in fair resource allocation in wireless
    networks, the authors of [[221](#bib.bib221)] define a fair-oriented goal $q$-FFL
    for joint resource optimization. $q$-FFL minimizes an aggregate re-weighted loss
    parameterized by $q$, so that devices with higher loss are given higher relative
    weight, thus encouraging less variance (i.e., more fairness) in the accuracy distribution.
    Adaptively minimizing $q$-FFL avoids the burden of hand-crafting fairness constraints,
    and can adjust the goal according to the required fairness dynamically, achieving
    the effect of reducing the variance of accuracy distribution among participated
    devices.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 参与 FL 的设备数量通常很大，从几百到几百万不等。在如此大规模的网络中，仅仅最小化平均损失可能不适合某些设备所需的模型性能。实际上，尽管在普通 FL
    下的平均准确率很高，但对个别设备所需的模型准确率可能无法保证。为此，基于无线网络中公平资源分配所使用的效用函数 $\alpha$-公平性 [[226](#bib.bib226)]，[[221](#bib.bib221)]
    的作者定义了一种公平导向的目标 $q$-FFL 进行联合资源优化。$q$-FFL 通过 $q$ 参数化的加权损失来最小化，从而对损失较大的设备赋予更高的相对权重，鼓励准确率分布的方差更小（即更多的公平性）。自适应地最小化
    $q$-FFL 避免了手动设计公平性约束的负担，并且可以根据所需的公平性动态调整目标，实现了减少参与设备之间准确率分布方差的效果。
- en: VII-E Security-enhanced FL
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 安全增强的 FL
- en: In vanilla FL, local data samples are processed on each edge device. Such a
    manner can prevent the devices from revealing private data to the server. However,
    the server also should not trust edge devices completely, since devices with abnormal
    behavior can forge or poison their training data, which results in worthless model
    updates, and hence harming the global model. To make FL capable of tolerating
    a small number of devices training on the poisoned dataset, robust federated optimization
    [[201](#bib.bib201)] defines a trimmed mean operation. By filtering out not only
    the the values produced by poisoned devices but also the natural outliers in the
    normal devices, robust aggregation protecting the global model from data poisoning
    is achieved.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通 FL 中，本地数据样本在每个边缘设备上进行处理。这种方式可以防止设备将私密数据泄露给服务器。然而，服务器也不应该完全信任边缘设备，因为具有异常行为的设备可能伪造或污染其训练数据，从而导致无效的模型更新，进而损害全局模型。为了使
    FL 能够容忍少数设备在污染数据集上进行训练，鲁棒联邦优化 [[201](#bib.bib201)] 定义了一种修剪均值操作。通过过滤掉不仅是由污染设备产生的值，还包括正常设备中的自然异常值，实现了保护全局模型免受数据污染的鲁棒聚合。
- en: Other than intentional attacks, passive adverse effects on the security, brought
    by unpredictable network conditions and computation capabilities, should be concerned
    as well. FL must be robust to the unexpectedly drop out of edge devices, or else
    once a device loses its connection, the synchronization of FL in one round will
    be failed. To solve this issue, Secure Aggregation protocol is proposed in [[222](#bib.bib222)]
    to achieve the robustness of tolerating up to one-third devices failing to timely
    process the local training or upload the updates.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有意的攻击之外，网络条件和计算能力的不确定性带来的被动安全性不良影响也应该受到关注。FL（联邦学习）必须对边缘设备的意外掉线具有鲁棒性，否则一旦某个设备失去连接，FL的一轮同步将会失败。为了解决这个问题，[[222](#bib.bib222)]
    提出了安全聚合协议，以实现对最多三分之一设备无法及时处理本地训练或上传更新的容错性。
- en: In turn, malfunctions of the aggregation server in FL may result in inaccurate
    global model updates and thereby distorting all local model updates. Besides,
    edge devices (with a larger number of data samples) may be less willing to participate
    FL with others (with less contribution). Therefore, in [[223](#bib.bib223)], combining
    Blockchain and FL as BlockFL is proposed to realize 1) locally global model updating
    at each edge device rather a specific server, ensuring device malfunction cannot
    affect other local updates when updating the global model; 2) appropriate reward
    mechanism for stimulating edge devices to participate in FL.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，FL中的聚合服务器故障可能导致全球模型更新不准确，从而扭曲所有本地模型更新。此外，边缘设备（数据样本数量较多）可能不愿意与其他（贡献较少）的设备参与FL。因此，在[[223](#bib.bib223)]中，提出了将区块链与FL结合的BlockFL方案，以实现：1）在每个边缘设备上进行本地的全球模型更新，而不是特定服务器，从而确保设备故障不会影响其他本地更新；2）适当的奖励机制以激励边缘设备参与FL。
- en: VIII Deep Learning for Optimizing Edge
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 用于优化边缘的深度学习
- en: 'DNNs (general DL models) can extract latent data features, while DRL can learn
    to deal with decision-making problems by interacting with the environment. Computation
    and storage capabilities of edge nodes, along with the collaboration of the cloud,
    make it possible to use DL to optimize edge computing networks and systems. With
    regard to various edge management issues such as edge caching, offloading, communication,
    security protection, etc., 1) DNNs can process user information and data metrics
    in the network, as well as perceiving the wireless environment and the status
    of edge nodes, and based on these information 2) DRL can be applied to learn the
    long-term optimal resource management and task scheduling strategies, so as to
    achieve the intelligent management of the edge, viz., intelligent edge as shown
    in Table [VII](#S8.T7 "TABLE VII ‣ VIII-B DL for Optimizing Edge Task Offloading
    ‣ VIII Deep Learning for Optimizing Edge ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey").'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 'DNN（通用DL模型）可以提取潜在的数据特征，而DRL则可以通过与环境交互来处理决策问题。边缘节点的计算和存储能力，加上云的协作，使得利用DL优化边缘计算网络和系统成为可能。关于各种边缘管理问题，如边缘缓存、卸载、通信、安全保护等，1）DNN可以处理网络中的用户信息和数据指标，并感知无线环境和边缘节点的状态，基于这些信息，2）DRL可以用于学习长期的最佳资源管理和任务调度策略，从而实现边缘的智能管理，即智能边缘，如表[
    VII ](#S8.T7 "TABLE VII ‣ VIII-B DL for Optimizing Edge Task Offloading ‣ VIII
    Deep Learning for Optimizing Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")所示。'
- en: VIII-A DL for Adaptive Edge Caching
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 自适应边缘缓存中的深度学习
- en: From Content Delivery Network (CDN) [[227](#bib.bib227)] to caching contents
    in cellular networks, caching in the network have been investigated over the years
    to deal with soaring demand for multimedia services [[228](#bib.bib228)]. Aligned
    with the concept of pushing contents near to users, edge caching [[229](#bib.bib229)],
    is deemed as a promising solution for further reducing the redundant data transmission,
    easing the pressure of cloud data centers and improving the QoE.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 从内容分发网络（CDN）[[227](#bib.bib227)]到在蜂窝网络中缓存内容，网络中的缓存已经被研究多年，以应对对多媒体服务的急剧需求[[228](#bib.bib228)]。与将内容推送到用户附近的概念一致，边缘缓存[[229](#bib.bib229)]被视为进一步减少冗余数据传输、减轻云数据中心压力和提高用户体验的有前途的解决方案。
- en: 'Edge caching meets two challenges: 1) the content popularity distribution among
    the coverage of edge nodes is hard to estimate, since it may be different and
    change with spatio-temporal variation [[230](#bib.bib230)]; 2) in view of massive
    heterogeneous devices in edge computing environments, the hierarchical caching
    architecture and complex network characteristics further perplex the design of
    content caching strategy [[231](#bib.bib231)]. Specifically, the optimal edge
    caching strategy can only be deduced when the content popularity distribution
    is known. However, users’ predilection for contents is actually unknown since
    the mobility, personal preference and connectivity of them may vary all the time.
    In this section, DL for determining edge caching policies, as illustrated in Fig.
    [19](#S8.F19 "Figure 19 ‣ VIII-A DL for Adaptive Edge Caching ‣ VIII Deep Learning
    for Optimizing Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"), are discussed.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘缓存面临两个挑战：1) 边缘节点覆盖范围内的内容流行度分布难以估计，因为它可能不同且随时空变化而变化 [[230](#bib.bib230)];
    2) 由于边缘计算环境中的大量异构设备，分层缓存架构和复杂的网络特性进一步困扰了内容缓存策略的设计 [[231](#bib.bib231)]。具体来说，只有在知道内容流行度分布的情况下才能推导出最佳的边缘缓存策略。然而，用户对内容的偏好实际上是未知的，因为他们的移动性、个人偏好和连接性可能随时发生变化。在这一部分，讨论了用于确定边缘缓存策略的
    DL，如图 [19](#S8.F19 "Figure 19 ‣ VIII-A DL for Adaptive Edge Caching ‣ VIII Deep
    Learning for Optimizing Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey") 所示。'
- en: '![Refer to caption](img/aa0f251ebd7cd7aa7715b610a00e9ff6.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aa0f251ebd7cd7aa7715b610a00e9ff6.png)'
- en: 'Figure 19: DL and DRL for optimizing the edge caching policy.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19: 用于优化边缘缓存策略的 DL 和 DRL。'
- en: VIII-A1 Use Cases of DNNs
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A1 DNNs 的应用案例
- en: Traditional caching methods are generally with high computational complexity
    since they require a large number of online optimization iterations to determine
    1) the features of users and contents and 2) the strategy of content placement
    and delivery.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的缓存方法通常具有较高的计算复杂性，因为它们需要大量的在线优化迭代来确定 1) 用户和内容的特征以及 2) 内容放置和传递策略。
- en: For the first purpose, DL can be used to process raw data collected from the
    mobile devices of users and hence extract the features of the users and content
    as a feature-based content popularity matrix. By this means, the popular content
    at the core network is estimated by applying feature-based collaborative filtering
    to the popularity matrix [[232](#bib.bib232)].
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个目的，DL 可用于处理从用户移动设备收集的原始数据，从而提取用户和内容的特征，形成基于特征的内容流行度矩阵。通过这种方式，可以通过将基于特征的协同过滤应用于流行度矩阵来估计核心网络中的热门内容
    [[232](#bib.bib232)]。
- en: For the second purpose, when using DNNs to optimize the strategy of edge caching,
    online heavy computation iterations can be avoided by offline training. A DNN,
    which consists of an encoder for data regularization and a followed hidden layer,
    can be trained with solutions generated by optimal or heuristic algorithms and
    be deployed to determine the cache policy [[233](#bib.bib233)], hence avoiding
    online optimization iterations. Similarly, in [[234](#bib.bib234)], inspired by
    the fact that the output of optimization problem about partial cache refreshing
    has some patterns, an MLP is trained for accepting the current content popularity
    and the last content placement probability as input to generate the cache refresh
    policy.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个目的，当使用 DNNs 来优化边缘缓存策略时，可以通过离线训练来避免在线重计算迭代。一个包含数据正则化的编码器和后续隐藏层的 DNN，可以通过最优或启发式算法生成的解决方案进行训练，并被部署以确定缓存策略
    [[233](#bib.bib233)]，从而避免在线优化迭代。同样，在 [[234](#bib.bib234)] 中，受到关于部分缓存刷新优化问题输出具有某些模式的启发，训练了一个
    MLP 以接受当前内容流行度和上一个内容放置概率作为输入来生成缓存刷新策略。
- en: As illustrated in [[233](#bib.bib233)][[234](#bib.bib234)], the complexity of
    optimization algorithms can be transferred to the training of DNNs, and thus breaking
    the practical limitation of employing them. In this case, DL is used to learn
    input-solution relations, and DNN-based methods are only available when optimization
    algorithms for the original caching problem exist. Therefore, the performance
    of DNN-based methods bounds by fixed optimization algorithms and is not self-adapted.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [[233](#bib.bib233)][[234](#bib.bib234)] 所示，优化算法的复杂性可以转移到 DNN 的训练中，从而打破了使用它们的实际限制。在这种情况下，DL
    被用来学习输入与解决方案之间的关系，而基于 DNN 的方法仅在存在原始缓存问题的优化算法时才可用。因此，基于 DNN 的方法的性能受限于固定的优化算法，且不具备自适应性。
- en: In addition, DL can be utilized for customized edge caching. For example, to
    minimize content-downloading delay in the self-driving car, an MLP is deployed
    in the cloud to predict the popularity of contents to be requested, and then the
    outputs of MLP are delivered to the edge nodes (namely MEC servers at RSUs in
    [[235](#bib.bib235)]). According to these outputs, each edge node caches contents
    that are most likely to be requested. On self-driving cars, CNN is chosen to predict
    the age and gender of the owner. Once these features of owners are identified,
    $k$-means clustering [[236](#bib.bib236)] and binary classification algorithms
    are used to determine which contents, already cached in edge nodes, should be
    further downloaded and cached from edge nodes to the car. Moreover, concerning
    taking full advantage of users’ features, [[237](#bib.bib237)] points out that
    the user’s willing to access the content in different environments is varying.
    Inspired by this, RNN is used to predict the trajectories of users. And based
    on these predictions, all contents of users’ interests are then prefetched and
    cached in advance at the edge node of each predicted location.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DL 可以用于定制化边缘缓存。例如，为了最小化自动驾驶汽车中的内容下载延迟，一个 MLP 被部署在云端以预测请求内容的热门程度，然后 MLP 的输出被传送到边缘节点（即
    [[235](#bib.bib235)] 中的 RSUs 的 MEC 服务器）。根据这些输出，每个边缘节点缓存最有可能被请求的内容。在自动驾驶汽车上，CNN
    被选择来预测车主的年龄和性别。一旦这些车主特征被识别，$k$-均值聚类 [[236](#bib.bib236)] 和二分类算法被用来确定哪些内容（已经缓存于边缘节点中）应进一步下载并从边缘节点缓存到汽车中。此外，关于充分利用用户特征，[[237](#bib.bib237)]
    指出用户在不同环境中访问内容的意愿是不同的。受到此启发，RNN 被用于预测用户的轨迹。基于这些预测，用户兴趣的所有内容将被提前预取并缓存到每个预测位置的边缘节点。
- en: VIII-A2 Use Cases of DRL
  id: totrans-426
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A2 DRL 的应用案例
- en: 'The function of DNNs described in Section [VIII-A1](#S8.SS1.SSS1 "VIII-A1 Use
    Cases of DNNs ‣ VIII-A DL for Adaptive Edge Caching ‣ VIII Deep Learning for Optimizing
    Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")
    can be deemed as a part of the whole edge caching solution, i.e., the DNN itself
    does not deal with the whole optimization problem. Different from these DNNs-based
    edge caching, DRL can exploit the context of users and networks and take adaptive
    strategies for maximizing the long-term caching performance [[238](#bib.bib238)]
    as the main body of the optimization method. Traditional RL algorithms are limited
    by the requirement for handcrafting features and the flaw that hardly handling
    high-dimensional observation data and actions [[239](#bib.bib239)]. Compared to
    traditional RL irrelevant to DL, such as $Q$-learning [[240](#bib.bib240)] and
    Multi-Armed Bandit (MAB) learning [[230](#bib.bib230)], the advantage of DRL lies
    in that DNNs can learn key features from the raw observation data. The integrated
    DRL agent combining RL and DL can optimize its strategies with respect to cache
    management in edge computing networks directly from high-dimensional observation
    data.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [VIII-A1](#S8.SS1.SSS1 "VIII-A1 Use Cases of DNNs ‣ VIII-A DL for Adaptive
    Edge Caching ‣ VIII Deep Learning for Optimizing Edge ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey") 节中描述的 DNNs 的功能可以被视为整个边缘缓存解决方案的一部分，即
    DNN 本身并不处理整个优化问题。与这些基于 DNNs 的边缘缓存不同，DRL 可以利用用户和网络的上下文，并采取适应性策略来最大化长期缓存性能 [[238](#bib.bib238)]，作为优化方法的主要内容。传统的
    RL 算法受到手工特征设计的要求和难以处理高维观测数据及动作的缺陷 [[239](#bib.bib239)] 的限制。与传统的与 DL 无关的 RL 方法，如
    $Q$-学习 [[240](#bib.bib240)] 和 多臂老虎机 (MAB) 学习 [[230](#bib.bib230)] 相比，DRL 的优势在于
    DNNs 能够从原始观测数据中学习关键特征。结合 RL 和 DL 的集成 DRL 代理能够直接从高维观测数据中优化其关于边缘计算网络的缓存管理策略。'
- en: In [[241](#bib.bib241)], DDPG is used to train a DRL agent, in order to maximize
    the long-term cache hit rate, to make proper cache replacement decisions. This
    work considers a scenario with a single BS, in which the DRL agent decides whether
    to cache the requested contents or replace the cached contents. While training
    the DRL agent, the reward is devised as the cache hit rate. In addition, Wolpertinger
    architecture [[242](#bib.bib242)] is utilized to cope with the challenge of large
    action space. In detail, a primary action set is first set for the DRL agent and
    then using $k$NN to map the practical action inputs to one out of this set. In
    this manner, the action space is narrowed deliberately without missing the optimal
    caching policy. Compared DQL-based algorithms searching the whole action space,
    the trained DRL agent with DDPG and Wolpertinger architecture is able to achieve
    competitive cache hit rates while reducing the runtime.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[241](#bib.bib241)]中，DDPG被用于训练DRL代理，以最大化长期缓存命中率，从而做出合适的缓存替换决策。此工作考虑了一个单一BS的场景，在该场景中，DRL代理决定是否缓存请求的内容或替换已缓存的内容。在训练DRL代理时，奖励被设计为缓存命中率。此外，Wolpertinger架构[[242](#bib.bib242)]被用于应对大动作空间的挑战。具体而言，首先为DRL代理设置一个主要动作集合，然后使用$k$NN将实际动作输入映射到这个集合中的一个动作。通过这种方式，动作空间被有意缩小，而不会错过最佳缓存策略。与基于DQL的算法搜索整个动作空间相比，经过DDPG和Wolpertinger架构训练的DRL代理能够在减少运行时间的同时实现具有竞争力的缓存命中率。
- en: VIII-B DL for Optimizing Edge Task Offloading
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 用于优化边缘任务卸载的深度学习
- en: 'Edge computing allows edge devices offload part of their computing tasks to
    the edge node [[243](#bib.bib243)], under constraints of energy, delay, computing
    capability, etc. As shown in Fig. [20](#S8.F20 "Figure 20 ‣ VIII-B DL for Optimizing
    Edge Task Offloading ‣ VIII Deep Learning for Optimizing Edge ‣ Convergence of
    Edge Computing and Deep Learning: A Comprehensive Survey"), these constraints
    put forward challenges of identifying 1) which edge nodes should receive tasks,
    2) what ratio of tasks edge devices should offload and 3) how many resources should
    be allocated to these tasks. To solve this kind of task offloading problem is
    NP-hard [[244](#bib.bib244)], since at least combination optimization of communication
    and computing resources along with the contention of edge devices is required.
    Particularly, the optimization should concern both the time-varying wireless environments
    (such as the varying channel quality) and requests of task offloading, hence drawing
    the attention of using learning methods [[245](#bib.bib245), [246](#bib.bib246),
    [247](#bib.bib247), [248](#bib.bib248), [249](#bib.bib249), [250](#bib.bib250),
    [251](#bib.bib251), [252](#bib.bib252), [253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255)]. Among all these works related to learning-based optimization
    methods, DL-based approaches have advantages over others when multiple edge nodes
    and radio channels are available for computation offloading. At this background,
    large state and action spaces in the whole offloading problem make the conventional
    learning algorithms [[245](#bib.bib245)][[256](#bib.bib256)][[247](#bib.bib247)]
    infeasible actually.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算允许边缘设备将部分计算任务卸载到边缘节点[[243](#bib.bib243)]，受限于能量、延迟、计算能力等因素。如图[20](#S8.F20
    "图20 ‣ VIII-B 用于优化边缘任务卸载的深度学习 ‣ VIII 优化边缘深度学习 ‣ 边缘计算与深度学习的融合：综合调查")所示，这些约束提出了以下挑战：1)
    哪些边缘节点应该接收任务，2) 边缘设备应该卸载多少比例的任务，3) 应该为这些任务分配多少资源。解决这种任务卸载问题是NP困难的[[244](#bib.bib244)]，因为至少需要优化通信和计算资源的组合，同时还需考虑边缘设备的争用。特别是，优化应关注时间变化的无线环境（如变化的信道质量）和任务卸载请求，因此吸引了对学习方法的关注[[245](#bib.bib245),
    [246](#bib.bib246), [247](#bib.bib247), [248](#bib.bib248), [249](#bib.bib249),
    [250](#bib.bib250), [251](#bib.bib251), [252](#bib.bib252), [253](#bib.bib253),
    [254](#bib.bib254), [255](#bib.bib255)]。在所有这些与学习优化方法相关的工作中，基于DL的方法在多个边缘节点和无线电信道可用于计算卸载时具有优势。在这种背景下，整个卸载问题中的大状态和动作空间使得传统学习算法[[245](#bib.bib245)][[256](#bib.bib256)][[247](#bib.bib247)]实际上不可行。
- en: '![Refer to caption](img/340d8baa2df2d85e79f511239d0ef07f.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/340d8baa2df2d85e79f511239d0ef07f.png)'
- en: 'Figure 20: Computation offloading problem in edge computing.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：边缘计算中的计算卸载问题。
- en: 'TABLE VII: DL for Optimizing Edge Application Scenarios'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 表VII：用于优化边缘应用场景的深度学习
- en: '|  | Ref. | DL | Comm. Scale | Inputs - DNN (States - DRL) | Outputs - DNN
    (Action - DRL) | Loss func. - DL (Reward - DRL) | Performance |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | 参考文献 | 深度学习 | 通信规模 | 输入 - DNN（状态 - DRL） | 输出 - DNN（动作 - DRL） | 损失函数 -
    DL（奖励 - DRL） | 性能 |'
- en: '| DL for Adaptive Edge Caching |  [[232](#bib.bib232)]  |  SDAE  | 60 users
    / 6 SBSs | User features, content features | Feature-based content popularity
    matrix | Normalized differences between input features and the consequent reconstruction
    | QoE improvement: up to 30%; Backhaul offloading: 6.2% |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| DL用于自适应边缘缓存 |  [[232](#bib.bib232)]  |  SDAE  | 60个用户 / 6个小基站 | 用户特征，内容特征
    | 基于特征的内容流行度矩阵 | 输入特征与随后的重构之间的归一化差异 | 质量体验改善：高达30%；回程卸载：6.2% |'
- en: '|  [[233](#bib.bib233)]  |  FCNN  | 100-200 UEs per cell / 7 BSs | Channel
    conditions, file requests | Caching decisions | Normalized differences between
    prediction decisions and the optimum | Prediction accuracy: up to 92%; Energy
    saving: 8% gaps to the optimum |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  [[233](#bib.bib233)]  |  FCNN  | 每个小区100-200个用户设备 / 7个基站 | 信道条件，文件请求 | 缓存决策
    | 预测决策与最优值之间的归一化差异 | 预测准确率：高达92%；节能：与最优值相比节省8% |'
- en: '|  [[234](#bib.bib234)]  |  FCNN  | UEs with density 25-30 / Multi-tier BSs
    | Current content popularity, last content placement probability | Content placement
    probability | Statistical average of the error between the model outputs and the
    optimal CVX solution | Prediction accuracy: slight degeneration to the optimum
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  [[234](#bib.bib234)]  |  FCNN  | 用户设备密度为25-30 / 多层基站 | 当前内容的流行度，上次内容放置概率
    | 内容放置概率 | 模型输出与最优CVX解之间的统计平均误差 | 预测准确率：略微低于最优值 |'
- en: '|  [[235](#bib.bib235)]  |  &#124; FCNN &#124; &#124; CNN &#124;  | Cars /
    6 RSUs with MEC servers | Facial images - CNN; Content features - FCNN | Gender
    and age prediction - CNN; Content request probability - FCNN | N/A - CNN; Cross
    entropy error - FCNN | Caching accuracy: up to 98.04% |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  [[235](#bib.bib235)]  |  &#124; FCNN &#124; &#124; CNN &#124;  | 汽车 / 6个带MEC服务器的RSU
    | 面部图像 - CNN；内容特征 - FCNN | 性别和年龄预测 - CNN；内容请求概率 - FCNN | 不适用 - CNN；交叉熵误差 - FCNN
    | 缓存准确率：高达98.04% |'
- en: '|  [[237](#bib.bib237)]  |  RNN  | 20 UEs / 10 servers | User historical traces
    | User location prediction | Cross entropy error | Caching accuracy: up to 75%
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  [[237](#bib.bib237)]  |  RNN  | 20个用户设备 / 10个服务器 | 用户历史轨迹 | 用户位置预测 | 交叉熵误差
    | 缓存准确率：高达75% |'
- en: '|  [[241](#bib.bib241)]  |  DDPG  | Multiple UEs / Single BS | Features of
    cached contents, current requests | Content replacement | Cache hit rate | Cache
    hit rate: about 50% |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  [[241](#bib.bib241)]  |  DDPG  | 多个用户设备 / 单个基站 | 缓存内容特征，当前请求 | 内容替换 | 缓存命中率
    | 缓存命中率：约50% |'
- en: '| DL for Optimizing Edge Task Offloading |  [[252](#bib.bib252)]  |  FCNN  |
    20 miners / Single edge node | Bidder valuation profiles of miners | Assignment
    probabilities, conditional payments | Expected, negated revenue of the service
    provider | Revenue increment |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| DL用于优化边缘任务卸载 |  [[252](#bib.bib252)]  |  FCNN  | 20个矿工 / 单个边缘节点 | 矿工的投标估值配置文件
    | 分配概率，条件支付 | 服务提供者的预期收入，已抵消 | 收入增加 |'
- en: '|  [[257](#bib.bib257)]  |  &#124; Double- &#124; &#124; DQL &#124;  | Single
    UE | System utilization states, dynamic slack states | DVFS algorithm selection
    | Average energy consumption | Energy saving: 2%-4% |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  [[257](#bib.bib257)]  |  &#124; 双重- &#124; &#124; DQL &#124;  | 单个用户设备 |
    系统利用状态，动态空闲状态 | DVFS算法选择 | 平均能耗 | 节能：2%-4% |'
- en: '|  [[253](#bib.bib253)]  |  DQL  | Multiple UEs / Single eNodeB | Sum cost
    of the entire system, available capacity of the MEC server | Offloading decision,
    resource allocation | Negatively correlated to the sum cost | System cost reduction
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  [[253](#bib.bib253)]  |  DQL  | 多个用户设备 / 单个eNodeB | 整个系统的总成本，MEC服务器的可用容量
    | 任务卸载决策，资源分配 | 与总成本负相关 | 系统成本降低 |'
- en: '|  [[255](#bib.bib255)]  |  DDPG  | Multiple UEs / Single BS with an MEC server
    | Channel vectors, task queue length | Offloading decision, power allocation |
    Negative wighted sum of the power consumption and task queue length | Computation
    cost reduction |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  [[255](#bib.bib255)]  |  DDPG  | 多个用户设备 / 单个基站带MEC服务器 | 信道向量，任务队列长度 | 卸载决策，功率分配
    | 功耗和任务队列长度的加权负和 | 计算成本降低 |'
- en: '|  [[254](#bib.bib254)]  |  DQL  | Single UE / Multiple MEC servers | Previous
    radio bandwidth, predicted harvested energy, current battery level | MEC server
    selection, offloading rate | Composition of overall data sharing gains, task drop
    loss, energy consumption and delay | Energy saving; Delay improvement |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  [[254](#bib.bib254)]  |  DQL  | 单个用户设备 / 多个MEC服务器 | 先前的无线带宽，预测的回收能量，当前电池电量
    | MEC服务器选择，卸载率 | 总数据共享收益、任务丢失损失、能耗和延迟的组成 | 节能；延迟改善 |'
- en: '|  [[251](#bib.bib251)]  |  &#124; Double- &#124; &#124; DQL &#124;  | Single
    UE / 6 BSs with MEC servers | Channel gain states, UE-BS association state, energy
    queue length, task queue length | Offloading decision, energy units allocation
    | Composition of task execution delay, task drop times, task queuing delay, task
    failing penalty and service payment | Offloading performance improvement |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  [[251](#bib.bib251)]  |  &#124; 双重- &#124; &#124; DQL &#124;  | 单个用户设备 /
    6个基站配有边缘计算服务器 | 信道增益状态、用户设备-基站关联状态、能量队列长度、任务队列长度 | 卸载决策、能量单位分配 | 任务执行延迟、任务丢弃次数、任务排队延迟、任务失败惩罚和服务支付的组成
    | 卸载性能提升'
- en: '|  [[258](#bib.bib258)]  |  DROO  | Multiple UEs / Single MEC server | Channel
    gain states | Offloading action | Computation rate | Algorithn execution time:
    less than 0.1s in 30-UE network |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  [[258](#bib.bib258)]  |  DROO  | 多个用户设备 / 单个边缘计算服务器 | 信道增益状态 | 卸载动作 | 计算速率
    | 算法执行时间：在30个用户设备网络中少于0.1秒 |'
- en: '| DL for Edge Management and Maintenance | Communication |  [[259](#bib.bib259)]  |  &#124;
    RNN & &#124; &#124; LSTM &#124;  | 53 vehicles / 20 fog servers | Coordinates
    of vehicles and interacting fog nodes, time, service cost | Cost prediction |
    Mean absolute error | Prediction accuracy: 99.2% |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 边缘管理和维护的深度学习 | 通信 |  [[259](#bib.bib259)]  |  &#124; RNN & &#124; &#124;
    LSTM &#124;  | 53辆车 / 20个雾计算服务器 | 车辆和交互雾节点的坐标、时间、服务成本 | 成本预测 | 平均绝对误差 | 预测准确率：99.2%
    |'
- en: '|  [[260](#bib.bib260)]  |  DQL  | 4 UEs / Multiple RRHs | Current on-off states
    of processors, current communication modes of UEs, cache states | Processor state
    control, communication mode selection | Negative of system energy consumption
    | System power consumption |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  [[260](#bib.bib260)]  |  DQL  | 4个用户设备 / 多个RRH | 处理器的当前开关状态、用户设备的当前通信模式、缓存状态
    | 处理器状态控制、通信模式选择 | 系统能量消耗的负值 | 系统功耗 |'
- en: '| Security |  [[261](#bib.bib261)]  |  DQL  | Multiple UEs / Multiple edge
    nodes | Jamming power, channel bandwidth, battery levels, user density | Edge
    node and channel selection, offloading rate, transmit power | Composition of defense
    costs and secrecy capacity | Signal SINR increasement |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 |  [[261](#bib.bib261)]  |  DQL  | 多个用户设备 / 多个边缘节点 | 干扰功率、信道带宽、电池水平、用户密度
    | 边缘节点和信道选择、卸载速率、发射功率 | 防御成本和保密容量的组成 | 信号SINR增加 |'
- en: '| Joint Optimization |  &#124; [[110](#bib.bib110)] &#124;  |  &#124; Double-
    &#124; &#124; Dueling &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '| 联合优化 |  &#124; [[110](#bib.bib110)] &#124;  |  &#124; 双重- &#124; &#124; 对抗性
    &#124;'
- en: '&#124; DQL &#124;  | Multiple UEs / 5 BSs and 5 MEC servers | Status from each
    BS, MEC server and content cache | BS allocation, caching decision, offloading
    decision | Composition of received SNRs, computation capabilities and cache states
    | System utility increasement |'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQL &#124;  | 多个用户设备 / 5个基站和5个边缘计算服务器 | 每个基站、边缘计算服务器和内容缓存的状态 | 基站分配、缓存决策、卸载决策
    | 接收SNR、计算能力和缓存状态的组成 | 系统效用增加 |'
- en: '|  [[262](#bib.bib262)]  |  &#124; AC &#124; &#124; DRL &#124;  | 20 UEs per
    router / 3 fog nodes | States of requests, fog nodes, tasks, contents and SINR
    | Decisions about fog node, channel, resource allocation, offloading and caching
    | Composition of computation offloading delay and content delivery delay | Average
    service latency: 1.5-4.0s |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  [[262](#bib.bib262)]  |  &#124; AC &#124; &#124; DRL &#124;  | 每个路由器20个用户设备
    / 3个雾计算节点 | 请求、雾计算节点、任务、内容和SINR的状态 | 关于雾计算节点、信道、资源分配、卸载和缓存的决策 | 计算卸载延迟和内容传递延迟的组成
    | 平均服务延迟：1.5-4.0秒 |'
- en: '|  [[112](#bib.bib112)]  |  DQL  | 50 vehicles / 10 RSUs | States of RSUs,
    vehicles and caches, contact rate, contact times | RSU assignment, caching control
    and control | Composition of communication, storage and computation cost | Backhaul
    capacity mitigation; Resource saving |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|  [[112](#bib.bib112)]  |  DQL  | 50辆车 / 10个RSU | RSU、车辆和缓存的状态、接触率、接触次数 |
    RSU分配、缓存控制和管理 | 通信、存储和计算成本的组成 | 回程容量缓解；资源节省 |'
- en: VIII-B1 Use Cases of DNNs
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-B1 DNN的使用案例
- en: In [[249](#bib.bib249)], the computation offloading problem is formulated as
    a multi-label classification problem. By exhaustively searching the solution in
    an offline way, the obtained optimal solution can be used to train a DNN with
    the composite state of the edge computing network as the input, and the offloading
    decision as the output. By this means, optimal solutions may not require to be
    solved online avoiding belated offloading decision making, and the computation
    complexity can be transferred to DL training.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[249](#bib.bib249)]中，计算卸载问题被形式化为多标签分类问题。通过离线方式穷举搜索解决方案，获得的最优解可以用来训练一个DNN，以边缘计算网络的复合状态作为输入，卸载决策作为输出。通过这种方式，最优解可能不需要在线求解，从而避免了延迟的卸载决策，并且计算复杂性可以转移到深度学习训练中。
- en: Further, a particular offloading scenario with respect to Blockchain is investigated
    in [[252](#bib.bib252)]. The computing and energy resources consumption of mining
    tasks on edge devices may limit the practical application of Blockchain in the
    edge computing network. Naturally, these mining tasks can be offloaded from edge
    devices to edge nodes, but it may cause unfair edge resource allocation. Thus,
    all available resources are allocated in the form of auctions to maximize the
    revenue of the Edge Computing Service Provider (ECSP). Based on an analytical
    solution of the optimal auction, an MLP can be constructed [[252](#bib.bib252)]
    and trained with valuations of the miners (i.e., edge devices) for maximizing
    the expected revenue of ECSP.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步地，关于区块链的特定卸载场景在[[252](#bib.bib252)]中进行了研究。边缘设备上挖矿任务的计算和能源资源消耗可能限制了区块链在边缘计算网络中的实际应用。自然地，这些挖矿任务可以从边缘设备卸载到边缘节点，但这可能导致边缘资源分配不公平。因此，所有可用资源以拍卖的形式进行分配，以最大化边缘计算服务提供商（ECSP）的收益。基于最优拍卖的分析解，可以构建一个MLP
    [[252](#bib.bib252)]，并通过矿工（即边缘设备）的估值来训练，以最大化ECSP的期望收益。
- en: VIII-B2 Use Cases of DRL
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-B2 DRL的使用案例
- en: Though offloading computation tasks to edge nodes can enhance the processing
    efficiency of the computation tasks, the reliability of offloading suffers from
    the potentially low quality of wireless environments. In [[248](#bib.bib248)],
    to maximize offloading utilities, the authors first quantify the influence of
    various communication modes on the task offloading performance and accordingly
    propose applying DQL to online select the optimal target edge node and transmission
    mode. For optimizing the total offloading cost, a DRL agent that modifies Dueling-
    and Double-DQL [[263](#bib.bib263)] can allocate edge computation and bandwidth
    resources for end devices.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将计算任务卸载到边缘节点可以提高计算任务的处理效率，但卸载的可靠性受到无线环境可能低质量的影响。在[[248](#bib.bib248)]中，为了最大化卸载效用，作者首先量化了各种通信模式对任务卸载性能的影响，并因此建议应用DQL在线选择最佳目标边缘节点和传输模式。为了优化总卸载成本，可以使用修改了Dueling-和Double-DQL
    [[263](#bib.bib263)]的DRL代理来为终端设备分配边缘计算和带宽资源。
- en: Besides, offloading reliability should also be concerned. The coding rate, by
    which transmitting the data, is crucial to make the offloading meet the required
    reliability level. Hence, in [[250](#bib.bib250)], effects of the coding block-length
    are investigated and an MDP concerning resource allocation is formulated and then
    solved by DQL, in order to improve the average offloading reliability. Exploring
    further on scheduling fine-grained computing resources of the edge device, in
    [[257](#bib.bib257)], Double-DQL [[89](#bib.bib89)] is used to determine the best
    Dynamic Voltage and Frequency Scaling (DVFS) algorithm. Compared to DQL, the experiment
    results indicate that Double-DQL can save more energy and achieve higher training
    efficiency. Nonetheless, the action space of DQL-based approaches may increase
    rapidly with increasing edge devices. Under the circumstances, a pre-classification
    step can be performed before learning [[253](#bib.bib253)] to narrow the action
    space.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，卸载的可靠性也应受到关注。传输数据的编码率对确保卸载达到所需的可靠性水平至关重要。因此，在[[250](#bib.bib250)]中，研究了编码块长度的影响，并形式化了一个与资源分配有关的MDP，然后通过DQL解决，以提高平均卸载可靠性。在[[257](#bib.bib257)]中，进一步探讨了边缘设备的精细计算资源调度，使用Double-DQL
    [[89](#bib.bib89)]来确定最佳的动态电压和频率调整（DVFS）算法。与DQL相比，实验结果表明Double-DQL可以节省更多的能源并实现更高的训练效率。尽管如此，基于DQL的方法的动作空间可能会随着边缘设备的增加而迅速增加。在这种情况下，可以在学习之前进行预分类
    [[253](#bib.bib253)] 以缩小动作空间。
- en: IoT edge environments powered by Energy Harvesting (EH) is investigated in [[254](#bib.bib254),
    [251](#bib.bib251)]. In EH environments, the energy harvesting makes the offloading
    problem more complicated, since IoT edge devices can harvest energy from ambient
    radio-frequency signals. Hence, CNN is used to compress the state space in the
    learning process [[254](#bib.bib254)]. Further, in [[251](#bib.bib251)], inspired
    by the additive structure of the reward function, $Q$-function decomposition is
    applied in Double-DQL, and it improves the vanilla Double-DQL. However, value-based
    DRL can only deal with discrete action space. To perform more fine-grained power
    control for local execution and task offloading, policy-gradient-based DRL should
    be considered. For example, compared tot he discrete power control strategy based
    on DQL, DDPG can adaptively allocate the power of edge devices with finer granularity
    [[255](#bib.bib255)].
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 由能量采集 (EH) 驱动的物联网边缘环境在 [[254](#bib.bib254), [251](#bib.bib251)] 中进行了研究。在 EH
    环境中，能量采集使得卸载问题更加复杂，因为物联网边缘设备可以从环境无线电频率信号中采集能量。因此，CNN 被用于在学习过程中压缩状态空间 [[254](#bib.bib254)]。进一步地，在
    [[251](#bib.bib251)] 中，受到奖励函数加法结构的启发，$Q$-函数分解在 Double-DQL 中应用，并且改进了原始 Double-DQL。然而，基于值的
    DRL 只能处理离散动作空间。为了对本地执行和任务卸载进行更精细的功率控制，应考虑基于策略梯度的 DRL。例如，与基于 DQL 的离散功率控制策略相比，DDPG
    可以以更精细的粒度自适应地分配边缘设备的功率 [[255](#bib.bib255)]。
- en: Freely letting DRL agents take over the whole process of computation offloading
    may lead to huge computational complexity. Therefore, only employing DNN to make
    partial decisions can largely reduce the complexity. For instance, in [[258](#bib.bib258)],
    the problem of maximizing the weighted sum computation rate is decomposed into
    two sub-problems, viz., offloading decision and resource allocation. By only using
    DRL to deal with the NP-hard offloading decision problem rather than both, the
    action space of the DRL agent is narrowed, and the offloading performance is not
    impaired as well since the resource allocation problem is solved optimally.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 自由让 DRL 代理接管整个计算卸载过程可能导致巨大的计算复杂性。因此，只使用 DNN 做出部分决策可以大大降低复杂性。例如，在 [[258](#bib.bib258)]
    中，将最大化加权和计算率的问题分解为两个子问题，即卸载决策和资源分配。通过仅使用 DRL 处理 NP 难度的卸载决策问题，而非两者，DRL 代理的行动空间被缩小，同时由于资源分配问题得到最优解决，卸载性能也不会受到影响。
- en: VIII-C DL for Edge Management and Maintenance
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 边缘管理和维护的 DL
- en: Edge DL services are envisioned to be deployed on BSs in cellular networks,
    as implemented in [[264](#bib.bib264)]. Therefore, edge management and maintenance
    require optimizations from multiple perspectives (including communication perspective).
    Many works focus on applying DL in wireless communication [[265](#bib.bib265),
    [266](#bib.bib266), [267](#bib.bib267)]. Nevertheless, management and maintenance
    at the edge should consider more aspects.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘 DL 服务被设想在蜂窝网络的基站上部署，如在 [[264](#bib.bib264)] 中实现的那样。因此，边缘管理和维护需要从多个角度（包括通信角度）进行优化。许多工作集中在将
    DL 应用于无线通信 [[265](#bib.bib265), [266](#bib.bib266), [267](#bib.bib267)]。然而，边缘管理和维护应考虑更多方面。
- en: VIII-C1 Edge Communication
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-C1 边缘通信
- en: When edge nodes are serving mobile devices (users), mobility issues in edge
    computing networks should be addressed. DL-based methods can be used to assist
    the smooth transition of connections between devices and edge nodes. To minimize
    energy consumption per bit, in [[268](#bib.bib268)], the optimal device association
    strategy is approximated by a DNN. Meanwhile, a digital twin of network environments
    is established at the central server for training this DNN off-line. To minimize
    the interruptions of a mobile device moving from an edge node to the next one
    throughout its moving trajectory, the MLP can be used to predict available edge
    nodes at a given location and time [[259](#bib.bib259)]. Moreover, determining
    the best edge node, with which the mobile device should associate, still needs
    to evaluate the cost (the latency of servicing a request) for the interaction
    between the mobile device and each edge node. Nonetheless, modeling the cost of
    these interactions requires a more capable learning model. Therefore, a two-layer
    stacked RNN with LSTM cells is implemented for modeling the cost of interaction.
    At last, based on the capability of predicting available edge nodes along with
    corresponding potential cost, the mobile device can associate with the best edge
    node, and hence the possibility of disruption is minimized.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 当边缘节点为移动设备（用户）提供服务时，应解决边缘计算网络中的移动性问题。基于DL的方法可以用于辅助设备与边缘节点之间连接的平稳过渡。为了最小化每比特的能量消耗，在[[268](#bib.bib268)]中，使用DNN来近似最佳设备关联策略。同时，在中央服务器上建立网络环境的数字双胞胎，以离线训练该DNN。为了最小化移动设备在其移动轨迹中从一个边缘节点移动到下一个边缘节点的中断，可以使用MLP来预测在给定位置和时间的可用边缘节点[[259](#bib.bib259)]。此外，确定移动设备应关联的最佳边缘节点仍然需要评估移动设备与每个边缘节点交互的成本（服务请求的延迟）。然而，建模这些交互的成本需要一个更强大的学习模型。因此，实现了一个具有LSTM单元的两层堆叠RNN来建模交互成本。最后，基于预测可用边缘节点及其潜在成本的能力，移动设备可以与最佳边缘节点关联，从而最小化中断的可能性。
- en: Aiming at minimizing long-term system power consumption in the communication
    scenario with multiple modes (to serve various IoT services), i.e., Cloud-Radio
    Access Networks (C-RAN) mode, Device-to-Device (D2D) mode, and Fog radio Access
    Point (FAP) mode, DQL can be used to control communication modes of edge devices
    and on-off states of processors throughout the communicating process [[260](#bib.bib260)].
    After determining the communication mode and the processors’ on-off states of
    a given edge device, the whole problem can be degraded into an Remote Radio Head
    (RRH) transmission power minimization problem and solved. Further, TL is integrated
    with DQL to reduce the required interactions with the environment in the DQL training
    process while maintaining a similar performance without TL.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 针对在具有多种模式的通信场景（服务于各种物联网服务）中最小化长期系统功耗的问题，即云无线接入网络（C-RAN）模式、设备对设备（D2D）模式和雾无线接入点（FAP）模式，可以使用DQL来控制边缘设备的通信模式和处理器的开关状态。在确定了给定边缘设备的通信模式和处理器的开关状态之后，整个问题可以简化为一个远程无线头（RRH）传输功率最小化问题并进行求解。此外，TL与DQL集成，以减少DQL训练过程中与环境的交互，同时在没有TL的情况下保持相似的性能。
- en: VIII-C2 Edge Security
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-C2 边缘安全
- en: Since edge devices generally equipped with limited computation, energy and radio
    resources, the transmission between them and the edge node is more vulnerable
    to various attacks, such as jamming attacks and Distributed Denial of Service
    (DDoS) attacks, compared to cloud computing. Therefore, the security of the edge
    computing system should be enhanced. First, the system should be able to actively
    detect unknown attacks, for instance, using DL techniques to extract features
    of eavesdropping and jamming attacks [[269](#bib.bib269)]. According to the attack
    mode detected, the system determines the strategy of security protection. Certainly,
    security protection generally requires additional energy consumption and the overhead
    of both computation and communication. Consequently, each edge device shall optimize
    its defense strategies, viz., choosing the transmit power, channel and time, without
    violating its resource limitation. The optimization is challenging since it is
    hard to estimate the attack model and the dynamic model of edge computing networks.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 由于边缘设备通常配备有限的计算、能源和无线资源，它们与边缘节点之间的传输比云计算更容易受到各种攻击，例如干扰攻击和分布式拒绝服务（DDoS）攻击。因此，边缘计算系统的安全性应该得到增强。首先，系统应该能够主动检测未知攻击，例如，使用深度学习技术提取窃听和干扰攻击的特征[[269](#bib.bib269)]。根据检测到的攻击模式，系统确定安全保护策略。当然，安全保护通常需要额外的能源消耗以及计算和通信的开销。因此，每个边缘设备应优化其防御策略，即选择传输功率、信道和时间，同时不违反其资源限制。优化具有挑战性，因为很难估计攻击模型和边缘计算网络的动态模型。
- en: DRL-based security solutions can provide secure offloading (from the edge device
    to the edge node) to against jamming attacks [[261](#bib.bib261)] or protect user
    location privacy and the usage pattern privacy [[270](#bib.bib270)]. The edge
    device observes the status of edge nodes and the attack characteristics and then
    determines the defense level and key parameters in security protocols. By setting
    the reward as the anti-jamming communication efficiency, such as the signal-to-interference-plus-noise
    ratio of the signals, the bit error rate of the received messages, and the protection
    overhead, the DQL-based security agent can be trained to cope with various types
    of attacks.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 基于DRL的安全解决方案可以提供安全卸载（从边缘设备到边缘节点），以防止干扰攻击[[261](#bib.bib261)]，或保护用户位置隐私和使用模式隐私[[270](#bib.bib270)]。边缘设备观察边缘节点的状态和攻击特征，然后确定安全协议中的防御级别和关键参数。通过将奖励设置为抗干扰通信效率，如信号的信噪比、接收消息的比特错误率和保护开销，基于DQL的安全代理可以进行训练，以应对各种类型的攻击。
- en: VIII-C3 Joint Edge Optimization
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-C3 联合边缘优化
- en: Edge computing can cater for the rapid growth of smart devices and the advent
    of massive computation-intensive and data-consuming applications. Nonetheless,
    it also makes the operation of future networks even more complex [[271](#bib.bib271)].
    To manage the complex networks with respect to comprehensive resource optimization
    [[16](#bib.bib16)] is challenging, particularly under the premise of considering
    key enablers of the future network, including Software-Defined Network (SDN) [[272](#bib.bib272)],
    IoTs, Internet of Vehicles (IoVs).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算可以应对智能设备的快速增长以及大量计算密集型和数据消耗型应用的出现。然而，这也使得未来网络的操作变得更加复杂[[271](#bib.bib271)]。管理复杂网络以实现全面资源优化[[16](#bib.bib16)]具有挑战性，特别是在考虑未来网络的关键支持技术时，包括软件定义网络（SDN）[[272](#bib.bib272)]、物联网（IoT）和车联网（IoV）。
- en: In general, SDN is designed for separating the control plane from the data plane,
    and thus allowing the operation over the whole network with a global view. Compared
    to the distributed nature of edge computing networks, SDN is a centralized approach,
    and it is challenging to apply SDN to edge computing networks directly. In [[273](#bib.bib273)],
    an SDN-enabled edge computing network catering for smart cities is investigated.
    To improve the servicing performance of this prototype network, DQL is deployed
    in its control plane to orchestrate networking, caching, and computing resources.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，SDN的设计目的是将控制平面与数据平面分离，从而允许在整个网络上进行全局视图的操作。与边缘计算网络的分布式性质相比，SDN是一种集中式方法，将SDN直接应用于边缘计算网络具有挑战性。在[[273](#bib.bib273)]中，研究了适用于智能城市的SDN支持的边缘计算网络。为了提高该原型网络的服务性能，DQL被部署在其控制平面中，以协调网络、缓存和计算资源。
- en: Edge computing can empower IoT systems with more computation-intensive and delay-sensitive
    services but also raises challenges for efficient management and synergy of storage,
    computation, and communication resources. For minimizing the average end-to-end
    servicing delay, policy-gradient-based DRL combined with AC architecture can deal
    with the assignment of edge nodes, the decision about whether to store the requesting
    content or not, the choice of the edge node performing the computation tasks and
    the allocation of computation resources [[262](#bib.bib262)].
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算可以赋能物联网系统，提供更多计算密集型和延迟敏感的服务，但也带来了存储、计算和通信资源高效管理和协同的挑战。为了最小化平均端到端服务延迟，基于策略梯度的DRL结合AC架构可以处理边缘节点的分配、是否存储请求内容的决策、执行计算任务的边缘节点的选择以及计算资源的分配[[262](#bib.bib262)]。
- en: IoVs is a special case of IoTs and focuses on connected vehicles. Similar to
    the consideration of integrating networking, caching and computing as in [[262](#bib.bib262)],
    Double-Dueling DQL (i.e., combining Double DQL and Dueling DQL) with more robust
    performance, can be used to orchestrate available resources to improve the performance
    of future IoVs [[110](#bib.bib110)]. In addition, considering the mobility of
    vehicles in the IoVs, the hard service deadline constraint might be easily broken,
    and this challenge is often either neglected or tackled inadequately because of
    high complexities. To deal with the mobility challenge, in [[112](#bib.bib112)],
    the mobility of vehicles is first modeled as discrete random jumping, and the
    time dimension is split into epochs, each of which comprises several time slots.
    Then, a small timescale DQL model, regarding the granularity of time slot, is
    devised for incorporating the impact of vehicles’ mobility in terms of the carefully
    designed immediate reward function. At last, a large timescale DQL model is proposed
    for every time epoch. By using such multi-timescale DRL, issues about both immediate
    impacts of the mobility and the unbearable large action space in the resource
    allocation optimization are solved.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 车联网（IoVs）是物联网（IoTs）的一个特殊案例，专注于联网车辆。类似于[[262](#bib.bib262)]中考虑的网络、缓存和计算的整合，性能更强的双重对抗DQL（即将双重DQL和对抗DQL结合）可以用于协调可用资源以提高未来车联网的性能[[110](#bib.bib110)]。此外，考虑到车联网中车辆的移动性，硬性服务截止时间约束可能容易被打破，这一挑战往往因为复杂性高而被忽视或处理不当。为了应对移动性挑战，在[[112](#bib.bib112)]中，车辆的移动性首先被建模为离散随机跳跃，并将时间维度划分为多个时期，每个时期包含若干时间槽。然后，设计了一个小时间尺度的DQL模型，考虑到时间槽的粒度，将车辆移动的影响纳入精心设计的即时奖励函数中。最后，为每个时间时期提出了一个大时间尺度的DQL模型。通过使用这种多时间尺度的DRL，解决了关于移动性即时影响以及资源分配优化中不可承受的大动作空间的问题。
- en: IX Lessons Learned and Open Challenges
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 经验教训与开放挑战
- en: To identify existing challenges and circumvent potential misleading directions,
    we briefly introduce the potential scenario of “DL application on Edge”, and separately
    discuss open issues related to four enabling technologies that we focus on, i.e.,
    “DL inference in Edge”, “Edge Computing for DL”, “DL training at Edge” and “DL
    for optimizing Edge”.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别现有的挑战并避免潜在的误导方向，我们简要介绍了“边缘上的深度学习应用”的潜在场景，并分别讨论了与我们关注的四种使能技术相关的开放问题，即“边缘上的深度学习推理”、“用于深度学习的边缘计算”、“边缘上的深度学习训练”和“优化边缘的深度学习”。
- en: IX-A More Promising Applications
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A 更有前景的应用
- en: if DL and edge are well-integrated, they can offer great potential for the development
    of innovative applications. There are still many areas to be explored to provide
    operators, suppliers and third parties with new business opportunities and revenue
    streams.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深度学习和边缘计算能够良好集成，它们可以为创新应用的发展提供巨大潜力。仍有许多领域待探索，为运营商、供应商和第三方提供新的商业机会和收入来源。
- en: For example, with more DL techniques are universally embedded in these emerged
    applications, the introduced processing delay and additional computation cost
    make the cloud gaming architecture struggle to meet the latency requirements.
    Edge computing architectures, near to users, can be leveraged with the cloud to
    form a hybrid gaming architecture. Besides, intelligent driving involves speech
    recognition, image recognition, intelligent decision making, etc. Various DL applications
    in intelligent driving, such as collision warning, require edge computing platforms
    to ensure millisecond-level interaction delay. In addition, edge perception is
    more conducive to analyze the traffic environment around the vehicle, thus enhancing
    driving safety.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，随着越来越多的DL技术普遍嵌入这些新兴应用中，引入的处理延迟和额外的计算成本使得云游戏架构难以满足延迟要求。靠近用户的边缘计算架构可以与云端结合，形成混合游戏架构。此外，智能驾驶涉及语音识别、图像识别、智能决策等。智能驾驶中的各种DL应用，如碰撞预警，需要边缘计算平台来确保毫秒级的交互延迟。此外，边缘感知更有助于分析车辆周围的交通环境，从而提高驾驶安全性。
- en: IX-B General DL Model for Inference
  id: totrans-481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B 通用DL推理模型
- en: When deploying DL in edge devices, it is necessary to accelerate DL inference
    by model optimization. In this section, lessons learned and future directions
    for “DL inference in Edge”, with respect to model compression, model segmentation,
    and EEoI, used to optimize DL models, is discussed.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘设备上部署深度学习（DL）时，必须通过模型优化来加速DL推理。本节讨论了“边缘DL推理”的经验教训和未来方向，涉及模型压缩、模型分割和EEoI，以优化DL模型。
- en: IX-B1 Ambiguous Performance Metrics
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B1 模糊的性能指标
- en: For an Edge DL service for a specific task, there are usually a series of DL
    model candidates that can accomplish the task. However, it is difficult for service
    providers to choose the right DL model for each service. Due to the uncertain
    characteristics of edge computing networks (varying wireless channel qualities,
    unpredictable concurrent service requests, etc.), commonly used standard performance
    indicators (such as top-$k$ accuracy [[138](#bib.bib138)] or mean average accuracy
    [[164](#bib.bib164)]) cannot reflect the runtime performance of DL model inference
    in the edge. For Edge DL services, besides model accuracy, inference delay, resource
    consumption, and service revenue are also key indicators. Therefore, we need to
    identify the key performance indicators of Edge DL, quantitatively analyze the
    factors affecting them, and explore the trade-offs between these indicators to
    help improve the efficiency of Edge DL deployment.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定任务的边缘DL服务，通常有一系列可以完成任务的DL模型候选者。然而，服务提供商很难为每个服务选择合适的DL模型。由于边缘计算网络的特性不确定（无线信道质量变化、不可预测的并发服务请求等），常用的标准性能指标（如
    top-$k$ 准确率 [[138](#bib.bib138)] 或平均准确率 [[164](#bib.bib164)]) 不能反映DL模型推理在边缘的运行时性能。对于边缘DL服务，除了模型准确性外，推理延迟、资源消耗和服务收益也是关键指标。因此，我们需要识别边缘DL的关键性能指标，定量分析影响因素，并探索这些指标之间的权衡，以帮助提高边缘DL部署的效率。
- en: IX-B2 Generalization of EEoI
  id: totrans-485
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B2 EEoI 的泛化
- en: 'Currently, EEoI can be applied to classification problems in DL [[160](#bib.bib160)],
    but there is no generalized solution for a wider range of DL applications. Furthermore,
    in order to build an intelligent edge and support edge intelligence, not only
    DL but also the possibility of applying EEoI to DRL should be explored, since
    applying DRL to real-time resource management for the edge, as discussed in Section
    [VIII](#S8 "VIII Deep Learning for Optimizing Edge ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey"), requires stringent response speed.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，EEoI 可以应用于DL中的分类问题 [[160](#bib.bib160)]，但尚无针对更广泛DL应用的通用解决方案。此外，为了建立智能边缘并支持边缘智能，不仅要探索DL的可能性，还要探索将EEoI
    应用于DRL 的可能性，因为如 [VIII](#S8 "VIII Deep Learning for Optimizing Edge ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey") 节所讨论，将DRL 应用于边缘的实时资源管理需要严格的响应速度。'
- en: IX-B3 Hybrid model modification
  id: totrans-487
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B3 混合模型修改
- en: Coordination issues with respect to model optimization, model segmentation,
    and EEoI should be thought over. These customized DL models are often used independently
    to enable “end-edge-cloud” collaboration. Model optimizations, such as model quantification
    and pruning, may be required on the end and edge sides, but because of the sufficient
    computation resources, the cloud does not need to take the risk of model accuracy
    to use these optimizations. Therefore, how to design a hybrid precision scheme,
    that is, to effectively combine the simplified DL models in the edge with the
    raw DL model in the cloud is important.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 关于模型优化、模型分割和EEoI的协调问题应加以考虑。这些定制的深度学习模型通常是独立使用，以实现“端-边缘-云”协作。模型优化，如模型量化和剪枝，可能在端和边缘侧需要，但由于计算资源充足，云端无需冒模型精度的风险来使用这些优化。因此，如何设计一种混合精度方案，即有效地将边缘中的简化深度学习模型与云中的原始深度学习模型结合起来是重要的。
- en: IX-B4 Coordination between training and inference
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-B4 训练与推理之间的协调
- en: Pruning, quantizing and introducing EEoI into trained raw DL models require
    retraining to give them the desired inference performance. In general, customized
    models can be trained offline in the cloud. However, the advantage of edge computing
    lies in its response speed and might be neutralized because of belated DL training.
    Moreover, due to a large number of heterogeneous devices in the edge and the dynamic
    network environment, the customization requirements of DL models are not monotonous.
    Then, is this continuous model training requirement reasonable, and will it affect
    the timeliness of model inference? How to design a mechanism to avoid these side-effects?
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝、量化和将EEoI引入训练后的原始深度学习模型需要重新训练以达到所需的推理性能。一般而言，定制模型可以在云端离线训练。然而，边缘计算的优势在于其响应速度，可能会因延迟的深度学习训练而被削弱。此外，由于边缘中大量的异构设备和动态的网络环境，深度学习模型的定制要求并不单一。那么，这种持续的模型训练需求是否合理？会不会影响模型推理的时效性？如何设计机制以避免这些副作用？
- en: IX-C Complete Edge Architecture for DL
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-C 完整的边缘架构用于深度学习
- en: Edge intelligence and intelligent edge require a complete system framework,
    covering data acquisition, service deployment and task processing. In this section,
    we discuss challenges for “Edge Computing for DL” to build a complete edge computing
    framework for DL.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘智能和智能边缘需要一个完整的系统框架，涵盖数据获取、服务部署和任务处理。在本节中，我们讨论“深度学习的边缘计算”面临的挑战，以构建一个完整的边缘计算框架。
- en: IX-C1 Edge for Data Processing
  id: totrans-493
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-C1 数据处理的边缘
- en: Both pervasively deployed DL services on the edge and DL algorithms for optimizing
    edge cannot be realized without data acquiring. Edge architecture should be able
    to efficiently acquire and process the original data, sensed or collected by edge
    devices, and then feed them to DL models.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘上普遍部署的深度学习服务和用于优化边缘的深度学习算法都无法实现没有数据获取。边缘架构应能够高效地获取和处理原始数据，这些数据由边缘设备感知或收集，然后将其输入到深度学习模型中。
- en: Adaptively acquiring data at the edge and then transmitting them to cloud (as
    done in [[7](#bib.bib7)]) is a natural way to alleviate the workload of edge devices
    and to reduce the potential resource overhead. In addition, it is better to further
    compress the data, which can alleviate the bandwidth pressure of the network,
    while the transmission delay can be reduced to provide better QoS. Most existed
    works focus only on vision applications [[102](#bib.bib102)]. However, the heterogeneous
    data structures and characteristics of a wide variety of DL-based services are
    not addressed well yet. Therefore, developing a heterogeneous, parallel and collaborative
    architecture for edge data processing for various DL services will be helpful.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 适应性地在边缘获取数据，然后将其传输到云端（如[[7](#bib.bib7)]所示）是一种自然的方法，可以减轻边缘设备的工作负担并减少潜在的资源开销。此外，进一步压缩数据有助于缓解网络带宽压力，同时可以减少传输延迟，从而提供更好的服务质量（QoS）。现有的大多数研究仅关注视觉应用[[102](#bib.bib102)]。然而，各种基于深度学习的服务的异构数据结构和特征尚未得到很好的解决。因此，为各种深度学习服务开发一个异构、并行和协作的边缘数据处理架构将是有益的。
- en: IX-C2 Microservice for Edge DL Services
  id: totrans-496
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-C2 面向边缘深度学习服务的微服务
- en: 'Edge and cloud services have recently started undergoing a major shift from
    monolithic entities to graphs of hundreds of loosely-coupled microservices [[274](#bib.bib274)].
    Executing DL computations may need a series of software dependencies, and it calls
    for a solution for isolating different DL services on the shared resources. At
    present, the microservice framework, deployed on the edge for hosting DL services,
    is in its infant [[275](#bib.bib275)], due to several critical challenges: 1)
    Handling DL deployment and management flexibly; 2) Achieving live migration of
    microservices to reduce migration times and unavailability of DL services due
    to user mobilities; 3) Orchestrating resources among the cloud and distributed
    edge infrastructures to achieve better performance, as illustrated in Section
    [VI-B3](#S6.SS2.SSS3 "VI-B3 Vertical Collaboration ‣ VI-B Communication and Computation
    Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge
    Computing and Deep Learning: A Comprehensive Survey").'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘和云服务最近开始从单体实体转变为数百个松散耦合的微服务图 [[274](#bib.bib274)]。执行深度学习计算可能需要一系列的软件依赖，这需要一个在共享资源上隔离不同深度学习服务的解决方案。目前，部署在边缘用于托管深度学习服务的微服务框架仍处于起步阶段
    [[275](#bib.bib275)]，面临几个关键挑战：1）灵活处理深度学习的部署和管理；2）实现微服务的实时迁移，以减少因用户移动导致的迁移时间和深度学习服务的不可用性；3）在云端和分布式边缘基础设施之间协调资源，以实现更好的性能，详见第
    [VI-B3](#S6.SS2.SSS3 "VI-B3 Vertical Collaboration ‣ VI-B Communication and Computation
    Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge
    Computing and Deep Learning: A Comprehensive Survey") 节。'
- en: IX-C3 Incentive and trusty offloading mechanism for DL
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-C3 深度学习的激励和信任卸载机制
- en: 'Heavy DL computations on resource-limited end devices can be offloaded to nearby
    edge nodes (Section [VI-B](#S6.SS2 "VI-B Communication and Computation Modes for
    Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")). However, there are still several
    issues, 1) an incentive mechanism should be established for stimulating edge nodes
    to take over DL computations; 2) the security should be guaranteed to avoid the
    risks from anonymous edge nodes [[276](#bib.bib276)].'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '资源有限的终端设备上的重度深度学习计算可以卸载到附近的边缘节点（见第 [VI-B](#S6.SS2 "VI-B Communication and Computation
    Modes for Edge DL ‣ VI Edge Computing for Deep Learning ‣ Convergence of Edge
    Computing and Deep Learning: A Comprehensive Survey") 节）。然而，仍然存在几个问题：1）应建立激励机制以刺激边缘节点接管深度学习计算；2）需要确保安全，以避免来自匿名边缘节点的风险
    [[276](#bib.bib276)]。'
- en: Blockchain, as a decentralized public database storing transaction records across
    participated devices, can avoid the risk of tampering the records [[277](#bib.bib277)].
    By taking advantage of these characteristics, incentive and trust problems with
    respect to computation offloading can potentially be tackled. To be specific,
    all end devices and edge nodes have to first put down deposits to the blockchain
    to participate. The end device request the help of edge nodes for DL computation,
    and meantime send a “require” transaction to the blockchain with a bounty. Once
    an edge nodes complete the computation, it returns results to the end device with
    sending a “complete” transaction to the blockchain. After a while, other participated
    edge nodes also execute the offloaded task and validate the former recorded result.
    At last, for incentives, firstly recorded edge nodes win the game and be awarded
    [[278](#bib.bib278)]. However, this idea about blockchained edge is still in its
    infancy. Existing blockchains such as Ethereum [[279](#bib.bib279)] do not support
    the execution of complex DL computations, which raises the challenge of adjusting
    blockchain structure and protocol in order to break this limitation.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 区块链作为一个去中心化的公共数据库，存储参与设备的交易记录，可以避免篡改记录的风险 [[277](#bib.bib277)]。通过利用这些特性，可以潜在地解决与计算卸载相关的激励和信任问题。具体来说，所有终端设备和边缘节点首先需要向区块链支付押金才能参与。终端设备请求边缘节点进行深度学习计算，同时向区块链发送一个带有赏金的“需求”交易。一旦边缘节点完成计算，它会向终端设备返回结果，并向区块链发送一个“完成”交易。过一段时间，其他参与的边缘节点也会执行卸载的任务并验证之前记录的结果。最后，为了激励，首先记录的边缘节点赢得游戏并获得奖励
    [[278](#bib.bib278)]。然而，这一关于区块链边缘的想法仍处于起步阶段。现有的区块链，如以太坊 [[279](#bib.bib279)]，不支持复杂的深度学习计算，这带来了调整区块链结构和协议以突破这一限制的挑战。
- en: IX-C4 Integration with “DL for optimizing Edge”
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-C4 与“优化边缘的深度学习”的集成
- en: 'End devices, edge nodes, and base stations in edge computing networks are expected
    to run various DL models and deploy corresponding services in the future. In order
    to make full use of decentralized resources of edge computing, and to establish
    connections with existing cloud computing infrastructure, dividing the computation-intensive
    DL model into sub-tasks and effectively offloading these tasks between edge devices
    for collaboration are essential. Owing to deployment environments of Edge DL are
    usually highly dynamic, edge computing frameworks need excellent online resource
    orchestration and parameter configuration to support a large number of DL services.
    Heterogeneous computation resources, real-time joint optimization of communication
    and cache resources, and high-dimensional system parameter configuration are critical.
    We have introduced various theoretical methods to optimize edge computing frameworks
    (networks) with DL technologies in Section [VIII](#S8 "VIII Deep Learning for
    Optimizing Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey"). Nonetheless, there is currently no relevant work to deeply study the
    performance analysis of deploying and using these DL technologies for long-term
    online resource orchestration in practical edge computing networks or testbeds.
    We believe that “Edge Computing for DL” should continue to focus on how to integrate
    “DL for optimizing Edge” into the edge computing framework to realize the above
    vision.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '边缘计算网络中的终端设备、边缘节点和基站预计将在未来运行各种DL模型并部署相应服务。为了充分利用边缘计算的去中心化资源，并与现有云计算基础设施建立连接，将计算密集的DL模型划分为子任务，并有效地在边缘设备之间卸载这些任务以实现协作是至关重要的。由于边缘DL的部署环境通常高度动态，边缘计算框架需要出色的在线资源编排和参数配置来支持大量的DL服务。异构计算资源、通信和缓存资源的实时联合优化以及高维系统参数配置都是关键因素。我们在[第八节](#S8
    "VIII Deep Learning for Optimizing Edge ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey")介绍了各种理论方法来优化边缘计算框架（网络）与DL技术的结合。然而，目前尚无相关工作深入研究这些DL技术在实际边缘计算网络或测试平台上进行长期在线资源编排的性能分析。我们认为，“DL的边缘计算”应继续关注如何将“优化边缘的DL”集成到边缘计算框架中，以实现上述愿景。'
- en: IX-D Practical Training Principles at Edge
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-D 边缘实际训练原则
- en: Compared with DL inference in the edge, DL training at the edge is currently
    mainly limited by the weak performance of edge devices and the fact that most
    Edge DL frameworks or libraries still do not support training. At present, most
    studies are at the theoretical level, i.e., simulating the process of DL training
    at the edge. In this section, we point out the lessons learned and challenges
    in “DL Training at Edge”.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 与边缘DL推断相比，边缘DL训练目前主要受限于边缘设备的性能较弱，以及大多数边缘DL框架或库仍然不支持训练。目前，大多数研究仍处于理论阶段，即模拟边缘DL训练的过程。在这一部分，我们指出了在“边缘DL训练”中的经验教训和挑战。
- en: IX-D1 Data Parallelism versus Model Parallelism
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-D1 数据并行性与模型并行性
- en: 'DL models are both computation and memory intensive. When they become deeper
    and larger, it is not feasible to acquire their inference results or train them
    well by a single device. Therefore, large DL models are trained in distributed
    manners over thousands of CPU or GPU cores, in terms of data parallelism, model
    parallelism or their combination (Section [III-C](#S3.SS3 "III-C Distributed DL
    Training ‣ III Fundamentals of Deep Learning ‣ Convergence of Edge Computing and
    Deep Learning: A Comprehensive Survey")). However, differing from parallel training
    over bus-or switch-connected CPUs or GPUs in the cloud, perform model training
    at distributed edge devices should further consider wireless environments, device
    configurations, privacies, etc.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 'DL模型既计算密集又内存密集。当它们变得更深、更大时，单个设备无法有效获取其推断结果或进行良好的训练。因此，大型DL模型在数千个CPU或GPU核心上以分布式方式进行训练，涉及数据并行性、模型并行性或它们的组合（见[III-C](#S3.SS3
    "III-C Distributed DL Training ‣ III Fundamentals of Deep Learning ‣ Convergence
    of Edge Computing and Deep Learning: A Comprehensive Survey")）。然而，与在云中通过总线或交换机连接的CPU或GPU进行并行训练不同，分布式边缘设备上的模型训练还应进一步考虑无线环境、设备配置、隐私等因素。'
- en: 'At present, FL only copies the whole DL model to every participated edge devices,
    namely in the manner of data parallelism. Hence, taking the limited computing
    capabilities of edge devices (at least for now) into consideration, partitioning
    a large-scale DL model and allocating these segments to different edge devices
    for training may be a more feasible and practical solution. Certainly, this does
    not mean abandoning the native data parallelism of FL, instead, posing the challenge
    of blending data parallelism and model parallelism particularly for training DL
    models at the edge, as illustrated in Fig. [21](#S9.F21 "Figure 21 ‣ IX-D1 Data
    Parallelism versus Model Parallelism ‣ IX-D Practical Training Principles at Edge
    ‣ IX Lessons Learned and Open Challenges ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey").'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，FL 仅将整个 DL 模型复制到每个参与的边缘设备，即以数据并行的方式。因此，考虑到边缘设备的有限计算能力（至少目前如此），将大规模 DL 模型进行分区，并将这些分段分配给不同的边缘设备进行训练可能是一个更可行和实际的解决方案。当然，这并不意味着放弃
    FL 的原生数据并行性，而是提出了将数据并行性和模型并行性融合的挑战，特别是在边缘训练 DL 模型时，如图 [21](#S9.F21 "图 21 ‣ IX-D1
    数据并行性与模型并行性 ‣ IX-D 边缘的实际训练原则 ‣ IX 经验教训和开放挑战 ‣ 边缘计算与深度学习的融合：综合调查") 所示。
- en: '![Refer to caption](img/262182344eacc927c835ae5908eb8f54.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/262182344eacc927c835ae5908eb8f54.png)'
- en: 'Figure 21: DL training at the edge by both data and model parallelism.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：通过数据和模型并行性在边缘进行 DL 训练。
- en: IX-D2 Where is training data from?
  id: totrans-510
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-D2 训练数据从哪里来？
- en: Currently, most of the DL training frameworks at the edge are aimed at supervised
    learning tasks, and test their performance with complete data sets. However, in
    practical scenarios, we cannot assume that all data in the edge computing network
    are labeled and with a correctness guarantee. For unsupervised learning tasks
    such as DRL, we certainly do not need to pay too much attention to the production
    of training data. For example, the training data required for DRL compose of the
    observed state vectors and rewards obtained by interacting with the environment.
    These training data can generate automatically when the system is running. But
    for a wider range of supervised learning tasks, how edge nodes and devices find
    the exact training data for model training? The application of vanilla FL is using
    RNN for next-word-prediction [[199](#bib.bib199)], in which the training data
    can be obtained along with users’ daily inputs. Nonetheless, for extensive Edge
    DL services concerning video analysis, where are their training data from. If
    all training data is manually labeled and uploaded to the cloud data center, and
    then distributed to edge devices by the cloud, the original intention of FL is
    obviously violated. One possible solution is to enable edge devices to construct
    their labeled data by learning “labeled data” from each other. We believe that
    the production of training data and the application scenarios of DL models training
    at the edge should first be clarified in the future, and the necessity and feasibility
    of DL model training at the edge should be discussed as well.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数边缘 DL 训练框架都针对监督学习任务，并使用完整的数据集来测试其性能。然而，在实际场景中，我们不能假设边缘计算网络中的所有数据都是标记的并且具有正确性保证。对于像
    DRL 这样的无监督学习任务，我们确实不需要过多关注训练数据的生产。例如，DRL 所需的训练数据由通过与环境互动获得的观察状态向量和奖励组成。这些训练数据可以在系统运行时自动生成。但对于范围更广的监督学习任务，边缘节点和设备如何找到用于模型训练的准确训练数据？普通
    FL 的应用是使用 RNN 进行下一个词预测 [[199](#bib.bib199)]，其中训练数据可以通过用户的日常输入获得。然而，对于涉及视频分析的广泛边缘
    DL 服务，它们的训练数据来自哪里？如果所有训练数据都由人工标记并上传到云数据中心，然后由云分发到边缘设备，那么 FL 的初衷显然被违反了。一个可能的解决方案是使边缘设备通过相互学习“标记数据”来构建它们的标记数据。我们认为，未来应首先明确训练数据的生产和边缘
    DL 模型训练的应用场景，并讨论 DL 模型在边缘训练的必要性和可行性。
- en: IX-D3 Asynchronous FL at Edge
  id: totrans-512
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-D3 边缘的异步 FL
- en: Existing FL methods [[199](#bib.bib199), [198](#bib.bib198)] focus on synchronous
    training, and can only process hundreds of devices in parallel. However, this
    synchronous updating mode potentially cannot scale well, and is inefficient and
    inflexible in view of two key properties of FL, specifically, 1) infrequent training
    tasks, since edge devices typically have weaker computing power and limited battery
    endurance and thus cannot afford intensive training tasks; 2) limited and uncertain
    communication between edge devices, compared to typical distributed training in
    the cloud.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的FL方法[[199](#bib.bib199), [198](#bib.bib198)]专注于同步训练，只能并行处理数百个设备。然而，这种同步更新模式可能无法很好地扩展，并且在考虑FL的两个关键属性时效率低下且不灵活，具体来说，1)
    训练任务不频繁，因为边缘设备通常计算能力较弱且电池续航有限，无法承担密集的训练任务；2) 与典型的云端分布式训练相比，边缘设备之间的通信有限且不确定。
- en: Thus, whenever the global model is updating, the server is limited to selecting
    from a subset of available edge devices to trigger a training task. In addition,
    due to limited computing power and battery endurance, task scheduling varies from
    device to device, making it difficult to synchronize selected devices at the end
    of each epoch. Some devices may no longer be available when they should be synchronized,
    and hence the server must determine the timeout threshold to discard the laggard.
    If the number of surviving devices is too small, the server has to discard the
    entire epoch including all received updates. These bottlenecks in FL can potentially
    be addressed by asynchronous training mechanisms [[280](#bib.bib280), [281](#bib.bib281),
    [282](#bib.bib282)]. Adequately selecting clients in each training period with
    resource constraints may also help. By setting a certain deadline for clients
    to download, update, and upload DL models, the central server can determine which
    clients to perform local training such that it can aggregate as many client updates
    as possible in each period, thus allowing the server to accelerate performance
    improvement in DL models [[283](#bib.bib283)].
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每当全球模型更新时，服务器只能从有限的边缘设备子集中选择以触发训练任务。此外，由于计算能力和电池续航的限制，任务调度在设备之间有所不同，使得在每个周期结束时同步选定设备变得困难。一些设备可能在需要同步时已不再可用，因此服务器必须确定超时阈值以丢弃落后的设备。如果存活的设备数量过少，服务器必须丢弃包括所有接收更新在内的整个周期。这些FL中的瓶颈可以通过异步训练机制[[280](#bib.bib280),
    [281](#bib.bib281), [282](#bib.bib282)]得到解决。在每个训练周期中，适当选择具有资源限制的客户端也可能有所帮助。通过为客户端设置一个下载、更新和上传深度学习模型的截止时间，中央服务器可以确定哪些客户端进行本地训练，以便在每个周期内尽可能多地汇总客户端更新，从而加速深度学习模型的性能改进[[283](#bib.bib283)]。
- en: IX-D4 Transfer Learning-based Training
  id: totrans-515
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-D4 基于迁移学习的训练
- en: Due to resource constraints, training and deploying computation-intensive DL
    models on edge devices such as mobile phones is challenging. In order to facilitate
    learning on such resource-constrained edge devices, TL can be utilized. For instance,
    in order to reduce the amount of training data and speeding up the training process,
    using unlabeled data to transfer knowledge between edge devices can be adopted
    [[284](#bib.bib284)]. By using the cross-modal transfer in the learning of edge
    devices across different sensing modalities, required labeled data and the training
    process can be largely reduced and accelerated, respectively.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，在如手机等边缘设备上训练和部署计算密集型深度学习模型是具有挑战性的。为了在这种资源受限的边缘设备上促进学习，可以利用迁移学习（TL）。例如，为了减少训练数据量并加快训练过程，可以采用使用未标记数据在边缘设备之间转移知识的方法[[284](#bib.bib284)]。通过在不同感知模态下的边缘设备学习中使用跨模态迁移，可以大幅减少所需的标记数据和训练过程，分别加快训练速度。
- en: 'Besides, KD, as a method of TL, can also be exploited thanks to several advantages
    [[136](#bib.bib136)]: 1) using information from well-trained large DL models (teachers)
    to help lightweight DL models (students), expected to be deployed on edge devices,
    converge faster; 2) improving the accuracy of students; 3) helping students become
    more general instead of being overfitted by a certain set of data. Although results
    of [[136](#bib.bib136), [284](#bib.bib284)] show some prospects, further research
    is needed to extend the TL-based training method to DL applications with different
    types of perceptual data.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，KD作为TL的一种方法，也可以利用几个优点[[136](#bib.bib136)]：1）利用经过良好训练的大型DL模型（教师）中的信息帮助预期将部署在边缘设备上的轻量级DL模型（学生）更快地收敛；2）提高学生模型的准确性；3）帮助学生模型变得更具通用性，而不是被某一数据集过度拟合。尽管[[136](#bib.bib136)、[284](#bib.bib284)]的结果显示了一些前景，但仍需进一步研究以将基于TL的训练方法扩展到具有不同类型感知数据的DL应用。
- en: IX-E Deployment and Improvement of Intelligent Edge
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-E 智能边缘的部署和改进
- en: There have been many attempts to use DL to optimize and schedule resources in
    edge computing networks. In this regard, there are many potential areas where
    DL can be applied, including online content streaming [[285](#bib.bib285)], routing
    and traffic control [[286](#bib.bib286)][[287](#bib.bib287)], etc. However, since
    DL solutions do not rely entirely on accurate modeling of networks and devices,
    finding a scenario where DL can be applied is not the most important concern.
    Besides, if applying DL to optimize real-time edge computing networks, the training
    and inference of DL models or DRL algorithms may bring certain side effects, such
    as the additional bandwidth consumed by training data transmission and the latency
    of DL inference.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有许多尝试使用DL来优化和调度边缘计算网络中的资源。在这方面，DL可以应用的潜在领域有很多，包括在线内容流[[285](#bib.bib285)]、路由和流量控制[[286](#bib.bib286)][[287](#bib.bib287)]等。然而，由于DL解决方案并不完全依赖于网络和设备的准确建模，找到DL可以应用的场景并不是最重要的问题。此外，应用DL优化实时边缘计算网络时，DL模型或DRL算法的训练和推理可能会带来一些副作用，例如训练数据传输消耗的额外带宽和DL推理的延迟。
- en: 'Existing works mainly concern about solutions of “DL for optimizing Edge” at
    the high level, but overlook the practical feasibility at the low level. Though
    DL exhibits its theoretical performance, the deployment issues of DNNs/DRL should
    be carefully considered (as illustrated in Fig. [22](#S9.F22 "Figure 22 ‣ IX-E
    Deployment and Improvement of Intelligent Edge ‣ IX Lessons Learned and Open Challenges
    ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive Survey")):'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的工作主要关注于“DL优化边缘”的高层解决方案，但忽略了低层的实际可行性。尽管DL展现了其理论性能，但DNNs/DRL的部署问题需要仔细考虑（如图[22](#S9.F22
    "Figure 22 ‣ IX-E Deployment and Improvement of Intelligent Edge ‣ IX Lessons
    Learned and Open Challenges ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey")所示）：'
- en: •
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Where DL and DRL should be deployed, in view of the resource overhead of them
    and the requirement of managing edge computing networks in real time?
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在DL和DRL应该部署的地方，考虑到它们的资源开销和实时管理边缘计算网络的要求？
- en: •
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When using DL to determine caching policies or optimize task offloading, will
    the benefits of DL be neutralized by the bandwidth consumption and the processing
    delay brought by DL itself?
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当使用DL来确定缓存策略或优化任务卸载时，DL的好处是否会被DL自身带来的带宽消耗和处理延迟所抵消？
- en: •
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'How to explore and improve edge computing architectures in Section [VI](#S6
    "VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing and Deep
    Learning: A Comprehensive Survey") to support “DL for optimizing Edge”?'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如何在第[VI](#S6 "VI Edge Computing for Deep Learning ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")节中探索和改进边缘计算架构，以支持“DL优化边缘”？'
- en: •
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Are the ideas of customized DL models, introduced in Section [V](#S5 "V Deep
    Learning Inference in Edge ‣ Convergence of Edge Computing and Deep Learning:
    A Comprehensive Survey"), can help to facilitate the practical deployment?'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[V](#S5 "V Deep Learning Inference in Edge ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")节中介绍的定制DL模型的理念是否有助于促进实际部署？'
- en: •
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'How to modify the training principles in Section [VII](#S7 "VII Deep Learning
    Training at Edge ‣ Convergence of Edge Computing and Deep Learning: A Comprehensive
    Survey") to enhance the performance of DL training, in order to meet the timeliness
    of edge management?'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如何修改第[VII](#S7 "VII Deep Learning Training at Edge ‣ Convergence of Edge Computing
    and Deep Learning: A Comprehensive Survey")节中的训练原则，以提高DL训练的性能，从而满足边缘管理的时效性？'
- en: '![Refer to caption](img/9fc4ee62d08250b7a9235dae753432b2.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9fc4ee62d08250b7a9235dae753432b2.png)'
- en: 'Figure 22: Deployment issues of intelligent edge, i.e., how and where to deploy
    DL models for optimizing edge computing networks (systems).'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：智能边缘的部署问题，即如何以及在何处部署 DL 模型以优化边缘计算网络（系统）。
- en: Besides, the abilities of the state-of-the-art DL or DRL, such as Multi-Agent
    Deep Reinforcement Learning [[288](#bib.bib288), [289](#bib.bib289), [290](#bib.bib290)],
    Graph Neural Networks (GNNs) [[291](#bib.bib291), [292](#bib.bib292)], can also
    be exploited to facilitate this process. For example, end devices, edge nodes,
    and the cloud can be deemed as individual agents. By this means, each agent trains
    its own strategy according to its local imperfect observations, and all participated
    agents work together for optimizing edge computing networks. In addition, the
    structure of edge computing networks across the end, the edge, and the cloud is
    actually an immense graph, which comprises massive latent structure information,
    e.g., the connection and bandwidth between devices. For better understanding edge
    computing networks, GNNs, which focuses on extracting features from graph structures
    instead of two-dimensional meshes and one-dimensional sequences, might be a promising
    method.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最先进的 DL 或 DRL 的能力，如多智能体深度强化学习 [[288](#bib.bib288), [289](#bib.bib289), [290](#bib.bib290)]，图神经网络
    (GNNs) [[291](#bib.bib291), [292](#bib.bib292)]，也可以被利用来促进这一过程。例如，终端设备、边缘节点和云可以被视为独立的智能体。通过这种方式，每个智能体根据其本地不完全观察训练自己的策略，所有参与的智能体共同优化边缘计算网络。此外，终端、边缘和云之间的边缘计算网络结构实际上是一个巨大的图，包括大量潜在的结构信息，例如设备之间的连接和带宽。为了更好地理解边缘计算网络，GNNs（专注于从图结构中提取特征，而不是二维网格和一维序列）可能是一种有前景的方法。
- en: X Conclusions
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 结论
- en: 'DL, as a key technique of artificial intelligence, and edge computing are expected
    to benefit each other. This survey has comprehensively introduced and discussed
    various applicable scenarios and fundamental enabling techniques for edge intelligence
    and intelligent edge. In summary, the key issue of extending DL from the cloud
    to the edge of the network is: under the multiple constraints of networking, communication,
    computing power, and energy consumption, how to devise and develop edge computing
    architecture to achieve the best performance of DL training and inference. As
    the computing power of the edge increases, edge intelligence will become common,
    and intelligent edge will play an important supporting role to improve the performance
    of edge intelligence. We hope that this survey will increase discussions and research
    efforts on DL/Edge integration that will advance future communication applications
    and services.'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: DL 作为人工智能的关键技术，与边缘计算预计会互相促进。本次调查全面介绍和讨论了边缘智能和智能边缘的各种适用场景和基础启用技术。总而言之，将 DL 从云端扩展到网络边缘的关键问题是：在网络、通信、计算能力和能耗的多重约束下，如何设计和开发边缘计算架构，以实现
    DL 训练和推理的最佳性能。随着边缘计算能力的提升，边缘智能将变得普遍，智能边缘将发挥重要的支持作用，提高边缘智能的性能。我们希望本次调查能增加对 DL/边缘整合的讨论和研究工作，从而推动未来的通信应用和服务。
- en: Acknowledgement
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Key R&D Program of China (No.2019YFB2101901
    and No.2018YFC0809803), National Science Foundation of China (No.61702364, No.61972432
    and No.U1711265), the Program for Guangdong Introducing Innovative and Enterpreneurial
    Teams (No.2017ZT07X355), Chinese National Engineering Laboratory for Big Data
    System Computing Technology and Canadian Natural Sciences and Engineering Research
    Council. It was also supported in part by Singapore NRF National Satellite of
    Excellence, Design Science and Technology for Secure Critical Infrastructure NSoE
    DeST-SCI2019-0007, A*STAR-NTU-SUTD Joint Research Grant Call on Artificial Intelligence
    for the Future of Manufacturing RGANS1906, WASP/NTU M4082187 (4080), Singapore
    MOE Tier 1 2017-T1-002-007 RG122/17, MOE Tier 2 MOE2014-T2-2-015 ARC4/15, Singapore
    NRF2015-NRF-ISF001-2277, and Singapore EMA Energy Resilience NRF2017EWT-EP003-041.
    Especially, we would like to thank the editors of IEEE COMST and the reviewers
    for their help and support in making this work possible.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作得到了中国国家重点研发计划（编号2019YFB2101901和编号2018YFC0809803）、中国国家自然科学基金（编号61702364、编号61972432和编号U1711265）、广东省引进创新团队项目（编号2017ZT07X355）、中国国家大数据系统计算技术工程实验室和加拿大自然科学与工程研究委员会的资助。部分资助来自新加坡NRF国家卓越中心、设计科学与技术保障关键基础设施NSoE
    DeST-SCI2019-0007、A*STAR-NTU-SUTD联合研究资助人工智能制造未来RGANS1906、WASP/NTU M4082187（4080）、新加坡MOE
    Tier 1 2017-T1-002-007 RG122/17、MOE Tier 2 MOE2014-T2-2-015 ARC4/15、新加坡NRF2015-NRF-ISF001-2277和新加坡EMA能源韧性NRF2017EWT-EP003-041。特别感谢IEEE
    COMST的编辑和审稿人为使本工作成为可能所提供的帮助和支持。
- en: References
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] “Fog Computing and the Internet of Things: Extend the Cloud to Where the
    Things Are.” [Online]. Available: [https://www.cisco.com/c/dam/en_us/solutions/trends/iot/docs/computing-overview.pdf](https://www.cisco.com/c/dam/en_us/solutions/trends/iot/docs/computing-overview.pdf)'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] “雾计算与物联网：将云计算扩展到事物所在的地方。” [在线]. 可用： [https://www.cisco.com/c/dam/en_us/solutions/trends/iot/docs/computing-overview.pdf](https://www.cisco.com/c/dam/en_us/solutions/trends/iot/docs/computing-overview.pdf)'
- en: '[2] “Cisco Global Cloud Index: Forecast and Methodology.” [Online]. Available:
    [https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] “思科全球云指数：预测与方法。” [在线]. 可用： [https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html)'
- en: '[3] M. V. Barbera, S. Kosta, A. Mei *et al.*, “To offload or not to offload?
    The bandwidth and energy costs of mobile cloud computing,” in *2013 IEEE Conference
    on Computer Communications (INFOCOM 2013)*, 2013, pp. 1285–1293.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. V. Barbera, S. Kosta, A. Mei *等*, “是卸载还是不卸载？移动云计算的带宽和能量成本，” 收录于 *2013
    IEEE计算机通信大会 (INFOCOM 2013)*, 2013年, pp. 1285–1293。'
- en: '[4] W. Hu, Y. Gao, K. Ha *et al.*, “Quantifying the Impact of Edge Computing
    on Mobile Applications,” in *Proc. 7th ACM SIGOPS Asia-Pacific Workshop Syst.
    (APSys 2016)*, 2016, pp. 1–8.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] W. Hu, Y. Gao, K. Ha *等*, “量化边缘计算对移动应用的影响，” 收录于 *第七届ACM SIGOPS亚太地区系统研讨会
    (APSys 2016)*, 2016年, pp. 1–8。'
- en: '[5] “Mobile-Edge Computing–Introductory Technical White Paper,” ETSI. [Online].
    Available: [https://portal.etsi.org/Portals/0/TBpages/MEC/Docs/Mobile-edge_Computing_-_Introductory_Technical_White_Paper_V1%2018-09-14.pdf](https://portal.etsi.org/Portals/0/TBpages/MEC/Docs/Mobile-edge_Computing_-_Introductory_Technical_White_Paper_V1%2018-09-14.pdf)'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] “移动边缘计算–介绍性技术白皮书，” ETSI. [在线]. 可用： [https://portal.etsi.org/Portals/0/TBpages/MEC/Docs/Mobile-edge_Computing_-_Introductory_Technical_White_Paper_V1%2018-09-14.pdf](https://portal.etsi.org/Portals/0/TBpages/MEC/Docs/Mobile-edge_Computing_-_Introductory_Technical_White_Paper_V1%2018-09-14.pdf)'
- en: '[6] W. Shi, J. Cao *et al.*, “Edge Computing: Vision and Challenges,” *IEEE
    Internet Things J.*, vol. 3, no. 5, pp. 637–646, Oct. 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] W. Shi, J. Cao *等*, “边缘计算：愿景与挑战，” *IEEE互联网事物杂志*, vol. 3, no. 5, pp. 637–646,
    2016年10月。'
- en: '[7] B. A. Mudassar, J. H. Ko, and S. Mukhopadhyay, “Edge-cloud collaborative
    processing for intelligent internet of things,” in *Proc. the 55th Annual Design
    Automation Conference (DAC 2018)*, 2018, pp. 1–6.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] B. A. Mudassar, J. H. Ko, 和 S. Mukhopadhyay, “边缘云协同处理用于智能物联网，” 收录于 *第55届年会设计自动化会议
    (DAC 2018)*, 2018年, pp. 1–6。'
- en: '[8] A. Yousefpour, C. Fung, T. Nguyen *et al.*, “All one needs to know about
    fog computing and related edge computing paradigms: A complete survey,” *J SYST
    ARCHITECT.*, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Yousefpour, C. Fung, T. Nguyen *等*, “关于雾计算及相关边缘计算范式的全面调查：你需要知道的一切，”
    *系统架构杂志*, 2019年。'
- en: '[9] J. Redmon, S. Divvala *et al.*, “You Only Look Once: Unified, Real-Time
    Object Detection,” in *Proc. 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR 2016)*, 2016, pp. 779–788.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Redmon, S. Divvala *等*，“你只需看一次：统一的实时物体检测”，在*2016年IEEE计算机视觉与模式识别会议（CVPR
    2016）*，2016年，第779–788页。'
- en: '[10] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    Networks*, vol. 61, pp. 85–117, Jan. 2015.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Schmidhuber，“神经网络中的深度学习：概述”，*神经网络*，第61卷，第85–117页，2015年1月。'
- en: '[11] H. Khelifi, S. Luo, B. Nour *et al.*, “Bringing deep learning at the edge
    of information-centric internet of things,” *IEEE Commun. Lett.*, vol. 23, no. 1,
    pp. 52–55, Jan. 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Khelifi, S. Luo, B. Nour *等*，“将深度学习引入以信息为中心的物联网边缘”，*IEEE通信快报*，第23卷，第1期，第52–55页，2019年1月。'
- en: '[12] Y. Kang, J. Hauswald, C. Gao *et al.*, “Neurosurgeon: Collaborative Intelligence
    Between the Cloud and Mobile Edge,” in *Proc. 22nd Int. Conf. Archit. Support
    Program. Lang. Oper. Syst. (ASPLOS 2017)*, 2017, pp. 615–629.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y. Kang, J. Hauswald, C. Gao *等*，“Neurosurgeon：云和移动边缘之间的协作智能”，在*第22届国际计算机体系结构支持程序语言与操作系统会议（ASPLOS
    2017）*，2017年，第615–629页。'
- en: '[13] “Democratizing AI.” [Online]. Available: [https://news.microsoft.com/features/democratizing-ai/](https://news.microsoft.com/features/democratizing-ai/)'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] “让人工智能民主化。” [在线]。可用： [https://news.microsoft.com/features/democratizing-ai/](https://news.microsoft.com/features/democratizing-ai/)'
- en: '[14] Y. Yang, “Multi-tier computing networks for intelligent IoT,” *Nature
    Electronics*, vol. 2, no. 1, pp. 4–5, Jan. 2019.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Yang，“智能物联网的多层计算网络”，*自然电子学*，第2卷，第1期，第4–5页，2019年1月。'
- en: '[15] C. Li, Y. Xue, J. Wang *et al.*, “Edge-Oriented Computing Paradigms: A
    Survey on Architecture Design and System Management,” *ACM Comput. Surv.*, vol. 51,
    no. 2, pp. 1–34, Apr. 2018.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. Li, Y. Xue, J. Wang *等*，“面向边缘的计算范式：架构设计和系统管理综述”，*ACM计算机调查*，第51卷，第2期，第1–34页，2018年4月。'
- en: '[16] S. Wang, X. Zhang, Y. Zhang *et al.*, “A Survey on Mobile Edge Networks:
    Convergence of Computing, Caching and Communications,” *IEEE Access*, vol. 5,
    pp. 6757–6779, 2017.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Wang, X. Zhang, Y. Zhang *等*，“移动边缘网络综述：计算、缓存和通信的融合”，*IEEE Access*，第5卷，第6757–6779页，2017年。'
- en: '[17] T. X. Tran, A. Hajisami *et al.*, “Collaborative Mobile Edge Computing
    in 5G Networks: New Paradigms, Scenarios, and Challenges,” *IEEE Commun. Mag.*,
    vol. 55, no. 4, pp. 54–61, Apr. 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. X. Tran, A. Hajisami *等*，“5G网络中的协作移动边缘计算：新范式、场景和挑战”，*IEEE通信杂志*，第55卷，第4期，第54–61页，2017年4月。'
- en: '[18] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless Network Intelligence
    at the Edge,” *Proc. IEEE*, vol. 107, no. 11, pp. 2204–2239, Nov. 2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Park, S. Samarakoon, M. Bennis 和 M. Debbah，“边缘的无线网络智能”，*IEEE汇刊*，第107卷，第11期，第2204–2239页，2019年11月。'
- en: '[19] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge Intelligence:
    Paving the Last Mile of Artificial Intelligence With Edge Computing,” *Proc. IEEE*,
    vol. 107, no. 8, pp. 1738–1762, Aug. 2019.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo 和 J. Zhang，“边缘智能：用边缘计算铺平人工智能的最后一公里”，*IEEE汇刊*，第107卷，第8期，第1738–1762页，2019年8月。'
- en: '[20] J. Chen and X. Ran, “Deep Learning With Edge Computing: A Review,” *Proc.
    IEEE*, vol. 107, no. 8, pp. 1655–1674, Aug. 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Chen 和 X. Ran，“边缘计算中的深度学习：综述”，*IEEE汇刊*，第107卷，第8期，第1655–1674页，2019年8月。'
- en: '[21] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
    D. Niyato *et al.*, “Federated Learning in Mobile Edge Networks: A Comprehensive
    Survey,” *arXiv preprint arXiv:1909.11875*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
    D. Niyato *等*，“移动边缘网络中的联邦学习：全面综述”，*arXiv预印本arXiv:1909.11875*，2019年。'
- en: '[22] C. Mouradian, D. Naboulsi, S. Yangui *et al.*, “A Comprehensive Survey
    on Fog Computing: State-of-the-Art and Research Challenges,” *IEEE Commun. Surveys
    Tuts.*, vol. 20, no. 1, pp. 416–464, 2018.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Mouradian, D. Naboulsi, S. Yangui *等*，“雾计算的全面综述：现状和研究挑战”，*IEEE通信调查与教程*，第20卷，第1期，第416–464页，2018年。'
- en: '[23] K. Bilal, O. Khalid, A. Erbad, and S. U. Khan, “Potentials, trends, and
    prospects in edge technologies: Fog, cloudlet, mobile edge, and micro data centers,”
    *Comput. Networks*, vol. 130, no. 2018, pp. 94–120, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. Bilal, O. Khalid, A. Erbad 和 S. U. Khan，“边缘技术中的潜力、趋势和前景：雾计算、云小站、移动边缘和微数据中心”，*计算机网络*，第130卷，第2018期，第94–120页，2018年。'
- en: '[24] M. Satyanarayanan, P. Bahl, R. Cáceres, and N. Davies, “The case for vm-based
    cloudlets in mobile computing,” *IEEE Pervasive Comput.*, vol. 8, no. 4, pp. 14–23,
    2009.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Satyanarayanan, P. Bahl, R. Cáceres 和 N. Davies，“移动计算中基于虚拟机的云小站的案例”，*IEEE普适计算*，第8卷，第4期，第14–23页，2009年。'
- en: '[25] M. Aazam and E. Huh, “Fog computing micro datacenter based dynamic resource
    estimation and pricing model for iot,” in *Proc. IEEE 29th International Conference
    on Advanced Information Networking and Applications (AINA 2019)*, Mar. 2015, pp.
    687–694.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Aazam 和 E. Huh, “基于雾计算微数据中心的动态资源估算与定价模型用于物联网,” *发表于 IEEE 第29届国际高级信息网络与应用会议（AINA
    2019）*, 2015年3月, 第687–694页。'
- en: '[26] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, “Fog computing and its
    role in the internet of things,” in *Proc. the first edition of the MCC workshop
    on Mobile cloud computing*, 2012, pp. 13–16.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] F. Bonomi, R. Milito, J. Zhu, 和 S. Addepalli, “雾计算及其在物联网中的作用,” *发表于 MCC
    工作坊第一届移动云计算会议*, 2012年, 第13–16页。'
- en: '[27] F. Bonomi, R. Milito, P. Natarajan, and J. Zhu, *Fog Computing: A Platform
    for Internet of Things and Analytics*.   Cham: Springer International Publishing,
    2014, pp. 169–186.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F. Bonomi, R. Milito, P. Natarajan, 和 J. Zhu, *《雾计算：物联网与分析平台》*. Cham:
    Springer International Publishing, 2014, 第169–186页。'
- en: '[28] “Multi-access Edge Computing.” [Online]. Available: [http://www.etsi.org/technologies-clusters/technologies/multi-access-edge-computing](http://www.etsi.org/technologies-clusters/technologies/multi-access-edge-computing)'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] “多接入边缘计算。” [在线]. 可用: [http://www.etsi.org/technologies-clusters/technologies/multi-access-edge-computing](http://www.etsi.org/technologies-clusters/technologies/multi-access-edge-computing)'
- en: '[29] “What is Azure Data Box Edge?” [Online]. Available: [https://docs.microsoft.com/zh-cn/azure/databox-online/data-box-edge-overview](https://docs.microsoft.com/zh-cn/azure/databox-online/data-box-edge-overview)'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] “什么是 Azure Data Box Edge？” [在线]. 可用: [https://docs.microsoft.com/zh-cn/azure/databox-online/data-box-edge-overview](https://docs.microsoft.com/zh-cn/azure/databox-online/data-box-edge-overview)'
- en: '[30] “Intel Movidius Neural Compute Stick.” [Online]. Available: [https://software.intel.com/en-us/movidius-ncs](https://software.intel.com/en-us/movidius-ncs)'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] “Intel Movidius 神经计算棒。” [在线]. 可用: [https://software.intel.com/en-us/movidius-ncs](https://software.intel.com/en-us/movidius-ncs)'
- en: '[31] “Latest Jetson Products.” [Online]. Available: [https://developer.nvidia.com/buy-jetson](https://developer.nvidia.com/buy-jetson)'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] “最新的 Jetson 产品。” [在线]. 可用: [https://developer.nvidia.com/buy-jetson](https://developer.nvidia.com/buy-jetson)'
- en: '[32] “An all-scenario AI infrastructure solution that bridges ’device, edge,
    and cloud’ and delivers unrivaled compute power to lead you towards an AI-fueled
    future.” [Online]. Available: [https://e.huawei.com/en/solutions/business-needs/data-center/atlas](https://e.huawei.com/en/solutions/business-needs/data-center/atlas)'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] “一种全场景 AI 基础设施解决方案，桥接‘设备、边缘和云’，并提供无与伦比的计算能力，引领你迈向 AI 驱动的未来。” [在线]. 可用:
    [https://e.huawei.com/en/solutions/business-needs/data-center/atlas](https://e.huawei.com/en/solutions/business-needs/data-center/atlas)'
- en: '[33] “Snapdragon 8 Series Mobile Platforms.” [Online]. Available: [https://www.qualcomm.com/products/snapdragon-8-series-mobile-platforms](https://www.qualcomm.com/products/snapdragon-8-series-mobile-platforms)'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] “Snapdragon 8 系列移动平台。” [在线]. 可用: [https://www.qualcomm.com/products/snapdragon-8-series-mobile-platforms](https://www.qualcomm.com/products/snapdragon-8-series-mobile-platforms)'
- en: '[34] “Kirin.” [Online]. Available: [http://www.hisilicon.com/en/Products/ProductList/Kirin](http://www.hisilicon.com/en/Products/ProductList/Kirin)'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] “Kirin.” [在线]. 可用: [http://www.hisilicon.com/en/Products/ProductList/Kirin](http://www.hisilicon.com/en/Products/ProductList/Kirin)'
- en: '[35] “The World’s First Full-Stack All-Scenario AI Chip.” [Online]. Available:
    [http://www.hisilicon.com/en/Products/ProductList/Ascend](http://www.hisilicon.com/en/Products/ProductList/Ascend)'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] “全球首款全栈全场景 AI 芯片。” [在线]. 可用: [http://www.hisilicon.com/en/Products/ProductList/Ascend](http://www.hisilicon.com/en/Products/ProductList/Ascend)'
- en: '[36] “MediaTek Helio P60.” [Online]. Available: [https://www.mediatek.com/products/smartphones/mediatek-helio-p60](https://www.mediatek.com/products/smartphones/mediatek-helio-p60)'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] “MediaTek Helio P60.” [在线]. 可用: [https://www.mediatek.com/products/smartphones/mediatek-helio-p60](https://www.mediatek.com/products/smartphones/mediatek-helio-p60)'
- en: '[37] “NVIDIA Turing GPU Architecture.” [Online]. Available: [https://www.nvidia.com/en-us/geforce/turing/](https://www.nvidia.com/en-us/geforce/turing/)'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] “NVIDIA Turing GPU 架构。” [在线]. 可用: [https://www.nvidia.com/en-us/geforce/turing/](https://www.nvidia.com/en-us/geforce/turing/)'
- en: '[38] N. P. Jouppi, A. Borchers, R. Boyle, P. L. Cantin, and B. Nan, “In-Datacenter
    Performance Analysis of a Tensor Processing Unit,” in *Proc. 44th Int. Symp. Comput.
    Archit. (ISCA 2017)*, 2017, pp. 1–12.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] N. P. Jouppi, A. Borchers, R. Boyle, P. L. Cantin, 和 B. Nan, “Tensor 处理单元的数据中心性能分析,”
    *发表于第44届国际计算机架构研讨会（ISCA 2017）*, 2017年, 第1–12页。'
- en: '[39] “Intel Xeon Processor D-2100 Product Brief: Advanced Intelligence for
    High-Density Edge Solutions.” [Online]. Available: [https://www.intel.cn/content/www/cn/zh/products/docs/processors/xeon/d-2100-brief.html](https://www.intel.cn/content/www/cn/zh/products/docs/processors/xeon/d-2100-brief.html)'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] “Intel Xeon处理器D-2100产品简介：为高密度边缘解决方案提供先进智能。” [在线]. 可用链接：[https://www.intel.cn/content/www/cn/zh/products/docs/processors/xeon/d-2100-brief.html](https://www.intel.cn/content/www/cn/zh/products/docs/processors/xeon/d-2100-brief.html)'
- en: '[40] “Mobile Processor: Exynos 9820.” [Online]. Available: [https://www.samsung.com/semiconductor/minisite/exynos/products/mobileprocessor/exynos-9-series-9820/](https://www.samsung.com/semiconductor/minisite/exynos/products/mobileprocessor/exynos-9-series-9820/)'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] “移动处理器：Exynos 9820。” [在线]. 可用链接：[https://www.samsung.com/semiconductor/minisite/exynos/products/mobileprocessor/exynos-9-series-9820/](https://www.samsung.com/semiconductor/minisite/exynos/products/mobileprocessor/exynos-9-series-9820/)'
- en: '[41] Y. Xiong, Y. Sun, L. Xing, and Y. Huang, “Extend Cloud to Edge with KubeEdge,”
    in *Proc. 2018 IEEE/ACM Symposium on Edge Computing (SEC 2018)*, 2018, pp. 373–377.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Y. Xiong, Y. Sun, L. Xing, 和 Y. Huang, “通过KubeEdge将云扩展到边缘，”在 *2018 IEEE/ACM边缘计算研讨会（SEC
    2018）*，2018，第373–377页。'
- en: '[42] “OpenEdge, extend cloud computing, data and service seamlessly to edge
    devices.” [Online]. Available: [https://github.com/baidu/openedge](https://github.com/baidu/openedge)'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] “OpenEdge，将云计算、数据和服务无缝扩展到边缘设备。” [在线]. 可用链接：[https://github.com/baidu/openedge](https://github.com/baidu/openedge)'
- en: '[43] “Azure IoT Edge, extend cloud intelligence and analytics to edge devices.”
    [Online]. Available: [https://github.com/Azure/iotedge](https://github.com/Azure/iotedge)'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] “Azure IoT Edge，将云智能和分析扩展到边缘设备。” [在线]. 可用链接：[https://github.com/Azure/iotedge](https://github.com/Azure/iotedge)'
- en: '[44] “EdgeX, the Open Platform for the IoT Edge.” [Online]. Available: [https://www.edgexfoundry.org/](https://www.edgexfoundry.org/)'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] “EdgeX，面向物联网边缘的开放平台。” [在线]. 可用链接：[https://www.edgexfoundry.org/](https://www.edgexfoundry.org/)'
- en: '[45] “Akraino Edge Stack.” [Online]. Available: [https://www.lfedge.org/projects/akraino/](https://www.lfedge.org/projects/akraino/)'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] “Akraino Edge Stack。” [在线]. 可用链接：[https://www.lfedge.org/projects/akraino/](https://www.lfedge.org/projects/akraino/)'
- en: '[46] “NVIDIA EGX Edge Computing Platform: Real-Time AI at the Edge.” [Online].
    Available: [https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] “NVIDIA EGX边缘计算平台：边缘实时AI。” [在线]. 可用链接：[https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/](https://www.nvidia.com/en-us/data-center/products/egx-edge-computing/)'
- en: '[47] “AWS IoT Greengrass: Bring local compute, messaging, data caching, sync,
    and ML inference capabilities to edge devices.” [Online]. Available: [https://aws.amazon.com/greengrass/](https://aws.amazon.com/greengrass/)'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] “AWS IoT Greengrass：将本地计算、消息传递、数据缓存、同步和机器学习推理功能带到边缘设备。” [在线]. 可用链接：[https://aws.amazon.com/greengrass/](https://aws.amazon.com/greengrass/)'
- en: '[48] “Google Cloud IoT: Unlock business insights from your global device network
    with an intelligent IoT platform.” [Online]. Available: [https://cloud.google.com/solutions/iot/](https://cloud.google.com/solutions/iot/)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] “Google Cloud IoT：通过智能物联网平台从全球设备网络中获取商业洞察。” [在线]. 可用链接：[https://cloud.google.com/solutions/iot/](https://cloud.google.com/solutions/iot/)'
- en: '[49] G. Li, L. Liu, X. Wang *et al.*, “Auto-tuning Neural Network Quantization
    Framework for Collaborative Inference Between the Cloud and Edge,” in *Proc. International
    Conference on Artificial Neural Networks (ICANN 2018)*, 2018, pp. 402–411.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] G. Li, L. Liu, X. Wang *等*, “用于云与边缘协作推理的神经网络量化框架自动调优，”在 *国际人工神经网络会议（ICANN
    2018）*，2018，第402–411页。'
- en: '[50] Y. Huang, Y. Zhu, X. Fan *et al.*, “Task Scheduling with Optimized Transmission
    Time in Collaborative Cloud-Edge Learning,” in *Proc. 27th International Conference
    on Computer Communication and Networks (ICCCN 2018)*, 2018, pp. 1–9.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Y. Huang, Y. Zhu, X. Fan *等*, “在协作云边学习中优化传输时间的任务调度，”在 *第27届国际计算机通信与网络会议（ICCCN
    2018）*，2018，第1–9页。'
- en: '[51] E. Nurvitadhi, G. Venkatesh, J. Sim *et al.*, “Can fpgas beat gpus in
    accelerating next-generation deep neural networks?” in *Proc. ACM/SIGDA International
    Symposium on Field-Programmable Gate Arrays (FPGA 2017)*, 2017, pp. 5–14.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] E. Nurvitadhi, G. Venkatesh, J. Sim *等*, “FPGA能否在加速下一代深度神经网络方面超越GPU？”在
    *ACM/SIGDA国际现场可编程门阵列研讨会（FPGA 2017）*，2017，第5–14页。'
- en: '[52] S. Jiang, D. He, C. Yang *et al.*, “Accelerating Mobile Applications at
    the Network Edge with Software-Programmable FPGAs,” in *2018 IEEE Conference on
    Computer Communications (INFOCOM 2018)*, 2018, pp. 55–62.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Jiang, D. He, C. Yang *等*, “使用软件可编程FPGA加速网络边缘的移动应用程序，”在 *2018 IEEE计算机通信会议（INFOCOM
    2018）*，2018，第55–62页。'
- en: '[53] “Qualcomm Neural Processing SDK for AI.” [Online]. Available: [https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] “Qualcomm 神经处理 SDK for AI。” [在线]. 可用: [https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)'
- en: '[54] A. Ignatov, R. Timofte, W. Chou *et al.*, “AI Benchmark: Running Deep
    Neural Networks on Android Smartphones,” *arXiv preprint arXiv:1810.01109*.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Ignatov, R. Timofte, W. Chou *等人*，“AI Benchmark: 在 Android 智能手机上运行深度神经网络”，*arXiv
    预印本 arXiv:1810.01109*。'
- en: '[55] D. Bernstein, “Containers and cloud: From lxc to docker to kubernetes,”
    *IEEE Cloud Comput.*, vol. 1, no. 3, pp. 81–84, Sep. 2014.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] D. Bernstein，“容器和云：从 lxc 到 docker 再到 kubernetes”，*IEEE 云计算*，第1卷，第3期，pp.
    81–84，2014年9月。'
- en: '[56] “Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit.”
    [Online]. Available: [https://github.com/microsoft/CNTK](https://github.com/microsoft/CNTK)'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] “Microsoft 认知工具包 (CNTK)，一个开源深度学习工具包。” [在线]. 可用: [https://github.com/microsoft/CNTK](https://github.com/microsoft/CNTK)'
- en: '[57] S. Tokui, K. Oono *et al.*, “Chainer: a next-generation open source framework
    for deep learning,” in *Proc. workshop on machine learning systems (LearningSys)
    in the twenty-ninth annual conference on neural information processing systems
    (NeurIPS 2015)*, 2015, pp. 1–6.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Tokui, K. Oono *等人*，“Chainer: 下一代开源深度学习框架”，见于 *第29届神经信息处理系统年会 (NeurIPS
    2015) 机器学习系统研讨会 (LearningSys) 论文集*，2015年，pp. 1–6。'
- en: '[58] M. Abadi, P. Barham *et al.*, “TensorFlow: A System for Large-Scale Machine
    Learning,” in *Proc. the 12th USENIX conference on Operating Systems Design and
    Implementation (OSDI 2016)*, 2016, pp. 265–283.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Abadi, P. Barham *等人*，“TensorFlow: 一种大规模机器学习系统”，见于 *第十二届 USENIX 操作系统设计与实现会议
    (OSDI 2016) 论文集*，2016年，pp. 265–283。'
- en: '[59] “Deeplearning4j: Open-source distributed deep learning for the JVM, Apache
    Software Foundation License 2.0.” [Online]. Available: [https://deeplearning4j.org](https://deeplearning4j.org)'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] “Deeplearning4j: JVM 上的开源分布式深度学习，Apache 软件基金会许可 2.0。” [在线]. 可用: [https://deeplearning4j.org](https://deeplearning4j.org)'
- en: '[60] “Deploy machine learning models on mobile and IoT devices.” [Online].
    Available: [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] “在移动和 IoT 设备上部署机器学习模型。” [在线]. 可用: [https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)'
- en: '[61] T. Chen, M. Li, Y. Li *et al.*, “MXNet: A Flexible and Efficient Machine
    Learning Library for Heterogeneous Distributed Systems,” *arXiv preprint arXiv:1512.01274*,
    2015.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] T. Chen, M. Li, Y. Li *等人*，“MXNet: 一个灵活高效的异构分布式系统机器学习库”，*arXiv 预印本 arXiv:1512.01274*，2015年。'
- en: '[62] “PyTorch: tensors and dynamic neural networks in Python with strong GPU
    acceleration.” [Online]. Available: [https://github.com/pytorch/](https://github.com/pytorch/)'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] “PyTorch: Python 中的张量和动态神经网络，具有强大的 GPU 加速。” [在线]. 可用: [https://github.com/pytorch/](https://github.com/pytorch/)'
- en: '[63] “Core ML: Integrate machine learning models into your app.” [Online].
    Available: [https://developer.apple.com/documentation/coreml?language=objc](https://developer.apple.com/documentation/coreml?language=objc)'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] “Core ML: 将机器学习模型集成到你的应用中。” [在线]. 可用: [https://developer.apple.com/documentation/coreml?language=objc](https://developer.apple.com/documentation/coreml?language=objc)'
- en: '[64] “NCNN is a high-performance neural network inference framework optimized
    for the mobile platform.” [Online]. Available: [https://github.com/Tencent/ncnn](https://github.com/Tencent/ncnn)'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] “NCNN 是一个针对移动平台优化的高性能神经网络推理框架。” [在线]. 可用: [https://github.com/Tencent/ncnn](https://github.com/Tencent/ncnn)'
- en: '[65] “MNN is a lightweight deep neural network inference engine.” [Online].
    Available: [https://github.com/alibaba/MNN](https://github.com/alibaba/MNN)'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] “MNN 是一个轻量级深度神经网络推理引擎。” [在线]. 可用: [https://github.com/alibaba/MNN](https://github.com/alibaba/MNN)'
- en: '[66] “Multi-platform embedded deep learning framework.” [Online]. Available:
    [https://github.com/PaddlePaddle/paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] “多平台嵌入式深度学习框架。” [在线]. 可用: [https://github.com/PaddlePaddle/paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)'
- en: '[67] “MACE is a deep learning inference framework optimized for mobile heterogeneous
    computing platforms.” [Online]. Available: [https://github.com/XiaoMi/mace](https://github.com/XiaoMi/mace)'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] “MACE 是一个针对移动异构计算平台优化的深度学习推理框架。” [在线]. 可用: [https://github.com/XiaoMi/mace](https://github.com/XiaoMi/mace)'
- en: '[68] X. Wang, M. Magno, L. Cavigelli, and L. Benini, “FANN-on-MCU: An Open-Source
    Toolkit for Energy-Efficient Neural Network Inference at the Edge of the Internet
    of Things,” *arXiv preprint arXiv:1911.03314*, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Wang, M. Magno, L. Cavigelli 和 L. Benini，“FANN-on-MCU: 一种用于物联网边缘节能神经网络推理的开源工具包”，*arXiv
    预印本 arXiv:1911.03314*，2019年。'
- en: '[69] Z. Tao, Q. Xia, Z. Hao, C. Li, L. Ma, S. Yi, and Q. Li, “A Survey of Virtual
    Machine Management in Edge Computing,” *Proc. IEEE*, vol. 107, no. 8, pp. 1482–1499,
    2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Z. Tao, Q. Xia, Z. Hao, C. Li, L. Ma, S. Yi, 和 Q. Li, “边缘计算中虚拟机管理的调查”，*Proc.
    IEEE*，第 107 卷，第 8 期，页码 1482–1499，2019 年。'
- en: '[70] R. Morabito, “Virtualization on internet of things edge devices with container
    technologies: A performance evaluation,” *IEEE Access*, vol. 5, pp. 8835–8850,
    2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] R. Morabito, “基于容器技术的物联网边缘设备虚拟化：性能评估”，*IEEE Access*，第 5 卷，页码 8835–8850，2017
    年。'
- en: '[71] L. Ma, S. Yi, N. Carter, and Q. Li, “Efficient Live Migration of Edge
    Services Leveraging Container Layered Storage,” *IEEE Trans. Mob. Comput.*, vol. 18,
    no. 9, pp. 2020–2033, Sep. 2019.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] L. Ma, S. Yi, N. Carter, 和 Q. Li, “利用容器分层存储的边缘服务高效迁移”，*IEEE Trans. Mob.
    Comput.*，第 18 卷，第 9 期，页码 2020–2033，2019 年 9 月。'
- en: '[72] A. Wang, Z. Zha, Y. Guo, and S. Chen, “Software-Defined Networking Enhanced
    Edge Computing: A Network-Centric Survey,” *Proc. IEEE*, vol. 107, no. 8, pp.
    1500–1519, Aug. 2019.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Wang, Z. Zha, Y. Guo, 和 S. Chen, “增强边缘计算的软定义网络：网络中心调查”，*Proc. IEEE*，第
    107 卷，第 8 期，页码 1500–1519，2019 年 8 月。'
- en: '[73] Y. D. Lin, C. C. Wang, C. Y. Huang, and Y. C. Lai, “Hierarchical CORD
    for NFV Datacenters: Resource Allocation with Cost-Latency Tradeoff,” *IEEE Netw.*,
    vol. 32, no. 5, pp. 124–130, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. D. Lin, C. C. Wang, C. Y. Huang, 和 Y. C. Lai, “用于 NFV 数据中心的分层 CORD：具有成本-延迟权衡的资源分配”，*IEEE
    Netw.*，第 32 卷，第 5 期，页码 124–130，2018 年。'
- en: '[74] L. Li, K. Ota, and M. Dong, “DeepNFV: A Lightweight Framework for Intelligent
    Edge Network Functions Virtualization,” *IEEE Netw.*, vol. 33, no. 1, pp. 136–141,
    Jan. 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. Li, K. Ota, 和 M. Dong, “DeepNFV：一种轻量级智能边缘网络功能虚拟化框架”，*IEEE Netw.*，第
    33 卷，第 1 期，页码 136–141，2019 年 1 月。'
- en: '[75] “Mobile Edge Computing A key technology towards 5G,” ETSI. [Online]. Available:
    [https://www.etsi.org/images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf](https://www.etsi.org/images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf)'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] “移动边缘计算：通向 5G 的关键技术，” ETSI。 [在线]. 可用： [https://www.etsi.org/images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf](https://www.etsi.org/images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf)'
- en: '[76] H.-T. Chien, Y.-D. Lin, C.-L. Lai, and C.-T. Wang, “End-to-End Slicing
    as a Service with Computing and Communication Resource Allocation for Multi-Tenant
    5G Systems,” *IEEE Wirel. Commun.*, vol. 26, no. 5, pp. 104–112, Oct. 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] H.-T. Chien, Y.-D. Lin, C.-L. Lai, 和 C.-T. Wang, “端到端切片即服务：为多租户 5G 系统提供计算和通信资源分配”，*IEEE
    Wirel. Commun.*，第 26 卷，第 5 期，页码 104–112，2019 年 10 月。'
- en: '[77] T. Taleb, K. Samdanis, B. Mada, H. Flinck, S. Dutta, and D. Sabella, “On
    Multi-Access Edge Computing: A Survey of the Emerging 5G Network Edge Cloud Architecture
    and Orchestration,” *IEEE Commun. Surv. Tutor.*, vol. 19, no. 3, pp. 1657–1681,
    2017.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] T. Taleb, K. Samdanis, B. Mada, H. Flinck, S. Dutta, 和 D. Sabella, “关于多接入边缘计算：新兴
    5G 网络边缘云架构和编排的调查”，*IEEE Commun. Surv. Tutor.*，第 19 卷，第 3 期，页码 1657–1681，2017 年。'
- en: '[78] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, May 2015.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习”，*Nature*，第 521 卷，第 7553 期，页码
    436–444，2015 年 5 月。'
- en: '[79] S. S. Haykin and K. Elektroingenieur, *Neural networks and learning machines*.   Pearson
    Prentice Hall, 2009.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. S. Haykin 和 K. Elektroingenieur, *神经网络与学习机器*。 Pearson Prentice Hall，2009
    年。'
- en: '[80] R. Collobert and S. Bengio, “Links between perceptrons, MLPs and SVMs,”
    in *Proc. the Twenty-first international conference on Machine learning (ICML
    2004)*, 2004, p. 23.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Collobert 和 S. Bengio, “感知机、MLP 和 SVM 之间的联系”，收录于 *第二十一届国际机器学习会议（ICML
    2004）*，2004 年，第 23 页。'
- en: '[81] C. D. Manning, C. D. Manning, and H. Schütze, *Foundations of statistical
    natural language processing*.   MIT press, 1999.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. D. Manning, C. D. Manning, 和 H. Schütze, *统计自然语言处理基础*。 MIT Press，1999
    年。'
- en: '[82] M. D. Zeiler and R. Fergus, “Visualizing and Understanding Convolutional
    Networks,” in *2014 European Conference on Computer Vision (ECCV 2014)*, 2014,
    pp. 818–833.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] M. D. Zeiler 和 R. Fergus, “可视化和理解卷积网络”，收录于 *2014 年欧洲计算机视觉会议（ECCV 2014）*，2014
    年，页码 818–833。'
- en: '[83] I. Goodfellow, J. Pouget-Abadie, M. Mirza *et al.*, “Generative adversarial
    nets,” in *Advances in Neural Information Processing Systems 27 (NeurIPS 2014)*,
    2014, pp. 2672–2680.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] I. Goodfellow, J. Pouget-Abadie, M. Mirza *等*， “生成对抗网络”，收录于 *神经信息处理系统进展
    27（NeurIPS 2014）*，2014 年，页码 2672–2680。'
- en: '[84] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, Nov. 1997.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] S. Hochreiter 和 J. Schmidhuber, “长短期记忆”，*Neural Computation*，第 9 卷，第 8
    期，页码 1735–1780，1997 年 11 月。'
- en: '[85] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Trans. Knowl.
    Data Eng.*, vol. 22, no. 10, pp. 1345–1359, Oct. 2010.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. J. Pan 和 Q. Yang, “转移学习综述”，*IEEE Trans. Knowl. Data Eng.*，第 22 卷，第
    10 期，页码 1345–1359，2010 年 10 月。'
- en: '[86] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] G. Hinton, O. Vinyals 和 J. Dean，“在神经网络中提取知识，”*arXiv 预印本 arXiv:1503.02531*，2015年。'
- en: '[87] S. S. Mousavi, M. Schukat, and E. Howley, “Deep Reinforcement Learning:
    An Overview,” in *Proc. the 2016 SAI Intelligent Systems Conference (IntelliSys
    2016)*, 2016, pp. 426–440.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. S. Mousavi, M. Schukat 和 E. Howley，“深度强化学习：概述，”发表于*2016年SAI智能系统会议（IntelliSys
    2016）*，2016年，第426–440页。'
- en: '[88] V. Mnih, K. Kavukcuoglu, D. Silver *et al.*, “Human-level control through
    deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp. 529–533, Feb.
    2015.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. Mnih, K. Kavukcuoglu, D. Silver *等*，“通过深度强化学习实现人类级控制，”*Nature*，第518卷，第7540期，第529–533页，2015年2月。'
- en: '[89] H. Van Hasselt, A. Guez, and D. Silver, “Deep Reinforcement Learning with
    Double Q-Learning,” in *Proc. the Thirtieth AAAI Conference on Artificial Intelligence
    (AAAI 2016)*, 2016, pp. 2094–2100.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Van Hasselt, A. Guez 和 D. Silver，“使用双重 Q 学习的深度强化学习，”发表于*第三十届美国人工智能协会会议（AAAI
    2016）*，2016年，第2094–2100页。'
- en: '[90] Z. Wang, T. Schaul, M. Hessel *et al.*, “Dueling network architectures
    for deep reinforcement learning,” in *Proc. the 33rd International Conference
    on Machine Learning (ICML 2016)*, 2016, pp. 1995–2003.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Z. Wang, T. Schaul, M. Hessel *等*，“深度强化学习的对抗网络架构，”发表于*第33届国际机器学习会议（ICML
    2016）*，2016年，第1995–2003页。'
- en: '[91] T. P. Lillicrap, J. J. Hunt, A. Pritzel *et al.*, “Continuous control
    with deep reinforcement learning,” in *Proc. the 6th International Conference
    on Learning Representations (ICLR 2016)*, 2016.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] T. P. Lillicrap, J. J. Hunt, A. Pritzel *等*，“利用深度强化学习进行连续控制，”发表于*第6届国际学习表征会议（ICLR
    2016）*，2016年。'
- en: '[92] V. Mnih, A. P. Badia, M. Mirza *et al.*, “Asynchronous Methods for Deep
    Reinforcement Learning,” in *Proc. the 33rd International Conference on Machine
    Learning (ICML 2016)*, 2016, pp. 1928–1937.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] V. Mnih, A. P. Badia, M. Mirza *等*，“深度强化学习的异步方法，”发表于*第33届国际机器学习会议（ICML
    2016）*，2016年，第1928–1937页。'
- en: '[93] J. Schulman, F. Wolski, P. Dhariwal *et al.*, “Proximal policy optimization
    algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Schulman, F. Wolski, P. Dhariwal *等*，“近端策略优化算法，”*arXiv 预印本 arXiv:1707.06347*，2017年。'
- en: '[94] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient
    methods for reinforcement learning with function approximation,” in *Proc. the
    12th International Conference on Neural Information Processing Systems (NeurIPS
    1999)*, 1999, pp. 1057–1063.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] R. S. Sutton, D. McAllester, S. Singh 和 Y. Mansour，“具有函数逼近的强化学习的策略梯度方法，”发表于*第12届神经信息处理系统国际会议（NeurIPS
    1999）*，1999年，第1057–1063页。'
- en: '[95] Monin and Yaglom, “Large Scale Distributed Deep Networks,” in *Proc. Advances
    in Neural Information Processing Systems 25 (NeurIPS 2012)*, 2012, pp. 1223–1231.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Monin 和 Yaglom，“大规模分布式深度网络，”发表于*神经信息处理系统进展第25卷（NeurIPS 2012）*，2012年，第1223–1231页。'
- en: '[96] Y. Zou, X. Jin, Y. Li *et al.*, “Mariana: Tencent deep learning platform
    and its applications,” in *Proc. VLDB Endow.*, vol. 7, no. 13, 2014, pp. 1772–1777.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Y. Zou, X. Jin, Y. Li *等*，“Mariana：腾讯深度学习平台及其应用，”发表于*VLDB 期刊*，第7卷，第13期，2014年，第1772–1777页。'
- en: '[97] X. Chen, A. Eversole, G. Li *et al.*, “Pipelined Back-Propagation for
    Context-Dependent Deep Neural Networks,” in *13th Annual Conference of the International
    Speech Communication Association (INTERSPEECH 2012)*, 2012, pp. 26–29.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] X. Chen, A. Eversole, G. Li *等*，“用于上下文相关深度神经网络的管道反向传播，”发表于*第13届国际语音通信协会年会（INTERSPEECH
    2012）*，2012年，第26–29页。'
- en: '[98] M. Stevenson, R. Winter *et al.*, “1-Bit Stochastic Gradient Descent and
    its Application to Data-Parallel Distributed Training of Speech DNNs,” in *15th
    Annual Conference of the International Speech Communication Association (INTERSPEECH
    2014)*, 2014, pp. 1058–1062.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Stevenson, R. Winter *等*，“1位随机梯度下降及其在数据并行分布式语音深度神经网络训练中的应用，”发表于*第15届国际语音通信协会年会（INTERSPEECH
    2014）*，2014年，第1058–1062页。'
- en: '[99] A. Coates, B. Huval, T. Wang *et al.*, “Deep learning with cots hpc systems,”
    in *Proc. the 30th International Conference on Machine Learning (PMLR 2013)*,
    2013, pp. 1337–1345.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Coates, B. Huval, T. Wang *等*，“使用商用高性能计算系统的深度学习，”发表于*第30届国际机器学习会议（PMLR
    2013）*，2013年，第1337–1345页。'
- en: '[100] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan, “SparkNet: Training
    Deep Networks in Spark,” *arXiv preprint arXiv:1511.06051*, 2015.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Moritz, R. Nishihara, I. Stoica 和 M. I. Jordan，“SparkNet：在 Spark 中训练深度网络，”*arXiv
    预印本 arXiv:1511.06051*，2015年。'
- en: '[101] “Theano is a Python library that allows you to define, optimize, and
    evaluate mathematical expressions involving multi-dimensional arrays efficiently.”
    [Online]. Available: [https://github.com/Theano/Theano](https://github.com/Theano/Theano)'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] “Theano 是一个Python库，可以高效地定义、优化和评估涉及多维数组的数学表达式。” [在线]. 可用: [https://github.com/Theano/Theano](https://github.com/Theano/Theano)'
- en: '[102] J. Ren, Y. Guo, D. Zhang *et al.*, “Distributed and Efficient Object
    Detection in Edge Computing: Challenges and Solutions,” *IEEE Netw.*, vol. 32,
    no. 6, pp. 137–143, Nov. 2018.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Ren, Y. Guo, D. Zhang *等*, “边缘计算中的分布式高效目标检测: 挑战与解决方案，” *IEEE 网络*,
    卷32, 期6, 第137–143页, 2018年11月。'
- en: '[103] C. Liu, Y. Cao, Y. Luo *et al.*, “A New Deep Learning-Based Food Recognition
    System for Dietary Assessment on An Edge Computing Service Infrastructure,” *IEEE
    Trans. Serv. Comput.*, vol. 11, no. 2, pp. 249–261, Mar. 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Liu, Y. Cao, Y. Luo *等*, “基于深度学习的食品识别系统用于边缘计算服务基础设施上的饮食评估，” *IEEE
    服务计算汇刊*, 卷11, 期2, 第249–261页, 2018年3月。'
- en: '[104] D. Li, T. Salonidis, N. V. Desai, and M. C. Chuah, “DeepCham: Collaborative
    Edge-Mediated Adaptive Deep Learning for Mobile Object Recognition,” in *Proc.
    the First ACM/IEEE Symposium on Edge Computing (SEC 2016)*, 2016, pp. 64–76.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] D. Li, T. Salonidis, N. V. Desai, 和 M. C. Chuah, “DeepCham: 协作边缘中介的自适应深度学习用于移动物体识别，”发表于
    *第一届ACM/IEEE边缘计算研讨会 (SEC 2016)*, 2016年, 第64–76页。'
- en: '[105] B. Fang, X. Zeng, and M. Zhang, “NestDNN: Resource-Aware Multi-Tenant
    On-Device Deep Learning for Continuous Mobile Vision,” in *Proc. the 24th Annual
    International Conference on Mobile Computing and Networking (MobiCom 2018)*, 2018,
    pp. 115–127.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] B. Fang, X. Zeng, 和 M. Zhang, “NestDNN: 资源感知的多租户设备深度学习用于连续移动视觉，”发表于 *第24届国际移动计算与网络会议
    (MobiCom 2018)*, 2018年, 第115–127页。'
- en: '[106] S. Yi, Z. Hao, Q. Zhang *et al.*, “LAVEA: Latency-aware Video Analytics
    on Edge Computing Platform,” in *Proc. the Second ACM/IEEE Symposium on Edge Computing
    (SEC 2017)*, 2017, pp. 1–13.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] S. Yi, Z. Hao, Q. Zhang *等*, “LAVEA: 基于延迟感知的边缘计算平台视频分析，”发表于 *第二届ACM/IEEE边缘计算研讨会
    (SEC 2017)*, 2017年, 第1–13页。'
- en: '[107] S. Y. Nikouei, Y. Chen, S. Song *et al.*, “Smart surveillance as an edge
    network service: From harr-cascade, svm to a lightweight cnn,” in *IEEE 4th International
    Conference on Collaboration and Internet Computing (CIC 2018)*, 2018, pp. 256–265.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] S. Y. Nikouei, Y. Chen, S. Song *等*, “作为边缘网络服务的智能监控: 从Haar级联、SVM到轻量级CNN，”发表于
    *IEEE 第四届国际协作与互联网计算会议 (CIC 2018)*, 2018年, 第256–265页。'
- en: '[108] P. Liu, B. Qi, and S. Banerjee, “EdgeEye - An Edge Service Framework
    for Real-time Intelligent Video Analytics,” in *Proc. the 1st International Workshop
    on Edge Systems, Analytics and Networking (EdgeSys 2018)*, 2018, pp. 1–6.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] P. Liu, B. Qi, 和 S. Banerjee, “EdgeEye - 实时智能视频分析的边缘服务框架，”发表于 *第1届国际边缘系统、分析与网络研讨会
    (EdgeSys 2018)*, 2018年, 第1–6页。'
- en: '[109] C.-C. Hung, G. Ananthanarayanan, P. Bodik, L. Golubchik, M. Yu, P. Bahl,
    and M. Philipose, “VideoEdge: Processing Camera Streams using Hierarchical Clusters,”
    in *Proc. 2018 IEEE/ACM Symposium on Edge Computing (SEC 2018)*, 2018, pp. 115–131.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C.-C. Hung, G. Ananthanarayanan, P. Bodik, L. Golubchik, M. Yu, P. Bahl,
    和 M. Philipose, “VideoEdge: 使用分层聚类处理摄像头流，”发表于 *2018 IEEE/ACM边缘计算研讨会 (SEC 2018)*,
    2018年, 第115–131页。'
- en: '[110] Y. He, N. Zhao *et al.*, “Integrated Networking, Caching, and Computing
    for Connected Vehicles: A Deep Reinforcement Learning Approach,” *IEEE Trans.
    Veh. Technol.*, vol. 67, no. 1, pp. 44–55, Jan. 2018.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. He, N. Zhao *等*, “连接车辆的集成网络、缓存和计算: 深度强化学习方法，” *IEEE 车辆技术汇刊*, 卷67,
    期1, 第44–55页, 2018年1月。'
- en: '[111] Q. Qi and Z. Ma, “Vehicular Edge Computing via Deep Reinforcement Learning,”
    *arXiv preprint arXiv:1901.04290*, 2018.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Q. Qi 和 Z. Ma, “通过深度强化学习进行车载边缘计算，” *arXiv 预印本 arXiv:1901.04290*, 2018年。'
- en: '[112] L. T. Tan and R. Q. Hu, “Mobility-Aware Edge Caching and Computing in
    Vehicle Networks: A Deep Reinforcement Learning,” *IEEE Trans. Veh. Technol.*,
    vol. 67, no. 11, pp. 10 190–10 203, Nov. 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] L. T. Tan 和 R. Q. Hu, “车载网络中的移动感知边缘缓存和计算: 深度强化学习，” *IEEE 车辆技术汇刊*, 卷67,
    期11, 第10 190–10 203页, 2018年11月。'
- en: '[113] L. Li, K. Ota, and M. Dong, “Deep Learning for Smart Industry: Efficient
    Manufacture Inspection System with Fog Computing,” *IEEE Trans. Ind. Inf.*, vol. 14,
    no. 10, pp. 4665–4673, 2018.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] L. Li, K. Ota, 和 M. Dong, “智能工业中的深度学习: 基于雾计算的高效制造检查系统，” *IEEE 工业信息汇刊*,
    卷14, 期10, 第4665–4673页, 2018年。'
- en: '[114] L. Hu, Y. Miao, G. Wu *et al.*, “iRobot-Factory: An intelligent robot
    factory based on cognitive manufacturing and edge computing,” *Future Gener. Comput.
    Syst.*, vol. 90, pp. 569–577, Jan. 2019.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] L. Hu, Y. Miao, G. Wu *等*，“iRobot-Factory：基于认知制造和边缘计算的智能机器人工厂”，*未来一代计算系统*，第90卷，第569–577页，2019年1月。'
- en: '[115] J. A. C. Soto, M. Jentsch *et al.*, “CEML: Mixing and moving complex
    event processing and machine learning to the edge of the network for IoT applications,”
    in *Proc. the 6th International Conference on the Internet of Things (IoT 2016)*,
    2016, pp. 103–110.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. A. C. Soto, M. Jentsch *等*，“CEML：将复杂事件处理和机器学习混合并移动到网络边缘以用于物联网应用”，见于
    *第6届国际物联网会议 (IoT 2016)*，2016年，第103–110页。'
- en: '[116] G. Plastiras, M. Terzi, C. Kyrkou, and T. Theocharidcs, “Edge Intelligence:
    Challenges and Opportunities of Near-Sensor Machine Learning Applications,” in
    *Proc. IEEE 29th International Conference on Application-specific Systems, Architectures
    and Processors (ASAP 2018)*, 2018, pp. 1–7.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] G. Plastiras, M. Terzi, C. Kyrkou, 和 T. Theocharidcs，“边缘智能：近传感器机器学习应用的挑战与机遇”，见于
    *IEEE 第29届应用特定系统、架构和处理器国际会议 (ASAP 2018)*，2018年，第1–7页。'
- en: '[117] Y. Hao, Y. Miao, Y. Tian *et al.*, “Smart-Edge-CoCaCo: AI-Enabled Smart
    Edge with Joint Computation, Caching, and Communication in Heterogeneous IoT,”
    *arXiv preprint arXiv:1901.02126*, 2019.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Y. Hao, Y. Miao, Y. Tian *等*，“Smart-Edge-CoCaCo：具有联合计算、缓存和通信的异构物联网AI智能边缘”，*arXiv
    预印本 arXiv:1901.02126*，2019年。'
- en: '[118] S. Liu, P. Si, M. Xu *et al.*, “Edge Big Data-Enabled Low-Cost Indoor
    Localization Based on Bayesian Analysis of RSS,” in *Proc. 2017 IEEE Wireless
    Communications and Networking Conference (WCNC 2017)*, 2017, pp. 1–6.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. Liu, P. Si, M. Xu *等*，“基于RSS贝叶斯分析的边缘大数据支持的低成本室内定位”，见于 *2017 IEEE 无线通信与网络会议
    (WCNC 2017)*，2017年，第1–6页。'
- en: '[119] A. Dhakal *et al.*, “Machine learning at the network edge for automated
    home intrusion monitoring,” in *Proc. IEEE 25th International Conference on Network
    Protocols (ICNP 2017)*, 2017, pp. 1–6.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] A. Dhakal *等*，“用于自动化家庭入侵监测的网络边缘机器学习”，见于 *IEEE 第25届国际网络协议会议 (ICNP 2017)*，2017年，第1–6页。'
- en: '[120] N. Tian, J. Chen, M. Ma *et al.*, “A Fog Robotic System for Dynamic Visual
    Servoing,” *arXiv preprint arXiv:1809.06716*, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] N. Tian, J. Chen, M. Ma *等*，“用于动态视觉伺服的雾机器人系统”，*arXiv 预印本 arXiv:1809.06716*，2018年。'
- en: '[121] L. Lu, L. Xu, B. Xu *et al.*, “Fog Computing Approach for Music Cognition
    System Based on Machine Learning Algorithm,” *IEEE Trans. Comput. Social Syst.*,
    vol. 5, no. 4, pp. 1142–1151, Dec. 2018.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] L. Lu, L. Xu, B. Xu *等*，“基于机器学习算法的音乐认知系统的雾计算方法”，*IEEE 计算社会系统汇刊*，第5卷，第4期，第1142–1151页，2018年12月。'
- en: '[122] B. Tang, Z. Chen, G. Hefferman *et al.*, “Incorporating Intelligence
    in Fog Computing for Big Data Analysis in Smart Cities,” *IEEE Trans. Ind. Inf.*,
    vol. 13, no. 5, pp. 2140–2150, Oct. 2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] B. Tang, Z. Chen, G. Hefferman *等*，“在智能城市的大数据分析中融合雾计算的智能”，*IEEE 工业信息学汇刊*，第13卷，第5期，第2140–2150页，2017年10月。'
- en: '[123] Y.-C. Chang and Y.-H. Lai, “Campus Edge Computing Network Based on IoT
    Street Lighting Nodes,” *IEEE Syst. J. (Early Access)*, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Y.-C. Chang 和 Y.-H. Lai，“基于物联网街道照明节点的校园边缘计算网络”，*IEEE 系统汇刊 (早期访问)*，2018年。'
- en: '[124] E. Denton *et al.*, “Exploiting Linear Structure Within Convolutional
    Networks for Efficient Evaluation,” in *Advances in Neural Information Processing
    Systems 27 (NeurIPS 2014)*, 2014, pp. 1269–1277.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] E. Denton *等*，“利用卷积网络中的线性结构进行高效评估”，见于 *神经信息处理系统进展第27卷 (NeurIPS 2014)*，2014年，第1269–1277页。'
- en: '[125] W. Chen, J. Wilson, S. Tyree *et al.*, “Compressing Neural Networks with
    the Hashing Trick,” in *Proc. the 32nd International Conference on International
    Conference on Machine Learning (ICML 2015)*, 2015, pp. 2285–2294.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] W. Chen, J. Wilson, S. Tyree *等*，“使用哈希技巧压缩神经网络”，见于 *第32届国际机器学习会议 (ICML
    2015)*，2015年，第2285–2294页。'
- en: '[126] C. Szegedy, Wei Liu, Yangqing Jia *et al.*, “Going deeper with convolutions,”
    in *2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015)*,
    2015, pp. 1–9.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] C. Szegedy, Wei Liu, Yangqing Jia *等*，“通过卷积深入”，见于 *2015 IEEE 计算机视觉与模式识别会议
    (CVPR 2015)*，2015年，第1–9页。'
- en: '[127] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
    Recognition,” in *2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR 2016)*, 2016, pp. 770–778.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. He, X. Zhang, S. Ren, 和 J. Sun，“深度残差学习用于图像识别”，见于 *2016 IEEE 计算机视觉与模式识别会议
    (CVPR 2016)*，2016年，第770–778页。'
- en: '[128] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A Survey of Model Compression
    and Acceleration for Deep Neural Networks,” *arXiv preprint arXiv:1710.09282*,
    2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Cheng, D. Wang, P. Zhou, 和 T. Zhang, “深度神经网络模型压缩与加速综述，” *arXiv预印本
    arXiv:1710.09282*，2017。'
- en: '[129] S. Han, J. Pool, J. Tran *et al.*, “Learning both Weights and Connections
    for Efficient Neural Networks,” in *Advances in Neural Information Processing
    Systems 28 (NeurIPS 2015)*, 2015, pp. 1135–1143.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] S. Han, J. Pool, J. Tran *等人*, “学习权重和连接以提高神经网络效率，” 见于 *第28届神经信息处理系统大会（NeurIPS
    2015）论文集*，2015，第1135–1143页。'
- en: '[130] M. Alwani, H. Chen, M. Ferdman, and P. Milder, “Fused-layer CNN accelerators,”
    in *49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2016)*,
    2016, pp. 1–12.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] M. Alwani, H. Chen, M. Ferdman, 和 P. Milder, “融合层CNN加速器，” 见于 *第49届年度IEEE/ACM国际微架构研讨会（MICRO
    2016）*，2016，第1–12页。'
- en: '[131] M. Courbariaux, Y. Bengio, and J.-P. David, “BinaryConnect: Training
    Deep Neural Networks with binary weights during propagations,” in *Advances in
    Neural Information Processing Systems 28 (NeurIPS 2015)*, 2015, pp. 3123–3131.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Courbariaux, Y. Bengio, 和 J.-P. David, “BinaryConnect: 在传播过程中使用二值权重训练深度神经网络，”
    见于 *第28届神经信息处理系统大会（NeurIPS 2015）论文集*，2015，第3123–3131页。'
- en: '[132] M. Rastegari, V. Ordonez *et al.*, “XNOR-Net: ImageNet Classification
    Using Binary Convolutional Neural Networks,” in *2018 European Conference on Computer
    Vision (ECCV 2016)*, 2016, pp. 525–542.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] M. Rastegari, V. Ordonez *等人*, “XNOR-Net: 使用二值卷积神经网络进行ImageNet分类，” 见于
    *2018年欧洲计算机视觉大会（ECCV 2016）*，2016，第525–542页。'
- en: '[133] B. Mcdanel, “Embedded Binarized Neural Networks,” in *Proc. the 2017
    International Conference on Embedded Wireless Systems and Networks (EWSN 2017)*,
    2017, pp. 168–173.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] B. Mcdanel, “嵌入式二值神经网络，” 见于 *2017国际嵌入式无线系统与网络会议（EWSN 2017）论文集*，2017，第168–173页。'
- en: '[134] F. N. Iandola, S. Han, M. W. Moskewicz *et al.*, “Squeezenet: Alexnet-level
    Accuracy with 50x Fewer Parameters and < 0.5 MB Model Size,” *arXiv preprint arXiv:1602.07360*,
    2016.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] F. N. Iandola, S. Han, M. W. Moskewicz *等人*, “Squeezenet: 以50倍更少参数和<
    0.5 MB模型大小实现与Alexnet相当的准确性，” *arXiv预印本 arXiv:1602.07360*，2016。'
- en: '[135] A. G. Howard, M. Zhu, B. Chen *et al.*, “MobileNets: Efficient Convolutional
    Neural Networks for Mobile Vision Applications,” *arXiv preprint arXiv:1704.04861*,
    2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. G. Howard, M. Zhu, B. Chen *等人*, “MobileNets: 用于移动视觉应用的高效卷积神经网络，”
    *arXiv预印本 arXiv:1704.04861*，2017。'
- en: '[136] R. Sharma, S. Biookaghazadeh *et al.*, “Are Existing Knowledge Transfer
    Techniques Effective For Deep Learning on Edge Devices?” in *Proc. the 27th International
    Symposium on High-Performance Parallel and Distributed Computing (HPDC 2018)*,
    2018, pp. 15–16.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] R. Sharma, S. Biookaghazadeh *等人*, “现有的知识转移技术对边缘设备上的深度学习有效吗？” 见于 *第27届高性能并行与分布式计算国际研讨会（HPDC
    2018）*，2018，第15–16页。'
- en: '[137] C. Zhang, Q. Cao, H. Jiang *et al.*, “FFS-VA: A Fast Filtering System
    for Large-scale Video Analytics,” in *Proc. the 47th International Conference
    on Parallel Processing (ICPP 2018)*, 2018, pp. 1–10.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] C. Zhang, Q. Cao, H. Jiang *等人*, “FFS-VA: 一种用于大规模视频分析的快速过滤系统，” 见于 *第47届国际并行处理大会（ICPP
    2018）论文集*，2018，第1–10页。'
- en: '[138] J. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, and I. Stoica, “Chameleon:
    Scalable adaptation of video analytics,” in *Proc. the 2018 Conference of the
    ACM Special Interest Group on Data Communication (SIGCOMM 2018)*, 2018, pp. 253–266.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J. Jiang, G. Ananthanarayanan, P. Bodik, S. Sen, 和 I. Stoica, “Chameleon:
    可扩展的视频分析适应系统，” 见于 *2018年ACM特殊兴趣组数据通信大会（SIGCOMM 2018）论文集*，2018，第253–266页。'
- en: '[139] S. Y. Nikouei *et al.*, “Real-time human detection as an edge service
    enabled by a lightweight cnn,” in *2018 IEEE International Conference on Edge
    Computing (IEEE EDGE 2018)*, 2018, pp. 125–129.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] S. Y. Nikouei *等人*, “作为边缘服务的实时人体检测，由轻量级cnn实现，” 见于 *2018 IEEE国际边缘计算大会（IEEE
    EDGE 2018）*，2018，第125–129页。'
- en: '[140] L. Liu, H. Li, and M. Gruteser, “Edge Assisted Real-time Object Detection
    for Mobile Augmented Reality,” in *Proc. the 25th Annual International Conference
    on Mobile Computing and Networking (MobiCom 2019)*, 2019, pp. 1–16.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] L. Liu, H. Li, 和 M. Gruteser, “边缘辅助的实时移动增强现实目标检测，” 见于 *第25届年度国际移动计算与网络会议（MobiCom
    2019）论文集*，2019，第1–16页。'
- en: '[141] Fox, “Homer simpson.” [Online]. Available: [https://simpsons.fandom.com/wiki/File:Homer_Simpson.svg](https://simpsons.fandom.com/wiki/File:Homer_Simpson.svg)'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Fox, “Homer Simpson。” [在线]. 可用： [https://simpsons.fandom.com/wiki/File:Homer_Simpson.svg](https://simpsons.fandom.com/wiki/File:Homer_Simpson.svg)'
- en: '[142] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient
    convolutional neural network for mobile devices,” in *2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR 2018)*, 2018, pp. 6848–6856.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] X. Zhang, X. Zhou, M. Lin 和 J. Sun, “Shufflenet: 一种用于移动设备的极高效卷积神经网络”，见
    *2018 IEEE/CVF计算机视觉与模式识别会议（CVPR 2018）论文集*，2018，第6848–6856页。'
- en: '[143] L. Du *et al.*, “A Reconfigurable Streaming Deep Convolutional Neural
    Network Accelerator for Internet of Things,” *IEEE Trans. Circuits Syst. I Regul.
    Pap.*, vol. 65, no. 1, pp. 198–208, Jan. 2018.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] L. Du *等人*, “用于物联网的可重配置流式深度卷积神经网络加速器”，*IEEE电路与系统学报I:正规论文*，第65卷，第1期，第198–208页，2018年1月。'
- en: '[144] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Zaharia, “NoScope:
    Optimizing Neural Network Queries over Video at Scale,” *Proceedings of the VLDB
    Endowment*, vol. 10, no. 11, pp. 1586–1597, Aug. 2017.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] D. Kang, J. Emmons, F. Abuzaid, P. Bailis 和 M. Zaharia, “NoScope: 在大规模视频中优化神经网络查询”，*VLDB
    纪要*，第10卷，第11期，第1586–1597页，2017年8月。'
- en: '[145] S. Han, Y. Wang, H. Yang *et al.*, “ESE: Efficient Speech Recognition
    Engine with Sparse LSTM on FPGA,” in *Proc. the 2017 ACM/SIGDA International Symposium
    on Field-Programmable Gate Arrays (FPGA 2017)*, 2017, pp. 75–84.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] S. Han, Y. Wang, H. Yang *等人*, “ESE: 基于FPGA的稀疏LSTM高效语音识别引擎”，见 *2017年ACM/SIGDA国际现场可编程门阵列研讨会（FPGA
    2017）论文集*，2017，第75–84页。'
- en: '[146] S. Han, H. Mao, and W. J. Dally, “Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding,” in *Proc.
    the 6th International Conference on Learning Representations (ICLR 2016)*, 2016.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] S. Han, H. Mao 和 W. J. Dally, “深度压缩: 使用剪枝、训练量化和霍夫曼编码压缩深度神经网络”，见 *第6届国际学习表征会议（ICLR
    2016）论文集*，2016。'
- en: '[147] S. Bhattacharya and N. D. Lane, “Sparsification and separation of deep
    learning layers for constrained resource inference on wearables,” in *Proc. the
    14th ACM Conference on Embedded Network Sensor Systems CD-ROM (SenSys 2016)*,
    2016, pp. 176–189.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Bhattacharya 和 N. D. Lane, “针对资源受限的可穿戴设备进行深度学习层的稀疏化和分离”，见 *第14届ACM嵌入式网络传感器系统会议CD-ROM（SenSys
    2016）论文集*，2016，第176–189页。'
- en: '[148] B. Taylor, V. S. Marco, W. Wolff *et al.*, “Adaptive deep learning model
    selection on embedded systems,” in *Proc. the 19th ACM SIGPLAN/SIGBED International
    Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 2018)*,
    2018, pp. 31–43.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] B. Taylor, V. S. Marco, W. Wolff *等人*, “嵌入式系统上的自适应深度学习模型选择”，见 *第19届ACM
    SIGPLAN/SIGBED嵌入式系统语言、编译器和工具国际会议（LCTES 2018）论文集*，2018，第31–43页。'
- en: '[149] S. Liu, Y. Lin, Z. Zhou *et al.*, “On-Demand Deep Model Compression for
    Mobile Devices,” in *Proc. the 16th Annual International Conference on Mobile
    Systems, Applications, and Services (MobiSys 2018)*, 2018, pp. 389–400.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. Liu, Y. Lin, Z. Zhou *等人*, “按需深度模型压缩用于移动设备”，见 *第16届国际移动系统、应用和服务年会（MobiSys
    2018）论文集*，2018，第389–400页。'
- en: '[150] L. Lai and N. Suda, “Enabling deep learning at the IoT edge,” in *Proc.
    the International Conference on Computer-Aided Design (ICCAD 2018)*, 2018, pp.
    1–6.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] L. Lai 和 N. Suda, “在IoT边缘实现深度学习”，见 *国际计算机辅助设计会议（ICCAD 2018）论文集*，2018，第1–6页。'
- en: '[151] S. Yao, Y. Zhao, A. Zhang *et al.*, “DeepIoT: Compressing Deep Neural
    Network Structures for Sensing Systems with a Compressor-Critic Framework,” in
    *Proc. the 15th ACM Conference on Embedded Network Sensor Systems (SenSys 2017)*,
    2017, pp. 1–14.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] S. Yao, Y. Zhao, A. Zhang *等人*, “DeepIoT: 使用压缩-批评框架压缩深度神经网络结构以用于传感系统”，见
    *第15届ACM嵌入式网络传感器系统会议（SenSys 2017）论文集*，2017，第1–14页。'
- en: '[152] S. Han, H. Shen, M. Philipose *et al.*, “MCDNN: An Execution Framework
    for Deep Neural Networks on Resource-Constrained Devices,” in *Proc. the 14th
    Annual International Conference on Mobile Systems, Applications, and Services
    (MobiSys 2016)*, 2016, pp. 123–136.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] S. Han, H. Shen, M. Philipose *等人*, “MCDNN: 一个在资源受限设备上运行深度神经网络的执行框架”，见
    *第14届国际移动系统、应用和服务年会（MobiSys 2016）论文集*，2016，第123–136页。'
- en: '[153] S. Han *et al.*, “EIE: Efficient Inference Engine on Compressed Deep
    Neural Network,” in *ACM/IEEE 43rd Annual International Symposium on Computer
    Architecture (ISCA 2016)*, 2016, pp. 243–254.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Han *等人*, “EIE: 在压缩深度神经网络上的高效推理引擎”，见 *ACM/IEEE第43届计算机架构年会（ISCA 2016）论文集*，2016，第243–254页。'
- en: '[154] N. D. Lane, S. Bhattacharya, P. Georgiev *et al.*, “DeepX: A Software
    Accelerator for Low-Power Deep Learning Inference on Mobile Devices,” in *15th
    ACM/IEEE International Conference on Information Processing in Sensor Networks
    (IPSN 2016)*, 2016, pp. 1–12.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. Zhang *et al.*, “A Locally Distributed Mobile Computing Framework
    for DNN based Android Applications,” in *Proc. the Tenth Asia-Pacific Symposium
    on Internetware (Internetware 2018)*, 2018, pp. 1–6.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Z. Zhao, K. M. Barijough, and A. Gerstlauer, “DeepThings: Distributed
    Adaptive Deep Learning Inference on Resource-Constrained IoT Edge Clusters,” *IEEE
    Trans. Comput. Aided Des. Integr. Circuits Syst.*, vol. 37, no. 11, pp. 2348–2359,
    Nov. 2018.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Z. Zhao, Z. Jiang, N. Ling *et al.*, “ECRT: An Edge Computing System
    for Real-Time Image-based Object Tracking,” in *Proc. the 16th ACM Conference
    on Embedded Networked Sensor Systems (SenSys 2018)*, 2018, pp. 394–395.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] H. Li, K. Ota, and M. Dong, “Learning IoT in Edge: Deep Learning for
    the Internet of Things with Edge Computing,” *IEEE Netw.*, vol. 32, no. 1, pp.
    96–101, Jan. 2018.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] S. S. Ogden and T. Guo, “MODI: Mobile Deep Inference Made Efficient by
    Edge Computing,” in *{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge
    2018)*, 2018.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S. Teerapittayanon *et al.*, “BranchyNet: Fast inference via early exiting
    from deep neural networks,” in *Proc. the 23rd International Conference on Pattern
    Recognition (ICPR 2016)*, 2016, pp. 2464–2469.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S. Teerapittayanon, B. McDanel, and H. T. Kung, “Distributed Deep Neural
    Networks over the Cloud, the Edge and End Devices,” in *IEEE 37th International
    Conference on Distributed Computing Systems (ICDCS 2017)*, 2017, pp. 328–339.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] E. Li, Z. Zhou, and X. Chen, “Edge Intelligence: On-Demand Deep Learning
    Model Co-Inference with Device-Edge Synergy,” in *Proc. the 2018 Workshop on Mobile
    Edge Communications (MECOMM 2018)*, 2018, pp. 31–36.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] U. Drolia, K. Guo, J. Tan *et al.*, “Cachier: Edge-Caching for Recognition
    Applications,” in *IEEE 37th International Conference on Distributed Computing
    Systems (ICDCS 2017)*, 2017, pp. 276–286.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] L. N. Huynh, Y. Lee, and R. K. Balan, “DeepMon: Mobile GPU-based Deep
    Learning Framework for Continuous Vision Applications,” in *Proc. the 15th Annual
    International Conference on Mobile Systems, Applications, and Services (MobiSys
    2017)*, 2017, pp. 82–95.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Xu, M. Zhu *et al.*, “DeepCache: Principled Cache for Mobile Deep
    Vision,” in *Proc. the 24th Annual International Conference on Mobile Computing
    and Networking (MobiCom 2018)*, 2018, pp. 129–144.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Guo, B. Hu *et al.*, “FoggyCache: Cross-Device Approximate Computation
    Reuse,” in *Proc. the 24th Annual International Conference on Mobile Computing
    and Networking (MobiCom 2018)*, 2018, pp. 19–34.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. H. Jiang, D. L.-K. Wong, C. Canel, L. Tang, I. Misra, M. Kaminsky,
    M. A. Kozuch, P. Pillai, D. G. Andersen, and G. R. Ganger, “Mainstream: Dynamic
    Stem-sharing for Multi-tenant Video Processing,” in *Proc. the 2018 USENIX Conference
    on Usenix Annual Technical Conference (USENIX ATC 2018)*, 2018, pp. 29–41.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. H. Jiang, D. L.-K. Wong, C. Canel, L. Tang, I. Misra, M. Kaminsky,
    M. A. Kozuch, P. Pillai, D. G. Andersen 和 G. R. Ganger，“Mainstream：多租户视频处理的动态主干共享，”在
    *2018年USENIX年会技术会议 (USENIX ATC 2018)*，2018年，第29–41页。'
- en: '[168] Y. Chen, S. Biookaghazadeh, and M. Zhao, “Exploring the Capabilities
    of Mobile Devices Supporting Deep Learning,” in *Proc. the 27th International
    Symposium on High-Performance Parallel and Distributed Computing (HPDC 2018)*,
    2018, pp. 17–18.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Y. Chen, S. Biookaghazadeh 和 M. Zhao，“探索支持深度学习的移动设备能力，”在 *第27届高性能并行与分布计算国际研讨会
    (HPDC 2018)*，2018年，第17–18页。'
- en: '[169] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深的卷积网络，” *arXiv 预印本 arXiv:1409.1556*，2014年。'
- en: '[170] R. Venkatesan and B. Li, “Diving deeper into mentee networks,” *arXiv
    preprint arXiv:1604.08220*, 2016.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] R. Venkatesan 和 B. Li，“深入探讨导师网络，” *arXiv 预印本 arXiv:1604.08220*，2016年。'
- en: '[171] S. Biookaghazadeh, F. Ren, and M. Zhao, “Are FPGAs Suitable for Edge
    Computing?” *arXiv preprint arXiv:1804.06404*, 2018.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Biookaghazadeh, F. Ren 和 M. Zhao，“FPGA 适合边缘计算吗？” *arXiv 预印本 arXiv:1804.06404*，2018年。'
- en: '[172] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen, “DeepDecision: A Mobile
    Deep Learning Framework for Edge Video Analytics,” in *2018 IEEE Conference on
    Computer Communications (INFOCOM 2018)*, 2018, pp. 1421–1429.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] X. Ran, H. Chen, X. Zhu, Z. Liu 和 J. Chen，“DeepDecision：用于边缘视频分析的移动深度学习框架，”在
    *2018 IEEE 计算机通讯会议 (INFOCOM 2018)*，2018年，第1421–1429页。'
- en: '[173] W. Zhang, Z. Zhang, S. Zeadally *et al.*, “MASM: A Multiple-algorithm
    Service Model for Energy-delay Optimization in Edge Artificial Intelligence,”
    *IEEE Trans. Ind. Inf. (Early Access)*, 2019.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] W. Zhang, Z. Zhang, S. Zeadally *等*，“MASM：一种多算法服务模型用于边缘人工智能中的能量延迟优化，”
    *IEEE 工业信息学学报 (早期访问)*，2019年。'
- en: '[174] M. Xu, F. Qian, M. Zhu, F. Huang, S. Pushp, and X. Liu, “DeepWear: Adaptive
    Local Offloading for On-Wearable Deep Learning,” *IEEE Trans. Mob. Comput. (Early
    Access)*, 2019.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] M. Xu, F. Qian, M. Zhu, F. Huang, S. Pushp 和 X. Liu，“DeepWear：适应性局部卸载的可穿戴深度学习，”
    *IEEE 移动计算学报 (早期访问)*，2019年。'
- en: '[175] H.-j. Jeong, H.-j. Lee, C. H. Shin, and S.-M. Moon, “IONN: Incremental
    Offloading of Neural Network Computations from Mobile Devices to Edge Servers,”
    in *Proc. the ACM Symposium on Cloud Computing (SoCC 2018)*, 2018, pp. 401–411.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] H.-j. Jeong, H.-j. Lee, C. H. Shin 和 S.-M. Moon，“IONN：从移动设备到边缘服务器的神经网络计算增量卸载，”在
    *ACM 云计算研讨会 (SoCC 2018)*，2018年，第401–411页。'
- en: '[176] Y. Huang, X. Ma, X. Fan *et al.*, “When deep learning meets edge computing,”
    in *IEEE 25th International Conference on Network Protocols (ICNP 2017)*, 2017,
    pp. 1–2.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Y. Huang, X. Ma, X. Fan *等*，“当深度学习遇上边缘计算，”在 *IEEE 第25届国际网络协议会议 (ICNP
    2017)*，2017年，第1–2页。'
- en: '[177] J. Mao, X. Chen, K. W. Nixon *et al.*, “MoDNN: Local distributed mobile
    computing system for Deep Neural Network,” in *Design, Automation & Test in Europe
    Conference & Exhibition (DATE 2017)*, 2017, pp. 1396–1401.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Mao, X. Chen, K. W. Nixon *等*，“MoDNN：用于深度神经网络的本地分布式移动计算系统，”在 *设计、自动化与测试欧洲会议及展览
    (DATE 2017)*，2017年，第1396–1401页。'
- en: '[178] E. Cuervo, A. Balasubramanian, D.-k. Cho *et al.*, “MAUI: Making Smartphones
    Last Longer with Code Offload,” in *Proc. the 8th international conference on
    Mobile systems, applications, and services (MobiSys 2010)*, 2010, pp. 49–62.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] E. Cuervo, A. Balasubramanian, D.-k. Cho *等*，“MAUI：通过代码卸载让智能手机使用更持久，”在
    *第8届国际移动系统、应用和服务会议 (MobiSys 2010)*，2010年，第49–62页。'
- en: '[179] X. Xu, Y. Ding, S. X. Hu, M. Niemier, J. Cong, Y. Hu, and Y. Shi, “Scaling
    for edge inference of deep neural networks,” *Nature Electronics*, vol. 1, no. 4,
    pp. 216–222, Apr. 2018.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] X. Xu, Y. Ding, S. X. Hu, M. Niemier, J. Cong, Y. Hu 和 Y. Shi，“深度神经网络的边缘推理扩展，”
    *自然电子学*，第1卷，第4期，第216–222页，2018年4月。'
- en: '[180] M. Polese, R. Jana, V. Kounev *et al.*, “Machine Learning at the Edge:
    A Data-Driven Architecture with Applications to 5G Cellular Networks,” *arXiv
    preprint arXiv:1808.07647*, 2018.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] M. Polese, R. Jana, V. Kounev *等*，“边缘的机器学习：一个数据驱动的架构及其在5G蜂窝网络中的应用，” *arXiv
    预印本 arXiv:1808.07647*，2018年。'
- en: '[181] L. Lai *et al.*, “Rethinking Machine Learning Development and Deployment
    for Edge Devices,” *arXiv preprint arXiv:1806.07846*, 2018.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] L. Lai *等*，“重新思考边缘设备的机器学习开发和部署，” *arXiv 预印本 arXiv:1806.07846*，2018年。'
- en: '[182] P. Meloni, O. Ripolles, D. Solans *et al.*, “ALOHA: an architectural-aware
    framework for deep learning at the edge,” in *Proc. the Workshop on INTelligent
    Embedded Systems Architectures and Applications (INTESA 2018)*, 2018, pp. 19–26.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] P. Meloni, O. Ripolles, D. Solans *等*, “ALOHA: 一种面向架构的边缘深度学习框架,” 载于*智能嵌入式系统架构与应用研讨会
    (INTESA 2018)*, 2018, 第 19–26 页。'
- en: '[183] X. Zhang, Y. Wang, S. Lu, L. Liu, L. Xu, and W. Shi, “OpenEI: An Open
    Framework for Edge Intelligence,” *arXiv preprint arXiv:1906.01864*, 2019.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] X. Zhang, Y. Wang, S. Lu, L. Liu, L. Xu 和 W. Shi, “OpenEI: 一个开源的边缘智能框架,”
    *arXiv 预印本 arXiv:1906.01864*, 2019年。'
- en: '[184] J. Zhao, T. Tiplea, R. Mortier, J. Crowcroft, and L. Wang, “Data Analytics
    Service Composition and Deployment on IoT Devices,” in *Proc. the 16th Annual
    International Conference on Mobile Systems, Applications, and Services (MobiSys
    2018)*, 2018, pp. 502–504.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] J. Zhao, T. Tiplea, R. Mortier, J. Crowcroft 和 L. Wang, “物联网设备上的数据分析服务组合与部署,”
    载于*第16届移动系统、应用与服务国际会议 (MobiSys 2018)*, 2018, 第 502–504 页。'
- en: '[185] N. Talagala, S. Sundararaman, V. Sridhar, D. Arteaga, Q. Luo, S. Subramanian,
    S. Ghanta, L. Khermosh, and D. Roselli, “ECO: Harmonizing edge and cloud with
    ml/dl orchestration,” in *USENIX Workshop on Hot Topics in Edge Computing (HotEdge
    2018)*.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] N. Talagala, S. Sundararaman, V. Sridhar, D. Arteaga, Q. Luo, S. Subramanian,
    S. Ghanta, L. Khermosh 和 D. Roselli, “ECO: 通过 ml/dl 编排协调边缘与云计算,” 载于*USENIX 边缘计算热点问题研讨会
    (HotEdge 2018)*。'
- en: '[186] X. Zhang, Y. Wang, and W. Shi, “pCAMP: Performance Comparison of Machine
    Learning Packages on the Edges,” in *{USENIX} Workshop on Hot Topics in Edge Computing
    (HotEdge 2018)*, 2018.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] X. Zhang, Y. Wang 和 W. Shi, “pCAMP: 边缘设备上机器学习包的性能比较,” 载于*{USENIX} 边缘计算热点问题研讨会
    (HotEdge 2018)*, 2018年。'
- en: '[187] C. Andrés Ramiro, C. Fiandrino, A. Blanco Pizarro *et al.*, “openLEON:
    An End-to-End Emulator from the Edge Data Center to the Mobile Users Carlos,”
    in *Proc. the 12th International Workshop on Wireless Network Testbeds, Experimental
    Evaluation & Characterization (WiNTECH 2018)*, 2018, pp. 19–27.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] C. Andrés Ramiro, C. Fiandrino, A. Blanco Pizarro *等*, “openLEON: 从边缘数据中心到移动用户的端到端模拟器
    Carlos,” 载于*第12届无线网络测试床、实验评估与特征化国际研讨会 (WiNTECH 2018)*, 2018, 第 19–27 页。'
- en: '[188] Y. Wang, S. Liu, X. Wu, and W. Shi, “CAVBench: A Benchmark Suite for
    Connected and Autonomous Vehicles,” in *2018 IEEE/ACM Symposium on Edge Computing
    (SEC 2018)*, 2018, pp. 30–42.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Y. Wang, S. Liu, X. Wu 和 W. Shi, “CAVBench: 连接和自动驾驶车辆的基准套件,” 载于*2018
    IEEE/ACM 边缘计算研讨会 (SEC 2018)*, 2018, 第 30–42 页。'
- en: '[189] G. Kamath, P. Agnihotri, M. Valero *et al.*, “Pushing Analytics to the
    Edge,” in *2016 IEEE Global Communications Conference (GLOBECOM 2016)*, 2016,
    pp. 1–6.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] G. Kamath, P. Agnihotri, M. Valero *等*, “将分析推向边缘,” 载于*2016 IEEE 全球通信大会
    (GLOBECOM 2016)*, 2016, 第 1–6 页。'
- en: '[190] L. Valerio, A. Passarella, and M. Conti, “A communication efficient distributed
    learning framework for smart environments,” *Pervasive Mob. Comput.*, vol. 41,
    pp. 46–68, Oct. 2017.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] L. Valerio, A. Passarella 和 M. Conti, “智能环境中的高效通信分布式学习框架,” *Pervasive
    Mob. Comput.*, 第 41 卷, 第 46–68 页, 2017年10月。'
- en: '[191] Y. Lin, S. Han, H. Mao *et al.*, “Deep Gradient Compression: Reducing
    the Communication Bandwidth for Distributed Training,” *eprint arXiv:1712.01887*,
    2017.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Y. Lin, S. Han, H. Mao *等*, “深度梯度压缩: 减少分布式训练的通信带宽,” *eprint arXiv:1712.01887*,
    2017年。'
- en: '[192] Z. Tao and C. William, “eSGD : Communication Efficient Distributed Deep
    Learning on the Edge,” in *{USENIX} Workshop on Hot Topics in Edge Computing (HotEdge
    2018)*, 2018, pp. 1–6.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Z. Tao 和 C. William, “eSGD: 边缘计算中的高效通信分布式深度学习,” 载于*{USENIX} 边缘计算热点问题研讨会
    (HotEdge 2018)*, 2018, 第 1–6 页。'
- en: '[193] N. Strom, “Scalable distributed DNN training using commodity GPU cloud
    computing,” in *16th Annual Conference of the International Speech Communication
    Association (INTERSPEECH 2015)*, 2015, pp. 1488–1492.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] N. Strom, “使用商品化 GPU 云计算进行可扩展的分布式 DNN 训练,” 载于*第16届国际语音通信协会年会 (INTERSPEECH
    2015)*, 2015, 第 1488–1492 页。'
- en: '[194] E. Jeong, S. Oh, H. Kim *et al.*, “Communication-Efficient On-Device
    Machine Learning: Federated Distillation and Augmentation under Non-IID Private
    Data,” *arXiv preprint arXiv:1811.11479*, 2018.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] E. Jeong, S. Oh, H. Kim *等*, “高效通信的设备端机器学习: 联邦蒸馏与非IID私人数据下的增强,” *arXiv
    预印本 arXiv:1811.11479*, 2018年。'
- en: '[195] M. Fredrikson, S. Jha, and T. Ristenpart, “Model Inversion Attacks That
    Exploit Confidence Information and Basic Countermeasures,” in *Proc. the 22nd
    ACM SIGSAC Conference on Computer and Communications Security (CCS 2015)*, 2015,
    pp. 1322–1333.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] M. Fredrikson, S. Jha 和 T. Ristenpart, “利用置信信息的模型反演攻击及其基本对策,” 载于*第22届
    ACM SIGSAC 计算机与通信安全会议 (CCS 2015)*, 2015, 第 1322–1333 页。'
- en: '[196] M. Du, K. Wang, Z. Xia, and Y. Zhang, “Differential Privacy Preserving
    of Training Model in Wireless Big Data with Edge Computing,” *IEEE Trans. Big
    Data (Early Access)*, 2018.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] M. Du, K. Wang, Z. Xia, 和 Y. Zhang, “无线大数据中训练模型的差分隐私保护与边缘计算，” *IEEE 大数据汇刊（早期访问）*，2018年。'
- en: '[197] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
    sensitivity in private data analysis,” in *Theory of Cryptography*.   Springer
    Berlin Heidelberg, 2006, pp. 265–284.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] C. Dwork, F. McSherry, K. Nissim, 和 A. Smith, “在私密数据分析中将噪声与敏感性校准，” 在
    *密码学理论*。 施普林格 柏林 赫尔德堡，2006年，第265–284页。'
- en: '[198] H. B. McMahan, E. Moore, D. Ramage *et al.*, “Communication-efficient
    learning of deep networks from decentralized data,” in *Proc. the 20th International
    Conference on Artificial Intelligence and Statistics (AISTATS 2017)*, 2017, pp.
    1273–1282.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] H. B. McMahan, E. Moore, D. Ramage *等*，“从分散数据中进行通信高效的深度网络学习，” 在 *第20届国际人工智能与统计学会议（AISTATS
    2017）*，2017年，第1273–1282页。'
- en: '[199] K. Bonawitz, H. Eichner *et al.*, “Towards Federated Learning at Scale:
    System Design,” *arXiv preprint arXiv:1902.01046*, 2019.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] K. Bonawitz, H. Eichner *等*，“面向大规模联邦学习：系统设计，” *arXiv 预印本 arXiv:1902.01046*，2019年。'
- en: '[200] S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Distributed federated
    learning for ultra-reliable low-latency vehicular communications,” *IEEE Trans.
    Commun. (Early Access)*, 2019.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] S. Samarakoon, M. Bennis, W. Saad, 和 M. Debbah, “超可靠低延迟车载通信的分布式联邦学习，”
    *IEEE 通信汇刊（早期访问）*，2019年。'
- en: '[201] C. Xie, S. Koyejo, and I. Gupta, “Practical Distributed Learning: Secure
    Machine Learning with Communication-Efficient Local Updates,” *arXiv preprint
    arXiv:1903.06996*, 2019.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] C. Xie, S. Koyejo, 和 I. Gupta, “实用分布式学习：具有通信高效本地更新的安全机器学习，” *arXiv 预印本
    arXiv:1903.06996*，2019年。'
- en: '[202] M. S. H. Abad, E. Ozfatura, D. Gunduz, and O. Ercetin, “Hierarchical
    Federated Learning Across Heterogeneous Cellular Networks,” *arXiv preprint arXiv:
    1909.02362*, 2019.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] M. S. H. Abad, E. Ozfatura, D. Gunduz, 和 O. Ercetin, “跨异构蜂窝网络的分层联邦学习，”
    *arXiv 预印本 arXiv: 1909.02362*，2019年。'
- en: '[203] J. Konečný, H. B. McMahan, F. X. Yu *et al.*, “Federated Learning: Strategies
    for Improving Communication Efficiency,” *arXiv preprint arXiv:1610.05492*, 2016.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] J. Konečný, H. B. McMahan, F. X. Yu *等*，“联邦学习：提高通信效率的策略，” *arXiv 预印本
    arXiv:1610.05492*，2016年。'
- en: '[204] A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, and R. Pedarsani,
    “FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging
    and Quantization,” *arXiv preprint arXiv:1909.13014*, 2019.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] A. Reisizadeh, A. Mokhtari, H. Hassani, A. Jadbabaie, 和 R. Pedarsani,
    “FedPAQ：一种具有周期性平均和量化的通信高效联邦学习方法，” *arXiv 预印本 arXiv:1909.13014*，2019年。'
- en: '[205] S. Caldas, J. Konečny, H. B. McMahan, and A. Talwalkar, “Expanding the
    Reach of Federated Learning by Reducing Client Resource Requirements,” *arXiv
    preprint arXiv:1812.07210*, 2018.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] S. Caldas, J. Konečný, H. B. McMahan, 和 A. Talwalkar, “通过减少客户端资源需求扩展联邦学习的覆盖范围，”
    *arXiv 预印本 arXiv:1812.07210*，2018年。'
- en: '[206] B. S. Kashin, “Diameters of some finite-dimensional sets and classes
    of smooth functions,” *Izv. Akad. Nauk SSSR Ser. Mat.*, vol. 41, pp. 334–351,
    1977.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] B. S. Kashin, “一些有限维集合和光滑函数类的直径，” *Izv. Akad. Nauk SSSR Ser. Mat.*，第41卷，第334–351页，1977年。'
- en: '[207] Y. Jiang, S. Wang, B. J. Ko, W.-H. Lee, and L. Tassiulas, “Model Pruning
    Enables Efficient Federated Learning on Edge Devices,” *arXiv preprint arXiv:1909.12326*,
    2019.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Jiang, S. Wang, B. J. Ko, W.-H. Lee, 和 L. Tassiulas, “模型剪枝使边缘设备上的高效联邦学习成为可能，”
    *arXiv 预印本 arXiv:1909.12326*，2019年。'
- en: '[208] S. Wang, T. Tuor, T. Salonidis *et al.*, “When Edge Meets Learning: Adaptive
    Control for Resource-Constrained Distributed Machine Learning,” in *IEEE Conference
    on Computer Communications (INFOCOM 2018)*, Apr. 2018, pp. 63–71.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Wang, T. Tuor, T. Salonidis *等*，“当边缘遇见学习：面向资源受限分布式机器学习的自适应控制，” 在 *IEEE
    计算机通信会议（INFOCOM 2018）*，2018年4月，第63–71页。'
- en: '[209] S. Wang, T. Tuor, T. Salonidis *et al.*, “Adaptive federated learning
    in resource constrained edge computing systems,” *IEEE J. Sel. Areas Commun.*,
    vol. 37, no. 6, pp. 1205–1221, Jun. 2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] S. Wang, T. Tuor, T. Salonidis *等*，“在资源受限的边缘计算系统中的自适应联邦学习，” *IEEE 选择领域通信期刊*，第37卷，第6期，第1205–1221页，2019年6月。'
- en: '[210] T. Tuor, S. Wang, T. Salonidis *et al.*, “Demo abstract: Distributed
    machine learning at resource-limited edge nodes,” in *2018 IEEE Conference on
    Computer Communications Workshops (INFOCOM WKSHPS 2018)*, 2018, pp. 1–2.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] T. Tuor, S. Wang, T. Salonidis *等*，“演示摘要：在资源有限的边缘节点上的分布式机器学习，” 在 *2018
    IEEE 计算机通信会议研讨会（INFOCOM WKSHPS 2018）*，2018年，第1–2页。'
- en: '[211] H. Hu, D. Wang, and C. Wu, “Distributed Machine Learning through Heterogeneous
    Edge Systems,” *arXiv preprint arXiv:1911.06949*, 2019.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] M. Duan, “Astraea: Self-balancing Federated Learning for Improving Classification
    Accuracy of Mobile Deep Learning Applications,” *arXiv preprint arXiv:1907.01132*,
    2019.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] S. Kullback and R. A. Leibler, “On information and sufficiency,” *The
    Annals of Mathematical Statistics*, vol. 22, no. 1, pp. 79–86, 1951.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated Learning via Over-the-Air
    Computation,” *arXiv preprint arXiv:1812.11750*, 2018.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] B. Nazer *et al.*, “Computation over multiple-access channels,” *IEEE
    Trans. Inf. Theory*, vol. 53, no. 10, pp. 3498–3516, Oct. 2007.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] L. Chen, N. Zhao, Y. Chen *et al.*, “Over-the-Air Computation for IoT
    Networks: Computing Multiple Functions With Antenna Arrays,” *IEEE Internet Things
    J.*, vol. 5, no. 6, pp. 5296–5306, Dec. 2018.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] G. Zhu, Y. Wang, and K. Huang, “Broadband Analog Aggregation for Low-Latency
    Federated Edge Learning (Extended Version),” *arXiv preprint arXiv:1812.11494*,
    2018.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Z. Xu, Z. Yang, J. Xiong, J. Yang, and X. Chen, “ELFISH: Resource-Aware
    Federated Learning on Heterogeneous Edge Devices,” *arXiv preprint arXiv:1912.01684*,
    2019.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] C. Dinh, N. H. Tran, M. N. H. Nguyen, C. S. Hong, W. Bao, A. Y. Zomaya,
    and V. Gramoli, “Federated Learning over Wireless Networks: Convergence Analysis
    and Resource Allocation,” *arXiv preprint arXiv:1910.13067*, 2019.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A Joint Learning
    and Communications Framework for Federated Learning over Wireless Networks,” *arXiv
    preprint arXiv:1909.07972*, 2019.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] T. Li, M. Sanjabi, and V. Smith, “Fair Resource Allocation in Federated
    Learning,” *arXiv preprint arXiv:1905.10497*, 2019.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] K. Bonawitz, V. Ivanov, B. Kreuter *et al.*, “Practical Secure Aggregation
    for Privacy-Preserving Machine Learning,” in *Proc. the 2017 ACM SIGSAC Conference
    on Computer and Communications Security (CCS 2017)*, 2017, pp. 1175–1191.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “On-Device Federated Learning
    via Blockchain and its Latency Analysis,” *arXiv preprint arXiv:1808.03949*, 2018.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] J. E. Stiglitz, “Self-selection and pareto efficient taxation,” *Journal
    of Public Economics*, vol. 17, no. 2, pp. 213 – 240, 1982.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] H. W. Kuhn, “The hungarian method for the assignment problem,” *Naval
    Research Logistics Quarterly*, vol. 2, no. 1‐2, pp. 83–97, 1955.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] H. SHI, R. V. Prasad, E. Onur, and I. G. M. M. Niemegeers, “Fairness
    in wireless networks:issues, measures and challenges,” *IEEE Commun. Surv. Tutor.*,
    vol. 16, no. 1, pp. 5–24, First 2014.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] M. Hofmann and L. Beaumont, “Chapter 3 - caching techniques for web content,”
    in *Content Networking*, 2005, pp. 53–79.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] X. Wang, M. Chen, T. Taleb *et al.*, “Cache in the air: Exploiting content
    caching and delivery techniques for 5G systems,” *IEEE Commun. Mag.*, vol. 52,
    no. 2, pp. 131–139, Feb. 2014.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] X. Wang, M. Chen, T. Taleb *等*，“空中缓存：利用内容缓存与传输技术优化5G系统，” *IEEE 通信杂志*，第52卷，第2期，页131–139，2014年2月。'
- en: '[229] E. Zeydan, E. Bastug, M. Bennis *et al.*, “Big data caching for networking:
    moving from cloud to edge,” *IEEE Commun. Mag.*, vol. 54, no. 9, pp. 36–42, Sep.
    2016.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] E. Zeydan, E. Bastug, M. Bennis *等*，“网络中的大数据缓存：从云端到边缘的转变，” *IEEE 通信杂志*，第54卷，第9期，页36–42，2016年9月。'
- en: '[230] J. Song, M. Sheng, T. Q. S. Quek *et al.*, “Learning-based content caching
    and sharing for wireless networks,” *IEEE Trans. Commun.*, vol. 65, no. 10, pp.
    4309–4324, Oct. 2017.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] J. Song, M. Sheng, T. Q. S. Quek *等*，“基于学习的无线网络内容缓存与共享，” *IEEE 通信汇刊*，第65卷，第10期，页4309–4324，2017年10月。'
- en: '[231] X. Li, X. Wang, P.-J. Wan *et al.*, “Hierarchical Edge Caching in Device-to-Device
    Aided Mobile Networks: Modeling, Optimization, and Design,” *IEEE J. Sel. Areas
    Commun.*, vol. 36, no. 8, pp. 1768–1785, Aug. 2018.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] X. Li, X. Wang, P.-J. Wan *等*，“基于层次化边缘缓存的设备到设备辅助移动网络：建模、优化与设计，” *IEEE
    选择性通信领域期刊*，第36卷，第8期，页1768–1785，2018年8月。'
- en: '[232] S. Rathore, J. H. Ryu, P. K. Sharma, and J. H. Park, “DeepCachNet: A
    Proactive Caching Framework Based on Deep Learning in Cellular Networks,” *IEEE
    Netw.*, vol. 33, no. 3, pp. 130–138, May 2019.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] S. Rathore, J. H. Ryu, P. K. Sharma 和 J. H. Park，“DeepCachNet：基于深度学习的蜂窝网络前瞻性缓存框架，”
    *IEEE 网络*，第33卷，第3期，页130–138，2019年5月。'
- en: '[233] Z. Chang, L. Lei, Z. Zhou *et al.*, “Learn to Cache: Machine Learning
    for Network Edge Caching in the Big Data Era,” *IEEE Wireless Commun.*, vol. 25,
    no. 3, pp. 28–35, Jun. 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] Z. Chang, L. Lei, Z. Zhou *等*，“学习缓存：大数据时代网络边缘缓存的机器学习，” *IEEE 无线通信*，第25卷，第3期，页28–35，2018年6月。'
- en: '[234] J. Yang, J. Zhang, C. Ma *et al.*, “Deep learning-based edge caching
    for multi-cluster heterogeneous networks,” *Neural Computing and Applications*,
    Feb. 2019.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] J. Yang, J. Zhang, C. Ma *等*，“基于深度学习的多集群异构网络边缘缓存，” *神经计算与应用*，2019年2月。'
- en: '[235] A. Ndikumana, N. H. Tran, and C. S. Hong, “Deep Learning Based Caching
    for Self-Driving Car in Multi-access Edge Computing,” *arXiv preprint arXiv:1810.01548*,
    2018.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] A. Ndikumana, N. H. Tran 和 C. S. Hong，“基于深度学习的自驾车多接入边缘计算缓存，” *arXiv 预印本
    arXiv:1810.01548*，2018年。'
- en: '[236] T. Kanungo, D. M. Mount *et al.*, “An Efficient k-Means Clustering Algorithm:
    Analysis and Implementation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 24,
    no. 7, pp. 881–892, Jul. 2002.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] T. Kanungo, D. M. Mount *等*，“高效的k均值聚类算法：分析与实现，” *IEEE 模式分析与机器智能汇刊*，第24卷，第7期，页881–892，2002年7月。'
- en: '[237] Y. Tang, K. Guo *et al.*, “A smart caching mechanism for mobile multimedia
    in information centric networking with edge computing,” *Future Gener. Comput.
    Syst.*, vol. 91, pp. 590–600, Feb. 2019.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] Y. Tang, K. Guo *等*，“面向信息中心网络与边缘计算的移动多媒体智能缓存机制，” *未来一代计算系统*，第91卷，页590–600，2019年2月。'
- en: '[238] D. Adelman and A. J. Mersereau, “Relaxations of weakly coupled stochastic
    dynamic programs,” *Operations Research*, vol. 56, no. 3, pp. 712–727, 2008.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] D. Adelman 和 A. J. Mersereau，“弱耦合随机动态规划的松弛，” *运筹学*，第56卷，第3期，页712–727，2008年。'
- en: '[239] H. Zhu, Y. Cao, W. Wang *et al.*, “Deep Reinforcement Learning for Mobile
    Edge Caching: Review, New Features, and Open Issues,” *IEEE Netw.*, vol. 32, no. 6,
    pp. 50–57, Nov. 2018.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] H. Zhu, Y. Cao, W. Wang *等*，“移动边缘缓存的深度强化学习：综述、新特性与开放问题，” *IEEE 网络*，第32卷，第6期，页50–57，2018年11月。'
- en: '[240] K. Guo, C. Yang, and T. Liu, “Caching in Base Station with Recommendation
    via Q-Learning,” in *2017 IEEE Wireless Communications and Networking Conference
    (WCNC 2017)*, 2017, pp. 1–6.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] K. Guo, C. Yang 和 T. Liu，“基站中的缓存与Q学习推荐，” 见 *2017 IEEE 无线通信与网络会议（WCNC
    2017）*，2017年，页1–6。'
- en: '[241] C. Zhong, M. C. Gursoy *et al.*, “A deep reinforcement learning-based
    framework for content caching,” in *52nd Annual Conference on Information Sciences
    and Systems (CISS 2018)*, 2018, pp. 1–6.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] C. Zhong, M. C. Gursoy *等*，“基于深度强化学习的内容缓存框架，” 见 *第52届年度信息科学与系统会议（CISS
    2018）*，2018年，页1–6。'
- en: '[242] G. Dulac-Arnold, R. Evans, H. van Hasselt *et al.*, “Deep Reinforcement
    Learning in Large Discrete Action Spaces,” *arXiv preprint arXiv:1512.07679*,
    2015.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] G. Dulac-Arnold, R. Evans, H. van Hasselt *等*，“大离散动作空间中的深度强化学习，” *arXiv
    预印本 arXiv:1512.07679*，2015年。'
- en: '[243] P. Mach and Z. Becvar, “Mobile edge computing: A survey on architecture
    and computation offloading,” *IEEE Commun. Surveys Tuts.*, vol. 19, no. 3, pp.
    1628–1656, Thirdquarter 2017.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] P. Mach 和 Z. Becvar，“移动边缘计算：架构与计算卸载综述，” *IEEE 通信调查与教程*，第19卷，第3期，页1628–1656，2017年第三季度。'
- en: '[244] X. Chen, L. Jiao, W. Li, and X. Fu, “Efficient multi-user computation
    offloading for mobile-edge cloud computing,” *IEEE/ACM Trans. Netw.*, vol. 24,
    no. 5, pp. 2795–2808, Oct. 2016.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] X. Chen, L. Jiao, W. Li 和 X. Fu， “面向移动边缘云计算的高效多用户计算卸载”， *IEEE/ACM 网络汇刊*，第24卷，第5期，页码
    2795–2808，2016年10月。'
- en: '[245] J. Xu, L. Chen *et al.*, “Online Learning for Offloading and Autoscaling
    in Energy Harvesting Mobile Edge Computing,” *IEEE Trans. on Cogn. Commun. Netw.*,
    vol. 3, no. 3, pp. 361–373, Sep. 2017.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] J. Xu, L. Chen *等*， “面向能量收集移动边缘计算的卸载和自动扩展的在线学习”， *IEEE 认知通信与网络汇刊*，第3卷，第3期，页码
    361–373，2017年9月。'
- en: '[246] T. Q. Dinh, Q. D. La, T. Q. S. Quek, and H. Shin, “Distributed Learning
    for Computation Offloading in Mobile Edge Computing,” *IEEE Trans. Commun.*, vol. 66,
    no. 12, pp. 6353–6367, Dec. 2018.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] T. Q. Dinh, Q. D. La, T. Q. S. Quek 和 H. Shin， “移动边缘计算中的分布式学习用于计算卸载”，
    *IEEE 通信汇刊*，第66卷，第12期，页码 6353–6367，2018年12月。'
- en: '[247] T. Chen and G. B. Giannakis, “Bandit convex optimization for scalable
    and dynamic iot management,” *IEEE Internet Things J.*, vol. 6, no. 1, pp. 1276–1286,
    Feb. 2019.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] T. Chen 和 G. B. Giannakis， “用于可扩展和动态IoT管理的赌博凸优化”， *IEEE 互联网事务期刊*，第6卷，第1期，页码
    1276–1286，2019年2月。'
- en: '[248] K. Zhang, Y. Zhu, S. Leng, Y. He, S. Maharjan, and Y. Zhang, “Deep Learning
    Empowered Task Offloading for Mobile Edge Computing in Urban Informatics,” *IEEE
    Internet Things J.*, vol. 6, no. 5, pp. 7635–7647, Oct. 2019.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] K. Zhang, Y. Zhu, S. Leng, Y. He, S. Maharjan 和 Y. Zhang， “深度学习赋能的城市信息学中的移动边缘计算任务卸载”，
    *IEEE 互联网事务期刊*，第6卷，第5期，页码 7635–7647，2019年10月。'
- en: '[249] S. Yu, X. Wang, and R. Langar, “Computation offloading for mobile edge
    computing: A deep learning approach,” in *IEEE 28th Annual International Symposium
    on Personal, Indoor, and Mobile Radio Communications (PIMRC 2017)*, 2017, pp.
    1–6.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] S. Yu, X. Wang 和 R. Langar， “移动边缘计算中的计算卸载：一种深度学习方法”，见 *IEEE 第28届年度个人、室内和移动无线通信国际研讨会（PIMRC
    2017）*，2017年，页码 1–6。'
- en: '[250] T. Yang, Y. Hu, M. C. Gursoy *et al.*, “Deep Reinforcement Learning based
    Resource Allocation in Low Latency Edge Computing Networks,” in *15th International
    Symposium on Wireless Communication Systems (ISWCS 2018)*, 2018, pp. 1–5.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] T. Yang, Y. Hu, M. C. Gursoy *等*， “基于深度强化学习的低延迟边缘计算网络中的资源分配”，见 *第15届国际无线通信系统研讨会（ISWCS
    2018）*，2018年，页码 1–5。'
- en: '[251] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized computation
    offloading performance in virtual edge computing systems via deep reinforcement
    learning,” *IEEE Internet Things J.*, vol. 6, no. 3, pp. 4005–4018, Jun. 2019.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji 和 M. Bennis， “通过深度强化学习优化虚拟边缘计算系统中的计算卸载性能”，
    *IEEE 互联网事务期刊*，第6卷，第3期，页码 4005–4018，2019年6月。'
- en: '[252] N. C. Luong, Z. Xiong, P. Wang, and D. Niyato, “Optimal Auction for Edge
    Computing Resource Management in Mobile Blockchain Networks: A Deep Learning Approach,”
    in *2018 IEEE International Conference on Communications (ICC 2018)*, 2018, pp.
    1–6.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] N. C. Luong, Z. Xiong, P. Wang 和 D. Niyato， “移动区块链网络中边缘计算资源管理的最优拍卖：一种深度学习方法”，见
    *2018 IEEE 国际通信会议（ICC 2018）*，2018年，页码 1–6。'
- en: '[253] J. Li, H. Gao, T. Lv, and Y. Lu, “Deep reinforcement learning based computation
    offloading and resource allocation for MEC,” in *2018 IEEE Wireless Communications
    and Networking Conference (WCNC 2018)*, 2018, pp. 1–6.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] J. Li, H. Gao, T. Lv 和 Y. Lu， “基于深度强化学习的计算卸载和资源分配用于移动边缘计算”，见 *2018 IEEE
    无线通信与网络会议（WCNC 2018）*，2018年，页码 1–6。'
- en: '[254] M. Min, L. Xiao, Y. Chen *et al.*, “Learning-based computation offloading
    for iot devices with energy harvesting,” *IEEE Trans. Veh. Technol.*, vol. 68,
    no. 2, pp. 1930–1941, Feb. 2019.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] M. Min, L. Xiao, Y. Chen *等*， “基于学习的能量收集IoT设备计算卸载”， *IEEE 车辆技术汇刊*，第68卷，第2期，页码
    1930–1941，2019年2月。'
- en: '[255] Z. Chen and X. Wang, “Decentralized Computation Offloading for Multi-User
    Mobile Edge Computing: A Deep Reinforcement Learning Approach,” *arXiv preprint
    arXiv:1812.07394*, 2018.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] Z. Chen 和 X. Wang， “面向多用户移动边缘计算的去中心化计算卸载：一种深度强化学习方法”， *arXiv 预印本 arXiv:1812.07394*，2018年。'
- en: '[256] T. Chen *et al.*, “Harnessing Bandit Online Learning to Low-Latency Fog
    Computing,” in *2018 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP 2018)*, 2018, pp. 6418–6422.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] T. Chen *等*， “利用赌博在线学习实现低延迟雾计算”，见 *2018 IEEE 国际声学、语音与信号处理会议（ICASSP 2018）*，2018年，页码
    6418–6422。'
- en: '[257] Q. Zhang, M. Lin, L. T. Yang, Z. Chen, S. U. Khan, and P. Li, “A double
    deep q-learning model for energy-efficient edge scheduling,” *IEEE Trans. Serv.
    Comput.*, vol. 12, no. 05, pp. 739–749, Jan. 2019.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Q. Zhang, M. Lin, L. T. Yang, Z. Chen, S. U. Khan 和 P. Li， “一种用于节能边缘调度的双重深度Q学习模型”，
    *IEEE 服务计算汇刊*，第12卷，第05期，页码 739–749，2019年1月。'
- en: '[258] L. Huang, S. Bi, and Y.-j. A. Zhang, “Deep Reinforcement Learning for
    Online Offloading in Wireless Powered Mobile-Edge Computing Networks,” *arXiv
    preprint arXiv:1808.01977*, 2018.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] L. 黄, S. 毕, 和 Y.-j. 张, “基于深度强化学习的无线供电移动边缘计算网络中的在线卸载，” *arXiv 预印本 arXiv:1808.01977*，2018年。'
- en: '[259] S. Memon *et al.*, “Using machine learning for handover optimization
    in vehicular fog computing,” in *Proc. the 34th ACM/SIGAPP Symposium on Applied
    Computing (SAC 2019)*, 2019, pp. 182–190.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] S. 梅蒙 *等*, “在车辆雾计算中使用机器学习进行切换优化，” 见 *第34届ACM/SIGAPP应用计算研讨会（SAC 2019）*，2019年，页182–190。'
- en: '[260] Y. Sun, M. Peng, and S. Mao, “Deep reinforcement learning-based mode
    selection and resource management for green fog radio access networks,” *IEEE
    Internet Things J.*, vol. 6, no. 2, pp. 1960–1971, 2019.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] Y. 孙, M. 彭, 和 S. 毛, “基于深度强化学习的模式选择与绿色雾无线接入网络的资源管理，” *IEEE 互联网事物杂志*, 第6卷，第2期，页1960–1971，2019年。'
- en: '[261] L. Xiao, X. Wan, C. Dai *et al.*, “Security in mobile edge caching with
    reinforcement learning,” *IEEE Wireless Commun.*, vol. 25, no. 3, pp. 116–122,
    Jun. 2018.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] L. 肖, X. 万, C. 戴 *等*, “移动边缘缓存中的安全性与强化学习，” *IEEE 无线通信*, 第25卷，第3期，页116–122，2018年6月。'
- en: '[262] Y. Wei, F. R. Yu, M. Song, and Z. Han, “Joint optimization of caching,
    computing, and radio resources for fog-enabled iot using natural actor–critic
    deep reinforcement learning,” *IEEE Internet Things J.*, vol. 6, no. 2, pp. 2061–2073,
    Apr. 2019.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] Y. 魏, F. R. 于, M. 宋, 和 Z. 韩, “基于自然演员-评论员深度强化学习的雾启用物联网缓存、计算和无线资源的联合优化，”
    *IEEE 互联网事物杂志*, 第6卷，第2期，页2061–2073，2019年4月。'
- en: '[263] D. C. Nguyen, P. N. Pathirana, M. Ding, and A. Seneviratne, “Secure Computation
    Offloading in Blockchain based IoT Networks with Deep Reinforcement Learning,”
    *arXiv preprint arXiv:1908.07466*, 2018.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] D. C. 阮, P. N. 帕提哈拉, M. 丁, 和 A. 塞内维拉特纳, “基于区块链的物联网网络中的安全计算卸载与深度强化学习，”
    *arXiv 预印本 arXiv:1908.07466*，2018年。'
- en: '[264] C.-Y. Li, H.-Y. Liu *et al.*, “Mobile Edge Computing Platform Deployment
    in 4G LTE Networks : A Middlebox Approach,” in *{USENIX} Workshop on Hot Topics
    in Edge Computing (HotEdge 2018)*, 2018.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] C.-Y. 李, H.-Y. 刘 *等*, “4G LTE网络中的移动边缘计算平台部署：中间盒方法，” 见 *{USENIX} 边缘计算热点研讨会（HotEdge
    2018）*，2018年。'
- en: '[265] Q. Mao, F. Hu, and Q. Hao, “Deep learning for intelligent wireless networks:
    A comprehensive survey,” *IEEE Commun. Surveys Tuts.*, vol. 20, no. 4, pp. 2595–2621,
    Fourthquarter 2018.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] Q. 毛, F. 胡, 和 Q. 郝, “智能无线网络中的深度学习：全面调查，” *IEEE 通信调查与教程*, 第20卷，第4期，页2595–2621，2018年第四季度。'
- en: '[266] R. Li, Z. Zhao, X. Zhou *et al.*, “Intelligent 5g: When cellular networks
    meet artificial intelligence,” *IEEE Wireless Commun.*, vol. 24, no. 5, pp. 175–183,
    Oct. 2017.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] R. 李, Z. 赵, X. 周 *等*, “智能5G：当蜂窝网络遇到人工智能，” *IEEE 无线通信*, 第24卷，第5期，页175–183，2017年10月。'
- en: '[267] X. Chen, J. Wu, Y. Cai *et al.*, “Energy-efficiency oriented traffic
    offloading in wireless networks: A brief survey and a learning approach for heterogeneous
    cellular networks,” *IEEE J. Sel. Areas Commun.*, vol. 33, no. 4, pp. 627–640,
    Apr. 2015.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] X. 陈, J. 吴, Y. 蔡 *等*, “面向能源效率的无线网络流量卸载：异质蜂窝网络的简要调查与学习方法，” *IEEE 选择领域通信杂志*,
    第33卷，第4期，页627–640，2015年4月。'
- en: '[268] R. Dong, C. She, W. Hardjawana, Y. Li, and B. Vucetic, “Deep Learning
    for Hybrid 5G Services in Mobile Edge Computing Systems: Learn From a Digital
    Twin,” *IEEE Trans. Wirel. Commun.*, vol. 18, no. 10, pp. 4692–4707, Oct. 2019.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] R. 董, C. 奢, W. 哈贾瓦纳, Y. 李, 和 B. 武切蒂奇, “移动边缘计算系统中混合5G服务的深度学习：从数字双胞胎中学习，”
    *IEEE 无线通信汇刊*, 第18卷，第10期，页4692–4707，2019年10月。'
- en: '[269] Y. Chen, Y. Zhang, S. Maharjan, M. Alam, and T. Wu, “Deep Learning for
    Secure Mobile Edge Computing in Cyber-Physical Transportation Systems,” *IEEE
    Netw.*, vol. 33, no. 4, pp. 36–41, 2019.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] Y. 陈, Y. 张, S. 马哈尔詹, M. 阿拉姆, 和 T. 吴, “用于网络物理交通系统的安全移动边缘计算的深度学习，” *IEEE
    网络*, 第33卷，第4期，页36–41，2019年。'
- en: '[270] M. Min, X. Wan, L. Xiao *et al.*, “Learning-Based Privacy-Aware Offloading
    for Healthcare IoT with Energy Harvesting,” *IEEE Internet Things J. (Early Access)*,
    2018.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] M. 闵, X. 万, L. 肖 *等*, “基于学习的隐私感知卸载用于医疗物联网与能源收集，” *IEEE 互联网事物杂志（早期访问）*，2018年。'
- en: '[271] T. E. Bogale, X. Wang, and L. B. Le, “Machine Intelligence Techniques
    for Next-Generation Context-Aware Wireless Networks,” *arXiv preprint arXiv:1801.04223*,
    2018.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] T. E. 博加勒, X. 王, 和 L. B. 乐, “面向下一代上下文感知无线网络的机器智能技术，” *arXiv 预印本 arXiv:1801.04223*，2018年。'
- en: '[272] D. Kreutz *et al.*, “Software-defined networking: A comprehensive survey,”
    *Proc. IEEE*, vol. 103, no. 1, pp. 14–76, Jan. 2015.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] D. 克劳茨 *等*, “软件定义网络：全面调查，” *IEEE 会议录*, 第103卷，第1期，页14–76，2015年1月。'
- en: '[273] Y. He, F. R. Yu, N. Zhao *et al.*, “Software-defined networks with mobile
    edge computing and caching for smart cities: A big data deep reinforcement learning
    approach,” *IEEE Commun. Mag.*, vol. 55, no. 12, pp. 31–37, Dec. 2017.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] Y. He, F. R. Yu, N. Zhao *等*，“结合移动边缘计算和缓存的软件定义网络用于智能城市：一种大数据深度强化学习方法，”
    *IEEE通信杂志*，第55卷，第12期，页31–37，2017年12月。'
- en: '[274] Y. Gan, Y. Zhang, D. Cheng *et al.*, “An Open-Source Benchmark Suite
    for Microservices and Their Hardware-Software Implications for Cloud and Edge
    Systems,” in *Proc. the Twenty Fourth International Conference on Architectural
    Support for Programming Languages and Operating Systems (ASPLOS 2019)*, 2019.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] Y. Gan, Y. Zhang, D. Cheng *等*，“针对云和边缘系统的微服务及其硬件-软件影响的开源基准套件，”见 *第24届国际编程语言和操作系统架构支持会议（ASPLOS
    2019）*，2019年。'
- en: '[275] M. Alam, J. Rufino, J. Ferreira, S. H. Ahmed, N. Shah, and Y. Chen, “Orchestration
    of Microservices for IoT Using Docker and Edge Computing,” *IEEE Commun. Mag.*,
    vol. 56, no. 9, pp. 118–123, 2018.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] M. Alam, J. Rufino, J. Ferreira, S. H. Ahmed, N. Shah 和 Y. Chen，“使用Docker和边缘计算的物联网微服务编排，”
    *IEEE通信杂志*，第56卷，第9期，页118–123，2018年。'
- en: '[276] J. Xu, S. Wang, B. Bhargava, and F. Yang, “A Blockchain-enabled Trustless
    Crowd-Intelligence Ecosystem on Mobile Edge Computing,” *IEEE Trans. Ind. Inf.
    (Early Access)*, 2019.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] J. Xu, S. Wang, B. Bhargava 和 F. Yang，“基于区块链的无信任众智生态系统在移动边缘计算中的应用，” *IEEE工业信息学期刊（早期访问）*，2019年。'
- en: '[277] Z. Zheng, S. Xie, H. Dai *et al.*, “An overview of blockchain technology:
    Architecture, consensus, and future trends,” in *2017 IEEE International Congress
    on Big Data (BigData Congress 2017)*, 2017, pp. 557–564.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] Z. Zheng, S. Xie, H. Dai *等*，“区块链技术概述：架构、共识与未来趋势，”见 *2017 IEEE 大数据国际大会（BigData
    Congress 2017）*，2017年，页557–564。'
- en: '[278] J.-y. Kim and S.-M. Moon, “Blockchain-based edge computing for deep neural
    network applications,” in *Proc. the Workshop on INTelligent Embedded Systems
    Architectures and Applications (INTESA 2018)*, 2018, pp. 53–55.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] J.-y. Kim 和 S.-M. Moon，“基于区块链的边缘计算用于深度神经网络应用，”见 *第八届智能嵌入式系统架构与应用研讨会（INTESA
    2018）*，2018年，页53–55。'
- en: '[279] G. Wood, “Ethereum: A secure decentralised generalised transaction ledger,”
    2014\. [Online]. Available: [http://gavwood.com/Paper.pdf](http://gavwood.com/Paper.pdf)'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[279] G. Wood，“以太坊：一种安全的去中心化通用交易账本，”2014年。[在线]。可用：[http://gavwood.com/Paper.pdf](http://gavwood.com/Paper.pdf)'
- en: '[280] S. Zheng, Q. Meng, T. Wang *et al.*, “Asynchronous stochastic gradient
    descent with delay compensation,” in *Proc. the 34th International Conference
    on Machine Learning (ICML 2017)*, 2017, pp. 4120–4129.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[280] S. Zheng, Q. Meng, T. Wang *等*，“带有延迟补偿的异步随机梯度下降，”见 *第34届国际机器学习会议（ICML
    2017）*，2017年，页4120–4129。'
- en: '[281] C. Xie, S. Koyejo, and I. Gupta, “Asynchronous Federated Optimization,”
    *arXiv preprint arXiv:1903.03934*, 2019.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[281] C. Xie, S. Koyejo 和 I. Gupta，“异步联邦优化，” *arXiv 预印本 arXiv:1903.03934*，2019年。'
- en: '[282] W. Wu, L. He, W. Lin, RuiMao, and S. Jarvis, “SAFA: a Semi-Asynchronous
    Protocol for Fast Federated Learning with Low Overhead,” *arXiv preprint arXiv:1910.01355*,
    2019.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[282] W. Wu, L. He, W. Lin, RuiMao 和 S. Jarvis，“SAFA：一种用于快速联邦学习的低开销半异步协议，”
    *arXiv 预印本 arXiv:1910.01355*，2019年。'
- en: '[283] T. Nishio and R. Yonetani, “Client Selection for Federated Learning with
    Heterogeneous Resources in Mobile Edge,” *arXiv preprint arXiv:1804.08333*, 2018.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[283] T. Nishio 和 R. Yonetani，“具有异质资源的移动边缘联邦学习中的客户端选择，” *arXiv 预印本 arXiv:1804.08333*，2018年。'
- en: '[284] T. Xing, S. S. Sandha, B. Balaji *et al.*, “Enabling Edge Devices that
    Learn from Each Other: Cross Modal Training for Activity Recognition,” in *Proc.
    the 1st International Workshop on Edge Systems, Analytics and Networking (EdgeSys
    2018)*, 2018, pp. 37–42.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[284] T. Xing, S. S. Sandha, B. Balaji *等*，“实现相互学习的边缘设备：用于活动识别的跨模态训练，”见 *第1届国际边缘系统、分析和网络研讨会（EdgeSys
    2018）*，2018年，页37–42。'
- en: '[285] J. Yoon, P. Liu, and S. Banerjee, “Low-Cost Video Transcoding at the
    Wireless Edge,” in *2016 IEEE/ACM Symposium on Edge Computing (SEC 2016)*, 2016,
    pp. 129–141.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[285] J. Yoon, P. Liu 和 S. Banerjee，“无线边缘的低成本视频转码，”见 *2016 IEEE/ACM 边缘计算研讨会（SEC
    2016）*，2016年，页129–141。'
- en: '[286] N. Kato *et al.*, “The deep learning vision for heterogeneous network
    traffic control: Proposal, challenges, and future perspective,” *IEEE Wireless
    Commun.*, vol. 24, no. 3, pp. 146–153, Jun. 2017.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[286] N. Kato *等*，“异质网络流量控制的深度学习愿景：提案、挑战与未来展望，” *IEEE无线通信*，第24卷，第3期，页146–153，2017年6月。'
- en: '[287] Z. M. Fadlullah, F. Tang, B. Mao *et al.*, “State-of-the-art deep learning:
    Evolving machine intelligence toward tomorrow’s intelligent network traffic control
    systems,” *IEEE Commun. Surveys Tuts.*, vol. 19, no. 4, pp. 2432–2455, Fourthquarter
    2017.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[287] Z. M. Fadlullah, F. Tang, B. Mao *等*，“最先进的深度学习：将机器智能引向明天的智能网络流量控制系统，”
    *IEEE通讯调查与教程*，第19卷，第4期，第2432–2455页，2017年第四季度。'
- en: '[288] J. Foerster, I. A. Assael *et al.*, “Learning to communicate with deep
    multi-agent reinforcement learning,” in *Advances in Neural Information Processing
    Systems 29 (NeurIPS 2016)*, 2016, pp. 2137–2145.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[288] J. Foerster, I. A. Assael *等*，“通过深度多智能体强化学习学习交流，”见 *NeurIPS 2016 第29届神经信息处理系统大会*，2016年，第2137–2145页。'
- en: '[289] S. Omidshafiei, J. Pazis, C. Amato *et al.*, “Deep decentralized multi-task
    multi-agent reinforcement learning under partial observability,” in *Proc. the
    34th International Conference on Machine Learning (ICML 2017)*, 2017, pp. 2681–2690.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[289] S. Omidshafiei, J. Pazis, C. Amato *等*，“在部分可观测条件下的深度去中心化多任务多智能体强化学习，”见
    *第34届国际机器学习会议（ICML 2017）论文集*，2017年，第2681–2690页。'
- en: '[290] R. Lowe, Y. WU *et al.*, “Multi-agent actor-critic for mixed cooperative-competitive
    environments,” in *Advances in Neural Information Processing Systems 30 (NeurIPS
    2017)*, 2017, pp. 6379–6390.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[290] R. Lowe, Y. WU *等*，“适用于混合合作-竞争环境的多智能体演员-评论家算法，”见 *NeurIPS 2017 第30届神经信息处理系统大会*，2017年，第6379–6390页。'
- en: '[291] J. Zhou, G. Cui *et al.*, “Graph neural networks: A review of methods
    and applications,” *arXiv preprint arXiv:1812.08434*, 2018.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[291] J. Zhou, G. Cui *等*，“图神经网络：方法与应用综述，” *arXiv 预印本 arXiv:1812.08434*，2018年。'
- en: '[292] Z. Zhang, P. Cui, and W. Zhu, “Deep learning on graphs: A survey,” *arXiv
    preprint arXiv:1812.04202*, 2018.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[292] Z. Zhang, P. Cui, 和 W. Zhu，“图上的深度学习：综述，” *arXiv 预印本 arXiv:1812.04202*，2018年。'
- en: '| ![[Uncaptioned image]](img/1bee7c3d772bdebc312641af9ff7741e.png) | Xiaofei
    Wang [S’06, M’13, SM’18] is currently a Professor with the Tianjin Key Laboratory
    of Advanced Networking, School of Computer Science and Technology, Tianjin University,
    China. He got master and doctor degrees in Seoul National University from 2006
    to 2013, and was a Post-Doctoral Fellow with The University of British Columbia
    from 2014 to 2016\. Focusing on the research of social-aware cloud computing,
    cooperative cell caching, and mobile traffic offloading, he has authored over
    100 technical papers in the IEEE JSAC, the IEEE TWC, the IEEE WIRELESS COMMUNICATIONS,
    the IEEE COMMUNICATIONS, the IEEE TMM, the IEEE INFOCOM, and the IEEE SECON. He
    was a recipient of the National Thousand Talents Plan (Youth) of China. He received
    the “Scholarship for Excellent Foreign Students in IT Field” by NIPA of South
    Korea from 2008 to 2011, the “Global Outstanding Chinese Ph.D. Student Award”
    by the Ministry of Education of China in 2012, and the Peiyang Scholar from Tianjin
    University. In 2017, he received the “Fred W. Ellersick Prize” from the IEEE Communication
    Society. |'
  id: totrans-831
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/1bee7c3d772bdebc312641af9ff7741e.png) | Xiaofei Wang [S’06,
    M’13, SM’18] 目前是中国天津大学计算机科学与技术学院天津先进网络重点实验室的教授。他在2006至2013年期间获得首尔国立大学的硕士和博士学位，并在2014至2016年期间担任不列颠哥伦比亚大学的博士后研究员。专注于社会感知云计算、协作单元缓存和移动流量卸载的研究，他在IEEE
    JSAC、IEEE TWC、IEEE WIRELESS COMMUNICATIONS、IEEE COMMUNICATIONS、IEEE TMM、IEEE INFOCOM和IEEE
    SECON等期刊上发表了100多篇技术论文。他曾获得中国国家千人计划（青年）称号，并于2008至2011年获得韩国NIPA颁发的“IT领域优秀外国学生奖”，2012年获得中国教育部颁发的“全球杰出中国博士生奖”，以及天津大学的培养学者奖。在2017年，他获得了IEEE通信学会颁发的“Fred
    W. Ellersick奖”。'
- en: '| ![[Uncaptioned image]](img/7131c7d234e7cc824c4ce4c8da1eb6a8.png) | Yiwen
    Han [S’18] received his B.S. degree from Nanchang University, China, and M.S.
    degree from Tianjin University, China, in 2015 and 2018, respectively, both in
    communication engineering. He received the Outstanding B.S. Graduates in 2015
    and M.S. National Scholarship of China in 2016. He is currently pursuing the Ph.D.
    degree in computer science at Tianjin University. His current research interests
    include edge computing, reinforcement learning, and deep learning. |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/7131c7d234e7cc824c4ce4c8da1eb6a8.png) | Yiwen Han [S’18]
    于2015年获得中国南昌大学的学士学位，并于2018年获得中国天津大学的硕士学位，两者均为通信工程专业。他于2015年获得优秀本科毕业生称号，并于2016年获得中国硕士国家奖学金。他目前在天津大学攻读计算机科学博士学位，当前研究兴趣包括边缘计算、强化学习和深度学习。'
- en: '| ![[Uncaptioned image]](img/ff420f55034a176c3920bf5c571248e0.png) | Victor
    C. M. Leung [S’75, M’89, SM’97, F’03] is a Distinguished Professor of Computer
    Science and Software Engineering at Shenzhen University. He was a Professor of
    Electrical and Computer Engineering and holder of the TELUS Mobility Research
    Chair at the University of British Columbia (UBC) when he retired from UBC in
    2018 and became a Professor Emeritus. His research is in the broad areas of wireless
    networks and mobile systems. He has co-authored more than 1300 journal/conference
    papers and book chapters. Dr. Leung is serving on the editorial boards of the
    IEEE Transactions on Green Communications and Networking, IEEE Transactions on
    Cloud Computing, IEEE Access, IEEE Network, and several other journals. He received
    the IEEE Vancouver Section Centennial Award, 2011 UBC Killam Research Prize, 2017
    Canadian Award for Telecommunications Research, and 2018 IEEE TCGCC Distinguished
    Technical Achievement Recognition Award. He co-authored papers that won the 2017
    IEEE ComSoc Fred W. Ellersick Prize, 2017 IEEE Systems Journal Best Paper Award,
    2018 IEEE CSIM Best Journal Paper Award, and 2019 IEEE TCGCC Best Journal Paper
    Award. He is a Fellow of IEEE, the Royal Society of Canada, Canadian Academy of
    Engineering, and Engineering Institute of Canada. He is named in the current Clarivate
    Analytics list of “Highly Cited Researchers”. |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/ff420f55034a176c3920bf5c571248e0.png) | Victor C. M. Leung
    [S’75, M’89, SM’97, F’03] 是深圳大学计算机科学与软件工程学科的杰出教授。他曾是英属哥伦比亚大学（UBC）电气与计算机工程教授以及TELUS移动研究主席，2018年从UBC退休并成为名誉教授。他的研究涉及无线网络和移动系统的广泛领域。他已共同撰写了超过1300篇期刊/会议论文和书籍章节。Leung博士在IEEE绿色通信与网络、IEEE云计算、IEEE
    Access、IEEE网络及其他若干期刊的编辑委员会中服务。他获得了IEEE温哥华分部百年奖、2011年UBC Killam研究奖、2017年加拿大电信研究奖以及2018年IEEE
    TCGCC杰出技术成就奖。他共同撰写的论文赢得了2017年IEEE ComSoc Fred W. Ellersick奖、2017年IEEE系统期刊最佳论文奖、2018年IEEE
    CSIM最佳期刊论文奖和2019年IEEE TCGCC最佳期刊论文奖。他是IEEE、加拿大皇家学会、加拿大工程院及加拿大工程研究所的会士，并在Clarivate
    Analytics的“高被引研究者”名单中。 |'
- en: '| ![[Uncaptioned image]](img/cd8cfc58098d295fe91f0fe7c1275d4b.png) | Dusit
    Niyato [M’09, SM’15, F’17] is currently a Professor in the School of Computer
    Science and Engineering, at Nanyang Technological University,Singapore. He received
    B.Eng. from King Mongkuts Institute of Technology Ladkrabang (KMITL), Thailand
    in 1999 and Ph.D. in Electrical and Computer Engineering from the University of
    Manitoba, Canada in 2008\. His research interests are in the area of Internet
    of Things (IoT) and network resource pricing. |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/cd8cfc58098d295fe91f0fe7c1275d4b.png) | Dusit Niyato [M’09,
    SM’15, F’17] 目前是新加坡南洋理工大学计算机科学与工程学院的教授。他于1999年在泰国国王蒙克特理工学院（KMITL）获得工程学学士学位，并于2008年在加拿大曼尼托巴大学获得电气与计算机工程博士学位。他的研究兴趣包括物联网（IoT）和网络资源定价。
    |'
- en: '| ![[Uncaptioned image]](img/a9a240598a23d31519dd946122a5a772.png) | Xueqiang
    Yan is currently a technology expert with Wireless Technology Lab at Huawei Technologies.
    He was a member of technical staff of Bell Labs from 2000 to 2004\. From 2004
    to 2016 he was a director of Strategy Department of Alcatel-Lucent Shanghai Bell.
    His current research interests include wireless networking, Internet of Things,
    edge AI, future mobile network architecture, network convergence and evolution.
    |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/a9a240598a23d31519dd946122a5a772.png) | Xueqiang Yan 目前是华为技术有限公司无线技术实验室的技术专家。他曾在2000至2004年间担任贝尔实验室技术人员。从2004年到2016年，他是阿尔卡特-朗讯上海贝尔战略部门的总监。他当前的研究兴趣包括无线网络、物联网、边缘人工智能、未来移动网络架构、网络融合与演进。
    |'
- en: '| ![[Uncaptioned image]](img/0a383f3bc8fa2ca6e13bc394af7319f1.png) | Xu Chen
    [M’12] is a Full Professor with Sun Yat-sen University, Guangzhou, China, and
    the vice director of National and Local Joint Engineering Laboratory of Digital
    Home Interactive Applications. He received the Ph.D. degree in information engineering
    from the Chinese University of Hong Kong in 2012, and worked as a Postdoctoral
    Research Associate at Arizona State University, Tempe, USA from 2012 to 2014,
    and a Humboldt Scholar Fellow at Institute of Computer Science of University of
    Goettingen, Germany from 2014 to 2016\. He received the prestigious Humboldt research
    fellowship awarded by Alexander von Humboldt Foundation of Germany, 2014 Hong
    Kong Young Scientist Runner-up Award, 2016 Thousand Talents Plan Award for Young
    Professionals of China, 2017 IEEE Communication Society Asia-Pacific Outstanding
    Young Researcher Award, 2017 IEEE ComSoc Young Professional Best Paper Award,
    Honorable Mention Award of 2010 IEEE international conference on Intelligence
    and Security Informatics (ISI), Best Paper Runner-up Award of 2014 IEEE International
    Conference on Computer Communications (INFOCOM), and Best Paper Award of 2017
    IEEE Intranational Conference on Communications (ICC). He is currently an Associate
    Editor of IEEE Internet of Things Journal and IEEE Transactions on Wireless Communications,
    and Area Editor of IEEE Open Journal of the Communications Society. |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '| ![[未加说明的图片]](img/0a383f3bc8fa2ca6e13bc394af7319f1.png) | 徐晨 [M’12] 是中国广州中山大学的全职教授，同时担任国家与地方联合工程实验室数字家庭互动应用副主任。他于2012年获得香港中文大学信息工程博士学位，随后在2012年至2014年期间担任美国亚利桑那州立大学的博士后研究员，并在2014年至2016年期间担任德国哥廷根大学计算机科学研究所的洪堡学者研究员。他获得了德国亚历山大·冯·洪堡基金会授予的洪堡研究奖学金、2014年香港青年科学家二等奖、2016年中国千人计划青年人才奖、2017年IEEE通信学会亚太地区杰出青年研究员奖、2017年IEEE通信学会青年专业人士最佳论文奖、2010年IEEE国际智能与安全信息学会议（ISI）荣誉提名奖、2014年IEEE国际计算机通信会议（INFOCOM）最佳论文二等奖以及2017年IEEE国际通信会议（ICC）最佳论文奖。他目前是IEEE物联网期刊和IEEE无线通信汇刊的副编辑，以及IEEE通信学会开放期刊的领域编辑。'
