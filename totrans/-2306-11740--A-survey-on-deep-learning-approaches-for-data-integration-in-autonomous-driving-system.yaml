- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:38:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2306.11740] A survey on deep learning approaches for data integration in autonomous
    driving system'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11740](https://ar5iv.labs.arxiv.org/html/2306.11740)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A survey on deep learning approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: for data integration in autonomous driving system
  prefs: []
  type: TYPE_NORMAL
- en: 'Xi Zhu^∗, Likang Wang^∗, Caifa Zhou, Xiya Cao, Yue Gong, Lei Chen^† Manuscript
    received DATE; revised DATE. ^∗ denotes equal contribution.^† Corresponding author.
    Email: leichen@cse.ust.hkXi Zhu, Caifa Zhou and Xiya Cao are with Riemann Laboratory,
    2012 Laboratories, Huawei Technologies, China.Likang Wang and Lei Chen are with
    Department of Computer Science and Engineering, Hong Kong University of Science
    and Technology, Hong Kong, China. This work is done when Likang Wang is an intern
    at Huawei Technologies.Yue Gong is with Parallel Distributed Computing Laboratory,
    2012 Laboratories, Huawei Technologies, China.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The perception module of self-driving vehicles relies on a multi-sensor system
    to understand its environment. Recent advancements in deep learning have led to
    the rapid development of approaches that integrate multi-sensory measurements
    to enhance perception capabilities. This paper surveys the latest deep learning
    integration techniques applied to the perception module in autonomous driving
    systems, categorizing integration approaches based on “what, how, and when to
    integrate.” A new taxonomy of integration is proposed, based on three dimensions:
    multi-view, multi-modality, and multi-frame. The integration operations and their
    pros and cons are summarized, providing new insights into the properties of an
    “ideal” data integration approach that can alleviate the limitations of existing
    methods. After reviewing hundreds of relevant papers, this survey concludes with
    a discussion of the key features of an optimal data integration approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'autonomous driving, multi-view, multi-modality, multi-frame, data integration,
    deep learning^†^†publicationid: pubid: 0000–0000/00$00.00 © 2021 IEEE'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perception is a crucial component of Autonomous Driving System (ADS)[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]. It enables self-driving vehicles to perceive
    and comprehend their surroundings and accurately position themselves. The performance
    of the perception module significantly influences downstream tasks, such as planning
    and control, as well as driving safety. The two primary functions of the perception
    module are environment perception and localization [[2](#bib.bib2)]. Environment
    perception involves the vehicle actively gathering information about both static
    elements (e.g., lane lines, road markings, and traffic signs) and dynamic objects
    (e.g., other vehicles, pedestrians). In contrast, localization requires the vehicle
    to also consider its own motion state measurements, such as speed, heading, and
    acceleration. All these perception data can be obtained with the help of a sensor
    suite built-in the vehicle [[4](#bib.bib4)]. Although the sensor suite consists
    of a bunch of sensors [[2](#bib.bib2), [5](#bib.bib5), [6](#bib.bib6)], the ones
    used for environmental perception are cameras, LiDARs, and Millimeter Wave Radar
    (MMW-Radar)[[1](#bib.bib1), [7](#bib.bib7), [4](#bib.bib4), [8](#bib.bib8)]. Detailed
    properties of these sensors are further explained in Section [II](#S2 "II Sensing
    modalities and pre-processing ‣ A survey on deep learning approaches for data
    integration in autonomous driving system").
  prefs: []
  type: TYPE_NORMAL
- en: Although all sensors contribute to information collection, single-sensor systems
    have limitations and shortages that make it difficult to perform complete, accurate,
    and real-time environmental perception in autonomous driving applications [[9](#bib.bib9),
    [10](#bib.bib10), [2](#bib.bib2)]. This is because different sensors have different
    temporal and spatial coverage, as well as areas of expertise and weakness [[11](#bib.bib11),
    [7](#bib.bib7), [12](#bib.bib12), [13](#bib.bib13)]. For example, cameras can
    capture high-resolution images with rich color information, but they cannot work
    well in low-visibility scenarios or provide reliable 3D geometry. Additionally,
    cameras’ angular coverage is limited by their field-of-view. LiDARs are superior
    at 3D geometry estimation, have a wide range of view, and can work in dim light,
    but the points they capture are usually sparse due to the low sampling rate. Radars
    cannot acquire texture information, but they can capture the velocity of moving
    objects, which neither LiDARs nor cameras can do. Furthermore, single-sensor systems
    may suffer from the problem of deprivation, which describes the circumstances
    of perception loss or failure when the sensor stops working or cannot function
    well [[9](#bib.bib9)]. High uncertainty and imprecision are also significant concerns
    of single-sensor systems when data are missing or or measurements are not accurate.
  prefs: []
  type: TYPE_NORMAL
- en: To address the limitations and challenges of single-sensor systems, various
    methods have been proposed to integrate data from different sensors in recent
    years [[9](#bib.bib9), [14](#bib.bib14), [4](#bib.bib4), [15](#bib.bib15)]. In
    the literature of Autonomous Driving System (ADS), data integration is often referred
    to as data fusion, data integration, or sensor fusion, which are used interchangeably
    [[16](#bib.bib16), [11](#bib.bib11), [17](#bib.bib17), [18](#bib.bib18)]. Data
    integration involves logically or physically transforming information from different
    sensors or sources to obtain a more consistent, informative, accurate, and reliable
    output than what could be achieved by using one sensor alone [[19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [18](#bib.bib18), [22](#bib.bib22), [17](#bib.bib17),
    [23](#bib.bib23), [16](#bib.bib16), [11](#bib.bib11)]. Data integration techniques
    can be broadly divided into classical algorithms [[9](#bib.bib9), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [2](#bib.bib2), [27](#bib.bib27)] and deep
    learning approaches [[2](#bib.bib2)], with the latter being the focus of this
    paper due to its increasing popularity and potential for higher accuracy results.
    Deep learning-based integration techniques have drawn increasing attention, thanks
    to the rapid development of deep learning in computer science and artificial intelligence
    [[11](#bib.bib11), [7](#bib.bib7), [22](#bib.bib22), [28](#bib.bib28), [12](#bib.bib12),
    [4](#bib.bib4), [14](#bib.bib14), [15](#bib.bib15)]. They not only yield better
    accuracy results but also eliminate the need for hand-crafted design and fully
    exploit information contained in big data per high computational power [[5](#bib.bib5)].
    Data integration techniques have also been explored in other research fields,
    such as smart living [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33)], medical field [[34](#bib.bib34)], transportation [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)], industry [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40)], and business [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)].
    However, since the current paper focuses on data integration in ADS, readers interested
    in other domains are referred to relevant reviews [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]. In the
    next, we will summarize the key issues that need to be addressed in data integration
    in ADS.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11d34f0be6283d50afb6fd9423485b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d2ada974d6c2fdc61341a4a929be2999.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Bi-level taxonomy of “what to integrate”. (a) Seven upper-level categories
    based on three dimensions, describing the content of integration. (b) Three lower-level
    paradigms of each two-dimensional integration category, based on the order of
    integration.'
  prefs: []
  type: TYPE_NORMAL
- en: Key problems of data integration
  prefs: []
  type: TYPE_NORMAL
- en: Three widely accepted key problems to be addressed in data integration include
    “what to integrate”, “how to integrate”, and “when to integrate” [[50](#bib.bib50),
    [51](#bib.bib51), [14](#bib.bib14)]. Many reviews related to deep learning based
    data integration in ADS perception have been published in recent years, each of
    which partially covers these three problems and related techniques. While some
    reviews take data integration as a technical approach to address certain perception
    tasks under specific circumstances [[52](#bib.bib52), [7](#bib.bib7), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)], others
    center their attention on the data integration itself, including techniques, categories,
    and applications [[11](#bib.bib11), [2](#bib.bib2), [28](#bib.bib28), [12](#bib.bib12),
    [4](#bib.bib4), [14](#bib.bib14), [58](#bib.bib58), [15](#bib.bib15)]. We mainly
    make comparisons with the latter.
  prefs: []
  type: TYPE_NORMAL
- en: I-1 What to integrate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '“What to integrate” is a question about the input to the integration process.
    In this survey, we divide “what to integrate” into two hierarchical sub-questions:
    what is the content to integrate, and what is the order to integrate.'
  prefs: []
  type: TYPE_NORMAL
- en: a) What is the content to integrate?
  prefs: []
  type: TYPE_NORMAL
- en: 'We divide the content of integration into three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-view integration involves integrating representations or views from multiple
    sensors of the same type that point in different directions. For example, integrating
    point clouds from multiple LiDARs placed at different locations on a car or integrating
    images taken from multiple cameras with different orientations [[59](#bib.bib59)].
    However, combining different view representations of one LiDAR point cloud, such
    as integrating a Bird’s-Eye-View (BEV)and a range view of LiDAR point cloud, is
    not considered multi-view integration since it does not introduce new information.
    These view transformations are considered part of the pre-processing of sensed
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-modality integration involves integrating data from different types of
    sensors, such as cameras, LiDAR, and Millimeter Wave Radar (MMW-Radar), in any
    combination. Research has shown that fusing camera and LiDAR data is one of the
    most common types of integration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-frame integration involves integrating data over time, while the former
    two dimensions focus on spatial integration at one time step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given these three dimensions, we could classify all data integration methods
    into seven upper-level categories, as shown in Figure [1a](#S1.F1.sf1 "In Figure
    1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). Data integration cannot only be implemented within
    one of three dimensions, but also be carried out over multiple dimensions. The
    former is denoted as single-dimensional integration.
  prefs: []
  type: TYPE_NORMAL
- en: b) What is the order to integrate?
  prefs: []
  type: TYPE_NORMAL
- en: 'Multidimensional integration, such as “multi-modality and multi-view,” is different
    from single-dimensional integration as it requires multiple combinations of operations.
    We further categorize multidimensional integration into lower-level integration
    paradigms based on the integration order, as illustrated in Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system"). For instance, in two-dimensional “multi-modality
    and multi-view” integration, there are three sub-level categories or paradigms:
    modality-first integration, view-first integration, and deeply coupled integration.
    Three-dimensional integration is even more complex, and to our knowledge, the
    corresponding paradigms have not been discussed in existing studies. Therefore,
    we limit our discussion of relevant literature to one- or two-dimensional integration
    in Section [III](#S3 "III Data Integration: What to Integrate ‣ A survey on deep
    learning approaches for data integration in autonomous driving system").'
  prefs: []
  type: TYPE_NORMAL
- en: I-2 When to integrate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The topic of “when to integrate” pertains to the level of data abstraction
    at which integration occurs. This has been extensively covered in literature,
    with most adopting a widely used categorization scheme based on the input data
    abstraction stage. This scheme includes data-level (or pixel, signal, or low-level),
    feature-level (middle-level), and decision-level (or result or high-level) fusion
    [[2](#bib.bib2), [12](#bib.bib12), [14](#bib.bib14), [58](#bib.bib58)]. Data-level
    integration methods fuse raw or preprocessed data before feature extraction, while
    feature-level integration combines extracted features at intermediate neural network
    layers. Decision-level integration involves merging the output data separately
    estimated by each sensor. In addition to these levels of integration, a new category
    of “when-to-integrate,” called multi-level integration, has emerged in recent
    years as more studies attempt to integrate data at different levels [[28](#bib.bib28)].
    An example of multi-level integration is using result-level 2D bounding boxes
    from RGB images to select data-level or feature-level 3D LiDAR data [[60](#bib.bib60),
    [61](#bib.bib61), [62](#bib.bib62)]. Each level of data integration has its own
    advantages and disadvantages, and no evidence supports one level being superior
    to another [[14](#bib.bib14)]. In Section [IV](#S4 "IV Data Integration: When
    to Integrate ‣ A survey on deep learning approaches for data integration in autonomous
    driving system"), we review existing integration methods based on these four categories
    and summarize their pros and concons.'
  prefs: []
  type: TYPE_NORMAL
- en: I-3 How to integrate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The question of “how to integrate” pertains to the specific integration operations
    used to mathematically combine data. These operations are often overlooked or
    not explicitly stated since they are typically simple and straightforward, such
    as addition or concatenation. However, in our survey, we present several common
    integration operations, including projection, concatenation, addition/average
    mean/weighted summation, probabilistic method, rule-based transaction, temporal
    integration approaches, and neural network/encoder-decoder structure. We provide
    a summary of each integration operation’s properties and practical applicationions.
  prefs: []
  type: TYPE_NORMAL
- en: Purposes and paper organization
  prefs: []
  type: TYPE_NORMAL
- en: Our survey aims to offer a comprehensive overview of the latest (from 2017 to
    2023) deep learning-based data integration techniques for camera, LiDAR, and MMW-Radar
    in ADS perception, following the previously mentioned taxonomy. After reviewing
    hundreds of pertinent papers, we identify limitations in existing integration
    methods and present a discussion addressing the open question of the ideal data
    integration approach for ADS perception.
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Task | Sensor | Data representation | Integration operation |
    Method Pros & Cons | Dataset | Calibration |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[11](#bib.bib11)] | Various | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fayyad et al. [[2](#bib.bib2)] | Various | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Cui et al. [[28](#bib.bib28)] | Various |  | $\boldsymbol{\checkmark}$ |  |
    $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Yeong et al. [[12](#bib.bib12)] | OD | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[4](#bib.bib4)] | Various |  | $\boldsymbol{\checkmark}$ |  |  |
    $\boldsymbol{\checkmark}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[14](#bib.bib14)] | OD & SS | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ |  | $\boldsymbol{\checkmark}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[58](#bib.bib58)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[15](#bib.bib15)] | OD | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | Various | $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |
    $\boldsymbol{\checkmark}$ | $\boldsymbol{\checkmark}$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Brief summary of recent reviews on data integration in ADS perception
    using deep learning approaches. “OD” represents object detestion, and “SS” means
    semantic segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [I](#S1.T1 "TABLE I ‣ I-3 How to integrate ‣ I Introduction ‣ A survey
    on deep learning approaches for data integration in autonomous driving system"),
    we briefly compare the content covered by related surveys on deep learning-based
    data integration with ours. The key contributions of our survey are summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose three fundamental dimensions (multi-view, multi-modality, multi-frame)
    for data integration, based on which a new bi-level taxonomy is created. The upper-level
    taxonomy outlines the content to integrate with seven categories, while the lower
    level examines the order of integration. This bi-level taxonomy allows for a comprehensive
    categorization of data integration approaches.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize the common integration operations used in deep learning models,
    as well as their pros and cons, which are often not fully explored in other reviews.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on our review, we provide a detailed summary and analysis of the limitations
    of existing integration techniques in ADS perception and suggest our vision for
    the ideal data integration approach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows: In Section [II](#S2 "II
    Sensing modalities and pre-processing ‣ A survey on deep learning approaches for
    data integration in autonomous driving system"), we discuss the measuring principles,
    characteristics, advantages, and disadvantages of three types of sensors commonly
    used in ADS perception and their representations. We then offer an overview of
    “what to integrate,” “when to integrate,” and “how to integrate” in Section [III](#S3
    "III Data Integration: What to Integrate ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), Section [IV](#S4 "IV Data
    Integration: When to Integrate ‣ A survey on deep learning approaches for data
    integration in autonomous driving system"), and Section [V](#S5 "V Data Integration:
    How to Integrate ‣ A survey on deep learning approaches for data integration in
    autonomous driving system"), respectively. In Section [VI](#S6 "VI Case Study
    ‣ A survey on deep learning approaches for data integration in autonomous driving
    system"), we present a case study illustrating the practical applications of these
    three questions. In the final section, we summarize our work and discuss ideal
    data integration approaches and future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: II Sensing modalities and pre-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Commonly used sensors in ADS perception can be classified into two groups according
    to their operational principles [[12](#bib.bib12), [28](#bib.bib28)]:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exteroceptive sensors, such as cameras, LiDARs, and MMW-Radar s, actively collect
    data of surroundings to perceive the external environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proprioceptive sensors, including IMU, wheelmeter, GPS receiver, etc., are mainly
    used to capture the internal states of vehicles and the dynamic measurements of
    the system. From the perspective of tasks, these sensors are widely employed together
    with exteroceptive sensors for positioning and localization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The data collected by the aforementioned sensors vary in terms of coordinate
    frames and characteristics, each with their own strengths and limitations. Integrating
    data from different coordinate frames can be challenging, and many existing deep
    learning architectures are designed to process specific data representations.
    To address this issue, a common approach is to apply pre-processing methods to
    transform raw data into appropriate representations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we focus primarily on three types of exteroceptive sensors,
    discussing their properties and pre-processing methods (representations). Table [II](#S2.T2
    "TABLE II ‣ II Sensing modalities and pre-processing ‣ A survey on deep learning
    approaches for data integration in autonomous driving system") provides a summary
    of the advantages and disadvantages of these sensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Comparison of camera, LiDAR, and radar sensors.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sensor | Data Format | Resolution | HFoV | Geometry | Texture | Bad weather
    | Dim/Dark | Velocity | Cost |'
  prefs: []
  type: TYPE_TB
- en: '| Camera | 2D pixels | ++ | + | - | ++ | + | - | - | Low |'
  prefs: []
  type: TYPE_TB
- en: '| LiDAR | 3D points | + | ++ | ++ | - | + | ++ | - | High |'
  prefs: []
  type: TYPE_TB
- en: '| Radar | 3D points | + | + | + | - | ++ | ++ | ++ | Low |'
  prefs: []
  type: TYPE_TB
- en: '++: Comparatively has strong capability. +: Has limited capability. -: Comparatively
    has weak capability.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Camera
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cameras are optical devices capable of capturing 2D visual images. While some
    cameras (e.g., infrared) can detect invisible light, the term “camera” usually
    refers to those sensing visible light. Cameras can generate both grayscale and
    colored images, although most modern cameras default to producing colored images.
    Some cameras (e.g., gated, time of flight (TOF), structured light) emit waves
    and detect their responses, but we do not focus on these as they operate similarly
    to LiDAR. Instead, this section concentrates on RGB cameras, the most common optical
    sensors, which passively receive visible light with wavelengths between 400 and
    700 nm and output colored images.
  prefs: []
  type: TYPE_NORMAL
- en: In general, cameras can be modeled with a pinhole model. Each point in 3D space
    is projected to a pixel according to an affine transformation determined by the
    projection matrix. This projection is related to both the camera’s intrinsic properties
    and its pose in the 3D world. They can be described by two affine transformations
    determined by the intrinsic and extrinsic matrices, respectively. Most cameras
    have only one lens, called monocular, while others, called stereo, may have multiple
    lenses. Stereo cameras mimic human binocular vision and can perceive 3D objects
    with algorithms. Monocular cameras can also recover 3D information by filming
    the same object with multiple cameras or from multiple poses. The main difference
    between these two types is that the relative positions of lenses and directions
    are fixed and known inside a stereo camera but are unknown and require further
    estimation for monocular systems.
  prefs: []
  type: TYPE_NORMAL
- en: RGB cameras aim to reproduce images perceived by human eyes and are essential
    for capturing colored and textured regions (e.g., road lanes, traffic signs, and
    traffic lights). These cameras generally have very high spatial (hundreds to thousands
    in height and width) and temporal (dozens to hundreds of frames per second) resolutions.
    They often have a long working range, with the maximum perception distance reaching
    1 km in good weather conditions. Their ability to capture color and texture information
    aids semantic comprehension. Moreover, cameras have low energy and manufacturing
    costs, allowing widespread deployment. However, RGB cameras have limitations in
    lighting conditions and line-of-sight visibility. Due to their working principle,
    they struggle to detect lightless objects (e.g., road lanes and obstacles) in
    poorly lit scenes and have weak occluded object detection capabilities. Additionally,
    single-frame images taken by monocular cameras lack geometric information, requiring
    multiple images and complex algorithms (e.g., depth recovery and 3D reconstruction)
    for depth and 3D structure estimation. Cameras are also vulnerable to external
    environments, as the lens may be blurred by liquid (e.g., rain, spray, and wheel
    splash), and distant objects may not be recognizable in fog.
  prefs: []
  type: TYPE_NORMAL
- en: In ADS, the integrated camera data are represented in either 2D or 3D formats.
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Pixel representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The representation of pixels involves storing pixel features on a 2D image plane,
    where each pixel has multiple channels to describe its properties. This results
    in the entire image being stored in a 3D matrix with dimensions of $[height,\
    width,\ channel]$. Typically, RGB raw images have three colored channels, but
    other cameras may have different channels such as depth, gray, infrared, or gated
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Point or voxel representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The representation of points or voxels takes into account the depth information
    by projecting each pixel into 3D space. These 3D points can be stored as either
    point clouds or voxel grids. Point clouds assign each point a float number 3D
    coordinate, resulting in a matrix with dimensions of $[n,\ c+3]$, where $n$ represents
    the number of pixels and $c$ represents the number of channels. Voxel grids divide
    the space into grids with dimensions of $[height,\ width,\ depth]$, and points
    are placed into these grids.
  prefs: []
  type: TYPE_NORMAL
- en: II-B LiDAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Light Detection and Ranging, a.k.a. LiDAR, is a technique commonly used for
    range measurements in autonomous driving [[63](#bib.bib63)]. Its working principle
    is to estimate the time intervals between emitted light pulses and received signals
    reflected by target objects, and obtain the distances with the time intervals
    and light speed.
  prefs: []
  type: TYPE_NORMAL
- en: Three types of LiDAR, including 1D, 2D, and 3D LiDAR, are used to collect different
    amount of environment information [[12](#bib.bib12)]. While 1D LiDAR can only
    provide measurement of distance, 2D LiDAR can obtain the spatial information on
    an X-Y coordinate horizontal plane of the target by spinning certain degrees horizontally.
    The horizontal degree a LiDAR sensor rotates over is called the Horizontal Field
    of View (HFoV)of the sensor. 3D LiDAR sensors expand the vertical view by firing
    multiple lasers vertically, making the data collected in the 3D X-Y-Z coordinate
    system. 3D LiDAR sensors are more commonly employed in autonomous vehicles, while
    the high price is the concern in implementation [[11](#bib.bib11), [64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: The process of generating data using LiDAR sensors involves the use of beams
    of light to draw samples from the surfaces of objects in the surrounding environment.
    This laser-firing working principle allows LiDAR sensors to function well in low-visibility
    conditions but makes them susceptible to external weather conditions such as rain,
    fog, snow, and dusty environments [[65](#bib.bib65)]. Additionally, the color
    of the target can impact the performance of LiDAR sensors, with darker-colored
    objects absorbing light and being less reflective than lighter-colored ones [[66](#bib.bib66)].
    The sampling range of the scene is determined by the HFoV and Vertical Field of
    View (VFoV), while other parameters such as horizontal/vertical resolution and
    fragment per second (FPS)contribute to the data intensity. Horizontal and vertical
    resolution refer to the density of sampling in space, with smaller resolution
    resulting in denser sampling given fixed HFoV and VFoV. FPS describes the sampling
    density in time, i.e., how many scans the LiDAR conducts per second. Detailed
    specifications of different LiDAR sensors from different companies are provided
    by Yeong et al. [[12](#bib.bib12)].
  prefs: []
  type: TYPE_NORMAL
- en: Unlike camera images, 3D LiDAR measurements are a set of irregular and unordered
    data points, referred to as point clouds in 3D structure [[28](#bib.bib28)]. To
    fit the input format of different deep learning models, point clouds can be transformed
    into several different representations using pre-processing methods. It is worth
    noting that LiDAR data is sparser compared to image data.
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Point representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 3D point clouds obtained from LiDAR sensors can be processed without format
    transformation with point processing deep learning networks such as PointNet [[67](#bib.bib67)],
    PointNet++ [[68](#bib.bib68)], PointCNN [[69](#bib.bib69)], and KPConv [[70](#bib.bib70)].
    LiDAR point clouds can be integrated with similar point-format data such as other
    LiDAR point clouds [[71](#bib.bib71)]. Though point clouds retain the original
    information and may provide larger receptive field [[68](#bib.bib68)], the volume
    of point clouds can be huge, requiring high computation power to process [[58](#bib.bib58),
    [28](#bib.bib28)]. Moreover, it is hard to integrate point clouds with other data
    formats such as images. Due to these two points, representations with additional
    pre-processing methods are developed and progressed rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Voxel representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Voxels are generated by dividing the whole 3D space into small regular 3D grids
    and partitioning the original points into corresponding grid based on the geometry.
    This gridization transforms irregular points into regular voxel representation,
    and make it possible to down-sample the original LiDAR points to reduce input
    volume. In fact, the volume and the resolution of the voxels can be adjusted by
    changing the grid size. Larger grids result in more information loss while smaller
    grids may still bring burdens to computation. Several 3D convolution methods can
    be used to process voxels and extract features, such as 3D ShapeNet [[72](#bib.bib72)],
    VoxelNet [[73](#bib.bib73)], and VoxNet [[74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B3 Pixel/View representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pixel or view representation of LiDAR points is to convert 3D point clouds into
    2D image views by projection. Bird’s-Eye-View (BEV)and range views (also known
    as perspective views) are two common types of views on different 2D view planes
    that can be transformed from point clouds to. Pixel representation can leverage
    the existing well-developed CNN-family image processing methods, though the 3D
    geometry information retained in original point clouds may be lost in the projection
    process. The pixel representation, or projection technique, is commonly adopted
    when integrating LiDAR point clouds with camera images [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B4 Integrated representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the previously mentioned representations, some researchers have
    attempted to combine various representations to obtain richer information or before
    further processing. They have developed point-voxel integration methods that merge
    the advantages of both representations. Point-Voxel CNN (PVCNN) [[78](#bib.bib78)]
    employs a dual-branch system consisting of a voxel-based branch and a point-based
    branch. It carries out convolutions on voxels while supplementing detailed geometry
    information from points. PV-RCNN [[79](#bib.bib79)] generates 3D proposals from
    voxel convolutions and selects a few key points in the space to serve as connections
    between voxel features and the refinement network for these proposals. Furthermore,
    different views obtained from point clouds can be integrated. Zhou et al. [[59](#bib.bib59)]
    propose a Multi-View Fusion (MVF) Network that converts Bird’s-Eye-View (BEV)and
    perspective views into voxels, fusing them together so that the complementary
    information can be used effectively. There are also studies integrating all aforementioned
    representations. In M3DETR [[80](#bib.bib80)], the point, voxel, and pixel representations
    are processed with PointNets, VoxelNet, and 2D ConvNets modules respectively,
    and fused with transformers at multiple scales. This design better extracts information
    in raw data by effectively exploiting the correlation between different representations
    leveraging the attention structure.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Millimeter wave radar (MMW-radar)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Radio Detection and Ranging, a.k.a. radar or MMW-Radar, is a technique that
    relies on radiating electromagnetic millimeter waves and scattered reflections
    to estimate the range information about targets [[12](#bib.bib12)]. Short-range,
    medium-range, and long-range radar detectors have different detection distances,
    and are commonly used for collision avoidance and obstacle detection.
  prefs: []
  type: TYPE_NORMAL
- en: Different from LiDAR sensors and cameras that are easily affected by external
    conditions, radar sensors are more robust in extreme weather or dim light [[81](#bib.bib81),
    [82](#bib.bib82), [55](#bib.bib55)]. Another significant advantage of this type
    of low-price sensor is its capability of accurately detecting the velocity of
    dynamic targets based on the Doppler effect, which is very important for perception
    tasks in autonomous driving scenarios. However, radars also have some disadvantages.
    Comparing with cameras, radars lack texture or semantic information. Comparing
    with LiDAR sensors, radars have lower angle resolution (see Table 4 in [[12](#bib.bib12)]
    for detailed configurations). Therefore, radars are not suitable for tasks such
    as object recognition, and may have troubles when distinguish static and stationary
    objects [[11](#bib.bib11), [12](#bib.bib12)]. Besides, the clutters, i.e., the
    unwanted echoes in electronic systems, may cause false detections and performance
    issues in radar systems [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: According to Wang et al. [[58](#bib.bib58)] and Zhou et al. [[55](#bib.bib55)],
    the data format of radar can be divided into raw data, cluster-layer data, and
    object-layer data according to different pre-processing stages. The raw output
    of radar is in the form of time frequency spectrograms. To improve its utility,
    signal processing methods such as those detailed presented in Zhou et al. [[83](#bib.bib83)]
    are often necessary. More commonly adopted radar data formats in autonomous driving
    applications are cluster-layer obtained after operating clustering algorithms,
    and object-layer after filtering and tracking. Comparing with the original raw
    data, the latter two formats provide more sparse and less noisy information.
  prefs: []
  type: TYPE_NORMAL
- en: Two different representations of radar signals can be found in ADS related research.
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Point representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point-based representation is to represent and process radar data as point clouds
    [[84](#bib.bib84)]. However, as raised by Wang et al. [[58](#bib.bib58)], the
    properties of radar point clouds differ from LiDAR point clouds, thus issues may
    arise when directly using LiDAR models on radar points.
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Map representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another representation is map-based, which is to accumulate radar data over
    several time-stamps and generate radar grid BEV maps [[14](#bib.bib14)]. Since
    grid maps alleviate the radar data sparsity issue, image processing networks such
    as CNN are used to extract features and get static environment classification.
    More details of different grid maps can be found in [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: 'III Data Integration: What to Integrate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we review and summarize the techniques with respect to the
    content to integrate by following the bi-level taxonomy presented in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). Specifically, we first summarize each one-dimensional
    integration as shown in Zone 1, 2, and 3 in Figure [1a](#S1.F1.sf1 "In Figure
    1 ‣ I Introduction ‣ A survey on deep learning approaches for data integration
    in autonomous driving system"). We then provide discussions for two-dimensional
    categories (Zone 4, 5, 6 in Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ I Introduction
    ‣ A survey on deep learning approaches for data integration in autonomous driving
    system")), with an additional focus on the order to integrate (Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ I Introduction ‣ A survey on deep learning approaches for data
    integration in autonomous driving system")). As research related to three-dimensional
    integration (Zone 7 in Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ I Introduction ‣
    A survey on deep learning approaches for data integration in autonomous driving
    system")) is rare, we omit three-dimensional integration in this section.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Multi-view integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Under our definition, the most common type of multi-view data integration is
    to integrate multi-view camera images captured by multiple cameras from different
    directions (e.g., 6 monocular cameras installed on self-driving vehicles). This
    camera multi-view integration can be used to generate BEV representation of the
    surrounding environment [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87)]
    or assist object detection in 2D images [[88](#bib.bib88)]. A stereo camera can
    also generate multi-view images as it has two or more lens [[89](#bib.bib89)].
    Besides visual sensors, researchers also explore alternative methods to fuse multiple
    LiDARs’ measurements mounted on a vehicle for object detection [[71](#bib.bib71),
    [90](#bib.bib90)]. Due to high expense of LiDARs, however, this approach is not
    widely adopted in industry and relevant works are limited. Thus we skip the discussion
    of Lidar multi-view integration. Similarly, multi-radar integration is also considered
    as helpful in 360-degree environmental perception [[91](#bib.bib91), [92](#bib.bib92)],
    while they are more commonly integrated with other sensor modalities in applications.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Camera multi-view
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most monocular RGB cameras can only capture objects within a frustum whose shape
    is determined by camera lens. Besides, many objects inside the frustum cannot
    be observed in images if occluded by others. Thus, we need to have multiple images
    shot from different positions to describe the 3D world more thoroughly. The depth
    information (distance between a point in 3D space and the camera’s image plane)
    can be recovered from multi-view cameras given their relative camera parameters
    (intrinsic and extrinsic transformation matrix) based on the consistency and uniqueness
    of spatial geometry. Intrinsic parameters describe the mapping from 2D camera
    coordinate to 2D image coordinate, and extrinsic ones describe the mapping from
    3D world coordinate to 2D camera coordinate. If images are taken from two paired
    cameras, it is called binocular depth estimation or stereo matching, or disparity
    estimation. If more than two views are provided, it is called multi-view stereo
    (MVS).
  prefs: []
  type: TYPE_NORMAL
- en: 'The inputs of most camera multi-view methods [[93](#bib.bib93), [89](#bib.bib89),
    [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102),
    [103](#bib.bib103)] are RGB images, but there are also papers [[104](#bib.bib104),
    [105](#bib.bib105), [106](#bib.bib106)] fusing RGB and thermal images, and papers
    [[85](#bib.bib85)] fusing RGB and depths. We categorize camera multi-view inputs
    as following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) Images: These images are captured by different cameras or by a monocular
    camera but from different positions and angles. The number of images is not limited,
    but at least two images are required. The images should be overlapped but not
    totally the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'b) Depth images: A 2D image with one channel, where each pixel denotes the
    camera’s distance to the corresponding 3D point. They can be obtained from either
    depth cameras or depth estimation algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'c) Thermal images: A 2D image with one channel, where each pixel denotes the
    intensity of infrared radiation emitted by an object. They can be captured by
    thermal cameras.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Radar multi-view
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Very few papers investigate integration among multiple radars. [[107](#bib.bib107)]
    uses three kinds of views: range-Doppler (RD), angle-Doppler (AD), and range-angle
    (RA). The RD view reveals the distance and velocity of objects. Specifically,
    non-moving objects respond at zero Doppler when the radar is stationary, and objects
    moving relative to the radar respond at nonzero Doppler. The AD view shows the
    direction and speed of objects. RA view represents the relationship between range
    and angle. All the views can be represented as 2D images.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Multi-modality integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since sensors have their own advantages and shortcomings as shown in Section [II](#S2
    "II Sensing modalities and pre-processing ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), multi-modality data integration
    is expected to utilize the mutual supplementary between different sensors and
    achieve improved accuracy [[20](#bib.bib20)]. Various methods have been developed
    and applied to tasks including object detection and tracking, depth completion,
    and segmentation. A few studies employ camera-radar integration [[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111), [112](#bib.bib112)]
    or LiDAR-radar integration [[113](#bib.bib113)] for object detection. Several
    research further integrate all these three sensors together for environmental
    perception [[114](#bib.bib114), [115](#bib.bib115)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, we summarize the input of existing multi-modality integration
    research for four multi-modality combinations: 1) camera and LiDAR, 2) camera
    and radar, 3) LiDAR and radar, and 4) camera, LiDAR, and radar. As for the remained
    combination of modalities, there is few research on them.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Camera and LiDAR integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since 2D camera images and 3D LiDAR points are in different coordinate frame,
    they usually need to be transformed to the same space before integration. In the
    following, we present discussions of 2D space integration and 3D space integration,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: a) 2D space integration
  prefs: []
  type: TYPE_NORMAL
- en: Two sub-approaches can be identified according to different LiDAR projected
    planes. One approach is to integrate LiDAR and camera views in the same plane
    [[116](#bib.bib116)]. In other words, the LiDAR points are projected onto the
    image plane for integration. For example, Caltagirone et al. [[117](#bib.bib117)]
    transforms LiDAR points to dense LiDAR images by projecting LiDAR points to image
    plane and up-sampling. The LiDAR images then can be integrated with RGB images
    for road segmentation. Berrio et al. [[118](#bib.bib118)] projects LiDAR points
    to image with mask techniques and probabilistic distribution to handle occlusions
    in semantic mapping. Object detection model, EPNet [[119](#bib.bib119)], projects
    LiDAR points to image plane for multiple times in order to enhance the LiDAR point
    features with corresponding image semantic information. Cheng et al. [[120](#bib.bib120)]
    transforms LiDAR points to right and left lens of stereo, respectively, and integrate
    across both modality and lens to get dense depth. Models of [[121](#bib.bib121),
    [122](#bib.bib122), [123](#bib.bib123)] integrate RGB images with sparse depth
    map generated from LiDAR points for image depth completion. [[124](#bib.bib124),
    [125](#bib.bib125), [126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129), [130](#bib.bib130)] also take RGB and LiDAR-generated depth
    image as inputs and return classification, bounding boxes, or mask proposals.
    There is also a study [[131](#bib.bib131)] expresses and fuses LiDAR features
    in the same coordinate system as the camera. [[132](#bib.bib132)] converts LiDAR
    points to range images of size $(5,w,h)$ before integration with RGB images.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to integrate views in different planes. For example, RGB
    image and BEV map generated by LiDAR projection are integrated in [[76](#bib.bib76),
    [133](#bib.bib133), [134](#bib.bib134)] with deep learning architectures. These
    works connect image feature map to BEV with different transformation or association
    methods. In [[135](#bib.bib135)], four LiDAR projection maps, including height
    bird view (HBV), intensity bird view (IBV), distance center view (DCV), and intensity
    center view (ICV), are integrated with RGB images and original LiDAR 3D points.
  prefs: []
  type: TYPE_NORMAL
- en: b) 3D space integration
  prefs: []
  type: TYPE_NORMAL
- en: One way to achieve 3D space integration is by projecting image data to 3D space
    and thus generate “pseudo-LiDAR points”. As RGB images lack depth estimates, a
    significant portion of these works leverage stereo or RGB-D data to obtain accurate
    geometry information. [[136](#bib.bib136), [137](#bib.bib137)] integrate LiDAR
    point cloud and pseudo-LiDAR point cloud generated from image depth completion
    for 3D object detection. Another way to integrate LiDAR and camera data in 3D
    space is via data association. 3D object detection models such as [[62](#bib.bib62),
    [61](#bib.bib61), [138](#bib.bib138)] project 2D bounding boxes obtained in RGB
    image to 3D space to get frustums as regions of interest to guide 3D point or
    feature searching. VPFNet [[139](#bib.bib139)] leverages virtual points from 3D
    proposals as bridge to associate image features and aggregate LiDAR point features.
    In LoGoNet [[140](#bib.bib140)], image features are associated with LiDAR generated
    3D voxel features by projecting voxel point centroid using camera projection matrix
    and generate reference points in the image plane.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Camera and radar integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to LiDAR-RGB integration, camera and radar can also be integrated in
    either 2D space or 3D space. Examples of 2D space integration include [[110](#bib.bib110),
    [141](#bib.bib141)], in which radar points are projected onto the 2D plane of
    camera’s perspective view. In [[142](#bib.bib142)], two-channel (radar cross section
    and range channels) radar image is generated with the same size as the visual
    image and combined with image feature maps with an extended VGG network. In 3D
    space integration, image information need to be converted to 3D space. RGB image
    in [[143](#bib.bib143)] is first processed by CNN for feature extraction, and
    then converted to 3D points via back-projection. Radar and RGB can also be combined
    with data association. For example, CenterFusion [[112](#bib.bib112)] projects
    2D object bounding boxes to 3D space to connect image information with 3D pillars
    generated from radar points.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 LiDAR and radar integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since both LiDAR and radar data capture 3D geometry information by nature, they
    are usually integrated in 3D space. Some researches integrate raw data of the
    two sensors in a point-by-point way. For example, [[144](#bib.bib144)] combines
    LiDAR and radar points based on heuristic rules to remove low-quality data affected
    by external environmental factors. LiDAR and radar points can also be combined
    via data association. [[145](#bib.bib145)] relies on radar detection to obtain
    frustums as regions of interest to filter relevant LiDAR points.
  prefs: []
  type: TYPE_NORMAL
- en: III-B4 Camera, LiDAR, and radar integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Few studies are trying to integrate data from three sensors together for perception.
    The key process of the integration is still to transform the data from different
    sensors into the same space. Camera, LiDAR, and radar can be integrated in 2D
    space if LiDAR and radar points are projected to camera’s plane. The input of
    the integration neural network in [[146](#bib.bib146)] is camera image layers
    concatenated with projected LiDAR and radar channels. The sensors can also be
    integrated in higher dimensional space. [[147](#bib.bib147)] integrate RGB images,
    LiDAR points, and radar points by aggregating data information from each sensor
    together to generate high dimensional points before further processing.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Multi-frame integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-frame data, i.e., temporal data, refer to the data sampled at multiple
    timestamps along timeline. Compared to single-frame data, integration of multi-frame
    data brings more information, and can potentially improve the accuracy of environmental
    perception. In this subsection, we focus on the inputs of single-sensor multi-frame
    integration. As few studies can be found on radar temporal integration, we only
    present details on camera image sequence and LiDAR point cloud sequence integration.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Camera image sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Camera multi-frame integration refers to fusing information of image sequences
    instead of a single image. Two types of data are commonly used to carry out the
    integration. One is the sequence of feature maps generated from each image, and
    the other is the sequence of processed information obtained from each pair of
    consecutive images.
  prefs: []
  type: TYPE_NORMAL
- en: a) Sequence of feature maps generated from each image
  prefs: []
  type: TYPE_NORMAL
- en: A traditional two-stage temporal integration approach is to perform feature
    extraction operations corresponding to different tasks in each image frame and
    then create association or fusion cross frames. For example, in 2D MOT models
    such as [[148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150), [151](#bib.bib151),
    [152](#bib.bib152), [98](#bib.bib98), [99](#bib.bib99)], 2D object detection bounding
    boxes and feature vectors/maps are first generated in each image, and then associated
    in different frames. Zhou et al. [[153](#bib.bib153)] conduct tracking in a similar
    way, yet they introduce the center of feature map for each image as an additional
    input. The same two-stage pattern can also be observed in 3D monocular MOT, while
    geometry or depth information is additionally incorporated [[154](#bib.bib154),
    [155](#bib.bib155)]. Multi-frame integration of visual odometry (VO) algorithms
    also decouple feature extraction and temporal integration. For example, [[156](#bib.bib156),
    [157](#bib.bib157)] use CNN to extract features and RNN to integrate information
    from different frames. Monocular depth estimation also takes RGB image sequence
    as inputs to extract information and integrate over time [[158](#bib.bib158),
    [159](#bib.bib159), [160](#bib.bib160)].
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage of this two-stage approach is that the results of the later
    stage may be affected by the earlier stage. To avoid this issue, some studies
    leveraging Transformer to integrate both spatial and temporal information for
    multi-frame image MOT have emerged in recent years [[161](#bib.bib161), [162](#bib.bib162),
    [163](#bib.bib163)].
  prefs: []
  type: TYPE_NORMAL
- en: b) Image pairs or sequence of processed information obtained from image pairs
  prefs: []
  type: TYPE_NORMAL
- en: Raw image pairs, specifically a preceding image and a current image, can directly
    be input into integration algorithms. For example, monocular depth estimation
    methods [[164](#bib.bib164), [165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167),
    [168](#bib.bib168)] utilize reference and target frame pairs for self-supervision.
    In these methods, features of the preceding image are re-projected to the view
    of target image with predicted depth and pose, thus a re-projection loss can be
    generated. In [[103](#bib.bib103)], image pairs from adjacent frames or different
    cameras can both be treated are images taken from different viewpoints with relative
    positions, and thus be used for weak supervision. Successive image pairs can also
    be exploited in knowledge distillation. By utilizing a three-level knowledge distillation
    method, Chen et al. [[169](#bib.bib169)] train the student model to incorporate
    adjacent frames which allows it to acquire a greater understanding of comprehensive
    representation knowledge from its corresponding teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the outputs of multiple image pairs can further be fused temporally.
    In other words, this approach consists of two stages. First, raw image pairs are
    combined respectively to get corresponding fused outputs. Then, the fused output
    sequence are integrated over time. One typical example of this approach is to
    generate optical flow from each pair of raw images and then integrate sequence
    of optical flows to, for instance, get or assist pose estimation [[170](#bib.bib170),
    [171](#bib.bib171), [172](#bib.bib172), [157](#bib.bib157), [152](#bib.bib152)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 LiDAR scan sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LiDAR multi-frame integration refers to the fusion of LiDAR scan sequence.
    As LiDAR scans have different representations, two types of LiDAR scan sequence
    input can be found in existing works: point cloud sequence, and view sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: a) Point cloud sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'Point cloud sequence refers to multiple 3D LiDAR point clouds measured in continuous
    time series. Similar to image sequence integration, two-stage approaches can be
    applied here: first to extract features or obtain target objects from each 3D
    point cloud, and then to create association between LiDAR scans. In [[173](#bib.bib173),
    [174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179)], the inputs of temporal integration are
    the sequences of extracted features or other forms of outcomes generated from
    LiDAR point clouds to achieve 3D object tracking. Another approach is to generate
    a denser point cloud by combining multiple LiDAR scans in order to enhance the
    data quality. This is usually done via data alignment, which means to align multiple
    point clouds to a unified coordinate frame. Wang et al. the conduct the cross-frame
    integration by aligning and fusing “thing class” data points from multiple consecutive
    LiDAR scans to create an enriched point cloud [[180](#bib.bib180)]. The enriched
    point cloud is then treated as single-frame input for later processing stages
    such as sampling, training, and segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: b) View sequence
  prefs: []
  type: TYPE_NORMAL
- en: Another type of input for LiDAR temporal integration is view sequence, which
    is a sequence of 2D view representations generated from 3D point clouds via projection.
    Since this approach convert 3D data sequence to 2D format, image-based temporal
    integration approaches can be leveraged for processing. [[181](#bib.bib181)] projects
    3D points scans to range view images for semantic segmentation. The view image
    sequences (specifically, the feature maps extracted from views) are served as
    the input of temporal integration. LO-SLAM [[182](#bib.bib182)] converts LiDAR
    3D point clouds to cylinder views with cylinder projection. This model takes view
    pairs generated from pair-wise scans as input to infer relative 6-DoF pose.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Multi-view multi-modality integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Researchers explore possible approaches to introduce multiple views into camera
    and LiDAR/radar integration to enhance the perception capability. Several works,
    including [[183](#bib.bib183), [184](#bib.bib184), [120](#bib.bib120), [185](#bib.bib185),
    [135](#bib.bib135), [186](#bib.bib186), [187](#bib.bib187)], integrate LiDAR with
    multiple camera images from different views. Some others such as [[135](#bib.bib135),
    [188](#bib.bib188), [189](#bib.bib189)] integrate multi-view LiDARs and monocular
    camera. MVFusion [[190](#bib.bib190)] propose a multi-view radar and multi-view
    camera fusion approach for 3D object detection. A mainstream pattern can be observed
    from these studies: first intra-modality (to fuse multiple views of a single modality),
    and then inter-modality (to fuse information from different modalities). Besides,
    research is also being conducted where multi-modality fusion is carried out first,
    followed by multi-view combination. In [[191](#bib.bib191)], LiDAR points are
    integrated with monocular camera to obtain single frame 3D point cloud at each
    view. Then multiple 3D point clouds obtained are incrementally spliced for excessive
    3D point cloud reconstruction with global optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also research that fuses all three modalities: camera, LiDAR, and
    radar. For example, in [[192](#bib.bib192)], the raw LiDAR inputs are in the form
    of point clouds, and features to be integrated are in the form of a multi-scale
    BEV. As for radar modality, the raw inputs are properties of radar points (such
    as location, speed, and intensity), and the features to be integrated are per-point
    hidden representations extracted by MLPs. Besides radar and LiDAR data, inputs
    also contain images from different views and corresponding camera parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: III-E Multi-view multi-frame integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-view multi-frame integration introduces temporal relation to multi-view
    fusion process. Most papers focus on adding time-series information into camera
    multi-view integration, and few works investigate LiDAR or radar multi-view. Therefore,
    we only present a brief discussion on camera multi-view multi-frame integration
    algorithms in this subsection.
  prefs: []
  type: TYPE_NORMAL
- en: BEV is an appropriate representation for multi-view camera data, thus some multi-view
    multi-frame papers adopt it into the integration pipeline. In [[193](#bib.bib193),
    [194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196), [197](#bib.bib197),
    [198](#bib.bib198), [199](#bib.bib199), [200](#bib.bib200), [201](#bib.bib201)],
    images from different views are fused into a BEV representation, further propagating
    information along the temporal dimension for perception and prediction. It can
    be observed that in general, these algorithms tend to prioritize the step of spatial
    integration (multi-view) over temporal integration (multi-frame) in the fusion
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Besides BEV, many works use regular image representations for temporal view
    fusion. [[202](#bib.bib202)] and [[203](#bib.bib203)] use view pair sequences
    from stereo cameras to recover object scale or track objects. As stereo cameras
    are not always available, a great number of papers [[204](#bib.bib204), [172](#bib.bib172),
    [182](#bib.bib182), [205](#bib.bib205), [206](#bib.bib206), [157](#bib.bib157),
    [207](#bib.bib207), [208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210),
    [211](#bib.bib211), [212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214),
    [197](#bib.bib197), [215](#bib.bib215)] utilize neighboring frames to form image
    pairs. These papers mainly target SLAM and reconstruction, which require multiple
    views to recover geometry.
  prefs: []
  type: TYPE_NORMAL
- en: III-F Multi-modality multi-frame integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-modality multi-frame integration introduces a temporal aspect to the
    spatial integration process for enhanced perception performance. Current research
    on camera and LiDAR sequence integration is primarily focused on Mutiple Object
    Tracking (MOT)and 3D object detection. For instance, 3D MOT models combine features
    or detected objects from LiDAR and camera to obtain instances in a single frame,
    followed by a data association process for multi-frame tracking [[216](#bib.bib216),
    [217](#bib.bib217), [218](#bib.bib218), [219](#bib.bib219)]. The Transformer-based
    3D object detection model proposed by Zeng et al. [[220](#bib.bib220)] generates
    gridwise BEV images with RGB and LiDAR points, and then applies multi-sensor temporal
    integration operations given the BEV grids. These integration algorithms typically
    follow a two-stage paradigm: first, spatial integration (multi-modality fusion
    in a single frame), and then temporal integration (multi-frame association) [[221](#bib.bib221),
    [222](#bib.bib222), [218](#bib.bib218)].'
  prefs: []
  type: TYPE_NORMAL
- en: There is limited research on the integration of LiDAR and radar point sequences.
    In one study [[113](#bib.bib113)], 2D regions of interest are detected in the
    BEV s converted from LiDAR and radar 3D points. The features are first integrated
    spatially according to the corresponding regions in LiDAR and radar BEV s and
    then temporally across frames, following the two-stage spatial-temporal integration
    pattern observed in camera and LiDAR integration.
  prefs: []
  type: TYPE_NORMAL
- en: There are few studies related to the temporal integration of camera, LiDAR,
    and radar combinations, possibly due to the complexity of the integration. Existing
    studies break down the multi-sensor temporal integration task in various ways.
    For example, in one study [[223](#bib.bib223)], Extended Kalman Filter (EKF)is
    deployed to each sensor respectively, and a reliability function is used for cross-modality
    integration. In another study [[115](#bib.bib115)], the inputs of integration
    vary in different tasks, with camera images and LiDAR depth maps fused for 2D
    segmentation and object detection, while LiDAR and radar points over time are
    used for 3D obstacle detection and tracking. Liang et al. [[224](#bib.bib224)]
    propose a loosely-coupled integration architecture for vehicle state estimation
    based on Error-state EKF, where camera, LiDAR, and radar data are used as observations
    to correct the estimated priori state.
  prefs: []
  type: TYPE_NORMAL
- en: 'IV Data Integration: When to Integrate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since deep learning networks extract features with multiple neural layers, integration
    strategies can take place at different stages and in various ways [[14](#bib.bib14)].
    Broadly speaking, the strategies are commonly classified into early (data-level),
    middle (feature-level), and late stages (decision-level). Data-level integration
    methods involve the fusion of raw or preprocessed data before feature extraction.
    Feature-level integration, on the other hand, combines extracted features at intermediate
    neural network layers. Decision-level integration involves the merging of output
    data separately estimated by each sensor. As features can be obtained at different
    depths of a neural network, several detailed integration paradigms have been designed
    and applied for feature-level integration. Feng et al. [[14](#bib.bib14)] further
    classifies the middle integration into three patterns, including fusion in one
    layer, deep fusion, and short-cut fusion.
  prefs: []
  type: TYPE_NORMAL
- en: However, as described in Section [I-2](#S1.SS0.SSS2 "I-2 When to integrate ‣
    I Introduction ‣ A survey on deep learning approaches for data integration in
    autonomous driving system"), this method fails to classify methods where integrated
    features are not at the same level (depth) of data abstraction, limiting its applicability
    to fit the increased amount of fusion schemes. Cui et al. [[28](#bib.bib28)] proposes
    an approach to incorporate more integration circumstances by introducing “multi-level”
    integration in addition to the original three classes. Thus, we review the related
    works with respect to these four categories in this section.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Data-level integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data-level integration is popular in SLAM [[202](#bib.bib202), [170](#bib.bib170),
    [204](#bib.bib204), [172](#bib.bib172), [182](#bib.bib182), [207](#bib.bib207),
    [144](#bib.bib144)], but rare in other tasks. [[170](#bib.bib170)] uses optical
    flow to exploit relationship between frames to learn poses and uses cost volume
    between frames to learn depth maps. [[204](#bib.bib204), [172](#bib.bib172), [182](#bib.bib182)]
    utilize convolutions to generate feature vectors. [[202](#bib.bib202)] stacks
    multi-frames together to regress pose. [[207](#bib.bib207)] regresses poses with
    convolutions. Fusing data in the early stage is also employed in detection [[225](#bib.bib225),
    [127](#bib.bib127), [143](#bib.bib143), [145](#bib.bib145)], depth completion
    [[123](#bib.bib123)] and semantic segmentation [[180](#bib.bib180)]. However,
    a large percent of these approaches lead to noncompetitive results.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Feature-level integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature-level integration is widely adopted in most areas including object detection,
    depth completion [[122](#bib.bib122), [120](#bib.bib120)], semantic segmentation
    [[87](#bib.bib87), [130](#bib.bib130), [226](#bib.bib226), [106](#bib.bib106),
    [105](#bib.bib105), [227](#bib.bib227), [181](#bib.bib181)], tracking [[221](#bib.bib221),
    [217](#bib.bib217), [149](#bib.bib149), [222](#bib.bib222), [150](#bib.bib150)],
    3D reconstruction [[97](#bib.bib97), [100](#bib.bib100)], SLAM [[228](#bib.bib228),
    [156](#bib.bib156), [229](#bib.bib229), [171](#bib.bib171), [199](#bib.bib199),
    [169](#bib.bib169)], action classification [[227](#bib.bib227), [230](#bib.bib230)]
    and navigation [[114](#bib.bib114)]. The popularity of feature-level integration
    comes from the extracted features, which are more compact and representative than
    the raw data. Nevertheless, feature-level merging has the adverse effect of diluting
    the single modalities’ strengths. The majority of 2D and 3D object detection methods
    fuse in feature level [[133](#bib.bib133), [136](#bib.bib136), [76](#bib.bib76),
    [124](#bib.bib124), [119](#bib.bib119), [231](#bib.bib231), [232](#bib.bib232),
    [59](#bib.bib59), [233](#bib.bib233), [130](#bib.bib130), [125](#bib.bib125),
    [134](#bib.bib134), [129](#bib.bib129), [112](#bib.bib112), [113](#bib.bib113),
    [88](#bib.bib88), [192](#bib.bib192), [234](#bib.bib234), [197](#bib.bib197),
    [235](#bib.bib235), [199](#bib.bib199), [236](#bib.bib236), [237](#bib.bib237),
    [238](#bib.bib238), [220](#bib.bib220), [177](#bib.bib177), [131](#bib.bib131),
    [142](#bib.bib142), [132](#bib.bib132), [140](#bib.bib140), [201](#bib.bib201),
    [187](#bib.bib187)]. [[133](#bib.bib133)] uses deep parametric continuous convolution
    layers with MLPs to directly output target feature in each BEV grid. [[140](#bib.bib140)]
    integrates LiDAR voxel features and image features with attention designs. Studies
    [[86](#bib.bib86), [193](#bib.bib193), [195](#bib.bib195), [196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198), [199](#bib.bib199)] focusing on spatiotemporal
    fusion in BEV map generation integrate features of surrounding images or BEV feature
    maps at different timestamps with various designs of neural networks. In [[136](#bib.bib136)],
    features from both cloud points are grid-wisely combined based on learned weights.
    Besides, most 3D reconstruction methods [[209](#bib.bib209), [239](#bib.bib239),
    [240](#bib.bib240), [241](#bib.bib241), [242](#bib.bib242), [243](#bib.bib243),
    [244](#bib.bib244), [245](#bib.bib245)] integrate at feature level because of
    two reasons. On the one, the extracted features are more stable to environmental
    changes, e.g., lighting, which are crucial to photo-consistency measurement. Thus,
    it is less suitable to directly integrate raw data. On the other hand, the 3D
    estimates obtained from each single view is ambiguous in scale, which makes integrating
    at decision level challenging.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Decision-level integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decision-level integration has wide applications in many areas including semantic
    segmentation [[85](#bib.bib85)], reconstruction [[246](#bib.bib246), [118](#bib.bib118)],
    object detection, tracking, SLAM and annotation [[77](#bib.bib77)]. Among all
    these domains, it is most popular in object detection [[247](#bib.bib247), [248](#bib.bib248),
    [109](#bib.bib109), [126](#bib.bib126), [110](#bib.bib110), [71](#bib.bib71),
    [185](#bib.bib185), [249](#bib.bib249), [250](#bib.bib250), [250](#bib.bib250)],
    tracking [[216](#bib.bib216), [148](#bib.bib148), [176](#bib.bib176), [175](#bib.bib175),
    [154](#bib.bib154), [251](#bib.bib251), [252](#bib.bib252), [111](#bib.bib111),
    [89](#bib.bib89), [250](#bib.bib250)] and SLAM [[253](#bib.bib253), [254](#bib.bib254),
    [255](#bib.bib255), [224](#bib.bib224), [256](#bib.bib256), [223](#bib.bib223),
    [98](#bib.bib98), [99](#bib.bib99), [152](#bib.bib152)]. Decisions from different
    sources can be either fused with learning-based strategies or non-learning-based
    ones.[[85](#bib.bib85)] adds up all input features from multiple views to generate
    fused features. [[246](#bib.bib246)] fuses the depth maps and uncertainty maps
    according to the weighted scheme, then output the tensor after weighted addition.
    [[247](#bib.bib247)] transforms cooperative awareness messages (CAM) to relative
    measurements, then uses buffering strategies to deal with time synchronization
    in out-of-sequence measurements (OOSM). [[248](#bib.bib248)] fuses each pair of
    2D bounding box (predicted from image) and 3D bounding box (projected to image)
    candidate by first generating a new tensor based on IOU and confidence, then further
    fuse with convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Multi-level integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-level integration is adopted in lots of applications including object
    detection, depth competition [[121](#bib.bib121)], semantic segmentation [[75](#bib.bib75)],
    [[128](#bib.bib128)], tracking, SLAM and 3D reconstruction [[208](#bib.bib208),
    [191](#bib.bib191)]. Most multi-level integration methods are designed for object
    detection and tracking. There are also quite a percent of papers focusing on SLAM.
    [[139](#bib.bib139)] fuses feature level information from the camera and data-level
    information from the LiDAR. [[62](#bib.bib62)] and [[61](#bib.bib61)] leverage
    the outputs of the image object detector as 2D proposals, which are then projected
    to form 3D searching space for 3D object detection. To specify, [[146](#bib.bib146)]
    projects LiDAR and radar information into 2D maps and use convolutions to fuse
    them with camera RGB. [[257](#bib.bib257)] fuses features and raw data fusion
    via concatenation. [[141](#bib.bib141)] fuses decision level information from
    the camera and data level information from the radar. [[258](#bib.bib258)] has
    one input at the object level and another at the feature level. [[121](#bib.bib121)]
    uses independent branches to extract features of images and LiDAR point cloud,
    and fuse (addition or concatenate) at multiple levels. [[208](#bib.bib208)] warps
    the former depth map prediction to current view’s hidden representation according
    to geometry. [[153](#bib.bib153)] uses tracking conditioned detector to detect
    objects in new frame (conditioned on previous frame and object detection result
    of previous frame) to get a temporally coherent set of detected objects, where
    the 2D displacement is predicted to associate detection results through time.
    [[173](#bib.bib173)] uses multiple faces features to refine locations. [[259](#bib.bib259)]
    firstly uses optical flow to get short tracklets, then uses pixelwise depth estimation
    based on camera ego-motion to get 3D motion, finally uses 3D motion consistency
    to get long term tracklets. [[155](#bib.bib155)] uses detected object position
    (decision-level) and feature vector from detected mask (feature-level) to associate
    frames, and uses nearby targets (neighbors) to constrain matching IoU distribution.
    [[218](#bib.bib218)] fuses 3D-2D detection by backproject 3D to 2D, and fuses
    frames by appearance association based on raw image and 2D/3D motion relation
    based on raw image and pointcloud. [[146](#bib.bib146)] fuses RGB features and
    raw data from LiDAR and radar. [[147](#bib.bib147)] fuses the region predicted
    by camera and raw data from LiDAR and radar.
  prefs: []
  type: TYPE_NORMAL
- en: 'V Data Integration: How to Integrate'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The “how to integrate” component of data integration involves the mathematical
    operations used to combine data/features. Feng et al. [[14](#bib.bib14)] summarize
    the fusion operations utilized in deep learning networks into four categories:
    addition/average mean, concatenation, ensemble, and mixture of experts. The characteristics
    of different integration operations are not further elaborated in their study.
    Wang et al. [[15](#bib.bib15)] categorize data combination approaches for 3D object
    detection into two primary types: alignment and fusion. Alignment is further subcategorized
    into projection-based and model-based methods, while fusion is divided into learning-based
    and learning-agnostic approaches. The authors of the present paper believe that
    alignment also serves as a means of fusion in many circumstances, and the model-based
    alignment and learning-based fusion techniques showcase significant overlaps with
    each other. Besides, since both [[14](#bib.bib14)] and [[15](#bib.bib15)] focus
    on methodologies for specific perception tasks (object detection and semantic
    segmentation), other integration operations such as EKF [[185](#bib.bib185), [154](#bib.bib154)]
    or probabilistic map [[89](#bib.bib89), [118](#bib.bib118)] are not discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: In Table [V](#A2.T5 "TABLE V ‣ Appendix B ‣ A survey on deep learning approaches
    for data integration in autonomous driving system"), we provide a brief summary
    of several commonly used operations in deep learning-based data integration processes,
    including projection, concatenation, addition-similar operations (addition/weighted
    summation/average mean), probabilistic methods, rule-based transaction, temporal
    integration approaches, and encoder-decoder methods. The application of integration
    operations may be highly dependent on the task, inputs, and data abstraction levels.
    Some operations are more versatile and applicable in various conditions, while
    others are more specific and implemented for certain purposes. Additionally, an
    integration algorithm may involve multiple different operations at various stages.
  prefs: []
  type: TYPE_NORMAL
- en: V-1 Projection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Projections, such as 3D-to-2D projection and 2D-to-3D back-projection (also
    known as “reverse projection”), are commonly used to connect images in 2D space
    and point clouds in 3D space, allowing for data to be operated within the same
    domain. 3D-to-2D projection reduces the dimensionality of 3D objects to a 2D plane,
    while 2D-to-3D back-projection may rely on geometric cues or depth information.
    Projections are frequently utilized to generate different data representations
    [[15](#bib.bib15)]. For example, pixel-based view representations of LiDAR points
    and point-based pseudo-LiDAR point representation of images, as mentioned in Section [II-B](#S2.SS2
    "II-B LiDAR ‣ II Sensing modalities and pre-processing ‣ A survey on deep learning
    approaches for data integration in autonomous driving system"), are outputs of
    projection and back-projection, respectively. Additionally, projection can also
    serve as an integration operation to combine data.
  prefs: []
  type: TYPE_NORMAL
- en: As an integration operation, projection may take place at raw-data level, feature
    level, or decision level.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw-data level integration refers to projecting LiDAR or radar point clouds
    onto 2D plane such that they can be combined with corresponding RGB image information
    [[136](#bib.bib136), [118](#bib.bib118), [141](#bib.bib141), [77](#bib.bib77),
    [124](#bib.bib124), [130](#bib.bib130), [260](#bib.bib260)]. An example is LATTE
    [[77](#bib.bib77)], where the points are projected onto the RGB image plane at
    the beginning of the sensor fusion pipeline. Similarly, LiDAR points are projected
    onto the image to obtain segmentation information in PointPainting [[124](#bib.bib124)].
    In [[191](#bib.bib191)], dense 3D points depth map is generated via projecting
    3D LiDAR points to the corresponding semantic 2D image followed by interpolation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-level projection is to project features extracted from LiDAR or radar
    points to 2D space for integration. This integration usually takes place in the
    middle layers of a neural network. An example is EPNet [[119](#bib.bib119)], where
    point features and image semantic features are combined in multiple scales in
    2D domain. In LoGoNet’s global and local fusion modules [[140](#bib.bib140)],
    voxel point centroids or center points are projected to the image plane to generate
    reference points to sample image features, and utilizes attention structure for
    integration. A problem for raw-data and feature level projections is how to deal
    with the resolution consistency between LiDAR and camera branches. In [[130](#bib.bib130)],
    point clouds are projected onto extracted RGB feature maps which have lower resolution
    to avoid discarding image information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision-level 3D outcomes are projected to 2D space for integration in some
    other works. For example, in [[216](#bib.bib216)], 3D bounding boxes obtained
    from point clouds are projected to 2D image plane, such that the original 3D bounding
    boxes and 2D bounding boxes generated from image can be associated based on box
    overlap in image domain. [[110](#bib.bib110)] projects radar points to image plane,
    and integrates radar predictions (slices in this paper) and camera predictions
    based on box overlaps. In the early stage of CAMO-MOT [[219](#bib.bib219)], 3D
    detection results of LiDAR point cloud are projected onto the pixel plane of the
    camera to obtain 2D detection results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A few works conduct decision level back-projection to integrate 2D image data
    with 3D points. A common operation for object detection is to back-project 2D
    proposals to 3D space, generating frustums as the regions of interest (RoI) to
    guide 3D point searching and crop clouds [[61](#bib.bib61), [62](#bib.bib62),
    [147](#bib.bib147), [147](#bib.bib147)]. To reduce the irrelevant background information
    wrapped in RoIs, Yang et al. back-projects segmentation masks instead of bounding
    boxes to the point clouds [[261](#bib.bib261)]. Since LiDAR points capture geometric
    information and by nature can easily be clustered to distinguish foreground from
    background, the points belong to background can be projected back onto the image
    to refine 2D segmentation. This types of integration operation based on back-projection
    is called “ensemble” [[14](#bib.bib14)]. However, since this back-projection operation
    integrates LiDAR points and images in a data association or supervision way, it
    usually fails to fully exploit the information in images and is considered as
    “weak fusion” in [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: After reviewing the aforementioned methods, we identify two common issues that
    need to be addressed when integrating data through projection techniques. For
    the first, since multi-modality data are collected with different sampling rates
    and in different coordinates, they must be carefully aligned into a unified coordinate
    before projection and mapping. This process heavily relies on the sensor extrinsic
    and intrinsic matrices, which may be vulnerable to parameter errors. For the second,
    how to deal with the resolution inconsistency may significantly impact the performance
    of the integration [[130](#bib.bib130)]. In existing works, these two issues are
    usually not fully explored or discussed.
  prefs: []
  type: TYPE_NORMAL
- en: V-2 Concatenation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Concatenation is an operation widely used to combine data or feature maps at
    different layers of a neural network [[139](#bib.bib139), [121](#bib.bib121),
    [122](#bib.bib122), [109](#bib.bib109), [262](#bib.bib262), [114](#bib.bib114),
    [106](#bib.bib106), [93](#bib.bib93), [263](#bib.bib263), [264](#bib.bib264),
    [184](#bib.bib184), [116](#bib.bib116), [97](#bib.bib97), [131](#bib.bib131)].
    Concatenation operation can either be in the way of stacking feature maps along
    the depth as additional channels, or be added to the end of flattened vectors.
    For example, [[127](#bib.bib127)] and [[115](#bib.bib115)] convert LiDAR point
    clouds to gray images, and concatenate with RGB image as additional channels.
    Similarly, the projected and processed LiDAR and radar channels are concatenated
    with camera images for multiple times in [[146](#bib.bib146)]. In [[217](#bib.bib217)],
    the image feature vector is obtained by concatenating motion and appearance feature
    vectors obtained from ResNet [[265](#bib.bib265)]. Then the image and LiDAR features
    are integrated again by concatenation. Qi et al. [[142](#bib.bib142)] combine
    camera and radar information by concatenating two-channel radar image with same-sized
    64-channel visual image feature maps. MVFusion [[190](#bib.bib190)] designs a
    semantic-aligned radar encoder (SARE) in which semantic indicator produced via
    all stages’ visual features are concatenated with radar inputs. In C2FNet’s [[97](#bib.bib97)]
    fine generation module, concatenation takes place to integrate global guidance
    features with point cloud features in different levels and in different views.
    In GARNet [[100](#bib.bib100)], the feature map of each view is concatenated with
    its deviation from the global feature maps in post merger block. One study [[117](#bib.bib117)]
    presents trials of early fusion approach in which LiDAR and camera data can be
    concatenated in depth dimension, and late fusion approach in which the multi-modality
    features are concatenated. Besides, concatenation can be used as a straightforward
    temporal fusion approach. In [[198](#bib.bib198), [197](#bib.bib197)], BEV features
    of previous frame are spatially aligned and concatenated with the ones of the
    current frame.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation in depth dimension requires the inputs to be in the same spatial
    size. Specifically, the width and height of feature maps to be concatenated together
    should be the same, while the number of channels can be different. For example,
    in MVFusion [[190](#bib.bib190)], raw radar points are preprocessed into a representation
    with the same shape as images before concatenation, by first extending them to
    pillars and then projecting pillars to the corresponding image view. Because of
    this, concatenation can be used to combine data from more than two data sources.
    In [[114](#bib.bib114)], information from camera, LiDAR, radar, and localization
    and mapping are concatenated together in perception module. In [[178](#bib.bib178)],
    input features pairs for cross-frame aggragation are the concatenation outcomes
    of template feature, reference frame feature, and reference frame point.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation is a straightforward operation with numerous applications, particularly
    as a fundamental combination technique in neural networks. The output format of
    concatenation closely resembles the input format, although the dimension might
    alter (for example, an increase in the number of channels or vector length). Nonetheless,
    concatenation has certain limitations in practice, apart from the stringent requirement
    of equal spatial size. Firstly, the input dimension of concatenation is typically
    inflexible. Consider a camera-LiDAR integration model where 3-channel LiDAR data
    is combined with 3-channel RGB data (3 RGB channels initially, followed by 3 LiDAR
    channels). If one sensor fails, the model will not function correctly because
    the integrated data’s dimension decreases from 6-channel to 3-channel, making
    it incompatible with downstream modules. Secondly, concatenation input sensors
    are not interchangeable. If the order of the two sensors is reversed, the model
    will not produce accurate results. As a result, concatenation struggles with varying
    input dimensions and is not ideal for integration systems that utilize interchangeable
    sensors.
  prefs: []
  type: TYPE_NORMAL
- en: V-3 Addition-similar operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Addition, weighted sum, and average mean are mathematical operations used to
    combine two data sources by adding them together with pre-determined weights.
    These operations can be applied to values, feature vectors, or feature maps in
    an element-wise way. An example of integration with addition can be found in [[85](#bib.bib85)],
    where the authors add all features up in the multi-view fusion module. Element-wise
    summation is also implemented in [[137](#bib.bib137), [266](#bib.bib266)]. As
    shown in the residual fusion module in [[262](#bib.bib262)], multiple feature
    maps from LiDAR and camera are integrated together via element-wise mean. One
    disadvantage of addition and average mean is that they combine different data
    sources with equal importance. Weighted sum makes it possible to give different
    weights to different inputs. In the depth map refinement stage of the model in
    [[246](#bib.bib246)], the depth maps and a uncertainty map are fused according
    to a weighted scheme. In [[136](#bib.bib136)], the authors predict a pair of weights
    for each pair of the LiDAR and image features, thus the features can be integrated
    considering the reliability of the information. In [[100](#bib.bib100)], score
    maps are generated in merger blocks as weights when fusing feature maps and when
    fusing the reconstructed voxels from all view images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition-similar operations are one of the simplest way for integrating features.
    While these operations are simple to implement, it is required that the two parties
    to be added to have the same spatial size and depth, i.e. to be in the exactly
    same format. The format of the output after these operations is also the same
    as the format of each input side. This is an even harder constraint comparing
    with concatenation: two feature maps can be concatenated if they can be added,
    otherwise not. This enables some researchers to compare addition and concatenation
    in their integration models [[121](#bib.bib121), [217](#bib.bib217), [222](#bib.bib222)].
    Another disadvantage of addition and average mean is that both parties to be combined
    have same weights, no matter how confident the respective data are. This may cause
    problems when one sensor fails to work or has low data quality. Also, as raised
    in [[14](#bib.bib14)], while addition and average mean help to achieve high average
    precision, the network may fail in corner cases thus have low robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: V-4 Probabilistic methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Probabilistic integration methods incorporate uncertainties into the integration
    process in different ways. Berrio et al. [[118](#bib.bib118)] assign a vector
    to represent probabilities of different semantic classes to each LiDAR point when
    doing back-projecting. Another segmentation research [[267](#bib.bib267)] also
    uses a probabilistic method to help select the optimal segmentation result from
    independent outputs of image and LiDAR. [[89](#bib.bib89)] and [[71](#bib.bib71)]
    utilize Bayesian rules to obtain the confidence of object detection after integration.
    [[255](#bib.bib255), [111](#bib.bib111), [221](#bib.bib221)] leverage Joint Probabilistic
    Data Association (JPDA) filter based on Kalman Filter (KF)methods for system matching
    and state updating in tracking. Different from KF methods that are usually used
    to update the status of a single track given an observation, JPDA takes all measurements
    and tracks in the scene into account with joint probabilities in order to obtain
    a better estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating uncertainties in the integration pipeline is essential for autonomous
    driving applications. This mechanism helps the system to assess risks and adjust
    confidences of choosing trustworthy sensory information for decision making. With
    this, the integration system can be more robust to different external environment
    and varying sensor data quality. However, they also have more parameters to be
    estimated and thus are usually applied to comparatively simple models.
  prefs: []
  type: TYPE_NORMAL
- en: V-5 Rule-based transaction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are a few researchers adopt handcrafted rules to guide the integration.
    [[248](#bib.bib248)] encodes each detected 2D and 3D bounding box pairs into a
    4-channel tensor, while the 4 elements of the tensor are chosen and designated
    carefully. [[144](#bib.bib144)] presents a method to integrate radar and LiDAR
    measurements/features by replacing low-quality LiDAR points affected by fog with
    radar data. To achieve this, the authors propose a set of manually-designed heuristic
    rules according to different conditions. Wang et al. [[147](#bib.bib147)] generate
    a 7-dimensional frustum by integrating RGB and LiDAR, and 8-dimensional points
    based on manually selected metrics of radar point clouds. Though these integration
    operations are proved to be effective in these papers, they may hardly be implemented
    in another scenario. Comparing with other integration operations, aforementioned
    rule-based ones are dedicatedly designed for one task or dataset, thus have poor
    generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: V-6 Temporal integration approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Non-deep-learning approaches for temporal information integration such as KF
    and relevant extensions (e.g., EKF, Unscented Kalman Filter) have long been the
    mainstream methods for multi-frame data association in localization and tracking
    [[216](#bib.bib216), [148](#bib.bib148), [174](#bib.bib174), [176](#bib.bib176),
    [175](#bib.bib175), [252](#bib.bib252), [111](#bib.bib111), [268](#bib.bib268),
    [154](#bib.bib154), [255](#bib.bib255), [224](#bib.bib224), [115](#bib.bib115),
    [223](#bib.bib223), [152](#bib.bib152), [269](#bib.bib269)]. In general, the pattern
    of tracking with KF methods is to first utilize feature extractor or detector
    to get the information of the object to track, and then employ KF models to integrate
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning approaches with comparatively intuitive and interpretable structure,
    such as RNN family (e.g., RNN, LSTM/ConvLSTM, GRU/ConvGRU) and 3D CNN, are designed
    to leverage temporal information [[270](#bib.bib270), [159](#bib.bib159), [160](#bib.bib160),
    [271](#bib.bib271), [158](#bib.bib158), [113](#bib.bib113), [181](#bib.bib181)].
    Similar to KF, the pattern of temporal integration with RNN models also has two
    stages: first single-frame object detection with various methods, and then temporal
    integration with RNN family models. ODMD proposed by Griffin et al. [[270](#bib.bib270)]
    utilizes 2D object bounding boxes of a target in two frames and corresponding
    camera motion to estimate the target’s depth with LSTM. [[271](#bib.bib271), [159](#bib.bib159),
    [158](#bib.bib158)] integrate information of previous frames and generate depth
    maps with ConvLSTM. DeepVO uses RNN to fuse extracted features [[156](#bib.bib156)],
    and [[171](#bib.bib171), [157](#bib.bib157)] leverage LSTM to integrate optical
    flows or feature maps in time sequence. Besides, a deep learning Visual Inertial
    Odometry (VIO) method uses CNN and LSTM to get image feature map and atures respectively,
    and then conducts temporal integration with LSTM [[206](#bib.bib206)]. Object
    detection and BEV map generation in [[272](#bib.bib272)] employ LSTM to integrate
    temporal information. In [[195](#bib.bib195), [196](#bib.bib196)], BEV map grids
    are also temporally fused and updated with recurrent network designs. 3D CNN method
    is extended from traditional 2D CNN by stacking 2D feature maps at different timestamps
    together and utilizing a 3D convolution kernel to extract and integrate spatio-temporal
    information. Different from RNN models where the spatial and temporal feature
    extractions are conducted in two independent stages, 3D CNN kernels can extract
    and exchange spatial and temporal information simultaneously. This structure has
    been used for video image processing, recognition, and analysis in many different
    realms [[273](#bib.bib273), [274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)].
    In autonomous driving area, Qian et al. [[113](#bib.bib113)] propose MVD-Net,
    a 3D CNN network combining LiDAR and radar signals, to detect vehicles via spatio-temporal
    integration. In BEVerse [[199](#bib.bib199)], BEV features generated at consecutive
    timestamps are warpped and aligned to form a stack of temporal block, and fused
    with 3D convolutions.'
  prefs: []
  type: TYPE_NORMAL
- en: KF and its variants, RNN families, and 3D CNN are commonly employed specifically
    for temporal data association and integration, yet they have their own limitations.
    KF methods heavily rely on the manually designed process model and the estimation
    of covariance matrix, which may not be appropriate or accurate. RNN methods tend
    to separate spatial and temporal integration into two independent stages. Though
    this order brings conveniences for model tuning, whether it is the optimal spatial-temporal
    integration paradigm still remains in doubt. Another disadvantage of RNN family
    models is that they usually have poor capability to track the long-term dependency
    and are difficult to train and converge. 3D CNN models integrate spatial-temporal
    information at the same time. However, dense 3D convolution networks usually have
    large number of parameters to be estimated, which may easily result in over-fitting
    when the training set is not large enough, or limited spatial and temporal reception
    fields if the kernel size is small.
  prefs: []
  type: TYPE_NORMAL
- en: V-7 Encoder-decoder methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many researchers exploit neural networks or encoder-decoder architecture models
    to integrate multi-view, multi-modality, and multi-frame sensory data. CNN network
    and its derivations are the most common neural networks used for feature extraction
    and integration. These methods are data driven, which means that they do not rely
    on handcraft-designed models. The kernels and convolution layers by nature can
    integrate spatial information of the input data. In many cases, these networks
    utilize well-designed combinations and procedure of multiple aforementioned integration
    operations and convolution layers [[137](#bib.bib137)]. In [[135](#bib.bib135)],
    the authors first use different sub-networks to extract features from image and
    proposal regions from multiple LiDAR view-representations and the original 3D
    point cloud. Then the proposals are projected into image tensors at multiple layers
    of their neural network. Caltagirone et al. [[117](#bib.bib117)] propose an innovative
    cross-fusion network architecture, in which the two input branches (2D RGB and
    LiDAR images) are connected by trainable scalar cross connections. Specifically,
    at each layer, the input feature tensors of LiDAR and camera are added together
    with learnable weights. [[146](#bib.bib146)] introduces Bayesian Neural Network
    along with concatenation operations in each layer to capture the uncertainties
    with the cost of almost doubled number of parameters to be estimated.
  prefs: []
  type: TYPE_NORMAL
- en: Though CNN models perform well in their respective fields, there are limitations
    for data integration. Firstly, they are also subject to the shortcomings of the
    integration operations included in the model structure. That is to say, for instance,
    a CNN model with concatenation operation in the network still suffers from the
    weakness of concatenation. Secondly, the mechanism of CNN and their limitations
    in the receptive field hinder its ability to perceive and integrate global information.
    Furthermore, the model procedure and structure are unchanged in application, which
    means that the integration is not adaptive to the changing environment.
  prefs: []
  type: TYPE_NORMAL
- en: Besides CNNs, an increasing amount of researchers set their sights on attention-based
    models (e.g., Transformer) for feature combination. Chen et al. [[192](#bib.bib192)]
    integrates camera, LiDAR, and radar data with Transformer for 3D object detection.
    A set of queries encoding 3D locations can be projected to the corresponding input
    space of each sensor and get their features. DETR3D [[88](#bib.bib88)] proposes
    a 3D object detector utilizing Transformer to integrate multi-view image information.
    LoGoNet [[140](#bib.bib140)] integrate multi-modality data with attention blocks
    by setting LiDAR features as Q (query) and image features as K (key) and V (value).
    In the radar-guided fusion transformer (RGFT) module of MVFusion [[190](#bib.bib190)],
    cross-attention mechanism is introduced to fuse radar and image features. [[76](#bib.bib76)]
    leverages two Transformer decoder layers to produce 3D bounding boxes with queries
    that associate both LiDAR BEV features and image features. BEVSegFormer adopts
    BEV queries to combine multi-camera features in cross-attention module [[101](#bib.bib101)]
    to obtain BEV segmentation results. RI-Fusion [[132](#bib.bib132)] presents an
    attention module that merges range view feature derived from LiDAR data and RGB
    image feature. Specifically, the former is transformed into Q while the latter
    is converted to K and V. FrustumFormer [[201](#bib.bib201)] designs both scene
    queries and instance queries to transform multi-scale multi-view image features
    to a unified BEV feature with cross-attention layers.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, recent trials in various domains start focusing on Transformer
    to integrate spatial and temporal information [[277](#bib.bib277), [278](#bib.bib278),
    [279](#bib.bib279), [193](#bib.bib193), [220](#bib.bib220), [102](#bib.bib102),
    [280](#bib.bib280)]. Theoretically, Transformer have unlimited receptive fields
    and can obtain global information with the Q, K, and V structure. BEVFormer [[193](#bib.bib193)]
    integrate camera image multi-view and multi-frame information with Transformer
    architecture. Decoder of BEVFormer has a temporal self-attention layer where BEV
    queries can interact with historical BEV features, and a spatial cross-attention
    layer where BEV queries interact with features of other camera views. [[220](#bib.bib220)]
    is another example leveraging Transformer to integrate spatial-temporal information.
    The authors process camera and LiDAR features with cross-sensor point-wise attention,
    and integrate grid-wise features from BEV maps obtained at different timestamps
    as 4D tensors. [[161](#bib.bib161), [162](#bib.bib162), [163](#bib.bib163)] propose
    different designs of cross-frame query-key structures for multi-object tracking
    over time given image sequences as inputs. Study [[201](#bib.bib201)] utilize
    deformable cross-attention to aggregate information of history queries into current
    instance queries. Comparing with CNN, Transformer can has significantly larger,
    or global, receptive field. Its flexible model design also makes it possible to
    applied to a wide range of applications. Furthermore, Transformer is also scalable
    to model any data size. Though models utilizing Transformer structure tend to
    be larger and harder to train, its advantages of high structure design freedom
    and cross-domain information interaction architecture make it very suitable for
    data integration. We believe that more diverse Transformer-based spatial-temporal
    integration designs will spring up in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: VI Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Camera | Number of sensors | Working range (meters) | Applicable scenarios
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rearview | 1 | 50 | Objects in the back |'
  prefs: []
  type: TYPE_TB
- en: '| Wide forward | 1 | 60 | Traffic lights, short-distance or crossing objects
    |'
  prefs: []
  type: TYPE_TB
- en: '| Forward-looking side | 2 | 80 | Neighboring or crossing objects |'
  prefs: []
  type: TYPE_TB
- en: '| Rearward-looking side | 2 | 100 | Objects in the neighboring lanes behind
    the car |'
  prefs: []
  type: TYPE_TB
- en: '| Main forward | 1 | 150 | Most cases |'
  prefs: []
  type: TYPE_TB
- en: '| Narrow forward | 1 | 250 | Distant objects |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: The cameras adopted by Tesla.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the prosperity and rapid development of autonomous driving technology
    in recent years, many companies, such as Tesla, Waymo, and General Motors, have
    launched ADS-related products and services via different technical paths. In this
    section, we take Tesla’s vision structure as an example to illustrate data integration
    applications in real-world ADS systems with the “what-when-how” analysis structure
    we described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Tesla builds a pure camera-based system, which consists of eight cameras in
    total. Table [III](#S6.T3 "TABLE III ‣ VI Case Study ‣ A survey on deep learning
    approaches for data integration in autonomous driving system") displays the perception
    range of the mounted cameras. These cameras have the field of view overlapped
    with each other and together cover 360° around the car. The three cameras (main/narrow/wide
    forward) installed behind the windshield help to detect distant objects within
    a broad view angle. Specifically, the wide camera, equipping a 120-degree fish-eye
    lens, is helpful in downtown and crossroads where vehicles drive slow. The narrow
    camera is applicable on the highway. Besides the front sensors, there are also
    cameras observing both sides and the rear of vehicles. The forward-looking side
    cameras with a view angle of 90 degrees are mainly used to detect cars that change
    lanes and objects around crossroads. The rearward-looking side cameras assist
    lane changing. The rear-view camera avoids rear-end collisions and facilitates
    parking.
  prefs: []
  type: TYPE_NORMAL
- en: All the cameras have different directions and are capturing images at certain
    frequencies, so that the integration of Tesla’s autonomous driving system can
    be regarded as multi-view and multi-frame integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the approach we described earlier, Tesla splits multi-view multi-frame
    integration into two steps: first, multi-view spatial integration, and then multi-frame
    temporal integration. Integration between different views and frames is done at
    the feature level. After raw image calibration and multi-scale feature extraction,
    a transformer-like neural network is used to fuse features from different views.
    Specifically, the key and value of the transformer come from the features of the
    images, while the query comes from the position encoding of each point in the
    occupancy space. The occupancy space mentioned here is similar to the BEV space,
    which is a top-down view, but with an additional dimension of height. Each point
    in the occupancy space queries its most relevant corresponding object in other
    views through the attention mechanism. After aggregating the spatial information,
    the model aligns the positions of objects at different times using displacement
    information provided by the odometer for temporal fusion, with features from nearby
    time steps having higher weights. The spatiotemporal features are then passed
    through a set of deconvolutions to obtain high-resolution 3D features in the occupancy
    space for further decision-making.'
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusion and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we review the latest deep learning-based data integration techniques
    for autonomous driving perception, focusing on three types of sensors: cameras,
    LiDAR, and radar. The need for data integration arises from the complementary
    nature of perception capabilities among these sensors. We present a unique perspective
    on integration techniques by examining “what, when, and how to integrate.” Under
    “what to integrate,” we propose a novel taxonomy that classifies data integration
    inputs into seven categories based on three dimensions (multi-view, multi-modality,
    and multi-frame). We observe that most existing research focuses on one-dimensional
    integration, while two-dimensional integration is slowly gaining interest. For
    “when to integrate,” we categorize data integration techniques based on data abstraction
    levels. In “how to integrate,” we not only discuss commonly used data/feature
    combination operations but also comprehensively outline their advantages and drawbacks,
    which is seldom addressed in previous studies. Furthermore, we demonstrate data
    integration techniques using Tesla’s ADS as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By examining related works published within the past five years, we identify
    several patterns or issues in current integration methodologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Spatial alignment based on explicit projection: As discussed in Section [V-1](#S5.SS0.SSS1
    "V-1 Projection ‣ V Data Integration: How to Integrate ‣ A survey on deep learning
    approaches for data integration in autonomous driving system"), projection and
    back-projection are two commonly used integration methods for spatially aligning
    data from different sensors. These methods rely on the geometric relationship
    between sensory measurements and intrinsic and extrinsic parameters for mutual
    projection. However, due to dimension and perspective differences between sensory
    measurements and estimation errors in the transformation matrix, these approaches
    can lead to information loss, redundancy, or cumulative error.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) High specificity but low generalizability: Many existing data integration
    methods/algorithms are designed for specific scenarios, making them sensor-specific,
    data-representation-specific, and task-specific. While this high specificity contributes
    to improved perception performance in their respective domains, it also results
    in low generalizability, limiting the applicability of each integration technique.'
  prefs: []
  type: TYPE_NORMAL
- en: '3) Fixed integration architecture with limited flexibility: Most existing integration
    techniques have fixed structures, which means that the weights, orders, dimensions,
    formats, and depths of the integration models usually do not change once a neural
    network is trained and ready for implementation. This property makes the model
    unable to handle some common practical situations, such as unavailability of data
    from one or several sensors (not sensor pluggable) or changes in the order of
    the input (not input permutation invariant). The unchangeable network is also
    difficult to scale from simple to complex applications, for example, extending
    from two-frame integration to multiple consecutive frames. Moreover, these integration
    architectures cannot automatically adapt to external changes and select the best
    integration method by adjusting their structure and weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comprehensive understanding of the aforementioned shortcomings sheds light
    on required properties of “ideal” approach for data integration:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Task- and modality-agnostic: Developing high-specificity individual integration
    methods for every scenario and task can result in redundant computation. Therefore,
    integration models that can be generalized and applied to a wide range of application
    scenarios is a future direction of development. Though existing research have
    provided much experience of integration, their nature of task-, representation-,
    and modality-specificity may also bring limitations when generalizing to other
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing Task- and modality-agnostic integration approaches, referring to
    data integration techniques without assumptions based on existing heuristic knowledge
    of sensors or different perception tasks, are essential for multi-view, multi-modality,
    and multi-frame integration. Specifically, in task- and modality-agnostic integration,
    the input data representations may not be described by one type of sensors or
    another, and the integration method may not be determined by one task.
  prefs: []
  type: TYPE_NORMAL
- en: '2) Sensor pluggable, permutation invariant, and spatial-temporal scalability:
    In practical applications, information missing may happen to a sensor for various
    reasons. Camera may not be able to take pictures if its lens is temporarily covered
    or contaminated. However, the perception module of the autonomous driving system
    can not fail due to this. An ideal integration approach is expected to provide
    comparatively high-quality integration outcomes even when a portion of information
    is lost. This also means that the data integration approach should be able to
    process data coming from different sensor configurations. In other words, the
    sensors are pluggable for the integration approach.'
  prefs: []
  type: TYPE_NORMAL
- en: To be able to process data coming from different sensor combinations, the integration
    approach needs to be invariant to input permutation, which means that the integration
    and perception performance should not be affected by the order of the inputs (e.g.,
    not affected by whether LiDAR data or image data arrives first).
  prefs: []
  type: TYPE_NORMAL
- en: Spatial-temporal scalability describes the capability of the integration method
    to adapt to increased demand both in space and time. Specifically, this refers
    to the ability of integrating a larger amount of data from more sensors, and integrating
    temporal information from more frames in a wider time window. This capability
    enables self-driving vehicles to deal with complex driving scenarios, in which
    the demand for processing spatial-temporal data is high.
  prefs: []
  type: TYPE_NORMAL
- en: '3) Adaptive: Traffic conditions are changing all the time with their own levels
    of complexity. For example, car following on highways with comparatively simple
    traffic scenes and limited road elements can be processed without complicated
    data integration, while driving through complex and crowded intersections in cities
    may require much more information to process for decision making. Handling perception
    demands of different scenes by integration data in the same way may lead to unnecessary
    computational consumption or errors due to under-computation. Thus, another important
    property of dynamic integration approach is adaptiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: The ideal integration approach is expected to adaptively learn the demands of
    perception according to the complexity of the scene, and thus adjust its structure,
    i.e. activating different neurons, to fit the requirement of different driving
    environments. In low-complexity conditions where part of sensory measurements
    is not needed, these sensors can be “unplugged” from the integration processing
    (not integrated for simplicity). In temporal integration, the “ideal” integration
    approach can also select the appropriate fusion window or key frame according
    to the importance of the previous frames.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Jiaheng Yang from Riemann Laboratory, Huawei
    Technologies, for his inspiring discussions. The authors also thank Qirui Wang
    from Parallel Distributed Computing Laboratory, Huawei Technologies, for collecting
    literature.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Janai, F. Güney, A. Behl, A. Geiger *et al.*, “Computer vision for autonomous
    vehicles: Problems, datasets and state of the art,” *Foundations and Trends® in
    Computer Graphics and Vision*, vol. 12, no. 1–3, pp. 1–308, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Fayyad, M. A. Jaradat, D. Gruyer, and H. Najjaran, “Deep learning sensor
    fusion for autonomous vehicle perception and localization: A review,” *Sensors*,
    vol. 20, no. 15, p. 4220, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. Chen, Z. Jian, Y. Huang, Y. Chen, Z. Zhou, and N. Zheng, “Autonomous
    driving: cognitive construction and situation understanding,” *Science China Information
    Sciences*, vol. 62, no. 8, pp. 1–27, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Huang, B. Shi, X. Li, X. Li, S. Huang, and Y. Li, “Multi-modal sensor
    fusion for auto driving perception: A survey,” *arXiv preprint arXiv:2202.02703*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Chen, B. Wang, C. X. Lu, N. Trigoni, and A. Markham, “A survey on deep
    learning for localization and mapping: Towards the age of spatial machine intelligence,”
    *arXiv preprint arXiv:2006.12567*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Kuutti, S. Fallah, K. Katsaros, M. Dianati, F. Mccullough, and A. Mouzakitis,
    “A survey of the state-of-the-art localization techniques and their potentials
    for autonomous vehicle applications,” *IEEE Internet of Things Journal*, vol. 5,
    no. 2, pp. 829–846, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] E. Marti, M. A. De Miguel, F. Garcia, and J. Perez, “A review of sensor
    technologies for perception in automated driving,” *IEEE Intelligent Transportation
    Systems Magazine*, vol. 11, no. 4, pp. 94–108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H. A. Ignatious, M. Khan *et al.*, “An overview of sensors in autonomous
    vehicles,” *Procedia Computer Science*, vol. 198, pp. 736–741, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. Kolar, P. Benavidez, and M. Jamshidi, “Survey of datafusion techniques
    for laser and vision based sensor integration for autonomous navigation,” *Sensors*,
    vol. 20, no. 8, p. 2180, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
    the kitti vision benchmark suite,” in *2012 IEEE conference on computer vision
    and pattern recognition*.   IEEE, 2012, pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z. Wang, Y. Wu, and Q. Niu, “Multi-sensor fusion in automated driving:
    A survey,” *Ieee Access*, vol. 8, pp. 2847–2868, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] D. J. Yeong, G. Velasco-Hernandez, J. Barry, J. Walsh *et al.*, “Sensor
    and sensor fusion technology in autonomous vehicles: A review,” *Sensors*, vol. 21,
    no. 6, p. 2140, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Morkar, S. Sonawane, A. Mahajan, and S. Shinde, “Autonomous vehicles:
    A survey on sensor fusion, lane detection and drivable area segmentation,” in
    *Computational Intelligence in Data Mining: Proceedings of ICCIDM 2021*.   Springer,
    2022, pp. 695–709.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm,
    W. Wiesbeck, and K. Dietmayer, “Deep multi-modal object detection and semantic
    segmentation for autonomous driving: Datasets, methods, and challenges,” *IEEE
    Transactions on Intelligent Transportation Systems*, vol. 22, no. 3, pp. 1341–1360,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L. Wang, X. Zhang, Z. Song, J. Bi, G. Zhang, H. Wei, L. Tang, L. Yang,
    J. Li, C. Jia *et al.*, “Multi-modal 3d object detection in autonomous driving:
    A survey and taxonomy,” *IEEE Transactions on Intelligent Vehicles*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Q. Li, J. P. Queralta, T. N. Gia, Z. Zou, and T. Westerlund, “Multi-sensor
    fusion for navigation and mapping in autonomous vehicles: Accurate localization
    in urban environments,” *Unmanned Systems*, vol. 8, no. 03, pp. 229–237, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] L. Yu, Y. Duan, and K.-C. Li, “A real-world service mashup platform based
    on data integration, information synthesis, and knowledge fusion,” *Connection
    Science*, vol. 33, no. 3, pp. 463–481, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T. Meng, X. Jing, Z. Yan, and W. Pedrycz, “A survey on machine learning
    for data fusion,” *Information Fusion*, vol. 57, pp. 115–129, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Bleiholder and F. Naumann, “Data fusion,” *ACM computing surveys (CSUR)*,
    vol. 41, no. 1, pp. 1–41, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] D. L. Hall and J. Llinas, “An introduction to multisensor data fusion,”
    *Proceedings of the IEEE*, vol. 85, no. 1, pp. 6–23, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] B. Khaleghi, A. Khamis, F. O. Karray, and S. N. Razavi, “Multisensor data
    fusion: A review of the state-of-the-art,” *Information fusion*, vol. 14, no. 1,
    pp. 28–44, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] G. Velasco-Hernandez, J. Barry, J. Walsh *et al.*, “Autonomous driving
    architectures, perception and data fusion: A review,” in *2020 IEEE 16th International
    Conference on Intelligent Computer Communication and Processing (ICCP)*.   IEEE,
    2020, pp. 315–321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Zhou, X. Hong, and P. Jin, “Information fusion for multi-source material
    data: progress and challenges,” *Applied Sciences*, vol. 9, no. 17, p. 3473, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh, A. Murphy,
    and C. Ryan, “Sensor technology in autonomous vehicles: A review,” in *2018 29th
    Irish Signals and Systems Conference (ISSC)*.   IEEE, 2018, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] D. Gruyer, V. Magnier, K. Hamdi, L. Claussmann, O. Orfila, and A. Rakotonirainy,
    “Perception, information processing and modeling: Critical stages for autonomous
    driving applications,” *Annual Reviews in Control*, vol. 44, pp. 323–341, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran, “Autonomous
    vehicle perception: The technology of today and tomorrow,” *Transportation research
    part C: emerging technologies*, vol. 89, pp. 384–406, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D. Feng, A. Harakeh, S. L. Waslander, and K. Dietmayer, “A review and
    comparative study on probabilistic object detection in autonomous driving,” *IEEE
    Transactions on Intelligent Transportation Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Cui, R. Chen, W. Chu, L. Chen, D. Tian, Y. Li, and D. Cao, “Deep learning
    for image and point cloud fusion in autonomous driving: A review,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Q. Cai, H. Wang, Z. Li, and X. Liu, “A survey on multimodal data-driven
    smart healthcare systems: approaches and applications,” *IEEE Access*, vol. 7,
    pp. 133 583–133 599, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Z. Uddin, M. M. Hassan, A. Alsanad, and C. Savaglio, “A body sensor
    data fusion and deep recurrent neural network-based behavior recognition approach
    for robust healthcare,” *Information Fusion*, vol. 55, pp. 105–115, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] W. Zhang, R. Sengupta, J. Fodero, and X. Li, “Deeppositioning: Intelligent
    fusion of pervasive magnetic field and wifi fingerprinting for smartphone indoor
    localization via deep learning,” in *2017 16th IEEE International Conference on
    Machine Learning and Applications (ICMLA)*.   IEEE, 2017, pp. 7–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] T. Liu, H. Lu, and Z. Wei, “Design and implementation of intelligent window
    control system based on multi-sensor fusion,” in *2019 IEEE 8th Data Driven Control
    and Learning Systems Conference (DDCLS)*.   IEEE, 2019, pp. 1368–1372.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L. Wu, L. Chen, and X. Hao, “Multi-sensor data fusion algorithm for indoor
    fire early warning based on bp neural network,” *Information*, vol. 12, no. 2,
    p. 59, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Guo, X. Li, H. Huang, N. Guo, and Q. Li, “Deep learning-based image
    segmentation on multimodal medical imaging,” *IEEE Transactions on Radiation and
    Plasma Medical Sciences*, vol. 3, no. 2, pp. 162–169, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] M. R. Shahrbabaki, A. A. Safavi, M. Papageorgiou, and I. Papamichail,
    “A data fusion approach for real-time traffic state estimation in urban signalized
    links,” *Transportation research part C: emerging technologies*, vol. 92, pp.
    525–548, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Du, T. Li, X. Gong, and S.-J. Horng, “A hybrid method for traffic flow
    forecasting using multimodal deep learning,” *arXiv preprint arXiv:1803.02099*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Z. Huang, X. Ling, P. Wang, F. Zhang, Y. Mao, T. Lin, and F.-Y. Wang,
    “Modeling real-time human mobility based on mobile phone and transportation data
    fusion,” *Transportation research part C: emerging technologies*, vol. 96, pp.
    251–269, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Guo, B. Zhang, T. Yang, D. Lyu, and W. Gao, “Multitask convolutional
    neural network with information fusion for bearing fault diagnosis and localization,”
    *IEEE Transactions on Industrial Electronics*, vol. 67, no. 9, pp. 8005–8015,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] A. B. Torres, A. R. da Rocha, T. L. C. da Silva, J. N. de Souza, and R. S.
    Gondim, “Multilevel data fusion for the internet of things in smart agriculture,”
    *Computers and Electronics in Agriculture*, vol. 171, p. 105309, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] K. Beard, M. Kimble, J. Yuan, K. S. Evans, W. Liu, D. Brady, and S. Moore,
    “A method for heterogeneous spatio-temporal data integration in support of marine
    aquaculture site selection,” *Journal of Marine Science and Engineering*, vol. 8,
    no. 2, p. 96, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Y. Guo, C. Yin, M. Li, X. Ren, and P. Liu, “Mobile e-commerce recommendation
    system based on multi-source information fusion for sustainable e-business,” *Sustainability*,
    vol. 10, no. 1, p. 147, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] D. Ayata, Y. Yaslan, and M. E. Kamasak, “Emotion based music recommendation
    system using wearable physiological sensors,” *IEEE transactions on consumer electronics*,
    vol. 64, no. 2, pp. 196–203, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Zhang, S. Yan, J. Li, X. Tian, and T. Yoshida, “Credit risk prediction
    of smes in supply chain finance by fusing demographic and behavioral data,” *Transportation
    Research Part E: Logistics and Transportation Review*, vol. 158, p. 102611, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang,
    and U.-X. Tan, “A survey of data fusion in smart city applications,” *Information
    Fusion*, vol. 52, pp. 357–374, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] H. F. Nweke, Y. W. Teh, G. Mujtaba, and M. A. Al-Garadi, “Data fusion
    and multiple classifier systems for human activity detection and health monitoring:
    Review and open research directions,” *Information Fusion*, vol. 46, pp. 147–170,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Qi, P. Yang, L. Newcombe, X. Peng, Y. Yang, and Z. Zhao, “An overview
    of data fusion techniques for internet of things enabled physical activity recognition
    and measure,” *Information Fusion*, vol. 55, pp. 269–280, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Muhammad, F. Alshehri, F. Karray, A. El Saddik, M. Alsulaiman, and
    T. H. Falk, “A comprehensive survey on multimodal medical signals fusion for smart
    healthcare systems,” *Information Fusion*, vol. 76, pp. 355–375, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Q.-V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu, T. Huynh-The
    *et al.*, “Fusion of federated learning and industrial internet of things: A survey,”
    *arXiv preprint arXiv:2101.00798*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] F. Alam, R. Mehmood, I. Katib, N. N. Albogami, and A. Albeshri, “Data
    fusion and iot for smart ubiquitous environments: A survey,” *IEEE Access*, vol. 5,
    pp. 9533–9554, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Singh, R. Singh, and A. Ross, “A comprehensive overview of biometric
    fusion,” *Information Fusion*, vol. 52, pp. 187–205, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] R. Ravindran, M. J. Santora, and M. M. Jamali, “Multi-object detection
    and tracking, based on dnn, for autonomous vehicles: A review,” *IEEE Sensors
    Journal*, vol. 21, no. 5, pp. 5668–5677, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao, “Review the state-of-the-art
    technologies of semantic segmentation based on deep learning,” *Neurocomputing*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Huang and Y. Chen, “Autonomous driving with deep learning: A survey
    of state-of-art technologies,” *arXiv preprint arXiv:2006.06091*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Gupta, A. Anpalagan, L. Guan, and A. S. Khwaja, “Deep learning for
    object detection and scene perception in self-driving cars: Survey, challenges,
    and open issues,” *Array*, vol. 10, p. 100057, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] T. Zhou, M. Yang, K. Jiang, H. Wong, and D. Yang, “Mmw radar-based technologies
    in autonomous driving: A review,” *Sensors*, vol. 20, no. 24, p. 7283, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Wang, J. Zhan, C. Duan, X. Guan, P. Lu, and K. Yang, “A review of vehicle
    detection techniques for intelligent vehicles,” *IEEE Transactions on Neural Networks
    and Learning Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] F. M. Barbosa and F. S. Osório, “Camera-radar perception for autonomous
    vehicles and adas: Concepts, datasets and metrics,” *arXiv preprint arXiv:2303.04302*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Wang, Q. Mao, H. Zhu, Y. Zhang, J. Ji, and Y. Zhang, “Multi-modal 3d
    object detection in autonomous driving: a survey,” *arXiv preprint arXiv:2106.12735*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Zhou, P. Sun, Y. Zhang, D. Anguelov, J. Gao, T. Ouyang, J. Guo, J. Ngiam,
    and V. Vasudevan, “End-to-end multi-view fusion for 3d object detection in lidar
    point clouds,” in *Conference on Robot Learning*.   PMLR, 2020, pp. 923–932.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] D. Xu, D. Anguelov, and A. Jain, “Pointfusion: Deep sensor fusion for
    3d bounding box estimation,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2018, pp. 244–253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for
    3d object detection from rgb-d data,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2018, pp. 918–927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. Paigwar, D. Sierra-Gonzalez, Ö. Erkent, and C. Laugier, “Frustum-pointpillars:
    A multi-stage approach for 3d object detection using rgb camera and lidar,” in
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, 2021,
    pp. 2926–2933.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, and J. Li, “Deep
    learning for lidar point clouds in autonomous driving: A review,” *IEEE Transactions
    on Neural Networks and Learning Systems*, vol. 32, no. 8, pp. 3412–3432, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. Qian, D. Garg, Y. Wang, Y. You, S. Belongie, B. Hariharan, M. Campbell,
    K. Q. Weinberger, and W.-L. Chao, “End-to-end pseudo-lidar for image-based 3d
    object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 5881–5890.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. Kutila, P. Pyykönen, H. Holzhüter, M. Colomb, and P. Duthon, “Automotive
    lidar performance verification in fog and rain,” in *2018 21st International Conference
    on Intelligent Transportation Systems (ITSC)*.   IEEE, 2018, pp. 1695–1701.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Petrovskaya and S. Thrun, “Model based vehicle tracking for autonomous
    driving in urban environments,” *Proceedings of robotics: science and systems
    IV, Zurich, Switzerland*, vol. 34, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
    feature learning on point sets in a metric space,” *Advances in neural information
    processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, “Pointcnn: Convolution
    on x-transformed points,” *Advances in neural information processing systems*,
    vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J.
    Guibas, “Kpconv: Flexible and deformable convolution for point clouds,” in *Proceedings
    of the IEEE/CVF international conference on computer vision*, 2019, pp. 6411–6420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] T. Wu, J. Hu, L. Ye, and K. Ding, “A pedestrian detection algorithm based
    on score fusion for multi-lidar systems,” *Sensors*, vol. 21, no. 4, p. 1159,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3d
    shapenets: A deep representation for volumetric shapes,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2015, pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud based
    3d object detection,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 4490–4499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural network
    for real-time object recognition,” in *2015 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2015, pp. 922–928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] I. Sobh, L. Amin, S. Abdelkarim, K. Elmadawy, M. Saeed, O. Abdeltawab,
    M. Gamal, and A. El Sallab, “End-to-end multi-modal sensors fusion system for
    urban automated driving,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, “Transfusion:
    Robust lidar-camera fusion for 3d object detection with transformers,” *arXiv
    preprint arXiv:2203.11496*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] B. Wang, V. Wu, B. Wu, and K. Keutzer, “Latte: accelerating lidar point
    cloud annotation via sensor fusion, one-click annotation, and tracking,” in *2019
    IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019, pp.
    265–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Liu, H. Tang, Y. Lin, and S. Han, “Point-voxel cnn for efficient 3d
    deep learning,” *Advances in Neural Information Processing Systems*, vol. 32,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-rcnn:
    Point-voxel feature set abstraction for 3d object detection,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 10 529–10 538.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha,
    “M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection
    with transformers,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision (WACV)*, January 2022, pp. 772–782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Zhang and S. Cao, “Real-time human motion behavior detection via cnn
    using mmwave radar,” *IEEE Sensors Letters*, vol. 3, no. 2, pp. 1–4, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Z. Wei, F. Zhang, S. Chang, Y. Liu, H. Wu, and Z. Feng, “Mmwave radar
    and vision fusion for object detection in autonomous driving: A review,” *Sensors*,
    vol. 22, no. 7, p. 2542, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Zhou, L. Liu, H. Zhao, M. López-Benítez, L. Yu, and Y. Yue, “Towards
    deep radar perception for autonomous driving: Datasets, methods, and challenges,”
    *Sensors*, vol. 22, no. 11, p. 4208, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] O. Schumann, M. Hahn, J. Dickmann, and C. Wöhler, “Semantic segmentation
    on radar point clouds,” in *2018 21st International Conference on Information
    Fusion (FUSION)*.   IEEE, 2018, pp. 2179–2186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] B. Pan, J. Sun, H. Y. T. Leung, A. Andonian, and B. Zhou, “Cross-view
    semantic segmentation for sensing surroundings,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 3, pp. 4867–4873, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Q. Li, Y. Wang, Y. Wang, and H. Zhao, “Hdmapnet: A local semantic map
    learning and evaluation framework,” *arXiv preprint arXiv:2107.06307*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] B. Zhou and P. Krähenbühl, “Cross-view transformers for real-time map-view
    semantic segmentation,” *arXiv preprint arXiv:2205.02833*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon,
    “Detr3d: 3d object detection from multi-view images via 3d-to-2d queries,” in
    *Conference on Robot Learning*.   PMLR, 2022, pp. 180–191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] S. Ramos, S. Gehrig, P. Pinggera, U. Franke, and C. Rother, “Detecting
    unexpected obstacles for self-driving cars: Fusing deep learning and geometric
    modeling,” in *2017 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2017, pp.
    1025–1032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Cao and J. Wang, “Obstacle detection for autonomous driving vehicles
    with multi-lidar sensor fusion,” *Journal of Dynamic Systems, Measurement, and
    Control*, vol. 142, no. 2, p. 021007, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] F. Roos, J. Bechter, C. Knill, B. Schweizer, and C. Waldschmidt, “Radar
    sensors for autonomous driving: Modulation schemes and interference mitigation,”
    *IEEE Microwave Magazine*, vol. 20, no. 9, pp. 58–72, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher, K. Werber,
    and A. Sailer, “Automotive radar the key technology for autonomous driving: From
    detection and ranging to environmental understanding,” in *2016 IEEE Radar Conference
    (RadarConf)*.   IEEE, 2016, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] G. L. Oliveira, N. Radwan, W. Burgard, and T. Brox, “Topometric localization
    with deep learning,” in *Robotics Research*.   Springer, 2020, pp. 505–520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] L. Wang, Y. Gong, X. Ma, Q. Wang, K. Zhou, and L. Chen, “Is-mvsnet:importance
    sampling-based mvsnet,” in *Computer Vision – ECCV 2022*, S. Avidan, G. Brostow,
    M. Cissé, G. M. Farinella, and T. Hassner, Eds.   Cham: Springer Nature Switzerland,
    2022, pp. 668–683.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Song, K. G. Truong, D. Kim, and S. Jo, “Prior depth-based multi-view
    stereo network for online 3d model reconstruction,” *Pattern Recognition*, vol.
    136, p. 109198, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Yu, W. Yin, Z. Hu, and Y. Liu, “3d reconstruction for multi-view objects,”
    *Computers and Electrical Engineering*, vol. 106, p. 108567, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Lei, J. Song, B. Peng, W. Li, Z. Pan, and Q. Huang, “C2fnet: A coarse-to-fine
    network for multi-view 3d point cloud generation,” *IEEE Transactions on Image
    Processing*, vol. 31, pp. 6707–6718, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Sun, J. Hu, J. Yun, Y. Liu, D. Bai, X. Liu, G. Zhao, G. Jiang, J. Kong,
    and B. Chen, “Multi-objective location and mapping based on deep learning and
    visual slam,” *Sensors*, vol. 22, no. 19, p. 7576, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] R. Tian, Y. Zhang, Y. Feng, L. Yang, Z. Cao, S. Coleman, and D. Kerr,
    “Accurate and robust object slam with 3d quadric landmark reconstruction in outdoor
    environment,” in *2022 IEEE International Conference on Robotics and Automation*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Z. Zhu, L. Yang, X. Lin, C. Jiang, N. Li, L. Yang, and Y. Liang, “Garnet:
    Global-aware multi-view 3d reconstruction network and the cost-performance tradeoff,”
    *arXiv preprint arXiv:2211.02299*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Peng, Z. Chen, Z. Fu, P. Liang, and E. Cheng, “Bevsegformer: Bird’s
    eye view semantic segmentation from arbitrary camera rigs,” in *Proceedings of
    the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2023, pp.
    5935–5943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Jiang, L. Zhang, Z. Miao, X. Zhu, J. Gao, W. Hu, and Y.-G. Jiang,
    “Polarformer: Multi-camera 3d object detection with polar transformer,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] R. Tao, W. Han, Z. Qiu, C.-z. Xu, and J. Shen, “Weakly supervised monocular
    3d object detection using multi-view projection and direction consistency,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2023, pp. 17 482–17 492.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] H. J. Lee and Y. M. Ro, “Adversarially robust multi-sensor fusion model
    training via random feature fusion for semantic segmentation,” in *2021 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2021, pp. 339–343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. Vachmanus, A. A. Ravankar, T. Emaru, and Y. Kobayashi, “Multi-modal
    sensor fusion-based semantic segmentation for snow driving scenarios,” *IEEE sensors
    journal*, vol. 21, no. 15, pp. 16 839–16 851, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Y. Sun, W. Zuo, P. Yun, H. Wang, and M. Liu, “Fuseseg: semantic segmentation
    of urban scenes based on rgb and thermal data fusion,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 18, no. 3, pp. 1000–1011, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Ouaknine, A. Newson, P. Pérez, F. Tupin, and J. Rebut, “Multi-view
    radar semantic segmentation,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2021, pp. 15 671–15 680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Wu, S. Decker, P. Chang, T. Camus, and J. Eledath, “Collision sensing
    by stereo vision and radar sensor fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 10, no. 4, pp. 606–614, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] H. Jha, V. Lodhi, and D. Chakravarty, “Object detection and identification
    using vision and radar data fusion system for ground-based navigation,” in *2019
    6th International Conference on Signal Processing and Integrated Networks (SPIN)*.   IEEE,
    2019, pp. 590–593.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] K. Kowol, M. Rottmann, S. Bracke, and H. Gottschalk, “Yodar: Uncertainty-based
    sensor fusion for vehicle detection with camera and radar sensors,” *arXiv preprint
    arXiv:2010.03320*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Z. Liu, Y. Cai, H. Wang, L. Chen, H. Gao, Y. Jia, and Y. Li, “Robust
    target recognition and tracking of self-driving cars with radar and camera information
    fusion under severe weather conditions,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] R. Nabati and H. Qi, “Centerfusion: Center-based radar and camera fusion
    for 3d object detection,” in *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision*, 2021, pp. 1527–1536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] K. Qian, S. Zhu, X. Zhang, and L. E. Li, “Robust multimodal vehicle detection
    in foggy weather using complementary lidar and radar signals,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 444–453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] P. Cai, S. Wang, Y. Sun, and M. Liu, “Probabilistic end-to-end vehicle
    navigation in complex dynamic environments with multimodal sensor fusion,” *IEEE
    Robotics and Automation Letters*, vol. 5, no. 3, pp. 4218–4224, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] B. Shahian Jahromi, T. Tulabandhula, and S. Cetin, “Real-time hybrid
    multi-sensor fusion framework for perception in autonomous vehicles,” *Sensors*,
    vol. 19, no. 20, p. 4357, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Li, A. W. Yu, T. Meng, B. Caine, J. Ngiam, D. Peng, J. Shen, Y. Lu,
    D. Zhou, Q. V. Le *et al.*, “Deepfusion: Lidar-camera deep fusion for multi-modal
    3d object detection,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 17 182–17 191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar–camera
    fusion for road detection using fully convolutional neural networks,” *Robotics
    and Autonomous Systems*, vol. 111, pp. 125–131, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. S. Berrio, M. Shan, S. Worrall, and E. Nebot, “Camera-lidar integration:
    Probabilistic sensor fusion for semantic mapping,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] T. Huang, Z. Liu, X. Chen, and X. Bai, “Epnet: Enhancing point features
    with image semantics for 3d object detection,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 35–52.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] X. Cheng, Y. Zhong, Y. Dai, P. Ji, and H. Li, “Noise-aware unsupervised
    deep lidar-stereo fusion,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 6339–6348.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Hu, S. Wang, B. Li, S. Ning, L. Fan, and X. Gong, “Penet: Towards
    precise and efficient image guided depth completion,” in *2021 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2021, pp. 13 656–13 662.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Yan, K. Wang, X. Li, Z. Zhang, B. Xu, J. Li, and J. Yang, “Rignet:
    Repetitive image guided network for depth completion,” *arXiv preprint arXiv:2107.13802*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] C. Fu, C. Mertz, and J. M. Dolan, “Lidar and monocular camera fusion:
    On-road depth completion for autonomous driving,” in *2019 IEEE Intelligent Transportation
    Systems Conference (ITSC)*.   IEEE, 2019, pp. 273–278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*, 2020, pp. 4604–4612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] K. Geng, G. Dong, G. Yin, and J. Hu, “Deep dual-modal traffic objects
    instance segmentation method using camera and lidar data for autonomous driving,”
    *Remote Sensing*, vol. 12, no. 20, p. 3274, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Y. Liu, Z. Wang, K. Han, Z. Shou, P. Tiwari, and J. H. Hansen, “Sensor
    fusion of camera and cloud digital twin information for intelligent vehicles,”
    in *2020 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 2020, pp. 182–187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Ouyang, C. Wang, Y. Liu, and J. Niu, “Multiview cnn model for sensor
    fusion based vehicle detection,” in *Pacific Rim Conference on Multimedia*.   Springer,
    2018, pp. 459–470.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Pfeuffer and K. Dietmayer, “Robust semantic segmentation in adverse
    weather conditions by means of sensor data fusion,” in *2019 22th International
    Conference on Information Fusion (FUSION)*.   IEEE, 2019, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Mendez, M. Molina, N. Rodriguez, M. P. Cuellar, and D. P. Morales,
    “Camera-lidar multi-level sensor fusion for target detection at the network edge,”
    *Sensors*, vol. 21, no. 12, p. 3992, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and C. Vallespi-Gonzalez,
    “Sensor fusion for joint 3d object detection and semantic segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*,
    2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] T.-L. Kim and T.-H. Park, “Camera-lidar fusion method with feature switch
    layer for object detection networks,” *Sensors*, vol. 22, no. 19, p. 7163, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] X. Zhang, L. Wang, G. Zhang, T. Lan, H. Zhang, L. Zhao, J. Li, L. Zhu,
    and H. Liu, “Ri-fusion: 3d object detection using enhanced point features with
    range-image fusion for autonomous driving,” *IEEE Transactions on Instrumentation
    and Measurement*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] M. Liang, B. Yang, S. Wang, and R. Urtasun, “Deep continuous fusion for
    multi-sensor 3d object detection,” in *Proceedings of the European conference
    on computer vision (ECCV)*, 2018, pp. 641–656.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] E. Schröder, S. Braun, M. Mählisch, J. Vitay, and F. Hamker, “Feature
    map transformation for multi-sensor fusion in object detection networks for autonomous
    driving,” in *Science and Information Conference*.   Springer, 2019, pp. 118–131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] J. Zhao, X. N. Zhang, H. Gao, J. Yin, M. Zhou, and C. Tan, “Object detection
    based on hierarchical multi-view proposal network for autonomous driving,” in
    *2018 international joint conference on neural networks (IJCNN)*.   IEEE, 2018,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] X. Wu, L. Peng, H. Yang, L. Xie, C. Huang, C. Deng, H. Liu, and D. Cai,
    “Sparse fuse dense: Towards high quality 3d detection with depth completion,”
    *arXiv preprint arXiv:2203.09780*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, “Multi-task multi-sensor
    fusion for 3d object detection,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 7345–7353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] A. Rövid and V. Remeli, “Towards raw sensor fusion in 3d object detection,”
    in *2019 IEEE 17th World Symposium on Applied Machine Intelligence and Informatics
    (SAMI)*.   IEEE, 2019, pp. 293–298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] H. Zhu, J. Deng, Y. Zhang, J. Ji, Q. Mao, H. Li, and Y. Zhang, “Vpfnet:
    Improving 3d object detection with virtual point based lidar and stereo data fusion,”
    *arXiv preprint arXiv:2111.14382*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] X. Li, T. Ma, Y. Hou, B. Shi, Y. Yang, Y. Liu, X. Wu, Q. Chen, Y. Li,
    Y. Qiao *et al.*, “Logonet: Towards accurate 3d object detection with local-to-global
    cross-modal fusion,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2023, pp. 17 524–17 534.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] X. Dong, B. Zhuang, Y. Mao, and L. Liu, “Radar camera fusion via representation
    learning in autonomous driving,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2021, pp. 1672–1681.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Qi, C. Song, N. Zhang, S. Song, X. Wang, and F. Xiao, “Millimeter-wave
    radar and vision fusion target detection algorithm based on an extended network,”
    *Machines*, vol. 10, no. 8, p. 675, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Z. Li, M. Yan, W. Jiang, and P. Xu, “Vehicle object detection based on
    rgb-camera and radar sensor fusion,” in *2019 International Joint Conference on
    Information, Media and Engineering (IJCIME)*.   IEEE, 2019, pp. 164–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] P. Fritsche and B. Wagner, “Modeling structure and aerosol concentration
    with fused radar and lidar data in environments with changing visibility,” in
    *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2017, pp. 2685–2690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] S. K. Kwon, E. Hyun, J.-H. Lee, J. Lee, and S. H. Son, “A low-complexity
    scheme for partially occluded pedestrian detection using lidar-radar sensor fusion,”
    in *2016 IEEE 22nd International Conference on Embedded and Real-Time Computing
    Systems and Applications (RTCSA)*.   IEEE, 2016, pp. 104–104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] R. Ravindran, M. J. Santora, and M. M. Jamali, “Camera, lidar, and radar
    sensor fusion based on bayesian neural network (clr-bnn),” *IEEE Sensors Journal*,
    vol. 22, no. 7, pp. 6964–6974, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] L. Wang, T. Chen, C. Anklam, and B. Goldluecke, “High dimensional frustum
    pointnet for 3d object detection from camera, lidar, and radar,” in *2020 IEEE
    Intelligent Vehicles Symposium (IV)*.   IEEE, 2020, pp. 1621–1628.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] J. Cao, X. Weng, R. Khirodkar, J. Pang, and K. Kitani, “Observation-centric
    sort: Rethinking sort for robust multi-object tracking,” *arXiv preprint arXiv:2203.14360*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Xu, Y. Cao, Z. Zhang, and H. Hu, “Spatial-temporal relation networks
    for multi-object tracking,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2019, pp. 3988–3998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger,
    and B. Leibe, “Mots: Multi-object tracking and segmentation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 7942–7951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Y. Liu, I. E. Zulfikar, J. Luiten, A. Dave, D. Ramanan, B. Leibe, A. Ošep,
    and L. Leal-Taixé, “Opening up open world tracking,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2022, pp. 19 045–19 055.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Y. Zhang and X. Li, “Multiple dynamic object tracking for visual slam,”
    in *2022 4th International Conference on Robotics and Computer Vision (ICRCV)*.   IEEE,
    2022, pp. 49–55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] X. Zhou, V. Koltun, and P. Krähenbühl, “Tracking objects as points,”
    in *European Conference on Computer Vision*.   Springer, 2020, pp. 474–490.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] A. Osep, W. Mehner, M. Mathias, and B. Leibe, “Combined image-and world-space
    tracking in traffic scenes,” in *2017 IEEE International Conference on Robotics
    and Automation (ICRA)*.   IEEE, 2017, pp. 1988–1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] U. Nguyen and C. Heipke, “3d pedestrian tracking using local structure
    constraints,” *ISPRS Journal of Photogrammetry and Remote Sensing*, vol. 166,
    pp. 347–358, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Wang, R. Clark, H. Wen, and N. Trigoni, “Deepvo: Towards end-to-end
    visual odometry with deep recurrent convolutional neural networks,” in *2017 IEEE
    international conference on robotics and automation (ICRA)*.   IEEE, 2017, pp.
    2043–2050.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] F. Xue, X. Wang, S. Li, Q. Wang, J. Wang, and H. Zha, “Beyond tracking:
    Selecting memory and refining poses for deep visual odometry,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 8575–8583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. CS Kumar, S. M. Bhandarkar, and M. Prasad, “Depthnet: A recurrent
    neural network architecture for monocular depth prediction,” in *Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition Workshops*, 2018,
    pp. 283–291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] V. Patil, W. Van Gansbeke, D. Dai, and L. Van Gool, “Don’t forget the
    past: Recurrent depth estimation from monocular video,” *IEEE Robotics and Automation
    Letters*, vol. 5, no. 4, pp. 6813–6820, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, and Y. Yan, “Exploiting temporal
    consistency for real-time video depth estimation,” in *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2019, pp. 1725–1734.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, and P. Luo,
    “Transtrack: Multiple object tracking with transformer,” *arXiv preprint arXiv:2012.15460*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] F. Zeng, B. Dong, T. Wang, X. Zhang, and Y. Wei, “Motr: End-to-end multiple-object
    tracking with transformer,” *arXiv preprint arXiv:2105.03247*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, “Trackformer:
    Multi-object tracking with transformers,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8844–8854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] J. Watson, O. Mac Aodha, V. Prisacariu, G. Brostow, and M. Firman, “The
    temporal opportunist: Self-supervised multi-frame monocular depth,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021,
    pp. 1164–1174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, “Digging into
    self-supervised monocular depth estimation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 3828–3838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised learning
    of depth and ego-motion from video,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2017, pp. 1851–1858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Z. Yin and J. Shi, “Geonet: Unsupervised learning of dense depth, optical
    flow and camera pose,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 1983–1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Wang, Y. Gong, Q. Wang, K. Zhou, and L. Chen, “Flora: dual-frequency
    loss-compensated real-time monocular 3d video reconstruction,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] L. Chen, Z. Ling, Y. Gao, R. Sun, and S. Jin, “A real-time semantic visual
    slam for dynamic environment based on deep learning and dynamic probabilistic
    propagation,” *Complex & Intelligent Systems*, pp. 1–25, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] H. Zhou, B. Ummenhofer, and T. Brox, “Deeptam: Deep tracking and mapping,”
    in *Proceedings of the European conference on computer vision (ECCV)*, 2018, pp.
    822–838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Li, F. Xue, X. Wang, Z. Yan, and H. Zha, “Sequential adversarial learning
    for self-supervised deep visual odometry,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 2851–2860.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] W. Wang, Y. Hu, and S. Scherer, “Tartanvo: A generalizable learning-based
    vo,” *arXiv preprint arXiv:2011.00359*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] T. Yin, X. Zhou, and P. Krahenbuhl, “Center-based 3d object detection
    and tracking,” in *Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition*, 2021, pp. 11 784–11 793.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] W. Wang, X. Chang, J. Yang, and G. Xu, “Lidar-based dense pedestrian
    detection and tracking,” *Applied Sciences*, vol. 12, no. 4, p. 1799, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] X. Weng, J. Wang, D. Held, and K. Kitani, “3d multi-object tracking:
    A baseline and new evaluation metrics,” in *2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 359–10 366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] H.-k. Chiu, A. Prioletti, J. Li, and J. Bohg, “Probabilistic 3d multi-object
    tracking for autonomous driving,” *arXiv preprint arXiv:2001.05673*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] X. Chen, S. Shi, B. Zhu, K. C. Cheung, H. Xu, and H. Li, “Mppnet: Multi-frame
    feature intertwining with proxy points for 3d temporal object detection,” *arXiv
    preprint arXiv:2205.05979*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Gao, X. Yan, W. Zhao, Z. Lyu, Y. Liao, and C. Zheng, “Spatio-temporal
    contextual learning for single object tracking on point clouds,” *IEEE Transactions
    on Neural Networks and Learning Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Y. Li, C. R. Qi, Y. Zhou, C. Liu, and D. Anguelov, “Modar: Using motion
    forecasting for 3d object detection in point cloud sequences,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 9329–9339.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] W. Wang, X. You, J. Yang, M. Su, L. Zhang, Z. Yang, and Y. Kuang, “Lidar-based
    real-time panoptic segmentation via spatiotemporal sequential data fusion,” *Remote
    Sensing*, vol. 14, no. 8, p. 1775, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] F. Duerr, M. Pfaller, H. Weigel, and J. Beyerer, “Lidar-based recurrent
    3d semantic segmentation with temporal memory alignment,” in *2020 International
    Conference on 3D Vision (3DV)*.   IEEE, 2020, pp. 781–790.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Q. Li, S. Chen, C. Wang, X. Li, C. Wen, M. Cheng, and J. Li, “Lo-net:
    Deep real-time lidar odometry,” in *Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 8473–8482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] J. Kümmerle, T. Kühner, and M. Lauer, “Automatic calibration of multiple
    cameras and depth sensors with a spherical target,” in *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2018, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M. López, “Multimodal
    end-to-end autonomous driving,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. Dinesh Kumar, R. Karthika, and K. Soman, “Stereo camera and lidar
    sensor fusion-based collision warning system for autonomous vehicles,” in *Advances
    in Computational Intelligence Techniques*.   Springer, 2020, pp. 239–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] O. Natan and J. Miura, “Towards compact autonomous driving perception
    with balanced learning and multi-sensor fusion,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han, “Bevfusion:
    Multi-task multi-sensor fusion with unified bird’s-eye view representation,” *arXiv
    preprint arXiv:2205.13542*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. Wu, A. Hadachi, D. Vivet, and Y. Prabhakar, “This is the way: Sensors
    auto-calibration approach based on deep learning for self-driving cars,” *IEEE
    Sensors Journal*, vol. 21, no. 24, pp. 27 779–27 788, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] G. Wang, B. Tian, Y. Zhang, L. Chen, D. Cao, and J. Wu, “Multi-view adaptive
    fusion network for 3d object detection,” *arXiv preprint arXiv:2011.00652*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Z. Wu, G. Chen, Y. Gan, L. Wang, and J. Pu, “Mvfusion: Multi-view 3d
    object detection with semantic-aligned radar and camera fusion,” *arXiv preprint
    arXiv:2302.10511*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] L. Lou, Y. Li, Q. Zhang, and H. Wei, “Slam and 3d semantic reconstruction
    based on the fusion of lidar and monocular vision,” *Sensors*, vol. 23, no. 3,
    p. 1502, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] X. Chen, T. Zhang, Y. Wang, Y. Wang, and H. Zhao, “Futr3d: A unified
    sensor fusion framework for 3d detection,” *arXiv preprint arXiv:2203.10642*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai, “Bevformer:
    Learning bird’s-eye-view representation from multi-camera images via spatiotemporal
    transformers,” *arXiv preprint arXiv:2203.17270*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Y. B. Can, A. Liniger, O. Unal, D. Paudel, and L. Van Gool, “Understanding
    bird’s-eye view semantic hd-maps using an onboard monocular camera,” *arXiv preprint
    arXiv:2012.03040*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] X. Xiong, Y. Liu, T. Yuan, Y. Wang, Y. Wang, and H. Zhao, “Neural map
    prior for autonomous driving,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2023, pp. 17 535–17 544.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] X. Zhu, X. Cao, Z. Dong, C. Zhou, Q. Liu, W. Li, and Y. Wang, “Nemo:
    Neural map growing system for spatiotemporal fusion in bird’s-eye-view and bdd-map
    benchmark,” *arXiv preprint arXiv:2306.04540*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Y. Liu, J. Yan, F. Jia, S. Li, Q. Gao, T. Wang, X. Zhang, and J. Sun,
    “Petrv2: A unified framework for 3d perception from multi-camera images,” *arXiv
    preprint arXiv:2206.01256*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Huang and G. Huang, “Bevdet4d: Exploit temporal cues in multi-camera
    3d object detection,” *arXiv preprint arXiv:2203.17054*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Y. Zhang, Z. Zhu, W. Zheng, J. Huang, G. Huang, J. Zhou, and J. Lu, “Beverse:
    Unified perception and prediction in birds-eye-view for vision-centric autonomous
    driving,” *arXiv preprint arXiv:2205.09743*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan, R. Cipolla,
    and A. Kendall, “Fiery: Future instance prediction in bird’s-eye view from surround
    monocular cameras,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2021, pp. 15 273–15 282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. Wang, Y. Chen, and Z. Zhang, “Frustumformer: Adaptive instance-aware
    resampling for multi-view 3d detection,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2023, pp. 5096–5105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular visual odometry
    through unsupervised deep learning,” in *2018 IEEE international conference on
    robotics and automation (ICRA)*.   IEEE, 2018, pp. 7286–7291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] P. Li, J. Shi, and S. Shen, “Joint spatial-temporal optimization for
    stereo 3d object tracking,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 6877–6886.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Q. Jia, Y. Pu, J. Chen, J. Cheng, C. Liao, and X. Yang, “D${}^{\mbox{2}}$vo:
    Monocular deep direct visual odometry,” in *IEEE/RSJ International Conference
    on Intelligent Robots and Systems, IROS 2020, Las Vegas, NV, USA, October 24,
    2020 - January 24, 2021*.   IEEE, 2020, pp. 10 158–10 165\. [Online]. Available:
    [https://doi.org/10.1109/IROS45743.2020.9341313](https://doi.org/10.1109/IROS45743.2020.9341313)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] C. Chen, S. Rosa, Y. Miao, C. X. Lu, W. Wu, A. Markham, and N. Trigoni,
    “Selective sensor fusion for neural visual-inertial odometry,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 10 542–10 551.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] R. Clark, S. Wang, H. Wen, A. Markham, and N. Trigoni, “Vinet: Visual-inertial
    odometry as a sequence-to-sequence learning problem,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 31, no. 1, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] H. Zhan, R. Garg, C. S. Weerasekera, K. Li, H. Agarwal, and I. D. Reid,
    “Unsupervised learning of monocular depth estimation and visual odometry with
    deep feature reconstruction,” in *2018 IEEE Conference on Computer Vision and
    Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*.   Computer
    Vision Foundation / IEEE Computer Society, 2018, pp. 340–349\. [Online]. Available:
    [http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] A. Düzçeker, S. Galliani, C. Vogel, P. Speciale, M. Dusmanu, and M. Pollefeys,
    “Deepvideomvs: Multi-view stereo on video with recurrent spatio-temporal fusion,”
    in *IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,
    June 19-25, 2021*.   Computer Vision Foundation / IEEE, 2021, pp. 15 324–15 333\.
    [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Duzceker_DeepVideoMVS_Multi-View_Stereo_on_Video_With_Recurrent_Spatio-Temporal_Fusion_CVPR_2021_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, “Neuralrecon: Real-time
    coherent 3d reconstruction from monocular video,” in *IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021*.   Computer
    Vision Foundation / IEEE, 2021, pp. 15 598–15 607\. [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] N. Stier, A. Rich, P. Sen, and T. Höllerer, “Vortx: Volumetric 3d reconstruction
    with transformers for voxelwise view selection and fusion,” in *2021 International
    Conference on 3D Vision (3DV)*.   IEEE, 2021, pp. 320–330.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] K. Wang and S. Shen, “Mvdepthnet: Real-time multiview depth estimation
    neural network,” in *2018 International conference on 3d vision (3DV)*.   IEEE,
    2018, pp. 248–257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] X. Long, L. Liu, C. Theobalt, and W. Wang, “Occlusion-aware depth estimation
    with adaptive normal constraints,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 640–657.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang, “Multi-view depth estimation
    using epipolar spatio-temporal networks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2021, pp. 8258–8267.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] A. Rich, N. Stier, P. Sen, and T. Höllerer, “3dvnet: Multi-view depth
    prediction and volumetric refinement,” in *2021 International Conference on 3D
    Vision (3DV)*.   IEEE, 2021, pp. 700–709.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] X. Han, H. Liu, Y. Ding, and L. Yang, “Ro-map: Real-time multi-object
    mapping with neural radiance fields,” arXiv:2304.05735, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] A. Kim, A. Ošep, and L. Leal-Taixé, “Eagermot: 3d multi-object tracking
    via sensor fusion,” in *2021 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2021, pp. 11 315–11 321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] X. Weng, Y. Wang, Y. Man, and K. Kitani, “Gnn3dmot: Graph neural network
    for 3d multi-object tracking with multi-feature learning,” *arXiv preprint arXiv:2006.07327*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] D. Frossard and R. Urtasun, “End-to-end learning of multi-sensor 3d tracking
    by detection,” in *2018 IEEE international conference on robotics and automation
    (ICRA)*.   IEEE, 2018, pp. 635–642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] L. Wang, X. Zhang, W. Qin, X. Li, J. Gao, L. Yang, Z. Li, J. Li, L. Zhu,
    H. Wang *et al.*, “Camo-mot: Combined appearance-motion optimization for 3d multi-object
    tracking with camera-lidar fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Y. Zeng, D. Zhang, C. Wang, Z. Miao, T. Liu, X. Zhan, D. Hao, and C. Ma,
    “Lift: Learning 4d lidar image fusion transformer for 3d object detection,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 17 172–17 181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi,
    R. Martín-Martín, and S. Savarese, “Jrmot: A real-time 3d multi-object tracker
    and a new large-scale dataset,” in *2020 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 335–10 342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, “Robust multi-modality
    multi-object tracking,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019, pp. 2365–2374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] T. Kim and T.-H. Park, “Extended kalman filter (ekf) design for vehicle
    position tracking using reliability function of radar and lidar,” *Sensors*, vol. 20,
    no. 15, p. 4126, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Y. Liang, S. Müller, D. Schwendner, D. Rolle, D. Ganesch, and I. Schaffer,
    “A scalable framework for robust vehicle state estimation with a fusion of a low-cost
    imu, the gnss, radar, a camera and lidar,” in *2020 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 1661–1668.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] B. Moshiri, H. G. Garakani *et al.*, “Pedestrian detection using image
    fusion and stereo vision in autonomous vehicles,” in *2018 9th International Symposium
    on Telecommunications (IST)*.   IEEE, 2018, pp. 592–596.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] H. Liu, Y. Yao, Z. Sun, X. Li, K. Jia, and Z. Tang, “Road segmentation
    with image-lidar data fusion in deep neural network,” *Multimedia Tools and Applications*,
    vol. 79, no. 47, pp. 35 503–35 518, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Y. Wei, H. Liu, T. Xie, Q. Ke, and Y. Guo, “Spatial-temporal transformer
    for 3d point cloud sequences,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2022, pp. 1171–1180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Z. Teed and J. Deng, “Droid-slam: Deep visual slam for monocular, stereo,
    and rgb-d cameras,” *Advances in Neural Information Processing Systems*, vol. 34,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Y. Almalioglu, M. Turan, A. E. Sari, M. R. U. Saputra, P. P. de Gusmão,
    A. Markham, and N. Trigoni, “Selfvio: Self-supervised deep monocular visual-inertial
    odometry and depth estimation,” *arXiv preprint arXiv:1911.09968*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] J.-X. Zhong, K. Zhou, Q. Hu, B. Wang, N. Trigoni, and A. Markham, “No
    pain, big gain: Classify dynamic point cloud sequences with static models by fitting
    feature-level space-time surfaces,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022, pp. 8510–8520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] M. Bijelic, T. Gruber, F. Mannan, F. Kraus, W. Ritter, K. Dietmayer,
    and F. Heide, “Seeing through fog without seeing fog: Deep multimodal sensor fusion
    in unseen adverse weather,” in *2020 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 11 679–11 689\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Bijelic_Seeing_Through_Fog_Without_Seeing_Fog_Deep_Multimodal_Sensor_Fusion_CVPR_2020_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric, and C. Vallespi-Gonzalez,
    “Multi-view fusion of sensor data for improved perception and prediction in autonomous
    driving,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2022, pp. 2349–2357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Y. Liu, T. Wang, X. Zhang, and J. Sun, “Petr: Position embedding transformation
    for multi-view 3d object detection,” *arXiv preprint arXiv:2203.05625*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] M. Yasuda, Y. Ohishi, S. Saito, and N. Harado, “Multi-view and multi-modal
    event detection utilizing transformer-based multi-sensor fusion,” in *ICASSP 2022-2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2022, pp. 4638–4642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Liu and Y. Gao, “A multi-frame lane detection method based on deep
    learning,” in *International Conference on Cognitive Systems and Signal Processing*.   Springer,
    2021, pp. 247–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Y. Chen and Z. Xiang, “Lane mark detection with pre-aligned spatial-temporal
    attention,” *Sensors*, vol. 22, no. 3, p. 794, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Z. Yuan, X. Song, L. Bai, Z. Wang, and W. Ouyang, “Temporal-channel transformer
    for 3d lidar-based video object detection for autonomous driving,” *IEEE Transactions
    on Circuits and Systems for Video Technology*, vol. 32, no. 4, pp. 2068–2078,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang, “Multi-view depth estimation
    using epipolar spatio-temporal networks,” in *IEEE Conference on Computer Vision
    and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021*.   Computer Vision
    Foundation / IEEE, 2021, pp. 8258–8267\. [Online]. Available: [https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html](https://openaccess.thecvf.com/content/CVPR2021/html/Long_Multi-view_Depth_Estimation_using_Epipolar_Spatio-Temporal_Networks_CVPR_2021_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, “Mvsnet: Depth inference
    for unstructured multi-view stereo,” in *Computer Vision - ECCV 2018 - 15th European
    Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part VIII*, ser.
    Lecture Notes in Computer Science, V. Ferrari, M. Hebert, C. Sminchisescu, and
    Y. Weiss, Eds., vol. 11212.   Springer, 2018, pp. 785–801\. [Online]. Available:
    [https://doi.org/10.1007/978-3-030-01237-3_47](https://doi.org/10.1007/978-3-030-01237-3_47)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, and P. Tan, “Cascade cost volume
    for high-resolution multi-view stereo and stereo matching,” in *2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
    USA, June 13-19, 2020*.   Computer Vision Foundation / IEEE, 2020, pp. 2492–2501.
    [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Cascade_Cost_Volume_for_High-Resolution_Multi-View_Stereo_and_Stereo_Matching_CVPR_2020_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] X. Ma, Y. Gong, Q. Wang, J. Huang, L. Chen, and F. Yu, “Epp-mvsnet: Epipolar-assembling
    based depth prediction for multi-view stereo,” in *2021 IEEE/CVF International
    Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17,
    2021*.   IEEE, 2021, pp. 5712–5720\. [Online]. Available: [https://doi.org/10.1109/ICCV48922.2021.00568](https://doi.org/10.1109/ICCV48922.2021.00568)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Z. Murez, T. van As, J. Bartolozzi, A. Sinha, V. Badrinarayanan, and
    A. Rabinovich, “Atlas: End-to-end 3d scene reconstruction from posed images,”
    in *Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August
    23-28, 2020, Proceedings, Part VII*, ser. Lecture Notes in Computer Science, A. Vedaldi,
    H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12352.   Springer, 2020, pp. 414–431\.
    [Online]. Available: [https://doi.org/10.1007/978-3-030-58571-6_25](https://doi.org/10.1007/978-3-030-58571-6_25)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] J. Yang, W. Mao, J. M. Alvarez, and M. Liu, “Cost volume pyramid based
    depth inference for multi-view stereo,” in *2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*.   Computer
    Vision Foundation / IEEE, 2020, pp. 4876–4885\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Yang_Cost_Volume_Pyramid_Based_Depth_Inference_for_Multi-View_Stereo_CVPR_2020_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, and H. Su,
    “Deep stereo using adaptive thin volume representation with uncertainty awareness,”
    in *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
    2020, Seattle, WA, USA, June 13-19, 2020*.   Computer Vision Foundation / IEEE,
    2020, pp. 2521–2531\. [Online]. Available: [https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html](https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Deep_Stereo_Using_Adaptive_Thin_Volume_Representation_With_Uncertainty_Awareness_CVPR_2020_paper.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense
    monocular slam with learned depth prediction,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 6243–6252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] M. Obst, L. Hobert, and P. Reisdorf, “Multi-sensor data fusion for checking
    plausibility of v2v communications by vision-based multiple-object tracking,”
    in *2014 IEEE Vehicular Networking Conference (VNC)*.   IEEE, 2014, pp. 143–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] S. Pang, D. Morris, and H. Radha, “Clocs: Camera-lidar object candidates
    fusion for 3d object detection,” in *2020 IEEE/RSJ International Conference on
    Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 10 386–10 393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] H. Pan, W. Sun, Q. Sun, and H. Gao, “Deep learning based data fusion
    for sensor fault diagnosis and tolerance in autonomous vehicles,” *Chinese Journal
    of Mechanical Engineering*, vol. 34, p. 72, 12 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] T. Clunie, M. DeFilippo, M. Sacarny, and P. Robinette, “Development of
    a perception system for an autonomous surface vehicle using monocular camera,
    lidar, and marine radar,” in *2021 IEEE International Conference on Robotics and
    Automation (ICRA)*.   IEEE, 2021, pp. 14 112–14 119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] S. Sharma, J. Ansari, K. Jatavallabhula, and M. Krishna, “Beyond pixels:
    Leveraging geometry and shape cues for online multi-object tracking,” 02 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Y. Zhang, B. Song, X. Du, and M. Guizani, “Vehicle tracking using surveillance
    with multimodal data fusion,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 19, no. 7, pp. 2353–2361, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] J. Czarnowski, T. Laidlow, R. Clark, and A. Davison, “Deepfactors: Real-time
    probabilistic dense monocular slam,” *IEEE Robotics and Automation Letters*, vol. PP,
    pp. 1–1, 01 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] J. Xue, D. Wang, S. Du, D. Cui, Y. Huang, and N. Zheng, “A vision-centered
    multi-sensor fusing approach to self-localization and obstacle perception for
    robotic cars,” *Frontiers Inf. Technol. Electron. Eng.*, vol. 18, no. 1, pp. 122–138,
    2017\. [Online]. Available: [https://doi.org/10.1631/FITEE.1601873](https://doi.org/10.1631/FITEE.1601873)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] F. Garcia, D. Martin, A. De La Escalera, and J. M. Armingol, “Sensor
    fusion methodology for vehicle detection,” *IEEE Intelligent Transportation Systems
    Magazine*, vol. 9, no. 1, pp. 123–133, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] T.-L. Kim, J.-S. Lee, and T.-H. Park, “Fusing lidar, radar, and camera
    using extended kalman filter for estimating the forward position of vehicles,”
    in *2019 IEEE International Conference on Cybernetics and Intelligent Systems
    (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)*.   IEEE,
    2019, pp. 374–379.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] L. Caltagirone, M. Bellone, L. Svensson, and M. Wahde, “Lidar-camera
    fusion for road detection using fully convolutional neural networks,” *Robotics
    and Autonomous Systems*, vol. 111, 11 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] R. Dheekonda, S. Panda, M. Khan, M. Hasan, and S. Anwar, “Object detection
    from a vehicle using deep learning network and future integration with multi-sensor
    fusion algorithm,” 03 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] J. Luiten, T. Fischer, and B. Leibe, “Track to reconstruct and reconstruct
    to track,” *IEEE Robotics Autom. Lett.*, vol. 5, no. 2, pp. 1803–1810, 2020\.
    [Online]. Available: [https://doi.org/10.1109/LRA.2020.2969183](https://doi.org/10.1109/LRA.2020.2969183)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] J. Fei, W. Chen, P. Heidenreich, S. Wirges, and C. Stiller, “Semanticvoxels:
    Sequential fusion for 3d pedestrian detection using lidar point cloud and semantic
    segmentation,” in *2020 IEEE International Conference on Multisensor Fusion and
    Integration for Intelligent Systems (MFI)*.   IEEE, 2020, pp. 185–190.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “Ipod: Intensive point-based
    object detector for point cloud,” *arXiv preprint arXiv:1812.05276*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] D. Yu, H. Xiong, Q. Xu, J. Wang, and K. Li, “Multi-stage residual fusion
    network for lidar-camera road detection,” in *2019 IEEE Intelligent Vehicles Symposium
    (IV)*.   IEEE, 2019, pp. 2323–2328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, and S. Yogamani,
    “Rgb and lidar fusion based 3d semantic segmentation for autonomous driving,”
    in *2019 IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019,
    pp. 7–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] F. Farahnakian and J. Heikkonen, “Rgb-depth fusion framework for object
    detection in autonomous vehicles,” in *2020 14th International Conference on Signal
    Processing and Communication Systems (ICSPCS)*.   IEEE, 2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] T. Lv, Y. Zhang, L. Luo, and X. Gao, “Maffnet: real-time multi-level
    attention feature fusion network with rgb-d semantic segmentation for autonomous
    driving,” *Applied Optics*, vol. 61, no. 9, pp. 2219–2229, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] J. Park, H. Yoo, and Y. Wang, “Drivable dirt road region identification
    using image and point cloud semantic segmentation fusion,” *IEEE Transactions
    on Intelligent Transportation Systems*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] D. Göhring, M. Wang, M. Schnürmacher, and T. Ganjineh, “Radar/lidar sensor
    fusion for car-following on highways,” in *The 5th International Conference on
    Automation, Robotics and Applications*.   IEEE, 2011, pp. 407–412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] F. Camarda, “Multi-sensor data fusion for lane boundaries detection applied
    to autonomous vehicle,” Ph.D. dissertation, Université de Technologie de Compiègne,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] B. A. Griffin and J. J. Corso, “Depth from camera motion and object detection,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 1397–1406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Y. Liu, “Multi-scale spatio-temporal feature extraction and depth estimation
    from sequences by ordinal classification,” *Sensors*, vol. 20, no. 7, p. 1979,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] B. Liu, B. Zhuang, S. Schulter, P. Ji, and M. Chandraker, “Understanding
    road layout from videos as a whole,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 4414–4423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] A. Diba, M. Fayyaz, V. Sharma, M. M. Arzani, R. Yousefzadeh, J. Gall,
    and L. Van Gool, “Spatio-temporal channel correlation networks for action classification,”
    in *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    284–299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical
    image analysis,” *arXiv preprint arXiv:1904.00625*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns retrace
    the history of 2d cnns and imagenet?” in *Proceedings of the IEEE conference on
    Computer Vision and Pattern Recognition*, 2018, pp. 6546–6555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] C. Feichtenhofer, “X3d: Expanding architectures for efficient video recognition,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 203–213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Q. Zhou, X. Li, L. He, Y. Yang, G. Cheng, Y. Tong, L. Ma, and D. Tao,
    “Transvod: End-to-end video object detection with spatial-temporal transformers,”
    *arXiv preprint arXiv:2201.05047*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all
    you need for video understanding,” *arXiv preprint arXiv:2102.05095*, vol. 2,
    no. 3, p. 4, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, “Uniformer:
    Unified transformer for efficient spatiotemporal representation learning,” *arXiv
    preprint arXiv:2201.04676*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Z. Pang, J. Li, P. Tokmakov, D. Chen, S. Zagoruyko, and Y.-X. Wang, “Standing
    between past and future: Spatio-temporal modeling for multi-camera 3d multi-object
    tracking,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, 2023, pp. 17 928–17 938.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of Recent Reviews on Data Integration Techniques in ADS Perception
    with Deep Learning Approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Sensors | Applications/Tasks | Fusion Taxonomy | Other Contents
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[11](#bib.bib11)] | Camera, LiDAR, MMW-radar, GPS, IMU, ultrasonic,
    V2X | Multi-target tracking and environment reconstruction (motion model and data
    association) | Discernible units (data level), feature complementarity (feature
    level), target attributes, decision making (decision level) | Present characteristics,
    advantages, and disadvantages of different sensors. |'
  prefs: []
  type: TYPE_TB
- en: '| Fayyad et al. [[2](#bib.bib2)] | Camera, LiDAR, MMW-radar, GPS, IMU, INS,
    map | Detection, ego-localization and mapping | Data level (early fusion), feature
    level (mid-level fusion), or decision level (late fusion) | A summary of deep
    learning algorithm architectures in the field of sensor fusion for autonomous
    vehicle systems. |'
  prefs: []
  type: TYPE_TB
- en: '| Cui et al. [[28](#bib.bib28)] | Camera, LiDAR | Depth completion, object
    detection, semantic segmentation, tracking and online cross-sensor calibration
    | Signal level, feature level, result level, and multi-level | A summary of deep
    learning fusion methods based on different sensor combinations and different input
    representations for each perception task. |'
  prefs: []
  type: TYPE_TB
- en: '| Yeong et al. [[12](#bib.bib12)] | Camera, LiDAR, radar | Obstacle detection
    | Low-, mid-, high-level fusion | 1\. Operating principles and characteristics
    of sensors, and a comparison of commercially hardware. 2\. Sensor calibration
    overview.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. A summary of related data fusion reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. A summary of recent studies on sensor fusion technologies. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[4](#bib.bib4)] | Camera, LiDAR | BEV object detection, 3D
    object detection | New taxonomy of multi-modal fusion: two major classes (strong-
    and weak-fusion), and four minor classes in strong-fusion (early-, deep-, late-,
    and asymmetry-fusion) | 1\. Summary of data formats and representations of LiDAR
    and camera. 2\. Summary of commonly used open datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Feng et al. [[14](#bib.bib14)] | Camera, LiDAR, radar | Object detection,
    semantic segmentation | Early-, late-, middle-fusion (fusion in one layer, deep
    fusion, short-cut fusion) | 1\. Discusses the fusion methodologies regarding “what
    to fuse”, “when to fuse” and “how to fuse”. 2\. Summary of multi-modal datasets
    and task-related algorithms in papers. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[58](#bib.bib58)] | Camera, LiDAR | 3D object detection | Feature
    fusion (granularity: RoI-wise, voxel-wise, point-wise, pixel-wise), decision fusion.
    | 1\. Summary of popular sensors in ADS, their data representations, and corresponding
    object detection deep learning networks. 2\. Datasets (and metrics) for multi-modal
    3D object detection. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[15](#bib.bib15)] | Camera, LiDAR, radar | 3D object detection
    | New taxonomy with aspects of representation, alignment, and fusion. Fusion methods
    are further divided into learning-agnostic based (element-wise operations and
    concatenation) and learning-based (attention mochanism) approaches. | 1\. Categorization
    of sensor data representation: unified representation (hybrid-based, stereoscopic-based,
    and BEV-based) and raw representation. 2\. Categorization of alignment methodology:
    projection-based (global projection and local projection) and model-based (cross-attention).'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Datasets (and metrics) for multi-modal 3D object detection. |
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE V: Summary of Integration Operations in ADS Perception with Deep Learning
    Approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operations / Methods | Advantages | Disadvantages | Output Format |'
  prefs: []
  type: TYPE_TB
- en: '| Projection | 1\. Can connect 2D space and 3D space. 2\. Based on the projection
    principle, which is easier to understand. | 1\. Heavily rely on the sensor extrinsic
    and intrinsic matrices, which may be vulnerable to errors. 2\. Need extra methods
    to deal with revolution inconsistency before projection. | Projected data with
    dimension reduction or dimension boosting. |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenation | 1\. Easy to operate. 2\. Can integrate data from more than
    two sources. | 1\. Can only applied to data with same spatial size. 2\. Can not
    deal with missing dimensions. 3\. The input sensors are not permutable. | Concategated
    tensors with same spatial size but different dimension. |'
  prefs: []
  type: TYPE_TB
- en: '| Addition / Average mean / Weighted sum | Simple operation and easy to implement.
    Wide range of applications at different stages of the integration process. | Require
    the data to be integrated to have exactly the same format (both spatial size and
    dimension). | Added data with same data format (both spatial size and dimension)
    as the input. |'
  prefs: []
  type: TYPE_TB
- en: '| Probabilistic methods | Incorporate uncertainties into the integration process.
    | Usually have more parameters to be estimated thus currently are applied to comparatively
    simple models. | Depend on the input and implementation scenes. |'
  prefs: []
  type: TYPE_TB
- en: '| Rule-based transaction or integration | High specificity to deal with a certain
    scenario or dataset. | Low generalizability, and usually cannot be applied to
    other scenes. | Depend on the input and implementation scenes. |'
  prefs: []
  type: TYPE_TB
- en: '| Kalman Filter and extensions | 1\. Easy to apply to integrate temporal information.
    2\. Do not acquire massive data to process and update. Can update system state
    with only a few data points. | 1\. Heavily rely on the manually designed process
    model and the covariance matrix, which may not be appropriate or accurate. 2\.
    Have short memory for temporal information. | Depend on the design of the system
    states. |'
  prefs: []
  type: TYPE_TB
- en: '| RNN family | 1\. Designed for temporal integration by nature. 2\. Comparatively
    easy to understand and implement. | 1\. Have poor long-term memory. 2\. Difficult
    to train and converge. | Concatenated tensors. |'
  prefs: []
  type: TYPE_TB
- en: '| CNN, 3D CNN, and variants | 1\. Data driven, and does not need manually designed
    system model. 2\. Kernels and convolution layers by nature can integrate spatial
    (CNN) and temporal (3D CNN) information. | 1\. Comparatively limited receptive
    field and memory. 2\. Subject to the shortcomings of the integration operations
    included in the model structure. 3\. Model procedure and structure are unchanged
    in application. | Concatenated tensors. |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | 1\. Can potentially integrate both spatial and temporal information.
    2\. Global receptive field. 3\. Flexible model design and scalable to model any
    data size. | 1\. Need well-designed Queries and positional encodings. 2\. Hard
    to train. | Tensor weighted summation. |'
  prefs: []
  type: TYPE_TB
