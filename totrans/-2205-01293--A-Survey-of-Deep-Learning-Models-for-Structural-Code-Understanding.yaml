- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:46:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2205.01293] A Survey of Deep Learning Models for Structural Code Understanding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01293](https://ar5iv.labs.arxiv.org/html/2205.01293)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Models for Structural Code Understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ruoting Wu [wurt8@mail2.sysu.edu.cn](mailto:wurt8@mail2.sysu.edu.cn) Sun Yat-sen
    University of China ,  Yuxin Zhang Sun Yat-sen University of China [zhangyx355@mail2.sysu.edu.cn](mailto:zhangyx355@mail2.sysu.edu.cn)
    ,  Qibiao Peng Sun Yat-sen University of China [pengqb3@mail2.sysu.edu.cn](mailto:pengqb3@mail2.sysu.edu.cn)
    ,  Liang Chen [chenliang6@mail.sysu.edu.cn](mailto:chenliang6@mail.sysu.edu.cn)
    Sun Yat-sen University of China  and  Zibin Zheng [zhzibin@mail.sysu.edu.cn](mailto:zhzibin@mail.sysu.edu.cn)
    Sun Yat-sen University of China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In recent years, the rise of deep learning and automation requirements in the
    software industry has elevated Intelligent Software Engineering to new heights.
    The number of approaches and applications in code understanding is growing, with
    deep learning techniques being used in many of them to better capture the information
    in code data. In this survey, we present a comprehensive overview of the structures
    formed from code data. We categorize the models for understanding code in recent
    years into two groups: sequence-based and graph-based models, further make a summary
    and comparison of them¹¹1We provide a paper collection about deep learning models
    and datasets for structural code understanding in https://github.com/codingClaire/Structural-Code-Understanding.
    We also introduce metrics, datasets and the downstream tasks. Finally, we make
    some suggestions for future research in structural code understanding field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code Representation,Intelligent Software Engineering, Graph Neural Networks,
    Deep Learning, Code Generation^†^†booktitle:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last several decades, deep learning has made remarkable achievements
    in various areas and permeated every aspect of human lives, especially in the
    domain of multi-media data processing such as image recognition, speech recognition,
    and natural language processing. With the booming development of deep learning
    techniques, as well as cooperatively increasing open-source code communities and
    automation requirements in the software industry, deep learning techniques began
    to be applied to more specific tasks in software engineering in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventionally, source codes are considered as plain text sequences that may
    be understood using various existing approaches, such as deep learning approaches
    in neural language processing(NLP). However, when applied directly to source code,
    NLP approaches have the drawback of ignoring the code’s structural information.
    When the code is learned merely as a sequence of plain text, syntactic and semantic
    information that is crucial to understanding the code, as well as the many relationships
    between program entities, may be overlooked. Hence, a surge of works of understanding
    source code with structural information is proposed in recent years, which lead
    by the research of deep learning on sequences and graphs, such as Transformer(Bahdanau
    et al., [2014](#bib.bib20)), Graph Neural Networks(Wu et al., [2019](#bib.bib174)).
    These techniques and their variants are developed to cope with various tasks in
    source code understanding including code representation and other downstream tasks.
    Although these methods have achieved some improvements, structural code understanding
    is still facing many challenges, which are identified and summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Structural Modeling: Since conventional language models feed the sequence
    of source code tokens as inputs, the structural information in the code is usually
    neglected. Therefore as result, a number of challenges arise about how to use
    structural information in code successfully, such as how to model structural information
    in code effectively and how to select effective structural information for specific
    downstream tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Generic Representation Learning: Much of the current research focuses
    on learning code representations for specific programming languages, making learning
    generic code representations a challenge. It’s about how to learn language-independent
    code representations that get beyond programming languages’ constraints.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code Task-specific Adaptation: The following adaptations remain a challenge:
    how to choose and design specific architectures for downstream applications such
    as code generation and program repair, how to process datasets for task specifications,
    and how to adapt models in few-shots learning, transfer learning, and cross-linguistic
    scenarios.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this survey, we present a comprehensive overview of structural learning
    techniques for code representation learning. In summary, our contributions can
    be listed as below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce the structures in code data as well as the generating procedure,
    then give a summary of the downstream tasks for structural code understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a new taxonomy of deep learning models for structural code understanding
    based on the structures, which are categorized into sequence-based models and
    graph-based models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We outline the challenges, the open problems, and future directions for structural
    code understanding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The survey is organized as follows. In Section [2](#S2 "2\. Preliminary ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"), we give some
    basic introduction of the structures in code and how they are extracted from code
    data. In Section [3](#S3 "3\. Sequence-based Models ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and Section [4](#S4 "4\. Graph-based
    Models ‣ A Survey of Deep Learning Models for Structural Code Understanding"),
    we separately introduce the models based on which structure they generally used,
    indeed, the sequence-based models and the graph-based models. We first give an
    overview of how they change the structures and then categorized them by the core
    models. In Section [5](#S5 "5\. Discussion and Comparison ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"), we offer a discussion and comparison
    between the sequence-based models and graph-based models. The downstream tasks
    after conducting code representation is introduced in Section [6](#S6 "6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). We then
    summarize the related metrics and datasets in Section [7](#S7 "7\. Metrics and
    Datasets ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    We try to discuss some open research questions in Section [8](#S8 "8\. Open Problems
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). Finally,
    we draw our conclusion in Section [9](#S9 "9\. Conclusion ‣ A Survey of Deep Learning
    Models for Structural Code Understanding").
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preliminary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Structures in Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1.1\. Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we’ll go through the basic structures that programs can produce. Table
    [1](#S2.T1 "Table 1 ‣ 2.1.1\. Overview ‣ 2.1\. Structures in Code ‣ 2\. Preliminary
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") summarizes
    the most commonly used notations. The following is a piece of code snippet in
    Python we used to illustrate the structures in code data.
  prefs: []
  type: TYPE_NORMAL
- en: '{python}'
  prefs: []
  type: TYPE_NORMAL
- en: 'def add(a,b): x=0 if(a¿b): x=a-b; else: x=a+b; return x'
  prefs: []
  type: TYPE_NORMAL
- en: res=add(1,2) print(res)
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Summary of notations.
  prefs: []
  type: TYPE_NORMAL
- en: '| Symbol | Description | Symbol | Description | Symbol | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $P$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Program &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Code Snippet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $F$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; function &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $A$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Abstract Syntax Tree of a code snippet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or a program &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $G_{c}$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Control flow graph of a code snippet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or a program &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $G_{d}$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Data flow graph of a code snippet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or a program &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $path$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sequence of nodes extracted &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from the AST &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $n$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The length (token number) of a code snippet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; or a program &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $y$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Labels of the code &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $D$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Code description in natural language &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; from the AST &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $t$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; single token &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $H$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the intermediate representation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of the code &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The general techniques for generating structures from the source code snippet
    are shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2.1.1\. Overview ‣ 2.1\. Structures in
    Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"). The Lexical Analyzer first converts the code into a token-based
    sequence. Each token in the sequence has two attributes, type and value. Lexical
    Analysis of code is similar to the tokenization stage in natural language. Inspired
    by Hindle et al. (Hindle et al., [2012](#bib.bib65)), we refer to these unprocessed
    code sequences as Natural Code Sequence (NCS) in this article for the consistence
    of discussion, which may be called as token sequence or code sequence in other
    articles.
  prefs: []
  type: TYPE_NORMAL
- en: The Syntax Analyzer, also known as a parser, takes the tokens and produces an
    Abstract Syntax Tree(AST) based on the code snippet’s grammar. The Abstract Syntax
    Tree is then utilized in the Semantic Analyzer to verify for semantic consistency,
    and the Intermediate Code Generator to construct the Intermediate Representation,
    which varies depending on the programming language. Control flow and Data flow
    are both graph-like Intermediate Representations known as Control Flow Graph(CFG)
    and Data Flow Graph(DFG), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Other structures established in software engineering, such as UML Graph and
    Program Dependency Graph, are in addition to the basic structures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa52b4cd8439eb0757a7766f8682d3ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The basic structures generated from code includes (a) Nature Code
    Sequence, (b) Abstract Syntax Tree, (c) Control Flow Graph and (d) Data Flow Graph.
    The front end of a compiler constitutes the four components (Lexical Analyzer,
    Semantic Analyzer, Syntax Analyzer, and Intermediate Code Generator)
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Nature Code Sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a Program $P$ or a Code snippet $S$, the Nature Code Sequence(NCS) $S=\{t_{1},t_{2},...,t_{n}\}$
    is obtained by Lexical Analyzer, $t_{i}$ refers to the token in the code. Using
    NCS to represent the code is the simplest and most common approach. The position
    of the token in the sequence corresponds to the order in which it appears in the
    code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3\. Abstract Syntax Tree
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given the language grammar, the Abstract Syntax Tree (AST) of code is generated
    by Syntax Analyzer and marked as $A$, which hierarchically reflects the structural
    and syntax information of code. The root node of the syntax tree represents the
    start symbol. The interior nodes are the nonterminals in the grammar, while the
    leaf nodes are the terminals, which are usually the variables and identifiers
    defined by the programmers from the NCS. Fig. [2](#S2.F2 "Figure 2 ‣ 2.1.3\. Abstract
    Syntax Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") is the AST of the above code snippet
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/608be885928af50420bfc8a0eb772b58.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. An example of Abstract Syntax Tree
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4\. Flow Graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Flow Graphs are the graphs that cover the semantic information of source code.
    The two typical flows in flow graphs are control flow and data flow, which separately
    represent how the program executes and how the data flows. Fig.[3](#S2.F3 "Figure
    3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") shows the example
    of control flow and data flow graphs for the python code snippet mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68188532b7f5270b542f0d2811530daa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. An example of Flow Graphs. (a) Control Flow Graph, (b) Data Flow
    Graph of expression $x=a-b$, (c)Data Flow Graph of expression $x1=a1+b1$.
  prefs: []
  type: TYPE_NORMAL
- en: Control Flow Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Control Flow Graph (CFG) of a program marked as $G_{c}$, represents different
    execution paths of a program. Each node of CFG is a basic block that represents
    a behavior without branches. The edges of the graph represent how these basic
    blocks flow from one to another. In the example code snippet, the judgement of
    $a>b$ will cause the program into 2 branches, one is $x=a-b$, the other is $x=a+b$.
    As the Fig.[3](#S2.F3 "Figure 3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures in Code
    ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    (a) , the CFG has two edges from the decision node(if-statement) to the two downward
    nodes(two basic blocks).
  prefs: []
  type: TYPE_NORMAL
- en: Data Flow Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The Data Flow Graph (DFG) of a program marked as $G_{d}$ represents the dependency
    relation between variables. DFG can represent code snippets without conditionals.
    In the example code snippet, we select two statements: $x=a-b$ and $x=a+b$ to
    draw their DFGs. To eliminate the repeated assignment to $x$, we rename variables
    in the second assignment, convert them to $x1=a1+b1$. Therefore, the two DFGs
    are shown in Fig.[3](#S2.F3 "Figure 3 ‣ 2.1.4\. Flow Graph ‣ 2.1\. Structures
    in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural Code
    Understanding") (b) and (c). Each data flow node represents the operation of variables
    and each edge represents how the value flows.'
  prefs: []
  type: TYPE_NORMAL
- en: Control/Data Flow Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the DFG can only represent basic blocks without branches, it can be
    used to replace the basic blocks of a CFG, resulting in a Control/Data Flow Graph
    (CDFG). There are two types of nodes in a program’s CDFG, the decision node and
    the data-flow node.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.5\. Other Structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the above structures, there are some other code structures that
    are less common in code understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Other Intermediate Representation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Intermediate Representation(IR) is a data structure that can be obtained from
    a compiler such as LLVM Compiler Infrastructure(Lattner and Adve, [2004](#bib.bib86)).
    The frontend Compiler compiles the source code and generates an IR for the backend
    Compiler to optimize and translate. In LLVM infrastructure, the IR is in Static
    Single Assignment(SSA) form. The Broad definition of IR includes the flow graphs
    as well as other graph structures. Program Dependency Graph(PDG)(Ferrante et al.,
    [1987a](#bib.bib54)) is one of the intermediate representations that make explicit
    both data and control dependencies for each operation in a program. PDG are useful
    to perform optimizations through a single walk.
  prefs: []
  type: TYPE_NORMAL
- en: The Unified Modeling Language
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Unified Modeling Language (UML) is a widely-used language for specifying,
    visualizing, and documenting the artifacts of a software-intensive system. UML
    class diagram is a type of static structure diagram that describes the structure
    of a system by showing the system classes and their attributes, operations (or
    methods), and relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Deep learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we focus on deep learning models commonly used in code understanding
    tasks that have shown to be effective in other domains and are now being used
    by a growing number of researchers in the code domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Recurrent Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs)(Rumelhart et al., [1986](#bib.bib135)) are
    the neural networks where each unit is connected by directed cycles. RNNs can
    use their hidden state to track the long-term information of the sequence data.
    Therefore, RNNs are a common choice for sequence modeling. Vanilla RNNs have gradient
    vanishing and gradient explosion problems, which can reduce the ability of a model
    to learn long-term information. Long Short-Term Memory(LSTM) and Gated Recurrent
    Units(GRU) are the two most used RNN models that can avoid the problem and achieve
    better results.
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory(LSTM)(Hochreiter and Schmidhuber, [1997](#bib.bib66))
    has basic model from RNNs. Every unit of LSTM considers the hidden state, the
    current input, and the information from its memory cell. LSTM uses 3 gates, input
    gate, forget gate and output gate to control the learning and transfer of information.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units(GRU) (Cho et al., [2014](#bib.bib39)) combine the input
    gate and forget gate in LSTM into one gate, called the update gate, and the other
    gate is the reset gate. The reset gate and the update gate can control the degree
    of memory or forgetting of sequence information by the hidden state. Compared
    with LSTM, GRU has comparable performance but lower computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bahdanau et al.(Bahdanau et al., [2014](#bib.bib20)) propose the Attention mechanism
    to solve the problems of excessive information length and information loss in
    machine translation tasks. It feeds all of the hidden states from the Encoder
    into the Decoder after linear weighting and assigns various attention weights
    to each input token, indicating which inputs are more significant to the output.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance computational performance and better describe global relationships
    in sequences, Vaswani et al.(Vaswani et al., [2017](#bib.bib154)) propose self-attention.
    It is a special attention mechanism, so that information from any position in
    the sequence can directly affect the encoding of the other token. Based on self-attention
    they propose a new neural network model called Transformer which consists of multiple
    attention blocks made up by self-attention. Transfomer’s encoder uses the self-attention
    mechanism to associate the tokens in the input sequence with all other tokens
    as it learns the representation of the input. Also, the input of the Transfomer’s
    decoder is associated with the output of the encoder through self-attention. Transformer
    and its variants, such as Bert, GPT, etc., are capable of processing complex data
    and can process large amounts of sequence data. Therefore they are often used
    as pretraining models to capture the rich information from large amounts of complex
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Graph Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Graph Neural Networks(GNNs) are deep learning models using message passing
    between nodes to capture structural information and aggregate semantic information
    in graphs. GNN can be categorized into four groups according to the survey by
    Wu et al.(Wu et al., [2019](#bib.bib174)): Recursive GNNs, Convolutional GNNs,
    Graph autoencoders, and spatial-temporal GNNs. GNNs are widely used in node classification,
    edge prediction, and graph classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: The following are the typical models used in code representation-related tasks.
    Gated graph Neural Network(GGNN) (Li et al., [2016](#bib.bib101)) uses gated recurrent
    units and unroll the propagation process for a fixed number of timesteps. The
    representations of nodes are the final step output. Graph Convolution Network(GCN)
    (Kipf and Welling, [2017](#bib.bib85)) is one of the convolution GNNs, which stack
    multiple graph convolutional layers to better extract the information from neighbors.
    Graph Attention Network(GAT)(Veličković et al., [2018](#bib.bib155)) uses the
    attention mechanism in the message-passing step to aggregate neighborhoods’ information
    with different weights and update the encoding of the node. The above Graph Neural
    Networks are typical deep learning models for learning the representation of graphs
    or nodes in code data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. Encoder-Decoder Framework
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Encoder-Decoder framework(Cho et al., [2014](#bib.bib39)) is presented as
    a solution to traditional machine translation difficulties. The input language
    is encoded in the encoder section to obtain the intermediate representation Context.
    And then in the decoder part, corresponding outputs are generated one by one based
    on Context and related inputs. Sutskever et al.(Sutskever et al., [2014](#bib.bib145))
    present the seq2seq model based on Encoder-Decoder framework to overcome the problem
    of indefinitely long input-output sequences, which aids sequence output with special
    markers such as ¡Eos¿. Different encoders and decoders can be selected according
    to particular tasks, such as RNN-based models, Transformer-based models, GNN-based
    models, etc. Encoder-decoder model architecture has become the mainstream approach
    to address the code generation issue and other tasks due to the naturalness and
    sequence of code. For example, Rabinovich et al.(Rabinovich et al., [2017](#bib.bib130))
    introduce a syntax network (ASN) that extends the encoder-decoder framework to
    generate AST.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Sequence-based Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sequential models that perform well in sequence-related tasks, such as the Recurrent
    Neural Network family(Rumelhart et al., [1986](#bib.bib135); Hochreiter and Schmidhuber,
    [1997](#bib.bib66); Cho et al., [2014](#bib.bib39)) and Transformer(Vaswani et al.,
    [2017](#bib.bib154)), can be effectively applied to code-related tasks. They can
    be used to encoder-decoder architecture to fulfill downstream tasks such as code
    summarization and code generation, as well as learn to access code representation
    for downstream activities.
  prefs: []
  type: TYPE_NORMAL
- en: The frequently used term code sequence refers to natural code sequences created
    by the code itself, which is referring to NCS introduced by section [2.1.2](#S2.SS1.SSS2
    "2.1.2\. Nature Code Sequence ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"). Because the
    code is highly structured, there are sequences formed by pre-processing the code
    structure input such as AST introduced in section [2.1.3](#S2.SS1.SSS3 "2.1.3\.
    Abstract Syntax Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of
    Deep Learning Models for Structural Code Understanding"), which refer to the flattened
    sequence. In this section, we introduce the models for processing serialized code
    data mentioned above. We first show the structural transformation of code including
    the different methods of pre-processing the structures of code to get the flattened
    sequence, and then show the models for processing NCS and flattened sequences
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Structure Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The AST format is commonly used to express source code structural information.
    For the sequence model to make efficient use of the code’s structural information,
    some strategies for flattening the AST are offered. Flattening procedures are
    divided into four types, as shown in Fig [4](#S3.F4 "Figure 4 ‣ Type-4: AST partial
    retention ‣ 3.1\. Structure Transformation ‣ 3\. Sequence-based Models ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), which shows the varied
    sequences obtained by different types of flattening approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-1: Depth-first traversal'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the AST represents the code’s structural information as a tree, traversing
    the tree structure depth-first, so that the nodes on each subtree are adjacent
    in the sequence, is the simplest way to extract the flattened sequence. More models
    advocate preorder depth-first traversal because the root node (operator) of each
    subtree in the AST is frequently the subtree’s center. Type-1 refers to the Structure
    Transformation method that uses a depth-first traversal on the AST.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-2: AST path'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The approach of serializing the AST via paths is recommended because every two
    nodes in the AST have a path between them. Methods for extracting paths from ASTs
    include paths between arbitrary nodes, paths between terminal nodes, paths from
    terminal nodes to root nodes, and so on. In code generation task, paths from terminal
    nodes to new nodes are also used. Type-2 refers to the approach of Structure Transformation
    that uses paths between nodes on the AST.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-3: Structure information addition'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To better keep structural information in the AST and make the flattened sequence
    unique, Type-1/2-based structure information adding methods are proposed. The
    Structure-based traversal approach, for example, uses brackets to represent the
    AST structure, and the brackets in the created sequence may be used to detect
    the subtree of a certain node, allowing the generated sequence to be translated
    back to the AST. The use of ”¡” and ”¿” to enclose the non-terminal node’s subtree,
    which corresponds to the code block, is another technique to preserve structural
    information. As a result, we refer to the Structure Transformation method as Type-3
    because it incorporates code structure information in the obtained sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-4: AST partial retention'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: AST contains a lot of structural and syntactic information about the code, but
    it also contains a lot of useless data. As a result, numerous ways suggest filtering
    the ASTs, preserving the nodes of interest, and then flattening the sequence derived
    from the filtered ASTs. To generate flattened sequences in code defect detection
    jobs, only three types of AST nodes are generally kept, for example, method call
    and class instance creation nodes, declaration nodes, and control flow nodes.
    Type-4 refers to the Structure Transformation approach, which keeps only important
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8e41cf0c77213096d47d079e50783536.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Examples of local AST using four types of structure transformation
    to obtain flattened sequence, (a) using depth first traversal to convert AST into
    sequence, (b) showing three paths in the leaf to leaf path set, (c) showing SBT(Hu
    et al., [2018](#bib.bib68)) using parentheses to retain structure information,
    (d) only retaining the type information of nodes in AST, and the obtained sequence
    is the same as (a).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Natural code sequence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NCS is a natural way to represent whole code fragments because code is made
    up of individual tokens. The approaches and models for comprehending code using
    NCS are described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: N-gram Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: N-gram models, widely used as early language processing approaches, assume a
    Markov property that the probability of the current word is only affected by its
    prefixed N-1 words, which can capture the statistical characteristics of a sequence
    to some extent. In order to take advantage of the statistical properties of NCS,
    some early models using N-Gram models to accomplish code representation tasks
    were proposed. Hindle et al. (Hindle et al., [2012](#bib.bib65)) firstly adopt
    N-gram models on NCS and find that the language models are conducive to extracting
    local statistics by exploring the naturalness rather than the syntax or semantics.
    Tu et al. (Tu et al., [2014](#bib.bib151)) cooperate N-gram model with a cache
    component to further capture the localness of source codes. Karaivanov et al. (Karaivanov
    et al., [2014](#bib.bib81)) exploit the N-gram model for phrase-based programming
    language translation.
  prefs: []
  type: TYPE_NORMAL
- en: The N-gram model takes all of the information from the N-1 tokens before the
    token to learn its representation, but it cannot use remote token information
    (such as the reuse of a variable in the code), and it also cannot build identical
    vector representations for tokens with similar meanings. As a result, the N-Gram
    model was gradually replaced by the more learning model introduced later in the
    code representation task.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Convolutional neural network (CNN) is first proposed for processing images to
    learn image representations by capturing features of local images through convolutional
    kernels. Also, It can extract significant n-gram features from input sentences
    to create a semantic representation of the potential of sentence information for
    downstream tasks, while effectively capturing rich structural patterns in the
    code, so some work on learning code representations with CNN was proposed. To
    summarize the code snippet, Allamanis et al. ([2016](#bib.bib8)) employ an attentional
    neural network that uses convolution on the input tokens to detect local time-invariant
    and long-range topical attention features in a context-dependent way. For better
    code searching, CARLCS-CNN(Shuai et al., [2020](#bib.bib143)) first embeds code
    and query respectively using CNN since CNN can capture the informative keywords
    in query and code, then learns interdependent representations for the embedded
    code and query by a co-attention mechanism. CNN is unable to model long-range
    dependencies in code sequences, there are local limitations on the sensitivity
    to word order, and since code sequences are often large, CNN models are less often
    used in practical applications to learn to understand code.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As previously section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural Network
    ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models
    for Structural Code Understanding") stated, RNN and their variants perform exceptionally
    well on sequential tasks, resulting in a huge number of RNN-based models for code-related
    tasks to handle NCS. Veselin et al. (RaychevVeselin et al., [2014](#bib.bib133))
    have shown that the well-trained RNN can outperform N-gram models (Hindle et al.,
    [2012](#bib.bib65)) when processing NCS. Dam et al. (Dam et al., [2016](#bib.bib43))
    propose to use LSTM to predict the next token in order to address the inability
    of n-gram models to capture token dependencies in sequences. CodeNN (Iyer et al.,
    [2016](#bib.bib73)) uses LSTM with attention to produce sentences that describe
    code snippets. Liu et al.(Liu et al., [2016](#bib.bib107)) employ latent attention
    over outputs of a Bi-LSTM to better translate natural language descriptions into
    If-Then program. Bhoopchand et al.(Bhoopchand et al., [2016](#bib.bib28)) enhance
    LSTM with a pointer network specialized in referring to predefined classes of
    identifiers to well capture long-range dependencies in the code, thus giving better
    suggestions for the next token input. CODEnn (Gu et al., [2018](#bib.bib58)) provides
    a deep architecture composed of a code embedding network, description embedding
    network, and a similarity module to align the embeddings of code-description pairs.
    The code embedding network and description embedding network are both use LSTM.
    Tal et al. (Ben-Nun et al., [2018a](#bib.bib24)) utilize RNNs to learn a language-agnostic
    intermediate representation that is generated from code syntactical structures.
    Vasic et al.(Vasic et al., [2019](#bib.bib153)) present a solution to the general
    variable-misuse problem in which enumerative search is replaced by a neural network
    containing LSTM that jointly localizes and repairs faults. CodeGRU (Hussain et al.,
    [2020](#bib.bib72)) further applies GRU in code sequence processing to capture
    contextual dependencies. Compared with CNN, RNN family can handle arbitrary length
    input and has more flexible code sequence modeling ability, but vanilla RNN has
    the problem of gradient disappearance and gradient explosion. LSTM and GRU, as
    variants of vanilla RNN, can learn long-term dependency and solve the problem
    of gradient explosion and disappearance to a certain extent, and gradually become
    the current RNN family Core.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Transformer can successfully handle the distant dependence problem, overcome
    the limitation that RNNs cannot be computed in parallel, and employ self-attention
    to generate more explanatory models, as described in section [2.2.2](#S2.SS2.SSS2
    "2.2.2\. Transformer ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"). Transformer is being
    used for a growing number of sequence-related work, and NCS is no exception. Ahmad
    et al. (Ahmad et al., [2020](#bib.bib3)) first employ the Transformer for code
    summarization to handle the ubiquitous long-range dependencies in source code
    from natural code sequence. TFix(Berabi et al., [2021](#bib.bib27)) works directly
    on program text and phrases the problem of code fixing as a text-to-text task,
    so it can leverage a powerful Transformer based model pre-trained on natural language
    and fine-tuned to generate code fixes. CodeBERT (Feng et al., [2020](#bib.bib52)),CuBert(Kanade
    et al., [2020](#bib.bib80)), GPT-C(Svyatkovskiy et al., [2020](#bib.bib146)) and
    CodeT5(Wang et al., [2021d](#bib.bib168)) use both NCS and related natural language
    to pre-train the Transformer architectures for downstream tasks, such as code
    search, code clone detection and code summarization. OSCAR(Peng et al., [2021](#bib.bib127))
    and GraphCodeBERT(Guo et al., [2020](#bib.bib59)) are also pre-trained models
    based on transformer using NCS while exploiting the semantic information of flow
    graph. OSCAR adds GCF information to the model training using positional encoding.
    GraphCodeBERT takes DFG as part of the input while exploiting the node and edge
    relationships for Graph-Guided Masked Attention to better understand the code.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer can effectively learn a big quantity of data, but its memory and
    processing requirements are enormous when compared to models like RNN. Following
    that, work should be measured in terms of resource consumption and performance
    improvement, and a suitable model should be chosen by taking into account the
    current circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Others
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Other models have been incorporated in related work in addition to CNN, RNN,
    and other models that are extensively utilized in NCS related tasks. Sachdev et
    al.(Sachdev et al., [2018](#bib.bib137)) create a continuous vector embedding
    of each code fragment at method–level granularity, map the given natural language
    query to the same vector space, and use vector distance to simulate relevance
    of code fragments to a given query. CCLearner(Li et al., [2017a](#bib.bib97))
    extracts token sequence from known method-level code clones and non-clones to
    train a deep Neural Network(classifier) and then uses the classifier to detect
    clones in a given codebase. SCC(Alreshedy et al., [2018](#bib.bib16)) is a classifier
    that can identify the programming language of code snippets written in 21 different
    programming languages, it employs a Multinomial Naive Bayes(MNB) classifier trained
    using Stack Overflow posts. Sachdev et al.(Sachdev et al., [2018](#bib.bib137))
    propose a simple yet effective unsupervised model that combines word2vec(Mikolov
    et al., [2013](#bib.bib118)) and information retrieval methods for code search.
    UNIF (Cambronero et al., [2019a](#bib.bib32)) first uses word2vec to embeds code/quey
    and then combines code embedding with attention. These models show better results
    in specific tasks, and therefore, subsequent work dealing with NCS should not
    be limited to the adoption of mainstream models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Flattened Sequence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The four structural transformations indicated in [3.1](#S3.SS1 "3.1\. Structure
    Transformation ‣ 3\. Sequence-based Models ‣ A Survey of Deep Learning Models
    for Structural Code Understanding") can be used to obtain flattened sequences.
    Although the flattening procedure consumes more resources, the flattened sequences
    preserve some structural information about the code, and the applicable models
    can learn more about the code from the flattened sequences than NCS. The sequences
    obtained by different structural transformations may be suitable for different
    models. The models for processing the flattened sequence are described in the
    following and these models also show good performance in the natural language
    processing field. We make a finer segmentation of the RNN family in this section,
    including Vanilla LSTM, Bi-direction LSTM, and GRU, because of the enormous variety
    of applications of RNNs and their variants on flattened sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: word2vec is a method of converting an input token into a vector representation,
    where the converted vector contains, to some extent, the contextual information
    of the token. Word2vec contains two training models, CBOW (Continuous Bag-of-Words
    Model) and Skip-gram (Continuous Skip-gram Model), which have strong generality
    and are therefore used in early work on flattened sequence for learning code representation.
    API2Vec(Nguyen et al., [2017](#bib.bib123)) traverses the AST using Type-4 to
    build an annotation sequence according to the syntactic units related to APIs.
    These sequences are then used to train CBOW to generate API embeddings that may
    be utilized to migrate equivalent API usage from Java to C#. Alon et al.(Alon
    et al., [2018c](#bib.bib13)) first use Type-3 to obtain flattened sequence by
    adding up and down momentum information to the paths between nodes and then use
    word2vec to complete the prediction of method names. Because Word2vec is unable
    to learn the representation of polysemous words and cannot successfully capture
    long-range dependencies, further work is being done to combine Word2vec with other
    models to learn flattened sequences better.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Belief Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Deep Belief Network(DBN)(Bengio, [2009](#bib.bib26)) is a generative model
    which uses a multi-level neural network to learn a representation from training
    data that could reconstruct the semantic and content of input data with Maximum
    probability. DBN can be used to identify features, classify data, generate data,
    and do other tasks. Wang et al.(Wang et al., [2016](#bib.bib162), [2020a](#bib.bib161))
    use Type-4 and then use DBN to complete the defect prediction. They produce flattened
    sequences from only three sorts of AST nodes: method invocation and class instance
    creation nodes, declaration nodes, and control flow nodes. Because the names of
    methods, classes, and types are usually project-specific, there are very few methods
    with the same name across multiple projects. To get better detection results,
    they extract all three classes of AST nodes for cross-project defect prediction(CPDP),
    but instead of utilizing their names, it uses their AST node type, such as method
    declaration and method invocation, for declaration nodes and control flow nodes.
    DBN can automatically learn semantic features from token vectors extracted from
    ASTs and is one of the first non-convolutional models to be successfully trained
    by applying deep architectures. However, DBN has mostly lost favor and is rarely
    used compared to other unsupervised or generative learning algorithms nowadays.'
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla Long Short-Term Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Vanilla LSTM is the LSTM introduced in section [2.2.1](#S2.SS2.SSS1 "2.2.1\.
    Recurrent Neural Network ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), which is capable
    of learning long-range dependencies in some extent and is heavily used in work
    dealing with flattened sequences. Liu et al.(Liu et al., [2017](#bib.bib108))
    use Type-1 and explore several variants of simple LSTM architecture for different
    variants of the code completion problem. Li et al.(Li et al., [2017b](#bib.bib94))
    utilize Type-3, which allows storing two extra bits of information about whether
    the AST has children and/or right siblings in the type node. And the pointer mixture
    network proposed consists of two main components: a global RNN component(LSTM)
    and a local pointer component, which utilizes the pointer network to point to
    the previous position in the local context according to the learned position weights
    to solve the OoV problem. DeepCom (Hu et al., [2018](#bib.bib68)) uses Type-3
    and design a new structure-based traversal(SBT) method to better preserve structural
    information in the code. The SBT traversal method uses parentheses to indicate
    the structure of the AST, and the parenthesis in the created sequence may be utilized
    to determine the subtree of a given node, allowing the generated sequence to be
    transformed back to AST. DeepCom employs the seq2seq model which uses LSTM as
    encoder and decoder to generate code fragment summaries. code2vec (Alon et al.,
    [2019c](#bib.bib15)) employs Type-2 to represent code fragments with the set of
    paths between all terminal node pairs in the code to complete the prediction of
    method names of the code and learns the representation of the sequence of internal
    non-terminal nodes using LSTM. For better program classification, Compton et al.(Compton
    et al., [2020](#bib.bib40)) investigate the effect of obfuscating variable names
    during the training of a code2vec model to force it to rely on the structure of
    the code rather than specific names and consider a simple approach to creating
    class-level embeddings by aggregating sets of method embeddings. Seml(Liang et al.,
    [[n.d.]](#bib.bib103)) uses Type-4 and sends the sequence obtained after filtering
    the AST to the LSTM to complete the defect detection of the code. Type-3 is used
    by SA-LSTM(Liu et al., [2020d](#bib.bib111)), which surrounds the sub-tree of
    each non-leaf node, which corresponds to code blocks, with ¡ and ¿. SA-LSTM enhances
    the LSTM network with a stack to store and recover contextual information based
    on the code’s structure for modeling the hierarchical structure of code.'
  prefs: []
  type: TYPE_NORMAL
- en: Different works will modify the LSTM to suit their own tasks, but the LSTM has
    a disadvantage in parallel processing and cannot fully solve the gradient problem,
    as well as cannot do anything for a very large order of magnitude sequences, and
    Bi-LSTM and GRU introduced later also face the same problem. The latest effort
    will use LSTM or other RNN variants as a component of the model and mix it with
    other models to accomplish the task, in order to better exploit the advantages
    of RNN and its variants on sequence processing.
  prefs: []
  type: TYPE_NORMAL
- en: Bi-direction Long Short-Term Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Compared with the traditional LSTM which only retains the previous information,
    the bidirectional Long short Memory (Bi-LSTM) can also use the later information,
    which can better capture the semantic dependencies in both directions. code2seq (Alon
    et al., [2018a](#bib.bib9)) employs the same Type-2 as code2vec to obtain the
    flattened sequence, and Bi-LSTM was used to learn the representation of the internal
    non-terminal nodes sequence. DeepCPDP(Chen et al., [2019a](#bib.bib35)) uses Type-4,
    using simplified Abstract Syntax Tree(SimAST) to represent the source code of
    each extracted program. DeepCPDP uses SimASTToken2Vec, an unsupervised-based embedding
    approach, and will classify the code inputted as defective or non-defective using
    Bi-LSTM with attention mechanism and Logistic regression. Pythia (Svyatkovskiy
    et al., [2019](#bib.bib147)) uses Type-1 to predict the method names and API calls
    that the developer wants to use in programming. Pythia aggregates the initial(obtained
    by word2vec) and intermediate(learned by Bi-LSTM) vector representations through
    a fully connected layer to obtain the final vector for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Recurrent Units
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GRU introduced in section [2.2.1](#S2.SS2.SSS1 "2.2.1\. Recurrent Neural
    Network ‣ 2.2\. Deep learning models ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") is a simplified version of LSTM with
    a reduced number of gates and therefore easier to converge with relatively few
    parameters, so some work will choose to use GRU to learn the representation of
    the flattened sequence. ast-attendgru(LeClair et al., [2019](#bib.bib90)) uses
    Type-3 and proposes SBT-AO which modifies the SBT AST Flastting procedure to simulate
    the case when only an AST can be extracted. SBT-AO replaces all words (except
    official Java API class names) in the code to a special ¡OTHER¿ token, remaining
    all the code structure in SBT. It uses two GRU with attention mechanism to process
    NCS and SBT-AO respectively for getting context vector and then predicts the summary
    one word at a time from the context vector, following what is typical in seq2seq
    models. Hybrid-Deepcom(Hu et al., [2020a](#bib.bib69)) uses the same Type-4 as
    Deepcom to obtain the flattened sequence. It also employs the seq2seq model for
    code summarization and utilizes two GRU as encoders for NCS and flattened sequence,
    respectively, to acquire both lexical and structural information of code fragments.
    GRU has one less gate and relatively fewer parameters than LSTM, so it is easier
    to converge and less computationally expensive, while having similar results in
    most tasks, but LSTM performs better with larger data sets, so the choice of LSTM
    and GRU in code sequence tasks needs to consider both data set size, training
    effect and training time.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As previously described, Transformer is a more complex and powerful model compared
    to RNN, and more and more work has been done in recent years using transform to
    learn flattened sequences to accomplish related tasks. SLM(Alon et al., [2020](#bib.bib11))
    uses Type-2 to represent the code as a path from the root node and all leaf nodes
    to the target node. SLM uses LSTM to obtain the representation of all paths separately
    and then uses Transformer contextualize the path representation of all leaf nodes
    to the target node, and at the same time adds the position index in the parent
    node to the path representation from the root node to the target node, and passes
    the attention mechanism. The final vector is obtained by combining the path representation
    to make predictions on the target node. Kim et al.(Kim et al., [2020](#bib.bib83))
    propose three methods based on Transformer to better predict the token that the
    developer is about to input: 1\. pathTrans, uses Type-2 to serialize the AST using
    the path from the leaf node to the root node; 2\. TravTrans, uses Type-1, and
    uses the method of preorder first traversal to obtain sequences from the AST;
    3\. TravTrans+, uses Type-3, adding a matrix that saves the unique path between
    two nodes in TravTrans to enhance the Transformer’s self Note the block. And the
    experiment proves that TravTrans+ works better. Liu et al.(Liu et al., [2020c](#bib.bib110))
    uses both Type-1 and Type-2 to complete the prediction of the code. It uses Transformer-XL
    to encode the sequence obtained by preorder traversal and uses Bi-LSTM to encode
    the path from the target node to the root node, preserving the hierarchical information
    of the target node. TreeBERT(Xue et al., [2021](#bib.bib180)) employs Type-2 to
    represent the entire code fragment using multiple paths from the root node to
    the leaf nodes in the AST and then completes the pre-training using the modified
    Transformer architecture. The position embedding of each node is generated from
    its hierarchical information in the AST and the position information of its parent
    node to make better use of the structural information of the code. Since Transformer
    requires a lot of data and a lot of computational resources, subsequent work should
    consider more than just model performance when using Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Graph-based Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the sequential model with serialized structure as input is simple and
    visible in many works, the linear order of code snippets is inevitably losing
    hierarchical syntactic information. Therefore, recent works pay more attention
    to capturing the syntactic information of the code. Graph-based models for code
    understanding are described and categorized in this section, based on the structures
    used in the methods without serialization.
  prefs: []
  type: TYPE_NORMAL
- en: The structures used in graph-based models are usually AST and Flow Graph, including
    CFG and DFG, introduced in section [2.1.3](#S2.SS1.SSS3 "2.1.3\. Abstract Syntax
    Tree ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and [2.1.4](#S2.SS1.SSS4 "2.1.4\. Flow
    Graph ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"). We also introduce models that use
    other structures in section [2.1.5](#S2.SS1.SSS5 "2.1.5\. Other Structures ‣ 2.1\.
    Structures in Code ‣ 2\. Preliminary ‣ A Survey of Deep Learning Models for Structural
    Code Understanding"). Unlike the sequence-based models, these structures of a
    program are considered as a tree structure or graph structure. In this section,
    we first introduce the transformation conducted in these structures and then categorized
    different models in AST, flow graphs, the combination of them, and other rarely
    seen structures generated from source code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Structure Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from the transformation of sequences, the modifications of graph structures
    tend to add nodes or edges based on one typical graph structure. We summarize
    the structure’s transformation of the graph into three categories, the first two
    transformations are proposed on the basis of preserving a graph structure for
    further learning the representation of the graph, while the last transformation
    is extracting the information of graph and constructing a new mechanism or DSL(Domain-specific
    language) without keeping the program graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-1: Adding Edges'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In AST-based structure, methods treat the structure from two different perspectives
    for better using the structural information. One takes the structure as a ”tree”,
    which means that the structure has directed edges with the hierarchy preserved.
    The other extends the AST structure by adding various types of edges and eventually
    makes the edge bidirectional, which makes the original AST structure a ”graph”.
    In two perspectives, adding edges are the most common transformation to preserve
    more information of structures. Allamanis et al. (Allamanis et al., [2018b](#bib.bib6))
    use AST as the backbone and add additional edges for capturing data-flow information.
    The edges contain types derived from AST(e.g. Child and NextToken) and from semantic(e.g.
    LastUse, LastWrite). Wang et al. (Wang and Li, [2021](#bib.bib166)) extend the
    AST with parent-child edges. Dinella(Dinella et al., [2020](#bib.bib46)) add SuccToken
    edges between the leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-2: Combination'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some methods use AST-based and Flow-graph-based structures in combination, such
    as the structure CDFG, which is a combination of CFG and DFG. Other structures
    for graph-based models have been proposed, as well as some new structures based
    on the three basic structures. The combination will be discussed and introduced
    in Section [4.4](#S4.SS4 "4.4\. Combined Structures ‣ 4\. Graph-based Models ‣
    A Survey of Deep Learning Models for Structural Code Understanding").
  prefs: []
  type: TYPE_NORMAL
- en: 'Type-3: Information Extracting'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some methods choose not to preserve the graph structure of AST, Flow graphs,
    or others. Instead, they propose new structures, mechanisms, or DSL based on the
    syntactic and semantic information extracted from these graph structures. It is
    not the priority of our introductions in graph-based models, but still an important
    kind of structures transformation from graph structures. For example, Raychev
    et al. (Raychev et al., [2016](#bib.bib132)) propose a method to build probabilistic
    models of code and generate DSL called TGEN, for traversing AST and accumulating
    a conditioning context. Cvitkovic et al. (Cvitkovic et al., [2019](#bib.bib41))
    propose a Graph-Structured Cache for the out-of-vocabulary problem. An edit DSL
    called Tocopo(Tarlow et al., [2020b](#bib.bib150)) is created for code editing
    in bug fixing problems, which contains token expressions, copy expressions, and
    pointer expressions. Tocopo is a sequence of edit operations that represents the
    modification of an AST.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. AST-based Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As previously mentioned, the following models are considerably varied as a result
    of the differences between the perspectives of the AST structure.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Tree perspective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the large and deep structure of the tree structure, certain methods tend
    to use recursion of the tree structure to reduce the complexity of programs, especially
    passing the information from sub-trees to the full tree. Shi et al.(Shi et al.,
    [2021](#bib.bib139)) propose a method to split and reconstruct the whole AST of
    a program into subtrees hierarchically to get the representation of the code snippets.
    ASTNN (Zhang et al., [2019a](#bib.bib194)) uses a preorder traversal algorithm
    to split an AST into a set of statement subtrees, recursively encodes them to
    vectors, and eventually learns the representation of source code through the captured
    naturalness by BiGRU and RvNN. The method can reduce the difficulty in training.
  prefs: []
  type: TYPE_NORMAL
- en: Most methods tend to modify the Recursive Neural Networks or Seq2seq model based
    on the structure of AST, such as AST-based LSTM (Wan et al., [2018](#bib.bib158))
    and tree-based Seq2seq model (Chakraborty et al., [2020](#bib.bib34)). The Tree-LSTM
    is one of the most often modified methods in models of code understanding. Tree-LSTM
    (Tai et al., [2015](#bib.bib148)) is a generalization of the standard LSTM for
    tree structures that composes the state from an input vector and hidden states
    of arbitrarily multiple children units. However, when the tree has ordered nodes,
    such as AST, the original Tree-LSTM is unable to manage the circumstance. To address
    the problem, a Multi-way Tree-LSTM model is developed, which extends the Tree-LSTM
    model. The model captures the interaction information between nodes by adding
    bidirectional LSTM to each gate before linear transformation to encapsulate the
    information of ordered children. In the code summarization task, the Multi-way
    Tree-LSTM learns the information in ASTs more effectively than a sequence model.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network is able to train easier and requires less time
    with the parallel computing mechanism compared to Recursive Neural Networks. Meanwhile,
    the adaptations to the vanilla CNN, such as tree-based CNN, have demonstrated
    their efficacy on code comprehensive tasks. Mou et al. (Mou et al., [2014](#bib.bib119))
    propose a Tree-Based Convolutional Neural Network (TBCNN) for the program classification
    task. The convolution layer can capture the information from AST. Chen et al.
    (Chen et al., [2019b](#bib.bib36)) propose a tree-based LSTM over API-enhanced
    AST for clone detection. The original AST is modified by adding a new node type
    to identify the API name. The model called TBCAA learns the representation of
    code using tree-based CNN, where each convolution kernel has a triangle shape.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Graph perspective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To encode the AST structure, most methods consider the structure as a graph
    and use a graph neural network. Two types of GNN are the most used model in the
    following graph-based models of AST, which are the Gated Graph Neural Networks(GGNN)
    and the Convolutional Graph Neural Networks. We also introduce other graph neural
    networks such as Graph Attention Networks that are used particularly for the AST
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Graph Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Gated Graph Neural Network, which is a kind of Recurrent Graph Neural Networks,
    is the first developed graph neural network for code-related tasks when Li et
    al.(Li et al., [2015](#bib.bib99)) proposed Gated Graph Neural Network to infer
    formulas for program verification. GGNN updates the state of a node with a GRU
    cell, with the information from neighboring nodes and the node state in previous
    timestamp. Fernandes et al. (Fernandes et al., [2019](#bib.bib53)) modify GGNN
    to a framework to extend existing sequence encoders and conduct the experiments
    on three summarization tasks. Graph-based Grammar Fix (GGF) (Wu et al., [2020](#bib.bib173))
    use a mixture of GRU and GGNN as an encoder to encode the sub-AST (created by
    the erroneous code) and a token replacement mechanism as a decoder to generate
    the code fixing actions. Graph2diff(Tarlow et al., [2020b](#bib.bib150)) uses
    an encoder-decoder framework to predict the diff, also described as edit operations.
    The model takes the source code, bug configures files and compiler diagnostic
    messages as the graph input. The GGNN is used in the encoder stage to explicit
    the structure information of code. Although GGNN has shown its ability to learn
    the syntactic information from the AST of the graph, as a kind of Graph Neural
    network, it can only aggregate local information rather than global information.
    Therefore, some works also combine it with sequential models, such as Graph Sandwiches
    (Hellendoorn et al., [2019](#bib.bib63)). Furthermore, since GGNN employs RNN,
    it requires more memory to retain the hidden state of all nodes in the graph,
    despite the fact that it eliminates the need to constrain parameters to ensure
    convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Graph Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Convolutional Graph Neural Network(convGNN) learns the representations of nodes
    based on the node vector and the neighbors of the node in the graph. The information
    from neighbors is combined through the aggregation process. Compare to Recurrent
    Graph Neural Networks such as GGNN, Convolutional Graph Neural Networks can stack
    multiple graph convolutional layers to better propagate the information across
    nodes, which can combine the information from neighbors. LeClair et al.(LeClair
    et al., [2020](#bib.bib89)) use convolution GNN to encode the AST nodes and edges
    for code summarization. The ConvGNN layer’s input is the AST token embedding and
    edges. after the ConvGNN layer, the model uses an attention mechanism to learn
    the important tokens in the source code and AST. Ji et al.(Ji et al., [2021b](#bib.bib75))
    also use GCN to encode AST for the code clone task. Liu et al.(Liu et al., [2021c](#bib.bib113))
    propose a task of code documentation generation for Jupyter notebooks. When generating
    documentation, the model HAConvGNN considers the relevant code cells and code
    token information. Convolutional GNN layers and a GRU layer are included in the
    encoder for code cells’ AST. The output of the GRU layer will be the input of
    a hierarchical attention mechanism to better preserve the graph structure.
  prefs: []
  type: TYPE_NORMAL
- en: Other Graph Neural Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are some methods using other Graph Neural Networks. Different from GCN,
    Graph Attention Network(GAT) (Veličković et al., [2018](#bib.bib155)) performs
    self-attention mechanism on message passing stage to learn the graph representation.
    Wang et al. (Wang and Li, [2021](#bib.bib166)) models the flattened sequence of
    a partial AST as an AST graph. To reduce the information loss, the parent-child
    relation and the positional information are recorded. Further, three types of
    AST Graph Attention Blocks are proposed to capture the structural information
    for learning the representation of the graph. Compare to GCN and GGNN, the GAT
    model improves the explainability of code completion or code summary tasks due
    to the utilization of the attention mechanism. Hoppity(Dinella et al., [2020](#bib.bib46))
    uses another type of GNN, which is Graph Isomorphism Network(GIN), as external
    memory to encode the AST of a buggy program, further using a central controller
    implemented by LSTM to predict the sequence of actions to fix bugs. The controller
    will expand or decline the memory when the graph structure is changed.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Flow-Graph-based Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As previously introduced, flow graphs are separated into two types: control
    flow graphs (CFG) and data flow graphs (DFG). Although a flow graph is more likely
    to be seen as a graph, there are few works that treat flow graphs as trees. For
    example, BAST (Lin et al., [2021](#bib.bib104)) splits the code of a method according
    to the blocks in the dominator tree of CFG and generates the corresponding AST
    for the split code. The split ASTs’ representations are used in the pre-trained
    stage by predicting the next split AST in the dominator tree. The CFG is only
    used in splitting AST but can make the model more efficient and scalable for large
    programs.'
  prefs: []
  type: TYPE_NORMAL
- en: The works that consider the CFG from a graph perspective and apply deep learning
    methods to represent the CFG are described in the following. The attributed Control
    Flow Graph (ACFG) is a common pre-processing step in some of the following works,
    especially in binary code similarity detection, such as Genius(Feng et al., [2016a](#bib.bib50)),
    Gemini (Xu et al., [2017](#bib.bib179)) and BugGraph (Ji et al., [2021a](#bib.bib76)).
    There are also some typical modifications for CFG, for example, lazy-binding CFG
    (Nguyen et al., [2018](#bib.bib122)), inter-procedural CFGs(ICFG) (Duan et al.,
    [2020](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Convolutional neural network (CNN) has translation invariance in many training
    data, therefore, it can not only capture the semantic information of code but
    is also suitable for order-aware modeling. Nguyen et al. (Nguyen et al., [2018](#bib.bib122))
    use CNN on the adjacency matrices converted by the lazy-binding CFG. The CFG will
    be converted into a pixel image, and later with a CNN model to recognize whether
    the target object is appears in the image. The method can be applied for malware
    detection. Yu et al. (Yu et al., [2020](#bib.bib191)) use Bert and CNN to learn
    CFG graph embedding, which can include semantic, structural, and order information.
    During the adjacent node prediction task, the Bert model is used to pre-train
    tokens and block embeddings. The order information of CFGs is extracted using
    CNN models. As indicated previously, CNN models are employed as a component for
    learning order information or for downstream tasks rather than directly for constructing
    the representation of the graph of code data.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Graph Neural Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: DGCNN(Zhang et al., [2018](#bib.bib196)), as one of the ConvGNN, is proposed
    with a similar pooling strategy SortPooling. The approaches can allow attributed
    information to be aggregated quickly through neighborhood message passing, therefore,
    it is suitable for embedding structural information into vectors for further classification.
    To solve the malware classification challenge, Yan et al.(Yan et al., [2019](#bib.bib182))
    use DGCNN to embed CFGs. The CFG will first be converted to an attributed CFG,
    with the code characteristics defining the attributes. The DGCNN is used to aggregate
    these attributes with an adaptive max pooling to concatenate the layer outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Attention Network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Li et al.(Li et al., [2020](#bib.bib92)) propose an event-based method CSEM
    for clone detection. GAT extracts the context information for each statement,
    which is modeled by the events that are embedded to capture execution semantics.
    BugGraph(Ji et al., [2021a](#bib.bib76)) compares the source-binary code similarity
    in two steps: source binary canonicalization and code similarity computation.
    In the code similarity computation step, BugGraph computes the similarity between
    the target and the comparing binary code. After disassembling both codes, each
    function will construct its ACFG and use GAT with the triplet loss as the output
    of the model to generate the embedding of each graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Graph Neural Networks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As previously mentioned, Yu et al. (Yu et al., [2020](#bib.bib191)) also uses
    MPNN(Gilmer et al., [2017](#bib.bib57)) to compute the graph embedding of a control-flow
    graph in order-aware modeling. BugLab (Allamanis et al., [2021](#bib.bib7)) uses
    standard message-passing graph neural networks to represent the graph of code
    entities. The graph includes syntactic entities and relations about the intraprocedural
    data and control flow, types, and documentation. BugLab trains two models, a selector
    model and a detector model to predict the rewrite on code snippet. The select
    model introduces buggy code and the detector model detects and repairs the bugs.
    Wang et al.(Wang et al., [2020b](#bib.bib167)) propose a new graph neural architecture
    called Graph Interval Neural Network(GINN) to learn the code embeddings. GINN
    takes the program’s control flow graph as input and abstracts it using three operators.
    The CFG is partitioned into a series of intervals using the partitioning operator.
    Messages between intervals passing are restricted. Then the heightening operator
    is applied to replace each activate interval with single created nodes until the
    sufficient propagation point is reached. The lowering operator will finally recover
    the original CFG. GINN model use only looping construct to learn feature representation,
    and the method shows improvement across program embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Combined Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some cases, the two types of flow graphs do not appear separately as well
    as AST. The following part will focus on how these structures are combined and
    what information can be retrieved from them.
  prefs: []
  type: TYPE_NORMAL
- en: Control Flow and Data Flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the combination of two types of graphs, some novel value graph is presented
    such as the program’s Interprocedural Value-Flow Graph (IVFG). The IVFG is created
    using LLVM-IR, which combines code control-flow and alias-aware data-flows. IVFG
    is a directed graph with multiple edges. Flow2Vec(Sui et al., [2020](#bib.bib144))
    is a novel approach for embedding the code with IVFG. Pre-embedding, value-flow
    reachability via matrix multiplication, and high order proximity approximation
    are the three steps in the method. Brauckmann et al.(Brauckmann et al., [2020](#bib.bib29))
    use AST or control-data flow graphs (CDFGs) as the input of the predictive model
    to learn the code representation. The core of the predictive model is the GGNN
    in the embedding propagation layer. The model has shown its effectiveness in two
    complex tasks on OpenCL kernels, the CPU/GPU mapping, and Thread Coarsening. Deepsim
    (Zhao and Huang, [2018](#bib.bib197)) encodes both code control flow graph and
    data flow graph into a semantic matrix and uses a deep learning model to measure
    function similarity. The semantic matrix contains three features: variable features,
    basic block features, and relationship features between variables and basic blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: AST and Flow Graphs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Allamanis et al. (Allamanis et al., [2018b](#bib.bib6)) propose a method based
    on GGNN to construct graphs for source code, naming variables, and detecting variable
    misuse. The program graph combines both AST and the data flow graph, which contains
    both syntactic and semantic structure information of source code. Some works combine
    AST and CFG for multi-modal learning for generating the hybrid representation
    of code, such as (Wan et al., [2019a](#bib.bib156)). Devign (Zhou et al., [2019](#bib.bib198))
    utilizes GGNN with a Conv module for graph-level classification. The graph representation
    is based on AST structure and added multiple types of edges from CFG, DFG, and
    NCS. The Conv module learns the hierarchy information of representation of node
    features. Multi-modal learning can reduce the limitation of the approaches only
    using AST to represent the code. The combination of AST and Flow Graphs can cover
    extra semantic information of source code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Other Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section will discuss other structures that differ significantly from the
    AST and flow graphs. Although the AST and flow-graphs have been employed in most
    previous works, a program project still contains other graphs. Some are traditional
    graphs, such as the Program Dependency Graph and UML Diagrams presented in [2.1.5](#S2.SS1.SSS5
    "2.1.5\. Other Structures ‣ 2.1\. Structures in Code ‣ 2\. Preliminary ‣ A Survey
    of Deep Learning Models for Structural Code Understanding"), while others are
    recently proposed structures for better semantic-aware or structure-aware code
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Program Dependence Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Li et al.(Li et al., [2019b](#bib.bib100)) propose an attention-based neural
    network to learn code representation for the bug detection task. The global context
    is extracted by the Program Dependence Graph and Data Flow Graph using Node2Vec,
    while the local context is extracted by previous bug fixes and AST paths. The
    global context and local context will be unified as the path vector for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: UML Diagrams
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CoCoSUM(Wang et al., [2021a](#bib.bib165)) model the UML diagrams for code summarization
    task. The framework encodes the class names with a transformer-based model as
    the intra-class context and the UML diagrams with a Multi-Relational Graph Neural
    Network (MRGNN) as the inter-class context. The two kinds of embeddings together
    with the embeddings of the token and AST will be passed to an attention-based
    decoder to generate code summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Code Property Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Yamaguchi et al.(Yamaguchi et al., [2014](#bib.bib181)) model three types of
    code-related structures: ASTs, CFGs, and Program Dependency Graph (Ferrante et al.,
    [1987b](#bib.bib55)) as property graphs and combine them into code property graph.
    The newly proposed data structure enables characterizing the vulnerability type
    through graph traversals. Liu et al.(Liu et al., [2020a](#bib.bib112)) use the
    code property graph and propose a Hybrid GNN framework in the code summarization
    task. The framework fuse the static graph and dynamic graph to capture the global
    information of graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Program Feedback Graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Yasunaga et al.(Yasunaga and Liang, [2020a](#bib.bib186)) introduce a program
    feedback graph to model the reasoning process in program repair task. The nodes
    in the program feedback graph consist of tokens in the diagnostic arguments, the
    occurrences in the source code, and the remaining identifiers in the code. The
    framework DrRepair uses LSTM to encode the source code initially and GAT to further
    reason over the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Discussion and Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have introduced two types of models in code understanding: sequence-based
    and graph-based models in section [3](#S3 "3\. Sequence-based Models ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") and Section [4](#S4
    "4\. Graph-based Models ‣ A Survey of Deep Learning Models for Structural Code
    Understanding") respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-based models are models for processing code sequences such as NCS and
    flatten sequences obtained from AST by several transformation methods introduced
    in Section [3.1](#S3.SS1 "3.1\. Structure Transformation ‣ 3\. Sequence-based
    Models ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    Traditional statistical language models such as N-gram models are used in a large
    amount of early works. With the development of deep learning, word2vec and DBN
    models have also been used for code representation and downstream tasks. Furthermore,
    CNN which has revolutionized the computer vision field, can effectively capture
    rich structural patterns in sequences and are therefore naturally adopted for
    code sequence tasks. However, the most dominant models among sequence-based models
    are vanilla RNN models with their variants, such as LSTM, GRU, and Bi-LSTM, which
    are tailored for sequence modeling tasks. In recent years, transformer-based models
    that incorporate self-attention blocks have made major contributions to code sequence-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based models are models that use the graph structures generated by codes
    such as AST and Flow Graphs to capture structural information. The transformation
    of graph structures has also been introduced in Section [4.1](#S4.SS1 "4.1\. Structure
    Transformation ‣ 4\. Graph-based Models ‣ A Survey of Deep Learning Models for
    Structural Code Understanding"). Some of these structures, especially AST can
    be seen as a tree or a graph, which leads to a different way to process the structures.
    From a tree perspective, these models use RNN models designed for tree structures,
    such as Tree-LSTM. From the perspective of the graph, CNN is also one of the most
    used models, while graph neural networks play an important role in the pipeline
    to learn the representation of nodes or graphs, such as GGNN, GCN, GAT, or MPNN
    with both AST and flow graphs. Above AST and Flow Graphs, there are also some
    combination structures and other rarely seen structures, for example, UML diagrams,
    Program Dependency Graph and so on used in graph-based models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two categories of models have both shown the effectiveness on code representation
    and downstream tasks which we will introduce in Section [6](#S6 "6\. Tasks ‣ A
    Survey of Deep Learning Models for Structural Code Understanding"). In this section,
    we emphasize three differences between the two types of models:'
  prefs: []
  type: TYPE_NORMAL
- en: First, sequence-based models and graph-based models view data in a different
    perspective. Because sequence-based models treat code as a collection of sequences,
    they must properly capture the relationships between sequences that correspond
    to semantic and syntactic information in the source code. Graph-based models view
    code data as a tree or a graph. Since nodes and edges in code graphs are rich
    in structural information, these models learn the representation in graphs to
    better understand the code. However, the models that use only sequential data
    are neglecting the syntactic or semantic information in structures, while the
    models learn solely on graph structures ignore the sequential information. As
    previously introduced, the boundary of the two kinds of models is not clear if
    we divide them with the information they used. There are sequences flattened from
    the AST structure that automatically combine the structural information in AST.
    In Graph-based models, some methods also use sequential information in code.
  prefs: []
  type: TYPE_NORMAL
- en: Second, sequence-based and graph-based models are both use serial models but
    with distinct proposes and scenarios. RNN and Transformer are the most commonly
    used serial models in deep learning models in code understanding, but the two
    types of models use RNN and Transformer with different proposes. Sequence-based
    models use RNN and Transformer as the core component of the models to learn relationships
    in code sequences. The RNN(RaychevVeselin et al., [2014](#bib.bib133); Ben-Nun
    et al., [2018a](#bib.bib24)) is used in the sequence-based models to tackle the
    problem of information passing in the code sequence. LSTM(Dam et al., [2016](#bib.bib43);
    Iyer et al., [2016](#bib.bib73)) and GRU(LeClair et al., [2019](#bib.bib90); Hu
    et al., [2020a](#bib.bib69)) which are the variant of RNN use the gating mechanism
    to better transfer useful information and solve the problem of gradient explosion
    and disappearance in code sequences. Transformer-based models(Ahmad et al., [2020](#bib.bib3);
    Alon et al., [2020](#bib.bib11)) are more commonly employed to address the issue
    of global reliance and use positional embedding to maintain the order information
    of code sequence, allowing for better learning of the entire code information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although RNN-based model designed for serial data, they are widely used in
    graph-based models as sequences are the original form of code that contains the
    context information. Furthermore, although Graph Neural Network can capture the
    structural information of structures, the context information cannot be preserved
    once the source code is converted to a graph structure. Therefore, approaches
    in graph-based models will also use RNN such as LSTM or GRU to gather long-distance
    information. Some works use RNN for tree structures such as RvNN(Shi et al., [2021](#bib.bib139)),
    or tree-LSTM(Wan et al., [2019a](#bib.bib156)). Most of the works combined the
    RNN with GNN as the whole model frameworks, mainly categorized as the following
    three usages: 1) Conducting bidirectional GRU(Zhang et al., [2019a](#bib.bib194))
    or bidirectional LSTM as the encoder of the models, 2) using GRU to fuse resulting
    vectors after GNN components(Liu et al., [2020a](#bib.bib112)), 3) using RNN such
    as LSTM as decoder(Wang et al., [2020b](#bib.bib167)). Graph Sandwich Structure(Hellendoorn
    et al., [2019](#bib.bib63)) is one of the most typical works that combine GNN
    with RNN and Transformer structure to combine both local and global information.'
  prefs: []
  type: TYPE_NORMAL
- en: Third, Attention mechanism are used in sequence-based and graph-based models
    but with various modifications.
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism is widely used in neural machine translation, especially
    in encoder-decoder frameworks. The attention mechanism can learn which words are
    important and further make predictions according to the importance of words. In
    code representation-related works, attention mechanisms are conducted in both
    sequence-based models and graph-based models.
  prefs: []
  type: TYPE_NORMAL
- en: In sequence-based models, the attention mechanism is primarily used to pay attention
    to the relationships between tokens in code sequences, including the relationships
    between input sequence tokens, between output sequence tokens and input sequence
    tokens, or between output sequence tokens, in order to facilitate more efficient
    information transmission. The Transformer-based models use self-attention to focus
    on the three relationships mentioned above while the other models pay more attention
    to the relationship between output sequence tokens and input sequence tokens.
    Attention mechanisms are added to RNN-based models by CodeNN(Iyer et al., [2016](#bib.bib73))
    and other models, allowing the model to make greater use of crucial token information
    while creating code summary or predicting the next token.
  prefs: []
  type: TYPE_NORMAL
- en: Some work in graph-based models conduct attention mechanism before output layer
    and create a context vector to predict the next token in the sequence, such as
    LeClair et al.(LeClair et al., [2020](#bib.bib89)). Others make modifications
    on vanilla deep learning models with attention mechanism, for example, using attention
    mechanism in message propagation process of GCN to hierarchically update the embeddings(Ji
    et al., [2021b](#bib.bib75)), combining attention mechanism in GRU layer and Convolutional
    layer to encode the order of the nodes in a path (Li et al., [2019b](#bib.bib100)).
    Different from sequence-based models, some works modify the original attention
    mechanism to better suit the hierarchical structure. Liu et al(Liu et al., [2021c](#bib.bib113))
    propose low-level attention and high-level attention. The former attention module
    attends the token in a sequence while the latter attends the code cell in the
    AST tree.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We categorized the tasks used in code understanding into the following downstream
    tasks and summarized the sequence-based models and graph-based models on each
    downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Code Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code Generation can provide varying levels of granularity of code output depending
    on the input. Code prediction in IDE that anticipates blank areas of input code
    snippets, such as method name prediction, next token prediction, and so on, is
    a typical code generation task. It’s also a kind of code generation task to generate
    snippets of code from natural language. Code generation considerably enhances
    developers’ efficiency and has been extensively researched in both industry and
    academia. The relevant approaches are summarized in Table [2](#S6.T2 "Table 2
    ‣ 6.1.4\. Function generation ‣ 6.1\. Code Generation ‣ 6\. Tasks ‣ A Survey of
    Deep Learning Models for Structural Code Understanding"), which is based mostly
    on the structure used by the model in the code generation task.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1\. Method name generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This task generates a summary method name based on the given method code body,
    which can help the code be more understandable, maintainable, and invocable. The
    formalization of the method name generation task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a code body of a method with n tokens $S={t_{1},t_{2},...t_{n}}$, the
    generative model can output the method name.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2\. Next token generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This task predicts the tokens including API calls that will be inputted by
    the developer. The models of the next token generation typically provide a list
    of tokens in order of probability depending on the developer’s previous input.
    It can substantially speed up development. The formalization of the next token
    generation task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a partial code snippet with $n-1$ tokens $S={t_{1},t_{2},...t_{n-1}}$,
    the generative model can generate a token list of $t_{n}$ sorted by probability.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3\. Expression generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Compared to the next token generation, this task is a more granular code prediction
    task based on existing code fragments. It generates complete code expressions
    with certain functions such as conditional statements, loops, etc. It also includes
    the task of predicting missing code statements. The formalization of the next
    expression generation task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a partial code snippet with $n$ tokens $S={t_{1},t_{2},...,t_{n-1}}$,
    the generative model can generate a whole code expression $S_{m}={t_{n},t_{n+1}...,t_{n+m-1}}$
    with a specific function. Or given a code with a missing piece $S={t_{1},t_{2},...,missingpiece,...,t_{n}}$,
    the generative model generates the missing piece code.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4\. Function generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This task can generate corresponding code snippets based on the user’s description
    of the code’s functionality, which can be seen as a natural language to the code
    translation process. Excellent natural language to code translation techniques
    can enable non-specialists to develop accordingly, but work in this area is still
    immature and needs to be further developed. The formalization of the function
    generation task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a natural language $D$ for the functional description of the code $S$,
    the generative model can generate the $S={t_{1},t_{2},...,t_{n}}$ according to
    $D$.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Code Generation
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Generation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | Code2seq(Alon et al., [2018a](#bib.bib9)) | BiLSTM | method name
    | Represents code by a collection of paths between terminal nodes n the AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | Code2vec(Alon et al., [2019c](#bib.bib15)) | LSTM | method name | Generates
    code’s representation by AST path |'
  prefs: []
  type: TYPE_TB
- en: '|  | SLM(Alon et al., [2020](#bib.bib11)) | LSTM | expression | Generates missing
    code using all the paths to it |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pythia(Svyatkovskiy et al., [2019](#bib.bib147)) | LSTM | next token |
    Predicts the methods and API calls by flattened AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Liu et al., [2017](#bib.bib108)) | LSTM | next token | Uses the sequence
    obtained by traversing the AST to predict the next possible node |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Li et al., [2017b](#bib.bib94)) | LSTM, Pointer Network | next token
    | Generates code through LSTM or pointer network |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Kim et al., [2020](#bib.bib83)) | Transformer | next token | Feeds different
    paths to Transformer |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Liu et al., [2020c](#bib.bib110)) | Transformer-XL | next token | Feeds
    different paths to Transformer-XL with multi-task learning |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Alon et al., [2018c](#bib.bib13)) | Word2vec | method name | Feeds different
    paths with up/down momentum to Word2vec |'
  prefs: []
  type: TYPE_TB
- en: '|  | API2Vec(Nguyen et al., [2017](#bib.bib123)) | Word2vec | next token |
    Feeds different API paths to Word2vec |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | DEEP3 (Raychev et al., [2016](#bib.bib132)) | Decision tree | next
    token | Represents code in a DSL called TGEN and build probabilistic models with
    decision trees |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Brockschmidt et al., [2019](#bib.bib30)) | GRU,GGNN | expression | A
    generated model on ExprGen task which first obtains a graph by attribute grammars
    and later compute the attribute representations with GGNN and GRU |'
  prefs: []
  type: TYPE_TB
- en: '|  | CCAG(Wang and Li, [2021](#bib.bib166)) | GAT | next token | Uses AST Graph
    Attention Block(ASTGab) to model the flattened sequence of AST to capture different
    dependencies |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | method name | Constructs
    graphs by adding different types of edges and use GGNN to learn the representation
    of the graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | Graph-Structured Cache(Cvitkovic et al., [2019](#bib.bib41)) | MPNN,CharCNN
    | method name | Introduces a Graph-Structured Cache representing vocabulary words
    as additional nodes, uses MPNN and CharCNN to generate outputs to further address
    open vocabulary issue |'
  prefs: []
  type: TYPE_TB
- en: 6.2\. Code Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code summarization is the work of using natural language to provide a concise
    explanation of the code’s functioning. It can help enhance the code’s readability,
    as well as the developer’s efficiency in understanding the program. The code summarization
    task can be thought of as a translation of the input code into natural language,
    hence a seq2seq model architecture is commonly used. In the encoder phase, (Hu
    et al., [2018](#bib.bib68), [2020a](#bib.bib69); LeClair et al., [2019](#bib.bib90))
    convert the input NSC or Flattened sequence into a context vector, and then in
    the decoder phase, they construct the words in the summarization one by one based
    on the context vector. To increase the effectiveness of code summarization, techniques
    such as the attention mechanism are applied to the task. The relevant techniques
    are summarized in Table [3](#S6.T3 "Table 3 ‣ 6.2\. Code Summarization ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") based on
    the model’s relevant code structures.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Code Summarization
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | DeepCom (Hu et al., [2018](#bib.bib68)) | LSTM | Employs the seq2seq
    model which uses LSTM as encoder and decoder to generate code fragment summaries
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hybrid-Deepcom (Hu et al., [2020a](#bib.bib69)) | LSTM | Utilizes two
    GRU as encoders for NCS and flattened sequence |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ast-attendgru (LeClair et al., [2019](#bib.bib90)) | GRU | Uses two GRU
    with attention mechanism to process NCS and flattened sequence respectively for
    getting context vector |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Allamanis et al., [2016](#bib.bib8)) | CNN | Uses Convolutional Attention
    Network to summarize the code |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Iyer et al., [2016](#bib.bib73)) | LSTM | Uses LSTM with attention to
    produce sentences that describe code snippets |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Zhang et al., [2020b](#bib.bib193)) | search engine | Retrieves flattened
    sequence on the search engine to obtain syntactically similar code fragments |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Ahmad et al., [2020](#bib.bib3)) | Transformer | Employs transformer
    for code summarization |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | (Wan et al., [2018](#bib.bib158)) | RNN, Tree-RNN | Uses a attention
    layer to fuse two representations, one from the structure of source code with
    AST-based LSTM, the other from the sequence with LSTM |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Fernandes et al., [2019](#bib.bib53)) | GGNN | Uses sequential encoder
    and a GGNN to generate the representations |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBCAA(Chen et al., [2019b](#bib.bib36)) | Tree-based LSTM | Uses Tree-based
    convolution for API-enhanced AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | (LeClair et al., [2020](#bib.bib89)) | ConvGNN | Encodes the node token
    embedding with recurrent layer and a ConvGNN, later uses an attention layer to
    learn important tokens |'
  prefs: []
  type: TYPE_TB
- en: '|  | Flow2Vec (Sui et al., [2020](#bib.bib144)) | Flow2Vec | Pre-embeds the
    interprocedural value-flow graph, considers the reachability via matrix multiplication
    problem and uses it to approximate the high-order proximity embedding |'
  prefs: []
  type: TYPE_TB
- en: '|  | BASTS (Lin et al., [2021](#bib.bib104)) | Tree-LSTM | Uses Tree-LSTM and
    Transformer architecture to combine the representations of split AST and source
    code |'
  prefs: []
  type: TYPE_TB
- en: '|  | CoCoSum (Wang et al., [2021a](#bib.bib165)) | Transformer, Multi-Relational
    GNN | The global encoder contains an MRGNN to embed the UML class diagrams and
    a transformer-based model for embedding the class names, while the local encoder
    uses the GRU |'
  prefs: []
  type: TYPE_TB
- en: '|  | HybridGNN (Liu et al., [2020a](#bib.bib112)) | GNN | A hybrid message
    passing GNN based which fuse the static and dynamic graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Shi et al., [2021](#bib.bib139)) | RNN, attention | A convolutional attention
    network for extreme summarization of source code |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Liu et al., [2021c](#bib.bib113)) | HAConvGNN | Hierarchically splits
    the AST into subtrees, learns the representation of split AST, and reconstructs
    them to combine the structural and semantic information with RvNN |'
  prefs: []
  type: TYPE_TB
- en: 6.3\. Code Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code search is a information retrieval task. The input of code search is the
    query which is natural language description or a code snippet. And the output
    is the best matching code snippets for input. The goal of Code Search is to retrieve
    code snippets from a large code corpus that most closely match a developer’s intent(Cambronero
    et al., [2019b](#bib.bib33)). Being able to explore and reuse existing code that
    is match to the developer’s intent is a fundamental productivity tool. Some online
    sites such as Stack Overflow are popular because of the convenience that searching
    for code relevant to a user’s question expressed in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Some articles refer to this task as semantic code search or code recommendation,
    these names emphasize characteristics of code search. It is based on the semantics
    of the input, and the semantic alignment between input and code snippets is crucial.
    And the information retrieval nature of this task is that all outputs are retrieved
    from the code corpus as they originally are. The retrieval nature distinguishes
    code search and code generation, where the generation task is aiming to synthesize
    and write codes not only already in the code corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The code search models are summarized in Table [4](#S6.T4 "Table 4 ‣ 6.3\. Code
    Search ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding").
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. Code Search
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | (Sachdev et al., [2018](#bib.bib137)) | word2vec | Combines natural
    language techniques and information retrieval methods |'
  prefs: []
  type: TYPE_TB
- en: '|  | CODEnn (Gu et al., [2018](#bib.bib58)) | RNN | Jointly embeds code snippets
    and natural language descriptions into a high-dimensional vector space |'
  prefs: []
  type: TYPE_TB
- en: '|  | UNIF(Cambronero et al., [2019b](#bib.bib33)) | word2vec | Uses attention
    to combine per-token embeddings and produce the code sentence embedding |'
  prefs: []
  type: TYPE_TB
- en: '|  | CARLCS-CNN (Shuai et al., [2020](#bib.bib143)) | CNN | Uses CNN to embed
    code and query respectively |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | TBCAA(Chen et al., [2019b](#bib.bib36)) | tree-based LSTM | Tree-based
    convolution for API-enhanced AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | MMAN(Wan et al., [2019a](#bib.bib156)) | GGNN, Tree-LSTM | A multi-modal
    Attention Network that uses attention mechanism to capture the information from
    an LSTM for embedding sequential tokens, a Tree-LSTM for embedding AST, and a
    GGNN for representing CFG |'
  prefs: []
  type: TYPE_TB
- en: 6.4\. Clone Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clone detection task indicates that there are two or multiple similar code
    snippets in the same software or system, which is also known as code clones. The
    code clones can support the modifications by the developers for better reusing
    them. Code clones can be described as the following 4 types (Bellon et al., [2007](#bib.bib23)):'
  prefs: []
  type: TYPE_NORMAL
- en: Type-1 clones are identical code fragments, which may contain slight differences
    in white-space, layouts, or comments.
  prefs: []
  type: TYPE_NORMAL
- en: Type-2 clones are identical code fragments, which may contain the differences
    of variable names, constants, function names, identifiers, literals, types, layouts,
    white-space, or comments.
  prefs: []
  type: TYPE_NORMAL
- en: Type-3 clones are syntactically similar code fragments with added, deleted,
    or modified statements.
  prefs: []
  type: TYPE_NORMAL
- en: Type-4 clones are semantic similar code fragments that may use different lexical
    and syntax to express the equivalent semantic.
  prefs: []
  type: TYPE_NORMAL
- en: As the similarities decrease in the four types, the difficulty of detecting
    the clones increases. The approaches to solve the clone detection is shown in
    Table [5](#S6.T5 "Table 5 ‣ 6.4\. Clone Detection ‣ 6\. Tasks ‣ A Survey of Deep
    Learning Models for Structural Code Understanding").
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Clone Detection
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | CCLearner (Li et al., [2017a](#bib.bib97)) | DNN | Extracts tokens
    from known method-level code clones and non-clones to train a classifier |'
  prefs: []
  type: TYPE_TB
- en: '|  | (White et al., [2016](#bib.bib171)) | RNN | Uses RNN modeling sequences
    of terms in a source code corpus |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | DeepSim (Zhao and Huang, [2018](#bib.bib197)) | Feed-forward neural
    network | An approach for measuring code functional similarity that uses two flow
    graphs as the basis and encodes them with Feed-forward neural network |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASTNN (Zhang et al., [2019a](#bib.bib194)) | bidirectional RNN | Encodes
    the statement subtree and uses Bi-GRU to model the naturalness of the statements
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | TBCAA (Chen et al., [2019b](#bib.bib36)) | tree-based LSTM | Tree-based
    convolution for API-enhanced AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | DEEPBINDIFF (Duan et al., [2020](#bib.bib48)) | Text-associated DeepWalk
    | Learns basic block embeddings with Text-associated DeepWalk algorithm, and match
    them with the k-hop greedy matching algorithm |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Yu et al., [2020](#bib.bib191)) | MPNN, CNN | Use BERT to pre-train token
    and block embeddings on an MLM task, and fine-tune them on 2 graph-level tasks
    with MPNN, GRU, and CNN |'
  prefs: []
  type: TYPE_TB
- en: '|  | CSEM (Li et al., [2020](#bib.bib92)) | Transformer, GAT, CNN | Converts
    source code to intermediate representation, generates Node vector matrix and inputs
    it into GAT later and CNN layer to obtain embedding of code fragment |'
  prefs: []
  type: TYPE_TB
- en: '|  | OSCAR (Peng et al., [2021](#bib.bib127)) | Transformer | A hierarchical
    multi-layer Transformer pre-trained model with a novel positional encoding, contrastive
    learning with optimization techniques |'
  prefs: []
  type: TYPE_TB
- en: '|  | HAG (Ji et al., [2021b](#bib.bib75)) | GCN | Uses GCN with layer-wise
    propagation and attention mechanism |'
  prefs: []
  type: TYPE_TB
- en: 6.5\. Safety Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Humans are relying more and more on programs and codes to handle various problems
    in life as computer technology advances, and defects and vulnerabilities in codes
    can result in significant losses. As a result, it is vital to examine the code’s
    reliability and security. Defects in code can cause programs to fail to run properly,
    and vulnerability can pose a potential threat to the safe operation of computer
    systems. Furthermore, Malware is Malicious software which is designed to attack
    the device. Therefore, in Safety analysis, we categorized three safety-related
    tasks: defect prediction, vulnerability prediction, and malware classification
    and further summarize the models for these three categories in Table [6](#S6.T6
    "Table 6 ‣ 6.5.3\. Malware Classification ‣ 6.5\. Safety Analysis ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1\. Defect Prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Defect prediction helps developers test more effectively while lowering the
    cost of software development by predicting areas of problematic code. The two
    types of defect prediction tasks now accessible are within-project defect prediction
    (WPDP), in which the training and test sets are from the same project, and cross-project
    defect prediction (CPDP), in which the test set is different from the training
    set. The formalization of the Defect prediction task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model can output a label $y$ which means the code snippet with defects(Buggy)
    or without defects(clean).
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2\. Vulnerability Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Vulnerability detection task, including vulnerability detection based on
    code similarity and code patterns, can prevent code from being attacked and improve
    the security of code. Some approaches use deep learning for vulnerability detection
    ,for example, VulDeePecker (Li et al., [2018](#bib.bib102)). The formalization
    of the Vulnerability detection task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model can output a label $y$ which means the code snippet with or without vulnerability.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3\. Malware Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Malware classification is one kind of malware detection, which is a binary
    classification problem. Malware classification can be formalized as: Given a program
    P, classify P as a normal program or malware.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6\. Safety analysis
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Kind | Description |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | (Wang et al., [2016](#bib.bib162)) | DBN | Defect | Uses DBN to
    automatically learn the semantic expression of code |'
  prefs: []
  type: TYPE_TB
- en: '|  | Seml(Liang et al., [[n.d.]](#bib.bib103)) | LSTM | Defect | Predicts software
    defect with the help with LSTM |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeepCPDP(Chen et al., [2019a](#bib.bib35)) | Bi-LSTM | Defect | Proposes
    SimAST and SimAST2Vec |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Wang et al., [2020a](#bib.bib161)) | DBN | Defect | Utilizes the DBN
    to learn the higher-level semantic characteristics of code AST token |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Le et al., [2018](#bib.bib87)) | CNN-BiLSTM | Malware | Proposes an approach
    to enable malware classification by malware analysis non-experts |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Gibert et al., [2019](#bib.bib56)) | CNN | Malware | Proposes a file
    agnostic deep learning approach for malware categorization to efficiently group
    malicious software into families based on a set of discriminant patterns extracted
    from their visualization as images |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | (Allamanis et al., [2018b](#bib.bib6)) | GGNN | Defect | Constructs
    graphs by adding different types of edges and uses GGNN to learn the representation
    of the graph |'
  prefs: []
  type: TYPE_TB
- en: '|  | MAGIC (Yan et al., [2019](#bib.bib182)) | DGCNN | Defect | Extends the
    standard DGCNN on Weighted Vertices layer and Adaptive Max Pooling to aggregate
    attributes from graph structures |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Li et al., [2019b](#bib.bib100)) | GRU, CNN, Attention mechanism | Defect
    | Extracts the global (from PDG and DFG) and local (from AST) context with Attention-Based
    GRU, CNN. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Devign (Zhou et al., [2019](#bib.bib198)) | GGNN, GRU, CNN | Vulnerability
    | Encodes the code into a joint graph structure and uses GGNN with the Conv module
    to learn the embedding. |'
  prefs: []
  type: TYPE_TB
- en: '|  | BugGraph (Ji et al., [2021a](#bib.bib76)) | GTN | Vulnerability | A Graph
    Triplet-loss Network on the attributed CFG to learn similarity ranking. |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Brauckmann et al., [2020](#bib.bib29)) | GGNN | Malware | Uses GGNN for
    learning predictive compiler tasks on AST and CDFGs |'
  prefs: []
  type: TYPE_TB
- en: 6.6\. Bug Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Bug localization is a task that localizes the position of bugs in a buggy code
    snippet. The bug localization can also be seen as the previous step of program
    repair. Bugs fall into two categories based on how they are discovered: static
    and dynamic. The static bug location is determined by the control and data dependencies,
    whereas the dynamic bug location is determined by the program execution. We provide
    works of bug localization task in Table [7](#S6.T7 "Table 7 ‣ 6.6\. Bug Localization
    ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural Code Understanding").
    The formalization of bug localization task is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a code snippet with $n$ tokens $S=\{t_{1},t_{2},...t_{n}\}$, the predictive
    model will predict the position $\lambda$ of buggy token $t_{\lambda}$ such as
    the misused variables or operators. The place where the correct token is needed
    to predict is also called a slot.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7\. Bug Localization
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | (Vasic et al., [2019](#bib.bib153)) | LSTM | Presents multi-headed
    pointer networks for training a model that jointly and directly localizes and
    repairs variable-misuse bugs |'
  prefs: []
  type: TYPE_TB
- en: '|  | BULNER(Barbosa et al., [2019](#bib.bib22)) | Word2vec | Proposes a method
    for Bug Localization with word embeddings and Network Regularization |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | Uses GRU and GGNN as
    encoder and a token replacement mechanism as decoder to encode the token information
    and generate the fixing actions |'
  prefs: []
  type: TYPE_TB
- en: '|  | (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | Introduces a program
    feedback graph and apply GNN to model the reasoning process. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | Proposes Graph Interval
    Neural Network, which includes the heightening and lowering operator to learn
    the representation of CFG |'
  prefs: []
  type: TYPE_TB
- en: 6.7\. Program Repair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Program Repair, also known as bug fix, code refinement, aims at fixing the localized
    bugs. Some works perform the bug localization and program repair tasks jointly,
    for example, BugLab (Allamanis et al., [2021](#bib.bib7)). After detecting the
    bugs, the repair is conducted on the typical line of the program. Some works combine
    bug detection and program repair, which predict the location and fix action at
    the same time with a sequence combining two of them(Vasic et al., [2019](#bib.bib153)).
    Other repair tasks will only fix bugs assuming the bug locations already exist,
    such as CODIT(Chakraborty et al., [2020](#bib.bib34)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular tasks in automated program repair is the VARMISUSE task
    proposed by Allamanis(Allamanis et al., [2018b](#bib.bib6)). The VARMISUSE task
    is to automatically detect the variable misuse mistakes in source code and repair
    it with the correct variable. In other words, program repair task can be seen
    as a kind of code generation in the slot where the mistake is detected. However,
    due to the difference of input and the proposal of generating the code, we consider
    program repair as a new task and summarize the models in Table [8](#S6.T8 "Table
    8 ‣ 6.7\. Program Repair ‣ 6\. Tasks ‣ A Survey of Deep Learning Models for Structural
    Code Understanding").
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the diverse datasets, languages, and granularities of the output,
    different works have distinct definitions of program repair. For example, the
    output may be a single word to repair a misused variable or a full sentence to
    repair a new code snippet line. The formalization of the repair of a token is
    similar to the formalization in [6.6](#S6.SS6 "6.6\. Bug Localization ‣ 6\. Tasks
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). After
    detecting the positions, the program repair task will generate the correct token
    $t_{\lambda}^{\prime}$. Another of the formalizations of the program repair task
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Given a broken code snippet $S=\{l_{1},l_{2},...l_{k}\}$ (with $k$ line) the
    diagnostic feedback from compiler(the feedback usually contains line number and
    error message) $I$ , the program repair task is to localize the erroneous line
    index $n$ and generate a repaired code version $l_{n}^{{}^{\prime}}$ replacing
    the wrong code of $l_{n}$.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8\. Program Repair
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Reference | Model | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sequence | DeepFix(Gupta et al., [2017](#bib.bib60)) | GRU | Fixes multiple
    errors by iteratively invoking a trained neural network |'
  prefs: []
  type: TYPE_TB
- en: '|  | TFix(Berabi et al., [2021](#bib.bib27)) | Transformer | Uses T5(Raffel
    et al., [2019](#bib.bib131)) to accurately synthesize fixes to a wide range of
    errors |'
  prefs: []
  type: TYPE_TB
- en: '| Graph | CODIT (Chakraborty et al., [2020](#bib.bib34)) | LSTM | An encoder-decoder
    model which first learns the structural changes in AST modifications with tree-to-tree
    model, then predicts the token conditioned on the AST |'
  prefs: []
  type: TYPE_TB
- en: '|  | GGF (Wu et al., [2020](#bib.bib173)) | GGNN | Uses GRU and GGNN as encoder
    and a token replacement mechanism as decoder to encode the token information and
    generate the fixing actions |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hoppity (Dinella et al., [2020](#bib.bib46)) | GAT, LSTM | Introduces
    a program feedback graph and applies GNN to model the reasoning process |'
  prefs: []
  type: TYPE_TB
- en: '|  | GINN (Wang et al., [2020b](#bib.bib167)) | GINN | Proposes Graph Interval
    Neural Network, which includes the heightening and lowering operator to learn
    the representation of CFG |'
  prefs: []
  type: TYPE_TB
- en: 6.8\. Pre-training Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pre-training is the process of training a model on a large amount of pre-training
    data to extract as many features as possible from the data so that the model can
    better solve the aiming task after fine-tuning in specific dataset. With the proliferation
    of open source corpora, pre-trained models have emerged for large amounts of code
    data, such as CodeBERT (Feng et al., [2020](#bib.bib52)),CuBert(Kanade et al.,
    [2020](#bib.bib80)), GPT-C(Svyatkovskiy et al., [2020](#bib.bib146)), CodeT5(Wang
    et al., [2021d](#bib.bib168))and GraphCodeBERT(Guo et al., [2020](#bib.bib59)),
    which can capture semantic information in code and be quickly and effectively
    applied to various downstream tasks. The pre-training models are becoming mainstream
    models for code-related tasks. A series of pre-training models have been proposed
    by large companies such as Microsoft and Facebook. Some models such as Alphacode²²2https://alphacode.deepmind.com/
    and Codex³³3https://openai.com/blog/openai-codex/ have been applied in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Metrics and Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We describe the metrics and datasets associated with code-related work in this
    section to serve as a reference for future work.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The relevant metrics in the code representation field are divided into two
    categories: NLP-related metrics and information retrieval-related metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1\. NLP-related Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the similarity between code and natural language, many metrics employed
    in the field of code are generated from the natural language field, particularly
    the generation tasks such as code summary.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy, Precision, Recall and F1-score One of the most intuitive performance
    metrics is accuracy, which is defined as the ratio of correct predictions to total
    predictions. Precision refers to the percentage of correct predictions of true
    samples among all predictions predicted as true samples whereas Recall refers
    to the percentage of correctly predictions of true samples among all true samples.
    The weighted average of Precision and Recall is the F1 Score. As a result, this
    score considers both false positives and false negatives. It is more useful, especially
    when the distribution of classes is uneven. The three metrics are commonly used
    in classification or prediction tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLEU BiLingual Evaluation Understudy(BLEU)(Papineni et al., [2002](#bib.bib126))
    is designed for automated evaluation of statistical machine translation and can
    be used to measure the performance of code summarization and generation tasks.
    The score is computed as Equation [1](#S7.E1 "In 2nd item ‣ 7.1.1\. NLP-related
    Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning
    Models for Structural Code Understanding") and [2](#S7.E2 "In 2nd item ‣ 7.1.1\.
    NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep
    Learning Models for Structural Code Understanding"), where the former is the Brevity
    Penalty(BP) with the length of the candidate translation $c$ and the effective
    reference sequence length $r$. TBP $p_{n}$ is the ratio of length n subsequences
    in the candidate that is also in the reference. And the $N$ in Equation [2](#S7.E2
    "In 2nd item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding") is usually
    set to 4(BLEU-4)(Hu et al., [2018](#bib.bib68)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathrm{BP}=\left\{\begin{array}[]{ll}1&amp;\text{ if }c>r\\ e^{(1-r/c)}&amp;\text{
    if }c\leq r\end{array}\right.$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| (2) |  | $\mathrm{BLEU}=\mathrm{BP}\cdot\exp\left(\sum_{n=1}^{N}w_{n}\log
    p_{n}\right)$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity (PPL) Perplexity is a great probabilistic measure used to evaluate
    exactly how confused the NLP models are. It’s typically used to evaluate language
    models, as well as the dialog generation tasks. PPL is defined as Equation [3](#S7.E3
    "In 3rd item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"),where $x_{i}$
    is the truth label and $P(x_{i})$ is the model output. A model with lower perplexity
    assigns higher probabilities to the true tokens and is expected to perform better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (3) |  | $PPL=\exp\left(-\sum_{i}^{T}P\left(x_{i}\right)\log P\left(x_{i}\right)\right),\forall
    i\in 0\ldots T.$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ROUGE As opposed to the BLEU score, the Recall-Oriented Understudy for Gisting
    Evaluation (ROUGE) evaluation metric(Lin, [2004](#bib.bib105)) measures the recall.It
    is commonly used in machine translation tasks to assess the quality of generated
    text. However, because it assesses recall, it is mostly employed in summarization
    tasks, where evaluating the amount of words the model can recall is more significant.
    (Lin, [2004](#bib.bib105)) proposes four Rouge methods: 1) ROUGE-N: calculate
    the recall rate on n-gram, 2) ROUGE-L: consider the longest common subsequence
    between the generated sequence $C$ and the target sequence $S$, 3) ROUGE-W: improve
    ROUGE-L and calculate the longest common subsequence by weighting method, and
    4) calculate the recall rate on n-gram that allows word skipping. The calculation
    method of ROUGE-L is shown in Equation [4](#S7.E4 "In 4th item ‣ 7.1.1\. NLP-related
    Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning
    Models for Structural Code Understanding"),where $F_{LCS}$ is ROUGE-L and $\beta$
    is a constant.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle Recall_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(S)}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle Precision_{LCS}$ | $\displaystyle=\frac{LCS(C,S)}{\operatorname{len}(C)}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle F_{LCS}$ | $\displaystyle=\frac{\left(1+\beta^{2}\right)Recall_{LCS}Precision_{LCS}}{Recall_{LCS}+\beta^{2}Precision_{LCS}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: METEOR The Metric for Evaluation of Translation with Explicit ORdering (METEOR)(Banerjee
    and Lavie, [2005](#bib.bib21)) is a precision-based metric for the evaluation
    of machine-translation output. It overcomes some of the pitfalls of the BLEU score,
    such as exact word matching whilst calculating precision. The METEOR score allows
    synonyms and stemmed words to be matched with a reference word. The most important
    thing in meteor is to use WordNet thesaurus to align the generated sequence with
    the target sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word Error Rate (WER) It is important to substitute, delete, or insert some
    words into the generated sequence during the generation task in order to keep
    the generated sequence consistent with the target sequence. WER, which is defined
    as Equation [5](#S7.E5 "In 6th item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics
    ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"), can be used to assess the quality of the generated sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (5) |  | $\mathrm{WER}=100\cdot\frac{Substitution+Deletion+Insertion}{N}\%$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeBLEU CodeBLEU(Ren et al., [2020](#bib.bib134)) is a metric designed for
    code based on BLEU, which can pay attention to the keywords, leverage the tree
    structure and consider the semantic logic. It is defined as the weighted combination
    of $\mathrm{BLEU}$, $\mathrm{BLEU}_{\text{weight }}$,$\mathrm{Match}_{\text{ast}}$
    and $\mathrm{Match}_{\text{df}}$, which is shown in Equation [6](#S7.E6 "In 7th
    item ‣ 7.1.1\. NLP-related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets
    ‣ A Survey of Deep Learning Models for Structural Code Understanding"). The first
    term refers to the standard $\mathrm{BLEU}$. $\mathrm{BLEU}_{\text{weight }}$
    is the weighted n-gram match, obtained by comparing the hypothesis code and the
    reference code tokens with different weights. $\mathrm{Match}_{\text{ast}}$ is
    the syntactic AST match, exploring the syntactic information of code. $\mathrm{Match}_{\text{df}}$
    is the semantic data-flow match, considering the semantic similarity between the
    prediction and the reference. The weighted n-gram match and the syntactic AST
    match are used to measure grammatical correctness, whereas the semantic data-flow
    match is used to calculate logic correctness.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (6) |  | CodeBLEU | $\displaystyle=\alpha\cdot\mathrm{BLEU}+\beta\cdot\mathrm{BLEU}_{\text{weightt
    }}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\gamma\cdot\mathrm{Match}_{\text{ast}}+\delta\cdot\mathrm{Match}_{\text{df}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 7.1.2\. Information Retrieval related Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because some code-related tasks, such as Code Search, are similar to information
    retrieval, many code-related metrics are associated to information retrieval field.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuccessRate(SR) If the information matching the input is in the top-k of the
    search information sorting list, the search is successful. SR@k Calculate the
    proportion of successful searches in all searches. The calculation method is shown
    in Equation [7](#S7.E7 "In 1st item ‣ 7.1.2\. Information Retrieval related Metrics
    ‣ 7.1\. Metrics ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models
    for Structural Code Understanding"), where $\delta$ is a constant, $Frank_{k}$
    is the rank of matched information in the search information sorting list and
    $Frank_{q}\leq K$ means successful retrieval.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (7) |  | $\mathrm{SR@K}=\frac{1}{&#124;Q&#124;}\sum_{q=1}^{Q}\delta\left(\operatorname{FRank}_{q}\leq
    K\right)$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank (MRR) In the retrieval information sorting list, MRR considers
    the ranking of the retrieved matching information. The score is $\frac{1}{N}$
    if the nth information in the list fits the input, and $0$ if there is no matching
    sentence. The sum of all scores is the final score. Equation [8](#S7.E8 "In 2nd
    item ‣ 7.1.2\. Information Retrieval related Metrics ‣ 7.1\. Metrics ‣ 7\. Metrics
    and Datasets ‣ A Survey of Deep Learning Models for Structural Code Understanding")
    shows how to calculate MRR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| (8) |  | $\mathrm{MRR}=\frac{1}{&#124;Q&#124;}\sum_{q=1}^{&#124;Q&#124;}\frac{1}{\operatorname{FRank}_{q}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best Hit Rank Best Hit Rank is the highest rank of the hit snippets for the
    query. A highest best hit implies lower user effort to inspect the desired hit
    snippet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7.2\. Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We summarize various datasets in the table [9](#S7.T9 "Table 9 ‣ 7.2\. Datasets
    ‣ 7\. Metrics and Datasets ‣ A Survey of Deep Learning Models for Structural Code
    Understanding"). Some works collect and generate their own datasets for study,
    which may cause the difficulty on comparing different works on the same tasks.
    Table [9](#S7.T9 "Table 9 ‣ 7.2\. Datasets ‣ 7\. Metrics and Datasets ‣ A Survey
    of Deep Learning Models for Structural Code Understanding") does not contain any
    datasets that are not open-source.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9\. Datasets
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Study | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [Genius Dataset](https://github.com/xiaojunxu/dnn-binary-code-similarity)
    | (Feng et al., [2016b](#bib.bib51); Xu et al., [2017](#bib.bib179)) | A real-world
    dataset of 33,045 devices which was collected from public sources and our system
    |'
  prefs: []
  type: TYPE_TB
- en: '| [notebookcdg](https://github.com/dakuo/haconvgnn) | (Liu et al., [2021c](#bib.bib113))
    | Has 28,625 code–documentation pairs |'
  prefs: []
  type: TYPE_TB
- en: '| [py150](http://www.srl.inf.ethz.ch/py150) | (Raychev et al., [2016](#bib.bib132);
    Kanade et al., [2019](#bib.bib79); Kim et al., [2020](#bib.bib83)) | Consists
    of parsed ASTs collected from GitHub python repositories by removing duplicate
    files |'
  prefs: []
  type: TYPE_TB
- en: '| [js150](http://www.srl.inf.ethz.ch/js150) | (Raychev et al., [2016](#bib.bib132);
    Wang and Li, [2021](#bib.bib166)) | Consists of 150’000 JavaScript files and their
    corresponding parsed ASTs |'
  prefs: []
  type: TYPE_TB
- en: '| [HGNN](https://github.com/shangqing-liu/CCSD-benchmark-for-code-summarization)
    | (Liu et al., [2020a](#bib.bib112)) | Crawled from diversified large-scale open-source
    C projects (total 95k+ unique functions in the dataset) |'
  prefs: []
  type: TYPE_TB
- en: '| [CodeSearchNet](https://github.com/github/CodeSearchNet) | (Husain et al.,
    [2019](#bib.bib71); Feng et al., [2020](#bib.bib52); Ugner et al., [[n.d.]](#bib.bib152))
    | Consists of 2 million (comment, code) pairs from open source libraries |'
  prefs: []
  type: TYPE_TB
- en: '| [CONCODE](https://github.com/sriniiyer/concode) | (Iyer et al., [2018](#bib.bib74);
    Allamanis, [2019](#bib.bib4)) | Consists over 100,000 examples consisting of Java
    classes from online code repositories |'
  prefs: []
  type: TYPE_TB
- en: '| [CodeXCLUE](https://github.com/microsoft/CodeXGLUE) | (Lu et al., [2021](#bib.bib116);
    Puri et al., [2021](#bib.bib129); Wang et al., [2021d](#bib.bib168)) | Includes
    a collection of 10 tasks across 14 datasets |'
  prefs: []
  type: TYPE_TB
- en: '| [TFix’s Code Patches Data](https://github.com/eth-sri/TFix) | (Berabi et al.,
    [2021](#bib.bib27)) | Contains more than 100k code patch pairs extracted from
    open source projects on GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| [CoDesc](https://github.com/csebuetnlp/CoDesc) | (Hasan et al., [2021](#bib.bib61))
    | A large dataset of 4.2m Java source code and parallel data of their description
    from code search, and code summarization studies |'
  prefs: []
  type: TYPE_TB
- en: '| [DeepFix](https://bitbucket.org/iiscseal/deepfix) | (Gupta et al., [2017](#bib.bib60);
    Yasunaga and Liang, [2020a](#bib.bib186); Chen et al., [2021](#bib.bib37)) | Consists
    of a program repair dataset (fix compiler errors in C programs) |'
  prefs: []
  type: TYPE_TB
- en: '| [SPoC](https://github.com/michiyasunaga/DrRepair) | (Yasunaga and Liang,
    [2020a](#bib.bib186)) | A program synthesis dataset, containing 18,356 programs
    with human-authored pseudocode and test cases |'
  prefs: []
  type: TYPE_TB
- en: '| [Defects4J](https://git.io/JJGwU) | (Chakraborty et al., [2020](#bib.bib34))
    | A large dataset of 32k real code change |'
  prefs: []
  type: TYPE_TB
- en: '| [FunCom](http://leclair.tech/data/funcom/) | (LeClair and McMillan, [2019](#bib.bib91);
    Shrivastava, [2021](#bib.bib142); Wei et al., [2019](#bib.bib169); Mahmud et al.,
    [2021](#bib.bib117)) | A collection of  2.1 million Java methods and their associated
    Javadoc comments |'
  prefs: []
  type: TYPE_TB
- en: '| [CoSQA](https://github.com/Jun-jie-Huang/CoCLR) | (Huang et al., [2021](#bib.bib70);
    Li et al., [2022](#bib.bib98)) | Includes 20,604 labels for pairs of natural language
    queries and codes, each annotated by at least 3 human annotators |'
  prefs: []
  type: TYPE_TB
- en: '| [CoNaLa](https://conala-corpus.github.io/) | (Yin et al., [2018b](#bib.bib189);
    Yin and Neubig, [2018](#bib.bib190)) | Consists 2379 training and 500 test examples
    that were manually annotated |'
  prefs: []
  type: TYPE_TB
- en: '| [Django](https://github.com/odashi/ase15-django-dataset) | (Oda et al., [2015](#bib.bib125);
    Lin et al., [2018](#bib.bib106)) | Comprises of 16000 training, 1000 development
    and 1805 test annotations |'
  prefs: []
  type: TYPE_TB
- en: '| [BLANCA](https://github.com/wala/blanca) | (Abdelaziz et al., [2021](#bib.bib2))
    | A collection of benchmarks that assess code understanding based on tasks |'
  prefs: []
  type: TYPE_TB
- en: '| [IndoNLG](https://github.com/indobenchmark/indonlg) | (Cahyawijaya et al.,
    [2021](#bib.bib31)) | A collection of Natural Language Generation (NLG) resources
    for Bahasa Indonesia with 6 kind of downstream tasks |'
  prefs: []
  type: TYPE_TB
- en: '| [Neural-Code-Search-Evaluation-Dataset](https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset)
    | (Li et al., [2019a](#bib.bib93)) | An evaluation dataset consisting of natural
    language query and code snippet pairs for code search |'
  prefs: []
  type: TYPE_TB
- en: 8\. Open Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is difficult to fully capture the information of the structures and semantics
    of codes with the existing technology. Most deep-learning models are designed
    for specific tasks and a single language, which are lack flexibility. The following
    open problems can be considered as the future work direction.
  prefs: []
  type: TYPE_NORMAL
- en: Information capturing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many approaches use structural information in code, however the majority of
    them just use structure information such as AST to capture syntax information.
    There are only a few approaches to learning the whole representation of code that
    combine structure and semantic information (such as DFG), and they are only applied
    to specific tasks, not all tasks. As a result, one of the goals of future study
    could be to improve the model’s ability to use the structure and semantic information
    of codes in various tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The methods we described above are suitable to a certain task or dataset and
    lack flexibility. The model’s flexibility implies it may be utilized in a range
    of scenarios, including those using datasets from shorter programs (with small
    graph structures) or smaller sample sizes, as well as scenarios involving several
    downstream jobs or programming languages. As a result, future study could concentrate
    on how to train a model that can accommodate a variety of circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Model Simplicity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Recent models, particularly those based on Transformer and its derivatives,
    have improved performance, but they often need more effort and machine capability.
    Therefore, it is necessary to propose lightweight model under the premise of ensuring
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deep learning approaches have always had explainability issues, and the approaches
    for code are no exception. While the current usage of attentional mechanisms in
    code generation tasks can explain the origins of token generation, there are still
    no good explanations for other tasks like safety analysis. Simultaneously, the
    model’s explainability is very useful in determining structural information and
    producing superior metrics in the code. As a result, additional research into
    the explainability of code models is still worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The robustness of code-domain models has not yet been studied, but as model
    performance improves, the robustness of code-domain models is sure to become a
    hot study area in the future. In the code domain, both sequence-based and graph-based
    models rely on the representation of code tokens to some extent, which leads to
    model performance reduction when test code fragments are not represented in the
    same way as training code fragments (e.g., different representations of API calls
    in the python language). Furthermore, while code graph-based models can better
    capture the information of code fragments, graph structures are sensitive to attacks.
    There have been numerous techniques to explore the robustness of graph models,
    and how to convert them to work on code graphs is an important research field.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the previous section, the metrics for evaluate the effectiveness of the
    code representation are based on the the Natural Language Processing area and
    Information Retrieval area. Although there are metrics designed specifically for
    code representation, there is a small amount of number of metrics proposed to
    suit the code data and downstream tasks. The following are the potential directions
    that can be studied:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Measure of information The requirement for appropriate measures for the information
    used in deep learning models is growing as the variety of structures employed
    in learning the representation for codes increases. Recent metrics that measure
    how models utilise this information in these structures are performed after the
    downstream tasks, however measures directly during the code representation stage
    have never been provided.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Measure of Explainability As previous mentioned, generating better metrics
    for measuring model efficacy on downstream tasks is vital for the explainability
    of models, which can better qualify how the model works.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Measure for Bias Problem The code data will inevitably contains repeat and
    duplication, which might contribute to a bias problem. As far as we know, there
    have been few studies and discussions on the bias problem in code representation.
    Therefore, it is a new future direction to consider the bias in code data, while
    reasonable metrics for measuring the bias of the models in code are required.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is critical to understand the structural and semantic meaning of codes when
    working on intelligent software. In this survey, We give a comprehensive overview
    of structure-based methods to code representation learning in recent years, which
    we divide into two groups: sequence-based models and graph-based models, then
    summarize and compare the methods in each group. The downstream tasks, as well
    as metrics and datasets, are also introduced here. It is shown that deep learning
    models are useful in code understanding, and further multiple downstream tasks.
    However, the field of code comprehension is still in its infancy, with numerous
    obstacles and unsolved issues. Finally, as future directions for code understanding,
    we offer four open questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abdelaziz et al. (2021) Ibrahim Abdelaziz, Julian Dolby, Jamie McCusker, and
    Kavitha Srinivas. 2021. Can Machines Read Coding Manuals Yet? – A Benchmark for
    Building Better Language Models for Code Understanding. *arXiv: Computation and
    Language* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmad et al. (2020) Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and
    Kai-Wei Chang. 2020. A transformer-based approach for source code summarization.
    *arXiv preprint arXiv:2005.00653* (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis (2019) Miltiadis Allamanis. 2019. The adverse effects of code duplication
    in machine learning models of code. In *Proceedings of the 2019 ACM SIGPLAN International
    Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software*.
    143–153.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2018a) Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
    and Charles Sutton. 2018a. A survey of machine learning for big code and naturalness.
    *ACM Computing Surveys (CSUR)* 51, 4 (2018), 1–37.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2018b) Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud
    Khademi. 2018b. Learning to represent programs with graphs. In *6th International
    Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings*.
    arXiv:1711.00740 [https://github.com/Microsoft/gated-graph-neural-network-samples](https://github.com/Microsoft/gated-graph-neural-network-samples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allamanis et al. (2021) Miltiadis Allamanis, Henry Jackson-Flux, and Marc Brockschmidt.
    2021. Self-Supervised Bug Detection and Repair. *Advances in Neural Information
    Processing Systems* 34 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Allamanis et al. (2016) Miltiadis Allamanis, Hao Peng, and Charles Sutton.
    2016. A Convolutional Attention Network for Extreme Summarization of Source Code.
    *arXiv: Learning* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2018a) Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018a.
    code2seq: Generating Sequences from Structured Representations of Code. *arXiv:
    Learning* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2019a) Uri Alon, Omer Levy, Shaked Brody, and Eran Yahav. 2019a.
    Code2Seq: Generating sequences from structured representations of code. *7th International
    Conference on Learning Representations, ICLR 2019* 1 (2019), 1–22. arXiv:1808.01400'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alon et al. (2020) Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural
    language models of code. In *International Conference on Machine Learning*. PMLR,
    245–256.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alon et al. (2018b) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2018b. A General Path-Based Representation for Predicting Program Properties.
    *ACM SIGPLAN Notices* 53, 4 (mar 2018), 404–419. [https://doi.org/10.1145/3192366.3192412](https://doi.org/10.1145/3192366.3192412)
    arXiv:1803.09544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alon et al. (2018c) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2018c. A general path-based representation for predicting program properties.
    In *Programming Language Design and Implementation*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2019b) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019b. Code2Vec: Learning Distributed Representations of Code. *Proceedings of
    the ACM on Programming Languages* 3, POPL (2019), 1–29. [https://doi.org/10.1145/3290353](https://doi.org/10.1145/3290353)
    arXiv:1803.09473'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2019c) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019c. code2vec: learning distributed representations of code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alreshedy et al. (2018) Kamel Alreshedy, Dhanush Dharmaretnam, Daniel M. German,
    Venkatesh Srinivasan, and T. Aaron Gulliver. 2018. SCC: Automatic Classification
    of Code Snippets. *arXiv: Software Engineering* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amodio et al. (2017) Matthew Amodio, Swarat Chaudhuri, and Thomas Reps. 2017.
    Neural Attribute Machines for Program Generation. *arXiv: Artificial Intelligence*
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arbaaz Qureshi et al. ([n.d.]) Syed Arbaaz Qureshi, Sonu Mehta, Ranjita Bhagwan,
    and Rahul Kumar. [n.d.]. Assessing the Effectiveness of Syntactic Structure to
    Learn Code Edit Representations. ([n. d.]). arXiv:2106.06110v1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arkesteijn and Saldanha ([n.d.]) Youri Arkesteijn and Nikhil Saldanha. [n.d.].
    Code Completion using Neural AAention and Byte Pair Encoding. ([n. d.]). arXiv:2004.06343v1
    [www.sri.inf.ethz.ch/py150](www.sri.inf.ethz.ch/py150)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
    Neural machine translation by jointly learning to align and translate. *arXiv
    preprint arXiv:1409.0473* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
    In *Meeting of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barbosa et al. (2019) Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini,
    Ricardo Britto, Frederico Soares, Solange Oliveira Rezende, Auri Marcelo Rizzo
    Vincenzi, and Márcio Eduardo Delamaro. 2019. BULNER: BUg Localization with word
    embeddings and NEtwork Regularization. *arXiv: Software Engineering* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellon et al. (2007) Stefan Bellon, Rainer Koschke, Giulio Antoniol, Jens Krinke,
    and Ettore Merlo. 2007. Comparison and Evaluation of Clone Detection Tools. *IEEE
    Transactions on Software Engineering* 33, 9 (2007), 577–591. [https://doi.org/10.1109/TSE.2007.70725](https://doi.org/10.1109/TSE.2007.70725)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben-Nun et al. (2018a) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
    Hoefler. 2018a. Neural Code Comprehension: A Learnable Representation of Code
    Semantics. *arXiv: Learning* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ben-Nun et al. (2018b) Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
    Hoefler. 2018b. Neural Code Comprehension: A Learnable Representation of Code
    Semantics. *Advances in Neural Information Processing Systems* 2018-December (jun
    2018), 3585–3597. arXiv:1806.07336 [https://arxiv.org/abs/1806.07336v3](https://arxiv.org/abs/1806.07336v3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio (2009) Yoshua Bengio. 2009. Learning Deep Architectures for AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berabi et al. (2021) Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin
    Vechev. 2021. TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.
    In *International Conference on Machine Learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhoopchand et al. (2016) Avishkar Bhoopchand, Tim Rocktäschel, Earl T. Barr,
    and Sebastian Riedel. 2016. Learning Python Code Suggestion with a Sparse Pointer
    Network. *arXiv: Neural and Evolutionary Computing* (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brauckmann et al. (2020) Alexander Brauckmann, Andrés Goens, Sebastian Ertel,
    and Jeronimo Castrillon. 2020. Compiler-based graph representations for deep learning
    models of code. In *Proceedings of the 29th International Conference on Compiler
    Construction*. 201–211.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brockschmidt et al. (2019) Marc Brockschmidt, Miltiadis Allamanis, Alexander
    Gaunt, and Oleksandr Polozov. 2019. Generative code modeling with graphs. In *7th
    International Conference on Learning Representations, ICLR 2019*. International
    Conference on Learning Representations, ICLR. arXiv:1805.08490 [https://arxiv.org/abs/1805.08490v2](https://arxiv.org/abs/1805.08490v2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cahyawijaya et al. (2021) Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,
    Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim,
    Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG:
    Benchmark and Resources for Evaluating Indonesian Natural Language Generation.
    *arXiv: Computation and Language* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cambronero et al. (2019a) José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik
    Sen, and Satish Chandra. 2019a. When deep learning met code search. In *FSE*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cambronero et al. (2019b) José Pablo Cambronero, Hongyu Li, Seohyun Kim, Koushik
    Sen, and Satish Chandra. 2019b. When deep learning met code search. In *Foundations
    of Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chakraborty et al. (2020) Saikat Chakraborty, Yangruibo Ding, Miltiadis Allamanis,
    and Baishakhi Ray. 2020. CODIT: Code Editing with Tree-Based Neural Models. *IEEE
    Transactions on Software Engineering* TBD (sep 2020), 1–1. [https://doi.org/10.1109/tse.2020.3020502](https://doi.org/10.1109/tse.2020.3020502)
    arXiv:1810.00314'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019a) Deyu Chen, Xiang Chen, Hao Li, Junfeng Xie, and Yanzhou
    Mu. 2019a. DeepCPDP: Deep Learning Based Cross-Project Defect Prediction. *IEEE
    Access* 7 (2019), 184832–184848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019b) Long Chen, Wei Ye, and Shikun Zhang. 2019b. Capturing source
    code semantics via tree-based convolution over API-enhanced AST. In *Proceedings
    of the 16th ACM International Conference on Computing Frontiers*. 174–182.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2021. SEQUENCER: Sequence-to-Sequence
    Learning for End-to-End Program Repair. *IEEE Transactions on Software Engineering*
    47 (2021), 1943–1959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Monperrus (2019) Zimin Chen and Martin Monperrus. 2019. A Literature
    Study of Embeddings on Source Code. (2019), 1–8. arXiv:1904.03061 [http://arxiv.org/abs/1904.03061](http://arxiv.org/abs/1904.03061)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
    representations using RNN encoder-decoder for statistical machine translation.
    *arXiv preprint arXiv:1406.1078* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compton et al. (2020) Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay.
    2020. Embedding Java Classes with code2vec: Improvements from Variable Obfuscation.
    In *Mining Software Repositories*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cvitkovic et al. (2019) Milan Cvitkovic, Badal Singh, and Anima Anandkumar.
    2019. Open vocabulary learning on source code with a graph-structured cache. In
    *36th International Conference on Machine Learning, ICML 2019*, Vol. 2019-June.
    International Machine Learning Society (IMLS), 2662–2674. arXiv:1810.08305 [https://arxiv.org/abs/1810.08305v2](https://arxiv.org/abs/1810.08305v2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2020) Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. 2020.
    Funnel-transformer: Filtering out sequential redundancy for efficient language
    processing. *Advances in Neural Information Processing Systems* 2020-Decem (2020),
    1–19. arXiv:2006.03236'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dam et al. (2016) Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A deep language
    model for software code. *arXiv preprint arXiv:1608.02715* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'de Rezende Martins and Gerosa (2020) Marcelo de Rezende Martins and Marco Aurélio
    Gerosa. 2020. CoNCRA: A Convolutional Neural Network Code Retrieval Approach.
    *arXiv: Learning* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denil et al. (2014) Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom,
    and Nando de Freitas. 2014. Modelling‚ Visualising and Summarising Documents with
    a Single Convolutional Neural Network. *arXiv: Computation and Language* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dinella et al. (2020) Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik,
    Le Song, and Ke Wang. 2020. Hoppity: Learning graph transformations to detect
    and fix bugs in programs. In *International Conference on Learning Representations
    (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2019) Steven H.H. Ding, Benjamin C.M. Fung, and Philippe Charland.
    2019. Asm2Vec: Boosting static representation robustness for binary clone search
    against code obfuscation and compiler optimization. *Proceedings - IEEE Symposium
    on Security and Privacy* 2019-May (2019), 472–489. [https://doi.org/10.1109/SP.2019.00003](https://doi.org/10.1109/SP.2019.00003)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duan et al. (2020) Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. 2020.
    Deepbindiff: Learning program-wide code representations for binary diffing. In
    *Network and Distributed System Security Symposium*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. (2021) Sen Fang, You-Shuai Tan, Tao Zhang, and Yepang Liu. 2021.
    Self-Attention Networks for Code Search. *Information & Software Technology* 134
    (2021), 106542.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2016a) Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian
    Testa, and Heng Yin. 2016a. Scalable graph-based bug search for firmware images.
    In *Proceedings of the ACM Conference on Computer and Communications Security*,
    Vol. 24-28-Octo. 480–491. [https://doi.org/10.1145/2976749.2978370](https://doi.org/10.1145/2976749.2978370)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2016b) Qian Feng, Rundong Zhou, Chengcheng Xu, Yao Cheng, Brian
    Testa, and Heng Yin. 2016b. Scalable Graph-based Bug Search for Firmware Images.
    In *Computer and Communications Security*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
    2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. *arXiv:
    Computation and Language* (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fernandes et al. (2019) Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt.
    2019. Structured neural summarization. In *7th International Conference on Learning
    Representations, ICLR 2019*. International Conference on Learning Representations,
    ICLR. arXiv:1811.01824 [https://arxiv.org/abs/1811.01824v4](https://arxiv.org/abs/1811.01824v4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrante et al. (1987a) Jeanne Ferrante, Karl J Ottenstein, and Joe D Warren.
    1987a. The program dependence graph and its use in optimization. *ACM Transactions
    on Programming Languages and Systems (TOPLAS)* 9, 3 (1987), 319–349.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrante et al. (1987b) Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren.
    1987b. The program dependence graph and its use in optimization. *ACM Transactions
    on Programming Languages and Systems (TOPLAS)* 9, 3 (jul 1987), 319–349. [https://doi.org/10.1145/24039.24041](https://doi.org/10.1145/24039.24041)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gibert et al. (2019) Daniel Gibert, Carles Mateu, Jordi Planes, and Ramon Vicens.
    2019. Using convolutional neural networks for classification of malware represented
    as images. *Journal of Computer Virology and Hacking Techniques* 15 (2019), 15–28.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol
    Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry.
    In *34th International Conference on Machine Learning, ICML 2017*, Vol. 3\. 2053–2070.
    arXiv:1704.01212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code
    search. In *International Conference on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert:
    Pre-training code representations with data flow. *arXiv preprint arXiv:2009.08366*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2017) Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade.
    2017. DeepFix: Fixing Common C Language Errors by Deep Learning. In *National
    Conference on Artificial Intelligence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hasan et al. (2021) Masum Hasan, Tanveer Muttaqueen, Abdullah Al Ishtiaq, Kazi Sajeed
    Mehrab, Md Haque, Mahim Anjum, Tahmid Hasan, Wasi Uddin Ahmad, Anindya Iqbal,
    and Rifat Shahriyar. 2021. CoDesc: A Large Code-Description Parallel Dataset.
    *arXiv preprint arXiv:2105.14220* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
    2020. Momentum Contrast for Unsupervised Visual Representation Learning. *Proceedings
    of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition*
    (2020), 9726–9735. [https://doi.org/10.1109/CVPR42600.2020.00975](https://doi.org/10.1109/CVPR42600.2020.00975)
    arXiv:1911.05722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hellendoorn et al. (2019) Vincent J Hellendoorn, Charles Sutton, Rishabh Singh,
    Petros Maniatis, and David Bieber. 2019. Global relational models of source code.
    In *International conference on learning representations*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermann et al. (2015) Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines
    to read and comprehend. *Advances in neural information processing systems* 28
    (2015), 1693–1701.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hindle et al. (2012) Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and
    Premkumar Devanbu. 2012. On the naturalness of software. In *ICSE*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long Short-Term Memory. *Neural Computation* 9, 8 (1997), 1735–1780. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020b) Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, and Mengting
    Yuan. 2020b. Neural joint attention code search over structure embeddings for
    software Q&A sites. *Journal of Systems and Software* 170 (2020), 110773.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2018) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep
    code comment generation. In *International Conference on Program Comprehension*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020a) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020a. Deep
    code comment generation with hybrid lexical and syntactical information. *Empirical
    Software Engineering* 25 (2020), 2179–2217.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu,
    Daxin Jiang, Ming Zhou, and Nan Duan. 2021. CoSQA: 20,000+ Web Queries for Code
    Search and Question Answering. *arXiv: Computation and Language* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of
    semantic code search. *arXiv preprint arXiv:1909.09436* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hussain et al. (2020) Yasir Hussain, Zhiqiu Huang, Yu Zhou, and Senzhang Wang.
    2020. CodeGRU: Context-aware deep learning with gated recurrent unit for source
    code modeling. *Information & Software Technology* 125 (2020), 106309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2016. Summarizing source code using a neural attention model. In
    *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*. 2073–2083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyer et al. (2018) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2018. Mapping Language to Code in Programmatic Context. In *Empirical
    Methods in Natural Language Processing*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2021b) Xiujuan Ji, Lei Liu, and Jingwen Zhu. 2021b. Code Clone Detection
    with Hierarchical Attentive Graph Embedding. *International Journal of Software
    Engineering and Knowledge Engineering* 31, 06 (2021), 837–861.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2021a) Yuede Ji, Lei Cui, and H. Howie Huang. 2021a. BugGraph: Differentiating
    Source-Binary Code Similarity with Graph Triplet-Loss Network. *ASIA CCS 2021
    - Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security*
    1, c (2021), 702–715. [https://doi.org/10.1145/3433210.3437533](https://doi.org/10.1145/3433210.3437533)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu.
    2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (may
    2021). arXiv:2105.12485 [https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485](https://arxiv.org/abs/2105.12485v2http://arxiv.org/abs/2105.12485)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalchbrenner et al. (2014) Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.
    2014. A Convolutional Neural Network for Modelling Sentences. In *ACL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanade et al. (2019) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2019. Learning and Evaluating Contextual Embedding of Source Code.
    (dec 2019). arXiv:2001.00059 [http://arxiv.org/abs/2001.00059](http://arxiv.org/abs/2001.00059)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code.
    In *International Conference on Machine Learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karaivanov et al. (2014) Svetoslav Karaivanov, Veselin Raychev, and Martin Vechev.
    2014. Phrase-Based Statistical Translation of Programming Languages. In *SIGPLAN
    symposium on New ideas, new paradigms, and reflections on programming and software*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karampatsis et al. (2020) Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
    Charles Sutton, and Andrea Janes. 2020. Big Code != Big Vocabulary: Open-Vocabulary
    Models for Source Code. *Proceedings - International Conference on Software Engineering*
    (mar 2020), 1073–1085. [https://doi.org/10.1145/3377811.3380342](https://doi.org/10.1145/3377811.3380342)
    arXiv:2003.07914v1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2020. Code Prediction by Feeding Trees to Transformers. *arXiv: Software Engineering*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2021) Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
    2021. Code prediction by feeding trees to transformers. *Proceedings - International
    Conference on Software Engineering* 1 (2021), 150–162. [https://doi.org/10.1109/ICSE43902.2021.00026](https://doi.org/10.1109/ICSE43902.2021.00026)
    arXiv:2003.13848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. arXiv:cs.LG/1609.02907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lattner and Adve (2004) Chris Lattner and Vikram Adve. 2004. LLVM: A compilation
    framework for lifelong program analysis & transformation. In *International Symposium
    on Code Generation and Optimization, 2004\. CGO 2004.* IEEE, 75–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2018) Quan Le, Oisín Boydell, Brian Mac Namee, and Mark Scanlon.
    2018. Deep learning at the shallow end: Malware classification for non-domain
    experts. *Digital Investigation* 26 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2020) Triet H.M. Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep
    Learning for Source Code Modeling and Generation: Models, Applications, and Challenges.
    *Comput. Surveys* 53, 3 (2020), 1–37. [https://doi.org/10.1145/3383458](https://doi.org/10.1145/3383458)
    arXiv:2002.05442'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2020) Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin
    McMillan. 2020. Improved code summarization via a graph neural network. In *IEEE
    International Conference on Program Comprehension*, Vol. 12\. IEEE Computer Society,
    184–195. [https://doi.org/10.1145/3387904.3389268](https://doi.org/10.1145/3387904.3389268)
    arXiv:2004.02843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair et al. (2019) Alexander LeClair, Siyuan Jiang, and Collin McMillan.
    2019. A neural model for generating natural language summaries of program subroutines.
    In *International Conference on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeClair and McMillan (2019) Alexander LeClair and Collin McMillan. 2019. Recommendations
    for Datasets for Source Code Summarization. In *North American Chapter of the
    Association for Computational Linguistics*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020) Bingzhuo Li, Chunyang Ye, Shouyang Guan, and Hui Zhou. 2020.
    Semantic Code Clone Detection Via Event Embedding Tree and GAT Network. *Proceedings
    - 2020 IEEE 20th International Conference on Software Quality, Reliability, and
    Security, QRS 2020* 3 (2020), 382–393. [https://doi.org/10.1109/QRS51102.2020.00057](https://doi.org/10.1109/QRS51102.2020.00057)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Hongyu Li, Seohyun Kim, and Satish Chandra. 2019a. Neural
    Code Search Evaluation Dataset. *arXiv: Software Engineering* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017b) Jian Li, Yue Wang, Irwin King, and Michael R. Lyu. 2017b.
    Code Completion with Neural Attention and Pointer Networks. *arXiv: Computation
    and Language* (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017c) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017c.
    Code Completion with Neural Attention and Pointer Networks. *IJCAI International
    Joint Conference on Artificial Intelligence* 2018-July (nov 2017), 4159–4165.
    [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017d) Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. 2017d.
    Code Completion with Neural Attention and Pointer Networks. *IJCAI International
    Joint Conference on Artificial Intelligence* 2018-July (nov 2017), 4159–4165.
    [https://doi.org/10.24963/ijcai.2018/578](https://doi.org/10.24963/ijcai.2018/578)
    arXiv:1711.09573v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017a) Liuqing Li, He Feng, Wenjie Zhuang, Na Meng, and Barbara
    Ryder. 2017a. Cclearner: A deep learning-based clone detection approach. In *2017
    IEEE International Conference on Software Maintenance and Evolution (ICSME)*.
    IEEE, 249–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang,
    Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022. CodeRetriever:
    Unimodal and Bimodal Contrastive Learning. *arXiv preprint arXiv:2201.10866* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019b) Yi Li, Shaohua Wang, Tien N. Nguyen, and Son Van Nguyen. 2019b.
    Improving bug detection via context-based code representation learning and attention-based
    neural networks. *Proceedings of the ACM on Programming Languages* 3, OOPSLA (oct
    2019), 30. [https://doi.org/10.1145/3360588](https://doi.org/10.1145/3360588)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016) Yujia Li, Richard Zemel, Marc Brockschmidt, and Daniel Tarlow.
    2016. Gated graph sequence neural networks. *4th International Conference on Learning
    Representations, ICLR 2016 - Conference Track Proceedings* 1 (2016), 1–20. arXiv:1511.05493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan
    Wang, Zhijun Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-Based System
    for Vulnerability Detection. (jan 2018). [https://doi.org/10.14722/ndss.2018.23158](https://doi.org/10.14722/ndss.2018.23158)
    arXiv:1801.01681v1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. ([n.d.]) Hongliang Liang, Yue Yu, Lin Jiang, and Zhuosi Xie. [n.d.].
    Seml: A semantic LSTM model for software defect prediction. *IEEE Access* 7 ([n. d.]),
    83812–83824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen,
    Hui Li, and Rongxin Wu. 2021. Improving Code Summarization with Block-wise Abstract
    Syntax Tree Splitting. In *IEEE International Conference on Program Comprehension*,
    Vol. 2021-May. IEEE Computer Society, 184–195. [https://doi.org/10.1109/ICPC52881.2021.00026](https://doi.org/10.1109/ICPC52881.2021.00026)
    arXiv:2103.07845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Meeting of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2018) Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D.
    Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for Natural Language Interface
    to the Linux Operating System.. In *Language Resources and Evaluation*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2016) Chang Liu, Xinyun Chen, Eui Chul Richard Shin, Mingcheng Chen,
    and Dawn Song. 2016. Latent Attention For If-Then Program Synthesis. In *NIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) Chang Liu, Xin Wang, Richard Shin, Joseph E. Gonzalez, and
    Dawn Song. 2017. Neural Code Completion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020b. A self-attentional neural architecture for code completion with multi-task
    learning. *IEEE International Conference on Program Comprehension* (2020), 37–47.
    [https://doi.org/10.1145/3387904.3389261](https://doi.org/10.1145/3387904.3389261)
    arXiv:1909.06983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
    2020c. A Self-Attentional Neural Architecture for Code Completion with Multi-Task
    Learning. In *International Conference on Program Comprehension*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020d) Fang Liu, Lu Zhang, and Zhi Jin. 2020d. Modeling programs
    hierarchically with stack-augmented LSTM. *Journal of Systems and Software* 164
    (2020), 110547.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang
    Liu. 2020a. Retrieval-Augmented Generation for Code Summarization via Hybrid GNN.
    (jun 2020). arXiv:2006.05405 [https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405](https://arxiv.org/abs/2006.05405v5http://arxiv.org/abs/2006.05405)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021c) Xuye Liu, Dakuo Wang, April Wang, Yufang Hou, and Lingfei
    Wu. 2021c. HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural
    Network for Code Documentation Generation in Jupyter Notebooks. (mar 2021). [https://doi.org/10.18653/v1/2021.findings-emnlp.381](https://doi.org/10.18653/v1/2021.findings-emnlp.381)
    arXiv:2104.01002'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021a) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021a. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *IEEE Transactions on Knowledge and
    Data Engineering* 01 (jul 2021), 1–1. [https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin
    Qiu, and Xun Wang. 2021b. Combining Graph Neural Networks with Expert Knowledge
    for Smart Contract Vulnerability Detection. *IEEE Transactions on Knowledge and
    Data Engineering* (2021). [https://doi.org/10.1109/TKDE.2021.3095196](https://doi.org/10.1109/TKDE.2021.3095196)
    arXiv:2107.11598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
    Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
    Duan, Neel Sundaresan, Shao Kun Deng, Fu Shengyu, and Shujie Liu. 2021. CodeXGLUE:
    A Machine Learning Benchmark Dataset for Code Understanding and Generation. *arXiv:
    Software Engineering* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahmud et al. (2021) Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios
    Anastasopoulos, and Kevin Moran. 2021. Code to Comment Translation: A Comparative
    Study on Model Effectiveness & Errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. *Advances in neural information processing systems* 26 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mou et al. (2014) Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN:
    A tree-based convolutional neural network for programming language processing.
    *arXiv preprint arXiv:1409.5718* (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mou et al. (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional
    neural networks over tree structures for programming language processing. In *Thirtieth
    AAAI Conference on Artificial Intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2014) Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen.
    2014. Migrating code with statistical machine translation. In *International Conference
    on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2018) Minh Hai Nguyen, Dung Le Nguyen, Xuan Mao Nguyen, and Tho Thanh
    Quan. 2018. Auto-detection of sophisticated malware using lazy-binding control
    flow graph and deep learning. *Computers & Security* 76 (jul 2018), 128–155. [https://doi.org/10.1016/J.COSE.2018.02.006](https://doi.org/10.1016/J.COSE.2018.02.006)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2017) Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and
    Tien N. Nguyen. 2017. Exploring API embedding for API usages and applications.
    In *International Conference on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2013) Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and
    Tien N. Nguyen. 2013. A statistical semantic language model for source code. In
    *FSE*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oda et al. (2015) Yusuke Oda, Hiroyuki Fudaba, Graham Neubig, Hideaki Hata,
    Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2015. Learning to Generate
    Pseudo-Code from Source Code Using Statistical Machine Translation (T). In *Automated
    Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In
    *Meeting of the Association for Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2021) Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and
    Tie-Yan Liu. 2021. How could Neural Networks understand Programs? (2021). arXiv:2105.04297
    [http://arxiv.org/abs/2105.04297](http://arxiv.org/abs/2105.04297)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prosser (1959) Reese T. Prosser. 1959. Applications of Boolean Matrices to the
    Analysis of Flow Diagrams. In *Papers Presented at the December 1-3, 1959, Eastern
    Joint IRE-AIEE-ACM Computer Conference* (Boston, Massachusetts) *(IRE-AIEE-ACM
    ’59 (Eastern))*. Association for Computing Machinery, New York, NY, USA, 133–138.
    [https://doi.org/10.1145/1460299.1460314](https://doi.org/10.1145/1460299.1460314)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puri et al. (2021) Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo
    Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey
    Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler,
    Susan Malaika, and Frederick Reiss. 2021. CodeNet: A Large-Scale AI for Code Dataset
    for Learning a Diversity of Coding Tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabinovich et al. (2017) Maxim Rabinovich, Mitchell Stern, and Dan Klein. 2017.
    Abstract Syntax Networks for Code Generation and Semantic Parsing. In *Meeting
    of the Association for Computational Linguistics*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *arXiv:
    Learning* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raychev et al. (2016) Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016.
    Probabilistic model for code with decision trees. *ACM SIGPLAN Notices* 51, 10
    (2016), 731–747. [https://doi.org/10.1145/2983990.2984041](https://doi.org/10.1145/2983990.2984041)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RaychevVeselin et al. (2014) RaychevVeselin, VechevMartin, and YahavEran. 2014.
    Code completion with statistical language models. *Sigplan Notices* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2020) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu
    Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. CodeBLEU:
    a Method for Automatic Evaluation of Code Synthesis. *arXiv: Software Engineering*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rush et al. (2015) Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015.
    A Neural Attention Model for Abstractive Sentence Summarization. In *EMNLP*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sachdev et al. (2018) Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim,
    Koushik Sen, and Satish Chandra. 2018. Retrieval on source code: a neural code
    search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention
    with relative position representations. *NAACL HLT 2018 - 2018 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies - Proceedings of the Conference* 2 (2018), 464–468. [https://doi.org/10.18653/v1/n18-2074](https://doi.org/10.18653/v1/n18-2074)
    arXiv:1803.02155'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2021) Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han,
    Dongmei Zhang, and Hongbin Sun. 2021. CAST: Enhancing Code Summarization with
    Hierarchical Splitting and Reconstruction of Abstract Syntax Trees. (aug 2021).
    arXiv:2108.12987 [http://arxiv.org/abs/2108.12987](http://arxiv.org/abs/2108.12987)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Ke Shi, Yang Lu, Jingfei Chang, and Zhen Wei. 2020. PathPair2Vec:
    An AST path pair-based code representation method for defect prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shido et al. (2019) Yusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto, Atsushi
    Miyamoto, and Tadayuki Matsumura. 2019. Automatic Source Code Summarization with
    Extended Tree-LSTM. In *Proceedings of the International Joint Conference on Neural
    Networks*, Vol. 2019-July. Institute of Electrical and Electronics Engineers Inc.
    [https://doi.org/10.1109/IJCNN.2019.8851751](https://doi.org/10.1109/IJCNN.2019.8851751)
    arXiv:1906.08094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shrivastava (2021) Piyush Shrivastava. 2021. Neural Code Summarization. *arXiv:
    Software Engineering* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuai et al. (2020) Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and
    Yan Lei. 2020. Improving Code Search with Co-Attentive Representation Learning.
    In *International Conference on Program Comprehension*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sui et al. (2020) Yulei Sui, Xiao Cheng, Guanqin Zhang, and Haoyu Wang. 2020.
    Flow2Vec: value-flow-based precise code embedding. *Proceedings of the ACM on
    Programming Languages* 4, OOPSLA (nov 2020), 27. [https://doi.org/10.1145/3428301](https://doi.org/10.1145/3428301)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
    Sequence to sequence learning with neural networks. In *Advances in neural information
    processing systems*. 3104–3112.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Svyatkovskiy et al. (2020) Alexey Svyatkovskiy, Shao Kun Deng, Fu Shengyu,
    and Neel Sundaresan. 2020. IntelliCode compose: code generation using transformer.
    In *FSE*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Svyatkovskiy et al. (2019) Alexey Svyatkovskiy, Ying Zhao, Fu Shengyu, and
    Neel Sundaresan. 2019. Pythia: AI-assisted Code Completion System. *arXiv: Software
    Engineering* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D. Manning.
    2015. Improved semantic representations from tree-structured long short-Term memory
    networks. In *ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    of the Asian Federation of Natural Language Processing, Proceedings of the Conference*,
    Vol. 1\. Association for Computational Linguistics (ACL), 1556–1566. [https://doi.org/10.3115/v1/p15-1150](https://doi.org/10.3115/v1/p15-1150)
    arXiv:1503.00075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarlow et al. (2020a) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020a. Learning
    to Fix Build Errors with Graph2Diff Neural Networks. In *Proceedings - 2020 IEEE/ACM
    42nd International Conference on Software Engineering Workshops, ICSEW 2020*.
    Association for Computing Machinery, Inc, 19–20. [https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarlow et al. (2020b) Daniel Tarlow, Subhodeep Moitra, Andrew Rice, Zimin Chen,
    Pierre Antoine Manzagol, Charles Sutton, and Edward Aftandilian. 2020b. Learning
    to Fix Build Errors with Graph2Diff Neural Networks. In *Proceedings - 2020 IEEE/ACM
    42nd International Conference on Software Engineering Workshops, ICSEW 2020*.
    19–20. [https://doi.org/10.1145/3387940.3392181](https://doi.org/10.1145/3387940.3392181)
    arXiv:1911.01205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. (2014) Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. 2014. On the
    localness of software. In *FSE*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ugner et al. ([n.d.]) Daniel Z ¨ Ugner, Tobias Kirschstein, Michele Catasta,
    Jure Leskovec, and Stephan G ¨ Unnemann. [n.d.]. Language-agnostic Representation
    Learning of Source Code from Structure and Context. ([n. d.]). arXiv:2103.11318v1
    [www.daml.in.tum.de/code-transformer,](www.daml.in.tum.de/code-transformer,)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vasic et al. (2019) Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber,
    and Rishabh Singh. 2019. Neural Program Repair by Jointly Learning to Localize
    and Repair. *arXiv: Learning* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Advances in neural information processing systems*. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2018) Petar Veličković, Arantxa Casanova, Pietro Liò, Guillem
    Cucurull, Adriana Romero, and Yoshua Bengio. 2018. Graph attention networks. In
    *6th International Conference on Learning Representations, ICLR 2018 - Conference
    Track Proceedings*. International Conference on Learning Representations, ICLR.
    arXiv:1710.10903 [https://arxiv.org/abs/1710.10903v3](https://arxiv.org/abs/1710.10903v3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2019a) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip Yu. 2019a. Multi-modal attention network learning for semantic
    source code retrieval. In *Proceedings - 2019 34th IEEE/ACM International Conference
    on Automated Software Engineering, ASE 2019*. Institute of Electrical and Electronics
    Engineers Inc., 13–25. [https://doi.org/10.1109/ASE.2019.00012](https://doi.org/10.1109/ASE.2019.00012)
    arXiv:1909.13516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2019b) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip S Yu. 2019b. Multi-modal attention network learning for semantic
    source code retrieval. *arXiv preprint arXiv:1909.13516* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via
    deep reinforcement learning. In *ASE 2018 - Proceedings of the 33rd ACM/IEEE International
    Conference on Automated Software Engineering*. Association for Computing Machinery,
    Inc, 397–407. [https://doi.org/10.1145/3238147.3238206](https://doi.org/10.1145/3238147.3238206)
    arXiv:1811.07234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, and X.
    Liao. 2021b. Bridging Pre-trained Models and Downstream Tasks for Source Code
    Understanding. (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang (2019) Ke Wang. 2019. Learning Scalable and Precise Representation of Program
    Semantics. (may 2019). arXiv:1905.05251 [https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251](https://arxiv.org/abs/1905.05251v3http://arxiv.org/abs/1905.05251)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020a) Song Wang, Taiyue Liu, Jaechang Nam, and Lin Tan. 2020a.
    Deep Semantic Feature Learning for Software Defect Prediction. *IEEE Transactions
    on Software Engineering* 46 (2020), 1267–1293.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning
    semantic features for defect prediction. In *International Conference on Software
    Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021c) Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. 2021c. Towards
    Full-line Code Completion with Neural Language Models. (2021). arXiv:2009.08603v1
    [www.aaai.org](www.aaai.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020c) Wenhan Wang, Kechi Zhang, Ge Li, and Zhi Jin. 2020c. Learning
    to Represent Programs with Heterogeneous Graphs. (2020), 1–10. arXiv:2012.04188
    [http://arxiv.org/abs/2012.04188](http://arxiv.org/abs/2012.04188)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Yanlin Wang, Shi Han, Dongmei Zhang, Ensheng Shi, Lun Du,
    Xiaodi Yang, Yuxuan Hu, Shi Han, and Hongyu Zhang. 2021a. CoCoSum: Contextual
    Code Summarization with Multi-Relational Graph Neural Network. *J. ACM* 1, 1 (jul
    2021), 24. [https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)
    arXiv:2107.01933'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Li (2021) Yanlin Wang and Hui Li. 2021. Code Completion by Modeling
    Flattened Abstract Syntax Trees as Graphs. (2021). arXiv:2103.09499 [http://arxiv.org/abs/2103.09499](http://arxiv.org/abs/2103.09499)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Yu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang. 2020b.
    Learning semantic program embeddings with graph interval neural network. *Proceedings
    of the ACM on Programming Languages* 4, OOPSLA (may 2020). [https://doi.org/10.1145/3428205](https://doi.org/10.1145/3428205)
    arXiv:2005.09997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021d) Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi.
    2021d. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for
    Code Understanding and Generation. *arXiv: Computation and Language* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2019) Bolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin. 2019.
    Retrieve and refine: exemplar-based neural comment generation. In *Automated Software
    Engineering*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei and Li (2017) Hui Hui Wei and Ming Li. 2017. Supervised deep features for
    Software functional clone detection by exploiting lexical and syntactical information
    in source code. In *IJCAI International Joint Conference on Artificial Intelligence*.
    3034–3040. [https://doi.org/10.24963/ijcai.2017/423](https://doi.org/10.24963/ijcai.2017/423)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2016) Martin White, Michele Tufano, Christopher Vendome, and Denys
    Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In *2016
    31st IEEE/ACM International Conference on Automated Software Engineering (ASE)*.
    IEEE, 87–98.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2015) Martin White, Christopher Vendome, Mario Linares-Vasquez,
    and Denys Poshyvanyk. 2015. Toward deep learning software repositories. In *Mining
    Software Repositories*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020) Liwei Wu, Fei Li, Youhua Wu, and Tao Zheng. 2020. GGF: A graph-based
    method for programming language syntax error correction. *IEEE International Conference
    on Program Comprehension* (2020), 139–148. [https://doi.org/10.1145/3387904.3389252](https://doi.org/10.1145/3387904.3389252)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi
    Zhang, and Philip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks.
    *IEEE Transactions on Neural Networks and Learning Systems* 32, 1 (jan 2019),
    4–24. [https://doi.org/10.1109/TNNLS.2020.2978386](https://doi.org/10.1109/TNNLS.2020.2978386)
    arXiv:1901.00596v4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2017) Yan Xiao, Jacky Keung, Qing Mi, and Kwabena Ebo Bennin. 2017.
    Improving Bug Localization with an Enhanced Convolutional Neural Network. In *Asia-Pacific
    Software Engineering Conference*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2021) Binbin Xie, Jinsong Su, Yubin Ge, Xiang Li, Jianwei Cui, Junfeng
    Yao, and Bin Wang. 2021. Improving Tree-Structured Decoder Training for Code Generation
    via Mutual Learning. (2021). arXiv:2105.14796v1 [www.aaai.org](www.aaai.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan,
    Yan Lei, and Zhou Xu. 2021. Two-Stage Attention-Based Model for Code Search with
    Textual and Structural Features. In *IEEE International Conference on Software
    Analysis, Evolution, and Reengineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Sihan Xu, Sen Zhang, Weijing Wang, Xinya Cao, Chenkai Guo,
    and Jing Xu. 2019. Method name suggestion with hierarchical attention networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2017) Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn
    Song. 2017. Neural network-based graph embedding for cross-platform binary code
    similarity detection. In *Proceedings of the ACM Conference on Computer and Communications
    Security*. Association for Computing Machinery, 363–376. [https://doi.org/10.1145/3133956.3134018](https://doi.org/10.1145/3133956.3134018)
    arXiv:1708.06525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2021) Jiang Xue, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu.
    2021. TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. In *Uncertainty
    in Artificial Intelligence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamaguchi et al. (2014) Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad
    Rieck. 2014. Modeling and discovering vulnerabilities with code property graphs.
    In *Proceedings - IEEE Symposium on Security and Privacy*. 590–604. [https://doi.org/10.1109/SP.2014.44](https://doi.org/10.1109/SP.2014.44)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2019) Jiaqi Yan, Guanhua Yan, and Dong Jin. 2019. Classifying Malware
    Represented as Control Flow Graphs using Deep Graph Convolutional Neural Network.
    In *Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable
    Systems and Networks, DSN 2019*. Institute of Electrical and Electronics Engineers
    Inc., 52–63. [https://doi.org/10.1109/DSN.2019.00020](https://doi.org/10.1109/DSN.2019.00020)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang and Kuang ([n.d.]) Hao Yang and Li Kuang. [n.d.]. CCMC: Code Completion
    with a Memory Mechanism and a Copy Mechanism; CCMC: Code Completion with a Memory
    Mechanism and a Copy Mechanism. ([n. d.]). [https://doi.org/10.1145/3463274.3463332](https://doi.org/10.1145/3463274.3463332)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020) Kang Yang, Huiqun Yu, Guisheng Fan, and Xingguang Yang. 2020.
    Graph embedding code prediction model integrating semantic features. *Computer
    Science and Information Systems* 17, 3 (2020), 907–926. [https://doi.org/10.2298/CSIS190908027Y](https://doi.org/10.2298/CSIS190908027Y)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Ke Yang, MingXing Zhang, Kang Chen, Xiaosong Ma, Yang Bai,
    and Yong Jiang. 2019. KnightKing: a fast distributed graph random walk engine.
    In *Proceedings of the 27th ACM Symposium on Operating Systems Principles*. 524–537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasunaga and Liang (2020a) Michihiro Yasunaga and Percy Liang. 2020a. Graph-based,
    Self-Supervised Program Repair from Diagnostic Feedback. (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yasunaga and Liang (2020b) Michihiro Yasunaga and Percy Liang. 2020b. Graph-based,
    Self-Supervised Program Repair from Diagnostic Feedback. In *International Conference
    on Machine Learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2018a) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu,
    and Graham Neubig. 2018a. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Proceedings - International Conference on Software Engineering*.
    IEEE Computer Society, 476–486. [https://doi.org/10.1145/3196398.3196408](https://doi.org/10.1145/3196398.3196408)
    arXiv:1805.08949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2018b) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu,
    and Graham Neubig. 2018b. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Mining Software Repositories*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yin and Neubig (2018) Pengcheng Yin and Graham Neubig. 2018. TRANX: A Transition-based
    Neural Abstract Syntax Parser for Semantic Parsing and Code Generation. In *Empirical
    Methods in Natural Language Processing*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020) Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and
    Shi Wu. 2020. Order matters: Semantic-aware neural networks for binary code similarity
    detection. In *AAAI 2020 - 34th AAAI Conference on Artificial Intelligence*, Vol. 34.
    AAAI press, 1145–1152. [https://doi.org/10.1609/aaai.v34i01.5466](https://doi.org/10.1609/aaai.v34i01.5466)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020a) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020a. Retrieval-based neural source code summarization. *Proceedings - International
    Conference on Software Engineering* (2020), 1385–1397. [https://doi.org/10.1145/3377811.3380383](https://doi.org/10.1145/3377811.3380383)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong
    Liu. 2020b. Retrieval-based neural source code summarization. In *International
    Conference on Software Engineering*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019a) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019a. A Novel Neural Source Code Representation Based on
    Abstract Syntax Tree. *Proceedings - International Conference on Software Engineering*
    2019-May (2019), 783–794. [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan
    Wang, and Xudong Liu. 2019b. A Novel Neural Source Code Representation Based on
    Abstract Syntax Tree. In *2019 IEEE/ACM 41st International Conference on Software
    Engineering (ICSE)*. 783–794. [https://doi.org/10.1109/ICSE.2019.00086](https://doi.org/10.1109/ICSE.2019.00086)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.
    2018. An end-to-end deep learning architecture for graph classification. In *Thirty-second
    AAAI conference on artificial intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao and Huang (2018) Gang Zhao and Jeff Huang. 2018. DeepSim: Deep Learning
    Code Functional Similarity. (2018). [https://doi.org/10.1145/3236024.3236068](https://doi.org/10.1145/3236024.3236068)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2019) Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and
    Yang Liu. 2019. Devign: Effective Vulnerability Identification by Learning Comprehensive
    Program Semantics via Graph Neural Networks. *Advances in Neural Information Processing
    Systems* 32 (sep 2019). arXiv:1909.03496 [https://arxiv.org/abs/1909.03496v1](https://arxiv.org/abs/1909.03496v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2016) Xiaowei Zhu, Wenguang Chen, Weimin Zheng, and Xiaosong Ma.
    2016. Gemini: A computation-centric distributed graph processing system. In *12th
    $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$
    16)*. 301–316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zuo et al. (2019) Fei Zuo, Xiaopeng Li, Patrick Young, Lannan Luo, Qiang Zeng,
    and Zhexin Zhang. 2019. Neural Machine Translation Inspired Binary Code Similarity
    Comparison beyond Function Pairs. Internet Society. [https://doi.org/10.14722/ndss.2019.23492](https://doi.org/10.14722/ndss.2019.23492)
    arXiv:1808.04706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
