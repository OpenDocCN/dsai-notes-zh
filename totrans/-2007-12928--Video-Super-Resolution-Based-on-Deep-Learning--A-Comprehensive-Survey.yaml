- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:00:10'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2007.12928] Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.12928](https://ar5iv.labs.arxiv.org/html/2007.12928)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹institutetext: Fanhua Shang (Corresponding author): ¹¹email: fhshang@xidian.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Hongying Liu and Fanhua Shang are with the Key Laboratory of Intelligent Perception
    and Image Understanding of Ministry of Education, School of Artificial Intelligence,
    Xidian University, China, and Peng Cheng Laboratory, Shenzhen, China.
  prefs: []
  type: TYPE_NORMAL
- en: Zhubo Ruan, Peng Zhao, Yuanyuan Liu and Linlin Yang are with the Key Laboratory
    of Intelligent Perception and Image Understanding of Ministry of Education, School
    of Artificial Intelligence, Xidian University, China.
  prefs: []
  type: TYPE_NORMAL
- en: Chao Dong is with the Shenzhen Institutes of Advanced Technology, Chinese Academy
    of Sciences.
  prefs: []
  type: TYPE_NORMAL
- en: Radu Timofte is with ETH Zurich, Switzerland and University of Wurzburg, Germany.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video Super-Resolution Based on Deep Learning: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hongying Liu    Zhubo Ruan    Peng Zhao    Chao Dong    Fanhua Shang    Yuanyuan Liu
       Linlin Yang    Radu Timofte
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Video super-resolution (VSR) is reconstructing high-resolution videos from low
    resolution ones. Recently, the VSR methods based on deep neural networks have
    made great progress. However, there is rarely systematical review on these methods.
    In this survey, we comprehensively investigate 37 state-of-the-art VSR methods
    based on deep learning. It is well known that the leverage of information contained
    in video frames is important for video super-resolution. Thus we propose a taxonomy
    and classify the methods into seven sub-categories according to the ways of utilizing
    inter-frame information. Moreover, descriptions on the architecture design and
    implementation details are also included. Finally, we summarize and compare the
    performance of the representative VSR methods on some benchmark datasets. We also
    discuss the applications, and some challenges, which need to be further addressed
    by researchers in the community of VSR. To the best of our knowledge, this work
    is the first systematic review on VSR tasks, and it is expected to make a contribution
    to the development of recent studies in this area and potentially deepen our understanding
    of the VSR techniques based on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Video super-resolution Deep learning Convolutional neural networks Inter-frame
    information
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Super-resolution (SR) aims at recovering a high-resolution (HR) image or multiple
    images from the corresponding low-resolution (LR) counterparts. It is a classic
    and challenging problem in computer vision and image processing, and it has extensive
    real-world applications, such as medical imagery reconstruction (Peng et al.,
    [2020](#bib.bib96)), remote sensing (Luo et al., [2017](#bib.bib89)), and panorama
    video super-resolution (Fakour-Sevom et al., [2018](#bib.bib23); Liu et al., [2020b](#bib.bib82)),
    surveillance systems (Deshmukh and Rani, [2019](#bib.bib18)), and high-definition
    television (Patti et al., [1997](#bib.bib95)). With the advent of the 5th generation
    mobile communication technology, large-sized images or videos can be transmitted
    within a shorter time. Meanwhile, with the popularity of high-definition (HD)
    and ultra-high-definition (UHD) display devices, video super-resolution is attracting
    more attention.
  prefs: []
  type: TYPE_NORMAL
- en: Video is one of the most common multimedia in our daily life, and thus super-resolution
    of low-resolution videos has become very important. In general, image super-resolution
    methods process a single image at a time, while video super-resolution algorithms
    deal with multiple successive images/frames at a time so as to utilize relationship
    within frames to super-resolve the target frame. In a broad sense, video super-resolution
    (VSR) can be regarded as an extension of image super-resolution and can be processed
    by image super-resolution algorithms frame by frame. However, the SR performance
    is not always satisfactory as artifacts and jams may be brought in, which causes
    unwanted temporal incoherence within frames.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d60f9e1e260b541a3a6034d07ce52c9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The general pipeline of deep learning methods for VSR tasks. Note
    that the inter-frame alignment module can be either traditional methods or deep
    CNNs, while both the feature extraction & fusion module and the upsampling module
    usually utilize deep CNNs. The dashed line box means that the module is optional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, many video super-resolution algorithms have been proposed.
    They mainly fall into two categories: traditional methods and deep learning based
    methods. For some traditional methods, the motions are simply estimated by affine
    models as in (Schultz and Stevenson, [1996](#bib.bib103)). In (Protter et al.,
    [2009](#bib.bib98); Takeda et al., [2009](#bib.bib112)), they adopt non-local
    mean and 3D steering kernel regression for video super-resolution, respectively.
    Liu and Sun ([2014](#bib.bib79)) proposed a Bayesian approach to simultaneously
    estimate underlying motion, blur kernel, and noise level for reconstructing high-resolution
    frames. In (Ma et al., [2015](#bib.bib90)), the expectation maximization (EM)
    method is adopted to estimate the blur kernel, and guide the reconstruction of
    high-resolution frames. However, these explicit models of high-resolution videos
    are still inadequate to fit various scenes in videos.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the great success of deep learning in a variety of areas (Zhang et al.,
    [2021](#bib.bib137)), super-resolution algorithms based on deep learning are studied
    extensively. Many video super-resolution methods based on deep neural networks
    such as convolutional neural network (CNN), generative adversarial network (GAN)
    and recurrent neural network (RNN) have been proposed. Generally, they employ
    a large number of both LR and HR video sequences to input the neural network for
    inter-frame alignment, feature extraction/fusion, and then to produce the high-resolution
    sequences for the corresponding low-resolution video sequences. The pipeline of
    most video super-resolution methods mainly includes one alignment module, one
    feature extraction and fusion module, and one reconstruction module, as shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Video Super-Resolution Based on
    Deep Learning: A Comprehensive Survey"). Because of the nonlinear learning capability
    of deep neural networks, the deep learning based methods usually achieve good
    performance on many public benchmark datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, there are few works about the overview on video super-resolution tasks,
    though many works  (Wang et al., [2021b](#bib.bib124); Singh and Singh, [2020](#bib.bib108);
    Yang et al., [2019](#bib.bib132)) on the investigation of single image super-resolution
    have been published. Daithankar and Ruikar ([2020](#bib.bib15)) presented a brief
    review on many frequency-spatial domain methods, while the deep learning methods
    are rarely mentioned. Unlike the previous work, we provide a comprehensive investigation
    on deep learning techniques for video super-resolution in recent years. It is
    well known that the main difference between video super-resolution and image super-resolution
    lies in the processing of inter-frame information. How to effectively leverage
    the information from neighboring frames is critical for VSR tasks. We focus on
    the ways of utilizing inter-frame information for various deep learning based
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: The contributions of this work are mainly summarized as follows. 1) We review
    recent works and progresses on developing techniques for deep learning based video
    super-resolution. To the best of our knowledge, this is the first comprehensive
    survey on deep learning based VSR methods. 2) We propose a taxonomy for deep learning
    based video super-resolution methods by categorizing their ways of utilizing inter-frame
    information and illustrate how the taxonomy can be used to categorize existing
    methods. 3) We summarize the performance of state-of-the-art methods on some public
    benchmark datasets, and list the applications of VSR algorithms in various areas.
    4) We further discuss some challenges and perspectives for video super-resolution
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the paper is organized as follows. In Section II, we briefly introduce
    the background of video super-resolution. Section III shows our taxonomy for recent
    works. In Sections IV and V, we describe the video super-resolution methods with
    and without alignment, respectively, according to the taxonomy. In Section VI,
    the performance of state-of-the-art methods is analyzed quantitatively. In Section
    VII, we discuss the challenges and prospective trends in video super-resolution.
    Finally, we conclude this work in Section VIII.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Video super-resolution stems from image super-resolution, and it aims at restoring
    high-resolution videos from multiple low-resolution frames. However, the difference
    between video and image super-resolution techniques is also obvious, that is,
    the former usually takes advantage of inter-frame information. Besides the RGB
    color space, the YUV including YCbCr color space is also widely used for VSR.
    $I_{i}\!\in\!\mathbb{R}^{H\times W\times 3}$ denotes the $i$-th frame in a LR
    video sequence $I$, and $\hat{I_{i}}\!\in\!\mathbb{R}^{sH\times sW\times 3}$ is
    the corresponding HR frame, where $s$ is the scale factor, e.g., $s\!=\!2$, 4
    or 8\. And $\{\hat{I}_{j}\}_{j=i-N}^{i+N}$ is a set of $2N\!+\!1$ HR frames for
    the center frame $\hat{I_{i}}$, where $N$ is the temporal radius. Then the degradation
    process of HR video sequences can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${I_{i}}=\phi(\hat{I}_{i},\{\hat{I}_{j}\}_{j=i-N}^{i+N};\theta_{\alpha})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\phi(\cdot;\cdot)$ is the degradation function, and the parameter $\theta_{\alpha}$
    represents various degradation factors such as noise, motion blur and downsampling
    factors. In most existing works (Liu and Sun, [2014](#bib.bib79); Ma et al., [2015](#bib.bib90);
    Farsiu et al., [2004](#bib.bib24); Pan et al., [2020](#bib.bib94)), the degradation
    process is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{j}=DBE_{i\rightarrow j}\hat{I}_{i}+n_{j}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $D$ and $B$ are the down-sampling and blur operations, $n_{j}$ denotes
    image noise, and $E_{i\rightarrow j}$ is the warping operation based on the motion
    from $\hat{I}_{i}$ to $\hat{I}_{j}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, it is easy to obtain LR image ${I_{j}}$, but the degradation factors,
    which may be quite complex or probably a combination of several factors, are unknown.
    Different from single image super-resolution (SISR) aiming at solving a single
    degraded image, VSR needs to deal with degraded video sequences, and recovers
    the corresponding HR video sequences, which should be as close to the ground truth
    (GT) videos as possible. Specifically, a VSR algorithm may use similar techniques
    to SISR for processing a single frame (spatial information), while it has to take
    relationships among frames (temporal information) into consideration to ensure
    motion consistency of the video. The super-resolution process, namely the reverse
    process of Eq. (1), can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\tilde{I_{i}}=\phi^{-1}({I_{i}},\{{I}_{j}\}_{j=i-N}^{i+N};\theta_{\beta})$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{I_{i}}$ denotes the estimation of the GT (i.e., $\hat{I}_{i}$),
    and $\theta_{\beta}$ is the model parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like SISR, video quality is mainly evaluated by calculating peak signal-noise
    ratio (PSNR) and structural similarity index (SSIM). These indexes measure the
    difference of pixels and similarity of structures between two images, respectively.
    PSNR of one SR frame is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textup{PSNR}=10\log_{10}\left(\frac{L^{2}}{\textup{MSE}}\right)$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $L$ represents the maximum range of color value, which is usually 255,
    and the mean squared error (MSE) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textup{MSE}=\frac{1}{N}\sum_{i=1}^{N}(\hat{I}_{i}-\tilde{I}_{i})^{2}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $N$ denotes the total number of pixels in an image or a frame, $\hat{I}$
    and $\tilde{I}$ are the ground truth HR frame and the SR recovered frame, respectively.
    A higher value of PSNR generally means superior quality. In addition, SSIM is
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textup{SSIM}(\hat{I},\tilde{I})=\frac{2u_{\hat{I}}u_{\tilde{I}}+k_{1}}{u_{\hat{I}}^{2}+u_{\tilde{I}}^{2}+k_{1}}\cdot\frac{2\sigma_{\hat{I}\tilde{I}}+k_{2}}{\sigma^{2}_{\hat{I}}+\sigma_{\tilde{I}}^{2}+k_{2}}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $u_{\hat{I}}$ and $u_{\tilde{I}}$ represent the mean values of the images
    $\hat{I}$ and $\tilde{I}$, respectively. $k_{1}$ and $k_{2}$ are constants, which
    are used to stabilize the calculation and are usually set to 0.01 and 0.03, respectively.
    $\sigma_{\hat{I}}$ and $\sigma_{\tilde{I}}$ denote the standard deviations, and
    $\sigma_{\hat{I}\tilde{I}}$ denotes the covariance.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Video Super-resolution Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the videos are a recording of moving visual images and sound, the methods
    for video super-resolution learn from existing single image super-resolution methods.
    There are many deep learning based image super-resolution methods such as Super-Resolution
    using deep Convolutional Neural Networks (SRCNN) (Dong et al., [2014](#bib.bib19)),
    Fast Super-Resolution Convolutional Neural Networks (FSRCNN) (Dong et al., [2016](#bib.bib20)),
    VDSR (Kim et al., [2016](#bib.bib60)), Efficient Sub-Pixel Convolutional neural
    Network (ESPCN) (Shi et al., [2016](#bib.bib105)), Residual Dense Network (RDN) (Zhang
    et al., [2018](#bib.bib140)), Residual Channel Attention Network (RCAN) (Zhang
    et al., [2018b](#bib.bib139)), “Zero-Shot” Super-Resolution (ZSSR) (Shocher et al.,
    [2018](#bib.bib107)) and Super-Resolution using a Generative Adversarial Network
    (SRGAN) (Ledig et al., [2017](#bib.bib67)). In 2016, based on SRCNN, Kappeler (Kappeler
    et al., [2016](#bib.bib59)) presented a video super-resolution method with convolutional
    neural networks (VSRnet). So far, many video super-resolution algorithms have
    been proposed. In the following, we summarize the characteristics of the deep
    learning based methods for video super-resolution in recent years, as shown in
    Table [1](#S3.T1 "Table 1 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd5ff663da6247785c972c041c10f012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A taxonomy for existing state-of-the-art video super-resolution methods.
    Here, $MEMC$ stands for motion estimation and compensation methods, $DC$ is deformable
    convolution methods, $3D\ Conv$ is 3D convolution methods, and $RCNN$ denotes
    recurrent convolutional neural network based methods. The links to these methods
    are as follows. *MEMC*: Deep-DE (Liao et al., [2015](#bib.bib76)),VSRnet (Kappeler
    et al., [2016](#bib.bib59)), VESPCN (Caballero et al., [2017](#bib.bib5)), DRVSR (Tao
    et al., [2017](#bib.bib113)), RVSR (Liu et al., [2017](#bib.bib80)), FRVSR (Sajjadi
    et al., [2018](#bib.bib102)), STTN (Kim et al., [2018a](#bib.bib62)), SOFVSR (Wang
    et al., [2019](#bib.bib119)), TOFlow (Xue et al., [2019](#bib.bib130)), MMCNN (Wang
    et al., [2019b](#bib.bib123)), MEMC-Net (Bao et al., [2021](#bib.bib1)), RRCN (Li
    et al., [2019](#bib.bib70)), RTVSR (Bare et al., [2019](#bib.bib2)), MultiBoot
    VSR (Kalarot and Porikli, [2019](#bib.bib58)), TecoGAN (Chu et al., [2020](#bib.bib12)),
    MuCAN (Li et al., [2020](#bib.bib73)), BasicVSR (Chan et al., [2021b](#bib.bib7)).
    *DC*: EDVR (Wang et al., [2019a](#bib.bib122)), DNLN (Wang et al., [2019](#bib.bib118)),
    TDAN (Tian et al., [2020](#bib.bib114)), D3Dnet (Ying et al., [2020](#bib.bib134)),
    VESR-Net (Chen et al., [2020](#bib.bib10)). *2D Conv*:VSRResFeatGAN (Lucas et al.,
    [2019](#bib.bib87)), FFCVSR (Yan et al., [2019](#bib.bib131)). *3D Conv*: DUF (Jo
    et al., [2018](#bib.bib57)), FSTRN (Li et al., [2019a](#bib.bib72)), 3DSRnet (Kim
    et al., [2019](#bib.bib61)), DSMC (Liu et al., [2021a](#bib.bib83)). *RCNN*: BRCN (Huang
    et al., [2015](#bib.bib41), [2018](#bib.bib42)), STCN (Guo and Chao, [2017](#bib.bib31)),
    RISTN (Zhu et al., [2019](#bib.bib142)), RLSP (Fuoli et al., [2019a](#bib.bib25)),
    RSDN (Isobe et al., [2020](#bib.bib50)). *Non-Local*: PFNL (Yi et al., [2019](#bib.bib133)).
    *Other*: RBPN (Haris et al., [2019](#bib.bib34)), STARnet (Haris et al., [2020](#bib.bib35)),
    DNSTNet (Sun et al., [2020](#bib.bib111)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several recent studies such as (Wang et al., [2019a](#bib.bib122); Jo et al.,
    [2018](#bib.bib57); Tian et al., [2020](#bib.bib114)) on video super-resolution
    tasks have indicated that the utilization of the information contained in frames
    greatly influences performance. The proper and adequate usage of such information
    can enhance the results of video super-resolution. Therefore, we build a taxonomy
    for existing video super-resolution methods according to their ways of the utilization
    of inter-frame information, as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution
    Methods ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Existing video super-resolution methods based on deep learning and
    their key strategies such as loss functions (see their source papers for the details
    of the loss functions). Here, $MEMC$ denotes motion estimation and motion compensation,
    $DC$ is deformable convolution, $3D\ Conv$ is 3D convolution, and $RCNN$ denotes
    recurrent convolutional neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Synonym | Type | Loss function | ​Align​ |'
  prefs: []
  type: TYPE_TB
- en: '| Deep-DE (Liao et al., [2015](#bib.bib76)) | ICCV 2015 | Deep Draft-Ensemble
    Learning | MEMC | $\ell_{1}$-norm loss with total variation regularization | $\checkmark$
    |'
  prefs: []
  type: TYPE_TB
- en: '| VSRnet (Kappeler et al., [2016](#bib.bib59)) | TCI 2016 | Video Super-Resolution
    with convolutional neural Networks | Mean Square Error (MSE) loss | $\checkmark$
    |'
  prefs: []
  type: TYPE_TB
- en: '| VESPCN (Caballero et al., [2017](#bib.bib5)) | CVPR 2017 | Video Efficient
    Sub-pixel Convolutional Network | MSE loss and Motion Compensation (MC) loss |
    $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| DRVSR (Tao et al., [2017](#bib.bib113)) | ICCV 2017 | Detail-Revealing deep
    Video Super-Resolution | MSE loss and MC loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| RVSR (Liu et al., [2017](#bib.bib80)) | ICCV 2017 | Robust Video Super-Resolution
    | Spatial alignment loss and spatio-temporal adaptive loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| FRVSR (Sajjadi et al., [2018](#bib.bib102)) | CVPR 2018 | Frame-Recurrent
    Video Super-Resolution | MSE loss and MC loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| STTN (Kim et al., [2018a](#bib.bib62)) | ECCV 2018 | Spatio-Temporal Transformer
    Network | MSE loss and MC loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| SOFVSR (Wang et al., [2019](#bib.bib119)) | ACCV 2018 | Super-resolution
    Optical Flow for Video Super-Resolution | MSE loss and MC loss | $\checkmark$
    |'
  prefs: []
  type: TYPE_TB
- en: '| TOFlow (Xue et al., [2019](#bib.bib130)) | IJCV 2019 | video enhancement
    with Task-Oriented Flow | $\ell_{1}$-norm loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| MMCNN (Wang et al., [2019b](#bib.bib123)) | TIP 2019 | Multi-Memory Convolutional
    Neural Network | MSE loss and MC loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| MEMC-Net (Bao et al., [2021](#bib.bib1)) | TPAMI 2019 | Motion Estimation
    and Motion Compensation Network | Charbonnier (Cb) loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| RRCN (Li et al., [2019](#bib.bib70)) | TIP 2019 | Residual Recurrent Convolutional
    Network | MSE loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| RTVSR (Bare et al., [2019](#bib.bib2)) | Neurocomp. 2019 | Real-Time Video
    Super-Resolution | MSE loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| ​​MultiBoot VSR (Kalarot and Porikli, [2019](#bib.bib58))​​ | CVPRW 2019
    | Multi-stage multi-reference Bootstrapping for Video Super-Resolution | Huber
    loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| MuCAN (Li et al., [2020](#bib.bib73)) | ECCV 2020 | Multi-Correspondence
    Aggregation Network for Video Super-Resolution | Edge-aware loss | $\checkmark$
    |'
  prefs: []
  type: TYPE_TB
- en: '| TecoGAN (Chu et al., [2020](#bib.bib12)) | ACMTOG 2020 | Temporally coherent
    GAN | MSE loss and ping-pong loss etc. | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| BasicVSR (Chan et al., [2021b](#bib.bib7)) | CVPR 2021 | search for essential
    components in Video Super-Resolution and beyond | Cb loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| EDVR (Wang et al., [2019a](#bib.bib122)) | CVPRW 2019 | Enhanced Deformable
    convolutional networks for Video Restoration | DC | Cb loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| DNLN (Wang et al., [2019](#bib.bib118)) | ACCESS 2019 | Deformable Non-Local
    Network for Video Super-Resolution | $\ell_{1}$-norm loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| TDAN (Tian et al., [2020](#bib.bib114)) | CVPR 2020 | Temporally-Deformable
    Alignment Network for Video Super-Resolution | $\ell_{1}$-norm loss | $\checkmark$
    |'
  prefs: []
  type: TYPE_TB
- en: '| D3Dnet (Ying et al., [2020](#bib.bib134)) | SPL 2020 | Deformable 3D Convolution
    for Video Super-Resolution | MSE loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| VESR-Net (Chen et al., [2020](#bib.bib10)) | ArXiv 2020 | Video Enhancement
    and Super-Resolution Network | $\ell_{1}$-norm loss | $\checkmark$ |'
  prefs: []
  type: TYPE_TB
- en: '| ​VSRResFeatGAN (Lucas et al., [2019](#bib.bib87))​ | TIP 2019 | Video Super-Resolution
    with Residual Networks | 2D Conv | Adversarial loss; content loss; and perceptual
    loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| FFCVSR (Yan et al., [2019](#bib.bib131)) | AAAI 2019 | Frame and Feature-Context
    Video Super-Resolution | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| DUF (Jo et al., [2018](#bib.bib57)) | CVPR 2018 | video super-resolution
    network using Dynamic Upsampling Filters | 3D Conv | Huber loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| FSTRN (Li et al., [2019a](#bib.bib72)) | CVPR 2019 | Fast Spatio-Temporal
    Residual Network for Video Super-Resolution | Cb loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| 3DSRnet (Kim et al., [2019](#bib.bib61)) | ICIP 2019 | 3D Super-Resolution
    Network | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| DSMC (Liu et al., [2021a](#bib.bib83)) | AAAI 2021 | Dual Subnet and Multi-stage
    Communicated upsampling | Cb loss; perceptual loss; the dual loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| ​​​BRCN (Huang et al., [2015](#bib.bib41), [2018](#bib.bib42))​​​ | ​NIPS
    2015/2018​ | video super-resolution via Bidirectional Recurrent Convolutional
    Networks | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| STCN (Guo and Chao, [2017](#bib.bib31)) | AAAI 2017 | Spatio-Temporal Convolutional
    Network for Video Super-Resolution | RCNN | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| RISTN (Zhu et al., [2019](#bib.bib142)) | AAAI 2019 | Residual Invertible
    Spatio-Temporal Network for Video Super-Resolution | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| RLSP (Fuoli et al., [2019a](#bib.bib25)) | ​ICCVW 2019​ | video super-resolution
    through Recurrent Latent Space Propagation | MSE loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| RSDN (Isobe et al., [2020](#bib.bib50)) | ​ECCV 2020​ | video super-resolution
    with Recurrent Structure-Detail Network | Cb loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| PFNL (Yi et al., [2019](#bib.bib133)) | ICCV 2019 | Progressive Fusion network
    via exploiting Non-Local spatio-temporal correlations | ​Non-Local​ | Cb loss
    | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| RBPN (Haris et al., [2019](#bib.bib34)) | CVPR 2019 | Recurrent Back-Projection
    Network |  | $\ell_{1}$-norm loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| STARnet (Haris et al., [2020](#bib.bib35)) | CVPR 2020 | Space-Time-Aware
    multi-Resolution network |  | Three losses | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| DNSTNet (Sun et al., [2020](#bib.bib111)) | Neurocomp. 2020 | video super-resolution
    via Dense Non-local Spatial-Temporal convolutional Network | ​Other ​ | $\ell_{1}$-norm
    loss | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution Methods ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") and Table
    [1](#S3.T1 "Table 1 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), we categorize the existing methods
    into two main categorises: methods with alignment and methods without alignment,
    according to whether the video frames are explicitly aligned. We will present
    the methods in detail in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Since all the methods are classified according to whether the frames are explicitly
    aligned and what techniques they are mainly used for alignment, other modules
    which they utilized for feature extraction, fusion, and reconstruction are ignored.
    These modules may be employed by multiple methods simultaneously. Therefore, some
    of the methods in our study are coupled. BasicVSR in the MEMC methods from the
    category of the methods with alignment adopts a typical bidirectional recurrent
    convolutional neural network (RCNN) as backbone. While the RCNN-based methods
    (e.g. BRCN, STCN, and RISTN), which are in the methods without alignment, mainly
    use RCNN to learn features. Similarly, VESR-Net in the DC category also uses a
    non-local block for feature learning as that of PFNL in the non-local category.
    Moreover, DSMC in the category of 3D convolution also utilizes a non-local block
    for global correlation computation. The category of ’other’ includes the methods
    which adopt optical flow but without frame alignment, e.g., RBPN, and STARnet.
    Finally, the learned offsets by deformable convolution share similar patterns
    as those from the optical flow-based methods, and the deformable and flow-based
    alignments are strongly correlated. This was indicated in the work (Chan et al.,
    [2021c](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we have observed several trends in these recently proposed methods.
  prefs: []
  type: TYPE_NORMAL
- en: 1) The diversification of methods. In the early years (2015-2017), most of the
    methods use frame alignment for VSR. Then since 2018, many different methods,
    especially which are the methods without alignment, have emerged, e.g., FFCVSR,
    DUF, RISTN, and PLNL. Some studies also indicate that both the methods with alignment
    and those without alignment can obtain sound performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2) The expansion of receptive field in methods. Earlier methods such as EDVR
    and RBPN mainly utilize certain numbers of input frames in sliding-window, while
    the subsequent methods resort to longer sequences. For example, BasicVSR employs
    bidirectional RCNN, by which the features are propagated forward and backward
    independently. Moreover, the non-local subnetwork, such as in the PFNL method,
    aims to compute the correlations between all possible pixels within and across
    frames. These indicate that the methods tend to capture longer-range dependencies
    in the video sequences, and they expand the receptive field in the network from
    local to global.
  prefs: []
  type: TYPE_NORMAL
- en: 3) In the MEMC methods such as FRVSR, STTN, SOFVSR, TecoGAN, and MuCAN, most
    of them adopt deep learning techniques for estimation the optical flow, since
    the deep learning may have adaptive ability for various data than the conventional
    methods. 4) The practicality of methods. As the requirements for super-resolving
    of higher quality videos develop, the recently proposed methods also become more
    practicable. The test videos evolve from Vid4 and UVGD, to REDS. All the discussions
    indicate that we will mainly focus on the methods for the videos with more complex
    motions and scene changes.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methods with Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The methods with alignment make neighboring frames explicitly align with the
    target frame by using extracted motion information before subsequent reconstruction.
    These methods mainly use motion estimation and motion compensation (MEMC) or deformable
    convolution, which are two common techniques for aligning frames. Next we will
    introduce state-of-the-art methods based on each of the techniques in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a6376b4e096b8da941e8d5e15fbd06f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) A target frame
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da4f1f77afee85d2356143a00ef93281.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Its neighboring frame
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42691625067f73dcab2cf9def54436cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The compensated image
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e4ca2043033c7ac28bf752b76eabb61.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) The estimated optical flow image
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: An example of motion estimation and compensation. Note that the small
    rightmost image is the legend of (d). Different colors represent different directions
    of motion and the intensity of the color is the range of motion.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Motion Estimation and Compensation Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the methods with alignment for video super-resolution, most of them apply
    the motion estimation and motion compensation techniques. Specifically, the purpose
    of motion estimation is to extract inter-frame motion information, while motion
    compensation is used to perform the warping operation between frames according
    to inter-frame motion information and to make one frame align with another frame.
    A majority of the motion estimation techniques are performed by the optical flow
    method (Dosovitskiy et al., [2015](#bib.bib21)). This method tries to calculate
    the motion between two neighboring frames through their correlations and variations
    in the temporal domain. The motion estimation methods can be divided into two
    categories: traditional methods (e.g., (Lucas and Kanade, [1981](#bib.bib88))
    and (Drulea and Nedevschi, [2011](#bib.bib22))) and deep learning methods such
    as FlowNet (Dosovitskiy et al., [2015](#bib.bib21)), FlowNet 2.0 (Ilg et al.,
    [2017](#bib.bib47)) and SpyNet (Ranjan and Black, [2017](#bib.bib99)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, an optical flow method takes two frames (e.g., $I_{i}$ and $I_{j}$)
    as inputs. One is the target frame and the other is the neighboring frame. Then
    the method computes a vector field of optical flow $F_{i\rightarrow{j}}$ from
    the frame $I_{i}$ to $I_{j}$ by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{i\rightarrow{j}}(h_{i\rightarrow{j}},v_{i\rightarrow{j}})=ME(I_{i},I_{j};\theta_{ME})$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $h_{i\rightarrow{j}}$ and $v_{i\rightarrow{j}}$ is the horizontal and
    vertical components of $F_{i\rightarrow{j}}$, $ME(\cdot)$ is a function used to
    compute optical flow, and $\theta_{ME}$ is its parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The motion compensation is used to perform image transformation between images
    in terms of motion information to make neighboring frames align with the target
    frame. In general, a compensated frame $I_{j}^{\prime}$ is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $I_{j}^{\prime}=MC(I_{i},F_{i\rightarrow{j}};\theta_{MC})$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $MC(\cdot)$ is a motion compensation function, $I_{i}$, $F_{i\rightarrow{j}}$
    and $\theta_{MC}$ are the neighboring frame, optical flow and the parameter. MC
    can be achieved by some methods such as bilinear interpolation and spatial transformer
    network (STN) (Jaderberg et al., [2015](#bib.bib52)). An example of motion estimation
    and motion compensation is shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Methods with
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the ME and MC processes can be conducted by a deep learning method or
    traditional one (non-deep learning). According to the technique that applies to
    ME or MC is traditional or deep learning, we further divide the MEMC methods into
    two subcategories. If any of the processes in ME or MC utilizes a deep neural
    network, then the method falls into the deep learning category, otherwise the
    method belongs to the traditional one. Therefore, the traditional methods in the
    MEMC methods comprise of the following three ones: Deep-DE (Liao et al., [2015](#bib.bib76)),
    VSRNet (Kappeler et al., [2016](#bib.bib59)), and RRCN (Li et al., [2019](#bib.bib70)).
    The other MEMC methods are included in the deep learning subcategory. Below we
    depict some representative methods in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4766a2f51bb259672dbae3abf08c353a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The architecture of Deep-DE (Liao et al., [2015](#bib.bib76)). Here
    Motion Estim. is a motion estimation block, Motion Comp. is a motion compensation
    block, Conv is a convolutional layer and Deconv is a deconvolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Deep-DE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The deep draft-ensemble learning method (Deep-DE)¹¹1Code: http://www.cse.cuhk.edu.hk/leojia/projects/DeepSR/
    (Liao et al., [2015](#bib.bib76)) has two phases, as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). It
    first generates a series of SR drafts by adjusting the TV-$\ell_{1}$ flow (Brox
    et al., [2004](#bib.bib3); Guo and Chao, [2017](#bib.bib31)) and the motion detail
    preserving (MDP) (Xu et al., [2012](#bib.bib129)). Then both the SR drafts and
    the bicubic-interpolated LR target frame are fed into a CNN for feature extraction,
    fusion and super-resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNN in Deep-DE consists of four convolutional layers: the first three layers
    are general convolutional layers, and the last layer is a deconvolution layer.
    Their kernel sizes are 11$\times$11, 1$\times$1, 3$\times$3 and 25$\times$25,
    respectively, and the numbers of channels are 256, 512, 1 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/66c60eff91b67c156dec04dbe9d5f600.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The network architecture of VSRnet (Kappeler et al., [2016](#bib.bib59)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 VSRnet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'VSRnet²²2Code: https://superresolution.tf.fau.de/ (Kappeler et al., [2016](#bib.bib59))
    is based on the image super-resolution method, SRCNN (Dong et al., [2014](#bib.bib19)),
    and its network architecture is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.1 Deep-DE
    ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). VSRnet
    mainly consists of motion estimation and compensation modules, and three convolutional
    layers, and each convolutional layer is followed by a rectified linear unit (ReLU)
    except for the last one. The main difference between VSRnet and SRCNN is the number
    of input frames. That is, SRCNN takes a single frame as input, while VSRnet uses
    multiple successive frames, which are compensated frames. The motion information
    between frames is computed by the Druleas algorithm (Drulea and Nedevschi, [2011](#bib.bib22)).
    In addition, VSRnet proposes a filter symmetry enforcement (FSE) mechanism and
    an adaptive motion compensation mechanism, which are separately used to accelerate
    training and reduce the impact of unreliable compensated frames, and thus can
    improve video super-resolution performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b12d86b68af0e4a3968f833e4330d977.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The network architecture of RRCN (Li et al., [2019](#bib.bib70)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 RRCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The residual recurrent convolutional network (RRCN) (Li et al., [2019](#bib.bib70)),
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.2 VSRnet ‣ 4.1 Motion Estimation and
    Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey"), is a bidirectional recurrent neural
    network, which learns a residual image. RRCN proposes an unsynchronized full recurrent
    convolutional network, where unsynchronization refers to the input of multiple
    consecutive video frames, and only the middle one is super-resolved.'
  prefs: []
  type: TYPE_NORMAL
- en: RRCN uses the combined local-global with total variable (GLG-TV) method (Drulea
    and Nedevschi, [2011](#bib.bib22)) to perform motion estimation and compensation
    for the target frame and its adjacent frames. The compensated frames are used
    as input to the network. The forward convolution and recurrent convolution are
    conducted in the forward network and the backward network, respectively, and their
    outputs are summed up. Finally, the result is obtained by adding the target frame
    to the input. In order to further improve the performance, RRCN also uses the
    self-ensemble strategy and combines it with the output of the single image super-resolution
    method, EDSR+ (Lim et al., [2017](#bib.bib77)), to obtain two models named RRCN+
    and RRCN++, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95f148fa0ac7003f6e2165f9a1ee0bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The network architecture of VESPCN (Caballero et al., [2017](#bib.bib5)).
    Here $\oplus$ denotes element-wise sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 VESPCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The video efficient sub-pixel convolutional network (VESPCN) (Caballero et al.,
    [2017](#bib.bib5)) proposes a spatial motion compensation transformer (MCT) module
    for motion estimation and compensation. Then the compensated frames are fed into
    a series of convolutional layers for feature extraction and fusion, as shown in
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.1.3 RRCN ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). Finally, the super-resolution results are obtained through
    a sub-pixel convolutional layer for upsampling.'
  prefs: []
  type: TYPE_NORMAL
- en: The MCT module adopts CNNs to extract motion information and perform motion
    compensation. MCT uses a coarse-to-fine approach to compute the optical flow for
    image sequences. Firstly, in the coarse estimation stage, the network takes two
    consecutive frames (i.e., the target frame and a neighboring frame) as inputs.
    The coarse network consists of 5 convolutional layers and a sub-pixel convolutional
    layer. And it first performs the $\times$2 downsampling operation two times and
    then performs the $\times$4 upsampling operation by a sub-pixel convolutional
    layer to get coarse optical flow estimation results. Secondly, the neighboring
    frame is warped according to the optical flow. In the fine estimation stage, the
    target frame, neighboring frame, optical flow computed in the coarse stage and
    the warped neighboring frame are the input of the fine network, whose architecture
    is similar to the coarse network. It first conducts $\times$2 downsampling and
    then perform $\times$2 upsampling at the end of the network to attain the fine
    optical flow. Together with the coarse optical flow, the fine optical flow is
    used to obtain the final estimation result. Finally, the neighboring frame is
    warped again by the final optical flow to make the warped frame align with the
    target frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e264dc6fce8ec776df5677ecaf0f02c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The network architecture of DRVSR (Tao et al., [2017](#bib.bib113)).
    Here SPMC denotes a sub-pixel motion compensation layer, ConvLSTM is the convolutional
    LSTM (Shi et al., [2015](#bib.bib106)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5 DRVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The detail-revealing deep video super-resolution (DRVSR)³³3Code: https://github.com/jiangsutx/SPMC_VideoSR
    (Tao et al., [2017](#bib.bib113)) method proposes a sub-pixel motion compensation
    layer (SPMC) that can perform the up-sampling and motion compensation operations
    simultaneously for neighboring input frames according to the estimated optical
    flow information. The network architecture of DRVSR is illustrated in Fig. [8](#S4.F8
    "Figure 8 ‣ 4.1.4 VESPCN ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4
    Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'DRVSR consists of three main modules: a motion estimation module, a motion
    compensation module using the SPMC layer, and a fusion module. The motion estimation
    module is implemented by the motion compensation transformer (MCT) network (Caballero
    et al., [2017](#bib.bib5)). The SPMC layer consists of two sub-modules, namely
    grid generator and sampler. The grid generator first transforms the coordinates
    in the LR space into the coordinates in the HR space according to the optical
    flow, and then the sampler performs the interpolation operation in the HR space.
    In the fusion module, it applies the convolution with stride 2 to perform down-sampling
    and then conducts the deconvolution for up-sampling to obtain the HR residual
    image of the target frame. Together with the upsampled LR target frame, this residual
    image yields the final result. DRVSR also adopts the ConvLSTM module (Shi et al.,
    [2015](#bib.bib106)) to handle spatio-temporal information.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3fb0488d91fbe05e292495593f074e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The network architecture of RVSR (Liu et al., [2017](#bib.bib80)),
    where SR denotes Super-Resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.6 RVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Robust video super-resolution (RVSR) (Liu et al., [2017](#bib.bib80)) proposes
    a spatial alignment module to attain great alignment performance and a temporal
    adaptive module to adaptively determine the optimal scale of temporal dependency.
    And its architecture is shown in Fig. [9](#S4.F9 "Figure 9 ‣ 4.1.5 DRVSR ‣ 4.1
    Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The spatial alignment module is responsible for the alignment of the multi-frames
    so that the neighboring frames are aligned with the target frame. It first estimates
    the transformation parameters between the neighboring frame and the target frame
    through a localization net, and then makes the neighboring frame align with the
    target frame through a spatial transformation layer (Jaderberg et al., [2015](#bib.bib52))
    based on the obtained parameters. The localization net consists of two convolutional
    layers, each of which is followed by a max-pooling layer, and two fully connected
    layers. The temporal adaptive module is composed of multiple branches of SR subnetwork
    and a temporal modulation. Each subnetwork is responsible for handling a temporal
    scale (i.e., the number of input frames), and outputting the corresponding super-resolution
    result. Then the super-resolution result of each subnetwork is allocated a weight
    through the temporal modulation. The final super-resolution result is the weight
    sum of the super-resolution result of each branch and its weight. The number of
    the input frames of the temporal modulation module is identical to the maximum
    number of input frames in the super-resolution network, and the network structure
    of the temporal modulation module is the same as that of the super-resolution
    network, and both of them are based on the structure of ESPCN (Shi et al., [2016](#bib.bib105)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d0828dc6b515da2146905361d1cbf47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The network architecture of FRVSR (Sajjadi et al., [2018](#bib.bib102)).
    Here FlowNet is an optical flow estimation module, and SR Module is a super-resolution
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.7 FRVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Frame recurrent video super-resolution (FRVSR)⁴⁴4Code: https://github.com/msmsajjadi/FRVSR
    (Sajjadi et al., [2018](#bib.bib102)) mainly proposes to use the previously inferred
    HR estimate to super-resolve the subsequent frame for producing temporally consistent
    results and reducing computational cost. The architecture of FRVSR is illustrated
    in Fig. [10](#S4.F10 "Figure 10 ‣ 4.1.6 RVSR ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The detailed implementation adopts an optical estimation network to compute
    the optical flow from the previous frame to the target frame. Then the LR optical
    flow is upsampled to the same size with the HR video by bilinear interpolation.
    The HR variant of the previous frame is warped by the upsampled LR optical flow,
    and then the warped HR frame is downsampled by space-to-depth transformation to
    get the LR version. Finally, the LR variant of the warped HR frame and the target
    frame are fed into the subsequent super-resolution network to attain the result
    for the target frame. In FRVSR, the optical flow network consists of 14 convolutional
    layers, 3 pooling layers and 3 bilinear upsampling layers. Each convolutional
    layer is followed by a LeakyReLU activation function, except for the last convolutional
    layer. The super-resolution network consists of 2 convolutional layers, 2 deconvolution
    layers with $\times$2 and 10 residual blocks, where each residual block consists
    of 2 convolutional layers and a ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ab06da37118ace2b6c5f736625a1b45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The network architecture of STTN (Kim et al., [2018a](#bib.bib62)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.8 STTN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spatio-temporal transformer network (STTN) (Kim et al., [2018a](#bib.bib62))
    proposes a spatio-temporal transformer module, which is used to address the problem
    that previous optical flow methods only process a pair of video frames, which
    may cause inaccurate estimation when occlusion and luminance variation exist in
    videos. The proposed module can handle multiple frames at a time. The architecture
    of STTN is illustrated in Fig. [11](#S4.F11 "Figure 11 ‣ 4.1.7 FRVSR ‣ 4.1 Motion
    Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'STTN consists of three major modules: a spatio-temporal flow estimation module,
    a spatio-temporal sampler module, and a super-resolution module. The first module
    is a U-style network, similar to U-Net (Ronneberger et al., [2015](#bib.bib101)),
    consisting of 12 convolutional layers and two up-sampling layers. It first performs
    $\times 4$ downsampling, and then $\times 4$ up-sampling to restore the size of
    the input frames. This module is responsible for optical flow estimation of the
    consecutive input frames including the target frame and multiple neighboring frames,
    and the final output is a 3-channel spatio-temporal flow that expresses the spatial
    and temporal changes between frames. The spatio-temporal sampler module is actually
    a trilinear interpolation method, which is responsible for performing warp operation
    for current multiple neighboring frames and obtaining the aligned video frames
    according to the spatio-temporal flow obtained by the spatio-temporal flow module.
    For video super-resolution, the aligned frames can then be fed into the super-resolution
    (SR) module for feature fusion and super-resolution of the target frame.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/992281c305f048b5cdf218b75fecd0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The network architecture of SOFVSR (Wang et al., [2019](#bib.bib119)).
    Here, OFRnet is an optical flow network, and SRnet is a super-resolution module.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.9 SOFVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Super-resolution optical flow for video super-resolution tasks (SOFVSR)⁵⁵5Code:
    https://github.com/LongguangWang/SOF-VSR (Wang et al., [2019](#bib.bib119)) is
    proposed to super-resolve LR estimated optical flow for attaining great SR performance,
    and its architecture is shown in Fig. [12](#S4.F12 "Figure 12 ‣ 4.1.8 STTN ‣ 4.1
    Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The optical flow between frames is estimated by a coarse-to-fine approach including
    the optical flow reconstruction network (OFRnet), which finally yields a high-resolution
    optical flow. Then the HR optical flow is converted to the LR optical flow by
    a space-to-depth transformation. The neighboring frames are warped by the LR optical
    flow to make the neighboring frames align with the target frame. Then the super-resolution
    network (SRnet) takes the target frame and warped frames as inputs to obtain the
    final super-resolution result. SRnet consists of two convolutional layers, five
    residual dense blocks and a sub-pixel convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d93af6d9331a9df7156e7f37371d073.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The network architecture of TOFlow (Xue et al., [2019](#bib.bib130)).
    Here O.F.Estimat. is the optical flow estimation, STN is a spatial transformer
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.10 TOFlow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of the task-oriented flow (TOFlow)⁶⁶6Code: https://github.com/anchen1011/toflow
    (Xue et al., [2019](#bib.bib130)) is shown in Fig. [13](#S4.F13 "Figure 13 ‣ 4.1.9
    SOFVSR ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). TOFlow
    combines the network for optical flow estimation with the reconstruction network,
    and trains them jointly to obtain optical flow network tailored to a specific
    task such as video SR, video interpolation and video deblurring.'
  prefs: []
  type: TYPE_NORMAL
- en: TOFlow adopts SpyNet (Ranjan and Black, [2017](#bib.bib99)) as the network for
    the optical flow estimation, and then adopts a spatial transformer network (STN)
    to warp the neighboring frame according to the computed optical flow. Then the
    final result is obtained by an image processing network. For the video super-resolution
    task, the image processing module consists of 4 convolutional layers, where kernel
    sizes are 9$\times$9, 9$\times$9, 1$\times$1, and 1$\times$1, respectively, and
    the numbers of channels are 64, 64, 64, and 3, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92e78cdd3216c6c02acc593d963a5062.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The network architecture of MMCNN (Wang et al., [2019b](#bib.bib123)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.11 MMCNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of the multi-memory convolutional neural network (MMCNN)⁷⁷7Code:
    https://github.com/psychopa4/MMCNN (Wang et al., [2019b](#bib.bib123)) is shown
    in Fig. [14](#S4.F14 "Figure 14 ‣ 4.1.10 TOFlow ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"), and it consists of 5 major modules: optical flow module
    for motion estimation and motion compensation, feature extraction, multi-memory
    detail fusion, feature reconstruction, and upsample modules, where the last module
    uses a sub-pixel convolutional layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Consecutive input frames are first processed by the optical flow estimation
    module to make neighboring frames align with the target frame and then the warped
    frames are fed into subsequent network modules to attain the residual image of
    the target frame. Finally, this residual image is added into the upsampled LR
    target frame, which is computed by bicubic interpolation, to obtain the super-resolution
    result. In the multi-memory detail fusion module, MMCNN adopts the ConvLSTM module
    (Shi et al., [2015](#bib.bib106)) to merge the spatio-temporal information. Moreover,
    the feature extraction, detail fusion, and feature reconstruction modules are
    all built based on residual dense blocks (Zhang et al., [2018](#bib.bib140); Huang
    et al., [2017](#bib.bib40)), where the key difference among them is merely the
    type of network layers.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.12 MEMC-Net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The motion estimation and motion compensation network (MEMC-Net)⁸⁸8Code: https://github.com/baowenbo/MEMC-Net
    (Bao et al., [2021](#bib.bib1)), as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.1.12
    MEMC-Net ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"), mainly
    proposes an adaptive warping layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb015776aa62c7e5876668e5051a8052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: The network architecture of MEMC-Net (Bao et al., [2021](#bib.bib1)).'
  prefs: []
  type: TYPE_NORMAL
- en: The adaptive warping layer warps the neighboring frame through the estimated
    optical flow and the convolutional kernel, which are resulted from a motion estimation
    network and a kernel estimation network, respectively, and aligns the neighboring
    frame with the target frame. The motion estimation network adopts FlowNet (Dosovitskiy
    et al., [2015](#bib.bib21)), and the kernel estimation network uses an improved
    U-Net (Ronneberger et al., [2015](#bib.bib101)) including five max-pooling layers,
    five un-pooling layers and skip connections from the encoder to the decoder. In
    MEMC-Net, the architecture of the super-resolution module, namely frame enhancement
    module, is similar to that of EDSR (Lim et al., [2017](#bib.bib77)). In order
    to deal with the occlusion problem, it adopts a pre-trained ResNet18 (He et al.,
    [2016](#bib.bib36)) to extract the feature of input frames. Moreover, it feeds
    the output of the first convolutional layer of ResNet18 as the context information
    into the adaptive warping layer to perform the same operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/351014f704225170e39c220de2cc5340.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The network architecture of RTVSR (Bare et al., [2019](#bib.bib2)).
    Here SR Module denotes super-resolution module.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.13 RTVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The real-time video super-resolution (RTVSR) (Bare et al., [2019](#bib.bib2)),
    as shown in Fig. [16](#S4.F16 "Figure 16 ‣ 4.1.12 MEMC-Net ‣ 4.1 Motion Estimation
    and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey"), adopts a convolutional network called
    motion convolutional kernel estimation network, which is a full convolution codec
    structure, to estimate the motion between the target frame and the neighboring
    frame and produce a pair of 1D convolutional kernel corresponding to the current
    target frame and neighboring frame. Then the neighboring frame is warped by using
    estimated convolutional kernels to make it align with the target frame. RTVSR
    designs an important component called gated enhance units (GEUs) to learn useful
    features, which is an improved variant based on (Li et al., [2018](#bib.bib71)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ae6ae539c0ae5fbd9f020cb36400fdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The network architecture of MultiBoot VSR (Kalarot and Porikli,
    [2019](#bib.bib58)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.14 MultiBoot VSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The multi-stage multi-reference bootstrapping for video super-resolution (MultiBoot
    VSR) (Kalarot and Porikli, [2019](#bib.bib58)) consists of two stages. That is,
    in order to further improve performance, the output of the first stage is used
    as the input of the second stage. The network architecture of MultiBoot VSR is
    shown in Fig. [17](#S4.F17 "Figure 17 ‣ 4.1.13 RTVSR ‣ 4.1 Motion Estimation and
    Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The LR frames are input to the FlowNet 2.0 to compute optical flow and perform
    the motion compensation. Then the processed frames are fed into the first-stage
    network to attain the super-resolution result of the target frame. In the second
    stage of MultiBoot VSR, the output from the previous stage is downsampled, concatenated
    with the initial LR frame, and then input to the network to obtain final super-resolution
    result for the target frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67138a5e70b4b5b1279cad638260b250.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The network architecture of MuCAN (Li et al., [2020](#bib.bib73)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.15 MuCAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of multi-correspondence aggregation network (MuCAN) (Li et al.,
    [2020](#bib.bib73)) is shown in Fig. [18](#S4.F18 "Figure 18 ‣ 4.1.14 MultiBoot
    VSR ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). MuCAN
    is an end-to-end network consisting of a temporal multi-correspondence aggregation
    module (TM-CAM), a cross-scale non-local-correspondence aggregation module (CN-CAM),
    and a reconstruction module.'
  prefs: []
  type: TYPE_NORMAL
- en: In TM-CAM, two neighboring LR frames are first encoded into lower-resolution
    features to be more stable and robust to noise. Then the aggregation starts in
    the original LR feature space by an aggregation unit (AU) to compensate large
    motion while progressively moving up to low-level/high-resolution stages for subtle
    sub-pixel shift. In a single AU, a patch-based matching strategy is used since
    it naturally contains structural information. Multiple candidates are then aggregated
    to obtain sufficient context information. The aggregated information is then passed
    to CN-CAM, which then uses a pyramid structure based on AvgPool to execute spatio-temporal
    non-local attention and coarse-to-fine spatial attention. Finally, the results
    are aggregated and sent to the reconstruction module to yield the final HR result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0d02a3bcd9bc70bdaa5b9bef118c118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The network architecture of TecoGAN (Chu et al., [2020](#bib.bib12)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.16 TecoGAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Temporally coherent GAN (TecoGAN)⁹⁹9Code: https://github.com/thunil/TecoGAN
    (Chu et al., [2020](#bib.bib12)) mainly proposes a spatio-temporal discriminator
    for realistic and coherent video super-resolution, and a novel “Ping-Pong” loss
    to tackle recurrent artifacts. Like GAN, TecoGAN also consists of a generator
    and a discriminator and its architecture is shown in Fig. [19](#S4.F19 "Figure
    19 ‣ 4.1.15 MuCAN ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods
    with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: The generator takes the target frame, the previous frame and previous estimated
    HR frames as inputs. First, input frames are fed into the optical flow module,
    which is a CNN similar to the optical flow estimation module in FRVSR (Sajjadi
    et al., [2018](#bib.bib102)). In this module, the LR optical flow between the
    target frame and neighboring frames is estimated and enlarged by the bicubic interpolation
    to attain the corresponding HR optical flow. Then the previous HR frame is warped
    by the HR optical flow. The warped previous HR frame and target frame are fed
    into subsequent convolutional modules that include two convolutional layers, a
    residual block and two upsample modules with a deconvolution layer, to yield a
    restored target frame. Moreover, the discriminator assesses the quality of super-resolution
    results. The discriminator takes the generated results and GT as inputs, where
    each of them has three components, that is, three consecutive HR frames, three
    corresponding upsampled LR frames and three warped HR frames. With such input
    formats, the spatial over-smooth and temporal inconsistence in the final results
    can be relieved. TecoGAN also proposes a “ping-pong” loss function to reduce the
    long-term temporal detail drift and make super-resolution results more natural.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85b7931833b4ec8d36e4c0f801b6750e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The network architecture of BasicVSR (Chan et al., [2021b](#bib.bib7)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.17 BasicVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The authors proposed a generic framework for video super resolution, called
    BasicVSR, as shown in Fig. [20](#S4.F20 "Figure 20 ‣ 4.1.16 TecoGAN ‣ 4.1 Motion
    Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"). It is a typical bidirectional
    recurrent network, which mainly consists of three modules: the backward (B) module,
    the forward (F) module, and the upsampling (U) module. The B module receives the
    output of the next B module, current frame, and the following frame, while the
    F module receives the output of the previous F module, current frame, and the
    preceding frame. Then the outputs of the two modules are fused through a U module
    to yield the super-resolved current frame. These processes iterate until all the
    frames are super-resolved. The B/F module composes of generic components: the
    motion estimation, spatial warping, and residual blocks. The authors further propose
    two processing mechanisms the information-refill and coupled propagation, which
    consist of the IconVSR algorithm. The former addresses the performance degradation
    caused by misalignment, and the latter deals with the lack of information interaction
    between the forward processing and the backward processing in BasicVSR. In the
    information-refill mechanism, if the currently processed frame is in the selected
    keyframe set, it will be fused; otherwise, the aligned result will be directly
    sent into the residual block without fusion. This mechanism relieves error accumulation
    caused by misalignment, thus avoiding the performance degradation. In the coupling
    propagation mechanism, the output of backward propagation is directly used as
    the input of forward propagation, so as to achieve information interaction between
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the MEMC techniques are used to align neighboring frames with a
    target frame, and are probably the most common method for solving video super-resolution
    tasks. However, the problem is that they cannot guarantee the accuracy of motion
    estimation when lighting changes dramatically or there are large motions in videos.
    In these cases, the performance of the video super-resolution degrades greatly.
    This is confirmed by the assumption in (Lucas and Kanade, [1981](#bib.bib88)).
    When dealing with complex motions (not only large motions) and varying illumination,
    the calculation of motion estimation based on optical flow methods may break the
    hypothesis of brightness consistency, small moti on, and spatial coherence. Then
    the estimation of optical flow becomes inaccurate, and there arises errors, which
    easily results in artifacts and blurring. To address this issue, the methods with
    alignment (e.g., the deformable convolution which is presented as one module in
    the deep network to align frames) and the methods without alignment are both proposed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae3890db745fc1b73d869a49c58a3309.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Deformable convolution for frame alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Deformable Convolution Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The deformable convolutional network was first proposed by Dai et al. ([2017](#bib.bib13))
    and the improved variant (Zhu et al., [2019](#bib.bib141)) was proposed in 2019\.
    In ordinary CNNs, the convention is to use a fixed geometric structure in a layer,
    which restricts the network’s capability to model geometric transformations. In
    contrast, the deformable convolution is able to overcome this limitation. The
    illustration of the deformable convolution for feature alignment is shown in Fig. [21](#S4.F21
    "Figure 21 ‣ 4.1.17 BasicVSR ‣ 4.1 Motion Estimation and Compensation Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey"). The target feature maps concatenating with the neighboring
    feature maps are projected to attain offsets via additional convolutional layers.
    The offsets are applied to the conventional convolution kernel to yield a deformable
    convolution kernel, and then it is convolved with the input feature maps to produce
    the output feature maps. The methods that adopt deformable convolution mainly
    include the enhanced deformable video restoration (EDVR) (Wang et al., [2019a](#bib.bib122)),
    deformable non-local network (DNLN) (Wang et al., [2019](#bib.bib118)), and temporally
    deformable alignment network (TDAN) (Tian et al., [2020](#bib.bib114)), which
    are depicted in detail as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8a3af6ece6c7b170e7e954b70ca2c6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: The network architecture of EDVR (Wang et al., [2019a](#bib.bib122)),
    where PCD is the pyramid, cascading and deformable alignment module, and TSA is
    the temporal-spatial attention fusion module.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 EDVR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The enhanced deformable video restoration (EDVR)^(10)^(10)10Code: https://github.com/xinntao/EDVR
    (Wang et al., [2019a](#bib.bib122)), as shown in Fig. [22](#S4.F22 "Figure 22
    ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), is the champion model in the
    NTIRE19 Challenge (Nah et al., [2019a](#bib.bib91), [b](#bib.bib92)). EDVR proposes
    two key modules: the pyramid, cascading and deformable (PCD) alignment module
    as in (Ranjan and Black, [2017](#bib.bib99); Sun et al., [2018](#bib.bib110);
    Hui et al., [2018](#bib.bib43), [2021](#bib.bib44)) and the temporal-spatial attention
    (TSA) fusion module, which are used to solve large motions in videos and to effectively
    fuse multiple frames, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'EDVR mainly consists of four parts: one PCD alignment module, a TSA fusion
    module, a reconstruction module, and an upsample module using a sub-pixel convolutional
    layer. Firstly, the input frames are aligned by the PCD alignment module, and
    then the aligned frames are fused by the TSA fusion module. Then the fused results
    are fed into the reconstruction module to refine the features, and then through
    the up-sampling, a HR image called the residual image is obtained. The final result
    is obtained by adding the residual image to a direct upsampling target frame.
    To further improve performance, EDVR also adopts a two-phase approach, whose second
    phase is similar to the first but with a shallower network depth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dac714c5861833d6188e49ff0e56ef01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: The network architecture of DNLN (Wang et al., [2019](#bib.bib118)).
    Here Non-local Att. is the non-local attention module.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 DNLN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The deformable non-local network (DNLN)^(11)^(11)11Code: https://github.com/wh1h/DNLN
    (Wang et al., [2019](#bib.bib118)), as shown in Fig. [23](#S4.F23 "Figure 23 ‣
    4.2.1 EDVR ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"), designs an
    alignment module and a non-local attention module based on the deformable convolution
    (Dai et al., [2017](#bib.bib13); Zhu et al., [2019](#bib.bib141)) and non-local
    networks (Wang et al., [2018](#bib.bib121)), respectively. The alignment module
    uses the hierarchical feature fusion module (HFFB) (Hui et al., [2021](#bib.bib45))
    within the original deformable convolution to generate convolutional parameters.
    Moreover, DNLN utilizes multiple deformable convolutions in a cascaded way, which
    makes inter-frame alignment more accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb60a3cf523334be4fe61f7e37fdefd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The network architecture of TDAN (Tian et al., [2020](#bib.bib114)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 TDAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The temporally deformable alignment network (TDAN)^(12)^(12)12Code: https://github.com/YapengTian/TDAN-VSR-CVPR-2020
    (Tian et al., [2020](#bib.bib114)), as shown in Fig. [24](#S4.F24 "Figure 24 ‣
    4.2.2 DNLN ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"), applies deformable
    convolution to the target frame and the neighboring frame, and attains corresponding
    offsets. Then the neighboring frame is warped in terms of the offsets to align
    with the target frame. TDAN is divided into three parts, i.e., a feature extraction
    module, a deformable convolution module and a reconstruction module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f4240107f6dbe92f02e771b95c87ca0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: The network architecture of D3Dnet (Ying et al., [2020](#bib.bib134)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 D3Dnet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of the deformable 3D convolution network (D3Dnet)^(13)^(13)13Code:
    https://github.com/XinyiYing/D3Dnet (Ying et al., [2020](#bib.bib134)) is shown
    in Fig. [25](#S4.F25 "Figure 25 ‣ 4.2.3 TDAN ‣ 4.2 Deformable Convolution Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey"). D3Dnet proposes 3D deformable convolution to achieve strong
    spatio-temporal feature modeling capability. The inputs are first fed to a 3D
    convolutional layer to generate features, which are then fed to 5 Residual Deformable
    3D Convolution (ResD3D) blocks to achieve motion compensation and capture spatial
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c461e198d202ad722d60b546b6dbcfb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: The architecture of VESR-Net (Chen et al., [2020](#bib.bib10)),
    where CARB is the channel-attention residual block, and Separate NL denotes the
    separate non-local architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 VESR-Net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of video enhancement and super-resolution network (VESR-Net)
    (Chen et al., [2020](#bib.bib10)), as shown in Fig. [26](#S4.F26 "Figure 26 ‣
    4.2.4 D3Dnet ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"), is the
    champion model in the Youku video enhancement and super-resolution challenge.
    VESR-Net mainly consists of a feature encoder, a fusion module and a reconstruction
    module.'
  prefs: []
  type: TYPE_NORMAL
- en: The LR frames are firstly processed by the feature encoder consisting of a convolution
    layer and several stacked channel-attention residual blocks (CARBs) (Zhang et al.,
    [2018b](#bib.bib139)). Then in the fusion module, the PCD convolution in (Wang
    et al., [2019a](#bib.bib122)) performs the inter-frame feature alignment. The
    separate non-local submodule (Separate NL) divides feature maps in spatial, channel
    and temporal dimensions and processes them to obtain correlation information separately.
    In contrast to the vanilla non-local (Wang et al., [2018](#bib.bib121)) architecture,
    Separate NL can fuse the information across video frames and across pixels in
    each frame with less parameters and shallower network. Finally, VESR-Net utilizes
    CARBs followed with a feature decoder for upsampling in the reconstruction module,
    where the upsample module is implemented by a sub-pixel convolutional layer. And
    it outputs the super-resolved frame by adding with the bicubic-interpolation LR
    target frame.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of methods with alignment. In the methods with alignment, the
    motion estimation and motion compensation techniques, as a classic research topic
    in computer vision, have been applied to video super-resolution in the early years.
    MEMC has wide range of applications such as video coding and enhancing the interlaced
    scanning. As the advent of deep learning based VSR, many works employ MEMC to
    capture the motion information contained in video frames. The early work of MEMC
    is Deep-DE (Liao et al., [2015](#bib.bib76)), and some recently proposed methods
    such as VESPCN (Caballero et al., [2017](#bib.bib5)), SOFVSR (Wang et al., [2019](#bib.bib119)),
    TOFlow (Xue et al., [2019](#bib.bib130)) and FRVSR (Sajjadi et al., [2018](#bib.bib102))
    also adopted MEMC techniques. Specifically, early video super-resolution algorithms
    adopt traditional MEMC methods such as Druleas in VSRnet (Kappeler et al., [2016](#bib.bib59)),
    while subsequent algorithms such as VESPCN (Caballero et al., [2017](#bib.bib5)),
    TOFlow (Xue et al., [2019](#bib.bib130)) and FRVSR (Sajjadi et al., [2018](#bib.bib102))
    mainly design sub-module or sub-network for MEMC.
  prefs: []
  type: TYPE_NORMAL
- en: However, the accuracy of most MEMC methods is usually not guaranteed. When the
    luminance changes or the videos contain large motions between frames, the performance
    of VSR degrades dramatically. Hence, the deformable convolution (DConv), which
    is not sensitive to varying lighting and motion conditions, has attracted more
    attention from researchers. DConv applies a learnable offset to each sampling
    point compared with the conventional convolution. Therefore, DConv can not only
    expand the receptive field of convolution kernel, but also enrich the shape of
    receptive field. When handling varying lighting and motion conditions, the conventional
    convolution with fixed kernel and limited receptive field may not be capable of
    capturing varying conditions. While DConv uses a learnable parameter for the kernel
    to analyze lighting and motion features, which can better capture complex motions
    and illumination changes. The deformable convolution was proposed by Dai et al.
    ([2017](#bib.bib13)) to enhance the transformation modeling capability of CNNs
    for the geometric variations of objects. In the VSR methods, TDAN (Tian et al.,
    [2020](#bib.bib114)) first utilized it to perform inter-frame alignment. After
    that, DNLN (Wang et al., [2019](#bib.bib118)), EDVR (Wang et al., [2019a](#bib.bib122)),
    and D3Dnet (Ying et al., [2020](#bib.bib134)) further promote it for frame alignment.
    Nevertheless, the deformable convolution still has some drawbacks including high
    computational complexity and harsh convergence conditions. Therefore, there is
    still room for improvement of this technique in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the performance of the MEMC-based methods will degrade greatly
    when there were dramatic lighting changes and large motions in videos. Nevertheless,
    the network architecture is one of the important factors to affect its performance.
    Other factors include the training dataset, training strategy, data preprocessing,
    hyper-parameter setting, iteration times, etc. Although the MEMC-based methods
    have the limitation to deal with videos containing lighting changes and large
    motions, they can be counteracted by other network designs and training settings.
    For example, BasicVSR/IconVSR adopts a bidirectional recurrent network as backbone,
    which fully utilizes the global information from the video sequences and expands
    receptive field. Thus, they may gain superior performance compared with the other
    MEMC methods, which mainly use convolutions. Moreover, the training process, which
    uses a Cosine annealing scheme  (Loshchilov and Hutter, [2017](#bib.bib86)), is
    probably more refined.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Methods without Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast to the methods with alignment, the methods without alignment do
    not align neighboring frames for video super-resolution. This type of methods
    mainly exploit the spatial or spatio-temporal information for feature extraction.
    According to the dominating techniques utilized for initial feature extraction,
    we further categorize them into five types: the 2D convolution methods (2D Conv),
    3D convolution methods (3D Conv), recurrent convolutional neural network (RCNN),
    non-local network based, and other methods. Among them, the first type falls into
    the spatial methods, while the following three are the spatio-temporal methods,
    whose characteristic is to exploit both the spatial and temporal information from
    input videos. Other methods include the ones do not belong to any of the former.
    We present them in detail as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 2D Convolution Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of alignment operations such as motion estimation and motion compensation
    between frames, the input frames are directly fed into a 2D convolutional network
    to spatially perform feature extraction, fusion and super-resolution operations.
    This may be a simple approach for solving the video super-resolution problem since
    it makes the network learn the correlation information within frames by itself.
    The representative methods are VSRResFeatGAN (Lucas et al., [2019](#bib.bib87))
    and FFCVSR (Yan et al., [2019](#bib.bib131)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06a6791f8ea145663ac859e4b4d02b99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: The architecture of the generator in VSRResFeatGAN (Lucas et al.,
    [2019](#bib.bib87)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 VSRResFeatGAN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'VSRResFeatGAN (Lucas et al., [2019](#bib.bib87)) utilizes GAN to address VSR
    tasks and find a good solution by adversarial training. The generator shown in
    Fig. [27](#S5.F27 "Figure 27 ‣ 5.1 2D Convolution Methods ‣ 5 Methods without
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")
    consists of convolutional layers and residual blocks. And each residual block
    is composed of two convolutional layers and is followed by a ReLU activation function.
    Moreover, the discriminator consists of three groups of convolutions and a fully
    connected layer, where each group includes a convolutional layer, Batch Normalization
    (BN), and LeakyReLU. The discriminator determines whether the output of the generator
    is a generated image or GT image. Then the result of the discriminator reacts
    to the generator, and promotes it to yield results closer to the GT images. Finally,
    a relative satisfactory solution is obtained through an iterative optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f8c5b246ee6b673e659e72ce6be40a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: The architecture of FFCVSR (Yan et al., [2019](#bib.bib131)). Here
    Net[C] is the context network, and Net[L] is the local network.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 FFCVSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of the frame and feature-context video super-resolution (FFCVSR)^(14)^(14)14Code:
    https://github.com/linchuming/FFCVSR (Yan et al., [2019](#bib.bib131)) is shown
    in Fig. [28](#S5.F28 "Figure 28 ‣ 5.1.1 VSRResFeatGAN ‣ 5.1 2D Convolution Methods
    ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). Unlike common MEMC techniques, FFCVSR consists of several
    local networks and context networks and utilizes inter-frame information in a
    different way. The LR unaligned video frames and the HR output of the previous
    frame are directly taken as inputs to the network for the purpose of restoring
    high-frequency details and maintaining temporal consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the above two methods both exploit spatial correlation between frames
    for VSR tasks. VSRResFeatGAN utilizes adversarial training of GANs to find an
    appropriate solution. As the discriminator in GANs has to guess whether the generated
    frame is close to the ground truth, the VSR results in terms of PSNR and SSIM
    are not always satisfactory compared with other methods, such as FFCVSR.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 3D Convolution Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The 3D convolutional module (Tran et al., [2015](#bib.bib116); Ji et al., [2013](#bib.bib53))
    operates on spatio-temporal domain, compared with 2D convolution, which only utilizes
    spatial information through the sliding kernel over input frame. This is beneficial
    to the processing of video sequences, as the correlations among frames are considered
    by extracting temporal information. The representative 3D convolution methods
    for VSR are DUF (Jo et al., [2018](#bib.bib57)), FSTRN (Li et al., [2019a](#bib.bib72)),
    and 3DSRnet (Kim et al., [2019](#bib.bib61)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b8fdd2f7d06f35e2eaa5daaed0576577.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: The network architecture of DUF (Jo et al., [2018](#bib.bib57)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 DUF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dynamic upsampling filters (DUF)^(15)^(15)15Code: https://github.com/yhjo09/VSR-DUF
    (Jo et al., [2018](#bib.bib57)) has been proposed, as shown in Fig. [29](#S5.F29
    "Figure 29 ‣ 5.2 3D Convolution Methods ‣ 5 Methods without Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"). It is inspired
    by the dynamic filter network (Jia et al., [2016](#bib.bib54)) that can generate
    corresponding filters for specific inputs and then apply them to generate corresponding
    feature maps.'
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the dynamic up-sampling filter, together with the spatio-temporal
    information learned by 3D convolution, can avoid the use of motion estimation
    and motion compensation. DUF performs not only filtering, but also the up-sampling
    operation. In order to enhance high-frequency details of the super-resolution
    result, DUF uses a network to estimate residual map for the target frame. The
    final result is the sum of the residual map and the LR target frame processed
    by the dynamic upsample module with learned filters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9b5b930bfe717665f67417da6d385ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: The network architecture of FSTRN (Li et al., [2019a](#bib.bib72)).
    Here FRB denotes the fast spatio-temporal residual block.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 FSTRN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The fast spatio-temporal residual network (FSTRN) (Li et al., [2019a](#bib.bib72))
    uses a factorized 3D convolution to extract information contained in consecutive
    frames, as shown in Fig. [30](#S5.F30 "Figure 30 ‣ 5.2.1 DUF ‣ 5.2 3D Convolution
    Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). In FSTRN, a $k\times k\times k$ 3D convolutional kernel
    is decomposed into 2 cascaded kernels, whose sizes are $1\times k\times k$ and
    $k\times 1\times 1$, respectively, to reduce the computation caused by directly
    using the 3D convolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'FSTRN consists of the following four parts: an LR video shallow feature extraction
    net (LFENet), fast spatio-temporal residual blocks (FRBs), an LR feature fusion
    and up-sampling SR net (LSRNet), and a global residual learning (GRL) module.
    The GRL is mainly composed of two parts: LR space residual learning (LRL) and
    cross-space residual learning (CRL). The LRL is introduced along with the FRBs.
    And the CRL directly maps the LR video to the HR space. The designs of CRL and
    LRL can communicate the LR and HR space. Besides, FSTRN adopts a dropout layer
    after LRL to enhance generalization ability of the network. LFENet using 3D convolution
    to extract features for consecutive LR input frames. FRBs, including the decomposed
    3D convolutional layers, are responsible for extracting spatio-temporal information
    contained in input frames. LSRNet is used to fuse information from previous layers
    and conducting up-sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/73a674997c191eb28fbeb1e3f1f66f43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: The network architecture of 3DSRNet (Kim et al., [2019](#bib.bib61)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 3DSRNet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The 3D super-resolution network (3DSRNet)^(16)^(16)16Code: https://github.com/sooyekim/3DSRnet
    (Kim et al., [2019](#bib.bib61)) uses 3D convolution to extract spatio-temporal
    information contained in consecutive frames for VSR tasks. The network architecture
    is shown in Fig. [31](#S5.F31 "Figure 31 ‣ 5.2.2 FSTRN ‣ 5.2 3D Convolution Methods
    ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). The sub net of 3DSRNet can preprocess scene change as
    shown in the figure. When frames of five different scenes getting involved into
    convolution, the sub net classifies the exact location of the scene boundary through
    the module of scene boundary detection, and replaces the different scene frames
    with the temporally closest frame of the same scene as the current middle frame.
    Finally, the updated five frames are sent for subsequent video super-resolution
    sub network. This approach overcomes performance degradation caused by scene change
    to some extent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bb65b3b4d73c0b6dee75661432d6760.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: The network architecture of DSMC (Liu et al., [2021a](#bib.bib83)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 DSMC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A deep neural network with Dual Subnet and Multi-stage Communicated Upsampling
    (DSMC)^(17)^(17)17Code: https://github.com/iPrayerr/DSMC-VSR (Liu et al., [2021a](#bib.bib83))
    is proposed for super-resolution of videos with large motion. The architecture
    is shown in Fig. [32](#S5.F32 "Figure 32 ‣ 5.2.3 3DSRNet ‣ 5.2 3D Convolution
    Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). It designs a U-shaped residual dense network with 3D
    convolution (U3D-RDN) for fine implicit MEMC as well as coarse spatial feature
    extraction. moreover, DSMC presents a new Multi-Stage Communicated Upsampling
    (MSCU) module to make full use of the intermediate results of upsampling for guiding
    the VSR. Besides, a dual subnet is devised to aid the training of DSMC, whose
    dual loss helps to reduce the solution space and enhance the generalization ability.'
  prefs: []
  type: TYPE_NORMAL
- en: DSMC firstly performs deformable convolution on input consecutive frames for
    coarse feature extraction. The output feature maps are then processed by a deformable
    residual network (DResNet) (Lei and Todorovic, [2018](#bib.bib69)) to extract
    fine spatial information. Next, the feature maps are input to U3D-RDN for dimension
    reduction and correlation analyzation of spatio-temporal feature. Followed by
    another DResNet module, the feature maps are sent to MSCU module. Finally, with
    the aid of a dual subnet for training, DSMC yields the super-resolved HR frames.
    It is noted that only the output of the dual subnet and the result of VSR subnet
    are used for the loss computation of DSMC.
  prefs: []
  type: TYPE_NORMAL
- en: In brief, these 3D convolutional methods can extract spatio-temporal correlations
    contained in consecutive frames, rather than perform the motion estimation to
    extract motion information contained in frames and motion compensation to align
    them. However, most of the methods have relatively higher computational complexities
    compared with those of 2D convolutional methods, which limits them for real-time
    video super-resolution tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Recurrent Convolutional Neural Networks (RCNNs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is well known that RCNNs have strong capacity in modelling temporal dependency
    in sequential data, e.g., natural language, video and audio. A straightforward
    way is to use RCNNs to handle video sequences. Based on this key idea, several
    RCNN methods such as BRCN (Huang et al., [2015](#bib.bib41), [2018](#bib.bib42)),
    STCN (Guo and Chao, [2017](#bib.bib31)), and RISTN (Zhu et al., [2019](#bib.bib142))
    have been proposed for video super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4cfe221ee6b004adc98f1b9c9ce2a6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: The network architecture of BRCN (Huang et al., [2015](#bib.bib41),
    [2018](#bib.bib42)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 BRCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The bidirectional recurrent convolutional network (BRCN) (Huang et al., [2015](#bib.bib41),
    [2018](#bib.bib42)), as shown in Fig. [33](#S5.F33 "Figure 33 ‣ 5.3 Recurrent
    Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), is composed of two modules:
    a forward sub-network and a backward one with a similar structure, which only
    differ in the order of processing sequence. The forward subnet is responsible
    for modeling the temporal dependency from previous frames, while the backward
    subnet models temporal dependency from subsequent frames.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/719c2e951db2da32b8f3aeea7527f837.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 34: The network architecture of STCN (STCN2107AAAI). Here BMC denotes
    the bidirectional multi-scale convolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 STCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The spatio-temporal convolutional network (STCN) (Guo and Chao, [2017](#bib.bib31))
    is an end-to-end VSR method without MEMC, as shown in Fig. [34](#S5.F34 "Figure
    34 ‣ 5.3.1 BRCN ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods
    without Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"). The temporal information within frames is extracted by using LSTM (Hochreiter
    and Schmidhuber, [1997](#bib.bib38)). Similar to RISTN (Zhu et al., [2019](#bib.bib142)),
    the network consists of three parts: a spatial module, a temporal module and a
    reconstruction module. Spatial module is responsible for extracting features from
    multiple consecutive LR frames. Temporal module is a bidirectional multi-scale
    convoluted variant of LSTM, and is designed for extracting temporal correlation
    among frames.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d93d9b6d40759a90dc41b2e9d6c10191.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 35: The network architecture of RISTN (Zhu et al., [2019](#bib.bib142)),
    where RIB denotes the residual invertible block, and RDC is the residual dense
    convolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 RISTN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The residual invertible spatio-temporal network (RISTN)^(18)^(18)18Code: https://github.com/lizhuangzi/RISTN
    (Zhu et al., [2019](#bib.bib142)) is inspired by the invertible block (Jacobsen
    et al., [2018](#bib.bib51)). As shown in Fig. [35](#S5.F35 "Figure 35 ‣ 5.3.2
    STCN ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"),
    it designs a residual invertible block (RIB), a LSTM with residual dense convolution
    (RDC-LSTM), and a sparse feature fusion strategy to adaptively select useful features.
    Here RIB is used to extract spatial information of video frames effectively, and
    RDC-LSTM is used to extract spatio-temporal features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The network is mainly divided into three parts: a spatial module, a temporal
    module and a reconstruction module. The spatial module is mainly composed of multiple
    parallel RIBs, and its output is used as the input of the temporal module. In
    the temporal module, after extracting spatio-temporal information, features are
    selectively fused by a sparse fusion strategy. Finally, the HR result of the target
    frame is reconstructed by the deconvolution in the reconstruction module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d7d3b567bee22e07620d693c1885fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 36: The network architecture of RLSP (Fuoli et al., [2019a](#bib.bib25)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 RLSP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Recurrent Latent Space Propagation (RLSP)^(19)^(19)19Code: https://github.com/dariofuoli/RLSP (Fuoli
    et al., [2019a](#bib.bib25)) shown in Fig. [36](#S5.F36 "Figure 36 ‣ 5.3.3 RISTN
    ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") proposes
    a recurrent video super-resolution algorithm, which avoids the problem that a
    single video frame is processed multiple times in a non-recurrent network. In
    addition, the algorithm implicitly transmits temporal information by introducing
    hidden states containing the temporal information yielded in previous moment as
    part of the input at current moment, and does not include explicit motion estimation
    and motion compensation.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden state is generated by the RLSP Cell, which is composed of several
    convolutions. The cell receives the hidden state of the previous moment, the super-resolved
    result of the previous moment, as well as the current frame and the adjacent frames
    as inputs to yield the super-resolved result and the hidden state of the current
    moment. This procedure repeats until all frames are processed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bd2ab8fef1b0b12147b6572e2a30a496.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 37: The network architecture of RSDN (Isobe et al., [2020](#bib.bib50)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.5 RSDN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Recurrent Structure-Detail Network (RSDN)^(20)^(20)20Code: https://github.com/junpan19/RSDN (Isobe
    et al., [2020](#bib.bib50)) shown in Fig. [37](#S5.F37 "Figure 37 ‣ 5.3.4 RLSP
    ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") proposes
    to divide the frame into two components, namely structure and detail, and then
    process these two by subsequent module respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm first uses the Bicubic interpolation algorithm to downsample and
    upsample the input LR frame to extract the structure and detail components. Then
    these two components are processed by convolution and multiple SD blocks to obtain
    the structure and detail components, super-resolved results and hidden states
    at the current moment. The SD block promotes information exchange between structure
    and detail components. In addition, the RSDN proposes a hidden-state adaption
    module to select the information that is beneficial to the super resolution and
    avoid the interference of redundant information.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the RCNN-based methods are suitable for modeling the spatio-temporal
    information contained in videos, since they can map neighboring frames and thus
    effectively establish long-term dependence with more lightweight structures. However,
    conventional RCNN-based methods are difficult to train and sometimes suffer from
    the gradient vanishing problem. And they may not capture long-term dependence
    when the length of input sequences is too large, and thus may not achieve great
    performance. LSTM-based methods can overcome these constraints to some extent
    with the help of the memorization of features from shallower layers. However,
    the complex design of LSTM is a factor that limits their depth on hardware, restraining
    them to model very long-term dependence.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Non-Local Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The non-local-based method is another one that utilizes both spatial and temporal
    information contained in video frames for super-resolution. This method benefit
    from the key idea of the non-local neural network (Wang et al., [2018](#bib.bib121)),
    which was proposed to capture long-range dependencies for video classifications.
    It overcomes the flaws that convolution and recurrent computations are limited
    to the local area. Intuitively, a non-local operation is to calculate the response
    value of a position, which is equal to the weight sum of all possible positions
    in the input feature maps. Its formula is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{i}=\dfrac{1}{\mathcal{C}(x)}\sum\limits_{\forall{j}}f(x_{i},x_{j})g(x_{j})$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $i$ is the index of the output location where the response value needs
    to be calculated, $j$ is the index of all possible locations, $x$ and $y$ are
    the input and output data with the same dimensions, $f$ is a function to calculate
    the correlation between $i$ and $j$, $g$ is the function which calculates the
    feature representation of input data and $\mathcal{C}(x)$ is the normalization
    factor. Here, $g$ is usually defined as: $g(x_{j})=W_{g}x_{j}$, where $W_{g}$
    is the weight matrix that needs to learn. It should be noted that $f$ has multiple
    choices such as Gaussian, dot product, and concatenation. Therefore, the non-local
    block can easily be added into existing deep CNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c2e4b8fef8bb4e698213f5bc65c48bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 38: The network architecture of PFNL (Yi et al., [2019](#bib.bib133)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 PFNL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The progressive fusion non-local (PFNL) (Yi et al., [2019](#bib.bib133)) method
    is illustrated in Fig. [38](#S5.F38 "Figure 38 ‣ 5.4 Non-Local Methods ‣ 5 Methods
    without Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"). It mainly includes three parts: a non-local resblock, progressive fusion
    residual blocks (PFRB) and an upsampling block.'
  prefs: []
  type: TYPE_NORMAL
- en: PFNL uses non-local residual blocks to extract spatio-temporal features, and
    PFRB is proposed to fuse them. Finally, the output through a sub-pixel convolutional
    layer is added to the input frame that is up-sampled by the bicubic interpolation,
    which is the final super-resolution result. PFRB is composed of three convolutional
    layers. Firstly, the input frames are convoluted with the 3$\times$3 kernels,
    respectively, then the output feature maps are concatenated, and the channel dimension
    is reduced by performing the 1$\times$1 convolution. And the results are concatenated
    with the previous convoluted feature maps, respectively, and conducted with a
    3$\times$3 convolution. The final results are added to each input frame to obtain
    the output for current PFRB.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Other
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The methods in this sub-category do not utilize the initial feature extractions
    mentioned above. They may combine multiple techniques for super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bec359b23136a95f683dc225b3c9dd13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 39: The network architecture of RBPN (Haris et al., [2019](#bib.bib34)),
    where $\copyright$ denotes concatenation, $\ominus$ is element subtraction, and
    MISR denotes multi-image super-resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 RBPN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The recurrent back-projection network (RBPN)^(21)^(21)21Code: https://github.com/alterzero/RBPN-PyTorch (Haris
    et al., [2019](#bib.bib34)) is inspired by the back-projection algorithm (Irani
    and Peleg, [1991](#bib.bib48), [1993](#bib.bib49); Haris et al., [2018](#bib.bib33)).
    RBPN mainly consists of one feature extraction module, a projection module, and
    a reconstruction module, and its architecture is shown in Fig. [39](#S5.F39 "Figure
    39 ‣ 5.5 Other ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on
    Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature extraction module includes two operations: One is to extract the
    features of the target frame, and the other is to extract the feature from the
    concatenation of the target frame, the neighboring frame, and the calculated optical
    flow which is from the neighboring frame to the target frame, and then perform
    alignment implicitly. The optical flow is obtained by the pyflow^(22)^(22)22https://github.com/pathak22/pyflow
    method. The projection module consists of an encoder and a decoder. The encoder
    is composed of a multiple image super-resolution (MISR), a single image super-resolution
    (SISR) and residual blocks (denoted as ResBlock). The decoder consists of ResBlock
    and a strided convolution, and it takes the output of the previous encoder as
    input to produce LR features for the encoder of the next projection module. The
    concatenation of the target frame, the next neighboring frame and pre-computed
    optical flow are input to the feature extraction module, whose output is also
    for the encoder in the next projection module. The above process does not stop
    until all neighboring frames are processed. That is, projection is used recurrently,
    which is the reason of the words “recurrent back-projection network”. Finally,
    the reconstruction module takes the output of the encoder in each projection module
    by the mean of concatenation as input to produce the final SR result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/397af488c403436e53325dfc9fa47bf7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 40: The network architecture of STARnet (Haris et al., [2020](#bib.bib35)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 STARnet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of space-time-aware multi-resolution networks (STARnet) (Haris
    et al., [2020](#bib.bib35)) is shown in Fig. [41](#S5.F41 "Figure 41 ‣ 5.5.2 STARnet
    ‣ 5.5 Other ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep
    Learning: A Comprehensive Survey"). STARnet is an end-to-end network that can
    simultaneously process video super-resolution and video interpolation. It consists
    of the following three stages: initialization, refinement and reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: In the initialization stage, STARnet receives four parts of inputs including
    two LR RGB frames and their bidirectional flow images. In this stage, the two
    spatial super-resolution (S-SR) modules can execute super-resolution to the two
    LR frames by DBPN (Haris et al., [2018](#bib.bib33)) or RBPN (Haris et al., [2019](#bib.bib34))
    and re-generate their LR counterparts by a similar network to prepare for frame
    interpolation in both LR and HR spaces in the spatio-temporal super-resolution
    (ST-SR) module. Meanwhile, the motion module align the bidirectional flow images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c154bc913de43a71b05c9a3d624ca2c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 41: The network architecture of DNSTNet (Sun et al., [2020](#bib.bib111)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 DNSTNet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DNSTNet (Sun et al., [2020](#bib.bib111)) is the video super-resolution via
    the Dense Non-local Spatial-Temporal convolutional Network. Here, the dense feature
    extraction sub-network composes of the Short-term Temporal Dependency Extraction
    Block (S-TBlock), Long-term TBlock (L-TBlock), and dense connections, as shown
    in this figure. It utilizes 3D convolution to capture short-term temporal dependency
    existing from adjacent frames in S-TBlock, and the bidirectional ConvLSTM for
    capturing long-term temporal dependency in L-TBlock. It also proposes a region-level
    nonlocal block following the dense feature extraction to exploit the global information,
    and to enlarge the limited receptive field of 3D convolution and ConvLSTM. This
    non-local network divides the feature maps into multiple patches and processes
    them respectively to decrease the computational cost. To sum up, DNSTNet adopts
    multiple modules to improve the performance of VSR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although DNSTNET uses a 3D convolution module, an LSTM module and a non-local
    sub-network, it does not imply better performance than EDVR and DSMC. As it is
    known, the network architecture is one of the important factors to affect its
    performance, other factors including the training strategy, and iteration number
    also influence its performance. Compared with the methods: EDVR and DSMC, the
    training strategy of DNSTNET is probably not sophisticated designed. It is a common
    initializing method. But EDVR is initialized by parameters from a shallower similar
    network. This can boost the performance. DSMC also has a deeper structure, which
    may be helpful to improve the performance. Moreover, in DNSTNET, too many features
    without selection through the dense feature concatenation are input to the non-local
    block for computation. These features may bring redundant information, which results
    in performance degradation. While in DSMC, the extracted features are refined
    through the U3D-RDN module before they are input to a non-local block. This processing
    can enhance the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Some widely used video super-resolution datasets. Note that ’*’ represents
    unknown information.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Type | Download Link | Video Number | Resolution | Color
    Space |'
  prefs: []
  type: TYPE_TB
- en: '| YUV25 | * | Train | [https://media.xiph.org/video/derf/](https://media.xiph.org/video/derf/)
    | 25 | $386\times 288$ | YUV |'
  prefs: []
  type: TYPE_TB
- en: '| TDTFF | * | Test | [www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html](www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html)
    | 5 | $648\times 528$ for Turbine, $960\times 530$ for Dancing ,$700\times 600$
    for Treadmill, and $1000\times 580$ for Flag, $990\times 740$ for Fan | YUV |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | 2011 | Test | [https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m](https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m)
    | 4 | $720\times 480$ for Foliage and Walk, $720\times 576$ for Calendar, and
    $704\times 576$ for City | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| YUV21 | 2014 | Test | [http://www.codersvoice.com/a/webbase/video/08/152014/130.html](http://www.codersvoice.com/a/webbase/video/08/152014/130.html)
    | 21 | $352\times 288$ | YUV |'
  prefs: []
  type: TYPE_TB
- en: '| Venice | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| Myanmar | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| CDVL | 2016 | Train | [http://www.cdvl.org/](http://www.cdvl.org/) | 100
    | $1,920\times 1,080$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| UVGD^(21)^(21)21UVGD denotes the Ultra Video Group Database | 2017 | Test
    | [http://ultravideo.cs.tut.fi/](http://ultravideo.cs.tut.fi/) | 7 | $3,840\times
    2,160$ | YUV |'
  prefs: []
  type: TYPE_TB
- en: '| LMT^(22)^(22)22LMT denotes the LIVE video quality assessment database, the
    MCL-V database, and the TUM 1080p dataset. | 2017 | Train | [http://mcl.usc.edu/mcl-v-database](http://mcl.usc.edu/mcl-v-database),
    [http://live.ece.utexas.edu/research/quality/live_video.html](http://live.ece.utexas.edu/research/quality/live_video.html),[https://vision.in.tum.de/data/datasets](https://vision.in.tum.de/data/datasets)
    | * | $1,920\times 1,080$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K | 2019 | Train+Test | [http://toflow.csail.mit.edu/](http://toflow.csail.mit.edu/)
    | 91,701 | $448\times 256$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| REDS^(23)^(23)23REDS denotes the REalistic and Diverse Scenes | 2019 | Train+Test
    | [https://seungjunnah.github.io/Datasets/reds.html](https://seungjunnah.github.io/Datasets/reds.html)
    | 270 | $1,280\times 720$ | RGB |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Some major video super-resolution competitions. Note that ’EDVR+’
    stands for a method based on EDVR, and ’*’ represents unknown information.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Year | Organizer | Location | Website | Dataset | Scale | Champion
    | PSNR | SSIM |'
  prefs: []
  type: TYPE_TB
- en: '| NTIRE 2019 Video Restoration and Enhancement Challenges | 2019 | CVPR | ​Long
    Beach, California​ | [https://data.vision.ee.ethz.ch/cvl/ntire19/](https://data.vision.ee.ethz.ch/cvl/ntire19/)
    | REDS | $\times$4 | EDVR (Wang et al., [2019a](#bib.bib122)) | 31.79 | 0.8962
    |'
  prefs: []
  type: TYPE_TB
- en: '| YOUKU Video Super-Resolution and Enhancement Challenge | 2019 | Alibaba |
    Hangzhou, China | [https://tianchi.aliyun.com/competition/entrance/231711/introduction](https://tianchi.aliyun.com/competition/entrance/231711/introduction)
    | ​Youku-VESR​ | $\times$4 | VESR-Net (Chen et al., [2020](#bib.bib10)) | 37.85
    | * |'
  prefs: []
  type: TYPE_TB
- en: '| AIM 2019 Challenge on Video Extreme Super-Resolution | 2019 | ECCV | Hong
    Kong, China | [https://www.aim2019.org/](https://www.aim2019.org/) | Vid3oC |
    $\times$16 | EDVR+ | 22.53 | 0.6400 |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile Video Restoration Challenge | 2019 | ​ICIP & Kwai​ | * | [https://www.kuaishou.com/activity/icip2019](https://www.kuaishou.com/activity/icip2019)
    | * | * | * | * | * |'
  prefs: []
  type: TYPE_TB
- en: '| AIM 2020 Challenge on Video Extreme Super-Resolution | 2020 | ECCV | ​Boston,
    Massachusetts​ | [http://aim2020.org/](http://aim2020.org/) | Vid3oC | $\times$16
    | ​EVESRNet (Dario et al., [2020](#bib.bib16))​ | 22.83 | 0.6450 |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile AI 2021 Real-Time Video Super-Resolution Challenge | 2021 | CVPR |
    ​ VIRTUAL ​ | [https://ai-benchmark.com/workshops/mai/2021/](https://ai-benchmark.com/workshops/mai/2021/)
    | REDS | $\times$4 | Diggers (Ignatov et al., [2021](#bib.bib46)) | 28.33 | 0.8112
    |'
  prefs: []
  type: TYPE_TB
- en: '| NTIRE 2021 Video Super-Resolution Challenge | 2021 | CVPR | ​ VIRTUAL ​ |
    [https://data.vision.ee.ethz.ch/cvl/ntire21/](https://data.vision.ee.ethz.ch/cvl/ntire21/)
    | REDS | $\times$4 | BasicVSR++ (Chan et al., [2021d](#bib.bib9)) | 33.36 | 0.9218
    |'
  prefs: []
  type: TYPE_TB
- en: In summary, the non-local based methods introduce attention mechanisms into
    VSR tasks. They can establish effective dependence of spatio-temporal information
    by extending the receptive field to the global. However, the non-local modules
    used in them need to calculate the response at each position by attending to all
    other positions and computing a weighted average of the features in all positions.
    Thus, this incurs high computational cost, and some efforts can be made to reduce
    the computational overhead of the methods.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the methods without alignment rely on the non-linear capability of
    the neural network to learn the motion correlation between frames for video super-resolution.
    They do not utilize additional modules to align frames. The learning ability largely
    depends on the design of the deep neural network. And an elaborate design is more
    likely leading to higher performance for video super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we discuss the deeper connections between all the methods below.
    1) The methods such as EDVR, DNLN, TDAN, D3Dnet and VESR-Net, which belong to
    the deformable convolution category, all attempt to overcome the flaw of optical
    flow-based methods by using the DConv structure. The estimation of optical flow
    is inaccurate when dealing with complex motions and varying illumination, while
    the receptive field of the convolution kernel can be expanded by utilizing DConv.
    And the network can better capture complex motions and illumination changes. 2)
    The methods such as DUF, FSTRN, 3DSRnet and DSMC all employ 3D convolutional layers
    to learn spatial and temporal features simultaneously instead of the 2D convolution
    from video data. Besides, they also try to avoid the inaccuracy of motion estimation
    and compensation when complex motions involve by designing new network structures.
    3) The methods such as BRCN, STCN, RISTN, RLSP, RSDN and BasicVSR exploit long-term
    contextual information contained in video frames by using bidirectional recurrent
    convolutional networks. The bidirectional RCNN can utilize temporal dependency
    from both previous and future frames through the combination of a forward recurrent
    network and a backward recurrent network. 4) The methods such as RVSR, STCN, BRCN,
    EDVR, DNLN, TDAN, D3DNet, VESR-Net, DUF, 3DSRNet and DSMC involve the dealing
    with complex motions in the videos. 5) The methods such as MuCAN (in MEMC class),
    EDVR (in DC class), VESR-Net (in DC class) and PFNL (in non-local class) attempt
    to capture the global dependency between different positions across frame. Specifically,
    the TSA module in the EDVR method assigns pixel-level weights on each frame for
    fusion. MuCAN, VESR-Net, and PFNL all design non-local modules to correlate different
    patches, which improve the ability to capture motion information. 6) The methods
    such as DRVSR, MultiBoot VSR, and DSMC all address the video super-resolution
    with multiple scaling factors. They not only consider x4 scale, but also regard
    $\times$2, $\times$3, or $\times$8 scales. 7) The methods such as MultiBoot VSR,
    PFNL and RBPN all pay attention to improve the training strategies. For example,
    PFNL adopts residual learning to stabilize the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Performance Comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Datasets and Competitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Details of some of the most popular datasets used in VSR tasks are summarized
    in Table [2](#S5.T2 "Table 2 ‣ 5.5.3 DNSTNet ‣ 5.5 Other ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). The
    most widely-used dataset for training is Vimeo-90K, since it is currently the
    largest VSR dataset with real scenes. The most popular dataset for testing is
    Vid4, whose frames contain more high-frequency details than others. Thus, Vid4
    is frequently used for evaluating the performance of VSR methods. REDS includes
    videos with extremely large movement, which is challenging for VSR methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, we also summarize several international competitions on video super-resolution
    in Table [3](#S5.T3 "Table 3 ‣ 5.5.3 DNSTNet ‣ 5.5 Other ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). The
    NTIRE 2019 Challenge (Nah et al., [2019a](#bib.bib91), [b](#bib.bib92)) aims at
    recovering videos with large movements and diverse real-world scenes. Its winning
    solution is EDVR (Wang et al., [2019a](#bib.bib122)), which may be one of the
    most popular works for VSR. The AIM Challenges in 2019 (Fuoli et al., [2019b](#bib.bib26))
    and 2020 (Fuoli et al., [2020](#bib.bib27)) both encourage solutions of VSR with
    large scale factors. A method enhanced from EDVR won the AIM 2019 Challenge, while
    EVESRNet (Dario et al., [2020](#bib.bib16)) won the AIM 2020 Challenge. Besides,
    the YOUKU Video Super-Resolution and Enhancement Challenge, and Mobile Video Restoration
    Challenge in 2019 are both for videos which are more relevant to entertainment.
    The winning solution of YOUKU challenge is VESR-Net  (Chen et al., [2020](#bib.bib10)).
    The Mobile AI 2021 Real-Time Video Super-Resolution challenge (Ignatov et al.,
    [2021](#bib.bib46)) evaluated the solutions on an OPPO Find X2 smartphone GPU.
    The most recent NTIRE 2021 Challenge on Video Super-Resolution gauges the state-of-the-art (Son
    et al., [2021](#bib.bib109)), its winner being BasicVSR++ (Chan et al., [2021d](#bib.bib9)).
    These competitions are making great contributions to the development of video
    super-resolution and helping develop new methods for various video super-resolution
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of all the methods on the datasets with scale factor $\times
    4$. Note that ‘Internet’ means that the dataset is collected from the internet.
    ‘*’ denotes that the source of the dataset is unknown, and ‘-’ indicates that
    the method does not be tested on the datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Training Set | Test Set | Channel | Params.(MB) | BI | BD |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR | SSIM | PSNR | SSIM |'
  prefs: []
  type: TYPE_TB
- en: '| Deep-DE | * | city+temple+penguin | * | $1.11^{[1]}$ | - | - | 29.00 | 0.8870
    |'
  prefs: []
  type: TYPE_TB
- en: '| VSRnet | Myanmar | Vid4 | Y | $0.27^{[2]}$ | 24.84 | 0.7049 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Myanmar–T | Y | 31.85 | 0.8834 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VESPCN | CDVL | Vid4 | Y | $0.88^{[2]}$ | 25.35 | 0.7557 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DRVSR | * | Vid4 | Y | $2.17^{[3]}$ | 25.52 | 0.7600 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPMCS | Y | 29.69 | 0.8400 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RVSR | LMT | Vid4+temple+penguin | Y | - | 28.05 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UVGD | Y | 39.71 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FRVSR | Vimeo-90K | Vid4 | Y | $2.81^{[3]}$ | - | - | 26.69 | 0.8103 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 35.64 | 0.9319 |'
  prefs: []
  type: TYPE_TB
- en: '| SOFVSR | CDVL | DAVIS-10 | Y | $1.71^{[3]}$ | 34.32 | 0.9250 | 34.27 | 0.9250
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 26.01 | 0.7710 | 26.19 | 0.7850 |'
  prefs: []
  type: TYPE_TB
- en: '| TecoGAN | * | ToS | Y | 3.00 | - | - | 32.75 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | - | - | 25.89 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TOFlow | Vimeo-90K | Vid4 | Y | $1.41$ | 23.54 | 0.8070 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 33.08 | 0.9417 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MMCNN | * | Vid4 | Y | 10.58 | 26.28 | 0.7844 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Myanmar-T | Y | 33.06 | 0.9040 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| YUV21 | Y | 28.90 | 0.7983 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4+temple+penguin | Y | 28.97 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MEMC-Net | Vimeo-90K | Vimeo-90K-T | Y | - | 33.47 | 0.9470 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 24.37 | 0.8380 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RRCN | Myanmar | Myanmar-T | Y | - | 32.35 | 0.9023 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 25.86 | 0.7591 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| YUV21 | Y | 29.08 | 0.7986 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RTVSR | harmonicinc.com | Vid4 | Y | 15.00 | 26.36 | 0.7900 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4+temple+penguin | Y | 29.03 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MultiBoot VSR | REDS | REDS-T | RGB | 60.86 | 31.00 | 0.8822 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MuCAN | Vimeo-90K | Vimeo-90K-T | Y | 19.90 | 37.32 | 0.9465 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 25.70 | 30.88 | 0.8750 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| IconVSR | Vimeo-90K | Vimeo-90K-T | Y | 8.70 | 37.47 | 0.9476 | 37.84 | 0.9524
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 27.39 | 0.8279 | 28.04 | 0.8570 |'
  prefs: []
  type: TYPE_TB
- en: '| UDM10 | Y | - | - | 40.03 | 0.9694 |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 31.67 | 0.8948 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| EDVR | Vimeo-90K | Vid4 | Y | $20.60$ | 27.35 | 0.8264 | 27.85 | 0.8503 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 37.61 | 0.9489 | 37.81 | 0.9523 |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 31.09 | 0.8800 | 28.88 | 0.8361 |'
  prefs: []
  type: TYPE_TB
- en: '| DNLN | Vimeo-90K | Vid4 | Y | 19.74 | 27.31 | 0.8257 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPMCS | Y | 30.36 | 0.8794 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TDAN | Vimeo-90K | Vid4 | Y | $1.97^{[2]}$ | 26.24 | 0.7800 | 26.58 | 0.8010
    |'
  prefs: []
  type: TYPE_TB
- en: '| D3Dnet | Vimeo-90K | Vid4 | Y | 2.58 | 26.52 | 0.7990 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VESR-Net | Youku-VESR | Youku-VESR-T | RGB | 21.65 | - | - | 35.97 | - |'
  prefs: []
  type: TYPE_TB
- en: '| VSRResFeatGAN | Myanmar | Vid4 | Y | - | 25.51 | 0.7530 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FFCVSR | Venice+Myanmar | Vid4 | Y | - | 26.97 | 0.8300 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DUF | Vimeo-90K | Vid4 | Y | $5.82$ | - | - | 27.38 | 0.8329 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 36.87 | 0.9447 |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | Y | 28.63 | 0.8251 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| FSTRN | YUV25 | TDTFF | Y | - | - | - | 29.95 | 0.8700 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DSRnet | largeSet | Vid4 | Y | $0.11^{[3]}$ | 25.71 | 0.7588 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSMC | REDS | REDS4 | RGB | 11.58 | 30.29 | 0.8381 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 27.29 | 0.8403 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| BRCN | YUV25 | Vid4 | Y | - | - | - | 24.43 | 0.6334 |'
  prefs: []
  type: TYPE_TB
- en: '| TDTFF | Y | - | - | 28.20 | 0.7739 |'
  prefs: []
  type: TYPE_TB
- en: '| STCN | * | Hollywood2 | Y | - | - | - | 34.58 | 0.9259 |'
  prefs: []
  type: TYPE_TB
- en: '| city+temple+penguin | * | - | - | 30.27 | 0.9103 |'
  prefs: []
  type: TYPE_TB
- en: '| RISTN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Vimeo-90K &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vid4 | Y | 3.67 | 26.13 | 0.7920 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RLSP | Vimeo-90K | Vid4 | Y | $4.21$ | - | - | 27.48 | 0.8388 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 36.49 | 0.9403 |'
  prefs: []
  type: TYPE_TB
- en: '| RSDN | Vimeo-90K | Vid4 | Y | 6.19 | - | - | 27.92 | 0.8505 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 37.23 | 0.9471 |'
  prefs: []
  type: TYPE_TB
- en: '| UDM10 | Y | - | - | 39.35 | 0.9653 |'
  prefs: []
  type: TYPE_TB
- en: '| PFNL | Vimeo-90K | Vid4 | Y | $3.00$ | 26.73 | 0.8029 | 27.16 | 0.8355 |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 36.14 | 0.9363 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 29.63 | 0.8502 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| RBPN | Vimeo-90K | Vid4 | Y | $12.20$ | 27.12 | 0.8180 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 37.07 | 0.9453 | 37.20 | 0.9458 |'
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 30.09 | 0.8590 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| STARnet | Vimeo-90K | UCF101 | * | $111.61^{[4]}$ | 29.11 | 0.9240 | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | * | 30.83 | 0.9290 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Middlebury | * | 27.16 | 0.8270 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DNSTNet | Vimeo-90K | Vid4 | Y | - | 27.21 | 0.8220 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 36.86 | 0.9387 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SPMCS | Y | 29.74 | 0.8710 | - | - |'
  prefs: []
  type: TYPE_TB
- en: 6.2 Performance of Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Moreover, we summarize the performance of the representative VSR methods with
    scale factor $4$ in Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions
    ‣ 6 Performance Comparisons ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey") in terms of both PSNR and SSIM. More experimental results
    for VSR tasks with magnification factors 2 and 3 are reported in Supplementary
    Materials. The degradation types are the bicubic downsampling with the image-resize
    function (BI) and Gaussian blurring and downsampling (BD). Note that part of the
    PSNR and SSIM are from their original works. And a simple comparison on the performance
    may not be fair, since the training data, the pre-processing, and the cropped
    area in videos are likely totally different in the methods. The details about
    the performance are listed to provide reference for readers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6
    Performance Comparisons ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"), the top 5 methods in the $\times 4$ VSR task on Vimeo-90K-T dataset
    are as follows. The methods are denoted by (PSNR, BI/BD, Params.). IconVSR (37.84,
    BD, 8.70), EDVR (37.61, BI, 20.60), IconVSR (37.47, BI, 8.70), MuCAN (37.32, BI,
    19.90), and RSDN (37.23, BD, 6.19). The top 5 methods on Vid4 dataset are IconVSR
    (28.04, BD, 8.70), RSDN (27.92, BD, 6.19), EDVR (27.85, BD, 20.60), RLSP (27.48,
    BD, 4.21), and DUF (27.38, BD, 5.82). The top 4 methods on REDS4 dataset are IconVSR
    (31.67, BI, 8.70), EDVR (31.09, BI, 20.60), MuCAN (30.88, BI, 25.70), and DSMC
    (30.29, BI, 11.58). In the method evaluation, we compare the results on Y channel
    for Vimeo-90K-T and Vid4 datasets, and on RGB channel for REDS4\. PFNL and DNLN
    do not utilize all the test frames.'
  prefs: []
  type: TYPE_NORMAL
- en: IconVSR and EDVR show superior performance on the three datasets. And IconVSR
    uses optical flow for feature alignment, a bidirectional recurrent network for
    temporal feature propagation, and an information-refill mechanism for feature
    refinement. With these properties, it outperforms some other methods in some cases,
    and achieves more performance gain with BD degradation than BI degradation on
    Vimeo-90K-T and Vid4\. EDVR employs cascaded multi-scale deformable convolutions
    for alignment, and TSA to fuse multiple frames. Unlike DNLN, which also adopts
    deformable convolutions, EDVR can capture multi-scale feature information. Compared
    with TDAN and D3Dnet, the architecture of EDVR is more complicated and may learn
    more information from inputs, though they all employ deformable convolutions for
    alignment. And EDVR costs 20.60 MB parameters, which is far more than other top
    networks. This may explain its better performance.
  prefs: []
  type: TYPE_NORMAL
- en: For the Vid4 dataset, RLSP and RSDN both adopt recurrent convolutional neural
    network as backbone to utilize the temporal information contained in multiple
    frames. RSDN further divides a frame into structure and detail to process them
    respectively, and also exchange the information between them. This refined extraction
    attributes to its performance. PFNL proposes the non-local residual block to capture
    long-range spatio-temporal dependencies between frames, which may outperform some
    conventional MEMC-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: For the Vimeo-90K-T dataset, the performance of MuCAN is likely attributed to
    the two main modules, CN-CAM and TM-CAM. The former module can hierarchically
    aggregate information for handling large and subtle motion, and the latter one
    captures nonlocal communication within different feature resolutions. RSDN relies
    the information exchange between the structure and detail to gain better performance
    on Vimeo-90K-T. It is noticed that MuCAN has 19.90 MB parameters, which is far
    more than those of RSDN and IconVSR on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, for the REDS4 dataset, it is noticed that EDVR and MuCAN both have
    more than 20.0 MB parameters, which is far more than those of IconVSR and DSMC,
    though they are in the second and the third places on the top list. DSMC proposes
    the U3D-RDN module which learns coarse-to-fine spatio-temporal features, and MSCU,
    which decomposes an upsampling into multiple sub-tasks for full use of the intermediate
    results as well as a dual subnet for aiding the training. DSMC demonstrates superior
    performance to other 3D convolution methods.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Guidelines for Model Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we provide some guidelines for readers to select different
    models according to the results in Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and
    Competitions ‣ 6 Performance Comparisons ‣ Video Super-Resolution Based on Deep
    Learning: A Comprehensive Survey"). For the super-resolving videos with realistic
    textures and rich details but without large motions, the following methods can
    be prime candidates: IconVSR, RSDN, EDVR, RLSP, DUF, DNLN, DSMC, PFNL, RBPN, and
    FRVSR. These methods are ordered according to the PSNR values on the Vid4 dataset,
    whose videos contain more high-frequency details. Among them, EDVR and DNLN both
    have more than 20.0 MB parameters, which are suitable for the applications without
    tight restriction on GPU memory. And the methods such as IconVSR, RSDN, RLSP,
    DUF, PFNL and FRVSR cost less than 10.0 MB model parameters, which might be more
    appropriate for the application of mobile devices and embedding systems.'
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with video sequences with complex and large motions, one can select
    the methods, IconVSR, EDVR, DSMC, RBPN, and PFNL. The performance of these methods
    is ranged in descending order and referred to their PSNR results on the REDS dataset.
    Similar to the above applications, the number of the parameters in EDVR exceeds
    20.0 MB, while those of IconVSR and PFNL are fewer than 10.0 MB.
  prefs: []
  type: TYPE_NORMAL
- en: For generic videos except for the above two videos, we recommend the methods,
    IconVSR, EDVR, MuCAN, RSDN, RBPN, RLSP, PFNL and FRVSR. These methods are ordered
    according to the PSNR values on the Vimeo-90k-T dataset. The number of the parameters
    in EDVR is larger than 20.0 MB, and those of IconVSR, MuCAN, RSDN, RLSP, PFNL
    and FRVSR are fewer than 10.0 MB.
  prefs: []
  type: TYPE_NORMAL
- en: There are some additional tips for selecting the methods with alignment. When
    inaccurate motion estimation and alignment may introduce artifacts for videos
    with large motions or lighting changes, the deformable convolution-based methods
    are more robust for VSR tasks. When considering the online applications of video
    super-resolution, a unidirectional network may be the best candidate, where the
    information is sequentially propagated from the first frame to the last frame.
    While for offline applications, a bidirectional network in which the features
    can propagate forward and backward in time independently, is a better choice for
    VSR. In this case, the optical flow can be estimated both sequentially and reversely.
    It is known that the motion estimation is one critical step for the methods with
    alignment, which directly influences the performance of VSR methods. When more
    advanced estimation methods are proposed, they can be used to improve VSR’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Applications of Video Super-Resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By using VSR techniques, the resolution of video frames can be enhanced, and
    better visual quality and recognition accuracy can be achieved. It has a variety
    of applications, such as remote sensing, medical diagnoses, video decoding, and
    3D reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Video Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In (Glaister et al., [2011](#bib.bib29)), a patch-based super-resolution method
    was presented to decode frames for video playback, and had been integrated in
    a video compression pipeline. Dai et al. ([2015](#bib.bib14)) proposed a VSR algorithm
    based on dictionary learning and sub-pixel motion compensation. This algorithm
    adopted multiple bilevel dictionaries for single-frame SR. Meanwhile, they presented
    a dictionary learning algorithm, where the dictionaries are trained from consecutive
    video frames. In (Liu and Cui, [2018](#bib.bib85)), an improved super-resolution
    reconstruction algorithm, which was a part of the proposed low bit-rate coding
    scheme, was applied to the decoded data for reconstructing high-definition videos.
    In (Umeda et al., [2018](#bib.bib117)), an anchored neighborhood regression SR
    method (Timofte et al., [2014](#bib.bib115)) was used for decoding in the proposed
    video coding system.
  prefs: []
  type: TYPE_NORMAL
- en: Kim et al. ([2018b](#bib.bib63)) proposed a hardware-friendly VSR algorithm
    which can upscale full-high-definition (FHD) video streams to their 4K ultra-high-definition
    counterparts, and implemented it in both field programmable gate array (FPGA)
    and application specific integrated circuit (ASIC) hardware for real-time video
    reconstruction. They further presented a FPGA-based network structure for SR.
    The number of parameters is reduced by using cascaded convolutions and depth-wise
    separable residual network (Kim et al., [2018c](#bib.bib64)). In (Wei et al.,
    [2019](#bib.bib125)), a CNN-based SR algorithm was implemented and accelerated
    through network pruning and quantization, and the algorithm was integrated in
    their real-time FPGA-based system, which supports video stream transcoding from
    H.264 FHD to H.265/HEVC UHD.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Remote Sensing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The image SR methods such as VDSR and ESPCN have been utilized to enhance the
    resolution of objects in satellite videos in (Luo et al., [2017](#bib.bib89);
    Xiao et al., [2018](#bib.bib126)). In (Jiang et al., [2018a](#bib.bib55)), a progressively
    enhanced network with a transition unit was proposed to strengthen residual images
    with fine details. Moreover, Jiang et al. ([2018b](#bib.bib56)) proposed a deep
    distillation recursive network with a multi-scale purification unit to super-resolve
    the images in the Jilin-1 satellite videos. Liu et al. ([2020a](#bib.bib81)) proposed
    a framework to pose the image priors in maximum a posteriori to regularize the
    solution space and generate the corresponding high-resolution video frames. The
    framework combines the implicitly captured local motion information through exploiting
    spatiotemporal neighbors and the nonlocal spatial similarity to recover HR frames.
    The experiments on the videos from the Jilin-1 satellite and the OVS-1A satellite
    verify that the approach can preserve edges and texture details.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Medical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Poot et al. ([2010](#bib.bib97)) and Odille et al. ([2015](#bib.bib93)) reconstructed
    isotropic 3D magnetic resonance imaging (MRI) data in high resolution from multiple
    low-resolution MRI slices of different orientations, and they did not utilize
    accurate motion estimation and alignment. In (Zhang et al., [2012](#bib.bib138)),
    HR 4D computerized tomography (CT) images are super-resolved with several frames
    for each slice at different respiratory phases. Yu et al. ([2017](#bib.bib135))
    proposed a multi-slice CT SR network, which inputs the consecutive CT slices as
    video frames. It is composed of several convolution layers and a rearranging layer,
    and a subset of 5,800 slices is used to train the model, and the other 1,000 slices
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Ren et al. ([2019](#bib.bib100)) proposed a framework, which adopts an iterative
    upsampling layer and one downsampling layer in DBPN (Haris et al., [2018](#bib.bib33))
    to provide an error feedback mechanism for the reconstruction of medical videos.
    Lin et al. ([2020](#bib.bib78)) proposed a network to super-resolve the cardiac
    MRI slices, which uses bidirectional ConvLSTM as the network backbone. It utilizes
    domain knowledge of cardiac, and iteratively to enhance low-resolution MRI slices.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Surveillance Videos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shamsolmoali et al. ([2019](#bib.bib104)) proposed a deep CNN to upsample the
    low-resolution surveillance videos. The CNN is composed of less than 20 layers
    and is trained and tested on two surveillance datasets, which are mainly indoor
    videos. Lee et al. ([2018](#bib.bib68)) utilized SRGAN (Ledig et al., [2017](#bib.bib67))
    to enhance the details of the characters on license plate, and they also collected
    a video dataset with low-resolution and evaluated their method to verify its effectiveness.
    Guo et al. ([2020](#bib.bib32)) adopted DeblurGAN (Kupyn et al., [2018](#bib.bib65))
    to remove the motion blur of the adjacent frames, and then the MEMC was performed
    on adjacent frames. Finally, high-resolution video frames can be reconstructed
    through a multi-frame super-resolution algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In order to super-resolve multi-view face video, Deshmukh and Rani ([2019](#bib.bib18))
    proposed a fractional-Grey Wolf optimizer-based kernel for the neighboring pixel
    estimation in the face video. Xin et al. ([2020](#bib.bib128)) proposed a simple
    but effective motion-adaptive feedback cell that can capture the motion information
    and feed it back to the network in an adaptive way for video face super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 3D Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using the input video sequences, Burns et al. ([2017](#bib.bib4)) presented
    a SR method, which produces a 3D mesh of the observed scene with enhanced texture.
    For multiview video SR methods, Li et al. ([2016](#bib.bib74)) adopted the kernel
    regression to upgrade the information extraction layer, and utilize nonlocal means
    to information merging layer. Furthermore, Li et al. ([2019b](#bib.bib75)) proposed
    the first framework that super-resolves the appearance of 3D objects that are
    captured from multiple view points. The framework combines the power of 2D deep
    learning-based techniques with the 3D geometric information in the multi-view
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Virtual Reality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Liu et al. ([2020b](#bib.bib82)) proposed a single frame and multi-frame joint
    super-resolution network, which includes a loss function with weighted mean squared
    error for the SR of 360-degree panorama videos. They also provided a new panorama
    video dataset: the MiG Panorama Video for evaluating the panorama VSR algorithms.
    Dasari et al. ([2020](#bib.bib17)) presented a video streaming system to reduce
    bandwidth requirements for 360-degree videos. The client runs a deep learning
    based SR model to recover the video, which is heavily compressed at the server.
    The authors also compared with other state-of-the-art video streaming systems
    on video quality of experience.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Thermal Videos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In (Kwasniewska et al., [2019](#bib.bib66)), a super-resolution model based
    on CNN and residual connection was proposed to enhance the thermal videos acquired
    by thermal cameras, and contactlessly estimate the respiratory rate. Compared
    with the previous methods, the performance is improved by using super-resolved
    sequences. Gautam and Singh ([2020](#bib.bib28)) discussed the performance of
    SR techniques by using different deep neural networks on benchmark thermal datasets,
    including SRCNN (Dong et al., [2014](#bib.bib19)), EDSR (Lim et al., [2017](#bib.bib77)),
    autoencoder and SRGAN (Ledig et al., [2017](#bib.bib67)). Based on the experimental
    results, they concluded that SRGAN yields superior performance on thermal frames
    when comparing with others.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Trends and Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although great progress has been made by state-of-the-art video super-resolution
    methods based on deep learning especially on some public benchmark datasets, there
    are still challenges and trends discussed below.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Lightweight Super-Resolution Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning based video super-resolution methods enjoy high performance,
    nevertheless they have difficulty in deploying efficiently in many real-world
    problems. It is noted that their models usually have a mass of parameters and
    require vast computational and storage resources, and their training also takes
    a long time. With the popularity of mobile devices in modern life, one expects
    to apply these models on such devices. To address this issue, several lightweight
    super-resolution methods have been proposed, e.g., RISTN (Zhu et al., [2019](#bib.bib142)),
    TDAN (Tian et al., [2020](#bib.bib114)), and (Xiao et al., [2021](#bib.bib127)).
    How to design and implement a lightweight super-resolution algorithm with high
    performance for real-world applicants is a major challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Interpretability of Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep neural networks are usually considered as black boxes. That is, we do not
    know what real information the model learns when the performance is good or bad.
    In existing video super-resolution models, there is not a theoretical interpretation
    about how convolution neural networks recover low-resolution video sequences.
    With a deeper investigation on its interpretation, the performance of super-resolution
    algorithms for both videos and images may be improved greatly. Some works have
    paid attention to this problem, e.g., (Chan et al., [2021c](#bib.bib8)) and (Liu
    et al., [2021b](#bib.bib84)).
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Super-Resolution with Larger Scaling Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For video super-resolution tasks, existing works mainly focus on the case of
    the magnification factors $\times 2$, $\times 3$ and $\times 4$. The more challenging
    scales such as $\times$8 and $\times$16 have been rarely explored. With the popularity
    of high-resolution (e.g., 8K and 16K) display devices, larger scaling factors
    are to be further studied. Obviously, as the scale becomes larger, it is more
    challenging to predict and restore unknown information in video sequences. This
    may result in performance degradation for the algorithms, and weaken robustness
    in the models. Therefore, how to develop stable deep learning algorithms for VSR
    tasks with larger scaling factors is still challenging. Until now, there is seldom
    such work on VSR, while several works such as (Chan et al., [2021a](#bib.bib6))
    and (Chen et al., [2021](#bib.bib11)) were proposed for the single image super-resolution
    tank with larger scaling factors, e.g., $\times$8.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Super-Resolution with Arbitrary Scaling Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6 Performance
    Comparisons ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"),
    we can see that most video super-resolution methods are designed for the case
    of the scaling factor $\times 4$, which is not appropriate for real scenes. On
    the one hand, other scales like $\times 2$, $\times 3$ or $\times 1.5$ are also
    very common in VSR tasks. On the other hand, a video super-resolution model with
    fixed scale will seriously limit its generalization and portability. Therefore,
    a universal VSR method for arbitrary scale factors is greatly needed in real-world
    applications. Several works about image super-resolution with arbitrary scaling
    factors have been presented, e.g., (Hu et al., [2019](#bib.bib39)), and (Wang
    et al., [2021a](#bib.bib120)), while the works on arbitrary scale factor upsampling
    for videos are still seldom.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 More Reasonable $\&amp;$ Proper Degradation Process of Videos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In existing works, the degraded LR videos are attained through the two methods:
    One is directly downsampling HR videos by using interpolation, such as bicubic.
    The other is performing the Gaussian blurring on HR videos and then downsampling
    the video sequences. Although both methods perform well in theory, they always
    perform poorly in practice. As it is known, the real-world degradation process
    is very complex and includes much uncertainty. The blurring and interpolation
    are not adequate for modeling this problem. Therefore, when constructing LR videos,
    the degradation should be modeled theoretically in consistent with the real-world
    case to reduce the gap between research and practice. There are a few works involving
    the degradation process of videos for super-resolution, such as (Zhang et al.,
    [2018a](#bib.bib136)).'
  prefs: []
  type: TYPE_NORMAL
- en: 8.6 Unsupervised Super-Resolution Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most state-of-the-art VSR methods adopt a supervised learning paradigm. In other
    words, the deep neural networks require a large number of paired LR and HR video
    frames for training. However, such paired datasets are hard or costly to obtain
    in practice. One may synthesize the LR/HR video frames, the performance of super-resolution
    methods is still not satisfied as the degradation model is too simple to characterize
    the real-world problem and results in inaccurate HR/LR datasets. Thus, unsupervised
    VSR methods are highly demanded. Some works of unsupervised VSR on satellite videos
    have been proposed, e.g., (He et al., [2020](#bib.bib37)), but not about generic
    videos.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7 More Effective Scene Change Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Existing video super-resolution methods rarely involve the videos with scene
    change. In practice, a video sequence usually has many different scenes. When
    we consider the problem of video super-resolution on such videos, they have to
    be split into multiple segments without scene change and processed individually.
    This may result in large computational time. In fact, a simple subnet in 3DSRnet (Kim
    et al., [2019](#bib.bib61)) has been proposed to deal with scene change, and it
    includes scene boundary detection and frame replacement. More dedicated networks
    that can process videos with complex scene changes are necessary for real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 8.8 More Reasonable Evaluation Criteria for Video Quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The criteria for evaluating the quality of super-resolution results mainly include
    PSNR and SSIM. However, their values are not able to reflect the video quality
    for human perception. That is, even if the PSNR value of a recovered video is
    high, the video also makes people uncomfortable. Therefore, new evaluation criteria
    for videos that are consistent with human perception need to be developed. More
    attentions have been attracted to the quality evaluation for images, such as (Gu
    et al., [2020](#bib.bib30)). However, the video quality including coherence between
    frames will be investigated in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 8.9 More Effective Methods for Leveraging Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important characteristic of video super-resolution methods is leveraging
    the information contained in video frames. The effectiveness of utilization influences
    the performance directly. Although many methods have been proposed, as mentioned
    in this paper, there are still some disadvantages. For instance, 3D convolution
    and non-local modules require a large amount of computation, and the accuracy
    of optical estimation can not be guaranteed. Therefore, the methods that can effectively
    utilize information contained in different frames is worth further studying.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we reviewed the development of deep learning approaches for
    video super-resolution in recent years. We first classified existing video super-resolution
    algorithms into seven subcategories by the way of leveraging information contained
    in video frames, described the key ideas of representative methods and summarized
    the advantages and disadvantages of each method. Furthermore, we also compared
    and analyzed the performance of those methods on benchmark datasets, and outlines
    the wide applications of video super-resolution algorithms. Although the deep
    learning based VSR methods have made great progress, we listed eight open issues
    for the development of VSR algorithms, which is expected to provide some enlightment
    for researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank all the reviewers for their valuable comments. We would like to thank
    Mr. Zekun Li (Master student at School of Artificial Intelligence in Xidian University)
    and Dr. Yaowei Wang (Associate Professor with Peng Cheng Laboratory, Shenzhen,
    China) for their help in improving the quality of this manuscript. This work was
    supported by the National Natural Science Foundation of China (Nos. 61976164,
    61876220, 61876221, and 61906184).
  prefs: []
  type: TYPE_NORMAL
- en: Author Biography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hongying Liu received her B.E. and M.Sc. degrees in Computer Science and Technology
    from Xi’An University of Technology, China, in 2006 and 2009, respectively, and
    Ph.D. in Engineering from Waseda University, Japan in 2012\. Currently, she is
    a faculty member at the School of Artificial Intelligence, and also with the Key
    Laboratory of Intelligent Perception and Image Understanding of Ministry of Education,
    Xidian University, China. In addition, she is a senior member of IEEE. Her major
    research interests include image and video processing, intelligent signal processing,
    machine learning, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Zhubo Ruan received his M. Sc. degree from School of Artificial Intelligence
    in Xidian University in 2021\. His research interests include machine learning
    and video super-resolution, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Peng Zhao is currently pursuing the M.Sc. degree with the School of Artificial
    Intelligence in Xidian University. His research interests include video super-resolution,
    medical image processing, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chao Dong received the Ph.D. degree from The Chinese University of Hong Kong
    in 2016, advised by Prof. Tang and Prof. Loy. He is currently an Associate Professor
    with the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences.
    His current research interests include low-level vision problems, such as image/video
    super-resolution, denoising, and enhancement. His team won the first place in
    international super-resolution challenges: NTIRE2018, PIRM2018, and NTIRE2019.'
  prefs: []
  type: TYPE_NORMAL
- en: Fanhua Shang received the Ph.D. degree in Circuits and Systems from Xidian University,
    Xi’an, China, in 2012\. He is currently a professor with the School of Artificial
    Intelligence, Xidian University, China. Prior to joining Xidian University, he
    was a Research Associate with the Department of Computer Science and Engineering,
    The Chinese University of Hong Kong. From 2013 to 2015, he was a Post-Doctoral
    Research Fellow with the Department of Computer Science and Engineering, The Chinese
    University of Hong Kong. From 2012 to 2013, he was a Post-Doctoral Research Associate
    with the Department of Electrical and Computer Engineering, Duke University, Durham,
    NC, USA. His current research interests include machine learning, data mining,
    and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Yuanyuan Liu received the Ph.D. degree in Circuits and Systems from Xidian University,
    Xi’an, China, in 2012\. He is currently a professor with the School of Artificial
    Intelligence, Xidian University, China. Prior to joining Xidian University, he
    was a Research Associate with the Department of Computer Science and Engineering,
    The Chinese University of Hong Kong. From 2013 to 2015, he was a Post-Doctoral
    Research Fellow with the Department of Computer Science and Engineering, The Chinese
    University of Hong Kong. From 2012 to 2013, he was a Post-Doctoral Research Associate
    with the Department of Electrical and Computer Engineering, Duke University, Durham,
    NC, USA. His current research interests include machine learning, data mining,
    and pattern recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Linlin Yang received the M.Sc. degree in circuits and systems from Taiyuan University
    of Science and Technology, Taiyuan, China, in 2020\. She is currently pursuing
    the Ph.D. degree with the School of Artificial Intelligence in Xidian University,
    China. Her research interests include image processing, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Radu Timofte is a professor and chair for computer vision at University of Wurzburg,
    Germany and the recipient of a 2022 Humboldt Professorship Award for Artificial
    Intelligence. He is also a lecturer and group leader at ETH Zurich, Switzerland.
    He obtained his PhD degree in Electrical Engineering at the KU Leuven, Belgium
    in 2013, the MSc at the Univ. of Eastern Finland in 2007, and the Dipl. Eng. at
    the Technical Univ. of Iasi, Romania in 2006\. He is co-founder of Merantix, co-organizer
    of NTIRE, CLIC, AIM, MAI and PIRM events, and member of IEEE, CVF, and ELLIS.
    He is associate/area editor for journals such as IEEE Trans. PAMI, Elsevier Neurocomputing,
    Elsevier CVIU and SIAM Journal on Imaging Sciences and he regularly serves as
    area chair/SPC for conferences such as CVPR, ICCV, ECCV, IJCAI. His current research
    interests include deep learning, visual tracking, mobile AI, image/video compression,
    restoration, manipulation and enhancement.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bao et al. (2021) Bao W, Lai W, Zhang X, Gao Z, Yang M (2021) MEMC-Net: Motion
    estimation and motion compensation driven neural network for video interpolation
    and enhancement. IEEE Trans Pattern Anal Mach Intell 43(3):933–948'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bare et al. (2019) Bare B, Yan B, Ma C, Li K (2019) Real-time video super-resolution
    via motion convolution kernel estimation. Neurocomputing 367:236–245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brox et al. (2004) Brox T, Bruhn A, Papenberg N, Weickert J (2004) High accuracy
    optical flow estimation based on a theory for warping. In: Pajdla T, Matas J (eds)
    Eur. Conf. Comput. Vis., pp 25–36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burns et al. (2017) Burns C, Plyer A, Champagnat F (2017) Texture super-resolution
    for 3d reconstruction. In: Proc. IAPR Int. Conf. Mach. Vis. Appl., pp 350–353'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caballero et al. (2017) Caballero J, Ledig C, Aitken A, Acosta A, Totz J, Wang
    Z, Shi W (2017) Real-time video super-resolution with spatio-temporal networks
    and motion compensation. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 2848–2857'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021a) Chan KC, Wang X, Xu X, Gu J, Loy CC (2021a) Glean: Generative
    latent bank for large-factor image super-resolution. In: Proc. IEEE/CVF Conf.
    Comput. Vis. Pattern Recognit., pp 14245–14254'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021b) Chan KC, Wang X, Yu K, Dong C, Loy CC (2021b) BasicVSR:
    The search for essential components in video super-resolution and beyond. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 4947–4956'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021c) Chan KC, Wang X, Yu K, Dong C, Loy CC (2021c) Understanding
    deformable alignment in video super-resolution. In: Proc. AAAI Conf. Artif. Intell.,
    vol 35, pp 973–981'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021d) Chan KCK, Zhou S, Xu X, Loy CC (2021d) BasicVSR++: Improving
    video super-resolution with enhanced propagation and alignment. arXiv preprint
    arXiv:210413371'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Chen J, Tan X, Shan C, Liu S, Chen Z (2020) VESR-Net: The
    winning solution to youku video enhancement and super-resolution challenge. arXiv
    preprint arXiv:200302115'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen Y, Liu S, Wang X (2021) Learning continuous image representation
    with local implicit image function. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp 8628–8638'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2020) Chu M, Xie Y, Mayer J, Leal-Taixé L, Thuerey N (2020) Learning
    temporal coherence via self-supervision for gan-based video generation. ACM Trans
    Graph 39(4):75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Dai J, Qi H, Xiong Y, Li Y, Zhang G, Hu H, Wei Y (2017) Deformable
    convolutional networks. In: Proc IEEE Int. Conf. Comput. Vis., pp 764–773'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2015) Dai Q, Yoo S, Kappeler A, Katsaggelos AK (2015) Dictionary-based
    multiple frame video super-resolution. In: Proc. IEEE Int. Conf. Image Process.,
    pp 83–87'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daithankar and Ruikar (2020) Daithankar MV, Ruikar SD (2020) Video super resolution:
    A review. In: ICDSMLA 2019, pp 488–495'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dario et al. (2020) Dario F, Huang Z, Gu S, Radu T, et al. (2020) Aim 2020
    challenge on video extreme super-resolution: Methods and results. arXiv preprint
    arXiv:200711803'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dasari et al. (2020) Dasari M, Bhattacharya A, Vargas S, Sahu P, Balasubramanian
    A, Das SR (2020) Streaming 360-degree videos using super-resolution. In: Proc.
    IEEE Conf. Comput. Commun., pp 1977–1986'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deshmukh and Rani (2019) Deshmukh AB, Rani NU (2019) Fractional-grey wolf optimizer-based
    kernel weighted regression model for multi-view face video super resolution. Int
    J Mach Learn Cybern 10(5):859–877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2014) Dong C, Loy CC, He K, Tang X (2014) Learning a deep convolutional
    network for image super-resolution. In: Eur. Conf. Comput. Vis., pp 184–199'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2016) Dong C, Loy CC, Tang X (2016) Accelerating the super-resolution
    convolutional neural network. In: Eur. Conf. Comput. Vis., pp 391–407'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2015) Dosovitskiy A, Fischer P, Ilg E, Husser P, Hazirbas
    C, Golkov V, v d Smagt P, Cremers D, Brox T (2015) FlowNet: Learning optical flow
    with convolutional networks. In: Proc. IEEE Int. Conf. Comput. Vis., pp 2758–2766'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Drulea and Nedevschi (2011) Drulea M, Nedevschi S (2011) Total variation regularization
    of local-global optical flow. In: 2011 14th Int. IEEE Conf. Intell. Transp. Syst.
    (ITSC), pp 318–323'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fakour-Sevom et al. (2018) Fakour-Sevom V, Guldogan E, Kämäräinen JK (2018)
    360 panorama super-resolution using deep convolutional networks. In: Int. Conf.
    Comput. Vis. Theory Appl. (VISAPP), pp 159–165'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farsiu et al. (2004) Farsiu S, Robinson MD, Elad M, Milanfar P (2004) Fast and
    robust multiframe super resolution. IEEE Trans Image Process 13(10):1327–1344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2019a) Fuoli D, Gu S, Timofte R (2019a) Efficient video super-resolution
    through recurrent latent space propagation. In: Proc. IEEE/CVF Int. Conf. Comput.
    Vis. Workshop, pp 3476–3485'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2019b) Fuoli D, Gu S, Timofte R, Tao X, Li W, Guo T, Deng Z,
    Lu L, Dai T, Shen X, et al. (2019b) Aim 2019 challenge on video extreme super-resolution:
    Methods and results. In: Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshop, pp 3467–3475'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2020) Fuoli D, Huang Z, Gu S, Timofte R, Raventos A, Esfandiari
    A, Karout S, Xu X, Li X, Xiong X, et al. (2020) Aim 2020 challenge on video extreme
    super-resolution: Methods and results. In: Eur. Conf. Comput. Vis., pp 57–81'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gautam and Singh (2020) Gautam A, Singh S (2020) A comparative analysis of
    deep learning based super-resolution techniques for thermal videos. In: Proc.
    Int. Conf. Smart Syst. Inven. Technol., pp 919–925'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glaister et al. (2011) Glaister J, Chan C, Frankovich M, Tang A, Wong A (2011)
    Hybrid video compression using selective keyframe identification and patch-based
    super-resolution. In: Proc. IEEE Int. Symp. Multimedia, pp 105–110'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2020) Gu J, Cai H, Chen H, Ye X, Ren J, Dong C (2020) Image quality
    assessment for perceptual image restoration: A new dataset, benchmark and metric.
    arXiv preprint arXiv: 201115002'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo and Chao (2017) Guo J, Chao H (2017) Building an end-to-end spatial-temporal
    convolutional network for video super-resolution. In: Proc. AAAI Conf. Artif.
    Intell., pp 4053–4060'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Guo K, Guo H, Ren S, Zhang J, Li X (2020) Towards efficient
    motion-blurred public security video super-resolution based on back-projection
    networks. J Netw Comput Appl 166:102691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2018) Haris M, Shakhnarovich G, Ukita N (2018) Deep back-projection
    networks for super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 1664–1673'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2019) Haris M, Shakhnarovich G, Ukita N (2019) Recurrent back-projection
    network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 3892–3901'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2020) Haris M, Shakhnarovich G, Ukita N (2020) Space-time-aware
    multi-resolution video enhancement. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp 2859–2868'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) He K, Zhang X, Ren S, Sun J (2016) Deep residual learning
    for image recognition. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp
    770–778'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) He Z, He D, Li X, Xu J (2020) Unsupervised video satellite
    super-resolution by using only a single video. IEEE Geosci Remote Sens Lett
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural Comput 9(8):1735–1780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2019) Hu X, Mu H, Zhang X, Wang Z, Tan T, Sun J (2019) Meta-sr:
    A magnification-arbitrary network for super-resolution. In: Proc. IEEE/CVF Conf.
    Comput. Vis. Pattern Recognit., pp 1575–1584'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2017) Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017)
    Densely connected convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 2261–2269'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2015) Huang Y, Wang W, Wang L (2015) Bidirectional recurrent
    convolutional networks for multi-frame super-resolution. In: Adv. Neural Inf.
    Process. Syst., pp 235–243'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Huang Y, Wang W, Wang L (2018) Video super-resolution via
    bidirectional recurrent convolutional networks. IEEE Trans Pattern Anal Mach Intell
    40(4):1015–1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui et al. (2018) Hui T, Tang X, Loy CC (2018) LiteFlowNet: A lightweight convolutional
    neural network for optical flow estimation. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 8981–8989'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hui et al. (2021) Hui T, Tang X, Loy CC (2021) A lightweight optical flow cnn
    - revisiting data fidelity and regularization. IEEE Trans Pattern Anal Mach Intell
    43(8):2555–2569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hui et al. (2021) Hui Z, Li J, Gao X, Wang X (2021) Progressive perception-oriented
    network for single image super-resolution. Information Sciences 546:769–786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ignatov et al. (2021) Ignatov A, Romero A, Kim H, Timofte R, et al. (2021)
    Real-time video super-resolution on smartphones with deep learning, mobile ai
    2021 challenge: Report. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.
    Workshops, pp 2535–2544'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilg et al. (2017) Ilg E, Mayer N, Saikia T, Keuper M, Dosovitskiy A, Brox T
    (2017) FlowNet 2.0: Evolution of optical flow estimation with deep networks. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 1647–1655'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irani and Peleg (1991) Irani M, Peleg S (1991) Improving resolution by image
    registration. CVGIP Graphical Models Image Process 53(3):231 – 239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irani and Peleg (1993) Irani M, Peleg S (1993) Motion analysis for image enhancement:
    Resolution, occlusion, and transparency. J Vis Commun Image Represent 4(4):324
    – 335'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isobe et al. (2020) Isobe T, Jia X, Gu S, Li S, Wang S, Tian Q (2020) Video
    super-resolution with recurrent structure-detail network. In: Eur. Conf. Comput.
    Vis., pp 645–660'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacobsen et al. (2018) Jacobsen JH, Smeulders AW, Oyallon E (2018) i-RevNet:
    Deep invertible networks. In: Proc. Int. Conf. Learn. Represent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaderberg et al. (2015) Jaderberg M, Simonyan K, Zisserman A, kavukcuoglu k
    (2015) Spatial transformer networks. In: Adv. Neural Inf. Process. Syst. 28, pp
    2017–2025'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2013) Ji S, Xu W, Yang M, Yu K (2013) 3D convolutional neural networks
    for human action recognition. IEEE Trans Pattern Anal Mach Intell 35(1):221–231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2016) Jia X, De Brabandere B, Tuytelaars T, Gool LV (2016) Dynamic
    filter networks. In: Adv. Neural Inf. Process. Syst. 29, pp 667–675'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018a) Jiang K, Wang Z, Yi P, Jiang J (2018a) A progressively
    enhanced network for video satellite imagery superresolution. IEEE Signal Process
    Lett 25(11):1630–1634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018b) Jiang K, Wang Z, Yi P, Jiang J, Xiao J, Yao Y (2018b) Deep
    distillation recursive network for remote sensing imagery super-resolution. Remote
    Sens 10(11):1700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jo et al. (2018) Jo Y, Oh SW, Kang J, Kim SJ (2018) Deep video super-resolution
    network using dynamic upsampling filters without explicit motion compensation.
    In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 3224–3232'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalarot and Porikli (2019) Kalarot R, Porikli F (2019) MultiBoot VSR: Multi-stage
    multi-reference bootstrapping for video super-resolution. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit. Workshops, pp 2060–2069'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kappeler et al. (2016) Kappeler A, Yoo S, Dai Q, Katsaggelos AK (2016) Video
    super-resolution with convolutional neural networks. IEEE Trans Comput Imag 2(2):109–122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2016) Kim J, Lee JK, Lee KM (2016) Accurate image super-resolution
    using very deep convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 1646–1654'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2019) Kim SY, Lim J, Na T, Kim M (2019) Video super-resolution
    based on 3d-cnns with consideration of scene change. In: Proc. IEEE Int. Conf.
    Image Process., pp 2831–2835'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018a) Kim TH, Sajjadi MSM, Hirsch M, Schölkopf B (2018a) Spatio-temporal
    transformer network for video restoration. In: Eur. Conf. Comput. Vis., pp 111–127'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2018b) Kim Y, Choi JS, Kim M (2018b) 2x super-resolution hardware
    using edge-orientation-based linear mapping for real-time 4k uhd 60 fps video
    applications. IEEE Trans Circuits Syst Express Briefs 65(9):1274–1278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2018c) Kim Y, Choi JS, Kim M (2018c) A real-time convolutional neural
    network for super-resolution on fpga with applications to 4k uhd 60 fps video
    services. IEEE Trans Circuits Syst Video Technol 29(8):2521–2534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kupyn et al. (2018) Kupyn O, Budzan V, Mykhailych M, Mishkin D, Matas J (2018)
    Deblurgan: Blind motion deblurring using conditional adversarial networks. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 8183–8192'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwasniewska et al. (2019) Kwasniewska A, Ruminski J, Szankin M (2019) Improving
    accuracy of contactless respiratory rate estimation by enhancing thermal sequences
    with deep neural networks. Applied Sciences 9(20)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ledig et al. (2017) Ledig C, Theis L, Huszr F, Caballero J, Cunningham A, Acosta
    A, Aitken A, Tejani A, Totz J, Wang Z, Shi W (2017) Photo-realistic single image
    super-resolution using a generative adversarial network. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit., pp 105–114'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2018) Lee Y, Yun J, Hong Y, Lee J, Jeon M (2018) Accurate license
    plate recognition and super-resolution using a generative adversarial networks
    on traffic surveillance video. In: Proc. IEEE Int. Conf. Consum. Electron. - Asia,
    ICCE-Asia, pp 1–4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei and Todorovic (2018) Lei P, Todorovic S (2018) Temporal deformable residual
    networks for action segmentation in videos. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 6742–6751'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li D, Liu Y, Wang Z (2019) Video super-resolution using non-simultaneous
    fully recurrent convolutional network. IEEE Trans Image Process 28(3):1342–1355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li K, Bare B, Yan B, Feng B, Yao C (2018) Face hallucination
    based on key parts enhancement. In: Proc. IEEE Int. Conf. Acoust. Speech Signal.
    Process., pp 1378–1382'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li S, He F, Du B, Zhang L, Xu Y, Tao D (2019a) Fast spatio-temporal
    residual network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 10522–10531'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Li W, Tao X, Guo T, Qi L, Lu J, Jia J (2020) MuCAN: Multi-correspondence
    aggregation network for video super-resolution. In: Eur. Conf. Comput. Vis., pp
    335–351'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016) Li Y, Li X, Fu Z, Zhong W (2016) Multiview video super-resolution
    via information extraction and merging. In: Proc. 24th ACM Int. Conf. Multimedia,
    pp 446–450'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Li Y, Tsiminaki V, Timofte R, Pollefeys M, Gool LV (2019b)
    3d appearance super-resolution with deep learning. In: Proc. IEEE/CVF Conf. Comput.
    Vis. Pattern Recognit., pp 9671–9680'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2015) Liao R, Tao X, Li R, Ma Z, Jia J (2015) Video super-resolution
    via deep draft-ensemble learning. In: Proc IEEE Int. Conf. Comput. Vis., pp 531–539'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lim et al. (2017) Lim B, Son S, Kim H, Nah S, Lee KM (2017) Enhanced deep residual
    networks for single image super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit. Workshops, pp 1132–1140'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Lin JY, Chang YC, Hsu WH (2020) Efficient and phase-aware
    video super-resolution for cardiac mri. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 66–76'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Sun (2014) Liu C, Sun D (2014) On Bayesian adaptive video super resolution.
    IEEE Trans Pattern Anal Mach Intell 36(2):346–360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2017) Liu D, Wang Z, Fan Y, Liu X, Wang Z, Chang S, Huang T (2017)
    Robust video super-resolution with learned temporal dynamics. In: Proc IEEE Int.
    Conf. Comput. Vis., pp 2526–2534'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Liu H, Gu Y, Wang T, Li S (2020a) Satellite video super-resolution
    based on adaptively spatiotemporal neighbors and nonlocal similarity regularization.
    IEEE Trans Geosci Remote Sens 58(12):8372–8383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Liu H, Ruan Z, Fang C, Zhao P, Shang F, Liu Y, Wang L (2020b)
    A single frame and multi-frame joint network for 360-degree panorama video super-resolution.
    arXiv preprint arXiv:200810320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Liu H, Zhao P, Ruan Z, Shang F, Liu Y (2021a) Large motion
    video super-resolution with dual subnet and multi-stage communicated upsampling.
    In: Proc. AAAI Conf. Artif. Intell., pp 2127–2135'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Liu X, Shi K, Wang Z, Chen J (2021b) Exploit camera raw data
    for video super-resolution via hidden markov model inference. IEEE Trans Image
    Process 30:2127–2140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Cui (2018) Liu Z, Cui C (2018) A new low bit-rate coding scheme for
    ultra high definition video based on super-resolution reconstruction. In: Proc.
    IEEE Int. Conf. Comput. Commun. Eng. Technol., pp 325–329'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loshchilov and Hutter (2017) Loshchilov I, Hutter F (2017) Sgdr: Stochastic
    gradient descent with warm restarts. In: Proc. Int. Conf. Learn. Represent. (ICLR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lucas et al. (2019) Lucas A, LApez-Tapia S, Molina R, Katsaggelos AK (2019)
    Generative adversarial networks and perceptual losses for video super-resolution.
    IEEE Trans Image Process 28(7):3312–3327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lucas and Kanade (1981) Lucas BD, Kanade T (1981) An iterative image registration
    technique with an application to stereo vision. In: Proc. Int. Joint Conf. Artif.
    Intell., pp 674–679'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2017) Luo Y, Zhou L, Wang S, Wang Z (2017) Video satellite imagery
    super resolution via convolutional neural networks. IEEE Geosci Remote Sens Lett
    14(12):2398–2402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2015) Ma Z, Liao R, Tao X, Xu L, Jia J, Wu E (2015) Handling motion
    blur in multi-frame super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 5224–5232'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nah et al. (2019a) Nah S, Baik S, Hong S, Moon G, Son S, Timofte R, Lee KM
    (2019a) NTIRE 2019 challenge on video deblurring and super-resolution: Dataset
    and study. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, pp 1996–2005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nah et al. (2019b) Nah S, Timofte R, Gu S, Baik S, Hong S, et al. (2019b) NTIRE
    2019 challenge on video super-resolution: Methods and results. In: Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit. Workshops, pp 1985–1995'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Odille et al. (2015) Odille F, Bustin A, Chen B, Vuissoz PA, Felblinger J (2015)
    Motion-corrected, super-resolution reconstruction for high-resolution 3d cardiac
    cine MRI. In: Med. Image Comput. Comput. Assist. Interv. (MICCAI), pp 435–442'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2020) Pan J, Cheng S, Zhang J, Tang J (2020) Deep blind video super-resolution.
    arXiv preprint arXiv:200304716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patti et al. (1997) Patti AJ, Sezan MI, Tekalp AM (1997) Superresolution video
    reconstruction with arbitrary sampling lattices and nonzero aperture time. IEEE
    Trans Image Process 6(8):1064–1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2020) Peng C, Lin WA, Liao H, Chellappa R, Zhou SK (2020) Saint:
    Spatially aware interpolation network for medical slice synthesis. In: Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit., pp 7750–7759'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poot et al. (2010) Poot DH, Van Meir V, Sijbers J (2010) General and efficient
    super-resolution method for multi-slice MRI. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 615–622'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protter et al. (2009) Protter M, Elad M, Takeda H, Milanfar P (2009) Generalizing
    the nonlocal-means to super-resolution reconstruction. IEEE Trans Image Process
    18(1):36–51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ranjan and Black (2017) Ranjan A, Black MJ (2017) Optical flow estimation using
    a spatial pyramid network. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 2720–2729'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2019) Ren S, Guo H, Guo K (2019) Towards efficient medical video
    super-resolution based on deep back-projection networks. In: Proc. IEEE Int. Conf.
    iThings/GreenCom/CPSCom/SmartData, pp 682–686'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional
    networks for biomedical image segmentation. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 234–241'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sajjadi et al. (2018) Sajjadi MSM, Vemulapalli R, Brown M (2018) Frame-recurrent
    video super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp
    6626–6634'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schultz and Stevenson (1996) Schultz RR, Stevenson RL (1996) Extraction of high-resolution
    frames from video sequences. IEEE Trans Image Process 5(6):996–1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shamsolmoali et al. (2019) Shamsolmoali P, Zareapoor M, Jain DK, Jain VK, Yang
    J (2019) Deep convolution network for surveillance records super-resolution. Multimed
    Tools Appl 78(17):23815–23829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2016) Shi W, Caballero J, Huszr F, Totz J, Aitken AP, Bishop R,
    Rueckert D, Wang Z (2016) Real-time single image and video super-resolution using
    an efficient sub-pixel convolutional neural network. In: Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit., pp 1874–1883'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2015) Shi X, Chen Z, Wang H, Yeung DY, Wong Wk, Woo Wc (2015) Convolutional
    LSTM network: A machine learning approach for precipitation nowcasting. In: Adv.
    Neural Inf. Process. Syst. 28, pp 802–810'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shocher et al. (2018) Shocher A, Cohen N, Irani M (2018) Zero-shot super-resolution
    using deep internal learning. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,
    pp 3118–3126'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Singh (2020) Singh A, Singh J (2020) Survey on single image based
    super-resolution-implementation challenges and solutions. Multimed Tools Appl
    79(3):1641–1672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Son et al. (2021) Son S, Lee S, Nah S, Timofte R, Lee KM, et al. (2021) Ntire
    2021 challenge on video super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit. Workshops, pp 166–181'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2018) Sun D, Yang X, Liu M, Kautz J (2018) PWC-Net: CNNs for optical
    flow using pyramid, warping, and cost volume. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 8934–8943'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Sun W, Sun J, Zhu Y, Zhang Y (2020) Video super-resolution
    via dense non-local spatial-temporal convolutional network. Neurocomputing 403:1–12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda et al. (2009) Takeda H, Milanfar P, Protter M, Elad M (2009) Super-resolution
    without explicit subpixel motion estimation. IEEE Trans Image Process 18(9):1958–1975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2017) Tao X, Gao H, Liao R, Wang J, Jia J (2017) Detail-revealing
    deep video super-resolution. In: Proc IEEE Int. Conf. Comput. Vis., pp 4482–4490'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2020) Tian Y, Zhang Y, Fu Y, Xu C (2020) TDAN: Temporally-deformable
    alignment network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 3360–3369'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timofte et al. (2014) Timofte R, De Smet V, Van Gool L (2014) A+: Adjusted
    anchored neighborhood regression for fast super-resolution. In: Proc. Asian Conf.
    Comput. Vis., pp 111–126'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2015) Tran D, Bourdev L, Fergus R, Torresani L, Paluri M (2015)
    Learning spatiotemporal features with 3d convolutional networks. In: Proc IEEE
    Int. Conf. Comput. Vis., pp 4489–4497'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Umeda et al. (2018) Umeda S, Yano N, Watanabe H, Ikai T, Chujoh T, Ito N (2018)
    Hdr video super-resolution for future video coding. In: Int. Workshop Adv. Image
    Technol., pp 1–4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang H, Su D, Liu C, Jin L, Sun X, Peng X (2019) Deformable
    non-local network for video super-resolution. IEEE Access 7:177734–177744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang L, Guo Y, Lin Z, Deng X, An W (2019) Learning for video
    super-resolution through HR optical flow estimation. In: Proc. Asian Conf. Comput.
    Vis., pp 514–529'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Wang L, Wang Y, Lin Z, Yang J, An W, Guo Y (2021a) Learning
    a single network for scale-arbitrary super-resolution. In: Proc. IEEE/CVF Int.
    Conf. Comput. Vis., pp 4801–4810'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Wang X, Girshick R, Gupta A, He K (2018) Non-local neural
    networks. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 7794–7803'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang X, Chan KCK, Yu K, Dong C, Loy CC (2019a) EDVR: Video
    restoration with enhanced deformable convolutional networks. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit. Workshops, pp 1954–1963'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Wang Z, Yi P, Jiang K, Jiang J, Han Z, Lu T, Ma J (2019b)
    Multi-memory convolutional neural network for video super-resolution. IEEE Trans
    Image Process 28(5):2530–2544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Wang Z, Chen J, Hoi SC (2021b) Deep learning for image
    super-resolution: A survey. IEEE Trans Pattern Anal Mach Intell 43(10):3365–3387'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2019) Wei Y, Chen L, Xie R, Song L, Zhang X, Gao Z (2019) FPGA
    based video transcoding system with 2k-4k super-resolution conversion. In: Proc.
    IEEE Int. Conf. Vis. Commun. Image Process., pp 1–2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2018) Xiao A, Wang Z, Wang L, Ren Y (2018) Super-resolution for
    ¡°jilin-1¡± satellite video imagery via a convolutional network. Sensors 18(4):1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2021) Xiao Z, Fu X, Huang J, Cheng Z, Xiong Z (2021) Space-time
    distillation for video super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit., pp 2113–2122'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. (2020) Xin J, Wang N, Li J, Gao X, Li Z (2020) Video face super-resolution
    with motion-adaptive feedback cell. Proc AAAI Conf Artif Intell 34(7):12468–12475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2012) Xu L, Jia J, Matsushita Y (2012) Motion detail preserving optical
    flow estimation. IEEE Trans Pattern Anal Mach Intell 34(9):1744–1757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2019) Xue T, Chen B, Wu J, Wei D, Freeman WT (2019) Video enhancement
    with task-oriented flow. Int J Comput Vis 127(8):1106–1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2019) Yan B, Lin C, Tan W (2019) Frame and feature-context video
    super-resolution. In: Proc. AAAI Conf. Artif. Intell., pp 5597–5604'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Yang W, Zhang X, Tian Y, Wang W, Xue JH, Liao Q (2019) Deep
    learning for single image super-resolution: A brief review. IEEE Trans Multimedia
    21(12):3106–3121'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2019) Yi P, Wang Z, Jiang K, Jiang J, Ma J (2019) Progressive fusion
    video super-resolution network via exploiting non-local spatio-temporal correlations.
    In: Proc IEEE Int. Conf. Comput. Vis., pp 3106–3115'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ying et al. (2020) Ying X, Wang L, Wang Y, Sheng W, An W, Guo Y (2020) Deformable
    3d convolution for video super-resolution. arXiv preprint arXiv:200402803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017) Yu H, Liu D, Shi H, Yu H, Wang Z, Wang X, Cross B, Bramler
    M, Huang TS (2017) Computed tomography super-resolution using convolutional neural
    networks. In: Proc. IEEE Int. Conf. Image Process., pp 3944–3948'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Zhang T, Gao K, Ni G, Fan G, Lu Y (2018a) Spatio-temporal
    super-resolution for multi-videos based on belief propagation. Signal Process
    Image Commun 68:1–12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Zhang W, Li H, Li Y, Liu H, Chen Y, Ding X (2021) Application
    of deep learning algorithms in geotechnical engineering: a short critical review.
    Artif Intell Review pp 1–41'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2012) Zhang Y, Wu G, Yap PT, Feng Q, Lian J, Chen W, Shen D (2012)
    Reconstruction of super-resolution lung 4d-ct using patch-based sparse representation.
    In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 925–931'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Zhang Y, Li K, Li K, Wang L, Zhong B, Fu Y (2018b) Image
    super-resolution using very deep residual channel attention networks. In: Eur.
    Conf. Comput. Vis., pp 294–310'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang Y, Tian Y, Kong Y, Zhong B, Fu Y (2018) Residual
    dense network for image super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit., pp 2472–2481'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Zhu X, Hu H, Lin S, Dai J (2019) Deformable ConvNets V2:
    More deformable, better results. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 9300–9308'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Zhu X, Li Z, Zhang X, Li C, Liu Y, Xue Z (2019) Residual
    invertible spatio-temporal network for video super-resolution. In: Proc. AAAI
    Conf. Artif. Intell., pp 5981–5988'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
