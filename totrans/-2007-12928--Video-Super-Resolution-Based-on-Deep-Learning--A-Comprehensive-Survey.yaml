- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:00:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:00:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2007.12928] Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2007.12928] 基于深度学习的视频超分辨率：全面综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2007.12928](https://ar5iv.labs.arxiv.org/html/2007.12928)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2007.12928](https://ar5iv.labs.arxiv.org/html/2007.12928)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: Fanhua Shang (Corresponding author): ¹¹email: fhshang@xidian.edu.cn'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: Fanhua Shang（通讯作者）： ¹¹email: fhshang@xidian.edu.cn'
- en: Hongying Liu and Fanhua Shang are with the Key Laboratory of Intelligent Perception
    and Image Understanding of Ministry of Education, School of Artificial Intelligence,
    Xidian University, China, and Peng Cheng Laboratory, Shenzhen, China.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hongying Liu 和 Fanhua Shang 现任职于中国西电大学人工智能学院教育部智能感知与图像理解重点实验室以及中国深圳鹏城实验室。
- en: Zhubo Ruan, Peng Zhao, Yuanyuan Liu and Linlin Yang are with the Key Laboratory
    of Intelligent Perception and Image Understanding of Ministry of Education, School
    of Artificial Intelligence, Xidian University, China.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Zhubo Ruan、Peng Zhao、Yuanyuan Liu 和 Linlin Yang 现任职于中国西电大学人工智能学院教育部智能感知与图像理解重点实验室。
- en: Chao Dong is with the Shenzhen Institutes of Advanced Technology, Chinese Academy
    of Sciences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Chao Dong 现任职于中国科学院深圳先进技术研究院。
- en: Radu Timofte is with ETH Zurich, Switzerland and University of Wurzburg, Germany.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Radu Timofte 现任职于瑞士苏黎世联邦理工学院和德国维尔茨堡大学。
- en: 'Video Super-Resolution Based on Deep Learning: A Comprehensive Survey'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的视频超分辨率：全面综述
- en: Hongying Liu    Zhubo Ruan    Peng Zhao    Chao Dong    Fanhua Shang    Yuanyuan Liu
       Linlin Yang    Radu Timofte
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Hongying Liu    Zhubo Ruan    Peng Zhao    Chao Dong    Fanhua Shang    Yuanyuan Liu
       Linlin Yang    Radu Timofte
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Video super-resolution (VSR) is reconstructing high-resolution videos from low
    resolution ones. Recently, the VSR methods based on deep neural networks have
    made great progress. However, there is rarely systematical review on these methods.
    In this survey, we comprehensively investigate 37 state-of-the-art VSR methods
    based on deep learning. It is well known that the leverage of information contained
    in video frames is important for video super-resolution. Thus we propose a taxonomy
    and classify the methods into seven sub-categories according to the ways of utilizing
    inter-frame information. Moreover, descriptions on the architecture design and
    implementation details are also included. Finally, we summarize and compare the
    performance of the representative VSR methods on some benchmark datasets. We also
    discuss the applications, and some challenges, which need to be further addressed
    by researchers in the community of VSR. To the best of our knowledge, this work
    is the first systematic review on VSR tasks, and it is expected to make a contribution
    to the development of recent studies in this area and potentially deepen our understanding
    of the VSR techniques based on deep learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频超分辨率（VSR）是从低分辨率视频重建高分辨率视频的技术。最近，基于深度神经网络的VSR方法取得了重大进展。然而，关于这些方法的系统性综述仍然很少。在这项调查中，我们全面研究了37种最先进的基于深度学习的VSR方法。众所周知，利用视频帧中包含的信息对视频超分辨率至关重要。因此，我们提出了一种分类法，并根据利用帧间信息的方式将这些方法划分为七个子类别。此外，还包括了架构设计和实施细节的描述。最后，我们总结并比较了代表性VSR方法在一些基准数据集上的性能。我们还讨论了应用以及一些挑战，这些挑战需要VSR领域的研究人员进一步解决。尽我们所知，这项工作是对VSR任务的首次系统综述，预计将对该领域的近期研究发展做出贡献，并可能加深我们对基于深度学习的VSR技术的理解。
- en: 'Keywords:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '关键词:'
- en: Video super-resolution Deep learning Convolutional neural networks Inter-frame
    information
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 视频超分辨率 深度学习 卷积神经网络 帧间信息
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Super-resolution (SR) aims at recovering a high-resolution (HR) image or multiple
    images from the corresponding low-resolution (LR) counterparts. It is a classic
    and challenging problem in computer vision and image processing, and it has extensive
    real-world applications, such as medical imagery reconstruction (Peng et al.,
    [2020](#bib.bib96)), remote sensing (Luo et al., [2017](#bib.bib89)), and panorama
    video super-resolution (Fakour-Sevom et al., [2018](#bib.bib23); Liu et al., [2020b](#bib.bib82)),
    surveillance systems (Deshmukh and Rani, [2019](#bib.bib18)), and high-definition
    television (Patti et al., [1997](#bib.bib95)). With the advent of the 5th generation
    mobile communication technology, large-sized images or videos can be transmitted
    within a shorter time. Meanwhile, with the popularity of high-definition (HD)
    and ultra-high-definition (UHD) display devices, video super-resolution is attracting
    more attention.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 超分辨率（SR）旨在从相应的低分辨率（LR）图像恢复高分辨率（HR）图像或多个图像。这是计算机视觉和图像处理中的一个经典且具有挑战性的问题，并且在医学影像重建（Peng
    et al., [2020](#bib.bib96)）、遥感（Luo et al., [2017](#bib.bib89)）、全景视频超分辨率（Fakour-Sevom
    et al., [2018](#bib.bib23); Liu et al., [2020b](#bib.bib82)）、监控系统（Deshmukh and
    Rani, [2019](#bib.bib18)）以及高清电视（Patti et al., [1997](#bib.bib95)）等实际应用中都有广泛的应用。随着第五代移动通信技术的到来，大尺寸图像或视频可以在更短的时间内传输。同时，随着高清（HD）和超高清（UHD）显示设备的普及，视频超分辨率正受到越来越多的关注。
- en: Video is one of the most common multimedia in our daily life, and thus super-resolution
    of low-resolution videos has become very important. In general, image super-resolution
    methods process a single image at a time, while video super-resolution algorithms
    deal with multiple successive images/frames at a time so as to utilize relationship
    within frames to super-resolve the target frame. In a broad sense, video super-resolution
    (VSR) can be regarded as an extension of image super-resolution and can be processed
    by image super-resolution algorithms frame by frame. However, the SR performance
    is not always satisfactory as artifacts and jams may be brought in, which causes
    unwanted temporal incoherence within frames.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 视频是我们日常生活中最常见的多媒体之一，因此低分辨率视频的超分辨率变得非常重要。一般来说，图像超分辨率方法一次处理一张图像，而视频超分辨率算法则一次处理多个连续的图像/帧，以利用帧间的关系来超分辨目标帧。从广义上讲，视频超分辨率（VSR）可以看作是图像超分辨率的扩展，并可以通过逐帧处理的图像超分辨率算法进行处理。然而，SR的性能并不总是令人满意，因为可能会引入伪影和卡顿，导致帧内出现不必要的时间不一致。
- en: '![Refer to caption](img/d60f9e1e260b541a3a6034d07ce52c9f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d60f9e1e260b541a3a6034d07ce52c9f.png)'
- en: 'Figure 1: The general pipeline of deep learning methods for VSR tasks. Note
    that the inter-frame alignment module can be either traditional methods or deep
    CNNs, while both the feature extraction & fusion module and the upsampling module
    usually utilize deep CNNs. The dashed line box means that the module is optional.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习方法在 VSR 任务中的一般流程图。注意，帧间对齐模块可以是传统方法或深度 CNN，而特征提取与融合模块以及上采样模块通常使用深度 CNN。虚线框表示该模块是可选的。
- en: 'In recent years, many video super-resolution algorithms have been proposed.
    They mainly fall into two categories: traditional methods and deep learning based
    methods. For some traditional methods, the motions are simply estimated by affine
    models as in (Schultz and Stevenson, [1996](#bib.bib103)). In (Protter et al.,
    [2009](#bib.bib98); Takeda et al., [2009](#bib.bib112)), they adopt non-local
    mean and 3D steering kernel regression for video super-resolution, respectively.
    Liu and Sun ([2014](#bib.bib79)) proposed a Bayesian approach to simultaneously
    estimate underlying motion, blur kernel, and noise level for reconstructing high-resolution
    frames. In (Ma et al., [2015](#bib.bib90)), the expectation maximization (EM)
    method is adopted to estimate the blur kernel, and guide the reconstruction of
    high-resolution frames. However, these explicit models of high-resolution videos
    are still inadequate to fit various scenes in videos.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已经提出了许多视频超分辨率算法。它们主要分为两类：传统方法和基于深度学习的方法。对于一些传统方法，运动通过仿射模型简单估计，如（Schultz
    and Stevenson, [1996](#bib.bib103)）。在（Protter et al., [2009](#bib.bib98); Takeda
    et al., [2009](#bib.bib112)）中，他们分别采用了非局部均值和3D引导核回归进行视频超分辨率。Liu and Sun ([2014](#bib.bib79))
    提出了一个贝叶斯方法来同时估计潜在的运动、模糊核和噪声水平，以重建高分辨率帧。在（Ma et al., [2015](#bib.bib90)）中，采用期望最大化（EM）方法来估计模糊核，并指导高分辨率帧的重建。然而，这些高分辨率视频的显式模型仍不足以适应视频中的各种场景。
- en: 'With the great success of deep learning in a variety of areas (Zhang et al.,
    [2021](#bib.bib137)), super-resolution algorithms based on deep learning are studied
    extensively. Many video super-resolution methods based on deep neural networks
    such as convolutional neural network (CNN), generative adversarial network (GAN)
    and recurrent neural network (RNN) have been proposed. Generally, they employ
    a large number of both LR and HR video sequences to input the neural network for
    inter-frame alignment, feature extraction/fusion, and then to produce the high-resolution
    sequences for the corresponding low-resolution video sequences. The pipeline of
    most video super-resolution methods mainly includes one alignment module, one
    feature extraction and fusion module, and one reconstruction module, as shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Video Super-Resolution Based on
    Deep Learning: A Comprehensive Survey"). Because of the nonlinear learning capability
    of deep neural networks, the deep learning based methods usually achieve good
    performance on many public benchmark datasets.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '随着深度学习在各个领域取得的巨大成功（Zhang et al., [2021](#bib.bib137)），基于深度学习的超分辨率算法得到了广泛研究。许多基于深度神经网络的视频超分辨率方法，如卷积神经网络（CNN）、生成对抗网络（GAN）和递归神经网络（RNN），已被提出。一般来说，它们使用大量的LR和HR视频序列输入神经网络进行帧间对齐、特征提取/融合，然后生成相应低分辨率视频序列的高分辨率序列。大多数视频超分辨率方法的流程主要包括一个对齐模块、一个特征提取和融合模块以及一个重建模块，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey")所示。由于深度神经网络的非线性学习能力，基于深度学习的方法通常在许多公共基准数据集上表现良好。'
- en: So far, there are few works about the overview on video super-resolution tasks,
    though many works  (Wang et al., [2021b](#bib.bib124); Singh and Singh, [2020](#bib.bib108);
    Yang et al., [2019](#bib.bib132)) on the investigation of single image super-resolution
    have been published. Daithankar and Ruikar ([2020](#bib.bib15)) presented a brief
    review on many frequency-spatial domain methods, while the deep learning methods
    are rarely mentioned. Unlike the previous work, we provide a comprehensive investigation
    on deep learning techniques for video super-resolution in recent years. It is
    well known that the main difference between video super-resolution and image super-resolution
    lies in the processing of inter-frame information. How to effectively leverage
    the information from neighboring frames is critical for VSR tasks. We focus on
    the ways of utilizing inter-frame information for various deep learning based
    methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，关于视频超分辨率任务的概述研究还较少，尽管已有许多关于单图像超分辨率的研究（Wang et al., [2021b](#bib.bib124);
    Singh and Singh, [2020](#bib.bib108); Yang et al., [2019](#bib.bib132)）。Daithankar和Ruikar
    ([2020](#bib.bib15)) 对许多频率-空间域方法进行了简要回顾，但很少提及深度学习方法。与以往的工作不同，我们提供了对近年来视频超分辨率中深度学习技术的全面调查。众所周知，视频超分辨率与图像超分辨率的主要区别在于帧间信息的处理。如何有效利用邻近帧的信息对VSR任务至关重要。我们关注于各种基于深度学习的方法中如何利用帧间信息。
- en: The contributions of this work are mainly summarized as follows. 1) We review
    recent works and progresses on developing techniques for deep learning based video
    super-resolution. To the best of our knowledge, this is the first comprehensive
    survey on deep learning based VSR methods. 2) We propose a taxonomy for deep learning
    based video super-resolution methods by categorizing their ways of utilizing inter-frame
    information and illustrate how the taxonomy can be used to categorize existing
    methods. 3) We summarize the performance of state-of-the-art methods on some public
    benchmark datasets, and list the applications of VSR algorithms in various areas.
    4) We further discuss some challenges and perspectives for video super-resolution
    tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的贡献主要总结如下。1) 我们回顾了基于深度学习的视频超分辨率技术的最新研究和进展。据我们所知，这是首个全面调查基于深度学习的VSR方法的综述。2)
    我们通过对帧间信息利用方式的分类，提出了基于深度学习的视频超分辨率方法的分类法，并说明了如何使用该分类法对现有方法进行分类。3) 我们总结了在一些公共基准数据集上先进方法的性能，并列出了VSR算法在各个领域的应用。4)
    我们进一步讨论了一些视频超分辨率任务的挑战和展望。
- en: The rest of the paper is organized as follows. In Section II, we briefly introduce
    the background of video super-resolution. Section III shows our taxonomy for recent
    works. In Sections IV and V, we describe the video super-resolution methods with
    and without alignment, respectively, according to the taxonomy. In Section VI,
    the performance of state-of-the-art methods is analyzed quantitatively. In Section
    VII, we discuss the challenges and prospective trends in video super-resolution.
    Finally, we conclude this work in Section VIII.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的组织结构如下。在第二节中，我们简要介绍了视频超分辨率的背景。第三节展示了我们对近期工作的分类。在第四节和第五节中，我们根据分类描述了有对齐和无对齐的视频超分辨率方法。第六节中，对现有最先进方法的性能进行了定量分析。在第七节中，我们讨论了视频超分辨率中的挑战和前景趋势。最后，在第八节中，我们总结了本文的工作。
- en: 2 Background
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'Video super-resolution stems from image super-resolution, and it aims at restoring
    high-resolution videos from multiple low-resolution frames. However, the difference
    between video and image super-resolution techniques is also obvious, that is,
    the former usually takes advantage of inter-frame information. Besides the RGB
    color space, the YUV including YCbCr color space is also widely used for VSR.
    $I_{i}\!\in\!\mathbb{R}^{H\times W\times 3}$ denotes the $i$-th frame in a LR
    video sequence $I$, and $\hat{I_{i}}\!\in\!\mathbb{R}^{sH\times sW\times 3}$ is
    the corresponding HR frame, where $s$ is the scale factor, e.g., $s\!=\!2$, 4
    or 8\. And $\{\hat{I}_{j}\}_{j=i-N}^{i+N}$ is a set of $2N\!+\!1$ HR frames for
    the center frame $\hat{I_{i}}$, where $N$ is the temporal radius. Then the degradation
    process of HR video sequences can be formulated as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 视频超分辨率源自图像超分辨率，其目标是从多个低分辨率帧中恢复高分辨率视频。然而，视频和图像超分辨率技术之间的区别也是显而易见的，即前者通常利用帧间信息。除了
    RGB 颜色空间，YUV（包括 YCbCr 颜色空间）也广泛用于 VSR。$I_{i}\!\in\!\mathbb{R}^{H\times W\times
    3}$ 表示低分辨率视频序列 $I$ 中的第 $i$ 帧，$\hat{I_{i}}\!\in\!\mathbb{R}^{sH\times sW\times
    3}$ 是相应的高分辨率帧，其中 $s$ 是缩放因子，例如 $s\!=\!2$, 4 或 8。$\{\hat{I}_{j}\}_{j=i-N}^{i+N}$
    是以中心帧 $\hat{I_{i}}$ 为中心的一组 $2N\!+\!1$ 个高分辨率帧，其中 $N$ 是时间半径。然后，高分辨率视频序列的降级过程可以表述如下：
- en: '|  | ${I_{i}}=\phi(\hat{I}_{i},\{\hat{I}_{j}\}_{j=i-N}^{i+N};\theta_{\alpha})$
    |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | ${I_{i}}=\phi(\hat{I}_{i},\{\hat{I}_{j}\}_{j=i-N}^{i+N};\theta_{\alpha})$
    |  | (1) |'
- en: 'where $\phi(\cdot;\cdot)$ is the degradation function, and the parameter $\theta_{\alpha}$
    represents various degradation factors such as noise, motion blur and downsampling
    factors. In most existing works (Liu and Sun, [2014](#bib.bib79); Ma et al., [2015](#bib.bib90);
    Farsiu et al., [2004](#bib.bib24); Pan et al., [2020](#bib.bib94)), the degradation
    process is expressed as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi(\cdot;\cdot)$ 是降级函数，参数 $\theta_{\alpha}$ 代表各种降级因素，如噪声、运动模糊和降采样因素。在大多数现有工作中（Liu
    和 Sun, [2014](#bib.bib79)；Ma 等, [2015](#bib.bib90)；Farsiu 等, [2004](#bib.bib24)；Pan
    等, [2020](#bib.bib94)），降级过程表示为：
- en: '|  | $I_{j}=DBE_{i\rightarrow j}\hat{I}_{i}+n_{j}$ |  | (2) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{j}=DBE_{i\rightarrow j}\hat{I}_{i}+n_{j}$ |  | (2) |'
- en: where $D$ and $B$ are the down-sampling and blur operations, $n_{j}$ denotes
    image noise, and $E_{i\rightarrow j}$ is the warping operation based on the motion
    from $\hat{I}_{i}$ to $\hat{I}_{j}$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 和 $B$ 分别是降采样和模糊操作，$n_{j}$ 表示图像噪声，$E_{i\rightarrow j}$ 是基于从 $\hat{I}_{i}$
    到 $\hat{I}_{j}$ 的运动的变形操作。
- en: 'In practice, it is easy to obtain LR image ${I_{j}}$, but the degradation factors,
    which may be quite complex or probably a combination of several factors, are unknown.
    Different from single image super-resolution (SISR) aiming at solving a single
    degraded image, VSR needs to deal with degraded video sequences, and recovers
    the corresponding HR video sequences, which should be as close to the ground truth
    (GT) videos as possible. Specifically, a VSR algorithm may use similar techniques
    to SISR for processing a single frame (spatial information), while it has to take
    relationships among frames (temporal information) into consideration to ensure
    motion consistency of the video. The super-resolution process, namely the reverse
    process of Eq. (1), can be formulated as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，获得低分辨率图像 ${I_{j}}$ 是容易的，但降级因素可能非常复杂，或者可能是多种因素的组合，这些因素是未知的。与旨在解决单个降级图像的单幅图像超分辨率（SISR）不同，视频超分辨率（VSR）需要处理降级的视频序列，并恢复相应的高分辨率视频序列，这些序列应尽可能接近真实视频（GT）。具体来说，VSR
    算法可能会使用类似于 SISR 的技术来处理单帧（空间信息），但必须考虑帧之间的关系（时间信息）以确保视频的运动一致性。超分辨率过程，即公式 (1) 的反向过程，可以表述如下：
- en: '|  | $\displaystyle\tilde{I_{i}}=\phi^{-1}({I_{i}},\{{I}_{j}\}_{j=i-N}^{i+N};\theta_{\beta})$
    |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{I_{i}}=\phi^{-1}({I_{i}},\{{I}_{j}\}_{j=i-N}^{i+N};\theta_{\beta})$
    |  | (3) |'
- en: where $\tilde{I_{i}}$ denotes the estimation of the GT (i.e., $\hat{I}_{i}$),
    and $\theta_{\beta}$ is the model parameter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{I_{i}}$ 表示 GT 的估计（即 $\hat{I}_{i}$），而 $\theta_{\beta}$ 是模型参数。
- en: 'Like SISR, video quality is mainly evaluated by calculating peak signal-noise
    ratio (PSNR) and structural similarity index (SSIM). These indexes measure the
    difference of pixels and similarity of structures between two images, respectively.
    PSNR of one SR frame is defined as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与单图像超分辨率（SISR）类似，视频质量主要通过计算峰值信噪比（PSNR）和结构相似性指数（SSIM）来评估。这些指标分别衡量两幅图像之间的像素差异和结构相似性。一个超分辨率帧的
    PSNR 定义为：
- en: '|  | $\textup{PSNR}=10\log_{10}\left(\frac{L^{2}}{\textup{MSE}}\right)$ |  |
    (4) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textup{PSNR}=10\log_{10}\left(\frac{L^{2}}{\textup{MSE}}\right)$ |  |
    (4) |'
- en: 'where $L$ represents the maximum range of color value, which is usually 255,
    and the mean squared error (MSE) is defined as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 代表颜色值的最大范围，通常为 255，均方误差 (MSE) 定义为：
- en: '|  | $\textup{MSE}=\frac{1}{N}\sum_{i=1}^{N}(\hat{I}_{i}-\tilde{I}_{i})^{2}$
    |  | (5) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textup{MSE}=\frac{1}{N}\sum_{i=1}^{N}(\hat{I}_{i}-\tilde{I}_{i})^{2}$
    |  | (5) |'
- en: 'where $N$ denotes the total number of pixels in an image or a frame, $\hat{I}$
    and $\tilde{I}$ are the ground truth HR frame and the SR recovered frame, respectively.
    A higher value of PSNR generally means superior quality. In addition, SSIM is
    defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 表示图像或帧中的总像素数，$\hat{I}$ 和 $\tilde{I}$ 分别是真实高分辨率帧和恢复的超分辨率帧。较高的 PSNR 值通常意味着更好的质量。此外，SSIM
    定义为：
- en: '|  | $\textup{SSIM}(\hat{I},\tilde{I})=\frac{2u_{\hat{I}}u_{\tilde{I}}+k_{1}}{u_{\hat{I}}^{2}+u_{\tilde{I}}^{2}+k_{1}}\cdot\frac{2\sigma_{\hat{I}\tilde{I}}+k_{2}}{\sigma^{2}_{\hat{I}}+\sigma_{\tilde{I}}^{2}+k_{2}}$
    |  | (6) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textup{SSIM}(\hat{I},\tilde{I})=\frac{2u_{\hat{I}}u_{\tilde{I}}+k_{1}}{u_{\hat{I}}^{2}+u_{\tilde{I}}^{2}+k_{1}}\cdot\frac{2\sigma_{\hat{I}\tilde{I}}+k_{2}}{\sigma^{2}_{\hat{I}}+\sigma_{\tilde{I}}^{2}+k_{2}}$
    |  | (6) |'
- en: where $u_{\hat{I}}$ and $u_{\tilde{I}}$ represent the mean values of the images
    $\hat{I}$ and $\tilde{I}$, respectively. $k_{1}$ and $k_{2}$ are constants, which
    are used to stabilize the calculation and are usually set to 0.01 and 0.03, respectively.
    $\sigma_{\hat{I}}$ and $\sigma_{\tilde{I}}$ denote the standard deviations, and
    $\sigma_{\hat{I}\tilde{I}}$ denotes the covariance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $u_{\hat{I}}$ 和 $u_{\tilde{I}}$ 分别表示图像 $\hat{I}$ 和 $\tilde{I}$ 的均值。$k_{1}$
    和 $k_{2}$ 是常数，用于稳定计算，通常设为 0.01 和 0.03。$\sigma_{\hat{I}}$ 和 $\sigma_{\tilde{I}}$
    表示标准差，$\sigma_{\hat{I}\tilde{I}}$ 表示协方差。
- en: 3 Video Super-resolution Methods
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 视频超分辨率方法
- en: 'As the videos are a recording of moving visual images and sound, the methods
    for video super-resolution learn from existing single image super-resolution methods.
    There are many deep learning based image super-resolution methods such as Super-Resolution
    using deep Convolutional Neural Networks (SRCNN) (Dong et al., [2014](#bib.bib19)),
    Fast Super-Resolution Convolutional Neural Networks (FSRCNN) (Dong et al., [2016](#bib.bib20)),
    VDSR (Kim et al., [2016](#bib.bib60)), Efficient Sub-Pixel Convolutional neural
    Network (ESPCN) (Shi et al., [2016](#bib.bib105)), Residual Dense Network (RDN) (Zhang
    et al., [2018](#bib.bib140)), Residual Channel Attention Network (RCAN) (Zhang
    et al., [2018b](#bib.bib139)), “Zero-Shot” Super-Resolution (ZSSR) (Shocher et al.,
    [2018](#bib.bib107)) and Super-Resolution using a Generative Adversarial Network
    (SRGAN) (Ledig et al., [2017](#bib.bib67)). In 2016, based on SRCNN, Kappeler (Kappeler
    et al., [2016](#bib.bib59)) presented a video super-resolution method with convolutional
    neural networks (VSRnet). So far, many video super-resolution algorithms have
    been proposed. In the following, we summarize the characteristics of the deep
    learning based methods for video super-resolution in recent years, as shown in
    Table [1](#S3.T1 "Table 1 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '由于视频是移动视觉图像和声音的录制，视频超分辨率的方法借鉴了现有的单图像超分辨率方法。许多基于深度学习的图像超分辨率方法如使用深度卷积神经网络的超分辨率（SRCNN）（Dong
    et al., [2014](#bib.bib19)），快速超分辨率卷积神经网络（FSRCNN）（Dong et al., [2016](#bib.bib20)），VDSR（Kim
    et al., [2016](#bib.bib60)），高效子像素卷积神经网络（ESPCN）（Shi et al., [2016](#bib.bib105)），残差密集网络（RDN）（Zhang
    et al., [2018](#bib.bib140)），残差通道注意网络（RCAN）（Zhang et al., [2018b](#bib.bib139)），“零样本”超分辨率（ZSSR）（Shocher
    et al., [2018](#bib.bib107)）以及使用生成对抗网络的超分辨率（SRGAN）（Ledig et al., [2017](#bib.bib67)）。2016年，基于SRCNN，Kappeler（Kappeler
    et al., [2016](#bib.bib59)）提出了一种基于卷积神经网络的视频超分辨率方法（VSRnet）。至今，已经提出了许多视频超分辨率算法。接下来，我们总结了近年来基于深度学习的视频超分辨率方法的特点，如表[1](#S3.T1
    "Table 1 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution Based on
    Deep Learning: A Comprehensive Survey")所示。'
- en: '![Refer to caption](img/dd5ff663da6247785c972c041c10f012.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dd5ff663da6247785c972c041c10f012.png)'
- en: 'Figure 2: A taxonomy for existing state-of-the-art video super-resolution methods.
    Here, $MEMC$ stands for motion estimation and compensation methods, $DC$ is deformable
    convolution methods, $3D\ Conv$ is 3D convolution methods, and $RCNN$ denotes
    recurrent convolutional neural network based methods. The links to these methods
    are as follows. *MEMC*: Deep-DE (Liao et al., [2015](#bib.bib76)),VSRnet (Kappeler
    et al., [2016](#bib.bib59)), VESPCN (Caballero et al., [2017](#bib.bib5)), DRVSR (Tao
    et al., [2017](#bib.bib113)), RVSR (Liu et al., [2017](#bib.bib80)), FRVSR (Sajjadi
    et al., [2018](#bib.bib102)), STTN (Kim et al., [2018a](#bib.bib62)), SOFVSR (Wang
    et al., [2019](#bib.bib119)), TOFlow (Xue et al., [2019](#bib.bib130)), MMCNN (Wang
    et al., [2019b](#bib.bib123)), MEMC-Net (Bao et al., [2021](#bib.bib1)), RRCN (Li
    et al., [2019](#bib.bib70)), RTVSR (Bare et al., [2019](#bib.bib2)), MultiBoot
    VSR (Kalarot and Porikli, [2019](#bib.bib58)), TecoGAN (Chu et al., [2020](#bib.bib12)),
    MuCAN (Li et al., [2020](#bib.bib73)), BasicVSR (Chan et al., [2021b](#bib.bib7)).
    *DC*: EDVR (Wang et al., [2019a](#bib.bib122)), DNLN (Wang et al., [2019](#bib.bib118)),
    TDAN (Tian et al., [2020](#bib.bib114)), D3Dnet (Ying et al., [2020](#bib.bib134)),
    VESR-Net (Chen et al., [2020](#bib.bib10)). *2D Conv*:VSRResFeatGAN (Lucas et al.,
    [2019](#bib.bib87)), FFCVSR (Yan et al., [2019](#bib.bib131)). *3D Conv*: DUF (Jo
    et al., [2018](#bib.bib57)), FSTRN (Li et al., [2019a](#bib.bib72)), 3DSRnet (Kim
    et al., [2019](#bib.bib61)), DSMC (Liu et al., [2021a](#bib.bib83)). *RCNN*: BRCN (Huang
    et al., [2015](#bib.bib41), [2018](#bib.bib42)), STCN (Guo and Chao, [2017](#bib.bib31)),
    RISTN (Zhu et al., [2019](#bib.bib142)), RLSP (Fuoli et al., [2019a](#bib.bib25)),
    RSDN (Isobe et al., [2020](#bib.bib50)). *Non-Local*: PFNL (Yi et al., [2019](#bib.bib133)).
    *Other*: RBPN (Haris et al., [2019](#bib.bib34)), STARnet (Haris et al., [2020](#bib.bib35)),
    DNSTNet (Sun et al., [2020](#bib.bib111)).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：现有最先进的视频超分辨率方法的分类法。这里，$MEMC$ 代表运动估计和补偿方法，$DC$ 是可变形卷积方法，$3D\ Conv$ 是 3D 卷积方法，$RCNN$
    表示基于递归卷积神经网络的方法。这些方法的链接如下。 *MEMC*：Deep-DE (Liao et al., [2015](#bib.bib76))，VSRnet
    (Kappeler et al., [2016](#bib.bib59))，VESPCN (Caballero et al., [2017](#bib.bib5))，DRVSR
    (Tao et al., [2017](#bib.bib113))，RVSR (Liu et al., [2017](#bib.bib80))，FRVSR
    (Sajjadi et al., [2018](#bib.bib102))，STTN (Kim et al., [2018a](#bib.bib62))，SOFVSR
    (Wang et al., [2019](#bib.bib119))，TOFlow (Xue et al., [2019](#bib.bib130))，MMCNN
    (Wang et al., [2019b](#bib.bib123))，MEMC-Net (Bao et al., [2021](#bib.bib1))，RRCN
    (Li et al., [2019](#bib.bib70))，RTVSR (Bare et al., [2019](#bib.bib2))，MultiBoot
    VSR (Kalarot and Porikli, [2019](#bib.bib58))，TecoGAN (Chu et al., [2020](#bib.bib12))，MuCAN
    (Li et al., [2020](#bib.bib73))，BasicVSR (Chan et al., [2021b](#bib.bib7))。*DC*：EDVR
    (Wang et al., [2019a](#bib.bib122))，DNLN (Wang et al., [2019](#bib.bib118))，TDAN
    (Tian et al., [2020](#bib.bib114))，D3Dnet (Ying et al., [2020](#bib.bib134))，VESR-Net
    (Chen et al., [2020](#bib.bib10))。*2D Conv*：VSRResFeatGAN (Lucas et al., [2019](#bib.bib87))，FFCVSR
    (Yan et al., [2019](#bib.bib131))。*3D Conv*：DUF (Jo et al., [2018](#bib.bib57))，FSTRN
    (Li et al., [2019a](#bib.bib72))，3DSRnet (Kim et al., [2019](#bib.bib61))，DSMC
    (Liu et al., [2021a](#bib.bib83))。*RCNN*：BRCN (Huang et al., [2015](#bib.bib41)，[2018](#bib.bib42))，STCN
    (Guo and Chao, [2017](#bib.bib31))，RISTN (Zhu et al., [2019](#bib.bib142))，RLSP
    (Fuoli et al., [2019a](#bib.bib25))，RSDN (Isobe et al., [2020](#bib.bib50))。*Non-Local*：PFNL
    (Yi et al., [2019](#bib.bib133))。*Other*：RBPN (Haris et al., [2019](#bib.bib34))，STARnet
    (Haris et al., [2020](#bib.bib35))，DNSTNet (Sun et al., [2020](#bib.bib111))。
- en: 'Several recent studies such as (Wang et al., [2019a](#bib.bib122); Jo et al.,
    [2018](#bib.bib57); Tian et al., [2020](#bib.bib114)) on video super-resolution
    tasks have indicated that the utilization of the information contained in frames
    greatly influences performance. The proper and adequate usage of such information
    can enhance the results of video super-resolution. Therefore, we build a taxonomy
    for existing video super-resolution methods according to their ways of the utilization
    of inter-frame information, as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution
    Methods ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的一些研究，如 (Wang et al., [2019a](#bib.bib122); Jo et al., [2018](#bib.bib57);
    Tian et al., [2020](#bib.bib114))，在视频超分辨率任务中表明，利用帧中包含的信息对性能有很大影响。恰当和充分地使用这些信息可以提升视频超分辨率的结果。因此，我们根据现有视频超分辨率方法对帧间信息的利用方式构建了一个分类法，如图
    [2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey") 所示。'
- en: 'Table 1: Existing video super-resolution methods based on deep learning and
    their key strategies such as loss functions (see their source papers for the details
    of the loss functions). Here, $MEMC$ denotes motion estimation and motion compensation,
    $DC$ is deformable convolution, $3D\ Conv$ is 3D convolution, and $RCNN$ denotes
    recurrent convolutional neural networks.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有的基于深度学习的视频超分辨率方法及其关键策略，如损失函数（有关损失函数的详细信息，请参见源论文）。这里，$MEMC$ 表示运动估计和运动补偿，$DC$
    是可变形卷积，$3D\ Conv$ 是 3D 卷积，$RCNN$ 表示递归卷积神经网络。
- en: '| Method | Year | Synonym | Type | Loss function | ​Align​ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 同义词 | 类型 | 损失函数 | 对齐 |'
- en: '| Deep-DE (Liao et al., [2015](#bib.bib76)) | ICCV 2015 | Deep Draft-Ensemble
    Learning | MEMC | $\ell_{1}$-norm loss with total variation regularization | $\checkmark$
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Deep-DE (廖等, [2015](#bib.bib76)) | ICCV 2015 | 深度草稿集成学习 | MEMC | $\ell_{1}$-范数损失与全变差正则化
    | $\checkmark$ |'
- en: '| VSRnet (Kappeler et al., [2016](#bib.bib59)) | TCI 2016 | Video Super-Resolution
    with convolutional neural Networks | Mean Square Error (MSE) loss | $\checkmark$
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| VSRnet (卡佩勒等, [2016](#bib.bib59)) | TCI 2016 | 卷积神经网络的视频超分辨率 | 均方误差 (MSE)
    损失 | $\checkmark$ |'
- en: '| VESPCN (Caballero et al., [2017](#bib.bib5)) | CVPR 2017 | Video Efficient
    Sub-pixel Convolutional Network | MSE loss and Motion Compensation (MC) loss |
    $\checkmark$ |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| VESPCN (卡巴列罗等, [2017](#bib.bib5)) | CVPR 2017 | 视频高效亚像素卷积网络 | MSE 损失和运动补偿
    (MC) 损失 | $\checkmark$ |'
- en: '| DRVSR (Tao et al., [2017](#bib.bib113)) | ICCV 2017 | Detail-Revealing deep
    Video Super-Resolution | MSE loss and MC loss | $\checkmark$ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| DRVSR (陶等, [2017](#bib.bib113)) | ICCV 2017 | 细节揭示深度视频超分辨率 | MSE 损失和 MC 损失
    | $\checkmark$ |'
- en: '| RVSR (Liu et al., [2017](#bib.bib80)) | ICCV 2017 | Robust Video Super-Resolution
    | Spatial alignment loss and spatio-temporal adaptive loss | $\checkmark$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| RVSR (刘等, [2017](#bib.bib80)) | ICCV 2017 | 稳健的视频超分辨率 | 空间对齐损失和时空自适应损失 |
    $\checkmark$ |'
- en: '| FRVSR (Sajjadi et al., [2018](#bib.bib102)) | CVPR 2018 | Frame-Recurrent
    Video Super-Resolution | MSE loss and MC loss | $\checkmark$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| FRVSR (萨贾迪等, [2018](#bib.bib102)) | CVPR 2018 | 帧递归视频超分辨率 | MSE 损失和 MC 损失
    | $\checkmark$ |'
- en: '| STTN (Kim et al., [2018a](#bib.bib62)) | ECCV 2018 | Spatio-Temporal Transformer
    Network | MSE loss and MC loss | $\checkmark$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| STTN (金等, [2018a](#bib.bib62)) | ECCV 2018 | 时空变换网络 | MSE 损失和 MC 损失 | $\checkmark$
    |'
- en: '| SOFVSR (Wang et al., [2019](#bib.bib119)) | ACCV 2018 | Super-resolution
    Optical Flow for Video Super-Resolution | MSE loss and MC loss | $\checkmark$
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| SOFVSR (王等, [2019](#bib.bib119)) | ACCV 2018 | 超分辨率光流用于视频超分辨率 | MSE 损失和 MC
    损失 | $\checkmark$ |'
- en: '| TOFlow (Xue et al., [2019](#bib.bib130)) | IJCV 2019 | video enhancement
    with Task-Oriented Flow | $\ell_{1}$-norm loss | $\checkmark$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| TOFlow (薛等, [2019](#bib.bib130)) | IJCV 2019 | 任务导向流的视频增强 | $\ell_{1}$-范数损失
    | $\checkmark$ |'
- en: '| MMCNN (Wang et al., [2019b](#bib.bib123)) | TIP 2019 | Multi-Memory Convolutional
    Neural Network | MSE loss and MC loss | $\checkmark$ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| MMCNN (王等, [2019b](#bib.bib123)) | TIP 2019 | 多记忆卷积神经网络 | MSE 损失和 MC 损失 |
    $\checkmark$ |'
- en: '| MEMC-Net (Bao et al., [2021](#bib.bib1)) | TPAMI 2019 | Motion Estimation
    and Motion Compensation Network | Charbonnier (Cb) loss | $\checkmark$ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| MEMC-Net (包等, [2021](#bib.bib1)) | TPAMI 2019 | 运动估计与运动补偿网络 | Charbonnier
    (Cb) 损失 | $\checkmark$ |'
- en: '| RRCN (Li et al., [2019](#bib.bib70)) | TIP 2019 | Residual Recurrent Convolutional
    Network | MSE loss | $\checkmark$ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| RRCN (李等, [2019](#bib.bib70)) | TIP 2019 | 残差递归卷积网络 | MSE 损失 | $\checkmark$
    |'
- en: '| RTVSR (Bare et al., [2019](#bib.bib2)) | Neurocomp. 2019 | Real-Time Video
    Super-Resolution | MSE loss | $\checkmark$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| RTVSR (贝尔等, [2019](#bib.bib2)) | Neurocomp. 2019 | 实时视频超分辨率 | MSE 损失 | $\checkmark$
    |'
- en: '| ​​MultiBoot VSR (Kalarot and Porikli, [2019](#bib.bib58))​​ | CVPRW 2019
    | Multi-stage multi-reference Bootstrapping for Video Super-Resolution | Huber
    loss | $\checkmark$ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ​​MultiBoot VSR (卡拉罗特和波利基, [2019](#bib.bib58))​​ | CVPRW 2019 | 多阶段多参考自举用于视频超分辨率
    | Huber 损失 | $\checkmark$ |'
- en: '| MuCAN (Li et al., [2020](#bib.bib73)) | ECCV 2020 | Multi-Correspondence
    Aggregation Network for Video Super-Resolution | Edge-aware loss | $\checkmark$
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| MuCAN (李等, [2020](#bib.bib73)) | ECCV 2020 | 多对应聚合网络用于视频超分辨率 | 边缘感知损失 | $\checkmark$
    |'
- en: '| TecoGAN (Chu et al., [2020](#bib.bib12)) | ACMTOG 2020 | Temporally coherent
    GAN | MSE loss and ping-pong loss etc. | $\checkmark$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| TecoGAN (朱等, [2020](#bib.bib12)) | ACMTOG 2020 | 时序一致生成对抗网络 | MSE 损失和乒乓损失等
    | $\checkmark$ |'
- en: '| BasicVSR (Chan et al., [2021b](#bib.bib7)) | CVPR 2021 | search for essential
    components in Video Super-Resolution and beyond | Cb loss | $\checkmark$ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| BasicVSR (陈等, [2021b](#bib.bib7)) | CVPR 2021 | 在视频超分辨率及其应用中寻找关键组件 | Cb 损失
    | $\checkmark$ |'
- en: '| EDVR (Wang et al., [2019a](#bib.bib122)) | CVPRW 2019 | Enhanced Deformable
    convolutional networks for Video Restoration | DC | Cb loss | $\checkmark$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| EDVR （Wang等， [2019a](#bib.bib122)） | CVPRW 2019 | 增强的可变形卷积网络用于视频修复 | DC |
    Cb 损失 | $\checkmark$ |'
- en: '| DNLN (Wang et al., [2019](#bib.bib118)) | ACCESS 2019 | Deformable Non-Local
    Network for Video Super-Resolution | $\ell_{1}$-norm loss | $\checkmark$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| DNLN （Wang等， [2019](#bib.bib118)） | ACCESS 2019 | 用于视频超分辨率的可变形非局部网络 | $\ell_{1}$-范数损失
    | $\checkmark$ |'
- en: '| TDAN (Tian et al., [2020](#bib.bib114)) | CVPR 2020 | Temporally-Deformable
    Alignment Network for Video Super-Resolution | $\ell_{1}$-norm loss | $\checkmark$
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| TDAN （Tian等， [2020](#bib.bib114)） | CVPR 2020 | 用于视频超分辨率的时间可变形对齐网络 | $\ell_{1}$-范数损失
    | $\checkmark$ |'
- en: '| D3Dnet (Ying et al., [2020](#bib.bib134)) | SPL 2020 | Deformable 3D Convolution
    for Video Super-Resolution | MSE loss | $\checkmark$ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| D3Dnet （Ying等， [2020](#bib.bib134)） | SPL 2020 | 用于视频超分辨率的可变形3D卷积 | MSE 损失
    | $\checkmark$ |'
- en: '| VESR-Net (Chen et al., [2020](#bib.bib10)) | ArXiv 2020 | Video Enhancement
    and Super-Resolution Network | $\ell_{1}$-norm loss | $\checkmark$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| VESR-Net （Chen等， [2020](#bib.bib10)） | ArXiv 2020 | 视频增强和超分辨率网络 | $\ell_{1}$-范数损失
    | $\checkmark$ |'
- en: '| ​VSRResFeatGAN (Lucas et al., [2019](#bib.bib87))​ | TIP 2019 | Video Super-Resolution
    with Residual Networks | 2D Conv | Adversarial loss; content loss; and perceptual
    loss | $\times$ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ​VSRResFeatGAN （Lucas等， [2019](#bib.bib87)）​ | TIP 2019 | 使用残差网络的视频超分辨率 |
    2D 卷积 | 对抗损失；内容损失；和感知损失 | $\times$ |'
- en: '| FFCVSR (Yan et al., [2019](#bib.bib131)) | AAAI 2019 | Frame and Feature-Context
    Video Super-Resolution | MSE loss | $\times$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FFCVSR （Yan等， [2019](#bib.bib131)） | AAAI 2019 | 帧和特征上下文视频超分辨率 | MSE 损失 |
    $\times$ |'
- en: '| DUF (Jo et al., [2018](#bib.bib57)) | CVPR 2018 | video super-resolution
    network using Dynamic Upsampling Filters | 3D Conv | Huber loss | $\times$ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| DUF （Jo等， [2018](#bib.bib57)） | CVPR 2018 | 使用动态上采样滤波器的视频超分辨率网络 | 3D 卷积 |
    Huber 损失 | $\times$ |'
- en: '| FSTRN (Li et al., [2019a](#bib.bib72)) | CVPR 2019 | Fast Spatio-Temporal
    Residual Network for Video Super-Resolution | Cb loss | $\times$ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| FSTRN （Li等， [2019a](#bib.bib72)） | CVPR 2019 | 用于视频超分辨率的快速时空残差网络 | Cb 损失
    | $\times$ |'
- en: '| 3DSRnet (Kim et al., [2019](#bib.bib61)) | ICIP 2019 | 3D Super-Resolution
    Network | MSE loss | $\times$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3DSRnet （Kim等， [2019](#bib.bib61)） | ICIP 2019 | 3D 超分辨率网络 | MSE 损失 | $\times$
    |'
- en: '| DSMC (Liu et al., [2021a](#bib.bib83)) | AAAI 2021 | Dual Subnet and Multi-stage
    Communicated upsampling | Cb loss; perceptual loss; the dual loss | $\times$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| DSMC （Liu等， [2021a](#bib.bib83)） | AAAI 2021 | 双子网络和多阶段通信上采样 | Cb 损失；感知损失；双重损失
    | $\times$ |'
- en: '| ​​​BRCN (Huang et al., [2015](#bib.bib41), [2018](#bib.bib42))​​​ | ​NIPS
    2015/2018​ | video super-resolution via Bidirectional Recurrent Convolutional
    Networks | MSE loss | $\times$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ​​​BRCN （Huang等， [2015](#bib.bib41)， [2018](#bib.bib42)）​​​ | ​NIPS 2015/2018​
    | 通过双向递归卷积网络的视频超分辨率 | MSE 损失 | $\times$ |'
- en: '| STCN (Guo and Chao, [2017](#bib.bib31)) | AAAI 2017 | Spatio-Temporal Convolutional
    Network for Video Super-Resolution | RCNN | MSE loss | $\times$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| STCN （Guo 和 Chao， [2017](#bib.bib31)） | AAAI 2017 | 用于视频超分辨率的时空卷积网络 | RCNN
    | MSE 损失 | $\times$ |'
- en: '| RISTN (Zhu et al., [2019](#bib.bib142)) | AAAI 2019 | Residual Invertible
    Spatio-Temporal Network for Video Super-Resolution | MSE loss | $\times$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| RISTN （Zhu等， [2019](#bib.bib142)） | AAAI 2019 | 用于视频超分辨率的残差可逆时空网络 | MSE 损失
    | $\times$ |'
- en: '| RLSP (Fuoli et al., [2019a](#bib.bib25)) | ​ICCVW 2019​ | video super-resolution
    through Recurrent Latent Space Propagation | MSE loss | $\times$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| RLSP （Fuoli等， [2019a](#bib.bib25)） | ​ICCVW 2019​ | 通过递归潜在空间传播的视频超分辨率 | MSE
    损失 | $\times$ |'
- en: '| RSDN (Isobe et al., [2020](#bib.bib50)) | ​ECCV 2020​ | video super-resolution
    with Recurrent Structure-Detail Network | Cb loss | $\times$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RSDN （Isobe等， [2020](#bib.bib50)） | ​ECCV 2020​ | 使用递归结构-细节网络的视频超分辨率 | Cb
    损失 | $\times$ |'
- en: '| PFNL (Yi et al., [2019](#bib.bib133)) | ICCV 2019 | Progressive Fusion network
    via exploiting Non-Local spatio-temporal correlations | ​Non-Local​ | Cb loss
    | $\times$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| PFNL （Yi等， [2019](#bib.bib133)） | ICCV 2019 | 通过利用非局部时空相关性的渐进融合网络 | ​非局部​
    | Cb 损失 | $\times$ |'
- en: '| RBPN (Haris et al., [2019](#bib.bib34)) | CVPR 2019 | Recurrent Back-Projection
    Network |  | $\ell_{1}$-norm loss | $\times$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| RBPN （Haris等， [2019](#bib.bib34)） | CVPR 2019 | 递归反投影网络 |  | $\ell_{1}$-范数损失
    | $\times$ |'
- en: '| STARnet (Haris et al., [2020](#bib.bib35)) | CVPR 2020 | Space-Time-Aware
    multi-Resolution network |  | Three losses | $\times$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| STARnet （Haris等， [2020](#bib.bib35)） | CVPR 2020 | 时空感知的多分辨率网络 |  | 三种损失
    | $\times$ |'
- en: '| DNSTNet (Sun et al., [2020](#bib.bib111)) | Neurocomp. 2020 | video super-resolution
    via Dense Non-local Spatial-Temporal convolutional Network | ​Other ​ | $\ell_{1}$-norm
    loss | $\times$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DNSTNet（Sun et al., [2020](#bib.bib111)） | Neurocomp. 2020 | 通过稠密非局部时空卷积网络进行视频超分辨率
    | 其他 | $\ell_{1}$-范数损失 | $\times$ |'
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution Methods ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") and Table
    [1](#S3.T1 "Table 1 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), we categorize the existing methods
    into two main categorises: methods with alignment and methods without alignment,
    according to whether the video frames are explicitly aligned. We will present
    the methods in detail in the following sections.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S3.F2 "Figure 2 ‣ 3 Video Super-resolution Methods ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey")和表[1](#S3.T1 "Table 1 ‣ 3 Video
    Super-resolution Methods ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey")所示，我们将现有方法分为两个主要类别：对齐方法和无对齐方法，根据视频帧是否明确对齐。我们将在以下章节中详细介绍这些方法。'
- en: Since all the methods are classified according to whether the frames are explicitly
    aligned and what techniques they are mainly used for alignment, other modules
    which they utilized for feature extraction, fusion, and reconstruction are ignored.
    These modules may be employed by multiple methods simultaneously. Therefore, some
    of the methods in our study are coupled. BasicVSR in the MEMC methods from the
    category of the methods with alignment adopts a typical bidirectional recurrent
    convolutional neural network (RCNN) as backbone. While the RCNN-based methods
    (e.g. BRCN, STCN, and RISTN), which are in the methods without alignment, mainly
    use RCNN to learn features. Similarly, VESR-Net in the DC category also uses a
    non-local block for feature learning as that of PFNL in the non-local category.
    Moreover, DSMC in the category of 3D convolution also utilizes a non-local block
    for global correlation computation. The category of ’other’ includes the methods
    which adopt optical flow but without frame alignment, e.g., RBPN, and STARnet.
    Finally, the learned offsets by deformable convolution share similar patterns
    as those from the optical flow-based methods, and the deformable and flow-based
    alignments are strongly correlated. This was indicated in the work (Chan et al.,
    [2021c](#bib.bib8)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有方法是根据帧是否明确对齐以及主要用于对齐的技术进行分类的，其他用于特征提取、融合和重建的模块被忽略。这些模块可能被多个方法同时使用。因此，我们研究中的一些方法是耦合的。MEMC方法中的BasicVSR属于对齐方法类别，采用典型的双向递归卷积神经网络（RCNN）作为骨干网络。而在没有对齐的方法中，如RCNN基础的方法（例如BRCN、STCN和RISTN），主要使用RCNN进行特征学习。类似地，DC类别中的VESR-Net也使用非局部块进行特征学习，就像非局部类别中的PFNL一样。此外，3D卷积类别中的DSMC也利用非局部块进行全局相关性计算。‘其他’类别包括那些采用光流但没有帧对齐的方法，例如RBPN和STARnet。最后，由可变形卷积学习到的偏移量与光流方法的模式相似，并且可变形和光流对齐之间有强关联。这在工作（Chan
    et al., [2021c](#bib.bib8)）中有所指出。
- en: Moreover, we have observed several trends in these recently proposed methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到这些新提出的方法中出现了一些趋势。
- en: 1) The diversification of methods. In the early years (2015-2017), most of the
    methods use frame alignment for VSR. Then since 2018, many different methods,
    especially which are the methods without alignment, have emerged, e.g., FFCVSR,
    DUF, RISTN, and PLNL. Some studies also indicate that both the methods with alignment
    and those without alignment can obtain sound performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 方法的多样化。在早期（2015-2017年），大多数方法使用帧对齐进行视频超分辨率。然后，从2018年起，出现了许多不同的方法，特别是无对齐的方法，例如FFCVSR、DUF、RISTN和PLNL。一些研究还表明，无论是对齐方法还是无对齐方法，都能获得良好的性能。
- en: 2) The expansion of receptive field in methods. Earlier methods such as EDVR
    and RBPN mainly utilize certain numbers of input frames in sliding-window, while
    the subsequent methods resort to longer sequences. For example, BasicVSR employs
    bidirectional RCNN, by which the features are propagated forward and backward
    independently. Moreover, the non-local subnetwork, such as in the PFNL method,
    aims to compute the correlations between all possible pixels within and across
    frames. These indicate that the methods tend to capture longer-range dependencies
    in the video sequences, and they expand the receptive field in the network from
    local to global.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 方法中感受野的扩展。早期的方法如EDVR和RBPN主要利用滑动窗口中的某些输入帧，而后续的方法则采用更长的序列。例如，BasicVSR采用双向RCNN，通过该方法，特征可以前向和后向独立传播。此外，像PFNL方法中的非局部子网络旨在计算所有可能像素之间的相关性。这些都表明这些方法倾向于捕捉视频序列中的长程依赖，并将网络中的感受野从局部扩展到全球。
- en: 3) In the MEMC methods such as FRVSR, STTN, SOFVSR, TecoGAN, and MuCAN, most
    of them adopt deep learning techniques for estimation the optical flow, since
    the deep learning may have adaptive ability for various data than the conventional
    methods. 4) The practicality of methods. As the requirements for super-resolving
    of higher quality videos develop, the recently proposed methods also become more
    practicable. The test videos evolve from Vid4 and UVGD, to REDS. All the discussions
    indicate that we will mainly focus on the methods for the videos with more complex
    motions and scene changes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 在MEMC方法中，如FRVSR、STTN、SOFVSR、TecoGAN和MuCAN，大多数采用深度学习技术来估计光流，因为深度学习可能比传统方法更具适应性。4)
    方法的实用性。随着对高质量视频超分辨率要求的提高，最近提出的方法也变得更加实用。测试视频从Vid4和UVGD发展到REDS。所有讨论表明，我们将主要关注具有更复杂运动和场景变化的视频的方法。
- en: 4 Methods with Alignment
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 对齐方法
- en: The methods with alignment make neighboring frames explicitly align with the
    target frame by using extracted motion information before subsequent reconstruction.
    These methods mainly use motion estimation and motion compensation (MEMC) or deformable
    convolution, which are two common techniques for aligning frames. Next we will
    introduce state-of-the-art methods based on each of the techniques in detail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐方法通过使用提取的运动信息使邻近帧明确地与目标帧对齐，然后进行后续重建。这些方法主要使用运动估计和运动补偿（MEMC）或可变形卷积，这两种是对齐帧的常见技术。接下来我们将详细介绍基于每种技术的最新方法。
- en: '![Refer to caption](img/4a6376b4e096b8da941e8d5e15fbd06f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a6376b4e096b8da941e8d5e15fbd06f.png)'
- en: (a) A target frame
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 目标帧
- en: '![Refer to caption](img/da4f1f77afee85d2356143a00ef93281.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da4f1f77afee85d2356143a00ef93281.png)'
- en: (b) Its neighboring frame
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 它的邻近帧
- en: '![Refer to caption](img/42691625067f73dcab2cf9def54436cd.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42691625067f73dcab2cf9def54436cd.png)'
- en: (c) The compensated image
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 补偿后的图像
- en: '![Refer to caption](img/2e4ca2043033c7ac28bf752b76eabb61.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2e4ca2043033c7ac28bf752b76eabb61.png)'
- en: (d) The estimated optical flow image
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 估计的光流图像
- en: 'Figure 3: An example of motion estimation and compensation. Note that the small
    rightmost image is the legend of (d). Different colors represent different directions
    of motion and the intensity of the color is the range of motion.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：运动估计和补偿的示例。请注意，最右侧的小图像是(d)的图例。不同的颜色表示不同的运动方向，颜色的强度表示运动范围。
- en: 4.1 Motion Estimation and Compensation Methods
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 运动估计和补偿方法
- en: 'In the methods with alignment for video super-resolution, most of them apply
    the motion estimation and motion compensation techniques. Specifically, the purpose
    of motion estimation is to extract inter-frame motion information, while motion
    compensation is used to perform the warping operation between frames according
    to inter-frame motion information and to make one frame align with another frame.
    A majority of the motion estimation techniques are performed by the optical flow
    method (Dosovitskiy et al., [2015](#bib.bib21)). This method tries to calculate
    the motion between two neighboring frames through their correlations and variations
    in the temporal domain. The motion estimation methods can be divided into two
    categories: traditional methods (e.g., (Lucas and Kanade, [1981](#bib.bib88))
    and (Drulea and Nedevschi, [2011](#bib.bib22))) and deep learning methods such
    as FlowNet (Dosovitskiy et al., [2015](#bib.bib21)), FlowNet 2.0 (Ilg et al.,
    [2017](#bib.bib47)) and SpyNet (Ranjan and Black, [2017](#bib.bib99)).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频超分辨率的对齐方法中，大多数应用了运动估计和运动补偿技术。具体来说，运动估计的目的是提取帧间运动信息，而运动补偿则用于根据帧间运动信息在帧之间执行变形操作，并使一个帧与另一个帧对齐。大多数运动估计技术通过光流方法实现
    (Dosovitskiy 等，[2015](#bib.bib21))。该方法试图通过帧间的相关性和时间域的变化来计算两个相邻帧之间的运动。运动估计方法可以分为两类：传统方法（例如，（Lucas
    和 Kanade，[1981](#bib.bib88)）和（Drulea 和 Nedevschi，[2011](#bib.bib22)））以及深度学习方法，如
    FlowNet (Dosovitskiy 等，[2015](#bib.bib21))、FlowNet 2.0 (Ilg 等，[2017](#bib.bib47))
    和 SpyNet (Ranjan 和 Black，[2017](#bib.bib99))。
- en: 'In general, an optical flow method takes two frames (e.g., $I_{i}$ and $I_{j}$)
    as inputs. One is the target frame and the other is the neighboring frame. Then
    the method computes a vector field of optical flow $F_{i\rightarrow{j}}$ from
    the frame $I_{i}$ to $I_{j}$ by the following formula:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，光流方法以两帧（例如 $I_{i}$ 和 $I_{j}$）作为输入。一个是目标帧，另一个是相邻帧。然后该方法通过以下公式计算从帧 $I_{i}$
    到 $I_{j}$ 的光流矢量场 $F_{i\rightarrow{j}}$：
- en: '|  | $F_{i\rightarrow{j}}(h_{i\rightarrow{j}},v_{i\rightarrow{j}})=ME(I_{i},I_{j};\theta_{ME})$
    |  | (7) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{i\rightarrow{j}}(h_{i\rightarrow{j}},v_{i\rightarrow{j}})=ME(I_{i},I_{j};\theta_{ME})$
    |  | (7) |'
- en: where $h_{i\rightarrow{j}}$ and $v_{i\rightarrow{j}}$ is the horizontal and
    vertical components of $F_{i\rightarrow{j}}$, $ME(\cdot)$ is a function used to
    compute optical flow, and $\theta_{ME}$ is its parameter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{i\rightarrow{j}}$ 和 $v_{i\rightarrow{j}}$ 是 $F_{i\rightarrow{j}}$ 的水平和垂直分量，$ME(\cdot)$
    是计算光流的函数，而 $\theta_{ME}$ 是其参数。
- en: 'The motion compensation is used to perform image transformation between images
    in terms of motion information to make neighboring frames align with the target
    frame. In general, a compensated frame $I_{j}^{\prime}$ is expressed as:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运动补偿用于根据运动信息在图像之间执行图像变换，以使相邻帧与目标帧对齐。一般来说，补偿后的帧 $I_{j}^{\prime}$ 表达为：
- en: '|  | $I_{j}^{\prime}=MC(I_{i},F_{i\rightarrow{j}};\theta_{MC})$ |  | (8) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{j}^{\prime}=MC(I_{i},F_{i\rightarrow{j}};\theta_{MC})$ |  | (8) |'
- en: 'where $MC(\cdot)$ is a motion compensation function, $I_{i}$, $F_{i\rightarrow{j}}$
    and $\theta_{MC}$ are the neighboring frame, optical flow and the parameter. MC
    can be achieved by some methods such as bilinear interpolation and spatial transformer
    network (STN) (Jaderberg et al., [2015](#bib.bib52)). An example of motion estimation
    and motion compensation is shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Methods with
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $MC(\cdot)$ 是运动补偿函数，$I_{i}$、$F_{i\rightarrow{j}}$ 和 $\theta_{MC}$ 分别表示相邻帧、光流和参数。MC
    可以通过一些方法实现，如双线性插值和空间变换网络（STN） (Jaderberg 等，[2015](#bib.bib52))。运动估计和运动补偿的示例如图
    [3](#S4.F3 "图 3 ‣ 4 方法与对齐 ‣ 基于深度学习的视频超分辨率：全面调查")所示。
- en: 'Both the ME and MC processes can be conducted by a deep learning method or
    traditional one (non-deep learning). According to the technique that applies to
    ME or MC is traditional or deep learning, we further divide the MEMC methods into
    two subcategories. If any of the processes in ME or MC utilizes a deep neural
    network, then the method falls into the deep learning category, otherwise the
    method belongs to the traditional one. Therefore, the traditional methods in the
    MEMC methods comprise of the following three ones: Deep-DE (Liao et al., [2015](#bib.bib76)),
    VSRNet (Kappeler et al., [2016](#bib.bib59)), and RRCN (Li et al., [2019](#bib.bib70)).
    The other MEMC methods are included in the deep learning subcategory. Below we
    depict some representative methods in detail.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ME和MC过程可以通过深度学习方法或传统方法（非深度学习）进行。根据应用于ME或MC的技术是传统的还是深度学习的，我们进一步将MEMC方法分为两类。如果ME或MC中的任何过程利用了深度神经网络，那么该方法属于深度学习类别，否则该方法属于传统类别。因此，MEMC方法中的传统方法包括以下三种：Deep-DE（Liao
    et al., [2015](#bib.bib76)）、VSRNet（Kappeler et al., [2016](#bib.bib59)）和RRCN（Li
    et al., [2019](#bib.bib70)）。其他MEMC方法则被归入深度学习子类别。下面我们详细描述一些代表性方法。
- en: '![Refer to caption](img/4766a2f51bb259672dbae3abf08c353a.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4766a2f51bb259672dbae3abf08c353a.png)'
- en: 'Figure 4: The architecture of Deep-DE (Liao et al., [2015](#bib.bib76)). Here
    Motion Estim. is a motion estimation block, Motion Comp. is a motion compensation
    block, Conv is a convolutional layer and Deconv is a deconvolutional layer.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: Deep-DE的结构（Liao et al., [2015](#bib.bib76)）。这里Motion Est.为运动估计模块，Motion
    Comp.为运动补偿模块，Conv为卷积层，Deconv为反卷积层。'
- en: 4.1.1 Deep-DE
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 Deep-DE
- en: 'The deep draft-ensemble learning method (Deep-DE)¹¹1Code: http://www.cse.cuhk.edu.hk/leojia/projects/DeepSR/
    (Liao et al., [2015](#bib.bib76)) has two phases, as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). It
    first generates a series of SR drafts by adjusting the TV-$\ell_{1}$ flow (Brox
    et al., [2004](#bib.bib3); Guo and Chao, [2017](#bib.bib31)) and the motion detail
    preserving (MDP) (Xu et al., [2012](#bib.bib129)). Then both the SR drafts and
    the bicubic-interpolated LR target frame are fed into a CNN for feature extraction,
    fusion and super-resolution.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '深度草稿集成学习方法（Deep-DE）¹¹1代码: [http://www.cse.cuhk.edu.hk/leojia/projects/DeepSR/](http://www.cse.cuhk.edu.hk/leojia/projects/DeepSR/)（Liao
    et al., [2015](#bib.bib76)）包含两个阶段，如图[4](#S4.F4 "图 4 ‣ 4.1 运动估计和补偿方法 ‣ 4 对齐方法 ‣
    基于深度学习的视频超分辨率: 综合调查")所示。它首先通过调整TV-$\ell_{1}$流（Brox et al., [2004](#bib.bib3);
    Guo and Chao, [2017](#bib.bib31)）和运动细节保留（MDP）（Xu et al., [2012](#bib.bib129)）生成一系列超分辨率草稿。然后，将超分辨率草稿和双三次插值的低分辨率目标帧输入到卷积神经网络（CNN）中进行特征提取、融合和超分辨率处理。'
- en: 'The CNN in Deep-DE consists of four convolutional layers: the first three layers
    are general convolutional layers, and the last layer is a deconvolution layer.
    Their kernel sizes are 11$\times$11, 1$\times$1, 3$\times$3 and 25$\times$25,
    respectively, and the numbers of channels are 256, 512, 1 and 1.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Deep-DE中的CNN包含四个卷积层：前三个层为普通卷积层，最后一个层为反卷积层。它们的核大小分别为11$\times$11、1$\times$1、3$\times$3和25$\times$25，通道数分别为256、512、1和1。
- en: '![Refer to caption](img/66c60eff91b67c156dec04dbe9d5f600.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66c60eff91b67c156dec04dbe9d5f600.png)'
- en: 'Figure 5: The network architecture of VSRnet (Kappeler et al., [2016](#bib.bib59)).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: VSRnet的网络结构（Kappeler et al., [2016](#bib.bib59)）。'
- en: 4.1.2 VSRnet
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 VSRnet
- en: 'VSRnet²²2Code: https://superresolution.tf.fau.de/ (Kappeler et al., [2016](#bib.bib59))
    is based on the image super-resolution method, SRCNN (Dong et al., [2014](#bib.bib19)),
    and its network architecture is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.1 Deep-DE
    ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). VSRnet
    mainly consists of motion estimation and compensation modules, and three convolutional
    layers, and each convolutional layer is followed by a rectified linear unit (ReLU)
    except for the last one. The main difference between VSRnet and SRCNN is the number
    of input frames. That is, SRCNN takes a single frame as input, while VSRnet uses
    multiple successive frames, which are compensated frames. The motion information
    between frames is computed by the Druleas algorithm (Drulea and Nedevschi, [2011](#bib.bib22)).
    In addition, VSRnet proposes a filter symmetry enforcement (FSE) mechanism and
    an adaptive motion compensation mechanism, which are separately used to accelerate
    training and reduce the impact of unreliable compensated frames, and thus can
    improve video super-resolution performance.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'VSRnet²²2Code: https://superresolution.tf.fau.de/ (Kappeler et al., [2016](#bib.bib59))
    基于图像超分辨率方法 SRCNN (Dong et al., [2014](#bib.bib19))，其网络结构如图 [5](#S4.F5 "Figure
    5 ‣ 4.1.1 Deep-DE ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods
    with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey") 所示。VSRnet 主要由运动估计和补偿模块以及三个卷积层组成，每个卷积层后面跟一个修正线性单元（ReLU），最后一个卷积层除外。VSRnet
    和 SRCNN 的主要区别在于输入帧的数量，即 SRCNN 以单帧为输入，而 VSRnet 使用多个连续帧作为输入，这些帧是补偿帧。帧间的运动信息通过 Druleas
    算法（Drulea 和 Nedevschi, [2011](#bib.bib22)）计算。此外，VSRnet 提出了滤波对称性强制（FSE）机制和自适应运动补偿机制，这两者分别用于加速训练和减少不可靠补偿帧的影响，从而提高视频超分辨率性能。'
- en: '![Refer to caption](img/b12d86b68af0e4a3968f833e4330d977.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![请参考标题](img/b12d86b68af0e4a3968f833e4330d977.png)'
- en: 'Figure 6: The network architecture of RRCN (Li et al., [2019](#bib.bib70)).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：RRCN 的网络结构（Li et al., [2019](#bib.bib70)）。
- en: 4.1.3 RRCN
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 RRCN
- en: 'The residual recurrent convolutional network (RRCN) (Li et al., [2019](#bib.bib70)),
    as shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1.2 VSRnet ‣ 4.1 Motion Estimation and
    Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey"), is a bidirectional recurrent neural
    network, which learns a residual image. RRCN proposes an unsynchronized full recurrent
    convolutional network, where unsynchronization refers to the input of multiple
    consecutive video frames, and only the middle one is super-resolved.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '残差递归卷积网络（RRCN）（Li et al., [2019](#bib.bib70)），如图 [6](#S4.F6 "Figure 6 ‣ 4.1.2
    VSRnet ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") 所示，是一种双向递归神经网络，用于学习残差图像。RRCN
    提出了一个不同步的全递归卷积网络，其中不同步指的是输入多个连续的视频帧，而仅对中间帧进行超分辨率处理。'
- en: RRCN uses the combined local-global with total variable (GLG-TV) method (Drulea
    and Nedevschi, [2011](#bib.bib22)) to perform motion estimation and compensation
    for the target frame and its adjacent frames. The compensated frames are used
    as input to the network. The forward convolution and recurrent convolution are
    conducted in the forward network and the backward network, respectively, and their
    outputs are summed up. Finally, the result is obtained by adding the target frame
    to the input. In order to further improve the performance, RRCN also uses the
    self-ensemble strategy and combines it with the output of the single image super-resolution
    method, EDSR+ (Lim et al., [2017](#bib.bib77)), to obtain two models named RRCN+
    and RRCN++, respectively.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: RRCN 使用局部-全局与总变量（GLG-TV）方法（Drulea 和 Nedevschi, [2011](#bib.bib22)）对目标帧及其相邻帧进行运动估计和补偿。补偿后的帧作为输入传递到网络中。前向卷积和递归卷积分别在前向网络和后向网络中进行，其输出被加总。最后，通过将目标帧添加到输入中来获得结果。为了进一步提高性能，RRCN
    还使用了自我集成策略，并将其与单图像超分辨率方法 EDSR+（Lim et al., [2017](#bib.bib77)）的输出结合，得到两个模型，分别为
    RRCN+ 和 RRCN++。
- en: '![Refer to caption](img/95f148fa0ac7003f6e2165f9a1ee0bd4.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![请参考标题](img/95f148fa0ac7003f6e2165f9a1ee0bd4.png)'
- en: 'Figure 7: The network architecture of VESPCN (Caballero et al., [2017](#bib.bib5)).
    Here $\oplus$ denotes element-wise sum.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：VESPCN 的网络结构（Caballero et al., [2017](#bib.bib5)）。这里 $\oplus$ 表示元素级加法。
- en: 4.1.4 VESPCN
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 VESPCN
- en: 'The video efficient sub-pixel convolutional network (VESPCN) (Caballero et al.,
    [2017](#bib.bib5)) proposes a spatial motion compensation transformer (MCT) module
    for motion estimation and compensation. Then the compensated frames are fed into
    a series of convolutional layers for feature extraction and fusion, as shown in
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.1.3 RRCN ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). Finally, the super-resolution results are obtained through
    a sub-pixel convolutional layer for upsampling.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 视频高效子像素卷积网络（VESPCN）（Caballero 等，[2017](#bib.bib5)）提出了一种空间运动补偿变换器（MCT）模块用于运动估计和补偿。然后，将补偿后的帧输入一系列卷积层进行特征提取和融合，如图
    [7](#S4.F7 "图 7 ‣ 4.1.3 RRCN ‣ 4.1 运动估计和补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查") 所示。最后，通过子像素卷积层进行上采样以获得超分辨率结果。
- en: The MCT module adopts CNNs to extract motion information and perform motion
    compensation. MCT uses a coarse-to-fine approach to compute the optical flow for
    image sequences. Firstly, in the coarse estimation stage, the network takes two
    consecutive frames (i.e., the target frame and a neighboring frame) as inputs.
    The coarse network consists of 5 convolutional layers and a sub-pixel convolutional
    layer. And it first performs the $\times$2 downsampling operation two times and
    then performs the $\times$4 upsampling operation by a sub-pixel convolutional
    layer to get coarse optical flow estimation results. Secondly, the neighboring
    frame is warped according to the optical flow. In the fine estimation stage, the
    target frame, neighboring frame, optical flow computed in the coarse stage and
    the warped neighboring frame are the input of the fine network, whose architecture
    is similar to the coarse network. It first conducts $\times$2 downsampling and
    then perform $\times$2 upsampling at the end of the network to attain the fine
    optical flow. Together with the coarse optical flow, the fine optical flow is
    used to obtain the final estimation result. Finally, the neighboring frame is
    warped again by the final optical flow to make the warped frame align with the
    target frame.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MCT 模块采用 CNN 提取运动信息并进行运动补偿。MCT 使用从粗到细的方法计算图像序列的光流。首先，在粗估计阶段，网络以两个连续帧（即目标帧和相邻帧）作为输入。粗网络由
    5 层卷积层和一层子像素卷积层组成。它首先进行 $\times$2 下采样操作两次，然后通过子像素卷积层进行 $\times$4 上采样操作，以获得粗光流估计结果。其次，根据光流对相邻帧进行扭曲。在细估计阶段，目标帧、相邻帧、在粗阶段计算的光流以及扭曲的相邻帧作为细网络的输入，其架构类似于粗网络。它首先进行
    $\times$2 下采样，然后在网络末尾进行 $\times$2 上采样，以获得细光流。结合粗光流，细光流用于获得最终估计结果。最后，通过最终光流再次扭曲相邻帧，使扭曲的帧与目标帧对齐。
- en: '![Refer to caption](img/e264dc6fce8ec776df5677ecaf0f02c0.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e264dc6fce8ec776df5677ecaf0f02c0.png)'
- en: 'Figure 8: The network architecture of DRVSR (Tao et al., [2017](#bib.bib113)).
    Here SPMC denotes a sub-pixel motion compensation layer, ConvLSTM is the convolutional
    LSTM (Shi et al., [2015](#bib.bib106)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: DRVSR 的网络架构（Tao 等，[2017](#bib.bib113)）。这里 SPMC 表示子像素运动补偿层，ConvLSTM 是卷积
    LSTM（Shi 等，[2015](#bib.bib106)）。'
- en: 4.1.5 DRVSR
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 DRVSR
- en: 'The detail-revealing deep video super-resolution (DRVSR)³³3Code: https://github.com/jiangsutx/SPMC_VideoSR
    (Tao et al., [2017](#bib.bib113)) method proposes a sub-pixel motion compensation
    layer (SPMC) that can perform the up-sampling and motion compensation operations
    simultaneously for neighboring input frames according to the estimated optical
    flow information. The network architecture of DRVSR is illustrated in Fig. [8](#S4.F8
    "Figure 8 ‣ 4.1.4 VESPCN ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4
    Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '细节揭示深度视频超分辨率（DRVSR）³³3代码: https://github.com/jiangsutx/SPMC_VideoSR (Tao 等，[2017](#bib.bib113))
    方法提出了一种子像素运动补偿层（SPMC），该层可以根据估计的光流信息同时对相邻输入帧进行上采样和运动补偿操作。DRVSR 的网络架构如图 [8](#S4.F8
    "图 8 ‣ 4.1.4 VESPCN ‣ 4.1 运动估计和补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查") 所示。'
- en: 'DRVSR consists of three main modules: a motion estimation module, a motion
    compensation module using the SPMC layer, and a fusion module. The motion estimation
    module is implemented by the motion compensation transformer (MCT) network (Caballero
    et al., [2017](#bib.bib5)). The SPMC layer consists of two sub-modules, namely
    grid generator and sampler. The grid generator first transforms the coordinates
    in the LR space into the coordinates in the HR space according to the optical
    flow, and then the sampler performs the interpolation operation in the HR space.
    In the fusion module, it applies the convolution with stride 2 to perform down-sampling
    and then conducts the deconvolution for up-sampling to obtain the HR residual
    image of the target frame. Together with the upsampled LR target frame, this residual
    image yields the final result. DRVSR also adopts the ConvLSTM module (Shi et al.,
    [2015](#bib.bib106)) to handle spatio-temporal information.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DRVSR由三个主要模块组成：运动估计模块、使用SPMC层的运动补偿模块和融合模块。运动估计模块由运动补偿变换器（MCT）网络（Caballero等，[2017](#bib.bib5)）实现。SPMC层由两个子模块组成，即网格生成器和采样器。网格生成器根据光流将LR空间中的坐标转换为HR空间中的坐标，然后采样器在HR空间中执行插值操作。在融合模块中，首先应用步幅为2的卷积进行下采样，然后进行反卷积以进行上采样，从而获得目标帧的HR残差图像。与上采样的LR目标帧一起，这个残差图像产生最终结果。DRVSR还采用了ConvLSTM模块（Shi等，[2015](#bib.bib106)）来处理时空信息。
- en: '![Refer to caption](img/a3fb0488d91fbe05e292495593f074e7.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a3fb0488d91fbe05e292495593f074e7.png)'
- en: 'Figure 9: The network architecture of RVSR (Liu et al., [2017](#bib.bib80)),
    where SR denotes Super-Resolution.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：RVSR（Liu等，[2017](#bib.bib80)）的网络架构，其中SR表示超分辨率。
- en: 4.1.6 RVSR
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.6 RVSR
- en: 'Robust video super-resolution (RVSR) (Liu et al., [2017](#bib.bib80)) proposes
    a spatial alignment module to attain great alignment performance and a temporal
    adaptive module to adaptively determine the optimal scale of temporal dependency.
    And its architecture is shown in Fig. [9](#S4.F9 "Figure 9 ‣ 4.1.5 DRVSR ‣ 4.1
    Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 强健的视频超分辨率（RVSR）（Liu等，[2017](#bib.bib80)）提出了一个空间对齐模块，以实现出色的对齐性能，并提出了一个时间自适应模块，以自适应地确定时间依赖的最佳尺度。其架构如图[9](#S4.F9
    "图 9 ‣ 4.1.5 DRVSR ‣ 4.1 运动估计与补偿方法 ‣ 4 方法与对齐 ‣ 基于深度学习的视频超分辨率：综合调查")所示。
- en: The spatial alignment module is responsible for the alignment of the multi-frames
    so that the neighboring frames are aligned with the target frame. It first estimates
    the transformation parameters between the neighboring frame and the target frame
    through a localization net, and then makes the neighboring frame align with the
    target frame through a spatial transformation layer (Jaderberg et al., [2015](#bib.bib52))
    based on the obtained parameters. The localization net consists of two convolutional
    layers, each of which is followed by a max-pooling layer, and two fully connected
    layers. The temporal adaptive module is composed of multiple branches of SR subnetwork
    and a temporal modulation. Each subnetwork is responsible for handling a temporal
    scale (i.e., the number of input frames), and outputting the corresponding super-resolution
    result. Then the super-resolution result of each subnetwork is allocated a weight
    through the temporal modulation. The final super-resolution result is the weight
    sum of the super-resolution result of each branch and its weight. The number of
    the input frames of the temporal modulation module is identical to the maximum
    number of input frames in the super-resolution network, and the network structure
    of the temporal modulation module is the same as that of the super-resolution
    network, and both of them are based on the structure of ESPCN (Shi et al., [2016](#bib.bib105)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 空间对齐模块负责多帧的对齐，使相邻的帧与目标帧对齐。它首先通过定位网络估计相邻帧与目标帧之间的变换参数，然后通过空间变换层（Jaderberg 等， [2015](#bib.bib52)）根据获得的参数使相邻帧与目标帧对齐。定位网络由两层卷积层组成，每层卷积层后跟一个最大池化层，以及两层全连接层。时间自适应模块由多个
    SR 子网络分支和一个时间调制组成。每个子网络负责处理一个时间尺度（即输入帧的数量），并输出相应的超分辨率结果。然后，通过时间调制为每个子网络的超分辨率结果分配权重。最终的超分辨率结果是每个分支的超分辨率结果及其权重的加权和。时间调制模块的输入帧数量与超分辨率网络中的最大输入帧数量相同，时间调制模块的网络结构与超分辨率网络的网络结构相同，两者都基于
    ESPCN 的结构（Shi 等，[2016](#bib.bib105)）。
- en: '![Refer to caption](img/8d0828dc6b515da2146905361d1cbf47.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/8d0828dc6b515da2146905361d1cbf47.png)'
- en: 'Figure 10: The network architecture of FRVSR (Sajjadi et al., [2018](#bib.bib102)).
    Here FlowNet is an optical flow estimation module, and SR Module is a super-resolution
    module.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：FRVSR 的网络架构（Sajjadi 等，[2018](#bib.bib102)）。这里 FlowNet 是一个光流估计模块，SR 模块是一个超分辨率模块。
- en: 4.1.7 FRVSR
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.7 FRVSR
- en: 'Frame recurrent video super-resolution (FRVSR)⁴⁴4Code: https://github.com/msmsajjadi/FRVSR
    (Sajjadi et al., [2018](#bib.bib102)) mainly proposes to use the previously inferred
    HR estimate to super-resolve the subsequent frame for producing temporally consistent
    results and reducing computational cost. The architecture of FRVSR is illustrated
    in Fig. [10](#S4.F10 "Figure 10 ‣ 4.1.6 RVSR ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey").'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '帧递归视频超分辨率（FRVSR）⁴⁴4代码: https://github.com/msmsajjadi/FRVSR（Sajjadi 等，[2018](#bib.bib102)）主要提出使用先前推断的高分辨率估计来对后续帧进行超分辨率处理，以产生时间一致的结果并减少计算成本。FRVSR
    的架构如图 [10](#S4.F10 "Figure 10 ‣ 4.1.6 RVSR ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey") 所示。'
- en: The detailed implementation adopts an optical estimation network to compute
    the optical flow from the previous frame to the target frame. Then the LR optical
    flow is upsampled to the same size with the HR video by bilinear interpolation.
    The HR variant of the previous frame is warped by the upsampled LR optical flow,
    and then the warped HR frame is downsampled by space-to-depth transformation to
    get the LR version. Finally, the LR variant of the warped HR frame and the target
    frame are fed into the subsequent super-resolution network to attain the result
    for the target frame. In FRVSR, the optical flow network consists of 14 convolutional
    layers, 3 pooling layers and 3 bilinear upsampling layers. Each convolutional
    layer is followed by a LeakyReLU activation function, except for the last convolutional
    layer. The super-resolution network consists of 2 convolutional layers, 2 deconvolution
    layers with $\times$2 and 10 residual blocks, where each residual block consists
    of 2 convolutional layers and a ReLU activation function.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的实现采用了光学估计网络来计算从前一帧到目标帧的光流。然后通过双线性插值将低分辨率（LR）光流上采样到与高分辨率（HR）视频相同的尺寸。前一帧的HR变体通过上采样的LR光流进行扭曲，然后通过空间到深度转换将扭曲后的HR帧下采样得到LR版本。最后，将扭曲后的HR帧的LR变体和目标帧输入到后续的超分辨率网络中，以获得目标帧的结果。在FRVSR中，光流网络包括14个卷积层、3个池化层和3个双线性上采样层。每个卷积层后面跟随一个LeakyReLU激活函数，最后一个卷积层除外。超分辨率网络包括2个卷积层、2个$\times$2的反卷积层和10个残差块，每个残差块包含2个卷积层和一个ReLU激活函数。
- en: '![Refer to caption](img/0ab06da37118ace2b6c5f736625a1b45.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0ab06da37118ace2b6c5f736625a1b45.png)'
- en: 'Figure 11: The network architecture of STTN (Kim et al., [2018a](#bib.bib62)).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：STTN的网络架构（Kim et al., [2018a](#bib.bib62)）。
- en: 4.1.8 STTN
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.8 STTN
- en: 'Spatio-temporal transformer network (STTN) (Kim et al., [2018a](#bib.bib62))
    proposes a spatio-temporal transformer module, which is used to address the problem
    that previous optical flow methods only process a pair of video frames, which
    may cause inaccurate estimation when occlusion and luminance variation exist in
    videos. The proposed module can handle multiple frames at a time. The architecture
    of STTN is illustrated in Fig. [11](#S4.F11 "Figure 11 ‣ 4.1.7 FRVSR ‣ 4.1 Motion
    Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 时空变换网络（STTN）（Kim et al., [2018a](#bib.bib62)）提出了一个时空变换模块，用于解决以往光流方法仅处理一对视频帧的问题，这在视频中存在遮挡和亮度变化时可能导致估计不准确。所提模块可以同时处理多帧。STTN的架构如图[11](#S4.F11
    "图11 ‣ 4.1.7 FRVSR ‣ 4.1 运动估计与补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综述")所示。
- en: 'STTN consists of three major modules: a spatio-temporal flow estimation module,
    a spatio-temporal sampler module, and a super-resolution module. The first module
    is a U-style network, similar to U-Net (Ronneberger et al., [2015](#bib.bib101)),
    consisting of 12 convolutional layers and two up-sampling layers. It first performs
    $\times 4$ downsampling, and then $\times 4$ up-sampling to restore the size of
    the input frames. This module is responsible for optical flow estimation of the
    consecutive input frames including the target frame and multiple neighboring frames,
    and the final output is a 3-channel spatio-temporal flow that expresses the spatial
    and temporal changes between frames. The spatio-temporal sampler module is actually
    a trilinear interpolation method, which is responsible for performing warp operation
    for current multiple neighboring frames and obtaining the aligned video frames
    according to the spatio-temporal flow obtained by the spatio-temporal flow module.
    For video super-resolution, the aligned frames can then be fed into the super-resolution
    (SR) module for feature fusion and super-resolution of the target frame.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: STTN包括三个主要模块：时空流估计模块、时空采样器模块和超分辨率模块。第一个模块是一个U型网络，类似于U-Net（Ronneberger et al.,
    [2015](#bib.bib101)），包含12个卷积层和两个上采样层。它首先进行$\times 4$下采样，然后$\times 4$上采样以恢复输入帧的尺寸。该模块负责连续输入帧（包括目标帧和多个邻近帧）的光流估计，最终输出一个3通道的时空流，表示帧之间的空间和时间变化。时空采样器模块实际上是一种三线性插值方法，负责对当前多个邻近帧进行扭曲操作，并根据时空流模块获得的时空流获取对齐的视频帧。对于视频超分辨率，对齐的帧可以输入到超分辨率（SR）模块中进行特征融合和目标帧的超分辨率处理。
- en: '![Refer to caption](img/992281c305f048b5cdf218b75fecd0e6.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/992281c305f048b5cdf218b75fecd0e6.png)'
- en: 'Figure 12: The network architecture of SOFVSR (Wang et al., [2019](#bib.bib119)).
    Here, OFRnet is an optical flow network, and SRnet is a super-resolution module.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：SOFVSR的网络架构（Wang et al., [2019](#bib.bib119)）。这里，OFRnet是一个光流网络，SRnet是一个超分辨率模块。
- en: 4.1.9 SOFVSR
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.9 SOFVSR
- en: 'Super-resolution optical flow for video super-resolution tasks (SOFVSR)⁵⁵5Code:
    https://github.com/LongguangWang/SOF-VSR (Wang et al., [2019](#bib.bib119)) is
    proposed to super-resolve LR estimated optical flow for attaining great SR performance,
    and its architecture is shown in Fig. [12](#S4.F12 "Figure 12 ‣ 4.1.8 STTN ‣ 4.1
    Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey").'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 针对视频超分辨率任务的超分辨率光流（SOFVSR）⁵⁵5代码： https://github.com/LongguangWang/SOF-VSR（Wang
    et al., [2019](#bib.bib119)）旨在超分辨率LR估计的光流，以实现出色的SR性能，其架构如图[12](#S4.F12 "图12 ‣
    4.1.8 STTN ‣ 4.1 运动估计与补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查")所示。
- en: The optical flow between frames is estimated by a coarse-to-fine approach including
    the optical flow reconstruction network (OFRnet), which finally yields a high-resolution
    optical flow. Then the HR optical flow is converted to the LR optical flow by
    a space-to-depth transformation. The neighboring frames are warped by the LR optical
    flow to make the neighboring frames align with the target frame. Then the super-resolution
    network (SRnet) takes the target frame and warped frames as inputs to obtain the
    final super-resolution result. SRnet consists of two convolutional layers, five
    residual dense blocks and a sub-pixel convolutional layer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 帧间的光流通过粗到细的方法进行估计，包括光流重建网络（OFRnet），最终生成高分辨率光流。然后通过空间到深度的转换将HR光流转换为LR光流。通过LR光流对邻近帧进行变形，以使邻近帧与目标帧对齐。然后，超分辨率网络（SRnet）将目标帧和变形帧作为输入，获得最终的超分辨率结果。SRnet由两个卷积层、五个残差密集块和一个子像素卷积层组成。
- en: '![Refer to caption](img/8d93af6d9331a9df7156e7f37371d073.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d93af6d9331a9df7156e7f37371d073.png)'
- en: 'Figure 13: The network architecture of TOFlow (Xue et al., [2019](#bib.bib130)).
    Here O.F.Estimat. is the optical flow estimation, STN is a spatial transformer
    network.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：TOFlow的网络架构（Xue et al., [2019](#bib.bib130)）。这里O.F.Estimat.是光流估计，STN是空间变换网络。
- en: 4.1.10 TOFlow
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.10 TOFlow
- en: 'The architecture of the task-oriented flow (TOFlow)⁶⁶6Code: https://github.com/anchen1011/toflow
    (Xue et al., [2019](#bib.bib130)) is shown in Fig. [13](#S4.F13 "Figure 13 ‣ 4.1.9
    SOFVSR ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). TOFlow
    combines the network for optical flow estimation with the reconstruction network,
    and trains them jointly to obtain optical flow network tailored to a specific
    task such as video SR, video interpolation and video deblurring.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向光流（TOFlow）⁶⁶6代码： https://github.com/anchen1011/toflow（Xue et al., [2019](#bib.bib130)）的架构如图[13](#S4.F13
    "图13 ‣ 4.1.9 SOFVSR ‣ 4.1 运动估计与补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查")所示。TOFlow将光流估计网络与重建网络相结合，并共同训练以获得针对特定任务（如视频SR、视频插值和视频去模糊）量身定制的光流网络。
- en: TOFlow adopts SpyNet (Ranjan and Black, [2017](#bib.bib99)) as the network for
    the optical flow estimation, and then adopts a spatial transformer network (STN)
    to warp the neighboring frame according to the computed optical flow. Then the
    final result is obtained by an image processing network. For the video super-resolution
    task, the image processing module consists of 4 convolutional layers, where kernel
    sizes are 9$\times$9, 9$\times$9, 1$\times$1, and 1$\times$1, respectively, and
    the numbers of channels are 64, 64, 64, and 3, respectively.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: TOFlow采用了SpyNet（Ranjan and Black, [2017](#bib.bib99)）作为光流估计网络，然后使用空间变换网络（STN）根据计算出的光流对邻近帧进行变形。最终结果通过图像处理网络得到。对于视频超分辨率任务，图像处理模块包含4个卷积层，其中核尺寸分别为9$\times$9、9$\times$9、1$\times$1和1$\times$1，通道数分别为64、64、64和3。
- en: '![Refer to caption](img/92e78cdd3216c6c02acc593d963a5062.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/92e78cdd3216c6c02acc593d963a5062.png)'
- en: 'Figure 14: The network architecture of MMCNN (Wang et al., [2019b](#bib.bib123)).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：MMCNN的网络架构（Wang et al., [2019b](#bib.bib123)）。
- en: 4.1.11 MMCNN
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.11 MMCNN
- en: 'The architecture of the multi-memory convolutional neural network (MMCNN)⁷⁷7Code:
    https://github.com/psychopa4/MMCNN (Wang et al., [2019b](#bib.bib123)) is shown
    in Fig. [14](#S4.F14 "Figure 14 ‣ 4.1.10 TOFlow ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"), and it consists of 5 major modules: optical flow module
    for motion estimation and motion compensation, feature extraction, multi-memory
    detail fusion, feature reconstruction, and upsample modules, where the last module
    uses a sub-pixel convolutional layer.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '多记忆卷积神经网络（**MMCNN**）⁷⁷7Code: https://github.com/psychopa4/MMCNN（Wang等，[2019b](#bib.bib123)）的架构如图[14](#S4.F14
    "Figure 14 ‣ 4.1.10 TOFlow ‣ 4.1 Motion Estimation and Compensation Methods ‣
    4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey")所示，包含5个主要模块：用于运动估计和运动补偿的光流模块、特征提取、多记忆细节融合、特征重建和上采样模块，其中最后一个模块使用了子像素卷积层。'
- en: Consecutive input frames are first processed by the optical flow estimation
    module to make neighboring frames align with the target frame and then the warped
    frames are fed into subsequent network modules to attain the residual image of
    the target frame. Finally, this residual image is added into the upsampled LR
    target frame, which is computed by bicubic interpolation, to obtain the super-resolution
    result. In the multi-memory detail fusion module, MMCNN adopts the ConvLSTM module
    (Shi et al., [2015](#bib.bib106)) to merge the spatio-temporal information. Moreover,
    the feature extraction, detail fusion, and feature reconstruction modules are
    all built based on residual dense blocks (Zhang et al., [2018](#bib.bib140); Huang
    et al., [2017](#bib.bib40)), where the key difference among them is merely the
    type of network layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 连续输入帧首先由光流估计模块处理，使邻近帧与目标帧对齐，然后将变形后的帧输入到后续的网络模块中，以获得目标帧的残差图像。最后，将这个残差图像添加到通过双三次插值计算出的上采样LR目标帧中，以获得超分辨率结果。在多记忆细节融合模块中，**MMCNN**采用了ConvLSTM模块（Shi等，[2015](#bib.bib106)）来融合时空信息。此外，特征提取、细节融合和特征重建模块都基于残差密集块（Zhang等，[2018](#bib.bib140)；Huang等，[2017](#bib.bib40)），它们之间的主要区别仅在于网络层的类型。
- en: 4.1.12 MEMC-Net
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.12 **MEMC-Net**
- en: 'The motion estimation and motion compensation network (MEMC-Net)⁸⁸8Code: https://github.com/baowenbo/MEMC-Net
    (Bao et al., [2021](#bib.bib1)), as shown in Fig. [15](#S4.F15 "Figure 15 ‣ 4.1.12
    MEMC-Net ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"), mainly
    proposes an adaptive warping layer.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '运动估计和运动补偿网络（**MEMC-Net**）⁸⁸8Code: https://github.com/baowenbo/MEMC-Net（Bao等，[2021](#bib.bib1)），如图[15](#S4.F15
    "Figure 15 ‣ 4.1.12 MEMC-Net ‣ 4.1 Motion Estimation and Compensation Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey")所示，主要提出了一种自适应变形层。'
- en: '![Refer to caption](img/bb015776aa62c7e5876668e5051a8052.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/bb015776aa62c7e5876668e5051a8052.png)'
- en: 'Figure 15: The network architecture of MEMC-Net (Bao et al., [2021](#bib.bib1)).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：**MEMC-Net**的网络架构（Bao等，[2021](#bib.bib1)）。
- en: The adaptive warping layer warps the neighboring frame through the estimated
    optical flow and the convolutional kernel, which are resulted from a motion estimation
    network and a kernel estimation network, respectively, and aligns the neighboring
    frame with the target frame. The motion estimation network adopts FlowNet (Dosovitskiy
    et al., [2015](#bib.bib21)), and the kernel estimation network uses an improved
    U-Net (Ronneberger et al., [2015](#bib.bib101)) including five max-pooling layers,
    five un-pooling layers and skip connections from the encoder to the decoder. In
    MEMC-Net, the architecture of the super-resolution module, namely frame enhancement
    module, is similar to that of EDSR (Lim et al., [2017](#bib.bib77)). In order
    to deal with the occlusion problem, it adopts a pre-trained ResNet18 (He et al.,
    [2016](#bib.bib36)) to extract the feature of input frames. Moreover, it feeds
    the output of the first convolutional layer of ResNet18 as the context information
    into the adaptive warping layer to perform the same operation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应扭曲层通过估计的光流和卷积核扭曲相邻帧，这些光流和卷积核分别来自运动估计网络和卷积核估计网络，并使相邻帧与目标帧对齐。运动估计网络采用FlowNet（Dosovitskiy
    et al., [2015](#bib.bib21)），而卷积核估计网络使用改进的U-Net（Ronneberger et al., [2015](#bib.bib101)），包括五个最大池化层、五个反池化层以及从编码器到解码器的跳跃连接。在MEMC-Net中，超分辨率模块，即帧增强模块，其架构类似于EDSR（Lim
    et al., [2017](#bib.bib77)）。为了解决遮挡问题，它采用了预训练的ResNet18（He et al., [2016](#bib.bib36)）来提取输入帧的特征。此外，它将ResNet18第一个卷积层的输出作为上下文信息输入自适应扭曲层，以执行相同的操作。
- en: '![Refer to caption](img/351014f704225170e39c220de2cc5340.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/351014f704225170e39c220de2cc5340.png)'
- en: 'Figure 16: The network architecture of RTVSR (Bare et al., [2019](#bib.bib2)).
    Here SR Module denotes super-resolution module.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：RTVSR的网络结构（Bare et al., [2019](#bib.bib2)）。这里的SR模块表示超分辨率模块。
- en: 4.1.13 RTVSR
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.13 RTVSR
- en: 'The real-time video super-resolution (RTVSR) (Bare et al., [2019](#bib.bib2)),
    as shown in Fig. [16](#S4.F16 "Figure 16 ‣ 4.1.12 MEMC-Net ‣ 4.1 Motion Estimation
    and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey"), adopts a convolutional network called
    motion convolutional kernel estimation network, which is a full convolution codec
    structure, to estimate the motion between the target frame and the neighboring
    frame and produce a pair of 1D convolutional kernel corresponding to the current
    target frame and neighboring frame. Then the neighboring frame is warped by using
    estimated convolutional kernels to make it align with the target frame. RTVSR
    designs an important component called gated enhance units (GEUs) to learn useful
    features, which is an improved variant based on (Li et al., [2018](#bib.bib71)).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '实时视频超分辨率（RTVSR）（Bare et al., [2019](#bib.bib2)），如图[16](#S4.F16 "Figure 16 ‣
    4.1.12 MEMC-Net ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")所示，采用了一种称为运动卷积核估计网络的卷积网络，这是一种全卷积编解码器结构，用于估计目标帧和相邻帧之间的运动，并生成一对1D卷积核，分别对应当前目标帧和相邻帧。然后，通过使用估计的卷积核对相邻帧进行扭曲，以使其与目标帧对齐。RTVSR设计了一个名为门控增强单元（GEUs）的重要组件，用于学习有用的特征，这是基于（Li
    et al., [2018](#bib.bib71)）的改进变体。'
- en: '![Refer to caption](img/2ae6ae539c0ae5fbd9f020cb36400fdd.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/2ae6ae539c0ae5fbd9f020cb36400fdd.png)'
- en: 'Figure 17: The network architecture of MultiBoot VSR (Kalarot and Porikli,
    [2019](#bib.bib58)).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：MultiBoot VSR的网络结构（Kalarot 和 Porikli，[2019](#bib.bib58)）。
- en: 4.1.14 MultiBoot VSR
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.14 MultiBoot VSR
- en: 'The multi-stage multi-reference bootstrapping for video super-resolution (MultiBoot
    VSR) (Kalarot and Porikli, [2019](#bib.bib58)) consists of two stages. That is,
    in order to further improve performance, the output of the first stage is used
    as the input of the second stage. The network architecture of MultiBoot VSR is
    shown in Fig. [17](#S4.F17 "Figure 17 ‣ 4.1.13 RTVSR ‣ 4.1 Motion Estimation and
    Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based
    on Deep Learning: A Comprehensive Survey").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '视频超分辨率的多阶段多参考自助法（MultiBoot VSR）（Kalarot 和 Porikli，[2019](#bib.bib58)）由两个阶段组成。即，为了进一步提高性能，第一个阶段的输出被用作第二个阶段的输入。MultiBoot
    VSR的网络结构如图[17](#S4.F17 "Figure 17 ‣ 4.1.13 RTVSR ‣ 4.1 Motion Estimation and Compensation
    Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey")所示。'
- en: The LR frames are input to the FlowNet 2.0 to compute optical flow and perform
    the motion compensation. Then the processed frames are fed into the first-stage
    network to attain the super-resolution result of the target frame. In the second
    stage of MultiBoot VSR, the output from the previous stage is downsampled, concatenated
    with the initial LR frame, and then input to the network to obtain final super-resolution
    result for the target frame.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 将低分辨率（LR）帧输入 FlowNet 2.0，以计算光流并执行运动补偿。然后，将处理后的帧输入到第一阶段网络，以获得目标帧的超分辨率结果。在 MultiBoot
    VSR 的第二阶段中，将上一阶段的输出下采样，与初始 LR 帧连接，然后输入到网络中，以获得目标帧的最终超分辨率结果。
- en: '![Refer to caption](img/67138a5e70b4b5b1279cad638260b250.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/67138a5e70b4b5b1279cad638260b250.png)'
- en: 'Figure 18: The network architecture of MuCAN (Li et al., [2020](#bib.bib73)).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18: MuCAN 的网络架构（Li 等人，[2020](#bib.bib73)）。'
- en: 4.1.15 MuCAN
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.15 MuCAN
- en: 'The architecture of multi-correspondence aggregation network (MuCAN) (Li et al.,
    [2020](#bib.bib73)) is shown in Fig. [18](#S4.F18 "Figure 18 ‣ 4.1.14 MultiBoot
    VSR ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods with Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). MuCAN
    is an end-to-end network consisting of a temporal multi-correspondence aggregation
    module (TM-CAM), a cross-scale non-local-correspondence aggregation module (CN-CAM),
    and a reconstruction module.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '多对应聚合网络（MuCAN）（Li 等人，[2020](#bib.bib73)）的架构如图 [18](#S4.F18 "图 18 ‣ 4.1.14 MultiBoot
    VSR ‣ 4.1 运动估计与补偿方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率: 综合调查") 所示。MuCAN 是一个端到端的网络，包括一个时间多对应聚合模块（TM-CAM）、一个跨尺度非局部对应聚合模块（CN-CAM）和一个重建模块。'
- en: In TM-CAM, two neighboring LR frames are first encoded into lower-resolution
    features to be more stable and robust to noise. Then the aggregation starts in
    the original LR feature space by an aggregation unit (AU) to compensate large
    motion while progressively moving up to low-level/high-resolution stages for subtle
    sub-pixel shift. In a single AU, a patch-based matching strategy is used since
    it naturally contains structural information. Multiple candidates are then aggregated
    to obtain sufficient context information. The aggregated information is then passed
    to CN-CAM, which then uses a pyramid structure based on AvgPool to execute spatio-temporal
    non-local attention and coarse-to-fine spatial attention. Finally, the results
    are aggregated and sent to the reconstruction module to yield the final HR result.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TM-CAM 中，两个相邻的低分辨率（LR）帧首先被编码成低分辨率特征，以提高稳定性和对噪声的鲁棒性。然后，通过一个聚合单元（AU）在原始 LR 特征空间中开始聚合，以补偿大范围的运动，同时逐步向低级/高分辨率阶段过渡，以处理细微的子像素位移。在单个
    AU 中，使用了基于补丁的匹配策略，因为它自然包含了结构信息。然后将多个候选结果进行聚合，以获得足够的上下文信息。聚合后的信息传递给 CN-CAM，CN-CAM
    使用基于 AvgPool 的金字塔结构来执行时空非局部注意力和从粗到细的空间注意力。最后，结果被聚合并发送到重建模块，以产生最终的高分辨率（HR）结果。
- en: '![Refer to caption](img/d0d02a3bcd9bc70bdaa5b9bef118c118.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d0d02a3bcd9bc70bdaa5b9bef118c118.png)'
- en: 'Figure 19: The network architecture of TecoGAN (Chu et al., [2020](#bib.bib12)).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19: TecoGAN 的网络架构（Chu 等人，[2020](#bib.bib12)）。'
- en: 4.1.16 TecoGAN
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.16 TecoGAN
- en: 'Temporally coherent GAN (TecoGAN)⁹⁹9Code: https://github.com/thunil/TecoGAN
    (Chu et al., [2020](#bib.bib12)) mainly proposes a spatio-temporal discriminator
    for realistic and coherent video super-resolution, and a novel “Ping-Pong” loss
    to tackle recurrent artifacts. Like GAN, TecoGAN also consists of a generator
    and a discriminator and its architecture is shown in Fig. [19](#S4.F19 "Figure
    19 ‣ 4.1.15 MuCAN ‣ 4.1 Motion Estimation and Compensation Methods ‣ 4 Methods
    with Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '时序一致性生成对抗网络（TecoGAN）⁹⁹9代码: https://github.com/thunil/TecoGAN（Chu 等人，[2020](#bib.bib12)）主要提出了一种用于真实且一致的视频超分辨率的时空判别器，以及一种新颖的“乒乓”损失来处理重复出现的伪影。与
    GAN 类似，TecoGAN 也由生成器和判别器组成，其架构如图 [19](#S4.F19 "图 19 ‣ 4.1.15 MuCAN ‣ 4.1 运动估计与补偿方法
    ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率: 综合调查") 所示。'
- en: The generator takes the target frame, the previous frame and previous estimated
    HR frames as inputs. First, input frames are fed into the optical flow module,
    which is a CNN similar to the optical flow estimation module in FRVSR (Sajjadi
    et al., [2018](#bib.bib102)). In this module, the LR optical flow between the
    target frame and neighboring frames is estimated and enlarged by the bicubic interpolation
    to attain the corresponding HR optical flow. Then the previous HR frame is warped
    by the HR optical flow. The warped previous HR frame and target frame are fed
    into subsequent convolutional modules that include two convolutional layers, a
    residual block and two upsample modules with a deconvolution layer, to yield a
    restored target frame. Moreover, the discriminator assesses the quality of super-resolution
    results. The discriminator takes the generated results and GT as inputs, where
    each of them has three components, that is, three consecutive HR frames, three
    corresponding upsampled LR frames and three warped HR frames. With such input
    formats, the spatial over-smooth and temporal inconsistence in the final results
    can be relieved. TecoGAN also proposes a “ping-pong” loss function to reduce the
    long-term temporal detail drift and make super-resolution results more natural.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器将目标帧、前一帧和之前估算的高分辨率（HR）帧作为输入。首先，将输入帧送入光流模块，该模块是一个类似于FRVSR中光流估计模块的卷积神经网络（CNN）（Sajjadi
    等，[2018](#bib.bib102)）。在这个模块中，目标帧与邻近帧之间的低分辨率（LR）光流被估算并通过双三次插值放大，以获得相应的高分辨率光流。然后，之前的高分辨率帧通过高分辨率光流进行扭曲。扭曲后的前一高分辨率帧和目标帧被送入后续的卷积模块，包括两个卷积层、一个残差块和两个上采样模块，其中有一个反卷积层，以生成恢复的目标帧。此外，判别器评估超分辨率结果的质量。判别器将生成的结果和真实值（GT）作为输入，每个输入包含三个组件，即三个连续的高分辨率帧、三个相应的上采样低分辨率帧和三个扭曲的高分辨率帧。通过这样的输入格式，可以缓解最终结果中的空间过度平滑和时间不一致问题。TecoGAN还提出了“乒乓”损失函数，以减少长期时间细节漂移，使超分辨率结果更加自然。
- en: '![Refer to caption](img/85b7931833b4ec8d36e4c0f801b6750e.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/85b7931833b4ec8d36e4c0f801b6750e.png)'
- en: 'Figure 20: The network architecture of BasicVSR (Chan et al., [2021b](#bib.bib7)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：BasicVSR的网络架构（Chan 等，[2021b](#bib.bib7)）。
- en: 4.1.17 BasicVSR
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.17 BasicVSR
- en: 'The authors proposed a generic framework for video super resolution, called
    BasicVSR, as shown in Fig. [20](#S4.F20 "Figure 20 ‣ 4.1.16 TecoGAN ‣ 4.1 Motion
    Estimation and Compensation Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"). It is a typical bidirectional
    recurrent network, which mainly consists of three modules: the backward (B) module,
    the forward (F) module, and the upsampling (U) module. The B module receives the
    output of the next B module, current frame, and the following frame, while the
    F module receives the output of the previous F module, current frame, and the
    preceding frame. Then the outputs of the two modules are fused through a U module
    to yield the super-resolved current frame. These processes iterate until all the
    frames are super-resolved. The B/F module composes of generic components: the
    motion estimation, spatial warping, and residual blocks. The authors further propose
    two processing mechanisms the information-refill and coupled propagation, which
    consist of the IconVSR algorithm. The former addresses the performance degradation
    caused by misalignment, and the latter deals with the lack of information interaction
    between the forward processing and the backward processing in BasicVSR. In the
    information-refill mechanism, if the currently processed frame is in the selected
    keyframe set, it will be fused; otherwise, the aligned result will be directly
    sent into the residual block without fusion. This mechanism relieves error accumulation
    caused by misalignment, thus avoiding the performance degradation. In the coupling
    propagation mechanism, the output of backward propagation is directly used as
    the input of forward propagation, so as to achieve information interaction between
    them.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了一种通用的视频超分辨率框架，称为 BasicVSR，如图[20](#S4.F20 "图 20 ‣ 4.1.16 TecoGAN ‣ 4.1 运动估计与补偿方法
    ‣ 4 帧对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查")所示。它是一个典型的双向递归网络，主要由三个模块组成：反向（B）模块、正向（F）模块和上采样（U）模块。B
    模块接收下一个 B 模块的输出、当前帧和后续帧，而 F 模块接收前一个 F 模块的输出、当前帧和前一帧。然后，两个模块的输出通过 U 模块融合，生成超分辨率的当前帧。这些过程会迭代进行，直到所有帧都得到超分辨率处理。B/F
    模块由通用组件组成：运动估计、空间变形和残差块。作者进一步提出了两种处理机制，信息填充和耦合传播，它们构成了 IconVSR 算法。前者解决了由于对齐不准导致的性能下降问题，后者则处理了
    BasicVSR 中正向处理和反向处理之间信息交互不足的问题。在信息填充机制中，如果当前处理的帧在选定的关键帧集内，它将被融合；否则，对齐结果将直接送入残差块而不进行融合。该机制缓解了由于对齐不准导致的误差累积，从而避免了性能下降。在耦合传播机制中，反向传播的输出直接作为正向传播的输入，以实现二者之间的信息交互。
- en: In summary, the MEMC techniques are used to align neighboring frames with a
    target frame, and are probably the most common method for solving video super-resolution
    tasks. However, the problem is that they cannot guarantee the accuracy of motion
    estimation when lighting changes dramatically or there are large motions in videos.
    In these cases, the performance of the video super-resolution degrades greatly.
    This is confirmed by the assumption in (Lucas and Kanade, [1981](#bib.bib88)).
    When dealing with complex motions (not only large motions) and varying illumination,
    the calculation of motion estimation based on optical flow methods may break the
    hypothesis of brightness consistency, small moti on, and spatial coherence. Then
    the estimation of optical flow becomes inaccurate, and there arises errors, which
    easily results in artifacts and blurring. To address this issue, the methods with
    alignment (e.g., the deformable convolution which is presented as one module in
    the deep network to align frames) and the methods without alignment are both proposed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，MEMC 技术用于将相邻帧与目标帧对齐，可能是解决视频超分辨率任务最常见的方法。然而，问题在于当光照剧烈变化或视频中存在大幅运动时，它们无法保证运动估计的准确性。在这些情况下，视频超分辨率的性能会大幅下降。这一点在
    (Lucas 和 Kanade，[1981](#bib.bib88)) 的假设中得到了验证。当处理复杂的运动（不仅仅是大运动）和变化的光照时，基于光流方法的运动估计计算可能会违背亮度一致性、小运动和空间一致性的假设。然后，光流估计变得不准确，出现错误，容易导致伪影和模糊。为了解决这个问题，提出了有对齐的方法（例如，可变形卷积作为深度网络中的一个模块来对齐帧）和无对齐的方法。
- en: '![Refer to caption](img/ae3890db745fc1b73d869a49c58a3309.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ae3890db745fc1b73d869a49c58a3309.png)'
- en: 'Figure 21: Deformable convolution for frame alignment.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：用于帧对齐的可变形卷积。
- en: 4.2 Deformable Convolution Methods
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 可变形卷积方法
- en: 'The deformable convolutional network was first proposed by Dai et al. ([2017](#bib.bib13))
    and the improved variant (Zhu et al., [2019](#bib.bib141)) was proposed in 2019\.
    In ordinary CNNs, the convention is to use a fixed geometric structure in a layer,
    which restricts the network’s capability to model geometric transformations. In
    contrast, the deformable convolution is able to overcome this limitation. The
    illustration of the deformable convolution for feature alignment is shown in Fig. [21](#S4.F21
    "Figure 21 ‣ 4.1.17 BasicVSR ‣ 4.1 Motion Estimation and Compensation Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey"). The target feature maps concatenating with the neighboring
    feature maps are projected to attain offsets via additional convolutional layers.
    The offsets are applied to the conventional convolution kernel to yield a deformable
    convolution kernel, and then it is convolved with the input feature maps to produce
    the output feature maps. The methods that adopt deformable convolution mainly
    include the enhanced deformable video restoration (EDVR) (Wang et al., [2019a](#bib.bib122)),
    deformable non-local network (DNLN) (Wang et al., [2019](#bib.bib118)), and temporally
    deformable alignment network (TDAN) (Tian et al., [2020](#bib.bib114)), which
    are depicted in detail as follows.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '可变形卷积网络是由Dai等人首次提出（[2017](#bib.bib13)），并且其改进变种（Zhu等人，[2019](#bib.bib141)）是在2019年提出的。在普通CNN中，惯例是在一层中使用固定的几何结构，这限制了网络对几何变换的建模能力。相比之下，可变形卷积能够克服这一限制。图[21](#S4.F21
    "Figure 21 ‣ 4.1.17 BasicVSR ‣ 4.1 Motion Estimation and Compensation Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey")显示了用于特征对齐的可变形卷积的示意图。目标特征图与相邻特征图连接并通过额外的卷积层投影以获得偏移量。这些偏移量被应用于传统卷积核以产生可变形卷积核，然后它与输入特征图进行卷积以产生输出特征图。采用可变形卷积的方法主要包括增强型可变形视频恢复（EDVR）（Wang等人，[2019a](#bib.bib122)），可变形非局部网络（DNLN）（Wang等人，[2019](#bib.bib118)），以及时间可变形对齐网络（TDAN）（Tian等人，[2020](#bib.bib114)），这些将详细描述如下。'
- en: '![Refer to caption](img/e8a3af6ece6c7b170e7e954b70ca2c6e.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e8a3af6ece6c7b170e7e954b70ca2c6e.png)'
- en: 'Figure 22: The network architecture of EDVR (Wang et al., [2019a](#bib.bib122)),
    where PCD is the pyramid, cascading and deformable alignment module, and TSA is
    the temporal-spatial attention fusion module.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：EDVR的网络架构（Wang等人，[2019a](#bib.bib122)），其中PCD是金字塔级联和可变形对齐模块，TSA是时空注意力融合模块。
- en: 4.2.1 EDVR
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 EDVR
- en: 'The enhanced deformable video restoration (EDVR)^(10)^(10)10Code: https://github.com/xinntao/EDVR
    (Wang et al., [2019a](#bib.bib122)), as shown in Fig. [22](#S4.F22 "Figure 22
    ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), is the champion model in the
    NTIRE19 Challenge (Nah et al., [2019a](#bib.bib91), [b](#bib.bib92)). EDVR proposes
    two key modules: the pyramid, cascading and deformable (PCD) alignment module
    as in (Ranjan and Black, [2017](#bib.bib99); Sun et al., [2018](#bib.bib110);
    Hui et al., [2018](#bib.bib43), [2021](#bib.bib44)) and the temporal-spatial attention
    (TSA) fusion module, which are used to solve large motions in videos and to effectively
    fuse multiple frames, respectively.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '增强型可变形视频恢复（EDVR）^(10)^(10)10代码：https://github.com/xinntao/EDVR (Wang等人，[2019a](#bib.bib122))，如图[22](#S4.F22
    "Figure 22 ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey")所示，是NTIRE19挑战赛（Nah等人，[2019a](#bib.bib91),
    [b](#bib.bib92)）中的冠军模型。EDVR提出了两个关键模块：金字塔级联和可变形（PCD）对齐模块（Ranjan和Black，[2017](#bib.bib99);
    Sun等人，[2018](#bib.bib110); Hui等人，[2018](#bib.bib43), [2021](#bib.bib44)）和时空注意力（TSA）融合模块，它们分别用于解决视频中的大运动和有效融合多个帧。'
- en: 'EDVR mainly consists of four parts: one PCD alignment module, a TSA fusion
    module, a reconstruction module, and an upsample module using a sub-pixel convolutional
    layer. Firstly, the input frames are aligned by the PCD alignment module, and
    then the aligned frames are fused by the TSA fusion module. Then the fused results
    are fed into the reconstruction module to refine the features, and then through
    the up-sampling, a HR image called the residual image is obtained. The final result
    is obtained by adding the residual image to a direct upsampling target frame.
    To further improve performance, EDVR also adopts a two-phase approach, whose second
    phase is similar to the first but with a shallower network depth.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: EDVR 主要由四个部分组成：一个 PCD 对齐模块，一个 TSA 融合模块，一个重构模块，以及使用子像素卷积层的上采样模块。首先，输入帧通过 PCD
    对齐模块进行对齐，然后通过 TSA 融合模块融合对齐后的帧。然后，融合的结果输入重构模块以细化特征，通过上采样获得称为残差图像的 HR 图像。最终结果通过将残差图像添加到直接上采样目标帧来获得。为了进一步提高性能，EDVR
    还采用了两阶段方法，第二阶段与第一阶段类似，但网络深度较浅。
- en: '![Refer to caption](img/dac714c5861833d6188e49ff0e56ef01.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dac714c5861833d6188e49ff0e56ef01.png)'
- en: 'Figure 23: The network architecture of DNLN (Wang et al., [2019](#bib.bib118)).
    Here Non-local Att. is the non-local attention module.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '图 23: DNLN 的网络架构（Wang 等，[2019](#bib.bib118)）。其中非局部 Att. 是非局部注意力模块。'
- en: 4.2.2 DNLN
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 DNLN
- en: 'The deformable non-local network (DNLN)^(11)^(11)11Code: https://github.com/wh1h/DNLN
    (Wang et al., [2019](#bib.bib118)), as shown in Fig. [23](#S4.F23 "Figure 23 ‣
    4.2.1 EDVR ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"), designs an
    alignment module and a non-local attention module based on the deformable convolution
    (Dai et al., [2017](#bib.bib13); Zhu et al., [2019](#bib.bib141)) and non-local
    networks (Wang et al., [2018](#bib.bib121)), respectively. The alignment module
    uses the hierarchical feature fusion module (HFFB) (Hui et al., [2021](#bib.bib45))
    within the original deformable convolution to generate convolutional parameters.
    Moreover, DNLN utilizes multiple deformable convolutions in a cascaded way, which
    makes inter-frame alignment more accurate.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '可变形非局部网络（DNLN）^(11)^(11)11代码：https://github.com/wh1h/DNLN（Wang 等，[2019](#bib.bib118)），如图所示。[23](#S4.F23
    "图 23 ‣ 4.2.1 EDVR ‣ 4.2 可变形卷积方法 ‣ 4 用于对齐的方法 ‣ 基于深度学习的视频超分辨率: 一项综合调查")，基于可变形卷积（Dai
    等，[2017](#bib.bib13)；Zhu 等，[2019](#bib.bib141)）和非局部网络（Wang 等，[2018](#bib.bib121)），设计了一个对齐模块和一个非局部注意力模块。对齐模块在原始可变形卷积内使用分层特征融合模块（HFFB）（Hui
    等，[2021](#bib.bib45)）生成卷积参数。此外，DNLN 采用多个可变形卷积级联方式，使得帧间对齐更加准确。'
- en: '![Refer to caption](img/cb60a3cf523334be4fe61f7e37fdefd9.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb60a3cf523334be4fe61f7e37fdefd9.png)'
- en: 'Figure 24: The network architecture of TDAN (Tian et al., [2020](#bib.bib114)).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '图 24: TDAN 的网络架构（Tian 等，[2020](#bib.bib114)）。'
- en: 4.2.3 TDAN
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 TDAN
- en: 'The temporally deformable alignment network (TDAN)^(12)^(12)12Code: https://github.com/YapengTian/TDAN-VSR-CVPR-2020
    (Tian et al., [2020](#bib.bib114)), as shown in Fig. [24](#S4.F24 "Figure 24 ‣
    4.2.2 DNLN ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"), applies deformable
    convolution to the target frame and the neighboring frame, and attains corresponding
    offsets. Then the neighboring frame is warped in terms of the offsets to align
    with the target frame. TDAN is divided into three parts, i.e., a feature extraction
    module, a deformable convolution module and a reconstruction module.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '时间可变形对齐网络（TDAN）^(12)^(12)12代码：https://github.com/YapengTian/TDAN-VSR-CVPR-2020（Tian
    等，[2020](#bib.bib114)），如图所示。[24](#S4.F24 "图 24 ‣ 4.2.2 DNLN ‣ 4.2 可变形卷积方法 ‣ 4
    用于对齐的方法 ‣ 基于深度学习的视频超分辨率: 一项综合调查")，应用可变形卷积到目标帧和相邻帧，并获得相应的偏移量。然后，相邻帧根据偏移量进行扭曲以与目标帧对齐。TDAN
    分为三部分，即特征提取模块，可变形卷积模块和重构模块。'
- en: '![Refer to caption](img/4f4240107f6dbe92f02e771b95c87ca0.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/4f4240107f6dbe92f02e771b95c87ca0.png)'
- en: 'Figure 25: The network architecture of D3Dnet (Ying et al., [2020](#bib.bib134)).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '图 25: D3Dnet 的网络架构（Ying 等，[2020](#bib.bib134)）。'
- en: 4.2.4 D3Dnet
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 D3Dnet
- en: 'The architecture of the deformable 3D convolution network (D3Dnet)^(13)^(13)13Code:
    https://github.com/XinyiYing/D3Dnet (Ying et al., [2020](#bib.bib134)) is shown
    in Fig. [25](#S4.F25 "Figure 25 ‣ 4.2.3 TDAN ‣ 4.2 Deformable Convolution Methods
    ‣ 4 Methods with Alignment ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey"). D3Dnet proposes 3D deformable convolution to achieve strong
    spatio-temporal feature modeling capability. The inputs are first fed to a 3D
    convolutional layer to generate features, which are then fed to 5 Residual Deformable
    3D Convolution (ResD3D) blocks to achieve motion compensation and capture spatial
    information.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '可变形3D卷积网络（D3Dnet）^(13)^(13)13代码: https://github.com/XinyiYing/D3Dnet（Ying et
    al., [2020](#bib.bib134)）的架构如图[25](#S4.F25 "图25 ‣ 4.2.3 TDAN ‣ 4.2 可变形卷积方法 ‣ 4
    对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查")所示。D3Dnet提出了3D可变形卷积，以实现强大的时空特征建模能力。输入首先传递到一个3D卷积层生成特征，然后输入到5个残差可变形3D卷积（ResD3D）块中，以实现运动补偿并捕捉空间信息。'
- en: '![Refer to caption](img/c461e198d202ad722d60b546b6dbcfb8.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c461e198d202ad722d60b546b6dbcfb8.png)'
- en: 'Figure 26: The architecture of VESR-Net (Chen et al., [2020](#bib.bib10)),
    where CARB is the channel-attention residual block, and Separate NL denotes the
    separate non-local architecture.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：VESR-Net的架构（Chen et al., [2020](#bib.bib10)），其中CARB是通道注意残差块，Separate NL表示分离的非局部架构。
- en: 4.2.5 VESR-Net
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 VESR-Net
- en: 'The architecture of video enhancement and super-resolution network (VESR-Net)
    (Chen et al., [2020](#bib.bib10)), as shown in Fig. [26](#S4.F26 "Figure 26 ‣
    4.2.4 D3Dnet ‣ 4.2 Deformable Convolution Methods ‣ 4 Methods with Alignment ‣
    Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"), is the
    champion model in the Youku video enhancement and super-resolution challenge.
    VESR-Net mainly consists of a feature encoder, a fusion module and a reconstruction
    module.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 视频增强与超分辨率网络（VESR-Net）的架构（Chen et al., [2020](#bib.bib10)），如图[26](#S4.F26 "图26
    ‣ 4.2.4 D3Dnet ‣ 4.2 可变形卷积方法 ‣ 4 对齐方法 ‣ 基于深度学习的视频超分辨率：综合调查")所示，是优酷视频增强与超分辨率挑战中的冠军模型。VESR-Net主要包括一个特征编码器、一个融合模块和一个重建模块。
- en: The LR frames are firstly processed by the feature encoder consisting of a convolution
    layer and several stacked channel-attention residual blocks (CARBs) (Zhang et al.,
    [2018b](#bib.bib139)). Then in the fusion module, the PCD convolution in (Wang
    et al., [2019a](#bib.bib122)) performs the inter-frame feature alignment. The
    separate non-local submodule (Separate NL) divides feature maps in spatial, channel
    and temporal dimensions and processes them to obtain correlation information separately.
    In contrast to the vanilla non-local (Wang et al., [2018](#bib.bib121)) architecture,
    Separate NL can fuse the information across video frames and across pixels in
    each frame with less parameters and shallower network. Finally, VESR-Net utilizes
    CARBs followed with a feature decoder for upsampling in the reconstruction module,
    where the upsample module is implemented by a sub-pixel convolutional layer. And
    it outputs the super-resolved frame by adding with the bicubic-interpolation LR
    target frame.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 低分辨率帧首先由包括卷积层和几个堆叠的通道注意残差块（CARBs）（Zhang et al., [2018b](#bib.bib139)）组成的特征编码器处理。然后在融合模块中，（Wang
    et al., [2019a](#bib.bib122)）中的PCD卷积执行帧间特征对齐。分离非局部子模块（Separate NL）在空间、通道和时间维度上划分特征图并分别处理以获得相关信息。与传统的非局部（Wang
    et al., [2018](#bib.bib121)）架构相比，Separate NL可以用更少的参数和更浅的网络融合视频帧和每帧中的像素信息。最后，VESR-Net在重建模块中利用CARBs和特征解码器进行上采样，其中上采样模块由一个子像素卷积层实现。通过与双三次插值的低分辨率目标帧相加，输出超分辨率帧。
- en: The evolution of methods with alignment. In the methods with alignment, the
    motion estimation and motion compensation techniques, as a classic research topic
    in computer vision, have been applied to video super-resolution in the early years.
    MEMC has wide range of applications such as video coding and enhancing the interlaced
    scanning. As the advent of deep learning based VSR, many works employ MEMC to
    capture the motion information contained in video frames. The early work of MEMC
    is Deep-DE (Liao et al., [2015](#bib.bib76)), and some recently proposed methods
    such as VESPCN (Caballero et al., [2017](#bib.bib5)), SOFVSR (Wang et al., [2019](#bib.bib119)),
    TOFlow (Xue et al., [2019](#bib.bib130)) and FRVSR (Sajjadi et al., [2018](#bib.bib102))
    also adopted MEMC techniques. Specifically, early video super-resolution algorithms
    adopt traditional MEMC methods such as Druleas in VSRnet (Kappeler et al., [2016](#bib.bib59)),
    while subsequent algorithms such as VESPCN (Caballero et al., [2017](#bib.bib5)),
    TOFlow (Xue et al., [2019](#bib.bib130)) and FRVSR (Sajjadi et al., [2018](#bib.bib102))
    mainly design sub-module or sub-network for MEMC.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 方法的演变与对齐。在对齐方法中，运动估计和运动补偿技术作为计算机视觉中的经典研究课题，早期已被应用于视频超分辨率。MEMC具有广泛的应用领域，如视频编码和增强交错扫描。随着基于深度学习的视频超分辨率（VSR）的出现，许多研究工作采用MEMC来捕捉视频帧中包含的运动信息。MEMC的早期工作是Deep-DE（Liao等，[2015](#bib.bib76)），一些最近提出的方法如VESPCN（Caballero等，[2017](#bib.bib5)），SOFVSR（Wang等，[2019](#bib.bib119)），TOFlow（Xue等，[2019](#bib.bib130)）和FRVSR（Sajjadi等，[2018](#bib.bib102)）也采用了MEMC技术。具体来说，早期的视频超分辨率算法采用传统的MEMC方法，如VSRnet中的Druleas（Kappeler等，[2016](#bib.bib59)），而后续算法如VESPCN（Caballero等，[2017](#bib.bib5)），TOFlow（Xue等，[2019](#bib.bib130)）和FRVSR（Sajjadi等，[2018](#bib.bib102)）则主要设计了针对MEMC的子模块或子网络。
- en: However, the accuracy of most MEMC methods is usually not guaranteed. When the
    luminance changes or the videos contain large motions between frames, the performance
    of VSR degrades dramatically. Hence, the deformable convolution (DConv), which
    is not sensitive to varying lighting and motion conditions, has attracted more
    attention from researchers. DConv applies a learnable offset to each sampling
    point compared with the conventional convolution. Therefore, DConv can not only
    expand the receptive field of convolution kernel, but also enrich the shape of
    receptive field. When handling varying lighting and motion conditions, the conventional
    convolution with fixed kernel and limited receptive field may not be capable of
    capturing varying conditions. While DConv uses a learnable parameter for the kernel
    to analyze lighting and motion features, which can better capture complex motions
    and illumination changes. The deformable convolution was proposed by Dai et al.
    ([2017](#bib.bib13)) to enhance the transformation modeling capability of CNNs
    for the geometric variations of objects. In the VSR methods, TDAN (Tian et al.,
    [2020](#bib.bib114)) first utilized it to perform inter-frame alignment. After
    that, DNLN (Wang et al., [2019](#bib.bib118)), EDVR (Wang et al., [2019a](#bib.bib122)),
    and D3Dnet (Ying et al., [2020](#bib.bib134)) further promote it for frame alignment.
    Nevertheless, the deformable convolution still has some drawbacks including high
    computational complexity and harsh convergence conditions. Therefore, there is
    still room for improvement of this technique in the future.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数MEMC方法的准确性通常无法保证。当亮度变化或视频帧之间存在大幅运动时，VSR的性能会急剧下降。因此，对不敏感于变化光照和运动条件的可变形卷积（DConv）引起了研究人员的更多关注。与传统卷积相比，DConv对每个采样点应用了可学习的偏移。因此，DConv不仅可以扩展卷积核的感受野，还可以丰富感受野的形状。在处理变化的光照和运动条件时，具有固定卷积核和有限感受野的传统卷积可能无法捕捉到变化的条件。而DConv使用可学习的参数来分析光照和运动特征，从而能够更好地捕捉复杂的运动和光照变化。可变形卷积由Dai等（[2017](#bib.bib13)）提出，以增强CNN在对象几何变换建模中的能力。在VSR方法中，TDAN（Tian等，[2020](#bib.bib114)）首次利用它进行帧间对齐。之后，DNLN（Wang等，[2019](#bib.bib118)），EDVR（Wang等，[2019a](#bib.bib122)）和D3Dnet（Ying等，[2020](#bib.bib134)）进一步推广了它在帧对齐中的应用。然而，可变形卷积仍然存在一些缺点，包括高计算复杂度和严格的收敛条件。因此，这项技术在未来仍有改进的空间。
- en: In addition, the performance of the MEMC-based methods will degrade greatly
    when there were dramatic lighting changes and large motions in videos. Nevertheless,
    the network architecture is one of the important factors to affect its performance.
    Other factors include the training dataset, training strategy, data preprocessing,
    hyper-parameter setting, iteration times, etc. Although the MEMC-based methods
    have the limitation to deal with videos containing lighting changes and large
    motions, they can be counteracted by other network designs and training settings.
    For example, BasicVSR/IconVSR adopts a bidirectional recurrent network as backbone,
    which fully utilizes the global information from the video sequences and expands
    receptive field. Thus, they may gain superior performance compared with the other
    MEMC methods, which mainly use convolutions. Moreover, the training process, which
    uses a Cosine annealing scheme  (Loshchilov and Hutter, [2017](#bib.bib86)), is
    probably more refined.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于 MEMC 的方法在视频中存在剧烈光照变化和大运动时会大大降低性能。然而，网络架构是影响其性能的重要因素之一。其他因素包括训练数据集、训练策略、数据预处理、超参数设置、迭代次数等。尽管
    MEMC 基的方法在处理包含光照变化和大运动的视频时存在局限性，但可以通过其他网络设计和训练设置来对抗。例如，BasicVSR/IconVSR 采用了双向递归网络作为骨干网，充分利用视频序列中的全局信息并扩展感受野。因此，与主要使用卷积的其他
    MEMC 方法相比，它们可能获得更优越的性能。此外，使用余弦退火方案的训练过程（Loshchilov 和 Hutter，[2017](#bib.bib86)）可能更为精细。
- en: 5 Methods without Alignment
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 无对齐方法
- en: 'In contrast to the methods with alignment, the methods without alignment do
    not align neighboring frames for video super-resolution. This type of methods
    mainly exploit the spatial or spatio-temporal information for feature extraction.
    According to the dominating techniques utilized for initial feature extraction,
    we further categorize them into five types: the 2D convolution methods (2D Conv),
    3D convolution methods (3D Conv), recurrent convolutional neural network (RCNN),
    non-local network based, and other methods. Among them, the first type falls into
    the spatial methods, while the following three are the spatio-temporal methods,
    whose characteristic is to exploit both the spatial and temporal information from
    input videos. Other methods include the ones do not belong to any of the former.
    We present them in detail as follows.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于有对齐的方法，无对齐的方法不对视频超分辨率进行邻近帧对齐。这类方法主要利用空间或时空信息进行特征提取。根据用于初步特征提取的主导技术，我们进一步将其分类为五种类型：2D
    卷积方法（2D Conv）、3D 卷积方法（3D Conv）、递归卷积神经网络（RCNN）、基于非局部网络的方法和其他方法。其中，第一种属于空间方法，而接下来的三种是时空方法，其特点是利用输入视频中的空间和时间信息。其他方法则包括不属于上述任何一种的方法。我们将详细介绍这些方法。
- en: 5.1 2D Convolution Methods
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 2D 卷积方法
- en: Instead of alignment operations such as motion estimation and motion compensation
    between frames, the input frames are directly fed into a 2D convolutional network
    to spatially perform feature extraction, fusion and super-resolution operations.
    This may be a simple approach for solving the video super-resolution problem since
    it makes the network learn the correlation information within frames by itself.
    The representative methods are VSRResFeatGAN (Lucas et al., [2019](#bib.bib87))
    and FFCVSR (Yan et al., [2019](#bib.bib131)).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用帧间的运动估计和运动补偿等对齐操作，而是将输入帧直接输入到 2D 卷积网络中进行空间特征提取、融合和超分辨率操作。这可能是解决视频超分辨率问题的一种简单方法，因为它使网络自行学习帧内的相关信息。代表性的方法有
    VSRResFeatGAN（Lucas 等，[2019](#bib.bib87)）和 FFCVSR（Yan 等，[2019](#bib.bib131)）。
- en: '![Refer to caption](img/06a6791f8ea145663ac859e4b4d02b99.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06a6791f8ea145663ac859e4b4d02b99.png)'
- en: 'Figure 27: The architecture of the generator in VSRResFeatGAN (Lucas et al.,
    [2019](#bib.bib87)).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '图 27: VSRResFeatGAN 中生成器的架构（Lucas 等，[2019](#bib.bib87)）。'
- en: 5.1.1 VSRResFeatGAN
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 VSRResFeatGAN
- en: 'VSRResFeatGAN (Lucas et al., [2019](#bib.bib87)) utilizes GAN to address VSR
    tasks and find a good solution by adversarial training. The generator shown in
    Fig. [27](#S5.F27 "Figure 27 ‣ 5.1 2D Convolution Methods ‣ 5 Methods without
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")
    consists of convolutional layers and residual blocks. And each residual block
    is composed of two convolutional layers and is followed by a ReLU activation function.
    Moreover, the discriminator consists of three groups of convolutions and a fully
    connected layer, where each group includes a convolutional layer, Batch Normalization
    (BN), and LeakyReLU. The discriminator determines whether the output of the generator
    is a generated image or GT image. Then the result of the discriminator reacts
    to the generator, and promotes it to yield results closer to the GT images. Finally,
    a relative satisfactory solution is obtained through an iterative optimization.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**VSRResFeatGAN**（Lucas等，[2019](#bib.bib87)）利用GAN解决VSR任务，通过对抗训练找到一个好的解决方案。如图 [27](#S5.F27
    "Figure 27 ‣ 5.1 2D Convolution Methods ‣ 5 Methods without Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey")所示，生成器由卷积层和残差块组成。每个残差块由两个卷积层组成，并紧随其后的是ReLU激活函数。此外，判别器由三组卷积和一个全连接层组成，每组包括一个卷积层、批量归一化（BN）和**LeakyReLU**。判别器判断生成器的输出是生成的图像还是GT图像。然后，判别器的结果反应到生成器上，促使其产生更接近GT图像的结果。最终，通过迭代优化获得相对满意的解决方案。'
- en: '![Refer to caption](img/f8c5b246ee6b673e659e72ce6be40a9f.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f8c5b246ee6b673e659e72ce6be40a9f.png)'
- en: 'Figure 28: The architecture of FFCVSR (Yan et al., [2019](#bib.bib131)). Here
    Net[C] is the context network, and Net[L] is the local network.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图28：**FFCVSR**（Yan等，[2019](#bib.bib131)）的架构。这里Net[C]是上下文网络，Net[L]是局部网络。
- en: 5.1.2 FFCVSR
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 **FFCVSR**
- en: 'The architecture of the frame and feature-context video super-resolution (FFCVSR)^(14)^(14)14Code:
    https://github.com/linchuming/FFCVSR (Yan et al., [2019](#bib.bib131)) is shown
    in Fig. [28](#S5.F28 "Figure 28 ‣ 5.1.1 VSRResFeatGAN ‣ 5.1 2D Convolution Methods
    ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). Unlike common MEMC techniques, FFCVSR consists of several
    local networks and context networks and utilizes inter-frame information in a
    different way. The LR unaligned video frames and the HR output of the previous
    frame are directly taken as inputs to the network for the purpose of restoring
    high-frequency details and maintaining temporal consistency.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**FFCVSR**（Yan等，[2019](#bib.bib131)）的框架和特征上下文视频超分辨率（FFCVSR）^(14)^(14)14代码：
    https://github.com/linchuming/FFCVSR 如图 [28](#S5.F28 "Figure 28 ‣ 5.1.1 VSRResFeatGAN
    ‣ 5.1 2D Convolution Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey")所示。与常见的MEMC技术不同，FFCVSR由多个局部网络和上下文网络组成，并以不同的方式利用帧间信息。将LR未对齐的视频帧和前一帧的HR输出直接作为网络的输入，以恢复高频细节并保持时间一致性。'
- en: In summary, the above two methods both exploit spatial correlation between frames
    for VSR tasks. VSRResFeatGAN utilizes adversarial training of GANs to find an
    appropriate solution. As the discriminator in GANs has to guess whether the generated
    frame is close to the ground truth, the VSR results in terms of PSNR and SSIM
    are not always satisfactory compared with other methods, such as FFCVSR.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，上述两种方法都利用了帧之间的空间相关性来处理VSR任务。**VSRResFeatGAN**利用对抗训练的GAN来寻找合适的解决方案。由于GAN中的判别器必须猜测生成的帧是否接近真实值，与其他方法（如**FFCVSR**）相比，VSR结果在PSNR和SSIM方面的表现并不总是令人满意。
- en: 5.2 3D Convolution Methods
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 3D卷积方法
- en: The 3D convolutional module (Tran et al., [2015](#bib.bib116); Ji et al., [2013](#bib.bib53))
    operates on spatio-temporal domain, compared with 2D convolution, which only utilizes
    spatial information through the sliding kernel over input frame. This is beneficial
    to the processing of video sequences, as the correlations among frames are considered
    by extracting temporal information. The representative 3D convolution methods
    for VSR are DUF (Jo et al., [2018](#bib.bib57)), FSTRN (Li et al., [2019a](#bib.bib72)),
    and 3DSRnet (Kim et al., [2019](#bib.bib61)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 3D卷积模块（Tran等，[2015](#bib.bib116)；Ji等，[2013](#bib.bib53)）在时空域上操作，相较于仅利用空间信息的2D卷积，通过滑动内核对输入帧进行处理。这对于视频序列的处理是有益的，因为提取时间信息可以考虑帧之间的相关性。用于VSR的代表性3D卷积方法有**DUF**（Jo等，[2018](#bib.bib57)）、**FSTRN**（Li等，[2019a](#bib.bib72)）和**3DSRnet**（Kim等，[2019](#bib.bib61)）。
- en: '![Refer to caption](img/b8fdd2f7d06f35e2eaa5daaed0576577.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8fdd2f7d06f35e2eaa5daaed0576577.png)'
- en: 'Figure 29: The network architecture of DUF (Jo et al., [2018](#bib.bib57)).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29：DUF 的网络架构（Jo 等，[2018](#bib.bib57)）。
- en: 5.2.1 DUF
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 DUF
- en: 'The dynamic upsampling filters (DUF)^(15)^(15)15Code: https://github.com/yhjo09/VSR-DUF
    (Jo et al., [2018](#bib.bib57)) has been proposed, as shown in Fig. [29](#S5.F29
    "Figure 29 ‣ 5.2 3D Convolution Methods ‣ 5 Methods without Alignment ‣ Video
    Super-Resolution Based on Deep Learning: A Comprehensive Survey"). It is inspired
    by the dynamic filter network (Jia et al., [2016](#bib.bib54)) that can generate
    corresponding filters for specific inputs and then apply them to generate corresponding
    feature maps.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 动态上采样滤波器（DUF）^(15)^(15)15代码：https://github.com/yhjo09/VSR-DUF（Jo 等，[2018](#bib.bib57)）已被提出，如图
    [29](#S5.F29 "图 29 ‣ 5.2 3D 卷积方法 ‣ 5 无对齐方法 ‣ 基于深度学习的视频超分辨率：全面综述") 所示。它的灵感来源于动态滤波器网络（Jia
    等，[2016](#bib.bib54)），该网络可以为特定输入生成相应的滤波器，然后应用这些滤波器生成对应的特征图。
- en: The structure of the dynamic up-sampling filter, together with the spatio-temporal
    information learned by 3D convolution, can avoid the use of motion estimation
    and motion compensation. DUF performs not only filtering, but also the up-sampling
    operation. In order to enhance high-frequency details of the super-resolution
    result, DUF uses a network to estimate residual map for the target frame. The
    final result is the sum of the residual map and the LR target frame processed
    by the dynamic upsample module with learned filters.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 动态上采样滤波器的结构，以及通过 3D 卷积学习到的时空信息，可以避免使用运动估计和运动补偿。DUF 不仅执行滤波，还执行上采样操作。为了增强超分辨率结果的高频细节，DUF
    使用网络来估计目标帧的残差图。最终结果是残差图和通过动态上采样模块处理的 LR 目标帧的总和。
- en: '![Refer to caption](img/b9b5b930bfe717665f67417da6d385ce.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9b5b930bfe717665f67417da6d385ce.png)'
- en: 'Figure 30: The network architecture of FSTRN (Li et al., [2019a](#bib.bib72)).
    Here FRB denotes the fast spatio-temporal residual block.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 30：FSTRN 的网络架构（Li 等，[2019a](#bib.bib72)）。这里 FRB 表示快速时空残差块。
- en: 5.2.2 FSTRN
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 FSTRN
- en: 'The fast spatio-temporal residual network (FSTRN) (Li et al., [2019a](#bib.bib72))
    uses a factorized 3D convolution to extract information contained in consecutive
    frames, as shown in Fig. [30](#S5.F30 "Figure 30 ‣ 5.2.1 DUF ‣ 5.2 3D Convolution
    Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). In FSTRN, a $k\times k\times k$ 3D convolutional kernel
    is decomposed into 2 cascaded kernels, whose sizes are $1\times k\times k$ and
    $k\times 1\times 1$, respectively, to reduce the computation caused by directly
    using the 3D convolution.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 快速时空残差网络（FSTRN）（Li 等，[2019a](#bib.bib72)）使用分解的 3D 卷积来提取连续帧中的信息，如图 [30](#S5.F30
    "图 30 ‣ 5.2.1 DUF ‣ 5.2 3D 卷积方法 ‣ 5 无对齐方法 ‣ 基于深度学习的视频超分辨率：全面综述") 所示。在 FSTRN 中，一个
    $k\times k\times k$ 的 3D 卷积核被分解为两个级联卷积核，尺寸分别为 $1\times k\times k$ 和 $k\times 1\times
    1$，以减少直接使用 3D 卷积所带来的计算量。
- en: 'FSTRN consists of the following four parts: an LR video shallow feature extraction
    net (LFENet), fast spatio-temporal residual blocks (FRBs), an LR feature fusion
    and up-sampling SR net (LSRNet), and a global residual learning (GRL) module.
    The GRL is mainly composed of two parts: LR space residual learning (LRL) and
    cross-space residual learning (CRL). The LRL is introduced along with the FRBs.
    And the CRL directly maps the LR video to the HR space. The designs of CRL and
    LRL can communicate the LR and HR space. Besides, FSTRN adopts a dropout layer
    after LRL to enhance generalization ability of the network. LFENet using 3D convolution
    to extract features for consecutive LR input frames. FRBs, including the decomposed
    3D convolutional layers, are responsible for extracting spatio-temporal information
    contained in input frames. LSRNet is used to fuse information from previous layers
    and conducting up-sampling.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: FSTRN 由以下四个部分组成：一个 LR 视频浅层特征提取网络（LFENet）、快速时空残差块（FRBs）、一个 LR 特征融合和上采样 SR 网络（LSRNet）以及一个全局残差学习（GRL）模块。GRL
    主要由两部分组成：LR 空间残差学习（LRL）和跨空间残差学习（CRL）。LRL 与 FRBs 一起引入。CRL 直接将 LR 视频映射到 HR 空间。CRL
    和 LRL 的设计可以实现 LR 和 HR 空间的通信。此外，FSTRN 在 LRL 后采用了 dropout 层，以增强网络的泛化能力。LFENet 使用
    3D 卷积提取连续 LR 输入帧的特征。FRBs，包括分解的 3D 卷积层，负责提取输入帧中的时空信息。LSRNet 用于融合来自前面层的信息并进行上采样。
- en: '![Refer to caption](img/73a674997c191eb28fbeb1e3f1f66f43.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/73a674997c191eb28fbeb1e3f1f66f43.png)'
- en: 'Figure 31: The network architecture of 3DSRNet (Kim et al., [2019](#bib.bib61)).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '图31: 3DSRNet的网络架构(Kim等人，[2019](#bib.bib61))。'
- en: 5.2.3 3DSRNet
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 3DSRNet
- en: 'The 3D super-resolution network (3DSRNet)^(16)^(16)16Code: https://github.com/sooyekim/3DSRnet
    (Kim et al., [2019](#bib.bib61)) uses 3D convolution to extract spatio-temporal
    information contained in consecutive frames for VSR tasks. The network architecture
    is shown in Fig. [31](#S5.F31 "Figure 31 ‣ 5.2.2 FSTRN ‣ 5.2 3D Convolution Methods
    ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). The sub net of 3DSRNet can preprocess scene change as
    shown in the figure. When frames of five different scenes getting involved into
    convolution, the sub net classifies the exact location of the scene boundary through
    the module of scene boundary detection, and replaces the different scene frames
    with the temporally closest frame of the same scene as the current middle frame.
    Finally, the updated five frames are sent for subsequent video super-resolution
    sub network. This approach overcomes performance degradation caused by scene change
    to some extent.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 3D超分辨率网络（3DSRNet）^(16)^(16)16代码：https://github.com/sooyekim/3DSRnet (Kim等人，[2019](#bib.bib61))
    使用3D卷积提取连续帧中包含的时空信息，用于VSR任务。其网络架构如图[31](#S5.F31 "图31 ‣ 5.2.2 FSTRN ‣ 5.2 3D卷积方法
    ‣ 5种无对齐方法 ‣ 基于深度学习的视频超分辨率：一项综合调查")所示。3DSRNet的子网络可以像图中所示一样预处理场景变化。当涉及到五个不同场景的帧进行卷积时，子网络通过场景边界检测模块精确分类场景边界的位置，并用同一场景的临时最接近帧替换不同场景帧作为当前中间帧。最后，更新后的五帧被发送到后续视频超分辨率子网络。这种方法在一定程度上克服了场景变化造成的性能下降。
- en: '![Refer to caption](img/2bb65b3b4d73c0b6dee75661432d6760.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2bb65b3b4d73c0b6dee75661432d6760.png)'
- en: 'Figure 32: The network architecture of DSMC (Liu et al., [2021a](#bib.bib83)).'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '图32: DSMC的网络架构(Liu等人，[2021a](#bib.bib83))。'
- en: 5.2.4 DSMC
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 DSMC
- en: 'A deep neural network with Dual Subnet and Multi-stage Communicated Upsampling
    (DSMC)^(17)^(17)17Code: https://github.com/iPrayerr/DSMC-VSR (Liu et al., [2021a](#bib.bib83))
    is proposed for super-resolution of videos with large motion. The architecture
    is shown in Fig. [32](#S5.F32 "Figure 32 ‣ 5.2.3 3DSRNet ‣ 5.2 3D Convolution
    Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey"). It designs a U-shaped residual dense network with 3D
    convolution (U3D-RDN) for fine implicit MEMC as well as coarse spatial feature
    extraction. moreover, DSMC presents a new Multi-Stage Communicated Upsampling
    (MSCU) module to make full use of the intermediate results of upsampling for guiding
    the VSR. Besides, a dual subnet is devised to aid the training of DSMC, whose
    dual loss helps to reduce the solution space and enhance the generalization ability.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 具有双子网络和多阶段通信上采样（DSMC）^(17)^(17)17代码：https://github.com/iPrayerr/DSMC-VSR (Liu等人，[2021a](#bib.bib83))是为了超分辨率处理大运动视频而提出的深度神经网络。其架构如图[32](#S5.F32
    "图32 ‣ 5.2.3 3DSRNet ‣ 5.2 3D卷积方法 ‣ 5种无对齐方法 ‣ 基于深度学习的视频超分辨率：一项综合调查")所示。它设计了一个具有3D卷积的U形残差密集网络（U3D-RDN）用于隐式MEMC的细微特性提取以及粗略的空间特征提取。此外，DSMC提出了一种新的多阶段通信上采样（MSCU）模块，充分利用上采样的中间结果来引导VSR。此外，还设计了一个双子网络来辅助DSMC的训练，其双重损失有助于减少解决方案空间并增强泛化能力。
- en: DSMC firstly performs deformable convolution on input consecutive frames for
    coarse feature extraction. The output feature maps are then processed by a deformable
    residual network (DResNet) (Lei and Todorovic, [2018](#bib.bib69)) to extract
    fine spatial information. Next, the feature maps are input to U3D-RDN for dimension
    reduction and correlation analyzation of spatio-temporal feature. Followed by
    another DResNet module, the feature maps are sent to MSCU module. Finally, with
    the aid of a dual subnet for training, DSMC yields the super-resolved HR frames.
    It is noted that only the output of the dual subnet and the result of VSR subnet
    are used for the loss computation of DSMC.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: DSMC首先对输入的连续帧执行可变形卷积以进行粗糙特征提取。然后，输出特征图通过可变形残差网络（DResNet）(Lei和Todorovic，[2018](#bib.bib69))提取细微空间信息。接下来，特征图输入到U3D-RDN进行维度减少和时空特征的相关性分析。再经过另一个DResNet模块，特征图送入MSCU模块。最后，在双子网络的训练辅助下，DSMC产生了超分辨率的HR帧。值得注意的是，只有双子网络的输出和VSR子网的结果用于DSMC的损失计算。
- en: In brief, these 3D convolutional methods can extract spatio-temporal correlations
    contained in consecutive frames, rather than perform the motion estimation to
    extract motion information contained in frames and motion compensation to align
    them. However, most of the methods have relatively higher computational complexities
    compared with those of 2D convolutional methods, which limits them for real-time
    video super-resolution tasks.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，这些3D卷积方法可以提取连续帧中的时空相关性，而不是执行运动估计以提取帧中的运动信息和运动补偿以对齐帧。然而，与2D卷积方法相比，大多数方法具有较高的计算复杂度，这限制了它们在实时视频超分辨率任务中的应用。
- en: 5.3 Recurrent Convolutional Neural Networks (RCNNs)
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 循环卷积神经网络（RCNNs）
- en: It is well known that RCNNs have strong capacity in modelling temporal dependency
    in sequential data, e.g., natural language, video and audio. A straightforward
    way is to use RCNNs to handle video sequences. Based on this key idea, several
    RCNN methods such as BRCN (Huang et al., [2015](#bib.bib41), [2018](#bib.bib42)),
    STCN (Guo and Chao, [2017](#bib.bib31)), and RISTN (Zhu et al., [2019](#bib.bib142))
    have been proposed for video super-resolution.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，RCNN在建模序列数据中的时间依赖性方面具有强大的能力，例如自然语言、视频和音频。一种直接的方法是使用RCNN处理视频序列。基于这一关键思想，已提出了几种RCNN方法，如BRCN（Huang
    et al., [2015](#bib.bib41), [2018](#bib.bib42)）、STCN（Guo and Chao, [2017](#bib.bib31)）和RISTN（Zhu
    et al., [2019](#bib.bib142)），用于视频超分辨率。
- en: '![Refer to caption](img/c4cfe221ee6b004adc98f1b9c9ce2a6c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c4cfe221ee6b004adc98f1b9c9ce2a6c.png)'
- en: 'Figure 33: The network architecture of BRCN (Huang et al., [2015](#bib.bib41),
    [2018](#bib.bib42)).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图33：BRCN的网络结构（Huang et al., [2015](#bib.bib41), [2018](#bib.bib42)）。
- en: 5.3.1 BRCN
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 BRCN
- en: 'The bidirectional recurrent convolutional network (BRCN) (Huang et al., [2015](#bib.bib41),
    [2018](#bib.bib42)), as shown in Fig. [33](#S5.F33 "Figure 33 ‣ 5.3 Recurrent
    Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey"), is composed of two modules:
    a forward sub-network and a backward one with a similar structure, which only
    differ in the order of processing sequence. The forward subnet is responsible
    for modeling the temporal dependency from previous frames, while the backward
    subnet models temporal dependency from subsequent frames.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[33](#S5.F33 "图33 ‣ 5.3 循环卷积神经网络（RCNNs） ‣ 5 无对齐方法 ‣ 基于深度学习的视频超分辨率：全面综述")所示，双向递归卷积网络（BRCN）（Huang
    et al., [2015](#bib.bib41), [2018](#bib.bib42)）由两个模块组成：一个前向子网络和一个结构类似的后向子网络，它们仅在处理序列的顺序上有所不同。前向子网负责建模来自前面帧的时间依赖性，而后向子网则建模来自后续帧的时间依赖性。
- en: '![Refer to caption](img/719c2e951db2da32b8f3aeea7527f837.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/719c2e951db2da32b8f3aeea7527f837.png)'
- en: 'Figure 34: The network architecture of STCN (STCN2107AAAI). Here BMC denotes
    the bidirectional multi-scale convolution.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图34：STCN的网络结构（STCN2107AAAI）。这里的BMC表示双向多尺度卷积。
- en: 5.3.2 STCN
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 STCN
- en: 'The spatio-temporal convolutional network (STCN) (Guo and Chao, [2017](#bib.bib31))
    is an end-to-end VSR method without MEMC, as shown in Fig. [34](#S5.F34 "Figure
    34 ‣ 5.3.1 BRCN ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods
    without Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"). The temporal information within frames is extracted by using LSTM (Hochreiter
    and Schmidhuber, [1997](#bib.bib38)). Similar to RISTN (Zhu et al., [2019](#bib.bib142)),
    the network consists of three parts: a spatial module, a temporal module and a
    reconstruction module. Spatial module is responsible for extracting features from
    multiple consecutive LR frames. Temporal module is a bidirectional multi-scale
    convoluted variant of LSTM, and is designed for extracting temporal correlation
    among frames.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 时空卷积网络（STCN）（Guo and Chao, [2017](#bib.bib31)）是一种端到端的VSR方法，无需MEMC，如图[34](#S5.F34
    "图34 ‣ 5.3.1 BRCN ‣ 5.3 循环卷积神经网络（RCNNs） ‣ 5 无对齐方法 ‣ 基于深度学习的视频超分辨率：全面综述")所示。帧内的时间信息是通过使用LSTM（Hochreiter
    and Schmidhuber, [1997](#bib.bib38)）提取的。类似于RISTN（Zhu et al., [2019](#bib.bib142)），该网络由三个部分组成：空间模块、时间模块和重建模块。空间模块负责从多个连续的LR帧中提取特征。时间模块是LSTM的双向多尺度卷积变体，旨在提取帧之间的时间相关性。
- en: '![Refer to caption](img/d93d9b6d40759a90dc41b2e9d6c10191.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d93d9b6d40759a90dc41b2e9d6c10191.png)'
- en: 'Figure 35: The network architecture of RISTN (Zhu et al., [2019](#bib.bib142)),
    where RIB denotes the residual invertible block, and RDC is the residual dense
    convolution.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 35：RISTN（Zhu et al., [2019](#bib.bib142)）的网络架构，其中RIB表示残差可逆块，RDC是残差密集卷积。
- en: 5.3.3 RISTN
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3 RISTN
- en: 'The residual invertible spatio-temporal network (RISTN)^(18)^(18)18Code: https://github.com/lizhuangzi/RISTN
    (Zhu et al., [2019](#bib.bib142)) is inspired by the invertible block (Jacobsen
    et al., [2018](#bib.bib51)). As shown in Fig. [35](#S5.F35 "Figure 35 ‣ 5.3.2
    STCN ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without
    Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"),
    it designs a residual invertible block (RIB), a LSTM with residual dense convolution
    (RDC-LSTM), and a sparse feature fusion strategy to adaptively select useful features.
    Here RIB is used to extract spatial information of video frames effectively, and
    RDC-LSTM is used to extract spatio-temporal features.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '残差可逆时空网络（RISTN）^(18)^(18)18代码: https://github.com/lizhuangzi/RISTN (Zhu et
    al., [2019](#bib.bib142)) 灵感来源于可逆块（Jacobsen et al., [2018](#bib.bib51)）。如图 [35](#S5.F35
    "Figure 35 ‣ 5.3.2 STCN ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs)
    ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep Learning:
    A Comprehensive Survey") 所示，它设计了一个残差可逆块（RIB）、一个带有残差密集卷积的LSTM（RDC-LSTM）和一个稀疏特征融合策略来自适应地选择有用的特征。这里，RIB用于有效提取视频帧的空间信息，RDC-LSTM用于提取时空特征。'
- en: 'The network is mainly divided into three parts: a spatial module, a temporal
    module and a reconstruction module. The spatial module is mainly composed of multiple
    parallel RIBs, and its output is used as the input of the temporal module. In
    the temporal module, after extracting spatio-temporal information, features are
    selectively fused by a sparse fusion strategy. Finally, the HR result of the target
    frame is reconstructed by the deconvolution in the reconstruction module.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络主要分为三个部分：空间模块、时间模块和重建模块。空间模块主要由多个并行的RIB组成，其输出作为时间模块的输入。在时间模块中，提取时空信息后，通过稀疏融合策略选择性地融合特征。最后，重建模块通过反卷积重建目标帧的高分辨率结果。
- en: '![Refer to caption](img/0d7d3b567bee22e07620d693c1885fd2.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d7d3b567bee22e07620d693c1885fd2.png)'
- en: 'Figure 36: The network architecture of RLSP (Fuoli et al., [2019a](#bib.bib25)).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 36：RLSP（Fuoli et al., [2019a](#bib.bib25)）的网络架构。
- en: 5.3.4 RLSP
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4 RLSP
- en: 'The Recurrent Latent Space Propagation (RLSP)^(19)^(19)19Code: https://github.com/dariofuoli/RLSP (Fuoli
    et al., [2019a](#bib.bib25)) shown in Fig. [36](#S5.F36 "Figure 36 ‣ 5.3.3 RISTN
    ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") proposes
    a recurrent video super-resolution algorithm, which avoids the problem that a
    single video frame is processed multiple times in a non-recurrent network. In
    addition, the algorithm implicitly transmits temporal information by introducing
    hidden states containing the temporal information yielded in previous moment as
    part of the input at current moment, and does not include explicit motion estimation
    and motion compensation.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '递归潜在空间传播（RLSP）^(19)^(19)19代码: https://github.com/dariofuoli/RLSP (Fuoli et
    al., [2019a](#bib.bib25)) 如图 [36](#S5.F36 "Figure 36 ‣ 5.3.3 RISTN ‣ 5.3 Recurrent
    Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey") 所示，提出了一种递归视频超分辨率算法，该算法避免了在非递归网络中单个视频帧被多次处理的问题。此外，该算法通过引入包含前一时刻产生的时间信息的隐藏状态作为当前时刻输入的一部分，隐式地传递时间信息，并且不包括显式的运动估计和运动补偿。'
- en: The hidden state is generated by the RLSP Cell, which is composed of several
    convolutions. The cell receives the hidden state of the previous moment, the super-resolved
    result of the previous moment, as well as the current frame and the adjacent frames
    as inputs to yield the super-resolved result and the hidden state of the current
    moment. This procedure repeats until all frames are processed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态由RLSP单元生成，该单元由多个卷积组成。该单元接收前一时刻的隐藏状态、前一时刻的超分辨率结果，以及当前帧和相邻帧作为输入，生成当前时刻的超分辨率结果和隐藏状态。该过程重复进行，直到所有帧都处理完毕。
- en: '![Refer to caption](img/bd2ab8fef1b0b12147b6572e2a30a496.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd2ab8fef1b0b12147b6572e2a30a496.png)'
- en: 'Figure 37: The network architecture of RSDN (Isobe et al., [2020](#bib.bib50)).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 37：RSDN（Isobe et al., [2020](#bib.bib50)）的网络架构。
- en: 5.3.5 RSDN
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.5 RSDN
- en: 'The Recurrent Structure-Detail Network (RSDN)^(20)^(20)20Code: https://github.com/junpan19/RSDN (Isobe
    et al., [2020](#bib.bib50)) shown in Fig. [37](#S5.F37 "Figure 37 ‣ 5.3.4 RLSP
    ‣ 5.3 Recurrent Convolutional Neural Networks (RCNNs) ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey") proposes
    to divide the frame into two components, namely structure and detail, and then
    process these two by subsequent module respectively.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '循环结构-细节网络（RSDN）^(20)^(20)20代码: https://github.com/junpan19/RSDN (Isobe 等，[2020](#bib.bib50))
    如图 [37](#S5.F37 "图 37 ‣ 5.3.4 RLSP ‣ 5.3 循环卷积神经网络（RCNNs） ‣ 5 没有对齐的方法 ‣ 基于深度学习的视频超分辨率:
    综合调查") 所示，提出将帧分为两个组件，即结构和细节，然后分别通过后续模块处理这两个组件。'
- en: The algorithm first uses the Bicubic interpolation algorithm to downsample and
    upsample the input LR frame to extract the structure and detail components. Then
    these two components are processed by convolution and multiple SD blocks to obtain
    the structure and detail components, super-resolved results and hidden states
    at the current moment. The SD block promotes information exchange between structure
    and detail components. In addition, the RSDN proposes a hidden-state adaption
    module to select the information that is beneficial to the super resolution and
    avoid the interference of redundant information.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先使用双三次插值算法对输入LR帧进行下采样和上采样，以提取结构和细节组件。然后，这两个组件通过卷积和多个SD块处理，以获得结构和细节组件、超分辨率结果和当前时刻的隐藏状态。SD块促进结构和细节组件之间的信息交换。此外，RSDN提出了一个隐藏状态适应模块，以选择有利于超分辨率的信息，避免冗余信息的干扰。
- en: In summary, the RCNN-based methods are suitable for modeling the spatio-temporal
    information contained in videos, since they can map neighboring frames and thus
    effectively establish long-term dependence with more lightweight structures. However,
    conventional RCNN-based methods are difficult to train and sometimes suffer from
    the gradient vanishing problem. And they may not capture long-term dependence
    when the length of input sequences is too large, and thus may not achieve great
    performance. LSTM-based methods can overcome these constraints to some extent
    with the help of the memorization of features from shallower layers. However,
    the complex design of LSTM is a factor that limits their depth on hardware, restraining
    them to model very long-term dependence.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于RCNN的方法适合建模视频中包含的时空信息，因为它们可以映射邻近的帧，从而有效地建立长期依赖关系，且结构更轻量。然而，传统的基于RCNN的方法难以训练，有时会遇到梯度消失问题。当输入序列的长度过大时，它们可能无法捕捉长期依赖关系，因此可能无法取得良好性能。基于LSTM的方法可以在一定程度上克服这些限制，通过记忆来自浅层的特征。然而，LSTM的复杂设计是限制其在硬件上深度的一个因素，使其难以建模非常长期的依赖关系。
- en: 5.4 Non-Local Methods
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 非局部方法
- en: 'The non-local-based method is another one that utilizes both spatial and temporal
    information contained in video frames for super-resolution. This method benefit
    from the key idea of the non-local neural network (Wang et al., [2018](#bib.bib121)),
    which was proposed to capture long-range dependencies for video classifications.
    It overcomes the flaws that convolution and recurrent computations are limited
    to the local area. Intuitively, a non-local operation is to calculate the response
    value of a position, which is equal to the weight sum of all possible positions
    in the input feature maps. Its formula is given as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 基于非局部的方法是另一种利用视频帧中空间和时间信息进行超分辨率的方法。这种方法受益于非局部神经网络（Wang 等，[2018](#bib.bib121)）的核心思想，该网络旨在捕捉视频分类的长距离依赖关系。它克服了卷积和递归计算局限于局部区域的缺陷。直观地说，非局部操作是计算一个位置的响应值，该值等于输入特征图中所有可能位置的加权和。其公式如下：
- en: '|  | $y_{i}=\dfrac{1}{\mathcal{C}(x)}\sum\limits_{\forall{j}}f(x_{i},x_{j})g(x_{j})$
    |  | (9) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}=\dfrac{1}{\mathcal{C}(x)}\sum\limits_{\forall{j}}f(x_{i},x_{j})g(x_{j})$
    |  | (9) |'
- en: 'where $i$ is the index of the output location where the response value needs
    to be calculated, $j$ is the index of all possible locations, $x$ and $y$ are
    the input and output data with the same dimensions, $f$ is a function to calculate
    the correlation between $i$ and $j$, $g$ is the function which calculates the
    feature representation of input data and $\mathcal{C}(x)$ is the normalization
    factor. Here, $g$ is usually defined as: $g(x_{j})=W_{g}x_{j}$, where $W_{g}$
    is the weight matrix that needs to learn. It should be noted that $f$ has multiple
    choices such as Gaussian, dot product, and concatenation. Therefore, the non-local
    block can easily be added into existing deep CNNs.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $i$ 是需要计算响应值的输出位置的索引，$j$ 是所有可能位置的索引，$x$ 和 $y$ 是具有相同维度的输入和输出数据，$f$ 是计算 $i$
    和 $j$ 之间相关性的函数，$g$ 是计算输入数据特征表示的函数，而 $\mathcal{C}(x)$ 是归一化因子。这里，$g$ 通常定义为：$g(x_{j})=W_{g}x_{j}$，其中
    $W_{g}$ 是需要学习的权重矩阵。需要注意的是，$f$ 有多种选择，如高斯函数、点积和拼接。因此，非局部块可以很容易地添加到现有的深度 CNN 中。
- en: '![Refer to caption](img/9c2e4b8fef8bb4e698213f5bc65c48bc.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c2e4b8fef8bb4e698213f5bc65c48bc.png)'
- en: 'Figure 38: The network architecture of PFNL (Yi et al., [2019](#bib.bib133)).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 38：PFNL 的网络架构 (Yi et al., [2019](#bib.bib133))。
- en: 5.4.1 PFNL
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 PFNL
- en: 'The progressive fusion non-local (PFNL) (Yi et al., [2019](#bib.bib133)) method
    is illustrated in Fig. [38](#S5.F38 "Figure 38 ‣ 5.4 Non-Local Methods ‣ 5 Methods
    without Alignment ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"). It mainly includes three parts: a non-local resblock, progressive fusion
    residual blocks (PFRB) and an upsampling block.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '逐步融合非局部 (PFNL) (Yi et al., [2019](#bib.bib133)) 方法如图 [38](#S5.F38 "Figure 38
    ‣ 5.4 Non-Local Methods ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey") 所示。它主要包括三个部分：非局部残差块、逐步融合残差块 (PFRB)
    和一个上采样块。'
- en: PFNL uses non-local residual blocks to extract spatio-temporal features, and
    PFRB is proposed to fuse them. Finally, the output through a sub-pixel convolutional
    layer is added to the input frame that is up-sampled by the bicubic interpolation,
    which is the final super-resolution result. PFRB is composed of three convolutional
    layers. Firstly, the input frames are convoluted with the 3$\times$3 kernels,
    respectively, then the output feature maps are concatenated, and the channel dimension
    is reduced by performing the 1$\times$1 convolution. And the results are concatenated
    with the previous convoluted feature maps, respectively, and conducted with a
    3$\times$3 convolution. The final results are added to each input frame to obtain
    the output for current PFRB.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: PFNL 使用非局部残差块提取时空特征，PFRB 被提出用于融合这些特征。最后，通过子像素卷积层的输出与通过双三次插值上采样的输入帧相加，得到最终的超分辨率结果。PFRB
    由三个卷积层组成。首先，输入帧分别与 $3\times3$ 核进行卷积，然后将输出特征图拼接，通过 $1\times1$ 卷积减少通道维度。结果与先前卷积的特征图分别拼接，并进行
    $3\times3$ 卷积。最终结果与每个输入帧相加，以获得当前 PFRB 的输出。
- en: 5.5 Other
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 其他
- en: The methods in this sub-category do not utilize the initial feature extractions
    mentioned above. They may combine multiple techniques for super-resolution.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 该子类别的方法不利用上述初始特征提取。它们可能结合多种技术进行超分辨率处理。
- en: '![Refer to caption](img/bec359b23136a95f683dc225b3c9dd13.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bec359b23136a95f683dc225b3c9dd13.png)'
- en: 'Figure 39: The network architecture of RBPN (Haris et al., [2019](#bib.bib34)),
    where $\copyright$ denotes concatenation, $\ominus$ is element subtraction, and
    MISR denotes multi-image super-resolution.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 39：RBPN 的网络架构 (Haris et al., [2019](#bib.bib34))，其中 $\copyright$ 表示拼接，$\ominus$
    表示元素减法，MISR 表示多图像超分辨率。
- en: 5.5.1 RBPN
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 RBPN
- en: 'The recurrent back-projection network (RBPN)^(21)^(21)21Code: https://github.com/alterzero/RBPN-PyTorch (Haris
    et al., [2019](#bib.bib34)) is inspired by the back-projection algorithm (Irani
    and Peleg, [1991](#bib.bib48), [1993](#bib.bib49); Haris et al., [2018](#bib.bib33)).
    RBPN mainly consists of one feature extraction module, a projection module, and
    a reconstruction module, and its architecture is shown in Fig. [39](#S5.F39 "Figure
    39 ‣ 5.5 Other ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on
    Deep Learning: A Comprehensive Survey").'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '循环反投影网络 (RBPN)^(21)^(21)21代码: https://github.com/alterzero/RBPN-PyTorch  (Haris
    et al., [2019](#bib.bib34)) 的灵感来源于反投影算法 (Irani 和 Peleg, [1991](#bib.bib48), [1993](#bib.bib49);
    Haris et al., [2018](#bib.bib33))。RBPN 主要由一个特征提取模块、一个投影模块和一个重建模块组成，其架构如图 [39](#S5.F39
    "Figure 39 ‣ 5.5 Other ‣ 5 Methods without Alignment ‣ Video Super-Resolution
    Based on Deep Learning: A Comprehensive Survey") 所示。'
- en: 'The feature extraction module includes two operations: One is to extract the
    features of the target frame, and the other is to extract the feature from the
    concatenation of the target frame, the neighboring frame, and the calculated optical
    flow which is from the neighboring frame to the target frame, and then perform
    alignment implicitly. The optical flow is obtained by the pyflow^(22)^(22)22https://github.com/pathak22/pyflow
    method. The projection module consists of an encoder and a decoder. The encoder
    is composed of a multiple image super-resolution (MISR), a single image super-resolution
    (SISR) and residual blocks (denoted as ResBlock). The decoder consists of ResBlock
    and a strided convolution, and it takes the output of the previous encoder as
    input to produce LR features for the encoder of the next projection module. The
    concatenation of the target frame, the next neighboring frame and pre-computed
    optical flow are input to the feature extraction module, whose output is also
    for the encoder in the next projection module. The above process does not stop
    until all neighboring frames are processed. That is, projection is used recurrently,
    which is the reason of the words “recurrent back-projection network”. Finally,
    the reconstruction module takes the output of the encoder in each projection module
    by the mean of concatenation as input to produce the final SR result.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取模块包括两个操作：一个是提取目标帧的特征，另一个是从目标帧、相邻帧以及从相邻帧到目标帧的计算光流的拼接中提取特征，并隐式地执行对齐。光流是通过pyflow^(22)^(22)22https://github.com/pathak22/pyflow方法获得的。投影模块由编码器和解码器组成。编码器由多个图像超分辨率（MISR）、单幅图像超分辨率（SISR）和残差块（标记为ResBlock）组成。解码器由ResBlock和步幅卷积组成，它将前一个编码器的输出作为输入，以生成用于下一个投影模块编码器的LR特征。目标帧、下一个相邻帧和预计算的光流的拼接作为输入传递给特征提取模块，其输出也作为下一个投影模块中编码器的输入。上述过程会持续进行，直到处理完所有相邻帧。即，投影被递归使用，这就是“递归回投影网络”这一术语的由来。最后，重建模块将每个投影模块中编码器的输出通过拼接的平均值作为输入，以生成最终的SR结果。
- en: '![Refer to caption](img/397af488c403436e53325dfc9fa47bf7.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/397af488c403436e53325dfc9fa47bf7.png)'
- en: 'Figure 40: The network architecture of STARnet (Haris et al., [2020](#bib.bib35)).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 40：STARnet的网络架构（Haris等，[2020](#bib.bib35)）。
- en: 5.5.2 STARnet
  id: totrans-305
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 STARnet
- en: 'The architecture of space-time-aware multi-resolution networks (STARnet) (Haris
    et al., [2020](#bib.bib35)) is shown in Fig. [41](#S5.F41 "Figure 41 ‣ 5.5.2 STARnet
    ‣ 5.5 Other ‣ 5 Methods without Alignment ‣ Video Super-Resolution Based on Deep
    Learning: A Comprehensive Survey"). STARnet is an end-to-end network that can
    simultaneously process video super-resolution and video interpolation. It consists
    of the following three stages: initialization, refinement and reconstruction.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 时空感知多分辨率网络（STARnet）的架构（Haris等，[2020](#bib.bib35)）如图[41](#S5.F41 "图 41 ‣ 5.5.2
    STARnet ‣ 5.5 其他 ‣ 5 无对齐方法 ‣ 基于深度学习的视频超分辨率：综合综述")所示。STARnet是一个端到端的网络，可以同时处理视频超分辨率和视频插值。它由以下三个阶段组成：初始化、优化和重建。
- en: In the initialization stage, STARnet receives four parts of inputs including
    two LR RGB frames and their bidirectional flow images. In this stage, the two
    spatial super-resolution (S-SR) modules can execute super-resolution to the two
    LR frames by DBPN (Haris et al., [2018](#bib.bib33)) or RBPN (Haris et al., [2019](#bib.bib34))
    and re-generate their LR counterparts by a similar network to prepare for frame
    interpolation in both LR and HR spaces in the spatio-temporal super-resolution
    (ST-SR) module. Meanwhile, the motion module align the bidirectional flow images.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化阶段，STARnet接收包括两个LR RGB帧及其双向流图像在内的四部分输入。在此阶段，两个空间超分辨率（S-SR）模块可以通过DBPN（Haris等，[2018](#bib.bib33)）或RBPN（Haris等，[2019](#bib.bib34)）对两个LR帧进行超分辨率处理，并通过类似的网络重新生成它们的LR对应帧，为在LR和HR空间的时空超分辨率（ST-SR）模块中进行帧插值做准备。同时，运动模块对齐双向流图像。
- en: '![Refer to caption](img/c154bc913de43a71b05c9a3d624ca2c6.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c154bc913de43a71b05c9a3d624ca2c6.png)'
- en: 'Figure 41: The network architecture of DNSTNet (Sun et al., [2020](#bib.bib111)).'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 41：DNSTNet的网络架构（Sun 等，[2020](#bib.bib111)）。
- en: 5.5.3 DNSTNet
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3 DNSTNet
- en: DNSTNet (Sun et al., [2020](#bib.bib111)) is the video super-resolution via
    the Dense Non-local Spatial-Temporal convolutional Network. Here, the dense feature
    extraction sub-network composes of the Short-term Temporal Dependency Extraction
    Block (S-TBlock), Long-term TBlock (L-TBlock), and dense connections, as shown
    in this figure. It utilizes 3D convolution to capture short-term temporal dependency
    existing from adjacent frames in S-TBlock, and the bidirectional ConvLSTM for
    capturing long-term temporal dependency in L-TBlock. It also proposes a region-level
    nonlocal block following the dense feature extraction to exploit the global information,
    and to enlarge the limited receptive field of 3D convolution and ConvLSTM. This
    non-local network divides the feature maps into multiple patches and processes
    them respectively to decrease the computational cost. To sum up, DNSTNet adopts
    multiple modules to improve the performance of VSR.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: DNSTNet（Sun 等，[2020](#bib.bib111)）是通过密集非局部空间时间卷积网络实现的视频超分辨率。这里，密集特征提取子网络包括短期时间依赖提取块（S-TBlock）、长期
    TBlock（L-TBlock）和密集连接，如图所示。它利用 3D 卷积在 S-TBlock 中捕获相邻帧之间的短期时间依赖关系，以及在 L-TBlock
    中使用双向 ConvLSTM 捕获长期时间依赖关系。它还提出了一种区域级非局部块，跟随密集特征提取以利用全局信息，并扩大 3D 卷积和 ConvLSTM 的有限感受野。该非局部网络将特征图划分为多个补丁并分别处理，以降低计算成本。总之，DNSTNet
    采用多个模块来提升 VSR 的性能。
- en: 'Although DNSTNET uses a 3D convolution module, an LSTM module and a non-local
    sub-network, it does not imply better performance than EDVR and DSMC. As it is
    known, the network architecture is one of the important factors to affect its
    performance, other factors including the training strategy, and iteration number
    also influence its performance. Compared with the methods: EDVR and DSMC, the
    training strategy of DNSTNET is probably not sophisticated designed. It is a common
    initializing method. But EDVR is initialized by parameters from a shallower similar
    network. This can boost the performance. DSMC also has a deeper structure, which
    may be helpful to improve the performance. Moreover, in DNSTNET, too many features
    without selection through the dense feature concatenation are input to the non-local
    block for computation. These features may bring redundant information, which results
    in performance degradation. While in DSMC, the extracted features are refined
    through the U3D-RDN module before they are input to a non-local block. This processing
    can enhance the performance.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DNSTNET 使用了 3D 卷积模块、LSTM 模块和非局部子网络，但这并不意味着其性能优于 EDVR 和 DSMC。众所周知，网络架构是影响性能的重要因素之一，其他因素如训练策略和迭代次数也会影响性能。与
    EDVR 和 DSMC 方法相比，DNSTNET 的训练策略可能并未经过精心设计，而是一种常见的初始化方法。而 EDVR 是通过来自较浅层类似网络的参数进行初始化，这可以提升性能。DSMC
    也具有更深的结构，这可能有助于提高性能。此外，在 DNSTNET 中，未经过选择的过多特征通过密集特征拼接输入到非局部块进行计算，这些特征可能带来冗余信息，从而导致性能下降。而在
    DSMC 中，提取的特征在输入到非局部块之前通过 U3D-RDN 模块进行精炼，这种处理可以增强性能。
- en: 'Table 2: Some widely used video super-resolution datasets. Note that ’*’ represents
    unknown information.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：一些广泛使用的视频超分辨率数据集。请注意，'*' 表示未知信息。
- en: '| Dataset | Year | Type | Download Link | Video Number | Resolution | Color
    Space |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 类型 | 下载链接 | 视频数量 | 分辨率 | 色彩空间 |'
- en: '| YUV25 | * | Train | [https://media.xiph.org/video/derf/](https://media.xiph.org/video/derf/)
    | 25 | $386\times 288$ | YUV |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| YUV25 | * | 训练 | [https://media.xiph.org/video/derf/](https://media.xiph.org/video/derf/)
    | 25 | $386\times 288$ | YUV |'
- en: '| TDTFF | * | Test | [www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html](www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html)
    | 5 | $648\times 528$ for Turbine, $960\times 530$ for Dancing ,$700\times 600$
    for Treadmill, and $1000\times 580$ for Flag, $990\times 740$ for Fan | YUV |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| TDTFF | * | 测试 | [www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html](www.wisdom.weizmann.ac.il/~vision/SingleVideoSR.html)
    | 5 | $648\times 528$（涡轮），$960\times 530$（舞蹈），$700\times 600$（跑步机），$1000\times
    580$（旗帜），$990\times 740$（风扇） | YUV |'
- en: '| Vid4 | 2011 | Test | [https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m](https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m)
    | 4 | $720\times 480$ for Foliage and Walk, $720\times 576$ for Calendar, and
    $704\times 576$ for City | RGB |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Vid4 | 2011 | 测试 | [https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m](https://drive.google.com/drive/folders/10-gUO6zBeOpWEamrWKCtSkkUFukB9W5m)
    | 4 | $720\times 480$（树叶和步行），$720\times 576$（日历），$704\times 576$（城市） | RGB |'
- en: '| YUV21 | 2014 | Test | [http://www.codersvoice.com/a/webbase/video/08/152014/130.html](http://www.codersvoice.com/a/webbase/video/08/152014/130.html)
    | 21 | $352\times 288$ | YUV |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| YUV21 | 2014 | Test | [http://www.codersvoice.com/a/webbase/video/08/152014/130.html](http://www.codersvoice.com/a/webbase/video/08/152014/130.html)
    | 21 | $352\times 288$ | YUV |'
- en: '| Venice | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 威尼斯 | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
- en: '| Myanmar | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 缅甸 | 2014 | Train | [https://www.harmonicinc.com/free-4k-demo-footage/](https://www.harmonicinc.com/free-4k-demo-footage/)
    | 1 | $3,840\times 2,160$ | RGB |'
- en: '| CDVL | 2016 | Train | [http://www.cdvl.org/](http://www.cdvl.org/) | 100
    | $1,920\times 1,080$ | RGB |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| CDVL | 2016 | Train | [http://www.cdvl.org/](http://www.cdvl.org/) | 100
    | $1,920\times 1,080$ | RGB |'
- en: '| UVGD^(21)^(21)21UVGD denotes the Ultra Video Group Database | 2017 | Test
    | [http://ultravideo.cs.tut.fi/](http://ultravideo.cs.tut.fi/) | 7 | $3,840\times
    2,160$ | YUV |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| UVGD^(21)^(21)21UVGD 表示超视频组数据库 | 2017 | Test | [http://ultravideo.cs.tut.fi/](http://ultravideo.cs.tut.fi/)
    | 7 | $3,840\times 2,160$ | YUV |'
- en: '| LMT^(22)^(22)22LMT denotes the LIVE video quality assessment database, the
    MCL-V database, and the TUM 1080p dataset. | 2017 | Train | [http://mcl.usc.edu/mcl-v-database](http://mcl.usc.edu/mcl-v-database),
    [http://live.ece.utexas.edu/research/quality/live_video.html](http://live.ece.utexas.edu/research/quality/live_video.html),[https://vision.in.tum.de/data/datasets](https://vision.in.tum.de/data/datasets)
    | * | $1,920\times 1,080$ | RGB |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| LMT^(22)^(22)22LMT 表示 LIVE 视频质量评估数据库、MCL-V 数据库和 TUM 1080p 数据集。 | 2017 | Train
    | [http://mcl.usc.edu/mcl-v-database](http://mcl.usc.edu/mcl-v-database), [http://live.ece.utexas.edu/research/quality/live_video.html](http://live.ece.utexas.edu/research/quality/live_video.html),[https://vision.in.tum.de/data/datasets](https://vision.in.tum.de/data/datasets)
    | * | $1,920\times 1,080$ | RGB |'
- en: '| Vimeo-90K | 2019 | Train+Test | [http://toflow.csail.mit.edu/](http://toflow.csail.mit.edu/)
    | 91,701 | $448\times 256$ | RGB |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Vimeo-90K | 2019 | Train+Test | [http://toflow.csail.mit.edu/](http://toflow.csail.mit.edu/)
    | 91,701 | $448\times 256$ | RGB |'
- en: '| REDS^(23)^(23)23REDS denotes the REalistic and Diverse Scenes | 2019 | Train+Test
    | [https://seungjunnah.github.io/Datasets/reds.html](https://seungjunnah.github.io/Datasets/reds.html)
    | 270 | $1,280\times 720$ | RGB |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| REDS^(23)^(23)23REDS 表示真实且多样的场景 | 2019 | Train+Test | [https://seungjunnah.github.io/Datasets/reds.html](https://seungjunnah.github.io/Datasets/reds.html)
    | 270 | $1,280\times 720$ | RGB |'
- en: 'Table 3: Some major video super-resolution competitions. Note that ’EDVR+’
    stands for a method based on EDVR, and ’*’ represents unknown information.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：一些主要的视频超分辨率比赛。注意“EDVR+”表示基于 EDVR 的方法，而“*”表示未知信息。
- en: '| Name | Year | Organizer | Location | Website | Dataset | Scale | Champion
    | PSNR | SSIM |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 主办方 | 地点 | 网站 | 数据集 | 缩放 | 冠军 | PSNR | SSIM |'
- en: '| NTIRE 2019 Video Restoration and Enhancement Challenges | 2019 | CVPR | ​Long
    Beach, California​ | [https://data.vision.ee.ethz.ch/cvl/ntire19/](https://data.vision.ee.ethz.ch/cvl/ntire19/)
    | REDS | $\times$4 | EDVR (Wang et al., [2019a](#bib.bib122)) | 31.79 | 0.8962
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| NTIRE 2019 视频恢复与增强挑战 | 2019 | CVPR | ​加利福尼亚州长滩​ | [https://data.vision.ee.ethz.ch/cvl/ntire19/](https://data.vision.ee.ethz.ch/cvl/ntire19/)
    | REDS | $\times$4 | EDVR (Wang et al., [2019a](#bib.bib122)) | 31.79 | 0.8962
    |'
- en: '| YOUKU Video Super-Resolution and Enhancement Challenge | 2019 | Alibaba |
    Hangzhou, China | [https://tianchi.aliyun.com/competition/entrance/231711/introduction](https://tianchi.aliyun.com/competition/entrance/231711/introduction)
    | ​Youku-VESR​ | $\times$4 | VESR-Net (Chen et al., [2020](#bib.bib10)) | 37.85
    | * |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| YOUKU 视频超分辨率与增强挑战 | 2019 | Alibaba | 中国杭州 | [https://tianchi.aliyun.com/competition/entrance/231711/introduction](https://tianchi.aliyun.com/competition/entrance/231711/introduction)
    | ​Youku-VESR​ | $\times$4 | VESR-Net (Chen et al., [2020](#bib.bib10)) | 37.85
    | * |'
- en: '| AIM 2019 Challenge on Video Extreme Super-Resolution | 2019 | ECCV | Hong
    Kong, China | [https://www.aim2019.org/](https://www.aim2019.org/) | Vid3oC |
    $\times$16 | EDVR+ | 22.53 | 0.6400 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| AIM 2019 视频极限超分辨率挑战 | 2019 | ECCV | 中国香港 | [https://www.aim2019.org/](https://www.aim2019.org/)
    | Vid3oC | $\times$16 | EDVR+ | 22.53 | 0.6400 |'
- en: '| Mobile Video Restoration Challenge | 2019 | ​ICIP & Kwai​ | * | [https://www.kuaishou.com/activity/icip2019](https://www.kuaishou.com/activity/icip2019)
    | * | * | * | * | * |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 移动视频恢复挑战 | 2019 | ​ICIP & Kwai​ | * | [https://www.kuaishou.com/activity/icip2019](https://www.kuaishou.com/activity/icip2019)
    | * | * | * | * | * |'
- en: '| AIM 2020 Challenge on Video Extreme Super-Resolution | 2020 | ECCV | ​Boston,
    Massachusetts​ | [http://aim2020.org/](http://aim2020.org/) | Vid3oC | $\times$16
    | ​EVESRNet (Dario et al., [2020](#bib.bib16))​ | 22.83 | 0.6450 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| AIM 2020 视频极限超分辨率挑战赛 | 2020 | ECCV | ​ 马萨诸塞州波士顿 ​ | [http://aim2020.org/](http://aim2020.org/)
    | Vid3oC | $\times$16 | ​EVESRNet (Dario et al., [2020](#bib.bib16))​ | 22.83
    | 0.6450 |'
- en: '| Mobile AI 2021 Real-Time Video Super-Resolution Challenge | 2021 | CVPR |
    ​ VIRTUAL ​ | [https://ai-benchmark.com/workshops/mai/2021/](https://ai-benchmark.com/workshops/mai/2021/)
    | REDS | $\times$4 | Diggers (Ignatov et al., [2021](#bib.bib46)) | 28.33 | 0.8112
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Mobile AI 2021 实时视频超分辨率挑战赛 | 2021 | CVPR | ​ 虚拟 ​ | [https://ai-benchmark.com/workshops/mai/2021/](https://ai-benchmark.com/workshops/mai/2021/)
    | REDS | $\times$4 | Diggers (Ignatov et al., [2021](#bib.bib46)) | 28.33 | 0.8112
    |'
- en: '| NTIRE 2021 Video Super-Resolution Challenge | 2021 | CVPR | ​ VIRTUAL ​ |
    [https://data.vision.ee.ethz.ch/cvl/ntire21/](https://data.vision.ee.ethz.ch/cvl/ntire21/)
    | REDS | $\times$4 | BasicVSR++ (Chan et al., [2021d](#bib.bib9)) | 33.36 | 0.9218
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| NTIRE 2021 视频超分辨率挑战赛 | 2021 | CVPR | ​ 虚拟 ​ | [https://data.vision.ee.ethz.ch/cvl/ntire21/](https://data.vision.ee.ethz.ch/cvl/ntire21/)
    | REDS | $\times$4 | BasicVSR++ (Chan et al., [2021d](#bib.bib9)) | 33.36 | 0.9218
    |'
- en: In summary, the non-local based methods introduce attention mechanisms into
    VSR tasks. They can establish effective dependence of spatio-temporal information
    by extending the receptive field to the global. However, the non-local modules
    used in them need to calculate the response at each position by attending to all
    other positions and computing a weighted average of the features in all positions.
    Thus, this incurs high computational cost, and some efforts can be made to reduce
    the computational overhead of the methods.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，基于非局部的方法将注意力机制引入了 VSR 任务中。它们通过扩展感受野到全局来建立时空信息的有效依赖。然而，它们使用的非局部模块需要通过关注所有其他位置并计算所有位置特征的加权平均值来计算每个位置的响应。因此，这会带来高计算成本，可能需要一些努力来减少方法的计算开销。
- en: Moreover, the methods without alignment rely on the non-linear capability of
    the neural network to learn the motion correlation between frames for video super-resolution.
    They do not utilize additional modules to align frames. The learning ability largely
    depends on the design of the deep neural network. And an elaborate design is more
    likely leading to higher performance for video super-resolution.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，不依赖对齐的方法依赖于神经网络的非线性能力来学习帧间的运动相关性以进行视频超分辨率。它们不使用额外的模块来对齐帧。学习能力在很大程度上依赖于深度神经网络的设计。精心设计的网络更可能在视频超分辨率上表现出更高的性能。
- en: In addition, we discuss the deeper connections between all the methods below.
    1) The methods such as EDVR, DNLN, TDAN, D3Dnet and VESR-Net, which belong to
    the deformable convolution category, all attempt to overcome the flaw of optical
    flow-based methods by using the DConv structure. The estimation of optical flow
    is inaccurate when dealing with complex motions and varying illumination, while
    the receptive field of the convolution kernel can be expanded by utilizing DConv.
    And the network can better capture complex motions and illumination changes. 2)
    The methods such as DUF, FSTRN, 3DSRnet and DSMC all employ 3D convolutional layers
    to learn spatial and temporal features simultaneously instead of the 2D convolution
    from video data. Besides, they also try to avoid the inaccuracy of motion estimation
    and compensation when complex motions involve by designing new network structures.
    3) The methods such as BRCN, STCN, RISTN, RLSP, RSDN and BasicVSR exploit long-term
    contextual information contained in video frames by using bidirectional recurrent
    convolutional networks. The bidirectional RCNN can utilize temporal dependency
    from both previous and future frames through the combination of a forward recurrent
    network and a backward recurrent network. 4) The methods such as RVSR, STCN, BRCN,
    EDVR, DNLN, TDAN, D3DNet, VESR-Net, DUF, 3DSRNet and DSMC involve the dealing
    with complex motions in the videos. 5) The methods such as MuCAN (in MEMC class),
    EDVR (in DC class), VESR-Net (in DC class) and PFNL (in non-local class) attempt
    to capture the global dependency between different positions across frame. Specifically,
    the TSA module in the EDVR method assigns pixel-level weights on each frame for
    fusion. MuCAN, VESR-Net, and PFNL all design non-local modules to correlate different
    patches, which improve the ability to capture motion information. 6) The methods
    such as DRVSR, MultiBoot VSR, and DSMC all address the video super-resolution
    with multiple scaling factors. They not only consider x4 scale, but also regard
    $\times$2, $\times$3, or $\times$8 scales. 7) The methods such as MultiBoot VSR,
    PFNL and RBPN all pay attention to improve the training strategies. For example,
    PFNL adopts residual learning to stabilize the training process.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还深入探讨了下面所有方法之间的联系。1) 如EDVR、DNLN、TDAN、D3Dnet和VESR-Net等方法，属于可变形卷积类别，都试图通过使用DConv结构克服基于光流的方法的缺陷。当处理复杂运动和变化的光照时，光流的估计是不准确的，而通过利用DConv，可以扩展卷积核的感受野，从而使网络更好地捕捉复杂的运动和光照变化。2)
    如DUF、FSTRN、3DSRnet和DSMC等方法都采用3D卷积层同时学习空间和时间特征，而不是来自视频数据的2D卷积。此外，它们还尝试通过设计新的网络结构来避免复杂运动中涉及的运动估计和补偿的不准确性。3)
    如BRCN、STCN、RISTN、RLSP、RSDN和BasicVSR等方法利用双向递归卷积网络来挖掘视频帧中的长期上下文信息。双向RCNN通过结合前向递归网络和后向递归网络，能够利用来自前后帧的时间依赖性。4)
    如RVSR、STCN、BRCN、EDVR、DNLN、TDAN、D3DNet、VESR-Net、DUF、3DSRNet和DSMC等方法都涉及视频中的复杂运动处理。5)
    如MuCAN（在MEMC类中）、EDVR（在DC类中）、VESR-Net（在DC类中）和PFNL（在非局部类中）等方法试图捕捉帧之间不同位置的全局依赖性。具体来说，EDVR方法中的TSA模块为每帧分配像素级权重进行融合。MuCAN、VESR-Net和PFNL都设计了非局部模块来关联不同的图块，从而提高了捕捉运动信息的能力。6)
    如DRVSR、MultiBoot VSR和DSMC等方法都处理具有多个缩放因子的超分辨率视频。它们不仅考虑了x4缩放，还考虑了$\times$2、$\times$3或$\times$8缩放。7)
    如MultiBoot VSR、PFNL和RBPN等方法都关注于改进训练策略。例如，PFNL采用残差学习来稳定训练过程。
- en: 6 Performance Comparisons
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 性能比较
- en: 6.1 Datasets and Competitions
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据集和竞赛
- en: 'Details of some of the most popular datasets used in VSR tasks are summarized
    in Table [2](#S5.T2 "Table 2 ‣ 5.5.3 DNSTNet ‣ 5.5 Other ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). The
    most widely-used dataset for training is Vimeo-90K, since it is currently the
    largest VSR dataset with real scenes. The most popular dataset for testing is
    Vid4, whose frames contain more high-frequency details than others. Thus, Vid4
    is frequently used for evaluating the performance of VSR methods. REDS includes
    videos with extremely large movement, which is challenging for VSR methods.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S5.T2 "Table 2 ‣ 5.5.3 DNSTNet ‣ 5.5 Other ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")总结了一些在VSR任务中使用的最受欢迎的数据集的详细信息。最广泛使用的训练数据集是Vimeo-90K，因为它目前是最大的包含真实场景的VSR数据集。最受欢迎的测试数据集是Vid4，其帧包含比其他数据集更多的高频细节。因此，Vid4常用于评估VSR方法的性能。REDS包括了具有极大运动的视频，这对VSR方法来说是一个挑战。'
- en: 'Besides, we also summarize several international competitions on video super-resolution
    in Table [3](#S5.T3 "Table 3 ‣ 5.5.3 DNSTNet ‣ 5.5 Other ‣ 5 Methods without Alignment
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"). The
    NTIRE 2019 Challenge (Nah et al., [2019a](#bib.bib91), [b](#bib.bib92)) aims at
    recovering videos with large movements and diverse real-world scenes. Its winning
    solution is EDVR (Wang et al., [2019a](#bib.bib122)), which may be one of the
    most popular works for VSR. The AIM Challenges in 2019 (Fuoli et al., [2019b](#bib.bib26))
    and 2020 (Fuoli et al., [2020](#bib.bib27)) both encourage solutions of VSR with
    large scale factors. A method enhanced from EDVR won the AIM 2019 Challenge, while
    EVESRNet (Dario et al., [2020](#bib.bib16)) won the AIM 2020 Challenge. Besides,
    the YOUKU Video Super-Resolution and Enhancement Challenge, and Mobile Video Restoration
    Challenge in 2019 are both for videos which are more relevant to entertainment.
    The winning solution of YOUKU challenge is VESR-Net  (Chen et al., [2020](#bib.bib10)).
    The Mobile AI 2021 Real-Time Video Super-Resolution challenge (Ignatov et al.,
    [2021](#bib.bib46)) evaluated the solutions on an OPPO Find X2 smartphone GPU.
    The most recent NTIRE 2021 Challenge on Video Super-Resolution gauges the state-of-the-art (Son
    et al., [2021](#bib.bib109)), its winner being BasicVSR++ (Chan et al., [2021d](#bib.bib9)).
    These competitions are making great contributions to the development of video
    super-resolution and helping develop new methods for various video super-resolution
    applications.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison of all the methods on the datasets with scale factor $\times
    4$. Note that ‘Internet’ means that the dataset is collected from the internet.
    ‘*’ denotes that the source of the dataset is unknown, and ‘-’ indicates that
    the method does not be tested on the datasets.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Training Set | Test Set | Channel | Params.(MB) | BI | BD |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: '| PSNR | SSIM | PSNR | SSIM |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
- en: '| Deep-DE | * | city+temple+penguin | * | $1.11^{[1]}$ | - | - | 29.00 | 0.8870
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '| VSRnet | Myanmar | Vid4 | Y | $0.27^{[2]}$ | 24.84 | 0.7049 | - | - |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '| Myanmar–T | Y | 31.85 | 0.8834 | - | - |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| VESPCN | CDVL | Vid4 | Y | $0.88^{[2]}$ | 25.35 | 0.7557 | - | - |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| DRVSR | * | Vid4 | Y | $2.17^{[3]}$ | 25.52 | 0.7600 | - | - |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| SPMCS | Y | 29.69 | 0.8400 | - | - |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| RVSR | LMT | Vid4+temple+penguin | Y | - | 28.05 | - | - | - |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| UVGD | Y | 39.71 | - | - | - |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| FRVSR | Vimeo-90K | Vid4 | Y | $2.81^{[3]}$ | - | - | 26.69 | 0.8103 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 35.64 | 0.9319 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| SOFVSR | CDVL | DAVIS-10 | Y | $1.71^{[3]}$ | 34.32 | 0.9250 | 34.27 | 0.9250
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 26.01 | 0.7710 | 26.19 | 0.7850 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| TecoGAN | * | ToS | Y | 3.00 | - | - | 32.75 | - |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | - | - | 25.89 | - |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| TOFlow | Vimeo-90K | Vid4 | Y | $1.41$ | 23.54 | 0.8070 | - | - |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 33.08 | 0.9417 | - | - |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| MMCNN | * | Vid4 | Y | 10.58 | 26.28 | 0.7844 | - | - |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| Myanmar-T | Y | 33.06 | 0.9040 | - | - |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| YUV21 | Y | 28.90 | 0.7983 | - | - |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| Vid4+temple+penguin | Y | 28.97 | - | - | - |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| MEMC-Net | Vimeo-90K | Vimeo-90K-T | Y | - | 33.47 | 0.9470 | - | - |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 24.37 | 0.8380 | - | - |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| RRCN | Myanmar | Myanmar-T | Y | - | 32.35 | 0.9023 | - | - |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 25.86 | 0.7591 | - | - |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| YUV21 | Y | 29.08 | 0.7986 | - | - |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| RTVSR | harmonicinc.com | Vid4 | Y | 15.00 | 26.36 | 0.7900 | - | - |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| Vid4+temple+penguin | Y | 29.03 | - | - | - |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| MultiBoot VSR | REDS | REDS-T | RGB | 60.86 | 31.00 | 0.8822 | - | - |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| MuCAN | Vimeo-90K | Vimeo-90K-T | Y | 19.90 | 37.32 | 0.9465 | - | - |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 25.70 | 30.88 | 0.8750 | - | - |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| IconVSR | Vimeo-90K | Vimeo-90K-T | Y | 8.70 | 37.47 | 0.9476 | 37.84 | 0.9524
    |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 27.39 | 0.8279 | 28.04 | 0.8570 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| UDM10 | Y | - | - | 40.03 | 0.9694 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 31.67 | 0.8948 | - | - |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| EDVR | Vimeo-90K | Vid4 | Y | $20.60$ | 27.35 | 0.8264 | 27.85 | 0.8503 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 37.61 | 0.9489 | 37.81 | 0.9523 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 31.09 | 0.8800 | 28.88 | 0.8361 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| DNLN | Vimeo-90K | Vid4 | Y | 19.74 | 27.31 | 0.8257 | - | - |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| SPMCS | Y | 30.36 | 0.8794 | - | - |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| TDAN | Vimeo-90K | Vid4 | Y | $1.97^{[2]}$ | 26.24 | 0.7800 | 26.58 | 0.8010
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| D3Dnet | Vimeo-90K | Vid4 | Y | 2.58 | 26.52 | 0.7990 | - | - |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| VESR-Net | Youku-VESR | Youku-VESR-T | RGB | 21.65 | - | - | 35.97 | - |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| VSRResFeatGAN | Myanmar | Vid4 | Y | - | 25.51 | 0.7530 | - | - |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| FFCVSR | Venice+Myanmar | Vid4 | Y | - | 26.97 | 0.8300 | - | - |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| DUF | Vimeo-90K | Vid4 | Y | $5.82$ | - | - | 27.38 | 0.8329 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 36.87 | 0.9447 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | Y | 28.63 | 0.8251 | - | - |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| FSTRN | YUV25 | TDTFF | Y | - | - | - | 29.95 | 0.8700 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| 3DSRnet | largeSet | Vid4 | Y | $0.11^{[3]}$ | 25.71 | 0.7588 | - | - |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| DSMC | REDS | REDS4 | RGB | 11.58 | 30.29 | 0.8381 | - | - |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Vid4 | Y | 27.29 | 0.8403 | - | - |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| BRCN | YUV25 | Vid4 | Y | - | - | - | 24.43 | 0.6334 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| TDTFF | Y | - | - | 28.20 | 0.7739 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| STCN | * | Hollywood2 | Y | - | - | - | 34.58 | 0.9259 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| city+temple+penguin | * | - | - | 30.27 | 0.9103 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| RISTN |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '&#124; Vimeo-90K &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '| Vid4 | Y | 3.67 | 26.13 | 0.7920 | - | - |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| RLSP | Vimeo-90K | Vid4 | Y | $4.21$ | - | - | 27.48 | 0.8388 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 36.49 | 0.9403 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| RSDN | Vimeo-90K | Vid4 | Y | 6.19 | - | - | 27.92 | 0.8505 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | - | - | 37.23 | 0.9471 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| UDM10 | Y | - | - | 39.35 | 0.9653 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| PFNL | Vimeo-90K | Vid4 | Y | $3.00$ | 26.73 | 0.8029 | 27.16 | 0.8355 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 36.14 | 0.9363 | - | - |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 29.63 | 0.8502 | - | - |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| RBPN | Vimeo-90K | Vid4 | Y | $12.20$ | 27.12 | 0.8180 | - | - |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | Y | 37.07 | 0.9453 | 37.20 | 0.9458 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| REDS | REDS4 | RGB | 30.09 | 0.8590 | - | - |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| STARnet | Vimeo-90K | UCF101 | * | $111.61^{[4]}$ | 29.11 | 0.9240 | - |
    - |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| Vimeo-90K-T | * | 30.83 | 0.9290 | - | - |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| Middlebury | * | 27.16 | 0.8270 | - | - |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Middlebury | * | 27.16 | 0.8270 | - | - |'
- en: '| DNSTNet | Vimeo-90K | Vid4 | Y | - | 27.21 | 0.8220 | - | - |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| DNSTNet | Vimeo-90K | Vid4 | Y | - | 27.21 | 0.8220 | - | - |'
- en: '| Vimeo-90K-T | Y | 36.86 | 0.9387 | - | - |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| Vimeo-90K-T | Y | 36.86 | 0.9387 | - | - |'
- en: '| SPMCS | Y | 29.74 | 0.8710 | - | - |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| SPMCS | Y | 29.74 | 0.8710 | - | - |'
- en: 6.2 Performance of Methods
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 方法性能
- en: 'Moreover, we summarize the performance of the representative VSR methods with
    scale factor $4$ in Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions
    ‣ 6 Performance Comparisons ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey") in terms of both PSNR and SSIM. More experimental results
    for VSR tasks with magnification factors 2 and 3 are reported in Supplementary
    Materials. The degradation types are the bicubic downsampling with the image-resize
    function (BI) and Gaussian blurring and downsampling (BD). Note that part of the
    PSNR and SSIM are from their original works. And a simple comparison on the performance
    may not be fair, since the training data, the pre-processing, and the cropped
    area in videos are likely totally different in the methods. The details about
    the performance are listed to provide reference for readers.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们在表格[4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6 Performance
    Comparisons ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")中总结了代表性VSR方法在尺度因子$4$下的表现，包括PSNR和SSIM。更多关于放大因子为2和3的VSR任务的实验结果请见附录材料。退化类型包括使用图像缩放函数的双三次下采样（BI）和高斯模糊及下采样（BD）。请注意，部分PSNR和SSIM数据来自其原始工作。简单的性能比较可能不公平，因为训练数据、预处理和视频中的裁剪区域在各方法中可能完全不同。性能的详细信息列出以供读者参考。'
- en: 'According to Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6
    Performance Comparisons ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive
    Survey"), the top 5 methods in the $\times 4$ VSR task on Vimeo-90K-T dataset
    are as follows. The methods are denoted by (PSNR, BI/BD, Params.). IconVSR (37.84,
    BD, 8.70), EDVR (37.61, BI, 20.60), IconVSR (37.47, BI, 8.70), MuCAN (37.32, BI,
    19.90), and RSDN (37.23, BD, 6.19). The top 5 methods on Vid4 dataset are IconVSR
    (28.04, BD, 8.70), RSDN (27.92, BD, 6.19), EDVR (27.85, BD, 20.60), RLSP (27.48,
    BD, 4.21), and DUF (27.38, BD, 5.82). The top 4 methods on REDS4 dataset are IconVSR
    (31.67, BI, 8.70), EDVR (31.09, BI, 20.60), MuCAN (30.88, BI, 25.70), and DSMC
    (30.29, BI, 11.58). In the method evaluation, we compare the results on Y channel
    for Vimeo-90K-T and Vid4 datasets, and on RGB channel for REDS4\. PFNL and DNLN
    do not utilize all the test frames.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表格[4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6 Performance Comparisons
    ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey")，在Vimeo-90K-T数据集上$\times
    4$ VSR任务中排名前5的方法如下。方法以（PSNR，BI/BD，Params.）表示。IconVSR（37.84，BD，8.70），EDVR（37.61，BI，20.60），IconVSR（37.47，BI，8.70），MuCAN（37.32，BI，19.90），和RSDN（37.23，BD，6.19）。在Vid4数据集上的前5名方法是IconVSR（28.04，BD，8.70），RSDN（27.92，BD，6.19），EDVR（27.85，BD，20.60），RLSP（27.48，BD，4.21），和DUF（27.38，BD，5.82）。在REDS4数据集上的前4名方法是IconVSR（31.67，BI，8.70），EDVR（31.09，BI，20.60），MuCAN（30.88，BI，25.70），和DSMC（30.29，BI，11.58）。在方法评估中，我们比较了Vimeo-90K-T和Vid4数据集上Y通道的结果，以及REDS4上的RGB通道结果。PFNL和DNLN没有利用所有测试帧。'
- en: IconVSR and EDVR show superior performance on the three datasets. And IconVSR
    uses optical flow for feature alignment, a bidirectional recurrent network for
    temporal feature propagation, and an information-refill mechanism for feature
    refinement. With these properties, it outperforms some other methods in some cases,
    and achieves more performance gain with BD degradation than BI degradation on
    Vimeo-90K-T and Vid4\. EDVR employs cascaded multi-scale deformable convolutions
    for alignment, and TSA to fuse multiple frames. Unlike DNLN, which also adopts
    deformable convolutions, EDVR can capture multi-scale feature information. Compared
    with TDAN and D3Dnet, the architecture of EDVR is more complicated and may learn
    more information from inputs, though they all employ deformable convolutions for
    alignment. And EDVR costs 20.60 MB parameters, which is far more than other top
    networks. This may explain its better performance.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: IconVSR和EDVR在三个数据集上表现优异。IconVSR使用光流进行特征对齐，采用双向递归网络进行时间特征传播，并通过信息填充机制进行特征细化。凭借这些特性，它在某些情况下优于其他方法，并在Vimeo-90K-T和Vid4数据集中，BD退化相较于BI退化能获得更大的性能提升。EDVR采用级联多尺度可变形卷积进行对齐，并使用TSA融合多个帧。与也采用可变形卷积的DNLN不同，EDVR可以捕捉多尺度特征信息。与TDAN和D3Dnet相比，EDVR的结构更复杂，可能从输入中学习到更多信息，尽管它们都采用可变形卷积进行对齐。而且EDVR的参数量为20.60
    MB，远高于其他顶级网络，这可能解释了它的更好性能。
- en: For the Vid4 dataset, RLSP and RSDN both adopt recurrent convolutional neural
    network as backbone to utilize the temporal information contained in multiple
    frames. RSDN further divides a frame into structure and detail to process them
    respectively, and also exchange the information between them. This refined extraction
    attributes to its performance. PFNL proposes the non-local residual block to capture
    long-range spatio-temporal dependencies between frames, which may outperform some
    conventional MEMC-based methods.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Vid4数据集，RLSP和RSDN都采用递归卷积神经网络作为主干，以利用多帧中的时间信息。RSDN进一步将帧分为结构和细节，分别处理，并且在它们之间交换信息。这种精细的提取有助于其性能。PFNL提出了非局部残差块来捕捉帧间的长距离时空依赖关系，这可能优于一些传统的基于MEMC的方法。
- en: For the Vimeo-90K-T dataset, the performance of MuCAN is likely attributed to
    the two main modules, CN-CAM and TM-CAM. The former module can hierarchically
    aggregate information for handling large and subtle motion, and the latter one
    captures nonlocal communication within different feature resolutions. RSDN relies
    the information exchange between the structure and detail to gain better performance
    on Vimeo-90K-T. It is noticed that MuCAN has 19.90 MB parameters, which is far
    more than those of RSDN and IconVSR on this dataset.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Vimeo-90K-T数据集，MuCAN的性能可能归因于两个主要模块：CN-CAM和TM-CAM。前者模块可以分层聚合信息以处理大范围和细微的运动，后者模块捕捉不同特征分辨率中的非局部通信。RSDN依赖于结构和细节之间的信息交换，在Vimeo-90K-T上取得了更好的性能。注意到MuCAN的参数为19.90
    MB，这远远超过了RSDN和IconVSR在该数据集上的参数量。
- en: Moreover, for the REDS4 dataset, it is noticed that EDVR and MuCAN both have
    more than 20.0 MB parameters, which is far more than those of IconVSR and DSMC,
    though they are in the second and the third places on the top list. DSMC proposes
    the U3D-RDN module which learns coarse-to-fine spatio-temporal features, and MSCU,
    which decomposes an upsampling into multiple sub-tasks for full use of the intermediate
    results as well as a dual subnet for aiding the training. DSMC demonstrates superior
    performance to other 3D convolution methods.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于REDS4数据集，注意到EDVR和MuCAN的参数量均超过20.0 MB，这远远超过了IconVSR和DMSC，尽管它们在排行榜上位居第二和第三。DMSC提出了U3D-RDN模块，学习粗到细的时空特征，还有MSCU模块，它将上采样分解为多个子任务，以充分利用中间结果，并且有一个双子网络来辅助训练。DMSC在3D卷积方法中表现优越。
- en: 6.3 Guidelines for Model Selection
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 模型选择指南
- en: 'In this subsection, we provide some guidelines for readers to select different
    models according to the results in Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and
    Competitions ‣ 6 Performance Comparisons ‣ Video Super-Resolution Based on Deep
    Learning: A Comprehensive Survey"). For the super-resolving videos with realistic
    textures and rich details but without large motions, the following methods can
    be prime candidates: IconVSR, RSDN, EDVR, RLSP, DUF, DNLN, DSMC, PFNL, RBPN, and
    FRVSR. These methods are ordered according to the PSNR values on the Vid4 dataset,
    whose videos contain more high-frequency details. Among them, EDVR and DNLN both
    have more than 20.0 MB parameters, which are suitable for the applications without
    tight restriction on GPU memory. And the methods such as IconVSR, RSDN, RLSP,
    DUF, PFNL and FRVSR cost less than 10.0 MB model parameters, which might be more
    appropriate for the application of mobile devices and embedding systems.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们提供了一些指导方针，帮助读者根据表[4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions
    ‣ 6 Performance Comparisons ‣ Video Super-Resolution Based on Deep Learning: A
    Comprehensive Survey")中的结果选择不同的模型。对于纹理真实且细节丰富但没有大幅运动的视频，以下方法可以作为首选：IconVSR、RSDN、EDVR、RLSP、DUF、DNLN、DMSC、PFNL、RBPN和FRVSR。这些方法按照Vid4数据集上的PSNR值排序，视频包含更多高频细节。其中，EDVR和DNLN的参数均超过20.0
    MB，适合GPU内存没有严格限制的应用。而像IconVSR、RSDN、RLSP、DUF、PFNL和FRVSR等方法的模型参数少于10.0 MB，更适合移动设备和嵌入系统的应用。'
- en: When dealing with video sequences with complex and large motions, one can select
    the methods, IconVSR, EDVR, DSMC, RBPN, and PFNL. The performance of these methods
    is ranged in descending order and referred to their PSNR results on the REDS dataset.
    Similar to the above applications, the number of the parameters in EDVR exceeds
    20.0 MB, while those of IconVSR and PFNL are fewer than 10.0 MB.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 处理具有复杂和大幅运动的视频序列时，可以选择IconVSR、EDVR、DMSC、RBPN和PFNL等方法。这些方法的性能按其在REDS数据集上的PSNR结果降序排列。与上述应用类似，EDVR的参数量超过20.0
    MB，而IconVSR和PFNL的参数量少于10.0 MB。
- en: For generic videos except for the above two videos, we recommend the methods,
    IconVSR, EDVR, MuCAN, RSDN, RBPN, RLSP, PFNL and FRVSR. These methods are ordered
    according to the PSNR values on the Vimeo-90k-T dataset. The number of the parameters
    in EDVR is larger than 20.0 MB, and those of IconVSR, MuCAN, RSDN, RLSP, PFNL
    and FRVSR are fewer than 10.0 MB.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除上述两个视频之外的一般视频，我们推荐使用IconVSR、EDVR、MuCAN、RSDN、RBPN、RLSP、PFNL和FRVSR这些方法。这些方法按照Vimeo-90k-T数据集上的PSNR值排序。EDVR的参数数量超过20.0
    MB，而IconVSR、MuCAN、RSDN、RLSP、PFNL和FRVSR的参数数量少于10.0 MB。
- en: There are some additional tips for selecting the methods with alignment. When
    inaccurate motion estimation and alignment may introduce artifacts for videos
    with large motions or lighting changes, the deformable convolution-based methods
    are more robust for VSR tasks. When considering the online applications of video
    super-resolution, a unidirectional network may be the best candidate, where the
    information is sequentially propagated from the first frame to the last frame.
    While for offline applications, a bidirectional network in which the features
    can propagate forward and backward in time independently, is a better choice for
    VSR. In this case, the optical flow can be estimated both sequentially and reversely.
    It is known that the motion estimation is one critical step for the methods with
    alignment, which directly influences the performance of VSR methods. When more
    advanced estimation methods are proposed, they can be used to improve VSR’s performance.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择对齐方法时，有一些额外的提示。当不准确的运动估计和对齐可能为大范围运动或光照变化的视频引入伪影时，基于可变形卷积的方法在VSR任务中更具鲁棒性。在考虑视频超分辨率的在线应用时，单向网络可能是最佳候选方案，其中信息从第一帧依次传递到最后一帧。而对于离线应用，双向网络，其中特征可以独立地向前和向后传播，是VSR的更好选择。在这种情况下，光流可以顺序和逆向估计。已知运动估计是对齐方法的一个关键步骤，它直接影响VSR方法的性能。当提出更先进的估计方法时，它们可以用来提高VSR的性能。
- en: 7 Applications of Video Super-Resolution
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 视频超分辨率的应用
- en: By using VSR techniques, the resolution of video frames can be enhanced, and
    better visual quality and recognition accuracy can be achieved. It has a variety
    of applications, such as remote sensing, medical diagnoses, video decoding, and
    3D reconstruction.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用VSR技术，可以提升视频帧的分辨率，从而实现更好的视觉质量和识别准确性。它有多种应用场景，如遥感、医学诊断、视频解码和3D重建。
- en: 7.1 Video Decoding
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 视频解码
- en: In (Glaister et al., [2011](#bib.bib29)), a patch-based super-resolution method
    was presented to decode frames for video playback, and had been integrated in
    a video compression pipeline. Dai et al. ([2015](#bib.bib14)) proposed a VSR algorithm
    based on dictionary learning and sub-pixel motion compensation. This algorithm
    adopted multiple bilevel dictionaries for single-frame SR. Meanwhile, they presented
    a dictionary learning algorithm, where the dictionaries are trained from consecutive
    video frames. In (Liu and Cui, [2018](#bib.bib85)), an improved super-resolution
    reconstruction algorithm, which was a part of the proposed low bit-rate coding
    scheme, was applied to the decoded data for reconstructing high-definition videos.
    In (Umeda et al., [2018](#bib.bib117)), an anchored neighborhood regression SR
    method (Timofte et al., [2014](#bib.bib115)) was used for decoding in the proposed
    video coding system.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在(Glaister 等，[2011](#bib.bib29))中，提出了一种基于块的超分辨率方法，用于解码视频播放帧，并已集成到视频压缩流程中。Dai
    等 ([2015](#bib.bib14)) 提出了基于字典学习和亚像素运动补偿的VSR算法。该算法采用了多个双层字典进行单帧SR。同时，他们提出了一种字典学习算法，其中字典是从连续视频帧中训练得到的。在(Liu
    和 Cui，[2018](#bib.bib85))中，应用了一种改进的超分辨率重建算法，该算法是提出的低比特率编码方案的一部分，用于对解码数据进行高清晰度视频重建。在(Umeda
    等，[2018](#bib.bib117))中，提出的视频编码系统中使用了一种锚定邻域回归SR方法 (Timofte 等，[2014](#bib.bib115))。
- en: Kim et al. ([2018b](#bib.bib63)) proposed a hardware-friendly VSR algorithm
    which can upscale full-high-definition (FHD) video streams to their 4K ultra-high-definition
    counterparts, and implemented it in both field programmable gate array (FPGA)
    and application specific integrated circuit (ASIC) hardware for real-time video
    reconstruction. They further presented a FPGA-based network structure for SR.
    The number of parameters is reduced by using cascaded convolutions and depth-wise
    separable residual network (Kim et al., [2018c](#bib.bib64)). In (Wei et al.,
    [2019](#bib.bib125)), a CNN-based SR algorithm was implemented and accelerated
    through network pruning and quantization, and the algorithm was integrated in
    their real-time FPGA-based system, which supports video stream transcoding from
    H.264 FHD to H.265/HEVC UHD.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 等人 ([2018b](#bib.bib63)) 提出了一个硬件友好的 VSR 算法，该算法可以将全高清 (FHD) 视频流放大至其 4K 超高清对应物，并在现场可编程门阵列
    (FPGA) 和应用特定集成电路 (ASIC) 硬件上实现了实时视频重建。他们进一步展示了一个基于 FPGA 的 SR 网络结构。通过使用级联卷积和深度可分离残差网络，参数数量得到了减少
    (Kim et al., [2018c](#bib.bib64))。在 (Wei et al., [2019](#bib.bib125)) 中，实施了一个基于
    CNN 的 SR 算法，并通过网络剪枝和量化加速了该算法，该算法集成在其实时 FPGA 基系统中，支持从 H.264 FHD 到 H.265/HEVC UHD
    的视频流转码。
- en: 7.2 Remote Sensing
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 遥感
- en: The image SR methods such as VDSR and ESPCN have been utilized to enhance the
    resolution of objects in satellite videos in (Luo et al., [2017](#bib.bib89);
    Xiao et al., [2018](#bib.bib126)). In (Jiang et al., [2018a](#bib.bib55)), a progressively
    enhanced network with a transition unit was proposed to strengthen residual images
    with fine details. Moreover, Jiang et al. ([2018b](#bib.bib56)) proposed a deep
    distillation recursive network with a multi-scale purification unit to super-resolve
    the images in the Jilin-1 satellite videos. Liu et al. ([2020a](#bib.bib81)) proposed
    a framework to pose the image priors in maximum a posteriori to regularize the
    solution space and generate the corresponding high-resolution video frames. The
    framework combines the implicitly captured local motion information through exploiting
    spatiotemporal neighbors and the nonlocal spatial similarity to recover HR frames.
    The experiments on the videos from the Jilin-1 satellite and the OVS-1A satellite
    verify that the approach can preserve edges and texture details.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 SR 方法如 VDSR 和 ESPCN 已被用于增强卫星视频中物体的分辨率 (Luo et al., [2017](#bib.bib89); Xiao
    et al., [2018](#bib.bib126))。在 (Jiang et al., [2018a](#bib.bib55)) 中，提出了一种渐进增强的网络，带有过渡单元，以加强具有细节的残差图像。此外，Jiang
    et al. ([2018b](#bib.bib56)) 提出了一个深度蒸馏递归网络，具有多尺度净化单元，用于超分辨率处理吉林一号卫星视频中的图像。Liu
    et al. ([2020a](#bib.bib81)) 提出了一个框架，以最大后验的形式对图像先验进行建模，以正则化解空间并生成相应的高分辨率视频帧。该框架通过利用时空邻域和非局部空间相似性来恢复
    HR 帧，从而隐式捕获局部运动信息。对吉林一号卫星和 OVS-1A 卫星的视频实验验证了该方法可以保持边缘和纹理细节。
- en: 7.3 Medical Analysis
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 医疗分析
- en: Poot et al. ([2010](#bib.bib97)) and Odille et al. ([2015](#bib.bib93)) reconstructed
    isotropic 3D magnetic resonance imaging (MRI) data in high resolution from multiple
    low-resolution MRI slices of different orientations, and they did not utilize
    accurate motion estimation and alignment. In (Zhang et al., [2012](#bib.bib138)),
    HR 4D computerized tomography (CT) images are super-resolved with several frames
    for each slice at different respiratory phases. Yu et al. ([2017](#bib.bib135))
    proposed a multi-slice CT SR network, which inputs the consecutive CT slices as
    video frames. It is composed of several convolution layers and a rearranging layer,
    and a subset of 5,800 slices is used to train the model, and the other 1,000 slices
    for testing.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: Poot 等人 ([2010](#bib.bib97)) 和 Odille 等人 ([2015](#bib.bib93)) 从多个不同方向的低分辨率 MRI
    切片中重建了高分辨率的各向同性 3D 磁共振成像 (MRI) 数据，并且他们没有利用准确的运动估计和对齐。在 (Zhang et al., [2012](#bib.bib138))
    中，HR 4D 计算机断层扫描 (CT) 图像通过多个帧在不同呼吸相位下进行超分辨率处理。Yu 等人 ([2017](#bib.bib135)) 提出了一个多切片
    CT SR 网络，该网络将连续的 CT 切片作为视频帧输入。它由若干卷积层和一个重新排列层组成，使用 5,800 张切片训练模型，其余 1,000 张切片用于测试。
- en: Ren et al. ([2019](#bib.bib100)) proposed a framework, which adopts an iterative
    upsampling layer and one downsampling layer in DBPN (Haris et al., [2018](#bib.bib33))
    to provide an error feedback mechanism for the reconstruction of medical videos.
    Lin et al. ([2020](#bib.bib78)) proposed a network to super-resolve the cardiac
    MRI slices, which uses bidirectional ConvLSTM as the network backbone. It utilizes
    domain knowledge of cardiac, and iteratively to enhance low-resolution MRI slices.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Surveillance Videos
  id: totrans-442
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shamsolmoali et al. ([2019](#bib.bib104)) proposed a deep CNN to upsample the
    low-resolution surveillance videos. The CNN is composed of less than 20 layers
    and is trained and tested on two surveillance datasets, which are mainly indoor
    videos. Lee et al. ([2018](#bib.bib68)) utilized SRGAN (Ledig et al., [2017](#bib.bib67))
    to enhance the details of the characters on license plate, and they also collected
    a video dataset with low-resolution and evaluated their method to verify its effectiveness.
    Guo et al. ([2020](#bib.bib32)) adopted DeblurGAN (Kupyn et al., [2018](#bib.bib65))
    to remove the motion blur of the adjacent frames, and then the MEMC was performed
    on adjacent frames. Finally, high-resolution video frames can be reconstructed
    through a multi-frame super-resolution algorithm.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: In order to super-resolve multi-view face video, Deshmukh and Rani ([2019](#bib.bib18))
    proposed a fractional-Grey Wolf optimizer-based kernel for the neighboring pixel
    estimation in the face video. Xin et al. ([2020](#bib.bib128)) proposed a simple
    but effective motion-adaptive feedback cell that can capture the motion information
    and feed it back to the network in an adaptive way for video face super-resolution.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 3D Reconstruction
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using the input video sequences, Burns et al. ([2017](#bib.bib4)) presented
    a SR method, which produces a 3D mesh of the observed scene with enhanced texture.
    For multiview video SR methods, Li et al. ([2016](#bib.bib74)) adopted the kernel
    regression to upgrade the information extraction layer, and utilize nonlocal means
    to information merging layer. Furthermore, Li et al. ([2019b](#bib.bib75)) proposed
    the first framework that super-resolves the appearance of 3D objects that are
    captured from multiple view points. The framework combines the power of 2D deep
    learning-based techniques with the 3D geometric information in the multi-view
    setting.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 7.6 Virtual Reality
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Liu et al. ([2020b](#bib.bib82)) proposed a single frame and multi-frame joint
    super-resolution network, which includes a loss function with weighted mean squared
    error for the SR of 360-degree panorama videos. They also provided a new panorama
    video dataset: the MiG Panorama Video for evaluating the panorama VSR algorithms.
    Dasari et al. ([2020](#bib.bib17)) presented a video streaming system to reduce
    bandwidth requirements for 360-degree videos. The client runs a deep learning
    based SR model to recover the video, which is heavily compressed at the server.
    The authors also compared with other state-of-the-art video streaming systems
    on video quality of experience.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 7.7 Thermal Videos
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In (Kwasniewska et al., [2019](#bib.bib66)), a super-resolution model based
    on CNN and residual connection was proposed to enhance the thermal videos acquired
    by thermal cameras, and contactlessly estimate the respiratory rate. Compared
    with the previous methods, the performance is improved by using super-resolved
    sequences. Gautam and Singh ([2020](#bib.bib28)) discussed the performance of
    SR techniques by using different deep neural networks on benchmark thermal datasets,
    including SRCNN (Dong et al., [2014](#bib.bib19)), EDSR (Lim et al., [2017](#bib.bib77)),
    autoencoder and SRGAN (Ledig et al., [2017](#bib.bib67)). Based on the experimental
    results, they concluded that SRGAN yields superior performance on thermal frames
    when comparing with others.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 8 Trends and Challenges
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although great progress has been made by state-of-the-art video super-resolution
    methods based on deep learning especially on some public benchmark datasets, there
    are still challenges and trends discussed below.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Lightweight Super-Resolution Models
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning based video super-resolution methods enjoy high performance,
    nevertheless they have difficulty in deploying efficiently in many real-world
    problems. It is noted that their models usually have a mass of parameters and
    require vast computational and storage resources, and their training also takes
    a long time. With the popularity of mobile devices in modern life, one expects
    to apply these models on such devices. To address this issue, several lightweight
    super-resolution methods have been proposed, e.g., RISTN (Zhu et al., [2019](#bib.bib142)),
    TDAN (Tian et al., [2020](#bib.bib114)), and (Xiao et al., [2021](#bib.bib127)).
    How to design and implement a lightweight super-resolution algorithm with high
    performance for real-world applicants is a major challenge.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Interpretability of Models
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep neural networks are usually considered as black boxes. That is, we do not
    know what real information the model learns when the performance is good or bad.
    In existing video super-resolution models, there is not a theoretical interpretation
    about how convolution neural networks recover low-resolution video sequences.
    With a deeper investigation on its interpretation, the performance of super-resolution
    algorithms for both videos and images may be improved greatly. Some works have
    paid attention to this problem, e.g., (Chan et al., [2021c](#bib.bib8)) and (Liu
    et al., [2021b](#bib.bib84)).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 8.3 Super-Resolution with Larger Scaling Factors
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For video super-resolution tasks, existing works mainly focus on the case of
    the magnification factors $\times 2$, $\times 3$ and $\times 4$. The more challenging
    scales such as $\times$8 and $\times$16 have been rarely explored. With the popularity
    of high-resolution (e.g., 8K and 16K) display devices, larger scaling factors
    are to be further studied. Obviously, as the scale becomes larger, it is more
    challenging to predict and restore unknown information in video sequences. This
    may result in performance degradation for the algorithms, and weaken robustness
    in the models. Therefore, how to develop stable deep learning algorithms for VSR
    tasks with larger scaling factors is still challenging. Until now, there is seldom
    such work on VSR, while several works such as (Chan et al., [2021a](#bib.bib6))
    and (Chen et al., [2021](#bib.bib11)) were proposed for the single image super-resolution
    tank with larger scaling factors, e.g., $\times$8.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 8.4 Super-Resolution with Arbitrary Scaling Factors
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From Table [4](#S6.T4 "Table 4 ‣ 6.1 Datasets and Competitions ‣ 6 Performance
    Comparisons ‣ Video Super-Resolution Based on Deep Learning: A Comprehensive Survey"),
    we can see that most video super-resolution methods are designed for the case
    of the scaling factor $\times 4$, which is not appropriate for real scenes. On
    the one hand, other scales like $\times 2$, $\times 3$ or $\times 1.5$ are also
    very common in VSR tasks. On the other hand, a video super-resolution model with
    fixed scale will seriously limit its generalization and portability. Therefore,
    a universal VSR method for arbitrary scale factors is greatly needed in real-world
    applications. Several works about image super-resolution with arbitrary scaling
    factors have been presented, e.g., (Hu et al., [2019](#bib.bib39)), and (Wang
    et al., [2021a](#bib.bib120)), while the works on arbitrary scale factor upsampling
    for videos are still seldom.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 8.5 More Reasonable $\&amp;$ Proper Degradation Process of Videos
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In existing works, the degraded LR videos are attained through the two methods:
    One is directly downsampling HR videos by using interpolation, such as bicubic.
    The other is performing the Gaussian blurring on HR videos and then downsampling
    the video sequences. Although both methods perform well in theory, they always
    perform poorly in practice. As it is known, the real-world degradation process
    is very complex and includes much uncertainty. The blurring and interpolation
    are not adequate for modeling this problem. Therefore, when constructing LR videos,
    the degradation should be modeled theoretically in consistent with the real-world
    case to reduce the gap between research and practice. There are a few works involving
    the degradation process of videos for super-resolution, such as (Zhang et al.,
    [2018a](#bib.bib136)).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的研究中，退化的LR视频是通过两种方法获得的：一种是通过插值方法直接下采样HR视频，例如双三次插值。另一种是对HR视频进行高斯模糊处理，然后再对视频序列进行下采样。虽然这两种方法在理论上表现良好，但在实际应用中总是效果不佳。众所周知，真实世界的退化过程非常复杂，并且包含很多不确定性。模糊和插值方法不足以对这种问题进行建模。因此，在构建LR视频时，退化过程应在理论上与实际情况一致，以减少研究与实践之间的差距。涉及视频超分辨率退化过程的工作很少，例如(Zhang
    et al., [2018a](#bib.bib136))。
- en: 8.6 Unsupervised Super-Resolution Methods
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 无监督超分辨率方法
- en: Most state-of-the-art VSR methods adopt a supervised learning paradigm. In other
    words, the deep neural networks require a large number of paired LR and HR video
    frames for training. However, such paired datasets are hard or costly to obtain
    in practice. One may synthesize the LR/HR video frames, the performance of super-resolution
    methods is still not satisfied as the degradation model is too simple to characterize
    the real-world problem and results in inaccurate HR/LR datasets. Thus, unsupervised
    VSR methods are highly demanded. Some works of unsupervised VSR on satellite videos
    have been proposed, e.g., (He et al., [2020](#bib.bib37)), but not about generic
    videos.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数最先进的VSR方法采用监督学习范式。换句话说，深度神经网络需要大量配对的LR和HR视频帧进行训练。然而，实际中很难或昂贵地获取这样的配对数据集。虽然可以合成LR/HR视频帧，但由于退化模型过于简单，无法准确表征现实问题，超分辨率方法的性能仍不令人满意，导致HR/LR数据集不准确。因此，对无监督VSR方法的需求非常高。已有一些关于卫星视频的无监督VSR工作，例如(He
    et al., [2020](#bib.bib37))，但对于通用视频尚无相关研究。
- en: 8.7 More Effective Scene Change Algorithms
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 更有效的场景切换算法
- en: Existing video super-resolution methods rarely involve the videos with scene
    change. In practice, a video sequence usually has many different scenes. When
    we consider the problem of video super-resolution on such videos, they have to
    be split into multiple segments without scene change and processed individually.
    This may result in large computational time. In fact, a simple subnet in 3DSRnet (Kim
    et al., [2019](#bib.bib61)) has been proposed to deal with scene change, and it
    includes scene boundary detection and frame replacement. More dedicated networks
    that can process videos with complex scene changes are necessary for real-world
    applications.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的视频超分辨率方法很少涉及场景切换的视频。在实际中，视频序列通常包含许多不同的场景。当我们考虑对这样的场景切换视频进行超分辨率时，它们必须被拆分成多个无场景切换的段落进行单独处理。这可能导致较大的计算时间。实际上，3DSRnet中提出了一个简单的子网（Kim
    et al., [2019](#bib.bib61)），用于处理场景切换，包括场景边界检测和帧替换。对于现实世界应用，需要更多专用的网络来处理具有复杂场景切换的视频。
- en: 8.8 More Reasonable Evaluation Criteria for Video Quality
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8 更合理的视频质量评估标准
- en: The criteria for evaluating the quality of super-resolution results mainly include
    PSNR and SSIM. However, their values are not able to reflect the video quality
    for human perception. That is, even if the PSNR value of a recovered video is
    high, the video also makes people uncomfortable. Therefore, new evaluation criteria
    for videos that are consistent with human perception need to be developed. More
    attentions have been attracted to the quality evaluation for images, such as (Gu
    et al., [2020](#bib.bib30)). However, the video quality including coherence between
    frames will be investigated in the future.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 评估超分辨率结果质量的标准主要包括PSNR和SSIM。然而，它们的值无法反映人眼对视频质量的感知。也就是说，即使恢复视频的PSNR值很高，该视频也可能让人感到不适。因此，需要开发与人眼感知一致的视频新评估标准。对图像质量评估的关注度已增加，例如(Gu
    et al., [2020](#bib.bib30))。但视频质量，包括帧间一致性，将在未来进行研究。
- en: 8.9 More Effective Methods for Leveraging Information
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.9 更有效的信息利用方法
- en: An important characteristic of video super-resolution methods is leveraging
    the information contained in video frames. The effectiveness of utilization influences
    the performance directly. Although many methods have been proposed, as mentioned
    in this paper, there are still some disadvantages. For instance, 3D convolution
    and non-local modules require a large amount of computation, and the accuracy
    of optical estimation can not be guaranteed. Therefore, the methods that can effectively
    utilize information contained in different frames is worth further studying.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusions
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we reviewed the development of deep learning approaches for
    video super-resolution in recent years. We first classified existing video super-resolution
    algorithms into seven subcategories by the way of leveraging information contained
    in video frames, described the key ideas of representative methods and summarized
    the advantages and disadvantages of each method. Furthermore, we also compared
    and analyzed the performance of those methods on benchmark datasets, and outlines
    the wide applications of video super-resolution algorithms. Although the deep
    learning based VSR methods have made great progress, we listed eight open issues
    for the development of VSR algorithms, which is expected to provide some enlightment
    for researchers.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We thank all the reviewers for their valuable comments. We would like to thank
    Mr. Zekun Li (Master student at School of Artificial Intelligence in Xidian University)
    and Dr. Yaowei Wang (Associate Professor with Peng Cheng Laboratory, Shenzhen,
    China) for their help in improving the quality of this manuscript. This work was
    supported by the National Natural Science Foundation of China (Nos. 61976164,
    61876220, 61876221, and 61906184).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Author Biography
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hongying Liu received her B.E. and M.Sc. degrees in Computer Science and Technology
    from Xi’An University of Technology, China, in 2006 and 2009, respectively, and
    Ph.D. in Engineering from Waseda University, Japan in 2012\. Currently, she is
    a faculty member at the School of Artificial Intelligence, and also with the Key
    Laboratory of Intelligent Perception and Image Understanding of Ministry of Education,
    Xidian University, China. In addition, she is a senior member of IEEE. Her major
    research interests include image and video processing, intelligent signal processing,
    machine learning, etc.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: Zhubo Ruan received his M. Sc. degree from School of Artificial Intelligence
    in Xidian University in 2021\. His research interests include machine learning
    and video super-resolution, etc.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Peng Zhao is currently pursuing the M.Sc. degree with the School of Artificial
    Intelligence in Xidian University. His research interests include video super-resolution,
    medical image processing, etc.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'Chao Dong received the Ph.D. degree from The Chinese University of Hong Kong
    in 2016, advised by Prof. Tang and Prof. Loy. He is currently an Associate Professor
    with the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences.
    His current research interests include low-level vision problems, such as image/video
    super-resolution, denoising, and enhancement. His team won the first place in
    international super-resolution challenges: NTIRE2018, PIRM2018, and NTIRE2019.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Fanhua Shang received the Ph.D. degree in Circuits and Systems from Xidian University,
    Xi’an, China, in 2012\. He is currently a professor with the School of Artificial
    Intelligence, Xidian University, China. Prior to joining Xidian University, he
    was a Research Associate with the Department of Computer Science and Engineering,
    The Chinese University of Hong Kong. From 2013 to 2015, he was a Post-Doctoral
    Research Fellow with the Department of Computer Science and Engineering, The Chinese
    University of Hong Kong. From 2012 to 2013, he was a Post-Doctoral Research Associate
    with the Department of Electrical and Computer Engineering, Duke University, Durham,
    NC, USA. His current research interests include machine learning, data mining,
    and computer vision.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Yuanyuan Liu received the Ph.D. degree in Circuits and Systems from Xidian University,
    Xi’an, China, in 2012\. He is currently a professor with the School of Artificial
    Intelligence, Xidian University, China. Prior to joining Xidian University, he
    was a Research Associate with the Department of Computer Science and Engineering,
    The Chinese University of Hong Kong. From 2013 to 2015, he was a Post-Doctoral
    Research Fellow with the Department of Computer Science and Engineering, The Chinese
    University of Hong Kong. From 2012 to 2013, he was a Post-Doctoral Research Associate
    with the Department of Electrical and Computer Engineering, Duke University, Durham,
    NC, USA. His current research interests include machine learning, data mining,
    and pattern recognition.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: Linlin Yang received the M.Sc. degree in circuits and systems from Taiyuan University
    of Science and Technology, Taiyuan, China, in 2020\. She is currently pursuing
    the Ph.D. degree with the School of Artificial Intelligence in Xidian University,
    China. Her research interests include image processing, etc.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Radu Timofte is a professor and chair for computer vision at University of Wurzburg,
    Germany and the recipient of a 2022 Humboldt Professorship Award for Artificial
    Intelligence. He is also a lecturer and group leader at ETH Zurich, Switzerland.
    He obtained his PhD degree in Electrical Engineering at the KU Leuven, Belgium
    in 2013, the MSc at the Univ. of Eastern Finland in 2007, and the Dipl. Eng. at
    the Technical Univ. of Iasi, Romania in 2006\. He is co-founder of Merantix, co-organizer
    of NTIRE, CLIC, AIM, MAI and PIRM events, and member of IEEE, CVF, and ELLIS.
    He is associate/area editor for journals such as IEEE Trans. PAMI, Elsevier Neurocomputing,
    Elsevier CVIU and SIAM Journal on Imaging Sciences and he regularly serves as
    area chair/SPC for conferences such as CVPR, ICCV, ECCV, IJCAI. His current research
    interests include deep learning, visual tracking, mobile AI, image/video compression,
    restoration, manipulation and enhancement.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bao et al. (2021) Bao W, Lai W, Zhang X, Gao Z, Yang M (2021) MEMC-Net: Motion
    estimation and motion compensation driven neural network for video interpolation
    and enhancement. IEEE Trans Pattern Anal Mach Intell 43(3):933–948'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bare et al. (2019) Bare B, Yan B, Ma C, Li K (2019) Real-time video super-resolution
    via motion convolution kernel estimation. Neurocomputing 367:236–245
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brox et al. (2004) Brox T, Bruhn A, Papenberg N, Weickert J (2004) High accuracy
    optical flow estimation based on a theory for warping. In: Pajdla T, Matas J (eds)
    Eur. Conf. Comput. Vis., pp 25–36'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burns et al. (2017) Burns C, Plyer A, Champagnat F (2017) Texture super-resolution
    for 3d reconstruction. In: Proc. IAPR Int. Conf. Mach. Vis. Appl., pp 350–353'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caballero et al. (2017) Caballero J, Ledig C, Aitken A, Acosta A, Totz J, Wang
    Z, Shi W (2017) Real-time video super-resolution with spatio-temporal networks
    and motion compensation. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 2848–2857'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021a) Chan KC, Wang X, Xu X, Gu J, Loy CC (2021a) Glean: Generative
    latent bank for large-factor image super-resolution. In: Proc. IEEE/CVF Conf.
    Comput. Vis. Pattern Recognit., pp 14245–14254'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021b) Chan KC, Wang X, Yu K, Dong C, Loy CC (2021b) BasicVSR:
    The search for essential components in video super-resolution and beyond. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 4947–4956'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021c) Chan KC, Wang X, Yu K, Dong C, Loy CC (2021c) Understanding
    deformable alignment in video super-resolution. In: Proc. AAAI Conf. Artif. Intell.,
    vol 35, pp 973–981'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. (2021d) Chan KCK, Zhou S, Xu X, Loy CC (2021d) BasicVSR++: Improving
    video super-resolution with enhanced propagation and alignment. arXiv preprint
    arXiv:210413371'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020) Chen J, Tan X, Shan C, Liu S, Chen Z (2020) VESR-Net: The
    winning solution to youku video enhancement and super-resolution challenge. arXiv
    preprint arXiv:200302115'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen Y, Liu S, Wang X (2021) Learning continuous image representation
    with local implicit image function. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp 8628–8638'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chu et al. (2020) Chu M, Xie Y, Mayer J, Leal-Taixé L, Thuerey N (2020) Learning
    temporal coherence via self-supervision for gan-based video generation. ACM Trans
    Graph 39(4):75
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Dai J, Qi H, Xiong Y, Li Y, Zhang G, Hu H, Wei Y (2017) Deformable
    convolutional networks. In: Proc IEEE Int. Conf. Comput. Vis., pp 764–773'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2015) Dai Q, Yoo S, Kappeler A, Katsaggelos AK (2015) Dictionary-based
    multiple frame video super-resolution. In: Proc. IEEE Int. Conf. Image Process.,
    pp 83–87'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daithankar and Ruikar (2020) Daithankar MV, Ruikar SD (2020) Video super resolution:
    A review. In: ICDSMLA 2019, pp 488–495'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dario et al. (2020) Dario F, Huang Z, Gu S, Radu T, et al. (2020) Aim 2020
    challenge on video extreme super-resolution: Methods and results. arXiv preprint
    arXiv:200711803'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dasari et al. (2020) Dasari M, Bhattacharya A, Vargas S, Sahu P, Balasubramanian
    A, Das SR (2020) Streaming 360-degree videos using super-resolution. In: Proc.
    IEEE Conf. Comput. Commun., pp 1977–1986'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deshmukh and Rani (2019) Deshmukh AB, Rani NU (2019) Fractional-grey wolf optimizer-based
    kernel weighted regression model for multi-view face video super resolution. Int
    J Mach Learn Cybern 10(5):859–877
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2014) Dong C, Loy CC, He K, Tang X (2014) Learning a deep convolutional
    network for image super-resolution. In: Eur. Conf. Comput. Vis., pp 184–199'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2016) Dong C, Loy CC, Tang X (2016) Accelerating the super-resolution
    convolutional neural network. In: Eur. Conf. Comput. Vis., pp 391–407'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al. (2015) Dosovitskiy A, Fischer P, Ilg E, Husser P, Hazirbas
    C, Golkov V, v d Smagt P, Cremers D, Brox T (2015) FlowNet: Learning optical flow
    with convolutional networks. In: Proc. IEEE Int. Conf. Comput. Vis., pp 2758–2766'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Drulea and Nedevschi (2011) Drulea M, Nedevschi S (2011) Total variation regularization
    of local-global optical flow. In: 2011 14th Int. IEEE Conf. Intell. Transp. Syst.
    (ITSC), pp 318–323'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fakour-Sevom et al. (2018) Fakour-Sevom V, Guldogan E, Kämäräinen JK (2018)
    360 panorama super-resolution using deep convolutional networks. In: Int. Conf.
    Comput. Vis. Theory Appl. (VISAPP), pp 159–165'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Farsiu et al. (2004) Farsiu S, Robinson MD, Elad M, Milanfar P (2004) Fast and
    robust multiframe super resolution. IEEE Trans Image Process 13(10):1327–1344
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2019a) Fuoli D, Gu S, Timofte R (2019a) Efficient video super-resolution
    through recurrent latent space propagation. In: Proc. IEEE/CVF Int. Conf. Comput.
    Vis. Workshop, pp 3476–3485'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2019b) Fuoli D, Gu S, Timofte R, Tao X, Li W, Guo T, Deng Z,
    Lu L, Dai T, Shen X, et al. (2019b) Aim 2019 challenge on video extreme super-resolution:
    Methods and results. In: Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshop, pp 3467–3475'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fuoli et al. (2020) Fuoli D, Huang Z, Gu S, Timofte R, Raventos A, Esfandiari
    A, Karout S, Xu X, Li X, Xiong X, et al. (2020) Aim 2020 challenge on video extreme
    super-resolution: Methods and results. In: Eur. Conf. Comput. Vis., pp 57–81'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gautam and Singh (2020) Gautam A, Singh S (2020) A comparative analysis of
    deep learning based super-resolution techniques for thermal videos. In: Proc.
    Int. Conf. Smart Syst. Inven. Technol., pp 919–925'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glaister et al. (2011) Glaister J, Chan C, Frankovich M, Tang A, Wong A (2011)
    Hybrid video compression using selective keyframe identification and patch-based
    super-resolution. In: Proc. IEEE Int. Symp. Multimedia, pp 105–110'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2020) Gu J, Cai H, Chen H, Ye X, Ren J, Dong C (2020) Image quality
    assessment for perceptual image restoration: A new dataset, benchmark and metric.
    arXiv preprint arXiv: 201115002'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo and Chao (2017) Guo J, Chao H (2017) Building an end-to-end spatial-temporal
    convolutional network for video super-resolution. In: Proc. AAAI Conf. Artif.
    Intell., pp 4053–4060'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Guo K, Guo H, Ren S, Zhang J, Li X (2020) Towards efficient
    motion-blurred public security video super-resolution based on back-projection
    networks. J Netw Comput Appl 166:102691
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2018) Haris M, Shakhnarovich G, Ukita N (2018) Deep back-projection
    networks for super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 1664–1673'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2019) Haris M, Shakhnarovich G, Ukita N (2019) Recurrent back-projection
    network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 3892–3901'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haris et al. (2020) Haris M, Shakhnarovich G, Ukita N (2020) Space-time-aware
    multi-resolution video enhancement. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern
    Recognit., pp 2859–2868'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) He K, Zhang X, Ren S, Sun J (2016) Deep residual learning
    for image recognition. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp
    770–778'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) He Z, He D, Li X, Xu J (2020) Unsupervised video satellite
    super-resolution by using only a single video. IEEE Geosci Remote Sens Lett
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural Comput 9(8):1735–1780
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2019) Hu X, Mu H, Zhang X, Wang Z, Tan T, Sun J (2019) Meta-sr:
    A magnification-arbitrary network for super-resolution. In: Proc. IEEE/CVF Conf.
    Comput. Vis. Pattern Recognit., pp 1575–1584'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2017) Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017)
    Densely connected convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 2261–2269'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2015) Huang Y, Wang W, Wang L (2015) Bidirectional recurrent
    convolutional networks for multi-frame super-resolution. In: Adv. Neural Inf.
    Process. Syst., pp 235–243'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) Huang Y, Wang W, Wang L (2018) Video super-resolution via
    bidirectional recurrent convolutional networks. IEEE Trans Pattern Anal Mach Intell
    40(4):1015–1028
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hui et al. (2018) Hui T, Tang X, Loy CC (2018) LiteFlowNet: A lightweight convolutional
    neural network for optical flow estimation. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 8981–8989'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hui et al. (2021) Hui T, Tang X, Loy CC (2021) A lightweight optical flow cnn
    - revisiting data fidelity and regularization. IEEE Trans Pattern Anal Mach Intell
    43(8):2555–2569
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hui et al. (2021) Hui Z, Li J, Gao X, Wang X (2021) Progressive perception-oriented
    network for single image super-resolution. Information Sciences 546:769–786
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ignatov et al. (2021) Ignatov A, Romero A, Kim H, Timofte R, et al. (2021)
    Real-time video super-resolution on smartphones with deep learning, mobile ai
    2021 challenge: Report. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.
    Workshops, pp 2535–2544'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilg et al. (2017) Ilg E, Mayer N, Saikia T, Keuper M, Dosovitskiy A, Brox T
    (2017) FlowNet 2.0: Evolution of optical flow estimation with deep networks. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 1647–1655'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irani and Peleg (1991) Irani M, Peleg S (1991) Improving resolution by image
    registration. CVGIP Graphical Models Image Process 53(3):231 – 239
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irani and Peleg (1993) Irani M, Peleg S (1993) Motion analysis for image enhancement:
    Resolution, occlusion, and transparency. J Vis Commun Image Represent 4(4):324
    – 335'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isobe et al. (2020) Isobe T, Jia X, Gu S, Li S, Wang S, Tian Q (2020) Video
    super-resolution with recurrent structure-detail network. In: Eur. Conf. Comput.
    Vis., pp 645–660'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jacobsen et al. (2018) Jacobsen JH, Smeulders AW, Oyallon E (2018) i-RevNet:
    Deep invertible networks. In: Proc. Int. Conf. Learn. Represent.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaderberg et al. (2015) Jaderberg M, Simonyan K, Zisserman A, kavukcuoglu k
    (2015) Spatial transformer networks. In: Adv. Neural Inf. Process. Syst. 28, pp
    2017–2025'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2013) Ji S, Xu W, Yang M, Yu K (2013) 3D convolutional neural networks
    for human action recognition. IEEE Trans Pattern Anal Mach Intell 35(1):221–231
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia et al. (2016) Jia X, De Brabandere B, Tuytelaars T, Gool LV (2016) Dynamic
    filter networks. In: Adv. Neural Inf. Process. Syst. 29, pp 667–675'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018a) Jiang K, Wang Z, Yi P, Jiang J (2018a) A progressively
    enhanced network for video satellite imagery superresolution. IEEE Signal Process
    Lett 25(11):1630–1634
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2018b) Jiang K, Wang Z, Yi P, Jiang J, Xiao J, Yao Y (2018b) Deep
    distillation recursive network for remote sensing imagery super-resolution. Remote
    Sens 10(11):1700
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jo et al. (2018) Jo Y, Oh SW, Kang J, Kim SJ (2018) Deep video super-resolution
    network using dynamic upsampling filters without explicit motion compensation.
    In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 3224–3232'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalarot and Porikli (2019) Kalarot R, Porikli F (2019) MultiBoot VSR: Multi-stage
    multi-reference bootstrapping for video super-resolution. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit. Workshops, pp 2060–2069'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kappeler et al. (2016) Kappeler A, Yoo S, Dai Q, Katsaggelos AK (2016) Video
    super-resolution with convolutional neural networks. IEEE Trans Comput Imag 2(2):109–122
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2016) Kim J, Lee JK, Lee KM (2016) Accurate image super-resolution
    using very deep convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 1646–1654'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2019) Kim SY, Lim J, Na T, Kim M (2019) Video super-resolution
    based on 3d-cnns with consideration of scene change. In: Proc. IEEE Int. Conf.
    Image Process., pp 2831–2835'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2018a) Kim TH, Sajjadi MSM, Hirsch M, Schölkopf B (2018a) Spatio-temporal
    transformer network for video restoration. In: Eur. Conf. Comput. Vis., pp 111–127'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2018b) Kim Y, Choi JS, Kim M (2018b) 2x super-resolution hardware
    using edge-orientation-based linear mapping for real-time 4k uhd 60 fps video
    applications. IEEE Trans Circuits Syst Express Briefs 65(9):1274–1278
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2018c) Kim Y, Choi JS, Kim M (2018c) A real-time convolutional neural
    network for super-resolution on fpga with applications to 4k uhd 60 fps video
    services. IEEE Trans Circuits Syst Video Technol 29(8):2521–2534
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kupyn et al. (2018) Kupyn O, Budzan V, Mykhailych M, Mishkin D, Matas J (2018)
    Deblurgan: Blind motion deblurring using conditional adversarial networks. In:
    Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 8183–8192'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwasniewska et al. (2019) Kwasniewska A, Ruminski J, Szankin M (2019) Improving
    accuracy of contactless respiratory rate estimation by enhancing thermal sequences
    with deep neural networks. Applied Sciences 9(20)
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ledig et al. (2017) Ledig C, Theis L, Huszr F, Caballero J, Cunningham A, Acosta
    A, Aitken A, Tejani A, Totz J, Wang Z, Shi W (2017) Photo-realistic single image
    super-resolution using a generative adversarial network. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit., pp 105–114'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2018) Lee Y, Yun J, Hong Y, Lee J, Jeon M (2018) Accurate license
    plate recognition and super-resolution using a generative adversarial networks
    on traffic surveillance video. In: Proc. IEEE Int. Conf. Consum. Electron. - Asia,
    ICCE-Asia, pp 1–4'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei and Todorovic (2018) Lei P, Todorovic S (2018) Temporal deformable residual
    networks for action segmentation in videos. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 6742–6751'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Li D, Liu Y, Wang Z (2019) Video super-resolution using non-simultaneous
    fully recurrent convolutional network. IEEE Trans Image Process 28(3):1342–1355
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li K, Bare B, Yan B, Feng B, Yao C (2018) Face hallucination
    based on key parts enhancement. In: Proc. IEEE Int. Conf. Acoust. Speech Signal.
    Process., pp 1378–1382'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li S, He F, Du B, Zhang L, Xu Y, Tao D (2019a) Fast spatio-temporal
    residual network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 10522–10531'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020) Li W, Tao X, Guo T, Qi L, Lu J, Jia J (2020) MuCAN: Multi-correspondence
    aggregation network for video super-resolution. In: Eur. Conf. Comput. Vis., pp
    335–351'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016) Li Y, Li X, Fu Z, Zhong W (2016) Multiview video super-resolution
    via information extraction and merging. In: Proc. 24th ACM Int. Conf. Multimedia,
    pp 446–450'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Li Y, Tsiminaki V, Timofte R, Pollefeys M, Gool LV (2019b)
    3d appearance super-resolution with deep learning. In: Proc. IEEE/CVF Conf. Comput.
    Vis. Pattern Recognit., pp 9671–9680'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2015) Liao R, Tao X, Li R, Ma Z, Jia J (2015) Video super-resolution
    via deep draft-ensemble learning. In: Proc IEEE Int. Conf. Comput. Vis., pp 531–539'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lim et al. (2017) Lim B, Son S, Kim H, Nah S, Lee KM (2017) Enhanced deep residual
    networks for single image super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit. Workshops, pp 1132–1140'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Lin JY, Chang YC, Hsu WH (2020) Efficient and phase-aware
    video super-resolution for cardiac mri. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 66–76'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Sun (2014) Liu C, Sun D (2014) On Bayesian adaptive video super resolution.
    IEEE Trans Pattern Anal Mach Intell 36(2):346–360
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2017) Liu D, Wang Z, Fan Y, Liu X, Wang Z, Chang S, Huang T (2017)
    Robust video super-resolution with learned temporal dynamics. In: Proc IEEE Int.
    Conf. Comput. Vis., pp 2526–2534'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Liu H, Gu Y, Wang T, Li S (2020a) Satellite video super-resolution
    based on adaptively spatiotemporal neighbors and nonlocal similarity regularization.
    IEEE Trans Geosci Remote Sens 58(12):8372–8383
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Liu H, Ruan Z, Fang C, Zhao P, Shang F, Liu Y, Wang L (2020b)
    A single frame and multi-frame joint network for 360-degree panorama video super-resolution.
    arXiv preprint arXiv:200810320
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) Liu H, Zhao P, Ruan Z, Shang F, Liu Y (2021a) Large motion
    video super-resolution with dual subnet and multi-stage communicated upsampling.
    In: Proc. AAAI Conf. Artif. Intell., pp 2127–2135'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) Liu X, Shi K, Wang Z, Chen J (2021b) Exploit camera raw data
    for video super-resolution via hidden markov model inference. IEEE Trans Image
    Process 30:2127–2140
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Cui (2018) Liu Z, Cui C (2018) A new low bit-rate coding scheme for
    ultra high definition video based on super-resolution reconstruction. In: Proc.
    IEEE Int. Conf. Comput. Commun. Eng. Technol., pp 325–329'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loshchilov and Hutter (2017) Loshchilov I, Hutter F (2017) Sgdr: Stochastic
    gradient descent with warm restarts. In: Proc. Int. Conf. Learn. Represent. (ICLR)'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lucas et al. (2019) Lucas A, LApez-Tapia S, Molina R, Katsaggelos AK (2019)
    Generative adversarial networks and perceptual losses for video super-resolution.
    IEEE Trans Image Process 28(7):3312–3327
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lucas and Kanade (1981) Lucas BD, Kanade T (1981) An iterative image registration
    technique with an application to stereo vision. In: Proc. Int. Joint Conf. Artif.
    Intell., pp 674–679'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2017) Luo Y, Zhou L, Wang S, Wang Z (2017) Video satellite imagery
    super resolution via convolutional neural networks. IEEE Geosci Remote Sens Lett
    14(12):2398–2402
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2015) Ma Z, Liao R, Tao X, Xu L, Jia J, Wu E (2015) Handling motion
    blur in multi-frame super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit., pp 5224–5232'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nah et al. (2019a) Nah S, Baik S, Hong S, Moon G, Son S, Timofte R, Lee KM
    (2019a) NTIRE 2019 challenge on video deblurring and super-resolution: Dataset
    and study. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, pp 1996–2005'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nah et al. (2019b) Nah S, Timofte R, Gu S, Baik S, Hong S, et al. (2019b) NTIRE
    2019 challenge on video super-resolution: Methods and results. In: Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit. Workshops, pp 1985–1995'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Odille et al. (2015) Odille F, Bustin A, Chen B, Vuissoz PA, Felblinger J (2015)
    Motion-corrected, super-resolution reconstruction for high-resolution 3d cardiac
    cine MRI. In: Med. Image Comput. Comput. Assist. Interv. (MICCAI), pp 435–442'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2020) Pan J, Cheng S, Zhang J, Tang J (2020) Deep blind video super-resolution.
    arXiv preprint arXiv:200304716
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patti et al. (1997) Patti AJ, Sezan MI, Tekalp AM (1997) Superresolution video
    reconstruction with arbitrary sampling lattices and nonzero aperture time. IEEE
    Trans Image Process 6(8):1064–1076
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2020) Peng C, Lin WA, Liao H, Chellappa R, Zhou SK (2020) Saint:
    Spatially aware interpolation network for medical slice synthesis. In: Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit., pp 7750–7759'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poot et al. (2010) Poot DH, Van Meir V, Sijbers J (2010) General and efficient
    super-resolution method for multi-slice MRI. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 615–622'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protter et al. (2009) Protter M, Elad M, Takeda H, Milanfar P (2009) Generalizing
    the nonlocal-means to super-resolution reconstruction. IEEE Trans Image Process
    18(1):36–51
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ranjan and Black (2017) Ranjan A, Black MJ (2017) Optical flow estimation using
    a spatial pyramid network. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 2720–2729'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2019) Ren S, Guo H, Guo K (2019) Towards efficient medical video
    super-resolution based on deep back-projection networks. In: Proc. IEEE Int. Conf.
    iThings/GreenCom/CPSCom/SmartData, pp 682–686'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional
    networks for biomedical image segmentation. In: Med. Image Comput. Comput. Assist.
    Interv. (MICCAI), pp 234–241'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sajjadi et al. (2018) Sajjadi MSM, Vemulapalli R, Brown M (2018) Frame-recurrent
    video super-resolution. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp
    6626–6634'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schultz and Stevenson (1996) Schultz RR, Stevenson RL (1996) Extraction of high-resolution
    frames from video sequences. IEEE Trans Image Process 5(6):996–1011
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shamsolmoali et al. (2019) Shamsolmoali P, Zareapoor M, Jain DK, Jain VK, Yang
    J (2019) Deep convolution network for surveillance records super-resolution. Multimed
    Tools Appl 78(17):23815–23829
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2016) Shi W, Caballero J, Huszr F, Totz J, Aitken AP, Bishop R,
    Rueckert D, Wang Z (2016) Real-time single image and video super-resolution using
    an efficient sub-pixel convolutional neural network. In: Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit., pp 1874–1883'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2015) Shi X, Chen Z, Wang H, Yeung DY, Wong Wk, Woo Wc (2015) Convolutional
    LSTM network: A machine learning approach for precipitation nowcasting. In: Adv.
    Neural Inf. Process. Syst. 28, pp 802–810'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shocher et al. (2018) Shocher A, Cohen N, Irani M (2018) Zero-shot super-resolution
    using deep internal learning. In: Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,
    pp 3118–3126'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh and Singh (2020) Singh A, Singh J (2020) Survey on single image based
    super-resolution-implementation challenges and solutions. Multimed Tools Appl
    79(3):1641–1672
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Son et al. (2021) Son S, Lee S, Nah S, Timofte R, Lee KM, et al. (2021) Ntire
    2021 challenge on video super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit. Workshops, pp 166–181'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2018) Sun D, Yang X, Liu M, Kautz J (2018) PWC-Net: CNNs for optical
    flow using pyramid, warping, and cost volume. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 8934–8943'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2020) Sun W, Sun J, Zhu Y, Zhang Y (2020) Video super-resolution
    via dense non-local spatial-temporal convolutional network. Neurocomputing 403:1–12
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takeda et al. (2009) Takeda H, Milanfar P, Protter M, Elad M (2009) Super-resolution
    without explicit subpixel motion estimation. IEEE Trans Image Process 18(9):1958–1975
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2017) Tao X, Gao H, Liao R, Wang J, Jia J (2017) Detail-revealing
    deep video super-resolution. In: Proc IEEE Int. Conf. Comput. Vis., pp 4482–4490'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2020) Tian Y, Zhang Y, Fu Y, Xu C (2020) TDAN: Temporally-deformable
    alignment network for video super-resolution. In: Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit., pp 3360–3369'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timofte et al. (2014) Timofte R, De Smet V, Van Gool L (2014) A+: Adjusted
    anchored neighborhood regression for fast super-resolution. In: Proc. Asian Conf.
    Comput. Vis., pp 111–126'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2015) Tran D, Bourdev L, Fergus R, Torresani L, Paluri M (2015)
    Learning spatiotemporal features with 3d convolutional networks. In: Proc IEEE
    Int. Conf. Comput. Vis., pp 4489–4497'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Umeda et al. (2018) Umeda S, Yano N, Watanabe H, Ikai T, Chujoh T, Ito N (2018)
    Hdr video super-resolution for future video coding. In: Int. Workshop Adv. Image
    Technol., pp 1–4'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang H, Su D, Liu C, Jin L, Sun X, Peng X (2019) Deformable
    non-local network for video super-resolution. IEEE Access 7:177734–177744
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019) Wang L, Guo Y, Lin Z, Deng X, An W (2019) Learning for video
    super-resolution through HR optical flow estimation. In: Proc. Asian Conf. Comput.
    Vis., pp 514–529'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Wang L, Wang Y, Lin Z, Yang J, An W, Guo Y (2021a) Learning
    a single network for scale-arbitrary super-resolution. In: Proc. IEEE/CVF Int.
    Conf. Comput. Vis., pp 4801–4810'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018) Wang X, Girshick R, Gupta A, He K (2018) Non-local neural
    networks. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 7794–7803'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang X, Chan KCK, Yu K, Dong C, Loy CC (2019a) EDVR: Video
    restoration with enhanced deformable convolutional networks. In: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit. Workshops, pp 1954–1963'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Wang Z, Yi P, Jiang K, Jiang J, Han Z, Lu T, Ma J (2019b)
    Multi-memory convolutional neural network for video super-resolution. IEEE Trans
    Image Process 28(5):2530–2544
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Wang Z, Chen J, Hoi SC (2021b) Deep learning for image
    super-resolution: A survey. IEEE Trans Pattern Anal Mach Intell 43(10):3365–3387'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2019) Wei Y, Chen L, Xie R, Song L, Zhang X, Gao Z (2019) FPGA
    based video transcoding system with 2k-4k super-resolution conversion. In: Proc.
    IEEE Int. Conf. Vis. Commun. Image Process., pp 1–2'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2018) Xiao A, Wang Z, Wang L, Ren Y (2018) Super-resolution for
    ¡°jilin-1¡± satellite video imagery via a convolutional network. Sensors 18(4):1194
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2021) Xiao Z, Fu X, Huang J, Cheng Z, Xiong Z (2021) Space-time
    distillation for video super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit., pp 2113–2122'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xin et al. (2020) Xin J, Wang N, Li J, Gao X, Li Z (2020) Video face super-resolution
    with motion-adaptive feedback cell. Proc AAAI Conf Artif Intell 34(7):12468–12475
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2012) Xu L, Jia J, Matsushita Y (2012) Motion detail preserving optical
    flow estimation. IEEE Trans Pattern Anal Mach Intell 34(9):1744–1757
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2019) Xue T, Chen B, Wu J, Wei D, Freeman WT (2019) Video enhancement
    with task-oriented flow. Int J Comput Vis 127(8):1106–1125
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2019) Yan B, Lin C, Tan W (2019) Frame and feature-context video
    super-resolution. In: Proc. AAAI Conf. Artif. Intell., pp 5597–5604'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2019) Yang W, Zhang X, Tian Y, Wang W, Xue JH, Liao Q (2019) Deep
    learning for single image super-resolution: A brief review. IEEE Trans Multimedia
    21(12):3106–3121'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2019) Yi P, Wang Z, Jiang K, Jiang J, Ma J (2019) Progressive fusion
    video super-resolution network via exploiting non-local spatio-temporal correlations.
    In: Proc IEEE Int. Conf. Comput. Vis., pp 3106–3115'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ying et al. (2020) Ying X, Wang L, Wang Y, Sheng W, An W, Guo Y (2020) Deformable
    3d convolution for video super-resolution. arXiv preprint arXiv:200402803
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2017) Yu H, Liu D, Shi H, Yu H, Wang Z, Wang X, Cross B, Bramler
    M, Huang TS (2017) Computed tomography super-resolution using convolutional neural
    networks. In: Proc. IEEE Int. Conf. Image Process., pp 3944–3948'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Zhang T, Gao K, Ni G, Fan G, Lu Y (2018a) Spatio-temporal
    super-resolution for multi-videos based on belief propagation. Signal Process
    Image Commun 68:1–12
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021) Zhang W, Li H, Li Y, Liu H, Chen Y, Ding X (2021) Application
    of deep learning algorithms in geotechnical engineering: a short critical review.
    Artif Intell Review pp 1–41'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2012) Zhang Y, Wu G, Yap PT, Feng Q, Lian J, Chen W, Shen D (2012)
    Reconstruction of super-resolution lung 4d-ct using patch-based sparse representation.
    In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp 925–931'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Zhang Y, Li K, Li K, Wang L, Zhong B, Fu Y (2018b) Image
    super-resolution using very deep residual channel attention networks. In: Eur.
    Conf. Comput. Vis., pp 294–310'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Zhang Y, Tian Y, Kong Y, Zhong B, Fu Y (2018) Residual
    dense network for image super-resolution. In: Proc. IEEE/CVF Conf. Comput. Vis.
    Pattern Recognit., pp 2472–2481'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Zhu X, Hu H, Lin S, Dai J (2019) Deformable ConvNets V2:
    More deformable, better results. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
    pp 9300–9308'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Zhu X, Li Z, Zhang X, Li C, Liu Y, Xue Z (2019) Residual
    invertible spatio-temporal network for video super-resolution. In: Proc. AAAI
    Conf. Artif. Intell., pp 5981–5988'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
