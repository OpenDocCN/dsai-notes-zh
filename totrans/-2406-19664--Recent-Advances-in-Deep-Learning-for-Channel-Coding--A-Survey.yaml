- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:31:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:31:36'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2406.19664] Recent Advances in Deep Learning for Channel Coding: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2406.19664] 深度学习在信道编码中的最新进展：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19664](https://ar5iv.labs.arxiv.org/html/2406.19664)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19664](https://ar5iv.labs.arxiv.org/html/2406.19664)
- en: \DeclareAcronym
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \DeclareAcronym
- en: dlshort=DL, long=deep learning \DeclareAcronymldpcshort=LDPC, long=low-density
    parity check \DeclareAcronymxrshort=XR, long=extended reality \DeclareAcronymv2xshort=V2X,
    long=vehicle-to-everything \DeclareAcronymembbshort=eMBB, long=enhanced mobile
    broadband \DeclareAcronymurllcshort=URLLC, long=ultra-reliable low-latency communication
    \DeclareAcronymmmtcshort=mMTC, long=massive machine type communication \DeclareAcronymkpishort=KPI,
    long=key performance indicator \DeclareAcronymmlshort=ML, long=machine learning
    \DeclareAcronymgpushort=GPU, long=graphics processing unit \DeclareAcronymcudashort=CUDA,
    long=compute unified device architecture \DeclareAcronymfpgashort=FPGA, long=field
    programmable gate array \DeclareAcronymtpushort=TPU, long=tensor processing unit
    \DeclareAcronymaeshort=AE, long=autoencoder \DeclareAcronymmushort=MU, long=multi-user
    \DeclareAcronymmimoshort=MIMO, long=multiple-input multiple-output \DeclareAcronym3gppshort=3GPP,
    long=third generation partnership project \DeclareAcronymdnnshort=DNN, long=deep
    neural network \DeclareAcronymmlpshort=MLP, long=multi-layer perceptron \DeclareAcronymrelushort=ReLU,
    long=rectified linear unit \DeclareAcronymbgdshort=BGD, long=batch gradient descent
    \DeclareAcronymsgdshort=SGD, long=stochastic gradient descent \DeclareAcronymmbsgdshort=MBSGD,
    long=mini-batch stochastic gradient descent \DeclareAcronymllmshort=LLM, long=large
    language model \DeclareAcronymrlshort=RL, long=reinforcement learning \DeclareAcronymmdpshort=MDP,
    long=Markov decision process \DeclareAcronymdrlshort=DRL, long=deep reinforcement
    learning \DeclareAcronymdqnshort=DQN, long=deep Q-network \DeclareAcronymbershort=BER,
    long=bit error rate \DeclareAcronympaprshort=PAPR, long=peak-to-average power
    ratio \DeclareAcronymofdmshort=OFDM, long=orthogonal frequency division multiplexing
    \DeclareAcronymcnnshort=CNN, long=convolutional neural network \DeclareAcronymnlpshort=NLP,
    long=natural language processing \DeclareAcronymilsvrcshort=ILSVRC, long=ImageNet
    Large Scale Visual Recognition Challenge \DeclareAcronymrnnshort=RNN, long=reccurent
    neural network \DeclareAcronymlstmshort=LSTM, long=long short term memory \DeclareAcronymgrushort=GRU,
    long=gated recurrent unit \DeclareAcronymgnnshort=GNN, long=graph neural network
    \DeclareAcronymmpnnshort=MPNN, long=message passing neural network \DeclareAcronymgptshort=GPT
    , long=generative pre-trained Transformer \DeclareAcronymllamashort=LLaMA, long=Large
    Language Model Meta AI \DeclareAcronympalmshort=PaLM, long=Pathways Language Model
    \DeclareAcronymdmshort=DM, long=diffusion model \DeclareAcronymddpmshort=DDPM,
    long=denoising diffusion probabilistic model \DeclareAcronymdeshort=DE, long=density
    evolution \DeclareAcronymexitshort=EXIT, long=extrinsic information transfer \DeclareAcronymbpshort=BP,
    long=belief propagation \DeclareAcronymawgnshort=AWGN, long=additive white Gaussian
    noise \DeclareAcronymbiawgnshort=BI-AWGN, long=binary-input additive white Gaussian
    noise \DeclareAcronymscshort=SC, long=successive cancellation \DeclareAcronymvnshort=VN,
    long=variable node \DeclareAcronymcnshort=CN, long=check node \DeclareAcronymsnrshort=SNR,
    long=signal-to-noise ratio \DeclareAcronymgashort=GA, long=Gaussian approximation
    \DeclareAcronymrcashort=RCA, long=reciprocal channel approximation \DeclareAcronymcrcshort=CRC,
    long=cyclic redundancy check \DeclareAcronymcasclshort=CA-SCL, long=cyclic redundancy
    check-aided successive cancellation list \DeclareAcronymblershort=BLER, long=block
    error rate \DeclareAcronympgdshort=PGD, long=projected gradient descent \DeclareAcronympccmpshort=PCCMP,
    long=polar-code-construction message-passing \DeclareAcronyma2cshort=A2C, long=advantage
    actor-critic \DeclareAcronymisitshort=ISIT, long=International Symposium on Information
    Theory \DeclareAcronympacshort=PAC, long=polarization adjusted convolutional \DeclareAcronymmapshort=MAP,
    long=maximum a posterior \DeclareAcronymbigrushort=Bi-GRU, long=bidirectional
    gated recurrent unit \DeclareAcronymecctshort=ECCT, long=error correction code
    Transformer \DeclareAcronympcmshort=PCM, long=parity check matrix \DeclareAcronymbpskshort=BPSK,
    long=binary phase shift keying \DeclareAcronymbchshort=BCH, long=Bose–Chaudhuri–Hocquenghem
    \DeclareAcronymsisoshort=SISO, long=soft-input soft-output \DeclareAcronymmindshort=MIND,
    long=model independent neural decoder \DeclareAcronymosdshort=OSD, long=ordered
    statistic decoding \DeclareAcronymsttmramshort=STT-MRAM, long=spin-torque transfer
    magnetic random access memory \DeclareAcronymmsshort=MS, long=min-sum \DeclareAcronymnmsshort=NMS,
    long=normalized min-sum \DeclareAcronymomsshort=OMS, long=offset min-sum \DeclareAcronymnomsshort=NOMS,
    long=normalized offset min-sum \DeclareAcronymamsshort=AMS, long=adjusted min-sum
    \DeclareAcronymsmmsshort=SMMS, long=single-minimum min-sum \DeclareAcronymvwmsshort=VWMS,
    long=variable weight min-sum \DeclareAcronymscmsshort=SCMS, long=self-corrected
    min-sum \DeclareAcronymrbmsshort=RBMS, long=reliability-based min-sum \DeclareAcronymsvmshort=SVM,
    long=support vector machine \DeclareAcronymrlmshort=RLM, long=regularized loss
    minimization \DeclareAcronymlamsshort=LAMS, long=min-sum decoding with linear
    approximation \DeclareAcronympbldpcshort=PB-LDPC, long=protograph-based low-density
    parity-check \DeclareAcronymlutshort=LUT, long=look-up table \DeclareAcronymllrshort=LLR,
    long=log-likelihood ratio \DeclareAcronymhddshort=HDD, long=hard-decision decoder
    \DeclareAcronymrrdshort=RRD, long=random redundant decoding \DeclareAcronymmrrdshort=mRRD,
    long=modified random redundant decoding \DeclareAcronymmbbpshort=MBBP, long=multiple-bases
    belief-propagation \DeclareAcronymncrdshort=NC-RD, long=node-classified redundant
    decoding \DeclareAcronymhdpcshort=HDPC, long=high-density parity-check \DeclareAcronymlpshort=LP,
    long=linear programming \DeclareAcronymadmmshort=ADMM, long=alternating direction
    method of multipliers \DeclareAcronymgldpcshort=GLDPC, long=generalized low-density
    parity-check \DeclareAcronymfaidshort=FAID, long=finite alphabet iterative decoder
    \DeclareAcronymrqnnshort=RQNN, long=recurrent quantized neural network \DeclareAcronymsteshort=STE,
    long=straight-through estimator \DeclareAcronymttshort=TT, long=tensor-train \DeclareAcronymtrshort=TR,
    long=tensor-ring \DeclareAcronymharqshort=HARQ, long=hybrid automatic repeat request
    \DeclareAcronympnnshort=PNN, long=partitioned neural network \DeclareAcronymspcshort=SPC,
    long=single parity check \DeclareAcronymrcshort=RC, long=repetition code \DeclareAcronymnscshort=NSC,
    long=neural successive cancellation \DeclareAcronymbfshort=BF, long=bit flipping
    \DeclareAcronymsclshort=SCL, long=successive cancellation list \DeclareAcronymscfshort=SCF,
    long=successive cancellation flip \DeclareAcronymsclfshort=SCLF, long=successive
    cancellation list flip \DeclareAcronymfsclshort=FSCL, long=fast successive cancellation
    list \DeclareAcronymfscfshort=FSCF, long=fast successive cancellation flip \DeclareAcronymfsclfshort=FSCLF,
    long=fast successive cancellation list clip \DeclareAcronymdscfshort=DSCF, long=dynamic
    successive cancellation flip \DeclareAcronymdsclfshort=DSCLF, long=dynamic successive
    cancellation list flip \DeclareAcronymdncshort=DNC, long=differentiable neural
    computer \DeclareAcronymspshort=SP, long=shifted-pruning \DeclareAcronymitdshort=ITD,
    long=iterative threshold decoding \DeclareAcronymhornnshort=HORNN, long=high-order
    recurrent neural network \DeclareAcronymrscshort=RSC, long=recursive systematic
    convolutional \DeclareAcronymbcjrshort=BCJR, long=Bahl-Cocke-Jelinek-Raviv \DeclareAcronymrmshort=RM,
    long=Reed-Muller \DeclareAcronymxaishort=XAI, long=explainable AI \DeclareAcronymqmlshort=QML,
    long=quantum machine learning \DeclareAcronymnisqshort=NISQ, long=noisy intermediate-scale
    quantum \DeclareAcronymvqeshort=VQE, long=variational quantum eigensolver \DeclareAcronymqaoashort=QAOA,
    long=quantum approximate optimization algorithm \DeclareAcronymganshort=GAN, long=generative
    adversarial network \DeclareAcronymisishort=ISI, long=inter-symbol interference
    \DeclareAcronymbisoshort=BISO, long=binary-input symmetric-output
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: dlshort=DL, long=深度学习 \DeclareAcronymldpcshort=LDPC, long=低密度奇偶校验 \DeclareAcronymxrshort=XR,
    long=扩展现实 \DeclareAcronymv2xshort=V2X, long=车联网 \DeclareAcronymembbshort=eMBB,
    long=增强型移动宽带 \DeclareAcronymurllcshort=URLLC, long=超可靠低延迟通信 \DeclareAcronymmmtcshort=mMTC,
    long=大规模机器类型通信 \DeclareAcronymkpishort=KPI, long=关键绩效指标 \DeclareAcronymmlshort=ML,
    long=机器学习 \DeclareAcronymgpushort=GPU, long=图形处理单元 \DeclareAcronymcudashort=CUDA,
    long=计算统一设备架构 \DeclareAcronymfpgashort=FPGA, long=现场可编程门阵列 \DeclareAcronymtpushort=TPU,
    long=张量处理单元 \DeclareAcronymaeshort=AE, long=自编码器 \DeclareAcronymmushort=MU, long=多用户
    \DeclareAcronymmimoshort=MIMO, long=多输入多输出 \DeclareAcronym3gppshort=3GPP, long=第三代合作伙伴计划
    \DeclareAcronymdnnshort=DNN, long=深度神经网络 \DeclareAcronymmlpshort=MLP, long=多层感知器
    \DeclareAcronymrelushort=ReLU, long=修正线性单元 \DeclareAcronymbgdshort=BGD, long=批量梯度下降
    \DeclareAcronymsgdshort=SGD, long=随机梯度下降 \DeclareAcronymmbsgdshort=MBSGD, long=迷你批量随机梯度下降
    \DeclareAcronymllmshort=LLM, long=大型语言模型 \DeclareAcronymrlshort=RL, long=强化学习
    \DeclareAcronymmdpshort=MDP, long=马尔可夫决策过程 \DeclareAcronymdrlshort=DRL, long=深度强化学习
    \DeclareAcronymdqnshort=DQN, long=深度Q网络 \DeclareAcronymbershort=BER, long=比特错误率
    \DeclareAcronympaprshort=PAPR, long=峰均功率比 \DeclareAcronymofdmshort=OFDM, long=正交频分复用
    \DeclareAcronymcnnshort=CNN, long=卷积神经网络 \DeclareAcronymnlpshort=NLP, long=自然语言处理
    \DeclareAcronymilsvrcshort=ILSVRC, long=ImageNet大规模视觉识别挑战 \DeclareAcronymrnnshort=RNN,
    long=递归神经网络 \DeclareAcronymlstmshort=LSTM, long=长短期记忆 \DeclareAcronymgrushort=GRU,
    long=门控递归单元 \DeclareAcronymgnnshort=GNN, long=图神经网络 \DeclareAcronymmpnnshort=MPNN,
    long=消息传递神经网络 \DeclareAcronymgptshort=GPT, long=生成预训练变换器 \DeclareAcronymllamashort=LLaMA,
    long=大型语言模型Meta AI \DeclareAcronympalmshort=PaLM, long=Pathways语言模型 \DeclareAcronymdmshort=DM,
    long=扩散模型 \DeclareAcronymddpmshort=DDPM, long=去噪扩散概率模型 \DeclareAcronymdeshort=DE,
    long=密度演化 \DeclareAcronymexitshort=EXIT, long=外在信息传递 \DeclareAcronymbpshort=BP,
    long=信念传播 \DeclareAcronymawgnshort=AWGN, long=加性白噪声 \DeclareAcronymbiawgnshort=BI-AWGN,
    long=二进制输入加性白噪声 \DeclareAcronymscshort=SC, long=连续取消 \DeclareAcronymvnshort=VN,
    long=变量节点 \DeclareAcronymcnshort=CN, long=校验节点 \DeclareAcronymsnrshort=SNR, long=信噪比
    \DeclareAcronymgashort=GA, long=高斯近似 \DeclareAcronymrcashort=RCA, long=互逆通道近似
    \DeclareAcronymcrcshort=CRC, long=循环冗余校验 \DeclareAcronymcasclshort=CA-SCL, long=循环冗余校验辅助连续取消列表
    \DeclareAcronymblershort=BLER, long=块错误率 \DeclareAcronympgdshort=PGD, long=投影梯度下降
    \DeclareAcronympccmpshort=PCCMP, long=极化码构造消息传递 \DeclareAcronyma2cshort=A2C, long=优势演员-评论家
    \DeclareAcronymisitshort=ISIT, long=国际信息理论研讨会 \DeclareAcronympacshort=PAC, long=极化调整卷积
    \DeclareAcronymmapshort=MAP, long=最大后验 \DeclareAcronymbigrushort=Bi-GRU, long=双向门控递归单元
    \DeclareAcronymecctshort=ECCT, long=纠错码变换器 \DeclareAcronympcmshort=PCM, long=奇偶校验矩阵
    \DeclareAcronymbpskshort=BPSK, long=二进制相位键控 \DeclareAcronymbchshort=BCH, long=博斯-乔杜里-霍克恩亨
    \DeclareAcronymsisoshort=SISO, long=软输入软输出 \DeclareAcronymmindshort=MIND, long=模型独立神经解码器
    \DeclareAcronymosdshort=OSD, long=有序统计解码 \DeclareAcronymsttmramshort=STT-MRAM,
    long=自旋转矩随机存取内存 \DeclareAcronymmsshort=MS, long=最小和 \DeclareAcronymnmsshort=NMS,
    long=归一化最小和 \DeclareAcronymomsshort=OMS, long=偏移最小和 \DeclareAcronymnomsshort=NOMS,
    long=归一化偏移最小和 \DeclareAcronymamsshort=AMS, long=调整最小和 \DeclareAcronymsmmsshort=SMMS,
    long=单最小最小和 \DeclareAcronymvwmsshort=VWMS, long=变量权重最小和 \DeclareAcronymscmsshort=SCMS,
    long=自校正最小和 \DeclareAcronymrbmsshort=RBMS, long=基于可靠性的最小和 \DeclareAcronymsvmshort=SVM,
    long=支持向量机 \DeclareAcronymrlmshort=RLM, long=正则化损失最小化 \DeclareAcronymlamsshort=LAMS,
    long=线性近似最小和解码 \DeclareAcronympbldpcshort=PB-LDPC, long=原型图基础低密度奇偶校验 \DeclareAcronymlutshort=LUT,
    long=查找表 \DeclareAcronymllrshort=LLR, long=对数似然比 \DeclareAcronymhddshort=HDD,
    long=硬判决解码器 \DeclareAcronymrrdshort=RRD, long=随机冗余解码 \DeclareAcronymmrrdshort=mRRD,
    long=修改版随机冗余解码 \DeclareAcronymmbbpshort=MBBP, long=多基信念传播 \DeclareAcronymncrdshort=NC-RD,
    long=节点分类冗余解码 \DeclareAcronymhdpcshort=HDPC, long=高密度奇偶校验 \DeclareAcronymlpshort=LP,
    long=线性规划 \DeclareAcronymadmmshort=ADMM, long=交替方向乘子法 \DeclareAcronymgldpcshort=GLDPC,
    long=广义低密度奇偶校验 \DeclareAcronymfaidshort=FAID, long=有限字母表迭代解码器 \DeclareAcronymrqnnshort=RQNN,
    long=递归量化神经网络 \DeclareAcronymsteshort=STE, long=直通估计 \DeclareAcronymttshort=TT,
    long=张量列 \DeclareAcronymtrshort=TR, long=张量环 \DeclareAcronymharqshort=HARQ, long=混合自动重传请求
    \DeclareAcronympnnshort=PNN, long=分区神经网络 \DeclareAcronymspcshort=SPC, long=单奇偶校验
    \DeclareAcronymrcshort=RC, long=重复码 \DeclareAcronymnscshort=NSC, long=神经连续取消 \DeclareAcronymbfshort=BF,
    long=比特翻转 \DeclareAcronyms
- en: Recent Advances in Deep Learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的最新进展
- en: 'for Channel Coding: A Survey'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关于信道编码的综述
- en: 'Toshiki Matsumine, , and Hideki Ochiai T. Matsumine is with the Institute of
    Advanced Sciences, Yokohama National University, Yokohama, Japan (e-mail: matsumine-toshiki-mh@ynu.ac.jp)H.
    Ochiai is with the Graduate School of Engineering, Osaka University, Osaka, Japan
    (e-mail: ochiai@comm.eng.osaka-u.ac.jp)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 松峰俊树，，以及落合秀树T. 松峰隶属于横滨国立大学先进科学研究所，位于日本横滨（电子邮箱：matsumine-toshiki-mh@ynu.ac.jp）H.
    落合隶属于大阪大学工程研究生院，位于日本大阪（电子邮箱：ochiai@comm.eng.osaka-u.ac.jp）
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper provides a comprehensive survey on recent advances in deep learning
    (DL) techniques for the channel coding problems. Inspired by the recent successes
    of DL in a variety of research domains, its applications to the physical layer
    technologies have been extensively studied in recent years, and are expected to
    be a potential breakthrough in supporting the emerging use cases of the next generation
    wireless communication systems such as 6G. In this paper, we focus exclusively
    on the channel coding problems and review existing approaches that incorporate
    advanced DL techniques into code design and channel decoding. After briefly introducing
    the background of recent DL techniques, we categorize and summarize a variety
    of approaches, including model-free and mode-based DL, for the design and decoding
    of modern error-correcting codes, such as low-density parity check (LDPC) codes
    and polar codes, to highlight their potential advantages and challenges. Finally,
    the paper concludes with a discussion of open issues and future research directions
    in channel coding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对深度学习（DL）技术在信道编码问题上的最新进展进行了全面综述。受DL在多个研究领域取得的成功启发，近年来其在物理层技术中的应用得到了广泛研究，并预计会成为支持下一代无线通信系统（如6G）新兴应用的潜在突破点。本文专注于信道编码问题，回顾了将先进DL技术纳入编码设计和信道解码的现有方法。在简要介绍了最新DL技术的背景后，我们对现代纠错码（如低密度奇偶校验（LDPC）码和极化码）的设计和解码进行了分类和总结，包括无模型和有模型的DL，以突出其潜在优势和挑战。最后，本文以讨论信道编码中的开放问题和未来研究方向作为结论。
- en: 'Index Terms:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Channel coding, deep learning (DL), low-density parity check (LDPC) codes, machine
    learning (ML), neural network, polar codes, turbo codes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 信道编码、深度学习（DL）、低密度奇偶校验（LDPC）码、机器学习（ML）、神经网络、极化码、涡轮码。
- en: I Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Channel coding is a well-established area of research with a long history dating
    back to the Shannon’s theory [[1](#bib.bib1)] where he introduced the *Shannon
    limit* as the maximum rate at which information can be transmitted over a given
    communication channel. Subsequently, researchers have made tremendous efforts
    to develop a practical coding scheme that approaches the Shannon limit at a realistic
    implementation cost [[2](#bib.bib2)]. The notable successes in coding theory include
    the invention of modern capacity-approaching codes such as turbo codes [[3](#bib.bib3)],
    \acldpc codes [[4](#bib.bib4)], and polar codes [[5](#bib.bib5)]. These coding
    techniques have contributed significantly to various communication systems, such
    as wired and wireless communications, as well as storage systems, from the viewpoint
    of improving reliability and energy efficiency.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 信道编码是一个有着悠久历史的成熟研究领域，源于香农的理论[[1](#bib.bib1)]，他提出了*香农极限*，作为在给定通信信道上可以传输的信息的最大速率。随后，研究人员付出了巨大的努力，以开发一种实际的编码方案，接近香农极限，并具有实际实施成本[[2](#bib.bib2)]。编码理论中的显著成功包括现代接近容量的编码的发明，如涡轮码[[3](#bib.bib3)]，\acl
    dpc码[[4](#bib.bib4)]和极化码[[5](#bib.bib5)]。这些编码技术从提高可靠性和能效的角度，对各种通信系统，如有线和无线通信以及存储系统，做出了重要贡献。
- en: Due to the emerging wireless applications, including \acxr for telemedicine,
    tactile Internet, \acv2x, and wireless data centers, the next generation wireless
    communication systems impose unprecedentedly diverse and stringent requirements
    for, e.g., ultra-high data rate, ultra-low latency, and high energy efficiency
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    In 5G, the main use cases are \acembb, \acurllc, and \acmmtc, and for each case,
    the system requirement is specified in terms of a single \ackpi, such as throughput,
    latency, reliability, and energy efficiency. On the other hand, due to the diversity
    of applications, many use cases in the next wireless communications such as 6G
    will require trade-offs among different \acpkpi, which poses a new challenge for
    the design of physical layer techniques [[33](#bib.bib33), [36](#bib.bib36), [37](#bib.bib37)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新兴的无线应用，包括\acxr用于远程医疗、触觉互联网、\acv2x和无线数据中心，下一代无线通信系统对例如超高数据速率、超低延迟和高能效等方面提出了前所未有的多样且严格的要求[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)]。在5G中，主要的使用案例包括\acembb、\acurllc和\acmmtc，对于每个案例，系统需求通过单个\ackpi来定义，例如吞吐量、延迟、可靠性和能效。另一方面，由于应用的多样性，下一代无线通信中的许多使用案例，如6G，将需要在不同的\acpkpi之间进行权衡，这对物理层技术的设计提出了新的挑战[[33](#bib.bib33),
    [36](#bib.bib36), [37](#bib.bib37)]。
- en: Traditionally, the design of coding schemes has been based on mathematical models
    and expert knowledge, such as coding theory and information theory. Although this
    approach has contributed significantly to the recent progress in practical channel
    coding, it also has limitations. Specifically, it relies on mathematical models
    that do not fully capture real-world environments, and thus there is always a
    mismatch between the model for which we design systems and the actual environment
    to which they are applied. In addition, in the next generation communications,
    the system design problem will become increasingly complex due to demanding requirements
    and therefore will not be mathematically tractable in most cases. To address these
    issues, *data-driven* approaches to communication system design based on \acml
    techniques have emerged as a new paradigm that supports or replaces the conventional
    system design based on mathematical models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，编码方案的设计基于数学模型和专家知识，如编码理论和信息理论。虽然这种方法对实际信道编码的近期进展做出了重要贡献，但也存在局限性。具体来说，它依赖于不完全捕捉真实世界环境的数学模型，因此我们设计系统的模型与实际应用的环境之间总是存在不匹配。此外，在下一代通信中，由于要求苛刻，系统设计问题将变得越来越复杂，因此在大多数情况下无法通过数学方法解决。为了解决这些问题，基于\acml技术的*数据驱动*通信系统设计方法已成为一种新的范式，支持或替代传统的基于数学模型的系统设计。
- en: In particular, inspired by the recent successes of \acdl technologies in broad
    research areas, their applications in communication systems have been extensively
    studied. This DL trend has been accelerated by the development of dedicated DL
    frameworks, such as Tensorflow [[38](#bib.bib38)] and Pytorch [[39](#bib.bib39)],
    which makes it easier for researchers to implementate their DL algorithms. Furthermore,
    most of them are built with \acgpu acceleration provided by the NVIDIA \accuda
    Deep Neural Network library (cuDNN), which significantly speeds up DL training
    due to its ability to perform parallel computations and high memory bandwidth.
    In addition, various DL processors such as \acpfpga and \actpu have been explored
    in the literature [[40](#bib.bib40), [41](#bib.bib41)], enabling efficient hardware
    implementations of DL-based communications and networking in beyond 5G and 6G.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，受到近期 \acdl 技术在广泛研究领域取得的成功的启发，它们在通信系统中的应用已被广泛研究。这一深度学习趋势由于专用深度学习框架的开发而加速，例如
    Tensorflow [[38](#bib.bib38)] 和 Pytorch [[39](#bib.bib39)]，这些框架使研究人员能够更容易地实现他们的深度学习算法。此外，大多数框架都利用了由
    NVIDIA \accuda 深度神经网络库（cuDNN）提供的 \acgpu 加速，这显著加快了深度学习的训练速度，因为其能够进行并行计算和高内存带宽。此外，各种深度学习处理器如
    \acpfpga 和 \actpu 在文献中也有所探讨 [[40](#bib.bib40), [41](#bib.bib41)]，这使得在超越 5G 和 6G
    的深度学习通信和网络硬件实现变得高效。
- en: 'TABLE I: Survey papers related to our work.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 与我们工作相关的调查论文。'
- en: '| Year | Reference | Contents related to channel coding. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 参考文献 | 与信道编码相关的内容。 |'
- en: '| --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 2017 | Wang et al. [[42](#bib.bib42)] | Review of early works on DL-based
    decoding. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | Wang 等人 [[42](#bib.bib42)] | 对早期基于深度学习的解码工作的综述。 |'
- en: '| 2019 | Zhang et al. [[43](#bib.bib43)] | Brief introduction of DL-based channel
    decoding. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | Zhang 等人 [[43](#bib.bib43)] | 对基于深度学习的信道解码进行简要介绍。 |'
- en: '| Gunduz et al. [[44](#bib.bib44)] | Brief review of DL-aided decoding. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Gunduz 等人 [[44](#bib.bib44)] | 深度学习辅助解码的简要综述。 |'
- en: '| Balatsoukas et al. [[45](#bib.bib45)] | Review of deep unfolding for channel
    decoding. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Balatsoukas 等人 [[45](#bib.bib45)] | 深度展开用于信道解码的综述。 |'
- en: '| 2020 | Samad et al. [[46](#bib.bib46)] | Addressing DL-based channel decoding.
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 2020 | Samad 等人 [[46](#bib.bib46)] | 针对基于深度学习的信道解码进行讨论。 |'
- en: '| Zhang et al. [[47](#bib.bib47)] | Review of DL-based decoding and code construction.
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 [[47](#bib.bib47)] | 对基于深度学习的解码和码构造的综述。 |'
- en: '| 2021 | Ly et al. [[48](#bib.bib48)] | Review of DL applications for LDPC
    code identification, decoding. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | Ly 等人 [[48](#bib.bib48)] | 深度学习在 LDPC 码识别和解码中的应用综述。 |'
- en: '| 2023 | Mao et al. [[49](#bib.bib49)] | Briefly addressing DL-aided decoder
    and genetic algorithm for code construction. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2023 | Mao 等人 [[49](#bib.bib49)] | 简要介绍深度学习辅助解码器和用于码构造的遗传算法。 |'
- en: '| Akrout et al. [[50](#bib.bib50)] | Domain generalization [[51](#bib.bib51),
    [52](#bib.bib52)] in the channel decoding problem. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Akrout 等人 [[50](#bib.bib50)] | 信道解码问题中的领域泛化 [[51](#bib.bib51), [52](#bib.bib52)]。
    |'
- en: '| 2024 | Ye et al. [[53](#bib.bib53)] | Review of DL-based decoding for turbo,
    LDPC, and polar codes. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2024 | Ye 等人 [[53](#bib.bib53)] | 对用于涡轮码、LDPC 码和极化码的深度学习解码的综述。 |'
- en: '| Rowshan et al. [[54](#bib.bib54)] | Comprehensive survey on channel coding
    with brief introduction of DL for channel decoding. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Rowshan 等人 [[54](#bib.bib54)] | 对信道编码的全面调查，并简要介绍了深度学习在信道解码中的应用。 |'
- en: I-A DL Applications to The Physical Layer
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 深度学习在物理层的应用
- en: In recent years, DL has been successfully applied to the physical layer of communication
    systems ¹¹1We note that ML techniques for the physical layer have been studied
    sporadically for many years, e.g., in [[55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)].
    However, the applications of DL are rather new, which started to flourish around
    2016. [[58](#bib.bib58), [42](#bib.bib42), [59](#bib.bib59), [60](#bib.bib60),
    [44](#bib.bib44), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [46](#bib.bib46), [65](#bib.bib65), [47](#bib.bib47), [48](#bib.bib48), [31](#bib.bib31),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [49](#bib.bib49), [69](#bib.bib69),
    [70](#bib.bib70), [50](#bib.bib50), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)].
    One of the seminal works is [[58](#bib.bib58)], where the authors introduced the
    concept of end-to-end learning for communication systems, which is considered
    as \acae. The authors also introduced other DL applications, such as modulation
    classification and radio transformer networks. Later on, many papers discussed
    potential applications of DL to communication problems, such as channel decoding,
    signal detection, channel modeling, and \acmimo signal detection [[42](#bib.bib42),
    [59](#bib.bib59), [60](#bib.bib60), [44](#bib.bib44), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习已成功应用于通信系统的物理层¹¹1我们注意到，物理层的机器学习技术已经被零星研究了很多年，例如，在 [[55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)] 中。然而，深度学习的应用相对较新，开始在 2016 年左右蓬勃发展 [[58](#bib.bib58),
    [42](#bib.bib42), [59](#bib.bib59), [60](#bib.bib60), [44](#bib.bib44), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [46](#bib.bib46), [65](#bib.bib65),
    [47](#bib.bib47), [48](#bib.bib48), [31](#bib.bib31), [66](#bib.bib66), [67](#bib.bib67),
    [68](#bib.bib68), [49](#bib.bib49), [69](#bib.bib69), [70](#bib.bib70), [50](#bib.bib50),
    [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)]。其中一项开创性的工作是 [[58](#bib.bib58)]，作者在其中引入了通信系统端到端学习的概念，这被认为是\acae。作者还介绍了其他深度学习应用，如调制分类和无线变换网络。后来，许多论文讨论了深度学习在通信问题上的潜在应用，如信道解码、信号检测、信道建模和
    \acmimo 信号检测 [[42](#bib.bib42), [59](#bib.bib59), [60](#bib.bib60), [44](#bib.bib44),
    [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)]。
- en: One of the notable advances in DL for the physical layer is the development
    of an open source Python library, called *Sionna*, which was released by NVIDIA
    in 2022 [[74](#bib.bib74)]. It supports link-level simulation of \acmu-MIMO systems
    with 5G-compliant channel codes, the \ac3gpp channel models, channel estimation,
    and so forth. Each building block is implemented using TensorFlow and allows for
    gradient-based optimization via backpropagation. Sionna alo has a native NVIDIA
    GPU support.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理层的深度学习领域，一个显著的进展是开源 Python 库的开发，该库名为*Sionna*，由 NVIDIA 于 2022 年发布 [[74](#bib.bib74)]。该库支持具有
    5G 兼容信道编码的 \acmu-MIMO 系统的链路级仿真、\ac3gpp 信道模型、信道估计等。每个构建模块都使用 TensorFlow 实现，并通过反向传播允许基于梯度的优化。Sionna
    还原生支持 NVIDIA GPU。
- en: As the number of publications related to DL-based approaches for physical layer
    technologies has been increasing almost exponentially, it is important to classify
    and summarize them to highlight the current state and challenges. However, despite
    its importance, only a handful of survey papers have been dedicated to the channel
    coding problems so far.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着与基于深度学习（DL）的物理层技术相关的出版物数量几乎呈指数增长，分类和总结这些文献以突出当前状态和挑战变得非常重要。然而，尽管其重要性，仅有少数几篇综述论文专注于信道编码问题。
- en: I-B Our Scope and Related Works
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 我们的范围与相关工作
- en: 'Existing papers on DL for channel coding may be classified into the following
    categories:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现有关于深度学习用于信道编码的论文可以分为以下几类：
- en: '1.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: DL-based code design,
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于深度学习的代码设计，
- en: '2.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: DL-based channel decoding,
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于深度学习的信道解码，
- en: '3.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: End-to-end learning for communication systems.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 面向通信系统的端到端学习。
- en: In DL-based code design, the optimization of code parameters, such as the degree
    distribution of LDPC codes and the locations of frozen bits in polar codes, is
    performed using DL techniques. Channel decoding is a popular DL application as
    the decoding problem is essentially the classification problem which is what DL
    is good at. This approach utilizes a \acdnn to replace or augment a conventional
    channel decoder with the purpose of improving the error correction performance
    or reducing the complexity and/or latency. End-to-end learning of communication
    systems is another popular application, where a transmitter-receiver pair is often
    completely replaced by “black-box” DNNs and trained over a differential channel
    model in an end-to-end fashion. In this approach, not only a channel encoder-decoder
    pair can be trained, but also other physical layer components such as source encoder-decoder
    and symbol mapper-demapper.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于深度学习的码设计中，代码参数的优化，如LDPC码的度分布和极化码中冻结位的位置，是使用深度学习技术进行的。信道解码是一个热门的深度学习应用，因为解码问题本质上是分类问题，而这正是深度学习擅长的领域。这种方法利用\acdnn替代或增强传统信道解码器，以提高错误更正性能或降低复杂性和/或延迟。端到端通信系统学习是另一个流行的应用，其中发射器-接收器对通常被完全替换为“黑箱”深度神经网络，并在端到端的方式下通过差分信道模型进行训练。在这种方法中，不仅可以训练信道编码器-解码器对，还可以训练其他物理层组件，如源编码器-解码器和符号映射器-解映射器。
- en: Although end-to-end learning has been extensively studied in the literature,
    and this approach would be particularly promising for a new paradigm of semantic
    communication [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [68](#bib.bib68), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [70](#bib.bib70), [72](#bib.bib72)], the complete replacement
    of transceivers by DNNs poses several challenges in practice and does not seem
    feasible at this time. Therefore, this paper focuses on DL-based approaches that
    may be applicable to existing systems with appropriate modifications. In particular,
    we consider code design using offline DL techniques and DL-based channel decoding
    to replace or support conventional decoders²²2Although we mainly focus on *classical*
    error-correcting codes in this paper, DL-based code design and decoding has also
    been extensively studied in the realm of *quantum* error-correcting codes, e.g.,
    in [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90)]..
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管端到端学习在文献中得到了广泛研究，并且这种方法对语义通信的新范式尤为有前景 [[75](#bib.bib75), [76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80), [68](#bib.bib68),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [70](#bib.bib70), [72](#bib.bib72)]，但用深度神经网络完全替代发射接收器在实践中存在一些挑战，目前似乎不可行。因此，本文专注于可能适用于现有系统的深度学习方法，进行适当的修改。特别是，我们考虑使用离线深度学习技术进行码设计和基于深度学习的信道解码，以替代或支持传统解码器²²2尽管本文主要关注*经典*纠错码，但基于深度学习的码设计和解码在*量子*纠错码领域也得到了广泛研究，例如在
    [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90)]。
- en: 'In Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), we summarize existing survey papers related to
    our scope in this work. We emphasize that most of the existing survey papers cover
    a wide range of DL applications in the physical layer, rather than dealing with
    DL-assisted channel coding in a comprehensive manner. Nevertheless, the paper
    most closely related to our work would be [[47](#bib.bib47)], in which the authors
    discussed a wide range of DL applications in the physical layer, including the
    channel coding problems such as channel decoding and code construction. However,
    since its publication in 2020, a significant number of new techniques have been
    proposed. By focusing solely on the channel coding problems, we attempt to provide
    a comprehensive survey including the state-of-the-art techniques.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey") 中，我们总结了与我们工作范围相关的现有调查论文。我们强调，大多数现有的调查论文涵盖了物理层中各种深度学习应用，而不是全面处理深度学习辅助的信道编码。然而，与我们的工作最相关的论文是
    [[47](#bib.bib47)]，其中作者讨论了物理层中各种深度学习应用，包括信道编码问题，如信道解码和码构造。然而，自2020年出版以来，已经提出了大量的新技术。通过专注于信道编码问题，我们尝试提供一个全面的调查，包括最先进的技术。'
- en: 'The overall organization of this paper is visualized in Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Our Scope and Related Works ‣ I Introduction ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey"). In Section [II](#S2 "II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"),
    we briefly introduce the basics of DL techniques and the state-of-the-art models
    to facilitate the understanding of our survey. In Section [III](#S3 "III DL for
    Code Design ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"),
    we review DL-based design of LDPC and polar codes. Then, in Section [IV](#S4 "IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"), we consider various DL approaches to the channel decoding problem.
    Finally, we conclude this paper by discussing the challenges and future directions
    in Section [V](#S5 "V Conclusion ‣ Recent Advances in Deep Learning for Channel
    Coding: A Survey") to stimulate further research. We provide a list of abbreviations
    that we use after Section [V](#S5 "V Conclusion ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的整体结构在图[1](#S1.F1 "Figure 1 ‣ I-B Our Scope and Related Works ‣ I Introduction
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")中可视化。在第[II](#S2
    "II Brief Introduction of Deep Learning ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey")节中，我们简要介绍了深度学习技术的基础和最前沿的模型，以便于理解我们的调研。在第[III](#S3 "III
    DL for Code Design ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")节中，我们回顾了基于深度学习的LDPC和极化码设计。接着，在第[IV](#S4
    "IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey")节中，我们考虑了各种深度学习方法用于信道解码问题。最后，在第[V](#S5 "V Conclusion ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey")节中，我们讨论了挑战和未来方向，以刺激进一步研究。我们在第[V](#S5
    "V Conclusion ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")节后提供了使用的缩写列表。'
- en: '![Refer to caption](img/840fb1ce69b8c50b4867e0155d019a2a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/840fb1ce69b8c50b4867e0155d019a2a.png)'
- en: 'Figure 1: Organization of this survey paper.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本文调查的组织结构。
- en: II Brief Introduction of Deep Learning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 深度学习简要介绍
- en: In this section, we briefly review the basics of DL technologies, starting with
    neural networks and their optimization. We then introduce the state-of-the-art
    training and DL models. For details on the theory of general DL techniques, please
    refer, e.g., to [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要回顾了深度学习技术的基础，从神经网络及其优化开始。然后，我们介绍了最前沿的训练和深度学习模型。有关深度学习技术理论的详细信息，请参见，例如，[[91](#bib.bib91),
    [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95), [96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100)]。
- en: II-A Basic Principle of DL
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度学习的基本原理
- en: DL is a subfield of ML that uses DNNs with multiple hidden layers between input
    and output layers. Among various neural network structures, \acmlp is a class
    of fully connected feedforward neural networks that consist of at least one hidden
    layer in addition to input and output layers [[101](#bib.bib101)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子领域，使用具有多个隐藏层的深度神经网络。在各种神经网络结构中，\acmlp是一类完全连接的前馈神经网络，除了输入和输出层外，还包括至少一个隐藏层[[101](#bib.bib101)]。
- en: '![Refer to caption](img/470593663da8895007c18d131cf83366.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/470593663da8895007c18d131cf83366.png)'
- en: 'Figure 2: An example of MLP with single hidden layer.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：单隐层MLP的示例。
- en: 'An example of a single hidden layer MLP is shown in Fig. [2](#S2.F2 "Figure
    2 ‣ II-A Basic Principle of DL ‣ II Brief Introduction of Deep Learning ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey"), where the input vector
    $\mathbf{x}\in\mathbb{R}^{3}$ is mapped to the output vector $\mathbf{y}\in\mathbb{R}^{2}$
    by applying a series of affine transformations and nonlinear activation functions
    as'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S2.F2 "Figure 2 ‣ II-A Basic Principle of DL ‣ II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")展示了一个单隐层MLP的示例，其中输入向量$\mathbf{x}\in\mathbb{R}^{3}$通过应用一系列仿射变换和非线性激活函数被映射到输出向量$\mathbf{y}\in\mathbb{R}^{2}$。'
- en: '|  | $\displaystyle\mathbf{y}$ | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\mathbf{h}+\mathbf{b}_{1})$
    |  | (1) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{y}$ | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\mathbf{h}+\mathbf{b}_{1})$
    |  | (1) |'
- en: '|  |  | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\phi_{0}(\mathbf{W}_{0}\mathbf{x}+\mathbf{b}_{0})+\mathbf{b}_{1}),$
    |  | (2) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\phi_{0}(\mathbf{W}_{0}\mathbf{x}+\mathbf{b}_{0})+\mathbf{b}_{1}),$
    |  | (2) |'
- en: where $\mathbf{W}_{0}\in\mathbb{R}^{3\times 3}$ and $\mathbf{W}_{1}\in\mathbb{R}^{3\times
    2}$ are weight matrices, $\mathbf{b}_{0}\in\mathbb{R}^{3}$ and $\mathbf{b}_{1}\in\mathbb{R}^{2}$
    are bias terms, and $\phi_{i}(\cdot)$ with $i\in\{0,1\}$ denotes the element-wise
    application of a nonlinear activation function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_{0}\in\mathbb{R}^{3\times 3}$ 和 $\mathbf{W}_{1}\in\mathbb{R}^{3\times
    2}$ 是权重矩阵，$\mathbf{b}_{0}\in\mathbb{R}^{3}$ 和 $\mathbf{b}_{1}\in\mathbb{R}^{2}$
    是偏置项，$\phi_{i}(\cdot)$ 其中 $i\in\{0,1\}$ 表示非线性激活函数的逐元素应用。
- en: The nonlinear activation function allows the neural network to approximate highly
    complex functions, and the choice of activation functions has a significant impact
    on the resulting performance. Although there are a number of activation functions
    [[102](#bib.bib102)], one of the most widely used modern activation functions
    is \acrelu [[103](#bib.bib103)] and its variants such as Leaky ReLU [[104](#bib.bib104)]
    and parametric ReLU [[105](#bib.bib105)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活函数使得神经网络能够逼近高度复杂的函数，激活函数的选择对最终性能有着显著的影响。虽然有许多激活函数[[102](#bib.bib102)]，但现代最广泛使用的激活函数之一是
    \acrelu [[103](#bib.bib103)] 及其变种，如 Leaky ReLU [[104](#bib.bib104)] 和 parametric
    ReLU [[105](#bib.bib105)]。
- en: As for the optimization of the DNN parameters, i.e., $\Theta\triangleq\{\mathbf{W}_{0},\mathbf{W}_{1},\mathbf{b}_{0},\mathbf{b}_{1}\}$
    in our example, the most common approach is gradient descent, which is a first-order
    iterative algorithm for finding a local minimum of a differentiable function.
    The basic idea of gradient descent is to update the parameters in the opposite
    direction of the gradient of the differentiable loss function that we wish to
    minimize. Letting $f(\Theta)$ denote the loss function that is differentiable
    with respect to the parameter set $\Theta$, in the $i$-th iteration of gradient
    descent, the trainable parameters are updated as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 DNN 参数的优化，即我们示例中的 $\Theta\triangleq\{\mathbf{W}_{0},\mathbf{W}_{1},\mathbf{b}_{0},\mathbf{b}_{1}\}$，最常见的方法是梯度下降，这是一种一阶迭代算法，用于寻找可微函数的局部最小值。梯度下降的基本思想是沿着我们希望最小化的可微损失函数的梯度的相反方向更新参数。令
    $f(\Theta)$ 表示相对于参数集 $\Theta$ 可微的损失函数，在梯度下降的第 $i$ 次迭代中，训练参数更新为
- en: '|  | $\displaystyle\Theta_{i+1}=\Theta_{i}-\eta\nabla f(\Theta_{i}),$ |  |
    (3) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Theta_{i+1}=\Theta_{i}-\eta\nabla f(\Theta_{i}),$ |  |
    (3) |'
- en: where $\eta\in\mathbb{R}_{+}$ is a learning rate that determines the size of
    the steps taken in the direction of steepest descent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta\in\mathbb{R}_{+}$ 是一个学习率，用于决定沿着最陡下降方向采取的步伐大小。
- en: Gradient descent algorithms can be classified according to the amount of data
    used to compute the gradient, namely, \acbgd, \acsgd, and \acmbsgd. BGD calculates
    the gradients for the entire training dataset, while SGD performs a parameter
    update (and thus gradient calculation) for each training data sample. Meanwhile,
    MBSGD substitutes small data batches for single samples in SGD, thereby reducing
    the variance of the parameter updates, which can lead to more stable convergence.
    Furthermore, the gradient computation in MBSGD can be efficiently performed by
    DL libraries. For these reasons, MBSGD is one of the most popular stochastic optimization
    methods for training DNNs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法可以根据用于计算梯度的数据量进行分类，即 \acbgd、\acsgd 和 \acmbsgd。BGD 为整个训练数据集计算梯度，而 SGD 对每个训练数据样本进行参数更新（从而进行梯度计算）。与此同时，MBSGD
    用小的数据批次替代 SGD 中的单个样本，从而减少参数更新的方差，这可能导致更稳定的收敛。此外，MBSGD 中的梯度计算可以通过深度学习库高效执行。因此，MBSGD
    是训练深度神经网络中最受欢迎的随机优化方法之一。
- en: However, the standard MBSGD does not necessarily guarantee good convergence,
    and many improvements have been proposed that adaptively control the learning
    rate during training. These approaches include Momentum [[106](#bib.bib106)],
    Adagrad [[107](#bib.bib107)], Adadelta [[108](#bib.bib108)], RMSprop ³³3Originally
    proposed in [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).,
    Nadam [[109](#bib.bib109)], and Adam [[110](#bib.bib110)]. Implementations of
    these optimizers are available in DL frameworks such as Tensorflow [[38](#bib.bib38)]
    and Pytorch [[39](#bib.bib39)]. For more details on SGD algorithms, see [[111](#bib.bib111)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，标准的 MBSGD 并不一定能保证良好的收敛性，因此提出了许多在训练过程中自适应控制学习率的改进方法。这些方法包括 Momentum [[106](#bib.bib106)]、Adagrad
    [[107](#bib.bib107)]、Adadelta [[108](#bib.bib108)]、RMSprop ³³3最初在 [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
    中提出、Nadam [[109](#bib.bib109)] 和 Adam [[110](#bib.bib110)]。这些优化器的实现可以在 Tensorflow
    [[38](#bib.bib38)] 和 Pytorch [[39](#bib.bib39)] 等深度学习框架中找到。有关 SGD 算法的更多细节，请参见
    [[111](#bib.bib111)]。
- en: II-B Learning Approaches
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 学习方法
- en: There are many training methods for ML techniques. In the following, we introduce
    some of the major approaches that are often applied to the design of communication
    systems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用于机器学习（ML）技术的训练方法。接下来，我们介绍一些在通信系统设计中经常应用的主要方法。
- en: II-B1 Supervised and Unsupervised Learning
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 监督学习与无监督学习
- en: Supervised learning trains algorithms based on labeled datasets consisting of
    pairs of inputs and corresponding correct outputs, i.e., ground truth. The goal
    is to analyze patterns from a large dataset and predict outcomes for new data.
    Supervised learning is commonly used for tasks such as classification and regression.
    Since the channel decoding problem can be seen as a type of classification, the
    simplest approach would be to train a DL-based channel decoder, where a DNN is
    trained to estimate a transmitted codeword by minimizing the error between the
    correct and estimated codewords.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习基于标记数据集训练算法，该数据集由输入与相应正确输出（即真实值）对组成。目标是分析大量数据集中的模式，并预测新数据的结果。监督学习通常用于分类和回归任务。由于信道解码问题可以被视为一种分类问题，最简单的方法是训练基于深度学习（DL）的信道解码器，其中深度神经网络（DNN）通过最小化正确码字和估计码字之间的误差来训练，以估计传输的码字。
- en: Unsupervised learning is another type of ML algorithm that learns patterns from
    data without human supervision. Self-supervised learning, often used to train
    \acpllm, can also be considered unsupervised learning in the sense that it uses
    the data itself to generate supervising signals, rather than relying on human
    supervision. There is also an approach called semi-supervised learning, which
    combines both supervised learning and unsupervised learning, i.e., it uses both
    labeled and unlabeled data. Although these approaches are particularly suitable
    for the practical scenario where a sufficiently large labeled dataset is not available,
    their applications to channel decoding are rather new topics to be investigated.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是另一种机器学习算法，它从数据中学习模式，而无需人工监督。自监督学习，通常用于训练 \acpllm，也可以被视为无监督学习，因为它使用数据本身生成监督信号，而不是依赖人工监督。还有一种称为半监督学习的方法，它结合了监督学习和无监督学习，即同时使用标记和未标记的数据。尽管这些方法特别适合于在没有足够大的标记数据集的实际场景，但它们在信道解码中的应用仍然是新兴的研究话题。
- en: II-B2 Reinforcement Learning
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 强化学习
- en: \Ac
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: rl is an experience-driven autonomous learning framework where an intelligent
    agent learns to take actions in a dynamic environment in order to maximize the
    cumulative reward [[112](#bib.bib112)]. RL is typically modeled as \acmdp which
    consists of
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种以经验驱动的自主学习框架，其中智能体在动态环境中学习采取行动，以最大化累计奖励[[112](#bib.bib112)]。RL 通常被建模为
    \acmdp，它由以下组成：
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a set of environment and agent states $\mathcal{S}$
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组环境和智能体状态 $\mathcal{S}$
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a set of actions $\mathcal{A}$
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一组动作 $\mathcal{A}$
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the transition probability from state $s_{t}$ to state $s_{t+1}$ upon action
    $a_{t}$ at time $t$, denoted by $\mathcal{P}(s_{t+1}|s_{t},a_{t})$, and the corresponding
    reward function $\mathcal{R}(s_{t},a_{t},s_{t+1})$.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态 $s_{t}$ 在时间 $t$ 采取动作 $a_{t}$ 转移到状态 $s_{t+1}$ 的转移概率，记作 $\mathcal{P}(s_{t+1}|s_{t},a_{t})$，以及相应的奖励函数
    $\mathcal{R}(s_{t},a_{t},s_{t+1})$。
- en: 'The process is shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-B2 Reinforcement Learning
    ‣ II-B Learning Approaches ‣ II Brief Introduction of Deep Learning ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). At each time step $t$, the agent
    observes a state $s_{t}\in\mathcal{S}$ and takes an action $a_{t}\in\mathcal{A}$,
    following a policy $\pi(a_{t}|s_{t})$. Then the agent receives a scalar reward
    $r_{t}$, and transitions to the next state $s_{t+1}$, according to the reward
    function $\mathcal{R}(s_{t},a_{t},s_{t+1})$ and state transition probability $\mathcal{P}(s_{t+1}|s_{t},a_{t})$,
    respectively. This process continues until the agent reaches a terminal state.
    The return is the discounted, accumulated reward with the discount factor $\gamma\in(0,1]$.
    The agent aims to maximize the expectation of such long term return from each
    state.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '该过程如图[3](#S2.F3 "Figure 3 ‣ II-B2 Reinforcement Learning ‣ II-B Learning Approaches
    ‣ II Brief Introduction of Deep Learning ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey")所示。在每个时间步骤$t$，智能体观察状态$s_{t}\in\mathcal{S}$并采取动作$a_{t}\in\mathcal{A}$，遵循策略$\pi(a_{t}|s_{t})$。然后，智能体接收一个标量奖励$r_{t}$，并根据奖励函数$\mathcal{R}(s_{t},a_{t},s_{t+1})$和状态转移概率$\mathcal{P}(s_{t+1}|s_{t},a_{t})$，转移到下一个状态$s_{t+1}$。这个过程持续进行，直到智能体到达终止状态。回报是折扣累积奖励，折扣因子为$\gamma\in(0,1]$。智能体的目标是最大化从每个状态的长期回报的期望。'
- en: '![Refer to caption](img/cdc352d0a7a28562faa5fb732e6729d3.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cdc352d0a7a28562faa5fb732e6729d3.png)'
- en: 'Figure 3: A typical reinforcement learning framework.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：典型的强化学习框架。
- en: In many practical problems, the states of the MDP are high-dimensional and difficult
    to solve with traditional RL algorithms. On the other hand, thanks to their powerful
    function approximation properties, the use of DNNs to approximate the optimal
    policy and/or the optimal value functions in RL provides an efficient way to overcome
    these problems. This approach, called \acdrl, has achieved remarkable results
    in a variety of research areas. In particular, \acdqn [[113](#bib.bib113)], where
    a DNN model is built to approximate the Q function (the value of an action in
    a given state), showed impressive results in Atari [[113](#bib.bib113)]. More
    details on DRL can be found, for example, in [[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际问题中，MDP的状态是高维的，难以用传统的强化学习算法解决。另一方面，得益于强大的函数逼近特性，使用DNN来逼近强化学习中的最优策略和/或最优值函数提供了一种高效的方法来克服这些问题。这种方法称为\acdrl，在各种研究领域取得了显著成果。特别是\acdqn[[113](#bib.bib113)]，其中建立了一个DNN模型来逼近Q函数（给定状态下一个动作的价值），在Atari
    [[113](#bib.bib113)]中显示了令人印象深刻的结果。有关DRL的更多细节，例如，可以参见[[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117)]。
- en: II-B3 Transfer Learning
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 迁移学习
- en: 'Transfer learning is a technique for transferring knowledge learned from a
    task in a source domain to imp performance on a related task in a target domain
    [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [122](#bib.bib122)]. Transfer learning addresses the problem of insufficient labeled
    training data by transferring the knowledge across task domains. This concept
    is illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ II-B3 Transfer Learning ‣ II-B Learning
    Approaches ‣ II Brief Introduction of Deep Learning ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey"). Note, however, that the transferred knowledge
    may be worthless if there is little or even nothing in common between the source
    and target domains.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '迁移学习是一种将从源领域任务中学到的知识转移到目标领域相关任务上以提高性能的技术[[118](#bib.bib118), [119](#bib.bib119),
    [120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122)]。迁移学习通过跨任务领域转移知识来解决标签训练数据不足的问题。这个概念在图[4](#S2.F4
    "Figure 4 ‣ II-B3 Transfer Learning ‣ II-B Learning Approaches ‣ II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")中进行了说明。然而，请注意，如果源领域和目标领域之间几乎没有共同点，那么转移的知识可能毫无价值。'
- en: '![Refer to caption](img/1357233c5e769ed7881384a8e4790871.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1357233c5e769ed7881384a8e4790871.png)'
- en: 'Figure 4: The concept of transfer learning.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：迁移学习的概念。
- en: In the channel coding problems, transfer learning can be useful to adapt DL-based
    code design and decoding trained for a certain channel model and code parameters
    to new channel models or parameters. For example, we usually train a DNN for design
    or decoding assuming a certain code rate, and adapting to a new code rate requires
    re-training of the DNN, which is time consuming and computationally expensive.
    Since codes derived from a single mother code by rate matching, i.e., puncturing
    and shortening, may have many similarities, transfer learning could be used to
    significantly reduce the computational burden of re-training or to improve the
    performance with the new parameter.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在信道编码问题中，迁移学习可以用于将基于深度学习（DL）的编码设计和解码从某一信道模型和代码参数适应到新的信道模型或参数。例如，我们通常训练一个深度神经网络（DNN）以设计或解码假设某一固定的码率，而适应新的码率需要重新训练DNN，这既耗时又计算量大。由于从单一母码通过速率匹配（即，打孔和缩短）衍生出的码可能具有许多相似性，因此迁移学习可以用来显著减少重新训练的计算负担，或者在新参数下提高性能。
- en: II-B4 Multi-Task Learning
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B4 多任务学习
- en: In multi-task learning, a set of multiple tasks is solved jointly, sharing an
    inductive bias among them [[123](#bib.bib123), [124](#bib.bib124)]. Multi-task
    learning is inherently a multi-objective problem because different tasks may conflict,
    requiring a trade-off. A common compromise is to optimize a proxy objective that
    minimizes a weighted linear combination of per-task losses. Since this joint representation
    must capture useful features across all tasks, multi-task learning can hinder
    individual task performance if the different tasks seek conflicting representations,
    i.e., the gradients of different tasks point in opposing directions or differ
    significantly in magnitude. This phenomenon is commonly known as negative transfer.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，一组多个任务被共同解决，任务之间共享归纳偏置[[123](#bib.bib123), [124](#bib.bib124)]。多任务学习本质上是一个多目标问题，因为不同任务可能会产生冲突，需要权衡。一个常见的折中方法是优化一个代理目标，该目标最小化每个任务损失的加权线性组合。由于这种联合表示必须捕捉到所有任务中的有用特征，如果不同任务寻求冲突的表示，即不同任务的梯度指向相反的方向或在幅度上有显著差异，这种现象通常被称为负迁移。
- en: There are some similarities between transfer learning and multi-task learning.
    Both aim to improve learners’ performance by knowledge transfer. On the other
    hand, the main difference is that the former transfers the knowledge contained
    in the related domains, while the latter transfers the knowledge by learning some
    related tasks simultaneously. In other words, multi-task learning pays equal attention
    to each task, while transfer learning pays more attention to the target task than
    to the source task.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习和多任务学习之间有一些相似之处。两者都旨在通过知识转移来提高学习者的性能。另一方面，主要的区别在于，前者是将相关领域中包含的知识进行转移，而后者则是通过同时学习一些相关任务来转移知识。换句话说，多任务学习对每个任务给予相等的关注，而迁移学习则更注重目标任务而非源任务。
- en: Similar to transfer learning, multi-task learning could be used to efficiently
    support multiple different code parameters. Furthermore, multi-task learning can
    be used for multi-objective optimization, which is common in many communication
    system design problems. Application examples of multi-task learning include AE-based
    constellation design that attempts to jointly minimize \acber and \acpapr for
    \acofdm systems [[125](#bib.bib125)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于迁移学习，多任务学习可以用于有效地支持多个不同的代码参数。此外，多任务学习还可以用于多目标优化，这在许多通信系统设计问题中很常见。多任务学习的应用示例包括基于AE的星座设计，尝试联合最小化\acber和\acpapr用于\acofdm系统[[125](#bib.bib125)]。
- en: II-B5 Meta-Learning
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B5 元学习
- en: Unlike traditional learning approaches that attempt to solve tasks from scratch
    with a fixed algorithm, meta-learning aims to learn the learning algorithm itself
    by learning from previous experience or tasks [[126](#bib.bib126), [127](#bib.bib127)].
    This *learning-to-learn* framework can lead to several benefits, such as improved
    data and computational efficiency.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与尝试通过固定算法从头开始解决任务的传统学习方法不同，元学习旨在通过从以前的经验或任务中学习来学习学习算法本身[[126](#bib.bib126),
    [127](#bib.bib127)]。这种*学习如何学习*的框架可以带来若干好处，例如提高数据和计算效率。
- en: In a meta-learning framework, there are two types of data, a larger data set
    of examples from related tasks (meta-training data) and a small training data
    set for a new task (meta-testing data). Standard meta-learning consists of two
    phases, 1) meta-training where a set of hyperparameters is optimized given the
    meta-training data set, and 2) meta-testing where model parameters, which are
    initialized with the meta-trained hyperparameters, are optimized using the meta-testing
    data. Thus, the meta-training phase aims to optimize hyperparameters that allow
    efficient training on a new, *a priori* unknown, target task in the meta-testing
    phase.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在元学习框架中，有两种类型的数据，一种是来自相关任务的大型数据集（元训练数据），另一种是用于新任务的小型训练数据集（元测试数据）。标准元学习包含两个阶段，1）元训练阶段，在此阶段给定元训练数据集优化一组超参数，2）元测试阶段，在此阶段使用通过元训练超参数初始化的模型参数，并利用元测试数据进行优化。因此，元训练阶段旨在优化超参数，以便在元测试阶段高效训练新的、*先验*未知的目标任务。
- en: Meta-learning could naturally be applied to adaptive decoder design, where the
    decoder parameters are initialized by meta-training and then optimized based on
    meta-testing to adapt to a new channel. In addition, the concept can be applied
    to a wide range of problems in the physical layer, such as signal demodulation,
    joint transmitter and receiver optimization via end-to-end learning, channel prediction,
    and so forth, as reviewed in [[128](#bib.bib128)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习可以自然地应用于自适应解码器设计，其中解码器参数通过元训练进行初始化，然后基于元测试进行优化，以适应新的信道。此外，这一概念可以应用于物理层的广泛问题，如信号解调、通过端到端学习进行的发射机和接收机联合优化、信道预测等，如在[[128](#bib.bib128)]中回顾。
- en: II-B6 Curriculum Learning
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B6 课程学习
- en: Curriculum learning, originally proposed in [[129](#bib.bib129)], is a training
    strategy that trains a machine learning model from easier data to harder data,
    imitating human learning [[130](#bib.bib130), [131](#bib.bib131)]. The basic idea
    is to “start small” [[132](#bib.bib132)], i.e., to train the ML model with easier
    data subsets, and then gradually increase the difficulty level of the data until
    the entire training dataset is used. Curriculum learning can be seen as a special
    form of the continuation method which is a general strategy for global optimization
    of non-convex functions [[129](#bib.bib129)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习，最初在[[129](#bib.bib129)]中提出，是一种训练策略，通过从简单数据到复杂数据的方式训练机器学习模型，模仿人类学习[[130](#bib.bib130),
    [131](#bib.bib131)]。基本思想是“从小做起”[[132](#bib.bib132)]，即用简单的数据子集训练机器学习模型，然后逐渐增加数据的难度级别，直到使用整个训练数据集。课程学习可以被看作是继续方法的一种特殊形式，继续方法是优化非凸函数的通用策略[[129](#bib.bib129)]。
- en: As the idea of curriculum learning serves as a general training strategy, it
    has been exploited in a considerable range of applications. In contrast to the
    standard training approach based on random data shuffling, curriculum learning
    can provide performance improvements with faster training convergence speed. In
    curriculum learning, the design of a curriculum strategy, i.e., a sequence of
    training criteria, plays a key role.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于课程学习的思想作为一种通用训练策略，它已被应用于相当广泛的应用领域。与基于随机数据打乱的标准训练方法相比，课程学习可以提供更快的训练收敛速度和性能提升。在课程学习中，课程策略的设计，即训练标准的序列，起着关键作用。
- en: Curriculum learning can be beneficial for designing communication systems including
    channel coding schemes. For example, designing and decoding long codes is generally
    a more difficult task than designing and decoding short codes. As such, instead
    of learning to design or decode long codes directly, curriculum learning strategies
    that start from training with short codes and then gradually increase the code
    length have the potential to achieve not only faster training convergence, but
    also better performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习对于设计包括信道编码方案在内的通信系统是有益的。例如，设计和解码长码通常比设计和解码短码更困难。因此，相比直接学习设计或解码长码，课程学习策略从训练短码开始，然后逐渐增加码长，有可能实现更快的训练收敛速度以及更好的性能。
- en: II-C State-of-the-Art Models
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 最先进的模型
- en: Finding an appropriate architecture is important when applying DL to the problems
    in communication systems. In the following, we briefly review the representative
    DL models that are useful for such problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在将深度学习应用于通信系统中的问题时，找到合适的架构是很重要的。接下来，我们将简要回顾对这些问题有用的代表性深度学习模型。
- en: II-C1 Convolutional Neural Network
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 卷积神经网络
- en: \Acp
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \Acp
- en: 'cnn have made tremendous success in a vast research field including computer
    vision and \acnlp [[133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)].
    A CNN is a type of feed-forward neural network with a convolutional layer that
    learns features by applying filters (or kernels) to data. A simple example of
    a CNN architecture is illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ II-C1 Convolutional
    Neural Network ‣ II-C State-of-the-Art Models ‣ II Brief Introduction of Deep
    Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"). The
    CNN has fewer connections and parameters than the MLP since each neuron in a convolutional
    layer receives input only from a restricted area of the previous layer. This restricted
    area is called the receptive field of the neuron, and in the case of a fully connected
    layer, the receptive field corresponds to the entire previous layer.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'CNN 在计算机视觉和 \acnlp 等广泛研究领域取得了巨大的成功 [[133](#bib.bib133), [134](#bib.bib134),
    [135](#bib.bib135)]。CNN 是一种前馈神经网络，其具有卷积层，通过将滤波器（或内核）应用于数据来学习特征。图 [5](#S2.F5 "Figure
    5 ‣ II-C1 Convolutional Neural Network ‣ II-C State-of-the-Art Models ‣ II Brief
    Introduction of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey") 中展示了 CNN 架构的一个简单示例。与 MLP 相比，CNN 拥有更少的连接和参数，因为卷积层中的每个神经元仅从前一层的一个受限区域接收输入。这个受限区域称为神经元的感受野，对于全连接层，感受野对应于整个前一层。'
- en: '![Refer to caption](img/c800063b51bcaadd7ab64e26da2e313b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/c800063b51bcaadd7ab64e26da2e313b.png)'
- en: 'Figure 5: A simple CNN architecture.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个简单的 CNN 架构。
- en: CNNs have been particularly successful in image recognition tasks, achieving
    state-of-the-art results on several benchmarks such as the \acilsvrc. Representative
    CNN models include AlexNet [[136](#bib.bib136)], VGGnet [[137](#bib.bib137)],
    Inception (GoogLeNet) [[138](#bib.bib138)], ResNet [[139](#bib.bib139)], and DenseNet
    [[140](#bib.bib140)]. Their success is due to their ability to capture spatial
    features and patterns by using a hierarchical architecture of layers that perform
    convolution operations and extract features at different levels of abstraction
    [[141](#bib.bib141)].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 在图像识别任务中取得了特别的成功，在多个基准测试中取得了最先进的结果，如 \acilsvrc。具有代表性的 CNN 模型包括 AlexNet [[136](#bib.bib136)]、VGGnet
    [[137](#bib.bib137)]、Inception (GoogLeNet) [[138](#bib.bib138)]、ResNet [[139](#bib.bib139)]
    和 DenseNet [[140](#bib.bib140)]。它们的成功归因于其通过使用分层的卷积操作架构来捕捉空间特征和模式，从而在不同的抽象层次上提取特征
    [[141](#bib.bib141)]。
- en: As for their applications in channel coding, they are often used for decoding,
    and have been demonstrated to be more efficient than standard MLP. Other popular
    applications of CNN in the physical layer include channel estimation in time-frequency
    domain for OFDM systems [[142](#bib.bib142)] and fully CNN receiver that replaces
    the conventional channel estimator, equalizer, and demapper [[143](#bib.bib143)].
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就在信道编码中的应用而言，CNN 通常用于解码，并已被证明比标准的 MLP 更高效。CNN 在物理层的其他流行应用包括 OFDM 系统中的时间频率域信道估计
    [[142](#bib.bib142)] 和完全 CNN 接收机，它取代了传统的信道估计器、均衡器和解调器 [[143](#bib.bib143)]。
- en: II-C2 Recurrent Neural Network
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 循环神经网络
- en: Unlike unidirectional feedforward neural networks, a \acrnn is a bi-directional
    artificial neural network that is capable of learning long-term dependencies from
    sequential data [[92](#bib.bib92), [144](#bib.bib144)]. Due to their ability to
    use internal state (memory) to process arbitrary input sequences, they are particularly
    suited for processing time-series data such as speech recognition. On the other
    hand, due to the recurrent connections, classical RNNs have the gradient vanishing
    and exploding problems, i.e., the long-term gradients may not converge and approach
    zero or infinity during backpropagation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与单向前馈神经网络不同，\acrnn 是一种双向人工神经网络，能够从顺序数据中学习长期依赖关系 [[92](#bib.bib92), [144](#bib.bib144)]。由于它们能够利用内部状态（记忆）处理任意输入序列，因此特别适合处理时间序列数据，如语音识别。另一方面，由于递归连接，经典的
    RNN 存在梯度消失和爆炸问题，即在反向传播过程中，长期梯度可能无法收敛，而趋近于零或无穷大。
- en: \Ac
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: 'lstm is one of the most popular RNN models that can reduce the effects of vanishing
    and exploding gradients [[145](#bib.bib145)] [[146](#bib.bib146)] by introducing
    a gating mechanism to input or forget certain features. Another popular model
    is a \acgru [[147](#bib.bib147)], in which recurrent units adaptively capture
    dependencies of different time scales to accommodate the higher memory requirements
    of LSTM. A comparison of these architectures is shown in Fig. [6](#S2.F6 "Figure
    6 ‣ II-C2 Recurrent Neural Network ‣ II-C State-of-the-Art Models ‣ II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: lstm是最流行的RNN模型之一，它可以通过引入一个门控机制来输入或遗忘某些特征，从而减少梯度消失和爆炸的影响 [[145](#bib.bib145)]
    [[146](#bib.bib146)]。另一种流行的模型是\acgru [[147](#bib.bib147)]，其中递归单元自适应地捕捉不同时间尺度的依赖关系，以适应LSTM的更高记忆需求。这些架构的比较见图[6](#S2.F6
    "图6 ‣ II-C2递归神经网络 ‣ II-C最新模型 ‣ II深度学习简要介绍 ‣ 深度学习在信道编码中的最新进展：综述")。
- en: '![Refer to caption](img/5373013e5ecd90824e08169359eb5364.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5373013e5ecd90824e08169359eb5364.png)'
- en: (a) RNN.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RNN。
- en: '![Refer to caption](img/4c304e920fa55d78b497fd40e507c97b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4c304e920fa55d78b497fd40e507c97b.png)'
- en: (b) LSTM with a forget gate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 带有忘记门的LSTM。
- en: '![Refer to caption](img/23564bdee343973cc561b1aa1085e54f.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/23564bdee343973cc561b1aa1085e54f.png)'
- en: (c) GRU.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GRU。
- en: 'Figure 6: Comparison of RNN, LSTM, and GRU architectures [[148](#bib.bib148)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：RNN、LSTM和GRU架构的比较 [[148](#bib.bib148)]。
- en: For more details of RNN, in particular LSTM, please refer to [[149](#bib.bib149),
    [148](#bib.bib148), [150](#bib.bib150)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有关RNN的更多细节，特别是LSTM，请参阅 [[149](#bib.bib149), [148](#bib.bib148), [150](#bib.bib150)]。
- en: The LSTM and GRU models have been widely applied to communication systems. In
    particular, due to the analogous structure to convolutional codes, RNNs may be
    well suited for decoding of convolutional codes. Similarly, RNNs have been successfully
    applied to signal detection for channels with memory [[151](#bib.bib151)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM和GRU模型已广泛应用于通信系统。特别是，由于与卷积码的结构类似，RNN可能非常适合于卷积码的解码。同样，RNN也已成功应用于具有记忆的信道的信号检测
    [[151](#bib.bib151)]。
- en: II-C3 Graph Neural Network
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 图神经网络
- en: While ML effectively captures hidden patterns in Euclidean data, a common assumption
    of existing ML algorithms is that instances are independent of each other. This
    assumption no longer holds for graph data, where every node is related to others.
    Extending DNN models to non-Euclidean domains, which is generally referred to
    as geometric DL, has been an emerging area of research. In particular, a \acgnn
    that operates on the graph domain has recently become a popular graph analysis
    method [[152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习有效地捕捉了欧几里得数据中的隐藏模式，但现有机器学习算法的一个常见假设是实例彼此独立。这个假设在图数据中不再成立，因为每个节点都与其他节点相关。将DNN模型扩展到非欧几里得领域，通常称为几何深度学习，一直是一个新兴的研究领域。特别是，最近在图领域操作的\acgnn已成为一种流行的图分析方法
    [[152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)]。
- en: Let $G\in(\mathcal{V},\mathcal{E})$ be a graph, where $\mathcal{V}$ is the node
    set and $\mathcal{E}$ is the edge set. Let $\mathcal{N}_{u}$ be the neighborhood
    of some node $u\in\mathcal{V}$. Additionally, let $\mathbf{x}_{u}$ be the properties
    of node $u\in\mathcal{V}$. GNN implements a permutation-equivalent layer, called
    a GNN layer, which maps a representation of a graph into an updated representation
    of the same graph. Although the design of GNN layers is one of the active research
    areas, one popular approach is \acmpnn layers (other popular approaches include
    graph convolutional networks [[156](#bib.bib156)] and graph attention networks
    [[157](#bib.bib157)]). In an MPNN layer in a generic GNN, nodes update their representations
    by aggregating the messages received from their immediate neighbors [[158](#bib.bib158)]
    and the output of the layer (node representations $\mathbf{h}_{u}$ for each $u\in\mathcal{V}$)
    is expressed as
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 设$G\in(\mathcal{V},\mathcal{E})$为一个图，其中$\mathcal{V}$是节点集，$\mathcal{E}$是边集。设$\mathcal{N}_{u}$为某个节点$u\in\mathcal{V}$的邻域。此外，设$\mathbf{x}_{u}$为节点$u\in\mathcal{V}$的属性。GNN实现了一种等效的层，称为GNN层，它将图的表示映射到同一图的更新表示。尽管GNN层的设计是一个活跃的研究领域，但一种流行的方法是\acmpnn层（其他流行的方法包括图卷积网络
    [[156](#bib.bib156)] 和图注意网络 [[157](#bib.bib157)]）。在通用GNN中的MPNN层，节点通过汇聚从其直接邻居处接收到的消息来更新其表示
    [[158](#bib.bib158)]，层的输出（每个$u\in\mathcal{V}$的节点表示$\mathbf{h}_{u}$）表示为
- en: '|  | $\displaystyle\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}\psi(\mathbf{x}_{u},\mathbf{x}_{v})\right),$
    |  | (4) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}\psi(\mathbf{x}_{u},\mathbf{x}_{v})\right),$
    |  | (4) |'
- en: where $\phi$ and $\psi$ are typically trainable differential functions, whereas
    $\bigoplus$ is a nonparametric permutation invariant aggregation operator that
    can take an arbitrary number of inputs. In particular, $\phi$ and $\psi$ are referred
    to as update and message functions, respectively.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 和 $\psi$ 通常是可训练的微分函数，而 $\bigoplus$ 是一个非参数的置换不变聚合运算符，可以接收任意数量的输入。特别地，$\phi$
    和 $\psi$ 分别称为更新函数和消息函数。
- en: Due to its close relationship to a Tanner graph, a GNN is particularly useful
    for designing and decoding codes over graph. One of the major advantages of GNN
    is their scalability, i.e., a GNN trained for a small code length will generalize
    to any code length, while this usually requires additional training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与 Tanner 图的紧密关系，GNN 在设计和解码图上的编码方面特别有用。GNN 的主要优势之一是其可扩展性，即，经过小代码长度训练的 GNN 将能推广到任何代码长度，而这通常需要额外的训练。
- en: II-C4 Transformer
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C4 Transformer
- en: 'Transformer is a DL architecture based on the multi-head attention mechanism
    [[159](#bib.bib159)]. As depicted in Fig. [7](#S2.F7 "Figure 7 ‣ II-C4 Transformer
    ‣ II-C State-of-the-Art Models ‣ II Brief Introduction of Deep Learning ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey"), it consists of an encoder
    and a decoder, each of which has several Transformer blocks having the same architecture.
    Each Transformer block consists of a multi-head attention layer, a feed-forward
    neural network, a shortcut connection, and a layer normalization. Given a sequence
    of elements, the self-attention mechanism explicitly models the dependencies among
    all entities of a sequence.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是一种基于多头注意力机制的 DL 架构 [[159](#bib.bib159)]。如图[7](#S2.F7 "图 7 ‣ II-C4
    Transformer ‣ II-C 先进模型 ‣ II 深度学习简介 ‣ 最近在信道编码的深度学习进展：调查") 所示，它由一个编码器和一个解码器组成，每个部分都有多个结构相同的
    Transformer 块。每个 Transformer 块包括一个多头注意力层、一个前馈神经网络、一个快捷连接和一个层归一化。给定一个元素序列，自注意力机制显式建模序列中所有实体之间的依赖关系。
- en: '![Refer to caption](img/e45f054bb76ca0fc357d3fe81ad50e48.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e45f054bb76ca0fc357d3fe81ad50e48.png)'
- en: 'Figure 7: The Transformer architecture [[159](#bib.bib159)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Transformer 架构 [[159](#bib.bib159)]。'
- en: It has no recurrent units, and thus requires less training time than previous
    RNN architectures, such as LSTM, and its later variant has been widely adopted
    for training most of representative LLMs, such as Open AI’s \acgpt series [[160](#bib.bib160)],
    Meta’s \acllama [[161](#bib.bib161)], Googles \acpalm [[162](#bib.bib162)], and
    Gemini [[163](#bib.bib163)] are based on the Transformer model. More details and
    the applications of the Transformer can be found in [[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有递归单元，因此比之前的 RNN 架构（如 LSTM）需要更少的训练时间，其后续变体已被广泛用于训练大多数代表性的 LLM，例如 Open AI 的
    \acgpt 系列 [[160](#bib.bib160)]、Meta 的 \acllama [[161](#bib.bib161)]、Google 的 \acpalm
    [[162](#bib.bib162)] 和 Gemini [[163](#bib.bib163)] 都基于 Transformer 模型。有关 Transformer
    的更多细节和应用，请参见 [[164](#bib.bib164)、[165](#bib.bib165)、[166](#bib.bib166)]。
- en: Although Transformer was originally proposed for NLP tasks [[159](#bib.bib159)],
    it has been successfully adopted for a variety of tasks such as computer vision
    [[167](#bib.bib167)], audio applications [[168](#bib.bib168)]. For the physical
    layer technologies, the use of Transformer is rather new [[169](#bib.bib169)].
    Due to its excellent performance, Transformer has the potential to improve the
    performance of existing DL methods for communication engineering problems.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Transformer 最初是为 NLP 任务提出的 [[159](#bib.bib159)]，但它已成功应用于计算机视觉 [[167](#bib.bib167)]、音频应用
    [[168](#bib.bib168)] 等各种任务。对于物理层技术，Transformer 的使用相对较新 [[169](#bib.bib169)]。由于其出色的性能，Transformer
    具有提升现有 DL 方法在通信工程问题中性能的潜力。
- en: II-C5 Diffusion Model
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C5 扩散模型
- en: \Acp
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: \Acp
- en: 'dm are a class of probabilistic generative models that progressively corrupt
    data by injecting noise, and then learn to reverse this process for sample generation.
    The training procedure consists of two phases: the forward diffusion process and
    the backward denoising process [[170](#bib.bib170)]. In the forward process, typically
    Gaussian noise is injected into the training data until it becomes pure Gaussian.
    In the backward process, the noise is sequentially removed to reconstruct the
    original image. The noise subtracted at each step is estimated by a neural network.
    Among different formulations [[171](#bib.bib171), [172](#bib.bib172)], \acddpm
    [[173](#bib.bib173)] is a representative DM inspired by the theory of non-equilibrium
    thermodynamics.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: dm 是一类概率生成模型，通过注入噪声逐渐损坏数据，然后学习逆转这一过程以生成样本。训练过程包括两个阶段：前向扩散过程和后向去噪过程 [[170](#bib.bib170)]。在前向过程中，通常将高斯噪声注入训练数据，直到其变为纯高斯噪声。在后向过程中，噪声被逐步去除以重建原始图像。在每一步中，去除的噪声由神经网络估计。在不同的公式中
    [[171](#bib.bib171), [172](#bib.bib172)]，\acddpm [[173](#bib.bib173)] 是一种受非平衡热力学理论启发的代表性
    DM。
- en: Due to its high generative quality and versatility, DM could be applied to many
    problems in communication systems, such as channel estimation [[174](#bib.bib174)],
    signal detection [[175](#bib.bib175)], AE [[176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178)], and network optimization problems [[179](#bib.bib179), [180](#bib.bib180)].
    However, the application of DMs to the physical layer is a relatively new area
    of research [[181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)].
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其高生成质量和多功能性，DM 可应用于通信系统中的许多问题，如信道估计 [[174](#bib.bib174)]、信号检测 [[175](#bib.bib175)]、AE
    [[176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178)] 和网络优化问题 [[179](#bib.bib179),
    [180](#bib.bib180)]。然而，DM 在物理层的应用仍是一个相对较新的研究领域 [[181](#bib.bib181), [182](#bib.bib182),
    [183](#bib.bib183)]。
- en: III DL for Code Design
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III DL 在码设计中的应用
- en: 'TABLE II: Summary of DL-based polar code design.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：基于 DL 的极化码设计总结。
- en: '| Category | Reference | Main Contributions |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 参考文献 | 主要贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Advanced Decoding Schemes | Ebada et al. [[184](#bib.bib184)] | Design for
    BP decoding with finite iteration count. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 高级解码方案 | Ebada et al. [[184](#bib.bib184)] | 具有有限迭代次数的 BP 解码设计。 |'
- en: '| Huang et al. [[185](#bib.bib185)] | RL-based design of polar codes for SCL
    decoding. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. [[185](#bib.bib185)] | 用于 SCL 解码的极化码 RL 设计。 |'
- en: '| Leonardon et al. [[186](#bib.bib186)] | Design that minimizes BLER under
    SCL decoding via projected gradient descent. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Leonardon et al. [[186](#bib.bib186)] | 通过投影梯度下降最小化 SCL 解码下的 BLER。 |'
- en: '| Liao et al. [[187](#bib.bib187)] | GNN-based polar code construction for
    CA-SCL decoding. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Liao et al. [[187](#bib.bib187)] | 基于 GNN 的极化码构造用于 CA-SCL 解码。 |'
- en: '| Miloslavskaya et al. [[188](#bib.bib188)] | Optimization of polar codes with
    dynamic frozen bits under SCL decoding. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Miloslavskaya et al. [[188](#bib.bib188)] | 在 SCL 解码下具有动态冻结比特的极化码优化。 |'
- en: '| Nested Polar Codes | Huang et al. [[185](#bib.bib185)] | Construction of
    nested polar codes via advantage actor-critic algorithms. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 嵌套极化码 | Huang et al. [[185](#bib.bib185)] | 通过优势演员-评论家算法构建嵌套极化码。 |'
- en: '| Li et al. [[189](#bib.bib189)] | Stochastic policy optimization by a customized
    network. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[189](#bib.bib189)] | 通过定制网络的随机策略优化。 |'
- en: '| Ankireddy et al. [[190](#bib.bib190)] | Nested polar code construction based
    on sequence modeling and Transformer. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Ankireddy et al. [[190](#bib.bib190)] | 基于序列建模和 Transformer 的嵌套极化码构造。 |'
- en: '| Polar Codes with Large Kernel | Hebbar et al. [[191](#bib.bib191)] | Polar
    codes via large nonlinear neural network-based kernels. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 大核极化码 | Hebbar et al. [[191](#bib.bib191)] | 通过大型非线性神经网络基础核的极化码。 |'
- en: '| PAC Codes | Mishra et al. [[192](#bib.bib192)] | RL-based algorithm for rate-profile
    construction of PAC codes. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| PAC 码 | Mishra et al. [[192](#bib.bib192)] | 基于 RL 的 PAC 码率-配置文件构造算法。 |'
- en: 'Modern capacity-approaching codes such as LDPC and polar codes are usually
    designed based on well-established analytical tools, such as \acde [[193](#bib.bib193),
    [194](#bib.bib194)] and \acexit chart [[195](#bib.bib195)]. However, these techniques
    rely on assumptions that do not hold in practice. For example, for the design
    of LDPC codes, DE and EXIT chart analyses assume simple channel models, such as
    \acbiawgn channels, infinite code length, and unlimited \acbp decoding iterations.
    These techniques are also used for polar code design, but they are again limited
    to simple channel models and decoding schemes such as \acsc decoding. For more
    realistic channel models and advanced decoding schemes, DL could replace or support
    existing code design techniques. We have summarized the DL-based approaches to
    polar code design in Table [II](#S3.T2 "TABLE II ‣ III DL for Code Design ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '现代接近容量的编码，如**LDPC**和**极化码**，通常基于成熟的分析工具进行设计，例如 \acde [[193](#bib.bib193), [194](#bib.bib194)]
    和 \acexit 图 [[195](#bib.bib195)]。然而，这些技术依赖于在实际中不成立的假设。例如，**LDPC**码的设计中，**DE**
    和 **EXIT** 图分析假设简单的信道模型，如 \acbiawgn 信道、无限的码长和无限的 \acbp 解码迭代。这些技术也用于极化码设计，但它们同样局限于简单的信道模型和解码方案，如
    \acsc 解码。对于更现实的信道模型和先进的解码方案，**DL** 可能取代或支持现有的编码设计技术。我们在表 [II](#S3.T2 "TABLE II
    ‣ III DL for Code Design ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey") 中总结了基于 **DL** 的极化码设计方法。'
- en: III-A LDPC Code Design
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A **LDPC** 码设计
- en: Irregular LDPC codes are characterized by a variable degree distribution $\lambda(x)$
    and a check degree distribution $\rho(x)$, which are expressed as
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不规则 **LDPC** 码的特征是变量度分布 $\lambda(x)$ 和检查度分布 $\rho(x)$，它们表示为
- en: '|  | $\displaystyle\lambda(x)=\sum^{d_{\text{v}}}_{i=2}\lambda_{i}x^{i-1},\
    \rho(x)=\sum^{d_{\text{c}}}_{i=2}\rho_{i}x^{i-1},$ |  | (5) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\lambda(x)=\sum^{d_{\text{v}}}_{i=2}\lambda_{i}x^{i-1},\
    \rho(x)=\sum^{d_{\text{c}}}_{i=2}\rho_{i}x^{i-1},$ |  | (5) |'
- en: where $\lambda_{i}$ and $\rho_{i}$ represent the fraction of edges emanating
    from \acpvn and \acpcn of degree $i$ and $\lambda(1)=\rho(1)=1$. The maximum variable
    degree and check degree are denoted by $d_{\text{v}}$ and $d_{\text{c}}$, respectively.
    The degree distribution is often optimized to maximize the iterative decoding
    threshold, which is defined as the lowest channel \acsnr at which the message
    distribution in BP evolves in such a way that its associated error probability
    converges to zero as the number of iterations tends to infinity. The method of
    identifying a threshold by tracking the evolution of the message distribution
    is termed DE [[193](#bib.bib193)].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{i}$ 和 $\rho_{i}$ 表示从 \acpvn 和 \acpcn 中度为 $i$ 的边的比例，且 $\lambda(1)=\rho(1)=1$。最大变量度和检查度分别用
    $d_{\text{v}}$ 和 $d_{\text{c}}$ 表示。度分布通常会被优化以最大化迭代解码阈值，该阈值定义为在该阈值下，消息分布在 **BP**
    中的演变使得其相关的错误概率在迭代次数趋于无限时收敛为零。通过跟踪消息分布的演变来识别阈值的方法称为 **DE** [[193](#bib.bib193)]。
- en: The code design problem belongs to the class of nonlinear constraint satisfaction
    problems with continuous space parameters, where we first explore the space of
    degree distributions to find degree distribution pairs, traditionally solved by
    differential evolution [[193](#bib.bib193)], and then evaluate the BP threshold
    of the selected pairs via DE. In [[196](#bib.bib196)], the authors modeled the
    code design process as a supervised learning problem by mapping the recursive
    update equation of DE to an RNN architecture, which they refer to as neural density
    evolution (NDE). They also proposed a multi-objective loss function for NDE that
    ensures its high configurability, i.e., various code rates and maximum degrees.
    Their simulations show that the proposed designs achieve the performance of state-of-the-art
    designs in asymptotic settings for a variety of codeword lengths and channels.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 码设计问题属于具有连续空间参数的非线性约束满足问题的类别，我们首先探索度分布的空间以找到度分布对，传统上通过**差分进化** [[193](#bib.bib193)]
    解决，然后通过 **DE** 评估选定对的 **BP** 阈值。在 [[196](#bib.bib196)] 中，作者通过将 **DE** 的递归更新方程映射到
    **RNN** 架构中，将代码设计过程建模为监督学习问题，他们称之为神经密度演化 (**NDE**) 。他们还提出了一个多目标损失函数用于 **NDE**，确保其高可配置性，即各种码率和最大度数。他们的模拟显示，所提设计在各种码字长度和信道的渐近设置下达到了最先进设计的性能。
- en: III-B Polar Code Design
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B **极化码** 设计
- en: An encoder of polar codes of length $N$ is represented by the generator matrix
    <math   alttext="\mathbf{G}_{N}=\left(\begin{smallmatrix}1&amp;0\\
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为 $N$ 的极化码的编码器由生成矩阵表示
- en: 1&amp;1\end{smallmatrix}\right)^{\otimes n}\in\mathbb{F}_{2}^{N\times N}" display="inline"><semantics
    ><mrow  ><msub ><mi >𝐆</mi><mi  >N</mi></msub><mo >=</mo><msup ><mrow  ><mo >(</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd  ><mn mathsize="70%"  >1</mn></mtd><mtd
    ><mn mathsize="70%" >0</mn></mtd></mtr><mtr ><mtd  ><mn mathsize="70%"  >1</mn></mtd><mtd
    ><mn mathsize="70%" >1</mn></mtd></mtr></mtable><mo >)</mo></mrow><mrow ><mo lspace="0.222em"
    rspace="0.222em" >⊗</mo><mi  >n</mi></mrow></msup><mo >∈</mo><msubsup ><mi  >𝔽</mi><mn
    >2</mn><mrow ><mi >N</mi><mo lspace="0.222em" rspace="0.222em" >×</mo><mi >N</mi></mrow></msubsup></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐆</ci><ci >𝑁</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><matrix
    ><matrixrow  ><cn type="integer"  >1</cn><cn type="integer"  >0</cn></matrixrow><matrixrow
    ><cn type="integer" >1</cn><cn type="integer" >1</cn></matrixrow></matrix><apply
    ><csymbol cd="latexml" >tensor-product</csymbol><csymbol cd="latexml"  >absent</csymbol><ci
    >𝑛</ci></apply></apply></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝔽</ci><cn type="integer" >2</cn></apply><apply
    ><ci  >𝑁</ci><ci >𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathbf{G}_{N}=\left(\begin{smallmatrix}1&0\\ 1&1\end{smallmatrix}\right)^{\otimes
    n}\in\mathbb{F}_{2}^{N\times N}</annotation></semantics></math>, where $n=\log_{2}{N}$
    is a positive integer and $\mathbf{A}^{\otimes n}=\mathbf{A}\otimes\mathbf{A}^{\otimes(n-1)}$
    is the $n$th Kronecker power of the matrix $\mathbf{A}$ [[5](#bib.bib5)]. Let
    $\mathcal{I}\subset\{0,1,\ldots,N-1\}$ denote a set of information bit indices
    with its cardinality $K=|\mathcal{I}|$, and $\mathcal{F}=\{0,1,\ldots,N-1\}\backslash\mathcal{I}$
    denote the complement of $\mathcal{I}$ with its cardinality $|\mathcal{F}|=N-K$.
    Letting $\mathbf{u}=(u_{0},u_{1},\ldots,u_{N-1})\in\mathbb{F}_{2}^{N}$ be an input
    vector to the polar encoder, the bits $u_{i}$ with $i\in\mathcal{I}$ are chosen
    to carry information, whereas those with $i\in\mathcal{F}$ are frozen (i.e., fixed
    to a predetermined bit value known by encoder and decoder). The code rate of the
    polar code is thus $R=K/N$. Polar code design or construction is equivalent to
    identifying an appropriate index set $\mathcal{I}$ for a given channel model and
    decoding scheme.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{G}_{N}=\left(\begin{smallmatrix}1&0\\ 1&1\end{smallmatrix}\right)^{\otimes
    n}\in\mathbb{F}_{2}^{N\times N}$，其中 $n=\log_{2}{N}$ 是一个正整数，$\mathbf{A}^{\otimes
    n}=\mathbf{A}\otimes\mathbf{A}^{\otimes(n-1)}$ 是矩阵 $\mathbf{A}$ 的第 $n$ 次 Kronecker
    幂 [[5](#bib.bib5)]。设 $\mathcal{I}\subset\{0,1,\ldots,N-1\}$ 表示信息比特索引的集合，其基数为 $K=|\mathcal{I}|$，$\mathcal{F}=\{0,1,\ldots,N-1\}\backslash\mathcal{I}$
    表示 $\mathcal{I}$ 的补集，其基数为 $|\mathcal{F}|=N-K$。设 $\mathbf{u}=(u_{0},u_{1},\ldots,u_{N-1})\in\mathbb{F}_{2}^{N}$
    为极化编码器的输入向量，其中 $i\in\mathcal{I}$ 的比特 $u_{i}$ 被选择用于携带信息，而 $i\in\mathcal{F}$ 的比特则被冻结（即固定为编码器和解码器已知的预定比特值）。因此，极化码的码率为
    $R=K/N$。极化码的设计或构造等同于为给定的信道模型和解码方案确定一个合适的索引集 $\mathcal{I}$。
- en: In his original paper, Arıkan suggested using Monte-Carlo simulations to estimate
    the reliabilities of bit channels [[5](#bib.bib5)]. Subsequently, DE [[194](#bib.bib194)]
    and its improved version [[197](#bib.bib197)] were proposed to accurately estimate
    the reliabilities at the cost of high complexity. This complexity was alleviated
    by \acga of DE [[198](#bib.bib198)], improved GA [[199](#bib.bib199)], and \acrca
    [[200](#bib.bib200)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在他原始的论文中，Arıkan 建议使用蒙特卡洛模拟来估计比特信道的可靠性 [[5](#bib.bib5)]。随后，提出了 DE [[194](#bib.bib194)]
    及其改进版本 [[197](#bib.bib197)]，以在高复杂度的代价下准确估计可靠性。这种复杂度通过 DE 的 \acga [[198](#bib.bib198)]、改进的
    GA [[199](#bib.bib199)] 和 \acrca [[200](#bib.bib200)] 得到了缓解。
- en: III-B1 Design for Advanced Decoding Schemes
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 高级解码方案的设计
- en: Although polar codes were originally proposed with SC decoding [[5](#bib.bib5)],
    their finite-length performance is unsatisfactory. In order for polar codes to
    achieve performance comparable to other capacity-approaching codes, advanced decoding
    schemes, such as \acscl or \accascl decoding [[201](#bib.bib201)], are required.
    However, the above-mentioned polar code construction schemes assume SC decoding
    and there is no explicit approach to designing polar codes for SCL and CA-SCL
    decoding.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管极化码最初是与 SC 解码一起提出的 [[5](#bib.bib5)]，但其有限长度的性能并不令人满意。为了使极化码达到与其他接近容量的码相当的性能，需要先进的解码方案，如
    \acscl 或 \accascl 解码 [[201](#bib.bib201)]。然而，上述极化码构造方案假设了 SC 解码，并且没有明确的方案来设计用于
    SCL 和 CA-SCL 解码的极化码。
- en: In [[184](#bib.bib184)], the authors proposed to design polar codes for BP decoding
    with limited number of iterations over both \acawgn and Rayleigh fading channels.
    By representing the frozen and non-frozen bit vectors by soft-valued vectors,
    which can be considered as training weights of a neural network, training is performed
    to minimize the cross entropy loss between transmitted and estimated codewords
    while satisfying the target code rate requirement and training convergence. The
    simulations showed that the learned polar code outperforms the performance of
    the 5G polar code under Arıkan’s conventional BP decoder.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[184](#bib.bib184)] 中，作者提出设计用于 BP 解码的极化码，在 \acawgn 和 Rayleigh 衰落信道上进行有限次数的迭代。通过将冻结和非冻结比特向量表示为软值向量，这些向量可以视为神经网络的训练权重，进行训练以最小化传输码字和估计码字之间的交叉熵损失，同时满足目标码率要求和训练收敛。模拟结果表明，学习到的极化码在
    Arıkan 传统 BP 解码器下的性能优于 5G 极化码。
- en: On the other hand, the authors in [[202](#bib.bib202), [185](#bib.bib185)] proposed
    a genetic algorithm and RL-based design of polar codes for SCL decoding. As a
    reward function in RL, the authors computed an SNR required for a code to achieve
    a target \acbler via Monte-Carlo simulations. In the same line of research, the
    authors in [[203](#bib.bib203)] proposed a tabular RL-based construction of polar
    codes for SCL decoding. Instead of evaluating the BLER based on the Monte-Carlo
    method as in [[202](#bib.bib202), [185](#bib.bib185)], they designed the reward
    function that sends a negative immediate reward (penalty) to the agent when the
    selected action causes a frame error in genie-aided SCL decoding⁴⁴4In genie-aided
    SCL decoding, the decoder can always output the correct codeword if it is in the
    list.. The proposed method achieved comparable or slightly better performance
    with a lower computational complexity in training than the method in [[185](#bib.bib185)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[[202](#bib.bib202), [185](#bib.bib185)] 中的作者提出了基于遗传算法和 RL 的极化码设计，用于 SCL
    解码。作为 RL 的奖励函数，作者计算了实现目标 \acbler 所需的 SNR，通过蒙特卡洛模拟得出。在同一研究方向中，[[203](#bib.bib203)]
    的作者提出了基于表格的 RL 极化码构造方法，用于 SCL 解码。他们设计了奖励函数，当选定的行动在天使辅助的 SCL 解码中导致帧错误时，将负的即时奖励（惩罚）发送给代理⁴⁴4
    在天使辅助的 SCL 解码中，如果码字在列表中，解码器总是可以输出正确的码字。 与 [[185](#bib.bib185)] 中的方法相比，所提出的方法在训练中的计算复杂度更低，性能相当或稍好。
- en: As another approach, the authors in [[186](#bib.bib186)] proposed a two-step
    optimization method. More specifically, they first trained MLPs to predict BLER
    under SCL decoding from input frozen bit sequences, and then the code that minimizes
    the BLER was constructed via \acpgd [[204](#bib.bib204)] which has been widely
    studied in the realm of adversarial attacks on neural networks. Simulations demonstrated
    that the proposed construction successfully improves the performance of the codes
    on the dataset used for predicting BLER.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一种方法，[[186](#bib.bib186)]中的作者提出了一种两步优化方法。更具体地说，他们首先训练了MLP以预测SCL解码下的BLER，然后通过
    \acpgd [[204](#bib.bib204)] 构造最小化BLER的编码，该方法在对抗性攻击神经网络的领域中得到了广泛研究。仿真结果表明，所提出的构造方法在用于预测BLER的数据集上成功提高了编码性能。
- en: More recently, the authors in [[187](#bib.bib187)] proposed a GNN-based polar
    code construction algorithm for CA-SCL decoding. More specifically, a polar code
    is first mapped onto a unique heterogeneous graph called the \acpccmp graph, and
    then a heterogeneous GNN-based iterative message-passing algorithm is proposed
    which aims to find a PCCMP graph corresponding to the polar code with minimum
    BLER under CA-SCL decoding. The proposed GNN-based iterative message-passing method
    has a salient property that a single trained model can be directly applied to
    constructions for different design SNRs and different block lengths without any
    additional training. Numerical experiments showed that the proposed constructions
    outperform classical constructions in [[197](#bib.bib197)] under CA-SCL decoding.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[[187](#bib.bib187)]中的作者提出了一种基于GNN的极化编码构造算法，用于CA-SCL解码。更具体地说，极化编码首先被映射到一个独特的异构图上，称为
    \acpccmp 图，然后提出了一种异构GNN基于迭代消息传递的算法，旨在找到一个与极化编码对应的最小BLER的PCCMP图。所提出的GNN基于迭代消息传递的方法具有一个显著特性，即一个训练好的模型可以直接应用于不同设计SNR和不同块长度的构造，而无需额外的训练。数值实验显示，所提出的构造方法在CA-SCL解码下优于[[197](#bib.bib197)]中的经典构造方法。
- en: In [[188](#bib.bib188)], the authors proposed neural network-based adaptive
    polar coding scheme that adapts to various channel conditions and quality of service
    requirements. Specifically, the authors developed an MLP-based performance prediction
    framework for polar codes with dynamic frozen bits under SCL decoding. Then the
    authors presented a new class of polar codes with dynamic frozen bits parameterized
    by a single integer parameter, and used the performance prediction framework to
    optimize the parameter for a given target BLER, list size, code length and rate.
    The simulation results show that the proposed codes outperform 5G polar codes
    under CA-SCL decoding with various list sizes. Although a neural network is not
    used due to the training difficulty, the same authors also proposed an RL-based
    method to design dynamic frozen bits of polar codes that minimize the BLER under
    SCL decoding in [[205](#bib.bib205)].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[188](#bib.bib188)]中，作者提出了一种基于神经网络的自适应极化编码方案，该方案可以适应各种信道条件和服务质量要求。具体来说，作者开发了一个基于MLP的极化编码性能预测框架，该框架处理具有动态冻结位的极化编码，并且在SCL解码下使用。接着，作者提出了一类新的极化编码，其动态冻结位由一个整数参数来参数化，并利用性能预测框架来优化该参数，以满足给定的目标BLER、列表大小、编码长度和速率。仿真结果表明，在CA-SCL解码下，所提出的编码在不同列表大小的情况下优于5G极化编码。尽管由于训练难度没有使用神经网络，但同一作者在[[205](#bib.bib205)]中也提出了一种基于RL的方法来设计动态冻结位，以在SCL解码下最小化BLER。
- en: III-B2 Nested Polar Code Construction
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 嵌套极化编码构造
- en: In general, on-the-fly design of polar codes that adaptively select frozen bits
    for a given channel may be too complex to implement in practical systems. On the
    other hand, the authors in [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] studied the design of polar codes based on a universal reliability
    order of bit channels that is independent of channel conditions. As such, it is
    preferable in practice to impose a nested property on polar codes with different
    rates, so that all polar codes can be derived from the same mother code based
    on the universal reliability sequence [[210](#bib.bib210)].
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，即时设计极化编码以自适应选择冻结位以适应特定信道可能在实际系统中过于复杂。另一方面，[[206](#bib.bib206), [207](#bib.bib207),
    [208](#bib.bib208), [209](#bib.bib209)]中的作者研究了基于与信道条件无关的通用可靠性顺序的极化编码设计。因此，在实践中，最好对具有不同速率的极化编码施加嵌套属性，使得所有极化编码都可以基于通用可靠性序列[[210](#bib.bib210)]从相同的母编码推导出来。
- en: 'The authors in [[202](#bib.bib202), [185](#bib.bib185)] proposed constructing
    nested polar codes via \aca2c algorithms [[211](#bib.bib211)]. The paper regarded
    code construction as a multi-step MDP, where for a given $(N,K)$ polar code (current
    state), a new action is taken to construct $(N,K+1)$ polar code (an updated state).
    This MDP is illustrated in Fig. [8](#S3.F8 "Figure 8 ‣ III-B2 Nested Polar Code
    Construction ‣ III-B Polar Code Design ‣ III DL for Code Design ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). The reliability ordered sequence
    is then constructed by sequentially appending the actions to the end of initial
    polar code construction. The proposed code design was shown to outperform the
    conventional DE construction under SCL decoding.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[[202](#bib.bib202)、[185](#bib.bib185)]中的作者提出通过 \aca2c 算法[[211](#bib.bib211)]构造嵌套极化码。论文将码的构造视为一个多步骤的
    MDP，其中对于给定的 $(N,K)$ 极化码（当前状态），采取新的行动构造 $(N,K+1)$ 极化码（更新状态）。该 MDP 如图[8](#S3.F8
    "图 8 ‣ III-B2 嵌套极化码构造 ‣ III-B 极化码设计 ‣ III DL 代码设计 ‣ 最近的深度学习进展在通道编码中的应用：综述")所示。然后，通过将操作顺序地追加到初始极化码构造的末尾来构造可靠性排序序列。所提出的码设计在
    SCL 解码下显示出优于传统 DE 构造的性能。'
- en: '![Refer to caption](img/1e7c6bed5cd64bf2354ee898241f2eda.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1e7c6bed5cd64bf2354ee898241f2eda.png)'
- en: 'Figure 8: RL-based construction of a rate-$K/N$ nested polar code at time $t\in\{1,\ldots,K\}$
    in [[202](#bib.bib202), [185](#bib.bib185)]. The goal is to minimize the expected
    cumulative BLER, i.e., $\sum^{K}_{t=1}r_{k}$. The ones in the frozen bit vector
    $s_{t}$ indicate locations of frozen bits.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于 RL 的速率为 $K/N$ 的嵌套极化码在时间 $t\in\{1,\ldots,K\}$ 的构造，如[[202](#bib.bib202)、[185](#bib.bib185)]所示。目标是最小化预期累计
    BLER，即 $\sum^{K}_{t=1}r_{k}$。冻结比特向量 $s_{t}$ 中的 1 表示冻结比特的位置。
- en: Meanwhile, the authors in [[189](#bib.bib189)] first transformed the problem
    of nested polar code construction into a stochastic policy optimization problem
    for sequential decision, and then represented the policy by a customized neural
    network. Furthermore, the authors proposed a gradient-based algorithm to minimize
    the average loss of the policy. Simulation results demonstrate that the proposed
    construction achieves better performance than the state-of-the-art nested polar
    codes for SCL decoding in [[202](#bib.bib202), [185](#bib.bib185)]. A similar
    construction of nested polar codes has also been proposed in [[190](#bib.bib190)],
    where the authors parameterized the policy network by a Transformer encoder-only
    model, which can directly predict the next information bit in the nested sequence.
    It was shown that the proposed Transformer-based construction can achieve better
    error rate performance than the approach proposed in [[189](#bib.bib189)].
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，[[189](#bib.bib189)]中的作者首先将嵌套极化码构造问题转化为一个用于顺序决策的随机策略优化问题，然后通过定制的神经网络表示该策略。此外，作者提出了一种基于梯度的算法来最小化策略的平均损失。模拟结果表明，所提出的构造方法在[[202](#bib.bib202)、[185](#bib.bib185)]中的
    SCL 解码性能优于最新的嵌套极化码。在[[190](#bib.bib190)]中还提出了一种类似的嵌套极化码构造方法，其中作者通过 Transformer
    仅编码器模型对策略网络进行参数化，这可以直接预测嵌套序列中的下一个信息比特。结果表明，所提出的基于 Transformer 的构造方法在误码率性能上优于[[189](#bib.bib189)]中提出的方法。
- en: III-B3 Design of Polar Codes with Large Kernel
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 大核极化码设计
- en: Another way to improve finite-length performance of binary polar codes is to
    increase the size of the polarization kernel. In fact, channel polarization holds
    for all kernels provided that they are not unitary and not upper triangular under
    any column permutation [[212](#bib.bib212)]. However, binary polar codes with
    large kernels exhibit poor performance for practical short-to-medium block lengths
    and face an exponential increase in computational complexity with kernel size.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 改善二进制极化码有限长度性能的另一种方法是增加极化核的大小。实际上，通道极化适用于所有核，前提是它们不是单位矩阵，并且在任何列排列下不是上三角矩阵[[212](#bib.bib212)]。然而，具有大核的二进制极化码在实际短到中等块长度下表现不佳，并且随着核大小的增加，计算复杂度呈指数增长。
- en: In [[191](#bib.bib191)], the authors proposed polar codes via large nonlinear
    neural network-based kernels, termed as DEEPPOLAR, and its decoder based on a
    generalization of SC decoding. They also developed a principled curriculum-based
    training methodology that allows DEEPPOLAR to generalize well to high SNR scenarios,
    characterized by rare error events. It was shown that the DEEPPOLAR outperforms
    the classical polar codes with SC decoding.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[191](#bib.bib191)]中，作者提出了通过大型非线性神经网络内核的极化码，称为DEEPPOLAR，以及基于SC解码的一般化解码器。他们还开发了一种原则性课程基础训练方法，使DEEPPOLAR能很好地推广到高SNR场景，这些场景特征是错误事件稀少。结果表明，DEEPPOLAR优于经典的SC解码极化码。
- en: III-B4 Design of Polarization Adjusted Convolutional Codes
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 极化调整卷积码的设计
- en: In the Shannon Lecture at the 2019 \acisit, Arıkan introduced a new class of
    codes, called \acpac codes, that concatenate convolutional precoding with the
    polar transform [[213](#bib.bib213)]. PAC codes significantly improve the performance
    of polar codes at short-to-moderate block lengths, where channel polarization
    occurs relatively slowly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年\acisit的香农讲座中，Arıkan介绍了一种新的代码类别，称为\acpac码，该码将卷积预编码与极化变换结合[[213](#bib.bib213)]。PAC码在短到中等块长度下显著提高了极化码的性能，此时信道极化相对缓慢发生。
- en: The first step in encoding of PAC codes is rate profiling. For PAC codes of
    rate-$K/N$, this step inserts $K$ information bits into a vector of length $N$,
    which is subsequently input to the convolutional precoder. The selection of $K$
    bit indices out of $N$ possible indices is called rate profile construction and
    its design significantly affects the performance of PAC codes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: PAC码编码的第一步是速率配置。对于速率为$K/N$的PAC码，此步骤将$K$个信息位插入长度为$N$的向量中，随后输入卷积预编码器。从$N$个可能的索引中选择$K$个位索引称为速率配置，其设计对PAC码的性能有显著影响。
- en: In [[192](#bib.bib192)], the authors proposed an RL-based algorithm for rate-profile
    construction of PAC codes. Specifically, by mapping the rate profile construction
    problem to MDP, the authors proposed Q-learning with a set of customized reward
    and update strategies. Simulation results showed that the proposed rate-profile
    construction provides better error rate performance compared to the Monte-Carlo-based
    rate profiling design in [[214](#bib.bib214)].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[192](#bib.bib192)]中，作者提出了一种基于RL的PAC码速率配置算法。具体而言，通过将速率配置问题映射到MDP，作者提出了带有定制奖励和更新策略的Q学习方法。仿真结果表明，提出的速率配置相比于[[214](#bib.bib214)]中的蒙特卡洛速率配置设计提供了更好的误码率性能。
- en: IV DL for Channel Decoding
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV DL用于信道解码
- en: DL methods for channel decoding have been an active area of research and have
    been extensively studied as a means to replace or assist conventional decoding
    algorithms. In what follows, we first review model-free decoders that do not assume
    specific code structure and thus are applicable to any codes. We then review model-based
    DL BP decoding methods take take into account specific factor graphs. Subsequently,
    we focus on DL methods for decoding polar codes, convolutional/turbo codes, and
    cyclic codes.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: DL方法用于信道解码一直是一个活跃的研究领域，并且已经被广泛研究作为替代或辅助传统解码算法的手段。接下来，我们首先回顾不假设特定码结构的无模型解码器，这些解码器适用于任何码。然后我们回顾考虑特定因子图的基于模型的DL
    BP解码方法。随后，我们专注于极化码、卷积/涡轮码和循环码的DL解码方法。
- en: IV-A Model-Free Decoders
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 无模型解码器
- en: 'TABLE III: Summary of model-free decoders.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：无模型解码器总结。
- en: '| Category | Reference | Main Contributions |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 参考文献 | 主要贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MLP Decoders | Gruber et al. [[215](#bib.bib215)] | Initial work on MLP-based
    channel decoding. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| MLP解码器 | Gruber等[[215](#bib.bib215)] | 关于基于MLP的信道解码的初步工作。 |'
- en: '| Seo et al. [[216](#bib.bib216)] | Investigation on the impact of various
    configurations of an MLP decoder. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Seo等[[216](#bib.bib216)] | 关于MLP解码器各种配置影响的调查。 |'
- en: '| Leung et al. [[217](#bib.bib217)] | Empirical study on the impact of hyperparameters
    of the MLP decoder. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Leung等[[217](#bib.bib217)] | 关于MLP解码器超参数影响的实证研究。 |'
- en: '| Leung et al. [[218](#bib.bib218)] | Investigated small MLP for applications
    with low energy and latency. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Leung等[[218](#bib.bib218)] | 研究了小型MLP用于低能量和延迟的应用。 |'
- en: '| Advanced DL Models | Lyu et al. [[219](#bib.bib219)] | Investigation on different
    types of DL decoders, namely, MLP, CNN, RNN. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 高级DL模型 | Lyu等[[219](#bib.bib219)] | 关于不同类型DL解码器的调查，即MLP、CNN、RNN。 |'
- en: '| Sattiraju et al. [[220](#bib.bib220)] | Bi-GRU-based decoder. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Sattiraju等[[220](#bib.bib220)] | 基于Bi-GRU的解码器。 |'
- en: '| Zhu et al. [[221](#bib.bib221)] | Residual MLP, CNN, RNN-based decoders.
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Zhu 等人 [[221](#bib.bib221)] | 残差MLP、CNN、RNN基础的解码器。 |'
- en: '| Choukroun et al. [[222](#bib.bib222)] | Novel Transformer architecture for
    decoding block codes. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Choukroun 等人 [[222](#bib.bib222)] | 新颖的Transformer架构用于块码解码。 |'
- en: '| Choukroun et al. [[223](#bib.bib223)] | DDPM for soft-decision decoding of
    linear codes. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Choukroun 等人 [[223](#bib.bib223)] | 用于线性码的软判决解码的DDPM。 |'
- en: '| Syndrome-Based Loss Function | Bennatan et al. [[224](#bib.bib224)] | Syndrome-based
    approach to soft-decision decoding of linear codes. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 基于综合症的损失函数 | Bennatan 等人 [[224](#bib.bib224)] | 基于综合症的线性码软判决解码方法。'
- en: '| Kamassury et al. [[225](#bib.bib225)] | Iterative algorithm, referred to
    as iterative error decimation. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Kamassury 等人 [[225](#bib.bib225)] | 被称为迭代误差消除的迭代算法。 |'
- en: '| Artemasov et al. [[226](#bib.bib226)] | SISO decoder based on Stacked-GRU
    for turbo product codes. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Artemasov 等人 [[226](#bib.bib226)] | 基于堆叠-GRU的SISO解码器用于涡轮乘积码。 |'
- en: '| Adaptability | Wang et al. [[227](#bib.bib227)] | Unified DL-based decoder
    for polar and LDPC codes. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 适应性 | Wang 等人 [[227](#bib.bib227)] | 极化码和LDPC码的统一深度学习解码器。 |'
- en: '| Jiang et al. [[228](#bib.bib228)] | A meta learning-based model independent
    neural decoder. |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等人 [[228](#bib.bib228)] | 基于元学习的模型无关神经解码器。 |'
- en: '| Lee et al. [[229](#bib.bib229)] | Transfer learning for decoding a set of
    rate-compatible polar codes. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Lee 等人 [[229](#bib.bib229)] | 用于解码一组速率兼容极化码的迁移学习。 |'
- en: '| Artemasov et al. [[230](#bib.bib230)] | A unified DL decoder for BCH and
    polar codes concatenated with CRC. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Artemasov 等人 [[230](#bib.bib230)] | 用于BCH和极化码的统一深度学习解码器，附加CRC。 |'
- en: '| RL-Based Approach | Carpi et al. [[231](#bib.bib231)] | DQN for iterative
    bit-flipping decoding of binary linear codes. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 基于强化学习的方法 | Carpi 等人 [[231](#bib.bib231)] | 用于二进制线性码的迭代位翻转解码的DQN。 |'
- en: '| Gao et al. [[232](#bib.bib232)] | Q-learning-based bit-flipping decoding
    for polar codes. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Gao 等人 [[232](#bib.bib232)] | 基于Q学习的极化码位翻转解码。 |'
- en: '| Complexity Reduction | Kavvousanos et al. [[233](#bib.bib233), [234](#bib.bib234),
    [235](#bib.bib235), [236](#bib.bib236)] | Magnitude-based pruning and quantization
    for parameter reduction. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 复杂度减少 | Kavvousanos 等人 [[233](#bib.bib233), [234](#bib.bib234), [235](#bib.bib235),
    [236](#bib.bib236)] | 基于幅度的修剪和量化用于参数减少。 |'
- en: '| Cavarec et al. [[237](#bib.bib237)] | A DL-aided adaptation of the order
    parameter in OSD. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Cavarec 等人 [[237](#bib.bib237)] | 在OSD中基于深度学习的有序参数适应。 |'
- en: '| Other Approaches | Raviv et al. [[238](#bib.bib238)] | Data-driven framework
    for permutation selection in permutation decoding. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 其他方法 | Raviv 等人 [[238](#bib.bib238)] | 数据驱动的框架用于排列解码中的排列选择。 |'
- en: '| Kurmukova et al. [[239](#bib.bib239)] | Friendly jamming for improving decoding
    performance. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Kurmukova 等人 [[239](#bib.bib239)] | 友好的干扰用于提高解码性能。 |'
- en: '| Tsvieli et al. [[240](#bib.bib240)] | Investigation on the problem of maximizing
    the margin of the decoder. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Tsvieli 等人 [[240](#bib.bib240)] | 研究解码器边界最大化的问题。 |'
- en: '| Zhong et al. [[241](#bib.bib241), [242](#bib.bib242)] | DL-based decoders
    for spin-torque transfer magnetic random access memory. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Zhong 等人 [[241](#bib.bib241), [242](#bib.bib242)] | 基于深度学习的自旋转矩磁性随机存取存储器解码器。
    |'
- en: 'A model-free decoder employs neural networks that do not take into account
    any specific structure of the codes and thus can potentially benefit from the
    powerful architectures of advanced DL models. However, such decoders typically
    suffer from the curse of dimensionality, since the size of the training dataset
    grows exponentially with the number of information bits. On the other hand, a
    potential advantage over conventional non-DL-based decoding is a highly parallelizable
    structure, allowing *one-shot* decoding instead of iterative decoding. The model-free
    approaches that we will discuss below is summarized in Table [III](#S4.T3 "TABLE
    III ‣ IV-A Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey").'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '无模型解码器利用神经网络，不考虑码的任何特定结构，因此可能从先进深度学习模型的强大架构中受益。然而，这些解码器通常遭遇维度诅咒，因为训练数据集的大小随着信息位数量的增加呈指数增长。另一方面，相较于传统的非深度学习解码方法，其潜在优势在于高度可并行的结构，允许*一次性*解码而非迭代解码。我们将讨论的无模型方法总结在表[III](#S4.T3
    "TABLE III ‣ IV-A Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey")中。 |'
- en: IV-A1 MLP Decoders
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 MLP 解码器
- en: The paper [[215](#bib.bib215)] is one of the initial works on DL-based channel
    decoding where the authors investigated the direct application of MLP to decoding
    of random and polar codes. Their empirical results demonstrated that for structured
    codes, the DL decoder can generalize even for codewords not seen in the training
    phase, and the DL decoder can achieve \acmap decoding performance for a very small
    code lengths such as $16$ bits, but learning for longer codes is prohibitively
    complex due to the exponentially increasing training complexity.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[215](#bib.bib215)]是关于基于DL的信道解码的初期工作之一，作者研究了MLP在随机和极化码解码中的直接应用。他们的实证结果表明，对于结构化码，DL解码器可以推广到训练阶段未见过的码字，且DL解码器能够在非常小的码长如$16$位上实现\acmap解码性能，但对于较长码的学习由于训练复杂度呈指数级增长而极其复杂。
- en: Meanwhile, the authors in [[216](#bib.bib216)] investigated the impact of various
    configurations of an MLP decoder on BER performance, such as the number of hidden
    layers, the number of nodes for each layer, and activation functions. Similarly,
    the paper [[217](#bib.bib217)] empirically studied the impact of the number of
    hidden layers and nodes as well as a training SNR of the MLP decoder on its performance
    and investigated the minimum numbers required to achieve similar performance to
    the optimal maximum-likelihood decoder for short linear and nonlinear block codes.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，[[216](#bib.bib216)]中的作者研究了MLP解码器的各种配置对BER性能的影响，例如隐藏层的数量、每层的节点数以及激活函数。类似地，论文[[217](#bib.bib217)]实证研究了隐藏层和节点数量以及MLP解码器的训练SNR对其性能的影响，并调查了实现与最佳最大似然解码器相似性能所需的最小数量，以适应短线性和非线性块码。
- en: In [[243](#bib.bib243), [218](#bib.bib218)], the authors investigated the application
    of small MLP decoders to low-energy and low-latency applications. In particular,
    the paper made comparisons between single-label and multi-label neural decoders,
    and demonstrated that the multi-label decoder generally requires more hidden layers
    and nodes to achieve similar performance to the single-label decoder.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[243](#bib.bib243), [218](#bib.bib218)]中，作者研究了小型MLP解码器在低能量和低延迟应用中的应用。特别是，论文对单标签和多标签神经解码器进行了比较，并证明了多标签解码器通常需要更多的隐藏层和节点，以实现与单标签解码器相似的性能。
- en: IV-A2 Advanced DL Models
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A2 先进的DL模型
- en: The performance of the above-mentioned MLP decoder can be enhanced by advanced
    DL models. For instance, in [[219](#bib.bib219)], the authors investigated different
    types of decoders based on MLP, CNN, and RNN (in particular, LSTM). Their empirical
    results demonstrated that the RNN and CNN decoders can actually achieve better
    BER performance than the MLP decoder at the cost of higher computation time. In
    [[220](#bib.bib220)], RNN (\acbigru) was used for encoding/decoding of turbo codes
    as in [[244](#bib.bib244)].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的MLP解码器的性能可以通过先进的DL模型得到提升。例如，在[[219](#bib.bib219)]中，作者研究了基于MLP、CNN和RNN（特别是LSTM）的不同类型解码器。他们的实证结果表明，RNN和CNN解码器实际上可以比MLP解码器实现更好的BER性能，但计算时间更长。在[[220](#bib.bib220)]中，RNN（\acbigru）被用于编码/解码涡轮码，类似于[[244](#bib.bib244)]。
- en: To further improve the performances, the authors of [[245](#bib.bib245), [221](#bib.bib221)]
    introduced the concept of residual learning [[139](#bib.bib139)] to the MLP, CNN,
    and RNN-based decoders. Specifically, the paper introduced a denoiser network
    prior to the decoder that simply aims to remove noise induced at the channel,
    and proposed a training loss that considers both denoising and decoding performance.
    It was demonstrated that the proposed denoiser network can improve the BER performance
    at the cost of marginal increase in run time.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高性能，[[245](#bib.bib245), [221](#bib.bib221)]的作者将残差学习[[139](#bib.bib139)]的概念引入了MLP、CNN和RNN基础的解码器中。具体而言，论文引入了一个解噪网络作为解码器之前的预处理，其目的是去除在信道中产生的噪声，并提出了一种考虑去噪和解码性能的训练损失。结果表明，所提出的解噪网络可以提高BER性能，但运行时间增加仅为微小程度。
- en: More recently, inspired by the success of the Transformer model in various applications
    [[159](#bib.bib159)], a novel Transformer architecture for decoding algebraic
    block codes, termed \acecct was proposed in [[222](#bib.bib222)]. The ECCT takes
    as input a concatenation of reliabilities of codeword bits (absolute values of
    received symbols in the case of BI-AWGN) and syndrome bits as its input where
    each element of which is embedded in a high-dimensional space with its own position-dependent
    embedding vector. Then, a self-attention mechanism is employed, where the interaction
    between bits specified by the code structure, i.e., \acpcm, is incorporated as
    domain knowledge. Extensive simulation results demonstrated that the proposed
    Transformer-based decoder outperforms state-of-the-art neural decoders.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，受到 Transformer 模型在各种应用中的成功[[159](#bib.bib159)]的启发，[[222](#bib.bib222)]中提出了一种用于解码代数块码的新型
    Transformer 架构，称为 \acecct。ECCT 的输入是编码字位的可靠性（在 BI-AWGN 的情况下为接收符号的绝对值）和综合症位的拼接，每个元素都嵌入在具有自身位置依赖嵌入向量的高维空间中。然后，采用自注意力机制，其中通过编码结构指定的位之间的交互，即
    \acpcm，被作为领域知识纳入。广泛的仿真结果表明，提出的基于 Transformer 的解码器优于最先进的神经解码器。
- en: Although the ECCT in [[222](#bib.bib222)] employs a mask matrix that is derived
    from the PCM, there exist numerous PCMs for the same code which will lead to different
    decoding performances. Motivated by this fact, the authors in [[246](#bib.bib246)]
    addressed the problem of identifying the optimal PCM. In particular, the authors
    proposed a systematic mask matrix constructed from the systematic PCM which results
    in sparse self-attention map, and proposed a novel Transformer architecture called
    a double-masked ECCT that consists of two parallel masked self-attention blocks
    employing distinct mask matrices.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管[[222](#bib.bib222)]中的 ECCT 使用了从 PCM 派生的掩码矩阵，但对于相同的编码存在许多不同的 PCM，这将导致不同的解码性能。受此事实的启发，[[246](#bib.bib246)]中的作者解决了识别最优
    PCM 的问题。特别地，作者提出了一种从系统 PCM 构造的系统化掩码矩阵，这导致稀疏的自注意力图，并提出了一种新型 Transformer 架构，称为双掩码
    ECCT，该架构由两个并行的掩码自注意力块组成，使用不同的掩码矩阵。
- en: Meanwhile, in [[223](#bib.bib223)], the authors employed DDPM [[173](#bib.bib173)]
    for soft decoding of linear codes with arbitrary block lengths. Their framework
    models the transmission over the AWGN channel as a series of diffusion steps that
    can be iteratively reversed. The paper also proposed to condition the diffusion
    decoder on the number of parity check errors and to employ a line-search procedure
    to control the reverse diffusion step size.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在[[223](#bib.bib223)]中，作者使用 DDPM [[173](#bib.bib173)] 进行任意块长度线性码的软解码。他们的框架将
    AWGN 通道上的传输建模为可以迭代反转的一系列扩散步骤。论文还提议将扩散解码器的条件设定为奇偶校验错误的数量，并采用线搜索过程来控制反向扩散步骤的大小。
- en: Furthermore, in [[247](#bib.bib247)], the authors proposed a foundation model
    [[248](#bib.bib248)] for channel codes by extending the Transformer architecture.
    A foundation model refers to a model that is initially trained on a wide range
    of data, generally based on self-supervision, and then adapted (e.g., transferred
    or fine-tuned) to a wide range of downstream tasks. Thus, the proposed framework
    provided a universal decoder that is capable of adapting and generalizing to any
    (unseen) code of any length.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[[247](#bib.bib247)]中，作者通过扩展 Transformer 架构提出了一种用于信道编码的基础模型[[248](#bib.bib248)]。基础模型指的是一种最初在广泛数据上进行训练的模型，通常基于自监督，然后适应（例如，迁移或微调）到各种下游任务。因此，提出的框架提供了一个通用解码器，能够适应并推广到任何（未见过的）任意长度的编码。
- en: IV-A3 Syndrome-Based Loss Function
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A3 基于综合症的损失函数
- en: Syndrome decoding is a well-known approach for decoding algebraic codes. Several
    approaches have been proposed for training DL-based syndrome decoders that estimate
    the transmitted codeword from the syndrome. Syndrome-based training does not rely
    on the knowledge of the transmitted codeword, and is thus promising for online
    adaptation to changing channel conditions.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 综合症解码是一种已知的代数码解码方法。已经提出了几种基于深度学习的综合症解码器训练方法，这些方法从综合症中估计传输的编码字。基于综合症的训练不依赖于传输编码字的知识，因此在在线适应变化的信道条件方面具有前景。
- en: 'The paper [[224](#bib.bib224)] is one of the early works on syndrome-based
    DL decoding, which proposed to use the absolute values of the received symbols,
    i.e., reliabilities in the case of the BI-AWGN channel, and the syndrome of its
    hard decisions for decoding, instead of directly using the received symbols as
    the input to the decoder. The proposed decoder is illustrated in Fig. [9](#S4.F9
    "Figure 9 ‣ IV-A3 Syndrome-Based Loss Function ‣ IV-A Model-Free Decoders ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"). Furthermore, the authors introduced permutations from the code’s automorphism
    group [[249](#bib.bib249), [250](#bib.bib250)] as a preprocessing. Permutations
    in this group have the property that the permuted version of any codeword is guaranteed
    to be also a valid codeword, i.e., the permuted input of the decoder is a noisy
    valid codeword. Simulations demonstrated that the proposed framework can achieve
    near maximum-likelihood performance for short \acbch codes.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '论文[[224](#bib.bib224)]是早期关于基于综合症的DL解码的研究之一，提出了使用接收符号的绝对值，即在BI-AWGN信道的情况下的可靠性，以及其硬决策的综合症进行解码，而不是直接使用接收符号作为解码器的输入。所提出的解码器如图[9](#S4.F9
    "Figure 9 ‣ IV-A3 Syndrome-Based Loss Function ‣ IV-A Model-Free Decoders ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey")所示。此外，作者引入了来自编码自同构群[[249](#bib.bib249), [250](#bib.bib250)]的排列作为预处理。这些排列具有这样的性质：任何码字的排列版本也保证是一个有效的码字，即解码器的排列输入是一个有噪声的有效码字。模拟结果表明，所提出的框架可以实现接近最大似然性能的短\acbch码。'
- en: '![Refer to caption](img/e11b0b32874817fe2a08be82c83cfbb9.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e11b0b32874817fe2a08be82c83cfbb9.png)'
- en: 'Figure 9: The syndrome-based DL decoder proposed in [[224](#bib.bib224)]. The
    channel input sequence $\mathbf{x}$ consists of \acbpsk symbols, and $\mathbf{y}$
    is the output of a \acbiso channel.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：基于综合症的DL解码器，参见[[224](#bib.bib224)]。信道输入序列$\mathbf{x}$由\acbpsk符号组成，而$\mathbf{y}$是\acbiso信道的输出。
- en: The syndrome-based approach, which attempts to predict the error vector from
    its syndrome alone, can suffer from the potential presence of inconsistent training
    examples, called *disturbance* [[251](#bib.bib251)], i.e., training examples with
    the same syndromes but with different error vectors. To solve this problem, the
    authors in [[225](#bib.bib225)] proposed an iterative algorithm, referred to as
    iterative error decimation, which is robust against the superposition of error
    patterns. In each iteration, the DL decoder estimates the error vector and then
    decimates (subtracts) it from the received vector. The simulation results demonstrated
    that the proposed scheme improves the performance of the scheme in [[224](#bib.bib224)].
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 基于综合症的方法尝试仅从其综合症预测误差向量，可能会受到不一致训练样本的潜在影响，这些样本被称为*干扰*[[251](#bib.bib251)]，即具有相同综合症但不同误差向量的训练样本。为解决此问题，[[225](#bib.bib225)]中的作者提出了一种迭代算法，称为迭代误差消除，该算法对误差模式的叠加具有鲁棒性。在每次迭代中，DL解码器估计误差向量，然后将其从接收向量中消除（减去）。模拟结果表明，所提出的方案改进了[[224](#bib.bib224)]中方案的性能。
- en: Furthermore, the authors in [[226](#bib.bib226)] proposed a syndrome-based approach
    to \acsiso decoding of BCH component codes in turbo product codes [[252](#bib.bib252)]
    based on Stacked-GRU, which is an RNN architecture composed of GRU. They introduced
    a regularization term into a loss function and demonstrated that the proposed
    DL decoder outperforms the original chase decoder in [[252](#bib.bib252)].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[[226](#bib.bib226)]中的作者提出了一种基于综合症的方法，用于对BCH组件码的Turbo产品码[[252](#bib.bib252)]进行\acsiso解码，该方法基于Stacked-GRU，这是一种由GRU组成的RNN架构。他们在损失函数中引入了正则化项，并证明所提出的DL解码器优于[[252](#bib.bib252)]中的原始追踪解码器。
- en: IV-A4 Adaptability
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A4 适应性
- en: Adaptive coding is a technique for adapting a code rate to a channel condition
    in wireless channels. For a DL-based decoder, supporting multiple code rates not
    only requires multiple training, which is computationally intensive, but also
    requires a large amount of memory to store the learned DL parameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应编码是一种根据无线信道条件调整编码速率的技术。对于基于DL的解码器，支持多个编码速率不仅需要多次训练，这在计算上是非常密集的，还需要大量内存来存储学习到的DL参数。
- en: To address this issue, a unified DL-based decoder for polar and LDPC codes was
    proposed in [[227](#bib.bib227)], which supports different codes, i.e., polar
    and LDPC codes, with a single DNN by utilizing a code indicator at the decoder
    input. The simulation results demonstrated the potential of the unified decoder
    for very short code lengths, e.g., $16$ bits. A similar unified DL decoder using
    the code indicator was also proposed in [[230](#bib.bib230)] for BCH and CRC-concatenated
    polar codes. Their results demonstrated that, for a code length of $64$, the proposed
    unified decoding scheme with code indicator achieves a small performance gap of
    less than $0.2$ dB from the decoder trained solely for the single code.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，[[227](#bib.bib227)] 提出了一个统一的DL解码器，用于极化码和LDPC码，该解码器通过在解码器输入处利用码指示符支持不同的码，即极化码和LDPC码。仿真结果展示了该统一解码器在非常短的码长（例如$16$位）中的潜力。类似的使用码指示符的统一DL解码器也在
    [[230](#bib.bib230)] 中提出，用于BCH和CRC连接的极化码。他们的结果表明，对于码长为$64$的码，所提出的带有码指示符的统一解码方案与仅为单一码训练的解码器相比，性能差距小于$0.2$
    dB。
- en: Meanwhile, the paper [[228](#bib.bib228)] introduced the meta-learning-based
    neural decoder, termed as \acmind, which can adapt to a new channel with a small
    number of pilots and few gradient descent steps. Specifically, the proposed approach
    consists of meta-learning and meta-training steps, where the model learns good
    initial parameters in the meta-learning step, and then adapts the parameters to
    the observed channel in the meta-testing phase using minimal adaptation data and
    pilots. It has been demonstrated that the proposed scheme can adapt to a channel
    while achieving a performance close to that of a DL decoder designed solely for
    the particular channel. In the same line of research, the authors in [[229](#bib.bib229)]
    proposed transfer learning to efficiently train decoders for a set of rate-compatible
    polar codes that are expurgated from the same mother code as in 5G NR.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，论文 [[228](#bib.bib228)] 介绍了一种基于元学习的神经解码器，称为\acmind，该解码器可以通过少量的导频和少数梯度下降步骤适应新的信道。具体而言，提出的方法包括元学习和元训练步骤，其中模型在元学习步骤中学习到良好的初始参数，然后在元测试阶段使用最少的适应数据和导频将参数调整到观测信道。研究表明，所提出的方案可以在接近仅为特定信道设计的DL解码器的性能的同时适应信道。在同一研究方向上，[[229](#bib.bib229)]
    的作者提出了迁移学习，以高效地训练一组速率兼容的极化码解码器，这些码是从与5G NR相同母码中筛选出来的。
- en: IV-A5 RL-Based Approaches
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A5 基于RL的方法
- en: Although the majority of previous works on DL-based channel decoding are based
    on supervised learning, several RL-based approaches have been proposed for the
    cases where supervisory data (ground truth) is unavailable.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数以前关于基于DL的信道解码的研究都是基于监督学习的，但已经提出了几种基于RL的方法，用于监督数据（真实数据）不可用的情况。
- en: The paper [[231](#bib.bib231)] is one of the earliest works on RL-based channel
    decoding. Unlike these studies, the authors in [[231](#bib.bib231)] proposed an
    RL framework for iterative \acbf decoding of binary linear codes. Specifically,
    they linked the BF decision at each step to MDP and applied RL to find good decision
    strategies. The authors also exploited the permutation automorphism group to improve
    the performance. The extensive simulations showed that the learned BF decoders
    with DQN can achieve near-optimal performance for short, high-rate codes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [[231](#bib.bib231)] 是关于基于强化学习（RL）的信道解码的最早研究之一。与这些研究不同，[[231](#bib.bib231)]
    的作者提出了一种用于二进制线性码的迭代\acbf解码的RL框架。具体而言，他们将每一步的BF决策与MDP联系起来，并应用RL寻找良好的决策策略。作者还利用了排列自同构群来提高性能。广泛的仿真结果表明，使用DQN的学习型BF解码器可以在短的高码率码中实现接近最优的性能。
- en: Later, the authors in [[232](#bib.bib232)] applied RL-based BF decoding to polar
    codes. In contrast to the DQN proposed in [[231](#bib.bib231)], they used simple
    Q-learning and attempted to map channel observations directly onto estimated codewords.
    Simulation results showed that the proposed Q-learning achieves comparable performance
    to the learned BF decoding in [[231](#bib.bib231)] with lower complexity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，[[232](#bib.bib232)] 的作者将基于RL的BF解码应用于极化码。与 [[231](#bib.bib231)] 中提出的DQN相比，他们使用了简单的Q学习，并尝试将信道观测直接映射到估计的码字上。仿真结果表明，所提出的Q学习在性能上与
    [[231](#bib.bib231)] 中学习型BF解码相当，但复杂度较低。
- en: IV-A6 Complexity Reduction
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A6 复杂度减少
- en: In general, decoding longer codes requires a larger neural network size. Such
    a network not only requires a huge amount of computational resources in the training
    phase, but also imposes high computational and space complexities in the inference
    phase. In particular, the complexity in the inference phase is of practical importance
    since the training is usually performed offline.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解码较长的码字需要更大的神经网络。这种网络不仅在训练阶段需要大量计算资源，而且在推断阶段也会带来高计算和空间复杂度。特别是推断阶段的复杂度具有实际重要性，因为训练通常是在离线进行的。
- en: The authors in [[233](#bib.bib233)] attempted to reduce the parameter size of
    a DL decoder, by introducing various simplified neural network structures with
    fewer parameters. In [[234](#bib.bib234)], the same authors further extended their
    work by introducing the magnitude-based pruning [[253](#bib.bib253)] and quantization
    of the parameters. The proposed decoder was demonstrated to achieve similar performance
    to the original system in [[224](#bib.bib224)] even with $80$% reduction of the
    network parameters and $8$-bit fixed-point representation. Furthermore, FPGA implementation
    of the proposed decoder has been presented in [[235](#bib.bib235), [236](#bib.bib236)].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[[233](#bib.bib233)]中的作者尝试通过引入各种简化的神经网络结构来减少深度学习解码器的参数规模。这些结构具有更少的参数。在[[234](#bib.bib234)]中，同一作者进一步扩展了他们的工作，介绍了基于幅度的剪枝[[253](#bib.bib253)]和参数量化。所提出的解码器在减少了$80$%网络参数和$8$位定点表示的情况下，仍能实现与原始系统[[224](#bib.bib224)]类似的性能。此外，所提出的解码器在[[235](#bib.bib235),
    [236](#bib.bib236)]中进行了FPGA实现。'
- en: Meanwhile, the authors in [[237](#bib.bib237)] considered DL-aided complexity
    reduction of \acosd [[254](#bib.bib254)], which is a soft-decision decoding algorithm
    of linear block codes that approaches the optimal maximum-likelihood decoding
    performance especially for short codes. Although increasing the order parameter
    of OSD leads to near-maximum-likelihood performance, it may waste computational
    resources when the received signal can be decoded with lower order. The paper
    [[237](#bib.bib237)] proposed a learning-based approach to adapt the required
    order parameter to the channel condition and demonstrated the effectiveness of
    the proposed scheme in terms of the performance-complexity trade-off through numerical
    simulations.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，[[237](#bib.bib237)]中的作者考虑了辅助复杂度减少的深度学习（DL）方法，对\acosd [[254](#bib.bib254)]进行了改进。该算法是一种软判决解码算法，适用于线性块码，特别是在短码情况下接近最优最大似然解码性能。尽管增加OSD的阶数可以达到接近最大似然的性能，但当接收到的信号可以用较低阶数解码时，可能会浪费计算资源。论文[[237](#bib.bib237)]提出了一种基于学习的方法，根据信道条件调整所需的阶数参数，并通过数值仿真展示了所提方案在性能与复杂度权衡方面的有效性。
- en: IV-A7 Other Approaches
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A7 其他方法
- en: It is known that multiple decoding attempts over different permutations of received
    codewords provide a performance gain [[250](#bib.bib250), [255](#bib.bib255)].
    However, it remains unclear how to choose the permutation that yields the best
    performance. To address this, the authors in [[238](#bib.bib238)] presented a
    DL approach to selecting candidates from the code’s automorphism group in permutation
    decoding. In this scheme, a trained network predicts the probability of successful
    decoding for each permutation, and decoding is performed only for permuted codewords
    with the highest probabilities. The proposed algorithm has been demonstrated by
    simulations to achieve remarkable performance gains over a random selection of
    permutations from the automorphism group.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，对接收到的码字进行多次解码尝试，通过不同的排列组合可以提高性能[[250](#bib.bib250), [255](#bib.bib255)]。然而，如何选择能够带来最佳性能的排列仍不清楚。为了解决这一问题，[[238](#bib.bib238)]中的作者提出了一种深度学习（DL）方法，用于从码字的自同构群中选择候选排列。在这种方案中，经过训练的网络预测每个排列成功解码的概率，并仅对具有最高概率的排列码字进行解码。通过仿真，所提出的算法被证明在自同构群的随机排列选择中实现了显著的性能提升。
- en: In [[239](#bib.bib239)], the authors proposed a novel approach referred to as
    *friendly attack* for improving channel decoding performance, inspired by the
    concept of adversarial attacks. The proposed scheme introduces small perturbations
    into the modulated symbols before transmission. The perturbations are designed
    by a modified iterative fast gradient method [[256](#bib.bib256)] such that a
    loss function between the decoded codeword and the transmitted codeword is minimized.
    The performance improvement by the proposed scheme has been demonstrated for various
    codes and decoders.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[239](#bib.bib239)] 中，作者提出了一种新颖的方法称为 *友好攻击*，旨在通过引入对抗攻击的概念来提高信道解码性能。该方案在传输之前向调制符号引入了小的扰动。扰动是通过修改的迭代快速梯度方法
    [[256](#bib.bib256)] 设计的，以使解码的码字与传输的码字之间的损失函数最小化。所提出方案的性能提升已在各种码和解码器中得到验证。
- en: Although the use of DL for channel decoding has been experimentally validated,
    the theoretical justification for the developed algorithm in terms of, e.g. the
    generalization properties, remains challenging. The authors in [[240](#bib.bib240)]
    addressed the problem of maximizing the margin of the decoder for an additive
    noise channel whose noise distribution is unknown, as well as for a nonlinear
    channel with AWGN. They formulated a maximum margin optimization problem, which
    is common in \acpsvm, for the decoder learning problem, and they relaxed it to
    a \acrlm problem by several approximation steps. The paper then provided expected
    generalization error bounds for both models, under optimal choice of the regularization
    parameter. The paper also presented a theoretical guidance for choosing the training
    SNR based on the bound for the additive noise channel.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 DL 在信道解码中的应用已得到实验验证，但开发算法的理论依据，例如泛化性能，仍然具有挑战性。[[240](#bib.bib240)] 的作者解决了在噪声分布未知的加性噪声信道以及具有
    AWGN 的非线性信道中最大化解码器边际的问题。他们将解码器学习问题表述为最大边际优化问题，这在 \acpsvm 中很常见，并通过若干近似步骤将其松弛为 \acrlm
    问题。本文随后为两种模型提供了在最佳正则化参数选择下的预期泛化误差界限。本文还基于加性噪声信道的界限提供了选择训练 SNR 的理论指导。
- en: IV-B DL-Aided BP Decoding
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B DL 辅助 BP 解码
- en: 'TABLE IV: Summary of DL-aided BP decoding.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：DL 辅助 BP 解码总结。
- en: '| Category | Reference | Main Contribution |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 参考文献 | 主要贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Neural MS Decoding and Its Variants | Nachmani et al. [[257](#bib.bib257),
    [258](#bib.bib258)] | DL-aided BP, NMS, and OMS decoding. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 神经 MS 解码及其变体 | Nachmani 等人 [[257](#bib.bib257), [258](#bib.bib258)] | DL
    辅助 BP、NMS 和 OMS 解码。 |'
- en: '| Lugosch et al. [[259](#bib.bib259)] | NOMS decoding. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Lugosch 等人 [[259](#bib.bib259)] | NOMS 解码。 |'
- en: '| Dai et al. [[260](#bib.bib260)] | Neural network-aided OMS and NOMS decoding.
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 [[260](#bib.bib260)] | 神经网络辅助的 OMS 和 NOMS 解码。 |'
- en: '| Yu et al. [[261](#bib.bib261)] | Neural AMS decoding. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Yu 等人 [[261](#bib.bib261)] | 神经 AMS 解码。 |'
- en: '| Hsu et al. [[262](#bib.bib262)] | Neural network-aided VWMS decoding. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Hsu 等人 [[262](#bib.bib262)] | 神经网络辅助的 VWMS 解码。 |'
- en: '| Wu et al. [[263](#bib.bib263)] | Neural MS decoding with linear approximation
    for PB-LDPC codes. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Wu 等人 [[263](#bib.bib263)] | 用于 PB-LDPC 码的神经 MS 解码和线性近似。 |'
- en: '| Kim et al. [[264](#bib.bib264)] | Neural SCMS decoding. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Kim 等人 [[264](#bib.bib264)] | 神经 SCMS 解码。 |'
- en: '| Performance Enhancement | Teng et al. [[265](#bib.bib265), [266](#bib.bib266),
    [267](#bib.bib267)] | CNN-based learned BF for BP. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 性能提升 | Teng 等人 [[265](#bib.bib265), [266](#bib.bib266), [267](#bib.bib267)]
    | 基于 CNN 的学习 BF 用于 BP。 |'
- en: '| Sun et al. [[268](#bib.bib268)] | LSTM-based learned BF for BP. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Sun 等人 [[268](#bib.bib268)] | 基于 LSTM 的学习 BF 用于 BP。 |'
- en: '| Variants of Random Redundant Decoding (RRD) | Nachmani et al. [[258](#bib.bib258)]
    | mRRD decoding with RNN-based BP decoders. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 随机冗余解码 (RRD) 的变体 | Nachmani 等人 [[258](#bib.bib258)] | 使用基于 RNN 的 BP 解码器的
    mRRD 解码。 |'
- en: '| Liu et al. [[269](#bib.bib269)] | Node-classified redundant decoding algorithm.
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人 [[269](#bib.bib269)] | 节点分类的冗余解码算法。 |'
- en: '| Optimization-Based Decoding | Wei et al. [[270](#bib.bib270)] | Trainable
    ADMM-penalized decoder. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 基于优化的解码 | Wei 等人 [[270](#bib.bib270)] | 可训练的 ADMM 惩罚解码器。 |'
- en: '| Wadayama et al. [[271](#bib.bib271)] | Trainable PGD decoder for LDPC codes.
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Wadayama 等人 [[271](#bib.bib271)] | 可训练的 PGD 解码器用于 LDPC 码。 |'
- en: '| Wadayama et al. [[272](#bib.bib272)] | Proximal decoding for LDPC codes.
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Wadayama 等人 [[272](#bib.bib272)] | LDPC 码的近端解码。 |'
- en: '| RL-Based Approach | Doan et al. [[273](#bib.bib273)] | RL-based selection
    of permutations on factor-graph. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 基于 RL 的方法 | Doan 等人 [[273](#bib.bib273)] | 基于 RL 的因子图排列选择。 |'
- en: '| Habib et al. [[274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)]
    | RL-based scheduling optimization for sequential BP decoding. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Habib 等人 [[274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)] | 基于
    RL 的顺序 BP 解码调度优化。 |'
- en: '| Customized Loss Function | Lugosch et al. [[277](#bib.bib277)] | Soft syndrome
    as a loss function for training a neural BP decoder. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 定制损失函数 | Lugosch 等人 [[277](#bib.bib277)] | 作为训练神经 BP 解码器的损失函数的软综合症。 |'
- en: '| Teng et al. [[278](#bib.bib278)] | New syndrome losses for syndrome-based
    DL decoding of polar codes. |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Teng 等人 [[278](#bib.bib278)] | 用于极化码的基于综合症的新损失函数。 |'
- en: '|  | Nachmani et al. [[279](#bib.bib279)] | New loss function based on sparse
    node and knowledge distillation losses. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | Nachmani 等人 [[279](#bib.bib279)] | 基于稀疏节点和知识蒸馏损失的新损失函数。 |'
- en: '| Memory/Complexity Reduction | Teng et al. [[280](#bib.bib280)] | Weight quantization
    mechanism for an RNN polar decoder. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 内存/复杂度减少 | Teng 等人 [[280](#bib.bib280)] | RNN 极化解码器的权重量化机制。 |'
- en: '| Ibrahim et al. [[281](#bib.bib281)] | Quantization of an RNN polar decoder.
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Ibrahim 等人 [[281](#bib.bib281)] | RNN 极化解码器的量化。 |'
- en: '| Xiao et al. [[282](#bib.bib282), [283](#bib.bib283)] | Finite alphabet iterative
    decoders for LDPC codes via quantized RNN. |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Xiao 等人 [[282](#bib.bib282), [283](#bib.bib283)] | 通过量化 RNN 的有限字母表迭代解码器用于
    LDPC 码。 |'
- en: '| Lyu et al. [[284](#bib.bib284)] | A joint optimization of message quantization
    and quantization thresholds. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Lyu 等人 [[284](#bib.bib284)] | 消息量化与量化阈值的联合优化。 |'
- en: '| Lian et al. [[285](#bib.bib285), [286](#bib.bib286)] | Weight-sharing across
    edges based on scalar parameters. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Lian 等人 [[285](#bib.bib285), [286](#bib.bib286)] | 基于标量参数的边缘间权重共享。 |'
- en: '| Wang et al. [[287](#bib.bib287), [288](#bib.bib288)] | A parameter sharing
    scheme within the same layer for a neural NMS decoder. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 [[287](#bib.bib287), [288](#bib.bib288)] | 用于神经 NMS 解码器的同层内参数共享方案。
    |'
- en: '| Dai et al. [[289](#bib.bib289)] | A weight-sharing scheme for a neural MS
    decoder of protograph LDPC codes. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Dai 等人 [[289](#bib.bib289)] | 用于原型 LDPC 码的神经 MS 解码器的权重共享方案。 |'
- en: '| Liang et al. [[290](#bib.bib290), [291](#bib.bib291)] | Tensor-train and
    tensor-ring decompositions for parameter size reduction. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Liang 等人 [[290](#bib.bib290), [291](#bib.bib291)] | 用于参数大小减少的张量训练和张量环分解。
    |'
- en: '| Cheng et al. [[292](#bib.bib292)] | A weight-sharing scheme for adapting
    to multiple code rates. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Cheng 等人 [[292](#bib.bib292)] | 适应多种码率的权重共享方案。 |'
- en: '| Buchberger et al. [[293](#bib.bib293)] | A novel pruning-based neural BP
    decoder for short linear block codes. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Buchberger 等人 [[293](#bib.bib293)] | 用于短线性块码的新型剪枝基础神经 BP 解码器。 |'
- en: '| Buchberger et al. [[294](#bib.bib294)] | A neural BP with decimation. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Buchberger 等人 [[294](#bib.bib294)] | 带有删减的神经 BP。 |'
- en: '| GNN Decoders | Satorras et al. [[295](#bib.bib295)] | A hybrid inference
    model that combines BP and GNN. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| GNN 解码器 | Satorras 等人 [[295](#bib.bib295)] | 结合 BP 和 GNN 的混合推理模型。 |'
- en: '| Cammerer et al. [[296](#bib.bib296)] | A fully GNN-based decoder. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Cammerer 等人 [[296](#bib.bib296)] | 完全基于 GNN 的解码器。 |'
- en: '| Tian et al. [[297](#bib.bib297)] | An edge-weighted GNN decoder. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Tian 等人 [[297](#bib.bib297)] | 边权重 GNN 解码器。 |'
- en: '| Understanding Neural BP Decoders | Ankireddy et al. [[298](#bib.bib298)]
    | Empirical study on how the learned weights attenuate the effect of these cycles.
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 理解神经 BP 解码器 | Ankireddy 等人 [[298](#bib.bib298)] | 关于学习到的权重如何减轻这些循环影响的实证研究。
    |'
- en: '| Adiga et al. [[299](#bib.bib299)] | Theoretical study on the generalization
    capabilities of neural BP decoders. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Adiga 等人 [[299](#bib.bib299)] | 对神经 BP 解码器泛化能力的理论研究。 |'
- en: '| Other Approaches | Clausius et al. [[300](#bib.bib300)] | GNN-based joint
    equalization and decoding. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 其他方法 | Clausius 等人 [[300](#bib.bib300)] | 基于 GNN 的联合均衡和解码。 |'
- en: '| Wiesmayr et al. [[301](#bib.bib301)] | Deep-unfolded interleaved detection
    and decoding for MIMO wireless systems. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Wiesmayr 等人 [[301](#bib.bib301)] | MIMO 无线系统的深度展开交织检测与解码。 |'
- en: '| Lee et al. [[302](#bib.bib302)] | Learning-aided multi-round BP decoding
    with impulsive perturbation. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| Lee 等人 [[302](#bib.bib302)] | 学习辅助的多轮 BP 解码与冲击扰动。 |'
- en: '| Wang et al. [[303](#bib.bib303)] | DL detection of decodable codewords for
    reducing the decoding delay. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 [[303](#bib.bib303)] | 深度学习检测可解码码字以减少解码延迟。 |'
- en: 'BP decoding is an efficient iterative decoding algorithm that is commonly used
    for decoding LDPC codes. BP decoding is performed on Tanner graph consisting of
    CNs and VNs, which correspond to codeword bits and parity check equations, respectively.
    An example of PCM with the corresponding Tanner graph representation of a (7,
    4) Hamming code is shown in Fig. [10(a)](#S4.F10.sf1 "In Figure 10 ‣ IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey"). In BP decoding, decoding messages are iteratively
    updated at CNs and VNs based on the Bayes’ rule. In practice, the min-sum approximation
    [[304](#bib.bib304)] is applied to the CN updates to reduce complexity, and this
    decoding is referred to as \acms decoding. The performance loss due to the min-sum
    approximation can be compensated for by \acnms and \acoms decoders [[305](#bib.bib305)]
    at the cost of slightly increased complexity.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BP 解码是一种高效的迭代解码算法，通常用于解码 LDPC 码。BP 解码在包含 CNs 和 VNs 的 Tanner 图上进行，其中 CNs 和 VNs
    分别对应于码字位和奇偶校验方程。图 [10(a)](#S4.F10.sf1 "在图 10 ‣ IV-B 深度学习辅助 BP 解码 ‣ IV 渠道解码的深度学习
    ‣ 近期在信道编码中深度学习的进展：综述") 展示了 (7, 4) 汉明码的 PCM 及其对应的 Tanner 图表示。在 BP 解码中，解码消息基于贝叶斯规则在
    CNs 和 VNs 上迭代更新。在实际应用中，最小和近似 [[304](#bib.bib304)] 被应用于 CN 更新以降低复杂性，这种解码被称为 \acms
    解码。由于最小和近似造成的性能损失可以通过 \acnms 和 \acoms 解码器 [[305](#bib.bib305)] 来弥补，但会稍微增加复杂性。
- en: 'In [[257](#bib.bib257)], the authors proposed a DL-based implementation of
    BP decoding by treating BP decoding as a differentiable process, where the decoding
    messages are passed through unrolled iterations in a feed-forward fashion. Additionally,
    trainable weights were introduced at the edges, which are then optimized via SGD.
    An example of the unrolled BP trellis for a (7, 4) Hamming code is illustrated
    in Fig. [10(b)](#S4.F10.sf2 "In Figure 10 ‣ IV-B DL-Aided BP Decoding ‣ IV DL
    for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding: A
    Survey"). For code length $N$, the number of edges $E$, and the number of iterations
    $L$, the unfolded trellis has $N$ neurons at the input and output layers, and
    $E$ neurons at the $2L$ hidden layers. The network architecture is a non-fully
    connected neural network. As we review below, the trainable BP decoder over the
    unfolded trellis has been extensively studied in the literature. We summarize
    these works in Table [IV](#S4.T4 "TABLE IV ‣ IV-B DL-Aided BP Decoding ‣ IV DL
    for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding: A
    Survey").'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[257](#bib.bib257)] 中，作者提出了一种基于深度学习的 BP 解码实现，将 BP 解码视为一个可微分的过程，其中解码消息通过展开的迭代以前馈的方式传递。此外，在边缘引入了可训练的权重，并通过
    SGD 进行优化。图 [10(b)](#S4.F10.sf2 "在图 10 ‣ IV-B 深度学习辅助 BP 解码 ‣ IV 渠道解码的深度学习 ‣ 近期在信道编码中深度学习的进展：综述")
    展示了 (7, 4) 汉明码的展开 BP 轨迹示例。对于码长 $N$、边数 $E$ 和迭代次数 $L$，展开的轨迹在输入和输出层有 $N$ 个神经元，在 $2L$
    个隐藏层中有 $E$ 个神经元。网络架构是一个非完全连接的神经网络。正如我们下面的回顾中所示，关于展开轨迹的可训练 BP 解码器在文献中已有广泛研究。我们在表
    [IV](#S4.T4 "表 IV ‣ IV-B 深度学习辅助 BP 解码 ‣ IV 渠道解码的深度学习 ‣ 近期在信道编码中深度学习的进展：综述") 中总结了这些工作。
- en: '![Refer to caption](img/3b6ae1346392806b5d36c10c9672fa43.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3b6ae1346392806b5d36c10c9672fa43.png)'
- en: (a) PCM and corresponding Tanner graph.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PCM 和相应的 Tanner 图。
- en: '![Refer to caption](img/6e10b6fe688b17e989b1d1f2a3cb5e8f.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6e10b6fe688b17e989b1d1f2a3cb5e8f.png)'
- en: (b) Unrolled BP trellis (two iterations).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 展开的 BP 轨迹（两个迭代）。
- en: 'Figure 10: An example of deep unfolded BP decoder for (7, 4) Hamming codes.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: (7, 4) 汉明码的深度展开 BP 解码器示例。'
- en: Note that the idea of unfolding an iterative algorithm into a structure analogous
    to a neural network, i.e., deep unfolding [[306](#bib.bib306)], is a common model-based
    DL approach [[307](#bib.bib307)] often considered in the design of communication
    systems. Besides channel decoding, the idea of deep unfolding has been successfully
    applied, for example, to MIMO signal detection and channel estimation [[62](#bib.bib62),
    [45](#bib.bib45), [308](#bib.bib308)].
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，将迭代算法展开为类似于神经网络的结构，即深度展开 [[306](#bib.bib306)]，是一种常见的基于模型的深度学习方法 [[307](#bib.bib307)]，通常在通信系统设计中考虑。除了信道解码，深度展开的思想也已成功应用于
    MIMO 信号检测和信道估计等 [[62](#bib.bib62), [45](#bib.bib45), [308](#bib.bib308)]。
- en: IV-B1 Neural MS Decoders and Its Variants
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 神经 MS 解码器及其变体
- en: As mentioned above, the paper [[257](#bib.bib257)] was the initial work that
    applied a feedforward network to BP decoding, where trainable weights are assigned
    to the edges of the factor graph, which are then optimized via SGD over the unrolled
    iterative BP decoding. The proposed parameterized decoder can compensate for the
    effect of small cycles in the Tanner graph of the code by properly scaling the
    weights. The effectiveness of the proposed decoder has been demonstrated for short
    BCH codes, for which standard BP decoding does not work well due to many short
    cycles in the graph. Later, in [[309](#bib.bib309)], the same authors extended
    the work by introducing an RNN architecture and showed that this architecture
    leads to comparable performance to the feedforward architecture in [[257](#bib.bib257)]
    even with fewer parameters. In [[258](#bib.bib258)], the authors also proposed
    neural network-based NMS and OMS decoders for reducing the complexity of the BP
    decoder, which are the generalized versions of the standard NMS and OMS decoders
    [[305](#bib.bib305)]. Neural network-based OMS decoding has also been studied
    independently in [[259](#bib.bib259)]. Another approach to enhance the performance
    of MS decoding while preserving the low complexity property was considered in
    [[263](#bib.bib263)]; the authors proposed \aclams for \acpbldpc codes, where
    the magnitudes of the check node updating and channel output are linearly adjusted
    by a small and shallow neural network. In contrast to the above-mentioned studies
    that considered the flooding schedule, the authors in [[310](#bib.bib310)] proposed
    a neural network-aided \acnoms decoding for the layered BP with application to
    5G LDPC codes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，论文 [[257](#bib.bib257)] 是首次将前馈网络应用于 BP 解码的研究工作，其中训练权重被分配到因子图的边缘，并通过 SGD
    在展开的迭代 BP 解码过程中进行优化。所提出的参数化解码器可以通过适当缩放权重来补偿代码的 Tanner 图中小循环的影响。该解码器的有效性已在短 BCH
    码上得到了验证，因为标准 BP 解码在图中存在许多短循环时效果不佳。随后，在 [[309](#bib.bib309)] 中，同一作者通过引入 RNN 架构扩展了该工作，并展示了这种架构在参数更少的情况下，能够与
    [[257](#bib.bib257)] 中的前馈架构达到相当的性能。在 [[258](#bib.bib258)] 中，作者还提出了基于神经网络的 NMS
    和 OMS 解码器，以降低 BP 解码器的复杂性，这些解码器是标准 NMS 和 OMS 解码器 [[305](#bib.bib305)] 的推广版本。基于神经网络的
    OMS 解码也在 [[259](#bib.bib259)] 中进行了独立研究。为了提升 MS 解码的性能同时保持低复杂度的特性，[[263](#bib.bib263)]
    中考虑了另一种方法；作者提出了用于 \acpbldpc 码的 \aclams，其中检查节点更新和信道输出的幅度通过一个小而浅的神经网络线性调整。与上述考虑洪水调度的研究不同，[[310](#bib.bib310)]
    中的作者提出了一种神经网络辅助的 \acnoms 解码，用于分层 BP 并应用于 5G LDPC 码。
- en: As another variant of MS decoding, the \acams decoding proposed by Qualcomm
    [[311](#bib.bib311)] has drawn attention, where it employs \acplut to simplify
    nonlinear CN processing. In [[261](#bib.bib261)], the authors introduced a neural
    network based selection mechanism to AMS decoding that automatically selects the
    check node updating rule from either the MS rule or the BP rule and demonstrated
    that the proposed decoder outperforms the conventional neural NMS decoder. The
    \acsmms algorithm [[312](#bib.bib312)] is another variant of MS decoding that
    simplifies the CN update in the MS algorithm. Specifically, in SMMS decoding,
    only one minimum magnitude is calculated at each CN over all the CN inputs and
    a correction is applied to outgoing messages if required. The SMMS decoder can
    be further improved by the \acvwms algorithm [[313](#bib.bib313)], which introduces
    variable correction factors into the CN update that depend on the number of iterations.
    To efficiently learn the optimal correction factors in the VWMS algorithm, the
    authors in [[262](#bib.bib262)] proposed a neural network-aided approach, instead
    of an exhaustive search in the original work [[313](#bib.bib313)]. The effectiveness
    of the proposed scheme in terms of throughput was demonstrated experimentally
    using the 40 nm CMOS TSMC process. Unlike the above-mentioned methods, which simplify
    CN updates, \acscms decoding [[314](#bib.bib314)] modifies the VN processing by
    deleting unreliable messages. More specifically, in SCMS decoding, any variable
    node message that changes sign between two consecutive iterations is discarded,
    i.e., set to zero. The authors in [[264](#bib.bib264)] introduced trainable normalization
    and offset weights to the SCMS decoder, which are trained by DL techniques. It
    was demonstrated that the error rate performance of the proposed neural SCMS decoder
    is close to that of the BP decoding.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 作为MS解码的另一种变体，Qualcomm提出的\acams解码[[311](#bib.bib311)]引起了关注，它采用\acplut来简化非线性CN处理。在[[261](#bib.bib261)]中，作者将基于神经网络的选择机制引入AMS解码，该机制自动选择MS规则或BP规则中的检查节点更新规则，并且展示了所提出的解码器优于传统的神经NMS解码器。\acsmms算法[[312](#bib.bib312)]是MS解码的另一种变体，它简化了MS算法中的CN更新。具体而言，在SMMS解码中，只计算每个CN所有CN输入中的最小幅值，并在需要时对传出的消息进行修正。SMMS解码器可以通过\acvwms算法[[313](#bib.bib313)]进一步改进，该算法在CN更新中引入依赖于迭代次数的变量修正因子。为了有效地学习VWMS算法中的最佳修正因子，[[262](#bib.bib262)]中的作者提出了一种神经网络辅助的方法，而不是原始工作[[313](#bib.bib313)]中的穷举搜索。通过使用40
    nm CMOS TSMC工艺实验验证了所提出方案在吞吐量方面的有效性。与上述简化CN更新的方法不同，\acscms解码[[314](#bib.bib314)]通过删除不可靠的消息来修改VN处理。更具体而言，在SCMS解码中，任何在两个连续迭代之间改变符号的变量节点消息都被丢弃，即设置为零。[[264](#bib.bib264)]中的作者向SCMS解码器引入了可训练的归一化和偏移权重，这些权重通过深度学习技术进行训练。实验表明，所提出的神经SCMS解码器的错误率性能接近BP解码。
- en: Although the above-mentioned works applied neural BP decoding to LDPC codes,
    it has also been used to decode polar codes. In [[315](#bib.bib315)], similar
    to [[257](#bib.bib257)], a neural MS decoder was applied to factor graphs of polar
    codes. The proposed decoder was demonstrated by simulations to outperform conventional
    BP decoding with the same number of iterations. Also, the authors presented an
    efficient hardware implementation of the basic computation block of the proposed
    decoder. In [[316](#bib.bib316)], a similar trainable BP decoding was applied
    to sparse graphs of polar codes [[317](#bib.bib317)], which was shown to achieve
    comparable performance to BP decoding even with a single trainable parameter.
    Furthermore, the authors in [[260](#bib.bib260)] proposed an NOMS decoder for
    polar codes that introduces both normalization (or scaling) factors and offsets,
    and demonstrated that the proposed decoder achieves better performance than the
    state-of-the-art schemes, including the decoder proposed in [[315](#bib.bib315)].
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述工作将神经BP解码应用于LDPC码，但它也被用于解码极化码。在[[315](#bib.bib315)]中，与[[257](#bib.bib257)]类似，将神经MS解码器应用于极化码的因子图。通过仿真证明，所提出的解码器在相同的迭代次数下优于传统的BP解码。此外，作者还展示了所提出解码器基本计算块的高效硬件实现。在[[316](#bib.bib316)]中，将类似的可训练BP解码应用于极化码的稀疏图[[317](#bib.bib317)]，即使只有一个可训练参数也显示出与BP解码相当的性能。此外，[[260](#bib.bib260)]中的作者提出了一种用于极化码的NOMS解码器，该解码器引入了归一化（或缩放）因子和偏移量，并展示了所提出的解码器性能优于包括[[315](#bib.bib315)]中提出的解码器在内的最先进方案。
- en: In order to enhance the performance of a standalone polar code and close the
    performance gap from CA-SCL decoding with lower latency, the authors in [[318](#bib.bib318)]
    proposed neural BP decoding for polar codes concatenated with a CRC code by exploiting
    the concatenated factor graph of the polar code and CRC, while the conventional
    BP decoding for concatenated CRC-polar codes is applied only to the factor graph
    of polar codes and the CRC is used only to verify the result of BP at each iteration.
    Furthermore, the authors in[[319](#bib.bib319)] considered concatenated polar
    and LDPC codes and proposed two-dimensional OMS decoding. They optimized the trainable
    parameters of the decoder by back propagation over the unfolded BP trellis and
    showed that the performance of the proposed decoder is comparable to the exact
    BP decoder.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升独立极化码的性能，并缩小与低延迟 CA-SCL 解码的性能差距，[[318](#bib.bib318)] 的作者提出了通过利用极化码和 CRC
    码的串联因子图来进行神经 BP 解码，而传统的 CRC-极化码 BP 解码仅应用于极化码的因子图，并且 CRC 仅用于验证每次迭代的 BP 结果。此外，[[319](#bib.bib319)]
    的作者考虑了串联的极化码和 LDPC 码，并提出了二维 OMS 解码。他们通过在展开的 BP 树形图上进行反向传播来优化解码器的可训练参数，并展示了所提出的解码器性能与精确
    BP 解码器相当。
- en: IV-B2 Performance Enhancement
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 性能增强
- en: To enhance BP decoding of polar codes, the authors in [[265](#bib.bib265), [266](#bib.bib266),
    [267](#bib.bib267)] proposed to combine BP decoding with a CNN-assisted bit flipping
    mechanism, which performs the flipping bit selection in the BP-BF decoder [[320](#bib.bib320)]
    based on a CNN trained using the metadata of the BP decoding process. The authors
    demonstrated that the proposed scheme can achieve a lower BLER than SCL decoding.
    Meanwhile, in order to reduce the computational complexity of the CNN-based approach,
    the paper [[268](#bib.bib268)] proposed an LSTM network that predicts error-prone
    bits to be flipped based on the magnitude of \acllr after the original BP decoding.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强极化码的 BP 解码，[[265](#bib.bib265), [266](#bib.bib266), [267](#bib.bib267)]
    的作者提出将 BP 解码与 CNN 辅助的比特翻转机制相结合，该机制在基于 BP 解码过程的元数据训练的 CNN 的基础上，在 BP-BF 解码器 [[320](#bib.bib320)]
    中执行翻转比特选择。作者展示了所提出的方案可以实现比 SCL 解码更低的 BLER。同时，为了降低基于 CNN 的方法的计算复杂性，论文 [[268](#bib.bib268)]
    提出了一个 LSTM 网络，该网络基于原始 BP 解码后的 \acllr 大小预测易错比特。
- en: The authors in [[321](#bib.bib321)] introduced a hypernetwork [[322](#bib.bib322)]
    that generates weights of a neural BP decoder to make the decoder more adaptive
    by letting the weights be a function of the node’s input. The same authors also
    introduced hypernetworks for decoding short polar codes [[323](#bib.bib323)] and
    showed that the proposed decoder achieves similar BER as SCL decoding in the high
    SNR region. Furthermore, they proposed an autoregressive BP decoder that incorporates
    the estimated SNR and multiple autoregressive signals obtained from the intermediate
    output of the network [[324](#bib.bib324)].
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '[[321](#bib.bib321)] 的作者引入了一种超网络 [[322](#bib.bib322)]，该超网络生成神经 BP 解码器的权重，以使解码器通过将权重作为节点输入的函数来更具适应性。同一作者还引入了用于解码短极化码的超网络
    [[323](#bib.bib323)]，并展示了所提出的解码器在高 SNR 区域内实现了与 SCL 解码相似的 BER。此外，他们提出了一种自回归 BP
    解码器，该解码器结合了估计的 SNR 和从网络中间输出获取的多个自回归信号 [[324](#bib.bib324)]。'
- en: Training data preparation is an essential part of training DNN-based decoders.
    In particular, the choice of training SNR plays an important role in training
    a DL-based channel decoder for generalization. A common approach is to train the
    decoder over varying SNR ranges [[257](#bib.bib257)]. Besides, the optimal choice
    of the training SNR has been studied either empirically [[244](#bib.bib244), [215](#bib.bib215)]
    or analytically [[325](#bib.bib325)]. To address the problem of choosing the optimal
    training SNR, the authors in [[326](#bib.bib326)] proposed *active deep decoding*,
    inspired by active learning [[327](#bib.bib327)]. Specifically, based on the observation
    that no optimal training SNR for all validation sets exists, the paper proposed
    to adaptively sample training data instead of passively generating examples during
    training. It was demonstrated that this active deep decoding scheme offers performance
    gain by effectively sampling the training data without increasing the inference
    (decoding) complexity.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据准备是训练基于DNN的译码器的一个重要部分。特别是，训练信噪比的选择在训练DL-based信道译码器的泛化中起着重要作用。一种常见的方法是在不同的SNR范围内训练译码器[[257](#bib.bib257)]。此外，训练SNR的最佳选择已经进行了经验性[[244](#bib.bib244)，[215](#bib.bib215)]或分析性[[325](#bib.bib325)]的研究。为了解决选择最佳训练SNR的问题，作者在[[326](#bib.bib326)]中提出了*主动深度译码*的概念，灵感来源于主动学习[[327](#bib.bib327)]。具体地，根据观察到并不存在适用于所有验证集的最佳训练SNR，该论文提出了在训练过程中自适应地采样训练数据而不是被动地生成示例。实验证明，这种主动深度译码方案通过有效地对训练数据进行采样而不增加推断（译码）复杂性，提高了性能。
- en: In [[328](#bib.bib328)], inspired by ensemble models that are widely used to
    solve complex tasks by decomposing them into multiple simpler tasks each of which
    is solved locally by a single expert member of the ensemble [[329](#bib.bib329)],
    the authors introduced the ensemble of neural BP decoders. The proposed scheme
    consists of a single classical \achdd and multiple trainable BP decoders, where
    the classical HDD is employed to assign a received codeword to a single expert
    BP decoder based on the number of the estimated codeword errors. It was demonstrated
    that this scheme achieves remarkable performance gains over the single neural
    BP. Furthermore, the data-driven ensemble scheme has been extended to BP polar
    decoders in [[330](#bib.bib330), [331](#bib.bib331)].
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[328](#bib.bib328)]中，受到将复杂任务分解为多个由集成模型解决的简单任务的方法的启发，其中每个任务由集成模型中的单个专家成员在本地解决[[329](#bib.bib329)]，作者引入了神经BP译码器的集成。所提出的方案包括一个经典的HDD和多个可训练的BP译码器，其中经典的HDD被用于根据估计的码字错误的数量将接收到的码字分配给单个专家BP译码器。实验证明，这种方案比单一的神经BP获得了显著的性能提升。此外，数据驱动的集成方案已经扩展到BP极化译码器[[330](#bib.bib330)，[331](#bib.bib331)]。
- en: Meanwhile, many practical LDPC codes exhibit an error floor⁵⁵5For modern iteratively
    decodable codes, such as LDPC codes and turbo codes, there is an SNR point after
    which the error rate decreases only slowly. This phenomenon is called *error floor*..
    For applications such as ultra-reliable and low-latency communications that require
    extremely low BLER, it can be critical to mitigate the error floor. Since the
    error floor of LDPC codes is commonly attributed to the suboptimality of the iterative
    message passing decoding algorithms for factor graphs with cycles, the paper [[332](#bib.bib332)]
    proposed training methods for neural NMS decoders to eliminate the error floor
    of LDPC codes. Specifically, inspired by the boosting learning technique [[333](#bib.bib333)],
    the authors divided the decoder into two cascaded neural decoders and trained
    the first decoder to improve the waterfall performance, while the second decoder
    was trained to handle the residual errors that are not corrected by the first
    decoder.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，许多实际的LDPC码表现出一个错误地板⁵⁵5对于需要极低BLER的超可靠低延迟通信等应用来说，减轻错误地板可能是至关重要的。由于LDPC码的错误地板通常归因于具有循环的因子图的迭代消息传递译码算法的次优性，[[332](#bib.bib332)]中的论文提出了神经NMS译码器的训练方法来消除LDPC码的错误地板。具体地，受到增强学习技术的启发[[333](#bib.bib333)]，作者将译码器分成两个级联的神经译码器，并训练第一个译码器以提高瀑布性能，而第二个译码器则被训练来处理第一个译码器无法纠正的剩余错误。
- en: IV-B3 Variants of Random Redundant Decoding
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 随机冗余译码的变种
- en: The \acrrd algorithm [[334](#bib.bib334)] and \acmbbp decoder [[335](#bib.bib335)]
    are other approaches to soft-decision decoding of short block codes based on a
    redundant PCM. \acmrrd [[250](#bib.bib250)] is an algorithm that attempts to benefit
    from both RRD and MBBP decoding, which make use of a permutation group (automorphism
    group) of codes and parallel iterative decoders, respectively.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: \acrrd 算法 [[334](#bib.bib334)] 和 \acmbbp 解码器 [[335](#bib.bib335)] 是其他基于冗余 PCM
    的短块码软判决解码方法。\acmrrd [[250](#bib.bib250)] 是一种试图从 RRD 和 MBBP 解码中受益的算法，前者利用了码的排列群（自同构群），后者则使用了并行迭代解码器。
- en: In [[269](#bib.bib269)], the authors proposed a \acncrd algorithm for \achdpc
    codes in order to improve the performance of RRD decoding. The NC-RD algorithm
    introduces two preprocessing steps to the RRD decoding. More specifically, the
    algorithm first classifies the variable nodes of the parity-check matrix by the
    $k$-median algorithm based on the number of shortest cycles associated with each
    variable node, and then generates a list of permutations of bit positions from
    the automorphism group based on the permutation reliability metrics. The authors
    further proposed the neural network-based NC-RD algorithm by unfolding the NC-RD
    decoding process and introducing trainable weights.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[269](#bib.bib269)] 中，作者提出了一种用于 \achdpc 码的 \acncrd 算法，以提高 RRD 解码的性能。NC-RD
    算法在 RRD 解码中引入了两个预处理步骤。更具体地说，该算法首先通过 $k$-中位数算法根据每个变量节点的最短循环数量对奇偶校验矩阵的变量节点进行分类，然后根据排列可靠性度量生成位位置的排列列表。作者进一步提出了基于神经网络的
    NC-RD 算法，通过展开 NC-RD 解码过程并引入可训练权重。
- en: 'In [[258](#bib.bib258)], the authors applied the concept of DL-based BP decoding
    to mRRD [[250](#bib.bib250)] by replacing the BP decoding blocks in the mRRD algorithm
    with the proposed RNN-based BP decoders. The resulting decoder structure is shown
    in Fig. [11](#S4.F11 "Figure 11 ‣ IV-B3 Variants of Random Redundant Decoding
    ‣ IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in
    Deep Learning for Channel Coding: A Survey"). The proposed RNN-based mRRD decoder
    has been demonstrated to achieve near maximum-likelihood performance with reasonable
    computational complexity.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '在 [[258](#bib.bib258)] 中，作者通过用提出的基于 RNN 的 BP 解码器替换 mRRD [[250](#bib.bib250)]
    算法中的 BP 解码块，将 DL 基于 BP 解码的概念应用于 mRRD。得到的解码器结构如图 [11](#S4.F11 "Figure 11 ‣ IV-B3
    Variants of Random Redundant Decoding ‣ IV-B DL-Aided BP Decoding ‣ IV DL for
    Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")
    所示。提出的基于 RNN 的 mRRD 解码器已被证明在合理的计算复杂度下实现了接近最大似然的性能。'
- en: '![Refer to caption](img/63b792ed288eb993bd21ec36a278556e.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/63b792ed288eb993bd21ec36a278556e.png)'
- en: 'Figure 11: The mRRD algorithm with RNN-based BP decoders in [[258](#bib.bib258)].'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：[[258](#bib.bib258)] 中的 mRRD 算法与基于 RNN 的 BP 解码器。
- en: IV-B4 Optimization-Based Decoding
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B4 基于优化的解码
- en: Optimization-based decoding is another research direction aimed at improving
    the performance of BP decoders. The origin of the optimization-based decoding
    goes back to the work by Feldman who introduced a \aclp formulation of decoding
    LDPC codes [[336](#bib.bib336)].
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 基于优化的解码是旨在改善 BP 解码器性能的另一个研究方向。基于优化的解码起源于 Feldman 的工作，他引入了 LDPC 码的 \aclp 公式 [[336](#bib.bib336)]。
- en: The LP decoder is based on the LP relaxation of the original maximum-likelihood
    decoding problem [[337](#bib.bib337)]. However, the LP decoder has higher computational
    complexity and worse error-correcting performance in the low SNR region compared
    with the BP decoder. In order to address the above drawbacks, the paper [[270](#bib.bib270)]
    proposed a trainable \acadmm-penalized decoder [[338](#bib.bib338)] by unfolding
    the ADMM iterations. It was demonstrated that the proposed decoder can outperform
    the conventional BP decoder in high SNR region with comparable execution time.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: LP 解码器基于原始最大似然解码问题的 LP 松弛 [[337](#bib.bib337)]。然而，与 BP 解码器相比，LP 解码器在低 SNR 区域具有更高的计算复杂性和较差的纠错性能。为了解决上述缺陷，论文
    [[270](#bib.bib270)] 提出了通过展开 ADMM 迭代的可训练 \acadmm-惩罚解码器 [[338](#bib.bib338)]。证明了该解码器在高
    SNR 区域能够超越传统的 BP 解码器，并具有相当的执行时间。
- en: Afterwords, the authors in [[271](#bib.bib271)] introduced a trainable projected
    gradient decoder for LDPC codes by unfolding the PGD algorithm and optimizing
    the parameters via backpropagation. The proposed decoder alternately performs
    the gradient and projection steps, where the former moves in the direction of
    the negative gradient of the objective function, while the latter maps the search
    point into a feasible region that nearly satisfies the optimization constraint.
    Also, in [[272](#bib.bib272)], the same authors proposed proximal decoding of
    LDPC codes based on the proximal gradient method [[339](#bib.bib339)], which is
    used for solving non-differentiable convex optimization problems.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，[[271](#bib.bib271)]中的作者通过展开PGD算法并通过反向传播优化参数，介绍了一种可训练的投影梯度解码器用于LDPC码。所提出的解码器交替执行梯度和投影步骤，其中前者朝目标函数的负梯度方向移动，而后者将搜索点映射到几乎满足优化约束的可行区域。此外，在[[272](#bib.bib272)]中，相同的作者提出了基于投影梯度方法[[339](#bib.bib339)]的LDPC码近端解码，这种方法用于解决不可微凸优化问题。
- en: IV-B5 RL-Based Approaches
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B5 基于RL的方法
- en: It is known that parallel BP decoders on independently permuted factor graphs
    can significantly improve the performance of single BP decoding for polar codes
    [[340](#bib.bib340), [341](#bib.bib341)]. In [[273](#bib.bib273)], the authors
    addressed the problem of selecting the permutations on the factor graph that lead
    to successful decoding given a channel observation. Specifically, they viewed
    the selection of permutations as a multi-armed bandit problem and proposed an
    RL-based CRC-aided BP decoder that attempts to select the best set of permutations.
    The proposed scheme was shown to achieve better performance than other approaches
    such as cyclically-shifted and random factor-graph permutations [[342](#bib.bib342),
    [340](#bib.bib340)].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，独立置换因子图上的并行BP解码器可以显著提高极化码单次BP解码的性能[[340](#bib.bib340)、[341](#bib.bib341)]。在[[273](#bib.bib273)]中，作者解决了选择因子图上的置换以在给定信道观察下实现成功解码的问题。具体来说，他们将置换选择视为多臂赌博机问题，并提出了一种基于RL的CRC辅助BP解码器，试图选择最佳的置换集合。提出的方案显示出比其他方法如循环移位和随机因子图置换[[342](#bib.bib342)、[340](#bib.bib340)]更好的性能。
- en: In [[274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)], the authors
    proposed a novel RL-based sequential BP decoding scheme to optimize the scheduling
    of CN clusters for moderate length LDPC codes⁶⁶6In contrast to the standard flooding
    scheduling where all CNs and VNs are updated simultaneously at each iteration,
    sequential BP decoding, or layered decoding, updates nodes or sets of nodes individually
    in sequence.. In the proposed scheme, $m$ CNs are divided into sets of $z$ CNs,
    called *cluster*, and the scheduling problem, i.e., cluster selection with $\lceil
    m/z\rceil$ possible actions, was optimized by Q-learning. Furthermore, they proposed
    novel meta-learning based sequential decoding schemes to quickly adapt to changing
    channel conditions due to fading in wireless scenarios. The RL-based scheduling
    of sequential BP decoding has also been proposed for generalized LDPC codes [[343](#bib.bib343)],
    where the authors showed that the proposed RL-based decoding scheme was shown
    to significantly outperform the standard BP flooding decoder, as well as a sequential
    decoder based on random scheduling with the smaller number of CN updates.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[274](#bib.bib274)、[275](#bib.bib275)、[276](#bib.bib276)]中，作者提出了一种基于RL的序列BP解码方案，以优化中等长度LDPC码的CN簇调度⁶⁶6与标准的洪泛调度不同，后者在每次迭代时同时更新所有CN和VN，序列BP解码或分层解码则是按顺序逐个更新节点或节点集合。在提出的方案中，$m$个CN被划分为$z$个CN的集合，称为*簇*，并通过Q学习优化调度问题，即具有$\lceil
    m/z\rceil$种可能操作的簇选择。此外，他们还提出了基于元学习的序列解码方案，以快速适应由于无线环境中的衰落而变化的信道条件。RL基础的序列BP解码调度也已被提出用于广义LDPC码[[343](#bib.bib343)]，其中作者显示，提出的RL基础解码方案在显著优于标准BP洪泛解码器以及基于随机调度的序列解码器（更新的CN数量较少）方面表现突出。
- en: IV-B6 Customized Loss Function
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B6 自定义损失函数
- en: 'As in Section [IV-A3](#S4.SS1.SSS3 "IV-A3 Syndrome-Based Loss Function ‣ IV-A
    Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), a syndrome loss function has been used for DL-based
    BP decoders. In [[277](#bib.bib277)], the authors proposed a soft syndrome as
    a loss function for training a neural BP decoder. Unlike the paper [[224](#bib.bib224)],
    which utilizes the hard syndrome as an input to the decoder, this paper introduced
    the *soft* syndrome which is defined similarly to the CN update rule in MS decoding,
    in addition to the conventional cross entropy loss function.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '如在章节 [IV-A3](#S4.SS1.SSS3 "IV-A3 Syndrome-Based Loss Function ‣ IV-A Model-Free
    Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel
    Coding: A Survey") 中所述，基于综合征的损失函数已被用于基于深度学习的BP解码器。在 [[277](#bib.bib277)] 中，作者提出了一种软综合征作为训练神经BP解码器的损失函数。与论文
    [[224](#bib.bib224)] 中使用硬综合征作为解码器输入不同，本论文引入了定义类似于MS解码中的CN更新规则的*软*综合征，此外还包括传统的交叉熵损失函数。'
- en: 'While the application of [[277](#bib.bib277)] was limited to decoders that
    output a soft estimate of the codeword, this is not the case for polar decoders
    that do not use a PCM. To address this, the authors in [[278](#bib.bib278)] proposed
    two modified syndrome losses: frozen-bit syndrome loss and CRC-enabled syndrome
    loss. The authors also introduced a syndrome-enabled blind equalizer based on
    the proposed syndrome loss, which does not require the transmission of training
    sequences.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 [[277](#bib.bib277)] 的应用仅限于输出软估计码字的解码器，但对于不使用PCM的极化解码器情况则不同。为此，[[278](#bib.bib278)]
    中的作者提出了两种修改的综合征损失：冻结比特综合征损失和CRC启用综合征损失。作者还引入了一种基于所提综合征损失的综合征启用盲均衡器，该均衡器不需要传输训练序列。
- en: Unlike the above syndrome-based approaches, the authors in [[279](#bib.bib279)]
    considered a linear combination of sparse node loss and knowledge distillation
    loss, in addition to the conventional cross entropy loss. Knowledge distillation
    is a technique in DL where one uses a teacher network to guide the training of
    a smaller student neural network [[344](#bib.bib344)]. Sparse node loss imposes
    a sparse constraint on the node activations based on the $L_{p}$ norm, whereas
    the knowledge distillation loss aims to mimic the teacher network, which was the
    standard MS decoder without trainable parameters, by transferring knowledge. It
    was shown that the proposed loss terms provide BER performance improvement of
    up to $1.1$ dB without increasing the runtime complexity and the model size.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述基于综合征的方法不同，[[279](#bib.bib279)] 中的作者考虑了稀疏节点损失和知识蒸馏损失的线性组合，此外还包括传统的交叉熵损失。知识蒸馏是一种深度学习技术，通过使用教师网络来指导较小学生神经网络的训练
    [[344](#bib.bib344)]。稀疏节点损失基于 $L_{p}$ 范数对节点激活施加稀疏约束，而知识蒸馏损失旨在通过转移知识来模拟教师网络，该教师网络是没有可训练参数的标准MS解码器。研究表明，所提出的损失项在不增加运行时间复杂性和模型大小的情况下提供了高达
    $1.1$ dB 的比特错误率（BER）性能提升。
- en: IV-B7 Memory and Complexity Reduction
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B7 内存和复杂性减少
- en: 'A neural network-based BP decoder introduces different weights (or scaling
    factors) to different edges in the Tanner graph, which can significantly increase
    the computational and space complexities of standard BP decoding. In fact, this
    issue has been studied extensively for generic DNNs, i.e., not limited to channel
    decoding applications, due to the large parameter sizes of modern DL models [[345](#bib.bib345)].
    A popular approach is neural network pruning [[346](#bib.bib346), [347](#bib.bib347),
    [348](#bib.bib348), [349](#bib.bib349), [350](#bib.bib350)], which aims to remove
    redundant parameters of an original network while preserving the accuracy. Parameter
    shaping is also an effective way to reduce parameters by sharing parameters between
    different neurons. Parameter shaping is typically exploited in CNN, where all
    neurons in a particular feature map share the same weight. Another popular approach
    is parameter quantization [[351](#bib.bib351), [352](#bib.bib352)]. These major
    approaches to addressing the complexity problem are illustrated in Fig. [12](#S4.F12
    "Figure 12 ‣ IV-B7 Memory and Complexity Reduction ‣ IV-B DL-Aided BP Decoding
    ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey").'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '基于神经网络的BP解码器为Tanner图中的不同边引入不同的权重（或缩放因子），这会显著增加标准BP解码的计算和空间复杂度。实际上，这个问题已经在通用DNN中得到了广泛研究，即不限于信道解码应用，因为现代DL模型的参数规模较大[[345](#bib.bib345)]。一种流行的方法是神经网络剪枝[[346](#bib.bib346),
    [347](#bib.bib347), [348](#bib.bib348), [349](#bib.bib349), [350](#bib.bib350)]，旨在去除原始网络中的冗余参数，同时保持准确性。参数塑形也是通过在不同神经元之间共享参数来减少参数的一种有效方式。参数塑形通常在CNN中应用，其中特定特征图中的所有神经元共享相同的权重。另一种流行的方法是参数量化[[351](#bib.bib351),
    [352](#bib.bib352)]。解决复杂性问题的这些主要方法在图[12](#S4.F12 "Figure 12 ‣ IV-B7 Memory and
    Complexity Reduction ‣ IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey")中进行了说明。'
- en: '![Refer to caption](img/b3cf195eaf3873ca3463d9e26b1639cb.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b3cf195eaf3873ca3463d9e26b1639cb.png)'
- en: (a) Quantization of weights.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 权重的量化。
- en: '![Refer to caption](img/0605e3ecd7c109cb2daddda44a541bd4.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0605e3ecd7c109cb2daddda44a541bd4.png)'
- en: (b) Sharing weights (connections with the same color have the same weight).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 共享权重（相同颜色的连接具有相同的权重）。
- en: '![Refer to caption](img/78c3c466ac97386661c65cb03bc4e1dc.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78c3c466ac97386661c65cb03bc4e1dc.png)'
- en: (c) Pruning neurons (pruned neurons are indicated by dashed circles).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 剪枝神经元（被剪枝的神经元由虚线圆圈标示）。
- en: 'Figure 12: Approaches to reducing computational and space complexities of a
    DL decoder.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：减少DL解码器计算和空间复杂度的方法。
- en: '(a) Quantization: In [[280](#bib.bib280)], the authors proposed a weight quantization
    mechanism for an RNN polar decoder. Specifically, they proposed a two-step approach,
    where floating-point weights are quantized into $2^{q}$ quantization levels and
    then they are further compressed into $2^{c}$ ($c<q$, $c,q\in\mathbb{N}$) quantization
    levels, which are the most commonly used among $2^{q}$ quantization levels. The
    quantization of the RNN polar decoder was also studied in [[281](#bib.bib281)],
    where the authors demonstrated that quantization after training leads to better
    performance compared to the case where quantization is applied after every epoch
    during training.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 量化：在[[280](#bib.bib280)]中，作者提出了一种用于RNN极化解码器的权重量化机制。具体而言，他们提出了一种两步方法，其中浮点权重被量化为$2^{q}$量化级别，然后进一步压缩为$2^{c}$（$c<q$，$c,q\in\mathbb{N}$）量化级别，这些级别在$2^{q}$量化级别中是最常用的。在[[281](#bib.bib281)]中也研究了RNN极化解码器的量化，作者证明了训练后的量化比在每个训练周期后进行量化的情况表现更好。
- en: Instead of quantizing DNN decoder parameters, several papers have investigated
    quantizing decoding messages or LLR values computed from channel observations.
    For example, in [[353](#bib.bib353)], the authors trained a parameterized quantization
    of LLR values that maximizes the performance of BP decoding. Similarly, the authors
    in [[354](#bib.bib354)] investigated the design of quantizers in an LDPC decoder
    that are used for quantizing both LLRs and iterative decoding messages. On the
    other hand, the authors in [[355](#bib.bib355)] proposed to train neural BP decoder
    for the system with one-bit quantizer.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文研究了量化解码消息或从信道观测中计算的LLR值，而不是量化DNN解码器参数。例如，在[[353](#bib.bib353)]中，作者训练了一种参数化LLR值量化，旨在最大化BP解码的性能。同样，[[354](#bib.bib354)]中的作者研究了用于量化LLR和迭代解码消息的LDPC解码器中的量化器设计。另一方面，[[355](#bib.bib355)]中的作者提出了为具有单比特量化器的系统训练神经BP解码器。
- en: Unlike existing studies that consider the AWGN channel, the authors in [[356](#bib.bib356),
    [282](#bib.bib282), [283](#bib.bib283)] considered BSC and proposed finite precision
    decoders, called \acfaid, for LDPC codes with \acrqnn. More specifically, they
    proposed the BER as the loss function to train the RQNNs over BSC by leveraging
    \acste [[357](#bib.bib357)] to overcome the issue of gradients vanishing caused
    by the low precision activations in the RQNN and quantization in the BER.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 与现有研究考虑AWGN通道不同，[[356](#bib.bib356)、[282](#bib.bib282)、[283](#bib.bib283)]中的作者考虑了BSC，并为LDPC码与\acrqnn提出了有限精度解码器，称为\acfaid。更具体地说，他们提出了将BER作为损失函数，通过利用\acste[[357](#bib.bib357)]来训练RQNNs，以解决RQNN中的低精度激活和BER中的量化所导致的梯度消失问题。
- en: In [[284](#bib.bib284)], the authors proposed a joint optimization of quantized
    message alphabets and quantization thresholds. Specifically, the authors utilized
    the softmax distribution [[358](#bib.bib358)] to make the quantization thresholds
    trainable by softening the one-hot distribution of the quantization. The proposed
    decoder was shown to outperform the original non-surjective FAIDs [[359](#bib.bib359)]
    in terms of error rate performance.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[284](#bib.bib284)]中，作者提出了量化消息字母和量化阈值的联合优化。具体而言，作者利用softmax分布[[358](#bib.bib358)]使量化阈值可训练，通过软化量化的一热分布来实现。所提出的解码器在误差率性能方面优于原始的非单射FAIDs[[359](#bib.bib359)]。
- en: '(b) Weight Sharing: In [[285](#bib.bib285), [286](#bib.bib286)], the authors
    proposed simple-scaling models for weighted BP decoding in [[257](#bib.bib257)]
    that share weights across edges, using only three scalar parameters per iteration:
    message weight, channel weight, and damping factor. The authors showed that such
    simple scaling models are often sufficient to achieve gains similar to the fully
    parameterized decoder.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 权重共享：在[[285](#bib.bib285)、[286](#bib.bib286)]中，作者提出了用于加权BP解码的简单缩放模型[[257](#bib.bib257)]，这些模型在边缘间共享权重，每次迭代只使用三个标量参数：消息权重、通道权重和阻尼因子。作者显示，这种简单的缩放模型通常足以达到与完全参数化解码器类似的增益。
- en: The authors in [[287](#bib.bib287), [288](#bib.bib288)] proposed a parameter
    sharing scheme for a neural NMS decoder that shares the same correction (normalization)
    factors in the same layer. In contrast, the authors in [[360](#bib.bib360), [361](#bib.bib361)]
    proposed a family of weight sharing schemes for a neural NMS decoder that uses
    the same weight for edges with the same check node degree and/or variable node
    degree. Similarly, the authors in [[289](#bib.bib289)] proposed a neural MS decoder
    for protograph LDPC codes where a bundle of edges derived from the same edge type
    share identical parameters. Due to the lifting structure of protograph LDPC codes,
    the same set of parameters can be employed for multiple codes derived from the
    same base code.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[287](#bib.bib287)、[288](#bib.bib288)]中，作者提出了一种参数共享方案，用于神经NMS解码器，该方案在同一层中共享相同的修正（归一化）因子。相比之下，在[[360](#bib.bib360)、[361](#bib.bib361)]中，作者提出了一系列权重共享方案，用于神经NMS解码器，该方案对具有相同检查节点度和/或变量节点度的边缘使用相同的权重。同样，在[[289](#bib.bib289)]中，作者提出了一种神经MS解码器，用于原型LDPC码，其中一组源自相同边缘类型的边共享相同的参数。由于原型LDPC码的提升结构，相同的参数集可以用于从相同基本码导出的多个码。
- en: In [[290](#bib.bib290)], the authors applied the \actt decomposition [[362](#bib.bib362)]
    to a neural NMS decoder, where it decomposes a high-order tensor into several
    low-order tensors. This not only reduces the number of weight parameters, but
    also the number of multiplications required in the CN and VN updates. Furthermore,
    in [[291](#bib.bib291)], the same authors proposed \actr decomposition [[363](#bib.bib363)]
    combined with weight sharing to further reduce the storage and computational complexity.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[290](#bib.bib290)]中，作者将\actt分解[[362](#bib.bib362)]应用于神经NMS解码器，该分解将高阶张量分解为几个低阶张量。这不仅减少了权重参数的数量，还减少了CN和VN更新所需的乘法次数。此外，在[[291](#bib.bib291)]中，同一作者提出了结合权重共享的\actr分解[[363](#bib.bib363)]，以进一步降低存储和计算复杂性。
- en: In [[292](#bib.bib292)], a weight sharing scheme was proposed for a neural BP
    or MS decoder to adapt to multiple code rates with a reasonable amount of parameters.
    Specifically, instead of training different decoders, they proposed to train a
    single rate-compatible decoder based on multi-task learning, where different parts
    of the parameters are activated to handle different code rates.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[292](#bib.bib292)]中，提出了一种权重共享方案用于神经BP或MS解码器，以适应多个码率，并保持参数数量合理。具体来说，他们提出了基于多任务学习的单一速率兼容解码器训练方案，而不是训练不同的解码器，其中不同部分的参数被激活以处理不同的码率。
- en: '(c) Pruning: In [[293](#bib.bib293)], the authors proposed a novel pruning-based
    neural BP decoder for short linear block codes. The key idea was to prune unimportant
    CNs with small weights of an overcomplete PCM. Similarly, in [[294](#bib.bib294)],
    the same authors proposed a neural BP with decimation [[364](#bib.bib364)] for
    LDPC codes. In particular, they identified the least reliable VN with the aid
    of DL, i.e., the VN with the lowest absolute *a posteriori* LLR, and then decimated
    it to $\pm\infty$. It has been demonstrated that the proposed decoder with decimation
    can significantly outperform the conventional neural BP decoder.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 剪枝：在[[293](#bib.bib293)]中，作者提出了一种新颖的基于剪枝的神经BP解码器，用于短线性块码。关键思想是剪除具有小权重的过完备PCM中不重要的CN。同样，在[[294](#bib.bib294)]中，相同的作者提出了用于LDPC码的剪枝神经BP[[364](#bib.bib364)]。特别是，他们借助深度学习（DL）识别了最不可靠的VN，即具有最低绝对值的*a
    posteriori* LLR，然后将其剪除到$\pm\infty$。已证明，具有剪枝的提出的解码器可以显著优于传统的神经BP解码器。
- en: IV-B8 GNN Decoders
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B8 GNN 解码器
- en: In general, BP computes the optimal (posterior) marginal probability distributions
    only for a non-loopy graphical model, and in practice it often computes a poor
    approximation of the true distribution. To tackle this limitation, the authors
    in [[295](#bib.bib295)] extended the standard GNN equations to factor graphs and
    presented a hybrid inference model that combines messages from BP and from GNN,
    where the GNN messages are learned to complement the BP messages.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，BP仅对无环图模型计算最佳（后验）边际概率分布，在实践中，它往往只能计算出真实分布的较差近似。为了解决这一限制，[[295](#bib.bib295)]中的作者将标准GNN方程扩展到因子图，并提出了一种混合推断模型，结合了来自BP和GNN的消息，其中GNN消息被学习以补充BP消息。
- en: 'Instead of the method in [[295](#bib.bib295)], which extends BP decoding by
    combining it with a GNN, the authors in [[296](#bib.bib296)] proposed a fully
    GNN-based decoder. In contrast to weighted BP decoding, they introduced two types
    of MPNN-based trainable message update functions: *the edge message update functions*
    and *the node update functions*. Independently, an edge-weighted GNN decoder has
    been proposed in [[297](#bib.bib297)]. In the proposed decoder, they applied an
    MPNN for updating messages and assigned a trainable weight to each edge message,
    which is optimized by a fully-connected feed-forward neural network, i.e., MLP.
    The major advantage of these GNN decoders over the standard neural BP decoder
    is that the number of trainable parameters is not affected by the code length.
    Therefore, after training, the trained decoder can be applied to codes with different
    rates and lengths without retraining.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 与[[295](#bib.bib295)]中将BP解码与GNN结合的方法不同，[[296](#bib.bib296)]中的作者提出了一种完全基于GNN的解码器。与加权BP解码不同，他们引入了两种基于MPNN的可训练消息更新函数：*边消息更新函数*和*节点更新函数*。此外，[[297](#bib.bib297)]中提出了一种边加权GNN解码器。在所提出的解码器中，他们应用了MPNN来更新消息，并为每个边消息分配了一个可训练的权重，这些权重由全连接前馈神经网络，即MLP优化。这些GNN解码器相较于标准神经BP解码器的主要优势在于，训练参数的数量不受码长的影响。因此，经过训练后，训练好的解码器可以应用于不同码率和长度的码而无需重新训练。
- en: IV-B9 Understanding Neural BP Decoders
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B9 理解神经BP解码器
- en: In [[298](#bib.bib298)], the authors empirically showed how the learned weights
    mitigate the effect of short cycles in Tanner graphs to improve the reliability
    of the posterior LLRs and contribute to the robustness of the decoders across
    channels. The authors also introduced an analytical approach for finding the weights
    using GA and compared the neural MS decoders, showing that for complicated fading
    channels, the neural network-based weight optimization leads to better performance
    than the GA-based optimization.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[298](#bib.bib298)]中，作者实证展示了学习到的权重如何减轻Tanner图中短周期的影响，以提高后验LLRs的可靠性，并增强解码器在信道中的鲁棒性。作者还介绍了一种使用GA寻找权重的分析方法，并比较了神经MS解码器，显示出对于复杂的衰落信道，基于神经网络的权重优化比基于GA的优化性能更好。
- en: The authors in [[299](#bib.bib299)] theoretically investigated the generalization
    capabilities [[365](#bib.bib365)] of neural BP decoders, i.e., the difference
    between empirical and expected BERs. The paper presented new theoretical results
    that bound the gap and showed its dependence on the decoder complexity, in terms
    of code parameters (such as message/code lengths, VN/CN degrees), decoding iterations,
    and the training dataset size. They empirically observed that the generalization
    gap increases with decoding iterations and code length, and decays with the training
    dataset size, supporting the theoretical results in their paper.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[299](#bib.bib299)]中，作者理论上研究了神经BP解码器的泛化能力[[365](#bib.bib365)]，即经验BER与期望BER之间的差异。论文提出了新的理论结果，给出了差距的界限，并展示了其对解码器复杂度的依赖，包括码参数（如消息/码字长度、VN/CN度数）、解码迭代次数和训练数据集大小。他们通过实证观察到，泛化差距随着解码迭代次数和码字长度增加而增加，并随着训练数据集大小增加而减少，支持了他们论文中的理论结果。
- en: IV-B10 Other Approaches
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B10 其他方法
- en: To improve the decoding of short Raptor-like LDPC codes, the authors in [[366](#bib.bib366)]
    considered multi-round BP decoding with impulsive perturbation [[367](#bib.bib367)].
    Perturbation is a process of making a small intentional change in the received
    signal, and this scheme iteratively performs conventional BP decoding and perturbation
    until a valid codeword is found. In [[302](#bib.bib302)], the authors proposed
    a neural network based perturbation symbol selection scheme where the symbols
    to be perturbed are selected from a pre-trained neural network and showed that
    the proposed scheme performs better than existing schemes such as [[366](#bib.bib366)]
    for Raptor-like LDPC codes.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改进短Raptor-like LDPC码的解码，在[[366](#bib.bib366)]中，作者考虑了带有冲击性扰动的多轮BP解码[[367](#bib.bib367)]。扰动是对接收信号进行小的有意改变的过程，这种方案迭代地执行常规BP解码和扰动，直到找到有效的码字。在[[302](#bib.bib302)]中，作者提出了一种基于神经网络的扰动符号选择方案，其中需要扰动的符号是从预训练的神经网络中选择的，并展示了该方案在Raptor-like
    LDPC码上表现优于现有方案，如[[366](#bib.bib366)]。
- en: The performance of a standalone neural BP decoder could be further enhanced
    by jointly optimizing signal detection and decoding. For instance, in [[301](#bib.bib301)],
    iterative signal detection and decoding via deep unfolding was proposed for MU-MIMO-OFDM.
    In [[300](#bib.bib300)], the authors proposed GNN-based joint detection and decoding
    for \acisi channels.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 通过联合优化信号检测和解码，可以进一步提升独立神经BP解码器的性能。例如，在[[301](#bib.bib301)]中，提出了通过深度展开的迭代信号检测和解码方法用于MU-MIMO-OFDM。在[[300](#bib.bib300)]中，作者提出了基于GNN的联合检测和解码方案用于\acisi信道。
- en: 'Besides the approaches introduced in Section [IV-B7](#S4.SS2.SSS7 "IV-B7 Memory
    and Complexity Reduction ‣ IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"), the authors
    in [[303](#bib.bib303)] proposed another approach to alleviate decoding complexity
    and latency. Specifically, they proposed a DL approach for detecting the decodable
    codewords and predicting the iteration number from the received signal to reduce
    the decoding delay. This could potentially be useful for early feedback prediction
    in \acharq. Furthermore, in [[368](#bib.bib368)], the authors accelerated neural
    BP decoding through coded distributed computing [[369](#bib.bib369)]. In particular,
    they reformulated the neural BP decoding operations as matrix-vectors to facilitate
    distributed parallel decoding.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在[IV-B7](#S4.SS2.SSS7 "IV-B7 内存和复杂度减少 ‣ IV-B 深度学习辅助BP解码 ‣ IV 深度学习用于信道解码 ‣
    深度学习在信道编码中的最新进展：综述")部分介绍的方法外，[[303](#bib.bib303)]中的作者提出了另一种方法来缓解解码复杂度和延迟。具体而言，他们提出了一种DL方法，用于从接收到的信号中检测可解码的码字并预测迭代次数，以减少解码延迟。这可能对\acharq中的早期反馈预测有用。此外，在[[368](#bib.bib368)]中，作者通过编码分布式计算
    [[369](#bib.bib369)] 加速了神经BP解码。特别是，他们将神经BP解码操作重新表述为矩阵-向量形式，以便于分布式并行解码。
- en: In addition to communication systems, DL-based decoders have also been studied
    for storage systems. In [[242](#bib.bib242)], the authors proposed DL-based decoder
    for \acsttmram [[370](#bib.bib370)]. In order to adapt to the process variation
    and unknown offset of the resistance caused by the change in working temperature,
    the authors proposed an adaptive decoding scheme based on the three DNN decoders,
    i.e., BF, MS, and BP decoders, which share the same DNN architecture but have
    different weights. In [[241](#bib.bib241)], the same research group proposed a
    neural normalized offset \acrbms decoding algorithm for STT-MRAM by introducing
    trainable parameters to the RBMS algorithm [[371](#bib.bib371)]. It has been demonstrated
    that the proposed scheme can outperform the RBMS algorithm over the STT-MRAM channel,
    while maintaining similar decoder structure and time complexity of the standard
    RBMS decoder.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通信系统，基于深度学习的解码器也在存储系统中进行了研究。在[[242](#bib.bib242)]中，作者提出了基于深度学习的\acsttmram
    [[370](#bib.bib370)]解码器。为了适应由工作温度变化引起的工艺变化和电阻未知偏移，作者提出了一种自适应解码方案，该方案基于三个DNN解码器，即BF、MS和BP解码器，具有相同的DNN架构但权重不同。在[[241](#bib.bib241)]中，相同的研究小组通过将可训练参数引入RBMS算法
    [[371](#bib.bib371)]，提出了一种神经归一化偏移\acrbms解码算法。已证明，所提方案在STT-MRAM信道上表现优于RBMS算法，同时保持了与标准RBMS解码器相似的解码器结构和时间复杂度。
- en: Neural network-based BP decoding has been studied not only for classical error-correcting
    codes, but also for quantum error-correcting codes. For example, neural BP decoding
    has been applied to quantum LDPC codes [[372](#bib.bib372)] for which standard
    BP decoding may be insufficient due to the error degeneracy feature of quantum
    error-correcting codes [[373](#bib.bib373)]. By designing the loss function to
    account for error degeneracy, the decoding accuracy was improved up to three orders
    of magnitude compared to the standard BP decoder without training. Neural BP decoding
    for quantum LDPC codes was also studied in [[374](#bib.bib374), [375](#bib.bib375)].
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 基于神经网络的BP解码不仅针对经典纠错码进行研究，也用于量子纠错码。例如，神经BP解码已应用于量子LDPC码 [[372](#bib.bib372)]，由于量子纠错码的错误退化特性，标准BP解码可能不够充分
    [[373](#bib.bib373)]。通过设计考虑错误退化的损失函数，解码准确性相比于没有训练的标准BP解码器提高了三个数量级。量子LDPC码的神经BP解码也在
    [[374](#bib.bib374), [375](#bib.bib375)] 中进行了研究。
- en: IV-C DL-Aided Decoding of Polar Codes
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 深度学习辅助的极化码解码
- en: 'TABLE V: Summary of DL-aided polar decoders.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 深度学习辅助极化解码器的总结。'
- en: '| Category | Reference | Main Contribution |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 参考文献 | 主要贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Neural Network-Based SC Decoding | Cammerer et al. [[376](#bib.bib376), [377](#bib.bib377)]
    | Partitioned neural network decoders. |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 基于神经网络的SC解码 | Cammerer等人 [[376](#bib.bib376), [377](#bib.bib377)] | 分区神经网络解码器。
    |'
- en: '| Doan et al. [[378](#bib.bib378)] | Neural successive cancellation (NSC) decoding.
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Doan等人 [[378](#bib.bib378)] | 神经连续取消 (NSC) 解码。 |'
- en: '| Wodiany et al. [[379](#bib.bib379)] | Efficient implementation of an low-precision
    NSC decoder. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Wodiany等人 [[379](#bib.bib379)] | 低精度NSC解码器的高效实现。 |'
- en: '| Hebbar et al. [[380](#bib.bib380)] | Novel curriculum learning-based sequential
    neural decoder. |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Hebbar等人 [[380](#bib.bib380)] | 新颖的基于课程学习的序列神经解码器。 |'
- en: '| DL-Aided SCF Decoding | Wang et al. [[381](#bib.bib381)] | LSTM network that
    estimates the first erroneous bit. |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| DL 辅助 SCF 解码 | Wang et al. [[381](#bib.bib381)] | 估计第一个错误比特的 LSTM 网络。 |'
- en: '| He et al. [[382](#bib.bib382)] | LSTM-based identification of erroneous bits
    for DSCF decoding. |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| He et al. [[382](#bib.bib382)] | 基于 LSTM 的 DSCF 解码中的错误比特识别。 |'
- en: '| Doan et al. [[383](#bib.bib383), [384](#bib.bib384)] | Neural DSCF with trainable
    bit-flipping metric. |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| Doan et al. [[383](#bib.bib383), [384](#bib.bib384)] | 带有可训练比特翻转度量的神经 DSCF。
    |'
- en: '| Wang et al. [[385](#bib.bib385)] | Q-learning-assisted SCF decoding algorithm.
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[385](#bib.bib385)] | Q 学习辅助的 SCF 解码算法。 |'
- en: '| Doan et al. [[386](#bib.bib386)] | RL-based bit-flipping strategy for fast
    SC decoding. |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| Doan et al. [[386](#bib.bib386)] | 基于 RL 的快速 SC 解码比特翻转策略。 |'
- en: '| DL-Aided SCLF Decoding | Hashemi et al. [[387](#bib.bib387)] | Trainable
    bit-flipping metric for SCL decoding. |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| DL 辅助 SCLF 解码 | Hashemi et al. [[387](#bib.bib387)] | 可训练的比特翻转度量用于 SCL 解码。
    |'
- en: '| Doan et al. [[388](#bib.bib388)] | FSCLF decoding algorithm. |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| Doan et al. [[388](#bib.bib388)] | FSCLF 解码算法。 |'
- en: '| Chen et al. [[389](#bib.bib389)] | LSTM-assisted bit-flipping algorithm for
    a CA-SCL decoder. |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. [[389](#bib.bib389)] | 用于 CA-SCL 解码器的 LSTM 辅助比特翻转算法。 |'
- en: '| Tao et al. [[390](#bib.bib390)] | New flip algorithm based on DNC. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| Tao et al. [[390](#bib.bib390)] | 基于 DNC 的新翻转算法。 |'
- en: '| Liang et al. [[391](#bib.bib391)] | Stacked LSTM to improve the accuracy
    of erroneous bit prediction. |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| Liang et al. [[391](#bib.bib391)] | 堆叠 LSTM 以提高错误比特预测的准确性。 |'
- en: '| Li et al. [[392](#bib.bib392)] | Approximated bit-flipping metric for DSCLF
    decoding. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[392](#bib.bib392)] | DSCLF 解码的近似比特翻转度量。 |'
- en: '| Other Approaches | Lu et al. [[393](#bib.bib393)] | DL-aided shifting metric
    for SCL decoding. |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 其他方法 | Lu et al. [[393](#bib.bib393)] | DL 辅助的 SCL 解码的平移度量。 |'
- en: '| Liu et al. [[394](#bib.bib394)] | CRC error-correction aided SCL decoding.
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. [[394](#bib.bib394)] | CRC 错误校正辅助 SCL 解码。 |'
- en: 'Model-free decoders in Section [IV-A](#S4.SS1 "IV-A Model-Free Decoders ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey") as well as neural BP decoders in Section [IV-B](#S4.SS2 "IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey") are easily applicable to polar codes. In the following,
    we also review methods for designing model-free decoders that take the specific
    code structure into account. Furthermore, we focus on DL approaches that augment
    conventional SC or SCL decoders, instead of replacing them with a DNN. In Table [V](#S4.T5
    "TABLE V ‣ IV-C DL-Aided Decoding of Polar Codes ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"), we provide
    the summary of these methods.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '第[IV-A](#S4.SS1 "IV-A Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey")节中的无模型解码器以及第[IV-B](#S4.SS2
    "IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey")节中的神经 BP 解码器均可轻松应用于极化码。接下来，我们还将回顾考虑特定码结构的无模型解码器设计方法。此外，我们重点关注增强传统
    SC 或 SCL 解码器的 DL 方法，而不是用 DNN 替代它们。在表[V](#S4.T5 "TABLE V ‣ IV-C DL-Aided Decoding
    of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey")中，我们提供了这些方法的总结。 |'
- en: IV-C1 Neural Network-Based SC Decoding
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 基于神经网络的 SC 解码
- en: Although the straightforward application of DNN is a viable option for decoding
    polar codes as in [[215](#bib.bib215)], the major issue was the exponential growth
    of training complexity. In [[376](#bib.bib376)], the authors addressed this issue
    by introducing \acpnn decoders. More specifically, inspired by the simplified
    successive cancellation algorithm in [[395](#bib.bib395)], which divides the decoding
    tree into \acpspc and \acprc), they replaced the SPC and RC subdecoders by neural
    networks. Simulations showed that the PNN decoder achieves similar BER performance
    to the SC and BP decoders for short lengths, such as $128$ bits, with potentially
    much lower latency. A similar concept of partitioning a neural network-based decoder
    for polar codes was also investigated in [[377](#bib.bib377)]. Furthermore, in
    order to reduce the latency of PNN, \acnsc decoding of polar codes was proposed
    in [[378](#bib.bib378)], where multiple constituent neural network decoders are
    incorporated into SC decoding, and its efficient implementation based on a low-precision
    neural network decoder was studied in [[379](#bib.bib379)].
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管直接应用DNN作为解码极化码的选项如[[215](#bib.bib215)]所示，但主要问题是训练复杂度的指数增长。在[[376](#bib.bib376)]中，作者通过引入\acpnn解码器解决了这一问题。具体来说，受到[[395](#bib.bib395)]中简化的连续取消算法的启发，该算法将解码树分为\acpspc和\acprc，他们用神经网络替代了SPC和RC子解码器。模拟结果表明，PNN解码器在短长度（如$128$位）上，表现出与SC和BP解码器类似的BER性能，并且潜在地具有更低的延迟。类似的基于神经网络的极化码解码器分区概念也在[[377](#bib.bib377)]中进行了研究。此外，为了减少PNN的延迟，[[378](#bib.bib378)]提出了极化码的\acnsc解码，其中将多个子神经网络解码器整合到SC解码中，并且其基于低精度神经网络解码器的高效实现也在[[379](#bib.bib379)]中进行了研究。
- en: Recently, another approach to tackle the difficulty of learning to decode long
    polar codes was proposed in [[380](#bib.bib380)], where a novel curriculum learning-based
    sequential neural decoder for polar and PAC codes was proposed. The paper designed
    a novel curriculum to train RNN, where the problem of joint estimation of information
    bits is decomposed into a sequence of sub-problems of increasing difficulty. The
    proposed decoder was shown to achieve better BER performance than the conventional
    supervised training without curriculum and standard SC decoding.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在[[380](#bib.bib380)]中提出了另一种解决学习解码长极化码难题的方法，提出了一种新颖的基于课程学习的序列神经解码器，用于极化码和PAC码。该论文设计了一种新型课程来训练RNN，将信息比特的联合估计问题分解为一系列逐渐增加难度的子问题。提出的解码器被证明在BER性能上优于传统的无课程监督训练和标准SC解码。
- en: Instead of completely replacing a conventional decoder with DNNs, DL-based approaches
    that support conventional decoders such as SC and SCL decoding have been extensively
    studied. Among them, DL-assisted \acscf decoding [[396](#bib.bib396)] is one of
    the most popular approaches, which will be reviewed in the following.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 与其完全用DNN替换传统解码器，不如支持传统解码器的DL-based方法如SC和SCL解码已被广泛研究。其中，DL辅助的\acscf解码[[396](#bib.bib396)]是最受欢迎的方法之一，以下将对其进行回顾。
- en: IV-C2 DL-Aided SC Flip Decoding
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 DL辅助的SC翻转解码
- en: 'Despite its low-complexity, the error correcting performance of SC decoding
    at finite block lengths is not comparable to other modern codes such as LDPC codes.
    In order to improve the finite block length performance, SCF decoding has been
    proposed in [[396](#bib.bib396)] inspired by the fact that the first erroneous
    bit decision in SC decoding has a detrimental impact on the resulting error rate.
    The SCF decoder first performs standard SC decoding to generate a first estimated
    codeword, and if the codeword passes the CRC, decoding is complete. If the CRC
    check fails, the SCF decoding makes $T$ additional attempts to identify the first
    error in the codeword. In each attempt, a single estimated codeword bit is flipped
    with respect to the initial decision. The algorithm terminates when a valid codeword
    has been found or when all $T$ attempts have been made. The SCF decoding procedures
    is shown in Fig. [13](#S4.F13 "Figure 13 ‣ IV-C2 DL-Aided SC Flip Decoding ‣ IV-C
    DL-Aided Decoding of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). SCF decoding retains the $O(N)$
    memory complexity of the original SC algorithm and has an average computational
    complexity that is practically $O(N\log N)$ at high SNR, while still providing
    a significant gain in terms of error correcting performance. While SCF decoding
    is limited to correcting a single erroneous bit in the codeword, \acdscf decoding
    [[397](#bib.bib397), [398](#bib.bib398)] is a generalization of SCF-based decoding
    that is able to correct higher-order erroneous information bits by dynamically
    updating the set of flipping bit indices after each decoding attempt.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 SC 解码具有较低的复杂性，但在有限块长度下，其纠错性能与其他现代编码（如 LDPC 码）相比不可同日而语。为了提高有限块长度下的性能，SCF 解码被提出
    [[396](#bib.bib396)]，其灵感来源于 SC 解码中第一个错误比特决策对结果误码率的负面影响。SCF 解码器首先执行标准 SC 解码以生成第一个估计码字，如果码字通过
    CRC 校验，则解码完成。如果 CRC 校验失败，SCF 解码会进行 $T$ 次额外尝试，以识别码字中的第一个错误。在每次尝试中，单个估计码字比特会相对于初始决策进行翻转。该算法在找到有效码字或所有
    $T$ 次尝试完成时终止。SCF 解码过程如图 [13](#S4.F13 "图 13 ‣ IV-C2 DL 辅助 SC 翻转解码 ‣ IV-C 极化码的 DL
    辅助解码 ‣ IV DL 用于信道解码 ‣ 深度学习在信道编码中的最新进展：综述") 所示。SCF 解码保持了原始 SC 算法的 $O(N)$ 内存复杂性，并且在高
    SNR 下具有实际的 $O(N\log N)$ 平均计算复杂性，同时在误差纠正性能方面仍提供了显著的提升。尽管 SCF 解码仅限于纠正码字中的单个错误比特，\acdscf
    解码 [[397](#bib.bib397), [398](#bib.bib398)] 是基于 SCF 的解码的一种推广，能够通过在每次解码尝试后动态更新翻转比特索引集来纠正更高阶的错误信息比特。
- en: '![Refer to caption](img/81452ac3d2b77383867938ba07a64ec8.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/81452ac3d2b77383867938ba07a64ec8.png)'
- en: 'Figure 13: Flowchart of an SCF decoding framework with the number of trials
    $T$.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：带有试验次数 $T$ 的 SCF 解码框架流程图。
- en: The common challenge in SCF and DSCF decoding is how to identify the first error
    bit that causes error propagation. In the original work [[396](#bib.bib396)],
    the estimated codeword bits with the smallest amplitudes of LLR are flipped, but
    they are not necessarily the first errors. In fact, the optimal bit flipping strategy
    is still an open problem due to the lack of a rigorous mathematical characterization.
    Furthermore, DSCF decoding requires expensive exponential and logarithmic computations
    to compute the BF metric, which is used to determine the bit flipping position.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: SCF 和 DSCF 解码的共同挑战在于如何识别导致错误传播的第一个错误比特。在原始工作 [[396](#bib.bib396)] 中，翻转的是 LLR
    值最小的估计码字比特，但这些比特不一定是第一个错误。事实上，由于缺乏严格的数学表征，最佳的比特翻转策略仍然是一个未解决的问题。此外，DSCF 解码需要昂贵的指数和对数计算来计算
    BF 度量，该度量用于确定比特翻转位置。
- en: A popular DL-based solution is to train an LSTM network to estimate the first
    erroneous bit to be flipped. The authors in [[381](#bib.bib381)] have proposed
    an LSTM network for SCF decoding that takes an LLR sequence of the previous SC
    decoding attempt and outputs a vector where each element corresponds to the probability
    that a bit is the first error. Furthermore, the authors proposed a two-step training
    method that combines supervised learning with RL to train the LSTM to reverse
    previous incorrect flips. Similarly, the authors in [[382](#bib.bib382)] proposed
    an LSTM-based error bit identification for DSCF decoding where the network is
    trained to identify the first erroneous bit and additional erroneous bits by supervised
    learning and RL, respectively.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的基于 DL 的解决方案是训练 LSTM 网络来估计需要翻转的第一个错误位。[[381](#bib.bib381)]中的作者提出了一种用于 SCF
    解码的 LSTM 网络，该网络接受先前 SC 解码尝试的 LLR 序列，并输出一个向量，其中每个元素对应于某个位是第一个错误的概率。此外，作者提出了一种将监督学习与
    RL 结合的两步训练方法，以训练 LSTM 纠正之前的错误翻转。类似地，[[382](#bib.bib382)]中的作者提出了一种基于 LSTM 的错误位识别方法用于
    DSCF 解码，其中网络通过监督学习和 RL 分别训练识别第一个错误位和额外错误位。
- en: There are other learning approaches that do not rely on model-free DNNs. In
    [[383](#bib.bib383), [384](#bib.bib384)], the authors proposed neural DSCF decoding
    where they properly approximated and introduced trainable parameters to the BF
    metric and optimized its parameter by RMSProp, a variant of the SGD optimization
    technique. In [[385](#bib.bib385)], the authors proposed a Q-learning-assisted
    SCF decoding algorithm that selects the candidate flipping bits through the learned
    Q-table instead of metric sorting. It was demonstrated that the proposed decoding
    algorithm is particularly effective in reducing the decoding delay caused by sorting
    during the decoding process without sacrificing performance. Similarly, in [[386](#bib.bib386)],
    an RL-based BF strategy is also investigated for fast SC decoding of polar codes
    [[399](#bib.bib399)], where the authors developed a new parameterized BF model
    based on [[387](#bib.bib387)] and optimized the trainable parameters using the
    policy gradient method.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他学习方法并不依赖于无模型的 DNN。在[[383](#bib.bib383), [384](#bib.bib384)]中，作者提出了神经 DSCF
    解码方法，他们对 BF 度量进行了适当的逼近，并引入了可训练的参数，并通过 RMSProp（一种 SGD 优化技术的变体）优化其参数。在[[385](#bib.bib385)]中，作者提出了一种
    Q 学习辅助的 SCF 解码算法，该算法通过学习的 Q 表选择候选翻转位，而不是通过度量排序。实验表明，所提出的解码算法在不牺牲性能的情况下特别有效地减少了由排序引起的解码延迟。类似地，在[[386](#bib.bib386)]中，针对极化码的快速
    SC 解码也研究了一种基于 RL 的 BF 策略[[399](#bib.bib399)]，作者开发了一个基于[[387](#bib.bib387)]的新参数化
    BF 模型，并使用策略梯度方法优化了可训练的参数。
- en: IV-C3 DL-Aided SCL Flip Decoding
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 DL 辅助的 SCL 翻转解码
- en: The DL-aided BF mechanism can be applied not only to SC decoding but also to
    SCL decoding to further improve the performance.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: DL 辅助的 BF 机制不仅可以应用于 SC 解码，还可以应用于 SCL 解码，以进一步提高性能。
- en: In [[387](#bib.bib387)], the authors proposed the BF metric for SCL decoding,
    which is expressed by a trainable correlation matrix representing the likelihood
    of each decoded bit. They optimized the trainable matrix using the RMSprop optimizer
    and demonstrated that compared to the conventional metric in [[397](#bib.bib397)],
    the proposed BF metric significantly reduces computational complexity associated
    with the bit metric calculation while maintaining similar error rate performance.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[387](#bib.bib387)]中，作者提出了用于 SCL 解码的 BF 度量，该度量由一个训练的相关矩阵表示每个位被解码的可能性。他们使用
    RMSprop 优化器优化了该训练矩阵，并证明与[[397](#bib.bib397)]中的传统度量相比，所提出的 BF 度量显著减少了与位度量计算相关的计算复杂性，同时保持了类似的错误率性能。
- en: In [[388](#bib.bib388)], the authors applied BF to \acfscl decoding [[400](#bib.bib400),
    [401](#bib.bib401)], referred to as the \acfsclf decoding algorithm, to address
    the high latency problem associated with the \acsclf decoding algorithm. Specifically,
    the authors introduced a BF strategy tailored to FSCL decoding that avoids tree-traversal
    in the binary tree representation of SCLF to reduce the latency of the decoding
    process, and then derived a path selection error metric with a trainable parameter.
    The proposed decoder was shown to significantly reduce the average decoding latency,
    average complexity, and memory consumption of the SCLF decoder at the cost of
    slight degradation in error rate performance.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[388](#bib.bib388)]中，作者将BF应用于\acfscl解码[[400](#bib.bib400), [401](#bib.bib401)]，称之为\acfsclf解码算法，以解决与\acsclf解码算法相关的高延迟问题。具体而言，作者引入了一种专门针对FSCL解码的BF策略，该策略避免了在SCLF的二叉树表示中进行树遍历，从而减少了解码过程的延迟，并提出了一个带有可训练参数的路径选择误差度量。实验结果表明，所提解码器显著降低了SCLF解码器的平均解码延迟、平均复杂度和内存消耗，代价是误差率性能略有下降。
- en: In contrast to the above approaches that do not utilize a DNN, several papers
    have proposed a neural network-based selection of the flipping bit position. For
    instance, in [[389](#bib.bib389)], an LSTM-assisted BF algorithm has been proposed
    for a CA-SCL decoder. Furthermore, the authors have used the domain knowledge
    to reduce the complexity and memory requirements and computational complexity
    for efficient hardware implementation.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 与以上不利用DNN的方法相比，几篇论文提出了基于神经网络的翻转比特位置选择。例如，在[[389](#bib.bib389)]中，提出了一种LSTM辅助的BF算法用于CA-SCL解码器。此外，作者还利用领域知识减少了复杂度和内存要求，并降低了高效硬件实现的计算复杂度。
- en: The authors in [[390](#bib.bib390)] proposed a new flip algorithm using \acdnc
    [[402](#bib.bib402)], which can be considered as an LSTM augmented with an external
    memory through attention-based soft read and write mechanisms. The proposed decoding
    algorithm is a two-phase decoding assisted by the two DNCs, i.e., flip DNC and
    flip-validate DNC. The former ranks flip positions for multi-bit flipping, while
    the latter is used to re-select flip positions when decoding fails. Simulation
    results show that the proposed DNC-aided SCLF achieves better error rate performance
    and reduction in the number of flipping attempts compared to the LSTM-based algorithms.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[390](#bib.bib390)]中，作者提出了一种新的翻转算法，使用\acdnc [[402](#bib.bib402)]，可以看作是通过基于注意力的软读写机制增强的外部内存的LSTM。所提解码算法是一个两阶段解码，由两个DNC辅助，即翻转DNC和翻转验证DNC。前者对多位翻转的位置进行排名，而后者用于在解码失败时重新选择翻转位置。仿真结果表明，所提的DNC辅助SCLF在误差率性能和翻转尝试次数的减少方面优于基于LSTM的算法。
- en: 'The authors in [[391](#bib.bib391)] proposed a stacked LSTM network to improve
    the accuracy of erroneous bit prediction. Specifically, they trained the three
    models separately: the first and second models predict the positions of the first
    and the second erroneous bits and the third model decides whether to continue
    flipping. Simulation results demonstrate that their proposed algorithms outperform
    existing SCLF decoding algorithms in terms of BLER performance and average number
    of decoding attempts.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[391](#bib.bib391)]中，作者提出了一个堆叠LSTM网络，以提高错误比特预测的准确性。具体而言，他们分别训练了三个模型：第一个和第二个模型预测第一个和第二个错误比特的位置，而第三个模型决定是否继续翻转。仿真结果表明，他们提出的算法在BLER性能和平均解码尝试次数方面优于现有的SCLF解码算法。
- en: In [[392](#bib.bib392)], the authors proposed an approximated error metric for
    \acdsclf decoding of polar codes to improve the performance while keeping the
    average complexity low. To compensate for the approximation error, they introduced
    learnable parameters into the metric and optimized it through the custom neural
    network model using the RMSprop optimizer.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[392](#bib.bib392)]中，作者提出了一种近似误差度量，用于极化码的\acdsclf解码，以提高性能同时保持平均复杂度低。为了补偿近似误差，他们在度量中引入了可学习参数，并通过自定义神经网络模型使用RMSprop优化器进行优化。
- en: IV-C4 Other Approaches to Enhancing SCL Decoding
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 提升SCL解码的其他方法
- en: \Ac
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: \Ac
- en: sp is another approach to enhance the performance of SCL decoding for polar
    codes [[403](#bib.bib403)], which aims to prevent the correct path from being
    eliminated from the list. In [[403](#bib.bib403)], it was demonstrated that the
    proposed SP mechanism offers remarkable performance gains over the BF approach.
    However, the SCL decoder with SP generally suffers from high computational complexity,
    due to the re-decoding attempts and the computation of the shifting metric. To
    alleviate this issue, the authors in [[393](#bib.bib393)] proposed a DL-aided
    shifting metric that is free from transcendental functions and can be computed
    on-the-fly based on the path metrics.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: sp 是另一种提高极化码 SCL 解码性能的方法 [[403](#bib.bib403)]，旨在防止正确路径从列表中被消除。在 [[403](#bib.bib403)]
    中，展示了所提出的 SP 机制相较于 BF 方法提供了显著的性能提升。然而，带有 SP 的 SCL 解码器通常面临高计算复杂度的问题，这是由于重新解码尝试和计算移位度量所致。为了缓解这个问题，[[393](#bib.bib393)]
    中的作者提出了一种无超越函数的 DL 辅助移位度量，该度量可以基于路径度量动态计算。
- en: Another approach to improve the performance of CA-SCL decoding was proposed
    in [[394](#bib.bib394)], where the authors take advantage of the inherent error
    correction capability of CRC, i.e., not just for error detection. The authors
    performed CRC-based error correction using an LSTM network, where the LSTM network
    estimates the error pattern from the LLR sequence and the CRC syndrome. The proposed
    CRC error-correction aided SCL decoding scheme was demonstrated to outperform
    the error rate of the conventional CRC error-detection aided SCL decoding scheme
    at the same list size.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了另一种提高 CA-SCL 解码性能的方法 [[394](#bib.bib394)]，其中作者利用了 CRC 的固有错误修正能力，即不仅仅用于错误检测。作者使用
    LSTM 网络进行基于 CRC 的错误修正，其中 LSTM 网络根据 LLR 序列和 CRC 综合症来估计错误模式。所提出的 CRC 错误修正辅助 SCL
    解码方案在相同的列表大小下，证明了其在错误率上优于传统的 CRC 错误检测辅助 SCL 解码方案。
- en: IV-D DL-Aided Convolutional and Turbo Decoding
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D DL 辅助卷积和涡轮解码
- en: 'DL decoders have also been applied to convolutional and turbo codes. In particular,
    as we review below, several DL-aided turbo decoders have been proposed in recent
    years. We have listed these approaches in Table [VI](#S4.T6 "TABLE VI ‣ IV-D DL-Aided
    Convolutional and Turbo Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey").'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 'DL 解码器也已应用于卷积码和涡轮码。特别是，正如我们在下面回顾的那样，近年来提出了几种 DL 辅助的涡轮解码器。我们在表 [VI](#S4.T6 "TABLE
    VI ‣ IV-D DL-Aided Convolutional and Turbo Decoding ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey") 中列出了这些方法。'
- en: 'TABLE VI: Summary of DL-aided convolutional and turbo decoders.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：DL 辅助卷积和涡轮解码器的总结。
- en: '| Category | Reference | Main Contribution |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 参考文献 | 主要贡献 |'
- en: '| --- | --- | --- |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Convolutional Decoders | Teich et al. [[404](#bib.bib404)] | A DNN decoder
    for convolutional codes. |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 卷积解码器 | Teich 等人 [[404](#bib.bib404)] | 用于卷积编码的 DNN 解码器。 |'
- en: '| Turbo Decoders | Kim et al. [[244](#bib.bib244)] | A neural network BCJR
    decoder, referred to as NEURALBCJR. |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 涡轮解码器 | Kim 等人 [[244](#bib.bib244)] | 一种神经网络 BCJR 解码器，称为 NEURALBCJR。 |'
- en: '| Jiang et al. [[405](#bib.bib405)] | Deep turbo decoder (DEEPTURBO) trained
    in an end-to-end manner. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| Jiang 等人 [[405](#bib.bib405)] | 以端到端方式训练的深度涡轮解码器（DEEPTURBO）。 |'
- en: '| He et al. [[406](#bib.bib406)] | A novel model-driven decoder, called TurboNet.
    |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| He 等人 [[406](#bib.bib406)] | 一种新型模型驱动解码器，称为 TurboNet。 |'
- en: '| Hebbar et al. [[407](#bib.bib407)] | TINYTURBO that reduces the parameters
    of TurboNet+. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Hebbar 等人 [[407](#bib.bib407)] | TINYTURBO，减少了 TurboNet+ 的参数。 |'
- en: IV-D1 Convolutional Decoder
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 卷积解码器
- en: Convolutional codes encode the input stream by convolving with the generator
    polynomial, which can be efficiently implemented by shift registers. Convolutional
    codes can be represented by a time-invariant trellis, which allows efficient maximum-likelihood
    decoding based on the well-known Viterbi algorithm [[408](#bib.bib408)].
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积码通过与生成多项式卷积来对输入流进行编码，这可以通过移位寄存器有效地实现。卷积码可以通过时间不变的网格表示，这允许基于著名的维特比算法 [[408](#bib.bib408)]
    进行有效的最大似然解码。
- en: In order to improve the performance of \acitd [[409](#bib.bib409)], the authors
    in [[404](#bib.bib404)] proposed a DNN decoder for convolutional codes by unfolding
    a \achornn decoder [[410](#bib.bib410)]. The unfolded HORNN can be seen as a feedforward
    DNN whose parameters are trained by backpropagation with MBSGD. It was shown that
    with proper optimization of the parameters, the proposed decoder outperforms the
    conventional ITD and achieves performance close to maximum-likelihood decoding.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高\acitd [[409](#bib.bib409)] 的性能，[[404](#bib.bib404)]中的作者提出了一种通过展开\achornn解码器
    [[410](#bib.bib410)] 的DNN解码器。展开的HORNN可以看作是一个前馈DNN，其参数通过MBSGD进行反向传播训练。研究表明，通过对参数进行适当优化，所提出的解码器在性能上优于传统的ITD，并且其性能接近最大似然解码。
- en: IV-D2 Turbo Decoder
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 涡轮解码器
- en: Turbo codes, also known as parallel concatenated convolutional codes [[3](#bib.bib3)],
    consist of two (usually identical) \acrsc codes concatenated in parallel and a
    bit interleaver. Turbo codes are typically decoded by iterative decoding between
    two constituent SISO decoders where one constituent decoder computes posterior
    probabilities based on the \acbcjr algorithm [[411](#bib.bib411)], and then passes
    them to the other decoder. The turbo decoder significantly improves the error-correction
    performance by iteratively exchanging extrinsic information between the two constituent
    SISO decoders.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 涡轮码，也称为并行级联卷积码 [[3](#bib.bib3)]，由两个（通常是相同的）\acrsc码并行级联和一个比特交织器组成。涡轮码通常通过两个组成SISO解码器之间的迭代解码来解码，其中一个组成解码器根据\acbcjr算法
    [[411](#bib.bib411)] 计算后验概率，然后将其传递给另一个解码器。涡轮解码器通过在两个组成SISO解码器之间迭代交换外部信息显著提高了纠错性能。
- en: In [[244](#bib.bib244)], the authors proposed a DL-based BCJR decoder for an
    RSC code based on bi-GRU, referred to as NEURALBCJR, and then extended it to a
    turbo decoder by replacing the component SISO decoder with the proposed NEURALBCJR
    decoder. Simulations demonstrated that the proposed scheme is particularly beneficial
    for non-Gaussian channels, such as $t$-distributed noise. Later, a DL-aided turbo
    decoder, termed DEEPTURBO, was introduced in [[405](#bib.bib405)]. They also used
    bi-GRUs to replace the conventional SISO decoders as in [[244](#bib.bib244)],
    but the authors in [[405](#bib.bib405)] trained different bi-GRU weights across
    different iterations, whereas the authors in [[244](#bib.bib244)] shared the same
    weight for all bi-GRU blocks. This enables a fully end-to-end training without
    imitating the BCJR algorithm. Furthermore, DEEPTURBO increased the number of posterior
    LLR values exchanged between the two decoders to expedite iterative decoding.
    Extensive simulations have demonstrated that DEEPTURBO exhibits an improved reliability,
    adaptivity, and lower error floor compared to NEURALBCJR.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[244](#bib.bib244)]中，作者提出了一种基于bi-GRU的DL BCJR解码器，称为NEURALBCJR，并将其扩展为涡轮解码器，通过将组件SISO解码器替换为所提出的NEURALBCJR解码器。模拟结果表明，该方案对非高斯信道，诸如$t$-分布噪声，特别有利。后来，在[[405](#bib.bib405)]中引入了一种DL辅助的涡轮解码器，称为DEEPTURBO。他们也使用bi-GRU替代传统的SISO解码器，如同[[244](#bib.bib244)]，但[[405](#bib.bib405)]中的作者在不同的迭代中训练了不同的bi-GRU权重，而[[244](#bib.bib244)]中的作者为所有bi-GRU块共享了相同的权重。这使得无需模仿BCJR算法即可进行完全的端到端训练。此外，DEEPTURBO增加了在两个解码器之间交换的后验LLR值的数量，以加快迭代解码的速度。大量模拟表明，DEEPTURBO在可靠性、适应性和较低的误码底层方面比NEURALBCJR表现更好。
- en: '![Refer to caption](img/fcf4b3be90abd4df43bc33b99f097a5e.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fcf4b3be90abd4df43bc33b99f097a5e.png)'
- en: (a) Turbo encoder.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 涡轮编码器。
- en: '![Refer to caption](img/ead6eb4230c464d9f5521ea85928175e.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ead6eb4230c464d9f5521ea85928175e.png)'
- en: (b) Model-free DL turbo decoders based on Bi-GRUs.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于Bi-GRUs的无模型DL涡轮解码器。
- en: 'Figure 14: An encoder and DL-based decoder of turbo codes. In the NEURALBCJR
    decoder [[244](#bib.bib244)], each Bi-GRU is pre-trained to imitate BCJR algorithm,
    followed by an end-to-end training. In the DEEPTURBO decoder [[405](#bib.bib405)],
    on the other hand, all Bi-GRUs are trained directly to optimize the end-to-end
    performance.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：涡轮码的编码器和基于DL的解码器。在NEURALBCJR解码器 [[244](#bib.bib244)] 中，每个Bi-GRU都经过预训练以模仿BCJR算法，然后进行端到端训练。而在DEEPTURBO解码器
    [[405](#bib.bib405)] 中，所有Bi-GRU都直接进行训练，以优化端到端性能。
- en: 'The aforementioned model-free turbo decoders, i.e., NEURALBCJR [[244](#bib.bib244)]
    and DEEPTURBO [[405](#bib.bib405)], are illustrated in Fig. [14](#S4.F14 "Figure
    14 ‣ IV-D2 Turbo Decoder ‣ IV-D DL-Aided Convolutional and Turbo Decoding ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"), where the constituent SISO decoders of the turbo decoder are replaced
    with Bi-GRUs without considering the specific trellis structure of the RSC encoders.
    In contrast, the authors in [[406](#bib.bib406)] proposed a novel model-driven
    decoder architecture, called TurboNet, which integrates DNN into the traditional
    max-log-MAP algorithm. Furthermore, they applied network pruning to TurboNet to
    effectively reduce the number of parameters. The resulting TurboNet+ decoder was
    shown to achieve state-of-the-art performance and outperform existing DL turbo
    decoders even with lower computational complexity.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[14](#S4.F14 "Figure 14 ‣ IV-D2 Turbo Decoder ‣ IV-D DL-Aided Convolutional
    and Turbo Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey")所示，前述的无模型Turbo解码器，即NEURALBCJR [[244](#bib.bib244)]
    和 DEEPTURBO [[405](#bib.bib405)]，其Turbo解码器的组成SISO解码器被Bi-GRUs替代，而没有考虑RSC编码器的特定格结构。相比之下，[[406](#bib.bib406)]中的作者提出了一种新颖的模型驱动解码器架构，称为TurboNet，该架构将DNN整合到传统的最大-对数-MAP算法中。此外，他们对TurboNet进行了网络剪枝，有效减少了参数数量。结果表明，TurboNet+解码器在较低的计算复杂度下，表现出领先的性能，并超越了现有的DL
    Turbo解码器。'
- en: Subsequently, the authors in [[407](#bib.bib407)] proposed TINYTURBO which significantly
    reduces the trainable parameters of TurboNet+ by sharing the same weight across
    bit indices in the computation of the posterior LLR. In particular, for a block
    length of $40$, it was demonstrated that TINYTURBO with $18$ parameters outperforms
    TurboNet+ with $720$ parameters over AWGN channels. Furthermore, the strong adaptability
    of TINYTURBO to other block lengths, rates, and trellises, as well as its robustness
    to channel variations, were demonstrated.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，[[407](#bib.bib407)]中的作者提出了TINYTURBO，通过在后验LLR计算中共享相同的权重，显著减少了TurboNet+的可训练参数。特别地，对于块长度为$40$的情况，TINYTURBO以$18$个参数的配置在AWGN信道下表现优于TurboNet+的$720$个参数。此外，还展示了TINYTURBO对其他块长度、速率和格结构的强适应性以及对信道变化的鲁棒性。
- en: IV-E DL-Aided Decoding of Cyclic Codes
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 深度学习辅助的环形码解码
- en: DL decoders exploiting algebraic properties of cyclic codes have also been studied.
    In [[412](#bib.bib412)], the authors proposed a neural network-based decoder for
    cyclic codes by exploiting their cyclically invariant property. More specifically,
    inspired by the fact that the maximum-likelihood decoder of any cyclic code is
    equivariant with respect to cyclic shifts, they imposed a shift-invariant structure
    on the weights of the neural decoder so that any cyclic shift of inputs results
    in the same cyclic shift of the outputs. Simulations of BCH codes and punctured
    \acrm codes showed that the proposed decoder consistently outperforms the neural
    BP decoder proposed in [[258](#bib.bib258)]. Furthermore, they proposed a list
    decoding procedure that can significantly reduce the decoding error for BCH codes
    and punctured RM codes.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 还研究了利用环形码代数特性的DL解码器。在[[412](#bib.bib412)]中，作者提出了一种基于神经网络的环形码解码器，利用其环形不变性。更具体地说，受制于任何环形码的最大似然解码器对环形移动的不变性，他们对神经解码器的权重施加了一个不变的结构，使得输入的任何环形移动都导致输出的相同环形移动。BCH码和穿孔\acrm码的模拟显示，所提出的解码器在性能上始终优于[[258](#bib.bib258)]中提出的神经BP解码器。此外，他们还提出了一种列表解码过程，可以显著减少BCH码和穿孔RM码的解码误差。
- en: While the list decoding significantly improves the BLER, the major drawback
    was its relatively high BER. To improve the BER, the same authors proposed the
    improved version of the list decoder in [[413](#bib.bib413)]. The new decoder
    achieved a significantly lower BER compared to the list decoder in [[412](#bib.bib412)]
    while maintaining the same BLER.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然列表解码显著提高了BLER，但主要缺点是其相对较高的BER。为了改善BER，同一作者在[[413](#bib.bib413)]中提出了改进版的列表解码器。新的解码器相比于[[412](#bib.bib412)]中的列表解码器，取得了显著更低的BER，同时保持了相同的BLER。
- en: V Conclusion
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this paper, we have provided a comprehensive survey on DL for the channel
    coding problems. In particular, we have focused on DL methods for the code design
    and channel decoding problems. In what follows, we summarize the potential advantages
    and challenges of these approaches.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提供了关于深度学习在信道编码问题中的应用的综合调查。特别是，我们重点关注了用于码设计和信道解码问题的深度学习方法。接下来，我们总结了这些方法的潜在优势和挑战。
- en: V-A Code Design Applications
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 码设计应用
- en: The conventional code design algorithms such as EXIT chart and variants of DE
    require ideal assumptions about channel models and decoding schemes. In contrast,
    the major advantage of using data-driven DL for code design is that one can tailor
    codes to more realistic channels and decoding schemes, for which theoretical analysis
    is intractable.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的码设计算法，如EXIT图和DE的变体，要求对信道模型和解码方案有理想化的假设。相比之下，使用数据驱动的深度学习进行码设计的主要优势在于，可以将码定制为更现实的信道和解码方案，对于这些方案，理论分析是不可处理的。
- en: 'In Section [III](#S3 "III DL for Code Design ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), we saw that RL is a particularly popular approach
    to designing polar codes, among others. In this method, an agent learns to choose
    a new information bit position that minimizes the cumulative reward, which corresponds
    to the actual performance in terms of BLER. Calculating the reward requires Monte-Carlo
    simulations, which can be computationally intensive depending on the code length
    and target error rate. This complexity issue can hinder its application to scenarios
    with long code lengths and low error rates. Therefore, a design objective that
    can be efficiently computed during the training process may be desirable.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[III](#S3 "III DL for Code Design ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey")节中，我们看到RL是一种特别受欢迎的极化码设计方法。在这种方法中，代理学习选择一个新的信息位位置，以最小化累积奖励，这对应于BLER的实际性能。计算奖励需要蒙特卡洛模拟，根据码长和目标错误率的不同，这可能会很耗费计算资源。这一复杂性问题可能会妨碍其在长码长和低错误率场景中的应用。因此，在训练过程中可以有效计算的设计目标可能是理想的。'
- en: V-B Channel Decoding Applications
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 信道解码应用
- en: As we have seen, channel decoding is a popular application of DL and a significant
    number of papers on this topic have become available. These approaches can be
    broadly classified into model-free and model-based approaches.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，信道解码是深度学习的一个流行应用，关于这一主题的大量论文已经出现。这些方法大致可以分为无模型方法和基于模型的方法。
- en: V-B1 Model-Free Approaches
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 无模型方法
- en: Model-free decoders employing a “black-box” neural network have the potential
    to outperform conventional decoding algorithms in terms of error rate performance
    and decoding complexity/latency. In particular, it has been demonstrated that
    model-free decoders can outperform existing decoding algorithms for short code
    lengths with highly parallelizable structures. This approach is thus potentially
    suitable for low-latency applications requiring short code lengths. Note that
    the performance is highly dependent on the DL model employed, and currently, the
    Transformer-based decoder achieves the state-of-the-art performance [[222](#bib.bib222)].
    However, the performance could be potentially improved by the advanced DL techniques.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 使用“黑箱”神经网络的无模型解码器在错误率性能和解码复杂性/延迟方面有潜力超越传统的解码算法。特别是，已证明无模型解码器可以在具有高度并行结构的短码长情况下超越现有的解码算法。因此，这种方法可能适用于要求短码长的低延迟应用。请注意，性能高度依赖于所使用的深度学习模型，目前，基于Transformer的解码器实现了最先进的性能[[222](#bib.bib222)]。然而，性能可能通过先进的深度学习技术进一步提升。
- en: Due to the curse of dimensionality, the applications of model-free decoders
    have generally been limited to short codes. In general, a larger model size is
    required to decode a longer code, which not only entails high computational complexity,
    but also high space complexity in both training and inference phases. Another
    concern about this method is the robustness against adversarial attacks [[204](#bib.bib204)],
    as wireless networks are always vulnerable to radio jamming attacks [[414](#bib.bib414),
    [415](#bib.bib415)] due to the openness of wireless channels. In particular, DL-based
    communication systems may have a higher risk of being disrupted by jamming attacks
    than classical systems [[416](#bib.bib416), [417](#bib.bib417)].
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 由于维度诅咒，无模型解码器的应用通常仅限于短码。一般来说，解码更长的码需要更大的模型规模，这不仅涉及高计算复杂度，还涉及训练和推理阶段的高空间复杂度。另一个关于该方法的关注点是对抗攻击的鲁棒性[[204](#bib.bib204)]，因为无线网络由于无线信道的开放性总是容易受到无线电干扰攻击[[414](#bib.bib414),
    [415](#bib.bib415)]。特别是，基于DL的通信系统可能比经典系统有更高的被干扰攻击的风险[[416](#bib.bib416), [417](#bib.bib417)]。
- en: 'Instead of completely replacing conventional decoders, employing a DNN to augment
    existing decoders is effective for arbitrary code lengths. For example, model-free
    training has been extensively studied for DNN-based selection of flipping bit
    indices in SCF decoding as we reviewed in Section [IV-C](#S4.SS3 "IV-C DL-Aided
    Decoding of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey").'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '采用深度神经网络（DNN）来增强现有解码器是应对任意码长的有效方法，而不是完全替代传统解码器。例如，我们在第[IV-C节](#S4.SS3 "IV-C
    DL-Aided Decoding of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey")中回顾的无模型训练方法在DNN基础的SCF解码中用于选择翻转比特索引已经得到了广泛研究。'
- en: V-B2 Model-Based Approaches
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 基于模型的方法
- en: 'In contrast to the model-free approach, the model-based approach realizes a
    scalable decoder by taking advantage of the knowledge of code structures and conventional
    decoding algorithms. One of the most promising approaches is deep unfolding, which
    unfolds an iterative algorithm [[306](#bib.bib306)] and introduces a set of trainable
    parameters. In particular, as we reviewed in Section [IV-B](#S4.SS2 "IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey"), neural network-based BP decoding over an unfolded
    Tanner graph augmented with trainable parameters [[257](#bib.bib257)] has been
    extensively investigated. As the underlying BP decoding algorithm has already
    been adopted in a wide range of communication systems, this approach can be applied
    to these systems with much less modification compared to model-free decoders.
    Many existing works have demonstrated that, by introducing and optimizing trainable
    weights that mitigate the effect of short cycles in the Tanner graph, the DL BP
    decoder can achieve better a trade-off between decoding performance and latency,
    i.e., the number of decoding iterations, compared to the standard BP decoder.
    This means that the performance advantage of the DL BP decoder becomes more significant
    as the number of short cycles increases.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '与无模型方法相比，基于模型的方法通过利用代码结构和传统解码算法的知识实现了可扩展的解码器。最有前途的方法之一是深度展开，这种方法展开了一个迭代算法[[306](#bib.bib306)]并引入了一组可训练参数。特别是，正如我们在第[IV-B节](#S4.SS2
    "IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey")中回顾的那样，基于神经网络的BP解码在一个展开的Tanner图上增强了可训练参数[[257](#bib.bib257)]，这已经得到了广泛的研究。由于底层BP解码算法已经被广泛应用于各种通信系统，因此与无模型解码器相比，这种方法可以在这些系统中进行较少的修改。许多现有工作已经证明，通过引入和优化可训练权重，以减轻Tanner图中短周期的影响，DL
    BP解码器可以在解码性能和延迟（即解码迭代次数）之间实现更好的折衷。这意味着随着短周期数量的增加，DL BP解码器的性能优势变得更加显著。'
- en: V-C Challenges and Future Directions
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 挑战与未来方向
- en: Despite their excellent performance, DL-based channel coding schemes face challenges
    that need to be addressed. We conclude this survey by highlighting several future
    research directions in this regard.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习（DL）基础的信道编码方案表现优秀，但仍面临需要解决的挑战。我们通过强调未来几个研究方向来总结这项调查。
- en: V-C1 Flexibility to Support Diverse Applications
  id: totrans-445
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C1 支持多样化应用的灵活性
- en: 'Next generation communication systems such as 6G will support heterogeneous
    applications that employ the channel codes with various block lengths, reliability,
    and latency requirements [[37](#bib.bib37)]. Since there is no one-size-fits-all
    channel coding scheme, multiple code parameters, i.e., rates and lengths, must
    be supported to meet these requirements. On the other hand, adapting a DL decoder
    to different channels and code parameters would require an enormous amount of
    different parameter sets. This issue could be alleviated, for example, by a parameter
    sharing scheme and scalable GNNs as discussed in Section [IV-B](#S4.SS2 "IV-B
    DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"). Furthermore, in order to support multiple code
    parameters, training must be performed multiple times, which is time consuming
    and computationally intensive. This complexity issue can be addressed by techniques
    such as transfer learning, meta-learning, and foundation models [[248](#bib.bib248)].'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '下一代通信系统如6G将支持采用各种块长度、可靠性和延迟要求的信道编码的异构应用[[37](#bib.bib37)]。由于没有一刀切的信道编码方案，必须支持多个编码参数，即速率和长度，以满足这些要求。另一方面，将深度学习解码器适配到不同的信道和编码参数将需要大量不同的参数集。例如，通过共享参数方案和可扩展的图神经网络（GNNs）可以缓解这个问题，如[IV-B](#S4.SS2
    "IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey")节所讨论的。此外，为了支持多个编码参数，必须进行多次训练，这既耗时又计算密集。可以通过迁移学习、元学习和基础模型等技术来解决这一复杂性问题[[248](#bib.bib248)]。'
- en: V-C2 Explainable AI
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C2 可解释人工智能
- en: One of the disadvantages of DL methods is their black-box nature, which can
    hinder physical insights into the phenomena. Thus, evaluating and enhancing the
    explainability of generic DL models, i.e., the ability to provide reasons for
    the outcomes of the system [[418](#bib.bib418)], remains an active field of research
    [[419](#bib.bib419), [420](#bib.bib420), [421](#bib.bib421), [422](#bib.bib422),
    [423](#bib.bib423), [424](#bib.bib424), [425](#bib.bib425)]. In general, the explainability
    of DL models tends to have an inverse relationship to their performance, e.g.,
    prediction accuracy [[426](#bib.bib426)]. Thus, a recent advanced DL model with
    a large number of parameters is particularly difficult to interpret and explain.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法的一个缺点是其黑箱特性，这可能会阻碍对现象的物理理解。因此，评估和提升通用深度学习模型的可解释性，即为系统结果提供理由的能力[[418](#bib.bib418)]，仍然是一个活跃的研究领域[[419](#bib.bib419),
    [420](#bib.bib420), [421](#bib.bib421), [422](#bib.bib422), [423](#bib.bib423),
    [424](#bib.bib424), [425](#bib.bib425)]。一般而言，深度学习模型的可解释性往往与其性能呈反比关系，例如预测准确性[[426](#bib.bib426)]。因此，最近具有大量参数的先进深度学习模型特别难以解释和说明。
- en: In the next generation communications such as 6G, the concept of \acxai will
    become increasingly important especially for the emerging mission-critical services,
    such as autonomous driving and remote surgery [[427](#bib.bib427), [428](#bib.bib428),
    [429](#bib.bib429)]. Although the effectiveness of DL for the physical layer has
    been demonstrated in terms of its performance, its explainability has not been
    well studied. Thus, XAI-based channel coding that increases the transparency of
    DL models and explains the reasons for decisions will be of practical importance.
    The new insights gained from XAI will also help us to devise code design and decoding
    algorithms.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一代通信系统如6G中，\acxai的概念将变得越来越重要，特别是对于新兴的关键任务服务，如自动驾驶和远程手术[[427](#bib.bib427),
    [428](#bib.bib428), [429](#bib.bib429)]。尽管深度学习在物理层的有效性已经通过性能得到了验证，但其可解释性尚未得到充分研究。因此，基于XAI的信道编码，通过提高深度学习模型的透明度并解释决策理由，将具有实际重要性。从XAI获得的新见解还将帮助我们设计编码和解码算法。
- en: V-C3 Efficient Training and Inference
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C3 高效训练与推断
- en: Recent advances in DL technologies have been driven by the exponential growth
    of data and computational power, with a focus on performance rather than the economic
    and environmental costs. This research trend is often referred to as *Red AI*
    [[430](#bib.bib430)]. Indeed, the computations required for DL algorithms result
    in a surprisingly large carbon footprint [[431](#bib.bib431), [432](#bib.bib432),
    [433](#bib.bib433)]. In contrast to Red AI, which prioritizes achieving state-of-the-art
    results, *Green AI* aims to produce innovative results while taking into account
    computational costs [[430](#bib.bib430), [434](#bib.bib434)]. This paradigm shift
    toward energy and cost efficiency is inevitable for the long-term success, and
    it is therefore important to carefully select and preprocess data, reduce redundancy,
    and avoid overfitting so as to minimize the amounts of data and computational
    resources required [[435](#bib.bib435)].
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术的最新进展得益于数据和计算能力的指数增长，重点关注性能而非经济和环境成本。这一研究趋势通常被称为*红色人工智能*[[430](#bib.bib430)]。实际上，深度学习算法所需的计算产生了令人惊讶的碳足迹[[431](#bib.bib431),
    [432](#bib.bib432), [433](#bib.bib433)]。与优先追求最先进结果的红色人工智能相比，*绿色人工智能*旨在在考虑计算成本的同时产生创新结果[[430](#bib.bib430),
    [434](#bib.bib434)]。这种向能源和成本效率转变的范式变化对于长期成功是不可避免的，因此重要的是仔细选择和预处理数据，减少冗余，避免过拟合，以最小化所需的数据量和计算资源[[435](#bib.bib435)]。
- en: Data-centric approaches are promising for reducing the energy consumption of
    DL algorithms [[434](#bib.bib434)], while their primary goal was to improve performance
    in terms of accuracy⁷⁷7[https://datacentricai.org/](https://datacentricai.org/).
    These approaches recognize that the quality of the training data has a significant
    impact on their performance, and thus prioritize data quality over model refinement
    [[436](#bib.bib436), [437](#bib.bib437), [438](#bib.bib438)]. These include active
    learning, knowledge transfer, dataset distillation, data augmentation, and curriculum
    learning. For channel decoding applications, some papers have employed these techniques
    for enhancing error rate performance, but more emphasis should be placed on the
    data efficiency.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心化方法在降低深度学习算法的能耗方面具有很大的潜力[[434](#bib.bib434)]，尽管它们的主要目标是提高准确性方面的性能⁷⁷7[https://datacentricai.org/](https://datacentricai.org/)。这些方法认识到训练数据的质量对其性能有显著影响，因此优先考虑数据质量而非模型的细化[[436](#bib.bib436),
    [437](#bib.bib437), [438](#bib.bib438)]。这些方法包括主动学习、知识迁移、数据集蒸馏、数据增强和课程学习。在信道解码应用中，一些文献已应用这些技术来提高错误率性能，但应更加注重数据效率。
- en: V-C4 Quantum Machine Learning
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-C4 量子机器学习
- en: Quantum computing has a great potential to solve the classical channel coding
    problems [[439](#bib.bib439), [440](#bib.bib440), [441](#bib.bib441), [442](#bib.bib442),
    [443](#bib.bib443), [444](#bib.bib444)] more efficiently than digital computers.
    Especially in the current \acnisq era [[445](#bib.bib445)], where the fidelity
    of quantum gates is limited by noise and decoherence, hybrid quantum-classical
    algorithms such as \acvqe [[446](#bib.bib446)] and \acqaoa [[447](#bib.bib447)]
    are promising [[448](#bib.bib448), [449](#bib.bib449)]. The potential of these
    algorithms for the classical channel decoding problems has been demonstrated in
    [[440](#bib.bib440)].
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算具有比数字计算机更高效地解决经典信道编码问题的巨大潜力[[439](#bib.bib439), [440](#bib.bib440), [441](#bib.bib441),
    [442](#bib.bib442), [443](#bib.bib443), [444](#bib.bib444)]。特别是在当前的\acnisq时代[[445](#bib.bib445)]，量子门的保真度受到噪声和退相干的限制，混合量子-经典算法如\acvqe
    [[446](#bib.bib446)]和\acqaoa [[447](#bib.bib447)]显示出很大的潜力[[448](#bib.bib448),
    [449](#bib.bib449)]。这些算法在经典信道解码问题中的潜力已在[[440](#bib.bib440)]中得到证明。
- en: Furthermore, \acqml, which integrates quantum algorithms into ML, has received
    increasing attention [[450](#bib.bib450), [451](#bib.bib451), [88](#bib.bib88),
    [452](#bib.bib452), [453](#bib.bib453), [454](#bib.bib454), [455](#bib.bib455)].
    For example, it has been shown that well-designed quantum neural networks can
    achieve a higher capacity and faster training ability than comparable classical
    feedforward neural networks [[456](#bib.bib456)]. Furthermore, quantum counterparts
    to classical CNNs, autoencoders, and \acpgan have been studied in [[457](#bib.bib457),
    [458](#bib.bib458), [459](#bib.bib459), [460](#bib.bib460)]. These methods have
    the potential to improve existing methods based on classical computers for a wide
    range of communication problems including channel coding.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，\acqml 将量子算法融入机器学习中，受到了越来越多的关注 [[450](#bib.bib450), [451](#bib.bib451), [88](#bib.bib88),
    [452](#bib.bib452), [453](#bib.bib453), [454](#bib.bib454), [455](#bib.bib455)]。例如，已经显示出，设计良好的量子神经网络可以比相应的经典前馈神经网络实现更高的容量和更快的训练能力
    [[456](#bib.bib456)]。此外，经典卷积神经网络、自动编码器和 \acpgan 的量子对应物在 [[457](#bib.bib457), [458](#bib.bib458),
    [459](#bib.bib459), [460](#bib.bib460)] 中进行了研究。这些方法有可能改善基于经典计算机的现有方法，解决包括信道编码在内的各种通信问题。
- en: \MFUhyphentrue\acsetup
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: \MFUhyphentrue\acsetup
- en: uppercase/list, list/uppercase/cmd=\ecapitalisewords \printacronyms[name=List
    of Abbreviations]
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 大写字母/列表，列表/大写字母/cmd=\ecapitalisewords \printacronyms[name=缩略词列表]
- en: References
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. E. Shannon, “A mathematical theory of communication,” *Bell Syst. Tech.
    J.*, vol. 27, pp. 379–423 and 623–656, July and Oct. 1948.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. E. Shannon， “通信的数学理论，” *贝尔系统技术杂志*，第27卷，第379–423页和623–656页，1948年7月和10月。'
- en: '[2] D. J. Costello and G. D. Forney, “Channel coding: The road to channel capacity,”
    *Proceedings of the IEEE*, vol. 95, no. 6, pp. 1150–1177, 2007.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. J. Costello 和 G. D. Forney， “信道编码：通向信道容量的道路，” *IEEE汇刊*，第95卷，第6期，第1150–1177页，2007年。'
- en: '[3] C. Berrou, A. Glavieux, and P. Thitimajshima, “Near shannon limit error-correcting
    coding and decoding: Turbo-codes,” in *Proc. IEEE International Conference on
    Communications (ICC)*, pp. 1064–1070, May 1993.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. Berrou, A. Glavieux, 和 P. Thitimajshima， “接近香农极限的纠错编码和解码：Turbo码，” 见
    *IEEE国际通信会议（ICC）论文集*，第1064–1070页，1993年5月。'
- en: '[4] R. G. Gallager, “Low-density parity-check codes,” *IRE Trans. Info. Theory,*,
    vol. 8, no. 1, pp. 21–28, Jan. 1962.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. G. Gallager， “低密度奇偶校验码，” *IRE信息理论汇刊*，第8卷，第1期，第21–28页，1962年1月。'
- en: '[5] E. Arıkan, “Channel polarization: A method for constructing capacity-achieving
    codes for symmetric binary-input memoryless channels,” *IEEE Trans. Inf. Theory*,
    vol. 55, no. 7, pp. 3051–3073, Jul. 2009.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] E. Arıkan， “信道极化：构建对称二进制输入记忆无关信道容量达到码的方法，” *IEEE信息理论汇刊*，第55卷，第7期，第3051–3073页，2009年7月。'
- en: '[6] P. Yang, Y. Xiao, M. Xiao, and S. Li, “6G wireless communications: Vision
    and potential techniques,” *IEEE Network*, vol. 33, no. 4, pp. 70–75, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] P. Yang, Y. Xiao, M. Xiao, 和 S. Li， “6G无线通信：愿景和潜在技术，” *IEEE网络*，第33卷，第4期，第70–75页，2019年。'
- en: '[7] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,
    and P. Fan, “6G wireless networks: Vision, requirements, architecture, and key
    technologies,” *IEEE Veh. Technol. Mag.*, vol. 14, no. 3, pp. 28–41, 2019.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,
    和 P. Fan， “6G无线网络：愿景、需求、架构和关键技术，” *IEEE车辆技术杂志*，第14卷，第3期，第28–41页，2019年。'
- en: '[8] T. Huang, W. Yang, J. Wu, J. Ma, X. Zhang, and D. Zhang, “A survey on green
    6G network: Architecture and technologies,” *IEEE Access*, vol. 7, pp. 175 758–175 768,
    2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Huang, W. Yang, J. Wu, J. Ma, X. Zhang, 和 D. Zhang， “绿色6G网络综述：架构和技术，”
    *IEEE Access*，第7卷，第175758–175768页，2019年。'
- en: '[9] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems: Applications,
    trends, technologies, and open research problems,” *IEEE Network*, vol. 34, no. 3,
    pp. 134–142, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Saad, M. Bennis, 和 M. Chen， “6G无线系统的愿景：应用、趋势、技术和开放的研究问题，” *IEEE网络*，第34卷，第3期，第134–142页，2019年。'
- en: '[10] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, “The roadmap
    to 6G: AI empowered wireless networks,” *IEEE Commun. Mag.*, vol. 57, no. 8, pp.
    84–90, 2019.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, 和 Y.-J. A. Zhang， “通向6G的路线图：AI赋能的无线网络，”
    *IEEE通讯杂志*，第57卷，第8期，第84–90页，2019年。'
- en: '[11] M. Z. Chowdhury, M. Shahjalal, S. Ahmed, and Y. M. Jang, “6G wireless
    communication systems: Applications, requirements, technologies, challenges, and
    research directions,” *IEEE Open J. Commun. Soc.*, vol. 1, pp. 957–975, 2020.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Z. Chowdhury, M. Shahjalal, S. Ahmed, 和 Y. M. Jang， “6G无线通信系统：应用、需求、技术、挑战和研究方向，”
    *IEEE开放通信社会杂志*，第1卷，第957–975页，2020年。'
- en: '[12] A. Dogra, R. K. Jha, and S. Jain, “A survey on beyond 5G network with
    the advent of 6G: Architecture and emerging technologies,” *IEEE Access*, vol. 9,
    pp. 67 512–67 547, 2020.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Dogra, R. K. Jha, 和 S. Jain，“6G到来下的超越5G网络调查：架构与新兴技术，” *IEEE Access*，第9卷，页码67 512–67 547，2020年。'
- en: '[13] G. Gui, M. Liu, F. Tang, N. Kato, and F. Adachi, “6G: Opening new horizons
    for integration of comfort, security, and intelligence,” *IEEE Wireless Communications*,
    vol. 27, no. 5, pp. 126–132, 2020.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Gui, M. Liu, F. Tang, N. Kato, 和 F. Adachi，“6G：开启舒适、安全与智能整合的新视野，” *IEEE无线通信*，第27卷，第5期，页码126–132，2020年。'
- en: '[14] N. Kato, B. Mao, F. Tang, Y. Kawamoto, and J. Liu, “Ten challenges in
    advancing machine learning technologies toward 6G,” *IEEE Wireless Communications*,
    vol. 27, no. 3, pp. 96–103, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. Kato, B. Mao, F. Tang, Y. Kawamoto, 和 J. Liu，“推进机器学习技术至6G的十大挑战，” *IEEE无线通信*，第27卷，第3期，页码96–103，2020年。'
- en: '[15] M. Giordani, M. Polese, M. Mezzavilla, S. Rangan, and M. Zorzi, “Toward
    6G networks: Use cases and technologies,” *IEEE Commun. Mag.*, vol. 58, no. 3,
    pp. 55–61, 2020.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Giordani, M. Polese, M. Mezzavilla, S. Rangan, 和 M. Zorzi，“迈向6G网络：应用场景与技术，”
    *IEEE通讯杂志*，第58卷，第3期，页码55–61，2020年。'
- en: '[16] N. Rajatheva, I. Atzeni, E. Bjornson, A. Bourdoux, S. Buzzi, J.-B. Dore,
    S. Erkucuk, M. Fuentes, K. Guan, Y. Hu *et al.*, “White paper on broadband connectivity
    in 6G,” *arXiv preprint arXiv:2004.14247*, 2020.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] N. Rajatheva, I. Atzeni, E. Bjornson, A. Bourdoux, S. Buzzi, J.-B. Dore,
    S. Erkucuk, M. Fuentes, K. Guan, Y. Hu *等*，“6G宽带连接白皮书，” *arXiv预印本arXiv:2004.14247*，2020年。'
- en: '[17] S. Chen, Y.-C. Liang, S. Sun, S. Kang, W. Cheng, and M. Peng, “Vision,
    requirements, and technology trend of 6G: How to tackle the challenges of system
    coverage, capacity, user data-rate and movement speed,” *IEEE Wireless Communications*,
    vol. 27, no. 2, pp. 218–228, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Chen, Y.-C. Liang, S. Sun, S. Kang, W. Cheng, 和 M. Peng，“6G的愿景、需求和技术趋势：如何应对系统覆盖、容量、用户数据速率和移动速度的挑战，”
    *IEEE无线通信*，第27卷，第2期，页码218–228，2020年。'
- en: '[18] S. Dang, O. Amin, B. Shihada, and M.-S. Alouini, “What should 6G be?”
    *Nature Electronics*, vol. 3, no. 1, pp. 20–29, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] S. Dang, O. Amin, B. Shihada, 和 M.-S. Alouini，“6G应该是什么？” *自然电子学*，第3卷，第1期，页码20–29，2020年。'
- en: '[19] I. F. Akyildiz, A. Kak, and S. Nie, “6G and beyond: The future of wireless
    communications systems,” *IEEE Access*, vol. 8, pp. 133 995–134 030, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I. F. Akyildiz, A. Kak, 和 S. Nie，“6G及其未来：无线通信系统的未来，” *IEEE Access*，第8卷，页码133 995–134 030，2020年。'
- en: '[20] L. U. Khan, I. Yaqoob, M. Imran, Z. Han, and C. S. Hong, “6G wireless
    systems: A vision, architectural elements, and future directions,” *IEEE Access*,
    vol. 8, pp. 147 029–147 044, 2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. U. Khan, I. Yaqoob, M. Imran, Z. Han, 和 C. S. Hong，“6G无线系统：愿景、架构元素与未来方向，”
    *IEEE Access*，第8卷，页码147 029–147 044，2020年。'
- en: '[21] E. Yaacoub and M.-S. Alouini, “A key 6G challenge and opportunity—connecting
    the base of the pyramid: A survey on rural connectivity,” *Proceedings of the
    IEEE*, vol. 108, no. 4, pp. 533–582, 2020.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] E. Yaacoub 和 M.-S. Alouini，“关键的6G挑战与机遇——连接金字塔底层：关于农村连接性的调查，” *IEEE汇刊*，第108卷，第4期，页码533–582，2020年。'
- en: '[22] H. Viswanathan and P. E. Mogensen, “Communications in the 6G era,” *IEEE
    Access*, vol. 8, pp. 57 063–57 074, 2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Viswanathan 和 P. E. Mogensen，“6G时代的通信，” *IEEE Access*，第8卷，页码57 063–57 074，2020年。'
- en: '[23] F. Tariq, M. R. Khandaker, K.-K. Wong, M. A. Imran, M. Bennis, and M. Debbah,
    “A speculative study on 6G,” *IEEE Wireless Communications*, vol. 27, no. 4, pp.
    118–125, 2020.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] F. Tariq, M. R. Khandaker, K.-K. Wong, M. A. Imran, M. Bennis, 和 M. Debbah，“对6G的猜测性研究，”
    *IEEE无线通信*，第27卷，第4期，页码118–125，2020年。'
- en: '[24] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjöland, and F. Tufvesson,
    “6G wireless systems: Vision, requirements, challenges, insights, and opportunities,”
    *Proceedings of the IEEE*, vol. 109, no. 7, pp. 1166–1199, 2021.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjöland, 和 F. Tufvesson，“6G无线系统：愿景、需求、挑战、洞察与机遇，”
    *IEEE汇刊*，第109卷，第7期，页码1166–1199，2021年。'
- en: '[25] D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, D. Niyato,
    O. Dobre, and H. V. Poor, “6G internet of things: A comprehensive survey,” *IEEE
    Internet Things J.*, vol. 9, no. 1, pp. 359–383, 2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, D. Niyato,
    O. Dobre, 和 H. V. Poor，“6G物联网：全面调查，” *IEEE互联网事物杂志*，第9卷，第1期，页码359–383，2021年。'
- en: '[26] W. Jiang, B. Han, M. A. Habibi, and H. D. Schotten, “The road towards
    6G: A comprehensive survey,” *IEEE Open J. Commun. Soc.*, vol. 2, pp. 334–366,
    2021.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W. Jiang, B. Han, M. A. Habibi, 和 H. D. Schotten，“迈向6G之路：全面调查，” *IEEE开放通信学报*，第2卷，页码334–366，2021年。'
- en: '[27] M. Alsabah, M. A. Naser, B. M. Mahmmod, S. H. Abdulhussain, M. R. Eissa,
    A. Al-Baidhani, N. K. Noordin, S. M. Sait, K. A. Al-Utaibi, and F. Hashim, “6G
    wireless communications networks: A comprehensive survey,” *IEEE Access*, vol. 9,
    pp. 148 191–148 243, 2021.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Alsabah, M. A. Naser, B. M. Mahmmod, S. H. Abdulhussain, M. R. Eissa,
    A. Al-Baidhani, N. K. Noordin, S. M. Sait, K. A. Al-Utaibi, 和 F. Hashim，“6G无线通信网络：综合调查，”
    *IEEE Access*，第9卷，页码148 191–148 243，2021年。'
- en: '[28] C. De Lima, D. Belot, R. Berkvens, A. Bourdoux, D. Dardari, M. Guillaud,
    M. Isomursu, E.-S. Lohan, Y. Miao, A. N. Barreto *et al.*, “Convergent communication,
    sensing and localization in 6G systems: An overview of technologies, opportunities
    and challenges,” *IEEE Access*, vol. 9, pp. 26 902–26 925, 2021.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] C. De Lima, D. Belot, R. Berkvens, A. Bourdoux, D. Dardari, M. Guillaud,
    M. Isomursu, E.-S. Lohan, Y. Miao, A. N. Barreto *等*，“6G系统中的融合通信、传感和定位：技术、机会和挑战概述，”
    *IEEE Access*，第9卷，页码26 902–26 925，2021年。'
- en: '[29] C. De Alwis, A. Kalla, Q.-V. Pham, P. Kumar, K. Dev, W.-J. Hwang, and
    M. Liyanage, “Survey on 6G frontiers: Trends, applications, requirements, technologies
    and future research,” *IEEE Open J. Commun. Soc.*, vol. 2, pp. 836–886, 2021.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. De Alwis, A. Kalla, Q.-V. Pham, P. Kumar, K. Dev, W.-J. Hwang, 和 M.
    Liyanage，“6G前沿调查：趋势、应用、需求、技术和未来研究，” *IEEE 开放通信学会杂志*，第2卷，页码836–886，2021年。'
- en: '[30] F. Guo, F. R. Yu, H. Zhang, X. Li, H. Ji, and V. C. Leung, “Enabling massive
    iot toward 6G: A comprehensive survey,” *IEEE Internet Things J.*, vol. 8, no. 15,
    pp. 11 891–11 915, 2021.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] F. Guo, F. R. Yu, H. Zhang, X. Li, H. Ji, 和 V. C. Leung，“推动6G的大规模物联网：综合调查，”
    *IEEE 互联网事物杂志*，第8卷，第15期，页码11 891–11 915，2021年。'
- en: '[31] M. Matthaiou, O. Yurduseven, H. Q. Ngo, D. Morales-Jimenez, S. L. Cotton,
    and V. F. Fusco, “The road to 6G: Ten physical layer challenges for communications
    engineers,” *IEEE Commun. Mag.*, vol. 59, no. 1, pp. 64–69, 2021.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] M. Matthaiou, O. Yurduseven, H. Q. Ngo, D. Morales-Jimenez, S. L. Cotton,
    和 V. F. Fusco，“通往6G的道路：通信工程师面临的十大物理层挑战，” *IEEE 通信杂志*，第59卷，第1期，页码64–69，2021年。'
- en: '[32] M. Noor-A-Rahim, Z. Liu, H. Lee, M. O. Khyam, J. He, D. Pesch, K. Moessner,
    W. Saad, and H. V. Poor, “6G for vehicle-to-everything (V2X) communications: Enabling
    technologies, challenges, and opportunities,” *Proceedings of the IEEE*, vol.
    110, no. 6, pp. 712–734, 2022.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. Noor-A-Rahim, Z. Liu, H. Lee, M. O. Khyam, J. He, D. Pesch, K. Moessner,
    W. Saad, 和 H. V. Poor，“6G 车辆与一切（V2X）通信：启用技术、挑战和机会，” *IEEE 会议录*，第110卷，第6期，页码712–734，2022年。'
- en: '[33] C.-X. Wang, X. You, X. Gao, X. Zhu, Z. Li, C. Zhang, H. Wang, Y. Huang,
    Y. Chen, H. Haas *et al.*, “On the road to 6G: Visions, requirements, key technologies
    and testbeds,” *IEEE Communications Surveys & Tutorials*, vol. 25, no. 2, pp.
    905–974, 2023.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] C.-X. Wang, X. You, X. Gao, X. Zhu, Z. Li, C. Zhang, H. Wang, Y. Huang,
    Y. Chen, H. Haas *等*，“通往6G的道路：愿景、需求、关键技术和测试平台，” *IEEE 通信调查与教程*，第25卷，第2期，页码905–974，2023年。'
- en: '[34] M. Chafii, L. Bariah, S. Muhaidat, and M. Debbah, “Twelve scientific challenges
    for 6G: Rethinking the foundations of communications theory,” *IEEE Communications
    Surveys & Tutorials*, vol. 25, no. 2, pp. 868–904, 2023.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Chafii, L. Bariah, S. Muhaidat, 和 M. Debbah，“6G 十二大科学挑战：重新审视通信理论的基础，”
    *IEEE 通信调查与教程*，第25卷，第2期，页码868–904，2023年。'
- en: '[35] C. You, Y. Cai, Y. Liu, M. Di Renzo, T. M. Duman, A. Yener, and A. L.
    Swindlehurst, “Next generation advanced transceiver technologies for 6G,” *arXiv
    preprint arXiv:2403.16458*, 2024.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] C. You, Y. Cai, Y. Liu, M. Di Renzo, T. M. Duman, A. Yener, 和 A. L. Swindlehurst，“6G
    下一代先进收发器技术，” *arXiv 预印本 arXiv:2403.16458*，2024年。'
- en: '[36] M. Geiselhart, F. Krieg, J. Clausius, D. Tandler, and S. ten Brink, “6G:
    A welcome chance to unify channel coding?” *IEEE BITS the Information Theory Magazine*,
    vol. 3, no. 1, pp. 67–80, 2023.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] M. Geiselhart, F. Krieg, J. Clausius, D. Tandler, 和 S. ten Brink，“6G：统一信道编码的良机？”
    *IEEE BITS 信息理论杂志*，第3卷，第1期，页码67–80，2023年。'
- en: '[37] H. Zhang and W. Tong, “Channel coding for 6G extreme connectivity-requirements,
    capabilities and fundamental tradeoffs,” *IEEE BITS the Information Theory Magazine*,
    vol. 3, no. 1, pp. 54–66, 2023.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Zhang 和 W. Tong，“6G极端连接的信道编码——要求、能力和基本权衡，” *IEEE BITS 信息理论杂志*，第3卷，第1期，页码54–66，2023年。'
- en: '[38] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard *et al.*, “Tensorflow: A system for large-scale machine learning,”
    in *Proc. 12th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI)*, pp. 265–283, 2016.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard *等*，“Tensorflow：大规模机器学习系统，” 收录于 *第12届USENIX操作系统设计与实施研讨会（OSDI）论文集*，页码265–283，2016年。'
- en: '[39] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A Matlab-like environment
    for machine learning,” in *Proc. Advances in Neural Information Processing Systems
    (NeurIPS) Workshop on Big Learning*, 2011.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] R. Collobert, K. Kavukcuoglu 和 C. Farabet, “Torch7：一个类似Matlab的机器学习环境”，见
    *神经信息处理系统（NeurIPS）大规模学习研讨会*，2011年。'
- en: '[40] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers *et al.*, “In-datacenter performance
    analysis of a tensor processing unit,” in *Proc. 44th Annual International Symposium
    on Computer Architecture (ISCA)*, pp. 1–12, 2017.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers *等*，“张量处理单元的数据中心性能分析”，见 *第44届国际计算机体系结构年会（ISCA）*，第1–12页，2017年。'
- en: '[41] M. Qasaimeh, K. Denolf, J. Lo, K. Vissers, J. Zambreno, and P. H. Jones,
    “Comparing energy efficiency of CPU, GPU and FPGA implementations for vision kernels,”
    in *Proc. IEEE International Conference on Embedded Software and Systems (ICESS)*,
    pp. 1–8, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Qasaimeh, K. Denolf, J. Lo, K. Vissers, J. Zambreno 和 P. H. Jones,
    “比较CPU、GPU和FPGA实现的视觉内核能效”，见 *IEEE国际嵌入式软件与系统会议（ICESS）*，第1–8页，2019年。'
- en: '[42] T. Wang, C.-K. Wen, H. Wang, F. Gao, T. Jiang, and S. Jin, “Deep learning
    for wireless physical layer: Opportunities and challenges,” *China Communications*,
    vol. 14, no. 11, pp. 92–111, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] T. Wang, C.-K. Wen, H. Wang, F. Gao, T. Jiang 和 S. Jin, “无线物理层的深度学习：机遇与挑战”，*中国通讯*，第14卷，第11期，第92–111页，2017年。'
- en: '[43] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and wireless
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 3,
    pp. 2224–2287, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. Zhang, P. Patras 和 H. Haddadi, “移动和无线网络中的深度学习：综述”，*IEEE通信调查与教程*，第21卷，第3期，第2224–2287页，2019年。'
- en: '[44] D. Gündüz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R. Murthy,
    and M. van der Schaar, “Machine learning in the air,” *IEEE J. Sel. Areas Commun.*,
    vol. 37, no. 10, pp. 2184–2199, 2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] D. Gündüz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R. Murthy
    和 M. van der Schaar, “空中机器学习”，*IEEE选择领域通讯期刊*，第37卷，第10期，第2184–2199页，2019年。'
- en: '[45] A. Balatsoukas-Stimming and C. Studer, “Deep unfolding for communications
    systems: A survey and some new directions,” in *Proc. IEEE International Workshop
    on Signal Processing Systems (SiPS)*, pp. 266–271, 2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Balatsoukas-Stimming 和 C. Studer, “通信系统的深度展开：调查及一些新方向”，见 *IEEE国际信号处理系统研讨会（SiPS）*，第266–271页，2019年。'
- en: '[46] A. Samad, W. Saad, R. Nandana, C. Kapseok, D. Steinbach, B. Sliwa, C. Wietfeld,
    K. Mei, S. Hamid, H.-J. Zepernick *et al.*, *White Paper on Machine Learning in
    6G Wireless Communication Networks*.   University of Oulu, 2020.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Samad, W. Saad, R. Nandana, C. Kapseok, D. Steinbach, B. Sliwa, C.
    Wietfeld, K. Mei, S. Hamid, H.-J. Zepernick *等*，*关于6G无线通信网络中的机器学习的白皮书*。   奥卢大学，2020年。'
- en: '[47] C. Zhang, Y.-L. Ueng, C. Studer, and A. Burg, “Artificial intelligence
    for 5G and beyond 5G: Implementations, algorithms, and optimizations,” *IEEE J.
    Emerg. Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 149–163, 2020.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] C. Zhang, Y.-L. Ueng, C. Studer 和 A. Burg, “5G及未来5G中的人工智能：实施、算法和优化”，*IEEE新兴精选电路系统期刊*，第10卷，第2期，第149–163页，2020年。'
- en: '[48] A. Ly and Y.-D. Yao, “A review of deep learning in 5G research: Channel
    coding, massive MIMO, multiple access, resource allocation, and network security,”
    *IEEE Open J. Commun. Soc.*, vol. 2, pp. 396–408, 2021.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Ly 和 Y.-D. Yao, “5G研究中的深度学习综述：信道编码、大规模MIMO、多路访问、资源分配和网络安全”，*IEEE开放通信学会期刊*，第2卷，第396–408页，2021年。'
- en: '[49] C. Mao, Z. Mu, Q. Liang, I. Schizas, and C. Pan, “Deep learning in physical
    layer communications: Evolution and prospects in 5G and 6G networks,” *IET Communications*,
    vol. 17, no. 16, pp. 1863–1876, 2023.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Mao, Z. Mu, Q. Liang, I. Schizas 和 C. Pan, “物理层通信中的深度学习：5G和6G网络中的演变与前景”，*IET通讯*，第17卷，第16期，第1863–1876页，2023年。'
- en: '[50] M. Akrout, A. Feriani, F. Bellili, A. Mezghani, and E. Hossain, “Domain
    generalization in machine learning models for wireless communications: Concepts,
    state-of-the-art, and open issues,” *IEEE Communications Surveys & Tutorials*,
    vol. 25, no. 4, pp. 3014–3037, 2023.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] M. Akrout, A. Feriani, F. Bellili, A. Mezghani 和 E. Hossain, “无线通信中机器学习模型的领域泛化：概念、现状及开放问题”，*IEEE通信调查与教程*，第25卷，第4期，第3014–3037页，2023年。'
- en: '[51] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization:
    A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 45, no. 4, pp. 4396–4415,
    2022.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. Zhou, Z. Liu, Y. Qiao, T. Xiang 和 C. C. Loy, “领域泛化：综述”，*IEEE模式分析与机器智能期刊*，第45卷，第4期，第4396–4415页，2022年。'
- en: '[52] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and
    P. Yu, “Generalizing to unseen domains: A survey on domain generalization,” *IEEE
    Trans. Knowl. Data Eng.*, vol. 35, no. 8, pp. 8052–8072, 2022.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, 和
    P. Yu，“向未见领域的泛化：领域泛化综述，” *IEEE Trans. Knowl. Data Eng.*, 第35卷，第8期，第8052–8072页，2022年。'
- en: '[53] N. Ye, S. Miao, J. Pan, Q. Ouyang, X. Li, and X. Hou, “Artificial intelligence
    for wireless physical-layer technologies (AI4PHY): A comprehensive survey,” *IEEE
    Trans. Cogn. Commun. Netw.*, 2024.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] N. Ye, S. Miao, J. Pan, Q. Ouyang, X. Li, 和 X. Hou，“无线物理层技术的人工智能（AI4PHY）：综合调查，”
    *IEEE Trans. Cogn. Commun. Netw.*, 2024年。'
- en: '[54] M. Rowshan, M. Qiu, Y. Xie, X. Gu, and J. Yuan, “Channel coding towards
    6G: Technical overview and outlook,” *IEEE Open J. Commun. Soc.*, vol. 5, pp.
    2585–2685, 2024.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Rowshan, M. Qiu, Y. Xie, X. Gu, 和 J. Yuan，“面向6G的信道编码：技术概述与展望，” *IEEE
    Open J. Commun. Soc.*, 第5卷，第2585–2685页，2024年。'
- en: '[55] X.-A. Wang and S. B. Wicker, “An artificial neural net Viterbi decoder,”
    *IEEE Trans. Commun.*, vol. 44, no. 2, pp. 165–171, 1996.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] X.-A. Wang 和 S. B. Wicker，“一种人工神经网络Viterbi解码器，” *IEEE Trans. Commun.*,
    第44卷，第2期，第165–171页，1996年。'
- en: '[56] A. Hamalainen and J. Henriksson, “A recurrent neural decoder for convolutional
    codes,” in *Proc. IEEE International Conference on Communications (ICC)*, vol. 2,
    pp. 1305–1309, 1999.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Hamalainen 和 J. Henriksson，“用于卷积码的递归神经解码器，” 在 *Proc. IEEE International
    Conference on Communications (ICC)*, 第2卷，第1305–1309页，1999年。'
- en: '[57] M. Ibnkahla, “Applications of neural networks to digital communications–A
    survey,” *Signal Processing*, vol. 80, no. 7, pp. 1185–1215, 2000.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M. Ibnkahla，“神经网络在数字通信中的应用—综述，” *Signal Processing*, 第80卷，第7期，第1185–1215页，2000年。'
- en: '[58] T. O’Shea and J. Hoydis, “An introduction to deep learning for the physical
    layer,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 3, no. 4, pp. 563–575, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] T. O’Shea 和 J. Hoydis，“物理层深度学习入门，” *IEEE Trans. Cogn. Commun. Netw.*,
    第3卷，第4期，第563–575页，2017年。'
- en: '[59] O. Simeone, “A very brief introduction to machine learning with applications
    to communication systems,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 4, no. 4, pp.
    648–664, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] O. Simeone，“机器学习及其在通信系统中的应用的简要介绍，” *IEEE Trans. Cogn. Commun. Netw.*,
    第4卷，第4期，第648–664页，2018年。'
- en: '[60] S. Dörner, S. Cammerer, J. Hoydis, and S. ten Brink, “Deep learning based
    communication over the air,” *IEEE J. Sel. Topics Signal Process.*, vol. 12, no. 1,
    pp. 132–143, 2018.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Dörner, S. Cammerer, J. Hoydis, 和 S. ten Brink，“基于深度学习的空中通信，” *IEEE
    J. Sel. Topics Signal Process.*, 第12卷，第1期，第132–143页，2018年。'
- en: '[61] H. Huang, S. Guo, G. Gui, Z. Yang, J. Zhang, H. Sari, and F. Adachi, “Deep
    learning for physical-layer 5G wireless techniques: Opportunities, challenges
    and solutions,” *IEEE Wireless Communications*, vol. 27, no. 1, pp. 214–222, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Huang, S. Guo, G. Gui, Z. Yang, J. Zhang, H. Sari, 和 F. Adachi，“物理层5G无线技术的深度学习：机遇、挑战和解决方案，”
    *IEEE Wireless Communications*, 第27卷，第1期，第214–222页，2019年。'
- en: '[62] H. He, S. Jin, C.-K. Wen, F. Gao, G. Y. Li, and Z. Xu, “Model-driven deep
    learning for physical layer communications,” *IEEE Wireless Communications*, vol. 26,
    no. 5, pp. 77–83, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. He, S. Jin, C.-K. Wen, F. Gao, G. Y. Li, 和 Z. Xu，“基于模型的物理层深度学习，” *IEEE
    Wireless Communications*, 第26卷，第5期，第77–83页，2019年。'
- en: '[63] M. Kim, W. Lee, J. Yoon, and O. Jo, “Toward the realization of encoder
    and decoder using deep neural networks,” *IEEE Commun. Mag.*, vol. 57, no. 5,
    pp. 57–63, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Kim, W. Lee, J. Yoon, 和 O. Jo，“使用深度神经网络实现编码器和解码器，” *IEEE Commun. Mag.*,
    第57卷，第5期，第57–63页，2019年。'
- en: '[64] M. Varasteh, J. Hoydis, and B. Clerckx, “Learning to communicate and energize:
    Modulation, coding, and multiple access designs for wireless information-power
    transmission,” *IEEE Trans. Commun.*, vol. 68, no. 11, pp. 6822–6839, 2020.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. Varasteh, J. Hoydis, 和 B. Clerckx，“学习通信和能量激活：无线信息功率传输的调制、编码和多址设计，”
    *IEEE Trans. Commun.*, 第68卷，第11期，第6822–6839页，2020年。'
- en: '[65] F. Restuccia and T. Melodia, “Deep learning at the physical layer: System
    challenges and applications to 5G and beyond,” *IEEE Commun. Mag.*, vol. 58, no. 10,
    pp. 58–64, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] F. Restuccia 和 T. Melodia，“物理层的深度学习：系统挑战及其在5G及未来技术中的应用，” *IEEE Commun.
    Mag.*, 第58卷，第10期，第58–64页，2020年。'
- en: '[66] S. Zhang, J. Liu, T. K. Rodrigues, and N. Kato, “Deep learning techniques
    for advancing 6G communications in the physical layer,” *IEEE Wireless Communications*,
    vol. 28, no. 5, pp. 141–147, 2021.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Zhang, J. Liu, T. K. Rodrigues, 和 N. Kato，“推进6G通信物理层的深度学习技术，” *IEEE
    Wireless Communications*, 第28卷，第5期，第141–147页，2021年。'
- en: '[67] B. Ozpoyraz, A. T. Dogukan, Y. Gevez, U. Altun, and E. Basar, “Deep learning-aided
    6G wireless networks: A comprehensive survey of revolutionary PHY architectures,”
    *IEEE Open J. Commun. Soc.*, vol. 3, pp. 1749–1809, 2022.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] B. Ozpoyraz, A. T. Dogukan, Y. Gevez, U. Altun, 和 E. Basar，“深度学习辅助的6G无线网络：革命性物理层架构的全面调查”，*IEEE
    Open J. Commun. Soc.*，第3卷，页码1749–1809，2022年。'
- en: '[68] Z. Lu, R. Li, K. Lu, X. Chen, E. Hossain, Z. Zhao, and H. Zhang, “Semantics-empowered
    communications: A tutorial-cum-survey,” *IEEE Communications Surveys & Tutorials*,
    vol. 26, no. 1, pp. 41–79, 2023.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Z. Lu, R. Li, K. Lu, X. Chen, E. Hossain, Z. Zhao, 和 H. Zhang，“语义赋能的通信：教程及调查”，*IEEE
    Communications Surveys & Tutorials*，第26卷，第1期，页码41–79，2023年。'
- en: '[69] T. Ohtsuki, “Machine learning in 6G wireless communications,” *IEICE Trans.
    Commun.*, vol. 106, no. 2, pp. 75–83, 2023.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] T. Ohtsuki，“6G无线通信中的机器学习”，*IEICE Trans. Commun.*，第106卷，第2期，页码75–83，2023年。'
- en: '[70] Y. Shi, L. Lian, Y. Shi, Z. Wang, Y. Zhou, L. Fu, L. Bai, J. Zhang, and
    W. Zhang, “Machine learning for large-scale optimization in 6G wireless networks,”
    *IEEE Communications Surveys & Tutorials*, vol. 25, no. 4, pp. 2088–2132, 2023.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Shi, L. Lian, Y. Shi, Z. Wang, Y. Zhou, L. Fu, L. Bai, J. Zhang, 和
    W. Zhang，“6G无线网络中的大规模优化机器学习”，*IEEE Communications Surveys & Tutorials*，第25卷，第4期，页码2088–2132，2023年。'
- en: '[71] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D. I.
    Kim, and K. B. Letaief, “Generative AI for physical layer communications: A survey,”
    *IEEE Trans. Cogn. Commun. Netw.*, 2024.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D.
    I. Kim, 和 K. B. Letaief，“生成式AI在物理层通信中的应用：一项调查”，*IEEE Trans. Cogn. Commun. Netw.*，2024年。'
- en: '[72] N. Islam and S. Shin, “Deep learning in physical layer: Review on data
    driven end-to-end communication systems and their enabling semantic applications,”
    *arXiv preprint arXiv:2401.12800*, 2024.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. Islam 和 S. Shin，“物理层中的深度学习：数据驱动的端到端通信系统及其支持的语义应用综述”，*arXiv preprint
    arXiv:2401.12800*，2024年。'
- en: '[73] T. M. Hoang, A. Vahid, H. D. Tuan, and L. Hanzo, “Physical layer authentication
    and security design in the machine learning era,” *IEEE Communications Surveys
    & Tutorials*, 2024.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] T. M. Hoang, A. Vahid, H. D. Tuan, 和 L. Hanzo，“机器学习时代的物理层认证与安全设计”，*IEEE
    Communications Surveys & Tutorials*，2024年。'
- en: '[74] J. Hoydis, S. Cammerer, F. A. Aoudia, A. Vem, N. Binder, G. Marcus, and
    A. Keller, “Sionna: An open-source library for next-generation physical layer
    research,” *arXiv preprint arXiv:2203.11854*, 2022.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Hoydis, S. Cammerer, F. A. Aoudia, A. Vem, N. Binder, G. Marcus, 和
    A. Keller，“Sionna：用于下一代物理层研究的开源库”，*arXiv preprint arXiv:2203.11854*，2022年。'
- en: '[75] H. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, “Deep learning enabled semantic
    communication systems,” *IEEE Trans. Signal Process.*, vol. 69, pp. 2663–2675,
    2021.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] H. Xie, Z. Qin, G. Y. Li, 和 B.-H. Juang，“深度学习驱动的语义通信系统”，*IEEE Trans. Signal
    Process.*，第69卷，页码2663–2675，2021年。'
- en: '[76] Z. Qin, X. Tao, J. Lu, W. Tong, and G. Y. Li, “Semantic communications:
    Principles and challenges,” *arXiv preprint arXiv:2201.01389*, 2021.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Z. Qin, X. Tao, J. Lu, W. Tong, 和 G. Y. Li，“语义通信：原理与挑战”，*arXiv preprint
    arXiv:2201.01389*，2021年。'
- en: '[77] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse:
    Visions, enabling technologies, and challenges,” *IEEE Communications Surveys
    & Tutorials*, vol. 25, no. 1, pp. 656–700, 2022.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, 和 C. Miao，“全面实现边缘支持的元宇宙：愿景、使能技术及挑战”，*IEEE Communications Surveys &
    Tutorials*，第25卷，第1期，页码656–700，2022年。'
- en: '[78] W. Yang, H. Du, Z. Q. Liew, W. Y. B. Lim, Z. Xiong, D. Niyato, X. Chi,
    X. S. Shen, and C. Miao, “Semantic communications for future internet: Fundamentals,
    applications, and challenges,” *IEEE Communications Surveys & Tutorials*, vol. 25,
    no. 1, pp. 213–250, 2022.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] W. Yang, H. Du, Z. Q. Liew, W. Y. B. Lim, Z. Xiong, D. Niyato, X. Chi,
    X. S. Shen, 和 C. Miao，“未来互联网的语义通信：基础、应用及挑战”，*IEEE Communications Surveys & Tutorials*，第25卷，第1期，页码213–250，2022年。'
- en: '[79] X. Luo, H.-H. Chen, and Q. Guo, “Semantic communications: Overview, open
    issues, and future research directions,” *IEEE Wireless Communications*, vol. 29,
    no. 1, pp. 210–219, 2022.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] X. Luo, H.-H. Chen, 和 Q. Guo，“语义通信：概述、开放问题及未来研究方向”，*IEEE Wireless Communications*，第29卷，第1期，页码210–219，2022年。'
- en: '[80] D. Gündüz, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener, K. K.
    Wong, and C.-B. Chae, “Beyond transmitting bits: Context, semantics, and task-oriented
    communications,” *IEEE J. Sel. Areas Commun.*, vol. 41, no. 1, pp. 5–41, 2022.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Gündüz, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener, K.
    K. Wong, 和 C.-B. Chae，"超越比特传输：背景、语义和任务导向通信"，*IEEE J. Sel. Areas Commun.*，第41卷，第1期，页码5–41，2022年。'
- en: '[81] T. M. Getu, G. Kaddoum, and M. Bennis, “Making sense of meaning: A survey
    on metrics for semantic and goal-oriented communication,” *IEEE Access*, vol. 11,
    pp. 2169–3536, 2023.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] T. M. Getu, G. Kaddoum 和 M. Bennis，“理解意义：关于语义和目标导向通信度量的调查，” *IEEE Access*，第11卷，第2169–3536页，2023年。'
- en: '[82] K. Li, B. P. L. Lau, X. Yuan, W. Ni, M. Guizani, and C. Yuen, “Towards
    ubiquitous semantic metaverse: Challenges, approaches, and opportunities,” *IEEE
    Internet Things J.*, vol. 10, no. 24, pp. 21 855–21 872, 2023.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] K. Li, B. P. L. Lau, X. Yuan, W. Ni, M. Guizani 和 C. Yuen，“迈向无处不在的语义元宇宙：挑战、方法和机遇，”
    *IEEE 互联网事物期刊*，第10卷，第24期，第21,855–21,872页，2023年。'
- en: '[83] D. Wheeler and B. Natarajan, “Engineering semantic communication: A survey,”
    *IEEE Access*, vol. 11, pp. 13 965–13 995, 2023.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] D. Wheeler 和 B. Natarajan，“工程化语义通信：调查，” *IEEE Access*，第11卷，第13,965–13,995页，2023年。'
- en: '[84] G. Torlai and R. G. Melko, “Neural decoder for topological codes,” *Physical
    Review Letters*, vol. 119, no. 3, p. 030501, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] G. Torlai 和 R. G. Melko，“拓扑编码的神经解码器，” *物理评论快报*，第119卷，第3期，第030501页，2017年。'
- en: '[85] S. Varsamopoulos, B. Criger, and K. Bertels, “Decoding small surface codes
    with feedforward neural networks,” *Quantum Science and Technology*, vol. 3, no. 1,
    p. 015004, 2017.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Varsamopoulos, B. Criger 和 K. Bertels，“使用前馈神经网络解码小型表面编码，” *量子科学与技术*，第3卷，第1期，第015004页，2017年。'
- en: '[86] S. Varsamopoulos, K. Bertels, and C. G. Almudever, “Comparing neural network
    based decoders for the surface code,” *IEEE Transactions on Computers*, vol. 69,
    no. 2, pp. 300–311, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. Varsamopoulos, K. Bertels 和 C. G. Almudever，“比较基于神经网络的表面码解码器，” *IEEE
    计算机交易*，第69卷，第2期，第300–311页，2019年。'
- en: '[87] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel, and N. Friis, “Optimizing
    quantum error correction codes with reinforcement learning,” *Quantum*, vol. 3,
    p. 215, 2019.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel 和 N. Friis，“利用强化学习优化量子误差纠正码，”
    *量子*，第3卷，第215页，2019年。'
- en: '[88] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto,
    and L. Zdeborová, “Machine learning and the physical sciences,” *Reviews of Modern
    Physics*, vol. 91, no. 4, p. 045002, 2019.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto
    和 L. Zdeborová，“机器学习与物理科学，” *现代物理评论*，第91卷，第4期，第045002页，2019年。'
- en: '[89] A. d. iOlius, P. Fuentes, R. Orús, P. M. Crespo, and J. E. Martinez, “Decoding
    algorithms for surface codes,” *arXiv preprint arXiv:2307.14989*, 2023.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] A. d. iOlius, P. Fuentes, R. Orús, P. M. Crespo 和 J. E. Martinez，“表面编码的解码算法，”
    *arXiv 预印本 arXiv:2307.14989*，2023年。'
- en: '[90] M. Krenn, J. Landgraf, T. Foesel, and F. Marquardt, “Artificial intelligence
    and machine learning for quantum technologies,” *Physical Review A*, vol. 107,
    no. 1, p. 010101, 2023.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. Krenn, J. Landgraf, T. Foesel 和 F. Marquardt，“人工智能与量子技术的机器学习，” *物理评论A*，第107卷，第1期，第010101页，2023年。'
- en: '[91] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
    in nervous activity,” *The Bulletin of Mathematical Biophysics*, vol. 5, no. 4,
    pp. 115–133, 1943.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] W. S. McCulloch 和 W. Pitts，“神经活动中固有思想的逻辑演算，” *数学生物物理学杂志*，第5卷，第4期，第115–133页，1943年。'
- en: '[92] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] D. E. Rumelhart, G. E. Hinton 和 R. J. Williams，“通过反向传播错误学习表示，” *自然*，第323卷，第6088期，第533–536页，1986年。'
- en: '[93] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Schmidhuber，“神经网络中的深度学习：概述，” *神经网络*，第61卷，第85–117页，2015年。'
- en: '[94] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. LeCun, Y. Bengio 和 G. Hinton，“深度学习，” *自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[95] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, “A survey
    of deep neural network architectures and their applications,” *Neurocomputing*,
    vol. 234, pp. 11–26, 2017.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu 和 F. E. Alsaadi，“深度神经网络架构及其应用调查，”
    *神经计算*，第234卷，第11–26页，2017年。'
- en: '[96] L. Deng, D. Yu *et al.*, “Deep learning: methods and applications,” *Foundations
    and Trends® in Signal Processing*, vol. 7, no. 3–4, pp. 197–387, 2014.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] L. Deng, D. Yu *等*，“深度学习：方法与应用，” *信号处理基础与趋势®*，第7卷，第3–4期，第197–387页，2014年。'
- en: '[97] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, and S. S. Iyengar, “A survey on deep learning: Algorithms, techniques,
    and applications,” *ACM Computing Surveys (CSUR)*, vol. 51, no. 5, pp. 1–36, 2018.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen 和 S. S. Iyengar，“深度学习调查：算法、技术与应用，” *ACM 计算调查（CSUR）*，第51卷，第5期，第1–36页，2018年。'
- en: '[98] M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S. Nasrin,
    M. Hasan, B. C. Van Essen, A. A. Awwal, and V. K. Asari, “A state-of-the-art survey
    on deep learning theory and architectures,” *Electronics*, vol. 8, no. 3, p. 292,
    2019.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] M. Z. Alom、T. M. Taha、C. Yakopcic、S. Westberg、P. Sidike、M. S. Nasrin、M.
    Hasan、B. C. Van Essen、A. A. Awwal 和 V. K. Asari，“关于深度学习理论和架构的前沿调查，” *电子学*，第8卷，第3期，文章编号292，2019年。'
- en: '[99] S. Dong, P. Wang, and K. Abbas, “A survey on deep learning and its applications,”
    *Computer Science Review*, vol. 40, p. 100379, 2021.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Dong、P. Wang 和 K. Abbas，“深度学习及其应用的调查，” *计算机科学评论*，第40卷，文章编号100379，2021年。'
- en: '[100] S. Dargan, M. Kumar, M. R. Ayyagari, and G. Kumar, “A survey of deep
    learning and its applications: a new paradigm to machine learning,” *Archives
    of Computational Methods in Engineering*, vol. 27, pp. 1071–1092, 2020.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Dargan、M. Kumar、M. R. Ayyagari 和 G. Kumar，“深度学习及其应用的调查：机器学习的新范式，”
    *工程计算方法档案*，第27卷，页码1071–1092，2020年。'
- en: '[101] D. E. Rumelhart, G. E. Hinton, R. J. Williams *et al.*, *Learning Internal
    Representations By Error Propagation*.   MIT Press, Cambridge, MA, 1985.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] D. E. Rumelhart、G. E. Hinton、R. J. Williams *等*，*通过误差传播学习内部表征*。MIT出版社，剑桥，MA，1985年。'
- en: '[102] S. R. Dubey, S. K. Singh, and B. B. Chaudhuri, “Activation functions
    in deep learning: A comprehensive survey and benchmark,” *Neurocomputing*, vol.
    503, pp. 92–108, 2022.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] S. R. Dubey、S. K. Singh 和 B. B. Chaudhuri，“深度学习中的激活函数：全面的调查和基准测试，” *神经计算*，第503卷，页码92–108，2022年。'
- en: '[103] V. Nair and G. E. Hinton, “Rectified linear units improve restricted
    boltzmann machines,” in *Proc. International Conference on Machine Learning (ICML)*,
    pp. 807–814, 2010.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] V. Nair 和 G. E. Hinton，“修正线性单元改进限制玻尔兹曼机，” 收录于 *国际机器学习会议（ICML）论文集*，页码807–814，2010年。'
- en: '[104] A. L. Maas, A. Y. Hannun, A. Y. Ng *et al.*, “Rectifier nonlinearities
    improve neural network acoustic models,” in *Proc. International Conference on
    Machine Learning (ICML)*, vol. 30, no. 1, p. 3, 2013.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. L. Maas、A. Y. Hannun、A. Y. Ng *等*，“修正非线性函数改进神经网络声学模型，” 收录于 *国际机器学习会议（ICML）论文集*，第30卷，第1期，页码3，2013年。'
- en: '[105] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification,” in *Proc. IEEE International
    Conference on Computer Vision (ICCV)*, pp. 1026–1034, 2015.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. He、X. Zhang、S. Ren 和 J. Sun，“深入研究修正器：在ImageNet分类上超越人类级别表现，” 收录于 *IEEE国际计算机视觉会议（ICCV）论文集*，页码1026–1034，2015年。'
- en: '[106] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance
    of initialization and momentum in deep learning,” in *Proc. International Conference
    on Machine Learning (ICML)*, pp. 1139–1147, 2013.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] I. Sutskever、J. Martens、G. Dahl 和 G. Hinton，“初始化和动量在深度学习中的重要性，” 收录于 *国际机器学习会议（ICML）论文集*，页码1139–1147，2013年。'
- en: '[107] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for
    online learning and stochastic optimization,” *Journal of Machine Learning Research*,
    vol. 12, no. Jul, pp. 2121–2159, 2011.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] J. Duchi、E. Hazan 和 Y. Singer，“在线学习和随机优化的自适应子梯度方法，” *机器学习研究期刊*，第12卷，第7期，页码2121–2159，2011年。'
- en: '[108] M. D. Zeiler, “ADADELTA: an adaptive learning rate method,” *arXiv preprint
    arXiv:1212.5701*, 2012.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. D. Zeiler，“ADADELTA：一种自适应学习率方法，” *arXiv 预印本 arXiv:1212.5701*，2012年。'
- en: '[109] T. Dozat, “Incorporating nesterov momentum into Adam,” in *Proc. International
    Conference on Learning Representations (ICLR)*, pp. 2013–2016, 2016.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] T. Dozat，“将Nesterov动量纳入Adam，” 收录于 *国际学习表征会议（ICLR）论文集*，页码2013–2016，2016年。'
- en: '[110] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. P. Kingma 和 J. Ba，“Adam：一种用于随机优化的方法，” *arXiv 预印本 arXiv:1412.6980*，2014年。'
- en: '[111] S. Ruder, “An overview of gradient descent optimization algorithms,”
    *arXiv preprint arXiv:1609.04747*, 2016.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] S. Ruder，“梯度下降优化算法概述，” *arXiv 预印本 arXiv:1609.04747*，2016年。'
- en: '[112] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*.   MIT
    press, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] R. S. Sutton 和 A. G. Barto，*强化学习：一种介绍*。MIT出版社，2018年。'
- en: '[113] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] V. Mnih、K. Kavukcuoglu、D. Silver、A. A. Rusu、J. Veness、M. G. Bellemare、A.
    Graves、M. Riedmiller、A. K. Fidjeland、G. Ostrovski *等*，“通过深度强化学习实现人类级别的控制，” *自然*，第518卷，第7540期，页码529–533，2015年。'
- en: '[114] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Process. Mag.*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. Arulkumaran、M. P. Deisenroth、M. Brundage 和 A. A. Bharath，“深度强化学习：简要调查，”
    *IEEE信号处理杂志*，第34卷，第6期，页码26–38，2017年。'
- en: '[115] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Li，“深度强化学习：概述，” *arXiv 预印本 arXiv:1701.07274*，2017。'
- en: '[116] S. E. Li, *Deep reinforcement learning*.   Springer, 2023.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. E. Li，*深度强化学习*。 Springer，2023。'
- en: '[117] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker *et al.*, “Model-based
    reinforcement learning: A survey,” *Foundations and Trends® in Machine Learning*,
    vol. 16, no. 1, pp. 1–118, 2023.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker *等*，“基于模型的强化学习：调查，”
    *机器学习基础与趋势®*，第16卷，第1期，页码 1–118，2023。'
- en: '[118] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Trans.
    Knowl. Data Eng.*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. J. Pan 和 Q. Yang，“迁移学习调查，” *IEEE 知识与数据工程学报*，第22卷，第10期，页码 1345–1359，2009。'
- en: '[119] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    *Journal of Big data*, vol. 3, pp. 1–40, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Weiss, T. M. Khoshgoftaar, 和 D. Wang，“迁移学习调查，” *大数据学报*，第3卷，页码 1–40，2016。'
- en: '[120] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on
    deep transfer learning,” in *Proc. International Conference on Artificial Neural
    Networks (ICANN)*, pp. 270–279, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, 和 C. Liu，“深度迁移学习调查，” 收录于
    *人工神经网络国际会议（ICANN）论文集*，页码 270–279，2018。'
- en: '[121] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He,
    “A comprehensive survey on transfer learning,” *Proceedings of the IEEE*, vol.
    109, no. 1, pp. 43–76, 2020.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, 和 Q. He，“迁移学习全面调查，”
    *IEEE 会议论文集*，第109卷，第1期，页码 43–76，2020。'
- en: '[122] M. Wang, Y. Lin, Q. Tian, and G. Si, “Transfer learning promotes 6G wireless
    communications: Recent advances and future challenges,” *IEEE Trans. Rel.*, vol. 70,
    no. 2, pp. 790–807, 2021.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] M. Wang, Y. Lin, Q. Tian, 和 G. Si，“迁移学习促进6G无线通信：近期进展与未来挑战，” *IEEE 可靠性学报*，第70卷，第2期，页码
    790–807，2021。'
- en: '[123] M. Crawshaw, “Multi-task learning with deep neural networks: A survey,”
    *arXiv preprint arXiv:2009.09796*, 2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. Crawshaw，“深度神经网络的多任务学习：调查，” *arXiv 预印本 arXiv:2009.09796*，2020。'
- en: '[124] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Trans.
    Knowl. Data Eng.*, vol. 34, no. 12, pp. 5586–5609, 2021.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Y. Zhang 和 Q. Yang，“多任务学习调查，” *IEEE 知识与数据工程学报*，第34卷，第12期，页码 5586–5609，2021。'
- en: '[125] M. Kim, W. Lee, and D.-H. Cho, “A novel PAPR reduction scheme for OFDM
    system based on deep learning,” *IEEE Commun. Lett.*, vol. 22, no. 3, pp. 510–513,
    2017.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. Kim, W. Lee, 和 D.-H. Cho，“基于深度学习的OFDM系统新型PAPR减少方案，” *IEEE 通信快报*，第22卷，第3期，页码
    510–513，2017。'
- en: '[126] J. Vanschoren, “Meta-learning: A survey,” *arXiv preprint arXiv:1810.03548*,
    2018.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Vanschoren，“元学习：调查，” *arXiv 预印本 arXiv:1810.03548*，2018。'
- en: '[127] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 44,
    no. 9, pp. 5149–5169, 2021.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] T. Hospedales, A. Antoniou, P. Micaelli, 和 A. Storkey，“神经网络中的元学习：调查，”
    *IEEE 模式分析与机器智能学报*，第44卷，第9期，页码 5149–5169，2021。'
- en: '[128] L. Chen, S. T. Jose, I. Nikoloska, S. Park, T. Chen, O. Simeone *et al.*,
    “Learning with limited samples: Meta-learning and applications to communication
    systems,” *Foundations and Trends® in Signal Processing*, vol. 17, no. 2, pp.
    79–208, 2023.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] L. Chen, S. T. Jose, I. Nikoloska, S. Park, T. Chen, O. Simeone *等*，“有限样本学习：元学习及其在通信系统中的应用，”
    *信号处理基础与趋势®*，第17卷，第2期，页码 79–208，2023。'
- en: '[129] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proc. International Conference on Machine Learning (ICML)*, pp. 41–48, 2009.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Y. Bengio, J. Louradour, R. Collobert, 和 J. Weston，“课程学习，” 收录于 *国际机器学习会议（ICML）论文集*，页码
    41–48，2009。'
- en: '[130] X. Wang, Y. Chen, and W. Zhu, “A survey on curriculum learning,” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, vol. 44, no. 9, pp. 4555–4576, 2021.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] X. Wang, Y. Chen, 和 W. Zhu，“课程学习调查，” *IEEE 模式分析与机器智能学报*，第44卷，第9期，页码 4555–4576，2021。'
- en: '[131] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe, “Curriculum learning:
    A survey,” *International Journal of Computer Vision*, vol. 130, no. 6, pp. 1526–1565,
    2022.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] P. Soviany, R. T. Ionescu, P. Rota, 和 N. Sebe，“课程学习：调查，” *计算机视觉国际期刊*，第130卷，第6期，页码
    1526–1565，2022。'
- en: '[132] J. L. Elman, “Learning and development in neural networks: The importance
    of starting small,” *Cognition*, vol. 48, no. 1, pp. 71–99, 1993.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] J. L. Elman，“神经网络中的学习与发展：从小处开始的重要性，” *认知*，第48卷，第1期，页码 71–99，1993。'
- en: '[133] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern recognition*, vol. 77, pp. 354–377, 2018.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *等*，“卷积神经网络的最新进展，” *模式识别*，第77卷，页码 354–377，2018。'
- en: '[134] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional
    neural networks: Analysis, applications, and prospects,” *IEEE Trans. Neural Netw.
    Learn. Syst.*, vol. 33, no. 12, pp. 6999–7019, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Z. Li, F. Liu, W. Yang, S. Peng, 和 J. Zhou, “卷积神经网络综述: 分析, 应用, 和前景,”
    *IEEE神经网络与学习系统汇刊*, 第33卷，第12期，页码 6999–7019, 2021。'
- en: '[135] M. Krichen, “Convolutional neural networks: A survey,” *Computers*, vol. 12,
    no. 8, p. 151, 2023.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Krichen, “卷积神经网络: 一项调查,” *计算机*, 第12卷，第8期，页码 151, 2023。'
- en: '[136] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proc. Advances in Neural Information
    Processing Systems (NeurIPS)*, pp. 1097–1105, 2012.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行Imagenet分类,”
    发表在 *神经信息处理系统进展会议（NeurIPS）论文集*中, 页码 1097–1105, 2012。'
- en: '[137] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络,” *arXiv预印本 arXiv:1409.1556*,
    2014。'
- en: '[138] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proc. IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 1–9, 2015.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich, “通过卷积深入学习,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）论文集*中, 页码
    1–9, 2015。'
- en: '[139] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, pp. 770–778, 2016.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] K. He, X. Zhang, S. Ren, 和 J. Sun, “深度残差学习用于图像识别,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）论文集*中,
    页码 770–778, 2016。'
- en: '[140] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proc. IEEE conference on computer vision and pattern
    recognition (CVPR)*, pp. 4700–4708, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger, “密集连接的卷积网络,”
    发表在 *IEEE计算机视觉与模式识别会议（CVPR）论文集*中, 页码 4700–4708, 2017。'
- en: '[141] Y. Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction
    and classification of hyperspectral images based on convolutional neural networks,”
    *IEEE Trans. Geosci. Remote Sens.*, vol. 54, no. 10, pp. 6232–6251, 2016.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Chen, H. Jiang, C. Li, X. Jia, 和 P. Ghamisi, “基于卷积神经网络的高光谱图像深度特征提取与分类,”
    *IEEE地球科学与遥感汇刊*, 第54卷，第10期，页码 6232–6251, 2016。'
- en: '[142] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, “Deep learning-based
    channel estimation,” *IEEE Commun. Lett.*, vol. 23, no. 4, pp. 652–655, 2019.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. Soltani, V. Pourahmadi, A. Mirzaei, 和 H. Sheikhzadeh, “基于深度学习的信道估计,”
    *IEEE通信快报*, 第23卷，第4期，页码 652–655, 2019。'
- en: '[143] M. Honkala, D. Korpi, and J. M. Huttunen, “DeepRx: Fully convolutional
    deep learning receiver,” *IEEE Trans. Wireless Commun.*, vol. 20, no. 6, pp. 3925–3940,
    2021.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] M. Honkala, D. Korpi, 和 J. M. Huttunen, “DeepRx: 完全卷积深度学习接收器,” *IEEE无线通信汇刊*,
    第20卷，第6期，页码 3925–3940, 2021。'
- en: '[144] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Proc. Advances in Neural Information Processing Systems
    (NeurIPS)*, pp. 3104–3112, 2014.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] I. Sutskever, O. Vinyals, 和 Q. V. Le, “使用神经网络进行序列到序列学习,” 发表在 *神经信息处理系统进展会议（NeurIPS）论文集*中,
    页码 3104–3112, 2014。'
- en: '[145] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] S. Hochreiter 和 J. Schmidhuber, “长短期记忆,” *神经计算*, 第9卷，第8期，页码 1735–1780,
    1997。'
- en: '[146] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual
    prediction with LSTM,” *Neural Computation*, vol. 12, pp. 2451–2471, 2000.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] F. A. Gers, J. Schmidhuber, 和 F. Cummins, “学习遗忘: 使用LSTM的持续预测,” *神经计算*,
    第12卷，页码 2451–2471, 2000。'
- en: '[147] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using RNN encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk, 和 Y. Bengio, “使用RNN编码解码器学习短语表示用于统计机器翻译,” *arXiv预印本 arXiv:1406.1078*,
    2014。'
- en: '[148] Y. Yu, X. Si, C. Hu, and J. Zhang, “A review of recurrent neural networks:
    LSTM cells and network architectures,” *Neural computation*, vol. 31, no. 7, pp.
    1235–1270, 2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Yu, X. Si, C. Hu, 和 J. Zhang, “递归神经网络综述: LSTM单元和网络架构,” *神经计算*, 第31卷，第7期，页码
    1235–1270, 2019。'
- en: '[149] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee, “Recent
    advances in recurrent neural networks,” *arXiv preprint arXiv:1801.01078*, 2017.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, 和 S. Valaee, “递归神经网络的最新进展,”
    *arXiv预印本 arXiv:1801.01078*, 2017。'
- en: '[150] G. Van Houdt, C. Mosquera, and G. Nápoles, “A review on the long short-term
    memory model,” *Artificial Intelligence Review*, vol. 53, pp. 5929–5955, 2020.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] N. Farsad and A. Goldsmith, “Neural network detection of data sequences
    in communication systems,” *IEEE Trans. Signal Process.*, vol. 66, no. 21, pp.
    5663–5678, 2018.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Trans. Neural Netw.*, vol. 20, no. 1,
    pp. 61–80, 2008.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” *arXiv preprint arXiv:1810.00826*, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
    M. Sun, “Graph neural networks: A review of methods and applications,” *AI Open*,
    vol. 1, pp. 57–81, 2020.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive
    survey on graph neural networks,” *IEEE Trans. Neural Netw. Learn. Syst.*, vol. 32,
    no. 1, pp. 4–24, 2020.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” *arXiv preprint arXiv:1710.10903*, 2017.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” *arXiv preprint arXiv:2104.13478*,
    2021.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Proc. Advances in
    Neural Information Processing Systems (NeurIPS)*, vol. 30, 2017.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Proc. Advances in neural information processing systems (NeurIPS)*, vol. 33,
    pp. 1877–1901, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “LLaMA: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, “PaLM: Scaling language
    modeling with pathways,” *Journal of Machine Learning Research*, vol. 24, no.
    240, pp. 1–113, 2023.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth *et al.*, “Gemini: a family of highly capable
    multimodal models,” *arXiv preprint arXiv:2312.11805*, 2023.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,” *AI
    Open*, vol. 3, pp. 111–132, 2022.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, “Transformers
    in vision: A survey,” *ACM computing surveys (CSUR)*, vol. 54, no. 10s, pp. 1–41,
    2022.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
    C. Xu, Y. Xu *et al.*, “A survey on vision transformer,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 45, no. 1, pp. 87–110, 2022.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
    C. Xu, Y. Xu *等*，“视觉变换器综述，” *IEEE 模式分析与机器智能学报*，第 45 卷，第 1 期，第 87–110 页，2022 年。'
- en: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth $16\times
    16$ words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.
    Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像价值 $16\times
    16$ 个词：大规模图像识别的变换器，” *arXiv 预印本 arXiv:2010.11929*，2020 年。'
- en: '[168] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
    “Robust speech recognition via large-scale weak supervision,” in *Proc. International
    Conference on Machine Learning (ICML)*, pp. 28 492–28 518, 2023.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey 和 I. Sutskever，“通过大规模弱监督实现鲁棒的语音识别，”
    见 *国际机器学习会议（ICML）*，第 28,492–28,518 页，2023 年。'
- en: '[169] Y. Wang, Z. Gao, D. Zheng, S. Chen, D. Gunduz, and H. V. Poor, “Transformer-empowered
    6G intelligent networks: From massive MIMO processing to semantic communication,”
    *IEEE Wireless Communications*, vol. 30, no. 6, pp. 127–135, 2022.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Wang, Z. Gao, D. Zheng, S. Chen, D. Gunduz 和 H. V. Poor，“变换器增强的 6G
    智能网络：从大规模 MIMO 处理到语义通信，” *IEEE 无线通信*，第 30 卷，第 6 期，第 127–135 页，2022 年。'
- en: '[170] Z. Chang, G. A. Koulieris, and H. P. Shum, “On the design fundamentals
    of diffusion models: A survey,” *arXiv preprint arXiv:2306.04542*, 2023.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Z. Chang, G. A. Koulieris 和 H. P. Shum，“扩散模型设计基础：综述，” *arXiv 预印本 arXiv:2306.04542*，2023
    年。'
- en: '[171] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui,
    and M.-H. Yang, “Diffusion models: A comprehensive survey of methods and applications,”
    *ACM Computing Surveys*, vol. 56, no. 4, pp. 1–39, 2023.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui
    和 M.-H. Yang，“扩散模型：方法和应用的全面综述，” *ACM 计算机调查*，第 56 卷，第 4 期，第 1–39 页，2023 年。'
- en: '[172] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li, “A
    survey on generative diffusion models,” *IEEE Trans. Knowl. Data Eng.*, 2024.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng 和 S. Z. Li，“生成扩散模型综述，”
    *IEEE 知识与数据工程学报*，2024 年。'
- en: '[173] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Proc. Advances in Neural Information Processing Systems (NeurIPS)*, vol. 33,
    pp. 6840–6851, 2020.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] J. Ho, A. Jain 和 P. Abbeel，“去噪扩散概率模型，” *神经信息处理系统会议（NeurIPS）*，第 33 卷，第
    6840–6851 页，2020 年。'
- en: '[174] B. Fesl, M. Baur, F. Strasser, M. Joham, and W. Utschick, “Diffusion-based
    generative prior for low-complexity MIMO channel estimation,” *arXiv preprint
    arXiv:2403.03545*, 2024.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] B. Fesl, M. Baur, F. Strasser, M. Joham 和 W. Utschick，“用于低复杂度 MIMO 通道估计的基于扩散的生成先验，”
    *arXiv 预印本 arXiv:2403.03545*，2024 年。'
- en: '[175] M. Letafati, S. Ali, and M. Latva-aho, “Denoising diffusion probabilistic
    models for hardware-impaired communications,” *arXiv preprint arXiv:2309.08568*,
    2023.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] M. Letafati, S. Ali 和 M. Latva-aho，“用于硬件受限通信的去噪扩散概率模型，” *arXiv 预印本 arXiv:2309.08568*，2023
    年。'
- en: '[176] ——, “Probabilistic constellation shaping with denoising diffusion probabilistic
    models: A novel approach,” *arXiv preprint arXiv:2309.08688*, 2023.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] ——，“使用去噪扩散概率模型进行概率星座塑造：一种新方法，” *arXiv 预印本 arXiv:2309.08688*，2023 年。'
- en: '[177] M. Kim, R. Fritschek, and R. F. Schaefer, “Learning end-to-end channel
    coding with diffusion models,” in *Proc. 26th International ITG Workshop on Smart
    Antennas and 13th Conference on Systems, Communications, and Coding (WSA & SCC)*,
    pp. 1–6, 2023.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] M. Kim, R. Fritschek 和 R. F. Schaefer，“通过扩散模型学习端到端通道编码，” 见 *第 26 届国际
    ITG 智能天线研讨会和第 13 届系统、通信与编码会议（WSA & SCC）*，第 1–6 页，2023 年。'
- en: '[178] T. Wu, Z. Chen, D. He, L. Qian, Y. Xu, M. Tao, and W. Zhang, “CDDM: Channel
    denoising diffusion models for wireless communications,” *IEEE Trans. Wireless
    Commun.*, 2024.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] T. Wu, Z. Chen, D. He, L. Qian, Y. Xu, M. Tao 和 W. Zhang，“CDDM：用于无线通信的通道去噪扩散模型，”
    *IEEE 无线通信学报*，2024 年。'
- en: '[179] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang,
    Z. Xiong, S. Cui *et al.*, “Beyond deep reinforcement learning: A tutorial on
    generative diffusion models in network optimization,” *arXiv preprint arXiv:2308.05384*,
    2023.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang,
    Z. Xiong, S. Cui *等*，“超越深度强化学习：网络优化中的生成扩散模型教程，” *arXiv 预印本 arXiv:2308.05384*，2023
    年。'
- en: '[180] L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader, and M. Debbah, “Large
    generative AI models for telecom: The next big thing?” *IEEE Commun. Mag.*, 2024.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader 和 M. Debbah，“电信领域的大型生成
    AI 模型：下一个重大突破？” *IEEE 通信杂志*，2024 年。'
- en: '[181] M. Letafati, S. Ali, and M. Latva-aho, “WiGenAI: The symphony of wireless
    and generative AI via diffusion models,” *arXiv preprint arXiv:2310.07312*, 2023.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] M. Letafati, S. Ali, 和 M. Latva-aho，“WiGenAI：通过扩散模型的无线和生成式 AI 的交响曲”，
    *arXiv 预印本 arXiv:2310.07312*，2023年。'
- en: '[182] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D. I.
    Kim, and K. B. Letaief, “Generative AI for physical layer communications: A survey,”
    *arXiv preprint arXiv:2312.05594*, 2023.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D.
    I. Kim, 和 K. B. Letaief，“用于物理层通信的生成式 AI：综述”， *arXiv 预印本 arXiv:2312.05594*，2023年。'
- en: '[183] C. Zhao, H. Du, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, K. B. Letaief
    *et al.*, “Generative AI for secure physical layer communications: A survey,”
    *arXiv preprint arXiv:2402.13553*, 2024.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] C. Zhao, H. Du, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, K. B. Letaief
    *等*， “用于安全物理层通信的生成式 AI：综述”， *arXiv 预印本 arXiv:2402.13553*，2024年。'
- en: '[184] M. Ebada, S. Cammerer, A. Elkelesh, and S. ten Brink, “Deep learning-based
    polar code design,” in *Proc. 57th Annual Allerton Conference on Communication,
    Control, and Computing (Allerton)*, pp. 177–183, 2019.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] M. Ebada, S. Cammerer, A. Elkelesh, 和 S. ten Brink，“基于深度学习的极化码设计”，在 *第57届年度
    Allerton 通信、控制与计算会议（Allerton）*，第177–183页，2019年。'
- en: '[185] L. Huang, H. Zhang, R. Li, Y. Ge, and J. Wang, “AI coding: Learning to
    construct error correction codes,” *IEEE Trans. Commun.*, vol. 68, no. 1, pp.
    26–39, 2019.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. Huang, H. Zhang, R. Li, Y. Ge, 和 J. Wang，“AI 编码：学习构建错误修正码”， *IEEE
    通信学报*，第68卷，第1期，第26–39页，2019年。'
- en: '[186] M. Leonardon and V. Gripon, “Using deep neural networks to predict and
    improve the performance of polar codes,” in *Proc. 11th International Symposium
    on Topics in Coding (ISTC)*, pp. 1–5, 2021.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] M. Leonardon 和 V. Gripon，“使用深度神经网络预测和改进极化码性能”，在 *第11届编码专题国际研讨会（ISTC）*，第1–5页，2021年。'
- en: '[187] Y. Liao, S. A. Hashemi, H. Yang, and J. M. Cioffi, “Scalable polar code
    construction for successive cancellation list decoding: A graph neural network-based
    approach,” *IEEE Trans. Commun.*, vol. 71, no. 11, pp. 6231–6245, 2023.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Y. Liao, S. A. Hashemi, H. Yang, 和 J. M. Cioffi，“用于连续取消列表解码的可扩展极化码构造：一种基于图神经网络的方法”，
    *IEEE 通信学报*，第71卷，第11期，第6231–6245页，2023年。'
- en: '[188] V. Miloslavskaya, Y. Li, and B. Vucetic, “Neural network based adaptive
    polar coding,” *IEEE Trans. Commun.*, 2023.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] V. Miloslavskaya, Y. Li, 和 B. Vucetic，“基于神经网络的自适应极化编码”， *IEEE 通信学报*，2023年。'
- en: '[189] Y. Li, Z. Chen, G. Liu, Y.-C. Wu, and K.-K. Wong, “Learning to construct
    nested polar codes: An attention-based set-to-element model,” *IEEE Commun. Lett.*,
    vol. 25, no. 12, pp. 3898–3902, 2021.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Y. Li, Z. Chen, G. Liu, Y.-C. Wu, 和 K.-K. Wong，“学习构建嵌套极化码：一种基于注意力的集合到元素模型”，
    *IEEE 通信快报*，第25卷，第12期，第3898–3902页，2021年。'
- en: '[190] S. K. Ankireddy, S. A. Hebbar, H. Wan, J. Cho, and C. Zhang, “Nested
    construction of polar codes via Transformers,” *arXiv preprint arXiv:2401.17188*,
    2024.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. K. Ankireddy, S. A. Hebbar, H. Wan, J. Cho, 和 C. Zhang，“通过 Transformers
    嵌套构造极化码”， *arXiv 预印本 arXiv:2401.17188*，2024年。'
- en: '[191] S. A. Hebbar, S. K. Ankireddy, H. Kim, S. Oh, and P. Viswanath, “DeepPolar:
    Inventing nonlinear large-kernel polar codes via deep learning,” *arXiv preprint
    arXiv:2402.08864*, 2024.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] S. A. Hebbar, S. K. Ankireddy, H. Kim, S. Oh, 和 P. Viswanath，“DeepPolar：通过深度学习发明非线性大核极化码”，
    *arXiv 预印本 arXiv:2402.08864*，2024年。'
- en: '[192] S. K. Mishra, D. Katyal, and S. A. Ganapathi, “A modified Q-learning
    algorithm for rate-profiling of polarization adjusted convolutional (PAC) codes,”
    in *Proc. IEEE Wireless Communications and Networking Conference (WCNC)*, pp.
    2363–2368, 2022.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] S. K. Mishra, D. Katyal, 和 S. A. Ganapathi，“用于极化调整卷积（PAC）码的速率轮廓的改进 Q
    学习算法”，在 *IEEE 无线通信与网络会议（WCNC）*，第2363–2368页，2022年。'
- en: '[193] T. J. Richardson, M. A. Shokrollahi, and R. L. Urbanke, “Design of capacity-approaching
    irregular low-density parity-check codes,” *IEEE Trans. Inf. Theory*, vol. 47,
    no. 2, pp. 619–637, 2001.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] T. J. Richardson, M. A. Shokrollahi, 和 R. L. Urbanke，“接近容量的不规则低密度奇偶校验码设计”，
    *IEEE 信息理论学报*，第47卷，第2期，第619–637页，2001年。'
- en: '[194] R. Mori and T. Tanaka, “Performance of polar codes with the construction
    using density evolution,” *IEEE Commun. Lett.*, vol. 13, no. 7, pp. 519–521, 2009.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] R. Mori 和 T. Tanaka，“使用密度演化构造的极化码性能”， *IEEE 通信快报*，第13卷，第7期，第519–521页，2009年。'
- en: '[195] S. ten Brink, “Convergence behavior of iteratively decoded parallel concatenated
    codes,” *IEEE Trans. Commun.*, vol. 49, no. 10, pp. 1727–1737, Oct. 2001.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] S. ten Brink，“迭代解码并行级联码的收敛行为”， *IEEE 通信学报*，第49卷，第10期，第1727–1737页，2001年10月。'
- en: '[196] E. Nisioti and N. Thomos, “Design of capacity-approaching low-density
    parity-check codes using recurrent neural networks,” *arXiv preprint arXiv:2001.01249*,
    2020.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] E. Nisioti 和 N. Thomos，“使用递归神经网络设计接近容量的低密度奇偶校验码”， *arXiv 预印本 arXiv:2001.01249*，2020年。'
- en: '[197] I. Tal and A. Vardy, “How to construct polar codes,” *IEEE Trans. Inf.
    Theory*, vol. 59, no. 10, pp. 6562–6582, 2013.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] I. Tal 和 A. Vardy，“如何构造极化码”，发表于 *IEEE 信息理论汇刊*，第 59 卷，第 10 期，第 6562–6582
    页，2013 年。'
- en: '[198] P. Trifonov, “Efficient design and decoding of polar codes,” *IEEE Trans.
    Commun.*, vol. 60, no. 11, pp. 3221–3227, 2012.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] P. Trifonov，“极化码的高效设计与解码”，发表于 *IEEE 通信汇刊*，第 60 卷，第 11 期，第 3221–3227 页，2012
    年。'
- en: '[199] H. Ochiai, P. Mitran, and H. V. Poor, “Capacity-approaching polar codes
    with long codewords and successive cancellation decoding based on improved gaussian
    approximation,” *IEEE Trans. Commun.*, vol. 69, no. 1, pp. 31–43, 2021.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] H. Ochiai、P. Mitran 和 H. V. Poor，“基于改进高斯近似的容量接近极化码，长码字和逐次取消解码”，发表于 *IEEE
    通信汇刊*，第 69 卷，第 1 期，第 31–43 页，2021 年。'
- en: '[200] H. Ochiai, K. Ikeya, and P. Mitran, “A new polar code design based on
    reciprocal channel approximation,” *IEEE Trans. Commun.*, vol. 71, no. 2, pp.
    631–643, 2023.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] H. Ochiai、K. Ikeya 和 P. Mitran，“基于倒数信道近似的新极化码设计”，发表于 *IEEE 通信汇刊*，第 71
    卷，第 2 期，第 631–643 页，2023 年。'
- en: '[201] I. Tal and A. Vardy, “List decoding of polar codes,” *IEEE Trans. Inf.
    Theory*, vol. 61, no. 5, pp. 2213–2226, May 2015.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] I. Tal 和 A. Vardy，“极化码的列表解码”，发表于 *IEEE 信息理论汇刊*，第 61 卷，第 5 期，第 2213–2226
    页，2015 年 5 月。'
- en: '[202] L. Huang, H. Zhang, R. Li, Y. Ge, and J. Wang, “Reinforcement learning
    for nested polar code construction,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2019.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] L. Huang、H. Zhang、R. Li、Y. Ge 和 J. Wang，“用于嵌套极化码构造的强化学习”，发表于 *IEEE 全球通信大会（GLOBECOM）*，第
    1–6 页，2019 年。'
- en: '[203] Y. Liao, S. A. Hashemi, J. M. Cioffi, and A. Goldsmith, “Construction
    of polar codes with reinforcement learning,” *IEEE Trans. Commun.*, vol. 70, no. 1,
    pp. 185–198, 2021.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Y. Liao、S. A. Hashemi、J. M. Cioffi 和 A. Goldsmith，“基于强化学习的极化码构造”，发表于
    *IEEE 通信汇刊*，第 70 卷，第 1 期，第 185–198 页，2021 年。'
- en: '[204] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” *arXiv preprint arXiv:1706.06083*,
    2017.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] A. Madry、A. Makelov、L. Schmidt、D. Tsipras 和 A. Vladu，“朝着对抗攻击具有抵抗力的深度学习模型”，*arXiv
    预印本 arXiv:1706.06083*，2017 年。'
- en: '[205] V. Miloslavskaya, Y. Li, and B. Vucetic, “Design of compactly specified
    polar codes with dynamic frozen bits based on reinforcement learning,” *IEEE Trans.
    Commun.*, vol. 72, no. 3, pp. 1257–1272, 2024.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] V. Miloslavskaya、Y. Li 和 B. Vucetic，“基于强化学习的具有动态冻结位的紧凑指定极化码的设计”，发表于 *IEEE
    通信汇刊*，第 72 卷，第 3 期，第 1257–1272 页，2024 年。'
- en: '[206] C. Schürch, “A partial order for the synthesized channels of a polar
    code,” in *Proc. IEEE International Symposium on Information Theory (ISIT)*, pp.
    220–224, 2016.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] C. Schürch，“极化码合成信道的部分序”，发表于 *IEEE 国际信息理论研讨会（ISIT）*，第 220–224 页，2016
    年。'
- en: '[207] M. Bardet, V. Dragoi, A. Otmani, and J.-P. Tillich, “Algebraic properties
    of polar codes from a new polynomial formalism,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 230–234, 2016.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] M. Bardet、V. Dragoi、A. Otmani 和 J.-P. Tillich，“一种新多项式形式下的极化码代数性质”，发表于
    *IEEE 国际信息理论研讨会（ISIT）*，第 230–234 页，2016 年。'
- en: '[208] G. He, J.-C. Belfiore, I. Land, G. Yang, X. Liu, Y. Chen, R. Li, J. Wang,
    Y. Ge, R. Zhang *et al.*, “Beta-expansion: A theoretical framework for fast and
    recursive construction of polar codes,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] G. He、J.-C. Belfiore、I. Land、G. Yang、X. Liu、Y. Chen、R. Li、J. Wang、Y.
    Ge、R. Zhang *等*，“Beta-expansion：极化码快速递归构造的理论框架”，发表于 *IEEE 全球通信大会（GLOBECOM）*，第
    1–6 页，2017 年。'
- en: '[209] M. Mondelli, S. H. Hassani, and R. Urbanke, “Construction of polar codes
    with sublinear complexity,” *IEEE Trans. Inf. Theory*, 2018.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] M. Mondelli、S. H. Hassani 和 R. Urbanke，“具有亚线性复杂度的极化码构造”，发表于 *IEEE 信息理论汇刊*，2018
    年。'
- en: '[210] V. Bioglio, C. Condo, and I. Land, “Design of polar codes in 5G new radio,”
    *IEEE Communications Surveys & Tutorials*, vol. 23, no. 1, pp. 29–40, 2020.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] V. Bioglio、C. Condo 和 I. Land，“5G 新无线中的极化码设计”，发表于 *IEEE 通信调查与教程*，第 23
    卷，第 1 期，第 29–40 页，2020 年。'
- en: '[211] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” *Proc. Advances
    in Neural Information Processing Systems (NeurIPS)*, vol. 12, 1999.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] V. Konda 和 J. Tsitsiklis，“演员-评论员算法”，发表于 *神经信息处理系统进展（NeurIPS）*，第 12 卷，1999
    年。'
- en: '[212] S. B. Korada, E. Şaşoğlu, and R. Urbanke, “Polar codes: Characterization
    of exponent, bounds, and constructions,” *IEEE Trans. Inf. Theory*, vol. 56, no. 12,
    pp. 6253–6264, 2010.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] S. B. Korada、E. Şaşoğlu 和 R. Urbanke，“极化码：指数、界限和构造的特征”，发表于 *IEEE 信息理论汇刊*，第
    56 卷，第 12 期，第 6253–6264 页，2010 年。'
- en: '[213] E. Arıkan, “From sequential decoding to channel polarization and back
    again,” *arXiv preprint arXiv:1908.09594*, 2019.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] E. Arıkan，“从顺序解码到信道极化再到原点”，*arXiv 预印本 arXiv:1908.09594*，2019 年。'
- en: '[214] M. Moradi and A. Mozammel, “A monte-carlo based construction of polarization-adjusted
    convolutional (PAC) codes,” *arXiv preprint arXiv:2106.08118*, 2021.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] M. Moradi 和 A. Mozammel， “基于蒙特卡罗的极化调整卷积（PAC）码的构造”，*arXiv预印本 arXiv:2106.08118*，2021年。'
- en: '[215] T. Gruber, S. Cammerer, J. Hoydis, and S. ten Brink, “On deep learning-based
    channel decoding,” in *Proc. 51st Annual Conference on Information Sciences and
    Systems (CISS)*, pp. 1–6, 2017.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] T. Gruber、S. Cammerer、J. Hoydis 和 S. ten Brink， “关于基于深度学习的信道解码”，见于*第51届信息科学与系统年会（CISS）*，第1–6页，2017年。'
- en: '[216] J. Seo, J. Lee, and K. Kim, “Decoding of polar code by using deep feed-forward
    neural networks,” in *Proc. International Conference on Computing, Networking
    and Communications (ICNC)*, pp. 238–242, 2018.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] J. Seo、J. Lee 和 K. Kim， “利用深度前馈神经网络的极化码解码”，见于*国际计算、网络与通信会议（ICNC）*，第238–242页，2018年。'
- en: '[217] C. T. Leung, R. V. Bhat, and M. Motani, “Low-latency neural decoders
    for linear and non-linear block codes,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2019.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] C. T. Leung、R. V. Bhat 和 M. Motani， “用于线性和非线性块码的低延迟神经解码器”，见于*IEEE全球通信会议（GLOBECOM）*，第1–6页，2019年。'
- en: '[218] ——, “Low latency energy-efficient neural decoders for block codes,” *IEEE
    Trans. Green Commun. Netw.*, vol. 7, no. 2, pp. 680–691, 2023.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] ——， “低延迟能效神经解码器用于块码”，*IEEE绿色通信与网络学报*，第7卷，第2期，第680–691页，2023年。'
- en: '[219] W. Lyu, Z. Zhang, C. Jiao, K. Qin, and H. Zhang, “Performance evaluation
    of channel decoding with deep neural networks,” in *Proc. IEEE International Conference
    on Communications (ICC)*, pp. 1–6, 2018.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] W. Lyu、Z. Zhang、C. Jiao、K. Qin 和 H. Zhang， “基于深度神经网络的信道解码性能评估”，见于*IEEE国际通信会议（ICC）*，第1–6页，2018年。'
- en: '[220] R. Sattiraju, A. Weinand, and H. D. Schotten, “Performance analysis of
    deep learning based on recurrent neural networks for channel coding,” in *Proc.
    IEEE International Conference on Advanced Networks and Telecommunications Systems
    (ANTS)*, pp. 1–6, 2018.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] R. Sattiraju、A. Weinand 和 H. D. Schotten， “基于递归神经网络的深度学习性能分析用于信道编码”，见于*IEEE国际先进网络与通信系统会议（ANTS）*，第1–6页，2018年。'
- en: '[221] H. Zhu, Z. Cao, Y. Zhao, and D. Li, “Learning to denoise and decode:
    A novel residual neural network decoder for polar codes,” *IEEE Trans. Veh. Technol.*,
    vol. 69, no. 8, pp. 8725–8738, 2020.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] H. Zhu、Z. Cao、Y. Zhao 和 D. Li， “学习去噪和解码：一种新型的残差神经网络解码器用于极化码”，*IEEE车辆技术学报*，第69卷，第8期，第8725–8738页，2020年。'
- en: '[222] Y. Choukroun and L. Wolf, “Error correction code transformer,” *Proc.
    Advances in Neural Information Processing Systems (NeurIPS)*, vol. 35, pp. 38 695–38 705,
    2022.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Y. Choukroun 和 L. Wolf， “错误纠正码变换器”，见于*神经信息处理系统进展（NeurIPS）*，第35卷，第38,695–38,705页，2022年。'
- en: '[223] ——, “Denoising diffusion error correction codes,” *arXiv preprint arXiv:2209.13533*,
    2022.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] ——， “去噪扩散误差纠正码”，*arXiv预印本 arXiv:2209.13533*，2022年。'
- en: '[224] A. Bennatan, Y. Choukroun, and P. Kisilev, “Deep learning for decoding
    of linear codes-a syndrome-based approach,” in *Proc. IEEE International Symposium
    on Information Theory (ISIT)*, pp. 1595–1599, 2018.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] A. Bennatan、Y. Choukroun 和 P. Kisilev， “深度学习用于解码线性码——基于综合症的方法”，见于*IEEE国际信息理论研讨会（ISIT）*，第1595–1599页，2018年。'
- en: '[225] J. K. S. Kamassury and D. Silva, “Iterative error decimation for syndrome-based
    neural network decoders,” *arXiv preprint arXiv:2012.00089*, 2020.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] J. K. S. Kamassury 和 D. Silva， “基于综合症的神经网络解码器的迭代错误消除”，*arXiv预印本 arXiv:2012.00089*，2020年。'
- en: '[226] D. Artemasov, K. Andreev, P. Rybin, and A. Frolov, “Soft-output deep
    neural network-based decoding,” *arXiv preprint arXiv:2304.08868*, 2023.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] D. Artemasov、K. Andreev、P. Rybin 和 A. Frolov， “软输出深度神经网络解码”，*arXiv预印本
    arXiv:2304.08868*，2023年。'
- en: '[227] Y. Wang, Z. Zhang, S. Zhang, S. Cao, and S. Xu, “A unified deep learning
    based polar-ldpc decoder for 5G communication systems,” in *Proc. 10th International
    Conference on Wireless Communications and Signal Processing (WCSP)*, pp. 1–6,
    2018.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Y. Wang、Z. Zhang、S. Zhang、S. Cao 和 S. Xu， “基于深度学习的统一极化- LDPC解码器用于5G通信系统”，见于*第10届国际无线通信与信号处理会议（WCSP）*，第1–6页，2018年。'
- en: '[228] Y. Jiang, H. Kim, H. Asnani, and S. Kannan, “Mind: Model independent
    neural decoder,” in *Proc. IEEE International Workshop on Signal Processing Advances
    in Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] Y. Jiang、H. Kim、H. Asnani 和 S. Kannan， “Mind：模型无关神经解码器”，见于*IEEE国际无线通信信号处理研讨会（SPAWC）*，第1–5页，2019年。'
- en: '[229] H. Lee, E. Y. Seo, H. Ju, and S.-H. Kim, “On training neural network
    decoders of rate compatible polar codes via transfer learning,” *Entropy*, vol. 22,
    no. 5, p. 496, 2020.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] H. Lee、E. Y. Seo、H. Ju 和 S.-H. Kim， “关于通过迁移学习训练速率兼容极化码的神经网络解码器”，*熵*，第22卷，第5期，第496页，2020年。'
- en: '[230] D. Artemasov, K. Andreev, and A. Frolov, “On a unified deep neural network
    decoding architecture,” in *Proc. IEEE 98th Vehicular Technology Conference (VTC2023-Fall)*,
    pp. 1–5, 2023.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] D. Artemasov, K. Andreev, 和 A. Frolov，“关于统一的深度神经网络解码架构，” 见 *IEEE 第98届车辆技术会议
    (VTC2023-Fall)*，第1–5页，2023年。'
- en: '[231] F. Carpi, C. Häger, M. Martalò, R. Raheli, and H. D. Pfister, “Reinforcement
    learning for channel coding: Learned bit-flipping decoding,” in *Proc. 57th Annual
    Allerton Conference on Communication, Control, and Computing (Allerton)*, pp.
    922–929, 2019.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] F. Carpi, C. Häger, M. Martalò, R. Raheli, 和 H. D. Pfister，“信道编码的强化学习：学习的位翻转解码，”
    见 *第57届 Allerton 通信、控制与计算年会 (Allerton)*，第922–929页，2019年。'
- en: '[232] J. Gao and K. Niu, “A reinforcement learning based decoding method of
    short polar codes,” in *Proc. IEEE Wireless Communications and Networking Conference
    Workshops (WCNCW)*, pp. 1–6, 2021.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] J. Gao 和 K. Niu，“基于强化学习的短极化码解码方法，” 见 *IEEE 无线通信与网络会议研讨会 (WCNCW)*，第1–6页，2021年。'
- en: '[233] E. Kavvousanos, V. Paliouras, and I. Kouretas, “Simplified deep-learning-based
    decoders for linear block codes,” in *Proc. IEEE International Conference on Electronics,
    Circuits and Systems (ICECS)*, pp. 769–772, 2018.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] E. Kavvousanos, V. Paliouras, 和 I. Kouretas，“简化的基于深度学习的线性块码解码器，” 见 *IEEE
    国际电子、电路与系统会议 (ICECS)*，第769–772页，2018年。'
- en: '[234] E. Kavvousanos and V. Paliouras, “Hardware implementation aspects of
    a syndrome-based neural network decoder for BCH codes,” in *Proc. IEEE Nordic
    Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium
    of System-on-Chip (SoC)*, pp. 1–6, 2019.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] E. Kavvousanos 和 V. Paliouras，“基于综合的BCH码的综合症神经网络解码器的硬件实现方面，” 见 *IEEE
    北欧电路与系统会议 (NORCAS): NORCHIP 和系统芯片国际研讨会 (SoC)*，第1–6页，2019年。'
- en: '[235] ——, “Optimizing deep learning decoders for FPGA implementation,” in *Proc.
    31st International Conference on Field-Programmable Logic and Applications (FPL)*,
    pp. 271–272, 2021.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] ——，“优化深度学习解码器以实现 FPGA 实现，” 见 *第31届国际现场可编程逻辑与应用会议 (FPL)*，第271–272页，2021年。'
- en: '[236] ——, “A low-latency syndrome-based deep learning decoder architecture
    and its FPGA implementation,” in *Proc. 11th International Conference on Modern
    Circuits and Systems Technologies (MOCAST)*, pp. 1–4, 2022.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] ——，“低延迟综合症基深度学习解码器架构及其 FPGA 实现，” 见 *第11届现代电路与系统技术国际会议 (MOCAST)*，第1–4页，2022年。'
- en: '[237] B. Cavarec, H. B. Celebi, M. Bengtsson, and M. Skoglund, “A learning-based
    approach to address complexity-reliability tradeoff in OS decoders,” in *Proc.
    54th Asilomar Conference on Signals, Systems, and Computers*, pp. 689–692, 2020.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] B. Cavarec, H. B. Celebi, M. Bengtsson, 和 M. Skoglund，“基于学习的方法解决 OS 解码器中的复杂性-可靠性权衡，”
    见 *第54届 Asilomar 信号、系统与计算机会议*，第689–692页，2020年。'
- en: '[238] N. Raviv, A. Caciularu, T. Raviv, J. Goldberger, and Y. Be’ery, “perm2vec:
    Graph permutation selection for decoding of error correction codes using self-attention,”
    *IEEE J. Sel. Areas Commun.*, vol. 39, no. 1, pp. 79–88, 2020.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] N. Raviv, A. Caciularu, T. Raviv, J. Goldberger, 和 Y. Be’ery，“perm2vec：用于纠错码解码的图排列选择，使用自注意力机制，”
    *IEEE 选择通信领域期刊*，第39卷，第1期，第79–88页，2020年。'
- en: '[239] A. Kurmukova and D. Gunduz, “Friendly attacks to improve channel coding
    reliability,” *arXiv preprint arXiv:2401.14184*, 2024.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] A. Kurmukova 和 D. Gunduz，“友好攻击以提高信道编码的可靠性，” *arXiv 预印本 arXiv:2401.14184*，2024年。'
- en: '[240] A. Tsvieli and N. Weinberger, “Learning maximum margin channel decoders,”
    *IEEE Trans. Inf. Theory*, vol. 69, no. 6, pp. 3597–3626, 2023.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] A. Tsvieli 和 N. Weinberger，“学习最大边际信道解码器，” *IEEE 信息理论期刊*，第69卷，第6期，第3597–3626页，2023年。'
- en: '[241] X. Zhong, K. Cai, Z. Mei, and T. Q. Quek, “Deep learning-based decoding
    of linear block codes for spin-torque transfer magnetic random access memory,”
    *IEEE Transactions on Magnetics*, vol. 57, no. 2, pp. 1–5, 2020.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] X. Zhong, K. Cai, Z. Mei, 和 T. Q. Quek，“基于深度学习的线性块码解码用于自旋转矩磁性随机存取存储器，”
    *IEEE 磁性期刊*，第57卷，第2期，第1–5页，2020年。'
- en: '[242] X. Zhong, K. Cai, P. Kang, G. Song, and B. Dai, “Deep learning-based
    adaptive error-correction decoding for spin-torque transfer magnetic random access
    memory (STT-MRAM),” *IEEE Transactions on Magnetics*, vol. 59, no. 11, 2023.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] X. Zhong, K. Cai, P. Kang, G. Song, 和 B. Dai，“基于深度学习的自适应错误修正解码用于自旋转矩磁性随机存取存储器
    (STT-MRAM)，” *IEEE 磁性期刊*，第59卷，第11期，2023年。'
- en: '[243] C. T. Leung, R. V. Bhat, and M. Motani, “Multi-label neural decoders
    for block codes,” in *Proc. IEEE International Conference on Communications (ICC)*,
    pp. 1–6, 2020.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] C. T. Leung, R. V. Bhat, 和 M. Motani，“用于块码的多标签神经解码器，” 见 *IEEE 国际通信会议
    (ICC)*，第1–6页，2020年。'
- en: '[244] H. Kim, Y. Jiang, R. Rana, S. Kannan, S. Oh, and P. Viswanath, “Communication
    algorithms via deep learning,” in *Proc. International Conference on Learning
    Representations (ICLR)*, pp. 1–17, 2018.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Z. Cao, H. Zhu, Y. Zhao, and D. Li, “Learning to denoise and decode:
    A novel residual neural network decoder for polar codes,” in *Proc. IEEE 92nd
    Vehicular Technology Conference (VTC2020-Fall)*, pp. 1–6, 2020.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] S.-J. Park, H.-Y. Kwak, S.-H. Kim, S. Kim, Y. Kim, and J.-S. No, “How
    to mask in error correction code Transformer: Systematic and double masking,”
    *arXiv preprint arXiv:2308.08128*, 2023.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Y. Choukroun and L. Wolf, “A foundation model for error correction codes,”
    in *Proc. International Conference on Machine Learning (ICML)*, 2024.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill *et al.*, “On the opportunities
    and risks of foundation models,” *arXiv preprint arXiv:2108.07258*, 2021.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] F. J. MacWilliams and N. J. A. Sloane, *The Theory of Error-Correcting
    Codes*.   Elsevier, 1977, vol. 16.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] I. Dimnik and Y. Be’ery, “Improved random redundant iterative HDPC decoding,”
    *IEEE Trans. Commun.*, vol. 57, no. 7, pp. 1982–1985, 2009.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] L. Tallini and P. Cull, “Neural nets for decoding error-correcting codes,”
    in *IEEE Technical applications conference and workshops*, p. 89, 1995.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] R. M. Pyndiah, “Near-optimum decoding of product codes: Block turbo codes,”
    *IEEE Trans. Commun.*, vol. 46, no. 8, pp. 1003–1010, 1998.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] M. Zhu and S. Gupta, “To prune, or not to prune: exploring the efficacy
    of pruning for model compression,” *arXiv preprint arXiv:1710.01878*, 2017.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] M. P. C. Fossorier and S. Lin, “Soft-decision decoding of linear block
    codes based on ordered statistics,” *IEEE Trans. Inf. Theory*, vol. 41, no. 5,
    pp. 1379–1396, Sep. 1995.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] A. Elkelesh, M. Ebada, S. Cammerer, and S. ten Brink, “Belief propagation
    list decoding of polar codes,” *IEEE Commun. Lett.*, vol. 22, no. 8, pp. 1536–1539,
    2018.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] A. Kurakin, I. J. Goodfellow, and S. Bengio, *Adversarial examples in
    the physical world*.   Chapman and Hall/CRC, 2018.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] E. Nachmani, Y. Be’ery, and D. Burshtein, “Learning to decode linear
    codes using deep learning,” in *Proc. 54th Annual Allerton Conference on Communication,
    Control, and Computing (Allerton)*, pp. 341–346, 2016.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and
    Y. Be’ery, “Deep learning methods for improved decoding of linear codes,” *IEEE
    J. Sel. Topics Signal Process.*, vol. 12, no. 1, pp. 119–131, 2018.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] L. Lugosch and W. J. Gross, “Neural offset min-sum decoding,” in *Proc.
    IEEE International Symposium on Information Theory (ISIT)*, pp. 1361–1365, 2017.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] B. Dai, R. Liu, and Z. Yan, “New min-sum decoders based on deep learning
    for polar codes,” in *Proc. IEEE International Workshop on Signal Processing Systems
    (SiPS)*, pp. 252–257, 2018.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] H. Yu, M.-M. Zhao, M. Lei, and M.-J. Zhao, “Neural adjusted min-sum decoding
    for LDPC codes,” in *Proc. IEEE 98th Vehicular Technology Conference (VTC2023-Fall)*,
    pp. 1–5, 2023.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Y.-L. Hsu, L.-W. Liu, Y.-C. Liao, and H.-C. Chang, “GC-Like LDPC code
    construction and its NN-aided decoder implementation,” *IEEE Open J. Circuits
    Syst.*, vol. 5, pp. 189–198, 2024.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] X. Wu, M. Jiang, and C. Zhao, “Decoding optimization for 5G LDPC codes
    by machine learning,” *IEEE Access*, vol. 6, pp. 50 179–50 186, 2018.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] T. Kim and J. Park, “Neural self-corrected min-sum decoder for NR LDPC
    codes,” *IEEE Commun. Lett.*, 2024.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] C.-F. Teng, K.-S. Ho, C.-H. Wu, S.-S. Wong, and A.-Y. Wu, “Convolutional
    neural network-aided bit-flipping for belief propagation decoding of polar codes,”
    *arXiv preprint arXiv:1911.01704*, 2019.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C.-F. Teng and A.-Y. A. Wu, “Convolutional neural network-aided tree-based
    bit-flipping framework for polar decoder using imitation learning,” *IEEE Trans.
    Signal Process.*, vol. 69, pp. 300–313, 2020.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] C.-F. Teng, A. K.-S. Ho, C.-H. D. Wu, S.-S. Wong, and A.-Y. A. Wu, “Convolutional
    neural network-aided bit-flipping for belief propagation decoding of polar codes,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 7898–7902, 2021.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Y. Sun, Y. Shen, W. Song, Z. Gong, X. You, and C. Zhang, “LSTM network-assisted
    belief propagation flip polar decoder,” in *Proc. Asilomar Conference on Signals,
    Systems, and Computers*, pp. 979–983, 2020.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] B. Liu, Y. Xie, and J. Yuan, “A deep learning assisted node-classified
    redundant decoding algorithm for BCH codes,” *IEEE Trans. Commun.*, pp. 1–1, 2020.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Y. Wei, M.-M. Zhao, M.-J. Zhao, and M. Lei, “ADMM-based decoder for binary
    linear codes aided by deep learning,” *IEEE Commun. Lett.*, vol. 24, no. 5, pp.
    1028–1032, 2020.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] T. Wadayama and S. Takabe, “Deep learning-aided trainable projected gradient
    decoding for LDPC codes,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2444–2448, 2019.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] ——, “Proximal decoding for LDPC codes,” *IEICE Transactions on Fundamentals
    of Electronics, Communications and Computer Sciences*, vol. 106, no. 3, pp. 359–367,
    2023.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] N. Doan, S. A. Hashemi, and W. J. Gross, “Decoding polar codes with reinforcement
    learning,” in *Proc. IEEE Global Communications Conference (GLOBECOM)*, pp. 1–6,
    2020.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] S. Habib, A. Beemer, and J. Kliewer, “Learned scheduling of LDPC decoders
    based on multi-armed bandits,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2789–2794, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] ——, “Belief propagation decoding of short graph-based channel codes via
    reinforcement learning,” *IEEE J. Sel. Inf. Theory*, vol. 2, no. 2, pp. 627–640,
    2021.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] ——, “Reldec: Reinforcement learning-based decoding of moderate length
    LDPC codes,” *IEEE Trans. Commun.*, vol. 71, no. 19, pp. 5661–5674, 2023.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] L. Lugosch and W. J. Gross, “Learning from the syndrome,” in *Proc. Asilomar
    Conference on Signals, Systems, and Computers*, pp. 594–598, 2018.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] C.-F. Teng and Y.-L. Chen, “Syndrome-enabled unsupervised learning for
    neural network-based polar decoder and jointly optimized blind equalizer,” *IEEE
    J. Emerg. Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 177–188, 2020.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] E. Nachmani and Y. Be’ery, “Neural decoding with optimization of node
    activations,” *IEEE Commun. Lett.*, vol. 26, no. 11, pp. 2527–2531, 2022.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] C.-F. Teng, C.-H. D. Wu, A. K.-S. Ho, and A.-Y. A. Wu, “Low-complexity
    recurrent neural network-based polar decoder with weight quantization mechanism,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 1413–1417, 2019.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Z. Ibrahim and Y. Fahmy, “Enhanced learning for recurrent neural network-based
    polar decoder,” in *Proc. International Conference on Electrical Engineering (ICEENG)*,
    pp. 105–109, 2022.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] X. Xiao, B. Vasic, R. Tandon, and S. Lin, “Finite alphabet iterative
    decoding of LDPC codes with coarsely quantized neural networks,” in *Proc. IEEE
    Global Communications Conference (GLOBECOM)*, pp. 1–6, 2019.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] X. Xiao, B. Vasić, R. Tandon, and S. Lin, “Designing finite alphabet
    iterative decoders of LDPC codes via recurrent quantized neural networks,” *IEEE
    Trans. Commun.*, vol. 68, no. 7, pp. 3963–3974, 2020.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Y. Lyu, M. Jiang, Y. Zhang, C. Zhao, N. Hu, and X. Xu, “Optimized non-surjective
    FAIDs for 5G LDPC codes with learnable quantization,” *IEEE Commun. Lett.*, vol. 28,
    no. 2, pp. 253–257, 2023.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] M. Lian, C. Häger, and H. D. Pfister, “What can machine learning teach
    us about communications?” in *Proc. IEEE Information Theory Workshop (ITW)*, pp.
    1–5, 2018.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] M. Lian, F. Carpi, C. Häger, and H. D. Pfister, “Learned belief-propagation
    decoding with simple scaling and SNR adaptation,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 161–165, 2019.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Q. Wang, S. Wang, H. Fang, L. Chen, L. Chen, and Y. Guo, “A model-driven
    deep learning method for normalized min-sum LDPC decoding,” in *Proc. IEEE International
    Conference on Communications Workshops (ICC Workshops)*, pp. 1–6, 2020.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Q. Wang, Q. Liu, S. Wang, L. Chen, H. Fang, L. Chen, Y. Guo, and Z. Wu,
    “Normalized min-sum neural network for LDPC decoding,” *IEEE Trans. Cogn. Commun.
    Netw.*, vol. 9, no. 1, pp. 70–81, 2022.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] J. Dai, K. Tan, Z. Si, K. Niu, M. Chen, H. V. Poor, and S. Cui, “Learning
    to decode protograph LDPC codes,” *IEEE J. Sel. Areas Commun.*, vol. 39, no. 7,
    pp. 1983–1999, 2021.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Y. Liang, C.-T. Lam, and B. K. Ng, “A low-complexity neural normalized
    min-sum LDPC decoding algorithm using tensor-train decomposition,” *IEEE Commun.
    Lett.*, vol. 26, no. 12, pp. 2914–2918, 2022.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] ——, “Joint-way compression for LDPC neural decoding algorithm with tensor-ring
    decomposition,” *IEEE Access*, vol. 11, pp. 22 871–22 879, 2023.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Y. Cheng, W. Chen, L. Li, and B. Ai, “Rate compatible LDPC neural decoding
    network: A multi-task learning approach,” *IEEE Trans. Veh. Technol.*, vol. 73,
    no. 5, pp. 7374–7378, 2024.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] A. Buchberger, C. Häger, H. D. Pfister, L. Schmalen, and A. G. i Amat,
    “Pruning and quantizing neural belief propagation decoders,” *IEEE J. Sel. Areas
    Commun.*, vol. 39, no. 7, pp. 1957–1966, 2020.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] ——, “Learned decimation for neural belief propagation decoders,” in *Proc.
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp. 8273–8277, 2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] V. G. Satorras and M. Welling, “Neural enhanced belief propagation on
    factor graphs,” in *Proc. International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, pp. 685–693, 2021.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] S. Cammerer, J. Hoydis, F. A. Aoudia, and A. Keller, “Graph neural networks
    for channel decoding,” in *Proc. IEEE Global Communications Conference Workshops
    (GC Wkshps)*, pp. 486–491, 2022.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] K. Tian, C. Yue, C. She, Y. Li, and B. Vucetic, “A scalable graph neural
    network decoder for short block codes,” in *Proc. IEEE International Conference
    on Communications*, pp. 1268–1273, 2023.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] S. K. Ankireddy and H. Kim, “Interpreting neural min-sum decoders,” in
    *Proc. IEEE International Conference on Communications (ICC)*, pp. 6645–6651,
    2023.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] S. Adiga, X. Xiao, R. Tandon, B. Vasić, and T. Bose, “Generalization
    bounds for neural belief propagation decoders,” *IEEE Trans. Inf. Theory*, vol. 70,
    no. 6, pp. 4280–4296, 2024.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] J. Clausius, M. Geiselhart, D. Tandler, and S. t. Brink, “Graph neural
    network-based joint equalization and decoding,” *arXiv preprint arXiv:2401.16187*,
    2024.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] R. Wiesmayr, C. Dick, J. Hoydis, and C. Studer, “DUIDD: Deep-unfolded
    interleaved detection and decoding for MIMO wireless systems,” in *Proc. 56th
    Asilomar Conference on Signals, Systems, and Computers*, pp. 181–188, 2022.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] H. Lee, Y.-S. Kil, M. Y. Chung, and S.-H. Kim, “Neural network aided
    impulsive perturbation decoding for short raptor-like LDPC codes,” *IEEE Commun.
    Lett.*, vol. 11, no. 2, pp. 268–272, 2021.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Y. Wang, S. Zhang, C. Zhang, X. Chen, and S. Xu, “A low-complexity belief
    propagation based decoding scheme for polar codes-decodability detection and early
    stopping prediction,” *IEEE Access*, vol. 7, pp. 159 808–159 820, 2019.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] M. P. Fossorier, M. Mihaljevic, and H. Imai, “Reduced complexity iterative
    decoding of low-density parity check codes based on belief propagation,” *IEEE
    Trans. Commun.*, vol. 47, no. 5, pp. 673–680, 1999.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] J. Chen, A. Dholakia, E. Eleftheriou, M. P. Fossorier, and X.-Y. Hu,
    “Reduced-complexity decoding of LDPC codes,” *IEEE Trans. Commun.*, vol. 53, no. 8,
    pp. 1288–1299, 2005.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] John R. Hershey, Jonathan Le Roux, and Felix Weninger, “Deep unfolding:
    Model-based inspiration of novel deep architectures,” *arXiv preprint arXiv:1409.2574*,
    2014.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] N. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Dimakis, “Model-based
    deep learning,” *Proceedings of the IEEE*, vol. 11, no. 5, pp. 465–499, 2023.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] A. Jagannath, J. Jagannath, and T. Melodia, “Redefining wireless communication
    for 6G: Signal processing meets deep learning with deep unfolding,” *IEEE Trans.
    Artif. Intell.*, vol. 2, no. 6, pp. 528–536, 2021.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] E. Nachmani, E. Marciano, D. Burshtein, and Y. Be’ery, “RNN decoding
    of linear block codes,” *arXiv preprint arXiv:1702.07560*, 2017.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] N. Shah and Y. Vasavada, “Neural layered decoding of 5G LDPC codes,”
    *IEEE Commun. Lett.*, vol. 25, no. 11, pp. 3590–3593, 2021.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] T. Richardson, S. Kudekar, and L. Vincent, “Adjusted mim-sum decoder,”
    *Patent*, vol. 20, no. 180, p. 109, 2018.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] A. Darabiha, A. C. Carusone, and F. R. Kschischang, “A bit-serial approximate
    min-sum LDPC decoder and FPGA implementation,” in *Proc. IEEE International Symposium
    on Circuits and Systems (ISCAS)*, pp. 4–pp, 2006.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] F. Angarita, J. Valls, V. Almenar, and V. Torres, “Reduced-complexity
    min-sum algorithm for decoding LDPC codes with low error-floor,” *IEEE Trans.
    Circuits Syst. I*, vol. 61, no. 7, pp. 2150–2158, 2014.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] V. Savin, “Self-corrected min-sum decoding of LDPC codes,” in *Proc.
    IEEE International Symposium on Information Theory (ISIT)*, pp. 146–150, 2008.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] W. Xu, Z. Wu, Y.-L. Ueng, X. You, and C. Zhang, “Improved polar decoder
    based on deep learning,” in *Proc. IEEE International workshop on signal processing
    systems (SiPS)*, pp. 1–6, 2017.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] W. Xu, X. You, C. Zhang, and Y. Be’ery, “Polar decoding on sparse graphs
    with deep learning,” in *Proc. Asilomar Conference on Signals, Systems, and Computers*,
    pp. 599–603, 2018.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] S. Cammerer, M. Ebada, A. Elkelesh, and S. ten Brink, “Sparse graphs
    for belief propagation decoding of polar codes,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 1465–1469, 2018.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] N. Doan, S. A. Hashemi, E. N. Mambou, T. Tonnellier, and W. J. Gross,
    “Neural belief propagation decoding of CRC-polar concatenated codes,” in *Proc.
    IEEE International Conference on Communications (ICC)*, pp. 1–6, 2019.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] W. Xu, X. Tan, Y. Be’ery, Y.-L. Ueng, Y. Huang, X. You, and C. Zhang,
    “Deep learning-aided belief propagation decoder for polar codes,” *IEEE J. Emerg.
    Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 189–203, 2020.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Y. Yu, Z. Pan, N. Liu, and X. You, “Belief propagation bit-flip decoder
    for polar codes,” *IEEE Access*, vol. 7, pp. 10 937–10 946, 2019.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] E. Nachmani and L. Wolf, “Hyper-graph-network decoders for block codes,”
    in *Proc. Advances in Neural Information Processing Systems (NeurIPS)*, pp. 2329–2339,
    2019.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, “A brief
    review of hypernetworks in deep learning,” *arXiv preprint arXiv:2306.06955*,
    2023.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] E. Nachmani and L. Wolf, “A gated hypernet decoder for polar codes,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 5210–5214, 2020.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] ——, “Autoregressive belief propagation for decoding block codes,” *arXiv
    preprint arXiv:2103.11780*, 2021.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] M. Benammar and P. Piantanida, “Optimal training channel statistics for
    neural-based decoders,” in *Proc. 52nd Asilomar Conference on Signals, Systems,
    and Computers*, pp. 2157–2161, 2018.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] I. Be’Ery, N. Raviv, T. Raviv, and Y. Be’Ery, “Active deep decoding of
    linear codes,” *IEEE Trans. Commun.*, vol. 68, no. 2, pp. 728–736, 2019.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] B. Settles, “Active learning literature survey,” *Technical Report*,
    2009.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] T. Raviv, N. Raviv, and Y. Be’ery, “Data-driven ensembles for deep and
    hard-decision hybrid decoding,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 321–326, 2020.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] L. Rokach, “Ensemble-based classifiers,” *Artificial Intelligence Review*,
    vol. 33, pp. 1–39, 2010.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] T. Raviv, A. Goldmann, O. Vayner, Y. Be’ery, and N. Shlezinger, “CRC-aided
    learned ensembles of belief-propagation polar decoders,” *arXiv preprint arXiv:2301.06060*,
    2023.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] T. Raviv, A. Goldman, O. Vayner, Y. Be’ery, and N. Shlezinger, “CRC-aided
    learned ensembles of belief-propagation polar decoders,” in *Proc. IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pp. 8856–8860,
    2024.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] H.-Y. Kwak, D.-Y. Yun, Y. Kim, S.-H. Kim, and J.-S. No, “Boosting learning
    for LDPC codes to improve the error-floor performance,” *arXiv preprint arXiv:2310.07194*,
    2023.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
    on-line learning and an application to boosting,” *Journal of Computer and System
    Sciences*, vol. 55, no. 1, pp. 119–139, 1997.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] T. R. Halford and K. M. Chugg, “Random redundant soft-in soft-out decoding
    of linear block codes,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2230–2234, 2006.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] T. Hehn, J. B. Huber, S. Laendner, and O. Milenkovic, “Multiple-bases
    belief-propagation for decoding of short block codes,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 311–315, 2007.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] J. Feldman, “Decoding error-correcting codes via linear programming,”
    Ph.D. dissertation, Massachusetts Institute of Technology, 2003.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] E. J. Candes and T. Tao, “Decoding by linear programming,” *IEEE Trans.
    Inf. Theory*, vol. 51, no. 12, pp. 4203–4215, 2005.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] S. Barman, X. Liu, S. C. Draper, and B. Recht, “Decomposition methods
    for large scale LP decoding,” *IEEE Trans. Inf. Theory*, vol. 59, no. 12, pp.
    7870–7886, 2013.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] N. Parikh, S. Boyd *et al.*, “Proximal algorithms,” *Foundations and
    trends® in Optimization*, vol. 1, no. 3, pp. 127–239, 2014.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] A. Elkelesh, M. Ebada, S. Cammerer, and S. ten Brink, “Belief propagation
    decoding of polar codes on permuted factor graphs,” in *Proc. IEEE Wireless Communications
    and Networking Conference (WCNC)*, pp. 1–6, 2018.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] N. Doan, S. A. Hashemi, M. Mondelli, and W. J. Gross, “On the decoding
    of polar codes on permuted factor graphs,” in *Proc. IEEE Global Communications
    Conference (GLOBECOM)*, pp. 1–6, 2018.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] N. Hussami, S. B. Korada, and R. Urbanke, “Performance of polar codes
    for channel and source coding,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 1488–1492, 2009.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] S. Habib and D. G. Mitchell, “Reinforcement learning for sequential decoding
    of generalized LDPC codes,” in *Proc. 12th International Symposium on Topics in
    Coding (ISTC)*, pp. 1–5, 2023.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] N. C. Thompson, K. Greenewald, K. Lee, and G. F. Manso, “The computational
    limits of deep learning,” *arXiv preprint arXiv:2007.05558*, 2020.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] E. D. Karnin, “A simple procedure for pruning back-propagation trained
    neural networks,” *IEEE Trans. Neural Netw.*, vol. 1, no. 2, pp. 239–242, 1990.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
    neural networks with pruning, trained quantization and huffman coding,” *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” *arXiv preprint arXiv:1810.05270*, 2018.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, “What is
    the state of neural network pruning?” *Proceedings of Machine Learning and Systems*,
    vol. 2, pp. 129–146, 2020.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, “Sparsity
    in deep learning: Pruning and growth for efficient inference and training in neural
    networks,” *Journal of Machine Learning Research*, vol. 22, no. 241, pp. 1–124,
    2021.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen,
    and T. Blankevoort, “A white paper on neural network quantization,” *arXiv preprint
    arXiv:2106.08295*, 2021.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A
    survey of quantization methods for efficient neural network inference,” in *Low-Power
    Computer Vision*.   Chapman and Hall/CRC, 2022, pp. 291–326.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] T. Wadayama and S. Takabe, “Joint quantizer optimization based on neural
    quantizer for sum-product decoder,” *arXiv preprint arXiv:1804.06002*, 2018.'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] M. Geiselhart, A. Elkelesh, J. Clausius, F. Liang, W. Xu, J. Liang, and
    S. Ten Brink, “Learning quantization in LDPC decoders,” in *Proc. IEEE Global
    Communications Conference Workshops (GC Wkshps)*, pp. 467–472, 2022.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] J. Gao, K. Niu, and C. Dong, “Learning to decode polar codes with one-bit
    quantizer,” *IEEE Access*, vol. 8, pp. 27 210–27 217, 2020.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] B. Vasić, X. Xiao, and S. Lin, “Learning to decode LDPC codes with finite-alphabet
    message passing,” in *Proc. Information Theory and Applications Workshop (ITA)*,
    pp. 1–9, 2018.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients
    through stochastic neurons for conditional computation,” *arXiv preprint arXiv:1308.3432*,
    2013.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] L. Wang, X. Dong, Y. Wang, L. Liu, W. An, and Y. Guo, “Learnable lookup
    table for neural network quantization,” in *Proc. IEEE/CVF conference on computer
    vision and pattern recognition (CVPR)*, pp. 12 423–12 433, 2022.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] T. T. Nguyen-Ly, K. Le, V. Savin, D. Declercq, F. Ghaffari, and O. Boncalo,
    “Non-surjective finite alphabet iterative decoders,” in *Proc. IEEE International
    Conference on Communications (ICC)*, pp. 1–6, 2016.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] L. Wang, S. Chen, J. Nguyen, D. Dariush, and R. Wesel, “Neural-network-optimized
    degree-specific weights for LDPC minsum decoding,” *arXiv preprint arXiv:2107.04221*,
    2021.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] L. Wang, C. Terrill, D. Divsalar, and R. Wesel, “LDPC decoding with degree-specific
    neural message weights and RCQ decoding,” *IEEE Trans. Commun.*, vol. 72, no. 4,
    pp. 1912–1924, 2024.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] I. V. Oseledets, “Tensor-train decomposition,” *SIAM Journal on Scientific
    Computing*, vol. 33, no. 5, pp. 2295–2317, 2011.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Q. Zhao, G. Zhou, S. Xie, L. Zhang, and A. Cichocki, “Tensor ring decomposition,”
    *arXiv preprint arXiv:1606.05535*, 2016.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] T. Filler and J. Fridrich, “Binary quantization using belief propagation
    with decimation over factor graphs of LDGM codes,” *arXiv preprint arXiv:0710.0192*,
    2007.'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] M. Mohri, A. Rostamizadeh, and A. Talwalkar, *Foundations of Machine
    Learning*.   MIT press, 2018.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] H. Lee, Y.-S. Kil, M. Jang, S.-H. Kim, O.-S. Park, and G. Park, “Multi-round
    belief propagation decoding with impulsive perturbation for short LDPC codes,”
    *IEEE Wireless Commun. Lett.*, vol. 9, no. 9, pp. 1491–1494, 2020.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] D. Xiao, J. Lu, C. Lin, and B. Jiao, “A perturbation method for decoding
    LDPC concatenated with CRC,” in *Proc. IEEE Wireless Communications and Networking
    Conference (WCNC)*, pp. 667–671, 2007.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] X. Han, R. Liu, Y. Li, C. Yi, J. He, and M. Wang, “Accelerating neural
    BP-based decoder using coded distributed computing,” *IEEE Trans. Veh. Technol.*,
    2024.'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] S. Li, M. A. Maddah-Ali, Q. Yu, and A. S. Avestimehr, “A fundamental
    tradeoff between computation and communication in distributed computing,” *IEEE
    Trans. Inf. Theory*, vol. 64, no. 1, pp. 109–128, 2017.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] E. Chen, D. Apalkov, Z. Diao, A. Driskill-Smith, D. Druist, D. Lottis,
    V. Nikitin, X. Tang, S. Watts, S. Wang *et al.*, “Advances and future prospects
    of spin-transfer torque random access memory,” *IEEE Transactions on Magnetics*,
    vol. 46, no. 6, pp. 1873–1878, 2010.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] H. Chen, K. Zhang, X. Ma, and B. Bai, “Comparisons between reliability-based
    iterative min-sum and majority-logic decoding algorithms for LDPC codes,” *IEEE
    Trans. Commun.*, vol. 59, no. 7, pp. 1766–1771, 2011.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Y.-H. Liu and D. Poulin, “Neural belief-propagation decoders for quantum
    error-correcting codes,” *Physical Review Letters*, vol. 122, no. 20, p. 200501,
    2019.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] D. Poulin and Y. Chung, “On the iterative decoding of sparse quantum
    codes,” *arXiv preprint arXiv:0801.1241*, 2008.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] A. Gong, S. Cammerer, and J. M. Renes, “Graph neural networkrs for enhanced
    decoding of quantum LDPC codes,” *arXiv preprint arXiv:2310.17758*, 2023.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] S. Miao, A. Schnerring, H. Li, and L. Schmalen, “Neural belief propagation
    decoding of quantum LDPC codes using overcomplete check matrices,” in *Proc. IEEE
    Information Theory Workshop (ITW)*, pp. 215–220, 2023.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] S. Cammerer, T. Gruber, J. Hoydis, and S. ten Brink, “Scaling deep learning-based
    decoding of polar codes via partitioning,” in *Proc. IEEE Global Communications
    Conference (GLOBECOM)*, pp. 1–6, 2017.'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] J. Gao and R. Liu, “Neural network aided SC decoder for polar codes,”
    in *Proc. IEEE 4th International Conference on Computer and Communications (ICCC)*,
    pp. 2153–2157, 2018.'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] N. Doan, S. A. Hashemi, and W. J. Gross, “Neural successive cancellation
    decoding of polar codes,” in *Proc. IEEE International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC)*, pp. 1–5, 2018.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] I. Wodiany and A. Pop, “Low-precision neural network decoding of polar
    codes,” in *Proc. IEEE International Workshop on Signal Processing Advances in
    Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] S. A. Hebbar, V. V. Nadkarni, A. V. Makkuva, S. Bhat, S. Oh, and P. Viswanath,
    “CRISP: Curriculum based sequential neural decoders for polar code family,” in
    *Proc. International Conference on Machine Learning (ICML)*, pp. 12 823–12 845,
    2023.'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] X. Wang, H. Zhang, R. Li, L. Huang, S. Dai, Y. Huangfu, and J. Wang,
    “Learning to flip successive cancellation decoding of polar codes with lstm networks,”
    in *Proc. IEEE International Symposium on Personal, Indoor and Mobile Radio Communications
    (PIMRC)*, pp. 1–5, 2019.'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] B. He, S. Wu, Y. Deng, H. Yin, J. Jiao, and Q. Zhang, “A machine learning
    based multi-flips successive cancellation decoding scheme of polar codes,” in
    *Proc. IEEE Vehicular Technology Conference (VTC2020-Spring)*, pp. 1–5, 2020.'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] N. Doan, S. A. Hashemi, F. Ercan, T. Tonnellier, and W. J. Gross, “Neural
    dynamic successive cancellation flip decoding of polar codes,” in *Proc. IEEE
    International Workshop on Signal Processing Systems (SiPS)*, pp. 272–277, 2019.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] ——, “Neural successive cancellation flip decoding of polar codes,” *Journal
    of Signal Processing Systems*, vol. 93, pp. 631–642, 2021.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] X. Wang, J. He, J. Li, and L. Shan, “Reinforcement learning for bit-flipping
    decoding of polar codes,” *Entropy*, vol. 23, no. 2, p. 171, 2021.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] N. Doan, S. A. Hashemi, F. Ercan, and W. J. Gross, “Fast SC-Flip decoding
    of polar codes with reinforcement learning,” in *Proc. IEEE International Conference
    on Communications (ICC)*, pp. 1–6, 2021.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] S. A. Hashemi, N. Doan, T. Tonnellier, and W. J. Gross, “Deep-learning-aided
    successive-cancellation decoding of polar codes,” in *Proc. Asilomar Conference
    on Signals, Systems, and Computers*, pp. 532–536, 2019.'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] N. Doan, S. A. Hashemi, and W. J. Gross, “Fast successive-cancellation
    list flip decoding of polar codes,” *IEEE Access*, vol. 10, pp. 5568–5584, 2022.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] C.-H. Chen, C.-F. Teng, and A.-Y. A. Wu, “Low-complexity LSTM-assisted
    bit-flipping algorithm for successive cancellation list polar decoder,” in *Proc.
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp. 1708–1712, 2020.'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Y. Tao and Z. Zhang, “DNC-aided SCL-flip decoding of polar codes,” in
    *Proc. IEEE Global Communications Conference (GLOBECOM)*, pp. 1–6, 2021.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] F.-S. Liang, S. Lu, and Y.-L. Ueng, “Deep-learning-aided successive cancellation
    list flip decoding for polar codes,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 10,
    no. 2, pp. 374–386, 2024.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] J. Li, L. Zhou, Z. Li, W. Gao, R. Ji, J. Zhu, and Z. Liu, “Deep learning-assisted
    adaptive dynamic-SCLF decoding of polar codes,” *IEEE Trans. Cogn. Commun. Netw.*,
    2024.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Y. Lu, M. Zhao, M. Lei, C. Wang, and M. Zhao, “Deep learning aided SCL
    decoding of polar codes with shifted-pruning,” *China Communications*, vol. 20,
    no. 1, pp. 153–170, 2023.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] X. Liu, S. Wu, Y. Wang, N. Zhang, J. Jiao, and Q. Zhang, “Exploiting
    error-correction-CRC for polar SCL decoding: A deep learning based approach,”
    *IEEE Trans. Cogn. Commun. Netw.*, vol. 6, no. 2, pp. 817–828, 2020.'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] G. Sarkis, P. Giard, A. Vardy, C. Thibeault, and W. J. Gross, “Fast list
    decoders for polar codes,” *IEEE J. Sel. Areas Commun.*, vol. 34, no. 2, pp. 318–328,
    2015.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] O. Afisiadis, A. Balatsoukas-Stimming, and A. Burg, “A low-complexity
    improved successive cancellation decoder for polar codes,” in *Proc. 48th Asilomar
    Conf. Signals, Systems and Computers (ACSSC)*, pp. 2116–2120, 2014.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] L. Chandesris, V. Savin, and D. Declercq, “Dynamic-SCFlip decoding of
    polar codes,” *IEEE Trans. Commun.*, vol. 66, no. 6, pp. 2333–2345, 2018.'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] F. Ercan, T. Tonnellier, N. Doan, and W. J. Gross, “Practical dynamic
    SC-flip polar decoders: Algorithm and implementation,” *IEEE Trans. Signal Process.*,
    vol. 68, pp. 5441–5456, 2020.'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] A. Alamdar-Yazdi and F. R. Kschischang, “A simplified successive-cancellation
    decoder for polar codes,” *IEEE Commun. Lett.*, vol. 15, no. 12, pp. 1378–1380,
    2011.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] S. A. Hashemi, C. Condo, and W. J. Gross, “A fast polar code list decoder
    architecture based on sphere decoding,” *IEEE Trans. Circuits Syst. I*, vol. 63,
    no. 12, pp. 2368–2380, 2016.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] ——, “Fast and flexible successive-cancellation list decoders for polar
    codes,” *IEEE Trans. Signal Process.*, vol. 65, no. 21, pp. 5756–5769, 2017.'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwińska,
    S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou *et al.*, “Hybrid computing
    using a neural network with dynamic external memory,” *Nature*, vol. 538, no.
    7626, pp. 471–476, 2016.'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] M. Rowshan and E. Viterbo, “Improved list decoding of polar codes by
    shifted-pruning,” in *Proc. IEEE Information Theory Workshop (ITW)*, pp. 1–5,
    2019.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] W. G. Teich, R. Liu, and V. Belagiannis, “Deep learning versus high-order
    recurrent neural network based decoding for convolutional codes,” in *Proc. IEEE
    Global Communications Conference (GLOBECOM)*, pp. 1–7, 2020.'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Y. Jiang, S. Kannan, H. Kim, S. Oh, H. Asnani, and P. Viswanath, “DeepTurbo:
    Deep turbo decoder,” in *Proc. IEEE International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Y. He, J. Zhang, S. Jin, C.-K. Wen, and G. Y. Li, “Model-driven DNN decoder
    for turbo codes: Design, simulation, and experimental results,” *IEEE Trans. Commun.*,
    vol. 68, no. 10, pp. 6127–6140, 2020.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] S. A. Hebbar, R. K. Mishra, S. K. Ankireddy, A. V. Makkuva, H. Kim, and
    P. Viswanath, “TinyTurbo: Efficient turbo decoders on edge,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 2797–2802, 2022.'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] A. Viterbi, “Error bounds for convolutional codes and an asymptotically
    optimum decoding algorithm,” *IEEE Trans. Inf. Theory*, vol. 13, no. 2, pp. 260–269,
    1967.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] C. Cardinal, D. Haccoun, F. Gagnon, and N. Baatani, “Turbo decoding using
    convolutional self doubly orthogonal codes,” in *Proc. IEEE International Conference
    on Communications (ICC)*, vol. 1, pp. 113–117, 1999.'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] M. Mostafa, W. G. Teich, and J. Lindner, “Analysis of high order recurrent
    neural networks for analog decoding,” in *Proc. International Symposium on Turbo
    Codes and Iterative Information Processing (ISTC)*, pp. 116–120, 2012.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] L. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear
    codes for minimizing symbol error rate,” *IEEE Trans. Inf. Theory*, vol. IT-20,
    pp. 284–287, Mar. 1974.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] X. Chen and M. Ye, “Cyclically equivariant neural decoders for cyclic
    codes,” in *Proc. International conference on machine learning (ICML)*, pp. 1771–1780,
    2021.'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] ——, “Improving the list decoding version of the cyclically equivariant
    neural decoder,” in *Proc. IEEE International Symposium on Information Theory
    (ISIT)*, pp. 2344–2349, 2022.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] H. Pirayesh and H. Zeng, “Jamming attacks and anti-jamming strategies
    in wireless networks: A comprehensive survey,” *IEEE Communications Surveys &
    Tutorials*, vol. 24, no. 2, pp. 767–809, 2022.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] D. Adesina, C.-C. Hsieh, Y. E. Sagduyu, and L. Qian, “Adversarial machine
    learning in wireless communications using RF data: A review,” *IEEE Communications
    Surveys & Tutorials*, vol. 25, no. 1, pp. 77–100, 2022.'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] M. Sadeghi and E. G. Larsson, “Physical adversarial attacks against end-to-end
    autoencoder communication systems,” *IEEE Commun. Lett.*, vol. 23, no. 5, pp.
    847–850, 2019.'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] J. Chen, J. Ge, S. Zheng, L. Ye, H. Zheng, W. Shen, K. Yue, and X. Yang,
    “AIR: Threats of adversarial attacks on deep learning-based information recovery,”
    *IEEE Trans. Wireless Commun.*, 2024.'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] P. J. Phillips, C. A. Hahn, P. C. Fontana, D. A. Broniatowski, and M. A.
    Przybocki, “Four principles of explainable artificial intelligence (draft),” *NIST
    Interagency/Internal Report (NISTIR)*, 2020.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] R. R. Hoffman, S. T. Mueller, G. Klein, and J. Litman, “Metrics for explainable
    AI: Challenges and prospects,” *arXiv preprint arXiv:1812.04608*, 2018.'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on explainable
    artificial intelligence (XAI),” *IEEE Access*, vol. 6, pp. 52 138–52 160, 2018.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] F. Xu, H. Uszkoreit, Y. Du, W. Fan, D. Zhao, and J. Zhu, “Explainable
    AI: A brief survey on history, research areas, approaches and challenges,” in
    *Proc. International Conference on Natural Language Processing and Chinese Computing*,
    pp. 563–574, 2019.'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis, “Explainable
    AI: A review of machine learning interpretability methods,” *Entropy*, vol. 23,
    no. 1, p. 18, 2020.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *et al.*, “Explainable
    artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
    toward responsible AI,” *Information Fusion*, vol. 58, pp. 82–115, 2020.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] D. Minh, H. X. Wang, Y. F. Li, and T. N. Nguyen, “Explainable artificial
    intelligence: A comprehensive review,” *Artificial Intelligence Review*, pp. 1–66,
    2022.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,
    Z. Wen, T. Shah, G. Morgan *et al.*, “Explainable AI (XAI): Core ideas, techniques,
    and solutions,” *ACM Computing Surveys*, vol. 55, no. 9, pp. 1–33, 2023.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] D. Gunning and D. Aha, “DARPA’s explainable artificial intelligence (XAI)
    program,” *AI magazine*, vol. 40, no. 2, pp. 44–58, 2019.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] W. Guo, “Explainable artificial intelligence for 6G: Improving trust
    between human and machine,” *IEEE Commun. Mag.*, vol. 58, no. 6, pp. 39–45, 2020.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] S. Wang, M. A. Qureshi, L. Miralles-Pechuan, T. Huynh-The, T. R. Gadekallu,
    and M. Liyanage, “Applications of explainable AI for 6G: Technical aspects, use
    cases, and research challenges,” *arXiv preprint arXiv:2112.04698*, 2021.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] S. Wang, M. A. Qureshi, L. Miralles-Pechuán, T. Huynh-The, T. R. Gadekallu,
    and M. Liyanage, “Explainable AI for 6G use cases: Technical aspects and research
    challenges,” *IEEE Open J. Commun. Soc.*, vol. 5, pp. 2490–2540, 2024.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green AI,” *Communications
    of the ACM*, vol. 63, no. 12, pp. 54–63, 2020.'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations
    for deep learning in NLP,” *arXiv preprint arXiv:1906.02243*, 2019.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] D. Patterson, J. Gonzalez, U. Hölzle, Q. Le, C. Liang, L.-M. Munguia,
    D. Rothchild, D. R. So, M. Texier, and J. Dean, “The carbon footprint of machine
    learning training will plateau, then shrink,” *Computer*, vol. 55, no. 7, pp.
    18–28, 2022.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang,
    F. Aga, J. Huang, C. Bai *et al.*, “Sustainable AI: Environmental implications,
    challenges and opportunities,” *Proceedings of Machine Learning and Systems*,
    vol. 4, pp. 795–813, 2022.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] S. Salehi and A. Schmeink, “Data-centric green artificial intelligence:
    A survey,” *IEEE Trans. Artif. Intell.*, vol. 5, no. 5, pp. 1973–1989, 2024.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] G. Menghani, “Efficient deep learning: A survey on making deep learning
    models smaller, faster, and better,” *ACM Computing Surveys*, vol. 55, no. 12,
    pp. 1–37, 2023.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] M. H. Jarrahi, A. Memariani, and S. Guha, “The principles of data-centric
    AI (DCAI),” *arXiv preprint arXiv:2211.14611*, 2022.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] D. Zha, Z. P. Bhat, K.-H. Lai, F. Yang, and X. Hu, “Data-centric AI:
    Perspectives and challenges,” in *Proc. SIAM International Conference on Data
    Mining (SDM)*, pp. 945–948, 2023.'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] D. Zha, Z. P. Bhat, K.-H. Lai, F. Yang, Z. Jiang, S. Zhong, and X. Hu,
    “Data-centric artificial intelligence: A survey,” *arXiv preprint arXiv:2303.10158*,
    2023.'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] P. Botsinis, D. Alanis, Z. Babar, H. Nguyen, D. Chandra, S. X. Ng, and
    L. Hanzo, “Quantum algorithms for wireless communications,” *IEEE Communications
    Surveys & Tutorials*, 2018.'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] T. Matsumine, T. Koike-Akino, and Y. Wang, “Channel decoding with quantum
    approximate optimization algorithm,” in *Proc. IEEE International Symposium on
    Information Theory (ISIT)*, pp. 2574–2578, 2019.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] J. Cui, Y. Xiong, S. X. Ng, and L. Hanzo, “Quantum approximate optimization
    algorithm based maximum likelihood detection,” *IEEE Trans. Commun.*, vol. 70,
    no. 8, pp. 5386–5400, 2022.'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] K. Yukiyoshi and N. Ishikawa, “Quantum search algorithm for binary constant
    weight codes,” *arXiv preprint arXiv:2211.04637*, 2022.'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] M. Kim, S. Kasi, P. A. Lott, D. Venturelli, J. Kaewell, and K. Jamieson,
    “Heuristic quantum optimization for 6G wireless communications,” *IEEE Network*,
    vol. 35, no. 4, pp. 8–15, 2021.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] S. Kasi, J. Kaewelh, and K. Jamieson, “A quantum annealer-enabled decoder
    and hardware topology for nextg wireless polar codes,” *IEEE Trans. Wireless Commun.*,
    vol. 23, no. 4, pp. 3780–3794, 2023.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] J. Preskill, “Quantum computing in the NISQ era and beyond,” *Quantum*,
    vol. 2, p. 79, 2018.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[446] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J. Love,
    A. Aspuru-Guzik, and J. L. O’brien, “A variational eigenvalue solver on a photonic
    quantum processor,” *Nature Communications*, vol. 5, no. 1, p. 4213, 2014.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[447] E. Farhi, J. Goldstone, and S. Gutmann, “A quantum approximate optimization
    algorithm,” *arXiv preprint arXiv:1411.4028*, 2014.'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[448] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand,
    M. Degroote, H. Heimonen, J. S. Kottmann, T. Menke *et al.*, “Noisy intermediate-scale
    quantum algorithms,” *Reviews of Modern Physics*, vol. 94, no. 1, p. 015004, 2022.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[449] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii,
    J. R. McClean, K. Mitarai, X. Yuan, L. Cincio *et al.*, “Variational quantum algorithms,”
    *Nature Reviews Physics*, vol. 3, no. 9, pp. 625–644, 2021.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[450] M. Schuld, I. Sinayskiy, and F. Petruccione, “An introduction to quantum
    machine learning,” *Contemporary Physics*, vol. 56, no. 2, pp. 172–185, 2015.'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[451] A. Perdomo-Ortiz, M. Benedetti, J. Realpe-Gómez, and R. Biswas, “Opportunities
    and challenges for quantum-assisted machine learning in near-term quantum computers,”
    *Quantum Science and Technology*, vol. 3, no. 3, p. 030502, 2018.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[452] S. J. Nawaz, S. K. Sharma, S. Wyne, M. N. Patwary, and M. Asaduzzaman,
    “Quantum machine learning for 6G communication networks: State-of-the-art and
    vision for the future,” *IEEE Access*, vol. 7, pp. 46 317–46 350, 2019.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[453] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven,
    and J. R. McClean, “Power of data in quantum machine learning,” *Nature Communications*,
    vol. 12, no. 1, p. 2631, 2021.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[454] O. Simeone *et al.*, “An introduction to quantum machine learning for
    engineers,” *Foundations and Trends® in Signal Processing*, vol. 16, no. 1-2,
    pp. 1–223, 2022.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[455] B. Narottama, Z. Mohamed, and S. Aïssa, “Quantum machine learning for
    next-g wireless communications: Fundamentals and the path ahead,” *IEEE Open J.
    Commun. Soc.*, vol. 4, pp. 2204–2224, 2023.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[456] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner,
    “The power of quantum neural networks,” *Nature Computational Science*, vol. 1,
    no. 6, pp. 403–409, 2021.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[457] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, “Quanvolutional neural
    networks: Powering image recognition with quantum circuits,” *Quantum Machine
    Intelligence*, vol. 2, no. 1, p. 2, 2020.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[458] J. Romero, J. P. Olson, and A. Aspuru-Guzik, “Quantum autoencoders for
    efficient compression of quantum data,” *Quantum Science and Technology*, vol. 2,
    no. 4, p. 045001, 2017.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[459] P.-L. Dallaire-Demers and N. Killoran, “Quantum generative adversarial
    networks,” *Physical Review A*, vol. 98, no. 1, p. 012324, 2018.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[460] S. Lloyd and C. Weedbrook, “Quantum generative adversarial learning,”
    *Physical Review Letters*, vol. 121, no. 4, p. 040502, 2018.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
