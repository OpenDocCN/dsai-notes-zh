- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2406.19664] Recent Advances in Deep Learning for Channel Coding: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.19664](https://ar5iv.labs.arxiv.org/html/2406.19664)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \DeclareAcronym
  prefs: []
  type: TYPE_NORMAL
- en: dlshort=DL, long=deep learning \DeclareAcronymldpcshort=LDPC, long=low-density
    parity check \DeclareAcronymxrshort=XR, long=extended reality \DeclareAcronymv2xshort=V2X,
    long=vehicle-to-everything \DeclareAcronymembbshort=eMBB, long=enhanced mobile
    broadband \DeclareAcronymurllcshort=URLLC, long=ultra-reliable low-latency communication
    \DeclareAcronymmmtcshort=mMTC, long=massive machine type communication \DeclareAcronymkpishort=KPI,
    long=key performance indicator \DeclareAcronymmlshort=ML, long=machine learning
    \DeclareAcronymgpushort=GPU, long=graphics processing unit \DeclareAcronymcudashort=CUDA,
    long=compute unified device architecture \DeclareAcronymfpgashort=FPGA, long=field
    programmable gate array \DeclareAcronymtpushort=TPU, long=tensor processing unit
    \DeclareAcronymaeshort=AE, long=autoencoder \DeclareAcronymmushort=MU, long=multi-user
    \DeclareAcronymmimoshort=MIMO, long=multiple-input multiple-output \DeclareAcronym3gppshort=3GPP,
    long=third generation partnership project \DeclareAcronymdnnshort=DNN, long=deep
    neural network \DeclareAcronymmlpshort=MLP, long=multi-layer perceptron \DeclareAcronymrelushort=ReLU,
    long=rectified linear unit \DeclareAcronymbgdshort=BGD, long=batch gradient descent
    \DeclareAcronymsgdshort=SGD, long=stochastic gradient descent \DeclareAcronymmbsgdshort=MBSGD,
    long=mini-batch stochastic gradient descent \DeclareAcronymllmshort=LLM, long=large
    language model \DeclareAcronymrlshort=RL, long=reinforcement learning \DeclareAcronymmdpshort=MDP,
    long=Markov decision process \DeclareAcronymdrlshort=DRL, long=deep reinforcement
    learning \DeclareAcronymdqnshort=DQN, long=deep Q-network \DeclareAcronymbershort=BER,
    long=bit error rate \DeclareAcronympaprshort=PAPR, long=peak-to-average power
    ratio \DeclareAcronymofdmshort=OFDM, long=orthogonal frequency division multiplexing
    \DeclareAcronymcnnshort=CNN, long=convolutional neural network \DeclareAcronymnlpshort=NLP,
    long=natural language processing \DeclareAcronymilsvrcshort=ILSVRC, long=ImageNet
    Large Scale Visual Recognition Challenge \DeclareAcronymrnnshort=RNN, long=reccurent
    neural network \DeclareAcronymlstmshort=LSTM, long=long short term memory \DeclareAcronymgrushort=GRU,
    long=gated recurrent unit \DeclareAcronymgnnshort=GNN, long=graph neural network
    \DeclareAcronymmpnnshort=MPNN, long=message passing neural network \DeclareAcronymgptshort=GPT
    , long=generative pre-trained Transformer \DeclareAcronymllamashort=LLaMA, long=Large
    Language Model Meta AI \DeclareAcronympalmshort=PaLM, long=Pathways Language Model
    \DeclareAcronymdmshort=DM, long=diffusion model \DeclareAcronymddpmshort=DDPM,
    long=denoising diffusion probabilistic model \DeclareAcronymdeshort=DE, long=density
    evolution \DeclareAcronymexitshort=EXIT, long=extrinsic information transfer \DeclareAcronymbpshort=BP,
    long=belief propagation \DeclareAcronymawgnshort=AWGN, long=additive white Gaussian
    noise \DeclareAcronymbiawgnshort=BI-AWGN, long=binary-input additive white Gaussian
    noise \DeclareAcronymscshort=SC, long=successive cancellation \DeclareAcronymvnshort=VN,
    long=variable node \DeclareAcronymcnshort=CN, long=check node \DeclareAcronymsnrshort=SNR,
    long=signal-to-noise ratio \DeclareAcronymgashort=GA, long=Gaussian approximation
    \DeclareAcronymrcashort=RCA, long=reciprocal channel approximation \DeclareAcronymcrcshort=CRC,
    long=cyclic redundancy check \DeclareAcronymcasclshort=CA-SCL, long=cyclic redundancy
    check-aided successive cancellation list \DeclareAcronymblershort=BLER, long=block
    error rate \DeclareAcronympgdshort=PGD, long=projected gradient descent \DeclareAcronympccmpshort=PCCMP,
    long=polar-code-construction message-passing \DeclareAcronyma2cshort=A2C, long=advantage
    actor-critic \DeclareAcronymisitshort=ISIT, long=International Symposium on Information
    Theory \DeclareAcronympacshort=PAC, long=polarization adjusted convolutional \DeclareAcronymmapshort=MAP,
    long=maximum a posterior \DeclareAcronymbigrushort=Bi-GRU, long=bidirectional
    gated recurrent unit \DeclareAcronymecctshort=ECCT, long=error correction code
    Transformer \DeclareAcronympcmshort=PCM, long=parity check matrix \DeclareAcronymbpskshort=BPSK,
    long=binary phase shift keying \DeclareAcronymbchshort=BCH, long=Bose–Chaudhuri–Hocquenghem
    \DeclareAcronymsisoshort=SISO, long=soft-input soft-output \DeclareAcronymmindshort=MIND,
    long=model independent neural decoder \DeclareAcronymosdshort=OSD, long=ordered
    statistic decoding \DeclareAcronymsttmramshort=STT-MRAM, long=spin-torque transfer
    magnetic random access memory \DeclareAcronymmsshort=MS, long=min-sum \DeclareAcronymnmsshort=NMS,
    long=normalized min-sum \DeclareAcronymomsshort=OMS, long=offset min-sum \DeclareAcronymnomsshort=NOMS,
    long=normalized offset min-sum \DeclareAcronymamsshort=AMS, long=adjusted min-sum
    \DeclareAcronymsmmsshort=SMMS, long=single-minimum min-sum \DeclareAcronymvwmsshort=VWMS,
    long=variable weight min-sum \DeclareAcronymscmsshort=SCMS, long=self-corrected
    min-sum \DeclareAcronymrbmsshort=RBMS, long=reliability-based min-sum \DeclareAcronymsvmshort=SVM,
    long=support vector machine \DeclareAcronymrlmshort=RLM, long=regularized loss
    minimization \DeclareAcronymlamsshort=LAMS, long=min-sum decoding with linear
    approximation \DeclareAcronympbldpcshort=PB-LDPC, long=protograph-based low-density
    parity-check \DeclareAcronymlutshort=LUT, long=look-up table \DeclareAcronymllrshort=LLR,
    long=log-likelihood ratio \DeclareAcronymhddshort=HDD, long=hard-decision decoder
    \DeclareAcronymrrdshort=RRD, long=random redundant decoding \DeclareAcronymmrrdshort=mRRD,
    long=modified random redundant decoding \DeclareAcronymmbbpshort=MBBP, long=multiple-bases
    belief-propagation \DeclareAcronymncrdshort=NC-RD, long=node-classified redundant
    decoding \DeclareAcronymhdpcshort=HDPC, long=high-density parity-check \DeclareAcronymlpshort=LP,
    long=linear programming \DeclareAcronymadmmshort=ADMM, long=alternating direction
    method of multipliers \DeclareAcronymgldpcshort=GLDPC, long=generalized low-density
    parity-check \DeclareAcronymfaidshort=FAID, long=finite alphabet iterative decoder
    \DeclareAcronymrqnnshort=RQNN, long=recurrent quantized neural network \DeclareAcronymsteshort=STE,
    long=straight-through estimator \DeclareAcronymttshort=TT, long=tensor-train \DeclareAcronymtrshort=TR,
    long=tensor-ring \DeclareAcronymharqshort=HARQ, long=hybrid automatic repeat request
    \DeclareAcronympnnshort=PNN, long=partitioned neural network \DeclareAcronymspcshort=SPC,
    long=single parity check \DeclareAcronymrcshort=RC, long=repetition code \DeclareAcronymnscshort=NSC,
    long=neural successive cancellation \DeclareAcronymbfshort=BF, long=bit flipping
    \DeclareAcronymsclshort=SCL, long=successive cancellation list \DeclareAcronymscfshort=SCF,
    long=successive cancellation flip \DeclareAcronymsclfshort=SCLF, long=successive
    cancellation list flip \DeclareAcronymfsclshort=FSCL, long=fast successive cancellation
    list \DeclareAcronymfscfshort=FSCF, long=fast successive cancellation flip \DeclareAcronymfsclfshort=FSCLF,
    long=fast successive cancellation list clip \DeclareAcronymdscfshort=DSCF, long=dynamic
    successive cancellation flip \DeclareAcronymdsclfshort=DSCLF, long=dynamic successive
    cancellation list flip \DeclareAcronymdncshort=DNC, long=differentiable neural
    computer \DeclareAcronymspshort=SP, long=shifted-pruning \DeclareAcronymitdshort=ITD,
    long=iterative threshold decoding \DeclareAcronymhornnshort=HORNN, long=high-order
    recurrent neural network \DeclareAcronymrscshort=RSC, long=recursive systematic
    convolutional \DeclareAcronymbcjrshort=BCJR, long=Bahl-Cocke-Jelinek-Raviv \DeclareAcronymrmshort=RM,
    long=Reed-Muller \DeclareAcronymxaishort=XAI, long=explainable AI \DeclareAcronymqmlshort=QML,
    long=quantum machine learning \DeclareAcronymnisqshort=NISQ, long=noisy intermediate-scale
    quantum \DeclareAcronymvqeshort=VQE, long=variational quantum eigensolver \DeclareAcronymqaoashort=QAOA,
    long=quantum approximate optimization algorithm \DeclareAcronymganshort=GAN, long=generative
    adversarial network \DeclareAcronymisishort=ISI, long=inter-symbol interference
    \DeclareAcronymbisoshort=BISO, long=binary-input symmetric-output
  prefs: []
  type: TYPE_NORMAL
- en: Recent Advances in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for Channel Coding: A Survey'
  prefs: []
  type: TYPE_NORMAL
- en: 'Toshiki Matsumine, , and Hideki Ochiai T. Matsumine is with the Institute of
    Advanced Sciences, Yokohama National University, Yokohama, Japan (e-mail: matsumine-toshiki-mh@ynu.ac.jp)H.
    Ochiai is with the Graduate School of Engineering, Osaka University, Osaka, Japan
    (e-mail: ochiai@comm.eng.osaka-u.ac.jp)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper provides a comprehensive survey on recent advances in deep learning
    (DL) techniques for the channel coding problems. Inspired by the recent successes
    of DL in a variety of research domains, its applications to the physical layer
    technologies have been extensively studied in recent years, and are expected to
    be a potential breakthrough in supporting the emerging use cases of the next generation
    wireless communication systems such as 6G. In this paper, we focus exclusively
    on the channel coding problems and review existing approaches that incorporate
    advanced DL techniques into code design and channel decoding. After briefly introducing
    the background of recent DL techniques, we categorize and summarize a variety
    of approaches, including model-free and mode-based DL, for the design and decoding
    of modern error-correcting codes, such as low-density parity check (LDPC) codes
    and polar codes, to highlight their potential advantages and challenges. Finally,
    the paper concludes with a discussion of open issues and future research directions
    in channel coding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Channel coding, deep learning (DL), low-density parity check (LDPC) codes, machine
    learning (ML), neural network, polar codes, turbo codes.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Channel coding is a well-established area of research with a long history dating
    back to the Shannon’s theory [[1](#bib.bib1)] where he introduced the *Shannon
    limit* as the maximum rate at which information can be transmitted over a given
    communication channel. Subsequently, researchers have made tremendous efforts
    to develop a practical coding scheme that approaches the Shannon limit at a realistic
    implementation cost [[2](#bib.bib2)]. The notable successes in coding theory include
    the invention of modern capacity-approaching codes such as turbo codes [[3](#bib.bib3)],
    \acldpc codes [[4](#bib.bib4)], and polar codes [[5](#bib.bib5)]. These coding
    techniques have contributed significantly to various communication systems, such
    as wired and wireless communications, as well as storage systems, from the viewpoint
    of improving reliability and energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the emerging wireless applications, including \acxr for telemedicine,
    tactile Internet, \acv2x, and wireless data centers, the next generation wireless
    communication systems impose unprecedentedly diverse and stringent requirements
    for, e.g., ultra-high data rate, ultra-low latency, and high energy efficiency
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    In 5G, the main use cases are \acembb, \acurllc, and \acmmtc, and for each case,
    the system requirement is specified in terms of a single \ackpi, such as throughput,
    latency, reliability, and energy efficiency. On the other hand, due to the diversity
    of applications, many use cases in the next wireless communications such as 6G
    will require trade-offs among different \acpkpi, which poses a new challenge for
    the design of physical layer techniques [[33](#bib.bib33), [36](#bib.bib36), [37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, the design of coding schemes has been based on mathematical models
    and expert knowledge, such as coding theory and information theory. Although this
    approach has contributed significantly to the recent progress in practical channel
    coding, it also has limitations. Specifically, it relies on mathematical models
    that do not fully capture real-world environments, and thus there is always a
    mismatch between the model for which we design systems and the actual environment
    to which they are applied. In addition, in the next generation communications,
    the system design problem will become increasingly complex due to demanding requirements
    and therefore will not be mathematically tractable in most cases. To address these
    issues, *data-driven* approaches to communication system design based on \acml
    techniques have emerged as a new paradigm that supports or replaces the conventional
    system design based on mathematical models.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, inspired by the recent successes of \acdl technologies in broad
    research areas, their applications in communication systems have been extensively
    studied. This DL trend has been accelerated by the development of dedicated DL
    frameworks, such as Tensorflow [[38](#bib.bib38)] and Pytorch [[39](#bib.bib39)],
    which makes it easier for researchers to implementate their DL algorithms. Furthermore,
    most of them are built with \acgpu acceleration provided by the NVIDIA \accuda
    Deep Neural Network library (cuDNN), which significantly speeds up DL training
    due to its ability to perform parallel computations and high memory bandwidth.
    In addition, various DL processors such as \acpfpga and \actpu have been explored
    in the literature [[40](#bib.bib40), [41](#bib.bib41)], enabling efficient hardware
    implementations of DL-based communications and networking in beyond 5G and 6G.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Survey papers related to our work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Reference | Contents related to channel coding. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | Wang et al. [[42](#bib.bib42)] | Review of early works on DL-based
    decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Zhang et al. [[43](#bib.bib43)] | Brief introduction of DL-based channel
    decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Gunduz et al. [[44](#bib.bib44)] | Brief review of DL-aided decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Balatsoukas et al. [[45](#bib.bib45)] | Review of deep unfolding for channel
    decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Samad et al. [[46](#bib.bib46)] | Addressing DL-based channel decoding.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[47](#bib.bib47)] | Review of DL-based decoding and code construction.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Ly et al. [[48](#bib.bib48)] | Review of DL applications for LDPC
    code identification, decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | Mao et al. [[49](#bib.bib49)] | Briefly addressing DL-aided decoder
    and genetic algorithm for code construction. |'
  prefs: []
  type: TYPE_TB
- en: '| Akrout et al. [[50](#bib.bib50)] | Domain generalization [[51](#bib.bib51),
    [52](#bib.bib52)] in the channel decoding problem. |'
  prefs: []
  type: TYPE_TB
- en: '| 2024 | Ye et al. [[53](#bib.bib53)] | Review of DL-based decoding for turbo,
    LDPC, and polar codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Rowshan et al. [[54](#bib.bib54)] | Comprehensive survey on channel coding
    with brief introduction of DL for channel decoding. |'
  prefs: []
  type: TYPE_TB
- en: I-A DL Applications to The Physical Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, DL has been successfully applied to the physical layer of communication
    systems ¹¹1We note that ML techniques for the physical layer have been studied
    sporadically for many years, e.g., in [[55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57)].
    However, the applications of DL are rather new, which started to flourish around
    2016. [[58](#bib.bib58), [42](#bib.bib42), [59](#bib.bib59), [60](#bib.bib60),
    [44](#bib.bib44), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [46](#bib.bib46), [65](#bib.bib65), [47](#bib.bib47), [48](#bib.bib48), [31](#bib.bib31),
    [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [49](#bib.bib49), [69](#bib.bib69),
    [70](#bib.bib70), [50](#bib.bib50), [71](#bib.bib71), [72](#bib.bib72), [73](#bib.bib73)].
    One of the seminal works is [[58](#bib.bib58)], where the authors introduced the
    concept of end-to-end learning for communication systems, which is considered
    as \acae. The authors also introduced other DL applications, such as modulation
    classification and radio transformer networks. Later on, many papers discussed
    potential applications of DL to communication problems, such as channel decoding,
    signal detection, channel modeling, and \acmimo signal detection [[42](#bib.bib42),
    [59](#bib.bib59), [60](#bib.bib60), [44](#bib.bib44), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: One of the notable advances in DL for the physical layer is the development
    of an open source Python library, called *Sionna*, which was released by NVIDIA
    in 2022 [[74](#bib.bib74)]. It supports link-level simulation of \acmu-MIMO systems
    with 5G-compliant channel codes, the \ac3gpp channel models, channel estimation,
    and so forth. Each building block is implemented using TensorFlow and allows for
    gradient-based optimization via backpropagation. Sionna alo has a native NVIDIA
    GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of publications related to DL-based approaches for physical layer
    technologies has been increasing almost exponentially, it is important to classify
    and summarize them to highlight the current state and challenges. However, despite
    its importance, only a handful of survey papers have been dedicated to the channel
    coding problems so far.
  prefs: []
  type: TYPE_NORMAL
- en: I-B Our Scope and Related Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing papers on DL for channel coding may be classified into the following
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DL-based code design,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DL-based channel decoding,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: End-to-end learning for communication systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In DL-based code design, the optimization of code parameters, such as the degree
    distribution of LDPC codes and the locations of frozen bits in polar codes, is
    performed using DL techniques. Channel decoding is a popular DL application as
    the decoding problem is essentially the classification problem which is what DL
    is good at. This approach utilizes a \acdnn to replace or augment a conventional
    channel decoder with the purpose of improving the error correction performance
    or reducing the complexity and/or latency. End-to-end learning of communication
    systems is another popular application, where a transmitter-receiver pair is often
    completely replaced by “black-box” DNNs and trained over a differential channel
    model in an end-to-end fashion. In this approach, not only a channel encoder-decoder
    pair can be trained, but also other physical layer components such as source encoder-decoder
    and symbol mapper-demapper.
  prefs: []
  type: TYPE_NORMAL
- en: Although end-to-end learning has been extensively studied in the literature,
    and this approach would be particularly promising for a new paradigm of semantic
    communication [[75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80), [68](#bib.bib68), [81](#bib.bib81), [82](#bib.bib82),
    [83](#bib.bib83), [70](#bib.bib70), [72](#bib.bib72)], the complete replacement
    of transceivers by DNNs poses several challenges in practice and does not seem
    feasible at this time. Therefore, this paper focuses on DL-based approaches that
    may be applicable to existing systems with appropriate modifications. In particular,
    we consider code design using offline DL techniques and DL-based channel decoding
    to replace or support conventional decoders²²2Although we mainly focus on *classical*
    error-correcting codes in this paper, DL-based code design and decoding has also
    been extensively studied in the realm of *quantum* error-correcting codes, e.g.,
    in [[84](#bib.bib84), [85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88),
    [89](#bib.bib89), [90](#bib.bib90)]..
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), we summarize existing survey papers related to
    our scope in this work. We emphasize that most of the existing survey papers cover
    a wide range of DL applications in the physical layer, rather than dealing with
    DL-assisted channel coding in a comprehensive manner. Nevertheless, the paper
    most closely related to our work would be [[47](#bib.bib47)], in which the authors
    discussed a wide range of DL applications in the physical layer, including the
    channel coding problems such as channel decoding and code construction. However,
    since its publication in 2020, a significant number of new techniques have been
    proposed. By focusing solely on the channel coding problems, we attempt to provide
    a comprehensive survey including the state-of-the-art techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall organization of this paper is visualized in Fig. [1](#S1.F1 "Figure
    1 ‣ I-B Our Scope and Related Works ‣ I Introduction ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey"). In Section [II](#S2 "II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"),
    we briefly introduce the basics of DL techniques and the state-of-the-art models
    to facilitate the understanding of our survey. In Section [III](#S3 "III DL for
    Code Design ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"),
    we review DL-based design of LDPC and polar codes. Then, in Section [IV](#S4 "IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"), we consider various DL approaches to the channel decoding problem.
    Finally, we conclude this paper by discussing the challenges and future directions
    in Section [V](#S5 "V Conclusion ‣ Recent Advances in Deep Learning for Channel
    Coding: A Survey") to stimulate further research. We provide a list of abbreviations
    that we use after Section [V](#S5 "V Conclusion ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/840fb1ce69b8c50b4867e0155d019a2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Organization of this survey paper.'
  prefs: []
  type: TYPE_NORMAL
- en: II Brief Introduction of Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we briefly review the basics of DL technologies, starting with
    neural networks and their optimization. We then introduce the state-of-the-art
    training and DL models. For details on the theory of general DL techniques, please
    refer, e.g., to [[91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99),
    [100](#bib.bib100)].
  prefs: []
  type: TYPE_NORMAL
- en: II-A Basic Principle of DL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL is a subfield of ML that uses DNNs with multiple hidden layers between input
    and output layers. Among various neural network structures, \acmlp is a class
    of fully connected feedforward neural networks that consist of at least one hidden
    layer in addition to input and output layers [[101](#bib.bib101)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/470593663da8895007c18d131cf83366.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An example of MLP with single hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a single hidden layer MLP is shown in Fig. [2](#S2.F2 "Figure
    2 ‣ II-A Basic Principle of DL ‣ II Brief Introduction of Deep Learning ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey"), where the input vector
    $\mathbf{x}\in\mathbb{R}^{3}$ is mapped to the output vector $\mathbf{y}\in\mathbb{R}^{2}$
    by applying a series of affine transformations and nonlinear activation functions
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{y}$ | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\mathbf{h}+\mathbf{b}_{1})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\phi_{1}(\mathbf{W}_{1}\phi_{0}(\mathbf{W}_{0}\mathbf{x}+\mathbf{b}_{0})+\mathbf{b}_{1}),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}_{0}\in\mathbb{R}^{3\times 3}$ and $\mathbf{W}_{1}\in\mathbb{R}^{3\times
    2}$ are weight matrices, $\mathbf{b}_{0}\in\mathbb{R}^{3}$ and $\mathbf{b}_{1}\in\mathbb{R}^{2}$
    are bias terms, and $\phi_{i}(\cdot)$ with $i\in\{0,1\}$ denotes the element-wise
    application of a nonlinear activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The nonlinear activation function allows the neural network to approximate highly
    complex functions, and the choice of activation functions has a significant impact
    on the resulting performance. Although there are a number of activation functions
    [[102](#bib.bib102)], one of the most widely used modern activation functions
    is \acrelu [[103](#bib.bib103)] and its variants such as Leaky ReLU [[104](#bib.bib104)]
    and parametric ReLU [[105](#bib.bib105)].
  prefs: []
  type: TYPE_NORMAL
- en: As for the optimization of the DNN parameters, i.e., $\Theta\triangleq\{\mathbf{W}_{0},\mathbf{W}_{1},\mathbf{b}_{0},\mathbf{b}_{1}\}$
    in our example, the most common approach is gradient descent, which is a first-order
    iterative algorithm for finding a local minimum of a differentiable function.
    The basic idea of gradient descent is to update the parameters in the opposite
    direction of the gradient of the differentiable loss function that we wish to
    minimize. Letting $f(\Theta)$ denote the loss function that is differentiable
    with respect to the parameter set $\Theta$, in the $i$-th iteration of gradient
    descent, the trainable parameters are updated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Theta_{i+1}=\Theta_{i}-\eta\nabla f(\Theta_{i}),$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\eta\in\mathbb{R}_{+}$ is a learning rate that determines the size of
    the steps taken in the direction of steepest descent.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent algorithms can be classified according to the amount of data
    used to compute the gradient, namely, \acbgd, \acsgd, and \acmbsgd. BGD calculates
    the gradients for the entire training dataset, while SGD performs a parameter
    update (and thus gradient calculation) for each training data sample. Meanwhile,
    MBSGD substitutes small data batches for single samples in SGD, thereby reducing
    the variance of the parameter updates, which can lead to more stable convergence.
    Furthermore, the gradient computation in MBSGD can be efficiently performed by
    DL libraries. For these reasons, MBSGD is one of the most popular stochastic optimization
    methods for training DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: However, the standard MBSGD does not necessarily guarantee good convergence,
    and many improvements have been proposed that adaptively control the learning
    rate during training. These approaches include Momentum [[106](#bib.bib106)],
    Adagrad [[107](#bib.bib107)], Adadelta [[108](#bib.bib108)], RMSprop ³³3Originally
    proposed in [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).,
    Nadam [[109](#bib.bib109)], and Adam [[110](#bib.bib110)]. Implementations of
    these optimizers are available in DL frameworks such as Tensorflow [[38](#bib.bib38)]
    and Pytorch [[39](#bib.bib39)]. For more details on SGD algorithms, see [[111](#bib.bib111)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B Learning Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many training methods for ML techniques. In the following, we introduce
    some of the major approaches that are often applied to the design of communication
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Supervised and Unsupervised Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Supervised learning trains algorithms based on labeled datasets consisting of
    pairs of inputs and corresponding correct outputs, i.e., ground truth. The goal
    is to analyze patterns from a large dataset and predict outcomes for new data.
    Supervised learning is commonly used for tasks such as classification and regression.
    Since the channel decoding problem can be seen as a type of classification, the
    simplest approach would be to train a DL-based channel decoder, where a DNN is
    trained to estimate a transmitted codeword by minimizing the error between the
    correct and estimated codewords.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is another type of ML algorithm that learns patterns from
    data without human supervision. Self-supervised learning, often used to train
    \acpllm, can also be considered unsupervised learning in the sense that it uses
    the data itself to generate supervising signals, rather than relying on human
    supervision. There is also an approach called semi-supervised learning, which
    combines both supervised learning and unsupervised learning, i.e., it uses both
    labeled and unlabeled data. Although these approaches are particularly suitable
    for the practical scenario where a sufficiently large labeled dataset is not available,
    their applications to channel decoding are rather new topics to be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: rl is an experience-driven autonomous learning framework where an intelligent
    agent learns to take actions in a dynamic environment in order to maximize the
    cumulative reward [[112](#bib.bib112)]. RL is typically modeled as \acmdp which
    consists of
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a set of environment and agent states $\mathcal{S}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a set of actions $\mathcal{A}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the transition probability from state $s_{t}$ to state $s_{t+1}$ upon action
    $a_{t}$ at time $t$, denoted by $\mathcal{P}(s_{t+1}|s_{t},a_{t})$, and the corresponding
    reward function $\mathcal{R}(s_{t},a_{t},s_{t+1})$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The process is shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-B2 Reinforcement Learning
    ‣ II-B Learning Approaches ‣ II Brief Introduction of Deep Learning ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). At each time step $t$, the agent
    observes a state $s_{t}\in\mathcal{S}$ and takes an action $a_{t}\in\mathcal{A}$,
    following a policy $\pi(a_{t}|s_{t})$. Then the agent receives a scalar reward
    $r_{t}$, and transitions to the next state $s_{t+1}$, according to the reward
    function $\mathcal{R}(s_{t},a_{t},s_{t+1})$ and state transition probability $\mathcal{P}(s_{t+1}|s_{t},a_{t})$,
    respectively. This process continues until the agent reaches a terminal state.
    The return is the discounted, accumulated reward with the discount factor $\gamma\in(0,1]$.
    The agent aims to maximize the expectation of such long term return from each
    state.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cdc352d0a7a28562faa5fb732e6729d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A typical reinforcement learning framework.'
  prefs: []
  type: TYPE_NORMAL
- en: In many practical problems, the states of the MDP are high-dimensional and difficult
    to solve with traditional RL algorithms. On the other hand, thanks to their powerful
    function approximation properties, the use of DNNs to approximate the optimal
    policy and/or the optimal value functions in RL provides an efficient way to overcome
    these problems. This approach, called \acdrl, has achieved remarkable results
    in a variety of research areas. In particular, \acdqn [[113](#bib.bib113)], where
    a DNN model is built to approximate the Q function (the value of an action in
    a given state), showed impressive results in Atari [[113](#bib.bib113)]. More
    details on DRL can be found, for example, in [[114](#bib.bib114), [115](#bib.bib115),
    [116](#bib.bib116), [117](#bib.bib117)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B3 Transfer Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transfer learning is a technique for transferring knowledge learned from a
    task in a source domain to imp performance on a related task in a target domain
    [[118](#bib.bib118), [119](#bib.bib119), [120](#bib.bib120), [121](#bib.bib121),
    [122](#bib.bib122)]. Transfer learning addresses the problem of insufficient labeled
    training data by transferring the knowledge across task domains. This concept
    is illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ II-B3 Transfer Learning ‣ II-B Learning
    Approaches ‣ II Brief Introduction of Deep Learning ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey"). Note, however, that the transferred knowledge
    may be worthless if there is little or even nothing in common between the source
    and target domains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1357233c5e769ed7881384a8e4790871.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The concept of transfer learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the channel coding problems, transfer learning can be useful to adapt DL-based
    code design and decoding trained for a certain channel model and code parameters
    to new channel models or parameters. For example, we usually train a DNN for design
    or decoding assuming a certain code rate, and adapting to a new code rate requires
    re-training of the DNN, which is time consuming and computationally expensive.
    Since codes derived from a single mother code by rate matching, i.e., puncturing
    and shortening, may have many similarities, transfer learning could be used to
    significantly reduce the computational burden of re-training or to improve the
    performance with the new parameter.
  prefs: []
  type: TYPE_NORMAL
- en: II-B4 Multi-Task Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In multi-task learning, a set of multiple tasks is solved jointly, sharing an
    inductive bias among them [[123](#bib.bib123), [124](#bib.bib124)]. Multi-task
    learning is inherently a multi-objective problem because different tasks may conflict,
    requiring a trade-off. A common compromise is to optimize a proxy objective that
    minimizes a weighted linear combination of per-task losses. Since this joint representation
    must capture useful features across all tasks, multi-task learning can hinder
    individual task performance if the different tasks seek conflicting representations,
    i.e., the gradients of different tasks point in opposing directions or differ
    significantly in magnitude. This phenomenon is commonly known as negative transfer.
  prefs: []
  type: TYPE_NORMAL
- en: There are some similarities between transfer learning and multi-task learning.
    Both aim to improve learners’ performance by knowledge transfer. On the other
    hand, the main difference is that the former transfers the knowledge contained
    in the related domains, while the latter transfers the knowledge by learning some
    related tasks simultaneously. In other words, multi-task learning pays equal attention
    to each task, while transfer learning pays more attention to the target task than
    to the source task.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to transfer learning, multi-task learning could be used to efficiently
    support multiple different code parameters. Furthermore, multi-task learning can
    be used for multi-objective optimization, which is common in many communication
    system design problems. Application examples of multi-task learning include AE-based
    constellation design that attempts to jointly minimize \acber and \acpapr for
    \acofdm systems [[125](#bib.bib125)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B5 Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike traditional learning approaches that attempt to solve tasks from scratch
    with a fixed algorithm, meta-learning aims to learn the learning algorithm itself
    by learning from previous experience or tasks [[126](#bib.bib126), [127](#bib.bib127)].
    This *learning-to-learn* framework can lead to several benefits, such as improved
    data and computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In a meta-learning framework, there are two types of data, a larger data set
    of examples from related tasks (meta-training data) and a small training data
    set for a new task (meta-testing data). Standard meta-learning consists of two
    phases, 1) meta-training where a set of hyperparameters is optimized given the
    meta-training data set, and 2) meta-testing where model parameters, which are
    initialized with the meta-trained hyperparameters, are optimized using the meta-testing
    data. Thus, the meta-training phase aims to optimize hyperparameters that allow
    efficient training on a new, *a priori* unknown, target task in the meta-testing
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-learning could naturally be applied to adaptive decoder design, where the
    decoder parameters are initialized by meta-training and then optimized based on
    meta-testing to adapt to a new channel. In addition, the concept can be applied
    to a wide range of problems in the physical layer, such as signal demodulation,
    joint transmitter and receiver optimization via end-to-end learning, channel prediction,
    and so forth, as reviewed in [[128](#bib.bib128)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B6 Curriculum Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Curriculum learning, originally proposed in [[129](#bib.bib129)], is a training
    strategy that trains a machine learning model from easier data to harder data,
    imitating human learning [[130](#bib.bib130), [131](#bib.bib131)]. The basic idea
    is to “start small” [[132](#bib.bib132)], i.e., to train the ML model with easier
    data subsets, and then gradually increase the difficulty level of the data until
    the entire training dataset is used. Curriculum learning can be seen as a special
    form of the continuation method which is a general strategy for global optimization
    of non-convex functions [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: As the idea of curriculum learning serves as a general training strategy, it
    has been exploited in a considerable range of applications. In contrast to the
    standard training approach based on random data shuffling, curriculum learning
    can provide performance improvements with faster training convergence speed. In
    curriculum learning, the design of a curriculum strategy, i.e., a sequence of
    training criteria, plays a key role.
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum learning can be beneficial for designing communication systems including
    channel coding schemes. For example, designing and decoding long codes is generally
    a more difficult task than designing and decoding short codes. As such, instead
    of learning to design or decode long codes directly, curriculum learning strategies
    that start from training with short codes and then gradually increase the code
    length have the potential to achieve not only faster training convergence, but
    also better performance.
  prefs: []
  type: TYPE_NORMAL
- en: II-C State-of-the-Art Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finding an appropriate architecture is important when applying DL to the problems
    in communication systems. In the following, we briefly review the representative
    DL models that are useful for such problems.
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Convolutional Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \Acp
  prefs: []
  type: TYPE_NORMAL
- en: 'cnn have made tremendous success in a vast research field including computer
    vision and \acnlp [[133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)].
    A CNN is a type of feed-forward neural network with a convolutional layer that
    learns features by applying filters (or kernels) to data. A simple example of
    a CNN architecture is illustrated in Fig. [5](#S2.F5 "Figure 5 ‣ II-C1 Convolutional
    Neural Network ‣ II-C State-of-the-Art Models ‣ II Brief Introduction of Deep
    Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"). The
    CNN has fewer connections and parameters than the MLP since each neuron in a convolutional
    layer receives input only from a restricted area of the previous layer. This restricted
    area is called the receptive field of the neuron, and in the case of a fully connected
    layer, the receptive field corresponds to the entire previous layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c800063b51bcaadd7ab64e26da2e313b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A simple CNN architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs have been particularly successful in image recognition tasks, achieving
    state-of-the-art results on several benchmarks such as the \acilsvrc. Representative
    CNN models include AlexNet [[136](#bib.bib136)], VGGnet [[137](#bib.bib137)],
    Inception (GoogLeNet) [[138](#bib.bib138)], ResNet [[139](#bib.bib139)], and DenseNet
    [[140](#bib.bib140)]. Their success is due to their ability to capture spatial
    features and patterns by using a hierarchical architecture of layers that perform
    convolution operations and extract features at different levels of abstraction
    [[141](#bib.bib141)].
  prefs: []
  type: TYPE_NORMAL
- en: As for their applications in channel coding, they are often used for decoding,
    and have been demonstrated to be more efficient than standard MLP. Other popular
    applications of CNN in the physical layer include channel estimation in time-frequency
    domain for OFDM systems [[142](#bib.bib142)] and fully CNN receiver that replaces
    the conventional channel estimator, equalizer, and demapper [[143](#bib.bib143)].
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Recurrent Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike unidirectional feedforward neural networks, a \acrnn is a bi-directional
    artificial neural network that is capable of learning long-term dependencies from
    sequential data [[92](#bib.bib92), [144](#bib.bib144)]. Due to their ability to
    use internal state (memory) to process arbitrary input sequences, they are particularly
    suited for processing time-series data such as speech recognition. On the other
    hand, due to the recurrent connections, classical RNNs have the gradient vanishing
    and exploding problems, i.e., the long-term gradients may not converge and approach
    zero or infinity during backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: 'lstm is one of the most popular RNN models that can reduce the effects of vanishing
    and exploding gradients [[145](#bib.bib145)] [[146](#bib.bib146)] by introducing
    a gating mechanism to input or forget certain features. Another popular model
    is a \acgru [[147](#bib.bib147)], in which recurrent units adaptively capture
    dependencies of different time scales to accommodate the higher memory requirements
    of LSTM. A comparison of these architectures is shown in Fig. [6](#S2.F6 "Figure
    6 ‣ II-C2 Recurrent Neural Network ‣ II-C State-of-the-Art Models ‣ II Brief Introduction
    of Deep Learning ‣ Recent Advances in Deep Learning for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5373013e5ecd90824e08169359eb5364.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4c304e920fa55d78b497fd40e507c97b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) LSTM with a forget gate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23564bdee343973cc561b1aa1085e54f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) GRU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparison of RNN, LSTM, and GRU architectures [[148](#bib.bib148)].'
  prefs: []
  type: TYPE_NORMAL
- en: For more details of RNN, in particular LSTM, please refer to [[149](#bib.bib149),
    [148](#bib.bib148), [150](#bib.bib150)].
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM and GRU models have been widely applied to communication systems. In
    particular, due to the analogous structure to convolutional codes, RNNs may be
    well suited for decoding of convolutional codes. Similarly, RNNs have been successfully
    applied to signal detection for channels with memory [[151](#bib.bib151)].
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 Graph Neural Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While ML effectively captures hidden patterns in Euclidean data, a common assumption
    of existing ML algorithms is that instances are independent of each other. This
    assumption no longer holds for graph data, where every node is related to others.
    Extending DNN models to non-Euclidean domains, which is generally referred to
    as geometric DL, has been an emerging area of research. In particular, a \acgnn
    that operates on the graph domain has recently become a popular graph analysis
    method [[152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155)].
  prefs: []
  type: TYPE_NORMAL
- en: Let $G\in(\mathcal{V},\mathcal{E})$ be a graph, where $\mathcal{V}$ is the node
    set and $\mathcal{E}$ is the edge set. Let $\mathcal{N}_{u}$ be the neighborhood
    of some node $u\in\mathcal{V}$. Additionally, let $\mathbf{x}_{u}$ be the properties
    of node $u\in\mathcal{V}$. GNN implements a permutation-equivalent layer, called
    a GNN layer, which maps a representation of a graph into an updated representation
    of the same graph. Although the design of GNN layers is one of the active research
    areas, one popular approach is \acmpnn layers (other popular approaches include
    graph convolutional networks [[156](#bib.bib156)] and graph attention networks
    [[157](#bib.bib157)]). In an MPNN layer in a generic GNN, nodes update their representations
    by aggregating the messages received from their immediate neighbors [[158](#bib.bib158)]
    and the output of the layer (node representations $\mathbf{h}_{u}$ for each $u\in\mathcal{V}$)
    is expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}\psi(\mathbf{x}_{u},\mathbf{x}_{v})\right),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ and $\psi$ are typically trainable differential functions, whereas
    $\bigoplus$ is a nonparametric permutation invariant aggregation operator that
    can take an arbitrary number of inputs. In particular, $\phi$ and $\psi$ are referred
    to as update and message functions, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its close relationship to a Tanner graph, a GNN is particularly useful
    for designing and decoding codes over graph. One of the major advantages of GNN
    is their scalability, i.e., a GNN trained for a small code length will generalize
    to any code length, while this usually requires additional training.
  prefs: []
  type: TYPE_NORMAL
- en: II-C4 Transformer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Transformer is a DL architecture based on the multi-head attention mechanism
    [[159](#bib.bib159)]. As depicted in Fig. [7](#S2.F7 "Figure 7 ‣ II-C4 Transformer
    ‣ II-C State-of-the-Art Models ‣ II Brief Introduction of Deep Learning ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey"), it consists of an encoder
    and a decoder, each of which has several Transformer blocks having the same architecture.
    Each Transformer block consists of a multi-head attention layer, a feed-forward
    neural network, a shortcut connection, and a layer normalization. Given a sequence
    of elements, the self-attention mechanism explicitly models the dependencies among
    all entities of a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e45f054bb76ca0fc357d3fe81ad50e48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The Transformer architecture [[159](#bib.bib159)].'
  prefs: []
  type: TYPE_NORMAL
- en: It has no recurrent units, and thus requires less training time than previous
    RNN architectures, such as LSTM, and its later variant has been widely adopted
    for training most of representative LLMs, such as Open AI’s \acgpt series [[160](#bib.bib160)],
    Meta’s \acllama [[161](#bib.bib161)], Googles \acpalm [[162](#bib.bib162)], and
    Gemini [[163](#bib.bib163)] are based on the Transformer model. More details and
    the applications of the Transformer can be found in [[164](#bib.bib164), [165](#bib.bib165),
    [166](#bib.bib166)].
  prefs: []
  type: TYPE_NORMAL
- en: Although Transformer was originally proposed for NLP tasks [[159](#bib.bib159)],
    it has been successfully adopted for a variety of tasks such as computer vision
    [[167](#bib.bib167)], audio applications [[168](#bib.bib168)]. For the physical
    layer technologies, the use of Transformer is rather new [[169](#bib.bib169)].
    Due to its excellent performance, Transformer has the potential to improve the
    performance of existing DL methods for communication engineering problems.
  prefs: []
  type: TYPE_NORMAL
- en: II-C5 Diffusion Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \Acp
  prefs: []
  type: TYPE_NORMAL
- en: 'dm are a class of probabilistic generative models that progressively corrupt
    data by injecting noise, and then learn to reverse this process for sample generation.
    The training procedure consists of two phases: the forward diffusion process and
    the backward denoising process [[170](#bib.bib170)]. In the forward process, typically
    Gaussian noise is injected into the training data until it becomes pure Gaussian.
    In the backward process, the noise is sequentially removed to reconstruct the
    original image. The noise subtracted at each step is estimated by a neural network.
    Among different formulations [[171](#bib.bib171), [172](#bib.bib172)], \acddpm
    [[173](#bib.bib173)] is a representative DM inspired by the theory of non-equilibrium
    thermodynamics.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to its high generative quality and versatility, DM could be applied to many
    problems in communication systems, such as channel estimation [[174](#bib.bib174)],
    signal detection [[175](#bib.bib175)], AE [[176](#bib.bib176), [177](#bib.bib177),
    [178](#bib.bib178)], and network optimization problems [[179](#bib.bib179), [180](#bib.bib180)].
    However, the application of DMs to the physical layer is a relatively new area
    of research [[181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)].
  prefs: []
  type: TYPE_NORMAL
- en: III DL for Code Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of DL-based polar code design.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Reference | Main Contributions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Advanced Decoding Schemes | Ebada et al. [[184](#bib.bib184)] | Design for
    BP decoding with finite iteration count. |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[185](#bib.bib185)] | RL-based design of polar codes for SCL
    decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Leonardon et al. [[186](#bib.bib186)] | Design that minimizes BLER under
    SCL decoding via projected gradient descent. |'
  prefs: []
  type: TYPE_TB
- en: '| Liao et al. [[187](#bib.bib187)] | GNN-based polar code construction for
    CA-SCL decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Miloslavskaya et al. [[188](#bib.bib188)] | Optimization of polar codes with
    dynamic frozen bits under SCL decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Nested Polar Codes | Huang et al. [[185](#bib.bib185)] | Construction of
    nested polar codes via advantage actor-critic algorithms. |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[189](#bib.bib189)] | Stochastic policy optimization by a customized
    network. |'
  prefs: []
  type: TYPE_TB
- en: '| Ankireddy et al. [[190](#bib.bib190)] | Nested polar code construction based
    on sequence modeling and Transformer. |'
  prefs: []
  type: TYPE_TB
- en: '| Polar Codes with Large Kernel | Hebbar et al. [[191](#bib.bib191)] | Polar
    codes via large nonlinear neural network-based kernels. |'
  prefs: []
  type: TYPE_TB
- en: '| PAC Codes | Mishra et al. [[192](#bib.bib192)] | RL-based algorithm for rate-profile
    construction of PAC codes. |'
  prefs: []
  type: TYPE_TB
- en: 'Modern capacity-approaching codes such as LDPC and polar codes are usually
    designed based on well-established analytical tools, such as \acde [[193](#bib.bib193),
    [194](#bib.bib194)] and \acexit chart [[195](#bib.bib195)]. However, these techniques
    rely on assumptions that do not hold in practice. For example, for the design
    of LDPC codes, DE and EXIT chart analyses assume simple channel models, such as
    \acbiawgn channels, infinite code length, and unlimited \acbp decoding iterations.
    These techniques are also used for polar code design, but they are again limited
    to simple channel models and decoding schemes such as \acsc decoding. For more
    realistic channel models and advanced decoding schemes, DL could replace or support
    existing code design techniques. We have summarized the DL-based approaches to
    polar code design in Table [II](#S3.T2 "TABLE II ‣ III DL for Code Design ‣ Recent
    Advances in Deep Learning for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-A LDPC Code Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Irregular LDPC codes are characterized by a variable degree distribution $\lambda(x)$
    and a check degree distribution $\rho(x)$, which are expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\lambda(x)=\sum^{d_{\text{v}}}_{i=2}\lambda_{i}x^{i-1},\
    \rho(x)=\sum^{d_{\text{c}}}_{i=2}\rho_{i}x^{i-1},$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{i}$ and $\rho_{i}$ represent the fraction of edges emanating
    from \acpvn and \acpcn of degree $i$ and $\lambda(1)=\rho(1)=1$. The maximum variable
    degree and check degree are denoted by $d_{\text{v}}$ and $d_{\text{c}}$, respectively.
    The degree distribution is often optimized to maximize the iterative decoding
    threshold, which is defined as the lowest channel \acsnr at which the message
    distribution in BP evolves in such a way that its associated error probability
    converges to zero as the number of iterations tends to infinity. The method of
    identifying a threshold by tracking the evolution of the message distribution
    is termed DE [[193](#bib.bib193)].
  prefs: []
  type: TYPE_NORMAL
- en: The code design problem belongs to the class of nonlinear constraint satisfaction
    problems with continuous space parameters, where we first explore the space of
    degree distributions to find degree distribution pairs, traditionally solved by
    differential evolution [[193](#bib.bib193)], and then evaluate the BP threshold
    of the selected pairs via DE. In [[196](#bib.bib196)], the authors modeled the
    code design process as a supervised learning problem by mapping the recursive
    update equation of DE to an RNN architecture, which they refer to as neural density
    evolution (NDE). They also proposed a multi-objective loss function for NDE that
    ensures its high configurability, i.e., various code rates and maximum degrees.
    Their simulations show that the proposed designs achieve the performance of state-of-the-art
    designs in asymptotic settings for a variety of codeword lengths and channels.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Polar Code Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An encoder of polar codes of length $N$ is represented by the generator matrix
    <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{G}_{N}=\left(\begin{smallmatrix}1&amp;0\\
  prefs: []
  type: TYPE_NORMAL
- en: 1&amp;1\end{smallmatrix}\right)^{\otimes n}\in\mathbb{F}_{2}^{N\times N}" display="inline"><semantics
    id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><msub
    id="S3.SS2.p1.2.m2.1.2.2" xref="S3.SS2.p1.2.m2.1.2.2.cmml"><mi id="S3.SS2.p1.2.m2.1.2.2.2"
    xref="S3.SS2.p1.2.m2.1.2.2.2.cmml">𝐆</mi><mi id="S3.SS2.p1.2.m2.1.2.2.3" xref="S3.SS2.p1.2.m2.1.2.2.3.cmml">N</mi></msub><mo
    id="S3.SS2.p1.2.m2.1.2.3" xref="S3.SS2.p1.2.m2.1.2.3.cmml">=</mo><msup id="S3.SS2.p1.2.m2.1.2.4"
    xref="S3.SS2.p1.2.m2.1.2.4.cmml"><mrow id="S3.SS2.p1.2.m2.1.2.4.2.2" xref="S3.SS2.p1.2.m2.1.1.cmml"><mo
    id="S3.SS2.p1.2.m2.1.2.4.2.2.1" xref="S3.SS2.p1.2.m2.1.1.cmml">(</mo><mtable columnspacing="5pt"
    rowspacing="0pt" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mtr id="S3.SS2.p1.2.m2.1.1a"
    xref="S3.SS2.p1.2.m2.1.1.cmml"><mtd id="S3.SS2.p1.2.m2.1.1b" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn
    mathsize="70%" id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">1</mn></mtd><mtd
    id="S3.SS2.p1.2.m2.1.1c" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn mathsize="70%" id="S3.SS2.p1.2.m2.1.1.1.2.1"
    xref="S3.SS2.p1.2.m2.1.1.1.2.1.cmml">0</mn></mtd></mtr><mtr id="S3.SS2.p1.2.m2.1.1d"
    xref="S3.SS2.p1.2.m2.1.1.cmml"><mtd id="S3.SS2.p1.2.m2.1.1e" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn
    mathsize="70%" id="S3.SS2.p1.2.m2.1.1.2.1.1" xref="S3.SS2.p1.2.m2.1.1.2.1.1.cmml">1</mn></mtd><mtd
    id="S3.SS2.p1.2.m2.1.1f" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn mathsize="70%" id="S3.SS2.p1.2.m2.1.1.2.2.1"
    xref="S3.SS2.p1.2.m2.1.1.2.2.1.cmml">1</mn></mtd></mtr></mtable><mo id="S3.SS2.p1.2.m2.1.2.4.2.2.2"
    xref="S3.SS2.p1.2.m2.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.p1.2.m2.1.2.4.3"
    xref="S3.SS2.p1.2.m2.1.2.4.3.cmml"><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.2.m2.1.2.4.3.1"
    xref="S3.SS2.p1.2.m2.1.2.4.3.1.cmml">⊗</mo><mi id="S3.SS2.p1.2.m2.1.2.4.3.3" xref="S3.SS2.p1.2.m2.1.2.4.3.3.cmml">n</mi></mrow></msup><mo
    id="S3.SS2.p1.2.m2.1.2.5" xref="S3.SS2.p1.2.m2.1.2.5.cmml">∈</mo><msubsup id="S3.SS2.p1.2.m2.1.2.6"
    xref="S3.SS2.p1.2.m2.1.2.6.cmml"><mi id="S3.SS2.p1.2.m2.1.2.6.2.2" xref="S3.SS2.p1.2.m2.1.2.6.2.2.cmml">𝔽</mi><mn
    id="S3.SS2.p1.2.m2.1.2.6.2.3" xref="S3.SS2.p1.2.m2.1.2.6.2.3.cmml">2</mn><mrow
    id="S3.SS2.p1.2.m2.1.2.6.3" xref="S3.SS2.p1.2.m2.1.2.6.3.cmml"><mi id="S3.SS2.p1.2.m2.1.2.6.3.2"
    xref="S3.SS2.p1.2.m2.1.2.6.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em"
    id="S3.SS2.p1.2.m2.1.2.6.3.1" xref="S3.SS2.p1.2.m2.1.2.6.3.1.cmml">×</mo><mi id="S3.SS2.p1.2.m2.1.2.6.3.3"
    xref="S3.SS2.p1.2.m2.1.2.6.3.3.cmml">N</mi></mrow></msubsup></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml"
    xref="S3.SS2.p1.2.m2.1.2"><apply id="S3.SS2.p1.2.m2.1.2b.cmml" xref="S3.SS2.p1.2.m2.1.2"><apply
    id="S3.SS2.p1.2.m2.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2"><csymbol cd="ambiguous"
    id="S3.SS2.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.2">subscript</csymbol><ci
    id="S3.SS2.p1.2.m2.1.2.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2.2">𝐆</ci><ci id="S3.SS2.p1.2.m2.1.2.2.3.cmml"
    xref="S3.SS2.p1.2.m2.1.2.2.3">𝑁</ci></apply><apply id="S3.SS2.p1.2.m2.1.2.4.cmml"
    xref="S3.SS2.p1.2.m2.1.2.4"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.4.1.cmml"
    xref="S3.SS2.p1.2.m2.1.2.4">superscript</csymbol><matrix id="S3.SS2.p1.2.m2.1.1.cmml"
    xref="S3.SS2.p1.2.m2.1.2.4.2.2"><matrixrow id="S3.SS2.p1.2.m2.1.1a.cmml" xref="S3.SS2.p1.2.m2.1.2.4.2.2"><cn
    type="integer" id="S3.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1">1</cn><cn
    type="integer" id="S3.SS2.p1.2.m2.1.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.2.1">0</cn></matrixrow><matrixrow
    id="S3.SS2.p1.2.m2.1.1b.cmml" xref="S3.SS2.p1.2.m2.1.2.4.2.2"><cn type="integer"
    id="S3.SS2.p1.2.m2.1.1.2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2.1.1">1</cn><cn type="integer"
    id="S3.SS2.p1.2.m2.1.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2.1">1</cn></matrixrow></matrix><apply
    id="S3.SS2.p1.2.m2.1.2.4.3.cmml" xref="S3.SS2.p1.2.m2.1.2.4.3"><csymbol cd="latexml"
    id="S3.SS2.p1.2.m2.1.2.4.3.1.cmml" xref="S3.SS2.p1.2.m2.1.2.4.3.1">tensor-product</csymbol><csymbol
    cd="latexml" id="S3.SS2.p1.2.m2.1.2.4.3.2.cmml" xref="S3.SS2.p1.2.m2.1.2.4.3.2">absent</csymbol><ci
    id="S3.SS2.p1.2.m2.1.2.4.3.3.cmml" xref="S3.SS2.p1.2.m2.1.2.4.3.3">𝑛</ci></apply></apply></apply><apply
    id="S3.SS2.p1.2.m2.1.2c.cmml" xref="S3.SS2.p1.2.m2.1.2"><apply id="S3.SS2.p1.2.m2.1.2.6.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.6.1.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6">superscript</csymbol><apply id="S3.SS2.p1.2.m2.1.2.6.2.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.6.2.1.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.2.6.2.2.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6.2.2">𝔽</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.2.6.2.3.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6.2.3">2</cn></apply><apply id="S3.SS2.p1.2.m2.1.2.6.3.cmml"
    xref="S3.SS2.p1.2.m2.1.2.6.3"><ci id="S3.SS2.p1.2.m2.1.2.6.3.2.cmml" xref="S3.SS2.p1.2.m2.1.2.6.3.2">𝑁</ci><ci
    id="S3.SS2.p1.2.m2.1.2.6.3.3.cmml" xref="S3.SS2.p1.2.m2.1.2.6.3.3">𝑁</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathbf{G}_{N}=\left(\begin{smallmatrix}1&0\\
    1&1\end{smallmatrix}\right)^{\otimes n}\in\mathbb{F}_{2}^{N\times N}</annotation></semantics></math>,
    where $n=\log_{2}{N}$ is a positive integer and $\mathbf{A}^{\otimes n}=\mathbf{A}\otimes\mathbf{A}^{\otimes(n-1)}$
    is the $n$th Kronecker power of the matrix $\mathbf{A}$ [[5](#bib.bib5)]. Let
    $\mathcal{I}\subset\{0,1,\ldots,N-1\}$ denote a set of information bit indices
    with its cardinality $K=|\mathcal{I}|$, and $\mathcal{F}=\{0,1,\ldots,N-1\}\backslash\mathcal{I}$
    denote the complement of $\mathcal{I}$ with its cardinality $|\mathcal{F}|=N-K$.
    Letting $\mathbf{u}=(u_{0},u_{1},\ldots,u_{N-1})\in\mathbb{F}_{2}^{N}$ be an input
    vector to the polar encoder, the bits $u_{i}$ with $i\in\mathcal{I}$ are chosen
    to carry information, whereas those with $i\in\mathcal{F}$ are frozen (i.e., fixed
    to a predetermined bit value known by encoder and decoder). The code rate of the
    polar code is thus $R=K/N$. Polar code design or construction is equivalent to
    identifying an appropriate index set $\mathcal{I}$ for a given channel model and
    decoding scheme.
  prefs: []
  type: TYPE_NORMAL
- en: In his original paper, Arıkan suggested using Monte-Carlo simulations to estimate
    the reliabilities of bit channels [[5](#bib.bib5)]. Subsequently, DE [[194](#bib.bib194)]
    and its improved version [[197](#bib.bib197)] were proposed to accurately estimate
    the reliabilities at the cost of high complexity. This complexity was alleviated
    by \acga of DE [[198](#bib.bib198)], improved GA [[199](#bib.bib199)], and \acrca
    [[200](#bib.bib200)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Design for Advanced Decoding Schemes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although polar codes were originally proposed with SC decoding [[5](#bib.bib5)],
    their finite-length performance is unsatisfactory. In order for polar codes to
    achieve performance comparable to other capacity-approaching codes, advanced decoding
    schemes, such as \acscl or \accascl decoding [[201](#bib.bib201)], are required.
    However, the above-mentioned polar code construction schemes assume SC decoding
    and there is no explicit approach to designing polar codes for SCL and CA-SCL
    decoding.
  prefs: []
  type: TYPE_NORMAL
- en: In [[184](#bib.bib184)], the authors proposed to design polar codes for BP decoding
    with limited number of iterations over both \acawgn and Rayleigh fading channels.
    By representing the frozen and non-frozen bit vectors by soft-valued vectors,
    which can be considered as training weights of a neural network, training is performed
    to minimize the cross entropy loss between transmitted and estimated codewords
    while satisfying the target code rate requirement and training convergence. The
    simulations showed that the learned polar code outperforms the performance of
    the 5G polar code under Arıkan’s conventional BP decoder.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the authors in [[202](#bib.bib202), [185](#bib.bib185)] proposed
    a genetic algorithm and RL-based design of polar codes for SCL decoding. As a
    reward function in RL, the authors computed an SNR required for a code to achieve
    a target \acbler via Monte-Carlo simulations. In the same line of research, the
    authors in [[203](#bib.bib203)] proposed a tabular RL-based construction of polar
    codes for SCL decoding. Instead of evaluating the BLER based on the Monte-Carlo
    method as in [[202](#bib.bib202), [185](#bib.bib185)], they designed the reward
    function that sends a negative immediate reward (penalty) to the agent when the
    selected action causes a frame error in genie-aided SCL decoding⁴⁴4In genie-aided
    SCL decoding, the decoder can always output the correct codeword if it is in the
    list.. The proposed method achieved comparable or slightly better performance
    with a lower computational complexity in training than the method in [[185](#bib.bib185)].
  prefs: []
  type: TYPE_NORMAL
- en: As another approach, the authors in [[186](#bib.bib186)] proposed a two-step
    optimization method. More specifically, they first trained MLPs to predict BLER
    under SCL decoding from input frozen bit sequences, and then the code that minimizes
    the BLER was constructed via \acpgd [[204](#bib.bib204)] which has been widely
    studied in the realm of adversarial attacks on neural networks. Simulations demonstrated
    that the proposed construction successfully improves the performance of the codes
    on the dataset used for predicting BLER.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the authors in [[187](#bib.bib187)] proposed a GNN-based polar
    code construction algorithm for CA-SCL decoding. More specifically, a polar code
    is first mapped onto a unique heterogeneous graph called the \acpccmp graph, and
    then a heterogeneous GNN-based iterative message-passing algorithm is proposed
    which aims to find a PCCMP graph corresponding to the polar code with minimum
    BLER under CA-SCL decoding. The proposed GNN-based iterative message-passing method
    has a salient property that a single trained model can be directly applied to
    constructions for different design SNRs and different block lengths without any
    additional training. Numerical experiments showed that the proposed constructions
    outperform classical constructions in [[197](#bib.bib197)] under CA-SCL decoding.
  prefs: []
  type: TYPE_NORMAL
- en: In [[188](#bib.bib188)], the authors proposed neural network-based adaptive
    polar coding scheme that adapts to various channel conditions and quality of service
    requirements. Specifically, the authors developed an MLP-based performance prediction
    framework for polar codes with dynamic frozen bits under SCL decoding. Then the
    authors presented a new class of polar codes with dynamic frozen bits parameterized
    by a single integer parameter, and used the performance prediction framework to
    optimize the parameter for a given target BLER, list size, code length and rate.
    The simulation results show that the proposed codes outperform 5G polar codes
    under CA-SCL decoding with various list sizes. Although a neural network is not
    used due to the training difficulty, the same authors also proposed an RL-based
    method to design dynamic frozen bits of polar codes that minimize the BLER under
    SCL decoding in [[205](#bib.bib205)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Nested Polar Code Construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, on-the-fly design of polar codes that adaptively select frozen bits
    for a given channel may be too complex to implement in practical systems. On the
    other hand, the authors in [[206](#bib.bib206), [207](#bib.bib207), [208](#bib.bib208),
    [209](#bib.bib209)] studied the design of polar codes based on a universal reliability
    order of bit channels that is independent of channel conditions. As such, it is
    preferable in practice to impose a nested property on polar codes with different
    rates, so that all polar codes can be derived from the same mother code based
    on the universal reliability sequence [[210](#bib.bib210)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in [[202](#bib.bib202), [185](#bib.bib185)] proposed constructing
    nested polar codes via \aca2c algorithms [[211](#bib.bib211)]. The paper regarded
    code construction as a multi-step MDP, where for a given $(N,K)$ polar code (current
    state), a new action is taken to construct $(N,K+1)$ polar code (an updated state).
    This MDP is illustrated in Fig. [8](#S3.F8 "Figure 8 ‣ III-B2 Nested Polar Code
    Construction ‣ III-B Polar Code Design ‣ III DL for Code Design ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). The reliability ordered sequence
    is then constructed by sequentially appending the actions to the end of initial
    polar code construction. The proposed code design was shown to outperform the
    conventional DE construction under SCL decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e7c6bed5cd64bf2354ee898241f2eda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: RL-based construction of a rate-$K/N$ nested polar code at time $t\in\{1,\ldots,K\}$
    in [[202](#bib.bib202), [185](#bib.bib185)]. The goal is to minimize the expected
    cumulative BLER, i.e., $\sum^{K}_{t=1}r_{k}$. The ones in the frozen bit vector
    $s_{t}$ indicate locations of frozen bits.'
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the authors in [[189](#bib.bib189)] first transformed the problem
    of nested polar code construction into a stochastic policy optimization problem
    for sequential decision, and then represented the policy by a customized neural
    network. Furthermore, the authors proposed a gradient-based algorithm to minimize
    the average loss of the policy. Simulation results demonstrate that the proposed
    construction achieves better performance than the state-of-the-art nested polar
    codes for SCL decoding in [[202](#bib.bib202), [185](#bib.bib185)]. A similar
    construction of nested polar codes has also been proposed in [[190](#bib.bib190)],
    where the authors parameterized the policy network by a Transformer encoder-only
    model, which can directly predict the next information bit in the nested sequence.
    It was shown that the proposed Transformer-based construction can achieve better
    error rate performance than the approach proposed in [[189](#bib.bib189)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Design of Polar Codes with Large Kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to improve finite-length performance of binary polar codes is to
    increase the size of the polarization kernel. In fact, channel polarization holds
    for all kernels provided that they are not unitary and not upper triangular under
    any column permutation [[212](#bib.bib212)]. However, binary polar codes with
    large kernels exhibit poor performance for practical short-to-medium block lengths
    and face an exponential increase in computational complexity with kernel size.
  prefs: []
  type: TYPE_NORMAL
- en: In [[191](#bib.bib191)], the authors proposed polar codes via large nonlinear
    neural network-based kernels, termed as DEEPPOLAR, and its decoder based on a
    generalization of SC decoding. They also developed a principled curriculum-based
    training methodology that allows DEEPPOLAR to generalize well to high SNR scenarios,
    characterized by rare error events. It was shown that the DEEPPOLAR outperforms
    the classical polar codes with SC decoding.
  prefs: []
  type: TYPE_NORMAL
- en: III-B4 Design of Polarization Adjusted Convolutional Codes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the Shannon Lecture at the 2019 \acisit, Arıkan introduced a new class of
    codes, called \acpac codes, that concatenate convolutional precoding with the
    polar transform [[213](#bib.bib213)]. PAC codes significantly improve the performance
    of polar codes at short-to-moderate block lengths, where channel polarization
    occurs relatively slowly.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in encoding of PAC codes is rate profiling. For PAC codes of
    rate-$K/N$, this step inserts $K$ information bits into a vector of length $N$,
    which is subsequently input to the convolutional precoder. The selection of $K$
    bit indices out of $N$ possible indices is called rate profile construction and
    its design significantly affects the performance of PAC codes.
  prefs: []
  type: TYPE_NORMAL
- en: In [[192](#bib.bib192)], the authors proposed an RL-based algorithm for rate-profile
    construction of PAC codes. Specifically, by mapping the rate profile construction
    problem to MDP, the authors proposed Q-learning with a set of customized reward
    and update strategies. Simulation results showed that the proposed rate-profile
    construction provides better error rate performance compared to the Monte-Carlo-based
    rate profiling design in [[214](#bib.bib214)].
  prefs: []
  type: TYPE_NORMAL
- en: IV DL for Channel Decoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL methods for channel decoding have been an active area of research and have
    been extensively studied as a means to replace or assist conventional decoding
    algorithms. In what follows, we first review model-free decoders that do not assume
    specific code structure and thus are applicable to any codes. We then review model-based
    DL BP decoding methods take take into account specific factor graphs. Subsequently,
    we focus on DL methods for decoding polar codes, convolutional/turbo codes, and
    cyclic codes.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Model-Free Decoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of model-free decoders.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Reference | Main Contributions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MLP Decoders | Gruber et al. [[215](#bib.bib215)] | Initial work on MLP-based
    channel decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Seo et al. [[216](#bib.bib216)] | Investigation on the impact of various
    configurations of an MLP decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Leung et al. [[217](#bib.bib217)] | Empirical study on the impact of hyperparameters
    of the MLP decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Leung et al. [[218](#bib.bib218)] | Investigated small MLP for applications
    with low energy and latency. |'
  prefs: []
  type: TYPE_TB
- en: '| Advanced DL Models | Lyu et al. [[219](#bib.bib219)] | Investigation on different
    types of DL decoders, namely, MLP, CNN, RNN. |'
  prefs: []
  type: TYPE_TB
- en: '| Sattiraju et al. [[220](#bib.bib220)] | Bi-GRU-based decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Zhu et al. [[221](#bib.bib221)] | Residual MLP, CNN, RNN-based decoders.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Choukroun et al. [[222](#bib.bib222)] | Novel Transformer architecture for
    decoding block codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Choukroun et al. [[223](#bib.bib223)] | DDPM for soft-decision decoding of
    linear codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Syndrome-Based Loss Function | Bennatan et al. [[224](#bib.bib224)] | Syndrome-based
    approach to soft-decision decoding of linear codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Kamassury et al. [[225](#bib.bib225)] | Iterative algorithm, referred to
    as iterative error decimation. |'
  prefs: []
  type: TYPE_TB
- en: '| Artemasov et al. [[226](#bib.bib226)] | SISO decoder based on Stacked-GRU
    for turbo product codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptability | Wang et al. [[227](#bib.bib227)] | Unified DL-based decoder
    for polar and LDPC codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[228](#bib.bib228)] | A meta learning-based model independent
    neural decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. [[229](#bib.bib229)] | Transfer learning for decoding a set of
    rate-compatible polar codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Artemasov et al. [[230](#bib.bib230)] | A unified DL decoder for BCH and
    polar codes concatenated with CRC. |'
  prefs: []
  type: TYPE_TB
- en: '| RL-Based Approach | Carpi et al. [[231](#bib.bib231)] | DQN for iterative
    bit-flipping decoding of binary linear codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Gao et al. [[232](#bib.bib232)] | Q-learning-based bit-flipping decoding
    for polar codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Complexity Reduction | Kavvousanos et al. [[233](#bib.bib233), [234](#bib.bib234),
    [235](#bib.bib235), [236](#bib.bib236)] | Magnitude-based pruning and quantization
    for parameter reduction. |'
  prefs: []
  type: TYPE_TB
- en: '| Cavarec et al. [[237](#bib.bib237)] | A DL-aided adaptation of the order
    parameter in OSD. |'
  prefs: []
  type: TYPE_TB
- en: '| Other Approaches | Raviv et al. [[238](#bib.bib238)] | Data-driven framework
    for permutation selection in permutation decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Kurmukova et al. [[239](#bib.bib239)] | Friendly jamming for improving decoding
    performance. |'
  prefs: []
  type: TYPE_TB
- en: '| Tsvieli et al. [[240](#bib.bib240)] | Investigation on the problem of maximizing
    the margin of the decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Zhong et al. [[241](#bib.bib241), [242](#bib.bib242)] | DL-based decoders
    for spin-torque transfer magnetic random access memory. |'
  prefs: []
  type: TYPE_TB
- en: 'A model-free decoder employs neural networks that do not take into account
    any specific structure of the codes and thus can potentially benefit from the
    powerful architectures of advanced DL models. However, such decoders typically
    suffer from the curse of dimensionality, since the size of the training dataset
    grows exponentially with the number of information bits. On the other hand, a
    potential advantage over conventional non-DL-based decoding is a highly parallelizable
    structure, allowing *one-shot* decoding instead of iterative decoding. The model-free
    approaches that we will discuss below is summarized in Table [III](#S4.T3 "TABLE
    III ‣ IV-A Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 MLP Decoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The paper [[215](#bib.bib215)] is one of the initial works on DL-based channel
    decoding where the authors investigated the direct application of MLP to decoding
    of random and polar codes. Their empirical results demonstrated that for structured
    codes, the DL decoder can generalize even for codewords not seen in the training
    phase, and the DL decoder can achieve \acmap decoding performance for a very small
    code lengths such as $16$ bits, but learning for longer codes is prohibitively
    complex due to the exponentially increasing training complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the authors in [[216](#bib.bib216)] investigated the impact of various
    configurations of an MLP decoder on BER performance, such as the number of hidden
    layers, the number of nodes for each layer, and activation functions. Similarly,
    the paper [[217](#bib.bib217)] empirically studied the impact of the number of
    hidden layers and nodes as well as a training SNR of the MLP decoder on its performance
    and investigated the minimum numbers required to achieve similar performance to
    the optimal maximum-likelihood decoder for short linear and nonlinear block codes.
  prefs: []
  type: TYPE_NORMAL
- en: In [[243](#bib.bib243), [218](#bib.bib218)], the authors investigated the application
    of small MLP decoders to low-energy and low-latency applications. In particular,
    the paper made comparisons between single-label and multi-label neural decoders,
    and demonstrated that the multi-label decoder generally requires more hidden layers
    and nodes to achieve similar performance to the single-label decoder.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Advanced DL Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The performance of the above-mentioned MLP decoder can be enhanced by advanced
    DL models. For instance, in [[219](#bib.bib219)], the authors investigated different
    types of decoders based on MLP, CNN, and RNN (in particular, LSTM). Their empirical
    results demonstrated that the RNN and CNN decoders can actually achieve better
    BER performance than the MLP decoder at the cost of higher computation time. In
    [[220](#bib.bib220)], RNN (\acbigru) was used for encoding/decoding of turbo codes
    as in [[244](#bib.bib244)].
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the performances, the authors of [[245](#bib.bib245), [221](#bib.bib221)]
    introduced the concept of residual learning [[139](#bib.bib139)] to the MLP, CNN,
    and RNN-based decoders. Specifically, the paper introduced a denoiser network
    prior to the decoder that simply aims to remove noise induced at the channel,
    and proposed a training loss that considers both denoising and decoding performance.
    It was demonstrated that the proposed denoiser network can improve the BER performance
    at the cost of marginal increase in run time.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, inspired by the success of the Transformer model in various applications
    [[159](#bib.bib159)], a novel Transformer architecture for decoding algebraic
    block codes, termed \acecct was proposed in [[222](#bib.bib222)]. The ECCT takes
    as input a concatenation of reliabilities of codeword bits (absolute values of
    received symbols in the case of BI-AWGN) and syndrome bits as its input where
    each element of which is embedded in a high-dimensional space with its own position-dependent
    embedding vector. Then, a self-attention mechanism is employed, where the interaction
    between bits specified by the code structure, i.e., \acpcm, is incorporated as
    domain knowledge. Extensive simulation results demonstrated that the proposed
    Transformer-based decoder outperforms state-of-the-art neural decoders.
  prefs: []
  type: TYPE_NORMAL
- en: Although the ECCT in [[222](#bib.bib222)] employs a mask matrix that is derived
    from the PCM, there exist numerous PCMs for the same code which will lead to different
    decoding performances. Motivated by this fact, the authors in [[246](#bib.bib246)]
    addressed the problem of identifying the optimal PCM. In particular, the authors
    proposed a systematic mask matrix constructed from the systematic PCM which results
    in sparse self-attention map, and proposed a novel Transformer architecture called
    a double-masked ECCT that consists of two parallel masked self-attention blocks
    employing distinct mask matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, in [[223](#bib.bib223)], the authors employed DDPM [[173](#bib.bib173)]
    for soft decoding of linear codes with arbitrary block lengths. Their framework
    models the transmission over the AWGN channel as a series of diffusion steps that
    can be iteratively reversed. The paper also proposed to condition the diffusion
    decoder on the number of parity check errors and to employ a line-search procedure
    to control the reverse diffusion step size.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in [[247](#bib.bib247)], the authors proposed a foundation model
    [[248](#bib.bib248)] for channel codes by extending the Transformer architecture.
    A foundation model refers to a model that is initially trained on a wide range
    of data, generally based on self-supervision, and then adapted (e.g., transferred
    or fine-tuned) to a wide range of downstream tasks. Thus, the proposed framework
    provided a universal decoder that is capable of adapting and generalizing to any
    (unseen) code of any length.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 Syndrome-Based Loss Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Syndrome decoding is a well-known approach for decoding algebraic codes. Several
    approaches have been proposed for training DL-based syndrome decoders that estimate
    the transmitted codeword from the syndrome. Syndrome-based training does not rely
    on the knowledge of the transmitted codeword, and is thus promising for online
    adaptation to changing channel conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper [[224](#bib.bib224)] is one of the early works on syndrome-based
    DL decoding, which proposed to use the absolute values of the received symbols,
    i.e., reliabilities in the case of the BI-AWGN channel, and the syndrome of its
    hard decisions for decoding, instead of directly using the received symbols as
    the input to the decoder. The proposed decoder is illustrated in Fig. [9](#S4.F9
    "Figure 9 ‣ IV-A3 Syndrome-Based Loss Function ‣ IV-A Model-Free Decoders ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"). Furthermore, the authors introduced permutations from the code’s automorphism
    group [[249](#bib.bib249), [250](#bib.bib250)] as a preprocessing. Permutations
    in this group have the property that the permuted version of any codeword is guaranteed
    to be also a valid codeword, i.e., the permuted input of the decoder is a noisy
    valid codeword. Simulations demonstrated that the proposed framework can achieve
    near maximum-likelihood performance for short \acbch codes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e11b0b32874817fe2a08be82c83cfbb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The syndrome-based DL decoder proposed in [[224](#bib.bib224)]. The
    channel input sequence $\mathbf{x}$ consists of \acbpsk symbols, and $\mathbf{y}$
    is the output of a \acbiso channel.'
  prefs: []
  type: TYPE_NORMAL
- en: The syndrome-based approach, which attempts to predict the error vector from
    its syndrome alone, can suffer from the potential presence of inconsistent training
    examples, called *disturbance* [[251](#bib.bib251)], i.e., training examples with
    the same syndromes but with different error vectors. To solve this problem, the
    authors in [[225](#bib.bib225)] proposed an iterative algorithm, referred to as
    iterative error decimation, which is robust against the superposition of error
    patterns. In each iteration, the DL decoder estimates the error vector and then
    decimates (subtracts) it from the received vector. The simulation results demonstrated
    that the proposed scheme improves the performance of the scheme in [[224](#bib.bib224)].
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the authors in [[226](#bib.bib226)] proposed a syndrome-based approach
    to \acsiso decoding of BCH component codes in turbo product codes [[252](#bib.bib252)]
    based on Stacked-GRU, which is an RNN architecture composed of GRU. They introduced
    a regularization term into a loss function and demonstrated that the proposed
    DL decoder outperforms the original chase decoder in [[252](#bib.bib252)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-A4 Adaptability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adaptive coding is a technique for adapting a code rate to a channel condition
    in wireless channels. For a DL-based decoder, supporting multiple code rates not
    only requires multiple training, which is computationally intensive, but also
    requires a large amount of memory to store the learned DL parameters.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, a unified DL-based decoder for polar and LDPC codes was
    proposed in [[227](#bib.bib227)], which supports different codes, i.e., polar
    and LDPC codes, with a single DNN by utilizing a code indicator at the decoder
    input. The simulation results demonstrated the potential of the unified decoder
    for very short code lengths, e.g., $16$ bits. A similar unified DL decoder using
    the code indicator was also proposed in [[230](#bib.bib230)] for BCH and CRC-concatenated
    polar codes. Their results demonstrated that, for a code length of $64$, the proposed
    unified decoding scheme with code indicator achieves a small performance gap of
    less than $0.2$ dB from the decoder trained solely for the single code.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the paper [[228](#bib.bib228)] introduced the meta-learning-based
    neural decoder, termed as \acmind, which can adapt to a new channel with a small
    number of pilots and few gradient descent steps. Specifically, the proposed approach
    consists of meta-learning and meta-training steps, where the model learns good
    initial parameters in the meta-learning step, and then adapts the parameters to
    the observed channel in the meta-testing phase using minimal adaptation data and
    pilots. It has been demonstrated that the proposed scheme can adapt to a channel
    while achieving a performance close to that of a DL decoder designed solely for
    the particular channel. In the same line of research, the authors in [[229](#bib.bib229)]
    proposed transfer learning to efficiently train decoders for a set of rate-compatible
    polar codes that are expurgated from the same mother code as in 5G NR.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A5 RL-Based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the majority of previous works on DL-based channel decoding are based
    on supervised learning, several RL-based approaches have been proposed for the
    cases where supervisory data (ground truth) is unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [[231](#bib.bib231)] is one of the earliest works on RL-based channel
    decoding. Unlike these studies, the authors in [[231](#bib.bib231)] proposed an
    RL framework for iterative \acbf decoding of binary linear codes. Specifically,
    they linked the BF decision at each step to MDP and applied RL to find good decision
    strategies. The authors also exploited the permutation automorphism group to improve
    the performance. The extensive simulations showed that the learned BF decoders
    with DQN can achieve near-optimal performance for short, high-rate codes.
  prefs: []
  type: TYPE_NORMAL
- en: Later, the authors in [[232](#bib.bib232)] applied RL-based BF decoding to polar
    codes. In contrast to the DQN proposed in [[231](#bib.bib231)], they used simple
    Q-learning and attempted to map channel observations directly onto estimated codewords.
    Simulation results showed that the proposed Q-learning achieves comparable performance
    to the learned BF decoding in [[231](#bib.bib231)] with lower complexity.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A6 Complexity Reduction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, decoding longer codes requires a larger neural network size. Such
    a network not only requires a huge amount of computational resources in the training
    phase, but also imposes high computational and space complexities in the inference
    phase. In particular, the complexity in the inference phase is of practical importance
    since the training is usually performed offline.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[233](#bib.bib233)] attempted to reduce the parameter size of
    a DL decoder, by introducing various simplified neural network structures with
    fewer parameters. In [[234](#bib.bib234)], the same authors further extended their
    work by introducing the magnitude-based pruning [[253](#bib.bib253)] and quantization
    of the parameters. The proposed decoder was demonstrated to achieve similar performance
    to the original system in [[224](#bib.bib224)] even with $80$% reduction of the
    network parameters and $8$-bit fixed-point representation. Furthermore, FPGA implementation
    of the proposed decoder has been presented in [[235](#bib.bib235), [236](#bib.bib236)].
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, the authors in [[237](#bib.bib237)] considered DL-aided complexity
    reduction of \acosd [[254](#bib.bib254)], which is a soft-decision decoding algorithm
    of linear block codes that approaches the optimal maximum-likelihood decoding
    performance especially for short codes. Although increasing the order parameter
    of OSD leads to near-maximum-likelihood performance, it may waste computational
    resources when the received signal can be decoded with lower order. The paper
    [[237](#bib.bib237)] proposed a learning-based approach to adapt the required
    order parameter to the channel condition and demonstrated the effectiveness of
    the proposed scheme in terms of the performance-complexity trade-off through numerical
    simulations.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A7 Other Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is known that multiple decoding attempts over different permutations of received
    codewords provide a performance gain [[250](#bib.bib250), [255](#bib.bib255)].
    However, it remains unclear how to choose the permutation that yields the best
    performance. To address this, the authors in [[238](#bib.bib238)] presented a
    DL approach to selecting candidates from the code’s automorphism group in permutation
    decoding. In this scheme, a trained network predicts the probability of successful
    decoding for each permutation, and decoding is performed only for permuted codewords
    with the highest probabilities. The proposed algorithm has been demonstrated by
    simulations to achieve remarkable performance gains over a random selection of
    permutations from the automorphism group.
  prefs: []
  type: TYPE_NORMAL
- en: In [[239](#bib.bib239)], the authors proposed a novel approach referred to as
    *friendly attack* for improving channel decoding performance, inspired by the
    concept of adversarial attacks. The proposed scheme introduces small perturbations
    into the modulated symbols before transmission. The perturbations are designed
    by a modified iterative fast gradient method [[256](#bib.bib256)] such that a
    loss function between the decoded codeword and the transmitted codeword is minimized.
    The performance improvement by the proposed scheme has been demonstrated for various
    codes and decoders.
  prefs: []
  type: TYPE_NORMAL
- en: Although the use of DL for channel decoding has been experimentally validated,
    the theoretical justification for the developed algorithm in terms of, e.g. the
    generalization properties, remains challenging. The authors in [[240](#bib.bib240)]
    addressed the problem of maximizing the margin of the decoder for an additive
    noise channel whose noise distribution is unknown, as well as for a nonlinear
    channel with AWGN. They formulated a maximum margin optimization problem, which
    is common in \acpsvm, for the decoder learning problem, and they relaxed it to
    a \acrlm problem by several approximation steps. The paper then provided expected
    generalization error bounds for both models, under optimal choice of the regularization
    parameter. The paper also presented a theoretical guidance for choosing the training
    SNR based on the bound for the additive noise channel.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B DL-Aided BP Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of DL-aided BP decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Reference | Main Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Neural MS Decoding and Its Variants | Nachmani et al. [[257](#bib.bib257),
    [258](#bib.bib258)] | DL-aided BP, NMS, and OMS decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Lugosch et al. [[259](#bib.bib259)] | NOMS decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[260](#bib.bib260)] | Neural network-aided OMS and NOMS decoding.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. [[261](#bib.bib261)] | Neural AMS decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Hsu et al. [[262](#bib.bib262)] | Neural network-aided VWMS decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[263](#bib.bib263)] | Neural MS decoding with linear approximation
    for PB-LDPC codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Kim et al. [[264](#bib.bib264)] | Neural SCMS decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Performance Enhancement | Teng et al. [[265](#bib.bib265), [266](#bib.bib266),
    [267](#bib.bib267)] | CNN-based learned BF for BP. |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et al. [[268](#bib.bib268)] | LSTM-based learned BF for BP. |'
  prefs: []
  type: TYPE_TB
- en: '| Variants of Random Redundant Decoding (RRD) | Nachmani et al. [[258](#bib.bib258)]
    | mRRD decoding with RNN-based BP decoders. |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[269](#bib.bib269)] | Node-classified redundant decoding algorithm.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Optimization-Based Decoding | Wei et al. [[270](#bib.bib270)] | Trainable
    ADMM-penalized decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Wadayama et al. [[271](#bib.bib271)] | Trainable PGD decoder for LDPC codes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wadayama et al. [[272](#bib.bib272)] | Proximal decoding for LDPC codes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| RL-Based Approach | Doan et al. [[273](#bib.bib273)] | RL-based selection
    of permutations on factor-graph. |'
  prefs: []
  type: TYPE_TB
- en: '| Habib et al. [[274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)]
    | RL-based scheduling optimization for sequential BP decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Customized Loss Function | Lugosch et al. [[277](#bib.bib277)] | Soft syndrome
    as a loss function for training a neural BP decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Teng et al. [[278](#bib.bib278)] | New syndrome losses for syndrome-based
    DL decoding of polar codes. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Nachmani et al. [[279](#bib.bib279)] | New loss function based on sparse
    node and knowledge distillation losses. |'
  prefs: []
  type: TYPE_TB
- en: '| Memory/Complexity Reduction | Teng et al. [[280](#bib.bib280)] | Weight quantization
    mechanism for an RNN polar decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Ibrahim et al. [[281](#bib.bib281)] | Quantization of an RNN polar decoder.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Xiao et al. [[282](#bib.bib282), [283](#bib.bib283)] | Finite alphabet iterative
    decoders for LDPC codes via quantized RNN. |'
  prefs: []
  type: TYPE_TB
- en: '| Lyu et al. [[284](#bib.bib284)] | A joint optimization of message quantization
    and quantization thresholds. |'
  prefs: []
  type: TYPE_TB
- en: '| Lian et al. [[285](#bib.bib285), [286](#bib.bib286)] | Weight-sharing across
    edges based on scalar parameters. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[287](#bib.bib287), [288](#bib.bib288)] | A parameter sharing
    scheme within the same layer for a neural NMS decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. [[289](#bib.bib289)] | A weight-sharing scheme for a neural MS
    decoder of protograph LDPC codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[290](#bib.bib290), [291](#bib.bib291)] | Tensor-train and
    tensor-ring decompositions for parameter size reduction. |'
  prefs: []
  type: TYPE_TB
- en: '| Cheng et al. [[292](#bib.bib292)] | A weight-sharing scheme for adapting
    to multiple code rates. |'
  prefs: []
  type: TYPE_TB
- en: '| Buchberger et al. [[293](#bib.bib293)] | A novel pruning-based neural BP
    decoder for short linear block codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Buchberger et al. [[294](#bib.bib294)] | A neural BP with decimation. |'
  prefs: []
  type: TYPE_TB
- en: '| GNN Decoders | Satorras et al. [[295](#bib.bib295)] | A hybrid inference
    model that combines BP and GNN. |'
  prefs: []
  type: TYPE_TB
- en: '| Cammerer et al. [[296](#bib.bib296)] | A fully GNN-based decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Tian et al. [[297](#bib.bib297)] | An edge-weighted GNN decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Understanding Neural BP Decoders | Ankireddy et al. [[298](#bib.bib298)]
    | Empirical study on how the learned weights attenuate the effect of these cycles.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adiga et al. [[299](#bib.bib299)] | Theoretical study on the generalization
    capabilities of neural BP decoders. |'
  prefs: []
  type: TYPE_TB
- en: '| Other Approaches | Clausius et al. [[300](#bib.bib300)] | GNN-based joint
    equalization and decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Wiesmayr et al. [[301](#bib.bib301)] | Deep-unfolded interleaved detection
    and decoding for MIMO wireless systems. |'
  prefs: []
  type: TYPE_TB
- en: '| Lee et al. [[302](#bib.bib302)] | Learning-aided multi-round BP decoding
    with impulsive perturbation. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[303](#bib.bib303)] | DL detection of decodable codewords for
    reducing the decoding delay. |'
  prefs: []
  type: TYPE_TB
- en: 'BP decoding is an efficient iterative decoding algorithm that is commonly used
    for decoding LDPC codes. BP decoding is performed on Tanner graph consisting of
    CNs and VNs, which correspond to codeword bits and parity check equations, respectively.
    An example of PCM with the corresponding Tanner graph representation of a (7,
    4) Hamming code is shown in Fig. [10(a)](#S4.F10.sf1 "In Figure 10 ‣ IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey"). In BP decoding, decoding messages are iteratively
    updated at CNs and VNs based on the Bayes’ rule. In practice, the min-sum approximation
    [[304](#bib.bib304)] is applied to the CN updates to reduce complexity, and this
    decoding is referred to as \acms decoding. The performance loss due to the min-sum
    approximation can be compensated for by \acnms and \acoms decoders [[305](#bib.bib305)]
    at the cost of slightly increased complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[257](#bib.bib257)], the authors proposed a DL-based implementation of
    BP decoding by treating BP decoding as a differentiable process, where the decoding
    messages are passed through unrolled iterations in a feed-forward fashion. Additionally,
    trainable weights were introduced at the edges, which are then optimized via SGD.
    An example of the unrolled BP trellis for a (7, 4) Hamming code is illustrated
    in Fig. [10(b)](#S4.F10.sf2 "In Figure 10 ‣ IV-B DL-Aided BP Decoding ‣ IV DL
    for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding: A
    Survey"). For code length $N$, the number of edges $E$, and the number of iterations
    $L$, the unfolded trellis has $N$ neurons at the input and output layers, and
    $E$ neurons at the $2L$ hidden layers. The network architecture is a non-fully
    connected neural network. As we review below, the trainable BP decoder over the
    unfolded trellis has been extensively studied in the literature. We summarize
    these works in Table [IV](#S4.T4 "TABLE IV ‣ IV-B DL-Aided BP Decoding ‣ IV DL
    for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding: A
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b6ae1346392806b5d36c10c9672fa43.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PCM and corresponding Tanner graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e10b6fe688b17e989b1d1f2a3cb5e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Unrolled BP trellis (two iterations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: An example of deep unfolded BP decoder for (7, 4) Hamming codes.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the idea of unfolding an iterative algorithm into a structure analogous
    to a neural network, i.e., deep unfolding [[306](#bib.bib306)], is a common model-based
    DL approach [[307](#bib.bib307)] often considered in the design of communication
    systems. Besides channel decoding, the idea of deep unfolding has been successfully
    applied, for example, to MIMO signal detection and channel estimation [[62](#bib.bib62),
    [45](#bib.bib45), [308](#bib.bib308)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 Neural MS Decoders and Its Variants
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned above, the paper [[257](#bib.bib257)] was the initial work that
    applied a feedforward network to BP decoding, where trainable weights are assigned
    to the edges of the factor graph, which are then optimized via SGD over the unrolled
    iterative BP decoding. The proposed parameterized decoder can compensate for the
    effect of small cycles in the Tanner graph of the code by properly scaling the
    weights. The effectiveness of the proposed decoder has been demonstrated for short
    BCH codes, for which standard BP decoding does not work well due to many short
    cycles in the graph. Later, in [[309](#bib.bib309)], the same authors extended
    the work by introducing an RNN architecture and showed that this architecture
    leads to comparable performance to the feedforward architecture in [[257](#bib.bib257)]
    even with fewer parameters. In [[258](#bib.bib258)], the authors also proposed
    neural network-based NMS and OMS decoders for reducing the complexity of the BP
    decoder, which are the generalized versions of the standard NMS and OMS decoders
    [[305](#bib.bib305)]. Neural network-based OMS decoding has also been studied
    independently in [[259](#bib.bib259)]. Another approach to enhance the performance
    of MS decoding while preserving the low complexity property was considered in
    [[263](#bib.bib263)]; the authors proposed \aclams for \acpbldpc codes, where
    the magnitudes of the check node updating and channel output are linearly adjusted
    by a small and shallow neural network. In contrast to the above-mentioned studies
    that considered the flooding schedule, the authors in [[310](#bib.bib310)] proposed
    a neural network-aided \acnoms decoding for the layered BP with application to
    5G LDPC codes.
  prefs: []
  type: TYPE_NORMAL
- en: As another variant of MS decoding, the \acams decoding proposed by Qualcomm
    [[311](#bib.bib311)] has drawn attention, where it employs \acplut to simplify
    nonlinear CN processing. In [[261](#bib.bib261)], the authors introduced a neural
    network based selection mechanism to AMS decoding that automatically selects the
    check node updating rule from either the MS rule or the BP rule and demonstrated
    that the proposed decoder outperforms the conventional neural NMS decoder. The
    \acsmms algorithm [[312](#bib.bib312)] is another variant of MS decoding that
    simplifies the CN update in the MS algorithm. Specifically, in SMMS decoding,
    only one minimum magnitude is calculated at each CN over all the CN inputs and
    a correction is applied to outgoing messages if required. The SMMS decoder can
    be further improved by the \acvwms algorithm [[313](#bib.bib313)], which introduces
    variable correction factors into the CN update that depend on the number of iterations.
    To efficiently learn the optimal correction factors in the VWMS algorithm, the
    authors in [[262](#bib.bib262)] proposed a neural network-aided approach, instead
    of an exhaustive search in the original work [[313](#bib.bib313)]. The effectiveness
    of the proposed scheme in terms of throughput was demonstrated experimentally
    using the 40 nm CMOS TSMC process. Unlike the above-mentioned methods, which simplify
    CN updates, \acscms decoding [[314](#bib.bib314)] modifies the VN processing by
    deleting unreliable messages. More specifically, in SCMS decoding, any variable
    node message that changes sign between two consecutive iterations is discarded,
    i.e., set to zero. The authors in [[264](#bib.bib264)] introduced trainable normalization
    and offset weights to the SCMS decoder, which are trained by DL techniques. It
    was demonstrated that the error rate performance of the proposed neural SCMS decoder
    is close to that of the BP decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Although the above-mentioned works applied neural BP decoding to LDPC codes,
    it has also been used to decode polar codes. In [[315](#bib.bib315)], similar
    to [[257](#bib.bib257)], a neural MS decoder was applied to factor graphs of polar
    codes. The proposed decoder was demonstrated by simulations to outperform conventional
    BP decoding with the same number of iterations. Also, the authors presented an
    efficient hardware implementation of the basic computation block of the proposed
    decoder. In [[316](#bib.bib316)], a similar trainable BP decoding was applied
    to sparse graphs of polar codes [[317](#bib.bib317)], which was shown to achieve
    comparable performance to BP decoding even with a single trainable parameter.
    Furthermore, the authors in [[260](#bib.bib260)] proposed an NOMS decoder for
    polar codes that introduces both normalization (or scaling) factors and offsets,
    and demonstrated that the proposed decoder achieves better performance than the
    state-of-the-art schemes, including the decoder proposed in [[315](#bib.bib315)].
  prefs: []
  type: TYPE_NORMAL
- en: In order to enhance the performance of a standalone polar code and close the
    performance gap from CA-SCL decoding with lower latency, the authors in [[318](#bib.bib318)]
    proposed neural BP decoding for polar codes concatenated with a CRC code by exploiting
    the concatenated factor graph of the polar code and CRC, while the conventional
    BP decoding for concatenated CRC-polar codes is applied only to the factor graph
    of polar codes and the CRC is used only to verify the result of BP at each iteration.
    Furthermore, the authors in[[319](#bib.bib319)] considered concatenated polar
    and LDPC codes and proposed two-dimensional OMS decoding. They optimized the trainable
    parameters of the decoder by back propagation over the unfolded BP trellis and
    showed that the performance of the proposed decoder is comparable to the exact
    BP decoder.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Performance Enhancement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To enhance BP decoding of polar codes, the authors in [[265](#bib.bib265), [266](#bib.bib266),
    [267](#bib.bib267)] proposed to combine BP decoding with a CNN-assisted bit flipping
    mechanism, which performs the flipping bit selection in the BP-BF decoder [[320](#bib.bib320)]
    based on a CNN trained using the metadata of the BP decoding process. The authors
    demonstrated that the proposed scheme can achieve a lower BLER than SCL decoding.
    Meanwhile, in order to reduce the computational complexity of the CNN-based approach,
    the paper [[268](#bib.bib268)] proposed an LSTM network that predicts error-prone
    bits to be flipped based on the magnitude of \acllr after the original BP decoding.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[321](#bib.bib321)] introduced a hypernetwork [[322](#bib.bib322)]
    that generates weights of a neural BP decoder to make the decoder more adaptive
    by letting the weights be a function of the node’s input. The same authors also
    introduced hypernetworks for decoding short polar codes [[323](#bib.bib323)] and
    showed that the proposed decoder achieves similar BER as SCL decoding in the high
    SNR region. Furthermore, they proposed an autoregressive BP decoder that incorporates
    the estimated SNR and multiple autoregressive signals obtained from the intermediate
    output of the network [[324](#bib.bib324)].
  prefs: []
  type: TYPE_NORMAL
- en: Training data preparation is an essential part of training DNN-based decoders.
    In particular, the choice of training SNR plays an important role in training
    a DL-based channel decoder for generalization. A common approach is to train the
    decoder over varying SNR ranges [[257](#bib.bib257)]. Besides, the optimal choice
    of the training SNR has been studied either empirically [[244](#bib.bib244), [215](#bib.bib215)]
    or analytically [[325](#bib.bib325)]. To address the problem of choosing the optimal
    training SNR, the authors in [[326](#bib.bib326)] proposed *active deep decoding*,
    inspired by active learning [[327](#bib.bib327)]. Specifically, based on the observation
    that no optimal training SNR for all validation sets exists, the paper proposed
    to adaptively sample training data instead of passively generating examples during
    training. It was demonstrated that this active deep decoding scheme offers performance
    gain by effectively sampling the training data without increasing the inference
    (decoding) complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In [[328](#bib.bib328)], inspired by ensemble models that are widely used to
    solve complex tasks by decomposing them into multiple simpler tasks each of which
    is solved locally by a single expert member of the ensemble [[329](#bib.bib329)],
    the authors introduced the ensemble of neural BP decoders. The proposed scheme
    consists of a single classical \achdd and multiple trainable BP decoders, where
    the classical HDD is employed to assign a received codeword to a single expert
    BP decoder based on the number of the estimated codeword errors. It was demonstrated
    that this scheme achieves remarkable performance gains over the single neural
    BP. Furthermore, the data-driven ensemble scheme has been extended to BP polar
    decoders in [[330](#bib.bib330), [331](#bib.bib331)].
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, many practical LDPC codes exhibit an error floor⁵⁵5For modern iteratively
    decodable codes, such as LDPC codes and turbo codes, there is an SNR point after
    which the error rate decreases only slowly. This phenomenon is called *error floor*..
    For applications such as ultra-reliable and low-latency communications that require
    extremely low BLER, it can be critical to mitigate the error floor. Since the
    error floor of LDPC codes is commonly attributed to the suboptimality of the iterative
    message passing decoding algorithms for factor graphs with cycles, the paper [[332](#bib.bib332)]
    proposed training methods for neural NMS decoders to eliminate the error floor
    of LDPC codes. Specifically, inspired by the boosting learning technique [[333](#bib.bib333)],
    the authors divided the decoder into two cascaded neural decoders and trained
    the first decoder to improve the waterfall performance, while the second decoder
    was trained to handle the residual errors that are not corrected by the first
    decoder.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 Variants of Random Redundant Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The \acrrd algorithm [[334](#bib.bib334)] and \acmbbp decoder [[335](#bib.bib335)]
    are other approaches to soft-decision decoding of short block codes based on a
    redundant PCM. \acmrrd [[250](#bib.bib250)] is an algorithm that attempts to benefit
    from both RRD and MBBP decoding, which make use of a permutation group (automorphism
    group) of codes and parallel iterative decoders, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In [[269](#bib.bib269)], the authors proposed a \acncrd algorithm for \achdpc
    codes in order to improve the performance of RRD decoding. The NC-RD algorithm
    introduces two preprocessing steps to the RRD decoding. More specifically, the
    algorithm first classifies the variable nodes of the parity-check matrix by the
    $k$-median algorithm based on the number of shortest cycles associated with each
    variable node, and then generates a list of permutations of bit positions from
    the automorphism group based on the permutation reliability metrics. The authors
    further proposed the neural network-based NC-RD algorithm by unfolding the NC-RD
    decoding process and introducing trainable weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[258](#bib.bib258)], the authors applied the concept of DL-based BP decoding
    to mRRD [[250](#bib.bib250)] by replacing the BP decoding blocks in the mRRD algorithm
    with the proposed RNN-based BP decoders. The resulting decoder structure is shown
    in Fig. [11](#S4.F11 "Figure 11 ‣ IV-B3 Variants of Random Redundant Decoding
    ‣ IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in
    Deep Learning for Channel Coding: A Survey"). The proposed RNN-based mRRD decoder
    has been demonstrated to achieve near maximum-likelihood performance with reasonable
    computational complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/63b792ed288eb993bd21ec36a278556e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The mRRD algorithm with RNN-based BP decoders in [[258](#bib.bib258)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B4 Optimization-Based Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optimization-based decoding is another research direction aimed at improving
    the performance of BP decoders. The origin of the optimization-based decoding
    goes back to the work by Feldman who introduced a \aclp formulation of decoding
    LDPC codes [[336](#bib.bib336)].
  prefs: []
  type: TYPE_NORMAL
- en: The LP decoder is based on the LP relaxation of the original maximum-likelihood
    decoding problem [[337](#bib.bib337)]. However, the LP decoder has higher computational
    complexity and worse error-correcting performance in the low SNR region compared
    with the BP decoder. In order to address the above drawbacks, the paper [[270](#bib.bib270)]
    proposed a trainable \acadmm-penalized decoder [[338](#bib.bib338)] by unfolding
    the ADMM iterations. It was demonstrated that the proposed decoder can outperform
    the conventional BP decoder in high SNR region with comparable execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Afterwords, the authors in [[271](#bib.bib271)] introduced a trainable projected
    gradient decoder for LDPC codes by unfolding the PGD algorithm and optimizing
    the parameters via backpropagation. The proposed decoder alternately performs
    the gradient and projection steps, where the former moves in the direction of
    the negative gradient of the objective function, while the latter maps the search
    point into a feasible region that nearly satisfies the optimization constraint.
    Also, in [[272](#bib.bib272)], the same authors proposed proximal decoding of
    LDPC codes based on the proximal gradient method [[339](#bib.bib339)], which is
    used for solving non-differentiable convex optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B5 RL-Based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is known that parallel BP decoders on independently permuted factor graphs
    can significantly improve the performance of single BP decoding for polar codes
    [[340](#bib.bib340), [341](#bib.bib341)]. In [[273](#bib.bib273)], the authors
    addressed the problem of selecting the permutations on the factor graph that lead
    to successful decoding given a channel observation. Specifically, they viewed
    the selection of permutations as a multi-armed bandit problem and proposed an
    RL-based CRC-aided BP decoder that attempts to select the best set of permutations.
    The proposed scheme was shown to achieve better performance than other approaches
    such as cyclically-shifted and random factor-graph permutations [[342](#bib.bib342),
    [340](#bib.bib340)].
  prefs: []
  type: TYPE_NORMAL
- en: In [[274](#bib.bib274), [275](#bib.bib275), [276](#bib.bib276)], the authors
    proposed a novel RL-based sequential BP decoding scheme to optimize the scheduling
    of CN clusters for moderate length LDPC codes⁶⁶6In contrast to the standard flooding
    scheduling where all CNs and VNs are updated simultaneously at each iteration,
    sequential BP decoding, or layered decoding, updates nodes or sets of nodes individually
    in sequence.. In the proposed scheme, $m$ CNs are divided into sets of $z$ CNs,
    called *cluster*, and the scheduling problem, i.e., cluster selection with $\lceil
    m/z\rceil$ possible actions, was optimized by Q-learning. Furthermore, they proposed
    novel meta-learning based sequential decoding schemes to quickly adapt to changing
    channel conditions due to fading in wireless scenarios. The RL-based scheduling
    of sequential BP decoding has also been proposed for generalized LDPC codes [[343](#bib.bib343)],
    where the authors showed that the proposed RL-based decoding scheme was shown
    to significantly outperform the standard BP flooding decoder, as well as a sequential
    decoder based on random scheduling with the smaller number of CN updates.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B6 Customized Loss Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As in Section [IV-A3](#S4.SS1.SSS3 "IV-A3 Syndrome-Based Loss Function ‣ IV-A
    Model-Free Decoders ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), a syndrome loss function has been used for DL-based
    BP decoders. In [[277](#bib.bib277)], the authors proposed a soft syndrome as
    a loss function for training a neural BP decoder. Unlike the paper [[224](#bib.bib224)],
    which utilizes the hard syndrome as an input to the decoder, this paper introduced
    the *soft* syndrome which is defined similarly to the CN update rule in MS decoding,
    in addition to the conventional cross entropy loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the application of [[277](#bib.bib277)] was limited to decoders that
    output a soft estimate of the codeword, this is not the case for polar decoders
    that do not use a PCM. To address this, the authors in [[278](#bib.bib278)] proposed
    two modified syndrome losses: frozen-bit syndrome loss and CRC-enabled syndrome
    loss. The authors also introduced a syndrome-enabled blind equalizer based on
    the proposed syndrome loss, which does not require the transmission of training
    sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the above syndrome-based approaches, the authors in [[279](#bib.bib279)]
    considered a linear combination of sparse node loss and knowledge distillation
    loss, in addition to the conventional cross entropy loss. Knowledge distillation
    is a technique in DL where one uses a teacher network to guide the training of
    a smaller student neural network [[344](#bib.bib344)]. Sparse node loss imposes
    a sparse constraint on the node activations based on the $L_{p}$ norm, whereas
    the knowledge distillation loss aims to mimic the teacher network, which was the
    standard MS decoder without trainable parameters, by transferring knowledge. It
    was shown that the proposed loss terms provide BER performance improvement of
    up to $1.1$ dB without increasing the runtime complexity and the model size.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B7 Memory and Complexity Reduction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A neural network-based BP decoder introduces different weights (or scaling
    factors) to different edges in the Tanner graph, which can significantly increase
    the computational and space complexities of standard BP decoding. In fact, this
    issue has been studied extensively for generic DNNs, i.e., not limited to channel
    decoding applications, due to the large parameter sizes of modern DL models [[345](#bib.bib345)].
    A popular approach is neural network pruning [[346](#bib.bib346), [347](#bib.bib347),
    [348](#bib.bib348), [349](#bib.bib349), [350](#bib.bib350)], which aims to remove
    redundant parameters of an original network while preserving the accuracy. Parameter
    shaping is also an effective way to reduce parameters by sharing parameters between
    different neurons. Parameter shaping is typically exploited in CNN, where all
    neurons in a particular feature map share the same weight. Another popular approach
    is parameter quantization [[351](#bib.bib351), [352](#bib.bib352)]. These major
    approaches to addressing the complexity problem are illustrated in Fig. [12](#S4.F12
    "Figure 12 ‣ IV-B7 Memory and Complexity Reduction ‣ IV-B DL-Aided BP Decoding
    ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b3cf195eaf3873ca3463d9e26b1639cb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Quantization of weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0605e3ecd7c109cb2daddda44a541bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sharing weights (connections with the same color have the same weight).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78c3c466ac97386661c65cb03bc4e1dc.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Pruning neurons (pruned neurons are indicated by dashed circles).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Approaches to reducing computational and space complexities of a
    DL decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Quantization: In [[280](#bib.bib280)], the authors proposed a weight quantization
    mechanism for an RNN polar decoder. Specifically, they proposed a two-step approach,
    where floating-point weights are quantized into $2^{q}$ quantization levels and
    then they are further compressed into $2^{c}$ ($c<q$, $c,q\in\mathbb{N}$) quantization
    levels, which are the most commonly used among $2^{q}$ quantization levels. The
    quantization of the RNN polar decoder was also studied in [[281](#bib.bib281)],
    where the authors demonstrated that quantization after training leads to better
    performance compared to the case where quantization is applied after every epoch
    during training.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of quantizing DNN decoder parameters, several papers have investigated
    quantizing decoding messages or LLR values computed from channel observations.
    For example, in [[353](#bib.bib353)], the authors trained a parameterized quantization
    of LLR values that maximizes the performance of BP decoding. Similarly, the authors
    in [[354](#bib.bib354)] investigated the design of quantizers in an LDPC decoder
    that are used for quantizing both LLRs and iterative decoding messages. On the
    other hand, the authors in [[355](#bib.bib355)] proposed to train neural BP decoder
    for the system with one-bit quantizer.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike existing studies that consider the AWGN channel, the authors in [[356](#bib.bib356),
    [282](#bib.bib282), [283](#bib.bib283)] considered BSC and proposed finite precision
    decoders, called \acfaid, for LDPC codes with \acrqnn. More specifically, they
    proposed the BER as the loss function to train the RQNNs over BSC by leveraging
    \acste [[357](#bib.bib357)] to overcome the issue of gradients vanishing caused
    by the low precision activations in the RQNN and quantization in the BER.
  prefs: []
  type: TYPE_NORMAL
- en: In [[284](#bib.bib284)], the authors proposed a joint optimization of quantized
    message alphabets and quantization thresholds. Specifically, the authors utilized
    the softmax distribution [[358](#bib.bib358)] to make the quantization thresholds
    trainable by softening the one-hot distribution of the quantization. The proposed
    decoder was shown to outperform the original non-surjective FAIDs [[359](#bib.bib359)]
    in terms of error rate performance.
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Weight Sharing: In [[285](#bib.bib285), [286](#bib.bib286)], the authors
    proposed simple-scaling models for weighted BP decoding in [[257](#bib.bib257)]
    that share weights across edges, using only three scalar parameters per iteration:
    message weight, channel weight, and damping factor. The authors showed that such
    simple scaling models are often sufficient to achieve gains similar to the fully
    parameterized decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[287](#bib.bib287), [288](#bib.bib288)] proposed a parameter
    sharing scheme for a neural NMS decoder that shares the same correction (normalization)
    factors in the same layer. In contrast, the authors in [[360](#bib.bib360), [361](#bib.bib361)]
    proposed a family of weight sharing schemes for a neural NMS decoder that uses
    the same weight for edges with the same check node degree and/or variable node
    degree. Similarly, the authors in [[289](#bib.bib289)] proposed a neural MS decoder
    for protograph LDPC codes where a bundle of edges derived from the same edge type
    share identical parameters. Due to the lifting structure of protograph LDPC codes,
    the same set of parameters can be employed for multiple codes derived from the
    same base code.
  prefs: []
  type: TYPE_NORMAL
- en: In [[290](#bib.bib290)], the authors applied the \actt decomposition [[362](#bib.bib362)]
    to a neural NMS decoder, where it decomposes a high-order tensor into several
    low-order tensors. This not only reduces the number of weight parameters, but
    also the number of multiplications required in the CN and VN updates. Furthermore,
    in [[291](#bib.bib291)], the same authors proposed \actr decomposition [[363](#bib.bib363)]
    combined with weight sharing to further reduce the storage and computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In [[292](#bib.bib292)], a weight sharing scheme was proposed for a neural BP
    or MS decoder to adapt to multiple code rates with a reasonable amount of parameters.
    Specifically, instead of training different decoders, they proposed to train a
    single rate-compatible decoder based on multi-task learning, where different parts
    of the parameters are activated to handle different code rates.
  prefs: []
  type: TYPE_NORMAL
- en: '(c) Pruning: In [[293](#bib.bib293)], the authors proposed a novel pruning-based
    neural BP decoder for short linear block codes. The key idea was to prune unimportant
    CNs with small weights of an overcomplete PCM. Similarly, in [[294](#bib.bib294)],
    the same authors proposed a neural BP with decimation [[364](#bib.bib364)] for
    LDPC codes. In particular, they identified the least reliable VN with the aid
    of DL, i.e., the VN with the lowest absolute *a posteriori* LLR, and then decimated
    it to $\pm\infty$. It has been demonstrated that the proposed decoder with decimation
    can significantly outperform the conventional neural BP decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B8 GNN Decoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, BP computes the optimal (posterior) marginal probability distributions
    only for a non-loopy graphical model, and in practice it often computes a poor
    approximation of the true distribution. To tackle this limitation, the authors
    in [[295](#bib.bib295)] extended the standard GNN equations to factor graphs and
    presented a hybrid inference model that combines messages from BP and from GNN,
    where the GNN messages are learned to complement the BP messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of the method in [[295](#bib.bib295)], which extends BP decoding by
    combining it with a GNN, the authors in [[296](#bib.bib296)] proposed a fully
    GNN-based decoder. In contrast to weighted BP decoding, they introduced two types
    of MPNN-based trainable message update functions: *the edge message update functions*
    and *the node update functions*. Independently, an edge-weighted GNN decoder has
    been proposed in [[297](#bib.bib297)]. In the proposed decoder, they applied an
    MPNN for updating messages and assigned a trainable weight to each edge message,
    which is optimized by a fully-connected feed-forward neural network, i.e., MLP.
    The major advantage of these GNN decoders over the standard neural BP decoder
    is that the number of trainable parameters is not affected by the code length.
    Therefore, after training, the trained decoder can be applied to codes with different
    rates and lengths without retraining.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B9 Understanding Neural BP Decoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[298](#bib.bib298)], the authors empirically showed how the learned weights
    mitigate the effect of short cycles in Tanner graphs to improve the reliability
    of the posterior LLRs and contribute to the robustness of the decoders across
    channels. The authors also introduced an analytical approach for finding the weights
    using GA and compared the neural MS decoders, showing that for complicated fading
    channels, the neural network-based weight optimization leads to better performance
    than the GA-based optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[299](#bib.bib299)] theoretically investigated the generalization
    capabilities [[365](#bib.bib365)] of neural BP decoders, i.e., the difference
    between empirical and expected BERs. The paper presented new theoretical results
    that bound the gap and showed its dependence on the decoder complexity, in terms
    of code parameters (such as message/code lengths, VN/CN degrees), decoding iterations,
    and the training dataset size. They empirically observed that the generalization
    gap increases with decoding iterations and code length, and decays with the training
    dataset size, supporting the theoretical results in their paper.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B10 Other Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To improve the decoding of short Raptor-like LDPC codes, the authors in [[366](#bib.bib366)]
    considered multi-round BP decoding with impulsive perturbation [[367](#bib.bib367)].
    Perturbation is a process of making a small intentional change in the received
    signal, and this scheme iteratively performs conventional BP decoding and perturbation
    until a valid codeword is found. In [[302](#bib.bib302)], the authors proposed
    a neural network based perturbation symbol selection scheme where the symbols
    to be perturbed are selected from a pre-trained neural network and showed that
    the proposed scheme performs better than existing schemes such as [[366](#bib.bib366)]
    for Raptor-like LDPC codes.
  prefs: []
  type: TYPE_NORMAL
- en: The performance of a standalone neural BP decoder could be further enhanced
    by jointly optimizing signal detection and decoding. For instance, in [[301](#bib.bib301)],
    iterative signal detection and decoding via deep unfolding was proposed for MU-MIMO-OFDM.
    In [[300](#bib.bib300)], the authors proposed GNN-based joint detection and decoding
    for \acisi channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the approaches introduced in Section [IV-B7](#S4.SS2.SSS7 "IV-B7 Memory
    and Complexity Reduction ‣ IV-B DL-Aided BP Decoding ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"), the authors
    in [[303](#bib.bib303)] proposed another approach to alleviate decoding complexity
    and latency. Specifically, they proposed a DL approach for detecting the decodable
    codewords and predicting the iteration number from the received signal to reduce
    the decoding delay. This could potentially be useful for early feedback prediction
    in \acharq. Furthermore, in [[368](#bib.bib368)], the authors accelerated neural
    BP decoding through coded distributed computing [[369](#bib.bib369)]. In particular,
    they reformulated the neural BP decoding operations as matrix-vectors to facilitate
    distributed parallel decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to communication systems, DL-based decoders have also been studied
    for storage systems. In [[242](#bib.bib242)], the authors proposed DL-based decoder
    for \acsttmram [[370](#bib.bib370)]. In order to adapt to the process variation
    and unknown offset of the resistance caused by the change in working temperature,
    the authors proposed an adaptive decoding scheme based on the three DNN decoders,
    i.e., BF, MS, and BP decoders, which share the same DNN architecture but have
    different weights. In [[241](#bib.bib241)], the same research group proposed a
    neural normalized offset \acrbms decoding algorithm for STT-MRAM by introducing
    trainable parameters to the RBMS algorithm [[371](#bib.bib371)]. It has been demonstrated
    that the proposed scheme can outperform the RBMS algorithm over the STT-MRAM channel,
    while maintaining similar decoder structure and time complexity of the standard
    RBMS decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network-based BP decoding has been studied not only for classical error-correcting
    codes, but also for quantum error-correcting codes. For example, neural BP decoding
    has been applied to quantum LDPC codes [[372](#bib.bib372)] for which standard
    BP decoding may be insufficient due to the error degeneracy feature of quantum
    error-correcting codes [[373](#bib.bib373)]. By designing the loss function to
    account for error degeneracy, the decoding accuracy was improved up to three orders
    of magnitude compared to the standard BP decoder without training. Neural BP decoding
    for quantum LDPC codes was also studied in [[374](#bib.bib374), [375](#bib.bib375)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-C DL-Aided Decoding of Polar Codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE V: Summary of DL-aided polar decoders.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Reference | Main Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Neural Network-Based SC Decoding | Cammerer et al. [[376](#bib.bib376), [377](#bib.bib377)]
    | Partitioned neural network decoders. |'
  prefs: []
  type: TYPE_TB
- en: '| Doan et al. [[378](#bib.bib378)] | Neural successive cancellation (NSC) decoding.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Wodiany et al. [[379](#bib.bib379)] | Efficient implementation of an low-precision
    NSC decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Hebbar et al. [[380](#bib.bib380)] | Novel curriculum learning-based sequential
    neural decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| DL-Aided SCF Decoding | Wang et al. [[381](#bib.bib381)] | LSTM network that
    estimates the first erroneous bit. |'
  prefs: []
  type: TYPE_TB
- en: '| He et al. [[382](#bib.bib382)] | LSTM-based identification of erroneous bits
    for DSCF decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Doan et al. [[383](#bib.bib383), [384](#bib.bib384)] | Neural DSCF with trainable
    bit-flipping metric. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[385](#bib.bib385)] | Q-learning-assisted SCF decoding algorithm.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Doan et al. [[386](#bib.bib386)] | RL-based bit-flipping strategy for fast
    SC decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| DL-Aided SCLF Decoding | Hashemi et al. [[387](#bib.bib387)] | Trainable
    bit-flipping metric for SCL decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Doan et al. [[388](#bib.bib388)] | FSCLF decoding algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[389](#bib.bib389)] | LSTM-assisted bit-flipping algorithm for
    a CA-SCL decoder. |'
  prefs: []
  type: TYPE_TB
- en: '| Tao et al. [[390](#bib.bib390)] | New flip algorithm based on DNC. |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[391](#bib.bib391)] | Stacked LSTM to improve the accuracy
    of erroneous bit prediction. |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[392](#bib.bib392)] | Approximated bit-flipping metric for DSCLF
    decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Other Approaches | Lu et al. [[393](#bib.bib393)] | DL-aided shifting metric
    for SCL decoding. |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[394](#bib.bib394)] | CRC error-correction aided SCL decoding.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Model-free decoders in Section [IV-A](#S4.SS1 "IV-A Model-Free Decoders ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey") as well as neural BP decoders in Section [IV-B](#S4.SS2 "IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey") are easily applicable to polar codes. In the following,
    we also review methods for designing model-free decoders that take the specific
    code structure into account. Furthermore, we focus on DL approaches that augment
    conventional SC or SCL decoders, instead of replacing them with a DNN. In Table [V](#S4.T5
    "TABLE V ‣ IV-C DL-Aided Decoding of Polar Codes ‣ IV DL for Channel Decoding
    ‣ Recent Advances in Deep Learning for Channel Coding: A Survey"), we provide
    the summary of these methods.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Neural Network-Based SC Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the straightforward application of DNN is a viable option for decoding
    polar codes as in [[215](#bib.bib215)], the major issue was the exponential growth
    of training complexity. In [[376](#bib.bib376)], the authors addressed this issue
    by introducing \acpnn decoders. More specifically, inspired by the simplified
    successive cancellation algorithm in [[395](#bib.bib395)], which divides the decoding
    tree into \acpspc and \acprc), they replaced the SPC and RC subdecoders by neural
    networks. Simulations showed that the PNN decoder achieves similar BER performance
    to the SC and BP decoders for short lengths, such as $128$ bits, with potentially
    much lower latency. A similar concept of partitioning a neural network-based decoder
    for polar codes was also investigated in [[377](#bib.bib377)]. Furthermore, in
    order to reduce the latency of PNN, \acnsc decoding of polar codes was proposed
    in [[378](#bib.bib378)], where multiple constituent neural network decoders are
    incorporated into SC decoding, and its efficient implementation based on a low-precision
    neural network decoder was studied in [[379](#bib.bib379)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, another approach to tackle the difficulty of learning to decode long
    polar codes was proposed in [[380](#bib.bib380)], where a novel curriculum learning-based
    sequential neural decoder for polar and PAC codes was proposed. The paper designed
    a novel curriculum to train RNN, where the problem of joint estimation of information
    bits is decomposed into a sequence of sub-problems of increasing difficulty. The
    proposed decoder was shown to achieve better BER performance than the conventional
    supervised training without curriculum and standard SC decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of completely replacing a conventional decoder with DNNs, DL-based approaches
    that support conventional decoders such as SC and SCL decoding have been extensively
    studied. Among them, DL-assisted \acscf decoding [[396](#bib.bib396)] is one of
    the most popular approaches, which will be reviewed in the following.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 DL-Aided SC Flip Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Despite its low-complexity, the error correcting performance of SC decoding
    at finite block lengths is not comparable to other modern codes such as LDPC codes.
    In order to improve the finite block length performance, SCF decoding has been
    proposed in [[396](#bib.bib396)] inspired by the fact that the first erroneous
    bit decision in SC decoding has a detrimental impact on the resulting error rate.
    The SCF decoder first performs standard SC decoding to generate a first estimated
    codeword, and if the codeword passes the CRC, decoding is complete. If the CRC
    check fails, the SCF decoding makes $T$ additional attempts to identify the first
    error in the codeword. In each attempt, a single estimated codeword bit is flipped
    with respect to the initial decision. The algorithm terminates when a valid codeword
    has been found or when all $T$ attempts have been made. The SCF decoding procedures
    is shown in Fig. [13](#S4.F13 "Figure 13 ‣ IV-C2 DL-Aided SC Flip Decoding ‣ IV-C
    DL-Aided Decoding of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey"). SCF decoding retains the $O(N)$
    memory complexity of the original SC algorithm and has an average computational
    complexity that is practically $O(N\log N)$ at high SNR, while still providing
    a significant gain in terms of error correcting performance. While SCF decoding
    is limited to correcting a single erroneous bit in the codeword, \acdscf decoding
    [[397](#bib.bib397), [398](#bib.bib398)] is a generalization of SCF-based decoding
    that is able to correct higher-order erroneous information bits by dynamically
    updating the set of flipping bit indices after each decoding attempt.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81452ac3d2b77383867938ba07a64ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Flowchart of an SCF decoding framework with the number of trials
    $T$.'
  prefs: []
  type: TYPE_NORMAL
- en: The common challenge in SCF and DSCF decoding is how to identify the first error
    bit that causes error propagation. In the original work [[396](#bib.bib396)],
    the estimated codeword bits with the smallest amplitudes of LLR are flipped, but
    they are not necessarily the first errors. In fact, the optimal bit flipping strategy
    is still an open problem due to the lack of a rigorous mathematical characterization.
    Furthermore, DSCF decoding requires expensive exponential and logarithmic computations
    to compute the BF metric, which is used to determine the bit flipping position.
  prefs: []
  type: TYPE_NORMAL
- en: A popular DL-based solution is to train an LSTM network to estimate the first
    erroneous bit to be flipped. The authors in [[381](#bib.bib381)] have proposed
    an LSTM network for SCF decoding that takes an LLR sequence of the previous SC
    decoding attempt and outputs a vector where each element corresponds to the probability
    that a bit is the first error. Furthermore, the authors proposed a two-step training
    method that combines supervised learning with RL to train the LSTM to reverse
    previous incorrect flips. Similarly, the authors in [[382](#bib.bib382)] proposed
    an LSTM-based error bit identification for DSCF decoding where the network is
    trained to identify the first erroneous bit and additional erroneous bits by supervised
    learning and RL, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: There are other learning approaches that do not rely on model-free DNNs. In
    [[383](#bib.bib383), [384](#bib.bib384)], the authors proposed neural DSCF decoding
    where they properly approximated and introduced trainable parameters to the BF
    metric and optimized its parameter by RMSProp, a variant of the SGD optimization
    technique. In [[385](#bib.bib385)], the authors proposed a Q-learning-assisted
    SCF decoding algorithm that selects the candidate flipping bits through the learned
    Q-table instead of metric sorting. It was demonstrated that the proposed decoding
    algorithm is particularly effective in reducing the decoding delay caused by sorting
    during the decoding process without sacrificing performance. Similarly, in [[386](#bib.bib386)],
    an RL-based BF strategy is also investigated for fast SC decoding of polar codes
    [[399](#bib.bib399)], where the authors developed a new parameterized BF model
    based on [[387](#bib.bib387)] and optimized the trainable parameters using the
    policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 DL-Aided SCL Flip Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DL-aided BF mechanism can be applied not only to SC decoding but also to
    SCL decoding to further improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In [[387](#bib.bib387)], the authors proposed the BF metric for SCL decoding,
    which is expressed by a trainable correlation matrix representing the likelihood
    of each decoded bit. They optimized the trainable matrix using the RMSprop optimizer
    and demonstrated that compared to the conventional metric in [[397](#bib.bib397)],
    the proposed BF metric significantly reduces computational complexity associated
    with the bit metric calculation while maintaining similar error rate performance.
  prefs: []
  type: TYPE_NORMAL
- en: In [[388](#bib.bib388)], the authors applied BF to \acfscl decoding [[400](#bib.bib400),
    [401](#bib.bib401)], referred to as the \acfsclf decoding algorithm, to address
    the high latency problem associated with the \acsclf decoding algorithm. Specifically,
    the authors introduced a BF strategy tailored to FSCL decoding that avoids tree-traversal
    in the binary tree representation of SCLF to reduce the latency of the decoding
    process, and then derived a path selection error metric with a trainable parameter.
    The proposed decoder was shown to significantly reduce the average decoding latency,
    average complexity, and memory consumption of the SCLF decoder at the cost of
    slight degradation in error rate performance.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the above approaches that do not utilize a DNN, several papers
    have proposed a neural network-based selection of the flipping bit position. For
    instance, in [[389](#bib.bib389)], an LSTM-assisted BF algorithm has been proposed
    for a CA-SCL decoder. Furthermore, the authors have used the domain knowledge
    to reduce the complexity and memory requirements and computational complexity
    for efficient hardware implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[390](#bib.bib390)] proposed a new flip algorithm using \acdnc
    [[402](#bib.bib402)], which can be considered as an LSTM augmented with an external
    memory through attention-based soft read and write mechanisms. The proposed decoding
    algorithm is a two-phase decoding assisted by the two DNCs, i.e., flip DNC and
    flip-validate DNC. The former ranks flip positions for multi-bit flipping, while
    the latter is used to re-select flip positions when decoding fails. Simulation
    results show that the proposed DNC-aided SCLF achieves better error rate performance
    and reduction in the number of flipping attempts compared to the LSTM-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in [[391](#bib.bib391)] proposed a stacked LSTM network to improve
    the accuracy of erroneous bit prediction. Specifically, they trained the three
    models separately: the first and second models predict the positions of the first
    and the second erroneous bits and the third model decides whether to continue
    flipping. Simulation results demonstrate that their proposed algorithms outperform
    existing SCLF decoding algorithms in terms of BLER performance and average number
    of decoding attempts.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[392](#bib.bib392)], the authors proposed an approximated error metric for
    \acdsclf decoding of polar codes to improve the performance while keeping the
    average complexity low. To compensate for the approximation error, they introduced
    learnable parameters into the metric and optimized it through the custom neural
    network model using the RMSprop optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Other Approaches to Enhancing SCL Decoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: sp is another approach to enhance the performance of SCL decoding for polar
    codes [[403](#bib.bib403)], which aims to prevent the correct path from being
    eliminated from the list. In [[403](#bib.bib403)], it was demonstrated that the
    proposed SP mechanism offers remarkable performance gains over the BF approach.
    However, the SCL decoder with SP generally suffers from high computational complexity,
    due to the re-decoding attempts and the computation of the shifting metric. To
    alleviate this issue, the authors in [[393](#bib.bib393)] proposed a DL-aided
    shifting metric that is free from transcendental functions and can be computed
    on-the-fly based on the path metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to improve the performance of CA-SCL decoding was proposed
    in [[394](#bib.bib394)], where the authors take advantage of the inherent error
    correction capability of CRC, i.e., not just for error detection. The authors
    performed CRC-based error correction using an LSTM network, where the LSTM network
    estimates the error pattern from the LLR sequence and the CRC syndrome. The proposed
    CRC error-correction aided SCL decoding scheme was demonstrated to outperform
    the error rate of the conventional CRC error-detection aided SCL decoding scheme
    at the same list size.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D DL-Aided Convolutional and Turbo Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DL decoders have also been applied to convolutional and turbo codes. In particular,
    as we review below, several DL-aided turbo decoders have been proposed in recent
    years. We have listed these approaches in Table [VI](#S4.T6 "TABLE VI ‣ IV-D DL-Aided
    Convolutional and Turbo Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances
    in Deep Learning for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Summary of DL-aided convolutional and turbo decoders.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Reference | Main Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional Decoders | Teich et al. [[404](#bib.bib404)] | A DNN decoder
    for convolutional codes. |'
  prefs: []
  type: TYPE_TB
- en: '| Turbo Decoders | Kim et al. [[244](#bib.bib244)] | A neural network BCJR
    decoder, referred to as NEURALBCJR. |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. [[405](#bib.bib405)] | Deep turbo decoder (DEEPTURBO) trained
    in an end-to-end manner. |'
  prefs: []
  type: TYPE_TB
- en: '| He et al. [[406](#bib.bib406)] | A novel model-driven decoder, called TurboNet.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hebbar et al. [[407](#bib.bib407)] | TINYTURBO that reduces the parameters
    of TurboNet+. |'
  prefs: []
  type: TYPE_TB
- en: IV-D1 Convolutional Decoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Convolutional codes encode the input stream by convolving with the generator
    polynomial, which can be efficiently implemented by shift registers. Convolutional
    codes can be represented by a time-invariant trellis, which allows efficient maximum-likelihood
    decoding based on the well-known Viterbi algorithm [[408](#bib.bib408)].
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve the performance of \acitd [[409](#bib.bib409)], the authors
    in [[404](#bib.bib404)] proposed a DNN decoder for convolutional codes by unfolding
    a \achornn decoder [[410](#bib.bib410)]. The unfolded HORNN can be seen as a feedforward
    DNN whose parameters are trained by backpropagation with MBSGD. It was shown that
    with proper optimization of the parameters, the proposed decoder outperforms the
    conventional ITD and achieves performance close to maximum-likelihood decoding.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 Turbo Decoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Turbo codes, also known as parallel concatenated convolutional codes [[3](#bib.bib3)],
    consist of two (usually identical) \acrsc codes concatenated in parallel and a
    bit interleaver. Turbo codes are typically decoded by iterative decoding between
    two constituent SISO decoders where one constituent decoder computes posterior
    probabilities based on the \acbcjr algorithm [[411](#bib.bib411)], and then passes
    them to the other decoder. The turbo decoder significantly improves the error-correction
    performance by iteratively exchanging extrinsic information between the two constituent
    SISO decoders.
  prefs: []
  type: TYPE_NORMAL
- en: In [[244](#bib.bib244)], the authors proposed a DL-based BCJR decoder for an
    RSC code based on bi-GRU, referred to as NEURALBCJR, and then extended it to a
    turbo decoder by replacing the component SISO decoder with the proposed NEURALBCJR
    decoder. Simulations demonstrated that the proposed scheme is particularly beneficial
    for non-Gaussian channels, such as $t$-distributed noise. Later, a DL-aided turbo
    decoder, termed DEEPTURBO, was introduced in [[405](#bib.bib405)]. They also used
    bi-GRUs to replace the conventional SISO decoders as in [[244](#bib.bib244)],
    but the authors in [[405](#bib.bib405)] trained different bi-GRU weights across
    different iterations, whereas the authors in [[244](#bib.bib244)] shared the same
    weight for all bi-GRU blocks. This enables a fully end-to-end training without
    imitating the BCJR algorithm. Furthermore, DEEPTURBO increased the number of posterior
    LLR values exchanged between the two decoders to expedite iterative decoding.
    Extensive simulations have demonstrated that DEEPTURBO exhibits an improved reliability,
    adaptivity, and lower error floor compared to NEURALBCJR.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fcf4b3be90abd4df43bc33b99f097a5e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Turbo encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ead6eb4230c464d9f5521ea85928175e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Model-free DL turbo decoders based on Bi-GRUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: An encoder and DL-based decoder of turbo codes. In the NEURALBCJR
    decoder [[244](#bib.bib244)], each Bi-GRU is pre-trained to imitate BCJR algorithm,
    followed by an end-to-end training. In the DEEPTURBO decoder [[405](#bib.bib405)],
    on the other hand, all Bi-GRUs are trained directly to optimize the end-to-end
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned model-free turbo decoders, i.e., NEURALBCJR [[244](#bib.bib244)]
    and DEEPTURBO [[405](#bib.bib405)], are illustrated in Fig. [14](#S4.F14 "Figure
    14 ‣ IV-D2 Turbo Decoder ‣ IV-D DL-Aided Convolutional and Turbo Decoding ‣ IV
    DL for Channel Decoding ‣ Recent Advances in Deep Learning for Channel Coding:
    A Survey"), where the constituent SISO decoders of the turbo decoder are replaced
    with Bi-GRUs without considering the specific trellis structure of the RSC encoders.
    In contrast, the authors in [[406](#bib.bib406)] proposed a novel model-driven
    decoder architecture, called TurboNet, which integrates DNN into the traditional
    max-log-MAP algorithm. Furthermore, they applied network pruning to TurboNet to
    effectively reduce the number of parameters. The resulting TurboNet+ decoder was
    shown to achieve state-of-the-art performance and outperform existing DL turbo
    decoders even with lower computational complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, the authors in [[407](#bib.bib407)] proposed TINYTURBO which significantly
    reduces the trainable parameters of TurboNet+ by sharing the same weight across
    bit indices in the computation of the posterior LLR. In particular, for a block
    length of $40$, it was demonstrated that TINYTURBO with $18$ parameters outperforms
    TurboNet+ with $720$ parameters over AWGN channels. Furthermore, the strong adaptability
    of TINYTURBO to other block lengths, rates, and trellises, as well as its robustness
    to channel variations, were demonstrated.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E DL-Aided Decoding of Cyclic Codes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL decoders exploiting algebraic properties of cyclic codes have also been studied.
    In [[412](#bib.bib412)], the authors proposed a neural network-based decoder for
    cyclic codes by exploiting their cyclically invariant property. More specifically,
    inspired by the fact that the maximum-likelihood decoder of any cyclic code is
    equivariant with respect to cyclic shifts, they imposed a shift-invariant structure
    on the weights of the neural decoder so that any cyclic shift of inputs results
    in the same cyclic shift of the outputs. Simulations of BCH codes and punctured
    \acrm codes showed that the proposed decoder consistently outperforms the neural
    BP decoder proposed in [[258](#bib.bib258)]. Furthermore, they proposed a list
    decoding procedure that can significantly reduce the decoding error for BCH codes
    and punctured RM codes.
  prefs: []
  type: TYPE_NORMAL
- en: While the list decoding significantly improves the BLER, the major drawback
    was its relatively high BER. To improve the BER, the same authors proposed the
    improved version of the list decoder in [[413](#bib.bib413)]. The new decoder
    achieved a significantly lower BER compared to the list decoder in [[412](#bib.bib412)]
    while maintaining the same BLER.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have provided a comprehensive survey on DL for the channel
    coding problems. In particular, we have focused on DL methods for the code design
    and channel decoding problems. In what follows, we summarize the potential advantages
    and challenges of these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Code Design Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conventional code design algorithms such as EXIT chart and variants of DE
    require ideal assumptions about channel models and decoding schemes. In contrast,
    the major advantage of using data-driven DL for code design is that one can tailor
    codes to more realistic channels and decoding schemes, for which theoretical analysis
    is intractable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [III](#S3 "III DL for Code Design ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"), we saw that RL is a particularly popular approach
    to designing polar codes, among others. In this method, an agent learns to choose
    a new information bit position that minimizes the cumulative reward, which corresponds
    to the actual performance in terms of BLER. Calculating the reward requires Monte-Carlo
    simulations, which can be computationally intensive depending on the code length
    and target error rate. This complexity issue can hinder its application to scenarios
    with long code lengths and low error rates. Therefore, a design objective that
    can be efficiently computed during the training process may be desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Channel Decoding Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have seen, channel decoding is a popular application of DL and a significant
    number of papers on this topic have become available. These approaches can be
    broadly classified into model-free and model-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 Model-Free Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Model-free decoders employing a “black-box” neural network have the potential
    to outperform conventional decoding algorithms in terms of error rate performance
    and decoding complexity/latency. In particular, it has been demonstrated that
    model-free decoders can outperform existing decoding algorithms for short code
    lengths with highly parallelizable structures. This approach is thus potentially
    suitable for low-latency applications requiring short code lengths. Note that
    the performance is highly dependent on the DL model employed, and currently, the
    Transformer-based decoder achieves the state-of-the-art performance [[222](#bib.bib222)].
    However, the performance could be potentially improved by the advanced DL techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the curse of dimensionality, the applications of model-free decoders
    have generally been limited to short codes. In general, a larger model size is
    required to decode a longer code, which not only entails high computational complexity,
    but also high space complexity in both training and inference phases. Another
    concern about this method is the robustness against adversarial attacks [[204](#bib.bib204)],
    as wireless networks are always vulnerable to radio jamming attacks [[414](#bib.bib414),
    [415](#bib.bib415)] due to the openness of wireless channels. In particular, DL-based
    communication systems may have a higher risk of being disrupted by jamming attacks
    than classical systems [[416](#bib.bib416), [417](#bib.bib417)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of completely replacing conventional decoders, employing a DNN to augment
    existing decoders is effective for arbitrary code lengths. For example, model-free
    training has been extensively studied for DNN-based selection of flipping bit
    indices in SCF decoding as we reviewed in Section [IV-C](#S4.SS3 "IV-C DL-Aided
    Decoding of Polar Codes ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep
    Learning for Channel Coding: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Model-Based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In contrast to the model-free approach, the model-based approach realizes a
    scalable decoder by taking advantage of the knowledge of code structures and conventional
    decoding algorithms. One of the most promising approaches is deep unfolding, which
    unfolds an iterative algorithm [[306](#bib.bib306)] and introduces a set of trainable
    parameters. In particular, as we reviewed in Section [IV-B](#S4.SS2 "IV-B DL-Aided
    BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning for
    Channel Coding: A Survey"), neural network-based BP decoding over an unfolded
    Tanner graph augmented with trainable parameters [[257](#bib.bib257)] has been
    extensively investigated. As the underlying BP decoding algorithm has already
    been adopted in a wide range of communication systems, this approach can be applied
    to these systems with much less modification compared to model-free decoders.
    Many existing works have demonstrated that, by introducing and optimizing trainable
    weights that mitigate the effect of short cycles in the Tanner graph, the DL BP
    decoder can achieve better a trade-off between decoding performance and latency,
    i.e., the number of decoding iterations, compared to the standard BP decoder.
    This means that the performance advantage of the DL BP decoder becomes more significant
    as the number of short cycles increases.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Challenges and Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite their excellent performance, DL-based channel coding schemes face challenges
    that need to be addressed. We conclude this survey by highlighting several future
    research directions in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Flexibility to Support Diverse Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next generation communication systems such as 6G will support heterogeneous
    applications that employ the channel codes with various block lengths, reliability,
    and latency requirements [[37](#bib.bib37)]. Since there is no one-size-fits-all
    channel coding scheme, multiple code parameters, i.e., rates and lengths, must
    be supported to meet these requirements. On the other hand, adapting a DL decoder
    to different channels and code parameters would require an enormous amount of
    different parameter sets. This issue could be alleviated, for example, by a parameter
    sharing scheme and scalable GNNs as discussed in Section [IV-B](#S4.SS2 "IV-B
    DL-Aided BP Decoding ‣ IV DL for Channel Decoding ‣ Recent Advances in Deep Learning
    for Channel Coding: A Survey"). Furthermore, in order to support multiple code
    parameters, training must be performed multiple times, which is time consuming
    and computationally intensive. This complexity issue can be addressed by techniques
    such as transfer learning, meta-learning, and foundation models [[248](#bib.bib248)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 Explainable AI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the disadvantages of DL methods is their black-box nature, which can
    hinder physical insights into the phenomena. Thus, evaluating and enhancing the
    explainability of generic DL models, i.e., the ability to provide reasons for
    the outcomes of the system [[418](#bib.bib418)], remains an active field of research
    [[419](#bib.bib419), [420](#bib.bib420), [421](#bib.bib421), [422](#bib.bib422),
    [423](#bib.bib423), [424](#bib.bib424), [425](#bib.bib425)]. In general, the explainability
    of DL models tends to have an inverse relationship to their performance, e.g.,
    prediction accuracy [[426](#bib.bib426)]. Thus, a recent advanced DL model with
    a large number of parameters is particularly difficult to interpret and explain.
  prefs: []
  type: TYPE_NORMAL
- en: In the next generation communications such as 6G, the concept of \acxai will
    become increasingly important especially for the emerging mission-critical services,
    such as autonomous driving and remote surgery [[427](#bib.bib427), [428](#bib.bib428),
    [429](#bib.bib429)]. Although the effectiveness of DL for the physical layer has
    been demonstrated in terms of its performance, its explainability has not been
    well studied. Thus, XAI-based channel coding that increases the transparency of
    DL models and explains the reasons for decisions will be of practical importance.
    The new insights gained from XAI will also help us to devise code design and decoding
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: V-C3 Efficient Training and Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent advances in DL technologies have been driven by the exponential growth
    of data and computational power, with a focus on performance rather than the economic
    and environmental costs. This research trend is often referred to as *Red AI*
    [[430](#bib.bib430)]. Indeed, the computations required for DL algorithms result
    in a surprisingly large carbon footprint [[431](#bib.bib431), [432](#bib.bib432),
    [433](#bib.bib433)]. In contrast to Red AI, which prioritizes achieving state-of-the-art
    results, *Green AI* aims to produce innovative results while taking into account
    computational costs [[430](#bib.bib430), [434](#bib.bib434)]. This paradigm shift
    toward energy and cost efficiency is inevitable for the long-term success, and
    it is therefore important to carefully select and preprocess data, reduce redundancy,
    and avoid overfitting so as to minimize the amounts of data and computational
    resources required [[435](#bib.bib435)].
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric approaches are promising for reducing the energy consumption of
    DL algorithms [[434](#bib.bib434)], while their primary goal was to improve performance
    in terms of accuracy⁷⁷7[https://datacentricai.org/](https://datacentricai.org/).
    These approaches recognize that the quality of the training data has a significant
    impact on their performance, and thus prioritize data quality over model refinement
    [[436](#bib.bib436), [437](#bib.bib437), [438](#bib.bib438)]. These include active
    learning, knowledge transfer, dataset distillation, data augmentation, and curriculum
    learning. For channel decoding applications, some papers have employed these techniques
    for enhancing error rate performance, but more emphasis should be placed on the
    data efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: V-C4 Quantum Machine Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quantum computing has a great potential to solve the classical channel coding
    problems [[439](#bib.bib439), [440](#bib.bib440), [441](#bib.bib441), [442](#bib.bib442),
    [443](#bib.bib443), [444](#bib.bib444)] more efficiently than digital computers.
    Especially in the current \acnisq era [[445](#bib.bib445)], where the fidelity
    of quantum gates is limited by noise and decoherence, hybrid quantum-classical
    algorithms such as \acvqe [[446](#bib.bib446)] and \acqaoa [[447](#bib.bib447)]
    are promising [[448](#bib.bib448), [449](#bib.bib449)]. The potential of these
    algorithms for the classical channel decoding problems has been demonstrated in
    [[440](#bib.bib440)].
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, \acqml, which integrates quantum algorithms into ML, has received
    increasing attention [[450](#bib.bib450), [451](#bib.bib451), [88](#bib.bib88),
    [452](#bib.bib452), [453](#bib.bib453), [454](#bib.bib454), [455](#bib.bib455)].
    For example, it has been shown that well-designed quantum neural networks can
    achieve a higher capacity and faster training ability than comparable classical
    feedforward neural networks [[456](#bib.bib456)]. Furthermore, quantum counterparts
    to classical CNNs, autoencoders, and \acpgan have been studied in [[457](#bib.bib457),
    [458](#bib.bib458), [459](#bib.bib459), [460](#bib.bib460)]. These methods have
    the potential to improve existing methods based on classical computers for a wide
    range of communication problems including channel coding.
  prefs: []
  type: TYPE_NORMAL
- en: \MFUhyphentrue\acsetup
  prefs: []
  type: TYPE_NORMAL
- en: uppercase/list, list/uppercase/cmd=\ecapitalisewords \printacronyms[name=List
    of Abbreviations]
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. E. Shannon, “A mathematical theory of communication,” *Bell Syst. Tech.
    J.*, vol. 27, pp. 379–423 and 623–656, July and Oct. 1948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. J. Costello and G. D. Forney, “Channel coding: The road to channel capacity,”
    *Proceedings of the IEEE*, vol. 95, no. 6, pp. 1150–1177, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. Berrou, A. Glavieux, and P. Thitimajshima, “Near shannon limit error-correcting
    coding and decoding: Turbo-codes,” in *Proc. IEEE International Conference on
    Communications (ICC)*, pp. 1064–1070, May 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. G. Gallager, “Low-density parity-check codes,” *IRE Trans. Info. Theory,*,
    vol. 8, no. 1, pp. 21–28, Jan. 1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] E. Arıkan, “Channel polarization: A method for constructing capacity-achieving
    codes for symmetric binary-input memoryless channels,” *IEEE Trans. Inf. Theory*,
    vol. 55, no. 7, pp. 3051–3073, Jul. 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] P. Yang, Y. Xiao, M. Xiao, and S. Li, “6G wireless communications: Vision
    and potential techniques,” *IEEE Network*, vol. 33, no. 4, pp. 70–75, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,
    and P. Fan, “6G wireless networks: Vision, requirements, architecture, and key
    technologies,” *IEEE Veh. Technol. Mag.*, vol. 14, no. 3, pp. 28–41, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] T. Huang, W. Yang, J. Wu, J. Ma, X. Zhang, and D. Zhang, “A survey on green
    6G network: Architecture and technologies,” *IEEE Access*, vol. 7, pp. 175 758–175 768,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems: Applications,
    trends, technologies, and open research problems,” *IEEE Network*, vol. 34, no. 3,
    pp. 134–142, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, “The roadmap
    to 6G: AI empowered wireless networks,” *IEEE Commun. Mag.*, vol. 57, no. 8, pp.
    84–90, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M. Z. Chowdhury, M. Shahjalal, S. Ahmed, and Y. M. Jang, “6G wireless
    communication systems: Applications, requirements, technologies, challenges, and
    research directions,” *IEEE Open J. Commun. Soc.*, vol. 1, pp. 957–975, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Dogra, R. K. Jha, and S. Jain, “A survey on beyond 5G network with
    the advent of 6G: Architecture and emerging technologies,” *IEEE Access*, vol. 9,
    pp. 67 512–67 547, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] G. Gui, M. Liu, F. Tang, N. Kato, and F. Adachi, “6G: Opening new horizons
    for integration of comfort, security, and intelligence,” *IEEE Wireless Communications*,
    vol. 27, no. 5, pp. 126–132, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] N. Kato, B. Mao, F. Tang, Y. Kawamoto, and J. Liu, “Ten challenges in
    advancing machine learning technologies toward 6G,” *IEEE Wireless Communications*,
    vol. 27, no. 3, pp. 96–103, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] M. Giordani, M. Polese, M. Mezzavilla, S. Rangan, and M. Zorzi, “Toward
    6G networks: Use cases and technologies,” *IEEE Commun. Mag.*, vol. 58, no. 3,
    pp. 55–61, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] N. Rajatheva, I. Atzeni, E. Bjornson, A. Bourdoux, S. Buzzi, J.-B. Dore,
    S. Erkucuk, M. Fuentes, K. Guan, Y. Hu *et al.*, “White paper on broadband connectivity
    in 6G,” *arXiv preprint arXiv:2004.14247*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Chen, Y.-C. Liang, S. Sun, S. Kang, W. Cheng, and M. Peng, “Vision,
    requirements, and technology trend of 6G: How to tackle the challenges of system
    coverage, capacity, user data-rate and movement speed,” *IEEE Wireless Communications*,
    vol. 27, no. 2, pp. 218–228, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Dang, O. Amin, B. Shihada, and M.-S. Alouini, “What should 6G be?”
    *Nature Electronics*, vol. 3, no. 1, pp. 20–29, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. F. Akyildiz, A. Kak, and S. Nie, “6G and beyond: The future of wireless
    communications systems,” *IEEE Access*, vol. 8, pp. 133 995–134 030, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. U. Khan, I. Yaqoob, M. Imran, Z. Han, and C. S. Hong, “6G wireless
    systems: A vision, architectural elements, and future directions,” *IEEE Access*,
    vol. 8, pp. 147 029–147 044, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] E. Yaacoub and M.-S. Alouini, “A key 6G challenge and opportunity—connecting
    the base of the pyramid: A survey on rural connectivity,” *Proceedings of the
    IEEE*, vol. 108, no. 4, pp. 533–582, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Viswanathan and P. E. Mogensen, “Communications in the 6G era,” *IEEE
    Access*, vol. 8, pp. 57 063–57 074, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] F. Tariq, M. R. Khandaker, K.-K. Wong, M. A. Imran, M. Bennis, and M. Debbah,
    “A speculative study on 6G,” *IEEE Wireless Communications*, vol. 27, no. 4, pp.
    118–125, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] H. Tataria, M. Shafi, A. F. Molisch, M. Dohler, H. Sjöland, and F. Tufvesson,
    “6G wireless systems: Vision, requirements, challenges, insights, and opportunities,”
    *Proceedings of the IEEE*, vol. 109, no. 7, pp. 1166–1199, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] D. C. Nguyen, M. Ding, P. N. Pathirana, A. Seneviratne, J. Li, D. Niyato,
    O. Dobre, and H. V. Poor, “6G internet of things: A comprehensive survey,” *IEEE
    Internet Things J.*, vol. 9, no. 1, pp. 359–383, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] W. Jiang, B. Han, M. A. Habibi, and H. D. Schotten, “The road towards
    6G: A comprehensive survey,” *IEEE Open J. Commun. Soc.*, vol. 2, pp. 334–366,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Alsabah, M. A. Naser, B. M. Mahmmod, S. H. Abdulhussain, M. R. Eissa,
    A. Al-Baidhani, N. K. Noordin, S. M. Sait, K. A. Al-Utaibi, and F. Hashim, “6G
    wireless communications networks: A comprehensive survey,” *IEEE Access*, vol. 9,
    pp. 148 191–148 243, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. De Lima, D. Belot, R. Berkvens, A. Bourdoux, D. Dardari, M. Guillaud,
    M. Isomursu, E.-S. Lohan, Y. Miao, A. N. Barreto *et al.*, “Convergent communication,
    sensing and localization in 6G systems: An overview of technologies, opportunities
    and challenges,” *IEEE Access*, vol. 9, pp. 26 902–26 925, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. De Alwis, A. Kalla, Q.-V. Pham, P. Kumar, K. Dev, W.-J. Hwang, and
    M. Liyanage, “Survey on 6G frontiers: Trends, applications, requirements, technologies
    and future research,” *IEEE Open J. Commun. Soc.*, vol. 2, pp. 836–886, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] F. Guo, F. R. Yu, H. Zhang, X. Li, H. Ji, and V. C. Leung, “Enabling massive
    iot toward 6G: A comprehensive survey,” *IEEE Internet Things J.*, vol. 8, no. 15,
    pp. 11 891–11 915, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] M. Matthaiou, O. Yurduseven, H. Q. Ngo, D. Morales-Jimenez, S. L. Cotton,
    and V. F. Fusco, “The road to 6G: Ten physical layer challenges for communications
    engineers,” *IEEE Commun. Mag.*, vol. 59, no. 1, pp. 64–69, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M. Noor-A-Rahim, Z. Liu, H. Lee, M. O. Khyam, J. He, D. Pesch, K. Moessner,
    W. Saad, and H. V. Poor, “6G for vehicle-to-everything (V2X) communications: Enabling
    technologies, challenges, and opportunities,” *Proceedings of the IEEE*, vol.
    110, no. 6, pp. 712–734, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C.-X. Wang, X. You, X. Gao, X. Zhu, Z. Li, C. Zhang, H. Wang, Y. Huang,
    Y. Chen, H. Haas *et al.*, “On the road to 6G: Visions, requirements, key technologies
    and testbeds,” *IEEE Communications Surveys & Tutorials*, vol. 25, no. 2, pp.
    905–974, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. Chafii, L. Bariah, S. Muhaidat, and M. Debbah, “Twelve scientific challenges
    for 6G: Rethinking the foundations of communications theory,” *IEEE Communications
    Surveys & Tutorials*, vol. 25, no. 2, pp. 868–904, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] C. You, Y. Cai, Y. Liu, M. Di Renzo, T. M. Duman, A. Yener, and A. L.
    Swindlehurst, “Next generation advanced transceiver technologies for 6G,” *arXiv
    preprint arXiv:2403.16458*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. Geiselhart, F. Krieg, J. Clausius, D. Tandler, and S. ten Brink, “6G:
    A welcome chance to unify channel coding?” *IEEE BITS the Information Theory Magazine*,
    vol. 3, no. 1, pp. 67–80, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Zhang and W. Tong, “Channel coding for 6G extreme connectivity-requirements,
    capabilities and fundamental tradeoffs,” *IEEE BITS the Information Theory Magazine*,
    vol. 3, no. 1, pp. 54–66, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard *et al.*, “Tensorflow: A system for large-scale machine learning,”
    in *Proc. 12th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI)*, pp. 265–283, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Collobert, K. Kavukcuoglu, and C. Farabet, “Torch7: A Matlab-like environment
    for machine learning,” in *Proc. Advances in Neural Information Processing Systems
    (NeurIPS) Workshop on Big Learning*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers *et al.*, “In-datacenter performance
    analysis of a tensor processing unit,” in *Proc. 44th Annual International Symposium
    on Computer Architecture (ISCA)*, pp. 1–12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M. Qasaimeh, K. Denolf, J. Lo, K. Vissers, J. Zambreno, and P. H. Jones,
    “Comparing energy efficiency of CPU, GPU and FPGA implementations for vision kernels,”
    in *Proc. IEEE International Conference on Embedded Software and Systems (ICESS)*,
    pp. 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] T. Wang, C.-K. Wen, H. Wang, F. Gao, T. Jiang, and S. Jin, “Deep learning
    for wireless physical layer: Opportunities and challenges,” *China Communications*,
    vol. 14, no. 11, pp. 92–111, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and wireless
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 3,
    pp. 2224–2287, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D. Gündüz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R. Murthy,
    and M. van der Schaar, “Machine learning in the air,” *IEEE J. Sel. Areas Commun.*,
    vol. 37, no. 10, pp. 2184–2199, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Balatsoukas-Stimming and C. Studer, “Deep unfolding for communications
    systems: A survey and some new directions,” in *Proc. IEEE International Workshop
    on Signal Processing Systems (SiPS)*, pp. 266–271, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Samad, W. Saad, R. Nandana, C. Kapseok, D. Steinbach, B. Sliwa, C. Wietfeld,
    K. Mei, S. Hamid, H.-J. Zepernick *et al.*, *White Paper on Machine Learning in
    6G Wireless Communication Networks*.   University of Oulu, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] C. Zhang, Y.-L. Ueng, C. Studer, and A. Burg, “Artificial intelligence
    for 5G and beyond 5G: Implementations, algorithms, and optimizations,” *IEEE J.
    Emerg. Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 149–163, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Ly and Y.-D. Yao, “A review of deep learning in 5G research: Channel
    coding, massive MIMO, multiple access, resource allocation, and network security,”
    *IEEE Open J. Commun. Soc.*, vol. 2, pp. 396–408, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Mao, Z. Mu, Q. Liang, I. Schizas, and C. Pan, “Deep learning in physical
    layer communications: Evolution and prospects in 5G and 6G networks,” *IET Communications*,
    vol. 17, no. 16, pp. 1863–1876, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Akrout, A. Feriani, F. Bellili, A. Mezghani, and E. Hossain, “Domain
    generalization in machine learning models for wireless communications: Concepts,
    state-of-the-art, and open issues,” *IEEE Communications Surveys & Tutorials*,
    vol. 25, no. 4, pp. 3014–3037, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy, “Domain generalization:
    A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 45, no. 4, pp. 4396–4415,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and
    P. Yu, “Generalizing to unseen domains: A survey on domain generalization,” *IEEE
    Trans. Knowl. Data Eng.*, vol. 35, no. 8, pp. 8052–8072, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] N. Ye, S. Miao, J. Pan, Q. Ouyang, X. Li, and X. Hou, “Artificial intelligence
    for wireless physical-layer technologies (AI4PHY): A comprehensive survey,” *IEEE
    Trans. Cogn. Commun. Netw.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Rowshan, M. Qiu, Y. Xie, X. Gu, and J. Yuan, “Channel coding towards
    6G: Technical overview and outlook,” *IEEE Open J. Commun. Soc.*, vol. 5, pp.
    2585–2685, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] X.-A. Wang and S. B. Wicker, “An artificial neural net Viterbi decoder,”
    *IEEE Trans. Commun.*, vol. 44, no. 2, pp. 165–171, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Hamalainen and J. Henriksson, “A recurrent neural decoder for convolutional
    codes,” in *Proc. IEEE International Conference on Communications (ICC)*, vol. 2,
    pp. 1305–1309, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Ibnkahla, “Applications of neural networks to digital communications–A
    survey,” *Signal Processing*, vol. 80, no. 7, pp. 1185–1215, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] T. O’Shea and J. Hoydis, “An introduction to deep learning for the physical
    layer,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 3, no. 4, pp. 563–575, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] O. Simeone, “A very brief introduction to machine learning with applications
    to communication systems,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 4, no. 4, pp.
    648–664, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Dörner, S. Cammerer, J. Hoydis, and S. ten Brink, “Deep learning based
    communication over the air,” *IEEE J. Sel. Topics Signal Process.*, vol. 12, no. 1,
    pp. 132–143, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Huang, S. Guo, G. Gui, Z. Yang, J. Zhang, H. Sari, and F. Adachi, “Deep
    learning for physical-layer 5G wireless techniques: Opportunities, challenges
    and solutions,” *IEEE Wireless Communications*, vol. 27, no. 1, pp. 214–222, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. He, S. Jin, C.-K. Wen, F. Gao, G. Y. Li, and Z. Xu, “Model-driven deep
    learning for physical layer communications,” *IEEE Wireless Communications*, vol. 26,
    no. 5, pp. 77–83, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Kim, W. Lee, J. Yoon, and O. Jo, “Toward the realization of encoder
    and decoder using deep neural networks,” *IEEE Commun. Mag.*, vol. 57, no. 5,
    pp. 57–63, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. Varasteh, J. Hoydis, and B. Clerckx, “Learning to communicate and energize:
    Modulation, coding, and multiple access designs for wireless information-power
    transmission,” *IEEE Trans. Commun.*, vol. 68, no. 11, pp. 6822–6839, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] F. Restuccia and T. Melodia, “Deep learning at the physical layer: System
    challenges and applications to 5G and beyond,” *IEEE Commun. Mag.*, vol. 58, no. 10,
    pp. 58–64, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Zhang, J. Liu, T. K. Rodrigues, and N. Kato, “Deep learning techniques
    for advancing 6G communications in the physical layer,” *IEEE Wireless Communications*,
    vol. 28, no. 5, pp. 141–147, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] B. Ozpoyraz, A. T. Dogukan, Y. Gevez, U. Altun, and E. Basar, “Deep learning-aided
    6G wireless networks: A comprehensive survey of revolutionary PHY architectures,”
    *IEEE Open J. Commun. Soc.*, vol. 3, pp. 1749–1809, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Z. Lu, R. Li, K. Lu, X. Chen, E. Hossain, Z. Zhao, and H. Zhang, “Semantics-empowered
    communications: A tutorial-cum-survey,” *IEEE Communications Surveys & Tutorials*,
    vol. 26, no. 1, pp. 41–79, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] T. Ohtsuki, “Machine learning in 6G wireless communications,” *IEICE Trans.
    Commun.*, vol. 106, no. 2, pp. 75–83, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Y. Shi, L. Lian, Y. Shi, Z. Wang, Y. Zhou, L. Fu, L. Bai, J. Zhang, and
    W. Zhang, “Machine learning for large-scale optimization in 6G wireless networks,”
    *IEEE Communications Surveys & Tutorials*, vol. 25, no. 4, pp. 2088–2132, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D. I.
    Kim, and K. B. Letaief, “Generative AI for physical layer communications: A survey,”
    *IEEE Trans. Cogn. Commun. Netw.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] N. Islam and S. Shin, “Deep learning in physical layer: Review on data
    driven end-to-end communication systems and their enabling semantic applications,”
    *arXiv preprint arXiv:2401.12800*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. M. Hoang, A. Vahid, H. D. Tuan, and L. Hanzo, “Physical layer authentication
    and security design in the machine learning era,” *IEEE Communications Surveys
    & Tutorials*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Hoydis, S. Cammerer, F. A. Aoudia, A. Vem, N. Binder, G. Marcus, and
    A. Keller, “Sionna: An open-source library for next-generation physical layer
    research,” *arXiv preprint arXiv:2203.11854*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] H. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, “Deep learning enabled semantic
    communication systems,” *IEEE Trans. Signal Process.*, vol. 69, pp. 2663–2675,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Z. Qin, X. Tao, J. Lu, W. Tong, and G. Y. Li, “Semantic communications:
    Principles and challenges,” *arXiv preprint arXiv:2201.01389*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse:
    Visions, enabling technologies, and challenges,” *IEEE Communications Surveys
    & Tutorials*, vol. 25, no. 1, pp. 656–700, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] W. Yang, H. Du, Z. Q. Liew, W. Y. B. Lim, Z. Xiong, D. Niyato, X. Chi,
    X. S. Shen, and C. Miao, “Semantic communications for future internet: Fundamentals,
    applications, and challenges,” *IEEE Communications Surveys & Tutorials*, vol. 25,
    no. 1, pp. 213–250, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X. Luo, H.-H. Chen, and Q. Guo, “Semantic communications: Overview, open
    issues, and future research directions,” *IEEE Wireless Communications*, vol. 29,
    no. 1, pp. 210–219, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. Gündüz, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener, K. K.
    Wong, and C.-B. Chae, “Beyond transmitting bits: Context, semantics, and task-oriented
    communications,” *IEEE J. Sel. Areas Commun.*, vol. 41, no. 1, pp. 5–41, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] T. M. Getu, G. Kaddoum, and M. Bennis, “Making sense of meaning: A survey
    on metrics for semantic and goal-oriented communication,” *IEEE Access*, vol. 11,
    pp. 2169–3536, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] K. Li, B. P. L. Lau, X. Yuan, W. Ni, M. Guizani, and C. Yuen, “Towards
    ubiquitous semantic metaverse: Challenges, approaches, and opportunities,” *IEEE
    Internet Things J.*, vol. 10, no. 24, pp. 21 855–21 872, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Wheeler and B. Natarajan, “Engineering semantic communication: A survey,”
    *IEEE Access*, vol. 11, pp. 13 965–13 995, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] G. Torlai and R. G. Melko, “Neural decoder for topological codes,” *Physical
    Review Letters*, vol. 119, no. 3, p. 030501, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Varsamopoulos, B. Criger, and K. Bertels, “Decoding small surface codes
    with feedforward neural networks,” *Quantum Science and Technology*, vol. 3, no. 1,
    p. 015004, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. Varsamopoulos, K. Bertels, and C. G. Almudever, “Comparing neural network
    based decoders for the surface code,” *IEEE Transactions on Computers*, vol. 69,
    no. 2, pp. 300–311, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel, and N. Friis, “Optimizing
    quantum error correction codes with reinforcement learning,” *Quantum*, vol. 3,
    p. 215, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto,
    and L. Zdeborová, “Machine learning and the physical sciences,” *Reviews of Modern
    Physics*, vol. 91, no. 4, p. 045002, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. d. iOlius, P. Fuentes, R. Orús, P. M. Crespo, and J. E. Martinez, “Decoding
    algorithms for surface codes,” *arXiv preprint arXiv:2307.14989*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Krenn, J. Landgraf, T. Foesel, and F. Marquardt, “Artificial intelligence
    and machine learning for quantum technologies,” *Physical Review A*, vol. 107,
    no. 1, p. 010101, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
    in nervous activity,” *The Bulletin of Mathematical Biophysics*, vol. 5, no. 4,
    pp. 115–133, 1943.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations
    by back-propagating errors,” *Nature*, vol. 323, no. 6088, pp. 533–536, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, “A survey
    of deep neural network architectures and their applications,” *Neurocomputing*,
    vol. 234, pp. 11–26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] L. Deng, D. Yu *et al.*, “Deep learning: methods and applications,” *Foundations
    and Trends® in Signal Processing*, vol. 7, no. 3–4, pp. 197–387, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L. Shyu,
    S.-C. Chen, and S. S. Iyengar, “A survey on deep learning: Algorithms, techniques,
    and applications,” *ACM Computing Surveys (CSUR)*, vol. 51, no. 5, pp. 1–36, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S. Nasrin,
    M. Hasan, B. C. Van Essen, A. A. Awwal, and V. K. Asari, “A state-of-the-art survey
    on deep learning theory and architectures,” *Electronics*, vol. 8, no. 3, p. 292,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Dong, P. Wang, and K. Abbas, “A survey on deep learning and its applications,”
    *Computer Science Review*, vol. 40, p. 100379, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Dargan, M. Kumar, M. R. Ayyagari, and G. Kumar, “A survey of deep
    learning and its applications: a new paradigm to machine learning,” *Archives
    of Computational Methods in Engineering*, vol. 27, pp. 1071–1092, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D. E. Rumelhart, G. E. Hinton, R. J. Williams *et al.*, *Learning Internal
    Representations By Error Propagation*.   MIT Press, Cambridge, MA, 1985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] S. R. Dubey, S. K. Singh, and B. B. Chaudhuri, “Activation functions
    in deep learning: A comprehensive survey and benchmark,” *Neurocomputing*, vol.
    503, pp. 92–108, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] V. Nair and G. E. Hinton, “Rectified linear units improve restricted
    boltzmann machines,” in *Proc. International Conference on Machine Learning (ICML)*,
    pp. 807–814, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. L. Maas, A. Y. Hannun, A. Y. Ng *et al.*, “Rectifier nonlinearities
    improve neural network acoustic models,” in *Proc. International Conference on
    Machine Learning (ICML)*, vol. 30, no. 1, p. 3, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers: Surpassing
    human-level performance on imagenet classification,” in *Proc. IEEE International
    Conference on Computer Vision (ICCV)*, pp. 1026–1034, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance
    of initialization and momentum in deep learning,” in *Proc. International Conference
    on Machine Learning (ICML)*, pp. 1139–1147, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for
    online learning and stochastic optimization,” *Journal of Machine Learning Research*,
    vol. 12, no. Jul, pp. 2121–2159, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. D. Zeiler, “ADADELTA: an adaptive learning rate method,” *arXiv preprint
    arXiv:1212.5701*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] T. Dozat, “Incorporating nesterov momentum into Adam,” in *Proc. International
    Conference on Learning Representations (ICLR)*, pp. 2013–2016, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    *arXiv preprint arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] S. Ruder, “An overview of gradient descent optimization algorithms,”
    *arXiv preprint arXiv:1609.04747*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*.   MIT
    press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, pp.
    529–533, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Process. Mag.*, vol. 34,
    no. 6, pp. 26–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. E. Li, *Deep reinforcement learning*.   Springer, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker *et al.*, “Model-based
    reinforcement learning: A survey,” *Foundations and Trends® in Machine Learning*,
    vol. 16, no. 1, pp. 1–118, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] S. J. Pan and Q. Yang, “A survey on transfer learning,” *IEEE Trans.
    Knowl. Data Eng.*, vol. 22, no. 10, pp. 1345–1359, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,”
    *Journal of Big data*, vol. 3, pp. 1–40, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on
    deep transfer learning,” in *Proc. International Conference on Artificial Neural
    Networks (ICANN)*, pp. 270–279, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He,
    “A comprehensive survey on transfer learning,” *Proceedings of the IEEE*, vol.
    109, no. 1, pp. 43–76, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M. Wang, Y. Lin, Q. Tian, and G. Si, “Transfer learning promotes 6G wireless
    communications: Recent advances and future challenges,” *IEEE Trans. Rel.*, vol. 70,
    no. 2, pp. 790–807, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. Crawshaw, “Multi-task learning with deep neural networks: A survey,”
    *arXiv preprint arXiv:2009.09796*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Zhang and Q. Yang, “A survey on multi-task learning,” *IEEE Trans.
    Knowl. Data Eng.*, vol. 34, no. 12, pp. 5586–5609, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Kim, W. Lee, and D.-H. Cho, “A novel PAPR reduction scheme for OFDM
    system based on deep learning,” *IEEE Commun. Lett.*, vol. 22, no. 3, pp. 510–513,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Vanschoren, “Meta-learning: A survey,” *arXiv preprint arXiv:1810.03548*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, “Meta-learning
    in neural networks: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 44,
    no. 9, pp. 5149–5169, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Chen, S. T. Jose, I. Nikoloska, S. Park, T. Chen, O. Simeone *et al.*,
    “Learning with limited samples: Meta-learning and applications to communication
    systems,” *Foundations and Trends® in Signal Processing*, vol. 17, no. 2, pp.
    79–208, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *Proc. International Conference on Machine Learning (ICML)*, pp. 41–48, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] X. Wang, Y. Chen, and W. Zhu, “A survey on curriculum learning,” *IEEE
    Trans. Pattern Anal. Mach. Intell.*, vol. 44, no. 9, pp. 4555–4576, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] P. Soviany, R. T. Ionescu, P. Rota, and N. Sebe, “Curriculum learning:
    A survey,” *International Journal of Computer Vision*, vol. 130, no. 6, pp. 1526–1565,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] J. L. Elman, “Learning and development in neural networks: The importance
    of starting small,” *Cognition*, vol. 48, no. 1, pp. 71–99, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang,
    G. Wang, J. Cai *et al.*, “Recent advances in convolutional neural networks,”
    *Pattern recognition*, vol. 77, pp. 354–377, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional
    neural networks: Analysis, applications, and prospects,” *IEEE Trans. Neural Netw.
    Learn. Syst.*, vol. 33, no. 12, pp. 6999–7019, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] M. Krichen, “Convolutional neural networks: A survey,” *Computers*, vol. 12,
    no. 8, p. 151, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proc. Advances in Neural Information
    Processing Systems (NeurIPS)*, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proc. IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 1–9, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proc. IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, pp. 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proc. IEEE conference on computer vision and pattern
    recognition (CVPR)*, pp. 4700–4708, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction
    and classification of hyperspectral images based on convolutional neural networks,”
    *IEEE Trans. Geosci. Remote Sens.*, vol. 54, no. 10, pp. 6232–6251, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, “Deep learning-based
    channel estimation,” *IEEE Commun. Lett.*, vol. 23, no. 4, pp. 652–655, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] M. Honkala, D. Korpi, and J. M. Huttunen, “DeepRx: Fully convolutional
    deep learning receiver,” *IEEE Trans. Wireless Commun.*, vol. 20, no. 6, pp. 3925–3940,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Proc. Advances in Neural Information Processing Systems
    (NeurIPS)*, pp. 3104–3112, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual
    prediction with LSTM,” *Neural Computation*, vol. 12, pp. 2451–2471, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using RNN encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Y. Yu, X. Si, C. Hu, and J. Zhang, “A review of recurrent neural networks:
    LSTM cells and network architectures,” *Neural computation*, vol. 31, no. 7, pp.
    1235–1270, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee, “Recent
    advances in recurrent neural networks,” *arXiv preprint arXiv:1801.01078*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] G. Van Houdt, C. Mosquera, and G. Nápoles, “A review on the long short-term
    memory model,” *Artificial Intelligence Review*, vol. 53, pp. 5929–5955, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] N. Farsad and A. Goldsmith, “Neural network detection of data sequences
    in communication systems,” *IEEE Trans. Signal Process.*, vol. 66, no. 21, pp.
    5663–5678, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” *IEEE Trans. Neural Netw.*, vol. 20, no. 1,
    pp. 61–80, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” *arXiv preprint arXiv:1810.00826*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and
    M. Sun, “Graph neural networks: A review of methods and applications,” *AI Open*,
    vol. 1, pp. 57–81, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A comprehensive
    survey on graph neural networks,” *IEEE Trans. Neural Netw. Learn. Syst.*, vol. 32,
    no. 1, pp. 4–24, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” *arXiv preprint arXiv:1710.10903*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veličković, “Geometric deep
    learning: Grids, groups, graphs, geodesics, and gauges,” *arXiv preprint arXiv:2104.13478*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Proc. Advances in
    Neural Information Processing Systems (NeurIPS)*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Proc. Advances in neural information processing systems (NeurIPS)*, vol. 33,
    pp. 1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “LLaMA: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, “PaLM: Scaling language
    modeling with pathways,” *Journal of Machine Learning Research*, vol. 24, no.
    240, pp. 1–113, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth *et al.*, “Gemini: a family of highly capable
    multimodal models,” *arXiv preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] T. Lin, Y. Wang, X. Liu, and X. Qiu, “A survey of transformers,” *AI
    Open*, vol. 3, pp. 111–132, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, “Transformers
    in vision: A survey,” *ACM computing surveys (CSUR)*, vol. 54, no. 10s, pp. 1–41,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,
    C. Xu, Y. Xu *et al.*, “A survey on vision transformer,” *IEEE Trans. Pattern
    Anal. Mach. Intell.*, vol. 45, no. 1, pp. 87–110, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth $16\times
    16$ words: Transformers for image recognition at scale,” *arXiv preprint arXiv:2010.11929*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
    “Robust speech recognition via large-scale weak supervision,” in *Proc. International
    Conference on Machine Learning (ICML)*, pp. 28 492–28 518, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Wang, Z. Gao, D. Zheng, S. Chen, D. Gunduz, and H. V. Poor, “Transformer-empowered
    6G intelligent networks: From massive MIMO processing to semantic communication,”
    *IEEE Wireless Communications*, vol. 30, no. 6, pp. 127–135, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Z. Chang, G. A. Koulieris, and H. P. Shum, “On the design fundamentals
    of diffusion models: A survey,” *arXiv preprint arXiv:2306.04542*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui,
    and M.-H. Yang, “Diffusion models: A comprehensive survey of methods and applications,”
    *ACM Computing Surveys*, vol. 56, no. 4, pp. 1–39, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li, “A
    survey on generative diffusion models,” *IEEE Trans. Knowl. Data Eng.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Proc. Advances in Neural Information Processing Systems (NeurIPS)*, vol. 33,
    pp. 6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] B. Fesl, M. Baur, F. Strasser, M. Joham, and W. Utschick, “Diffusion-based
    generative prior for low-complexity MIMO channel estimation,” *arXiv preprint
    arXiv:2403.03545*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. Letafati, S. Ali, and M. Latva-aho, “Denoising diffusion probabilistic
    models for hardware-impaired communications,” *arXiv preprint arXiv:2309.08568*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] ——, “Probabilistic constellation shaping with denoising diffusion probabilistic
    models: A novel approach,” *arXiv preprint arXiv:2309.08688*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] M. Kim, R. Fritschek, and R. F. Schaefer, “Learning end-to-end channel
    coding with diffusion models,” in *Proc. 26th International ITG Workshop on Smart
    Antennas and 13th Conference on Systems, Communications, and Coding (WSA & SCC)*,
    pp. 1–6, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] T. Wu, Z. Chen, D. He, L. Qian, Y. Xu, M. Tao, and W. Zhang, “CDDM: Channel
    denoising diffusion models for wireless communications,” *IEEE Trans. Wireless
    Commun.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] H. Du, R. Zhang, Y. Liu, J. Wang, Y. Lin, Z. Li, D. Niyato, J. Kang,
    Z. Xiong, S. Cui *et al.*, “Beyond deep reinforcement learning: A tutorial on
    generative diffusion models in network optimization,” *arXiv preprint arXiv:2308.05384*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] L. Bariah, Q. Zhao, H. Zou, Y. Tian, F. Bader, and M. Debbah, “Large
    generative AI models for telecom: The next big thing?” *IEEE Commun. Mag.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] M. Letafati, S. Ali, and M. Latva-aho, “WiGenAI: The symphony of wireless
    and generative AI via diffusion models,” *arXiv preprint arXiv:2310.07312*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] N. Van Huynh, J. Wang, H. Du, D. T. Hoang, D. Niyato, D. N. Nguyen, D. I.
    Kim, and K. B. Letaief, “Generative AI for physical layer communications: A survey,”
    *arXiv preprint arXiv:2312.05594*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] C. Zhao, H. Du, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, K. B. Letaief
    *et al.*, “Generative AI for secure physical layer communications: A survey,”
    *arXiv preprint arXiv:2402.13553*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] M. Ebada, S. Cammerer, A. Elkelesh, and S. ten Brink, “Deep learning-based
    polar code design,” in *Proc. 57th Annual Allerton Conference on Communication,
    Control, and Computing (Allerton)*, pp. 177–183, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] L. Huang, H. Zhang, R. Li, Y. Ge, and J. Wang, “AI coding: Learning to
    construct error correction codes,” *IEEE Trans. Commun.*, vol. 68, no. 1, pp.
    26–39, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] M. Leonardon and V. Gripon, “Using deep neural networks to predict and
    improve the performance of polar codes,” in *Proc. 11th International Symposium
    on Topics in Coding (ISTC)*, pp. 1–5, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Y. Liao, S. A. Hashemi, H. Yang, and J. M. Cioffi, “Scalable polar code
    construction for successive cancellation list decoding: A graph neural network-based
    approach,” *IEEE Trans. Commun.*, vol. 71, no. 11, pp. 6231–6245, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] V. Miloslavskaya, Y. Li, and B. Vucetic, “Neural network based adaptive
    polar coding,” *IEEE Trans. Commun.*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Y. Li, Z. Chen, G. Liu, Y.-C. Wu, and K.-K. Wong, “Learning to construct
    nested polar codes: An attention-based set-to-element model,” *IEEE Commun. Lett.*,
    vol. 25, no. 12, pp. 3898–3902, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] S. K. Ankireddy, S. A. Hebbar, H. Wan, J. Cho, and C. Zhang, “Nested
    construction of polar codes via Transformers,” *arXiv preprint arXiv:2401.17188*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] S. A. Hebbar, S. K. Ankireddy, H. Kim, S. Oh, and P. Viswanath, “DeepPolar:
    Inventing nonlinear large-kernel polar codes via deep learning,” *arXiv preprint
    arXiv:2402.08864*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] S. K. Mishra, D. Katyal, and S. A. Ganapathi, “A modified Q-learning
    algorithm for rate-profiling of polarization adjusted convolutional (PAC) codes,”
    in *Proc. IEEE Wireless Communications and Networking Conference (WCNC)*, pp.
    2363–2368, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] T. J. Richardson, M. A. Shokrollahi, and R. L. Urbanke, “Design of capacity-approaching
    irregular low-density parity-check codes,” *IEEE Trans. Inf. Theory*, vol. 47,
    no. 2, pp. 619–637, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] R. Mori and T. Tanaka, “Performance of polar codes with the construction
    using density evolution,” *IEEE Commun. Lett.*, vol. 13, no. 7, pp. 519–521, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] S. ten Brink, “Convergence behavior of iteratively decoded parallel concatenated
    codes,” *IEEE Trans. Commun.*, vol. 49, no. 10, pp. 1727–1737, Oct. 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] E. Nisioti and N. Thomos, “Design of capacity-approaching low-density
    parity-check codes using recurrent neural networks,” *arXiv preprint arXiv:2001.01249*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] I. Tal and A. Vardy, “How to construct polar codes,” *IEEE Trans. Inf.
    Theory*, vol. 59, no. 10, pp. 6562–6582, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] P. Trifonov, “Efficient design and decoding of polar codes,” *IEEE Trans.
    Commun.*, vol. 60, no. 11, pp. 3221–3227, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] H. Ochiai, P. Mitran, and H. V. Poor, “Capacity-approaching polar codes
    with long codewords and successive cancellation decoding based on improved gaussian
    approximation,” *IEEE Trans. Commun.*, vol. 69, no. 1, pp. 31–43, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] H. Ochiai, K. Ikeya, and P. Mitran, “A new polar code design based on
    reciprocal channel approximation,” *IEEE Trans. Commun.*, vol. 71, no. 2, pp.
    631–643, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] I. Tal and A. Vardy, “List decoding of polar codes,” *IEEE Trans. Inf.
    Theory*, vol. 61, no. 5, pp. 2213–2226, May 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] L. Huang, H. Zhang, R. Li, Y. Ge, and J. Wang, “Reinforcement learning
    for nested polar code construction,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Y. Liao, S. A. Hashemi, J. M. Cioffi, and A. Goldsmith, “Construction
    of polar codes with reinforcement learning,” *IEEE Trans. Commun.*, vol. 70, no. 1,
    pp. 185–198, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” *arXiv preprint arXiv:1706.06083*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] V. Miloslavskaya, Y. Li, and B. Vucetic, “Design of compactly specified
    polar codes with dynamic frozen bits based on reinforcement learning,” *IEEE Trans.
    Commun.*, vol. 72, no. 3, pp. 1257–1272, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] C. Schürch, “A partial order for the synthesized channels of a polar
    code,” in *Proc. IEEE International Symposium on Information Theory (ISIT)*, pp.
    220–224, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] M. Bardet, V. Dragoi, A. Otmani, and J.-P. Tillich, “Algebraic properties
    of polar codes from a new polynomial formalism,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 230–234, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] G. He, J.-C. Belfiore, I. Land, G. Yang, X. Liu, Y. Chen, R. Li, J. Wang,
    Y. Ge, R. Zhang *et al.*, “Beta-expansion: A theoretical framework for fast and
    recursive construction of polar codes,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] M. Mondelli, S. H. Hassani, and R. Urbanke, “Construction of polar codes
    with sublinear complexity,” *IEEE Trans. Inf. Theory*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] V. Bioglio, C. Condo, and I. Land, “Design of polar codes in 5G new radio,”
    *IEEE Communications Surveys & Tutorials*, vol. 23, no. 1, pp. 29–40, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] V. Konda and J. Tsitsiklis, “Actor-critic algorithms,” *Proc. Advances
    in Neural Information Processing Systems (NeurIPS)*, vol. 12, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] S. B. Korada, E. Şaşoğlu, and R. Urbanke, “Polar codes: Characterization
    of exponent, bounds, and constructions,” *IEEE Trans. Inf. Theory*, vol. 56, no. 12,
    pp. 6253–6264, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] E. Arıkan, “From sequential decoding to channel polarization and back
    again,” *arXiv preprint arXiv:1908.09594*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] M. Moradi and A. Mozammel, “A monte-carlo based construction of polarization-adjusted
    convolutional (PAC) codes,” *arXiv preprint arXiv:2106.08118*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] T. Gruber, S. Cammerer, J. Hoydis, and S. ten Brink, “On deep learning-based
    channel decoding,” in *Proc. 51st Annual Conference on Information Sciences and
    Systems (CISS)*, pp. 1–6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] J. Seo, J. Lee, and K. Kim, “Decoding of polar code by using deep feed-forward
    neural networks,” in *Proc. International Conference on Computing, Networking
    and Communications (ICNC)*, pp. 238–242, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] C. T. Leung, R. V. Bhat, and M. Motani, “Low-latency neural decoders
    for linear and non-linear block codes,” in *Proc. IEEE Global Communications Conference
    (GLOBECOM)*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] ——, “Low latency energy-efficient neural decoders for block codes,” *IEEE
    Trans. Green Commun. Netw.*, vol. 7, no. 2, pp. 680–691, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] W. Lyu, Z. Zhang, C. Jiao, K. Qin, and H. Zhang, “Performance evaluation
    of channel decoding with deep neural networks,” in *Proc. IEEE International Conference
    on Communications (ICC)*, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] R. Sattiraju, A. Weinand, and H. D. Schotten, “Performance analysis of
    deep learning based on recurrent neural networks for channel coding,” in *Proc.
    IEEE International Conference on Advanced Networks and Telecommunications Systems
    (ANTS)*, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] H. Zhu, Z. Cao, Y. Zhao, and D. Li, “Learning to denoise and decode:
    A novel residual neural network decoder for polar codes,” *IEEE Trans. Veh. Technol.*,
    vol. 69, no. 8, pp. 8725–8738, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Y. Choukroun and L. Wolf, “Error correction code transformer,” *Proc.
    Advances in Neural Information Processing Systems (NeurIPS)*, vol. 35, pp. 38 695–38 705,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] ——, “Denoising diffusion error correction codes,” *arXiv preprint arXiv:2209.13533*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] A. Bennatan, Y. Choukroun, and P. Kisilev, “Deep learning for decoding
    of linear codes-a syndrome-based approach,” in *Proc. IEEE International Symposium
    on Information Theory (ISIT)*, pp. 1595–1599, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] J. K. S. Kamassury and D. Silva, “Iterative error decimation for syndrome-based
    neural network decoders,” *arXiv preprint arXiv:2012.00089*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] D. Artemasov, K. Andreev, P. Rybin, and A. Frolov, “Soft-output deep
    neural network-based decoding,” *arXiv preprint arXiv:2304.08868*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Y. Wang, Z. Zhang, S. Zhang, S. Cao, and S. Xu, “A unified deep learning
    based polar-ldpc decoder for 5G communication systems,” in *Proc. 10th International
    Conference on Wireless Communications and Signal Processing (WCSP)*, pp. 1–6,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Y. Jiang, H. Kim, H. Asnani, and S. Kannan, “Mind: Model independent
    neural decoder,” in *Proc. IEEE International Workshop on Signal Processing Advances
    in Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] H. Lee, E. Y. Seo, H. Ju, and S.-H. Kim, “On training neural network
    decoders of rate compatible polar codes via transfer learning,” *Entropy*, vol. 22,
    no. 5, p. 496, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] D. Artemasov, K. Andreev, and A. Frolov, “On a unified deep neural network
    decoding architecture,” in *Proc. IEEE 98th Vehicular Technology Conference (VTC2023-Fall)*,
    pp. 1–5, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] F. Carpi, C. Häger, M. Martalò, R. Raheli, and H. D. Pfister, “Reinforcement
    learning for channel coding: Learned bit-flipping decoding,” in *Proc. 57th Annual
    Allerton Conference on Communication, Control, and Computing (Allerton)*, pp.
    922–929, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] J. Gao and K. Niu, “A reinforcement learning based decoding method of
    short polar codes,” in *Proc. IEEE Wireless Communications and Networking Conference
    Workshops (WCNCW)*, pp. 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] E. Kavvousanos, V. Paliouras, and I. Kouretas, “Simplified deep-learning-based
    decoders for linear block codes,” in *Proc. IEEE International Conference on Electronics,
    Circuits and Systems (ICECS)*, pp. 769–772, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] E. Kavvousanos and V. Paliouras, “Hardware implementation aspects of
    a syndrome-based neural network decoder for BCH codes,” in *Proc. IEEE Nordic
    Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium
    of System-on-Chip (SoC)*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] ——, “Optimizing deep learning decoders for FPGA implementation,” in *Proc.
    31st International Conference on Field-Programmable Logic and Applications (FPL)*,
    pp. 271–272, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] ——, “A low-latency syndrome-based deep learning decoder architecture
    and its FPGA implementation,” in *Proc. 11th International Conference on Modern
    Circuits and Systems Technologies (MOCAST)*, pp. 1–4, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] B. Cavarec, H. B. Celebi, M. Bengtsson, and M. Skoglund, “A learning-based
    approach to address complexity-reliability tradeoff in OS decoders,” in *Proc.
    54th Asilomar Conference on Signals, Systems, and Computers*, pp. 689–692, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] N. Raviv, A. Caciularu, T. Raviv, J. Goldberger, and Y. Be’ery, “perm2vec:
    Graph permutation selection for decoding of error correction codes using self-attention,”
    *IEEE J. Sel. Areas Commun.*, vol. 39, no. 1, pp. 79–88, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] A. Kurmukova and D. Gunduz, “Friendly attacks to improve channel coding
    reliability,” *arXiv preprint arXiv:2401.14184*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] A. Tsvieli and N. Weinberger, “Learning maximum margin channel decoders,”
    *IEEE Trans. Inf. Theory*, vol. 69, no. 6, pp. 3597–3626, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] X. Zhong, K. Cai, Z. Mei, and T. Q. Quek, “Deep learning-based decoding
    of linear block codes for spin-torque transfer magnetic random access memory,”
    *IEEE Transactions on Magnetics*, vol. 57, no. 2, pp. 1–5, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] X. Zhong, K. Cai, P. Kang, G. Song, and B. Dai, “Deep learning-based
    adaptive error-correction decoding for spin-torque transfer magnetic random access
    memory (STT-MRAM),” *IEEE Transactions on Magnetics*, vol. 59, no. 11, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] C. T. Leung, R. V. Bhat, and M. Motani, “Multi-label neural decoders
    for block codes,” in *Proc. IEEE International Conference on Communications (ICC)*,
    pp. 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] H. Kim, Y. Jiang, R. Rana, S. Kannan, S. Oh, and P. Viswanath, “Communication
    algorithms via deep learning,” in *Proc. International Conference on Learning
    Representations (ICLR)*, pp. 1–17, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Z. Cao, H. Zhu, Y. Zhao, and D. Li, “Learning to denoise and decode:
    A novel residual neural network decoder for polar codes,” in *Proc. IEEE 92nd
    Vehicular Technology Conference (VTC2020-Fall)*, pp. 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] S.-J. Park, H.-Y. Kwak, S.-H. Kim, S. Kim, Y. Kim, and J.-S. No, “How
    to mask in error correction code Transformer: Systematic and double masking,”
    *arXiv preprint arXiv:2308.08128*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Y. Choukroun and L. Wolf, “A foundation model for error correction codes,”
    in *Proc. International Conference on Machine Learning (ICML)*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill *et al.*, “On the opportunities
    and risks of foundation models,” *arXiv preprint arXiv:2108.07258*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] F. J. MacWilliams and N. J. A. Sloane, *The Theory of Error-Correcting
    Codes*.   Elsevier, 1977, vol. 16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] I. Dimnik and Y. Be’ery, “Improved random redundant iterative HDPC decoding,”
    *IEEE Trans. Commun.*, vol. 57, no. 7, pp. 1982–1985, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] L. Tallini and P. Cull, “Neural nets for decoding error-correcting codes,”
    in *IEEE Technical applications conference and workshops*, p. 89, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] R. M. Pyndiah, “Near-optimum decoding of product codes: Block turbo codes,”
    *IEEE Trans. Commun.*, vol. 46, no. 8, pp. 1003–1010, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] M. Zhu and S. Gupta, “To prune, or not to prune: exploring the efficacy
    of pruning for model compression,” *arXiv preprint arXiv:1710.01878*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] M. P. C. Fossorier and S. Lin, “Soft-decision decoding of linear block
    codes based on ordered statistics,” *IEEE Trans. Inf. Theory*, vol. 41, no. 5,
    pp. 1379–1396, Sep. 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] A. Elkelesh, M. Ebada, S. Cammerer, and S. ten Brink, “Belief propagation
    list decoding of polar codes,” *IEEE Commun. Lett.*, vol. 22, no. 8, pp. 1536–1539,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] A. Kurakin, I. J. Goodfellow, and S. Bengio, *Adversarial examples in
    the physical world*.   Chapman and Hall/CRC, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] E. Nachmani, Y. Be’ery, and D. Burshtein, “Learning to decode linear
    codes using deep learning,” in *Proc. 54th Annual Allerton Conference on Communication,
    Control, and Computing (Allerton)*, pp. 341–346, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] E. Nachmani, E. Marciano, L. Lugosch, W. J. Gross, D. Burshtein, and
    Y. Be’ery, “Deep learning methods for improved decoding of linear codes,” *IEEE
    J. Sel. Topics Signal Process.*, vol. 12, no. 1, pp. 119–131, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] L. Lugosch and W. J. Gross, “Neural offset min-sum decoding,” in *Proc.
    IEEE International Symposium on Information Theory (ISIT)*, pp. 1361–1365, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] B. Dai, R. Liu, and Z. Yan, “New min-sum decoders based on deep learning
    for polar codes,” in *Proc. IEEE International Workshop on Signal Processing Systems
    (SiPS)*, pp. 252–257, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] H. Yu, M.-M. Zhao, M. Lei, and M.-J. Zhao, “Neural adjusted min-sum decoding
    for LDPC codes,” in *Proc. IEEE 98th Vehicular Technology Conference (VTC2023-Fall)*,
    pp. 1–5, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Y.-L. Hsu, L.-W. Liu, Y.-C. Liao, and H.-C. Chang, “GC-Like LDPC code
    construction and its NN-aided decoder implementation,” *IEEE Open J. Circuits
    Syst.*, vol. 5, pp. 189–198, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] X. Wu, M. Jiang, and C. Zhao, “Decoding optimization for 5G LDPC codes
    by machine learning,” *IEEE Access*, vol. 6, pp. 50 179–50 186, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] T. Kim and J. Park, “Neural self-corrected min-sum decoder for NR LDPC
    codes,” *IEEE Commun. Lett.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] C.-F. Teng, K.-S. Ho, C.-H. Wu, S.-S. Wong, and A.-Y. Wu, “Convolutional
    neural network-aided bit-flipping for belief propagation decoding of polar codes,”
    *arXiv preprint arXiv:1911.01704*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C.-F. Teng and A.-Y. A. Wu, “Convolutional neural network-aided tree-based
    bit-flipping framework for polar decoder using imitation learning,” *IEEE Trans.
    Signal Process.*, vol. 69, pp. 300–313, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] C.-F. Teng, A. K.-S. Ho, C.-H. D. Wu, S.-S. Wong, and A.-Y. A. Wu, “Convolutional
    neural network-aided bit-flipping for belief propagation decoding of polar codes,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 7898–7902, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Y. Sun, Y. Shen, W. Song, Z. Gong, X. You, and C. Zhang, “LSTM network-assisted
    belief propagation flip polar decoder,” in *Proc. Asilomar Conference on Signals,
    Systems, and Computers*, pp. 979–983, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] B. Liu, Y. Xie, and J. Yuan, “A deep learning assisted node-classified
    redundant decoding algorithm for BCH codes,” *IEEE Trans. Commun.*, pp. 1–1, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Y. Wei, M.-M. Zhao, M.-J. Zhao, and M. Lei, “ADMM-based decoder for binary
    linear codes aided by deep learning,” *IEEE Commun. Lett.*, vol. 24, no. 5, pp.
    1028–1032, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] T. Wadayama and S. Takabe, “Deep learning-aided trainable projected gradient
    decoding for LDPC codes,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2444–2448, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] ——, “Proximal decoding for LDPC codes,” *IEICE Transactions on Fundamentals
    of Electronics, Communications and Computer Sciences*, vol. 106, no. 3, pp. 359–367,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] N. Doan, S. A. Hashemi, and W. J. Gross, “Decoding polar codes with reinforcement
    learning,” in *Proc. IEEE Global Communications Conference (GLOBECOM)*, pp. 1–6,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] S. Habib, A. Beemer, and J. Kliewer, “Learned scheduling of LDPC decoders
    based on multi-armed bandits,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2789–2794, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] ——, “Belief propagation decoding of short graph-based channel codes via
    reinforcement learning,” *IEEE J. Sel. Inf. Theory*, vol. 2, no. 2, pp. 627–640,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] ——, “Reldec: Reinforcement learning-based decoding of moderate length
    LDPC codes,” *IEEE Trans. Commun.*, vol. 71, no. 19, pp. 5661–5674, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] L. Lugosch and W. J. Gross, “Learning from the syndrome,” in *Proc. Asilomar
    Conference on Signals, Systems, and Computers*, pp. 594–598, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] C.-F. Teng and Y.-L. Chen, “Syndrome-enabled unsupervised learning for
    neural network-based polar decoder and jointly optimized blind equalizer,” *IEEE
    J. Emerg. Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 177–188, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] E. Nachmani and Y. Be’ery, “Neural decoding with optimization of node
    activations,” *IEEE Commun. Lett.*, vol. 26, no. 11, pp. 2527–2531, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] C.-F. Teng, C.-H. D. Wu, A. K.-S. Ho, and A.-Y. A. Wu, “Low-complexity
    recurrent neural network-based polar decoder with weight quantization mechanism,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 1413–1417, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Z. Ibrahim and Y. Fahmy, “Enhanced learning for recurrent neural network-based
    polar decoder,” in *Proc. International Conference on Electrical Engineering (ICEENG)*,
    pp. 105–109, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] X. Xiao, B. Vasic, R. Tandon, and S. Lin, “Finite alphabet iterative
    decoding of LDPC codes with coarsely quantized neural networks,” in *Proc. IEEE
    Global Communications Conference (GLOBECOM)*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] X. Xiao, B. Vasić, R. Tandon, and S. Lin, “Designing finite alphabet
    iterative decoders of LDPC codes via recurrent quantized neural networks,” *IEEE
    Trans. Commun.*, vol. 68, no. 7, pp. 3963–3974, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Y. Lyu, M. Jiang, Y. Zhang, C. Zhao, N. Hu, and X. Xu, “Optimized non-surjective
    FAIDs for 5G LDPC codes with learnable quantization,” *IEEE Commun. Lett.*, vol. 28,
    no. 2, pp. 253–257, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] M. Lian, C. Häger, and H. D. Pfister, “What can machine learning teach
    us about communications?” in *Proc. IEEE Information Theory Workshop (ITW)*, pp.
    1–5, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] M. Lian, F. Carpi, C. Häger, and H. D. Pfister, “Learned belief-propagation
    decoding with simple scaling and SNR adaptation,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 161–165, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Q. Wang, S. Wang, H. Fang, L. Chen, L. Chen, and Y. Guo, “A model-driven
    deep learning method for normalized min-sum LDPC decoding,” in *Proc. IEEE International
    Conference on Communications Workshops (ICC Workshops)*, pp. 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Q. Wang, Q. Liu, S. Wang, L. Chen, H. Fang, L. Chen, Y. Guo, and Z. Wu,
    “Normalized min-sum neural network for LDPC decoding,” *IEEE Trans. Cogn. Commun.
    Netw.*, vol. 9, no. 1, pp. 70–81, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] J. Dai, K. Tan, Z. Si, K. Niu, M. Chen, H. V. Poor, and S. Cui, “Learning
    to decode protograph LDPC codes,” *IEEE J. Sel. Areas Commun.*, vol. 39, no. 7,
    pp. 1983–1999, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Y. Liang, C.-T. Lam, and B. K. Ng, “A low-complexity neural normalized
    min-sum LDPC decoding algorithm using tensor-train decomposition,” *IEEE Commun.
    Lett.*, vol. 26, no. 12, pp. 2914–2918, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] ——, “Joint-way compression for LDPC neural decoding algorithm with tensor-ring
    decomposition,” *IEEE Access*, vol. 11, pp. 22 871–22 879, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Y. Cheng, W. Chen, L. Li, and B. Ai, “Rate compatible LDPC neural decoding
    network: A multi-task learning approach,” *IEEE Trans. Veh. Technol.*, vol. 73,
    no. 5, pp. 7374–7378, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] A. Buchberger, C. Häger, H. D. Pfister, L. Schmalen, and A. G. i Amat,
    “Pruning and quantizing neural belief propagation decoders,” *IEEE J. Sel. Areas
    Commun.*, vol. 39, no. 7, pp. 1957–1966, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] ——, “Learned decimation for neural belief propagation decoders,” in *Proc.
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp. 8273–8277, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] V. G. Satorras and M. Welling, “Neural enhanced belief propagation on
    factor graphs,” in *Proc. International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, pp. 685–693, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] S. Cammerer, J. Hoydis, F. A. Aoudia, and A. Keller, “Graph neural networks
    for channel decoding,” in *Proc. IEEE Global Communications Conference Workshops
    (GC Wkshps)*, pp. 486–491, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] K. Tian, C. Yue, C. She, Y. Li, and B. Vucetic, “A scalable graph neural
    network decoder for short block codes,” in *Proc. IEEE International Conference
    on Communications*, pp. 1268–1273, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] S. K. Ankireddy and H. Kim, “Interpreting neural min-sum decoders,” in
    *Proc. IEEE International Conference on Communications (ICC)*, pp. 6645–6651,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] S. Adiga, X. Xiao, R. Tandon, B. Vasić, and T. Bose, “Generalization
    bounds for neural belief propagation decoders,” *IEEE Trans. Inf. Theory*, vol. 70,
    no. 6, pp. 4280–4296, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] J. Clausius, M. Geiselhart, D. Tandler, and S. t. Brink, “Graph neural
    network-based joint equalization and decoding,” *arXiv preprint arXiv:2401.16187*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] R. Wiesmayr, C. Dick, J. Hoydis, and C. Studer, “DUIDD: Deep-unfolded
    interleaved detection and decoding for MIMO wireless systems,” in *Proc. 56th
    Asilomar Conference on Signals, Systems, and Computers*, pp. 181–188, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] H. Lee, Y.-S. Kil, M. Y. Chung, and S.-H. Kim, “Neural network aided
    impulsive perturbation decoding for short raptor-like LDPC codes,” *IEEE Commun.
    Lett.*, vol. 11, no. 2, pp. 268–272, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Y. Wang, S. Zhang, C. Zhang, X. Chen, and S. Xu, “A low-complexity belief
    propagation based decoding scheme for polar codes-decodability detection and early
    stopping prediction,” *IEEE Access*, vol. 7, pp. 159 808–159 820, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] M. P. Fossorier, M. Mihaljevic, and H. Imai, “Reduced complexity iterative
    decoding of low-density parity check codes based on belief propagation,” *IEEE
    Trans. Commun.*, vol. 47, no. 5, pp. 673–680, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] J. Chen, A. Dholakia, E. Eleftheriou, M. P. Fossorier, and X.-Y. Hu,
    “Reduced-complexity decoding of LDPC codes,” *IEEE Trans. Commun.*, vol. 53, no. 8,
    pp. 1288–1299, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] John R. Hershey, Jonathan Le Roux, and Felix Weninger, “Deep unfolding:
    Model-based inspiration of novel deep architectures,” *arXiv preprint arXiv:1409.2574*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] N. Shlezinger, J. Whang, Y. C. Eldar, and A. G. Dimakis, “Model-based
    deep learning,” *Proceedings of the IEEE*, vol. 11, no. 5, pp. 465–499, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] A. Jagannath, J. Jagannath, and T. Melodia, “Redefining wireless communication
    for 6G: Signal processing meets deep learning with deep unfolding,” *IEEE Trans.
    Artif. Intell.*, vol. 2, no. 6, pp. 528–536, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] E. Nachmani, E. Marciano, D. Burshtein, and Y. Be’ery, “RNN decoding
    of linear block codes,” *arXiv preprint arXiv:1702.07560*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] N. Shah and Y. Vasavada, “Neural layered decoding of 5G LDPC codes,”
    *IEEE Commun. Lett.*, vol. 25, no. 11, pp. 3590–3593, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] T. Richardson, S. Kudekar, and L. Vincent, “Adjusted mim-sum decoder,”
    *Patent*, vol. 20, no. 180, p. 109, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] A. Darabiha, A. C. Carusone, and F. R. Kschischang, “A bit-serial approximate
    min-sum LDPC decoder and FPGA implementation,” in *Proc. IEEE International Symposium
    on Circuits and Systems (ISCAS)*, pp. 4–pp, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] F. Angarita, J. Valls, V. Almenar, and V. Torres, “Reduced-complexity
    min-sum algorithm for decoding LDPC codes with low error-floor,” *IEEE Trans.
    Circuits Syst. I*, vol. 61, no. 7, pp. 2150–2158, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] V. Savin, “Self-corrected min-sum decoding of LDPC codes,” in *Proc.
    IEEE International Symposium on Information Theory (ISIT)*, pp. 146–150, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] W. Xu, Z. Wu, Y.-L. Ueng, X. You, and C. Zhang, “Improved polar decoder
    based on deep learning,” in *Proc. IEEE International workshop on signal processing
    systems (SiPS)*, pp. 1–6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] W. Xu, X. You, C. Zhang, and Y. Be’ery, “Polar decoding on sparse graphs
    with deep learning,” in *Proc. Asilomar Conference on Signals, Systems, and Computers*,
    pp. 599–603, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] S. Cammerer, M. Ebada, A. Elkelesh, and S. ten Brink, “Sparse graphs
    for belief propagation decoding of polar codes,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 1465–1469, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] N. Doan, S. A. Hashemi, E. N. Mambou, T. Tonnellier, and W. J. Gross,
    “Neural belief propagation decoding of CRC-polar concatenated codes,” in *Proc.
    IEEE International Conference on Communications (ICC)*, pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] W. Xu, X. Tan, Y. Be’ery, Y.-L. Ueng, Y. Huang, X. You, and C. Zhang,
    “Deep learning-aided belief propagation decoder for polar codes,” *IEEE J. Emerg.
    Sel. Top. Circuits Syst.*, vol. 10, no. 2, pp. 189–203, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Y. Yu, Z. Pan, N. Liu, and X. You, “Belief propagation bit-flip decoder
    for polar codes,” *IEEE Access*, vol. 7, pp. 10 937–10 946, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] E. Nachmani and L. Wolf, “Hyper-graph-network decoders for block codes,”
    in *Proc. Advances in Neural Information Processing Systems (NeurIPS)*, pp. 2329–2339,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, “A brief
    review of hypernetworks in deep learning,” *arXiv preprint arXiv:2306.06955*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] E. Nachmani and L. Wolf, “A gated hypernet decoder for polar codes,”
    in *Proc. IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pp. 5210–5214, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] ——, “Autoregressive belief propagation for decoding block codes,” *arXiv
    preprint arXiv:2103.11780*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] M. Benammar and P. Piantanida, “Optimal training channel statistics for
    neural-based decoders,” in *Proc. 52nd Asilomar Conference on Signals, Systems,
    and Computers*, pp. 2157–2161, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] I. Be’Ery, N. Raviv, T. Raviv, and Y. Be’Ery, “Active deep decoding of
    linear codes,” *IEEE Trans. Commun.*, vol. 68, no. 2, pp. 728–736, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] B. Settles, “Active learning literature survey,” *Technical Report*,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] T. Raviv, N. Raviv, and Y. Be’ery, “Data-driven ensembles for deep and
    hard-decision hybrid decoding,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 321–326, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] L. Rokach, “Ensemble-based classifiers,” *Artificial Intelligence Review*,
    vol. 33, pp. 1–39, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] T. Raviv, A. Goldmann, O. Vayner, Y. Be’ery, and N. Shlezinger, “CRC-aided
    learned ensembles of belief-propagation polar decoders,” *arXiv preprint arXiv:2301.06060*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] T. Raviv, A. Goldman, O. Vayner, Y. Be’ery, and N. Shlezinger, “CRC-aided
    learned ensembles of belief-propagation polar decoders,” in *Proc. IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pp. 8856–8860,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] H.-Y. Kwak, D.-Y. Yun, Y. Kim, S.-H. Kim, and J.-S. No, “Boosting learning
    for LDPC codes to improve the error-floor performance,” *arXiv preprint arXiv:2310.07194*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
    on-line learning and an application to boosting,” *Journal of Computer and System
    Sciences*, vol. 55, no. 1, pp. 119–139, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] T. R. Halford and K. M. Chugg, “Random redundant soft-in soft-out decoding
    of linear block codes,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 2230–2234, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] T. Hehn, J. B. Huber, S. Laendner, and O. Milenkovic, “Multiple-bases
    belief-propagation for decoding of short block codes,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 311–315, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] J. Feldman, “Decoding error-correcting codes via linear programming,”
    Ph.D. dissertation, Massachusetts Institute of Technology, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] E. J. Candes and T. Tao, “Decoding by linear programming,” *IEEE Trans.
    Inf. Theory*, vol. 51, no. 12, pp. 4203–4215, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] S. Barman, X. Liu, S. C. Draper, and B. Recht, “Decomposition methods
    for large scale LP decoding,” *IEEE Trans. Inf. Theory*, vol. 59, no. 12, pp.
    7870–7886, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] N. Parikh, S. Boyd *et al.*, “Proximal algorithms,” *Foundations and
    trends® in Optimization*, vol. 1, no. 3, pp. 127–239, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] A. Elkelesh, M. Ebada, S. Cammerer, and S. ten Brink, “Belief propagation
    decoding of polar codes on permuted factor graphs,” in *Proc. IEEE Wireless Communications
    and Networking Conference (WCNC)*, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] N. Doan, S. A. Hashemi, M. Mondelli, and W. J. Gross, “On the decoding
    of polar codes on permuted factor graphs,” in *Proc. IEEE Global Communications
    Conference (GLOBECOM)*, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] N. Hussami, S. B. Korada, and R. Urbanke, “Performance of polar codes
    for channel and source coding,” in *Proc. IEEE International Symposium on Information
    Theory (ISIT)*, pp. 1488–1492, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] S. Habib and D. G. Mitchell, “Reinforcement learning for sequential decoding
    of generalized LDPC codes,” in *Proc. 12th International Symposium on Topics in
    Coding (ISTC)*, pp. 1–5, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] N. C. Thompson, K. Greenewald, K. Lee, and G. F. Manso, “The computational
    limits of deep learning,” *arXiv preprint arXiv:2007.05558*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] E. D. Karnin, “A simple procedure for pruning back-propagation trained
    neural networks,” *IEEE Trans. Neural Netw.*, vol. 1, no. 2, pp. 239–242, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep
    neural networks with pruning, trained quantization and huffman coding,” *arXiv
    preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value
    of network pruning,” *arXiv preprint arXiv:1810.05270*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, “What is
    the state of neural network pruning?” *Proceedings of Machine Learning and Systems*,
    vol. 2, pp. 129–146, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, “Sparsity
    in deep learning: Pruning and growth for efficient inference and training in neural
    networks,” *Journal of Machine Learning Research*, vol. 22, no. 241, pp. 1–124,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen,
    and T. Blankevoort, “A white paper on neural network quantization,” *arXiv preprint
    arXiv:2106.08295*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A
    survey of quantization methods for efficient neural network inference,” in *Low-Power
    Computer Vision*.   Chapman and Hall/CRC, 2022, pp. 291–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] T. Wadayama and S. Takabe, “Joint quantizer optimization based on neural
    quantizer for sum-product decoder,” *arXiv preprint arXiv:1804.06002*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] M. Geiselhart, A. Elkelesh, J. Clausius, F. Liang, W. Xu, J. Liang, and
    S. Ten Brink, “Learning quantization in LDPC decoders,” in *Proc. IEEE Global
    Communications Conference Workshops (GC Wkshps)*, pp. 467–472, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] J. Gao, K. Niu, and C. Dong, “Learning to decode polar codes with one-bit
    quantizer,” *IEEE Access*, vol. 8, pp. 27 210–27 217, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] B. Vasić, X. Xiao, and S. Lin, “Learning to decode LDPC codes with finite-alphabet
    message passing,” in *Proc. Information Theory and Applications Workshop (ITA)*,
    pp. 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] Y. Bengio, N. Léonard, and A. Courville, “Estimating or propagating gradients
    through stochastic neurons for conditional computation,” *arXiv preprint arXiv:1308.3432*,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] L. Wang, X. Dong, Y. Wang, L. Liu, W. An, and Y. Guo, “Learnable lookup
    table for neural network quantization,” in *Proc. IEEE/CVF conference on computer
    vision and pattern recognition (CVPR)*, pp. 12 423–12 433, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] T. T. Nguyen-Ly, K. Le, V. Savin, D. Declercq, F. Ghaffari, and O. Boncalo,
    “Non-surjective finite alphabet iterative decoders,” in *Proc. IEEE International
    Conference on Communications (ICC)*, pp. 1–6, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] L. Wang, S. Chen, J. Nguyen, D. Dariush, and R. Wesel, “Neural-network-optimized
    degree-specific weights for LDPC minsum decoding,” *arXiv preprint arXiv:2107.04221*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] L. Wang, C. Terrill, D. Divsalar, and R. Wesel, “LDPC decoding with degree-specific
    neural message weights and RCQ decoding,” *IEEE Trans. Commun.*, vol. 72, no. 4,
    pp. 1912–1924, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[362] I. V. Oseledets, “Tensor-train decomposition,” *SIAM Journal on Scientific
    Computing*, vol. 33, no. 5, pp. 2295–2317, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[363] Q. Zhao, G. Zhou, S. Xie, L. Zhang, and A. Cichocki, “Tensor ring decomposition,”
    *arXiv preprint arXiv:1606.05535*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[364] T. Filler and J. Fridrich, “Binary quantization using belief propagation
    with decimation over factor graphs of LDGM codes,” *arXiv preprint arXiv:0710.0192*,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] M. Mohri, A. Rostamizadeh, and A. Talwalkar, *Foundations of Machine
    Learning*.   MIT press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] H. Lee, Y.-S. Kil, M. Jang, S.-H. Kim, O.-S. Park, and G. Park, “Multi-round
    belief propagation decoding with impulsive perturbation for short LDPC codes,”
    *IEEE Wireless Commun. Lett.*, vol. 9, no. 9, pp. 1491–1494, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] D. Xiao, J. Lu, C. Lin, and B. Jiao, “A perturbation method for decoding
    LDPC concatenated with CRC,” in *Proc. IEEE Wireless Communications and Networking
    Conference (WCNC)*, pp. 667–671, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] X. Han, R. Liu, Y. Li, C. Yi, J. He, and M. Wang, “Accelerating neural
    BP-based decoder using coded distributed computing,” *IEEE Trans. Veh. Technol.*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[369] S. Li, M. A. Maddah-Ali, Q. Yu, and A. S. Avestimehr, “A fundamental
    tradeoff between computation and communication in distributed computing,” *IEEE
    Trans. Inf. Theory*, vol. 64, no. 1, pp. 109–128, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[370] E. Chen, D. Apalkov, Z. Diao, A. Driskill-Smith, D. Druist, D. Lottis,
    V. Nikitin, X. Tang, S. Watts, S. Wang *et al.*, “Advances and future prospects
    of spin-transfer torque random access memory,” *IEEE Transactions on Magnetics*,
    vol. 46, no. 6, pp. 1873–1878, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[371] H. Chen, K. Zhang, X. Ma, and B. Bai, “Comparisons between reliability-based
    iterative min-sum and majority-logic decoding algorithms for LDPC codes,” *IEEE
    Trans. Commun.*, vol. 59, no. 7, pp. 1766–1771, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[372] Y.-H. Liu and D. Poulin, “Neural belief-propagation decoders for quantum
    error-correcting codes,” *Physical Review Letters*, vol. 122, no. 20, p. 200501,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[373] D. Poulin and Y. Chung, “On the iterative decoding of sparse quantum
    codes,” *arXiv preprint arXiv:0801.1241*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[374] A. Gong, S. Cammerer, and J. M. Renes, “Graph neural networkrs for enhanced
    decoding of quantum LDPC codes,” *arXiv preprint arXiv:2310.17758*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[375] S. Miao, A. Schnerring, H. Li, and L. Schmalen, “Neural belief propagation
    decoding of quantum LDPC codes using overcomplete check matrices,” in *Proc. IEEE
    Information Theory Workshop (ITW)*, pp. 215–220, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[376] S. Cammerer, T. Gruber, J. Hoydis, and S. ten Brink, “Scaling deep learning-based
    decoding of polar codes via partitioning,” in *Proc. IEEE Global Communications
    Conference (GLOBECOM)*, pp. 1–6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[377] J. Gao and R. Liu, “Neural network aided SC decoder for polar codes,”
    in *Proc. IEEE 4th International Conference on Computer and Communications (ICCC)*,
    pp. 2153–2157, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[378] N. Doan, S. A. Hashemi, and W. J. Gross, “Neural successive cancellation
    decoding of polar codes,” in *Proc. IEEE International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC)*, pp. 1–5, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[379] I. Wodiany and A. Pop, “Low-precision neural network decoding of polar
    codes,” in *Proc. IEEE International Workshop on Signal Processing Advances in
    Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[380] S. A. Hebbar, V. V. Nadkarni, A. V. Makkuva, S. Bhat, S. Oh, and P. Viswanath,
    “CRISP: Curriculum based sequential neural decoders for polar code family,” in
    *Proc. International Conference on Machine Learning (ICML)*, pp. 12 823–12 845,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[381] X. Wang, H. Zhang, R. Li, L. Huang, S. Dai, Y. Huangfu, and J. Wang,
    “Learning to flip successive cancellation decoding of polar codes with lstm networks,”
    in *Proc. IEEE International Symposium on Personal, Indoor and Mobile Radio Communications
    (PIMRC)*, pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[382] B. He, S. Wu, Y. Deng, H. Yin, J. Jiao, and Q. Zhang, “A machine learning
    based multi-flips successive cancellation decoding scheme of polar codes,” in
    *Proc. IEEE Vehicular Technology Conference (VTC2020-Spring)*, pp. 1–5, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[383] N. Doan, S. A. Hashemi, F. Ercan, T. Tonnellier, and W. J. Gross, “Neural
    dynamic successive cancellation flip decoding of polar codes,” in *Proc. IEEE
    International Workshop on Signal Processing Systems (SiPS)*, pp. 272–277, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[384] ——, “Neural successive cancellation flip decoding of polar codes,” *Journal
    of Signal Processing Systems*, vol. 93, pp. 631–642, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[385] X. Wang, J. He, J. Li, and L. Shan, “Reinforcement learning for bit-flipping
    decoding of polar codes,” *Entropy*, vol. 23, no. 2, p. 171, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[386] N. Doan, S. A. Hashemi, F. Ercan, and W. J. Gross, “Fast SC-Flip decoding
    of polar codes with reinforcement learning,” in *Proc. IEEE International Conference
    on Communications (ICC)*, pp. 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[387] S. A. Hashemi, N. Doan, T. Tonnellier, and W. J. Gross, “Deep-learning-aided
    successive-cancellation decoding of polar codes,” in *Proc. Asilomar Conference
    on Signals, Systems, and Computers*, pp. 532–536, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[388] N. Doan, S. A. Hashemi, and W. J. Gross, “Fast successive-cancellation
    list flip decoding of polar codes,” *IEEE Access*, vol. 10, pp. 5568–5584, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[389] C.-H. Chen, C.-F. Teng, and A.-Y. A. Wu, “Low-complexity LSTM-assisted
    bit-flipping algorithm for successive cancellation list polar decoder,” in *Proc.
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    pp. 1708–1712, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[390] Y. Tao and Z. Zhang, “DNC-aided SCL-flip decoding of polar codes,” in
    *Proc. IEEE Global Communications Conference (GLOBECOM)*, pp. 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[391] F.-S. Liang, S. Lu, and Y.-L. Ueng, “Deep-learning-aided successive cancellation
    list flip decoding for polar codes,” *IEEE Trans. Cogn. Commun. Netw.*, vol. 10,
    no. 2, pp. 374–386, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[392] J. Li, L. Zhou, Z. Li, W. Gao, R. Ji, J. Zhu, and Z. Liu, “Deep learning-assisted
    adaptive dynamic-SCLF decoding of polar codes,” *IEEE Trans. Cogn. Commun. Netw.*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[393] Y. Lu, M. Zhao, M. Lei, C. Wang, and M. Zhao, “Deep learning aided SCL
    decoding of polar codes with shifted-pruning,” *China Communications*, vol. 20,
    no. 1, pp. 153–170, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[394] X. Liu, S. Wu, Y. Wang, N. Zhang, J. Jiao, and Q. Zhang, “Exploiting
    error-correction-CRC for polar SCL decoding: A deep learning based approach,”
    *IEEE Trans. Cogn. Commun. Netw.*, vol. 6, no. 2, pp. 817–828, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[395] G. Sarkis, P. Giard, A. Vardy, C. Thibeault, and W. J. Gross, “Fast list
    decoders for polar codes,” *IEEE J. Sel. Areas Commun.*, vol. 34, no. 2, pp. 318–328,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[396] O. Afisiadis, A. Balatsoukas-Stimming, and A. Burg, “A low-complexity
    improved successive cancellation decoder for polar codes,” in *Proc. 48th Asilomar
    Conf. Signals, Systems and Computers (ACSSC)*, pp. 2116–2120, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[397] L. Chandesris, V. Savin, and D. Declercq, “Dynamic-SCFlip decoding of
    polar codes,” *IEEE Trans. Commun.*, vol. 66, no. 6, pp. 2333–2345, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[398] F. Ercan, T. Tonnellier, N. Doan, and W. J. Gross, “Practical dynamic
    SC-flip polar decoders: Algorithm and implementation,” *IEEE Trans. Signal Process.*,
    vol. 68, pp. 5441–5456, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[399] A. Alamdar-Yazdi and F. R. Kschischang, “A simplified successive-cancellation
    decoder for polar codes,” *IEEE Commun. Lett.*, vol. 15, no. 12, pp. 1378–1380,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[400] S. A. Hashemi, C. Condo, and W. J. Gross, “A fast polar code list decoder
    architecture based on sphere decoding,” *IEEE Trans. Circuits Syst. I*, vol. 63,
    no. 12, pp. 2368–2380, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[401] ——, “Fast and flexible successive-cancellation list decoders for polar
    codes,” *IEEE Trans. Signal Process.*, vol. 65, no. 21, pp. 5756–5769, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[402] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwińska,
    S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou *et al.*, “Hybrid computing
    using a neural network with dynamic external memory,” *Nature*, vol. 538, no.
    7626, pp. 471–476, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[403] M. Rowshan and E. Viterbo, “Improved list decoding of polar codes by
    shifted-pruning,” in *Proc. IEEE Information Theory Workshop (ITW)*, pp. 1–5,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[404] W. G. Teich, R. Liu, and V. Belagiannis, “Deep learning versus high-order
    recurrent neural network based decoding for convolutional codes,” in *Proc. IEEE
    Global Communications Conference (GLOBECOM)*, pp. 1–7, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[405] Y. Jiang, S. Kannan, H. Kim, S. Oh, H. Asnani, and P. Viswanath, “DeepTurbo:
    Deep turbo decoder,” in *Proc. IEEE International Workshop on Signal Processing
    Advances in Wireless Communications (SPAWC)*, pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[406] Y. He, J. Zhang, S. Jin, C.-K. Wen, and G. Y. Li, “Model-driven DNN decoder
    for turbo codes: Design, simulation, and experimental results,” *IEEE Trans. Commun.*,
    vol. 68, no. 10, pp. 6127–6140, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[407] S. A. Hebbar, R. K. Mishra, S. K. Ankireddy, A. V. Makkuva, H. Kim, and
    P. Viswanath, “TinyTurbo: Efficient turbo decoders on edge,” in *Proc. IEEE International
    Symposium on Information Theory (ISIT)*, pp. 2797–2802, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[408] A. Viterbi, “Error bounds for convolutional codes and an asymptotically
    optimum decoding algorithm,” *IEEE Trans. Inf. Theory*, vol. 13, no. 2, pp. 260–269,
    1967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[409] C. Cardinal, D. Haccoun, F. Gagnon, and N. Baatani, “Turbo decoding using
    convolutional self doubly orthogonal codes,” in *Proc. IEEE International Conference
    on Communications (ICC)*, vol. 1, pp. 113–117, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[410] M. Mostafa, W. G. Teich, and J. Lindner, “Analysis of high order recurrent
    neural networks for analog decoding,” in *Proc. International Symposium on Turbo
    Codes and Iterative Information Processing (ISTC)*, pp. 116–120, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[411] L. Bahl, J. Cocke, F. Jelinek, and J. Raviv, “Optimal decoding of linear
    codes for minimizing symbol error rate,” *IEEE Trans. Inf. Theory*, vol. IT-20,
    pp. 284–287, Mar. 1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[412] X. Chen and M. Ye, “Cyclically equivariant neural decoders for cyclic
    codes,” in *Proc. International conference on machine learning (ICML)*, pp. 1771–1780,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[413] ——, “Improving the list decoding version of the cyclically equivariant
    neural decoder,” in *Proc. IEEE International Symposium on Information Theory
    (ISIT)*, pp. 2344–2349, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[414] H. Pirayesh and H. Zeng, “Jamming attacks and anti-jamming strategies
    in wireless networks: A comprehensive survey,” *IEEE Communications Surveys &
    Tutorials*, vol. 24, no. 2, pp. 767–809, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[415] D. Adesina, C.-C. Hsieh, Y. E. Sagduyu, and L. Qian, “Adversarial machine
    learning in wireless communications using RF data: A review,” *IEEE Communications
    Surveys & Tutorials*, vol. 25, no. 1, pp. 77–100, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[416] M. Sadeghi and E. G. Larsson, “Physical adversarial attacks against end-to-end
    autoencoder communication systems,” *IEEE Commun. Lett.*, vol. 23, no. 5, pp.
    847–850, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[417] J. Chen, J. Ge, S. Zheng, L. Ye, H. Zheng, W. Shen, K. Yue, and X. Yang,
    “AIR: Threats of adversarial attacks on deep learning-based information recovery,”
    *IEEE Trans. Wireless Commun.*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[418] P. J. Phillips, C. A. Hahn, P. C. Fontana, D. A. Broniatowski, and M. A.
    Przybocki, “Four principles of explainable artificial intelligence (draft),” *NIST
    Interagency/Internal Report (NISTIR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[419] R. R. Hoffman, S. T. Mueller, G. Klein, and J. Litman, “Metrics for explainable
    AI: Challenges and prospects,” *arXiv preprint arXiv:1812.04608*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[420] A. Adadi and M. Berrada, “Peeking inside the black-box: A survey on explainable
    artificial intelligence (XAI),” *IEEE Access*, vol. 6, pp. 52 138–52 160, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[421] F. Xu, H. Uszkoreit, Y. Du, W. Fan, D. Zhao, and J. Zhu, “Explainable
    AI: A brief survey on history, research areas, approaches and challenges,” in
    *Proc. International Conference on Natural Language Processing and Chinese Computing*,
    pp. 563–574, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[422] P. Linardatos, V. Papastefanopoulos, and S. Kotsiantis, “Explainable
    AI: A review of machine learning interpretability methods,” *Entropy*, vol. 23,
    no. 1, p. 18, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[423] A. B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik,
    A. Barbado, S. García, S. Gil-López, D. Molina, R. Benjamins *et al.*, “Explainable
    artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
    toward responsible AI,” *Information Fusion*, vol. 58, pp. 82–115, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[424] D. Minh, H. X. Wang, Y. F. Li, and T. N. Nguyen, “Explainable artificial
    intelligence: A comprehensive review,” *Artificial Intelligence Review*, pp. 1–66,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[425] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,
    Z. Wen, T. Shah, G. Morgan *et al.*, “Explainable AI (XAI): Core ideas, techniques,
    and solutions,” *ACM Computing Surveys*, vol. 55, no. 9, pp. 1–33, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[426] D. Gunning and D. Aha, “DARPA’s explainable artificial intelligence (XAI)
    program,” *AI magazine*, vol. 40, no. 2, pp. 44–58, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[427] W. Guo, “Explainable artificial intelligence for 6G: Improving trust
    between human and machine,” *IEEE Commun. Mag.*, vol. 58, no. 6, pp. 39–45, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[428] S. Wang, M. A. Qureshi, L. Miralles-Pechuan, T. Huynh-The, T. R. Gadekallu,
    and M. Liyanage, “Applications of explainable AI for 6G: Technical aspects, use
    cases, and research challenges,” *arXiv preprint arXiv:2112.04698*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[429] S. Wang, M. A. Qureshi, L. Miralles-Pechuán, T. Huynh-The, T. R. Gadekallu,
    and M. Liyanage, “Explainable AI for 6G use cases: Technical aspects and research
    challenges,” *IEEE Open J. Commun. Soc.*, vol. 5, pp. 2490–2540, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[430] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green AI,” *Communications
    of the ACM*, vol. 63, no. 12, pp. 54–63, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[431] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations
    for deep learning in NLP,” *arXiv preprint arXiv:1906.02243*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[432] D. Patterson, J. Gonzalez, U. Hölzle, Q. Le, C. Liang, L.-M. Munguia,
    D. Rothchild, D. R. So, M. Texier, and J. Dean, “The carbon footprint of machine
    learning training will plateau, then shrink,” *Computer*, vol. 55, no. 7, pp.
    18–28, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[433] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang,
    F. Aga, J. Huang, C. Bai *et al.*, “Sustainable AI: Environmental implications,
    challenges and opportunities,” *Proceedings of Machine Learning and Systems*,
    vol. 4, pp. 795–813, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[434] S. Salehi and A. Schmeink, “Data-centric green artificial intelligence:
    A survey,” *IEEE Trans. Artif. Intell.*, vol. 5, no. 5, pp. 1973–1989, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[435] G. Menghani, “Efficient deep learning: A survey on making deep learning
    models smaller, faster, and better,” *ACM Computing Surveys*, vol. 55, no. 12,
    pp. 1–37, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[436] M. H. Jarrahi, A. Memariani, and S. Guha, “The principles of data-centric
    AI (DCAI),” *arXiv preprint arXiv:2211.14611*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[437] D. Zha, Z. P. Bhat, K.-H. Lai, F. Yang, and X. Hu, “Data-centric AI:
    Perspectives and challenges,” in *Proc. SIAM International Conference on Data
    Mining (SDM)*, pp. 945–948, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[438] D. Zha, Z. P. Bhat, K.-H. Lai, F. Yang, Z. Jiang, S. Zhong, and X. Hu,
    “Data-centric artificial intelligence: A survey,” *arXiv preprint arXiv:2303.10158*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[439] P. Botsinis, D. Alanis, Z. Babar, H. Nguyen, D. Chandra, S. X. Ng, and
    L. Hanzo, “Quantum algorithms for wireless communications,” *IEEE Communications
    Surveys & Tutorials*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[440] T. Matsumine, T. Koike-Akino, and Y. Wang, “Channel decoding with quantum
    approximate optimization algorithm,” in *Proc. IEEE International Symposium on
    Information Theory (ISIT)*, pp. 2574–2578, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[441] J. Cui, Y. Xiong, S. X. Ng, and L. Hanzo, “Quantum approximate optimization
    algorithm based maximum likelihood detection,” *IEEE Trans. Commun.*, vol. 70,
    no. 8, pp. 5386–5400, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[442] K. Yukiyoshi and N. Ishikawa, “Quantum search algorithm for binary constant
    weight codes,” *arXiv preprint arXiv:2211.04637*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[443] M. Kim, S. Kasi, P. A. Lott, D. Venturelli, J. Kaewell, and K. Jamieson,
    “Heuristic quantum optimization for 6G wireless communications,” *IEEE Network*,
    vol. 35, no. 4, pp. 8–15, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[444] S. Kasi, J. Kaewelh, and K. Jamieson, “A quantum annealer-enabled decoder
    and hardware topology for nextg wireless polar codes,” *IEEE Trans. Wireless Commun.*,
    vol. 23, no. 4, pp. 3780–3794, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[445] J. Preskill, “Quantum computing in the NISQ era and beyond,” *Quantum*,
    vol. 2, p. 79, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[446] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J. Love,
    A. Aspuru-Guzik, and J. L. O’brien, “A variational eigenvalue solver on a photonic
    quantum processor,” *Nature Communications*, vol. 5, no. 1, p. 4213, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[447] E. Farhi, J. Goldstone, and S. Gutmann, “A quantum approximate optimization
    algorithm,” *arXiv preprint arXiv:1411.4028*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[448] K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand,
    M. Degroote, H. Heimonen, J. S. Kottmann, T. Menke *et al.*, “Noisy intermediate-scale
    quantum algorithms,” *Reviews of Modern Physics*, vol. 94, no. 1, p. 015004, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[449] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii,
    J. R. McClean, K. Mitarai, X. Yuan, L. Cincio *et al.*, “Variational quantum algorithms,”
    *Nature Reviews Physics*, vol. 3, no. 9, pp. 625–644, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[450] M. Schuld, I. Sinayskiy, and F. Petruccione, “An introduction to quantum
    machine learning,” *Contemporary Physics*, vol. 56, no. 2, pp. 172–185, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[451] A. Perdomo-Ortiz, M. Benedetti, J. Realpe-Gómez, and R. Biswas, “Opportunities
    and challenges for quantum-assisted machine learning in near-term quantum computers,”
    *Quantum Science and Technology*, vol. 3, no. 3, p. 030502, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[452] S. J. Nawaz, S. K. Sharma, S. Wyne, M. N. Patwary, and M. Asaduzzaman,
    “Quantum machine learning for 6G communication networks: State-of-the-art and
    vision for the future,” *IEEE Access*, vol. 7, pp. 46 317–46 350, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[453] H.-Y. Huang, M. Broughton, M. Mohseni, R. Babbush, S. Boixo, H. Neven,
    and J. R. McClean, “Power of data in quantum machine learning,” *Nature Communications*,
    vol. 12, no. 1, p. 2631, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[454] O. Simeone *et al.*, “An introduction to quantum machine learning for
    engineers,” *Foundations and Trends® in Signal Processing*, vol. 16, no. 1-2,
    pp. 1–223, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[455] B. Narottama, Z. Mohamed, and S. Aïssa, “Quantum machine learning for
    next-g wireless communications: Fundamentals and the path ahead,” *IEEE Open J.
    Commun. Soc.*, vol. 4, pp. 2204–2224, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[456] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner,
    “The power of quantum neural networks,” *Nature Computational Science*, vol. 1,
    no. 6, pp. 403–409, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[457] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, “Quanvolutional neural
    networks: Powering image recognition with quantum circuits,” *Quantum Machine
    Intelligence*, vol. 2, no. 1, p. 2, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[458] J. Romero, J. P. Olson, and A. Aspuru-Guzik, “Quantum autoencoders for
    efficient compression of quantum data,” *Quantum Science and Technology*, vol. 2,
    no. 4, p. 045001, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[459] P.-L. Dallaire-Demers and N. Killoran, “Quantum generative adversarial
    networks,” *Physical Review A*, vol. 98, no. 1, p. 012324, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[460] S. Lloyd and C. Weedbrook, “Quantum generative adversarial learning,”
    *Physical Review Letters*, vol. 121, no. 4, p. 040502, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
