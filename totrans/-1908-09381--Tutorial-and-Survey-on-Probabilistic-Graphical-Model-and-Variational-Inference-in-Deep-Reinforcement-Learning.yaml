- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:05:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1908.09381] Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1908.09381](https://ar5iv.labs.arxiv.org/html/1908.09381)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xudong Sun Department of Statistics
  prefs: []
  type: TYPE_NORMAL
- en: Ludwig Maximillian University of Munich
  prefs: []
  type: TYPE_NORMAL
- en: Munich, Germany
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: xudong.sun@stat.uni-muenchen.de    Bernd Bischl Department of Statistics'
  prefs: []
  type: TYPE_NORMAL
- en: Ludwig Maximillian University of Munich
  prefs: []
  type: TYPE_NORMAL
- en: Munich, Germany
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Aiming at a comprehensive and concise tutorial survey, recap of variational
    inference and reinforcement learning with Probabilistic Graphical Models are given
    with detailed derivations. Reviews and comparisons on recent advances in deep
    reinforcement learning are made from various aspects. We offer detailed derivations
    to a taxonomy of Probabilistic Graphical Model and Variational Inference methods
    in deep reinforcement learning, which serves as a complementary material on top
    of the original contributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Probabilistic Graphical Models; Variational Inference; Deep Reinforcement Learning
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the recent successes of Reinforcement Learning, powered by Deep Neural
    Networks, in complicated tasks like games [[1](#bib.bib1)] and robot locomotion
    [[2](#bib.bib2)], as well as optimization tasks like Automatic Machine Learning
    [[3](#bib.bib3)]. The field still faces many challenges including expressing high
    dimensional state and policy, exploration in sparse reward, etc. Probabilistic
    Graphical Model and Variational Inference offers a great tool to express a wide
    spectrum of trajectory distributions as well as conducting inference which can
    serve as a control method. Due to the emerging popularity, we present a comprehensive
    and concise tutorial survey paper with the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide Probabilistic Graphical Models for many basic concepts of Reinforcement
    Learning, which is rarely covered in literature. We also provide Probabilistic
    Graphical Models to some recent works on Deep Reinforcement Learning [[4](#bib.bib4),
    [5](#bib.bib5)] which does not exist in the original contributions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover a taxonomy of Probabilistic Graphical Model and Variational Inference
    [[6](#bib.bib6)] methods used in Deep Reinforcement Learning and give detailed
    derivations to many of the critical equations, which is not given in the original
    contributions. Together with the recap of variational inference and deep reinforcement
    learning, the paper serves as a self-inclusive tutorial to both beginner and advanced
    readers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: I-A Organization of the paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In section [I-B](#S1.SS2 "I-B Prerequisite on Probabilistic Graphical Models
    and Variational Inference, Terminologies and Conventions ‣ I Introduction ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning"), we first introduce the fundamentals of Probabilistic
    Graphical Models and Variational Inference, then we review the basics about reinforcement
    learning by connecting probabilistic graphical models (PGM) in section [II-A](#S2.SS1
    "II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), [II-B](#S2.SS2
    "II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), [II-C](#S2.SS3
    "II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"), as well as an overview about deep
    reinforcement learning, accompanied with a comparison of different methods in
    section [II-D](#S2.SS4 "II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"). In
    section [III-A](#S3.SS1 "III-A Policy and value function with undirected graphs
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), we discuss how undirected graph could be used in modeling both the
    value function and the policy, which works well on high dimensional discrete state
    and action spaces. In section [III-B](#S3.SS2 "III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), we introduce the directed acyclic graph framework
    on how to treat the policy as posterior on actions, while adding many proofs that
    does not exist in the original contributions. In section [III-C](#S3.SS3 "III-C
    Variational Inference on the Environment ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), we introduce works
    on how to use variational inference to approximate the environment model, while
    adding graphical models and proofs which does not exist in the original contributions.
  prefs: []
  type: TYPE_NORMAL
- en: I-B Prerequisite on Probabilistic Graphical Models and Variational Inference,
    Terminologies and Conventions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use capital letter to denote a Random Variable (RV), while using the lower
    case letter to represent the realization. To avoid symbol collision of using $A$
    to represent advantage in many RL literature, we use $A^{act}$ explicitly to represent
    action. We use $(B\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}C)\mid
    A$ to represent $B$ is conditionally independent from $C$, given $A$, or equivalently
    $p(B|A,C)=p(B|A)$ or $p(BC|A)=P(B|A)P(C|A)$. Directed Acyclic Graphs (DAG) [[7](#bib.bib7)]
    as a PGM offers an instinctive way of defining factorized joint distributions
    of RV by assuming the conditional independence [[7](#bib.bib7)] through d-separation
    [[7](#bib.bib7)]. Undirected Graph including Markov Random Fields also specifies
    the conditional independence with local Markov property and global Markov property
    [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Variational Inference (VI) approximates intractable posterior distribution $p(z\mid
    x)=\frac{1}{\int_{z^{{}^{\prime}}}p(z^{{}^{\prime}})p(x|z^{{}^{\prime}})dz^{{}^{\prime}}}p(z)p(x\mid
    z)$ with latent variable $z$ specified in a probabilistic graphical model, by
    a variational proposal posterior distribution $q_{\phi}(z\mid x)$, characterized
    by variational parameter $\phi$. By optimizing the Evidence Lower Bound (ELBO)
    [[6](#bib.bib6)], VI assigns the values to observed and latent variables at the
    same time. VI is widely used in Deep Learning Community like variational resampling
    [[9](#bib.bib9)]. VI is also used in approximating the posterior on the weights
    distribution of neural networks for Thompson Sampling to tackle the exploration-exploitation
    trade off in bandit problems [[10](#bib.bib10)], as well as approximating on the
    activations distribution like Variational AutoEncoder [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: As a contribution of this paper, we summarize the relationship of evidence $\log
    p(x)$, KL divergence $D_{KL}$, cross entropy $H_{q}(p)$, entropy $H(q)$, free
    energy $F(\phi,\theta)$ and $ELBO(\phi,\theta)$ in Equation ([1](#S1.E1 "In I-B
    Prerequisite on Probabilistic Graphical Models and Variational Inference, Terminologies
    and Conventions ‣ I Introduction ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\log p(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{q}\left[\log p(x\mid z)\right]-E_{q}\left[\log\frac{q_{\phi}{(z\mid
    x)}}{p(z)}\right]+E_{q}\left[\log\frac{q_{\phi}{(z\mid x)}}{p(z\mid x)}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q_{\phi}(z&#124;x)&#124;&#124;p(x,z))-E_{q}\left[\log
    p(z&#124;x)\right]+$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle E_{q}\log q_{\phi}(z&#124;x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-F(\phi,\theta)+H_{q}(p)-H(q)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle ELBO(\phi,\theta)+D_{KL}(q_{\phi}(z\mid
    x)&#124;&#124;p(z\mid x))$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: II Reinforcement Learning and Deep Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Basics about Reinforcement Learning with graphical model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.6]tikz_rl_def.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Concept of Reinforcement Learning'
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 RL Concepts, Terminology and Convention
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Figure [1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), Reinforcement Learning (RL) involves optimizing
    the behavior of an agent via interaction with the environment. At time $t$, the
    agent lives on state $S_{t}$, By executing an action $a_{t}$ according to a policy
    [[12](#bib.bib12)] $\pi(a|S_{t})$, the agent jumps to another state $S_{t+1}$,
    while receiving a reward $R_{t}$. Let discount factor $\gamma$ decides how much
    the immediate reward is favored compared to longer term return, with which one
    could also allow tractability in infinite horizon reinforcement learning [[12](#bib.bib12)],
    as well as reducing variance in Monte Carlo setting [[13](#bib.bib13)]. The goal
    is to maximize the accumulated rewards, $G=\sum_{t=0}^{T}\gamma^{t}R_{t}$ which
    is usually termed return in RL literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we interchangeably use two conventions whenever convenient:
    Suppose an episode last from $t=0:T$, with $T\rightarrow\infty$ correspond to
    continuous non-episodic reinforcement learning. We use another convention of $t\in\{0,\cdots,\infty\}$
    by assuming when episode ends, the agent stays at a self absorbing state with
    a null action, while receiving null reward.'
  prefs: []
  type: TYPE_NORMAL
- en: By unrolling Figure [1](#S2.F1 "Figure 1 ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"), we get a sequence of state, action and reward
    tuples $\{(S_{t},A^{act}_{t},R_{t})\}$ in an episode, which is coined trajectory
    $\tau$ [[14](#bib.bib14)]. Figure [2](#S2.F2 "Figure 2 ‣ II-A1 RL Concepts, Terminology
    and Convention ‣ II-A Basics about Reinforcement Learning with graphical model
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning") illustrates part of a trajectory in one rollout. The state space $\mathcal{S}$
    and action space $\mathcal{A}$, which can be either discrete or continuous and
    multi-dimensional, are each represented with one continuous dimension in Figure
    [2](#S2.F2 "Figure 2 ‣ II-A1 RL Concepts, Terminology and Convention ‣ II-A Basics
    about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning") and plotted in
    an orthogonal way with different colors, while we use the thickness of the plate
    to represent the reward space $\mathcal{R}$.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.6]tikz_drl_illustrator.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Illustration of State, Action and Reward Trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 DAGs for (Partially Observed ) Markov Decision Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement Learning is a stochastic decision process, which usually comes
    with three folds of uncertainty. That is, under a particular stochastic policy
    characterized by $\pi(a|s)=p(a|s)$, within a particular environment characterized
    by state transition probability $p(s_{t+1}|s_{t},a)$ and reward distribution function
    $p(r_{t}|s_{t},a_{t})$, a learning agent could observe different trajectories
    with different unrolling realizations. This is usually modeled as a Markov Decision
    Process [[12](#bib.bib12)], with its graphical model shown in Figure [3](#S2.F3
    "Figure 3 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A
    Basics about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where we could
    define a joint probability distribution over the trajectory of state , action
    and reward RVs. In Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs for (Partially Observed
    ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), we use dashed arrows connecting state and action to represent the
    policy, upon fixed policy $\pi$, we have the trajectory likelihood in Equation
    ([2](#S2.E2 "In II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣
    II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"))
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(\tau)=p(s_{0})\prod_{t=0}^{T}p(s_{t+1}&#124;s_{t},a_{t})p(r_{t}&#124;s_{t},a_{t})\pi(a_{t}&#124;s_{t})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Upon observation of a state $s_{t}$ in Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs
    for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"), the action at the time step in question
    is conditionally independent with the state and action history $\mathcal{E}_{t}=\{S_{0},A^{act}_{0},\cdots,S_{t-1}\}$,
    which could be denoted as $(A^{act}_{t}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}\mathcal{E}_{t})\mid
    S_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.7]tikz_pgm_mdp.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Directed Acyclic Graph For Markov Decision Process'
  prefs: []
  type: TYPE_NORMAL
- en: A more realistic model, however, is the Partially Observable Markov Decision
    process [[15](#bib.bib15)], with its DAG representation shown in Figure [4](#S2.F4
    "Figure 4 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process ‣ II-A
    Basics about Reinforcement Learning with graphical model ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where the agent
    could only observe the state partially by observing $O_{t}$ through a non invertible
    function of the next state $S_{t+1}$ and the action $a_{t}$, as indicated the
    Figure by $p(o_{t}|s_{t+1},a_{t})$, while the distributions on other edges are
    omitted since they are the same as in Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs
    for (Partially Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement
    Learning with graphical model ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning"). Under the graph specification of Figure
    [4](#S2.F4 "Figure 4 ‣ II-A2 DAGs for (Partially Observed ) Markov Decision Process
    ‣ II-A Basics about Reinforcement Learning with graphical model ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), the
    observable $O_{t}$ is no longer Markov, but depends on the whole history, however,
    the latent state $S_{t}$ is still Markov. For POMDP, belief state $b_{t}$ is defined
    at time $t$, which is associated with a probability distribution $b_{t}(s_{t})$
    over the hidden state $S_{t}$, with $\sum_{\mathcal{S}}b(S_{t})=1$, where state
    $S$ takes value in latent state space $\mathcal{S}$ [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.8]tikz_pgm_pomdp.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Probabilistic Graphical Model for POMDP'
  prefs: []
  type: TYPE_NORMAL
- en: The latent state distribution associated with belief state can be updated in
    a Bayesian way in Equation ([6](#S2.E6 "In II-A2 DAGs for (Partially Observed
    ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning with graphical
    model ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and
    Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle b_{t+1}(s_{t+1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(s_{t+1}\mid o_{t},a_{t},b_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1},o_{t},a_{t},b_{t})}{p(o_{t},a_{t},b_{t})}\frac{p(s_{t+1},a_{t},b_{t})}{p(s_{t+1},a_{t},b_{t})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid a_{t},s_{t+1},b_{t})\frac{p(s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t},s_{t+1}\mid
    a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t},b_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (5)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(o_{t}\mid s_{t+1},a_{t})\frac{\sum_{{s_{t}}}p(s_{t+1}\mid
    s_{t},a_{t})p(s_{t}\mid a_{t},b_{t})}{p(o_{t}\mid a_{t},b_{t})}$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: II-B Value Function, Bellman Equation, Policy Iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define state value function of state $s\in\mathcal{S}$ in Equation ([7](#S2.E7
    "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), where
    the corresponding Bellman Equation is derived in Equation ([8](#S2.E8 "In II-B
    Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle V^{\pi}(s)=E_{\pi,\varepsilon}[\sum_{i=0}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})\mid\forall
    S_{t}=s]$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i=1}^{\infty}\gamma^{(i-1)}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma\sum_{i^{{}^{\prime}}=0}^{\infty}\gamma^{i^{{}^{\prime}}}R_{t+1+i^{{}^{\prime}}}(S_{t+1+i^{{}^{\prime}}},A^{act}_{t+1+i^{{}^{\prime}}})]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t},A^{act}_{t})+\gamma
    V^{\pi}(S_{t+1})]$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $S_{t+i}\sim p(s_{t+i+1}|s_{t+i},a_{t+i})$ takes value from $\mathcal{S}$,
    $A^{act}_{t+i}\sim\pi(a|S_{t+i+1})$ taking value from $\mathcal{A}$, and we have
    used the $\pi$ and $\varepsilon$ in the subscript of the expectation $E$ operation
    to represent the probability distribution of the policy and the environment (including
    transition probability and reward probability) respectively. State action value
    function [[12](#bib.bib12)] is defined in Equation ([9](#S2.E9 "In II-B Value
    Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), where in Equation
    ([10](#S2.E10 "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II
    Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on
    Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), its relationship to the state value function is stated.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle Q^{\pi}(s,a)\,(\forall S_{t}=s,A^{act}_{t}=a)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\pi,\varepsilon}[R_{t}(S_{t}=s,A^{act}_{t}=a)+\gamma
    V^{\pi}(S_{t+1})]$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: Combining Equation ([8](#S2.E8 "In II-B Value Function, Bellman Equation, Policy
    Iteration ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and Equation ([9](#S2.E9 "In II-B Value Function, Bellman
    Equation, Policy Iteration ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V(s)=\sum_{a}\pi(a&#124;s)Q(s,a)$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: Define optimal policy [[12](#bib.bib12)] to be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi^{*}$ | $\displaystyle=\underset{\pi}{arg\,max}\,V^{\pi}(s),\forall
    s\in S$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\underset{\pi}{arg\,max}\,E_{\pi}[R_{t}+\gamma V^{\pi}(S_{t+1})]$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: Taking the optimal policy $\pi^{*}$ into the Bellman Equation in Equation ([8](#S2.E8
    "In II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), we
    have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle V^{\pi^{*}}(s)=E_{\pi^{*},\varepsilon}\left[R_{t}(s,A^{act}_{t})+\gamma
    V^{\pi^{*}}(S_{t+1})\right]$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: Taking the optimal policy $\pi^{*}$ into Equation ([9](#S2.E9 "In II-B Value
    Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\pi^{*},\varepsilon}[R_{t}(s,a)+\sum_{i=1}^{\infty}\gamma^{i}R_{t+i}(S_{t+i},A^{act}_{t+i})]$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: Based on Equation ([14](#S2.E14 "In II-B Value Function, Bellman Equation, Policy
    Iteration ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and Equation ([13](#S2.E13 "In II-B Value Function,
    Bellman Equation, Policy Iteration ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we get
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle V^{\pi^{*}}(s)=\underset{a}{max}\,Q^{\pi^{*}}(s,a)$ |  |
    (15) |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q^{\pi^{*}}(s,a)=E_{\varepsilon,\pi^{*}}\left[R_{t}(s,a)+\gamma\underset{\bar{a}}{max}\,Q^{\pi^{*}}(S_{t+1},\bar{a})\right]$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: For learning the optimal policy and value function, General Policy Iteration
    [[12](#bib.bib12)] can be conducted, as shown in Figure [5](#S2.F5 "Figure 5 ‣
    II-B Value Function, Bellman Equation, Policy Iteration ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where a contracting
    process [[12](#bib.bib12)] is drawn. Starting from initial policy $\pi_{0}$, the
    corresponding value function $V^{\pi_{0}}$ could be estimated, which could result
    in improved policy $\pi_{1}$ by greedy maximization over actions. The contracting
    process is supposed to converge to the optimal policy $\pi^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: As theoretically fundamentals of learning algorithms, Dynamic programming and
    Monte Carlo learning serve as two extremities of complete knowledge of environment
    and complete model free [[12](#bib.bib12)], while time difference learning [[12](#bib.bib12)]
    is more ubiquitously used, like a bridge connecting the two extremities. Time
    difference learning is based on the Bellman update error $\delta_{t}=Q(s_{t},a_{t})-\left(R_{t}(s,a)+\gamma\underset{a}{max}\,Q(s_{t+1},a)\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.6]tikz_gpi.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: General Policy Iteration'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Policy Gradient and Actor Critic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement Learning could be viewed as a functional optimization process.
    We could define an objective function over a policy $\pi_{\theta}(a|s)$, as a
    functional, characterized by parameter $\theta$, which could correspond to the
    neural network weights, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose all episodes start from an auxiliary initial state $s_{0}$, which with
    probability $h(s)$, jumps to different state $s\in\mathcal{S}$ without reward.
    $h(s)$ characterizes the initial state distribution which only depends on the
    environment. Let $\eta(s)$ represent the expected number of steps spent on state
    $s$, which can be calculated by summing up the $\gamma$ discounted probability
    $P^{\pi}(s_{0}\rightarrow s,k+1)$ of entering state $s$ with $k+1$ steps from
    auxiliary state $s_{0}$, as stated in Equation ([17](#S2.E17 "In II-C Policy Gradient
    and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣
    Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which can be thought of as the expectation
    of $\gamma^{k}$ conditional on state $s$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\eta(s)$ | $\displaystyle=\sum_{k=0}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s,k+1)$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=h(s)+\sum_{\bar{s},a}\gamma\eta(\bar{s})\pi_{\theta}(a&#124;\bar{s})P(s&#124;\bar{s},a)$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: In Equation ([18](#S2.E18 "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), the
    quantity is calculated by either directly starting from state $s$, which correspond
    to $k=0$ in Equation ([17](#S2.E17 "In II-C Policy Gradient and Actor Critic ‣
    II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), or entering state $s$ from state $\bar{s}$ with one step, corresponding
    to $k+1\geq 2$ in Equation ([17](#S2.E17 "In II-C Policy Gradient and Actor Critic
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: For an arbitrary state $s\in\mathcal{S}$, using $s^{{}^{\prime}}$ and $s^{{}^{\prime\prime}}$
    to represent subsequent states as dummy index,
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\nabla_{\theta}\left[\sum_{a}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)\right]$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\left[\nabla_{\theta}Q^{\pi(\theta)}(s,a)\pi_{\theta}(a&#124;s)+\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\left[\sum_{s^{{}^{\prime}},R}P(s^{{}^{\prime}},R&#124;s,a)\left(R+\gamma
    V^{\pi(\theta)}(s^{{}^{\prime}})\right)\right]\pi_{\theta}(a&#124;s)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime}})\pi_{\theta}(a&#124;s)+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+\sum_{a}\sum_{s^{{}^{\prime}}}\gamma
    P(s^{{}^{\prime}}&#124;s,a)\pi_{\theta}(a&#124;s)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\left[\sum_{a^{{}^{\prime}}}\sum_{s^{{}^{\prime\prime}}}\gamma
    P(s^{{}^{\prime\prime}}&#124;s^{{}^{\prime}},a^{{}^{\prime}})\nabla_{\theta}V^{\pi(\theta)}(s^{{}^{\prime\prime}})\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})+\right.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\left.\sum_{a^{{}^{\prime}}}\nabla_{\theta}\pi_{\theta}(a^{{}^{\prime}}&#124;s^{{}^{\prime}})Q^{\pi(\theta)}(s^{{}^{\prime}},a^{{}^{\prime}})\right]$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: the terms in square brackets in Equation ([22](#S2.E22 "In II-C Policy Gradient
    and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣
    Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) are simply Equation ([21](#S2.E21 "In II-C Policy
    Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) with $a$ and $s^{{}^{\prime}}$ replaced by $a^{{}^{\prime}}$
    and $s^{{}^{\prime\prime}}$. Since $\nabla_{\theta}V^{\pi(\theta)}(s^{\infty})=0$,
    Equation ([22](#S2.E22 "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement
    Learning and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) could
    be written as Equation ([23](#S2.E23 "In II-C Policy Gradient and Actor Critic
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), where $s_{k}$ represent the state of $k$ steps after $s$ and $P^{\pi}(s\rightarrow
    s_{k},k)$ already includes integration of intermediate state $s_{k-1},\ldots s_{1}$
    before reaching state $s_{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\nabla_{\theta}V^{\pi(\theta)}(s)=\sum_{a}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sum_{k=1}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: Let objective function with respect to policy be defined to be the value function
    starting from auxiliary state $s_{0}$ as in Equation ([24](#S2.E24 "In II-C Policy
    Gradient and Actor Critic ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\pi_{\theta})=V^{\pi}(s_{0})=E_{\pi,\varepsilon}\sum_{t=0}^{\infty}\gamma^{t}R_{t}(S_{0}=s)$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: The optimal policy could be obtained by gradient accent optimization, leading
    to the policy gradient algorithm [[12](#bib.bib12)], as in Equation ([29](#S2.E29
    "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\nabla_{\theta}J(\pi_{\theta})=\nabla_{\theta}V^{\pi}(s_{0})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{k=0}\sum_{s_{k}}\sum_{a_{k}}\gamma^{k}P^{\pi}(s_{0}\rightarrow
    s_{k},k)\nabla_{\theta}\pi_{\theta}(a_{k}&#124;s_{k})Q^{\pi(\theta)}(s_{k},a_{k})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\sum_{s}\eta(s)}{\sum_{s}\eta(s)}\sum_{s}\sum_{a}\eta(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{\bar{s}}\eta(\bar{s})\sum_{s}\sum_{a}\mu(s)\frac{\pi_{\theta}(a&#124;s)}{\pi_{\theta}(a&#124;s)}\nabla_{\theta}\pi_{\theta}(a&#124;s)Q^{\pi(\theta)}(s,a)$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\propto$ | $\displaystyle\,E_{\pi}\left[\frac{\nabla_{\theta}\pi_{\theta}(A&#124;S)}{\pi_{\theta}(A&#124;S)}\hat{Q}^{\pi(\theta)}(S,A)\right]$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: The policy gradient could be augmented to include zero gradient baseline $b(s)$,
    with respect to objective function $J(\pi_{\theta})$ in Equation ([28](#S2.E28
    "In II-C Policy Gradient and Actor Critic ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")), as a function of
    state $s$, which does not include parameters for policy $\theta$, since $\sum_{a}\nabla_{\theta}\pi_{\theta}(a|s)=0$.
    To reduce variance of the gradient, the baseline is usually chosen to be the state
    value function estimator $\hat{V}_{w}(s)$ to smooth out the variation of $Q(s,a)$
    at each state, while $\hat{V}_{w}(s)$ is updated in a Monte Carlo way by comparing
    with $\hat{Q}^{\pi_{\theta}}(S,A)=G_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: The actor-critic algorithm [[12](#bib.bib12)] decomposes $G_{t}-V_{w}(s_{t})$
    to be $R_{t}+\gamma V_{w}(s_{t+1})-V_{w}(s_{t})$, so bootstrap is used instead
    of Monte Carlo.
  prefs: []
  type: TYPE_NORMAL
- en: II-D Basics of Deep Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep Q learning [[1](#bib.bib1)] makes a breakthrough in using neural network
    as the functional approximator for value function on complicated tasks. It solves
    the transition correlation problem by random sampling from a replay memory. Specifically,
    the reinforcement learning is transformed in a supervised learning task by fitting
    on the target $R_{t}+\gamma\underset{a}{max}\,Q(s_{t+1},a)$ from the replay memory
    with state $s_{t}$ as input. However, the target can get drifted easily which
    leads to unstable learning. In [[1](#bib.bib1)], a target network is used to provide
    a stable target for the updating network to be learned before getting updated
    occasionally. Double Deep Q learning [[16](#bib.bib16)], however, solves the problem
    by having two Q network and update the parameters in a alternating way. We review
    some state of art deep reinforcement learning algorithms from different aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: II-D1 Off Policy methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Except for Deep Q Learning [[1](#bib.bib1)] mentioned above, DDPG [[17](#bib.bib17)]
    extends Deterministic Policy Gradient (DPG) [[18](#bib.bib18)] with deep neural
    network functional approximator, which is an actor-critic algorithm and works
    well in continuous action spaces.
  prefs: []
  type: TYPE_NORMAL
- en: II-D2 On Policy methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A3C [[19](#bib.bib19)] stands out in the asynchronous methods in deep learning
    [[19](#bib.bib19)] which can be run in parallel on a single multi-core CPU. Trust
    Region Policy Optimization [[2](#bib.bib2)] and Proximal Policy Optimization [[20](#bib.bib20)]
    assimilates the natural policy gradient, which use a local approximation to the
    expected return. The local approximation could serve as a lower bound for the
    expected return, which can be optimized safely subject to the KL divergence constraint
    between two subsequent policies, while in practice, the constraint is relaxed
    to be a regularization.
  prefs: []
  type: TYPE_NORMAL
- en: II-D3 Goal based Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In robot manipulation tasks, the goal could be represented with state in some
    cases [[14](#bib.bib14)]. Universal Value Function Approximator (UVFA) [[21](#bib.bib21)]
    incorporate the goal into the deep neural network, which let the neural network
    functional approximator also generalize to goal changes in tasks, similar to Recommendation
    System [[22](#bib.bib22)]. Work of this direction include [[23](#bib.bib23), [14](#bib.bib14)],
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: II-D4 Replay Memory Manipulation based Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Replay memory is a critical component in Deep Reinforcement Learning, which
    solves the problem of correlated transition in one episode. Beyond the uniform
    sampling of replay memory in Deep Q Network [[1](#bib.bib1)], Prioritized Experience
    Replay [[24](#bib.bib24)] improves the performance by giving priority to those
    transitions with bigger TD error, while Hindsight Experience Replay (HER) [[23](#bib.bib23)]
    manipulate the replay memory with changing goals to transition so as to change
    reward to promote exploration. Maximum entropy regularized multi goal reinforcement
    learning [[14](#bib.bib14)] gives priority to those rarely occurred trajectory
    in sampling, which has been shown to improve over HER [[14](#bib.bib14)].
  prefs: []
  type: TYPE_NORMAL
- en: II-D5 Surrogate policy optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Like surrogate model used in Bayesian Optimization [[25](#bib.bib25)], lower
    bound surrogate is also used in Reinforcement Learning. Trust Region Policy Optimization
    (TRPO) [[2](#bib.bib2)] is built on the identity from [[26](#bib.bib26)] in Equation
    ([30](#S2.E30 "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement
    Learning ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")), where $\eta_{\pi^{new}}(s)$ means the state visitation
    frequency under policy $\pi^{new}$ and advantage $A^{\pi^{old}}(a_{t},s_{t})=Q^{\pi^{old}}(a_{t},s_{t})-V^{\pi^{old}}(s_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\pi^{new})$ | $\displaystyle=J(\pi^{old})+\sum_{s}\eta_{\pi^{new}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a,s)$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: Based on Policy Advantage [[26](#bib.bib26)] $A_{\pi^{old},\eta_{old}}(\pi^{new})=\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a|s)A^{\pi^{old}}(a,s)$,
    a local approximation $L_{\pi^{old}}(\pi^{new})$ to Equation ([30](#S2.E30 "In
    II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement Learning
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) can be defined in Equation ([31](#S2.E31 "In II-D5 Surrogate policy
    optimization ‣ II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning
    and Deep Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), based on which,
    a surrogate function $M(\pi^{new},\pi^{old})$ is defined in Equation ([32](#S2.E32
    "In II-D5 Surrogate policy optimization ‣ II-D Basics of Deep Reinforcement Learning
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) that minorizes $J(\pi^{new})$ at $\pi^{old}$, where $D_{KL}^{max}(\pi^{old},\pi^{new})=\underset{s}{\max}D_{KL}(\pi^{old}(a|s),\pi^{new}(a|s))$
    is the maximum KL divergence, so MM [[2](#bib.bib2)] algorithm could be used to
    improve the policy, leading to the trust region method [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle L_{\pi^{old}}(\pi^{new})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle J(\pi^{old})+\sum_{s}\eta_{\pi^{old}}(s)\sum_{a}\pi^{new}(a&#124;s)A^{\pi^{old}}(a_{t},s_{t})$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle M(\pi^{new},\pi^{old})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle L_{\pi^{old}}(\pi^{new})-\frac{4\underset{a,s}{max}&#124;A^{\pi^{old}}(s,a)&#124;\gamma}{1-\gamma^{2}}D_{KL}^{max}(\pi^{old},\pi^{new})$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: Comparison of deep reinforcement learning methods: ”S” means state
    and ”A” means action, where ”c” means continuous, ”d” means discrete. ”standalone”
    means whether the algorithm work independently or needs to be combined with another
    learning algorithm. ”var” means which probability the variational inference is
    approximating, ”p” means whether the method is on policy or off policy. ”na” means
    not applicable'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | S | A | standalone | var | p |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Q | c | d | y | na | off |'
  prefs: []
  type: TYPE_TB
- en: '| A3C | c | c/d | y | na | on |'
  prefs: []
  type: TYPE_TB
- en: '| TRPO/PPO | c | c/d | y | na | on |'
  prefs: []
  type: TYPE_TB
- en: '| DDPG | c | c | y | na | off |'
  prefs: []
  type: TYPE_TB
- en: '| Boltzmann | d | d | y | na | on |'
  prefs: []
  type: TYPE_TB
- en: '| VIME | c | c | n | $p_{\theta}(s_{t+1}&#124;s_{t},a_{t})$ | na |'
  prefs: []
  type: TYPE_TB
- en: '| VAST | c | d | n | $p(s_{t}&#124;o_{t-k})$ | na |'
  prefs: []
  type: TYPE_TB
- en: '| SoftQ | c | c/d | y | $p(a_{t}&#124;s_{t})$ | on |'
  prefs: []
  type: TYPE_TB
- en: III Taxonomy of PGM and VI in deep reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the success of deep reinforcement learning in many talks, the field
    still faces some critical challenges. One problem is exploration with sparse reward.
    In complicated real environment, an agent has to explore for a long trajectory
    before it can get any reward as feedback. Due to lack of enough rewards, traditional
    Reinforcement Learning methods performs poorly, which lead to a lot of recent
    contributions in the exploration methods. Another challenge is how to represent
    policy in extremely large state and action spaces. Furthermore, sometimes it is
    beneficial to have multimodal behavior for a agent when some trajectory might
    be equivalent to other trajectories and we want to learn all of them.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we give detailed explanation on how graphical model and variational
    inference could be used to model and optimize the reinforcement learning process
    under these challenges and form a taxonomy of these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Together with the deep reinforcement learning methods mentioned in section [II-D](#S2.SS4
    "II-D Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning and Deep
    Reinforcement Learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), we make a comparison
    of them in Table [I](#S2.T1 "TABLE I ‣ II-D5 Surrogate policy optimization ‣ II-D
    Basics of Deep Reinforcement Learning ‣ II Reinforcement Learning and Deep Reinforcement
    Learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning").
  prefs: []
  type: TYPE_NORMAL
- en: III-A Policy and value function with undirected graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We first discuss the application of undirected graphs in deep reinforcement
    learning, which models joint distribution of variables with cliques [[7](#bib.bib7)].
    In [[27](#bib.bib27)], the authors use Restricted Boltzmann Machine (RBM) [[8](#bib.bib8)],
    which has nice property of tractable factorized posterior distribution over the
    latent variables conditioned on observed variables. To deal with MDPs of large
    state and action spaces, they model the state-action value function with the negative
    free energy of a Restricted Boltzmann Machine. Specifically, the visible states
    of the Restricted Boltzmann Machine [[27](#bib.bib27)] consists of both state
    $s$ and action $a$ binary variables, as shown in Figure [6](#S3.F6 "Figure 6 ‣
    III-A Policy and value function with undirected graphs ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), where the hidden
    nodes consist of $L$ binary variables, while state variables $s_{i}$ are dark
    colored to represent it can be observed and actions $a_{j}$ are light colored
    to represent it need to be sampled. Together with the auxiliary hidden variables,
    the undirected graph defines a joint probability distribution over state and action
    pairs, which defines a stochastic policy network that could sample actions out
    for on policy learning. Since it is pretty easy to calculate the derivative of
    the free energy $F(s,a)$ with respect to the coefficient $w_{k,j}$ of the network,
    one could use temporal difference learning to update the coefficients in the network.
    Thanks to properties of Boltzmann Machine, the conditional distribution of action
    over state $p(a|s)$, which could be used as a policy, is still Boltzmann distributed
    as in Equation ([33](#S3.E33 "In III-A Policy and value function with undirected
    graphs ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")), governed by the free energy $F(a,s)$, where $Z(s)$
    is the partition function [[7](#bib.bib7)] and the negative free energy to approximate
    the state action value function $Q(s,a)$. By adjusting the temperature $T$, one
    could also change between different exploration strength.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.6]tikz_pgm_boltzmanmachine.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Restricted Boltzmann Machine Value and Policy'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(a&#124;s)={1/Z(s)}e^{-F(s,a)/T}={1/Z(s)}e^{Q(s,a)/T}$
    |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: A few steps of MCMC sampling [[7](#bib.bib7)] could be used to sample actions,
    as an approximation of the policy, which can be fed into a time difference learning
    method like SARSA [[12](#bib.bib12)], to update the state value function $Q(s,a)$’s
    estimation. Such an on-policy process has been shown to be empirically effective
    in the large state actions spaces [[27](#bib.bib27)].
  prefs: []
  type: TYPE_NORMAL
- en: III-B Variational Inference on ”optimal” Policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: III-B1 policy as ”optimal” posterior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Boltzmann Machine defined Product of Expert Model in [[27](#bib.bib27)]
    works well for large state and action spaces, but are limited to discrete specifically
    binary state and action variables. For continuous state and action spaces, in
    [[28](#bib.bib28)], the author proposed deep energy based models with Directed
    Acyclic Graphs (DAG) [[7](#bib.bib7)], which we re-organize in a different form
    in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning") with annotations added.
    The difference with respect to Figure [3](#S2.F3 "Figure 3 ‣ II-A2 DAGs for (Partially
    Observed ) Markov Decision Process ‣ II-A Basics about Reinforcement Learning
    with graphical model ‣ II Reinforcement Learning and Deep Reinforcement Learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning") is that, in Figure [7](#S3.F7 "Figure 7 ‣ III-B1
    policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), the reward is not explicit expressed in the directed graphical model.
    Instead, an auxilliary binary Observable $O$ is used to define whether the corresponding
    action at the current step is optimal or not. The conditional probability of the
    action being optimal is $p(O_{t}=1|s_{t},a_{t})=\exp(r(s_{t},a_{t}))$, which connects
    conditional optimality with the amount of award received by encouraging the agent
    to take highly rewarded actions in an exponential manner. Note that the reward
    here must be negative to ensure the validity of probability, which does not hurt
    generality since reward range can be translated [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The Graphical Model in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal”
    posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of
    PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning") in
    total defines the trajectory likelihood or the evidence in Equation ([34](#S3.E34
    "In III-B1 policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal”
    Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(\tau)$ | $\displaystyle=\left[p(s_{1})\prod_{t}p(s_{t+1}&#124;s_{t},a_{t})\right]\exp\left(\sum_{t}r(s_{t},a_{t})\right)$
    |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: By doing so, the author is forcing a form of functional expression on top of
    the conditional independence structure of the graph by assigning a likelihood.
    In this way, calculating the optimal policy of actions distributions becomes an
    inference problem of calculating the posterior $p(a_{t}|s_{t},O_{t:T}=1)$, which
    reads as, conditional on optimality from current time step until end of episode,
    and the current current state to be $s_{t}$, the distribution of action $a_{t}$,
    and this posterior corresponds to the optimal policy. Observing the d-separation
    from Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning"), $O_{1:{t-1}}$ is conditionally
    independent of $a_{t}$ given $s_{t}$, $(O_{1:{t-1}}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}A^{act}_{t})\mid{S_{t}}$,
    so $p(a_{t}|s_{t},O_{1:t-1},O_{t:T})=p(a_{t}|s_{t},O_{t:T})$
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.7]tikz_pgm_soft_q.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Optimal Policy as posterior on actions: $p(a_{t}|s_{t},O_{t:T}=1)$'
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Message passing for exact inference on the posterior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we give detailed derivation on conducting exact inference on
    the policy posterior which is not given in [[13](#bib.bib13)]. Similar to the
    forward-backward message passing algorithm [[7](#bib.bib7)] in Hidden Markov Models
    [[7](#bib.bib7)], the posterior $p(a_{t}|s_{t},O_{t:T}=1)$ could also be calculated
    by passing messages. We offer a detailed derivation of the decomposition of the
    posterior $p(a_{t}|s_{t},O_{t:T}=1)$ in Equation ([35](#S3.E35 "In III-B2 Message
    passing for exact inference on the posterior ‣ III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which is not available in [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle p(a_{t}&#124;s_{t},O_{t:T}=1)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(a_{t},s_{t},O_{t:T}=1)}{p(s_{t},O_{t:T}=1)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t},s_{t})}{p(s_{t},O_{t:T}=1)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})p(s_{t})}{\int_{a_{t}{{}^{\prime}}}p(s_{t},a_{t}^{{}^{\prime}},O_{t:T}=1)d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})p(s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1&#124;a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}&#124;s_{t})p(s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(O_{t:T}=1&#124;a_{t},s_{t})p(a_{t}&#124;s_{t})}{\int_{a_{t}^{{}^{\prime}}}p(O_{t:T}=1&#124;a_{t}{{}^{\prime}},s_{t})p(a_{t}{{}^{\prime}}&#124;s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\beta(a_{t},s_{t})}{\beta(s_{t})}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: In Equation ([35](#S3.E35 "In III-B2 Message passing for exact inference on
    the posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), we
    define message $\beta(a_{t},s_{t})=p(O_{t:T}=1|a_{t},s_{t})p(a_{t}|s_{t})$ and
    message $\beta(s_{t})=\int_{a_{t}^{{}^{\prime}}}\beta(a_{t}^{{}^{\prime}},s_{t})d\{a_{t}^{{}^{\prime}}\}$.
    If we consider $p(a_{t}|s_{t})$ as a prior with a trivial form of uniform distribution
    [[13](#bib.bib13)], the only policy related term becomes $p(O_{t:T}=1|a_{t},s_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to HMM, here, only the backward messages are relevant. Additionally,
    the backward message $\beta(a_{t},s_{t})$ here is not a probability distribution
    as in HMM, instead, is just a probability. In Figure [7](#S3.F7 "Figure 7 ‣ III-B1
    policy as ”optimal” posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), the backward message $\beta(a_{t},s_{t})$ could be decomposed recursively.
    Since in [[13](#bib.bib13)] the author only give the conclusion without derivation,
    we give a detailed derivaion of this recursion in Equation ([36](#S3.E36 "In III-B2
    Message passing for exact inference on the posterior ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\beta(s_{t},a_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle p(O_{t}=1,O_{t+1:T}=1&#124;s_{t},a_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{\int p(O_{t}=1,O_{t+1:T}=1,s_{t},a_{t},s_{t+1},a_{t+1})d\{s_{t+1},a_{t+1}\}}{p(s_{t},a_{t})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1},O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1,s_{t+1},a_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  | ($(O_{t+1:T},S_{t+1},A_{t+1}\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10.0mu\perp$}}}O_{t})\mid
    S_{t},A_{t}$) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\int\frac{p(O_{t+1:T}=1,s_{t+1},a_{t+1})}{p(s_{t+1},a_{t+1})}\frac{p(s_{t+1},s_{t},a_{t})}{p(s_{t},a_{t})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle p(O_{t}=1&#124;s_{t},a_{t})d\{s_{t+1},a_{t+1}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\int p(O_{t+1:T}=1&#124;s_{t+1},a_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle d\{s_{t+1},a_{t+1}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: The recursion in Equation ([36](#S3.E36 "In III-B2 Message passing for exact
    inference on the posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) start from the last time point $T$ of an episode.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Connection between Message Passing and Bellman equation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we define Q function in Equation ([37](#S3.E37 "In III-B3 Connection between
    Message Passing and Bellman equation ‣ III-B Variational Inference on ”optimal”
    Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial
    and Survey on Probabilistic Graphical Model and Variational Inference in Deep
    Reinforcement Learning")) and V function in Equation ([38](#S3.E38 "In III-B3
    Connection between Message Passing and Bellman equation ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning"))
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(s_{t},a_{t})=\log(\beta(a_{t},s_{t}))$ |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle V(s_{t})$ | $\displaystyle=\log\beta(s_{t})=\log\int\beta(s_{t},a_{t})da_{t}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\log\int exp(Q(s_{t},a_{t}))da_{t}\approx\underset{a_{t}}{max}Q(s_{t},a_{t})$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: then the corresponding policy could be written as Equation ([39](#S3.E39 "In
    III-B3 Connection between Message Passing and Bellman equation ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi(a_{t}&#124;s_{t})=p(a_{t}&#124;s_{t},O_{t:T}=1)=\exp(Q(s_{t},a_{t})-V(s_{t}))$
    |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: Taking the logrithm of Equation ([36](#S3.E36 "In III-B2 Message passing for
    exact inference on the posterior ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), we get Equation ([40](#S3.E40 "In III-B3 Connection between Message
    Passing and Bellman equation ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"))
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log(\beta(s_{t},a_{t}))$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\log\int\beta(s_{t+1})p(s_{t+1}&#124;s_{t},a_{t})p(O_{t}=1&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\log\int\exp[r(s_{t},a_{t})+V(s_{t+1})]p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=r(s_{t},a_{t})+\log\int\exp(V(s_{t+1}))p(s_{t+1}&#124;s_{t},a_{t})ds_{t+1}$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: 'which reduces to the risk seeking backup in Equation ([41](#S3.E41 "In III-B3
    Connection between Message Passing and Bellman equation ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")) as mentioned in [[13](#bib.bib13)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\log E_{s_{t+1}\sim p(s_{t+1}&#124;s_{t},a_{t})}[\exp(V(s_{t+1}))]$
    |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: 'The mathematical insight here is that if we define the messages passed on the
    Directed Acyclic Graph in Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal”
    posterior ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of
    PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning"), then
    message passing correspond to a peculiar version Bellman Equation like backup,
    which lead to an unwanted risk seeking behavior [[13](#bib.bib13)]: when compared
    to Equation ([10](#S2.E10 "In II-B Value Function, Bellman Equation, Policy Iteration
    ‣ II Reinforcement Learning and Deep Reinforcement Learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")), the Q function here is taking a softmax instead of expectation over
    the next state.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B4 Variational approximation to ”optimal” policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since the exact inference lead to unexpected behavior, approximate inference
    could be used. The optimization of the policy could be considered as a variational
    inference problem, and we use the variational policy of the action posterior distribution
    $q(a_{t}|s_{t})$, which could be represented by a neural network, to compose the
    proposal variational likelihood of the trajectory as in Equation ([42](#S3.E42
    "In III-B4 Variational approximation to ”optimal” policy ‣ III-B Variational Inference
    on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(\tau)$ | $\displaystyle=p(s_{1})\prod_{t}[p(s_{t+1}&#124;s_{t},a_{t})q(a_{t}&#124;s_{t})]$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: where the initial state distribution $p(s_{1})$ and the environmental dynamics
    of state transmission is kept intact. Using the proposal trajectory as a pivot,
    we could derive the Evidence Lower Bound (ELBO) of the optimal trajectory as in
    Equation ([43](#S3.E43 "In III-B4 Variational approximation to ”optimal” policy
    ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), which correspond
    to an interesting objective function of reward plus entropy return, as in Equation
    ([45](#S3.E45 "In III-B4 Variational approximation to ”optimal” policy ‣ III-B
    Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep
    reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical Model
    and Variational Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\log(p(O_{1:T}))$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\log\int p(O_{1:T}=1,s_{1:T},a_{1:T})\frac{q(s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}ds_{1:T}da_{1:T}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\log E_{q(s_{1:T},a_{1:T})}\frac{p(O_{1:T}=1,s_{1:T},a_{1:T})}{q(s_{1:T},a_{1:T})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\geq$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\log p(O_{1:T}=1,s_{1:T},a_{1:T})-\log
    q(s_{1:T},a_{1:T})]$ |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(q(\tau)&#124;p(\tau))$ |  | (44)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{q(s_{1:T},a_{1:T})}[\sum_{t=1:T}[r(s_{t},a_{t})-\log
    q(a_{t}&#124;s_{t})]]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t=1:T}E_{s_{t},a_{t}}[r(s_{t},a_{t})+H(\pi(a_{t}&#124;s_{t}))]$
    |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: III-B5 Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[28](#bib.bib28)], the state action value function is defined in Equation
    ([46](#S3.E46 "In III-B5 Examples ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{\pi}_{soft}(s,a)=r_{0}+E_{r\sim\pi,s_{0}=s,a_{0}=a}[\sum_{t=1}^{\infty}\gamma^{t}(r_{t}+\alpha
    H(\pi(.&#124;s_{t})))]$ |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: and a soft version of Bellman update similar to Q Learning [[12](#bib.bib12)]
    is carried out, which lead to policy improvement with respect to the corresponding
    functional objective in Equation ([47](#S3.E47 "In III-B5 Examples ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle J(\pi)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}\sum_{l=t}^{\infty}\gamma^{l-t}E_{(s_{l},a_{l})}[r(s_{l},a_{l})+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\alpha H(\pi(.&#124;s_{l}))&#124;s_{t},a_{t}]]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\sum_{t}E_{(s_{t},a_{t})\sim\rho_{\pi}}[Q_{soft}^{\pi}(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))]$ |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: Setting policy as Equation ([39](#S3.E39 "In III-B3 Connection between Message
    Passing and Bellman equation ‣ III-B Variational Inference on ”optimal” Policies
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) lead to policy improvement. We offer a detailed proof for a key formula
    in Equation ([48](#S3.E48 "In III-B5 Examples ‣ III-B Variational Inference on
    ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")), which is stated in Equation (19) of [[28](#bib.bib28)]
    without proof. In Equation ([48](#S3.E48 "In III-B5 Examples ‣ III-B Variational
    Inference on ”optimal” Policies ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), we use $\pi(\cdot|s)$ to implicitly
    represent $\pi(a|s)$ to avoid symbol aliasing whenever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle H(\pi(\cdot&#124;s))+E_{a\sim\pi}[Q_{soft}^{\pi}(s,a)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-Q_{soft}^{\pi}(s,a)]da$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\exp(Q_{soft}^{\pi}(s,a))]]da$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\frac{\exp(Q_{soft}^{\pi}(s,a))}{\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}]]da$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-\int_{a}\pi(a&#124;s)[\log\pi(a&#124;s)-\log[\tilde{\pi}(a&#124;s)]-$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle log\int exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))]da^{{}^{\prime}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle-D_{KL}(\pi(\cdot&#124;s)&#124;&#124;\tilde{\pi}(\cdot&#124;s))+\log\int\exp(Q_{soft}^{\pi}(s,a^{{}^{\prime}}))da^{{}^{\prime}}$
    |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: For the rest of the proof, we invite the reader to read the appendix of [[28](#bib.bib28)].
    Algorithms of the this kind of maximum entropy family also include Soft Actor
    Critic [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: III-C Variational Inference on the Environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another direction of using Variational Inference in Reinforcement Learning is
    to learn an environmental model, either on the dynamics or the latent state space
    posterior.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Variational inference on transition model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Variational Information Maximizing Exploration (VIME) [[4](#bib.bib4)], where
    dynamic model $p_{\theta}(s_{t+1}|s_{t},a_{t})$ for the agent’s interaction with
    the environment is modeled using Bayesian Neural Network [[10](#bib.bib10)]. The
    R.V. for $\theta$ is denoted by $\Theta$, and is treated in a Bayesian way by
    modeling the weight $\theta$ uncertainty of a neural network. We represent this
    model with the graphical model in Figure [8](#S3.F8 "Figure 8 ‣ III-C1 Variational
    inference on transition model ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"), which is not given in [[4](#bib.bib4)]. The belief uncertainty about
    the environment is modeled as entropy of the posterior distribution of the neural
    network weights $H(\Theta|\xi_{t})$ based on trajectory observations $\xi_{t}=\{s_{1:t},a_{1:t-1}\}$.
    The method encourages taking exploratory actions by alleviating the average information
    gain of the agent’s belief about the environment after observing a new state $s_{t+1}$,
    which is $E_{p(s_{t+1}|\xi_{t},a_{t})}D_{KL}(p(\theta|\xi_{t+1})||p(\theta|\xi_{t}))$,
    and this is equivalent to the entropy minus conditional entropy $H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t},a_{t},s_{t+1})=H(\Theta|\xi_{t},a_{t})-H(\Theta|\xi_{t+1})$.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.6]tikz_pgm_mdp_vime.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Probabilistic Graphical Model For VIME'
  prefs: []
  type: TYPE_NORMAL
- en: With the help of Equation ([49](#S3.E49 "In III-C1 Variational inference on
    transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")), as
    derived following the definition of conditional mutual information, we derive
    in Equation ([50](#S3.E50 "In III-C1 Variational inference on transition model
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")) that the conditional
    entropy difference is actually the average information gain, which is equal to
    the conditional mutual information $I(\Theta,S_{t+1}|\xi_{t},a_{t})$ between environmental
    parameter $\Theta$ and the new state $S_{t+1}$. Such a derivation is not given
    in [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle I(X;Y\mid Z)$ | $\displaystyle=\int_{x,y,z}p(z)p(x,y&#124;z)\log\frac{p(x,y&#124;z)}{p(x&#124;z)p(y&#124;z)}dxdydz$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=-\int_{x,y,z}p(z)p(x,y&#124;z)\log p(x&#124;z)dxdzdy+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\int_{x,y,z}p(x,y,z)\log p(x&#124;y,z)dxdzdy$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=H(X\mid Z)-H(X\mid Y,Z)$ |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle H(\Theta&#124;\xi_{t},a_{t})-H(\Theta&#124;\xi_{t},a_{t},s_{t+1})=I(\Theta,S_{t+1}&#124;\xi_{t},a_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1},\theta&#124;\xi_{t},a_{t})\log[\frac{p(s_{t+1},\theta&#124;\xi_{t},a_{t})}{p(\theta&#124;\xi_{t})p(s_{t+1}&#124;\xi_{t},a_{t})}]d\theta$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle ds_{t+1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}\int_{\Theta,\mathcal{S}}p(s_{t+1}&#124;\xi_{t},a_{t})p(\theta&#124;\xi_{t+1})\log[\frac{p(\theta&#124;\xi_{t+1})}{p(\theta&#124;\xi_{t})}]d\theta
    ds_{t+1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle E_{\xi_{t},a_{t}}E_{p(s_{t+1}&#124;\xi_{t},a_{t})}D_{KL}(p(\theta&#124;\xi_{t+1})&#124;&#124;p(\theta&#124;\xi_{t}))$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: Based on Equation ([50](#S3.E50 "In III-C1 Variational inference on transition
    model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), an intrinsic
    reward can be augmented from the environmental reward function, thus the method
    could be incorporated with any existing reinforcement learning algorithms for
    exploration, TRPO [[2](#bib.bib2)], for example. Upon additional observation of
    action $a_{t}$ and state $s_{t+1}$ pair on top of trajectory history $\xi_{t}$,
    the posterior on the distribution of the environmental parameter $\theta$, $p(\theta|\xi_{t})$,
    could be updated to be $p(\theta|\xi_{t+1})$ in a Bayesian way as derived in Equation
    ([51](#S3.E51 "In III-C1 Variational inference on transition model ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")), which is first proposed in [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(\theta&#124;\xi_{t+1})=$ | $\displaystyle\frac{p(\theta,\xi_{t},a_{t},s_{t+1})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(\xi_{t},a_{t},s_{t+1})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta,\xi_{t},a_{t})}{p(a_{t},\xi_{t})p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t},a_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=$ | $\displaystyle\frac{p(s_{t+1}&#124;\theta,\xi_{t},a_{t})p(\theta&#124;\xi_{t})}{p(s_{t+1}&#124;a_{t},\xi_{t})}$
    |  | (51) |'
  prefs: []
  type: TYPE_TB
- en: In Equation ([51](#S3.E51 "In III-C1 Variational inference on transition model
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), the denominator
    can be written as Equation ([52](#S3.E52 "In III-C1 Variational inference on transition
    model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")), so that the
    dynamics of the environment modeled by neural network weights $\theta$, $p(s_{t+1}|\theta,a_{t},\xi_{t})$,
    could be used.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(s_{t+1}&#124;a_{t},\xi_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1},\theta&#124;a_{t},\xi_{t})d\theta$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1},\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\int_{\Theta}\frac{p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta,a_{t},\xi_{t})}{p(a_{t},\xi_{t})}d\theta$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\int_{\Theta}p(s_{t+1}&#124;\theta,a_{t},\xi_{t})p(\theta&#124;\xi_{t})d\theta$
    |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: The last step of Equation ([52](#S3.E52 "In III-C1 Variational inference on
    transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) makes
    use of $p(\theta|\xi_{t},a_{t})=p(\theta|\xi_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: Since the integral in Equation ([52](#S3.E52 "In III-C1 Variational inference
    on transition model ‣ III-C Variational Inference on the Environment ‣ III Taxonomy
    of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic
    Graphical Model and Variational Inference in Deep Reinforcement Learning")) is
    not tractable, variational treatment over the neural network weights posterior
    distribution $p(\theta|\xi_{t})$ is used, characterized by variational parameter
    $\phi$, as shown in the dotted line in Figure [8](#S3.F8 "Figure 8 ‣ III-C1 Variational
    inference on transition model ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning"). The variational posterior about the model parameter $\theta$, updated
    at each step, could than be used to calculate the intrinsic reward in Equation
    ([50](#S3.E50 "In III-C1 Variational inference on transition model ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Variational Inference on hidden state posterior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Variational State Tabulation (VaST) [[5](#bib.bib5)], the author assume the
    high dimensional observed state to be represented by Observable $O$, while the
    transition happens at the latent state space represented by $S$, which is finite
    and discrete. The author assume a factorized form of observation and latent space
    joint probability, which we explicitly state in Equation ([53](#S3.E53 "In III-C2
    Variational Inference on hidden state posterior ‣ III-C Variational Inference
    on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p(O,S)=\pi_{\theta_{0}}(s_{0})\prod_{t=0}^{T}p_{\theta^{R}}(o_{t}&#124;s_{t})\prod_{t=1}^{T}p_{\theta^{T}}(s_{t}&#124;s_{t-1},a_{t-1})$
    |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: Additionally, we characterize Equation ([53](#S3.E53 "In III-C2 Variational
    Inference on hidden state posterior ‣ III-C Variational Inference on the Environment
    ‣ III Taxonomy of PGM and VI in deep reinforcement learning ‣ Tutorial and Survey
    on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement
    Learning")) with the probabilistic graphical model in Figure [9](#S3.F9 "Figure
    9 ‣ III-C2 Variational Inference on hidden state posterior ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning") which does not exist in [[5](#bib.bib5)].
    Compared to Figure [7](#S3.F7 "Figure 7 ‣ III-B1 policy as ”optimal” posterior
    ‣ III-B Variational Inference on ”optimal” Policies ‣ III Taxonomy of PGM and
    VI in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning"), here the latent
    state $S$ is in discrete space instead of high dimension, and the observation
    is a high dimensional image instead of binary variable to indicate optimal action.
    By assuming a factorized form of the variational posterior in Equation ([54](#S3.E54
    "In III-C2 Variational Inference on hidden state posterior ‣ III-C Variational
    Inference on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement
    learning ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational
    Inference in Deep Reinforcement Learning")),
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle q(S_{0:T}&#124;O_{0:T})=\prod_{t=0}^{T}q_{\phi}(S_{t}&#124;O_{t-k:t})$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: The author assume the episode length to be $T$, and default frame prior observation
    to be blank frames. The Evidence Lower Bound (ELBO) of the observed trajectory
    of Equation ([53](#S3.E53 "In III-C2 Variational Inference on hidden state posterior
    ‣ III-C Variational Inference on the Environment ‣ III Taxonomy of PGM and VI
    in deep reinforcement learning ‣ Tutorial and Survey on Probabilistic Graphical
    Model and Variational Inference in Deep Reinforcement Learning")) could be easily
    represented by a Varitional AutoEncoder [[31](#bib.bib31)] like architecture,
    where the encoder $q_{\phi}$, together with the reparametrization trick [[31](#bib.bib31)],
    maps the observed state $O$ into parameters for the Con-crete distribution [[32](#bib.bib32)],
    so backprobagation could be used on deterministic variables to update the weight
    of the network based on the ELBO, which is decomposed into different parts of
    the reconstruction losses of the variational autoencoder like architecture. Like
    VIME [[4](#bib.bib4)], VaSt could be combined with other reinforcement learning
    algorithms. Here prioritized sweeping [[12](#bib.bib12)] is carried out on the
    Heviside activation of the encoder output directly, by counting the transition
    frequency, instead of waiting for the slowly learned environmental transition
    model $p_{\theta^{T}}(s_{t}|s_{t-1},a_{t-1})$ in Equation ([53](#S3.E53 "In III-C2
    Variational Inference on hidden state posterior ‣ III-C Variational Inference
    on the Environment ‣ III Taxonomy of PGM and VI in deep reinforcement learning
    ‣ Tutorial and Survey on Probabilistic Graphical Model and Variational Inference
    in Deep Reinforcement Learning")). A potential problem of doing so is aliasing
    between latent state $s$ and observed state $o$. To alleviate this problem, in
    [[5](#bib.bib5)], the author actively relabel the transition history in the replay
    memory once found the observable has been assigned a different latent discrete
    state.
  prefs: []
  type: TYPE_NORMAL
- en: \includegraphics
  prefs: []
  type: TYPE_NORMAL
- en: '[scale=0.7]tikz_pgm_vast.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Graphical Model for Variation State Tabulation'
  prefs: []
  type: TYPE_NORMAL
- en: IV Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a tutorial survey, we recap Reinforcement Learning with Probabilistic Graphical
    Models, summarizes recent advances of Deep Reinforcement Learning and offer a
    taxonomy of Probabilistic Graphical Model and Variational Inference in DRL with
    detailed derivations which are not included in the original contributions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International conference on machine learning*, 2015,
    pp. 1889–1897.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] X. Sun, J. Lin, and B. Bischl, “Reinbo: Machine learning pipeline search
    and configuration with bayesian optimization embedded reinforcement learning,”
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel,
    “Vime: Variational information maximizing exploration,” in *Advances in Neural
    Information Processing Systems*, 2016, pp. 1109–1117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Corneil, W. Gerstner, and J. Brea, “Efficient model-based deep reinforcement
    learning with variational state tabulation,” *arXiv preprint arXiv:1802.04325*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational inference:
    A review for statisticians,” *Journal of the American Statistical Association*,
    vol. 112, no. 518, pp. 859–877, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. M. Bishop, *Pattern recognition and machine learning*.   springer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. Fischer and C. Igel, “Training restricted boltzmann machines: An introduction,”
    *Pattern Recognition*, vol. 47, pp. 25–39, 01 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Sun, A. Gossmann, Y. Wang, and B. Bischl, “Variational resampling based
    assessment of deep neural networks under distribution shift,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight uncertainty
    in neural networks,” *arXiv preprint arXiv:1505.05424*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] R. S. Sutton, A. G. Barto *et al.*, *Introduction to reinforcement learning*.   MIT
    press Cambridge, 1998, vol. 2, no. 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] S. Levine, “Reinforcement learning and control as probabilistic inference:
    Tutorial and review,” *arXiv preprint arXiv:1805.00909*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. Zhao, X. Sun, and V. Tresp, “Maximum entropy-regularized multi-goal
    reinforcement learning,” *arXiv preprint arXiv:1905.08786*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and acting
    in partially observable stochastic domains,” *Artificial i ntelligence*, vol.
    101, no. 1-2, pp. 99–134, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Thirtieth AAAI conference on artificial intelligence*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *arXiv preprint arXiv:1509.02971*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International conference on machine learning*, 2016, pp. 1928–1937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] T. Schaul, D. Horgan, K. Gregor, and D. Silver, “Universal value function
    approximators,” in *International Conference on Machine Learning*, 2015, pp. 1312–1320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Kushwaha, X. Sun, B. Singh, and O. Vyas, “A lesson learned from pmf
    based approach for semantic recommender system,” *Journal of Intelligent Information
    Systems*, vol. 50, no. 3, pp. 441–453, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,
    B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, “Hindsight experience replay,”
    in *Advances in Neural Information Processing Systems*, 2017, pp. 5048–5058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] X. Sun, A. Bommert, F. Pfisterer, J. Rahnenführer, M. Lang, and B. Bischl,
    “High dimensional restrictive federated model selection with multi-objective bayesian
    optimization over shifted distributions,” *arXiv preprint arXiv:1902.08999*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Kakade and J. Langford, “Approximately optimal approximate reinforcement
    learning,” in *ICML*, vol. 2, 2002, pp. 267–274.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B. Sallans and G. E. Hinton, “Reinforcement learning with factored states
    and actions,” *Journal of Machine Learning Research*, vol. 5, no. Aug, pp. 1063–1088,
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning
    with deep energy-based policies,” in *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*.   JMLR. org, 2017, pp. 1352–1361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *arXiv preprint
    arXiv:1801.01290*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Y. Sun, F. Gomez, and J. Schmidhuber, “Planning to be surprised: Optimal
    bayesian exploration in dynamic environments,” in *International Conference on
    Artificial General Intelligence*.   Springer, 2011, pp. 41–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C. Doersch, “Tutorial on variational autoencoders,” *arXiv preprint arXiv:1606.05908*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] C. J. Maddison, A. Mnih, and Y. W. Teh, “The concrete distribution: A
    continuous relaxation of discrete random variables,” *arXiv preprint arXiv:1611.00712*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
