- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:06:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1905.08110] Image Captioning based on Deep Learning Methods: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1905.08110](https://ar5iv.labs.arxiv.org/html/1905.08110)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Image Captioning based on Deep Learning Methods: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yiyu Wang¹¹1Y. Wang and J. Xu are corresponding authors    Jungang Xu^∗    Yingfei
    Sun &Ben He University of Chinese Academy of Sciences, Beijing
  prefs: []
  type: TYPE_NORMAL
- en: wangyiyu18@mails.ucas.ac.cn, {xujg,yfsun,benhe}@ucas.ac.cn
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Image captioning is a challenging task and attracting more and more attention
    in the field of Artificial Intelligence, and which can be applied to efficient
    image retrieval, intelligent blind guidance and human-computer interaction, etc.
    In this paper, we present a survey on advances in image captioning based on Deep
    Learning methods, including Encoder-Decoder structure, improved methods in Encoder,
    improved methods in Decoder, and other improvements. Furthermore, we discussed
    future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a large number of unlabeled images in the network; it is impossible
    to label them manually. How to automatically generate natural language descriptions
    for images by computer is a challenging task in the field of artificial intelligence.
    Image captioning can be applied to efficient image retrieval, intelligent blind
    guidance and human-computer interaction, so it is also a task with practical value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of image captioning is to generate a trusted description for a given
    image. So, it is necessary to ensure the correctness of the objects, attribute
    information, semantic information, and position relationship information in the
    description. Therefore, we can decompose image captioning into two sub-tasks:
    (1) understanding the image, acquiring the relevant information correctly; (2)
    generating description based on the understanding of the image. Image captioning
    is a challenging task because it connects the two fields of Computer Vision(CV)
    and Natural Language Processing(NLP).'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, image understanding is equivalent to feature extraction. In
    traditional methods, the bottom visual features (such as geometry, texture, colour,
    etc.) are extracted by using artificially designed feature operators, and then
    combined to form high-level global features. However, there are some drawbacks
    in these traditional methods. On one hand, the design of feature operator relies
    too much on luck and experience. On the other hand, the problem of ”semantic gap”
    leads to the inability of low-level visual features to accurately express semantic
    features. Therefore, traditional methods lack robustness and generalisation performance.
  prefs: []
  type: TYPE_NORMAL
- en: For a given image, the retrieval-based method selects sentence(s) from a specified
    image-description pool as the description(s) of the image; the template-based
    method detects a series of specified visual features from the image, and then
    fills them into the blank position of the given template. Images are very complex
    data. The description extracted by the retrieval-based method may not fully conform
    to the image. The image description generated by template-based method seems too
    rigid and lacks diversity.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, Convolutional Neural Networks (CNN) have obtained outstanding
    effects in CV tasks, such as image classification, object detection. Recurrent
    Neural Networks (RNN) also played a significant role in NLP. In addition, inspired
    by Encoder-Decoder structure in machine translation Sutskever et al. ([2014](#bib.bib21)),
    Vinyals et al. ([2015](#bib.bib23)) uses GoogLeNet as Encoder to automatically
    extract image features, and then uses Long and Short-Term Memory network (LSTM)
    Hochreiter and Schmidhuber ([1997](#bib.bib9)) as Decoder to generate description,
    which is a pioneering work of image captioning using deep learning. Since then,
    Deep Learning methods based on Encoder-Decoder structure have become the basic
    framework of image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: In the past few years, a large number of research works based on Deep Learning
    methods were published. Many useful improvements are proposed based on Encoder-Decoder
    structure, such as semantic attention You et al. ([2016](#bib.bib28)), visual
    sentinel Lu et al. ([2017](#bib.bib17)), and review network Yang et al. ([2016](#bib.bib26)).
    We divide them into (1) Improvements in Encoder (2) Improvements in Decoder and
    (3) Other Improvements.
  prefs: []
  type: TYPE_NORMAL
- en: The main contributions of this paper include:(1) introduced and analyzed traditional
    methods such as Retrieval-Based and Template-Based methods; (2) provided an overview
    of Encoder-Decoder structure; (3)summarized improvements in Encoder and Decoder
    for image captioning; (4)discussed and proposed future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this paper is organized as follows. Section 2 introduces the traditional
    image captioning methods. Section 3 focuses on the improvements in Encoder-Decoder.
    Section 4 and 5 introduce the existing standard Datasets and evaluation metrics.
    Section 6 discusses the future research directions. Section 7 gives the conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Traditional Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper mainly focuses on deep learning methods. Hence, in this part, we
    only briefly review retrieval-based and template-based methods as traditional
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45033adeb54fc55c9b82bf134e835d3c.png)![Refer to caption](img/47ec25feb38face042b9eeaf51708e36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The fundamental process of traditional methods: Reterival-Based Method
    (top). Template-Based Method (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Retrieval-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a given image, the retrieval-based image captioning methods aim to retrieve
    the matching sentence(s) from a set of given image-description pairs as the language
    description of the image, see Figure 1 (top). Therefore, the quality of this method
    depends on not only the diversity of image-description pairs but also the image-based
    retrieval algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Ordonez et al. ([2011](#bib.bib18)) firstly retrieves a series of related images
    from the image-description pairs by Gist and Tiny image descriptors, then detects
    and classifies the query images by specific objects and scenes, and reorders the
    retrieved images in turn, choosing the description of the first image to be ranked
    as the description of the query image. This method can be regarded as retrieval
    in visual space.
  prefs: []
  type: TYPE_NORMAL
- en: Hodosh et al. ([2015](#bib.bib10)) regards image captioning as a ranking task,
    and KCCA (A kernelized version of canonical correlation analysis) is employed
    to project images and descriptions into a common multimodal space. Then the query
    image is projected into the multimode space, and the Cosine similarity between
    the query image and the descriptions in datasets are calculated. The top rank
    is accepted as the description of the query image. However, KCCA is only suitable
    for small datasets, which can affect the performance of this method. This method
    can be regarded as retrieval in multimodal space.
  prefs: []
  type: TYPE_NORMAL
- en: But, the shortcomings of the retrieval-based method are also explicit. The quality
    of the description generated by this method depends extensively on the given image-description
    pool. The image-description pairs are established artificially, so it is sufficient
    to ensure the fluency of the description sentence and the accuracy of the grammar;
    however, to ensure the accuracy of the description content and semantics, the
    pre-given image-description pairs need to be large sufficient to cover enough
    rich scenes. The limitation of this method may not suit the object and scene of
    new images correctly, so it also limits the generalisation performance of this
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Template-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a given image, the template-based image captioning method usually requires
    to extract some objects, attributes or semantic information from the image, and
    then uses a specified grammar rule to combine the information or fills the obtained
    data into the pre-defined blanks of the sentence template to form the image description,
    see Figure 1 (bottom).
  prefs: []
  type: TYPE_NORMAL
- en: Li et al. ([2011](#bib.bib14)) firstly uses an image recogniser to obtain visual
    information from the image, including objects, attributes of objects and spatial
    relationships between different objects. And then they encode the information
    as a triple form of $<<$adj1, obj1 $>$, prep, $<$adj2, obj2 $>>$. Furthermore,
    an approach based on web-scale n-gram is used to get the frequency counts of all
    possible n-gram sequences ($1\leq n\leq 5$). Finally, the phrases are selected
    and fused, and the best combination is accepted as the description of the query
    image by the dynamic programming algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Kulkarni et al. ([2011](#bib.bib13)) uses an object detector to detect objects
    in the image, and then sends candidate object regions into attribute classifier
    and prepositional relation function to obtain attribute information of candidate
    objects and prepositional relation information between objects. Furthermore, a
    Conditional Random Field (CRF) is constructed to deduce the relevant information
    previously obtained for final use.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with retrieval-based methods, template-based methods can also generate
    grammatically correct description statements, and because this method needs to
    detect objects from the image, the generated description is more relevant to the
    image to some extent. But, the deficiencies of template-based methods are also
    apparent. On one hand, sentence templates or grammar rules need to be pre-designed
    artificially, so this method can not generate variable-length sentences, which
    limits the diversity of descriptions between different images, and descriptions
    may seem rigid and unnatural; On the other hand, the performance of the object
    detector limits the accuracy of image description, so the generated description
    may omit the details of the query image.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, deep learning methods have made significant progress in CV
    and NLP. Inspired by machine translation Sutskever et al. ([2014](#bib.bib21)),
    the Encoder-Decoder structure is also applied to image captioning. Usually, CNN
    is used to construct an Encoder to extract and encode information from images.
    RNN is used to construct a Decoder to generate descriptions. On this basis, many
    researchers have also proposed various efficient improvement methods, but they
    have different focuses. Therefore, we divide them into multiple sub-categories
    according to the improved focus, then introduce and discuss each sub-category
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Basic Encoder-Decoder structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Show and Tell Vinyals et al. ([2015](#bib.bib23)) is the first work to apply
    the Encoder-Decoder structure proposed in machine translation to image captioning.
    It also serves as the basis for subsequent improvements and a baseline model for
    performance comparison between models. The model structure is shown in Figure
    2 (top).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b0e531c92ca8aa7e7794c2befcfa250.png)![Refer to caption](img/1068558985fbd3be09071a67ea9228f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Models Based on Encoder-Decoder structure: Show and tell (top). Show,
    attend and tell (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: This model first uses the CNN as the Encoder part, encodes the image into a
    fixed-length vector representation as the image feature map, and then sends the
    image feature map to the Decoder part of the RNN to decode and generate an image
    description. It can be expressed as Eq.(1)-Eq.(3). The Encoder part is a CNN,
    which corresponds to GoogLeNet (Inception V3); the Decoder part is LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle x_{-1}$ | $\displaystyle={\rm Encoder}(I)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t}),$ | $\displaystyle
    t\in\{0,...,N-1\}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Suppose the vocabulary size is $D$, where $I$ represents the input image, $x_{-1}$
    is the feature map, which is only used to initialize the LSTM; $S_{t}$ is the
    one-hot vector in size $D$, representing the $t$-th word of the image description,
    and $S_{0}$ is the $<$START$>$ tag, $S_{N}$ is the $<$END$>$ tag; $W_{e}$ is the
    word embedding matrix; $p_{t+1}\in\mathbb{R}^{D}$ represents the probability vector
    generated by the $t+1$ time step, wherein the most probable one corresponds to
    the word as the time step word output.
  prefs: []
  type: TYPE_NORMAL
- en: Show Attend and Tell Xu et al. ([2015](#bib.bib25)) is an extension of Vinyals
    et al. ([2015](#bib.bib23)), which introduces a visual attention mechanism based
    on the Encoder-Decoder structure, which can dynamically focus on the salient regions
    of the image during the process of generating descriptions in Decoder. The model
    structure is shown in Figure 2 (bottom).
  prefs: []
  type: TYPE_NORMAL
- en: This model also uses a CNN as Encoder to extract $L$ vectors of $K$ dimensions
    from the image, each vector corresponds to a portion of the image. But unlike
    Vinyals et al. ([2015](#bib.bib23)), the model uses the underlying convolutional
    layer output instead of the final fully connected layer output as the image feature
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $a=\{a_{1},...,a_{L}\},a_{i}\in\mathbb{R}^{K}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: In the Decoder part, Xu et al. ([2015](#bib.bib25)) also uses LSTM for description
    generation. But this model needs to use the image-based feature vector $a$ for
    each time step $t$ to generate the context vector $z_{t}=\sum_{i=1}^{L}\alpha_{ti}a_{i}$.
    This is the embodiment of the attention mechanism, $\alpha_{t}\in\mathbb{R}^{L}$
    is the attention weight vector of the $t$ time step, which satisfies $\sum_{i=1}^{L}\alpha_{ti}=1$.
    $a$ can be predicted by the simple neural network $f_{\rm att}$ and the Softmax
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha_{ti}\propto\exp\{f_{\rm att}(a_{i},m_{t-1})\}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: Therefore, the attention Encoder-Decoder structure can be expressed as Eq.(6)-Eq.(9).
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle a$ | $\displaystyle={\rm Encoder}(I)$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i},$
    | $\displaystyle\alpha_{ti}\in\mathbb{R},a_{i}\in\mathbb{R}^{K}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=W_{e}S_{t},$ | $\displaystyle t\in\{0,...,N-1\}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p_{t+1}$ | $\displaystyle={\rm Decoder}(x_{t},z_{t}),$
    | $\displaystyle t\in\{0,...,N-1\}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: The above Eqs is the Soft attention mechanism proposed in the paper, details
    are shown in Figure 3 (left), and another Hard attention is also proposed. However,
    most of the improved models use Soft attention easy to implement, so only the
    Soft attention mechanism is introduced here.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Improvements in Encoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You et al. ([2016](#bib.bib28)) proposes a semantic attention model, in addition
    to using CNN’s intermediate activation output as the global feature of the image
    $v$, and also using a set of attribute detectors to extract $\{A_{i}\}$ the most
    likely to appear in the image. Each attribute $A_{i}$ corresponds to an entry
    in the vocabulary, so the model encodes the image as a collection of visual features
    and semantic features. Then adaptively process $\{A_{i}\}$ to calculate the input
    of the Decoder $x_{t}$ and get the current word output $p_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle v$ | $\displaystyle={\rm Encoder}(I)$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle={\rm Decoder}(h_{t-1},x_{t})$ |  |
    (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p_{t}$ | $\displaystyle=\varphi(h_{t},\{A_{i}\})$ |  |
    (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle x_{t}$ | $\displaystyle=\phi(p_{t-1},\{A_{i}\})$ |  | (13)
    |'
  prefs: []
  type: TYPE_TB
- en: Liu et al. ([2017](#bib.bib16)) returns the image captioning problem back to
    machine translation, which first uses the object detector to represent the image
    $I$ as the sequence of detection objects $seq(I)=\{O_{1},O_{2},...,O_{T_{A}}\}$,
    where $\{O_{1},...,O_{T_{A}-1}\}$ is the image objects feature representation,
    the last item $O_{T_{A}}$ is the global feature of the image; then applies the
    Sequence 2 Sequence framework in machine translation to $seq(I)$ to generate the
    image description $S=\{S_{1},S_{2},...,S_{T_{B}}\}$, Encoder and Decoder are implemented
    using LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h_{t_{E}}={\rm Encoder}(O_{t_{E}},h_{t_{E}-1}),t_{E}=1,2,...,T_{A}$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle d_{t_{D}}={\rm Decoder}(S_{t_{D}},d_{t_{D}-1}),t_{D}=1,2,...,T_{B}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: In addition, when $S_{T}$ is generated, the model applies the attention mechanism
    to generate $d_{t-1}^{\prime}$ on the Encoder hidden layer sequence output $h=\{h_{1},h_{2},...,h_{T_{A}}\}$.
    Then cocat $d_{t-1}^{\prime}$ and $d_{t}$ use the Softmax activation function
    to generate the current $S_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: Chen et al. ([2017](#bib.bib5)) believes that CNN’s kernels can be used as pattern
    detectors, and each channel of image feature map is activated by the corresponding
    convolution kernel. Therefore, the application of attention mechanism on the channel
    can be regarded as a process of selecting image semantic attributes. They proposed
    SCA-CNN, which applies the attention mechanism to both space and channel. However,
    unlike the previous attention mechanism, when calculating the context vector,
    they only weight the region features without summing, which can ensure that the
    feature vector and the context vector are the same sizes, so the SCA-CNN can be
    embedded in the stack multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Fu et al. ([2017](#bib.bib8)) introduced advanced semantic information to improve
    image description based on attention. Firstly, the object detection generates
    a series of candidate regions, and a two-classifier is used to classify the candidate
    regions (good/bad). Finally, the first 29 regions and the image global region
    are selected as the visual feature information. The attention mechanism generates
    a context vector $z_{t}$. In addition, they use LDA to model all descriptions
    in the dataset to map the images flexibly to 80-Dimensional topic vectors (corresponding
    to the implicit 80 scene categories) and then train a multi-layer perceptron to
    predict the scene context vector $s$ to better generate image descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Yao et al. ([2018](#bib.bib27)) believes that the semantic relationship and
    spatial relationship between image objects are helpful for image description generation.
    They first use the object detection module Faster R-CNN Ren et al. ([2015](#bib.bib20))
    to detect objects in the image, and represent the image as $K$ image saliency
    area containing the object $V=\{v_{i}\}_{i=1}^{K}$; Then use a simple classification
    network to predict the semantic relationship between the objects and construct
    a semantic relationship graph $\mathcal{G}_{sem}=(V,\varepsilon_{sem})$, and construct
    a spatial relationship graph $\mathcal{G}_{spa}=(V,\varepsilon_{spa})$ by using
    the positional relationship of the object area. Then they design a [GCN]-based
    image Encoder to fuse the semantic and spatial relationships between the objects
    to obtain visual features $V^{(1)}=\{v_{i}^{(1)}\}_{i=1}^{K}$ containing more
    information.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the above, the original intention of improving Encoder is
    mostly to extract more useful information from images, such as adding semantic
    information on the basis of visual information, replacing the original CNN response
    activation region with the object detection module. Therefore, these methods have
    improved the image description effect, but there are also some inherent defects.
    On one hand, object detection may affect the efficiency of image description generation,
    on the other hand, it is difficult to effectively interpret the reliability of
    the semantic information of the image acquired implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84c437269faa531571c909aa4337d7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Attention Language Model Details.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Improvements in Decoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lu et al. ([2017](#bib.bib17)) believes that in the process of generating image
    description, visual attention should not be added to non-visual words such as
    prepositions and quantifiers. So they introduced a visual sentinel in Decoder,
    essentially adding a gating to the LSTM for generating the sentinel vector $s_{t}$
    for each time step. In addition, they think that visual attention should be more
    relevant to the current time step hidden layer state of LSTM, so the visual attention
    mechanism is improved compared to Xu et al. ([2015](#bib.bib25)), see Figure 3
    (center). When visual attention weights $\alpha_{t}$ are generated, the weight
    value $\beta_{t}$ is calculated to determine whether to visually focus on the
    image. Therefore the context vector for each time step is calculated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm Vatt}(a_{i},m_{t})\}$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\beta_{t}$ | $\displaystyle\propto\exp\{f_{\rm Satt}(s_{t},m_{t})\}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\beta_{t}s_{t}+(1-\beta_{t})\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: Anderson et al. ([2018](#bib.bib2)) combines Bottom-Up and Top-Down attention.
    Firstly, based on Faster R-CNN as Bottom-Up attention model, a variable-size image
    feature set,$V=\{v_{i}\}_{i=1}^{K}$, is obtained. Each feature is the encoding
    of a salient region of the image. Decoder used to generate language descriptions
    uses a two-tier LSTM structure, see Figure 3 (right). The first LSTM acts as Top-Down
    attention layer, which applies attention mechanism on hidden layer output and
    visual feature $V$ to calculates context vector $z_{t}$. Then it is fed into the
    second LSTM and delivers the output of the second LSTM to Softmax classifier to
    generate the current time step word prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Zhou et al. ([2017](#bib.bib30)) pointed out that in previous work, image features
    are only initially fed into LSTM, or on the basis of which attention mechanism
    is introduced to compute context vectors to input LSTM. Whether text context could
    be used to improve image description performance has not been solved yet, that
    is, the relationship between generated words and visual information was not involved.
    To explore this problem, they proposed a Text-Conditional attention mechanism,
    which allows attention to focus on image features related to previously generated
    words. They fused the previously generated words with global image features $I$
    to generate context vector $z_{t}$, and then input them to LSTM to generate words
    $S_{t+1}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}S_{t})$ |  | (19)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z_{t}$ | $\displaystyle=\phi(I\odot W_{C}\sum_{k=1}^{t}\frac{S_{k-1}}{t})$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: Eq.(19) is Text-Conditional attention in the form of 1-gram, and the context
    information is limited to the previous word; Eq.(20) is an extreme form, and the
    context information takes advantage of all the previously generated words.
  prefs: []
  type: TYPE_NORMAL
- en: In most work, RNN in one or two layers is used as a language model to generate
    descriptive words. Fang et al. ([2018](#bib.bib7)) thinks that this structure
    can deal with visual words such as nouns more easily, but it may not be able to
    learn verbs and adjectives. Therefore, they proposed a deep attention language
    model based on multi-layer LSTM, which can learn more abstract word information,
    and design three overlapping methods to generate attention context vectors.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM is often used as Decoder part in image captioning tasks, but LSTM is relatively
    complex and can not be performed in parallel. Aneja et al. ([2018](#bib.bib3))
    and Wang and Chan ([2018](#bib.bib24)) proposed that CNN is used as Decoder part
    to generate image description, which can achieve the same effect as LSTM and greatly
    improve the computing speed.
  prefs: []
  type: TYPE_NORMAL
- en: When using RNN (e.g. LSTM and GRU) as Decoder to generate description, Decoder’s
    input, hidden states and output are usually expressed as 1-D vectors. Dai et al.
    ([2018](#bib.bib6)) considers that 2-D feature mapping is more effective in interpretation
    and convenient for visual analysis to study the relationship between input visual
    information and output descriptive words; secondly, 2-D features can retain important
    spatial structure information. Therefore, they proposed to design Decoder on 2-D
    feature maps. Firstly, CNN is used to transform an image into multi-channel 2-D
    feature mapping. Decoder still uses GRU structure, but the state mapping transformations
    is replaced by convolution operations.
  prefs: []
  type: TYPE_NORMAL
- en: The above works show that the improvement of Decoder mainly focuses on the richness
    of information and the correctness of the attention when generating the description.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Other Improvements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the basis of Encoder and Decoder, Yang et al. ([2016](#bib.bib26)) introduced
    a Reviewer module, which is essentially an improved LSTM unit introducing attention
    mechanism. It is used to perform multiple Reviews on the local features of Encoder
    output, and calculate a fact vector $f_{t}$ at each step as the input of attention
    module in Decoder. The author considers that the fact vector extracted by Reviewer
    module is more compact and abstract than the image feature maps obtained by Encoder.
    Therefore, the visual attention of the model is applied to the Reviewer module,
    while the Decoder module applies the attention mechanism to the fact vector.
  prefs: []
  type: TYPE_NORMAL
- en: Two forms of Reviwer module are introduced in this paper. One is Attention Input
    Reviewer, which first applies the attention mechanism to the image region features
    $a$ and then uses the attention output as the input of LSTM unit to generate the
    fact vector $f_{t}$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\alpha_{ti}$ | $\displaystyle\propto\exp\{f_{\rm att}(a_{i},f_{t-1})\}$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\tilde{f}_{t}$ | $\displaystyle=\sum_{i=1}^{L}\alpha_{ti}a_{i}$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(\tilde{f}_{t},f_{t-1})$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: Another one is Attention Output Reviewer, which also applies attention mechanism
    to image region features, but uses zero vector as input of LSTM unit, fact vector
    is calculated as the sum of LSTM output and attention output,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle={\rm LSTM_{R}}(0,f_{t-1})+W\tilde{f}_{t}$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: Inspired by Yang et al. ([2016](#bib.bib26)), Jiang et al. ([2018](#bib.bib11))
    designs a Guiding Network based on a simple neural network in Encoder and Decoder
    structure. The region feature set of the image is used as input to generate a
    guidance vector $v$ containing the global information of the image. The guidance
    vector $v$ will then be fused with the original input of the Decoder to ensure
    that richer image information is input when generating image descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Flickr30K | MS COCO |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | B-1 | B-2 | B-3 | B-4 | MT | CD | B-1 | B-2 | B-3 | B-4 | MT |
    CD | SP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Vinyals et al. ([2015](#bib.bib23)) | 66.3 | 42.3 | 27.7 | 18.3 | - | - |
    66.6 | 46.1 | 32.9 | 24.6 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. ([2015](#bib.bib25)) | 66.7 | 43.4 | 28.8 | 19.1 | 18.49 | - |
    70.7 | 49.2 | 34.4 | 24.3 | 23.9 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| You et al. ([2016](#bib.bib28)) | 64.7 | 46.0 | 32.4 | 23.0 | 18.9 | - |
    70.9 | 53.7 | 40.2 | 30.4 | 24.3 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2017](#bib.bib16)) | - | - | - | - | - | - | 73.1 | 56.7 | 42.9
    | 32.3 | 25.8 | 105.8 | 18.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([2017](#bib.bib5)) | 66.2 | 46.8 | 32.5 | 22.3 | 19.5 | - |
    71.9 | 54.8 | 41.1 | 31.1 | 25.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Fu et al. ([2017](#bib.bib8)) | 64.9 | 46.2 | 32.4 | 22.4 | 19.4 | 47.2 |
    72.4 | 55.5 | 41.8 | 31.3 | 24.8 | 95.5 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Yao et al. ([2018](#bib.bib27)) | - | - | - | - | - | - | 77.4 | - | - |
    37.1 | 28.1 | 117.1 | 21.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Anderson et al. ([2018](#bib.bib2)) | - | - | - | - | - | - | 77.2 | - |
    - | 36.2 | 27.0 | 113.5 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. ([2017](#bib.bib30)) | - | - | - | - | - | - | 71.6 | 54.5 |
    40.5 | 30.1 | 24.7 | 97.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Fang et al. ([2018](#bib.bib7)) | - | - | 32.8 | 23.4 | 18.7 | 43.7 | - |
    - | 44.2 | 34.0 | 26.4 | 105.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Aneja et al. ([2018](#bib.bib3)) | - | - | - | - | - | - | 71.1 | 53.8 |
    39.4 | 28.7 | 24.4 | 91.2 | 17.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Chan ([2018](#bib.bib24)) | 60.7 | 42.5 | 29.2 | 19.9 | 19.1 | 39.5
    | 68.5 | 51.1 | 36.9 | 26.7 | 23.4 | 84.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. ([2018](#bib.bib6)) | - | - | - | 22.0 | - | 42.7 | - | - | -
    | 31.9 | - | 99.4 | 18.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Evaluation results of some models. B-n, MT, CD, SP stand for BLEU-n,
    METEOR, CIDEr and SPICE respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image captioning based on deep learning methods requires a lot of label data.
    Fortunately, many researchers and research organization have collected and tagged
    data sets. Here we mainly introduce four common data sets: Flickr 8K Hodosh et
    al. ([2015](#bib.bib10)), Flickr 30K Young et al. ([2014](#bib.bib29)), MS COCO
    Lin et al. ([2014](#bib.bib15)) and Visual GenomeKrishna et al. ([2017](#bib.bib12)).'
  prefs: []
  type: TYPE_NORMAL
- en: Flickr8K Hodosh et al. ([2015](#bib.bib10)) contains a total of 8092 images,
    which were collected from Flickr.com and captions were obtained through crowdsourcing
    services provided by Amazon Mechanical Turk. Each image contains five different
    captions for reference with an average length of 11.8 words, and these descriptions
    are required to accurately describe the objects, scenes, and activities displayed
    in the image. In practical applications, 8000 images are usually selected, of
    which 6000 for train, 1000 for verification, and 1000 for test.
  prefs: []
  type: TYPE_NORMAL
- en: Flickr30K Young et al. ([2014](#bib.bib29)) is an extension to Flickr8K. It
    contains 31,783 images (including 8092 images in Flickr8K) and 158,915 descriptions.
    An annotation guide similar to Flickr8K is used to obtain image descriptions,
    control description quality, and correct description errors. Usually, 1000 images
    are selected as validation data, 1000 images as test data, and the remaining images
    are used as train data.
  prefs: []
  type: TYPE_NORMAL
- en: MicroSoft COCO Lin et al. ([2014](#bib.bib15)) is a large-scale dataset that
    can be used for object detection, instance segmentation, and image captioning.
    It is also the most popular dataset in image captioning. The dataset contains
    91 object categories, a total of 328K images, 2.5 million tag instances, and each
    image contains 5 descriptions. The dataset is divided into two parts. The part
    released in 2014 includes 82,783 train data, 40,504 validation data and 40,775
    test data. However, the description of the test set is not publicly available,
    so the train set data and the validation set data are often re-divided into training/validation/test
    set in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Genome Krishna et al. ([2017](#bib.bib12)) contains more than 108K images.
    Each image contains an average of 35 objects with dense description annotations,
    26 attributes and 21 interactions between objects. Therefore, Visual Genome dataset
    can be used to pre-train image captioning tasks that introduce spatial and semantic
    relationships between objects.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BLEUPapineni et al. ([2002](#bib.bib19)) is the most commonly used evaluation
    metric in image captioning tasks. It was originally used to measure the quality
    of machine translation. The core idea of BLEU is that ”the closer the test sentences
    are to the reference sentences, the better”. In other words, BLEU is evaluated
    by comparing the similarity of the test sentences and the reference sentences
    at the n-gram level. Therefore, this method does not consider the grammatical
    correctness, synonyms, similar expressions, and is more credible only in the case
    of shorter sentences.
  prefs: []
  type: TYPE_NORMAL
- en: METEOR Banerjee and Lavie ([2005](#bib.bib4)) is also a commonly used evaluation
    metric for machine translation. Firstly, test sentences are aligned with reference
    sentences, such as word precise matching, stemmer-based matching, synonym matching
    and alignment based on WordNet, etc. Then, similarity scores between the test
    and the reference sentences are calculated based on alignment results. The calculation
    of similarity scores involves such indicators as matching word accuracy and recall
    rate. This method solves some shortcomings of BLEU and can express better relevance
    at the sentence level.
  prefs: []
  type: TYPE_NORMAL
- en: CIDEr Vedantam et al. ([2015](#bib.bib22)) is an evaluation metric aiming at
    image captioning. The authors think that the past evaluation metrics have a strong
    correlation with human, but they can not evaluate the similarity between them
    and human. So they proposed Consensus-based evaluation metric. Each sentence is
    regarded as a ”document” and expressed as a TF-IDF vector. The weight of TF-IDF
    is calculated for each n-gram, and then the cosine similarity between the test
    sentences and the reference sentences is calculated for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: SPICE Anderson et al. ([2016](#bib.bib1)) is also an evaluation metric designed
    for image captioning. The metric codes the objects, attributes and relationships
    in image description into a semantic graph. This method captures the human’s judgment
    of model generation description better than the existing n-gram based evaluation
    metrics and can reflect the advantages and disadvantages of the language model
    more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion & Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evaluation results of some deep learning methods are shown in Table 1, which
    shows that deep learning methods have achieved great success in image captioning
    tasks. In the previous part, we mainly discussed the improved model based on Encoder-Decoder
    structure. The emphasis of different improvements is different, but most of them
    aim to enrich the visual feature information of images, which is also a common
    original intention of them. For example, the improvement of Encoder includes extracting
    more accurate salient region features from images by object detection, enriching
    visual information of images by extracting semantic relations between salient
    objects from images, and implicitly extracting a scene vector from images to guide
    the generation of descriptions, all of which are in order to obtain richer and
    more abstract information from images or obtain additional information. Further
    improvements of Decoder include increasing the use of previously generated descriptive
    words, adding control gates to language models to ensure proper application of
    attention mechanisms, and implicitly increasing the number of layers of LSTM to
    obtain more abstract information.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, image captioning is far from the human level, so there is still
    much space for improvement. On one hand, we can continue to study how to extract
    richer visual information from images or combine the extracted feature maps into
    more abstract information to enhance the context features of Decoder. Such as
    introducing semantic segmentation into Encoder part and using the latest language
    models as Decoder; on the other hand, I think we can deepen the development of
    datasets. Existing image captioning datasets only correspond images and descriptions,
    regions of interest of descriptions and how to generate descriptions are not reflected.
    If the development of datasets can be strengthened, more monitoring information
    can be introduced into the training of models, which may improve the performance
    of image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, image captioning based on deep learning methods is summarized.
    Firstly, traditional template-based and retrieval-based methods are briefly introduced.
    Secondly, the deep learning methods and their improvements based on Encoder-Decoder
    structure in recent years are mainly introduced. According to the emphasis on
    improvements, these improvements are divided into three parts: Encoder Improvements,
    Decoder Improvements, and Other Improvements. Then, we introduce the commonly
    used datasets and evaluation metrics in image captioning. Although image captioning
    based on deep learning has been improved, they also have much space for improvements.
    So finally, we summarize the results of some deep learning methods and forecast
    future research directions.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anderson et al. [2016] P. Anderson, B. Fernando, M. Johnson, and S. Gould.
    SPICE: semantic propositional image caption evaluation. In ECCV, pages 382–398,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson et al. [2018] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,
    S. Gould, and L. Zhang. Bottom-up and top-down attention for image captioning
    and visual question answering. In CVPR, pages 6077–6086, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aneja et al. [2018] J. Aneja, A. Deshpande, and A. G. Schwing. Convolutional
    image captioning. In CVPR, pages 5561–5570, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie [2005] S. Banerjee and A. Lavie. METEOR: an automatic metric
    for MT evaluation with improved correlation with human judgments. In Proceedings
    of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation,
    pages 65–72, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2017] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and
    T. Chua. SCA-CNN: spatial and channel-wise attention in convolutional networks
    for image captioning. In CVPR, pages 6298–6306, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dai et al. [2018] B. Dai, D. Ye, and D. Lin. Rethinking the form of latent states
    in image captioning. In ECCV, pages 294–310, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fang et al. [2018] F. Fang, H. Wang, Y. Chen, and P. Tang. Looking deeper and
    transferring attention for image captioning. Multimedia Tools Appl., 77(23):31159–31175,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2017] K. Fu, J. Jin, R. Cui, F. Sha, and C. Zhang. Aligning where
    to see and what to tell: Image captioning with region-based attention and scene-specific
    contexts. IEEE Trans. Pattern Anal. Mach. Intell., 39(12):2321–2334, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber [1997] S. Hochreiter and J. Schmidhuber. Long short-term
    memory. Neural Computation, 9(8):1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hodosh et al. [2015] M. Hodosh, P. Young, and J. Hockenmaier. Framing image
    description as a ranking task: Data, models and evaluation metrics (extended abstract).
    In IJCAI, pages 4188–4192, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. [2018] W. Jiang, L. Ma, X. Chen, H. Zhang, and W. Liu. Learning
    to guide decoding for image captioning. In AAAI, pages 6959–6966, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. [2017] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
    S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei.
    Visual genome: Connecting language and vision using crowdsourced dense image annotations.
    International Journal of Computer Vision, 123(1):32–73, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. [2011] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
    Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions.
    In CVPR, pages 1601–1608, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2011] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing
    simple image descriptions using web-scale n-grams. In CoNLL, pages 220–228, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2014] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
    P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. In ECCV,
    pages 740–755, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2017] C. Liu, F. Sun, C. Wang, F. Wang, and A. L. Yuille. MAT:
    A multimodal attentive translator for image captioning. In IJCAI, pages 4033–4039,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. [2017] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when to
    look: Adaptive attention via a visual sentinel for image captioning. In CVPR,
    pages 3242–3250, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ordonez et al. [2011] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing
    images using 1 million captioned photographs. In NIPS, pages 1143–1151, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. [2002] K. Papineni, S. Roukos, T. Ward, and W. Zhu. Bleu: a
    method for automatic evaluation of machine translation. In ACL, pages 311–318,
    2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. [2015] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN towards
    real-time object detection with region proposal networks. In NIPS, pages 91–99,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. [2014] I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence
    learning with neural networks. In NIPS, pages 3104–3112, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. [2015] R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based
    image description evaluation. In CVPR, pages 4566–4575, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. [2015] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show
    and tell: A neural image caption generator. In CVPR, pages 3156–3164, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Chan [2018] Q. Wang and A. B. Chan. CNN+CNN: convolutional decoders
    for image captioning. CoRR, abs/1805.09019, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2015] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov,
    R. S. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation
    with visual attention. In ICML, pages 2048–2057, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2016] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. Salakhutdinov.
    Review networks for caption generation. In NIPS, pages 2361–2369, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. [2018] T. Yao, Y. Pan, Y. Li, and T. Mei. Exploring visual relationship
    for image captioning. In ECCV, pages 711–727, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You et al. [2016] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image captioning
    with semantic attention. In CVPR, pages 4651–4659, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Young et al. [2014] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image
    descriptions to visual denotations: New similarity metrics for semantic inference
    over event descriptions. TACL, 2:67–78, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. [2017] L. Zhou, C. Xu, P. A. Koch, and J. J. Corso. Watch what
    you just said: Image captioning with text-conditional attention. In Proceedings
    of the on Thematic Workshops of ACM Multimedia, pages 305–313, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
