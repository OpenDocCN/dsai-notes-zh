- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:54:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:54:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2105.12694] Deep Learning for Weakly-Supervised Object Detection and Object
    Localization: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2105.12694] 弱监督目标检测与目标定位的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.12694](https://ar5iv.labs.arxiv.org/html/2105.12694)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2105.12694](https://ar5iv.labs.arxiv.org/html/2105.12694)
- en: 'Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《弱监督目标检测与目标定位的深度学习：综述》
- en: 'Feifei Shao, Long Chen, Jian Shao  Wei Ji, Shaoning Xiao, Lu Ye, Yueting Zhuang 
    Jun Xiao Feifei Shao, Jian Shao, Shaoning Xiao, Yueting Zhuang, Jun Xiao are with
    Zhejiang University, Hangzhou, 310027, China. Emails: {sff, jshao, shaoningx,
    yzhuang, junx}@zju.edu.cnLong Chen is with Columbia University, New York, 10027,
    USA. This work was partially done when L. Chen was a Ph.D. student at Zhejiang
    University. Email: zjuchenlong@gmail.com (Corresponding author).Wei Ji is with
    National University of Singapore, 117417, Singapore. Email: weiji0523@gmail.comLu
    Ye is with Zhejiang University of Science and Technology, Hangzhou, 310023, China.
    Email: yelue@zust.edu.cn'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**费飞·邵**、**龙·陈**、**建·邵**、**伟·季**、**少宁·肖**、**陆·叶**、**悦婷·庄**、**君·肖** 这些作者均来自中国杭州的浙江大学，邮政编码
    310027。电子邮件：{sff, jshao, shaoningx, yzhuang, junx}@zju.edu.cn。**龙·陈** 现为美国纽约的哥伦比亚大学的研究员，本工作部分完成时
    **L. Chen** 是浙江大学的博士生。电子邮件：zjuchenlong@gmail.com（通讯作者）。**伟·季** 现为新加坡国立大学的研究员，邮政编码
    117417。电子邮件：weiji0523@gmail.com。**陆·叶** 现为中国杭州的浙江科技学院的研究员，邮政编码 310023。电子邮件：yelue@zust.edu.cn'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e., detecting
    multiple and single instances with bounding boxes in an image using image-level
    labels, are long-standing and challenging tasks in the CV community. With the
    success of deep neural networks in object detection, both WSOD and WSOL have received
    unprecedented attention. Hundreds of WSOD and WSOL methods and numerous techniques
    have been proposed in the deep learning era. To this end, in this paper, we consider
    WSOL is a sub-task of WSOD and provide a comprehensive survey of the recent achievements
    of WSOD. Specifically, we firstly describe the formulation and setting of the
    WSOD, including the background, challenges, basic framework. Meanwhile, we summarize
    and analyze all advanced techniques and training tricks for improving detection
    performance. Then, we introduce the widely-used datasets and evaluation metrics
    of WSOD. Lastly, we discuss the future directions of WSOD. We believe that these
    summaries can help pave a way for future research on WSOD and WSOL.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督目标检测（WSOD）和定位（WSOL），即利用图像级标签在图像中检测多个和单个实例的边界框，是计算机视觉领域长期存在且具有挑战性的任务。随着深度神经网络在目标检测中的成功，WSOD
    和 WSOL 都受到了前所未有的关注。在深度学习时代，已经提出了数百种 WSOD 和 WSOL 方法以及大量技术。为此，本文将 WSOL 视为 WSOD 的一个子任务，并对
    WSOD 的最新成果进行了全面的综述。具体来说，我们首先描述了 WSOD 的定义和设置，包括背景、挑战和基本框架。同时，我们总结并分析了所有先进的技术和提高检测性能的训练技巧。然后，我们介绍了
    WSOD 中广泛使用的数据集和评估指标。最后，我们讨论了 WSOD 的未来方向。我们相信，这些总结能够为未来的 WSOD 和 WSOL 研究铺平道路。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Weakly-Supervised Object Detection, Weakly-Supervised Object Localization, Basic
    Framework, Techniques, Future Directions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督目标检测，弱监督目标定位，基本框架，技术，未来方向。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Object detection [[1](#bib.bib1), [2](#bib.bib2)] is a fundamental and challenging
    task that aims to locate and classify object instances in an image. The object
    localization is to use a bounding box (an axis-aligned rectangle tightly bounding
    the object) to search for the spatial location and range of an object in an image
    as much as possible [[3](#bib.bib3), [4](#bib.bib4)]. Object classification is
    to assess the presence of objects from a given set of object classes in an image.
    As one of the most fundamental tasks in computer vision, object detection is an
    indispensable technique for many high-level applications, e.g., robot vision [[5](#bib.bib5)],
    face recognition [[6](#bib.bib6)], image retrieval [[7](#bib.bib7), [8](#bib.bib8)],
    augmented reality [[9](#bib.bib9)], autonomous driving [[10](#bib.bib10)], change
    detection [[11](#bib.bib11)] and so on. With the development of convolutional
    neural networks (CNNs) in visual recognition [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)] and release of large scale dateset [[4](#bib.bib4), [15](#bib.bib15)],
    today’s state-of-the-art object detector can achieve near-perfect performance
    under fully-supervised setting, i.e., Fully-Supervised Object Detection (FSOD) [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Unfortunately, these fully-supervised object detection methods suffer from two
    inevitable limitations: 1) The large-scale instance annotations are difficult
    to obtain and labor-intensive. 2) When labeling these data, they may introduce
    annotation noises inadvertently.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测 [[1](#bib.bib1), [2](#bib.bib2)] 是一项基础而富有挑战性的任务，旨在定位和分类图像中的目标实例。目标定位是使用边界框（一个紧密包围目标的轴对齐矩形）尽可能多地搜索图像中目标的空间位置和范围
    [[3](#bib.bib3), [4](#bib.bib4)]。目标分类是评估图像中是否存在给定类别的目标。作为计算机视觉中的核心任务之一，目标检测是许多高级应用的**不可或缺**的技术，例如机器人视觉
    [[5](#bib.bib5)]、人脸识别 [[6](#bib.bib6)]、图像检索 [[7](#bib.bib7), [8](#bib.bib8)]、增强现实
    [[9](#bib.bib9)]、自动驾驶 [[10](#bib.bib10)]、变化检测 [[11](#bib.bib11)] 等。随着卷积神经网络（CNNs）在视觉识别中的发展
    [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] 和大规模数据集的发布 [[4](#bib.bib4),
    [15](#bib.bib15)]，目前最先进的目标检测器在完全监督设置下可以实现近乎完美的性能，即完全监督的目标检测（FSOD） [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)]。不幸的是，这些完全监督的目标检测方法存在两个不可避免的限制：1）大规模实例注释难以获取且劳动密集。2）标注这些数据时可能会无意中引入注释噪声。
- en: 'To avoid the mentioned problems, the community starts to solve object detection
    in a weakly-supervised setting, i.e., Weakly-Supervised Object Detection (WSOD).
    Different from the fully-supervised setting (cf. Fig. [1](#S1.F1 "Figure 1 ‣ I
    Introduction ‣ Deep Learning for Weakly-Supervised Object Detection and Object
    Localization: A Survey") (a)), WSOD aims to detect instances with only image-level
    labels (e.g., categories of instances in the whole images). Meanwhile, WSOD can
    benefit from the large-scale datasets on the web, such as Facebook and Twitter.
    Another similar task is Weakly-Supervised Object Localization (WSOL), which only
    detects one instance in an image. Because WSOD and WSOL detect multiple instances
    and single instances respectively, we consider WSOL as a sub-task of WSOD. In
    the following paper, we use WSOD to represent both WSOD and WSOL.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免上述问题，社区开始在弱监督设置下解决目标检测，即弱监督的目标检测（WSOD）。与完全监督设置不同（见图[1](#S1.F1 "图1 ‣ I 引言
    ‣ 深度学习在弱监督目标检测和目标定位中的应用：综述")（a）），WSOD旨在仅使用图像级标签（例如，整幅图像中的实例类别）来检测实例。同时，WSOD可以利用来自网络的大规模数据集，如Facebook和Twitter。另一个类似的任务是弱监督的目标定位（WSOL），其仅检测图像中的一个实例。由于WSOD和WSOL分别检测多个实例和单个实例，我们将WSOL视为WSOD的一个子任务。在以下论文中，我们使用WSOD来代表WSOD和WSOL。
- en: '![Refer to caption](img/0d604d432d8a56c86c097e52f1d7a474.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/0d604d432d8a56c86c097e52f1d7a474.png)'
- en: 'Figure 1: (a) Fully-Supervised Object Detection (FSOD) uses the *instance-level*
    annotations as supervision. (b) Weakly-Supervised Object Detection (WSOD) uses
    the *image-level* annotations as supervision.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）完全监督的目标检测（FSOD）使用*实例级*注释作为监督。（b）弱监督的目标检测（WSOD）使用*图像级*注释作为监督。
- en: '![Refer to caption](img/3adc94595f7ca7b2ad3c22eda7a5e709.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/3adc94595f7ca7b2ad3c22eda7a5e709.png)'
- en: 'Figure 2: The main content of this paper.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：本文的主要内容。
- en: 'In this paper, we go over all typical WSOD methods and give a comprehensive
    survey (cf. Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey")) of recent advances in WSOD.
    Since the number of papers on WSOD is breathtaking, we sincerely apologize to
    those authors whose research on WSOD and other related fields are not included
    in this survey. In Section [II](#S2 "II WSOD ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we introduce the background,
    main challenges, and basic framework. In Section [III](#S3 "III Milestones of
    WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey"), according to the development timeline of WSOD, we introduce several
    modern classical methods in detail. Then, in-depth analyses are provided towards
    the all advanced techniques and tricks for the main challenges. In Section [VIII](#S8
    "VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we demonstrate all prevailing
    benchmarks and standard evaluation metrics for WSOD. In Section [IX](#S9 "IX Future
    Directions and Tasks ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey"), we briefly discuss the future directions.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了所有典型的 WSOD 方法，并对 WSOD 的最新进展进行了全面综述（参见图 [2](#S1.F2 "图 2 ‣ I 引言 ‣ 深度学习在弱监督目标检测与定位中的应用：综述")）。由于
    WSOD 论文数量庞大，我们对未能纳入本综述的 WSOD 及其他相关领域的研究者表示诚挚的歉意。在第 [II](#S2 "II WSOD ‣ 深度学习在弱监督目标检测与定位中的应用：综述")
    节中，我们介绍了背景、主要挑战和基本框架。在第 [III](#S3 "III WSOD 的里程碑 ‣ 深度学习在弱监督目标检测与定位中的应用：综述") 节中，我们根据
    WSOD 的发展时间线详细介绍了几种现代经典方法。接着，对主要挑战的所有先进技术和技巧进行了深入分析。在第 [VIII](#S8 "VIII 数据集与性能评估
    ‣ 深度学习在弱监督目标检测与定位中的应用：综述") 节中，我们展示了 WSOD 的所有流行基准和标准评估指标。在第 [IX](#S9 "IX 未来方向与任务
    ‣ 深度学习在弱监督目标检测与定位中的应用：综述") 节中，我们简要讨论了未来的研究方向。
- en: II WSOD
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II WSOD
- en: II-A A Problem Definition
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 问题定义
- en: 'WSOD aims to classify and locate object instances using only image-level labels
    in the training phase. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (b), given an image with cat and dog, WSOD not only classifies the
    cat and dog but also locates their location using bounding boxes. Different from
    FSOD that can use instance-level annotations in the training phase shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey") (a), WSOD only accesses image-level labels.
    Because of this restriction, though hundreds of WSOD methods have been proposed,
    the performance gap between WSOD and FSOD is still large. For example, the mAP
    of state-of-the-art FSOD approach [[22](#bib.bib22)] and WSOD approach [[23](#bib.bib23)]
    is 86.9% and 54.9% on PASCAL VOC 2007 dataset [[24](#bib.bib24)], respectively.
    Therefore, there are still many challenges in terms of the task of WSOD for researchers
    to solve, especially in the direction of improving the detection performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: WSOD 旨在仅使用图像级标签来分类和定位目标实例。如图 [1](#S1.F1 "图 1 ‣ I 引言 ‣ 深度学习在弱监督目标检测与定位中的应用：综述")
    (b) 所示，给定一张包含猫和狗的图像，WSOD 不仅对猫和狗进行分类，还使用边界框来定位它们的位置。与图 [1](#S1.F1 "图 1 ‣ I 引言 ‣
    深度学习在弱监督目标检测与定位中的应用：综述") (a) 所示的 FSOD 不同，后者可以在训练阶段使用实例级标注，而 WSOD 仅访问图像级标签。由于这一限制，尽管已有数百种
    WSOD 方法被提出，但 WSOD 与 FSOD 之间的性能差距仍然很大。例如，最先进的 FSOD 方法 [[22](#bib.bib22)] 和 WSOD
    方法 [[23](#bib.bib23)] 在 PASCAL VOC 2007 数据集上，mAP 分别为 86.9% 和 54.9% [[24](#bib.bib24)]。因此，在
    WSOD 任务中，研究人员仍面临许多挑战，特别是在提高检测性能方面。
- en: 'TABLE I: A summary of the state-of-the-art WSOD methods. For the proposals,
    SS represents selective search, EB represents edge boxes, and SW represents sliding
    window. The Challenges denotes the main contributions of corresponding papers.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：当前最先进的 WSOD 方法总结。对于提议，SS 代表选择性搜索，EB 代表边缘框，SW 代表滑动窗口。挑战项表示对应论文的主要贡献。
- en: '| Approach | Year | Proposals | Network | Challenges | Code on Github |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 提议 | 网络 | 挑战 | Github 代码 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MIL-based | CAM-based | Discriminative Region | Multiple Instances | Speed
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 基于 MIL | 基于 CAM | 区分性区域 | 多实例 | 速度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | EB | $\surd$ |  |  |  |  | hbilen/WSDDN
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | EB | $\surd$ |  |  |  |  | hbilen/WSDDN
    |'
- en: '| CAM [[26](#bib.bib26)] | CVPR2016 | Heatmap |  | $\surd$ |  |  | $\surd$
    | zhoubolei/CAM |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| CAM [[26](#bib.bib26)] | CVPR2016 | Heatmap |  | $\surd$ |  |  | $\surd$
    | zhoubolei/CAM |'
- en: '| WSLPDA [[27](#bib.bib27)] | CVPR2016 | EB | $\surd$ |  | $\surd$ |  |  |
    jbhuang0604/WSL |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| WSLPDA [[27](#bib.bib27)] | CVPR2016 | EB | $\surd$ |  | $\surd$ |  |  |
    jbhuang0604/WSL |'
- en: '| WELDON [[28](#bib.bib28)] | CVPR2016 | SW | $\surd$ |  | $\surd$ |  | $\surd$
    |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| WELDON [[28](#bib.bib28)] | CVPR2016 | SW | $\surd$ |  | $\surd$ |  | $\surd$
    |  |'
- en: '| ContextLocNet [[29](#bib.bib29)] | ECCV2016 | SS | $\surd$ |  | $\surd$ |  |  |
    vadimkantorov/contextlocnet |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ContextLocNet [[29](#bib.bib29)] | ECCV2016 | SS | $\surd$ |  | $\surd$ |  |  |
    vadimkantorov/contextlocnet |'
- en: '| Grad-CAM [[30](#bib.bib30)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | ramprs/grad-cam |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Grad-CAM [[30](#bib.bib30)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | ramprs/grad-cam |'
- en: '| OICR [[31](#bib.bib31)] | CVPR2017 | SS | $\surd$ |  | $\surd$ |  |  | ppengtang/oicr
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| OICR [[31](#bib.bib31)] | CVPR2017 | SS | $\surd$ |  | $\surd$ |  |  | ppengtang/oicr
    |'
- en: '| WCCN [[32](#bib.bib32)] | CVPR2017 | EB | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| WCCN [[32](#bib.bib32)] | CVPR2017 | EB | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| ST-WSL [[33](#bib.bib33)] | CVPR2017 | EB | $\surd$ |  | $\surd$ | $\surd$
    |  |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ST-WSL [[33](#bib.bib33)] | CVPR2017 | EB | $\surd$ |  | $\surd$ | $\surd$
    |  |  |'
- en: '| WILDCAT [[34](#bib.bib34)] | CVPR2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | durandtibo/wildcat.pytorch |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT [[34](#bib.bib34)] | CVPR2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | durandtibo/wildcat.pytorch |'
- en: '| SPN [[35](#bib.bib35)] | ICCV2017 | SW | $\surd$ |  |  |  | $\surd$ | ZhouYanzhao/SPN
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| SPN [[35](#bib.bib35)] | ICCV2017 | SW | $\surd$ |  |  |  | $\surd$ | ZhouYanzhao/SPN
    |'
- en: '| TP-WSL [[36](#bib.bib36)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| TP-WSL [[36](#bib.bib36)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ |  |'
- en: '| PCL [[37](#bib.bib37)] | TPAMI2018 | SS | $\surd$ |  | $\surd$ | $\surd$
    |  | ppengtang/pcl.pytorch |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)] | TPAMI2018 | SS | $\surd$ |  | $\surd$ | $\surd$
    |  | ppengtang/pcl.pytorch |'
- en: '| GAL-fWSD [[38](#bib.bib38)] | CVPR2018 | EB | $\surd$ |  |  |  | $\surd$
    |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| GAL-fWSD [[38](#bib.bib38)] | CVPR2018 | EB | $\surd$ |  |  |  | $\surd$
    |  |'
- en: '| W2F [[39](#bib.bib39)] | CVPR2018 | SS | $\surd$ |  | $\surd$ | $\surd$ |
    $\surd$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| W2F [[39](#bib.bib39)] | CVPR2018 | SS | $\surd$ |  | $\surd$ | $\surd$ |
    $\surd$ |  |'
- en: '| ACoL [[40](#bib.bib40)] | CVPR2018 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | xiaomengyc/ACoL |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ACoL [[40](#bib.bib40)] | CVPR2018 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | xiaomengyc/ACoL |'
- en: '| ZLDN [[41](#bib.bib41)] | CVPR2018 | EB | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ZLDN [[41](#bib.bib41)] | CVPR2018 | EB | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| TS²C [[42](#bib.bib42)] | ECCV2018 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| TS²C [[42](#bib.bib42)] | ECCV2018 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| SPG [[43](#bib.bib43)] | ECCV2018 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xiaomengyc/SPG |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SPG [[43](#bib.bib43)] | ECCV2018 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xiaomengyc/SPG |'
- en: '| WSRPN [[44](#bib.bib44)] | ECCV2018 | EB | $\surd$ |  |  |  |  |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| WSRPN [[44](#bib.bib44)] | ECCV2018 | EB | $\surd$ |  |  |  |  |  |'
- en: '| C-MIL [[45](#bib.bib45)] | CVPR2019 | SS | $\surd$ |  |  |  |  | WanFang13/C-MIL
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| C-MIL [[45](#bib.bib45)] | CVPR2019 | SS | $\surd$ |  |  |  |  | WanFang13/C-MIL
    |'
- en: '| WS-JDS [[46](#bib.bib46)] | CVPR2019 | EB | $\surd$ |  | $\surd$ |  |  |
    shenyunhang/WS-JDS |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| WS-JDS [[46](#bib.bib46)] | CVPR2019 | EB | $\surd$ |  | $\surd$ |  |  |
    shenyunhang/WS-JDS |'
- en: '| ADL [[47](#bib.bib47)] | CVPR2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | junsukchoe/ADL |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ADL [[47](#bib.bib47)] | CVPR2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | junsukchoe/ADL |'
- en: '| Pred NET [[48](#bib.bib48)] | CVPR2019 | SS | $\surd$ |  |  |  |  |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Pred NET [[48](#bib.bib48)] | CVPR2019 | SS | $\surd$ |  |  |  |  |  |'
- en: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  | researchmm/WSOD2
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  | researchmm/WSOD2
    |'
- en: '| OAILWSD [[50](#bib.bib50)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| OAILWSD [[50](#bib.bib50)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| TPWSD [[51](#bib.bib51)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| TPWSD [[51](#bib.bib51)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| SDCN [[52](#bib.bib52)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SDCN [[52](#bib.bib52)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| DANet [[54](#bib.bib54)] | ICCV2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xuehaolan/DANet |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[54](#bib.bib54)] | ICCV2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xuehaolan/DANet |'
- en: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | Yangseung/NL-CCAM |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | Yangseung/NL-CCAM |'
- en: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: '| EIL [[56](#bib.bib56)] | CVPR2020 | Heatmap |  | $\surd$ | $\surd$ |  | $\surd$
    | Wayne-Mai/EIL |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| EIL [[56](#bib.bib56)] | CVPR2020 | Heatmap |  | $\surd$ | $\surd$ |  | $\surd$
    | Wayne-Mai/EIL |'
- en: '| SLV [[57](#bib.bib57)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SLV [[57](#bib.bib57)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
- en: II-B Main Challenges
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 主要挑战
- en: 'The main challenges of WSOD come from two aspects: localization accuracy and
    speed. For localization accuracy, it consists of discriminative region problem
    and multiple instances with the same category problem. For speed, it is an important
    characteristic of real applications. In TABLE [I](#S2.T1 "TABLE I ‣ II-A A Problem
    Definition ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey"), we summarize all typical WSOD methods and their
    contributions to these challenges.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: WSOD的主要挑战来自两个方面：定位准确性和速度。在定位准确性方面，问题包括判别区域问题和同一类别的多个实例问题。在速度方面，它是实际应用中的一个重要特性。在TABLE [I](#S2.T1
    "TABLE I ‣ II-A 问题定义 ‣ II WSOD ‣ 深度学习在弱监督目标检测和目标定位中的应用：综述")中，我们总结了所有典型的WSOD方法及其对这些挑战的贡献。
- en: 'Discriminative Region Problem. It is that detectors [[25](#bib.bib25), [26](#bib.bib26)]
    tend to focus on the most discriminative parts of the object. During training,
    there may exist more than one proposal around an object, and the most discriminative
    part region of the object is likely to have the highest score (e.g., the region
    A is the most discriminative region in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges
    ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (left) and it has a higher score than that of other regions). If the
    model selects positive proposals only based on scores, it is easy to focus on
    the most discriminative part of the object rather than the whole object extent.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 判别区域问题。检测器[[25](#bib.bib25), [26](#bib.bib26)]倾向于关注物体的最具判别性的部分。在训练过程中，物体周围可能存在多个提议，且物体的最具判别性部分区域可能具有最高分（例如，图[3](#S2.F3
    "图 3 ‣ II-B 主要挑战 ‣ II WSOD ‣ 深度学习在弱监督目标检测和目标定位中的应用：综述")（左）中的区域A是最具判别性的区域，其分数高于其他区域）。如果模型仅根据分数选择正提议，则容易关注物体的最具判别性的部分，而忽视整个物体的范围。
- en: '![Refer to caption](img/d3d5ad0bb477103618a916c4410bf95b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d3d5ad0bb477103618a916c4410bf95b.png)'
- en: 'Figure 3: Detection results between model without classifier refinement (left)
    and model with classifier refinement (right). The figure comes from [[31](#bib.bib31)].'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：未进行分类器优化（左）和进行了分类器优化（右）的模型检测结果。该图来自[[31](#bib.bib31)]。
- en: Multiple Instance Problem. It denotes that detectors [[25](#bib.bib25), [31](#bib.bib31)]
    are difficult to accurately recognize multiple instances when there may exist
    several objects with the same category in an image. Although there are multiple
    instances with the same category in an image, these detectors [[25](#bib.bib25),
    [31](#bib.bib31)] only select the highest score proposal of each category as the
    positive proposal and ignores other possible instance proposals.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多实例问题。它表示检测器[[25](#bib.bib25), [31](#bib.bib31)]在图像中可能存在多个相同类别的物体时，很难准确识别多个实例。虽然图像中有多个相同类别的实例，但这些检测器[[25](#bib.bib25),
    [31](#bib.bib31)]仅选择每个类别的最高分提议作为正提议，并忽略其他可能的实例提议。
- en: Speed Problem. At present, the speed bottleneck of the WSOD approaches is mainly
    concentrated in proposal generation. Selective search (SS) [[58](#bib.bib58)]
    and Edge boxes (EB) [[59](#bib.bib59)] that are widely used in WSOD are too time-consuming.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 速度问题。目前，WSOD方法的速度瓶颈主要集中在提议生成上。广泛使用于WSOD的选择性搜索（SS）[[58](#bib.bib58)]和边缘框（EB）[[59](#bib.bib59)]过于耗时。
- en: II-C Basic WSOD Framework
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 基本WSOD框架
- en: The basic framework of WSOD methods can be categorized into MIL-based networks
    and CAM-based networks according to the detection of multiple instances and a
    single instance.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: WSOD方法的基本框架可以根据对多个实例和单个实例的检测分为MIL基础网络和CAM基础网络。
- en: II-C1 MIL-based Network
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C1 MIL基础网络
- en: 'When the detection network predicts multiple instances in an image, it is considered
    a Multiple Instance Learning (MIL) problem [[60](#bib.bib60)]. Taking Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey") (b) for example, an image is interpreted as
    a bag of proposals in the MIL problem. If the image is labeled cat, it means that
    at least one of the proposals tightly contains the cat instance. Otherwise, all
    of the regions do not contain the cat instance (likewise for dogs). The MIL-based
    network is based on the structure of WSDDN [[25](#bib.bib25)] that consists of
    three components: proposal generator, backbone, and detection head.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '当检测网络预测图像中的多个实例时，它被视为多实例学习（MIL）问题 [[60](#bib.bib60)]。以图 Fig. [1](#S1.F1 "Figure
    1 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey") (b) 为例，在 MIL 问题中，一张图像被解释为提案的集合。如果图像标记为猫，则意味着至少有一个提案紧密包含了猫实例。否则，所有区域都不包含猫实例（狗也是如此）。基于
    MIL 的网络基于 WSDDN [[25](#bib.bib25)] 的结构，包含三个组件：提案生成器、主干网络和检测头。'
- en: 'Proposal Generator. Numerous proposal generators are usually used in MIL-based
    networks. 1) Selective search (SS) [[58](#bib.bib58)]: it leverages the advantages
    of both exhaustive search and segmentation to generate initial proposals. 2) Edge
    boxes (EB) [[59](#bib.bib59)]: it uses object edges to generate proposals and
    is widely used in many approaches [[25](#bib.bib25), [27](#bib.bib27), [32](#bib.bib32),
    [33](#bib.bib33), [41](#bib.bib41), [44](#bib.bib44), [46](#bib.bib46)]. 3) Sliding
    window (SW): it denotes that each point of the feature maps corresponds to one
    or more proposals in the relative position of the original image. And SW is faster
    than SS [[58](#bib.bib58)] and EB [[59](#bib.bib59)] in proposal generation.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 提案生成器。在基于 MIL 的网络中通常使用多种提案生成器。1) 选择性搜索（SS）[[58](#bib.bib58)]：它利用了全面搜索和分割的优势来生成初始提案。2)
    边缘框（EB）[[59](#bib.bib59)]：它利用物体边缘生成提案，并在许多方法中广泛使用 [[25](#bib.bib25), [27](#bib.bib27),
    [32](#bib.bib32), [33](#bib.bib33), [41](#bib.bib41), [44](#bib.bib44), [46](#bib.bib46)]。3)
    滑动窗口（SW）：它表示特征图的每个点对应于原始图像相对位置上的一个或多个提案。滑动窗口在提案生成中比选择性搜索 [[58](#bib.bib58)] 和边缘框
    [[59](#bib.bib59)] 更快。
- en: Backbone. With the development of CNNs and large scale datasets (e.g., ImageNet [[4](#bib.bib4)]),
    the pretrained AlexNet [[61](#bib.bib61)], VGG16 [[12](#bib.bib12)], GoogLeNet [[13](#bib.bib13)],
    ResNet [[14](#bib.bib14)], and SENet [[62](#bib.bib62)] are prevailing feature
    representation networks for both classification and object detection.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Backbone。随着卷积神经网络（CNNs）和大规模数据集（如 ImageNet [[4](#bib.bib4)]）的发展，预训练的 AlexNet
    [[61](#bib.bib61)]、VGG16 [[12](#bib.bib12)]、GoogLeNet [[13](#bib.bib13)]、ResNet
    [[14](#bib.bib14)] 和 SENet [[62](#bib.bib62)] 成为分类和物体检测中的主流特征表示网络。
- en: Detection Head. It includes a classification stream and a localization stream.
    The classification stream predicts class scores for each proposal, and the localization
    stream predicts every proposal’s existing probability score for each class. Then
    the two scores are aggregated to predict the confidence scores of an image as
    a whole, which are used to inject image-level supervision in learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 检测头。它包括一个分类流和一个定位流。分类流预测每个提案的类别分数，定位流预测每个提案在每个类别中的存在概率分数。然后，这两个分数被汇总以预测整个图像的置信度分数，这些分数用于在学习中注入图像级的监督。
- en: Given an image, we first feed it into the proposal generator and backbone to
    generate proposals and feature maps, respectively. Then, the feature maps and
    proposals are forwarded into a spatial pyramid pooling (SPP) [[63](#bib.bib63)]
    layer to generate fixed-size regions. Finally, these regions are fed into the
    detection head to classify and localize object instances.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张图像，我们首先将其输入到提案生成器和主干网络中，分别生成提案和特征图。然后，特征图和提案被传送到空间金字塔池化（SPP）[[63](#bib.bib63)]
    层，以生成固定大小的区域。最后，这些区域被送入检测头，以分类和定位物体实例。
- en: II-C2 CAM-based Network
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C2 基于 CAM 的网络
- en: 'When the detection network only predicts a single instance in an image, it
    is considered an object localization problem. The CAM-based network is based on
    the structure of CAM [[26](#bib.bib26)], which consists of three components: backbone,
    classifier, and class activation maps.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当检测网络仅预测图像中的单一实例时，它被视为物体定位问题。基于 CAM 的网络基于 CAM [[26](#bib.bib26)] 的结构，包含三个组件：主干网络、分类器和类别激活图。
- en: 'Backbone. It is similar to that of the MIL-based network introduced in Section [II-C1](#S2.SS3.SSS1
    "II-C1 MIL-based Network ‣ II-C Basic WSOD Framework ‣ II WSOD ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '主干网络。它类似于在第 [II-C1](#S2.SS3.SSS1 "II-C1 MIL-based Network ‣ II-C Basic WSOD
    Framework ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey") 节中介绍的基于 MIL 的网络。'
- en: Classifier. It is designed to classify the classes of an image, which includes
    a global average pooling (GAP) layer and a fully connected layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器。它旨在对图像的类别进行分类，包括一个全局平均池化（GAP）层和一个全连接层。
- en: Class Activation Maps. It is responsible for locating object instances by using
    a simple segmentation technique. Because the class activation maps are produced
    by matrix multiplying the weight of the fully connected layer to the feature maps
    of the last convolutional layer, it spotlights the class-specific discriminative
    regions in every activation map. Therefore, it is easy to generate bounding boxes
    of every class by segmenting the activation map of the class.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 类别激活图。它通过使用简单的分割技术来定位物体实例。由于类别激活图是通过将全连接层的权重与最后一层卷积层的特征图进行矩阵乘法生成的，因此它突出显示了每个激活图中的类别特定区分区域。因此，通过对类别的激活图进行分割，可以轻松生成每个类别的边界框。
- en: Given an image, we first feed it into the backbone to generate feature maps
    of this image. Then, the feature maps are forwarded into the classifier to classify
    the image’s classes. Meanwhile, we matrix multiply the weight of the fully connected
    layer to the feature maps of the last convolutional layer to produce class activation
    maps. Finally, we segment the activation map of the highest probability class
    to yield bounding boxes for object localization.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张图像，我们首先将其输入到主干网络中以生成该图像的特征图。然后，将特征图传递到分类器中以对图像的类别进行分类。同时，我们将全连接层的权重与最后一层卷积层的特征图进行矩阵乘法，以生成类别激活图。最后，我们对最高概率类别的激活图进行分割，以产生用于物体定位的边界框。
- en: II-C3 Discussions
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-C3 讨论
- en: In this section, we discuss the differences between MIL-based networks and CAM-based
    networks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论基于 MIL 的网络和基于 CAM 的网络之间的区别。
- en: Firstly, MIL-based network leverages SS [[58](#bib.bib58)], EB [[59](#bib.bib59)]
    or SW to generate thousands of initial proposals, but CAM-based network segments
    the activation map to one proposal for each class. Therefore, MIL-based network
    is better than CAM-based network when detecting multiple instances with the same
    category in an image, but the training and inference speed of CAM-based network
    is faster than MIL-based.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基于 MIL 的网络利用 SS [[58](#bib.bib58)]、EB [[59](#bib.bib59)] 或 SW 生成数千个初始提案，而基于
    CAM 的网络将激活图分割成每个类别一个提案。因此，在检测图像中具有相同类别的多个实例时，基于 MIL 的网络优于基于 CAM 的网络，但基于 CAM 的网络的训练和推理速度比基于
    MIL 的网络更快。
- en: Secondly, because the size of the proposals produced by SS or EB is not consistent,
    MIL-based network leverages an SPP layer to generate fixed-size vectors followed
    by feeding these fixed-size vectors into the fully connected layers for later
    training. However, a CAM-based network leverages a GAP layer to generate a fixed-size
    vector on the feature maps. Then, it feeds the vector into a fully connected layer
    for classifying.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于 SS 或 EB 产生的提案大小不一致，基于 MIL 的网络利用 SPP 层生成固定大小的向量，然后将这些固定大小的向量输入到全连接层进行后续训练。然而，基于
    CAM 的网络利用 GAP 层在特征图上生成固定大小的向量。然后，将向量输入到全连接层进行分类。
- en: Finally, Both MIL-based networks and CAM-based networks will face the discriminative
    region problem and multiple instance problem. In addition, MIL-based networks
    will face the training and test speed problem, since SS and EB are too time-consuming
    and yield plenty of initial proposals.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，无论是基于 MIL 的网络还是基于 CAM 的网络都会面临区分区域问题和多实例问题。此外，基于 MIL 的网络还会面临训练和测试速度问题，因为 SS
    和 EB 太耗时，并且会产生大量的初始提案。
- en: III Milestones of WSOD
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WSOD 的里程碑
- en: 'Since 2016, there are some landmark methods (cf. Fig. [4](#S3.F4 "Figure 4
    ‣ III Milestones of WSOD ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey")) for the research of WSOD. In the following,
    we will briefly introduce these milestones.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '自 2016 年以来，WSOD 研究中出现了一些重要的方法（参见图 [4](#S3.F4 "Figure 4 ‣ III Milestones of
    WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey)）。接下来，我们将简要介绍这些里程碑。'
- en: '![Refer to caption](img/a0a74cebeef01c1fef7336c3ad9d78b4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a0a74cebeef01c1fef7336c3ad9d78b4.png)'
- en: 'Figure 4: The Milestones of WSOD since 2016.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：自 2016 年以来 WSOD 的里程碑。
- en: III-A MIL-based Methods
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于MIL的方法
- en: WSDDN. The biggest contribution of WSDDN [[25](#bib.bib25)] is using two streams
    network, which aims to perform classification and localization respectively. WSDDN
    first uses a SPP [[63](#bib.bib63)] on the top of the feature maps and generates
    a feature vector after two fully connected layer procedures. Next, the feature
    vector is fed into the classification stream and localization steam. Specifically,
    the classification stream is responsible for computing the class scores of each
    region, and the localization stream is designed to compute every region’s existing
    probability for each class. Then, the matrix product of the class scores of each
    region and the existing probability for each class is considered as the final
    prediction scores. However, because of only accessing image-level labels in the
    training phase, the most discriminative part of the object will be paid more attention
    than the whole object instance in training. Due to the above limitation, WSDDN
    suffers from the discriminative region problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: WSDDN。WSDDN [[25](#bib.bib25)] 的最大贡献是使用双流网络，旨在分别执行分类和定位。WSDDN 首先在特征图的顶部使用SPP
    [[63](#bib.bib63)]，并在经过两次全连接层处理后生成一个特征向量。接下来，特征向量被输入到分类流和定位流中。具体而言，分类流负责计算每个区域的类别得分，而定位流则用于计算每个类别的区域存在概率。然后，将每个区域的类别得分和每个类别的存在概率的矩阵积视为最终预测得分。然而，由于训练阶段仅访问图像级标签，因此训练时最具区分性的部分会比整个对象实例受到更多关注。由于上述限制，WSDDN
    遭遇了区分区域问题。
- en: OICR. To alleviate the discriminative region problem, OICR [[31](#bib.bib31)]
    uses WSDDN as its baseline and adds three instance classifier refinement procedures
    after the baseline. Every instance classifier refinement procedure, which consists
    of two fully connected layers, is designed to further predict the class scores
    for each proposal. Because the output of each instance classifier refinement procedure
    is the supervision of its latter refinement procedure, OICR can continue to learn
    so that larger area can have higher scores than WSDDN. Although the prediction
    of WSDDN may only focus on the discriminative part of the object, it will be refined
    after several instance classifier refinement procedures.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: OICR。为了缓解区分区域问题，OICR [[31](#bib.bib31)] 使用WSDDN作为其基线，并在基线之后添加了三个实例分类器细化程序。每个实例分类器细化程序，包括两个全连接层，旨在进一步预测每个提议的类别得分。由于每个实例分类器细化程序的输出是其后续细化程序的监督，OICR
    可以继续学习，从而使较大区域的得分高于WSDDN。尽管WSDDN的预测可能仅关注物体的区分部分，但在经过几个实例分类器细化程序后会得到改进。
- en: SDCN. SDCN [[52](#bib.bib52)] introduces a segmentation-detection collaborative
    mechanism. It consists of a detection branch and segmentation branch, which are
    responsible for detecting bounding boxes and generating segmentation masks respectively.
    In SDCN, the detection results will be converted to a heatmap by setting a classification
    score to all pixels within each proposal as the supervision mask of the segmentation
    branch. Meanwhile, the proposals of the highest overlap with the connected regions
    from the segmentation masks will be the pseudo ground-truth boxes of the detection
    branch. Both detection branch and segmentation branch are optimized alternatively
    and promoted each other, so SDCN achieves better detection performance than OICR.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SDCN。SDCN [[52](#bib.bib52)] 引入了分割-检测协作机制。它由检测分支和分割分支组成，分别负责检测边界框和生成分割掩膜。在SDCN中，检测结果将通过将所有像素的分类得分设置为分割分支的监督掩膜来转换为热图。同时，与分割掩膜中的连接区域重叠度最高的提议将成为检测分支的伪地面真实框。检测分支和分割分支交替优化并相互促进，因此SDCN比OICR实现了更好的检测性能。
- en: ICMWSD. Different from SDCN which uses object detection with segmentation collaboration
    mechanism, ICMWSD [[23](#bib.bib23)] addresses the problem of focusing on the
    most discriminative part of an object by leveraging context information. Firstly,
    ICMWSD obtains a dropped features by dropping the most discriminative parts. Then,
    maximizing the loss of the dropped features to force ICMWSD to look at the surrounding
    context regions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ICMWSD。与使用对象检测和分割协作机制的SDCN不同，ICMWSD [[23](#bib.bib23)] 通过利用上下文信息来解决集中于物体最具区分性部分的问题。首先，ICMWSD
    通过丢弃最具区分性的部分来获得丢失的特征。然后，最大化丢失特征的损失以迫使ICMWSD关注周围的上下文区域。
- en: III-B CAM-based Methods
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于CAM的方法
- en: CAM. The biggest contribution of CAM [[26](#bib.bib26)] is using class activation
    maps to predict instances. CAM firstly leverages a GAP layer on the last convolutional
    feature maps to generate a feature vector. Then, the feature vector is fed into
    a classifier with a fully connected layer to generate prediction scores of an
    image. Finally, CAM generates bounding boxes of each class by using a simple thresholding
    technique to segment the activation map of every class. However, class activation
    maps of CAM spotlight the regions that are the most discriminative parts of the
    object, so CAM also faces the discriminative region problem as WSDDN.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: CAM。CAM [[26](#bib.bib26)] 的最大贡献是使用类别激活图预测实例。CAM 首先在最后的卷积特征图上利用 GAP 层生成特征向量。然后，特征向量被输入到具有全连接层的分类器中，生成图像的预测分数。最后，CAM
    通过使用简单的阈值技术对每个类别的激活图进行分割，生成每个类别的边界框。然而，CAM 的类别激活图突出了物体最具区分性的区域，因此 CAM 也面临与 WSDDN
    相同的区分区域问题。
- en: WCCN. To alleviate the discriminative region problem, WCCN [[32](#bib.bib32)]
    proposes to use a cascaded network that has three cascade stages trained in an
    end-to-end pipeline. The first stage is the CAM [[26](#bib.bib26)] network that
    aims to generate class activation maps and initial proposals. The second stage
    is a segmentation network that uses the class activation maps to train object
    segmentation for refining object localization. The final stage is a MIL network
    that performs multiple instances of learning on proposals extracted in the second
    stage. Because the second and third stages refine object localization, WCCN alleviates
    the problem that tends to focus on the most discriminative part of the object.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: WCCN。为了缓解区分区域问题，WCCN [[32](#bib.bib32)] 提出了使用一个具有三个级联阶段的级联网络，该网络在端到端的管道中进行训练。第一个阶段是
    CAM [[26](#bib.bib26)] 网络，旨在生成类别激活图和初始提案。第二个阶段是一个分割网络，利用类别激活图训练物体分割以优化物体定位。最终阶段是
    MIL 网络，对第二阶段提取的提案进行多实例学习。由于第二和第三阶段优化物体定位，WCCN 缓解了通常集中于物体最具区分性的部分的问题。
- en: ACoL. To alleviate the discriminative region problem, ACoL [[40](#bib.bib40)]
    introduces two parallel-classifiers for object localization using adversarial
    complementary learning. Specifically, it first leverages the first classifier
    to localize the most discriminative regions. Then, ACoL uses the masked feature
    maps by masking the most discriminative regions discovered in the first classifier
    as the input feature maps of the second classifier. This forces the second classifier
    to select the next discriminative regions. Finally, ACoL fuses the class activation
    maps of both classifiers to generate bounding boxes of every class by segmenting
    the activation map of the highest probability class.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ACoL。为了缓解区分区域问题，ACoL [[40](#bib.bib40)] 引入了两个并行分类器，用于通过对抗性互补学习进行物体定位。具体来说，它首先利用第一个分类器来定位最具区分性的区域。然后，ACoL
    通过将第一个分类器发现的最具区分性的区域进行遮罩，来使用遮罩后的特征图作为第二个分类器的输入特征图。这迫使第二个分类器选择下一个区分区域。最后，ACoL 融合两个分类器的类别激活图，通过分割最高概率类别的激活图生成每个类别的边界框。
- en: IV Specific Techniques for Discriminative Region Problem
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 解决区分区域问题的具体技术
- en: In this section, we will introduce several advanced techniques for solving the
    discriminative region problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍几种解决区分区域问题的高级技术。
- en: IV-A Context Modeling
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 上下文建模
- en: The context of one region is external information of this region, which can
    be obtained by masking the region of the feature maps with special numbers (e.g.,
    zero). There are two types of the strategy of using context modeling as follows.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个区域的上下文是该区域的外部信息，可以通过用特定数字（如零）对特征图的区域进行遮罩来获取。使用上下文建模的策略有两种类型，如下所示。
- en: Strategy A. It selects the regions that have a big gap between their scores
    and their contextual region’s scores as positive proposals. For example, WSLPDA [[27](#bib.bib27)]
    first replaces the pixel values within one proposal with zero to obtain the contextual
    region. Then, WSLPDA compares the scores of proposals and their contextual region.
    If the gap between the two scores is large, it indicates that the proposal is
    likely positive. ContextLocNet [[29](#bib.bib29)] subtracts the localization score
    of one proposal from the localization score of the external rectangle region of
    the proposal. Then, the subtraction is considered as the final localization score
    of the proposal. Similar to WSLPDA and ContextLocNet, TS²C [[42](#bib.bib42)]
    selects a positive proposal by comparing the mean objectness scores of the pixels
    in one proposal and its surrounding region. But to alleviate the impact of background
    pixels in the surrounding region, TS²C computes the mean objectness scores only
    using pixels with large confidence values in the surrounding region.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 A。它选择那些分数与其上下文区域的分数之间存在较大差距的区域作为正提议。例如，WSLPDA [[27](#bib.bib27)] 首先将一个提议中的像素值替换为零，以获得上下文区域。然后，WSLPDA
    比较提议及其上下文区域的分数。如果两个分数之间的差距很大，则表示该提议可能为正提议。ContextLocNet [[29](#bib.bib29)] 从一个提议的定位分数中减去提议的外部矩形区域的定位分数。然后，这一减法被视为提议的最终定位分数。类似于
    WSLPDA 和 ContextLocNet，TS²C [[42](#bib.bib42)] 通过比较一个提议及其周围区域的像素的平均对象性分数来选择正提议。但为了减轻周围区域背景像素的影响，TS²C
    仅使用周围区域中置信度较高的像素计算平均对象性分数。
- en: Strategy B. It selects positive proposals by leveraging the loss of context
    regions. For example, OAILWSD [[50](#bib.bib50)] believes that a proposal not
    tightly covers the object instance if the loss of the context feature maps of
    this proposal tends to decrease. Thus, OAILWSD first leverages the context classification
    loss to label regions. Then, it selects the top-scoring regions whose context
    class probabilities are low as positive proposals. ICMWSD [[23](#bib.bib23)] first
    drops the most discriminative parts of the feature maps to obtain contextual feature
    maps. Then, it maximizes the loss of the contextual feature maps to force it focuses
    on the context regions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 策略 B。它通过利用上下文区域的损失来选择正提议。例如，OAILWSD [[50](#bib.bib50)] 认为，如果一个提议的上下文特征图的损失倾向于减少，那么该提议可能并未紧密覆盖对象实例。因此，OAILWSD
    首先利用上下文分类损失对区域进行标记。然后，它选择那些上下文类别概率低的高分区域作为正提议。ICMWSD [[23](#bib.bib23)] 首先丢弃特征图中最具区分性的部分，以获得上下文特征图。然后，它最大化上下文特征图的损失，以迫使其关注上下文区域。
- en: IV-B Self-training Algorithm
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 自训练算法
- en: 'In the self-training algorithm, the early prediction instances are then used
    in the detector for latter training as the pseudo ground-truth instances. There
    are two types of self-training algorithms: inter-stream and inter-epoch. In inter-stream
    self-training, the instances of each stream supervise its later stream. In inter-epoch
    self-training, the instances of each epoch supervise its later epoch. The key
    idea of self-training is that even if the early top-scoring proposals may only
    focus on the discriminative part of the object, they will be refined after several
    refinement procedures.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在自训练算法中，早期预测实例会被用作检测器的伪真实实例用于后续训练。自训练算法有两种类型：流间自训练和历时自训练。在流间自训练中，每个流的实例监督其后续流。在历时自训练中，每个历时的实例监督其后续历时。自训练的关键思想是，即使早期高分提议可能只关注对象的区分部分，但经过若干次精细化程序后，它们会被进一步优化。
- en: IV-B1 Inter-stream Self-training
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 流间自训练
- en: 'OICR [[31](#bib.bib31)] expects B, C, and D can inherit the class score of
    A to correctly localize objects in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges
    ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (right). So, OICR adds three refinement classifiers with two fully
    connected layers in WSDDN to address the issue shown in Fig. [3](#S2.F3 "Figure
    3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object
    Detection and Object Localization: A Survey") (left). Specifically, the supervision
    of the first refinement classifier is the output of WSDDN. As for other refinement
    classifiers, the supervision of the current refinement classifier is the output
    of its previous refinement classifier. Inspired by OICR, WSOD2 [[49](#bib.bib49)]
    consists of numerous classifiers. ICMWSD [[23](#bib.bib23)] also inserts refinement
    streams in WSDDN, however, every refinement stream includes a classifier and a
    regressor. Besides, some approaches [[39](#bib.bib39), [53](#bib.bib53), [50](#bib.bib50),
    [51](#bib.bib51)] use OICR as their baseline.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'OICR [[31](#bib.bib31)] 期望 B、C 和 D 可以继承 A 的类别分数，从而正确地定位图 [3](#S2.F3 "Figure
    3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object
    Detection and Object Localization: A Survey") (右侧) 中的对象。因此，OICR 在 WSDDN 中添加了三个包含两个全连接层的细化分类器，以解决图 [3](#S2.F3
    "Figure 3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey") (左侧) 中显示的问题。具体来说，第一个细化分类器的监督是
    WSDDN 的输出。至于其他细化分类器，当前细化分类器的监督是其前一个细化分类器的输出。受 OICR 启发，WSOD2 [[49](#bib.bib49)]
    包含了许多分类器。ICMWSD [[23](#bib.bib23)] 也在 WSDDN 中插入了细化流，但每个细化流包括一个分类器和一个回归器。此外，一些方法 [[39](#bib.bib39),
    [53](#bib.bib53), [50](#bib.bib50), [51](#bib.bib51)] 使用 OICR 作为其基准。'
- en: IV-B2 Inter-epoch Self-training
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 代间自训练
- en: Self-Taught-WS [[33](#bib.bib33)] uses relative improvement (RI) of the scores
    of each proposal of two adjacent epochs as a criterion for selecting the positive
    sample. Specifically, it chooses the proposals of the previous epoch whose intersection
    over union (IoU) $\geq 0.5$ with the maximal RI proposal as the positive samples
    of the current epoch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Self-Taught-WS [[33](#bib.bib33)] 使用相邻两个周期中每个提案得分的相对改善 (RI) 作为选择正样本的标准。具体来说，它选择与最大
    RI 提案的交并比 (IoU) $\geq 0.5$ 的前一周期提案作为当前周期的正样本。
- en: IV-C Cascaded Network
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 级联网络
- en: The cascaded network includes several stages and the supervision of the current
    stage is the output of the previous stage. Such as WCCN [[32](#bib.bib32)] and
    TS²C [[42](#bib.bib42)] consist of three stages. The first stage is the CAM module
    that is to generate initial proposals using class activation maps. The intermediate
    stage is the object segmentation module that is designed to refine initial proposals.
    The final stage is a multiple instance learning module that is responsible for
    detecting accurate objects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 级联网络包括多个阶段，当前阶段的监督是前一阶段的输出。例如，WCCN [[32](#bib.bib32)] 和 TS²C [[42](#bib.bib42)]
    包含三个阶段。第一阶段是 CAM 模块，用于生成初始提案，使用类别激活图。中间阶段是对象分割模块，旨在细化初始提案。最终阶段是多实例学习模块，负责检测准确的对象。
- en: IV-D Bounding Box Regression
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 边界框回归
- en: Bounding box regression can improve object localization performance using instance-level
    annotations in the training phase, but the WSOD task only accesses image-level
    labels. To use bounding box regression for refining the initial proposals from
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)], some approaches propose to yield
    high-quality pseudo ground-truth boxes as the supervision of bounding box regression.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框回归可以在训练阶段使用实例级注释来提高对象定位性能，但 WSOD 任务仅访问图像级标签。为了使用边界框回归来细化 SS [[58](#bib.bib58)]
    或 EB [[59](#bib.bib59)] 中的初始提案，一些方法建议生成高质量的伪真实框作为边界框回归的监督。
- en: Now, numerous approaches [[48](#bib.bib48), [49](#bib.bib49), [51](#bib.bib51),
    [23](#bib.bib23), [57](#bib.bib57)] include at least one of the bounding box regressors.
    The supervision of the regressor is the output of previous classifiers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，许多方法 [[48](#bib.bib48), [49](#bib.bib49), [51](#bib.bib51), [23](#bib.bib23),
    [57](#bib.bib57)] 包含至少一个边界框回归器。回归器的监督是前一分类器的输出。
- en: IV-E Discriminative Region Removal
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 判别区域移除
- en: 'From Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey") (left),
    some researchers find that the highest score region only covers the most discriminative
    part of the object. To localize the whole object extent, masking the most discriminative
    part of the object is designed to force the detector to find the next discriminative
    region.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '从图 [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning for
    Weakly-Supervised Object Detection and Object Localization: A Survey")（左侧），一些研究人员发现最高得分的区域仅覆盖了对象最具判别性的部分。为了定位整个对象的范围，设计了遮罩对象最具判别性的部分，以迫使检测器找到下一个判别性区域。'
- en: TP-WSL [[36](#bib.bib36)] is a two-phase learning network that detects the next
    discriminative regions by masking the most discriminative region. In the first
    phase, it yields class activation maps followed by masking the most discriminative
    region using a threshold among the activation map of the highest probability class.
    In the second phase, it multiplies the masked activation map by the feature maps
    of the second network to refine the feature maps for detecting the next discriminative
    regions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TP-WSL [[36](#bib.bib36)] 是一个两阶段学习网络，通过遮罩最具判别性的区域来检测下一个判别性区域。在第一阶段，它生成类别激活图，并使用最高概率类别的激活图中的阈值遮罩最具判别性的区域。在第二阶段，它将遮罩后的激活图与第二个网络的特征图相乘，以细化特征图，从而检测下一个判别性区域。
- en: Different from TP-WSL that has two backbones, ACoL [[40](#bib.bib40)] consists
    of one shared backbone and two parallel-classifiers. The masked feature maps from
    the first classifier are fed into the second classifier to generate class activation
    maps. Finally, ACoL locates object instances in the fused activation maps by fusing
    the two-class activation maps of both classifiers. EIL [[56](#bib.bib56)] proposes
    to share the weights of the two parallel-classifiers of ACoL, and it only segments
    the activation map of the highest probability class from the unmasked branch to
    yields object proposals. Comparing C-MIDN [[53](#bib.bib53)] with ACoL, there
    are three differences. First, the detection network of C-MIDN is WSDDN [[25](#bib.bib25)],
    but the detection network of ACoL is CAM [[26](#bib.bib26)]. Second, C-MIDN does
    not compute the loss of high overlap with the first detection module’s top-scoring
    proposal in the second branch, but ACoL masks the first detection module’s top-scoring
    proposal’s region with zero in the second branch. Finally, C-MIDN chooses the
    top-scoring proposals of the second detection module and the top-scoring proposals
    of the first detection module with low overlap with selected proposals as positive
    proposals, but ACoL yields positive proposals by segmenting the fused class activation
    maps.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于具有两个主干网络的 TP-WSL，ACoL [[40](#bib.bib40)] 由一个共享主干网络和两个并行分类器组成。第一个分类器的遮罩特征图被输入到第二个分类器中，以生成类别激活图。最终，ACoL
    通过融合两个分类器的类别激活图来定位对象实例。EIL [[56](#bib.bib56)] 提议共享 ACoL 的两个并行分类器的权重，并且它仅从未遮罩的分支中分割最高概率类别的激活图，以生成对象提议。与
    ACoL 相比，C-MIDN [[53](#bib.bib53)] 有三个区别。首先，C-MIDN 的检测网络是 WSDDN [[25](#bib.bib25)]，而
    ACoL 的检测网络是 CAM [[26](#bib.bib26)]。其次，C-MIDN 在第二个分支中不计算与第一个检测模块的高重叠提议的损失，但 ACoL
    在第二个分支中将第一个检测模块的高分提议的区域遮罩为零。最后，C-MIDN 选择第二个检测模块的高分提议和第一个检测模块中与所选提议低重叠的高分提议作为正提议，而
    ACoL 通过分割融合的类别激活图生成正提议。
- en: IV-F Incorporating Low-level Features
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 结合低级特征
- en: Low-level features usually retain richer object details, such as edges, corners,
    colors, pixels, and so on. We can obtain accurate object localization if making
    full use of these low-level features. For example, Grad-CAM [[30](#bib.bib30)]
    leverages high-resolution Guided Backpropagation [[64](#bib.bib64)] that highlights
    the image’s details to create both high-resolution and class-discriminative visualizations.
    WSOD2 [[49](#bib.bib49)] first computes the score of a region proposal. Then,
    it selects the same region in low-level image features (e.g., superpixels) and
    computes the score of this region. Finally, the product of the two scores is the
    final score of the region proposal.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 低级特征通常保留更丰富的对象细节，如边缘、角点、颜色、像素等。如果充分利用这些低级特征，我们可以获得准确的对象定位。例如，Grad-CAM [[30](#bib.bib30)]
    利用高分辨率的 Guided Backpropagation [[64](#bib.bib64)] 强调图像的细节，以创建高分辨率且具有类别区分性的可视化。WSOD2 [[49](#bib.bib49)]
    首先计算区域提议的得分。然后，它在低级图像特征（例如超像素）中选择相同的区域，并计算该区域的得分。最后，两者得分的乘积即为区域提议的最终得分。
- en: IV-G Segmentation-detection Collaborative Mechanism
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 分割-检测协作机制
- en: 'Segmentation-detection collaborative mechanism includes a segmentation branch
    and a detection branch. The primary reasons for the collaborative mechanism are
    the following: 1) MIL (detection) can correctly distinguish an area as an object,
    but it is not good at detecting whether the area contains the entire object. 2)
    Segmentation can cover the entire object instance, but it cannot distinguish whether
    the area is a real object or not [[52](#bib.bib52)]. So, some models leverage
    deep cooperation between detection and segmentation by supervising each other
    to achieve accurate localization.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 分割-检测协作机制包括一个分割分支和一个检测分支。协作机制的主要原因如下：1）MIL（检测）可以正确地将一个区域区分为物体，但它不擅长检测该区域是否包含整个物体。2）分割可以覆盖整个物体实例，但无法区分该区域是否为真实物体[[52](#bib.bib52)]。因此，一些模型通过相互监督来深度合作检测与分割，以实现准确的定位。
- en: 'WS-JDS [[46](#bib.bib46)] first chooses the region proposals with top-scoring
    pixels generated by the semantic segmentation branch as the positive samples of
    the detection branch. Then, it sets the classification score to all pixels within
    each positive proposal of the detection branch as the supervision mask of the
    segmentation branch. Similar to WS-JDS, SDCN [[52](#bib.bib52)] also combines
    the detection branch with the segmentation branch which is introduced in Section [III-A](#S3.SS1
    "III-A MIL-based Methods ‣ III Milestones of WSOD ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'WS-JDS [[46](#bib.bib46)] 首先选择由语义分割分支生成的得分最高的区域提案作为检测分支的正样本。然后，它将检测分支中每个正样本内的所有像素的分类得分设为分割分支的监督掩码。类似于
    WS-JDS，SDCN [[52](#bib.bib52)] 也将检测分支与分割分支结合起来，这在第 [III-A](#S3.SS1 "III-A MIL-based
    Methods ‣ III Milestones of WSOD ‣ Deep Learning for Weakly-Supervised Object
    Detection and Object Localization: A Survey") 节中介绍。'
- en: IV-H Transforming WSOD to FSOD
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H 将 WSOD 转化为 FSOD
- en: 'Transforming WSOD to FSOD is another popular technique to achieve object detection
    using image-level labels, which is designed to train an FSOD model using the output
    of the WSOD model. The primary problem of transformation is to yield good pseudo
    ground-truth boxes from WSOD. There are several strategies to mine boxes as the
    pseudo ground-truth boxes. 1) top score: numerous approaches [[31](#bib.bib31),
    [37](#bib.bib37), [42](#bib.bib42), [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53)]
    select top score detection boxes of WSOD as the pseudo ground-truth boxes. 2)
    relative improvement (RI): ST-WSL [[33](#bib.bib33)] selects the boxes with the
    maximal relative score improvement of two adjacent epochs as the pseudo ground-truth
    boxes. 3) mergence: W2F [[39](#bib.bib39)] merges several small boxes into a big
    candidate box and uses these merged boxes as the pseudo ground-truth boxes for
    later training. SLV [[57](#bib.bib57)] first merges the scores of several boxes
    to the pixels and then generates bounding boxes of each class by using a simple
    thresholding technique to segment the map of every class.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将 WSOD 转化为 FSOD 是另一种利用图像级标签实现目标检测的流行技术，旨在使用 WSOD 模型的输出训练 FSOD 模型。转化的主要问题是从 WSOD
    中生成良好的伪地面实况框。挖掘伪地面实况框的策略有几种：1）最高得分：许多方法[[31](#bib.bib31), [37](#bib.bib37), [42](#bib.bib42),
    [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53)] 选择 WSOD 的最高得分检测框作为伪地面实况框。2）相对改进（RI）：ST-WSL [[33](#bib.bib33)]
    选择两个相邻轮次中相对得分改进最大的框作为伪地面实况框。3）合并：W2F [[39](#bib.bib39)] 将几个小框合并为一个大的候选框，并使用这些合并后的框作为后续训练的伪地面实况框。SLV [[57](#bib.bib57)]
    首先将几个框的得分合并到像素上，然后使用简单的阈值技术生成每个类别的边界框。
- en: 'In addition, there are several FSOD models that have been used as follows:
    Fast R-CNN [[65](#bib.bib65)], Faster R-CNN [[16](#bib.bib16)], and SSD [[17](#bib.bib17)].
    Numerous approaches [[31](#bib.bib31), [33](#bib.bib33), [37](#bib.bib37), [42](#bib.bib42),
    [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53), [57](#bib.bib57)] use prediction
    boxes of WSOD as the pseudo ground-truth boxes to train Fast R-CNN. W2F [[39](#bib.bib39)]
    uses prediction boxes of WSOD to train Faster R-CNN. GAL-fWSD [[38](#bib.bib38)]
    uses prediction boxes of WSOD to train SSD.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有几个 FSOD 模型被使用，包括：Fast R-CNN [[65](#bib.bib65)]、Faster R-CNN [[16](#bib.bib16)]
    和 SSD [[17](#bib.bib17)]。许多方法[[31](#bib.bib31), [33](#bib.bib33), [37](#bib.bib37),
    [42](#bib.bib42), [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53), [57](#bib.bib57)]
    使用 WSOD 的预测框作为伪地面实况框来训练 Fast R-CNN。W2F [[39](#bib.bib39)] 使用 WSOD 的预测框来训练 Faster
    R-CNN。GAL-fWSD [[38](#bib.bib38)] 使用 WSOD 的预测框来训练 SSD。
- en: IV-I Discussions
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-I 讨论
- en: In the previous sections, we individually introduce several techniques that
    are commonly used to improve the detection performance by detailed listing numerous
    approaches. In this section, we will compare and discuss these techniques.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们逐一介绍了几种常用的技术，通过详细列举众多方法来改善检测性能。在本节中，我们将比较和讨论这些技术。
- en: Firstly, context modeling and discriminative region removal are two similar
    techniques. Context modeling is to calculate the scores of the proposal and its
    context region respectively. Then it chooses the positive proposal derives from
    the two scores. On the other hand, the discriminative region removal is to directly
    erases top-scoring regions by setting zero value in the feature maps of the first
    branch followed by feeding the erased feature maps into the second branch.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，背景建模和区分区域移除是两种类似的技术。背景建模是分别计算提议及其背景区域的得分。然后，它选择由这两个得分派生的正提议。另一方面，区分区域移除则是通过在第一个分支的特征图中设置零值直接擦除得分最高的区域，然后将擦除后的特征图输入到第二个分支中。
- en: Secondly, the self-training algorithm usually co-occurs with bounding box regression.
    Bounding box regression is responsible for refining the initial proposals from
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)]. And self-training algorithm is
    designed to refine the prediction result of the baseline. The core problem of
    both the self-training algorithm and bounding box regression is yielding good
    pseudo ground-truth boxes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，自训练算法通常与边界框回归同时出现。边界框回归负责细化来自SS [[58](#bib.bib58)]或EB [[59](#bib.bib59)]的初始提议。而自训练算法旨在优化基线的预测结果。自训练算法和边界框回归的核心问题是生成好的伪地面真实框。
- en: Thirdly, the cascaded network and segmentation-detection collaborative mechanism
    are two similar techniques. They leverage the segmentation module to improve the
    performance of the object detection module. A cascaded network is a sequential
    structure that the previous module is responsible for training the latter module.
    However, the segmentation-detection collaborative mechanism is a circular structure
    that leverages deep cooperation between detection and segmentation supervising
    each other to achieve accurate localization.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，级联网络和分割-检测协作机制是两种类似的技术。它们利用分割模块来提高对象检测模块的性能。级联网络是一个顺序结构，其中前一个模块负责训练后一个模块。然而，分割-检测协作机制是一个循环结构，通过检测和分割之间的深度合作来相互监督以实现准确定位。
- en: Finally, incorporating low-level features technique leverages the advantage
    of the high-resolution characteristics of low-level features to improve object
    localization. The key idea of transforming WSOD to FSOD technique is to make full
    use of the advantages of the network structure of the FSOD model (e.g., Fast R-CNN [[65](#bib.bib65)]).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结合低级特征技术利用低级特征的高分辨率特性来提高对象定位。将WSOD技术转化为FSOD技术的关键思想是充分利用FSOD模型的网络结构优势（例如，Fast
    R-CNN [[65](#bib.bib65)]）。
- en: V Specific Techniques for Multiple Instance problem
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 多实例问题的具体技术
- en: 'In this section, we will introduce how to make full use of the spatial relationship
    of proposals to solve multiple instance problem introduced in Section [II-B](#S2.SS2
    "II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey"). Specifically, the two proposals that are
    far away from each other are likely to correspond to two object instances, while
    the two proposals with large overlap may correspond to the same object instance.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍如何充分利用提议的空间关系来解决第[II-B](#S2.SS2 "II-B Main Challenges ‣ II WSOD ‣
    Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey")节中提出的多实例问题。具体来说，彼此距离较远的两个提议很可能对应两个对象实例，而重叠较大的两个提议可能对应同一个对象实例。'
- en: ST-WSL [[33](#bib.bib33)] leverages a graph to detect multiple instances with
    the same category in an image. It first chooses N top-scoring proposals of each
    positive class as the nodes of the graph. The edge between two nodes indicates
    a large overlap between them. Then it selects the greatest degree (number of connections
    to other nodes) nodes as positive proposals using Non-Maximum Suppression (NMS)
    algorithm  [[66](#bib.bib66)]. PCL [[37](#bib.bib37)] introduces the proposal
    cluster to replace the proposal bag that includes all of the proposals of each
    category. PCL assigns the same label and spatially adjacent proposals to the same
    proposal cluster. If proposals do not overlap each other, they will be assigned
    in different proposal clusters. Then, PCL selects the highest score proposal from
    each proposal cluster as the positive proposal. W2F [[39](#bib.bib39)] iteratively
    merges the highly overlapping proposals with top-scoring into big proposals. Finally,
    these big proposals are considered positive proposals.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ST-WSL [[33](#bib.bib33)] 利用图形来检测图像中相同类别的多个实例。它首先选择每个正类的 N 个高评分提案作为图的节点。两个节点之间的边表示它们之间有较大的重叠。然后，它使用非极大值抑制（NMS）算法
    [[66](#bib.bib66)] 选择度数最大的（与其他节点的连接数）节点作为正提案。PCL [[37](#bib.bib37)] 引入了提案簇，取代了包含每个类别所有提案的提案包。PCL
    将相同标签和空间相邻的提案分配到同一个提案簇中。如果提案之间没有重叠，它们将被分配到不同的提案簇中。然后，PCL 从每个提案簇中选择得分最高的提案作为正提案。W2F [[39](#bib.bib39)]
    迭代地将高度重叠的高评分提案合并为大的提案。最后，这些大的提案被视为正提案。
- en: VI Specific Techniques for Speed Problem
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 速度问题的具体技术
- en: 'In this section, we will introduce several advanced techniques for solving
    the speed problem introduced in Section [II-B](#S2.SS2 "II-B Main Challenges ‣
    II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey"). The main reason for the slow speed is that the MIL-based method adopts
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)] to generate a large number of initial
    proposals that most of which are negative.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍几种解决第 [II-B](#S2.SS2 "II-B Main Challenges ‣ II WSOD ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey") 节中提出的速度问题的高级技术。速度慢的主要原因是
    MIL 基于方法采用 SS [[58](#bib.bib58)] 或 EB [[59](#bib.bib59)] 生成大量初始提案，其中大多数是负样本。'
- en: 'The methods for improving speed can be broadly categorized into three groups:
    1) Transformation-based [[39](#bib.bib39), [38](#bib.bib38)]: these approaches
    use their prediction boxes as the pseudo ground-truth boxes to train Faster R-CNN [[16](#bib.bib16)]
    or SSD [[17](#bib.bib17)] and then use Faster R-CNN or SSD to infer images. 2)
    Sliding-window-based [[28](#bib.bib28), [35](#bib.bib35)]: these approaches use
    the sliding window technique to quickly generate proposals by walking through
    every point on the feature map. 3) Heatmap-based [[26](#bib.bib26), [30](#bib.bib30),
    [34](#bib.bib34), [36](#bib.bib36), [40](#bib.bib40), [43](#bib.bib43), [47](#bib.bib47),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]: these approaches segment
    the heatmap using a threshold to generate proposals to improve the speed of proposal
    generation.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '提高速度的方法可以大致分为三类：1) 基于变换的 [[39](#bib.bib39), [38](#bib.bib38)]: 这些方法将预测框用作伪真实框来训练
    Faster R-CNN [[16](#bib.bib16)] 或 SSD [[17](#bib.bib17)]，然后使用 Faster R-CNN 或 SSD
    来推断图像。2) 基于滑动窗口的 [[28](#bib.bib28), [35](#bib.bib35)]: 这些方法使用滑动窗口技术，通过遍历特征图上的每个点快速生成提案。3)
    基于热图的 [[26](#bib.bib26), [30](#bib.bib30), [34](#bib.bib34), [36](#bib.bib36),
    [40](#bib.bib40), [43](#bib.bib43), [47](#bib.bib47), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56)]: 这些方法通过使用阈值对热图进行分割来生成提案，从而提高提案生成的速度。'
- en: VII Training Tricks
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 训练技巧
- en: Besides the techniques in the previous chapter, training tricks without changing
    network structure also can improve detection results. In this section, we will
    introduce several training tricks for improving object localization.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上一章中的技术外，不改变网络结构的训练技巧也能提高检测结果。在本节中，我们将介绍几种用于改善物体定位的训练技巧。
- en: VII-A Easy-to-hard Strategy
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 从易到难的策略
- en: Previous approaches [[25](#bib.bib25), [26](#bib.bib26), [28](#bib.bib28), [29](#bib.bib29),
    [31](#bib.bib31), [32](#bib.bib32), [36](#bib.bib36)] use all of the images at
    once without a training sequence to train the detection model. The easy-to-hard
    strategy denotes that the model is trained by using the images with progressively
    increasing difficulty. In this way, the model can gain better detection results.
    For example, ZLDN [[41](#bib.bib41)] first computes the difficulty scores of images.
    Then, all of the images are ranked in an ascending order based on the difficulty
    scores. Finally, ZLDN uses the images with increasing difficulty to progressively
    train themselves.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的方法 [[25](#bib.bib25), [26](#bib.bib26), [28](#bib.bib28), [29](#bib.bib29),
    [31](#bib.bib31), [32](#bib.bib32), [36](#bib.bib36)] 直接使用所有图像进行检测模型的训练，而没有训练序列。易到难策略意味着模型通过逐渐增加难度的图像进行训练。这样，模型可以获得更好的检测结果。例如，ZLDN [[41](#bib.bib41)]
    首先计算图像的难度分数。然后，所有图像根据难度分数按升序排列。最后，ZLDN 使用难度逐渐增加的图像进行逐步训练。
- en: VII-B Negative Evidence
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 负证据
- en: Negative evidence contains the low-scoring regions, activations, and activation
    maps. For example, WELDON [[28](#bib.bib28)] uses the classification scores of
    the $k$ top-scoring proposals and the $m$ low-scoring proposals to generate the
    classification scores of the image by simply summing. WILDCAT [[34](#bib.bib34)]
    leverages the $k^{+}$ highest probability activations and $k^{-}$ lowest probability
    activations of the activation map to generate the prediction score. NL-CCAM [[55](#bib.bib55)]
    uses the lowest probability activation maps. Specifically, it first ranks all
    of the activation maps in a descending order based on the probability of every
    class. Then, it fuses these class activation maps using a specific combinational
    function into one map, which is segmented to predict object instances.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 负证据包含低分区域、激活和激活图。例如，WELDON [[28](#bib.bib28)] 使用前 $k$ 个高分提案和 $m$ 个低分提案的分类分数，通过简单求和生成图像的分类分数。WILDCAT [[34](#bib.bib34)]
    利用激活图中的 $k^{+}$ 个最高概率激活和 $k^{-}$ 个最低概率激活生成预测分数。NL-CCAM [[55](#bib.bib55)] 使用最低概率的激活图。具体来说，它首先根据每个类别的概率对所有激活图进行降序排序。然后，它使用特定的组合函数将这些类别激活图融合成一个图，从中分割出目标实例进行预测。
- en: VII-C Optimizing Smoothed Loss Functions
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 优化平滑损失函数
- en: If the loss function of the model is non-convex, it tends to fall into sub-optimal
    and falsely localizes object parts while missing a full object extent during training [[45](#bib.bib45)].
    So C-MIL [[45](#bib.bib45)] replaces the non-convex loss function with a series
    of smoothed loss functions to alleviate the problem that the model tends to get
    stuck into local minima. At the beginning of training, C-MIL first performs the
    image classification task. During the training process, the loss function of C-MIL
    is slowly transformed from the convex image classification loss to the non-convex
    object detection loss function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的损失函数是非凸的，它倾向于陷入次优解并在训练过程中错误地定位对象部分而漏掉完整对象的范围 [[45](#bib.bib45)]。因此，C-MIL [[45](#bib.bib45)]
    用一系列平滑损失函数替代非凸损失函数，以缓解模型倾向于陷入局部最小值的问题。在训练开始时，C-MIL 首先执行图像分类任务。在训练过程中，C-MIL 的损失函数会逐渐从凸的图像分类损失转变为非凸的目标检测损失函数。
- en: VII-D Discussions
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 讨论
- en: In the previous sections, we individually introduce several training tricks
    that are independent of the network structure. In this section, we will compare
    and discuss these tricks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们分别介绍了几个独立于网络结构的训练技巧。在这一部分，我们将比较和讨论这些技巧。
- en: Firstly, an easy-to-hard strategy is applied to the data phase, which is responsible
    for adjusting the order of the training images. Secondly, negative evidence acts
    on the training phase, which is designed to refine positive proposals or feature
    maps. Finally, optimizing smoothed loss functions act on the optimizing phase,
    which is responsible for avoiding suboptimal.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，采用从易到难的策略来处理数据阶段，这一阶段负责调整训练图像的顺序。其次，负证据作用于训练阶段，旨在优化正样本提案或特征图。最后，优化平滑损失函数作用于优化阶段，负责避免次优解。
- en: VIII Datasets and Performance Evaluation
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 数据集和性能评估
- en: VIII-A Datasets
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 数据集
- en: Datasets play an important role in WSOD task. Most approaches of the WSOD use
    PASCAL VOC [[3](#bib.bib3)], MSCOCO [[15](#bib.bib15)], ILSVRC [[4](#bib.bib4)],
    or CUB-200 [[67](#bib.bib67)] as training and test datasets.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在 WSOD 任务中发挥着重要作用。大多数 WSOD 方法使用 PASCAL VOC [[3](#bib.bib3)]、MSCOCO [[15](#bib.bib15)]、ILSVRC [[4](#bib.bib4)]
    或 CUB-200 [[67](#bib.bib67)] 作为训练和测试数据集。
- en: PASCAL VOC. It includes 20 categories and tens of thousands of images with instance
    annotations. PASCAL VOC has several versions, such as PASCAL VOC 2007, 2010, and
    2012\. Specifically, PASCAL VOC 2007 consists of 2,501 training images, 2,510
    validation images, and 4,092 test images. PASCAL VOC 2010 consists of 4,998 training
    images, 5,105 validation images, and 9,637 test images. PASCAL VOC 2012 consists
    of 5,717 training images, 5,823 validation images, and 10,991 test images.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL VOC。它包括 20 个类别和数万张带有实例注释的图像。PASCAL VOC 有几个版本，例如 PASCAL VOC 2007、2010
    和 2012。具体来说，PASCAL VOC 2007 包含 2,501 张训练图像、2,510 张验证图像和 4,092 张测试图像。PASCAL VOC
    2010 包含 4,998 张训练图像、5,105 张验证图像和 9,637 张测试图像。PASCAL VOC 2012 包含 5,717 张训练图像、5,823
    张验证图像和 10,991 张测试图像。
- en: MSCOCO. It is large-scale object detection, segmentation, and captioning dataset.
    MSCOCO has 80 object categories, 330K images ($>$200K labeled), 1.5 million object
    instances. In object detection, MSCOCO is as popular as PASCAL VOC datasets. Because
    MSCOCO has more images and categories than PASCAL VOC datasets, the difficulty
    of training on the MSCOCO dataset is higher than that of PASCAL VOC datasets.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: MSCOCO。它是一个大规模的目标检测、分割和描述数据集。MSCOCO 具有 80 个目标类别、33 万张图像（>20 万张带标签）、150 万个目标实例。在目标检测中，MSCOCO
    与 PASCAL VOC 数据集同样受欢迎。由于 MSCOCO 的图像和类别比 PASCAL VOC 数据集更多，因此在 MSCOCO 数据集上的训练难度高于
    PASCAL VOC 数据集。
- en: ILSVRC. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a
    large-scale dataset. In ILSVRC, the model usually uses 200 fully labeled categories
    and 1,000 categories in object detection and object localization, respectively.
    ILSVRC has several versions, such as ILSVRC 2013, ILSVRC 2014, and ILSVRC 2016\.
    Specifically, ILSVRC 2013 which is usually used in object detection has 12,125
    images for training, 20,121 images for validation, and 40,152 images for testing.
    In addition, ILSVRC 2014 and 2016 inherit the ILSVRC 2012 dataset in object localization,
    which contains 1.2 million images of 1,000 categories in the training set. And
    ILSVRC 2012 dataset has 50,000 and 100,000 images with labels in the validation
    and test set, respectively.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ILSVRC。ImageNet 大规模视觉识别挑战赛 (ILSVRC) 是一个大规模数据集。在 ILSVRC 中，模型通常使用 200 个完全标记的类别和
    1,000 个目标检测和目标定位的类别。ILSVRC 有几个版本，例如 ILSVRC 2013、ILSVRC 2014 和 ILSVRC 2016。具体来说，ILSVRC
    2013 通常用于目标检测，包含 12,125 张训练图像、20,121 张验证图像和 40,152 张测试图像。此外，ILSVRC 2014 和 2016
    继承了 ILSVRC 2012 数据集中的目标定位，训练集中包含 1,000 个类别的 120 万张图像。ILSVRC 2012 数据集在验证集和测试集中分别有
    50,000 张和 100,000 张带标签的图像。
- en: CUB-200. Caltech-UCSD Birds 200 (CUB-200) contains 200 bird species which is
    a challenging image dataset. It focuses on the study of subordinate categorization.
    CUB-200-2011 [[68](#bib.bib68)] is an extended version of CUB-200, which adds
    many images for each category and labels new part localization annotations. CUB-200-2011
    contains 5,994 images in the training set and 5,794 images in the test set.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: CUB-200。Caltech-UCSD 鸟类 200 (CUB-200) 包含 200 种鸟类，是一个具有挑战性的图像数据集。它专注于从属分类的研究。CUB-200-2011 [[68](#bib.bib68)]
    是 CUB-200 的扩展版本，增加了每个类别的许多图像并标注了新的部位定位注释。CUB-200-2011 包含 5,994 张训练图像和 5,794 张测试图像。
- en: VIII-B Evaluation Metrics
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 评估指标
- en: 'In the state-of-the-art WSOD approaches, there are three standard evaluation
    metrics: mAP, CorLoc, and top error.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在最先进的 WSOD 方法中，有三种标准评估指标：mAP、CorLoc 和 top error。
- en: mAP (mean Average Precision). Average Precision (AP) is usually used in image
    classification and object detection. It consists of precision and recall. If $tp$
    denotes the number of the correct prediction samples among all of the positive
    samples, $fp$ denotes the number of the wrong prediction samples among all of
    the positive samples, and $fn$ denotes the number of the wrong prediction samples
    among all of the negative samples, precision and recall can be computed as
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: mAP（均值平均精度）。平均精度 (AP) 通常用于图像分类和目标检测。它由精度和召回率组成。如果 $tp$ 表示所有正样本中正确预测样本的数量，$fp$
    表示所有正样本中错误预测样本的数量，$fn$ 表示所有负样本中错误预测样本的数量，则精度和召回率可以计算为
- en: '|  | recall | $\displaystyle=tp/(tp+fn),$ |  | (1) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | $\displaystyle=tp/(tp+fn),$ |  | (1) |'
- en: '|  | precision | $\displaystyle=tp/(tp+fp),$ |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 精度 | $\displaystyle=tp/(tp+fp),$ |  |'
- en: where the correct prediction sample denotes IoU of the positive sample and its
    corresponding ground-truth box $\geq$ 0.5\. Meanwhile, the IoU is defined as
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 其中正确的预测样本表示正样本及其对应真实框的 IoU $\geq$ 0.5。与此同时，IoU 被定义为
- en: '|  | ${\rm IoU}(b,b^{g})=area(b\cap b^{g})/area(b\cup b^{g}),$ |  | (2) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm IoU}(b,b^{g})=area(b\cap b^{g})/area(b\cup b^{g}),$ |  | (2) |'
- en: where $b$ denotes a prediction sample, $b^{g}$ denotes a corresponding ground-truth
    box, and $area$ denotes the region size of the intersection or union. The mAP
    is the mean of all of the class average precisions and is a final evaluation metric
    of performance on the test dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ 表示一个预测样本，$b^{g}$ 表示一个对应的真实框，$area$ 表示交集或并集的区域大小。mAP 是所有类别平均精度的均值，是测试数据集上性能的最终评估指标。
- en: CorLoc (Correct Localization). CorLoc denotes the percentage of images that
    exist at least one instance of the prediction boxes whose IoU $\geq$ 50% with
    ground-truth boxes for every class in these images. CorLoc is a final evaluation
    metric of localization accuracy on the trainval dataset.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: CorLoc（正确定位）。CorLoc 表示在这些图像中每个类别的预测框与真实框的 IoU $\geq$ 50% 的图像百分比。CorLoc 是训练验证数据集上定位准确度的最终评估指标。
- en: Top Error. Top error consists of Top-1 classification error (1-err cls), Top-5
    classification error (5-err cls), Top-1 localization error (1-err loc), and Top-5
    localization error (5-err loc). Specifically, Top-1 classification error is equal
    to $1.0-cls_{1}$, where $cls_{1}$ denotes the accuracy of the highest prediction
    score (likewise for Top-1 localization error). Top-5 classification error is equal
    to $1.0-cls_{5}$, where $cls_{5}$ denotes that it counts as correct if one of
    the five predictions with the highest score is correct (likewise for Top-5 localization
    error). Numerous approaches [[26](#bib.bib26), [40](#bib.bib40), [43](#bib.bib43),
    [54](#bib.bib54), [55](#bib.bib55)] use top error to evaluate the performance
    of the model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Top Error（顶部错误）。Top Error 包括 Top-1 分类错误（1-err cls）、Top-5 分类错误（5-err cls）、Top-1
    定位错误（1-err loc）和 Top-5 定位错误（5-err loc）。具体来说，Top-1 分类错误等于 $1.0-cls_{1}$，其中 $cls_{1}$
    表示最高预测分数的准确度（Top-1 定位错误也类似）。Top-5 分类错误等于 $1.0-cls_{5}$，其中 $cls_{5}$ 表示如果五个预测中有一个是正确的，就算正确（Top-5
    定位错误也类似）。许多方法 [[26](#bib.bib26), [40](#bib.bib40), [43](#bib.bib43), [54](#bib.bib54),
    [55](#bib.bib55)] 使用 Top Error 来评估模型性能。
- en: VIII-C Experimental Results
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 实验结果
- en: 'Results on Pascal VOC. The results of state-of-the-art WSOD methods on datasets
    Pascal VOC 2007, 2010, and 2012 are shown in Table [II](#S8.T2 "TABLE II ‣ VIII-C
    Experimental Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey"). The
    WSOD methods with “+FR” denote that their initial predictions are fed into the
    Fast R-CNN [[65](#bib.bib65)] and serve as pseudo ground-truth bounding box annotations,
    i.e., these methods transform the WSOD into FSOD problems. From the results, we
    can observe the performance on all three Pascal VOC datasets have achieved unprecedented
    progress in recent few years (e.g., mAP 52.1% in CVPR’20 vs. 29.1% in CVPR’16
    on Pascal VOC 2012). Meanwhile, comparing the methods and their counterparts with
    Fast R-CNN (e.g., OICR vs. OICR+FR), we can find the detection performance can
    be further improved by using this FSOD transforming strategy.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pascal VOC 上的结果。最先进的 WSOD 方法在 Pascal VOC 2007、2010 和 2012 数据集上的结果如表 [II](#S8.T2
    "TABLE II ‣ VIII-C Experimental Results ‣ VIII Datasets and Performance Evaluation
    ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") 所示。带有“+FR”标记的 WSOD 方法表示它们的初始预测被输入到 Fast R-CNN [[65](#bib.bib65)] 中，并作为伪真实框标注，即这些方法将
    WSOD 转换为 FSOD 问题。从结果中可以观察到，所有三个 Pascal VOC 数据集上的性能在最近几年取得了前所未有的进展（例如，CVPR’20 上
    Pascal VOC 2012 的 mAP 为 52.1%，而 CVPR’16 为 29.1%）。同时，比较这些方法与其 Fast R-CNN 对应方法（例如，OICR
    与 OICR+FR），我们可以发现，使用这种 FSOD 转换策略可以进一步提升检测性能。'
- en: 'Results on MSCOCO. The results of state-of-the-art WSOD methods on dataset
    MSCOCO are shown in Table [III](#S8.T3 "TABLE III ‣ VIII-C Experimental Results
    ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"). We only report the AP metric,
    and the AP[50] denotes that the IoU threshold is equal to $0.5$. Similarly, the
    performance on MSCOCO also doubled in the last few years (e.g., AP[50] 11.5% vs.
    24.8% in test set). Since MSCOCO contains more object categories than PASCAL VOC
    datasets, the results on MSCOCO are still far from satisfactory. However, the
    performance gains by transforming WSOD to FSOD is relative marginal (e.g., 0.7%
    gains in AP for PCL model).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'MSCOCO上的结果。表格[III](#S8.T3 "TABLE III ‣ VIII-C Experimental Results ‣ VIII Datasets
    and Performance Evaluation ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey")展示了最先进的WSOD方法在MSCOCO数据集上的结果。我们仅报告AP指标，AP[50]表示IoU阈值为$0.5$。类似地，MSCOCO上的性能在过去几年也翻了一番（例如，测试集中的AP[50]从11.5%提升到24.8%）。由于MSCOCO包含的物体类别比PASCAL
    VOC数据集更多，因此在MSCOCO上的结果仍然远未令人满意。然而，将WSOD转换为FSOD带来的性能提升相对较小（例如，PCL模型在AP上的提升为0.7%）。'
- en: 'Results on ILSVRC 2020 and CUB-200. TABLE [IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental
    Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey") summaries the object localization
    performance of state-of-the-art WSOD methods on these two datasets. Compared to
    the PASCAL VOC and MSCOCO, quite few WSOD methods have evaluated their performance
    on these two benchmarks. From TABLE [IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental
    Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we can find the performance
    gains are also significant (1-err cls 35.6% vs. 27.7% in ILSVRC 2012).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'ILSVRC 2020 和 CUB-200上的结果。表[IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental Results
    ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey")总结了最先进的WSOD方法在这两个数据集上的物体定位性能。与PASCAL
    VOC和MSCOCO相比，很少有WSOD方法在这两个基准上评估其性能。从表[IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental
    Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey")中可以发现，性能提升也很显著（ILSVRC 2012中的1-err
    cls 35.6% vs. 27.7%）。'
- en: 'TABLE II: The summary of detection results (mAp (%) and CorLoc (%)) of state-of-the-art
    WSOD methods on Pascal VOC 2007, 2010, and 2012 datasets. The FR means Fast R-CNN [[65](#bib.bib65)].'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：最先进的WSOD方法在Pascal VOC 2007、2010和2012数据集上的检测结果汇总（mAp (%) 和 CorLoc (%)）。FR指的是Fast
    R-CNN [[65](#bib.bib65)]。
- en: '| Approach | 2007 | 2010 | 2012 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 2007 | 2010 | 2012 |'
- en: '| mAP | CorLoc | mAP | CorLoc | mAP | CorLoc |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| mAP | CorLoc | mAP | CorLoc | mAP | CorLoc |'
- en: '| WSDDN [[25](#bib.bib25)]${}_{\text{CVPR2016}}$ | 39.3 | 58.0 | 36.2 | 59.7
    | - | - |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| WSDDN [[25](#bib.bib25)]${}_{\text{CVPR2016}}$ | 39.3 | 58.0 | 36.2 | 59.7
    | - | - |'
- en: '| WSLPDA [[27](#bib.bib27)]${}_{\text{CVPR2016}}$ | 39.5 | 52.4 | 30.7 | -
    | 29.1 | - |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| WSLPDA [[27](#bib.bib27)]${}_{\text{CVPR2016}}$ | 39.5 | 52.4 | 30.7 | -
    | 29.1 | - |'
- en: '| ContextLocNet [[29](#bib.bib29)]${}_{\text{ECCV2016}}$ | 36.3 | 55.1 | -
    | - | 35.3 | 54.8 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ContextLocNet [[29](#bib.bib29)]${}_{\text{ECCV2016}}$ | 36.3 | 55.1 | -
    | - | 35.3 | 54.8 |'
- en: '| OICR [[31](#bib.bib31)]${}_{\text{CVPR2017}}$ | 42.0 | 61.2 | - | - | 38.2
    | 63.5 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| OICR [[31](#bib.bib31)]${}_{\text{CVPR2017}}$ | 42.0 | 61.2 | - | - | 38.2
    | 63.5 |'
- en: '| WCCN [[32](#bib.bib32)]${}_{\text{CVPR2017}}$ | 42.8 | 56.9 | 39.5 | - |
    37.9 | - |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| WCCN [[32](#bib.bib32)]${}_{\text{CVPR2017}}$ | 42.8 | 56.9 | 39.5 | - |
    37.9 | - |'
- en: '| ST-WSL [[33](#bib.bib33)]${}_{\text{CVPR2017}}$ | 41.7 | 56.1 | - | - | 39.0
    | 58.8 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ST-WSL [[33](#bib.bib33)]${}_{\text{CVPR2017}}$ | 41.7 | 56.1 | - | - | 39.0
    | 58.8 |'
- en: '| SPN [[35](#bib.bib35)]${}_{\text{ICCV2017}}$ | - | 60.6 | - | - | - | - |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| SPN [[35](#bib.bib35)]${}_{\text{ICCV2017}}$ | - | 60.6 | - | - | - | - |'
- en: '| TST [[69](#bib.bib69)]${}_{\text{ICCV2017}}$ | 34.5 | 60.8 | - | - | - |
    - |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TST [[69](#bib.bib69)]${}_{\text{ICCV2017}}$ | 34.5 | 60.8 | - | - | - |
    - |'
- en: '| PCL [[37](#bib.bib37)]${}_{\text{TPAMI2018}}$ | 45.8 | 63.0 | - | - | 41.6
    | 65.0 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)]${}_{\text{TPAMI2018}}$ | 45.8 | 63.0 | - | - | 41.6
    | 65.0 |'
- en: '| GAL-fWSD [[38](#bib.bib38)]${}_{\text{CVPR2018}}$ | 47.0 | 68.1 | 45.1 |
    68.3 | 43.1 | 67.2 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| GAL-fWSD [[38](#bib.bib38)]${}_{\text{CVPR2018}}$ | 47.0 | 68.1 | 45.1 |
    68.3 | 43.1 | 67.2 |'
- en: '| W2F [[39](#bib.bib39)]${}_{\text{CVPR2018}}$ | 52.4 | 70.3 | - | - | 47.8
    | 69.4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| W2F [[39](#bib.bib39)]${}_{\text{CVPR2018}}$ | 52.4 | 70.3 | - | - | 47.8
    | 69.4 |'
- en: '| ZLDN [[41](#bib.bib41)]${}_{\text{CVPR2018}}$ | 47.6 | 61.2 | - | - | 42.9
    | 61.5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ZLDN [[41](#bib.bib41)]${}_{\text{CVPR2018}}$ | 47.6 | 61.2 | - | - | 42.9
    | 61.5 |'
- en: '| MELM [[70](#bib.bib70)]${}_{\text{CVPR2018}}$ | 47.3 | 61.4 | - | - | 42.4
    | - |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| MELM [[70](#bib.bib70)]${}_{\text{CVPR2018}}$ | 47.3 | 61.4 | - | - | 42.4
    | - |'
- en: '| TS²C [[42](#bib.bib42)]${}_{\text{ECCV2018}}$ | 44.3 | 61.0 | - | - | 40.0
    | 64.4 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| TS²C [[42](#bib.bib42)]${}_{\text{ECCV2018}}$ | 44.3 | 61.0 | - | - | 40.0
    | 64.4 |'
- en: '| C-WSL [[71](#bib.bib71)]${}_{\text{ECCV2018}}$ | 45.6 | 63.3 | - | - | 43.0
    | 64.9 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| C-WSL [[71](#bib.bib71)]${}_{\text{ECCV2018}}$ | 45.6 | 63.3 | - | - | 43.0
    | 64.9 |'
- en: '| WSRPN [[44](#bib.bib44)]${}_{\text{ECCV2018}}$ | 47.9 | 66.9 | - | - | 43.4
    | 67.2 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| WSRPN [[44](#bib.bib44)]${}_{\text{ECCV2018}}$ | 47.9 | 66.9 | - | - | 43.4
    | 67.2 |'
- en: '| C-MIL [[45](#bib.bib45)]${}_{\text{CVPR2019}}$ | 40.7 | 59.5 | - | - | 46.7
    | 67.4 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| C-MIL [[45](#bib.bib45)]${}_{\text{CVPR2019}}$ | 40.7 | 59.5 | - | - | 46.7
    | 67.4 |'
- en: '| WS-JDS [[46](#bib.bib46)]${}_{\text{CVPR2019}}$ | 45.6 | 64.5 | 39.9 | 63.1
    | 39.1 | 63.5 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| WS-JDS [[46](#bib.bib46)]${}_{\text{CVPR2019}}$ | 45.6 | 64.5 | 39.9 | 63.1
    | 39.1 | 63.5 |'
- en: '| Pred NET [[48](#bib.bib48)]${}_{\text{CVPR2019}}$ | 53.6 | 71.4 | - | - |
    49.5 | 70.2 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Pred NET [[48](#bib.bib48)]${}_{\text{CVPR2019}}$ | 53.6 | 71.4 | - | - |
    49.5 | 70.2 |'
- en: '| WSOD2 [[49](#bib.bib49)]${}_{\text{ICCV2019}}$ | 53.6 | 69.5 | - | - | 47.2
    | 71.9 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| WSOD2 [[49](#bib.bib49)]${}_{\text{ICCV2019}}$ | 53.6 | 69.5 | - | - | 47.2
    | 71.9 |'
- en: '| OAILWSD [[50](#bib.bib50)]${}_{\text{ICCV2019}}$ | 47.6 | 66.7 | - | - |
    43.4 | 66.7 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| OAILWSD [[50](#bib.bib50)]${}_{\text{ICCV2019}}$ | 47.6 | 66.7 | - | - |
    43.4 | 66.7 |'
- en: '| TPWSD [[51](#bib.bib51)]${}_{\text{ICCV2019}}$ | 51.5 | 68.0 | - | - | 45.6
    | 68.7 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| TPWSD [[51](#bib.bib51)]${}_{\text{ICCV2019}}$ | 51.5 | 68.0 | - | - | 45.6
    | 68.7 |'
- en: '| SDCN [[52](#bib.bib52)]${}_{\text{ICCV2019}}$ | 50.2 | 68.6 | - | - | 43.5
    | 67.9 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SDCN [[52](#bib.bib52)]${}_{\text{ICCV2019}}$ | 50.2 | 68.6 | - | - | 43.5
    | 67.9 |'
- en: '| C-MIDN [[53](#bib.bib53)]${}_{\text{ICCV2019}}$ | 52.6 | 68.7 | - | - | 50.2
    | 71.2 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| C-MIDN [[53](#bib.bib53)]${}_{\text{ICCV2019}}$ | 52.6 | 68.7 | - | - | 50.2
    | 71.2 |'
- en: '| ICMWSD [[23](#bib.bib23)]${}_{\text{CVPR2020}}$ | 54.9 | 68.8 | - | - | 52.1
    | 70.9 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| ICMWSD [[23](#bib.bib23)]${}_{\text{CVPR2020}}$ | 54.9 | 68.8 | - | - | 52.1
    | 70.9 |'
- en: '| SLV [[57](#bib.bib57)]${}_{\text{CVPR2020}}$ | 53.5 | 71.0 | - | - | 49.2
    | 69.2 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| SLV [[57](#bib.bib57)]${}_{\text{CVPR2020}}$ | 53.5 | 71.0 | - | - | 49.2
    | 69.2 |'
- en: '| OICR [[31](#bib.bib31)]+FR${}_{\text{CVPR2017}}$ | 47.0 | 64.3 | - | - |
    42.5 | 65.6 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| OICR [[31](#bib.bib31)]+FR${}_{\text{CVPR2017}}$ | 47.0 | 64.3 | - | - |
    42.5 | 65.6 |'
- en: '| PCL [[37](#bib.bib37)]+FR${}_{\text{TPAMI2018}}$ | 48.8 | 66.6 | - | - |
    44.2 | 68.0 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)]+FR${}_{\text{TPAMI2018}}$ | 48.8 | 66.6 | - | - |
    44.2 | 68.0 |'
- en: '| MEFF [[72](#bib.bib72)]+FR${}_{\text{CVPR2018}}$ | 51.2 | - | - | - | - |
    - |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| MEFF [[72](#bib.bib72)]+FR${}_{\text{CVPR2018}}$ | 51.2 | - | - | - | - |
    - |'
- en: '| C-WSL [[71](#bib.bib71)]+FR${}_{\text{ECCV2018}}$ | 47.8 | 65.6 | - | - |
    45.4 | 66.9 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| C-WSL [[71](#bib.bib71)]+FR${}_{\text{ECCV2018}}$ | 47.8 | 65.6 | - | - |
    45.4 | 66.9 |'
- en: '| WSRPN [[44](#bib.bib44)]+FR${}_{\text{ECCV2018}}$ | 50.4 | 68.4 | - | - |
    45.7 | 69.3 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| WSRPN [[44](#bib.bib44)]+FR${}_{\text{ECCV2018}}$ | 50.4 | 68.4 | - | - |
    45.7 | 69.3 |'
- en: '| WS-JDS [[46](#bib.bib46)]+FR${}_{\text{CVPR2019}}$ | 52.5 | 68.6 | 45.7 |
    68.1 | 46.1 | 69.5 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| WS-JDS [[46](#bib.bib46)]+FR${}_{\text{CVPR2019}}$ | 52.5 | 68.6 | 45.7 |
    68.1 | 46.1 | 69.5 |'
- en: '| SDCN [[52](#bib.bib52)]+FR${}_{\text{ICCV2019}}$ | 53.7 | 72.5 | - | - |
    46.7 | 69.5 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| SDCN [[52](#bib.bib52)]+FR${}_{\text{ICCV2019}}$ | 53.7 | 72.5 | - | - |
    46.7 | 69.5 |'
- en: '| C-MIDN [[53](#bib.bib53)]+FR${}_{\text{ICCV2019}}$ | 53.6 | 71.9 | - | -
    | 50.3 | 73.3 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| C-MIDN [[53](#bib.bib53)]+FR${}_{\text{ICCV2019}}$ | 53.6 | 71.9 | - | -
    | 50.3 | 73.3 |'
- en: '| SLV [[57](#bib.bib57)]+FR${}_{\text{CVPR2020}}$ | 53.9 | 72.0 | - | - | -
    | - |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SLV [[57](#bib.bib57)]+FR${}_{\text{CVPR2020}}$ | 53.9 | 72.0 | - | - | -
    | - |'
- en: 'TABLE III: Detetion results on MSCOCO dataset comes from [[23](#bib.bib23)].
    These models use VGG16 as their convolutional neural network. There is no difference
    between AP and mAP under the MSCOCO context.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: MSCOCO 数据集的检测结果来自 [[23](#bib.bib23)]。这些模型使用 VGG16 作为其卷积神经网络。在 MSCOCO
    上，AP 和 mAP 没有差别。'
- en: '| Approach | Year | Val | Test |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | 验证集 | 测试集 |'
- en: '| AP | AP[50] | AP | AP[50] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| AP | AP[50] | AP | AP[50] |'
- en: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | - | - | - | 11.5 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | - | - | - | 11.5 |'
- en: '| WCCN [[32](#bib.bib32)] | CVPR2017 | - | - | - | 12.3 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| WCCN [[32](#bib.bib32)] | CVPR2017 | - | - | - | 12.3 |'
- en: '| PCL [[37](#bib.bib37)] | TRAMI2018 | 8.5 | 19.4 | - | - |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)] | TRAMI2018 | 8.5 | 19.4 | - | - |'
- en: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | 9.6 | 21.4 | - | - |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | 9.6 | 21.4 | - | - |'
- en: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | 10.8 | 22.7 | - | - |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | 10.8 | 22.7 | - | - |'
- en: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | 11.4 | 24.3 | 12.1 | 24.8 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | 11.4 | 24.3 | 12.1 | 24.8 |'
- en: '| Diba et al. [[73](#bib.bib73)]+SSD [[17](#bib.bib17)] | arXiv 2017 | - |
    - | - | 13.6 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Diba et al. [[73](#bib.bib73)]+SSD [[17](#bib.bib17)] | arXiv 2017 | - |
    - | - | 13.6 |'
- en: '| OICR [[31](#bib.bib31)]+Ens+FR [[65](#bib.bib65)] | CVPR2017 | 7.7 | 17.4
    | - | - |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| OICR [[31](#bib.bib31)]+Ens+FR [[65](#bib.bib65)] | CVPR2017 | 7.7 | 17.4
    | - | - |'
- en: '| MEFF [[72](#bib.bib72)]+FR [[65](#bib.bib65)] | CVPR2018 | 8.9 | 19.3 | -
    | - |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| MEFF [[72](#bib.bib72)]+FR [[65](#bib.bib65)] | CVPR2018 | 8.9 | 19.3 | -
    | - |'
- en: '| PCL [[37](#bib.bib37)]+Ens.+FR [[65](#bib.bib65)] | TPAMI2018 | 9.2 | 19.6
    | - | - |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)]+Ens.+FR [[65](#bib.bib65)] | TPAMI2018 | 9.2 | 19.6
    | - | - |'
- en: 'TABLE IV: Object localization performance on ILSVRC 2012 and CUB-200-2011 datasets.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: ILSVRC 2012 和 CUB-200-2011 数据集上的目标定位性能。'
- en: '| Approach | Year | ILSVRC 2012 (top error %) | CUB-200-2011 (top error %)
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 年份 | ILSVRC 2012 (最高错误率 %) | CUB-200-2011 (最高错误率 %) |'
- en: '| 1-err cls | 5-err cls | 1-err loc | 5-err loc | 1-err cls | 5-err cls | 1-err
    loc | 5-err loc |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 1-err cls | 5-err cls | 1-err loc | 5-err loc | 1-err cls | 5-err cls | 1-err
    loc | 5-err loc |'
- en: '| CAM [[26](#bib.bib26)] | CVPR2016 | 35.6 | 13.9 | 57.78 | 45.26 | - | - |
    - | - |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| CAM [[26](#bib.bib26)] | CVPR2016 | 35.6 | 13.9 | 57.78 | 45.26 | - | - |
    - | - |'
- en: '| ACoL [[40](#bib.bib40)] | CVPR2018 | 32.5 | 12.0 | 54.17 | 36.66 | - | -
    | 54.08 | 39.05 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| ACoL [[40](#bib.bib40)] | CVPR2018 | 32.5 | 12.0 | 54.17 | 36.66 | - | -
    | 54.08 | 39.05 |'
- en: '| SPG [[43](#bib.bib43)] | ECCV2018 | - | - | 51.4 | 35.05 | - | - | 53.36
    | 40.62 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| SPG [[43](#bib.bib43)] | ECCV2018 | - | - | 51.4 | 35.05 | - | - | 53.36
    | 40.62 |'
- en: '| DANet [[54](#bib.bib54)] | ICCV2019 | 32.5 | 12.0 | 54.17 | 40.57 | 24.6
    | 7.7 | 47.48 | 38.04 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[54](#bib.bib54)] | ICCV2019 | 32.5 | 12.0 | 54.17 | 40.57 | 24.6
    | 7.7 | 47.48 | 38.04 |'
- en: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | 27.7 | - | 49.83 | 39.31 | 26.6 |
    - | 47.6 | 34.97 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | 27.7 | - | 49.83 | 39.31 | 26.6 |
    - | 47.6 | 34.97 |'
- en: '| EIL [[56](#bib.bib56)] | CVPR2020 | 29.73 | - | 53.19 | - | 25.23 | - | 42.54
    | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| EIL [[56](#bib.bib56)] | CVPR2020 | 29.73 | - | 53.19 | - | 25.23 | - | 42.54
    | - |'
- en: 'TABLE V: Some techniques and tricks for improving detection results and the
    approaches that utilize them. 1) Cont: Context modeling, 2) Self-t: Self-training
    algorithm, 3) Casc: Cascaded network, 4) BboxR: Bounding box regression, 5) DisRR:
    Discriminative region removal, 6) Low-l: Incorporating low-level features, 7)
    Seg-D: Segmentation-detection collaborative mechanism, 8) Trans: Transforming
    WSOD to FSOD, 9) E-t-h: Easy-to-hard strategy, 10) NegE: Negative evidence, 11)
    SmoL: Optimizing smoothed loss functions.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 提高检测结果的某些技术和技巧以及利用这些技术的方法。1) Cont: 上下文建模，2) Self-t: 自我训练算法，3) Casc: 级联网络，4)
    BboxR: 边界框回归，5) DisRR: 判别区域移除，6) Low-l: 融合低级特征，7) Seg-D: 分割-检测协作机制，8) Trans: 将WSOD转化为FSOD，9)
    E-t-h: 从易到难策略，10) NegE: 负证据，11) SmoL: 优化平滑损失函数。'
- en: '| Approach | Specific techniques for discriminative region problem | Training
    tricks |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 针对判别区域问题的特定技术 | 训练技巧 |'
- en: '| Cont | Self-t | Casc | BboxR | DisRR | Low-l | Seg-D | Trans | E-t-h | NegE
    | SmoL |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Cont | Self-t | Casc | BboxR | DisRR | Low-l | Seg-D | Trans | E-t-h | NegE
    | SmoL |'
- en: '| WSDDN [[25](#bib.bib25)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| WSDDN [[25](#bib.bib25)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| CAM [[26](#bib.bib26)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| CAM [[26](#bib.bib26)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| WSLPDA [[27](#bib.bib27)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| WSLPDA [[27](#bib.bib27)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
- en: '| WELDON [[28](#bib.bib28)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| WELDON [[28](#bib.bib28)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
- en: '| ContextLocNet [[29](#bib.bib29)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| ContextLocNet [[29](#bib.bib29)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
- en: '| Grad-CAM [[30](#bib.bib30)] |  |  |  |  |  | $\surd$ |  |  |  |  |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Grad-CAM [[30](#bib.bib30)] |  |  |  |  |  | $\surd$ |  |  |  |  |  |'
- en: '| OICR [[31](#bib.bib31)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| OICR [[31](#bib.bib31)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
- en: '| WCCN [[32](#bib.bib32)] |  |  | $\surd$ |  |  |  |  |  |  |  |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| WCCN [[32](#bib.bib32)] |  |  | $\surd$ |  |  |  |  |  |  |  |  |'
- en: '| ST-WSL [[33](#bib.bib33)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| ST-WSL [[33](#bib.bib33)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
- en: '| WILDCAT [[34](#bib.bib34)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| WILDCAT [[34](#bib.bib34)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
- en: '| SPN [[35](#bib.bib35)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| SPN [[35](#bib.bib35)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| TP-WSL [[36](#bib.bib36)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| TP-WSL [[36](#bib.bib36)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
- en: '| PCL [[37](#bib.bib37)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| PCL [[37](#bib.bib37)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
- en: '| GAL-fWSD [[38](#bib.bib38)] |  |  |  |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| GAL-fWSD [[38](#bib.bib38)] |  |  |  |  |  |  |  | $\surd$ |  |  |  |'
- en: '| W2F [[39](#bib.bib39)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| W2F [[39](#bib.bib39)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
- en: '| ACoL [[40](#bib.bib40)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| ACoL [[40](#bib.bib40)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
- en: '| ZLDN [[41](#bib.bib41)] |  |  |  |  |  |  |  |  | $\surd$ |  |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ZLDN [[41](#bib.bib41)] |  |  |  |  |  |  |  |  | $\surd$ |  |  |'
- en: '| TS²C [[42](#bib.bib42)] | $\surd$ |  | $\surd$ |  |  |  |  | $\surd$ |  |  |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| TS²C [[42](#bib.bib42)] | $\surd$ |  | $\surd$ |  |  |  |  | $\surd$ |  |  |  |'
- en: '| SPG [[43](#bib.bib43)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| SPG [[43](#bib.bib43)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| WSRPN [[44](#bib.bib44)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| WSRPN [[44](#bib.bib44)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| C-MIL [[45](#bib.bib45)] |  |  |  |  |  |  |  |  |  |  | $\surd$ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| C-MIL [[45](#bib.bib45)] |  |  |  |  |  |  |  |  |  |  | $\surd$ |'
- en: '| WS-JDS [[46](#bib.bib46)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| WS-JDS [[46](#bib.bib46)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
- en: '| ADL [[47](#bib.bib47)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| ADL [[47](#bib.bib47)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Pred NET [[48](#bib.bib48)] |  |  |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Pred NET [[48](#bib.bib48)] |  |  |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
- en: '| WSOD2 [[49](#bib.bib49)] |  | $\surd$ |  | $\surd$ |  | $\surd$ |  |  |  |  |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| WSOD2 [[49](#bib.bib49)] |  | $\surd$ |  | $\surd$ |  | $\surd$ |  |  |  |  |  |'
- en: '| OAILWSD [[50](#bib.bib50)] | $\surd$ | $\surd$ |  |  |  |  |  |  |  |  |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| OAILWSD [[50](#bib.bib50)] | $\surd$ | $\surd$ |  |  |  |  |  |  |  |  |  |'
- en: '| TPWSD [[51](#bib.bib51)] |  | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| TPWSD [[51](#bib.bib51)] |  | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
- en: '| SDCN [[52](#bib.bib52)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| SDCN [[52](#bib.bib52)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
- en: '| C-MIDN [[53](#bib.bib53)] |  | $\surd$ |  |  | $\surd$ |  |  | $\surd$ |  |  |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| C-MIDN [[53](#bib.bib53)] |  | $\surd$ |  |  | $\surd$ |  |  | $\surd$ |  |  |  |'
- en: '| DANet [[54](#bib.bib54)] |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[54](#bib.bib54)] |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| NL-CCAM [[55](#bib.bib55)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| NL-CCAM [[55](#bib.bib55)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
- en: '| ICMWSD [[23](#bib.bib23)] | $\surd$ | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ICMWSD [[23](#bib.bib23)] | $\surd$ | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
- en: '| EIL [[56](#bib.bib56)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| EIL [[56](#bib.bib56)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
- en: '| SLV [[57](#bib.bib57)] |  | $\surd$ |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| SLV [[57](#bib.bib57)] |  | $\surd$ |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
- en: IX Future Directions and Tasks
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 未来方向与任务
- en: Although we have summarized many advanced techniques and tricks for improving
    detection results, there are still several research directions that can be further
    explored.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们总结了许多提升检测结果的先进技术和技巧，但仍有几个研究方向可以进一步探索。
- en: IX-A Model Directions
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A 模型方向
- en: Better Initial Proposals. The main proposal generators of the existing methods
    are selective search [[58](#bib.bib58)], edge boxes [[59](#bib.bib59)], heatmap,
    and sliding window. Selective search and edge boxes are too time-consuming and
    yield plenty of initial proposals that most of them are negative proposal. Segmenting
    heatmap tends to focus on the discriminative part of the object. The performance
    of the sliding window is strongly dependent on the size of objects. For example,
    if the size of the object instance is roughly fixed, then the sliding window works
    very well. Otherwise, it works badly. Because these generators have inherent disadvantages,
    we need to design a proposal generator that can yield fewer and more accurate
    initial proposals. The quality of the initial proposals directly affects the detection
    performance of the detector. So how to yield good initial proposals may be a new
    research direction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的初始提案。现有方法的主要提案生成器包括选择性搜索 [[58](#bib.bib58)]、边缘框 [[59](#bib.bib59)]、热图和滑动窗口。选择性搜索和边缘框耗时太长，并产生大量初始提案，其中大多数是负样本提案。热图分割往往集中在物体的区分部分。滑动窗口的性能严重依赖于物体的大小。例如，如果物体实例的大小基本固定，则滑动窗口效果很好。否则，效果较差。由于这些生成器存在固有缺陷，我们需要设计一个能够生成更少且更准确初始提案的生成器。初始提案的质量直接影响检测器的检测性能。因此，如何生成良好的初始提案可能是一个新的研究方向。
- en: Better Positive Proposals. Most WSOD methods select the proposals with the top
    score as positive proposals, which tend to focus on the most discriminative parts
    of the object rather than the whole object region. Because of the above problem,
    ST-WSL [[33](#bib.bib33)] selects positive proposals according to the number of
    their surrounding proposals. And Self-Taught-WS [[33](#bib.bib33)] selects positive
    proposals relying on the relative improvement (RI) of the scores of each proposal
    of two adjacent epochs. Besides, the key of self-training and cascaded network
    is to select accurate proposals as the pseudo ground-truth boxes for later training.
    Thus, how can we design a better algorithm that can accurately select positive
    proposals may be an important research direction.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的正样本提案。大多数WSOD方法选择得分最高的提案作为正样本，这些提案通常集中在物体的最具区分性的部分，而不是整个物体区域。由于上述问题，ST-WSL [[33](#bib.bib33)]根据周围提案的数量选择正样本。Self-Taught-WS [[33](#bib.bib33)]依赖于两个相邻时期每个提案得分的相对提升（RI）来选择正样本。此外，自我训练和级联网络的关键是选择准确的提案作为后续训练的伪真值框。因此，如何设计一个能够准确选择正样本的更好算法可能是一个重要的研究方向。
- en: Lightweight Network. Today’s state-of-the-art object detectors [[14](#bib.bib14),
    [20](#bib.bib20)] leverage a very deep CNN to extract image feature maps and high-dimension
    fully connected layers to detect object instances that can achieve satisfactory
    detection performance. But the deep CNN and high-dimension fully connected layers
    rely on large memory and strong GPU computation power. Hence, a deep network is
    difficult to deploy on CPU devices (e.g., mobile phones). With the popularity
    of mobile devices, lightweight network with few parameters has received more and
    more attention from researchers, such as Light-Head R-CNN [[74](#bib.bib74)].
    Thus, designing a lightweight network in weakly supervised object detection may
    be a new research direction.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 轻量级网络。当前最先进的目标检测器[[14](#bib.bib14), [20](#bib.bib20)]利用非常深的 CNN 提取图像特征图和高维全连接层来检测目标实例，从而实现令人满意的检测性能。但深度
    CNN 和高维全连接层依赖于大内存和强大的 GPU 计算能力。因此，深度网络难以在 CPU 设备（如手机）上部署。随着移动设备的普及，具有较少参数的轻量级网络受到了越来越多研究人员的关注，例如
    Light-Head R-CNN[[74](#bib.bib74)]。因此，在弱监督目标检测中设计轻量级网络可能是一个新的研究方向。
- en: IX-B Application Directions
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B 应用方向
- en: Medical Imaging. With the development of deep learning, it has evolved into
    cross-learning with multiple disciplines, especially the medical field. Because
    of lacking brain’s Magnetic Resonance Imaging (MRI) and X-rays images with sufficient
    labels, weakly-supervised brain lesion detection [[75](#bib.bib75), [76](#bib.bib76)]
    has received attention from researchers. The purpose of weakly-supervised brain
    lesion detection is to give the model the ability to accurately locate lesion
    region and classify lesion category that helps the doctor complete the diagnosis
    of the disease. Weakly-supervised lesion detection is not only applied in brain
    disease, but also other organ diseases, such as chest, abdomen, and pelvis. In
    addition to lesion detection, weakly-supervised learning is applied in disease
    prognosis [[77](#bib.bib77)]. During hospital visits, patients need to undergo
    a large number of tests to pinpoint their disease. These tests are generally presented
    to doctors and patients in the form of graphic reports. However, these numerous
    graphic reports lack correct labeling information. So, medical imaging may be
    another potential research direction in a weakly supervised setting.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 医学影像。随着深度学习的发展，它已经演变为与多个学科的交叉学习，特别是医学领域。由于缺乏足够标签的脑部磁共振成像（MRI）和 X 光图像，弱监督脑部病变检测[[75](#bib.bib75),
    [76](#bib.bib76)]引起了研究人员的关注。弱监督脑部病变检测的目的是赋予模型准确定位病变区域和分类病变类别的能力，以帮助医生完成疾病的诊断。弱监督病变检测不仅应用于脑部疾病，还应用于其他器官疾病，如胸部、腹部和盆腔。除了病变检测，弱监督学习还应用于疾病预后[[77](#bib.bib77)]。在医院就诊期间，患者需要进行大量检查以确定疾病。这些检查通常以图形报告的形式呈现给医生和患者。然而，这些众多的图形报告缺乏正确的标注信息。因此，医学影像可能是弱监督设置下的另一个潜在研究方向。
- en: 3D Object Detection. In recent years, with the continuous improvement of the
    accuracy of object detection of images [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)], 3D object detection [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)] has received unprecedented attention. The purpose of 3D object
    detection is to detect object instances in the point cloud using 3D bounding boxes.
    Comparing with 2D object detection, 3D object detection tends to cost more computation
    and its supervision is more difficult to obtain and labor-intensive. Therefore,
    how to train light and accurate 3D detection models in the point cloud using simple
    labels may be a big challenge. Fortunately, weakly-supervised object detection
    is successfully applied in 2D object detection. According to the above analysis,
    we think that 3D weakly-supervised object detection that uses weak supervision(e.g.,
    2D bounding boxes and text labels) to train object detection models in the 3D
    scene may be a hot research direction.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 3D目标检测。近年来，随着图像目标检测精度的不断提高 [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)]，3D目标检测 [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)] 收到了前所未有的关注。3D目标检测的目的是在点云中使用3D边界框检测目标实例。与2D目标检测相比，3D目标检测通常需要更多的计算，其监督更难获得且劳动密集。因此，如何在点云中使用简单标签训练轻量且准确的3D检测模型可能是一个重大挑战。幸运的是，弱监督目标检测已成功应用于2D目标检测。根据以上分析，我们认为在3D场景中使用弱监督（例如，2D边界框和文本标签）训练目标检测模型的3D弱监督目标检测可能是一个热门研究方向。
- en: Video Object Detection. Video object detection [[85](#bib.bib85), [86](#bib.bib86)]
    is to classify and locate object instances in a piece of video. One of the solutions
    is to break the video into many frames and the detector achieves object detection
    in these frame images [[87](#bib.bib87), [88](#bib.bib88)]. However, the detector
    will face one big problem that the quality of these frame images has deteriorated.
    To improve the performance of video object detection, expanding the training dataset
    is a good approach. Unfortunately, tagging object location and category in videos
    is much more difficult than in 2D images. Therefore, training video object detection
    in the weakly-supervised setting is necessary.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 视频目标检测。视频目标检测 [[85](#bib.bib85), [86](#bib.bib86)] 是在一段视频中对目标实例进行分类和定位。一个解决方案是将视频切分为许多帧，检测器在这些帧图像中实现目标检测
    [[87](#bib.bib87), [88](#bib.bib88)]。然而，检测器将面临一个大问题，即这些帧图像的质量已经下降。为了提高视频目标检测的性能，扩展训练数据集是一个很好的方法。不幸的是，在视频中标注目标位置和类别比在2D图像中要困难得多。因此，在弱监督设置下训练视频目标检测是必要的。
- en: X Conclusions
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 结论
- en: In this paper, we summarize plenty of the deep learning WSOD methods and give
    a lot of solutions to solve the above challenges. In summary, the main contents
    of this paper are listed as follows.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们总结了大量深度学习WSOD方法，并提供了许多解决上述挑战的方案。总而言之，本文的主要内容如下。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze the background, and main challenges, and basic framework of WSOD.
    Furthermore, we introduce several landmark methods in detail.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了WSOD的背景、主要挑战和基本框架。此外，我们详细介绍了几个标志性方法。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For main challenges, we analyze almost all of the WSOD methods since 2016 and
    summarize numerous techniques and training tricks (cf. TABLE [V](#S8.T5 "TABLE
    V ‣ VIII-C Experimental Results ‣ VIII Datasets and Performance Evaluation ‣ Deep
    Learning for Weakly-Supervised Object Detection and Object Localization: A Survey")).'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '针对主要挑战，我们分析了自2016年以来几乎所有的WSOD方法，并总结了大量的技术和训练技巧（参见 TABLE [V](#S8.T5 "TABLE V
    ‣ VIII-C Experimental Results ‣ VIII Datasets and Performance Evaluation ‣ Deep
    Learning for Weakly-Supervised Object Detection and Object Localization: A Survey")）。'
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce currently popular datasets and important evaluation metrics in
    the WSOD task.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了当前流行的数据集和WSOD任务中的重要评估指标。
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conclude and discuss valuable insights and guidelines for future progress
    in model and application directions.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们总结并讨论了对未来模型和应用方向进展的宝贵见解和指导方针。
- en: Acknowledgment
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the National Natural Science Foundation of China
    (U19B2043, 61976185), Zhejiang Natural Science Foundation (LR19F020002, LZ17F020001),
    the Fundamental Research Funds for the Central Universities, Chinese Knowledge
    Center for Engineering Sciences and Technology.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了中国国家自然科学基金（U19B2043, 61976185）、浙江省自然科学基金（LR19F020002, LZ17F020001）、中央高校基本科研业务费、中国工程科学与技术知识中心的支持。
- en: References
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
    learning: A review,” *IEEE transactions on neural networks and learning systems*,
    2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Z.-Q. Zhao, P. Zheng, S.-t. Xu, 和 X. Wu，“基于深度学习的目标检测：综述”，*IEEE神经网络与学习系统汇刊*，2019年。'
- en: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *International Journal
    of Computer Vision*, 2020.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, 和 M. Pietikäinen，“通用目标检测的深度学习：综述”，*国际计算机视觉杂志*，2020年。'
- en: '[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International Journal of
    Computer Vision*, 2010.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, 和 A. Zisserman，“Pascal
    视觉目标类别（VOC）挑战”，*国际计算机视觉杂志*，2010年。'
- en: '[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *et al.*, “Imagenet large scale visual recognition
    challenge,” *International Journal of Computer Vision*, 2015.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *等*，“ImageNet 大规模视觉识别挑战”，*国际计算机视觉杂志*，2015年。'
- en: '[5] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz,
    S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim-to-sim: Data-efficient
    robotic grasping via randomized-to-canonical adaptation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz,
    S. Levine, R. Hadsell, 和 K. Bousmalis，“通过模拟到模拟的真实：通过随机到规范的适配网络实现数据高效的机器人抓取”，在*IEEE计算机视觉与模式识别会议论文集*，2019年。'
- en: '[6] P. Hu and D. Ramanan, “Finding tiny faces,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] P. Hu 和 D. Ramanan，“寻找微小人脸”，在*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[7] C. Bai, H. Li, J. Zhang, L. Huang, and L. Zhang, “Unsupervised adversarial
    instance-level image retrieval,” *IEEE Transactions on Multimedia*, 2021.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Bai, H. Li, J. Zhang, L. Huang, 和 L. Zhang，“无监督对抗实例级图像检索”，*IEEE多媒体汇刊*，2021年。'
- en: '[8] W. Chen, Y. Liu, N. Pu, W. Wang, L. Liu, and M. S. Lew, “Feature estimations
    based correlation distillation for incremental image retrieval,” *IEEE Transactions
    on Multimedia*, 2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] W. Chen, Y. Liu, N. Pu, W. Wang, L. Liu, 和 M. S. Lew，“基于特征估计的相关性蒸馏用于增量图像检索”，*IEEE多媒体汇刊*，2021年。'
- en: '[9] M. Tomei, M. Cornia, L. Baraldi, and R. Cucchiara, “Art2real: Unfolding
    the reality of artworks via semantically-aware image-to-image translation,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Tomei, M. Cornia, L. Baraldi, 和 R. Cucchiara，“Art2real：通过语义感知的图像到图像翻译展开艺术品的现实”，在*IEEE计算机视觉与模式识别会议论文集*，2019年。'
- en: '[10] A. Behl, O. Hosseini Jafari, S. Karthik Mustikovela, H. Abu Alhaija, C. Rother,
    and A. Geiger, “Bounding boxes, segmentations and object coordinates: How important
    is recognition for 3d scene flow estimation in autonomous driving scenarios?”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2017.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Behl, O. Hosseini Jafari, S. Karthik Mustikovela, H. Abu Alhaija, C.
    Rother, 和 A. Geiger，“边界框、分割和目标坐标：在自动驾驶场景中，识别对于3D场景流估计有多重要？”在*IEEE/CVF国际计算机视觉会议论文集*，2017年。'
- en: '[11] J. Liu, M. Gong, K. Qin, and P. Zhang, “A deep convolutional coupling
    network for change detection based on heterogeneous optical and radar images,”
    *IEEE transactions on neural networks and learning systems*, 2016.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Liu, M. Gong, K. Qin, 和 P. Zhang，“一种基于异构光学和雷达图像的深度卷积耦合网络用于变化检测”，*IEEE神经网络与学习系统汇刊*，2016年。'
- en: '[12] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K. Simonyan 和 A. Zisserman，“用于大规模图像识别的非常深的卷积网络”，*arXiv 预印本 arXiv:1409.1556*，2014年。'
- en: '[13] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich, “深入卷积网络”，见 *IEEE计算机视觉与模式识别会议论文集*，2015年。'
- en: '[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K. He, X. Zhang, S. Ren 和 J. Sun, “深度残差学习用于图像识别”，见 *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[15] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *European Conference
    on Computer Vision*, 2014.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár
    和 C. L. Zitnick, “Microsoft coco：上下文中的常见物体”，见 *欧洲计算机视觉会议*，2014年。'
- en: '[16] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Ren, K. He, R. Girshick 和 J. Sun, “Faster r-cnn：基于区域提议网络的实时物体检测”，见
    *IEEE模式分析与机器智能学报*，2015年。'
- en: '[17] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European Conference on Computer
    Vision*, 2016.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu 和 A. C. Berg,
    “Ssd：单次多框检测器”，见 *欧洲计算机视觉会议*，2016年。'
- en: '[18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Redmon, S. Divvala, R. Girshick 和 A. Farhadi, “你只看一次：统一的实时物体检测”，见 *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[19] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] K. He, G. Gkioxari, P. Dollár 和 R. Girshick, “Mask r-cnn”，见 *IEEE/CVF国际计算机视觉会议论文集*，2017年。'
- en: '[20] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan 和 S. Belongie,
    “用于物体检测的特征金字塔网络”，见 *IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[21] H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,”
    in *European Conference on Computer Vision*, 2018.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] H. Law 和 J. Deng, “Cornernet：将物体检测为配对关键点”，见 *欧洲计算机视觉会议*，2018年。'
- en: '[22] B. Singh, M. Najibi, and L. S. Davis, “Sniper: Efficient multi-scale training,”
    in *arXiv preprint arXiv:1805.09300*, 2018.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] B. Singh, M. Najibi 和 L. S. Davis, “Sniper：高效的多尺度训练”，见 *arXiv预印本 arXiv:1805.09300*，2018年。'
- en: '[23] Z. Ren, Z. Yu, X. Yang, M.-Y. Liu, Y. J. Lee, A. G. Schwing, and J. Kautz,
    “Instance-aware, context-focused, and memory-efficient weakly supervised object
    detection,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2020.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Ren, Z. Yu, X. Yang, M.-Y. Liu, Y. J. Lee, A. G. Schwing 和 J. Kautz,
    “实例感知、关注上下文和高效记忆的弱监督物体检测”，见 *IEEE计算机视觉与模式识别会议论文集*，2020年。'
- en: '[24] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes challenge 2007 (voc2007) results,” 2007.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Everingham, L. Van Gool, C. K. Williams, J. Winn 和 A. Zisserman, “Pascal视觉目标类别挑战赛2007（voc2007）结果”，2007年。'
- en: '[25] H. Bilen and A. Vedaldi, “Weakly supervised deep detection networks,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2016.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Bilen 和 A. Vedaldi, “弱监督深度检测网络”，见 *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[26] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva 和 A. Torralba, “用于区分定位的深度特征学习”，见
    *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[27] D. Li, J.-B. Huang, Y. Li, S. Wang, and M.-H. Yang, “Weakly supervised
    object localization with progressive domain adaptation,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] D. Li, J.-B. Huang, Y. Li, S. Wang 和 M.-H. Yang, “弱监督物体定位与渐进式领域适应”，见 *IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[28] T. Durand, N. Thome, and M. Cord, “Weldon: Weakly supervised learning
    of deep convolutional neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Durand, N. Thome, 和 M. Cord，“Weldon：弱监督学习深度卷积神经网络，”发表于*IEEE计算机视觉与模式识别会议论文集*，2016年。'
- en: '[29] V. Kantorov, M. Oquab, M. Cho, and I. Laptev, “Contextlocnet: Context-aware
    deep network models for weakly supervised localization,” in *European Conference
    on Computer Vision*, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] V. Kantorov, M. Oquab, M. Cho, 和 I. Laptev，“Contextlocnet：针对弱监督定位的上下文感知深度网络模型，”发表于*欧洲计算机视觉会议*，2016年。'
- en: '[30] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, 和 D. Batra，“Grad-cam：通过基于梯度的定位从深度网络中获得视觉解释，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2017年。'
- en: '[31] P. Tang, X. Wang, X. Bai, and W. Liu, “Multiple instance detection network
    with online instance classifier refinement,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] P. Tang, X. Wang, X. Bai, 和 W. Liu，“带有在线实例分类器精化的多实例检测网络，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[32] A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash, and L. Van Gool, “Weakly
    supervised cascaded convolutional networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash, 和 L. Van Gool，“弱监督级联卷积网络，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[33] Z. Jie, Y. Wei, X. Jin, J. Feng, and W. Liu, “Deep self-taught learning
    for weakly supervised object localization,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. Jie, Y. Wei, X. Jin, J. Feng, 和 W. Liu，“针对弱监督物体定位的深度自我学习，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[34] T. Durand, T. Mordan, N. Thome, and M. Cord, “Wildcat: Weakly supervised
    learning of deep convnets for image classification, pointwise localization and
    segmentation,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T. Durand, T. Mordan, N. Thome, 和 M. Cord，“Wildcat：弱监督学习深度卷积网络用于图像分类、逐点定位和分割，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[35] Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao, “Soft proposal networks for
    weakly supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, 和 J. Jiao，“用于弱监督物体定位的软提议网络，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2017年。'
- en: '[36] D. Kim, D. Cho, D. Yoo, and I. So Kweon, “Two-phase learning for weakly
    supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Kim, D. Cho, D. Yoo, 和 I. So Kweon，“弱监督物体定位的两阶段学习，”发表于*IEEE/CVF国际计算机视觉会议论文集*，2017年。'
- en: '[37] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, and A. Yuille, “Pcl:
    Proposal cluster learning for weakly supervised object detection,” *IEEE transactions
    on pattern analysis and machine intelligence*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, 和 A. Yuille，“Pcl：用于弱监督物体检测的提议集群学习，”*IEEE模式分析与机器智能学报*，2018年。'
- en: '[38] Y. Shen, R. Ji, S. Zhang, W. Zuo, and Y. Wang, “Generative adversarial
    learning towards fast weakly supervised detection,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Shen, R. Ji, S. Zhang, W. Zuo, 和 Y. Wang，“生成对抗学习用于快速弱监督检测，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年。'
- en: '[39] Y. Zhang, Y. Bai, M. Ding, Y. Li, and B. Ghanem, “W2f: A weakly-supervised
    to fully-supervised framework for object detection,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Zhang, Y. Bai, M. Ding, Y. Li, 和 B. Ghanem，“W2f：从弱监督到完全监督的物体检测框架，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年。'
- en: '[40] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang, “Adversarial complementary
    learning for weakly supervised object localization,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] X. Zhang, Y. Wei, J. Feng, Y. Yang, 和 T. S. Huang，“针对弱监督物体定位的对抗互补学习，”发表于*IEEE计算机视觉与模式识别会议论文集*，2018年。'
- en: '[41] X. Zhang, J. Feng, H. Xiong, and Q. Tian, “Zigzag learning for weakly
    supervised object detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] X. Zhang, J. Feng, H. Xiong, 和 Q. Tian，“弱监督物体检测的锯齿形学习”，发表于*IEEE 计算机视觉与模式识别会议论文集*，2018年。'
- en: '[42] Y. Wei, Z. Shen, B. Cheng, H. Shi, J. Xiong, J. Feng, and T. Huang, “Ts2c:
    Tight box mining with surrounding segmentation context for weakly supervised object
    detection,” in *European Conference on Computer Vision*, 2018.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Wei, Z. Shen, B. Cheng, H. Shi, J. Xiong, J. Feng, 和 T. Huang，“Ts2c:
    紧凑框挖掘与周围分割上下文用于弱监督物体检测”，发表于*欧洲计算机视觉会议*，2018年。'
- en: '[43] X. Zhang, Y. Wei, G. Kang, Y. Yang, and T. Huang, “Self-produced guidance
    for weakly-supervised object localization,” in *European Conference on Computer
    Vision*, 2018.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] X. Zhang, Y. Wei, G. Kang, Y. Yang, 和 T. Huang，“自生成指导用于弱监督物体定位”，发表于*欧洲计算机视觉会议*，2018年。'
- en: '[44] P. Tang, X. Wang, A. Wang, Y. Yan, W. Liu, J. Huang, and A. Yuille, “Weakly
    supervised region proposal network and object detection,” in *European Conference
    on Computer Vision*, 2018.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] P. Tang, X. Wang, A. Wang, Y. Yan, W. Liu, J. Huang, 和 A. Yuille，“弱监督区域提议网络与物体检测”，发表于*欧洲计算机视觉会议*，2018年。'
- en: '[45] F. Wan, C. Liu, W. Ke, X. Ji, J. Jiao, and Q. Ye, “C-mil: Continuation
    multiple instance learning for weakly supervised object detection,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] F. Wan, C. Liu, W. Ke, X. Ji, J. Jiao, 和 Q. Ye，“C-mil: 续接多实例学习用于弱监督物体检测”，发表于*IEEE
    计算机视觉与模式识别会议论文集*，2019年。'
- en: '[46] Y. Shen, R. Ji, Y. Wang, Y. Wu, and L. Cao, “Cyclic guidance for weakly
    supervised joint detection and segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Shen, R. Ji, Y. Wang, Y. Wu, 和 L. Cao，“循环指导用于弱监督联合检测与分割”，发表于*IEEE 计算机视觉与模式识别会议论文集*，2019年。'
- en: '[47] J. Choe and H. Shim, “Attention-based dropout layer for weakly supervised
    object localization,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2019.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Choe 和 H. Shim，“基于注意力的 dropout 层用于弱监督物体定位”，发表于*IEEE 计算机视觉与模式识别会议论文集*，2019年。'
- en: '[48] A. Arun, C. Jawahar, and M. P. Kumar, “Dissimilarity coefficient based
    weakly supervised object detection,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Arun, C. Jawahar, 和 M. P. Kumar，“基于相异性系数的弱监督物体检测”，发表于*IEEE 计算机视觉与模式识别会议论文集*，2019年。'
- en: '[49] Z. Zeng, B. Liu, J. Fu, H. Chao, and L. Zhang, “Wsod2: Learning bottom-up
    and top-down objectness distillation for weakly-supervised object detection,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. Zeng, B. Liu, J. Fu, H. Chao, 和 L. Zhang，“Wsod2: 学习自下而上与自上而下的物体性蒸馏用于弱监督物体检测”，发表于*IEEE/CVF
    国际计算机视觉会议论文集*，2019年。'
- en: '[50] S. Kosugi, T. Yamasaki, and K. Aizawa, “Object-aware instance labeling
    for weakly supervised object detection,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Kosugi, T. Yamasaki, 和 K. Aizawa，“面向物体的实例标注用于弱监督物体检测”，发表于*IEEE/CVF
    国际计算机视觉会议论文集*，2019年。'
- en: '[51] K. Yang, D. Li, and Y. Dou, “Towards precise end-to-end weakly supervised
    object detection network,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] K. Yang, D. Li, 和 Y. Dou，“迈向精准的端到端弱监督物体检测网络”，发表于*IEEE/CVF 国际计算机视觉会议论文集*，2019年。'
- en: '[52] X. Li, M. Kan, S. Shan, and X. Chen, “Weakly supervised object detection
    with segmentation collaboration,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] X. Li, M. Kan, S. Shan, 和 X. Chen，“结合分割的弱监督物体检测”，发表于*IEEE/CVF 国际计算机视觉会议论文集*，2019年。'
- en: '[53] Y. Gao, B. Liu, N. Guo, X. Ye, F. Wan, H. You, and D. Fan, “C-midn: Coupled
    multiple instance detection network with segmentation guidance for weakly supervised
    object detection,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Gao, B. Liu, N. Guo, X. Ye, F. Wan, H. You, 和 D. Fan，“C-midn: 结合分割指导的耦合多实例检测网络用于弱监督物体检测”，发表于*IEEE/CVF
    国际计算机视觉会议论文集*，2019年。'
- en: '[54] H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, and Q. Ye, “Danet: Divergent activation
    for weakly supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, 和 Q. Ye，“Danet: 异质激活用于弱监督物体定位”，发表于*IEEE/CVF
    国际计算机视觉会议论文集*，2019年。'
- en: '[55] S. Yang, Y. Kim, Y. Kim, and C. Kim, “Combinational class activation maps
    for weakly supervised object localization,” in *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, 2020.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Yang, Y. Kim, Y. Kim 和 C. Kim，“用于弱监督对象定位的组合类别激活图，” 在*IEEE/CVF计算机视觉应用冬季会议论文集*，2020。'
- en: '[56] J. Mai, M. Yang, and W. Luo, “Erasing integrated learning: A simple yet
    effective approach for weakly supervised object localization,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. Mai, M. Yang 和 W. Luo，“集成学习消除：一种简单而有效的弱监督对象定位方法，” 在*IEEE计算机视觉与模式识别会议论文集*，2020。'
- en: '[57] Z. Chen, Z. Fu, R. Jiang, Y. Chen, and X.-S. Hua, “Slv: Spatial likelihood
    voting for weakly supervised object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2020.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Z. Chen, Z. Fu, R. Jiang, Y. Chen 和 X.-S. Hua，“SLV：用于弱监督对象检测的空间似然投票，”
    在*IEEE计算机视觉与模式识别会议论文集*，2020。'
- en: '[58] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Selective
    search for object recognition,” *International Journal of Computer Vision*, 2013.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. R. Uijlings, K. E. Van De Sande, T. Gevers 和 A. W. Smeulders，“用于对象识别的选择性搜索，”
    *计算机视觉国际杂志*，2013。'
- en: '[59] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
    edges,” in *European Conference on Computer Vision*, 2014.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. L. Zitnick 和 P. Dollár，“边缘框：从边缘定位对象建议，” 在*欧洲计算机视觉会议*，2014。'
- en: '[60] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, “Solving the multiple
    instance problem with axis-parallel rectangles,” *Artificial intelligence*, 1997.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. G. Dietterich, R. H. Lathrop 和 T. Lozano-Pérez，“用轴对齐矩形解决多个实例问题，” *人工智能*，1997。'
- en: '[61] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，“使用深度卷积神经网络进行Imagenet分类，” 在*神经信息处理系统进展*，2012。'
- en: '[62] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Hu, L. Shen 和 G. Sun，“挤压与激励网络，” 在*IEEE计算机视觉与模式识别会议论文集*，2018。'
- en: '[63] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
    convolutional networks for visual recognition,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] K. He, X. Zhang, S. Ren 和 J. Sun，“用于视觉识别的深度卷积网络中的空间金字塔池化，” *IEEE模式分析与机器智能汇刊*，2015。'
- en: '[64] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving
    for simplicity: The all convolutional net,” *arXiv preprint arXiv:1412.6806*,
    2014.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. T. Springenberg, A. Dosovitskiy, T. Brox 和 M. Riedmiller，“追求简洁：全卷积网络，”
    *arXiv预印本 arXiv:1412.6806*，2014。'
- en: '[65] R. Girshick, “Fast r-cnn,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2015.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] R. Girshick，“快速 R-CNN，” 在*IEEE/CVF国际计算机视觉会议论文集*，2015。'
- en: '[66] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in *18th
    International Conference on Pattern Recognition*, 2006.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Neubeck 和 L. Van Gool，“高效的非最大抑制，” 在*第18届国际模式识别会议*，2006。'
- en: '[67] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and
    P. Perona, “Caltech-ucsd birds 200,” 2010.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie 和 P.
    Perona，“Caltech-UCSD鸟类200，” 2010。'
- en: '[68] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd
    birds-200-2011 dataset,” 2011.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] C. Wah, S. Branson, P. Welinder, P. Perona 和 S. Belongie，“Caltech-UCSD鸟类数据集-200-2011，”
    2011。'
- en: '[69] M. Shi, H. Caesar, and V. Ferrari, “Weakly supervised object localization
    using things and stuff transfer,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. Shi, H. Caesar 和 V. Ferrari，“使用物品和物质转移进行弱监督对象定位，” 在*IEEE/CVF国际计算机视觉会议论文集*，2017。'
- en: '[70] F. Wan, P. Wei, J. Jiao, Z. Han, and Q. Ye, “Min-entropy latent model
    for weakly supervised object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] F. Wan, P. Wei, J. Jiao, Z. Han 和 Q. Ye，“用于弱监督对象检测的最小熵潜在模型，” 在*IEEE计算机视觉与模式识别会议论文集*，2018。'
- en: '[71] M. Gao, A. Li, R. Yu, V. I. Morariu, and L. S. Davis, “C-wsl: Count-guided
    weakly supervised localization,” in *European Conference on Computer Vision*,
    2018.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] M. Gao, A. Li, R. Yu, V. I. Morariu 和 L. S. Davis，“C-WSL：计数指导的弱监督定位，”
    在*欧洲计算机视觉会议*，2018。'
- en: '[72] W. Ge, S. Yang, and Y. Yu, “Multi-evidence filtering and fusion for multi-label
    classification, object detection and semantic segmentation based on weakly supervised
    learning,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2018.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Ge, S. Yang, 和 Y. Yu，“基于弱监督学习的多标签分类、物体检测和语义分割的多证据过滤与融合，”在*IEEE计算机视觉与模式识别会议论文集*，2018年。'
- en: '[73] A. Diba, V. Sharma, R. Stiefelhagen, and L. Van Gool, “Object discovery
    by generative adversarial & ranking networks,” *arXiv preprint arXiv:1711.08174*,
    2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] A. Diba, V. Sharma, R. Stiefelhagen, 和 L. Van Gool，“通过生成对抗网络和排序网络进行物体发现，”*arXiv预印本
    arXiv:1711.08174*，2017年。'
- en: '[74] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn:
    In defense of two-stage object detector,” *arXiv preprint arXiv:1711.07264*, 2017.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, 和 J. Sun，“Light-Head R-CNN：捍卫双阶段物体检测器，”*arXiv预印本
    arXiv:1711.07264*，2017年。'
- en: '[75] K. Wu, B. Du, M. Luo, H. Wen, Y. Shen, and J. Feng, “Weakly supervised
    brain lesion segmentation via attentional representation learning,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] K. Wu, B. Du, M. Luo, H. Wen, Y. Shen, 和 J. Feng，“通过注意力表示学习的弱监督脑病灶分割，”在*国际医学图像计算与计算机辅助干预会议*，2019年。'
- en: '[76] Z. Ji, Y. Shen, C. Ma, and M. Gao, “Scribble-based hierarchical weakly
    supervised learning for brain tumor segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Z. Ji, Y. Shen, C. Ma, 和 M. Gao，“基于涂鸦的层次弱监督学习用于脑肿瘤分割，”在*国际医学图像计算与计算机辅助干预会议*，2019年。'
- en: '[77] M. Liu, J. Zhang, C. Lian, and D. Shen, “Weakly supervised deep learning
    for brain disease prognosis using mri and incomplete clinical scores,” *IEEE transactions
    on cybernetics*, 2019.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] M. Liu, J. Zhang, C. Lian, 和 D. Shen，“基于MRI和不完整临床评分的弱监督深度学习脑病预后，”*IEEE控制论学报*，2019年。'
- en: '[78] Q. Ren, S. Lu, J. Zhang, and R. Hu, “Salient object detection by fusing
    local and global contexts,” *IEEE Transactions on Multimedia*, 2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Q. Ren, S. Lu, J. Zhang, 和 R. Hu，“通过融合局部和全局上下文进行显著性物体检测，”*IEEE多媒体学报*，2020年。'
- en: '[79] S. Wu, Y. Xu, B. Zhang, J. Yang, and D. Zhang, “Deformable template network
    (dtn) for object detection,” *IEEE Transactions on Multimedia*, 2021.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Wu, Y. Xu, B. Zhang, J. Yang, 和 D. Zhang，“用于物体检测的可变形模板网络（dtn），”*IEEE多媒体学报*，2021年。'
- en: '[80] C. Deng, M. Wang, L. Liu, Y. Liu, and Y. Jiang, “Extended feature pyramid
    network for small object detection,” *IEEE Transactions on Multimedia*, 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] C. Deng, M. Wang, L. Liu, Y. Liu, 和 Y. Jiang，“用于小物体检测的扩展特征金字塔网络，”*IEEE多媒体学报*，2021年。'
- en: '[81] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] X. Chen, H. Ma, J. Wan, B. Li, 和 T. Xia，“用于自动驾驶的多视角3D物体检测网络，”在*IEEE计算机视觉与模式识别会议论文集*，2017年。'
- en: '[82] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “Std: Sparse-to-dense 3d
    object detector for point cloud,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Z. Yang, Y. Sun, S. Liu, X. Shen, 和 J. Jia，“STD：稀疏到密集的3D物体检测器，”在*IEEE/CVF国际计算机视觉会议论文集*，2019年。'
- en: '[83] J. Chen, B. Lei, Q. Song, H. Ying, D. Z. Chen, and J. Wu, “A hierarchical
    graph network for 3d object detection on point clouds,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Chen, B. Lei, Q. Song, H. Ying, D. Z. Chen, 和 J. Wu，“用于点云3D物体检测的层次图网络，”在*IEEE计算机视觉与模式识别会议论文集*，2020年。'
- en: '[84] W. Shi and R. Rajkumar, “Point-gnn: Graph neural network for 3d object
    detection in a point cloud,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] W. Shi 和 R. Rajkumar，“Point-GNN：用于点云中3D物体检测的图神经网络，”在*IEEE计算机视觉与模式识别会议论文集*，2020年。'
- en: '[85] F. Xiao and Y. Jae Lee, “Video object detection with an aligned spatial-temporal
    memory,” in *European Conference on Computer Vision*, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] F. Xiao 和 Y. Jae Lee，“具有对齐时空记忆的视频物体检测，”在*欧洲计算机视觉会议*，2018年。'
- en: '[86] J. Deng, Y. Pan, T. Yao, W. Zhou, H. Li, and T. Mei, “Single shot video
    object detector,” *IEEE Transactions on Multimedia*, 2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. Deng, Y. Pan, T. Yao, W. Zhou, H. Li, 和 T. Mei，“单次视频物体检测器，”*IEEE多媒体学报*，2020年。'
- en: '[87] H. Deng, Y. Hua, T. Song, Z. Zhang, Z. Xue, R. Ma, N. Robertson, and H. Guan,
    “Object guided external memory network for video object detection,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] H. Deng, Y. Hua, T. Song, Z. Zhang, Z. Xue, R. Ma, N. Robertson, 和 H.
    Guan，《面向视频物体检测的对象引导外部记忆网络》，见于 *IEEE/CVF 国际计算机视觉大会论文集*，2019年。'
- en: '[88] Y. Chen, Y. Cao, H. Hu, and L. Wang, “Memory enhanced global-local aggregation
    for video object detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2020.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Chen, Y. Cao, H. Hu, 和 L. Wang，《用于视频物体检测的记忆增强全球-局部聚合》，见于 *IEEE 计算机视觉与模式识别大会论文集*，2020年。'
