- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:54:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2105.12694] Deep Learning for Weakly-Supervised Object Detection and Object
    Localization: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.12694](https://ar5iv.labs.arxiv.org/html/2105.12694)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feifei Shao, Long Chen, Jian Shao  Wei Ji, Shaoning Xiao, Lu Ye, Yueting Zhuang 
    Jun Xiao Feifei Shao, Jian Shao, Shaoning Xiao, Yueting Zhuang, Jun Xiao are with
    Zhejiang University, Hangzhou, 310027, China. Emails: {sff, jshao, shaoningx,
    yzhuang, junx}@zju.edu.cnLong Chen is with Columbia University, New York, 10027,
    USA. This work was partially done when L. Chen was a Ph.D. student at Zhejiang
    University. Email: zjuchenlong@gmail.com (Corresponding author).Wei Ji is with
    National University of Singapore, 117417, Singapore. Email: weiji0523@gmail.comLu
    Ye is with Zhejiang University of Science and Technology, Hangzhou, 310023, China.
    Email: yelue@zust.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e., detecting
    multiple and single instances with bounding boxes in an image using image-level
    labels, are long-standing and challenging tasks in the CV community. With the
    success of deep neural networks in object detection, both WSOD and WSOL have received
    unprecedented attention. Hundreds of WSOD and WSOL methods and numerous techniques
    have been proposed in the deep learning era. To this end, in this paper, we consider
    WSOL is a sub-task of WSOD and provide a comprehensive survey of the recent achievements
    of WSOD. Specifically, we firstly describe the formulation and setting of the
    WSOD, including the background, challenges, basic framework. Meanwhile, we summarize
    and analyze all advanced techniques and training tricks for improving detection
    performance. Then, we introduce the widely-used datasets and evaluation metrics
    of WSOD. Lastly, we discuss the future directions of WSOD. We believe that these
    summaries can help pave a way for future research on WSOD and WSOL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weakly-Supervised Object Detection, Weakly-Supervised Object Localization, Basic
    Framework, Techniques, Future Directions.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Object detection [[1](#bib.bib1), [2](#bib.bib2)] is a fundamental and challenging
    task that aims to locate and classify object instances in an image. The object
    localization is to use a bounding box (an axis-aligned rectangle tightly bounding
    the object) to search for the spatial location and range of an object in an image
    as much as possible [[3](#bib.bib3), [4](#bib.bib4)]. Object classification is
    to assess the presence of objects from a given set of object classes in an image.
    As one of the most fundamental tasks in computer vision, object detection is an
    indispensable technique for many high-level applications, e.g., robot vision [[5](#bib.bib5)],
    face recognition [[6](#bib.bib6)], image retrieval [[7](#bib.bib7), [8](#bib.bib8)],
    augmented reality [[9](#bib.bib9)], autonomous driving [[10](#bib.bib10)], change
    detection [[11](#bib.bib11)] and so on. With the development of convolutional
    neural networks (CNNs) in visual recognition [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)] and release of large scale dateset [[4](#bib.bib4), [15](#bib.bib15)],
    today’s state-of-the-art object detector can achieve near-perfect performance
    under fully-supervised setting, i.e., Fully-Supervised Object Detection (FSOD) [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    Unfortunately, these fully-supervised object detection methods suffer from two
    inevitable limitations: 1) The large-scale instance annotations are difficult
    to obtain and labor-intensive. 2) When labeling these data, they may introduce
    annotation noises inadvertently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the mentioned problems, the community starts to solve object detection
    in a weakly-supervised setting, i.e., Weakly-Supervised Object Detection (WSOD).
    Different from the fully-supervised setting (cf. Fig. [1](#S1.F1 "Figure 1 ‣ I
    Introduction ‣ Deep Learning for Weakly-Supervised Object Detection and Object
    Localization: A Survey") (a)), WSOD aims to detect instances with only image-level
    labels (e.g., categories of instances in the whole images). Meanwhile, WSOD can
    benefit from the large-scale datasets on the web, such as Facebook and Twitter.
    Another similar task is Weakly-Supervised Object Localization (WSOL), which only
    detects one instance in an image. Because WSOD and WSOL detect multiple instances
    and single instances respectively, we consider WSOL as a sub-task of WSOD. In
    the following paper, we use WSOD to represent both WSOD and WSOL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d604d432d8a56c86c097e52f1d7a474.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) Fully-Supervised Object Detection (FSOD) uses the *instance-level*
    annotations as supervision. (b) Weakly-Supervised Object Detection (WSOD) uses
    the *image-level* annotations as supervision.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3adc94595f7ca7b2ad3c22eda7a5e709.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The main content of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we go over all typical WSOD methods and give a comprehensive
    survey (cf. Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey")) of recent advances in WSOD.
    Since the number of papers on WSOD is breathtaking, we sincerely apologize to
    those authors whose research on WSOD and other related fields are not included
    in this survey. In Section [II](#S2 "II WSOD ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we introduce the background,
    main challenges, and basic framework. In Section [III](#S3 "III Milestones of
    WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey"), according to the development timeline of WSOD, we introduce several
    modern classical methods in detail. Then, in-depth analyses are provided towards
    the all advanced techniques and tricks for the main challenges. In Section [VIII](#S8
    "VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we demonstrate all prevailing
    benchmarks and standard evaluation metrics for WSOD. In Section [IX](#S9 "IX Future
    Directions and Tasks ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey"), we briefly discuss the future directions.'
  prefs: []
  type: TYPE_NORMAL
- en: II WSOD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A A Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'WSOD aims to classify and locate object instances using only image-level labels
    in the training phase. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (b), given an image with cat and dog, WSOD not only classifies the
    cat and dog but also locates their location using bounding boxes. Different from
    FSOD that can use instance-level annotations in the training phase shown in Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey") (a), WSOD only accesses image-level labels.
    Because of this restriction, though hundreds of WSOD methods have been proposed,
    the performance gap between WSOD and FSOD is still large. For example, the mAP
    of state-of-the-art FSOD approach [[22](#bib.bib22)] and WSOD approach [[23](#bib.bib23)]
    is 86.9% and 54.9% on PASCAL VOC 2007 dataset [[24](#bib.bib24)], respectively.
    Therefore, there are still many challenges in terms of the task of WSOD for researchers
    to solve, especially in the direction of improving the detection performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A summary of the state-of-the-art WSOD methods. For the proposals,
    SS represents selective search, EB represents edge boxes, and SW represents sliding
    window. The Challenges denotes the main contributions of corresponding papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Year | Proposals | Network | Challenges | Code on Github |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MIL-based | CAM-based | Discriminative Region | Multiple Instances | Speed
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | EB | $\surd$ |  |  |  |  | hbilen/WSDDN
    |'
  prefs: []
  type: TYPE_TB
- en: '| CAM [[26](#bib.bib26)] | CVPR2016 | Heatmap |  | $\surd$ |  |  | $\surd$
    | zhoubolei/CAM |'
  prefs: []
  type: TYPE_TB
- en: '| WSLPDA [[27](#bib.bib27)] | CVPR2016 | EB | $\surd$ |  | $\surd$ |  |  |
    jbhuang0604/WSL |'
  prefs: []
  type: TYPE_TB
- en: '| WELDON [[28](#bib.bib28)] | CVPR2016 | SW | $\surd$ |  | $\surd$ |  | $\surd$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| ContextLocNet [[29](#bib.bib29)] | ECCV2016 | SS | $\surd$ |  | $\surd$ |  |  |
    vadimkantorov/contextlocnet |'
  prefs: []
  type: TYPE_TB
- en: '| Grad-CAM [[30](#bib.bib30)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | ramprs/grad-cam |'
  prefs: []
  type: TYPE_TB
- en: '| OICR [[31](#bib.bib31)] | CVPR2017 | SS | $\surd$ |  | $\surd$ |  |  | ppengtang/oicr
    |'
  prefs: []
  type: TYPE_TB
- en: '| WCCN [[32](#bib.bib32)] | CVPR2017 | EB | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ST-WSL [[33](#bib.bib33)] | CVPR2017 | EB | $\surd$ |  | $\surd$ | $\surd$
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WILDCAT [[34](#bib.bib34)] | CVPR2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | durandtibo/wildcat.pytorch |'
  prefs: []
  type: TYPE_TB
- en: '| SPN [[35](#bib.bib35)] | ICCV2017 | SW | $\surd$ |  |  |  | $\surd$ | ZhouYanzhao/SPN
    |'
  prefs: []
  type: TYPE_TB
- en: '| TP-WSL [[36](#bib.bib36)] | ICCV2017 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)] | TPAMI2018 | SS | $\surd$ |  | $\surd$ | $\surd$
    |  | ppengtang/pcl.pytorch |'
  prefs: []
  type: TYPE_TB
- en: '| GAL-fWSD [[38](#bib.bib38)] | CVPR2018 | EB | $\surd$ |  |  |  | $\surd$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| W2F [[39](#bib.bib39)] | CVPR2018 | SS | $\surd$ |  | $\surd$ | $\surd$ |
    $\surd$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| ACoL [[40](#bib.bib40)] | CVPR2018 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | xiaomengyc/ACoL |'
  prefs: []
  type: TYPE_TB
- en: '| ZLDN [[41](#bib.bib41)] | CVPR2018 | EB | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TS²C [[42](#bib.bib42)] | ECCV2018 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SPG [[43](#bib.bib43)] | ECCV2018 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xiaomengyc/SPG |'
  prefs: []
  type: TYPE_TB
- en: '| WSRPN [[44](#bib.bib44)] | ECCV2018 | EB | $\surd$ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIL [[45](#bib.bib45)] | CVPR2019 | SS | $\surd$ |  |  |  |  | WanFang13/C-MIL
    |'
  prefs: []
  type: TYPE_TB
- en: '| WS-JDS [[46](#bib.bib46)] | CVPR2019 | EB | $\surd$ |  | $\surd$ |  |  |
    shenyunhang/WS-JDS |'
  prefs: []
  type: TYPE_TB
- en: '| ADL [[47](#bib.bib47)] | CVPR2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | junsukchoe/ADL |'
  prefs: []
  type: TYPE_TB
- en: '| Pred NET [[48](#bib.bib48)] | CVPR2019 | SS | $\surd$ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  | researchmm/WSOD2
    |'
  prefs: []
  type: TYPE_TB
- en: '| OAILWSD [[50](#bib.bib50)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TPWSD [[51](#bib.bib51)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SDCN [[52](#bib.bib52)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DANet [[54](#bib.bib54)] | ICCV2019 | Heatmap |  | $\surd$ |  |  | $\surd$
    | xuehaolan/DANet |'
  prefs: []
  type: TYPE_TB
- en: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | Heatmap |  | $\surd$ | $\surd$ |  |
    $\surd$ | Yangseung/NL-CCAM |'
  prefs: []
  type: TYPE_TB
- en: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| EIL [[56](#bib.bib56)] | CVPR2020 | Heatmap |  | $\surd$ | $\surd$ |  | $\surd$
    | Wayne-Mai/EIL |'
  prefs: []
  type: TYPE_TB
- en: '| SLV [[57](#bib.bib57)] | CVPR2020 | SS | $\surd$ |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: II-B Main Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main challenges of WSOD come from two aspects: localization accuracy and
    speed. For localization accuracy, it consists of discriminative region problem
    and multiple instances with the same category problem. For speed, it is an important
    characteristic of real applications. In TABLE [I](#S2.T1 "TABLE I ‣ II-A A Problem
    Definition ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and
    Object Localization: A Survey"), we summarize all typical WSOD methods and their
    contributions to these challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discriminative Region Problem. It is that detectors [[25](#bib.bib25), [26](#bib.bib26)]
    tend to focus on the most discriminative parts of the object. During training,
    there may exist more than one proposal around an object, and the most discriminative
    part region of the object is likely to have the highest score (e.g., the region
    A is the most discriminative region in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges
    ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (left) and it has a higher score than that of other regions). If the
    model selects positive proposals only based on scores, it is easy to focus on
    the most discriminative part of the object rather than the whole object extent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d3d5ad0bb477103618a916c4410bf95b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Detection results between model without classifier refinement (left)
    and model with classifier refinement (right). The figure comes from [[31](#bib.bib31)].'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Instance Problem. It denotes that detectors [[25](#bib.bib25), [31](#bib.bib31)]
    are difficult to accurately recognize multiple instances when there may exist
    several objects with the same category in an image. Although there are multiple
    instances with the same category in an image, these detectors [[25](#bib.bib25),
    [31](#bib.bib31)] only select the highest score proposal of each category as the
    positive proposal and ignores other possible instance proposals.
  prefs: []
  type: TYPE_NORMAL
- en: Speed Problem. At present, the speed bottleneck of the WSOD approaches is mainly
    concentrated in proposal generation. Selective search (SS) [[58](#bib.bib58)]
    and Edge boxes (EB) [[59](#bib.bib59)] that are widely used in WSOD are too time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Basic WSOD Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic framework of WSOD methods can be categorized into MIL-based networks
    and CAM-based networks according to the detection of multiple instances and a
    single instance.
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 MIL-based Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When the detection network predicts multiple instances in an image, it is considered
    a Multiple Instance Learning (MIL) problem [[60](#bib.bib60)]. Taking Fig. [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey") (b) for example, an image is interpreted as
    a bag of proposals in the MIL problem. If the image is labeled cat, it means that
    at least one of the proposals tightly contains the cat instance. Otherwise, all
    of the regions do not contain the cat instance (likewise for dogs). The MIL-based
    network is based on the structure of WSDDN [[25](#bib.bib25)] that consists of
    three components: proposal generator, backbone, and detection head.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proposal Generator. Numerous proposal generators are usually used in MIL-based
    networks. 1) Selective search (SS) [[58](#bib.bib58)]: it leverages the advantages
    of both exhaustive search and segmentation to generate initial proposals. 2) Edge
    boxes (EB) [[59](#bib.bib59)]: it uses object edges to generate proposals and
    is widely used in many approaches [[25](#bib.bib25), [27](#bib.bib27), [32](#bib.bib32),
    [33](#bib.bib33), [41](#bib.bib41), [44](#bib.bib44), [46](#bib.bib46)]. 3) Sliding
    window (SW): it denotes that each point of the feature maps corresponds to one
    or more proposals in the relative position of the original image. And SW is faster
    than SS [[58](#bib.bib58)] and EB [[59](#bib.bib59)] in proposal generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Backbone. With the development of CNNs and large scale datasets (e.g., ImageNet [[4](#bib.bib4)]),
    the pretrained AlexNet [[61](#bib.bib61)], VGG16 [[12](#bib.bib12)], GoogLeNet [[13](#bib.bib13)],
    ResNet [[14](#bib.bib14)], and SENet [[62](#bib.bib62)] are prevailing feature
    representation networks for both classification and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Detection Head. It includes a classification stream and a localization stream.
    The classification stream predicts class scores for each proposal, and the localization
    stream predicts every proposal’s existing probability score for each class. Then
    the two scores are aggregated to predict the confidence scores of an image as
    a whole, which are used to inject image-level supervision in learning.
  prefs: []
  type: TYPE_NORMAL
- en: Given an image, we first feed it into the proposal generator and backbone to
    generate proposals and feature maps, respectively. Then, the feature maps and
    proposals are forwarded into a spatial pyramid pooling (SPP) [[63](#bib.bib63)]
    layer to generate fixed-size regions. Finally, these regions are fed into the
    detection head to classify and localize object instances.
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 CAM-based Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When the detection network only predicts a single instance in an image, it
    is considered an object localization problem. The CAM-based network is based on
    the structure of CAM [[26](#bib.bib26)], which consists of three components: backbone,
    classifier, and class activation maps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backbone. It is similar to that of the MIL-based network introduced in Section [II-C1](#S2.SS3.SSS1
    "II-C1 MIL-based Network ‣ II-C Basic WSOD Framework ‣ II WSOD ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Classifier. It is designed to classify the classes of an image, which includes
    a global average pooling (GAP) layer and a fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Maps. It is responsible for locating object instances by using
    a simple segmentation technique. Because the class activation maps are produced
    by matrix multiplying the weight of the fully connected layer to the feature maps
    of the last convolutional layer, it spotlights the class-specific discriminative
    regions in every activation map. Therefore, it is easy to generate bounding boxes
    of every class by segmenting the activation map of the class.
  prefs: []
  type: TYPE_NORMAL
- en: Given an image, we first feed it into the backbone to generate feature maps
    of this image. Then, the feature maps are forwarded into the classifier to classify
    the image’s classes. Meanwhile, we matrix multiply the weight of the fully connected
    layer to the feature maps of the last convolutional layer to produce class activation
    maps. Finally, we segment the activation map of the highest probability class
    to yield bounding boxes for object localization.
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 Discussions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we discuss the differences between MIL-based networks and CAM-based
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, MIL-based network leverages SS [[58](#bib.bib58)], EB [[59](#bib.bib59)]
    or SW to generate thousands of initial proposals, but CAM-based network segments
    the activation map to one proposal for each class. Therefore, MIL-based network
    is better than CAM-based network when detecting multiple instances with the same
    category in an image, but the training and inference speed of CAM-based network
    is faster than MIL-based.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, because the size of the proposals produced by SS or EB is not consistent,
    MIL-based network leverages an SPP layer to generate fixed-size vectors followed
    by feeding these fixed-size vectors into the fully connected layers for later
    training. However, a CAM-based network leverages a GAP layer to generate a fixed-size
    vector on the feature maps. Then, it feeds the vector into a fully connected layer
    for classifying.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Both MIL-based networks and CAM-based networks will face the discriminative
    region problem and multiple instance problem. In addition, MIL-based networks
    will face the training and test speed problem, since SS and EB are too time-consuming
    and yield plenty of initial proposals.
  prefs: []
  type: TYPE_NORMAL
- en: III Milestones of WSOD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since 2016, there are some landmark methods (cf. Fig. [4](#S3.F4 "Figure 4
    ‣ III Milestones of WSOD ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey")) for the research of WSOD. In the following,
    we will briefly introduce these milestones.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0a74cebeef01c1fef7336c3ad9d78b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The Milestones of WSOD since 2016.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A MIL-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: WSDDN. The biggest contribution of WSDDN [[25](#bib.bib25)] is using two streams
    network, which aims to perform classification and localization respectively. WSDDN
    first uses a SPP [[63](#bib.bib63)] on the top of the feature maps and generates
    a feature vector after two fully connected layer procedures. Next, the feature
    vector is fed into the classification stream and localization steam. Specifically,
    the classification stream is responsible for computing the class scores of each
    region, and the localization stream is designed to compute every region’s existing
    probability for each class. Then, the matrix product of the class scores of each
    region and the existing probability for each class is considered as the final
    prediction scores. However, because of only accessing image-level labels in the
    training phase, the most discriminative part of the object will be paid more attention
    than the whole object instance in training. Due to the above limitation, WSDDN
    suffers from the discriminative region problem.
  prefs: []
  type: TYPE_NORMAL
- en: OICR. To alleviate the discriminative region problem, OICR [[31](#bib.bib31)]
    uses WSDDN as its baseline and adds three instance classifier refinement procedures
    after the baseline. Every instance classifier refinement procedure, which consists
    of two fully connected layers, is designed to further predict the class scores
    for each proposal. Because the output of each instance classifier refinement procedure
    is the supervision of its latter refinement procedure, OICR can continue to learn
    so that larger area can have higher scores than WSDDN. Although the prediction
    of WSDDN may only focus on the discriminative part of the object, it will be refined
    after several instance classifier refinement procedures.
  prefs: []
  type: TYPE_NORMAL
- en: SDCN. SDCN [[52](#bib.bib52)] introduces a segmentation-detection collaborative
    mechanism. It consists of a detection branch and segmentation branch, which are
    responsible for detecting bounding boxes and generating segmentation masks respectively.
    In SDCN, the detection results will be converted to a heatmap by setting a classification
    score to all pixels within each proposal as the supervision mask of the segmentation
    branch. Meanwhile, the proposals of the highest overlap with the connected regions
    from the segmentation masks will be the pseudo ground-truth boxes of the detection
    branch. Both detection branch and segmentation branch are optimized alternatively
    and promoted each other, so SDCN achieves better detection performance than OICR.
  prefs: []
  type: TYPE_NORMAL
- en: ICMWSD. Different from SDCN which uses object detection with segmentation collaboration
    mechanism, ICMWSD [[23](#bib.bib23)] addresses the problem of focusing on the
    most discriminative part of an object by leveraging context information. Firstly,
    ICMWSD obtains a dropped features by dropping the most discriminative parts. Then,
    maximizing the loss of the dropped features to force ICMWSD to look at the surrounding
    context regions.
  prefs: []
  type: TYPE_NORMAL
- en: III-B CAM-based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CAM. The biggest contribution of CAM [[26](#bib.bib26)] is using class activation
    maps to predict instances. CAM firstly leverages a GAP layer on the last convolutional
    feature maps to generate a feature vector. Then, the feature vector is fed into
    a classifier with a fully connected layer to generate prediction scores of an
    image. Finally, CAM generates bounding boxes of each class by using a simple thresholding
    technique to segment the activation map of every class. However, class activation
    maps of CAM spotlight the regions that are the most discriminative parts of the
    object, so CAM also faces the discriminative region problem as WSDDN.
  prefs: []
  type: TYPE_NORMAL
- en: WCCN. To alleviate the discriminative region problem, WCCN [[32](#bib.bib32)]
    proposes to use a cascaded network that has three cascade stages trained in an
    end-to-end pipeline. The first stage is the CAM [[26](#bib.bib26)] network that
    aims to generate class activation maps and initial proposals. The second stage
    is a segmentation network that uses the class activation maps to train object
    segmentation for refining object localization. The final stage is a MIL network
    that performs multiple instances of learning on proposals extracted in the second
    stage. Because the second and third stages refine object localization, WCCN alleviates
    the problem that tends to focus on the most discriminative part of the object.
  prefs: []
  type: TYPE_NORMAL
- en: ACoL. To alleviate the discriminative region problem, ACoL [[40](#bib.bib40)]
    introduces two parallel-classifiers for object localization using adversarial
    complementary learning. Specifically, it first leverages the first classifier
    to localize the most discriminative regions. Then, ACoL uses the masked feature
    maps by masking the most discriminative regions discovered in the first classifier
    as the input feature maps of the second classifier. This forces the second classifier
    to select the next discriminative regions. Finally, ACoL fuses the class activation
    maps of both classifiers to generate bounding boxes of every class by segmenting
    the activation map of the highest probability class.
  prefs: []
  type: TYPE_NORMAL
- en: IV Specific Techniques for Discriminative Region Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will introduce several advanced techniques for solving the
    discriminative region problem.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Context Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The context of one region is external information of this region, which can
    be obtained by masking the region of the feature maps with special numbers (e.g.,
    zero). There are two types of the strategy of using context modeling as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy A. It selects the regions that have a big gap between their scores
    and their contextual region’s scores as positive proposals. For example, WSLPDA [[27](#bib.bib27)]
    first replaces the pixel values within one proposal with zero to obtain the contextual
    region. Then, WSLPDA compares the scores of proposals and their contextual region.
    If the gap between the two scores is large, it indicates that the proposal is
    likely positive. ContextLocNet [[29](#bib.bib29)] subtracts the localization score
    of one proposal from the localization score of the external rectangle region of
    the proposal. Then, the subtraction is considered as the final localization score
    of the proposal. Similar to WSLPDA and ContextLocNet, TS²C [[42](#bib.bib42)]
    selects a positive proposal by comparing the mean objectness scores of the pixels
    in one proposal and its surrounding region. But to alleviate the impact of background
    pixels in the surrounding region, TS²C computes the mean objectness scores only
    using pixels with large confidence values in the surrounding region.
  prefs: []
  type: TYPE_NORMAL
- en: Strategy B. It selects positive proposals by leveraging the loss of context
    regions. For example, OAILWSD [[50](#bib.bib50)] believes that a proposal not
    tightly covers the object instance if the loss of the context feature maps of
    this proposal tends to decrease. Thus, OAILWSD first leverages the context classification
    loss to label regions. Then, it selects the top-scoring regions whose context
    class probabilities are low as positive proposals. ICMWSD [[23](#bib.bib23)] first
    drops the most discriminative parts of the feature maps to obtain contextual feature
    maps. Then, it maximizes the loss of the contextual feature maps to force it focuses
    on the context regions.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Self-training Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the self-training algorithm, the early prediction instances are then used
    in the detector for latter training as the pseudo ground-truth instances. There
    are two types of self-training algorithms: inter-stream and inter-epoch. In inter-stream
    self-training, the instances of each stream supervise its later stream. In inter-epoch
    self-training, the instances of each epoch supervise its later epoch. The key
    idea of self-training is that even if the early top-scoring proposals may only
    focus on the discriminative part of the object, they will be refined after several
    refinement procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 Inter-stream Self-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'OICR [[31](#bib.bib31)] expects B, C, and D can inherit the class score of
    A to correctly localize objects in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges
    ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey") (right). So, OICR adds three refinement classifiers with two fully
    connected layers in WSDDN to address the issue shown in Fig. [3](#S2.F3 "Figure
    3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object
    Detection and Object Localization: A Survey") (left). Specifically, the supervision
    of the first refinement classifier is the output of WSDDN. As for other refinement
    classifiers, the supervision of the current refinement classifier is the output
    of its previous refinement classifier. Inspired by OICR, WSOD2 [[49](#bib.bib49)]
    consists of numerous classifiers. ICMWSD [[23](#bib.bib23)] also inserts refinement
    streams in WSDDN, however, every refinement stream includes a classifier and a
    regressor. Besides, some approaches [[39](#bib.bib39), [53](#bib.bib53), [50](#bib.bib50),
    [51](#bib.bib51)] use OICR as their baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Inter-epoch Self-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Self-Taught-WS [[33](#bib.bib33)] uses relative improvement (RI) of the scores
    of each proposal of two adjacent epochs as a criterion for selecting the positive
    sample. Specifically, it chooses the proposals of the previous epoch whose intersection
    over union (IoU) $\geq 0.5$ with the maximal RI proposal as the positive samples
    of the current epoch.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Cascaded Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cascaded network includes several stages and the supervision of the current
    stage is the output of the previous stage. Such as WCCN [[32](#bib.bib32)] and
    TS²C [[42](#bib.bib42)] consist of three stages. The first stage is the CAM module
    that is to generate initial proposals using class activation maps. The intermediate
    stage is the object segmentation module that is designed to refine initial proposals.
    The final stage is a multiple instance learning module that is responsible for
    detecting accurate objects.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Bounding Box Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bounding box regression can improve object localization performance using instance-level
    annotations in the training phase, but the WSOD task only accesses image-level
    labels. To use bounding box regression for refining the initial proposals from
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)], some approaches propose to yield
    high-quality pseudo ground-truth boxes as the supervision of bounding box regression.
  prefs: []
  type: TYPE_NORMAL
- en: Now, numerous approaches [[48](#bib.bib48), [49](#bib.bib49), [51](#bib.bib51),
    [23](#bib.bib23), [57](#bib.bib57)] include at least one of the bounding box regressors.
    The supervision of the regressor is the output of previous classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Discriminative Region Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From Fig. [3](#S2.F3 "Figure 3 ‣ II-B Main Challenges ‣ II WSOD ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey") (left),
    some researchers find that the highest score region only covers the most discriminative
    part of the object. To localize the whole object extent, masking the most discriminative
    part of the object is designed to force the detector to find the next discriminative
    region.'
  prefs: []
  type: TYPE_NORMAL
- en: TP-WSL [[36](#bib.bib36)] is a two-phase learning network that detects the next
    discriminative regions by masking the most discriminative region. In the first
    phase, it yields class activation maps followed by masking the most discriminative
    region using a threshold among the activation map of the highest probability class.
    In the second phase, it multiplies the masked activation map by the feature maps
    of the second network to refine the feature maps for detecting the next discriminative
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Different from TP-WSL that has two backbones, ACoL [[40](#bib.bib40)] consists
    of one shared backbone and two parallel-classifiers. The masked feature maps from
    the first classifier are fed into the second classifier to generate class activation
    maps. Finally, ACoL locates object instances in the fused activation maps by fusing
    the two-class activation maps of both classifiers. EIL [[56](#bib.bib56)] proposes
    to share the weights of the two parallel-classifiers of ACoL, and it only segments
    the activation map of the highest probability class from the unmasked branch to
    yields object proposals. Comparing C-MIDN [[53](#bib.bib53)] with ACoL, there
    are three differences. First, the detection network of C-MIDN is WSDDN [[25](#bib.bib25)],
    but the detection network of ACoL is CAM [[26](#bib.bib26)]. Second, C-MIDN does
    not compute the loss of high overlap with the first detection module’s top-scoring
    proposal in the second branch, but ACoL masks the first detection module’s top-scoring
    proposal’s region with zero in the second branch. Finally, C-MIDN chooses the
    top-scoring proposals of the second detection module and the top-scoring proposals
    of the first detection module with low overlap with selected proposals as positive
    proposals, but ACoL yields positive proposals by segmenting the fused class activation
    maps.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Incorporating Low-level Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Low-level features usually retain richer object details, such as edges, corners,
    colors, pixels, and so on. We can obtain accurate object localization if making
    full use of these low-level features. For example, Grad-CAM [[30](#bib.bib30)]
    leverages high-resolution Guided Backpropagation [[64](#bib.bib64)] that highlights
    the image’s details to create both high-resolution and class-discriminative visualizations.
    WSOD2 [[49](#bib.bib49)] first computes the score of a region proposal. Then,
    it selects the same region in low-level image features (e.g., superpixels) and
    computes the score of this region. Finally, the product of the two scores is the
    final score of the region proposal.
  prefs: []
  type: TYPE_NORMAL
- en: IV-G Segmentation-detection Collaborative Mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Segmentation-detection collaborative mechanism includes a segmentation branch
    and a detection branch. The primary reasons for the collaborative mechanism are
    the following: 1) MIL (detection) can correctly distinguish an area as an object,
    but it is not good at detecting whether the area contains the entire object. 2)
    Segmentation can cover the entire object instance, but it cannot distinguish whether
    the area is a real object or not [[52](#bib.bib52)]. So, some models leverage
    deep cooperation between detection and segmentation by supervising each other
    to achieve accurate localization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'WS-JDS [[46](#bib.bib46)] first chooses the region proposals with top-scoring
    pixels generated by the semantic segmentation branch as the positive samples of
    the detection branch. Then, it sets the classification score to all pixels within
    each positive proposal of the detection branch as the supervision mask of the
    segmentation branch. Similar to WS-JDS, SDCN [[52](#bib.bib52)] also combines
    the detection branch with the segmentation branch which is introduced in Section [III-A](#S3.SS1
    "III-A MIL-based Methods ‣ III Milestones of WSOD ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: IV-H Transforming WSOD to FSOD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transforming WSOD to FSOD is another popular technique to achieve object detection
    using image-level labels, which is designed to train an FSOD model using the output
    of the WSOD model. The primary problem of transformation is to yield good pseudo
    ground-truth boxes from WSOD. There are several strategies to mine boxes as the
    pseudo ground-truth boxes. 1) top score: numerous approaches [[31](#bib.bib31),
    [37](#bib.bib37), [42](#bib.bib42), [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53)]
    select top score detection boxes of WSOD as the pseudo ground-truth boxes. 2)
    relative improvement (RI): ST-WSL [[33](#bib.bib33)] selects the boxes with the
    maximal relative score improvement of two adjacent epochs as the pseudo ground-truth
    boxes. 3) mergence: W2F [[39](#bib.bib39)] merges several small boxes into a big
    candidate box and uses these merged boxes as the pseudo ground-truth boxes for
    later training. SLV [[57](#bib.bib57)] first merges the scores of several boxes
    to the pixels and then generates bounding boxes of each class by using a simple
    thresholding technique to segment the map of every class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, there are several FSOD models that have been used as follows:
    Fast R-CNN [[65](#bib.bib65)], Faster R-CNN [[16](#bib.bib16)], and SSD [[17](#bib.bib17)].
    Numerous approaches [[31](#bib.bib31), [33](#bib.bib33), [37](#bib.bib37), [42](#bib.bib42),
    [46](#bib.bib46), [52](#bib.bib52), [53](#bib.bib53), [57](#bib.bib57)] use prediction
    boxes of WSOD as the pseudo ground-truth boxes to train Fast R-CNN. W2F [[39](#bib.bib39)]
    uses prediction boxes of WSOD to train Faster R-CNN. GAL-fWSD [[38](#bib.bib38)]
    uses prediction boxes of WSOD to train SSD.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-I Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we individually introduce several techniques that
    are commonly used to improve the detection performance by detailed listing numerous
    approaches. In this section, we will compare and discuss these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, context modeling and discriminative region removal are two similar
    techniques. Context modeling is to calculate the scores of the proposal and its
    context region respectively. Then it chooses the positive proposal derives from
    the two scores. On the other hand, the discriminative region removal is to directly
    erases top-scoring regions by setting zero value in the feature maps of the first
    branch followed by feeding the erased feature maps into the second branch.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the self-training algorithm usually co-occurs with bounding box regression.
    Bounding box regression is responsible for refining the initial proposals from
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)]. And self-training algorithm is
    designed to refine the prediction result of the baseline. The core problem of
    both the self-training algorithm and bounding box regression is yielding good
    pseudo ground-truth boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Thirdly, the cascaded network and segmentation-detection collaborative mechanism
    are two similar techniques. They leverage the segmentation module to improve the
    performance of the object detection module. A cascaded network is a sequential
    structure that the previous module is responsible for training the latter module.
    However, the segmentation-detection collaborative mechanism is a circular structure
    that leverages deep cooperation between detection and segmentation supervising
    each other to achieve accurate localization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, incorporating low-level features technique leverages the advantage
    of the high-resolution characteristics of low-level features to improve object
    localization. The key idea of transforming WSOD to FSOD technique is to make full
    use of the advantages of the network structure of the FSOD model (e.g., Fast R-CNN [[65](#bib.bib65)]).
  prefs: []
  type: TYPE_NORMAL
- en: V Specific Techniques for Multiple Instance problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will introduce how to make full use of the spatial relationship
    of proposals to solve multiple instance problem introduced in Section [II-B](#S2.SS2
    "II-B Main Challenges ‣ II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection
    and Object Localization: A Survey"). Specifically, the two proposals that are
    far away from each other are likely to correspond to two object instances, while
    the two proposals with large overlap may correspond to the same object instance.'
  prefs: []
  type: TYPE_NORMAL
- en: ST-WSL [[33](#bib.bib33)] leverages a graph to detect multiple instances with
    the same category in an image. It first chooses N top-scoring proposals of each
    positive class as the nodes of the graph. The edge between two nodes indicates
    a large overlap between them. Then it selects the greatest degree (number of connections
    to other nodes) nodes as positive proposals using Non-Maximum Suppression (NMS)
    algorithm  [[66](#bib.bib66)]. PCL [[37](#bib.bib37)] introduces the proposal
    cluster to replace the proposal bag that includes all of the proposals of each
    category. PCL assigns the same label and spatially adjacent proposals to the same
    proposal cluster. If proposals do not overlap each other, they will be assigned
    in different proposal clusters. Then, PCL selects the highest score proposal from
    each proposal cluster as the positive proposal. W2F [[39](#bib.bib39)] iteratively
    merges the highly overlapping proposals with top-scoring into big proposals. Finally,
    these big proposals are considered positive proposals.
  prefs: []
  type: TYPE_NORMAL
- en: VI Specific Techniques for Speed Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will introduce several advanced techniques for solving
    the speed problem introduced in Section [II-B](#S2.SS2 "II-B Main Challenges ‣
    II WSOD ‣ Deep Learning for Weakly-Supervised Object Detection and Object Localization:
    A Survey"). The main reason for the slow speed is that the MIL-based method adopts
    SS [[58](#bib.bib58)] or EB [[59](#bib.bib59)] to generate a large number of initial
    proposals that most of which are negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods for improving speed can be broadly categorized into three groups:
    1) Transformation-based [[39](#bib.bib39), [38](#bib.bib38)]: these approaches
    use their prediction boxes as the pseudo ground-truth boxes to train Faster R-CNN [[16](#bib.bib16)]
    or SSD [[17](#bib.bib17)] and then use Faster R-CNN or SSD to infer images. 2)
    Sliding-window-based [[28](#bib.bib28), [35](#bib.bib35)]: these approaches use
    the sliding window technique to quickly generate proposals by walking through
    every point on the feature map. 3) Heatmap-based [[26](#bib.bib26), [30](#bib.bib30),
    [34](#bib.bib34), [36](#bib.bib36), [40](#bib.bib40), [43](#bib.bib43), [47](#bib.bib47),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]: these approaches segment
    the heatmap using a threshold to generate proposals to improve the speed of proposal
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: VII Training Tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides the techniques in the previous chapter, training tricks without changing
    network structure also can improve detection results. In this section, we will
    introduce several training tricks for improving object localization.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Easy-to-hard Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous approaches [[25](#bib.bib25), [26](#bib.bib26), [28](#bib.bib28), [29](#bib.bib29),
    [31](#bib.bib31), [32](#bib.bib32), [36](#bib.bib36)] use all of the images at
    once without a training sequence to train the detection model. The easy-to-hard
    strategy denotes that the model is trained by using the images with progressively
    increasing difficulty. In this way, the model can gain better detection results.
    For example, ZLDN [[41](#bib.bib41)] first computes the difficulty scores of images.
    Then, all of the images are ranked in an ascending order based on the difficulty
    scores. Finally, ZLDN uses the images with increasing difficulty to progressively
    train themselves.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Negative Evidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Negative evidence contains the low-scoring regions, activations, and activation
    maps. For example, WELDON [[28](#bib.bib28)] uses the classification scores of
    the $k$ top-scoring proposals and the $m$ low-scoring proposals to generate the
    classification scores of the image by simply summing. WILDCAT [[34](#bib.bib34)]
    leverages the $k^{+}$ highest probability activations and $k^{-}$ lowest probability
    activations of the activation map to generate the prediction score. NL-CCAM [[55](#bib.bib55)]
    uses the lowest probability activation maps. Specifically, it first ranks all
    of the activation maps in a descending order based on the probability of every
    class. Then, it fuses these class activation maps using a specific combinational
    function into one map, which is segmented to predict object instances.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Optimizing Smoothed Loss Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the loss function of the model is non-convex, it tends to fall into sub-optimal
    and falsely localizes object parts while missing a full object extent during training [[45](#bib.bib45)].
    So C-MIL [[45](#bib.bib45)] replaces the non-convex loss function with a series
    of smoothed loss functions to alleviate the problem that the model tends to get
    stuck into local minima. At the beginning of training, C-MIL first performs the
    image classification task. During the training process, the loss function of C-MIL
    is slowly transformed from the convex image classification loss to the non-convex
    object detection loss function.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Discussions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we individually introduce several training tricks
    that are independent of the network structure. In this section, we will compare
    and discuss these tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, an easy-to-hard strategy is applied to the data phase, which is responsible
    for adjusting the order of the training images. Secondly, negative evidence acts
    on the training phase, which is designed to refine positive proposals or feature
    maps. Finally, optimizing smoothed loss functions act on the optimizing phase,
    which is responsible for avoiding suboptimal.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Datasets and Performance Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VIII-A Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets play an important role in WSOD task. Most approaches of the WSOD use
    PASCAL VOC [[3](#bib.bib3)], MSCOCO [[15](#bib.bib15)], ILSVRC [[4](#bib.bib4)],
    or CUB-200 [[67](#bib.bib67)] as training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: PASCAL VOC. It includes 20 categories and tens of thousands of images with instance
    annotations. PASCAL VOC has several versions, such as PASCAL VOC 2007, 2010, and
    2012\. Specifically, PASCAL VOC 2007 consists of 2,501 training images, 2,510
    validation images, and 4,092 test images. PASCAL VOC 2010 consists of 4,998 training
    images, 5,105 validation images, and 9,637 test images. PASCAL VOC 2012 consists
    of 5,717 training images, 5,823 validation images, and 10,991 test images.
  prefs: []
  type: TYPE_NORMAL
- en: MSCOCO. It is large-scale object detection, segmentation, and captioning dataset.
    MSCOCO has 80 object categories, 330K images ($>$200K labeled), 1.5 million object
    instances. In object detection, MSCOCO is as popular as PASCAL VOC datasets. Because
    MSCOCO has more images and categories than PASCAL VOC datasets, the difficulty
    of training on the MSCOCO dataset is higher than that of PASCAL VOC datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ILSVRC. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a
    large-scale dataset. In ILSVRC, the model usually uses 200 fully labeled categories
    and 1,000 categories in object detection and object localization, respectively.
    ILSVRC has several versions, such as ILSVRC 2013, ILSVRC 2014, and ILSVRC 2016\.
    Specifically, ILSVRC 2013 which is usually used in object detection has 12,125
    images for training, 20,121 images for validation, and 40,152 images for testing.
    In addition, ILSVRC 2014 and 2016 inherit the ILSVRC 2012 dataset in object localization,
    which contains 1.2 million images of 1,000 categories in the training set. And
    ILSVRC 2012 dataset has 50,000 and 100,000 images with labels in the validation
    and test set, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: CUB-200. Caltech-UCSD Birds 200 (CUB-200) contains 200 bird species which is
    a challenging image dataset. It focuses on the study of subordinate categorization.
    CUB-200-2011 [[68](#bib.bib68)] is an extended version of CUB-200, which adds
    many images for each category and labels new part localization annotations. CUB-200-2011
    contains 5,994 images in the training set and 5,794 images in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-B Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the state-of-the-art WSOD approaches, there are three standard evaluation
    metrics: mAP, CorLoc, and top error.'
  prefs: []
  type: TYPE_NORMAL
- en: mAP (mean Average Precision). Average Precision (AP) is usually used in image
    classification and object detection. It consists of precision and recall. If $tp$
    denotes the number of the correct prediction samples among all of the positive
    samples, $fp$ denotes the number of the wrong prediction samples among all of
    the positive samples, and $fn$ denotes the number of the wrong prediction samples
    among all of the negative samples, precision and recall can be computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | recall | $\displaystyle=tp/(tp+fn),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | precision | $\displaystyle=tp/(tp+fp),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the correct prediction sample denotes IoU of the positive sample and its
    corresponding ground-truth box $\geq$ 0.5\. Meanwhile, the IoU is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm IoU}(b,b^{g})=area(b\cap b^{g})/area(b\cup b^{g}),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $b$ denotes a prediction sample, $b^{g}$ denotes a corresponding ground-truth
    box, and $area$ denotes the region size of the intersection or union. The mAP
    is the mean of all of the class average precisions and is a final evaluation metric
    of performance on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: CorLoc (Correct Localization). CorLoc denotes the percentage of images that
    exist at least one instance of the prediction boxes whose IoU $\geq$ 50% with
    ground-truth boxes for every class in these images. CorLoc is a final evaluation
    metric of localization accuracy on the trainval dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Top Error. Top error consists of Top-1 classification error (1-err cls), Top-5
    classification error (5-err cls), Top-1 localization error (1-err loc), and Top-5
    localization error (5-err loc). Specifically, Top-1 classification error is equal
    to $1.0-cls_{1}$, where $cls_{1}$ denotes the accuracy of the highest prediction
    score (likewise for Top-1 localization error). Top-5 classification error is equal
    to $1.0-cls_{5}$, where $cls_{5}$ denotes that it counts as correct if one of
    the five predictions with the highest score is correct (likewise for Top-5 localization
    error). Numerous approaches [[26](#bib.bib26), [40](#bib.bib40), [43](#bib.bib43),
    [54](#bib.bib54), [55](#bib.bib55)] use top error to evaluate the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-C Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Results on Pascal VOC. The results of state-of-the-art WSOD methods on datasets
    Pascal VOC 2007, 2010, and 2012 are shown in Table [II](#S8.T2 "TABLE II ‣ VIII-C
    Experimental Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning
    for Weakly-Supervised Object Detection and Object Localization: A Survey"). The
    WSOD methods with “+FR” denote that their initial predictions are fed into the
    Fast R-CNN [[65](#bib.bib65)] and serve as pseudo ground-truth bounding box annotations,
    i.e., these methods transform the WSOD into FSOD problems. From the results, we
    can observe the performance on all three Pascal VOC datasets have achieved unprecedented
    progress in recent few years (e.g., mAP 52.1% in CVPR’20 vs. 29.1% in CVPR’16
    on Pascal VOC 2012). Meanwhile, comparing the methods and their counterparts with
    Fast R-CNN (e.g., OICR vs. OICR+FR), we can find the detection performance can
    be further improved by using this FSOD transforming strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results on MSCOCO. The results of state-of-the-art WSOD methods on dataset
    MSCOCO are shown in Table [III](#S8.T3 "TABLE III ‣ VIII-C Experimental Results
    ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"). We only report the AP metric,
    and the AP[50] denotes that the IoU threshold is equal to $0.5$. Similarly, the
    performance on MSCOCO also doubled in the last few years (e.g., AP[50] 11.5% vs.
    24.8% in test set). Since MSCOCO contains more object categories than PASCAL VOC
    datasets, the results on MSCOCO are still far from satisfactory. However, the
    performance gains by transforming WSOD to FSOD is relative marginal (e.g., 0.7%
    gains in AP for PCL model).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results on ILSVRC 2020 and CUB-200. TABLE [IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental
    Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey") summaries the object localization
    performance of state-of-the-art WSOD methods on these two datasets. Compared to
    the PASCAL VOC and MSCOCO, quite few WSOD methods have evaluated their performance
    on these two benchmarks. From TABLE [IV](#S8.T4 "TABLE IV ‣ VIII-C Experimental
    Results ‣ VIII Datasets and Performance Evaluation ‣ Deep Learning for Weakly-Supervised
    Object Detection and Object Localization: A Survey"), we can find the performance
    gains are also significant (1-err cls 35.6% vs. 27.7% in ILSVRC 2012).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: The summary of detection results (mAp (%) and CorLoc (%)) of state-of-the-art
    WSOD methods on Pascal VOC 2007, 2010, and 2012 datasets. The FR means Fast R-CNN [[65](#bib.bib65)].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | 2007 | 2010 | 2012 |'
  prefs: []
  type: TYPE_TB
- en: '| mAP | CorLoc | mAP | CorLoc | mAP | CorLoc |'
  prefs: []
  type: TYPE_TB
- en: '| WSDDN [[25](#bib.bib25)]${}_{\text{CVPR2016}}$ | 39.3 | 58.0 | 36.2 | 59.7
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| WSLPDA [[27](#bib.bib27)]${}_{\text{CVPR2016}}$ | 39.5 | 52.4 | 30.7 | -
    | 29.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ContextLocNet [[29](#bib.bib29)]${}_{\text{ECCV2016}}$ | 36.3 | 55.1 | -
    | - | 35.3 | 54.8 |'
  prefs: []
  type: TYPE_TB
- en: '| OICR [[31](#bib.bib31)]${}_{\text{CVPR2017}}$ | 42.0 | 61.2 | - | - | 38.2
    | 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| WCCN [[32](#bib.bib32)]${}_{\text{CVPR2017}}$ | 42.8 | 56.9 | 39.5 | - |
    37.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ST-WSL [[33](#bib.bib33)]${}_{\text{CVPR2017}}$ | 41.7 | 56.1 | - | - | 39.0
    | 58.8 |'
  prefs: []
  type: TYPE_TB
- en: '| SPN [[35](#bib.bib35)]${}_{\text{ICCV2017}}$ | - | 60.6 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TST [[69](#bib.bib69)]${}_{\text{ICCV2017}}$ | 34.5 | 60.8 | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)]${}_{\text{TPAMI2018}}$ | 45.8 | 63.0 | - | - | 41.6
    | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GAL-fWSD [[38](#bib.bib38)]${}_{\text{CVPR2018}}$ | 47.0 | 68.1 | 45.1 |
    68.3 | 43.1 | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| W2F [[39](#bib.bib39)]${}_{\text{CVPR2018}}$ | 52.4 | 70.3 | - | - | 47.8
    | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ZLDN [[41](#bib.bib41)]${}_{\text{CVPR2018}}$ | 47.6 | 61.2 | - | - | 42.9
    | 61.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MELM [[70](#bib.bib70)]${}_{\text{CVPR2018}}$ | 47.3 | 61.4 | - | - | 42.4
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| TS²C [[42](#bib.bib42)]${}_{\text{ECCV2018}}$ | 44.3 | 61.0 | - | - | 40.0
    | 64.4 |'
  prefs: []
  type: TYPE_TB
- en: '| C-WSL [[71](#bib.bib71)]${}_{\text{ECCV2018}}$ | 45.6 | 63.3 | - | - | 43.0
    | 64.9 |'
  prefs: []
  type: TYPE_TB
- en: '| WSRPN [[44](#bib.bib44)]${}_{\text{ECCV2018}}$ | 47.9 | 66.9 | - | - | 43.4
    | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIL [[45](#bib.bib45)]${}_{\text{CVPR2019}}$ | 40.7 | 59.5 | - | - | 46.7
    | 67.4 |'
  prefs: []
  type: TYPE_TB
- en: '| WS-JDS [[46](#bib.bib46)]${}_{\text{CVPR2019}}$ | 45.6 | 64.5 | 39.9 | 63.1
    | 39.1 | 63.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Pred NET [[48](#bib.bib48)]${}_{\text{CVPR2019}}$ | 53.6 | 71.4 | - | - |
    49.5 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: '| WSOD2 [[49](#bib.bib49)]${}_{\text{ICCV2019}}$ | 53.6 | 69.5 | - | - | 47.2
    | 71.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OAILWSD [[50](#bib.bib50)]${}_{\text{ICCV2019}}$ | 47.6 | 66.7 | - | - |
    43.4 | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| TPWSD [[51](#bib.bib51)]${}_{\text{ICCV2019}}$ | 51.5 | 68.0 | - | - | 45.6
    | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SDCN [[52](#bib.bib52)]${}_{\text{ICCV2019}}$ | 50.2 | 68.6 | - | - | 43.5
    | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIDN [[53](#bib.bib53)]${}_{\text{ICCV2019}}$ | 52.6 | 68.7 | - | - | 50.2
    | 71.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ICMWSD [[23](#bib.bib23)]${}_{\text{CVPR2020}}$ | 54.9 | 68.8 | - | - | 52.1
    | 70.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SLV [[57](#bib.bib57)]${}_{\text{CVPR2020}}$ | 53.5 | 71.0 | - | - | 49.2
    | 69.2 |'
  prefs: []
  type: TYPE_TB
- en: '| OICR [[31](#bib.bib31)]+FR${}_{\text{CVPR2017}}$ | 47.0 | 64.3 | - | - |
    42.5 | 65.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)]+FR${}_{\text{TPAMI2018}}$ | 48.8 | 66.6 | - | - |
    44.2 | 68.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MEFF [[72](#bib.bib72)]+FR${}_{\text{CVPR2018}}$ | 51.2 | - | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| C-WSL [[71](#bib.bib71)]+FR${}_{\text{ECCV2018}}$ | 47.8 | 65.6 | - | - |
    45.4 | 66.9 |'
  prefs: []
  type: TYPE_TB
- en: '| WSRPN [[44](#bib.bib44)]+FR${}_{\text{ECCV2018}}$ | 50.4 | 68.4 | - | - |
    45.7 | 69.3 |'
  prefs: []
  type: TYPE_TB
- en: '| WS-JDS [[46](#bib.bib46)]+FR${}_{\text{CVPR2019}}$ | 52.5 | 68.6 | 45.7 |
    68.1 | 46.1 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: '| SDCN [[52](#bib.bib52)]+FR${}_{\text{ICCV2019}}$ | 53.7 | 72.5 | - | - |
    46.7 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIDN [[53](#bib.bib53)]+FR${}_{\text{ICCV2019}}$ | 53.6 | 71.9 | - | -
    | 50.3 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SLV [[57](#bib.bib57)]+FR${}_{\text{CVPR2020}}$ | 53.9 | 72.0 | - | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Detetion results on MSCOCO dataset comes from [[23](#bib.bib23)].
    These models use VGG16 as their convolutional neural network. There is no difference
    between AP and mAP under the MSCOCO context.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Year | Val | Test |'
  prefs: []
  type: TYPE_TB
- en: '| AP | AP[50] | AP | AP[50] |'
  prefs: []
  type: TYPE_TB
- en: '| WSDDN [[25](#bib.bib25)] | CVPR2016 | - | - | - | 11.5 |'
  prefs: []
  type: TYPE_TB
- en: '| WCCN [[32](#bib.bib32)] | CVPR2017 | - | - | - | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)] | TRAMI2018 | 8.5 | 19.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIDN [[53](#bib.bib53)] | ICCV2019 | 9.6 | 21.4 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| WSOD2 [[49](#bib.bib49)] | ICCV2019 | 10.8 | 22.7 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ICMWSD [[23](#bib.bib23)] | CVPR2020 | 11.4 | 24.3 | 12.1 | 24.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Diba et al. [[73](#bib.bib73)]+SSD [[17](#bib.bib17)] | arXiv 2017 | - |
    - | - | 13.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OICR [[31](#bib.bib31)]+Ens+FR [[65](#bib.bib65)] | CVPR2017 | 7.7 | 17.4
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MEFF [[72](#bib.bib72)]+FR [[65](#bib.bib65)] | CVPR2018 | 8.9 | 19.3 | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)]+Ens.+FR [[65](#bib.bib65)] | TPAMI2018 | 9.2 | 19.6
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Object localization performance on ILSVRC 2012 and CUB-200-2011 datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Year | ILSVRC 2012 (top error %) | CUB-200-2011 (top error %)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1-err cls | 5-err cls | 1-err loc | 5-err loc | 1-err cls | 5-err cls | 1-err
    loc | 5-err loc |'
  prefs: []
  type: TYPE_TB
- en: '| CAM [[26](#bib.bib26)] | CVPR2016 | 35.6 | 13.9 | 57.78 | 45.26 | - | - |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ACoL [[40](#bib.bib40)] | CVPR2018 | 32.5 | 12.0 | 54.17 | 36.66 | - | -
    | 54.08 | 39.05 |'
  prefs: []
  type: TYPE_TB
- en: '| SPG [[43](#bib.bib43)] | ECCV2018 | - | - | 51.4 | 35.05 | - | - | 53.36
    | 40.62 |'
  prefs: []
  type: TYPE_TB
- en: '| DANet [[54](#bib.bib54)] | ICCV2019 | 32.5 | 12.0 | 54.17 | 40.57 | 24.6
    | 7.7 | 47.48 | 38.04 |'
  prefs: []
  type: TYPE_TB
- en: '| NL-CCAM [[55](#bib.bib55)] | WACV2020 | 27.7 | - | 49.83 | 39.31 | 26.6 |
    - | 47.6 | 34.97 |'
  prefs: []
  type: TYPE_TB
- en: '| EIL [[56](#bib.bib56)] | CVPR2020 | 29.73 | - | 53.19 | - | 25.23 | - | 42.54
    | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Some techniques and tricks for improving detection results and the
    approaches that utilize them. 1) Cont: Context modeling, 2) Self-t: Self-training
    algorithm, 3) Casc: Cascaded network, 4) BboxR: Bounding box regression, 5) DisRR:
    Discriminative region removal, 6) Low-l: Incorporating low-level features, 7)
    Seg-D: Segmentation-detection collaborative mechanism, 8) Trans: Transforming
    WSOD to FSOD, 9) E-t-h: Easy-to-hard strategy, 10) NegE: Negative evidence, 11)
    SmoL: Optimizing smoothed loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Approach | Specific techniques for discriminative region problem | Training
    tricks |'
  prefs: []
  type: TYPE_TB
- en: '| Cont | Self-t | Casc | BboxR | DisRR | Low-l | Seg-D | Trans | E-t-h | NegE
    | SmoL |'
  prefs: []
  type: TYPE_TB
- en: '| WSDDN [[25](#bib.bib25)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| CAM [[26](#bib.bib26)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WSLPDA [[27](#bib.bib27)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WELDON [[28](#bib.bib28)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| ContextLocNet [[29](#bib.bib29)] | $\surd$ |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Grad-CAM [[30](#bib.bib30)] |  |  |  |  |  | $\surd$ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OICR [[31](#bib.bib31)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WCCN [[32](#bib.bib32)] |  |  | $\surd$ |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ST-WSL [[33](#bib.bib33)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WILDCAT [[34](#bib.bib34)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| SPN [[35](#bib.bib35)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TP-WSL [[36](#bib.bib36)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| PCL [[37](#bib.bib37)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| GAL-fWSD [[38](#bib.bib38)] |  |  |  |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| W2F [[39](#bib.bib39)] |  | $\surd$ |  |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ACoL [[40](#bib.bib40)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ZLDN [[41](#bib.bib41)] |  |  |  |  |  |  |  |  | $\surd$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TS²C [[42](#bib.bib42)] | $\surd$ |  | $\surd$ |  |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SPG [[43](#bib.bib43)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WSRPN [[44](#bib.bib44)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIL [[45](#bib.bib45)] |  |  |  |  |  |  |  |  |  |  | $\surd$ |'
  prefs: []
  type: TYPE_TB
- en: '| WS-JDS [[46](#bib.bib46)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ADL [[47](#bib.bib47)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Pred NET [[48](#bib.bib48)] |  |  |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| WSOD2 [[49](#bib.bib49)] |  | $\surd$ |  | $\surd$ |  | $\surd$ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| OAILWSD [[50](#bib.bib50)] | $\surd$ | $\surd$ |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TPWSD [[51](#bib.bib51)] |  | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SDCN [[52](#bib.bib52)] |  |  |  |  |  |  | $\surd$ | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| C-MIDN [[53](#bib.bib53)] |  | $\surd$ |  |  | $\surd$ |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DANet [[54](#bib.bib54)] |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| NL-CCAM [[55](#bib.bib55)] |  |  |  |  |  |  |  |  |  | $\surd$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| ICMWSD [[23](#bib.bib23)] | $\surd$ | $\surd$ |  | $\surd$ |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| EIL [[56](#bib.bib56)] |  |  |  |  | $\surd$ |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SLV [[57](#bib.bib57)] |  | $\surd$ |  | $\surd$ |  |  |  | $\surd$ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: IX Future Directions and Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although we have summarized many advanced techniques and tricks for improving
    detection results, there are still several research directions that can be further
    explored.
  prefs: []
  type: TYPE_NORMAL
- en: IX-A Model Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Better Initial Proposals. The main proposal generators of the existing methods
    are selective search [[58](#bib.bib58)], edge boxes [[59](#bib.bib59)], heatmap,
    and sliding window. Selective search and edge boxes are too time-consuming and
    yield plenty of initial proposals that most of them are negative proposal. Segmenting
    heatmap tends to focus on the discriminative part of the object. The performance
    of the sliding window is strongly dependent on the size of objects. For example,
    if the size of the object instance is roughly fixed, then the sliding window works
    very well. Otherwise, it works badly. Because these generators have inherent disadvantages,
    we need to design a proposal generator that can yield fewer and more accurate
    initial proposals. The quality of the initial proposals directly affects the detection
    performance of the detector. So how to yield good initial proposals may be a new
    research direction.
  prefs: []
  type: TYPE_NORMAL
- en: Better Positive Proposals. Most WSOD methods select the proposals with the top
    score as positive proposals, which tend to focus on the most discriminative parts
    of the object rather than the whole object region. Because of the above problem,
    ST-WSL [[33](#bib.bib33)] selects positive proposals according to the number of
    their surrounding proposals. And Self-Taught-WS [[33](#bib.bib33)] selects positive
    proposals relying on the relative improvement (RI) of the scores of each proposal
    of two adjacent epochs. Besides, the key of self-training and cascaded network
    is to select accurate proposals as the pseudo ground-truth boxes for later training.
    Thus, how can we design a better algorithm that can accurately select positive
    proposals may be an important research direction.
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight Network. Today’s state-of-the-art object detectors [[14](#bib.bib14),
    [20](#bib.bib20)] leverage a very deep CNN to extract image feature maps and high-dimension
    fully connected layers to detect object instances that can achieve satisfactory
    detection performance. But the deep CNN and high-dimension fully connected layers
    rely on large memory and strong GPU computation power. Hence, a deep network is
    difficult to deploy on CPU devices (e.g., mobile phones). With the popularity
    of mobile devices, lightweight network with few parameters has received more and
    more attention from researchers, such as Light-Head R-CNN [[74](#bib.bib74)].
    Thus, designing a lightweight network in weakly supervised object detection may
    be a new research direction.
  prefs: []
  type: TYPE_NORMAL
- en: IX-B Application Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Medical Imaging. With the development of deep learning, it has evolved into
    cross-learning with multiple disciplines, especially the medical field. Because
    of lacking brain’s Magnetic Resonance Imaging (MRI) and X-rays images with sufficient
    labels, weakly-supervised brain lesion detection [[75](#bib.bib75), [76](#bib.bib76)]
    has received attention from researchers. The purpose of weakly-supervised brain
    lesion detection is to give the model the ability to accurately locate lesion
    region and classify lesion category that helps the doctor complete the diagnosis
    of the disease. Weakly-supervised lesion detection is not only applied in brain
    disease, but also other organ diseases, such as chest, abdomen, and pelvis. In
    addition to lesion detection, weakly-supervised learning is applied in disease
    prognosis [[77](#bib.bib77)]. During hospital visits, patients need to undergo
    a large number of tests to pinpoint their disease. These tests are generally presented
    to doctors and patients in the form of graphic reports. However, these numerous
    graphic reports lack correct labeling information. So, medical imaging may be
    another potential research direction in a weakly supervised setting.
  prefs: []
  type: TYPE_NORMAL
- en: 3D Object Detection. In recent years, with the continuous improvement of the
    accuracy of object detection of images [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80)], 3D object detection [[81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [84](#bib.bib84)] has received unprecedented attention. The purpose of 3D object
    detection is to detect object instances in the point cloud using 3D bounding boxes.
    Comparing with 2D object detection, 3D object detection tends to cost more computation
    and its supervision is more difficult to obtain and labor-intensive. Therefore,
    how to train light and accurate 3D detection models in the point cloud using simple
    labels may be a big challenge. Fortunately, weakly-supervised object detection
    is successfully applied in 2D object detection. According to the above analysis,
    we think that 3D weakly-supervised object detection that uses weak supervision(e.g.,
    2D bounding boxes and text labels) to train object detection models in the 3D
    scene may be a hot research direction.
  prefs: []
  type: TYPE_NORMAL
- en: Video Object Detection. Video object detection [[85](#bib.bib85), [86](#bib.bib86)]
    is to classify and locate object instances in a piece of video. One of the solutions
    is to break the video into many frames and the detector achieves object detection
    in these frame images [[87](#bib.bib87), [88](#bib.bib88)]. However, the detector
    will face one big problem that the quality of these frame images has deteriorated.
    To improve the performance of video object detection, expanding the training dataset
    is a good approach. Unfortunately, tagging object location and category in videos
    is much more difficult than in 2D images. Therefore, training video object detection
    in the weakly-supervised setting is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: X Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we summarize plenty of the deep learning WSOD methods and give
    a lot of solutions to solve the above challenges. In summary, the main contents
    of this paper are listed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We analyze the background, and main challenges, and basic framework of WSOD.
    Furthermore, we introduce several landmark methods in detail.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For main challenges, we analyze almost all of the WSOD methods since 2016 and
    summarize numerous techniques and training tricks (cf. TABLE [V](#S8.T5 "TABLE
    V ‣ VIII-C Experimental Results ‣ VIII Datasets and Performance Evaluation ‣ Deep
    Learning for Weakly-Supervised Object Detection and Object Localization: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce currently popular datasets and important evaluation metrics in
    the WSOD task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We conclude and discuss valuable insights and guidelines for future progress
    in model and application directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    (U19B2043, 61976185), Zhejiang Natural Science Foundation (LR19F020002, LZ17F020001),
    the Fundamental Research Funds for the Central Universities, Chinese Knowledge
    Center for Engineering Sciences and Technology.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
    learning: A review,” *IEEE transactions on neural networks and learning systems*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” *International Journal
    of Computer Vision*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International Journal of
    Computer Vision*, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein *et al.*, “Imagenet large scale visual recognition
    challenge,” *International Journal of Computer Vision*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz,
    S. Levine, R. Hadsell, and K. Bousmalis, “Sim-to-real via sim-to-sim: Data-efficient
    robotic grasping via randomized-to-canonical adaptation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] P. Hu and D. Ramanan, “Finding tiny faces,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Bai, H. Li, J. Zhang, L. Huang, and L. Zhang, “Unsupervised adversarial
    instance-level image retrieval,” *IEEE Transactions on Multimedia*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] W. Chen, Y. Liu, N. Pu, W. Wang, L. Liu, and M. S. Lew, “Feature estimations
    based correlation distillation for incremental image retrieval,” *IEEE Transactions
    on Multimedia*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Tomei, M. Cornia, L. Baraldi, and R. Cucchiara, “Art2real: Unfolding
    the reality of artworks via semantically-aware image-to-image translation,” in
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Behl, O. Hosseini Jafari, S. Karthik Mustikovela, H. Abu Alhaija, C. Rother,
    and A. Geiger, “Bounding boxes, segmentations and object coordinates: How important
    is recognition for 3d scene flow estimation in autonomous driving scenarios?”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Liu, M. Gong, K. Qin, and P. Zhang, “A deep convolutional coupling
    network for change detection based on heterogeneous optical and radar images,”
    *IEEE transactions on neural networks and learning systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *European Conference
    on Computer Vision*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *European Conference on Computer
    Vision*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,”
    in *European Conference on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] B. Singh, M. Najibi, and L. S. Davis, “Sniper: Efficient multi-scale training,”
    in *arXiv preprint arXiv:1805.09300*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Z. Ren, Z. Yu, X. Yang, M.-Y. Liu, Y. J. Lee, A. G. Schwing, and J. Kautz,
    “Instance-aware, context-focused, and memory-efficient weakly supervised object
    detection,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes challenge 2007 (voc2007) results,” 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] H. Bilen and A. Vedaldi, “Weakly supervised deep detection networks,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] D. Li, J.-B. Huang, Y. Li, S. Wang, and M.-H. Yang, “Weakly supervised
    object localization with progressive domain adaptation,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. Durand, N. Thome, and M. Cord, “Weldon: Weakly supervised learning
    of deep convolutional neural networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] V. Kantorov, M. Oquab, M. Cho, and I. Laptev, “Contextlocnet: Context-aware
    deep network models for weakly supervised localization,” in *European Conference
    on Computer Vision*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Tang, X. Wang, X. Bai, and W. Liu, “Multiple instance detection network
    with online instance classifier refinement,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash, and L. Van Gool, “Weakly
    supervised cascaded convolutional networks,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Z. Jie, Y. Wei, X. Jin, J. Feng, and W. Liu, “Deep self-taught learning
    for weakly supervised object localization,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] T. Durand, T. Mordan, N. Thome, and M. Cord, “Wildcat: Weakly supervised
    learning of deep convnets for image classification, pointwise localization and
    segmentation,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao, “Soft proposal networks for
    weakly supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Kim, D. Cho, D. Yoo, and I. So Kweon, “Two-phase learning for weakly
    supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] P. Tang, X. Wang, S. Bai, W. Shen, X. Bai, W. Liu, and A. Yuille, “Pcl:
    Proposal cluster learning for weakly supervised object detection,” *IEEE transactions
    on pattern analysis and machine intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Shen, R. Ji, S. Zhang, W. Zuo, and Y. Wang, “Generative adversarial
    learning towards fast weakly supervised detection,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Zhang, Y. Bai, M. Ding, Y. Li, and B. Ghanem, “W2f: A weakly-supervised
    to fully-supervised framework for object detection,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] X. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang, “Adversarial complementary
    learning for weakly supervised object localization,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] X. Zhang, J. Feng, H. Xiong, and Q. Tian, “Zigzag learning for weakly
    supervised object detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. Wei, Z. Shen, B. Cheng, H. Shi, J. Xiong, J. Feng, and T. Huang, “Ts2c:
    Tight box mining with surrounding segmentation context for weakly supervised object
    detection,” in *European Conference on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] X. Zhang, Y. Wei, G. Kang, Y. Yang, and T. Huang, “Self-produced guidance
    for weakly-supervised object localization,” in *European Conference on Computer
    Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] P. Tang, X. Wang, A. Wang, Y. Yan, W. Liu, J. Huang, and A. Yuille, “Weakly
    supervised region proposal network and object detection,” in *European Conference
    on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] F. Wan, C. Liu, W. Ke, X. Ji, J. Jiao, and Q. Ye, “C-mil: Continuation
    multiple instance learning for weakly supervised object detection,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Shen, R. Ji, Y. Wang, Y. Wu, and L. Cao, “Cyclic guidance for weakly
    supervised joint detection and segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Choe and H. Shim, “Attention-based dropout layer for weakly supervised
    object localization,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Arun, C. Jawahar, and M. P. Kumar, “Dissimilarity coefficient based
    weakly supervised object detection,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Z. Zeng, B. Liu, J. Fu, H. Chao, and L. Zhang, “Wsod2: Learning bottom-up
    and top-down objectness distillation for weakly-supervised object detection,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Kosugi, T. Yamasaki, and K. Aizawa, “Object-aware instance labeling
    for weakly supervised object detection,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] K. Yang, D. Li, and Y. Dou, “Towards precise end-to-end weakly supervised
    object detection network,” in *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] X. Li, M. Kan, S. Shan, and X. Chen, “Weakly supervised object detection
    with segmentation collaboration,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Gao, B. Liu, N. Guo, X. Ye, F. Wan, H. You, and D. Fan, “C-midn: Coupled
    multiple instance detection network with segmentation guidance for weakly supervised
    object detection,” in *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. Xue, C. Liu, F. Wan, J. Jiao, X. Ji, and Q. Ye, “Danet: Divergent activation
    for weakly supervised object localization,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] S. Yang, Y. Kim, Y. Kim, and C. Kim, “Combinational class activation maps
    for weakly supervised object localization,” in *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] J. Mai, M. Yang, and W. Luo, “Erasing integrated learning: A simple yet
    effective approach for weakly supervised object localization,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Z. Chen, Z. Fu, R. Jiang, Y. Chen, and X.-S. Hua, “Slv: Spatial likelihood
    voting for weakly supervised object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Selective
    search for object recognition,” *International Journal of Computer Vision*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. L. Zitnick and P. Dollár, “Edge boxes: Locating object proposals from
    edges,” in *European Conference on Computer Vision*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, “Solving the multiple
    instance problem with axis-parallel rectangles,” *Artificial intelligence*, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
    convolutional networks for visual recognition,” *IEEE transactions on pattern
    analysis and machine intelligence*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving
    for simplicity: The all convolutional net,” *arXiv preprint arXiv:1412.6806*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] R. Girshick, “Fast r-cnn,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] A. Neubeck and L. Van Gool, “Efficient non-maximum suppression,” in *18th
    International Conference on Pattern Recognition*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and
    P. Perona, “Caltech-ucsd birds 200,” 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The caltech-ucsd
    birds-200-2011 dataset,” 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. Shi, H. Caesar, and V. Ferrari, “Weakly supervised object localization
    using things and stuff transfer,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] F. Wan, P. Wei, J. Jiao, Z. Han, and Q. Ye, “Min-entropy latent model
    for weakly supervised object detection,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Gao, A. Li, R. Yu, V. I. Morariu, and L. S. Davis, “C-wsl: Count-guided
    weakly supervised localization,” in *European Conference on Computer Vision*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] W. Ge, S. Yang, and Y. Yu, “Multi-evidence filtering and fusion for multi-label
    classification, object detection and semantic segmentation based on weakly supervised
    learning,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] A. Diba, V. Sharma, R. Stiefelhagen, and L. Van Gool, “Object discovery
    by generative adversarial & ranking networks,” *arXiv preprint arXiv:1711.08174*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn:
    In defense of two-stage object detector,” *arXiv preprint arXiv:1711.07264*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] K. Wu, B. Du, M. Luo, H. Wen, Y. Shen, and J. Feng, “Weakly supervised
    brain lesion segmentation via attentional representation learning,” in *International
    Conference on Medical Image Computing and Computer-Assisted Intervention*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Z. Ji, Y. Shen, C. Ma, and M. Gao, “Scribble-based hierarchical weakly
    supervised learning for brain tumor segmentation,” in *International Conference
    on Medical Image Computing and Computer-Assisted Intervention*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] M. Liu, J. Zhang, C. Lian, and D. Shen, “Weakly supervised deep learning
    for brain disease prognosis using mri and incomplete clinical scores,” *IEEE transactions
    on cybernetics*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Q. Ren, S. Lu, J. Zhang, and R. Hu, “Salient object detection by fusing
    local and global contexts,” *IEEE Transactions on Multimedia*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Wu, Y. Xu, B. Zhang, J. Yang, and D. Zhang, “Deformable template network
    (dtn) for object detection,” *IEEE Transactions on Multimedia*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. Deng, M. Wang, L. Liu, Y. Liu, and Y. Jiang, “Extended feature pyramid
    network for small object detection,” *IEEE Transactions on Multimedia*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object detection
    network for autonomous driving,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Z. Yang, Y. Sun, S. Liu, X. Shen, and J. Jia, “Std: Sparse-to-dense 3d
    object detector for point cloud,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] J. Chen, B. Lei, Q. Song, H. Ying, D. Z. Chen, and J. Wu, “A hierarchical
    graph network for 3d object detection on point clouds,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] W. Shi and R. Rajkumar, “Point-gnn: Graph neural network for 3d object
    detection in a point cloud,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] F. Xiao and Y. Jae Lee, “Video object detection with an aligned spatial-temporal
    memory,” in *European Conference on Computer Vision*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Deng, Y. Pan, T. Yao, W. Zhou, H. Li, and T. Mei, “Single shot video
    object detector,” *IEEE Transactions on Multimedia*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Deng, Y. Hua, T. Song, Z. Zhang, Z. Xue, R. Ma, N. Robertson, and H. Guan,
    “Object guided external memory network for video object detection,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Chen, Y. Cao, H. Hu, and L. Wang, “Memory enhanced global-local aggregation
    for video object detection,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
