- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:48:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:48:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2201.03323] Gait Recognition Based on Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2201.03323] 基于深度学习的步态识别：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2201.03323](https://ar5iv.labs.arxiv.org/html/2201.03323)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2201.03323](https://ar5iv.labs.arxiv.org/html/2201.03323)
- en: 'Gait Recognition Based on Deep Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的步态识别：综述
- en: Claudio Filipi Gonçalves dos Santos [cfsantos@ufscar.br](mailto:cfsantos@ufscar.br)
    Federal Institute of São Carlos - UFSCarRod. Washington Luiz, 235São CarlosSão
    PauloBrazil Eldorado Research InstituteAv. Alan Turing, 275CampinasSão PauloBrazil
    ,  Diego de Souza Oliveira [diego.s.oliveira@unesp.br](mailto:diego.s.oliveira@unesp.br)
    ,  Leandro A. Passos [leandro.passos@unesp.br](mailto:leandro.passos@unesp.br)
    [0000-0003-3529-3109](https://orcid.org/0000-0003-3529-3109 "ORCID identifier")
    ,  Rafael Gonçalves Pires [rafapires@gmail.com](mailto:rafapires@gmail.com) [0000-0001-9597-055X](https://orcid.org/0000-0001-9597-055X
    "ORCID identifier") ,  Daniel Felipe Silva Santos [danielfssantos1@gmail.com](mailto:danielfssantos1@gmail.com)
    ,  Lucas Pascotti Valem [lucas.valem@unesp.br](mailto:lucas.valem@unesp.br) [0000-0002-3833-9072](https://orcid.org/0000-0002-3833-9072
    "ORCID identifier") ,  Thierry P. Moreira [thierrypin@gmail.com](mailto:thierrypin@gmail.com)
    [0000-0002-3410-6247](https://orcid.org/0000-0002-3410-6247 "ORCID identifier")
    ,  Marcos Cleison S. Santana [marcoscleison@gmail.com](mailto:marcoscleison@gmail.com)
    [0000-0003-2568-8019](https://orcid.org/0000-0003-2568-8019 "ORCID identifier")
    ,  Mateus Roder [mateus.roder@unesp.br](mailto:mateus.roder@unesp.br) [0000-0002-3112-5290](https://orcid.org/0000-0002-3112-5290
    "ORCID identifier") ,  João Paulo Papa [joao.papa@unesp.br](mailto:joao.papa@unesp.br)
    [0000-0002-6494-7514](https://orcid.org/0000-0002-6494-7514 "ORCID identifier")
    São Paulo State University - UNESPAv. Eng. Luís Edmundo Carrijo Coube, 14-01BauruSão
    PauloBrazil  and  Danilo Colombo [colombo.danilo@petrobras.com.br](mailto:colombo.danilo@petrobras.com.br)
    Cenpes, Petroleo Brasileiro S.A. - PetrobrasRio de JaneiroRio de JaneiroBrazil
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Claudio Filipi Gonçalves dos Santos [cfsantos@ufscar.br](mailto:cfsantos@ufscar.br)
    圣卡洛斯联邦大学 - UFSCarRod. Washington Luiz, 235São CarlosSão PauloBrazil Eldorado Research
    InstituteAv. Alan Turing, 275CampinasSão PauloBrazil， Diego de Souza Oliveira
    [diego.s.oliveira@unesp.br](mailto:diego.s.oliveira@unesp.br)， Leandro A. Passos
    [leandro.passos@unesp.br](mailto:leandro.passos@unesp.br) [0000-0003-3529-3109](https://orcid.org/0000-0003-3529-3109
    "ORCID identifier")， Rafael Gonçalves Pires [rafapires@gmail.com](mailto:rafapires@gmail.com)
    [0000-0001-9597-055X](https://orcid.org/0000-0001-9597-055X "ORCID identifier")，
    Daniel Felipe Silva Santos [danielfssantos1@gmail.com](mailto:danielfssantos1@gmail.com)，
    Lucas Pascotti Valem [lucas.valem@unesp.br](mailto:lucas.valem@unesp.br) [0000-0002-3833-9072](https://orcid.org/0000-0002-3833-9072
    "ORCID identifier")， Thierry P. Moreira [thierrypin@gmail.com](mailto:thierrypin@gmail.com)
    [0000-0002-3410-6247](https://orcid.org/0000-0002-3410-6247 "ORCID identifier")，
    Marcos Cleison S. Santana [marcoscleison@gmail.com](mailto:marcoscleison@gmail.com)
    [0000-0003-2568-8019](https://orcid.org/0000-0003-2568-8019 "ORCID identifier")，
    Mateus Roder [mateus.roder@unesp.br](mailto:mateus.roder@unesp.br) [0000-0002-3112-5290](https://orcid.org/0000-0002-3112-5290
    "ORCID identifier")， João Paulo Papa [joao.papa@unesp.br](mailto:joao.papa@unesp.br)
    [0000-0002-6494-7514](https://orcid.org/0000-0002-6494-7514 "ORCID identifier")
    圣保罗州立大学 - UNESPAv. Eng. Luís Edmundo Carrijo Coube, 14-01BauruSão PauloBrazil
    和 Danilo Colombo [colombo.danilo@petrobras.com.br](mailto:colombo.danilo@petrobras.com.br)
    Cenpes, Petroleo Brasileiro S.A. - PetrobrasRio de JaneiroRio de JaneiroBrazil
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In general, biometry-based control systems may not rely on individual expected
    behavior or cooperation to operate appropriately. Instead, such systems should
    be aware of malicious procedures for unauthorized access attempts. Some works
    available in the literature suggest addressing the problem through gait recognition
    approaches. Such methods aim at identifying human beings through intrinsic perceptible
    features, despite dressed clothes or accessories. Although the issue denotes a
    relatively long-time challenge, most of the techniques developed to handle the
    problem present several drawbacks related to feature extraction and low classification
    rates, among other issues. However, deep learning-based approaches recently emerged
    as a robust set of tools to deal with virtually any image and computer-vision
    related problem, providing paramount results for gait recognition as well. Therefore,
    this work provides a surveyed compilation of recent works regarding biometric
    detection through gait recognition with a focus on deep learning approaches, emphasizing
    their benefits, and exposing their weaknesses. Besides, it also presents categorized
    and characterized descriptions of the datasets, approaches, and architectures
    employed to tackle associated constraints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，基于生物识别的控制系统可能不依赖于个体预期的行为或合作来正常运作。相反，这些系统应该能够识别恶意程序以防止未经授权的访问尝试。文献中的一些研究建议通过步态识别方法来解决这个问题。这些方法旨在通过内在的可感知特征来识别个体，尽管存在穿着衣物或配件的情况。尽管这个问题相对长期存在，大多数开发的技术在处理问题时都存在与特征提取和低分类率相关的若干缺陷。然而，基于深度学习的方法最近作为一套强大的工具出现，能够处理几乎任何图像和计算机视觉相关的问题，并为步态识别提供了重要的结果。因此，本文提供了关于通过步态识别进行生物识别的近期研究的综合汇编，重点介绍了深度学习方法的优点，并揭示了它们的弱点。此外，还呈现了对用于解决相关约束的数据集、方法和架构的分类和特征化描述。
- en: 'Gait Recognition, Biometrics, Deep Learning^†^†copyright: acmcopyright^†^†journal:
    CSUR^†^†journalyear: 2021^†^†journalvolume: 1^†^†journalnumber: 1^†^†article:
    1^†^†publicationmonth: 1^†^†price: 15.00^†^†doi: 10.1145/3490235^†^†journal: JACM^†^†journalvolume:
    37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8^†^†ccs: Computing
    methodologies Machine learning^†^†ccs: Security and privacy Biometrics'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 步态识别、生物识别、深度学习^†^†版权：acmcopyright^†^†期刊：CSUR^†^†期刊年份：2021^†^†期刊卷号：1^†^†期刊期号：1^†^†文章：1^†^†出版月份：1^†^†价格：15.00^†^†doi：10.1145/3490235^†^†期刊：JACM^†^†期刊卷号：37^†^†期刊期号：4^†^†文章：111^†^†出版月份：8^†^†ccs：计算方法 机器学习^†^†ccs：安全与隐私 生物识别
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Gait recognition emerged in the last decades as a branch of biometric identification
    that focuses on detecting individuals through personal measurements and relationships,
    e.g., trunk and limbs’ size, as well as space-time information related to the
    intrinsic patterns in individuals movements (Bolle et al., [2013](#bib.bib7)).
    Such an approach presented itself extremely useful in the contexts of surveillance
    systems or hazy environments monitoring, for instance, where unique elements usually
    employed for biometric identification such as the fingerprint and the face are
    hard or impossible to distinguish.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 步态识别作为生物识别的一种分支在最近几十年中出现，重点关注通过个人的测量和关系来检测个体，例如躯干和四肢的大小，以及与个体运动的内在模式相关的时空信息（Bolle等，[2013](#bib.bib7)）。这种方法在监控系统或模糊环境监测等背景下表现出极大的实用性，例如，当通常用于生物识别的独特元素如指纹和面部难以或不可能区分时。
- en: Besides, gait recognition approaches possess some advantages regarding other
    biometric identification models, since hacking such architectures poses a hard
    task (Nixon and Carter, [2004](#bib.bib72)). The difficulty primary lies in the
    concept’s intrinsic characteristics, i.e., identification based on the silhouette
    and its movement, whose reproduction is particularly complicated. The same is
    not valid for other techniques, where the individual can trick the system by hiding
    the face, for instance. Further, gait recognition models do not require high-resolution
    images and specialized equipment for proper identification, like iris and fingerprints,
    for instance. Furthermore, gait recognition methods are independent of individual
    cooperation, while other methods require the analyzed person collaborates with
    the identification system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相比其他生物特征识别模型，步态识别方法具有一些优势，因为破解这些架构是一项艰巨的任务（Nixon 和 Carter，[2004](#bib.bib72)）。困难主要在于概念的固有特性，即基于轮廓及其运动的识别，其再现特别复杂。这一点不适用于其他技术，例如，个体可以通过隐藏面部来欺骗系统。此外，步态识别模型不需要高分辨率图像和专用设备进行正确的识别，例如虹膜和指纹识别。此外，步态识别方法不依赖于个体的合作，而其他方法则需要被分析者与识别系统合作。
- en: Moreover, gait information is easily collected from distance, which stands for
    an enormous advantage regarding other techniques, especially when the identification
    is not assisted by the analyzed person, e.g., criminal investigations. Besides,
    since it does not require sophisticated equipment for data extraction, these methods
    are commonly cheaper than other approaches, majorly due to the popularization
    of surveillance systems and the advent of cellphones equipped with accelerometers,
    which transformed the burden of extracting data signals into a straightforward
    task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，步态信息可以从远处轻松收集，这相对于其他技术是一个巨大的优势，尤其是当识别不依赖被分析者的协助时，例如，刑事调查。此外，由于它不需要复杂的设备进行数据提取，这些方法通常比其他方法便宜，这主要得益于监控系统的普及和配备有加速度计的手机的出现，这使得数据信号提取变得简单明了。
- en: Despite the simplicity regarding the tools mentioned above, identifying people
    by walking and moving is far from a trivial task. Standard gait recognition methods,
    i.e., which comprise data pre-processing and features extracted in a handcrafted
    fashion for further recognition, often suffer from several constraints and challenges
    imposed by the complexity of the task, such as viewing angle and large intra-class
    variations, occlusions, shadows, and locating the body segments, among others.
    A new trend on machine learning, known as deep learning, emerged in the last years
    as a revolutionary tool to handle topics in image and sound processing, computer
    vision, and speech, overwhelmingly outperforming virtually any baseline established
    until then. The new paradigm exempts the necessity of manually extract representative
    features from experts and also provides paramount results regarding gait recognition,
    surpassing existing challenges and opening room for further research.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述工具相对简单，但通过步态和动作识别个体远非一项简单任务。标准步态识别方法，即包含数据预处理和手工提取特征以进行进一步识别，通常受到任务复杂性带来的多种约束和挑战，例如视角和大范围的类别内变异、遮挡、阴影以及身体部位的定位等。近年来，机器学习中的一个新趋势，称为深度学习，作为一种革命性工具出现，处理图像和声音处理、计算机视觉和语音等话题，远远超越了当时几乎所有的基准。新的范式免去了专家手动提取代表性特征的必要，并且在步态识别方面提供了重要的成果，超越了现有挑战，并为进一步研究打开了空间。
- en: 'Due to the increasing number of biometric and gait recognition works developed
    in the last years, many authors provided studies summarizing each field’s main
    achievements. Pisani et al. (Pisani et al., [2019](#bib.bib76)), Khan et al. (Khan
    et al., [2020](#bib.bib48)), and Sundararajan et al. (Sundararajan et al., [2019](#bib.bib96)),
    for instance, exposed the main advances in the last years regarding biometrics
    identification in general, while Gui et al. (Gui et al., [2019](#bib.bib26)) explored
    the novelties regarding brain biometrics methods. Regarding gait recognition,
    both Wan et al. (Wan et al., [2018](#bib.bib111)), and Rida et al. (Rida et al.,
    [2018](#bib.bib80)) provided a comprehensive survey comprising general topics
    on the context in 2018\. Meanwhile, Singh et al. (Singh et al., [2018](#bib.bib90))
    published a study on vision-based gait recognition, while Bouchrika (Bouchrika,
    [2018](#bib.bib8)) proposed a similar work considering smart visual surveillance.
    Further in 2019, Nambiar et al. (Nambiar et al., [2019](#bib.bib68)) summarized
    the works regarding gait-based person re-identification, while Marsico and Mecca (Marsico
    and Mecca, [2019](#bib.bib63)) focused on gait recognition via wearable sensors.
    However, none of them focus on presenting the main advances regarding deep learning-based
    approaches for gait recognition. Moreover, even though there are reviews in this
    context (Alharthi et al., [2019](#bib.bib3)), most of the surveyed works comprise
    Convolutional Neural Networks (CNN) (LeCun et al., [1998](#bib.bib50)) and Long
    short-term memory (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib33); Lobo
    et al., [2020](#bib.bib56)), leaving behind some relevant and novel architectures,
    such as Autoencoders (Vincent et al., [2010](#bib.bib109)), Capsule Networks (CapsNet) (Sabour
    et al., [2017](#bib.bib83)), Deep Belief Networks (Hinton et al., [2006](#bib.bib32)),
    and Generative Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib24)).
    Therefore, the main objectives of this work are three-fold:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于近年来生物特征识别和步态识别领域的研究数量不断增加，许多作者提供了总结各领域主要成就的研究。例如，Pisani等人（Pisani et al., [2019](#bib.bib76)）、Khan等人（Khan
    et al., [2020](#bib.bib48)）和Sundararajan等人（Sundararajan et al., [2019](#bib.bib96)）展示了近年来生物识别领域的一些主要进展，而Gui等人（Gui
    et al., [2019](#bib.bib26)）则探讨了脑部生物特征识别方法的创新。关于步态识别，Wan等人（Wan et al., [2018](#bib.bib111)）和Rida等人（Rida
    et al., [2018](#bib.bib80)）在2018年提供了涵盖一般主题的全面调查。同时，Singh等人（Singh et al., [2018](#bib.bib90)）发布了一项基于视觉的步态识别研究，而Bouchrika（Bouchrika,
    [2018](#bib.bib8)）则提出了一项类似的工作，考虑了智能视觉监控。进一步地，2019年Nambiar等人（Nambiar et al., [2019](#bib.bib68)）总结了有关步态识别的工作，而Marsico和Mecca（Marsico
    and Mecca, [2019](#bib.bib63)）则专注于通过可穿戴传感器进行步态识别。然而，他们都没有关注深度学习方法在步态识别中的主要进展。此外，尽管在这一背景下已有综述（Alharthi
    et al., [2019](#bib.bib3)），但大多数调查的工作仅涉及卷积神经网络（CNN）（LeCun et al., [1998](#bib.bib50)）和长短期记忆网络（LSTM）（Hochreiter
    and Schmidhuber, [1997](#bib.bib33); Lobo et al., [2020](#bib.bib56)），而忽略了一些相关且新颖的架构，如自编码器（Autoencoders）（Vincent
    et al., [2010](#bib.bib109)）、胶囊网络（CapsNet）（Sabour et al., [2017](#bib.bib83)）、深度置信网络（Deep
    Belief Networks）（Hinton et al., [2006](#bib.bib32)）和生成对抗网络（GANs）（Goodfellow et
    al., [2014](#bib.bib24)）。因此，本工作的主要目标有三点：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: to systematically introduce the most recent and significant works comprising
    strategies for gait recognition through deep learning approaches; and
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统介绍包括步态识别策略的最新和重要的深度学习方法；以及
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: to provide the reader with a substantial and illustrated theoretical background
    regarding gait recognition, exploring its roots on biometric recognition and exposing
    the most popular tools employed to gait feature extraction and the architectures
    used to tackle the associated constraints; and
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为读者提供有关步态识别的丰富且有图示的理论背景，探讨其在生物识别中的根源，并揭示用于步态特征提取的最流行工具及应对相关约束的架构；以及
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: to present an illustrated, categorized, and characterized catalog of the public
    datasets available for the task of gait recognition.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供一个图示化、分类和特征化的公共数据集目录，供步态识别任务使用。
- en: Regarding the selection of the reviewed papers, the keywords employed in the
    search were “gait recognition using *deep learning technique*”, such that *deep
    learning technique* was replaced by the architectures themselves, e.g., convolutional
    neural networks, LSTM, and so on. Another filter considered was the work’s innovation,
    i.e., papers with similar techniques and architectures were discarded. Finally,
    the studies considered in this research were published five years ago at most.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于所回顾论文的选择，搜索中使用的关键词为“使用*深度学习技术*的步态识别”，其中*深度学习技术*被具体的架构所替代，例如卷积神经网络、LSTM等。另一个考虑的筛选条件是工作的创新性，即类似技术和架构的论文被排除。最后，本文研究考虑的研究最多发表在五年前。
- en: 'The remainder of this work is presented as follows. Section [2](#S2 "2\. Theoretical
    Background ‣ Gait Recognition Based on Deep Learning: A Survey") presents a theoretical
    background regarding the concepts and available methods for biometric detection,
    bestows an overview regarding the deep learning techniques employed in this work,
    and revisits the main concepts regarding gait recognition. Further, Section [3](#S3
    "3\. Gait Recognition Through Deep Learning-Based Approaches ‣ Gait Recognition
    Based on Deep Learning: A Survey") introduces the most recent approaches using
    deep learning for gait recognition. Besides, the section also provides a brief
    discussion regarding the surveyed works. Datasets employed for the task are presented
    in Section [4](#S4 "4\. Datasets ‣ Gait Recognition Based on Deep Learning: A
    Survey"). Finally, Section [5](#S5 "5\. Conclusions and Future Directions ‣ Gait
    Recognition Based on Deep Learning: A Survey") presents the conclusions and future
    works.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分如下展示。第[2](#S2 "2\. Theoretical Background ‣ Gait Recognition Based on
    Deep Learning: A Survey")节提供了有关生物识别检测概念和现有方法的理论背景，概述了本文中使用的深度学习技术，并重新审视了步态识别的主要概念。此外，第[3](#S3
    "3\. Gait Recognition Through Deep Learning-Based Approaches ‣ Gait Recognition
    Based on Deep Learning: A Survey")节介绍了使用深度学习进行步态识别的最新方法。该节还简要讨论了所调查的研究工作。用于任务的数据集在第[4](#S4
    "4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")节中呈现。最后，第[5](#S5
    "5\. Conclusions and Future Directions ‣ Gait Recognition Based on Deep Learning:
    A Survey")节总结了结论和未来的工作。'
- en: 2\. Theoretical Background
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 理论背景
- en: This section presents a theoretical background introducing the problem of biometric
    identification, describing the deep learning approaches employed for gait recognition,
    and a detailed introduction regarding the problem of gait recognition.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了生物识别识别的问题，描述了用于步态识别的深度学习方法，并详细介绍了步态识别问题。
- en: 2.1\. Biometric Identification
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 生物识别识别
- en: The problem of people identification poses a challenging task for humankind
    since long before the existence of computers, when specialists were responsible
    for analyzing and comparing documents, signatures, and other features in a handcrafted
    fashion to present some restricted information or to allow some banking transaction,
    for instance. The importance of an accurate identification was intensified insofar
    as the society informatization progressed, leading to the necessity of robust
    solutions for individuals’ recognition. In this context, a wide range of techniques
    emerged in the literature as alternatives for effectively identifying people through
    images or another biometric means.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人员识别的问题对人类提出了具有挑战性的任务，早在计算机出现之前，专家们就负责以手工方式分析和比较文件、签名和其他特征，以提供一些受限的信息或进行一些银行交易等。随着社会信息化的进展，对准确识别的需求不断增加，导致了对个人识别的强大解决方案的需求。在这种背景下，文献中出现了各种技术作为通过图像或其他生物识别手段有效识别人员的替代方案。
- en: 'To be considered a biometric qualification criterion, a candidate feature must
    meet the following conditions (Jain et al., [2004](#bib.bib40)):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 被视为生物识别资格标准的候选特征必须满足以下条件（Jain 等，[2004](#bib.bib40)）：
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Universality: each person should have the characteristic;'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 普遍性：每个人都应该具备这一特征；
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distinctiveness: any two persons should be sufficiently different in terms
    of the characteristic;'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区别性：任何两个人在特征上应足够不同；
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Permanence: the characteristic should be sufficiently invariant over a period
    of time;'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持久性：特征在一段时间内应保持足够的不变性；
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Collectability: the characteristic can be measured quantitatively.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可收集性：该特征可以定量测量。
- en: 'However, in a practical biometric system, other issues should also be considered,
    including:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际的生物识别系统中，还应考虑其他问题，包括：
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance: which refers to the achievable recognition accuracy and speed,
    the resources required to achieve the desired recognition accuracy and speed,
    as well as the operational and environmental factors that can affect the accuracy
    and the speed;'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能：指的是可实现的识别准确性和速度、达到所需识别准确性和速度所需的资源，以及可能影响准确性和速度的操作和环境因素；
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Acceptability: which indicates the extent to which people are willing to accept
    the use of a particular biometric identifier in their daily lives;'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可接受性：指的是人们愿意在日常生活中接受特定生物识别标识符的程度；
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Circumvention: which denotes the easiness the system is fooled through fraudulent
    methods.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规避：指的是系统通过欺诈方法被愚弄的难易程度。
- en: Among such techniques, stand out the fingerprint, iris, face, and gait recognition,
    among others. The next section briefly revisits some of these biometric approaches.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些技术中，指纹、虹膜、面部和步态识别等方法尤为突出。下一节简要回顾了这些生物识别方法中的一些。
- en: 2.1.1\. Fingerprint Recognition
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1. 指纹识别
- en: Formally known as dactyloscopy, fingerprint recognition systems are widely used
    due to the singularity of the ridges and furrows on the surface of a finger, which
    provides an intrinsic characteristic for each individual (Li, [2009](#bib.bib55)).
    Moreover, such features are steady and poorly degraded over time, making the creation
    of a digital fingerprint image database extremely reliable.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正式称为dactyloscopy，指纹识别系统被广泛使用，因为指纹表面的脊线和凹槽的独特性为每个人提供了固有的特征（Li, [2009](#bib.bib55)）。此外，这些特征稳定且随时间退化较少，使得创建数字指纹图像数据库极为可靠。
- en: The first model for fingerprint recognition was designed in the late 1960s,
    based on a system created in the XIX century by Francis Galton called Galton points (Mitchell,
    [1920](#bib.bib66)). Since then, many works addressed the problem through different
    perspectives, such as digital image processing (Alsmirat et al., [2019](#bib.bib4);
    Zneit et al., [2017](#bib.bib139)), Generative Adversarial Networks  (Yu et al.,
    [2019](#bib.bib127)), and filter representation (Lee et al., [2017](#bib.bib51)),
    to cite a few.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个指纹识别模型是在1960年代末设计的，基于19世纪Francis Galton 创建的Galton points 系统（Mitchell, [1920](#bib.bib66)）。自那时起，许多研究通过不同的视角解决了这一问题，如数字图像处理（Alsmirat
    等人, [2019](#bib.bib4); Zneit 等人, [2017](#bib.bib139)）、生成对抗网络（Yu 等人, [2019](#bib.bib127)）和滤波表示（Lee
    等人, [2017](#bib.bib51)），仅举几例。
- en: Fingerprint recognition systems are considered the most reliable and accurate
    biometric identification system. Nevertheless, the field is still facing several
    challenges, such as unsatisfactory accuracy under non-ideal conditions and security
    issues such as spoofing attacks. In this context, Hemalatha et al. (Hemalatha,
    [2020](#bib.bib30)) present a systematic review comprising the most recent techniques
    employed to deal with the drawbacks mentioned above and others.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 指纹识别系统被认为是最可靠和准确的生物识别系统。然而，该领域仍面临多个挑战，如在非理想条件下的准确性不令人满意以及伪造攻击等安全问题。在这种背景下，Hemalatha
    等人（Hemalatha, [2020](#bib.bib30)）提出了一项系统评估，涵盖了应对上述缺陷和其他问题的最新技术。
- en: 2.1.2\. Iris Recognition
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2. 虹膜识别
- en: The term iris denotes a colorful thin circular structure in the eye responsible
    for controlling the pupil diameter, as well as the amount of light that reaches
    the retina. Such a structure presents a great advantage for people’s identification
    since it is consistent against alterations on environmental conditions and generally
    suffers low degradation over time. Moreover, iris recognition is among the most
    accurate, low-cost, and convenient identification methods, since it is performed
    by image and do not require contact with the person (Wang and Kumar, [2019](#bib.bib113)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虹膜一词指的是眼睛中一个彩色的薄圆形结构，负责控制瞳孔直径以及到达视网膜的光线量。这种结构对个人身份识别具有巨大优势，因为它在环境条件变化下保持一致，并且通常随着时间的推移退化很少。此外，虹膜识别是最准确、低成本和便捷的识别方法之一，因为它通过图像进行，不需要接触被识别者（Wang
    和 Kumar, [2019](#bib.bib113)）。
- en: Most commercial models employing iris recognition are developed based on the
    identification of iris’ lower and upper limits using an integral-differential
    operator (Karn et al., [2020](#bib.bib47)), even when using eyelid demarcation.
    Such an operator assumes the pupil is circular and acts as an orbicular border
    detector. Further researches introduced distinct mathematical operations to the
    process to make it more flexible and robust, such as parabolic curve identifiers
    and normalizations, to cite a few.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数商业模型使用虹膜识别是基于使用积分-微分算子（Karn 等，[2020](#bib.bib47)）来识别虹膜的上下边界，即使在使用眼睑分界时也是如此。这种算子假设瞳孔是圆形的，并作为圆周边界检测器。进一步的研究引入了不同的数学操作，使过程更加灵活和稳健，例如抛物线曲线识别器和归一化，仅举几例。
- en: Moreover, a different approach was proposed by Garagad and Iyer (Garagad and
    Iyer, [2014](#bib.bib23)), which considers a scale- and slope-invariant method
    that radially draws the iris region for feature extraction. The method segments
    the pupil’s central area using concentric circles and discards irrelevant regions
    using discontinuity detection techniques. Besides, the method is straightforward
    if compared to the approach mentioned above since it does not use integral-differential
    operators, even though its effectiveness is confirmed by hit rates above $83\%$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Garagad 和 Iyer（Garagad 和 Iyer，[2014](#bib.bib23)）提出了一种不同的方法，该方法考虑了一个尺度和斜率不变的方法，径向地绘制虹膜区域以进行特征提取。该方法使用同心圆分割瞳孔的中央区域，并使用不连续性检测技术丢弃无关区域。此外，与上述方法相比，该方法比较简单，因为它不使用积分-微分算子，尽管其有效性已通过超过
    $83\%$ 的命中率得到确认。
- en: Recent studies toward iris recognition aim to overcome some challenges in the
    field, such as the adverse noise caused by specular reflections, the absence of
    iris, gaze deviation, motion/defocus blur, iris rotation, and occlusions due to
    hair/eyelid/glasses/eyelash (Wang et al., [2020](#bib.bib112)). Besides, He et
    al. (He et al., [2020](#bib.bib28)) discuss how new trends towards long-distance
    iris recognition have been tackled recently and the future trends to deal with
    the problem.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的虹膜识别研究旨在克服该领域的一些挑战，例如由镜面反射引起的不利噪声、虹膜缺失、视线偏差、运动/失焦模糊、虹膜旋转和由于头发/眼睑/眼镜/睫毛造成的遮挡（Wang
    等，[2020](#bib.bib112)）。此外，He 等（He 等，[2020](#bib.bib28)）讨论了如何最近应对长距离虹膜识别的新趋势以及未来处理这些问题的趋势。
- en: 2.1.3\. Face Recognition
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3. 人脸识别
- en: Face recognition is a prominent biometric system extensively used in identification
    and authentication systems in the most diverse areas, like banks, military services,
    and public security (Zou et al., [2018a](#bib.bib141)). Such methods became popular
    in the earlies 90’s when Turk and Pentland proposed the Eigenfaces method (Turk
    and Pentland, [1991](#bib.bib106)). In the following decade emerged several approaches
    so-called holistic, which are derived from low-dimensional distribution representations,
    like linear sub-spaces (Deng et al., [2014](#bib.bib19)), and sparse reproduction (Deng
    et al., [2018b](#bib.bib18)), among others.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸识别是一种突出的生物特征系统，广泛用于各种领域的身份识别和认证系统，如银行、军事服务和公共安全（Zou 等，[2018a](#bib.bib141)）。这种方法在90年代初变得流行，当时
    Turk 和 Pentland 提出了特征脸方法（Turk 和 Pentland，[1991](#bib.bib106)）。在接下来的十年中，出现了几种所谓的整体方法，这些方法来源于低维分布表示，如线性子空间（Deng
    等，[2014](#bib.bib19)）和稀疏重建（Deng 等，[2018b](#bib.bib18)）等。
- en: Nevertheless, the major drawback regarding holistic methods lies is the intolerance
    to face positioning change, thus leading to a search for characteristics based
    methods. In this context, emerged the Local Binary Patters (LBP) and the Gabor
    feature based classification, achieving relevant results through filters embodying
    invariant properties. Later on, subsequent works started to focus on local descriptors-based
    approaches for face recognition (Lei et al., [2014](#bib.bib52)). In short, the
    objective of these methods is to train spatial filters for image feature extraction,
    such that the difference between images from the same person is minimized. On
    the other hand, the distance between features extracted from different people
    should be maximized. Even though such methods obtained consistent results, techniques
    considered “shallow”, in general, suffer from inevitable limitations due to the
    non-linear complexity of the face variations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，整体方法的主要缺点在于对面部位置变化的容忍度较低，因此导致了对特征基础方法的探索。在这种背景下，出现了局部二值模式（LBP）和基于 Gabor
    特征的分类方法，通过体现不变属性的滤波器取得了相关结果。随后，后续工作开始关注基于局部描述符的面部识别方法（Lei 等，[2014](#bib.bib52)）。简而言之，这些方法的目标是训练空间滤波器进行图像特征提取，从而最小化来自同一人的图像之间的差异。另一方面，应该最大化来自不同人的特征之间的距离。尽管这些方法取得了一致的结果，但一般认为的“浅层”技术由于面部变化的非线性复杂性，仍然面临不可避免的局限性。
- en: A few years ago, a counterpoint for such approaches arose after the AlexNet
    neural network had won the ImageNet (Krizhevsky et al., [2012](#bib.bib49)) competition.
    Since then, deep learning models gained attention in diverse areas of biometrics,
    including face recognition. A particular family of networks known as Convolutional
    Neural Networks was the first to achieve a level of recognition near humans, i.e.,
    the DeepFace (Taigman et al., [2014](#bib.bib98)). The model composed of six convolutional
    layers achieved accuracies less than $0.2\%$ different from one human competitor
    ($97.35\%$ against $97.53\%$). Nowadays, the model is considered relatively simple
    when compared to popular architectures, like Inception (Szegedy et al., [2016](#bib.bib97)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，随着 AlexNet 神经网络赢得了 ImageNet（Krizhevsky 等，[2012](#bib.bib49)）竞赛，对于这种方法的反对意见出现了。从那时起，深度学习模型在生物识别的各个领域获得了关注，包括面部识别。一类被称为卷积神经网络的网络家族首次达到了接近人类的识别水平，即
    DeepFace（Taigman 等，[2014](#bib.bib98)）。由六层卷积层组成的模型达到了与一名人类竞争者准确率相差不到 $0.2\%$ 的水平（$97.35\%$
    对比 $97.53\%$）。如今，与流行的架构如 Inception（Szegedy 等，[2016](#bib.bib97)）相比，这个模型被认为相对简单。
- en: Even though face recognition approaches evolved to unprecedented standards in
    the last years, the field is still facing several challenges in terms of expression,
    pose, illumination, aging, and face partially or entirely obstructed by masks
    or veils. Recent research presented by Tiong et al. (Tiong et al., [2020](#bib.bib103))
    suggest using multimodal biometrics to tackle the issue. Besides, Jaraman et al. (Jayaraman
    et al., [2020](#bib.bib41)) describe the most relevant alternatives freshly employed
    for face recognition in the last years, which comprise local and global features,
    neural networks, and deep learning-based approaches.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面部识别方法在过去几年中发展到了前所未有的标准，但该领域在表达、姿势、光照、衰老以及面部被面具或面纱部分或完全遮挡等方面仍面临着许多挑战。Tiong
    等（Tiong 等，[2020](#bib.bib103)）提出了使用多模态生物识别来解决这一问题。此外，Jayaraman 等（Jayaraman 等，[2020](#bib.bib41)）描述了近年来在面部识别中最新采用的最相关的替代方案，包括局部和全局特征、神经网络和基于深度学习的方法。
- en: 2.1.4\. Multimodal Biometric Recognition
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. 多模态生物识别
- en: One of the main advantages of the human brain towards individual recognition
    over computer-based approaches concerns the ability to simultaneously evaluate
    multiple modalities of descriptive information, such as face, gait, and hair and
    eyes color. To adapt to this trend, a new paradigm of biometrics recognition emerged,
    the so-called “multimodal biometrics”. Such an approach aims at combining distinct
    biometric recognition methods and auxiliary information to improve the performance
    and reliability when considering a single technique. In this context, Sultana
    et al. (Sultana et al., [2017](#bib.bib94)) proposed a person recognition approach
    based on a combination of visual cues, such as face and ear, with the person’s
    social behavior. Experiments over semi-real datasets provided a significant advantage
    over standard biometric systems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑在个体识别方面相较于计算机方法的主要优势之一是能够同时评估多种描述性信息的能力，例如面部、步态以及头发和眼睛的颜色。为了适应这一趋势，出现了一种新的生物识别范式，即所谓的“多模态生物识别”。这种方法旨在结合不同的生物识别方法和辅助信息，以提高在考虑单一技术时的性能和可靠性。在这种背景下，Sultana
    等人（Sultana et al., [2017](#bib.bib94)）提出了一种基于面部和耳朵等视觉线索与个人社会行为相结合的个人识别方法。对半真实数据集的实验提供了显著的优势，相较于标准生物识别系统。
- en: A recent work proposed by Li et al. (Li et al., [2021](#bib.bib54)) implements
    a finger-based multimodal biometrics system that considers a fusion strategy to
    extract correlated features from different modalities of finger patterns, i.e.,
    finger veins and knuckle print, obtaining state-of-the-art results over finger
    recognition methods. A similar work proposed by Tiong et al. (Tiong et al., [2020](#bib.bib103))
    implements a multi-feature fusion convolutional neural network for multimodal
    facial biometrics, obtaining competing results over several benchmarking datasets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人（Li et al., [2021](#bib.bib54)）提出的一项近期工作实现了一个基于手指的多模态生物识别系统，该系统考虑了一种融合策略，以从不同的手指模式模态中提取相关特征，即手指静脉和指关节印记，在手指识别方法上取得了最先进的结果。Tiong
    等人（Tiong et al., [2020](#bib.bib103)）提出的一项类似工作实现了一个多特征融合卷积神经网络，用于多模态面部生物识别，在多个基准数据集上取得了具有竞争力的结果。
- en: The major advances of multimodal biometrics recognition are described in a systematic
    review presented by Dargan et al. (Dargan and Kumar, [2020](#bib.bib14)). The
    authors provide a literature review concerning unimodal and multimodal biometric
    systems, datasets, feature extraction techniques, classifiers, results, efficiency,
    and reliability. Besides, they also discuss how multimodal biometric approaches
    can be employed to overcome some common challenges faced by unimodal methods,
    such as face, fingerprint, and iris recognition.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态生物识别的主要进展在 Dargan 等人（Dargan and Kumar, [2020](#bib.bib14)）所呈现的系统综述中进行了描述。作者提供了关于单模态和多模态生物识别系统、数据集、特征提取技术、分类器、结果、效率和可靠性的文献综述。此外，他们还讨论了多模态生物识别方法如何用于克服单模态方法面临的一些常见挑战，如面部、指纹和虹膜识别。
- en: 2.1.5\. Other Identifiers
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. 其他标识符
- en: For different reasons, the traits mentioned so far may not be interesting in
    certain environments, either due to the imprecision of the model or the lack of
    specialized equipment for the correct identification. Among some unusual approaches,
    Ragan et al. (Ragan et al., [2016](#bib.bib78)) showed that ear-based biometrics
    has intrinsic characteristics capable of holding unique information from individuals.
    Applications can be observed in smart-phone-based applications¹¹1https://www.popsci.com/article/technology-tested-app-authenticates-you-shape-your-ear.
    Meanwhile, Ramli et al. (Ramli et al., [2016](#bib.bib79)) employed the heartbeat
    rate pattern for biometric identification. The method obtained significant accuracy
    and also has been employed for unlocking electronic devices²²2https://www.extremetech.com/computing/165537-nymi-wristband-turns-your-heartbeat-into-an-electronic-key-that-unlocks-your-devices.
    Similarly, Chen et al. (Chen et al., [2020](#bib.bib9)) offered a Raspberry Pi-based
    system for gesture recognition on an Internet of Things environment. The same
    group of authors (Dewi and Chen, [2019](#bib.bib20)) presented a study comparing
    several machine learning approaches for human activity recognition.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不同的原因，到目前为止提到的特征在某些环境下可能不太有趣，这可能是由于模型的不精确或缺乏用于正确识别的专门设备。在一些不寻常的方法中，Ragan 等人（Ragan
    et al., [2016](#bib.bib78)）展示了基于耳朵的生物特征具有固有的特性，能够保存个人的独特信息。这些应用可以在基于智能手机的应用中观察到¹¹1https://www.popsci.com/article/technology-tested-app-authenticates-you-shape-your-ear。与此同时，Ramli
    等人（Ramli et al., [2016](#bib.bib79)）采用了心率模式进行生物特征识别。该方法取得了显著的准确性，并且也被用于解锁电子设备²²2https://www.extremetech.com/computing/165537-nymi-wristband-turns-your-heartbeat-into-an-electronic-key-that-unlocks-your-devices。类似地，Chen
    等人（Chen et al., [2020](#bib.bib9)）提供了一个基于 Raspberry Pi 的系统，用于在物联网环境中的手势识别。相同的作者组（Dewi
    和 Chen, [2019](#bib.bib20)）展示了一项比较几种机器学习方法进行人类活动识别的研究。
- en: 'As described in Section [2.1.2](#S2.SS1.SSS2 "2.1.2\. Iris Recognition ‣ 2.1\.
    Biometric Identification ‣ 2\. Theoretical Background ‣ Gait Recognition Based
    on Deep Learning: A Survey"), the characteristics of the eye present powerful
    descriptive biometric features, such that iris recognition plays the leading role
    in this context. However, some distinct approaches have been successfully employed
    for the task. The Research developed by Ma et al. (Ma et al., [2019](#bib.bib59)),
    for instance, showed the eyes’ movement pattern while following objects is unique
    in each person and employed such a feature for portable remote authentication
    smart grids. A study proposed by Zehngut et al. (Zehngut et al., [2015](#bib.bib133))
    has shown that it is possible to divide the types of noses between six different
    groups and, combined with specific proportions of the width and height of the
    nose, enable the identification of individuals with accuracy as good as recognition
    by iris. Among the method’s advantages, one can highlight its visibility in almost
    $100\%$ of cases, even when people use glasses or hats. The major disadvantage
    regards plastic surgery, changing features relevant to the correct identification
    of the individual.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.1.2](#S2.SS1.SSS2 "2.1.2\. Iris Recognition ‣ 2.1\. Biometric Identification
    ‣ 2\. Theoretical Background ‣ Gait Recognition Based on Deep Learning: A Survey")节所述，眼睛的特征呈现出强大的描述性生物特征，使得虹膜识别在这一领域中发挥了主导作用。然而，也有一些独特的方法成功地被用于这个任务。例如，Ma
    等人（Ma et al., [2019](#bib.bib59)）的研究表明，跟随物体的眼动模式在每个人中都是独特的，并利用这一特征用于便携式远程身份验证智能网格。Zehngut
    等人（Zehngut et al., [2015](#bib.bib133)）提出的一项研究表明，可以将鼻子的类型分为六个不同的组，并结合鼻子的宽度和高度的特定比例，使得识别个人的准确性与虹膜识别相当。该方法的一个主要优点是几乎在$100\%$的情况下都能显现，即使人们戴眼镜或帽子时也是如此。主要缺点涉及整形手术，它可能改变对个人正确识别至关重要的特征。'
- en: The main drawback regarding image-based biometric systems, e.g., fingerprints
    or irises methods, lies in the susceptibility to malicious attempts to fool the
    system. In short, a simple image could fool such systems, granting access to someone
    else’s information in a bank account, for instance. On the other hand, some more
    sophisticated methods can identify people by fancier characteristics, such as
    the veins configuration (Alariki et al., [2018](#bib.bib2)). The method is more
    robust against fraud than simple image-based techniques since it demands specialized
    equipment for proper identification.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于图像的生物特征识别系统（例如指纹或虹膜方法）相关的主要缺点在于其容易受到恶意尝试欺骗系统的影响。简而言之，一张简单的图像可能欺骗这些系统，从而访问他人的银行账户信息。另一方面，一些更复杂的方法可以通过更精致的特征（如静脉配置（Alariki
    等，[2018](#bib.bib2)））来识别个人。与简单的基于图像的技术相比，这种方法对欺诈更加稳健，因为它需要专门的设备进行适当的识别。
- en: 2.2\. Deep Learning Approaches Considered for Gait Recognition
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 考虑用于步态识别的深度学习方法
- en: Even though standard machine learning and gait recognition strategies provided
    relatively satisfactory results in the past years, such methods are usually constrained
    to hand-crafted features and limited capacity for learning intrinsic patterns
    in data. In this context, deep learning-based approaches emerged as an elegant
    solution for tackling image/video and sequential problems, among others, also
    revealing themselves as a powerful tool for gait recognition. Thus, this section
    presents the most popular deep learning architectures once employed for gait recognition,
    namely Convolutional Neural Networks (LeCun et al., [1998](#bib.bib50)), Recurrent
    Neural Networks (Jun et al., [2020](#bib.bib44)), Autoencoders (Babaee et al.,
    [2019](#bib.bib6)), Capsule Networks (Sabour et al., [2017](#bib.bib83)), Generative
    Adversarial Networks (Goodfellow et al., [2014](#bib.bib24)), and Deep Belief
    Networks (Hinton et al., [2006](#bib.bib32)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标准的机器学习和步态识别策略在过去几年中提供了相对令人满意的结果，但这些方法通常受到手工制作特征的限制，并且在学习数据的内在模式方面能力有限。在这种情况下，基于深度学习的方法作为解决图像/视频和序列问题等的优雅解决方案出现，同时也展现了作为步态识别的强大工具。因此，本节介绍了曾用于步态识别的最流行的深度学习架构，即卷积神经网络（LeCun
    等，[1998](#bib.bib50)）、递归神经网络（Jun 等，[2020](#bib.bib44)）、自编码器（Babaee 等，[2019](#bib.bib6)）、胶囊网络（Sabour
    等，[2017](#bib.bib83)）、生成对抗网络（Goodfellow 等，[2014](#bib.bib24)）和深度置信网络（Hinton 等，[2006](#bib.bib32)）。
- en: 2.2.1\. Convolutional Neural Networks
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. 卷积神经网络
- en: 'Convolutional Neural Networks (LeCun et al., [1998](#bib.bib50)) obtained exceptional
    popularity since the beginning of the 2010s, becoming paramount to solve image
    processing-related problems, such as image classification (Tan and Le, [2019](#bib.bib102))
    and segmentation (Zoph et al., [2020](#bib.bib140)). As the name suggests, CNNs
    basic blocks are composed of convolutional neurons, usually composed of $3\times
    3$ or $5\times 5$ kernels, which are responsible for performing convolutional
    operations over the input data. Briefly speaking, this process’s output generates
    a new set of matrices, which are then employed as the subsequent layers of the
    model. In signal processing, convolution is described as a multiplication of two
    signals to generate a third one (Oppenheim et al., [2001](#bib.bib73)). Figure [1](#S2.F1
    "Figure 1 ‣ 2.2.1\. Convolutional Neural Networks ‣ 2.2\. Deep Learning Approaches
    Considered for Gait Recognition ‣ 2\. Theoretical Background ‣ Gait Recognition
    Based on Deep Learning: A Survey") depicts the idea.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积神经网络（LeCun 等，[1998](#bib.bib50)）自2010年代初期以来获得了极大的普及，成为解决图像处理相关问题（如图像分类（Tan
    和 Le，[2019](#bib.bib102)）和分割（Zoph 等，[2020](#bib.bib140)））的关键。顾名思义，CNN 的基本模块由卷积神经元组成，通常由
    $3\times 3$ 或 $5\times 5$ 核心构成，负责对输入数据执行卷积操作。简而言之，这个过程的输出会生成一组新的矩阵，这些矩阵随后作为模型的后续层使用。在信号处理领域，卷积被描述为两个信号的乘法生成第三个信号（Oppenheim
    等，[2001](#bib.bib73)）。图 [1](#S2.F1 "Figure 1 ‣ 2.2.1\. Convolutional Neural Networks
    ‣ 2.2\. Deep Learning Approaches Considered for Gait Recognition ‣ 2\. Theoretical
    Background ‣ Gait Recognition Based on Deep Learning: A Survey") 展示了这一概念。'
- en: <svg   height="178.45" overflow="visible" version="1.1" width="518.82"><g transform="translate(0,178.45)
    matrix(1 0 0 -1 0 0) translate(101.01,0) translate(0,89.23)"><g stroke="#000000"
    fill="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -96.4
    -84.61)"><g transform="matrix(1 0 0 -1 0 157.14)"><g  transform="matrix(1 0 0
    1 0 12.09)"><g  transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    75.23 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41 -2.89)"
    fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1_{\times 1}$</foreignobject></g></g><g transform="matrix(1
    0 0 -1 109.58 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41
    -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0_{\times 0}$</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 141.64 0) translate(12.51,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -8.41 -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82"
    height="12.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0_{\times
    1}$</foreignobject></g></g><g transform="matrix(1 0 0 -1 169.41 0) translate(11.3,0)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g
    transform="matrix(1 0 0 1 0 36.27)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 75.23 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41
    -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1_{\times 0}$</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    109.58 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41 -2.89)"
    fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1_{\times 1}$</foreignobject></g></g><g transform="matrix(1
    0 0 -1 141.64 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41
    -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0_{\times 0}$</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 169.41 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g
    transform="matrix(1 0 0 1 0 60.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 75.23 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41
    -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1_{\times 1}$</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    109.58 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41 -2.89)"
    fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1_{\times 0}$</foreignobject></g></g><g transform="matrix(1
    0 0 -1 141.64 0) translate(12.51,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -8.41
    -2.89)" fill="#000000" stroke="#000000"><foreignobject width="16.82" height="12.06"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1_{\times 1}$</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 169.41 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g
    transform="matrix(1 0 0 1 0 84.62)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0
    0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 73.26 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    105.31 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g> <g  transform="matrix(1
    0 0 -1 139.67 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    169.41 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 108.79)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g>
    <g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0
    -1 105.31 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g> <g  transform="matrix(1
    0 0 -1 139.67 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    169.41 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 132.97)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g>
    <g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0
    -1 105.31 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g> <g  transform="matrix(1
    0 0 -1 139.67 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    169.41 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 157.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g transform="matrix(1
    0 0 -1 73.26 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    105.31 0) translate(16.79,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -12.68 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="18.45" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g> <g  transform="matrix(1
    0 0 -1 139.67 0) translate(14.48,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -10.38
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="13.84" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0  </foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    169.41 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 124.94 -4.64)" fill="#000000" stroke="#000000"><foreignobject width="9.96"
    height="9.27" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\ast$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 161.37 -36.26)"><g  transform="matrix(1 0 0
    -1 0 60.44)"><g  transform="matrix(1 0 0 1 0 12.09)"><g transform="matrix(1 0
    0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 47.56 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></g><g
    transform="matrix(1 0 0 1 0 36.27)"><g transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 60.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></g></g></g><g transform="matrix(1.0
    0.0 0.0 1.0 252.09 -3.66)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="7.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$=$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 295.47 -60.44)"><g  transform="matrix(1 0 0
    -1 0 108.79)"><g  transform="matrix(1 0 0 1 0 12.09)"><g transform="matrix(1 0
    0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 47.56 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    70.95 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g><g transform="matrix(1
    0 0 -1 94.34 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></g><g
    transform="matrix(1 0 0 1 0 36.27)"><g transform="matrix(1 0 0 -1 0 0) translate(12.09,0)"><g
    transform="matrix(1.0 0.0 0.0 1.0 -1.92 -4.8)" fill="#000000" stroke="#000000"><foreignobject
    width="3.84" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">l</foreignobject></g></g><g
    transform="matrix(1 0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0
    0.0 0.0 1.0 -3.46 -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    94.34 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 60.44)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">4</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    94.34 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 84.62)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    94.34 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g></g><g transform="matrix(1
    0 0 1 0 108.79)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0) translate(12.09,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g transform="matrix(1
    0 0 -1 24.18 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">3</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    47.56 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g transform="matrix(1
    0 0 -1 70.95 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46
    -4.46)" fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">1</foreignobject></g></g><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    94.34 0) translate(11.3,0)"><g transform="matrix(1.0 0.0 0.0 1.0 -3.46 -4.46)"
    fill="#000000" stroke="#000000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">0</foreignobject></g></g></g></g></g></g></g></svg>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. An example of $3\times 3$ convolution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. $3\times 3$ 卷积示例。
- en: 'Therefore, a CNN can be interpreted as a stack of convolutional kernels generating
    the successive layers’ input. Notice some intermediate pooling layers may be included
    in the process, and a fully connected layer is coupled at the top of the architecture
    for classification purposes. The learning process is performed by the backpropagation
    algorithm working with a gradient descent calculation. Figure [2](#S2.F2 "Figure
    2 ‣ 2.2.1\. Convolutional Neural Networks ‣ 2.2\. Deep Learning Approaches Considered
    for Gait Recognition ‣ 2\. Theoretical Background ‣ Gait Recognition Based on
    Deep Learning: A Survey"), inspired in LeNet-5, pictures a CNN architecture.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CNN 可以被解释为一个卷积核堆栈，生成连续层的输入。请注意，过程中可能包括一些中间池化层，并且最上层附有一个全连接层用于分类目的。学习过程由反向传播算法执行，该算法使用梯度下降计算。图
    [2](#S2.F2 "图 2 ‣ 2.2.1. 卷积神经网络 ‣ 2.2. 深度学习方法用于步态识别 ‣ 2. 理论背景 ‣ 基于深度学习的步态识别：综述")，受到
    LeNet-5 启发，展示了一个 CNN 架构。
- en: '![Refer to caption](img/ec4014500c993041a60b25313531bdda.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ec4014500c993041a60b25313531bdda.png)'
- en: Figure 2\. A standard CNN architecture’s example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 标准 CNN 架构示例。
- en: 2.2.2\. Capsule Networks
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2. 胶囊网络
- en: Even though CNNs work very well for image features’ understanding, they are
    prone to confuse spatial relationships between complex peculiarities. In other
    words, a trained CNN is usually capable of recognizing dogs if it finds a dog’s
    body, face, tail, and so on, even if the dog is assembled in a different sequence
    or if its parts are located in distinct sections of the image. On the other hand,
    Capsule Networks (Sabour et al., [2017](#bib.bib83)) consider a hierarchical approach
    to tackle this problem. In short, the model comprises a two-layer structure. The
    first is a convolutional encoder, which performs the recognition of low-level
    features. The second stands for a fully-connected linear decoder that employs
    the routing by agreement algorithm (Sabour et al., [2017](#bib.bib83)) to address
    such low-level features to the correct position in a hierarchical higher-level.
    Therefore, CapsNets are more robust to object orientation. Besides, they may perform
    better for identifying multiple or overlapping objects in a scene.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CNN 在图像特征理解方面表现良好，但它们容易混淆复杂特征之间的空间关系。换句话说，一个经过训练的 CNN 通常能够识别狗的身体、脸、尾巴等，即使狗的各个部分在图像中的排列顺序不同或位于不同的区域。另一方面，胶囊网络（Sabour
    等，[2017](#bib.bib83)）考虑了一种分层的方法来解决这个问题。简而言之，该模型包含一个两层结构。第一层是卷积编码器，执行低级特征的识别。第二层是全连接线性解码器，利用协议路由算法（Sabour
    等，[2017](#bib.bib83)）将这些低级特征分配到分层的高级正确位置。因此，CapsNets 对物体方向更具鲁棒性。此外，它们在识别场景中的多个或重叠物体时可能表现更好。
- en: 2.2.3\. Recurrent Neural Networks
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3. 递归神经网络
- en: Many works addressed the gait recognition problem as a sequence of images defining
    the individual’s movement. A standard method to acknowledge such strategy using
    deep learning concerns Recurrent Neural Networks (RNNs) (Jun et al., [2020](#bib.bib44)),
    which compute each neuron’s activation considering the information from the input
    data, as well as other neuron’s output, in a recurrent fashion. Occasionally,
    the architecture is combined with a CNN to extract more information about the
    input images to perform the inference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究将步态识别问题视为定义个体运动的图像序列。使用深度学习来认识这种策略的标准方法涉及递归神经网络（RNNs）（Jun 等，[2020](#bib.bib44)），这些网络在递归的方式下计算每个神经元的激活，考虑输入数据的信息以及其他神经元的输出。有时，架构会与
    CNN 结合，以提取更多输入图像的信息进行推理。
- en: Since describing a person’s gait usually requires a considerable amount of sequential
    features, a particular set of RNNs, namely gated RNN, are more suitable for the
    task due to their abilities to deal with long sequences. In this context, one
    can refer to two main architectures, i.e., the Long-Short Term Memory (Hochreiter
    and Schmidhuber, [1997](#bib.bib33)) and the Gated Recurrent Unit (GRU) (Chung
    et al., [2014](#bib.bib11)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于描述一个人的步态通常需要大量的连续特征，一种特定的 RNN，即门控 RNN，更适合这个任务，因为它们能处理长序列。在这种情况下，可以参考两种主要架构，即长短期记忆（Hochreiter
    和 Schmidhuber，[1997](#bib.bib33)）和门控循环单元（GRU）（Chung 等，[2014](#bib.bib11)）。
- en: (1)
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Long-Short Term Memory:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长短期记忆：
- en: 'The Long-Short Term Memory was first implemented in 1997 by Hochreiter and
    Schmidhuber (Hochreiter and Schmidhuber, [1997](#bib.bib33)), where the main objective
    was to improve results on long sequences of data. In a nutshell, LSTMs work similarly
    to the traditional RNN, i.e., the output of a given neuron depends on recurrent
    information from previous neurons’ outcomes. The main difference lies in the LSTM
    cell’s architecture, which comprises more complex and fancy relationships. Such
    an architecture is composed of three main gates that control the flow of information,
    described as follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）最早由Hochreiter和Schmidhuber于1997年实现（Hochreiter and Schmidhuber, [1997](#bib.bib33)），其主要目标是提高长序列数据上的结果。简而言之，LSTM的工作原理类似于传统RNN，即给定神经元的输出依赖于之前神经元结果的递归信息。主要区别在于LSTM单元的架构，该架构包含更复杂的关系。该架构由三个主要门控组成，用于控制信息流动，具体描述如下：
- en: •
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The forget gate: this gate defines how much of the information should be kept.
    The previous and current state data is passed through a sigmoid function, which
    outputs values between $0$ and $1$. The closer to $1$, the more information is
    preserved.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遗忘门：该门定义应保留多少信息。先前和当前状态数据通过sigmoid函数输出值在$0$和$1$之间。越接近$1$，保留的信息越多。
- en: •
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The input gate: computes a new value to update the current hidden state. The
    input gate considers two central values: (i) a sigmoid function calculates the
    importance of the previously hidden state, and (ii) the original value is forwarded
    to a hyperbolic tangent (tanh) function, which is responsible for squishing this
    value between $-1$ and $1$. The multiplication of these two values defines the
    current hidden state.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入门：计算一个新值以更新当前隐藏状态。输入门考虑两个主要值：（i）sigmoid函数计算之前隐藏状态的重要性，（ii）原始值被提交给双曲正切（tanh）函数，该函数负责将该值压缩在$-1$和$1$之间。这两个值的乘积定义当前隐藏状态。
- en: •
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The output gate: after estimating the forget and the input gates, the output
    gate defines the cell’s output value. The process is performed as follows: (i)
    the values from the forget and the input gate are summed up and submitted to a
    tahn function, (ii) the cell’s previous state is submitted to a sigmoid function,
    (iii) the output from both the sigmoid and the tahn functions are multiplied,
    yilding the cell’s output.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出门：在估计遗忘门和输入门之后，输出门定义单元的输出值。过程如下：（i）遗忘门和输入门的值相加并提交给tanh函数，（ii）单元的先前状态提交给sigmoid函数，（iii）sigmoid函数和tanh函数的输出相乘，得到单元的输出。
- en: (2)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Gated Recurrent Unity:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 门控递归单元：
- en: The Gated Recurrent Unit (Cho et al., [2014](#bib.bib10)) is a recurrent neural
    network originally idealized to improve results on neural machine translations.
    Like LSTMs, GRUs possess internal gates that control the flow of information,
    and the main difference lies in the number of gates available in each model, i.e.,
    GRU comprises only two, namely the forget and the output gates, instead of three.
    Studies show that, even though the GRU uses fewer gates than LSTM, it can reach
    similar results (Chung et al., [2014](#bib.bib11)), with the advantage of a reduced
    computational burden and faster performance considering both tasks of training
    and inference.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 门控递归单元（Cho et al., [2014](#bib.bib10)）是一种递归神经网络，最初旨在改进神经机器翻译的结果。与LSTM类似，GRU具有控制信息流动的内部门控，而主要区别在于每个模型中可用门的数量，即GRU仅包含两个门，即遗忘门和输出门，而不是三个。研究表明，尽管GRU使用的门比LSTM少，但它能够达到类似的结果（Chung
    et al., [2014](#bib.bib11)），并且在训练和推理任务中具有减少计算负担和更快性能的优势。
- en: 2.2.4\. Autoencoders
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 自编码器
- en: 'Autoencoders (Vincent et al., [2010](#bib.bib109)) are generative neural networks
    usually employed for data reduction and image denoising. The model comprises two
    main steps, described as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（Vincent et al., [2010](#bib.bib109)）是通常用于数据降维和图像去噪的生成神经网络。该模型包含两个主要步骤，如下所述：
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Encoder: it is responsible for encoding the input information into an, usually,
    smaller feature space.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器：负责将输入信息编码为通常较小的特征空间。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Decoder: it performs the unsupervised reconstruction of encoded data.'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器：对编码数据进行无监督的重建。
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.2.4\. Autoencoders ‣ 2.2\. Deep Learning Approaches
    Considered for Gait Recognition ‣ 2\. Theoretical Background ‣ Gait Recognition
    Based on Deep Learning: A Survey") depicts the architecture of the model.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S2.F3 "图 3 ‣ 2.2.4\. 自编码器 ‣ 2.2\. 深度学习方法在步态识别中的应用 ‣ 2\. 理论背景 ‣ 基于深度学习的步态识别：综述")
    展示了模型的架构。
- en: '![Refer to caption](img/6049653b5f0d2ff2d44d8631332d5301.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6049653b5f0d2ff2d44d8631332d5301.png)'
- en: Figure 3\. A standard Autoencoder architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 标准自编码器架构。
- en: 2.2.5\. Deep Belief Networks
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5\. 深度置信网络
- en: Deep Belief Networks (Hinton et al., [2006](#bib.bib32)) are stochastic neural
    networks ideally designed for generative tasks, such that each layer denotes a
    greedy-fashioned trained Restricted Boltzmann Machine (RBM) (Hinton et al., [2006](#bib.bib32)).
    In short, RBM is a graphical model composed of a visible and a latent set of units,
    namely visible and hidden layers, respectively, which are connected by a weight
    matrix with no connection among neurons from the same layer. The model’s learning
    approach consists of attaching a collection of input data into the visible layer
    and finding a representation of this data in the hidden units. Besides, the training
    procedure is performed by minimizing the system’s energy, usually conducted using
    a Markov chain procedure through Gibbs sampling for optimization purposes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 深度置信网络 (Hinton 等，[2006](#bib.bib32)) 是一种理想用于生成任务的随机神经网络，每一层代表一个贪婪训练的限制玻尔兹曼机
    (RBM) (Hinton 等，[2006](#bib.bib32))。简而言之，RBM 是一个图形模型，由可见层和隐含层组成，这两层通过一个权重矩阵连接，同一层内的神经元之间没有连接。该模型的学习方法包括将一组输入数据输入到可见层，并在隐含单元中找到这些数据的表示。此外，训练过程通过最小化系统的能量来完成，通常使用马尔可夫链程序通过
    Gibbs 采样进行优化。
- en: Concerning classification tasks, the most common approach considers coupling
    a softmax layer at the top of the DBN architecture and, after greedily pre-training
    all the RBMs, performing a fine-tuning in the weights using Backpropagation, for
    instance, to adjust the weight matrices and fitting the labels for proper identification.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分类任务，最常见的方法是在 DBN 架构的顶部添加一个 softmax 层，在贪婪地预训练所有 RBM 之后，使用反向传播等方法对权重进行微调，以调整权重矩阵并使标签适合正确的识别。
- en: 2.2.6\. Generative Adversarial Networks
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6\. 生成对抗网络
- en: 'Generative Adversarial Networks (Goodfellow et al., [2014](#bib.bib24)) became
    popular in the last years due to their outstanding ability to generate realistic
    synthetic images. The model comprises two distinct networks, i.e., a generator,
    which is responsible for learning the data’s distribution and generating synthetic
    samples, and a discriminator, which tries to identify whether a given instance
    is original or synthetically created. The generator and the discriminator compete
    in an adversarial fashion such that the generator attempts to generate samples
    realistic enough to fool the discriminator. In contrast, the discriminator improves
    itself more and more to recognize such fake images. Figure [4](#S2.F4 "Figure
    4 ‣ 2.2.6\. Generative Adversarial Networks ‣ 2.2\. Deep Learning Approaches Considered
    for Gait Recognition ‣ 2\. Theoretical Background ‣ Gait Recognition Based on
    Deep Learning: A Survey") illustrates the model.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络 (Goodfellow 等，[2014](#bib.bib24)) 在近年来因其生成逼真的合成图像的杰出能力而变得流行。该模型包括两个不同的网络，即生成器，负责学习数据的分布并生成合成样本，以及判别器，尝试识别给定实例是否为原始的或合成的。生成器和判别器以对抗的方式竞争，使生成器尝试生成足够逼真的样本以欺骗判别器。相反，判别器不断提升自己，以识别这些虚假的图像。图
    [4](#S2.F4 "图 4 ‣ 2.2.6\. 生成对抗网络 ‣ 2.2\. 深度学习方法在步态识别中的应用 ‣ 2\. 理论背景 ‣ 基于深度学习的步态识别：综述")
    说明了该模型。
- en: '![Refer to caption](img/1857058b17303e46edfb577955c6fcf0.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1857058b17303e46edfb577955c6fcf0.png)'
- en: Figure 4\. A standard GAN architecture.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 标准 GAN 架构。
- en: 2.2.7\. Deep learning techniques’ summary
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.7\. 深度学习技术总结
- en: Table LABEL:t.architectures presents a summary concerning the deep learning
    techniques considered in this work and how they are usually employed for gait
    recognition tasks. Notice that the same work can eventually use distinct architectures,
    thus appearing more than once in the table.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:t.architectures 总结了本工作中考虑的深度学习技术及其在步态识别任务中的常见应用。请注意，同一工作可能会使用不同的架构，因此在表中可能出现多次。
- en: Table 1\. Deep learning-based gait recognition approaches organized byt type
    of neural network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 基于深度学习的步态识别方法，按神经网络类型组织。 |
- en: '| References | Technique | Task |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 技术 | 任务 |'
- en: '| --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| (Shiraga et al., [2016](#bib.bib87); Wu et al., [2017a](#bib.bib117); Li
    et al., [2017](#bib.bib53); Sokolova and Konushin, [2017a](#bib.bib91); Takemura
    et al., [2017](#bib.bib99); Sokolova and Konushin, [2017b](#bib.bib92); Yu et al.,
    [2017](#bib.bib128); Zou et al., [2018a](#bib.bib141); Wang and Yan, [2020](#bib.bib116);
    Xu et al., [2020](#bib.bib121)) | Convolutional Neural Networks (CNN) | Feature
    extraction from images or frames of a video. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| (Shiraga et al., [2016](#bib.bib87); Wu et al., [2017a](#bib.bib117); Li
    et al., [2017](#bib.bib53); Sokolova and Konushin, [2017a](#bib.bib91); Takemura
    et al., [2017](#bib.bib99); Sokolova and Konushin, [2017b](#bib.bib92); Yu et
    al., [2017](#bib.bib128); Zou et al., [2018a](#bib.bib141); Wang and Yan, [2020](#bib.bib116);
    Xu et al., [2020](#bib.bib121)) | 卷积神经网络 (CNN) | 从图像或视频帧中提取特征。 |'
- en: '| (Zhang et al., [2019](#bib.bib134); Babaee et al., [2019](#bib.bib6)) | Auto
    Encoder | Works by compressing and decompressing features from the input. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang et al., [2019](#bib.bib134); Babaee et al., [2019](#bib.bib6)) | 自编码器
    | 通过压缩和解压输入的特征来工作。 |'
- en: '| (Xu et al., [2019](#bib.bib123); Sepas-Moghaddam et al., [2021](#bib.bib86);
    Zhao et al., [2021](#bib.bib135)) | Capsule | Improve the semantic organization
    of the outputs from a CNN. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| (Xu et al., [2019](#bib.bib123); Sepas-Moghaddam et al., [2021](#bib.bib86);
    Zhao et al., [2021](#bib.bib135)) | 胶囊网络 | 改善来自CNN的输出的语义组织。 |'
- en: '| (Fernandes et al., [2018](#bib.bib22); Xiong et al., [2020](#bib.bib120))
    | Deep Belief Networks | Encode features and patterns into compressed representations.
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| (Fernandes et al., [2018](#bib.bib22); Xiong et al., [2020](#bib.bib120))
    | 深度信念网络 | 将特征和模式编码成压缩表示。 |'
- en: '| (He et al., [2018](#bib.bib29); Jia et al., [2019](#bib.bib42); Hu et al.,
    [2018](#bib.bib36)) | Generative Adversarial Networks | A training method that
    relies on the differentiaton of an original input and a generate counterpart from
    a model, such as a CNN. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| (He et al., [2018](#bib.bib29); Jia et al., [2019](#bib.bib42); Hu et al.,
    [2018](#bib.bib36)) | 生成对抗网络 | 一种训练方法，依赖于将原始输入和来自模型的生成对手进行区分，例如CNN。 |'
- en: '| (Zou et al., [2018a](#bib.bib141); Zhang et al., [2019](#bib.bib134); Potluri
    et al., [2019](#bib.bib77); Wang and Yan, [2020](#bib.bib116); Tran et al., [2021](#bib.bib104))
    | Recurrent Neural Networks | Comprise both GRUs and LSTMs, which are composed
    with several gates to controle the flow of information and are employed to deal
    with temporal information. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (Zou et al., [2018a](#bib.bib141); Zhang et al., [2019](#bib.bib134); Potluri
    et al., [2019](#bib.bib77); Wang and Yan, [2020](#bib.bib116); Tran et al., [2021](#bib.bib104))
    | 循环神经网络 | 包括GRU和LSTM，它们由多个门组成以控制信息流，并用于处理时间信息。 |'
- en: 2.3\. Gait Recognition
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 步态识别 |
- en: 'Several methods for people recognition through biological characteristics have
    been presented so far. Although the methods’ reliability and safety are confirmed
    by their success in banks and public governance systems, two main hindrances must
    be stressed: (i) they depend on the passive provision of personal biometric information,
    i.e., the person must provide or register the required information for the recognition;
    and (ii) some of these systems rely on specialized equipment.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，已经提出了多种基于生物特征的人体识别方法。虽然这些方法的可靠性和安全性已通过银行和公共治理系统的成功得到验证，但必须强调两个主要障碍：（i）它们依赖于被动提供个人生物信息，即人们必须提供或注册所需的识别信息；（ii）其中一些系统依赖于专业设备。
    |
- en: An alternative to deal with such drawbacks may comprise gait recognition models,
    especially considering video-based approaches. Such methods do not suffer from
    the problems mentioned above, once the acquisition of the biometric information
    depends, most of the time, only on a camera with no specific features or non-evasive
    sensors and devices, and, disregarding legal problems, the collection of such
    information is performed passively. Therefore, the observed person is no longer
    required to cooperating in the identification process or providing any information.
    On the other hand, although video-based recognition is the most intuitive method,
    it does not stand for the only possibility. Gait identification can also be performed
    by distinct characteristics, e.g., the footprint (Costilla-Reyes et al., [2018](#bib.bib12)),
    which considers the pressure and size of the area. The obvious disadvantage of
    such models is the cost of deploying the equipment necessary to acquire such features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些缺点的一个替代方法可能包括步态识别模型，特别是考虑基于视频的方法。这些方法不受上述问题的影响，因为生物特征信息的获取大多数情况下仅依赖于普通摄像头，而不需要特定功能的或非侵入式的传感器和设备，且在忽略法律问题的前提下，信息的收集是被动的。因此，观察者不再需要参与识别过程或提供任何信息。另一方面，虽然基于视频的识别是最直观的方法，但这并不是唯一的选择。步态识别还可以通过其他特征来进行，例如足迹（Costilla-Reyes
    等，[2018](#bib.bib12)），该方法考虑了压力和面积的大小。这些模型的明显缺点是部署所需设备的成本。
- en: Moreover, techniques proposed for gait recognition can be divided into two main
    groups, i.e., template- and non-template-based methods. Template-based methods
    aim to obtain the movement of the trunk or legs, i.e., they usually focus on the
    dynamics of movement through space or spatio-temporal based methods (Yeo and Park,
    [2020](#bib.bib125)). Among such techniques, one can refer to Walking Path Image
    (WPI) information (Zhao et al., [2016](#bib.bib136)), Gait Information Image (GII) (Arora
    et al., [2015](#bib.bib5)), and Gait Energy Image (GEI) features, which can be
    extracted through Canonical Correlation Analysis (Luo and Tjahjadi, [2020](#bib.bib57)),
    Joint Sparsity Models (Yogarajah et al., [2015](#bib.bib126)), segmentation using
    Group Lasso Motion (Rida et al., [2016](#bib.bib81)), among others (Shiraga et al.,
    [2016](#bib.bib87); Li et al., [2017](#bib.bib53); Wu et al., [2017b](#bib.bib118)).
    On the other hand, non-template based methods consider the shape and its attributes
    as the more relevant characteristics, i.e., the recognition of the individual
    is performed with measurements that reflect its shape (Deng et al., [2018a](#bib.bib17)).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，步态识别的方法可以分为两大类，即基于模板的方法和非基于模板的方法。基于模板的方法旨在获取躯干或腿部的运动，即它们通常关注空间中的运动动态或时空基的方法（Yeo
    和 Park，[2020](#bib.bib125)）。在这些技术中，可以参考步态路径图像（WPI）信息（Zhao 等，[2016](#bib.bib136)），步态信息图像（GII）（Arora
    等，[2015](#bib.bib5)），以及通过典型相关分析（Luo 和 Tjahjadi，[2020](#bib.bib57)）、联合稀疏模型（Yogarajah
    等，[2015](#bib.bib126)）、使用 Group Lasso Motion 进行的分割（Rida 等，[2016](#bib.bib81)）等提取的步态能量图像（GEI）特征（Shiraga
    等，[2016](#bib.bib87)；Li 等，[2017](#bib.bib53)；Wu 等，[2017b](#bib.bib118)）。另一方面，非基于模板的方法则将形状及其属性视为更相关的特征，即通过反映其形状的测量来进行个体识别（Deng
    等，[2018a](#bib.bib17)）。
- en: 'Regarding the process of gait information acquisition, it may comprise several
    sorts of sensors and devices, as illustrated in Figure [5](#S2.F5 "Figure 5 ‣
    2.3\. Gait Recognition ‣ 2\. Theoretical Background ‣ Gait Recognition Based on
    Deep Learning: A Survey"). The image depicts a laboratory-like environment designed
    with several embedded devices for gait data acquisition, such as cameras, which
    are the most commonly employed device for the task since they are quite accessible
    and are capable of acquiring data over greater distances, and multispectral photographic
    sensors that are very efficient, but with less accessible due to high costs. The
    image also comprises gyroscope and velocity sensors on the roof, responsible for
    describing the spatio-temporal context acquisition. Finally, the walking rhythm
    is measured using a step-pressure carpet-like sensor, which is highly valuable
    to detecte gait abnormalities, such as the ones caused by Parkinson’s disease
    and other neurodegenerative problems.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步态信息采集过程，它可能包含几种不同的传感器和设备，如图 [5](#S2.F5 "图 5 ‣ 2.3\. 步态识别 ‣ 2\. 理论背景 ‣ 基于深度学习的步态识别：综述")所示。图像展示了一个类似实验室的环境，配备了多个嵌入式设备用于步态数据采集，例如相机，这是最常用的设备，因为它们比较便捷，并且能够在较远距离上采集数据，以及多光谱摄影传感器，虽然效率很高，但由于成本高而不易获得。图像中还包括屋顶上的陀螺仪和速度传感器，负责描述时空上下文的获取。最后，步态节奏通过类似地毯的压力传感器进行测量，这对于检测步态异常（如帕金森病和其他神经退行性问题造成的异常）非常有价值。
- en: '![Refer to caption](img/6fd92f5564985b71045c17ae7e11433f.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6fd92f5564985b71045c17ae7e11433f.png)'
- en: 'Figure 5\. Example of gait data acquisition environment. The scene comprises
    the following devices: sidewall cameras, followed by a multi-spectrum photographic
    sensor. The roof accommodates velocity sensors and gyroscopes, which measure the
    space-time relationship during the walk. Finally, the floor is equipped with a
    carpet-like sensor for collecting the foot pressure on the ground, completing
    the gait cycle.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 步态数据采集环境的示例。场景包含以下设备：侧壁摄像头，随后是多光谱摄影传感器。屋顶上安装有速度传感器和陀螺仪，用于测量行走过程中的时空关系。最后，地板上配备了类似地毯的传感器，用于收集脚对地面的压力，完成步态周期。
- en: 'Despite the success obtained by the methods mentioned above, deep learning-based
    approaches induced a paradigm shift in the field of gait recognition, obtaining
    paramount results in a variety of applications. Therefore, the next section introduces
    an in-depth presentation of several surveyed works in the context of deep learning-based
    approaches for gait recognition. Figure [6](#S2.F6 "Figure 6 ‣ 2.3\. Gait Recognition
    ‣ 2\. Theoretical Background ‣ Gait Recognition Based on Deep Learning: A Survey")
    provides a schematic diagram comprising an overview of the main differences between
    deep learning and the standard methods for gait recognition.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述方法取得了成功，但基于深度学习的方法在步态识别领域引发了范式转变，在各种应用中取得了极其重要的成果。因此，下一节将深入介绍在基于深度学习的步态识别方法中的几个调研工作。图 [6](#S2.F6
    "图 6 ‣ 2.3\. 步态识别 ‣ 2\. 理论背景 ‣ 基于深度学习的步态识别：综述")提供了一个示意图，概述了深度学习与标准步态识别方法之间的主要差异。
- en: '![Refer to caption](img/f7c691dcfe1136967eae16f439ee27db.png)![Refer to caption](img/c92b4070e1d8098361313512341b06eb.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f7c691dcfe1136967eae16f439ee27db.png)![参见说明](img/c92b4070e1d8098361313512341b06eb.png)'
- en: Figure 6\. Schematic diagrams from deep learning (top) and standard (bottom)
    gait recognition pipelines. As the image suggests, deep learning approaches abstract
    the steps concerning data pre-processing, feature extraction, and classification
    in a single architecture.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 深度学习（上）和标准（下）步态识别流程的示意图。正如图像所示，深度学习方法在单一架构中抽象了数据预处理、特征提取和分类的步骤。
- en: 3\. Gait Recognition Through Deep Learning-Based Approaches
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 基于深度学习的方法进行步态识别
- en: This section presents a systematic review of recent works using distinct deep
    learning architectures, i.e., Convolutional Neural Networks, Recurrent Neural
    Networks, Generative Adversarial Networks, Deep Belief Networks, and Autoencoder-based
    approaches for gait recognition. Besides, it also presents a discussion regarding
    the compared methods and a summary of the surveyed studies.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 本节系统地回顾了使用不同深度学习架构（即卷积神经网络、递归神经网络、生成对抗网络、深度信念网络和基于自编码器的方法）进行步态识别的最新工作。此外，还讨论了比较的方法和调研研究的总结。
- en: 3.1\. Convolutional Neural Networks
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 卷积神经网络
- en: Convolutional Neural Networks (LeCun et al., [1998](#bib.bib50)) use a concept
    of a neuron based on the visual cortex of mammals, which was first validated for
    the task of digit classification and today is probably the most widely employed
    neural network for classification, reconstruction, and object detection, among
    others. In the context of gait recognition, Shiraga et al. (Shiraga et al., [2016](#bib.bib87))
    used an architecture similar to LENet (LeCun et al., [1998](#bib.bib50)) to create
    a gleaming-based recognizer through Gait Energy Image (GEI). The model achieved
    an accuracy of $91.5\%$ in the OU-ISIR (Makihara et al., [2012](#bib.bib60)) Large
    Population dataset. A similar work proposed by Wang et al. (Wang et al., [2019](#bib.bib114))
    employ nonstandard periodic GEI approaches for gait recognition and data augmentation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（LeCun et al., [1998](#bib.bib50)）使用基于哺乳动物视觉皮层的神经元概念，最初被验证用于数字分类任务，如今可能是分类、重建和物体检测等任务中最广泛使用的神经网络。在步态识别的背景下，Shiraga等（Shiraga
    et al., [2016](#bib.bib87)）使用了类似于LENet（LeCun et al., [1998](#bib.bib50)）的架构，通过步态能量图（GEI）创建了一个基于步态的识别器。该模型在OU-ISIR（Makihara
    et al., [2012](#bib.bib60)）大型数据集上取得了$91.5\%$的准确率。Wang等（Wang et al., [2019](#bib.bib114)）提出的类似研究使用了非标准的周期GEI方法进行步态识别和数据增强。
- en: 'Regarding cross-view gait-based human identification, Wu et al. (Wu et al.,
    [2017a](#bib.bib117)) claim to propose the first work using CNNs in the context
    of grimace recognition. The authors employed several network architectures and
    demonstrated the power of CNNs in the context of biometric-based identification,
    obtaining gains above $10\%$ to previous results. Besides, they also compared
    three types of data arrangement, described as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于跨视角步态的人体识别，吴等（Wu et al., [2017a](#bib.bib117)）声称提出了在面部表情识别背景下使用CNN的首个研究。作者使用了多种网络架构，并展示了CNN在生物特征识别中的强大能力，相较于之前的结果，取得了超过$10\%$的提升。此外，他们还比较了三种数据排列方式，具体描述如下：
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Local Bottom: a combination is made among the input data, and then it is determined
    whether the inputs belong to the same or different individuals;'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Local Bottom: 在输入数据之间进行组合，然后确定这些输入是否属于同一个人或不同的人；'
- en: •
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mid-Level Top: the neural network extracts some characteristics of both inputs
    before combining them and then determining whether or not they come from the same
    person; and'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Mid-Level Top: 神经网络在将输入数据组合之前，提取一些特征，然后判断这些输入是否来自同一个人；'
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Global Top: similar to the previous network. However, it has an extra level
    of convolutions and Perceptrons, such that the combination of characteristics
    is made in the penultimate layer.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Global Top: 类似于之前的网络。然而，它有额外的卷积层和感知机层，使得特征的组合在倒数第二层完成。'
- en: Further, Li et al. (Li et al., [2017](#bib.bib53)) proposed the DeepGait, a
    model that combines deep convolutional features and Joint Bayesian for video sensor-based
    gait representation. The model outperformed hand-crafted features, such as GEI,
    Frequency-Domain Feature, and Gait Flow Image, obtaining state-of-the-art results
    over OU-ISR large population dataset (Makihara et al., [2012](#bib.bib60)). Meanwhile,
    a study proposed by Sokolova and Konushin (Sokolova and Konushin, [2017a](#bib.bib91))
    demonstrated the difficulty in identifying people by their behavior due to the
    intersection of information, despite satisfactory results over TUM-GAID database (Hofmann
    et al., [2014](#bib.bib34)), obtaining $97.5\%$ accuracy and $99.89\%$ Rank-5\.
    However, the performance was severely degraded concerning CASIA B dataset (Yu
    et al., [2006](#bib.bib129)), bringing an accuracy of $58.20\%$ over such a scenario,
    demonstrating how this type of biometry is sensitive to the data’s context.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，李等（Li et al., [2017](#bib.bib53)）提出了DeepGait，一个结合深度卷积特征和联合贝叶斯的视频传感器步态表示模型。该模型在OU-ISR大型数据集（Makihara
    et al., [2012](#bib.bib60)）上超越了手工特征，如GEI、频域特征和步态流图像，取得了最先进的结果。同时，Sokolova和Konushin（Sokolova
    and Konushin, [2017a](#bib.bib91)）提出的研究展示了由于信息交叉而导致根据行为识别人的困难，尽管在TUM-GAID数据库（Hofmann
    et al., [2014](#bib.bib34)）上取得了$97.5\%$的准确率和$99.89\%$的Rank-5准确率。然而，在CASIA B数据集（Yu
    et al., [2006](#bib.bib129)）上的表现严重下降，在这种情况下准确率为$58.20\%$，显示了这种生物测定技术对数据背景的敏感性。
- en: 'The same authors proposed a multiple-stage model for gait recognition using
    Optical Flow (OF) (Sokolova and Konushin, [2017b](#bib.bib92)). In this model,
    the data is pre-processed in two main steps: motion map computation and frame-by-frame
    evaluation of the individual’s pose. Further, two distinct neural networks, i.e.,
    VGG-19 (Simonyan and Zisserman, [2014](#bib.bib89)) and Wide Residual Network
    (WRN) (Zagoruyko and Komodakis, [2016](#bib.bib132)), are employed to validate
    the technique in the context of video-based gait recognition. To increase classification
    speed, they incorporated the Principal Component Analysis (PCA) for dimensionality
    reduction. Such information goes through the normalization L2, and finally, the
    features produced are fed to a Nearest Neighbor (NN) classifier. Using this combination
    of methods, they achieved very accurate results over TUM-GAID (Hofmann et al.,
    [2014](#bib.bib34)), CASIA B (Yu et al., [2006](#bib.bib129)), and OU-ISIR (Makihara
    et al., [2012](#bib.bib60)) datasets.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的作者提出了一种多阶段的步态识别模型，该模型使用光流（OF）（Sokolova和Konushin，[2017b](#bib.bib92)）。在这个模型中，数据经过两个主要步骤的预处理：运动图的计算和逐帧评估个体的姿势。此外，使用了两个不同的神经网络，即VGG-19（Simonyan和Zisserman，[2014](#bib.bib89)）和Wide
    Residual Network（WRN）（Zagoruyko和Komodakis，[2016](#bib.bib132)），以在基于视频的步态识别背景下验证该技术。为了提高分类速度，他们结合了主成分分析（PCA）进行降维。这些信息经过L2归一化处理，最后，生成的特征输入到最近邻（NN）分类器中。通过这种方法的组合，他们在TUM-GAID（Hofmann等，[2014](#bib.bib34)）、CASIA
    B（Yu等，[2006](#bib.bib129)）和OU-ISIR（Makihara等，[2012](#bib.bib60)）数据集上取得了非常准确的结果。
- en: 'Recently, an architecture called Siamese Networks, which accepts two separate
    entries as input and computes a similarity value between them, was successfully
    employed for similar tasks, such as identifying obstructed routes or misbehavior
    in hazardous environments (Santana et al., [2019](#bib.bib84)). In this context,
    Takemura et al. (Takemura et al., [2017](#bib.bib99)) implemented four different
    frameworks to create a biometric identification system. All networks rely on GEI
    to make the comparisons. Two of these networks were developed based on the Triplet
    Ranking Loss, which necessarily needs three inputs for execution: (i) the information
    of the compared person, (ii) data from the same person, and (iii) an entrance
    from any other person. The other two networks use contrastive loss in their execution,
    whose entries depend only on the person’s information for comparison. The authors
    obtained $91.9\%$ of accuracy over the OU-ISIR Multi-view Large Population (OU-MVLP) (Takemura
    et al., [2018](#bib.bib100)) database.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一种名为Siamese Networks的架构被成功应用于类似任务，它接受两个独立的输入并计算它们之间的相似度值，例如识别受阻的路线或危险环境中的不良行为（Santana等，[2019](#bib.bib84)）。在这个背景下，Takemura等（Takemura等，[2017](#bib.bib99)）实现了四种不同的框架来创建一个生物特征识别系统。所有网络都依赖于GEI进行比较。其中两个网络基于Triplet
    Ranking Loss开发，该方法需要三个输入进行执行：（i）被比较者的信息，（ii）同一个人的数据，以及（iii）其他任何人的数据。其他两个网络在执行中使用对比损失，其输入仅依赖于被比较者的信息。作者在OU-ISIR
    Multi-view Large Population（OU-MVLP）（Takemura等，[2018](#bib.bib100)）数据库上获得了$91.9\%$的准确率。
- en: Still, Xu et al. (Xu et al., [2020](#bib.bib121)) proposed the Pairwise Spatial
    Transformer Network, a unified model composed of pairwise spatial transformers
    and a recognition network for cross-view gait recognition. The model computes
    non-rigid deformation fields to match input pairs into intermediate frames, which
    are compared against a deformation suffered from source view to target. Experiments
    conducted over the OU-MVLP (Takemura et al., [2018](#bib.bib100)), OU-ISIR Large
    Population (OU-LP) (Iwama et al., [2012](#bib.bib37)), and CASIA B (Yu et al.,
    [2006](#bib.bib129)) datasets grant the model robustness.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，Xu等（Xu等，[2020](#bib.bib121)）提出了Pairwise Spatial Transformer Network，这是一种统一的模型，由成对空间变换器和用于跨视角步态识别的识别网络组成。该模型计算非刚性变形场，将输入对匹配到中间帧，并与从源视角到目标的变形进行比较。在OU-MVLP（Takemura等，[2018](#bib.bib100)）、OU-ISIR
    Large Population（OU-LP）（Iwama等，[2012](#bib.bib37)）和CASIA B（Yu等，[2006](#bib.bib129)）数据集上进行的实验验证了模型的鲁棒性。
- en: 3.2\. Capsule Networks
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. Capsule Networks
- en: 'Another well-known type of deep architecture employed for gait recognition
    is the Capsule Neural Network (Sabour et al., [2017](#bib.bib83)). The network
    has been developed for image classification by modeling the hierarchical relationships
    between objects, i.e., capsules, in a scene. In this context, Xu et al. (Xu et al.,
    [2019](#bib.bib123)) explored such features to propose two architectures for gait
    recognition:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于步态识别的著名深度架构是胶囊神经网络（Sabour 等，[2017](#bib.bib83)）。该网络通过建模场景中对象之间的层次关系，即胶囊，已被开发用于图像分类。在这个背景下，Xu
    等人（Xu 等，[2019](#bib.bib123)）探索了这些特征并提出了两种步态识别架构：
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: using the Local Bottom Feature (LBC) combination of local features, which employs
    two input images and computes the differences between then in a unique network
    flow; and
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用局部底层特征（LBC）组合局部特征，该方法采用两张输入图像并在独特的网络流中计算它们之间的差异；以及
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: employing the so-called Matching Mid-Level Feature (MMF) to combine both images’
    characteristics after passing through a specific part of the neural network.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采用所谓的匹配中级特征（MMF）来结合经过神经网络特定部分处理后的两张图像的特征。
- en: In summary, the first architecture merges the images before inputting them to
    the neural network, while the other combines the images after going through two
    layers of transformations. Experiments considering GEIs, Chrono-gait image (CGI),
    and resolution of the input image present an identification accuracy of $74.4\%$
    over OU-ISIR (Makihara et al., [2012](#bib.bib60)) dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，第一个架构在将图像输入神经网络之前进行融合，而另一个则在经过两层变换后合并图像。实验考虑了 GEIs、Chrono-gait 图像（CGI）以及输入图像的分辨率，针对
    OU-ISIR （Makihara 等，[2012](#bib.bib60)）数据集的识别准确率为 $74.4\%$。
- en: In a similar work, Sepas et al. (Sepas-Moghaddam et al., [2021](#bib.bib86))
    used capsule networks to develop a model capable of learning more discriminative
    features by transferring multi-scale partial gait representations. The model employs
    Bi-directional Gated Recurrent Units (BGRU) to learn the co-occurrences and correlations
    among patterns and further uses a capsule network to extract deeper relationships
    among such features. Experiments conducted over CASIA B (Yu et al., [2006](#bib.bib129))
    and OU-MVLP (Takemura et al., [2018](#bib.bib100)) assesses the superiority of
    the model against state-of-the-art approaches, especially when considering challenging
    conditions.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的工作中，Sepas 等人（Sepas-Moghaddam 等，[2021](#bib.bib86)）使用胶囊网络开发了一种能够通过转移多尺度部分步态表示来学习更具区分性的特征的模型。该模型利用双向门控循环单元（BGRU）来学习模式之间的共现和关联，并进一步使用胶囊网络提取这些特征之间的更深层次关系。针对
    CASIA B （Yu 等，[2006](#bib.bib129)）和 OU-MVLP （Takemura 等，[2018](#bib.bib100)）进行的实验评估了该模型相对于最先进方法的优越性，尤其是在考虑挑战性条件时。
- en: Finally, Zhao et al. (Zhao et al., [2021](#bib.bib135)) introduced an automated
    learning system called Associated Spatio-Temporal Capsule Network (ASTCapsNet).
    The model was trained over multi-sensor datasets to show that multi-modality data
    is more conducive to gait recognition. The model’s effectiveness is confirmed
    on the experiments conducted over several datasets, in which results are compared
    to state-of-the-art approaches.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，赵等人（Zhao 等，[2021](#bib.bib135)）介绍了一种名为关联时空胶囊网络（ASTCapsNet）的自动学习系统。该模型在多传感器数据集上进行训练，表明多模态数据更有利于步态识别。通过对多个数据集进行的实验确认了该模型的有效性，并将结果与最先进的方法进行了比较。
- en: 3.3\. Recurrent Neural Networks
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 递归神经网络
- en: Concerning Recurrent Neural Networks (Jun et al., [2020](#bib.bib44)), Wang
    and Yan (Wang and Yan, [2020](#bib.bib116)) used the Long short-term memory (Hochreiter
    and Schmidhuber, [1997](#bib.bib33)), a popular RNN architecture capable of learning
    long-term dependencies, for cross-view human gait recognition based on frame-by-frame
    GEIs. Experiments conducted over CASIA B (Yu et al., [2006](#bib.bib129)) and
    OU-ISIR (Makihara et al., [2012](#bib.bib60)) Large Population datasets demonstrate
    the model’s robustness over several baselines.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于递归神经网络（Jun 等，[2020](#bib.bib44)），Wang 和 Yan（Wang 和 Yan，[2020](#bib.bib116)）使用长短期记忆（Hochreiter
    和 Schmidhuber，[1997](#bib.bib33)），这种流行的 RNN 架构能够学习长期依赖关系，用于基于逐帧 GEIs 的跨视角人步态识别。对
    CASIA B （Yu 等，[2006](#bib.bib129)）和 OU-ISIR （Makihara 等，[2012](#bib.bib60)）大规模数据集进行的实验展示了该模型在多个基线上的鲁棒性。
- en: A similar work proposed by Potluri et al. (Potluri et al., [2019](#bib.bib77))
    employed LSTMs to detect gait abnormalities through a wearable sensor system.
    The experiments were conducted over a set of data extracted from ten healthy individuals,
    such that seven behave normally, while the three remaining simulate specific gait
    abnormalities, i.e., sensory ataxic, Parkinson’s gait, and hemiplegic gait. The
    experiments focus on employing advanced technologies for gait-based diagnosis
    and treatment assistant systems. Other models combined specific information, such
    as data obtained from accelerometer and gyroscope, for gait recognition through
    deep learning-based approaches, reaching recognition rates above $91\%$ (Zou et al.,
    [2018b](#bib.bib142)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Potluri 等人 (Potluri et al., [2019](#bib.bib77)) 提出的类似工作利用 LSTM 通过可穿戴传感器系统检测步态异常。实验在从十个健康个体中提取的数据集上进行，其中七人表现正常，而剩余的三人则模拟特定的步态异常，即感觉性共济失调、帕金森步态和偏瘫步态。实验集中于利用先进技术进行基于步态的诊断和治疗辅助系统。其他模型结合了特定的信息，如从加速度计和陀螺仪获得的数据，通过基于深度学习的方法进行步态识别，识别率超过
    $91\%$ (Zou et al., [2018b](#bib.bib142))。
- en: Recently, Tran et al. (Tran et al., [2021](#bib.bib104)) proposed an Inertial
    Measurement Units (IMUs)-based gait recognition approach. The authors employed
    LSTMs to exploit the temporal information on video sequences, thus extracting
    hidden patterns inside such sequences. Experiments conducted over whuGAIT (Zou
    et al., [2018b](#bib.bib142)) and OU-ISIR (Makihara et al., [2012](#bib.bib60))
    provided state-of-the-art performance for both verification and identification
    tasks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Tran 等人 (Tran et al., [2021](#bib.bib104)) 提出了一个基于惯性测量单元 (IMUs) 的步态识别方法。作者利用
    LSTM 利用视频序列的时间信息，从而提取序列中的隐藏模式。在 whuGAIT (Zou et al., [2018b](#bib.bib142)) 和 OU-ISIR
    (Makihara et al., [2012](#bib.bib60)) 上进行的实验为验证和识别任务提供了最先进的性能。
- en: 3.4\. Autoencoders
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 自编码器
- en: Babaee at al. (Babaee et al., [2019](#bib.bib6)) also proposed a multiple-stage
    model for the recognition of gestures. The work tackle a problem commonly observed
    in several GEI-based identification datasets, i.e., the person’s cycle of moviment
    is not entirely formed. In other words, the data available for identification
    is not complete. To deal with the problem, the authors proposed an autoencoder-based
    approach called Incomplete to Complete GEI Network (ITCNet). The model was trained
    to reconstruct the missing images using examples from the training set with a
    complete cycle. The network consists of $9$ fully convolutional networks, each
    of responsible for $1/9$ of the components of the cycle. Further, PCA is employed
    to compute the main features and an RNN is used for gait recognition. The model
    presented an index of $86\%$ and $95\%$ for Rank-1 and Rank-5, respectively, over
    OU-ISR Large Population dataset (Makihara et al., [2012](#bib.bib60)) using only
    $20$ images per instance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Babaee 等人 (Babaee et al., [2019](#bib.bib6)) 还提出了一个多阶段模型用于手势识别。该工作解决了在多个 GEI
    基于识别数据集中常见的问题，即个体的运动周期没有完全形成。换句话说，用于识别的数据不完整。为了解决这一问题，作者提出了一种基于自编码器的方法，称为不完整到完整
    GEI 网络 (ITCNet)。该模型经过训练，使用训练集中具有完整周期的示例来重建缺失的图像。该网络由 $9$ 个完全卷积网络组成，每个网络负责周期的 $1/9$
    组件。此外，PCA 被用来计算主要特征，RNN 被用于步态识别。该模型在 OU-ISR 大型人群数据集 (Makihara et al., [2012](#bib.bib60))
    上对 Rank-1 和 Rank-5 的指标分别为 $86\%$ 和 $95\%$，每实例仅使用 $20$ 张图像。
- en: Autoencoders can also be employed to extract distinct GEI features. In this
    context, Yu et al. (Yu et al., [2017](#bib.bib128)) proposed a study using Stacked
    Progressive Auto-Encoders (SPAE) (Kan et al., [2014](#bib.bib46)) with appropriate
    changes for the extraction of invariant characteristics of an individual’s behavior.
    In short, the model extracts information from independent components of the scene,
    such as clothes and other objects the person may eventually be carrying, instead
    of the movement exclusively. Experiments conducted over CASIA B (Yu et al., [2006](#bib.bib129))
    and SZU RGB (Yu et al., [2013](#bib.bib130)) datasets show that, despite the GEI
    information for personal identification, intermediate and last layers also provide
    relevant characteristics regarding such components. Finally, the work combines
    such information and perform a dimensionality reduction step through PCA.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器还可以用于提取不同的GEI特征。在这种背景下，Yu等人（Yu等，[2017](#bib.bib128)）提出了使用堆叠渐进自动编码器（SPAE）（Kan等，[2014](#bib.bib46)）进行适当更改，以提取个体行为的不变特征的研究。简而言之，该模型从场景的独立组件中提取信息，例如服装和个体可能携带的其他物体，而不仅仅是动作。在CASIA
    B（Yu等，[2006](#bib.bib129)）和SZU RGB（Yu等，[2013](#bib.bib130)）数据集上进行的实验表明，尽管GEI信息用于个人识别，但中间和最后层还提供了有关这些组件的相关特征。最后，该工作将这些信息结合起来，并通过主成分分析进行维度降低。
- en: Zhang et al. (Zhang et al., [2019](#bib.bib134)) also employed autoencoders
    for gait recognition. The authors used the technique to tackle a common issue
    faced by biometrical recognition literature, i.e., dealing with clothes blocking
    body members’ views, such as arms and legs. The main problem concerning such occlusions
    is the difficulty of reading the movements and recognizing other features such
    as ligaments and the point of contact with the ground. To minimize the problem,
    they used autoencoders to disconnect the images’ pose and appearance characteristics
    for further linking those characteristics to another architecture of recurrent
    networks to analyze the way the person gaits. Besides, the authors also propose
    the Frontal-View Gait (FVG) dataset.The system proved to be particularly robust
    when filming the person walking towards the camera, whose angulation is more complicated
    due to the difficulty of obtaining specific characteristics, such as the distance
    walked by each step.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人（Zhang等，[2019](#bib.bib134)）还采用了自动编码器进行步态识别。作者们使用这一技术解决了生物测量识别文献中面临的常见问题，即处理阻挡身体部位视图的衣物，如手臂和腿。关于这种遮挡的主要问题是难以读取动作和识别其他特征，例如韧带和与地面接触的点。为了最小化问题，他们使用自动编码器来分离图像的姿势和外观特征，然后将这些特征与循环网络的另一个架构链接，以分析个体步态的方式。此外，作者还提出了正面视图步态（FVG）数据集。当摄像机拍摄人向摄像头走动时，系统被证明是特别稳健的，这更复杂的角度导致了获取特定特征的困难，例如每步行走的距离。
- en: 3.5\. Deep Belief Networks
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 深度信念网络
- en: Deep Belief Networks (DBNs) (Hinton et al., [2006](#bib.bib32)) are stochastic
    neural networks constructed using Restricted Boltzmann Machines (Hinton, [2002](#bib.bib31))
    as building blocks. Such models became very popular due to their ability of performing
    several tasks, such as feature selection (de Souza et al., [2021](#bib.bib16)),
    classification (Roder et al., [ress](#bib.bib82)), and image reconstruction (Passos
    et al., [2019](#bib.bib75), [2017](#bib.bib74)), among others. Regarding gait
    recognition, Fernandes et al. (Fernandes et al., [2018](#bib.bib22)) employed
    DBNs to support gait assessment in the diagnosis of Parkinson’s and movement disorder
    diseases. The authors employed wearable sensors to extract features from the subjects
    and performed a comparative classification analysis of parkinsonian gait, confirming
    that DBN-based approaches are suitable for the task.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 深度信念网络（DBNs）（Hinton等，[2006](#bib.bib32)）是使用受限玻尔兹曼机（Hinton，[2002](#bib.bib31)）作为构建模块构建的随机神经网络。由于其执行多项任务的能力，这种模型变得非常流行，例如特征选择（de
    Souza等，[2021](#bib.bib16)）、分类（Roder等，[ress](#bib.bib82)）和图像重建（Passos等，[2019](#bib.bib75)、[2017](#bib.bib74)）等。关于步态识别，Fernandes等人（Fernandes等，[2018](#bib.bib22)）采用了DBNs来支持帕金森病和运动障碍疾病的步态评估诊断。作者们使用可穿戴传感器从受试者身上提取特征，并对帕金森步态进行了比较分类分析，证实DBN-based方法适用于该任务。
- en: Further, Xiong et al. (Xiong et al., [2020](#bib.bib120)) demonstrated how to
    encode patterns using surface electromyography (sEMG) through Deep Belief Networks.
    The authors consider the knee and ankle joint angle during walking to estimate
    a combination of four-time domain features. The work showed a high potential for
    gait tracking problems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Xiong 等人（Xiong et al., [2020](#bib.bib120)）展示了如何通过深度置信网络（Deep Belief Networks）使用表面肌电图（sEMG）编码模式。作者考虑了步态中膝关节和踝关节角度，以估计四种时间域特征的组合。该研究显示了对步态跟踪问题的高度潜力。
- en: 3.6\. Generative Adversarial Networks
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6. 生成对抗网络
- en: A very interesting work by Hu et al. (Hu et al., [2018](#bib.bib36)) introduces
    Generative Adversarial Networks (Goodfellow et al., [2014](#bib.bib24); Souza Jr
    et al., [2020](#bib.bib93)) to the context of Gait recognition. The authors propose
    the Discriminant Gait Generative Adversarial Network, i.e., DiGGAN, to extract
    view-invariant gait characteristics for cross-view gait recognition. Experiments
    conducted over OU-MVLP (Takemura et al., [2018](#bib.bib100)) and CASIA B (Yu
    et al., [2006](#bib.bib129)) outperformed state-of-the-art results, demonstrating
    the model’s capabilities.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Hu 等人（Hu et al., [2018](#bib.bib36)）的一个非常有趣的工作将生成对抗网络（Goodfellow et al., [2014](#bib.bib24);
    Souza Jr et al., [2020](#bib.bib93)）引入了步态识别的背景。作者提出了判别步态生成对抗网络，即 DiGGAN，用于提取视角不变的步态特征以进行跨视角步态识别。在
    OU-MVLP（Takemura et al., [2018](#bib.bib100)）和 CASIA B（Yu et al., [2006](#bib.bib129)）上的实验超过了最先进的结果，展示了模型的能力。
- en: Meanwhile, He et al. (He et al., [2018](#bib.bib29)) proposed the Multi-task
    Generative Adversarial Networks (MGANs), a GAN-based network designed to learn
    view-specific gait features through the so-called Period Energy Image (PEI), a
    multi-channel gait template proposed to tackle the problems of view angles’ variation
    faced by cross-view methods and loss of temporal information faced by GEI-based
    approaches. The model employs a view-angle manifold to extract more significant
    features from video sequences, thus providing competitive results over OU-ISIR (Makihara
    et al., [2012](#bib.bib60)), CASIA B (Yu et al., [2006](#bib.bib129)), and USF
    against various works available in the literature.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，He 等人（He et al., [2018](#bib.bib29)）提出了多任务生成对抗网络（MGANs），这是一种基于GAN的网络，旨在通过所谓的周期能量图（Period
    Energy Image，PEI）学习视角特定的步态特征。PEI 是一种多通道步态模板，旨在解决跨视角方法面临的视角变化问题和 GEI 基于方法面临的时间信息丧失问题。该模型利用视角流形从视频序列中提取更显著的特征，从而在
    OU-ISIR（Makihara et al., [2012](#bib.bib60)）、CASIA B（Yu et al., [2006](#bib.bib129)）和
    USF 的各种文献工作中提供了竞争力的结果。
- en: Further, Jia et al.  (Jia et al., [2019](#bib.bib42)) studied methods to avoid
    attacks in gait recognition systems through GAN-generated syntectic image sequences.
    In such a scenario, the authors proposed a GAN-based approach capable of rendering
    fake videos from source walking sequence with realistic details. The method is
    compared against two state-of-the-art gait recognition systems, and results are
    analyzed under attacking scenarios considering both CASIA A (Wang et al., [2003](#bib.bib115))
    and CASIA B (Yu et al., [2006](#bib.bib129)) datasets. The effectiveness of the
    model is verified in both attacking detection ability and visual fidelity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Jia 等人（Jia et al., [2019](#bib.bib42)）研究了通过生成对抗网络（GAN）生成的合成图像序列来避免步态识别系统中的攻击。在这种情况下，作者提出了一种基于GAN的方法，能够从源步态序列中渲染出具有现实细节的伪造视频。该方法与两种最先进的步态识别系统进行比较，并在攻击情景下考虑了
    CASIA A（Wang et al., [2003](#bib.bib115)）和 CASIA B（Yu et al., [2006](#bib.bib129)）数据集下进行了分析。该模型在攻击检测能力和视觉逼真度方面的有效性得到了验证。
- en: 3.7\. Discussion
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7. 讨论
- en: Table LABEL:t.summary provides a summary of the surveyed studies presented in
    this section. In general, Convolutional Neural Networks are the most popular choice
    when considering deep learning solutions, especially concerning image/video-based
    issues, including gait recognition. Such behavior is expected since CNNs obtained
    outstanding results in various applications and won most of the benchmark challenges
    in the last years. Nevertheless, the other architectures also provide a valuable
    contribution to the field, performing better for several specific tasks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 LABEL:t.summary 提供了本节中调查研究的总结。一般而言，卷积神经网络（Convolutional Neural Networks）是考虑深度学习解决方案时最受欢迎的选择，尤其是与基于图像/视频的问题有关，包括步态识别。这种行为是可以预期的，因为
    CNN 在各种应用中取得了杰出的结果，并在过去几年赢得了大多数基准挑战。然而，其他架构也为该领域做出了有价值的贡献，在若干特定任务中表现更佳。
- en: Capsule Networks, for instance, are capable of extracting partial gait representations
    in a hierarchical fashion, providing better results for cases where individuals
    or objects are presented in the scene with multiple orientations or overlapped.
    Similarly, Recurrent Neural Networks are paramount to deal with sequential data,
    such as videos, thus presenting themselves as an essential tool for this kind
    of gait recognition approach.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，胶囊网络（Capsule Networks）能够以分层的方式提取部分步态表示，对于在场景中出现多个方向或重叠的个体或物体的情况提供了更好的结果。同样，递归神经网络（Recurrent
    Neural Networks）对处理序列数据（如视频）至关重要，因此它们在这种步态识别方法中成为必要工具。
- en: Although most deep learning-based approaches for gait recognition comprise image/video
    domain, other data sources, such as accelerometers, gyroscope, sensor-based, and
    handcrafted features in general, also provided impressive results in a considerable
    number of works. Most of these works address unsupervised deep learning approaches
    in the process, such as Autoencoders and DBNs, which usually are more expressive
    over these data types, while CNNs are paramount when dealing with raw image/video
    files. These unsupervised methods can extract information regarding the data distribution
    and manipulate it, usually in a lower-dimensional space, providing more representative
    features for gait recognition.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数基于深度学习的步态识别方法涉及图像/视频领域，但其他数据来源，如加速度计、陀螺仪、传感器数据和手工特征，一般也在大量工作中提供了令人印象深刻的结果。这些工作大多数涉及无监督深度学习方法，如自编码器（Autoencoders）和深度信念网络（DBNs），这些方法通常在这些数据类型上更具表现力，而卷积神经网络（CNNs）在处理原始图像/视频文件时至关重要。这些无监督方法可以提取关于数据分布的信息并进行操作，通常在较低维空间中，提供更具代表性的步态识别特征。
- en: Finally, Generative Adversarial Networks describe a particular case when gait
    systems can learn a broader range of features, such as orientations, clothes,
    number of individuals in the scene, and so on, since they can generate synthetic
    data for training models. Besides, they are also useful for evaluating frauds
    in gait-based systems for security purposes by producing fake images for testing
    purposes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成对抗网络（Generative Adversarial Networks）描述了步态系统可以学习更广泛特征的特定情况，例如方向、服装、场景中的个体数量等，因为它们可以生成合成数据用于模型训练。此外，它们在安全目的上评估步态系统中的欺诈行为时也很有用，通过生成假图像进行测试。
- en: 'Concerning the most used methods to represent gait image data, Gait Energy
    Image reflects the sequence of a simple energy image cycle using the weighted
    average method. Further, the sequences in a travel cycle are processed to align
    the binary silhouette (Jing Luo and Xiu, [2015](#bib.bib43)). Therefore, GEI maintains
    the static and dynamic characteristics of human walking and significantly reduces
    image processing’s computational cost. From an in-depth analysis of the method,
    one can observe a few points that characterize the model (Han and Bhanu, [2016](#bib.bib27)),
    described as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最常用的步态图像数据表示方法，步态能量图像（Gait Energy Image, GEI）通过加权平均法反映了一个简单能量图像循环的序列。此外，旅行周期中的序列被处理以对齐二进制轮廓（Jing
    Luo 和 Xiu，[2015](#bib.bib43)）。因此，GEI 保持了人类行走的静态和动态特征，并显著降低了图像处理的计算成本。从对该方法的深入分析中，可以观察到一些特征，这些特征定义了模型（Han
    和 Bhanu，[2016](#bib.bib27)），具体描述如下：
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GEIs are a little sensitive to silhouette noise in individual pictures.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GEI 对单张图像中的轮廓噪声有些敏感。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It focuses on specific representations of the human walk, which does not soften
    the context of vector images.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它专注于人体行走的特定表现形式，这不会软化矢量图像的背景。
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It represents human motion in a single image while preserving temporal information.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它在单张图像中表示人体运动，同时保留了时间信息。
- en: Similarly, cross-view-based gait recognition is a popular approach used to deal
    with different visual angles. The input type requires multiple fully controlled
    cameras and cooperative environments, thus being restrained to real scenarios.
    Moreover, it visually normalizes gait characteristics before performing any combination,
    which allows the model to learn the relationships among visual movements in the
    scene (e MS Nixon, [2011](#bib.bib21); W. Kusakunniran and Li, [2013](#bib.bib110);
    Yasushi Makihara and Yagi, [2006](#bib.bib124)). An example of the approach can
    be observed in the UK’s Newcastle University cross-view gait recognition system,
    the so-called DiGGAN, which obtained state-of-the-art results over the largest
    multiview gait dataset in the world (Takemura et al., [2018](#bib.bib100)) (comprising
    more than $10,000$ people) using cross-view approaches (Hu et al., [2018](#bib.bib36)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，基于跨视角的步态识别是一种流行的方法，用于处理不同的视觉角度。输入类型要求多个完全控制的摄像头和协作环境，因此受到现实场景的限制。此外，它在进行任何组合之前会视觉上归一化步态特征，这使得模型能够学习场景中视觉运动之间的关系 (e MS Nixon,
    [2011](#bib.bib21); W. Kusakunniran 和 Li, [2013](#bib.bib110); Yasushi Makihara
    和 Yagi, [2006](#bib.bib124))。这种方法的一个例子可以在英国纽卡斯尔大学的跨视角步态识别系统DiGGAN中观察到，该系统在世界上最大的多视角步态数据集中取得了最先进的结果 (Takemura
    et al., [2018](#bib.bib100))（包括超过$10,000$人），使用跨视角方法 (Hu et al., [2018](#bib.bib36))。
- en: Despite the success obtained by such methods, they still lack in some aspects,
    in which some less popular techniques aim to overcome. The Period Energy Image,
    for instance, comprises a multi-channel gait template proposed to tackle view
    angles’ variation faced by cross-view and GEI loss of temporal information (He
    et al., [2018](#bib.bib29)). Another approach observed in some works comprises
    Optical-flow, which provides relevant information regarding the objects observed
    in the scene, their spatial arrangements, and the changes in sich arrangements (Luo
    et al., [2016](#bib.bib58)). Finally, a wide range of hand-crafted features or
    sensor-based input data types is also employed for deep learning-based gait recognition
    approaches. Among such methods, one can find the Inertial Measurement Units, accelerometers,
    gyroscopes, sensor output, and so on.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法取得了成功，但在某些方面仍然存在不足，一些不太受欢迎的技术旨在克服这些不足。例如，周期能量图像包括一个多通道步态模板，旨在应对跨视角和GEI丧失时间信息的视角变化问题 (He
    et al., [2018](#bib.bib29))。另一种在一些工作中观察到的方法包括光流，它提供了关于场景中观察到的物体、它们的空间排列以及这些排列的变化的相关信息 (Luo
    et al., [2016](#bib.bib58))。最后，广泛使用的手工特征或传感器输入数据类型也被用于基于深度学习的步态识别方法。在这些方法中，可以找到惯性测量单元、加速度计、陀螺仪、传感器输出等。
- en: Concerning gait recognition’s actual state-of-the-art scenario, one can consider
    two main approaches, i.e., literature- and representation-based. Regarding the
    literature-based, 2D-CNNs are the most widely used type of deep neural network
    (DNN) for gait recognition using deep learning, with approximately 50% of solutions
    just based on 2D-CNN architectures for classification. Solutions using 3D-CNN
    and GAN are the next popular categories, each corresponding to 10% of the published
    content. In addition, deep autoencoder (DAE), RNN, CapsNet, DBN, and graph convolutional
    networks are less considered among DNNs, corresponding to $5\%$, $3\%$, $2\%$,
    $1\%$, and $1\%$, respectively. On the other hand, the hybrid methods that constitute
    $26\%$ of the solutions, in which the CNN-RNN combinations are the most widely
    adopted approach with about $15\%$ of presence, while the combination of DAE with
    GAN and RNN corresponds to $10\%$ of the methods, followed by the RNN-CapsNet
    methods that constitute 2% of the solutions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步态识别的实际最先进场景，可以考虑两种主要方法，即基于文献的方法和基于表示的方法。关于基于文献的方法，2D-CNN是使用深度学习进行步态识别时最广泛使用的深度神经网络（DNN）类型，大约50%的解决方案仅基于2D-CNN架构进行分类。使用3D-CNN和GAN的方法是下一个受欢迎的类别，每种方法占已发布内容的10%。此外，深度自编码器（DAE）、RNN、CapsNet、DBN和图卷积网络在DNN中较少被考虑，分别对应$5\%$、$3\%$、$2\%$、$1\%$和$1\%$。另一方面，混合方法构成了$26\%$的解决方案，其中CNN-RNN组合是最广泛采用的方法，占约$15\%$，而DAE与GAN和RNN的组合占方法的$10\%$，其次是构成2%解决方案的RNN-CapsNet方法。
- en: Regarding state-of-the-art solutions based on representation, the silhouettes
    are the most widely adopted for gait recognition, corresponding to more than $85\%$
    of the solutions. Although it is a promising approach, skeletons have been considered
    less frequently in relation to silhouettes, corresponding to only $10\%$ of the
    available solutions. There were also some methods, that is, approximately $5\%$
    of the available literature, that explore the representations of the skeleton
    and the silhouette, notably using unraveled representation learning or score of
    fusion strategies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于表示的最先进解决方案，轮廓是最广泛应用于步态识别的方法，占所有解决方案的$85\%$以上。尽管这是一个有前景的方法，但与轮廓相比，骨架的使用频率较低，仅占现有解决方案的$10\%$。还有一些方法，即大约$5\%$的现有文献，探讨了骨架和轮廓的表示，特别是使用解开的表示学习或融合策略的评分。
- en: Considering deep learning-based gait recognition main challenges, one can refer
    to the complexity of gait data, which arises from the interaction between many
    factors, such as occlusion/obstruction, camera points of view, the appearance
    of individuals, order of sequence, movement of body parts, or light sources present
    in the data, among others (Z. Zhang and Liu, [2020](#bib.bib131); X. Li and Ren,
    [2020](#bib.bib119)). Such factors can interfere in a complicated way and can
    hinder the task of gait recognition. Currently, there is an increasing number
    of methods in other areas related to pattern recognition, such as face recognition,
    emotion, and pose estimation. These professionals focus on learning confusing
    contexts, extracting representations that separate the various explanatory factors
    in the data’s high-dimensional space. However, most gait recognition methods available
    using deep learning have not yet explored such approaches. Therefore, they are
    not explicitly able to separate the underlying structure of gait data in the form
    of significant disjoint variables. Despite recent progress in using confusing
    context approaches in some gait recognition methods, there is still room for improvement.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到基于深度学习的步态识别主要挑战，可以参考步态数据的复杂性，这源于许多因素的相互作用，例如遮挡/阻碍、摄像机视角、个体外观、序列顺序、身体部位的运动或数据中存在的光源等（Z.
    Zhang 和 Liu, [2020](#bib.bib131); X. Li 和 Ren, [2020](#bib.bib119)）。这些因素可以以复杂的方式干扰步态识别任务。目前，其他与模式识别相关的领域，如面部识别、情感和姿态估计，方法数量正在增加。这些领域的专家专注于学习混淆上下文，提取在数据的高维空间中分离各种解释因素的表示。然而，现有的大多数使用深度学习的步态识别方法尚未探索这种方法。因此，它们不能明确地分离步态数据的基础结构，即显著的不相交变量。尽管最近在一些步态识别方法中使用混淆上下文方法取得了一些进展，但仍有改进的空间。
- en: Table 2\. Deep learning-based gait recognition approaches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 基于深度学习的步态识别方法。
- en: '| Ref. | Year | Model | Input Type | Dataset | Result | Measure |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 年份 | 模型 | 输入类型 | 数据集 | 结果 | 衡量标准 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| (Shiraga et al., [2016](#bib.bib87)) | 2016 | CNN | GEI | OU-ISIR | $94.6\%$
    | Identification rate |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| (Shiraga et al., [2016](#bib.bib87)) | 2016 | CNN | GEI | OU-ISIR | $94.6\%$
    | 识别率 |'
- en: '| (Wu et al., [2017a](#bib.bib117)) | 2017 | CNN | Cross-view | CASIA B | $90.8\%$
    | Accuracy |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| (Wu et al., [2017a](#bib.bib117)) | 2017 | CNN | 跨视角 | CASIA B | $90.8\%$
    | 准确率 |'
- en: '| (Li et al., [2017](#bib.bib53)) | 2017 | CNN + Joint Bayesian | Sensors |
    OU-ISR | $97.6\%$ | Identification rate |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| (Li et al., [2017](#bib.bib53)) | 2017 | CNN + 联合贝叶斯 | 传感器 | OU-ISR | $97.6\%$
    | 识别率 |'
- en: '| (Sokolova and Konushin, [2017a](#bib.bib91)) | 2017 | CNN | Optical flow
    | TUM-GAID and CASIA B | $97.52\%$ | Accuracy |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (Sokolova 和 Konushin, [2017a](#bib.bib91)) | 2017 | CNN | 光流 | TUM-GAID 和
    CASIA B | $97.52\%$ | 准确率 |'
- en: '| (Takemura et al., [2017](#bib.bib99)) | 2017 | CNN + Siamese networks | Cross-view
    | OU-ISIR | $98.8\%$ | Accuracy |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| (Takemura et al., [2017](#bib.bib99)) | 2017 | CNN + 孪生网络 | 跨视角 | OU-ISIR
    | $98.8\%$ | 准确率 |'
- en: '| (Sokolova and Konushin, [2017b](#bib.bib92)) | 2017 | CNN + Nearest Neighbor
    | Optical Flow | TUM-GAID, CASIA B, and OU-ISIR | $99.8\%$ | Identification rate
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (Sokolova 和 Konushin, [2017b](#bib.bib92)) | 2017 | CNN + 最近邻 | 光流 | TUM-GAID、CASIA
    B 和 OU-ISIR | $99.8\%$ | 识别率 |'
- en: '| (Yu et al., [2017](#bib.bib128)) | 2017 | Autoencoders + PCA | GEI | CASIA
    B and SZU RGB | $97.58\%$ | Identification rate |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| (Yu et al., [2017](#bib.bib128)) | 2017 | 自编码器 + PCA | GEI | CASIA B 和 SZU
    RGB | $97.58\%$ | 识别率 |'
- en: '| (Zou et al., [2018b](#bib.bib142)) | 2018 | CNN + LSTM | Accelerometer and
    Gyroscope | whuGAIT and OU-ISIR | $99.75\%$ | Accuracy |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (Zou et al., [2018b](#bib.bib142)) | 2018 | CNN + LSTM | 加速度计和陀螺仪 | whuGAIT
    和 OU-ISIR | $99.75\%$ | 准确率 |'
- en: '| (He et al., [2018](#bib.bib29)) | 2018 | GAN | PEI | OU-ISIR, CASIA B and
    USF | $94.7\%$ | Accuracy |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| (He et al., [2018](#bib.bib29)) | 2018 | GAN | PEI | OU-ISIR, CASIA B 和 USF
    | $94.7\%$ | 准确率 |'
- en: '| (Fernandes et al., [2018](#bib.bib22)) | 2018 | DBN | Sensors | Data collected
    by the authors | $93\%$ | Accuracy |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (Fernandes et al., [2018](#bib.bib22)) | 2018 | DBN | 传感器 | 作者收集的数据 | $93\%$
    | 准确率 |'
- en: '| (Xu et al., [2019](#bib.bib123)) | 2019 | Capsule | LBC and MMF | OU-ISIR
    | $74.4\%$ | Accuracy |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| (Xu et al., [2019](#bib.bib123)) | 2019 | Capsule | LBC 和 MMF | OU-ISIR |
    $74.4\%$ | 准确率 |'
- en: '| (Wang et al., [2019](#bib.bib114)) | 2019 | CNN | GEI + Data Augmentation
    | CASIA B | $98\%$ | Accuracy |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| (Wang et al., [2019](#bib.bib114)) | 2019 | CNN | GEI + 数据增强 | CASIA B |
    $98\%$ | 准确率 |'
- en: '| (Zhang et al., [2019](#bib.bib134)) | 2019 | Autoencoders + LSTN | Croos-
    and Frontal-view | CASIA B, USF, and FVG | $99.1\%$ | Accuracy |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| (Zhang et al., [2019](#bib.bib134)) | 2019 | 自编码器 + LSTN | 交叉视角和正面视角 | CASIA
    B, USF 和 FVG | $99.1\%$ | 准确率 |'
- en: '| (Babaee et al., [2019](#bib.bib6)) | 2019 | Autoencoders + PCA | GEI | OU-ISIR
    and CASIA B | $96.15\%$ | Accuracy |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| (Babaee et al., [2019](#bib.bib6)) | 2019 | 自编码器 + PCA | GEI | OU-ISIR 和
    CASIA B | $96.15\%$ | 准确率 |'
- en: '| (Potluri et al., [2019](#bib.bib77)) | 2019 | LSTM | Sensors | Data collected
    by the authors | $0.02$ | Prediction error |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| (Potluri et al., [2019](#bib.bib77)) | 2019 | LSTM | 传感器 | 作者收集的数据 | $0.02$
    | 预测误差 |'
- en: '| (Jia et al., [2019](#bib.bib42)) | 2019 | GAN | GEI | CASIA A and CASIA B
    | $82\%$ | Recognition result |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| (Jia et al., [2019](#bib.bib42)) | 2019 | GAN | GEI | CASIA A 和 CASIA B |
    $82\%$ | 识别结果 |'
- en: '| (Wang and Yan, [2020](#bib.bib116)) | 2020 | LSTM | GEI | CASIA B and OU-ISIR
    | $99.1\%$ | Recognition rate |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (Wang and Yan, [2020](#bib.bib116)) | 2020 | LSTM | GEI | CASIA B 和 OU-ISIR
    | $99.1\%$ | 识别率 |'
- en: '| (Xu et al., [2020](#bib.bib121)) | 2020 | CNN | Cross-view | OU-MVLP, OU-LP,
    and CASIA B | $98.93\%$ | Identification rate |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| (Xu et al., [2020](#bib.bib121)) | 2020 | CNN | 交叉视角 | OU-MVLP, OU-LP 和 CASIA
    B | $98.93\%$ | 识别率 |'
- en: '| (Hu et al., [2018](#bib.bib36)) | 2020 | GAN | Cross-view | OU-MVLP and CASIA
    B | $93.2\%$ | Identification rate |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| (Hu et al., [2018](#bib.bib36)) | 2020 | GAN | 交叉视角 | OU-MVLP 和 CASIA B |
    $93.2\%$ | 识别率 |'
- en: '| (Sepas-Moghaddam et al., [2021](#bib.bib86)) | 2020 | Capsule | Multi-scale
    representations | CASIA-B and OU-MVLP | $84.5\%$ | Identification rate |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| (Sepas-Moghaddam et al., [2021](#bib.bib86)) | 2020 | Capsule | 多尺度表示 | CASIA-B
    和 OU-MVLP | $84.5\%$ | 识别率 |'
- en: '| (Xiong et al., [2020](#bib.bib120)) | 2020 | DBN | Sensors | Data collected
    by the authors | $2.61$ | RMSE |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (Xiong et al., [2020](#bib.bib120)) | 2020 | DBN | 传感器 | 作者收集的数据 | $2.61$
    | RMSE |'
- en: '| (Tran et al., [2021](#bib.bib104)) | 2021 | LSTM | IMU | whuGAIT and OU-ISIR
    | $94.15\%$ | Accuracy |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| (Tran et al., [2021](#bib.bib104)) | 2021 | LSTM | IMU | whuGAIT 和 OU-ISIR
    | $94.15\%$ | 准确率 |'
- en: '| (Zhao et al., [2021](#bib.bib135)) | 2021 | Capsule | Sensors | Several |
    $99.69\%$ | Accuracy |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| (Zhao et al., [2021](#bib.bib135)) | 2021 | Capsule | 传感器 | 多个 | $99.69\%$
    | 准确率 |'
- en: 4\. Datasets
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 数据集
- en: Machine learning models’ training and evaluation steps, regardless of the adopted
    paradigm, i.e., supervised, unsupervised, or any other, depend on a dataset comprising
    the task’s subject. Besides, the employment of such datasets makes it possible
    to determine how effective a method is to solve a specific problem and compare
    it with other solutions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 无论采用哪种范式，即监督、无监督或其他，机器学习模型的训练和评估步骤都依赖于包含任务主题的数据集。此外，使用这些数据集可以确定某种方法解决特定问题的有效性，并将其与其他解决方案进行比较。
- en: 'Regarding gait recognition, the availability of specific datasets for the task
    is minimal, considering both public or private solutions. Such a lack of training
    data hampers the development of new artificial intelligence models capable of
    recognizing people by how they walk or move. Two problems stand out for obtaining
    and creating a dataset:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 关于步态识别，考虑到公共或私有解决方案，针对该任务的特定数据集可用性极其有限。这种训练数据的缺乏阻碍了能够通过步态或动作识别人的新人工智能模型的发展。获取和创建数据集存在两个突出问题：
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Gait biometry demands a reasonable amount of movement recordings for a subject,
    implying recording and generating multiple videos for each individual. Besides,
    such videos usually possess an inherent high dimensionality, entailing in the
    dataset’s growth and thus requiring a high storage capacity.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步态生物计量学要求对每个对象进行合理数量的运动记录，这意味着需要为每个个体录制和生成多个视频。此外，这些视频通常具有固有的高维度，导致数据集的增长，因此需要较高的存储容量。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The extraction and public distribution of biometric data require permission
    from each participant. The eventual creation of a dataset without formal consent
    from each individual may bring about lawsuits.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生物特征数据的提取和公开分发需要每个参与者的许可。未经每个个人正式同意创建数据集可能引发诉讼。
- en: The next section gathers the most-used datasets available for gait recognition
    related tasks.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节汇集了用于步态识别相关任务的最常用数据集。
- en: 4.1\. CMU MoBo Dataset
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. CMU MoBo 数据集
- en: The CMU MoBo (Gross and Shi, [2001](#bib.bib25)) dataset possesses a relatively
    small amount of data, comprising several videos for gait recognition extracted
    from $20$ people. The dataset also provides a set of silhouette masks and bounding
    boxes, thus alleviating the segmentation process.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: CMU MoBo (Gross 和 Shi, [2001](#bib.bib25)) 数据集包含相对较少的数据，包括从 $20$ 个人那里提取的几个步态识别视频。该数据集还提供了一组轮廓掩膜和边界框，从而缓解了分割过程。
- en: One of the main advantages of the dataset is that it is available for download
    without reservations or need to sign forms of agreement, requiring just connecting
    to the Calgary University File Transfer Protocol (FTP) server³³3ftp://ftp.cc.gatech.edu/pub/gvu/cpl/.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的主要优势之一是可以下载，无需预订或签署协议，只需连接到卡尔加里大学文件传输协议 (FTP) 服务器³³3ftp://ftp.cc.gatech.edu/pub/gvu/cpl/。
- en: 4.2\. TUM GAID Dataset
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. TUM GAID 数据集
- en: The TUM Gait from Audio, Image, and Depth (GAID) (Hofmann et al., [2014](#bib.bib34))
    database, as the name suggests, is compounded by RGB images, audio, and depth,
    recorded from $305$ people in three different variations at first. Further, $32$
    people were re-recorded to have some variation, comprising $3,370$ records in
    total. According to the authors, it is the only dataset that allows recognition
    by combining video, depth, and audio.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: TUM Gait from Audio, Image, and Depth (GAID) (Hofmann 等，[2014](#bib.bib34))
    数据库，顾名思义，由 RGB 图像、音频和深度数据组成，最初记录了 $305$ 个人的三种不同变体。此外，$32$ 个人被重新记录以增加一些变化，总共包含
    $3,370$ 条记录。根据作者的说法，这是唯一一个通过结合视频、深度和音频来进行识别的数据集。
- en: 'Variations include different shows, carrying conditions (with and without bag),
    and time. The first record was performed in 2012’s winter (January) and comprised
    $176$ recordings, using a jacket and winter boots. The second performance was
    made in April 2012 by $161$ people using considerably different clothes, since
    it was warmer. From this amount, $32$ people were recorded both times, which grant
    cloth and time variation. A strong advantage of this dataset is a well-defined
    evaluation protocol. Figure [7](#S4.F7 "Figure 7 ‣ 4.2\. TUM GAID Dataset ‣ 4\.
    Datasets ‣ Gait Recognition Based on Deep Learning: A Survey") depicts some examples
    of images and depth. The dataset is available after sign a document request⁴⁴4https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '变化包括不同的表演、携带条件（有包和无包）以及时间。第一次记录是在2012年冬季（1月）进行的，包含 $176$ 条记录，使用了夹克和冬靴。第二次记录是在2012年4月，由
    $161$ 个人在较温暖的天气下穿着明显不同的衣物进行的。从这些记录中，有 $32$ 个人在两次记录中都有被拍摄，带来了服装和时间上的变化。该数据集的一个强大优势是明确的评估协议。图
    [7](#S4.F7 "Figure 7 ‣ 4.2\. TUM GAID Dataset ‣ 4\. Datasets ‣ Gait Recognition
    Based on Deep Learning: A Survey") 展示了一些图像和深度的示例。数据集在签署文档请求后可以获得⁴⁴4https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/。'
- en: '![Refer to caption](img/8a1f3b02ea4ebaac5c51f2f5bbb5f216.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a1f3b02ea4ebaac5c51f2f5bbb5f216.png)'
- en: 'Figure 7\. Examples of three male (top rows) and three female (bottom rows)
    participants in six variations: normal (columns $1$ and $2$), backpack (columns
    $3$ and $4$), coating shoes (columns $5$ and $6$), time (columns $7$ and $8$),
    time + backpack (columns $9$ and $10$), and time + coating shoes (columns $10$
    and $11$). Extracted from [https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/](https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 三名男性（上行）和三名女性（下行）参与者在六种变化下的示例：正常（第 $1$ 和第 $2$ 列），背包（第 $3$ 和第 $4$ 列），涂层鞋（第
    $5$ 和第 $6$ 列），时间（第 $7$ 和第 $8$ 列），时间 + 背包（第 $9$ 和第 $10$ 列），以及时间 + 涂层鞋（第 $10$ 和第
    $11$ 列）。摘自 [https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/](https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/)
- en: .
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 4.3\. HID-UMD Dataset
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. HID-UMD 数据集
- en: 'The Human Identification at a Distance (HID)-UMD dataset comprises several
    videos from people walking captured in four different angles and their respective
    binary masks for foreground segmentation. Its primary purpose is to help researchers
    develop new gait and facial biometrics recognition methods. Moreover, the dataset
    is an aggregate composed of two datasets, described as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 人体远程识别 (HID)-UMD 数据集包括从四个不同角度捕捉到的行走视频及其各自的前景分割二进制掩膜。其主要目的是帮助研究人员开发新的步态和面部生物识别方法。此外，该数据集是由两个数据集组成的，描述如下：
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset 1 (Kale et al., [2002](#bib.bib45)): composed of walking sequences
    of $25$ individuas in $4$ distinct poses:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集 1 (Kale 等，[2002](#bib.bib45))：由 $25$ 个人在 $4$ 种不同姿势下的步态序列组成：
- en: (1)
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Frontal view/walking-toward;
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正面视图/走向；
- en: (2)
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Frontal view/walking-away;
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正面视图/走开；
- en: (3)
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Frontal-parallel view/toward left; and
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正面平行视图/向左；
- en: (4)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: Frontal-parallel view/toward right.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正面平行视图/向右。
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset 2 (Cuntoor et al., [2003](#bib.bib13)): comprises videos from $55$
    individuals walking through a T-shape pathway. The sequences were obtained by
    two cameras that line orthogonal to each other.'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集2（Cuntoor et al., [2003](#bib.bib13)）：包含$55$名个体在T形路径上行走的视频。序列由两台正交的摄像机获取。
- en: More details can be found at [http://www.umiacs.umd.edu/labs/pirl/hid/umd-eval.html](http://www.umiacs.umd.edu/labs/pirl/hid/umd-eval.html).
    Further, the dataset is available to download through FTP, requiring credentials
    that can be requested at [http://www.umiacs.umd.edu/labs/pirl/hid/data.html](http://www.umiacs.umd.edu/labs/pirl/hid/data.html).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息请参见[http://www.umiacs.umd.edu/labs/pirl/hid/umd-eval.html](http://www.umiacs.umd.edu/labs/pirl/hid/umd-eval.html)。此外，数据集可以通过FTP下载，下载需要凭证，凭证可以在[http://www.umiacs.umd.edu/labs/pirl/hid/data.html](http://www.umiacs.umd.edu/labs/pirl/hid/data.html)申请。
- en: 4.4\. CASIA
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. CASIA
- en: 'The Institute of Automation, Chinese Academy of Sciences (CASIA) provides the
    CASIA Gait Database (Zheng et al., [2011](#bib.bib138)), a collection of four
    datasets designed for gait recognition purposes, and described as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 中国科学院自动化研究所（CASIA）提供了CASIA步态数据库（Zheng et al., [2011](#bib.bib138)），这是一个为步态识别目的设计的四个数据集的集合，描述如下：
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CASIA A: Created in December 2001 and formerly known as the NLPR Gait Database (Wang
    et al., [2003](#bib.bib115)), the CASIA A dataset includes 20 individuals, each
    of them comprising $12$ videos, i.e., $4$ videos for each of the $3$ directions,
    namely parallel, $45$ and $90$ degrees to the image plane. Besides, each image
    sequence possesses its duration, varying with the individual walking velocity.
    Figure [8](#S4.F8 "Figure 8 ‣ 4.4\. CASIA ‣ 4\. Datasets ‣ Gait Recognition Based
    on Deep Learning: A Survey") shows some image examples from different angles.
    The dataset’s total size is approximately 2.2GB;'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CASIA A：创建于2001年12月，前身为NLPR步态数据库（Wang et al., [2003](#bib.bib115)），CASIA A数据集包含20名个体，每人包括$12$个视频，即$3$个方向中的每个方向$4$个视频，分别是平行、$45$度和$90$度于图像平面。此外，每个图像序列具有不同的持续时间，随着个体行走速度的不同而变化。图[8](#S4.F8
    "图 8 ‣ 4.4\. CASIA ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")展示了来自不同角度的一些图像示例。数据集的总大小约为2.2GB；
- en: •
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CASIA B: Created in 2005, the CASIA B (Yu et al., [2006](#bib.bib129)) comprises
    filming $124$ individuals from $11$ different angles. Each sequence was repeated
    three times, with variations such as clothing and walking speed. Moreover, the
    dataset also comprises a set of silhouettes provided for all sequences, provided
    for foreground segmentation. Figures [9](#S4.F9 "Figure 9 ‣ 4.4\. CASIA ‣ 4\.
    Datasets ‣ Gait Recognition Based on Deep Learning: A Survey") and [10](#S4.F10
    "Figure 10 ‣ 4.4\. CASIA ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning:
    A Survey") show variations in angle and clothing.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CASIA B：创建于2005年，CASIA B（Yu et al., [2006](#bib.bib129)）包含$124$名个体，从$11$个不同角度拍摄。每个序列重复三次，并且存在如服装和行走速度等变化。此外，数据集还包含一组为所有序列提供的轮廓图，用于前景分割。图[9](#S4.F9
    "图 9 ‣ 4.4\. CASIA ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")和[10](#S4.F10 "图 10 ‣ 4.4\. CASIA
    ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")展示了角度和服装的变化。
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CASIA C: Acquired in 2005, the CASIA C (Tan et al., [2006](#bib.bib101)) dataset
    contains $153$ subjects filmed by infrared cameras (thermal spectrum) in four
    different variations: normal walking, slow walking, fast walking, and normal walking
    carrying a backpack. Figure [11](#S4.F11 "Figure 11 ‣ 4.4\. CASIA ‣ 4\. Datasets
    ‣ Gait Recognition Based on Deep Learning: A Survey") show some examples. All
    images were captured at night;'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CASIA C：该数据集于2005年获得（Tan et al., [2006](#bib.bib101)），包含$153$名受试者，使用红外摄像机（热谱）拍摄，分为四种不同的变化：正常行走、慢速行走、快速行走和背着背包的正常行走。图[11](#S4.F11
    "图 11 ‣ 4.4\. CASIA ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")展示了一些示例。所有图像均在夜间拍摄；
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CASIA D: The CASIA gait–footprint dataset (Zheng et al., [2012](#bib.bib137))
    contains both images and cumulative foot pressure information. The dataset comprises
    $3,496$ gait pose images and $2,658$ cumulative foot pressure images from $88$
    individuals with a broad distribution of age, $20$ female and $68$ male, in an
    indoor environment. Figure [12](#S4.F12 "Figure 12 ‣ 4.4\. CASIA ‣ 4\. Datasets
    ‣ Gait Recognition Based on Deep Learning: A Survey") illustrates the data acquisition
    schematics.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CASIA D: CASIA步态–足迹数据集 (Zheng et al., [2012](#bib.bib137))包含图像和累计足部压力信息。该数据集包括$3,496$张步态姿势图像和$2,658$张累计足部压力图像，来自$88$名个人，年龄分布广泛，包括$20$名女性和$68$名男性，数据采集于室内环境。图 [12](#S4.F12
    "图 12 ‣ 4.4\. CASIA ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")展示了数据采集示意图。'
- en: Silhouette for datasets A, B, and C are freely available for download⁵⁵5http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp.
    Regarding data acquisition, candidates should fill a form and wait for the dataset
    owner’s approval.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集A、B和C的轮廓图可以免费下载⁵⁵5http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp。关于数据获取，申请者需填写表格并等待数据集所有者的批准。
- en: '| ![Refer to caption](img/7392657dedac751622e650b0515e173b.png) | ![Refer to
    caption](img/0be4b0fb416757eda5b55374bdf00800.png) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/7392657dedac751622e650b0515e173b.png) | ![参考说明](img/0be4b0fb416757eda5b55374bdf00800.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: '| ![Refer to caption](img/f49ac6bf0100e0ec9960291c76a1361d.png) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/f49ac6bf0100e0ec9960291c76a1361d.png) |'
- en: '| (c) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| (c) |'
- en: Figure 8\. Example frames from CASIA A dataset from each angle. (a) corresponds
    to the parallel view, (b) corresponds to $90$ degrees, and (c) is an example from
    $45$ degrees. Images collectd from [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. CASIA A数据集的示例帧，展示了每个角度的图像。(a) 对应于平行视图，(b) 对应于$90$度，(c) 是$45$度的示例。图像摘自 [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
- en: '![Refer to caption](img/a2eb66397300dcc15de0deead7d7c960.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a2eb66397300dcc15de0deead7d7c960.png)'
- en: Figure 9\. Example frames from CASIA B dataset. Each image corresponds to one
    of the $11$ comprised angles. Extracted from [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. CASIA B数据集中的示例帧。每张图像对应$11$个角度之一。图像摘自 [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
- en: '| ![Refer to caption](img/50eecb507242f5f6ca8f0c3d61575182.png) | ![Refer to
    caption](img/a2f0f2d6beaf2b3a15e0a072cc91d781.png) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/50eecb507242f5f6ca8f0c3d61575182.png) | ![参考说明](img/a2f0f2d6beaf2b3a15e0a072cc91d781.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: '| ![Refer to caption](img/a92572586adc174391f6bd908677cdf0.png) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/a92572586adc174391f6bd908677cdf0.png) |'
- en: '| (c) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| (c) |'
- en: Figure 10\. Example frames from CASIA B dataset. One can notice the change in
    clothes and personal objects, like backpacks. Images collectd from [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. CASIA B数据集中的示例帧。可以注意到衣物和个人物品（如背包）的变化。图像摘自 [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
- en: '![Refer to caption](img/f2a66f16d67b81f3d10ec41d1ef1ffa2.png) ![Refer to caption](img/5f6773e308a9b7a9df61b8c09a5662e9.png)  (a)
    (b)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f2a66f16d67b81f3d10ec41d1ef1ffa2.png) ![参考说明](img/5f6773e308a9b7a9df61b8c09a5662e9.png)  (a)
    (b)'
- en: Figure 11\. Example frames from CASIA C dataset. The images were obtained through
    an infrared camera at night and with variations in the manner of walking. Adapted
    from [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. CASIA C数据集中的示例帧。这些图像通过红外摄像机在夜间获取，并且行走方式有所变化。图像摘自 [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
- en: '![Refer to caption](img/6be7e17ad5bc5ffb15a2b2e37a23c0ee.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6be7e17ad5bc5ffb15a2b2e37a23c0ee.png)'
- en: Figure 12\. Example frames from CASIA D dataset. The images illustrate the image’s
    acquisition process synchronously with the footsteps data. Image collectd from [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. CASIA D数据集中的示例帧。这些图像同步展示了图像采集过程和脚步数据。图像摘自 [http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.](http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.)
- en: 4.5\. OU-ISIR Biometric Database
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. OU-ISIR生物特征数据库
- en: 'The Institute of Scientific and Industrial Research (ISIR) of Osaka University
    (OU) has been creating, since 2007, the largest dataset for gait recognition in
    the world. The project is an aggregation of eight different groups:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 大阪大学（OU）的科学与工业研究所（ISIR）自2007年以来创建了全球最大的步态识别数据集。该项目由八个不同的组共同组成：
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Treadmill Dataset: The group is composed of sequences from people walking on
    electronic treadmills surrounded by $25$ cameras filming at $60$ frames per second
    at a resolution of $640\times 480$. It has $4$ subdivisions:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跑步机数据集：该组由人在电子跑步机上行走的序列组成，周围有$25$台摄像机以$60$帧每秒的速度拍摄，分辨率为$640\times 480$。它有$4$个子集：
- en: (1)
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Treadmill dataset A – Speed variation (Tsuji et al., [2010](#bib.bib105)):
    this subset comprises $34$ subjects in a lateral vision with speed varying between
    $2$ and $10$km/h in $1$hm/h intervals;'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跑步机数据集 A – 速度变化（Tsuji et al., [2010](#bib.bib105)）：该子集包含$34$名受试者的侧视图，速度在$2$到$10$
    km/h之间变化，步长为$1$ km/h；
- en: (2)
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: 'Treadmill dataset B – Clothes variation (Hossain et al., [2010](#bib.bib35)):
    It is composed of $68$ people in lateral vision with $32$ clothing variations;'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跑步机数据集 B – 服装变化（Hossain et al., [2010](#bib.bib35)）：包含$68$名侧视人员，具有$32$种服装变化；
- en: (3)
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （3）
- en: 'Treadmill dataset C – View variation (Makihara et al., [2010](#bib.bib61)):
    A large-scale database comprising $168$ people with ages ranging from $4$ to $75$
    years old. Moreover, the subset is composed of 25 views observed through a multi-view
    synchronous gait system;'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跑步机数据集 C – 视角变化（Makihara et al., [2010](#bib.bib61)）：一个大规模数据库，包含$168$名年龄范围从$4$岁到$75$岁的人员。此外，该子集包含通过多视角同步步态系统观察的$25$个视角；
- en: (4)
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: 'Treadmill dataset D – Gait fluctuation (Mori et al., [2010](#bib.bib67)): this
    set is composed of gait silhouette sequences with $185$ subjects, viewed from
    a lateral angle and with variations in velocity. The data were subdivided into
    two groups of $100$ subjects (with an overlap of 15 people) by high and low speed
    variations.'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跑步机数据集 D – 步态波动（Mori et al., [2010](#bib.bib67)）：该数据集包含$185$名受试者的步态轮廓序列，从侧面角度观察，速度有波动。数据分为两个组，每组$100$名受试者（有15人重叠），分别为高速度波动和低速度波动。
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Large Population Dataset (Iwama et al., [2012](#bib.bib37)): Collected since
    2009 through outreach activity events, the Large Population Dataset is composed
    of $4,016$ subjects, each of them filmed twice from $4$ camera angles at $30$
    FPS and a resolution of $640\times 480$ pixels.'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大规模人口数据集（Iwama et al., [2012](#bib.bib37)）：自2009年起通过外展活动收集，该数据集包含$4,016$名受试者，每位受试者从$4$个摄像机角度以$30$
    FPS的帧率和$640\times 480$像素的分辨率拍摄两次。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Speed Transition Dataset (Mansur et al., [2014](#bib.bib62)): The Speed Transition
    Dataset comprises two subsets, described as follows:'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 速度过渡数据集（Mansur et al., [2014](#bib.bib62)）：速度过渡数据集包含两个子集，如下所述：
- en: (1)
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Dataset A: It contains $179$ scenes from people walking at a constant velocity
    of $4$km/h on a treadmill or the ground. In this set, the background has been
    removed using a wallpaper;'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集 A：包含$179$个场景，来自在跑步机或地面上以$4$ km/h的恒定速度行走的人员。在这个数据集中，背景通过壁纸被移除；
- en: (2)
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: 'Dataset B: It comprises sequences from $25$ people walking on a treadmill with
    a velocity varying between $1$ and $5$ km/h. Each person is filmed twice. Acceleration
    and deacceleration are performed in three seconds, and middle sequences with one
    second were extracted from both.'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集 B：包含$25$名人员在跑步机上行走的序列，速度在$1$到$5$ km/h之间变化。每人拍摄两次。加速和减速在三秒钟内完成，从中提取了一秒钟的中间序列。
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-view Large Population Dataset (Takemura et al., [2018](#bib.bib100)):
    This dataset is composed of $10,307$ samples such that $5,114$ regards men and
    the remaining $5,193$ stand for women, whose age ranges from $2$ to $87$ years,
    developed for motion recognition methods with cross-vision. The images were filmed
    in $14$ different angles, at a frame rate of $25$ frames per second and a resolution
    of $1280\times 980$. The devices employed for capture were placed at a lateral
    distance and height of $8$ and $5$ meters, respectively.'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多视角大规模人口数据集（Takemura et al., [2018](#bib.bib100)）：该数据集包含$10,307$个样本，其中$5,114$个是男性，$5,193$个是女性，年龄范围从$2$岁到$87$岁，旨在用于具有交叉视角的动作识别方法。图像在$14$个不同角度拍摄，帧率为$25$帧每秒，分辨率为$1280\times
    980$。捕捉设备的横向距离和高度分别为$8$米和$5$米。
- en: •
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Large Population Dataset with Bag (Uddin et al., [2018](#bib.bib107)): The
    dataset focuses on gait recognition concerning people carrying objects, aiming
    not only to rely on biometrics information but also on identifying the position
    of the transported part (if any) regarding the body. The Large Population Dataset
    with Bag comprises $62,528$ people aged from $2$ to $95$ years obtained through
    a camera at a distance of approximately $8$ meters and $5$ meters height. The
    sequences were filmed at $25$ frames per second with a resolution of $1280\times
    980$ pixels. Each person was filmed three times, such that the first, i.e., A1,
    is carrying or not an object, while the second and third do not bring anything.
    Finally, a total of four regions are marked in case of something being carried,
    i.e., lower side, upper side, front, and back. All videos are also presented with
    a respective binary mask for background removal.'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Large Population Dataset with Bag (Uddin et al., [2018](#bib.bib107))：该数据集关注携带物品的步态识别，旨在不仅依赖生物特征信息，还识别所携带物品相对于身体的位置（如有的话）。Large
    Population Dataset with Bag 包含了$62,528$名年龄从$2$到$95$岁的个体，通过摄像机在约$8$米距离和$5$米高度处获取。序列以$25$帧每秒的速度拍摄，分辨率为$1280\times
    980$像素。每个人拍摄了三次，其中第一次，即A1，携带或不携带物品，而第二次和第三次均未携带任何物品。最后，在携带物品的情况下，标记了四个区域，即下侧、上侧、前侧和背侧。所有视频还提供了背景去除的二值化掩模。
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Large Population Dataset with Age (Xu et al., [2017](#bib.bib122)): The Large
    Population Dataset with Age was created to investigate gait recognition concerning
    people’s age and gender. The dataset comprises $62,846$ individuals walking on
    a particular path with cameras capturing $640\times 480$ pixels’ resolution at
    a rate of $30$ frames per second. The sequences’ people are between $2$ and $90$
    years old, and all videos have binary masks obtained after the background removal.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Large Population Dataset with Age (Xu et al., [2017](#bib.bib122))：Large Population
    Dataset with Age 旨在研究与年龄和性别相关的步态识别。该数据集包含了$62,846$名在特定路径上行走的个体，摄像机以$640\times
    480$像素分辨率，每秒$30$帧的速度进行拍摄。序列中的人物年龄在$2$到$90$岁之间，所有视频均配有在背景去除后获得的二值化掩模。
- en: •
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inertial Sensor Dataset (Ngo et al., [2014](#bib.bib69)): Designated for research
    and evaluation of methods of individual identification by movements through motion
    sensors and accelerometers, the Inertial Sensor Dataset is the largest inertial
    sensor-based gait database, composed of images collected from $744$ subjects ($389$
    males and $355$ females) whose ages range from $2$ to $78$ years.'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Inertial Sensor Dataset (Ngo et al., [2014](#bib.bib69))：为研究和评估通过运动传感器和加速度计进行个体识别的方法而设计，Inertial
    Sensor Dataset 是最大的基于惯性传感器的步态数据库，由来自$744$名受试者（$389$名男性和$355$名女性）的图像组成，年龄范围为$2$到$78$岁。
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Similar Actions Inertial Dataset (Ngo et al., [2015](#bib.bib70)): The Similar
    Actions Inertial Dataset comprises $460$ participants aged between $8$ and $78$
    with gender virtually equal distributed, whose walking properties were obtained
    together with the data presented in (Ngo et al., [2014](#bib.bib69)). Additionally,
    this dataset also presents six distinct characteristics of the floor: invalid,
    flat, stair climbing, stair climbing, ramp climbing, and ramp descent.'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Similar Actions Inertial Dataset (Ngo et al., [2015](#bib.bib70))：Similar Actions
    Inertial Dataset 包含了$460$名年龄在$8$到$78$岁之间的参与者，性别几乎均等分布，他们的步态特征与(Ngo et al., [2014](#bib.bib69))中的数据一同获取。此外，该数据集还展示了地板的六种不同特性：无效、平坦、楼梯上升、楼梯下降、坡道上升和坡道下降。
- en: 4.6\. University of South Florida Dataset
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. University of South Florida Dataset
- en: The University of South Florida (USF) dataset (Sarkar et al., [2005](#bib.bib85))
    comprises $1,870$ sequences from $122$ subjects using two different shoe types.
    The dataset also considers individuals carrying or not a briefcase, diverse surface
    conditions such as grass and concrete, and distinct camera views, i.e., left or
    right viewpoints. The videos are captured at two different time instants filmed
    in outdoor environments
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: University of South Florida (USF) 数据集 (Sarkar et al., [2005](#bib.bib85)) 包含了$1,870$个序列，来自$122$名受试者，使用了两种不同的鞋子类型。数据集还考虑了携带或不携带公文包的个体、不同的表面条件如草地和混凝土，以及不同的摄像机视角，即左视角或右视角。视频在两个不同的时间点拍摄，地点为户外环境。
- en: 4.7\. Southampton Dataset
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7\. Southampton Dataset
- en: 'The Southampton Human ID at a Distance (SOTON) database is a contribution of
    the University of Southampton composed of three major segments⁶⁶6More information
    available at [http://www.eng.usf.edu/cvprg/Gait_Data.html](http://www.eng.usf.edu/cvprg/Gait_Data.html).:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 南安普顿远程人类身份识别（SOTON）数据库是南安普顿大学的一个贡献，由三个主要部分组成⁶⁶6更多信息见 [http://www.eng.usf.edu/cvprg/Gait_Data.html](http://www.eng.usf.edu/cvprg/Gait_Data.html)。
- en: •
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SOTON Small database (Nixon et al., [2001](#bib.bib71)): Comprises $12$ subjects
    walking around an inside track at varying speeds, wearing different shoes and
    clothes, and carrying or not bags;'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOTON 小型数据库（Nixon 等，[2001](#bib.bib71)）：包含 $12$ 名被试在内侧跑道上以不同速度行走，穿着不同的鞋子和衣物，且携带或不携带包。
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SOTON Large database (Shutler et al., [2004](#bib.bib88)): Containing $114$
    subjects walking outside, inside on the laboratory track, and inside in a treadmill.
    Images were filmed from six different angles and provided in a collection with
    more than $5,000$ sequences.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOTON 大型数据库（Shutler 等，[2004](#bib.bib88)）：包含 $114$ 名被试在室外、实验室跑道上以及跑步机上行走。图像从六个不同的角度拍摄，并以超过
    $5,000$ 个序列的形式提供。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SOTON Temporal (Matovski et al., [2010](#bib.bib64)): The data was captured
    using a Multi-Biometric Tunnel, which contains $12$ synchronized cameras to capture
    people’s gait over time. The dataset is composed of dynamic environments, comprising
    distinct background, lighting, walking surface, and position of cameras. The dataset
    includes $25$ subjects ($17$ male and $8$ female) with ages ranging from $20$
    to $55$ years old. Notice they are all filmed barefoot.'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOTON 时间序列（Matovski 等，[2010](#bib.bib64)）：数据通过一个包含 $12$ 台同步摄像机的多生物识别隧道进行捕捉，以捕捉人们步态的变化。数据集包含动态环境，包括不同的背景、光照、步态表面和摄像机位置。数据集包含
    $25$ 名被试（$17$ 名男性和 $8$ 名女性），年龄从 $20$ 岁到 $55$ 岁不等。请注意，他们都是赤脚拍摄的。
- en: 4.8\. AVA Multi-View Dataset for Gait Recognition (AVAMVG)
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8\. AVA 多视角步态识别数据集（AVAMVG）
- en: 'The AVA Multi-View Dataset for Gait Recognition (AVAMVG)⁷⁷7More information
    at: [http://www.uco.es/grupos/ava/node/41](http://www.uco.es/grupos/ava/node/41) (David
    López-Fernández, Francisco J. Madrid-Cuevas, Ángel Carmona-Poyato, Manuel J. Marín-Jiménez
    and Rafael Muñoz-Salinas, [2014](#bib.bib15)) is a database specifically designed
    for 3D-based gait recognition algorithms, which comprises gait images from $20$
    actors depicting different trajectories. The sequences were obtained using cameras
    specifically calibrated for the task, followed by a post-processing step using
    3D image reconstruction algorithms. Besides, each sequence is also provided with
    a respective binary silhouette for segmentation. Finally, the database contains
    $200$ six-channel multi-view videos that can also be employed as $1,200$ single
    view videos, i.e., $6\times 200$. Figure [13](#S4.F13 "Figure 13 ‣ 4.8\. AVA Multi-View
    Dataset for Gait Recognition (AVAMVG) ‣ 4\. Datasets ‣ Gait Recognition Based
    on Deep Learning: A Survey") depicts some samples.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: AVA 多视角步态识别数据集（AVAMVG）⁷⁷7更多信息见： [http://www.uco.es/grupos/ava/node/41](http://www.uco.es/grupos/ava/node/41) （David
    López-Fernández、Francisco J. Madrid-Cuevas、Ángel Carmona-Poyato、Manuel J. Marín-Jiménez
    和 Rafael Muñoz-Salinas，[2014](#bib.bib15)）是一个专为基于 3D 的步态识别算法设计的数据库，包含 $20$ 名演员步态图像，展示了不同的轨迹。这些序列是通过专门为此任务校准的摄像机获得的，随后经过使用
    3D 图像重建算法的后处理步骤。此外，每个序列还附带一个用于分割的二进制轮廓。最后，该数据库包含 $200$ 个六通道多视角视频，这些视频也可以用作 $1,200$
    个单视角视频，即 $6\times 200$。图 [13](#S4.F13 "图 13 ‣ 4.8\. AVA 多视角步态识别数据集（AVAMVG） ‣ 4\.
    数据集 ‣ 基于深度学习的步态识别：综述") 展示了一些样本。
- en: '![Refer to caption](img/fb2c9d7410847bf19d4ee96235c19081.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb2c9d7410847bf19d4ee96235c19081.png)'
- en: Figure 13\. Example of the multiview dataset. The image presents people walking
    in different directions, from multiple points of view. Adapted from [https://www.uco.es/investiga/grupos/ava/node/41.](https://www.uco.es/investiga/grupos/ava/node/41.)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. 多视角数据集示例。该图展示了人们在不同方向行走的情况，从多个视角进行拍摄。改编自 [https://www.uco.es/investiga/grupos/ava/node/41.](https://www.uco.es/investiga/grupos/ava/node/41.)
- en: 4.9\. Kyushu University 4D Gait Database
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9\. 九州大学 4D 步态数据库
- en: 'The Kyushu University 4D Gait Database (KY4D)⁸⁸8Available at [http://robotics.ait.kyushu-u.ac.jp/~yumi/db.html](http://robotics.ait.kyushu-u.ac.jp/~yumi/db.html) (Iwashita
    et al., [2014b](#bib.bib39)) is composed of sequential 3D models and image sequences
    of $42$ subjects walking along four straight and two curved trajectories. The
    videos were recorded by $16$ cameras, at a resolution of $1032\times 776$ pixels,
    and divided into three subsets, described as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 九州大学4D步态数据库（KY4D）⁸⁸8可在 [http://robotics.ait.kyushu-u.ac.jp/~yumi/db.html](http://robotics.ait.kyushu-u.ac.jp/~yumi/db.html) (Iwashita
    et al., [2014b](#bib.bib39)) 上获得，包含$42$名受试者沿四条直线和两条曲线路径行走的连续3D模型和图像序列。视频由$16$台相机录制，分辨率为$1032\times
    776$像素，并分为三个子集，描述如下：
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset A (Straight): It is composed of sequential 3D models and image sequences
    of people walking along straight trajectories. Figure [14](#S4.F14 "Figure 14
    ‣ 1st item ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition
    Based on Deep Learning: A Survey")(a) illustrates the trajectory, described as
    a red arrow. Figure [14](#S4.F14 "Figure 14 ‣ 1st item ‣ 4.9\. Kyushu University
    4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")(b)
    depicts multiple 3D reconstructed models.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集A（直线）：它由沿直线轨迹行走的人的连续3D模型和图像序列组成。图[14](#S4.F14 "Figure 14 ‣ 1st item ‣ 4.9\.
    Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on
    Deep Learning: A Survey")(a) 展示了轨迹，描述为红色箭头。图[14](#S4.F14 "Figure 14 ‣ 1st item
    ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based
    on Deep Learning: A Survey")(b) 描述了多个3D重建模型。'
- en: '| ![Refer to caption](img/a3fba47323a23aee5fd38b7dd03de02c.png) | ![Refer to
    caption](img/157937db7a7b56811195a6fbc83787de.png) |'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![参见说明](img/a3fba47323a23aee5fd38b7dd03de02c.png) | ![参见说明](img/157937db7a7b56811195a6fbc83787de.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: 'Figure 14\. The studio from KY4D Gait Database A: (a) describes the trajectory
    indicated by the arrow and (b) depicts sequential 3D models of a person walking
    straight. Images adapted from [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图14\. KY4D步态数据库A中的工作室： (a) 描述了由箭头指示的轨迹， (b) 描述了一个人直线行走的连续3D模型。图像改编自 [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
- en: •
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Database B (Curve): It comprises image sequences from people walking along
    curved trajectories, as depicted in Figure [15](#S4.F15 "Figure 15 ‣ 2nd item
    ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based
    on Deep Learning: A Survey")(a). The radius r varies either $1.5$m or $3.0$m.
    Figure [15](#S4.F15 "Figure 15 ‣ 2nd item ‣ 4.9\. Kyushu University 4D Gait Database
    ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")(b) depicts
    multiple 3D reconstructed models considering the curve path.'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据库B（曲线）：它包含了沿曲线轨迹行走的人们的图像序列，如图[15](#S4.F15 "Figure 15 ‣ 2nd item ‣ 4.9\. Kyushu
    University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning:
    A Survey")(a)所示。半径r的值为$1.5$m或$3.0$m。图[15](#S4.F15 "Figure 15 ‣ 2nd item ‣ 4.9\.
    Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on
    Deep Learning: A Survey")(b)展示了考虑曲线路径的多个3D重建模型。'
- en: '| ![Refer to caption](img/cd7f109071845571cb6fbbb2bd9745d3.png) | ![Refer to
    caption](img/1dc8bd99fd55bc588964829afb232d01.png) |'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![参见说明](img/cd7f109071845571cb6fbbb2bd9745d3.png) | ![参见说明](img/1dc8bd99fd55bc588964829afb232d01.png)
    |'
- en: '| (a) | (b) |'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (a) | (b) |'
- en: 'Figure 15\. The studio from KY4D Gait Database B: (a) describes the radius
    variation and (b) illustrates sequential 3D models of a person walking along a
    curved trajectory. Images adapted from [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图15\. KY4D步态数据库B中的工作室： (a) 描述了半径的变化， (b) 展示了一个人沿曲线路径行走的连续3D模型。图像改编自 [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
- en: •
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'KY Infrared (IR) Shadow Gait Database: It is composed of time-series shadow
    images of $54$ subjects. As indicated by the walking direction arrow in Figure [16](#S4.F16
    "Figure 16 ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition
    Based on Deep Learning: A Survey")(a), all people walk straight. Figure [16](#S4.F16
    "Figure 16 ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition
    Based on Deep Learning: A Survey")(b) depicts two infrared lights and a camera
    employed to collect the shadow database (Iwashita et al., [2014a](#bib.bib38)).
    The infrared lights were placed obliquely, and the camera was placed on the ceiling
    perpendicular to the ground. A sample result from such captures is observed in
    Figure [17](#S4.F17 "Figure 17 ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\.
    Datasets ‣ Gait Recognition Based on Deep Learning: A Survey").'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'KY 红外（IR）阴影步态数据库：由$54$个受试者的时间序列阴影图像组成。图[16](#S4.F16 "Figure 16 ‣ 4.9\. Kyushu
    University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning:
    A Survey")(a)中所示的步行方向箭头表明所有人都直行。图[16](#S4.F16 "Figure 16 ‣ 4.9\. Kyushu University
    4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")(b)展示了用于收集阴影数据库的两个红外灯和一台相机 (Iwashita
    et al., [2014a](#bib.bib38))。红外灯斜放，相机垂直于地面放置在天花板上。从这些拍摄中观察到的样本结果见图[17](#S4.F17
    "Figure 17 ‣ 4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait Recognition
    Based on Deep Learning: A Survey")。'
- en: '![Refer to caption](img/d4c5c61ef583d1cc3a99de2079c12b42.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4c5c61ef583d1cc3a99de2079c12b42.png)'
- en: Figure 16\. (a) Experimental setting, (b) actual scene. Adapted from [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图16。 (a) 实验设置，(b) 实际场景。改编自 [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
- en: '![Refer to caption](img/f14c94382264873af31d10b66a2261d2.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f14c94382264873af31d10b66a2261d2.png)'
- en: Figure 17\. An example movie from IR shadow images. Image collectd from [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图17。一个来自IR阴影图像的示例电影。图像采集自 [http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.](http://robotics.ait.kyushu-u.ac.jp/~yumi/db/gait_b.html.)
- en: 4.10\. WhuGAIT Datasets
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10\. WhuGAIT 数据集
- en: 'The whuGAIT Datasets (Zou et al., [2018a](#bib.bib141)) were released in 2018
    by the Wuhan University and made available along with the source code and pre-trained
    models to replicate the paper results⁹⁹9Available at https://github.com/qinnzou/Gait-Recognition-Using-Smartphones.
    Unlike other data collections, whuGAIT comprises $3$D accelerometers and $3$-axis
    gyroscope information collected from $118$ people, $20$ of them collected during
    three days, and $98$ collected in just one day. The dataset is divided into $6$
    different sub-sets, according to the desired task:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: whuGAIT 数据集 (Zou et al., [2018a](#bib.bib141))于2018年由武汉大学发布，并提供了源代码和预训练模型以复制论文结果⁹⁹9可在
    https://github.com/qinnzou/Gait-Recognition-Using-Smartphones 上获取。与其他数据集合不同，whuGAIT包含$118$个人收集的$3$D加速度计和$3$轴陀螺仪信息，其中$20$人在三天内收集，$98$人在一天内收集。该数据集根据所需任务划分为$6$个不同的子集：
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #1: composed of $33,104$ samples for training and $3,740$ for testing
    from $118$ individuals, divided into a two-step segmentation.'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #1：由$33,104$个用于训练的样本和$3,740$个用于测试的样本组成，共$118$个人，分为两步分割。'
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #2: similar to Dataset #1, comprises a two-step segmentation dataset
    composed of $49,275$ samples for training and $4,936$ for testing, extracted from
    the $3$ days collected data of $20$ people.'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #2：类似于数据集 #1，包含一个两步分割的数据集，由$49,275$个用于训练的样本和$4,936$个用于测试的样本组成，提取自$20$人的$3$天数据。'
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #3: this subset is divided into time size windows, comprising $2.56$
    seconds for each sample. The set consists of $26,283$ instances used for training
    and $2,991$ employed for testing purposes.'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #3：该子集被划分为时间大小窗口，每个样本为$2.56$秒。该数据集包含$26,283$个用于训练的实例和$2,991$个用于测试的实例。'
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #4: similar to Dataset #3, this subset is divided into time frames
    of $2.56$ seconds, but using the data from $20$ individuals collected during three
    days. The subset comprises $35,373$ for training and $3,941$ for testing purposes.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #4：类似于数据集 #3，该子集被划分为$2.56$秒的时间帧，但使用了来自$20$个人在三天内收集的数据。该子集包含$35,373$个用于训练的样本和$3,941$个用于测试的样本。'
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #5: the subset is employed for authentication purposes. It is composed
    of $74,142$ instances from $118$ people, such that information extracted from
    $98$ individuals is utilized for training, while the remaining $20$ are used for
    validation. The authentication procedure is compounded by a pair of samples from
    one or two different subjects. The instances comprise two-step acceleration and
    gyroscopic data.'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #5：该子集用于身份验证目的。它由$74,142$个实例组成，这些实例来自$118$人，其中$98$个个体的信息用于训练，其余$20$个用于验证。身份验证过程由来自一个或两个不同受试者的样本对构成。实例包括两步加速度和陀螺仪数据。'
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset #6: this subset employs the same structure used in Dataset #5, but
    instead of horizontal align, it uses vertical alignment.'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '数据集 #6：该子集采用与数据集 #5 相同的结构，但使用的是垂直对齐方式，而不是水平对齐。'
- en: 4.11\. Datasets in Usage Contexts
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.11\. 数据集使用背景
- en: 'This section presented the datasets most commonly employed for gait recognition
    and used in the works considered in this survey. Thus, in the sequence, we also
    provide an overview regarding particular aspects, such as usage contexts, acquisition
    environments, and spectrum. Table [3](#S4.T3 "Table 3 ‣ 4.11\. Datasets in Usage
    Contexts ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")
    shows such information for each dataset. The covariates were divided into twelve
    main features, i.e., viewpoint, pace, objects, shoe, clothing, time, surface,
    silhouette, gait fluctuation, treadmill walking, overground walking, and foot
    pressure.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了在步态识别中最常用的数据集，并用于本调查中考虑的工作中。因此，在接下来的内容中，我们还提供了有关特定方面的概述，例如使用背景、采集环境和谱。表
    [3](#S4.T3 "表 3 ‣ 4.11\. 数据集使用背景 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") 显示了每个数据集的信息。协变量被分为十二个主要特征，即视角、步速、物体、鞋子、服装、时间、表面、轮廓、步态波动、跑步机行走、地面行走和足部压力。
- en: Table 3\. Datasets in Usage Contexts.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 数据集使用背景。
- en: '| Context/Dataset | [4.1](#S4.SS1 "4.1\. CMU MoBo Dataset ‣ 4\. Datasets ‣
    Gait Recognition Based on Deep Learning: A Survey") | [4.2](#S4.SS2 "4.2\. TUM
    GAID Dataset ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")
    | [4.3](#S4.SS3 "4.3\. HID-UMD Dataset ‣ 4\. Datasets ‣ Gait Recognition Based
    on Deep Learning: A Survey") | [4.4](#S4.SS4 "4.4\. CASIA ‣ 4\. Datasets ‣ Gait
    Recognition Based on Deep Learning: A Survey") | [4.5](#S4.SS5 "4.5\. OU-ISIR
    Biometric Database ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A
    Survey") | [4.6](#S4.SS6 "4.6\. University of South Florida Dataset ‣ 4\. Datasets
    ‣ Gait Recognition Based on Deep Learning: A Survey") | [4.6](#S4.SS6 "4.6\. University
    of South Florida Dataset ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning:
    A Survey") | [4.8](#S4.SS8 "4.8\. AVA Multi-View Dataset for Gait Recognition
    (AVAMVG) ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")
    | [4.9](#S4.SS9 "4.9\. Kyushu University 4D Gait Database ‣ 4\. Datasets ‣ Gait
    Recognition Based on Deep Learning: A Survey") | [4.10](#S4.SS10 "4.10\. WhuGAIT
    Datasets ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey")
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 背景/数据集 | [4.1](#S4.SS1 "4.1\. CMU MoBo 数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") |
    [4.2](#S4.SS2 "4.2\. TUM GAID 数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.3](#S4.SS3
    "4.3\. HID-UMD 数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.4](#S4.SS4 "4.4\. CASIA ‣
    4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.5](#S4.SS5 "4.5\. OU-ISIR 生物特征数据库 ‣ 4\. 数据集 ‣
    基于深度学习的步态识别：综述") | [4.6](#S4.SS6 "4.6\. 南佛罗里达大学数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述")
    | [4.6](#S4.SS6 "4.6\. 南佛罗里达大学数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.8](#S4.SS8
    "4.8\. AVA 多视角步态识别数据集 (AVAMVG) ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.9](#S4.SS9 "4.9\.
    九州大学 4D 步态数据库 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") | [4.10](#S4.SS10 "4.10\. WhuGAIT 数据集
    ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Viewpoint | x |  | x | x | x | x | x | x | x | x |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 视角 | x |  | x | x | x | x | x | x | x | x |'
- en: '| Pace | x |  |  | x | x | x | x |  |  | x |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 步速 | x |  |  | x | x | x | x |  |  | x |'
- en: '| Objects | x | x |  | x |  |  | x |  |  |  |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 物体 | x | x |  | x |  |  | x |  |  |  |'
- en: '| Shoe | x | x | x | x | x | x | x | x |  |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 鞋子 | x | x | x | x | x | x | x | x |  |  |'
- en: '| Clothing | x | x | x | x | x |  | x | x |  |  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 服装 | x | x | x | x | x |  | x | x |  |  |'
- en: '| Time |  |  |  | x | x |  | x |  | x | x |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 时间 |  |  |  | x | x |  | x |  | x | x |'
- en: '| Surface | x | x | x | x |  | x | x |  | x | x |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 表面 | x | x | x | x |  | x | x |  | x | x |'
- en: '| Silhouette |  | x |  | x | x |  |  |  | x |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 轮廓 |  | x |  | x | x |  |  |  | x |  |'
- en: '| Gait Fluctuation |  |  |  |  | x |  |  |  |  |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 步态波动 |  |  |  |  | x |  |  |  |  |  |'
- en: '| Treadmill walking | x |  |  |  | x |  | x |  |  |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 跑步机步态 | x |  |  |  | x |  | x |  |  |  |'
- en: '| Overground walking | x | x | x | x |  | x |  | x | x | x |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 地面步态 | x | x | x | x |  | x |  | x | x | x |'
- en: '| Foot pressure |  |  |  | x |  |  |  |  |  |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 足部压力 |  |  |  | x |  |  |  |  |  |  |'
- en: 'Further, Figure [18](#S4.F18 "Figure 18 ‣ 4.11\. Datasets in Usage Contexts
    ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey") presents
    the datasets’ construction environments, which is divided into four scenarios,
    i.e., static indoor, static outdoor, active indoor, and active outdoor.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图 [18](#S4.F18 "图 18 ‣ 4.11\. 使用环境中的数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") 展示了数据集的构建环境，这些环境分为四种情境，即静态室内、静态室外、动态室内和动态室外。
- en: '![Refer to caption](img/9f0a5de036817be4b921a51cc669781a.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f0a5de036817be4b921a51cc669781a.png)'
- en: Figure 18\. Representation of the dataset development environment.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图18\. 数据集开发环境的表示。
- en: 'Finally, Figure [19](#S4.F19 "Figure 19 ‣ 4.11\. Datasets in Usage Contexts
    ‣ 4\. Datasets ‣ Gait Recognition Based on Deep Learning: A Survey") provides
    an illustrated schema categorizing the datasets’ spectrum representation into
    two main classes, i.e., color-based and thermal information.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图 [19](#S4.F19 "图 19 ‣ 4.11\. 使用环境中的数据集 ‣ 4\. 数据集 ‣ 基于深度学习的步态识别：综述") 提供了一个示意图，将数据集的光谱表示分为两类，即基于颜色和热信息。
- en: '![Refer to caption](img/234d9e78113ecdd062e62e8d4b5e4047.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/234d9e78113ecdd062e62e8d4b5e4047.png)'
- en: Figure 19\. Data spectrum representation of the datasets.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图19\. 数据集的数据光谱表示。
- en: 5\. Conclusions and Future Directions
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论与未来方向
- en: This survey provided an in-depth research towards the most significant works
    developed in the last years regarding gait recognition, highlighting deep learning
    techniques for such a task. Besides, it also provided a historical background
    concerning both methods for biometric identification, such as fingerprint, iris,
    and face, among others, as well as gait recognition, exposing the main concerns
    and challenges faced by the field.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述深入研究了近年来在步态识别方面开发的最重要工作，突出了深度学习技术在此任务中的应用。此外，还提供了关于生物识别方法的历史背景，如指纹、虹膜和面部识别等，以及步态识别，揭示了该领域面临的主要关注点和挑战。
- en: Further, it provided a detailed description of the nine most employed datasets
    for the task, i.e., CMU MoBo (Gross and Shi, [2001](#bib.bib25)), TUM GAID (Hofmann
    et al., [2014](#bib.bib34)), HID-UMD (Kale et al., [2002](#bib.bib45); Cuntoor
    et al., [2003](#bib.bib13)), CASIA (Wang et al., [2003](#bib.bib115); Yu et al.,
    [2006](#bib.bib129); Tan et al., [2006](#bib.bib101); Zheng et al., [2012](#bib.bib137)),
    OU-ISIR (Tsuji et al., [2010](#bib.bib105); Hossain et al., [2010](#bib.bib35);
    Makihara et al., [2010](#bib.bib61); Mori et al., [2010](#bib.bib67); Iwama et al.,
    [2012](#bib.bib37); Mansur et al., [2014](#bib.bib62); Takemura et al., [2018](#bib.bib100);
    Uddin et al., [2018](#bib.bib107); Xu et al., [2017](#bib.bib122); Ngo et al.,
    [2014](#bib.bib69), [2014](#bib.bib69)), USF (Sarkar et al., [2005](#bib.bib85)),
    SOTON (Nixon et al., [2001](#bib.bib71); Shutler et al., [2004](#bib.bib88); Matovski
    et al., [2010](#bib.bib64)), AVAMVG (David López-Fernández, Francisco J. Madrid-Cuevas,
    Ángel Carmona-Poyato, Manuel J. Marín-Jiménez and Rafael Muñoz-Salinas, [2014](#bib.bib15)),
    KY4D (Iwashita et al., [2014b](#bib.bib39)), and WhuGait (Zou et al., [2018a](#bib.bib141))
    as well as their extraction process and limitations. Regarding such datasets,
    one can clearly notice the efforts towards an detection independent from gender,
    angle in the scene, clothes and shoes, carry of bags and other itens, among other
    common issues.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，详细描述了九个最常用的数据集，包括 CMU MoBo (Gross 和 Shi, [2001](#bib.bib25))、TUM GAID (Hofmann
    等, [2014](#bib.bib34))、HID-UMD (Kale 等, [2002](#bib.bib45); Cuntoor 等, [2003](#bib.bib13))、CASIA (Wang
    等, [2003](#bib.bib115); Yu 等, [2006](#bib.bib129); Tan 等, [2006](#bib.bib101);
    Zheng 等, [2012](#bib.bib137))、OU-ISIR (Tsuji 等, [2010](#bib.bib105); Hossain 等,
    [2010](#bib.bib35); Makihara 等, [2010](#bib.bib61); Mori 等, [2010](#bib.bib67);
    Iwama 等, [2012](#bib.bib37); Mansur 等, [2014](#bib.bib62); Takemura 等, [2018](#bib.bib100);
    Uddin 等, [2018](#bib.bib107); Xu 等, [2017](#bib.bib122); Ngo 等, [2014](#bib.bib69),
    [2014](#bib.bib69))、USF (Sarkar 等, [2005](#bib.bib85))、SOTON (Nixon 等, [2001](#bib.bib71);
    Shutler 等, [2004](#bib.bib88); Matovski 等, [2010](#bib.bib64))、AVAMVG (David López-Fernández,
    Francisco J. Madrid-Cuevas, Ángel Carmona-Poyato, Manuel J. Marín-Jiménez 和 Rafael
    Muñoz-Salinas, [2014](#bib.bib15))、KY4D (Iwashita 等, [2014b](#bib.bib39)) 和 WhuGait (Zou
    等, [2018a](#bib.bib141)) 及其提取过程和局限性。关于这些数据集，可以清楚地看到在性别、场景角度、服装和鞋子、携带包物等常见问题上努力实现检测独立性。
- en: The works surveyed in this paper discuss the main concerns and efforts made
    by the scientific community towards the development of techniques to guarantee
    access to monitored resources via gait identification and authorization. It also
    explores the advantages of a gait-based system against commonly employed biometric
    features, e.g., gait systems do not require the individuals’ willingness to be
    identified. In this sense, the review attempted to point out the advantages of
    gait biometrics compared to other biometric techniques and highlight the most
    recent methodologies and state-of-the-art architectures.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 本文综述的工作讨论了科学界在开发技术以通过步态识别和授权确保对监测资源的访问方面的主要关注点和努力。它还探讨了基于步态的系统相较于常用生物识别特征的优势，例如步态系统不需要个人的同意就能被识别。在这方面，综述试图指出步态生物识别相对于其他生物识别技术的优势，并突出最新的方法论和最先进的架构。
- en: 'There are a few critical points about video-based models and databases that
    are worth highlighting:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于视频的模型和数据库，有几个关键点值得强调：
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The presented datasets do not introduce more than one person per video, either
    for training or validation purposes.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所展示的数据集每个视频中不包含多于一个人，无论是用于训练还是验证目的。
- en: •
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The environments in which they were recorded are fully controlled. Despite the
    variation of angles in some sets, it is possible to notice no variation of objects
    or even of colors in the background. Others videos were recorded with people walking
    on electric treadmills, denoting even greater control of the movements and the
    environment.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 录制环境完全受控。尽管某些数据集的角度有所变化，但可以注意到背景中的物体或颜色没有变化。其他视频则是在电动跑步机上录制的，显示出对运动和环境的更大控制。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Consequently, the presented models cited in the work are probably prone to show
    lack and misbehavior when applied to the real-world operation, such as detecting
    people of interest walking on public streets.
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，工作中提到的模型在应用于实际操作时可能会表现出不足和误行为，例如在公共街道上检测感兴趣的人。
- en: 'Regarding future trends, we expect an increase in the number of works related
    to:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 关于未来的趋势，我们预期与以下方面相关的工作将会增加：
- en: (1)
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Transformer networks: the technique (Vaswani et al., [2017](#bib.bib108)) targets
    data streaming such as video and fits the context of gait recognition perfectly.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer 网络：该技术（Vaswani 等，[2017](#bib.bib108)）针对数据流如视频，非常适合步态识别的背景。
- en: (2)
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Gender and age recognition: such an approach (Sun et al., [2019](#bib.bib95))
    has gained popularity in the last years due to its importance in several applications.'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性别和年龄识别：这种方法（Sun 等，[2019](#bib.bib95)）由于其在多个应用中的重要性，近年来获得了广泛关注。
- en: (3)
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Hazardous environment monitoring: gait features fit perfectly the purpose of
    monitoring hazardous environments since they usually are composed of limited illumination,
    and extracting more descriptive biometric identification characteristics denotes
    a difficult task (Santana et al., [2019](#bib.bib84)).'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 危险环境监测：步态特征非常适合用于监测危险环境，因为这些环境通常光线有限，而提取更具描述性的生物识别特征是一项困难的任务（Santana 等，[2019](#bib.bib84)）。
- en: (4)
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Multiple people in the scene: most of the gait recognition works focus on a
    single individual in the scene in controlled environments. However, real-life
    problems usually require solutions robust to non-controlled environments with
    multiple people in the scene.'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景中的多个人：大多数步态识别工作集中于受控环境中的单一个体。然而，现实生活中的问题通常需要对非受控环境中多个人的解决方案具有鲁棒性。
- en: Additionally, we observed an increasing demand for gait recognition on individuals
    with different clothes or carrying objects (Yu et al., [2017](#bib.bib128)), as
    well as hybrid approaches composed of both gait and complementary biometric characteristics,
    such as face and ear (Mehraj and Mir, [2020](#bib.bib65)).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到对穿着不同衣物或携带物品的个体进行步态识别的需求不断增加（Yu 等，[2017](#bib.bib128)），以及包括步态和补充生物特征（如面部和耳朵）的混合方法（Mehraj
    和 Mir，[2020](#bib.bib65)）。
- en: Acknowledgements.
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: 'The authors are grateful to São Paulo Research Foundation (Fapesp) grants #2013/07375-0,
    #2014/12236-1, #2019/07665-4, and #2020/12101-0, to the Brazilian National Council
    for Research and Development (CNPq) #307066/2017-7 and #427968/2018-6, Eldorado
    Research Institute, as well as Petroleo Brasileiro S.A. (Petrobras) grant #2017/00285-6.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '作者感谢圣保罗研究基金会（Fapesp）资助 #2013/07375-0，#2014/12236-1，#2019/07665-4 和 #2020/12101-0，巴西国家研究与发展委员会（CNPq）
    #307066/2017-7 和 #427968/2018-6，Eldorado 研究所，以及巴西石油公司（Petrobras）资助 #2017/00285-6。'
- en: References
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: Alariki et al. (2018) Ala Abdulhakim Alariki, Muqadas Faiz, Sanaullah Balagh,
    and Christine Murray. 2018. A Review of Finger-vein Biometric Recognition. In
    *Proceedings of the World Congress on Engineering and Computer Science*, Vol. 1.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alariki 等（2018）Ala Abdulhakim Alariki、Muqadas Faiz、Sanaullah Balagh 和 Christine
    Murray。2018. 指纹静脉生物识别的综述。在*《世界工程与计算机科学大会论文集》*第 1 卷。
- en: 'Alharthi et al. (2019) Abdullah S Alharthi, Syed U Yunas, and Krikor B Ozanyan.
    2019. Deep learning for monitoring of human gait: A review. *IEEE Sensors Journal*
    19, 21 (2019), 9575–9591.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alharthi 等（2019）Abdullah S Alharthi、Syed U Yunas 和 Krikor B Ozanyan。2019. 用于监测人类步态的深度学习：综述。*《IEEE
    传感器期刊》* 19，21（2019），9575–9591。
- en: Alsmirat et al. (2019) Mohammad A Alsmirat, Fatimah Al-Alem, Mahmoud Al-Ayyoub,
    Yaser Jararweh, and Brij Gupta. 2019. Impact of digital fingerprint image quality
    on the fingerprint recognition accuracy. *Multimedia Tools and Applications* 78,
    3 (2019), 3649–3688.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alsmirat 等（2019）Mohammad A Alsmirat、Fatimah Al-Alem、Mahmoud Al-Ayyoub、Yaser
    Jararweh 和 Brij Gupta。2019. 数字指纹图像质量对指纹识别准确度的影响。*《多媒体工具与应用》* 78，3（2019），3649–3688。
- en: Arora et al. (2015) P. Arora, M. Hanmandlu, and S. Srivastava. 2015. Gait based
    authentication using gait information image features. *Pattern Recognition Letters*
    68 (2015), 336–342.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等（2015）P. Arora、M. Hanmandlu 和 S. Srivastava。2015. 基于步态信息图像特征的步态认证。*《模式识别快报》*
    68（2015），336–342。
- en: Babaee et al. (2019) Maryam Babaee, Linwei Li, and Gerhard Rigoll. 2019. Person
    identification from partial gait cycle using fully convolutional neural networks.
    *Neurocomputing* (2019).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babaee 等（2019）Maryam Babaee、Linwei Li 和 Gerhard Rigoll。2019. 使用全卷积神经网络从部分步态周期中识别个人。*《神经计算》*（2019）。
- en: Bolle et al. (2013) Ruud M Bolle, Jonathan H Connell, Sharath Pankanti, Nalini K
    Ratha, and Andrew W Senior. 2013. *Guide to biometrics*. Springer Science & Business
    Media.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolle 等（2013）Ruud M Bolle、Jonathan H Connell、Sharath Pankanti、Nalini K Ratha
    和 Andrew W Senior。2013. *《生物特征指南》*。Springer Science & Business Media。
- en: 'Bouchrika (2018) Imed Bouchrika. 2018. A survey of using biometrics for smart
    visual surveillance: Gait recognition. In *Surveillance in Action*. Springer,
    3–23.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouchrika (2018) Imed Bouchrika. 2018. 通过生物特征识别进行智能视觉监控的调查：步态识别。在*《行动中的监控》*中，Springer，3–23。
- en: Chen et al. (2020) Rung-Ching Chen, Christine Dewi, Wei-Wei Zhang, Jia-Ming
    Liu, Su-Wen Huang, et al. 2020. Integrating gesture control board and image recognition
    for gesture recognition based on deep learning. *International Journal of Applied
    Science and Engineering* 17, 3 (2020), 237–248.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）Rung-Ching Chen、Christine Dewi、Wei-Wei Zhang、Jia-Ming Liu、Su-Wen
    Huang 等。2020. 集成手势控制板和图像识别的深度学习基础上的手势识别。*《应用科学与工程国际期刊》* 17，3（2020），237–248。
- en: 'Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and
    Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder
    approaches. *arXiv preprint arXiv:1409.1259* (2014).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014）Kyunghyun Cho、Bart Van Merriënboer、Dzmitry Bahdanau 和 Yoshua Bengio。2014.
    神经机器翻译的性质：编码器-解码器方法。*arXiv 预印本 arXiv:1409.1259*（2014）。
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. *arXiv preprint arXiv:1412.3555* (2014).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2014）Junyoung Chung、Caglar Gulcehre、KyungHyun Cho 和 Yoshua Bengio。2014.
    门控递归神经网络在序列建模中的实证评估。*arXiv 预印本 arXiv:1412.3555*（2014）。
- en: Costilla-Reyes et al. (2018) Omar Costilla-Reyes, Ruben Vera-Rodriguez, Patricia
    Scully, and Krikor B Ozanyan. 2018. Analysis of spatio-temporal representations
    for robust footstep recognition with deep residual neural networks. *IEEE transactions
    on pattern analysis and machine intelligence* 41, 2 (2018), 285–296.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costilla-Reyes 等（2018）Omar Costilla-Reyes、Ruben Vera-Rodriguez、Patricia Scully
    和 Krikor B Ozanyan。2018. 使用深度残差神经网络进行稳健脚步识别的时空表示分析。*《IEEE 模式分析与机器智能期刊》* 41，2（2018），285–296。
- en: Cuntoor et al. (2003) Naresh Cuntoor, Amit Kale, and Rama Chellappa. 2003. Combining
    multiple evidences for gait recognition. In *2003 IEEE International Conference
    on Acoustics, Speech, and Signal Processing, 2003\. Proceedings.(ICASSP’03).*,
    Vol. 3\. IEEE, III–33.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuntoor等人（2003）Naresh Cuntoor，Amit Kale和Rama Chellappa。 2003. 将多个证据结合起来进行步态识别。
    在*2003年IEEE国际会议上的声学，语音和信号处理，2003年。会议纵横（ICASSP’03）。*，Vol。 3。 IEEE，III–33。
- en: Dargan and Kumar (2020) Shaveta Dargan and Munish Kumar. 2020. A comprehensive
    survey on the biometric recognition systems based on physiological and behavioral
    modalities. *Expert Systems with Applications* 143 (2020), 113114.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dargan和Kumar（2020）Shaveta Dargan和Munish Kumar。 2020. 基于生理和行为模态的生物识别系统全面调查。 *专家系统与应用*
    143（2020），113114。
- en: David López-Fernández, Francisco J. Madrid-Cuevas, Ángel Carmona-Poyato, Manuel
    J. Marín-Jiménez and Rafael Muñoz-Salinas (2014) David López-Fernández, Francisco
    J. Madrid-Cuevas, Ángel Carmona-Poyato, Manuel J. Marín-Jiménez and Rafael Muñoz-Salinas.
    2014. The AVA Multi-View Dataset for Gait Recognition. In *Activity Monitoring
    by Multiple Distributed Sensing*, Pier Luigi Mazzeo, Paolo Spagnolo, and Thomas B.
    Moeslund (Eds.). Springer International Publishing, 26–39. [https://doi.org/10.1007/978-3-319-13323-2_3](https://doi.org/10.1007/978-3-319-13323-2_3)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: David López-Fernández，Francisco J. Madrid-Cuevas，Ángel Carmona-Poyato，Manuel
    J. Marín-Jiménez和Rafael Muñoz-Salinas（2014）David López-Fernández，Francisco J.
    Madrid-Cuevas，Ángel Carmona-Poyato，Manuel J. Marín-Jiménez和Rafael Muñoz-Salinas。
    2014. 用于步态识别的AVA多视图数据集。 在*多个分布式感知的活动监测*，Pier Luigi Mazzeo，Paolo Spagnolo和Thomas B.
    Moeslund（编者）。 Springer International Publishing，26–39。 [https://doi.org/10.1007/978-3-319-13323-2_3](https://doi.org/10.1007/978-3-319-13323-2_3)
- en: de Souza et al. (2021) Renato WR de Souza, Daniel S Silva, Leandro A Passos,
    Mateus Roder, Marcos C Santana, Plácido R Pinheiro, and Victor Hugo C de Albuquerque.
    2021. Computer-Assisted Parkinson’s Disease Diagnosis Using Fuzzy Optimum-Path
    Forest and Restricted Boltzmann Machines. *Computers in Biology and Medicine*
    (2021), 104260.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Souza等人（2021）Renato WR de Souza，Daniel S Silva，Leandro A Passos，Mateus Roder，Marcos C
    Santana，Plácido R Pinheiro和Victor Hugo C de Albuquerque。 2021. 利用模糊最佳路径森林和受限玻尔兹曼机辅助帕金森病诊断。
    *生物医学中的计算*（2021），104260。
- en: Deng et al. (2018a) Muqing Deng et al. 2018a. Gait recognition under different
    clothing conditions via deterministic learning. *in IEEE/CAA Journal of Automatica
    Sinica* (2018).
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等人（2018a）Muqing Deng等人。 2018a. 通过确定性学习在不同服装条件下进行步态识别。 *在IEEE/CAA Journal
    of Automatica Sinica中*（2018）。
- en: 'Deng et al. (2018b) Weihong Deng, Jiani Hu, and Jun Guo. 2018b. Face recognition
    via collaborative representation: Its discriminant nature and superposed representation.
    *IEEE transactions on pattern analysis and machine intelligence* 40, 10 (2018),
    2513–2521.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等人（2018b）Weihong Deng，Jiani Hu和Jun Guo。 2018b. 通过协作表示进行人脸识别：其判别性质和叠加表示。
    *IEEE模式分析和机器智能交易* 40, 10（2018），2513–2521。
- en: 'Deng et al. (2014) Weihong Deng, Jiani Hu, Jiwen Lu, and Jun Guo. 2014. Transform-invariant
    PCA: A unified approach to fully automatic facealignment, representation, and
    recognition. *IEEE transactions on pattern analysis and machine intelligence*
    36, 6 (2014), 1275–1284.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等人（2014）Weihong Deng，Jiani Hu，Jiwen Lu和Jun Guo。 2014. Transform-invariant
    PCA：全自动面部对齐，表示和识别的统一方法。 *IEEE模式分析和机器智能交易* 36, 6（2014），1275–1284。
- en: Dewi and Chen (2019) Christine Dewi and Rung-Ching Chen. 2019. Human activity
    recognition based on evolution of features selection and random Forest. In *2019
    IEEE international conference on systems, man and cybernetics (SMC)*. IEEE, 2496–2501.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dewi和Chen（2019）Christine Dewi和Rung-Ching Chen。 2019. 基于特征选择和随机森林演变的人类活动识别。 在*2019年IEEE系统，人和机器会议（SMC）*。
    IEEE，2496–2501。
- en: e MS Nixon (2011) G. Ariyanto e MS Nixon. 2011. Model-based 3D gait biometrics.
    *in IJCB* (2011).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: e MS Nixon（2011）G. Ariyanto e MS Nixon。 2011. 基于模型的3D步态生物识别。 *在IJCB中*（2011）。
- en: Fernandes et al. (2018) Carlos Fernandes, Luís Fonseca, Flora Ferreira, Miguel
    Gago, Lus Costa, Nuno Sousa, Carlos Ferreira, Joao Gama, Wolfram Erlhagen, and
    Estela Bicho. 2018. Artificial neural networks classification of patients with
    parkinsonism based on gait. In *2018 IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM)*. IEEE, 2024–2030.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes等人（2018）Carlos Fernandes，Luís Fonseca，Flora Ferreira，Miguel Gago，Lus
    Costa，Nuno Sousa，Carlos Ferreira，Joao Gama，Wolfram Erlhagen和Estela Bicho。 2018.
    基于神经网络的帕金森病患者分类。 在*2018 IEEE生物信息学和生物医学国际会议（BIBM）*。 IEEE，2024–2030。
- en: Garagad and Iyer (2014) Vishwanath G Garagad and Nalini C Iyer. 2014. A novel
    technique of iris identification for biometric systems. In *2014 International
    Conference on Advances in Computing, Communications and Informatics (ICACCI)*.
    IEEE, 973–978.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garagad 和 Iyer（2014）Vishwanath G Garagad 和 Nalini C Iyer。2014。用于生物识别系统的新型虹膜识别技术。在*2014
    年国际计算、通信与信息学会议（ICACCI）*。IEEE，973–978。
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    2014. Generative adversarial networks. *arXiv preprint arXiv:1406.2661* (2014).
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2014）Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
    David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio。2014。生成对抗网络。*arXiv
    预印本 arXiv:1406.2661*（2014）。
- en: Gross and Shi (2001) Ralph Gross and Jianbo Shi. 2001. *The CMU Motion of Body
    (MoBo) Database*. Technical Report CMU-RI-TR-01-18. Carnegie Mellon University.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gross 和 Shi（2001）Ralph Gross 和 Jianbo Shi。2001。*CMU 人体运动（MoBo）数据库*。技术报告 CMU-RI-TR-01-18。卡内基梅隆大学。
- en: Gui et al. (2019) Qiong Gui, Maria V Ruiz-Blondet, Sarah Laszlo, and Zhanpeng
    Jin. 2019. A survey on brain biometrics. *ACM Computing Surveys (CSUR)* 51, 6
    (2019), 1–38.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui 等（2019）Qiong Gui, Maria V Ruiz-Blondet, Sarah Laszlo, 和 Zhanpeng Jin。2019。脑部生物特征的综述。*ACM
    计算调查（CSUR）* 51, 6（2019），1–38。
- en: Han and Bhanu (2016) Ju Han and Bir Bhanu. 2016. Individual Recognition UsingGait
    Energy Image. *IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE*
    (2016).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 和 Bhanu（2016）Ju Han 和 Bir Bhanu。2016。使用步态能量图进行个体识别。*IEEE 模式分析与机器智能学报*（2016）。
- en: He et al. (2020) Jing Selena He, Mingyuan Yan, and Sahil Arora. 2020. Long Range
    Iris Recognition a Reality or a Myth? A Survey. In *Proceedings of the 2020 ACM
    Southeast Conference*. 305–306.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2020）Jing Selena He, Mingyuan Yan, 和 Sahil Arora。2020。长距离虹膜识别：现实还是神话？一项调查。在*2020
    年 ACM 东南会议论文集*。305–306。
- en: He et al. (2018) Yiwei He, Junping Zhang, Hongming Shan, and Liang Wang. 2018.
    Multi-task GANs for view-specific feature learning in gait recognition. *IEEE
    Transactions on Information Forensics and Security* 14, 1 (2018), 102–113.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2018）Yiwei He, Junping Zhang, Hongming Shan, 和 Liang Wang。2018。用于步态识别的视角特定特征学习的多任务生成对抗网络（GANs）。*IEEE
    信息取证与安全学报* 14, 1（2018），102–113。
- en: Hemalatha (2020) S Hemalatha. 2020. A systematic review on Fingerprint based
    Biometric Authentication System. In *2020 International Conference on Emerging
    Trends in Information Technology and Engineering (ic-ETITE)*. IEEE, 1–4.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemalatha（2020）S Hemalatha。2020。基于指纹的生物识别认证系统的系统性综述。在*2020 年信息技术与工程新兴趋势国际会议（ic-ETITE）*。IEEE，1–4。
- en: Hinton (2002) G. E. Hinton. 2002. Training Products of Experts by Minimizing
    Contrastive Divergence. *Neural Computation* 14, 8 (2002), 1771–1800.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton（2002）G. E. Hinton。2002。通过最小化对比散度训练专家的产品。*神经计算* 14, 8（2002），1771–1800。
- en: Hinton et al. (2006) G. E. Hinton, S. Osindero, and Y.-W. Teh. 2006. A Fast
    Learning Algorithm for Deep Belief Nets. *Neural Computation* 18, 7 (2006), 1527–1554.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2006）G. E. Hinton, S. Osindero, 和 Y.-W. Teh。2006。深度置信网络的快速学习算法。*神经计算*
    18, 7（2006），1527–1554。
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber（1997）Sepp Hochreiter 和 Jürgen Schmidhuber。1997。长短期记忆。*神经计算*
    9, 8（1997），1735–1780。
- en: 'Hofmann et al. (2014) Martin Hofmann, Jürgen Geiger, Sebastian Bachmann, Björn
    Schuller, and Gerhard Rigoll. 2014. The tum gait from audio, image and depth (gaid)
    database: Multimodal recognition of subjects and traits. *Journal of Visual Communication
    and Image Representation* 25, 1 (2014), 195–206.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofmann 等（2014）Martin Hofmann, Jürgen Geiger, Sebastian Bachmann, Björn Schuller,
    和 Gerhard Rigoll。2014。基于音频、图像和深度（gaid）数据库的肿瘤步态：多模态识别主体和特征。*视觉通信与图像表示期刊* 25, 1（2014），195–206。
- en: Hossain et al. (2010) Md Altab Hossain, Yasushi Makihara, Junqiu Wang, and Yasushi
    Yagi. 2010. Clothing-invariant gait identification using part-based clothing categorization
    and adaptive weight control. *Pattern Recognition* 43, 6 (2010), 2281–2291.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hossain 等（2010）Md Altab Hossain, Yasushi Makihara, Junqiu Wang, 和 Yasushi Yagi。2010。使用基于部分的服装分类和自适应权重控制的服装不变步态识别。*模式识别*
    43, 6（2010），2281–2291。
- en: 'Hu et al. (2018) BingZhang Hu, Yu Guan, Yan Gao, Yang Long, Nicholas Lane,
    and Thomas Ploetz. 2018. Robust Cross-View Gait Recognition with Evidence: A Discriminant
    Gait GAN (DiGGAN) Approach. *arXiv preprint arXiv:1811.10493* (2018).'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2018）BingZhang Hu, Yu Guan, Yan Gao, Yang Long, Nicholas Lane, 和 Thomas
    Ploetz。2018。具有证据的鲁棒跨视角步态识别：一种判别步态生成对抗网络（DiGGAN）方法。*arXiv 预印本 arXiv:1811.10493*（2018）。
- en: Iwama et al. (2012) H. Iwama, M. Okumura, Y. Makihara, and Y. Yagi. 2012. The
    OU-ISIR Gait Database Comprising the Large Population Dataset and Performance
    Evaluation of Gait Recognition. *IEEE Trans. on Information Forensics and Security*
    7, Issue 5 (Oct. 2012), 1511–1521.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iwama 等人（2012）H. Iwama、M. Okumura、Y. Makihara 和 Y. Yagi。2012. 包含大规模数据集的 OU-ISIR
    步态数据库及步态识别的性能评估。*IEEE Trans. on Information Forensics and Security* 7，第 5 期（2012
    年 10 月），1511–1521。
- en: 'Iwashita et al. (2014a) Y. Iwashita, R. Kurazume, and A. Stoica. 2014a. Gait
    Identification Using Invisible Shadows: Robustness to Appearance Changes. In *Int.
    Conf. Emerging Security Technologies (EST)*. UK.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iwashita 等人（2014a）Y. Iwashita、R. Kurazume 和 A. Stoica。2014a. 使用隐形阴影进行步态识别：对外观变化的鲁棒性。见于*Int.
    Conf. Emerging Security Technologies (EST)*。英国。
- en: Iwashita et al. (2014b) Y. Iwashita, K. Ogawara, and R. Kurazume. 2014b. Identification
    of people walking along curved trajectories. In *Pattern Recognition Letters*.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iwashita 等人（2014b）Y. Iwashita、K. Ogawara 和 R. Kurazume。2014b. 识别沿曲线路径行走的人。见于*Pattern
    Recognition Letters*。
- en: Jain et al. (2004) Anil K Jain et al. 2004. An Introduction to Biometric Recognition.
    *in IEEE Transactions on Circuits and Systems for Video Technology, Special Issue
    on Image- and Video-Based Biometrics, Vol. 14, No. 1* (2004).
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等人（2004）Anil K Jain 等人。2004. 生物识别介绍。*见于 IEEE Transactions on Circuits and
    Systems for Video Technology, Special Issue on Image- and Video-Based Biometrics,
    Vol. 14, No. 1*（2004）。
- en: Jayaraman et al. (2020) Umarani Jayaraman, Phalguni Gupta, Sandesh Gupta, Geetika
    Arora, and Kamlesh Tiwari. 2020. Recent development in face recognition. *Neurocomputing*
    408 (2020), 231–245.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jayaraman 等人（2020）Umarani Jayaraman、Phalguni Gupta、Sandesh Gupta、Geetika Arora
    和 Kamlesh Tiwari。2020. 面部识别的最新进展。*Neurocomputing* 408（2020），231–245。
- en: Jia et al. (2019) Meijuan Jia, Hongyu Yang, Di Huang, and Yunhong Wang. 2019.
    Attacking gait recognition systems via silhouette guided GANs. In *Proceedings
    of the 27th ACM International Conference on Multimedia*. 638–646.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等人（2019）Meijuan Jia、Hongyu Yang、Di Huang 和 Yunhong Wang。2019. 通过轮廓引导的 GAN
    攻击步态识别系统。见于*Proceedings of the 27th ACM International Conference on Multimedia*，638–646。
- en: Jing Luo and Xiu (2015) Chunyuan Zi Ying Niu Huixin Tian Jing Luo, Jianliang Zhang
    and Chunbo Xiu. 2015. Gait Recognition Using GEI and AFDEI. *International Journal
    of Optics* (2015).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing Luo 和 Xiu（2015）Chunyuan Zi Ying Niu Huixin Tian Jing Luo、Jianliang Zhang
    和 Chunbo Xiu。2015. 基于 GEI 和 AFDEI 的步态识别。*International Journal of Optics*（2015）。
- en: Jun et al. (2020) K. Jun, D. Lee, K. Lee, S. Lee, and M. S. Kim. 2020. Feature
    Extraction Using an RNN Autoencoder for Skeleton-Based Abnormal Gait Recognition.
    *IEEE Access* 8 (2020), 19196–19207.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jun 等人（2020）K. Jun、D. Lee、K. Lee、S. Lee 和 M. S. Kim。2020. 基于 RNN 自编码器的骨架异常步态识别特征提取。*IEEE
    Access* 8（2020），19196–19207。
- en: Kale et al. (2002) Amit Kale, Naresh Cuntoor, and Rama Chellappa. 2002. A framework
    for activity-specific human identification. In *2002 IEEE International Conference
    on Acoustics, Speech, and Signal Processing*, Vol. 4\. IEEE, IV–3660.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kale 等人（2002）Amit Kale、Naresh Cuntoor 和 Rama Chellappa。2002. 针对特定活动的人类识别框架。见于*2002
    IEEE International Conference on Acoustics, Speech, and Signal Processing*，第 4
    卷。IEEE，IV–3660。
- en: Kan et al. (2014) Meina Kan, Shiguang Shan, Hong Chang, and Xilin Chen. 2014.
    Stacked progressive auto-encoders (spae) for face recognition across poses. In
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    1883–1890.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kan 等人（2014）Meina Kan、Shiguang Shan、Hong Chang 和 Xilin Chen。2014. 用于姿态识别的堆叠渐进自编码器（spae）。见于*Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*，1883–1890。
- en: Karn et al. (2020) Pradeep Karn, XiaoHai He, Jin Zhang, and Yanteng Zhang. 2020.
    An experimental study of relative total variation and probabilistic collaborative
    representation for iris recognition. *Multimedia Tools and Applications* 79, 43
    (2020), 31783–31801.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karn 等人（2020）Pradeep Karn、XiaoHai He、Jin Zhang 和 Yanteng Zhang。2020. 绝对总变化和概率协同表示在虹膜识别中的实验研究。*Multimedia
    Tools and Applications* 79，43（2020），31783–31801。
- en: 'Khan et al. (2020) Saad Khan, Simon Parkinson, Liam Grant, Na Liu, and Stephen
    Mcguire. 2020. Biometric systems utilising health data from wearable devices:
    applications and future challenges in computer security. *ACM Computing Surveys
    (CSUR)* 53, 4 (2020), 1–29.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等人（2020）Saad Khan、Simon Parkinson、Liam Grant、Na Liu 和 Stephen Mcguire。2020.
    利用可穿戴设备健康数据的生物识别系统：计算机安全中的应用和未来挑战。*ACM Computing Surveys (CSUR)* 53, 4（2020），1–29。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Advances
    in neural information processing systems*. 1097–1105.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人（2012）Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton。2012.
    使用深度卷积神经网络进行 Imagenet 分类。见于*Advances in neural information processing systems*，1097–1105。
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner,
    et al. 1998. Gradient-based learning applied to document recognition. *Proc. IEEE*
    86, 11 (1998), 2278–2324.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner 等。1998。基于梯度的学习应用于文档识别。*Proc.
    IEEE* 86, 11 (1998), 2278–2324。
- en: 'Lee et al. (2017) Wonjune Lee, Sungchul Cho, Heeseung Choi, and Jaihie Kim.
    2017. Partial fingerprint matching using minutiae and ridge shape features for
    small fingerprint scanners. *Expert Systems with Applications: An International
    Journal* 87, C (2017), 183–198.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等 (2017) Wonjune Lee, Sungchul Cho, Heeseung Choi 和 Jaihie Kim。2017。基于细节和脊形特征的小型指纹扫描仪的部分指纹匹配。*Expert
    Systems with Applications: An International Journal* 87, C (2017), 183–198。'
- en: Lei et al. (2014) Zhen Lei, Matti Pietikäinen, and Stan Z Li. 2014. Learning
    discriminant face descriptor. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence* 36, 2 (2014), 289–302.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等 (2014) Zhen Lei, Matti Pietikäinen 和 Stan Z Li。2014。学习判别性面部描述符。*IEEE Transactions
    on Pattern Analysis and Machine Intelligence* 36, 2 (2014), 289–302。
- en: 'Li et al. (2017) C. Li, X. Min, S. Sun, W. Lin, and Z. Tang. 2017. Deepgait:
    A learning deep convolutional representation for view-invariant gait recognition
    using joint bayesian. *Applied Sciences* 7, 3 (2017), 210.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2017) C. Li, X. Min, S. Sun, W. Lin 和 Z. Tang。2017。Deepgait: 使用联合贝叶斯的视角不变步态识别的深度卷积表示学习。*Applied
    Sciences* 7, 3 (2017), 210。'
- en: Li et al. (2021) Shuyi Li, Bob Zhang, Lunke Fei, and Shuping Zhao. 2021. Joint
    discriminative feature learning for multimodal finger recognition. *Pattern Recognition*
    111 (2021), 107704.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2021) Shuyi Li, Bob Zhang, Lunke Fei 和 Shuping Zhao。2021。多模态指纹识别的联合判别特征学习。*Pattern
    Recognition* 111 (2021), 107704。
- en: 'Li (2009) Stan Z Li. 2009. *Encyclopedia of Biometrics: I-Z.* Vol. 2. Springer
    Science & Business Media.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li (2009) Stan Z Li。2009。*Encyclopedia of Biometrics: I-Z.* 第2卷。Springer Science
    & Business Media。'
- en: Lobo et al. (2020) V. C. Lobo, L. A. Passos, and J. P. Papa. 2020. Evolving
    Long Short-Term Memory Networks. In *International Conference on Computational
    Science (ICCS)*. IEEE.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lobo 等 (2020) V. C. Lobo, L. A. Passos 和 J. P. Papa。2020。演变的长短期记忆网络。在*International
    Conference on Computational Science (ICCS)*会议上。IEEE。
- en: Luo and Tjahjadi (2020) Jian Luo and Tardi Tjahjadi. 2020. Multi-Set Canonical
    Correlation Analysis for 3D Abnormal Gait Behaviour Recognition Based on Virtual
    Sample Generation. *IEEE Access* 8 (2020), 32485–32501.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 和 Tjahjadi (2020) Jian Luo 和 Tardi Tjahjadi。2020。基于虚拟样本生成的三维异常步态行为识别的多集典型相关分析。*IEEE
    Access* 8 (2020), 32485–32501。
- en: Luo et al. (2016) Zhengping Luo, Tianqi Yang, and Yanjun Liu. 2016. Gait optical
    flow image decomposition for human recognition. In *2016 IEEE Information Technology,
    Networking, Electronic and Automation Control Conference*. IEEE, 581–586.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等 (2016) Zhengping Luo, Tianqi Yang 和 Yanjun Liu。2016。用于人类识别的步态光流图像分解。在*2016
    IEEE Information Technology, Networking, Electronic and Automation Control Conference*会议上。IEEE,
    581–586。
- en: 'Ma et al. (2019) Zhuo Ma, Yilong Yang, Ximeng Liu, Yang Liu, Siqi Ma, Kui Ren,
    and Chang Yao. 2019. Emir-auth: Eye-movement and iris based portable remote authentication
    for smart grid. *IEEE Transactions on Industrial Informatics* (2019).'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2019) Zhuo Ma, Yilong Yang, Ximeng Liu, Yang Liu, Siqi Ma, Kui Ren 和
    Chang Yao。2019。Emir-auth: 基于眼动和虹膜的便携式远程认证系统。*IEEE Transactions on Industrial Informatics*
    (2019)。'
- en: Makihara et al. (2012) Y. Makihara, H. Mannami, A. Tsuji, M.A. Hossain, K. Sugiura,
    A. Mori, and Y. Yagi. 2012. The OU-ISIR Gait Database Comprising the Treadmill
    Dataset. *IPSJ Trans. on Computer Vision and Applications* 4 (Apr. 2012), 53–62.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makihara 等 (2012) Y. Makihara, H. Mannami, A. Tsuji, M.A. Hossain, K. Sugiura,
    A. Mori 和 Y. Yagi。2012。包含跑步机数据集的 OU-ISIR 步态数据库。*IPSJ Trans. on Computer Vision
    and Applications* 4 (2012年4月), 53–62。
- en: Makihara et al. (2010) Yasushi Makihara, Hidetoshi Mannami, and Yasushi Yagi.
    2010. Gait analysis of gender and age using a large-scale multi-view gait database.
    In *Asian Conference on Computer Vision*. Springer, 440–451.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makihara 等 (2010) Yasushi Makihara, Hidetoshi Mannami 和 Yasushi Yagi。2010。使用大规模多视角步态数据库进行性别和年龄的步态分析。在*Asian
    Conference on Computer Vision*会议上。Springer, 440–451。
- en: Mansur et al. (2014) Al Mansur, Yasushi Makihara, Rasyid Aqmar, and Yasushi
    Yagi. 2014. Gait recognition under speed transition. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 2521–2528.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mansur 等 (2014) Al Mansur, Yasushi Makihara, Rasyid Aqmar 和 Yasushi Yagi。2014。在速度过渡下的步态识别。在*IEEE
    Conference on Computer Vision and Pattern Recognition*会议上。2521–2528。
- en: Marsico and Mecca (2019) Maria De Marsico and Alessio Mecca. 2019. A survey
    on gait recognition via wearable sensors. *ACM Computing Surveys (CSUR)* 52, 4
    (2019), 1–39.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marsico 和 Mecca (2019) Maria De Marsico 和 Alessio Mecca。2019。关于通过可穿戴传感器进行步态识别的调查。*ACM
    Computing Surveys (CSUR)* 52, 4 (2019), 1–39。
- en: 'Matovski et al. (2010) Darko S Matovski, Mark S Nixon, Sasan Mahmoodi, and
    John N Carter. 2010. The effect of time on the performance of gait biometrics.
    In *2010 Fourth IEEE International Conference on Biometrics: Theory, Applications
    and Systems (BTAS)*. IEEE, 1–6.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matovski 等（2010）Darko S Matovski、Mark S Nixon、Sasan Mahmoodi 和 John N Carter.
    2010. 时间对步态生物识别性能的影响. 在 *2010年第四届IEEE国际生物识别：理论、应用与系统会议（BTAS）* 中. IEEE, 1–6.
- en: Mehraj and Mir (2020) Haider Mehraj and Ajaz Hussain Mir. 2020. Feature vector
    extraction and optimisation for multimodal biometrics employing face, ear and
    gait utilising artificial neural networks. *International Journal of Cloud Computing*
    9, 2-3 (2020), 131–149.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehraj 和 Mir（2020）Haider Mehraj 和 Ajaz Hussain Mir. 2020. 利用人工神经网络进行面部、耳朵和步态的多模态生物识别的特征向量提取和优化.
    *国际云计算杂志* 9, 2-3 (2020), 131–149.
- en: Mitchell (1920) C. Ainsworth Mitchell. 1920. The detection of finger-prints
    on documents. *Analyst* 45, 529 (1920), 122–129.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell（1920）C. Ainsworth Mitchell. 1920. 文件上的指纹检测. *分析师* 45, 529 (1920), 122–129.
- en: Mori et al. (2010) Atsushi Mori, Yasushi Makihara, and Yasushi Yagi. 2010. Gait
    recognition using period-based phase synchronization for low frame-rate videos.
    In *2010 20th International Conference on Pattern Recognition*. IEEE, 2194–2197.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mori 等（2010）Atsushi Mori、Yasushi Makihara 和 Yasushi Yagi. 2010. 基于周期性相位同步的步态识别，用于低帧率视频.
    在 *2010年第20届国际模式识别会议* 中. IEEE, 2194–2197.
- en: 'Nambiar et al. (2019) Athira Nambiar, Alexandre Bernardino, and Jacinto C Nascimento.
    2019. Gait-based person re-identification: A survey. *ACM Computing Surveys (CSUR)*
    52, 2 (2019), 1–34.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nambiar 等（2019）Athira Nambiar、Alexandre Bernardino 和 Jacinto C Nascimento. 2019.
    基于步态的人物重新识别：综述. *ACM计算机调查（CSUR）* 52, 2 (2019), 1–34.
- en: Ngo et al. (2014) Thanh Trung Ngo, Yasushi Makihara, Hajime Nagahara, Yasuhiro
    Mukaigawa, and Yasushi Yagi. 2014. The largest inertial sensor-based gait database
    and performance evaluation of gait-based personal authentication. *Pattern Recognition*
    47, 1 (2014), 228–237.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ngo 等（2014）Thanh Trung Ngo、Yasushi Makihara、Hajime Nagahara、Yasuhiro Mukaigawa
    和 Yasushi Yagi. 2014. 最大的基于惯性传感器的步态数据库及步态个人认证性能评估. *模式识别* 47, 1 (2014), 228–237.
- en: Ngo et al. (2015) Trung Thanh Ngo, Yasushi Makihara, Hajime Nagahara, Yasuhiro
    Mukaigawa, and Yasushi Yagi. 2015. Similar gait action recognition using an inertial
    sensor. *Pattern Recognition* 48, 4 (2015), 1289–1301.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ngo 等（2015）Trung Thanh Ngo、Yasushi Makihara、Hajime Nagahara、Yasuhiro Mukaigawa
    和 Yasushi Yagi. 2015. 使用惯性传感器的相似步态动作识别. *模式识别* 48, 4 (2015), 1289–1301.
- en: Nixon et al. (2001) M Nixon, J Carter, J Shutler, and M Grant. 2001. Experimental
    plan for automatic gait recognition. *University of Southampton, Southampton,
    UK* (2001).
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nixon 等（2001）M Nixon、J Carter、J Shutler 和 M Grant. 2001. 自动步态识别的实验计划. *南安普顿大学，南安普顿，英国*
    (2001).
- en: Nixon and Carter (2004) Mark S Nixon and John N Carter. 2004. Advances in automatic
    gait recognition. In *null*. IEEE, 139.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nixon 和 Carter（2004）Mark S Nixon 和 John N Carter. 2004. 自动步态识别的进展. 在 *null*
    中. IEEE, 139.
- en: 'Oppenheim et al. (2001) Alan V Oppenheim, John R Buck, and Ronald W Schafer.
    2001. *Discrete-time signal processing. Vol. 2*. Upper Saddle River, NJ: Prentice
    Hall.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oppenheim 等（2001）Alan V Oppenheim、John R Buck 和 Ronald W Schafer. 2001. *离散时间信号处理.
    第2卷*. 上萨德尔河，NJ: Prentice Hall.'
- en: Passos et al. (2017) Leandro A. Passos, Kelton A.P. Costa, and João P. Papa.
    2017. Deep Boltzmann Machines Using Adaptive Temperatures. In *International Conference
    on Computer Analysis of Images and Patterns*. Springer, 172–183.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Passos 等（2017）Leandro A. Passos、Kelton A.P. Costa 和 João P. Papa. 2017. 使用自适应温度的深度玻尔兹曼机.
    在 *国际计算机图像和模式分析会议* 中. Springer, 172–183.
- en: Passos et al. (2019) L. A. Passos, M. C. Santana, T. Moreira, and J. P. Papa.
    2019. $\kappa$-Entropy Based Restricted Boltzmann Machines. In *The 2019 International
    Joint Conference on Neural Networks (IJCNN)*. IEEE, 1–8.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Passos 等（2019）L. A. Passos、M. C. Santana、T. Moreira 和 J. P. Papa. 2019. $\kappa$-熵基限制玻尔兹曼机.
    在 *2019国际神经网络联合会议（IJCNN）* 中. IEEE, 1–8.
- en: 'Pisani et al. (2019) Paulo Henrique Pisani, Abir Mhenni, Romain Giot, Estelle
    Cherrier, Norman Poh, André Carlos Ponce de Leon Ferreira de Carvalho, Christophe
    Rosenberger, and Najoua Essoukri Ben Amara. 2019. Adaptive biometric systems:
    Review and perspectives. *ACM Computing Surveys (CSUR)* 52, 5 (2019), 1–38.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pisani 等（2019）Paulo Henrique Pisani、Abir Mhenni、Romain Giot、Estelle Cherrier、Norman
    Poh、André Carlos Ponce de Leon Ferreira de Carvalho、Christophe Rosenberger 和 Najoua
    Essoukri Ben Amara. 2019. 自适应生物识别系统：综述与展望. *ACM计算机调查（CSUR）* 52, 5 (2019), 1–38.
- en: Potluri et al. (2019) Sasanka Potluri, Srinivas Ravuri, Christian Diedrich,
    and Lutz Schega. 2019. Deep Learning based Gait Abnormality Detection using Wearable
    Sensor System. In *2019 41st Annual International Conference of the IEEE Engineering
    in Medicine and Biology Society (EMBC)*. IEEE, 3613–3619.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Potluri et al. (2019) Sasanka Potluri, Srinivas Ravuri, Christian Diedrich,
    和 Lutz Schega. 2019. 基于深度学习的步态异常检测，使用可穿戴传感器系统。在*2019年第41届IEEE医学与生物学工程学会年会（EMBC）*。IEEE,
    3613–3619.
- en: 'Ragan et al. (2016) Elizabeth J Ragan, Courtney Johnson, Jacqueline N Milton,
    and Christopher J Gill. 2016. Ear biometrics for patient identification in global
    health: a cross-sectional study to test the feasibility of a simplified algorithm.
    *BMC research notes* 9, 1 (2016), 484.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ragan et al. (2016) Elizabeth J Ragan, Courtney Johnson, Jacqueline N Milton,
    和 Christopher J Gill. 2016. 用于全球健康患者识别的耳部生物识别：测试简化算法的可行性的横断面研究。*BMC研究笔记* 9, 1
    (2016), 484.
- en: Ramli et al. (2016) DA Ramli, MY Hooi, and KJ Chee. 2016. Development of Heartbeat
    Detection Kit for Biometric Authentication System. *Procedia Computer Science*
    96 (2016), 305–314.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramli et al. (2016) DA Ramli, MY Hooi, 和 KJ Chee. 2016. 用于生物特征认证系统的心跳检测套件的开发。*计算机科学程序集*
    96 (2016), 305–314.
- en: 'Rida et al. (2018) Imad Rida, Noor Almaadeed, and Somaya Almaadeed. 2018. Robust
    gait recognition: a comprehensive survey. *IET Biometrics* 8, 1 (2018), 14–28.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rida et al. (2018) Imad Rida, Noor Almaadeed, 和 Somaya Almaadeed. 2018. 稳健的步态识别：全面综述。*IET生物特征识别*
    8, 1 (2018), 14–28.
- en: Rida et al. (2016) I. Rida, X. Jiang, , and G. L. Marcialis. 2016. Human body
    part selection by group lasso of motion for model-free gait recognition. *IEEE
    Signal Processing Letters* 23, 1 (2016), 154–158.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rida et al. (2016) I. Rida, X. Jiang, 和 G. L. Marcialis. 2016. 基于运动的组lasso方法的人体部位选择，用于无模型步态识别。*IEEE信号处理通讯*
    23, 1 (2016), 154–158.
- en: Roder et al. (ress) M. Roder, L. A. Passos, L. C. F. Ribeiro, B. C. Benato,
    A. L. Falcão, and J. P. Papa. In Press. Intestinal Parasites Classification Using
    Deep Belief Networks. In *The 19th International Conference on Artificial Intelligence
    and Soft Computing (ICAISC)*. IEEE.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roder et al. (ress) M. Roder, L. A. Passos, L. C. F. Ribeiro, B. C. Benato,
    A. L. Falcão, 和 J. P. Papa. 即将出版。使用深度信念网络进行肠道寄生虫分类。在*第19届国际人工智能与软计算会议（ICAISC）*。IEEE.
- en: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017.
    Dynamic routing between capsules. In *Advances in neural information processing
    systems*. 3856–3866.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour et al. (2017) Sara Sabour, Nicholas Frosst, 和 Geoffrey E Hinton. 2017.
    动态路由胶囊之间的连接。在*神经信息处理系统进展*。3856–3866.
- en: Santana et al. (2019) Marcos C. Santana, Leandro A. Passos, Thierry P. Moreira,
    Danilo Colombo, Victor Hugo C. de Albuquerque, and João P. Papa. 2019. A Novel
    Siamese-Based Approach for Scene Change Detection With Applications to Obstructed
    Routes in Hazardous Environments. *IEEE Intelligent Systems* 35, 1 (2019), 44–53.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santana et al. (2019) Marcos C. Santana, Leandro A. Passos, Thierry P. Moreira,
    Danilo Colombo, Victor Hugo C. de Albuquerque, 和 João P. Papa. 2019. 一种新颖的基于Siamese的场景变化检测方法，应用于危险环境中的障碍路线。*IEEE智能系统*
    35, 1 (2019), 44–53.
- en: 'Sarkar et al. (2005) Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro Robledo
    Vega, Patrick Grother, and Kevin W Bowyer. 2005. The humanid gait challenge problem:
    Data sets, performance, and analysis. *IEEE transactions on pattern analysis and
    machine intelligence* 27, 2 (2005), 162–177.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarkar et al. (2005) Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro
    Robledo Vega, Patrick Grother, 和 Kevin W Bowyer. 2005. humanid步态挑战问题：数据集、性能与分析。*IEEE模式分析与机器智能学报*
    27, 2 (2005), 162–177.
- en: Sepas-Moghaddam et al. (2021) Alireza Sepas-Moghaddam, Saeed Ghorbani, Nikolaus F
    Troje, and Ali Etemad. 2021. Gait Recognition using Multi-Scale Partial Representation
    Transformation with Capsules. In *2020 25th International Conference on Pattern
    Recognition (ICPR)*. IEEE, 8045–8052.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sepas-Moghaddam et al. (2021) Alireza Sepas-Moghaddam, Saeed Ghorbani, Nikolaus
    F Troje, 和 Ali Etemad. 2021. 使用胶囊的多尺度部分表示变换进行步态识别。在*2020年第25届国际模式识别会议（ICPR）*。IEEE,
    8045–8052.
- en: 'Shiraga et al. (2016) Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio
    Echigo, and Yasushi Yagi. 2016. Geinet: View-invariant gait recognition using
    a convolutional neural network. In *2016 international conference on biometrics
    (ICB)*. IEEE, 1–8.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shiraga et al. (2016) Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio
    Echigo, 和 Yasushi Yagi. 2016. Geinet: 使用卷积神经网络的视角不变步态识别。在*2016年国际生物识别会议（ICB）*。IEEE,
    1–8.'
- en: Shutler et al. (2004) Jamie D Shutler, Michael G Grant, Mark S Nixon, and John N
    Carter. 2004. On a large sequence-based human gait database. In *Applications
    and Science in Soft Computing*. Springer, 339–346.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shutler et al. (2004) Jamie D Shutler, Michael G Grant, Mark S Nixon, 和 John
    N Carter. 2004. 关于一个大型基于序列的人体步态数据库。在*软计算的应用与科学*。Springer, 339–346.
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv preprint
    arXiv:1409.1556* (2014).
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman (2014) Karen Simonyan 和 Andrew Zisserman. 2014. 用于大规模图像识别的非常深的卷积网络。*arXiv
    预印本 arXiv:1409.1556* (2014)。
- en: 'Singh et al. (2018) Jasvinder Pal Singh, Sanjeev Jain, Sakshi Arora, and Uday Pratap
    Singh. 2018. Vision-based gait recognition: A survey. *IEEE Access* 6 (2018),
    70497–70527.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等 (2018) Jasvinder Pal Singh, Sanjeev Jain, Sakshi Arora 和 Uday Pratap
    Singh. 2018. 基于视觉的步态识别：综述。*IEEE Access* 6 (2018), 70497–70527。
- en: Sokolova and Konushin (2017a) A Sokolova and A Konushin. 2017a. Gait Recognition
    Based on Convolutional Neural Networks. *International Archives of the Photogrammetry,
    Remote Sensing & Spatial Information Sciences* 42 (2017).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sokolova 和 Konushin (2017a) A Sokolova 和 A Konushin. 2017a. 基于卷积神经网络的步态识别。*国际摄影测量、遥感与空间信息科学档案*
    42 (2017)。
- en: Sokolova and Konushin (2017b) Anna Sokolova and Anton Konushin. 2017b. Pose-based
    Deep Gait Recognition. *CoRR* abs/1710.06512 (2017). arXiv:1710.06512 [http://arxiv.org/abs/1710.06512](http://arxiv.org/abs/1710.06512)
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sokolova 和 Konushin (2017b) Anna Sokolova 和 Anton Konushin. 2017b. 基于姿态的深度步态识别。*CoRR*
    abs/1710.06512 (2017)。arXiv:1710.06512 [http://arxiv.org/abs/1710.06512](http://arxiv.org/abs/1710.06512)
- en: Souza Jr et al. (2020) Luis A. Souza Jr, Leandro A. Passos, Robert Mendel, Alanna
    Ebigbo, Andreas Probst, Helmut Messmann, Christoph Palm, and João P. Papa. 2020.
    Assisting Barrett’s esophagus identification using endoscopic data augmentation
    based on Generative Adversarial Networks. *Computers in Biology and Medicine*
    126 (2020), 104029.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Souza Jr 等 (2020) Luis A. Souza Jr, Leandro A. Passos, Robert Mendel, Alanna
    Ebigbo, Andreas Probst, Helmut Messmann, Christoph Palm 和 João P. Papa. 2020.
    使用基于生成对抗网络的内窥镜数据增强辅助 Barrett 食管识别。*生物医学计算机* 126 (2020), 104029。
- en: 'Sultana et al. (2017) Madeena Sultana, Padma Polash Paul, and Marina L Gavrilova.
    2017. Social behavioral information fusion in multimodal biometrics. *IEEE Transactions
    on Systems, Man, and Cybernetics: Systems* 48, 12 (2017), 2176–2187.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sultana 等 (2017) Madeena Sultana, Padma Polash Paul 和 Marina L Gavrilova. 2017.
    多模态生物识别中的社会行为信息融合。*IEEE 系统、人类与控制论学报: 系统* 48, 12 (2017), 2176–2187。'
- en: Sun et al. (2019) Yingnan Sun, Frank P-W Lo, and Benny Lo. 2019. A deep learning
    approach on gender and age recognition using a single inertial sensor. In *2019
    IEEE 16th International Conference on Wearable and Implantable Body Sensor Networks
    (BSN)*. IEEE, 1–4.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2019) Yingnan Sun, Frank P-W Lo 和 Benny Lo. 2019. 基于深度学习的性别和年龄识别方法，使用单一惯性传感器。在
    *2019 IEEE第16届可穿戴和植入式体感网络国际会议 (BSN)* 中。IEEE, 1–4。
- en: Sundararajan et al. (2019) Aditya Sundararajan, Arif I Sarwat, and Alexander
    Pons. 2019. A survey on modality characteristics, performance evaluation metrics,
    and security for traditional and wearable biometric systems. *ACM Computing Surveys
    (CSUR)* 52, 2 (2019), 1–36.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundararajan 等 (2019) Aditya Sundararajan, Arif I Sarwat 和 Alexander Pons. 2019.
    关于传统和可穿戴生物识别系统的模态特征、性能评估指标和安全性的综述。*ACM计算调查 (CSUR)* 52, 2 (2019), 1–36。
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*. 2818–2826.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等 (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens
    和 Zbigniew Wojna. 2016. 重新思考计算机视觉的 Inception 架构。在 *IEEE计算机视觉与模式识别会议论文集* 中，2818–2826。
- en: 'Taigman et al. (2014) Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior
    Wolf. 2014. Deepface: Closing the gap to human-level performance in face verification.
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    1701–1708.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taigman 等 (2014) Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato 和 Lior Wolf.
    2014. Deepface: 缩小面部验证中的人类水平表现差距。在 *IEEE计算机视觉与模式识别会议论文集* 中，1701–1708。'
- en: Takemura et al. (2017) Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio
    Echigo, and Yasushi Yagi. 2017. On input/output architectures for convolutional
    neural network-based cross-view gait recognition. *IEEE Transactions on Circuits
    and Systems for Video Technology* (2017).
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takemura 等 (2017) Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio
    Echigo 和 Yasushi Yagi. 2017. 基于卷积神经网络的跨视角步态识别的输入/输出架构。*IEEE 视频技术电路与系统学报* (2017)。
- en: Takemura et al. (2018) Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio
    Echigo, and Yasushi Yagi. 2018. Multi-view large population gait dataset and its
    performance evaluation for cross-view gait recognition. *IPSJ Transactions on
    Computer Vision and Applications* 10, 1 (2018), 4.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takemura等（2018）Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo,
    和 Yasushi Yagi。2018。多视角大规模人群步态数据集及其跨视角步态识别性能评估。*IPSJ计算机视觉与应用论文集* 10, 1 (2018),
    4。
- en: Tan et al. (2006) Daoliang Tan, Kaiqi Huang, Shiqi Yu, and Tieniu Tan. 2006.
    Efficient night gait recognition based on template matching. In *18th International
    Conference on Pattern Recognition (ICPR’06)*, Vol. 3\. IEEE, 1000–1003.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan等（2006）Daoliang Tan, Kaiqi Huang, Shiqi Yu, 和 Tieniu Tan。2006。基于模板匹配的高效夜间步态识别。发表于*第18届国际模式识别大会（ICPR’06）*，第3卷。IEEE,
    1000–1003。
- en: 'Tan and Le (2019) Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking
    model scaling for convolutional neural networks. In *International Conference
    on Machine Learning*. PMLR, 6105–6114.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan和Le（2019）Mingxing Tan 和 Quoc Le。2019。Efficientnet：重新思考卷积神经网络的模型缩放。发表于*国际机器学习会议*。PMLR,
    6105–6114。
- en: 'Tiong et al. (2020) Leslie Ching Ow Tiong, Seong Tae Kim, and Yong Man Ro.
    2020. Multimodal facial biometrics recognition: Dual-stream convolutional neural
    networks with multi-feature fusion layers. *Image and Vision Computing* 102 (2020),
    103977.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiong等（2020）Leslie Ching Ow Tiong, Seong Tae Kim, 和 Yong Man Ro。2020。多模态面部生物识别：具有多特征融合层的双流卷积神经网络。*图像与视觉计算*
    102 (2020), 103977。
- en: Tran et al. (2021) Lam Tran, Thang Hoang, Thuc Nguyen, Hyunil Kim, and Deokjai
    Choi. 2021. Multi-Model Long Short-Term Memory Network for Gait Recognition Using
    Window-Based Data Segment. *IEEE Access* 9 (2021), 23826–23839.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等（2021）Lam Tran, Thang Hoang, Thuc Nguyen, Hyunil Kim, 和 Deokjai Choi。2021。基于窗口数据段的步态识别多模型长短期记忆网络。*IEEE
    Access* 9 (2021), 23826–23839。
- en: Tsuji et al. (2010) Akira Tsuji, Yasushi Makihara, and Yasushi Yagi. 2010. Silhouette
    transformation based on walking speed for gait identification. In *2010 IEEE Computer
    Society Conference on Computer Vision and Pattern Recognition*. IEEE, 717–722.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsuji等（2010）Akira Tsuji, Yasushi Makihara, 和 Yasushi Yagi。2010。基于步速的轮廓变换用于步态识别。发表于*2010
    IEEE计算机视觉与模式识别会议*。IEEE, 717–722。
- en: Turk and Pentland (1991) Matthew Turk and Alex Pentland. 1991. Eigenfaces for
    recognition. *Journal of cognitive neuroscience* 3, 1 (1991), 71–86.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turk和Pentland（1991）Matthew Turk 和 Alex Pentland。1991。用于识别的特征脸。*认知神经科学杂志* 3,
    1 (1991), 71–86。
- en: Uddin et al. (2018) Md Zasim Uddin, Thanh Trung Ngo, Yasushi Makihara, Noriko
    Takemura, Xiang Li, Daigo Muramatsu, and Yasushi Yagi. 2018. The OU-ISIR Large
    Population Gait Database with real-life carried object and its performance evaluation.
    *IPSJ Transactions on Computer Vision and Applications* 10, 1 (2018), 1–11.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uddin等（2018）Md Zasim Uddin, Thanh Trung Ngo, Yasushi Makihara, Noriko Takemura,
    Xiang Li, Daigo Muramatsu, 和 Yasushi Yagi。2018。OU-ISIR大规模步态数据库及其实际携带物体和性能评估。*IPSJ计算机视觉与应用论文集*
    10, 1 (2018), 1–11。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *arXiv preprint arXiv:1706.03762* (2017).
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Lukasz Kaiser, 和 Illia Polosukhin。2017。注意力即是你所需。*arXiv预印本
    arXiv:1706.03762* (2017)。
- en: 'Vincent et al. (2010) Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
    Bengio, Pierre-Antoine Manzagol, and Léon Bottou. 2010. Stacked denoising autoencoders:
    Learning useful representations in a deep network with a local denoising criterion.
    *Journal of machine learning research* 11, 12 (2010).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent等（2010）Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio,
    Pierre-Antoine Manzagol, 和 Léon Bottou。2010。堆叠去噪自编码器：通过局部去噪标准在深度网络中学习有用的表示。*机器学习研究期刊*
    11, 12 (2010)。
- en: W. Kusakunniran and Li (2013) Jian Zhang Yi Ma W. Kusakunniran, Qiang Wu and
    Hongdong Li. 2013. A New View-Invariant Feature for Cross-View Gait Recognition.
    *IEEE TIFS* (2013), 1642–1653.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W. Kusakunniran和Li（2013）Jian Zhang Yi Ma W. Kusakunniran, Qiang Wu 和 Hongdong
    Li。2013。用于跨视角步态识别的新视角不变特征。*IEEE TIFS* (2013), 1642–1653。
- en: Wan et al. (2018) Changsheng Wan, Li Wang, and Vir V Phoha. 2018. A survey on
    gait recognition. *ACM Computing Surveys (CSUR)* 51, 5 (2018), 1–35.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan等（2018）Changsheng Wan, Li Wang, 和 Vir V Phoha。2018。步态识别综述。*ACM计算机调查（CSUR）*
    51, 5 (2018), 1–35。
- en: Wang et al. (2020) Caiyong Wang, Jawad Muhammad, Yunlong Wang, Zhaofeng He,
    and Zhenan Sun. 2020. Towards complete and accurate iris segmentation using deep
    multi-task attention network for non-cooperative iris recognition. *IEEE Transactions
    on information forensics and security* 15 (2020), 2944–2959.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2020) Caiyong Wang、Jawad Muhammad、Yunlong Wang、Zhaofeng He 和 Zhenan
    Sun. 2020. 通过深度多任务注意力网络实现完整且准确的虹膜分割，用于非合作虹膜识别。*IEEE信息取证与安全汇刊* 15 (2020), 2944–2959。
- en: Wang and Kumar (2019) Kuo Wang and Ajay Kumar. 2019. Cross-spectral iris recognition
    using CNN and supervised discrete hashing. *Pattern Recognition* 86 (2019), 85–98.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Kumar (2019) Kuo Wang 和 Ajay Kumar. 2019. 使用 CNN 和监督离散哈希的跨光谱虹膜识别。*模式识别*
    86 (2019), 85–98。
- en: Wang et al. (2019) Kejun Wang, Liangliang Liu, Yilong Lee, Xinnan Ding, and
    Junyu Lin. 2019. Nonstandard Periodic Gait Energy Image for Gait Recognition and
    Data Augmentation. In *Chinese Conference on Pattern Recognition and Computer
    Vision (PRCV)*. Springer, 197–208.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2019) Kejun Wang、Liangliang Liu、Yilong Lee、Xinnan Ding 和 Junyu Lin.
    2019. 用于步态识别和数据增强的非标准周期性步态能量图像。在*中国模式识别与计算机视觉会议 (PRCV)*。Springer，197–208。
- en: Wang et al. (2003) Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming Hu. 2003.
    Silhouette analysis-based gait recognition for human identification. *IEEE transactions
    on pattern analysis and machine intelligence* 25, 12 (2003), 1505–1518.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2003) Liang Wang、Tieniu Tan、Huazhong Ning 和 Weiming Hu. 2003. 基于轮廓分析的步态识别用于人类身份识别。*IEEE模式分析与机器智能汇刊*
    25, 12 (2003), 1505–1518。
- en: Wang and Yan (2020) Xiuhui Wang and Wei Qi Yan. 2020. Human gait recognition
    based on frame-by-frame gait energy images and convolutional long short-term memory.
    *International journal of neural systems* 30, 01 (2020), 1950027.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Yan (2020) Xiuhui Wang 和 Wei Qi Yan. 2020. 基于逐帧步态能量图像和卷积长短期记忆网络的人体步态识别。*国际神经系统杂志*
    30, 01 (2020), 1950027。
- en: Wu et al. (2017a) Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, and
    Tieniu Tan. 2017a. A comprehensive study on cross-view gait based human identification
    with deep cnns. *IEEE transactions on pattern analysis and machine intelligence*
    39, 2 (2017), 209–226.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2017a) Zifeng Wu、Yongzhen Huang、Liang Wang、Xiaogang Wang 和 Tieniu Tan.
    2017a. 基于深度 CNN 的跨视角步态人类识别综合研究。*IEEE模式分析与机器智能汇刊* 39, 2 (2017), 209–226。
- en: Wu et al. (2017b) Z. Wu, Y. Huang, L. Wang, X. Wang, and T. Tan. 2017b. A comprehensive
    study on cross-view gait based human identification with deep cnns. *IEEE transactions
    on pattern analysis and machine intelligence* 39, 2 (2017), 209–226.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2017b) Z. Wu、Y. Huang、L. Wang、X. Wang 和 T. Tan. 2017b. 基于深度 CNN 的跨视角步态人类识别综合研究。*IEEE模式分析与机器智能汇刊*
    39, 2 (2017), 209–226。
- en: X. Li and Ren (2020) C. Xu Y. Yagi X. Li, Y. Makihara and M. Ren. 2020. Gait
    recognitionvia semi-supervised disentangled representation learning to identityand
    covariate features. *in Computer Vision and Pattern Recognition* (2020).
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X. Li 和 Ren (2020) C. Xu、Y. Yagi、X. Li、Y. Makihara 和 M. Ren. 2020. 通过半监督解耦表示学习进行步态识别，以识别身份和协变量特征。*计算机视觉与模式识别*
    (2020)。
- en: Xiong et al. (2020) Dezhen Xiong, Daohui Zhang, Xingang Zhao, and Yiwen Zhao.
    2020. Continuous human gait tracking using sEMG signals. In *2020 42nd Annual
    International Conference of the IEEE Engineering in Medicine & Biology Society
    (EMBC)*. IEEE, 3094–3097.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人 (2020) Dezhen Xiong、Daohui Zhang、Xingang Zhao 和 Yiwen Zhao. 2020. 使用
    sEMG 信号进行连续的人体步态跟踪。在*2020年第42届国际IEEE医学与生物学工程学会年会（EMBC）*。IEEE，3094–3097。
- en: Xu et al. (2020) Chi Xu, Yasushi Makihara, Xiang Li, Yasushi Yagi, and Jianfeng
    Lu. 2020. Cross-view gait recognition using pairwise spatial transformer networks.
    *IEEE Transactions on Circuits and Systems for Video Technology* (2020).
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2020) Chi Xu、Yasushi Makihara、Xiang Li、Yasushi Yagi 和 Jianfeng Lu. 2020.
    使用成对空间变换网络进行跨视角步态识别。*IEEE视频技术电路与系统汇刊* (2020)。
- en: Xu et al. (2017) Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Yasushi Yagi,
    and Jianfeng Lu. 2017. The OU-ISIR Gait Database Comprising the Large Population
    Dataset with Age and Performance Evaluation of Age Estimation. *IPSJ Trans. on
    Computer Vision and Applications* 9, 24 (2017), 1–14.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2017) Chi Xu、Yasushi Makihara、Gakuto Ogi、Xiang Li、Yasushi Yagi 和 Jianfeng
    Lu. 2017. OU-ISIR 步态数据库，包括大规模人群数据集及年龄估计的性能评估。*IPSJ计算机视觉与应用汇刊* 9, 24 (2017), 1–14。
- en: Xu et al. (2019) Zhaopeng Xu, Wei Lu, Qin Zhang, Yuileong Yeung, and Xin Chen.
    2019. Gait Recognition Based on Capsule Network. *Journal of Visual Communication
    and Image Representation* (2019).
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2019) Zhaopeng Xu、Wei Lu、Qin Zhang、Yuileong Yeung 和 Xin Chen. 2019. 基于胶囊网络的步态识别。*视觉通信与图像表示杂志*
    (2019)。
- en: 'Yasushi Makihara and Yagi (2006) Yasuhiro Mukaigawa Tomio Echigo Yasushi Makihara,
    Ryusuke Sagawa and Yasushi Yagi. 2006. Gait Recognition Using a View Transformation
    Model in the Frequency Domain. *In ECCV: v.3953* (2006), 151–163.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yasushi Makihara 和 Yagi (2006) Yasuhiro Mukaigawa Tomio Echigo Yasushi Makihara,
    Ryusuke Sagawa 和 Yasushi Yagi. 2006. 使用频域视图变换模型的步态识别。*在 ECCV: v.3953* (2006),
    151–163。'
- en: Yeo and Park (2020) Sang Seok Yeo and Ga Young Park. 2020. Accuracy verification
    of spatio-temporal and kinematic parameters for gait using inertial measurement
    unit system. *Sensors* 20, 5 (2020), 1343.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeo 和 Park (2020) Sang Seok Yeo 和 Ga Young Park. 2020. 使用惯性测量单元系统对步态的时空和运动参数进行精度验证。*传感器*
    20, 5 (2020), 1343。
- en: Yogarajah et al. (2015) P. Yogarajah, P. Chaurasia, J. Condell, and G. Prasad.
    2015. Enhancing gait based person identification using joint sparsity model and
    -norm minimization. *Pattern Recognition* 38 (2015), 3–22.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yogarajah 等 (2015) P. Yogarajah, P. Chaurasia, J. Condell 和 G. Prasad. 2015.
    使用联合稀疏模型和 -范数最小化增强步态识别。*模式识别* 38 (2015), 3–22。
- en: 'Yu et al. (2019) Ning Yu, Larry S Davis, and Mario Fritz. 2019. Attributing
    fake images to gans: Learning and analyzing gan fingerprints. In *Proceedings
    of the IEEE International Conference on Computer Vision*. 7556–7566.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等 (2019) Ning Yu, Larry S Davis 和 Mario Fritz. 2019. 将虚假图像归因于 GANs: 学习和分析
    GAN 指纹。在 *IEEE 国际计算机视觉会议论文集* 中。7556–7566。'
- en: Yu et al. (2017) Shiqi Yu, Haifeng Chen, Qing Wang, Linlin Shen, and Yongzhen
    Huang. 2017. Invariant feature extraction for gait recognition using only one
    uniform model. *Neurocomputing* 239 (2017), 81–93.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2017) Shiqi Yu, Haifeng Chen, Qing Wang, Linlin Shen 和 Yongzhen Huang.
    2017. 使用单一统一模型进行步态识别的恒定特征提取。*神经计算* 239 (2017), 81–93。
- en: Yu et al. (2006) Shiqi Yu, Daoliang Tan, and Tieniu Tan. 2006. A Framework for
    Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition.
    In *Pattern Recognition*, Vol. 4. 441–444. [https://doi.org/10.1109/ICPR.2006.67](https://doi.org/10.1109/ICPR.2006.67)
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2006) Shiqi Yu, Daoliang Tan 和 Tieniu Tan. 2006. 评估视角、服装和携带条件对步态识别影响的框架。在
    *模式识别* 中, 第 4 卷。441–444。 [https://doi.org/10.1109/ICPR.2006.67](https://doi.org/10.1109/ICPR.2006.67)
- en: Yu et al. (2013) Shiqi Yu, Qing Wang, and Yongzhen Huang. 2013. A large RGB-D
    gait dataset and the baseline algorithm. In *Chinese Conference on Biometric Recognition*.
    Springer, 417–424.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2013) Shiqi Yu, Qing Wang 和 Yongzhen Huang. 2013. 一个大型 RGB-D 步态数据集及其基线算法。在
    *中国生物特征识别会议* 上。Springer, 417–424。
- en: Z. Zhang and Liu (2020) F. Liu Z. Zhang, L. Tran and X. Liu. 2020. On learning
    disentangled representations for gait recognition. *IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. in press* (2020).
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Zhang 和 Liu (2020) F. Liu Z. Zhang, L. Tran 和 X. Liu. 2020. 关于为步态识别学习解缠表示的研究。*IEEE
    模式分析与机器智能汇刊, vol. in press* (2020)。
- en: Zagoruyko and Komodakis (2016) Sergey Zagoruyko and Nikos Komodakis. 2016. Wide
    residual networks. *arXiv preprint arXiv:1605.07146* (2016).
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zagoruyko 和 Komodakis (2016) Sergey Zagoruyko 和 Nikos Komodakis. 2016. 宽残差网络。*arXiv
    预印本 arXiv:1605.07146* (2016)。
- en: Zehngut et al. (2015) Niv Zehngut, Felix Juefei-Xu, Rishabh Bardia, Dipan K
    Pal, Chandrasekhar Bhagavatula, and Marios Savvides. 2015. Investigating the feasibility
    of image-based nose biometrics. In *Image Processing (ICIP), 2015 IEEE International
    Conference on*. IEEE, 522–526.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zehngut 等 (2015) Niv Zehngut, Felix Juefei-Xu, Rishabh Bardia, Dipan K Pal,
    Chandrasekhar Bhagavatula 和 Marios Savvides. 2015. 调查基于图像的鼻部生物识别的可行性。在 *图像处理 (ICIP),
    2015 IEEE 国际会议* 上。IEEE, 522–526。
- en: Zhang et al. (2019) Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming
    Liu, Jian Wan, and Nanxin Wang. 2019. Gait Recognition via Disentangled Representation
    Learning. *arXiv preprint arXiv:1904.04925* (2019).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019) Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu,
    Jian Wan 和 Nanxin Wang. 2019. 通过解缠表示学习进行步态识别。*arXiv 预印本 arXiv:1904.04925* (2019)。
- en: Zhao et al. (2021) Aite Zhao, Junyu Dong, Jianbo Li, Lin Qi, and Huiyu Zhou.
    2021. Associated Spatio-Temporal Capsule Network for Gait Recognition. *arXiv
    preprint arXiv:2101.02458* (2021).
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2021) Aite Zhao, Junyu Dong, Jianbo Li, Lin Qi 和 Huiyu Zhou. 2021. 用于步态识别的关联时空胶囊网络。*arXiv
    预印本 arXiv:2101.02458* (2021)。
- en: Zhao et al. (2016) X. Zhao, Y. Jiang, T. Stathaki, and H. Zhang. 2016. Gait
    recognition method for arbitrary straight walking paths using appearance conversion
    machine. *Neurocomputing* 173 (2016), 530–540.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2016) X. Zhao, Y. Jiang, T. Stathaki 和 H. Zhang. 2016. 使用外观转换机器的任意直线行走路径步态识别方法。*神经计算*
    173 (2016), 530–540。
- en: Zheng et al. (2012) Shuai Zheng, Kaiqi Huang, Tieniu Tan, and Dacheng Tao. 2012.
    A cascade fusion scheme for gait and cumulative foot pressure image recognition.
    *Pattern Recognition* 45, 10 (2012), 3603–3610.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2012）帅·郑、凯奇·黄、田钊·谭和达成·陶。2012。用于步态和累计足压图像识别的级联融合方案。*模式识别* 45, 10（2012），3603–3610。
- en: Zheng et al. (2011) Shuai Zheng, Junge Zhang, Kaiqi Huang, Ran He, and Tieniu
    Tan. 2011. Robust view transformation model for gait recognition. In *2011 18th
    IEEE International Conference on Image Processing*. IEEE, 2073–2076.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2011）帅·郑、君格·张、凯奇·黄、冉·何和田钊·谭。2011。步态识别的鲁棒视图变换模型。在*2011年第18届IEEE图像处理国际会议*中。IEEE，2073–2076。
- en: Zneit et al. (2017) R Abu Zneit, Ziad AlQadi, and M Abu Zalata. 2017. A Methodology
    to Create a Fingerprint for RGB Color Image. *International Journal of Computer
    Science and Mobile Computing* 16, 1 (2017), 205–212.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zneit等人（2017）R·阿布·兹内特、齐亚德·阿尔卡迪和M·阿布·扎拉塔。2017。为RGB彩色图像创建指纹的方法。*国际计算机科学与移动计算期刊*
    16, 1（2017），205–212。
- en: Zoph et al. (2020) Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao
    Liu, Ekin D Cubuk, and Quoc V Le. 2020. Rethinking pre-training and self-training.
    *arXiv preprint arXiv:2006.06882* (2020).
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoph等人（2020）巴雷特·佐夫、戈尔纳兹·贾希、林聪毅、尹·崔、韩晓·刘、伊金·D·库布克和阮伟·乐。2020。重新思考预训练和自训练。*arXiv预印本
    arXiv:2006.06882*（2020）。
- en: Zou et al. (2018a) Qin Zou, Yanling Wang, Yi Zhao, Qian Wang, Chao Shen, and
    Qingquan Li. 2018a. Deep Learning Based Gait Recognition Using Smartphones in
    the Wild. *CoRR* abs/1811.00338 (2018). arXiv:1811.00338 [http://arxiv.org/abs/1811.00338](http://arxiv.org/abs/1811.00338)
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou等人（2018a）秦·邹、闫玲·王、易·赵、钱·王、超·沈和青泉·李。2018a。基于深度学习的野外智能手机步态识别。*CoRR* abs/1811.00338（2018）。arXiv:1811.00338
    [http://arxiv.org/abs/1811.00338](http://arxiv.org/abs/1811.00338)
- en: Zou et al. (2018b) Qin Zou, Yanling Wang, Yi Zhao, Qian Wang, Chao Shen, and
    Qingquan Li. 2018b. Deep Learning Based Gait Recognition Using Smartphones in
    the Wild. *arXiv preprint arXiv:1811.00338* (2018).
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou等人（2018b）秦·邹、闫玲·王、易·赵、钱·王、超·沈和青泉·李。2018b。基于深度学习的野外智能手机步态识别。*arXiv预印本 arXiv:1811.00338*（2018）。
