- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.01040] A Survey on Uncertainty Toolkits for Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.01040] 深度学习中的不确定性工具包调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01040](https://ar5iv.labs.arxiv.org/html/2205.01040)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01040](https://ar5iv.labs.arxiv.org/html/2205.01040)
- en: A Survey on Uncertainty Toolkits
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不确定性工具包调查
- en: for Deep Learning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 针对深度学习
- en: Maximilian Pintz
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 马克西米利安·平茨
- en: University of Bonn, Fraunhofer IAIS
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 波恩大学，弗劳恩霍夫 IAIS
- en: Schloss Birlinghoven
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 比尔林霍芬城堡
- en: 53757 Sankt Augustin, Germany
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 53757 圣奥古斯丁，德国
- en: '{maximilian.alexander.pintz}@iais.fraunhofer.de'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{maximilian.alexander.pintz}@iais.fraunhofer.de'
- en: '&Joachim Sicking, Maximilian Poretschkin, Maram Akila'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '&乔阿希姆·西金、马克西米利安·波雷奇金、马拉姆·阿基拉'
- en: Fraunhofer IAIS
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 弗劳恩霍夫 IAIS
- en: Schloss Birlinghoven
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 比尔林霍芬城堡
- en: 53757 Sankt Augustin, Germany
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 53757 圣奥古斯丁，德国
- en: '{joachim.sicking,maximilian.poretschkin,maram.akila}@iais.fraunhofer.de'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '{joachim.sicking,maximilian.poretschkin,maram.akila}@iais.fraunhofer.de'
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The success of deep learning (DL) fostered the creation of unifying frameworks
    such as tensorflow or pytorch as much as it was driven by their creation in return.
    Having common building blocks facilitates the exchange of, e.g., models or concepts
    and makes developments easier replicable. Nonetheless, robust and reliable evaluation
    and assessment of DL models has often proven challenging. This is at odds with
    their increasing safety relevance, which recently culminated in the field of “trustworthy
    ML”. We believe that, among others, further unification of evaluation and safeguarding
    methodologies in terms of toolkits, i.e. small and specialized framework derivatives,
    might positively impact problems of trustworthiness as well as reproducibility.
    To this end, we present the first survey on toolkits for uncertainty estimation
    (UE) in DL, as UE forms a cornerstone in assessing model reliability. We investigate
    $11$ toolkits with respect to modeling and evaluation capabilities, providing
    an in-depth comparison for the three most promising ones, namely Pyro, Tensorflow
    Probability, and Uncertainty Quantification 360. While the first two provide a
    large degree of flexibility and seamless integration into their respective framework,
    the last one has the larger methodological scope.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的成功促进了统一框架的创建，例如 tensorflow 或 pytorch，同时它们的创建又反过来推动了深度学习的发展。拥有共同的构建模块有助于，例如，模型或概念的交流，并使得开发更容易复制。然而，对
    DL 模型进行稳健和可靠的评估通常具有挑战性。这与其日益增加的安全相关性相矛盾，最近这一点在“可信机器学习”领域达到了顶峰。我们相信，进一步统一评估和保障方法论，特别是工具包，即小型且专门化的框架衍生物，可能会对可信度和可重复性问题产生积极影响。为此，我们提供了关于深度学习中不确定性估计（UE）工具包的首次调查，因为
    UE 是评估模型可靠性的基石。我们调查了 $11$ 个工具包，针对建模和评估能力进行了深入比较，重点比较了最有前景的三个工具包，即 Pyro、Tensorflow
    Probability 和 Uncertainty Quantification 360。前两个工具包提供了较大的灵活性和无缝集成到各自框架中的能力，而最后一个则具有更大的方法学范围。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Supervised deep learning (DL) has spurred progress in various application domains
    including computer vision (Mahony et al., [2019](#bib.bib56); Chai et al., [2021](#bib.bib14)),
    natural language processing (Torfi et al., [2020](#bib.bib82)), and speech recognition
    (Alam et al., [2020](#bib.bib4)) and is increasingly employed in safety-critical
    systems such as autonomous vehicles (Yurtsever et al., [2020](#bib.bib91)) or
    medical diagnosis systems (Hafiz & Bhat, [2020](#bib.bib35)). These systems potentially
    harm the environment, destroy equipment or put humans at risk (Sommerville, [2010](#bib.bib79)),
    for instance, when vulnerable road users (in case of autonomous driving) or a
    severe disease (in case of a diagnostics system) are not recognized. The field
    of trustworthy ML (Brundage et al., [2020](#bib.bib13); Chatila et al., [2021](#bib.bib15);
    Liu et al., [2021](#bib.bib52); Houben et al., [2021](#bib.bib42)) seeks to identify
    and mitigate such risks of ML systems to enable their responsible and safe operation.
    To this end, a more holistic notion of quality is proposed, that extends beyond
    task performance and considers aspects like fairness, interpretability and reliability.
    Central to the latter is the identification and handling of uncertainties, i.e. factors
    that affect the system but are often not or only poorly accounted for. Such uncertainties
    are ubiquitous in DL and stem, for instance, from data (e.g. measurement noise
    and incorrect annotations) or variabilities in training procedures (e.g. hyperparameters,
    initializations and limited training data). Quantifying their impact on a system
    can contribute to its safety: for instance, an autonomous vehicle, that recognizes
    abnormal sensor input, may switch into a conservative travel mode or may activate
    redundant safety systems.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督深度学习（DL）推动了计算机视觉（Mahony 等，[2019](#bib.bib56); Chai 等，[2021](#bib.bib14)）、自然语言处理（Torfi
    等，[2020](#bib.bib82)）和语音识别（Alam 等，[2020](#bib.bib4)）等多个应用领域的进步，并越来越多地应用于安全关键系统，如自动驾驶汽车（Yurtsever
    等，[2020](#bib.bib91)）或医疗诊断系统（Hafiz & Bhat，[2020](#bib.bib35)）。这些系统可能对环境造成危害，破坏设备或使人类面临风险（Sommerville，[2010](#bib.bib79)），例如，当无法识别脆弱的道路使用者（在自动驾驶情况下）或严重的疾病（在诊断系统情况下）时。值得信赖的机器学习领域（Brundage
    等，[2020](#bib.bib13); Chatila 等，[2021](#bib.bib15); Liu 等，[2021](#bib.bib52);
    Houben 等，[2021](#bib.bib42)）旨在识别和缓解这些机器学习系统的风险，以实现其负责任和安全的运行。为此，提出了一种更全面的质量观念，不仅超越任务表现，还考虑公平性、可解释性和可靠性等方面。其中的核心是识别和处理不确定性，即影响系统的因素，但往往没有或仅有较差的考虑。这些不确定性在深度学习中普遍存在，例如，来自数据（如测量噪声和错误注释）或训练过程中的变异性（如超参数、初始化和有限的训练数据）。量化这些因素对系统的影响可以有助于其安全性：例如，识别异常传感器输入的自动驾驶汽车，可能会切换到保守的行驶模式或激活冗余的安全系统。
- en: The desirable adaptation of uncertainty estimation (UE) for a wide range of
    use cases (Sicking et al., [2022](#bib.bib78)) is often hindered by high implementation
    complexity. This adaptation could be accelerated by open-source toolkits, which
    reduce this effort by providing easy-to-use building blocks and guidance in the
    form of minimal working examples. Being used and reviewed by the community, toolkits
    may moreover strengthen technical quality of UE components and advance common
    practices. The latter, i.e. the use of common tools and standards, may drive further
    progress in the field as it facilitates easier reviews and reusing existing work.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对不确定性估计（UE）的期望适应性（Sicking 等，[2022](#bib.bib78)）常常受到高实现复杂性的阻碍。这种适应性可以通过开源工具包加快，该工具包通过提供易于使用的构建模块和最小工作示例形式的指导来减少这一努力。由于被社区使用和审查，工具包还可能增强UE组件的技术质量并推进常见实践。后者，即使用常见工具和标准，可能推动该领域的进一步进展，因为它促进了更容易的审查和重用现有工作。
- en: 'In light of this, we survey deep uncertainty toolkits, to the best of our knowledge
    for the first time. The survey is structured as follows: we first give a brief
    overview on DL software and UE (section [2](#S2 "2 Related work ‣ A Survey on
    Uncertainty Toolkits for Deep Learning")). Next, we select $11$ publicly available
    open-source deep uncertainty toolkits from $180$ considered repositories (section
    [3](#S3 "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty Toolkits
    for Deep Learning")) and present a catalog of criteria for their evaluation (section
    [4](#S4 "4 Evaluation criteria for uncertainty toolkits ‣ A Survey on Uncertainty
    Toolkits for Deep Learning")). We then apply these criteria to the collected toolkits
    in an comparative analysis (section [5](#S5 "5 Comparative analysis of the selected
    uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning")),
    describing their strengths and weaknesses. Finally, we highlight limitations and
    desirable future advancements of current deep uncertainty toolkits in the discussion
    (section [6](#S6 "6 Discussion ‣ A Survey on Uncertainty Toolkits for Deep Learning")).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我们首次调查了深度不确定性工具包。调查结构如下：我们首先简要概述DL软件和UE（第[2](#S2 "2 Related work ‣ A Survey
    on Uncertainty Toolkits for Deep Learning")节）。接下来，我们从180个考虑的仓库中选择了11个公开可用的开源深度不确定性工具包（第[3](#S3
    "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for
    Deep Learning")节），并提出了一套评估这些工具包的标准（第[4](#S4 "4 Evaluation criteria for uncertainty
    toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning")节）。然后，我们将这些标准应用于收集的工具包进行比较分析（第[5](#S5
    "5 Comparative analysis of the selected uncertainty toolkits ‣ A Survey on Uncertainty
    Toolkits for Deep Learning")节），描述其优缺点。最后，我们在讨论中突出当前深度不确定性工具包的局限性和期望的未来发展（第[6](#S6
    "6 Discussion ‣ A Survey on Uncertainty Toolkits for Deep Learning")节）。
- en: 2 Related work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: We first describe two types of DL software, DL frameworks and specialized toolkits
    that are discussed throughout this survey. Next, we give an overview over surveys
    on DL software and sketch their methodology and focus points in the second paragraph.
    To provide a basis for the subsequent survey on UE toolkits, some foundational
    concepts of UE, especially w.r.t. their modeling and assessment, are reviewed
    in the third paragraph. Some simple UE capabilities are shipped as part of standard
    DL frameworks like tensorflow and pytorch. We outline these “default” capabilities
    and sketch their limitations in the last paragraph of the section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先描述两种类型的深度学习（DL）软件，即在本次调查中讨论的**DL框架**和**专业工具包**。接下来，我们概述了关于DL软件的调查，并在第二段中勾勒出其方法论和重点。为了为随后的UE工具包调查提供基础，第三段回顾了UE的一些基础概念，特别是关于其建模和评估的内容。一些简单的UE能力作为标准DL框架（如tensorflow和pytorch）的一部分进行发布。我们在本节的最后一段概述了这些“默认”能力并勾勒了它们的局限性。
- en: DL frameworks and specialized toolkits
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**DL框架**和**专业工具包**'
- en: There is a wide range of DL software covering different facets of DL. Two special
    types, namely DL frameworks and specialized toolkits, are in focus throughout
    this survey and described in the following.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DL软件涵盖了深度学习的不同方面。两种特殊类型，即**DL框架**和**专业工具包**，在本次调查中是重点讨论的内容，并在以下内容中进行描述。
- en: DL frameworks provide a comprehensive, reliable (well-tested/-maintained) and
    highly usable (modular structure, high-level interfaces) set of base components
    for building and evaluating deep learning systems. They typically provide layer
    modules that can be put together to form custom model architectures and gradient-descent-based
    optimizers for model training. They are designed in respect of DL-specific software
    requirements including flexible data pipelines, automatic differentiation and
    efficient processing (GPU/multicore utilization, device distribution). Examples
    include tensorflow (Abadi et al., [2015](#bib.bib1)), pytorch (Paszke et al.,
    [2019](#bib.bib71)) and MXNet (Chen et al., [2015](#bib.bib16)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**DL框架**提供了一组全面、可靠（经过良好测试/维护）且高度可用（模块化结构，高级接口）的基础组件，用于构建和评估深度学习系统。它们通常提供可以组合成自定义模型架构的层模块和用于模型训练的基于梯度下降的优化器。它们在设计时考虑了DL特定的软件需求，包括灵活的数据管道、自动微分和高效处理（GPU/多核利用，设备分配）。示例包括tensorflow（Abadi等，[2015](#bib.bib1)），pytorch（Paszke等，[2019](#bib.bib71)）和MXNet（Chen等，[2015](#bib.bib16)）。'
- en: Specialized toolkits extend the capabilities of DL frameworks. They typically
    provide a collection of commonly used tools within a sub-field of deep learning.
    Examples include spaCy (Honnibal et al., [2020](#bib.bib41)) for natural language
    processing, GluonCV (Guo et al., [2020](#bib.bib34)) for computer vision as well
    as the adversarial robustness toolbox (Nicolae et al., [2018](#bib.bib66)). The
    UE toolkits analyzed in this work fall into this category of DL software.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 专用工具包扩展了 DL 框架的能力。它们通常在深度学习的一个子领域中提供一组常用工具。示例包括用于自然语言处理的 spaCy (Honnibal et
    al., [2020](#bib.bib41))，用于计算机视觉的 GluonCV (Guo et al., [2020](#bib.bib34)) 以及对抗鲁棒性工具箱
    (Nicolae et al., [2018](#bib.bib66))。本研究中分析的 UE 工具包属于这一类别的 DL 软件。
- en: Surveys of DL software
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: DL 软件的调查
- en: Surveys on DL software concentrate mostly on the systematic assessment of DL
    frameworks (Nguyen et al., [2019](#bib.bib65); Wang et al., [2019](#bib.bib87);
    Landset et al., [2015](#bib.bib50); Druzhkov & Kustikova, [2016](#bib.bib20);
    Bahrampour et al., [2015](#bib.bib8)). Typical criteria for analysis include the
    range of provided components, GPU/multiprocessing support, integration of big
    data frameworks and general software criteria such as efficiency/scalability,
    ease of use and framework design/extensibility. Only few studies exist regarding
    specialized toolkits, such as Zacharias et al. ([2018](#bib.bib92)) who concentrate
    on libraries for intelligent user interfaces or Agarwal & Das ([2020](#bib.bib2))
    who compare toolkits for interpretable machine learning under the aspects of range
    of provided methods and use cases. We seek to extend the overview on software
    for trustworthy AI with this survey on UE toolkits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DL 软件的调查主要集中在对 DL 框架的系统评估（Nguyen et al., [2019](#bib.bib65); Wang et al., [2019](#bib.bib87);
    Landset et al., [2015](#bib.bib50); Druzhkov & Kustikova, [2016](#bib.bib20);
    Bahrampour et al., [2015](#bib.bib8)）。分析的典型标准包括提供组件的范围、GPU/多处理支持、大数据框架的集成以及一般软件标准，如效率/可扩展性、易用性和框架设计/扩展性。关于专用工具包的研究很少，例如
    Zacharias et al. ([2018](#bib.bib92)) 关注于智能用户界面的库，或 Agarwal & Das ([2020](#bib.bib2))
    比较了可解释机器学习工具包在提供方法和使用案例方面的范围。我们希望通过这项关于 UE 工具包的调查扩展对可信 AI 软件的概述。
- en: Elements of uncertainty estimation
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不确定性估计的元素
- en: In the following, we summarize central aspects of uncertainty estimation for
    the analysis of this survey. Further details to these aspects are provided in
    a methodological “deep dive” in the appendix (section [6.1](#Sx1.SS1 "6.1 Detailed
    overview on uncertainty modeling and assessment ‣ Appendix ‣ A Survey on Uncertainty
    Toolkits for Deep Learning")).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们总结了不确定性估计的核心方面，以便分析本次调查。对这些方面的进一步细节请参阅附录中的方法学“深入探讨”（第 [6.1](#Sx1.SS1
    "6.1 Detailed overview on uncertainty modeling and assessment ‣ Appendix ‣ A Survey
    on Uncertainty Toolkits for Deep Learning") 节）。
- en: DL systems are affected by several types of uncertainties (Hüllermeier & Waegeman,
    [2021](#bib.bib43)). A distinction can be made between reducible epistemic uncertainty
    that arises from a lack of knowledge about the perfect model for solving a task,
    which includes lack of training data or approximation errors, and irreducible
    aleatoric uncertainty, which includes inherent randomness in the data-generating
    process (e.g. sensor noise or label ambiguity).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: DL 系统受到几种不确定性类型的影响（Hüllermeier & Waegeman, [2021](#bib.bib43)）。可以区分可减少的知识不确定性，这种不确定性源于对解决任务的完美模型缺乏知识，包括训练数据的缺乏或近似误差，以及不可减少的偶然不确定性，这包括数据生成过程中的固有随机性（例如传感器噪声或标签歧义）。
- en: 'UE methods seek to quantify such uncertainties. In this survey we categorize
    them along two independent “axes”: their integration depth into a given model
    and the methodological framework they are based on. Along the former axis, we
    differentiate between (i) intrinsic methods that integrate the uncertainty estimation
    directly into the architecture or training procedure (e.g. dropout layers or ensemble
    training), (ii) post-hoc methods that equip standard deep learning models with
    probability estimates and (iii) recalibration methods that seek to improve existing
    uncertainty estimates (Guo et al., [2017](#bib.bib33); Kuleshov et al., [2018](#bib.bib48);
    Navrátil et al., [2021](#bib.bib62)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: UE 方法旨在量化这些不确定性。在本调查中，我们将其按照两个独立的“轴”进行分类：它们在给定模型中的集成深度和它们所基于的方法框架。在前一个轴上，我们区分（i）内在方法，这些方法将不确定性估计直接集成到架构或训练过程中（例如，dropout
    层或集成训练），（ii）后验方法，这些方法为标准深度学习模型提供概率估计，以及（iii）重新校准方法，这些方法旨在改善现有的不确定性估计（Guo et al.,
    [2017](#bib.bib33); Kuleshov et al., [2018](#bib.bib48); Navrátil et al., [2021](#bib.bib62)）。
- en: The “axis” of methodological approaches comprises the following. (i) Parametric
    likelihood methods consider networks that directly output distributional parameters
    instead of point estimates and are trained via likelihood optimization (Nix &
    Weigend, [1994](#bib.bib67); Amini et al., [2020](#bib.bib6); Sensoy et al., [2018](#bib.bib75)).
    (ii) Bayesian neural networks (BNN) extend these approaches by incorporating (epistemic)
    uncertainty on network parameters. Examples include variational-inference based
    BNNs (VI-BNN, Graves ([2011](#bib.bib32)); Blundell et al. ([2015](#bib.bib12));
    Rezende & Mohamed ([2015](#bib.bib73))), their variants based on dropout-sampling
    (Gal & Ghahramani, [2016](#bib.bib24); Kendall & Gal, [2017](#bib.bib44))), Markov-chain
    Monte Carlo sampling (MCMC, Neal et al. ([2011](#bib.bib63)); Welling & Teh ([2011](#bib.bib89));
    Li et al. ([2016](#bib.bib51))) or assumed density filtering (ADF-BNN, Hernández-Lobato
    & Adams ([2015](#bib.bib39)); Gast & Roth ([2018](#bib.bib26))). A related Bayesian
    approach are Gaussian processes (GP, Rasmussen & Williams ([2006](#bib.bib72)))
    and their scalable variants (Hensman et al., [2013](#bib.bib38)) that incorporate
    uncertainty in function space. (iii) Frequentist approaches directly leverage
    a combination of different models for UE, e.g. deep ensembles (Lakshminarayanan
    et al., [2017](#bib.bib49); Wen et al., [2019](#bib.bib90); Durasov et al., [2021](#bib.bib21))
    or the jackknife method (Giordano et al., [2019](#bib.bib29); Alaa & Van Der Schaar,
    [2020](#bib.bib3); Kim et al., [2020](#bib.bib45)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 方法论的“轴”包括以下内容。(i) 参数似然方法考虑网络直接输出分布参数而不是点估计，并通过似然优化进行训练 (Nix & Weigend, [1994](#bib.bib67);
    Amini et al., [2020](#bib.bib6); Sensoy et al., [2018](#bib.bib75))。 (ii) 贝叶斯神经网络
    (BNN) 通过在网络参数上引入 (认识论的) 不确定性扩展了这些方法。示例包括基于变分推断的 BNN (VI-BNN, Graves ([2011](#bib.bib32));
    Blundell et al. ([2015](#bib.bib12)); Rezende & Mohamed ([2015](#bib.bib73)))，其基于丢弃采样的变体
    (Gal & Ghahramani, [2016](#bib.bib24); Kendall & Gal, [2017](#bib.bib44)))，马尔可夫链蒙特卡洛采样
    (MCMC, Neal et al. ([2011](#bib.bib63)); Welling & Teh ([2011](#bib.bib89)); Li
    et al. ([2016](#bib.bib51))) 或假设密度过滤 (ADF-BNN, Hernández-Lobato & Adams ([2015](#bib.bib39));
    Gast & Roth ([2018](#bib.bib26)))。相关的贝叶斯方法还有高斯过程 (GP, Rasmussen & Williams ([2006](#bib.bib72)))
    及其可扩展变体 (Hensman et al., [2013](#bib.bib38))，它们在函数空间中引入不确定性。(iii) 频率学派方法直接利用不同模型的组合进行不确定性估计，例如深度集成
    (Lakshminarayanan et al., [2017](#bib.bib49); Wen et al., [2019](#bib.bib90);
    Durasov et al., [2021](#bib.bib21)) 或切除法 (Giordano et al., [2019](#bib.bib29);
    Alaa & Van Der Schaar, [2020](#bib.bib3); Kim et al., [2020](#bib.bib45))。
- en: Several assessment techniques have been developed that enable the benchmarking
    of uncertainty methods and the evaluation of their quality. Most commonly, these
    include (i) (proper) scoring rules (Gneiting & Raftery, [2007](#bib.bib30)) that
    measure the fit of a predicted distribution to a ground-truth value (e.g. negative
    log-likelihood (NLL) or brier score), (ii) calibration testing (Guo et al., [2017](#bib.bib33);
    Kuleshov et al., [2018](#bib.bib48); Navrátil et al., [2021](#bib.bib62)) that
    assesses alignment with a validation dataset (e.g. reliability diagrams, excepted
    calibration error (ECE)), (iii) qualitative assessments (e.g. inspecting predicted
    distributions or the average predicted variance across a dataset) and (iv) the
    performance on auxiliary tasks, e.g. uncertainty-based separation of in-distribution
    and out-of-distribution datapoints as in Ovadia et al. ([2019](#bib.bib69)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 已开发出多种评估技术，用于对不确定性方法进行基准测试并评估其质量。最常见的评估技术包括 (i) (适当的) 评分规则 (Gneiting & Raftery,
    [2007](#bib.bib30))，这些规则测量预测分布与真实值的拟合程度（例如，负对数似然 (NLL) 或 Brier 分数），(ii) 校准测试 (Guo
    et al., [2017](#bib.bib33); Kuleshov et al., [2018](#bib.bib48); Navrátil et al.,
    [2021](#bib.bib62))，评估与验证数据集的一致性（例如，可靠性图，预期校准误差 (ECE)），(iii) 定性评估（例如，检查预测分布或数据集上的平均预测方差），以及
    (iv) 辅助任务的性能，例如 Ovadia et al. ([2019](#bib.bib69)) 中的基于不确定性的分布内和分布外数据点的分离。
- en: Uncertainty estimation in standard DL frameworks
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标准深度学习框架中的不确定性估计
- en: Common DL frameworks natively provide basic building blocks for uncertainty
    estimation. TensorFlow/Keras¹¹1Keras was originally a separate library but has
    become a part of tensorflow. It provides high-level APIs for model building, training
    and evaluation., MXNet or Pytorch²²2The UE toolkits that we consider in our analysis
    build upon these three DL frameworks. provide dropout functions that are originally
    intended for regularization (Srivastava et al., [2014](#bib.bib80)) but can also
    be used for building dropout-based uncertainty models. Pytorch exhibits the distributions
    package, which implements backpropagation-compatible probability distributions
    using the reparametrization trick (Kingma et al., [2015](#bib.bib46)). All mentioned
    frameworks support basic assessment techniques, e.g. mean squared error or histograms.
    Despite these implementations, building a fully featured uncertainty model still
    typically requires significant implementation effort. For example, implementing
    a VI-BNN requires custom layer modules that impose a probability distribution
    on network parameters and the optimization of a custom objective function. We
    instead focus on toolkits that build on these existing functionalities and provide
    a higher level of abstraction for reducing implementation effort.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的深度学习框架原生提供了基本的不确定性估计构件。TensorFlow/Keras¹¹Keras 最初是一个独立的库，但已成为 TensorFlow
    的一部分。它提供了用于模型构建、训练和评估的高级 API。MXNet 或 Pytorch²²我们在分析中考虑的不确定性工具包是基于这三个深度学习框架的。提供了原本用于正则化的
    dropout 函数（Srivastava 等，[2014](#bib.bib80)），但也可以用于构建基于 dropout 的不确定性模型。Pytorch
    展示了分布包，它使用重新参数化技巧（Kingma 等，[2015](#bib.bib46)）实现了兼容反向传播的概率分布。所有提到的框架都支持基本的评估技术，例如均方误差或直方图。尽管有这些实现，构建一个功能齐全的不确定性模型仍然通常需要大量的实现工作。例如，实现一个
    VI-BNN 需要自定义的层模块，这些模块在网络参数上施加概率分布，并优化一个自定义目标函数。我们则专注于那些基于这些现有功能并提供更高抽象层次以减少实现工作量的工具包。
- en: 3 Selection of deep uncertainty toolkits
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度不确定性工具包的选择
- en: The first step of our systematic survey is to compile a collection of open-source
    toolkits on deep uncertainty estimation, which is the goal of this section. For
    this, we first describe our procedure for compiling a pre-selection of potential
    code repositories on deep UE. Next, we collect basic criteria a code repository
    needs to fulfill to be considered as an uncertainty toolkit in the scope of this
    work. We finally apply these criteria to the pre-selection to obtain a collection
    of $11$ deep uncertainty toolkits that we use in our comparative analysis (see
    section [5](#S5 "5 Comparative analysis of the selected uncertainty toolkits ‣
    A Survey on Uncertainty Toolkits for Deep Learning")).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统性调查的第一步是编制一个深度不确定性估计的开源工具包集合，这是本节的目标。为此，我们首先描述了编制潜在代码库预选的过程。接下来，我们收集了一个代码库需要满足的基本标准，以便在本工作的范围内被视为不确定性工具包。最后，我们将这些标准应用于预选，获得了一组
    $11$ 个深度不确定性工具包，这些工具包用于我们的比较分析（见第 [5](#S5 "5 Comparative analysis of the selected
    uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning") 节）。
- en: Search for code repositories related to UE in DL
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 搜索与深度学习中的不确定性估计相关的代码库
- en: In this survey, we focus on open-source code repositories that are available
    on `github.com`. We concentrate on github as it is the largest platform to host
    open-source code and to our knowledge hosts most of the publicly available deep
    uncertainty estimation software. We use the public search function on github to
    find uncertainty-related code repositories. In particular, we make use of “topics”,
    specific keywords for tagging repositories that allow other users to find these
    repositories more easily. In addition, users can “star” repositories they are
    especially interested in, which is a way of bookmarking the repository. We examined
    the topics “uncertainty-quantification”, “uncertainty-estimation” and also searched
    for the keywords “uncertainty” and “probabilistic” restricted to repositories
    of the topics “machine-learning” and “deep-learning”. In each case, we ordered
    the search results by the total number of “stars” they received so far and examined
    the first $30$ repositories in the resulting list, which adds up to a total of
    $180$ repositories. Among these repositories, there are deep uncertainty estimation
    libraries, various implementations accompanying research papers as well as software
    without a clear focus on deep learning, e.g. general libraries on sensitivity
    analysis for numerical models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们重点关注在 `github.com` 上可用的开源代码库。我们集中在 github 上，因为它是最大的开源代码托管平台，并且据我们所知，托管了大多数公开可用的深度不确定性估计软件。我们使用
    github 的公共搜索功能来查找与不确定性相关的代码库。特别地，我们利用“主题”，即用于标记代码库的特定关键字，使其他用户能够更容易地找到这些代码库。此外，用户可以为他们特别感兴趣的代码库“加星”，这是一种书签式的标记方法。我们检查了“uncertainty-quantification”、“uncertainty-estimation”这些主题，并且还搜索了关键字“uncertainty”和“probabilistic”，并将搜索限制在“machine-learning”和“deep-learning”主题的代码库中。在每种情况下，我们按收到的“星标”总数对搜索结果进行排序，并检查了结果列表中的前
    $30$ 个代码库，总计 $180$ 个代码库。在这些代码库中，有深度不确定性估计库、各种配套研究论文的实现，以及一些不明确集中在深度学习上的软件，例如关于数值模型的灵敏度分析的通用库。
- en: Criteria constituting an uncertainty toolkit
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 组成不确定性工具包的标准
- en: 'As the focus of our work is on toolkits in supervised deep learning, we further
    filter the $180$ repositories. We count a software library as a *deep uncertainty
    toolkit* if it contains multiple methods for at least one of the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的工作重点是监督式深度学习中的工具包，我们进一步筛选了这 $180$ 个代码库。如果一个软件库包含至少一种以下方法，我们将其视为 *深度不确定性工具包*：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: building and training a deep learning model that (inherently) supports UE (intrinsic
    methods),
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建和训练一个（内在地）支持 UE 的深度学习模型（内在方法），
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: extending a given deep learning model with uncertainty estimates (post-hoc methods),
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩展给定的深度学习模型以包含不确定性估计（事后方法），
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: improving already existing uncertainty estimates (recalibration) or
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 改进已经存在的不确定性估计（重新校准）或
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: assessing uncertainty estimates in terms of metrics or visualizations.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过指标或可视化来评估不确定性估计。
- en: Additionally, we require a toolkit to provide methods that are generally applicable
    to a broad range of scenarios, in particular to different datasets or network
    architectures. Sufficient documentation must be provided and either an application
    programming interface (API) or an (interactive) graphical user interface (GUI)
    application (in case of a standalone program). The toolkit should have a clear
    focus on deep learning as evident from the documentation and should be integrated
    with common DL frameworks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们要求工具包提供对广泛场景普遍适用的方法，特别是对不同数据集或网络架构。必须提供充分的文档，并且需要有应用编程接口（API）或（交互式）图形用户界面（GUI）应用程序（如果是独立程序）。工具包应明确集中于深度学习，从文档中可以看出，并且应与常见的
    DL 框架集成。
- en: Selected toolkits
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择的工具包
- en: Out of the $180$ repositories, $11$ satisfy the above outlined criteria constituting
    uncertainty toolkits. They are listed in [Table 1](#Sx1.T1 "Table 1 ‣ 6.2 Comparative
    analysis of the selected uncertainty toolkits ‣ Appendix ‣ A Survey on Uncertainty
    Toolkits for Deep Learning") in the appendix. Among these, we find toolkits dedicated
    purely to deep uncertainty estimation (dedicated UE) as well as libraries with
    a broader scope that also provide UE capabilities. The latter category includes
    GluonTS (GTS, Alexandrov et al. ([2020](#bib.bib5))), a probabilistic time series
    library, and numerous deep probabilistic programming libraries (PPLs) that aim
    at specifying general Bayesian networks and performing inference for such models
    (van de Meent et al., [2018](#bib.bib85)). Namely these are Tensorflow Probability
    (TFP, Dillon et al. ([2017](#bib.bib19))), Pyro (Bingham et al., [2019](#bib.bib11)),
    Edward2 (ED2, Tran et al. ([2018](#bib.bib84))), ZhuSuan (ZS, Shi et al. ([2017](#bib.bib76)))
    and MXFusion (MXF, Meissner et al. ([2019](#bib.bib57))).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在$180$个仓库中，有$11$个满足上述不确定性工具包的标准。它们列在附录中的[表1](#Sx1.T1 "Table 1 ‣ 6.2 Comparative
    analysis of the selected uncertainty toolkits ‣ Appendix ‣ A Survey on Uncertainty
    Toolkits for Deep Learning")。其中，我们发现了专门用于深度不确定性估计的工具包（专门的UE）以及提供UE能力的范围更广的库。后者类别包括**GluonTS**（GTS,
    Alexandrov等人 ([2020](#bib.bib5)))，一个概率时间序列库，以及多个深度概率编程库（PPLs），这些库旨在指定通用贝叶斯网络并为这些模型执行推断（van
    de Meent等人, [2018](#bib.bib85)）。具体包括**Tensorflow概率**（TFP, Dillon等人 ([2017](#bib.bib19)))、**Pyro**（Bingham等人,
    [2019](#bib.bib11)）、**Edward2**（ED2, Tran等人 ([2018](#bib.bib84)))、**ZhuSuan**（ZS,
    Shi等人 ([2017](#bib.bib76))) 和**MXFusion**（MXF, Meissner等人 ([2019](#bib.bib57)))。
- en: 'Among the dedicated UE toolkits is, for instance, Uncertainty Quantification
    360 (UQ360, Ghosh et al. ([2021](#bib.bib28))), which provides several uncertainty
    estimation tools and is part of a larger set of toolkits developed by IBM Research,
    each targeting individual dimensions of AI trustworthiness, including “AI Fairness
    360” (Bellamy et al., [2018](#bib.bib9)) and “AI Explainability 360” (Arya et al.,
    [2020](#bib.bib7)). We also find several libraries with a more narrow range of
    functionality: Uncertainty Toolbox (UT, Chung et al. ([2021](#bib.bib18))) focuses
    on the recalibration and the assessment of uncertainty estimates in standard regression
    tasks. Uncertainty Wizard (UW, Weiss & Tonella ([2021](#bib.bib88))) is a package
    for extending keras models with dropout and ensemble-based uncertainty estimation
    capabilities. Bayesian Torch (BT, Krishnan et al. ([2022](#bib.bib47))) provides
    pytorch layer modules for building variational Bayesian neural networks and Keras-ADF
    (KADF, Maces & Contributors ([2019](#bib.bib54))) provides keras layer modules
    for assumed density filtering. All of these toolkits are released under permissive
    free software licenses (namely Apache-2.0, MIT or BSD3) that come along with only
    minimal restrictions and allow, amongst others, their commercial usage.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在专门的UE工具包中，例如**不确定性量化360**（UQ360, Ghosh等人 ([2021](#bib.bib28)))，它提供了多个不确定性估计工具，并且是IBM研究开发的更大工具包集的一部分，每个工具包针对AI可信度的不同维度，包括“**AI公平性360**”（Bellamy等人,
    [2018](#bib.bib9)）和“**AI可解释性360**”（Arya等人, [2020](#bib.bib7)）。我们还发现了几种功能范围较窄的库：**不确定性工具箱**（UT,
    Chung等人 ([2021](#bib.bib18))) 专注于标准回归任务中的不确定性估计的再校准和评估。**不确定性向导**（UW, Weiss &
    Tonella ([2021](#bib.bib88))) 是一个扩展keras模型以支持丢弃和基于集成的不确定性估计能力的包。**贝叶斯火炬**（BT,
    Krishnan等人 ([2022](#bib.bib47))) 提供了用于构建变分贝叶斯神经网络的pytorch层模块，而**Keras-ADF**（KADF,
    Maces & Contributors ([2019](#bib.bib54))) 提供了用于假定密度滤波的keras层模块。所有这些工具包均在宽松的自由软件许可证下发布（即Apache-2.0,
    MIT或BSD3），附带的限制非常少，并允许包括商业使用在内的多种使用方式。
- en: 4 Evaluation criteria for uncertainty toolkits
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 不确定性工具包的评估标准
- en: In this section, we detail our criteria that we use for analyzing the uncertainty
    estimation toolkits in section [5](#S5 "5 Comparative analysis of the selected
    uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning"). We
    divide both the criteria and the analysis in a “core” part, which we evaluate
    for all toolkits, and an extended more in-depth analysis for a selected subset
    of the most versatile tools, for which we also consider “additional” quality criteria.
    The “core” part focuses on the range of uncertainty modeling and evaluation techniques
    a framework supports as well as the neural architectures and data types/structures
    it is compatible with. The “additional” criteria moreover take code quality into
    account, both in terms of integration with standard DL frameworks as well as their
    level of documentation, modularity and testing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了用于分析不确定性估计工具包的标准，这些工具包在[5](#S5 "5 Comparative analysis of the selected
    uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning")中进行了比较分析。我们将标准和分析分为“核心”部分，所有工具包都进行评估，以及对选定的最具多功能性的工具进行更深入的扩展分析，我们还考虑了“附加”质量标准。“核心”部分着重于框架支持的不确定性建模和评估技术的范围以及它兼容的神经架构和数据类型/结构。“附加”标准还考虑了代码质量，包括与标准深度学习框架的集成程度以及文档、模块化和测试水平。
- en: '*Core criteria*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*核心标准*'
- en: Range of supported uncertainty methods
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的不确定性方法范围
- en: Each uncertainty estimation method comes with its own set of strengths and weaknesses
    w.r.t. estimation quality, computational/storage costs, practicability (ease of
    implementation, training and evaluation), flexibility or theoretical soundness.
    This implies that depending on the exact application scenario, some methods may
    be more suited than others. Thus, we consider the range of functionalities for
    building or improving uncertainty models or extending standard (deterministic)
    models a relevant (core) evaluation criterion.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每种不确定性估计方法都有其自身的优缺点，包括估计质量、计算/存储成本、可实践性（实施、培训和评估的难易程度）、灵活性或理论合理性。这意味着，根据具体应用场景，有些方法可能比其他方法更合适。因此，我们将构建或改进不确定性模型或扩展标准（确定性）模型的功能范围视为相关（核心）评估标准。
- en: Range of supported evaluation techniques
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的评估技术范围
- en: A crucial part in research and development of DL models is the assessment of
    their prediction quality and the comparison against other models. To enable such
    an assessment for uncertainty models, a toolkit should provide dedicated metrics
    for measuring the quality of uncertainty estimates, such as proper scoring rules,
    calibration or auxiliary scores. As no metric is universally suited for every
    application scenario, a toolkit should cover a wide range of different uncertainty
    metrics, potentially aided by visualizations (e.g. calibration plots or confidence
    bands).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的研究和开发中一个关键部分是评估其预测质量并与其他模型进行比较。为了使不确定性模型能够进行这种评估，工具包应提供专门的指标来测量不确定性估计的质量，如适当的评分规则、校准或辅助评分。由于没有任何指标适用于所有应用场景，工具包应覆盖各种不同的不确定性指标，可能还配有可视化（如校准图或置信区间）。
- en: Range of supported architectures and data structures
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的架构和数据结构范围
- en: A toolkit that only supports the multi-layer perceptron architecture hardly
    finds use in computer vision tasks due to its lack of support for convolutional
    layers or image inputs. This illustrates that to be applicable to different use
    cases, a toolkit should support a wide range of network architectures, optimizers
    and data structures, which we consider a relevant criterion for our evaluation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 仅支持多层感知器架构的工具包在计算机视觉任务中几乎没有用处，因为它不支持卷积层或图像输入。这表明，为了适用于不同的使用案例，工具包应支持广泛的网络架构、优化器和数据结构，我们将其视为评估的相关标准。
- en: '*Additional criteria*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*附加标准*'
- en: Integration with DL frameworks
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与深度学习框架的集成
- en: Deep learning software in general comes with a special set of requirements that
    include flexible data pipelines, efficient optimization (automatic differentiation,
    GPU/multicore utilization), reproducibility (logging of hyperparameters, seeds
    and configurations) and modular model building. DL frameworks like tensorflow
    or pytorch are (i) geared to these desiderata, (ii) well-maintained and exhibit
    a large community of developers and users and (iii) can be seen as de-facto standards
    when developing deep learning software. In order to profit from their properties,
    an uncertainty toolkit should actively employ tools from DL frameworks and exhibit
    high interoperability with them, e.g. by providing layer modules that can be inserted
    into existing framework-native models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习软件通常具有一套特殊的需求，包括灵活的数据管道、高效的优化（自动微分、GPU/多核利用）、可重复性（超参数、种子和配置的记录）和模块化模型构建。像tensorflow或pytorch这样的深度学习框架
    (i) 满足这些需求，(ii) 维护良好，并拥有庞大的开发者和用户社区，(iii) 可以被视为开发深度学习软件的事实标准。为了利用这些属性，一个不确定性工具包应积极使用深度学习框架中的工具，并与其高度互操作，例如，通过提供可以插入现有框架本地模型的层模块。
- en: Software quality
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软件质量
- en: We also evaluate the toolkits in terms of general software quality criteria
    including (i) usability and documentation quality, (ii) modularity and integrability
    and (iii) maintenance and testing. A highly usable toolkit is, among others, delivered
    with a simple installation procedure, a code architecture/API that is easy to
    understand and to work with and is well-documented. A highly modular toolkit provides
    flexible building blocks, ideally at different abstraction levels from high-level
    (being easy-to-use, but less flexible) to low-level (highest level of customization,
    but higher implementation effort and a higher degree of technical understanding
    required) that can be combined with each other. Important aspects of code maintenance
    include actively supporting code contributions, heeding coding style conventions
    (e.g. via code linting) and enforcing code testing (e.g. continuous integration
    and reporting code coverage).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还根据一般软件质量标准对工具包进行评估，包括 (i) 可用性和文档质量，(ii) 模块化和可集成性以及 (iii) 维护和测试。一个高可用的工具包通常具有简单的安装过程、易于理解和操作的代码架构/API，并且文档完善。一个高度模块化的工具包提供灵活的构建块，理想情况下，在从高层（易于使用，但灵活性较差）到低层（最高自定义级别，但实现难度更大且需要更高的技术理解）的不同抽象层次上，可以相互组合。代码维护的重要方面包括积极支持代码贡献、遵循编码风格规范（例如，通过代码检查）和强制代码测试（例如，持续集成和报告代码覆盖率）。
- en: 5 Comparative analysis of the selected uncertainty toolkits
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 选定不确定性工具包的比较分析
- en: In the following, we first analyze all uncertainty toolkits from section [3](#S3
    "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for
    Deep Learning") w.r.t. the core criteria of supported uncertainty methods, evaluation
    techniques and architectures/data structures (cf. section [4](#S4 "4 Evaluation
    criteria for uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep
    Learning")). Based on this analysis, we select the three most relevant toolkits,
    namely UQ360, Pyro and TFP, for an in-depth analysis in [subsection 5.2](#S5.SS2
    "5.2 Detailed analysis of TFP, Pyro and UQ360 ‣ 5 Comparative analysis of the
    selected uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning").
    This will, additionally, include the extended set of evaluation criteria from
    [section 4](#S4 "4 Evaluation criteria for uncertainty toolkits ‣ A Survey on
    Uncertainty Toolkits for Deep Learning"). We structure the analysis in both cases
    along the criteria (and not along the toolkits) to better contrast the toolkits
    against one another.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们首先根据核心标准分析第[3](#S3 "3 Selection of deep uncertainty toolkits ‣ A Survey
    on Uncertainty Toolkits for Deep Learning")节中的所有不确定性工具包，包括支持的不确定性方法、评估技术和架构/数据结构（参见第[4](#S4
    "4 Evaluation criteria for uncertainty toolkits ‣ A Survey on Uncertainty Toolkits
    for Deep Learning")节）。基于此分析，我们选择了三个最相关的工具包，即UQ360、Pyro和TFP，在[5.2节](#S5.SS2 "5.2
    Detailed analysis of TFP, Pyro and UQ360 ‣ 5 Comparative analysis of the selected
    uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for Deep Learning")进行深入分析。此分析还将包括第[4](#S4
    "4 Evaluation criteria for uncertainty toolkits ‣ A Survey on Uncertainty Toolkits
    for Deep Learning")节中的扩展评估标准。在这两种情况下，我们都按照标准而不是工具包来组织分析，以便更好地对比工具包之间的差异。
- en: 5.1 Analysis with respect to core criteria
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 针对核心标准的分析
- en: We now briefly compare all $11$ uncertainty toolkits selected in [section 3](#S3
    "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for
    Deep Learning") w.r.t. the core criteria. A concise summary of these findings
    per toolkit can be found in the upper part of [Table 2](#Sx1.T2 "Table 2 ‣ 6.2
    Comparative analysis of the selected uncertainty toolkits ‣ Appendix ‣ A Survey
    on Uncertainty Toolkits for Deep Learning") in the appendix.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在简要比较所有 $11$ 个不确定性工具包，这些工具包在[第 3 节](#S3 "3 选择深度不确定性工具包 ‣ 深度学习不确定性工具包调查")中被选择，重点关注核心标准。每个工具包的简要总结可以在附录中的[表
    2](#Sx1.T2 "表 2 ‣ 6.2 所选不确定性工具包的比较分析 ‣ 附录 ‣ 深度学习不确定性工具包调查")的上部找到。
- en: Range of supported uncertainty methods
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的不确定性方法范围
- en: UQ360 provides the widest range of methods, covering intrinsic, post-hoc and
    recalibration methods. All other toolkits cover only intrinsic methods, except
    for UT, which has a narrow focus on basic recalibration methods. The deep PPLs
    provide intrinsic Bayesian methods, mainly MCMC, VI-BNNs or Gaussian processes.
    In contrast to comparable methods from BT or UQ360, they typically cover a broader
    range of prior, likelihood and variational posterior distributions. The time series
    library GTS only provides parametric likelihood methods, but supports a large
    range of distributions (especially compared to UQ360 that only supports Gaussian
    likelihoods). UW, BT and KADF are dedicated UE libraries with a narrow focus on
    the intrinsic methods dropout/ensembling, VI-BNN and ADF-BNN, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360 提供最广泛的方法，包括内在、后验和重新校准方法。所有其他工具包仅覆盖内在方法，除了 UT，其关注于基本的重新校准方法。深度 PPLs 提供内在贝叶斯方法，主要是
    MCMC、VI-BNNs 或高斯过程。与 BT 或 UQ360 的可比方法相比，它们通常涵盖更广泛的先验、似然和变分后验分布。时间序列库 GTS 仅提供参数化似然方法，但支持大量分布（尤其是相比
    UQ360 仅支持高斯似然）。UW、BT 和 KADF 是专门的UE库，分别专注于内在方法 dropout/ensembling、VI-BNN 和 ADF-BNN。
- en: Range of supported evaluation techniques
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的评估技术范围
- en: UQ360 and UT have the most comprehensive set of assessment tools ranging from
    scoring rules over calibration scores and plotting functions, followed by TFP,
    which lacks plotting functions. GTS provides scoring rules and a function for
    plotting forecasted time series with confidence bands. In comparison, Pyro, ED2
    and ZS have more narrow capabilities and focus on the standard evaluation metric
    in Bayesian inference, namely computing log-probabilities of the predictive distribution.
    Some toolkits (UW, BT, KADF and MXF) do not contain any assessment capabilities.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360 和 UT 拥有最全面的评估工具集，从评分规则到校准评分和绘图功能，其次是 TFP，缺乏绘图功能。GTS 提供评分规则以及用于绘制带置信区间的预测时间序列的函数。相比之下，Pyro、ED2
    和 ZS 的能力较为狭窄，专注于贝叶斯推断中的标准评估指标，即计算预测分布的对数概率。一些工具包（UW、BT、KADF 和 MXF）不包含任何评估能力。
- en: Range of supported architectures and data structures
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的架构和数据结构范围
- en: Deep PPLs are constructed to enable Bayesian inference for a broad range of
    models with different architectures. UW, BT and KDF provide support for classification
    and regression models. The former provides a keras-based model interface for this
    purpose, while BT and KDF provide drop-in replacements for dense and convolutional
    layers. BT additionally covers recurrent models by including probabilistic long
    short-term memory layers (Hochreiter & Schmidhuber, [1997](#bib.bib40)). In contrast,
    GTS and UT focus on specific domains such as time series modelling (GTS) or 1D
    regression (UT).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 深度概率编程语言（PPLs）旨在实现对各种不同架构模型的贝叶斯推断。UW、BT 和 KDF 提供对分类和回归模型的支持。前者提供基于 Keras 的模型接口，而
    BT 和 KDF 则提供密集层和卷积层的替代方案。BT 还通过包含概率长短期记忆层（Hochreiter & Schmidhuber, [1997](#bib.bib40)）来覆盖递归模型。相比之下，GTS
    和 UT 专注于特定领域，如时间序列建模（GTS）或一维回归（UT）。
- en: Overall, we find TFP, Pyro and UQ360 to provide the most comprehensive catalog
    of models, assessment techniques and supported architectures.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们发现 TFP、Pyro 和 UQ360 提供了最全面的模型、评估技术和支持架构目录。
- en: 5.2 Detailed analysis of TFP, Pyro and UQ360
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 TFP、Pyro 和 UQ360 的详细分析
- en: We now extend our analysis for the three most promising toolkits, namely UQ360,
    Pyro and TFP. In particular, we provide more details w.r.t. the core criteria
    and consider the two “additional” criteria, integration with DL frameworks and
    software quality. For a tabular summary of the results, see the bottom part of
    [Table 2](#Sx1.T2 "Table 2 ‣ 6.2 Comparative analysis of the selected uncertainty
    toolkits ‣ Appendix ‣ A Survey on Uncertainty Toolkits for Deep Learning") in
    the appendix.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在扩展了对三种最有前景的工具包的分析，即 UQ360、Pyro 和 TFP。特别是，我们提供了更多关于核心标准的细节，并考虑了两个“额外”标准：与深度学习框架的集成和软件质量。有关结果的表格摘要，请参见附录中[表
    2](#Sx1.T2 "Table 2 ‣ 6.2 Comparative analysis of the selected uncertainty toolkits
    ‣ Appendix ‣ A Survey on Uncertainty Toolkits for Deep Learning")的底部部分。
- en: Range of supported uncertainty methods
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的不确定性方法范围
- en: As discussed before, UQ360 provides a wide range of methods at different levels
    of integration depth. Specifically, its intrinsic methods comprise pure aleatoric
    uncertainty estimation via Gaussian likelihoods as well as joint modeling approaches
    for aleatoric and epistemic uncertainty via VI-BNNs or deep ensembles. Post-hoc
    methods include the infinitesimal jackknife (Giordano et al., [2019](#bib.bib29))
    and surrogate model approaches (Chen et al., [2019](#bib.bib17)). UQ360 moreover
    is the only toolkit with recalibration methods other than standard Platt scaling
    or isotonic regression and additionally includes e.g. auxiliary interval predictors
    (Thiagarajan et al., [2020](#bib.bib81)) and UCC rescaling (Navrátil et al., [2021](#bib.bib62)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，UQ360 提供了不同集成深度下的广泛方法。具体来说，它的内在方法包括通过高斯似然进行的纯偶然不确定性估计，以及通过 VI-BNNs 或深度集成进行的偶然和认知不确定性的联合建模方法。事后方法包括无穷小的拔刀法（Giordano
    et al., [2019](#bib.bib29)）和替代模型方法（Chen et al., [2019](#bib.bib17)）。此外，UQ360 还是唯一提供了除标准
    Platt 缩放或等距回归外的重新校准方法的工具包，并额外包括了例如辅助区间预测器（Thiagarajan et al., [2020](#bib.bib81)）和
    UCC 重新缩放（Navrátil et al., [2021](#bib.bib62)）。
- en: The PPLs TFP and Pyro cover intrinsic methods such as parametric likelihood
    (aleatoric uncertainty), VI-BNNs (aleatoric + epistemic) and (variational) Gaussian
    processes (functional uncertainty). They also provide non-parametric posterior
    sampling methods such as Hamiltonian Monte Carlo (Neal et al., [2011](#bib.bib63))
    or general Metropolis-Hastings (Metropolis et al., [1953](#bib.bib58); Hastings,
    [1970](#bib.bib37)), which may be useful for researchers due to its high estimation
    accuracy, but is typically infeasible to apply in large-scale models. A more scalable
    variant, SGLD (Welling & Teh, [2011](#bib.bib89); Li et al., [2016](#bib.bib51)),
    is additionally provided by TFP. By design, the PPLs support a much broader range
    of prior, likelihood and variational posterior distributions compared to the corresponding
    modules of UQ360\. For example, it is possible to employ normalizing flows (Rezende
    & Mohamed, [2015](#bib.bib73)) for more flexible priors or posteriors.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PPLs TFP 和 Pyro 涵盖了内在方法，如参数似然（偶然不确定性）、VI-BNNs（偶然 + 认知）和（变分）高斯过程（功能不确定性）。它们还提供了非参数后验采样方法，如哈密顿蒙特卡洛（Hamiltonian
    Monte Carlo）（Neal et al., [2011](#bib.bib63)）或一般的梅特罗波利斯-哈斯廷斯方法（Metropolis et al.,
    [1953](#bib.bib58); Hastings, [1970](#bib.bib37)），由于其高估计准确性，对于研究人员可能很有用，但通常在大规模模型中难以应用。TFP
    还提供了一个更具可扩展性的变体，即 SGLD（Welling & Teh, [2011](#bib.bib89); Li et al., [2016](#bib.bib51)）。从设计上讲，与
    UQ360 的相应模块相比，PPLs 支持更广泛的先验、似然和变分后验分布。例如，可以使用归一化流（Rezende & Mohamed, [2015](#bib.bib73)）来实现更灵活的先验或后验分布。
- en: Range of supported evaluation techniques
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的评估技术范围
- en: UQ360 offers a wide range of assessment techniques. It covers standard metrics,
    namely the Gaussian NLL for regression, the classification ECE and the Brier score
    as well as less frequently used scores such as the area under the risk-rejection-rate
    curve (Franc & Prusa, [2019](#bib.bib23)) or the uncertainty calibration curve
    (UCC, Navrátil et al. ([2021](#bib.bib62))). Apart from these metrics, it has
    helpful “tools” for assessment such as a function that decomposes samples of class
    probabilities into aleatoric and epistemic uncertainty or various plotting functions
    (e.g. calibration curves and prediction intervals).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360 提供了广泛的评估技术。它涵盖了标准指标，即回归的高斯负对数似然（Gaussian NLL）、分类的期望校准误差（ECE）和布里尔评分（Brier
    score），以及一些不常用的评分，如风险-拒绝率曲线下面积（Franc & Prusa，[2019](#bib.bib23)）或不确定性校准曲线（UCC，Navrátil
    et al. ([2021](#bib.bib62)））。除了这些指标外，它还有一些有用的评估“工具”，如一个将类别概率样本分解为偶然不确定性和认知不确定性的函数，或各种绘图函数（例如，校准曲线和预测区间）。
- en: In comparison, TFP has more narrow assessment capabilities when it comes to
    comparing uncertainty estimates with ground truth labels and provides the above
    mentioned standard metrics for this purpose. Instead of scalar values, a TFP model
    predicts distribution objects. These provide functions for computing standard
    statistics such as log-probabilities, moments, quantiles, correlation, cumulative
    distribution functions (cdf’s) and KL divergences to other distributions. These
    statistics can be computed for a wide range of distributions besides the Gaussian,
    including e.g. multivariate distributions and mixture models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，TFP 在将不确定性估计与真实标签进行比较时具有较窄的评估能力，并为此目的提供了上述标准指标。与标量值不同，TFP 模型预测分布对象。这些对象提供了计算标准统计数据的函数，如对数概率、矩、分位数、相关性、累积分布函数
    (cdf’s) 和与其他分布的 KL 散度。这些统计数据可以计算广泛的分布范围，除了高斯分布，还包括例如多变量分布和混合模型。
- en: Pyro offers a module for approximate sampling from the predictive distribution
    of a given Bayesian model, which serves as a basis for further assessment. For
    this, it provides general statistical utilities (e.g. for computing quantiles,
    autocorrelation or prediction intervals) and the CRPS (Gneiting & Raftery, [2007](#bib.bib30))
    as a scoring rule.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Pyro 提供了一个用于从给定贝叶斯模型的预测分布中进行近似采样的模块，作为进一步评估的基础。为此，它提供了通用的统计工具（例如计算分位数、自相关或预测区间）和
    CRPS (Gneiting & Raftery, [2007](#bib.bib30)) 作为评分规则。
- en: Range of supported architectures and data structures
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 支持的架构和数据结构范围
- en: UQ360’s method catalog covers regression and classification scenarios (mostly
    in 1D). However, many model classes (e.g. VI-BNN or deep ensembles) have pre-written
    training procedures or even architectures, which limits the ability to extend
    a given deep learning model with uncertainty estimates. Inputs are typically required
    to be of (2D) tabular form, limiting the ability to deal with sequential data
    or other data types. In contrast, TFP is a collection of smaller building blocks
    and provides layer and optimizer modules that can be used in addition to or as
    drop-in replacements for the standard tensorflow modules. It provides, for instance,
    probabilistic drop-in replacement modules for dense or convolutional layers to
    turn given NNs into BNNs. For pytorch, Pyro provides comparable capacities for
    turning NNs into BNNs, essentially by subclassing the respective NN classes and
    replacing the pytorch parameters with stochastic modules. The highly modular structure
    of both PPLs allows to build models with a wide variety of different architectures
    and inputs, for instance, for multidimensional regression and for sequential data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360 的方法目录涵盖了回归和分类场景（主要是 1D）。然而，许多模型类（例如 VI-BNN 或深度集成）具有预写的训练程序或甚至架构，这限制了将给定深度学习模型扩展为包含不确定性估计的能力。输入通常要求为（2D）表格形式，限制了处理序列数据或其他数据类型的能力。相比之下，TFP
    是一组较小的构建块，提供了可以用作标准 tensorflow 模块的补充或替代的层和优化器模块。例如，它提供了概率的替代模块，用于密集层或卷积层，以将给定的
    NNs 转换为 BNNs。对于 pytorch，Pyro 提供了将 NNs 转换为 BNNs 的类似能力，主要通过对相应的 NN 类进行子类化，并用随机模块替换
    pytorch 参数。这两个 PPL 的高度模块化结构允许构建具有各种不同架构和输入的模型，例如多维回归和序列数据。
- en: Integration with deep learning frameworks
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与深度学习框架的集成
- en: As Tensorflow Probability (TFP) is a part of the tensorflow ecosystem, its modules
    are designed to interface seamlessly with other modules of tensorflow/keras. In
    particular, it integrates with the keras model API, which allows the developer
    to build and train probabilistic models using keras’ native `compile` and `fit`
    functions. Pyro utilizes layer, optimizer, data and distribution functions from
    its base framework pytorch. When building BNNs in Pyro, one can use standard pytorch
    modules (e.g. layers and activation functions) in addition to the newly provided
    stochastic modules. Moreover, models and inference procedures can be encapsulated
    as pytorch modules, which enables the creation of TorchScript programs for deployment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Tensorflow Probability (TFP) 是 Tensorflow 生态系统的一部分，它的模块旨在与 tensorflow/keras
    的其他模块无缝接口。特别是，它与 keras 模型 API 集成，这使得开发者可以使用 keras 的原生 `compile` 和 `fit` 函数来构建和训练概率模型。Pyro
    利用其基础框架 pytorch 的层、优化器、数据和分布函数。在 Pyro 中构建 BNN 时，除了新提供的随机模块外，还可以使用标准的 pytorch 模块（例如层和激活函数）。此外，模型和推理过程可以被封装为
    pytorch 模块，从而能够创建用于部署的 TorchScript 程序。
- en: Most model classes of UQ360 are based on pytorch. These are easy-to-use, but
    not as flexible and modular as TFP’s building blocks for keras models. Changing
    network architectures, input pipelines or the training procedure often requires
    modifications to the code, instead of simply passing different modules (e.g. optimizers
    and dataloaders) to the model class.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360的大多数模型类基于pytorch。这些类易于使用，但不如TFP为keras模型提供的构建块那样灵活和模块化。更改网络架构、输入管道或训练过程通常需要修改代码，而不是简单地将不同的模块（例如优化器和数据加载器）传递给模型类。
- en: Software quality
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 软件质量
- en: All three toolkits have a simple installation procedure and are listed on the
    Python package index. The toolkits and all their dependencies can be installed
    via a single pip command, except for TFP, which requires installing a compatible
    version of tensorflow beforehand. Pyro additionally provides, besides pip, a docker
    image for installation. There is a sensible choice of dependencies across the
    toolkits, using a base DL framework plus standard libraries like numpy, scipy,
    matplotlib or pandas. All toolkits are actively maintained, welcome to post issues
    and pull requests and provide an explicit documentation for contributing (via
    a readme). TFP and Pyro follow coding style guides (e.g. PEP8 ³³3PEP8 are coding
    style guidelines used by the official python distribution.) to ensure that newly
    added code fits to coding conventions and, moreover, employ continuous integration
    for code testing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 三个工具包都有简单的安装过程，并且在Python包索引中列出。除了TFP需要事先安装兼容的tensorflow版本外，这些工具包及其所有依赖项都可以通过单个pip命令进行安装。Pyro除了提供pip，还提供了一个用于安装的docker镜像。各工具包之间的依赖选择合理，使用了基础的深度学习框架以及numpy、scipy、matplotlib或pandas等标准库。所有工具包都在积极维护中，欢迎提交问题和拉取请求，并提供明确的贡献文档（通过readme）。TFP和Pyro遵循编码风格指南（例如PEP8³³3PEP8是官方python发行版使用的编码风格指南），以确保新增代码符合编码规范，并且采用持续集成进行代码测试。
- en: We next discuss interface structures of the toolkits. UQ360 has a simple scikit-learn-esque
    API (i.e. model classes providing `fit` and `predict` functions taking numpy arrays
    as input) that is equally intuitive to understand. TFP employs the keras model
    API, which is highly flexible and also easy to understand. Pyro’s API puts more
    emphasis on the Bayesian modelling perspective of uncertainty estimation. The
    user needs to define two separate classes (or functions), where one defines the
    model with its prior and likelihood distributions and the other (the so-called
    guide) defines the variational posterior distribution. As Pyro uses an API structure
    different from the widely known APIs (for instance of such as sklearn or keras),
    it might be more difficult for users to quickly get started with it. On the other
    hand, the possibility to automatically generate standard posterior distributions
    (e.g. mean-field VI) or to create constrained network parameters (e.g. positive
    scale parameters for distributions) improves API usability. In comparison to TFP,
    Pyro misses direct drop-in replacements for standard layers, which results in
    a higher implementation effort for customizations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论工具包的接口结构。UQ360具有简单的类似scikit-learn的API（即模型类提供`fit`和`predict`函数，接收numpy数组作为输入），理解起来同样直观。TFP采用keras模型API，这个API非常灵活且易于理解。Pyro的API则更侧重于不确定性估计的贝叶斯建模视角。用户需要定义两个单独的类（或函数），其中一个定义模型及其先验和似然分布，另一个（所谓的guide）定义变分后验分布。由于Pyro使用的API结构与广泛知晓的API（例如sklearn或keras）不同，用户可能更难迅速上手。另一方面，自动生成标准后验分布（例如均值场VI）或创建约束的网络参数（例如分布的正尺度参数）的可能性提高了API的可用性。与TFP相比，Pyro缺乏对标准层的直接替代品，这导致自定义实现的工作量更大。
- en: The code of UQ360 is simple and highly readable, but could more frequently employ
    input checks (e.g. for types or shapes) and proper exception handling. The code
    bases of Pyro and TFP are designed to be flexible and to satisfy compatibility
    requirements (e.g. implementing functions of superclasses) of their respective
    base library (pytorch or tensorflow) and have (partly as a result from this) a
    more complex and covert structure. It is of high overall quality, which manifests
    in appropriate input checking and exception handling and in a clean and consistent
    coding style.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: UQ360 的代码简单且高度可读，但可以更频繁地进行输入检查（例如类型或形状）和适当的异常处理。Pyro 和 TFP 的代码基础设计得更加灵活，以满足其各自基础库（pytorch
    或 tensorflow）的兼容性要求（例如实现超类的函数），因此其结构较为复杂且隐蔽。整体质量较高，体现在适当的输入检查和异常处理，以及干净且一致的编码风格上。
- en: 6 Discussion
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Reliable models should be able to identify the boundaries at which they function
    properly. Finding sources of uncertainty and quantifying their impact on the model
    performance contributes to this aspect. While many approaches to uncertainty estimation
    have been developed, there are entry barriers on their use, including high technical
    complexity. By providing high-quality software components, toolkits for UE help
    to overcome such entry barriers and additionally facilitate standardized evaluation.
    To help the reader in selecting an appropriate toolkit, we provide the first survey
    on existing deep uncertainty toolkits. We defined minimum requirements for such
    toolkits and analyzed $11$ of them with respect to range of supported uncertainty
    methods, evaluation techniques, architectures and data structures. The three most
    relevant ones (TFP, Pyro and UQ360) were additionally examined under the aspects
    of integration with DL frameworks and software quality. All analyzed toolkits
    provide modules to ease the development and assessment of uncertainty models.
    They encompass deep probabilistic programming libraries (e.g. Pyro, MXF, ZS) that
    focus on infusing Bayesian inference into DL models as well as toolkits dedicated
    to UE (e.g. UQ360, UT) that cover a broader range of methods or assessments. We
    plan to extend our detailed analysis to more toolkits in future iterations of
    this work. The survey also reveals desirable improvements to UE software and future
    incentives that are discussed in the following.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠的模型应该能够识别它们正常工作的边界。寻找不确定性来源并量化其对模型性能的影响有助于这一方面。尽管已经开发了许多不确定性估计方法，但它们的使用仍面临技术复杂性等障碍。通过提供高质量的软件组件，不确定性评估工具包有助于克服这些障碍，并促进标准化评估。为了帮助读者选择合适的工具包，我们提供了现有深度不确定性工具包的第一次调查。我们定义了这些工具包的最低要求，并分析了$11$个工具包在支持的不确定性方法、评估技术、架构和数据结构方面的表现。我们还在与深度学习框架的集成和软件质量方面进一步检查了三个最相关的工具包（TFP、Pyro
    和 UQ360）。所有分析的工具包都提供了简化不确定性模型开发和评估的模块。它们包括专注于将贝叶斯推断引入深度学习模型的深度概率编程库（例如 Pyro、MXF、ZS），以及涵盖更广泛方法或评估的专用不确定性工具包（例如
    UQ360、UT）。我们计划在未来的工作迭代中将详细分析扩展到更多工具包。调查还揭示了对不确定性评估软件的期望改进和未来的激励措施，接下来将讨论这些内容。
- en: Extend the technical capabilities of uncertainty toolkits
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 扩展不确定性工具包的技术能力
- en: 'The considered UE toolkits are either more comprehensive (e.g. UQ360) or more
    modular and interoperable with DL frameworks (e.g. TFP, Pyro, UW). An UE toolkit
    should ideally combine both aspects. Further concrete means for improving the
    technical comprehensiveness of currently available toolkits include: (i) providing
    more post-hoc methods, which are of high interest in scenarios where rebuilding
    a model or retraining is associated with high costs; (ii) infusing uncertainty
    into application-specific network components, e.g. into non-maximum suppression
    or clustering procedures of computer vision models (as in Meyer et al. ([2019](#bib.bib59));
    Harakeh et al. ([2020](#bib.bib36))); (iii) providing tools for task performance
    benchmarking that account for uncertainties (e.g. based on hypothesis testing
    as in (Gorman & Bedrick, [2019](#bib.bib31))).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑的UE工具包要么更全面（例如 UQ360），要么更模块化并且与DL框架具有互操作性（例如TFP，Pyro，UW）。 UE工具包理想情况下应该结合这两个方面。目前可用工具包技术全面性的进一步具体改进包括：（i）提供更多的事后方法，在重新构建模型或重新训练成本高昂的情景中尤为重要；（ii）将不确定性注入到特定于应用的网络组件中，例如注入到计算机视觉模型的非最大抑制或聚类过程中（如
    Meyer et al。（[2019](#bib.bib59)）; Harakeh et al.（[2020](#bib.bib36)））；（iii）提供考虑不确定性的任务性能基准测试工具（例如基于假设检验的，如（Gorman＆Bedrick，[2019](#bib.bib31)））。
- en: Interfacing with other toolkits
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与其他工具包的接口
- en: There is a multitude of relevant tasks and problems in deep learning besides
    uncertainty estimation. For example, different metrics and approaches are considered
    to assess and ensure different aspects of AI trustworthiness, such as robustness,
    interpretability or fairness. To address multiple such problems simultaneously
    employed toolkits should provide interoperable interfaces. This might be facilitated
    if, as we evaluated, the toolkits integrate seamlessly with the (same) underlying
    DL framework. Such interoperability between toolkits can even directly impact
    UE. For instance, data augmentation toolkits (e.g. augLy (Papakipos & Bitton,
    [2022](#bib.bib70))) provide input corruptions for out-of-distribution assessments
    and online-learning toolkits (e.g. river (Montiel et al., [2020](#bib.bib61)))
    can be used to adapt uncertainty models to future observations. A future prospect
    is the combination of several toolkits on trustworthy AI (e.g. AI Fairness 360
    (Bellamy et al., [2018](#bib.bib9)), Adversarial Robustness Toolbox (Nicolae et al.,
    [2018](#bib.bib66))) into a highly comprehensive testing framework.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，除了不确定性估计之外，还有大量相关任务和问题。例如，考虑到AI的可信度的不同方面，会采用不同的指标和方法来评估和确保AI的可信度，如鲁棒性、可解释性或公平性。为了同时解决多个这样的问题，工具包应提供可互操作的接口。如果工具包能与相同的DL框架（如我们评估的那样）无缝集成，这种互操作性可能会更加便利。工具包之间的互操作性甚至可以直接影响UE。例如，数据增强工具包（例如
    augLy（Papakipos＆Bitton， [2022](#bib.bib70)））提供了用于超出分布评估的输入损坏，而在线学习工具包（例如 river（Montiel等，[2020](#bib.bib61)））
    可用于将不确定性模型调整到未来观察中。未来的展望是将多个AI可信度工具包（例如 AI Fairness 360(Bellamy等，[2018](#bib.bib9))，对抗性鲁棒性工具包（Nicolae等，[2018](#bib.bib66)））组合成一个高度全面的测试框架。
- en: Guidance on correct tool usage
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正确使用工具的指导
- en: By providing usable high-level interfaces, toolkits reduce an entry barrier
    against employing (potentially highly complex) algorithms by a broad range of
    users. They should, additionally, support correct usage of the provided tools,
    e.g. by providing comprehensive guidelines. UQ360’s documentation provides guidance
    on choosing an uncertainty method and on communicating uncertainty estimates to
    stakeholders, which we see as a step in the right direction. Additional means
    that support a more informed and effective use of UE include the attribution of
    uncertainty to concrete sources (i.e. how much uncertainty arises from data, hyperparameter
    tuning, initializations, etc.), instructions on reducing uncertainties as well
    as contrasting evaluation scores against one another and describing their properties.
    Visual and interactive interfaces can aid correct tool use further. To generally
    improve documentation practices, standards for documenting toolkits and their
    tools could be developed, compare for instance “model cards” (Mitchell et al.,
    [2019](#bib.bib60)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供可用的高级接口，工具包降低了广泛用户使用（可能非常复杂）算法的门槛。此外，它们还应支持正确使用所提供的工具，例如，通过提供全面的指南。UQ360
    的文档提供了选择不确定性方法和向利益相关者传达不确定性估计的指导，我们认为这是朝着正确方向迈出的第一步。其他支持更明智和有效使用UE的手段包括将不确定性归因于具体来源（即数据、超参数调整、初始化等产生了多少不确定性）、减少不确定性的说明，以及对比评估分数并描述其属性。视觉和互动界面可以进一步帮助正确使用工具。为了普遍改善文档实践，可以制定工具包及其工具的文档标准，例如“模型卡”（Mitchell等，[2019](#bib.bib60)）。
- en: Maintaining code quality
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 维护代码质量
- en: Open-source seems to be a prerequisite for trustworthy implementations of evaluation
    standards and many of the reviewed toolkits principally place value on code quality
    as evident by supporting code contributions or employing automated testing tools.
    But, this is not always sufficient to ensure high quality and absence of critical
    bugs. Especially in the context of Trustworthy AI and safety-critical systems,
    which we see as a large application field for UE, this can become an issue. To
    further improve in that regard, it seems desirable to incentivize systematic code
    reviews, e.g. by introducing bug bounty programs for major toolkits and generally
    calling greater attention towards this topic (e.g. via dedicated workshops on
    code quality).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 开源似乎是可靠评估标准实现的一个前提条件，许多被审查的工具包主要重视代码质量，这通过支持代码贡献或采用自动化测试工具可以看出。但这并不足以确保高质量和避免关键错误。特别是在可信AI和安全关键系统的背景下，我们认为这是UE的一个重要应用领域，这可能会成为一个问题。为了进一步改善这一点，似乎有必要激励系统化的代码审查，例如通过为主要工具包引入漏洞赏金计划，并通常提高对该话题的关注（例如，通过专门的代码质量研讨会）。
- en: Acknowledgments
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: The development of this publication was supported by the Ministry of Economic
    Affairs, Innovation, Digitalization and Energy of the State of North Rhine-Westphalia
    as part of the flagship project ZERTIFIZIERTE KI.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本出版物的开发得到了北莱茵-威斯特法伦州经济事务、创新、数字化和能源部的支持，作为旗舰项目 ZERTIFIZIERTE KI 的一部分。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abadi et al. (2015) Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
    Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
    Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
    Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale
    machine learning on heterogeneous systems, 2015. URL [https://www.tensorflow.org/](https://www.tensorflow.org/).
    Software available from tensorflow.org.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi等（2015） Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng
    Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
    Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
    Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: 大规模异构系统上的机器学习,
    2015. 网址 [https://www.tensorflow.org/](https://www.tensorflow.org/)。软件可从 tensorflow.org
    获得。'
- en: 'Agarwal & Das (2020) Namita Agarwal and Saikat Das. Interpretable machine learning
    tools: A survey. In *2020 IEEE Symposium Series on Computational Intelligence
    (SSCI)*, pp.  1528–1534\. IEEE, 2020.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal & Das (2020) Namita Agarwal 和 Saikat Das. 可解释的机器学习工具：综述。见 *2020 IEEE
    计算智能研讨会（SSCI）*，第1528–1534页。IEEE，2020年。
- en: 'Alaa & Van Der Schaar (2020) Ahmed Alaa and Mihaela Van Der Schaar. Discriminative
    jackknife: Quantifying uncertainty in deep learning via higher-order influence
    functions. In Hal Daumé III and Aarti Singh (eds.), *Proceedings of the 37th International
    Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning
    Research*, pp.  165–174\. PMLR, 13–18 Jul 2020. URL [https://proceedings.mlr.press/v119/alaa20a.html](https://proceedings.mlr.press/v119/alaa20a.html).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alaa & Van Der Schaar (2020) Ahmed Alaa 和 Mihaela Van Der Schaar. 判别性切除法：通过高阶影响函数量化深度学习中的不确定性。见
    Hal Daumé III 和 Aarti Singh (编)，*第37届国际机器学习大会论文集*，第119卷，*机器学习研究论文集*，第165–174页。PMLR，2020年7月13–18日。网址
    [https://proceedings.mlr.press/v119/alaa20a.html](https://proceedings.mlr.press/v119/alaa20a.html)。
- en: 'Alam et al. (2020) M. Alam, M.D. Samad, L. Vidyaratne, A. Glandon, and K.M.
    Iftekharuddin. Survey on deep neural networks in speech and vision systems. *Neurocomputing*,
    417:302–321, 2020. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2020.07.053.
    URL [https://www.sciencedirect.com/science/article/pii/S0925231220311619](https://www.sciencedirect.com/science/article/pii/S0925231220311619).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alam et al. (2020) M. Alam, M.D. Samad, L. Vidyaratne, A. Glandon 和 K.M. Iftekharuddin.
    语音和视觉系统中的深度神经网络综述。*神经计算*，417:302–321，2020年。ISSN 0925-2312。doi: https://doi.org/10.1016/j.neucom.2020.07.053。网址
    [https://www.sciencedirect.com/science/article/pii/S0925231220311619](https://www.sciencedirect.com/science/article/pii/S0925231220311619)。'
- en: 'Alexandrov et al. (2020) Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C.
    Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner
    Türkmen, and Yuyang Wang. GluonTS: Probabilistic and Neural Time Series Modeling
    in Python. *Journal of Machine Learning Research*, 21(116):1–6, 2020. URL [http://jmlr.org/papers/v21/19-820.html](http://jmlr.org/papers/v21/19-820.html).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alexandrov et al. (2020) Alexander Alexandrov, Konstantinos Benidis, Michael
    Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle
    C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali
    Caner Türkmen 和 Yuyang Wang. GluonTS：Python中的概率和神经时间序列建模。*机器学习研究杂志*，21(116):1–6，2020年。网址
    [http://jmlr.org/papers/v21/19-820.html](http://jmlr.org/papers/v21/19-820.html)。
- en: Amini et al. (2020) Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela
    Rus. Deep evidential regression. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
    Balcan, and H. Lin (eds.), *Advances in neural information processing systems*,
    volume 33, pp.  14927–14937\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini et al. (2020) Alexander Amini, Wilko Schwarting, Ava Soleimany 和 Daniela
    Rus. 深度证据回归。见 H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan 和 H. Lin (编)，*神经信息处理系统进展*，第33卷，第14927–14937页。Curran
    Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf)。
- en: 'Arya et al. (2020) Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar,
    Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra
    Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna
    Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei,
    and Yunfeng Zhang. Ai explainability 360: Hands-on tutorial. In *Proceedings of
    the 2020 Conference on Fairness, Accountability, and Transparency*, FAT* ’20,
    pp.  696, New York, NY, USA, 2020\. Association for Computing Machinery. ISBN
    9781450369367. doi: 10.1145/3351095.3375667. URL [https://doi.org/10.1145/3351095.3375667](https://doi.org/10.1145/3351095.3375667).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arya et al. (2020) Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar,
    Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra
    Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna
    Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei
    和 Yunfeng Zhang. Ai explainability 360: 实用教程。见 *2020年公平性、问责制与透明度会议论文集*，FAT* ’20，第696页，美国纽约，2020年。计算机协会。ISBN
    9781450369367。doi: 10.1145/3351095.3375667。网址 [https://doi.org/10.1145/3351095.3375667](https://doi.org/10.1145/3351095.3375667)。'
- en: 'Bahrampour et al. (2015) Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott,
    and Mohak Shah. Comparative study of deep learning software frameworks. *arXiv:
    Learning*, 2015.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bahrampour et al. (2015) Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott
    和 Mohak Shah. 深度学习软件框架的比较研究。*arXiv: Learning*，2015年。'
- en: 'Bellamy et al. (2018) Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C.
    Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Kr. Lohia, Jacquelyn Martino,
    Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy,
    John T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R.
    Varshney, and Yunfeng Zhang. Ai fairness 360: An extensible toolkit for detecting,
    understanding, and mitigating unwanted algorithmic bias. *ArXiv*, abs/1810.01943,
    2018.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellamy 等（2018）Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman,
    Stephanie Houde, Kalapriya Kannan, Pranay Kr. Lohia, Jacquelyn Martino, Sameep
    Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John
    T. Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney,
    和 Yunfeng Zhang. AI 公平性 360：检测、理解和减轻不希望的算法偏见的可扩展工具包。*ArXiv*, abs/1810.01943, 2018。
- en: 'Beluch et al. (2018) William H. Beluch, Tim Genewein, Andreas Nurnberger, and
    Jan M. Kohler. The power of ensembles for active learning in image classification.
    In *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 
    9368–9377, 2018. doi: 10.1109/CVPR.2018.00976.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beluch 等（2018）William H. Beluch, Tim Genewein, Andreas Nurnberger, 和 Jan M.
    Kohler. 集成方法在图像分类中的主动学习能力。见 *2018 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pp. 9368–9377, 2018. doi: 10.1109/CVPR.2018.00976。'
- en: 'Bingham et al. (2019) Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz
    Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip,
    Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic programming.
    *J. Mach. Learn. Res.*, 20:28:1–28:6, 2019. URL [http://jmlr.org/papers/v20/18-403.html](http://jmlr.org/papers/v20/18-403.html).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bingham 等（2019）Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer,
    Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall,
    和 Noah D. Goodman. Pyro: 深度通用概率编程。*J. Mach. Learn. Res.*, 20:28:1–28:6, 2019.
    URL [http://jmlr.org/papers/v20/18-403.html](http://jmlr.org/papers/v20/18-403.html)。'
- en: Blundell et al. (2015) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu,
    and Daan Wierstra. Weight uncertainty in neural networks. In *Proceedings of the
    32nd International Conference on International Conference on Machine Learning
    - Volume 37*, ICML’15, pp. 1613–1622\. JMLR.org, 2015.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell 等（2015）Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, 和 Daan
    Wierstra. 神经网络中的权重不确定性。见 *Proceedings of the 32nd International Conference on
    International Conference on Machine Learning - Volume 37*, ICML’15, pp. 1613–1622.
    JMLR.org, 2015。
- en: 'Brundage et al. (2020) Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield,
    Gretchen Krueger, Gillian K. Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner,
    Ruth Fong, Tegan Maharaj, Pang Wei Koh, Sara Hooker, Jade Leung, Andrew Trask,
    Emma Bluemke, Jonathan Lebensbold, Cullen O’Keefe, Mark Koren, Theo Ryffel, J. B.
    Rubinovitz, Tamay Besiroglu, Federica Carugati, Jack Clark, Peter Eckersley, Sarah
    de Haas, Maritza L. Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk, Amanda
    Askell, Rosario Cammarota, Andrew J. Lohn, David Krueger, Charlotte Stix, Peter
    Henderson, Logan Graham, Carina E. A. Prunkl, Bianca Martin, Elizabeth Seger,
    Noa Zilberman, Se’an ’O h’Eigeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan,
    Adrian Weller, Brian Tse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-Voss,
    Martijn Rasser, Shagun Sodhani, Carrick Flynn, Thomas Krendl Gilbert, Lisa Dyer,
    Saif Khan, Yoshua Bengio, and Markus Anderljung. Toward trustworthy AI development:
    Mechanisms for supporting verifiable claims. *ArXiv*, abs/2004.07213, 2020.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brundage 等（2020）Miles Brundage, Shahar Avin, Jasmine Wang, Haydn Belfield, Gretchen
    Krueger, Gillian K. Hadfield, Heidy Khlaaf, Jingying Yang, Helen Toner, Ruth Fong,
    Tegan Maharaj, Pang Wei Koh, Sara Hooker, Jade Leung, Andrew Trask, Emma Bluemke,
    Jonathan Lebensbold, Cullen O’Keefe, Mark Koren, Theo Ryffel, J. B. Rubinovitz,
    Tamay Besiroglu, Federica Carugati, Jack Clark, Peter Eckersley, Sarah de Haas,
    Maritza L. Johnson, Ben Laurie, Alex Ingerman, Igor Krawczuk, Amanda Askell, Rosario
    Cammarota, Andrew J. Lohn, David Krueger, Charlotte Stix, Peter Henderson, Logan
    Graham, Carina E. A. Prunkl, Bianca Martin, Elizabeth Seger, Noa Zilberman, Se’an
    ’O h’Eigeartaigh, Frens Kroeger, Girish Sastry, Rebecca Kagan, Adrian Weller,
    Brian Tse, Elizabeth Barnes, Allan Dafoe, Paul Scharre, Ariel Herbert-Voss, Martijn
    Rasser, Shagun Sodhani, Carrick Flynn, Thomas Krendl Gilbert, Lisa Dyer, Saif
    Khan, Yoshua Bengio, 和 Markus Anderljung. 朝向值得信赖的 AI 发展：支持可验证声明的机制。*ArXiv*, abs/2004.07213,
    2020。
- en: 'Chai et al. (2021) Junyi Chai, Hao Zeng, Anming Li, and Eric W.T. Ngai. Deep
    learning in computer vision: A critical review of emerging techniques and application
    scenarios. *Machine Learning with Applications*, 6:100134, 2021. ISSN 2666-8270.
    doi: https://doi.org/10.1016/j.mlwa.2021.100134. URL [https://www.sciencedirect.com/science/article/pii/S2666827021000670](https://www.sciencedirect.com/science/article/pii/S2666827021000670).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chai 等（2021）Junyi Chai, Hao Zeng, Anming Li, 和 Eric W.T. Ngai. 计算机视觉中的深度学习：新兴技术和应用场景的关键综述。*Machine
    Learning with Applications*, 6:100134, 2021. ISSN 2666-8270. doi: https://doi.org/10.1016/j.mlwa.2021.100134.
    URL [https://www.sciencedirect.com/science/article/pii/S2666827021000670](https://www.sciencedirect.com/science/article/pii/S2666827021000670)。'
- en: Chatila et al. (2021) Raja Chatila, Virginia Dignum, Michael Fisher, Fosca Giannotti,
    Katharina Morik, Stuart Russell, and Karen Yeung. Trustworthy ai. In *Reflections
    on Artificial Intelligence for Humanity*, pp. 13–39\. Springer, 2021.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chatila等（2021）Raja Chatila、Virginia Dignum、Michael Fisher、Fosca Giannotti、Katharina
    Morik、Stuart Russell 和 Karen Yeung。在*人工智能对人类的反思*中，值得信赖的人工智能，页码13–39。Springer，2021年。
- en: 'Chen et al. (2015) Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie
    Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible
    and efficient machine learning library for heterogeneous distributed systems.
    *CoRR*, abs/1512.01274, 2015. URL [http://dblp.uni-trier.de/db/journals/corr/corr1512.html#ChenLLLWWXXZZ15](http://dblp.uni-trier.de/db/journals/corr/corr1512.html#ChenLLLWWXXZZ15).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2015）Tianqi Chen、Mu Li、Yutian Li、Min Lin、Naiyan Wang、Minjie Wang、Tianjun
    Xiao、Bing Xu、Chiyuan Zhang 和 Zheng Zhang。在*CoRR*中，Mxnet：一种灵活高效的异构分布式系统机器学习库，abs/1512.01274，2015年。URL
    [http://dblp.uni-trier.de/db/journals/corr/corr1512.html#ChenLLLWWXXZZ15](http://dblp.uni-trier.de/db/journals/corr/corr1512.html#ChenLLLWWXXZZ15)。
- en: Chen et al. (2019) Tongfei Chen, Jirí Navrátil, Vijay Iyengar, and Karthikeyan
    Shanmugam. Confidence scoring using whitebox meta-models with linear classifier
    probes. In *The 22nd International Conference on Artificial Intelligence and Statistics*,
    pp.  1467–1475\. PMLR, 2019.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2019）陈同飞、Jirí Navrátil、Vijay Iyengar 和 Karthikeyan Shanmugam。在*第22届国际人工智能与统计会议*上，使用白盒元模型和线性分类器探针进行置信度评分，页码1467–1475。PMLR，2019年。
- en: 'Chung et al. (2021) Youngseog Chung, Ian Char, Han Guo, Jeff Schneider, and
    Willie Neiswanger. Uncertainty toolbox: an open-source library for assessing,
    visualizing, and improving uncertainty quantification. *arXiv preprint arXiv:2109.10254*,
    2021.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等（2021）Youngseog Chung、Ian Char、Han Guo、Jeff Schneider 和 Willie Neiswanger。在*arXiv预印本
    arXiv:2109.10254*中，不确定性工具箱：一个用于评估、可视化和改进不确定性量化的开源库，2021年。
- en: Dillon et al. (2017) Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo,
    Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matthew D. Hoffman,
    and Rif A. Saurous. Tensorflow distributions. *CoRR*, abs/1711.10604, 2017. URL
    [http://arxiv.org/abs/1711.10604](http://arxiv.org/abs/1711.10604).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dillon等（2017）Joshua V. Dillon、Ian Langmore、Dustin Tran、Eugene Brevdo、Srinivas
    Vasudevan、Dave Moore、Brian Patton、Alex Alemi、Matthew D. Hoffman 和 Rif A. Saurous。在*CoRR*中，Tensorflow分布，abs/1711.10604，2017年。URL
    [http://arxiv.org/abs/1711.10604](http://arxiv.org/abs/1711.10604)。
- en: 'Druzhkov & Kustikova (2016) Paul Druzhkov and Valentina Kustikova. A survey
    of deep learning methods and software tools for image classification and object
    detection. *Pattern Recognition and Image Analysis*, 26:9–15, 01 2016. doi: 10.1134/S1054661816010065.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Druzhkov & Kustikova（2016）Paul Druzhkov 和 Valentina Kustikova。在*模式识别与图像分析*中，深度学习方法和软件工具的综述，26:9–15，2016年01月。doi:
    10.1134/S1054661816010065。'
- en: Durasov et al. (2021) N. Durasov, T. Bagautdinov, P. Baque, and P. Fua. Masksembles
    for Uncertainty Estimation. In *CVPR*, 2021.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durasov等（2021）N. Durasov、T. Bagautdinov、P. Baque 和 P. Fua。在*CVPR*上，不确定性估计的Masksembles，2021年。
- en: 'Fan et al. (2021) Xinjie Fan, Shujian Zhang, Korawat Tanwisuth, Xiaoning Qian,
    and Mingyuan Zhou. Contextual dropout: An efficient sample-dependent dropout module.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net, 2021. URL [https://openreview.net/forum?id=ct8_a9h1M](https://openreview.net/forum?id=ct8_a9h1M).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等（2021）Xinjie Fan、Shujian Zhang、Korawat Tanwisuth、Xiaoning Qian 和 Mingyuan
    Zhou。在*第9届国际学习表征会议，ICLR 2021，虚拟会议，奥地利，2021年5月3-7日*中，情境丢弃：一种高效的样本依赖丢弃模块。OpenReview.net，2021年。URL
    [https://openreview.net/forum?id=ct8_a9h1M](https://openreview.net/forum?id=ct8_a9h1M)。
- en: Franc & Prusa (2019) Vojtech Franc and Daniel Prusa. On discriminative learning
    of prediction uncertainty. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
    *Proceedings of the 36th International Conference on Machine Learning*, volume 97
    of *Proceedings of Machine Learning Research*, pp. 1963–1971\. PMLR, 09–15 Jun
    2019. URL [https://proceedings.mlr.press/v97/franc19a.html](https://proceedings.mlr.press/v97/franc19a.html).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Franc & Prusa（2019）Vojtech Franc 和 Daniel Prusa。在Kamalika Chaudhuri 和 Ruslan
    Salakhutdinov（编辑），*第36届国际机器学习大会论文集*中，关于预测不确定性的辨别学习，*机器学习研究论文集*第97卷，页码1963–1971。PMLR，2019年6月09–15日。URL
    [https://proceedings.mlr.press/v97/franc19a.html](https://proceedings.mlr.press/v97/franc19a.html)。
- en: 'Gal & Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian
    approximation: Representing model uncertainty in deep learning. In Maria-Florina
    Balcan and Kilian Q. Weinberger (eds.), *Proceedings of the 33nd International
    Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24,
    2016*, volume 48 of *JMLR Workshop and Conference Proceedings*, pp.  1050–1059.
    JMLR.org, 2016. URL [http://proceedings.mlr.press/v48/gal16.html](http://proceedings.mlr.press/v48/gal16.html).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal & Ghahramani (2016) Yarin Gal 和 Zoubin Ghahramani。Dropout作为贝叶斯近似：在深度学习中表示模型不确定性。见于
    Maria-Florina Balcan 和 Kilian Q. Weinberger (编辑)，*第33届国际机器学习大会论文集, ICML 2016,
    纽约市, NY, USA, 2016年6月19-24日*，*JMLR工作坊和会议论文集*第48卷，第1050–1059页。JMLR.org，2016年。网址
    [http://proceedings.mlr.press/v48/gal16.html](http://proceedings.mlr.press/v48/gal16.html)。
- en: Gal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian
    active learning with image data. In *Proceedings of the 34th International Conference
    on Machine Learning - Volume 70*, ICML’17, pp.  1183–1192\. JMLR.org, 2017.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal et al. (2017) Yarin Gal, Riashat Islam 和 Zoubin Ghahramani。带有图像数据的深度贝叶斯主动学习。见于
    *第34届国际机器学习大会论文集 - 第70卷*，ICML’17，第1183–1192页。JMLR.org，2017年。
- en: 'Gast & Roth (2018) Jochen Gast and Stefan Roth. Lightweight probabilistic deep
    networks. In *2018 IEEE Conference on Computer Vision and Pattern Recognition,
    CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*, pp.  3369–3378\. IEEE Computer
    Society, 2018. doi: 10/gfx44n.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gast & Roth (2018) Jochen Gast 和 Stefan Roth。轻量级概率深度网络。见于 *2018 IEEE计算机视觉与模式识别会议，CVPR
    2018, 盐湖城, UT, USA, 2018年6月18-22日*，第3369–3378页。IEEE计算机学会，2018年。doi: 10/gfx44n。'
- en: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural
    networks. *arXiv preprint arXiv:2107.03342*, 2021.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gawlikowski et al. (2021) Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi,
    Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph
    Triebel, Peter Jung, Ribana Roscher 等人。深度神经网络中的不确定性调查。*arXiv 预印本 arXiv:2107.03342*，2021年。
- en: 'Ghosh et al. (2021) Soumya Ghosh, Q. Vera Liao, Karthikeyan Natesan Ramamurthy,
    Jiri Navratil, Prasanna Sattigeri, Kush R. Varshney, and Yunfeng Zhang. Uncertainty
    quantification 360: A holistic toolkit for quantifying and communicating the uncertainty
    of ai, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh et al. (2021) Soumya Ghosh, Q. Vera Liao, Karthikeyan Natesan Ramamurthy,
    Jiri Navratil, Prasanna Sattigeri, Kush R. Varshney 和 Yunfeng Zhang。 不确定性量化360：量化和传达人工智能不确定性的全方位工具包，2021年。
- en: Giordano et al. (2019) Ryan Giordano, William T. Stephenson, Runjing Liu, Michael I.
    Jordan, and Tamara Broderick. A Swiss army infinitesimal jackknife. In *AISTATS*,
    2019.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giordano et al. (2019) Ryan Giordano, William T. Stephenson, Runjing Liu, Michael
    I. Jordan 和 Tamara Broderick。瑞士军刀微小化。见于 *AISTATS*，2019年。
- en: Gneiting & Raftery (2007) Tilmann Gneiting and Adrian E Raftery. Strictly proper
    scoring rules, prediction, and estimation. *Journal of the American statistical
    Association*, 102(477):359–378, 2007.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gneiting & Raftery (2007) Tilmann Gneiting 和 Adrian E Raftery。严格的适当评分规则、预测和估计。*美国统计协会杂志*，102(477):359–378，2007年。
- en: 'Gorman & Bedrick (2019) Kyle Gorman and Steven Bedrick. We need to talk about
    standard splits. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pp.  2786–2791, Florence, Italy, July 2019. Association
    for Computational Linguistics. doi: 10.18653/v1/P19-1267. URL [https://aclanthology.org/P19-1267](https://aclanthology.org/P19-1267).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gorman & Bedrick (2019) Kyle Gorman 和 Steven Bedrick。我们需要讨论标准拆分。见于 *第57届计算语言学协会年会论文集*，第2786–2791页，意大利佛罗伦萨，2019年7月。计算语言学协会。doi:
    10.18653/v1/P19-1267。网址 [https://aclanthology.org/P19-1267](https://aclanthology.org/P19-1267)。'
- en: Graves (2011) Alex Graves. Practical variational inference for neural networks.
    In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger (eds.),
    *Advances in neural information processing systems*, volume 24\. Curran Associates,
    Inc., 2011. URL [https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves (2011) Alex Graves。神经网络的实用变分推断。见于 J. Shawe-Taylor, R. Zemel, P. Bartlett,
    F. Pereira 和 K. Q. Weinberger (编辑)，*神经信息处理系统进展*，第24卷。Curran Associates, Inc.，2011年。网址
    [https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf](https://proceedings.neurips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)。
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    On calibration of modern neural networks. In Doina Precup and Yee Whye Teh (eds.),
    *Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
    Sydney, NSW, Australia, 6-11 August 2017*, volume 70 of *Proceedings of Machine
    Learning Research*, pp.  1321–1330\. PMLR, 2017. URL [http://proceedings.mlr.press/v70/guo17a.html](http://proceedings.mlr.press/v70/guo17a.html).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2017) 全·郭、杰夫·普利斯、余·孙和基利安·Q·温伯格。现代神经网络的校准。在多伊娜·普雷库普和易·惠·特（编辑），*第34届国际机器学习大会论文集，ICML
    2017，澳大利亚悉尼，2017年8月6-11日*，第70卷*机器学习研究论文集*，第1321–1330页。PMLR，2017年。URL [http://proceedings.mlr.press/v70/guo17a.html](http://proceedings.mlr.press/v70/guo17a.html)。
- en: 'Guo et al. (2020) Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin,
    Xingjian Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang,
    Zhi Zhang, Zhongyue Zhang, Shuai Zheng, and Yi Zhu. Gluoncv and gluonnlp: Deep
    learning in computer vision and natural language processing. *Journal of Machine
    Learning Research*, 21(23):1–7, 2020. URL [http://jmlr.org/papers/v21/19-429.html](http://jmlr.org/papers/v21/19-429.html).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2020) 简·郭、何赫、童赫、伦纳德·劳森、穆·李、海滨·林、邢建·石、程光·王、俊远·谢、盛扎、阿斯顿·张、杭张、智张、钟跃·张、帅郑和易朱。Gluoncv
    和 gluonnlp：深度学习在计算机视觉和自然语言处理中的应用。*机器学习研究杂志*，21(23):1–7，2020。URL [http://jmlr.org/papers/v21/19-429.html](http://jmlr.org/papers/v21/19-429.html)。
- en: Hafiz & Bhat (2020) Abdul Mueed Hafiz and Ghulam Mohiuddin Bhat. *A survey of
    deep learning techniques for medical diagnosis*, pp.  161–170. Springer, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafiz & Bhat (2020) 阿卜杜勒·穆伊德·哈菲兹和古拉姆·穆希丁·巴特。*医学诊断中的深度学习技术综述*，第161–170页。Springer，2020年。
- en: 'Harakeh et al. (2020) Ali Harakeh, Michael Smart, and Steven L Waslander. BayesOD:
    A Bayesian approach for uncertainty estimation in deep object detectors. In *2020
    IEEE International Conference on Robotics and Automation (ICRA)*, pp.  87–93\.
    IEEE, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harakeh et al. (2020) 阿里·哈拉克、迈克尔·斯马特和史蒂文·L·瓦斯兰德。BayesOD：一种用于深度目标检测器不确定性估计的贝叶斯方法。在*2020
    IEEE国际机器人与自动化会议（ICRA）*，第87–93页。IEEE，2020年。
- en: Hastings (1970) W. K. Hastings. Monte carlo sampling methods using markov chains
    and their applications. *Biometrika*, 57(1):97–109, 1970. ISSN 00063444. URL [http://www.jstor.org/stable/2334940](http://www.jstor.org/stable/2334940).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastings (1970) W·K·海斯廷斯。使用马尔可夫链的蒙特卡洛采样方法及其应用。*生物计量学*，57(1):97–109，1970年。ISSN
    00063444。URL [http://www.jstor.org/stable/2334940](http://www.jstor.org/stable/2334940)。
- en: Hensman et al. (2013) James Hensman, Nicolò Fusi, and Neil D. Lawrence. Gaussian
    processes for big data. In *Proceedings of the Twenty-Ninth Conference on Uncertainty
    in Artificial Intelligence*, UAI’13, pp.  282–290, Arlington, Virginia, USA, 2013\.
    AUAI Press.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hensman et al. (2013) 詹姆斯·亨斯曼、尼科洛·富西和尼尔·D·劳伦斯。大数据的高斯过程。在*第二十九届人工智能不确定性会议论文集*，UAI’13，第282–290页，美国弗吉尼亚州阿灵顿，2013年。AUAI出版社。
- en: Hernández-Lobato & Adams (2015) José Miguel Hernández-Lobato and Ryan P. Adams.
    Probabilistic backpropagation for scalable learning of Bayesian neural networks.
    In *Proceedings of the 32nd International Conference on International Conference
    on Machine Learning - Volume 37*, ICML’15, pp. 1861–1869\. JMLR.org, 2015.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernández-Lobato & Adams (2015) 何塞·米格尔·埃尔南德斯-洛巴托和瑞安·P·亚当斯。可扩展学习贝叶斯神经网络的概率反向传播。在*第32届国际机器学习大会论文集
    - 第37卷*，ICML’15，第1861–1869页。JMLR.org，2015年。
- en: Hochreiter & Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural computation*, 9(8):1735–1780, 1997.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter & Schmidhuber (1997) 塞普·霍赫赖特和尤尔根·施密德胡伯。长短期记忆。*神经计算*，9(8):1735–1780，1997年。
- en: 'Honnibal et al. (2020) Matthew Honnibal, Ines Montani, Sofie Van Landeghem,
    and Adriane Boyd. spacy: Industrial-strength natural language processing in python,
    2020.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Honnibal et al. (2020) 马修·霍尼巴尔、伊内斯·蒙塔尼、索菲·范·兰德赫姆和阿德里安·博伊德。spacy：工业级自然语言处理工具，2020年。
- en: 'Houben et al. (2021) Sebastian Houben, Stephanie Abrecht, Maram Akila, Andreas
    Bär, Felix Brockherde, Patrick Feifel, Tim Fingscheidt, Sujan Sai Gannamaneni,
    Seyed Eghbal Ghobadi, Ahmed Hammam, Anselm Haselhoff, Felix Hauser, Christian
    Heinzemann, Marco Hoffmann, Nikhil Kapoor, Falk Kappel, Marvin Klingner, Jan Kronenberger,
    Fabian Küppers, Jonas Löhdefink, Michael Mlynarski, Michael Mock, Firas Mualla,
    Svetlana Pavlitskaya, Maximilian Poretschkin, Alexander Pohl, Varun Ravi Kumar,
    Julia Rosenzweig, Matthias Rottmann, Stefan Rüping, Timo Sämann, Jan David Schneider,
    Elena Schulz, Gesina Schwalbe, Joachim Sicking, Toshika Srivastava, Serin Varghese,
    Michael Weber, Sebastian Wirkert, Tim Wirtz, and Matthias Woehrle. Inspect, understand,
    overcome: A survey of practical methods for AI safety. *CoRR*, abs/2104.14235,
    2021. URL [https://arxiv.org/abs/2104.14235](https://arxiv.org/abs/2104.14235).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houben et al. (2021) Sebastian Houben、Stephanie Abrecht、Maram Akila、Andreas
    Bär、Felix Brockherde、Patrick Feifel、Tim Fingscheidt、Sujan Sai Gannamaneni、Seyed
    Eghbal Ghobadi、Ahmed Hammam、Anselm Haselhoff、Felix Hauser、Christian Heinzemann、Marco
    Hoffmann、Nikhil Kapoor、Falk Kappel、Marvin Klingner、Jan Kronenberger、Fabian Küppers、Jonas
    Löhdefink、Michael Mlynarski、Michael Mock、Firas Mualla、Svetlana Pavlitskaya、Maximilian
    Poretschkin、Alexander Pohl、Varun Ravi Kumar、Julia Rosenzweig、Matthias Rottmann、Stefan
    Rüping、Timo Sämann、Jan David Schneider、Elena Schulz、Gesina Schwalbe、Joachim Sicking、Toshika
    Srivastava、Serin Varghese、Michael Weber、Sebastian Wirkert、Tim Wirtz 和 Matthias
    Woehrle。检查、理解、克服：AI安全的实用方法综述。*CoRR*，abs/2104.14235，2021年。网址 [https://arxiv.org/abs/2104.14235](https://arxiv.org/abs/2104.14235)。
- en: 'Hüllermeier & Waegeman (2021) Eyke Hüllermeier and Willem Waegeman. Aleatoric
    and epistemic uncertainty in machine learning: An introduction to concepts and
    methods. *Machine Learning*, 110(3):457–506, 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hüllermeier & Waegeman (2021) Eyke Hüllermeier 和 Willem Waegeman。机器学习中的偶然性和认知不确定性：概念和方法简介。*机器学习*，110(3):457–506，2021。
- en: 'Kendall & Gal (2017) Alex Kendall and Yarin Gal. What uncertainties do we need
    in Bayesian deep learning for computer vision? In Isabelle Guyon, Ulrike von Luxburg,
    Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
    (eds.), *Advances in Neural Information Processing Systems 30: Annual Conference
    on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
    CA, USA*, pp.  5574–5584, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kendall & Gal (2017) Alex Kendall 和 Yarin Gal。计算机视觉中的贝叶斯深度学习需要哪些不确定性？在 Isabelle
    Guyon、Ulrike von Luxburg、Samy Bengio、Hanna M. Wallach、Rob Fergus、S. V. N. Vishwanathan
    和 Roman Garnett (编者) 主编的 *神经信息处理系统进展 30：2017年神经信息处理系统年会，2017年12月4-9日，加利福尼亚州长滩，USA*
    中，第5574–5584页，2017年。网址 [https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html)。
- en: Kim et al. (2020) Byol Kim, Chen Xu, and Rina Barber. Predictive inference is
    free with the jackknife+-after-bootstrap. *Advances in Neural Information Processing
    Systems*, 33:4138–4149, 2020.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2020) Byol Kim、Chen Xu 和 Rina Barber。预测推断在 jackknife+-after-bootstrap
    中是免费的。*神经信息处理系统进展*，33:4138–4149，2020。
- en: 'Kingma et al. (2015) Diederik P. Kingma, Tim Salimans, and Max Welling. Variational
    dropout and the local reparameterization trick. In *Proceedings of the 28th international
    conference on neural information processing systems - volume 2*, NIPS’15, pp. 
    2575–2583. MIT Press, 2015. Number of pages: 9 Place: Montreal, Canada.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma et al. (2015) Diederik P. Kingma、Tim Salimans 和 Max Welling。变分丢弃法和局部重参数化技巧。收录于
    *第28届国际神经信息处理系统会议 - 第2卷*，NIPS’15，第2575–2583页。MIT出版社，2015年。页数：9 地点：加拿大蒙特利尔。
- en: 'Krishnan et al. (2022) Ranganath Krishnan, Pi Esposito, and Mahesh Subedar.
    Bayesian-Torch: Bayesian neural network layers for uncertainty estimation. [https://github.com/IntelLabs/bayesian-torch](https://github.com/IntelLabs/bayesian-torch),
    January 2022. URL [https://doi.org/10.5281/zenodo.5908307](https://doi.org/10.5281/zenodo.5908307).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnan et al. (2022) Ranganath Krishnan、Pi Esposito 和 Mahesh Subedar。Bayesian-Torch：用于不确定性估计的贝叶斯神经网络层。
    [https://github.com/IntelLabs/bayesian-torch](https://github.com/IntelLabs/bayesian-torch)，2022年1月。网址
    [https://doi.org/10.5281/zenodo.5908307](https://doi.org/10.5281/zenodo.5908307)。
- en: 'Kuleshov et al. (2018) Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon.
    Accurate uncertainties for deep learning using calibrated regression. In Jennifer
    Dy and Andreas Krause (eds.), *Proceedings of the 35th international conference
    on machine learning*, volume 80 of *Proceedings of machine learning research*,
    pp.  2796–2804\. PMLR, 2018. URL [https://proceedings.mlr.press/v80/kuleshov18a.html](https://proceedings.mlr.press/v80/kuleshov18a.html).
    tex.pdf: http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kuleshov et al. (2018) Volodymyr Kuleshov, Nathan Fenner, 和 Stefano Ermon.
    使用校准回归的深度学习准确不确定性。收录于 Jennifer Dy 和 Andreas Krause (编), *第 35 届国际机器学习大会论文集*, *机器学习研究论文集*第
    80 卷, 第 2796–2804 页。PMLR, 2018。网址 [https://proceedings.mlr.press/v80/kuleshov18a.html](https://proceedings.mlr.press/v80/kuleshov18a.html).
    tex.pdf: http://proceedings.mlr.press/v80/kuleshov18a/kuleshov18a.pdf。'
- en: 'Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel,
    and Charles Blundell. Simple and scalable predictive uncertainty estimation using
    deep ensembles. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), *Advances in Neural
    Information Processing Systems 30: Annual Conference on Neural Information Processing
    Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp.  6402–6413, 2017.
    URL [https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakshminarayanan et al. (2017) Balaji Lakshminarayanan, Alexander Pritzel, 和
    Charles Blundell. 使用深度集成进行简单且可扩展的预测不确定性估计。收录于 Isabelle Guyon, Ulrike von Luxburg,
    Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, 和 Roman Garnett
    (编), *第 30 届神经信息处理系统年会：2017年12月4-9日，美国加利福尼亚州长滩*, 第 6402–6413 页, 2017。网址 [https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html)。
- en: 'Landset et al. (2015) Sara Landset, Taghi Khoshgoftaar, Aaron Richter, and
    Tawfiq Hasanin. A survey of open source tools for machine learning with big data
    in the hadoop ecosystem. *Journal of Big Data*, 2, 11 2015. doi: 10.1186/s40537-015-0032-1.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Landset et al. (2015) Sara Landset, Taghi Khoshgoftaar, Aaron Richter, 和 Tawfiq
    Hasanin. 关于 Hadoop 生态系统中大数据机器学习的开源工具调查。*大数据杂志*, 2, 11 2015。doi: 10.1186/s40537-015-0032-1。'
- en: Li et al. (2016) Chunyuan Li, Changyou Chen, David E. Carlson, and Lawrence
    Carin. Preconditioned stochastic gradient langevin dynamics for deep neural networks.
    In Dale Schuurmans and Michael P. Wellman (eds.), *Proceedings of the Thirtieth
    AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona,
    USA*, pp.  1788–1794\. AAAI Press, 2016. URL [http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11835](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11835).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2016) Chunyuan Li, Changyou Chen, David E. Carlson, 和 Lawrence Carin.
    用于深度神经网络的预处理随机梯度朗之万动力学。收录于 Dale Schuurmans 和 Michael P. Wellman (编), *第三十届 AAAI
    人工智能会议论文集, 2016年2月12-17日, 美国亚利桑那州凤凰城*, 第 1788–1794 页。AAAI Press, 2016。网址 [http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11835](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11835)。
- en: 'Liu et al. (2021) Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li,
    Shaili Jain, Yunhao Liu, Anil K Jain, and Jiliang Tang. Trustworthy AI: A computational
    perspective. *arXiv preprint arXiv:2107.06641*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Yaxin Li,
    Shaili Jain, Yunhao Liu, Anil K Jain, 和 Jiliang Tang. 可信赖的人工智能：计算视角。*arXiv 预印本
    arXiv:2107.06641*, 2021。
- en: Liu et al. (2020) Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss,
    and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with
    deterministic deep learning via distance awareness. *Advances in Neural Information
    Processing Systems*, 33:7498–7512, 2020.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax
    Weiss, 和 Balaji Lakshminarayanan. 通过距离感知进行简单而有原则的不确定性估计。*神经信息处理系统进展*, 33:7498–7512,
    2020。
- en: Maces & Contributors (2019) Jan Maces and Repository Contributors. Github repository
    of keras-adf, 2019. URL [https://github.com/jmaces/keras-adf](https://github.com/jmaces/keras-adf).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maces & Contributors (2019) Jan Maces 和 Repository Contributors. keras-adf 的
    Github 仓库, 2019。网址 [https://github.com/jmaces/keras-adf](https://github.com/jmaces/keras-adf)。
- en: MacKay (1992) David JC MacKay. A practical Bayesian framework for backpropagation
    networks. *Neural computation*, 4(3):448–472, 1992.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacKay (1992) David JC MacKay. 用于反向传播网络的实用贝叶斯框架。*神经计算*, 4(3):448–472, 1992。
- en: Mahony et al. (2019) Niall O’ Mahony, Sean Campbell, Anderson Carvalho, Suman
    Harapanahalli, Gustavo Adolfo Velasco-Hernández, Lenka Krpalkova, Daniel Riordan,
    and Joseph Walsh. Deep learning vs. traditional computer vision. In *CVC*, 2019.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahony 等人（2019）**奈尔·O’马霍尼**、**肖恩·坎贝尔**、**安德森·卡瓦略**、**苏曼·哈拉帕纳赫利**、**古斯塔沃·阿道夫·维拉斯科-埃尔南德斯**、**连卡·克尔帕尔科娃**、**丹尼尔·里奥丹**
    和 **约瑟夫·沃尔什**。深度学习与传统计算机视觉。在 *CVC*，2019。
- en: Meissner et al. (2019) Eric Meissner, Zhenwen Dai, Tom Diethe, and Repository
    Contributors. Github repository of MXFusion, 2019. URL [https://github.com/amzn/MXFusion](https://github.com/amzn/MXFusion).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meissner 等人（2019）**埃里克·梅斯纳**、**甄文·戴**、**汤姆·迪特** 和 **仓库贡献者**。MXFusion 的 GitHub
    仓库，2019。网址 [https://github.com/amzn/MXFusion](https://github.com/amzn/MXFusion)。
- en: 'Metropolis et al. (1953) Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N.
    Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of state calculations
    by fast computing machines. *The Journal of Chemical Physics*, 21(6):1087–1092,
    1953. doi: 10.1063/1.1699114. URL [https://doi.org/10.1063/1.1699114](https://doi.org/10.1063/1.1699114).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Metropolis 等人（1953）**尼古拉斯·梅特罗波利斯**、**阿丽安娜·W·罗森布鲁斯**、**马歇尔·N·罗森布鲁斯**、**奥古斯塔·H·泰勒**
    和 **爱德华·泰勒**。通过快速计算机进行状态方程计算。*化学物理学杂志*，21(6)：1087–1092，1953。doi: 10.1063/1.1699114。网址
    [https://doi.org/10.1063/1.1699114](https://doi.org/10.1063/1.1699114)。'
- en: 'Meyer et al. (2019) Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez,
    and Carl K. Wellington. Lasernet: An efficient probabilistic 3d object detector
    for autonomous driving. In *CVPR*, pp.  12677–12686\. Computer Vision Foundation
    / IEEE, 2019. URL [http://dblp.uni-trier.de/db/conf/cvpr/cvpr2019.html#MeyerLKVW19](http://dblp.uni-trier.de/db/conf/cvpr/cvpr2019.html#MeyerLKVW19).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyer 等人（2019）**格雷戈里·P·迈耶**、**安基特·拉达**、**埃里克·基**、**卡洛斯·瓦莱斯皮-冈萨雷斯** 和 **卡尔·K·威灵顿**。Lasernet：一种高效的概率性三维物体检测器用于自动驾驶。在
    *CVPR*，第12677–12686页。计算机视觉基金会 / IEEE，2019。网址 [http://dblp.uni-trier.de/db/conf/cvpr/cvpr2019.html#MeyerLKVW19](http://dblp.uni-trier.de/db/conf/cvpr/cvpr2019.html#MeyerLKVW19)。
- en: 'Mitchell et al. (2019) Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker
    Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji,
    and Timnit Gebru. Model cards for model reporting. In *Proceedings of the Conference
    on Fairness, Accountability, and Transparency*, FAT* ’19, pp.  220–229, New York,
    NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596.
    URL [https://doi.org/10.1145/3287560.3287596](https://doi.org/10.1145/3287560.3287596).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mitchell 等人（2019）**玛格丽特·米切尔**、**西蒙·吴**、**安德鲁·扎尔迪瓦尔**、**帕克·巴恩斯**、**露西·瓦瑟曼**、**本·哈钦森**、**埃琳娜·斯皮策**、**伊尼奥鲁瓦·黛博拉·拉吉**
    和 **提姆尼特·盖布鲁**。模型卡片用于模型报告。在 *公平性、问责制和透明度会议论文集*，FAT* ’19，第220–229页，美国纽约，2019。计算机协会。ISBN
    9781450361255。doi: 10.1145/3287560.3287596。网址 [https://doi.org/10.1145/3287560.3287596](https://doi.org/10.1145/3287560.3287596)。'
- en: 'Montiel et al. (2020) Jacob Montiel, Max Halford, Saulo Martiello Mastelini,
    Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, Heitor Murilo Gomes,
    Jesse Read, Talel Abdessalem, and Albert Bifet. River: machine learning for streaming
    data in python, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montiel 等人（2020）**雅各布·蒙蒂尔**、**马克斯·哈尔福德**、**绍罗·马尔蒂耶洛·马斯特林尼**、**杰奥夫里·博尔米耶**、**拉斐尔·苏尔蒂**、**罗宾·维斯**、**阿迪尔·祖伊廷**、**海托尔·穆里洛·戈梅斯**、**杰西·里德**、**塔莱尔·阿卜杜萨勒姆**
    和 **阿尔贝特·比费**。River：用于流数据的机器学习工具，2020。
- en: 'Navrátil et al. (2021) Jirí Navrátil, Benjamin Elder, Matthew Arnold, Soumya Shubhra
    Ghosh, and Prasanna Sattigeri. Uncertainty characteristics curves: A systematic
    assessment of prediction intervals. *ArXiv*, abs/2106.00858, 2021.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Navrátil 等人（2021）**吉里·纳夫拉蒂尔**、**本杰明·埃尔德**、**马修·阿诺德**、**苏玛雅·舒布拉·戈什** 和 **普拉萨纳·萨蒂吉里**。不确定性特征曲线：预测区间的系统评估。*ArXiv*，abs/2106.00858，2021。
- en: Neal et al. (2011) Radford M Neal et al. MCMC using hamiltonian dynamics. *Handbook
    of markov chain monte carlo*, 2(11):2, 2011.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neal 等人（2011）**拉德福德·M·尼尔** 等人。使用哈密顿动力学的 MCMC。*马尔科夫链蒙特卡洛手册*，2(11)：2，2011。
- en: Nguyen et al. (2018) Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E
    Turner. Variational continual learning. In *International Conference on Learning
    Representations*, 2018.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人（2018）**光阮·V**、**英震·李**、**汤常·D·布伊** 和 **理查德·E·特纳**。变分持续学习。在 *国际学习表征会议*，2018。
- en: 'Nguyen et al. (2019) Giang Nguyen, Stefan Dlugolinsky, Martin Bobák, Viet Tran,
    Álvaro López García, Ignacio Heredia, Peter Malík, and Ladislav Hluch? Machine
    learning and deep learning frameworks and libraries for large-scale data mining:
    A survey. *Artif. Intell. Rev.*, 52(1):77–124, jun 2019. ISSN 0269-2821. doi:
    10.1007/s10462-018-09679-z. URL [https://doi.org/10.1007/s10462-018-09679-z](https://doi.org/10.1007/s10462-018-09679-z).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen 等人（2019）**江阮**、**斯特凡·杜洛林斯基**、**马丁·博巴克**、**越·陈**、**阿尔瓦罗·洛佩斯·加西亚**、**伊格纳西奥·埃雷迪亚**、**彼得·马利克**
    和 **拉迪斯拉夫·赫卢赫**。大规模数据挖掘的机器学习和深度学习框架与库：综述。*人工智能评论*，52(1)：77–124，2019年6月。ISSN 0269-2821。doi:
    10.1007/s10462-018-09679-z。网址 [https://doi.org/10.1007/s10462-018-09679-z](https://doi.org/10.1007/s10462-018-09679-z)。'
- en: Nicolae et al. (2018) Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat
    Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo,
    Bryant Chen, Heiko Ludwig, Ian Molloy, and Ben Edwards. Adversarial robustness
    toolbox v1.2.0. *CoRR*, 1807.01069, 2018. URL [https://arxiv.org/pdf/1807.01069](https://arxiv.org/pdf/1807.01069).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nicolae 等人 (2018) Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser,
    Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant
    Chen, Heiko Ludwig, Ian Molloy, 和 Ben Edwards. 对抗性鲁棒性工具箱 v1.2.0. *CoRR*, 1807.01069,
    2018. URL [https://arxiv.org/pdf/1807.01069](https://arxiv.org/pdf/1807.01069)。
- en: 'Nix & Weigend (1994) D.A. Nix and A.S. Weigend. Estimating the mean and variance
    of the target probability distribution. In *Proceedings of 1994 IEEE International
    Conference on Neural Networks (ICNN’94)*, volume 1, pp.  55–60 vol.1, 1994. doi:
    10/fth5jt.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nix & Weigend (1994) D.A. Nix 和 A.S. Weigend. 估计目标概率分布的均值和方差. 在 *1994年IEEE国际神经网络会议论文集
    (ICNN’94)* 中, 第1卷, 页码 55–60 第1卷, 1994. doi: 10/fth5jt。'
- en: Osawa et al. (2019) Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan,
    Anirudh Jain, Runa Eschenhagen, Richard E Turner, and Rio Yokota. Practical deep
    learning with Bayesian principles. *Advances in neural information processing
    systems*, 32, 2019.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osawa 等人 (2019) Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh
    Jain, Runa Eschenhagen, Richard E Turner, 和 Rio Yokota. 基于贝叶斯原则的实用深度学习. *神经信息处理系统进展*,
    32, 2019。
- en: 'Ovadia et al. (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley,
    Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
    Can you trust your model’s uncertainty? evaluating predictive uncertainty under
    dataset shift. In *Proceedings of the 33rd international conference on neural
    information processing systems*. Curran Associates Inc., 2019. Number of pages:
    12 tex.articleno: 1254.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ovadia 等人 (2019) Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley,
    Sebastian Nowozin, Joshua V. Dillon, Balaji Lakshminarayanan, 和 Jasper Snoek.
    你能相信你模型的不确定性吗？在数据集变化下评估预测不确定性. 在 *第33届神经信息处理系统国际会议论文集* 中. Curran Associates Inc.,
    2019. 页数：12 tex.articleno: 1254。'
- en: 'Papakipos & Bitton (2022) Zoe Papakipos and Joanna Bitton. Augly: Data augmentations
    for robustness, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papakipos & Bitton (2022) Zoe Papakipos 和 Joanna Bitton. Augly: 数据增强以提高鲁棒性,
    2022。'
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. Pytorch: An imperative style, high-performance deep learning
    library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d''Alché-Buc, E. Fox,
    and R. Garnett (eds.), *Advances in Neural Information Processing Systems 32*,
    pp.  8024–8035\. Curran Associates, Inc., 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等人 (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, 和 Soumith
    Chintala. Pytorch: 一种命令式风格的高性能深度学习库. 在 H. Wallach, H. Larochelle, A. Beygelzimer,
    F. d''Alché-Buc, E. Fox, 和 R. Garnett (编), *神经信息处理系统进展 32*, 页码 8024–8035. Curran
    Associates, Inc., 2019。'
- en: Rasmussen & Williams (2006) Carl Edward Rasmussen and Christopher K. I. Williams.
    *Gaussian processes for machine learning.* Adaptive computation and machine learning.
    MIT Press, 2006. ISBN 026218253X.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasmussen & Williams (2006) Carl Edward Rasmussen 和 Christopher K. I. Williams.
    *机器学习中的高斯过程*. 自适应计算与机器学习. MIT Press, 2006. ISBN 026218253X。
- en: 'Rezende & Mohamed (2015) Danilo Jimenez Rezende and Shakir Mohamed. Variational
    inference with normalizing flows. In *Proceedings of the 32nd international conference
    on international conference on machine learning - volume 37*, ICML’15, pp. 1530–1538\.
    JMLR.org, 2015. Place: Lille, France Number of pages: 9.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezende & Mohamed (2015) Danilo Jimenez Rezende 和 Shakir Mohamed. 使用归一化流的变分推断.
    在 *第32届国际机器学习会议论文集 - 第37卷* 中, ICML’15, 页码 1530–1538. JMLR.org, 2015. 地点：法国里尔 页数：9。
- en: Ritter et al. (2018) Hippolyt Ritter, Aleksandar Botev, and David Barber. A
    scalable laplace approximation for neural networks. In *International Conference
    on Learning Representations*, 2018. URL [https://openreview.net/forum?id=Skdvd2xAZ](https://openreview.net/forum?id=Skdvd2xAZ).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter 等人 (2018) Hippolyt Ritter, Aleksandar Botev, 和 David Barber. 神经网络的可扩展拉普拉斯近似.
    在 *国际学习表征会议* 中, 2018. URL [https://openreview.net/forum?id=Skdvd2xAZ](https://openreview.net/forum?id=Skdvd2xAZ)。
- en: Sensoy et al. (2018) Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential
    deep learning to quantify classification uncertainty. *Advances in Neural Information
    Processing Systems*, 31, 2018.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sensoy 等人 (2018) Murat Sensoy, Lance Kaplan, 和 Melih Kandemir. 证据深度学习以量化分类不确定性.
    *神经信息处理系统进展*, 31, 2018。
- en: 'Shi et al. (2017) Jiaxin Shi, Jianfei. Chen, Jun Zhu, Shengyang Sun, Yucen
    Luo, Yihong Gu, and Yuhao Zhou. ZhuSuan: A library for Bayesian deep learning.
    *arXiv preprint arXiv:1709.05870*, 2017.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. (2017) Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo,
    Yihong Gu, 和 Yuhao Zhou. ZhuSuan: 一个用于贝叶斯深度学习的库。*arXiv预印本 arXiv:1709.05870*，2017年。'
- en: Sicking et al. (2021) Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz,
    Asja Fischer, and Stefan Wrobel. Wasserstein dropout, 2021. URL [https://arxiv.org/abs/2012.12687](https://arxiv.org/abs/2012.12687).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sicking et al. (2021) Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz,
    Asja Fischer, 和 Stefan Wrobel. Wasserstein dropout，2021年。URL [https://arxiv.org/abs/2012.12687](https://arxiv.org/abs/2012.12687)。
- en: Sicking et al. (2022) Joachim Sicking, Maram Akila, Jan David Schneider, Fabian
    Hüger, Peter Schlicht, Tim Wirtz, and Stefan Wrobel. Tailored uncertainty estimation
    for deep learning systems, 2022. URL [https://arxiv.org/abs/2204.13963](https://arxiv.org/abs/2204.13963).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sicking et al. (2022) Joachim Sicking, Maram Akila, Jan David Schneider, Fabian
    Hüger, Peter Schlicht, Tim Wirtz, 和 Stefan Wrobel. 针对深度学习系统的定制不确定性估计，2022年。URL
    [https://arxiv.org/abs/2204.13963](https://arxiv.org/abs/2204.13963)。
- en: Sommerville (2010) Ian Sommerville. *Software Engineering*. Addison-Wesley,
    Harlow, England, 9 edition, 2010. ISBN 978-0-13-703515-1.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sommerville (2010) Ian Sommerville. *软件工程*。Addison-Wesley，哈罗英国，第9版，2010年。ISBN
    978-0-13-703515-1。
- en: 'Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural
    networks from overfitting. *The journal of machine learning research*, 15(1):1929–1958,
    2014.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
    Ilya Sutskever, 和 Ruslan Salakhutdinov. Dropout：防止神经网络过拟合的简单方法。*机器学习研究期刊*，15(1):1929–1958，2014年。
- en: Thiagarajan et al. (2020) Jayaraman J. Thiagarajan, Bindya Venkatesh, Prasanna
    Sattigeri, and Peer-Timo Bremer. Building calibrated deep models via uncertainty
    matching with auxiliary interval predictors. In *AAAI*, 2020.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thiagarajan et al. (2020) Jayaraman J. Thiagarajan, Bindya Venkatesh, Prasanna
    Sattigeri, 和 Peer-Timo Bremer. 通过与辅助区间预测器的不确定性匹配构建校准深度模型。见于 *AAAI*，2020年。
- en: 'Torfi et al. (2020) Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader
    Tavaf, and Edward A Fox. Natural language processing advancements by deep learning:
    A survey. *arXiv preprint arXiv:2003.01200*, 2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torfi et al. (2020) Amirsina Torfi, Rouzbeh A Shirvani, Yaser Keneshloo, Nader
    Tavaf, 和 Edward A Fox. 深度学习推动的自然语言处理进展：一项调查。*arXiv预印本 arXiv:2003.01200*，2020年。
- en: Tran et al. (2016) Dustin Tran, Rajesh Ranganath, and David M Blei. The variational
    gaussian process. In *4th International Conference on Learning Representations,
    ICLR 2016*, 2016.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran et al. (2016) Dustin Tran, Rajesh Ranganath, 和 David M Blei. 变分高斯过程。见于
    *第4届国际学习表征大会，ICLR 2016*，2016年。
- en: Tran et al. (2018) Dustin Tran, Matthew D. Hoffman, Dave Moore, Christopher
    Suter, Srinivas Vasudevan, Alexey Radul, Matthew Johnson, and Rif A. Saurous.
    Simple, distributed, and accelerated probabilistic programming. In *Neural Information
    Processing Systems*, 2018.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran et al. (2018) Dustin Tran, Matthew D. Hoffman, Dave Moore, Christopher
    Suter, Srinivas Vasudevan, Alexey Radul, Matthew Johnson, 和 Rif A. Saurous. 简单、分布式和加速的概率编程。见于
    *神经信息处理系统*，2018年。
- en: van de Meent et al. (2018) Jan-Willem van de Meent, Brooks Paige, Hongseok Yang,
    and Frank Wood. An introduction to probabilistic programming. *arXiv preprint
    arXiv:1809.10756*, 2018.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van de Meent et al. (2018) Jan-Willem van de Meent, Brooks Paige, Hongseok Yang,
    和 Frank Wood. 概率编程导论。*arXiv预印本 arXiv:1809.10756*，2018年。
- en: Vyas et al. (2018) Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das,
    Bharat Kaul, and Theodore L Willke. Out-of-distribution detection using an ensemble
    of self supervised leave-out classifiers. In *Proceedings of the European Conference
    on Computer Vision (ECCV)*, pp.  550–564, 2018.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vyas et al. (2018) Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das,
    Bharat Kaul, 和 Theodore L Willke. 使用自监督留出分类器的分布外检测。见于 *欧洲计算机视觉大会（ECCV）*，第550–564页，2018年。
- en: 'Wang et al. (2019) Zhaobin Wang, Ke Liu, Jian Li, Ying Zhu, and Yaonan Zhang.
    Various frameworks and libraries of machine learning and deep learning: A survey.
    *Archives of Computational Methods in Engineering*, 02 2019. doi: 10.1007/s11831-018-09312-w.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019) Zhaobin Wang, Ke Liu, Jian Li, Ying Zhu, 和 Yaonan Zhang.
    机器学习和深度学习的各种框架和库：一项调查。*计算方法工程档案*，2019年2月。doi: 10.1007/s11831-018-09312-w。'
- en: 'Weiss & Tonella (2021) Michael Weiss and Paolo Tonella. Uncertainty-wizard:
    Fast and user-friendly neural network uncertainty quantification. In *2021 14th
    IEEE Conference on Software Testing, Verification and Validation (ICST)*, pp. 
    436–441\. IEEE, 2021.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weiss & Tonella (2021) Michael Weiss 和 Paolo Tonella. Uncertainty-wizard: 快速且用户友好的神经网络不确定性量化。见于
    *2021年第14届IEEE软件测试、验证与验证会议（ICST）*，第436–441页。IEEE，2021年。'
- en: Welling & Teh (2011) Max Welling and Yee Whye Teh. Bayesian learning via stochastic
    gradient Langevin dynamics. In *Proceedings of the 28th International Conference
    on International Conference on Machine Learning*, ICML’11, pp.  681–688, Madison,
    WI, USA, 2011\. Omnipress. ISBN 9781450306195.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welling & Teh (2011) Max Welling 和 Yee Whye Teh. 通过随机梯度朗之万动力学的贝叶斯学习。发表于*第28届国际机器学习大会论文集*，ICML’11,
    pp. 681–688, 麦迪逊, WI, USA, 2011. Omnipress. ISBN 9781450306195.
- en: 'Wen et al. (2019) Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an
    alternative approach to efficient ensemble and lifelong learning. In *International
    Conference on Learning Representations*, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人 (2019) Yeming Wen, Dustin Tran 和 Jimmy Ba. Batchensemble：一种高效的集成和终身学习的替代方法。发表于*国际学习表征大会*，2019.
- en: 'Yurtsever et al. (2020) Ekim Yurtsever, Jacob Lambert, Alexander Carballo,
    and Kazuya Takeda. A survey of autonomous driving: Common practices and emerging
    technologies. *IEEE Access*, 8:58443–58469, 2020. doi: 10.1109/ACCESS.2020.2983149.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yurtsever 等人 (2020) Ekim Yurtsever, Jacob Lambert, Alexander Carballo 和 Kazuya
    Takeda. 自主驾驶调查：常见做法与新兴技术。*IEEE Access*, 8:58443–58469, 2020. doi: 10.1109/ACCESS.2020.2983149.'
- en: Zacharias et al. (2018) Jan Zacharias, Michael Barz, and Daniel Sonntag. A survey
    on deep learning toolkits and libraries for intelligent user interfaces. *CoRR*,
    abs/1803.04818, 2018. URL [http://arxiv.org/abs/1803.04818](http://arxiv.org/abs/1803.04818).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zacharias 等人 (2018) Jan Zacharias, Michael Barz 和 Daniel Sonntag. 智能用户界面的深度学习工具包与库调查。*CoRR*,
    abs/1803.04818, 2018. URL [http://arxiv.org/abs/1803.04818](http://arxiv.org/abs/1803.04818).
- en: Zhao et al. (2020) Shengjia Zhao, Tengyu Ma, and Stefano Ermon. Individual calibration
    with randomized forecasting. In *International Conference on Machine Learning*,
    pp. 11387–11397\. PMLR, 2020.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 (2020) Shengjia Zhao, Tengyu Ma 和 Stefano Ermon. 基于随机预测的个体校准。发表于*国际机器学习大会*，pp.
    11387–11397. PMLR, 2020.
- en: Appendix
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 6.1 Detailed overview on uncertainty modeling and assessment
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 不确定性建模与评估的详细概述
- en: The following section gives an literature overview over uncertainty estimation
    in DL, describing sources of uncertainty, approaches and applications for uncertainty
    estimation and assessment. It provides further details to the paragraph on “elements
    of uncertainty estimation” in [section 2](#S2 "2 Related work ‣ A Survey on Uncertainty
    Toolkits for Deep Learning").
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分提供了关于深度学习中不确定性估计的文献综述，描述了不确定性的来源、估计和评估的不确定性方法及其应用。它提供了[第2节](#S2 "2 Related
    work ‣ A Survey on Uncertainty Toolkits for Deep Learning")中“不确定性估计的元素”段落的进一步细节。
- en: 'Neural networks are subject to several kinds of uncertainties (Hüllermeier
    & Waegeman, [2021](#bib.bib43)). We generally do not know whether the chosen neural
    network model is optimal for the given task. There are also approximation errors
    caused by the optimization procedure (e.g. due to (suboptimal) hyperparameter
    choices, random initializations, lack of data). Such uncertainties that arise
    from lack of knowledge about the optimal model for a given task are subsumed under
    the term epistemic uncertainty, which is theoretically reducible e.g. by selecting
    better models or supplying more training data. Additionally, there is irreducible
    aleatoric uncertainty, which is stochasticity in the outcome of the training procedure
    caused by inherently random factors, such as most importantly the data (e.g. sensor
    noise, mislabeled inputs or imperfect information). Besides for quantifying prediction
    and estimation variance, uncertainty estimation has also been explored for auxiliary
    tasks, such as detecting out-of-distribution inputs (Ovadia et al., [2019](#bib.bib69);
    Vyas et al., [2018](#bib.bib86)), active learning (Gal et al., [2017](#bib.bib25);
    Beluch et al., [2018](#bib.bib10)), detection of adversarial examples (Ritter
    et al., [2018](#bib.bib74); Amini et al., [2020](#bib.bib6)) and continual learning
    (Osawa et al., [2019](#bib.bib68); Nguyen et al., [2018](#bib.bib64)). The main
    tool for deep uncertainty estimation are probability distributions, typically
    over network outputs. The expected value serves as main network prediction and
    uncertainty is quantified in terms of variance, quantiles or entropy. Approaches
    for uncertainty estimation include the following (see Gawlikowski et al. ([2021](#bib.bib27))
    for a comprehensive survey):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络存在几种不确定性（Hüllermeier & Waegeman, [2021](#bib.bib43)）。我们通常不知道所选择的神经网络模型是否最适合给定任务。此外，优化过程还会引入近似误差（例如由于（次优的）超参数选择、随机初始化、数据不足等）。这些因对给定任务的最优模型缺乏了解而产生的不确定性被统称为认识不确定性，它在理论上可以通过选择更好的模型或提供更多的训练数据来减少。此外，还有不可减少的随机不确定性，即由本质上随机的因素引起的训练过程结果的随机性，最重要的是数据（例如传感器噪声、标签错误的输入或信息不完善）。除了用于量化预测和估计的方差外，不确定性估计也被探索用于辅助任务，如检测分布外的输入（Ovadia
    et al., [2019](#bib.bib69); Vyas et al., [2018](#bib.bib86)）、主动学习（Gal et al.,
    [2017](#bib.bib25); Beluch et al., [2018](#bib.bib10)）、对抗样本检测（Ritter et al., [2018](#bib.bib74);
    Amini et al., [2020](#bib.bib6)）和持续学习（Osawa et al., [2019](#bib.bib68); Nguyen
    et al., [2018](#bib.bib64)）。深度不确定性估计的主要工具是概率分布，通常是网络输出的分布。期望值作为主要的网络预测，而不确定性则通过方差、分位数或熵来量化。不确定性估计的方法包括以下几种（详见
    Gawlikowski et al. ([2021](#bib.bib27)) 的综合调查）：
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In parametric likelihood (PL) methods, the network outputs distributional parameters
    instead of point estimates and is trained via likelihood optimization (Nix & Weigend,
    [1994](#bib.bib67); Amini et al., [2020](#bib.bib6); Sensoy et al., [2018](#bib.bib75)).
    They are commonly used to estimate aleatoric uncertainty and are combined with
    other methods to incorporate epistemic uncertainty in addition (Kendall & Gal,
    [2017](#bib.bib44)).
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在参数化似然（PL）方法中，网络输出的是分布参数而非点估计，通过似然优化进行训练（Nix & Weigend, [1994](#bib.bib67);
    Amini et al., [2020](#bib.bib6); Sensoy et al., [2018](#bib.bib75)）。这些方法通常用于估计随机不确定性，并且与其他方法结合以同时纳入认识不确定性（Kendall
    & Gal, [2017](#bib.bib44)）。
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bayesian neural networks (BNNs) extend the parametric likelihood approach and
    incorporate (epistemic) uncertainty on network parameters by estimating a posterior
    and predictive distribution based on Bayes rule. Different posterior estimation
    procedures have been considered for DL, including variational inference (VI-BNNs,
    Graves ([2011](#bib.bib32)); Blundell et al. ([2015](#bib.bib12)); Rezende & Mohamed
    ([2015](#bib.bib73))) with dropout sampling (Gal & Ghahramani, [2016](#bib.bib24);
    Kingma et al., [2015](#bib.bib46)) being an important variant, expectation propagation
    (Hernández-Lobato & Adams, [2015](#bib.bib39); Gast & Roth, [2018](#bib.bib26)),
    Markov-chain Monte Carlo sampling (MCMC, Neal et al. ([2011](#bib.bib63)); Welling
    & Teh ([2011](#bib.bib89)); Li et al. ([2016](#bib.bib51))) or the Laplace approximation
    (MacKay, [1992](#bib.bib55); Ritter et al., [2018](#bib.bib74)). Gaussian processes
    (GP, Rasmussen & Williams ([2006](#bib.bib72))) are a related Bayesian approach
    that incorporate uncertainty in a more general function space, instead of the
    network’s parameter space. Standard GPs are computationally infeasible for large-scale
    models. However, there are scalable variants (Hensman et al., [2013](#bib.bib38))
    that can even be used as a differentiable network component (Tran et al., [2016](#bib.bib83))
    and as a building block for UE in deep learning models (Liu et al., [2020](#bib.bib53)).
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯神经网络（BNNs）扩展了参数化似然方法，通过基于贝叶斯规则估计后验和预测分布来引入（认识上的）不确定性。针对深度学习（DL）考虑了不同的后验估计程序，包括变分推断（VI-BNNs,
    Graves ([2011](#bib.bib32)); Blundell et al. ([2015](#bib.bib12)); Rezende & Mohamed
    ([2015](#bib.bib73)))，其中dropout采样（Gal & Ghahramani, [2016](#bib.bib24); Kingma
    et al., [2015](#bib.bib46)）是一个重要的变体，期望传播（Hernández-Lobato & Adams, [2015](#bib.bib39);
    Gast & Roth, [2018](#bib.bib26)），马尔可夫链蒙特卡洛采样（MCMC, Neal et al. ([2011](#bib.bib63));
    Welling & Teh ([2011](#bib.bib89)); Li et al. ([2016](#bib.bib51))) 或拉普拉斯近似（MacKay,
    [1992](#bib.bib55); Ritter et al., [2018](#bib.bib74)）。高斯过程（GP, Rasmussen & Williams
    ([2006](#bib.bib72))) 是一种相关的贝叶斯方法，它在更一般的函数空间中引入不确定性，而不是网络的参数空间。标准GP对于大规模模型在计算上不可行。然而，存在可扩展的变体（Hensman
    et al., [2013](#bib.bib38)），这些变体甚至可以用作可微分网络组件（Tran et al., [2016](#bib.bib83)）以及深度学习模型中UE的构建块（Liu
    et al., [2020](#bib.bib53)）。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Frequentist approaches directly leverage a combination of different models for
    uncertainty estimation. Common approaches are based on ensembling (Lakshminarayanan
    et al., [2017](#bib.bib49); Wen et al., [2019](#bib.bib90); Durasov et al., [2021](#bib.bib21)))
    or the jackknife method (Giordano et al., [2019](#bib.bib29); Alaa & Van Der Schaar,
    [2020](#bib.bib3); Kim et al., [2020](#bib.bib45)).
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 频率主义方法直接利用不同模型的组合来进行不确定性估计。常见的方法基于集成方法（Lakshminarayanan et al., [2017](#bib.bib49);
    Wen et al., [2019](#bib.bib90); Durasov et al., [2021](#bib.bib21))) 或者切除法（Giordano
    et al., [2019](#bib.bib29); Alaa & Van Der Schaar, [2020](#bib.bib3); Kim et al.,
    [2020](#bib.bib45))。
- en: Uncertainty methods can be further categorized into (i) intrinsic methods, that
    integrate the uncertainty estimation directly into the architecture or training
    procedure (e.g. ensemble training (Lakshminarayanan et al., [2017](#bib.bib49))
    and dropout-based approaches (Gal & Ghahramani, [2016](#bib.bib24); Kendall &
    Gal, [2017](#bib.bib44); Fan et al., [2021](#bib.bib22); Sicking et al., [2021](#bib.bib77))),
    (ii) post-hoc methods, that extend standard deep learning models (e.g. Laplace
    approximation, infinitesimal jackknife, surrogate models (Chen et al., [2019](#bib.bib17)))
    and (iii) recalibration methods, that seek to improve existing uncertainty estimates
    (Guo et al., [2017](#bib.bib33); Kuleshov et al., [2018](#bib.bib48); Navrátil
    et al., [2021](#bib.bib62)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性方法可以进一步分类为：（i）内在方法，这些方法将不确定性估计直接集成到架构或训练过程中（例如，集成训练（Lakshminarayanan et
    al., [2017](#bib.bib49)）和基于dropout的方法（Gal & Ghahramani, [2016](#bib.bib24); Kendall
    & Gal, [2017](#bib.bib44); Fan et al., [2021](#bib.bib22); Sicking et al., [2021](#bib.bib77)）），（ii）后处理方法，这些方法扩展标准深度学习模型（例如，拉普拉斯近似、微小切除、代理模型（Chen
    et al., [2019](#bib.bib17)））以及（iii）重新校准方法，这些方法旨在改进现有的不确定性估计（Guo et al., [2017](#bib.bib33);
    Kuleshov et al., [2018](#bib.bib48); Navrátil et al., [2021](#bib.bib62)）。
- en: We now discuss several techniques for assessing the quality of uncertainty estimates.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们讨论几种评估不确定性估计质量的技术。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: (Proper) Scoring rules are point-wise metrics that measure for the fit of a
    predicted distribution to a ground-truth value and are typically evaluated on
    held-out validation datasets. Common scoring rules include the brier score, negative
    log-likelihood (NLL), continuous ranked probability score (CRPS) or the interval
    score (see Gneiting & Raftery ([2007](#bib.bib30))).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （适当的）评分规则是逐点度量，用于衡量预测分布与真实值的拟合程度，通常在保留的验证数据集上进行评估。常见的评分规则包括布雷尔分数、负对数似然（NLL）、连续排名概率分数（CRPS）或区间分数（见
    Gneiting & Raftery ([2007](#bib.bib30))）。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Confidence calibration measures the alignment of confidence estimates with a
    validation dataset (e.g. a $90\%$ confidence interval should contain $90\%$ correct
    labels). Calibration can be visually assessed via reliability diagrams or curves
    (Guo et al., [2017](#bib.bib33); Navrátil et al., [2021](#bib.bib62)) that plot
    confidence levels against a correctness metric (e.g. classification accuracy).
    Quantitative metrics typically consider the area under the aforementioned curves,
    most notably the expected calibration error (ECE) (Guo et al., [2017](#bib.bib33);
    Kuleshov et al., [2018](#bib.bib48)). Adversarial group calibration (Zhao et al.,
    [2020](#bib.bib93)) is a more strict variant of calibration assessment that considers
    alignment of confidence estimates with every subset of the dataset, instead of
    only the whole dataset.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信心校准衡量的是信心估计与验证数据集的对齐程度（例如，$90\%$ 的信心区间应包含 $90\%$ 的正确标签）。校准可以通过可靠性图或曲线（Guo et
    al., [2017](#bib.bib33); Navrátil et al., [2021](#bib.bib62)）在视觉上进行评估，这些图表绘制了信心水平与正确性指标（例如分类准确性）之间的关系。定量指标通常考虑上述曲线下的面积，最著名的是预期校准误差（ECE）（Guo
    et al., [2017](#bib.bib33); Kuleshov et al., [2018](#bib.bib48)）。对抗性组校准（Zhao et
    al., [2020](#bib.bib93)）是一种更严格的校准评估变体，它考虑了信心估计与数据集每个子集的对齐程度，而不仅仅是整个数据集。
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty estimates can also be qualitatively assessed, e.g. by visually inspecting
    predicted distributions, confidence intervals against ground-truth values. Considering
    the average uncertainty across a dataset (also called sharpness) is crucial in
    calibration assessments as argued by Kuleshov et al. ([2018](#bib.bib48)).
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不确定性估计也可以通过定性方式进行评估，例如通过视觉检查预测分布、信心区间与真实值的关系。考虑数据集中的平均不确定性（也称为锐度）在校准评估中至关重要，正如
    Kuleshov et al. ([2018](#bib.bib48)) 所讨论的那样。
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Auxiliary scores measure the performance of uncertainty estimates in auxiliary
    tasks such as out-of-distribution detection. Standard evaluation methods for binary
    threshold classifiers (e.g. separating true from false detections) include precision-recall
    and receiver operating curves for visual assessment or computing average precision
    or AUCROC scores. Another approach is to compare histograms of uncertainty estimates
    (e.g. variance or predictive entropy) on inputs from each class (e.g. in-distribution
    or out-of-distribution), as done in Ovadia et al. ([2019](#bib.bib69)).
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 辅助分数衡量不确定性估计在辅助任务中的表现，如异常检测。标准的评估方法包括二分类阈值分类器（例如，区分真阳性与假阳性）的精确度-召回率和接收者操作特征曲线用于视觉评估，或计算平均精确度或
    AUCROC 分数。另一种方法是比较来自每个类别（例如，分布内或分布外）的不确定性估计（例如，方差或预测熵）的直方图，正如 Ovadia et al. ([2019](#bib.bib69))
    所做的那样。
- en: 6.2 Comparative analysis of the selected uncertainty toolkits
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 选择的不确定性工具包的比较分析
- en: This section provides two tables with additional information about the toolkits.
    [Table 1](#Sx1.T1 "Table 1 ‣ 6.2 Comparative analysis of the selected uncertainty
    toolkits ‣ Appendix ‣ A Survey on Uncertainty Toolkits for Deep Learning") lists
    all toolkits with their license, intended purpose and the exact version that has
    been considered in this survey. [Table 2](#Sx1.T2 "Table 2 ‣ 6.2 Comparative analysis
    of the selected uncertainty toolkits ‣ Appendix ‣ A Survey on Uncertainty Toolkits
    for Deep Learning") provides a concise summary of the results of our analysis
    from [section 5](#S5 "5 Comparative analysis of the selected uncertainty toolkits
    ‣ A Survey on Uncertainty Toolkits for Deep Learning").
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了两个表格，包含有关工具包的附加信息。[表格 1](#Sx1.T1 "Table 1 ‣ 6.2 Comparative analysis of
    the selected uncertainty toolkits ‣ Appendix ‣ A Survey on Uncertainty Toolkits
    for Deep Learning") 列出了所有工具包及其许可证、预期用途和在本次调查中考虑的确切版本。[表格 2](#Sx1.T2 "Table 2 ‣
    6.2 Comparative analysis of the selected uncertainty toolkits ‣ Appendix ‣ A Survey
    on Uncertainty Toolkits for Deep Learning") 提供了我们分析结果的简要总结，来自[第 5 节](#S5 "5 Comparative
    analysis of the selected uncertainty toolkits ‣ A Survey on Uncertainty Toolkits
    for Deep Learning")。
- en: 'Table 1: General information on the examined uncertainty estimation toolkits.
    All toolkits use python as programming language. We provide the github commit
    number in cases where no version number is given.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：所考察的不确定性估计工具包的一般信息。所有工具包均使用 Python 作为编程语言。对于没有版本号的情况，我们提供 GitHub 提交号。
- en: '| UE toolkit | Developer | License | Version/ commit | Base libraries/ frameworks
    | Toolkit type |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| UE 工具包 | 开发者 | 许可证 | 版本/提交 | 基础库/框架 | 工具包类型 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| TFP | Dillon et al. ([2017](#bib.bib19)) (Google Brain) | Apache-2.0 | `0.15.0`
    | Tensorflow/ Keras | PPL |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| TFP | Dillon 等人 ([2017](#bib.bib19))（Google Brain） | Apache-2.0 | `0.15.0`
    | Tensorflow/ Keras | PPL |'
- en: '| Pyro | Bingham et al. ([2019](#bib.bib11)) (Uber AI Labs) | Apache-2.0 |
    `1.8.0` | Pytorch, JAX | PPL |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Pyro | Bingham 等人 ([2019](#bib.bib11))（Uber AI Labs） | Apache-2.0 | `1.8.0`
    | Pytorch, JAX | PPL |'
- en: '| MXF | Meissner et al. ([2019](#bib.bib57)) (Amazon Web Services) | Apache-2.0
    | `0.3.1` | MXNet | PPL |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| MXF | Meissner 等人 ([2019](#bib.bib57))（Amazon Web Services） | Apache-2.0
    | `0.3.1` | MXNet | PPL |'
- en: '| ZS | Shi et al. ([2017](#bib.bib76)) | MIT | `4386b2a` | Tensorflow | PPL
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ZS | Shi 等人 ([2017](#bib.bib76)) | MIT | `4386b2a` | Tensorflow | PPL |'
- en: '| ED2 | Tran et al. ([2018](#bib.bib84)) (Google Brain) | Apache-2.0 | `f420d83`
    | Tensorflow/ Keras | PPL |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ED2 | Tran 等人 ([2018](#bib.bib84))（Google Brain） | Apache-2.0 | `f420d83`
    | Tensorflow/ Keras | PPL |'
- en: '| GTS | Alexandrov et al. ([2020](#bib.bib5)) (Amazon Web Services) | Apache-2.0
    | `0.9.0` | MXNet, Pytorch | Time series |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| GTS | Alexandrov 等人 ([2020](#bib.bib5))（Amazon Web Services） | Apache-2.0
    | `0.9.0` | MXNet, Pytorch | 时间序列 |'
- en: '| UQ360 | Ghosh et al. ([2021](#bib.bib28)) (IBM Research) | Apache-2.0 | `2378bfa`
    | Pytorch | Dedicated UE |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| UQ360 | Ghosh 等人 ([2021](#bib.bib28))（IBM Research） | Apache-2.0 | `2378bfa`
    | Pytorch | 专用 UE |'
- en: '| UT | Chung et al. ([2021](#bib.bib18)) | MIT | `v0.1.0` | Scipy, Scikit-learn
    | Dedicated UE |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| UT | Chung 等人 ([2021](#bib.bib18)) | MIT | `v0.1.0` | Scipy, Scikit-learn
    | 专用 UE |'
- en: '| UW | Weiss & Tonella ([2021](#bib.bib88)) | MIT | `v0.2.0` | Tensorflow/
    Keras | Dedicated UE |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| UW | Weiss & Tonella ([2021](#bib.bib88)) | MIT | `v0.2.0` | Tensorflow/
    Keras | 专用 UE |'
- en: '| BT | Krishnan et al. ([2022](#bib.bib47)) (Intel Labs) | BSD3 | `99876f3`
    | Pytorch | Dedicated UE |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| BT | Krishnan 等人 ([2022](#bib.bib47))（Intel Labs） | BSD3 | `99876f3` | Pytorch
    | 专用 UE |'
- en: '| KADF | Maces & Contributors ([2019](#bib.bib54)) | MIT | `19.1.0` | Tensorflow/
    Keras | Dedicated UE |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| KADF | Maces & Contributors ([2019](#bib.bib54)) | MIT | `19.1.0` | Tensorflow/
    Keras | 专用 UE |'
- en: 'Table 2: Analysis summary of the uncertainty toolkits selected in [section 3](#S3
    "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty Toolkits for
    Deep Learning"). The top table considers all toolkits with respect to our core
    criteria (cf. [section 4](#S4 "4 Evaluation criteria for uncertainty toolkits
    ‣ A Survey on Uncertainty Toolkits for Deep Learning")). The bottom table provides
    additional criteria and considers the three best-performing toolkits with respect
    to the core criteria. “Standard statistics” refers to general (sample) measures
    such as variance, quantiles or correlation.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在[第 3 节](#S3 "3 Selection of deep uncertainty toolkits ‣ A Survey on Uncertainty
    Toolkits for Deep Learning")中选择的不确定性工具包的分析总结。顶部表格考虑了所有工具包相对于我们的核心标准（参见 [第 4 节](#S4
    "4 Evaluation criteria for uncertainty toolkits ‣ A Survey on Uncertainty Toolkits
    for Deep Learning")）。底部表格提供了额外标准，并考虑了相对于核心标准的三种表现最佳的工具包。“标准统计”指的是一般的（样本）度量，如方差、分位数或相关性。
- en: '| UE toolkit | Range of supported uncertainty methods | Range of supported
    evaluation techniques | Range of supported architectures and data structures |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| UE 工具包 | 支持的不确定性方法范围 | 支持的评估技术范围 | 支持的架构和数据结构范围 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| TFP | • intrinsic (PL, VI-BNN, GP, MCMC) • high range of supported distributions
    | • broad range of standard statistics • scoring rules (NLL, Brier) • classification
    calibration (ECE) | • regression, classification, sequential |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| TFP | • 内在（PL, VI-BNN, GP, MCMC） • 支持的分布范围广 | • 广泛的标准统计 • 评分规则（NLL, Brier）
    • 分类校准（ECE） | • 回归、分类、序列 |'
- en: '| Pyro | • intrinsic (PL, VI-BNN, GP, MCMC) • high range of supported distributions
    | • standard statistics • scoring rules (NLL, CRPS) | • regression, classification,
    sequential |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Pyro | • 内在（PL, VI-BNN, GP, MCMC） • 支持的分布范围广 | • 标准统计 • 评分规则（NLL, CRPS） |
    • 回归、分类、序列 |'
- en: '| ED2 | • intrinsic (PL, VI-BNN, GP, MCMC) • high range of supported distributions
    | • narrow range of scoring rules (NLL) | • regression, classification, sequential
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| ED2 | • 内在（PL, VI-BNN, GP, MCMC） • 支持的分布范围广 | • 狭窄的评分规则范围（NLL） | • 回归、分类、序列
    |'
- en: '| ZS | • intrinsic (PL, VI-BNN, MCMC) • high range of supported distributions
    | • narrow range of standard statistics • narrow range of scoring rules (NLL)
    | • regression, classification, sequential |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| ZS | • 内在（PL, VI-BNN, MCMC） • 支持的分布范围广 | • 标准统计范围窄 • 评分规则范围窄（NLL） | • 回归、分类、序列
    |'
- en: '| MXF | • intrinsic (PL, VI-BNN, GP) | [none] | • regression, classification
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| MXF | • 内在（PL, VI-BNN, GP） | [无] | • 回归、分类 |'
- en: '| GTS | • intrinsic (PL) | • scoring rules • plotting function for conf. intervals
    | • sequential |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| GTS | • 内在（PL） | • 评分规则 • 置信区间绘图函数 | • 序列 |'
- en: '| UQ360 | • intrinsic (PL, VI-BNN, ensembling) • post-hoc (jackknife-based,
    surrogate models) • recalibration | • scoring rules • calibration assessment (ECE)
    • plotting functions | • regression, classification • tabular inputs • low flexibility
    in intrinsic methods |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| UQ360 | • 内在（PL, VI-BNN, 集成） • 后验（基于切片的，代理模型） • 再校准 | • 评分规则 • 校准评估（ECE）
    • 绘图功能 | • 回归、分类 • 表格输入 • 内在方法灵活性低 |'
- en: '| UT | • basic recalibration | • broad range of scoring rules • calibration
    assessment (ECE, AGC) • plotting functions | • 1D-regression |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| UT | • 基本再校准 | • 广泛的评分规则 • 校准评估（ECE, AGC） • 绘图功能 | • 1D回归 |'
- en: '| UW | • intrinsic (ensembling, dropout) | [none] | • regression, classification
    • custom uncertainty quantifiers |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| UW | • 内在（集成，dropout） | [无] | • 回归、分类 • 自定义不确定性量化器 |'
- en: '| BT | • intrinsic (VI-BNN) | [none] | • regression, classification, recurrent
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| BT | • 内在（VI-BNN） | [无] | • 回归、分类、递归 |'
- en: '| KADF | • intrinsic (VI-ADF) | [none] | • regression, classification |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| KADF | • 内在（VI-ADF） | [无] | • 回归、分类 |'
- en: 'Abbreviations: PL: parametric likelihood, VI-BNN: variational inference-based
    Bayesian neural networks, GP: Gaussian processes, VI-ADF: assumed density filtering
    Bayesian neural network (a variant of expectation propagation), MCMC: Markov chain
    Monte Carlo-based posterior sampling methods, AGC: adversarial group calibration
    (Zhao et al., [2020](#bib.bib93))'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写：PL：参数似然，VI-BNN：基于变分推断的贝叶斯神经网络，GP：高斯过程，VI-ADF：假设密度过滤贝叶斯神经网络（期望传播的一种变体），MCMC：基于马尔科夫链蒙特卡罗的后验采样方法，AGC：对抗组校准（Zhao
    et al., [2020](#bib.bib93)）
- en: '| UE toolkit | Integration with DL frameworks | Software quality |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| UE工具包 | 与深度学习框架集成 | 软件质量 |'
- en: '| --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| TFP | • part of tensorflow ecosystem • high integration with keras model
    API | • high code quality • continuous testing • well-documented |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| TFP | • TensorFlow生态系统的一部分 • 与keras模型API高集成 | • 代码质量高 • 持续测试 • 文档详尽 |'
- en: '| Pyro | • pytorch-based implementation • models/inference procedures can be
    encapsulated as torch modules | • high code quality • continuous testing • well-documented
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Pyro | • 基于pytorch的实现 • 模型/推断过程可以封装为torch模块 | • 代码质量高 • 持续测试 • 文档详尽 |'
- en: '| UQ360 | • pytorch-based implementation • custom pytorch models can be passed
    to some modules | • simple, easy-to-use sklearn-esque API • well-documented, provides
    user guidance on uncertainty methods/metrics |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| UQ360 | • 基于pytorch的实现 • 可以将自定义pytorch模型传递给某些模块 | • 简单、易用的类似sklearn的API •
    文档详尽，提供关于不确定性方法/指标的用户指导 |'
