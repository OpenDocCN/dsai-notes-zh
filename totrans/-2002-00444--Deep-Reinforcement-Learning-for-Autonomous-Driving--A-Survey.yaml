- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:02:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2002.00444] Deep Reinforcement Learning for Autonomous Driving: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2002.00444] 深度强化学习在自动驾驶中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2002.00444](https://ar5iv.labs.arxiv.org/html/2002.00444)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2002.00444](https://ar5iv.labs.arxiv.org/html/2002.00444)
- en: Deep Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: 'for Autonomous Driving: A Survey'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶：综述
- en: B Ravi Kiran¹, Ibrahim Sobh², Victor Talpaert³, Patrick Mannion⁴,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: B Ravi Kiran¹, Ibrahim Sobh², Victor Talpaert³, Patrick Mannion⁴,
- en: Ahmad A. Al Sallab², Senthil Yogamani⁵, Patrick Pérez⁶ ¹Navya, Paris. ✉ ravi.kiran@navya.tech²Valeo
    Cairo AI team, Egypt. ✉ ibrahim.sobh, ahmad.el-sallab@{valeo.com}³U2IS, ENSTA
    Paris, Institut Polytechnique de Paris & AKKA Technologies, France. ✉  victor.talpaert@ensta.fr⁴School
    of Computer Science, National University of Ireland, Galway. ✉ patrick.mannion@nuigalway.ie⁵Valeo
    Vision Systems. ✉  senthil.yogamani@valeo.com⁶Valeo.ai. ✉  patrick.perez@valeo.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ahmad A. Al Sallab², Senthil Yogamani⁵, Patrick Pérez⁶ ¹Navya, 巴黎。 ✉ ravi.kiran@navya.tech²Valeo
    开罗 AI 团队，埃及。 ✉ ibrahim.sobh, ahmad.el-sallab@{valeo.com}³U2IS, ENSTA 巴黎，巴黎综合理工学院
    & AKKA Technologies，法国。 ✉  victor.talpaert@ensta.fr⁴爱尔兰国立大学计算机学院，戈尔韦。 ✉ patrick.mannion@nuigalway.ie⁵Valeo
    Vision Systems。 ✉  senthil.yogamani@valeo.com⁶Valeo.ai。 ✉  patrick.perez@valeo.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the development of deep representation learning, the domain of reinforcement
    learning (RL) has become a powerful learning framework now capable of learning
    complex policies in high dimensional environments. This review summarises deep
    reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving
    tasks where (D)RL methods have been employed, while addressing key computational
    challenges in real world deployment of autonomous driving agents. It also delineates
    adjacent domains such as behavior cloning, imitation learning, inverse reinforcement
    learning that are related but are not classical RL algorithms. The role of simulators
    in training agents, methods to validate, test and robustify existing solutions
    in RL are discussed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度表示学习的发展，强化学习（RL）领域已成为一个强大的学习框架，现在能够在高维环境中学习复杂的策略。本文综述了深度强化学习（DRL）算法，并提供了自动驾驶任务的分类，其中应用了（D）RL
    方法，同时解决了自主驾驶代理在现实世界中部署的关键计算挑战。还描述了相关的领域，如行为克隆、模仿学习、逆向强化学习，这些领域相关但不是经典的 RL 算法。讨论了模拟器在训练代理中的作用、验证、测试和增强现有
    RL 解决方案的方法。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep reinforcement learning, Autonomous driving, Imitation learning, Inverse
    reinforcement learning, Controller learning, Trajectory optimisation, Motion planning,
    Safe reinforcement learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习，自动驾驶，模仿学习，逆向强化学习，控制器学习，轨迹优化，运动规划，安全强化学习。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Autonomous driving (AD)¹¹1For easy reference, the main acronyms used in this
    article are listed in Appendix (Table [IV](#S7.T4 "TABLE IV ‣ VII Conclusion ‣
    Deep Reinforcement Learning for Autonomous Driving: A Survey")). systems constitute
    of multiple perception level tasks that have now achieved high precision on account
    of deep learning architectures. Besides the perception, autonomous driving systems
    constitute of multiple tasks where classical supervised learning methods are no
    more applicable. First, when the prediction of the agent’s action changes future
    sensor observations received from the environment under which the autonomous driving
    agent operates, for example the task of optimal driving speed in an urban area.
    Second, supervisory signals such as time to collision (TTC), lateral error w.r.t
    to optimal trajectory of the agent, represent the dynamics of the agent, as well
    uncertainty in the environment. Such problems would require defining the stochastic
    cost function to be maximized. Third, the agent is required to learn new configurations
    of the environment, as well as to predict an optimal decision at each instant
    while driving in its environment. This represents a high dimensional space given
    the number of unique configurations under which the agent & environment are observed,
    this is combinatorially large. In all such scenarios we are aiming to solve a
    sequential decision process, which is formalized under the classical settings
    of Reinforcement Learning (RL), where the agent is required to learn and represent
    its environment as well as act optimally given at each instant [[1](#bib.bib1)].
    The optimal action is referred to as the policy.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '自动驾驶（AD）¹¹1为了便于参考，本文中使用的主要缩略语列在附录（表格[IV](#S7.T4 "TABLE IV ‣ VII Conclusion
    ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey")）。系统包含多个感知层级任务，这些任务由于深度学习架构现在已达到高精度。除了感知，自动驾驶系统还包括多个任务，其中经典的监督学习方法已不再适用。首先，当代理的行为预测改变了从环境中接收的未来传感器观测，例如在城市区域的最佳驾驶速度任务。其次，像碰撞时间（TTC）、与代理的最佳轨迹的横向误差这样的监督信号，代表了代理的动态以及环境的不确定性。这类问题需要定义需要最大化的随机成本函数。第三，代理需要学习环境的新配置，并在每个时刻预测最佳决策。这表示了一个高维空间，考虑到代理和环境被观察的独特配置数量，这在组合上是庞大的。在所有这些场景中，我们旨在解决一个序列决策过程，这在经典的强化学习（RL）设置下被形式化，其中代理需要学习和表示其环境，并在每个时刻做出最优行动[[1](#bib.bib1)]。最优行动被称为策略。'
- en: 'In this review we cover the notions of reinforcement learning, the taxonomy
    of tasks where RL is a promising solution especially in the domains of driving
    policy, predictive perception, path and motion planning, and low level controller
    design. We also focus our review on the different real world deployments of RL
    in the domain of autonomous driving expanding our conference paper [[2](#bib.bib2)]
    since their deployment has not been reviewed in an academic setting. Finally,
    we motivate users by demonstrating the key computational challenges and risks
    when applying current day RL algorithms such imitation learning, deep Q learning,
    among others. We also note from the trends of publications in figure [2](#S2.F2
    "Figure 2 ‣ II-C Planning and Driving policy ‣ II Components of AD System ‣ Deep
    Reinforcement Learning for Autonomous Driving: A Survey") that the use of RL or
    Deep RL applied to autonomous driving or the self driving domain is an emergent
    field. This is due to the recent usage of RL/DRL algorithms domain, leaving open
    multiple real world challenges in implementation and deployment. We address the
    open problems in [VI](#S6 "VI Real world challenges and future perspectives ‣
    Deep Reinforcement Learning for Autonomous Driving: A Survey").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '在这次回顾中，我们涵盖了强化学习的概念、任务分类，其中RL尤其在驾驶策略、预测感知、路径和运动规划以及低级控制器设计领域是一个有前途的解决方案。我们还将我们的回顾重点放在RL在自动驾驶领域的不同实际应用上，扩展了我们的会议论文[[2](#bib.bib2)]，因为这些应用尚未在学术环境中进行过评审。最后，我们通过展示当前RL算法（如模仿学习、深度Q学习等）在应用时的关键计算挑战和风险来激励用户。我们还从图[2](#S2.F2
    "Figure 2 ‣ II-C Planning and Driving policy ‣ II Components of AD System ‣ Deep
    Reinforcement Learning for Autonomous Driving: A Survey")的出版趋势中注意到，将RL或深度RL应用于自动驾驶或自驾领域是一个新兴领域。这是由于RL/DRL算法领域的近期使用，留下了在实施和部署中的多个实际挑战。我们在[VI](#S6
    "VI Real world challenges and future perspectives ‣ Deep Reinforcement Learning
    for Autonomous Driving: A Survey")中讨论了这些未解的问题。'
- en: 'The main contributions of this work can be summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献可总结如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Self-contained overview of RL background for the automotive community as it
    is not well known.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这篇文章为汽车行业提供了一个自包含的强化学习（RL）背景概述，因为它并不为人所熟知。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detailed literature review of using RL for different autonomous driving tasks.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对使用RL进行不同自动驾驶任务的详细文献综述。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussion of the key challenges and opportunities for RL applied to real world
    autonomous driving.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探讨了应用于现实世界自动驾驶的RL面临的关键挑战和机遇。
- en: 'The rest of the paper is organized as follows. Section [II](#S2 "II Components
    of AD System ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey")
    provides an overview of components of a typical autonomous driving system. Section
    [III](#S3 "III Reinforcement learning ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") provides an introduction to reinforcement learning and briefly
    discusses key concepts. Section [IV](#S4 "IV Extensions to reinforcement learning
    ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey") discusses more
    sophisticated extensions on top of the basic RL framework. Section [V](#S5 "V
    Reinforcement learning for Autonomous driving tasks ‣ Deep Reinforcement Learning
    for Autonomous Driving: A Survey") provides an overview of RL applications for
    autonomous driving problems. Section [VI](#S6 "VI Real world challenges and future
    perspectives ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey")
    discusses challenges in deploying RL for real-world autonomous driving systems.
    Section [VII](#S7 "VII Conclusion ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") concludes this paper with some final remarks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分安排如下。第 [II](#S2 "II AD系统的组成 ‣ 自动驾驶的深度强化学习：一项调研") 节概述了典型自动驾驶系统的组成部分。第
    [III](#S3 "III 强化学习 ‣ 自动驾驶的深度强化学习：一项调研") 节介绍了强化学习并简要讨论了关键概念。第 [IV](#S4 "IV 强化学习的扩展
    ‣ 自动驾驶的深度强化学习：一项调研") 节讨论了基本RL框架之上的更复杂的扩展。第 [V](#S5 "V 自动驾驶任务的强化学习 ‣ 自动驾驶的深度强化学习：一项调研")
    节概述了应用于自动驾驶问题的RL应用。第 [VI](#S6 "VI 现实世界的挑战和未来展望 ‣ 自动驾驶的深度强化学习：一项调研") 节讨论了在实际自动驾驶系统中应用RL的挑战。第
    [VII](#S7 "VII 结论 ‣ 自动驾驶的深度强化学习：一项调研") 节以一些最终的评论来结束本文。
- en: II Components of AD System
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II AD系统的组成
- en: '![Refer to caption](img/67d9b060bc399d08ce4fe622a04b9496.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/67d9b060bc399d08ce4fe622a04b9496.png)'
- en: 'Figure 1: Standard components in a modern autonomous driving systems pipeline
    listing the various tasks. The key problems addressed by these modules are Scene
    Understanding, Decision and Planning.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: 现代自动驾驶系统管道中的标准组件列表，列出了各种任务。这些模块解决的关键问题包括场景理解、决策和规划。'
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ II Components of AD System ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey") comprises of the standard blocks of
    an AD system demonstrating the pipeline from sensor stream to control actuation.
    The sensor architecture in a modern autonomous driving system notably includes
    multiple sets of cameras, radars and LIDARs as well as a GPS-GNSS system for absolute
    localisation and inertial measurement Units (IMUs) that provide 3D pose of the
    vehicle in space.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S2.F1 "图1 ‣ II AD系统的组成 ‣ 自动驾驶的深度强化学习：一项调研") 包括了AD系统的标准模块，展示了从传感器流到控制执行的管道。现代自动驾驶系统的传感器架构特别包括多组摄像机、雷达和激光雷达，以及用于绝对定位的GPS-GNSS系统和提供车辆在空间中的3D姿态的惯性测量单元（IMUs）。
- en: The goal of the perception module is the creation of an intermediate level representation
    of the environment state (for example bird-eye view map of all obstacles and agents)
    that is to be later utilised by a decision making system that ultimately produces
    the driving policy. This state would include lane position, drivable zone, location
    of agents such as cars & pedestrians, state of traffic lights and others. Uncertainties
    in the perception propagate to the rest of the information chain. Robust sensing
    is critical for safety thus using redundant sources increases confidence in detection.
    This is achieved by a combination of several perception tasks like semantic segmentation
    [[3](#bib.bib3), [4](#bib.bib4)], motion estimation [[5](#bib.bib5)], depth estimation
    [[6](#bib.bib6)], soiling detection [[7](#bib.bib7)], etc which can be efficiently
    unified into a multi-task model [[8](#bib.bib8), [9](#bib.bib9)].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 感知模块的目标是创建环境状态的中间级表示（例如所有障碍物和代理的鸟瞰图），这一表示将被后续的决策制定系统利用，最终生成驾驶策略。这一状态包括车道位置、可驾驶区域、代理（如汽车和行人）的位置、交通信号灯状态等。感知中的不确定性会传播到信息链的其余部分。鲁棒的传感对于安全至关重要，因此使用冗余源可以提高检测的信心。这通过结合多种感知任务，如语义分割[[3](#bib.bib3),
    [4](#bib.bib4)]、运动估计[[5](#bib.bib5)]、深度估计[[6](#bib.bib6)]、污垢检测[[7](#bib.bib7)]等，可以有效地统一为一个多任务模型[[8](#bib.bib8),
    [9](#bib.bib9)]来实现。
- en: II-A Scene Understanding
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 场景理解
- en: 'This key module maps the abstract mid-level representation of the perception
    state obtained from the perception module to the high level action or decision
    making module. Conceptually, three tasks are grouped by this module: Scene understanding,
    Decision and Planning as seen in figure [1](#S2.F1 "Figure 1 ‣ II Components of
    AD System ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey") module
    aims to provide a higher level understanding of the scene, it is built on top
    of the algorithmic tasks of detection or localisation. By fusing heterogeneous
    sensor sources, it aims to robustly generalise to situations as the content becomes
    more abstract. This information fusion provides a general and simplified context
    for the Decision making components.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关键模块将从感知模块获得的抽象中级表示映射到高级动作或决策制定模块。从概念上讲，这个模块包含三个任务：场景理解、决策和规划，如图[1](#S2.F1
    "图 1 ‣ 自动驾驶系统的 II 组件 ‣ 自动驾驶深度强化学习：综述")所示。该模块旨在提供更高级别的场景理解，建立在检测或定位的算法任务之上。通过融合异质传感器源，旨在对情况进行鲁棒性泛化，因为内容变得更为抽象。这种信息融合为决策制定组件提供了通用且简化的背景。
- en: Fusion provides a sensor agnostic representation of the environment and models
    the sensor noise and detection uncertainties across multiple modalities such as
    LIDAR, camera, radar, ultra-sound. This basically requires weighting the predictions
    in a principled way.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 融合提供了对环境的传感器无关表示，并对多种模式下的传感器噪声和检测不确定性进行建模，如LIDAR、摄像头、雷达、超声波。这基本上需要以原则化的方式对预测结果进行加权。
- en: II-B Localization and Mapping
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 定位与映射
- en: Mapping is one of the key pillars of automated driving [[10](#bib.bib10)]. Once
    an area is mapped, current position of the vehicle can be localized within the
    map. The first reliable demonstrations of automated driving by Google were primarily
    reliant on localisation to pre-mapped areas. Because of the scale of the problem,
    traditional mapping techniques are augmented by semantic object detection for
    reliable disambiguation. In addition, localised high definition maps (HD maps)
    can be used as a prior for object detection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 映射是自动驾驶的关键支柱之一[[10](#bib.bib10)]。一旦区域被映射，车辆的当前位置可以在地图中定位。谷歌首次可靠的自动驾驶演示主要依赖于对预映射区域的定位。由于问题的规模，传统的映射技术通过语义对象检测来增强可靠性，以避免歧义。此外，本地化的高清地图（HD
    maps）可以用作对象检测的先验。
- en: II-C Planning and Driving policy
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 规划与驾驶策略
- en: Trajectory planning is a crucial module in the autonomous driving pipeline.
    Given a route-level plan from HD maps or GPS based maps, this module is required
    to generate motion-level commands that steer the agent.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹规划是自动驾驶流程中的关键模块。根据来自HD地图或基于GPS的地图的路线级计划，该模块需要生成控制代理的运动级命令。
- en: Classical motion planning ignores dynamics and differential constraints while
    using translations and rotations required to move an agent from source to destination
    poses [[11](#bib.bib11)]. A robotic agent capable of controlling 6-degrees of
    freedom (DOF) is said to be holonomic, while an agent with fewer controllable
    DOFs than its total DOF is said to be non-holonomic. Classical algorithms such
    as $A^{\ast}$ algorithm based on Djisktra’s algorithm do not work in the non-holonomic
    case for autonomous driving. Rapidly-exploring random trees (RRT) [[12](#bib.bib12)]
    are non-holonomic algorithms that explore the configuration space by random sampling
    and obstacle free path generation. There are various versions of RRT currently
    used in for motion planning in autonomous driving pipelines.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 经典运动规划忽略了动力学和微分约束，同时使用了将代理从源点移动到目标点所需的平移和旋转 [[11](#bib.bib11)]。能够控制6自由度（DOF）的机器人代理被称为全自性，而具有比其总自由度更少的可控自由度的代理被称为非全自性。经典算法如基于Dijkstra算法的$A^{\ast}$算法在自主驾驶的非全自性情况下无法工作。快速扩展随机树（RRT）[[12](#bib.bib12)]是通过随机采样和障碍物自由路径生成来探索配置空间的非全自性算法。目前有多种版本的RRT用于自主驾驶管道中的运动规划。
- en: '![Refer to caption](img/c25c07fbd261e798d1076b05ec89ea04.png)![Refer to caption](img/b7693fbc0ec58fc7abb401c279d89cbc.png)![Refer
    to caption](img/0b39e73bab0394b36883da8cdfa93931.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c25c07fbd261e798d1076b05ec89ea04.png)![参见标题](img/b7693fbc0ec58fc7abb401c279d89cbc.png)![参见标题](img/0b39e73bab0394b36883da8cdfa93931.png)'
- en: 'Figure 2: Trend of publications for keywords 1\. ”reinforcement learning”,
    2.”deep reinforcement”, and 3.”reinforcement learning” AND (”autonomous cars”
    OR ”autonomous vehicles” OR ”self driving”) for academic publication trends from
    this [[13](#bib.bib13)].'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：关键词1.“强化学习”、2.“深度强化学习”和3.“强化学习”与（“自动驾驶汽车”或“自动驾驶车辆”或“自动驾驶”）的学术出版趋势 [[13](#bib.bib13)]。
- en: II-D Control
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 控制
- en: A controller defines the speed, steering angle and braking actions necessary
    over every point in the path obtained from a pre-determined map such as Google
    maps, or expert driving recording of the same values at every waypoint. Trajectory
    tracking in contrast involves a temporal model of the dynamics of the vehicle
    viewing the waypoints sequentially over time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器定义了在从预定地图（如 Google 地图）获得的每个路径点上的速度、转向角和刹车动作，或在每个航路点上的相同值的专家驾驶记录。与之相比，轨迹跟踪涉及车辆动力学的时间模型，通过时间顺序查看航路点。
- en: Current vehicle control methods are founded in classical optimal control theory
    which can be stated as a minimisation of a cost function $\dot{x}=f(x(t),u(t))$
    defined over a set of states $x(t)$ and control actions $u(t)$. The control input
    is usually defined over a finite time horizon and restricted on a feasible state
    space $x\in X_{\text{free}}$ [[14](#bib.bib14)]. The velocity control are based
    on classical methods of closed loop control such as PID (proportional-integral-derivative)
    controllers, MPC (Model predictive control). PIDs aim to minimise a cost function
    constituting of three terms current error with proportional term, effect of past
    errors with integral term, and effect of future errors with the derivative term.
    While the family of MPC methods aim to stabilize the behavior of the vehicle while
    tracking the specified path [[15](#bib.bib15)]. A review on controllers, motion
    planning and learning based approaches for the same are provided in this review
    [[16](#bib.bib16)] for interested readers. Optimal control and reinforcement learning
    are intimately related, where optimal control can be viewed as a model based reinforcement
    learning problem where the dynamics of the vehicle/environment are modeled by
    well defined differential equations. Reinforcement learning methods were developed
    to handle stochastic control problems as well ill-posed problems with unknown
    rewards and state transition probabilities. Autonomous vehicle stochastic control
    is large domain, and we advise readers to read the survey on this subject by authors
    in [[17](#bib.bib17)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的车辆控制方法基于经典的最优控制理论，这可以表述为对成本函数$\dot{x}=f(x(t),u(t))$的最小化，该函数定义在一组状态$x(t)$和控制动作$u(t)$上。控制输入通常在有限的时间范围内定义，并限制在可行的状态空间$x\in
    X_{\text{free}}$ [[14](#bib.bib14)]。速度控制基于经典的闭环控制方法，如PID（比例-积分-微分）控制器、MPC（模型预测控制）。PID旨在最小化由三个部分组成的成本函数，包括当前误差的比例项、过去误差的积分项以及未来误差的微分项。而MPC方法则旨在稳定车辆行为，同时跟踪指定路径
    [[15](#bib.bib15)]。对控制器、运动规划以及基于学习的方法的综述在此综述中提供 [[16](#bib.bib16)]，供感兴趣的读者参考。最优控制和强化学习密切相关，其中最优控制可以被视为一种基于模型的强化学习问题，其中车辆/环境的动态由明确的微分方程建模。强化学习方法是为了处理随机控制问题以及具有未知奖励和状态转换概率的难题而开发的。自主车辆的随机控制是一个广泛领域，我们建议读者阅读[[17](#bib.bib17)]中的相关综述。
- en: III Reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 强化学习
- en: 'Machine learning (ML) is a process whereby a computer program learns from experience
    to improve its performance at a specified task [[18](#bib.bib18)]. ML algorithms
    are often classified under one of three broad categories: supervised learning,
    unsupervised learning and reinforcement learning (RL). Supervised learning algorithms
    are based on inductive inference where the model is typically trained using labelled
    data to perform classification or regression, whereas unsupervised learning encompasses
    techniques such as density estimation or clustering applied to unlabelled data.
    By contrast, in the RL paradigm an autonomous agent learns to improve its performance
    at an assigned task by interacting with its environment. Russel and Norvig define
    an agent as “anything that can be viewed as perceiving its environment through
    sensors and acting upon that environment through actuators” [[19](#bib.bib19)].
    RL agents are not told explicitly how to act by an expert; rather an agent’s performance
    is evaluated by a reward function $R$. For each state experienced, the agent chooses
    an action and receives an occasional reward from its environment based on the
    usefulness of its decision. The goal for the agent is to maximize the cumulative
    rewards received over its lifetime. Gradually, the agent can increase its long-term
    reward by exploiting knowledge learned about the expected utility (i.e. discounted
    sum of expected future rewards) of different state-action pairs. One of the main
    challenges in reinforcement learning is managing the trade-off between exploration
    and exploitation. To maximize the rewards it receives, an agent must exploit its
    knowledge by selecting actions which are known to result in high rewards. On the
    other hand, to discover such beneficial actions, it has to take the risk of trying
    new actions which may lead to higher rewards than the current best-valued actions
    for each system state. In other words, the learning agent has to exploit what
    it already knows in order to obtain rewards, but it also has to explore the unknown
    in order to make better action selections in the future. Examples of strategies
    which have been proposed to manage this trade-off include $\epsilon$-greedy and
    softmax. When adopting the ubiquitous $\epsilon$-greedy strategy, an agent either
    selects an action at random with probability $0<\epsilon<1$, or greedily selects
    the highest valued action for the current state with the remaining probability
    $1-\epsilon$. Intuitively, the agent should explore more at the beginning of the
    training process when little is known about the problem environment. As training
    progresses, the agent may gradually conduct more exploitation than exploration.
    The design of exploration strategies for RL agents is an area of active research
    (see e.g. [[20](#bib.bib20)]).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是一个过程，通过该过程计算机程序从经验中学习，以提高其在特定任务上的表现[[18](#bib.bib18)]。ML 算法通常被归类为三大类之一：监督学习、无监督学习和强化学习（RL）。监督学习算法基于归纳推理，其中模型通常使用标记数据进行训练，以执行分类或回归，而无监督学习包括应用于未标记数据的技术，如密度估计或聚类。相比之下，在
    RL 范式中，自主代理通过与环境交互来学习如何提高其在分配任务中的表现。Russel 和 Norvig 将代理定义为“任何可以通过传感器感知其环境，并通过执行器对环境采取行动的事物”[[19](#bib.bib19)]。RL
    代理不会被专家明确告知如何行动；相反，代理的表现通过奖励函数 $R$ 进行评估。对于每一个经历的状态，代理选择一个动作，并根据其决策的有用性从环境中获得偶尔的奖励。代理的目标是最大化其生命周期内获得的累积奖励。逐渐地，代理可以通过利用关于不同状态-动作对的期望效用（即折扣后的期望未来奖励总和）的知识来增加其长期奖励。强化学习中的主要挑战之一是管理探索与利用之间的权衡。为了最大化获得的奖励，代理必须通过选择已知会产生高奖励的动作来利用其知识。另一方面，为了发现这些有益的动作，代理必须冒险尝试新动作，这些新动作可能会为每个系统状态带来比当前最佳值更高的奖励。换句话说，学习代理必须利用已知的信息来获得奖励，但也必须探索未知领域，以便在未来做出更好的动作选择。已经提出的一些管理这种权衡的策略包括
    $\epsilon$-贪婪和软最大策略。当采用普遍的 $\epsilon$-贪婪策略时，代理会以概率 $0<\epsilon<1$ 随机选择一个动作，或者以剩余概率
    $1-\epsilon$ 贪婪地选择当前状态下价值最高的动作。直观上，代理在训练过程开始时应更多地进行探索，因为对问题环境知之甚少。随着训练的进展，代理可能会逐渐进行更多的利用而不是探索。为
    RL 代理设计探索策略是一个活跃的研究领域（见例如[[20](#bib.bib20)]）。
- en: <svg   height="265.58" overflow="visible" version="1.1" width="346.61"><g transform="translate(0,265.58)
    matrix(1 0 0 -1 0 0) translate(172.31,0) translate(0,204.78)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(0.75 0.0 0.0 0.75 -41.51 7.42)" fill="#000000"
    stroke="#000000"><foreignobject width="110.7" height="39.17" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Vehicle/RL Agent State $s_{t}\in S$ $S$ either
    continuous or discrete <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 168.48 60.52 L 58.48
    60.52 C 55.42 60.52 52.95 58.05 52.95 54.99 L 52.95 21.78 C 52.95 18.72 55.42
    16.25 58.48 16.25 L 168.48 16.25 C 171.54 16.25 174.02 18.72 174.02 21.78 L 174.02
    54.99 C 174.02 58.05 171.54 60.52 168.48 60.52 Z M 52.95 16.25" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 168.48 60.52 L 58.48 60.52 C 55.42 60.52 52.95
    58.05 52.95 54.99 L 52.95 21.78 C 52.95 18.72 55.42 16.25 58.48 16.25 L 168.48
    16.25 C 171.54 16.25 174.02 18.72 174.02 21.78 L 174.02 54.99 C 174.02 58.05 171.54
    60.52 168.48 60.52 Z M 52.95 16.25" style="stroke:none"></path></g><g fill="#99CCCC"><path
    d="M 168.48 60.52 L 58.48 60.52 C 55.42 60.52 52.95 58.05 52.95 54.99 L 52.95
    21.78 C 52.95 18.72 55.42 16.25 58.48 16.25 L 168.48 16.25 C 171.54 16.25 174.02
    18.72 174.02 21.78 L 174.02 54.99 C 174.02 58.05 171.54 60.52 168.48 60.52 Z M
    52.95 16.25"></path></g><g transform="matrix(0.75 0.0 0.0 0.75 56.4 49.8)" fill="#000000"
    stroke="#000000"><foreignobject width="152.21" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Challenges Sample Complexity, Validation, Safe-exploration,
    Credit assignment, Simulator-Real Gap, Learning</foreignobject></g> <g stroke-opacity="0.5"
    fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path
    d="M -82.1 51.7 L -172.04 51.7 L -172.04 25.07 L -82.1 25.07 Z M -172.04 25.07"
    style="stroke:none"></path></g><g fill="#FFCCCC"><path d="M -82.1 51.7 L -172.04
    51.7 L -172.04 25.07 L -82.1 25.07 Z M -172.04 25.07"></path></g><g transform="matrix(0.75
    0.0 0.0 0.75 -168.58 40.98)" fill="#000000" stroke="#000000"><foreignobject width="110.7"
    height="26.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Function
    Approximators $o_{t}\to s_{t}$ (SRL) CNNs</foreignobject></g> <g stroke-opacity="0.5"
    fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path
    d="M -82.1 19.54 L -172.04 19.54 L -172.04 -19.54 L -82.1 -19.54 Z M -172.04 -19.54"
    style="stroke:none"></path></g><g fill="#FFCCCC"><path d="M -82.1 19.54 L -172.04
    19.54 L -172.04 -19.54 L -82.1 -19.54 Z M -172.04 -19.54"></path></g><g transform="matrix(0.75
    0.0 0.0 0.75 -168.58 8.82)" fill="#000000" stroke="#000000"><foreignobject width="110.7"
    height="42.89" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RL methods
    Value/Policy-based Actor-Critic On/Off policy Model-based/Model-free</foreignobject></g>
    <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0
    0.0 0.0 1.0 2.98 -2.98)"><path d="M 152.3 -10.02 L 91.59 -10.02 C 88.54 -10.02
    86.06 -12.5 86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54 -41.16 91.59 -41.16
    L 152.3 -41.16 C 155.36 -41.16 157.84 -38.68 157.84 -35.62 L 157.84 -15.56 C 157.84
    -12.5 155.36 -10.02 152.3 -10.02 Z M 86.06 -41.16" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 152.3 -10.02 L 91.59 -10.02 C 88.54 -10.02 86.06
    -12.5 86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54 -41.16 91.59 -41.16 L 152.3
    -41.16 C 155.36 -41.16 157.84 -38.68 157.84 -35.62 L 157.84 -15.56 C 157.84 -12.5
    155.36 -10.02 152.3 -10.02 Z M 86.06 -41.16" style="stroke:none"></path></g><g
    fill="#CCFFCC"><path d="M 152.3 -10.02 L 91.59 -10.02 C 88.54 -10.02 86.06 -12.5
    86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54 -41.16 91.59 -41.16 L 152.3 -41.16
    C 155.36 -41.16 157.84 -38.68 157.84 -35.62 L 157.84 -15.56 C 157.84 -12.5 155.36
    -10.02 152.3 -10.02 Z M 86.06 -41.16"></path></g><g transform="matrix(0.75 0.0
    0.0 0.75 89.52 -24.03)" fill="#000000" stroke="#000000"><foreignobject width="86.48"
    height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\pi:S\to
    A$ Driving Policy stochastic/deterministic</foreignobject></g> <g stroke-dasharray="1.2pt,2.0pt"
    stroke-dashoffset="0.0pt" stroke-width="1.2pt"><path d="M -81.89 38.39 L -45.18
    16.44" style="fill:none"></path></g><g stroke-dasharray="1.2pt,2.0pt" stroke-dashoffset="0.0pt"
    stroke-width="1.2pt"><path d="M -81.89 0 L -45.18 0" style="fill:none"></path></g><g
    stroke-dasharray="1.2pt,2.0pt" stroke-dashoffset="0.0pt" stroke-width="1.2pt"><path
    d="M 45.18 0 L 52.74 38.39" style="fill:none"></path></g><g stroke="#5A5A5A" fill="#5A5A5A"
    color="#5A5A5A"><g stroke-width="1.2pt"><path d="M 45.18 0 C 73.64 -10.36 137.09
    16.42 122.61 -8.67" style="fill:none"></path></g><g transform="matrix(-0.5 -0.86603
    0.86603 -0.5 122.61 -8.67)" stroke-dasharray="none" stroke-dashoffset="0.0pt"
    stroke-linecap="round" stroke-linejoin="round" stroke-width="0.96pt"><path d="M
    -2.66 3.54 C -2.44 2.21 0 0.22 0.66 0 C 0 -0.22 -2.44 -2.21 -2.66 -3.54" style="fill:none"></path></g></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 60.19 -73 L -60.19 -73 C -63.25 -73 -65.73 -75.48
    -65.73 -78.53 L -65.73 -129.73 C -65.73 -132.79 -63.25 -135.26 -60.19 -135.26
    L 60.19 -135.26 C 63.25 -135.26 65.73 -132.79 65.73 -129.73 L 65.73 -78.53 C 65.73
    -75.48 63.25 -73 60.19 -73 Z M -65.73 -135.26" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 60.19 -73 L -60.19 -73 C -63.25 -73 -65.73 -75.48
    -65.73 -78.53 L -65.73 -129.73 C -65.73 -132.79 -63.25 -135.26 -60.19 -135.26
    L 60.19 -135.26 C 63.25 -135.26 65.73 -132.79 65.73 -129.73 L 65.73 -78.53 C 65.73
    -75.48 63.25 -73 60.19 -73 Z M -65.73 -135.26" style="stroke:none"></path></g><g
    fill="#D9D9D9"><path d="M 60.19 -73 L -60.19 -73 C -63.25 -73 -65.73 -75.48 -65.73
    -78.53 L -65.73 -129.73 C -65.73 -132.79 -63.25 -135.26 -60.19 -135.26 L 60.19
    -135.26 C 63.25 -135.26 65.73 -132.79 65.73 -129.73 L 65.73 -78.53 C 65.73 -75.48
    63.25 -73 60.19 -73 Z M -65.73 -135.26"></path></g><g transform="matrix(0.75 0.0
    0.0 0.75 -62.27 -107.76)" fill="#000000" stroke="#000000"><foreignobject width="166.04"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Environment</foreignobject></g>
    <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0
    0.0 0.0 1.0 2.98 -2.98)"><path d="M -42.6 -101.71 L -100.71 -101.71 C -103.77
    -101.71 -106.25 -104.19 -106.25 -107.25 L -106.25 -119.53 C -106.25 -122.58 -103.77
    -125.06 -100.71 -125.06 L -42.6 -125.06 C -39.54 -125.06 -37.06 -122.58 -37.06
    -119.53 L -37.06 -107.25 C -37.06 -104.19 -39.54 -101.71 -42.6 -101.71 Z M -106.25
    -125.06" style="stroke:none"></path></g><g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -42.6
    -101.71 L -100.71 -101.71 C -103.77 -101.71 -106.25 -104.19 -106.25 -107.25 L
    -106.25 -119.53 C -106.25 -122.58 -103.77 -125.06 -100.71 -125.06 L -42.6 -125.06
    C -39.54 -125.06 -37.06 -122.58 -37.06 -119.53 L -37.06 -107.25 C -37.06 -104.19
    -39.54 -101.71 -42.6 -101.71 Z M -106.25 -125.06" style="stroke:none"></path></g><g
    fill="#FFFFB3"><path d="M -42.6 -101.71 L -100.71 -101.71 C -103.77 -101.71 -106.25
    -104.19 -106.25 -107.25 L -106.25 -119.53 C -106.25 -122.58 -103.77 -125.06 -100.71
    -125.06 L -42.6 -125.06 C -39.54 -125.06 -37.06 -122.58 -37.06 -119.53 L -37.06
    -107.25 C -37.06 -104.19 -39.54 -101.71 -42.6 -101.71 Z M -106.25 -125.06"></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 -102.79 -117.02)" fill="#000000" stroke="#000000"><foreignobject
    width="83.02" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Real-world</foreignobject></g>
    <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0
    0.0 0.0 1.0 2.98 -2.98)"><path d="M 100.71 -101.71 L 42.6 -101.71 C 39.54 -101.71
    37.06 -104.19 37.06 -107.25 L 37.06 -119.53 C 37.06 -122.58 39.54 -125.06 42.6
    -125.06 L 100.71 -125.06 C 103.77 -125.06 106.25 -122.58 106.25 -119.53 L 106.25
    -107.25 C 106.25 -104.19 103.77 -101.71 100.71 -101.71 Z M 37.06 -125.06" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 100.71 -101.71 L 42.6 -101.71 C 39.54 -101.71
    37.06 -104.19 37.06 -107.25 L 37.06 -119.53 C 37.06 -122.58 39.54 -125.06 42.6
    -125.06 L 100.71 -125.06 C 103.77 -125.06 106.25 -122.58 106.25 -119.53 L 106.25
    -107.25 C 106.25 -104.19 103.77 -101.71 100.71 -101.71 Z M 37.06 -125.06" style="stroke:none"></path></g><g
    fill="#FFFFB3"><path d="M 100.71 -101.71 L 42.6 -101.71 C 39.54 -101.71 37.06
    -104.19 37.06 -107.25 L 37.06 -119.53 C 37.06 -122.58 39.54 -125.06 42.6 -125.06
    L 100.71 -125.06 C 103.77 -125.06 106.25 -122.58 106.25 -119.53 L 106.25 -107.25
    C 106.25 -104.19 103.77 -101.71 100.71 -101.71 Z M 37.06 -125.06"></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 40.52 -117.02)" fill="#000000" stroke="#000000"><foreignobject
    width="83.02" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Simulator</foreignobject></g>
    <g stroke-width="1.2pt" fill="#4D4D4D" stroke="#4D4D4D" stroke-dasharray="1.2pt,2.0pt"
    stroke-dashoffset="0.0pt" color="#4D4D4D"><path d="M 71.65 -125.27 C 71.65 -181.15
    -71.65 -181.15 -71.65 -126.6" style="fill:none"><g transform="matrix(0.0 1.0 -1.0
    0.0 -71.65 -126.6)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round"
    stroke-linejoin="round" stroke-width="0.96pt"><path d="M -2.66 3.54 C -2.44 2.21
    0 0.22 0.66 0 C 0 -0.22 -2.44 -2.21 -2.66 -3.54" style="fill:none"></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 -51.67 -199.36)" fill="#4D4D4D" stroke="#4D4D4D"><foreignobject
    width="137.8" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">simulator-to-real
    gap</foreignobject></g></path></g> <g stroke-width="1.2pt"><g stroke="#FF0000"
    fill="#FF0000" color="#FF0000"><path d="M 121.95 -41.36 C 121.95 -66.43 65.93
    -47.73 65.93 -71.46" style="fill:none"><g transform="matrix(0.0 -1.0 1.0 0.0 65.93
    -71.46)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round"
    stroke-linejoin="round" stroke-width="0.96pt"><path d="M -2.66 3.54 C -2.44 2.21
    0 0.22 0.66 0 C 0 -0.22 -2.44 -2.21 -2.66 -3.54" style="fill:none"></path></g></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 28.98 -35.35)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="173.23" height="33.21" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">action $a_{t}$ discrete/continuous</foreignobject></g></g>
    <g stroke-width="1.2pt"><g stroke="#0000FF" fill="#0000FF" color="#0000FF"><path
    d="M 0 -72.79 C 0 -52.58 0 -41.17 0 -22.29" style="fill:none"><g transform="matrix(0.0
    1.0 -1.0 0.0 0 -22.29)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round"
    stroke-linejoin="round" stroke-width="0.96pt"><path d="M -2.66 3.54 C -2.44 2.21
    0 0.22 0.66 0 C 0 -0.22 -2.44 -2.21 -2.66 -3.54" style="fill:none"></path></g></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 -66.44 -43.42)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="98.43" height="28.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">reward $r_{t}$ observation $o_{t}$</foreignobject></g></g>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg   height="265.58" overflow="visible" version="1.1" width="346.61"><g
    transform="translate(0,265.58) matrix(1 0 0 -1 0 0) translate(172.31,0) translate(0,204.78)"
    fill="#000000" stroke="#000000"><g stroke-width="0.4pt"><g transform="matrix(0.75
    0.0 0.0 0.75 -41.51 7.42)" fill="#000000" stroke="#000000"><foreignobject width="110.7"
    height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">车辆/RL 代理状态
    $s_{t}\in S$ $S$ 可以是连续的或离散的 <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 168.48 60.52 L 58.48
    60.52 C 55.42 60.52 52.95 58.05 52.95 54.99 L 52.95 21.78 C 52.95 18.72 55.42
    16.25 58.48 16.25 L 168.48 16.25 C 171.54 16.25 174.02 18.72 174.02 21.78 L 174.02
    54.99 C 174.02 58.05 171.54 60.52 168.48 60.52 Z M 52.95 16.25" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 168.48 60.52 L 58.48 60.52 C 55.42 60.52 52.95
    58.05 52.95 54.99 L 52.95 21.78 C 52.95 18.72 55.42 16.25 58.48 16.25 L 168.48
    16.25 C 171.54 16.25 174.02 18.72 174.02 21.78 L 174.02 54.99 C 174.02 58.05 171.54
    60.52 168.48 60.52 Z M 52.95 16.25" style="stroke:none"></path></g><g fill="#99CCCC"><path
    d="M 168.48 60.52 L 58.48 60.52 C 55.42 60.52 52.95 58.05 52.95 54.99 L 52.95
    21.78 C 52.95 18.72 55.42 16.25 58.48 16.25 L 168.48 16.25 C 171.54 16.25 174.02
    18.72 174.02 21.78 L 174.02 54.99 C 174.02 58.05 171.54 60.52 168.48 60.52 Z M
    52.95 16.25"></path></g><g transform="matrix(0.75 0.0 0.0 0.75 56.4 49.8)" fill="#000000"
    stroke="#000000"><foreignobject width="152.21" height="49.81" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">挑战样本复杂度、验证、安全探索、信用分配、模拟器-现实差距、学习</foreignobject></g>
    <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0
    0.0 0.0 1.0 2.98 -2.98)"><path d="M -82.1 51.7 L -172.04 51.7 L -172.04 25.07
    L -82.1 25.07 Z M -172.04 25.07" style="stroke:none"></path></g><g fill="#FFCCCC"><path
    d="M -82.1 51.7 L -172.04 51.7 L -172.04 25.07 L -82.1 25.07 Z M -172.04 25.07"></path></g><g
    transform="matrix(0.75 0.0 0.0 0.75 -168.58 40.98)" fill="#000000" stroke="#000000"><foreignobject
    width="110.7" height="26.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">函数逼近器
    $o_{t}\to s_{t}$ (SRL) CNNs</foreignobject></g> <g stroke-opacity="0.5" fill="#808080"
    fill-opacity="0.5" transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M -82.1
    19.54 L -172.04 19.54 L -172.04 -19.54 L -82.1 -19.54 Z M -172.04 -19.54" style="stroke:none"></path></g><g
    fill="#FFCCCC"><path d="M -82.1 19.54 L -172.04 19.54 L -172.04 -19.54 L -82.1
    -19.54 Z M -172.04 -19.54"></path></g><g transform="matrix(0.75 0.0 0.0 0.75 -168.58
    8.82)" fill="#000000" stroke="#000000"><foreignobject width="110.7" height="42.89"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RL 方法值/策略基础的演员-评论家 开/关策略
    基于模型/无模型</foreignobject></g> <g stroke-opacity="0.5" fill="#808080" fill-opacity="0.5"
    transform="matrix(1.0 0.0 0.0 1.0 2.98 -2.98)"><path d="M 152.3 -10.02 L 91.59
    -10.02 C 88.54 -10.02 86.06 -12.5 86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54
    -41.16 91.59 -41.16 L 152.3 -41.16 C 155.36 -41.16 157.84 -38.68 157.84 -35.62
    L 157.84 -15.56 C 157.84 -12.5 155.36 -10.02 152.3 -10.02 Z M 86.06 -41.16" style="stroke:none"></path></g><g
    stroke-opacity="0.5" fill="#808080" fill-opacity="0.5" transform="matrix(1.0 0.0
    0.0 1.0 2.98 -2.98)"><path d="M 152.3 -10.02 L 91.59 -10.02 C 88.54 -10.02 86.06
    -12.5 86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54 -41.16 91.59 -41.16 L 152.3
    -41.16 C 155.36 -41.16 157.84 -38.68 157.84 -35.62 L 157.84 -15.56 C 157.84 -12.5
    155.36 -10.02 152.3 -10.02 Z M 86.06 -41.16" style="stroke:none"></path></g><g
    fill="#CCFFCC"><path d="M 152.3 -10.02 L 91.59 -10.02 C 88.54 -10.02 86.06 -12.5
    86.06 -15.56 L 86.06 -35.62 C 86.06 -38.68 88.54 -41.16 91.59 -41.16 L 152.3 -41.16
    C 155.36 -41.16 157.84 -38.68 157.84 -35.62 L 157.84 -15.56 C 157.84 -12.5 155.36
    -10.02 152.3 -10.02 Z M 86.06 -41.16"></path></g><g transform="matrix(0.'
- en: 'Figure 3: A graphical decomposition of the different components of an RL algorithm.
    It also demonstrates the different challenges encountered while training a D(RL)
    algorithm.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：一个 RL 算法的不同组成部分的图形分解。它还展示了训练 D(RL) 算法时遇到的不同挑战。
- en: 'Markov decision processes (MDPs) are considered the de facto standard when
    formalising sequential decision making problems involving a single RL agent [[21](#bib.bib21)].
    An MDP consists of a set $S$ of states, a set $A$ of actions, a transition function
    $T$ and a reward function $R$ [[22](#bib.bib22)], i.e. a tuple ${<S,A,T,R>}$.
    When in any state ${s\in S}$, selecting an action ${a\in A}$ will result in the
    environment entering a new state ${s^{\prime}\in S}$ with a transition probability
    ${T(s,a,s^{\prime})\in(0,1)}$, and give a reward ${R(s,a)}$. This process is illustrated
    in Fig. [3](#S3.F3 "Figure 3 ‣ III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey"). The stochastic policy $\pi:S\to\mathcal{D}$
    is a mapping from the state space to a probability over the set of actions, and
    $\pi(a|s)$ represents the probability of choosing action $a$ at state $s$. The
    goal is to find the optimal policy $\pi^{*}$, which results in the highest expected
    sum of discounted rewards [[21](#bib.bib21)]:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '在形式化涉及单个 RL 代理的顺序决策问题时，马尔可夫决策过程（MDP）被认为是事实上的标准 [[21](#bib.bib21)]。一个 MDP 包含一组状态
    $S$，一组动作 $A$，一个转移函数 $T$ 以及一个奖励函数 $R$ [[22](#bib.bib22)]，即一个元组 ${<S,A,T,R>}$。当处于任何状态
    ${s\in S}$ 时，选择一个动作 ${a\in A}$ 会导致环境进入一个新的状态 ${s^{\prime}\in S}$，其转移概率为 ${T(s,a,s^{\prime})\in(0,1)}$，并给出一个奖励
    ${R(s,a)}$。这个过程在图 [3](#S3.F3 "Figure 3 ‣ III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey") 中进行了说明。随机策略 $\pi:S\to\mathcal{D}$
    是从状态空间到动作集合的概率映射，$\pi(a|s)$ 表示在状态 $s$ 下选择动作 $a$ 的概率。目标是找到最优策略 $\pi^{*}$，其能够产生具有最高预期折扣奖励之和的结果
    [[21](#bib.bib21)]：'
- en: '|  | $\pi^{*}=\underset{\pi}{\operatorname{argmax}}\underbrace{\mathbb{E}_{\pi}\Bigg{\{}\sum_{k=0}^{H-1}\gamma^{k}r_{k+1}\mid
    s_{0}=s\Bigg{\}}}_{:=V_{\pi}(s)},$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}=\underset{\pi}{\operatorname{argmax}}\underbrace{\mathbb{E}_{\pi}\Bigg{\{}\sum_{k=0}^{H-1}\gamma^{k}r_{k+1}\mid
    s_{0}=s\Bigg{\}}}_{:=V_{\pi}(s)},$ |  | (1) |'
- en: 'for all states $s\in S$, where $r_{k}=R(s_{k},a_{k})$ is the reward at time
    $k$ and $V_{\pi}(s)$, the ‘value function’ at state $s$ following a policy $\pi$,
    is the expected ‘return’ (or ‘utility’) when starting at $s$ and following the
    policy $\pi$ thereafter [[1](#bib.bib1)]. An important, related concept is the
    action-value function, a.k.a.‘Q-function’ defined as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有的状态 $s\in S$，其中 $r_{k}=R(s_{k},a_{k})$ 是时间 $k$ 的奖励，$V_{\pi}(s)$ 是在遵循策略 $\pi$
    时从状态 $s$ 开始的预期“收益”（或“效用”） [[1](#bib.bib1)]。一个相关的重要概念是动作值函数，也被称为“Q-函数”，定义为：
- en: '|  | $Q_{\pi}(s,a)=\mathbb{E}_{\pi}\Bigg{\{}\sum_{k=0}^{H-1}\gamma^{k}r_{k+1}\mid
    s_{0}=s,a_{0}=a\Bigg{\}}.$ |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q_{\pi}(s,a)=\mathbb{E}_{\pi}\Bigg{\{}\sum_{k=0}^{H-1}\gamma^{k}r_{k+1}\mid
    s_{0}=s,a_{0}=a\Bigg{\}}.$ |  | (2) |'
- en: The discount factor $\gamma~{}\in~{}[0,1]$ controls how an agent regards future
    rewards. Low values of $\gamma$ encourage myopic behaviour where an agent will
    aim to maximise short term rewards, whereas high values of $\gamma$ cause agents
    to be more forward-looking and to maximise rewards over a longer time frame. The
    horizon $H$ refers to the number of time steps in the MDP. In infinite-horizon
    problems ${H=\infty}$, whereas in episodic domains $H$ has a finite value. Episodic
    domains may terminate after a fixed number of time steps, or when an agent reaches
    a specified goal state. The last state reached in an episodic domain is referred
    to as the terminal state. In finite-horizon or goal-oriented domains discount
    factors of (close to) 1 may be used to encourage agents to focus on achieving
    the goal, whereas in infinite-horizon domains lower discount factors may be used
    to strike a balance between short- and long-term rewards. If the optimal policy
    for a MDP is known, then $V_{\pi^{*}}$ may be used to determine the maximum expected
    discounted sum of rewards available from any arbitrary initial state. A rollout
    is a trajectory produced in the state space by sequentially applying a policy
    to an initial state. A MDP satisfies the Markov property, i.e. system state transitions
    are dependent only on the most recent state and action, not on the full history
    of states and actions in the decision process. Moreover, in many real-world application
    domains, it is not possible for an agent to observe all features of the environment
    state; in such cases the decision-making problem is formulated as a partially-observable
    Markov decision process (POMDP). Solving a reinforcement learning task means finding
    a policy $\pi$ that maximises the expected discounted sum of rewards over trajectories
    in the state space. RL agents may learn value function estimates, policies and/or
    environment models directly. Dynamic programming (DP) refers to a collection of
    algorithms that can be used to compute optimal policies given a perfect model
    of the environment in terms of reward and transition functions. Unlike DP, in
    Monte Carlo methods there is no assumption of complete environment knowledge.
    Monte Carlo methods are incremental in an episode-by-episode sense. Upon the completion
    of an episode, the value estimates and policies are updated. Temporal Difference
    (TD) methods, on the other hand, are incremental in a step-by-step sense, making
    them applicable to non-episodic scenarios. Like Monte Carlo methods, TD methods
    can learn directly from raw experience without a model of the environment’s dynamics.
    Like DP, TD methods learn their estimates based on other estimates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子 $\gamma~{}\in~{}[0,1]$ 控制智能体如何看待未来的奖励。低值的 $\gamma$ 鼓励短视行为，智能体会着重于最大化短期奖励，而高值的
    $\gamma$ 则使智能体更加前瞻，最大化较长时间框架内的奖励。视野 $H$ 指的是MDP中的时间步数。在无限视野问题中，$H=\infty$，而在情节域中，$H$
    具有有限值。情节域可能在固定时间步数后终止，或当智能体达到指定目标状态时终止。在情节域中，最后达到的状态称为终结状态。在有限视野或目标导向域中，可能使用接近1的折扣因子以鼓励智能体专注于实现目标，而在无限视野域中，可能使用较低的折扣因子以在短期和长期奖励之间取得平衡。如果MDP的最优策略已知，则可以使用
    $V_{\pi^{*}}$ 来确定从任意初始状态可获得的最大期望折扣奖励总和。一个回滚（rollout）是通过将策略顺序地应用于初始状态在状态空间中产生的轨迹。MDP
    满足马尔可夫性质，即系统状态转移仅依赖于最近的状态和动作，而不依赖于决策过程中的所有状态和动作历史。此外，在许多实际应用领域，智能体无法观察到环境状态的所有特征；在这种情况下，决策问题被表述为部分可观察马尔可夫决策过程（POMDP）。解决强化学习任务意味着找到一个策略
    $\pi$，使其最大化状态空间中轨迹的期望折扣奖励总和。RL智能体可以直接学习价值函数估计、策略和/或环境模型。动态规划（DP）指的是一系列算法，这些算法可以在对环境的奖励和转移函数有完美模型的情况下计算最优策略。与DP不同，蒙特卡洛方法没有完全环境知识的假设。蒙特卡洛方法在每个情节中是增量的。在一个情节完成后，价值估计和策略会被更新。另一方面，时序差分（TD）方法是逐步增量的，使其适用于非情节性场景。与蒙特卡洛方法一样，TD方法可以直接从原始经验中学习，而不需要环境动态的模型。与DP类似，TD方法基于其他估计来学习其估计值。
- en: III-A Value-based methods
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于价值的方法
- en: 'Q-learning is one of the most commonly used RL algorithms. It is a model-free
    TD algorithm that learns estimates of the utility of individual state-action pairs
    (Q-functions defined in Eqn. [2](#S3.E2 "In III Reinforcement learning ‣ Deep
    Reinforcement Learning for Autonomous Driving: A Survey")). Q-learning has been
    shown to converge to the optimum state-action values for a MDP with probability
    $1$, so long as all actions in all states are sampled infinitely often and the
    state-action values are represented discretely [[23](#bib.bib23)]. In practice,
    Q-learning will learn (near) optimal state-action values provided a sufficient
    number of samples are obtained for each state-action pair. If a Q-learning agent
    has converged to the optimal Q values for a MDP and selects actions greedily thereafter,
    it will receive the same expected sum of discounted rewards as calculated by the
    value function with $\pi^{*}$ (assuming that the same arbitrary initial starting
    state is used for both). Agents implementing Q-learning update their Q values
    according to the following update rule:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是最常用的 RL 算法之一。它是一种无模型的 TD 算法，学习单个状态-动作对的效用估计（在 Eqn. [2](#S3.E2 "在
    III 强化学习 ‣ 自动驾驶的深度强化学习综述") 中定义的 Q 函数）。已经证明，Q-learning 能以概率 $1$ 收敛到 MDP 的最佳状态-动作值，只要所有状态中的所有动作都被无限次采样，并且状态-动作值是离散表示的
    [[23](#bib.bib23)]。实际上，只要为每个状态-动作对获得足够数量的样本，Q-learning 将学习到（接近）最佳的状态-动作值。如果 Q-learning
    代理已经收敛到 MDP 的最佳 Q 值，并且随后贪婪地选择动作，它将获得与值函数 $\pi^{*}$ 计算的折扣奖励总和相同的期望值（假设两个都使用相同的任意初始状态）。实现
    Q-learning 的代理根据以下更新规则更新它们的 Q 值：
- en: '|  | $Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma\max_{a^{\prime}\in A}Q(s^{\prime},a^{\prime})-Q(s,a)],$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma\max_{a^{\prime}\in A}Q(s^{\prime},a^{\prime})-Q(s,a)],$
    |  | (3) |'
- en: 'where $Q(s,a)$ is an estimate of the utility of selecting action $a$ in state
    $s$, $\alpha~{}\in~{}[0,1]$ is the learning rate which controls the degree to
    which Q values are updated at each time step, and $\gamma~{}\in~{}[0,1]$ is the
    same discount factor used in Eqn. [1](#S3.E1 "In III Reinforcement learning ‣
    Deep Reinforcement Learning for Autonomous Driving: A Survey"). The theoretical
    guarantees of Q-learning hold with any arbitrary initial Q values [[23](#bib.bib23)];
    therefore the optimal Q values for a MDP can be learned by starting with any initial
    action value function estimate. The initialisation can be optimistic (each $Q(s,a)$
    returns the maximum possible reward), pessimistic (minimum) or even using knowledge
    of the problem to ensure faster convergence. Deep Q-Networks (DQN) [[24](#bib.bib24)]
    incorporates a variant of the Q-learning algorithm [[25](#bib.bib25)], by using
    deep neural networks (DNNs) as a non-linear Q function approximator over high-dimensional
    state spaces (e.g. the pixels in a frame of an Atari game). Practically, the neural
    network predicts the value of all actions without the use of any explicit domain-specific
    information or hand-designed features. DQN applies experience replay technique
    to break the correlation between successive experience samples and also for better
    sample efficiency. For increased stability, two networks are used where the parameters
    of the target network for DQN are fixed for a number of iterations while updating
    the parameters of the online network. Readers are directed to sub-section [III-E](#S3.SS5
    "III-E Deep reinforcement learning (DRL) ‣ III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey") for a more detailed introduction to
    the use of DNNs in Deep RL.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q(s,a)$ 是选择动作 $a$ 在状态 $s$ 下的效用估计，$\alpha~{}\in~{}[0,1]$ 是学习率，它控制 Q 值在每个时间步更新的程度，$\gamma~{}\in~{}[0,1]$
    是在 Eqn. [1](#S3.E1 "在 III 强化学习 ‣ 自动驾驶的深度强化学习综述") 中使用的折扣因子。Q-learning 的理论保证适用于任何任意初始
    Q 值 [[23](#bib.bib23)]；因此，MDP 的最佳 Q 值可以通过从任何初始动作值函数估计开始来学习。初始化可以是乐观的（每个 $Q(s,a)$
    返回最大可能奖励）、悲观的（最小奖励）或甚至使用问题的知识以确保更快的收敛。深度 Q 网络（DQN） [[24](#bib.bib24)] 结合了 Q-learning
    算法的一种变体 [[25](#bib.bib25)]，通过使用深度神经网络（DNNs）作为高维状态空间（例如，Atari 游戏的一帧像素）的非线性 Q 函数逼近器。实际上，神经网络预测所有动作的值，而不使用任何显式的领域特定信息或手工设计的特征。DQN
    应用经验回放技术来打破连续经验样本之间的相关性，并且提高样本效率。为了增加稳定性，使用了两个网络，其中 DQN 的目标网络参数在更新在线网络参数的过程中被固定了若干迭代次数。读者可以参考子节
    [III-E](#S3.SS5 "III-E 深度强化学习 (DRL) ‣ III 强化学习 ‣ 自动驾驶的深度强化学习综述")，以获得更详细的深度 RL
    中 DNN 使用的介绍。
- en: III-B Policy-based methods
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于策略的方法
- en: 'The difference between value-based and policy-based methods is essentially
    a matter of where the burden of optimality resides. Both method types must propose
    actions and evaluate the resulting behaviour, but while value-based methods focus
    on evaluating the optimal cumulative reward and have a policy follows the recommendations,
    policy-based methods aim to estimate the optimal policy directly, and the value
    is a secondary if calculated at all. Typically, a policy is parameterised as a
    neural network $\pi_{\theta}$. Policy gradient methods use gradient descent to
    estimate the parameters of the policy that maximise the expected reward. The result
    can be a stochastic policy where actions are selected by sampling, or a deterministic
    policy. Many real-world applications have continuous action spaces. Deterministic
    policy gradient (DPG) algorithms [[26](#bib.bib26)] [[1](#bib.bib1)] allow reinforcement
    learning in domains with continuous actions. Silver et al. [[26](#bib.bib26)]
    proved that a deterministic policy gradient exists for MDPs satisfying certain
    conditions, and that deterministic policy gradients have a simple model-free form
    that follows the gradient of the action-value function. As a result, instead of
    integrating over both state and action spaces in stochastic policy gradients,
    DPG integrates over the state space only leading to fewer samples in problems
    with large action spaces. To ensure sufficient exploration, actions are chosen
    using a stochastic policy, while learning a deterministic target policy. The REINFORCE
    [[27](#bib.bib27)] algorithm is a straight forward policy-based method. The discounted
    cumulative reward $g_{t}=\sum_{k=0}^{H-1}\gamma^{k}r_{k+t+1}$ at one time step
    is calculated by playing the entire episode, so no estimator is required for policy
    evaluation. The parameters are updated into the direction of the performance gradient:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的方法和基于策略的方法之间的区别本质上在于最优性负担的所在。两种方法都必须提出动作并评估结果行为，但基于值的方法专注于评估最佳累计回报，并且有一个遵循推荐的策略，而基于策略的方法则直接旨在估计最优策略，价值如果有计算也是次要的。通常，策略被参数化为神经网络
    $\pi_{\theta}$。策略梯度方法使用梯度下降来估计最大化期望回报的策略参数。结果可以是一个随机策略，其中动作通过采样选择，或是一个确定性策略。许多实际应用具有连续的动作空间。确定性策略梯度（DPG）算法
    [[26](#bib.bib26)] [[1](#bib.bib1)] 允许在具有连续动作的领域中进行强化学习。Silver 等人 [[26](#bib.bib26)]
    证明了在满足特定条件的 MDP 中存在确定性策略梯度，并且确定性策略梯度具有遵循动作-价值函数梯度的简单无模型形式。因此，与在随机策略梯度中对状态和动作空间进行积分不同，DPG
    仅对状态空间进行积分，从而在具有大动作空间的问题中需要较少的样本。为了确保充分的探索，使用随机策略选择动作，同时学习确定性目标策略。REINFORCE [[27](#bib.bib27)]
    算法是一种直接的基于策略的方法。一次时间步的折扣累计回报 $g_{t}=\sum_{k=0}^{H-1}\gamma^{k}r_{k+t+1}$ 通过玩整个回合来计算，因此不需要策略评估的估计器。参数沿着性能梯度的方向进行更新：
- en: '|  | $\theta\leftarrow\theta+\alpha\gamma^{t}g\nabla\log\pi_{\theta}(a&#124;s),$
    |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\gamma^{t}g\nabla\log\pi_{\theta}(a&#124;s),$
    |  | (4) |'
- en: where $\alpha$ is the learning rate for a stable incremental update. Intuitively,
    we want to encourage state-action pairs that result in the best possible returns.
    Trust Region Policy Optimization (TRPO) [[28](#bib.bib28)], works by preventing
    the updated policies from deviating too much from previous policies, thus reducing
    the chance of a bad update. TRPO optimises a surrogate objective function where
    the basic idea is to limit each policy gradient update as measured by the Kullback-Leibler
    (KL) divergence between the current and the new proposed policy. This method results
    in monotonic improvements in policy performance. While Proximal Policy Optimization
    (PPO) [[29](#bib.bib29)] proposed a clipped surrogate objective function by adding
    a penalty for having a too large policy change. Accordingly, PPO policy optimisation
    is simpler to implement, and has better sample complexity while ensuring the deviation
    from the previous policy is relatively small.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是用于稳定增量更新的学习率。直观上，我们希望鼓励那些能够获得最佳回报的状态-动作对。信任域策略优化（TRPO）[[28](#bib.bib28)]
    通过防止更新后的策略偏离之前的策略过多，从而减少了坏更新的机会。TRPO 优化一个代理目标函数，其基本思想是通过测量当前策略和新提议策略之间的 Kullback-Leibler
    (KL) 散度来限制每次策略梯度更新。这种方法在策略性能上产生单调的改进。而近端策略优化（PPO）[[29](#bib.bib29)] 提出了一个剪切的代理目标函数，通过对过大的策略变化添加惩罚。因此，PPO
    策略优化更易于实现，且具有更好的样本复杂度，同时确保与之前策略的偏差相对较小。
- en: III-C Actor-critic methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 演员-评论员方法
- en: 'Actor-critic methods are hybrid methods that combine the benefits of policy-based
    and value-based algorithms. The policy structure that is responsible for selecting
    actions is known as the ‘actor’. The estimated value function criticises the actions
    made by the actor and is known as the ‘critic’. After each action selection, the
    critic evaluates the new state to determine whether the result of the selected
    action was better or worse than expected. Both networks need their gradient to
    learn. Let $J(\theta):=\mathbb{E}_{\pi_{\theta}}\left[r\right]$ represent a policy
    objective function, where $\theta$ designates the parameters of a DNN. Policy
    gradient methods search for local maximum of $J(\theta)$. Since optimization in
    continuous action spaces could be costly and slow, the DPG (Direct Policy Gradient)
    algorithm represents actions as parameterised function $\mu(s|\theta^{\mu})$,
    where $\theta^{\mu}$ refers to the parameters of the actor network. Then the unbiased
    estimate of the policy gradient gradient step is given as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法是结合了基于策略和基于价值算法优点的混合方法。负责选择动作的策略结构称为 ‘演员’。估计的价值函数评价演员所采取的动作，称为 ‘评论家’。在每次选择动作后，评论家评估新状态以确定所选动作的结果是否比预期更好或更差。两个网络都需要它们的梯度来进行学习。让
    $J(\theta):=\mathbb{E}_{\pi_{\theta}}\left[r\right]$ 表示一个策略目标函数，其中 $\theta$ 指定了
    DNN 的参数。策略梯度方法寻找 $J(\theta)$ 的局部最大值。由于在连续动作空间中的优化可能代价高昂且缓慢，因此 DPG (Direct Policy
    Gradient) 算法将动作表示为参数化函数 $\mu(s|\theta^{\mu})$，其中 $\theta^{\mu}$ 指的是演员网络的参数。然后，策略梯度的无偏估计步长表示为：
- en: '|  | $\nabla_{\theta}J=-\mathbb{E}_{\pi_{\theta}}\Big{\{}(g-b)\log\pi_{\theta}(a&#124;s)\Big{\}},$
    |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{\theta}J=-\mathbb{E}_{\pi_{\theta}}\Big{\{}(g-b)\log\pi_{\theta}(a&#124;s)\Big{\}},$
    |  | (5) |'
- en: 'where $b$ is the baseline. While using $b\equiv 0$ is the simplification that
    leads to the REINFORCE formulation. Williams [[27](#bib.bib27)] explains a well
    chosen baseline can reduce variance leading to a more stable learning. The baseline,
    $b$ can be chosen as $V_{\pi}(s)$, $Q_{\pi}(s,a)$ or ‘Advantage’ $A_{\pi}(s,a)$
    based methods. Deep Deterministic Policy Gradient (DDPG) [[30](#bib.bib30)] is
    a model-free, off-policy (please refer to subsection [III-D](#S3.SS4 "III-D Model-based
    (vs. Model-free) & On/Off Policy methods ‣ III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey") for a detailed distinction), actor-critic
    algorithm that can learn policies for continuous action spaces using deep neural
    net based function approximation, extending prior work on DPG to large and high-dimensional
    state-action spaces. When selecting actions, exploration is performed by adding
    noise to the actor policy. Like DQN, to stabilise learning a replay buffer is
    used to minimize data correlation. A separate actor-critic specific target network
    is also used. Normal Q-learning is adapted with a restricted number of discrete
    actions, and DDPG also needs a straightforward way to choose an action. Starting
    from Q-learning, we extend Eqn. [2](#S3.E2 "In III Reinforcement learning ‣ Deep
    Reinforcement Learning for Autonomous Driving: A Survey") to define the optimal
    Q-value and optimal action as $Q^{*}$ and $a^{*}$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $b$ 是基线。使用 $b\equiv 0$ 是简化方法，导致了 REINFORCE 公式的形成。Williams [[27](#bib.bib27)]
    解释了精心选择的基线可以减少方差，从而导致更稳定的学习。基线 $b$ 可以选择为 $V_{\pi}(s)$、$Q_{\pi}(s,a)$ 或 ''Advantage''
    $A_{\pi}(s,a)$ 基础的方法。深度确定性策略梯度 (DDPG) [[30](#bib.bib30)] 是一种无模型、脱离策略（详细区分请参见 [III-D](#S3.SS4
    "III-D Model-based (vs. Model-free) & On/Off Policy methods ‣ III Reinforcement
    learning ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey")）的演员-评论家算法，能够利用深度神经网络基础的函数近似学习连续动作空间的策略，将之前的
    DPG 工作扩展到大规模和高维状态-动作空间。在选择动作时，通过向演员策略中添加噪声来进行探索。与 DQN 类似，为了稳定学习，使用重放缓冲区来最小化数据相关性。同时也使用了一个单独的演员-评论家特定目标网络。普通的
    Q 学习在有限数量的离散动作下进行了调整，而 DDPG 也需要一种简单的方法来选择动作。从 Q 学习开始，我们扩展公式 [2](#S3.E2 "In III
    Reinforcement learning ‣ Deep Reinforcement Learning for Autonomous Driving: A
    Survey") 来定义最优 Q 值和最优动作为 $Q^{*}$ 和 $a^{*}$。'
- en: '|  | $Q^{*}(s,a)=\underset{\pi}{\operatorname{max}}~{}Q_{\pi}(s,a),\ \ \ a^{*}=\underset{a}{\operatorname{argmax}}~{}Q^{*}(s,a).$
    |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{*}(s,a)=\underset{\pi}{\operatorname{max}}~{}Q_{\pi}(s,a),\ \ \ a^{*}=\underset{a}{\operatorname{argmax}}~{}Q^{*}(s,a).$
    |  | (6) |'
- en: 'In the case of Q-learning, the action is chosen according to the Q-function
    as in Eqn. [6](#S3.E6 "In III-C Actor-critic methods ‣ III Reinforcement learning
    ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey"). But DDPG chains
    the evaluation of Q after the action has already been chosen according to the
    policy. By correcting the Q-values towards the optimal values using the chosen
    action, we also update the policy towards the optimal action proposition. Thus
    two separate networks work at estimating $Q^{*}$ and $\pi^{*}$.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q-learning 的情况下，行动是根据 Q 函数选择的，如公式 [6](#S3.E6 "在 III-C Actor-critic 方法 ‣ III
    强化学习 ‣ 自动驾驶的深度强化学习：综述") 所示。但 DDPG 在根据策略选择行动后，对 Q 进行评估。通过使用选择的行动来校正 Q 值以接近最优值，我们还更新了政策以接近最优行动提议。因此，两个独立的网络分别用于估计
    $Q^{*}$ 和 $\pi^{*}$。
- en: 'Asynchronous Advantage Actor Critic (A3C) [[31](#bib.bib31)] uses asynchronous
    gradient descent for optimization of deep neural network controllers. Deep reinforcement
    learning algorithms based on experience replay such as DQN and DDPG have demonstrated
    considerable success in difficult domains such as playing Atari games. However,
    experience replay uses a large amount of memory to store experience samples and
    requires off-policy learning algorithms. In A3C, instead of using an experience
    replay buffer, agents asynchronously execute on multiple parallel instances of
    the environment. In addition to the reducing correlation of the experiences, the
    parallel actor-learners have a stabilizing effect on training process. This simple
    setup enables a much larger spectrum of on-policy as well as off-policy reinforcement
    learning algorithms to be applied robustly using deep neural networks. A3C exceeded
    the performance of the previous state-of-the-art at the time on the Atari domain
    while training for half the time on a single multi-core CPU instead of a GPU by
    combining several ideas. It also demonstrates how using an estimate of the value
    function as the previously explained baseline $b$ reduces variance and improves
    convergence time. By defining the advantage as $A_{\pi}(a,s)=Q_{\pi}(s,a)-V_{\pi}(s)$,
    the expression of the policy gradient from Eqn. [5](#S3.E5 "In III-C Actor-critic
    methods ‣ III Reinforcement learning ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") is rewritten as $\nabla_{\theta}L=-\mathbb{E}_{\pi_{\theta}}\{A_{\pi}(a,s)\log\pi_{\theta}(a|s)\}$.
    The critic is trained to minimize $\frac{1}{2}\left\lVert A_{\pi_{\theta}}(a,s)\right\rVert^{2}$.
    The intuition of using advantage estimates rather than just discounted returns
    is to allow the agent to determine not just how good its actions were, but also
    how much better they turned out to be than expected, leading to reduced variance
    and more stable training. The A3C model also demonstrated good performance in
    3D environments such as labyrinth exploration. Advantage Actor Critic (A2C) is
    a synchronous version of the asynchronous advantage actor critic model, that waits
    for each agent to finish its experience before conducting an update. The performance
    of both A2C and A3C is comparable. Most greedy policies must alternate between
    exploration and exploitation, and good exploration visits the states where the
    value estimate is uncertain. This way, exploration focuses on trying to find the
    most uncertain state paths as they bring valuable information. In addition to
    advantage, explained earlier, some methods use the entropy as the uncertainty
    quantity. Most A3C implementations include this as well. Two methods with common
    authors are energy-based policies [[32](#bib.bib32)] and more recent and with
    widespread use, the Soft Actor Critic (SAC) algorithm [[33](#bib.bib33)], both
    rely on adding an entropy term to the reward function, so we update the policy
    objective from Eqn. [1](#S3.E1 "In III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey") to Eqn. [7](#S3.E7 "In III-C Actor-critic
    methods ‣ III Reinforcement learning ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey"). We refer readers to [[33](#bib.bib33)] for an in depth explanation
    of the expression'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 异步优势演员评论家（A3C）[[31](#bib.bib31)]使用异步梯度下降来优化深度神经网络控制器。基于经验回放的深度强化学习算法，如DQN和DDPG，在如玩Atari游戏等困难领域取得了相当大的成功。然而，经验回放需要大量内存来存储经验样本，并且需要脱离策略学习算法。在A3C中，代理不使用经验回放缓冲区，而是在环境的多个并行实例上异步执行。除了减少经验的相关性外，平行的演员-学习者对训练过程具有稳定作用。这种简单的设置使得应用深度神经网络的策略和脱离策略强化学习算法具有更大的范围。通过结合多个思想，A3C在Atari领域的表现超越了当时的前沿技术，同时在单个多核CPU上训练的时间仅为GPU的一半。它还展示了如何使用估计的价值函数作为之前解释的基准$b$，以减少方差并提高收敛速度。通过将优势定义为$A_{\pi}(a,s)=Q_{\pi}(s,a)-V_{\pi}(s)$，从Eqn.
    [5](#S3.E5 "在III-C演员-评论家方法 ‣ III 强化学习 ‣ 自主驾驶的深度强化学习：综述")中的策略梯度表达式被重写为$\nabla_{\theta}L=-\mathbb{E}_{\pi_{\theta}}\{A_{\pi}(a,s)\log\pi_{\theta}(a|s)\}$。评论家被训练以最小化$\frac{1}{2}\left\lVert
    A_{\pi_{\theta}}(a,s)\right\rVert^{2}$。使用优势估计而不仅仅是折扣回报的直觉是允许代理不仅判断其动作的好坏，还判断这些动作相较于预期的好多少，从而减少方差并使训练更加稳定。A3C模型在如迷宫探索的3D环境中也表现良好。优势演员评论家（A2C）是异步优势演员评论家模型的同步版本，等待每个代理完成其经验后再进行更新。A2C和A3C的表现相当。大多数贪婪策略必须在探索和利用之间交替，而良好的探索访问状态不确定的地方。这样，探索着重于尝试找到最不确定的状态路径，因为它们带来了有价值的信息。除了之前解释的优势外，一些方法还使用熵作为不确定性量。大多数A3C实现也包括这一点。两个具有共同作者的方法是基于能量的策略[[32](#bib.bib32)]和更近期且广泛使用的Soft
    Actor Critic（SAC）算法[[33](#bib.bib33)]，这两者都依赖于在奖励函数中加入熵项，因此我们将策略目标从Eqn. [1](#S3.E1
    "在III 强化学习 ‣ 自主驾驶的深度强化学习：综述")更新为Eqn. [7](#S3.E7 "在III-C演员-评论家方法 ‣ III 强化学习 ‣ 自主驾驶的深度强化学习：综述")。我们建议读者参考[[33](#bib.bib33)]以深入了解该表达式。
- en: '|  | $\pi^{*}_{MaxEnt}=\underset{\pi}{\operatorname{argmax}}\mathbb{E}_{\pi}\big{\{}\sum_{t}[r(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))]\big{\}},$ |  | (7) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}_{MaxEnt}=\underset{\pi}{\operatorname{argmax}}\mathbb{E}_{\pi}\big{\{}\sum_{t}[r(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))]\big{\}},$ |  | (7) |'
- en: shown here for illustration of how the entropy $H$ is added.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的是熵 $H$ 如何被添加的示例。
- en: III-D Model-based (vs. Model-free) & On/Off Policy methods
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 基于模型（与非基于模型）& 在/离策略方法
- en: In practical situations, interacting with the real environment could be limited
    due to many reasons including safety and cost. Learning a model for environment
    dynamics may reduce the amount of interactions required with the real environment.
    Moreover, exploration can be performed on the learned models. In the case of model-based
    approaches (e.g. Dyna-Q [[34](#bib.bib34)], R-max [[35](#bib.bib35)]), agents
    attempt to learn the transition function $T$ and reward function $R$, which can
    be used when making action selections. Keeping a model approximation of the environment
    means storing knowledge of its dynamics, and allows for fewer, and sometimes,
    costly environment interactions. By contrast, in model-free approaches such knowledge
    is not a requirement. Instead, model-free learners sample the underlying MDP directly
    in order to gain knowledge about the unknown model, in the form of value function
    estimates for example. In Dyna-2 [[36](#bib.bib36)], the learning agent stores
    long-term and short-term memories, where a memory is defined as the set of features
    and corresponding parameters used by an agent to estimate the value function.
    Long-term memory is for general domain knowledge which is updated from real experience,
    while short-term memory is for specific local knowledge about the current situation,
    and the value function is a linear combination of long and short term memories.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，与真实环境的互动可能受到诸多原因的限制，包括安全和成本。学习环境动态的模型可能会减少与真实环境所需的互动次数。此外，探索可以在学习到的模型上进行。在基于模型的方法（如
    Dyna-Q [[34](#bib.bib34)]，R-max [[35](#bib.bib35)]）中，智能体尝试学习转移函数 $T$ 和奖励函数 $R$，这些可以在做出行动选择时使用。保持环境模型的近似意味着存储其动态知识，并允许更少的、且有时是昂贵的环境互动。相比之下，在无模型的方法中，这种知识不是必需的。相反，无模型的学习者直接对底层
    MDP 进行采样，以获得有关未知模型的知识，例如值函数估计。在 Dyna-2 [[36](#bib.bib36)] 中，学习智能体存储长期和短期记忆，其中记忆定义为智能体用于估计值函数的一组特征和相应的参数。长期记忆用于从真实经验中更新的通用领域知识，而短期记忆用于当前情况的特定本地知识，值函数是长期和短期记忆的线性组合。
- en: 'Learning algorithms can be on-policy or off-policy depending on whether the
    updates are conducted on fresh trajectories generated by the policy or by another
    policy, that could be generated by an older version of the policy or provided
    by an expert. On-policy methods such as SARSA [[37](#bib.bib37)], estimate the
    value of a policy while using the same policy for control. However, off-policy
    methods such as Q-learning [[25](#bib.bib25)], use two policies: the behavior
    policy, the policy used to generate behavior; and the target policy, the one being
    improved on. An advantage of this separation is that the target policy may be
    deterministic (greedy), while the behavior policy can continue to sample all possible
    actions, [[1](#bib.bib1)].'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法可以是在线策略或离线策略，取决于更新是否在由策略生成的新轨迹上进行，或由另一种策略进行，该策略可能是由旧版策略生成或由专家提供的。在线策略方法如
    SARSA [[37](#bib.bib37)]，在使用相同策略进行控制时估计策略的价值。然而，离线策略方法如 Q-learning [[25](#bib.bib25)]，使用两种策略：行为策略，即用于生成行为的策略；以及目标策略，即正在改进的策略。这样分离的一个优势是目标策略可以是确定性的（贪婪的），而行为策略可以继续采样所有可能的动作
    [[1](#bib.bib1)]。
- en: III-E Deep reinforcement learning (DRL)
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 深度强化学习 (DRL)
- en: Tabular representations are the simplest way to store learned estimates (of
    e.g. values, policies or models), where each state-action pair has a discrete
    estimate associated with it. When estimates are represented discretely, each additional
    feature tracked in the state leads to an exponential growth in the number of state-action
    pair values that must be stored [[38](#bib.bib38)]. This problem is commonly referred
    to in the literature as the “curse of dimensionality”, a term originally coined
    by Bellman [[39](#bib.bib39)]. In simple environments this is rarely an issue,
    but it may lead to an intractable problem in real-world applications, due to memory
    and/or computational constraints. Learning over a large state-action space is
    possible, but may take an unacceptably long time to learn useful policies. Many
    real-world domains feature continuous state and/or action spaces; these can be
    discretised in many cases. However, large discretisation steps may limit the achievable
    performance in a domain, whereas small discretisation steps may result in a large
    state-action space where obtaining a sufficient number of samples for each state-action
    pair is impractical. Alternatively, function approximation may be used to generalise
    across states and/or actions, whereby a function approximator is used to store
    and retrieve estimates. Function approximation is an active area of research in
    RL, offering a way to handle continuous state and/or action spaces, mitigate against
    the state-action space explosion and generalise prior experience to previously
    unseen state-action pairs. Tile coding is one of the simplest forms of function
    approximation, where one tile represents multiple states or state-action pairs
    [[38](#bib.bib38)]. Neural networks are also commonly used to implement function
    approximation, one of the most famous examples being Tesuaro’s application of
    RL to backgammon [[40](#bib.bib40)]. Recent work has applied deep neural networks
    as a function approximation method; this emerging paradigm is known as deep reinforcement
    learning (DRL). DRL algorithms have achieved human level performance (or above)
    on complex tasks such as playing Atari games [[24](#bib.bib24)] and playing the
    board game Go [[41](#bib.bib41)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表格表示是存储学习到的估计值（例如，值、策略或模型）最简单的方法，每个状态-动作对都有一个离散的估计值。 当估计值以离散形式表示时，状态中每个附加特征都会导致需要存储的状态-动作对值的指数增长
    [[38](#bib.bib38)]。 这个问题在文献中通常被称为“维度诅咒”，这个术语最早由 Bellman [[39](#bib.bib39)] 创造。
    在简单的环境中这通常不是问题，但在现实世界应用中，可能由于内存和/或计算限制导致不可处理的问题。 在大型状态-动作空间中进行学习是可能的，但可能需要不可接受的长时间才能学习有用的策略。
    许多现实世界领域具有连续的状态和/或动作空间； 在许多情况下，这些可以被离散化。然而，大的离散化步骤可能会限制在一个领域内能够实现的性能，而小的离散化步骤可能会导致一个大型状态-动作空间，其中为每个状态-动作对获得足够的样本是不可行的。
    另外，可以使用函数逼近来对状态和/或动作进行概括，通过使用函数逼近器来存储和检索估计值。 函数逼近是强化学习中的一个活跃研究领域，提供了一种处理连续状态和/或动作空间的方法，减轻状态-动作空间爆炸，并将先前的经验概括到之前未见过的状态-动作对中。
    瓷砖编码是最简单的函数逼近形式之一，其中一个瓷砖表示多个状态或状态-动作对 [[38](#bib.bib38)]。 神经网络也常用于实现函数逼近，其中一个最著名的例子是
    Tesuaro 将强化学习应用于背游戏 [[40](#bib.bib40)]。 最近的工作将深度神经网络应用于函数逼近方法； 这一新兴范式被称为深度强化学习（DRL）。
    DRL 算法在复杂任务上实现了人类水平的表现（或更高），如玩 Atari 游戏 [[24](#bib.bib24)] 和围棋 [[41](#bib.bib41)]。
- en: In DQN [[24](#bib.bib24)] it is demonstrated how a convolutional neural network
    can learn successful control policies from just raw video data for different Atari
    environments. The network was trained end-to-end and was not provided with any
    game specific information. The input to the convolutional neural network consists
    of a $84\times 84\times 4$ tensor of 4 consecutive stacked frames used to capture
    the temporal information. Through consecutive layers, the network learns how to
    combine features in order to identify the action most likely to bring the best
    outcome. One layer consists of several convolutional filters. For instance, the
    first layer uses 32 filters with $8\times 8$ kernels with stride 4 and applies
    a rectifier non linearity. The second layer is 64 filters of $4\times 4$ with
    stride 2, followed by a rectifier non-linearity. Next comes a third convolutional
    layer of 64 filters of $3\times 3$ with stride 1 followed by a rectifier. The
    last intermediate layer is composed of 512 rectifier units fully connected. The
    output layer is a fully-connected linear layer with a single output for each valid
    action. For DQN training stability, two networks are used while the parameters
    of the target network are fixed for a number of iterations while updating the
    online network parameters. For practical reasons, the $Q(s,a)$ function is modeled
    as a deep neural network that predicts the value of all actions given the input
    state. Accordingly, deciding what action to take requires performing a single
    forward pass of the network. Moreover, in order to increase sample efficiency,
    experiences of the agent are stored in a replay memory (experience replay), where
    the Q-learning updates are conducted on randomly selected samples from the replay
    memory. This random selection breaks the correlation between successive samples.
    Experience replay enables reinforcement learning agents to remember and reuse
    experiences from the past where observed transitions are stored for some time,
    usually in a queue, and sampled uniformly from this memory to update the network.
    However, this approach simply replays transitions at the same frequency that they
    were originally experienced, regardless of their significance. An alternative
    method is to use two separate experience buckets, one for positive and one for
    negative rewards [[42](#bib.bib42)]. Then a fixed fraction from each bucket is
    selected to replay. This method is only applicable in domains that have a natural
    notion of binary experience. Experience replay has also been extended with a framework
    for prioritising experience [[43](#bib.bib43)], where important transitions, based
    on the TD error, are replayed more frequently, leading to improved performance
    and faster training when compared to the standard experience replay approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN [[24](#bib.bib24)]中，展示了卷积神经网络如何仅从原始视频数据中学习成功的控制策略，适用于不同的Atari环境。网络进行了端到端训练，且未提供任何特定于游戏的信息。卷积神经网络的输入是一个$84\times
    84\times 4$的张量，由4帧连续的堆叠帧组成，用于捕捉时间信息。通过连续的层，网络学习如何组合特征，以识别最可能带来最佳结果的动作。一个层由几个卷积滤波器组成。例如，第一层使用32个$8\times
    8$的滤波器，步幅为4，并应用了一个修正非线性函数。第二层是64个$4\times 4$的滤波器，步幅为2，随后是一个修正非线性函数。接下来是一个第三卷积层，包含64个$3\times
    3$的滤波器，步幅为1，之后是一个修正层。最后一个中间层由512个修正单元全连接组成。输出层是一个全连接的线性层，为每个有效动作提供一个输出。为了DQN训练的稳定性，使用了两个网络，同时目标网络的参数在更新在线网络参数时保持不变。出于实际原因，$Q(s,a)$函数被建模为一个深度神经网络，用于预测给定输入状态下所有动作的值。因此，决定采取什么动作需要执行网络的一次前向传递。此外，为了提高样本效率，智能体的经验被存储在一个重放记忆（经验重放）中，其中Q-learning更新在从重放记忆中随机选择的样本上进行。这种随机选择打破了连续样本之间的相关性。经验重放使强化学习智能体能够记住和重用过去的经验，其中观察到的过渡会被存储一段时间，通常在一个队列中，并从这个记忆中均匀抽样以更新网络。然而，这种方法仅以原始经历的频率重放过渡，而不考虑它们的重要性。另一种方法是使用两个独立的经验桶，一个用于正奖励，一个用于负奖励
    [[42](#bib.bib42)]。然后从每个桶中选择固定的比例进行重放。这种方法仅适用于具有自然二元经验概念的领域。经验重放还扩展了一个优先级经验的框架
    [[43](#bib.bib43)]，根据TD误差，重要的过渡被更频繁地重放，相比标准经验重放方法，导致性能提升和训练速度加快。
- en: The max operator in standard Q-learning and DQN uses the same values both to
    select and to evaluate an action resulting in over optimistic value estimates.
    In Double DQN (D-DQN) [[44](#bib.bib44)] the over estimation problem in DQN is
    tackled where the greedy policy is evaluated according to the online network and
    uses the target network to estimate its value. It was shown that this algorithm
    not only yields more accurate value estimates, but leads to much higher scores
    on several games.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 Q-learning 和 DQN 中的最大化操作符使用相同的值来选择和评估动作，这会导致过于乐观的价值估计。在 Double DQN (D-DQN)
    [[44](#bib.bib44)] 中，DQN 的过估计问题得到了处理，其中贪婪策略根据在线网络进行评估，并使用目标网络来估计其值。研究表明，这种算法不仅提供了更准确的价值估计，还在多个游戏中获得了更高的分数。
- en: In Dueling network architecture [[45](#bib.bib45)] the state value function
    and associated advantage function are estimated, and then combined together to
    estimate action value function. The advantage of the dueling architecture lies
    partly in its ability to learn the state-value function efficiently. In a single-stream
    architecture only the value for one of the actions is updated. However in dueling
    architecture, the value stream is updated with every update, allowing for better
    approximation of the state values, which in turn need to be accurate for temporal
    difference methods like Q-learning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗网络架构 [[45](#bib.bib45)] 中，状态价值函数和相关的优势函数被估计，然后结合在一起以估计动作价值函数。对抗架构的优势部分在于其高效地学习状态价值函数的能力。在单流架构中，仅更新一个动作的价值。然而，在对抗架构中，价值流在每次更新时都会被更新，从而更好地近似状态值，而这些状态值对于如
    Q-learning 这样的时间差分方法来说需要准确。
- en: DRQN [[46](#bib.bib46)] applied a modification to the DQN by combining a Long
    Short Term Memory (LSTM) with a Deep Q-Network. Accordingly, the DRQN is capable
    of integrating information across frames to detect information such as velocity
    of objects. DRQN showed to generalize its policies in case of complete observations
    and when trained on Atari games and evaluated against flickering games, it was
    shown that DRQN generalizes better than DQN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: DRQN [[46](#bib.bib46)] 通过将长短期记忆（LSTM）与深度 Q 网络（DQN）结合，对 DQN 进行了修改。因此，DRQN 能够整合跨帧的信息来检测如物体速度等信息。DRQN
    在完全观察的情况下展示了其策略的泛化能力，并且在训练于 Atari 游戏并评估闪烁游戏时，显示出 DRQN 比 DQN 具有更好的泛化能力。
- en: IV Extensions to reinforcement learning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 强化学习的扩展
- en: This section introduces and discusses some of the main extensions to the basic
    single-agent RL paradigms which have been introduced over the years. As well as
    broadening the applicability of RL algorithms, many of the extensions discussed
    here have been demonstrated to improve scalability, learning speed and/or converged
    performance in complex problem domains.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍和讨论了近年来对基本单智能体强化学习（RL）范式的一些主要扩展。这些扩展不仅扩大了 RL 算法的适用性，还被证明在复杂问题领域提高了可扩展性、学习速度和/或收敛性能。
- en: IV-A Reward shaping
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 奖励塑形
- en: 'As noted in Section [III](#S3 "III Reinforcement learning ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey"), the design of the reward function
    is crucial: RL agents seek to maximise the return from the reward function, therefore
    the optimal policy for a domain is defined with respect to the reward function.
    In many real-world application domains, learning may be difficult due to sparse
    and/or delayed rewards. RL agents typically learn how to act in their environment
    guided merely by the reward signal. Additional knowledge can be provided to a
    learner by the addition of a shaping reward to the reward naturally received from
    the environment, with the goal of improving learning speed and converged performance.
    This principle is referred to as reward shaping. The term shaping has its origins
    in the field of experimental psychology, and describes the idea of rewarding all
    behaviour that leads to the desired behaviour. Skinner [[47](#bib.bib47)] discovered
    while training a rat to push a lever that any movement in the direction of the
    lever had to be rewarded to encourage the rat to complete the task. Analogously
    to the rat, a RL agent may take an unacceptably long time to discover its goal
    when learning from delayed rewards, and shaping offers an opportunity to speed
    up the learning process. Reward shaping allows a reward function to be engineered
    in a way to provide more frequent feedback signal on appropriate behaviours [[48](#bib.bib48)],
    which is especially useful in domains with sparse rewards. Generally, the return
    from the reward function is modified as follows: $r^{\prime}=r+f$ where $r$ is
    the return from the original reward function $R$, $f$ is the additional reward
    from a shaping function $F$, and $r^{\prime}$ is the signal given to the agent
    by the augmented reward function $R^{\prime}$. Empirical evidence has shown that
    reward shaping can be a powerful tool to improve the learning speed of RL agents
    [[49](#bib.bib49)]. However, it can have unintended consequences. The implication
    of adding a shaping reward is that a policy which is optimal for the augmented
    reward function $R^{\prime}$ may not in fact also be optimal for the original
    reward function $R$. A classic example of reward shaping gone wrong for this exact
    reason is reported by [[49](#bib.bib49)] where the experimented bicycle agent
    would turn in circle to stay upright rather than reach its goal. Difference rewards
    (D) [[50](#bib.bib50)] and potential-based reward shaping (PBRS) [[51](#bib.bib51)]
    are two commonly used shaping approaches. Both D and PBRS have been successfully
    applied to a wide range of application domains and have the added benefit of convenient
    theoretical guarantees, meaning that they do not suffer from the same issues as
    the unprincipled reward shaping approaches described above (see e.g. [[51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '正如在第[III](#S3 "III Reinforcement learning ‣ Deep Reinforcement Learning for
    Autonomous Driving: A Survey")节中所述，奖励函数的设计至关重要：RL智能体旨在最大化奖励函数的回报，因此某一领域的最优策略是相对于奖励函数来定义的。在许多实际应用领域，由于奖励稀疏和/或延迟，学习可能会变得困难。RL智能体通常通过奖励信号来指导其在环境中的行动。通过在自然接收到的环境奖励之外增加一个塑造奖励，可以向学习者提供额外的知识，目标是提高学习速度和收敛性能。这一原则被称为奖励塑造。塑造一词源于实验心理学领域，描述了奖励所有导致期望行为的行为的概念。斯金纳[[47](#bib.bib47)]在训练一只老鼠按杠杆时发现，任何朝向杠杆的动作都必须给予奖励，以鼓励老鼠完成任务。类似地，RL智能体在从延迟奖励中学习时可能需要非常长的时间才能发现其目标，而塑造提供了加快学习过程的机会。奖励塑造允许通过工程化奖励函数的方式，提供对适当行为的更频繁反馈信号[[48](#bib.bib48)]，这在奖励稀疏的领域尤为有用。通常，奖励函数的回报修改如下：$r^{\prime}=r+f$
    其中 $r$ 是来自原始奖励函数 $R$ 的回报，$f$ 是来自塑造函数 $F$ 的额外奖励，而 $r^{\prime}$ 是由增强奖励函数 $R^{\prime}$
    给智能体的信号。实证证据表明，奖励塑造可以成为提高RL智能体学习速度的有力工具[[49](#bib.bib49)]。然而，它可能会产生意想不到的后果。添加塑造奖励的含义是，对于增强奖励函数
    $R^{\prime}$ 最优的策略可能实际上并不一定对于原始奖励函数 $R$ 也是最优的。一个经典的奖励塑造失败的例子正是由于这个原因被[[49](#bib.bib49)]报告，其中实验的自行车智能体为了保持直立而在原地打圈，而不是到达目标。差异奖励（D）[[50](#bib.bib50)]
    和基于潜在的奖励塑造（PBRS）[[51](#bib.bib51)] 是两种常用的塑造方法。D 和 PBRS 已成功应用于广泛的应用领域，并具有便利的理论保证，这意味着它们不会像上述未经原则的奖励塑造方法那样遭遇相同的问题（参见例如[[51](#bib.bib51)，[52](#bib.bib52)，[53](#bib.bib53)，[54](#bib.bib54)，[55](#bib.bib55)]）。'
- en: IV-B Multi-agent reinforcement learning (MARL)
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 多智能体强化学习（MARL）
- en: In multi-agent reinforcement learning, multiple RL agents are deployed into
    a common environment. The single-agent MDP framework becomes inadequate when multiple
    autonomous agents act simultaneously in the same domain. Instead, the more general
    stochastic game (SG) may be used in the case of a Multi-Agent System (MAS) [[56](#bib.bib56)].
    A SG is defined as a tuple $<~{}S,A_{1...N},T,R_{1...N}~{}>$, where $N$ is the
    number of agents, $S$ is the set of system states, $A_{i}$ is the set of actions
    for agent $i$ (and $A$ is the joint action set), $T$ is the transition function,
    and $R_{i}$ is the reward function for agent $i$. The SG looks very similar to
    the MDP framework, apart from the addition of multiple agents. In fact, for the
    case of $N=1$ a SG then becomes a MDP. The next system state and the rewards received
    by each agent depend on the joint action $a$ of all of the agents in a SG, where
    $a$ is derived from the combination of the individual actions $a_{i}$ for each
    agent in the system. Each agent may have its own local state perception $s_{i}$,
    which is different to the system state $s$ (i.e. individual agents are not assumed
    to have full observability of the system). Note also that each agent may receive
    a different reward for the same system state transition, as each agent has its
    own separate reward function $R_{i}$. In a SG, the agents may all have the same
    goal (collaborative SG), totally opposing goals (competitive SG), or there may
    be elements of collaboration and competition between agents (mixed SG). Whether
    RL agents in a MAS will learn to act together or at cross-purposes depends on
    the reward scheme used for a specific application.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体强化学习中，多个RL智能体被部署到一个共同的环境中。当多个自主智能体在同一领域中同时行动时，单智能体MDP框架变得不再适用。相反，针对多智能体系统（MAS）的情况，可以使用更一般的随机博弈（SG）[[56](#bib.bib56)]。SG定义为一个元组$<~{}S,A_{1...N},T,R_{1...N}~{}>$，其中$N$是智能体的数量，$S$是系统状态的集合，$A_{i}$是智能体$i$的行动集合（$A$是联合行动集合），$T$是转移函数，$R_{i}$是智能体$i$的奖励函数。SG与MDP框架非常相似，除了增加了多个智能体。实际上，对于$N=1$的情况，SG变成了MDP。下一系统状态和每个智能体获得的奖励取决于SG中所有智能体的联合行动$a$，其中$a$是系统中每个智能体个体行动$a_{i}$的组合。每个智能体可能有自己的局部状态感知$s_{i}$，这与系统状态$s$不同（即假设个体智能体无法完全观测系统）。还要注意的是，每个智能体可能会在相同系统状态转移下获得不同的奖励，因为每个智能体有其自己的奖励函数$R_{i}$。在SG中，智能体可能都有相同的目标（协作SG），完全对立的目标（竞争SG），或者在智能体之间可能存在协作和竞争的元素（混合SG）。MAS中的RL智能体是否会学习到协同作用还是相互对抗，取决于特定应用所使用的奖励机制。
- en: IV-C Multi-objective reinforcement learning
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 多目标强化学习
- en: 'In multi-objective reinforcement learning (MORL) the reward signal is a vector,
    where each component represents the performance on a different objective. The
    MORL framework was developed to handle sequential decision making problems where
    tradeoffs between conflicting objective functions must be considered. Examples
    of real-world problems with multiple objectives include selecting energy sources
    (tradeoffs between fuel cost and emissions) [[57](#bib.bib57)] and watershed management
    (tradeoffs between generating electricity, preserving reservoir levels and supplying
    drinking water) [[58](#bib.bib58)]. Solutions to MORL problems are often evaluated
    using the concept of Pareto dominance [[59](#bib.bib59)] and MORL algorithms typically
    seek to learn or approximate the set of non-dominated solutions. MORL problems
    may be defined using the MDP or SG framework as appropriate, in a similar manner
    to single-objective problems. The main difference lies in the definition of the
    reward function: instead of returning a single scalar value $r$, the reward function
    $\mathbf{R}$ in multi-objective domains returns a vector $\mathbf{r}$ consisting
    of the rewards for each individual objective $c\in C$. Therefore, a regular MDP
    or SG can be extended to a Multi-Objective MDP (MOMDP) or Multi-Objective SG (MOSG)
    by modifying the return of the reward function. For a more complete overview of
    MORL beyond the brief summary presented in this section, the interested reader
    is referred to recent surveys [[60](#bib.bib60), [61](#bib.bib61)].'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在多目标强化学习（MORL）中，奖励信号是一个向量，每个分量表示不同目标上的表现。MORL 框架的开发旨在处理序列决策问题，其中必须考虑冲突目标函数之间的权衡。具有多个目标的现实世界问题的例子包括选择能源来源（燃料成本和排放之间的权衡）[[57](#bib.bib57)]和流域管理（发电、保持水库水位和供给饮用水之间的权衡）[[58](#bib.bib58)]。MORL
    问题的解决方案通常使用帕累托优势的概念进行评估 [[59](#bib.bib59)]，MORL 算法通常试图学习或逼近非支配解的集合。MORL 问题可以使用
    MDP 或 SG 框架进行定义，类似于单目标问题。主要的区别在于奖励函数的定义：在多目标领域，奖励函数 $\mathbf{R}$ 返回一个包含每个个体目标
    $c\in C$ 奖励的向量 $\mathbf{r}$，而不是返回单一的标量值 $r$。因此，常规的 MDP 或 SG 可以通过修改奖励函数的返回值扩展为多目标
    MDP（MOMDP）或多目标 SG（MOSG）。有关 MORL 的更完整概述，感兴趣的读者可以参阅最近的综述 [[60](#bib.bib60), [61](#bib.bib61)]。
- en: IV-D State Representation Learning (SRL)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 状态表示学习（SRL）
- en: State Representation Learning refers to feature extraction & dimensionality
    reduction to represent the state space with its history conditioned by the actions
    and environment of the agent. A complete review of SRL for control is discussed
    in [[62](#bib.bib62)]. In the simplest form SRL maps a high dimensional vector
    $o_{t}$ into a small dimensional latent space $s_{t}$. The inverse operation decodes
    the state back into an estimate of the original observation $\hat{o}_{t}$. The
    agent then learns to map from the latent space to the action. Training the SRL
    chain is unsupervised in the sense that no labels are required. Reducing the dimension
    of the input effectively simplifies the task as it removes noise and decreases
    the domain’s size as shown in [[63](#bib.bib63)]. SRL could be a simple auto-encoder
    (AE), though various methods exist for observation reconstruction such as Variational
    Auto-Encoders (VAE) or Generative Adversarial Networks (GANs), as well as forward
    models for predicting the next state or inverse models for predicting the action
    given a transition. A good learned state representation should be Markovian; i.e.
    it should encode all necessary information to be able to select an action based
    on the current state only, and not any previous states or actions [[64](#bib.bib64),
    [62](#bib.bib62)].
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 状态表示学习指的是特征提取和降维，用以表示由智能体的动作和环境所条件的状态空间及其历史。关于控制的 SRL 的完整回顾可以参见 [[62](#bib.bib62)]。在最简单的形式中，SRL
    将一个高维向量 $o_{t}$ 映射到一个低维潜在空间 $s_{t}$。反向操作将状态解码回原始观测值的估计 $\hat{o}_{t}$。然后，智能体学习从潜在空间映射到动作。训练
    SRL 链是无监督的，因为不需要标签。有效地减少输入的维度简化了任务，因为它去除了噪声并减少了领域的规模，如 [[63](#bib.bib63)] 所示。SRL
    可以是一个简单的自编码器（AE），尽管也存在多种用于观测重建的方法，如变分自编码器（VAE）或生成对抗网络（GANs），以及用于预测下一个状态的前向模型或给定过渡预测动作的逆向模型。一个良好的学习状态表示应该是马尔可夫的；即它应该编码所有必要的信息，以便仅基于当前状态选择动作，而不依赖于任何先前的状态或动作
    [[64](#bib.bib64), [62](#bib.bib62)]。
- en: IV-E Learning from Demonstrations
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 从示范中学习
- en: 'Learning from Demonstrations (LfD) is used by humans to acquire new skills
    in an expert to learner knowledge transmission process. LfD is important for initial
    exploration where reward signals are too sparse or the input domain is too large
    to cover. In LfD, an agent learns to perform a task from demonstrations, usually
    in the form of state-action pairs, provided by an expert without any feedback
    rewards. However, high quality and diverse demonstrations are hard to collect,
    leading to learning sub-optimal policies. Accordingly, learning merely from demonstrations
    can be used to initialize the learning agent with a good or safe policy, and then
    reinforcement learning can be conducted to enable the discovery of a better policy
    by interacting with the environment. Combining demonstrations and reinforcement
    learning has been conducted in recent research. AlphaGo [[41](#bib.bib41)], combines
    search tree with deep neural networks, initializes the policy network by supervised
    learning on state-action pairs provided by recorded games played by human experts.
    Additionally, a value network is trained to tell how desirable a board state is.
    By conducting self-play and reinforcement learning, AlphaGo is able to discover
    new stronger actions and learn from its mistakes, achieving super human performance.
    More recently, AlphaZero [[65](#bib.bib65)], developed by the same team, proposed
    a general framework for self-play models. AlphaZero is trained entirely using
    reinforcement learning and self play, starting from completely random play, and
    requires no prior knowledge of human players. AlphaZero taught itself from scratch
    how to master the games of chess, shogi, and Go game, beating a world-champion
    program in each case. In [[66](#bib.bib66)] it is shown that given the initial
    demonstration, no explicit exploration is necessary, and we can attain near-optimal
    performance. Measuring the divergence between the current policy and the expert
    policy for optimization is proposed in [[67](#bib.bib67)]. DQfD [[68](#bib.bib68)]
    pre-trains the agent and uses expert demonstrations by adding them into the replay
    buffer with additional priority. Moreover, a training framework that combines
    learning from both demonstrations and reinforcement learning is proposed in [[69](#bib.bib69)]
    for fast learning agents. Two policies close to maximizing the reward function
    can still have large differences in behaviour. To avoid degenerating a solution
    which would fit the reward but not the original behaviour, authors [[70](#bib.bib70)]
    proposed a method for enforcing that the optimal policy learnt over the rewards
    should still match the observed policy in behavior. Behavior Cloning (BC) is applied
    as a supervised learning that maps states to actions based on demonstrations provided
    by an expert. On the other hand, Inverse Reinforcement Learning (IRL) is about
    inferring the reward function that justifies demonstrations of the expert. IRL
    is the problem of extracting a reward function given observed, optimal behavior
    [[71](#bib.bib71)]. A key motivation is that the reward function provides a succinct
    and robust definition of a task. Generally, IRL algorithms can be expensive to
    run, requiring reinforcement learning in an inner loop between cost estimation
    to policy training and evaluation. Generative Adversarial Imitation Learning (GAIL)
    [[72](#bib.bib72)] introduces a way to avoid this expensive inner loop. In practice,
    GAIL trains a policy close enough to the expert policy to fool a discriminator.
    This process is similar to GANs [[73](#bib.bib73), [74](#bib.bib74)]. The resulting
    policy must travel the same MDP states as the expert, or the discriminator would
    pick up the differences. The theory behind GAIL is an equation simplification:
    qualitatively, if IRL is going from demonstrations to a cost function and RL from
    a cost function to a policy, then we should altogether be able to go from demonstration
    to policy in a single equation while avoiding the cost function estimation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从示范学习（LfD）是人类在专家与学习者之间传递知识过程中用来获得新技能的方法。LfD 对于初始探索尤其重要，当奖励信号过于稀疏或输入领域过于庞大时，LfD
    显得尤为重要。在 LfD 中，智能体通过专家提供的示范（通常是状态-动作对形式），学习执行任务，而没有任何反馈奖励。然而，高质量和多样化的示范很难收集，这会导致学习到次优策略。因此，仅仅依赖示范学习可以用来初始化学习智能体，以获得一个良好或安全的策略，然后通过强化学习与环境互动，发现更优的策略。最近的研究已经结合了示范学习和强化学习。AlphaGo
    [[41](#bib.bib41)] 结合了搜索树和深度神经网络，通过对人类专家所下棋局的状态-动作对进行监督学习来初始化策略网络。此外，还训练了一个价值网络，用来判断棋盘状态的期望值。通过自我对弈和强化学习，AlphaGo
    能够发现新的、更强的动作，并从错误中学习，达到了超越人类的表现。更近期的，AlphaZero [[65](#bib.bib65)]，由同一团队开发，提出了一个自我对弈模型的一般框架。AlphaZero
    完全通过强化学习和自我对弈进行训练，从完全随机的对弈开始，不需要任何关于人类玩家的先验知识。AlphaZero 从零开始自学如何掌握国际象棋、将棋和围棋，并在每个游戏中击败了世界冠军程序。在
    [[66](#bib.bib66)] 中展示了在给定初始示范的情况下，不需要明确的探索，我们可以达到接近最优的表现。[[67](#bib.bib67)] 提出了测量当前策略与专家策略之间的差异以进行优化的方法。DQfD
    [[68](#bib.bib68)] 通过将专家示范添加到重放缓冲区并赋予额外优先级来预训练智能体。此外，在 [[69](#bib.bib69)] 中提出了一个结合了示范学习和强化学习的训练框架，用于快速学习智能体。两个接近最大化奖励函数的策略可能在行为上仍然存在较大差异。为了避免退化出一个符合奖励但不符合原始行为的解决方案，作者
    [[70](#bib.bib70)] 提出了一个方法来强制确保在奖励上学习到的最优策略仍然与观察到的行为策略一致。行为克隆（BC）被应用为一种监督学习，它根据专家提供的示范将状态映射到动作。另一方面，逆强化学习（IRL）旨在推断出能够解释专家示范的奖励函数。IRL
    是从观察到的最优行为中提取奖励函数的问题 [[71](#bib.bib71)]。一个关键动机是，奖励函数提供了任务的简洁而稳健的定义。通常，IRL 算法可能运行开销较大，需要在成本估算到策略训练和评估之间进行强化学习的内循环。生成对抗模仿学习（GAIL）
    [[72](#bib.bib72)] 引入了一种避免这种昂贵内循环的方法。在实践中，GAIL 训练一个足够接近专家策略的策略，以欺骗鉴别器。这个过程类似于
    GANs [[73](#bib.bib73)，[74](#bib.bib74)]。最终的策略必须遍历与专家相同的 MDP 状态，否则鉴别器会发现差异。GAIL
    背后的理论是方程简化：从定性上讲，如果 IRL 是从示范到成本函数，而 RL 是从成本函数到策略，那么我们应该能够在一个方程中从示范到策略，同时避免成本函数的估算。
- en: V Reinforcement learning for Autonomous driving tasks
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 自动驾驶任务的强化学习
- en: 'Autonomous driving tasks where RL could be applied include: controller optimization,
    path planning and trajectory optimization, motion planning and dynamic path planning,
    development of high-level driving policies for complex navigation tasks, scenario-based
    policy learning for highways, intersections, merges and splits, reward learning
    with inverse reinforcement learning from expert data for intent prediction for
    traffic actors such as pedestrian, vehicles and finally learning of policies that
    ensures safety and perform risk estimation. Before discussing the applications
    of DRL to AD tasks we briefly review the state space, action space and rewards
    schemes in autonomous driving setting.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 可以应用强化学习（RL）的自动驾驶任务包括：控制器优化、路径规划和轨迹优化、运动规划和动态路径规划、复杂导航任务的高层驾驶策略开发、高速公路、交叉路口、合流和分流的场景化策略学习、使用逆强化学习从专家数据中进行的意图预测奖励学习，如行人、车辆，以及最后确保安全和进行风险评估的策略学习。在讨论深度强化学习（DRL）在自动驾驶任务中的应用之前，我们简要回顾一下自动驾驶环境中的状态空间、动作空间和奖励方案。
- en: V-A State Spaces, Action Spaces and Rewards
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 状态空间、动作空间和奖励
- en: 'To successfully apply DRL to autonomous driving tasks, designing appropriate
    state spaces, action spaces, and reward functions is important. Leurent et al.
    [[75](#bib.bib75)] provided a comprehensive review of the different state and
    action representations which are used in autonomous driving research. Commonly
    used state space features for an autonomous vehicle include: position, heading
    and velocity of ego-vehicle, as well as other obstacles in the sensor view extent
    of the ego-vehicle. To avoid variations in the dimension of the state space, a
    Cartesian or Polar occupancy grid around the ego vehicle is frequently employed.
    This is further augmented with lane information such as lane number (ego-lane
    or others), path curvature, past and future trajectory of the ego-vehicle, longitudinal
    information such as Time-to-collision (TTC), and finally scene information such
    as traffic laws and signal locations.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功将深度强化学习（DRL）应用于自动驾驶任务，设计适当的状态空间、动作空间和奖励函数是非常重要的。Leurent 等人[[75](#bib.bib75)]
    对自动驾驶研究中使用的不同状态和动作表示进行了全面的综述。常用的自动驾驶车辆状态空间特征包括：自车的位置、航向和速度，以及自车传感器视野范围内的其他障碍物。为了避免状态空间维度的变化，通常采用自车周围的笛卡尔或极坐标占用网格。这个网格还会进一步补充车道信息，如车道编号（自车车道或其他车道）、路径曲率、自车的过去和未来轨迹、纵向信息如碰撞时间（TTC），以及场景信息如交通法规和信号位置。
- en: 'Using raw sensor data such as camera images, LiDAR, radar, etc. provides the
    benefit of finer contextual information, while using condensed abstracted data
    reduces the complexity of the state space. In between, a mid-level representation
    such as 2D bird eye view (BEV) is sensor agnostic but still close to the spatial
    organization of the scene. Fig. [4](#S5.F4 "Figure 4 ‣ V-A State Spaces, Action
    Spaces and Rewards ‣ V Reinforcement learning for Autonomous driving tasks ‣ Deep
    Reinforcement Learning for Autonomous Driving: A Survey") is an illustration of
    a top down view showing an occupancy grid, past and projected trajectories, and
    semantic information about the scene such as the position of traffic lights. This
    intermediary format retains the spatial layout of roads when graph-based representations
    would not. Some simulators offer this view such as Carla or Flow (see Table [II](#S5.T2
    "TABLE II ‣ V-C Simulator & Scenario generation tools ‣ V Reinforcement learning
    for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous Driving:
    A Survey")).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '使用原始传感器数据，如摄像头图像、激光雷达、雷达等，提供了更细致的上下文信息，而使用浓缩的抽象数据则降低了状态空间的复杂性。在这两者之间，中级表示，如二维鸟瞰图（BEV），虽然传感器无关，但仍然接近场景的空间组织。图
    [4](#S5.F4 "Figure 4 ‣ V-A State Spaces, Action Spaces and Rewards ‣ V Reinforcement
    learning for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") 是一个从上向下查看的示例，显示了占用网格、过去和预测轨迹，以及关于场景的语义信息，如交通信号灯的位置。这种中间格式在图形化表示无法保留道路空间布局时提供了帮助。一些模拟器提供这种视图，如
    Carla 或 Flow（见表 [II](#S5.T2 "TABLE II ‣ V-C Simulator & Scenario generation tools
    ‣ V Reinforcement learning for Autonomous driving tasks ‣ Deep Reinforcement Learning
    for Autonomous Driving: A Survey")）。'
- en: 'A vehicle policy must control a number of different actuators. Continuous-valued
    actuators for vehicle control include steering angle, throttle and brake. Other
    actuators such as gear changes are discrete. To reduce complexity and allow the
    application of DRL algorithms which work with discrete action spaces only (e.g.
    DQN), an action space may be discretised uniformly by dividing the range of continuous
    actuators such as steering angle, throttle and brake into equal-sized bins (see
    Section [VI-C](#S6.SS3 "VI-C Sample efficiency ‣ VI Real world challenges and
    future perspectives ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey")).
    Discretisation in log-space has also been suggested, as many steering angles which
    are selected in practice are close to the centre [[76](#bib.bib76)]. Discretisation
    does have disadvantages however; it can lead to jerky or unstable trajectories
    if the step values between actions are too large. Furthermore, when selecting
    the number of bins for an actuator there is a trade-off between having enough
    discrete steps to allow for smooth control, and not having so many steps that
    action selections become prohibitively expensive to evaluate. As an alternative
    to discretisation, continuous values for actuators may also be handled by DRL
    algorithms which learn a policy directly, (e.g. DDPG). Temporal abstractions options
    framework [[77](#bib.bib77)]) may also be employed to simplify the process of
    selecting actions, where agents select options instead of low-level actions. These
    options represent a sub-policy that could extend a primitive action over multiple
    time steps.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一辆车的策略必须控制多个不同的执行器。用于车辆控制的连续值执行器包括转向角、油门和刹车。其他如换挡的执行器是离散的。为了降低复杂性并允许只适用于离散动作空间的
    DRL 算法（如 DQN）的应用，可以通过将转向角、油门和刹车等连续执行器的范围均匀地分割成等大小的区间来离散化动作空间（见第 [VI-C](#S6.SS3
    "VI-C 样本效率 ‣ VI 现实世界挑战与未来展望 ‣ 深度强化学习在自主驾驶中的应用：综述") 节）。也有建议在对数空间中进行离散化，因为在实践中选择的许多转向角接近中心
    [[76](#bib.bib76)]。然而，离散化确实有其缺点；如果动作之间的步长值过大，它可能导致抖动或不稳定的轨迹。此外，在选择执行器的区间数时，需要在拥有足够的离散步骤以实现平稳控制和步骤数量过多以至于动作选择的评估代价过高之间进行权衡。作为离散化的替代方案，DRL
    算法也可以处理执行器的连续值，直接学习策略（例如 DDPG）。时间抽象选项框架 [[77](#bib.bib77)] 也可以用于简化选择动作的过程，其中代理选择选项而不是低级动作。这些选项代表一个子策略，可以将原始动作扩展到多个时间步长。
- en: 'Designing reward functions for DRL agents for autonomous driving is still very
    much an open question. Examples of criteria for AD tasks include: distance travelled
    towards a destination [[78](#bib.bib78)], speed of the ego vehicle [[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)], keeping the ego vehicle at a standstill [[81](#bib.bib81)],
    collisions with other road users or scene objects [[78](#bib.bib78), [79](#bib.bib79)],
    infractions on sidewalks [[78](#bib.bib78)], keeping in lane, and maintaining
    comfort and stability while avoiding extreme acceleration, braking or steering
    [[81](#bib.bib81), [80](#bib.bib80)], and following traffic rules [[79](#bib.bib79)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于自主驾驶的深度强化学习（DRL）代理的奖励函数仍然是一个悬而未决的问题。自动驾驶任务的标准包括：驶向目的地的距离 [[78](#bib.bib78)]，自车的速度
    [[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]，保持自车静止 [[81](#bib.bib81)]，与其他道路用户或场景物体的碰撞
    [[78](#bib.bib78), [79](#bib.bib79)]，在人行道上的违规行为 [[78](#bib.bib78)]，保持车道内行驶，并在避免极端加速、刹车或转向的同时保持舒适性和稳定性
    [[81](#bib.bib81), [80](#bib.bib80)]，以及遵守交通规则 [[79](#bib.bib79)]。
- en: '![Refer to caption](img/f87c1f2540c963784c2f55f8eb01e081.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f87c1f2540c963784c2f55f8eb01e081.png)'
- en: 'Figure 4: Bird Eye View (BEV) 2D representation of a driving scene. Left demonstrates
    an occupancy grid. Right shows the combination of semantic information (traffic
    lights) with past (red) and projected (green) trajectories. The ego car is represented
    by a green rectangle in both images.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：驾驶场景的鸟瞰图（BEV）二维表示。左侧展示了占用网格。右侧显示了将语义信息（交通信号灯）与过去（红色）和预测（绿色）轨迹的结合。自车在两幅图像中均由绿色矩形表示。
- en: V-B Motion Planning & Trajectory optimization
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 运动规划与轨迹优化
- en: '| AD Task | (D)RL method & description | Improvements & Tradeoffs |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 自动驾驶任务 | （D）RL 方法及描述 | 改进与权衡 |'
- en: '| Lane Keep | 1\. Authors [[82](#bib.bib82)] propose a DRL system for discrete
    actions (DQN) and continuous actions (DDAC) using the TORCS simulator (see Table
    [II](#S5.T2 "TABLE II ‣ V-C Simulator & Scenario generation tools ‣ V Reinforcement
    learning for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey")) 2\. Authors [[83](#bib.bib83)] learn discretised and continuous
    policies using DQNs and Deep Deterministic Actor Critic (DDAC) to follow the lane
    and maximize average velocity. | 1\. This study concludes that using continuous
    actions provide smoother trajectories, though on the negative side lead to more
    restricted termination conditions & slower convergence time to learn. 2\. Removing
    memory replay in DQNs help for faster convergence & better performance. The one
    hot encoding of action space resulted in abrupt steering control. While DDAC’s
    continuous policy helps smooth the actions and provides better performance. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 车道保持 | 1\. 作者 [[82](#bib.bib82)] 提出了一个DRL系统，用于离散动作（DQN）和连续动作（DDAC），使用TORCS模拟器（见表
    [II](#S5.T2 "TABLE II ‣ V-C Simulator & Scenario generation tools ‣ V Reinforcement
    learning for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey")） 2\. 作者 [[83](#bib.bib83)] 使用DQNs和深度确定性演员评论家（DDAC）学习离散和连续的策略，以跟随车道并最大化平均速度。
    | 1\. 研究结论认为，使用连续动作能提供更平滑的轨迹，但负面影响是导致更严格的终止条件和学习速度更慢。 2\. 移除DQNs中的记忆重放有助于更快的收敛和更好的性能。动作空间的独热编码导致了突发的转向控制，而DDAC的连续策略帮助平滑动作并提供更好的性能。
    |'
- en: '| Lane Change | Authors [[84](#bib.bib84)] use Q-learning to learn a policy
    for ego-vehicle to perform no operation, lane change to left/right, accelerate/decelerate.
    | This approach is more robust compared to traditional approaches which consist
    in defining fixed way points, velocity profiles and curvature of path to be followed
    by the ego vehicle. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 车道变换 | 作者 [[84](#bib.bib84)] 使用Q-learning来学习自车执行无操作、向左/右变道、加速/减速的策略。 | 这种方法比传统方法更具鲁棒性，传统方法包括定义固定的路径点、速度曲线和车道曲率
    |'
- en: '| Ramp Merging | Authors [[85](#bib.bib85)] propose recurrent architectures
    namely LSTMs to model longterm dependencies for ego vehicles ramp merging into
    a highway. | Past history of the state information is used to perform the merge
    more robustly. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 匝道合并 | 作者 [[85](#bib.bib85)] 提出了递归架构，即LSTM，用于建模自车在高速公路上匝道合并的长期依赖性。 | 使用过去的状态信息来更稳健地进行合并。
    |'
- en: '| Overtaking | Authors [[86](#bib.bib86)] propose Multi-goal RL policy that
    is learnt by Q-Learning or Double action Q-Learning(DAQL) is employed to determine
    individual action decisions based on whether the other vehicle interacts with
    the agent for that particular goal. | Improved speed for lane keeping and overtaking
    with collision avoidance. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 超车 | 作者 [[86](#bib.bib86)] 提出了多目标RL策略，通过Q-Learning或双重动作Q-Learning（DAQL）来确定个体动作决策，基于其他车辆是否与代理互动来实现特定目标。
    | 提升了车道保持和超车的速度，同时避免碰撞。'
- en: '| Intersections | Authors use DQN to evalute the Q-value for state-action pairs
    to negotiate intersection [[87](#bib.bib87)], | Creep-Go actions defined by authors
    enables the vehicle to maneuver intersections with restricted spaces and visibility
    more safely |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 交叉路口 | 作者使用DQN来评估状态-动作对的Q值以协商交叉路口 [[87](#bib.bib87)]， | 作者定义的Creep-Go动作使车辆在空间和视野受限的交叉路口更安全地行驶
    |'
- en: '| Motion Planning | Authors [[88](#bib.bib88)] propose an improved $A^{\ast}$
    algorithm to learn a heuristic function using deep neural netowrks over image-based
    input obstacle map | Smooth control behavior of vehicle and better peformance
    compared to multi-step DQN |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 运动规划 | 作者 [[88](#bib.bib88)] 提出了改进的$A^{\ast}$算法，通过深度神经网络在基于图像的输入障碍图上学习启发式函数
    | 相比于多步DQN，该方法能提供平滑的车辆控制行为和更好的性能 |'
- en: 'TABLE I: List of AD tasks that require D(RL) to learn a policy or behavior.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：需要D(RL)来学习策略或行为的自动驾驶任务列表。
- en: Motion planning is the task of ensuring the existence of a path between target
    and destination points. This is necessary to plan trajectories for vehicles over
    prior maps usually augmented with semantic information. Path planning in dynamic
    environments and varying vehicle dynamics is a key problem in autonomous driving,
    for example negotiating right to pass through in an intersection [[87](#bib.bib87)],
    merging into highways. Recent work by authors [[89](#bib.bib89)] contains real
    world motions by various traffic actors, observed in diverse interactive driving
    scenarios. Recently, authors demonstrated an application of DRL (DDPG) for AD
    using a full-sized autonomous vehicle [[90](#bib.bib90)]. The system was first
    trained in simulation, before being trained in real time using on board computers,
    and was able to learn to follow a lane, successfully completing a real-world trial
    on a 250 metre section of road. Model-based deep RL algorithms have been proposed
    for learning models and policies directly from raw pixel inputs [[91](#bib.bib91)],
    [[92](#bib.bib92)]. In [[93](#bib.bib93)], deep neural networks have been used
    to generate predictions in simulated environments over hundreds of time steps.
    RL is also suitable for Control. Classical optimal control methods like LQR/iLQR
    are compared with RL methods in [[94](#bib.bib94)]. Classical RL methods are used
    to perform optimal control in stochastic settings, for example the Linear Quadratic
    Regulator (LQR) in linear regimes and iterative LQR (iLQR) for non-linear regimes
    are utilized. A recent study in [[95](#bib.bib95)] demonstrates that random search
    over the parameters for a policy network can perform as well as LQR.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 运动规划是确保目标点与目的地点之间存在路径的任务。这对于在通常附加了语义信息的先前地图上规划车辆轨迹是必要的。在动态环境和变化的车辆动态中的路径规划是自动驾驶中的关键问题，例如在交叉路口右转通行[[87](#bib.bib87)]，并入高速公路。最近，作者们[[89](#bib.bib89)]的工作展示了各种交通参与者在不同互动驾驶场景中的真实世界运动。最近，作者们演示了在全尺寸自动驾驶汽车上应用
    DRL（DDPG）的案例[[90](#bib.bib90)]。该系统首先在模拟中进行训练，然后使用车载计算机进行实时训练，能够学会跟随车道，并成功完成了 250
    米道路段的实际测试。已经提出了基于模型的深度 RL 算法，用于直接从原始像素输入中学习模型和策略[[91](#bib.bib91)]，[[92](#bib.bib92)]。在[[93](#bib.bib93)]中，深度神经网络被用于生成模拟环境中数百个时间步骤的预测。RL
    也适用于控制。经典的最优控制方法如 LQR/iLQR 与 RL 方法在[[94](#bib.bib94)]中进行比较。经典 RL 方法用于在随机设置中执行最优控制，例如在线性环境中使用线性二次调节器（LQR），在非线性环境中使用迭代
    LQR（iLQR）。[[95](#bib.bib95)]中的最新研究表明，对策略网络参数进行随机搜索可以达到与 LQR 相当的效果。
- en: V-C Simulator & Scenario generation tools
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 模拟器与场景生成工具
- en: '| Simulator | Description |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Simulator | 描述 |'
- en: '| CARLA [[78](#bib.bib78)] | Urban simulator, Camera & LIDAR streams, with
    depth & semantic segmentation, Location information |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CARLA [[78](#bib.bib78)] | 城市模拟器，提供相机和 LIDAR 流，具有深度和语义分割，包含位置信息 |'
- en: '| TORCS [[96](#bib.bib96)] | Racing Simulator, Camera stream, agent positions,
    testing control policies for vehicles |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| TORCS [[96](#bib.bib96)] | 赛车模拟器，提供相机流、代理位置，测试车辆控制策略 |'
- en: '| AIRSIM [[97](#bib.bib97)] | Camera stream with depth and semantic segmentation,
    support for drones |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AIRSIM [[97](#bib.bib97)] | 提供深度和语义分割的相机流，支持无人机 |'
- en: '| GAZEBO (ROS) [[98](#bib.bib98)] | Multi-robot physics simulator employed
    for path planning & vehicle control in complex 2D & 3D maps |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GAZEBO (ROS) [[98](#bib.bib98)] | 多机器人物理模拟器，用于复杂 2D 和 3D 地图中的路径规划和车辆控制 |'
- en: '| SUMO [[99](#bib.bib99)] | Macro-scale modelling of traffic in cities motion
    planning simulators are used |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| SUMO [[99](#bib.bib99)] | 城市交通的宏观规模建模，使用运动规划模拟器 |'
- en: '| DeepDrive [[100](#bib.bib100)] | Driving simulator based on unreal, providing
    multi-camera (eight) stream with depth |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DeepDrive [[100](#bib.bib100)] | 基于 Unreal 的驾驶模拟器，提供多相机（八个）流和深度信息 |'
- en: '| Constellation [[101](#bib.bib101)] | NVIDIA DRIVE Constellation^(TM) simulates
    camera, LIDAR & radar for AD (Proprietary) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Constellation [[101](#bib.bib101)] | NVIDIA DRIVE Constellation^(TM) 模拟相机、LIDAR
    和雷达，用于自动驾驶（专有） |'
- en: '| MADRaS [[102](#bib.bib102)] | Multi-Agent Autonomous Driving Simulator built
    on top of TORCS |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MADRaS [[102](#bib.bib102)] | 基于 TORCS 的多智能体自动驾驶模拟器 |'
- en: '| Flow [[103](#bib.bib103)] | Multi-Agent Traffic Control Simulator built on
    top of SUMO |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Flow [[103](#bib.bib103)] | 基于 SUMO 的多智能体交通控制模拟器 |'
- en: '| Highway-env [[104](#bib.bib104)] | A gym-based environment that provides
    a simulator for highway based road topologies |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Highway-env [[104](#bib.bib104)] | 基于健身房环境的模拟器，提供高速公路道路拓扑 |'
- en: '| Carcraft | Waymo’s simulation environment (Proprietary) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Carcraft | Waymo 的模拟环境（专有） |'
- en: 'TABLE II: Simulators for RL applications in advanced driving assistance systems
    (ADAS) and autonomous driving.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：高级驾驶辅助系统 (ADAS) 和自动驾驶中的强化学习应用模拟器。
- en: 'Autonomous driving datasets address supervised learning setup with training
    sets containing image, label pairs for various modalities. Reinforcement learning
    requires an environment where state-action pairs can be recovered while modelling
    dynamics of the vehicle state, environment as well as the stochasticity in the
    movement and actions of the environment and agent respectively. Various simulators
    are actively used for training and validating reinforcement learning algorithms.
    Table [II](#S5.T2 "TABLE II ‣ V-C Simulator & Scenario generation tools ‣ V Reinforcement
    learning for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") summarises various high fidelity perception simulators capable
    of simulating cameras, LiDARs and radar. Some simulators are also capable of providing
    the vehicle state and dynamics. A complete review of sensors and simulators utilised
    within the autonomous driving community is available in [[105](#bib.bib105)] for
    readers. Learned driving policies are stress tested in simulated environments
    before moving on to costly evaluations in the real world. Multi-fidelity reinforcement
    learning (MFRL) framework is proposed in [[106](#bib.bib106)] where multiple simulators
    are available. In MFRL, a cascade of simulators with increasing fidelity are used
    in representing state dynamics (and thus computational cost) that enables the
    training and validation of RL algorithms, while finding near optimal policies
    for the real world with fewer expensive real world samples using a remote controlled
    car. CARLA Challenge [[107](#bib.bib107)] is a Carla simulator based AD competition
    with pre-crash scenarios characterized in a National Highway Traffic Safety Administration
    report [[108](#bib.bib108)]. The systems are evaluated in critical scenarios such
    as: Ego-vehicle loses control, ego-vehicle reacts to unseen obstacle, lane change
    to evade slow leading vehicle among others. The scores of agents are evaluated
    as a function of the aggregated distance travelled in different circuits, and
    total points discounted due to infractions.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '自动驾驶数据集针对有监督学习设置，包含各种模式下的图像和标签对。强化学习需要一个环境，在这个环境中可以恢复状态-动作对，同时对车辆状态、环境的动态以及环境和代理的动作和运动的随机性进行建模。各种模拟器被积极用于训练和验证强化学习算法。表格
    [II](#S5.T2 "TABLE II ‣ V-C Simulator & Scenario generation tools ‣ V Reinforcement
    learning for Autonomous driving tasks ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") 总结了各种高保真感知模拟器，能够模拟相机、激光雷达和雷达。一些模拟器还能够提供车辆状态和动态。有关自动驾驶社区中使用的传感器和模拟器的完整综述，请参阅
    [[105](#bib.bib105)]。在进行昂贵的现实世界评估之前，学习到的驾驶策略会在模拟环境中经过压力测试。多保真度强化学习 (MFRL) 框架在
    [[106](#bib.bib106)] 中被提出，其中提供了多个模拟器。在 MFRL 中，使用一系列具有递增保真度的模拟器来表示状态动态（从而计算成本），这使得能够在使用远程控制车辆的情况下，进行强化学习算法的训练和验证，并找到接近最优的现实世界策略，而不需要大量昂贵的现实世界样本。CARLA
    Challenge [[107](#bib.bib107)] 是一个基于 Carla 模拟器的自动驾驶竞赛，包含了在国家公路交通安全管理局报告 [[108](#bib.bib108)]
    中描述的碰撞前场景。系统在关键场景中进行评估，例如：自车失控、自车对未见障碍物的反应、变道以避开慢速前车等。代理的得分根据在不同赛道上行驶的累计距离和因违规扣分的总分进行评估。'
- en: V-D LfD and IRL for AD applications
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D LfD 和 IRL 在自动驾驶应用中的应用
- en: Early work on Behavior Cloning (BC) for driving cars in [[109](#bib.bib109)],
    [[110](#bib.bib110)] presented agents that learn form demonstrations (LfD) that
    tries to mimic the behavior of an expert. BC is typically implemented as a supervised
    learning, and accordingly, it is hard for BC to adapt to new, unseen situations.
    An architecture for learning a convolutional neural network, end to end, in self-driving
    cars domain was proposed in [[111](#bib.bib111), [112](#bib.bib112)]. The CNN
    is trained to map raw pixels from a single front facing camera directly to steering
    commands. Using a relatively small training dataset from humans/experts, the system
    learns to drive in traffic on local roads with or without lane markings and on
    highways. The network learns image representations that detect the road successfully,
    without being explicitly trained to do so. Authors of [[113](#bib.bib113)] proposed
    to learn comfortable driving trajectories optimization using expert demonstration
    from human drivers using Maximum Entropy Inverse RL. Authors of [[114](#bib.bib114)]
    used DQN as the refinement step in IRL to extract the rewards, in an effort learn
    human-like lane change behavior.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于驾驶汽车的行为克隆（BC）研究 [[109](#bib.bib109)], [[110](#bib.bib110)] 提出了从示范中学习（LfD）的代理，试图模仿专家的行为。BC
    通常被实施为监督学习，因此 BC 很难适应新的、未见过的情况。[[111](#bib.bib111), [112](#bib.bib112)] 中提出了一种在自动驾驶汽车领域端到端学习卷积神经网络的架构。CNN
    被训练直接将来自单个前置摄像头的原始像素映射到转向指令。利用来自人类/专家的相对较小的训练数据集，系统学会在有或没有车道标记的地方的地方道路和高速公路上驾驶。网络学习图像表示，成功地检测到道路，而无需明确训练来实现这一点。[[113](#bib.bib113)]
    的作者提出了使用最大熵逆强化学习（Maximum Entropy Inverse RL）通过人类驾驶员的专家示范来优化舒适驾驶轨迹。[[114](#bib.bib114)]
    的作者使用 DQN 作为逆强化学习中的精炼步骤来提取奖励，努力学习类似人类的变道行为。
- en: VI Real world challenges and future perspectives
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 现实世界挑战与未来展望
- en: In this section, challenges for conducting reinforcement learning for real-world
    autonomous driving are presented and discussed along with the related research
    approaches for solving them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了进行强化学习以应对现实世界自动驾驶中的挑战，并探讨了解决这些挑战的相关研究方法。
- en: VI-A Validating RL systems
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 验证强化学习系统
- en: Henderson et al. [[115](#bib.bib115)] described challenges in validating reinforcement
    learning methods focusing on policy gradient methods for continuous control algorithms
    such as PPO, DDPG and TRPO as well as in reproducing benchmarks. They demonstrate
    with real examples that implementations often have varying code-bases and different
    hyper-parameter values, and that unprincipled ways to estimate the top-k rollouts
    could lead to incoherent interpretations on the performance of the reinforcement
    learning algorithms, and further more on how well they generalize. Authors concluded
    that evaluation could be performed either on a well defined common setup or on
    real-world tasks. Authors in [[116](#bib.bib116)] proposed automated generation
    of challenging and rare driving scenarios in high-fidelity photo-realistic simulators.
    These adversarial scenarios are automatically discovered by parameterising the
    behavior of pedestrians and other vehicles on the road. Moreover, it is shown
    that by adding these scenarios to the training data of imitation learning, the
    safety is increased.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Henderson 等人 [[115](#bib.bib115)] 描述了在验证强化学习方法中的挑战，重点关注连续控制算法中的策略梯度方法，如 PPO、DDPG
    和 TRPO 以及基准测试的重现。他们通过实际例子展示了实现中往往有不同的代码库和超参数值，且不规范的 top-k rollout 估计方法可能导致对强化学习算法性能的解释不一致，进而影响对其泛化能力的评价。作者总结道，评估可以在明确的共同设置下或在现实世界任务中进行。[[116](#bib.bib116)]
    的作者提出了在高保真照片级真实感模拟器中自动生成具有挑战性和稀有的驾驶场景。这些对抗性场景通过对道路上行人和其他车辆的行为进行参数化自动发现。此外，研究表明，通过将这些场景添加到模仿学习的训练数据中，可以提高安全性。
- en: VI-B Bridging the simulation-reality gap
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 弥合模拟与现实之间的差距
- en: Simulation-to-real-world transfer learning is an active domain, since simulations
    are a source large & cheap data with perfect annotations. Authors [[117](#bib.bib117)]
    train a robot arm to grasp objects in the real world by performing domain adaption
    from simulation-to-reality, at both feature-level and pixel-level. The vision-based
    grasping system achieved comparable performance with 50 times fewer real-world
    samples. Authors in [[118](#bib.bib118)], randomized the dynamics of the simulator
    during training. The resulting policies were capable of generalising to different
    dynamics without requiring retraining on real system. In the domain of autonomous
    driving, authors [[119](#bib.bib119)] train an A3C agent using simulation-real
    translated images of the driving environment. Following which, the trained policy
    was evaluated on a real world driving dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从模拟到现实世界的迁移学习是一个活跃的领域，因为模拟提供了大量且便宜的数据，并且注释完美。作者[[117](#bib.bib117)]通过在特征级别和像素级别上执行从模拟到现实的领域适应，训练机器人手臂在现实世界中抓取物体。基于视觉的抓取系统在实际样本减少50倍的情况下达到了可比的性能。作者[[118](#bib.bib118)]在训练过程中对模拟器的动态进行了随机化。结果策略能够对不同的动态进行泛化，而无需在真实系统上重新训练。在自动驾驶领域，作者[[119](#bib.bib119)]使用模拟-现实转换的驾驶环境图像训练A3C代理。训练后的策略在实际驾驶数据集上进行了评估。
- en: Authors in [[120](#bib.bib120)] addressed the issue of performing imitation
    learning in simulation that transfers well to images from real world. They achieved
    this by unsupervised domain translation between simulated and real world images,
    that enables learning the prediction of steering in the real world domain with
    only ground truth from the simulated domain. Authors remark that there were no
    pairwise correspondences between images in the simulated training set and the
    unlabelled real-world image set. Similarly, [[121](#bib.bib121)] performs domain
    adaptation to map real world images to simulated images. In contrast to sim-to-real
    methods they handle the reality gap during deployment of agents in real scenarios,
    by adapting the real camera streams to the synthetic modality, so as to map the
    unfamiliar or unseen features of real images back into the simulated environment
    and states. The agents have already learnt a policy in simulation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 作者[[120](#bib.bib120)]解决了在模拟中执行模仿学习并很好地转移到现实世界图像的问题。他们通过在模拟和现实世界图像之间进行无监督的领域转换实现了这一目标，这使得在现实世界领域中仅通过模拟领域的真实标签进行方向预测成为可能。作者指出，模拟训练集中的图像与未标记的现实世界图像集之间没有一对一的对应关系。同样，[[121](#bib.bib121)]执行领域适应以将现实世界图像映射到模拟图像。与模拟到现实方法不同，他们在现实场景中部署代理时处理现实差距，通过将真实摄像头流调整为合成模式，以将现实图像中的陌生或未见特征映射回模拟环境和状态。代理已经在模拟中学习了一个策略。
- en: VI-C Sample efficiency
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 样本效率
- en: Animals are usually able to learn new tasks in just a few trials, benefiting
    from their prior knowledge about the environment. However, one of the key challenges
    for reinforcement learning is sample efficiency. The learning process requires
    too many samples to learn a reasonable policy. This issue becomes more noticeable
    when collection of valuable experience is expensive or even risky to acquire.
    In the case of robot control and autonomous driving, sample efficiency is a difficult
    issue due to the delayed and sparse rewards found in typical settings, along with
    an unbalanced distribution of observations in a large state space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 动物通常能够在仅仅几次尝试中学习新任务，这得益于它们对环境的先前知识。然而，强化学习面临的关键挑战之一是样本效率。学习过程需要过多的样本来学习合理的策略。当获取有价值经验的过程昂贵或甚至具有风险时，这个问题变得更加明显。在机器人控制和自动驾驶的情况下，由于典型设置中存在延迟和稀疏的奖励，以及在大状态空间中观察分布的不均衡，样本效率是一个困难的问题。
- en: Reward shaping enables the agent to learn intermediate goals by designing a
    more frequent reward function to encourage the agent to learn faster from fewer
    samples. Authors in [[122](#bib.bib122)] design a second ”trauma” replay memory
    that contains only collision situations in order to pool positive and negative
    experiences at each training step.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励塑造通过设计更频繁的奖励函数，使代理能够学习中间目标，从而鼓励代理从较少的样本中更快地学习。作者[[122](#bib.bib122)]设计了第二个“创伤”重放记忆，只包含碰撞情况，以便在每个训练步骤中汇集正负经验。
- en: 'IL boostrapped RL: Further efficiency can be achieved where the agent first
    learns an initial policy offline performing imitation learning from roll-outs
    provided by an expert. Following which, the agent can self-improve by applying
    RL while interacting with the environment.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: IL引导的RL：进一步的效率可以通过代理首先在离线状态下从专家提供的回合中进行模仿学习来实现。随后，代理可以通过在与环境交互的过程中应用RL来自我提升。
- en: Actor Critic with Experience Replay (ACER) [[123](#bib.bib123)], is a sample-efficient
    policy gradient algorithm that makes use of a replay buffer, enabling it to perform
    more than one gradient update using each piece of sampled experience, as well
    as a trust region policy optimization method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放中的演员-评论家（ACER）[[123](#bib.bib123)]是一种样本高效的策略梯度算法，利用回放缓冲区，使其能够使用每个样本经验进行多次梯度更新，并且使用信任区域策略优化方法。
- en: Transfer learning is another approach for sample efficiency, which enables the
    reuse of previously trained policy for a source task to initialize the learning
    of a target task. Policy composition presented in [[124](#bib.bib124)] propose
    composing previously learned basis policies to be able to reuse them for a novel
    task, which leads to faster learning of new policies. A survey on transfer learning
    in RL is presented in [[125](#bib.bib125)]. Multi-fidelity reinforcement learning
    (MFRL) framework [[106](#bib.bib106)] showed to transfer heuristics to guide exploration
    in high fidelity simulators and find near optimal policies for the real world
    with fewer real world samples. Authors in [[126](#bib.bib126)] transferred policies
    learnt to handle simulated intersections to real world examples between DQN agents.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是另一种提高样本效率的方法，它使得可以重用先前训练的源任务策略来初始化目标任务的学习。[[124](#bib.bib124)]中提出的策略组合方法建议将以前学到的基础策略组合起来，以便能够在新任务中重用它们，从而加快新策略的学习。[[125](#bib.bib125)]中提供了关于RL中迁移学习的调查。多保真度强化学习（MFRL）框架[[106](#bib.bib106)]展示了如何将启发式方法转移到高保真模拟器中，以更少的实际样本找到接近最优的实际世界策略。[[126](#bib.bib126)]的作者将学到的处理模拟交叉口的策略转移到DQN代理之间的实际世界示例中。
- en: Meta-learning algorithms enable agents adapt to new tasks and learn new skills
    rapidly from small amounts of experiences, benefiting from their prior knowledge
    about the world. Authors of [[127](#bib.bib127)] addressed this issue through
    training a recurrent neural network on a training set of interrelated tasks, where
    the network input includes the action selected in addition to the reward received
    in the previous time step. Accordingly, the agent is trained to learn to exploit
    the structure of the problem dynamically and solve new problems by adjusting its
    hidden state. A similar approach for designing RL algorithms is presented in [[128](#bib.bib128)].
    Rather than designing a “fast” reinforcement learning algorithm, it is represented
    as a recurrent neural network, and learned from data. In Model-Agnostic Meta-Learning
    (MAML) proposed in [[129](#bib.bib129)], the meta-learner seeks to find an initialisation
    for the parameters of a neural network, that can be adapted quickly for a new
    task using only few examples. Reptile [[130](#bib.bib130)] includes a similar
    model. Authors [[131](#bib.bib131)] present simple gradient-based meta-learning
    algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习算法使代理能够快速适应新任务并从少量经验中学习新技能，受益于其对世界的先前知识。[[127](#bib.bib127)]的作者通过在一个互相关联任务的训练集上训练递归神经网络来解决这个问题，其中网络输入除了包含选择的动作外，还包括之前时间步收到的奖励。因此，代理被训练以动态地利用问题的结构，并通过调整其隐藏状态来解决新问题。[[128](#bib.bib128)]中提出了类似的RL算法设计方法。它不是设计一个“快速”的强化学习算法，而是表示为一个递归神经网络，并从数据中学习。在[[129](#bib.bib129)]中提出的模型无关元学习（MAML）中，元学习者试图为神经网络的参数找到一个初始化，使其能够通过仅使用少量示例快速适应新任务。Reptile
    [[130](#bib.bib130)] 包含一个类似的模型。[[131](#bib.bib131)]的作者提出了一个简单的基于梯度的元学习算法。
- en: 'Efficient state representations : World models proposed in [[132](#bib.bib132)]
    learn a compressed spatial and temporal representation of the environment using
    VAEs. Further on a compact and simple policy directly from the compressed state
    representation.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的状态表示：[[132](#bib.bib132)]中提出的世界模型使用VAE学习环境的压缩空间和时间表示。进一步从压缩状态表示中直接获得紧凑和简单的策略。
- en: VI-D Exploration issues with Imitation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 模仿的探索问题
- en: In imitation learning, the agent makes use of trajectories provided by an expert.
    However, the distribution of states the expert encounters usually does not cover
    all the states the trained agent may encounter during testing. Furthermore imitation
    assumes that the actions are independent and identically distributed (i.i.d.).
    One solution consists in using the Data Aggregation (DAgger) methods [[133](#bib.bib133)]
    where the end-to-end learned policy is executed, and extracted observation-action
    pairs are again labelled by the expert, and aggregated to the original expert
    observation-action dataset. Thus, iteratively collecting training examples from
    both reference and trained policies explores more valuable states and solves this
    lack of exploration. Following work on Search-based Structured Prediction (SEARN)
    [[133](#bib.bib133)], Stochastic Mixing Iterative Learning (SMILE) trains a stochastic
    stationary policy over several iterations and then makes use of a geometric stochastic
    mixing of the policies trained. In a standard imitation learning scenario, the
    demonstrator is required to cover sufficient states so as to avoid unseen states
    during test. This constraint is costly and requires frequent human intervention.
    More recently, Chauffeurnet [[134](#bib.bib134)] demonstrated the limits of imitation
    learning where even 30 million state-action samples were insufficient to learn
    an optimal policy that mapped bird-eye view images (states) to control (action).
    The authors propose the use of simulated examples which introduced perturbations,
    higher diversity of scenarios such as collisions and/or going off the road. The
    featurenet includes an agent RNN that outputs the way point, agent box position
    and heading at each iteration. Authors [[135](#bib.bib135)] identify limits of
    imitation learning, and train a DNN end-to-end using the ego vehicles on input
    raw image, and 2d and 3d locations of neighboring vehicles to simultaneously predict
    the ego-vehicle action as well as neighbouring vehicle trajectories.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习中，智能体利用专家提供的轨迹。然而，专家遇到的状态分布通常不覆盖训练的智能体在测试过程中可能遇到的所有状态。此外，模仿学习假设动作是独立且同分布的（i.i.d.）。一种解决方案是使用数据聚合（DAgger）方法[[133](#bib.bib133)]，在这种方法中，端到端学习的策略被执行，提取的观察-动作对再次由专家标注，并与原始专家观察-动作数据集合并。因此，迭代地从参考策略和训练策略中收集训练样本可以探索更有价值的状态，从而解决了这种探索不足的问题。继基于搜索的结构化预测（SEARN）[[133](#bib.bib133)]的工作之后，随机混合迭代学习（SMILE）在多个迭代中训练一个随机的静态策略，然后利用训练出的策略进行几何随机混合。在标准的模仿学习场景中，演示者需要覆盖足够的状态，以避免在测试过程中出现未见的状态。这一限制成本高昂，并且需要频繁的人为干预。最近，Chauffeurnet
    [[134](#bib.bib134)] 展示了模仿学习的局限性，即使是 3000 万个状态-动作样本也不足以学习一个将鸟瞰图像（状态）映射到控制（动作）的最佳策略。作者提出使用模拟示例，这些示例引入了扰动、更高的场景多样性，例如碰撞和/或偏离道路。featurenet
    包括一个智能体 RNN，该 RNN 在每次迭代中输出路线点、智能体框位置和航向。作者[[135](#bib.bib135)] 识别了模仿学习的局限性，并使用输入原始图像的自车、邻近车辆的二维和三维位置，训练一个端到端的
    DNN，以同时预测自车动作和邻近车辆轨迹。
- en: VI-E Intrinsic Reward functions
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 内在奖励函数
- en: In controlled simulated environments such as games, an explicit reward signal
    is given to the agent along with its sensor stream. However, in real-world robotics
    and autonomous driving deriving, designing a good reward functions is essential
    so that the desired behaviour may be learned. The most common solution has been
    reward shaping [[136](#bib.bib136)] and consists in supplying additional well
    designed rewards to the agent to encourage the optimization into the direction
    of the optimal policy. Rewards as already pointed earlier in the paper, could
    be estimated by inverse RL (IRL) [[137](#bib.bib137)], which depends on expert
    demonstrations. In the absence of an explicit reward shaping and expert demonstrations,
    agents can use intrinsic rewards or intrinsic motivation [[138](#bib.bib138)]
    to evaluate if their actions were good or not. Authors of [[139](#bib.bib139)]
    define curiosity as the error in an agent’s ability to predict the consequence
    of its own actions in a visual feature space learned by a self-supervised inverse
    dynamics model. In [[140](#bib.bib140)] the agent learns a next state predictor
    model from its experience, and uses the error of the prediction as an intrinsic
    reward. This enables that agent to determine what could be a useful behavior even
    without extrinsic rewards.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在受控的模拟环境中，如游戏，系统会将明确的奖励信号与传感器流一同提供给智能体。然而，在现实世界的机器人技术和自动驾驶中，设计良好的奖励函数是至关重要的，以便能够学习到期望的行为。最常见的解决方案是奖励塑形[[136](#bib.bib136)]，其包括向智能体提供额外精心设计的奖励，以鼓励优化朝向最优策略的方向。如论文早前所述，奖励可以通过逆强化学习（IRL）[[137](#bib.bib137)]来估计，这依赖于专家演示。在缺乏明确的奖励塑形和专家演示的情况下，智能体可以使用内在奖励或内在动机[[138](#bib.bib138)]来评估其行为是否良好。[[139](#bib.bib139)]的作者将好奇心定义为智能体在自监督逆向动力学模型中学习的视觉特征空间中预测自己行动后果的能力的误差。在[[140](#bib.bib140)]中，智能体从经验中学习下一状态预测模型，并使用预测误差作为内在奖励。这使得智能体能够在没有外在奖励的情况下确定哪些行为可能是有用的。
- en: VI-F Incorporating safety in DRL
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-F 将安全性纳入深度强化学习
- en: Deploying an autonomous vehicle in real environments after training directly
    could be dangerous. Different approaches to incorporate safety into DRL algorithms
    are presented here. For imitation learning based systems, Safe DAgger [[141](#bib.bib141)]
    introduces a safety policy that learns to predict the error made by a primary
    policy trained initially with the supervised learning approach, without querying
    a reference policy. An additional safe policy takes both the partial observation
    of a state and a primary policy as inputs, and returns a binary label indicating
    whether the primary policy is likely to deviate from a reference policy without
    querying it. Authors of [[142](#bib.bib142)] addressed safety in multi-agent Reinforcement
    Learning for Autonomous Driving, where a balance is maintained between unexpected
    behavior of other drivers or pedestrians and not to be too defensive, so that
    normal traffic flow is achieved. While hard constraints are maintained to guarantee
    the safety of driving, the problem is decomposed into a composition of a policy
    for desires to enable comfort driving and trajectory planning. The deep reinforcement
    learning algorithms for control such as DDPG and safety based control are combined
    in [[143](#bib.bib143)], including artificial potential field method that is widely
    used for robot path planning. Using TORCS environment, the DDPG is applied first
    for learning a driving policy in a stable and familiar environment, then policy
    network and safety-based control are combined to avoid collisions. It was found
    that combination of DRL and safety-based control performs well in most scenarios.
    In order to enable DRL to escape local optima, speed up the training process and
    avoid danger conditions or accidents, Survival-Oriented Reinforcement Learning
    (SORL) model is proposed in [[144](#bib.bib144)], where survival is favored over
    maximizing total reward through modeling the autonomous driving problem as a constrained
    MDP and introducing Negative-Avoidance Function to learn from previous failure.
    The SORL model was found to be not sensitive to reward function and can use different
    DRL algorithms like DDPG. Furthermore, a comprehensive survey on safe reinforcement
    learning can be found in [[145](#bib.bib145)] for interested readers.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 直接在真实环境中部署经过训练的自动驾驶车辆可能是危险的。这里介绍了将安全性纳入深度强化学习（DRL）算法的不同方法。对于基于模仿学习的系统，Safe DAgger
    [[141](#bib.bib141)] 引入了一种安全策略，该策略学习预测由初始用监督学习方法训练的主要策略所犯的错误，而无需查询参考策略。额外的安全策略将状态的部分观察和主要策略作为输入，并返回一个二进制标签，指示主要策略是否可能偏离参考策略而无需查询它。[[142](#bib.bib142)]
    的作者解决了多智能体强化学习中的安全问题，在确保其他驾驶员或行人行为意外与不过于防御之间保持平衡，从而实现正常的交通流动。在保持硬性约束以保证驾驶安全的同时，该问题被分解为实现舒适驾驶和轨迹规划的愿望策略组合。[[143](#bib.bib143)]
    中结合了用于控制的深度强化学习算法，如DDPG和基于安全的控制，包括广泛用于机器人路径规划的人工势场法。通过使用TORCS环境，首先应用DDPG在稳定且熟悉的环境中学习驾驶策略，然后将策略网络和基于安全的控制结合起来以避免碰撞。研究发现，DRL和基于安全的控制的组合在大多数场景中表现良好。为了使DRL逃离局部最优，加速训练过程并避免危险条件或事故，[[144](#bib.bib144)]
    中提出了生存导向强化学习（SORL）模型，其中生存优于通过将自动驾驶问题建模为受限的MDP并引入负避让函数来学习从以前的失败中。发现SORL模型对奖励函数不敏感，并且可以使用不同的DRL算法，如DDPG。此外，感兴趣的读者可以在
    [[145](#bib.bib145)] 中找到有关安全强化学习的全面综述。
- en: VI-G Multi-agent reinforcement learning
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-G 多智能体强化学习
- en: Autonomous driving is a fundamentally multi-agent task; as well as the ego vehicle
    being controlled by an agent, there will also be many other actors present in
    simulated and real world autonomous driving settings, such as pedestrians, cyclists
    and other vehicles. Therefore, the continued development of explicitly multi-agent
    approaches to learning to drive autonomous vehicles is an important future research
    direction. Several prior methods have already approached the autonomous driving
    problem using a MARL perspective, e.g. [[142](#bib.bib142), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶本质上是一个多智能体任务；除了由智能体控制的自驾车外，模拟和现实世界的自动驾驶环境中还会有许多其他参与者，例如行人、骑车人和其他车辆。因此，继续发展明确的多智能体方法来学习如何驾驶自动车辆是一个重要的未来研究方向。一些先前的方法已经使用多智能体强化学习（MARL）视角来处理自动驾驶问题，例如
    [[142](#bib.bib142), [146](#bib.bib146), [147](#bib.bib147), [148](#bib.bib148),
    [149](#bib.bib149)]。
- en: One important area where MARL techniques could be very beneficial is in high-level
    decision making and coordination between groups of autonomous vehicles, in scenarios
    such as overtaking in highway scenarios [[149](#bib.bib149)], or negotiating intersections
    without signalised control. Another area where MARL approaches could be of benefit
    is in the development of adversarial agents for testing autonomous driving policies
    before deployment [[148](#bib.bib148)], i.e. agents controlling other vehicles
    in a simulation that learn to expose weaknesses in the behaviour of autonomous
    driving policies by acting erratically or against the rules of the road. Finally,
    MARL approaches could potentially have an important role to play in developing
    safe policies for autonomous driving [[142](#bib.bib142)], as discussed earlier.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: MARL 技术可能在高级决策制定和自主车辆群体之间的协调方面非常有益，例如在高速公路场景中的超车 [[149](#bib.bib149)]，或在没有信号控制的交叉口进行协商。另一个MARL方法可能有益的领域是在部署前测试自主驾驶政策的对抗代理的开发
    [[148](#bib.bib148)]，即在模拟中控制其他车辆的代理，通过不规则或违反道路规则的行为来暴露自主驾驶政策的弱点。最后，MARL方法可能在制定自主驾驶的安全政策方面发挥重要作用
    [[142](#bib.bib142)]，正如前文所述。
- en: '| Framework | Description |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 描述 |'
- en: '| OpenAI Baselines [[150](#bib.bib150)] | Set of high-quality implementations
    of different RL and DRL algorithms. The main goal for these Baselines is to make
    it easier for the research community to replicate, refine and create reproducible
    research. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI Baselines [[150](#bib.bib150)] | 一组高质量的不同强化学习和深度强化学习算法的实现。这些 Baselines
    的主要目标是使研究社区更容易复制、改进和创建可重复的研究。 |'
- en: '| Unity ML Agents Toolkit [[151](#bib.bib151)] | Implements core RL algorithms,
    games, simulations environments for training RL or IL based agents . |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Unity ML Agents Toolkit [[151](#bib.bib151)] | 实现了核心的强化学习算法、游戏和模拟环境，用于训练基于强化学习或模仿学习的代理。
    |'
- en: '| RL Coach [[152](#bib.bib152)] | Intel AI Lab’s implementation of modular
    RL algorithms implementation with simple integration of new environments by extending
    and reusing existing components. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| RL Coach [[152](#bib.bib152)] | 英特尔 AI 实验室对模块化强化学习算法的实现，通过扩展和重用现有组件来简单地集成新环境。
    |'
- en: '| Tensorflow Agents [[153](#bib.bib153)] | RL algorithms package with Bandits
    from TF. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Tensorflow Agents [[153](#bib.bib153)] | 具有 Bandits 的强化学习算法包。 |'
- en: '| rlpyt [[154](#bib.bib154)] | implements deep Q-learning, policy gradients
    algorithm families in a single python package |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| rlpyt [[154](#bib.bib154)] | 在一个 Python 包中实现了深度 Q 学习、策略梯度算法系列 |'
- en: '| bsuite [[155](#bib.bib155)] | DeepMind Behaviour Suite for Reinforcement
    Learning aims at defining metrics for RL agents. Automating evaluation and analysis.
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| bsuite [[155](#bib.bib155)] | DeepMind 行为套件用于强化学习，旨在定义强化学习代理的指标。自动化评估和分析。
    |'
- en: 'TABLE III: Open-source frameworks and packages for state of the art RL/DRL
    algorithms and evaluation.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：用于最先进的强化学习/深度强化学习算法和评估的开源框架和软件包。
- en: VII Conclusion
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: Reinforcement learning is still an active and emerging area in real-world autonomous
    driving applications. Although there are a few successful commercial applications,
    there is very little literature or large-scale public datasets available. Thus
    we were motivated to formalize and organize RL applications for autonomous driving.
    Autonomous driving scenarios involve interacting agents and require negotiation
    and dynamic decision making which suits RL. However, there are many challenges
    to be resolved in order to have mature solutions which we discuss in detail. In
    this work, a detailed theoretical reinforcement learning is presented, along with
    a comprehensive literature survey about applying RL for autonomous driving tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习仍然是现实世界自主驾驶应用中的一个活跃且新兴的领域。尽管有一些成功的商业应用，但相关文献或大规模公开数据集非常有限。因此，我们受到启发，决定对自主驾驶的强化学习应用进行形式化和组织。自主驾驶场景涉及交互代理，需要协商和动态决策，这非常适合强化学习。然而，为了获得成熟的解决方案，还需要解决许多挑战，我们在文中进行了详细讨论。在这项工作中，我们呈现了详细的理论强化学习，并对应用强化学习于自主驾驶任务的文献进行了全面的调查。
- en: 'Challenges, future research directions and opportunities are discussed in section
    [VI](#S6 "VI Real world challenges and future perspectives ‣ Deep Reinforcement
    Learning for Autonomous Driving: A Survey"). This includes : validating the performance
    of RL based systems, the simulation-reality gap, sample efficiency, designing
    good reward functions, incorporating safety into decision making RL systems for
    autonomous agents.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '挑战、未来研究方向和机会在章节 [VI](#S6 "VI Real world challenges and future perspectives
    ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey") 中进行了讨论。这包括：验证基于强化学习的系统性能、模拟与现实之间的差距、样本效率、设计良好的奖励函数、将安全性融入自主代理的决策强化学习系统中。'
- en: 'Reinforcement learning results are usually difficult to reproduce and are highly
    sensitive to hyper-parameter choices, which are often not reported in detail.
    Both researchers and practitioners need to have a reliable starting point where
    the well known reinforcement learning algorithms are implemented, documented and
    well tested. These frameworks have been covered in table [III](#S6.T3 "TABLE III
    ‣ VI-G Multi-agent reinforcement learning ‣ VI Real world challenges and future
    perspectives ‣ Deep Reinforcement Learning for Autonomous Driving: A Survey").'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '强化学习结果通常难以复现，并且对超参数选择非常敏感，这些超参数通常没有详细报告。研究人员和实践者需要一个可靠的起点，在那里，已知的强化学习算法得到实现、记录和充分测试。这些框架已在表
    [III](#S6.T3 "TABLE III ‣ VI-G Multi-agent reinforcement learning ‣ VI Real world
    challenges and future perspectives ‣ Deep Reinforcement Learning for Autonomous
    Driving: A Survey") 中进行了介绍。'
- en: The development of explicitly multi-agent reinforcement learning approaches
    to the autonomous driving problem is also an important future challenge that has
    not received a lot of attention to date. MARL techniques have the potential to
    make coordination and high-level decision making between groups of autonomous
    vehicles easier, as well as providing new opportunities for testing and validating
    the safety of autonomous driving policies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 显式多智能体强化学习方法在自主驾驶问题上的发展也是一个重要的未来挑战，目前尚未受到很多关注。多智能体强化学习技术有潜力使得自主车辆群体之间的协调和高层决策变得更加容易，并为测试和验证自主驾驶策略的安全性提供了新的机会。
- en: Furthermore, implementation of RL algorithms is a challenging task for researchers
    and practitioners. This work presents examples of well known and active open-source
    RL frameworks that provide well documented implementations that enables the opportunity
    of using, evaluating and extending different RL algorithms. Finally, We hope that
    this overview paper encourages further research and applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，强化学习算法的实现对研究人员和实践者来说是一项具有挑战性的任务。本文介绍了几个著名的、活跃的开源强化学习框架，这些框架提供了文档齐全的实现，便于使用、评估和扩展不同的强化学习算法。最后，我们希望这篇概述性论文能激励更多的研究和应用。
- en: '| A2C, A3C | Advantage Actor Critic, Asynchronous A2C |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| A2C, A3C | 优势演员评论家，异步A2C |'
- en: '| BC | Behavior Cloning |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| BC | 行为克隆 |'
- en: '| DDPG | Deep DPG |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| DDPG | 深度确定性策略梯度 |'
- en: '| DP | Dynamic Programming |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DP | 动态规划 |'
- en: '| DPG | Deterministic PG |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| DPG | 确定性策略梯度 |'
- en: '| DQN | Deep Q-Network |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| DQN | 深度Q网络 |'
- en: '| DRL | Deep RL |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DRL | 深度强化学习 |'
- en: '| IL | Imitation Learning |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| IL | 模仿学习 |'
- en: '| IRL | Inverse RL |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| IRL | 逆向强化学习 |'
- en: '| LfD | Learning from Demonstration |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| LfD | 从示范中学习 |'
- en: '| MAML | Model-Agnostic Meta-Learning |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| MAML | 模型无关的元学习 |'
- en: '| MARL | Multi-Agent RL |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| MARL | 多智能体强化学习 |'
- en: '| MDP | Markov Decision Process |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| MDP | 马尔可夫决策过程 |'
- en: '| MOMDP | Multi-Objective MDP |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| MOMDP | 多目标马尔可夫决策过程 |'
- en: '| MOSG | Multi-Objective SG |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| MOSG | 多目标SG |'
- en: '| PG | Policy Gradient |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| PG | 策略梯度 |'
- en: '| POMDP | Partially Observed MDP |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| POMDP | 部分可观察马尔可夫决策过程 |'
- en: '| PPO | Proximal Policy Optimization |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| PPO | 近端策略优化 |'
- en: '| QL | Q-Learning |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| QL | Q-学习 |'
- en: '| RRT | Rapidly-exploring Random Trees |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| RRT | 快速探索随机树 |'
- en: '| SG | Stochastic Game |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| SG | 随机博弈 |'
- en: '| SMDP | Semi-Markov Decision Process |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| SMDP | 半马尔可夫决策过程 |'
- en: '| TDL | Time Difference Learning |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| TDL | 时间差分学习 |'
- en: '| TRPO | Trust Region Policy Optimization |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| TRPO | 信赖域策略优化 |'
- en: 'TABLE IV: Acronyms related to Reinforcement learning (RL).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 与强化学习（RL）相关的缩写。'
- en: References
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction
    (Second Edition)*.   MIT Press, 2018.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. S. Sutton 和 A. G. Barto, *强化学习：导论（第二版）*。MIT出版社，2018年。'
- en: '[2] V. Talpaert., I. Sobh., B. R. Kiran., P. Mannion., S. Yogamani., A. El-Sallab.,
    and P. Perez., “Exploring applications of deep reinforcement learning for real-world
    autonomous driving systems,” in *Proceedings of the 14th International Joint Conference
    on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume
    5 VISAPP: VISAPP,*, INSTICC.   SciTePress, 2019, pp. 564–572.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] V. Talpaert., I. Sobh., B. R. Kiran., P. Mannion., S. Yogamani., A. El-Sallab.,
    和 P. Perez.，“探索深度强化学习在现实世界自动驾驶系统中的应用”，见 *第14届国际计算机视觉、图像与计算机图形理论与应用联合会议论文集 - 第5卷
    VISAPP: VISAPP*，INSTICC。SciTePress，2019年，第564–572页。'
- en: '[3] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani, “Deep semantic segmentation
    for automated driving: Taxonomy, roadmap and challenges,” in *2017 IEEE 20th international
    conference on intelligent transportation systems (ITSC)*.   IEEE, 2017, pp. 1–8.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Siam, S. Elkerdawy, M. Jagersand, 和 S. Yogamani，“自动驾驶中的深度语义分割：分类、路线图和挑战”，见
    *2017 IEEE第20届智能交通系统国际会议（ITSC）*。IEEE，2017年，第1–8页。'
- en: '[4] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, and S. Yogamani,
    “Rgb and lidar fusion based 3d semantic segmentation for autonomous driving,”
    in *2019 IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE, 2019,
    pp. 7–12.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, 和 S. Yogamani，“基于RGB和激光雷达融合的自动驾驶三维语义分割”，见
    *2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019年，第7–12页。'
- en: '[5] M. Siam, H. Mahgoub, M. Zahran, S. Yogamani, M. Jagersand, and A. El-Sallab,
    “Modnet: Motion and appearance based moving object detection network for autonomous
    driving,” in *2018 21st International Conference on Intelligent Transportation
    Systems (ITSC)*.   IEEE, 2018, pp. 2859–2864.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Siam, H. Mahgoub, M. Zahran, S. Yogamani, M. Jagersand, 和 A. El-Sallab，“Modnet：基于运动和外观的移动物体检测网络”，见
    *2018年第21届国际智能交通系统会议（ITSC）*。IEEE，2018年，第2859–2864页。'
- en: '[6] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold, S. Yogamani,
    and T. Pech, “Monocular fisheye camera depth estimation using sparse lidar supervision,”
    in *2018 21st International Conference on Intelligent Transportation Systems (ITSC)*.   IEEE,
    2018, pp. 2853–2858.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold, S. Yogamani,
    和 T. Pech，“基于稀疏激光雷达监督的单目鱼眼相机深度估计”，见 *2018年第21届国际智能交通系统会议（ITSC）*。IEEE，2018年，第2853–2858页。'
- en: '[7] M. Uřičář, P. Křížek, G. Sistu, and S. Yogamani, “Soilingnet: Soiling detection
    on automotive surround-view cameras,” in *2019 IEEE Intelligent Transportation
    Systems Conference (ITSC)*.   IEEE, 2019, pp. 67–72.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Uřičář, P. Křížek, G. Sistu, 和 S. Yogamani，“Soilingnet：汽车全景摄像头上的污垢检测”，见
    *2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019年，第67–72页。'
- en: '[8] G. Sistu, I. Leang, S. Chennupati, S. Yogamani, C. Hughes, S. Milz, and
    S. Rawashdeh, “Neurall: Towards a unified visual perception model for automated
    driving,” in *2019 IEEE Intelligent Transportation Systems Conference (ITSC)*.   IEEE,
    2019, pp. 796–803.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Sistu, I. Leang, S. Chennupati, S. Yogamani, C. Hughes, S. Milz, 和 S.
    Rawashdeh，“Neurall：迈向统一的自动驾驶视觉感知模型”，见 *2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019年，第796–803页。'
- en: '[9] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea, M. Uricár,
    S. Milz, M. Simon, K. Amende *et al.*, “Woodscape: A multi-task, multi-camera
    fisheye dataset for autonomous driving,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 9308–9318.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea, M. Uricár,
    S. Milz, M. Simon, K. Amende *等*，“Woodscape：用于自动驾驶的多任务、多摄像头鱼眼数据集”，见 *IEEE国际计算机视觉会议论文集*，2019年，第9308–9318页。'
- en: '[10] S. Milz, G. Arbeiter, C. Witt, B. Abdallah, and S. Yogamani, “Visual slam
    for automated driving: Exploring the applications of deep learning,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*,
    2018, pp. 247–257.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Milz, G. Arbeiter, C. Witt, B. Abdallah, 和 S. Yogamani，“自动驾驶中的视觉 SLAM：深度学习的应用探讨”，见
    *IEEE计算机视觉与模式识别研讨会论文集*，2018年，第247–257页。'
- en: '[11] S. M. LaValle, *Planning Algorithms*.   New York, NY, USA: Cambridge University
    Press, 2006.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. M. LaValle, *规划算法*。纽约，NY，美国：剑桥大学出版社，2006年。'
- en: '[12] S. M. LaValle and J. James J. Kuffner, “Randomized kinodynamic planning,”
    *The International Journal of Robotics Research*, vol. 20, no. 5, pp. 378–400,
    2001.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. M. LaValle 和 J. James J. Kuffner，“随机化运动规划”，*国际机器人研究杂志*，第20卷，第5期，第378–400页，2001年。'
- en: '[13] T. D. Team. Dimensions publication trends. [Online]. Available: [https://app.dimensions.ai/discover/publication](https://app.dimensions.ai/discover/publication)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. D. Team. Dimensions出版趋势。[在线]。可用：[https://app.dimensions.ai/discover/publication](https://app.dimensions.ai/discover/publication)'
- en: '[14] Y. Kuwata, J. Teo, G. Fiore, S. Karaman, E. Frazzoli, and J. P. How, “Real-time
    motion planning with applications to autonomous urban driving,” *IEEE Transactions
    on Control Systems Technology*, vol. 17, no. 5, pp. 1105–1118, 2009.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Kuwata, J. Teo, G. Fiore, S. Karaman, E. Frazzoli 和 J. P. How, “实时运动规划及其在自动驾驶城市驾驶中的应用，”
    *IEEE控制系统技术汇刊*，第17卷，第5期，第1105–1118页，2009年。'
- en: '[15] B. Paden, M. Čáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of
    motion planning and control techniques for self-driving urban vehicles,” *IEEE
    Transactions on intelligent vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Paden, M. Čáp, S. Z. Yong, D. Yershov 和 E. Frazzoli, “关于自动驾驶城市车辆的运动规划和控制技术的调查，”
    *IEEE智能车辆汇刊*，第1卷，第1期，第33–55页，2016年。'
- en: '[16] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decision-making
    for autonomous vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, no. 0, 2018.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] W. Schwarting, J. Alonso-Mora 和 D. Rus, “自动驾驶车辆的规划和决策，” *控制、机器人学与自主系统年鉴*，第0期，2018年。'
- en: '[17] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey of deep
    learning applications to autonomous vehicle control,” *IEEE Transactions on Intelligent
    Transportation Systems*, 2020.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. Kuutti, R. Bowden, Y. Jin, P. Barber 和 S. Fallah, “深度学习在自动驾驶车辆控制中的应用调查，”
    *IEEE智能交通系统汇刊*，2020年。'
- en: '[18] T. M. Mitchell, *Machine learning*, ser. McGraw-Hill series in computer
    science.   Boston (Mass.), Burr Ridge (Ill.), Dubuque (Iowa): McGraw-Hill, 1997.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] T. M. Mitchell, *机器学习*，McGraw-Hill计算机科学系列。   波士顿（马萨诸塞州）、布尔里奇（伊利诺伊州）、杜比克（爱荷华州）：McGraw-Hill,
    1997年。'
- en: '[19] S. J. Russell and P. Norvig, *Artificial intelligence: a modern approach
    (3rd edition)*.   Prentice Hall, 2009.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. J. Russell 和 P. Norvig, *人工智能：现代方法（第3版）*。   Prentice Hall, 2009.'
- en: '[20] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.-Y. Lee,
    “Diversity-driven exploration strategy for deep reinforcement learning,” in *Advances
    in Neural Information Processing Systems 31*, S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., 2018, pp. 10 489–10 500.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu 和 C.-Y. Lee,
    “基于多样性的深度强化学习探索策略，” 发表在 *神经信息处理系统进展 31*，S. Bengio, H. Wallach, H. Larochelle,
    K. Grauman, N. Cesa-Bianchi 和 R. Garnett, 主编, 2018年, 第10 489–10 500页。'
- en: '[21] M. Wiering and M. van Otterlo, Eds., *Reinforcement Learning: State-of-the-Art*.   Springer,
    2012.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. Wiering 和 M. van Otterlo, 主编, *强化学习：前沿进展*。   Springer, 2012.'
- en: '[22] M. L. Puterman, *Markov Decision Processes: Discrete Stochastic Dynamic
    Programming*, 1st ed.   New York, NY, USA: John Wiley & Sons, Inc., 1994.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] M. L. Puterman, *马尔可夫决策过程：离散随机动态规划*，第1版。   纽约, NY, USA: John Wiley & Sons,
    Inc., 1994年。'
- en: '[23] C. J. Watkins and P. Dayan, “Technical note: Q-learning,” *Machine Learning*,
    vol. 8, no. 3-4, 1992.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C. J. Watkins 和 P. Dayan, “技术说明：Q学习，” *机器学习*，第8卷，第3-4期，1992年。'
- en: '[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, 2015.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制，”
    *自然*，第518卷，2015年。'
- en: '[25] C. J. C. H. Watkins, “Learning from delayed rewards,” Ph.D. dissertation,
    King’s College, Cambridge, 1989.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. J. C. H. Watkins, “从延迟奖励中学习，” 博士学位论文，剑桥大学国王学院，1989年。'
- en: '[26] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *ICML*, 2014.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra 和 M. Riedmiller,
    “确定性策略梯度算法，” 发表在 *ICML*，2014年。'
- en: '[27] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine Learning*, vol. 8, pp. 229–256,
    1992.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] R. J. Williams, “用于联结主义强化学习的简单统计梯度跟踪算法，” *机器学习*，第8卷，第229–256页，1992年。'
- en: '[28] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015,
    pp. 1889–1897.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz, “信任区域策略优化，”
    发表在 *国际机器学习会议*，2015年, 第1889–1897页。'
- en: '[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov, “邻近策略优化算法，”
    *arXiv预印本 arXiv:1707.06347*，2017年。'
- en: '[30] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning.”
    in *4th International Conference on Learning Representations, ICLR 2016, San Juan,
    Puerto Rico, May 2-4, 2016, Conference Track Proceedings*, Y. Bengio and Y. LeCun,
    Eds., 2016.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver 和 D. Wierstra, “通过深度强化学习进行连续控制。” 见 *第4届学习表征国际会议，ICLR 2016，圣胡安，波多黎各，2016年5月2-4日，会议论文集*，Y.
    Bengio 和 Y. LeCun 主编，2016年。'
- en: '[31] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning*, 2016.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver 和 K. Kavukcuoglu, “深度强化学习的异步方法，” 见 *国际机器学习大会*，2016年。'
- en: '[32] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning
    with deep energy-based policies,” in *Proceedings of the 34th International Conference
    on Machine Learning - Volume 70*, ser. ICML’17.   JMLR.org, 2017, pp. 1352–1361.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. Haarnoja, H. Tang, P. Abbeel 和 S. Levine, “基于深度能量的策略的强化学习，” 见 *第34届国际机器学习大会论文集
    - 第70卷*，系列 ICML’17。 JMLR.org，2017年，第1352–1361页。'
- en: '[33] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar,
    H. Zhu, A. Gupta, P. Abbeel *et al.*, “Soft actor-critic algorithms & applications,”
    *arXiv:1812.05905*, 2018.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar,
    H. Zhu, A. Gupta, P. Abbeel *等*，“软演员-评论家算法与应用，” *arXiv:1812.05905*，2018年。'
- en: '[34] R. S. Sutton, “Integrated architectures for learning, planning, and reacting
    based on approximating dynamic programming,” in *Machine Learning Proceedings
    1990*.   Elsevier, 1990.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] R. S. Sutton, “基于动态规划近似的学习、规划和反应的集成架构，” 见 *1990年机器学习会议论文集*。 爱思唯尔，1990年。'
- en: '[35] R. I. Brafman and M. Tennenholtz, “R-max-a general polynomial time algorithm
    for near-optimal reinforcement learning,” *Journal of Machine Learning Research*,
    vol. 3, no. Oct, 2002.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] R. I. Brafman 和 M. Tennenholtz, “R-max 一种近似最优强化学习的通用多项式时间算法，” *机器学习研究杂志*，第3卷，第10期，2002年。'
- en: '[36] D. Silver, R. S. Sutton, and M. Müller, “Sample-based learning and search
    with permanent and transient memories,” in *Proceedings of the 25th international
    conference on Machine learning*.   ACM, 2008, pp. 968–975.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Silver, R. S. Sutton 和 M. Müller, “基于样本的学习和搜索，具有永久和暂态记忆，” 见 *第25届国际机器学习大会论文集*。
    ACM，2008年，第968–975页。'
- en: '[37] G. A. Rummery and M. Niranjan, “On-line Q-learning using connectionist
    systems,” Cambridge University Engineering Department, Cambridge, England, Tech.
    Rep. TR 166, 1994.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] G. A. Rummery 和 M. Niranjan, “使用连接主义系统的在线 Q 学习，” 剑桥大学工程系，剑桥，英格兰，技术报告 TR
    166，1994年。'
- en: '[38] R. S. Sutton and A. G. Barto, “Reinforcement learning an introduction–second
    edition, in progress (draft),” 2015.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. S. Sutton 和 A. G. Barto, “强化学习介绍–第二版，进行中（草稿），” 2015年。'
- en: '[39] R. Bellman, *Dynamic Programming*.   Princeton, NJ, USA: Princeton University
    Press, 1957.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] R. Bellman, *动态规划*。 普林斯顿，新泽西，美国：普林斯顿大学出版社，1957年。'
- en: '[40] G. Tesauro, “Td-gammon, a self-teaching backgammon program, achieves master-level
    play,” *Neural Computing*, vol. 6, no. 2, Mar. 1994.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] G. Tesauro, “Td-gammon，一种自我教学的西洋双陆棋程序，达到大师级水平，” *神经计算*，第6卷，第2期，1994年3月。'
- en: '[41] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *nature*, vol. 529,
    no. 7587, pp. 484–489, 2016.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *等*，“使用深度神经网络和树搜索掌握围棋游戏，”
    *自然*，第529卷，第7587期，第484–489页，2016年。'
- en: '[42] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language understanding for
    text-based games using deep reinforcement learning,” in *Proceedings of the 2015
    Conference on Empirical Methods in Natural Language Processing*.   Association
    for Computational Linguistics, 2015, pp. 1–11.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Narasimhan, T. Kulkarni 和 R. Barzilay, “使用深度强化学习进行基于文本的游戏语言理解，” 见 *2015年自然语言处理经验方法会议论文集*。
    计算语言学协会，2015年，第1–11页。'
- en: '[43] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] T. Schaul, J. Quan, I. Antonoglou 和 D. Silver, “优先经验重放，” *arXiv 预印本 arXiv:1511.05952*，2015年。'
- en: '[44] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning.” in *AAAI*, vol. 16, 2016, pp. 2094–2100.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Van Hasselt, A. Guez 和 D. Silver, “使用双重 Q 学习的深度强化学习。” 见 *AAAI*，第16卷，2016年，第2094–2100页。'
- en: '[45] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” *arXiv preprint
    arXiv:1511.06581*, 2015.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot 和 N. De Freitas,
    “深度强化学习中的对抗网络架构，” *arXiv预印本 arXiv:1511.06581*，2015年。'
- en: '[46] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially observable
    mdps,” *CoRR, abs/1507.06527*, 2015.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] M. Hausknecht 和 P. Stone, “用于部分可观察MDP的深度递归Q学习，” *CoRR, abs/1507.06527*，2015年。'
- en: '[47] B. F. Skinner, *The behavior of organisms: An experimental analysis.*   Appleton-Century,
    1938.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] B. F. Skinner, *有机体行为：实验分析.*   Appleton-Century，1938年。'
- en: '[48] E. Wiewiora, “Reward shaping,” in *Encyclopedia of Machine Learning and
    Data Mining*, C. Sammut and G. I. Webb, Eds.   Boston, MA: Springer US, 2017,
    pp. 1104–1106.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] E. Wiewiora, “奖励塑形，” 见于 *机器学习与数据挖掘百科全书*，C. Sammut 和 G. I. Webb 编。   波士顿，马萨诸塞州：施普林格美国，2017年，pp.
    1104–1106。'
- en: '[49] J. Randløv and P. Alstrøm, “Learning to drive a bicycle using reinforcement
    learning and shaping,” in *Proceedings of the Fifteenth International Conference
    on Machine Learning*, ser. ICML ’98.   San Francisco, CA, USA: Morgan Kaufmann
    Publishers Inc., 1998, pp. 463–471.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] J. Randløv 和 P. Alstrøm, “使用强化学习和塑形技术学习骑自行车，” 见于 *第十五届国际机器学习会议论文集*，ICML
    ’98系列。   旧金山，加州，美国：摩根·考夫曼出版社，1998年，pp. 463–471。'
- en: '[50] D. H. Wolpert, K. R. Wheeler, and K. Tumer, “Collective intelligence for
    control of distributed dynamical systems,” *EPL (Europhysics Letters)*, vol. 49,
    no. 6, p. 708, 2000.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] D. H. Wolpert, K. R. Wheeler 和 K. Tumer, “分布式动态系统的集体智能控制，” *EPL (Europhysics
    Letters)*, vol. 49, no. 6, p. 708, 2000。'
- en: '[51] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invariance under reward
    transformations: Theory and application to reward shaping,” in *Proceedings of
    the Sixteenth International Conference on Machine Learning*, ser. ICML ’99, 1999,
    pp. 278–287.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Y. Ng, D. Harada 和 S. J. Russell, “奖励变换下的策略不变性：理论及其在奖励塑形中的应用，” 见于 *第十六届国际机器学习会议论文集*，ICML
    ’99系列，1999年，pp. 278–287。'
- en: '[52] S. Devlin and D. Kudenko, “Theoretical considerations of potential-based
    reward shaping for multi-agent systems,” in *Proceedings of the 10th International
    Conference on Autonomous Agents and Multiagent Systems (AAMAS)*, 2011.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. Devlin 和 D. Kudenko, “基于潜力的奖励塑形在多智能体系统中的理论考量，” 见于 *第10届国际自主代理和多智能体系统会议（AAMAS）论文集*，2011年。'
- en: '[53] P. Mannion, S. Devlin, K. Mason, J. Duggan, and E. Howley, “Policy invariance
    under reward transformations for multi-objective reinforcement learning,” *Neurocomputing*,
    vol. 263, 2017.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] P. Mannion, S. Devlin, K. Mason, J. Duggan 和 E. Howley, “多目标强化学习中的奖励变换下的策略不变性，”
    *神经计算*，vol. 263，2017年。'
- en: '[54] M. Colby and K. Tumer, “An evolutionary game theoretic analysis of difference
    evaluation functions,” in *Proceedings of the 2015 Annual Conference on Genetic
    and Evolutionary Computation*.   ACM, 2015, pp. 1391–1398.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Colby 和 K. Tumer, “差异评价函数的进化博弈理论分析，” 见于 *2015年遗传与进化计算年会论文集*。   ACM，2015年，pp.
    1391–1398。'
- en: '[55] P. Mannion, J. Duggan, and E. Howley, “A theoretical and empirical analysis
    of reward transformations in multi-objective stochastic games,” in *Proceedings
    of the 16th International Conference on Autonomous Agents and Multiagent Systems
    (AAMAS)*, 2017.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] P. Mannion, J. Duggan 和 E. Howley, “多目标随机游戏中奖励变换的理论与实证分析，” 见于 *第16届国际自主代理和多智能体系统会议（AAMAS）论文集*，2017年。'
- en: '[56] L. Buşoniu, R. Babuška, and B. Schutter, “Multi-agent reinforcement learning:
    An overview,” in *Innovations in Multi-Agent Systems and Applications - 1*, ser.
    Studies in Computational Intelligence, D. Srinivasan and L. Jain, Eds.   Springer
    Berlin Heidelberg, 2010, vol. 310.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] L. Buşoniu, R. Babuška 和 B. Schutter, “多智能体强化学习：概述，” 见于 *多智能体系统与应用创新 -
    1*，计算智能系列，D. Srinivasan 和 L. Jain 编。   施普林格·柏林·海德堡，2010年，第310卷。'
- en: '[57] P. Mannion, K. Mason, S. Devlin, J. Duggan, and E. Howley, “Multi-objective
    dynamic dispatch optimisation using multi-agent reinforcement learning,” in *Proceedings
    of the 15th International Conference on Autonomous Agents and Multiagent Systems
    (AAMAS)*, 2016.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. Mannion, K. Mason, S. Devlin, J. Duggan 和 E. Howley, “使用多智能体强化学习进行多目标动态调度优化，”
    见于 *第15届国际自主代理和多智能体系统会议（AAMAS）论文集*，2016年。'
- en: '[58] K. Mason, P. Mannion, J. Duggan, and E. Howley, “Applying multi-agent
    reinforcement learning to watershed management,” in *Proceedings of the Adaptive
    and Learning Agents workshop (at AAMAS 2016)*, 2016.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Mason, P. Mannion, J. Duggan 和 E. Howley, “将多智能体强化学习应用于流域管理，” 见于 *2016年自适应与学习代理研讨会（AAMAS
    2016）论文集*，2016年。'
- en: '[59] V. Pareto, *Manual of political economy*.   OUP Oxford, 1906.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] V. Pareto，*《政治经济学手册》*。 OUP Oxford，1906年。'
- en: '[60] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley, “A survey of multi-objective
    sequential decision-making,” *Journal of Artificial Intelligence Research*, vol. 48,
    pp. 67–113, 2013.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] D. M. Roijers、P. Vamplew、S. Whiteson 和 R. Dazeley，“多目标序贯决策的调查”，*《人工智能研究期刊》*，第48卷，第67–113页，2013年。'
- en: '[61] R. Rădulescu, P. Mannion, D. M. Roijers, and A. Nowé, “Multi-objective
    multi-agent decision making: a utility-based analysis and survey,” *Autonomous
    Agents and Multi-Agent Systems*, vol. 34, no. 1, p. 10, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. Rădulescu、P. Mannion、D. M. Roijers 和 A. Nowé，“多目标多智能体决策：基于效用的分析与调查”，*《自主智能体与多智能体系统》*，第34卷，第1期，第10页，2020年。'
- en: '[62] T. Lesort, N. Diaz-Rodriguez, J.-F. Goudou, and D. Filliat, “State representation
    learning for control: An overview,” *Neural Networks*, vol. 108, pp. 379 – 392,
    2018.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T. Lesort、N. Diaz-Rodriguez、J.-F. Goudou 和 D. Filliat，“控制的状态表示学习：概述”，*《神经网络》*，第108卷，第379–392页，2018年。'
- en: '[63] A. Raffin, A. Hill, K. R. Traoré, T. Lesort, N. D. Rodríguez, and D. Filliat,
    “Decoupling feature extraction from policy learning: assessing benefits of state
    representation learning in goal based robotics,” *CoRR*, vol. abs/1901.08651,
    2019.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] A. Raffin、A. Hill、K. R. Traoré、T. Lesort、N. D. Rodríguez 和 D. Filliat，“将特征提取与策略学习解耦：评估基于目标的机器人中的状态表示学习的好处”，*CoRR*，第abs/1901.08651卷，2019年。'
- en: '[64] W. Böhmer, J. T. Springenberg, J. Boedecker, M. Riedmiller, and K. Obermayer,
    “Autonomous learning of state representations for control: An emerging field aims
    to autonomously learn state representations for reinforcement learning agents
    from their real-world sensor observations,” *KI-Künstliche Intelligenz*, vol. 29,
    no. 4, pp. 353–362, 2015.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] W. Böhmer、J. T. Springenberg、J. Boedecker、M. Riedmiller 和 K. Obermayer，“自主学习状态表示以用于控制：一个新兴领域旨在从真实世界传感器观察中自主学习状态表示以供强化学习智能体使用”，*《KI-Künstliche
    Intelligenz》*，第29卷，第4期，第353–362页，2015年。'
- en: '[65] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,
    T. Hubert, L. Baker, M. Lai, A. Bolton *et al.*, “Mastering the game of go without
    human knowledge,” *Nature*, vol. 550, no. 7676, p. 354, 2017.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] D. Silver、J. Schrittwieser、K. Simonyan、I. Antonoglou、A. Huang、A. Guez、T.
    Hubert、L. Baker、M. Lai、A. Bolton *等*，“在没有人类知识的情况下掌握围棋”，*《自然》*，第550卷，第7676期，第354页，2017年。'
- en: '[66] P. Abbeel and A. Y. Ng, “Exploration and apprenticeship learning in reinforcement
    learning,” in *Proceedings of the 22nd international conference on Machine learning*.   ACM,
    2005, pp. 1–8.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] P. Abbeel 和 A. Y. Ng，“强化学习中的探索与学徒学习”，见于*第22届国际机器学习会议论文集*。 ACM，2005年，第1–8页。'
- en: '[67] B. Kang, Z. Jie, and J. Feng, “Policy optimization with demonstrations,”
    in *International Conference on Machine Learning*, 2018, pp. 2474–2483.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] B. Kang、Z. Jie 和 J. Feng，“带有示范的策略优化”，见于*国际机器学习会议*，2018年，第2474–2483页。'
- en: '[68] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan,
    J. Quan, A. Sendonaris, I. Osband *et al.*, “Deep q-learning from demonstrations,”
    in *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. Hester、M. Vecerik、O. Pietquin、M. Lanctot、T. Schaul、B. Piot、D. Horgan、J.
    Quan、A. Sendonaris、I. Osband *等*，“从示范中进行深度Q学习”，见于*第三十二届AAAI人工智能大会*，2018年。'
- en: '[69] S. Ibrahim and D. Nevin, “End-to-end framework for fast learning asynchronous
    agents,” in *the 32nd Conference on Neural Information Processing Systems, Imitation
    Learning and its Challenges in Robotics workshop*, 2018.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. Ibrahim 和 D. Nevin，“快速学习异步智能体的端到端框架”，见于*第32届神经信息处理系统会议，模仿学习及其在机器人中的挑战研讨会*，2018年。'
- en: '[70] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement
    learning,” in *Proceedings of the twenty-first international conference on Machine
    learning*.   ACM, 2004, p. 1.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] P. Abbeel 和 A. Y. Ng，“通过逆强化学习进行学徒学习”，见于*第21届国际机器学习会议论文集*。 ACM，2004年，第1页。'
- en: '[71] A. Y. Ng, S. J. Russell *et al.*, “Algorithms for inverse reinforcement
    learning.” in *ICML*, 2000.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. Y. Ng、S. J. Russell *等*，“逆强化学习的算法”，见于*ICML*，2000年。'
- en: '[72] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *Advances
    in Neural Information Processing Systems*, 2016, pp. 4565–4573.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Ho 和 S. Ermon，“生成对抗模仿学习”，见于*《神经信息处理系统进展》*，2016年，第4565–4573页。'
- en: '[73] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems 27*, 2014, pp. 2672–2680.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] I. Goodfellow、J. Pouget-Abadie、M. Mirza、B. Xu、D. Warde-Farley、S. Ozair、A.
    Courville 和 Y. Bengio，“生成对抗网络”，见于*《神经信息处理系统进展 27》*，2014年，第2672–2680页。'
- en: '[74] M. Uřičář, P. Křížek, D. Hurych, I. Sobh, S. Yogamani, and P. Denny, “Yes,
    we gan: Applying adversarial techniques for autonomous driving,” *Electronic Imaging*,
    vol. 2019, no. 15, pp. 48–1, 2019.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] M. Uřičář, P. Křížek, D. Hurych, I. Sobh, S. Yogamani 和 P. Denny，“是的，我们可以：应用对抗技术进行自动驾驶，”*电子成像*，第2019卷，第15期，第48–1页，2019年。'
- en: '[75] E. Leurent, Y. Blanco, D. Efimov, and O.-A. Maillard, “A survey of state-action
    representations for autonomous driving,” *HAL archives*, 2018.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] E. Leurent, Y. Blanco, D. Efimov 和 O.-A. Maillard，“关于自动驾驶的状态-动作表示的调查，”*HAL档案*，2018年。'
- en: '[76] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving
    models from large-scale video datasets,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2174–2182.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] H. Xu, Y. Gao, F. Yu 和 T. Darrell，“从大规模视频数据集中端到端学习驾驶模型，”发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第2174–2182页。'
- en: '[77] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps: A
    framework for temporal abstraction in reinforcement learning,” *Artificial intelligence*,
    vol. 112, no. 1-2, pp. 181–211, 1999.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] R. S. Sutton, D. Precup 和 S. Singh，“在mdps和semi-mdps之间：一种用于强化学习中的时间抽象的框架，”*人工智能*，第112卷，第1-2期，第181–211页，1999年。'
- en: '[78] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An open urban driving simulator,” in *Proceedings of the 1st Annual Conference
    on Robot Learning*, 2017, pp. 1–16.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez 和 V. Koltun，“CARLA：一个开放的城市驾驶模拟器，”发表于*第1届机器人学习年会论文集*，2017年，第1–16页。'
- en: '[79] C. Li and K. Czarnecki, “Urban driving with multi-objective deep reinforcement
    learning,” in *Proceedings of the 18th International Conference on Autonomous
    Agents and MultiAgent Systems*.   International Foundation for Autonomous Agents
    and Multiagent Systems, 2019, pp. 359–367.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Li 和 K. Czarnecki，“使用多目标深度强化学习进行城市驾驶，”发表于*第18届国际自主代理与多代理系统会议论文集*。国际自主代理与多代理系统基金会，2019年，第359–367页。'
- en: '[80] S. Kardell and M. Kuosku, “Autonomous vehicle control via deep reinforcement
    learning,” Master’s thesis, Chalmers University of Technology, 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. Kardell 和 M. Kuosku，“通过深度强化学习进行自动驾驶控制，”硕士论文，查尔姆斯科技大学，2017年。'
- en: '[81] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement learning
    for urban autonomous driving,” in *2019 IEEE Intelligent Transportation Systems
    Conference (ITSC)*.   IEEE, 2019, pp. 2765–2771.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] J. Chen, B. Yuan 和 M. Tomizuka，“用于城市自动驾驶的无模型深度强化学习，”发表于*2019 IEEE智能交通系统会议（ITSC）*。IEEE，2019年，第2765–2771页。'
- en: '[82] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-end deep reinforcement
    learning for lane keeping assist,” in *MLITS, NIPS Workshop*, vol. 2, 2016.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. E. Sallab, M. Abdou, E. Perot 和 S. Yogamani，“用于车道保持辅助的端到端深度强化学习，”发表于*MLITS,
    NIPS Workshop*，第2卷，2016年。'
- en: '[83] A.-E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement
    learning framework for autonomous driving,” *Electronic Imaging*, vol. 2017, no. 19,
    pp. 70–76, 2017.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A.-E. Sallab, M. Abdou, E. Perot 和 S. Yogamani，“用于自动驾驶的深度强化学习框架，”*电子成像*，第2017卷，第19期，第70–76页，2017年。'
- en: '[84] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning
    based approach for automated lane change maneuvers,” in *2018 IEEE Intelligent
    Vehicles Symposium (IV)*.   IEEE, 2018, pp. 1379–1384.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] P. Wang, C.-Y. Chan 和 A. de La Fortelle，“基于强化学习的自动车道变换策略，”发表于*2018 IEEE智能车辆研讨会（IV）*。IEEE，2018年，第1379–1384页。'
- en: '[85] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning architecture
    toward autonomous driving for on-ramp merge,” in *Intelligent Transportation Systems
    (ITSC), 2017 IEEE 20th International Conference on*.   IEEE, 2017, pp. 1–6.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] P. Wang 和 C.-Y. Chan，“面向自动驾驶的深度强化学习架构的制定——用于车道合并的端到端策略，”发表于*智能交通系统（ITSC），2017
    IEEE第20届国际会议*。IEEE，2017年，第1–6页。'
- en: '[86] D. C. K. Ngai and N. H. C. Yung, “A multiple-goal reinforcement learning
    method for complex vehicle overtaking maneuvers,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 12, no. 2, pp. 509–522, 2011.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. C. K. Ngai 和 N. H. C. Yung，“用于复杂车辆超车动作的多目标强化学习方法，”*IEEE智能交通系统汇刊*，第12卷，第2期，第509–522页，2011年。'
- en: '[87] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, “Navigating
    occluded intersections with autonomous vehicles using deep reinforcement learning,”
    in *2018 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2018, pp. 2034–2039.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian 和 K. Fujimura，“使用深度强化学习导航遮挡交叉口的自动驾驶车辆，”发表于*2018
    IEEE国际机器人与自动化会议（ICRA）*。IEEE，2018年，第2034–2039页。'
- en: '[88] A. Keselman, S. Ten, A. Ghazali, and M. Jubeh, “Reinforcement learning
    with a* and a deep heuristic,” *arXiv preprint arXiv:1811.07745*, 2018.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] A. Keselman, S. Ten, A. Ghazali和M. Jubeh，“使用a*和深度启发式的强化学习”，*arXiv预印本 arXiv:1811.07745*，2018年。'
- en: '[89] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kümmerle,
    H. Königshof, C. Stiller, A. de La Fortelle, and M. Tomizuka, “INTERACTION Dataset:
    An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving
    Scenarios with Semantic Maps,” *arXiv:1910.03088 [cs, eess]*, 2019.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kümmerle,
    H. Königshof, C. Stiller, A. de La Fortelle和M. Tomizuka，“INTERACTION数据集：具有语义地图的交互驾驶场景中的国际、对抗性和合作行动数据集”，*arXiv:1910.03088
    [cs, eess]*，2019年。'
- en: '[90] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley, and A. Shah, “Learning to drive in a day,” in *2019 International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2019, pp. 8248–8254.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam,
    A. Bewley和A. Shah，“一天学会驾驶”，载于*2019年国际机器人和自动化大会(ICRA)*。IEEE, 2019年，8248–8254页。'
- en: '[91] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, “Embed to
    control: A locally linear latent dynamics model for control from raw images,”
    in *Advances in neural information processing systems*, 2015.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Watter, J. Springenberg, J. Boedecker和M. Riedmiller，“嵌入控制：一种用于从原始图像进行控制的局部线性潜在动力学模型”，载于*2015年神经信息处理系统进展*。'
- en: '[92] N. Wahlström, T. B. Schön, and M. P. Deisenroth, “Learning deep dynamical
    models from image pixels,” *IFAC-PapersOnLine*, vol. 48, no. 28, pp. 1059–1064,
    2015.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] N. Wahlström, T. B. Schön和M. P. Deisenroth，“从图像像素学习深层动态模型”，*IFAC-PapersOnLine*，第48卷，第28期，1059–1064页，2015年。'
- en: '[93] S. Chiappa, S. Racanière, D. Wierstra, and S. Mohamed, “Recurrent environment
    simulators,” in *5th International Conference on Learning Representations, ICLR
    2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*.   OpenReview.net,
    2017.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Chiappa, S. Racanière, D. Wierstra和S. Mohamed，“环境的循环模拟器”，载于*第五届国际学习表示会议(ICLR
    2017，法国图伦，2017年4月24-26日，会议跟踪会议记录)*。OpenReview.net, 2017年。'
- en: '[94] B. Recht, “A tour of reinforcement learning: The view from continuous
    control,” *Annual Review of Control, Robotics, and Autonomous Systems*, 2008.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] B. Recht，“一场强化学习之旅：从连续控制的视角”，*年度控制、机器人和自主系统评论*，2008年。'
- en: '[95] H. Mania, A. Guy, and B. Recht, “Simple random search of static linear
    policies is competitive for reinforcement learning,” in *Advances in Neural Information
    Processing Systems 31*, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
    and R. Garnett, Eds., 2018, pp. 1800–1809.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] H. Mania, A. Guy和B. Recht，“简单的静态线性政策的随机搜索在强化学习中是有竞争力的”，载于*第31届神经信息处理系统进展*，S.
    Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi和R. Garnett编，2018年,
    1800–1809页。'
- en: '[96] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner,
    “Torcs, the open racing car simulator,” *Software available at http://torcs. sourceforge.
    net*, vol. 4, 2000.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom和A. Sumner，“Torcs，这个开放赛车模拟器”，*软件可在http://torcs.
    sourceforge. net找到*，第4卷，2000年。'
- en: '[97] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual
    and physical simulation for autonomous vehicles,” in *Field and Service Robotics*.   Springer,
    2018, pp. 621–635.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Shah, D. Dey, C. Lovett和A. Kapoor，“Aisim：自主车辆的高保真视觉和物理仿真”，载于*田野和服务机器人*。Springer，2018年，621–635页。'
- en: '[98] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an open-source
    multi-robot simulator,” in *2004 International Conference on Intelligent Robots
    and Systems (IROS)*, vol. 3.   IEEE, 2004, pp. 2149–2154.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] N. Koenig和A. Howard，“Gazebo的设计和使用范式，一个开源多机器人模拟器”，载于*2004年智能机器人和系统国际会议(IROS)*，卷3。IEEE,
    2004年，2149–2154页。'
- en: '[99] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd,
    R. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. Wießner, “Microscopic traffic
    simulation using sumo,” in *The 21st IEEE International Conference on Intelligent
    Transportation Systems*.   IEEE, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd,
    R. Hilbrich, L. Lücken, J. Rummel, P. Wagner和E. Wießner，“使用sumo进行微观交通仿真”，载于*第21届IEEE国际智能交通系统大会*。IEEE,
    2018年。'
- en: '[100] C. Quiter and M. Ernst, “deepdrive/deepdrive: 2.0,” Mar. 2018\. [Online].
    Available: [https://doi.org/10.5281/zenodo.1248998](https://doi.org/10.5281/zenodo.1248998)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] C. Quiter和M. Ernst，“deepdrive/deepdrive: 2.0”，2018年3月。[在线]. 网址：[https://doi.org/10.5281/zenodo.1248998](https://doi.org/10.5281/zenodo.1248998)'
- en: '[101] Nvidia, “Drive Constellation now available,” [https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/](https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/),
    2019, [accessed 14-April-2019].'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Nvidia，“Drive Constellation 现已上线，” [https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/](https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/)，2019年，[访问时间：2019年4月14日]。'
- en: '[102] A. S. et al., “Multi-Agent Autonomous Driving Simulator built on top
    of TORCS,” [https://github.com/madras-simulator/MADRaS](https://github.com/madras-simulator/MADRaS),
    2019, [Online; accessed 14-April-2019].'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] A. S. *等*，“基于 TORCS 的多智能体自主驾驶模拟器，” [https://github.com/madras-simulator/MADRaS](https://github.com/madras-simulator/MADRaS)，2019年，[在线；访问时间：2019年4月14日]。'
- en: '[103] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, and A. M. Bayen, “Flow:
    Architecture and benchmarking for reinforcement learning in traffic control,”
    *CoRR*, vol. abs/1710.05465, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, 和 A. M. Bayen，“Flow: 交通控制中的强化学习架构和基准测试，”
    *CoRR*，第 abs/1710.05465 号，2017年。'
- en: '[104] E. Leurent, “A collection of environments for autonomous driving and
    tactical decision-making tasks,” [https://github.com/eleurent/highway-env](https://github.com/eleurent/highway-env),
    2019, [Online; accessed 14-April-2019].'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] E. Leurent，“用于自主驾驶和战术决策任务的环境集合，” [https://github.com/eleurent/highway-env](https://github.com/eleurent/highway-env)，2019年，[在线；访问时间：2019年4月14日]。'
- en: '[105] F. Rosique, P. J. Navarro, C. Fernández, and A. Padilla, “A systematic
    review of perception system and simulators for autonomous vehicles research,”
    *Sensors*, vol. 19, no. 3, p. 648, 2019.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] F. Rosique, P. J. Navarro, C. Fernández, 和 A. Padilla，“关于自主车辆研究的感知系统和模拟器的系统评审，”
    *传感器*，第19卷，第3期，第648页，2019年。'
- en: '[106] M. Cutler, T. J. Walsh, and J. P. How, “Reinforcement learning with multi-fidelity
    simulators,” in *2014 IEEE International Conference on Robotics and Automation
    (ICRA)*.   IEEE, 2014, pp. 3888–3895.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] M. Cutler, T. J. Walsh, 和 J. P. How，“使用多保真度模拟器进行强化学习，”发表于 *2014 IEEE
    国际机器人与自动化会议（ICRA）*。 IEEE，2014年，第3888–3895页。'
- en: '[107] F. C. German Ros, Vladlen Koltun and A. M. Lopez, “Carla autonomous driving
    challenge,” [https://carlachallenge.org/](https://carlachallenge.org/), 2019,
    [Online; accessed 14-April-2019].'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] F. C. German Ros, Vladlen Koltun 和 A. M. Lopez，“Carla 自主驾驶挑战，” [https://carlachallenge.org/](https://carlachallenge.org/)，2019年，[在线；访问时间：2019年4月14日]。'
- en: '[108] W. G. Najm, J. D. Smith, M. Yanagisawa *et al.*, “Pre-crash scenario
    typology for crash avoidance research,” United States. National Highway Traffic
    Safety Administration, Tech. Rep., 2007.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] W. G. Najm, J. D. Smith, M. Yanagisawa *等*，“用于碰撞规避研究的预碰撞场景类型学，”美国国家公路交通安全管理局，技术报告，2007年。'
- en: '[109] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural network,”
    in *Advances in neural information processing systems*, 1989.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] D. A. Pomerleau，“Alvinn：一种神经网络中的自主地面车辆，”发表于 *神经信息处理系统的进展*，1989年。'
- en: '[110] D. Pomerleau, “Efficient training of artificial neural networks for autonomous
    navigation,” *Neural Computation*, vol. 3, no. 1, 1991.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. Pomerleau，“高效训练用于自主导航的人工神经网络，” *神经计算*，第3卷，第1期，1991年。'
- en: '[111] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *et al.*, “End to end learning for
    self-driving cars,” in *NIPS 2016 Deep Learning Symposium*, 2016.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang *等*，“用于自动驾驶汽车的端到端学习，”发表于 *NIPS 2016
    深度学习研讨会*，2016年。'
- en: '[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    and U. Muller, “Explaining how a deep neural network trained with end-to-end learning
    steers a car,” *arXiv preprint arXiv:1704.07911*, 2017.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L.
    Jackel, 和 U. Muller，“解释如何通过端到端学习训练的深度神经网络操控汽车，” *arXiv 预印本 arXiv:1704.07911*，2017年。'
- en: '[113] M. Kuderer, S. Gulati, and W. Burgard, “Learning driving styles for autonomous
    vehicles from demonstration,” in *Robotics and Automation (ICRA), 2015 IEEE International
    Conference on*.   IEEE, 2015, pp. 2641–2646.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Kuderer, S. Gulati, 和 W. Burgard，“从演示中学习自动驾驶车辆的驾驶风格，”发表于 *机器人与自动化（ICRA），2015
    IEEE 国际会议*。 IEEE，2015年，第2641–2646页。'
- en: '[114] S. Sharifzadeh, I. Chiotellis, R. Triebel, and D. Cremers, “Learning
    to drive using inverse reinforcement learning and deep q-networks,” in *NIPS Workshops*,
    December 2016.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] S. Sharifzadeh, I. Chiotellis, R. Triebel, 和 D. Cremers，“使用逆向强化学习和深度
    Q 网络学习驾驶，”发表于 *NIPS 研讨会*，2016年12月。'
- en: '[115] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,
    “Deep reinforcement learning that matters,” in *Thirty-Second AAAI Conference
    on Artificial Intelligence*, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, 和 D. Meger，“重要的深度强化学习，”发表于
    *第三十二届 AAAI 人工智能会议*，2018年。'
- en: '[116] Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek, “Generating adversarial
    driving scenarios in high-fidelity simulators,” in *2019 IEEE International Conference
    on Robotics and Automation (ICRA)*.   ICRA, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Y. Abeysirigoonawardena, F. Shkurti, 和 G. Dudek，“在高保真模拟器中生成对抗驾驶场景，” 收录于
    *2019 IEEE 国际机器人与自动化会议 (ICRA)*。 ICRA，2019 年。'
- en: '[117] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *et al.*, “Using simulation and domain
    adaptation to improve efficiency of deep robotic grasping,” in *2018 IEEE International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2018, pp. 4243–4250.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan,
    L. Downs, J. Ibarz, P. Pastor, K. Konolige *等*，“利用仿真和领域适配提高深度机器人抓取的效率，” 收录于 *2018
    IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2018 年，页码 4243–4250。'
- en: '[118] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-real
    transfer of robotic control with dynamics randomization,” in *2018 IEEE international
    conference on robotics and automation (ICRA)*.   IEEE, 2018, pp. 1–8.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] X. B. Peng, M. Andrychowicz, W. Zaremba, 和 P. Abbeel，“通过动态随机化的机器人控制仿真到现实转移，”
    收录于 *2018 IEEE 国际机器人与自动化会议 (ICRA)*。 IEEE，2018 年，页码 1–8。'
- en: '[119] Z. W. Xinlei Pan, Yurong You and C. Lu, “Virtual to real reinforcement
    learning for autonomous driving,” in *Proceedings of the British Machine Vision
    Conference (BMVC)*, G. B. Tae-Kyun Kim, Stefanos Zafeiriou and K. Mikolajczyk,
    Eds.   BMVA Press, September 2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. W. Xinlei Pan, Yurong You 和 C. Lu，“用于自主驾驶的虚拟到现实强化学习，” 收录于 *英国机器视觉会议
    (BMVC) 论文集*，G. B. Tae-Kyun Kim, Stefanos Zafeiriou 和 K. Mikolajczyk 主编。 BMVA Press，2017
    年 9 月。'
- en: '[120] A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D. Lam, and A. Kendall,
    “Learning to drive from simulation without real world labels,” in *2019 International
    Conference on Robotics and Automation (ICRA)*.   IEEE, 2019, pp. 4818–4824.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D. Lam, 和 A. Kendall，“从仿真中学习驾驶而无需现实世界标签，”
    收录于 *2019 国际机器人与自动化会议 (ICRA)*。 IEEE，2019 年，页码 4818–4824。'
- en: '[121] J. Zhang, L. Tai, P. Yun, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard,
    “Vr-goggles for robots: Real-to-sim domain adaptation for visual control,” *IEEE
    Robotics and Automation Letters*, vol. 4, no. 2, pp. 1148–1155, 2019.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Zhang, L. Tai, P. Yun, Y. Xiong, M. Liu, J. Boedecker, 和 W. Burgard，“用于机器人的
    VR 眼镜：视觉控制的现实到模拟领域适配，” *IEEE Robotics and Automation Letters*，第 4 卷，第 2 期，页码 1148–1155，2019
    年。'
- en: '[122] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W. Choi, “Autonomous
    braking system via deep reinforcement learning,” *2017 IEEE 20th International
    Conference on Intelligent Transportation Systems (ITSC)*, pp. 1–6, 2017.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, 和 J. W. Choi，“通过深度强化学习实现自主制动系统，”
    *2017 IEEE 第 20 届智能交通系统国际会议 (ITSC)*，页码 1–6，2017 年。'
- en: '[123] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” in *5th International
    Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
    2017, Conference Track Proceedings*.   OpenReview.net, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, 和 N.
    de Freitas，“样本高效的演员-评论员与经验回放，” 收录于 *第 5 届国际学习表征会议，ICLR 2017，法国图盎，2017 年 4 月 24-26
    日，会议论文集*。 OpenReview.net，2017 年。'
- en: '[124] R. Liaw, S. Krishnan, A. Garg, D. Crankshaw, J. E. Gonzalez, and K. Goldberg,
    “Composing meta-policies for autonomous driving using hierarchical deep reinforcement
    learning,” *arXiv preprint arXiv:1711.01503*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] R. Liaw, S. Krishnan, A. Garg, D. Crankshaw, J. E. Gonzalez, 和 K. Goldberg，“使用层次化深度强化学习组合自主驾驶的元策略，”
    *arXiv 预印本 arXiv:1711.01503*，2017 年。'
- en: '[125] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning
    domains: A survey,” *Journal of Machine Learning Research*, vol. 10, no. Jul,
    pp. 1633–1685, 2009.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] M. E. Taylor 和 P. Stone，“强化学习领域的迁移学习：综述，” *Journal of Machine Learning
    Research*，第 10 卷，7 月期，页码 1633–1685，2009 年。'
- en: '[126] D. Isele and A. Cosgun, “Transferring autonomous driving knowledge on
    simulated and real intersections,” in *Lifelong Learning: A Reinforcement Learning
    Approach,ICML WORKSHOP 2017*, 2017.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Isele 和 A. Cosgun，“在模拟和实际交叉口转移自主驾驶知识，” 收录于 *Lifelong Learning: A Reinforcement
    Learning Approach, ICML WORKSHOP 2017*，2017 年。'
- en: '[127] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos,
    C. Blundell, D. Kumaran, and M. Botvinick, “Learning to reinforcement learn,”
    *Complete CogSci 2017 Proceedings*, 2016.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos,
    C. Blundell, D. Kumaran, 和 M. Botvinick，“学习强化学习，” *Complete CogSci 2017 Proceedings*，2016
    年。'
- en: '[128] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel,
    “Fast reinforcement learning via slow reinforcement learning,” *arXiv preprint
    arXiv:1611.02779*, 2016.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, 和 P. Abbeel，“通过慢强化学习实现快速强化学习，”
    *arXiv 预印本 arXiv:1611.02779*，2016年。'
- en: '[129] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *Proceedings of the 34th International Conference
    on Machine Learning - Volume 70*, ser. ICML’17.   JMLR.org, 2017, p. 1126–1135.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] C. Finn, P. Abbeel, 和 S. Levine，“用于深度网络快速适应的模型无关元学习，” 见 *第34届国际机器学习会议论文集
    - 卷70*，系列 ICML’17。   JMLR.org，2017年，页码1126–1135。'
- en: '[130] A. Nichol, J. Achiam, and J. Schulman, “On first-order meta-learning
    algorithms,” *CoRR, abs/1803.02999*, 2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] A. Nichol, J. Achiam, 和 J. Schulman，“关于一阶元学习算法，” *CoRR, abs/1803.02999*，2018年。'
- en: '[131] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and P. Abbeel,
    “Continuous adaptation via meta-learning in nonstationary and competitive environments,”
    in *6th International Conference on Learning Representations, ICLR 2018, Vancouver,
    BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.   OpenReview.net,
    2018.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, 和 P.
    Abbeel，“在非平稳和竞争环境中通过元学习进行连续适应，” 见 *第六届国际学习表征会议，ICLR 2018，加拿大不列颠哥伦比亚省温哥华，2018年4月30日
    - 5月3日，会议论文集*。   OpenReview.net，2018年。'
- en: '[132] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy evolution,”
    in *Advances in Neural Information Processing Systems*, 2018.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Ha 和 J. Schmidhuber，“递归世界模型促进策略演化，” 见 *神经信息处理系统进展*，2018年。'
- en: '[133] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”
    in *Proceedings of the thirteenth international conference on artificial intelligence
    and statistics*, 2010, pp. 661–668.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S. Ross 和 D. Bagnell，“模仿学习的高效简化方法，” 见 *第十三届国际人工智能与统计会议论文集*，2010年，页码661–668。'
- en: '[134] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to drive
    by imitating the best and synthesizing the worst,” in *Robotics: Science and Systems
    XV*, 2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] M. Bansal, A. Krizhevsky, 和 A. Ogale，“Chauffeurnet：通过模仿最佳并合成最差来学习驾驶，”
    见 *机器人学：科学与系统 XV*，2018年。'
- en: '[135] T. Buhet, E. Wirbel, and X. Perrotton, “Conditional vehicle trajectories
    prediction in carla urban environment,” in *Proceedings of the IEEE International
    Conference on Computer Vision Workshops*, 2019.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] T. Buhet, E. Wirbel, 和 X. Perrotton，“在 Carla 城市环境中预测条件车辆轨迹，” 见 *IEEE国际计算机视觉会议论文集*，2019年。'
- en: '[136] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward
    transformations: Theory and application to reward shaping,” in *ICML*, vol. 99,
    1999, pp. 278–287.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Y. Ng, D. Harada, 和 S. Russell，“奖励变换下的策略不变性：理论及其在奖励塑造中的应用，” 见 *ICML*，卷99，1999年，页码278–287。'
- en: '[137] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement
    learning,” in *Proceedings of the Twenty-first International Conference on Machine
    Learning*, ser. ICML ’04.   ACM, 2004.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] P. Abbeel 和 A. Y. Ng，“通过逆向强化学习进行学徒学习，” 见 *第二十一届国际机器学习会议论文集*，系列 ICML ’04。   ACM，2004年。'
- en: '[138] N. Chentanez, A. G. Barto, and S. P. Singh, “Intrinsically motivated
    reinforcement learning,” in *Advances in neural information processing systems*,
    2005, pp. 1281–1288.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] N. Chentanez, A. G. Barto, 和 S. P. Singh，“内在动机驱动的强化学习，” 见 *神经信息处理系统进展*，2005年，页码1281–1288。'
- en: '[139] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven
    exploration by self-supervised prediction,” in *International Conference on Machine
    Learning (ICML)*, vol. 2017, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] D. Pathak, P. Agrawal, A. A. Efros, 和 T. Darrell，“通过自我监督预测驱动的好奇探索，” 见
    *国际机器学习会议 (ICML)*，卷2017，2017年。'
- en: '[140] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros,
    “Large-scale study of curiosity-driven learning,” *arXiv preprint arXiv:1808.04355*,
    2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, 和 A. A. Efros，“关于好奇驱动学习的大规模研究，”
    *arXiv 预印本 arXiv:1808.04355*，2018年。'
- en: '[141] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end
    simulated driving,” in *Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence, San Francisco, California, USA.*, 2017, pp. 2891–2897.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Zhang 和 K. Cho，“针对端到端模拟驾驶的查询高效模仿学习，” 见 *第三十一届AAAI人工智能会议论文集，加州旧金山，美国*，2017年，页码2891–2897。'
- en: '[142] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” *arXiv preprint arXiv:1610.03295*, 2016.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] S. Shalev-Shwartz, S. Shammah, 和 A. Shashua，“安全、多智能体强化学习用于自主驾驶，” *arXiv
    预印本 arXiv:1610.03295*，2016年。'
- en: '[143] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement
    learning and safety based control for autonomous driving,” *arXiv preprint arXiv:1612.00147*,
    2016.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] X. Xiong, J. Wang, F. Zhang, 和 K. Li，“结合深度强化学习和基于安全的控制用于自动驾驶，” *arXiv
    预印本 arXiv:1612.00147*，2016年。'
- en: '[144] C. Ye, H. Ma, X. Zhang, K. Zhang, and S. You, “Survival-oriented reinforcement
    learning model: An effcient and robust deep reinforcement learning algorithm for
    autonomous driving problem,” in *International Conference on Image and Graphics*.   Springer,
    2017, pp. 417–429.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] C. Ye, H. Ma, X. Zhang, K. Zhang, 和 S. You，“面向生存的强化学习模型：一种高效且稳健的深度强化学习算法，用于自动驾驶问题，”
    在 *国际图像与图形会议*。Springer，2017年，第417–429页。'
- en: '[145] J. Garcıa and F. Fernández, “A comprehensive survey on safe reinforcement
    learning,” *Journal of Machine Learning Research*, vol. 16, no. 1, pp. 1437–1480,
    2015.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. García 和 F. Fernández，“关于安全强化学习的全面调查，” *机器学习研究期刊*，第16卷，第1期，第1437–1480页，2015年。'
- en: '[146] P. Palanisamy, “Multi-agent connected autonomous driving using deep reinforcement
    learning,” in *2020 International Joint Conference on Neural Networks (IJCNN)*.   IEEE,
    2020, pp. 1–7.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] P. Palanisamy，“使用深度强化学习的多智能体连接自动驾驶，” 在 *2020国际神经网络联合会议 (IJCNN)*。IEEE，2020年，第1–7页。'
- en: '[147] S. Bhalla, S. Ganapathi Subramanian, and M. Crowley, “Deep multi agent
    reinforcement learning for autonomous driving,” in *Advances in Artificial Intelligence*,
    C. Goutte and X. Zhu, Eds.   Cham: Springer International Publishing, 2020, pp.
    67–78.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Bhalla, S. Ganapathi Subramanian, 和 M. Crowley，“用于自动驾驶的深度多智能体强化学习，”
    在 *人工智能进展*，C. Goutte 和 X. Zhu 编。Cham: Springer International Publishing，2020年，第67–78页。'
- en: '[148] A. Wachi, “Failure-scenario maker for rule-based agent using multi-agent
    adversarial reinforcement learning and its application to autonomous driving,”
    *arXiv preprint arXiv:1903.10654*, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Wachi，“基于规则的智能体的失败场景生成器，使用多智能体对抗强化学习及其在自动驾驶中的应用，” *arXiv 预印本 arXiv:1903.10654*，2019年。'
- en: '[149] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
    G. Tan, “Distributed multiagent coordinated learning for autonomous driving in
    highways based on dynamic coordination graphs,” *IEEE Transactions on Intelligent
    Transportation Systems*, vol. 21, no. 2, pp. 735–748, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, 和 G.
    Tan，“基于动态协调图的高速公路自动驾驶的分布式多智能体协调学习，” *IEEE 智能交通系统汇刊*，第21卷，第2期，第735–748页，2020年。'
- en: '[150] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,
    J. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,” [https://github.com/openai/baselines](https://github.com/openai/baselines),
    2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,
    J. Schulman, S. Sidor, Y. Wu, 和 P. Zhokhov，“OpenAI 基线，” [https://github.com/openai/baselines](https://github.com/openai/baselines)，2017年。'
- en: '[151] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and
    D. Lange, “Unity: A general platform for intelligent agents,” *arXiv preprint
    arXiv:1809.02627*, 2018.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, 和 D.
    Lange，“Unity: 一个通用智能体平台，” *arXiv 预印本 arXiv:1809.02627*，2018年。'
- en: '[152] I. Caspi, G. Leibovich, G. Novik, and S. Endrawis, “Reinforcement learning
    coach,” Dec. 2017\. [Online]. Available: [https://doi.org/10.5281/zenodo.1134899](https://doi.org/10.5281/zenodo.1134899)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] I. Caspi, G. Leibovich, G. Novik, 和 S. Endrawis，“强化学习教练，” 2017年12月。[在线].
    可用: [https://doi.org/10.5281/zenodo.1134899](https://doi.org/10.5281/zenodo.1134899)'
- en: '[153] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan
    Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Chris Harris, Vincent
    Vanhoucke, Eugene Brevdo, “TF-Agents: A library for reinforcement learning in
    tensorflow,” [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents),
    2018, [Online; accessed 25-June-2019]. [Online]. Available: [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan
    Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Chris Harris, Vincent
    Vanhoucke, Eugene Brevdo，“TF-Agents: TensorFlow中的强化学习库，” [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)，2018年，[在线；访问日期为2019年6月25日]。[在线].
    可用: [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)'
- en: '[154] A. Stooke and P. Abbeel, “rlpyt: A research code base for deep reinforcement
    learning in pytorch,” *arXiv preprint arXiv:1909.01500*, 2019.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Stooke 和 P. Abbeel，“rlpyt: PyTorch中深度强化学习的研究代码库，” *arXiv 预印本 arXiv:1909.01500*，2019年。'
- en: '[155] I. Osband, Y. Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva,
    K. McKinney, T. Lattimore, C. Szepezvari, S. Singh *et al.*, “Behaviour suite
    for reinforcement learning,” *arXiv preprint arXiv:1908.03568*, 2019.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] I. Osband, Y. Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva,
    K. McKinney, T. Lattimore, C. Szepezvari, S. Singh *等*，“强化学习行为套件，” *arXiv 预印本
    arXiv:1908.03568*，2019年。'
- en: '| ![[Uncaptioned image]](img/3e67d3ac06ec78c2c94bd94ed9a978dc.png) | B Ravi
    Kiran is the technical lead of machine learning team at Navya, designing and deploying
    realtime deep learning architectures for perception tasks on autonomous shuttles.
    During his 6 years in academic research, he has worked on DNNs for video anomaly
    detection, online time series anomaly detection, hyperspectral image processing
    for tumor detection. He finished his PhD at Paris-Est in 2014 entitled Energetic
    lattice based optimization which was awarded the MSTIC prize. He has worked in
    academic research for over 4 years in embedded programming and in autonomous driving.
    During his career he has published over 40 articles and journals. |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/3e67d3ac06ec78c2c94bd94ed9a978dc.png) | B Ravi Kiran 是 Navya
    机器学习团队的技术负责人，设计和部署实时深度学习架构以用于自动驾驶班车的感知任务。在6年的学术研究中，他曾从事视频异常检测、在线时间序列异常检测、肿瘤检测的高光谱图像处理等工作。他于2014年在巴黎东部大学获得博士学位，论文题目为“基于能量晶格的优化”，并获得
    MSTIC 奖。他在嵌入式编程和自动驾驶领域有超过4年的学术研究经验。在他的职业生涯中，他发表了40多篇文章和期刊论文。 |'
- en: '| ![[Uncaptioned image]](img/d55e25f7bd9bcf9ac6ba744605ebf4d8.png) | Ibrahim
    Sobh Ibrahim has more than 20 years of experience in the area of Machine Learning
    and Software Development. Dr. Sobh received his PhD in Deep Reinforcement Learning
    for fast learning agents acting in 3D environments. He received his B.Sc. and
    M.Sc. degrees in Computer Engineering from Cairo University Faculty of Engineering.
    His M.Sc. Thesis is in the field of Machine Learning applied on automatic documents
    summarization. Ibrahim has participated in several related national and international
    mega projects, conferences and summits. He delivers training and lectures for
    academic and industrial entities. Ibrahim’s publications including international
    journals and conference papers are mainly in the machine and deep learning fields.
    His area of research is mainly in Computer vision, Natural language processing
    and Speech processing. Currently, Dr. Sobh is a Senior Expert of AI, Valeo. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/d55e25f7bd9bcf9ac6ba744605ebf4d8.png) | Ibrahim Sobh Ibrahim
    在机器学习和软件开发领域拥有超过20年的经验。Sobh 博士获得了有关快速学习代理在3D环境中进行深度强化学习的博士学位。他在开罗大学工程学院获得了计算机工程学士和硕士学位。他的硕士论文涉及应用于自动文档摘要的机器学习领域。Ibrahim
    参与了多个相关的国家和国际大型项目、会议和峰会。他为学术和工业实体提供培训和讲座。Ibrahim 的出版物，包括国际期刊和会议论文，主要集中在机器学习和深度学习领域。他的研究领域主要为计算机视觉、自然语言处理和语音处理。目前，Sobh
    博士是 Valeo 的 AI 高级专家。'
- en: '| ![[Uncaptioned image]](img/4472393fb6ad7e7cef1590650322cede.png) | Victor
    Talpaert is a PhD student at U2IS, ENSTA Paris, Institut Polytechnique de Paris,
    91120 Palaiseau, France. His PhD is directed by Bruno Monsuez since 2017, the
    lab speciality is in robotics and complex systems. His PhD is co-sponsored by
    AKKA Technologies in Gyuancourt, France, through the guidance of AKKA’s Autonomous
    Systems Team. This team has a large focus on Autonomous Driving (AD) and the automotive
    industry in general. His PhD subject is learning decision making for AD, with
    assumptions such as a modular AD pipeline, learned features compatible with classic
    robotic approaches and ontology based hierarchical abstractions. |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/4472393fb6ad7e7cef1590650322cede.png) | Victor Talpaert 是巴黎综合理工学院
    U2IS 的博士生，他的博士研究自2017年起由 Bruno Monsuez 指导，实验室专注于机器人技术和复杂系统。他的博士项目由法国 Gyuancourt
    的 AKKA Technologies 共同资助，受 AKKA 自动系统团队的指导。该团队专注于**自动驾驶 (AD)**及汽车行业。其博士课题为 AD 的决策学习，假设包括模块化
    AD 管道、与经典机器人方法兼容的学习特征和基于本体的层次抽象。 |'
- en: '| ![[Uncaptioned image]](img/a6ed0fecc5287617c88dd538a5b3cc05.png) | Patrick
    Mannion is a permanent member of academic staff at National University of Ireland
    Galway, where he lectures in Computer Science. He is also Deputy Editor of The
    Knowledge Engineering Review journal. He received a BEng in Civil Engineering,
    a HDip in Software Development and a PhD in Machine Learning from National University
    of Ireland Galway, a PgCert in Teaching & Learning from Galway-Mayo IT and a PgCert
    in Sensors for Autonomous Vehicles from IT Sligo. He is a former Irish Research
    Council Scholar and a former Fulbright Scholar. His main research interests include
    machine learning, multi-agent systems, multi-objective optimisation, game theory
    and metaheuristic algorithms, with applications to domains such as transportation,
    autonomous vehicles, energy systems and smart grids. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/a6ed0fecc5287617c88dd538a5b3cc05.png) | **Patrick Mannion**
    是爱尔兰国立大学戈尔韦分校的常任学术成员，讲授计算机科学课程。他还是《知识工程评论》期刊的副主编。他获得了爱尔兰国立大学戈尔韦分校的土木工程学士学位、软件开发高级文凭和机器学习博士学位，还获得了戈尔韦-梅奥理工学院的教学与学习研究生证书以及斯莱戈理工学院的自动驾驶车辆传感器研究生证书。他曾是爱尔兰研究委员会奖学金获得者和富布赖特奖学金获得者。他的主要研究兴趣包括机器学习、多智能体系统、多目标优化、博弈论和元启发式算法，应用于交通、自动驾驶车辆、能源系统和智能电网等领域。
    |'
- en: '| ![[Uncaptioned image]](img/0e55b016d33ecd1e393068281ee32604.png) | Ahmad
    El Sallab Ahmad El Sallab is the Senior Chief Engineer of Deep Learning at Valeo
    Egypt, and Senior Expert at Valeo Group. Ahmad has 15 years of experience in Machine
    Learning and Deep Learning, where he acquired his M.Sc. and Ph.D. on 2009 and
    2013 in the field. He has worked for reputable multi-national organizations in
    the industry since 2005 like Intel and Valeo. He has over 35 publications and
    book chapters in Deep Learning in top IEEE and ACM journals and conferences, in
    addition to 30 patents, with applications in Speech, NLP, Computer Vision and
    Robotics. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/0e55b016d33ecd1e393068281ee32604.png) | **Ahmad El Sallab**
    是 Valeo Egypt 的深度学习高级首席工程师，同时也是 Valeo Group 的高级专家。**Ahmad** 在机器学习和深度学习领域有 15 年的经验，他于
    2009 年和 2013 年获得硕士和博士学位。他自 2005 年以来在 Intel 和 Valeo 等知名跨国组织工作。他在顶级 IEEE 和 ACM 期刊及会议上发表了
    35 篇以上的论文和书籍章节，并拥有 30 项专利，应用于语音、自然语言处理、计算机视觉和机器人技术。 |'
- en: '| ![[Uncaptioned image]](img/e924afc62901b44fe36a8898b617067b.png) | Senthil
    Yogamani is an Artificial Intelligence architect and technical leader at Valeo
    Ireland. He leads the research and design of AI algorithms for various modules
    of autonomous driving systems. He has over 14 years of experience in computer
    vision and machine learning including 12 years of experience in industrial automotive
    systems. He is an author of over 90 publications and 60 patents with 1300+ citations.
    He serves in the editorial board of various leading IEEE automotive conferences
    including ITSC, IV and ICVES and advisory board of various industry consortia
    including Khronos, Cognitive Vehicles and IS Auto. He is a recipient of best associate
    editor award at ITSC 2015 and best paper award at ITST 2012. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/e924afc62901b44fe36a8898b617067b.png) | **Senthil Yogamani**
    是 Valeo Ireland 的人工智能架构师和技术领导者。他领导了各种自动驾驶系统模块的 AI 算法研究和设计。他在计算机视觉和机器学习领域拥有超过 14
    年的经验，包括 12 年的工业汽车系统经验。他是 90 多篇论文和 60 项专利的作者，引用次数超过 1300 次。他在多项领先的 IEEE 汽车会议（如
    ITSC、IV 和 ICVES）以及多个行业联盟（如 Khronos、Cognitive Vehicles 和 IS Auto）的咨询委员会中担任编辑委员会成员。他获得了
    ITSC 2015 年最佳副编辑奖和 ITST 2012 年最佳论文奖。'
- en: '| ![[Uncaptioned image]](img/00020080cf9cdddf71a942644c512680.png) | Patrick
    Pérez is Scientific Director of Valeo.ai, a Valeo research lab on artificial intelligence
    for automotive applications. He is currently on the Editorial Board of the International
    Journal of Computer Vision. Before joining Valeo, Patrick Pérez has been Distinguished
    Scientist at Technicolor (2009-2918), researcher at Inria (1993-2000, 2004-2009)
    and at Microsoft Research Cambridge (2000-2004). His research interests include
    multimodal scene understanding and computational imaging. |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图像]](img/00020080cf9cdddf71a942644c512680.png) | **Patrick Pérez**
    是 Valeo.ai 的科学主任，这是一个专注于汽车应用的人工智能研究实验室。他目前是《国际计算机视觉期刊》的编辑委员会成员。在加入 Valeo 之前，**Patrick
    Pérez** 曾在 Technicolor 担任杰出科学家（2009-2018），在 Inria（1993-2000, 2004-2009）和微软研究院剑桥分部（2000-2004）担任研究员。他的研究兴趣包括多模态场景理解和计算成像。
    |'
