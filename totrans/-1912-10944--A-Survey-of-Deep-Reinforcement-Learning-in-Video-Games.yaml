- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:03:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1912.10944] A Survey of Deep Reinforcement Learning in Video Games'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10944](https://ar5iv.labs.arxiv.org/html/1912.10944)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Reinforcement Learning in Video Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kun Shao, Zhentao Tang, Yuanheng Zhu, , Nannan Li, and Dongbin Zhao K. Shao,
    Z. Tang, Y. Zhu, N. Li, and D. Zhao are with the State Key Laboratory of Management
    and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences.
    Beijing 100190, China. They are also with the University of Chinese Academy of
    Sciences, Beijing, China (e-mail: shaokun2014@ia.ac.cn; tangzhentao2016@ia.ac.cn;
    yuanheng.zhu@ia.ac.cn; linannan2017@ia.ac.cn, dongbin.zhao@ia.ac.cn).This work
    is supported by National Natural Science Foundation of China (NSFC) under Grants
    No.61573353, No.61603382, No.6180337, and No.61533017.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep reinforcement learning (DRL) has made great achievements since proposed.
    Generally, DRL agents receive high-dimensional inputs at each step, and make actions
    according to deep-neural-network-based policies. This learning mechanism updates
    the policy to maximize the return with an end-to-end method. In this paper, we
    survey the progress of DRL methods, including value-based, policy gradient, and
    model-based algorithms, and compare their main techniques and properties. Besides,
    DRL plays an important role in game artificial intelligence (AI). We also take
    a review of the achievements of DRL in various video games, including classical
    Arcade games, first-person perspective games and multi-agent real-time strategy
    games, from 2D to 3D, and from single-agent to multi-agent. A large number of
    video game AIs with DRL have achieved super-human performance, while there are
    still some challenges in this domain. Therefore, we also discuss some key points
    when applying DRL methods to this field, including exploration-exploitation, sample
    efficiency, generalization and transfer, multi-agent learning, imperfect information,
    and delayed spare rewards, as well as some research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: reinforcement learning, deep learning, deep reinforcement learning, game AI,
    video games.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial intelligence (AI) in video games is a long-standing research area.
    It studies how to use AI technologies to achieve human-level performance when
    playing games. More generally, it studies the complex interactions between agents
    and game environments. Various games provide interesting and complex problems
    for agents to solve, making video games perfect environments for AI research.
    These virtual environments are safe and controllable. In addition, these game
    environments provide infinite supply of useful data for machine learning algorithms,
    and they are much faster than real-time. These characteristics make games the
    unique and favorite domain for AI research. On the other side, AI has been helping
    games to become better in the way we play, understand and design them [[1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: Broadly speaking, game AI involves the perception and the decision-making in
    game environments. With these components, there are some crucial challenges and
    proposed solutions. The first challenge is that the state space of the game is
    very large, especially in strategic games. With the rise of representation learning,
    the whole system has successfully modeled large-scale state space with deep neural
    networks. The second challenge is that learning proper policies to make decisions
    in dynamic unknown environment is difficult. For this problem, data-driven methods,
    such as supervised learning and reinforcement learning (RL), are feasible solutions.
    The third challenge is that the vast majority of game AI is developed in a specified
    virtual environment. How to transfer the AI’s ability among different games is
    a core challenge. A more general learning system is also necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b44f2244796a155e7348c7d0c847bf84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The framework diagram of the typical DRL for video games. The deep
    learning model takes input from video games API, and extract meaningful features
    automatically. DRL agents produces actions based on these features, and make the
    environments transfer to next state.'
  prefs: []
  type: TYPE_NORMAL
- en: For a long time, solving these challenges with reinforcement learning is widely
    used in game AI. And in the last few years, deep learning (DL) has achieved remarkable
    performance in computer vision and natural language processing [[2](#bib.bib2)].
    The combination, deep reinforcement learning (DRL), teaches agents to make decisions
    in high-dimensional state space in an end-to-end framework, and dramatically improves
    the generalization and scalability of traditional RL algorithms. Especially, DRL
    has made great progress in video games, including Atari, ViZDoom, StarCraft, Dota2,
    and so on. There are some related works to introduce these achievements in this
    field. Zhao et al. [[3](#bib.bib3)] and Tang et al. [[4](#bib.bib4)] survey the
    development of DRL research, and focus on AlphaGo and AlphaGo Zero. Justesen et
    al. [[5](#bib.bib5)] reviews DL-based methods in video game play, including supervised
    learning, unsupervised learning, reinforcement learning, evolutionary approaches,
    and some hybrid approaches. Arulkumaran et al. [[6](#bib.bib6)] make a brief introduction
    of DRL, covering central algorithms and presenting a range of visual RL domains.
    Li [[7](#bib.bib7)] gives an overview of recent achievements of DRL, and discusses
    core elements, important mechanisms, and various applications. In this paper,
    we focus on DRL-based game AI, from 2D to 3D, and from single-agent to multi-agent.
    The main contributions include the comprehensive and detailed comparisons of various
    DRL methods, their techniques, properties, and the impressive and diverse performances
    in these given video games.
  prefs: []
  type: TYPE_NORMAL
- en: The organization of the remaining paper is arranged as follows. In Section II,
    we introduce the background of DL and RL. In Section III, we focus on recent DRL
    methods, including value-based, policy gradient, and model-based DRL methods.
    After that, we make a brief introduction of research platforms and competitions,
    and present performances of DRL methods in classical single-agent Arcade games,
    first-person perspective games, and multi-agent real-time strategy games. In Section
    V, we discuss some key points and research directions in this field. In the end,
    we draw a conclusion of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally speaking, training an agent to make decisions with high-dimensional
    inputs is difficult. With the development of deep learning, researchers take deep
    neural networks as function approximations, and use plenty of samples to optimize
    policies successfully. The framework diagram of typical DRL for video games is
    depicted in Fig. 1.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning comes from artificial neural networks, and is used to learn data
    representation. It is inspired by the theory of brain development, and can be
    learned in supervised learning, unsupervised learning and semi-supervised learning.
    Although the term deep learning is introduced in 1986 [[8](#bib.bib8)], deep learning
    has a winter time because of lacking data and incapable computation hardware.
    However, with more and more large-scale datasets being released, and capable hardware
    being available, a big revolution happens in DL [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural network (CNN) [[10](#bib.bib10)] is a class of deep neural
    networks, which is widely applied to computer vision. CNN is inspired by biological
    processes, and is shift invariant based on shared-weights architecture. Recurrent
    Neural Network (RNN) is another kind of deep nerial network, especially for natural
    language processing. As a special kind of RNN, Long Short Term Memory (LSTM) [[11](#bib.bib11)]
    is capable of learning long-term dependencies. Deep learning architectures have
    been applied into many fields, and have achieved significant successes, such as
    speech recognition, image classification and segmentation, semantic comprehension,
    and machine translation [[2](#bib.bib2)]. DL-based methods with efficient parallel
    distributed computing resources can break the limit of traditional machine learning
    methods. This method inspires scientists and researchers to achieve more and more
    state-of-the-art performance in respective fields.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f67c17bcdb93a07a86471a4ba12f261d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The network architectures of typical DRL methods, with increased
    complexity and performance. (a): DQN network; (b)Dueling DQN network; (c): DRQN
    network; (d): Actor-critic network; (e): Reactor network.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning is a kind of machine learning methods where agents learn
    the optimal policy by trial and error [[12](#bib.bib12)]. By interacting with
    the environment, RL can be successfully applied to sequential decision-making
    tasks. Considering a discounted episodic Markov decision process (MDP) $(S,A,\gamma,P,r)$,
    the agent chooses an action $a_{t}$ according to the policy $\pi(a_{t}|s_{t})$
    at state $s_{t}$. The environment receives the action, produces a reward $r_{t+1}$
    and transfers to the next state $s_{t+1}$ according to the transition probability
    $P(s_{t+1}|s_{t},a_{t})$. This transition probability is unknown in RL domain.
    The process continues until the agent reaches a terminal state or a maximum time
    step. The objective is to maximize the expected discounted cumulative rewards
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{E}_{\pi}[R_{t}]=\mathbb{E}_{\pi}[\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}],$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma\in(0,1]$ is the discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning can be devided into off-policy and on-policy methods.
    Off-policy RL algorithms mean that the behavior policy used for selecting actions
    is different from the learning policy. On the contrary, behavior policy is the
    same with the learning policy in on-policy RL algorithms. Besides, reinforcement
    learning can also be devided into value-based and policy-based methods. In value-based
    RL, agents update the value function to learn suitable policy, while policy-based
    RL agents learn the policy directly.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning is a typical off-policy value-based method. The update rule of Q-learning
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\delta_{t}=r_{t+1}+\gamma\arg\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t}),$
    |  | (2a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\delta_{t}.$
    |  | (2b) |'
  prefs: []
  type: TYPE_TB
- en: $\delta_{t}$ is the temporal difference (TD) error, and $\alpha$ is the learning
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient [[13](#bib.bib13)] parameterizes the policy and updates parameters
    $\theta$. In its general form, the objective function of policy gradient is defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\log\pi_{\theta}(a_{t}&#124;s_{t})R].$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: $R$ is the total accumulated return.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critic [[12](#bib.bib12)] reinforcement learning improves the policy gradient
    with an value-based critic
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\Psi_{t}\log\pi_{\theta}(a_{t}&#124;s_{t})].$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: $\Psi_{t}$ is the critic, which can be the state-action value function $Q^{\pi}(s_{t},a_{t})$,
    the advantage function $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$
    or the TD error $r_{t}+V^{\pi}(s_{t+1})-V^{\pi}(s_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DRL makes a combination of DL and RL, achieving rapid developments since proposed.
    This section will introduce various DRL methods, including value-based methods,
    policy gradient methods, and model-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Value-based DRL methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Q-network (DQN) [[14](#bib.bib14)] is the most famous DRL model which learns
    policies directly from high-dimensional inputs. It receives raw pixels, and outputs
    a value function to estimate future rewards, as shown in Fig. 2(a). DQN uses the
    experience replay method to break the sample correlation, and stabilizes the learning
    process with a target Q-network. The loss function at iteration $i$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{aligned} L_{i}(\theta_{i})=E_{(s,a,r,s^{\prime})\sim
    U(D)}[(y_{i}^{DQN}-Q(s,a;{\theta_{i}}))^{2}],\end{aligned}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{i}^{DQN}=r+\gamma\mathop{\max}\limits_{a^{\prime}}Q(s^{\prime},a^{\prime};{\theta_{i}{{}^{-}}}).$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'DQN bridges the gap between high-dimensional visual inputs and actions. After
    that, researchers have improved DQN in different aspects. Double DQN [[15](#bib.bib15)]
    introduces double Q-learning to reduce observed overestimations, and it leads
    to much better performance. Prioritized experience replay [[16](#bib.bib16)] helps
    prioritize experience to replay important transitions more frequently. The sample
    probability of transition $i$ as $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}p_{k}^{\alpha}}$,
    where $p_{i}$ is the priority of transition $i$. Dueling DQN [[17](#bib.bib17)]
    uses the dueling neural network architecture for model-free DRL. It includes two
    separate estimators: one for state value function $V(s;\theta,\beta)$ and the
    other for advantage function $A(s,a;\theta,\alpha)$, as shown in Fig. 2(b).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(s,a:\theta,\alpha,\beta)=V(s;\theta,\beta)+A(s,a;\theta,\alpha).$ |  |
    (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Pop-Art [[18](#bib.bib18)] is proposed to adapt to different and non-stationary
    target magnitudes, which successfully replaces the clipping of rewards as done
    in DQN to handle various magnitudes of targets. Fast reward propagation [[19](#bib.bib19)]
    is a novel training algorithm for reinforcement learning, which combines the strength
    of DQN, and exploits longer state-transitions in experience replays by tightening
    the optimization via constraints. This novel technique makes DRL more practical
    by drastically reducing training time. Gorila [[20](#bib.bib20)] is the first
    massively distributed architecture for DRL. This architecture uses four main components:
    parallel actors; parallel learners; a distributed neural network to represent
    the value function or behavior policy; and a distributed store of experience.
    To address the limited memory and imperfect game information at each decision
    point, Deep Recurrent Q-Network (DRQN) [[21](#bib.bib21)] replaces the first fully-connected
    layer with a recurrent neural network in DQN, as shown in Fig. 2(c).'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, DQN learns rich domain representations and approximates the value
    function with deep neural networks, while batch RL algorithms with linear representations
    are more stable and require less hyperparameter tuning. The Least Squares DQN
    (LS-DQN) [[22](#bib.bib22)] combines DQN’s rich feature representations with the
    stability of a linear least squares method. In order to reduce approximation error
    variance in DQN‘s target values, averaged-DQN [[23](#bib.bib23)] averages previous
    Q-values estimates, leading to a more stable training and improved performance.
    Deep Q-learning from Demonstrations (DQfD) [[24](#bib.bib24)] combines DQN with
    human demonstrations, which improves the sample efficiency greatly. DQV [[25](#bib.bib25)]
    uses TD learning to train a Value neural network, and uses this network to train
    a second Quality-value network to estimate state-action values. DQV learns significantly
    faster and better than double-DQN. Researchers have proposed several improvements
    to DQN. However, it is unclear which of these are complementary and how much can
    be combined. Rainbow [[26](#bib.bib26)] combines with main extensions to DQN,
    and gives each component’s contribution to overall performance. RUDDER [[27](#bib.bib27)]
    is a novel reinforcement learning approach for finite MDPs with delayed rewards,
    which is also a return decomposition method, RUDDER is exponentially faster on
    tasks with different lengths of reward delays. Ape-X DQfD[[28](#bib.bib28)] uses
    a new transformed Bellman operator to process rewards of varying densities and
    scales, and applies human demonstrations to ease the exploration problem to guide
    agents towards rewarding states. Additional, it proposes an auxiliary temporal
    consistency loss to train stably extending the effective planning horizon by an
    order of magnitude. Soft DQN [[29](#bib.bib29)] is an entropy-regularized versions
    of Q-learning, with better robustness and generalization .
  prefs: []
  type: TYPE_NORMAL
- en: Distributional DRL learns the value distribution, in contrast to common RL that
    models the expectation of return, or value. C51 [[30](#bib.bib30)] focuses on
    the distribution of value, and designs distributional DQN algorithm to learn approximate
    value distributions. QR-DQN [[31](#bib.bib31)] methods close a number of gaps
    between theoretical and algorithmic results. Distributional reinforcement learning
    with Quantile regression in which the distribution over returns is modeled explicitly
    instead of only estimating the mean. Implicit Quantile Networks (IQN) [[32](#bib.bib32)]
    is a flexible, applicable, and state-of-the-art distributional DQN. IQN approximates
    the full Quantile function for the return distribution with Quantile regression,
    and provides a fully integrated distributional RL agent without prior assumptions
    on the parameterization of the return distribution. Furthermore, IQN allows to
    expand the class of control policies to a wide range of risk-sensitive policies
    connected to distortion risk measures.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Policy gradient DRL methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy gradient DRL optimizes the parameterized policy directly. Actor-critic
    architecture computes the policy gradient using a value-based critic function
    to estimate expected future reward, as shown in Fig. 2(d). Asynchronous DRL is
    an efficient framework for DRL that uses asynchronous gradient descent to optimize
    the policy[[33](#bib.bib33)]. Asynchronous advantage actor-critic (A3C) trains
    several agents on multiple environments, showing a stabilizing effect on training.
    The objective function of the actor is demonstrated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}A_{\theta,\theta_{v}}(s_{t},a_{t})\log\pi_{\theta}(a_{t}&#124;s_{t})+\beta
    H_{\theta}(\pi(s_{t}))],$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $H_{\theta}(\pi(s_{t}))$ is an entropy term used to encourage exploration.
  prefs: []
  type: TYPE_NORMAL
- en: GA3C [[34](#bib.bib34)] is a hybrid CPU/GPU version of A3C, which achieves a
    significant speed up compared to the original CPU implementation. UNsupervised
    REinforcement and Auxiliary Learning (UNREAL) [[35](#bib.bib35)] learns separate
    policies for maximizing many other pseudo-reward functions simultaneously, including
    value function replay, reward prediction, and pixel control. This agent drastically
    improves both data efficiency and robustness to hyperparameter settings. PAAC
    [[36](#bib.bib36)] is a novel framework for efficient parallelization of DRL,
    where multiple actors learn the policy on a single machine. Policy gradient methods
    are efficient techniques for policies improvement, while they are usually on-policy
    and unable to take advantage of off-policy data. The new method is referred as
    PGQ[[37](#bib.bib37)], which combines policy gradient with Q-learning. PGQ establishes
    an equivalency between regularized policy gradient techniques and advantage function
    learning algorithms. Retrace($\lambda$) [[38](#bib.bib38)] takes the best of the
    importance sampling, off-policy Q($\lambda$), and tree-backup($\lambda$), resulting
    in low variance, safety, and efficiency. It makes a combination of dueling DRQN
    architecture and actor-critic architecture, as shown in Fig. 2(e). Reactor [[39](#bib.bib39)]
    is a sample-efficient and numerical efficient reinforcement learning agent based
    on a multi-step return off-policy actor-critic architecture. The network outputs
    a target policy, an action-value Q-function, and an estimated behavioral policy.
    The critic is trained with the off-policy multi-step Retrace method and the actor
    is trained by a $\beta$-leave-one-out policy gradient. Importance-Weighted Actor
    Learner Architecture (IMPALA) [[40](#bib.bib40)] is a new distributed DRL, which
    can scale to thousands of machine. IMPALA uses a single reinforcement learning
    agent with a single set of parameters to solve a mass of tasks. This method achieves
    stable learning by combining decoupled acting and learning with a novel V-trace
    off-policy correction method, which is critical for achieving learning stability.
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Trust region method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Trust Region Policy Optimization (TRPO) [[41](#bib.bib41)] is proposed for optimizing
    control policies, with guaranteed monotonic improvement. TRPO computes an ascent
    direction to improve on policy gradient, which can ensure a small change in the
    policy distribution. The constrained optimization problem of TRPO in each epoch
    is
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle maximize_{\theta}\ \ \ E_{s\sim\rho_{\theta^{\prime}},a\sim\pi_{\theta^{\prime}}}[\frac{\pi_{\theta}(a&#124;s)}{\pi_{\theta^{\prime}}(a&#124;s)}A_{\theta^{\prime}}(s,a)],$
    |  | (9a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.\ \ \ \ E_{s\sim\rho_{\theta^{\prime}}}[D_{KL}(\pi_{\theta^{\prime}}(\cdot&#124;s))]\leq\delta_{KL}.$
    |  | (9b) |'
  prefs: []
  type: TYPE_TB
- en: This algorithm is effective for optimizing large nonlinear policies. Proximal
    policy optimization (PPO) [[42](#bib.bib42)] samples data by interaction with
    the environment, and optimizes the objective function with stochastic gradient
    ascent
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}&#124;s_{t})}{\pi_{\theta_{old}}(a_{t}&#124;s_{t})},$
    |  | (10a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L(\theta)=\hat{\mathbb{E}}_{t}[min(r_{t}(\theta)\hat{A}_{t},clip(r_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t}].$
    |  | (10b) |'
  prefs: []
  type: TYPE_TB
- en: $r_{t}(\theta)$ denotes the probability ratio. This objective function clips
    the probability ratio to modify the surrogate objective. PPO has some benefits
    over TRPO, and is much simpler to implement, with better sample complexity. Actor-critic
    with experience replay (ACER) [[43](#bib.bib43)] introduces several innovations,
    including stochastic dueling network, truncated importance sampling, and a new
    trust region method, which is stable and sample efficient. Actor-critic using
    Kronecker-Factored Trust Region (ACKTR) [[44](#bib.bib44)] bases on natural policy
    gradient, and uses Kronecker-factored approximate curvature (K-FAC) with trust
    region to optimize the actor and the critic. ACKTR is sample efficient compared
    with other actor-critic methods.
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Deterministic policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apart from stochastic policy, deep deterministic policy gradient (DDPG) [[45](#bib.bib45)]
    is a kind of deterministic policy gradient method which adapts the success of
    DQN to continuous control. The update rule of DDPG is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\gamma Q(s_{t+1},\pi_{\theta}(s_{t+1})).$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: DDPG is an actor-critic, off-policy algorithm, and is able to learn reasonable
    policies on various tasks. Distributed Distributional DDPG (D4PG) [[46](#bib.bib46)]
    is a distributional update to DDPG, combined with the use of multiple distributed
    workers all writing into the same replay table. This method has a much better
    performance on a number of difficult continuous control problems.
  prefs: []
  type: TYPE_NORMAL
- en: III-B3 Entropy-regularized policy gradient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Soft Actor Critic (SAC) is an off-policy policy gradient method, which establishes
    a bridge between DDPG and stochastic policy optimization. SAC incorporates the
    clipped double-Q trick, and the objective function of maximum entropy DRL is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $J(\pi)=\sum_{t=0}^{T}\mathbb{E}_{(s_{t},a_{t})\sim\rho_{\pi}}[r(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))],$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: SAC uses an entropy regularization in its objective function. It trains the
    policy to maximize a trade-off between entropy and expected return. The entropy
    is a measure of randomness in the policy. This mechanism is similar to the trade-off
    between exploration and exploitation. Increasing entropy can encourage more exploration,
    and accelerate learning process. Moreover, it can also prevent the learning policy
    from converging to a poor local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Model-based DRL methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining model-free reinforcement learning with on-line planning is a promising
    approach to solve the sample efficiency problem. TreeQN [[47](#bib.bib47)] is
    proposed to address these challenges. It is a differentiable, recursive, tree-structured
    model that serves as a drop-in replacement for any value function network in DRL
    with discrete actions. TreeQN dynamically constructs a tree by recursively applying
    a transition model in a learned abstract state space and then aggregating predicted
    rewards and state-values using a tree backup to estimate Q-values. ATreeC is an
    actor-critic variant that augments TreeQN with a softmax layer to form a stochastic
    policy network. Both approaches are trained end-to-end, such that the learned
    model is optimized for its actual use in the planner. TreeQN and ATreeC outperform
    n-step DQN and value prediction networks on multiple Atari games. Vezhnevets et
    al. [[48](#bib.bib48)] presents STRategic Attentive Writer (STRAW) neural network
    architecture to build implicit plans. STRAW purely interacts with an environment,
    and is an end-to-end method. STRAW model can learn temporally abstracted high-level
    macro-actions, which enables both economic computation and structured exploration.
    STRAW employs temporally extended planning strategies and achieves strong improvements
    on Atari games. The world model [[49](#bib.bib49)] uses an unsupervised manner
    to train a generative recurrent neural network, which can model RL environments
    through compressed spatiotemporal representations. It feeds extracted features
    into simple and compact policies, achieving impressive results in several environments.
    Value propagation (VProp) [[50](#bib.bib50)] bases on value iteration, and is
    an efficient differentiable planning module. It can successfully be trained to
    learn to plan using reinforcement learning. As a general framework of AlphaZero,
    MuZero[[54](#bib.bib54)] combines MCTS with a learned model, and predicts the
    reward, the action-selection policy, and the value function to make planning.
    It extends model-based RL to a range of logically complex and visually complex
    domains, and achieves superhuman performance.
  prefs: []
  type: TYPE_NORMAL
- en: A general review of various DRL methods from 2017 to 2019 is presented in Table
    I.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A general review of recent DRL methods from 2017 to 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRL Algorithms | Main Techniques | Networks | Category |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DQN [[14](#bib.bib14)] | experience replay, target Q-network | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Double DQN [[15](#bib.bib15)] | double Q-learning | CNN | value-based, off-policy
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dueling DQN [[17](#bib.bib17)] | dueling neural network architecture | CNN
    | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Prioritized DQN [[16](#bib.bib16)] | prioritized experience replay | CNN
    | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Bootstrapped DQN [[51](#bib.bib51)] | combine deep exploration with DNNs
    | CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Gorila [[20](#bib.bib20)] | massively distributed architecture | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| LS-DQN [[22](#bib.bib22)] | combine least-squares updates in DRL | CNN |
    value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Averaged-DQN [[23](#bib.bib23)] | averaging learned Q-values estimates |
    CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| DQfD [[24](#bib.bib24)] | learn from the demonstration data | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| DQN with Pop-Art [[18](#bib.bib18)] | adaptive normalization with Pop-Art
    | CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Soft DQN [[29](#bib.bib29)] | KL penalty and entropy bonus | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| DQV [[25](#bib.bib25)] | training a Quality-value network | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Rainbow [[26](#bib.bib26)] | integrate six extensions to DQN | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| RUDDER [[27](#bib.bib27)] | return decomposition | CNN-LSTM | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Ape-X DQfD [[28](#bib.bib28)] | transformed Bellman operator, temporal consistency
    loss | CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| C51 [[30](#bib.bib30)] | distributional Bellman optimality | CNN | value-based,
    off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| QR-DQN [[31](#bib.bib31)] | distributional RL with Quantile regression |
    CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| IQN [[32](#bib.bib32)] | an implicit representation of the return distribution
    | CNN | value-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| A3C [[33](#bib.bib33)] | asynchronous gradient descent | CNN-LSTM | policy
    gradient, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| GA3C [[34](#bib.bib34)] | hybrid CPU/GPU version | CNN-LSTM | policy gradient,
    on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| PPO [[42](#bib.bib42)] | clipped surrogate objective, adaptive KL penalty
    coefficient | CNN-LSTM | policy gradient, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| ACER [[43](#bib.bib43)] | experience replay, truncated importance sampling
    | CNN-LSTM | policy gradient, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| ACKTR [[44](#bib.bib44)] | K-FAC with trust region | CNN-LSTM | policy gradient,
    on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Soft Actor-Critic [[52](#bib.bib52)] | entropy regularization | CNN | policy
    gradient, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| UNREAL [[35](#bib.bib35)] | unsupervised auxiliary tasks | CNN-LSTM | policy
    gradient, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Reactor [[39](#bib.bib39)] | Retrace($\lambda$), $\beta$-leave-one-out policy
    gradient estimate | CNN-LSTM | policy gradient, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| PAAC [[36](#bib.bib36)] | parallel framework for A3C | CNN | policy gradient,
    on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| DDPG [[45](#bib.bib45)] | DQN with deterministic policy gradient | CNN-LSTM
    | policy gradient, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| TRPO [[41](#bib.bib41)] | incorporate a KL divergence constraint | CNN-LSTM
    | policy gradient , on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| D4PG [[46](#bib.bib46)] | distributed distributional DDPG | CNN | policy
    gradient , on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| PGQ [[37](#bib.bib37)] | combine policy gradient and Q-learning | CNN | policy
    gradient, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| IMPALA [[40](#bib.bib40)] | importance-weighted actor learner architecture
    | CNN-LSTM | policy gradient, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| FiGAR-A3C [[53](#bib.bib53)] | fine grained action repetition | CNN-LSTM
    | policy gradient, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| TreeQN/ATreeC [[47](#bib.bib47)] | on-line planning, tree-structured model
    | CNN | model-based, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| STRAW [[48](#bib.bib48)] | macro-actions, planning strategies | CNN | model-based,
    on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| World model [[49](#bib.bib49)] | mixture density network, variational autoencoder
    | CNN-LSTM | model-based, on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| MuZero [[54](#bib.bib54)] | representation function, dynamics function, and
    prediction function | CNN | model-based, off-policy |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/6559cf0b8f00379e6085db5aecc91fbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The diagram of various video games AI, from 2D to 3D, and from single-agent
    to multi-agent.'
  prefs: []
  type: TYPE_NORMAL
- en: IV DRL in video games
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Playing video games like human experts is challenging for computers. With the
    development of DRL, agents are able to play various games end-to-end. Here we
    focus on game research platforms and competitions, and impressive progress in
    various video games, from 2D to 3D, and from single-agent to multi-agent, as shown
    in Fig. 3.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Game research platforms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE II: A list of game AI competitions suitable for DRL research.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Competition Name | Time |'
  prefs: []
  type: TYPE_TB
- en: '| ViZDoom AI competition | 2016, 2017, 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| StarCraft AI competitions (AIIDE, CIG, SSCAIT) | 2010 — 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| microRTS competition | 2017, 2018, 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| The GVGAI competition – learning track | 2017, 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Microsoft Malmo collaborative AI challenge | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| The multi-agent RL in Malmo competition | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| The OpenAI Retro contest | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| NeurIPS Pommerman competition | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Unity Obstacle Tower Challenge | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| NeurIPS MineRL competition | 2019 |'
  prefs: []
  type: TYPE_TB
- en: 'Platforms and competitions make great contributions to the development of game
    AI, and help to evaluate agents’ intelligence, as presented in Table II. Most
    platforms can be described by two major categories: General Platforms and Specific
    Platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'General Platforms: Arcade Learning Environment (ALE) [[55](#bib.bib55)] is
    the pioneer evaluation platform for DRL algorithms, which provides an interface
    to plenty of Atari 2600 games. ALE presents both game images and signals, such
    as player scores, which makes it a suitable testbed. To promote the progress of
    DRL research, OpenAI integrates a collection of reinforcement learning tasks into
    a platform called Gym [[56](#bib.bib56)], which mainly contains Algorithmic, Atari,
    Classical Control, Board games, 2D and 3D robots. After that, OpenAI Universe
    [[57](#bib.bib57)] is a platform for measuring and training agents’ general intelligence
    across a large supply of games. Gym Retro [[58](#bib.bib58)] is a wrapper for
    video game emulator with a unified interface as Gym, and makes Gym easy to be
    extended with a large collection of video games, not only Atari but also NEC,
    Nintendo, and Sega, for RL research. The OpenAI Retro contest aims at exploring
    the development of DRL that can generalize from previous experience. OpenAI bases
    on the Sonic the Hedgehog^(TM) video game, and presents a new DRL benchmark [[59](#bib.bib59)].
    This benchmark can help to measure the performance of few-shot learning and transfer
    learning in reinforcement learning. General Video Game Playing [[60](#bib.bib60)]
    is intended to design an agent to play multiple video games without human intervention.
    The General Video Game AI (GVGAI) [[61](#bib.bib61)] competition is proposed to
    provide a easy-to-use and open-source platform for evaluating AI methods, including
    DRL. DeepMind Lab [[62](#bib.bib62)] is a first-person perspective learning environment,
    and provides multiple complicated tasks in partially observed, large-scale, and
    visually diverse worlds. Unity ML-Agents Toolkit[[63](#bib.bib63)] is a new toolkit
    for creating and interacting with simulation environments. This platform has sensory,
    physical, cognitive, and social complexity, and enables fast and distributed simulation,
    and flexible control.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specific Platforms: Malmo[[64](#bib.bib64)] is a research platform for AI experiments,
    which is built on top of Minecraft. It is a first-person 3D environment, and can
    be used for multi-agent research in Microsoft Malmo collaborative AI challenge
    2017 and the multi-agent RL in MalmO competition 2018\. TORCS [[65](#bib.bib65)]
    is a racing car simulator which has both low-level and visual features for the
    self-driving car with DRL. ViZDoom [[66](#bib.bib66)] is a first-person shooter
    game platform, and encourages DRL agent to utilize the visual information to perform
    navigation and shooting tasks in a semi-realistic 3D world. ViZDoom AI competition
    has attracted plenty of researchers to develop their DRL-based Doom agents since
    2016\. As far as we know, real-time strategy (RTS) games are very challenging
    for reinforcement learning method. Facebook proposes TorchCraft for StarCraft
    I [[67](#bib.bib67)], and DeepMind releases StarCraft II learning environment
    [[68](#bib.bib68)]. They expect researchers to propose powerful DRL agents to
    achieve high-level performance in RTS games and annual StarCraft AI competitions.
    CoinRun [[69](#bib.bib69)] provides a metric for an agent’s ability to transfer
    its experience to novel situations. This new training environment strikes a desirable
    balance in complexity: the environment is much simpler than traditional platform
    games, but it still poses a worthy generalization challenge for DRL algorithms.
    Google Research Football is a new environment based on open-source game Gameplay
    Football for DRL research.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Atari games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ALE is an evaluation platform that aims at building agents with general intelligence
    across hundreds of Atari 2600 games. As the most popular testbed for DRL research,
    a large number of DRL methods have achieved outstanding performance consecutively.
    Machado et al. [[70](#bib.bib70)] takes a review at the ALE in DRL research community,
    proposes diverse evaluation methodologies and some key concerns. In this section,
    we will introduce the main achievements in the ALE domain, including the extremely
    difficult Montezuma’s Revenge.
  prefs: []
  type: TYPE_NORMAL
- en: As the milestone in this domain, DQN is able to surpass the performances of
    previous algorithms, and achieves human-level performance across 49 games [[14](#bib.bib14)].
    Averaged-DQN examines the source of value function estimation errors, and demonstrates
    significantly improved stability and performance on the ALE benchmark [[23](#bib.bib23)].
    UNREAL significantly outperforms the previous best performance on Atari, averaging
    880% expert human performance [[35](#bib.bib35)]. PAAC achieves sufficiently good
    performance on ALE after a few hours of training [[36](#bib.bib36)]. DQfD has
    better initial performance than DQN on most Atari games, and receives more average
    rewards than DQN on 27 of 42\. In addition, DQfD learns faster than DQN even when
    given poor demonstration data [[24](#bib.bib24)]. Noisy DQN replaces the conventional
    exploration heuristics with NoisyNet, and yields substantially higher scores in
    ALE domain. As a distributional DRL method, C51 obtains a new series of impressive
    results, and demonstrates the importance of the value distribution in approximated
    RL [[30](#bib.bib30)]. Rainbow provides improvements in terms of sample efficiency
    and final performance. The authors also show the contribution of each component
    to overall performance [[26](#bib.bib26)]. QR-DQN algorithm significantly outperforms
    recent improvements on DQN, including the related C51 [[30](#bib.bib30)]. IQN
    shows substantial gains on the Atari benchmark over QR-DQN, and even halves the
    distance between QR-DQN and Rainbow [[32](#bib.bib32)]. Ape-X DQN substantially
    improves the performance on the ALE, achieving better final score in less wall-clock
    training time [[71](#bib.bib71)]. When tested on a set of 42 Atari games, the
    Ape-X DQfD algorithm exceeds the performance of an average human on 40 games using
    a common set of hyperparameters. Mean and median scores across multiple Atari
    games of typical DRL methods that achieve state-of-the-art performance consecutively
    are presented in Table III.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Mean and median scores across 57 Atari games of typical DRL methods,
    measured as percentages of human baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Mean | Median | year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DQN [[14](#bib.bib14)] | 228% | 79% | 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| C51 [[30](#bib.bib30)] | 701% | 178% | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| UNREAL [[35](#bib.bib35)] | 880% | 250% | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| QR-DQN [[30](#bib.bib30)] | 915% | 211% | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| IQN [[32](#bib.bib32)] | 1019% | 218% | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Rainbow [[26](#bib.bib26)] | 1189% | 230% | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Ape-X DQN [[71](#bib.bib71)] | 1695% | 434% | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| Ape-X DQfD ^∗[[28](#bib.bib28)] | 2346% | 702% | 2018 |'
  prefs: []
  type: TYPE_TB
- en: 'Note: ^∗ means this method is measured across 42 Atari games.'
  prefs: []
  type: TYPE_NORMAL
- en: Montezuma’s Revenge is one of the most difficult Atari video games. It is a
    goal-directed behavior learning environment with long horizons and sparse reward
    feedback signals. Players must navigate through a number of different rooms, avoid
    obstacles and traps, climb ladders up and down, and then pick up the key to open
    new rooms. It requires a long sequence of actions before reaching the goal and
    receiving a reward, and is difficult to explore an optimal policy to tackle tasks.
    Efficient exploration is considered as a crucial factor to learn in a delayed
    feedback environment. Then, Ostrovski et al. [[72](#bib.bib72)] provide an improved
    version of count-based exploration with PixelCNN as a supplement for pseudo-count,
    also reveals the importance of Monte Carlo return for effective exploration. In
    addition to improve the exploration efficiency, learning from human data is also
    a proper method to reach better performance in this problem. Le et al. [[73](#bib.bib73)]
    leverage imitation learning from expert interaction and hierarchical reinforcement
    learning at different levels. This method learns obviously faster than original
    hierarchical reinforcement learning, and also significantly more efficiently than
    conventional imitation learning. Other than gameplay, the demonstration is also
    a valuable kind of sample for agent to learn. DQfD utilizes a small set of demonstration
    data to speed up the learning process [[24](#bib.bib24)]. It combines prioritized
    replay mechanism with temporal difference updates and supervised classification,
    and finally achieves a better and impressive result. Further, Aytar et al. [[74](#bib.bib74)]
    only use YouTube video as a demonstration sample and invests a transformed Bellman
    operator for learning from human demonstrations. Interestingly, these two works
    both claim being the first to solve the entire first level of Montezuma’s Revenge.
    Go-explore [[75](#bib.bib75)] makes further progress, and achieves scores over
    400,000 on average. Go-Explore separates learning into exploration and robustification.
    It reliably solves the whole game, and generalizes well.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C First-person perspective games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from Atari games, agents in first-person perspective video games can
    only receive observations from their own perspectives, resulting from imperfect
    information inputs. In RL domain, this is a POMDP problem which requires efficient
    exploration and memory.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 ViZDoom
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First-person shooter (FPS) games play an important role in game AI research.
    Doom is a classical FPS game, and ViZDoom is presented as a novel testbed for
    DRL [[66](#bib.bib66)]. Agents learn from visual inputs, and interact with the
    ViZDoom environment in a first-person perspective. Wu et al. [[76](#bib.bib76)]
    propose a method that combines A3C and curriculum learning. The agent learns to
    navigate and attack via playing against built-in agents progressively. Parisotto
    et al. [[77](#bib.bib77)] develop Neural Map, which is a memory system with an
    adaptable write operator. Neural Map uses a spatially structured 2D memory image
    to store the environment’s information. This method surpasses other DRL memories
    on several challenging ViZDoom maze tasks and shows a capable generalization ability.
    Shao et al. [[78](#bib.bib78)] show that ACKTR can successfully teach agents to
    battle in ViZDoom environment, and significantly outperform A2C agents by a significant
    margin.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 TORCS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TORCS is a racing game where actions are acceleration, braking and steering.
    This game has more realistic graphics than Atari games, but also requires agents
    to learn the dynamic of the car. FIGAR-DDPG can successfully complete the race
    task and finish 20 laps of the circuit, with a 10$\times$ total reward against
    that obtained by DDPG, and much smoother policies [[53](#bib.bib53)]. Normalized
    Actor-Critic (NAC) normalizes the Q-function effectively, and learns an initial
    policy network from demonstration and refine the policy in a real environment
    [[79](#bib.bib79)]. NAC is robust to suboptimal demonstration data, learns robustly
    and outperforms existing baselines when evaluated on TORCS. Mazumder et al. [[80](#bib.bib80)]
    incorporate state-action permissibility (SAP) and DDPG, and applies it to tackle
    the lane keeping problem in TORCS. The proposed method can speedup DRL training
    remarkably for this task. In [[81](#bib.bib81)], a two-stage approach is proposed
    for the vision-based vehicle lateral control problem which includes an multi-task
    learning perception stage and an RL control stage. By exerting the correlation
    between multiple learning task, the perception module can robustly extract track
    features. Additionally, the RL agent learns by maximizing the geometry-based reward
    and performs better than the LQR and MPC controllers. Zhu et al. [[82](#bib.bib82)]
    use DRL to train a CNN to perceive driving data from images of first-person view,
    and learns a controller to get driving commands, showing a promising performance.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Minecraft
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Minecraft is a sandbox construction game, where players can build creative creations,
    structures, and artwork across various game modes. Recently, it becomes a popular
    platform for game AI research, with 3D infinitely varied data. Project Malmo is
    an experimentation platform [[83](#bib.bib83)] that builts on the Minecraft for
    AI research. It supports a large number of scenarios, including navigation, problem
    solving tasks, and survival to collaboration. Xiong et al. [[84](#bib.bib84)]
    propose a novel Q-learning approach with state-action abstraction and warm start
    using human reasoning to learn effective policies in the Microsoft Malmo collaborative
    AI challenge. The ability to transfer knowledge from source task to target task
    in Minecraft is one of the major challenges. Tessler et al. [[85](#bib.bib85)]
    provides a DRL agent which can transfer knowledge by learning reusable skills,
    and then incorporated into hierarchical DRL network (H-DRLN). H-DRLN exhibits
    superior performance and low learning sample complexity compared to regular DQN
    in Minecraft, and the potential to transfer knowledge between related Minecraft
    tasks without any additional learning. To solve the partial or non-Markovian observations
    problems, Jin et al. [[86](#bib.bib86)] propose a new DRL algorithm based on counterfactual
    regret minimization that iteratively updates an approximation to a cumulative
    clipped advantage function. On the challenging Minecraft first-person navigation
    benchmarks, this algorithm can substantially outperform strong baseline methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 DeepMind lab
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DeepMind lab is a 3D first-person game platform extended from OpenArena, which
    is based on Quake3. Comparable to other first-person game platforms, DeepMind
    lab has considerably richer visuals and more realistic physics, making it a significantly
    complex platform. On a challenging suite of DeepMind lab tasks, the UNREAL agent
    leads to a mean speedup in learning of 10$\times$ over A3C and averaging 87% expert
    human performance. As learning agents become more powerful, continual learning
    has made quick progress recently. To test continual learning capabilities, Mankowitz
    et al. [[87](#bib.bib87)] consider an implicit sequence of tasks with sparse rewards
    in DeepMind lab. The novel agent architecture called Unicorn, demonstrates strong
    continual learning and outperforms several baseline agents on the proposed domain.
    Schmitt et al. [[88](#bib.bib88)] present a method which uses teacher agents to
    kickstart the training of a new student agent. On a multi-task and challenging
    DMLab-30 suite, kickstarted training improves new agents’ sample efficiency to
    a great extend, and surpasses the final performance by 42%. Jaderberg et al. [[89](#bib.bib89)]
    focus on Quake III Arena Capture the Flag, which is a popular 3D first-person
    multiplayer video game, and demonstrates that DRL agents can achieve human-level
    performance with only pixels and game points as input. The agent uses population
    based training to optimize the policy. This method trains a large number of agents
    concurrently from thousands of parallel matches, where agents plays cooperatively
    in teams and against each other on randomly generated environments. In an evaluation,
    the trained agents exceed the winrate of self-play baseline and high-level human
    players both as teammates and opponents, and are proved far stronger than existing
    DRL agents.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Real-time strategy games
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Real-time strategy games are very popular among players, and have become popular
    platforms for AI research.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D1 StarCraft
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In StarCraft, players need to perform actions according to real-time game states,
    and defeat the enemies. Generally speaking, designing an AI bot have many challenges,
    including multi-agent collaboration, spatial and temporal reasoning, adversarial
    planning, and opponent modeling. Currently, most bots are based on human experiences
    and replays, with limited flexibility and intelligence. DRL is proved to be a
    promising direction for StarCraft AI, especially in micromanagement, build order,
    mini-games and full-games [[90](#bib.bib90)].
  prefs: []
  type: TYPE_NORMAL
- en: Recently, micromanagement is widely studied as the first step to solve StarCraft
    AI. Usunier et al. [[91](#bib.bib91)] introduce the greedy MDP with episodic zero-order
    optimization (GMEZO) algorithm to tackle micromanagement scenarios, which performs
    better than DQN and policy gradient. BiCNet [[92](#bib.bib92)] is a multi-agent
    deep reinforcement learning method to play StarCraft combat games. It bases on
    actor-critic reinforcement learning, and uses bi-directional neural networks to
    learn collaboration. BiCNet successfully learns some cooperative strategies, and
    is adaptable to various tasks, showing better performances than GMEZO. In aforementioned
    works, researchers mainly develops centralized methods to play micromanagement.
    Foerster et al. [[93](#bib.bib93)] focus on decentralized control for micromanagement,
    and propose a multi-agent actor-critic method. To stabilize experience replay
    and solve nonstationarity, they use fingerprints and importance sampling, which
    can improve the final performance. Shao et al. [[94](#bib.bib94)] follow decentralized
    micromanagement task, and propose parameter sharing multi-agent gradient descent
    SARSA($\lambda$) (PS-MAGDS) method. To resue the knowledge between various micromanagement
    scenarios, they also combine curriculum transfer learning to this method. This
    improves the sample efficiency, and outperforms GMEZO and BiCNet in large-scale
    scenarios. Kong et al. [[95](#bib.bib95)] bases on master-slave architecture,
    and proposes master-slave multi-agent reinforcement learning (MS-MARL). MS-MARL
    includes composed action representation, independent reasoning, and learnable
    communication. This method has better performance than other methods in micromanagement
    tasks. Rashid et al. [[96](#bib.bib96)] focus on several challenging StarCraft
    II micromanagement tasks, and use centralized training and decentralized execution
    to learn cooperative behaviors. This eventually outperforms state-of-the-art multi-agent
    deep reinforcement learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers also use DRL methods to optimize the build order in StarCraft. Tang
    et al. [[97](#bib.bib97)] put forward neural network fitted Q-learning (NNFQ)
    and convolutional neural network fitted Q-learning (CNNFQ) to build units in simple
    StarCraft maps. These models are able to find effective production sequences,
    and eventually defeat enemies. In [[68](#bib.bib68)], researchers present baseline
    results of several main DRL agents in the StarCraft II domain. The fully convolutional
    advantage actor-critic (FullyConv-A2C) agents achieve a beginner-level in StarCraft
    II mini-games. Zambaldi et al. [[98](#bib.bib98)] introduce the relational DRL
    to StarCraft, which iteratively reasons about the relations between entities with
    self-attention, and uses it to guide a model-free RL policy. This method improves
    sample efficiency, generalization ability, and interpretability of conventional
    DRL approaches. Relational DRL agent achieves impressive performance on SC2LE
    mini-games. Sun et al. [[99](#bib.bib99)] develop the DRL based agent TStarBot,
    which uses flat action structure. This agent defeats the built-in AI agents from
    level 1 to level 10 in a full game firstly. Lee et al. [[100](#bib.bib100)] focus
    on StarCraft II AI, and present a novel modular architecture, which splits responsibilities
    between multiple modules. Each module controls one aspect of the game, and two
    modules are trained with self-play DRL methods. This method defeats the built-in
    bot in ”Harder” level. Pang et al. [[101](#bib.bib101)] investigate a two-level
    hierarchical RL approach for StarCraft II. The macro-action is automatically extracted
    from expert’s data, and the other is a flexible and scaleable hierarchical architecture.
    More recently, DeepMind proposes AlphaStar, and defeats professional players for
    the first time.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D2 MOBA and Dota2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MOBA (Multiplayer Online Battle Arena) is originated from RTS games, which has
    two teams, and each team consists of five players. To beat the opponent, five
    players in a team must cooperate together, kill enemies, upgrade heros, and eventually
    destroy the opponent base. Since MOBA research is still in a primary stage, there
    are fewer works than conventional RTS games. Most works on MOBA concentrate on
    dataset analysis and case study. However, due to a series of breakthroughs that
    DRL achieves in game AI, researchers start to pay more attention to MOBA recently.
    King of Glory (a simplified mobile version of Dota) is the most popular mobile-end
    MOBA game in China. Jiang et al. [[102](#bib.bib102)] apply Monte-Carlo Tree Search
    and deep neural networks to this game. The experimental results indicate that
    MCTS-based DRL method is efficient and can be used in 1v1 MOBA scenario. Most
    impressive works on MOBA are proposed by OpenAI. Their results prove that DRL
    method with self-play can not only be successful in a 1v1 and 2v2 Dota2 scenarios
    [[103](#bib.bib103)], but also in 5v5 [[104](#bib.bib104)][[105](#bib.bib105)].
    The model architecture is simple, using a LSTM layer as the core component of
    neural network. Under the support of massively distributed cloud computing and
    PPO optimization algorithm, OpenAI Five can master the critical abilities of team
    fighting, searching forest, focusing, chasing, and diversion for team victory,
    and defeat human champion OG with 2:0\. Their works truly open a new door to MOBA
    research with DRL method.
  prefs: []
  type: TYPE_NORMAL
- en: V Challenges in Games with DRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since DRL has achieved large progress in some video games, it is considered
    as one of most promising ways to realize the artificial general intelligence.
    However, there are still some challenges should be conquered towards goal. In
    this secition, we discuss some crucial challenges for DRL in video games, such
    as tradeoff between exploration and exploitation, low sample efficiency, dilemma
    in generalization and overfiting, multi-agent learning, incomplete information
    and delayed sparse rewards. Though there are some proposed approaches have been
    tried to solve these problems, as presented in Fig. 4, there are still some limitations
    should be broken.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Exploration-exploitation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exploration can help to obtain more diversity samples, while exploitation is
    the way to learn the high reward policy with valuable samples. The trade-off between
    exploration and exploitation remains a major challenge for RL. Common methods
    for exploration require a large amount of data, and can not tackle temporally-extended
    exploration. Most model-free RL algorithms are not computationally tractable in
    complicated environments.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric noise can help exploration to a large extend in the training process
    [[106](#bib.bib106)] [[107](#bib.bib107)]. Besides, randomized value functions
    become an effective approach for efficient exploration. Combining exploration
    with deep neural networks can help to learn much faster, which greatly improves
    the learning speed and final performance in most games [[51](#bib.bib51)].
  prefs: []
  type: TYPE_NORMAL
- en: A simple generalization of popular count-based approach can reach satisfactory
    performance on various high-dimensional DRL benchmarks [[108](#bib.bib108)]. This
    method maps states to hash codes, and counts their occurrences via a hash table.
    Then, according to the classic count-based method, we can use these counts to
    compute a reward bonus. On many challenging tasks, these simple hash functions
    can achieve impressive performance. This exploration strategy provides a simple
    and powerful baseline to solve MDPs requiring considerable exploration.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Sample efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DRL algorithms usually take millions of samples to achieve human-level performance.
    While humans can quickly master highly rewarding actions of an environment. Most
    model-free DRL algorithms are data inefficient, especially for a environment with
    high dimension and large explore space. They have to interact with environment
    in a large time cost for seek out high reward experiences in a complex sample
    space, which limits their applicability to many scenarios. In order to reduce
    the exploration dimension of environment and ease the expenditure of time on interaction,
    some solutions can be used for improving data efficiency, such as hierarchy and
    demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical reinforcement learning (HRL) allows agents to decompose the task
    into several simple subtasks, which can speed up training and improve sample efficiency.
    Temporal abstraction is key to scaling up learning, while creating such abstractions
    autonomously has remained challenging. The option-critic architecture has the
    ability to learn the internal policies and the options’ termination conditions,
    without any additional rewards or subgoals [[109](#bib.bib109)]. FeUdal Networks
    (FuNs) include a Manager module and a Worker module [[110](#bib.bib110)]. The
    Manager sets abstract goals at high-level. The Worker receives these goals, and
    generates actions in the environment. FuN dramatically outperforms baseline agents
    on tasks that involve long-term credit assignment or memorization. Representation
    learning methods can also be used to guide the option discovery process in HRL
    domain [[111](#bib.bib111)].
  prefs: []
  type: TYPE_NORMAL
- en: Demonstration is a proper technique to improve sample efficiency. Current approaches
    that learn from demonstration use supervised learning on expert data and use reinforcement
    learning to improve the performance. This method is difficult to jointly optimize
    divergent losses, and is very sensitive to noisy demonstrations. Leveraging data
    from previous control of the system can greatly accelerate the learning process
    even with small amounts of demonstration data [[24](#bib.bib24)]. Goals defined
    with human preferences can effectively solve complicated RL tasks without the
    reward function, while greatly reducing the cost of human oversight [[112](#bib.bib112)].
  prefs: []
  type: TYPE_NORMAL
- en: V-C Generalization and Transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ability to transfer knowledge across multiple environments is considered
    as a critical aspect of intelligent agents. With the purpose of promoting the
    performance of generalization in multiple environments, multi-task learning and
    policy distillation have been focus on these situations.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task learning with shared neural network parameters can solve the generalization
    problem, and efficiency can be improved through transfer across related tasks.
    Hybrid reward architecture takes a decomposed reward function as input and learns
    a separate value function for each component [[113](#bib.bib113)]. The whole value
    function is much smoother, which can be easily approximated with a low-dimensional
    representation, and learns more effectively. IMPALA shows the effectiveness for
    multi-task reinforcement learning, using less data and exhibiting positive transfer
    between tasks [[40](#bib.bib40)] . PopArt-IMPALA combines PopArt’s adaptive normalization
    with IMPALA, and allows a more efficient use of parallel data generation, showing
    impressive performance on multi-task domain [[114](#bib.bib114)].
  prefs: []
  type: TYPE_NORMAL
- en: To successfully learn complex tasks with DRL, we usually need large task-specific
    networks and extensive training to achieve good performance. Distral shares a
    distilled policy which can learn common knowledge across multiple tasks [[115](#bib.bib115)].
    Each worker is trained to solve individual task and to be close to the shared
    policy, while the shared policy is trained by distillation. This approach shows
    efficient transfer on complex tasks, with more robust and more stable performance.
    Mix & Match is a training framework that is designed to encourage effective and
    rapid learning in DRL agents [[116](#bib.bib116)]. It allows to automatically
    form a curriculum over agent, and progressively trains more complex agents from
    simpler agents.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Multi-agent learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-agent learning is very important in video games, such as StarCraft. In
    a cooperative multi-agent setting, curse-of-dimensionality, communication, and
    credit assignment are major challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Team learning uses a single learner to learn joint solutions in multi-agent
    system, while concurrent learning uses multiple learners for each agent. Recently,
    the centralised training of decentralised policies is becoming a standard paradigm
    for multi-agent training. Multi-agent DDPG considers other agents’ action policy
    and can successfully learn complex multi-agent coordination behavior [[117](#bib.bib117)].
    Counterfactual multi-agent policy gradients uses a centralized critic to estimate
    the action-value function and decentralized actors to optimize each agents’ policies,
    with a counterfactual advantage function to address the multi-agent credit assignment
    problem [[118](#bib.bib118)] . In addition, communication protocols is important
    to share information to solve multi-agent tasks. Reinforced Inter-Agent Learning
    (RIAL) and Differentiable Inter-Agent Learning (DIAL) use deep reinforcement learning
    to learn end-to-end communication protocols in complex environments. Analogously,
    CommNet is able to learn continuous communication between multiple agents.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Imperfect information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In partially observable and first-perspective games, DRL agents need to tackle
    imperfect information to learn a suitable policy. Making decisions in these environments
    is challenging for DRL agents.
  prefs: []
  type: TYPE_NORMAL
- en: A critical component of enabling effective learning in these environment is
    the use of memory. DRL agents have used some simple memory architectures, such
    as several past frames or an LSTM layer. But these architectures are limited to
    only remember transitory information. Model-free episode control learns difficult
    sequential decision-making tasks much faster, and achieves a higher overall reward
    [[119](#bib.bib119)]. Differentiable neural computer uses a neural network to
    read from and write to an external memory matrix [[120](#bib.bib120)]. This method
    can solve complex, structured tasks which can not access to neural networks without
    external read and write memory. Neural episodic control inserts recent state representations
    paired with corresponding value functions into the appropriate neural dictionary,
    and learns significantly faster than other baseline agents [[121](#bib.bib121)].
  prefs: []
  type: TYPE_NORMAL
- en: V-F Delayed spare rewards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sparse and delayed reward is very common in many games, and is also one
    of the reasons that reduce sample efficiency in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: In many scenarios, researchers use curiosity as an intrinsic reward to encourage
    agents to explore environment and learn useful skills. Curiosity can be formulated
    as the error that the agent predicts its own actions’ consequence in a visual
    space [[122](#bib.bib122)]. This can scale to high-dimensional continuous state
    spaces. Moreover, it leaves out the aspects of environment that cannot affect
    agents. Curiosity search for DRL encourages intra-life exploration by rewarding
    agents for visiting as many different states as possible within each episode [[123](#bib.bib123)].
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion and discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Game AI with deep reinforcement learning is a challenging and promising direction.
    Recent progress in this domain has promote the development of artificial intelligence
    research. In this paper, we review the achievements of deep reinforcement learning
    in video games. Different DRL methods and their successful applications are introduced.
    These DRL agents achieve human-level or super-human performances in various games,
    from 2D perfect information to 3D imperfect information, and from single-agent
    to multi-agent. In addition to these achievements, there are still some major
    problems when applying DRL methods to this field, especially in 3D imperfect information
    multi-agent video game. A high-level game AI requires to explore more efficient
    and robust DRL techniques, and needs novel frameworks to be implemented in complex
    environment. These challenges have not been fully investigated and could be opened
    for further study in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors would like to thank Qichao Zhang, Dong Li and Weifan Li for the
    helpful comments and discussions about this work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] N. Y. Georgios and T. Julian, *Artificial Intelligence and Games*.   New
    York: Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Zhao, K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and
    C. Wang, “Review of deep reinforcement learning and discussions on the development
    of computer Go,” *Control Theory and Applications*, vol. 33, no. 6, pp. 701–717,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep reinforcement
    learning: from AlphaGo to AlphaGo Zero,” *Control Theory and Applications*, vol. 34,
    no. 12, pp. 1529–1546, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Niels, B. Philip, T. Julian, and R. Sebastian, “Deep learning for video
    game playing,” *CoRR*, vol. abs/1708.07902, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, “Deep reinforcement learning:
    A brief survey,” *IEEE Signal Processing Magazine*, vol. 34, pp. 26–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] L. Yuxi, “Deep reinforcement learning: An overview,” *CoRR*, vol. abs/1701.07274,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] R. Dechter, “Learning while searching in constraint-satisfaction-problems,”
    pp. 178–183, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] J. Schmidhuber, “Deep learning in neural networks,” *Neural Networks*,
    vol. 61, pp. 85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *International Conference on Neural
    Information Processing Systems*, 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*.   MIT
    Press, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. W. Ronald, “Simple statistical gradient-following algorithms for connectionist
    reinforcement learning,” *Machine Learning*, vol. 8, pp. 229–256, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski, “Human-level control
    through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p. 529, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] v. H. Hado, G. Arthur, and S. David, “Deep reinforcement learning with
    double Q-learning,” in *AAAI Conference on Artificial Intelligence*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. Tom, Q. John, A. Ioannis, and S. David, “Prioritized experience replay,”
    in *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] W. Ziyu, S. Tom, H. Matteo, v. H. Hado, L. Marc, and d. F. Nando, “Dueling
    network architectures for deep reinforcement learning,” in *International Conference
    on Machine Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] P. v. H. Hado, G. Arthur, H. Matteo, M. Volodymyr, and S. David, “Learning
    values across many orders of magnitude,” in *Advances in Neural Information Processing
    Systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. H. Frank, L. Yang, G. S. Alexander, and P. Jian, “Learning to play
    in a day: faster deep reinforcement learning by optimality tightening,” in *International
    Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] “Massively parallel methods for deep reinforcement learning,” in *International
    Conference on Machine Learning Workshop on Deep Learning*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. H. Matthew and S. Peter, “Deep recurrent Q-learning for partially observable
    MDPs,” *CoRR*, vol. abs/1507.06527, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] L. Nir, Z. Tom, J. M. Daniel, T. Aviv, and M. Shie, “Shallow updates for
    deep reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Oron, B. Nir, and S. Nahum, “Averaged-DQN: variance reduction and stabilization
    for deep reinforcement learning,” in *International Conference on Machine Learning*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] “Deep Q-learning from demonstrations,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] M. Sabatelli, G. Louppe, P. Geurts, and M. Wiering, “Deep quality-value
    (dqv) learning.” *abs/1810.00368*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] “Rainbow: combining improvements in deep reinforcement learning,” in *AAAI
    Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. A.-M. Jose, G. Michael, W. Michael, U. Thomas, and H. Sepp, “RUDDER:
    return decomposition for delayed rewards,” *CoRR*, vol. abs/1806.07857, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] “Observe and look further: achieving consistent performance on Atari,”
    *CoRR*, vol. abs/1805.11593, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. John, A. Pieter, and C. Xi, “Equivalence between policy gradients and
    soft Q-learning,” *CoRR*, vol. abs/1704.06440, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] G. B. Marc, D. Will, and M. Remi, “A distributional perspective on reinforcement
    learning,” in *International Conference on Machine Learning*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Will, R. Mark, G. B. Marc, and M. Rémi, “Distributional reinforcement
    learning with quantile regression,” in *AAAI Conference on Artificial Intelligence*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Will, O. Georg, S. David, and M. Rémi, “Implicit quantile networks
    for distributional reinforcement learning,” *CoRR*, vol. abs/1806.06923, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] “Asynchronous methods for deep reinforcement learning,” in *International
    Conference on Machine Learnin*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] B. Mohammad, F. Iuri, T. Stephen, C. Jason, and K. Jan, “Reinforcement
    learning through asynchronous advantage actor-critic on a GPU,” in *International
    Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Max, M. Volodymyr, C. Wojciech, S. Tom, Z. L. Joel, S. David, and K. Koray,
    “Reinforcement learning with unsupervised auxiliary tasks,” in *International
    Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] “Efficient parallel methods for deep reinforcement learning,” *CoRR*,
    vol. abs/1705.04862, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] O. Brendan, M. Rémi, K. Koray, and M. Volodymyr, “PGQ: combining policy
    gradient and Q-learning,” in *International Conference on Learning Representations*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. Rémi, S. Tom, H. Anna, and G. B. Marc, “Safe and efficient off-policy
    reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] G. Audrunas, G. A. Mohammad, G. B. Marc, and R. Munos, “The Reactor: a
    sample-efficient actor-critic architecture,” *CoRR*, vol. abs/1704.04651, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] “IMPALA: scalable distributed deep-RL with importance weighted actor-learner
    architectures,” *CoRR*, vol. abs/1802.01561, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. John, L. Sergey, A. Pieter, I. J. Michael, and M. Philipp, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. John, W. Filip, D. Prafulla, R. Alec, and K. Oleg, “Proximal policy
    optimization algorithms,” *CoRR*, vol. abs/1707.06347, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Ziyu, B. Victor, H. Nicolas, M. Volodymyr, M. Rémi, K. Koray, and d. F.
    Nando, “Sample efficient actor-critic with experience replay,” in *International
    Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] W. Yuhuai, M. Elman, L. Shun, B. G. Roger, and B. Jimmy, “Scalable trust-region
    method for deep reinforcement learning using Kronecker-factored approximation,”
    in *Advances in Neural Information Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] P. L. Timothy, J. H. Jonathan, P. Alexander, H. Nicolas, E. Tom, T. Yuval,
    S. David, and W. Daan, “Continuous control with deep reinforcement learning,”
    *CoRR*, vol. abs/1509.02971, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] “Distributed distributional deterministic policy gradients,” *CoRR*, vol.
    abs/1804.08617, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] F. Gregory, R. Tim, I. Maximilian, and W. Shimon, “TreeQN and ATreeC:
    differentiable tree planning for deep reinforcement learning,” in *International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] V. Alexander, M. Volodymyr, O. Simon, G. Alex, V. Oriol, A. John, and
    K. Koray, “Strategic attentive writer for learning macro-actions,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy evolution,”
    *Neural Information Processing Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Nantas, S. Gabriel, L. Zeming, K. Pushmeet, H. S. T. Philip, and U. Nicolas,
    “Value propagation networks,” *CoRR*, vol. abs/1805.11199, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] O. Ian, B. Charles, P. Alexander, and V. R. Benjamin, “Deep exploration
    via bootstrapped DQN,” in *Advances in Neural Information Processing Systems*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *international
    conference on machine learning*, pp. 1856–1865, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Sahil, S. L. Aravind, and R. Balaraman, “Learning to repeat: fine grained
    action repetition for deep reinforcement learning,” in *International Conference
    on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Julian, A. Ioannis, and H. Thomas, “Mastering atari, go, chess and
    shogi by planning with a learned model,” *abs/1911.08265*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] G. B. Marc, N. Yavar, V. Joel, and H. B. Michael, “The Arcade learning
    environment: An evaluation platform for general agents,” *J. Artif. Intell. Res.*,
    vol. 47, pp. 253–279, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] B. Greg, C. Vicki, P. Ludwig, S. Jonas, S. John, T. Jie, and Z. Wojciech,
    “OpenAI Gym,” *CoRR*, vol. abs/1606.01540, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] “OpenAI Universe github,” https://github.com/openai/universe, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] “OpenAI Retro github,” https://github.com/openai/retro, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Alex, P. Vicki, H. Christopher, K. Oleg, and S. John, “Gotta learn
    fast: a new benchmark for generalization in RL,” *CoRR*, vol. abs/1804.03720,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] “General video game AI: a multi-track framework for evaluating agents,
    games and content generation algorithms,” *CoRR*, vol. abs/1802.10363, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] R. T. Ruben, B. Philip, T. Julian, L. Jialin, and P.-L. Diego, “Deep reinforcement
    learning for general video game AI,” *CoRR*, vol. abs/1806.02448, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] “DeepMind Lab,” *CoRR*, vol. abs/1612.03801, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] J. Arthur, B. Vincent-Pierre, V. Esh, G. Yuan, H. Hunter, M. Marwan, and
    L. Danny, “Unity: a general platform for intelligent agentst,” *CoRR*, vol. abs/1809.02627,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] “OpenAI Malmo github,” https://github.com/Microsoft/malmo, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner,
    “Torcs, the open racing car simulator,” *Software available at http://torcs. sourceforge.
    net*, vol. 4, p. 6, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jakowski, “ViZDoom:
    a Doom-based AI research platform for visual reinforcement learning,” in *IEEE
    Conference on Computational Intelligence and Games*, 2017, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. Gabriel, N. Nantas, A. Alex, C. Soumith, L. Timothée, L. Zeming, R. Florian,
    and U. Nicolas, “TorchCraft: a library for machine learning research on real-time
    strategy games,” *CoRR*, vol. abs/1611.00625, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] “StarCraft II: a new challenge for reinforcement learning,” *CoRR*, vol.
    abs/1708.04782, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] C. Karl, K. Oleg, H. Chris, K. Taehoon, and S. John, “Quantifying generalization
    in reinforcement learning,” *CoRR*, vol. abs/1812.02341, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. M. Marlos, G. B. Marc, T. Erik, V. Joel, J. H. Matthew, and B. Michael,
    “Revisiting the Arcade learning environment: evaluation protocols and open problems
    for general agents,” *Journal of Artificial Intelligence Research*, vol. 61, pp.
    523–562, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] H. Dan, Q. John, B. David, B.-M. Gabriel, H. Matteo, v. H. Hado, and S. David,
    “Distributed prioritized experience replay,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] O. Georg, G. B. Marc, O. Aaron, and M. Remi, “Count-based exploration
    with neural density models,” in *International Conference on Machine Learning*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. L. Hoang, J. Nan, A. Alekh, D. Miroslav, Y. Yisong, and D. Hal, “Hierarchical
    imitation and reinforcement learning,” in *International Conference on Machine
    Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y. Aytar, T. Pfaff, D. Budden, T. L. Paine, Z. Wang, and N. D. Freitas,
    “Playing hard exploration games by watching youtube,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] “Montezuma’s revenge solved by go-explore, a new algorithm for hard-exploration
    problems,” https://eng.uber.com/go-explore/, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Wu and Y. Tian, “Training agent for first-person shooter game with
    actor-critic curriculum learning,” in *International Conference on Learning Representations*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] P. Emilio and S. Ruslan, “Neural map: structured memory for deep reinforcement
    learning,” *CoRR*, vol. abs/1702.08360, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] K. Shao, D. Zhao, N. Li, and Y. Zhu, “Learning battles in ViZDoom via
    deep reinforcement learning,” in *IEEE Conference on Computational Intelligence
    and Games*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] G. Yang, X. Huazhe, L. Ji, Y. Fisher, L. Sergey, and D. Trevor, “Reinforcement
    learning from imperfect demonstrations,” *CoRR*, vol. abs/1802.05313, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] M. Sahisnu, L. Bing, W. Shuai, Z. Yingxuan, L. Lifeng, and L. Jian, “Action
    permissibility in deep reinforcement learning and application to autonomous driving,”
    in *ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] D. Li, D. Zhao, Q. Zhang, and Y. Chen, “Reinforcement learning and deep
    learning based lateral control for autonomous driving,” *IEEE Computational Intelligence
    Magazine*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Zhu and D. Zhao, “Driving control with deep and reinforcement learning
    in the open racing car simulator,” in *International Conference on Neural Information
    Processing*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] J. Matthew, H. Katja, H. Tim, and B. David, “The Malmo platform for artificial
    intelligence experimentation,” in *International Joint Conferences on Artificial
    Intelligence*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] X. Yanhai, C. Haipeng, Z. Mengchen, and A. Bo, “HogRider: champion agent
    of Microsoft Malmo collaborative AI challenge,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Chen, G. Shahar, Z. Tom, J. M. Daniel, and M. Shie, “A deep hierarchical
    approach to lifelong learning in Minecraft,” in *AAAI Conference on Artificial
    Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. J. Peter, L. Sergey, and K. Kurt, “Regret minimization for partially
    observable deep reinforcement learning,” in *International Conference on Learning
    Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] “Unicorn: continual learning with a universal, off-policy agent,” *CoRR*,
    vol. abs/1802.08294, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] “Kickstarting deep reinforcement learning,” *CoRR*, vol. abs/1803.03835,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] “Human-level performance in first-person multiplayer games with population-based
    deep reinforcement learning,” *CoRR*, vol. abs/1807.01281, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Z. Tang, K. Shao, Y. Zhu, D. Li, D. Zhao, and T. Huang, “A review of computational
    intelligence for StarCraft AI,” in *IEEE Symposium Series on Computational Intelligence
    (SSCI)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] N. Usunier, G. Synnaeve, Z. Lin, and S. Chintala, “Episodic exploration
    for deep deterministic policies: an application to StarCraft micromanagement tasks,”
    in *International Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, and J. Wang, “Multiagent
    bidirectionally-coordinated nets for learning to play StarCraft combat games,”
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr, P. Kohli,
    and S. Whiteson, “Stabilising experience replay for deep multi-agent reinforcement
    learning,” in *International Conference on Machine Learning*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] K. Shao, Y. Zhu, and D. Zhao, “StarCraft micromanagement with reinforcement
    learning and curriculum transfer learning,” *IEEE Transactions on Emerging Topics
    in Computational Intelligence*, DOI:10.1109/TETCI.2018.2823329, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] K. Xiangyu, X. Bo, L. Fangchen, and W. Yizhou, “Revisiting the master-slave
    architecture in multi-agent deep reinforcement learning,” *CoRR*, vol. abs/1712.07305,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] R. Tabish, S. Mikayel, S. d. W. Christian, F. Gregory, N. F. Jakob, and
    W. Shimon, “QMIX: monotonic value function factorisation for deep multi-agent
    reinforcement learning,” *CoRR*, vol. abs/1803.11485, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] T. Zhentao, Z. Dongbin, Z. Yuanheng, and G. Ping, “Reinforcement learning
    for build-order production in StarCraft II,” in *International Conference on Information
    Science and Technology*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls,
    D. Reichert, T. Lillicrap, E. Lockhart *et al.*, “Relational deep reinforcement
    learning,” *CoRR*, vol. abs/806.01830, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] P. Sun, X. Sun, L. Han, J. Xiong, Q. Wang, B. Li, Y. Zheng, J. Liu, Y. Liu,
    H. Liu, and T. Zhang, “TStarBots: defeating the cheating level builtin AI in StarCraft
    II in the full game,” *CoRR*, vol. abs/1809.07193, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] L. Dennis, T. Haoran, O. Z. Jeffrey, X. Huazhe, D. Trevor, and A. Pieter,
    “Modular architecture for starcraft ii with deep reinforcement learning,” *CoRR*,
    p. abs/1811.03555, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] P. Zhen-Jia, L. Ruo-Ze, M. Zhou-Yu, Z. Yi, Y. Yang, and L. Tong, “On
    reinforcement learning for full-length game of starcraft,” *CoRR*, p. abs/1809.09095,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] R. J. Daniel, E. Emmanuel, and L. Hao, “Feedback-based tree search for
    reinforcement learning,” in *International Conference on Machine Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] “OpenAI Dota 1v1,” https://blog.openai.com/dota-2/, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] “OpenAI Dota Five,” https://blog.openai.com/openai-five/, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Christopher, B. Greg, and C. Brooke, “Dota 2 with large scale deep
    reinforcement learning,” *abs/1912.06680*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] “Noisy networks for exploration,” *CoRR*, vol. abs/1706.10295, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] “Parameter space noise for exploration,” *CoRR*, vol. abs/1706.01905,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] T. Haoran, H. Rein, F. Davis, S. Adam, C. Xi, D. Yan, S. John, D. T.
    Filip, and A. Pieter, “Exploration: a study of count-based exploration for deep
    reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] B. Pierre-Luc, H. Jean, and P. Doina, “The option-critic architecture,”
    in *AAAI Conference on Artificial Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. V. Alexander, O. Simon, S. Tom, H. Nicolas, J. Max, S. David, and
    K. Koray, “FeUdal networks for hierarchical reinforcement learning,” in *International
    Conference on Machine Learning*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. M. Marlos, R. Clemens, G. Xiaoxiao, L. Miao, T. Gerald, and C. Murray,
    “Eigenoption discovery through the deep successor representation,” *CoRR*, vol.
    abs/1710.11089, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] F. C. Paul, L. Jan, B. B. Tom, M. Miljan, L. Shane, and A. Dario, “Deep
    reinforcement learning from human preferences,” in *Advances in Neural Information
    Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] v. S. Harm, F. Mehdi, L. Romain, R. Joshua, B. Tavian, and T. Jeffrey,
    “Hybrid reward architecture for reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Matteo, S. Hubert, E. Lasse, C. Wojciech, S. Simon, and v. H. Hado,
    “Multi-task deep reinforcement learning with popart,” *CoRR*, vol. abs/1809.04474,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] “Distral: robust multitask reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] “Mix& Match - agent curricula for reinforcement learning,” *CoRR*, vol.
    abs/1806.01780, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] L. Ryan, W. Yi, T. Aviv, H. Jean, A. Pieter, and M. Igor, “Multi-agent
    actor-critic for mixed cooperative-competitive environments,” 2017, pp. 6382–6393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual
    multi-agent policy gradients,” in *AAAI Conference on Artificial Intelligence*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] B. Charles, U. Benigno, P. Alexander, L. Yazhe, R. Avraham, Z. L. Joel,
    W. R. Jack, W. Daan, and H. Demis, “Model-free episodic control,” *CoRR*, vol.
    abs/1606.04460, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] “Hybrid computing using a neural network with dynamic external memory,”
    *Nature*, vol. 538, pp. 471–476, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] “Neural episodic control,” in *International Conference on Machine Learning*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Deepak, A. Pulkit, A. E. Alexei, and D. Trevor, “Curiosity-driven
    exploration by self-supervised prediction,” in *IEEE Conference on Computer Vision
    and Pattern Recognition Workshops*, 2017, pp. 488–489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Christopher and C. Jeff, “Deep curiosity search: intra-life exploration
    improves performance on challenging deep reinforcement learning problems,” *CoRR*,
    vol. abs/1806.00553, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
