- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:03:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:03:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1912.10944] A Survey of Deep Reinforcement Learning in Video Games'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1912.10944] 深度强化学习在视频游戏中的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10944](https://ar5iv.labs.arxiv.org/html/1912.10944)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1912.10944](https://ar5iv.labs.arxiv.org/html/1912.10944)
- en: A Survey of Deep Reinforcement Learning in Video Games
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在视频游戏中的综述
- en: 'Kun Shao, Zhentao Tang, Yuanheng Zhu, , Nannan Li, and Dongbin Zhao K. Shao,
    Z. Tang, Y. Zhu, N. Li, and D. Zhao are with the State Key Laboratory of Management
    and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences.
    Beijing 100190, China. They are also with the University of Chinese Academy of
    Sciences, Beijing, China (e-mail: shaokun2014@ia.ac.cn; tangzhentao2016@ia.ac.cn;
    yuanheng.zhu@ia.ac.cn; linannan2017@ia.ac.cn, dongbin.zhao@ia.ac.cn).This work
    is supported by National Natural Science Foundation of China (NSFC) under Grants
    No.61573353, No.61603382, No.6180337, and No.61533017.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Kun Shao、Zhentao Tang、Yuanheng Zhu、Nannan Li 和 Dongbin Zhao K. Shao、Z. Tang、Y.
    Zhu、N. Li 和 D. Zhao 皆来自中国科学院自动化研究所复杂系统管理与控制国家重点实验室，北京 100190，中国。他们还与中国科学院大学，北京，中国相关（电子邮件：shaokun2014@ia.ac.cn;
    tangzhentao2016@ia.ac.cn; yuanheng.zhu@ia.ac.cn; linannan2017@ia.ac.cn, dongbin.zhao@ia.ac.cn）。本研究得到中国国家自然科学基金（NSFC）资助，资助编号为
    No.61573353、No.61603382、No.6180337 和 No.61533017。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning (DRL) has made great achievements since proposed.
    Generally, DRL agents receive high-dimensional inputs at each step, and make actions
    according to deep-neural-network-based policies. This learning mechanism updates
    the policy to maximize the return with an end-to-end method. In this paper, we
    survey the progress of DRL methods, including value-based, policy gradient, and
    model-based algorithms, and compare their main techniques and properties. Besides,
    DRL plays an important role in game artificial intelligence (AI). We also take
    a review of the achievements of DRL in various video games, including classical
    Arcade games, first-person perspective games and multi-agent real-time strategy
    games, from 2D to 3D, and from single-agent to multi-agent. A large number of
    video game AIs with DRL have achieved super-human performance, while there are
    still some challenges in this domain. Therefore, we also discuss some key points
    when applying DRL methods to this field, including exploration-exploitation, sample
    efficiency, generalization and transfer, multi-agent learning, imperfect information,
    and delayed spare rewards, as well as some research directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自提出以来，深度强化学习（DRL）取得了显著的成就。通常，DRL代理在每一步接收高维输入，并根据基于深度神经网络的策略采取行动。这种学习机制使用端到端的方法来更新策略，以最大化回报。本文综述了DRL方法的进展，包括基于价值的方法、策略梯度方法和基于模型的算法，并比较了它们的主要技术和特性。此外，DRL在游戏人工智能（AI）中发挥了重要作用。我们还回顾了DRL在各种视频游戏中的成就，包括经典的街机游戏、第一人称视角游戏和多智能体实时战略游戏，从2D到3D，从单智能体到多智能体。大量使用DRL的视频游戏AI已达到了超越人类的表现，但该领域仍面临一些挑战。因此，我们还讨论了将DRL方法应用于该领域的一些关键点，包括探索-开发、样本效率、泛化与迁移、多智能体学习、不完全信息和延迟稀疏奖励，以及一些研究方向。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: reinforcement learning, deep learning, deep reinforcement learning, game AI,
    video games.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习、深度学习、深度强化学习、游戏AI、视频游戏。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Artificial intelligence (AI) in video games is a long-standing research area.
    It studies how to use AI technologies to achieve human-level performance when
    playing games. More generally, it studies the complex interactions between agents
    and game environments. Various games provide interesting and complex problems
    for agents to solve, making video games perfect environments for AI research.
    These virtual environments are safe and controllable. In addition, these game
    environments provide infinite supply of useful data for machine learning algorithms,
    and they are much faster than real-time. These characteristics make games the
    unique and favorite domain for AI research. On the other side, AI has been helping
    games to become better in the way we play, understand and design them [[1](#bib.bib1)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 视频游戏中的人工智能（AI）是一个长期研究领域。它研究如何利用 AI 技术在玩游戏时达到人类水平的表现。更一般来说，它研究代理与游戏环境之间的复杂交互。各种游戏为代理提供了有趣且复杂的问题，使视频游戏成为
    AI 研究的理想环境。这些虚拟环境是安全且可控的。此外，这些游戏环境为机器学习算法提供了无限量的有用数据，而且它们的速度远快于实时。这些特性使游戏成为 AI
    研究独特且受欢迎的领域。另一方面，AI 也在帮助游戏改进我们玩、理解和设计游戏的方式 [[1](#bib.bib1)]。
- en: Broadly speaking, game AI involves the perception and the decision-making in
    game environments. With these components, there are some crucial challenges and
    proposed solutions. The first challenge is that the state space of the game is
    very large, especially in strategic games. With the rise of representation learning,
    the whole system has successfully modeled large-scale state space with deep neural
    networks. The second challenge is that learning proper policies to make decisions
    in dynamic unknown environment is difficult. For this problem, data-driven methods,
    such as supervised learning and reinforcement learning (RL), are feasible solutions.
    The third challenge is that the vast majority of game AI is developed in a specified
    virtual environment. How to transfer the AI’s ability among different games is
    a core challenge. A more general learning system is also necessary.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 广义来说，游戏 AI 涉及到游戏环境中的感知和决策。涉及这些组件时，有一些关键挑战和提出的解决方案。第一个挑战是游戏的状态空间非常大，尤其是在战略游戏中。随着表示学习的兴起，整个系统已经成功地使用深度神经网络建模大规模状态空间。第二个挑战是学习在动态未知环境中做出正确决策的策略是困难的。针对这个问题，数据驱动的方法，如监督学习和强化学习（RL），是可行的解决方案。第三个挑战是绝大多数游戏
    AI 都是在特定的虚拟环境中开发的。如何在不同的游戏之间转移 AI 的能力是一个核心挑战。一个更通用的学习系统也是必要的。
- en: '![Refer to caption](img/b44f2244796a155e7348c7d0c847bf84.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b44f2244796a155e7348c7d0c847bf84.png)'
- en: 'Figure 1: The framework diagram of the typical DRL for video games. The deep
    learning model takes input from video games API, and extract meaningful features
    automatically. DRL agents produces actions based on these features, and make the
    environments transfer to next state.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：典型 DRL 视频游戏框架图。深度学习模型从视频游戏 API 获取输入，并自动提取有意义的特征。DRL 代理基于这些特征产生动作，并使环境转移到下一个状态。
- en: For a long time, solving these challenges with reinforcement learning is widely
    used in game AI. And in the last few years, deep learning (DL) has achieved remarkable
    performance in computer vision and natural language processing [[2](#bib.bib2)].
    The combination, deep reinforcement learning (DRL), teaches agents to make decisions
    in high-dimensional state space in an end-to-end framework, and dramatically improves
    the generalization and scalability of traditional RL algorithms. Especially, DRL
    has made great progress in video games, including Atari, ViZDoom, StarCraft, Dota2,
    and so on. There are some related works to introduce these achievements in this
    field. Zhao et al. [[3](#bib.bib3)] and Tang et al. [[4](#bib.bib4)] survey the
    development of DRL research, and focus on AlphaGo and AlphaGo Zero. Justesen et
    al. [[5](#bib.bib5)] reviews DL-based methods in video game play, including supervised
    learning, unsupervised learning, reinforcement learning, evolutionary approaches,
    and some hybrid approaches. Arulkumaran et al. [[6](#bib.bib6)] make a brief introduction
    of DRL, covering central algorithms and presenting a range of visual RL domains.
    Li [[7](#bib.bib7)] gives an overview of recent achievements of DRL, and discusses
    core elements, important mechanisms, and various applications. In this paper,
    we focus on DRL-based game AI, from 2D to 3D, and from single-agent to multi-agent.
    The main contributions include the comprehensive and detailed comparisons of various
    DRL methods, their techniques, properties, and the impressive and diverse performances
    in these given video games.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，解决这些挑战的强化学习在游戏 AI 中被广泛应用。而近年来，深度学习（DL）在计算机视觉和自然语言处理方面取得了显著成果[[2](#bib.bib2)]。这种结合，深度强化学习（DRL），教会智能体在高维状态空间中进行决策，采用端到端的框架，并大幅提升了传统
    RL 算法的泛化能力和可扩展性。特别是，DRL 在视频游戏中取得了重大进展，包括 Atari、ViZDoom、StarCraft、Dota2 等等。有关这些成就的相关工作有：赵等人[[3](#bib.bib3)]和唐等人[[4](#bib.bib4)]回顾了
    DRL 研究的发展，重点关注 AlphaGo 和 AlphaGo Zero。Justesen 等人[[5](#bib.bib5)]回顾了视频游戏中基于 DL
    的方法，包括监督学习、无监督学习、强化学习、进化方法和一些混合方法。Arulkumaran 等人[[6](#bib.bib6)]简要介绍了 DRL，涵盖了核心算法，并展示了一系列视觉
    RL 领域。Li [[7](#bib.bib7)]概述了 DRL 的最新成果，并讨论了核心要素、重要机制和各种应用。本文重点关注基于 DRL 的游戏 AI，从
    2D 到 3D，从单一智能体到多智能体。主要贡献包括对各种 DRL 方法、技术、属性的全面而详细的比较，以及这些视频游戏中的令人印象深刻和多样化的表现。
- en: The organization of the remaining paper is arranged as follows. In Section II,
    we introduce the background of DL and RL. In Section III, we focus on recent DRL
    methods, including value-based, policy gradient, and model-based DRL methods.
    After that, we make a brief introduction of research platforms and competitions,
    and present performances of DRL methods in classical single-agent Arcade games,
    first-person perspective games, and multi-agent real-time strategy games. In Section
    V, we discuss some key points and research directions in this field. In the end,
    we draw a conclusion of this survey.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余论文的组织安排如下。在第 II 节中，我们介绍深度学习（DL）和强化学习（RL）的背景。在第 III 节中，我们重点介绍最近的深度强化学习（DRL）方法，包括基于价值的方法、策略梯度方法和基于模型的方法。之后，我们简要介绍研究平台和竞赛，并展示
    DRL 方法在经典单一智能体街机游戏、第一人称视角游戏以及多智能体实时战略游戏中的表现。在第 V 节中，我们讨论该领域的一些关键点和研究方向。最后，我们对本次调查做出结论。
- en: II Background
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: Generally speaking, training an agent to make decisions with high-dimensional
    inputs is difficult. With the development of deep learning, researchers take deep
    neural networks as function approximations, and use plenty of samples to optimize
    policies successfully. The framework diagram of typical DRL for video games is
    depicted in Fig. 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，训练一个能够处理高维输入的智能体是困难的。随着深度学习的发展，研究人员将深度神经网络作为函数近似，并利用大量样本成功优化策略。典型的视频游戏
    DRL 框架图如图 1 所示。
- en: II-A Deep learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度学习
- en: Deep learning comes from artificial neural networks, and is used to learn data
    representation. It is inspired by the theory of brain development, and can be
    learned in supervised learning, unsupervised learning and semi-supervised learning.
    Although the term deep learning is introduced in 1986 [[8](#bib.bib8)], deep learning
    has a winter time because of lacking data and incapable computation hardware.
    However, with more and more large-scale datasets being released, and capable hardware
    being available, a big revolution happens in DL [[9](#bib.bib9)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习来源于人工神经网络，用于学习数据表示。它的灵感来自于大脑发展的理论，并且可以在监督学习、无监督学习和半监督学习中进行。尽管“深度学习”这个术语在1986年被引入[[8](#bib.bib8)]，但由于缺乏数据和计算硬件的能力，深度学习曾经历过寒冬。然而，随着越来越多的大规模数据集的发布和强大硬件的出现，深度学习发生了重大革命[[9](#bib.bib9)]。
- en: Convolutional neural network (CNN) [[10](#bib.bib10)] is a class of deep neural
    networks, which is widely applied to computer vision. CNN is inspired by biological
    processes, and is shift invariant based on shared-weights architecture. Recurrent
    Neural Network (RNN) is another kind of deep nerial network, especially for natural
    language processing. As a special kind of RNN, Long Short Term Memory (LSTM) [[11](#bib.bib11)]
    is capable of learning long-term dependencies. Deep learning architectures have
    been applied into many fields, and have achieved significant successes, such as
    speech recognition, image classification and segmentation, semantic comprehension,
    and machine translation [[2](#bib.bib2)]. DL-based methods with efficient parallel
    distributed computing resources can break the limit of traditional machine learning
    methods. This method inspires scientists and researchers to achieve more and more
    state-of-the-art performance in respective fields.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）[[10](#bib.bib10)] 是一种深度神经网络，广泛应用于计算机视觉。CNN 的灵感来自于生物过程，并基于共享权重架构具有平移不变性。递归神经网络（RNN）是另一种深度神经网络，特别用于自然语言处理。作为RNN的一个特例，长短期记忆网络（LSTM）[[11](#bib.bib11)]
    能够学习长期依赖关系。深度学习架构已应用于许多领域，并取得了显著成功，例如语音识别、图像分类和分割、语义理解和机器翻译[[2](#bib.bib2)]。基于DL的方法，结合高效的并行分布式计算资源，可以突破传统机器学习方法的限制。这种方法激励科学家和研究人员在各自领域取得越来越多的最先进成果。
- en: '![Refer to caption](img/f67c17bcdb93a07a86471a4ba12f261d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f67c17bcdb93a07a86471a4ba12f261d.png)'
- en: 'Figure 2: The network architectures of typical DRL methods, with increased
    complexity and performance. (a): DQN network; (b)Dueling DQN network; (c): DRQN
    network; (d): Actor-critic network; (e): Reactor network.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：典型DRL方法的网络架构，随着复杂性和性能的增加。(a)：DQN网络；(b)：对抗DQN网络；(c)：DRQN网络；(d)：Actor-critic网络；(e)：Reactor网络。
- en: II-B Reinforcement learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 强化学习
- en: Reinforcement learning is a kind of machine learning methods where agents learn
    the optimal policy by trial and error [[12](#bib.bib12)]. By interacting with
    the environment, RL can be successfully applied to sequential decision-making
    tasks. Considering a discounted episodic Markov decision process (MDP) $(S,A,\gamma,P,r)$,
    the agent chooses an action $a_{t}$ according to the policy $\pi(a_{t}|s_{t})$
    at state $s_{t}$. The environment receives the action, produces a reward $r_{t+1}$
    and transfers to the next state $s_{t+1}$ according to the transition probability
    $P(s_{t+1}|s_{t},a_{t})$. This transition probability is unknown in RL domain.
    The process continues until the agent reaches a terminal state or a maximum time
    step. The objective is to maximize the expected discounted cumulative rewards
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习方法，其中代理通过试错学习最佳策略[[12](#bib.bib12)]。通过与环境交互，RL可以成功应用于顺序决策任务。考虑一个折扣的情节马尔可夫决策过程（MDP）$(S,A,\gamma,P,r)$，代理在状态
    $s_{t}$ 下根据策略 $\pi(a_{t}|s_{t})$ 选择一个动作 $a_{t}$。环境接受这个动作，产生奖励 $r_{t+1}$ 并根据转移概率
    $P(s_{t+1}|s_{t},a_{t})$ 转移到下一个状态 $s_{t+1}$。在RL领域，这个转移概率是未知的。该过程持续进行，直到代理达到终止状态或最大时间步。目标是最大化期望的折扣累积奖励：
- en: '|  | $\mathbb{E}_{\pi}[R_{t}]=\mathbb{E}_{\pi}[\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}],$
    |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}_{\pi}[R_{t}]=\mathbb{E}_{\pi}[\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}],$
    |  | (1) |'
- en: where $\gamma\in(0,1]$ is the discount factor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma\in(0,1]$ 是折扣因子。
- en: Reinforcement learning can be devided into off-policy and on-policy methods.
    Off-policy RL algorithms mean that the behavior policy used for selecting actions
    is different from the learning policy. On the contrary, behavior policy is the
    same with the learning policy in on-policy RL algorithms. Besides, reinforcement
    learning can also be devided into value-based and policy-based methods. In value-based
    RL, agents update the value function to learn suitable policy, while policy-based
    RL agents learn the policy directly.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以分为脱离策略和在策略方法。脱离策略 RL 算法意味着用于选择动作的行为策略与学习策略不同。相反，在策略 RL 算法中，行为策略与学习策略相同。此外，强化学习还可以分为基于值的方法和基于策略的方法。在基于值的
    RL 中，智能体更新值函数以学习适当的策略，而基于策略的 RL 智能体直接学习策略。
- en: Q-learning is a typical off-policy value-based method. The update rule of Q-learning
    is
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是一种典型的脱离策略的基于值的方法。Q-learning 的更新规则为
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\delta_{t}=r_{t+1}+\gamma\arg\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t}),$
    |  | (2a) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\delta_{t}=r_{t+1}+\gamma\arg\max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t}),$
    |  | (2a) |'
- en: '|  | $\displaystyle Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\delta_{t}.$
    |  | (2b) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\delta_{t}.$
    |  | (2b) |'
- en: $\delta_{t}$ is the temporal difference (TD) error, and $\alpha$ is the learning
    rate.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: $\delta_{t}$ 是时间差（TD）误差，$\alpha$ 是学习率。
- en: Policy gradient [[13](#bib.bib13)] parameterizes the policy and updates parameters
    $\theta$. In its general form, the objective function of policy gradient is defined
    as
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度 [[13](#bib.bib13)] 对策略进行参数化并更新参数 $\theta$。在其一般形式中，策略梯度的目标函数定义为
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\log\pi_{\theta}(a_{t}&#124;s_{t})R].$
    |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\log\pi_{\theta}(a_{t}&#124;s_{t})R].$
    |  | (3) |'
- en: $R$ is the total accumulated return.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $R$ 是总累计回报。
- en: Actor-critic [[12](#bib.bib12)] reinforcement learning improves the policy gradient
    with an value-based critic
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic [[12](#bib.bib12)] 强化学习通过基于值的评论家改进了策略梯度
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\Psi_{t}\log\pi_{\theta}(a_{t}&#124;s_{t})].$
    |  | (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\Psi_{t}\log\pi_{\theta}(a_{t}&#124;s_{t})].$
    |  | (4) |'
- en: $\Psi_{t}$ is the critic, which can be the state-action value function $Q^{\pi}(s_{t},a_{t})$,
    the advantage function $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$
    or the TD error $r_{t}+V^{\pi}(s_{t+1})-V^{\pi}(s_{t})$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $\Psi_{t}$ 是评论家，它可以是状态-动作值函数 $Q^{\pi}(s_{t},a_{t})$、优势函数 $A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})$
    或 TD 误差 $r_{t}+V^{\pi}(s_{t+1})-V^{\pi}(s_{t})$。
- en: III Deep Reinforcement Learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度强化学习
- en: DRL makes a combination of DL and RL, achieving rapid developments since proposed.
    This section will introduce various DRL methods, including value-based methods,
    policy gradient methods, and model-based methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 结合了 DL 和 RL，自提出以来取得了快速发展。本节将介绍各种 DRL 方法，包括基于值的方法、策略梯度方法和基于模型的方法。
- en: III-A Value-based DRL methods
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于值的 DRL 方法
- en: Deep Q-network (DQN) [[14](#bib.bib14)] is the most famous DRL model which learns
    policies directly from high-dimensional inputs. It receives raw pixels, and outputs
    a value function to estimate future rewards, as shown in Fig. 2(a). DQN uses the
    experience replay method to break the sample correlation, and stabilizes the learning
    process with a target Q-network. The loss function at iteration $i$ is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 网络（DQN） [[14](#bib.bib14)] 是最著名的 DRL 模型之一，它直接从高维输入中学习策略。它接收原始像素，并输出一个值函数来估计未来的奖励，如图
    2(a) 所示。DQN 使用经验重放方法来打破样本相关性，并通过目标 Q 网络稳定学习过程。第 $i$ 次迭代的损失函数为
- en: '|  | $\displaystyle\begin{aligned} L_{i}(\theta_{i})=E_{(s,a,r,s^{\prime})\sim
    U(D)}[(y_{i}^{DQN}-Q(s,a;{\theta_{i}}))^{2}],\end{aligned}$ |  | (5) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{aligned} L_{i}(\theta_{i})=E_{(s,a,r,s^{\prime})\sim
    U(D)}[(y_{i}^{DQN}-Q(s,a;{\theta_{i}}))^{2}],\end{aligned}$ |  | (5) |'
- en: with
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $y_{i}^{DQN}=r+\gamma\mathop{\max}\limits_{a^{\prime}}Q(s^{\prime},a^{\prime};{\theta_{i}{{}^{-}}}).$
    |  | (6) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{i}^{DQN}=r+\gamma\mathop{\max}\limits_{a^{\prime}}Q(s^{\prime},a^{\prime};{\theta_{i}{{}^{-}}}).$
    |  | (6) |'
- en: 'DQN bridges the gap between high-dimensional visual inputs and actions. After
    that, researchers have improved DQN in different aspects. Double DQN [[15](#bib.bib15)]
    introduces double Q-learning to reduce observed overestimations, and it leads
    to much better performance. Prioritized experience replay [[16](#bib.bib16)] helps
    prioritize experience to replay important transitions more frequently. The sample
    probability of transition $i$ as $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}p_{k}^{\alpha}}$,
    where $p_{i}$ is the priority of transition $i$. Dueling DQN [[17](#bib.bib17)]
    uses the dueling neural network architecture for model-free DRL. It includes two
    separate estimators: one for state value function $V(s;\theta,\beta)$ and the
    other for advantage function $A(s,a;\theta,\alpha)$, as shown in Fig. 2(b).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 架起了高维视觉输入与动作之间的桥梁。此后，研究人员在不同方面改进了 DQN。Double DQN [[15](#bib.bib15)] 引入了双重
    Q 学习以减少观察到的过度估计，并带来了更好的性能。优先经验回放 [[16](#bib.bib16)] 帮助优先考虑经验，以更频繁地回放重要的过渡。过渡 $i$
    的样本概率为 $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}p_{k}^{\alpha}}$，其中 $p_{i}$ 是过渡 $i$
    的优先级。Dueling DQN [[17](#bib.bib17)] 使用了用于无模型 DRL 的对战神经网络架构。它包括两个独立的估计器：一个用于状态值函数
    $V(s;\theta,\beta)$，另一个用于优势函数 $A(s,a;\theta,\alpha)$，如图 2(b) 所示。
- en: '|  | $Q(s,a:\theta,\alpha,\beta)=V(s;\theta,\beta)+A(s,a;\theta,\alpha).$ |  |
    (7) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s,a:\theta,\alpha,\beta)=V(s;\theta,\beta)+A(s,a;\theta,\alpha).$ |  |
    (7) |'
- en: 'Pop-Art [[18](#bib.bib18)] is proposed to adapt to different and non-stationary
    target magnitudes, which successfully replaces the clipping of rewards as done
    in DQN to handle various magnitudes of targets. Fast reward propagation [[19](#bib.bib19)]
    is a novel training algorithm for reinforcement learning, which combines the strength
    of DQN, and exploits longer state-transitions in experience replays by tightening
    the optimization via constraints. This novel technique makes DRL more practical
    by drastically reducing training time. Gorila [[20](#bib.bib20)] is the first
    massively distributed architecture for DRL. This architecture uses four main components:
    parallel actors; parallel learners; a distributed neural network to represent
    the value function or behavior policy; and a distributed store of experience.
    To address the limited memory and imperfect game information at each decision
    point, Deep Recurrent Q-Network (DRQN) [[21](#bib.bib21)] replaces the first fully-connected
    layer with a recurrent neural network in DQN, as shown in Fig. 2(c).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Pop-Art [[18](#bib.bib18)] 被提出以适应不同和非静态的目标幅度，这成功地替代了 DQN 中的奖励剪辑，以处理各种幅度的目标。快速奖励传播
    [[19](#bib.bib19)] 是一种新颖的强化学习训练算法，它结合了 DQN 的优势，并通过优化约束来利用经验回放中的较长状态过渡。这一新技术通过大幅度减少训练时间，使
    DRL 更具实用性。Gorila [[20](#bib.bib20)] 是第一个大规模分布式 DRL 架构。该架构使用四个主要组件：并行执行者；并行学习者；一个分布式神经网络来表示价值函数或行为策略；以及一个分布式经验存储。为了解决每个决策点的有限内存和不完美的游戏信息，Deep
    Recurrent Q-Network (DRQN) [[21](#bib.bib21)] 在 DQN 中用递归神经网络替代了第一个全连接层，如图 2(c)
    所示。
- en: Generally, DQN learns rich domain representations and approximates the value
    function with deep neural networks, while batch RL algorithms with linear representations
    are more stable and require less hyperparameter tuning. The Least Squares DQN
    (LS-DQN) [[22](#bib.bib22)] combines DQN’s rich feature representations with the
    stability of a linear least squares method. In order to reduce approximation error
    variance in DQN‘s target values, averaged-DQN [[23](#bib.bib23)] averages previous
    Q-values estimates, leading to a more stable training and improved performance.
    Deep Q-learning from Demonstrations (DQfD) [[24](#bib.bib24)] combines DQN with
    human demonstrations, which improves the sample efficiency greatly. DQV [[25](#bib.bib25)]
    uses TD learning to train a Value neural network, and uses this network to train
    a second Quality-value network to estimate state-action values. DQV learns significantly
    faster and better than double-DQN. Researchers have proposed several improvements
    to DQN. However, it is unclear which of these are complementary and how much can
    be combined. Rainbow [[26](#bib.bib26)] combines with main extensions to DQN,
    and gives each component’s contribution to overall performance. RUDDER [[27](#bib.bib27)]
    is a novel reinforcement learning approach for finite MDPs with delayed rewards,
    which is also a return decomposition method, RUDDER is exponentially faster on
    tasks with different lengths of reward delays. Ape-X DQfD[[28](#bib.bib28)] uses
    a new transformed Bellman operator to process rewards of varying densities and
    scales, and applies human demonstrations to ease the exploration problem to guide
    agents towards rewarding states. Additional, it proposes an auxiliary temporal
    consistency loss to train stably extending the effective planning horizon by an
    order of magnitude. Soft DQN [[29](#bib.bib29)] is an entropy-regularized versions
    of Q-learning, with better robustness and generalization .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，DQN 学习丰富的领域表示，并使用深度神经网络来逼近价值函数，而具有线性表示的批量 RL 算法则更稳定且需要更少的超参数调节。最小二乘 DQN（LS-DQN）[[22](#bib.bib22)]
    将 DQN 的丰富特征表示与线性最小二乘方法的稳定性相结合。为了减少 DQN 目标值中的逼近误差方差，平均-DQN [[23](#bib.bib23)] 平均先前的
    Q 值估计，从而实现了更稳定的训练和改进的性能。基于示范的深度 Q 学习（DQfD）[[24](#bib.bib24)] 将 DQN 与人工示范结合，极大地提高了样本效率。DQV
    [[25](#bib.bib25)] 使用 TD 学习来训练一个价值神经网络，并利用该网络来训练第二个质量-价值网络以估计状态-动作值。DQV 学习速度和效果明显优于双重
    DQN。研究人员提出了若干对 DQN 的改进。然而，这些改进中哪些是互补的以及可以结合多少尚不清楚。Rainbow [[26](#bib.bib26)] 结合了
    DQN 的主要扩展，并给出了每个组件对整体性能的贡献。RUDDER [[27](#bib.bib27)] 是一种针对有限 MDP 的延迟奖励的新型强化学习方法，也是一种回报分解方法，RUDDER
    在处理不同奖励延迟长度的任务时速度呈指数级提高。Ape-X DQfD [[28](#bib.bib28)] 使用了一种新的变换贝尔曼算子来处理不同密度和尺度的奖励，并应用人工示范来缓解探索问题，指导智能体趋向于奖励状态。此外，它提出了一种辅助时间一致性损失，以稳定训练，并将有效规划范围扩展了一个数量级。Soft
    DQN [[29](#bib.bib29)] 是一种熵正则化版本的 Q 学习，具有更好的鲁棒性和泛化能力。
- en: Distributional DRL learns the value distribution, in contrast to common RL that
    models the expectation of return, or value. C51 [[30](#bib.bib30)] focuses on
    the distribution of value, and designs distributional DQN algorithm to learn approximate
    value distributions. QR-DQN [[31](#bib.bib31)] methods close a number of gaps
    between theoretical and algorithmic results. Distributional reinforcement learning
    with Quantile regression in which the distribution over returns is modeled explicitly
    instead of only estimating the mean. Implicit Quantile Networks (IQN) [[32](#bib.bib32)]
    is a flexible, applicable, and state-of-the-art distributional DQN. IQN approximates
    the full Quantile function for the return distribution with Quantile regression,
    and provides a fully integrated distributional RL agent without prior assumptions
    on the parameterization of the return distribution. Furthermore, IQN allows to
    expand the class of control policies to a wide range of risk-sensitive policies
    connected to distortion risk measures.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式 DRL 学习价值分布，与常见的 RL 模型返回值或价值的期望值相对。C51 [[30](#bib.bib30)] 关注价值的分布，并设计了分布式
    DQN 算法来学习近似的价值分布。QR-DQN [[31](#bib.bib31)] 方法缩小了理论结果与算法结果之间的多个差距。分布式强化学习通过分位回归对返回分布进行显式建模，而不是仅估计均值。隐式分位网络（IQN）[[32](#bib.bib32)]
    是一种灵活、适用且最先进的分布式 DQN。IQN 通过分位回归近似返回分布的完整分位函数，并提供了一个完全集成的分布式 RL 智能体，无需对返回分布的参数化进行先验假设。此外，IQN
    允许将控制策略的类别扩展到与扭曲风险度量相关的广泛风险敏感策略。
- en: III-B Policy gradient DRL methods
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 策略梯度深度强化学习方法
- en: Policy gradient DRL optimizes the parameterized policy directly. Actor-critic
    architecture computes the policy gradient using a value-based critic function
    to estimate expected future reward, as shown in Fig. 2(d). Asynchronous DRL is
    an efficient framework for DRL that uses asynchronous gradient descent to optimize
    the policy[[33](#bib.bib33)]. Asynchronous advantage actor-critic (A3C) trains
    several agents on multiple environments, showing a stabilizing effect on training.
    The objective function of the actor is demonstrated as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度深度强化学习（DRL）直接优化参数化的策略。演员-评论家架构使用基于值的评论家函数来计算策略梯度，以估计预期的未来奖励，如图2(d)所示。异步深度强化学习是一个高效的DRL框架，它使用异步梯度下降来优化策略[[33](#bib.bib33)]。异步优势演员-评论家（A3C）在多个环境中训练多个代理，显示出对训练的稳定作用。演员的目标函数如下所示
- en: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}A_{\theta,\theta_{v}}(s_{t},a_{t})\log\pi_{\theta}(a_{t}&#124;s_{t})+\beta
    H_{\theta}(\pi(s_{t}))],$ |  | (8) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}A_{\theta,\theta_{v}}(s_{t},a_{t})\log\pi_{\theta}(a_{t}&#124;s_{t})+\beta
    H_{\theta}(\pi(s_{t}))],$ |  | (8) |'
- en: where $H_{\theta}(\pi(s_{t}))$ is an entropy term used to encourage exploration.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{\theta}(\pi(s_{t}))$ 是一个熵项，用于鼓励探索。
- en: GA3C [[34](#bib.bib34)] is a hybrid CPU/GPU version of A3C, which achieves a
    significant speed up compared to the original CPU implementation. UNsupervised
    REinforcement and Auxiliary Learning (UNREAL) [[35](#bib.bib35)] learns separate
    policies for maximizing many other pseudo-reward functions simultaneously, including
    value function replay, reward prediction, and pixel control. This agent drastically
    improves both data efficiency and robustness to hyperparameter settings. PAAC
    [[36](#bib.bib36)] is a novel framework for efficient parallelization of DRL,
    where multiple actors learn the policy on a single machine. Policy gradient methods
    are efficient techniques for policies improvement, while they are usually on-policy
    and unable to take advantage of off-policy data. The new method is referred as
    PGQ[[37](#bib.bib37)], which combines policy gradient with Q-learning. PGQ establishes
    an equivalency between regularized policy gradient techniques and advantage function
    learning algorithms. Retrace($\lambda$) [[38](#bib.bib38)] takes the best of the
    importance sampling, off-policy Q($\lambda$), and tree-backup($\lambda$), resulting
    in low variance, safety, and efficiency. It makes a combination of dueling DRQN
    architecture and actor-critic architecture, as shown in Fig. 2(e). Reactor [[39](#bib.bib39)]
    is a sample-efficient and numerical efficient reinforcement learning agent based
    on a multi-step return off-policy actor-critic architecture. The network outputs
    a target policy, an action-value Q-function, and an estimated behavioral policy.
    The critic is trained with the off-policy multi-step Retrace method and the actor
    is trained by a $\beta$-leave-one-out policy gradient. Importance-Weighted Actor
    Learner Architecture (IMPALA) [[40](#bib.bib40)] is a new distributed DRL, which
    can scale to thousands of machine. IMPALA uses a single reinforcement learning
    agent with a single set of parameters to solve a mass of tasks. This method achieves
    stable learning by combining decoupled acting and learning with a novel V-trace
    off-policy correction method, which is critical for achieving learning stability.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GA3C [[34](#bib.bib34)] 是A3C的混合CPU/GPU版本，相比于原始的CPU实现，它在速度上有显著提升。无监督强化与辅助学习（UNREAL）[[35](#bib.bib35)]
    同时为多个伪奖励函数（包括值函数回放、奖励预测和像素控制）学习不同的策略。该代理大幅提高了数据效率和对超参数设置的鲁棒性。PAAC [[36](#bib.bib36)]
    是一个新颖的高效并行化DRL框架，其中多个演员在单台机器上学习策略。策略梯度方法是改进策略的有效技术，但通常是基于策略的，无法利用离策略数据。新的方法称为PGQ[[37](#bib.bib37)]，它将策略梯度与Q学习结合起来。PGQ在正则化策略梯度技术和优势函数学习算法之间建立了等效性。Retrace($\lambda$)
    [[38](#bib.bib38)] 结合了重要性采样、离策略Q($\lambda$)和树备份($\lambda$)的优点，具有低方差、安全性和效率。它将对抗DRQN架构和演员-评论家架构结合起来，如图2(e)所示。Reactor
    [[39](#bib.bib39)] 是一个样本高效和数值高效的强化学习代理，基于多步返回离策略演员-评论家架构。网络输出一个目标策略、一个动作值Q函数和一个估计的行为策略。评论家通过离策略多步Retrace方法进行训练，演员则通过$\beta$-留一法策略梯度进行训练。重要性加权演员学习架构（IMPALA）[[40](#bib.bib40)]
    是一个新的分布式DRL，可以扩展到数千台机器。IMPALA使用单个强化学习代理和一组参数来解决大量任务。该方法通过将解耦的行动和学习与新颖的V-trace离策略校正方法结合起来，达到了稳定学习，这对于实现学习稳定性至关重要。
- en: III-B1 Trust region method
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 信任区域方法
- en: Trust Region Policy Optimization (TRPO) [[41](#bib.bib41)] is proposed for optimizing
    control policies, with guaranteed monotonic improvement. TRPO computes an ascent
    direction to improve on policy gradient, which can ensure a small change in the
    policy distribution. The constrained optimization problem of TRPO in each epoch
    is
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 信任区域策略优化（TRPO）[[41](#bib.bib41)] 被提出用于优化控制策略，并且保证单调改进。TRPO 计算一个上升方向来改善策略梯度，这可以确保策略分布的变化很小。TRPO
    在每个时期的约束优化问题是
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle maximize_{\theta}\ \ \ E_{s\sim\rho_{\theta^{\prime}},a\sim\pi_{\theta^{\prime}}}[\frac{\pi_{\theta}(a&#124;s)}{\pi_{\theta^{\prime}}(a&#124;s)}A_{\theta^{\prime}}(s,a)],$
    |  | (9a) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle maximize_{\theta}\ \ \ E_{s\sim\rho_{\theta^{\prime}},a\sim\pi_{\theta^{\prime}}}[\frac{\pi_{\theta}(a|s)}{\pi_{\theta^{\prime}}(a|s)}A_{\theta^{\prime}}(s,a)],$
    |  | (9a) |'
- en: '|  | $\displaystyle s.t.\ \ \ \ E_{s\sim\rho_{\theta^{\prime}}}[D_{KL}(\pi_{\theta^{\prime}}(\cdot&#124;s))]\leq\delta_{KL}.$
    |  | (9b) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.\ \ \ \ E_{s\sim\rho_{\theta^{\prime}}}[D_{KL}(\pi_{\theta^{\prime}}(\cdot|s))]\leq\delta_{KL}.$
    |  | (9b) |'
- en: This algorithm is effective for optimizing large nonlinear policies. Proximal
    policy optimization (PPO) [[42](#bib.bib42)] samples data by interaction with
    the environment, and optimizes the objective function with stochastic gradient
    ascent
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在优化大型非线性策略方面有效。近端策略优化（PPO）[[42](#bib.bib42)] 通过与环境的交互采样数据，并使用随机梯度上升来优化目标函数
- en: '|  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}&#124;s_{t})}{\pi_{\theta_{old}}(a_{t}&#124;s_{t})},$
    |  | (10a) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{t}(\theta)=\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})},$
    |  | (10a) |'
- en: '|  | $\displaystyle L(\theta)=\hat{\mathbb{E}}_{t}[min(r_{t}(\theta)\hat{A}_{t},clip(r_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t}].$
    |  | (10b) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(\theta)=\hat{\mathbb{E}}_{t}[min(r_{t}(\theta)\hat{A}_{t},clip(r_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t}].$
    |  | (10b) |'
- en: $r_{t}(\theta)$ denotes the probability ratio. This objective function clips
    the probability ratio to modify the surrogate objective. PPO has some benefits
    over TRPO, and is much simpler to implement, with better sample complexity. Actor-critic
    with experience replay (ACER) [[43](#bib.bib43)] introduces several innovations,
    including stochastic dueling network, truncated importance sampling, and a new
    trust region method, which is stable and sample efficient. Actor-critic using
    Kronecker-Factored Trust Region (ACKTR) [[44](#bib.bib44)] bases on natural policy
    gradient, and uses Kronecker-factored approximate curvature (K-FAC) with trust
    region to optimize the actor and the critic. ACKTR is sample efficient compared
    with other actor-critic methods.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: $r_{t}(\theta)$ 表示概率比率。这个目标函数裁剪概率比率以修改替代目标。PPO 相较于 TRPO 有一些好处，并且实现起来更简单，样本复杂度更好。具有经验重放的演员-评论家（ACER）[[43](#bib.bib43)]
    引入了几个创新，包括随机对抗网络、截断重要性采样和一种新的信任区域方法，该方法稳定且样本高效。使用 Kronecker 分解信任区域的演员-评论家（ACKTR）[[44](#bib.bib44)]
    基于自然策略梯度，并使用 Kronecker 分解的近似曲率（K-FAC）与信任区域来优化演员和评论家。与其他演员-评论家方法相比，ACKTR 更加样本高效。
- en: III-B2 Deterministic policy
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 确定性策略
- en: Apart from stochastic policy, deep deterministic policy gradient (DDPG) [[45](#bib.bib45)]
    is a kind of deterministic policy gradient method which adapts the success of
    DQN to continuous control. The update rule of DDPG is
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机策略，深度确定性策略梯度（DDPG）[[45](#bib.bib45)] 是一种确定性策略梯度方法，它将 DQN 的成功应用于连续控制。DDPG
    的更新规则是
- en: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\gamma Q(s_{t+1},\pi_{\theta}(s_{t+1})).$
    |  | (11) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(s_{t},a_{t})=r(s_{t},a_{t})+\gamma Q(s_{t+1},\pi_{\theta}(s_{t+1})).$
    |  | (11) |'
- en: DDPG is an actor-critic, off-policy algorithm, and is able to learn reasonable
    policies on various tasks. Distributed Distributional DDPG (D4PG) [[46](#bib.bib46)]
    is a distributional update to DDPG, combined with the use of multiple distributed
    workers all writing into the same replay table. This method has a much better
    performance on a number of difficult continuous control problems.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是一种演员-评论家型的离策略算法，能够在各种任务上学习到合理的策略。分布式分布式 DDPG（D4PG）[[46](#bib.bib46)] 是对
    DDPG 的分布式更新，结合了多个分布式工作者共同写入同一重放表的使用。这种方法在许多困难的连续控制问题上表现出更好的性能。
- en: III-B3 Entropy-regularized policy gradient
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 熵正则化策略梯度
- en: Soft Actor Critic (SAC) is an off-policy policy gradient method, which establishes
    a bridge between DDPG and stochastic policy optimization. SAC incorporates the
    clipped double-Q trick, and the objective function of maximum entropy DRL is
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 软演员评论家（SAC）是一种离策略策略梯度方法，它在 DDPG 和随机策略优化之间建立了桥梁。SAC 融入了裁剪的双 Q 技巧，最大熵 DRL 的目标函数是
- en: '|  | $J(\pi)=\sum_{t=0}^{T}\mathbb{E}_{(s_{t},a_{t})\sim\rho_{\pi}}[r(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))],$ |  | (12) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\pi)=\sum_{t=0}^{T}\mathbb{E}_{(s_{t},a_{t})\sim\rho_{\pi}}[r(s_{t},a_{t})+\alpha
    H(\pi(.&#124;s_{t}))],$ |  | (12) |'
- en: SAC uses an entropy regularization in its objective function. It trains the
    policy to maximize a trade-off between entropy and expected return. The entropy
    is a measure of randomness in the policy. This mechanism is similar to the trade-off
    between exploration and exploitation. Increasing entropy can encourage more exploration,
    and accelerate learning process. Moreover, it can also prevent the learning policy
    from converging to a poor local optimum.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 在其目标函数中使用熵正则化。它训练策略以最大化熵和期望回报之间的权衡。熵是政策中随机性的度量。这种机制类似于探索与利用之间的权衡。增加熵可以鼓励更多的探索，并加速学习过程。此外，它还可以防止学习策略收敛到较差的局部最优解。
- en: III-C Model-based DRL methods
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于模型的 DRL 方法
- en: Combining model-free reinforcement learning with on-line planning is a promising
    approach to solve the sample efficiency problem. TreeQN [[47](#bib.bib47)] is
    proposed to address these challenges. It is a differentiable, recursive, tree-structured
    model that serves as a drop-in replacement for any value function network in DRL
    with discrete actions. TreeQN dynamically constructs a tree by recursively applying
    a transition model in a learned abstract state space and then aggregating predicted
    rewards and state-values using a tree backup to estimate Q-values. ATreeC is an
    actor-critic variant that augments TreeQN with a softmax layer to form a stochastic
    policy network. Both approaches are trained end-to-end, such that the learned
    model is optimized for its actual use in the planner. TreeQN and ATreeC outperform
    n-step DQN and value prediction networks on multiple Atari games. Vezhnevets et
    al. [[48](#bib.bib48)] presents STRategic Attentive Writer (STRAW) neural network
    architecture to build implicit plans. STRAW purely interacts with an environment,
    and is an end-to-end method. STRAW model can learn temporally abstracted high-level
    macro-actions, which enables both economic computation and structured exploration.
    STRAW employs temporally extended planning strategies and achieves strong improvements
    on Atari games. The world model [[49](#bib.bib49)] uses an unsupervised manner
    to train a generative recurrent neural network, which can model RL environments
    through compressed spatiotemporal representations. It feeds extracted features
    into simple and compact policies, achieving impressive results in several environments.
    Value propagation (VProp) [[50](#bib.bib50)] bases on value iteration, and is
    an efficient differentiable planning module. It can successfully be trained to
    learn to plan using reinforcement learning. As a general framework of AlphaZero,
    MuZero[[54](#bib.bib54)] combines MCTS with a learned model, and predicts the
    reward, the action-selection policy, and the value function to make planning.
    It extends model-based RL to a range of logically complex and visually complex
    domains, and achieves superhuman performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 将无模型强化学习与在线规划结合起来是解决样本效率问题的一种有前景的方法。TreeQN [[47](#bib.bib47)] 被提出以应对这些挑战。它是一个可微分的递归树状模型，可替代
    DRL 中任何离散动作的价值函数网络。TreeQN 通过在学习到的抽象状态空间中递归应用转移模型来动态构建树，然后使用树备份来汇总预测的奖励和状态值，从而估计
    Q 值。ATreeC 是一个演员-评论家变体，通过添加一个 softmax 层来形成一个随机策略网络。两种方法都经过端到端训练，使得学习到的模型针对实际在规划中的使用进行优化。TreeQN
    和 ATreeC 在多个 Atari 游戏中表现优于 n 步 DQN 和价值预测网络。Vezhnevets 等人 [[48](#bib.bib48)] 提出了
    STRategic Attentive Writer (STRAW) 神经网络架构来构建隐式计划。STRAW 完全与环境进行交互，是一种端到端的方法。STRAW
    模型可以学习时间抽象的高层次宏动作，从而实现经济的计算和结构化探索。STRAW 采用时间扩展的规划策略，在 Atari 游戏中取得了显著的改进。世界模型 [[49](#bib.bib49)]
    使用无监督的方式训练生成递归神经网络，能够通过压缩的时空表示来建模 RL 环境。它将提取的特征输入到简单而紧凑的策略中，在多个环境中取得了令人印象深刻的结果。价值传播（VProp）
    [[50](#bib.bib50)] 基于价值迭代，是一个高效的可微分规划模块。它能够成功地训练以使用强化学习进行规划。作为 AlphaZero 的通用框架，MuZero[[54](#bib.bib54)]
    将 MCTS 与学习到的模型相结合，预测奖励、动作选择策略和价值函数以进行规划。它将基于模型的 RL 扩展到一系列逻辑复杂和视觉复杂的领域，并实现了超越人类的表现。
- en: A general review of various DRL methods from 2017 to 2019 is presented in Table
    I.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I 展示了2017年至2019年间各种 DRL 方法的一般综述。
- en: 'TABLE I: A general review of recent DRL methods from 2017 to 2018.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：2017年至2018年间最新 DRL 方法的一般综述。
- en: '| DRL Algorithms | Main Techniques | Networks | Category |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| DRL 算法 | 主要技术 | 网络 | 分类 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DQN [[14](#bib.bib14)] | experience replay, target Q-network | CNN | value-based,
    off-policy |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DQN [[14](#bib.bib14)] | 经验重放，目标Q网络 | CNN | 基于价值，离线学习 |'
- en: '| Double DQN [[15](#bib.bib15)] | double Q-learning | CNN | value-based, off-policy
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Double DQN [[15](#bib.bib15)] | 双重Q学习 | CNN | 基于价值，离线学习 |'
- en: '| Dueling DQN [[17](#bib.bib17)] | dueling neural network architecture | CNN
    | value-based, off-policy |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Dueling DQN [[17](#bib.bib17)] | 对抗神经网络架构 | CNN | 基于价值，离线学习 |'
- en: '| Prioritized DQN [[16](#bib.bib16)] | prioritized experience replay | CNN
    | value-based, off-policy |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Prioritized DQN [[16](#bib.bib16)] | 优先经验重放 | CNN | 基于价值，离线学习 |'
- en: '| Bootstrapped DQN [[51](#bib.bib51)] | combine deep exploration with DNNs
    | CNN | value-based, off-policy |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Bootstrapped DQN [[51](#bib.bib51)] | 将深度探索与DNN结合 | CNN | 基于价值，离线学习 |'
- en: '| Gorila [[20](#bib.bib20)] | massively distributed architecture | CNN | value-based,
    off-policy |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Gorila [[20](#bib.bib20)] | 大规模分布式架构 | CNN | 基于价值，离线学习 |'
- en: '| LS-DQN [[22](#bib.bib22)] | combine least-squares updates in DRL | CNN |
    value-based, off-policy |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LS-DQN [[22](#bib.bib22)] | 在DRL中结合最小二乘更新 | CNN | 基于价值，离线学习 |'
- en: '| Averaged-DQN [[23](#bib.bib23)] | averaging learned Q-values estimates |
    CNN | value-based, off-policy |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Averaged-DQN [[23](#bib.bib23)] | 平均化学习的Q值估计 | CNN | 基于价值，离线学习 |'
- en: '| DQfD [[24](#bib.bib24)] | learn from the demonstration data | CNN | value-based,
    off-policy |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| DQfD [[24](#bib.bib24)] | 从演示数据中学习 | CNN | 基于价值，离线学习 |'
- en: '| DQN with Pop-Art [[18](#bib.bib18)] | adaptive normalization with Pop-Art
    | CNN | value-based, off-policy |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| DQN with Pop-Art [[18](#bib.bib18)] | 带有Pop-Art的自适应归一化 | CNN | 基于价值，离线学习
    |'
- en: '| Soft DQN [[29](#bib.bib29)] | KL penalty and entropy bonus | CNN | value-based,
    off-policy |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Soft DQN [[29](#bib.bib29)] | KL惩罚和熵奖励 | CNN | 基于价值，离线学习 |'
- en: '| DQV [[25](#bib.bib25)] | training a Quality-value network | CNN | value-based,
    off-policy |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| DQV [[25](#bib.bib25)] | 训练质量值网络 | CNN | 基于价值，离线学习 |'
- en: '| Rainbow [[26](#bib.bib26)] | integrate six extensions to DQN | CNN | value-based,
    off-policy |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Rainbow [[26](#bib.bib26)] | 将六个扩展整合到DQN中 | CNN | 基于价值，离线学习 |'
- en: '| RUDDER [[27](#bib.bib27)] | return decomposition | CNN-LSTM | value-based,
    off-policy |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| RUDDER [[27](#bib.bib27)] | 回报分解 | CNN-LSTM | 基于价值，离线学习 |'
- en: '| Ape-X DQfD [[28](#bib.bib28)] | transformed Bellman operator, temporal consistency
    loss | CNN | value-based, off-policy |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Ape-X DQfD [[28](#bib.bib28)] | 变换的贝尔曼算子，时间一致性损失 | CNN | 基于价值，离线学习 |'
- en: '| C51 [[30](#bib.bib30)] | distributional Bellman optimality | CNN | value-based,
    off-policy |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| C51 [[30](#bib.bib30)] | 分布式贝尔曼最优性 | CNN | 基于价值，离线学习 |'
- en: '| QR-DQN [[31](#bib.bib31)] | distributional RL with Quantile regression |
    CNN | value-based, off-policy |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| QR-DQN [[31](#bib.bib31)] | 具有分位回归的分布式强化学习 | CNN | 基于价值，离线学习 |'
- en: '| IQN [[32](#bib.bib32)] | an implicit representation of the return distribution
    | CNN | value-based, off-policy |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| IQN [[32](#bib.bib32)] | 回报分布的隐式表示 | CNN | 基于价值，离线学习 |'
- en: '| A3C [[33](#bib.bib33)] | asynchronous gradient descent | CNN-LSTM | policy
    gradient, on-policy |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| A3C [[33](#bib.bib33)] | 异步梯度下降 | CNN-LSTM | 策略梯度，在线学习 |'
- en: '| GA3C [[34](#bib.bib34)] | hybrid CPU/GPU version | CNN-LSTM | policy gradient,
    on-policy |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GA3C [[34](#bib.bib34)] | 混合CPU/GPU版本 | CNN-LSTM | 策略梯度，在线学习 |'
- en: '| PPO [[42](#bib.bib42)] | clipped surrogate objective, adaptive KL penalty
    coefficient | CNN-LSTM | policy gradient, on-policy |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| PPO [[42](#bib.bib42)] | 剪切的代理目标，自适应KL惩罚系数 | CNN-LSTM | 策略梯度，在线学习 |'
- en: '| ACER [[43](#bib.bib43)] | experience replay, truncated importance sampling
    | CNN-LSTM | policy gradient, off-policy |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ACER [[43](#bib.bib43)] | 经验重放，截断的重要性采样 | CNN-LSTM | 策略梯度，离线学习 |'
- en: '| ACKTR [[44](#bib.bib44)] | K-FAC with trust region | CNN-LSTM | policy gradient,
    on-policy |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ACKTR [[44](#bib.bib44)] | 具有信任区域的K-FAC | CNN-LSTM | 策略梯度，在线学习 |'
- en: '| Soft Actor-Critic [[52](#bib.bib52)] | entropy regularization | CNN | policy
    gradient, off-policy |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Soft Actor-Critic [[52](#bib.bib52)] | 熵正则化 | CNN | 策略梯度，离线学习 |'
- en: '| UNREAL [[35](#bib.bib35)] | unsupervised auxiliary tasks | CNN-LSTM | policy
    gradient, on-policy |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| UNREAL [[35](#bib.bib35)] | 无监督辅助任务 | CNN-LSTM | 策略梯度，在线学习 |'
- en: '| Reactor [[39](#bib.bib39)] | Retrace($\lambda$), $\beta$-leave-one-out policy
    gradient estimate | CNN-LSTM | policy gradient, off-policy |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Reactor [[39](#bib.bib39)] | Retrace($\lambda$)，$\beta$-留一法策略梯度估计 | CNN-LSTM
    | 策略梯度，离线学习 |'
- en: '| PAAC [[36](#bib.bib36)] | parallel framework for A3C | CNN | policy gradient,
    on-policy |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| PAAC [[36](#bib.bib36)] | A3C的并行框架 | CNN | 策略梯度，在线学习 |'
- en: '| DDPG [[45](#bib.bib45)] | DQN with deterministic policy gradient | CNN-LSTM
    | policy gradient, off-policy |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DDPG [[45](#bib.bib45)] | 带有确定性策略梯度的DQN | CNN-LSTM | 策略梯度，离线学习 |'
- en: '| TRPO [[41](#bib.bib41)] | incorporate a KL divergence constraint | CNN-LSTM
    | policy gradient , on-policy |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| TRPO [[41](#bib.bib41)] | 结合 KL 散度约束 | CNN-LSTM | 策略梯度，策略 |'
- en: '| D4PG [[46](#bib.bib46)] | distributed distributional DDPG | CNN | policy
    gradient , on-policy |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| D4PG [[46](#bib.bib46)] | 分布式分布式 DDPG | CNN | 策略梯度，策略 |'
- en: '| PGQ [[37](#bib.bib37)] | combine policy gradient and Q-learning | CNN | policy
    gradient, off-policy |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| PGQ [[37](#bib.bib37)] | 结合策略梯度和 Q 学习 | CNN | 策略梯度，非策略 |'
- en: '| IMPALA [[40](#bib.bib40)] | importance-weighted actor learner architecture
    | CNN-LSTM | policy gradient, on-policy |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| IMPALA [[40](#bib.bib40)] | 重要性加权演员学习架构 | CNN-LSTM | 策略梯度，策略 |'
- en: '| FiGAR-A3C [[53](#bib.bib53)] | fine grained action repetition | CNN-LSTM
    | policy gradient, on-policy |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| FiGAR-A3C [[53](#bib.bib53)] | 细粒度动作重复 | CNN-LSTM | 策略梯度，策略 |'
- en: '| TreeQN/ATreeC [[47](#bib.bib47)] | on-line planning, tree-structured model
    | CNN | model-based, on-policy |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| TreeQN/ATreeC [[47](#bib.bib47)] | 在线规划，树状模型 | CNN | 基于模型，策略梯度 |'
- en: '| STRAW [[48](#bib.bib48)] | macro-actions, planning strategies | CNN | model-based,
    on-policy |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| STRAW [[48](#bib.bib48)] | 宏动作，规划策略 | CNN | 基于模型，策略梯度 |'
- en: '| World model [[49](#bib.bib49)] | mixture density network, variational autoencoder
    | CNN-LSTM | model-based, on-policy |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| World model [[49](#bib.bib49)] | 混合密度网络，变分自编码器 | CNN-LSTM | 基于模型，策略梯度 |'
- en: '| MuZero [[54](#bib.bib54)] | representation function, dynamics function, and
    prediction function | CNN | model-based, off-policy |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| MuZero [[54](#bib.bib54)] | 表示函数，动态函数和预测函数 | CNN | 基于模型，非策略 |'
- en: '![Refer to caption](img/6559cf0b8f00379e6085db5aecc91fbc.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6559cf0b8f00379e6085db5aecc91fbc.png)'
- en: 'Figure 3: The diagram of various video games AI, from 2D to 3D, and from single-agent
    to multi-agent.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：各种视频游戏 AI 的示意图，从 2D 到 3D，从单代理到多代理。
- en: IV DRL in video games
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 视频游戏中的 DRL
- en: Playing video games like human experts is challenging for computers. With the
    development of DRL, agents are able to play various games end-to-end. Here we
    focus on game research platforms and competitions, and impressive progress in
    various video games, from 2D to 3D, and from single-agent to multi-agent, as shown
    in Fig. 3.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对计算机而言，像人类专家一样玩视频游戏是具有挑战性的。随着 DRL 的发展，代理能够端到端地玩各种游戏。我们专注于游戏研究平台和竞赛，以及在各种视频游戏中的显著进展，从
    2D 到 3D，从单代理到多代理，如图 3 所示。
- en: IV-A Game research platforms
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 游戏研究平台
- en: 'TABLE II: A list of game AI competitions suitable for DRL research.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：适合 DRL 研究的游戏 AI 竞赛列表。
- en: '| Competition Name | Time |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 竞赛名称 | 时间 |'
- en: '| ViZDoom AI competition | 2016, 2017, 2018 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ViZDoom AI 竞赛 | 2016, 2017, 2018 |'
- en: '| StarCraft AI competitions (AIIDE, CIG, SSCAIT) | 2010 — 2019 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| StarCraft AI 竞赛 (AIIDE, CIG, SSCAIT) | 2010 — 2019 |'
- en: '| microRTS competition | 2017, 2018, 2019 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| microRTS 竞赛 | 2017, 2018, 2019 |'
- en: '| The GVGAI competition – learning track | 2017, 2018 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GVGAI 竞赛 – 学习赛道 | 2017, 2018 |'
- en: '| Microsoft Malmo collaborative AI challenge | 2017 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Microsoft Malmo 协作 AI 挑战 | 2017 |'
- en: '| The multi-agent RL in Malmo competition | 2018 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Malmo 竞赛中的多代理 RL | 2018 |'
- en: '| The OpenAI Retro contest | 2018 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI Retro 竞赛 | 2018 |'
- en: '| NeurIPS Pommerman competition | 2018 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| NeurIPS Pommerman 竞赛 | 2018 |'
- en: '| Unity Obstacle Tower Challenge | 2019 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Unity 障碍塔挑战 | 2019 |'
- en: '| NeurIPS MineRL competition | 2019 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| NeurIPS MineRL 竞赛 | 2019 |'
- en: 'Platforms and competitions make great contributions to the development of game
    AI, and help to evaluate agents’ intelligence, as presented in Table II. Most
    platforms can be described by two major categories: General Platforms and Specific
    Platforms.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 平台和竞赛对游戏 AI 的发展贡献巨大，并有助于评估代理的智能，如表 II 所示。大多数平台可以分为两个主要类别：通用平台和特定平台。
- en: 'General Platforms: Arcade Learning Environment (ALE) [[55](#bib.bib55)] is
    the pioneer evaluation platform for DRL algorithms, which provides an interface
    to plenty of Atari 2600 games. ALE presents both game images and signals, such
    as player scores, which makes it a suitable testbed. To promote the progress of
    DRL research, OpenAI integrates a collection of reinforcement learning tasks into
    a platform called Gym [[56](#bib.bib56)], which mainly contains Algorithmic, Atari,
    Classical Control, Board games, 2D and 3D robots. After that, OpenAI Universe
    [[57](#bib.bib57)] is a platform for measuring and training agents’ general intelligence
    across a large supply of games. Gym Retro [[58](#bib.bib58)] is a wrapper for
    video game emulator with a unified interface as Gym, and makes Gym easy to be
    extended with a large collection of video games, not only Atari but also NEC,
    Nintendo, and Sega, for RL research. The OpenAI Retro contest aims at exploring
    the development of DRL that can generalize from previous experience. OpenAI bases
    on the Sonic the Hedgehog^(TM) video game, and presents a new DRL benchmark [[59](#bib.bib59)].
    This benchmark can help to measure the performance of few-shot learning and transfer
    learning in reinforcement learning. General Video Game Playing [[60](#bib.bib60)]
    is intended to design an agent to play multiple video games without human intervention.
    The General Video Game AI (GVGAI) [[61](#bib.bib61)] competition is proposed to
    provide a easy-to-use and open-source platform for evaluating AI methods, including
    DRL. DeepMind Lab [[62](#bib.bib62)] is a first-person perspective learning environment,
    and provides multiple complicated tasks in partially observed, large-scale, and
    visually diverse worlds. Unity ML-Agents Toolkit[[63](#bib.bib63)] is a new toolkit
    for creating and interacting with simulation environments. This platform has sensory,
    physical, cognitive, and social complexity, and enables fast and distributed simulation,
    and flexible control.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通用平台：**Arcade Learning Environment (ALE)** [[55](#bib.bib55)] 是深度强化学习（DRL）算法的开创性评估平台，提供了大量的Atari
    2600游戏接口。ALE展示了游戏图像和信号，如玩家得分，使其成为一个合适的测试平台。为了推动DRL研究的进展，**OpenAI** 将一系列强化学习任务整合到一个名为**Gym**
    [[56](#bib.bib56)] 的平台中，该平台主要包括算法、Atari、经典控制、棋盘游戏、2D和3D机器人。之后，**OpenAI Universe**
    [[57](#bib.bib57)] 是一个用于测量和训练代理在大量游戏中的通用智能的平台。**Gym Retro** [[58](#bib.bib58)]
    是一个用于视频游戏模拟器的包装器，具有与Gym相统一的接口，使Gym能够轻松扩展，包含大量视频游戏，不仅限于Atari，还包括NEC、Nintendo和Sega，以支持RL研究。**OpenAI
    Retro**竞赛旨在探索可以从以前经验中泛化的DRL的发展。**OpenAI** 基于**Sonic the Hedgehog^(TM)** 视频游戏，并提出了一个新的DRL基准
    [[59](#bib.bib59)]。该基准可以帮助衡量在强化学习中少样本学习和迁移学习的表现。**General Video Game Playing**
    [[60](#bib.bib60)] 旨在设计一个能够在没有人工干预的情况下玩多个视频游戏的代理。**General Video Game AI (GVGAI)**
    [[61](#bib.bib61)] 竞赛提出了一个易于使用且开源的平台，用于评估包括DRL在内的AI方法。**DeepMind Lab** [[62](#bib.bib62)]
    是一个第一人称视角的学习环境，提供多个复杂任务，在部分观察、规模大且视觉多样的世界中进行。**Unity ML-Agents Toolkit** [[63](#bib.bib63)]
    是一个用于创建和互动模拟环境的新工具包。该平台具有感官、物理、认知和社会复杂性，并支持快速和分布式模拟及灵活控制。
- en: 'Specific Platforms: Malmo[[64](#bib.bib64)] is a research platform for AI experiments,
    which is built on top of Minecraft. It is a first-person 3D environment, and can
    be used for multi-agent research in Microsoft Malmo collaborative AI challenge
    2017 and the multi-agent RL in MalmO competition 2018\. TORCS [[65](#bib.bib65)]
    is a racing car simulator which has both low-level and visual features for the
    self-driving car with DRL. ViZDoom [[66](#bib.bib66)] is a first-person shooter
    game platform, and encourages DRL agent to utilize the visual information to perform
    navigation and shooting tasks in a semi-realistic 3D world. ViZDoom AI competition
    has attracted plenty of researchers to develop their DRL-based Doom agents since
    2016\. As far as we know, real-time strategy (RTS) games are very challenging
    for reinforcement learning method. Facebook proposes TorchCraft for StarCraft
    I [[67](#bib.bib67)], and DeepMind releases StarCraft II learning environment
    [[68](#bib.bib68)]. They expect researchers to propose powerful DRL agents to
    achieve high-level performance in RTS games and annual StarCraft AI competitions.
    CoinRun [[69](#bib.bib69)] provides a metric for an agent’s ability to transfer
    its experience to novel situations. This new training environment strikes a desirable
    balance in complexity: the environment is much simpler than traditional platform
    games, but it still poses a worthy generalization challenge for DRL algorithms.
    Google Research Football is a new environment based on open-source game Gameplay
    Football for DRL research.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 具体平台：Malmo[[64](#bib.bib64)]是一个用于AI实验的研究平台，建立在Minecraft之上。它是一个第一人称3D环境，可用于2017年Microsoft
    Malmo合作AI挑战赛和2018年MalmO比赛中的多智能体研究。TORCS[[65](#bib.bib65)]是一个赛车模拟器，具有低级和视觉特征，适用于自驾车与DRL的结合。ViZDoom[[66](#bib.bib66)]是一个第一人称射击游戏平台，鼓励DRL智能体利用视觉信息在半现实的3D世界中执行导航和射击任务。自2016年以来，ViZDoom
    AI竞赛吸引了大量研究人员开发基于DRL的Doom智能体。据我们了解，实时战略（RTS）游戏对强化学习方法来说非常具有挑战性。Facebook为StarCraft
    I[[67](#bib.bib67)]提出了TorchCraft，而DeepMind发布了StarCraft II学习环境[[68](#bib.bib68)]。他们期望研究人员提出强大的DRL智能体，在RTS游戏和年度StarCraft
    AI竞赛中取得高水平的表现。CoinRun[[69](#bib.bib69)]提供了一个衡量智能体将经验转移到新情境中的能力的指标。这个新的训练环境在复杂性上达到了理想的平衡：环境比传统平台游戏简单，但仍然对DRL算法提出了值得的泛化挑战。Google
    Research Football是一个基于开源游戏Gameplay Football的DRL研究新环境。
- en: IV-B Atari games
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B Atari游戏
- en: ALE is an evaluation platform that aims at building agents with general intelligence
    across hundreds of Atari 2600 games. As the most popular testbed for DRL research,
    a large number of DRL methods have achieved outstanding performance consecutively.
    Machado et al. [[70](#bib.bib70)] takes a review at the ALE in DRL research community,
    proposes diverse evaluation methodologies and some key concerns. In this section,
    we will introduce the main achievements in the ALE domain, including the extremely
    difficult Montezuma’s Revenge.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ALE是一个评估平台，旨在构建在数百款Atari 2600游戏中具有通用智能的智能体。作为最受欢迎的DRL研究测试平台，许多DRL方法已经连续取得了卓越的表现。Machado等人[[70](#bib.bib70)]对DRL研究社区中的ALE进行了回顾，提出了多样的评估方法和一些关键问题。在这一部分，我们将介绍ALE领域的主要成就，包括极具挑战性的Montezuma’s
    Revenge。
- en: As the milestone in this domain, DQN is able to surpass the performances of
    previous algorithms, and achieves human-level performance across 49 games [[14](#bib.bib14)].
    Averaged-DQN examines the source of value function estimation errors, and demonstrates
    significantly improved stability and performance on the ALE benchmark [[23](#bib.bib23)].
    UNREAL significantly outperforms the previous best performance on Atari, averaging
    880% expert human performance [[35](#bib.bib35)]. PAAC achieves sufficiently good
    performance on ALE after a few hours of training [[36](#bib.bib36)]. DQfD has
    better initial performance than DQN on most Atari games, and receives more average
    rewards than DQN on 27 of 42\. In addition, DQfD learns faster than DQN even when
    given poor demonstration data [[24](#bib.bib24)]. Noisy DQN replaces the conventional
    exploration heuristics with NoisyNet, and yields substantially higher scores in
    ALE domain. As a distributional DRL method, C51 obtains a new series of impressive
    results, and demonstrates the importance of the value distribution in approximated
    RL [[30](#bib.bib30)]. Rainbow provides improvements in terms of sample efficiency
    and final performance. The authors also show the contribution of each component
    to overall performance [[26](#bib.bib26)]. QR-DQN algorithm significantly outperforms
    recent improvements on DQN, including the related C51 [[30](#bib.bib30)]. IQN
    shows substantial gains on the Atari benchmark over QR-DQN, and even halves the
    distance between QR-DQN and Rainbow [[32](#bib.bib32)]. Ape-X DQN substantially
    improves the performance on the ALE, achieving better final score in less wall-clock
    training time [[71](#bib.bib71)]. When tested on a set of 42 Atari games, the
    Ape-X DQfD algorithm exceeds the performance of an average human on 40 games using
    a common set of hyperparameters. Mean and median scores across multiple Atari
    games of typical DRL methods that achieve state-of-the-art performance consecutively
    are presented in Table III.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该领域的里程碑，DQN 能够超越之前算法的表现，并在 49 款游戏中实现了人类水平的表现 [[14](#bib.bib14)]。Averaged-DQN
    检查了价值函数估计误差的来源，并在 ALE 基准测试中表现出显著的稳定性和性能提升 [[23](#bib.bib23)]。UNREAL 在 Atari 上的表现显著优于之前的最佳表现，平均达到
    880% 的专家人类表现 [[35](#bib.bib35)]。PAAC 在经过几小时训练后，在 ALE 上取得了足够好的表现 [[36](#bib.bib36)]。DQfD
    在大多数 Atari 游戏中具有比 DQN 更好的初始表现，并在 42 款游戏中的 27 款中获得了比 DQN 更多的平均奖励。此外，即使在给定较差的演示数据时，DQfD
    的学习速度也比 DQN 更快 [[24](#bib.bib24)]。Noisy DQN 用 NoisyNet 替代了传统的探索启发式方法，并在 ALE 领域中取得了显著更高的得分。作为一种分布式
    DRL 方法，C51 获得了一系列新的令人印象深刻的结果，并展示了价值分布在近似 RL 中的重要性 [[30](#bib.bib30)]。Rainbow 在样本效率和最终表现方面提供了改进。作者还展示了每个组件对整体表现的贡献
    [[26](#bib.bib26)]。QR-DQN 算法显著优于 DQN 的近期改进，包括相关的 C51 [[30](#bib.bib30)]。IQN 在
    Atari 基准测试中比 QR-DQN 有显著提升，甚至将 QR-DQN 和 Rainbow 之间的距离减半 [[32](#bib.bib32)]。Ape-X
    DQN 显著改善了 ALE 的表现，在较短的墙钟训练时间内取得了更好的最终得分 [[71](#bib.bib71)]。在 42 款 Atari 游戏的测试中，Ape-X
    DQfD 算法在 40 款游戏中超过了平均人类的表现，使用了一组常见的超参数。表 III 列出了典型 DRL 方法在多个 Atari 游戏中的平均分数和中位数分数，这些方法连续实现了最先进的表现。
- en: 'TABLE III: Mean and median scores across 57 Atari games of typical DRL methods,
    measured as percentages of human baseline.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：典型 DRL 方法在 57 款 Atari 游戏中的平均和中位数分数，作为人类基线的百分比测量。
- en: '| Methods | Mean | Median | year |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 平均 | 中位数 | 年份 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| DQN [[14](#bib.bib14)] | 228% | 79% | 2015 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| DQN [[14](#bib.bib14)] | 228% | 79% | 2015 |'
- en: '| C51 [[30](#bib.bib30)] | 701% | 178% | 2017 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| C51 [[30](#bib.bib30)] | 701% | 178% | 2017 |'
- en: '| UNREAL [[35](#bib.bib35)] | 880% | 250% | 2017 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| UNREAL [[35](#bib.bib35)] | 880% | 250% | 2017 |'
- en: '| QR-DQN [[30](#bib.bib30)] | 915% | 211% | 2017 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| QR-DQN [[30](#bib.bib30)] | 915% | 211% | 2017 |'
- en: '| IQN [[32](#bib.bib32)] | 1019% | 218% | 2018 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| IQN [[32](#bib.bib32)] | 1019% | 218% | 2018 |'
- en: '| Rainbow [[26](#bib.bib26)] | 1189% | 230% | 2018 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Rainbow [[26](#bib.bib26)] | 1189% | 230% | 2018 |'
- en: '| Ape-X DQN [[71](#bib.bib71)] | 1695% | 434% | 2018 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Ape-X DQN [[71](#bib.bib71)] | 1695% | 434% | 2018 |'
- en: '| Ape-X DQfD ^∗[[28](#bib.bib28)] | 2346% | 702% | 2018 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Ape-X DQfD ^∗[[28](#bib.bib28)] | 2346% | 702% | 2018 |'
- en: 'Note: ^∗ means this method is measured across 42 Atari games.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注：^∗ 表示该方法在 42 款 Atari 游戏中测量。
- en: Montezuma’s Revenge is one of the most difficult Atari video games. It is a
    goal-directed behavior learning environment with long horizons and sparse reward
    feedback signals. Players must navigate through a number of different rooms, avoid
    obstacles and traps, climb ladders up and down, and then pick up the key to open
    new rooms. It requires a long sequence of actions before reaching the goal and
    receiving a reward, and is difficult to explore an optimal policy to tackle tasks.
    Efficient exploration is considered as a crucial factor to learn in a delayed
    feedback environment. Then, Ostrovski et al. [[72](#bib.bib72)] provide an improved
    version of count-based exploration with PixelCNN as a supplement for pseudo-count,
    also reveals the importance of Monte Carlo return for effective exploration. In
    addition to improve the exploration efficiency, learning from human data is also
    a proper method to reach better performance in this problem. Le et al. [[73](#bib.bib73)]
    leverage imitation learning from expert interaction and hierarchical reinforcement
    learning at different levels. This method learns obviously faster than original
    hierarchical reinforcement learning, and also significantly more efficiently than
    conventional imitation learning. Other than gameplay, the demonstration is also
    a valuable kind of sample for agent to learn. DQfD utilizes a small set of demonstration
    data to speed up the learning process [[24](#bib.bib24)]. It combines prioritized
    replay mechanism with temporal difference updates and supervised classification,
    and finally achieves a better and impressive result. Further, Aytar et al. [[74](#bib.bib74)]
    only use YouTube video as a demonstration sample and invests a transformed Bellman
    operator for learning from human demonstrations. Interestingly, these two works
    both claim being the first to solve the entire first level of Montezuma’s Revenge.
    Go-explore [[75](#bib.bib75)] makes further progress, and achieves scores over
    400,000 on average. Go-Explore separates learning into exploration and robustification.
    It reliably solves the whole game, and generalizes well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 《蒙特祖马的复仇》是最困难的Atari视频游戏之一。它是一个目标导向的行为学习环境，具有长时间跨度和稀疏的奖励反馈信号。玩家必须穿越多个不同的房间，避开障碍和陷阱，上下爬梯子，然后捡起钥匙打开新房间。它需要长时间的动作序列才能达到目标并获得奖励，因此很难探索出一个最优策略来解决任务。高效的探索被认为是学习延迟反馈环境中一个关键因素。然后，Ostrovski等人[[72](#bib.bib72)]提供了一种改进的基于计数的探索方法，使用PixelCNN作为伪计数的补充，还揭示了Monte
    Carlo返回在有效探索中的重要性。除了提高探索效率外，从人类数据中学习也是达到更好性能的适当方法。Le等人[[73](#bib.bib73)]利用专家交互的模仿学习和不同层级的层次化强化学习。这种方法明显比原始的层次化强化学习学习速度更快，而且比传统的模仿学习效率更高。除了游戏玩法之外，示范也是代理学习的有价值样本。DQfD利用一小部分示范数据来加速学习过程[[24](#bib.bib24)]。它结合了优先回放机制、时序差分更新和监督分类，最终取得了更好和令人印象深刻的结果。此外，Aytar等人[[74](#bib.bib74)]仅使用YouTube视频作为示范样本，并投入了转化的Bellman算子以从人类示范中学习。有趣的是，这两项工作均声称首次解决了《蒙特祖马的复仇》第一关。Go-explore
    [[75](#bib.bib75)]取得了进一步的进展，平均分数超过400,000。Go-Explore将学习分为探索和稳健化两个阶段。它可靠地解决了整个游戏，并且具有良好的泛化能力。
- en: IV-C First-person perspective games
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 第一人称视角游戏
- en: Different from Atari games, agents in first-person perspective video games can
    only receive observations from their own perspectives, resulting from imperfect
    information inputs. In RL domain, this is a POMDP problem which requires efficient
    exploration and memory.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与Atari游戏不同，在第一人称视角视频游戏中，代理只能从自身的视角接收观察，这导致信息输入不完全。在强化学习领域，这是一种部分可观测马尔可夫决策过程（POMDP）问题，需要高效的探索和记忆。
- en: IV-C1 ViZDoom
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 ViZDoom
- en: First-person shooter (FPS) games play an important role in game AI research.
    Doom is a classical FPS game, and ViZDoom is presented as a novel testbed for
    DRL [[66](#bib.bib66)]. Agents learn from visual inputs, and interact with the
    ViZDoom environment in a first-person perspective. Wu et al. [[76](#bib.bib76)]
    propose a method that combines A3C and curriculum learning. The agent learns to
    navigate and attack via playing against built-in agents progressively. Parisotto
    et al. [[77](#bib.bib77)] develop Neural Map, which is a memory system with an
    adaptable write operator. Neural Map uses a spatially structured 2D memory image
    to store the environment’s information. This method surpasses other DRL memories
    on several challenging ViZDoom maze tasks and shows a capable generalization ability.
    Shao et al. [[78](#bib.bib78)] show that ACKTR can successfully teach agents to
    battle in ViZDoom environment, and significantly outperform A2C agents by a significant
    margin.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第一人称射击（FPS）游戏在游戏 AI 研究中发挥了重要作用。《Doom》是一款经典的 FPS 游戏，而 ViZDoom 则作为 DRL 的新型测试平台
    [[66](#bib.bib66)] 被提出。智能体通过视觉输入进行学习，并以第一人称视角与 ViZDoom 环境互动。吴等人 [[76](#bib.bib76)]
    提出了一种将 A3C 和课程学习相结合的方法。智能体通过逐步与内置智能体对战来学习导航和攻击。Parisotto 等人 [[77](#bib.bib77)]
    开发了 Neural Map，这是一个具有可适应写入操作的记忆系统。Neural Map 使用结构化的二维记忆图像来存储环境信息。这种方法在多个具有挑战性的
    ViZDoom 迷宫任务中超越了其他 DRL 记忆，并表现出强大的泛化能力。Shao 等人 [[78](#bib.bib78)] 表明 ACKTR 可以成功地教会智能体在
    ViZDoom 环境中战斗，并且显著超越 A2C 智能体。
- en: IV-C2 TORCS
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 TORCS
- en: TORCS is a racing game where actions are acceleration, braking and steering.
    This game has more realistic graphics than Atari games, but also requires agents
    to learn the dynamic of the car. FIGAR-DDPG can successfully complete the race
    task and finish 20 laps of the circuit, with a 10$\times$ total reward against
    that obtained by DDPG, and much smoother policies [[53](#bib.bib53)]. Normalized
    Actor-Critic (NAC) normalizes the Q-function effectively, and learns an initial
    policy network from demonstration and refine the policy in a real environment
    [[79](#bib.bib79)]. NAC is robust to suboptimal demonstration data, learns robustly
    and outperforms existing baselines when evaluated on TORCS. Mazumder et al. [[80](#bib.bib80)]
    incorporate state-action permissibility (SAP) and DDPG, and applies it to tackle
    the lane keeping problem in TORCS. The proposed method can speedup DRL training
    remarkably for this task. In [[81](#bib.bib81)], a two-stage approach is proposed
    for the vision-based vehicle lateral control problem which includes an multi-task
    learning perception stage and an RL control stage. By exerting the correlation
    between multiple learning task, the perception module can robustly extract track
    features. Additionally, the RL agent learns by maximizing the geometry-based reward
    and performs better than the LQR and MPC controllers. Zhu et al. [[82](#bib.bib82)]
    use DRL to train a CNN to perceive driving data from images of first-person view,
    and learns a controller to get driving commands, showing a promising performance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: TORCS 是一款赛车游戏，其动作包括加速、刹车和转向。这款游戏的图形比 Atari 游戏更为逼真，但也要求智能体学习汽车的动态。FIGAR-DDPG
    能够成功完成比赛任务，并完成 20 圈的赛道，总奖励是 DDPG 的 10$\times$，策略也更加平滑 [[53](#bib.bib53)]。Normalized
    Actor-Critic (NAC) 有效地规范化了 Q 函数，从演示中学习初始策略网络，并在实际环境中细化策略 [[79](#bib.bib79)]。NAC
    对次优演示数据具有鲁棒性，能够稳健学习，并在 TORCS 上的评估中超越现有基线。Mazumder 等人 [[80](#bib.bib80)] 结合了状态-动作许可（SAP）和
    DDPG，并将其应用于解决 TORCS 中的车道保持问题。所提出的方法可以显著加快该任务的 DRL 训练。在 [[81](#bib.bib81)] 中，提出了一种用于基于视觉的车辆横向控制问题的两阶段方法，包括多任务学习感知阶段和
    RL 控制阶段。通过发挥多个学习任务之间的相关性，感知模块能够鲁棒地提取赛道特征。此外，RL 智能体通过最大化基于几何的奖励进行学习，并表现出比 LQR 和
    MPC 控制器更好的性能。Zhu 等人 [[82](#bib.bib82)] 使用 DRL 训练 CNN 从第一人称视角的图像中感知驾驶数据，并学习控制器以获取驾驶指令，表现出良好的性能。
- en: IV-C3 Minecraft
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 Minecraft
- en: Minecraft is a sandbox construction game, where players can build creative creations,
    structures, and artwork across various game modes. Recently, it becomes a popular
    platform for game AI research, with 3D infinitely varied data. Project Malmo is
    an experimentation platform [[83](#bib.bib83)] that builts on the Minecraft for
    AI research. It supports a large number of scenarios, including navigation, problem
    solving tasks, and survival to collaboration. Xiong et al. [[84](#bib.bib84)]
    propose a novel Q-learning approach with state-action abstraction and warm start
    using human reasoning to learn effective policies in the Microsoft Malmo collaborative
    AI challenge. The ability to transfer knowledge from source task to target task
    in Minecraft is one of the major challenges. Tessler et al. [[85](#bib.bib85)]
    provides a DRL agent which can transfer knowledge by learning reusable skills,
    and then incorporated into hierarchical DRL network (H-DRLN). H-DRLN exhibits
    superior performance and low learning sample complexity compared to regular DQN
    in Minecraft, and the potential to transfer knowledge between related Minecraft
    tasks without any additional learning. To solve the partial or non-Markovian observations
    problems, Jin et al. [[86](#bib.bib86)] propose a new DRL algorithm based on counterfactual
    regret minimization that iteratively updates an approximation to a cumulative
    clipped advantage function. On the challenging Minecraft first-person navigation
    benchmarks, this algorithm can substantially outperform strong baseline methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Minecraft 是一个沙盒建造游戏，玩家可以在各种游戏模式中构建创意作品、结构和艺术品。最近，它成为了游戏 AI 研究的热门平台，拥有 3D 无限变化的数据。Project
    Malmo 是一个实验平台 [[83](#bib.bib83)]，基于 Minecraft 用于 AI 研究。它支持大量的场景，包括导航、问题解决任务和生存协作。Xiong
    等人 [[84](#bib.bib84)] 提出了一种新颖的 Q 学习方法，通过状态-动作抽象和使用人类推理的暖启动来学习有效的策略，用于微软 Malmo
    协作 AI 挑战。在 Minecraft 中从源任务到目标任务的知识迁移是主要挑战之一。Tessler 等人 [[85](#bib.bib85)] 提供了一种
    DRL 代理，通过学习可重用技能进行知识迁移，然后将其纳入到层次 DRL 网络 (H-DRLN) 中。与普通 DQN 相比，H-DRLN 在 Minecraft
    中展示了卓越的性能和低学习样本复杂度，并且具有在相关 Minecraft 任务之间迁移知识的潜力，而无需额外学习。为了解决部分或非马尔可夫观测问题，Jin
    等人 [[86](#bib.bib86)] 提出了一种基于反事实后悔最小化的新 DRL 算法，该算法迭代更新对累积剪切优势函数的近似。在具有挑战性的 Minecraft
    第一人称导航基准测试中，该算法可以显著优于强基线方法。
- en: IV-C4 DeepMind lab
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 DeepMind 实验室
- en: DeepMind lab is a 3D first-person game platform extended from OpenArena, which
    is based on Quake3. Comparable to other first-person game platforms, DeepMind
    lab has considerably richer visuals and more realistic physics, making it a significantly
    complex platform. On a challenging suite of DeepMind lab tasks, the UNREAL agent
    leads to a mean speedup in learning of 10$\times$ over A3C and averaging 87% expert
    human performance. As learning agents become more powerful, continual learning
    has made quick progress recently. To test continual learning capabilities, Mankowitz
    et al. [[87](#bib.bib87)] consider an implicit sequence of tasks with sparse rewards
    in DeepMind lab. The novel agent architecture called Unicorn, demonstrates strong
    continual learning and outperforms several baseline agents on the proposed domain.
    Schmitt et al. [[88](#bib.bib88)] present a method which uses teacher agents to
    kickstart the training of a new student agent. On a multi-task and challenging
    DMLab-30 suite, kickstarted training improves new agents’ sample efficiency to
    a great extend, and surpasses the final performance by 42%. Jaderberg et al. [[89](#bib.bib89)]
    focus on Quake III Arena Capture the Flag, which is a popular 3D first-person
    multiplayer video game, and demonstrates that DRL agents can achieve human-level
    performance with only pixels and game points as input. The agent uses population
    based training to optimize the policy. This method trains a large number of agents
    concurrently from thousands of parallel matches, where agents plays cooperatively
    in teams and against each other on randomly generated environments. In an evaluation,
    the trained agents exceed the winrate of self-play baseline and high-level human
    players both as teammates and opponents, and are proved far stronger than existing
    DRL agents.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind lab 是一个基于 Quake3 的 3D 第一人称游戏平台，扩展自 OpenArena。与其他第一人称游戏平台相比，DeepMind
    lab 拥有更丰富的视觉效果和更真实的物理效果，使其成为一个极其复杂的平台。在 DeepMind lab 的一套具有挑战性的任务中，UNREAL 代理在学习上的平均加速比为
    A3C 的 10$\times$，并且平均达到了 87% 的专家人类表现。随着学习代理变得更强大，持续学习最近取得了快速进展。为了测试持续学习能力，Mankowitz
    等人 [[87](#bib.bib87)] 考虑了 DeepMind lab 中稀疏奖励的隐式任务序列。名为 Unicorn 的新型代理架构展示了强大的持续学习能力，并在所提议的领域中超越了几种基线代理。Schmitt
    等人 [[88](#bib.bib88)] 提出了一种方法，使用教师代理启动新学生代理的训练。在多任务且具有挑战性的 DMLab-30 套件上，启动训练显著提高了新代理的样本效率，并超越了最终性能
    42%。Jaderberg 等人 [[89](#bib.bib89)] 关注 Quake III Arena Capture the Flag，这是一个流行的
    3D 第一人称多人视频游戏，展示了 DRL 代理仅凭像素和游戏点数作为输入即可达到人类水平的表现。该代理使用基于种群的训练来优化策略。这种方法通过数千场并行比赛同时训练大量代理，代理在随机生成的环境中协作组队并互相对抗。在评估中，训练后的代理在作为队友和对手时都超过了自我对战基线和高级人类玩家的胜率，证明其远远强于现有的
    DRL 代理。
- en: IV-D Real-time strategy games
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 实时策略游戏
- en: Real-time strategy games are very popular among players, and have become popular
    platforms for AI research.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 实时策略游戏在玩家中非常受欢迎，并且已经成为人工智能研究的热门平台。
- en: IV-D1 StarCraft
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 星际争霸
- en: In StarCraft, players need to perform actions according to real-time game states,
    and defeat the enemies. Generally speaking, designing an AI bot have many challenges,
    including multi-agent collaboration, spatial and temporal reasoning, adversarial
    planning, and opponent modeling. Currently, most bots are based on human experiences
    and replays, with limited flexibility and intelligence. DRL is proved to be a
    promising direction for StarCraft AI, especially in micromanagement, build order,
    mini-games and full-games [[90](#bib.bib90)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在《星际争霸》中，玩家需要根据实时游戏状态执行操作，并击败敌人。一般来说，设计一个 AI 机器人面临许多挑战，包括多代理协作、空间和时间推理、对抗性规划以及对手建模。目前，大多数机器人基于人类经验和回放，灵活性和智能性有限。DRL
    被证明是《星际争霸》AI 的一个有前途的方向，特别是在微观管理、建造顺序、迷你游戏和完整游戏 [[90](#bib.bib90)]。
- en: Recently, micromanagement is widely studied as the first step to solve StarCraft
    AI. Usunier et al. [[91](#bib.bib91)] introduce the greedy MDP with episodic zero-order
    optimization (GMEZO) algorithm to tackle micromanagement scenarios, which performs
    better than DQN and policy gradient. BiCNet [[92](#bib.bib92)] is a multi-agent
    deep reinforcement learning method to play StarCraft combat games. It bases on
    actor-critic reinforcement learning, and uses bi-directional neural networks to
    learn collaboration. BiCNet successfully learns some cooperative strategies, and
    is adaptable to various tasks, showing better performances than GMEZO. In aforementioned
    works, researchers mainly develops centralized methods to play micromanagement.
    Foerster et al. [[93](#bib.bib93)] focus on decentralized control for micromanagement,
    and propose a multi-agent actor-critic method. To stabilize experience replay
    and solve nonstationarity, they use fingerprints and importance sampling, which
    can improve the final performance. Shao et al. [[94](#bib.bib94)] follow decentralized
    micromanagement task, and propose parameter sharing multi-agent gradient descent
    SARSA($\lambda$) (PS-MAGDS) method. To resue the knowledge between various micromanagement
    scenarios, they also combine curriculum transfer learning to this method. This
    improves the sample efficiency, and outperforms GMEZO and BiCNet in large-scale
    scenarios. Kong et al. [[95](#bib.bib95)] bases on master-slave architecture,
    and proposes master-slave multi-agent reinforcement learning (MS-MARL). MS-MARL
    includes composed action representation, independent reasoning, and learnable
    communication. This method has better performance than other methods in micromanagement
    tasks. Rashid et al. [[96](#bib.bib96)] focus on several challenging StarCraft
    II micromanagement tasks, and use centralized training and decentralized execution
    to learn cooperative behaviors. This eventually outperforms state-of-the-art multi-agent
    deep reinforcement learning methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，微观管理作为解决《星际争霸》人工智能的第一步被广泛研究。Usunier等人[[91](#bib.bib91)]提出了贪心MDP与阶段性零阶优化（GMEZO）算法来应对微观管理场景，其表现优于DQN和策略梯度。BiCNet[[92](#bib.bib92)]是一种用于《星际争霸》战斗游戏的多智能体深度强化学习方法。它基于演员-评论家强化学习，并使用双向神经网络来学习协作。BiCNet成功学习了一些合作策略，并且适应各种任务，其表现优于GMEZO。在上述工作中，研究人员主要开发了集中式方法来处理微观管理。Foerster等人[[93](#bib.bib93)]关注于微观管理的分散控制，并提出了一种多智能体演员-评论家方法。为了稳定经验回放和解决非平稳性，他们使用了指纹和重要性采样，这可以提升最终性能。Shao等人[[94](#bib.bib94)]继续研究分散式微观管理任务，并提出了参数共享多智能体梯度下降SARSA($\lambda$)（PS-MAGDS）方法。为了在各种微观管理场景之间重用知识，他们还将课程迁移学习结合到该方法中。这提高了样本效率，并在大规模场景中优于GMEZO和BiCNet。Kong等人[[95](#bib.bib95)]基于主从架构，提出了主从多智能体强化学习（MS-MARL）。MS-MARL包括组合动作表示、独立推理和可学习通信。这种方法在微观管理任务中的表现优于其他方法。Rashid等人[[96](#bib.bib96)]关注于几个具有挑战性的《星际争霸II》微观管理任务，并使用集中式训练和分散式执行来学习合作行为。这最终超越了现有的多智能体深度强化学习方法。
- en: Researchers also use DRL methods to optimize the build order in StarCraft. Tang
    et al. [[97](#bib.bib97)] put forward neural network fitted Q-learning (NNFQ)
    and convolutional neural network fitted Q-learning (CNNFQ) to build units in simple
    StarCraft maps. These models are able to find effective production sequences,
    and eventually defeat enemies. In [[68](#bib.bib68)], researchers present baseline
    results of several main DRL agents in the StarCraft II domain. The fully convolutional
    advantage actor-critic (FullyConv-A2C) agents achieve a beginner-level in StarCraft
    II mini-games. Zambaldi et al. [[98](#bib.bib98)] introduce the relational DRL
    to StarCraft, which iteratively reasons about the relations between entities with
    self-attention, and uses it to guide a model-free RL policy. This method improves
    sample efficiency, generalization ability, and interpretability of conventional
    DRL approaches. Relational DRL agent achieves impressive performance on SC2LE
    mini-games. Sun et al. [[99](#bib.bib99)] develop the DRL based agent TStarBot,
    which uses flat action structure. This agent defeats the built-in AI agents from
    level 1 to level 10 in a full game firstly. Lee et al. [[100](#bib.bib100)] focus
    on StarCraft II AI, and present a novel modular architecture, which splits responsibilities
    between multiple modules. Each module controls one aspect of the game, and two
    modules are trained with self-play DRL methods. This method defeats the built-in
    bot in ”Harder” level. Pang et al. [[101](#bib.bib101)] investigate a two-level
    hierarchical RL approach for StarCraft II. The macro-action is automatically extracted
    from expert’s data, and the other is a flexible and scaleable hierarchical architecture.
    More recently, DeepMind proposes AlphaStar, and defeats professional players for
    the first time.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员还使用深度强化学习（DRL）方法来优化《星际争霸》的建造顺序。Tang 等人[[97](#bib.bib97)] 提出了神经网络拟合 Q 学习（NNFQ）和卷积神经网络拟合
    Q 学习（CNNFQ），用于在简单的《星际争霸》地图中建造单位。这些模型能够找到有效的生产顺序，并最终击败敌人。在[[68](#bib.bib68)]中，研究人员展示了《星际争霸
    II》领域中几种主要 DRL 代理的基线结果。完全卷积优势演员-评论员（FullyConv-A2C）代理在《星际争霸 II》迷你游戏中达到了初学者水平。Zambaldi
    等人[[98](#bib.bib98)] 将关系型 DRL 引入《星际争霸》，该方法通过自注意力机制迭代推理实体之间的关系，并用它来指导无模型的强化学习策略。这种方法提高了传统
    DRL 方法的样本效率、泛化能力和可解释性。关系型 DRL 代理在 SC2LE 迷你游戏中表现出色。Sun 等人[[99](#bib.bib99)] 开发了基于
    DRL 的代理 TStarBot，该代理使用平坦的行动结构。该代理首先击败了从等级 1 到等级 10 的内置 AI 代理。Lee 等人[[100](#bib.bib100)]
    专注于《星际争霸 II》 AI，并提出了一种新颖的模块化架构，该架构将职责分配到多个模块中。每个模块控制游戏的一个方面，两个模块通过自对弈 DRL 方法进行训练。这种方法在“更难”级别上击败了内置机器人。Pang
    等人[[101](#bib.bib101)] 研究了一种用于《星际争霸 II》的两级层次化强化学习方法。宏观动作是从专家数据中自动提取的，另一种是灵活且可扩展的层次化架构。最近，DeepMind
    提出了 AlphaStar，并首次击败了职业玩家。
- en: IV-D2 MOBA and Dota2
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 MOBA 和 Dota2
- en: MOBA (Multiplayer Online Battle Arena) is originated from RTS games, which has
    two teams, and each team consists of five players. To beat the opponent, five
    players in a team must cooperate together, kill enemies, upgrade heros, and eventually
    destroy the opponent base. Since MOBA research is still in a primary stage, there
    are fewer works than conventional RTS games. Most works on MOBA concentrate on
    dataset analysis and case study. However, due to a series of breakthroughs that
    DRL achieves in game AI, researchers start to pay more attention to MOBA recently.
    King of Glory (a simplified mobile version of Dota) is the most popular mobile-end
    MOBA game in China. Jiang et al. [[102](#bib.bib102)] apply Monte-Carlo Tree Search
    and deep neural networks to this game. The experimental results indicate that
    MCTS-based DRL method is efficient and can be used in 1v1 MOBA scenario. Most
    impressive works on MOBA are proposed by OpenAI. Their results prove that DRL
    method with self-play can not only be successful in a 1v1 and 2v2 Dota2 scenarios
    [[103](#bib.bib103)], but also in 5v5 [[104](#bib.bib104)][[105](#bib.bib105)].
    The model architecture is simple, using a LSTM layer as the core component of
    neural network. Under the support of massively distributed cloud computing and
    PPO optimization algorithm, OpenAI Five can master the critical abilities of team
    fighting, searching forest, focusing, chasing, and diversion for team victory,
    and defeat human champion OG with 2:0\. Their works truly open a new door to MOBA
    research with DRL method.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: MOBA（多人在线战术竞技场）起源于RTS游戏，通常由两个团队组成，每个团队有五名玩家。为了击败对手，团队中的五名玩家必须协作，击杀敌人，升级英雄，并最终摧毁对方基地。由于MOBA研究仍处于初级阶段，其研究工作少于传统的RTS游戏。大多数MOBA的研究集中在数据集分析和案例研究上。然而，由于深度强化学习在游戏AI中取得了一系列突破，研究人员最近开始更多地关注MOBA。王者荣耀（Dota的简化移动版本）是中国最受欢迎的移动端MOBA游戏。姜等人[[102](#bib.bib102)]将蒙特卡洛树搜索和深度神经网络应用于这款游戏。实验结果表明，基于MCTS的深度强化学习方法是有效的，并且可以用于1v1的MOBA场景。最令人印象深刻的MOBA研究由OpenAI提出。他们的结果证明，使用自我对弈的深度强化学习方法不仅在1v1和2v2的Dota2场景中成功[[103](#bib.bib103)]，而且在5v5场景中也同样成功[[104](#bib.bib104)][[105](#bib.bib105)]。该模型架构简单，使用LSTM层作为神经网络的核心组件。在大规模分布式云计算和PPO优化算法的支持下，OpenAI
    Five能够掌握团队战斗、寻找森林、专注、追击和分散的关键能力，并以2:0击败人类冠军OG。他们的工作真正为MOBA研究的深度强化学习方法打开了一扇新门。
- en: V Challenges in Games with DRL
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 深度强化学习在游戏中的挑战
- en: Since DRL has achieved large progress in some video games, it is considered
    as one of most promising ways to realize the artificial general intelligence.
    However, there are still some challenges should be conquered towards goal. In
    this secition, we discuss some crucial challenges for DRL in video games, such
    as tradeoff between exploration and exploitation, low sample efficiency, dilemma
    in generalization and overfiting, multi-agent learning, incomplete information
    and delayed sparse rewards. Though there are some proposed approaches have been
    tried to solve these problems, as presented in Fig. 4, there are still some limitations
    should be broken.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度强化学习在一些视频游戏中取得了巨大进展，它被认为是实现人工通用智能的最有前途的方式之一。然而，仍然存在一些挑战需要克服。在本节中，我们讨论了深度强化学习在视频游戏中面临的一些关键挑战，如探索与利用之间的权衡、低样本效率、泛化与过拟合的困境、多智能体学习、不完整信息和延迟稀疏奖励。尽管已经有一些提出的办法尝试解决这些问题，如图4所示，但仍然存在一些限制需要突破。
- en: V-A Exploration-exploitation
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 探索-利用
- en: Exploration can help to obtain more diversity samples, while exploitation is
    the way to learn the high reward policy with valuable samples. The trade-off between
    exploration and exploitation remains a major challenge for RL. Common methods
    for exploration require a large amount of data, and can not tackle temporally-extended
    exploration. Most model-free RL algorithms are not computationally tractable in
    complicated environments.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 探索有助于获得更多的多样化样本，而利用则是通过有价值的样本学习高回报策略的方式。探索与利用之间的权衡仍然是强化学习中的主要挑战。常见的探索方法需要大量的数据，且无法处理时间上扩展的探索。大多数无模型强化学习算法在复杂环境中并不可计算。
- en: Parametric noise can help exploration to a large extend in the training process
    [[106](#bib.bib106)] [[107](#bib.bib107)]. Besides, randomized value functions
    become an effective approach for efficient exploration. Combining exploration
    with deep neural networks can help to learn much faster, which greatly improves
    the learning speed and final performance in most games [[51](#bib.bib51)].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 参数噪声在训练过程中可以在很大程度上帮助探索[[106](#bib.bib106)] [[107](#bib.bib107)]。此外，随机化价值函数成为了高效探索的一种有效方法。将探索与深度神经网络结合可以加速学习，这在大多数游戏中大大提高了学习速度和最终性能[[51](#bib.bib51)]。
- en: A simple generalization of popular count-based approach can reach satisfactory
    performance on various high-dimensional DRL benchmarks [[108](#bib.bib108)]. This
    method maps states to hash codes, and counts their occurrences via a hash table.
    Then, according to the classic count-based method, we can use these counts to
    compute a reward bonus. On many challenging tasks, these simple hash functions
    can achieve impressive performance. This exploration strategy provides a simple
    and powerful baseline to solve MDPs requiring considerable exploration.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的流行计数方法的推广可以在各种高维深度强化学习基准测试中达到令人满意的性能[[108](#bib.bib108)]。该方法将状态映射到哈希码，并通过哈希表计算其出现次数。然后，根据经典的计数方法，我们可以使用这些计数来计算奖励奖金。在许多具有挑战性的任务中，这些简单的哈希函数可以取得令人印象深刻的性能。这种探索策略提供了一个简单而强大的基线，以解决需要大量探索的马尔可夫决策过程（MDP）。
- en: V-B Sample efficiency
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 样本效率
- en: DRL algorithms usually take millions of samples to achieve human-level performance.
    While humans can quickly master highly rewarding actions of an environment. Most
    model-free DRL algorithms are data inefficient, especially for a environment with
    high dimension and large explore space. They have to interact with environment
    in a large time cost for seek out high reward experiences in a complex sample
    space, which limits their applicability to many scenarios. In order to reduce
    the exploration dimension of environment and ease the expenditure of time on interaction,
    some solutions can be used for improving data efficiency, such as hierarchy and
    demonstration.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习算法通常需要数百万个样本才能达到人类水平的表现。而人类可以迅速掌握环境中的高奖励动作。大多数无模型的深度强化学习算法在数据利用效率方面表现不佳，尤其是在高维度和广泛探索空间的环境中。它们必须在较长的时间成本下与环境进行交互，以寻找复杂样本空间中的高奖励体验，这限制了它们在许多场景中的应用。为了减少环境的探索维度和减少交互的时间开销，可以使用一些解决方案来提高数据效率，例如层次结构和示范。
- en: Hierarchical reinforcement learning (HRL) allows agents to decompose the task
    into several simple subtasks, which can speed up training and improve sample efficiency.
    Temporal abstraction is key to scaling up learning, while creating such abstractions
    autonomously has remained challenging. The option-critic architecture has the
    ability to learn the internal policies and the options’ termination conditions,
    without any additional rewards or subgoals [[109](#bib.bib109)]. FeUdal Networks
    (FuNs) include a Manager module and a Worker module [[110](#bib.bib110)]. The
    Manager sets abstract goals at high-level. The Worker receives these goals, and
    generates actions in the environment. FuN dramatically outperforms baseline agents
    on tasks that involve long-term credit assignment or memorization. Representation
    learning methods can also be used to guide the option discovery process in HRL
    domain [[111](#bib.bib111)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 层次强化学习（HRL）允许代理将任务分解为几个简单的子任务，这可以加快训练速度并提高样本效率。时间抽象是扩展学习的关键，而自主创建这种抽象一直是一个挑战。选项-批评家架构能够学习内部策略和选项的终止条件，而无需任何额外的奖励或子目标[[109](#bib.bib109)]。FeUdal
    网络（FuNs）包括一个管理者模块和一个工人模块[[110](#bib.bib110)]。管理者在高层设置抽象目标。工人接收这些目标，并在环境中生成动作。FuN在涉及长期信用分配或记忆的任务中显著优于基线代理。表示学习方法也可以用于指导HRL领域中的选项发现过程[[111](#bib.bib111)]。
- en: Demonstration is a proper technique to improve sample efficiency. Current approaches
    that learn from demonstration use supervised learning on expert data and use reinforcement
    learning to improve the performance. This method is difficult to jointly optimize
    divergent losses, and is very sensitive to noisy demonstrations. Leveraging data
    from previous control of the system can greatly accelerate the learning process
    even with small amounts of demonstration data [[24](#bib.bib24)]. Goals defined
    with human preferences can effectively solve complicated RL tasks without the
    reward function, while greatly reducing the cost of human oversight [[112](#bib.bib112)].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 示范是一种提高样本效率的有效技术。当前从示范中学习的方法使用专家数据进行监督学习，并通过强化学习提高性能。这种方法难以联合优化不同的损失，并且对噪声示范非常敏感。利用系统先前控制的数据可以大大加速学习过程，即使只有少量示范数据
    [[24](#bib.bib24)]。通过人类偏好定义的目标可以有效解决复杂的强化学习任务，无需奖励函数，同时大大降低了人类监督的成本 [[112](#bib.bib112)]。
- en: V-C Generalization and Transfer
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 泛化与转移
- en: The ability to transfer knowledge across multiple environments is considered
    as a critical aspect of intelligent agents. With the purpose of promoting the
    performance of generalization in multiple environments, multi-task learning and
    policy distillation have been focus on these situations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 跨多个环境转移知识的能力被认为是智能体的一个关键方面。为了促进在多个环境中的泛化性能，多任务学习和策略精炼已成为关注的重点。
- en: Multi-task learning with shared neural network parameters can solve the generalization
    problem, and efficiency can be improved through transfer across related tasks.
    Hybrid reward architecture takes a decomposed reward function as input and learns
    a separate value function for each component [[113](#bib.bib113)]. The whole value
    function is much smoother, which can be easily approximated with a low-dimensional
    representation, and learns more effectively. IMPALA shows the effectiveness for
    multi-task reinforcement learning, using less data and exhibiting positive transfer
    between tasks [[40](#bib.bib40)] . PopArt-IMPALA combines PopArt’s adaptive normalization
    with IMPALA, and allows a more efficient use of parallel data generation, showing
    impressive performance on multi-task domain [[114](#bib.bib114)].
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 具有共享神经网络参数的多任务学习可以解决泛化问题，并且通过在相关任务之间的转移可以提高效率。混合奖励架构将分解的奖励函数作为输入，并为每个组件学习一个单独的价值函数
    [[113](#bib.bib113)]。整体价值函数更加平滑，可以用低维表示容易逼近，从而学习效果更佳。IMPALA 显示了多任务强化学习的有效性，使用较少的数据并在任务之间展现了正向转移
    [[40](#bib.bib40)]。PopArt-IMPALA 将 PopArt 的自适应归一化与 IMPALA 结合起来，允许更高效地使用并行数据生成，并在多任务领域中展示了令人印象深刻的性能
    [[114](#bib.bib114)]。
- en: To successfully learn complex tasks with DRL, we usually need large task-specific
    networks and extensive training to achieve good performance. Distral shares a
    distilled policy which can learn common knowledge across multiple tasks [[115](#bib.bib115)].
    Each worker is trained to solve individual task and to be close to the shared
    policy, while the shared policy is trained by distillation. This approach shows
    efficient transfer on complex tasks, with more robust and more stable performance.
    Mix & Match is a training framework that is designed to encourage effective and
    rapid learning in DRL agents [[116](#bib.bib116)]. It allows to automatically
    form a curriculum over agent, and progressively trains more complex agents from
    simpler agents.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 成功地使用深度强化学习（DRL）学习复杂任务通常需要大型任务特定网络和大量训练，以达到良好的性能。Distral 共享一种精炼策略，能够在多个任务之间学习共通知识
    [[115](#bib.bib115)]。每个工作者被训练以解决单个任务，并接近共享策略，而共享策略则通过精炼进行训练。这种方法在复杂任务中表现出高效的转移，更加稳健和稳定的性能。Mix
    & Match 是一种训练框架，旨在鼓励 DRL 代理的有效和快速学习 [[116](#bib.bib116)]。它可以自动形成代理的课程，并逐步训练更复杂的代理，从简单代理开始。
- en: V-D Multi-agent learning
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 多智能体学习
- en: Multi-agent learning is very important in video games, such as StarCraft. In
    a cooperative multi-agent setting, curse-of-dimensionality, communication, and
    credit assignment are major challenges.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体学习在视频游戏中非常重要，如《星际争霸》。在合作的多智能体设置中，维度诅咒、通信和信用分配是主要挑战。
- en: Team learning uses a single learner to learn joint solutions in multi-agent
    system, while concurrent learning uses multiple learners for each agent. Recently,
    the centralised training of decentralised policies is becoming a standard paradigm
    for multi-agent training. Multi-agent DDPG considers other agents’ action policy
    and can successfully learn complex multi-agent coordination behavior [[117](#bib.bib117)].
    Counterfactual multi-agent policy gradients uses a centralized critic to estimate
    the action-value function and decentralized actors to optimize each agents’ policies,
    with a counterfactual advantage function to address the multi-agent credit assignment
    problem [[118](#bib.bib118)] . In addition, communication protocols is important
    to share information to solve multi-agent tasks. Reinforced Inter-Agent Learning
    (RIAL) and Differentiable Inter-Agent Learning (DIAL) use deep reinforcement learning
    to learn end-to-end communication protocols in complex environments. Analogously,
    CommNet is able to learn continuous communication between multiple agents.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 团队学习使用一个学习者在多智能体系统中学习联合解决方案，而并行学习则为每个智能体使用多个学习者。最近，集中式的去中心化政策训练正在成为多智能体训练的标准范式。多智能体DDPG考虑了其他智能体的行动政策，并且可以成功地学习复杂的多智能体协调行为[[117](#bib.bib117)]。反事实多智能体政策梯度使用集中式评论员来估计行动价值函数，并使用去中心化的演员来优化每个智能体的政策，采用反事实优势函数来解决多智能体信用分配问题[[118](#bib.bib118)]。此外，通信协议对于共享信息以解决多智能体任务是非常重要的。增强型智能体间学习（RIAL）和可微分智能体间学习（DIAL）使用深度强化学习在复杂环境中学习端到端的通信协议。类似地，CommNet能够学习多个智能体之间的连续通信。
- en: V-E Imperfect information
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 不完全信息
- en: In partially observable and first-perspective games, DRL agents need to tackle
    imperfect information to learn a suitable policy. Making decisions in these environments
    is challenging for DRL agents.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在部分可观察和第一人称视角的游戏中，DRL代理需要应对不完整的信息以学习适当的策略。在这些环境中做决策对DRL代理来说具有挑战性。
- en: A critical component of enabling effective learning in these environment is
    the use of memory. DRL agents have used some simple memory architectures, such
    as several past frames or an LSTM layer. But these architectures are limited to
    only remember transitory information. Model-free episode control learns difficult
    sequential decision-making tasks much faster, and achieves a higher overall reward
    [[119](#bib.bib119)]. Differentiable neural computer uses a neural network to
    read from and write to an external memory matrix [[120](#bib.bib120)]. This method
    can solve complex, structured tasks which can not access to neural networks without
    external read and write memory. Neural episodic control inserts recent state representations
    paired with corresponding value functions into the appropriate neural dictionary,
    and learns significantly faster than other baseline agents [[121](#bib.bib121)].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些环境中实现有效学习的关键组成部分是使用记忆。深度强化学习（DRL）代理已经使用了一些简单的记忆架构，例如多个过去的帧或一个LSTM层。但这些架构仅限于记住暂时性的信息。无模型的阶段控制在学习困难的序列决策任务时要快得多，并且获得了更高的整体奖励[[119](#bib.bib119)]。可微分神经计算机使用神经网络从外部记忆矩阵中读取和写入信息[[120](#bib.bib120)]。这种方法可以解决复杂的结构化任务，而这些任务在没有外部读写记忆的情况下无法访问神经网络。神经情节控制将最近的状态表示与相应的价值函数配对，插入到适当的神经字典中，并且比其他基线代理学习得显著更快[[121](#bib.bib121)]。
- en: V-F Delayed spare rewards
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 延迟稀疏奖励
- en: The sparse and delayed reward is very common in many games, and is also one
    of the reasons that reduce sample efficiency in reinforcement learning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏和延迟的奖励在许多游戏中非常常见，也是减少强化学习样本效率的原因之一。
- en: In many scenarios, researchers use curiosity as an intrinsic reward to encourage
    agents to explore environment and learn useful skills. Curiosity can be formulated
    as the error that the agent predicts its own actions’ consequence in a visual
    space [[122](#bib.bib122)]. This can scale to high-dimensional continuous state
    spaces. Moreover, it leaves out the aspects of environment that cannot affect
    agents. Curiosity search for DRL encourages intra-life exploration by rewarding
    agents for visiting as many different states as possible within each episode [[123](#bib.bib123)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，研究人员使用好奇心作为内在奖励来鼓励代理探索环境并学习有用的技能。好奇心可以被公式化为代理预测其自身行动后果在视觉空间中的误差[[122](#bib.bib122)]。这种方法可以扩展到高维连续状态空间。此外，它排除了那些不会影响代理的环境方面。好奇心搜索对DRL鼓励在每个阶段内奖励代理访问尽可能多的不同状态[[123](#bib.bib123)]。
- en: VI Conclusion and discussion
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论与讨论
- en: Game AI with deep reinforcement learning is a challenging and promising direction.
    Recent progress in this domain has promote the development of artificial intelligence
    research. In this paper, we review the achievements of deep reinforcement learning
    in video games. Different DRL methods and their successful applications are introduced.
    These DRL agents achieve human-level or super-human performances in various games,
    from 2D perfect information to 3D imperfect information, and from single-agent
    to multi-agent. In addition to these achievements, there are still some major
    problems when applying DRL methods to this field, especially in 3D imperfect information
    multi-agent video game. A high-level game AI requires to explore more efficient
    and robust DRL techniques, and needs novel frameworks to be implemented in complex
    environment. These challenges have not been fully investigated and could be opened
    for further study in the future.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏AI与深度强化学习是一个充满挑战和前景广阔的方向。该领域的最新进展促进了人工智能研究的发展。本文回顾了深度强化学习在视频游戏中的成就。介绍了不同的DRL方法及其成功应用。这些DRL代理在各种游戏中实现了人类水平或超人类表现，从2D完全信息到3D不完全信息，从单一代理到多代理。除了这些成就外，在将DRL方法应用于此领域时，尤其是在3D不完全信息的多代理视频游戏中，仍然存在一些主要问题。高水平的游戏AI需要探索更高效和更稳健的DRL技术，并且需要在复杂环境中实现新的框架。这些挑战尚未被完全研究，未来可以进一步探讨。
- en: Acknowledgment
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Qichao Zhang, Dong Li and Weifan Li for the
    helpful comments and discussions about this work.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Qichao Zhang, Dong Li 和 Weifan Li 对本工作的有益评论和讨论。
- en: References
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Y. Georgios and T. Julian, *Artificial Intelligence and Games*.   New
    York: Springer, 2018.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N. Y. Georgios 和 T. Julian, *人工智能与游戏*。纽约：Springer，2018。'
- en: '[2] Y. Lecun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Y. Lecun, Y. Bengio 和 G. Hinton，“深度学习，”*自然*，第521卷，第7553期，页436–444，2015。'
- en: '[3] D. Zhao, K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and
    C. Wang, “Review of deep reinforcement learning and discussions on the development
    of computer Go,” *Control Theory and Applications*, vol. 33, no. 6, pp. 701–717,
    2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Zhao, K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou 和 C.
    Wang，“深度强化学习综述及计算机围棋发展的讨论，”*控制理论与应用*，第33卷，第6期，页701–717，2016。'
- en: '[4] Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep reinforcement
    learning: from AlphaGo to AlphaGo Zero,” *Control Theory and Applications*, vol. 34,
    no. 12, pp. 1529–1546, 2017.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Tang, K. Shao, D. Zhao 和 Y. Zhu，“深度强化学习的最新进展：从AlphaGo到AlphaGo Zero，”*控制理论与应用*，第34卷，第12期，页1529–1546，2017。'
- en: '[5] J. Niels, B. Philip, T. Julian, and R. Sebastian, “Deep learning for video
    game playing,” *CoRR*, vol. abs/1708.07902, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Niels, B. Philip, T. Julian 和 R. Sebastian，“用于视频游戏的深度学习，”*CoRR*，第abs/1708.07902卷，2017。'
- en: '[6] A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, “Deep reinforcement learning:
    A brief survey,” *IEEE Signal Processing Magazine*, vol. 34, pp. 26–38, 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Kailash, P. D. Marc, B. Miles 和 A. B. Anil，“深度强化学习：简要调查，”*IEEE信号处理杂志*，第34卷，页26–38，2017。'
- en: '[7] L. Yuxi, “Deep reinforcement learning: An overview,” *CoRR*, vol. abs/1701.07274,
    2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Yuxi，“深度强化学习：概述，”*CoRR*，第abs/1701.07274卷，2017。'
- en: '[8] R. Dechter, “Learning while searching in constraint-satisfaction-problems,”
    pp. 178–183, 1986.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] R. Dechter，“在约束满足问题中边搜索边学习，”页178–183，1986。'
- en: '[9] J. Schmidhuber, “Deep learning in neural networks,” *Neural Networks*,
    vol. 61, pp. 85–117, 2015.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Schmidhuber，“神经网络中的深度学习，”*神经网络*，第61卷，页85–117，2015。'
- en: '[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *International Conference on Neural
    Information Processing Systems*, 2012, pp. 1097–1105.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，“利用深度卷积神经网络进行Imagenet分类”，发表于*国际神经信息处理系统会议*，2012，页1097–1105。'
- en: '[11] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural Computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第9卷，第8期，页1735–1780，1997。'
- en: '[12] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*.   MIT
    Press, 1998.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] R. S. Sutton 和 A. G. Barto，*强化学习：导论*。MIT出版社，1998。'
- en: '[13] J. W. Ronald, “Simple statistical gradient-following algorithms for connectionist
    reinforcement learning,” *Machine Learning*, vol. 8, pp. 229–256, 1992.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. W. Ronald，“用于连接主义强化学习的简单统计梯度跟踪算法，”*机器学习*，第8卷，页229–256，1992。'
- en: '[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, and G. Ostrovski, “Human-level control
    through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p. 529, 2015.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland 和 G. Ostrovski, “通过深度强化学习实现人类水平的控制，”
    *自然*, vol. 518, no. 7540, p. 529, 2015。'
- en: '[15] v. H. Hado, G. Arthur, and S. David, “Deep reinforcement learning with
    double Q-learning,” in *AAAI Conference on Artificial Intelligence*, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] v. H. Hado, G. Arthur 和 S. David, “带有双重 Q 学习的深度强化学习，” 发表在 *AAAI 人工智能会议*，2016
    年。'
- en: '[16] S. Tom, Q. John, A. Ioannis, and S. David, “Prioritized experience replay,”
    in *International Conference on Learning Representations*, 2016.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Tom, Q. John, A. Ioannis 和 S. David, “优先经验回放，” 发表在 *学习表征国际会议*，2016
    年。'
- en: '[17] W. Ziyu, S. Tom, H. Matteo, v. H. Hado, L. Marc, and d. F. Nando, “Dueling
    network architectures for deep reinforcement learning,” in *International Conference
    on Machine Learning*, 2016.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. Ziyu, S. Tom, H. Matteo, v. H. Hado, L. Marc 和 d. F. Nando, “用于深度强化学习的对抗网络架构，”
    发表在 *国际机器学习会议*，2016 年。'
- en: '[18] P. v. H. Hado, G. Arthur, H. Matteo, M. Volodymyr, and S. David, “Learning
    values across many orders of magnitude,” in *Advances in Neural Information Processing
    Systems*, 2016.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. v. H. Hado, G. Arthur, H. Matteo, M. Volodymyr 和 S. David, “跨越多个数量级学习值，”
    发表在 *神经信息处理系统进展*，2016 年。'
- en: '[19] S. H. Frank, L. Yang, G. S. Alexander, and P. Jian, “Learning to play
    in a day: faster deep reinforcement learning by optimality tightening,” in *International
    Conference on Learning Representations*, 2017.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. H. Frank, L. Yang, G. S. Alexander 和 P. Jian, “一天内学习游戏：通过优化紧缩加速深度强化学习，”
    发表在 *学习表征国际会议*，2017 年。'
- en: '[20] “Massively parallel methods for deep reinforcement learning,” in *International
    Conference on Machine Learning Workshop on Deep Learning*, 2015.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] “用于深度强化学习的大规模并行方法，” 发表在 *国际机器学习会议深度学习研讨会*，2015 年。'
- en: '[21] J. H. Matthew and S. Peter, “Deep recurrent Q-learning for partially observable
    MDPs,” *CoRR*, vol. abs/1507.06527, 2015.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. H. Matthew 和 S. Peter, “用于部分可观测 MDP 的深度递归 Q 学习，” *CoRR*, vol. abs/1507.06527,
    2015。'
- en: '[22] L. Nir, Z. Tom, J. M. Daniel, T. Aviv, and M. Shie, “Shallow updates for
    deep reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2017.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] L. Nir, Z. Tom, J. M. Daniel, T. Aviv 和 M. Shie, “深度强化学习的浅层更新，” 发表在 *神经信息处理系统进展*，2017
    年。'
- en: '[23] A. Oron, B. Nir, and S. Nahum, “Averaged-DQN: variance reduction and stabilization
    for deep reinforcement learning,” in *International Conference on Machine Learning*,
    2017.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Oron, B. Nir 和 S. Nahum, “Averaged-DQN：深度强化学习中的方差减少和稳定化，” 发表在 *国际机器学习会议*，2017
    年。'
- en: '[24] “Deep Q-learning from demonstrations,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] “从示例中进行深度 Q 学习，” 发表在 *AAAI 人工智能会议*，2018 年。'
- en: '[25] M. Sabatelli, G. Louppe, P. Geurts, and M. Wiering, “Deep quality-value
    (dqv) learning.” *abs/1810.00368*, 2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] M. Sabatelli, G. Louppe, P. Geurts 和 M. Wiering, “深度质量价值（dqv）学习。” *abs/1810.00368*，2018
    年。'
- en: '[26] “Rainbow: combining improvements in deep reinforcement learning,” in *AAAI
    Conference on Artificial Intelligence*, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] “Rainbow：结合深度强化学习中的改进，” 发表在 *AAAI 人工智能会议*，2018 年。'
- en: '[27] A. A.-M. Jose, G. Michael, W. Michael, U. Thomas, and H. Sepp, “RUDDER:
    return decomposition for delayed rewards,” *CoRR*, vol. abs/1806.07857, 2018.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. A.-M. Jose, G. Michael, W. Michael, U. Thomas 和 H. Sepp, “RUDDER：延迟奖励的回报分解，”
    *CoRR*, vol. abs/1806.07857, 2018。'
- en: '[28] “Observe and look further: achieving consistent performance on Atari,”
    *CoRR*, vol. abs/1805.11593, 2018.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] “观察并进一步探索：在 Atari 上实现一致性能，” *CoRR*, vol. abs/1805.11593, 2018。'
- en: '[29] S. John, A. Pieter, and C. Xi, “Equivalence between policy gradients and
    soft Q-learning,” *CoRR*, vol. abs/1704.06440, 2017.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. John, A. Pieter 和 C. Xi, “策略梯度与软 Q 学习之间的等价性，” *CoRR*, vol. abs/1704.06440,
    2017。'
- en: '[30] G. B. Marc, D. Will, and M. Remi, “A distributional perspective on reinforcement
    learning,” in *International Conference on Machine Learning*, 2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] G. B. Marc, D. Will 和 M. Remi, “对强化学习的分布式视角，” 发表在 *国际机器学习会议*，2017 年。'
- en: '[31] D. Will, R. Mark, G. B. Marc, and M. Rémi, “Distributional reinforcement
    learning with quantile regression,” in *AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Will, R. Mark, G. B. Marc 和 M. Rémi, “带有分位数回归的分布式强化学习，” 发表在 *AAAI 人工智能会议*，2018
    年。'
- en: '[32] D. Will, O. Georg, S. David, and M. Rémi, “Implicit quantile networks
    for distributional reinforcement learning,” *CoRR*, vol. abs/1806.06923, 2018.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Will, O. Georg, S. David 和 M. Rémi, “用于分布式强化学习的隐式分位数网络，” *CoRR*, vol.
    abs/1806.06923, 2018。'
- en: '[33] “Asynchronous methods for deep reinforcement learning,” in *International
    Conference on Machine Learnin*, 2016.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] “深度强化学习的异步方法，” 在 *国际机器学习会议*，2016年。'
- en: '[34] B. Mohammad, F. Iuri, T. Stephen, C. Jason, and K. Jan, “Reinforcement
    learning through asynchronous advantage actor-critic on a GPU,” in *International
    Conference on Learning Representations*, 2017.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] B. Mohammad, F. Iuri, T. Stephen, C. Jason 和 K. Jan, “通过异步优势演员-评论家在GPU上进行强化学习，”
    在 *国际学习表示会议*，2017年。'
- en: '[35] J. Max, M. Volodymyr, C. Wojciech, S. Tom, Z. L. Joel, S. David, and K. Koray,
    “Reinforcement learning with unsupervised auxiliary tasks,” in *International
    Conference on Learning Representations*, 2017.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Max, M. Volodymyr, C. Wojciech, S. Tom, Z. L. Joel, S. David 和 K. Koray,
    “具有无监督辅助任务的强化学习，” 在 *国际学习表示会议*，2017年。'
- en: '[36] “Efficient parallel methods for deep reinforcement learning,” *CoRR*,
    vol. abs/1705.04862, 2017.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] “深度强化学习的高效并行方法，” *CoRR*，卷 abs/1705.04862，2017年。'
- en: '[37] O. Brendan, M. Rémi, K. Koray, and M. Volodymyr, “PGQ: combining policy
    gradient and Q-learning,” in *International Conference on Learning Representations*,
    2017.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] O. Brendan, M. Rémi, K. Koray 和 M. Volodymyr, “PGQ：结合策略梯度和Q学习，” 在 *国际学习表示会议*，2017年。'
- en: '[38] M. Rémi, S. Tom, H. Anna, and G. B. Marc, “Safe and efficient off-policy
    reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2016.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] M. Rémi, S. Tom, H. Anna 和 G. B. Marc, “安全且高效的离策略强化学习，” 在 *神经信息处理系统进展*，2016年。'
- en: '[39] G. Audrunas, G. A. Mohammad, G. B. Marc, and R. Munos, “The Reactor: a
    sample-efficient actor-critic architecture,” *CoRR*, vol. abs/1704.04651, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] G. Audrunas, G. A. Mohammad, G. B. Marc 和 R. Munos, “反应器：一种样本高效的演员-评论家架构，”
    *CoRR*，卷 abs/1704.04651，2017年。'
- en: '[40] “IMPALA: scalable distributed deep-RL with importance weighted actor-learner
    architectures,” *CoRR*, vol. abs/1802.01561, 2018.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] “IMPALA：具有重要性加权演员-学习者架构的可扩展分布式深度强化学习，” *CoRR*，卷 abs/1802.01561，2018年。'
- en: '[41] S. John, L. Sergey, A. Pieter, I. J. Michael, and M. Philipp, “Trust region
    policy optimization,” in *International Conference on Machine Learning*, 2015.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. John, L. Sergey, A. Pieter, I. J. Michael 和 M. Philipp, “信任区域策略优化，”
    在 *国际机器学习会议*，2015年。'
- en: '[42] S. John, W. Filip, D. Prafulla, R. Alec, and K. Oleg, “Proximal policy
    optimization algorithms,” *CoRR*, vol. abs/1707.06347, 2017.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. John, W. Filip, D. Prafulla, R. Alec 和 K. Oleg, “近端策略优化算法，” *CoRR*，卷
    abs/1707.06347，2017年。'
- en: '[43] W. Ziyu, B. Victor, H. Nicolas, M. Volodymyr, M. Rémi, K. Koray, and d. F.
    Nando, “Sample efficient actor-critic with experience replay,” in *International
    Conference on Learning Representations*, 2017.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] W. Ziyu, B. Victor, H. Nicolas, M. Volodymyr, M. Rémi, K. Koray 和 d. F.
    Nando, “具有经验重放的样本高效演员-评论家，” 在 *国际学习表示会议*，2017年。'
- en: '[44] W. Yuhuai, M. Elman, L. Shun, B. G. Roger, and B. Jimmy, “Scalable trust-region
    method for deep reinforcement learning using Kronecker-factored approximation,”
    in *Advances in Neural Information Processing Systems*, 2017.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] W. Yuhuai, M. Elman, L. Shun, B. G. Roger 和 B. Jimmy, “使用 Kronecker 近似的可扩展信任区域方法用于深度强化学习，”
    在 *神经信息处理系统进展*，2017年。'
- en: '[45] P. L. Timothy, J. H. Jonathan, P. Alexander, H. Nicolas, E. Tom, T. Yuval,
    S. David, and W. Daan, “Continuous control with deep reinforcement learning,”
    *CoRR*, vol. abs/1509.02971, 2015.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] P. L. Timothy, J. H. Jonathan, P. Alexander, H. Nicolas, E. Tom, T. Yuval,
    S. David 和 W. Daan, “深度强化学习中的连续控制，” *CoRR*，卷 abs/1509.02971，2015年。'
- en: '[46] “Distributed distributional deterministic policy gradients,” *CoRR*, vol.
    abs/1804.08617, 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] “分布式分布确定性策略梯度，” *CoRR*，卷 abs/1804.08617，2018年。'
- en: '[47] F. Gregory, R. Tim, I. Maximilian, and W. Shimon, “TreeQN and ATreeC:
    differentiable tree planning for deep reinforcement learning,” in *International
    Conference on Learning Representations*, 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] F. Gregory, R. Tim, I. Maximilian 和 W. Shimon, “TreeQN 和 ATreeC：用于深度强化学习的可微分树规划，”
    在 *国际学习表示会议*，2018年。'
- en: '[48] V. Alexander, M. Volodymyr, O. Simon, G. Alex, V. Oriol, A. John, and
    K. Koray, “Strategic attentive writer for learning macro-actions,” in *Advances
    in Neural Information Processing Systems*, 2016.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] V. Alexander, M. Volodymyr, O. Simon, G. Alex, V. Oriol, A. John 和 K.
    Koray, “用于学习宏动作的战略注意编写器，” 在 *神经信息处理系统进展*，2016年。'
- en: '[49] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy evolution,”
    *Neural Information Processing Systems*, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] D. Ha 和 J. Schmidhuber, “递归世界模型促进策略演化，” *神经信息处理系统*，2018年。'
- en: '[50] N. Nantas, S. Gabriel, L. Zeming, K. Pushmeet, H. S. T. Philip, and U. Nicolas,
    “Value propagation networks,” *CoRR*, vol. abs/1805.11199, 2018.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Nantas, S. Gabriel, L. Zeming, K. Pushmeet, H. S. T. Philip 和 U. Nicolas,
    “价值传播网络，” *CoRR*，卷 abs/1805.11199，2018年。'
- en: '[51] O. Ian, B. Charles, P. Alexander, and V. R. Benjamin, “Deep exploration
    via bootstrapped DQN,” in *Advances in Neural Information Processing Systems*,
    2016.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] O. Ian, B. Charles, P. Alexander 和 V. R. Benjamin，“通过引导 DQN 的深度探索”，发表于*神经信息处理系统进展*，2016年。'
- en: '[52] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor,” *international
    conference on machine learning*, pp. 1856–1865, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] T. Haarnoja, A. Zhou, P. Abbeel 和 S. Levine，“软演员-评论家：具有随机演员的离线最大熵深度强化学习”，*国际机器学习会议*，页
    1856–1865，2018年。'
- en: '[53] S. Sahil, S. L. Aravind, and R. Balaraman, “Learning to repeat: fine grained
    action repetition for deep reinforcement learning,” in *International Conference
    on Learning Representations*, 2017.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. Sahil, S. L. Aravind 和 R. Balaraman，“学习重复：深度强化学习的精细化动作重复”，发表于*国际学习表征会议*，2017年。'
- en: '[54] S. Julian, A. Ioannis, and H. Thomas, “Mastering atari, go, chess and
    shogi by planning with a learned model,” *abs/1911.08265*, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] S. Julian, A. Ioannis 和 H. Thomas，“通过规划与学习模型掌握 Atari、围棋、国际象棋和将棋”，*abs/1911.08265*，2019年。'
- en: '[55] G. B. Marc, N. Yavar, V. Joel, and H. B. Michael, “The Arcade learning
    environment: An evaluation platform for general agents,” *J. Artif. Intell. Res.*,
    vol. 47, pp. 253–279, 2013.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] G. B. Marc, N. Yavar, V. Joel 和 H. B. Michael，“街机学习环境：通用智能体的评估平台”，*人工智能研究期刊*，卷
    47，页 253–279，2013年。'
- en: '[56] B. Greg, C. Vicki, P. Ludwig, S. Jonas, S. John, T. Jie, and Z. Wojciech,
    “OpenAI Gym,” *CoRR*, vol. abs/1606.01540, 2016.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] B. Greg, C. Vicki, P. Ludwig, S. Jonas, S. John, T. Jie 和 Z. Wojciech，“OpenAI
    Gym”，*CoRR*，卷 abs/1606.01540，2016年。'
- en: '[57] “OpenAI Universe github,” https://github.com/openai/universe, 2016.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] “OpenAI Universe github”，https://github.com/openai/universe，2016年。'
- en: '[58] “OpenAI Retro github,” https://github.com/openai/retro, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] “OpenAI Retro github”，https://github.com/openai/retro，2018年。'
- en: '[59] N. Alex, P. Vicki, H. Christopher, K. Oleg, and S. John, “Gotta learn
    fast: a new benchmark for generalization in RL,” *CoRR*, vol. abs/1804.03720,
    2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Alex, P. Vicki, H. Christopher, K. Oleg 和 S. John，“必须快速学习：强化学习中的新基准”，*CoRR*，卷
    abs/1804.03720，2018年。'
- en: '[60] “General video game AI: a multi-track framework for evaluating agents,
    games and content generation algorithms,” *CoRR*, vol. abs/1802.10363, 2018.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] “通用视频游戏 AI：评估智能体、游戏和内容生成算法的多轨框架”，*CoRR*，卷 abs/1802.10363，2018年。'
- en: '[61] R. T. Ruben, B. Philip, T. Julian, L. Jialin, and P.-L. Diego, “Deep reinforcement
    learning for general video game AI,” *CoRR*, vol. abs/1806.02448, 2018.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. T. Ruben, B. Philip, T. Julian, L. Jialin 和 P.-L. Diego，“通用视频游戏 AI
    的深度强化学习”，*CoRR*，卷 abs/1806.02448，2018年。'
- en: '[62] “DeepMind Lab,” *CoRR*, vol. abs/1612.03801, 2016.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] “DeepMind Lab”，*CoRR*，卷 abs/1612.03801，2016年。'
- en: '[63] J. Arthur, B. Vincent-Pierre, V. Esh, G. Yuan, H. Hunter, M. Marwan, and
    L. Danny, “Unity: a general platform for intelligent agentst,” *CoRR*, vol. abs/1809.02627,
    2018.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] J. Arthur, B. Vincent-Pierre, V. Esh, G. Yuan, H. Hunter, M. Marwan 和
    L. Danny，“Unity：智能体的通用平台”，*CoRR*，卷 abs/1809.02627，2018年。'
- en: '[64] “OpenAI Malmo github,” https://github.com/Microsoft/malmo, 2017.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] “OpenAI Malmo github”，https://github.com/Microsoft/malmo，2017年。'
- en: '[65] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner,
    “Torcs, the open racing car simulator,” *Software available at http://torcs. sourceforge.
    net*, vol. 4, p. 6, 2000.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom 和 A. Sumner，“Torcs，开放赛车模拟器”，*软件可在
    http://torcs.sourceforge.net*，卷 4，页 6，2000年。'
- en: '[66] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jakowski, “ViZDoom:
    a Doom-based AI research platform for visual reinforcement learning,” in *IEEE
    Conference on Computational Intelligence and Games*, 2017, pp. 1–8.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] M. Kempka, M. Wydmuch, G. Runc, J. Toczek 和 W. Jakowski，“ViZDoom：基于 Doom
    的视觉强化学习研究平台”，发表于*IEEE 计算智能与游戏会议*，2017年，页 1–8。'
- en: '[67] S. Gabriel, N. Nantas, A. Alex, C. Soumith, L. Timothée, L. Zeming, R. Florian,
    and U. Nicolas, “TorchCraft: a library for machine learning research on real-time
    strategy games,” *CoRR*, vol. abs/1611.00625, 2016.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. Gabriel, N. Nantas, A. Alex, C. Soumith, L. Timothée, L. Zeming, R.
    Florian 和 U. Nicolas，“TorchCraft：实时策略游戏的机器学习研究库”，*CoRR*，卷 abs/1611.00625，2016年。'
- en: '[68] “StarCraft II: a new challenge for reinforcement learning,” *CoRR*, vol.
    abs/1708.04782, 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] “StarCraft II：强化学习的新挑战”，*CoRR*，卷 abs/1708.04782，2017年。'
- en: '[69] C. Karl, K. Oleg, H. Chris, K. Taehoon, and S. John, “Quantifying generalization
    in reinforcement learning,” *CoRR*, vol. abs/1812.02341, 2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] C. Karl, K. Oleg, H. Chris, K. Taehoon 和 S. John，“量化强化学习中的泛化”，*CoRR*，卷
    abs/1812.02341，2018年。'
- en: '[70] C. M. Marlos, G. B. Marc, T. Erik, V. Joel, J. H. Matthew, and B. Michael,
    “Revisiting the Arcade learning environment: evaluation protocols and open problems
    for general agents,” *Journal of Artificial Intelligence Research*, vol. 61, pp.
    523–562, 2018.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] C. M. Marlos, G. B. Marc, T. Erik, V. Joel, J. H. Matthew, 和 B. Michael，“重访街机学习环境：通用代理的评估协议和开放问题”，*人工智能研究期刊*，第61卷，第523–562页，2018年。'
- en: '[71] H. Dan, Q. John, B. David, B.-M. Gabriel, H. Matteo, v. H. Hado, and S. David,
    “Distributed prioritized experience replay,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H. Dan, Q. John, B. David, B.-M. Gabriel, H. Matteo, v. H. Hado, 和 S.
    David，“分布式优先经验回放”，发表于*国际学习表征会议*，2018年。'
- en: '[72] O. Georg, G. B. Marc, O. Aaron, and M. Remi, “Count-based exploration
    with neural density models,” in *International Conference on Machine Learning*,
    2017.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] O. Georg, G. B. Marc, O. Aaron, 和 M. Remi，“基于计数的探索与神经密度模型”，发表于*国际机器学习会议*，2017年。'
- en: '[73] M. L. Hoang, J. Nan, A. Alekh, D. Miroslav, Y. Yisong, and D. Hal, “Hierarchical
    imitation and reinforcement learning,” in *International Conference on Machine
    Learning*, 2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. L. Hoang, J. Nan, A. Alekh, D. Miroslav, Y. Yisong, 和 D. Hal，“层次模仿和强化学习”，发表于*国际机器学习会议*，2018年。'
- en: '[74] Y. Aytar, T. Pfaff, D. Budden, T. L. Paine, Z. Wang, and N. D. Freitas,
    “Playing hard exploration games by watching youtube,” 2018.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Aytar, T. Pfaff, D. Budden, T. L. Paine, Z. Wang, 和 N. D. Freitas，“通过观看
    YouTube 玩困难探索游戏”，2018年。'
- en: '[75] “Montezuma’s revenge solved by go-explore, a new algorithm for hard-exploration
    problems,” https://eng.uber.com/go-explore/, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] “Montezuma’s Revenge 通过 go-explore 解决，这是一个用于困难探索问题的新算法”，https://eng.uber.com/go-explore/，2018年。'
- en: '[76] Y. Wu and Y. Tian, “Training agent for first-person shooter game with
    actor-critic curriculum learning,” in *International Conference on Learning Representations*,
    2017.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Wu 和 Y. Tian，“使用演员-评论员课程学习训练第一人称射击游戏代理”，发表于*国际学习表征会议*，2017年。'
- en: '[77] P. Emilio and S. Ruslan, “Neural map: structured memory for deep reinforcement
    learning,” *CoRR*, vol. abs/1702.08360, 2017.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] P. Emilio 和 S. Ruslan，“神经地图：用于深度强化学习的结构化记忆”，*CoRR*，第 abs/1702.08360 号，2017年。'
- en: '[78] K. Shao, D. Zhao, N. Li, and Y. Zhu, “Learning battles in ViZDoom via
    deep reinforcement learning,” in *IEEE Conference on Computational Intelligence
    and Games*, 2018.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] K. Shao, D. Zhao, N. Li, 和 Y. Zhu，“通过深度强化学习在 ViZDoom 中进行战斗学习”，发表于*IEEE
    计算智能与游戏会议*，2018年。'
- en: '[79] G. Yang, X. Huazhe, L. Ji, Y. Fisher, L. Sergey, and D. Trevor, “Reinforcement
    learning from imperfect demonstrations,” *CoRR*, vol. abs/1802.05313, 2018.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] G. Yang, X. Huazhe, L. Ji, Y. Fisher, L. Sergey, 和 D. Trevor，“从不完美示范中进行强化学习”，*CoRR*，第
    abs/1802.05313 号，2018年。'
- en: '[80] M. Sahisnu, L. Bing, W. Shuai, Z. Yingxuan, L. Lifeng, and L. Jian, “Action
    permissibility in deep reinforcement learning and application to autonomous driving,”
    in *ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, 2018.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. Sahisnu, L. Bing, W. Shuai, Z. Yingxuan, L. Lifeng, 和 L. Jian，“深度强化学习中的行动许可性及其在自主驾驶中的应用”，发表于*ACM
    SIGKDD 知识发现与数据挖掘会议*，2018年。'
- en: '[81] D. Li, D. Zhao, Q. Zhang, and Y. Chen, “Reinforcement learning and deep
    learning based lateral control for autonomous driving,” *IEEE Computational Intelligence
    Magazine*, 2018.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] D. Li, D. Zhao, Q. Zhang, 和 Y. Chen，“基于强化学习和深度学习的自主驾驶横向控制”，*IEEE 计算智能杂志*，2018年。'
- en: '[82] Y. Zhu and D. Zhao, “Driving control with deep and reinforcement learning
    in the open racing car simulator,” in *International Conference on Neural Information
    Processing*, 2018.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Zhu 和 D. Zhao，“在开放赛车模拟器中使用深度和强化学习进行驾驶控制”，发表于*国际神经信息处理会议*，2018年。'
- en: '[83] J. Matthew, H. Katja, H. Tim, and B. David, “The Malmo platform for artificial
    intelligence experimentation,” in *International Joint Conferences on Artificial
    Intelligence*, 2016.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] J. Matthew, H. Katja, H. Tim, 和 B. David，“Malmo 平台用于人工智能实验”，发表于*国际联合人工智能会议*，2016年。'
- en: '[84] X. Yanhai, C. Haipeng, Z. Mengchen, and A. Bo, “HogRider: champion agent
    of Microsoft Malmo collaborative AI challenge,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] X. Yanhai, C. Haipeng, Z. Mengchen, 和 A. Bo，“HogRider: 微软 Malmo 协作 AI
    挑战的冠军代理”，发表于*AAAI 人工智能会议*，2018年。'
- en: '[85] T. Chen, G. Shahar, Z. Tom, J. M. Daniel, and M. Shie, “A deep hierarchical
    approach to lifelong learning in Minecraft,” in *AAAI Conference on Artificial
    Intelligence*, 2017.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] T. Chen, G. Shahar, Z. Tom, J. M. Daniel, 和 M. Shie，“在 Minecraft 中进行终身学习的深度层次方法”，发表于*AAAI
    人工智能会议*，2017年。'
- en: '[86] H. J. Peter, L. Sergey, and K. Kurt, “Regret minimization for partially
    observable deep reinforcement learning,” in *International Conference on Learning
    Representations*, 2018.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] “Unicorn: continual learning with a universal, off-policy agent,” *CoRR*,
    vol. abs/1802.08294, 2018.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] “Kickstarting deep reinforcement learning,” *CoRR*, vol. abs/1803.03835,
    2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] “Human-level performance in first-person multiplayer games with population-based
    deep reinforcement learning,” *CoRR*, vol. abs/1807.01281, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Z. Tang, K. Shao, Y. Zhu, D. Li, D. Zhao, and T. Huang, “A review of computational
    intelligence for StarCraft AI,” in *IEEE Symposium Series on Computational Intelligence
    (SSCI)*, 2018.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] N. Usunier, G. Synnaeve, Z. Lin, and S. Chintala, “Episodic exploration
    for deep deterministic policies: an application to StarCraft micromanagement tasks,”
    in *International Conference on Learning Representations*, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] P. Peng, Q. Yuan, Y. Wen, Y. Yang, Z. Tang, H. Long, and J. Wang, “Multiagent
    bidirectionally-coordinated nets for learning to play StarCraft combat games,”
    2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr, P. Kohli,
    and S. Whiteson, “Stabilising experience replay for deep multi-agent reinforcement
    learning,” in *International Conference on Machine Learning*, 2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] K. Shao, Y. Zhu, and D. Zhao, “StarCraft micromanagement with reinforcement
    learning and curriculum transfer learning,” *IEEE Transactions on Emerging Topics
    in Computational Intelligence*, DOI:10.1109/TETCI.2018.2823329, 2018.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] K. Xiangyu, X. Bo, L. Fangchen, and W. Yizhou, “Revisiting the master-slave
    architecture in multi-agent deep reinforcement learning,” *CoRR*, vol. abs/1712.07305,
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] R. Tabish, S. Mikayel, S. d. W. Christian, F. Gregory, N. F. Jakob, and
    W. Shimon, “QMIX: monotonic value function factorisation for deep multi-agent
    reinforcement learning,” *CoRR*, vol. abs/1803.11485, 2018.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] T. Zhentao, Z. Dongbin, Z. Yuanheng, and G. Ping, “Reinforcement learning
    for build-order production in StarCraft II,” in *International Conference on Information
    Science and Technology*, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls,
    D. Reichert, T. Lillicrap, E. Lockhart *et al.*, “Relational deep reinforcement
    learning,” *CoRR*, vol. abs/806.01830, 2018.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] P. Sun, X. Sun, L. Han, J. Xiong, Q. Wang, B. Li, Y. Zheng, J. Liu, Y. Liu,
    H. Liu, and T. Zhang, “TStarBots: defeating the cheating level builtin AI in StarCraft
    II in the full game,” *CoRR*, vol. abs/1809.07193, 2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] L. Dennis, T. Haoran, O. Z. Jeffrey, X. Huazhe, D. Trevor, and A. Pieter,
    “Modular architecture for starcraft ii with deep reinforcement learning,” *CoRR*,
    p. abs/1811.03555, 2018.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] P. Zhen-Jia, L. Ruo-Ze, M. Zhou-Yu, Z. Yi, Y. Yang, and L. Tong, “On
    reinforcement learning for full-length game of starcraft,” *CoRR*, p. abs/1809.09095,
    2018.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] R. J. Daniel, E. Emmanuel, and L. Hao, “Feedback-based tree search for
    reinforcement learning,” in *International Conference on Machine Learning*, 2018.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] “OpenAI Dota 1v1,” https://blog.openai.com/dota-2/, 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] “OpenAI Dota Five,” https://blog.openai.com/openai-five/, 2018.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Christopher, B. Greg, and C. Brooke, “Dota 2 with large scale deep
    reinforcement learning,” *abs/1912.06680*, 2019.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] “Noisy networks for exploration,” *CoRR*, vol. abs/1706.10295, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] “Parameter space noise for exploration,” *CoRR*, vol. abs/1706.01905,
    2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] T. Haoran, H. Rein, F. Davis, S. Adam, C. Xi, D. Yan, S. John, D. T.
    Filip, and A. Pieter, “Exploration: a study of count-based exploration for deep
    reinforcement learning,” in *Advances in Neural Information Processing Systems*,
    2017.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] B. Pierre-Luc, H. Jean, and P. Doina, “The option-critic architecture,”
    in *AAAI Conference on Artificial Intelligence*, 2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. V. Alexander, O. Simon, S. Tom, H. Nicolas, J. Max, S. David, and
    K. Koray, “FeUdal networks for hierarchical reinforcement learning,” in *International
    Conference on Machine Learning*, 2017.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. M. Marlos, R. Clemens, G. Xiaoxiao, L. Miao, T. Gerald, and C. Murray,
    “Eigenoption discovery through the deep successor representation,” *CoRR*, vol.
    abs/1710.11089, 2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] F. C. Paul, L. Jan, B. B. Tom, M. Miljan, L. Shane, and A. Dario, “Deep
    reinforcement learning from human preferences,” in *Advances in Neural Information
    Processing Systems*, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] v. S. Harm, F. Mehdi, L. Romain, R. Joshua, B. Tavian, and T. Jeffrey,
    “Hybrid reward architecture for reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Matteo, S. Hubert, E. Lasse, C. Wojciech, S. Simon, and v. H. Hado,
    “Multi-task deep reinforcement learning with popart,” *CoRR*, vol. abs/1809.04474,
    2018.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] “Distral: robust multitask reinforcement learning,” in *Advances in Neural
    Information Processing Systems*, 2017.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] “Mix& Match - agent curricula for reinforcement learning,” *CoRR*, vol.
    abs/1806.01780, 2018.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] L. Ryan, W. Yi, T. Aviv, H. Jean, A. Pieter, and M. Igor, “Multi-agent
    actor-critic for mixed cooperative-competitive environments,” 2017, pp. 6382–6393.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual
    multi-agent policy gradients,” in *AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] B. Charles, U. Benigno, P. Alexander, L. Yazhe, R. Avraham, Z. L. Joel,
    W. R. Jack, W. Daan, and H. Demis, “Model-free episodic control,” *CoRR*, vol.
    abs/1606.04460, 2016.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] “Hybrid computing using a neural network with dynamic external memory,”
    *Nature*, vol. 538, pp. 471–476, 2016.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] “Neural episodic control,” in *International Conference on Machine Learning*,
    2017.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Deepak, A. Pulkit, A. E. Alexei, and D. Trevor, “Curiosity-driven
    exploration by self-supervised prediction,” in *IEEE Conference on Computer Vision
    and Pattern Recognition Workshops*, 2017, pp. 488–489.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Deepak, A. Pulkit, A. E. Alexei, 和 D. Trevor， “基于自监督预测的好奇心驱动探索”，发表于*IEEE计算机视觉与模式识别会议研讨会*，2017年，第488–489页。'
- en: '[123] S. Christopher and C. Jeff, “Deep curiosity search: intra-life exploration
    improves performance on challenging deep reinforcement learning problems,” *CoRR*,
    vol. abs/1806.00553, 2018.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Christopher 和 C. Jeff，“深度好奇搜索：生命周期内的探索提高了在挑战性深度强化学习问题上的表现”，*CoRR*，第abs/1806.00553卷，2018年。'
