- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:07:12'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1810.07862] Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1810.07862](https://ar5iv.labs.arxiv.org/html/1810.07862)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nguyen Cong Luong, Dinh Thai Hoang, Member, IEEE, Shimin Gong, Member, IEEE,
    Dusit Niyato, Fellow, IEEE, Ping Wang, Senior Member, IEEE, Ying-Chang Liang,
    Fellow, IEEE, Dong In Kim, Senior Member, IEEE N. C. Luong and D. Niyato are with
    School of Computer Science and Engineering, Nanyang Technological University,
    Singapore. E-mails: clnguyen@ntu.edu.sg, dniyato@ntu.edu.sg.D. T. Hoang is with
    the Faculty of Engineering and Information Technology, University of Technology
    Sydney, Australia. E-mail: hoang.dinh@uts.edu.au.S. Gong is with the Shenzhen
    Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055,
    China. E-mail: sm.gong@siat.ac.cn.P. Wang is with Department of Electrical Engineering
    & Computer Science, York University, Canada. E-mail: pingw@yorku.ca.Y.-C. Liang
    is with Center for Intelligent Networking and Communications (CINC), with University
    of Electronic Science and Technology of China, Chengdu, China. E-mail: liangyc@ieee.org.D. I. Kim
    is with School of Information and Communication Engineering, Sungkyunkwan University,
    Korea. Email: dikim@skku.ac.kr.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper presents a comprehensive literature review on applications of deep
    reinforcement learning in communications and networking. Modern networks, e.g.,
    Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more
    decentralized and autonomous. In such networks, network entities need to make
    decisions locally to maximize the network performance under uncertainty of network
    environment. Reinforcement learning has been efficiently used to enable the network
    entities to obtain the optimal policy including, e.g., decisions or actions, given
    their states when the state and action spaces are small. However, in complex and
    large-scale networks, the state and action spaces are usually large, and the reinforcement
    learning may not be able to find the optimal policy in reasonable time. Therefore,
    deep reinforcement learning, a combination of reinforcement learning with deep
    learning, has been developed to overcome the shortcomings. In this survey, we
    first give a tutorial of deep reinforcement learning from fundamental concepts
    to advanced models. Then, we review deep reinforcement learning approaches proposed
    to address emerging issues in communications and networking. The issues include
    dynamic network access, data rate control, wireless caching, data offloading,
    network security, and connectivity preservation which are all important to next
    generation networks such as 5G and beyond. Furthermore, we present applications
    of deep reinforcement learning for traffic routing, resource sharing, and data
    collection. Finally, we highlight important challenges, open issues, and future
    research directions of applying deep reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Keywords- Deep reinforcement learning, deep Q-learning, networking, communications,
    spectrum access, rate control, security, caching, data offloading, data collection.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning [[1](#bib.bib1)] is one of the most important research
    directions of machine learning which has significant impacts to the development
    of Artificial Intelligence (AI) over the last 20 years. Reinforcement learning
    is a learning process in which an agent can periodically make decisions, observe
    the results, and then automatically adjust its strategy to achieve the optimal
    policy. However, this learning process, even though proved to converge, takes
    a lot of time to reach the best policy as it has to explore and gain knowledge
    of an entire system, making it unsuitable and inapplicable to large-scale networks.
    Consequently, applications of reinforcement learning are very limited in practice.
    Recently, deep learning [[2](#bib.bib2)] has been introduced as a new breakthrough
    technique. It can overcome the limitations of reinforcement learning, and thus
    open a new era for the development of reinforcement learning, namely *Deep Reinforcement
    Learning* (DRL). The DRL embraces the advantage of Deep Neural Networks (DNNs)
    to train the learning process, thereby improving the learning speed and the performance
    of reinforcement learning algorithms. As a result, DRL has been adopted in a numerous
    applications of reinforcement learning in practice such as robotics, computer
    vision, speech recognition, and natural language processing [[2](#bib.bib2)].
    One of the most famous applications of DRL is AlphaGo [[3](#bib.bib3)], the first
    computer program which can beat a human professional without handicaps on a full-sized
    19$\times$19 board.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the areas of communications and networking, DRL has been recently used as
    an emerging tool to effectively address various problems and challenges. In particular,
    modern networks such as Internet of Things (IoT), Heterogeneous Networks (HetNets),
    and Unmanned Aerial Vehicle (UAV) network become more decentralized, ad-hoc, and
    autonomous in nature. Network entities such as IoT devices, mobile users, and
    UAVs need to make local and autonomous decisions, e.g., spectrum access, data
    rate selection, transmit power control, and base station association, to achieve
    the goals of different networks including, e.g., throughput maximization and energy
    consumption minimization. Under uncertain and stochastic environments, most of
    the decision-making problems can be modeled by a so-called Markov Decision Process
    (MDP) [[4](#bib.bib4)]. Dynamic programming [[5](#bib.bib5)], [[6](#bib.bib6)]
    and other algorithms such as value iteration, as well as reinforcement learning
    techniques can be adopted to solve the MDP. However, the modern networks are large-scale
    and complicated, and thus the computational complexity of the techniques rapidly
    becomes unmanageable. As a result, DRL has been developing to be an alternative
    solution to overcome the challenge. In general, the DRL approaches provide the
    following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL can obtain the solution of sophisticated network optimizations. Thus, it
    enables network controllers, e.g., base stations, in modern networks to solve
    non-convex and complex problems, e.g., joint user association, computation, and
    transmission schedule, to achieve the optimal solutions without complete and accurate
    network information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL allows network entities to learn and build knowledge about the communication
    and networking environment. Thus, by using DRL, the network entities, e.g., a
    mobile user, can learn optimal policies, e.g., base station selection, channel
    selection, handover decision, caching and offloading decisions, without knowing
    channel model and mobility pattern.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL provides autonomous decision-making. With the DRL approaches, network entities
    can make observation and obtain the best policy locally with minimum or without
    information exchange among each other. This not only reduces communication overheads
    but also improves security and robustness of the networks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DRL improves significantly the learning speed, especially in the problems with
    large state and action spaces. Thus, in large-scale networks, e.g., IoT systems
    with thousands of devices, DRL allows network controller or IoT gateways to control
    dynamically user association, spectrum access, and transmit power for a massive
    number of IoT devices and mobile users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several other problems in communications and networking such as cyber-physical
    attacks, interference management, and data offloading can be modeled as games,
    e.g., the non-cooperative game. DRL has been recently used as an efficient tool
    to solve the games, e.g., finding the Nash equilibrium, without the complete information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Although there are some surveys related to DRL, they do not focus on communications
    and networking. For example, the surveys of applications of DRL for computer vision
    and natural language processing can be found in [[7](#bib.bib7)] and[[8](#bib.bib8)].
    Also, there are surveys related to the use of only “deep learning” for networking.
    For example, the survey of machine learning for wireless networks is given in
    [[9](#bib.bib9)], but it does not focus on the DRL approaches. To the best of
    our knowledge, there is no survey specifically discussing the applications of
    DRL in communications and networking. This motivates us to deliver the survey
    with the tutorial of DRL and the comprehensive literature review on the applications
    of DRL to address issues in communications and networking. For convenience, the
    related works in this survey are classified based on issues in communications
    and networking as shown in Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey"). The
    major issues include network access, data rate control, wireless caching, data
    offloading, network security, connectivity preservation, traffic routing, and
    data collection. Also, the percentages of DRL related works for different networks
    and different issues in the networks are shown in Figs. [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(a) and [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Applications
    of Deep Reinforcement Learning in Communications and Networking: A Survey")(b),
    respectively. From the figures, we observe that the majority of the related works
    are for the cellular networks. Also, the related works to the wireless caching
    and offloading have received more attention than the other issues.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d251dbc4d76c342dc33ff3f912b9a01f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdaef3e72e3b30684c37d74841d9a0a5.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Percentages of related work for (a) different networks and (b) different
    issues in the networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: List of abbreviations'
  prefs: []
  type: TYPE_NORMAL
- en: '| Abbreviation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| ANN/APF | Artificial Neural Network/Artificial Potential Field |'
  prefs: []
  type: TYPE_TB
- en: '| A3C | Asynchronous Advantage Actor- Critic |'
  prefs: []
  type: TYPE_TB
- en: '| CRN | Cognitive Radio Network |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | Convolutional Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DRL/DQL | Deep Reinforcement Learning/Deep Q-Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DQN/DDQN/DRQN | Deep Q-Network/Double DQN/Deep Recurrent Q-Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DASH | Dynamic Adaptive Streaming over HTTP |'
  prefs: []
  type: TYPE_TB
- en: '| DoS | Denial-of-Service |'
  prefs: []
  type: TYPE_TB
- en: '| ESN | Echo State Network |'
  prefs: []
  type: TYPE_TB
- en: '| FNN/RNN | Feedforward Neural Network/Recurrent Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| FSMC | Finite-State Markov Channel |'
  prefs: []
  type: TYPE_TB
- en: '| HVFT | High Volume Flexible Time |'
  prefs: []
  type: TYPE_TB
- en: '| ITS | Intelligent Transportation System |'
  prefs: []
  type: TYPE_TB
- en: '| LSM/LSTM | Liquid State Machine/Long Short-Term Memory |'
  prefs: []
  type: TYPE_TB
- en: '| MEC | Mobile Edge Computing |'
  prefs: []
  type: TYPE_TB
- en: '| MDP/POMDP | Markov Decision Process/Partially Observable MDP |'
  prefs: []
  type: TYPE_TB
- en: '| NFSP | Neural Fictitious Self-Play |'
  prefs: []
  type: TYPE_TB
- en: '| NFV | Network Function Virtualization |'
  prefs: []
  type: TYPE_TB
- en: '| RDPG | Recurrent Deterministic Policy Gradient |'
  prefs: []
  type: TYPE_TB
- en: '| RCNN | Recursive Convolutional Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| RRH/BBU | Remote Radio Head/BaseBand Unit |'
  prefs: []
  type: TYPE_TB
- en: '| RSSI | Received Signal Strength Indicators |'
  prefs: []
  type: TYPE_TB
- en: '| SPD | Sequential Prisoner’s Dilemma |'
  prefs: []
  type: TYPE_TB
- en: '| SBS/BS | Small Base Station/Base Station |'
  prefs: []
  type: TYPE_TB
- en: '| SDN | Software-Defined Network |'
  prefs: []
  type: TYPE_TB
- en: '| SU/PU | Secondary User/Primary User |'
  prefs: []
  type: TYPE_TB
- en: '| UDN/UAN | Ultra-Density Network/Underwater Acoustic Network |'
  prefs: []
  type: TYPE_TB
- en: '| UAV | Unmanned Aerial Vehicle |'
  prefs: []
  type: TYPE_TB
- en: '| VANET/V2V | Vehicular Ad hoc Network/Vehicle-to-Vehicle |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/feb9df8e689a1c909c8df987c4aee146.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A taxonomy of the applications of deep reinforcement learning for
    communications and networking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows. Section [II](#S2 "II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") presents the introduction of reinforcement learning
    and discusses DRL techniques as well as their extensions. Section [III](#S3 "III
    Network Access and Rate Control ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey") reviews the applications of DRL for
    dynamic network access and adaptive data rate control. Section [IV](#S4 "IV Caching
    and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") discusses the applications of DRL for wireless caching
    and data offloading. Section [V](#S5 "V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey") presents DRL related works for network security and connectivity preservation.
    Section [VI](#S6 "VI Miscellaneous Issues ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") considers how to use DRL
    to deal with other issues in communications and networking. Important challenges,
    open issues, and future research directions are outlined in Section [VII](#S7
    "VII Challenges, Open Issues, and Future Research Directions ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey"). Section [VIII](#S8
    "VIII Conclusions ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") concludes the paper. The list of abbreviations commonly
    appeared in this paper is given in Table [I](#S1.T1 "TABLE I ‣ I Introduction
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"). Note that DRL consists of two different algorithms which are Deep
    Q-Learning (DQL) and policy gradients [[10](#bib.bib10)]. In particular, DQL is
    mostly used for the DRL related works. Therefore, in the rest of the paper, we
    use “DRL” and “DQL” interchangeably to refer to the DRL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'II Deep Reinforcement Learning: An Overview'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first present fundamental knowledge of Markov decision processes,
    reinforcement learning, and deep learning techniques which are important branches
    of machine learning theory. We then discuss DRL technique that can capitalize
    on the capability of the deep learning to improve efficiency and performance in
    terms of the learning rate for reinforcement learning algorithms. Afterward, advanced
    DRL models and their extensions are reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Markov Decision Processes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MDP [[4](#bib.bib4)] is a discrete time stochastic control process. MDP provides
    a mathematical framework for modeling decision-making problems in which outcomes
    are partly random and under control of a decision maker or an agent. MDPs are
    useful for studying optimization problems which can be solved by dynamic programming
    and reinforcement learning techniques. Typically, an MDP is defined by a tuple
    $(\mathcal{S},\mathcal{A},p,r)$ where $\mathcal{S}$ is a finite set of states,
    $\mathcal{A}$ is a finite set of actions, $p$ is a transition probability from
    state $s$ to state $s^{\prime}$ after action $a$ is executed, and $r$ is the immediate
    reward obtained after action $a$ is performed. We denote $\pi$ as a “policy” which
    is a mapping from a state to an action. The goal of an MDP is to find an optimal
    policy to maximize the reward function. An MDP can be finite or infinite time
    horizon. For an infinite time horizon MDP, we aim to find an optimal policy $\pi^{*}$
    to maximize the expected total reward defined by $\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},a_{t})$, where $a_{t}=\pi^{*}(s_{t})$, and $\gamma\in[0,1]$ is the
    discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 Partially Observable Markov Decision Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In MDPs, we assume that the system state is fully observable by the agent. However,
    in many cases, the agent only can observe a part of the system state, and thus
    Partially Observable Markov Decision Processes (POMDPs) [[11](#bib.bib11)] can
    be used to model the decision-making problems. A typical POMDP model is defined
    by a 6-tuple $(\mathcal{S},\mathcal{A},p,r,\Omega,\mathcal{O})$, where $\mathcal{S},\mathcal{A},p,r$
    are defined the same as in the MDP model, $\Omega$ and $\mathcal{O}$ are defined
    as the set of observations and observation probabilities, respectively. At each
    time step, the system is at state $s\in\mathcal{S}$. Then, the agent takes an
    action $a\in\mathcal{A}$ and the system transits to state $s^{\prime}\in\mathcal{S}$.
    At the same time, the agent has a new observation $o\in\Omega$ with probability
    $\mathcal{O}(o|s,a,s^{\prime})$. Finally, the agent receives an immediate reward
    $r$ that is equal to $r(s,a)$ in the MDP. Similar to the MDP model, the agent
    in POMDP also aims to find the optimal policy $\pi^{*}$ in order to maximize its
    expected long-term discounted reward $\overset{\infty}{\underset{t=0}{\sum}}\gamma
    r_{t}(s_{t},\pi^{*}(s_{t}))$.
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Markov Games
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In game theory, a Markov game, or a stochastic game [[12](#bib.bib12)], is a
    dynamic game with probabilistic transitions played by multiple players, i.e.,
    agents. A typical Markov game model is defined by a tuple $(\mathcal{I},\mathcal{S},\{\mathcal{A}^{i}\}_{i\in\mathcal{I}},p,\{r^{i}\}_{i\in\mathcal{I}})$,
    where
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{I}\triangleq\{1,\ldots,i,\ldots,I\}$ is a set of agents,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{S}\triangleq\{\mathcal{S}^{1},\ldots,\mathcal{S}^{i},\ldots,\mathcal{S}^{I}\}$
    is the global state space of the all agents with $\mathcal{S}^{i}$ being the state
    space of agent $i$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\{\mathcal{A}^{i}\}_{i\in\mathcal{I}}$ are sets of action spaces of the agents
    with $\mathcal{A}^{i}$ being the action space of agent $i$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $p\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow[0,1]$
    is the transition probability function of the system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\{r^{i}\}_{i\in\mathcal{I}}$ are payoff functions of the agents with
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $r^{i}\triangleq\mathcal{S}\times\mathcal{A}^{1}\times\cdots\times\mathcal{A}^{I}\rightarrow\mathbb{R}$,
    i.e., the payoff of agent $i$ obtained after all actions of the agents are executed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a Markov game, the agents start at some initial state $s_{0}\in\mathcal{S}$.
    After observing the current state, all the agents simultaneously select their
    actions $a=\{a^{1},\ldots,a^{I}\}$ and they will receive their corresponding rewards
    together with their own new observations. At the same time, the system will transit
    to a new state $s^{\prime}\in\mathcal{S}$ with probability $p(s^{\prime}|s,a)$.
    The procedure is repeated at the new state and continues for a finite or infinite
    number of stages. In this game, all the agents try to find their optimal policies
    to maximize their own expected long-term average rewards, i.e., $\overset{\infty}{\underset{t=0}{\sum}}\gamma_{i}r^{i}_{t}(s_{t},\pi_{i}^{*}(s_{t}))$,
    $\forall i$. The set of all optimal policies of this game, i.e., $\{\pi^{*}_{1},\ldots,\pi^{*}_{I}\}$
    is known to be the equilibrium of this game. If there is a finite number of players
    and the sets of states and actions are finite, then the Markov game always has
    a Nash equilibrium [[13](#bib.bib13)] under a finite number of stages. The same
    is true for Markov games with infinite stages, but the total payoff of agents
    is the discounted sum [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: II-B Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reinforcement learning, an important branch of machine learning, is an effective
    tool and widely used in the literature to address MDPs [[1](#bib.bib1)]. In a
    reinforcement learning process, an agent can learn its optimal policy through
    interaction with its environment. In particular, the agent first observes its
    current state, and then takes an action, and receives its immediate reward together
    with its new state as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")(a). The observed
    information, i.e., the immediate reward and new state, is used to adjust the agent’s
    policy, and this process will be repeated until the agent’s policy approaches
    to the optimal policy. In reinforcement learning, $Q$-learning is the most effective
    method and widely used in the literature. In the following, we will discuss the
    $Q$-learning algorithm and its extensions for advanced MDP models.'
  prefs: []
  type: TYPE_NORMAL
- en: <math  class="ltx_Math" alttext="\begin{array}[]{ccc}\epsfbox{RL}\hfil&amp;\epsfbox{ANN}\hfil&amp;\epsfbox{DeepQLearning}\hfil\\
  prefs: []
  type: TYPE_NORMAL
- en: \text{(a)}&amp;\text{(b)}&amp;\text{(c)}\end{array}" display="inline"><semantics
    ><mtable columnspacing="5pt" rowspacing="0pt" 
    ><mtr  ><mtd
     ><mtext 
    >(a)</mtext></mtd><mtd 
    ><mtext  >(b)</mtext></mtd><mtd
     ><mtext 
    >(c)</mtext></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix 
    ><matrixrow  ><cerror
     ><csymbol cd="ambiguous" 
    >missing-subexpression</csymbol></cerror><cerror 
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror><cerror
     ><csymbol cd="ambiguous" 
    >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
     ><ci 
    ><mtext  >(a)</mtext></ci><ci
     ><mtext 
    >(b)</mtext></ci><ci 
    ><mtext  >(c)</mtext></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{ccc}\epsfbox{RL}\hfil&\epsfbox{ANN}\hfil&\epsfbox{DeepQLearning}\hfil\\
    \text{(a)}&\text{(b)}&\text{(c)}\end{array}</annotation></semantics></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: (a) Reinforcement learning, (b) Artificial neural network, and (c)
    Deep Q-learning.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 $Q$-Learning Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In an MDP, we aim to find an optimal policy $\pi^{*}:\mathcal{S}\rightarrow\mathcal{A}$
    for the agent to minimize the overall cost for the system. Accordingly, we first
    define value function $\mathcal{V}^{\pi}:\mathcal{S}\rightarrow\mathbb{R}$ that
    represents the expected value obtained by following policy $\pi$ from each state
    $s\in\mathcal{S}$. The value function $\mathcal{V}$ for policy $\pi$ quantifies
    the goodness of the policy through an infinite horizon and discounted MDP that
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{V}^{\pi}(s)$ | $\displaystyle=\mathbb{E}_{\pi}\Big{[}\sum_{t=0}^{\infty}\gamma
    r_{t}(s_{t},a_{t})&#124;s_{0}=s\Big{]}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\mathbb{E}_{\pi}\Big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})&#124;s_{0}=s\Big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Since we aim to find the optimal policy $\pi^{*}$, an optimal action at each
    state can be found through the optimal value function expressed by $\mathcal{V}^{*}(s)=\underset{a_{t}}{\max}\Big{\{}\mathbb{E}_{\pi}\big{[}r_{t}(s_{t},a_{t})+\gamma\mathcal{V}^{\pi}(s_{t+1})\big{]}\Big{\}}$
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'If we denote $\mathcal{Q}^{*}(s,a)\triangleq r_{t}(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}\big{[}\mathcal{V}^{\pi}(s_{t+1})\big{]}$
    as the optimal $Q$-function for all state-action pairs, then the optimal value
    function can be written by $\mathcal{V}^{*}(s)=\underset{a}{\max}\big{\{}\mathcal{Q}^{*}(s,a)\big{\}}$.
    Now, the problem is reduced to find optimal values of $Q$-function, i.e., $\mathcal{Q}^{*}(s,a)$,
    for all state-action pairs, and this can be done through iterative processes.
    In particular, the $Q$-function is updated according to the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)=$ | $\displaystyle\mathcal{Q}_{t}(s,a)+$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\alpha_{t}\Big{[}r_{t}(s,a)+\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s,a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The core idea behind this update is to find the Temporal Difference (TD) between
    the predicted $\mathcal{Q}$-value, i.e., $r_{t}(s,a)+\gamma\underset{a^{\prime}}{\max}\mathcal{Q}_{t}(s,a^{\prime})$
    and its current value, i.e., $\mathcal{Q}_{t}(s,a)$. In ([2](#S2.E2 "In II-B1
    𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey")), the learning rate $\alpha_{t}$ is used to determine the
    impact of new information to the existing $\mathcal{Q}$-value. The learning rate
    can be chosen to be a constant, or it can be adjusted dynamically during the learning
    process. However, it must satisfy Assumption [1](#Thmassumption1 "Assumption 1\.
    ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") to guarantee the convergence for the $Q$-learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Assumption 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The step size $\alpha_{t}$ is deterministic, nonnegative and satisfies the
    following conditions: $\alpha_{t}\in[0,1]$, $\overset{\infty}{\underset{t=0}{\sum}}\alpha_{t}=\infty$,
    and $\phantom{5}\overset{\infty}{\underset{t=0}{\sum}}(\alpha_{t})^{2}<\infty$
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The step size adaptation $\alpha_{t}=\frac{1}{t}$ is one of the most common
    examples used in reinforcement learning. More discussions for selecting an appropriate
    step size can be found in [[14](#bib.bib14)]. The details of the $Q$-learning
    algorithm are then provided in Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning
    Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 The $Q$-learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: For each state-action pair $(s,a)$, initialize the table entry $\mathcal{Q}(s,a)$
    arbitrarily, e.g., to zero. Observe the current state $s$, initialize a value
    for the learning rate $\alpha$ and the discount factor $\gamma$.  for $t:=1$ to
    $T$ do     From the current state-action pair $(s,a)$, execute action $a$ and
    obtain the immediate reward $r$ and a new state $s^{\prime}$.     Select an action
    $a^{\prime}$ based on the state $s^{\prime}$ and then update the table entry for
    $\mathcal{Q}(s,a)$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{Q}_{t+1}(s,a)$ | $\displaystyle\leftarrow\mathcal{Q}_{t}(s,a)+\alpha_{t}\Big{[}r_{t}(s,a)+$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\gamma\max_{a^{\prime}}\mathcal{Q}_{t}(s^{\prime},a^{\prime})-\mathcal{Q}_{t}(s,a)\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Replace $s\leftarrow s^{\prime}$.  end for  Output: $\pi^{*}(s)=\arg\max_{a}\mathcal{Q}^{*}(s,a)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once either all $\mathcal{Q}$-values converge or a certain number of iterations
    is reached, the algorithm will terminate. The algorithm then yields the optimal
    policy indicating an action to be taken at each state such that $\mathcal{Q}^{*}(s,a)$
    is maximized for all states in the state space, i.e., $\pi^{*}(s)=\arg\underset{a}{\max}\mathcal{Q}^{*}(s,a)$.
    Under the assumption of the step size (i.e., Assumption [1](#Thmassumption1 "Assumption
    1\. ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")), it is proved in [[15](#bib.bib15)] that the $Q$-learning
    algorithm converges to the optimum action-values with probability one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'II-B2 SARSA: An Online Q-Learning Algorithm'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although the $Q$-learning algorithm can find the optimal policy for the agent
    without requiring knowledge about the environment, this algorithm works in an
    offline fashion. In particular, Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning
    Algorithm ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey") can obtain the optimal policy only after all $\mathcal{Q}$-values converge.
    Therefore, this section presents an alternative online learning algorithm, i.e.,
    the SARSA algorithm, which allows the agent to approach the optimal policy in
    an online fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: Different from the $Q$-learning algorithm, the SARSA algorithm is an online
    algorithm which allows the agent to choose optimal actions at each time step in
    a real-time fashion without waiting until the algorithm converges. In the $Q$-learning
    algorithm, the policy is updated according to the maximum reward of available
    actions regardless of which policy is applied, i.e., an off-policy method. In
    contrast, the SARSA algorithm interacts with the environment and updates the policy
    directly from the actions taken, i.e., an on-policy method. Note that the SARSA
    algorithm updates $\mathcal{Q}$-values from the quintuple $\mathcal{Q}(s,a,r,s^{\prime},a^{\prime})$.
  prefs: []
  type: TYPE_NORMAL
- en: II-B3 Q-Learning for Markov Games
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To apply Q-learning algorithm to the Markov game context, we first define the
    $Q$-function for agent $i$ by $\mathcal{Q}_{i}(s,a^{i},\textbf{a}^{-i})$, where
    $\textbf{a}^{-i}\triangleq\{a^{1},\ldots,a^{i-1},a^{i+1},\ldots,a^{I}\}$ denotes
    the set of actions of all agents except $i$. Then, the Nash $Q$-function of agent
    $i$ is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{Q}^{*}_{i}(s,a^{i},\textbf{a}^{-i})$ | $\displaystyle=r^{i}(s,a^{i},\textbf{a}^{-i})+$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\beta\sum_{s^{\prime}\in\mathcal{S}}p(s^{\prime}&#124;s,a^{i},\textbf{a}^{-i})\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $(\pi_{1}^{*},\ldots,\pi_{I}^{*})$ is the joint Nash equilibrium strategy,
    $r^{i}(s,a^{i},\textbf{a}^{-i})$ is agent $i$’s immediate reward in state $s$
    under the joint action $(a^{i},\textbf{a}^{-i})$, and $\mathcal{V}^{i}(s^{\prime},\pi_{1}^{*},\ldots,\pi_{I}^{*})$
    is the total discounted reward over an infinite time horizon starting from state
    $s^{\prime}$ given that all the agents follow the equilibrium strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[13](#bib.bib13)], the authors propose a multi-agent Q-learning algorithm
    for general-sum Markov games which allows the agents to perform updates based
    on assuming Nash equilibrium behavior over the current Q-values. In particular,
    agent $i$ will learn its $Q$-values by forming an arbitrary guess from starting
    time of the game. At each time step $t$, agent $i$ observes the current state
    and takes an action $a^{i}$. Then, it observes its immediate reward $r^{i}$, actions
    taken by others $\textbf{a}^{-i}$, others’ immediate rewards, and the new system
    state $s^{\prime}$. After that, agent $i$ calculates a Nash equilibrium $(\pi_{1}(s^{\prime}),\ldots,\pi_{I}(s^{\prime}))$
    for the state game $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$,
    and updates its $Q$-values according to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{Q}_{i}^{t+1}(s,a^{i},\textbf{a}^{-i})=(1-\alpha_{t})\mathcal{Q}_{i}^{t}(s,a^{i},\textbf{a}^{-i})+\alpha_{t}[r_{t}^{i}+\gamma\mathscr{N}_{t}^{i}(s^{\prime})],$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{t}\in(0,1)$ is the learning rate and $\mathscr{N}_{t}^{i}(s^{\prime})\triangleq\mathcal{Q}_{i}^{t}(s^{\prime})\times\pi_{1}(s^{\prime})\times\cdots\times\pi_{I}(s^{\prime})$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to calculate the Nash equilibrium, agent $i$ needs to know $(\mathcal{Q}_{1}^{t}(s^{\prime}),\ldots,\mathcal{Q}_{I}^{t}(s^{\prime}))$.
    However, the information about other agents’ $\mathcal{Q}$-values is not given,
    and thus agent $i$ must learn this information too. To do so, agent $i$ will set
    estimations about others’ $\mathcal{Q}$-values at the beginning of the game, e.g.,
    $\mathcal{Q}_{0}^{j}(s,a^{i},\textbf{a}^{-i})=0,\forall j,s$. As the game proceeds,
    agent $i$ observes other agents’ immediate rewards and previous actions. That
    information can then be used to update agent $i$’s conjectures on other agents’
    $Q$-functions. Agent $i$ updates its beliefs about agent $j$’s $Q$-function, according
    to the same updating rule in ([5](#S2.E5 "In II-B3 Q-Learning for Markov Games
    ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement Learning: An Overview ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")). Then, the authors prove that under some highly restrictive assumptions
    on the form of the state games during learning, the proposed multi-agent $Q$-learning
    algorithm is guaranteed to be converged.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning [[2](#bib.bib2)] is composed of a set of algorithms and techniques
    that attempt to find important features of data and to model its high-level abstractions.
    The main goal of deep learning is to avoid manual description of a data structure
    (like hand-written features) by automatic learning from the data. Its name refers
    to the fact that typically any neural network with two or more hidden layers is
    called DNN. Most deep learning models are based on an Artificial Neural Network
    (ANN), even though they can also include propositional formulas or latent variables
    organized layer-wise in deep generative models such as the nodes in Deep Belief
    Networks and Deep Boltzmann Machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ANN is a computational nonlinear model based on the neural structure of
    the brain that is able to learn to perform tasks such as classification, prediction,
    decision-making, and visualization. An ANN consists of artificial neurons and
    is organized into three interconnected layers: input, hidden, and output as illustrated
    in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(b). The input layer contains input neurons that send
    information to the hidden layer. The hidden layer sends data to the output layer.
    Every neuron has weighted inputs (synapses), an activation function (defines the
    output given an input), and one output. Synapses are the adjustable parameters
    that convert a neural network to a parameterized system.'
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, ANNs use backpropagation as an effective learning
    algorithm to compute quickly a gradient descent with respect to the weights. Backpropagation
    is a special case of automatic differentiation. In the context of learning, backpropagation
    is commonly used by the gradient descent optimization algorithm to adjust the
    weights of neurons by calculating the gradient of the loss function. This technique
    is also sometimes called backward propagation of errors, because the error is
    calculated at the output and distributed back through the network layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A DNN is defined as an ANN with multiple hidden layers. There are two typical
    DNN models, i.e., Feedforward Neural Network (FNN) and Recurrent Neural Network
    (RNN). In the FNN, the information moves in only one direction, i.e., from the
    input nodes, through the hidden nodes and to the output nodes, and there are no
    cycles or loops in the network as shown in Fig. [4](#S2.F4 "Figure 4 ‣ II-C Deep
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey"). In FNNs,
    Convolutional Neural Network (CNN) is the most well known model with a wide range
    of applications especially in image and speech recognition. The CNN contains one
    or more convolutional layers, pooling or fully connected, and uses a variation
    of multilayer perceptrons discussed above. Convolutional layers use a convolution
    operation to the input passing the result to the next layer. This operation allows
    the network to be deeper with much fewer parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35562a59856188e83faf82a64b063247.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: RNN vs CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike FNNs, the RNN is a variant of a recursive artificial neural network in
    which connections between neurons make directed cycles. It means that an output
    depends not only on its immediate inputs, but also on the previous further step’s
    neuron state. The RNNs are designed to utilize sequential data, when the current
    step has some relation with the previous steps. This makes the RNNs ideal for
    applications with a time component, e.g., time-series data, and natural language
    processing. However, all RNNs have feedback loops in the recurrent layer. This
    lets RNNs maintain information in memory over time. Nevertheless, it can be difficult
    to train standard RNNs to solve problems that require learning long-term temporal
    dependencies. The reason is that the gradient of the loss function decays exponentially
    with time, which is called the vanishing gradient problem. Thus, Long Short-Term
    Memory (LSTM) is often used in RNNs to address this issue. The LSTM is designed
    to model temporal sequences and their long-range dependencies are more accurate
    than conventional RNNs. The LSTM does not use an activation function within its
    recurrent components, the stored values are not modified, and the gradient does
    not tend to vanish during training. Usually, LSTM units are implemented in “blocks”
    with several units. These blocks have three or four “gates”, e.g., input gate,
    forget gate, output gate, that control information flow drawing on the logistic
    function.
  prefs: []
  type: TYPE_NORMAL
- en: II-D Deep $Q$-Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The $Q$-learning algorithm can efficiently obtain an optimal policy when the
    state space and action space are small. However, in practice, with complicated
    system models, these spaces are usually large. As a result, the $Q$-learning algorithm
    may not be able to find the optimal policy. Thus, Deep $Q$-Learning (DQL) algorithm
    is introduced to overcome this shortcoming. Intuitively, the DQL algorithm implements
    a Deep $Q$-Network (DQN), i.e., a DNN, instead of the $Q$-table to derive an approximate
    value of $Q^{*}(s,a)$ as shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")(c).'
  prefs: []
  type: TYPE_NORMAL
- en: As stated in [[16](#bib.bib16)], the average reward obtained by reinforcement
    learning algorithms may not be stable or even diverge when a nonlinear function
    approximator is used. This stems from the fact that a small change of $\mathcal{Q}$-values
    may greatly affect the policy. Thus, the data distribution and the correlations
    between the $\mathcal{Q}$-values and the target values $R+\gamma\max_{a^{\prime}}\mathcal{Q}(s^{\prime},a^{\prime})$
    are varied. To address this issue, two mechanisms, i.e., experience replay and
    target $Q$-network, can be used.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Experience replay mechanism*: The algorithm first initializes a replay memory
    $\mathbf{D}$, i.e., the memory pool, with transitions $(s_{t},a_{t},r_{t},s_{t+1})$,
    i.e., experiences, generated randomly, e.g., through using $\epsilon$-greedy policy.
    Then, the algorithm randomly selects samples, i.e., minibatches, of transitions
    from $\mathbf{D}$ to train the DNN. The Q-values obtained by the trained DNN will
    be used to obtain new experiences, i.e., transitions, and these experiences will
    be then stored in the memory pool $\mathbf{D}$. This mechanism allows the DNN
    trained more efficiently by using both old and new experiences. In addition, by
    using the experience replay, the transitions are more independent and identically
    distributed, and thus the correlations between observations can be removed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fixed target $Q$-network: In the training process, the $\mathcal{Q}$-value
    will be shifted. Thus, the value estimations can be out of control if a constantly
    shifting set of values is used to update the $Q$-network. This leads to the destabilization
    of the algorithm. To address this issue, the target $Q$-network is used to update
    frequently but slowly the primary $Q$-networks’ values. In this way, the correlations
    between the target and estimated $\mathcal{Q}$-values are significantly reduced,
    thereby stabilizing the algorithm.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The DQL algorithm with experience replay and fixed target $Q$-network is presented
    in Algorithm [2](#alg2 "Algorithm 2 ‣ II-D Deep Q-Learning ‣ II Deep Reinforcement
    Learning: An Overview ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey"). DQL inherits and promotes advantages of both reinforcement
    and deep learning techniques, and thus it has a wide range of applications in
    practice such as game development [[3](#bib.bib3)], transportation [[17](#bib.bib17)],
    and robotics [[18](#bib.bib18)].'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 The DQL Algorithm with Experience Replay and Fixed Target $Q$-Network
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialize replay memory $\mathbf{D}$.2:  Initialize the $Q$-network $\mathbf{Q}$
    with random weights $\boldsymbol{\theta}$.3:  Initialize the target $Q$-network
    $\hat{\mathbf{Q}}$ with random weights $\boldsymbol{\theta^{\prime}}$.4:  for episode=1
    to T do5:     With probability $\epsilon$ select a random action $a_{t}$, otherwise
    select $a_{t}=\arg\max\mathcal{Q}^{*}(s_{t},a_{t},\boldsymbol{\theta})$.6:     Perform
    action $a_{t}$ and observe immediate reward $r_{t}$ and next state $s_{t+1}$.7:     Store
    transition $(s_{t},a_{t},r_{t},s_{t+1})$ in $\mathbf{D}$.8:     Select randomly
    samples c$(s_{j},a_{j},r_{j},s_{j+1})$ from $\mathbf{D}$.9:     The weights of
    the neural network then are optimized by using stochastic gradient descent with
    respect to the network parameter $\boldsymbol{\theta}$ to minimize the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Big{[}r_{j}+\gamma\max_{a_{j+1}}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 10:     Reset $\hat{\mathbf{Q}}=\mathbf{Q}$ after every a fixed number of steps.11:  end for
  prefs: []
  type: TYPE_NORMAL
- en: II-E Advanced Deep $Q$-Learning Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-E1 Double Deep $Q$-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In some stochastic environments, the $Q$-learning algorithm performs poorly
    due to the large over-estimations of action values [[19](#bib.bib19)]. These over-estimations
    result from a positive bias that is introduced because $Q$-learning uses the maximum
    action value as an approximation for the maximum expected action value as shown
    in Eq. ([3](#S2.E3 "In 4 ‣ Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey")). The reason
    is that the same samples are used to decide which action is the best, i.e., with
    highest expected reward, and the same samples are also used to estimate that action-value.
    Thus, to overcome the over-estimation problem of the $Q$-learning algorithm, the
    authors in [[20](#bib.bib20)] introduce a solution using two $Q$-value functions,
    i.e., $\mathcal{Q}_{1}$ and $\mathcal{Q}_{2}$, to simultaneously select and evaluate
    action values through the loss function as follows: $\Big{[}r_{j}+\gamma\mathcal{Q}_{2}\Big{(}s_{j+1},\arg\underset{a_{j+1}}{\max}\mathcal{Q}_{1}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}_{1}\big{)};\boldsymbol{\theta}_{2}\Big{)}-\mathcal{Q}_{1}(s_{j},a_{j};\boldsymbol{\theta}_{1})\Big{]}^{2}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the selection of an action, in the $\arg\max$, is still due to the
    online weights $\boldsymbol{\theta}_{1}$. This means that, as in $Q$-learning,
    we are still estimating the value of the greedy policy according to the current
    values, as defined by $\boldsymbol{\theta}_{1}$. However, the second set of weights
    $\boldsymbol{\theta}_{2}$ is used to evaluate fairly the value of this policy.
    This second set of weights can be updated symmetrically by switching the roles
    of $\boldsymbol{\theta}_{1}$ and $\boldsymbol{\theta}_{2}$. Inspired by this idea,
    the authors in [[20](#bib.bib20)] then develop Double Deep $Q$-Learning (DDQL)
    model [[21](#bib.bib21)] using a Double Deep Q-Network (DDQN) with the loss function
    updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Big{[}r_{j}$ | $\displaystyle+\gamma\hat{\mathcal{Q}}\Big{(}s_{j+1},\arg\max_{a_{j+1}}\mathcal{Q}\big{(}s_{j+1},a_{j+1};\boldsymbol{\theta}\big{)};\boldsymbol{\theta^{\prime}}\Big{)}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Unlike double $Q$-learning, the weights of the second network $\boldsymbol{\theta_{2}}$
    are replaced with the weights of the target networks $\boldsymbol{\theta^{\prime}}$
    for the evaluation of the current greedy policy as shown in Eq. ([7](#S2.E7 "In
    II-E1 Double Deep 𝑄-Learning ‣ II-E Advanced Deep Q-Learning Models ‣ II Deep
    Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")). The update to the target network
    stays unchanged from DQN, and remains a periodic copy of the online network. Due
    to the effectiveness of DDQL, there are some applications of DDQL introduced recently
    to address dynamic spectrum access problems in multichannel wireless networks [[22](#bib.bib22)]
    and resource allocation in heterogeneous networks [[23](#bib.bib23)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-E2 Deep $Q$-Learning with Prioritized Experience Replay
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Experience replay mechanism allows the reinforcement learning agent to remember
    and reuse experiences, i.e., transitions, from the past. In particular, transitions
    are uniformly sampled from the replay memory $\mathbf{D}$. However, this approach
    simply replays transitions at the same frequency as that the agent was originally
    experienced, regardless of their significance. Therefore, the authors in [[24](#bib.bib24)]
    develop a framework for prioritizing experiences, so as to replay important transitions
    more frequently, and therefore learn more efficiently. Ideally, we want to sample
    more frequently those transitions from which there is much to learn. As a proxy
    for learning potential, the proposed Prioritized Experience Replay (PER) [[24](#bib.bib24)]
    samples transitions with probability $p_{t}$ relative to the last encountered
    absolute error defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{t}\varpropto\Big{&#124;}r_{j}+\gamma\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+1},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta)}\Big{&#124;}^{\omega},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega$ is a hyper-parameter that determines the shape of the distribution.
    New transitions are inserted into the replay buffer with maximum priority, providing
    a bias towards recent transitions. Note that stochastic transitions may also be
    favoured, even when there is little left to learn about them. Through real experiments
    on many Atari games, the authors demonstrate that DQL with PER outperforms DQL
    with uniform replay on 41 out of 49 games. However, this solution is only appropriate
    to implement when we can find and define the important experiences in the replay
    memory $\mathbf{D}$.
  prefs: []
  type: TYPE_NORMAL
- en: II-E3 Dueling Deep $Q$-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The $Q$-values, i.e., $\mathcal{Q}(s,a)$, used in the $Q$-learning algorithm,
    i.e., Algorithm [1](#alg1 "Algorithm 1 ‣ II-B1 𝑄-Learning Algorithm ‣ II-B Reinforcement
    Learning ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep
    Reinforcement Learning in Communications and Networking: A Survey"), are to express
    how good it is to take a certain action at a given state. The value of an action
    $a$ at a given state $s$ can actually be decomposed into two fundamental values.
    The first value is the state-value function, i.e., $\mathscr{V}(s)$, to estimate
    the importance of being in a particular state $s$. The second value is the action-value
    function, i.e., $\mathscr{A}(a)$, to estimate the importance of selecting an action
    $a$ compared with other actions. As a result, the $Q$-value function can be expressed
    by two fundamental value functions as follows: $\mathcal{Q}(s,a)=\mathscr{V}(s)+\mathscr{A}(a)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stemming from the fact that in many MDPs, it is unnecessary to estimate both
    values, i.e., action and state values of Q-function $\mathcal{Q}(s,a)$, at the
    same time. For example, in many racing games, moving left or right matters if
    and only if the agent meets the obstacles or enemies. Inspired by this idea, the
    authors in [[25](#bib.bib25)] introduce an idea of using two streams, i.e., two
    sequences, of fully connected layers instead of using a single sequence with fully
    connected layers for the DQN. The two streams are constructed such that they are
    able to provide separate estimations on the action and state value functions,
    i.e., $\mathscr{V}(s)$ and $\mathscr{A}(a)$. Finally, the two streams are combined
    to generate a single output $\mathcal{Q}(s,a)$ as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{Q}(s,a;\boldsymbol{\alpha},\boldsymbol{\beta})=\mathscr{V}(s;\boldsymbol{\beta})+\Big{(}\mathscr{A}(s,a;\boldsymbol{\alpha})-\frac{\sum_{a^{\prime}}\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})}{&#124;\mathcal{A}&#124;}\Big{)},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{\beta}$ and $\boldsymbol{\alpha}$ are the parameters of
    the two streams $\mathscr{V}(s;\boldsymbol{\beta})$ and $\mathscr{A}(s,a^{\prime};\boldsymbol{\alpha})$,
    respectively. Here, $|\mathcal{A}|$ is the total number of actions in the action
    space $\mathcal{A}$. Then, the loss function is derived in the similar way to ([6](#S2.E6
    "In 9 ‣ Algorithm 2 ‣ II-D Deep Q-Learning ‣ II Deep Reinforcement Learning: An
    Overview ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\Big{[}r_{j}+\gamma\underset{a_{j+1}}{\max}\hat{\mathcal{Q}}(s_{j+1},a_{j+1};\boldsymbol{\alpha^{\prime}},\boldsymbol{\beta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\alpha},\boldsymbol{\beta})\Big{]}^{2}$.
    Through the simulation, the authors show that the proposed dueling DQN can outperform
    DDQN [[21](#bib.bib21)] in 50 out of 57 learned Atari games. However, the proposed
    dueling architecture only clearly benefits for MDPs with large action spaces.
    For small state spaces, the performance of dueling DQL is even not as good as
    that of double DQL as shown in simulation results in [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: II-E4 Asynchronous Multi-step Deep Q-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most of the $Q$-learning methods such as DQL and dueling DQL rely on the experience
    replay method. However, such kind of method has several drawbacks. For example,
    it uses more memory and computation resources per real interaction, and it requires
    off-policy learning algorithms that can update from data generated by an older
    policy. This limits the applications of DQL. Therefore, the authors in [[26](#bib.bib26)]
    introduce a method using multiple agents to train the DNN in parallel. In particular,
    the authors propose a training procedure which utilizes asynchronous gradient
    decent updates from multiple agents at once. Instead of training one single agent
    that interacts with its environment, multiple agents are interacting with their
    own version of the environment simultaneously. After a certain amount of timesteps,
    accumulated gradient updates from an agent are applied to a global model, i.e.,
    the DNN. These updates are asynchronous and lock free. In addition, to tradeoff
    between bias and variance in the policy gradient, the authors adopt $n$-step updates
    method [[1](#bib.bib1)] to update the reward function. In particular, the truncated
    $n$-step reward function can be defined by $r_{t}^{(n)}=\underset{k=0}{\overset{n-1}{\sum}}\gamma^{(k)}r_{t+k+1}$.
    Thus, the alternative loss for each agent will be derived by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Big{[}r_{j}^{(n)}+\gamma_{j}^{(n)}\max_{a^{\prime}}\hat{\mathcal{Q}}(s_{j+n},a^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s_{j},a_{j};\boldsymbol{\theta})\Big{]}^{2}.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: The effects of training speed and quality of the proposed asynchronous DQL with
    multi-step learning are analyzed for various reinforcement learning methods, e.g.,
    1-step $Q$-learning, 1-step SARSA, and n-step $Q$-learning. They show that asynchronous
    updates have a stabilizing effect on policy and value updates. Also, the proposed
    method outperforms the current state-of-the-art algorithms on the Atari games
    while training for half of the time on a single multi-core CPU instead of a GPU.
    As a result, some recent applications of asynchronous DQL have been developed
    for handover control problems in wireless systems [[27](#bib.bib27)]
  prefs: []
  type: TYPE_NORMAL
- en: II-E5 Distributional Deep Q-learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All aforementioned methods use the Bellman equation to approximate the expected
    value of future rewards. However, if the environment is stochastic in nature and
    the future rewards follow multimodal distribution, choosing actions based on expected
    value may not lead to the optimal outcome. For example, we know that the expected
    transmission time of a packet in a wireless network is 20 minutes. However, this
    information may not be so meaningful because it may overestimate the transmission
    time most of the time. For example, the expected transmission time is calculated
    based on the normal transmissions (without collisions) and the interference transmissions
    (with collisions). Although the interference transmissions are very rare to happen,
    but it takes a lot of time. Then, the estimation about the expected transmission
    is overestimated most of the time. This makes estimations not useful for the DQL
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the authors in [[28](#bib.bib28)] introduce a solution using distributional
    reinforcement learning to update $Q$-value function based on its distribution
    rather than its expectation. In particular, let $\mathcal{Z}(s,a)$ be the return
    obtained by starting from state $s$, executing action $a$, and following the current
    policy, then $\mathcal{Q}(s,a)=\mathbb{E}[\mathcal{Z}(s,a)]$. Here, $\mathcal{Z}$
    represents the distribution of future rewards, which is no longer a scalar quantity
    like $Q$-values. Then we obtain the distributional version of Bellman equation
    as follows: $\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{\prime})$. For
    example, if we use the DQN and extract an experience $(s,a,r,s^{\prime})$ from
    the replay buffer, then the sample of the target distribution is $\mathcal{Z}(s,a)=r+\gamma\mathcal{Z}(s^{\prime},a^{*})$
    with $a^{*}=\arg\underset{a^{\prime}}{\max}\mathcal{Q}(s,a^{\prime})$. Although
    the proposed distributional deep Q-learning is demonstrated to outperform the
    conventional DQL [[16](#bib.bib16)] on many Atari 2600 Games (45 out of 57 games),
    its performance relies much on the distribution function $\mathcal{Z}$. If $\mathcal{Z}$
    is well defined, the performance of distributional deep $Q$-learning is much more
    significant than that of the DQL. Otherwise, its performance is even worse than
    that of the DQL.'
  prefs: []
  type: TYPE_NORMAL
- en: II-E6 Deep $Q$-learning with Noisy Nets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[29](#bib.bib29)], the authors introduce Noisy Net, a type of neural network
    whose bias and weights are iteratively perturbed during training by a parametric
    function of the noise. This network basically adds the Gaussian noise to the last
    (fully-connected) layers of the network. The parameters of this noise can be adjusted
    by the model during training, which allows the agent to decide when and in what
    proportion it wants to introduce the uncertainty to its weights. In particular,
    to implement the noisy network, we first replace the $\epsilon$-greedy policy
    by a randomized action-value function. Then, the fully connected layers of the
    value network are parameterized as a noisy network, where the parameters are drawn
    from the noisy network parameter distribution after every replay step. For replay,
    the current noisy network parameter sample is held fixed across the batch. Since
    the DQL takes one step of optimization for every action step, the noisy network
    parameters are re-sampled before every action. After that, the loss function can
    be updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}=\mathbb{E}\Big{[}\mathbb{E}_{(s,a,r,s^{\prime})\thicksim\mathbf{D}}\big{[}r+\gamma\max_{a^{\prime}\in\mathcal{A}}\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})-\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})\big{]}\Big{]},$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where the outer and inner expectations are with respect to distributions of
    the noise variables $\epsilon$ and $\epsilon^{\prime}$ for the noisy value functions
    $\hat{\mathcal{Q}}(s^{\prime},a^{\prime},\epsilon^{\prime};\boldsymbol{\theta^{\prime}})$
    and $\mathcal{Q}(s,a,\epsilon;\boldsymbol{\theta})$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Through experimental results, the authors demonstrate that by adding the Gaussian
    noise layer to the DNN, the performance of conventional DQL [[16](#bib.bib16)],
    dueling DQL [[25](#bib.bib25)], and asynchronous DQL [[26](#bib.bib26)] can be
    significantly improved for a wide range of Atari games. However, the impact of
    noise to the performance of the deep DQL algorithms is still under debating in
    the literature, and thus analysis on the impact of noise layer requires further
    investigations.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Performance comparison among DQL algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '| DQL Algorithms | No Operations | Human Starts | Publish | Developer |'
  prefs: []
  type: TYPE_TB
- en: '| DQL | 79% | 68% | Nature 2015 [[16](#bib.bib16)] | Google DeepMind |'
  prefs: []
  type: TYPE_TB
- en: '| DDQL | 117% | 110% | AAAI 2016 [[21](#bib.bib21)] | Google DeepMind |'
  prefs: []
  type: TYPE_TB
- en: '| Prioritized DDQL | 140% | 128% | ICLR 2015 [[24](#bib.bib24)] | Google DeepMind
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dueling DDQL | 151% | 117% | ICML 2016 [[25](#bib.bib25)] | Google DeepMind
    |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous DQL | - | 116% | ICML 2016 [[26](#bib.bib26)] | Google DeepMind
    |'
  prefs: []
  type: TYPE_TB
- en: '| Distributional DQL | 164% | 125% | ICML 2017 [[28](#bib.bib28)] | Google
    DeepMind |'
  prefs: []
  type: TYPE_TB
- en: '| Noisy Nets DQL | 118% | 102% | ICLR 2018 [[29](#bib.bib29)] | Google DeepMind
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rainbow | 223% | 153% | AAAI 2018 [[30](#bib.bib30)] | Google DeepMind |'
  prefs: []
  type: TYPE_TB
- en: II-E7 Rainbow Deep $Q$-learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[30](#bib.bib30)], the authors propose a solution which integrates all advantages
    of seven aforementioned solutions (including DQL) into a single learning agent,
    called Rainbow DQL. In particular, this algorithm first defines the loss function
    based on the asynchronous multi-step and distributional DQL. Then, the authors
    combine the multi-step distributional loss with double $Q$-learning by using the
    greedy action in $s_{t+n}$ selected according to the $Q$-network as the bootstrap
    action $a^{*}_{t+n}$, and evaluate the action by using the target network.
  prefs: []
  type: TYPE_NORMAL
- en: In standard proportional prioritized replay [[24](#bib.bib24)] technique, the
    absolute TD-error is used to prioritize the transitions. Here, TD-error at a time
    slot is the error in the estimate made at the time slot. However, in the proposed
    Rainbow DQL algorithm, all distributional Rainbow variants prioritize transitions
    by the Kullbeck-Leibler (KL) loss because this loss may be more robust to noisy
    stochastic environment. Alternatively, the dueling architecture of DNNs is presented
    in [[25](#bib.bib25)]. Finally, the Noisy Net layer [[30](#bib.bib30)] is used
    to replace all linear layers in order to reduce the number of independent noise
    variables. Through simulation, the authors show that this is the most advanced
    technique which outperforms almost all current DQL algorithms in the literature
    over 57 Atari 2600 games.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [II](#S2.T2 "TABLE II ‣ II-E6 Deep 𝑄-learning with Noisy Nets ‣ II-E
    Advanced Deep Q-Learning Models ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), we summarize the DQL algorithms and their performance under the parameter
    settings used in [[30](#bib.bib30)]. As observed in Table [II](#S2.T2 "TABLE II
    ‣ II-E6 Deep 𝑄-learning with Noisy Nets ‣ II-E Advanced Deep Q-Learning Models
    ‣ II Deep Reinforcement Learning: An Overview ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey"), all of the DQL algorithms
    have been developed by Google DeepMind based on the original work in [[16](#bib.bib16)].
    So far, through experimental results on Atari 2600 games, the Rainbow DQL presents
    very impressive results over all other DQL algorithms. However, more experiments
    need to be further conducted in different domains to confirm the real efficiency
    of the Rainbow DQL algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: II-F Deep Q-Learning for Extensions of MDPs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although DQL algorithm can solve problems with high-dimensional state spaces,
    it can only handle discrete and low-dimensional action spaces. However, systems
    in many applications have continuous, i.e., real values, and high dimensional
    action spaces. The DQL algorithms cannot be straightforwardly applied to continuous
    actions since they rely on choosing the best action that maximizes the $Q$-value
    function. In particular, a full search in a continuous action space to find the
    optimal action is often infeasible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[31](#bib.bib31)], the authors introduce a model-free off-policy actor-critic
    algorithm using deep function approximators that can learn policies in high-dimensional,
    continuous action spaces. The key idea is based on the deterministic policy gradient
    (DPG) algorithm proposed in [[32](#bib.bib32)]. In particular, the DPG algorithm
    maintains a parameterized actor function $\mu(s;\boldsymbol{\theta}^{\mu})$ with
    parameter vector $\boldsymbol{\theta}$ which specifies the current policy by deterministically
    mapping states to a specific action. The critic $Q(s,a)$ is learned by using the
    Bellman equation as in $Q$-learning. The actor is updated by applying the chain
    rule to the expected return from the start distribution $J$ with respect to the
    actor parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\big{[}\nabla_{\boldsymbol{\theta}^{\mu}}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t}&#124;\boldsymbol{\theta}^{\mu})}\big{]}$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\thickapprox\mathbb{E}_{s_{t}\thicksim\rho^{\beta}}\Big{[}\nabla_{a}Q(s,a;\boldsymbol{\theta}^{Q})&#124;_{s=s_{t},a=\mu(s_{t})}\nabla_{\boldsymbol{\theta}{\mu}}\mu(s;\boldsymbol{\theta}^{\mu})&#124;_{s=s_{t}}\Big{]}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Based on this update rule, the authors then introduce Deep DPG (DDPG) algorithm
    which can learn competitive policies by using low-dimensional observations (e.g.
    cartesian coordinates or joint angles) under the same hyper-parameters and network
    structure. The detail of the DDPG algorithm is presented in [3](#alg3 "Algorithm
    3 ‣ II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action
    ‣ II-F Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"). The algorithm makes a copy of the actor and critic networks
    $Q^{\prime}(s,a;\boldsymbol{\theta}^{Q^{\prime}})$ and $\mu^{\prime}(s;\boldsymbol{\theta}^{\mu^{\prime}})$,
    respectively, to calculate the target values. The weights of these target networks
    are then updated with slowly tracking on the learned networks, i.e., $\boldsymbol{\theta}^{\prime}\leftarrow\tau\boldsymbol{\theta}+(1-\tau)\boldsymbol{\theta}^{\prime}$
    with $\tau\ll 1$. This means that the target values are constrained to change
    slowly, greatly improving the stability of learning. Note that a major challenge
    of learning in continuous action spaces is exploration. Therefore, in Algorithm [3](#alg3
    "Algorithm 3 ‣ II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous
    Action ‣ II-F Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning:
    An Overview ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), an exploration policy $\mu^{\prime}$ is constructed by
    adding noise sampled from a noise process $\mathcal{N}$ to the actor policy.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 DDPG algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Randomly initialize critic network $Q(s,a;\boldsymbol{\theta}^{Q})$ and
    actor $\mu(s;\boldsymbol{\theta}^{\mu})$ with weights $\boldsymbol{\theta}^{Q}$
    and $\boldsymbol{\theta}^{\mu}$, respectively.2:  Initialize target network $Q^{\prime}$
    and $\mu^{\prime}$ with weights $\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\boldsymbol{\theta}^{Q}$,
    and $\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\boldsymbol{\theta}^{\mu}$, respectively.3:  Initialize
    replay memory $\mathbf{D}$.4:  for episode=1 to M do5:     Initialize a random
    process $N$ for action exploration6:     Receive initial observation state $s_{1}$7:     for t=1
    to T do8:        Select action $a_{t}=\mu(s_{t};\boldsymbol{\theta}^{\mu})+\mathcal{N}_{t}$
    according to the current policy and exploration noise.9:        Execute action
    $a_{t}$ and observe reward $r_{t}$ and new state $s_{t+1}$.10:        Store transition
    $(s_{t},a_{t},r_{t},s_{t+1})$ in $\mathbf{D}$.11:        Sample a random mini-batch
    of $N$ transitions $(s_{i},a_{i},r_{i},s_{i+1})$ from $\mathbf{D}$.12:        Set
    $y_{i}=r_{i}+\gamma\mathcal{Q}^{{}^{\prime}}\big{(}s_{i+1},\mu^{{}^{\prime}}(s_{i+1};\boldsymbol{\theta}^{\mu^{\prime}});\boldsymbol{\theta}^{Q^{\prime}}\big{)}$.13:        Update
    critic by minimizing the loss: $L=\frac{1}{N}\sum_{i}(y_{i}-\mathcal{Q}\big{(}s_{i},a_{i};\boldsymbol{\theta}^{Q})\big{)}^{2}$14:        Update
    the actor policy by using the sampled policy gradient: $\nabla_{\boldsymbol{\theta}^{\mu}}J\thickapprox\frac{1}{N}\sum_{i}\nabla_{a}\mathcal{Q}(s,a;\boldsymbol{\theta}^{Q})|_{s=s_{i},a=\mu(s_{i})}$
    $\nabla_{\boldsymbol{\theta}^{\mu}}\mu(s|\boldsymbol{\theta}^{\mu})|_{s=s_{i}}$15:        Update
    the target networks:$\boldsymbol{\theta}^{Q^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{Q}+(1-\tau)\boldsymbol{\theta}^{Q^{\prime}}$$\boldsymbol{\theta}^{\mu^{\prime}}\leftarrow\tau\boldsymbol{\theta}^{\mu}+(1-\tau)\boldsymbol{\theta}^{\mu^{\prime}}$16:     end for17:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: II-F2 Deep Recurrent Q-Learning for POMDPs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To tackle problems with partially observable environments by deep reinforcement
    learning, the authors in [[33](#bib.bib33)] propose a framework called Deep Recurrent
    Q-Learning (DRQN) in which an LSTM layer was used to replace the first post-convolutional
    fully-connected layer of the conventional DQN. The recurrent structure is able
    to integrate an arbitrarily long history to better estimate the current state
    instead of utilizing a fixed-length history as in DQNs. Thus, DRQNs estimate the
    function $\mathcal{Q}(o_{t},h_{t-1};\boldsymbol{\theta})$ instead of $\mathcal{Q}(s_{t},a_{t});\boldsymbol{\theta})$,
    where $\boldsymbol{\theta}$ denotes the parameters of entire network, $h_{t-1}$
    denotes the output of the LSTM layer at the previous step, i.e., $h_{t}=LSTM(h_{t-1},o_{t})$.
    DRQN matches DQN’s performance on standard MDP problems and outperforms DQN in
    partially observable domains. Regarding the training process, DRQN only considers
    the convolutional features of the observation history instead of explicitly incorporating
    the actions. Through the experiments, the authors demonstrate that DRQN is capable
    of handling partial observability, and recurrency confers benefits when the quality
    of observations changes during evaluation time.
  prefs: []
  type: TYPE_NORMAL
- en: II-F3 Deep SARSA Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In [[34](#bib.bib34)], the authors introduce a DQL technique based on SARSA
    learning to help the agent determine optimal policies in an online fashion. As
    shown in Algorithm [4](#alg4 "Algorithm 4 ‣ II-F3 Deep SARSA Learning ‣ II-F Deep
    Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), given the current state $s$, a CNN is used to obtain the current state-action
    value $\mathcal{Q}(s,a)$. Then, the current action $a$ is selected by the $\epsilon$-greedy
    algorithm. After that, the immediate reward $r$ and the next state $s^{\prime}$
    can be observed. In order to estimate the current $\mathcal{Q}(s,a)$, the next
    state-action value $\mathcal{Q}(s^{\prime},a^{\prime})$ is obtained. Here, when
    the next state $s^{\prime}$ is used as the input of the CNN, $\mathcal{Q}(s^{\prime},a^{\prime})$
    can be obtained as the output. Then, a label vector related to $\mathcal{Q}(s,a)$
    is defined as $\mathcal{Q}(s^{\prime},a^{\prime})$ which represents the target
    vector. The two vectors only have one different component, i.e., $r+\gamma\mathcal{Q}(s^{\prime},a^{\prime})\rightarrow\mathcal{Q}(s,a)$.
    It should be noted that during the training phase, the next action $a^{\prime}$
    for estimating the current state-action value is never greedy. On the contrary,
    there is a small probability that a random action is chosen for exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Deep SARSA learning algorithm
  prefs: []
  type: TYPE_NORMAL
- en: '1:  Initialize data stack $\mathbf{D}$ with size of $N$2:  Initialize parameters
    $\boldsymbol{\theta}$ of the CNN3:  for episode=1 to M do4:     Initialize state
    $s_{1}$ and pre-process state $\phi_{1}=\phi(s_{1})$5:     Select $a_{1}$ by the
    $\epsilon$-greedy method6:     for t=1 to T do7:        Take action $a_{t}$, observe
    $r_{t}$ and next state $s_{t+1}$8:        $\phi_{t+1}=\phi(s_{t+1})$9:        Store
    data $(\phi_{t},a_{t},r_{t},\phi_{t+1})$ into stack $\mathbf{D}$10:        Sample
    data from stack $\mathbf{D}$11:        Select action $a^{\prime}$ by the $\epsilon$-greedy
    method12:        if episode terminates at step $j+1$ then13:           Set $y_{j}=r_{j}$14:        else15:           set
    $y_{j}=r_{j}+\mathcal{Q}(\phi_{t+1},a^{\prime};\boldsymbol{\theta})$16:        end if17:        Minimize
    the loss function: $(y_{j}-\mathcal{Q}(\phi_{t},a^{\prime};\boldsymbol{\theta}))^{2}$18:        Update
    $a_{t}\leftarrow a^{\prime}$19:     end for20:  end for'
  prefs: []
  type: TYPE_NORMAL
- en: II-F4 Deep $Q$-Learning for Markov Games
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[35](#bib.bib35)], the authors introduce the general notion of sequential
    prisoner’s dilemma (SPD) to model real world prisoner’s dilemma (PD) problems.
    Since SPD is more complicated than PD, existing approaches addressing learning
    in matrix PD games cannot be directly applied in SPD. Thus, the authors propose
    a multi-agent DRL approach for mutual cooperation in SDP games. The deep multi-agent
    reinforcement learning towards mutual cooperation consists of two phases, i.e.,
    offline and online phases. The offline phase generates policies with varying cooperation
    degrees. Since the number of policies with different cooperation degrees is infinite,
    it is computationally infeasible to train all the policies from scratch. To address
    this issue, the algorithm first trains representative policies using actor-critic
    until it converges, i.e., cooperation and defection baseline policy. Second, the
    algorithm synthesizes the full range of policies from the above baseline policies.
    Another task is to detect effectively the cooperation degree of the opponent.
    The algorithm divides this task into two steps. First, the algorithm trains an
    LSTM-based cooperation degree detection network offline, which will be then used
    for real-time detection during the online phase. In the online phase, the agent
    plays against the opponents by reciprocating with a policy of a slightly higher
    cooperation degree than that of the opponent. On one hand, intuitively the algorithm
    is cooperation-oriented and seeks for mutual cooperation whenever possible. On
    the other hand, the algorithm is also robust against selfish exploitation and
    resorts to defection strategy to avoid being exploited whenever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike [[35](#bib.bib35)] which considers a repeated normal form game with complete
    information, in [[36](#bib.bib36)], the authors introduce an application of DRL
    for extensive form games with imperfect information. In particular, the authors
    in [[36](#bib.bib36)] introduce Neural Fictitious Self-Play (NFSP), a DRL method
    for learning approximate Nash equilibria of imperfect-information games. NFSP
    combines FSP with neural network function approximation. An NFSP agent has two
    neural networks. The first network is trained by reinforcement learning from memorized
    experience of play against fellow agents. This network learns an approximate best
    response to the historical behaviour of other agents. The second network is trained
    by supervised learning from memorized experience of the agent’s own behaviour.
    This network learns a model that averages over the agent’s own historical strategies.
    The agent behaves according to a mixture of its average strategy and best response
    strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In the NSFP, all players of the game are controlled by separate NFSP agents
    that learn from simultaneous play against each other, i.e., self-play. An NFSP
    agent interacts with its fellow agents and memorizes its experience of game transitions
    and its own best response behaviour in two memories, $\mathcal{M}_{RL}$ and $\mathcal{M}_{SL}$.
    NFSP treats these memories as two distinct datasets suitable for DRL and supervised
    classification, respectively. The agent trains a neural network, $Q(s,a;\boldsymbol{\theta}^{Q})$,
    to predict action values from data in $\mathcal{M}_{RL}$ using off-policy reinforcement
    learning. The resulting network defines the agent’s approximate best response
    strategy, $\beta=\epsilon$-greedy($Q$), which selects a random action with probability
    $\epsilon$ and otherwise chooses the action that maximizes the predicted action
    values. The agent trains a separate neural network $\Pi(s,a;\boldsymbol{\theta}^{\Pi})$
    to imitate its own past best response behavior by using supervised classification
    on the data in $\mathcal{M}_{SL}$. NFSP also makes use of two technical innovations
    in order to ensure the stability of the resulting algorithm as well as to enable
    simultaneous self-play learning. Through experimental results, the authors show
    that the NFSP can converge to approximate Nash equilibria in a small poker game.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: In this section, we have presented the basics of reinforcement learning,
    deep learning, and DQL. Furthermore, we have discussed various advanced DQL techniques
    and their extensions. Different DQL techniques can be used to solve different
    problems in different network scenarios. In the next sections, we review DQL related
    works for various problems in communications and networking.'
  prefs: []
  type: TYPE_NORMAL
- en: III Network Access and Rate Control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modern networks such as IoT become more decentralized and ad-hoc in nature.
    In such networks, entities such as sensors and mobile users need to make independent
    decisions, e.g., channel and base station selections, to achieve their own goals,
    e.g., throughput maximization. However, this is challenging due to the dynamic
    and the uncertainty of network status. Learning algorithms such as DQL allow to
    learn and build knowledge about the networks that are used to enable the network
    entities to make their optimal decisions. In this section, we review the applications
    of DQL for the following issues:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamic spectrum access: Dynamic spectrum access allows users to locally select
    channels to maximize their throughput. However, the users may not have full observations
    of the system, e.g., channel states. Thus, DQL can be used as an effective tool
    for dynamic spectrum access.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joint user association and spectrum access: User association is implemented
    to determine which user to be assigned to which Base Station (BS). The joint user
    association and spectrum access problems are studied in [[37](#bib.bib37)] and
    [[38](#bib.bib38)]. However, the problems are typically combinatorial and non-convex
    which require nearly complete and accurate network information to obtain the optimal
    strategy. DQL is able to provide distributed solutions which can be effectively
    used for the problems.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adaptive rate control: This refers to bitrate/data rate control in dynamic
    and unpredictable environments such as Dynamic Adaptive Streaming over HTTP (DASH).
    Such a system allows clients or users to independently choose video segments with
    different bitrates to download. The client’s objective is to maximize its Quality
    of Experience (QoE). DQL can be adopted to effectively solve the problem instead
    of dynamic programming which has high complexity and demands complete information.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-A Network Access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section discusses how to use DQL to solve the spectrum access and user
    association in networks.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Dynamic Spectrum Access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The authors in [[39](#bib.bib39)] propose a dynamic channel access scheme of
    a sensor based on the DQL for IoT. At each time slot, the sensor selects one of
    $M$ channels for transmitting its packet. The channel state is either in low interference,
    i.e., successful transmission, or in high interference, i.e., transmission failure.
    Since the sensor only knows the channel state after selecting the channel, the
    sensor’s optimization decision problem can be formulated as a POMDP. In particular,
    the action of sensor is to select one of $M$ channels. The sensor receives a positive
    reward “+1” if the selected channel is in low interference, and a negative reward
    “-1” otherwise. The objective is to find an optimal policy which maximizes the
    sensor’s the expected accumulated discounted reward over time slots. A DQN¹¹1Remind
    that DQN is the core of the DQL algorithms. using FNN with experience replay [[40](#bib.bib40)]
    is then adopted to find the optimal policy. The input of the DQN is a state of
    the sensor which is the combination of actions and observations, i.e., the rewards,
    in the past time slots. The output includes Q-values corresponding to the actions.
    To balance the exploration of the current best Q-value with the exploration of
    the better one, the $\epsilon$-greedy policy is adopted for the action selection
    mechanism. The simulation results based on real data from [[41](#bib.bib41)] show
    that the proposed scheme can achieve the average accumulated reward close to the
    myopic policy [[42](#bib.bib42)] without a full knowledge of the system.
  prefs: []
  type: TYPE_NORMAL
- en: '[[39](#bib.bib39)] can be considered to be a pioneer work using the DQL for
    the channel access. However, the DQL keeps following the learned policy over time
    slots and stops learning a suitable policy. Actual IoT environments are dynamic,
    and the DQN in the DQL needs to be re-trained. An adaptive DQL scheme is proposed
    in [[43](#bib.bib43)] which evaluates the accumulated reward of the current policy
    for every period. When the reward is reduced by a given threshold, the DQN is
    re-trained to find a new good policy. The simulation results [[43](#bib.bib43)]
    show that when the states of the channels change, the adaptive DQL scheme can
    detect the change and start re-learning to obtain the high reward.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/227cde3560faff5b3756c0c3109f5119.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Joint channel selection and packet forwarding in IoT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The models in [[39](#bib.bib39)] and [[43](#bib.bib43)] are constrained to
    only one sensor. Consider a multi-sensor scenario, the authors in [[44](#bib.bib44)]
    address the joint channel selection and packet forwarding using the DQL. The model
    is shown in Fig. [5](#S3.F5 "Figure 5 ‣ III-A1 Dynamic Spectrum Access ‣ III-A
    Network Access ‣ III Network Access and Rate Control ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey") in which one sensor as a
    relay forwards packets received from its neighboring sensors to the sink. The
    sensor is equipped with a buffer to store the received packets. At each time slot,
    the sensor selects a set of channels for the packet forwarding so as to maximize
    its utility, i.e., the ratio of the number of transmitted packets to the transmit
    power. Similar to [[39](#bib.bib39)], the sensor’s problem can be formulated as
    an MDP. The action is to select a set of channels, the number of packets transmitted
    on the channels, and a modulation mode. To avoid packet loss, the state is defined
    as the combination of the buffer state and channel state. The MDP is then solved
    by the DQL in which the input is the state and the output is the action selection.
    The DQL uses the stacked autoencoder to reduce the massive calculation and storage
    in the Q-learning phase. The sensor’s utility function is proved to be bounded
    which can guarantee the convergence of the algorithm. As shown in the simulation
    results, the proposed scheme can converge after a certain number of iterations.
    Also, the proposed scheme significantly improves the system utility compared with
    the random action selection scheme. However, as the packet arrival rate increases,
    the system utility of the proposed scheme decreases since the sensor needs to
    consume more power to transmit all packets.'
  prefs: []
  type: TYPE_NORMAL
- en: Consuming more power leads to poor sensor’s performance due to its energy constraint,
    i.e., a shorter IoT system lifetime. The channel access problem in the energy
    harvesting-enabled IoT system is investigated in [[45](#bib.bib45)]. The model
    consists of one BS and energy harvesting-based sensors. The BS as a controller
    allocates channels to the sensors. However, the uncertainty of ambient energy
    availability at the sensors may make the channel allocation inefficient. For example,
    the channel allocated to the sensor with low available energy may not be fully
    utilized since the sensor cannot communicate later.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the BS’s problem is to predict the sensors’ battery states and select
    sensors for the channel access so as to maximize the total rate. Since the sensors
    are distributed randomly over a geographical area, the complete statistical knowledge
    of the system dynamics, e.g., the battery states and channel states, may not be
    available. Thus, the DQL is used to solve the problem of the BS, i.e., the agent.
    The DQL uses a DQN consisting of two LSTM-based neural network layers. The first
    layer generates the predicted battery states of sensors, and the second layer
    uses the predicted states along with Channel State Information (CSI) to determine
    the channel access policy. The state space consists of (i) channel access scheduling
    history, (ii) the history of predicted battery information, (iii) the history
    of the true battery information, and (iv) the current CSI of the sensors. The
    action space contains all sets of sensors to be selected for the channel access,
    and the reward is the difference between the total rate and the prediction error.
    As shown in the simulation results, the proposed scheme outperforms the myopic
    policy [[42](#bib.bib42)] in terms of total rate. Moreover, the battery prediction
    error obtained from the proposed scheme is close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The above schemes, e.g., [[39](#bib.bib39)] and [[45](#bib.bib45)], focus on
    the rate maximization. In IoT systems such as Vehicle-to-Vehicle (V2V) communications,
    latency also needs to be considered due to the mobility of V2V transmitters/receivers
    and vital applications in the traffic safety. One of the problems of each V2V
    transmitter is to select a channel and a transmit power level to maximize its
    capacity under a latency constraint. Given the decentralized network, a DQN is
    adopted to make optimal decisions as proposed in [[46](#bib.bib46)]. The model
    consists of V2V transmitters, i.e., agents, which share a set of channels. The
    actions of each V2V transmitter include choosing channels and transmit power levels.
    The reward is a function of the V2V transmitter’s capacity and latency. The state
    observed by the V2V transmitter consists of (i) the instantaneous CSI of the corresponding
    V2V link, (ii) the interference to the V2V link in the previous time slot, (iii)
    the channels selected by the V2V transmitter’ neighbors in the previous time slot,
    and (iv) the remaining time to meet the latency constraint. The state is also
    an input of the DQN. The output includes Q-values corresponding to the actions.
    As shown in the simulation results, by dynamically adjusting the power and channel
    selection when V2V links are likely to violate the latency constraint, the proposed
    scheme has more V2V transmitters meeting the latency constraint compared with
    the random channel allocation.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce spectrum cost, the above IoT systems often use unlicensed channels.
    However, this may cause the interference to existing networks, e.g., WLANs. The
    authors in [[47](#bib.bib47)] propose to use the DQN to jointly address the dynamic
    channel access and interference management. The model consists of Small Base Stations
    (SBSs) which share unlicensed channels in an LTE network. At each time slot, the
    SBS selects one of channels for transmitting its packet. However, there may be
    WLAN traffics on the selected channel, and thus the SBS accesses the selected
    channel with a probability. The actions of the SBS include pairs of channel selection
    and channel access probability. The problem of the SBS is to determine an action
    vector so as to maximize its total throughput, i.e., its utility, over all channels
    and time slots. The resource allocation problem can be formulated as a non-cooperative
    game, and the DQN using LSTM can be adopted to solve the game. The input of the
    DQN is the history traffic of the SBSs and the WLAN on the channels. The output
    includes predicted action vectors of the SBSs. The utility function of each SBS
    is proved to be convex, and thus the DQN-based algorithm converges to a Nash equilibrium
    of the game. The simulation results based on real traffic data from [[48](#bib.bib48)]
    show that the proposed scheme can improve the average throughput up to 28% compared
    with the standard Q-learning [[15](#bib.bib15)]. Moreover, deploying more SBSs
    in the LTE network does not allow more airtime fraction for the network. This
    implies that the proposed scheme can avoid causing performance degradation to
    the WLAN. However, the proposed scheme requires synchronization between the SBSs
    and the WLAN which is challenging in real networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the same cellular network context, the authors in [[22](#bib.bib22)] address
    the dynamic spectrum access problem for multiple users sharing $K$ channels. At
    a time slot, the user selects a channel with a certain attempt probability or
    chooses not to transmit at all. The state is the history of the user’s actions
    and its local observations, and the user’s strategy is mapping from the history
    to an attempt probability. The problem of the user is to find a vector of the
    strategies, i.e., the policy, over time slots to maximize its expected accumulated
    discounted data rate of the user.
  prefs: []
  type: TYPE_NORMAL
- en: The above problem is solved by training a DQN. The input of the DQN includes
    past actions and the corresponding observations. The output includes estimated
    Q-values of the actions. To avoid the overestimation in the Q-learning, the DDQN [[20](#bib.bib20)]
    is used. Moreover, the dueling DQN [[49](#bib.bib49)] is employed to improve the
    estimated Q-value. The DQN is then offline trained at a base station. Similar
    to [[47](#bib.bib47)], the multichannel random access is modeled as a non-cooperative
    game. As proved in [[22](#bib.bib22)], the game has a subgame perfect equilibrium.
    Note that some users can keep increasing their attempt probability to increase
    their rates. This makes the equilibrium point inefficient, and thus the strategy
    space of the users is restricted to avoid the situation. The simulation results
    show that the proposed scheme can achieve twice the channel throughput compared
    with the slotted-Aloha [[50](#bib.bib50)]. The reason is that in the proposed
    scheme, each user only learns from its local observation without an online coordination
    or carrier sensing. However, the proposed scheme requires the central unit which
    may raise the message exchanges as the training is frequently updated.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned models, the number of users is fixed in all time slots,
    and the arrival of new users is not considered. The authors in [[51](#bib.bib51)]
    address the channel allocation to new arrival users in a multibeam satellite system.
    The multibeam satellite system generates a geographical footprint subdivided into
    multiple beams which provide services to ground User Terminals (UTs). The system
    has a set of channels. If there exist available channels, the system allocates
    a channel to the new arrived UT, i.e., the new service is satisfied. Otherwise,
    the service is blocked. The system’s problem is to find a channel allocation decision
    to minimize the total service blocking probability of the new UT over time slots
    without causing the interference to the current UTs.
  prefs: []
  type: TYPE_NORMAL
- en: The system’s problem can be viewed as a temporal correlated sequential decision-making
    optimization problem which is effectively solved by the DQN. Here, the satellite
    system is the agent. The action is an index indicating which channel is allocated
    to the new arrived UT. The reward is positive when the new service is satisfied
    and is negative when the service is blocked. The state includes the set of current
    UTs, the current channel allocation matrix, and the new arrived UT. Note that
    the state has the spatial correlation feature due to the co-channel interference,
    and thus it can be represented in an image-like fashion, i.e., an image tensor.
    Therefore, the DQN adopts the CNN to extract useful features of the state. The
    simulation results show that the proposed DQN algorithm converges after a certain
    number of training steps. Also, by allocating available channels to the new arrived
    UTs, the proposed scheme can improve the system traffic up to 24.4% compared with
    the fixed channel allocation scheme. However, as the number of current UTs increases,
    the number of available channels is low or even zero. Therefore, the dynamic channel
    allocation decisions of the proposed scheme become meaningless, and the performance
    difference between the two schemes becomes insignificant. For the future work,
    a joint channel and power allocation algorithm based on the DQL can be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Joint User Association and Spectrum Access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The joint user association and spectrum access problems are typically non-convex.
    DQL is able to provide distributed solutions, and thus it can be effectively used
    to solve the problems without requiring complete and accurate network information.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[23](#bib.bib23)] consider a HetNet which consists of multiple
    users and BSs including macro base stations and femto base stations. The BSs share
    a set of orthogonal channels, and the users are randomly located in the network.
    The problem of each user is to select one BS and a channel to maximize its data
    rate while guaranteeing that the Signal-to-Interference-plus-Noise Ratio (SINR)
    of the user is higher than a minimum Qualtiy of Service (QoS) requirement. The
    DQL is adopted to solve the problem in which each user is an agent, and its state
    is a vector including QoS states of all users, i.e., the global state. Here, the
    QoS state of the user refers to whether its SINR exceeds the minimum QoS requirement
    or not. At each time slot, the user takes an action. If the QoS is satisfied,
    the user receives utility as its immediate reward. Otherwise, it receives a negative
    reward, i.e., an action selection cost. Note that the cumulative reward of one
    user depends on actions of other users, then the user’s problem can be defined
    as an MDP. Similar to [[22](#bib.bib22)], the DDQN and the dueling DQN are used
    to learn the optimal policy, i.e., the joint BS and channel selections, for the
    user to maximize its cumulative reward. The simulation results from [[23](#bib.bib23)]
    show that the proposed scheme outperforms the Q-learning implemented in [[15](#bib.bib15)]
    in terms of convergence speed and system capacity.
  prefs: []
  type: TYPE_NORMAL
- en: The scheme proposed in [[23](#bib.bib23)] is considered to be the first work
    using the DQL for the joint user association and spectrum access problem. Inspired
    by this work, the authors in [[52](#bib.bib52)] propose to use the DQL for a joint
    user association, spectrum access, and content caching problem. The network model
    is an LTE network which consists of UAVs serving ground users. The UAVs are equipped
    with storage units and can act as cached-enabled LTE-BSs. The UAVs are able to
    access both licensed and unlicensed bands in the network. The UAVs are controlled
    by a cloud-based server, and the transmissions from the cloud to the UAVs are
    implemented by using the licensed cellular band. The problem of each UAV is to
    determine (i) its optimal user association, (ii) the bandwidth allocation indicators
    on the licensed band, (iii) the time slot indicators on the unlicensed band, and
    (iv) a set of popular contents that the users can request to maximize the number
    of users with stable queue, i.e., users satisfied with content transmission delay.
  prefs: []
  type: TYPE_NORMAL
- en: The UAV’s problem is combinatorial and non-convex, and the DQL can be used to
    solve it. The UAVs do not know the users’ content requests, and thus the Liquid
    State Machine approach (LSM) [[53](#bib.bib53)] is adopted to predict the content
    request distribution of the users and to perform resource allocation. In particular,
    predicting the content request distribution is implemented at the cloud based
    on an LSM-based prediction algorithm. Then, given the request distributions, each
    UAV as an agent uses an LSM-based learning algorithm to find its optimal users
    association. Specifically, the input of the LSM-based learning algorithm consists
    of actions, i.e., UAV-user association schemes, that other UAVs take, and the
    output includes the expected numbers of users with stable queues corresponding
    to actions that the UAV can take. After the user association is done, the optimal
    content caching is determined based on the results of [[54](#bib.bib54), Theorem
    2], and the optimal spectrum allocation is done by using linear programming. Based
    on the Gordon’s Theorem [[55](#bib.bib55)], the proposed DQL is proved to converge
    with probability one. The simulation results using content request data from [[56](#bib.bib56)]
    show that the proposed DQL can converge in around 400 iterations. Compared with
    the Q-learning, the proposed DQN improves the convergence time up to 33% . Moreover,
    the proposed DQL significantly improves the number of users with stable queues
    up to 50% compared with the Q-learning without cache. In fact, energy efficiency
    is also important for the UAVs, and thus applying the DQL for a joint user association,
    spectrum access, and power allocation problem needs to be investigated.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Adaptive Rate Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4b7c81765a2c8c18dbfe075ff3e16099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: A dynamic adaptive streaming system based on HTTP standard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic Adaptive Streaming over HTTP (DASH) becomes the dominant standard for
    video streaming [[57](#bib.bib57)]. DASH is able to leverage existing content
    delivery network infrastructure and is compatible with a multitude of client-side
    applications. A general DASH system is shown in Fig. [6](#S3.F6 "Figure 6 ‣ III-B
    Adaptive Rate Control ‣ III Network Access and Rate Control ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey") in which
    the videos are stored in servers as multiple segments, i.e., chunks. Each segment
    is encoded at different compression levels to generate representations with different
    bitrates, i.e., different video visual quality. At each time slot, the client
    chooses a representation, i.e., a segment with a certain bitrate, to download.
    The client’s problem is to find an optimal policy which maximizes its QoE such
    as maximizing average bitrate and minimizing rebuffering, i.e., the time which
    the video playout freezes.'
  prefs: []
  type: TYPE_NORMAL
- en: As presented in [[58](#bib.bib58)], the above problem can be modeled as an MDP
    in which the agent is the client and the action is choosing a representation to
    download. To maximize the QoE, the reward is defined as a function of (i) visual
    quality of the video, (ii) video quality stability, (iii) rebuffering event, and
    (iv) buffer state. Given the reward formulation, the state of the client should
    include (i) the video quality of the last downloaded segment, (ii) the current
    buffer state, (iii) the rebuffering time, and (iv) the channel capacities experienced
    during downloading of segments in the past time slots. The MDP can be solved by
    using dynamic programming, but the computational complexity rapidly becomes unmanageable
    as the size of the problem increases. Thus, the authors in [[58](#bib.bib58)]
    adopt the DQL to solve the problem. Similar to [[45](#bib.bib45)], the LSTM networks
    are used in which the input is the state of the client, and the output includes
    Q-values corresponding to the client’s possible actions. To improve the performance
    of the standard LSTM, peephole connections are added into the LSTM networks. The
    simulation results based on dataset from [[59](#bib.bib59)] show that the proposed
    DQL algorithm can converge much faster than Q-learning. Moreover, the proposed
    DQL improves the video quality and reduces the rebuffering since it is able to
    dynamically manage the buffer by considering the buffer state and channel capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network model and the optimization problem in [[58](#bib.bib58)] are also
    found in [[60](#bib.bib60)]. However, different from [[58](#bib.bib58)], the authors
    in [[60](#bib.bib60)] adopt the Asynchronous Advantage Actor- Critic (A3C) method [[26](#bib.bib26)]
    for the DQL to further enhance and speed up the training. As presented in Section [II-F1](#S2.SS6.SSS1
    "II-F1 Deep Deterministic Policy Gradient Q-Learning for Continuous Action ‣ II-F
    Deep Q-Learning for Extensions of MDPs ‣ II Deep Reinforcement Learning: An Overview
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"), A3C includes two neural networks, namely, actor network and critic
    network. The actor network is to choose bitrates for the client, and the critic
    network helps train the actor network. For the actor network, the input is the
    client’s state, and the output is a policy, i.e., a probability distribution over
    possible actions given states that the client can take. Here, the action is choosing
    the next representation, i.e., the next segment with a certain bitrate, to download.
    For the critic network, the input is the client’s state, and the output is the
    expected total reward when following the policy obtained from the actor network.
    The simulation results based on the mobile dataset from [[61](#bib.bib61)] show
    that the proposed DQL can improve the average QoE up to 25% compared with the
    bitrate control scheme [[62](#bib.bib62)]. Also, by having sufficient buffer to
    handle the network’s throughput fluctuations, the proposed DQL reduces the rebuffering
    around 32.8% compared with the baseline scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the DQL algorithm proposed in [[60](#bib.bib60)] can be easily
    deployed in a multi-client network since A3C is able to support parallel training
    for multiple agents. Accordingly, each client, i.e., an agent, is configured to
    observe its reward. Then, the client sends a tuple including its state, action,
    and reward to a server. The server uses the actor-critic algorithm to update its
    actor network model. The server then pushes the newest model to the agent. This
    update process can happen asynchronously among all agents which improves quality
    and speeds up the training. Although the parallel training scheme may incur a
    Round-Trip Time (RTT) between the clients and the server, the simulation results
    in [[60](#bib.bib60)] show that the RTT between the clients and the server reduces
    the average QoE by only 3.5%. The performance degradation is small, and thus the
    proposed DQL can be implemented in real network systems.
  prefs: []
  type: TYPE_NORMAL
- en: In [[58](#bib.bib58)] and [[60](#bib.bib60)], the input of the DQL, i.e., the
    client’s state, includes the video quality of the last downloaded video segment.
    The video segment is raw which may cause “state explosion” to the state space [[63](#bib.bib63)].
    To reduce the state space and to improve the QoE, the authors in [[63](#bib.bib63)]
    propose to use a video quality prediction network. The prediction network extracts
    useful features from the raw video segments using CNN and RNN. Then, the output
    of the prediction network, i.e., the predicted video quality, is used as one of
    the inputs of the DQL which is proposed in [[60](#bib.bib60)]. The simulation
    results based on the broadband dataset from [[64](#bib.bib64)] show that the proposed
    DQL can improve the average QoE up to 25% compared with the Google Hangout, i.e.,
    a communication platform developed by Google. Moreover, the proposed DQL can reduce
    the average latency of video transmission around 45% due to the small state space.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the DASH systems, the DQL can be effectively used for the rate control
    in High Volume Flexible Time (HVFT) applications. HVFT applications use cellular
    networks to deliver IoT traffic. The HVFT applications have a large volume of
    traffic, and the traffic scheduling, e.g., data rate control, in the HVFT applications
    is necessary. One common approach is to assign static priority classes per traffic
    type, and then traffic scheduling is based on its priority class. However, such
    an approach does not evolve to accommodate new traffic classes. Thus, learning
    methods such as DQL should be used to provide adaptive rate control mechanisms
    as proposed in [[65](#bib.bib65)]. The network model is a single cell including
    one BS as a central controller and multiple mobile users. The problem at the BS
    is to find a proper policy, i.e., data rate for the users, to maximize the amount
    of transmitted HVFT traffic while minimizing performance degradation to existing
    data traffics. It is shown in [[65](#bib.bib65)] that the problem can be formulated
    as an MDP. The agent is the BS, and the state includes the current network state
    and the useful features extracted from network states in the past time slots.
    The network state at a time slot includes (i) the congestion metric, i.e., the
    cell’s traffic load, at the time slot, (ii) the total number of network connections,
    and (iii) the cell efficiency, i.e., the cell quality. The action that the BS
    takes is a combination of the traffic rate for the users. To achieve the BS’ objective,
    the reward is defined as a function of (i) the sum of HVFT traffic, (ii) traffic
    loss to existing applications due to the presence of the HVFT traffic, and (iii)
    the amount of bytes served below desired minimum throughput. The DQL using the
    actor and critic networks with LSTM is then adopted. By using the real network
    data collected in Melbourne, the simulation results show that the proposed DQL
    increases the HVFT traffic up to 2 times compared with the heuristic control scheme.
    However, how the proposed scheme reduces the traffic loss is not shown.
  prefs: []
  type: TYPE_NORMAL
- en: In the aforementioned approaches, the maximum number of objectives is constrained,
    e.g., to 3 in [[66](#bib.bib66)]. The authors in [[67](#bib.bib67)] show that
    the DQL can be used for the rate control to achieve multiple objectives in complex
    communication systems. The network model is a future space communication system
    which is expected to operate in unpredictable environments, e.g., orbital dynamics,
    atmospheric and space weather, and dynamic channels. In the system, the transmitter
    needs to be configured with several transmit parameters, e.g., symbol rate and
    encoding rate, to achieve multiple conflict objectives, e.g., low Bit Error Rate
    (BER), throughput improvement, power and spectral efficiency. The adaptive coding
    and modulation schemes, i.e., [[68](#bib.bib68)], can be used. However, the methods
    allow to achieve only limited number objectives. Learning algorithms such as the
    DQL can be thus used. The agent is the transmitter in the system. The action is
    a combination of (i) symbol rate, (ii) energy per symbol, (iii) modulation mode,
    (iv) number of bits per symbol, and (v) encoding rate. The objective is to maximize
    the system performance. Thus, the reward is defined as a fitness function of performance
    parameters including (i) BER estimated at the receiver, (ii) throughput, (iii)
    spectral efficiency, (iv) power consumption, and (v) transmit power efficiency.
    The state is the system performance measured by the transmitter, and thus the
    state is the reward. To achieve multiple objectives, the DQL is implemented by
    using a set of multiple neural networks in parallel. The input of the DQL is the
    current state and the channel conditions, and the output is the predicted action.
    The neural networks are trained by using the Levenberg-Marquardt backpropagation
    algorithm [[69](#bib.bib69)]. The simulation results show that the proposed DQL
    can achieve the fitness score, i.e., the weighted sum of different objectives,
    close to the ideal, i.e., the exhaustive search approach. This implies that the
    DQL is able to select near-optimal actions and learn the relationship between
    rewards and actions given dynamic channel conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: This section reviews applications of DQL for the dynamic network access
    and adaptive rate control. The reviewed approaches are summarized along with the
    references in Table [III](#S3.T3 "TABLE III ‣ III-B Adaptive Rate Control ‣ III
    Network Access and Rate Control ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey"). We observe that the problems are
    mostly modeled as an MDP. Moreover, DQL approaches for the IoT and DASH systems
    receive more attentions than other networks. Future networks, e.g., 5G networks,
    involve multiple network entities with multiple conflicting objectives, e.g.,
    provider’s revenue versus users’ utility maximization. This poses a number of
    challenges to the traditional resource management mechanisms that deserve in-depth
    investigation. In the next section, we review the adoption of DQL for the emerging
    services, i.e., offloading and caching.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: A summary of approaches using DQL for network access and adaptive
    rate control.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  prefs: []
  type: TYPE_TB
- en: '| Network access | [[39](#bib.bib39)] | POMDP | DQN using FNN | Sensor | Past
    channel selections and observations | Channel selection | Score +1 or -1 | IoT
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[44](#bib.bib44)] | MDP | DQN using FNN | Sensor | Current buffer state
    and channel state | Channel, packets, and modulation mode selection | Ratio of
    number of transmitted packets to transmit power | IoT |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[45](#bib.bib45)] | MDP | DQN with LSTM | Base station | Channel access
    history, predicted and true battery information history, and current CSI | Sensor
    selection for channel access | Total rate and prediction error | IoT |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[46](#bib.bib46)] | MDP | DQN with LSTM | V2V transmitter | Current CSI,
    past interference, past channel selections, and remaining time to meet the latency
    constraints | Channel and transmit power selection | Capacity and latency | IoT
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[47](#bib.bib47)] | Game | DQN with LSTM | Small base station | Traffic
    history of small base stations and the WLAN | Channel selection and channel access
    probability | Throughput | LTE network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[22](#bib.bib22)] | Game | DDQN and dueling DQN | Mobile user | Past
    channel selections and observations | Channel selection | Data rate | CRN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[51](#bib.bib51)] | MDP | DQN with CNN | Satellite system | Current user
    terminals, channel allocation matrix, and the new arrival user | Channel selection
    | Score +1 or -1 | Satellite system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[23](#bib.bib23)] | MDP | DDQN and dueling DQN | Mobile user | QoS states
    | Base station and channel selection | Utility | HetNet |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[52](#bib.bib52)] | Game | DQN with LSM | UAV | Content request distribution
    | Base station selection | Users with stable queues | LTE network |'
  prefs: []
  type: TYPE_TB
- en: '| Rate control | [[58](#bib.bib58)] | MDP | DQN with LSTM and peephole connections
    | Client | Last segment quality, current buffer state, rebuffering time, and channel
    capacities | Bitrate selection for segment | Video quality, rebuffering even,
    and buffer state | DASH system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[60](#bib.bib60)] | MDP | DQN with A3C | Client | Last segment quality,
    current buffer state, rebuffering time, and channel capacities | Bitrate selection
    for segment | Video quality, rebuffering even, and buffer state | DASH system
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[63](#bib.bib63)] | MDP | DQN with CNN and RNN | Client | Predicted video
    quality, current buffer state, rebuffering time, and channel capacities | Bitrate
    selection for segment | Video quality, rebuffering even, and buffer state | DASH
    system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[65](#bib.bib65)] | MDP | DQN using A3C and LSTM | Base station | Congestion
    metric, current network connections, and cell efficiency | Traffic rate decisions
    for mobile users | HVFT traffic, traffic loss to existing applications, and the
    amount of served bytes | HVFT application |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[67](#bib.bib67)] | MDP | DQN using FNN | Base station | Measurements
    of BER, throughput, spectral efficiency, power consumption, and transmit power
    efficiency | Symbol rate, energy per symbol, modulation mode, number of bits per
    symbol, and encoding rate | Same as the state | Space communication system |'
  prefs: []
  type: TYPE_TB
- en: IV Caching and Offloading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As one of the key features of information-centric networking, in-network caching
    can efficiently reduce duplicated content transmissions. The studies on wireless
    caching has shown that access delays, energy consumption, and the total amount
    of traffic can be reduced significantly by caching contents in wireless devices.
    Big data analytics [[70](#bib.bib70)] also demonstrate that with limited cache
    size, proactive caching at network edge nodes can achieve 100% user satisfaction
    while offloading 98% of the backhaul traffic. Joint content caching and offloading
    can address the gap between the mobile users’ large data demands and the limited
    capacities in data storage and processing. This motivates the study on Mobile
    Edge Computing (MEC). By deploying both computational resources and caching capabilities
    close to end users, MEC significantly improves energy efficiency and QoS for applications
    that require intensive computations and low latency. A unified study on caching,
    offloading, networking, and transmission control in MEC scenarios involves very
    complicated system analysis because of strong couplings among mobile users with
    heterogeneities in application demand, QoS provisioning, mobility pattern, radio
    access interface, and wireless resources. A learning-based and model-free approach
    becomes a promising candidate to manage huge state space and optimization variables,
    especially by using DNNs. In this section, we review the modeling and optimization
    of caching and offloading policies in wireless networks by leveraging the DRL
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Wireless Proactive Caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wireless proactive caching has attracted great attentions from both academia
    and industry. Statistically, a few popular contents are usually requested by many
    users during a short time span, which accounts for most of the traffic load. Therefore,
    proactively caching popular contents can avoid the heavy traffic burden of the
    backhaul links. In particular, this technique aims at pre-caching the contents
    from the remote content servers at the edge devices or BSs that are close to the
    end users. If the requested contents are already cached locally, the BS can directly
    serve the end users with small delay. Otherwise, the BS requests these contents
    from the original content server and updates the local cache based on the caching
    policy, which is one of the main design problem for wireless proactive caching.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 QoS-Aware Caching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Content popularity is the key factor used to solve the content caching problem.
    With a large number of contents and their time-varying popularities, DQL is an
    attractive strategy to tackle this problem with high-dimensional state and action
    spaces. The authors in [[70](#bib.bib70)] present a DQL scheme to improve the
    caching performance. The system model consists of a single BS with a fixed cache
    size. For each request, the BS as an agent makes a decision on whether or not
    to store the currently requested content in the cache. If the new content is kept,
    the BS determines which local content will be replaced. The state is the feature
    space of the cached contents and the currently requested content. The feature
    space consists of the total number of requests for each content in a specific
    short-, medium-, and long-term. There are two types of actions: (i) to find a
    pair of contents and exchange the cache states of the two contents and (ii) to
    keep the cache states of the contents unchanged. The aim of the BS is to maximize
    the long-term cache hit rate, i.e., reward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DQL scheme in [[70](#bib.bib70)] trains the policy by using the DDPG method [[71](#bib.bib71)]
    and employs Wolpertinger architecture [[72](#bib.bib72)] to reduce the size of
    the action space and avoid missing an optimal policy. The Wolpertinger architecture
    consists of three main parts: an actor network, K-Nearest Neighbors (K-NN), and
    a critic network. The actor network is to avoid a large action space. The critic
    network is to correct the decision made by the actor network. The DDPG method
    is applied to update both critic and actor networks. K-NN can help to explore
    a set of actions to avoid poor decisions. The actor and critic networks are then
    implemented by using FNNs. The simulation results show that the proposed DQL scheme
    outperforms the first-in first-out scheme in terms of long-term cache hit rate.'
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing the long-term cache hit rate in [[70](#bib.bib70)] implies that the
    cache stores the most popular contents. In a dynamic environment, contents stored
    in a cache have to be replaced according to the users’ dynamic requests. An optimization
    of the placement or replacement of cached contents is studied in [[73](#bib.bib73)]
    by a deep learning method. The optimization algorithm is trained by a DNN in advance
    and then used for real-time caching or scheduling with minimum delay. The authors
    in [[74](#bib.bib74)] propose an optimal caching policy to learn the cache expiration
    times, i.e., Time-To-Live (TTL), for dynamically changing requests in content
    delivery networks. The system includes a cloud database server and multiple mobile
    devices that can issue queries and update entries in a single database. The query
    results can be cached for a specified time interval at server-controlled caches.
    All cached queries will become invalid if one of the cached records has been updated.
    A large TTL will strain cache capacities while a small TTL increases latencies
    significantly if the database server is physically remote.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the DDPG approach used in [[70](#bib.bib70)], the authors in [[74](#bib.bib74)]
    propose to utilize Normalized Advantage Functions (NAFs) for continuous DQL scheme
    to learn optimal cache expiration duration. The key problem in continuous DQL
    is to select an action maximizing the Q-function, while avoiding performing a
    costly numerical optimization at each step. The use of NAFs obviates a second
    actor network that needs to be trained separately. Instead, a single neural network
    is used to output both a value function and an advantage term. The DQL agent at
    the cloud database uses an encoding of a query itself and the query miss rates
    as the system states, which allows for an easier generalization. The system reward
    is linearly proportional to the current load, i.e., the number of cached queries
    divided by the total capacity. This reward function can encourage longer TTLs
    when fewer queries are cached, and shorter TTLs when the load is close to the
    system capacity. Considering incomplete measurements for rewards and next-states
    at run-time, the authors introduce the Delayed Experience Injection (DEI) approach
    that allows the DQL agent to keep track of incomplete transitions when measurements
    are not immediately available. The authors evaluate the learning algorithm by
    Yahoo! cloud serving benchmark with customized web workloads [[75](#bib.bib75)].
    The simulation results verify that the learning approach based on NAFs and DEI
    outperforms a statistical estimator.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Joint Caching and Transmission Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The caching policies determine where to store and retrieve the requested content
    efficiently, e.g., by learning the contents’ popularities [[70](#bib.bib70)] and
    cache expiration time [[74](#bib.bib74)]. Another important aspect of caching
    design is the transmission control of the content delivery from caches to end
    users, especially for wireless systems with dynamic channel conditions. To avoid
    mutual interference in multi-user wireless networks, the transmission control
    decides which cached contents can be transmitted concurrently as well as the most
    appropriate control parameters, e.g., transmit power, precoding, data rate, and
    channel allocation. Hence, the joint design of caching and transmission control
    is required to enable efficient content delivery in multi-user wireless networks.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] propose
    a DQL framework to address the joint caching and interference alignment to tackle
    mutual interference in multi-user wireless networks. The authors consider an MIMO
    system with limited backhaul capacity and the caches at the transmitter. The precoding
    design for interference alignment requires the global CSI at each transmitter.
    A central scheduler is responsible for collecting CSI and cache status from each
    user via the backhaul, scheduling the users’ transmission, and optimizing the
    resource allocation. By enabling content caching at individual transmitters, we
    can decrease the demand for data transfer and thus save more backhaul capacity
    for real-time CSI update and sharing. Using the DQL-based approach at the central
    scheduler can reduce the explicit demand for CSI and the computational complexity
    in matrix optimization, especially with time-varying channel conditions. The DQL
    agent implements the DNN to approximate the Q-function with experience replay
    in training. To make the learning process more stable, the target Q-network parameter
    is updated by the Q-network for every a few time instants. The collected information
    is assembled into a system state and sent to the DQL agent, which feeds back an
    optimal action for the current time instant. The action indicates which users
    to be active, and the resource allocation among active users. The system reward
    represents the total throughput of multiple users. An extended work of [[76](#bib.bib76)]
    and [[77](#bib.bib77)] with a similar DQL framework is presented in [[78](#bib.bib78)],
    in which a CNN-based DQN is adopted and evaluated in a more practical conditions
    with imperfect or delayed CSI. Simulation results show that the performance of
    the MIMO system is significantly improved in terms of the total throughput and
    energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Interference management is an important requirement of wireless systems. The
    application-related QoS or user experience is also an essential metric. Different
    from [[76](#bib.bib76), [78](#bib.bib78), [77](#bib.bib77)], the authors in [[79](#bib.bib79)]
    propose a DQL approach to maximize Quality of Experience (QoE) of IoT devices
    by jointly optimizing the cache allocation and transmission rate in content-centric
    wireless networks. The system state is specified by the nodes’ caching conditions,
    e.g., the service information and cached contents, as well as the transmission
    rates of the cached contents. The aim of the DQL agent is to minimize continuously
    the network cost or maximize the QoE. The proposed DQL framework is further enhanced
    with the use of PER and DDQN. PER replays important transitions more frequently
    so that DQN can learn from samples more efficiently. The use of DDQN can stabilize
    the learning by providing two value functions in separated neural networks. This
    avoids an overestimation of the DQN with the increasing number of actions. These
    two neural networks are not completely decoupled as the target network is a periodic
    copy of estimation network. A discrete simulator ccnSim [[80](#bib.bib80)] is
    used to model the caching behavior in various graph structures. The output data
    trace of the simulator is then imported to Matlab and used to evaluate the learning
    algorithm. The simulation results show that the DQL framework by using PER and
    DDQN outperforms the standard penetration test scheme in terms of QoE.
  prefs: []
  type: TYPE_NORMAL
- en: The QoE can be used to characterize the users’ perception of Virtual Reality
    (VR) services. The authors in [[81](#bib.bib81)] address the joint content caching
    and transmission strategy in a wireless VR network, where UAVs capture videos
    on live games and transmit them to small-cell BSs servicing the VR users. Millimeter
    wave (mmWave) downlink backhaul links are used for VR content transmission from
    the UAVs to BSs. The BSs can also cache the popular contents that may be requested
    frequently by end users. The joint content caching and transmission problem is
    formulated as an optimization to maximize the users’ reliability, i.e., the probability
    that the content transmission delay satisfies the instantaneous delay target.
    The maximization involves the control of transmission format, users’ association,
    the set and format of cached contents. A DQL framework combining the Liquid State
    Machine (LSM) and Echo State Network (ESN) is proposed for each BS to find the
    optimal transmission and caching strategies. As a randomly generated spiking neural
    network [[82](#bib.bib82)], LSM can store information about the network environment
    over time and adjust the users’ association policy, cached contents and formats
    according to the users’ content requests. It has been used in [[83](#bib.bib83)]
    to predict the users’ content request distribution while having only limited information
    regarding the network and different users. Conventional LSM uses FNNs as the output
    function, which demands high complexity in training due to the computation of
    gradients for all of the neurons. Conversely, the proposed DQL framework uses
    an ESN as the output function, which uses historical information to find the relationship
    between the users’ reliability, caching, and content transmission. It also has
    a lower complexity in training and a better memory for network information. Simulation
    results show that the proposed DQL framework can yield 25.4% gain in terms of
    users’ reliability compared to the baseline Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 Joint Caching, Networking, and Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Caching and transmission control will become more involved in a HetNet that
    integrates different communication technologies, e.g., cellular system, device-to-device
    network, vehicular network, and networked UAVs, to support various application
    demands. The network heterogeneity raises the problem of complicated system design
    that needs to address challenging issues such as mutual interference, differentiated
    QoS provisioning, and resource allocation, hopefully in a unified framework. Obviously
    this demands a joint optimization far beyond the extent of joint caching and transmission
    control.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, the authors in [[84](#bib.bib84)] propose a DQL framework for energy-efficient
    resource allocation in green wireless networks, jointly considering the couplings
    among networking, in-network caching and computation. The system consists of a
    Software-Defined Network (SDN) with multiple virtual networks and mobile users
    requesting for video on-demand files that require a certain amount of computational
    resource at either the content server or at local devices. In each virtual network,
    an authorized user issues a request to download files from a set of available
    SBSs in its neighborhood area. The wireless channels between each mobile user
    and the SBSs are characterized as Finite-State Markov Channels (FSMC). The states
    are the available cache capacity at the SBSs, the channel conditions between mobile
    users and SBSs, the computational capability of the content servers and mobile
    users. The DQL agent at each SBS decides an association between each mobile user
    and SBS, where to perform the computational task, and how to schedule the transmissions
    of SBSs to deliver the required data. The objective is to minimize the total energy
    consumption of the system from data caching, wireless transmission, and computation.
  prefs: []
  type: TYPE_NORMAL
- en: The DQL scheme proposed in [[84](#bib.bib84)] has been applied to improve the
    performance of Vehicular Ad doc NETworks (VANETs) in [[85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87)]. The network model includes multiple BSs, Road Side Units (RSUs),
    MEC servers, and content servers. All devices are controlled by a mobile virtual
    network operator. The vehicles request for video contents that can be cached at
    the BSs or retrieved from remote content servers. The authors in [[85](#bib.bib85)]
    formulate the resource allocation problem as a joint optimization of caching,
    networking, and computing, e.g., compressing and encoding operations of the video
    contents. The system states include the CSI from each BS, the computational capability,
    and cache size of each MEC/content server. The network operator feeds the system
    state to the FNN-based DQN and gets the optimal policy that determines the resource
    allocation for each vehicle. To exploit spatial correlations in learning, the
    authors in [[86](#bib.bib86)] enhance Q-learning by using CNNs in DQN. This makes
    it possible to extract high-level features from raw input data. Two schemes have
    been introduced in [[87](#bib.bib87)] to improve stability and performance of
    the ordinary DQN method. Firstly, DDQN is designed to avoid over-estimation of
    Q-value in ordinary DQN. Hence, the action can be decoupled from the target Q-value
    generation. This makes the training process faster and more reliable. Secondly,
    the dueling DQN approach is also integrated in the design with the intuition that
    it is not always necessary to estimate the reward by taking some action. The state-action
    Q-value in dueling DQN is decomposed into one value function representing the
    reward in the current state, and the advantage function that measures the relative
    importance of a certain action compared with other actions. The enhanced DQL agent
    combining these two schemes can achieve better performance and faster training
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the huge action space and high complexity with the vehicle’s mobility
    and service delay deadline $T_{d}$, a multi-time scale DQN framework is proposed
    in [[88](#bib.bib88)] to minimize the system cost by the joint design of communication,
    caching and computing in VANET. The policy design accounts for limited storage
    capacities and computational resources at the vehicles and the RSUs. The small
    timescale DQN is for every time slot and aims to maximize the exact immediate
    reward. Additionally, the large timescale DQN is designed for every $T_{d}$ time
    slots within the service delay deadline, and used to estimate the reward considering
    the vehicle’s mobility in a large timescale.
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned DQL framework for VANETs, e.g., [[86](#bib.bib86), [85](#bib.bib85),
    [87](#bib.bib87)], has also been generalized to smart city applications in [[89](#bib.bib89)],
    which necessitates dynamic orchestration of networking, caching, and computation
    to meet different servicing requirements. Through Network Function Virtualization
    (NFV) [[90](#bib.bib90)], the physical wireless network in smart cities can be
    divided logically into several virtual ones by the network operator, which is
    responsible for network slicing and resource scheduling, as well as allocation
    of caching and computing capacities. The use cases in smart cities are presented
    in [[91](#bib.bib91), [92](#bib.bib92)], which apply the generalized DQL framework
    to improve the security and efficiency for trust-based data exchange, sharing,
    and delivery in mobile social networks through the resource allocation and optimization
    of MEC allocation, caching, and D2D (Device-to-Device) networking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f10303b4ec79a63c805db54b8fd1e356.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Joint caching, networking, and transmission control to optimize cache
    hit rate [[70](#bib.bib70)], cache expiration time [[74](#bib.bib74)], interference
    alignment [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)], Quality of Experience [[79](#bib.bib79),
    [81](#bib.bib81)], energy efficiency [[84](#bib.bib84)], resource allocation [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)], traffic latency, or redundancy [[89](#bib.bib89),
    [91](#bib.bib91)].'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Data and Computation Offloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With limited computation, memory and power supplies, IoT devices such as sensors,
    wearable devices, and handheld devices become the bottleneck to support advanced
    applications such as interactive online gaming and face recognition. To address
    such a challenge, IoT devices can offload the computational tasks to nearby MEC
    servers, integrated with the BSs, Access Points (APs), and even neighboring Mobile
    Users (MUs). As a result, data and computation offloading can potentially reduce
    the processing delay, save the battery energy, and even enhance security for computation-intensive
    IoT applications. However, the critical problem in the computation offloading
    is to determine the offloading rate, i.e., the amount of computational workload,
    and choose the MEC server from all available servers. If the chosen MEC server
    experiences heavy workloads and degraded channel conditions, it may take even
    longer time for the IoT devices to offload data and receive the results from the
    MEC server. Hence, the design of an offloading policy has to take into account
    the time-varying channel conditions, user mobility, energy supply, computation
    workload and the computational capabilities of different MEC servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors in [[93](#bib.bib93)] focus on minimizing the mobile user’s cost
    and energy consumption by offloading cellular traffic to WLAN. Each mobile user
    can either access the cellular network, or the complimentary WLAN as illustrated
    in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching
    and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(a), but with different monetary costs. The mobile user
    also has to pay a penalty if the data transmission does not finish before the
    deadline. The mobile user’s data offloading decision can be modeled as an MDP.
    The system state includes the mobile user’s location and the remaining file size
    of all data flows. The mobile user will choose to transmit data through either
    WLAN or cellular network, and decide how to allocate channel capacities to concurrent
    flows. Without knowing the mobility pattern in advance, the DQL is proposed for
    each mobile user to learn the optimal offloading policy from past experiences.
    CNNs are employed in the DQL to predict a continuous value of the mobile user’s
    remaining data. Simulation results reveal that the DQN-based scheme generally
    outperforms the dynamic programming algorithm for the MDP. The reason is that
    the DQN can learn from experience while the dynamic programming algorithm cannot
    obtain the optimal policy with incorrect transition probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The allocation of limited computational resources at the MEC server is critical
    for cost and energy minimization. The authors in [[94](#bib.bib94)] consider an
    MEC-enabled cellular system, in which multiple mobile users can offload their
    computational tasks via wireless channels to one MEC server, co-located with the
    cellular BS as shown in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation
    Offloading ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")(b). Each mobile user has a computational-intensive
    task, characterized by the required computational resources, CPU cycles, and the
    maximum tolerable delay. The capacity of the MEC server is limited to accommodate
    all mobile users’ task loads. The bandwidth sharing between different mobile users’
    offloading also affects the overall delay performance and energy consumptions.
    The DQL is used to minimize the cost of delay and power consumptions for all mobile
    users, by jointly optimizing the offloading decision and computational resource
    allocation. The system states include the sum of cost of the entire system and
    the available computational capacity of the MEC server. The action of BS is to
    determine the resource allocation and offloading decision for each mobile user.
    To limit the size of action space, a pre-classification step is proposed to check
    the mobile users’ feasible set of actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to [[94](#bib.bib94)], multiple BSs in an ultra-dense network is
    considered in [[95](#bib.bib95)] and [[96](#bib.bib96)], as shown in Fig. [8](#S4.F8
    "Figure 8 ‣ IV-B Data and Computation Offloading ‣ IV Caching and Offloading ‣
    Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")(c), with the objective of minimizing the long-term cost of delay in
    computation offloading. All computational tasks are offloaded to the shared MEC
    server via different BSs. Besides the allocation of computational resources and
    transmission control, the offloading policy also has to optimize the association
    between mobile users and the BSs. With dynamic network conditions, the mobile
    users’ decision-making can be formulated as an MDP. The system states are the
    channel conditions between the mobile user and the BSs, the states of energy and
    task queues. The cost function is defined as a weighted sum of the execution delay,
    the handover delay and the computational task dropping cost. The authors in [[96](#bib.bib96)]
    firstly propose a DDQN-based DQL algorithm to learn the optimal offloading policy
    without knowing the network dynamics. By leveraging the additive structure of
    the utility function, the Q-function decomposition combined with the DDQN further
    leads to a novel online SARSA-based DRL algorithm. Numerical experiments show
    that the new algorithm achieves a significant improvement in computation offloading
    performance compared with the baseline policies, e.g., the DQN-based DQL algorithm
    and some heuristic offloading strategies without learning. The high density of
    SBSs can relieve the data offloading pressure in peak traffic hours but consume
    a large amount of energy in off-peak time. Therefore, the authors in [[97](#bib.bib97)],
    [[98](#bib.bib98)], and [[99](#bib.bib99)] propose a DQL-based strategy for controlling
    the (de)activation of different SBSs to minimize the energy consumption without
    compromising the quality of provisioning. In particular, in [[97](#bib.bib97)],
    the on/off decision framework uses a DQL scheme to approximate both the policy
    and value functions in an actor-critic method. The reward of the DQL agent is
    defined as a cost function relating to energy consumption, QoS degradation, and
    the switching cost of SBSs. The DDPG approach is also employed together with an
    action refinement scheme to expedite the training process. Through extensive numerical
    simulations, the proposed scheme is shown to greatly outperform other baseline
    methods in terms of both energy and computational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: With a similar model to that in [[96](#bib.bib96)], computation offloading finds
    a proper application for cloud-based malware detection in [[100](#bib.bib100)].
    A review of the threat models and the RL-based solutions for security and privacy
    protection in mobile offloading and caching are discussed in [[101](#bib.bib101)].
    With limited energy supply, computational resources, and channel capacity, mobile
    users cannot always update the local malware database and process all application
    data in time and thus are vulnerable to zero-day attacks [[102](#bib.bib102)].
    By leveraging the remote MEC server, all mobile users can offload their application
    data and detection tasks via different BSs to the MEC/security server with larger
    and more sophisticated malware database, more computational capabilities, and
    powerful security services. This can be modeled by a dynamic malware detection
    game in which multiple mobile users interact with each other in resource competition,
    e.g., the allocation of wireless channel capacities and the computational capabilities
    of the MEC/security server. A DQL scheme is proposed for each mobile user to learn
    its offloading data rate to the MEC/security server. The system states include
    the channel state and the size of application traces. The objective is to optimize
    the detection accuracy of the security server, which is defined as a concave function
    in the total amount of malware samples. The Q-value is estimated by using a CNN
    in the DQL framework. The authors also propose the hotbooting Q-learning technique
    that provides a better initialization for Q-learning by exploiting the offloading
    experiences in similar scenarios. It can save exploration time at the initial
    stage and accelerate the learning speed compared with a standard Q-learning algorithm
    with all-zero initialization of the Q-value [[103](#bib.bib103)]. The proposed
    DQL scheme not only improves the detection speed and accuracy, but also increases
    the mobile users’ battery life. The simulation results reveal that compared with
    the hotbooting Q-learning, the DQL-based malware detection has the faster learning
    rate, the higher accuracy, and the shorter detection delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple MEC servers have been considered in [[104](#bib.bib104), [105](#bib.bib105)],
    as illustrated in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation Offloading
    ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey")(d). The authors in [[104](#bib.bib104)] aim to design
    optimal offloading policy for IoT devices with energy harvesting capabilities.
    The system consists of multiple MEC servers, such as BSs and APs, with different
    capabilities in computation and communications. The IoT devices are equipped with
    energy storage and energy harvesters. They can execute computational tasks locally
    and offload the tasks to the MEC servers. The IoT device’s offloading decision
    can be formulated as an MDP. The system states include the battery status, the
    channel capacity, and the predicted amount of harvested energy in the future.
    The IoT device evaluates the reward based on the overall delay, energy consumption,
    the task drop loss and the data sharing gains in each time slot. Similar to [[100](#bib.bib100)],
    the authors in [[104](#bib.bib104)] enhance Q-learning by the hotbooting technique
    to save the random exploration time at the beginning of learning. The authors
    also propose a fast DQL offloading scheme that uses hotbooting to initialize the
    CNN and accelerates the learning speed. The authors in [[105](#bib.bib105)] view
    the MEC-enabled BSs as different physical machines constituting a part of the
    cloud resources. The cloud optimizes the MUs’ computation offloading to different
    virtual machines residing on the physical machines. A two-layered DQL algorithm
    is proposed for the offloading problem to maximize the utilization of cloud resources.
    The system state relates to the waiting time of each computational task and the
    number of virtual machines. The first layer is implemented by a CNN-based DQL
    framework to estimate an optimal cluster for each computational task. Different
    clusters of physical machines are generated based on the K-NN algorithm. The second
    layer determines the optimal serving physical machine within the cluster by Q-learning
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a21b14fa5bfaf174d7a4626767736c40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Data/computation offloading models in cellular networks: (a) Offloading
    cellular traffic to WLAN [[93](#bib.bib93)], (b) Offloading to a single MEC-enabled
    BS [[94](#bib.bib94)], (c) Offloading to one shared MEC server via multiple BSs [[95](#bib.bib95),
    [96](#bib.bib96), [100](#bib.bib100)], (d) Offloading to multiple MEC-enabled
    BSs [[104](#bib.bib104), [105](#bib.bib105)] and mobile cloudlets [[106](#bib.bib106),
    [107](#bib.bib107)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned works all focus on data or computation offloading in cellular
    system via BSs to remote MEC servers, e.g., [[93](#bib.bib93), [94](#bib.bib94),
    [95](#bib.bib95), [96](#bib.bib96), [105](#bib.bib105), [100](#bib.bib100), [104](#bib.bib104)].
    In [[107](#bib.bib107)] and [[106](#bib.bib106)], the authors study QoS-aware
    computation offloading in an ad-hoc mobile network. By making a certain payment,
    the mobile user can offload its computational tasks to nearby mobile users constituting
    a mobile cloudlet, as shown in Fig. [8](#S4.F8 "Figure 8 ‣ IV-B Data and Computation
    Offloading ‣ IV Caching and Offloading ‣ Applications of Deep Reinforcement Learning
    in Communications and Networking: A Survey")(d). Each mobile user has a first-in-first-out
    queue with limited buffer size to store the arriving tasks arriving as a Poisson
    process. The mobile user selects nearby cloudlets within D2D communication range
    for task offloading. The offloading decision depends on the states including the
    number of remaining tasks, the quality of the links between mobile users and the
    cloudlet, and the availability of the cloudlet’s resources. The objective is to
    maximize a composite utility function, subject to the mobile user’s QoS requirements,
    e.g., energy consumption and processing delay. The utility function is firstly
    an increasing function of the total number of tasks that have been processed either
    locally or remotely by the cloudlets. It is also related to the user’s benefit
    such as energy efficiency and payment for task offloading. This problem can be
    formulated as an MDP, which can be solved by linear programming and Q-learning
    approaches, depending on the availability of information about the state transition
    probabilities. This work is further enhanced by leveraging DNN or DQN to learn
    the decision strategy more efficiently. A similar model is studied in [[108](#bib.bib108)],
    where the computation offloading is formulated as an MDP to minimize the cost
    of computation offloading. The solution to the MDP can be used to train a DNN
    by supervised learning. The well-trained DNN is then applied to unseen network
    conditions for real-time decision-making. Simulation results show that the use
    of deep supervised learning achieves significant performance gain in offloading
    accuracy and cost saving.'
  prefs: []
  type: TYPE_NORMAL
- en: Data and computation offloading is also used in fog computing. The mobile application
    demanding a set of data and computational resources can be hosted in a container,
    e.g., virtual machine of a fog node. With user’s mobility, the container has to
    be migrated or offloaded to other nodes and dynamically consolidated. With the
    container migration, some nodes with low resource utilization can be switched
    off to reduce power consumption. The authors in [[109](#bib.bib109)] model the
    container migration as a multi-dimensional MDP, which is solved by the DQL. The
    system states consist of the delay, the power consumption and the migration cost.
    The action includes the selection policy that selects the containers to be emigrated
    from each source node, and the allocation policy that determines the destination
    node of each container. The action space can be optimized for more efficient exploration
    by dividing fog nodes into under-utilization, normal-utilization, and over-utilization
    groups. By powering off under-utilization nodes, all their containers will be
    migrated to other nodes to reduce power consumption. The training process is also
    optimized by using DDQN and PER which assigns different priorities to the transitions
    in experience memory. This helps the DQL agent at each fog node to perform better
    in terms of faster learning speed and more stability. Simulation results reveal
    that the DQL scheme achieves fast decision-making and outperforms the existing
    baseline approaches significantly in terms of delay, power consumption, and migration
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: This section reviews the applications of the DQL for wireless caching
    and data/computation offloading, which are inherently coupled with networking
    and allocation of channel capacity, computational resources, and caching capabilities,
    etc. We observe that the DQL framework for caching is typically centralized and
    mostly implemented at the network controller, e.g., the BS, service provider,
    and central scheduler, which is more powerful in information collection and cross-layer
    policy design. On the contrary, the end users have more control over their offloading
    decisions, and hence we observe more popular implementation of the DQL agent at
    local devices, e.g., mobile users, IoT devices, and fog nodes. Though an orchestration
    of networking, caching, data and computation offloading in one unified DQL framework
    is promising for network performance maximization, we face many challenges in
    designing highly-stable and fast-convergent learning algorithms, due to excessive
    delay and unsynchronized information collection from different network entities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: A summary of approaches using DQL for caching and offloading.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  prefs: []
  type: TYPE_TB
- en: '| Wireless proactive caching | [[70](#bib.bib70)] | MDP | DQN using actor-critic,
    DDPG | Base station | Cached contents and requested content | Replace selected
    content or not | Cache hit rate |  |'
  prefs: []
  type: TYPE_TB
- en: '| (score 1 or 0) | CRN |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[84](#bib.bib84)] | MDP | DQN using FNN | Base station | Channel states
    and computational capabilities | User association, computational unit, content
    delivery | Energy consumption | CRN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[74](#bib.bib74)] | MDP | DQN using NAFs | Cloud database | Encoding
    of a query, query cache miss rate | Cache expiration times | Cache hit rates,
    CDN utilization | Cloud database |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[76](#bib.bib76)][[77](#bib.bib77)] | MDP | DQN using FNN | Central scheduler
    | Channel coefficients, cache state | Active users and resource allocation | Network
    throughput | MU MIMO system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[78](#bib.bib78)] | MDP | DQN using CNN | Central scheduler | Channel
    coefficients, cache state | Active users and resource allocation | Network throughput
    | MU MIMO system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[79](#bib.bib79)] | MDP | DDQN | Service provider | Conditions of cache
    nodes, transmission rates of content chunks | The content chunks to cache and
    to remove | Network cost, QoE | Content centric IoT |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[81](#bib.bib81)] | MDP | DQN using LSM and ESN | Base station | Historical
    content request | User association, cached contents and formats | Reliability
    | Cellular system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[86](#bib.bib86)][[89](#bib.bib89)] | MDP | DQN using CNN | Service provider
    | Available BS, MEC, and cache | User association, caching, and offloading | Composite
    revenue | Vehicular ad hoc network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[85](#bib.bib85)] | MDP | DQN using FNN | Service provider | Available
    BS, MEC, and cache | User association, caching, and offloading | Composite revenue
    | Vehicular ad hoc network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[87](#bib.bib87)] | MDP | DDQN and dueling DQN | Service provider | Available
    BS, MEC, and cache | User association, caching, and offloading | Composite revenue
    | Vehicular ad hoc network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[91](#bib.bib91)] | MDP | DQN using CNN | Base station | Channel state,
    computational capability, content/version indicator, and the trust value | User
    association, caching, and offloading | Revenue | Mobile social network |'
  prefs: []
  type: TYPE_TB
- en: '| Data and computation offloading | [[93](#bib.bib93)] | MDP | DQN using CNN
    | Mobile user | User’s location and remaining file size | Idle, transmit via WLAN
    or cellular network | Total data rate | Cellular system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[94](#bib.bib94)] | MDP | DQN using FNN | Base station | Sum of cost
    and computational capacity of the MEC server | Offloading decision and resource
    allocation | Sum of cost of delay and energy consumption | Cellular system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[95](#bib.bib95)] | MDP | DQN using FNN | Mobile user | Channel qualities,
    states of energy and task queues | Offloading and resource allocation | Long term
    cost function | Cellular system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[96](#bib.bib96)] | MDP | DDQN, SARSA | Mobile user | Channel qualities,
    states of energy and task queues | Offloading decision and computational resource
    allocation | Long term cost function | Cellular system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[100](#bib.bib100)] | Game | DQN using CNN, hotbooting Q-learning | Mobile
    user | Channel states, size of App traces | Offloading rate | Utility related
    to detection accuracy, response speed, and the transmission cost | Cellular system
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[109](#bib.bib109)] | MDP | DDQN | Fog node | Delay, container’s location
    and resource allocation | Container’s next location | Composite utility related
    to delay, power consumption, and migration cost | Fog computing |'
  prefs: []
  type: TYPE_TB
- en: V Network Security and Connectivity Preservation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Future networks become more decentralized and ad-hoc in nature which are vulnerable
    to various attacks such as Denial-of-Service (DoS) and cyber-physical attack.
    Recently, the DQL has been used as an effective solution to avoid and prevent
    the attacks. In this section, we review the applications of DQL in addressing
    the following security issues:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jamming attack: In the jamming attack, attackers as jammers transmit Radio
    Frequency (RF) jamming signals with high power to cause interference to the legitimate
    communication channels, thus reducing the SINR at legitimate receivers. Anti-jamming
    techniques such as the frequency hopping [[110](#bib.bib110)] and user mobility,
    i.e., moving out from the heavy jamming area, have been commonly used. However,
    without being aware of the radio channel model and the jamming methods, it is
    challenging for the users to choose an appropriate frequency channel as well as
    to determine how to leave and avoid the attack. DQL enables the users to learn
    an optimal policy based on their past observations, and thus DQL can be used to
    address the above challenge.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cyber-physical attack: The cyber-physical attack is an integrity attack in
    which an attacker manipulates data to alter control signals in the system. This
    attack often happens in autonomous systems such as Intelligent Transportation
    Systems (ITSs) and increases the risk of accidents to Autonomous Vehicles (AVs).
    The DQL allows the AVs to learn optimal actions based on their time-varying observations
    of the attacker’ activities. Thus, the DQL can be used to achieve robust and dynamic
    control of the AV to the attacks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Connectivity preserving: This refers to maintaining the connectivity among
    the robots, e.g., UAVs, to support the communication and exchange of information
    among them. The system and network environment is generally dynamic and complex,
    and thus the DQL which allows each robot to make dynamic decisions based on its
    state can be effectively used to preserve the connectivity in the system.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V-A Network Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section discusses the applications of DQL to address the jamming attack
    and the cyber-physical attack.
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Jamming Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0cd6213775e8ecc76f10e8c2dff3242d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Jamming attack in cognitive radio network [[111](#bib.bib111)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'A pioneer work using the DQL for the anti-jamming is [[111](#bib.bib111)].
    The network model is a Cognitive Radio Network (CRN) as shown in Fig. [9](#S5.F9
    "Figure 9 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and
    Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in Communications
    and Networking: A Survey") which consists of one Secondary User (SU), multiple
    Primary Users (PUs), and multiple jammers. The network has a set of frequency
    channels for hopping. At each time slot, each jammer can arbitrarily select one
    of the channels to send its jamming signal, and the SU, i.e., the agent, needs
    to choose a proper action based on the SU’s current state. The action is (i) selecting
    one of the channels to send its signals or (ii) leaving the area to connect to
    another BS. The jammers are assumed to avoid causing interference to the PUs,
    and thus the SU’s current state consists of the number of PUs and the discretized
    SINR of the SU signal at the last time slot. The objective of the SU is to maximize
    its expected discounted utility over time slots. Note that when the SU chooses
    to leave the area to connect to another BS, it spends a mobility cost. Thus, the
    utility is defined as a function of the SINR of the SU signal and the mobility
    cost. Since the number of frequency channels may be large that results in a large
    action set, the CNN is used for the DQL to quickly learn the optimal policy. As
    shown in the simulation results, the proposed DQL has a faster convergence speed
    than that of the Q-learning algorithm. Moreover, considering the scenario with
    two jammers, the proposed DQL outperforms the frequency-hopping method in terms
    of the SINR and the mobility cost.'
  prefs: []
  type: TYPE_NORMAL
- en: The model in [[111](#bib.bib111)] is constrained to two jammers. As the number
    of jammers in the network increases, the proposed scheme may not be effective.
    The reason is that it becomes hard for the SU to find good actions when the number
    of jammed channels increases. An appropriate solution, as proposed in  [[112](#bib.bib112)],
    allows the receiver of the SU to leave its current location. Since the leaving
    incurs the mobility cost, the receiver, i.e., the agent, needs an optimal policy,
    i.e., staying at or leaving the current location, to maximize its utility. In
    this scenario, the DQL based on CNN can be used for the receiver to find the optimal
    action to maximize its expected utility. Here, the utility and state of the receiver
    are essentially defined similarly to that of the agent in [[111](#bib.bib111)].
    In particular, the state includes the discretized SINR of the signal measured
    by the receiver at the last time slot.
  prefs: []
  type: TYPE_NORMAL
- en: The above approaches, i.e., in [[111](#bib.bib111)] and [[112](#bib.bib112)],
    define states of the agents based on raw SINR values of the signals. In practical
    wireless environments, the number of SINR values may be large and even infinite.
    Moreover, the raw SINR can be inaccurate and noisy. To cope with the challenge
    of the infinite number of states, the DQL can use a recursive Convolutional Neural
    Network (RCNN) as proposed in [[113](#bib.bib113)]. By using the pre-processing
    layer and recursive convolution layers, the RCNN is able to remove noise from
    the network environment and extract useful features of the SINR, i.e., discrete
    spectrum sample values greater than a noise threshold, thus reducing the computational
    complexity. The network model and the problem formulation considered in [[113](#bib.bib113)]
    are similar to those in [[111](#bib.bib111)]. However, instead of directly using
    the raw SINR, the state of the SU is the extracted features of the SINR. Also,
    the action of the SU includes only frequency-hopping decision. The simulation
    results show that the proposed DQL based on the RCNN can converge in both fixed
    and dynamic jamming scenarios while the Q-learning cannot converge in the dynamic
    jamming one. Furthermore, the proposed DQL can achieve the average throughput
    close to that of the optimal scheme, i.e., an anti-jamming scheme with completely
    known jamming actions.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of finding the frequency-hopping decisions, the authors in [[114](#bib.bib114)]
    propose the use of DQL to find an optimal power control policy for the anti-jamming.
    The model is an IoT network including IoT devices and one jammer. The jammer can
    observe the communications of the transmitter and chooses a jamming strategy to
    reduce the SINR at the receiver. Thus, the transmitter chooses an action, i.e.,
    transmit power level, to maximize its utility. Here, the utility is the difference
    between the SINR and the energy consumption cost due to the transmission. Note
    that choosing the transmit power impacts the future jamming strategy, and thus
    the interaction between the transmitter and the jammer can be formulated as an
    MDP. The transmitter is the agent, and the state is SINR measured at its receiver
    at the last time slot. The DQN using the CNN is then adopted to find an optimal
    power control policy for the transmitter to maximize its expected accumulated
    discounted reward, i.e., the utility, over time slots. The simulation results
    show that the proposed DQL can improve the utility of the transmitter up to 17.7%
    compared with the Q-learning scheme. Also, the proposed DQL reduces the utility
    of the jammer around 18.1% compared with the Q-learning scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7dca9c8fb69c8a571ee0afd8168a77ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Anti-jamming scheme based on UAV [[115](#bib.bib115)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent the jammer’s observations of communications, the transmitter can
    change its communication strategy, e.g., by using relays that are far from the
    jamming area. The relays can be UAVs as proposed in [[115](#bib.bib115)]. The
    model consists of one UAV, i.e., a relay, one jammer, one mobile user and its
    serving BS (see Fig. [10](#S5.F10 "Figure 10 ‣ V-A1 Jamming Attack ‣ V-A Network
    Security ‣ V Network Security and Connectivity Preservation ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey")). The
    mobile user transmits messages to its server via the serving BS. In the case that
    the serving BS is heavily jammed, the UAV helps the mobile user to relay the messages
    to the server through a backup BS. In particular, depending on the SINR and Bit
    Error Rate (BER) values sent from the serving BS, the UAV as an agent decides
    the relay power level to maximize its utility, i.e., the difference between the
    SINR and the relay cost. The relay power level can be considered to be the UAV’s
    actions, and the SINR and BER are its states. As such, the next state observed
    by the UAV is independent of all the past states and actions. The problem is formulated
    as an MDP. To quickly achieve the optimal relay policy for the UAV, the DQL based
    on CNN is then adopted. The simulation results in [[115](#bib.bib115)] show that
    the proposed DQL scheme takes only 200 time slots to converge to the optimal policy,
    which is 83.3% less than that of the relay scheme based on Q-learning [[116](#bib.bib116)].
    Moreover, the proposed DQL scheme reduces the BER of the user by 46.6% compared
    with the hill climbing-based UAV relay scheme [[117](#bib.bib117)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scheme proposed in [[115](#bib.bib115)] assumes that the relay UAV is sufficiently
    far from the jamming area. However, as illustrated in Fig. [10](#S5.F10 "Figure
    10 ‣ V-A1 Jamming Attack ‣ V-A Network Security ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), the attacker can use a compromised UAV close to the relay
    UAV to launch the jamming attack to the relay UAV. In such a scenario, the authors
    in [[118](#bib.bib118)] show that the DQL can still be used to address the attack.
    The system model is based on physical layer security and consists of one UAV and
    one attacker. The attacker is assumed to be “smarter” than that in the model in [[115](#bib.bib115)].
    This means that the attacker can observe channels that the UAV uses to communicate
    with the BS in the past time slots and then chooses jamming power levels on the
    target channels. Therefore, the UAV needs to find a power allocation policy, i.e.,
    transmit power levels on the channels, to maximize the secrecy capacity of the
    UAV-BS communication. Similar to [[115](#bib.bib115)], the DQL based on CNN is
    used which enables the UAV to choose its actions, i.e., transmit power levels
    on the channels, based on its state, i.e., the attacker’s jamming power level
    in the last time slot. The reward is the difference between the secrecy capacity
    of the UAV and BS and the energy consumption cost.'
  prefs: []
  type: TYPE_NORMAL
- en: The simulation results in [[118](#bib.bib118)] show that the proposed DQL can
    improve the UAV’s utility up to 13% compared with the baseline scheme [[119](#bib.bib119)]
    which uses the Win or Learn Faster-Policy Hill Climbing (WoLF-PHC) to prevent
    the attack. Also, the safe rate of the UAV, i.e., the probability that the UAV
    is attacked, obtained by the proposed DQL is 7% higher than that of the baseline.
    However, the proposed DQL is applied only to a single-UAV system. For the future
    work, scenarios with multiple UAVs need to be considered. In such a scenario,
    more computational overhead is expected and multi-agent DQL algorithms can be
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Cyber-Physical Attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In autonomous systems such as ITSs, the attacker can seek to inject faulty data
    to information transmitted from the sensors to the AVs. The AVs which receive
    the injected information may inaccurately estimate the safe spacing among them.
    This increases the risk of AV accidents. Vehicular communication security algorithms,
    e.g., [[120](#bib.bib120)], can be used to minimize the spacing deviation. However,
    the attacker’s actions in these algorithms are assumed to be stable which may
    not be applicable in practical systems. The DQL that enables the AVs to learn
    optimal actions based on their time-varying observations of the attacker’ actions
    can be thus used.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8c9737ddbf646022c024c845ceba6936.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Car-following model with cyber-physical attack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first work using the DQL for the cyber-physical attack in an ITS can be
    found in [[121](#bib.bib121)]. The system is a car-following model [[122](#bib.bib122)]
    of the General Motors as shown in Fig. [11](#S5.F11 "Figure 11 ‣ V-A2 Cyber-Physical
    Attack ‣ V-A Network Security ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey"). In the model, each AV updates its speed based on measurement information
    received from the closest road smart sensors. The attacker attempts to inject
    faulty data to the measurement information. However, the attacker cannot inject
    the measurements of different sensors equally due to its resource constraint.
    Thus, the AV can choose less-faulty measurements by selecting a vector of measurement
    weights. The objective of the attacker is to maximize the deviation, i.e., the
    utility, from the safe spacing between the AV and its nearby AV while that of
    the AV is to minimize the deviation. The interaction between the attacker and
    the AV can be modeled as a zero-sum game. The authors in [[121](#bib.bib121)]
    show that the DQL can be used to find the equilibrium strategies. In particular,
    the action of the AV is to choose a weight vector. Its state includes the past
    actions, i.e., the weight vectors, and the past deviation values. Since the actions
    and deviations have continuous values, the state space is infinite. Thus, LSTM
    units that are able to extract useful features are adopted for the DQL to reduce
    the state space. The simulation results show that by using the past actions and
    deviations for learning the attacker’s action, the proposed DQL scheme can guarantee
    a lower steady-state deviation than the Kalmar filter-based scheme [[120](#bib.bib120)].
    Moreover, by using the LSTM units, the results show that the proposed DQL scheme
    can converge much faster than the baseline scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another work that uses the LSTM to extract useful features from the measurement
    information to detect the cyber-physical attack is proposed in [[123](#bib.bib123)].
    The model is an IoT system including a cloud and a set of IoT devices. The IoT
    devices generate signals and transmit the signals to the cloud (see Fig. [12](#S5.F12
    "Figure 12 ‣ V-A2 Cyber-Physical Attack ‣ V-A Network Security ‣ V Network Security
    and Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in
    Communications and Networking: A Survey")). The cloud uses the received signals
    for estimation and control of the IoT devices’ operation. An attacker can launch
    the cyber-physical attack by manipulating the IoT devices’ output signals that
    causes control errors at the cloud and degrades the performance of the IoT system.
    To detect the attack, the cloud uses LSTM units to extract stochastic features
    or fingerprints such as flatness, skewness, and kurtosis, of the IoT devices’
    signals. The cloud sends the fingerprints back to the IoT devices, and the IoT
    devices embed, i.e., watermark, the fingerprints inside the signals. The cloud
    uses the fingerprints to authenticate the IoT devices’ signals to detect the attack.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c355e211fe48250cd1c54f43d026ad50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Cyber-physical detection in IoT systems using DQL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm proposed in [[123](#bib.bib123)] is also called dynamic watermarking
    [[124](#bib.bib124)] which is able to detect the cyber-physical attack and to
    prevent eavesdropping attacks. However, the algorithm requires large computational
    resources at the cloud for the IoT device signal authentication. Consequently,
    the cloud can only authenticate a limited number of vulnerable IoT devices. The
    cloud can choose the vulnerable IoT devices by observing their security status.
    However, this can be impractical since the IoT devices may not report their security
    status. Thus, the authors in [[125](#bib.bib125)] propose to use the DQL that
    enables the cloud to decide which IoT devices to authenticate with the incomplete
    information. Since IoT devices with more valuable data are likely to be attacked,
    the reward is defined as a function of data values of IoT devices. The cloud’s
    state includes attack actions of the attacker on the IoT devices in the past time
    slots. The actions of the attacker on the IoT devices can be obtained by using
    the dynamic watermarking algorithm in [[123](#bib.bib123)] (see Fig. [12](#S5.F12
    "Figure 12 ‣ V-A2 Cyber-Physical Attack ‣ V-A Network Security ‣ V Network Security
    and Connectivity Preservation ‣ Applications of Deep Reinforcement Learning in
    Communications and Networking: A Survey")). The DQL then uses an LSTM unit to
    find the optimal policy. The input of the LSTM unit is the state of the cloud,
    and the output includes probabilities of attacking the IoT devices. By using a
    real dataset from the accelerometers, the simulation results show that the proposed
    DQL can improve the cloud’s utility up to 30% compared with the case in which
    the cloud chooses the IoT devices with equal probability.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Connectivity Preservation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Multi-robot systems such as multi-UAV cooperative networks have been widely
    applied in many fields such as military, e.g., enemy detecting. In the cooperative
    multi-robot system, the connectivity among the robots, e.g., UAVs in Fig [13](#S5.F13
    "Figure 13 ‣ V-B Connectivity Preservation ‣ V Network Security and Connectivity
    Preservation ‣ Applications of Deep Reinforcement Learning in Communications and
    Networking: A Survey"), is required to enable the communication and exchange of
    information. To tackle the connectivity preservation problem, the Artificial Potential
    Field (APF) algorithm [[126](#bib.bib126)] has been used. However, the algorithm
    cannot be directly adopted when the robots are undertaking missions in dynamic
    and complex environments. The DQL which allows each robot to make dynamic decisions
    based on its own state can be effectively applied to preserve the connectivity
    in the multi-robot system. Such an approach is proposed in [[127](#bib.bib127)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b52aa2f0a7b0e02e5361aabe3b8a9fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Connectivity preservation of a multi-UAV network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model in [[127](#bib.bib127)] consists of two robots or UAVs, i.e., one
    leader robot and one follower robot. In the model, a central control, i.e., a
    ground BS, adjusts the velocity of the follower such that the follower stays in
    the communication range of the leader at all time (see Fig [13](#S5.F13 "Figure
    13 ‣ V-B Connectivity Preservation ‣ V Network Security and Connectivity Preservation
    ‣ Applications of Deep Reinforcement Learning in Communications and Networking:
    A Survey")). The connectivity preservation problem can be thus formulated as an
    MDP. The agent is the BS, and the states are the relative position and the velocity
    of the leader with respect to the follower. The action space consists of possible
    velocity values of the follower. Taking an action returns a reward which is +1
    if the follower is in the range of the leader, and -1 otherwise. A DQN using FNN
    is used which enables the BS to learn an optimal policy to maximize the expected
    discounted cumulative reward. The input of the DQN includes the states of the
    two robots, and the output is the action space of the follower. The simulation
    results show that the proposed scheme can achieve better connectivity between
    the two robots than that of the APF method. However, a general scenario with more
    than one leader and one follower needs to be investigated.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering the general scenario, the authors in [[128](#bib.bib128)] address
    the connectivity preservation between multiple leaders and multiple followers.
    The robot system is definitely connected if any two robots are connected via a
    direct link or multi-hop link. To express the connectivity in such a robot system,
    the authors introduce the concept of algebraic connectivity[[129](#bib.bib129)]
    which is the second smallest eigenvalue of a Laplacian matrix. The robot system
    is connected if the algebraic connectivity of the system is positive. Thus, the
    problem is to adjust the velocity of the followers such that the algebraic connectivity
    is positive over time slots. This problem can be formulated as an MDP in which
    the agent is the ground BS, the state is a combination of the states of all robots,
    the action is a set of possible velocity values for the followers. The reward
    is +1 if the algebraic connectivity of the system increases or holds, and becomes
    a penalty of -1 if the algebraic connectivity decreases. Similar to [[127](#bib.bib127)],
    a DQN is adopted. Due to the large action space of the followers, the actor-critic
    neural network [[26](#bib.bib26)] is used. The simulation results show that the
    followers always follow the motion of the leaders even if the leaders’ trajectory
    dynamically changes. However, the proposed DQN requires more time to converge
    than that in [[127](#bib.bib127)] because of the presence of more followers.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed schemes in [[127](#bib.bib127)] and [[128](#bib.bib128)] do not
    consider a minimum distance between the leaders and followers. The leaders and
    followers can collide with each other if the distance between them is too short.
    Thus, the BS needs to guarantee the minimum distance between them. One solution
    is to have the minimum distance in the reward as proposed in [[130](#bib.bib130)].
    In particular, if the leader is too close to its follower, the reward of the system
    is penalized regarding the minimum distance. The DQL algorithm proposed in [[128](#bib.bib128)]
    is then used such that the BS learns proper actions, e.g., turning left and right,
    to maximize the cumulative reward.
  prefs: []
  type: TYPE_NORMAL
- en: When BSs are densely deployed, the UAVs or mobile users need to trigger a frequent
    handover to preserve the connectivity. The frequent handover increases communication
    overhead and energy consumption of the mobile users, and interrupts data flows.
    Thus, it is essential to maintain an appropriate handover rate. The authors in [[27](#bib.bib27)]
    address the handover decision problem in an ultra-density network. The network
    model consists of multiple mobile users, SBSs, and one central controller. At
    each time slot, the user needs to decide its serving SBS. The handover decision
    process can be modeled as an MDP, and the DQL is adopted to find an optimal handover
    policy for each user to minimize the number of handover occurrences while ensuring
    certain throughput. The state of the user, i.e., the agent, includes reference
    signal quality received from candidate SBSs and the last action of the user. The
    reward is defined as the difference between the data rate of the user and its
    energy consumption for the handover process. Given a high density of users, the
    DQL using A3C and LSTM is adopted to find the optimal policy in short training
    time. The simulation results show that the proposed DQL can achieve better throughput
    and lower handover rate than those of the upper confidence bandit algorithm [[131](#bib.bib131)]
    with similar training time.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the reliability of the communication between the SBSs and the mobile
    users, the SBSs should be able to handle network faults and failure automatically
    as self-healing. The DQL can be applied as proposed in [[132](#bib.bib132)] to
    make optimal parameter adjustments based on the observation of the network performance.
    The model is the 5G network including one MBS. The MBS as an agent needs to handle
    network faults such as transmit diversity faults and antenna azimuth change, e.g.,
    because of wind. These faults are represented as the MBS’s state that is the number
    of active alarms. Based on the alarms, the MBS can take actions including (i)
    enabling the transmit diversity and (ii) setting the antenna azimuth to default
    value. The reward that the MBS receives is the scores, e.g., -1, 0, and +1, depending
    on the number of faults happening. The DQL is used to learn the optimal policy.
    The simulation results show that the proposed DQL can achieve network throughput
    close to that of the oracle-based self-healing, i.e., the upper-performance bound,
    but incurs less fault message passing overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A summary of approaches using DQL for network security and connectivity
    preservation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Networks |'
  prefs: []
  type: TYPE_TB
- en: '| Network security | [[111](#bib.bib111)] | Game | DQN using CNN | Secondary
    user | Number of PUs and signal SINR | Channel selection and leaving decision
    | SINR and mobility cost | CRN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[112](#bib.bib112)] | Game | DQN using CNN | Receiving transducer | Signal
    SINR | Staying and leaving decisions | SINR and mobility cost | Underwater acoustic
    network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[113](#bib.bib113)] | MDP | DQN using RCNN | SU | Signal SINR | Channel
    selection | SINR and mobility cost | CRN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[114](#bib.bib114)] | MDP | DQN using CNN | Transmit IoT device | Signal
    SINR | Channel selection | SINR and energy consumption cost | IoT |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[115](#bib.bib115)] | MDP | DQN using CNN | Relay UAV | Signal SINR and
    BER | Relay power | SINR and relay cost | UAV |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[118](#bib.bib118)] | MDP | DQN using CNN | Transmit UAV | Jamming power
    | Transmit power | Secrecy capacity and energy consumption cost | UAV |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[121](#bib.bib121)] | Game | DQN using LSTM units | Autonomous vehicle
    | Deviation values | Measurement weight selection | Safe spacing deviation | ITS
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[125](#bib.bib125)] | Game | DQN using LSTM units | Cloud | Attack actions
    on IoT devices | IoT device set selection | IoT devices’ data values | IoT |'
  prefs: []
  type: TYPE_TB
- en: '| Connectivity preservation | [[127](#bib.bib127)] | MDP | DQN using FNN |
    Ground base station | Relative positions and the velocity of robots | Velocity
    decision | Sore +1 and -1 | Robot system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[128](#bib.bib128)] | MDP | DQN using A3C | Ground base station | Relative
    positions and the velocity of robots | Velocity decision | Sore +1 and -1 | Robot
    system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[130](#bib.bib130)] | POMDP | DQN using A3C | Ground base station | Information
    of distances among robots | Turning left and turning right decisions | Sore +1
    and -1 | Robot system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[27](#bib.bib27)] | MDP | DQN using A3C and LSTM | Mobile users | Reference
    signal received quality and the last action | Serving SBS selection | Data rate
    and energy consumption | Ultra-dense network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[132](#bib.bib132)] | MDP | DQN using CNN | MBS | The number of active
    alarms | Enabling transmit diversity and changing antenna azimuth | Score -1,
    0, +1, and +5 | Self-organization network |'
  prefs: []
  type: TYPE_TB
- en: 'Summary: This section reviews applications of DQL for the network security
    and connectivity preservation. The reviewed approaches are summarized along with
    the references in Table [V](#S5.T5 "TABLE V ‣ V-B Connectivity Preservation ‣
    V Network Security and Connectivity Preservation ‣ Applications of Deep Reinforcement
    Learning in Communications and Networking: A Survey"). We observe that the CNN
    is mostly used for the DQL to enhance the network security. Moreover, DQL approaches
    for the anonymous system such as robot systems and ITS receive more attentions
    than other networks. However, the applications of DQL for the cyber-physical security
    are relatively few and need to be investigated.'
  prefs: []
  type: TYPE_NORMAL
- en: VI Miscellaneous Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section reviews applications of DRL to solve some other issues in communications
    and networking. The issues include (i) traffic engineering and routing, (ii) resource
    sharing and scheduling, and (iii) data collection.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Traffic Engineering and Routing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traffic Engineering (TE) in communication networks refers to Network Utility
    Maximization (NUM) by optimizing a path to forward the data traffic, given a set
    of network flows from source to destination nodes. Traditional NUM problems are
    mostly model-based. However, with the advances of wireless communication technologies,
    the network environment becomes more complicated and dynamic, which makes it hard
    to model, predict, and control. The recent development of DQL methods provides
    a feasible and efficient way to design experience-driven and model-free schemes
    that can learn and adapt to the dynamic wireless network from past observations.
  prefs: []
  type: TYPE_NORMAL
- en: Routing optimization is one of the major control problems in traffic engineering.
    The authors in [[133](#bib.bib133)] present the first attempt to use the DQL for
    the routing optimization. Through the interaction with the network environment,
    the DQL agent at the network controller determines the paths for all source-destination
    pairs. The system state is represented by the bandwidth request between each source-destination
    pair, and the reward is a function of the mean network delay. The DQL agent leverages
    the actor-critic method for solving the routing problem that minimizes the network
    delay, by adapting routing configurations automatically to current traffic conditions.
    The DQL agent is trained using the traffic information generated by a gravity
    model [[134](#bib.bib134)]. The routing solution is then evaluated by OMNet+ discrete
    event simulator [[135](#bib.bib135)]. The well-trained DQL agent can produce a
    near-optimal routing configuration in a single step and thus the agent is agile
    for real-time network control. The proposed approach is attractive as the traditional
    optimization-based techniques require a large number of steps to produce a new
    configuration. The authors in [[136](#bib.bib136)] consider a similar network
    model with multiple end-to-end communication sessions. Each source-destination
    pair has a set of candidate paths that can transport the traffic load. Experimental
    results show that the conventional DDPG method does not work well for the continuous
    control problem in [[136](#bib.bib136)]. One possible explanation is that DDPG
    utilizes uniform sampling for experience replay, which ignores different significance
    of the transition samples.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[136](#bib.bib136)] also combine two new techniques to optimize
    DDPG particularly for traffic engineering problems, i.e., TE-aware exploration
    and actor-critic-based PER methods. The TE-aware exploration leverages the shortest
    path algorithm and NUM-based solution as the baseline during exploration. The
    PER method is conventionally used in DQL, e.g., [[79](#bib.bib79)] and [[109](#bib.bib109)],
    while the authors in [[136](#bib.bib136)] integrate the PER method with the actor-critic
    framework for the first time. The proposed scheme assigns different priorities
    to transitions in the experience replay. Based on the priority, the proposed scheme
    samples the transitions in each epoch. The system state consists of throughput
    and delay performance of each communication session. The action specifies the
    amount of traffic load going through each of the paths. By learning the dynamics
    of network environment, the DQL agent aims to maximize the total utility of all
    the communication sessions, which is defined based on end-to-end throughput and
    delay [[137](#bib.bib137)]. Packet-level simulations using NS-3 [[138](#bib.bib138)],
    tested on well-known network topologies as well as random topologies generated
    by BRITE [[139](#bib.bib139)], reveal that the proposed DQL scheme significantly
    reduces the end-to-end delay and improves the network utility, compared with the
    baseline schemes including DDPG and the NUM-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The networking and routing optimization become more complicated in the UAV-based
    wireless communications. The authors in [[130](#bib.bib130)] model autonomous
    navigation of one single UAV in a large-scale unknown complex environment as a
    POMDP, which can be solved by actor-critic-based DRL method. The system state
    includes its distances and orientation angles to nearby obstacles, the distance
    and angle between its present position and the destination. The UAV’s action is
    to turn left or right or keep ahead. The reward is composed of four parts: an
    exponential penalty term if it is too close to any obstacles, a linear penalty
    term to encourage minimum time delay, the transition and direction rewards if
    the UAV is getting close to the target position in a proper direction. Instead
    of using conventional DDPG for continuous control, the Recurrent Deterministic
    Policy Gradient (RDPG) is proposed for the POMDP by approximating the actor and
    critic using RNNs. Considering that RDPG is not suitable for learning using memory
    replay, the authors in [[130](#bib.bib130)] propose the fast-RDPG method by utilizing
    the actor-critic framework with function approximation [[140](#bib.bib140)]. The
    proposed method derives policy update for POMDP by directly maximizing the expected
    long-term accumulated discounted reward.'
  prefs: []
  type: TYPE_NORMAL
- en: Path planning for multiple UAVs connected via cellular systems is studied in [[141](#bib.bib141)]
    and [[142](#bib.bib142)]. Each UAV aims to achieve a tradeoff between maximizing
    energy efficiency and minimizing both latency and interference caused to the ground
    network along its path. The network state observable by each UAV includes its
    distances and orientation angles to cellular BSs, the orientation angle to its
    destination, and the horizontal coordinates of all UAVs. The action of each UAV
    includes an optimal path, transmit power, and cell association along its path.
    The interaction among UAVs is cast as a dynamic game and solved by a multi-agent
    DRL framework. The use of ESN in the DRL framework allows each UAV to retain previous
    memory states and make a decision for unseen network states, based on the reward
    obtained from previous states. ESN is a new type of RNNs with feedback connections,
    consisting of the input, recurrent, and output weight matrices. ESN training is
    typically quick and computationally efficient compared with other RNNs. Deep ESNs
    can exploit the advantages of a hierarchical temporal feature representation at
    different levels of abstraction, hence disentangling the difficulties in modeling
    complex tasks. Simulation results show that the proposed scheme improves the tradeoff
    between energy efficiency, wireless latency, and the interference caused to the
    ground network. Results also show that each UAV’s altitude is a function of the
    ground network density and the UAV’s objective function is an important factor
    in achieving the UAV’s target.
  prefs: []
  type: TYPE_NORMAL
- en: Besides networked UAVs, vehicle-to-infrastructure also constitutes an important
    part and provides rich application implications in 5G ecosystem. The authors in [[143](#bib.bib143)]
    adopt the DQL to achieve an optimal control policy in communication-based train
    control system, which is supported by bidirectional train-ground communications.
    The control problem aims to optimize the handoff decision and train control policy,
    i.e., accelerate or decelerate, based on the states of stochastic channel conditions
    and real-time information including train position, speed, measured SNR from APs,
    and handoff indicator. The objective of the DQL agent is to minimize a weighted
    combination of operation profile tracking error and energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Resource Sharing and Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: System capacity is one of the most important performance metrics in wireless
    communication networks. System capacity enhancements can be based on the optimization
    of resource sharing and scheduling among multiple wireless nodes. The integration
    of DRL into 5G systems would revolutionize the resource sharing and scheduling
    schemes from model-based to model-free approaches and meet various application
    demands by learning from the network environment.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[144](#bib.bib144)] study the user scheduling in a multi-user
    massive MIMO system. User scheduling is responsible for allocating resource blocks
    to BSs and mobile users, taking into account the channel conditions and QoS requirements.
    Based on this user scheduling strategy, a DRL-based coverage and capacity optimization
    is proposed to obtain dynamically the scheduling parameters and a unified threshold
    of QoS metric. The performance indicators are calculated as the average spectrum
    efficiency of all the users. The system state is an indicator of the average spectrum
    efficiency. The action of the scheduler is a set of scheduling parameters to maximize
    the reward as a function of the average spectrum efficiency. The DRL scheme uses
    policy gradient method to learn a policy function (instead of a Q-function) directly
    from trajectories generated by the current policy. The policy network is trained
    with a variant of the REINFORCE algorithm [[140](#bib.bib140)]. The simulation
    results in [[144](#bib.bib144)] show that compared with the optimization-based
    algorithms that suffer from incomplete network information, the policy gradient
    method achieves much better performance in terms of network coverage and capacity.
  prefs: []
  type: TYPE_NORMAL
- en: In [[145](#bib.bib145)], the authors focus on dynamic resource allocation in
    a cloud radio access network and present a DQL-based framework to minimize the
    total power consumption while fulfilling mobile users’ QoS requirements. The system
    model contains multiple Remote Radio Heads (RRHs) connected to a cloud BaseBand
    Unit (BBU). The information of RRHs can be shared in a centralized manner. The
    system state contains information about the mobile users’ demands and the RRHs’
    working states, e.g., active or sleep. According to the system state and the result
    of last execution, the DQL agent at the BBU decides whether to turn on or off
    certain RRH(s), and how to allocate beamforming weight for each active RRH. The
    objective is to minimize the total expected power consumption. The authors propose
    a two-step decision framework to reduce the size of action space. In the first
    step, the DQL agent determines the set of active RRHs by Q-learning and DNNs.
    In the second step, the BBU derives the optimal resource allocation for the active
    RRHs by solving a convex optimization problem. Through the combination of DQL
    and optimization techniques, the proposed framework results in a relatively small
    action space and low online computational complexity. Simulation results show
    that the framework achieves significant power savings while satisfying user demands
    and is robust in highly dynamic network environment. The aforementioned works
    mostly focus on simulations and numerical comparisons. With one step further,
    the authors in [[146](#bib.bib146)] implement a multi-objective DQL framework
    as the radio-resource-allocation controller for space communications. The implementation
    uses modular software architecture to encourage re-use and easy modification for
    different algorithms, which is integrated into the real space-ground system developed
    by NASA Glenn Research Center.
  prefs: []
  type: TYPE_NORMAL
- en: In emerging and future wireless networks, BSs are deployed with a high density,
    and thus the interference among the BSs must be considered. The authors in [[147](#bib.bib147)]
    propose to use a DQL scheme which allows the BSs to learn their optimal power
    control policy. In the proposed scheme, each BS is an agent, the action is choosing
    power levels, and the state includes interference that the BS caused to its neighbors
    in the last time slot. The objective is to maximize the BS’s data rate. The DQN
    using FNN is then adopted to implement the DQL algorithm. For the future work,
    a joint power control and channel selection can be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Network slicing [[148](#bib.bib148)] and NFV [[90](#bib.bib90)] are two emerging
    concepts for resource allocation in the 5G ecosystem to provide cost-effective
    services with better performance. The network infrastructure, e.g., cache, computation,
    and radio resources, is comparatively static while the upper-layer Virtualized
    Network Functions (VNFs) are dynamic to support time-varying application-specific
    service requests. The concept of network slicing is to divide the network resources
    into multi-layer slices, managed by different service renderers independently
    with minimal conflicts. The concept of Service Function Chaining (SFC) is to orchestrate
    different VNFs to provide required functionalities and QoS provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[149](#bib.bib149)] propose a DQL scheme for QoS/QoE-aware SFC
    in NFV-enabled 5G systems. Typical QoS metrics are bandwidth, delay, throughput,
    etc. The evaluation of QoE normally involves the end-user’s participation in rating
    the service based on direct user perception. The authors quantify QoE by measurable
    QoS metrics without end-user involvements, according to the Weber-Fechner Law
    (WFL) [[150](#bib.bib150)] and exponential interdependency of QoE and QoS hypothesis [[151](#bib.bib151)].
    These two principles actually define nonlinear relationship between QoE and QoS.
    The system state represents the network environment including network topology,
    QoS/QoE status of the VNF instances, and the QoS requirements of the SFC request.
    The DQL agent selects a certain direct successive VNF instance as an action. The
    reward is a composite function of the QoE gain, the QoS constraint penalty, and
    the OPEX penalty. A DQL based on CNNs is implemented to approximate the action-value
    function. The authors in [[152](#bib.bib152)] review the application of a DQL
    framework in two typical resource management scenarios using network slicing.
    For radio resource slicing, the authors simulate a scenario containing one single
    BS with different types of services. The reward can be defined as a weighted sum
    of spectrum efficiency and QoE. For priority-based core network slicing, the authors
    simulate a scenario with 3 SFCs demanding different computational resources and
    waiting time. The reward is the sum of waiting time in different SFCs. Simulation
    results in both scenarios show that the DQL framework could exploit more implicit
    relationship between user activities and resource allocation in resource constrained
    scenarios, and enhance the effectiveness and agility for network slicing.
  prefs: []
  type: TYPE_NORMAL
- en: Resource allocation and scheduling problems are also important for computer
    clusters or database systems. This usually leads to an online decision-making
    problem depending on the information of workload and environment. The authors
    in [[153](#bib.bib153)] propose a DRL-based solution, DeepRM, by employing policy
    gradient methods [[140](#bib.bib140)] to manage resources in computer systems
    directly from experience. The same policy gradient method is also used in [[144](#bib.bib144)]
    for user scheduling and resource management in wireless systems. DeepRM is a multi-resource
    cluster scheduler that learns to optimize various objectives such as minimizing
    average job slowdown or completion time. The system state is the current allocation
    of cluster resources and the resource profiles of jobs in the queue. The action
    of the scheduler is to decide how to schedule the pending jobs. By simulations
    with synthetic dataset, DeepRM is shown to perform comparably or better than state-of-the-art
    heuristics, e.g., Shortest-Job-First (SJF). It adapts to different conditions
    and converges quickly, without any prior knowledge of system behavior. In [[154](#bib.bib154)],
    the authors use the actor-critic method to address the scheduling problem in a
    general-purpose distributed data stream processing systems, which deal with processing
    of continuous data flow in real time or near-real-time. The system model contains
    multiple threads, processes, and machines. The system state consists of the current
    scheduling decision and the workload of each data source. The scheduling problem
    is to assign each thread to a process of a machine. The agent at the scheduler
    determines the assignment of each thread, with the objective of minimizing the
    average processing time. The DRL framework includes three components, i.e., an
    actor network, an optimizer producing a K-NN set of the actor network’s output
    action, and the critic network predicting the Q-value for each action in the set.
    The action is selected from the K-NN set with the maximum Q-value. The use of
    optimizer may avoid unstable learning and divergence problems in conventional
    actor-critic methods [[155](#bib.bib155)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Power Control and Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the prevalence of IoT and smart mobile devices, mobile crowdsensing becomes
    a cost-effective solution for network information collection to support more intelligent
    operations of wireless systems. The authors in [[156](#bib.bib156)] consider spectrum
    sensing and power control in non-cooperative cognitive radio networks. There is
    no information exchange between PUs and SUs. As such, the SU outsources the sensing
    task to a set of spatially distributed sensing devices to collect information
    about the PU’s power control strategy. The SU’s power control can be formulated
    as an MDP. The system state is determined by the Received Signal Strength (RSS)
    at individual sensing devices. The SU chooses its transmit power from the set
    of pre-specified power levels based on the current state. A reward is obtained
    if both primary and SUs can fulfill their SNR requirements. Considering the randomness
    in RSS measurements, the authors propose a DQL scheme for the SU to learn and
    adjust its transmit power. The DQL is then implemented by a DQN by using FNN.
    The simulation results show that the proposed DQL scheme is able to converge to
    a close-to-optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[157](#bib.bib157)] leverage the DQL framework for sensing and
    control problems in a Wireless Sensor and Actor Network (WSAN), which is a group
    of wireless devices with the ability to sense events and to perform actions based
    on the sensed data shared by all sensors. The system state includes processing
    power, mobility abilities, and functionalities of the actors and sensors. The
    mobile actor can choose its moving direction, networking, sensing and actuation
    policies to maximize the number of connected actor nodes and the number of sensing
    events.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[158](#bib.bib158)] focus on mobile crowdsensing paradigm, where
    data inference is incorporated to reduce sensing costs while maintaining the quality
    of sensing. The target sensing area is split into a set of cells. The objective
    of a sensing task is to collect data (e.g., temperature, air quality) in all the
    cells. A DQL-based cell selection mechanism is proposed for the mobile sensors
    to decide which cell is a better choice to perform sensing tasks. The system state
    includes the selection matrices for a few past decision epochs. The reward function
    is determined by the sensing quality and cost in the chosen cells. To extract
    temporal correlations in learning, the authors propose the DRQN that uses LSTM
    layers in DQL to capture the hidden patterns in state transitions. Considering
    inter-data correlations, the authors use the transfer learning method to reduce
    the amount of data in training. That is, the cell selection strategy learned for
    one task can benefit another correlated task. Hence, the parameters of DRQN can
    be initialized by another DRQN with rich training data. Simulations are conducted
    based on two real-life datasets collected from sensor networks, i.e., the Sensor-Scope
    dataset [[159](#bib.bib159)] in the EPFL campus and the U-Air dataset of air quality
    readings in Beijing [[160](#bib.bib160)]. The experiments verify that DRQN reduces
    up to 15% of the sensed cells with the same inference quality guarantee. The authors
    in [[161](#bib.bib161)] combine UAV and unmanned vehicle in mobile crowdsensing
    for smart city applications. The UAV cruises in the above of the target region
    for city-level data collection. Meanwhile, the unmanned vehicle carrying mobile
    charging stations moves on the ground and can charge the UAV at a preset charging
    point.
  prefs: []
  type: TYPE_NORMAL
- en: The target region is divided into multiple subregions and each subregion has
    a different sample priority. The authors in [[161](#bib.bib161)] propose a DQL-based
    control framework for the unmanned vehicle to schedule its data collection, constrained
    by limited energy supply. The system state includes information about the sample
    priority of each subregion, the location of charging point, and the moving trace
    of the UAV and unmanned vehicle. The UAV and unmanned vehicle can choose the moving
    direction. The DQL framework utilizes CNNs for extracting the correlation of adjacent
    subregions, which can increase the convergence speed in training. The DQL algorithm
    can be enhanced by using a feasible control solution as the baseline during exploration.
    The PER method is also used in DQL to assign higher priorities to important transitions
    so that the DQL agent can learn from samples more efficiently. The proposed scheme
    is evaluated by using real dataset of taxi traces in Rome [[162](#bib.bib162)].
    Simulation results reveal that the proposed DQL algorithm can obtain the highest
    data collection rate compared with the MDP and other heuristic baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile crowdsensing is vulnerable to faked sensing attacks, as selfish users
    may report faked sensing results to save their sensing costs and avoid compromising
    their privacy. The authors in [[163](#bib.bib163)] formulate the interactions
    between the server and a number of crowdsensing users as a Stackelberg game. The
    server is the leader that sets and broadcasts its payment policy for different
    sensing accuracy. In particular, the higher payment is set for more sensing accuracy.
    Based on the server’s sensing policy, each user as a follower then chooses its
    sensing effort and thus the sensing accuracy to receive the payment. The payment
    motivates the users to put in sensing efforts and thus the payment decision process
    can be modeled as an MDP. In a dynamic network, the server uses the DQL to derive
    the optimal payment to maximize its utility, based on the system state consisting
    of the previous sensing quality and the payment policy. The DQL uses a deep CNN
    to accelerate the learning process and improve the crowdsensing performance against
    selfish users. Simulation results show that the DQL-based scheme produces a higher
    sensing quality, lower attack rate, and higher utility of the server, exceeding
    those of both the Q-learning and the random payment strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Social networking is an important component of smart city applications. The
    authors in [[164](#bib.bib164)] aim to extract useful information by observing
    and analyzing the users’ behaviors in social networking. One of the main difficulties
    is that the social behaviors are usually fuzzy and divergent. The authors model
    pervasive social networking as a monopolistically competitive market, which contains
    different users as data providers selling information at a certain price. Given
    the market model, the DQL can be used to estimate the users’ behavior patterns
    and find the market equilibrium. Considering the costly deep learning structure,
    the authors in [[164](#bib.bib164)] propose a Decentralized DRL (DDRL) framework
    that decomposes the costly deep component from the RL algorithms at individual
    users. The deep component can be a feature extractor integrated with the network
    infrastructure and provide mutual knowledge for all individuals. Multiple RL agents
    can purchase the most desirable data from the mutual knowledge. The authors combine
    well-known RL algorithms, i.e., Q-learning and learning automata, to estimate
    users’ patterns which are described by vectors of probabilities representing the
    users’ preferences or altitudes to different information. In social networking
    and smart city applications with human involvement, there can be both labeled
    and unlabeled data and hence a semi-supervised DRL framework can be designed,
    by combining the strengths of DNNs and statistical modeling to improve the performance
    and accuracy in learning. Then, the authors in [[165](#bib.bib165)] introduce
    the semi-supervised DRL framework that utilizes variational auto-encoders [[166](#bib.bib166)]
    as an inference engine to infer the classification of unlabeled data. As a case
    study, the proposed DRL framework is customized to provide indoor localization
    based on the RSS from Bluetooth devices. The positioning environment contains
    a set of positions. Each position is associated with the set of RSS values from
    the set of anchor devices with known positions. The system state includes a vector
    of RSS values, the current location, and the distance to the target. The DQL agent,
    i.e., the positioning algorithm itself, chooses a moving direction to minimize
    the error distance to the target point. Simulations tested on real-world dataset
    show an improvement of 23% in terms of the error distance to the target compared
    with the supervised DRL scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: In this section, we review miscellaneous uses of DRL in wireless and
    networked systems. DRL provides a flexible tool in rich and diversified applications,
    conventionally involving dynamic system modeling and multi-agent interactions.
    All these imply a huge space of state transitions and actions. These approaches
    are summarized along with the references in Table [VI](#S6.T6 "TABLE VI ‣ VI-C
    Power Control and Data Collection ‣ VI Miscellaneous Issues ‣ Applications of
    Deep Reinforcement Learning in Communications and Networking: A Survey"). We observe
    that the NUM problems in 5G ecosystem for traffic engineering and resource allocation
    face very diversified control variables, including discrete indicators, e.g.,
    for BS (de)activation, user/cell association, and path selection, as well as continuous
    variables such as bandwidth allocation, transmit power, and beamforming optimization.
    Hence, both DQL and policy gradient methods are used extensively for discrete
    and continuous control problems, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: A summary of applications of DQL for traffic engineering, resource
    scheduling, and data collection.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Issues | Ref. | Model | Learning algorithms | Agent | States | Actions |
    Rewards | Scenarios |'
  prefs: []
  type: TYPE_TB
- en: '| Traffic engineering and routing | [[133](#bib.bib133)] | MDP | DQN using
    actor-critic networks | Network controller | Bandwidth request of each node pair
    | Traffic load split on different paths | Mean network delay | 5G network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[136](#bib.bib136)] | NUM | DQN using actor-critic networks | Network
    controller | Throughput and delay performance | Traffic load split on different
    paths | $\alpha$-fairness utility | 5G network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[130](#bib.bib130)] | POMDP | DQN using actor-critic networks | UAV |
    Local sensory information, e.g., distances and angles | Turn left or right | Composite
    reward | UAV navigation |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[141](#bib.bib141)] [[142](#bib.bib142)] | Game | DQN using ESN | UAV
    | Coordinates, distances, and orientation angles | Path, transmit power, and cell
    association | Weighted sum of energy efficiency, latency, and interference | Cellular-connected
    UAVs |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[143](#bib.bib143)] | MDP | DQN using FNN | Train scheduler | Channel
    conditions, train position, speed, SNR, and handoff indicator | Making handoff
    of connection, or accelerate or decelerate the train | Tracking error and energy
    consumption | Vehicle-to-infrastructure system |'
  prefs: []
  type: TYPE_TB
- en: '| Resource sharing and scheduling | [[145](#bib.bib145)] | MDP | DQN using
    FNN | Cloud baseband unit | MUs’ demands and the RRHs’ working states | Turn on
    or off certain RRH(s), and beamforming allocation | Expected power consumption
    | Cloud RAN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[149](#bib.bib149)] | MDP | DQN with CNN | Network controller | Network
    topology, QoS/QoE status, and the QoS requirements | Successive VNF instance |
    Composite function of QoE gain, QoS constraints penalty, and OPEX penalty | Cellular
    system |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[152](#bib.bib152)] | MDP | DQN using FNN | Network controller | The
    number of arrived packets/the priority and time-stamp of flows | Bandwidth/SFC
    allocation | Weighted sum of spectrum efficiency and QoE/waiting time in SFCs
    | 5G network |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[154](#bib.bib154)] | MDP | DQN using actor-critic networks | Central
    scheduler | Current scheduling decision and the workload | Assignment of each
    thread | Average processing time | Distributed stream data processing |'
  prefs: []
  type: TYPE_TB
- en: '| Data collection | [[156](#bib.bib156)] | MDP | DQN using FNN | Secondary
    user | Received signal strength at individual sensors | Transmit power | Fixed
    reward if QoS satisfied | CRN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[158](#bib.bib158)] | MDP | DRQN, LSTM, transfer learning | Mobile sensors
    | Cell selection matrices | Next cell for sensing | A function of the sensing
    quality and cost | WSN |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[161](#bib.bib161)] | MDP | DQN using CNN | UAV and unmanned vehicle
    | Subregions’ sample priority, charging point’s location, and trace of the UAV
    and unmanned vehicle | Moving direction of the UAV and unmanned vehicle | Fixed
    reward related to subregions’ sample priority | UAV and vehicle |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[163](#bib.bib163)] | Game | DQN using CNN | Crowdsensing server | Previous
    sensing quality and payment policy | Current payment policy | Utility | Mobile
    crowdsensing |'
  prefs: []
  type: TYPE_TB
- en: '|  | [[164](#bib.bib164)] | Game | DDQN | Mobile users | Current preferences
    | Positive or negative altitude | Reward or profit | Mobile social network |'
  prefs: []
  type: TYPE_TB
- en: VII Challenges, Open Issues, and Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different approaches reviewed in this survey evidently show that DRL can effectively
    address various emerging issues in communications and networking. There are existing
    challenges, open issues, and new research directions which are discussed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VII-A1 State Determination in Density Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DRL approaches, e.g., [[23](#bib.bib23)], allow the users to find an optimal
    access policy without having complete and/or accurate network information. However,
    the DRL approaches often require the users to report their local states at every
    time slot. To observe the local state, the user needs to monitor Received Signal
    Strength Indicators (RSSIs) from its neighboring BSs, and then it temporarily
    connects to the BS with the maximum RSSI. However, the future networks will deploy
    a high density of the BSs, and the RSSIs from different BSs may not be different.
    Thus, it is challenging for the users to determine the temporary BS [[167](#bib.bib167)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-A2 Knowledge of Jammers’ Channel Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DRL approach for wireless security as proposed in [[118](#bib.bib118)] enables
    the UAV to find optimal transmit power levels to maximize the security capacity
    of the UAV and the BS. However, to formulate the reward of the UAV, a perfect
    knowledge of channel information of the jammers is required. This is challenging
    and even impossible in practice.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A3 Multi-agent DRL in Dynamic HetNets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the existing works focus on the customizations of DRL framework for
    individual network entities, based on locally observed or exchanged network information.
    Hopefully, the network environment is relatively static to ensure convergent learning
    results and stable policies. This requirement may be challenged in a dynamic heterogenous
    5G network, which consists of hierarchically nested IoT devices/networks with
    fast changing service requirements and networking conditions. In such a situation,
    the DQL agents for individual entities have to be light-weighted and agile to
    the change of network conditions. This implies a reduce to the state and action
    spaces in learning, which however may compromise the performance of the convergent
    policy. The interactions among multiple agents also complicate the network environment
    and cause a considerable increase to the state space, which inevitably slows down
    the learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A4 Training and Performance Evaluation of DRL Framework
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DRL framework requires large amounts of data for both training and performance
    evaluation. In wireless systems, such data is not easily accessible as we rarely
    have referential data pools as other deep learning scenarios, e.g., computer vision.
    Most of the existing works rely on simulated dataset, which undermines the confidence
    of the DRL framework in practical system. The simulated data set is usually generated
    by a specific stochastic model, which is a simplification of the real system and
    may overlook the hidden patterns. Hence, a more effective way for generating simulation
    data is required to ensure that the training and performance evaluation of the
    DRL framework are more consistent with practical system.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Open Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VII-B1 Distributed DRL Framework in Wireless Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The DRL framework requires large amounts of training for DNNs. This may be implemented
    at a centralized network controller, which has sufficient computational capacity
    and the capability for information collection. However, for massive end users
    with limited capabilities, it becomes a meaningful task to design distributed
    implementation for the DRL framework that decomposes resource-demanding basic
    functionalities, e.g., information collection, sharing, and DNN training, from
    reinforcement learning algorithms at individual devices. The basic functionalities
    can be integrated with the network controller. It remains an open issue for the
    design of network infrastructure that supports these common functionalities for
    distributed DRL. The overhead of information exchange between end users and network
    controller also has to be well controlled.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B2 Balance between Information Quality and Learning Performance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The majority of the existing works consider the orchestration of networking,
    transmission control, offloading, and caching decisions in one DRL framework to
    derive the optimal policy, e.g., [[84](#bib.bib84), [86](#bib.bib86), [85](#bib.bib85),
    [87](#bib.bib87), [88](#bib.bib88), [89](#bib.bib89), [91](#bib.bib91), [92](#bib.bib92)].
    However, from a practical viewpoint, the network system will have to pay substantially
    increasing cost for information gathering. The cost is incurred from large delay,
    pre-processing of asynchronous information, excessive energy consumption, reduced
    learning speed, etc. Hence, an open issue is to find the optimal balance between
    information quality and learning performance so that the DQL agent does not consume
    too much resources only to achieve insignificantly marginal increase in the learning
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Future Research Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: VII-C1 DRL for Channel Estimation in Wireless Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Massive MIMO will be deployed for 5G to achieve high-speed communications at
    Gbps. For this, the channel estimation is the prerequisite for realizing massive
    MIMO. However, in a large-scale heterogeneous cellular network foreseen for 5G
    or beyond, the required channel estimation is very challenging. Thus, DRL will
    play an important role in acquiring the channel estimates with regard to dynamic
    time-varying wireless channels.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we expect that the combination of Wireless Power Transfer (WPT) and Mobile
    Crowd Sensing (MCS), namely Wireless-Powered Crowd Sensing (WPCS) will be a promising
    technique for the emerging IoT services. To this end, a higher power transfer
    efficiency of WPT is very critical to enable the deployment of WPCS in low-power
    wide area network. A "large-scale array antenna based WPT" will achieve this goal
    of higher WPT efficiency, but the channel estimation should be performed with
    minimal power consumption at a sensor node. This is because of that the sensor
    must operate with self-powering via WPT from the dedicated energy source, e.g.,
    power beacon, Wi-Fi or small-cell access point, and/or ambient RF sources, e.g.,
    TV tower, Wi-Fi AP and cellular BS. In this regard, the channel estimation based
    on the receive power measurements at the sensor node is one viable solution, because
    the receive power can be measured by the passive-circuit power meter with negligible
    power consumption. DRL can be used for the time-varying wireless channels with
    temporal correlations over time by taking the receive power measurements from
    the sensor node as the input for DRL, which will enable the channel estimation
    for WPT efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C2 DRL for Crowdsensing Service Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In MCS, mobile users contribute sensing data to a crowdsensing service provider
    and receive an incentive in return. However, due to limited resources, e.g., bandwidth
    and energy, the mobile user has to decide on whether and how much data to be uploaded
    to the provider. Likewise, the provider aiming to maximize its profit has to determine
    the amount of incentive to be given. The provider’s decision depends on the actions
    of the mobile users. For example, with many mobile users contributing data to
    the crowdsensing service provider, the provider can lower the incentive. Due to
    a large state space of a large number of users and dynamic environment, DRL can
    be applied to obtain an optimal crowdsensing policy similar to [[168](#bib.bib168)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-C3 DRL for Cryptocurrency Management in Wireless Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pricing and economic models have been widely applied to wireless networks [[169](#bib.bib169)], [[170](#bib.bib170)].
    For example, wireless users pay money to access radio resources or mobile services.
    Alternatively, the users can receive money if they contribute to the networks,
    e.g., offering a relay or cache function. However, using real money and cash in
    such scenarios faces many issues related to accounting, security, and privacy.
    Recently, the concept of cryptocurrency based on the blockchain technology has
    been introduced and adopted in wireless networks, e.g., [[171](#bib.bib171)],
    which has been shown to be a secure and efficient solution. However, the value
    of cryptocurrency, i.e., token or coin, can be highly dynamic depending on many
    market factors. The wireless users possessing the tokens can decide to keep or
    spend the tokens, e.g., for radio resource access and service usage or exchange
    into real money. In the random cryptocurrency market environment, DRL can be applied
    to achieve the maximum long-term reward of the cryptocurrency management for wireless
    users as in [[172](#bib.bib172)].
  prefs: []
  type: TYPE_NORMAL
- en: VII-C4 DRL for Auction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An auction has been effectively used for radio resource management, e.g., spectrum
    allocation [[173](#bib.bib173)]. However, obtaining the solution of the auction,
    e.g., a winner determination problem, can be complicated and intractable when
    the number of participants, i.e., bidders and sellers, become very large. Such
    a scenario is typical in next-generation wireless networks such as 5G highly-dense
    heterogeneous networks. DRL appears to be an efficient approach for solving different
    types of auctions such as in [[174](#bib.bib174)].
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper has presented a comprehensive survey of the applications of deep
    reinforcement learning to communications and networking. First, we have presented
    an overview of reinforcement learning, deep learning, and deep reinforcement learning.
    Then, we have introduced various deep reinforcement learning techniques and their
    extensions. Afterwards, we have provided detailed reviews, analyses, and comparisons
    of the deep reinforcement learning to solve different issues in communications
    and networking. The issues include dynamic network access, data rate control,
    wireless caching, data offloading, network security, connectivity preservation,
    traffic routing, and data collection. Finally, we have outlined important challenges,
    open issues as well as future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    press Cambridge, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, *Deep learning*.   MIT
    press Cambridge, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] (2016, Jan.) Google achieves ai “breakthrough” by beating go champion.
    BBC. [Online]. Available: https://www.bbc.com/news/technology-35420579'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. L. Puterman, *Markov decision processes: discrete stochastic dynamic
    programming*.   John Wiley & Sons, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas,
    *Dynamic programming and optimal control*.   Athena scientific Belmont, MA, 2005,
    vol. 1, no. 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] R. Bellman, *Dynamic programming*.   Mineola, NY: Courier Corporation,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “A brief
    survey of deep reinforcement learning,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, pp. 26–38, Nov. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine learning
    for wireless networks with artificial intelligence: A tutorial on neural networks,”
    *arXiv preprint arXiv:1710.02913*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch, “Multi-agent
    actor-critic for mixed cooperative-competitive environments,” in *Advances in
    Neural Information Processing Systems*, 2017, pp. 6379–6390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] G. E. Monahan, “State of the art-a survey of partially observable markov
    decision processes: theory, models, and algorithms,” *Management Science*, vol. 28,
    no. 1, pp. 1–16, 1982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] L. S. Shapley, “Stochastic games,” *Proceedings of the national academy
    of sciences*, vol. 39, no. 10, pp. 1095–1100, 1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Hu and M. P. Wellman, “Nash q-learning for general-sum stochastic games,”
    *Journal of machine learning research*, vol. 4, no. Nov, pp. 1039–1069, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] W. C. Dabney, “Adaptive step-sizes for reinforcement learning,” Ph.D.
    dissertation, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C. J. Watkins and P. Dayan, “Q-learning,” *Machine learning*, vol. 8,
    no. 3-4, pp. 279–292, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, p.
    529, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Lin, X. Dai, L. Li, and F.-Y. Wang, “An efficient deep reinforcement
    learning model for urban traffic control,” *arXiv preprint arXiv:1808.01876*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning
    for robotic manipulation with asynchronous off-policy updates,” in *IEEE International
    Conference on Robotics and Automation (ICRA)*, 2017, pp. 3389–3396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Thrun and A. Schwartz, “Issues in using function approximation for
    reinforcement learning,” in *Proceedings of Connectionist Models Summer School
    Hillsdale, NJ. Lawrence Erlbaum*, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] H. V. Hasselt, “Double q-learning,” in *Advances in Neural Information
    Processing Systems*, 2010, pp. 2613–2621.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning.” in *AAAI*, vol. 2, Phoenix, AZ, Feb. 2016, pp. 2094–2100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning for
    dynamic spectrum access in multichannel wireless networks,” *arXiv preprint arXiv:1704.02613*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, “Deep reinforcement
    learning for user association and resource allocation in heterogeneous networks,”
    in *IEEE GLOBECOM*, Abu Dhabi, UAE, Dec. 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *arXiv preprint arXiv:1511.05952*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning*, New York, NY, Jun. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning*, New York City, New York, Jun.
    2016, pp. 1928–1937.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Z. Wang, Y. Xu, L. Li, H. Tian, and S. Cui, “Handover control in wireless
    systems via asynchronous multi-user deep reinforcement learning,” *arXiv preprint
    arXiv:1801.02077*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” *arXiv preprint arXiv:1707.06887*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih,
    R. Munos, D. Hassabis, O. Pietquin *et al.*, “Noisy networks for exploration,”
    in *International Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining improvements in
    deep reinforcement learning,” in *The Thirty-Second AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    San Juan, Puerto Rico, USA, May 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *ICML*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially observable
    mdps,” *CoRR, abs/1507.06527*, vol. 7, no. 1, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. Zhao, H. Wang, K. Shao, and Y. Zhu, “Deep reinforcement learning with
    experience replay based on sarsa,” in *IEEE Symposium Series on Computational
    Intelligence (SSCI)*, 2016, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] W. Wang, J. Hao, Y. Wang, and M. Taylor, “Towards cooperation in sequential
    prisoner’s dilemmas: a deep multiagent reinforcement learning approach,” *arXiv
    preprint arXiv:1803.00162*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Heinrich and D. Silver, “Deep reinforcement learning from self-play
    in imperfect-information games,” *arXiv preprint arXiv:1603.01121*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] D. Fooladivanda and C. Rosenberg, “Joint resource allocation and user
    association for heterogeneous wireless cellular networks,” *IEEE Transactions
    on Wireless Communications*, vol. 12, no. 1, pp. 248–257, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Lin, W. Bao, W. Yu, and B. Liang, “Optimizing user association and
    spectrum allocation in hetnets: A utility perspective,” *IEEE Journal on Selected
    Areas in Communications*, vol. 33, no. 6, pp. 1025–1039, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access,” in *International Conference on Computing,
    Networking and Communications (ICNC)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. A. Riedmiller, “Playing atari with deep reinforcement learning,” *CoRR*,
    vol. abs/1312.5602, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Govindan. Tutornet: A low power wireless iot testbed. [Online]. Available:
    http://anrg.usc.edu/www/tutornet/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Q. Zhao, B. Krishnamachari, and K. Liu, “On myopic sensing for multi-channel
    opportunistic access: structure, optimality, and performance,” *IEEE Transactions
    on Wireless Communications*, vol. 7, no. 12, pp. 5431–5440, December 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement
    learning for dynamic multichannel access in wireless networks,” *IEEE Transactions
    on Cognitive Communications and Networking*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-q-learning-based transmission
    scheduling mechanism for the cognitive internet of things,” *IEEE Internet of
    Things Journal*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning based multi-access
    control and battery prediction with energy harvesting in iot systems,” *arXiv
    preprint arXiv:1805.05929*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] H. Ye and G. Y. Li, “Deep reinforcement learning for resource allocation
    in v2v communications,” *arXiv preprint arXiv:1711.00968*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] U. Challita, L. Dong, and W. Saad, “Proactive resource management in lte-u
    systems: A deep learning perspective,” *arXiv preprint arXiv:1702.07031*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Balazinska and P. Castro. (2003) Ibm watson research center. [Online].
    Available: https://crawdad.org/ibm/watson/20030219'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and N. De Freitas,
    “Dueling network architectures for deep reinforcement learning,” 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] H. Li, “Multiagent learning for aloha-like spectrum access in cognitive
    radio systems,” *EURASIP Journal on Wireless Communications and Networking*, vol.
    2010, no. 1, pp. 1–15, May 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] S. Liu, X. Hu, and W. Wang, “Deep reinforcement learning based dynamic
    channel allocation algorithm in multibeam satellite systems,” *IEEE ACCESS*, vol. 6,
    pp. 15 733–15 742, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for resource
    allocation in a network of cache-enabled lte-u uavs,” in *IEEE GLOBECOM*, 2017,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] W. Maass, “Liquid state machines: motivation, theory, and applications,”
    in *Computability in context: computation and logic in the real world*.   World
    Scientific, 2011, pp. 275–296.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Chen, M. Mozaffari, W. Saad, C. Yin, M. Debbah, and C. S. Hong, “Caching
    in the sky: Proactive deployment of cache-enabled unmanned aerial vehicles for
    optimized quality-of-experience,” *IEEE Journal on Selected Areas in Communications*,
    vol. 35, no. 5, pp. 1046–1061, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] I. Szita, V. Gyenes, and A. Lőrincz, “Reinforcement learning with echo
    state networks,” in *International Conference on Artificial Neural Networks*.   Springer,
    2006, pp. 830–839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Tyouku of china network video index. [Online]. Available: http://index.youku.com/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] T. Stockhammer, “Dynamic adaptive streaming over http–: standards and
    design principles,” in *Proceedings of the second annual ACM conference on Multimedia
    systems*.   ACM, 2011, pp. 133–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] M. Gadaleta, F. Chiariotti, M. Rossi, and A. Zanella, “D-dash: A deep
    q-learning framework for dash video streaming,” *IEEE Transactions on Cognitive
    Communications and Networking*, vol. 3, no. 4, pp. 703–718, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Klaue, B. Rathke, and A. Wolisz, “Evalvid–a framework for video transmission
    and quality evaluation,” in *International conference on modelling techniques
    and tools for computer performance evaluation*, 2003, pp. 255–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Mao, R. Netravali, and M. Alizadeh, “Neural adaptive video streaming
    with pensieve,” in *Proceedings of the Conference of the ACM Special Interest
    Group on Data Communication*.   ACM, 2017, pp. 197–210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] H. Riiser, P. Vigmostad, C. Griwodz, and P. Halvorsen, “Commute path bandwidth
    traces from 3g networks: analysis and applications,” in *Proceedings of the 4th
    ACM Multimedia Systems Conference*.   ACM, 2013, pp. 114–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] X. Yin, A. Jindal, V. Sekar, and B. Sinopoli, “A control-theoretic approach
    for dynamic adaptive video streaming over http,” in *ACM SIGCOMM Computer Communication
    Review*, vol. 45, no. 4.   ACM, 2015, pp. 325–338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] T. Huang, R.-X. Zhang, C. Zhou, and L. Sun, “Qarc: Video quality aware
    rate control for real-time video streaming based on deep reinforcement learning,”
    *arXiv preprint arXiv:1805.02482*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] (2016) Measuring fixed broadband report. [Online]. Available: https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Chinchali, P. Hu, T. Chu, M. Sharma, M. Bansal, R. Misra, M. Pavone,
    and K. Sachin, “Cellular network traffic scheduling with deep reinforcement learning,”
    in *National Conference on Artificial Intelligence (AAAI)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Z. Zhang, Y. Zheng, M. Hua, Y. Huang, and L. Yang, “Cache-enabled dynamic
    rate allocation via deep self-transfer reinforcement learning,” *arXiv preprint
    arXiv:1803.11334*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G.
    Bilén, R. C. Reinhart, and D. J. Mortensen, “Multi-objective reinforcement learning
    for cognitive satellite communications using deep neural network ensembles,” *IEEE
    Journal on Selected Areas in Communications*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. Tarchi, G. E. Corazza, and A. Vanelli-Coralli, “Adaptive coding and
    modulation techniques for next generation hand-held mobile satellite communications,”
    in *IEE ICC*, 2013, pp. 4504–4508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] M. T. Hagan and M. B. Menhaj, “Training feedforward networks with the
    marquardt algorithm,” *IEEE transactions on Neural Networks*, vol. 5, no. 6, pp.
    989–993, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Zhong, M. C. Gursoy, and S. Velipasalar, “A deep reinforcement learning-based
    framework for content caching,” *arXiv preprint arXiv:1712.08132*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    *CoRR*, vol. abs/1509.02971, 2015\. [Online]. Available: http://arxiv.org/abs/1509.02971'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] G. Dulac-Arnold, R. Evans, P. Sunehag, and B. Coppin, “Reinforcement learning
    in large discrete action spaces,” *CoRR*, vol. abs/1512.07679, 2015\. [Online].
    Available: http://arxiv.org/abs/1512.07679'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] L. Lei, L. You, G. Dai, T. X. Vu, D. Yuan, and S. Chatzinotas, “A deep
    learning approach for optimizing content delivering in cache-enabled HetNet,”
    in *Int’l Sym. Wireless Commun. Systems (ISWCS)*, Aug. 2017, pp. 449–453.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Schaarschmidt, F. Gessert, V. Dalibard, and E. Yoneki, “Learning runtime
    parameters in computer systems with delayed experience injection,” *arXiv preprint
    arXiv:1610.09903*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, “Benchmarking
    cloud serving systems with YCSB,” in *proc. 1st ACM Sym. Cloud Comput.*, 2010,
    pp. 143–154.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. He and S. Hu, “Cache-enabled wireless networks with opportunistic interference
    alignment,” *arXiv preprint arXiv:1706.09024*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. He, C. Liang, F. R. Yu, N. Zhao, and H. Yin, “Optimization of cache-enabled
    opportunistic interference alignment wireless networks: A big data deep reinforcement
    learning approach,” in *IEEE ICC*, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. Leung, and Y. Zhang,
    “Deep-reinforcement-learning-based optimization for cache-enabled opportunistic
    interference alignment wireless networks,” *IEEE Transactions on Vehicular Technology*,
    vol. 66, no. 11, pp. 10 433–10 445, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] X. He, K. Wang, H. Huang, T. Miyazaki, Y. Wang, and S. Guo, “Green resource
    allocation based on deep reinforcement learning in content-centric iot,” *IEEE
    Transactions on Emerging Topics in Computing*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Q. Wu, Z. Li, and G. Xie, “CodingCache: Multipath-aware CCN cache with
    network coding,” in *proc. ACM SIGCOMM Workshop on Information-centric Networking*,
    2013, pp. 41–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Chen, W. Saad, and C. Yin, “Echo-liquid state deep learning for 360
    content transmission and caching in wireless vr networks with cellular-connected
    uavs,” *arXiv preprint arXiv:1804.03284*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Machine learning
    for wireless networks with artificial intelligence: A tutorial on neural networks,”
    *CoRR*, vol. abs/1710.02913, 2017\. [Online]. Available: http://arxiv.org/abs/1710.02913'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] M. Chen, W. Saad, and C. Yin, “Liquid state machine learning for resource
    allocation in a network of cache-enabled LTE-U UAVs,” in *IEEE GLOBECOM*, Dec.
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. He, Z. Zhang, and Y. Zhang, “A big data deep reinforcement learning
    approach to next generation green wireless networks,” in *IEEE GLOBECOM*, 2017,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. He, C. Liang, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, and Y. Zhang, “Resource
    allocation in software-defined and information-centric vehicular networks with
    mobile edge computing,” in *IEEE Vehicular Technology Conference*, 2017, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Y. He, F. R. Yu, N. Zhao, H. Yin, and A. Boukerche, “Deep reinforcement
    learning (drl)-based resource management in software-defined and virtualized vehicular
    ad hoc networks,” in *Proceedings of the 6th ACM Symposium on Development and
    Analysis of Intelligent Vehicular Networks and Applications*, 2017, pp. 47–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. He, N. Zhao, and H. Yin, “Integrated networking, caching, and computing
    for connected vehicles: A deep reinforcement learning approach,” *IEEE Transactions
    on Vehicular Technology*, vol. 67, no. 1, pp. 44–55, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T. L. Thanh and R. Q. Hu, “Mobility-aware edge caching and computing framework
    in vehicle networks: A deep reinforcement learning,” *IEEE Transactions on Vehicular
    Technology*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. He, F. R. Yu, N. Zhao, V. C. Leung, and H. Yin, “Software-defined networks
    with mobile edge computing and caching for smart cities: A big data deep reinforcement
    learning approach,” *IEEE Communications Magazine*, vol. 55, no. 12, pp. 31–37,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] B. Han, V. Gopalakrishnan, L. Ji, and S. Lee, “Network function virtualization:
    Challenges and opportunities for innovations,” *IEEE Communications Magazine*,
    vol. 53, no. 2, pp. 90–97, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. He, F. R. Yu, N. Zhao, and H. Yin, “Secure social networks in 5g systems
    with mobile edge computing, caching and device-to-device (d2d) communications,”
    *IEEE Wireless Communications*, vol. 25, no. 3, pp. 103–109, Jun. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. He, C. Liang, F. R. Yu, and Z. Han, “Trust-based social networks with
    computing, caching and communications: A deep reinforcement learning approach,”
    *IEEE Transactions on Network Science and Engineering*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] C. Zhang, Z. Liu, B. Gu, K. Yamori, and Y. Tanaka, “A deep reinforcement
    learning based approach for cost-and energy-aware multi-flow mobile data offloading,”
    *IEICE Transactions on Communications*, pp. 2017–2025.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] L. Ji, G. Hui, L. Tiejun, and L. Yueming, “Deep reinforcement learning
    based computation offloading and resource allocation for mec,” in *IEEE WCNC*,
    2018, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Performance optimization
    in mobile-edge computing via deep reinforcement learning,” *arXiv preprint arXiv:1804.00514*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] X. Chen, H. Zhang, C. Wu, S. Mao, Y. Ji, and M. Bennis, “Optimized computation
    offloading performance in virtual edge computing systems via deep reinforcement
    learning,” *arXiv preprint arXiv:1805.06146*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Ye and Y.-J. A. Zhang, “DRAG: Deep reinforcement learning based base
    station activation in heterogeneous networks,” *arXiv:1809.02159*, Sep. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Li, H. Gao, T. Lv, and Y. Lu, “Deep q-learning based dynamic resource
    allocation for self-powered ultra-dense networks,” in *IEEE ICC (ICC Workshops)*,
    2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. Liu, B. Krishnamachari, S. Zhou, and Z. Niu, “Deepnap: Data-driven
    base station sleeping operations through deep reinforcement learning,” *IEEE Internet
    of Things Journal*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Wan, G. Sheng, Y. Li, L. Xiao, and X. Du, “Reinforcement learning
    based mobile offloading for cloud-based malware detection,” in *IEEE GLOBECOM*,
    2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Xiao, X. Wan, C. Dai, X. Du, X. Chen, and M. Guizani, “Security in
    mobile edge caching with reinforcement learning,” *CoRR*, vol. abs/1801.05915,
    2018\. [Online]. Available: http://arxiv.org/abs/1801.05915'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. S. Shamili, C. Bauckhage, and T. Alpcan, “Malware detection on mobile
    devices using distributed machine learning,” in *proc. Int’l Conf. Pattern Recognition*,
    Aug. 2010, pp. 4348–4351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Li, J. Liu, Q. Li, and L. Xiao, “Mobile cloud offloading for malware
    detections with learning,” in *IEEE INFOCOM Workshops*, Apr. 2015, pp. 197–201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] M. Min, D. Xu, L. Xiao, Y. Tang, and D. Wu, “Learning-based computation
    offloading for iot devices with energy harvesting,” *arXiv preprint arXiv:1712.08768*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] L. Quan, Z. Wang, and F. Ren, “A novel two-layered reinforcement learning
    for task offloading with tradeoff between physical machine utilization rate and
    delay,” *Future Internet*, vol. 10, no. 7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] D. V. Le and C. Tham, “Quality of service aware computation offloading
    in an ad-hoc mobile cloud,” *IEEE Transactions on Vehicular Technology*, pp. 1–1,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] D. V. Le and C.-K. Tham, “A deep reinforcement learning based offloading
    scheme in ad-hoc mobile clouds,” in *Proceedings of IEEE INFOCOM IECCO Workshop*,
    Honolulu, USA., apr 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] S. Yu, X. Wang, and R. Langar, “Computation offloading for mobile edge
    computing: A deep learning approach,” in *IEEE PIMRC*, Oct. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Z. Tang, X. Zhou, F. Zhang, W. Jia, and W. Zhao, “Migration modeling
    and learning algorithms for containers in fog computing,” *IEEE Transactions on
    Services Computing*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] P. Popovski, H. Yomo, and R. Prasad, “Strategies for adaptive frequency
    hopping in the unlicensed bands,” *IEEE Wireless Communications*, vol. 13, no. 6,
    pp. 60–67, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] G. Han, L. Xiao, and H. V. Poor, “Two-dimensional anti-jamming communication
    based on deep reinforcement learning,” in *Proceedings of the 42nd IEEE International
    Conference on Acoustics, Speech and Signal Processing,*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] L. Xiao, D. Jiang, X. Wan, W. Su, and Y. Tang, “Anti-jamming underwater
    transmission with mobility and learning,” *IEEE Communications Letters*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Liu, Y. Xu, L. Jia, Q. Wu, and A. Anpalagan, “Anti-jamming communications
    using spectrum waterfall: A deep reinforcement learning approach,” *IEEE Communications
    Letters*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Chen, Y. Li, D. Xu, and L. Xiao, “Dqn-based power control for iot
    transmission against jamming,” in *IEEE 87th Vehicular Technology Conference (VTC
    Spring)*, 2018, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] X. Lu, L. Xiao, and C. Dai, “Uav-aided 5g communications with deep reinforcement
    learning against jamming,” *arXiv preprint arXiv:1805.06628*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] L. Xiao, X. Lu, D. Xu, Y. Tang, L. Wang, and W. Zhuang, “Uav relay in
    vanets against smart jamming with reinforcement learning,” *IEEE Transactions
    on Vehicular Technology*, vol. 67, no. 5, pp. 4087–4097, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Lv, L. Xiao, Q. Hu, X. Wang, C. Hu, and L. Sun, “Anti-jamming power
    control game in unmanned aerial vehicle networks,” in *IEEE GLOBECOM*, 2017, pp.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] L. Xiao, C. Xie, M. Min, and W. Zhuang, “User-centric view of unmanned
    aerial vehicle transmission against smart attacks,” *IEEE Transactions on Vehicular
    Technology*, vol. 67, no. 4, pp. 3420–3430, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Bowling and M. Veloso, “Multiagent learning using a variable learning
    rate,” *Artificial Intelligence*, vol. 136, no. 2, pp. 215–250, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Y. Chen, S. Kar, and J. M. Moura, “Cyber-physical attacks with control
    objectives,” *IEEE Transactions on Automatic Control*, vol. 63, no. 5, pp. 1418–1425,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust deep reinforcement
    learning for security and safety in autonomous vehicle systems,” *arXiv preprint
    arXiv:1805.00983*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M. Brackstone and M. McDonald, “Car-following: a historical review,”
    *Transportation Research Part F: Traffic Psychology and Behaviour*, vol. 2, no. 4,
    pp. 181–196, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. Ferdowsi and W. Saad, “Deep learning-based dynamic watermarking for
    secure signal authentication in the internet of things,” in *IEEE ICC*, 2018,
    pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] B. Satchidanandan and P. R. Kumar, “Dynamic watermarking: Active defense
    of networked cyber–physical systems,” *Proceedings of the IEEE*, vol. 105, no. 2,
    pp. 219–240, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. Ferdowsi and W. Saad, “Deep learning for signal authentication and
    security in massive internet of things systems,” *arXiv preprint arXiv:1803.00916*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] P. Vadakkepat, K. C. Tan, and W. Ming-Liang, “Evolutionary artificial
    potential fields and their application in real time robot path planning,” in *Proceedings
    of the 2000 Congress on Evolutionary Computation*, vol. 1, 2000, pp. 256–263.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] W. Huang, Y. Wang, and X. Yi, “Deep q-learning to preserve connectivity
    in multi-robot systems,” in *Proceedings of the 9th International Conference on
    Signal Processing Systems*.   ACM, 2017, pp. 45–50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] W. Huang, Y. Wang, and X. Yi, “A deep reinforcement learning approach
    to preserve connectivity for multi-robot systems,” in *International Congress
    on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)*,
    2017, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] H. A. Poonawala, A. C. Satici, H. Eckert, and M. W. Spong, “Collision-free
    formation control with decentralized connectivity preservation for nonholonomic-wheeled
    mobile robots,” *IEEE Transactions on control of Network Systems*, vol. 2, no. 2,
    pp. 122–130, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] C. Wang, J. Wang, X. Zhang, and X. Zhang, “Autonomous navigation of uav
    in large-scale unknown complex environment with deep reinforcement learning,”
    in *IEEE GlobalSIP*, 2017, pp. 858–862.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] C. Shen, C. Tekin, and M. van der Schaar, “A non-stochastic learning
    approach to energy efficient mobility management,” *IEEE Journal on Selected Areas
    in Communications*, vol. 34, no. 12, pp. 3854–3868, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] M. Faris and E. Brian, “Deep q-learning for self-organizing networks
    fault management and radio performance improvement,” *https://arxiv.org/abs/1707.02329*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] G. Stampa, M. Arias, D. Sanchez-Charles, V. Muntes-Mulero, and A. Cabellos,
    “A deep-reinforcement learning approach for software-defined networking routing
    optimization,” *arXiv preprint arXiv:1709.07080*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M. Roughan, “Simplifying the synthesis of internet traffic matrices,”
    *ACM SIGCOMM Computer Communication Review*, vol. 35, no. 5, pp. 93–96, 2015\.
    [Online]. Available: http://arxiv.org/abs/1710.02913'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Varga and R. Hornig, “An overview of the OMNeT++ simulation environment,”
    in *proc. Int’l Conf. Simulation Tools and Techniques for Communications, Networks
    and Systems & Workshops*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Z. Xu, J. Tang, J. Meng, W. Zhang, Y. Wang, C. H. Liu, and D. Yang, “Experience-driven
    networking: A deep reinforcement learning based approach,” *arXiv preprint arXiv:1801.05757*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] K. Winstein and H. Balakrishnan, “TCP ex Machina: Computer-generated
    congestion control,” in *ACM SIGCOMM*, 2013, pp. 123–134.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] R. G.F. and H. T.R., *Modeling and Tools for Network Simulation*.   Springer,
    Berlin, Heidelberg, 2010, ch. The ns-3 Network Simulator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. Medina, A. Lakhina, I. Matta, and J. Byers, “BRITE: an approach to
    universal topology generation,” in *IEEE MASCOTS*, Aug. 2001, pp. 346–353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient
    methods for reinforcement learning with function approximation,” in *proc. 12th
    Int’l Conf. Neural Inform. Process. Syst.*, 1999, pp. 1057–1063.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] U. Challita, W. Saad, and C. Bettstetter, “Deep reinforcement learning
    for interference-aware path planning of cellular connected uavs,” in *IEEE ICC*,
    Kansas City, MO, May 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] U. Challita, W. Saad, and C. Bettstetter, “Cellular-connected uavs over
    5g: Deep reinforcement learning for interference management,” *arXiv preprint
    arXiv:1801.05500*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] L. Zhu, Y. He, F. R. Yu, B. Ning, T. Tang, and N. Zhao, “Communication-based
    train control system performance optimization using deep reinforcement learning,”
    *IEEE Transactions on Vehicular Technology*, vol. 66, no. 12, pp. 10 705–10 717,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Yang, Y. Li, K. Li, S. Zhao, R. Chen, J. Wang, and S. Ci, “Decco:
    Deep-learning enabled coverage and capacity optimization for massive mimo systems,”
    *IEEE Access*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Z. Xu, Y. Wang, J. Tang, J. Wang, and M. C. Gursoy, “A deep reinforcement
    learning based framework for power-efficient resource allocation in cloud rans,”
    in *IEEE ICC*, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] T. M. Hackett, S. G. Bilén, P. V. R. Ferreira, A. M. Wyglinski, and R. C.
    Reinhart, “Implementation of a space communications cognitive engine,” in *Cognitive
    Communications for Aerospace Applications Workshop (CCAA)*, 2017, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Y. S. Nasir and D. Guo, “Deep reinforcement learning for distributed
    dynamic power allocation in wireless networks,” *arXiv preprint arXiv:1808.00490*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] X. Foukas, G. Patounas, A. Elmokashfi, and M. K. Marina, “Network slicing
    in 5g: Survey and challenges,” *IEEE Communications Magazine*, vol. 55, no. 5,
    pp. 94–100, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] X. Chen, Z. Li, Y. Zhang, R. Long, H. Yu, X. Du, and M. Guizani, “Reinforcement
    learning based qos/qoe-aware service function chaining in software-driven 5g slices,”
    *arXiv preprint arXiv:1804.02099*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] P. Reichl, S. Egger, R. Schatz, and A. D’Alconzo, “The logarithmic nature
    of qoe and the role of the weber-fechner law in qoe assessment,” in *IEEE ICC*,
    Cape Town, South Africa, May 2010, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] M. Fiedler, T. Hossfeld, and P. Tran-Gia, “A generic quantitative relationship
    between quality of experience and quality of service,” *IEEE Network*, vol. 24,
    no. 2, pp. 36–41, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Z. Zhao, R. Li, Q. Sun, Y. Yang, X. Chen, M. Zhao, H. Zhang *et al.*,
    “Deep reinforcement learning for network slicing,” *arXiv preprint arXiv:1805.06591*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource management
    with deep reinforcement learning,” in *Proceedings of the 15th ACM Workshop on
    Hot Topics in Networks*, 2016, pp. 50–56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] T. Li, Z. Xu, J. Tang, and Y. Wang, “Model-free control for distributed
    stream data processing using deep reinforcement learning,” *Proceedings of the
    VLDB Endowment*, vol. 11, no. 6, pp. 705–718, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] G. D. Arnold, R. Evans, H. v. Hasselt, P. Sunehag, T. Lillicrap, J. Hunt,
    T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement learning in large
    discrete action spaces,” *arXiv: 1512.07679*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] X. Li, J. Fang, W. Cheng, H. Duan, Z. Chen, and H. Li, “Intelligent power
    control for spectrum sharing in cognitive radios: A deep reinforcement learning
    approach,” *arXiv preprint arXiv:1712.07365*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] T. Oda, R. Obukata, M. Ikeda, L. Barolli, and M. Takizawa, “Design and
    implementation of a simulation system based on deep q-network for mobile actor
    node control in wireless sensor and actor networks,” in *International Conference
    on Advanced Information Networking and Applications Workshops (WAINA)*, 2017,
    pp. 195–200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] L. Wang, W. Liu, D. Zhang, Y. Wang, E. Wang, and Y. Yang, “Cell selection
    with deep reinforcement learning in sparse mobile crowdsensing,” *arXiv preprint
    arXiv:1804.07047*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] F. Ingelrest, G. Barrenetxea, G. Schaefer, M. Vetterli, O. Couach, and
    M. Parlange., “SensorScope: Application-specific sensor network for environmental
    monitoring,” *ACM Transactions on Sensor Networks*, vol. 6, no. 2, pp. 1–32, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Y. Zheng, F. Liu, and H. P. Hsieh, “U-Air: when urban air quality inference
    meets big data,” in *ACM SIGKDD Int’l Conf. Knowledge Discovery and Data Mining*,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] B. Zhang, C. H. Liu, J. Tang, Z. Xu, J. Ma, and W. Wang, “Learning-based
    energy-efficient data collection by unmanned vehicles in smart cities,” *IEEE
    Transactions on Industrial Informatics*, vol. 14, no. 4, pp. 1666–1676, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] L. Bracciale, M. Bonola, P. Loreti, G. Bianchi, R. Amici, and A. Rabuffi.
    (2014, Jul.) CRAWDAD dataset roma/taxi (v. 2014-07-17). [Online]. Available: http://crawdad.org/roma/taxi/20140717'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] L. Xiao, Y. Li, G. Han, H. Dai, and H. V. Poor, “A secure mobile crowdsensing
    game with deep reinforcement learning,” *IEEE Transactions on Information Forensics
    and Security*, vol. 13, no. 1, pp. 35–47, Jan. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Zhang, B. Song, and P. Zhang, “Social behavior study under pervasive
    social networking based on decentralized deep reinforcement learning,” *Journal
    of Network and Computer Applications*, vol. 86, pp. 72–81, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Mohammadi, A. Al-Fuqaha, M. Guizani, and J.-S. Oh, “Semisupervised
    deep reinforcement learning in support of iot and smart city services,” *IEEE
    Internet of Things Journal*, vol. 5, no. 2, pp. 624–635, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, “Semi-supervised
    learning with deep generative models,” in *Advances in Neural Information Processing
    Systems*, 2014, pp. 3581–3589.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] G. Cao, Z. Lu, X. Wen, T. Lei, and Z. Hu, “Aif: An artificial intelligence
    framework for smart wireless network management,” *IEEE Communications Letters*,
    vol. 22, no. 2, pp. 400–403, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Zhan, Y. Xia, J. Zhang, T. Li, and Y. Wang, “Crowdsensing game with
    demand uncertainties: A deep reinforcement learning approach,” submitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] N. C. Luong, P. Wang, D. Niyato, Y. Wen, and Z. Han, “Resource management
    in cloud networking using economic analysis and pricing models: a survey,” *IEEE
    Communications Surveys & Tutorials*, vol. 19, no. 2, pp. 954–1001, Jan. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] N. C. Luong, D. T. Hoang, P. Wang, D. Niyato, D. I. Kim, and Z. Han,
    “Data collection and wireless communication in internet of things (iot) using
    economic analysis and pricing models: A survey,” *IEEE Communications Surveys
    & Tutorials*, vol. 18, no. 4, pp. 2546–2590, Jun. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] F. Shi, Z. Qin, and J. A. McCann, “Oppay: Design and implementation of
    a payment system for opportunistic data services,” in *IEEE International Conference
    on Distributed Computing Systems*, Atlanta, GA, Jul. 2017, pp. 1618–1628.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Z. Jiang and J. Liang, “Cryptocurrency portfolio management with deep
    reinforcement learning,” in *Intelligent Systems Conference (IntelliSys)*, 2017,
    pp. 905–913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] N. C. Luong, P. Wang, D. Niyato, Y.-C. Liang, F. Hou, and Z. Han, “Applications
    of economic and pricing models for resource management in 5g wireless networks:
    A survey,” *IEEE Communications Surveys and Tutorials*, to appear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] J. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He, “Deep reinforcement learning
    for sponsored search real-time bidding,” *arXiv preprint arXiv:1803.00259*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
