- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:03:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[1912.12528] Applications of Unsupervised Deep Transfer Learning to Intelligent
    Fault Diagnosis: A Survey and Comparative Study'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.12528](https://ar5iv.labs.arxiv.org/html/1912.12528)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zhibin Zhao, Qiyang Zhang, Xiaolei Yu, Chuang Sun, Shibin Wang, Ruqiang Yan, 
    and Xuefeng Chen Z. Zhao, Q. Zhang, X. Yu, C. Sun, S. Wang, R. Yan and X. Chen
    are with the State Key Laboratory for Manufacturing Systems Engineering, Xi’an
    Jiaotong University, Xi’an 710049, China. E-mail: (zhaozhibin@xjtu.edu.cn; zhangqiyang@stu.xjtu.edu.cn;
    yxl007@stu.xjtu.edu.cn; ch.sun@xjtu.edu.cn; wangshibin2008@gmail.com; yanruqiang@xjtu.edu.cn; chenxf@mail.xjtu.edu.cn)
    This work was supported by the Natural Science Foundation of China (No. 52105116)
    and by the China Postdoctoral Science Foundation (No. 2021M692557 and No. 2021TQ0263).
    R. Yan is the corresponding author.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent progress on intelligent fault diagnosis (IFD) has greatly depended on
    deep representation learning and plenty of labeled data. However, machines often
    operate with various working conditions or the target task has different distributions
    with the collected data used for training (the domain shift problem). Besides,
    the newly collected test data in the target domain are usually unlabeled, leading
    to unsupervised deep transfer learning based (UDTL-based) IFD problem. Although
    it has achieved huge development, a standard and open source code framework as
    well as a comparative study for UDTL-based IFD are not yet established. In this
    paper, we construct a new taxonomy and perform a comprehensive review of UDTL-based
    IFD according to different tasks. Comparative analysis of some typical methods
    and datasets reveals some open and essential issues in UDTL-based IFD which are
    rarely studied, including transferability of features, influence of backbones,
    negative transfer, physical priors, etc. To emphasize the importance and reproducibility
    of UDTL-based IFD, the whole test framework will be released to the research community
    to facilitate future research. In summary, the released framework and comparative
    study can serve as an extended interface and basic results to carry out new studies
    on UDTL-based IFD. The code framework is available at [https://github.com/ZhaoZhibin/UDTL](https://github.com/ZhaoZhibin/UDTL).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Intelligent fault diagnosis; Unsupervised deep transfer learning; Taxonomy and
    survey; Comparative study; Reproducibility
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid development of industrial big data and Internet of Things, Prognostic
    and Health Management (PHM) for industrial equipments, such as aero-engine, helicopter
    and high-speed train, is becoming increasingly popular, bringing out many intelligent
    maintenance systems. Intelligent fault diagnosis (IFD) is becoming an essential
    branch among PHM systems. IFD based on traditional machine learning methods [[1](#bib.bib1)],
    including random forest [[2](#bib.bib2)] and support vector machine [[3](#bib.bib3)],
    has been widely applied in research and industry scenarios. However, these methods
    often need to extract features manually or to combine with other advanced signal
    processing techniques, such as time frequency analysis [[4](#bib.bib4)] and sparse
    representation [[5](#bib.bib5), [6](#bib.bib6)]. While, with the increment of
    available data, data-driven methods with the representation learning ability are
    also becoming more and more important. Thus, Deep Learning (DL) [[7](#bib.bib7)],
    which can extract useful features automatically from original signals, gradually
    becomes a hot research topic for many fields [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)] as well as PHM [[12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14)]. Effective DL models, such as Convolutional Neural Network (CNN)
    [[15](#bib.bib15)], Sparse Autoencoder (SAE) [[16](#bib.bib16)], etc., for tasks
    in PHM have been validated successfully in current research, and a benchmark study
    is also given in [[17](#bib.bib17)] for better comparison and development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着工业大数据和物联网的快速发展，工业设备如航空发动机、直升机和高速列车的预测与健康管理（PHM）变得越来越受欢迎，催生了许多智能维护系统。智能故障诊断（IFD）正在成为PHM系统中的一个重要分支。基于传统机器学习方法的IFD
    [[1](#bib.bib1)]，包括随机森林 [[2](#bib.bib2)] 和支持向量机 [[3](#bib.bib3)]，已广泛应用于研究和工业场景。然而，这些方法通常需要手动提取特征或结合其他先进的信号处理技术，如时频分析
    [[4](#bib.bib4)] 和稀疏表示 [[5](#bib.bib5), [6](#bib.bib6)]。随着可用数据的增加，具有表示学习能力的数据驱动方法也变得越来越重要。因此，能够从原始信号中自动提取有用特征的深度学习（DL）
    [[7](#bib.bib7)]，逐渐成为许多领域 [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]
    以及PHM [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)] 的热门研究话题。有效的DL模型，如卷积神经网络（CNN）
    [[15](#bib.bib15)]、稀疏自编码器（SAE） [[16](#bib.bib16)] 等，已在当前研究中成功验证了其在PHM任务中的应用，同时[[17](#bib.bib17)]也提供了基准研究以便于更好的比较和发展。
- en: 'Behind the effectiveness of DL-based IFD, there exist two necessary assumptions:
    1) samples from the training dataset (source domain) should have the same distribution
    with that from the test dataset (target domain); 2) plenty of labeled data are
    available during the training phase. Although the labeled data might be generated
    by dynamic simulations or fault seeding experiments, the generated data are not
    strictly consistent with the test data in the real scenario. That is, DL models
    based on the training dataset only possess a weak generalization ability, when
    deployed to the test dataset from real applications. In addition, rotating machinery
    often operates with varying working conditions, such as loads and speeds, which
    also requires that trained models using the dataset from one working condition
    can successfully transfer to the test dataset from another working condition.
    In short, these factors make models trained in the source domain hard to be generalized
    or transferred to the target domain, directly.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL基础的IFD的有效性背后，存在两个必要的假设：1）训练数据集（源领域）中的样本应与测试数据集（目标领域）中的样本具有相同的分布；2）在训练阶段有大量标记数据可用。虽然标记数据可能通过动态仿真或故障种植实验生成，但生成的数据与实际场景中的测试数据并不完全一致。也就是说，基于训练数据集的DL模型在部署到真实应用的测试数据集时，仅具备较弱的泛化能力。此外，旋转机械往往在不同的工作条件下运行，例如负载和速度，这也要求使用一种工作条件下的数据集训练的模型能够成功转移到另一种工作条件下的测试数据集。简而言之，这些因素使得在源领域训练的模型难以直接泛化或迁移到目标领域。
- en: Shared features existing in these two domains due to the intrinsic similarity
    in different application scenarios or different working conditions allow this
    domain shift manageable. Hence, to let DL models trained in the source domain
    be able to be transferred well to the target domain, a new paradigm, called deep
    transfer learning (DTL) should be introduced into IFD. One of the effective and
    direct DTL is to fine-tune DL models with a few labeled data in the target domain,
    and then the fine-tuned model can be used to diagnose the test samples. However,
    the newly collected data or the data under different working conditions are usually
    unlabeled and it is sometimes very difficult, or even impossible to label these
    data. Therefore, in this paper, we investigate the unsupervised version of DTL,
    called unsupervised deep transfer learning-based (UDTL-based) IFD, which is to
    make predictions for unlabeled data on a target domain given labeled data on a
    source domain. It is worth mentioning that UDTL is sometimes called unsupervised
    domain adaptation, and in this paper, we do not make a strict distinction between
    two concepts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在不同应用场景或不同工作条件下的内在相似性，这两个领域之间存在的共享特性使得领域迁移变得可管理。因此，为了让在源领域训练的深度学习（DL）模型能够很好地迁移到目标领域，应该在图像故障检测（IFD）中引入一种新的范式，称为深度迁移学习（DTL）。一种有效且直接的DTL方法是使用目标领域中少量标记数据对DL模型进行微调，然后可以使用微调后的模型来诊断测试样本。然而，新收集的数据或在不同工作条件下的数据通常是未标记的，有时标记这些数据非常困难，甚至不可能。因此，本文探讨了DTL的无监督版本，即基于无监督深度迁移学习（UDTL-based）的IFD，其目的是在给定源领域标记数据的情况下对目标领域的未标记数据进行预测。值得一提的是，UDTL有时被称为无监督领域适应，本文不对这两个概念做严格区分。
- en: 'UDTL is widely used and has achieved tremendous success in the field of computer
    vision and natural language processing, due to the application value, open source
    codes, and the baseline accuracy. However, there are few open source codes or
    the baseline accuracy in the field of UDTL-based IFD, plenty of research has been
    published for UDTL-based IFD via simply using models that already have been published
    in other fields. Due to the lack of open source codes, results in these papers
    are very hard to repeat for further comparisons. This is not beneficial to identify
    the state-of-the-art methods, and furthermore, it is unfavorable to the advancement
    of this field on a long view. Hence, it is very important to perform a comparative
    study, provide a baseline accuracy, and release open source codes of UDTL-based
    algorithms. For testing UDTL-based algorithms, the unified test framework, parameter
    settings, and datasets are three important aspects to affect fairness and effectiveness
    of comparisons. While, due to the inconsistency of these factors, there are a
    lot of unfair and unsuitable comparisons. It seems that scholars are continuing
    to combine new techniques, and the proposed algorithms always have better performance
    than other former algorithms, which comes to the question: Is the improvement
    beneficial to IFD or just depends on the excessive parameter adjustment? However,
    the open and essential issues in UDTL-based IFD are rarely studied, such as transferability
    of the features, influence of backbones, etc.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: UDTL在计算机视觉和自然语言处理领域得到了广泛应用，并取得了巨大的成功，这得益于其应用价值、开源代码和基准准确率。然而，在UDTL-based IFD领域，开源代码和基准准确率较少，很多研究通过简单使用在其他领域已发布的模型来进行UDTL-based
    IFD。由于缺乏开源代码，这些论文中的结果很难重复进行进一步比较。这不利于识别最先进的方法，进一步来说，也不利于该领域的长期发展。因此，进行比较研究、提供基准准确率和发布UDTL-based算法的开源代码非常重要。在测试UDTL-based算法时，统一的测试框架、参数设置和数据集是影响比较公平性和有效性的三个重要方面。然而，由于这些因素的不一致，存在很多不公平和不合适的比较。似乎学者们继续结合新技术，提出的算法总是比以前的算法表现更好，这就引出了一个问题：这种改进对IFD有益还是仅仅依赖于过度的参数调整？然而，UDTL-based
    IFD中开放和关键的问题，如特征的可迁移性、骨干网的影响等，鲜有研究。
- en: There are already some good review papers about transfer learning in IFD. Zheng
    et al. [[18](#bib.bib18)] summarized the cross-domain fault diagnosis using the
    knowledge transfer strategy based on transfer learning and presented some open
    source datasets, which could be used to verify the performance of diagnosis methods.
    Yan et al. [[19](#bib.bib19)] reviewed recent development of knowledge transfer
    for rotary machine fault diagnosis via using different transfer learning methods
    and provided four case studies to compare the performance of different methods.
    Lei et al. [[20](#bib.bib20)] reviewed IFD based on machine learning methods with
    the emphasis on transfer learning theories, which adopt diagnosis knowledge from
    one or multiple datasets to other related ones, and also pointed out that transfer
    learning theories might be the essential way to narrow the gap between experimental
    verification and real applications. However, all above review papers did not focus
    on UDTL-based IFD and provide the open source test framework for fair and suitable
    comparisons. They all payed more attention to label-consistent (also called closed
    set) UDTL-based IFD, which assumes that the source domain has the same label space
    with the target domain, but many recent research papers focused on label-inconsistent
    or multi-domain UDTL, which is closer to the engineering scenarios. Thus, a comprehensive
    review is still required to cover the advanced development of UDTL-based IFD from
    the cradle to the bloom and to guide the future development.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一些关于IFD中迁移学习的优秀综述论文。郑等人[[18](#bib.bib18)]总结了基于迁移学习的知识迁移策略在跨域故障诊断中的应用，并提供了一些开源数据集，可以用于验证诊断方法的性能。严等人[[19](#bib.bib19)]回顾了通过使用不同的迁移学习方法进行旋转机械故障诊断的最新发展，并提供了四个案例研究来比较不同方法的性能。雷等人[[20](#bib.bib20)]回顾了基于机器学习方法的IFD，重点讨论了迁移学习理论，该理论将一个或多个数据集的诊断知识迁移到其他相关数据集中，并指出迁移学习理论可能是缩小实验验证与实际应用之间差距的关键方法。然而，上述综述论文均未集中于UDTL-based
    IFD，也未提供用于公平和适当比较的开源测试框架。它们都更关注标签一致（也称为封闭集）UDTL-based IFD，假设源域与目标域具有相同的标签空间，但许多最近的研究论文关注标签不一致或多域UDTL，更接近工程场景。因此，仍需进行全面的综述，以涵盖UDTL-based
    IFD从起步到成熟的先进发展，并指导未来的发展。
- en: In this paper, to fill in this gap, commonly used UDTL-based settings and algorithms
    are discussed and a new taxonomy of UDTL-based IFD is constructed. In each separate
    category, we also give a comprehensive review about recent development of UDTL-based
    IFD. Some typical methods are integrated into a unified test framework, which
    is tested on five datasets. This test framework with source codes will be released
    to the research community to facilitate the research on UDTL-based IFD. With this
    comparative study and open source codes, the authors try to give a depth discussion
    (it is worth mentioning that results are just a lower bound of the accuracy) of
    current algorithms and attempt to find the core that determines the transfer performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文为填补这一空白，讨论了常用的UDTL-based设置和算法，并构建了UDTL-based IFD的新分类法。在每个单独的类别中，我们还对UDTL-based
    IFD的最新发展进行了全面的综述。一些典型的方法被整合到一个统一的测试框架中，该框架在五个数据集上进行了测试。这个测试框架及其源代码将发布给研究社区，以促进对UDTL-based
    IFD的研究。通过这项比较研究和开源代码，作者试图深入探讨当前算法（值得一提的是，结果仅为准确性的下界），并尝试找出决定迁移性能的核心因素。
- en: 'The main contributions of this paper are summarized as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下：
- en: 1)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1)
- en: 'New taxonomy and review: we establish a new taxonomy of UDTL-based IFD according
    to different tasks of UDTL. The hierarchical order follows the number of source
    domains, the usage of target data in the training phase, the label consistence
    of source and target domains, inclusion relationship between label sets of source
    and target domains, and a transfer methodological level. We also provide the most
    comprehensive overview of UDTL-based IFD for each type of categories.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新的分类法和综述：我们根据UDTL的不同任务建立了基于UDTL的IFD的新分类法。层级顺序遵循源域的数量、训练阶段目标数据的使用、源域和目标域标签的一致性、源域和目标域标签集之间的包含关系，以及转移方法论水平。我们还提供了每种类别的UDTL-based
    IFD的最全面的概述。
- en: 2)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2)
- en: 'Various datasets and data splitting: We collect most of the publicly available
    datasets suitable for UDTL-based IFD and provide a detailed discussion about its
    adaptability. We also discuss the way of data splitting and explain that it is
    more appropriate to split data into training and test datasets regardless of whether
    they are in source or target domains.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 各种数据集和数据划分：我们收集了大多数适用于基于UDTL的IFD的公开数据集，并详细讨论了其适应性。我们还讨论了数据划分的方式，并解释了无论数据处于源领域还是目标领域，将数据划分为训练集和测试集更为合适。
- en: 3)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3)
- en: 'Comparative study and further discussion: We evaluate various UDTL-based IFD
    methods and provide a systematic and comparative analysis from several perspectives
    to make the future studies more comparable and meaningful. We also discuss the
    transferability of features, influence of backbones, negative transfer, etc.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比较研究与进一步讨论：我们评估了各种基于UDTL的IFD方法，并从多个角度提供系统化的比较分析，以使未来的研究更加可比和有意义。我们还讨论了特征的可迁移性、骨干网络的影响、负迁移等。
- en: 4)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4)
- en: 'Open source codes: To emphasize the importance and reproducibility of UDTL-based
    IFD, we release the whole evaluation code framework that implements all UDTL-based
    methods discussed in this paper. Meanwhile, this is an extensible framework that
    retains an extended interface for everyone to combine different algorithms and
    load their own datasets to carry out new studies. The code framework is available
    at [https://github.com/ZhaoZhibin/UDTL](https://github.com/ZhaoZhibin/UDTL).'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开源代码：为了强调UDTL-based IFD的重要性和可重复性，我们发布了实现本文讨论的所有UDTL-based方法的完整评价代码框架。同时，这是一个可扩展的框架，保留了一个扩展接口，供大家结合不同的算法并加载自己的数据集以进行新的研究。代码框架可在[https://github.com/ZhaoZhibin/UDTL](https://github.com/ZhaoZhibin/UDTL)获得。
- en: 'The rest of this paper is organized as follows: Section [II](#S2 "II Background
    and Definition ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent
    Fault Diagnosis: A Survey and Comparative Study") provides background and definition
    of UDTL-based IFD. Basic concepts, evaluation algorithms and comprehensive review
    of UDTL-based IFD are introduced in Section [III](#S3 "III Label-consistent UDTL
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study") to [V](#S5 "V Multi-domain UDTL ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"). After that, in Section [VI](#S6 "VI Datasets ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study") to [VIII](#S8 "VIII Further discussions ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"), datasets, evaluation results and further discussions
    are investigated, followed by the conclusion part in Section [IX](#S9 "IX Conclusion
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '本文其余部分组织如下：第[II](#S2 "II Background and Definition ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")节提供了基于UDTL的IFD的背景和定义。基本概念、评价算法以及对基于UDTL的IFD的综合评述在第[III](#S3 "III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study")节至[V](#S5 "V Multi-domain UDTL ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")节中介绍。之后，在第[VI](#S6 "VI Datasets ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")节至第[VIII](#S8 "VIII Further discussions ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")节中，讨论了数据集、评价结果和进一步讨论，最后在第[IX](#S9 "IX Conclusion ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")节给出了结论部分。'
- en: II Background and Definition
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景与定义
- en: II-A The Definition of UDTL
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A UDTL的定义
- en: 'To briefly describe the definition of UDTL, we introduce some basic symbols.
    It is assumed that labels in the source domain are all available, and the source
    domain can be defined as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简要描述UDTL的定义，我们介绍了一些基本符号。假设源领域中的标签都是可用的，源领域可以定义如下：
- en: '|  | $\displaystyle\mathcal{D}_{s}=\left\{{\left({x_{i}^{s},y_{i}^{s}}\right)}\right\}_{i=1}^{{n_{s}}}\quad
    x_{i}^{s}\in{X_{s}},\;y_{i}^{s}\in{Y_{s}},$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{D}_{s}=\left\{{\left({x_{i}^{s},y_{i}^{s}}\right)}\right\}_{i=1}^{{n_{s}}}\quad
    x_{i}^{s}\in{X_{s}},\;y_{i}^{s}\in{Y_{s}},$ |  | (1) |'
- en: 'where $\mathcal{D}_{s}$ represents the source domain, $x_{i}^{s}\in\mathbb{R}^{d}$
    is the $i$-th sample, $X_{s}$ is the union of all samples, $y_{i}^{s}$ is the
    $i$-th label of the $i$-th sample, $Y_{s}$ is the union of all different labels,
    and $n_{s}$ means the total number of source samples. Besides, it is assumed that
    labels in the target domain are unavailable, and thus the target domain can be
    defined as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}_{s}$ 代表源领域，$x_{i}^{s}\in\mathbb{R}^{d}$ 是第 $i$ 个样本，$X_{s}$ 是所有样本的并集，$y_{i}^{s}$
    是第 $i$ 个样本的标签，$Y_{s}$ 是所有不同标签的并集，$n_{s}$ 表示源样本的总数。此外，假设目标领域中的标签不可用，因此目标领域可以定义如下：
- en: '|  | $\displaystyle\mathcal{D}_{t}=\left\{{\left({x_{i}^{t}}\right)}\right\}_{i=1}^{{n_{t}}}\quad
    x_{i}^{t}\in{X_{t}},$ |  | (2) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{D}_{t}=\left\{{\left({x_{i}^{t}}\right)}\right\}_{i=1}^{{n_{t}}}\quad
    x_{i}^{t}\in{X_{t}},$ |  | (2) |'
- en: where $\mathcal{D}_{t}$ represents the target domain, $x_{i}^{t}\in\mathbb{R}^{d}$
    is the $i$-th sample, $X_{t}$ is the union of all samples, and $n_{t}$ means the
    total number of target samples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}_{t}$ 代表目标领域，$x_{i}^{t}\in\mathbb{R}^{d}$ 是第 $i$ 个样本，$X_{t}$
    是所有样本的并集，$n_{t}$ 表示目标样本的总数。
- en: 'The source and target domains follow the probability distributions $P$ and
    $Q$, respectively. We hope to build a model $\beta(\cdot)$ which can classify
    unlabeled samples $x$ in the target domain:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 源领域和目标领域分别遵循概率分布 $P$ 和 $Q$。我们希望建立一个模型 $\beta(\cdot)$，该模型可以对目标领域中的未标记样本 $x$ 进行分类：
- en: '|  | $\displaystyle\hat{y}=\beta\left(x\right),$ |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{y}=\beta\left(x\right),$ |  | (3) |'
- en: 'where $\hat{y}$ is the prediction. Thus, UDTL is aimed to minimize the target
    risk ${\varepsilon}_{t}\left(\beta\right)$ using source data supervision [[21](#bib.bib21)]:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{y}$ 是预测值。因此，UDTL 旨在通过源数据监督来最小化目标风险 ${\varepsilon}_{t}\left(\beta\right)$
    [[21](#bib.bib21)]：
- en: '|  | $\displaystyle{\varepsilon}_{t}\left(\beta\right)=\Pr_{\left({x,y}\right)\sim
    Q}\left[{\beta\left(x\right)\neq y}\right].$ |  | (4) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\varepsilon}_{t}\left(\beta\right)=\Pr_{\left({x,y}\right)\sim
    Q}\left[{\beta\left(x\right)\neq y}\right].$ |  | (4) |'
- en: 'Also, the total loss of UDTL can be written as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，UDTL的总损失可以写作：
- en: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda\mathcal{L}_{\text{UDTL}},$
    |  | (5) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda\mathcal{L}_{\text{UDTL}},$
    |  | (5) |'
- en: 'where $\mathcal{L}_{c}$ is the Softmax cross-entropy loss shown in ([6](#S2.E6
    "In II-A The Definition of UDTL ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")), $\lambda$ is the trade-off parameter, and $\mathcal{L}_{\text{UDTL}}$
    represents the partial loss to reduce the feature difference between source and
    target domains.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{c}$ 是在 ([6](#S2.E6 "在II-A UDTL的定义 ‣ II 背景与定义 ‣ 无监督深度迁移学习在智能故障诊断中的应用：一项调查与比较研究"))
    中展示的Softmax交叉熵损失，$\lambda$ 是权衡参数，$\mathcal{L}_{\text{UDTL}}$ 代表减少源领域和目标领域之间特征差异的部分损失。
- en: '|  | $\displaystyle\mathcal{L}_{c}=-\mathbb{E}_{\left(x_{i}^{s},y_{i}^{s}\right)\in\mathcal{D}_{s}}\sum_{c=0}^{C-1}\mathbf{1}_{[y_{i}^{s}=c]}\log\left[\beta\left(x_{i}^{s}\right)\right],$
    |  | (6) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{c}=-\mathbb{E}_{\left(x_{i}^{s},y_{i}^{s}\right)\in\mathcal{D}_{s}}\sum_{c=0}^{C-1}\mathbf{1}_{[y_{i}^{s}=c]}\log\left[\beta\left(x_{i}^{s}\right)\right],$
    |  | (6) |'
- en: where $C$ is the number of all possible classes, $\mathbb{E}$ denotes the mathematical
    expectation, and $\mathbf{1}$ is the indicator function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C$ 是所有可能类别的数量，$\mathbb{E}$ 表示数学期望，$\mathbf{1}$ 是指示函数。
- en: II-B Taxonomy of UDTL-based IFD
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 基于UDTL的IFD分类
- en: 'In this section, we present our taxonomy of UDTL-based IFD, as shown in Fig. [1](#S2.F1
    "Figure 1 ‣ II-B Taxonomy of UDTL-based IFD ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"). We categorize UDTL-based IFD into single-domain and multi-domain
    UDTL according to the number of source domains from a macro perspective. In the
    following, we give a brief introduction of each category and detailed description
    is given in the next part.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了基于UDTL的IFD分类，如图[1](#S2.F1 "图1 ‣ II-B 基于UDTL的IFD分类 ‣ II 背景与定义 ‣ 无监督深度迁移学习在智能故障诊断中的应用：一项调查与比较研究")所示。我们从宏观角度根据源领域的数量将基于UDTL的IFD分为单领域和多领域UDTL。在下面，我们简要介绍每个类别，详细描述将在下一部分给出。
- en: 'Figure 1: A taxonomy of UDTL-based methods.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于UDTL的方法分类。
- en: '1) Single-domain UDTL: These can be further categorized into label-consistent
    (closed set) and label-inconsistent UDTL. As shown in Fig. [2](#S2.F2 "Figure
    2 ‣ II-B Taxonomy of UDTL-based IFD ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"), label-consistent UDTL represents the label sets of source
    and target domains are consistent. According to Tan et al. [[22](#bib.bib22)],
    label-consistent UDTL can be classified into four categories: network-based, instanced-based,
    mapping-based, and adversarial-based methods from a methodological level. Additionally,
    We categorize label-inconsistent UDTL into partial, open set, and universal tasks
    based on the inclusion relationship between label sets. As shown in Fig. [2](#S2.F2
    "Figure 2 ‣ II-B Taxonomy of UDTL-based IFD ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"), partial UDTL means that the target label set is a subspace
    of the source label set; open set UDTL means that the target label set contains
    unknown labels; universal UDTL is a combination of the first two conditions. It
    is worth mentioning that three tasks can be further divided into the above four
    methods from a methodological level.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '1) 单领域 UDTL：这些可以进一步分为标签一致（封闭集）和标签不一致的 UDTL。如图 [2](#S2.F2 "Figure 2 ‣ II-B Taxonomy
    of UDTL-based IFD ‣ II Background and Definition ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")所示，标签一致的 UDTL 表示源领域和目标领域的标签集是一致的。根据 Tan 等人 [[22](#bib.bib22)] 的研究，标签一致的
    UDTL 可以从方法论层面分为四类：基于网络的方法、基于实例的方法、基于映射的方法和基于对抗的方法。此外，我们将标签不一致的 UDTL 按照标签集之间的包含关系分为部分、开放集和通用任务。如图 [2](#S2.F2
    "Figure 2 ‣ II-B Taxonomy of UDTL-based IFD ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")所示，部分 UDTL 意味着目标标签集是源标签集的一个子空间；开放集 UDTL 意味着目标标签集包含未知标签；通用
    UDTL 是前两种情况的结合。值得一提的是，这三种任务从方法论层面上可以进一步细分为上述四种方法。'
- en: '2) Multi-domain UDTL: These can be further categorized into multi-domain adaptation
    and domain generalization (DG) based on the usage of the target data in the training
    phase. Multi-domain adaptation means that the unlabeled samples from the target
    domain participate into the training phase, and DG is the opposite. Besides, these
    two conditions can also be further categorized into label-consistent and label-inconsistent
    UDTL.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 多领域 UDTL：这些可以进一步分为多领域适应和领域泛化（DG），具体取决于目标数据在训练阶段的使用情况。多领域适应意味着来自目标领域的未标记样本参与训练阶段，而
    DG 则相反。此外，这两种情况还可以进一步细分为标签一致和标签不一致的 UDTL。
- en: '![Refer to caption](img/c7337e1f567b9b415ebf94e97d2ba7cd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c7337e1f567b9b415ebf94e97d2ba7cd.png)'
- en: 'Figure 2: Visualization explanation of different transfer settings. Additionally,
    different colors represent different domains and dotted lines denote that this
    domain does not participate in training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同迁移设置的可视化说明。此外，不同的颜色表示不同的领域，虚线表示该领域未参与训练。
- en: II-C Motivation of UDTL-based IFD
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C UDTL 基于 IFD 的动机
- en: 'Distributions of training and test samples are often different, due to the
    influence of working conditions, fault sizes, fault types, etc. Consequently,
    UDTL-based IFD has been introduced recently to tackle this domain shift problem
    since there are some shared features in the specific space. Using these shared
    features, applications of UDTL-based IFD can be mainly classified into four categories:
    different working conditions, different types of faults, different locations,
    and different machines.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作条件、故障大小、故障类型等因素的影响，训练样本和测试样本的分布通常不同。因此，最近引入了基于 UDTL 的 IFD 来解决这个领域迁移问题，因为在特定空间中存在一些共享特征。利用这些共享特征，基于
    UDTL 的 IFD 的应用主要可以分为四类：不同的工作条件、不同类型的故障、不同的位置和不同的机器。
- en: '1) Different working conditions: Due to the influence of speed, load, temperature,
    etc., working conditions often vary during the monitoring period. Collected signals
    may contain domain shift, which means that the distribution of data may differ
    significantly under different working conditions [[23](#bib.bib23)]. The aim of
    UDTL-based IFD is that the model trained using signals under one working condition
    can be transferred to signals under another different working condition.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不同工作条件：由于速度、负载、温度等因素的影响，工作条件在监测期间往往会有所变化。收集到的信号可能包含领域转移，这意味着在不同工作条件下数据的分布可能显著不同[[23](#bib.bib23)]。基于UDTL的IFD的目标是使使用一种工作条件下的信号训练的模型能够转移到另一种不同工作条件下的信号。
- en: '2) Different types of faults: Label difference between source and target domains
    may exist since different types of faults would happen on the same component.
    Therefore, there are three cases in UDTL-based IFD. The first one is that unknown
    fault types appear in the target domain (open set transfer). The second one is
    that partial fault types of the source domain appear in the target domain (partial
    transfer). The third one is that the first two cases occur at the same time (universal
    transfer). The aim of UDTL-based IFD is that the model trained with some types
    of faults can be transferred to the target domain with different types of faults.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的故障：由于不同类型的故障可能发生在同一组件上，源领域和目标领域之间可能存在标签差异。因此，基于UDTL的IFD有三种情况。第一种情况是目标领域出现未知故障类型（开放集转移）。第二种情况是目标领域出现源领域的部分故障类型（部分转移）。第三种情况是前两种情况同时发生（通用转移）。基于UDTL的IFD的目标是使使用某些类型故障训练的模型能够转移到具有不同类型故障的目标领域。
- en: '3) Different locations: Because sensors installed on the same machine are often
    responsible for monitoring different components, and sensors located near the
    fault component are more suitable to indicate the fault information. However,
    key components have different probabilities of failure rates, leading to the situation
    where signals from different locations have different numbers of labeled data.
    The aim of UDTL-based IFD is that the model trained with plenty of labeled data
    from one location can be transferred to the target domain with unlabeled data
    from other locations.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不同位置：由于安装在同一台机器上的传感器通常负责监测不同的组件，并且靠近故障组件的传感器更适合指示故障信息。然而，关键组件的故障率概率不同，导致不同位置的信号具有不同数量的标注数据。基于UDTL的IFD的目标是使使用大量标注数据训练的模型能够转移到具有来自其他位置的未标注数据的目标领域。
- en: '4) Different machines: Enough labeled fault samples of real machines are difficult
    to collect due to the test cost and security. Besides, enough labeled data can
    be generated from dynamic simulations or fault seeding experiments. However, distributions
    of data from dynamic simulations or fault seeding experiments are different but
    similar to those from real machines, due to the similar structure and measurement
    situations. Thus, the aim of UDTL-based IFD is that the model can be transferred
    to test data gathered from real machines.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不同机器：由于测试成本和安全问题，收集足够的标注故障样本非常困难。此外，可以通过动态模拟或故障种植实验生成足够的标注数据。然而，由于结构和测量情况相似，动态模拟或故障种植实验的数据分布与真实机器的数据分布不同但相似。因此，基于UDTL的IFD的目标是使模型能够转移到从真实机器收集的测试数据上。
- en: II-D The structure of backbone
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 骨干结构
- en: One of the most important parts of UDTL-based IFD is the structure of the backbone,
    which acts as feature extraction and has a huge impact on the test accuracy. For
    example, in the field of image classification, different backbones, such as VGG
    [[24](#bib.bib24)], ResNet [[25](#bib.bib25)], etc., have different abilities
    of feature extraction, leading to different classification performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于UDTL的IFD最重要的部分之一是骨干结构，它作为特征提取具有巨大的测试准确度影响。例如，在图像分类领域，不同的骨干网络，如VGG [[24](#bib.bib24)]、ResNet
    [[25](#bib.bib25)]等，具有不同的特征提取能力，从而导致不同的分类性能。
- en: However, for UDTL-based IFD, different studies have their own backbones, and
    it is difficult to determine whose backbone is better. Therefore, direct comparisons
    with the results listed in other published papers are unfair and unsuitable due
    to different representative capacities of backbones. In this paper, we try to
    verify the performance of different UDTL-based IFD methods using the same CNN
    backbone to ensure a fair comparison.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于基于UDTL的IFD，不同的研究有各自的骨干网，因此很难确定哪个骨干网更好。因此，由于骨干网的代表性能力不同，与其他已发布论文中的结果进行直接比较是不公平和不合适的。在本文中，我们尝试使用相同的CNN骨干网来验证不同基于UDTL的IFD方法的性能，以确保公平比较。
- en: 'As shown in Fig. [3](#S2.F3 "Figure 3 ‣ II-D The structure of backbone ‣ II
    Background and Definition ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"), the CNN backbone
    consists of four one dimension (1D) convolutional layers that come with an 1D
    Batch Normalization (BN) layer and a ReLU activation function. Besides, the second
    combination comes with an 1D Max Pooling layer, and the fourth combination comes
    with an 1D Adaptive Max Pooling layer to realize the adaptation of the input length.
    The convolutional output is then flattened and passed through a fully-connected
    (Fc) layer, a ReLU activation function, and a Dropout layer. The detailed parameters
    are listed in Table [I](#S2.T1 "TABLE I ‣ II-D The structure of backbone ‣ II
    Background and Definition ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[3](#S2.F3 "Figure 3 ‣ II-D The structure of backbone ‣ II Background and
    Definition ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent
    Fault Diagnosis: A Survey and Comparative Study")所示，CNN骨干网由四个一维（1D）卷积层组成，这些层配备了一个1D批量归一化（BN）层和一个ReLU激活函数。此外，第二组合配备了一个1D最大池化层，第四组合配备了一个1D自适应最大池化层，以实现输入长度的自适应。卷积输出随后被展平，通过一个全连接（Fc）层，一个ReLU激活函数和一个Dropout层。详细参数列在表[I](#S2.T1
    "TABLE I ‣ II-D The structure of backbone ‣ II Background and Definition ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")中。'
- en: '![Refer to caption](img/4407fab246f96d407ce5fda3413efc13.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4407fab246f96d407ce5fda3413efc13.png)'
- en: 'Figure 3: The structure of the backbone.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：骨干网的结构。
- en: 'TABLE I: Parameters of the backbone.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：骨干网的参数。
- en: '| Layers | Parameters |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 参数 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Conv1 | out_channels=16, kernel_size=15 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Conv1 | out_channels=16, kernel_size=15 |'
- en: '| Conv2 | out_channels=32, kernel_size=3 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Conv2 | out_channels=32, kernel_size=3 |'
- en: '| Max Pooling | kernel_size=2, stride=2 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 最大池化 | kernel_size=2, stride=2 |'
- en: '| Conv3 | out_channels=64, kernel_size=3 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Conv3 | out_channels=64, kernel_size=3 |'
- en: '| Conv4 | out_channels=128, kernel_size=3 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Conv4 | out_channels=128, kernel_size=3 |'
- en: '| Adaptive Max Pooling | output_size=4 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 自适应最大池化 | output_size=4 |'
- en: '| Fc | out_features=256 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Fc | out_features=256 |'
- en: '| Dropout | p=0.5 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Dropout | p=0.5 |'
- en: III Label-consistent UDTL
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 标签一致的UDTL
- en: Label-consistent (also called closed set) UDTL-based IFD assumes that the source
    domain has the same label space with the target domain. In this section, we categorize
    label-consistent UDTL into network-based, instanced-based, mapping-based, and
    adversarial-based methods from a methodological level.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 标签一致（也称为闭集）基于UDTL的IFD假设源领域与目标领域具有相同的标签空间。在本节中，我们从方法论的层面将标签一致的UDTL分类为网络基础、实例基础、映射基础和对抗基础方法。
- en: III-A Network-based UDTL
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 网络基础的UDTL
- en: III-A1 Basic concepts
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 基本概念
- en: 'Network-based DTL means that partial network parameters pre-trained in the
    source domain are transferred directly to be partial network parameters of the
    test procedure or network parameters are fine-tuned with a few labeled data in
    the target domain. The most popular network-based DTL method is to fine-tune the
    trained model utilizing a few labeled data in the target domain. However, for
    UDTL-based IFD, labels in the target domain are unavailable. We use the backbone
    coming with a bottleneck layer, consisting of a Fc layer (out_features=256), a
    ReLU activation function, a Dropout layer ($p=0.5$), and a basic Softmax classifier
    to construct our basic model (we call it Basis), which is shown in Fig. [4](#S3.F4
    "Figure 4 ‣ III-A1 Basic concepts ‣ III-A Network-based UDTL ‣ III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study"). The trained model is used to test
    samples in the target domain directly, which means that source and target domains
    share the same model and parameters.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '基于网络的DTL（深度迁移学习）指的是将源领域中预训练的部分网络参数直接转移到测试过程的部分网络参数中，或者使用少量标记数据在目标领域微调网络参数。最流行的基于网络的DTL方法是利用目标领域中少量标记数据对训练模型进行微调。然而，对于基于UDTL的IFD（智能故障诊断），目标领域中的标签是不可用的。我们使用带有瓶颈层的主干网络，包含一个Fc层（out_features=256）、一个ReLU激活函数、一个Dropout层（$p=0.5$）和一个基本的Softmax分类器来构建我们的基本模型（我们称之为Basis），如图[4](#S3.F4
    "Figure 4 ‣ III-A1 Basic concepts ‣ III-A Network-based UDTL ‣ III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study")所示。训练好的模型直接用于测试目标领域中的样本，这意味着源领域和目标领域共享相同的模型和参数。'
- en: '![Refer to caption](img/7e83fc5e2c328e7672c3ee84e5d39196.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7e83fc5e2c328e7672c3ee84e5d39196.png)'
- en: 'Figure 4: The structure of the basic model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基本模型的结构。
- en: III-A2 Applications to IFD
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 应用于IFD
- en: Pre-trained deep neural networks using the source data were used in [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)] via frozing their partial parameters, and then part of network
    parameters were transferred to the target network and other parameters were fine-tuned
    with a small amount of target data. Pre-trained deep neural networks on ImageNet
    were used in [[38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)] and were fine-tuned with limited target data to adapt the domain
    of engineering applications. Ensemble techniques and multi-channel signals were
    used in [[43](#bib.bib43), [44](#bib.bib44)] to initialize the target network
    which was fine-tuned by a few training samples from the target domain. Two-dimensional
    images, such as grey images [[45](#bib.bib45)], time-frequency images [[46](#bib.bib46)],
    and thermal images [[47](#bib.bib47)], were used to pre-train the specific-designed
    networks, which were transferred to the target tasks via fine-tuning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用源数据进行预训练的深度神经网络被用于[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]，通过冻结其部分参数，然后将部分网络参数转移到目标网络，其他参数则使用少量目标数据进行微调。在[[38](#bib.bib38),
    [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)]上使用了在ImageNet上预训练的深度神经网络，并使用有限的目标数据对其进行微调，以适应工程应用领域。集成技术和多通道信号被用于[[43](#bib.bib43),
    [44](#bib.bib44)]来初始化目标网络，然后通过来自目标领域的少量训练样本对其进行微调。二维图像，如灰度图像[[45](#bib.bib45)]、时频图像[[46](#bib.bib46)]和热图像[[47](#bib.bib47)]，被用来预训练特定设计的网络，这些网络通过微调被转移到目标任务中。
- en: 'Qureshi et al. [[48](#bib.bib48)] pre-trained nine deep sparse auto-encoders
    on one wind farm, and predictions on another wind farm were taken by fine-tuning
    the pre-trained networks. Zhong et al. [[49](#bib.bib49)] trained a CNN on enough
    normal samples and then replaced Fc layers with support vector machine as the
    target model. Han et al. [[50](#bib.bib50)] discussed and compared three fine-tuning
    strategies: only fine-tuning the classifier, fine-tuning the feature descriptor,
    and fine-tuning both the feature descriptor and the classifier for diagnosing
    unseen machine conditions. Xu et al. [[51](#bib.bib51)] pre-trained the offline
    CNN on the source domain and directly transferred them to the shallow layers of
    the online CNN via fine-tuning the online CNN on the target domain for online
    IFD. Zhao et al. [[52](#bib.bib52)] proposed a multi-scale convolutional transfer
    learning network pre-trained on the source domain, and then the model was transferred
    to the other different but similar domains with proper fine-tuning.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Qureshi等人[[48](#bib.bib48)] 在一个风电场上预训练了九个深度稀疏自编码器，并通过微调预训练的网络对另一个风电场进行预测。Zhong等人[[49](#bib.bib49)]
    在足够的正常样本上训练了一个CNN，然后用支持向量机替换Fc层作为目标模型。Han等人[[50](#bib.bib50)] 讨论并比较了三种微调策略：仅微调分类器、微调特征描述符，以及同时微调特征描述符和分类器以诊断未见过的机器条件。Xu等人[[51](#bib.bib51)]
    在源领域上对离线CNN进行预训练，并通过微调目标领域的在线CNN直接转移到在线CNN的浅层，以进行在线IFD。Zhao等人[[52](#bib.bib52)]
    提出了一个在源领域上预训练的多尺度卷积转移学习网络，然后通过适当的微调将模型转移到其他不同但类似的领域。
- en: III-B Instanced-based UDTL
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于实例的UDTL
- en: III-B1 Basic concepts
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 基本概念
- en: Instanced-based UDTL refers to re-weight instances in the source domain to assist
    the classifier to predict labels or use statistics of instances to help align
    the target domain, such as TrAdaBoost [[53](#bib.bib53)] and adaptive Batch Normalization
    (AdaBN) [[54](#bib.bib54)]. In this paper, we use AdaBN to represent one of instanced-based
    UDTL methods, which does not require labels from the target domain.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基于实例的UDTL指的是在源领域重新加权实例以帮助分类器预测标签，或使用实例统计信息来帮助对齐目标领域，例如TrAdaBoost[[53](#bib.bib53)]
    和自适应批归一化（AdaBN）[[54](#bib.bib54)]。在本文中，我们使用AdaBN来表示一种基于实例的UDTL方法，它不需要来自目标领域的标签。
- en: BN, which can be used to avoid the issue of the internal covariate shifting,
    is one of the most important techniques. BN can promote much faster training speed
    since it makes the input distribution more stable. Detailed descriptions and properties
    can be referred to [[55](#bib.bib55)]. It is worth mentioning that BN layers are
    only updated in the training procedure and the global statistics of training samples
    are used to normalize test samples during the test procedure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: BN，作为一种避免内部协变量偏移问题的技术，是最重要的技术之一。BN可以显著提高训练速度，因为它使输入分布更加稳定。详细描述和属性可以参考[[55](#bib.bib55)]。值得一提的是，BN层仅在训练过程中更新，测试过程中使用训练样本的全局统计信息来标准化测试样本。
- en: AdaBN, which is a simple and parameter-free technique for the domain shift problem,
    was proposed in [[54](#bib.bib54)] to enhance the generalization ability. The
    main idea of AdaBN is that the global statistics of each BN layer are replaced
    with statistics in the target domain during the test phase. In our AdaBN realization,
    after training, we provide two updating strategies to fine-tune statistics of
    BN layers using target data, including updating via each batch and the whole data.
    In this paper, we update statistics of BN layers via each batch considering the
    memory limit.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBN，是一种简单且无参数的领域迁移问题技术，在[[54](#bib.bib54)]中提出，用于增强泛化能力。AdaBN的主要思想是，在测试阶段，用目标领域的统计数据替代每个BN层的全局统计数据。在我们的AdaBN实现中，训练后，我们提供了两种更新策略以使用目标数据微调BN层的统计数据，包括通过每个批次和整个数据进行更新。在本文中，考虑到内存限制，我们通过每个批次更新BN层的统计数据。
- en: III-B2 Applications to IFD
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 应用于IFD
- en: Xiao et al. [[56](#bib.bib56)] used TrAdaBoost to enhance the diagnostic capability
    of the fault classifier by adjusting the weight factor of each training sample.
    Zhang et al. [[57](#bib.bib57)] and Qian et al. [[58](#bib.bib58)] used AdaBN
    to improve the domain adaptation ability of the model by ensuring that each layer
    receives data from a similar distribution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao等人[[56](#bib.bib56)] 使用TrAdaBoost通过调整每个训练样本的权重因子来增强故障分类器的诊断能力。Zhang等人[[57](#bib.bib57)]
    和Qian等人[[58](#bib.bib58)] 使用AdaBN通过确保每一层接收来自类似分布的数据来提高模型的领域适应能力。
- en: III-C Mapping-based UDTL
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于映射的UDTL
- en: III-C1 Basic concepts
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 基本概念
- en: Mapping-based UDTL refers to map instances from both source and target domains
    to the feature space via a feature extractor. There are many methods belonging
    to mapping-based UDTL, such as Euclidean distance, Minkowski distance, Kullback-Leibler,
    correlation alignment (CORAL) [[59](#bib.bib59)], maximum mean discrepancy (MMD)
    [[60](#bib.bib60), [61](#bib.bib61)], multi kernels MMD (MK-MMD) [[62](#bib.bib62),
    [21](#bib.bib21)], joint distribution adaptation (JDA) [[63](#bib.bib63)], balanced
    distribution adaptation (BDA) [[64](#bib.bib64)], and Joint Maximum Mean Discrepancy
    (JMMD) [[65](#bib.bib65)]. In this paper, we use MK-MMD, JMMD, and CORAL to represent
    mapping-based methods and test their performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于映射的 UDTL 指的是通过特征提取器将源域和目标域的实例映射到特征空间。基于映射的 UDTL 有许多方法，例如欧几里得距离、明可夫斯基距离、Kullback-Leibler、相关对齐
    (CORAL) [[59](#bib.bib59)]、最大均值差异 (MMD) [[60](#bib.bib60), [61](#bib.bib61)]、多核
    MMD (MK-MMD) [[62](#bib.bib62), [21](#bib.bib21)]、联合分布适应 (JDA) [[63](#bib.bib63)]、平衡分布适应
    (BDA) [[64](#bib.bib64)] 和联合最大均值差异 (JMMD) [[65](#bib.bib65)]。在本文中，我们使用 MK-MMD、JMMD
    和 CORAL 来表示基于映射的方法并测试其性能。
- en: 'MK-MMD: To introduce the definition of MK-MMD, we briefly explain the concept
    of MMD. MMD was first proposed in [[60](#bib.bib60)] and was used in transfer
    learning by many other scholars [[66](#bib.bib66), [67](#bib.bib67)]. MMD defined
    in Reproducing Kernel Hilbert Space (RKHS) is a squared distance between the kernel
    embedding of marginal distributions $P(X_{s})$ and $Q(X_{t})$. RKHS is a Hilbert
    space of functions in which point evaluation is a continuous linear functional,
    and some examples can be found in [[68](#bib.bib68)]. The formula of MMD can be
    written as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MK-MMD：为了介绍 MK-MMD 的定义，我们简要解释 MMD 的概念。MMD 首次在 [[60](#bib.bib60)] 中提出，并被许多其他学者在迁移学习中使用
    [[66](#bib.bib66), [67](#bib.bib67)]。在再生核希尔伯特空间 (RKHS) 中定义的 MMD 是边际分布 $P(X_{s})$
    和 $Q(X_{t})$ 的核嵌入之间的平方距离。RKHS 是一个函数的希尔伯特空间，其中点评估是一个连续的线性泛函，一些例子可以在 [[68](#bib.bib68)]
    中找到。MMD 的公式可以写作：
- en: '|  | $\displaystyle\mathcal{L}_{\text{MMD}}\left(P,Q\right)=\left\&#124;\mathbb{E}_{P}\left({\phi\left({x^{s}}\right)}\right)-\mathbb{E}_{Q}\left({\phi\left({x^{t}}\right)}\right)\right\&#124;^{2}_{\mathcal{H}_{k}},$
    |  | (7) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{MMD}}\left(P,Q\right)=\left\&#124;\mathbb{E}_{P}\left({\phi\left({x^{s}}\right)}\right)-\mathbb{E}_{Q}\left({\phi\left({x^{t}}\right)}\right)\right\&#124;^{2}_{\mathcal{H}_{k}},$
    |  | (7) |'
- en: where $\mathcal{H}_{k}$ is RKHS using the kernel $k$ (in general, Gaussian kernel
    is used as the kernel), and $\phi(\cdot)$ is the mapping to RKHS.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{H}_{k}$ 是使用核 $k$ 的 RKHS（通常使用高斯核作为核），$\phi(\cdot)$ 是映射到 RKHS。
- en: Parameter selection of each kernel is crucial to the final performance. To tackle
    this problem, MK-MMD, which could maximize the two-sample test power and minimize
    the Type II error jointly, was proposed by Gretton et al [[62](#bib.bib62)]. For
    MK-MMD, scholars often use the convex combination of $m$ kernels $\left\{k_{u}\right\}$
    to provide effective estimations of the mapping.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 每个核的参数选择对最终性能至关重要。为了解决这个问题，Gretton 等人提出了 MK-MMD，该方法能够最大化双样本检验的效能并共同最小化第二类错误[[62](#bib.bib62)]。对于
    MK-MMD，学者们通常使用 $m$ 个核 $\left\{k_{u}\right\}$ 的凸组合来提供有效的映射估计。
- en: '|  | $\displaystyle K\mathop{=}\limits^{\Delta}\left\{{k=\sum\limits_{u=1}^{m}{{\alpha_{u}}{k_{u}}:\sum\limits_{u=1}^{m}{{\alpha_{u}}=1},\alpha\geq
    0,\forall u}}\right\},$ |  | (8) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K\mathop{=}\limits^{\Delta}\left\{{k=\sum\limits_{u=1}^{m}{{\alpha_{u}}{k_{u}}:\sum\limits_{u=1}^{m}{{\alpha_{u}}=1},\alpha\geq
    0,\forall u}}\right\},$ |  | (8) |'
- en: where $\left\{\alpha_{u}\right\}$ are weighted parameters of different kernels
    (in this paper, all $\alpha_{u}=\frac{1}{m}$).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\left\{\alpha_{u}\right\}$ 是不同核的加权参数（在本文中，所有 $\alpha_{u}=\frac{1}{m}$）。
- en: 'Inspired by deep adaptation networks (DAN) proposed in [[21](#bib.bib21)],
    we design an UDTL-based IFD model by adding MK-MMD into the loss function to realize
    the feature alignment shown in Fig. [5](#S3.F5 "Figure 5 ‣ III-C1 Basic concepts
    ‣ III-C Mapping-based UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"). In addition, the final loss function is defined as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '受 [[21](#bib.bib21)] 中提出的深度适应网络 (DAN) 的启发，我们通过将 MK-MMD 添加到损失函数中来设计基于 UDTL 的
    IFD 模型，以实现图中的特征对齐 [5](#S3.F5 "Figure 5 ‣ III-C1 Basic concepts ‣ III-C Mapping-based
    UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study")。此外，最终的损失函数定义如下：'
- en: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{MK-MMD}}\mathcal{L}_{\text{MK-MMD}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (9) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{MK-MMD}}\mathcal{L}_{\text{MK-MMD}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (9) |'
- en: where $\lambda_{\text{MK-MMD}}$ is a trade-off parameter and $\mathcal{L}_{\text{MK-MMD}}$
    means the multi-kernel version of MMD. Besides, we simply use the Gaussian kernel
    and the number of kernels is equal to five. The bandwidth of each kernel is set
    to be median pairwise distances on training data according to the median heuristic
    [[62](#bib.bib62)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{\text{MK-MMD}}$ 是一个权衡参数，$\mathcal{L}_{\text{MK-MMD}}$ 表示 MMD 的多核版本。此外，我们简单地使用高斯核，核的数量为五个。每个核的带宽设置为训练数据中的中位数成对距离，根据中位数启发式
    [[62](#bib.bib62)]。
- en: '![Refer to caption](img/aefa33b49e4e7497ac86a4c2fd94c347.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/aefa33b49e4e7497ac86a4c2fd94c347.png)'
- en: 'Figure 5: The UDTL-based IFD model based on MK-MMD.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 MK-MMD 的 UDTL 模型。
- en: 'JMMD: MMD and MK-MMD, which are defined to solve the problem $P(X_{s})\neq
    Q(X_{t})$, cannot be used to tackle the domain shift generated by joint distributions
    (e.g. $P(X_{s},Y_{s})\neq Q(X_{t},Y_{t})$). Thus, JMMD, proposed in [[65](#bib.bib65)],
    was designed to measure the distance of empirical joint distributions $P(X_{s},Y_{s})$
    and $Q(X_{t},Y_{t})$. The formula of JMMD is written as follows [[65](#bib.bib65)]:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: JMMD：MMD 和 MK-MMD 被定义用来解决问题 $P(X_{s})\neq Q(X_{t})$，但不能解决由联合分布生成的领域转移（例如 $P(X_{s},Y_{s})\neq
    Q(X_{t},Y_{t})$）。因此，JMMD，提出于 [[65](#bib.bib65)]，旨在衡量经验联合分布 $P(X_{s},Y_{s})$ 和
    $Q(X_{t},Y_{t})$ 之间的距离。JMMD 的公式如下 [[65](#bib.bib65)]：
- en: '|  |  | $\displaystyle\mathcal{L}_{\text{JMMD}}\left(P,Q\right)=$ |  | (10)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{\text{JMMD}}\left(P,Q\right)=$ |  | (10)
    |'
- en: '|  |  | $\displaystyle\left\&#124;\mathbb{E}_{P}\left({\otimes_{l=1}^{&#124;L&#124;}\phi^{l}\left({z_{l}^{s}}\right)}\right)-\mathbb{E}_{Q}\left(\otimes_{l=1}^{&#124;L&#124;}{\phi^{l}\left({z_{l}^{t}}\right)}\right)\right\&#124;^{2}_{\otimes_{l=1}^{&#124;L&#124;}{\mathcal{H}^{l}}},$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left\&#124;\mathbb{E}_{P}\left({\otimes_{l=1}^{&#124;L&#124;}\phi^{l}\left({z_{l}^{s}}\right)}\right)-\mathbb{E}_{Q}\left(\otimes_{l=1}^{&#124;L&#124;}{\phi^{l}\left({z_{l}^{t}}\right)}\right)\right\&#124;^{2}_{\otimes_{l=1}^{&#124;L&#124;}{\mathcal{H}^{l}}},$
    |  |'
- en: where $\otimes_{l=1}^{|L|}\phi^{l}\left({z_{l}}\right)=\phi^{1}\left({z_{1}}\right)\otimes\dots\otimes\phi^{|L|}\left({z_{|L|}}\right)$
    is the feature mapping in the tensor product Hilbert space, $L$ is the set of
    higher network layers, $|L|$ is the number of layers, $z_{l}^{s}$ means the activation
    of the $l-$th layer generated by the source domain, and $z_{l}^{t}$ means the
    activation of the $l-$th layer generated by the target domain.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\otimes_{l=1}^{|L|}\phi^{l}\left({z_{l}}\right)=\phi^{1}\left({z_{1}}\right)\otimes\dots\otimes\phi^{|L|}\left({z_{|L|}}\right)$
    是张量积希尔伯特空间中的特征映射，$L$ 是高层网络层的集合，$|L|$ 是层数，$z_{l}^{s}$ 表示由源领域生成的第 $l-$ 层激活，$z_{l}^{t}$
    表示由目标领域生成的第 $l-$ 层激活。
- en: 'Inspired by Joint Adaptation Network (JAN) which uses JMMD to align the domain
    shift [[65](#bib.bib65)], we design an UDTL-based IFD method by adding JMMD into
    the loss function to realize feature alignment shown in Fig. [6](#S3.F6 "Figure
    6 ‣ III-C1 Basic concepts ‣ III-C Mapping-based UDTL ‣ III Label-consistent UDTL
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study"). The final loss function is defined as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '受使用 JMMD 对齐领域转移的联合适应网络（JAN） [[65](#bib.bib65)] 的启发，我们通过将 JMMD 添加到损失函数中设计了一种基于
    UDTL 的 IFD 方法，以实现图 [6](#S3.F6 "Figure 6 ‣ III-C1 Basic concepts ‣ III-C Mapping-based
    UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study") 所示的特征对齐。最终的损失函数定义如下：'
- en: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{JMMD}}\mathcal{L}_{\text{JMMD}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (11) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{JMMD}}\mathcal{L}_{\text{JMMD}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (11) |'
- en: where $\lambda_{\text{JMMD}}$ is a trade-off parameter. Additionally, the parameter
    setting of JMMD is the same as that in JAN.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{\text{JMMD}}$ 是一个权衡参数。此外，JMMD 的参数设置与 JAN 相同。
- en: '![Refer to caption](img/dee7f9a868370940c5d6c42177d550cc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dee7f9a868370940c5d6c42177d550cc.png)'
- en: 'Figure 6: The UDTL-based IFD model based on JMMD.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于 JMMD 的 UDTL 模型。
- en: 'CORAL: The CORAL loss, which aims to align the second-order statistics of source
    and target distributions, was first proposed in [[69](#bib.bib69)] and was further
    used in UDTL [[59](#bib.bib59)]. First of all, following [[69](#bib.bib69)] and
    [[59](#bib.bib59)], we give the basic definition of the CORAL loss as:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: CORAL：CORAL 损失旨在对齐源领域和目标领域分布的二阶统计量，首次提出于 [[69](#bib.bib69)]，并在 UDTL [[59](#bib.bib59)]
    中进一步使用。首先，参考 [[69](#bib.bib69)] 和 [[59](#bib.bib59)]，我们给出 CORAL 损失的基本定义如下：
- en: '|  | $\displaystyle\mathcal{L}_{\text{CORAL}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right)=\dfrac{1}{4d^{2}}&#124;&#124;C^{s}-C^{t}&#124;&#124;_{F}^{2},$
    |  | (12) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{CORAL}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right)=\dfrac{1}{4d^{2}}&#124;&#124;C^{s}-C^{t}&#124;&#124;_{F}^{2},$
    |  | (12) |'
- en: 'where $||\cdot||_{F}$ is the Frobenius norm and $d$ is the dimension of each
    sample. $C^{s}$ and $C^{t}$ defined in ([13](#S3.E13 "In III-C1 Basic concepts
    ‣ III-C Mapping-based UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")) are covariance matrices.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $||\cdot||_{F}$ 是 Frobenius 范数，$d$ 是每个样本的维度。定义在 ([13](#S3.E13 "在 III-C1 基本概念
    ‣ III-C 基于映射的 UDTL ‣ III 标签一致的 UDTL ‣ 无监督深度迁移学习在智能故障诊断中的应用：调查与比较研究")) 中的 $C^{s}$
    和 $C^{t}$ 是协方差矩阵。
- en: '|  | $\displaystyle C^{s}$ | $\displaystyle=\dfrac{1}{n_{s}-1}\left(X_{s}^{T}X_{s}-\dfrac{1}{n_{s}}(\textbf{1}^{T}X_{s})^{T}(\textbf{1}^{T}X_{s})\right),$
    |  | (13) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle C^{s}$ | $\displaystyle=\dfrac{1}{n_{s}-1}\left(X_{s}^{T}X_{s}-\dfrac{1}{n_{s}}(\textbf{1}^{T}X_{s})^{T}(\textbf{1}^{T}X_{s})\right),$
    |  | (13) |'
- en: '|  | $\displaystyle C^{t}$ | $\displaystyle=\dfrac{1}{n_{t}-1}\left(X_{t}^{T}X_{t}-\dfrac{1}{n_{t}}(\textbf{1}^{T}X_{t})^{T}(\textbf{1}^{T}X_{t})\right),$
    |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle C^{t}$ | $\displaystyle=\dfrac{1}{n_{t}-1}\left(X_{t}^{T}X_{t}-\dfrac{1}{n_{t}}(\textbf{1}^{T}X_{t})^{T}(\textbf{1}^{T}X_{t})\right),$
    |  |'
- en: where 1 represents the column vector whose elements are all equal to one.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 1 表示元素均为 1 的列向量。
- en: 'Inspired by Deep CORAL proposed in [[59](#bib.bib59)], we design an UDTL-based
    IFD method by adding the CORAL loss into the loss function to realize the feature
    transfer shown in Fig. [7](#S3.F7 "Figure 7 ‣ III-C1 Basic concepts ‣ III-C Mapping-based
    UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study"). Also,
    the final loss function is defined as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 受 [[59](#bib.bib59)] 中提出的 Deep CORAL 启发，我们设计了一种基于 UDTL 的 IFD 方法，通过将 CORAL 损失添加到损失函数中来实现特征迁移，如图
    [7](#S3.F7 "图 7 ‣ III-C1 基本概念 ‣ III-C 基于映射的 UDTL ‣ III 标签一致的 UDTL ‣ 无监督深度迁移学习在智能故障诊断中的应用：调查与比较研究")
    所示。此外，最终的损失函数定义如下：
- en: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{CORAL}}\mathcal{L}_{\text{CORAL}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (14) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\mathcal{L}_{c}+\lambda_{\text{CORAL}}\mathcal{L}_{\text{CORAL}}\left(\mathcal{D}_{s},\mathcal{D}_{t}\right),$
    |  | (14) |'
- en: where $\lambda_{\text{CORAL}}$ is a trade-off parameter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{\text{CORAL}}$ 是一个权衡参数。
- en: '![Refer to caption](img/5655f24ff777c5a4c57328dffe9492d8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5655f24ff777c5a4c57328dffe9492d8.png)'
- en: 'Figure 7: The UDTL-based IFD model based on CORAL.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于 CORAL 的 UDTL 训练的 IFD 模型。
- en: III-C2 Applications to IFD
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 应用到 IFD
- en: BDA was used in [[70](#bib.bib70), [71](#bib.bib71)] to adaptively balance the
    importance of the marginal and conditional distribution discrepancy between feature
    domains learned by deep neural networks for IFD. The CORAL loss [[72](#bib.bib72),
    [73](#bib.bib73)] and maximum variance discrepancy (MVD) [[74](#bib.bib74)] were
    used to reduce the distribution discrepancy between different domains. Qian et
    al. [[58](#bib.bib58), [75](#bib.bib75)] considered the higher-order moments and
    proposed an HKL divergence to adjust domain distributions for rotating machine
    fault diagnosis. The distance designed to measure source and target tensor representations
    was proposed in [[76](#bib.bib76)] to align tensor representations into the invariant
    tensor subspace for bearing fault diagnosis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: BDA 被用来[[70](#bib.bib70), [71](#bib.bib71)]自适应地平衡由深度神经网络学习的特征领域之间的边际和条件分布差异。CORAL
    损失 [[72](#bib.bib72), [73](#bib.bib73)] 和最大方差差异 (MVD) [[74](#bib.bib74)] 被用来减少不同领域之间的分布差异。Qian
    等人 [[58](#bib.bib58), [75](#bib.bib75)] 考虑了高阶矩，并提出了一种 HKL 散度来调整领域分布以进行旋转机器故障诊断。[[76](#bib.bib76)]
    提出了用于测量源和目标张量表示的距离，以将张量表示对齐到不变张量子空间，以进行轴承故障诊断。
- en: Another metric distance, called MMD, was widely used in the field of intelligent
    diagnosis [[77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)].
    Tong et al. [[86](#bib.bib86), [87](#bib.bib87)] reduced marginal and conditional
    distributions simultaneously across domains based on MMD in the feature space
    by refining pseudo test labels for bearing fault diagnosis. Wang et al. [[88](#bib.bib88)]
    proposed a conditional MMD based on estimated pseudo labels to shorten the conditional
    distribution distance for bearing fault diagnosis. The marginal and conditional
    distributions were aligned simultaneously in multiple layers via minimizing MMD
    [[89](#bib.bib89), [90](#bib.bib90)]. Yang et al. [[91](#bib.bib91)] replaced
    the Gaussian kernel with a polynomial kernel in MMD for better aligning the distribution
    discrepancy. Cao et al. [[92](#bib.bib92)] proposed a pseudo-categorized MMD to
    narrow the intra-class cross-domain distribution discrepancy. MMD was also combined
    with other techniques, such as Grassmann manifold [[93](#bib.bib93)], locality
    preserving projection [[94](#bib.bib94)], and graph Laplacian regularization [[95](#bib.bib95),
    [96](#bib.bib96)], to boost the performance of distribution alignment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种度量距离，称为 MMD，在智能诊断领域广泛使用 [[77](#bib.bib77), [78](#bib.bib78), [79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84),
    [85](#bib.bib85)]。Tong 等人 [[86](#bib.bib86), [87](#bib.bib87)] 通过改进伪测试标签来减少特征空间中跨领域的边际和条件分布，同时用于轴承故障诊断。Wang
    等人 [[88](#bib.bib88)] 提出了基于估计伪标签的条件 MMD，以缩短轴承故障诊断中的条件分布距离。通过最小化 MMD [[89](#bib.bib89),
    [90](#bib.bib90)]，在多个层中同时对齐边际和条件分布。Yang 等人 [[91](#bib.bib91)] 用多项式核替代 MMD 中的高斯核，以更好地对齐分布差异。Cao
    等人 [[92](#bib.bib92)] 提出了伪分类 MMD，以缩小同类跨领域的分布差异。MMD 还与其他技术结合使用，如 Grassmann 流形 [[93](#bib.bib93)]、局部保持投影
    [[94](#bib.bib94)] 和图拉普拉斯正则化 [[95](#bib.bib95), [96](#bib.bib96)]，以提升分布对齐的性能。
- en: MK-MMD was used in [[97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [23](#bib.bib23),
    [100](#bib.bib100), [101](#bib.bib101)] to better transfer the distribution of
    learned features in the source domain to that in the target domain for IFD. Han
    et al. [[102](#bib.bib102)] and Qian et al. [[103](#bib.bib103)] used JDA to align
    both conditional and marginal distributions simultaneously to construct a more
    effective and robust feature representation for substantial distribution difference.
    Wu et al. [[104](#bib.bib104)] further used the grey wolf optimization algorithm
    to learn the parameters of JDA. Based on JMMD, Cao et al. [[105](#bib.bib105)]
    proposed a soft JMMD to reduce both the marginal and conditional distribution
    discrepancy with the enhancement of auxiliary soft labels.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: MK-MMD 被用于 [[97](#bib.bib97), [98](#bib.bib98), [99](#bib.bib99), [23](#bib.bib23),
    [100](#bib.bib100), [101](#bib.bib101)]，以更好地将源领域中学习到的特征分布转移到目标领域中的分布，用于 IFD。Han
    等人 [[102](#bib.bib102)] 和 Qian 等人 [[103](#bib.bib103)] 使用 JDA 同时对齐条件和边际分布，以构建更有效且稳健的特征表示来应对显著的分布差异。Wu
    等人 [[104](#bib.bib104)] 进一步使用了灰狼优化算法来学习 JDA 的参数。基于 JMMD，Cao 等人 [[105](#bib.bib105)]
    提出了一个软 JMMD，通过增强辅助软标签来减少边际和条件分布的不一致。
- en: III-D Adversarial-based UDTL
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 基于对抗的 UDTL
- en: III-D1 Basic concepts
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 基本概念
- en: Adversarial-based UDTL refers to an adversarial method using a domain discriminator
    to reduce the feature distribution discrepancy between source and target domains
    produced by a feature extractor. In this paper, we use two commonly used methods
    including domain adversarial neural network (DANN) [[106](#bib.bib106)] and conditional
    domain adversarial network (CDAN) [[107](#bib.bib107)] to represent adversarial-based
    methods and test the corresponding accuracy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对抗的 UDTL 指使用领域判别器的对抗方法，以减少由特征提取器产生的源领域与目标领域之间的特征分布差异。在本文中，我们使用两种常用的方法，包括领域对抗神经网络
    (DANN) [[106](#bib.bib106)] 和条件领域对抗网络 (CDAN) [[107](#bib.bib107)]，来代表基于对抗的方法并测试相应的准确性。
- en: 'DANN: Similar to MMD and MK-MMD, DANN is defined to solve the problem $P(X_{s})\neq
    Q(X_{t})$. It aims to train a feature extractor, a domain discriminator distinguishing
    source and target domains, and a class predictor, simultaneously to align source
    and target distributions. That is, DANN trains the feature extractor to prevent
    the domain discriminator from distinguishing differences between two domains.
    Let $G_{f}$ be the feature extractor whose parameters are $\theta_{f}$, $G_{c}$
    be the class predictor whose parameters are $\theta_{c}$, and $G_{d}$ be the domain
    discriminator whose parameters are $\theta_{d}$. After that, the prediction loss
    and the adversarial loss (the binary cross-entropy loss) can be rewritten as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: DANN：类似于 MMD 和 MK-MMD，DANN 被定义为解决 $P(X_{s})\neq Q(X_{t})$ 的问题。它旨在同时训练特征提取器、区分源领域和目标领域的领域判别器以及分类预测器，以对齐源领域和目标领域的分布。也就是说，DANN
    训练特征提取器，以防止领域判别器区分两个领域之间的差异。设 $G_{f}$ 为特征提取器，其参数为 $\theta_{f}$，$G_{c}$ 为分类预测器，其参数为
    $\theta_{c}$，$G_{d}$ 为领域判别器，其参数为 $\theta_{d}$。之后，预测损失和对抗损失（二元交叉熵损失）可以重新写为：
- en: '|  |  | $\displaystyle\mathcal{L}_{c}(\theta_{f},\theta_{c})=$ |  | (15) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{c}(\theta_{f},\theta_{c})=$ |  | (15) |'
- en: '|  |  | $\displaystyle-\mathbb{E}_{\left(x_{i}^{s},y_{i}^{s}\right)\in\mathcal{D}_{s}}\sum_{c=0}^{C-1}\mathbf{1}_{[y_{i}^{s}=c]}\log\left[G_{c}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{c}\right)\right],$
    |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\mathbb{E}_{\left(x_{i}^{s},y_{i}^{s}\right)\in\mathcal{D}_{s}}\sum_{c=0}^{C-1}\mathbf{1}_{[y_{i}^{s}=c]}\log\left[G_{c}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{c}\right)\right],$
    |  |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{DANN}}\left(\theta_{f},\theta_{d}\right)=$
    | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\log\left[G_{d}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{d}\right)\right]-$
    |  | (16) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{DANN}}\left(\theta_{f},\theta_{d}\right)=$
    | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\log\left[G_{d}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{d}\right)\right]-$
    |  | (16) |'
- en: '|  |  | $\displaystyle\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d}\right)\right].$
    |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d}\right)\right].$
    |  |'
- en: 'To sum up, the total loss of DANN can be defined as:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DANN 的总损失可以定义为：
- en: '|  | $\displaystyle\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)=\mathcal{L}_{c}\left(\theta_{f},\theta_{c}\right)-\lambda_{\text{DANN}}\mathcal{L}_{\text{DANN}}\left(\theta_{f},\theta_{d}\right),$
    |  | (17) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)=\mathcal{L}_{c}\left(\theta_{f},\theta_{c}\right)-\lambda_{\text{DANN}}\mathcal{L}_{\text{DANN}}\left(\theta_{f},\theta_{d}\right),$
    |  | (17) |'
- en: where $\lambda_{\text{DANN}}$ is a trade-off parameter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{\text{DANN}}$ 是一个权衡参数。
- en: 'During the training procedure, we need to minimize the prediction loss to allow
    the class predictor to predict true labels as much as possible. Additionally,
    we also need to maximize the adversarial loss to make the domain discriminator
    difficult to distinguish differences. Thus, solving the saddle point problem ($\hat{\theta}_{f},\hat{\theta}_{c},\hat{\theta}_{d}$)
    is equivalent to the following minimax optimization problem:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们需要最小化预测损失，以使分类预测器尽可能准确地预测真实标签。此外，我们还需要最大化对抗损失，使领域判别器难以区分差异。因此，解决鞍点问题
    ($\hat{\theta}_{f},\hat{\theta}_{c},\hat{\theta}_{d}$) 等同于以下的极小极大优化问题：
- en: '|  | $\displaystyle\left(\hat{\theta_{f}},\hat{\theta_{c}}\right)$ | $\displaystyle=\arg\min\limits_{\theta_{f},\theta_{c}}\mathcal{L}\left(\theta_{f},\theta_{c},\hat{\theta}_{d}\right),$
    |  | (18) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\hat{\theta_{f}},\hat{\theta_{c}}\right)$ | $\displaystyle=\arg\min\limits_{\theta_{f},\theta_{c}}\mathcal{L}\left(\theta_{f},\theta_{c},\hat{\theta}_{d}\right),$
    |  | (18) |'
- en: '|  | $\displaystyle\left(\hat{\theta_{d}}\right)$ | $\displaystyle=\arg\max\limits_{\theta_{d}}\mathcal{L}\left(\hat{\theta}_{f},\hat{\theta}_{c},\theta_{d}\right).$
    |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\hat{\theta_{d}}\right)$ | $\displaystyle=\arg\max\limits_{\theta_{d}}\mathcal{L}\left(\hat{\theta}_{f},\hat{\theta}_{c},\theta_{d}\right).$
    |  |'
- en: Following the statement in [[106](#bib.bib106)], we can simply add a special
    gradient reversal layer (GRL), which changes signs of the gradient from the subsequent
    level and is parameter-free, to solve the above optimization problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[106](#bib.bib106)]中的说法，我们可以简单地添加一个特殊的梯度反转层（GRL），它会改变来自后续层的梯度符号，并且没有参数，以解决上述优化问题。
- en: 'We design an UDTL-based IFD model via adding the adversarial idea into the
    loss function to realize the feature transfer between source and target domains
    shown in Fig. [8](#S3.F8 "Figure 8 ‣ III-D1 Basic concepts ‣ III-D Adversarial-based
    UDTL ‣ III Label-consistent UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study"). It
    can be observed that we use a three-layer Fc binary classifier as our domain discriminator
    which is the same as [[106](#bib.bib106)]. The output features of these Fc layers
    are 1024 (Fc1), 1024 (Fc2), and 2 (Fc3), respectively. The parameter of a Dropout
    layer is $p=0.5$.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一个基于 UDTL 的 IFD 模型，通过将对抗思想添加到损失函数中，实现源领域和目标领域之间的特征转移，如图 [8](#S3.F8 "图 8
    ‣ III-D1 基本概念 ‣ III-D 基于对抗的 UDTL ‣ III 标签一致的 UDTL ‣ 无监督深度迁移学习在智能故障诊断中的应用：调查与比较研究")
    所示。可以观察到，我们使用了一个三层 Fc 二分类器作为我们的领域鉴别器，这与 [[106](#bib.bib106)] 中的相同。这些 Fc 层的输出特征分别为
    1024（Fc1）、1024（Fc2）和 2（Fc3）。Dropout 层的参数为 $p=0.5$。
- en: '![Refer to caption](img/a48484e84fa19dfbb0fdfc7461fa5396.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a48484e84fa19dfbb0fdfc7461fa5396.png)'
- en: 'Figure 8: The UDTL-based IFD model based on DANN.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：基于 DANN 的 UDTL 领域智能故障诊断模型。
- en: 'CDAN: Although DANN can align the distributions of two domains efficiently,
    there may still exist some bottlenecks. As stated in [[107](#bib.bib107)], DANN
    cannot capture complex multi-modal structures and it is hard to condition the
    domain discriminator safely. Based on this statement, Long et al. [[107](#bib.bib107)]
    proposed a new adversarial-based UDTL model called CDAN to solve the problem $P(X_{s},Y_{s})\neq
    Q(X_{t},Y_{t})$. To briefly introduce the main idea inside CDAN, we first need
    to define the multi-linear map $\otimes$, which means the outer product of multiple
    random vectors. If two random vectors $x$ and $y$ are given, the mean mapping
    $x\otimes y$ can capture the complex multi-modal structures inside the data completely.
    Besides, the cross-covariance $\mathbb{E}_{xy}[\phi(x)\otimes\phi(y)]$ can be
    used to model the joint distribution $P(x,y)$ successfully. Thus, the conditional
    adversarial loss is defined as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: CDAN：虽然 DANN 能够有效对齐两个领域的分布，但仍可能存在一些瓶颈。如[[107](#bib.bib107)]所述，DANN 无法捕捉复杂的多模态结构，并且难以安全地调整领域鉴别器。基于这一点，Long
    等人[[107](#bib.bib107)] 提出了一个新的基于对抗的 UDTL 模型，称为 CDAN，以解决问题 $P(X_{s},Y_{s})\neq
    Q(X_{t},Y_{t})$。要简要介绍 CDAN 的主要思想，我们首先需要定义多线性映射 $\otimes$，它表示多个随机向量的外积。如果给定两个随机向量
    $x$ 和 $y$，均值映射 $x\otimes y$ 可以完全捕捉数据中的复杂多模态结构。此外，交叉协方差 $\mathbb{E}_{xy}[\phi(x)\otimes\phi(y)]$
    可以成功地建模联合分布 $P(x,y)$。因此，条件对抗损失定义如下：
- en: '|  | $\displaystyle\mathcal{L}_{\text{CDAN}}$ | $\displaystyle(\theta_{f},\theta_{d})=-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\log\left[G_{d}\left(G_{f}(x_{i}^{s})\otimes
    G_{c}\left(G_{f}(x_{i}^{s})\right)\right)\right]$ |  | (19) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{CDAN}}$ | $\displaystyle(\theta_{f},\theta_{d})=-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\log\left[G_{d}\left(G_{f}(x_{i}^{s})\otimes
    G_{c}\left(G_{f}(x_{i}^{s})\right)\right)\right]$ |  | (19) |'
- en: '|  |  | $\displaystyle-\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d}\left(G_{f}(x_{i}^{t})\otimes
    G_{c}\left(G_{f}(x_{i}^{t})\right)\right)\right],$ |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d}\left(G_{f}(x_{i}^{t})\otimes
    G_{c}\left(G_{f}(x_{i}^{t})\right)\right)\right],$ |  |'
- en: 'and the prediction loss is the same as that in ([15](#S3.E15 "In III-D1 Basic
    concepts ‣ III-D Adversarial-based UDTL ‣ III Label-consistent UDTL ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 预测损失与 ([15](#S3.E15 "在 III-D1 基本概念 ‣ III-D 基于对抗的 UDTL ‣ III 标签一致的 UDTL ‣ 无监督深度迁移学习在智能故障诊断中的应用：调查与比较研究"))
    中的相同。
- en: 'To relax the influence with uncertain predictions, the entropy criterion $H(p)=-\sum_{c=0}^{C-1}p_{c}\log
    p_{c}$ is used to define the uncertainty of predictions by classifiers, where
    $p_{c}$ is the probability of the predicted result corresponding to the label
    $c$. According to the defined entropy-aware weight function shown in ([20](#S3.E20
    "In III-D1 Basic concepts ‣ III-D Adversarial-based UDTL ‣ III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study")), those hard-to-transfer samples are
    re-weighted with lower weights in the modified conditional adversarial loss ([21](#S3.E21
    "In III-D1 Basic concepts ‣ III-D Adversarial-based UDTL ‣ III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study")):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w\left(H\left(p\right)\right)=1+e^{-H\left(p\right)}.$
    |  | (20) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{L}_{\text{CDAN}}(\theta_{f},\theta_{d})=$ | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}w\left(H\left(p_{i}^{s}\right)\right)$
    |  | (21) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\log\left[G_{d}\left(G_{f}(x_{i}^{s})\otimes G_{c}\left(G_{f}(x_{i}^{s})\right)\right)\right]$
    |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}w\left(H\left(p_{i}^{t}\right)\right)$
    |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\log\left[1-G_{d}\left(G_{f}(x_{i}^{t})\otimes G_{c}\left(G_{f}(x_{i}^{t})\right)\right)\right].$
    |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: 'We design an UDTL-based IFD model via embedding the conditional adversarial
    idea into the loss function to realize the feature transfer shown in Fig. [9](#S3.F9
    "Figure 9 ‣ III-D1 Basic concepts ‣ III-D Adversarial-based UDTL ‣ III Label-consistent
    UDTL ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study"). Also, the final loss function is
    defined as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)=\mathcal{L}_{c}\left(\theta_{f},\theta_{c}\right)-\lambda_{\text{CDAN}}\mathcal{L}_{\text{CDAN}}\left(\theta_{f},\theta_{d}\right),$
    |  | (22) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{\text{CDAN}}$ is a trade-off parameter.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3f1bf7d525a26bf43115bd4ec006b3a.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The UDTL-based IFD model based on CDAN.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: III-D2 Applications to IFD
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111),
    [112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114), [115](#bib.bib115)],
    the feature extractor was pre-trained with the labeled source data and was used
    to generate target features. After that, features from source and target domains
    were trained to maximize the domain discriminator loss, leading to distribution
    alignment for IFD. Classifier discrepancy [[116](#bib.bib116), [117](#bib.bib117),
    [118](#bib.bib118)], which means using separate classifiers for source and target
    domains, was introduced in UDTL-based IFD via an adversarial training process.
    Meanwhile, adversarial training was also combined with other metric distances,
    such as L1 alignment [[119](#bib.bib119)], MMD [[120](#bib.bib120)], MK-MMD [[121](#bib.bib121)],
    and JMMD [[122](#bib.bib122)], to better match the feature distributions between
    different domains for IFD. Li et al. [[123](#bib.bib123)] used two feature extractors
    and classifiers trained using MMD and domain adversarial training, respectively,
    and meanwhile ensemble learning was further utilized to obtain the final results.
    Qin et al. [[124](#bib.bib124)] proposed a multiscale transfer voting mechanism
    (MSTVM) to improve the classical domain adaption models and the verified model
    was trained by MMD and domain adversarial training. In addition, Qin et al. also
    proposed the parameter sharing [[125](#bib.bib125)] and multiscale [[126](#bib.bib126)]
    ideologies to reduce the complexity of network structures and extract more domain-invariant
    features. The verified models were trained by domain adversarial training embedded
    with metric distances, like MMD and CORAL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[108](#bib.bib108)、[109](#bib.bib109)、[110](#bib.bib110)、[111](#bib.bib111)、[112](#bib.bib112)、[113](#bib.bib113)、[114](#bib.bib114)、[115](#bib.bib115)]中，特征提取器经过预训练，使用标记的源数据，并用于生成目标特征。之后，源域和目标域的特征被训练以最大化领域判别器损失，从而实现IFD的分布对齐。分类器差异[[116](#bib.bib116)、[117](#bib.bib117)、[118](#bib.bib118)]，即为源域和目标域使用不同的分类器，通过对抗训练过程引入到基于UDTL的IFD中。同时，对抗训练也与其他度量距离结合，如L1对齐[[119](#bib.bib119)]、MMD[[120](#bib.bib120)]、MK-MMD[[121](#bib.bib121)]和JMMD[[122](#bib.bib122)]，以更好地匹配不同域之间的特征分布以用于IFD。Li等人[[123](#bib.bib123)]使用了两个特征提取器和使用MMD和领域对抗训练分别训练的分类器，同时进一步利用集成学习获得最终结果。Qin等人[[124](#bib.bib124)]提出了一种多尺度迁移投票机制（MSTVM）来改进经典的领域适应模型，并通过MMD和领域对抗训练对验证模型进行了训练。此外，Qin等人还提出了参数共享[[125](#bib.bib125)]和多尺度[[126](#bib.bib126)]理念，以减少网络结构的复杂性并提取更多领域不变特征。验证模型通过嵌入度量距离的领域对抗训练进行训练，如MMD和CORAL。
- en: Wasserstein distance was used in [[127](#bib.bib127), [128](#bib.bib128), [129](#bib.bib129),
    [130](#bib.bib130)] to guide adversarial training for aligning the discrepancy
    of distributions for IFD. Yu et al. [[131](#bib.bib131)] combined conditional
    adversarial DA with a center-based discriminative loss to realize both distribution
    discrepancy and feature discrimination for locomotive fault diagnosis. Li et al.
    [[132](#bib.bib132)] proposed a strategy for bearing fault diagnosis based on
    minimizing the joint distribution domain-adversarial loss which embedded the pseudo-label
    information into the adversarial training process. Besides, another strategy using
    adversarial-based methods contained adopting GAN to generate samples for the target
    domain [[133](#bib.bib133), [134](#bib.bib134)].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein距离在[[127](#bib.bib127)、[128](#bib.bib128)、[129](#bib.bib129)、[130](#bib.bib130)]中用于指导对抗训练，以对齐IFD的分布差异。Yu等人[[131](#bib.bib131)]将条件对抗DA与基于中心的判别损失结合起来，实现了对分布差异和特征判别的联合优化，用于机车故障诊断。Li等人[[132](#bib.bib132)]提出了一种基于最小化联合分布领域对抗损失的策略，该策略将伪标签信息嵌入到对抗训练过程中。此外，另一种使用对抗方法的策略包括采用GAN生成目标域样本[[133](#bib.bib133)、[134](#bib.bib134)]。
- en: IV Label-inconsistent UDTL
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 标签不一致的UDTL
- en: Considering that the label sets of source and target domains are hard to be
    consistent in real application, it is significant to study the label-inconsistent
    UDTL. In this paper, three label-inconsistent transfer settings, including partial
    UDTL, open set UDTL, and universal UDTL, are studied.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到源域和目标域的标签集在实际应用中难以一致，研究标签不一致的UDTL是重要的。在本文中，研究了三种标签不一致的迁移设置，包括部分UDTL、开放集UDTL和通用UDTL。
- en: IV-A Partial UDTL
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 部分 UDTL
- en: IV-A1 Basic concepts
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-A1 基本概念
- en: Partial UDTL, which was proposed in [[135](#bib.bib135)], is a transfer learning
    paradigm where the target label set ${{\cal C}_{t}}$ is a subspace of the source
    label set ${{\cal C}_{s}}$, i.e. ${{\cal C}_{t}}\subset{{\cal C}_{s}}$.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Partial adversarial domain adaptation (PADA)
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of popular partial UDTL methods named partial adversarial domain adaptation
    (PADA) was proposed by Cao et. al [[136](#bib.bib136)]. The model of PADA is similar
    to DANN and further considers that probabilities of assigning target data to the
    source-private classes would be small, and label predictions on all target data
    are average to quantify the contribution of each source class.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\gamma=\frac{1}{n_{t}}\sum\limits_{i=1}^{n_{t}}{y_{i}^{t}}.$
    |  | (23) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: 'After normalizing $\gamma$ by its maximum value, $\hat{\gamma}$ is served as
    a class-level weight:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{\gamma}=\frac{\gamma}{\max\left(\gamma\right)}.$ |  |
    (24) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: 'Via applying this class-level weight to the loss of the class predictor and
    domain discriminator, contributions of source samples belonging to source-private
    classes can be reduced. The prediction and adversarial losses are rewritten as
    follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{c}(\theta_{f},\theta_{c})=$ | $\displaystyle-\mathbb{E}_{\left(x_{i}^{s},y_{i}^{s}\right)\in\mathcal{D}_{s}}{\hat{\gamma}_{y_{i}^{s}}}$
    |  | (25) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sum_{c=0}^{C-1}\mathbf{1}_{[y_{i}^{s}=c]}\log\left[G_{c}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{c}\right)\right].$
    |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathcal{L}_{\text{PADA}}\left(\theta_{f},\theta_{d}\right)=$
    | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}{\hat{\gamma}_{y_{i}^{s}}}\log\left[G_{d}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{d}\right)\right]-$
    |  | (26) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d}\right)\right],$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: where $y_{i}^{s}$ is the truth label of source sample $x_{i}^{s}$, $\hat{\gamma}_{y_{i}^{s}}$
    is the normalized class weight, and $\lambda_{\text{PADA}}$ is a trade-off parameter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a4305db3ff04881b7d2ce7a4d1bde976.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The UDTL-based model based on PADA.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'We design an UDTL-based model via applying the class-level weight to the loss
    function to reduce the influence of outlier source classes as shown in Fig. [10](#S4.F10
    "Figure 10 ‣ IV-A2 Partial adversarial domain adaptation (PADA) ‣ IV-A Partial
    UDTL ‣ IV Label-inconsistent UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study"). The
    final loss function is defined as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)=\mathcal{L}_{c}\left(\theta_{f},\theta_{c}\right)-\lambda_{\text{PADA}}\mathcal{L}_{\text{PADA}}\left(\theta_{f},\theta_{d}\right).$
    |  | (27) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: IV-A3 Applications to IFD
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[137](#bib.bib137)], two classification networks were constructed and the
    class-level weights for the source domain were calculated using the target label
    predictions of two networks, and then weights were applied to the source classification
    loss to down-weight the influence of outlier source samples. Li et al. [[138](#bib.bib138)]
    added weight modules to the adversarial transfer network and a weighting learning
    strategy was constructed to quantify the transferability of source samples. Via
    filtering out irrelevant source samples, the distribution discrepancy across domains
    in the shared label space could be reduced. Li et al. [[139](#bib.bib139)] proposed
    a conditional data alignment technique to align distributions of healthy data
    and a prediction consistency technique to align the distributions of other classes
    in two domains. In [[140](#bib.bib140)], to facilitate the positive transfer of
    shared classes and reduce the negative transfer of outlier classes, the average
    domain prediction loss of each source class was used as the class-level weight.
    To avoid the potential negative effect and preserve the inter-class relationships,
    Wang et al. [[141](#bib.bib141)] proposed to unilaterally align the target domain
    to the source domain via adding a consistency loss which forces aligned source
    features to be close to pre-trained source features. Deng et al. [[142](#bib.bib142)]
    constructed sub-domain discriminator for each class to achieve better flexibility.
    A double layer attention mechanism was proposed to assign different attentions
    to sub-domain discriminators and different attentions to samples for selecting
    relevant samples. Yang et al. [[143](#bib.bib143)] proposed to learn domain-asymmetry
    factors via training a domain discriminator and source samples were weighted in
    the distribution adaptation to block irrelevant knowledge.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Open set UDTL
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IV-B1 Basic concepts
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering that the label space of target domain is uncertain for UDTL, Saito
    et al. proposed open set domain adaptation (OSDA) that the target domain could
    contain samples of classes which were absent in the source domain [[144](#bib.bib144)],
    i.e. ${{\cal C}_{s}}\subset{{\cal C}_{t}}$. The goal of OSDA is to correctly classify
    known-class target samples and recognize unknown-class target samples as an additional
    class.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Open set back-propagation (OSBP)
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Saito et al. [[144](#bib.bib144)] proposed an adversarial-based UDTL method,
    named OSBP, which aimed to make a pseudo decision boundary for unknown class.
    The model of OSBP is composed of a feature extractor $G_{f}$ and a $C+1$ classifier
    $G_{c}$, where $C$ denotes the number of source classes. The outputs of $G_{c}$
    are then input into Softmax to obtain class probabilities. The probability of
    $x$ being classified into class $c$ is defined as $p_{c}^{t}=\frac{\exp\left(G_{c}\left(G_{f}(x)\right)\right)}{\sum_{k=0}^{C}\exp\left(G_{k}\left(G_{f}(x)\right)\right)}$.
    1$\sim{C}$ and $C+1$ dimensions indicate the probability of known and unknown
    classes, respectively.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'To correctly classify source samples, the feature extractor and classifier
    are trained using the prediction loss $\mathcal{L}_{c}$ in ([6](#S2.E6 "In II-A
    The Definition of UDTL ‣ II Background and Definition ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study")). Moreover, the classifier is trained to recognize target samples as an
    unknown class via training the classifier to output $p_{C+1}^{t}=\tau$, where
    $\tau$ ranges from 0 to 1. While the feature extractor is trained to deceive the
    classifier via training the feature extractor to allow $p_{C+1}^{t}$ higher or
    lower than $\tau$. In this way, a good boundary between known and unknown target
    samples can be constructed. A binary cross-entropy loss is used for the adversarial
    training:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{{\cal L}_{\text{OSBP}}}\left({{\theta_{f}},{\theta_{c}}}\right)=-\tau\log\left({p_{C+1}^{t}}\right)-\left({1-\tau}\right)\log\left({1-p_{C+1}^{t}}\right).$
    |  | (28) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: 'We design an UDTL-based model via introducing the $C+1$ classifier and adding
    the adversarial idea to the loss function to make a pseudo decision boundary for
    the unknown class as shown in Fig. [11](#S4.F11 "Figure 11 ‣ IV-B2 Open set back-propagation
    (OSBP) ‣ IV-B Open set UDTL ‣ IV Label-inconsistent UDTL ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"). The saddle point ($\hat{\theta}_{f},\hat{\theta}_{c}$) is solved using
    the following min-max optimization problem:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left({\hat{{\theta_{f}}}}\right)$ | $\displaystyle=\arg\mathop{\max}\limits_{{\theta_{f}}}{{\cal
    L}_{\rm{c}}}\left({{\theta_{f}},{\hat{\theta}}_{c}}\right)-\lambda_{\text{OSBP}}{\cal
    L}_{\text{OSBP}}\left(\theta_{f},{\hat{\theta}}_{c}\right),$ |  | (29) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left({\hat{{\theta_{c}}}}\right)$ | $\displaystyle=\arg\mathop{\min}\limits_{{\theta_{c}}}{{\cal
    L}_{c}}\left({\hat{{\theta_{f}}},{\theta_{c}}}\right)+{\lambda_{\text{OSBP}}}{{\cal
    L}_{\text{OSBP}}}\left({\hat{{\theta_{f}}},{\theta_{c}}}\right).$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/2cda4c3c3663dc54234bdfeb223c8858.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The UDTL-based model based on OSBP.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 Applications to IFD
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Li et al. [[145](#bib.bib145)] proposed a new fault classifier to detect the
    unknown class, and a convolutional auto-encoder model was further built to recognize
    the number of new fault types in [[146](#bib.bib146)]. Zhang et al. [[147](#bib.bib147)]
    proposed an instance-level weighted UDTL method to apply similarities of target
    samples during feature alignment. To identify target samples with outlier classes,
    an outlier classifier was trained using target instances with pseudo outlier labels.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Universal UDTL
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IV-C1 Basic concepts
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You et al. [[148](#bib.bib148)] proposed universal domain adaptation (UDA) which
    imposed no prior knowledge on label sets. In UDA, for a given source label set
    and a target label set, they might contain a common label set and hold a private
    label set, respectively. UDA requires the model to either classify the target
    sample correctly if it is associated with a label in the common label set, or
    mark it as “unknown” otherwise. Let ${\cal C}_{s}$ denotes the source label set,
    ${\cal C}_{t}$ denotes the target label set, and ${\cal C}={\cal C}_{s}\cap{\cal
    C}_{t}$ denotes the common label set. $\overline{\cal C}_{s}={\cal C}_{s}\backslash\cal
    C$ and $\overline{\cal C}_{t}={\cal C}_{t}\backslash\cal C$ represent the source
    and target private label sets, respectively.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Universal adaptation network (UAN)
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You et al. [[148](#bib.bib148)] proposed UAN and designed an instance-level
    transferability criterion, exploiting the domain similarity and prediction uncertainty.
    The model of UAN is similar to that of DANN, while the difference is that UAN
    adds a non-adversarial domain discriminator $G_{d}^{\prime}$. The non-adversarial
    domain discriminator $G_{d}^{\prime}$ obtains the domain similarity $d^{\prime}=G_{d}^{\prime}(G_{f}(x))$.
    They assumed that ${\mathbb{E}_{x\sim{p_{{{\overline{\cal C}}_{s}}}}}}d^{\prime}>{\mathbb{E}_{x\sim{p_{{}_{\cal
    C}}}}}d^{\prime}>{\mathbb{E}_{x\sim{q_{{}_{\cal C}}}}}d^{\prime}>{\mathbb{E}_{x\sim{q_{{{\overline{\cal
    C}}_{t}}}}}}d^{\prime}$, where ${p_{{{\overline{\cal C}}_{s}}}}$ is the distribution
    of source data belonging to the label set ${\overline{C}_{s}}$ and ${q_{{{\overline{\cal
    C}}_{t}}}}$ is the distribution of target data belonging to label set ${\overline{C}_{t}}$.
    ${p_{\cal C}}$ and ${q_{\cal C}}$ are the distributions of source and target data
    belonging to ${\cal C}$, respectively. Considering that entropy can quantify the
    prediction uncertainty, they assumed that ${\mathbb{E}_{x\sim{q_{{{\overline{\cal
    C}}_{t}}}}}}H\left(p\right)>{\mathbb{E}_{x\sim{q_{{}_{\cal C}}}}}H\left(p\right)>{\mathbb{E}_{x\sim{p_{{}_{\cal
    C}}}}}H\left(p\right)>{\mathbb{E}_{x\sim{p_{{{\overline{\cal C}}_{s}}}}}}H\left(p\right)$.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The instance-level transferability criterion for source and target samples
    can be defined as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\omega\left({x_{i}^{s}}\right)=\frac{{H\left({p\left({x_{i}^{s}}\right)}\right)}}{{\log\left&#124;{{{\cal
    C}_{s}}}\right&#124;}}-d^{\prime}\left({x_{i}^{s}}\right),$ |  | (30) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\omega\left({x_{i}^{t}}\right)=d^{\prime}\left({x_{i}^{t}}\right)-\frac{{H\left({p\left({x_{i}^{t}}\right)}\right)}}{{\log\left&#124;{{{\cal
    C}_{s}}}\right&#124;}},$ |  | (31) |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: where $\omega\left({x_{i}^{s}}\right)$ and $\omega\left({x_{i}^{t}}\right)$
    indicate the probability of a source sample $x_{i}^{s}$ and a target sample $x_{i}^{t}$
    belonging to the common label set ${\cal C}$.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2f0ba488c966261227b80e89c12ba1d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The UDTL-based model based on UAN.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss of domain discriminator $G_{d}$ in ([16](#S3.E16 "In III-D1 Basic
    concepts ‣ III-D Adversarial-based UDTL ‣ III Label-consistent UDTL ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study")) is modified to:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{UAN}}\left(\theta_{f},\theta_{d}\right)=$
    | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\omega\left({x_{i}^{s}}\right)\log\left[G_{d}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{d}\right)\right]-$
    |  | (32) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\omega\left({x_{i}^{t}}\right)\log\left[1-G_{d}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d}\right)\right].$
    |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: 'The loss of non-adversarial domain discriminator $G_{d^{\prime}}$ is:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{d^{\prime}}\left(\theta_{f},\theta_{d^{\prime}}\right)=$
    | $\displaystyle-\mathbb{E}_{x_{i}^{s}\in\mathcal{D}_{s}}\log\left[G_{d^{\prime}}\left(G_{f}\left(x_{i}^{s};\theta_{f}\right);\theta_{d^{\prime}}\right)\right]-$
    |  | (33) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{E}_{x_{i}^{t}\in\mathcal{D}_{t}}\log\left[1-G_{d^{\prime}}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d^{\prime}}\right)\right].$
    |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: 'where $\theta_{d^{\prime}}$ is the parameters of non-adversarial domain discriminator
    $G_{d^{\prime}}$. The saddle point ($\hat{\theta}_{f},\hat{\theta}_{c},\hat{\theta}_{d},\hat{\theta}_{d^{\prime}}$)
    can be solved using the following min-max optimization problem:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left(\hat{\theta_{f}},\hat{\theta_{c}}\right)$ | $\displaystyle=\arg\min\limits_{\theta_{f},\theta_{c}}{\cal
    L}_{c}\left(\theta_{f},\theta_{c}\right)-\lambda_{\text{UAN}}{\cal L}_{\text{UAN}}\left(\theta_{f},{\hat{\theta}}_{d}\right),$
    |  | (34) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left(\hat{\theta_{d}}\right)$ | $\displaystyle=\arg\min\limits_{\theta_{d}}{\cal
    L}_{\text{UAN}}\left({\hat{\theta}}_{f},{\hat{\theta}}_{c},{\theta_{d}}\right),$
    |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left(\hat{\theta_{d^{\prime}}}\right)$ | $\displaystyle=\arg\min\limits_{\theta_{d^{\prime}}}{\cal
    L}_{d^{\prime}}\left({\hat{\theta}}_{f},\theta_{d^{\prime}}\right).$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: Via training UAN, distributions of source and target data in the shared label
    set can be maximally aligned and the category gap can be reduced. In the test
    phase, for a target sample $x_{i}^{t}$, if its $\omega(x_{i}^{t})$ is higher than
    the threshold $\omega_{0}$, it is regarded as the unknown class, otherwise it
    is predicted by its label prediction.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Applications to IFD
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhang et al. [[149](#bib.bib149)] proposed a selective UDTL method. Class-wise
    weights were applied to the source domain and instance-wise weights were applied
    to the target domain. An outlier identifier was trained to recognize unknown fault
    modes. Yu et al. [[150](#bib.bib150)] proposed a bilateral weighted adversarial
    network to align feature distributions of shared-class source and target samples,
    and to disentangle shared-class and outlier-class samples. After model training,
    the extreme value theory (EVT) model was established on the feature representation
    of source samples and was further used to detect unknown-class samples in the
    target domain.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: V Multi-domain UDTL
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering that a single source domain might not be enough for UDTL in real
    applications, it is also important to consider multi-domain UDTL, which can help
    learn domain-invariant features. In this paper, two kinds of multi-domain UDTL
    settings, including multi-domain adaptation (using the target data in the training
    phase) and domain generalization (not using the target data in the training phase),
    are studied. Because there are multiple source domains in multi-domain UDTL, we
    first need to redefine some basic symbols. Let $\left\{\mathcal{D}_{s,n}\right\}_{0}^{n_{sd}-1}$
    denote the source domains, where $n_{sd}$ denotes the number of source domains.
    $\mathcal{D}_{t}$ denotes the target domain. $\mathcal{D}_{s,n}$ means the $n$-th
    source domain. $x_{i,n}^{s}$ and $y_{i,n}^{s}$ are the $i$-th sample and its corresponding
    label. Besides, $d_{i,n}^{s}$ is the domain label of $x_{i,n}^{s}$ and $d_{i}^{t}$
    ($d_{i}^{t}={n}_{sd}$) is the domain label of $x_{i}^{t}$.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: V-A Multi-domain adaptation
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-A1 Basic concepts
  id: totrans-229
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The traditional UDTL based on one single source domain cannot make full use
    of the data from multi-source domains, which might fail to find the private relationship
    and domain-invariant features. Thus, multi-domain adaptation aims to utilize labeled
    multi-source domains and unlabeled target domains to dig the relationship and
    domain-invariant features.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Multi-source unsupervised adversarial domain adaptation (MS-UADA)
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are mainly two ways to realize multi-domain adaptation. One is that features
    should be domain-invariant [[151](#bib.bib151)], that is, the gap between different
    domains, including source and target domains in the feature space should be as
    small as possible. The other way is to find a source domain, which is the most
    similar to the target domain [[152](#bib.bib152), [153](#bib.bib153)]. The second
    way requires a distance to measure the similarity among domains. In this paper,
    we used the method proposed in [[151](#bib.bib151)] called multi-source unsupervised
    adversarial adaptation (MS-UADA) to realize multi-domain adaptation, and the structure
    is shown in Fig. [13](#S5.F13 "Figure 13 ‣ V-A2 Multi-source unsupervised adversarial
    domain adaptation (MS-UADA) ‣ V-A Multi-domain adaptation ‣ V Multi-domain UDTL
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study"). The loss of MS-UADA for the domain discriminator
    $G_{d}$ is defined as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{split}&amp;\mathcal{L}{{}_{\text{MS-UADA}}}=-{{\mathbb{E}}_{x_{i}^{t}\in{\mathcal{D}_{t}}}}\mathbf{1}_{[d_{i}^{t}=n_{sd}]}\log\left[G_{d}\left(G_{f}\left(x_{i}^{t};\theta_{f}\right);\theta_{d}\right)\right]\\
    &amp;-\sum\limits_{n=0}^{{{n}_{sd}}-1}{{\mathbb{E}}_{x_{i,n}^{s}\in{\mathcal{D}_{s,n}}}}\sum\limits_{d=0}^{{{n}_{sd}}-1}{{\mathbf{1}_{[d_{i,n}^{s}=d]}}\log\left[G_{d}\left(G_{f}\left(x_{i,n}^{s};\theta_{f}\right);\theta_{d}\right)\right]}.\end{split}$
    |  | (35) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: 'To reduce the gap, the features from ${{G}_{f}}$ should confuse ${{G}_{d}}$,
    which means ${{G}_{d}}$ cannot realize domain classification. Thus the training
    processing can be seen as a minimax game, and the total loss is:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left({{\theta}_{f}},{{\theta}_{c}},{{\theta}_{d}}\right)=\mathcal{L}_{c}-{{\lambda}_{\text{MS-UADA}}}\mathcal{L}{{}_{\text{MS-UADA}}},$
    |  | (36) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: where ${{\lambda}_{\text{MS-UADA}}}$ is the trade-off parameter. The way to
    optimize $\mathcal{L}\left({{\theta}_{f}},{{\theta}_{c}},{{\theta}_{d}}\right)$
    is consistent with the DANN.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/333d27ac97724626a0e0995cb7b745a2.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The UDTL-based model based on MS-UADA.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: V-A3 Applications to IFD
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhu et al. [[154](#bib.bib154)] proposed an adversarial learning strategy in
    multi-domain adaptation to capture the fault feature representation. Rezaeianjouybari
    et al. [[155](#bib.bib155)] proposed a novel multi-source adaptation framework,
    which could realize the alignment in both feature and task levels. Zhang et al.
    [[156](#bib.bib156)] proposed an adversarial multi-domain adaptation according
    to a classifier alignment method to capture domain-invariant features from multiple
    source domains. He et al. [[157](#bib.bib157)] proposed a method based on K-means
    and space transformation in multi-source domains. Wei et al. [[158](#bib.bib158)]
    proposed a multi-source adaptation framework to learn domain-invariant features
    on the basis of distributional similarities. Zhang et al. [[74](#bib.bib74)] proposed
    an enhanced transfer joint matching approach based on MVD and MMD for multi-domain
    adaptation. Li et al. [[159](#bib.bib159)] proposed a multi-domain adaptation
    method to learn the diagnostic knowledge via domain adversarial training. Huang
    et al. [[160](#bib.bib160)] proposed a multi-source dense adaptation adversarial
    network to realize fault diagnosis from various working conditions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: V-B Domain generalization (DG)
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: V-B1 Basic concepts
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Domain generalization (DG) is to learn shared knowledge from multiple source
    domains and generalize knowledge to the target domain which is unseen in the training
    phase. The biggest difference of DG is that unlabeled samples in the target domain
    only appear in the test phase. Based on the discussion in [[161](#bib.bib161)],
    the core idea of DG is that learned domain-invariant features should satisfy the
    following two properties: 1) Features extracted by ${{G}_{f}}$ should be discriminative.
    2) Features extracted from different source domains should be domain-invariant.
    More detailed information can be referred to [[161](#bib.bib161)].'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Invariant adversarial network (IAN)
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'According to the above description, the performance of DG depends on discriminative
    and domain-invariant features. Domain-invariant features require diagnosis models
    to reduce the feature gap among different domains. As described in the previous
    section, adversarial training can reduce the gap of features among different domains.
    In this paper, a simple adversarial training method called invariant adversarial
    network (IAN) [[162](#bib.bib162), [163](#bib.bib163)] based on DANN is used in
    DG to help ${{G}_{f}}$ extract domain-invariant features via aligning the marginal
    distribution. The structure of IAN is shown in Fig. [13](#S5.F13 "Figure 13 ‣
    V-A2 Multi-source unsupervised adversarial domain adaptation (MS-UADA) ‣ V-A Multi-domain
    adaptation ‣ V Multi-domain UDTL ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study") and
    the loss of IAN for the domain discriminator $G_{d}$ is defined as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\begin{split}\mathcal{L}_{\text{IAN}}=&amp;-\sum\limits_{n=0}^{{{n}_{sd}}-1}{{\mathbb{E}}_{x_{i,n}^{s}\in{\mathcal{D}_{s,n}}}}\sum\limits_{d=0}^{{{n}_{sd}}-1}\\
    &amp;{{\mathbf{1}_{[d_{i,n}^{s}=d]}}\log\left[G_{d}\left(G_{f}\left(x_{i,n}^{s};\theta_{f}\right);\theta_{d}\right)\right]}.\end{split}$
    |  | (37) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '${{G}_{f}}$ should confuse ${{G}_{d}}$. Thus the total loss of IAN is a minimax
    game:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)=\mathcal{L}_{c}-\lambda_{\text{IAN}}\mathcal{L}_{\text{IAN}},$
    |  | (38) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{\text{IAN}}$ is the trade-off parameter. The way to optimize
    $\mathcal{L}\left(\theta_{f},\theta_{c},\theta_{d}\right)$ is consistent with
    DANN.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/758ea46e18ccdab471180d4f62a5e6b9.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The UDTL-based model based on IAN.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Applications to IFD
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zheng et al. [[164](#bib.bib164)] proposed a DG network based on a priori diagnosis
    knowledge and preprocessing techniques for IFD. Liao et al. [[165](#bib.bib165)]
    proposed a deep semi-supervised DG network to use unlabeled and labeled source
    data by the Earth-mover distance. Li et al. [[162](#bib.bib162)] proposed a DG
    method in IFD by a combination of data augmentation, adversarial training, and
    distance-based metric. Yang et al. [[166](#bib.bib166)] used a center loss to
    learn domain-invariant features across various source domains to realize DG. Zhang
    et al. [[167](#bib.bib167)] proposed a conditional adversarial DG method based
    on a single discriminator for better transfer and low computational complexity.
    Han et al. [[168](#bib.bib168)] proposed a DG based hybrid diagnosis network for
    deploying to unseen working conditions via the triplet loss and adversarial training.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: VI Datasets
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VI-A Open source Datasets
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open source datasets are very important for development, comparisons, and evaluation
    of different algorithms. In this comparative study, we mainly test five datasets
    to verify the performance of different UDTL methods. The detailed description
    of five datasets is given as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Case Western Reserve University (CWRU) dataset
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The CWRU dataset provided by Case Western Reserve University Bearing Data Center
    [[169](#bib.bib169)] is one of the most famous open source datasets in IFD and
    has been already used by tremendous published papers. Following other papers,
    this paper also uses the drive end bearing fault data whose sampling frequency
    is equal to 12 kHz and ten bearing conditions are listed in Table [II](#S6.T2
    "TABLE II ‣ VI-A1 Case Western Reserve University (CWRU) dataset ‣ VI-A Open source
    Datasets ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning to
    Intelligent Fault Diagnosis: A Survey and Comparative Study"). In Table [II](#S6.T2
    "TABLE II ‣ VI-A1 Case Western Reserve University (CWRU) dataset ‣ VI-A Open source
    Datasets ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning to
    Intelligent Fault Diagnosis: A Survey and Comparative Study"), one normal bearing
    (NA) and three fault types including inner fault (IF), ball fault (BF) and outer
    fault (OF) are classified into ten categories (one health state and nine fault
    states) according to different fault sizes.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, as shown in Table [III](#S6.T3 "TABLE III ‣ VI-A1 Case Western Reserve
    University (CWRU) dataset ‣ VI-A Open source Datasets ‣ VI Datasets ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"), CWRU consists of four motor loads corresponding to four
    operating speeds. For the transfer learning task, this paper considers these working
    conditions as different tasks including 0, 1, 2, and 3. For example, task 0 $\longrightarrow$
    1 means that the source domain with a motor load 0 HP transfers to the target
    domain with a motor load 1 HP. In total, there are twelve transfer learning tasks.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: The description of class labels of CWRU.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '| Class Label | 0 | 1 | 2 | 3 | 4 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| Fault Location | NA | IF | BF | OF | IF |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| Fault Size (mils) | 0 | 7 | 7 | 7 | 14 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| Class Label | 5 | 6 | 7 | 8 | 9 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| Fault Location | BF | OF | IF | BF | OF |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| Fault Size (mils) | 14 | 14 | 21 | 21 | 21 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: The transfer learning tasks of CWRU.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | 0 | 1 | 2 | 3 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Load (HP) | 0 | 1 | 2 | 3 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Speed (rpm) | 1797 | 1772 | 1750 | 1730 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: VI-A2 Paderborn University (PU) dataset
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The PU dataset acquired from Paderborn University is a bearing dataset [[170](#bib.bib170),
    [171](#bib.bib171)] which consists of artificially induced and real damages. The
    sampling frequency is equal to 64 kHz. Via changing the rotating speed of the
    drive system, the radial force onto the test bearing and the load torque on the
    drive train, the PU dataset consists of four operating conditions as shown in
    Table [IV](#S6.T4 "TABLE IV ‣ VI-A2 Paderborn University (PU) dataset ‣ VI-A Open
    source Datasets ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study").'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Thirteen bearings with real damages caused by accelerated lifetime tests [[170](#bib.bib170)]
    are used to study transfer learning tasks among different working conditions (twenty
    experiments were performed on each bearing code, and each experiment sustained
    four seconds). The categorization information is presented in Table [V](#S6.T5
    "TABLE V ‣ VI-A2 Paderborn University (PU) dataset ‣ VI-A Open source Datasets
    ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent
    Fault Diagnosis: A Survey and Comparative Study") (the meaning of contents is
    explained in [[170](#bib.bib170)]). In total, there are twelve transfer learning
    settings.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: The transfer learning tasks and operating parameters of PU.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | 0 | 1 | 2 | 3 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| Load Torque (Nm) | 0.7 | 0.7 | 0.1 | 0.7 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| Radial Force (N) | 1000 | 1000 | 1000 | 400 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| Speed (rpm) | 1500 | 900 | 1500 | 1500 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: The information of bearings with real damages.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Bearing &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Code &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '| Damage | Bearing Element | Combination | Characteristic of Damage | Label
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| KA04 | fatigue: pitting | OR | S | single point | 0 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| KA15 | plastic deform: indentations | OR | S | single point | 1 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| KA16 | fatigue: pitting | OR | R | single point | 2 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| KA22 | fatigue: pitting | OR | S | single point | 3 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| KA30 | plastic deform: indentations | OR | R | distributed | 4 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| KB23 | fatigue: pitting | IR(+OR) | M | single point | 5 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| KB24 | fatigue: pitting | IR(+OR) | M | distributed | 6 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| KB27 | plastic deform: indentations | OR+IR | M | distributed | 7 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| KI14 | fatigue: pitting | IR | M | single point | 8 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| KI16 | fatigue: pitting | IR | S | single point | 9 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| KI17 | fatigue: pitting | IR | R | single point | 10 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| KI18 | fatigue: pitting | IR | S | single point | 11 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| KI21 | fatigue: pitting | IR | S | single point | 12 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OR: outer ring; IR: inner ring; &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S: single damage; R: repetitive damage; M: multiple damages &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: VI-A3 JiangNan University (JNU) dataset
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The JNU dataset is a bearing dataset acquired by Jiang Nan University, China.
    JNU can be downloaded from [[172](#bib.bib172)] and scholars can refer to [[173](#bib.bib173)]
    for more detailed information. Four kinds of health conditions, including NA,
    IF, OF, and BF, were carried out. Vibration signals were sampled under three rotating
    speeds (600 rpm, 800 rpm, and 1000 rpm) with the sampling frequency 50 kHz. Four
    rotating speeds set to be 600 rpm, 800 rpm, and 1000 rpm are considered as different
    tasks denoted as task 0, 1, and 2. In total, there are six transfer learning settings.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: VI-A4 PHM Data Challenge on 2009 (PHM2009) dataset
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The PHM2009 dataset is a generic industrial gearbox dataset provided by the
    PHM Data Challenge competition [[174](#bib.bib174)]. The sampling frequency is
    set to 200 KHz/3. Fourteen experiments (eight for spur gears and six for helical
    gears) were performed.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we utilize the helical gears dataset (six conditions) collected
    from accelerometers mounted on input shaft retaining plates. PHM2009 contains
    five rotating speeds and two loads, but only data collected from the former four
    shaft speeds under a high load are considered. Four rotating speeds set to be
    30 Hz, 35 Hz, 40 Hz, and 45 Hz are considered as different tasks denoted as task
    0, 1, 2, and 3. In total, there are twelve transfer learning settings.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: VI-A5 Southeast University (SEU) dataset
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Southeast University (SEU) dataset is a gearbox dataset provided by Southeast
    University, China [[33](#bib.bib33), [175](#bib.bib175)]. This dataset consists
    of two sub-datasets, including the bearing and gear datasets, which were both
    collected from Drivetrain Dynamics Simulator. Eight channels were collected, and
    we use the data from the channel 2. As shown in Table [VI](#S6.T6 "TABLE VI ‣
    VI-A5 Southeast University (SEU) dataset ‣ VI-A Open source Datasets ‣ VI Datasets
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study"), each sub-dataset consists of five conditions:
    one health state and four fault states.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Two kinds of working conditions with rotating speed - load configuration set
    to be 20 Hz - 0 V and 30 Hz - 2 V are considered as different tasks denoted as
    task 0 and 1. In total, there are two transfer learning settings.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: The transfer learning tasks of SEU.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Location | Type | Description |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| 0 | Gear | Health |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Bearing |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| 1 | Bearing | Ball | Crack in the ball |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| 2 | Bearing | Outer | Crack in the outer ring |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| 3 | Bearing | Inner | Crack in the inner ring |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| 4 | Bearing | Combination | Crack in the inner and outer rings |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| 5 | Gear | Chipped | Crack in the gear feet |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| 6 | Gear | Miss | Missing feet in the gear |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| 7 | Gear | Surface | Wear in the surface of the gear |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| 8 | Gear | Root | Crack in the root of the gear feet |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: VI-B Data preprocessing and splitting
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data preprocessing and splitting are two important aspects in terms of performance
    of UDTL-based IFD. Although UDTL-based methods often possess automatic feature
    learning capabilities, some data processing steps can help models achieve better
    performance, such as Short-time Fourier Transform (STFT) in speech signal classification
    and the data normalization in image classification. Besides, there often exist
    some pitfalls in the training phase, especially test leakage. That is, test samples
    are unheedingly used in the training phase.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Input types
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two kinds of input types tested in this paper, including the time
    domain input and the frequency domain input. For the former one, signals are used
    as the input directly and the sample length is 1024 without any overlapping. For
    the latter one, signals are first transformed into the frequency domain and the
    sample length is 512 due to the symmetry of spectral coefficients.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Normalization
  id: totrans-329
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data normalization is the basic procedure in UDTL-based IFD, which can keep
    input values into a certain range. In this paper, we use the Z-score normalization.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: VI-B3 Data splitting
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since this paper does not use the validation set to select the best model,
    the splitting of the validation set is ignored here. In UDTL-based IFD, data in
    the target domain are used in the training procedure to realize the domain alignment
    and are also used as the test sets. In fact, data in these two situations should
    not overlap, otherwise there would exist test leakage. Therefore, as shown in
    Fig. [15](#S6.F15 "Figure 15 ‣ VI-B3 Data splitting ‣ VI-B Data preprocessing
    and splitting ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"), we take 80%
    of total samples as the training set and 20% of total samples as the test set
    in source and target domains to avoid this test leakage.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a238c1605124eeba067379361da02c2b.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Data splitting for UDTL-based IFD.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: VII Comparative studies
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will discuss evaluation results, which are shown in Appendix A. To make the
    accuracy more readable, we use some visualization methods to present the results.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Training details
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We implement all UDTL-based IFD methods in Pytorch and put them into a unified
    code framework. Each model is trained for 300 epochs and model training and test
    processes are alternated during the training procedure. We adapt mini-batch Adam
    optimizer and the batch size is equal to 64. The “step” strategy in Pytorch is
    used as the learning rate annealing method and the initial learning rate is 0.001
    with a decay (multiplied by 0.1) in the epoch 150 and 250, respectively. We use
    a progressive training method increasing the trade-off parameter from 0 to 1 via
    multiplying by $\frac{1-\exp(-\zeta\kappa)}{1+\exp(-\zeta\kappa)}$ [[107](#bib.bib107)],
    where $\zeta=10$ and $\kappa$ means the training progress changing from 0 to 1
    after transfer learning strategies are activated.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: All experiments are executed under Window 10 and Pytorch 1.3 running on a computer
    with an Intel Core i7-9700K, GeForce RTX 2080Ti, and 16G RAM.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Label-consistent UDTL
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For MK-MMD, JMMD, CORAL, DANN, and CDAN, we train models with source samples
    in the former 50 epochs to get a so-called pre-trained model, and then transfer
    learning strategies are activated. For AdaBN, we update the statistics of BN layers
    via each batch for 3 extra epochs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: VII-B1 Evaluation metrics
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For simplicity, we use the overall accuracy, which is the number of correctly
    classified samples divided by the total number of samples in test data, to verify
    the performance of different models. To avoid the randomness, we perform experiments
    five times, and mean as well as maximum values of the overall accuracy are used
    to evaluate the final performance because variance of five experiments is not
    statistically useful. In this paper, we use mean and maximum accuracy in the last
    epoch denoted as Last-Mean and Last-Max to represent the test accuracy without
    any test leakage. Meanwhile, we also list mean and maximum accuracy denoted as
    Best-Mean and Best-Max in the epoch where models achieve the best performance.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: VII-B2 Results of datasets
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To make comparisons clearer, we summarize the highest average accuracy of different
    datasets among all methods, and results are shown in Fig. [16](#S7.F16 "Figure
    16 ‣ VII-B2 Results of datasets ‣ VII-B Label-consistent UDTL ‣ VII Comparative
    studies ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study"). We can observe that CWRU and JNU
    can achieve the accuracy over 95% and other datasets can only achieve an accuracy
    of around 60%. It is also worth mentioning that the accuracy is just a lower bound
    due to the fact that it is very hard to fine-tune every parameter in detail.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34b25e7cd6e07f049b17a9ae1db081bf.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The highest average accuracy of different datasets among all methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: VII-B3 Results of models
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Results of different methods are shown in Fig. [17](#S7.F17 "Figure 17 ‣ VII-B3
    Results of models ‣ VII-B Label-consistent UDTL ‣ VII Comparative studies ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study") to Fig. [21](#S7.F21 "Figure 21 ‣ VII-B3 Results of models
    ‣ VII-B Label-consistent UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"), and Fig. [21](#S7.F21 "Figure 21 ‣ VII-B3 Results of models ‣ VII-B Label-consistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study") is not set as
    the radar chart because two transfer tasks are not suitable for this visualization.
    For all datasets, methods discussed in this paper can improve the accuracy of
    Basis, except CORAL. For CORAL, it can only improve the accuracy in CWRU with
    the frequency domain input or in some transfer tasks. For AdaBN, the improvement
    is much smaller than other methods.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: In general, results of JMMD are better than those of MK-MMD, which indicates
    that the assumption of joint distribution in source and target domains is useful
    for improving the performance. Results of DANN and CDAN are generally better than
    those of MK-MMD, which indicates that adversarial training is helpful for aligning
    the domain shift.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46027de21b9fff925c1178e83eb02700.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The accuracy comparisons of different methods in CWRU.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1bc86d5e768c75c7f25400e93159bc4c.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The accuracy comparisons of different methods in PU.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d1405b1f6a9bc5989d2e9db94cc37fea.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: The accuracy comparisons of different methods in JNU.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a8a8b81723c095e686d604ad45eccd72.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: The accuracy comparisons of different methods in PHM2009.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/377d993665a132e5e5824a679a08dc71.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: The accuracy comparisons of different methods in SEU.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: VII-B4 Results of input types
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accuracy comparisons of two input types are shown in Fig. [22](#S7.F22 "Figure
    22 ‣ VII-B4 Results of input types ‣ VII-B Label-consistent UDTL ‣ VII Comparative
    studies ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study"), and it can be concluded that the
    time domain input achieves better accuracy in CWRU, JNU, and SEU, while the frequency
    domain input gets better accuracy in PU and PHM2009. Besides, the accuracy gap
    between these two input types is relatively large, and we cannot simply infer
    which one is better due to the influence of backbones.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for a new dataset, we should test results of different input types instead
    of just using the more advanced techniques to improve the performance of one input
    type, because using a different input type might improve the accuracy more efficient
    than using advanced techniques.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c72412c9450e989d0486e3356d41aab3.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: The accuracy comparisons of two input types with different datasets.
    (F) means the frequency domain input, and (T) means the time domain input.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: VII-B5 Results of accuracy types
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in Section [VII](#S7 "VII Comparative studies ‣ Applications of
    Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and
    Comparative Study"), we use four kinds of accuracy including Best-Mean, Best-Max,
    Last-Mean, and Last-Max to evaluate the performance. As shown in Fig. [23](#S7.F23
    "Figure 23 ‣ VII-B5 Results of accuracy types ‣ VII-B Label-consistent UDTL ‣
    VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"), the fluctuation
    of different experiments is sometimes large, especially for those datasets whose
    overall accuracy is not very high, which indicates that the used algorithms are
    not very stable and robust. Besides, it seems that the fluctuation of the time
    domain input is smaller than that of the frequency domain input, and the reason
    might be that the backbone used in this paper is more suitable for the time domain
    input.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [24](#S7.F24 "Figure 24 ‣ VII-B5 Results of accuracy types
    ‣ VII-B Label-consistent UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"), the fluctuation of different experiments is also large, which is dangerous
    for evaluating the true performance. Since Best uses the test set to choose the
    best model (it is a kind of test leakage), Last may be more suitable for representing
    the generalization accuracy.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Thus, on the one hand, the stability and robustness of UDTL-based IFD need more
    attention instead of just improving the accuracy. On the other hand, as we analyze
    above, the accuracy of the last epoch (Last) is more suitable for representing
    the generalization ability of algorithms when the fluctuation between Best and
    Last is large.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/96488a8fc3d2da06d24c0c6556d6cf77.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: The difference between Max and Mean according to Best average.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/caf701b07ef7d984a902c3935b8549cf.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: The difference between Best average and Last average according to
    Mean.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Label-inconsistent UDTL
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In these methods, the transfer learning strategies are activated from the beginning.
    For UAN, the trade-off parameter of the loss of non-adversarial domain discriminator
    is fixed as 1. The value $\tau$ of OSBP and the threshold $\omega_{0}$ of UAN
    are both set to 0.5 for all tasks.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: VII-C1 Evaluation metrics
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For partial-based transfer learning, the evaluation metrics are the same as
    that of label-consistent UDTL, including Last-Mean, Last-Max, Best-Mean, and Best-Max.
    For open set and universal transfer learning, due to the existence of unknown
    classes, only the overall accuracy is not sufficient for evaluating the model
    performance. To clearly explain evaluation metrics, several mathematical notations
    are defined. $M_{S}$ and $M_{U}$ are the number of correctly classified shared-class
    and successfully detected unknown-class test samples, respectively. $N_{S}$ and
    $N_{U}$ are the number of all shared-class and unknown-class test samples, respectively.
    $Acc_{c}$ is the accuracy of test samples from the $c$-th class.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: 'Following previous work in [[150](#bib.bib150), [176](#bib.bib176), [144](#bib.bib144),
    [177](#bib.bib177)], five evaluation metrics are employed:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 1)
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accuracy of shared classes: $\text{ALL}^{*}=\frac{M_{S}}{N_{S}}$'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2)
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accuracy of unknown classes: $\text{UNK}=\frac{M_{U}}{N_{U}}$'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3)
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Averaged accuracy of all classes: $\text{OS}=\frac{1}{C+1}\sum_{c=0}^{C}{Acc_{c}}$'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4)
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accuracy of all test samples: $\text{ALL}=\frac{M_{S}+M_{U}}{N_{S}+N_{U}}$'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5)
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Harmonic mean: $\text{H-score}=\frac{2\text{ALL}^{*}\;\text{UNK}}{\text{ALL}^{*}+\text{UNK}}$'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to the label-consistent UDTL, mean and maximum values of the overall
    accuracy are used to evaluate the final performance. We use the mean accuracy
    of all five evaluation metrics in the last epoch denoted as Last-Mean-ALL*, Last-Mean-UNK,
    Last-Mean-OS, Last-Mean-ALL, and Last-Mean-H-score. We use the accuracy when models
    perform the best H-score among five tests in the last epoch, denoted as Last-Max-ALL*,
    Last-Max-UNK, Last-Max-OS, Last-Max-ALL, and Last-Max-H-score. Meanwhile, we also
    list the mean accuracy denoted as Best-Mean-ALL*, Best-Mean-UNK, Best-Mean-OS,
    Best-Mean-ALL, and Best-Mean-H-score in the epoch where models achieve the best
    performance on H-score. We also list the accuracy denoted as Best-Max-ALL*, Best-Max-UNK,
    Best-Max-OS, Best-Max-ALL, and Best-Max-H-score where models achieve the best
    performance on H-score among five tests.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: VII-C2 Dataset settings
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CWRU is selected for testing the performance. Following recent works in [[150](#bib.bib150)],
    different classes are randomly selected to form transfer learning tasks to validate
    the effectiveness of models on different label sets. The fault diagnosis tasks
    for partial, open set, and universal transfer learning are presented in Table
    [VII](#S7.T7 "TABLE VII ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent UDTL
    ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study") respectively.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: The fault diagnosis tasks of CWRU.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Partial UDTL | Open Set UDTL | Universal UDTL |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Source Label Set | Target Label Set | Source Label Set | Target Label Set
    | Source Label Set | Target Label Set |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| 0-1 | 0$\sim$9 | 0,1,2,4,5,7,8,9 | 0,2,3,5,6,7,8,9 | 0$\sim$9 | 0,1,2,4,5,6,7,8,9
    | 1,2,3,4,5,7,8,9 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| 0-2 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| 0-3 | 0$\sim$9 | 1,2,4,6,7,8,9 | 0,1,2,3,4,5,6 | 0$\sim$9 | 0,1,2,3,4,5,7,8
    | 0,2,3,4,5,6,7,8,9 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| 1-0 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| 1-2 | 0$\sim$9 | 0,1,3,7,9 | 1,3,4,5,7,8 | 0$\sim$9 | 1,2,4,5,7,8,9 | 1,3,6,8
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| 1-3 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| 2-0 | 0$\sim$9 | 1,2,4,7 | 0,2,4,5,6,9 | 0$\sim$9 | 1,3,4,6,7,8 | 0,1,2,3,5,6,8,9
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| 2-1 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| 2-3 | 0$\sim$9 | 0,2,6 | 0,1,2,5,7 | 0$\sim$9 | 0,1,2,7,8 | 1,2,6,9 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| 3-0 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| 3-1 | 0$\sim$9 | 5,8 | 4,8,9 | 0$\sim$9 | 0,1,5,7,8 | 0,4,8 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| 3-2 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/6e01e6b0c309626cf5ea8820c4321e74.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: The overall accuracy of PADA with the time domain input.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b680cd4c67a3e2839d9f9ff6e110fee3.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: The overall accuracy of OSBP with the time domain input: (a) Best-Mean
    and (b) Last-Mean.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30021a77279fb161cf1be9f49cf13c15.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: The overall accuracy of UAN with the time domain input: (a) Best-Mean
    and (b) Last-Mean.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: VII-C3 Results of partial UDTL
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For simplicity, as shown in Fig. [25](#S7.F25 "Figure 25 ‣ VII-C2 Dataset settings
    ‣ VII-C Label-inconsistent UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"), we only list Best-Mean and Last-Mean of PADA with the time domain input
    due to the similarity between time and frequency domain inputs. We can observe
    that PADA can achieve good performance on most tasks according to the overall
    training phase. But for tasks 3-1, 2-3, and 3-2, Last-Mean is obviously lower
    than Best-Mean, indicating that negative transfer resulting from extra source
    labels cannot be addressed totally by PADA and there exist the overfitting problem
    during the training procedure.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: VII-C4 Results of open set UDTL
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Best-Mean and Last-Mean accuracy of OSBP with the time domain input are shown
    in Fig. [26](#S7.F26 "Figure 26 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"). From Fig. [26](#S7.F26
    "Figure 26 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent UDTL ‣ VII Comparative
    studies ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study") (a), it can be seen that OSBP can
    achieve relatively good performance on most transfer tasks. However as shown in
    Fig. [26](#S7.F26 "Figure 26 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study") (b), performance
    obviously degrades on the later stage, especially for UNK, which reveals that
    the model overfits on the source samples, and thus unknown-class samples cannot
    be effectively recognized. Moreover, the lowest $\text{ALL}^{*}$ is only about
    50 %, which means that only half of shared-class samples can be correctly classified.
    Therefore, more effective models, which not only have the ability to detect unknown-class
    samples but also ensure accurate shared-class classification, are required.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: VII-C5 Results of universal UDTL
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Best-Mean and Last-Mean accuracy of UAN with the time domain input are shown
    in Fig. [27](#S7.F27 "Figure 27 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"). Generally speaking,
    UAN can achieve excellent performance on the CWRU dataset according to Fig. [27](#S7.F27
    "Figure 27 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent UDTL ‣ VII Comparative
    studies ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study") (a). Similar to the results of OSBP,
    the performance of UAN also degrades on the later stage because of the overfitting
    problem and wrong feature alignment. In addition, the shared-class classification
    accuracy still need be improved. It is still difficult for the model to separate
    extra source classes and detect unknown classes from the target domain.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: VII-C6 Results of multi-criterion evaluation metric
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Due to the fact that we have five evaluation metrics for open set UDTL and
    universal UDTL, it is better to have a final score concerning different metrics
    for a better understanding of the result. Thus, we use Technique for Order Preference
    by Similarity to an Ideal Solution (TOPSIS), which is a famous method in the multi-criterion
    evluation metric, as the final score. Meanwhile, TOPSIS was also widely applied
    to the field of fault diagnosis[[178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)].
    In this paper, we use $\text{ALL}^{*}$, UNK, OS, ALL, and H-score to calculate
    TOPSIS for the multi-criterion evaluation. For the sake of simplicity, the weight
    of every index is set to 0.25 in TOPSIS. As shown in Fig. [28](#S7.F28 "Figure
    28 ‣ VII-C6 Results of multi-criterion evaluation metric ‣ VII-C Label-inconsistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study"), we can observe
    the TOPSIS comparsions of OSBP and UAN for different transfer learning tasks.
    It is not unexpected that the evaluation using TOPSIS is similar to these metrics
    in Fig. [26](#S7.F26 "Figure 26 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent
    UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study") and Fig. [27](#S7.F27
    "Figure 27 ‣ VII-C2 Dataset settings ‣ VII-C Label-inconsistent UDTL ‣ VII Comparative
    studies ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault
    Diagnosis: A Survey and Comparative Study"). The overall performance also degrades
    at the later stage due to the overfitting problem and wrong feature alignment.
    The shared-class classification accuracy has a relatively large space to be promoted.
    In addition, separating extra source classes and detecting unknown classes in
    the target domain are still not well solved.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edd8def60ce23fbd780acf71311f74e4.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: The TOPSIS of OSBP and UAN: (a) Best-Mean of OSBP, (b) Last-Mean
    of OSBP, (c) Best-Mean of UAN, and (d) Last-Mean of UAN.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Multi-domain UDTL
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For multi-domain UDTL, the evaluation metrics are the same as that of label-consistent
    UDTL, including Last-Mean, Last-Max, Best-mean, and Best-Max.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: VII-D1 Dataset settings
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similarity to label-inconsistent UDTL, CWRU is selected to test the performance
    of multi-domain UDTL, including MS-UADA and IAN. The types of inputs consist of
    the time and frequency domain inputs. The fault diagnosis tasks for multi-domain
    UDTL are listed in Table [VIII](#S7.T8 "TABLE VIII ‣ VII-D1 Dataset settings ‣
    VII-D Multi-domain UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"). For example, 123-0_T means that task 1, 2, and 3 (shown in Table [III](#S6.T3
    "TABLE III ‣ VI-A1 Case Western Reserve University (CWRU) dataset ‣ VI-A Open
    source Datasets ‣ VI Datasets ‣ Applications of Unsupervised Deep Transfer Learning
    to Intelligent Fault Diagnosis: A Survey and Comparative Study")) are used as
    multiple source domains; task 0 is used as the target domain; the time domain
    input is used as the model input. It should be mentioned that we do not use the
    target data in the training phase when testing the performance of DG.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: The task of multi-domain UDTL.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Source 1 | Source 2 | Source 3 | Target |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
- en: '| 012-3 | 0 | 1 | 2 | 3 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '| 123-0 | 1 | 2 | 3 | 0 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| 013-2 | 0 | 1 | 3 | 2 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| 230-1 | 2 | 3 | 0 | 1 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/9a79db5c78859bdddb61caa2da2e5fb2.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: The overall accuracy of multi-domain UDTL (F and T mean the time
    domain and the frequency domain inputs, respectively): (a) Best-Mean, (b) Last-Mean.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: VII-D2 Results of multi-domain adaptation
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [29](#S7.F29 "Figure 29 ‣ VII-D1 Dataset settings ‣ VII-D
    Multi-domain UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep
    Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study")
    (a) and (b), we can observe that MS-UADA can always improve the accuracy of CWRU
    compared with Basis which directly transfers the trained model using multiple
    source domains to the target domain. Performance of the time domain input is slightly
    better than that of the frequency domain input, but the overall difference is
    very tiny.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: VII-D3 Results of DG
  id: totrans-440
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As shown in Fig. [29](#S7.F29 "Figure 29 ‣ VII-D1 Dataset settings ‣ VII-D
    Multi-domain UDTL ‣ VII Comparative studies ‣ Applications of Unsupervised Deep
    Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study")
    (a) and (b), we can observe that the performance of IAN for CWRU is similar to
    that of Basis in most tasks. However, for the task 012-3_F, the accuracy of IAN
    decreases greatly. The main reason might be that IAN only uses multiple sources
    to find the domain-invariant features, which is not suitable for the unseen target
    domain. Thus, more complex DG methods should be further designed to dig the discriminative
    and domain-invariant features.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: VIII Further discussions
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VIII-A Transferability of features
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reason why DL models embedded transfer learning methods can achieve breakthrough
    performance in computer vision is that many studies have shown and proved that
    DL models can learn more transferable features for these tasks than traditional
    hand-crafted features [[181](#bib.bib181), [182](#bib.bib182)]. In spite of the
    ability to learn general and transferable features, DL models also exist transition
    from general features to specific features and their transferability drops significantly
    in the last layers [[182](#bib.bib182)]. Therefore, fine-tuning DL models or adding
    various transfer learning strategies into the training process need to be investigated
    for realizing the valid transfer.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: However, for IFD, there is no research about how transferable are features in
    DL models, and actually, answering this problem is the most important cornerstone
    in UDTL-based IFD. Since the aim of this paper is to give a comparative accuracy
    and release a code library, we just assume that the bottleneck layer is the task-specific
    layer and its output features are restrained with various transfer learning strategies.
    Thus, it is imperative and vital for scholars to study transferability of features
    and answer the question about how transferable features are learned. In order
    to make transferability of features more reasonable, we suggest that scholars
    might need to visualize neurons to analyze learned features by existing visualization
    algorithms [[183](#bib.bib183), [184](#bib.bib184)].
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: VIII-B Influence of backbones and bottleneck
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the field of computer vision, many strong CNN models (also called backbones),
    such as VGG [[24](#bib.bib24)] and ResNet [[25](#bib.bib25)] can be extended without
    caring about the model selection. Scholars often use the same backbones to test
    the performance of proposed algorithms and can pay more attention to construct
    specific algorithms to align source and target domains.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: However, backbones of published UDTL-based IFD are often different, which makes
    results hard to compare directly, and influence of different backbones has never
    been studied thoroughly. Whereas, backbones of UDTL-based algorithms do have a
    huge impact on results from comparisons between CWRU with the frequency domain
    input and “Table II” in [[111](#bib.bib111)] (the main difference is the backbone
    used in this paper and [[111](#bib.bib111)]). We can observe that the accuracy
    related to the task 3 in CWRU with the frequency domain input is much worse than
    that in “Table II” [[111](#bib.bib111)]. However, the backbone used in this paper
    can achieve excellent results with the time domain input and some accuracies are
    even higher than those in [[111](#bib.bib111)].
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a stronger statement, we also use the well-known backbone called ResNet18
    (we modify the structure of ResNet18 to adapt one dimensional input) to test SEU
    and PHM2009 datasets for explaining the huge impact of backbones. From comparisons
    of PHM2009 shown in Fig. [30](#S8.F30 "Figure 30 ‣ VIII-B Influence of backbones
    and bottleneck ‣ VIII Further discussions ‣ Applications of Unsupervised Deep
    Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study"),
    ResNet18 can improve the accuracy of each algorithm significantly. Besides, from
    comparisons of SEU shown in Fig. [31](#S8.F31 "Figure 31 ‣ VIII-B Influence of
    backbones and bottleneck ‣ VIII Further discussions ‣ Applications of Unsupervised
    Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative
    Study"), ResNet18 with the time domain input actually reduces the accuracy, and
    on the contrary, ResNet18 with the frequency domain input improve the accuracy
    significantly. In summary, different backbones behave differently with variant
    datasets and input types.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53d413f94616df13422020ae0569d31a.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: Comparisons of PHM2009.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ecf1b6d4fabd6b5f5ee677bdbd46365.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Comparisons of SEU.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, finding a strong and suitable backbone, which can learn more transferable
    features for IFD, is also very important for UDTL-based methods (sometimes choosing
    a more effective backbone is even more important than using a more advanced algorithm)
    We suggest that scholars should first find a strong backbone and then use the
    same backbone to compare results for avoiding unfair comparisons.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: In the top comparsion, we discuss the influence of backbones. However, in our
    designed structure, the bottleneck layer in the source domain also shares parameters
    with that in the target domain. Thus, it is necessary to discuss the influence
    of the bottleneck layer during the transfer learning procedure. For the sake of
    simplicity, we only use CWRU with two different inputs to test two representative
    UDTL methods, including MK-MMD and DANN.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Type I to represent original models in this paper, Type II to represent
    models without the bottleneck layer, and Type III to represent models with fixed
    parameters of backbones (their parameters are pretrained by the source data) when
    starting transfer learning (only updating parameters of the bottleneck layer during
    the transfer learning procedure). The comparison results are shown in Fig. [32](#S8.F32
    "Figure 32 ‣ VIII-B Influence of backbones and bottleneck ‣ VIII Further discussions
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study"). We can observe that for the time domain input,
    it is almost the same with and without the bottleneck layer. Likewise, for the
    frequency domain input, it is also difficult to judge which one is better. Thus,
    choosing a suitable network (according to datasets, transfer learning methods,
    input types, etc.), which can learn more transferable features, is very important
    for UDTL-based methods. In addition, it is clear that when parameters of backbones
    are fixed during the transfer learning procedure, the accuracy in the target domain
    decreases dramatically, which means that backbones trained using the source data
    cannot be transferred directly to the target domain.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c260a2a4f237283f05410244198f1cd8.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: Comparisons of three conditions related to the bottleneck layer.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: VIII-C Negative transfer
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in Section [IV](#S4 "IV Label-inconsistent UDTL ‣ Applications
    of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey
    and Comparative Study"), there are mainly four kinds of scenarios of UDTL-based
    IFD, but all experiments with five datasets are about transfer between different
    working conditions. To state that these scenarios are not always suitable for
    generating the positive transfer, we use the PU dataset to design another transfer
    task considering the transfer between different methods of generating damages.
    Each task consists of three health conditions, and detailed information is listed
    in Table [IX](#S8.T9 "TABLE IX ‣ VIII-C Negative transfer ‣ VIII Further discussions
    ‣ Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis:
    A Survey and Comparative Study"). There are two transfer learning settings in
    total.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: The information of bearings with artificial damages.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Precast Method | Damage Location | Damage Extent | Bearing Code |
    Label |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| 0 | Electric Engraver | OR | 1 | KA05 | 0 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| Electric Engraver | OR | 2 | KA03 | 1 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| Electric Engraver | IR | 1 | KI03 | 2 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| 1 | EDM and Drilling | OR | 1 | KA01 and KA07 | 0 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| Drilling | OR | 2 | KA08 | 1 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| EDM | IR | 1 | KI01 | 2 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: 'The transfer results are shown in Fig. [33](#S8.F33 "Figure 33 ‣ VIII-C Negative
    transfer ‣ VIII Further discussions ‣ Applications of Unsupervised Deep Transfer
    Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study") and
    Appendix A called PU-Types. We can observe that each method has a negative transfer
    with the time or frequency domain inputs, and this phenomenon indicates that this
    constructed task may not be suitable for the transfer learning task. Actually,
    there are also some published papers designing transfer learning tasks which tackle
    transferring the gear samples to the bearing samples (it may not be a reliable
    transfer task) or transferring the experimental data to the real data (if structures
    of two machines are different, it also may not be a reliable transfer task). Thus,
    it is very important to first figure out whether this task is suitable for transfer
    learning and whether two domains do have shared features.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c2e56b5cf2d1b4abd204927664c38663.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: The accuracy biases of these five methods corresponding to Basis.
    (F) means the frequency domain input, and (T) means the time domain input.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: VIII-D Physical priors
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the field of computer vision and natural language processing, new transfer
    learning methods often use the existing knowledge or laws to provide a meaningful
    explanation, such as attention mechanism [[185](#bib.bib185)] and multi-modal
    structures [[107](#bib.bib107)].
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: However, for UDTL-based IFD, many scholars only introduce methods, which have
    already existed in other fields, to perform IFD tasks and pay less attention to
    the prior knowledge behind the data (lack of using special phenomena or rules
    in physical systems). Therefore, we suggest that scholars can learn from core
    ideas in the field of transfer learning (not just use the existing methods) and
    introduce prior knowledge of physical systems into the proposed method to construct
    more targeted and suitable diagnostic models with higher recognition rates in
    industrial applications.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: VIII-E Label-inconsistent transfer
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, some scholars have considered the label-inconsistent scenario and
    proposed some specific methods to allow the model to adapt to this situation (detailed
    references can be found in the above review). However, as discussed in comparative
    results of label-inconsistent transfer, selected methods often face the risk of
    overfitting. That is, although the best average accuracy is acceptable, the last
    average accuracy often has a big drop. The main reason might be that models cannot
    focus on shared classes effectively, leading to poor domain alignment.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Hence, more attention should be paid to the label-inconsistent scenario to realize
    effective extra source classes separation and unknown classes detection from the
    target domain. A possible solution is to combine other valid open set recognition
    algorithms for better unknown class detection [[186](#bib.bib186), [150](#bib.bib150)].
    For example, an EVT model using deep features of source samples, was applied to
    detect unknown-class samples [[150](#bib.bib150)].
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: VIII-F Multi-domain transfer
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of published papers are based on a single source domain, but in real applications,
    the labeled data might be from multiple source domains. These domains often follow
    different distributions, but shared or related features exist among multiple source
    domains. A common step is to align the shared features via multi-domain adaptation
    or DG. However, how to balance contributions of multiple source domains is still
    not well solved. For example, in comparative analysis, we simply assume that each
    domain contributes equally to transfer learning. Thus, suitable weights should
    be carefully designed and added into the process of multi-domain transfer.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, to make better use of some data in unlabeled source domains, semi-supervised
    multi-domain learning [[165](#bib.bib165)] might also be worth focusing on. To
    further improve the accuracy, minimizing the gap of conditional distributions
    might be an effective way to align shared features [[167](#bib.bib167), [163](#bib.bib163)].
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: VIII-G Other aspects
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although a large amount of data in different conditions can be collected, fault
    data in some conditions are still scarce. Due to the fact that most machines operate
    in a normal condition, the class-imbalanced problem often naturally exists in
    real applications. Thus, imbalanced learning or few shot learning combined with
    transfer learning methods [[187](#bib.bib187)] might also be an important direction
    for better getting constructed algorithms off the ground.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Federated transfer learning (FTL) [[188](#bib.bib188)] provides a safer and
    more reliable approach for specific industries. At the same time, based on characteristics
    of transfer learning, FTL participants can own their own feature space without
    requiring all participants to own or use the same feature data, which makes FTL
    suitable for more application scenarios. FTL was initially used in IFD [[189](#bib.bib189)]
    and more in-depth research is required.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty quantification plays a critical role in assessing the safety of
    DL models during construction, optimization, and decision making procedures. Bayesian
    networks [[190](#bib.bib190)] and ensemble learning techniques [[191](#bib.bib191)]
    are two widely-used uncertainty quantification methods, and their effectiveness
    has been verified by different kinds of applications, such as bioinformatics,
    self-driving car, etc. Thus, uncertainty as an auxiliary term can be used to further
    correct some inappropriate predictions or results during the transfer learning.
    For example, the prediction uncertainty is explicitly estimated during training
    to rectify the pseudo label learning for UDTL of semantic segmentation [[192](#bib.bib192)].
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: IX Conclusion
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we construct a new taxonomy and perform a comprehensive review
    of UDTL-based IFD according to different tasks of UDTL. Five publicly available
    datasets are gathered to perform a comparative analysis of different UDTL-based
    IFD methods from several perspectives. Based on the systematically comparative
    study, we conclude that some useful results might be helpful for further research.
    Firstly, the accuracy of CWRU and JNU is larger than 95%. Secondly, results of
    different methods indicate that the assumption of joint distributions and adversarial
    training are two helpful techniques for promoting the accuracy. Thirdly, different
    input types often behave differently on each dataset, and choosing a suitable
    input type might also be important to improve the accuracy. Finally, the stability
    and robustness of UDTL-based IFD need to be taken seriously. To sum up, it might
    be useful for scholars to think ahead of these results before developing new models.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Also, we release the code library at [https://github.com/ZhaoZhibin/UDTL](https://github.com/ZhaoZhibin/UDTL)
    and try to give a basic performance of current algorithms to find the core that
    determines the transfer performance of algorithms to guide future research.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Pavan Kumar Kankar, Satish C Sharma, and Suraj Prakash Harsha, “Fault diagnosis
    of ball bearings using machine learning methods,” Expert Systems with applications,
    vol. 38, no. 3, pp. 1876–1886, 2011.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Mariela Cerrada, Grover Zurita, Diego Cabrera, René-Vinicio Sánchez, Mariano
    Artés, and Chuan Li, “Fault diagnosis in spur gears based on genetic algorithm
    and random forest,” Mechanical Systems and Signal Processing, vol. 70, pp. 87–103,
    2016.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Achmad Widodo and Bo-Suk Yang, “Support vector machine in machine condition
    monitoring and fault diagnosis,” Mechanical systems and signal processing, vol.
    21, no. 6, pp. 2560–2574, 2007.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Chaowei Tong, Shibin Wang, Ivan Selesnick, Ruqiang Yan, and Xuefeng Chen,
    “Ridge-aware weighted sparse time-frequency representation,” IEEE Transactions
    on Signal Processing, vol. 69, pp. 136–149, 2020.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Zhibin Zhao, Shuming Wu, Baijie Qiao, Shibin Wang, and Xuefeng Chen, “Enhanced
    sparse period-group lasso for bearing fault diagnosis,” IEEE Transactions on Industrial
    Electronics, vol. 66, no. 3, pp. 2143–2153, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Zhibin Zhao, Shibin Wang, David Wong, Wendong Wang, Ruqiang Yan, and Xuefeng
    Chen, “Fast sparsity-assisted signal decomposition with non-convex enhancement
    for bearing fault diagnosis,” IEEE/ASME Transactions on Mechatronics, 2021.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, “Deep learning,” nature,
    vol. 521, no. 7553, pp. 436, 2015.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Daniele Ravì, Charence Wong, Fani Deligianni, Melissa Berthelot, Javier
    Andreu-Perez, Benny Lo, and Guang-Zhong Yang, “Deep learning for health informatics,”
    IEEE journal of biomedical and health informatics, vol. 21, no. 1, pp. 4–21, 2016.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Seonwoo Min, Byunghan Lee, and Sungroh Yoon, “Deep learning in bioinformatics,”
    Briefings in bioinformatics, vol. 18, no. 5, pp. 851–869, 2017.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Lei Yang, Yunfei Wang, Yanjie Guo, Weiqiang Zhang, and Zhibin Zhao, “Robust
    working mechanism of water droplet-driven triboelectric nanogenerator: Triboelectric
    output versus dynamic motion of water droplet,” Advanced Materials Interfaces,
    vol. 6, no. 24, pp. 1901547, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Lei Yang, Yunfei Wang, Zhibin Zhao, Yanjie Guo, Sicheng Chen, Weiqiang
    Zhang, and Xiao Guo, “Particle-laden droplet-driven triboelectric nanogenerator
    for real-time sediment monitoring using a deep learning method,” ACS Applied Materials
    & Interfaces, vol. 12, no. 34, pp. 38192–38201, 2020.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Haiping Zhu, Jiaxin Cheng, Cong Zhang, Jun Wu, and Xinyu Shao, “Stacked
    pruning sparse denoising autoencoder based intelligent fault diagnosis of rolling
    bearings,” Applied Soft Computing, p. 106060, 2020.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Wenfeng Zhang, Gautam Biswas, Qi Zhao, Hongbo Zhao, and Wenquan Feng,
    “Knowledge distilling based model compression and feature learning in fault diagnosis,”
    Applied Soft Computing, vol. 88, pp. 105958, 2020.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Fan Xu, Yiu Lun Tse, et al., “Roller bearing fault diagnosis using stacked
    denoising autoencoder in deep learning and gath–geva clustering algorithm without
    principal component analysis and data label,” Applied Soft Computing, vol. 73,
    pp. 898–913, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Advances in neural information processing
    systems, 2012, pp. 1097–1105.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Andrew Ng et al., “Sparse autoencoder,” CS294A Lecture notes, vol. 72,
    no. 2011, pp. 1–19, 2011.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Zhibin Zhao, Tianfu Li, Jingyao Wu, Chuang Sun, Shibin Wang, Ruqiang Yan,
    and Xuefeng Chen, “Deep learning algorithms for rotating machinery intelligent
    diagnosis: An open source benchmark study,” ISA transactions, vol. 107, pp. 224–255,
    2020.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Huailiang Zheng, Rixin Wang, Yuantao Yang, Jiancheng Yin, Yongbo Li, Yuqing
    Li, and Minqiang Xu, “Cross-domain fault diagnosis using knowledge transfer strategy:
    a review,” IEEE Access, vol. 7, pp. 129260–129290, 2019.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Ruqiang Yan, Fei Shen, Chuang Sun, and Xuefeng Chen, “Knowledge transfer
    for rotary machine fault diagnosis,” IEEE Sensors Journal, vol. 20, no. 15, pp.
    8374–8393, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yaguo Lei, Bin Yang, Xinwei Jiang, Feng Jia, Naipeng Li, and Asoke K Nandi,
    “Applications of machine learning to machine fault diagnosis: A review and roadmap,”
    Mechanical Systems and Signal Processing, vol. 138, pp. 106587, 2020.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan, “Learning transferable
    features with deep adaptation networks,” in International conference on machine
    learning. PMLR, 2015, pp. 97–105.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang
    Liu, “A survey on deep transfer learning,” in International Conference on Artificial
    Neural Networks. Springer, 2018, pp. 270–279.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Xiang Li, Wei Zhang, Qian Ding, and Jian-Qiao Sun, “Multi-layer domain
    adaptation method for rolling bearing fault diagnosis,” Signal Processing, vol.
    157, pp. 180–197, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks
    for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual
    learning for image recognition,” in Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2016, pp. 770–778.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Ran Zhang, Hongyang Tao, Lifeng Wu, and Yong Guan, “Transfer learning
    with neural networks for bearing fault diagnosis in changing working conditions,”
    IEEE Access, vol. 5, pp. 14347–14357, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Cheng Zhang, Liqing Xu, Xingwang Li, and Huiyun Wang, “A method of fault
    diagnosis for rotary equipment based on deep learning,” in 2018 Prognostics and
    System Health Management Conference (PHM-Chongqing). IEEE, 2018, pp. 958–962.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Danmin Chen, Shuai Yang, and Funa Zhou, “Incipient fault diagnosis based
    on dnn with transfer learning,” in 2018 International Conference on Control, Automation
    and Information Sciences (ICCAIS). IEEE, 2018, pp. 303–308.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Md Junayed Hasan, Muhammad Sohaib, and Jong-Myon Kim, “1d cnn-based transfer
    learning model for bearing fault diagnosis under variable working conditions,”
    in International Conference on Computational Intelligence in Information System.
    Springer, 2018, pp. 13–23.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Hyunjae Kim and Byeng D Youn, “A new parameter repurposing method for
    parameter transfer with small dataset and its application in fault diagnosis of
    rolling element bearings,” IEEE Access, vol. 7, pp. 46917–46930, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Md Junayed Hasan, MM Manjurul Islam, and Jong-Myon Kim, “Acoustic spectral
    imaging and transfer learning for reliable bearing fault diagnosis under variable
    speed conditions,” Measurement, vol. 138, pp. 620–631, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Chuang Sun, Meng Ma, Zhibin Zhao, Shaohua Tian, Ruqiang Yan, and Xuefeng
    Chen, “Deep transfer learning based on sparse autoencoder for remaining useful
    life prediction of tool in manufacturing,” IEEE Transactions on Industrial Informatics,
    vol. 15, no. 4, pp. 2416–2425, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Siyu Shao, Stephen McAleer, Ruqiang Yan, and Pierre Baldi, “Highly accurate
    machine fault diagnosis using deep transfer learning,” IEEE Transactions on Industrial
    Informatics, vol. 15, no. 4, pp. 2446–2455, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Danmin Chen, Shuai Yang, and Funa Zhou, “Transfer learning based fault
    diagnosis with missing data due to multi-rate sampling,” Sensors, vol. 19, no.
    8, pp. 1826, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Wentao Mao, Ling Ding, Siyu Tian, and Xihui Liang, “Online detection for
    bearing incipient fault based on deep transfer learning,” Measurement, p. 107278,
    2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Zhiyi He, Haidong Shao, Ping Wang, Janet Jing Lin, Junsheng Cheng, and
    Yu Yang, “Deep transfer multi-wavelet auto-encoder for intelligent fault diagnosis
    of gearbox with few target training samples,” Knowledge-Based Systems, vol. 191,
    pp. 105313, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Fudong Li, Jinglong Chen, Jun Pan, and Tongyang Pan, “Cross-domain learning
    in rotating machinery fault diagnosis under various operating conditions based
    on parameter transfer,” Measurement Science and Technology, vol. 31, no. 8, pp.
    085104, 2020.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Sayed Ali Sharaf, “Beam pump dynamometer card prediction using artificial
    neural networks,” KnE Engineering, vol. 3, no. 7, pp. 198–212, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Pei Cao, Shengli Zhang, and Jiong Tang, “Preprocessing-free gear fault
    diagnosis using small datasets with deep convolutional neural network-based transfer
    learning,” IEEE Access, vol. 6, pp. 26241–26253, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Zhuyun Chen, Konstantinos Gryllias, and Weihua Li, “Intelligent fault
    diagnosis for rotary machinery using transferable convolutional neural network,”
    IEEE Transactions on Industrial Informatics, vol. 16, no. 1, pp. 339–349, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] D Iba, Y Ishii, Y Tsutsui, N Miura, T Iizuka, A Masuda, A Sone, and I Moriwaki,
    “Vibration analysis of a meshing gear pair by neural network (visualization of
    meshing vibration and detection of a crack at tooth root by vgg16 with transfer
    learning),” in Smart Structures and NDE for Energy Systems and Industry 4.0. International
    Society for Optics and Photonics, 2019, vol. 10973, p. 109730Y.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Ping Ma, Hongli Zhang, Wenhui Fan, Cong Wang, Guangrui Wen, and Xining
    Zhang, “A novel bearing fault diagnosis method based on 2d image representation
    and transfer learning-convolutional neural network,” Measurement Science and Technology,
    vol. 30, no. 5, pp. 055402, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Zhiyi He, Haidong Shao, Xiang Zhong, and Xianzhu Zhao, “Ensemble transfer
    cnns driven by multi-channel signals for fault diagnosis of rotating machinery
    cross working conditions,” Knowledge-Based Systems, vol. 207, pp. 106396, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] ZiYang Di, HaiDong Shao, and JiaWei Xiang, “Ensemble deep transfer learning
    driven by multisensor signals for the fault diagnosis of bevel-gear cross-operation
    conditions,” Science China Technological Sciences, vol. 64, no. 3, pp. 481–492,
    2021.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Zheng Wang, Qingxiu Liu, Hansi Chen, and Xuening Chu, “A deformable cnn-dlstm
    based transfer learning method for fault diagnosis of rolling bearing under multiple
    working conditions,” International Journal of Production Research, pp. 1–15, 2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Jianyu Wang, Zhenling Mo, Heng Zhang, and Qiang Miao, “A deep learning
    method for bearing fault diagnosis based on time-frequency image,” IEEE Access,
    vol. 7, pp. 42373–42383, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Haidong Shao, Min Xia, Guangjie Han, Yu Zhang, and Jiafu Wan, “Intelligent
    fault diagnosis of rotor-bearing system under varying working conditions with
    modified transfer convolutional neural network and thermal images,” IEEE Transactions
    on Industrial Informatics, vol. 17, no. 5, pp. 3488–3496, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Aqsa Saeed Qureshi, Asifullah Khan, Aneela Zameer, and Anila Usman, “Wind
    power prediction using deep neural network based meta regression and transfer
    learning,” Applied Soft Computing, vol. 58, pp. 742–755, 2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Shi-sheng Zhong, Song Fu, and Lin Lin, “A novel gas turbine fault diagnosis
    method based on transfer learning with cnn,” Measurement, vol. 137, pp. 435–453,
    2019.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Te Han, Chao Liu, Wenguang Yang, and Dongxiang Jiang, “Learning transferable
    features in deep convolutional neural networks for diagnosing unseen machine conditions,”
    ISA transactions, vol. 93, pp. 341–353, 2019.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Gaowei Xu, Min Liu, Zhuofu Jiang, Weiming Shen, and Chenxi Huang, “Online
    fault diagnosis method based on transfer convolutional neural networks,” IEEE
    Transactions on Instrumentation and Measurement, vol. 69, no. 2, pp. 509–520,
    2019.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Bo Zhao, Xianmin Zhang, Zhenhui Zhan, and Shuiquan Pang, “Deep multi-scale
    convolutional transfer learning network: A novel method for intelligent fault
    diagnosis of rolling bearings under variable working conditions and domains,”
    Neurocomputing, vol. 407, pp. 24–38, 2020.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu, “Boosting for transfer
    learning,” in Proceedings of the 24th international conference on Machine learning.
    ACM, 2007, pp. 193–200.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou, “Revisiting
    batch normalization for practical domain adaptation,” arXiv preprint arXiv:1603.04779,
    2016.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Sergey Ioffe and Christian Szegedy, “Batch normalization: Accelerating
    deep network training by reducing internal covariate shift,” arXiv preprint arXiv:1502.03167,
    2015.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Dengyu Xiao, Yixiang Huang, Chengjin Qin, Zhiyu Liu, Yanming Li, and Chengliang
    Liu, “Transfer learning with convolutional neural networks for small sample size
    problem in machinery fault diagnosis,” Proceedings of the Institution of Mechanical
    Engineers, Part C: Journal of Mechanical Engineering Science, p. 0954406219840381,
    2019.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Wei Zhang, Gaoliang Peng, Chuanhao Li, Yuanhang Chen, and Zhujun Zhang,
    “A new deep learning model for fault diagnosis with good anti-noise and domain
    adaptation ability on raw vibration signals,” Sensors, vol. 17, no. 2, pp. 425,
    2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Weiwei Qian, Shunming Li, and Jinrui Wang, “A new transfer learning method
    and its application on rotating machine fault diagnosis under variant working
    conditions,” IEEE Access, vol. 6, pp. 69907–69917, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Baochen Sun and Kate Saenko, “Deep coral: Correlation alignment for deep
    domain adaptation,” in European Conference on Computer Vision. Springer, 2016,
    pp. 443–450.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel,
    Bernhard Schölkopf, and Alex J Smola, “Integrating structured biological data
    by kernel maximum mean discrepancy,” Bioinformatics, vol. 22, no. 14, pp. e49–e57,
    2006.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu,
    et al., “Equivalence of distance-based and rkhs-based statistics in hypothesis
    testing,” The Annals of Statistics, vol. 41, no. 5, pp. 2263–2291, 2013.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan,
    Massimiliano Pontil, Kenji Fukumizu, and Bharath K Sriperumbudur, “Optimal kernel
    choice for large-scale two-sample tests,” in Advances in neural information processing
    systems, 2012, pp. 1205–1213.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S
    Yu, “Transfer feature learning with joint distribution adaptation,” in Proceedings
    of the IEEE international conference on computer vision, 2013, pp. 2200–2207.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, and Zhiqi Shen, “Balanced
    distribution adaptation for transfer learning,” in 2017 IEEE International Conference
    on Data Mining (ICDM). IEEE, 2017, pp. 1129–1134.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan, “Deep transfer
    learning with joint adaptation networks,” in Proceedings of the 34th International
    Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 2208–2217.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang, “Domain
    adaptation via transfer component analysis,” IEEE Transactions on Neural Networks,
    vol. 22, no. 2, pp. 199–210, 2010.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell,
    “Deep domain confusion: Maximizing for domain invariance,” arXiv preprint arXiv:1412.3474,
    2014.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] TŁ Żynda, “On weights which admit reproducing kernel of szegő type,” Journal
    of Contemporary Mathematical Analysis (Armenian Academy of Sciences), vol. 55,
    no. 5, pp. 320–327, 2020.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Baochen Sun, Jiashi Feng, and Kate Saenko, “Return of frustratingly easy
    domain adaptation,” in Thirtieth AAAI Conference on Artificial Intelligence, 2016.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Kaijie Wang and Bin Wu, “Power equipment fault diagnosis model based on
    deep transfer learning with balanced distribution adaptation,” in International
    Conference on Advanced Data Mining and Applications. Springer, 2018, pp. 178–188.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Yujing Wang, Chao Wang, Shouqiang Kang, Jinbao Xie, Qingyan Wang, and
    VI Mikulovich, “Network-combined broad learning and transfer learning: a new intelligent
    fault diagnosis method for rolling bearings,” Measurement Science and Technology,
    vol. 31, no. 11, pp. 115013, 2020.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Xiaoxia Wang, Haibo He, and Lusi Li, “A hierarchical deep domain adaptation
    approach for fault diagnosis of power plant thermal system,” IEEE Transactions
    on Industrial Informatics, vol. 15, no. 9, pp. 5139–5148, 2019.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Jing An, Ping Ai, and Dakun Liu, “Deep domain adaptation model for bearing
    fault diagnosis with domain alignment and discriminative feature learning,” Shock
    and Vibration, vol. 2020, 2020.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Zhongwei Zhang, Huaihai Chen, Shunming Li, and Zenghui An, “Unsupervised
    domain adaptation via enhanced transfer joint matching for bearing fault diagnosis,”
    Measurement, vol. 165, pp. 108071, 2020.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Weiwei Qian, Shunming Li, and Xingxing Jiang, “Deep transfer network for
    rotating machine fault analysis,” Pattern Recognition, vol. 96, pp. 106993, 2019.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Chaofan Hu, Yanxue Wang, and Jiawei Gu, “Cross-domain intelligent fault
    classification of bearings based on tensor-aligned invariant subspace learning
    and two-dimensional convolutional neural networks,” Knowledge-Based Systems, vol.
    209, pp. 106214, 2020.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Weining Lu, Bin Liang, Yu Cheng, Deshan Meng, Jun Yang, and Tao Zhang,
    “Deep model based domain adaptation for fault diagnosis,” IEEE Transactions on
    Industrial Electronics, vol. 64, no. 3, pp. 2296–2305, 2016.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Bo Zhang, Wei Li, Xiao-Li Li, and See-Kiong Ng, “Intelligent fault diagnosis
    under varying working conditions based on domain adaptive convolutional neural
    networks,” IEEE Access, vol. 6, pp. 66367–66384, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Long Wen, Liang Gao, and Xinyu Li, “A new deep transfer learning based
    on sparse auto-encoder for fault diagnosis,” IEEE Transactions on Systems, Man,
    and Cybernetics: Systems, vol. 49, no. 1, pp. 136–144, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Bin Yang, Yaguo Lei, Feng Jia, and Saibo Xing, “An intelligent fault diagnosis
    approach based on transfer learning from laboratory bearings to locomotive bearings,”
    Mechanical Systems and Signal Processing, vol. 122, pp. 692–706, 2019.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Shanxuan Tang, Hailong Tang, and Min Chen, “Transfer-learning based gas
    path analysis method for gas turbines,” Applied Thermal Engineering, vol. 155,
    pp. 1–13, 2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Xiang Li, Wei Zhang, and Qian Ding, “Cross-domain fault diagnosis of rolling
    element bearings using deep generative neural networks,” IEEE Transactions on
    Industrial Electronics, vol. 66, no. 7, pp. 5525–5534, 2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Yan Xu, Yanming Sun, Xiaolong Liu, and Yonghua Zheng, “A digital-twin-assisted
    fault diagnosis using deep transfer learning,” IEEE Access, vol. 7, pp. 19990–19999,
    2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Zenghui An, Shunming Li, Yu Xin, Kun Xu, and Huijie Ma, “An intelligent
    fault diagnosis framework dealing with arbitrary length inputs under different
    working conditions,” Measurement Science and Technology, vol. 30, no. 12, pp.
    125107, 2019.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Xiang Li, Xiao-Dong Jia, Wei Zhang, Hui Ma, Zhong Luo, and Xu Li, “Intelligent
    cross-machine fault diagnosis approach with deep auto-encoder and domain adaptation,”
    Neurocomputing, vol. 383, pp. 235–247, 2020.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Zhe Tong, Wei Li, Bo Zhang, and Meng Zhang, “Bearing fault diagnosis based
    on domain adaptation using transferable features under different working conditions,”
    Shock and Vibration, vol. 2018, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Zhe Tong, Wei Li, Bo Zhang, Fan Jiang, and Gongbo Zhou, “Bearing fault
    diagnosis under variable working conditions based on domain adaptation using feature
    transfer learning,” IEEE Access, vol. 6, pp. 76187–76197, 2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Xu Wang, Changqing Shen, Min Xia, Dong Wang, Jun Zhu, and Zhongkui Zhu,
    “Multi-scale deep intra-class transfer learning for bearing fault diagnosis,”
    Reliability Engineering & System Safety, vol. 202, pp. 107050, 2020.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Qikang Li, Baoping Tang, Lei Deng, Yanling Wu, and Yi Wang, “Deep balanced
    domain adaptation neural networks for fault diagnosis of planetary gearboxes with
    limited labeled data,” Measurement, vol. 156, pp. 107570, 2020.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Nannan Lu, Hanhan Xiao, Yanjing Sun, Min Han, and Yanfen Wang, “A new
    method for intelligent fault diagnosis of machines based on unsupervised domain
    adaptation,” Neurocomputing, vol. 427, pp. 96–109, 2021.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Bin Yang, Yaguo Lei, Feng Jia, Naipeng Li, and Zhaojun Du, “A polynomial
    kernel induced distance metric to improve deep transfer learning for fault diagnosis
    of machines,” IEEE Transactions on Industrial Electronics, vol. 67, no. 11, pp.
    9747–9757, 2020.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Xincheng Cao, Yu Wang, Binqiang Chen, and Nianyin Zeng, “Domain-adaptive
    intelligence for fault diagnosis based on deep transfer learning from scientific
    test rigs to industrial applications,” Neural Computing and Applications, vol.
    33, no. 9, pp. 4483–4499, 2021.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Ke Zhao, Hongkai Jiang, Zhenghong Wu, and Tengfei Lu, “A novel transfer
    learning fault diagnosis method based on manifold embedded distribution alignment
    with a little labeled data,” Journal of Intelligent Manufacturing, pp. 1–15, 2020.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Huailiang Zheng, Rixin Wang, Jiancheng Yin, Yuqing Li, Haiqing Lu, and
    Minqiang Xu, “A new intelligent fault identification method based on transfer
    locality preserving projection for actual diagnosis scenario of rotating machinery,”
    Mechanical Systems and Signal Processing, vol. 135, pp. 106344, 2020.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Zhongwei Zhang, Huaihai Chen, Shunming Li, and Zenghui An, “A novel unsupervised
    domain adaptation based on deep neural network and manifold regularization for
    mechanical fault diagnosis,” Measurement Science and Technology, vol. 31, no.
    8, pp. 085101, 2020.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Weiwei Qian, Shunming Li, Tong Yao, and Kun Xu, “Discriminative feature-based
    adaptive distribution alignment (dfada) for rotating machine fault diagnosis under
    variable working conditions,” Applied Soft Computing, vol. 99, pp. 106886, 2021.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Xiang Li, Wei Zhang, and Qian Ding, “A robust intelligent fault diagnosis
    method for rolling element bearings based on deep distance metric learning,” Neurocomputing,
    vol. 310, pp. 77–95, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Bin Yang, Yaguo Lei, Feng Jia, and Saibo Xing, “A transfer learning method
    for intelligent fault diagnosis from laboratory machines to real-case machines,”
    in 2018 International Conference on Sensing, Diagnostics, Prognostics, and Control
    (SDPC). IEEE, 2018, pp. 35–40.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Zenghui An, Shunming Li, Jinrui Wang, Yu Xin, and Kun Xu, “Generalization
    of deep neural network for bearing fault diagnosis under different working conditions
    using multiple kernel method,” Neurocomputing, vol. 352, pp. 42–53, 2019.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Jun Zhu, Nan Chen, and Changqing Shen, “A new deep transfer learning
    method for bearing fault diagnosis under different working conditions,” IEEE Sensors
    Journal, vol. 20, no. 15, pp. 8394–8402, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Changchang Che, Huawei Wang, Xiaomei Ni, and Qiang Fu, “Domain adaptive
    deep belief network for rolling bearing fault diagnosis,” Computers & Industrial
    Engineering, vol. 143, pp. 106427, 2020.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Te Han, Chao Liu, Wenguang Yang, and Dongxiang Jiang, “Deep transfer
    network with joint distribution adaptation: A new intelligent fault diagnosis
    framework for industry application,” ISA transactions, vol. 97, pp. 269–281, 2020.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Weiwei Qian, Shunming Li, Pengxing Yi, and Kaicheng Zhang, “A novel transfer
    learning method for robust fault diagnosis of rotating machines under variable
    working conditions,” Measurement, vol. 138, pp. 514–525, 2019.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Zhenghong Wu, Hongkai Jiang, Ke Zhao, and Xingqiu Li, “An adaptive deep
    transfer learning method for bearing fault diagnosis,” Measurement, vol. 151,
    pp. 107227, 2020.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Xincheng Cao, Binqiang Chen, and Nianyin Zeng, “A deep domain adaption
    model with multi-task networks for planetary gearbox fault diagnosis,” Neurocomputing,
    vol. 409, pp. 173–190, 2020.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
    Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky, “Domain-adversarial
    training of neural networks,” The Journal of Machine Learning Research, vol. 17,
    no. 1, pp. 2096–2030, 2016.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan, “Conditional
    adversarial domain adaptation,” in Advances in Neural Information Processing Systems,
    2018, pp. 1640–1650.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Bo Zhang, Wei Li, Jie Hao, Xiao-Li Li, and Meng Zhang, “Adversarial adaptive
    1-d convolutional neural networks for bearing fault diagnosis under varying working
    condition,” arXiv preprint arXiv:1805.00778, 2018.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Te Han, Chao Liu, Wenguang Yang, and Dongxiang Jiang, “A novel adversarial
    learning framework in deep convolutional neural network for intelligent diagnosis
    of mechanical faults,” Knowledge-Based Systems, vol. 165, pp. 474–487, 2019.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Liang Guo, Yaguo Lei, Saibo Xing, Tao Yan, and Naipeng Li, “Deep convolutional
    transfer learning network: A new method for intelligent fault diagnosis of machines
    with unlabeled data,” IEEE Transactions on Industrial Electronics, vol. 66, no.
    9, pp. 7316–7325, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Qin Wang, Gabriel Michau, and Olga Fink, “Domain adaptive transfer learning
    for fault diagnosis,” in 2019 Prognostics and System Health Management Conference
    (PHM-Paris). IEEE, 2019, pp. 279–285.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Zhuyun Chen, Guolin He, Jipu Li, Yixiao Liao, Konstantinos Gryllias,
    and Weihua Li, “Domain adversarial transfer network for cross-domain fault diagnosis
    of rotary machinery,” IEEE Transactions on Instrumentation and Measurement, vol.
    69, no. 11, pp. 8702–8712, 2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Lei Zou, Yang Li, and Feiyun Xu, “An adversarial denoising convolutional
    neural network for fault diagnosis of rotating machinery under noisy environment
    and limited sample size case,” Neurocomputing, vol. 407, pp. 105–120, 2020.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Qi Li, Changqing Shen, Liang Chen, and Zhongkui Zhu, “Knowledge mapping-based
    adversarial domain adaptation: A novel fault diagnosis method with high generalizability
    under variable working conditions,” Mechanical Systems and Signal Processing,
    vol. 147, pp. 107095, 2021.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Tianfu Li, Zhibin Zhao, Chuang Sun, Ruqiang Yan, and Xuefeng Chen, “Domain
    adversarial graph convolutional network for fault diagnosis under variable working
    conditions,” IEEE Transactions on Instrumentation and Measurement, vol. 70, pp.
    1–10, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Jinyang Jiao, Ming Zhao, and Jing Lin, “Unsupervised adversarial adaptation
    network for intelligent fault diagnosis,” IEEE Transactions on Industrial Electronics,
    vol. 67, no. 11, pp. 9904–9913, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Kun Yu, Hongzheng Han, Qiang Fu, Hui Ma, and Jin Zeng, “Symmetric co-training
    based unsupervised domain adaptation approach for intelligent fault diagnosis
    of rolling bearing,” Measurement Science and Technology, vol. 31, no. 11, pp.
    115008, 2020.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Jinyang Jiao, Jing Lin, Ming Zhao, and Kaixuan Liang, “Double-level adversarial
    domain adaptation network for intelligent fault diagnosis,” Knowledge-Based Systems,
    vol. 205, pp. 106236, 2020.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Xiang Li, Wei Zhang, Nan-Xi Xu, and Qian Ding, “Deep learning-based machinery
    fault diagnostics with domain adaptation across sensors at different places,”
    IEEE Transactions on Industrial Electronics, vol. 67, no. 8, pp. 6785–6794, 2019.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Sixiang Jia, Jinrui Wang, Baokun Han, Guowei Zhang, Xiaoyu Wang, and
    Jingtao He, “A novel transfer learning method for fault diagnosis using maximum
    classifier discrepancy with marginal probability distribution adaptation,” IEEE
    Access, vol. 8, pp. 71475–71485, 2020.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Yongchao Zhang, Zhaohui Ren, and Shihua Zhou, “A new deep convolutional
    domain adaptation network for bearing fault diagnosis under different working
    conditions,” Shock and Vibration, vol. 2020, 2020.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Jinyang Jiao, Ming Zhao, Jing Lin, and Kaixuan Liang, “Residual joint
    adaptation adversarial network for intelligent transfer fault diagnosis,” Mechanical
    Systems and Signal Processing, vol. 145, pp. 106962, 2020.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yibin Li, Yan Song, Lei Jia, Shengyao Gao, Qiqiang Li, and Meikang Qiu,
    “Intelligent fault diagnosis by fusing domain adversarial training and maximum
    mean discrepancy via ensemble learning,” IEEE Transactions on Industrial Informatics,
    vol. 17, no. 4, pp. 2833–2841, 2021.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Yi Qin, Xin Wang, Quan Qian, Huayan Pu, and Jun Luo, “Multiscale transfer
    voting mechanism: A new strategy for domain adaption,” IEEE Transactions on Industrial
    Informatics, vol. 17, no. 10, pp. 7103–7113, 2021.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Yi Qin, Qunwang Yao, Yi Wang, and Yongfang Mao, “Parameter sharing adversarial
    domain adaptation networks for fault transfer diagnosis of planetary gearboxes,”
    Mechanical Systems and Signal Processing, vol. 160, pp. 107936, 2021.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Qunwang Yao, Yi Qin, Xin Wang, and Quan Qian, “Multiscale domain adaption
    models and their application in fault transfer diagnosis of planetary gearboxes,”
    Engineering Applications of Artificial Intelligence, vol. 104, pp. 104383, 2021.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Cheng Cheng, Beitong Zhou, Guijun Ma, Dongrui Wu, and Ye Yuan, “Wasserstein
    distance based deep adversarial transfer learning for intelligent fault diagnosis
    with unlabeled or insufficient labeled data,” Neurocomputing, vol. 409, pp. 35–45,
    2020.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Ming Zhang, Duo Wang, Weining Lu, Jun Yang, Zhiheng Li, and Bin Liang,
    “A deep transfer model with wasserstein distance guided multi-adversarial networks
    for bearing fault diagnosis under different working conditions,” IEEE Access,
    vol. 7, pp. 65303–65318, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Xiaodong Wang and Feng Liu, “Triplet loss guided adversarial domain adaptation
    for bearing fault diagnosis,” Sensors, vol. 20, no. 1, pp. 320, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D She, N Peng, M Jia, and MG Pecht, “Wasserstein distance based deep
    multi-feature adversarial transfer diagnosis approach under variable working conditions,”
    Journal of Instrumentation, vol. 15, no. 06, pp. P06002, 2020.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Xiaolei Yu, Zhibin Zhao, Xingwu Zhang, Chuang Sun, Baogui Gong, Ruqiang
    Yan, and Xuefeng Chen, “Conditional adversarial domain adaptation with discrimination
    embedding for locomotive fault diagnosis,” IEEE Transactions on Instrumentation
    and Measurement, vol. 70, pp. 1–12, 2020.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Feng Li, Tuojiang Tang, Baoping Tang, and Qiyuan He, “Deep convolution
    domain-adversarial transfer learning for fault diagnosis of rolling bearings,”
    Measurement, vol. 169, pp. 108339, 2021.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Yuan Xie and Tao Zhang, “A transfer learning strategy for rotation machinery
    fault diagnosis based on cycle-consistent generative adversarial networks,” in
    2018 Chinese Automation Congress (CAC). IEEE, 2018, pp. 1309–1313.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Sonal Dixit, Nishchal K Verma, and AK Ghosh, “Intelligent fault diagnosis
    of rotary machines: Conditional auxiliary classifier gan coupled with meta learning
    using limited data,” IEEE Transactions on Instrumentation and Measurement, vol.
    70, pp. 1–11, 2021.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan, “Partial
    transfer learning with selective adversarial networks,” in Proceedings of the
    IEEE conference on computer vision and pattern recognition, 2018, pp. 2724–2732.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang, “Partial adversarial
    domain adaptation,” in Proceedings of the European Conference on Computer Vision
    (ECCV), 2018, pp. 135–150.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Jinyang Jiao, Ming Zhao, Jing Lin, and Chuancang Ding, “Classifier inconsistency-based
    domain adaptation network for partial transfer intelligent diagnosis,” IEEE Transactions
    on Industrial Informatics, vol. 16, no. 9, pp. 5965–5974, 2019.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Weihua Li, Zhuyun Chen, and Guolin He, “A novel weighted adversarial
    transfer network for partial domain fault diagnosis of machinery,” IEEE Transactions
    on Industrial Informatics, vol. 17, no. 3, pp. 1753–1762, 2020.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Xiang Li and Wei Zhang, “Deep learning-based partial domain adaptation
    method on intelligent machinery fault diagnostics,” IEEE Transactions on Industrial
    Electronics, vol. 68, no. 5, pp. 4351–4361, 2020.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Xiang Li, Wei Zhang, Hui Ma, Zhong Luo, and Xu Li, “Partial transfer
    learning in machinery cross-domain fault diagnostics using class-weighted adversarial
    networks,” Neural Networks, vol. 129, pp. 313–322, 2020.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Qin Wang, Gabriel Michau, and Olga Fink, “Missing-class-robust domain
    adaptation by unilateral alignment,” IEEE Transactions on Industrial Electronics,
    vol. 68, no. 1, pp. 663–671, 2020.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Yafei Deng, Delin Huang, Shichang Du, Guilong Li, Chen Zhao, and Jun
    Lv, “A double-layer attention based adversarial network for partial transfer learning
    in machinery fault diagnosis,” Computers in Industry, vol. 127, pp. 103399, 2021.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Bin Yang, Chi-Guhn Lee, Yaguo Lei, Naipeng Li, and Na Lu, “Deep partial
    transfer learning network: A method to selectively transfer diagnostic knowledge
    across related machines,” Mechanical Systems and Signal Processing, vol. 156,
    pp. 107618, 2021.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada,
    “Open set domain adaptation by backpropagation,” in Proceedings of the European
    Conference on Computer Vision (ECCV), 2018, pp. 153–168.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Jipu Li, Ruyi Huang, Guolin He, Yixiao Liao, Zhen Wang, and Weihua Li,
    “A two-stage transfer adversarial network for intelligent fault diagnosis of rotating
    machinery with multiple new faults,” IEEE/ASME Transactions on Mechatronics, vol.
    26, no. 3, pp. 1591–1601, 2020.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Jipu Li, Ruyi Huang, Guolin He, Shuhua Wang, Guanghui Li, and Weihua
    Li, “A deep adversarial transfer learning network for machinery emerging fault
    detection,” IEEE Sensors Journal, vol. 20, no. 15, pp. 8413–8422, 2020.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Wei Zhang, Xiang Li, Hui Ma, Zhong Luo, and Xu Li, “Open set domain adaptation
    in machinery fault diagnostics using instance-level weighted adversarial learning,”
    IEEE Transactions on Industrial Informatics, vol. 17, no. 11, pp. 7445–7455, 2021.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I
    Jordan, “Universal domain adaptation,” in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 2720–2729.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Wei Zhang, Xiang Li, Hui Ma, Zhong Luo, and Xu Li, “Universal domain
    adaptation in fault diagnostics with hybrid weighted deep adversarial learning,”
    IEEE Transactions on Industrial Informatics, vol. 17, no. 12, pp. 7957–7967, 2021.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Xiaolei Yu, Zhibin Zhao, Xingwu Zhang, Qiyang Zhang, Yilong Liu, Chuang
    Sun, and Xuefeng Chen, “Deep learning-based open set fault diagnosis by extreme
    value theory,” IEEE Transactions on Industrial Informatics, pp. 1–1, 2021.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Yong Dai, Jian Liu, Xiancong Ren, and Zenglin Xu, “Adversarial training
    based multi-source unsupervised domain adaptation for sentiment analysis,” in
    Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34,
    pp. 7618–7625.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu, Yaxian Li, Zhichao
    Song, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer, “Multi-source distilling
    domain adaptation,” in Proceedings of the AAAI Conference on Artificial Intelligence,
    2020, vol. 34, pp. 12975–12983.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Fei Shen, Yun Hui, Ruqiang Yan, Chuang Sun, and Jiawen Xu, “A new penalty
    domain selection machine enabled transfer learning for gearbox fault recognition,”
    IEEE Transactions on Industrial Electronics, vol. 67, no. 10, pp. 8743–8754, 2020.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Jun Zhu, Nan Chen, and Changqing Shen, “A new multiple source domain
    adaptation fault diagnosis method between different rotating machines,” IEEE Transactions
    on Industrial Informatics, vol. 17, no. 7, pp. 4788–4797, 2021.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Behnoush Rezaeianjouybari and Yi Shang, “A novel deep multi-source domain
    adaptation framework for bearing fault diagnosis based on feature-level and task-specific
    distribution alignment,” Measurement, vol. 178, pp. 109359, 2021.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Yongchao Zhang, Zhaohui Ren, Shihua Zhou, and Tianzhuang Yu, “Adversarial
    domain adaptation with classifier alignment for cross-domain intelligent fault
    diagnosis of multiple source domains,” Measurement Science and Technology, vol.
    32, no. 3, pp. 035102, dec 2020.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Ya He, Minghui Hu, Kun Feng, and Zhinong Jiang, “An intelligent fault
    diagnosis scheme using transferred samples for intershaft bearings under variable
    working conditions,” IEEE Access, vol. 8, pp. 203058–203069, 2020.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Dongdong Wei, Te Han, Fulei Chu, and Ming Jian Zuo, “Weighted domain
    adaptation networks for machinery fault diagnosis,” Mechanical Systems and Signal
    Processing, vol. 158, pp. 107744, 2021.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Xiang Li, Wei Zhang, Qian Ding, and Xu Li, “Diagnosing rotating machines
    with weakly supervised data using deep transfer learning,” IEEE Transactions on
    Industrial Informatics, vol. 16, no. 3, pp. 1688–1697, 2020.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Ziling Huang, Zihao Lei, Guangrui Wen, Xin Huang, Haoxuan Zhou, Ruqiang
    Yan, and Xuefeng Chen, “A multi-source dense adaptation adversarial network for
    fault diagnosis of machinery,” IEEE Transactions on Industrial Electronics, 2021.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot, “Domain generalization
    with adversarial feature learning,” in 2018 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2018, pp. 5400–5409.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Xiang Li, Wei Zhang, Hui Ma, Zhong Luo, and Xu Li, “Domain generalization
    in rotating machinery fault diagnostics using deep neural networks,” Neurocomputing,
    vol. 403, pp. 409–420, 2020.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang,
    and Dacheng Tao, “Deep domain generalization via conditional invariant adversarial
    networks,” in Proceedings of the European Conference on Computer Vision (ECCV),
    2018, pp. 624–639.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Huailiang Zheng, Yuantao Yang, Jiancheng Yin, Yuqing Li, Rixin Wang,
    and Minqiang Xu, “Deep domain generalization combining a priori diagnosis knowledge
    toward cross-domain fault diagnosis of rolling bearing,” IEEE Transactions on
    Instrumentation and Measurement, vol. 70, pp. 1–11, 2021.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Yixiao Liao, Ruyi Huang, Jipu Li, Zhuyun Chen, and Weihua Li, “Deep semisupervised
    domain generalization network for rotary machinery fault diagnosis under variable
    speed,” IEEE Transactions on Instrumentation and Measurement, vol. 69, no. 10,
    pp. 8064–8075, 2020.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Yuantao Yang, Jiancheng Yin, Huailiang Zheng, Yuqing Li, Minqiang Xu,
    and Yushu Chen, “Learn generalization feature via convolutional neural network:
    A fault diagnosis scheme toward unseen operating conditions,” IEEE Access, vol.
    8, pp. 91103–91115, 2020.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Qiyang Zhang, Zhibin Zhao, Xingwu Zhang, Yilong Liu, Chuang Sun, Ming
    Li, Shibin Wang, and Xuefeng Chen, “Conditional adversarial domain generalization
    with a single discriminator for bearing fault diagnosis,” IEEE Transactions on
    Instrumentation and Measurement, vol. 70, pp. 1–15, 2021.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Te Han, Yan-Fu Li, and Min Qian, “A hybrid generalization network for
    intelligent fault diagnosis of rotating machinery under unseen working conditions,”
    IEEE Transactions on Instrumentation and Measurement, 2021.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Case Western Reserve University, “Case Western Reserve University (CWRU)
    Bearing Data Center, [Online],” Available: [https://csegroups.case.edu/bearingdatacenter/pages/download-data-file/](https://csegroups.case.edu/bearingdatacenter/pages/download-data-file/),
    accessed on August 2019.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Christian Lessmeier, James Kuria Kimotho, Detmar Zimmer, and Walter Sextro,
    “Condition monitoring of bearing damage in electromechanical drive systems by
    using motor current signals of electric motors: A benchmark data set for data-driven
    classification,” in Proceedings of the European conference of the prognostics
    and health management society, 2016, pp. 05–08.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Christian Lessmeier, “et al. KAt-DataCenter, Chair of Design and Drive
    Technology, Paderborn University,” Available: [https://mb.uni-paderborn.de/kat/forschung/datacenter/bearing-datacenter/](https://mb.uni-paderborn.de/kat/forschung/datacenter/bearing-datacenter/),
    accessed on August 2019.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Ke Li, “School of Mechanical Engineering, Jiangnan University,” Available:
    [http://mad-net.org:8765/explore.html?t=0.5831516555847212.](http://mad-net.org:8765/explore.html?t=0.5831516555847212.),
    accessed on August 2019.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Ke Li, Xueliang Ping, Huaqing Wang, Peng Chen, and Yi Cao, “Sequential
    fuzzy diagnosis method for motor roller bearing in variable operating conditions
    based on vibration analysis,” Sensors, vol. 13, no. 6, pp. 8013–8041, 2013.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] PHMSociety, “PHM09 Data Challenge,” Available: [https://www.phmsociety.org/competition/PHM/09/apparatus](https://www.phmsociety.org/competition/PHM/09/apparatus),
    accessed on August 2019.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Siyu Shao, Stephen McAleer, Ruqiang Yan, and Pierre Baldi, “Mechanical
    dataset,” Available: [http://mlmechanics.ics.uci.edu./](http://mlmechanics.ics.uci.edu./),
    accessed on August 2019.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang,
    “Separate to adapt: Open set domain adaptation via progressive separation,” in
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2019, pp. 2927–2936.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang, “Learning to detect
    open classes for universal domain adaptation,” in European Conference on Computer
    Vision. Springer, 2020, pp. 567–583.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Jianrong Wang, Kai Fan, and Wanshan Wang, “Integration of fuzzy ahp and
    fpp with topsis methodology for aeroengine health assessment,” Expert Systems
    with Applications, vol. 37, no. 12, pp. 8516–8526, 2010.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Yi-Hai He, Lin-Bo Wang, Zhen-Zhen He, and Min Xie, “A fuzzy topsis and
    rough set based approach for mechanism analysis of product infant failure,” Engineering
    Applications of Artificial Intelligence, vol. 47, pp. 25–37, 2016.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Wen Jiang, Meijuan Wang, Xinyang Deng, and Linfeng Gou, “Fault diagnosis
    based on topsis method with manhattan distance,” Advances in Mechanical Engineering,
    vol. 11, no. 3, pp. 1687814019833279, 2019.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Xavier Glorot, Antoine Bordes, and Yoshua Bengio, “Domain adaptation
    for large-scale sentiment classification: A deep learning approach,” in Proceedings
    of the 28th international conference on machine learning (ICML-11), 2011, pp.
    513–520.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson, “How transferable
    are features in deep neural networks?,” in Advances in neural information processing
    systems, 2014, pp. 3320–3328.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Matthew D Zeiler and Rob Fergus, “Visualizing and understanding convolutional
    networks,” in European conference on computer vision. Springer, 2014, pp. 818–833.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna
    Vedantam, Devi Parikh, and Dhruv Batra, “Grad-cam: Visual explanations from deep
    networks via gradient-based localization,” in Proceedings of the IEEE International
    Conference on Computer Vision, 2017, pp. 618–626.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin, “Attention is all you need,”
    in Advances in neural information processing systems, 2017, pp. 5998–6008.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Mengmeng Jing, Jingjing Li, Lei Zhu, Zhengming Ding, Ke Lu, and Yang
    Yang, “Balanced open set domain adaptation via centroid alignment,” in Proceedings
    of the AAAI Conference on Artificial Intelligence, 2021, vol. 35, pp. 8013–8020.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Jingyao Wu, Zhibin Zhao, Chuang Sun, Ruqiang Yan, and Xuefeng Chen, “Few-shot
    transfer learning for intelligent fault diagnosis of machine,” Measurement, vol.
    166, pp. 108202, 2020.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang, “A
    secure federated transfer learning framework,” IEEE Intelligent Systems, vol.
    35, no. 4, pp. 70–82, 2020.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Wei Zhang and Xiang Li, “Federated transfer learning for intelligent
    fault diagnostics using deep adversarial networks with data privacy,” IEEE/ASME
    Transactions on Mechatronics, 2021.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Yarin Gal and Zoubin Ghahramani, “Dropout as a bayesian approximation:
    Representing model uncertainty in deep learning,” in international conference
    on machine learning. PMLR, 2016, pp. 1050–1059.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell, “Simple
    and scalable predictive uncertainty estimation using deep ensembles,” in Proceedings
    of the 31st International Conference on Neural Information Processing Systems,
    2017, pp. 6405–6416.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Zhedong Zheng and Yi Yang, “Rectifying pseudo label learning via uncertainty
    estimation for domain adaptive semantic segmentation,” International Journal of
    Computer Vision, vol. 129, no. 4, pp. 1106–1120, 2021.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See pages 1-8 of [AppendixA/EvaluationResults.pdf](AppendixA/EvaluationResults.pdf)
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
