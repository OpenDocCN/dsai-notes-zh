- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:58:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:58:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2010.10563] A Survey on Deep Learning and Explainability for Automatic Report
    Generation from Medical Images'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2010.10563] 关于深度学习和可解释性的调查：医学图像自动报告生成'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2010.10563](https://ar5iv.labs.arxiv.org/html/2010.10563)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2010.10563](https://ar5iv.labs.arxiv.org/html/2010.10563)
- en: A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于深度学习和可解释性的调查：医学图像自动报告生成
- en: Pablo Messina [pamessina@uc.cl](mailto:pamessina@uc.cl) ,  Pablo Pino [pdpino@uc.cl](mailto:pdpino@uc.cl)
    ,  Denis Parra [dparra@ing.puc.cl](mailto:dparra@ing.puc.cl) ,  Alvaro Soto [asoto@uc.cl](mailto:asoto@uc.cl)
    Computer Science Department, Pontificia Universidad Católica de ChileVicuña Mackenna
    48607820436SantiagoChile ,  Cecilia Besa [besacecilia@gmail.com](mailto:besacecilia@gmail.com)
    ,  Sergio Uribe [suribe@uc.cl](mailto:suribe@uc.cl) ,  Marcelo Andía [meandia@uc.cl](mailto:meandia@uc.cl)
    Department of Radiology, School of Medicine, Pontificia Universidad Católica de
    ChileAvda. Libertador Bernando O’Higgins 3408320000SantiagoChile ,  Cristian Tejos
    [ctejos@puc.cl](mailto:ctejos@puc.cl) Department of Electrical Engineering, Pontificia
    Universidad Católica de ChileVicuña Mackenna 48607820436SantiagoChile ,  Claudia
    Prieto [cdprieto@gmail.com](mailto:cdprieto@gmail.com) School of Biomedical Engineering
    and Imaging Sciences, King’s College London, St Thomas’ HospitalLambeth Palace
    RdSE1 7EHLondonUK  and  Daniel Capurro [dcapurro@unimelb.edu.au](mailto:dcapurro@unimelb.edu.au)
    School of Computing and Information Systems, The University of MelbourneLevel
    8, Doug McDonell Building3010MelbourneAustralia(2020)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pablo Messina [pamessina@uc.cl](mailto:pamessina@uc.cl) ,  Pablo Pino [pdpino@uc.cl](mailto:pdpino@uc.cl)
    ,  Denis Parra [dparra@ing.puc.cl](mailto:dparra@ing.puc.cl) ,  Alvaro Soto [asoto@uc.cl](mailto:asoto@uc.cl)
    计算机科学系，智利天主教大学Vicuña Mackenna 48607820436SantiagoChile ,  Cecilia Besa [besacecilia@gmail.com](mailto:besacecilia@gmail.com)
    ,  Sergio Uribe [suribe@uc.cl](mailto:suribe@uc.cl) ,  Marcelo Andía [meandia@uc.cl](mailto:meandia@uc.cl)
    放射学系，医学学院，智利天主教大学Avda. Libertador Bernando O’Higgins 3408320000SantiagoChile ,  Cristian
    Tejos [ctejos@puc.cl](mailto:ctejos@puc.cl) 电气工程系，智利天主教大学Vicuña Mackenna 48607820436SantiagoChile
    ,  Claudia Prieto [cdprieto@gmail.com](mailto:cdprieto@gmail.com) 生物医学工程与成像科学学院，伦敦国王学院，圣托马斯医院Lambeth
    Palace RdSE1 7EHLondonUK 以及 Daniel Capurro [dcapurro@unimelb.edu.au](mailto:dcapurro@unimelb.edu.au)
    计算与信息系统学院，墨尔本大学Level 8, Doug McDonell Building3010MelbourneAustralia（2020）
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Every year physicians face an increasing demand of image-based diagnosis from
    patients, a problem that can be addressed with recent artificial intelligence
    methods. In this context, we survey works in the area of automatic report generation
    from medical images, with emphasis on methods using deep neural networks, with
    respect to: (1) Datasets, (2) Architecture Design, (3) Explainability and (4)
    Evaluation Metrics. Our survey identifies interesting developments, but also remaining
    challenges. Among them, the current evaluation of generated reports is especially
    weak, since it mostly relies on traditional Natural Language Processing (NLP)
    metrics, which do not accurately capture medical correctness.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，医生面临来自患者的图像诊断需求不断增加的问题，这一问题可以通过近期的人工智能方法解决。在这种背景下，我们调查了医学图像自动报告生成领域的工作，重点关注使用深度神经网络的方法，包括：（1）数据集，（2）架构设计，（3）可解释性和（4）评估指标。我们的调查识别了有趣的发展，但也存在挑战。其中，目前生成报告的评估尤其薄弱，因为它主要依赖于传统的自然语言处理（NLP）指标，这些指标不能准确捕捉医学正确性。
- en: 'medical report generation, medical image captioning, natural language report,
    medical images, deep learning, explainable artificial intelligence^†^†copyright:
    acmcopyright^†^†journalyear: 2020^†^†doi: 10.1145/1122445.1122456^†^†ccs: Computing
    methodologies Computer vision^†^†ccs: Computing methodologies Natural language
    generation^†^†ccs: Applied computing Health care information systems^†^†ccs: Computing
    methodologies Neural networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 医学报告生成、医学图像说明、自然语言报告、医学图像、深度学习、可解释人工智能^†^†版权：acmcopyright^†^†期刊年份：2020^†^†doi：10.1145/1122445.1122456^†^†ccs：计算方法
    计算机视觉^†^†ccs：计算方法 自然语言生成^†^†ccs：应用计算 健康护理信息系统^†^†ccs：计算方法 神经网络
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The rapid and successful development of deep learning in research fields such
    as Computer Vision (Khan et al., [2020](#bib.bib75)) and Natural Language Processing
    (NLP) (Otter et al., [2020](#bib.bib102)) has found an important application area
    in healthcare, sustaining the promise of a future with more efficient and affordable
    medical care. Research over the last five years shows a clear improvement in computer-aided
    detection (CAD), specifically in disease prediction from medical images (Wang
    et al., [2016](#bib.bib143); Rajpurkar et al., [2017](#bib.bib111); Gale et al.,
    [2017](#bib.bib38); Tsai and Tao, [2019](#bib.bib137); Hwang et al., [2019](#bib.bib63))
    as well as from Electronic Health Records (EHR) (Shickel et al., [2017](#bib.bib120)),
    by using deep neural networks (DNN) and treating the problem as supervised classification
    or segmentation tasks. Recently, Topol (Topol, [2019](#bib.bib136)) indicates
    that the need for diagnosis and reporting from image-based examinations far exceeds
    the current medical capacity of physicians in the US. This situation promotes
    the development of automatic image-based diagnosis as well as automatic reporting.
    Furthermore, the lack of specialist physicians is even more critical in resource-limited
    countries (Rosman et al., [2019](#bib.bib118)), and therefore the expected impacts
    of this technology would become even more relevant.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉（Khan et al., [2020](#bib.bib75)）和自然语言处理（NLP）（Otter et al., [2020](#bib.bib102)）等研究领域的快速成功发展，在医疗保健中找到了重要的应用领域，维持了未来更高效、更经济医疗护理的承诺。过去五年的研究表明，计算机辅助检测（CAD）在医学图像（Wang
    et al., [2016](#bib.bib143); Rajpurkar et al., [2017](#bib.bib111); Gale et al.,
    [2017](#bib.bib38); Tsai and Tao, [2019](#bib.bib137); Hwang et al., [2019](#bib.bib63)）以及电子健康记录（EHR）（Shickel
    et al., [2017](#bib.bib120)）中的疾病预测方面有了明显改善，这得益于深度神经网络（DNN）和将问题视作监督分类或分割任务的处理方式。最近，Topol（Topol,
    [2019](#bib.bib136)）指出，从基于图像的检查中对诊断和报告的需求远远超出了美国医生的当前医疗能力。这种情况促进了自动化图像诊断和自动报告的发展。此外，资源有限的国家中缺乏专科医生的情况更加严重（Rosman
    et al., [2019](#bib.bib118)），因此这种技术的预期影响将变得更加重要。
- en: However, the elaboration of high-quality medical reports from medical images,
    such as chest X-rays, computed tomography (CT) or magnetic resonance (MRI) scans,
    is a task that requires a trained radiologist with years of experience. In this
    context, deep learning (DL) combined with other Artificial Intelligence (AI) techniques
    appears as a viable and promising solution to alleviate the physician scarcity
    problem, by both automating the report generation process and enhancing radiologists’
    performance through assisted report-generation. AI is set to have a significant
    impact on the medical imaging market and, hence, how radiologists work, with the
    ultimate goal of better patient outcomes. The pace of research in this area is
    rapid, and to the best of our knowledge, previous surveys on this topic (Pavlopoulos
    et al., [2019](#bib.bib105); Allaouzi et al., [2018](#bib.bib7); Monshi et al.,
    [2020](#bib.bib98)) do not cover aspects of explainability (Gunning, [2017](#bib.bib47)),
    medical correctness and physician-centered evaluation. This article enhances these
    previous surveys by analyzing more than twenty additional works and datasets.
    Furthermore, unlike previous surveys, in this article we pay special attention
    to explainable AI (XAI). XAI is a set of methods and technologies, which will
    allow physicians to better understand the rationale behind automatic reports from
    black-box algorithms (Guidotti et al., [2018](#bib.bib46)), potentially increasing
    trust for their actual clinical use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从医学图像（如胸部X光、计算机断层扫描（CT）或磁共振成像（MRI））中详细编写高质量的医学报告是需要具有多年经验的放射科医师的任务。在这种背景下，深度学习（DL）结合其他人工智能（AI）技术，作为一种可行且有前景的解决方案，通过自动化报告生成过程和通过辅助报告生成提高放射科医师的表现，从而缓解医生稀缺的问题。人工智能有望对医学影像市场产生重大影响，从而影响放射科医师的工作，*最终目标是改善患者的结果*。该领域的研究进展迅速，据我们所知，之前的调查（Pavlopoulos
    et al., [2019](#bib.bib105); Allaouzi et al., [2018](#bib.bib7); Monshi et al.,
    [2020](#bib.bib98)）未涵盖解释性（Gunning, [2017](#bib.bib47)）、医学准确性和以医生为中心的评估等方面。本文通过分析超过二十项额外的研究和数据集，增强了这些之前的调查。此外，与之前的调查不同，本文特别关注解释性人工智能（XAI）。XAI是一组方法和技术，将使医生更好地理解黑箱算法自动报告背后的原理（Guidotti
    et al., [2018](#bib.bib46)），从而可能提高其实际临床使用的信任。
- en: 'Contribution. We summarize the state of research in automatic report generation
    from medical images. We perform an exhaustive review of the literature, consisting
    of 40 articles published in journals, conferences, and conference workshops proceedings.
    We first present an overview of the task (section [2](#S2 "2\. Task Overview ‣
    A Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images")), followed by the survey methodology for search and selection
    of papers (section [3](#S3 "3\. Survey Methodology: Search and selection of papers
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")), and the research questions driving this research (section
    [4](#S4 "4\. Research Questions ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")). We then analyze papers
    regarding four dimensions: Datasets used (image modalities and clinical conditions,
    in section [5.1](#S5.SS1 "5.1\. Datasets ‣ 5\. Analysis of papers reviewed ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images")), Model Design (standard practices, input and output, visual
    and language components, domain knowledge, auxiliary tasks, and optimization strategies,
    in section [5.2](#S5.SS2 "5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")), Explainability (section [5.3](#S5.SS3 "5.3\. Explainability
    ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")) and Evaluation Metrics
    (section [5.4](#S5.SS4 "5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). We also compare model performance of several articles
    (section [5.5](#S5.SS5 "5.5\. Comparison of papers’ performance ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images")), identifying unsolved challenges across
    all reviewed papers and proposing potential avenues for future research (section
    [6](#S6 "6\. Challenges and future work ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")). Lastly, we discuss the
    limitations of this work (section [7](#S7 "7\. Limitations ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images"))
    and offer the main conclusions (section [8](#S8 "8\. Conclusions ‣ A Survey on
    Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")). Our survey provides valuable insights to guide future research on automatic
    report generation from medical images.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。我们总结了从医学图像自动生成报告的研究现状。我们进行了详尽的文献回顾，包括在期刊、会议和会议研讨会论文集中发表的40篇文章。我们首先介绍任务概述（见[2](#S2
    "2\. 任务概述 ‣ 关于深度学习和自动生成医学报告的可解释性调查")），接着介绍文献搜索和选择的方法（见[3](#S3 "3\. 调查方法：文献搜索和选择
    ‣ 关于深度学习和自动生成医学报告的可解释性调查")），以及驱动本研究的研究问题（见[4](#S4 "4\. 研究问题 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）。然后，我们从四个维度分析文献：使用的数据集（图像模态和临床条件，见[5.1](#S5.SS1
    "5.1\. 数据集 ‣ 5\. 文献分析 ‣ 关于深度学习和自动生成医学报告的可解释性调查")），模型设计（标准实践、输入和输出、视觉和语言组件、领域知识、辅助任务和优化策略，见[5.2](#S5.SS2
    "5.2\. 模型设计 ‣ 5\. 文献分析 ‣ 关于深度学习和自动生成医学报告的可解释性调查")），可解释性（见[5.3](#S5.SS3 "5.3\.
    可解释性 ‣ 5\. 文献分析 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）以及评估指标（见[5.4](#S5.SS4 "5.4\. 评估指标 ‣
    5\. 文献分析 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）。我们还比较了几篇文章的模型性能（见[5.5](#S5.SS5 "5.5\. 文章性能比较
    ‣ 5\. 文献分析 ‣ 关于深度学习和自动生成医学报告的可解释性调查")），识别所有回顾论文中的未解决挑战，并提出未来研究的潜在方向（见[6](#S6 "6\.
    挑战与未来工作 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）。最后，我们讨论了本工作的局限性（见[7](#S7 "7\. 局限性 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）并给出了主要结论（见[8](#S8
    "8\. 结论 ‣ 关于深度学习和自动生成医学报告的可解释性调查")）。我们的调查为指导未来医学图像自动报告生成的研究提供了宝贵的见解。
- en: 2\. Task Overview
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 任务概述
- en: 'From a purely computational perspective, the following is the main task addressed
    by most articles analyzed in this survey: given as input one or more medical images
    of a patient, a text report is output that is as similar as possible to one generated
    by a radiologist. From a machine learning point of view, creating a system that
    performs such a task would require learning a generative model from instances
    of reports written by radiologists. Figure [1](#S2.F1 "Figure 1 ‣ 2\. Task Overview
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") presents one example of such a report, taken from the IU
    X-ray dataset (Demner-Fushman et al., [2015](#bib.bib29)). We see two input X-ray
    images (frontal and lateral), and below them some annotations (Tags) –some manually
    annotated by a radiologist and others automatically annotated–, and on the right
    side the report with four different sections (comparison, indication, findings,
    and impression). If we consider the clinical workflow of generating a medical
    imaging report, several aspects should be taken into account before diving into
    a concrete implementation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从纯计算的角度来看，以下是本调查中大多数分析文章所解决的主要任务：给定一个或多个患者的医学图像作为输入，输出与放射科医师生成的报告尽可能相似的文本报告。从机器学习的角度来看，创建一个执行此任务的系统将需要从放射科医师撰写的报告实例中学习生成模型。图
    [1](#S2.F1 "图 1 ‣ 2\. 任务概述 ‣ 深度学习与可解释性在医学图像自动报告生成中的应用调查") 展示了一个这样的报告示例，取自 IU X
    射线数据集（Demner-Fushman 等，[2015](#bib.bib29)）。我们看到两个输入的 X 射线图像（正面和侧面），以及下面的一些注释（标签）——一些由放射科医师手动标注，其他则自动标注——右侧是包含四个不同部分（比较、指征、发现和印象）的报告。如果我们考虑生成医学影像报告的临床工作流程，应该在深入具体实现之前考虑几个方面。
- en: '| ![Refer to caption](img/068cba35c95ffc1608118132f9fc7dd2.png)![Refer to caption](img/068d1260412783259271a6d95793526a.png)
    Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified
    granuloma  | Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male,
    chest pain. Findings: The cardiomediastinal silhouette is within normal limits
    for size and contour. The lungs are normally inflated without evidence of focal
    airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma
    within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary
    process. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![参考标题](img/068cba35c95ffc1608118132f9fc7dd2.png)![参考标题](img/068d1260412783259271a6d95793526a.png)
    手动标签：钙化性肉芽肿/肺/上叶/右侧 自动标签：钙化性肉芽肿  | 比较：胸部放射照片 XXXX。指征：XXXX岁男性，胸痛。发现：心纵隔轮廓在大小和轮廓上均在正常范围内。肺部正常充气，没有局部气腔病变、胸腔积液或气胸的证据。右上肺稳定的钙化性肉芽肿。无急性骨骼异常。印象：无急性心肺过程。
    |'
- en: Figure 1\. Example from the IU X-ray dataset, frontal and lateral chest x-rays
    from a patient, alongside the natural language report and the annotated tags.
    XXXX is used for anonimization of the report.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 来自 IU X 射线数据集的示例，患者的正面和侧面胸部 X 射线图像，旁边是自然语言报告和注释标签。XXXX 用于报告的匿名化。
- en: \Description
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Frontal and lateral chest x-rays, manually and automatically annotated tags,
    and a written report with four sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正面和侧面胸部 X 射线，手动和自动标注的标签，以及包含四个部分的书面报告。
- en: The first aspect is considering additional patient information in the process
    of report generation. Most of the time, the physician asking for medical imaging
    is the primary care physician or a medical specialist. This implies that when
    radiologists write a report, they generally have patient-relevant clinical information,
    usually provided in the section Indication as shown in Figure [1](#S2.F1 "Figure
    1 ‣ 2\. Task Overview ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"). Also, the Comparison section can provide
    information of a serial follow-up procedure, to evaluate the evolution of a patient
    over time (e.g., aneurysm, congenital heart disease). Then, one decision can be
    whether or not to use these Indication and Comparison data to generate the sections
    Findings, Impression, or both of them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方面是考虑报告生成过程中的额外患者信息。大多数情况下，要求医学影像的医生是初级保健医生或医疗专家。这意味着，当放射科医师撰写报告时，他们通常会拥有患者相关的临床信息，通常在图
    [1](#S2.F1 "图 1 ‣ 2\. 任务概述 ‣ 深度学习与可解释性在医学图像自动报告生成中的应用调查") 中显示的指征部分提供。此外，比较部分可以提供连续跟踪程序的信息，以评估患者随时间的变化（例如，动脉瘤、先天性心脏病）。然后，一个决定是是否使用这些指征和比较数据来生成发现、印象，或两者。
- en: Second, the model for report generation should consider the diversity on medical
    images as well as body regions and conditions. There are several types of medical
    images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for
    text report generation that deals with only one type of input medical image might
    not solve it for other types. Also, ideally, a model should be able to generate
    reports from different parts of the human anatomy and diverse medical conditions.
    To adequately achieve this task, different body regions must have a balanced and
    sizable training set. Many works surveyed in this article focus on one specific
    part of the body and particular illnesses which limits the applicability of these
    methods to generalize to all possible diagnosis tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，用于报告生成的模型应考虑医学图像以及身体部位和状况的多样性。医学图像有多种类型，如 X 射线、CT、MRI、PET 和 SPECT。这意味着处理单一类型医学图像的文本报告生成模型可能无法适用于其他类型。此外，理想情况下，模型应能够从人体不同部位和各种医学状况生成报告。为了充分完成这一任务，必须有一个平衡且规模适当的训练集。本文调查的许多研究集中在身体的一个特定部位和特定疾病上，这限制了这些方法在所有可能的诊断任务中的泛化能力。
- en: Lastly, even if an AI system has perfect report generation accuracy, we might
    wonder if we can trust a machine in such a critical domain. One of the reasons
    for preferring a radiologist rather than an automated, highly accurate AI system
    is the chance of understanding the rationale behind the findings and impressions.
    In this sense, explainable AI (Gunning, [2017](#bib.bib47)) is of great importance
    in securing their adoption in a clinical setting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，即使 AI 系统在报告生成方面的准确性完美，我们仍然可能会怀疑是否可以在这样一个关键领域信任机器。选择放射科医生而不是自动化的高精度 AI 系统的原因之一是可以理解发现和印象背后的理由。从这个角度来看，可解释的
    AI (Gunning, [2017](#bib.bib47)) 在确保其在临床环境中的应用方面具有重要意义。
- en: '3\. Survey Methodology: Search and selection of papers'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 调查方法论：论文的搜索和选择
- en: 'To collect the papers reviewed, we performed three main steps: retrieval, selection,
    and exclusion. We further describe each step in the following paragraphs.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集所审阅的论文，我们执行了三个主要步骤：检索、选择和排除。我们将在接下来的段落中进一步描述每个步骤。
- en: 'Study retrieval. To retrieve the articles we used seven search engines, namely
    Google Scholar, PubMed, Scopus, ACM Digital Library, Web of Knowledge, IEEE Xplore
    and Springer; and two specific queries, plus other more relaxed queries, described
    in Table [1](#S3.T1 "Table 1 ‣ 3\. Survey Methodology: Search and selection of
    papers ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"). The relaxed queries returned articles already found with
    the two main queries. In this step we only considered journals, conference and
    conference workshop proceedings.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '研究检索。我们使用了七个搜索引擎进行文章检索，即 Google Scholar、PubMed、Scopus、ACM Digital Library、Web
    of Knowledge、IEEE Xplore 和 Springer；以及两个具体查询和其他一些较宽松的查询，详见表 [1](#S3.T1 "Table
    1 ‣ 3\. Survey Methodology: Search and selection of papers ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images")。这些宽松的查询返回了已通过两个主要查询找到的文章。在此步骤中，我们仅考虑了期刊、会议和会议研讨会论文集。'
- en: 'Study selection. Given the query results, a selection was performed applying
    inclusion criteria by reading title, abstract, and keywords of each paper. If
    there was uncertainty after reading these sections, we included it for revision
    and decided afterward if it should be excluded with exclusion criteria. The inclusion
    criteria were the following: at least a part of the study focused on report generation
    from medical images. The images can be from any kind (e.g., X-ray, MRI scans,
    CT scans), must be from humans, and may include one or more pathologies of any
    type¹¹1In practice, most datasets reviewed present one or more pathologies, since
    the detection of medical conditions is one of the main motivations of these studies..
    The report must be in natural language form, comprising at least one or more sentences,
    and must be automatically or semi-automatically generated by a computational system
    that employs a DL technique. Note that the method may contain steps that do not
    involve DL, such as rule-based decisions. The system must receive as input one
    or more medical images, and it also might receive additional input, such as patient
    clinical history. A semi-automated system may include a human in the process,
    expressly, by using additional input provided by the human. We included 45 works
    in total.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 选择研究。根据查询结果，通过阅读每篇论文的标题、摘要和关键词，应用了纳入标准进行选择。如果在阅读这些部分后存在不确定性，我们将其纳入修订，并在之后决定是否按排除标准排除。纳入标准如下：研究的至少一部分集中在医学图像的报告生成上。这些图像可以是任何类型的（例如，X射线、MRI扫描、CT扫描），必须来自人类，并且可能包括一种或多种类型的病理学¹¹1实际上，大多数审查过的数据集呈现了一种或多种病理学，因为医学状况的检测是这些研究的主要动机之一..
    报告必须是自然语言形式，包含至少一个或多个句子，且必须由使用DL技术的计算系统自动或半自动生成。注意，方法可能包含不涉及DL的步骤，如基于规则的决策。系统必须接收一个或多个医学图像作为输入，还可能接收额外输入，如患者的临床历史。半自动化系统可能包括人为因素，特别是通过使用人为提供的额外输入。我们共纳入了45篇工作。
- en: 'Study exclusion. After thoroughly reading each paper selected, we used two
    exclusion criteria to discard works that were not relevant for this survey. First,
    if the paper did not propose a specific computational approach to solve the report
    generation problem, for example, if presented a web application using existing
    methods, or presented an assessment of feasibility. Second, if the task being
    addressed was different from natural language report generation from medical images,
    for example, report summarizing, disease classification from images, medical image
    segmentation, or any others. We ruled out 5 works with these exclusion criteria,
    leaving a total of 40 papers. The amount of papers found in each step is detailed
    in Table [1](#S3.T1 "Table 1 ‣ 3\. Survey Methodology: Search and selection of
    papers ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '排除研究。在彻底阅读每篇选择的论文后，我们使用了两个排除标准来剔除与本调查无关的工作。首先，如果论文没有提出解决报告生成问题的特定计算方法，例如，如果呈现了一个使用现有方法的网页应用程序，或提出了可行性评估。其次，如果处理的任务与医学图像的自然语言报告生成不同，例如，报告总结、图像中的疾病分类、医学图像分割或其他任何任务。我们根据这些排除标准排除了5篇工作，剩下40篇论文。每一步找到的论文数量详见表[1](#S3.T1
    "Table 1 ‣ 3\. Survey Methodology: Search and selection of papers ‣ A Survey on
    Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")。'
- en: '| Query | Google Scholar | PubMed | Scopus | ACM | WoK | IEEE Xplore | Springer
    | Total |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 查询 | Google Scholar | PubMed | Scopus | ACM | WoK | IEEE Xplore | Springer
    | 总计 |'
- en: '| 1 | 32 | 1 | 19 | 2 | 9 | 7 | 13 | 34 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 1 | 19 | 2 | 9 | 7 | 13 | 34 |'
- en: '| 2 | 21 | 2 | 20 | 2 | 11 | 3 | 18 | 37 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 21 | 2 | 20 | 2 | 11 | 3 | 18 | 37 |'
- en: '| Selected with inclusion criteria (all queries) | 45 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 按纳入标准选择的论文（所有查询） | 45 |'
- en: '| Discarded with exclusion criteria (Hicks et al., [2018](#bib.bib57); Wang
    et al., [2019](#bib.bib146); Akazawa et al., [2019](#bib.bib6); Wu et al., [2018](#bib.bib148);
    Loveymi et al., [2020](#bib.bib94)) | 5 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 按排除标准剔除的论文（Hicks等，[2018](#bib.bib57)；Wang等，[2019](#bib.bib146)；Akazawa等，[2019](#bib.bib6)；Wu等，[2018](#bib.bib148)；Loveymi等，[2020](#bib.bib94)）
    | 5 |'
- en: '| Total articles (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Li et al.,
    [2018](#bib.bib86), [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Xue
    et al., [2018](#bib.bib155); Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90);
    Xiong et al., [2019](#bib.bib152); Singh et al., [2019](#bib.bib124); Maksoud
    et al., [2019](#bib.bib96); Gale et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132);
    Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Ma et al., [2018](#bib.bib95); Alsharid et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41);
    Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Biswal
    et al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Zeng et al., [2018](#bib.bib159);
    Harzig et al., [2019b](#bib.bib50); Xue and Huang, [2019](#bib.bib154); Sun et al.,
    [2019](#bib.bib129); Zhang et al., [2020b](#bib.bib163); Han et al., [2018](#bib.bib48);
    Li and Hong, [2019](#bib.bib88); Jing et al., [2019](#bib.bib68); Shin et al.,
    [2016](#bib.bib121); Hasan et al., [2018b](#bib.bib52); Shin et al., [2016](#bib.bib121);
    Zhang et al., [2017a](#bib.bib164); Zeng et al., [2018](#bib.bib159); Kisilev
    et al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99); Wu et al., [2017](#bib.bib149);
    Spinks and Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) | 40
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 总文章数 (Jing 等，[2018](#bib.bib69); Liu 等，[2019](#bib.bib93); Huang 等，[2019](#bib.bib62);
    Yuan 等，[2019](#bib.bib157); Li 等，[2018](#bib.bib86), [2019b](#bib.bib87); Wang
    等，[2018](#bib.bib145); Xue 等，[2018](#bib.bib155); Zhang 等，[2017a](#bib.bib164);
    Li 等，[2019a](#bib.bib90); Xiong 等，[2019](#bib.bib152); Singh 等，[2019](#bib.bib124);
    Maksoud 等，[2019](#bib.bib96); Gale 等，[2019](#bib.bib39); Tian 等，[2018](#bib.bib132);
    Gu 等，[2019](#bib.bib45); Yin 等，[2019](#bib.bib156); Tian 等，[2019](#bib.bib133);
    Ma 等，[2018](#bib.bib95); Alsharid 等，[2019](#bib.bib8); Gasimova，[2019](#bib.bib41);
    Gajbhiye 等，[2020](#bib.bib37); Harzig 等，[2019a](#bib.bib49); Biswal 等，[2020](#bib.bib17);
    Xie 等，[2019](#bib.bib151); Zeng 等，[2018](#bib.bib159); Harzig 等，[2019b](#bib.bib50);
    Xue 和 Huang，[2019](#bib.bib154); Sun 等，[2019](#bib.bib129); Zhang 等，[2020b](#bib.bib163);
    Han 等，[2018](#bib.bib48); Li 和 Hong，[2019](#bib.bib88); Jing 等，[2019](#bib.bib68);
    Shin 等，[2016](#bib.bib121); Hasan 等，[2018b](#bib.bib52); Shin 等，[2016](#bib.bib121);
    Zhang 等，[2017a](#bib.bib164); Zeng 等，[2018](#bib.bib159); Kisilev 等，[2016](#bib.bib77);
    Moradi 等，[2016](#bib.bib99); Wu 等，[2017](#bib.bib149); Spinks 和 Moens，[2019](#bib.bib127);
    Zeng 等，[2020](#bib.bib158)) | 40 |'
- en: Table 1\. Papers found for each query and database, and included or discarded
    with different criteria. WoK stands for Web of Knowledge. In both queries, only
    papers from journal, conference or conference workshops proceedings were included.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 每个查询和数据库找到的论文，以及根据不同标准包括或排除的论文。WoK 代表知识网。在这两个查询中，仅包括了期刊、会议或会议研讨会论文集中的论文。
- en: 'Query 1: (medical OR medicine OR health) AND ”report generation” AND (images
    OR image).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '查询 1: (医学 OR 医疗 OR 健康) AND “报告生成” AND (图像 OR 图片)。'
- en: 'Query 2: (medical OR medicine OR health) AND (images OR image) AND (report
    OR diagnostic OR description OR caption) AND (generation OR automatic) in ABSTRACT.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '查询 2: (医学 OR 医疗 OR 健康) AND (图像 OR 图片) AND (报告 OR 诊断 OR 描述 OR 标注) AND (生成 OR
    自动) 在摘要中。'
- en: 'Relaxed queries: (medical report generation), (medical report image), (diagnostic
    captioning).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 放宽的查询： (医学报告生成)，(医学报告图像)，(诊断标注)。
- en: 4\. Research Questions
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 研究问题
- en: 'This survey aims to answer the following research questions regarding the task
    of natural language report generation from medical images:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在回答关于从医学图像生成自然语言报告的任务的以下研究问题：
- en: (1)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: What datasets are used in this area? What diseases and imaging techniques are
    considered?
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个领域使用了哪些数据集？考虑了哪些疾病和成像技术？
- en: (2)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: What deep learning methods are the most commonly employed?
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最常用的深度学习方法是什么？
- en: (3)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: What explainability or interpretability techniques are used?
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了哪些可解释性或解释技术？
- en: (4)
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: How are the proposed models evaluated? What metrics are used?
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出的模型如何进行评估？使用了哪些指标？
- en: (5)
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: How is the performance of the automatic methods? Which method can be considered
    state of the art or showing the best performance?
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动方法的性能如何？哪些方法可以被认为是最先进的或表现最佳的？
- en: (6)
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (6)
- en: What are the main unsolved challenges? What are the potential avenues for future
    work?
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主要未解决的挑战是什么？未来工作的潜在方向是什么？
- en: 5\. Analysis of papers reviewed
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 分析所审查的论文
- en: 5.1\. Datasets
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 数据集
- en: We identify 18 report datasets containing images and reports written by experts,
    and 9 classification datasets, which provide an image and the presence or absence
    of a list of abnormalities. Most of the collections are publicly available (10
    and 8 report and classification datasets, respectively), while the rest are proprietary.
    In most cases, the datasets focus on one or more pathologies, and include both
    samples with presence and absence of these. Table [2](#S5.T2 "Table 2 ‣ 5.1\.
    Datasets ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images") presents the main characteristics
    for the public collections, including a list of papers that used them. We next
    discuss the main remarks regarding report and classification datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出 18 个包含由专家编写的图像和报告的报告数据集，以及 9 个分类数据集，这些数据集提供图像及一系列异常的存在或缺失情况。大多数集合是公开可用的（分别有
    10 个报告和 8 个分类数据集），其余的是专有的。在大多数情况下，这些数据集专注于一种或多种病理，并包含这些病理的有无样本。表格 [2](#S5.T2 "表
    2 ‣ 5.1\. 数据集 ‣ 5\. 论文分析 ‣ 关于从医学图像自动生成报告的深度学习与可解释性调查") 展示了公开集合的主要特征，包括使用它们的文献列表。接下来我们讨论报告和分类数据集的主要备注。
- en: '| Dataset | Year | Image Type | # images | # reports | # patients | Used by
    papers |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 年份 | 图像类型 | 图片数量 | 报告数量 | 患者数量 | 被文献使用情况 |'
- en: '| Report datasets |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 报告数据集 |'
- en: '| IU X-ray (Demner-Fushman et al., [2015](#bib.bib29)) | 2015 | Chest X-Ray
    | 7,470 | 3,955 | 3,955 | (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Li et al.,
    [2018](#bib.bib86), [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Xue
    et al., [2018](#bib.bib155); Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152);
    Singh et al., [2019](#bib.bib124); Yin et al., [2019](#bib.bib156); Tian et al.,
    [2019](#bib.bib133); Gasimova, [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37);
    Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17); Xie et al.,
    [2019](#bib.bib151); Xue and Huang, [2019](#bib.bib154); Zhang et al., [2020b](#bib.bib163);
    Jing et al., [2019](#bib.bib68); Shin et al., [2016](#bib.bib121)) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| IU X-ray (Demner-Fushman 等, [2015](#bib.bib29)) | 2015 | 胸部 X 光 | 7,470 |
    3,955 | 3,955 | (Jing 等, [2018](#bib.bib69); Liu 等, [2019](#bib.bib93); Huang
    等, [2019](#bib.bib62); Yuan 等, [2019](#bib.bib157); Li 等, [2018](#bib.bib86),
    [2019b](#bib.bib87); Wang 等, [2018](#bib.bib145); Xue 等, [2018](#bib.bib155);
    Li 等, [2019a](#bib.bib90); Xiong 等, [2019](#bib.bib152); Singh 等, [2019](#bib.bib124);
    Yin 等, [2019](#bib.bib156); Tian 等, [2019](#bib.bib133); Gasimova, [2019](#bib.bib41);
    Gajbhiye 等, [2020](#bib.bib37); Harzig 等, [2019a](#bib.bib49); Biswal 等, [2020](#bib.bib17);
    Xie 等, [2019](#bib.bib151); Xue 和 Huang, [2019](#bib.bib154); Zhang 等, [2020b](#bib.bib163);
    Jing 等, [2019](#bib.bib68); Shin 等, [2016](#bib.bib121)) |'
- en: '| MIMIC-CXR (Johnson et al., [2019a](#bib.bib71), [b](#bib.bib70)) | 2019 |
    Chest X-Ray | 377,110 | 227,827 | 65,379 | (Liu et al., [2019](#bib.bib93)) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-CXR (Johnson 等, [2019a](#bib.bib71), [b](#bib.bib70)) | 2019 | 胸部 X
    光 | 377,110 | 227,827 | 65,379 | (Liu 等, [2019](#bib.bib93)) |'
- en: '| PadChest${}^{\textrm{(sp)}}$(Bustos et al., [2019](#bib.bib20)) | 2019 |
    Chest X-Ray | 160,868 | 109,931 | 67,625 | None${}^{\textrm{(5)}}$ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| PadChest${}^{\textrm{(sp)}}$(Bustos 等, [2019](#bib.bib20)) | 2019 | 胸部 X
    光 | 160,868 | 109,931 | 67,625 | 无${}^{\textrm{(5)}}$ |'
- en: '| ImageCLEF Caption 2017 (Eickhoff et al., [2017](#bib.bib36)) | 2017 | Biomedical${}^{\textrm{(2)}}$
    | 184,614 | 184,614 | - | (Hasan et al., [2018b](#bib.bib52)) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ImageCLEF Caption 2017 (Eickhoff 等, [2017](#bib.bib36)) | 2017 | 生物医学${}^{\textrm{(2)}}$
    | 184,614 | 184,614 | - | (Hasan 等, [2018b](#bib.bib52)) |'
- en: '| ImageCLEF Caption 2018 (García Seco de Herrera et al., [2018](#bib.bib40))
    | 2018 | Biomedical${}^{\textrm{(2)}}$ | 232,305 | 232,305 | - | None${}^{\textrm{(5)}}$
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ImageCLEF Caption 2018 (García Seco de Herrera 等, [2018](#bib.bib40)) | 2018
    | 生物医学${}^{\textrm{(2)}}$ | 232,305 | 232,305 | - | 无${}^{\textrm{(5)}}$ |'
- en: '| ROCO (Pelka et al., [2018](#bib.bib106)) | 2018 | Multiple radiology${}^{\textrm{(3)}}$
    | 81,825 | 81,825 | - | None${}^{\textrm{(5)}}$ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ROCO (Pelka 等, [2018](#bib.bib106)) | 2018 | 多种放射学${}^{\textrm{(3)}}$ | 81,825
    | 81,825 | - | 无${}^{\textrm{(5)}}$ |'
- en: '| PEIR Gross (Jing et al., [2018](#bib.bib69)) | 2017 | Gross lesions | 7,442
    | 7,442 | - | (Jing et al., [2018](#bib.bib69)) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| PEIR Gross (Jing 等, [2018](#bib.bib69)) | 2017 | 粗大病变 | 7,442 | 7,442 | -
    | (Jing 等, [2018](#bib.bib69)) |'
- en: '| INBreast${}^{\textrm{(pt)}}$(Moreira et al., [2012](#bib.bib100)) | 2012
    | Mammography X-ray | 410 | 115 | 115 | (Sun et al., [2019](#bib.bib129); Li and
    Hong, [2019](#bib.bib88)) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| INBreast${}^{\textrm{(pt)}}$(Moreira 等, [2012](#bib.bib100)) | 2012 | 乳腺
    X 光 | 410 | 115 | 115 | (Sun 等, [2019](#bib.bib129); Li 和 Hong, [2019](#bib.bib88))
    |'
- en: '| STARE (Hoover, [1975](#bib.bib59)) | 1975 | Retinal fundus | 400 | 400 |
    - | None${}^{\textrm{(5)}}$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| STARE (Hoover, [1975](#bib.bib59)) | 1975 | 视网膜底部 | 400 | 400 | - | 无${}^{\textrm{(5)}}$
    |'
- en: '| RDIF${}^{\textrm{(1)}}$(Maksoud et al., [2019](#bib.bib96)) | 2019 | Kidney
    Biopsy | 1,152 | 144 | 144 | (Maksoud et al., [2019](#bib.bib96)) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| RDIF${}^{\textrm{(1)}}$(Maksoud et al., [2019](#bib.bib96)) | 2019 | 肾活检
    | 1,152 | 144 | 144 | (Maksoud et al., [2019](#bib.bib96)) |'
- en: '| Classification datasets |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 分类数据集 |'
- en: '| CheXpert (Irvin et al., [2019](#bib.bib64)) | 2019 | Chest X-Ray | 224,316
    | 0 | 65,240 | (Yuan et al., [2019](#bib.bib157); Zhang et al., [2020b](#bib.bib163))
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| CheXpert (Irvin et al., [2019](#bib.bib64)) | 2019 | 胸部X光 | 224,316 | 0 |
    65,240 | (Yuan et al., [2019](#bib.bib157); Zhang et al., [2020b](#bib.bib163))
    |'
- en: '| ChestX-ray14 (Wang et al., [2017](#bib.bib144)) | 2017 | Chest X-Ray | 112,120
    | 0 | 30,805 | (Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Biswal et al.,
    [2020](#bib.bib17); Xue and Huang, [2019](#bib.bib154); Jing et al., [2019](#bib.bib68))
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ChestX-ray14 (Wang et al., [2017](#bib.bib144)) | 2017 | 胸部X光 | 112,120 |
    0 | 30,805 | (Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Biswal et al.,
    [2020](#bib.bib17); Xue and Huang, [2019](#bib.bib154); Jing et al., [2019](#bib.bib68))
    |'
- en: '| LiTS (Christ et al., [2017](#bib.bib26)) | 2017 | Liver CT scans | 200 |
    0 | - | (Tian et al., [2018](#bib.bib132)) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LiTS (Christ et al., [2017](#bib.bib26)) | 2017 | 肝脏CT扫描 | 200 | 0 | - |
    (Tian et al., [2018](#bib.bib132)) |'
- en: '| ACM Biomedia 2019 (Hicks et al., [2019](#bib.bib56)) | 2019 | Gastrointestinal
    tract ${}^{\textrm{(4)}}$ | 14,033 | 0 | - | (Harzig et al., [2019b](#bib.bib50))
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ACM Biomedia 2019 (Hicks et al., [2019](#bib.bib56)) | 2019 | 胃肠道 ${}^{\textrm{(4)}}$
    | 14,033 | 0 | - | (Harzig et al., [2019b](#bib.bib50)) |'
- en: '| DIARETDB0 (Kauppi et al., [2006](#bib.bib74)) | 2006 | Retinal fundus | 130
    | 0 | - | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| DIARETDB0 (Kauppi et al., [2006](#bib.bib74)) | 2006 | 视网膜眼底 | 130 | 0 |
    - | (Wu et al., [2017](#bib.bib149)) |'
- en: '| DIARETDB1 (Kälviäinen and Uusitalo, [2007](#bib.bib73)) | 2007 | Retinal
    fundus | 89 | 0 | - | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| DIARETDB1 (Kälviäinen and Uusitalo, [2007](#bib.bib73)) | 2007 | 视网膜眼底 |
    89 | 0 | - | (Wu et al., [2017](#bib.bib149)) |'
- en: '| Messidor (Decencière et al., [2014](#bib.bib28); Abràmoff et al., [2013](#bib.bib2))
    | 2013 | Retinal fundus | 1,748 | 0 | 874 | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Messidor (Decencière et al., [2014](#bib.bib28); Abràmoff et al., [2013](#bib.bib2))
    | 2013 | 视网膜眼底 | 1,748 | 0 | 874 | (Wu et al., [2017](#bib.bib149)) |'
- en: '| DDSM (Heath et al., [2001](#bib.bib55)) | 2001 | Mammography X-ray | 10,480
    | 0 | - | (Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| DDSM (Heath et al., [2001](#bib.bib55)) | 2001 | 乳腺X光 | 10,480 | 0 | - |
    (Kisilev et al., [2016](#bib.bib77)) |'
- en: 'Table 2\. Public datasets used in the literature. All reports are written in
    English, except those marked with ${}^{\textrm{(sp)}}$ which are in Spanish, and
    ${}^{\textrm{(pt)}}$ in Portuguese. Other notes, ${}^{\textrm{(1)}}$: the RDIF
    dataset is pending release. ${}^{\textrm{(2)}}$: for the ImageCLEF datasets, images
    were extracted from PubMed Central papers and filtered automatically in order
    to keep only clinical images, but some unintended samples from other domains are
    also included. ${}^{\textrm{(3)}}$: contains multiple modalities, namely CT, Ultrasound,
    X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT. ${}^{\textrm{(4)}}$:
    the images are frames extracted from videos. ${}^{\textrm{(5)}}$: none of the
    papers reviewed used this dataset.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表2\. 文献中使用的公开数据集。所有报告均为英文，标记为${}^{\textrm{(sp)}}$的为西班牙语，标记为${}^{\textrm{(pt)}}$的为葡萄牙语。其他注释，${}^{\textrm{(1)}}$:
    RDIF 数据集待发布。${}^{\textrm{(2)}}$: 对于ImageCLEF数据集，图像从PubMed Central论文中提取，并通过自动过滤只保留临床图像，但也包含了一些来自其他领域的无意样本。${}^{\textrm{(3)}}$:
    包含多种模态，即CT、超声、X光、荧光镜、PET、乳腺X光、MRI、血管造影和PET-CT。${}^{\textrm{(4)}}$: 图像为从视频中提取的帧。${}^{\textrm{(5)}}$:
    所有审查过的论文中没有使用此数据集。'
- en: The third column in Table [2](#S5.T2 "Table 2 ‣ 5.1\. Datasets ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images") lists the image modalities for each dataset,
    showing chest X-rays concentrates most of the efforts in report datasets (Demner-Fushman
    et al., [2015](#bib.bib29); Johnson et al., [2019b](#bib.bib70); Bustos et al.,
    [2019](#bib.bib20); Li et al., [2018](#bib.bib86); Gu et al., [2019](#bib.bib45)),
    though there are also datasets with biomedical images from varied types (Eickhoff
    et al., [2017](#bib.bib36); García Seco de Herrera et al., [2018](#bib.bib40);
    Pelka et al., [2018](#bib.bib106); Jing et al., [2018](#bib.bib69)), mammography
    (Moreira et al., [2012](#bib.bib100)) and hip X-rays (Gale et al., [2017](#bib.bib38)),
    ultrasound images (Alsharid et al., [2019](#bib.bib8); Zeng et al., [2018](#bib.bib159)),
    retinal images (Hoover, [1975](#bib.bib59)), doppler echocardiographies (Moradi
    et al., [2016](#bib.bib99)), cervical images (Ma et al., [2018](#bib.bib95)),
    and kidney (Maksoud et al., [2019](#bib.bib96)) and bladder biopsies (Zhang et al.,
    [2017a](#bib.bib164)). This adds an extra challenge, since different kinds of
    exams may need different solutions, as the clinical conditions will be diverse.
    For example, a fundus retinal image may differ significantly from a chest X-ray;
    or a radiologist analyzing an X-ray may follow a different procedure than a pathologist
    reading a biopsy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[2](#S5.T2 "Table 2 ‣ 5.1\. Datasets ‣ 5\. Analysis of papers reviewed ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images")中的第三列列出了每个数据集的图像模态，显示出胸部X光片在报告数据集中占据了大部分努力（Demner-Fushman等，[2015](#bib.bib29)；Johnson等，[2019b](#bib.bib70)；Bustos等，[2019](#bib.bib20)；Li等，[2018](#bib.bib86)；Gu等，[2019](#bib.bib45)），尽管也有来自不同类型的生物医学图像的数据集（Eickhoff等，[2017](#bib.bib36)；García
    Seco de Herrera等，[2018](#bib.bib40)；Pelka等，[2018](#bib.bib106)；Jing等，[2018](#bib.bib69)），乳腺X光（Moreira等，[2012](#bib.bib100)）和髋关节X光（Gale等，[2017](#bib.bib38)），超声图像（Alsharid等，[2019](#bib.bib8)；Zeng等，[2018](#bib.bib159)），视网膜图像（Hoover，[1975](#bib.bib59)），多普勒超声心动图（Moradi等，[2016](#bib.bib99)），宫颈图像（Ma等，[2018](#bib.bib95)），以及肾脏（Maksoud等，[2019](#bib.bib96)）和膀胱活检（Zhang等，[2017a](#bib.bib164)）。这增加了额外的挑战，因为不同种类的检查可能需要不同的解决方案，因为临床条件会多种多样。例如，眼底视网膜图像可能与胸部X光片有显著差异；或者分析X光片的放射科医生可能会遵循不同的程序，而不是解读活检的病理学家。
- en: From the public report datasets, IU X-ray (Demner-Fushman et al., [2015](#bib.bib29))
    is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays
    and 3,955 reports. Additionally, each report was manually annotated with Medical
    Subject Heading (MeSH)²²2[https://www.nlm.nih.gov/mesh/meshhome.html](https://www.nlm.nih.gov/mesh/meshhome.html)
    (Rogers, [1963](#bib.bib116)) and RadLex (Langlotz, [2006](#bib.bib82)) terms,
    and automatically annotated with MeSH terms using the MTI (Mork et al., [2013](#bib.bib101))
    system plus the negation tool from MetaMap (Aronson and Lang, [2010](#bib.bib11)).
    Figure [1](#S2.F1 "Figure 1 ‣ 2\. Task Overview ‣ A Survey on Deep Learning and
    Explainability for Automatic Report Generation from Medical Images") shows a sample
    image and report from this dataset. Note that for deep learning methods, the amount
    of data may seem insufficient, compared to general domain datasets with millions
    of samples, such as ImageNet (Deng et al., [2009](#bib.bib30)). This issue could
    be addressed with pre-training or data augmentation techniques. Also, this may
    be partially solved with the more recent datasets MIMIC-CXR (Johnson et al., [2019b](#bib.bib70))
    or PadChest (Bustos et al., [2019](#bib.bib20)), which contain 377,110 and 160,868
    images respectively, but have not been widely used yet.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从公开的报告数据集中，IU X光（Demner-Fushman等，[2015](#bib.bib29)）是使用最广泛的，由7,470张正面和侧面胸部X光片以及3,955份报告组成。此外，每份报告都用医学主题词表（MeSH）²²2[https://www.nlm.nih.gov/mesh/meshhome.html](https://www.nlm.nih.gov/mesh/meshhome.html)（Rogers，[1963](#bib.bib116)）和RadLex（Langlotz，[2006](#bib.bib82)）术语手动标注，并使用MTI（Mork等，[2013](#bib.bib101)）系统加上MetaMap（Aronson和Lang，[2010](#bib.bib11)）的否定工具自动标注。图[1](#S2.F1
    "Figure 1 ‣ 2\. Task Overview ‣ A Survey on Deep Learning and Explainability for
    Automatic Report Generation from Medical Images")显示了来自该数据集的一个样本图像和报告。请注意，对于深度学习方法来说，与拥有数百万样本的通用领域数据集（如ImageNet（Deng等，[2009](#bib.bib30)））相比，数据量可能显得不足。这个问题可以通过预训练或数据增强技术来解决。此外，这也可以通过较新的数据集MIMIC-CXR（Johnson等，[2019b](#bib.bib70)）或PadChest（Bustos等，[2019](#bib.bib20)）部分解决，这些数据集分别包含377,110和160,868张图像，但尚未被广泛使用。
- en: All report datasets include images and reports, and most of them also include
    labels for each report. Furthermore, INbreast (Moreira et al., [2012](#bib.bib100))
    includes contours locating the labels in the images, the Ultrasound collection
    (Zeng et al., [2018](#bib.bib159), [2020](#bib.bib158)) includes bounding boxes
    locating organs, and IU X-ray (Demner-Fushman et al., [2015](#bib.bib29)) and
    RDIF (Maksoud et al., [2019](#bib.bib96)) include additional text written by the
    physician who requested the exam. The complete detail of additional information
    is shown in Table [9](#S9.T9 "Table 9 ‣ 9.1\. Datasets ‣ 9\. Supplementary Material
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") in appendix [9.1](#S9.SS1 "9.1\. Datasets ‣ 9\. Supplementary
    Material ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"). This information can be leveraged as a supplementary context
    to further improve the system performance. On the one hand, the labels and image
    localization can be used to design auxiliary tasks (see section [5.2.5](#S5.SS2.SSS5
    "5.2.5\. Auxiliary Tasks ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")), and to further evaluate the text generation process (see
    section [5.4](#S5.SS4 "5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). On the other hand, the indication may contain additional
    information not present in the image, such as a patient’s previous condition,
    which in some cases may be essential to address the task (Maksoud et al., [2019](#bib.bib96)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所有报告数据集包括图像和报告，大多数还包括每份报告的标签。此外，INbreast（Moreira et al., [2012](#bib.bib100)）包括定位图像中标签的轮廓，超声波收集（Zeng
    et al., [2018](#bib.bib159), [2020](#bib.bib158)）包括定位器官的边界框，IU X-ray（Demner-Fushman
    et al., [2015](#bib.bib29)）和RDIF（Maksoud et al., [2019](#bib.bib96)）包含由请求检查的医生撰写的额外文本。额外信息的完整细节见附录[9.1](#S9.SS1
    "9.1\. 数据集 ‣ 9\. 附录材料 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")中的表[9](#S9.T9 "表 9 ‣ 9.1\.
    数据集 ‣ 9\. 附录材料 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")。这些信息可以作为补充背景，进一步提升系统性能。一方面，标签和图像定位可以用于设计辅助任务（见[5.2.5](#S5.SS2.SSS5
    "5.2.5\. 辅助任务 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")节），以及进一步评估文本生成过程（见[5.4](#S5.SS4
    "5.4\. 评估指标 ‣ 5\. 论文分析 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")节）。另一方面，指示可能包含图像中没有的额外信息，如患者的既往病史，在某些情况下这可能对任务至关重要（Maksoud
    et al., [2019](#bib.bib96)）。
- en: Lastly, many works use classification datasets, which do not provide a report
    for each image, but a set of clinical conditions or abnormalities present or absent
    in the image. In most cases, this kind of information is used to perform image
    classification as pre-training, an intermediate, or an auxiliary task to generate
    the report. One remarkable case is the CheXpert dataset (Irvin et al., [2019](#bib.bib64)),
    which contains 224,316 images, and was also presented with the CheXpert labeler,
    an automatic rule-based tool that annotates 14 labels (abnormalities) as present,
    absent or uncertain from the natural language reports. This tool was used to label
    the images from the dataset, is also used in MIMIC-CXR (Johnson et al., [2019b](#bib.bib70))
    to tag the reports, and in some works to evaluate the generated reports, as discussed
    in the Metrics section ([5.4](#S5.SS4 "5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images")). Notice the classification dataset list
    is not comprehensive, as it only includes datasets that were used in at least
    one of the reviewed works.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多研究使用分类数据集，这些数据集不为每张图像提供报告，而是提供一组临床状况或图像中存在或缺失的异常。在大多数情况下，这种信息用于执行图像分类作为预训练、中间任务或生成报告的辅助任务。一个显著的案例是CheXpert数据集（Irvin
    et al., [2019](#bib.bib64)），该数据集包含224,316张图像，并且配有CheXpert标注工具，这是一个基于规则的自动工具，用于从自然语言报告中标注14个标签（异常），标记为存在、缺失或不确定。该工具用于标记数据集中的图像，也用于MIMIC-CXR（Johnson
    et al., [2019b](#bib.bib70)）中的报告标记，以及一些研究中用于评估生成的报告，如指标部分（[5.4](#S5.SS4 "5.4\.
    评估指标 ‣ 5\. 论文分析 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")）。请注意，分类数据集列表并不全面，因为它仅包括在至少一项审查的工作中使用的数据集。
- en: Synthesis
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合成
- en: The datasets cover multiple image modalities and body parts, though most efforts
    focus on chest X-rays. This opens a potential research avenue to explore other
    image types and diseases, using existing solutions or raising new methods. Additionally,
    most collections provide valuable supplementary information, such as abnormality
    tags and/or localization, which can be used to design auxiliary tasks and to evaluate
    the performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集涵盖了多种图像模态和身体部位，尽管大多数努力集中在胸部X光片上。这为探索其他图像类型和疾病提供了潜在的研究方向，可以使用现有解决方案或提出新的方法。此外，大多数数据集提供了有价值的补充信息，如异常标记和/或定位，这些信息可用于设计辅助任务并评估性能。
- en: 5.2\. Model Design
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 模型设计
- en: 'In this section, we present an analysis of existent DL model designs, starting
    with a general overview of common design practices. Most models in the literature
    follow a standard design pattern. There is a visual component consisting at its
    core of a Convolutional Neural Network (CNN) (Krizhevsky et al., [2012](#bib.bib80))
    that processes one or more input images in order to extract visual features. Then,
    a language component follows, typically based on well-known NLP neural architectures
    (e.g., LSTM (Hochreiter and Schmidhuber, [1997](#bib.bib58)), BiLSTM (Graves and
    Schmidhuber, [2005](#bib.bib43)), GRU (Chung et al., [2014](#bib.bib27)), Transformer
    (Vaswani et al., [2017](#bib.bib140))) responsible for text processing and report
    generation. Also, a widespread practice for the language component is to retrieve
    the visual information in an adaptive manner via an attention mechanism, as the
    report is written. Many papers follow variations of this pattern inspired by influential
    works from the image captioning domain (Vinyals et al., [2015](#bib.bib142); Xu
    et al., [2015](#bib.bib153)), which are frequently cited and used as baselines.
    Optionally, some models receive or generate additional input or output, and a
    few models incorporate some form of domain knowledge explicitly in the generation
    process. Figure [2](#S5.F2 "Figure 2 ‣ 5.2\. Model Design ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") presents a summary illustration of a general model architecture
    found in the literature. Next, we analyze model designs according to 6 dimensions:
    (1) input and output, (2) visual component, (3) language component, (4) domain
    knowledge, (5) auxiliary tasks and (6) optimization strategies.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们对现有深度学习模型设计进行了分析，从常见设计实践的总体概述开始。文献中的大多数模型遵循标准设计模式。其核心是一个卷积神经网络（CNN）(Krizhevsky
    et al., [2012](#bib.bib80))，用于处理一个或多个输入图像以提取视觉特征。然后，紧接着是一个语言组件，通常基于知名的自然语言处理神经架构（例如，LSTM
    (Hochreiter and Schmidhuber, [1997](#bib.bib58))，BiLSTM (Graves and Schmidhuber,
    [2005](#bib.bib43))，GRU (Chung et al., [2014](#bib.bib27))，Transformer (Vaswani
    et al., [2017](#bib.bib140)))，负责文本处理和报告生成。此外，语言组件的普遍做法是通过注意力机制以自适应方式检索视觉信息，报告在写作过程中会生成。许多论文遵循受图像描述领域影响的这一模式的变体（Vinyals
    et al., [2015](#bib.bib142)；Xu et al., [2015](#bib.bib153)），这些工作经常被引用并作为基准。可选地，一些模型接收或生成额外的输入或输出，还有少数模型在生成过程中明确地纳入了一些领域知识。图
    [2](#S5.F2 "Figure 2 ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣
    A Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images") 展示了文献中常见的模型架构的概述插图。接下来，我们将根据6个维度分析模型设计：（1）输入和输出，（2）视觉组件，（3）语言组件，（4）领域知识，（5）辅助任务和（6）优化策略。
- en: '![Refer to caption](img/2d0c6b89aa20ef00eb2d3f80585227fe.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2d0c6b89aa20ef00eb2d3f80585227fe.png)'
- en: 'Figure 2\. General scheme of components of the architectures reviewed, including
    inputs on the left and outputs on the right. The blue box represents the whole
    model, while the orange boxes show the inner components. Solid line arrows show
    the flow shared by almost every work reviewed, while dashed line arrows show optional
    inputs and outputs seen only in some papers. Note *: in some works, the visual
    component may transfer classification or segmentation outputs besides or instead
    of visual features.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 综述中架构组件的通用方案，包括左侧的输入和右侧的输出。蓝色框表示整个模型，而橙色框显示内部组件。实线箭头表示几乎所有审阅的工作共享的流程，而虚线箭头表示仅在某些论文中出现的可选输入和输出。注意
    *：在某些工作中，视觉组件可能会传输分类或分割输出，而不仅仅是视觉特征。
- en: \Description
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \说明
- en: General model architecture found in the literature review. The model consists
    of a visual and a language component. There are two possible inputs, image(s)
    and patient background; and five possible outputs, free text report, a heatmap
    over the input text, a heatmap over the input image, a counter-factual image,
    and classification output. Additionally, explicit domain knowledge can be incorporated
    in the model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 文献综述中发现的一般模型架构。模型包括一个视觉组件和一个语言组件。有两个可能的输入，图像和患者背景；以及五个可能的输出，自由文本报告、输入文本的热图、输入图像的热图、对比图像和分类输出。此外，可以在模型中加入明确的领域知识。
- en: 5.2.1\. Input and Output
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 输入与输出
- en: '| Category | Value or Type | Used by papers |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 值或类型 | 参考论文 |'
- en: '| Input |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| Image Type | Chest X-Ray | (Jing et al., [2018](#bib.bib69); Liu et al.,
    [2019](#bib.bib93); Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157);
    Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Xue et al., [2018](#bib.bib155); Li et al., [2019a](#bib.bib90); Xiong et al.,
    [2019](#bib.bib152); Singh et al., [2019](#bib.bib124); Gu et al., [2019](#bib.bib45);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133); Gasimova, [2019](#bib.bib41);
    Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Biswal
    et al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Xue and Huang, [2019](#bib.bib154);
    Zhang et al., [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68); Shin et al.,
    [2016](#bib.bib121); Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 图像类型 | 胸部 X 光 | (Jing 等, [2018](#bib.bib69); Liu 等, [2019](#bib.bib93); Huang
    等, [2019](#bib.bib62); Yuan 等, [2019](#bib.bib157); Li 等, [2018](#bib.bib86),
    [2019b](#bib.bib87); Wang 等, [2018](#bib.bib145); Xue 等, [2018](#bib.bib155);
    Li 等, [2019a](#bib.bib90); Xiong 等, [2019](#bib.bib152); Singh 等, [2019](#bib.bib124);
    Gu 等, [2019](#bib.bib45); Yin 等, [2019](#bib.bib156); Tian 等, [2019](#bib.bib133);
    Gasimova, [2019](#bib.bib41); Gajbhiye 等, [2020](#bib.bib37); Harzig 等, [2019a](#bib.bib49);
    Biswal 等, [2020](#bib.bib17); Xie 等, [2019](#bib.bib151); Xue 和 Huang, [2019](#bib.bib154);
    Zhang 等, [2020b](#bib.bib163); Jing 等, [2019](#bib.bib68); Shin 等, [2016](#bib.bib121);
    Spinks 和 Moens, [2019](#bib.bib127)) |'
- en: '| Mammography X-ray | (Sun et al., [2019](#bib.bib129); Li and Hong, [2019](#bib.bib88);
    Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺 X 光 | (Sun 等, [2019](#bib.bib129); Li 和 Hong, [2019](#bib.bib88); Kisilev
    等, [2016](#bib.bib77)) |'
- en: '| Hip X-Ray | (Gale et al., [2019](#bib.bib39)) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 髋关节 X 光 | (Gale 等, [2019](#bib.bib39)) |'
- en: '| Ultrasound video frames | (Alsharid et al., [2019](#bib.bib8); Zeng et al.,
    [2018](#bib.bib159); Kisilev et al., [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158))
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 超声视频帧 | (Alsharid 等, [2019](#bib.bib8); Zeng 等, [2018](#bib.bib159); Kisilev
    等, [2016](#bib.bib77); Zeng 等, [2020](#bib.bib158)) |'
- en: '| CW Doppler echocardiography | (Moradi et al., [2016](#bib.bib99)) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| CW 多普勒超声心动图 | (Moradi 等, [2016](#bib.bib99)) |'
- en: '| Gastrointestinal tract examination frames | (Harzig et al., [2019b](#bib.bib50))
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 胃肠道检查帧 | (Harzig 等, [2019b](#bib.bib50)) |'
- en: '| Gross lesions | (Jing et al., [2018](#bib.bib69)) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 大体病变 | (Jing 等, [2018](#bib.bib69))'
- en: '| Bladder biopsy | (Zhang et al., [2017a](#bib.bib164)) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 膀胱活检 | (Zhang 等, [2017a](#bib.bib164)) |'
- en: '| Kidney biopsy | (Maksoud et al., [2019](#bib.bib96)) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 肾脏活检 | (Maksoud 等, [2019](#bib.bib96)) |'
- en: '| Liver tumor CT scans | (Tian et al., [2018](#bib.bib132)) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 肝肿瘤 CT 扫描 | (Tian 等, [2018](#bib.bib132)) |'
- en: '| Cervical neoplasm WSI | (Ma et al., [2018](#bib.bib95)) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 宫颈肿瘤 WSI | (Ma 等, [2018](#bib.bib95)) |'
- en: '| Spine MRI | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 脊柱 MRI | (Han 等, [2018](#bib.bib48)) |'
- en: '| Fundus retinal images | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 眼底视网膜图像 | (Wu 等, [2017](#bib.bib149)) |'
- en: '| Biomedical images | (Hasan et al., [2018b](#bib.bib52)) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 生物医学图像 | (Hasan 等, [2018b](#bib.bib52)) |'
- en: '| Number of images | 1 | (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Wang et al., [2018](#bib.bib145); Zhang et al.,
    [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152);
    Singh et al., [2019](#bib.bib124); Gale et al., [2019](#bib.bib39); Gu et al.,
    [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Ma et al., [2018](#bib.bib95); Alsharid et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41);
    Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Biswal
    et al., [2020](#bib.bib17); Zeng et al., [2018](#bib.bib159); Harzig et al., [2019b](#bib.bib50);
    Xue and Huang, [2019](#bib.bib154); Sun et al., [2019](#bib.bib129); Han et al.,
    [2018](#bib.bib48); Li and Hong, [2019](#bib.bib88); Jing et al., [2019](#bib.bib68);
    Shin et al., [2016](#bib.bib121); Hasan et al., [2018b](#bib.bib52); Kisilev et al.,
    [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99); Wu et al., [2017](#bib.bib149);
    Spinks and Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 图像数量 | 1 | （Jing 等，[2018](#bib.bib69)；Liu 等，[2019](#bib.bib93)；Huang 等，[2019](#bib.bib62)；Wang
    等，[2018](#bib.bib145)；Zhang 等，[2017a](#bib.bib164)；Li 等，[2019a](#bib.bib90)；Xiong
    等，[2019](#bib.bib152)；Singh 等，[2019](#bib.bib124)；Gale 等，[2019](#bib.bib39)；Gu
    等，[2019](#bib.bib45)；Yin 等，[2019](#bib.bib156)；Tian 等，[2019](#bib.bib133)；Ma 等，[2018](#bib.bib95)；Alsharid
    等，[2019](#bib.bib8)；Gasimova，[2019](#bib.bib41)；Gajbhiye 等，[2020](#bib.bib37)；Harzig
    等，[2019a](#bib.bib49)；Biswal 等，[2020](#bib.bib17)；Zeng 等，[2018](#bib.bib159)；Harzig
    等，[2019b](#bib.bib50)；Xue 和 Huang，[2019](#bib.bib154)；Sun 等，[2019](#bib.bib129)；Han
    等，[2018](#bib.bib48)；Li 和 Hong，[2019](#bib.bib88)；Jing 等，[2019](#bib.bib68)；Shin
    等，[2016](#bib.bib121)；Hasan 等，[2018b](#bib.bib52)；Kisilev 等，[2016](#bib.bib77)；Moradi
    等，[2016](#bib.bib99)；Wu 等，[2017](#bib.bib149)；Spinks 和 Moens，[2019](#bib.bib127)；Zeng
    等，[2020](#bib.bib158)） |'
- en: '| 2 | (Yuan et al., [2019](#bib.bib157); Li et al., [2018](#bib.bib86), [2019b](#bib.bib87);
    Xue et al., [2018](#bib.bib155); Xie et al., [2019](#bib.bib151); Zhang et al.,
    [2020b](#bib.bib163)) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 2 | （Yuan 等，[2019](#bib.bib157)；Li 等，[2018](#bib.bib86)，[2019b](#bib.bib87)；Xue
    等，[2018](#bib.bib155)；Xie 等，[2019](#bib.bib151)；Zhang 等，[2020b](#bib.bib163)）
    |'
- en: '| Any | (Maksoud et al., [2019](#bib.bib96); Tian et al., [2018](#bib.bib132))
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 任何 | （Maksoud 等，[2019](#bib.bib96)；Tian 等，[2018](#bib.bib132)） |'
- en: '| Text | Indication | (Huang et al., [2019](#bib.bib62); Maksoud et al., [2019](#bib.bib96))
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 指示 | （Huang 等，[2019](#bib.bib62)；Maksoud 等，[2019](#bib.bib96)） |'
- en: '| Indication and findings | (Tian et al., [2019](#bib.bib133)) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 指示和发现 | （Tian 等，[2019](#bib.bib133)） |'
- en: '| Prefix sentence and keywords | (Biswal et al., [2020](#bib.bib17)) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 前缀句子和关键词 | （Biswal 等，[2020](#bib.bib17)） |'
- en: '| Partial report or caption | (Alsharid et al., [2019](#bib.bib8); Gajbhiye
    et al., [2020](#bib.bib37)) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 部分报告或说明 | （Alsharid 等，[2019](#bib.bib8)；Gajbhiye 等，[2020](#bib.bib37)） |'
- en: '| Output |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 输出 |'
- en: '| Report | Generative multi-sentence (unstructured) | (Jing et al., [2018](#bib.bib69);
    Liu et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib62); Yuan et al.,
    [2019](#bib.bib157); Wang et al., [2018](#bib.bib145); Xue et al., [2018](#bib.bib155);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Singh et al.,
    [2019](#bib.bib124); Maksoud et al., [2019](#bib.bib96); Gu et al., [2019](#bib.bib45);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133); Gajbhiye et al.,
    [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Xie et al., [2019](#bib.bib151);
    Xue and Huang, [2019](#bib.bib154); Sun et al., [2019](#bib.bib129); Zhang et al.,
    [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68)) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 报告 | 生成的多句（无结构） | （Jing 等，[2018](#bib.bib69)；Liu 等，[2019](#bib.bib93)；Huang
    等，[2019](#bib.bib62)；Yuan 等，[2019](#bib.bib157)；Wang 等，[2018](#bib.bib145)；Xue
    等，[2018](#bib.bib155)；Li 等，[2019a](#bib.bib90)；Xiong 等，[2019](#bib.bib152)；Singh
    等，[2019](#bib.bib124)；Maksoud 等，[2019](#bib.bib96)；Gu 等，[2019](#bib.bib45)；Yin
    等，[2019](#bib.bib156)；Tian 等，[2019](#bib.bib133)；Gajbhiye 等，[2020](#bib.bib37)；Harzig
    等，[2019a](#bib.bib49)；Xie 等，[2019](#bib.bib151)；Xue 和 Huang，[2019](#bib.bib154)；Sun
    等，[2019](#bib.bib129)；Zhang 等，[2020b](#bib.bib163)；Jing 等，[2019](#bib.bib68)）
    |'
- en: '| Generative multi-sentence structured | (Zhang et al., [2017a](#bib.bib164);
    Tian et al., [2018](#bib.bib132)) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 生成的多句结构化 | （Zhang 等，[2017a](#bib.bib164)；Tian 等，[2018](#bib.bib132)）'
- en: '| Generative single-sentence | (Gale et al., [2019](#bib.bib39); Alsharid et al.,
    [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Zeng et al., [2018](#bib.bib159);
    Li and Hong, [2019](#bib.bib88); Shin et al., [2016](#bib.bib121); Hasan et al.,
    [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149); Spinks and Moens, [2019](#bib.bib127);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 生成的单句 | （Gale 等，[2019](#bib.bib39)；Alsharid 等，[2019](#bib.bib8)；Gasimova，[2019](#bib.bib41)；Zeng
    等，[2018](#bib.bib159)；Li 和 Hong，[2019](#bib.bib88)；Shin 等，[2016](#bib.bib121)；Hasan
    等，[2018b](#bib.bib52)；Wu 等，[2017](#bib.bib149)；Spinks 和 Moens，[2019](#bib.bib127)；Zeng
    等，[2020](#bib.bib158)） |'
- en: '| Template-based | (Ma et al., [2018](#bib.bib95); Harzig et al., [2019b](#bib.bib50);
    Han et al., [2018](#bib.bib48); Kisilev et al., [2016](#bib.bib77); Moradi et al.,
    [2016](#bib.bib99)) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 基于模板 | (Ma et al., [2018](#bib.bib95); Harzig et al., [2019b](#bib.bib50);
    Han et al., [2018](#bib.bib48); Kisilev et al., [2016](#bib.bib77); Moradi et
    al., [2016](#bib.bib99)) |'
- en: '| Hybrid template - generation/edition | (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87);
    Biswal et al., [2020](#bib.bib17)) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 混合模板 - 生成/编辑 | (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Biswal
    et al., [2020](#bib.bib17)) |'
- en: '| Classification | MeSH concepts or similar | (Jing et al., [2018](#bib.bib69);
    Yuan et al., [2019](#bib.bib157); Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156);
    Tian et al., [2019](#bib.bib133); Harzig et al., [2019a](#bib.bib49), [b](#bib.bib50);
    Sun et al., [2019](#bib.bib129); Shin et al., [2016](#bib.bib121)) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | MeSH概念或类似的 | (Jing et al., [2018](#bib.bib69); Yuan et al., [2019](#bib.bib157);
    Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Harzig et al., [2019a](#bib.bib49), [b](#bib.bib50); Sun et al., [2019](#bib.bib129);
    Shin et al., [2016](#bib.bib121)) |'
- en: '| Abnormalities/diseases presence or absence | (Li et al., [2019b](#bib.bib87);
    Wang et al., [2018](#bib.bib145); Li et al., [2019a](#bib.bib90); Xiong et al.,
    [2019](#bib.bib152); Biswal et al., [2020](#bib.bib17); Zeng et al., [2018](#bib.bib159);
    Zhang et al., [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68); Spinks and
    Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 异常/疾病的存在或缺失 | (Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Biswal et al.,
    [2020](#bib.bib17); Zeng et al., [2018](#bib.bib159); Zhang et al., [2020b](#bib.bib163);
    Jing et al., [2019](#bib.bib68); Spinks and Moens, [2019](#bib.bib127); Zeng et
    al., [2020](#bib.bib158)) |'
- en: '| Abnormalities/diseases characterization or severity level | (Zhang et al.,
    [2017a](#bib.bib164); Gale et al., [2019](#bib.bib39); Ma et al., [2018](#bib.bib95);
    Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 异常/疾病的特征化或严重程度 | (Zhang et al., [2017a](#bib.bib164); Gale et al., [2019](#bib.bib39);
    Ma et al., [2018](#bib.bib95); Kisilev et al., [2016](#bib.bib77)) |'
- en: '| Body parts or organs | (Alsharid et al., [2019](#bib.bib8); Zeng et al.,
    [2018](#bib.bib159); Moradi et al., [2016](#bib.bib99); Zeng et al., [2020](#bib.bib158))
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 身体部位或器官 | (Alsharid et al., [2019](#bib.bib8); Zeng et al., [2018](#bib.bib159);
    Moradi et al., [2016](#bib.bib99); Zeng et al., [2020](#bib.bib158)) |'
- en: '| Image modality | (Hasan et al., [2018b](#bib.bib52)) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 图像模态 | (Hasan et al., [2018b](#bib.bib52)) |'
- en: '| Normal or abnormal sentence | (Harzig et al., [2019a](#bib.bib49); Xie et al.,
    [2019](#bib.bib151); Jing et al., [2019](#bib.bib68)) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 正常或异常句子 | (Harzig et al., [2019a](#bib.bib49); Xie et al., [2019](#bib.bib151);
    Jing et al., [2019](#bib.bib68)) |'
- en: '| Image Heatmap | Attention-based per word | (Liu et al., [2019](#bib.bib93);
    Wang et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164)) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 图像热图 | 基于注意力的逐词分析 | (Liu et al., [2019](#bib.bib93); Wang et al., [2018](#bib.bib145);
    Zhang et al., [2017a](#bib.bib164)) |'
- en: '| Attention-based per sentence | (Jing et al., [2018](#bib.bib69); Huang et al.,
    [2019](#bib.bib62); Xue et al., [2018](#bib.bib155); Xue and Huang, [2019](#bib.bib154))
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 基于注意力的逐句分析 | (Jing et al., [2018](#bib.bib69); Huang et al., [2019](#bib.bib62);
    Xue et al., [2018](#bib.bib155); Xue and Huang, [2019](#bib.bib154)) |'
- en: '| Attention-based per report | (Li et al., [2019b](#bib.bib87)) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 基于注意力的逐报告分析 | (Li et al., [2019b](#bib.bib87)) |'
- en: '| CAM (Zhou et al., [2016](#bib.bib166)) | (Ma et al., [2018](#bib.bib95);
    Harzig et al., [2019b](#bib.bib50)) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| CAM (Zhou et al., [2016](#bib.bib166)) | (Ma et al., [2018](#bib.bib95);
    Harzig et al., [2019b](#bib.bib50)) |'
- en: '| Grad-CAM (Selvaraju et al., [2017](#bib.bib119)) | (Yuan et al., [2019](#bib.bib157);
    Li et al., [2019a](#bib.bib90)) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Grad-CAM (Selvaraju et al., [2017](#bib.bib119)) | (Yuan et al., [2019](#bib.bib157);
    Li et al., [2019a](#bib.bib90)) |'
- en: '| SmoothGrad (Smilkov et al., [2017](#bib.bib125)) | (Gale et al., [2019](#bib.bib39))
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SmoothGrad (Smilkov et al., [2017](#bib.bib125)) | (Gale et al., [2019](#bib.bib39))
    |'
- en: '| Activation-based attention (Komodakis and Zagoruyko, [2017](#bib.bib78))
    | (Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 基于激活的注意力 (Komodakis and Zagoruyko, [2017](#bib.bib78)) | (Spinks and Moens,
    [2019](#bib.bib127)) |'
- en: '| Bounding Box (Faster R-CNN (Ren et al., [2015](#bib.bib113))) | (Zeng et al.,
    [2020](#bib.bib158); Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 边界框 (Faster R-CNN (Ren et al., [2015](#bib.bib113))) | (Zeng et al., [2020](#bib.bib158);
    Kisilev et al., [2016](#bib.bib77)) |'
- en: '| Disease and body part pixel-level classification | (Tian et al., [2018](#bib.bib132);
    Han et al., [2018](#bib.bib48)) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 疾病和身体部位像素级分类 | (Tian et al., [2018](#bib.bib132); Han et al., [2018](#bib.bib48))
    |'
- en: '| Text Heatmap | Attention based per word | (Huang et al., [2019](#bib.bib62))
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 文本热图 | 基于注意力的逐词分析 | (Huang et al., [2019](#bib.bib62)) |'
- en: '| Others | Counter-factual example generation | (Spinks and Moens, [2019](#bib.bib127))
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 反事实例生成 | (Spinks and Moens, [2019](#bib.bib127)) |'
- en: Table 3\. Summary of input and output analysis of the reviewed literature.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 综述文献的输入和输出分析汇总。
- en: Table [3](#S5.T3 "Table 3 ‣ 5.2.1\. Input and Output ‣ 5.2\. Model Design ‣
    5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images") presents a summary of this
    analysis.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[3](#S5.T3 "表格 3 ‣ 5.2.1\. 输入与输出 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 关于深度学习和自动报告生成的医学图像可解释性的调查")展示了这项分析的总结。
- en: Input. With respect to image type, most papers (24) used chest X-rays, whereas
    the other papers are more or less equally distributed over other image types.
    A total of 32 models receive a single image (e.g. a single chest X-ray view),
    6 models receive 2 images (both frontal and lateral chest X-ray views), and 2
    models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans).
    Most models in the literature only handle visual input. However, 6 works (Huang
    et al., [2019](#bib.bib62); Maksoud et al., [2019](#bib.bib96); Tian et al., [2019](#bib.bib133);
    Biswal et al., [2020](#bib.bib17); Alsharid et al., [2019](#bib.bib8); Gajbhiye
    et al., [2020](#bib.bib37)) explored the use of complementary input text, reporting
    performance gains in most cases. For example, two works (Huang et al., [2019](#bib.bib62);
    Maksoud et al., [2019](#bib.bib96)) encode an indication paragraph with a BiLSTM.
    Similarly, MTMA (Tian et al., [2019](#bib.bib133)) encodes the report’s indication
    and findings sections with a BiLSTM per sentence first, and then a LSTM produces
    a final vector representation. Similarly, two works (Alsharid et al., [2019](#bib.bib8);
    Gajbhiye et al., [2020](#bib.bib37)) use LSTM/BiLSTM to encode a partial report
    or caption as input, in order to predict the next word. Unlike other works, CLARA
    (Biswal et al., [2020](#bib.bib17)) uses a software package, Lucene (Branko et al.,
    [2010](#bib.bib19)), to perform text-based retrieval of report templates. The
    input text is processed by Lucene as a search query, and the retrieved templates
    are paraphrased by an encoder-decoder network to generate the final report.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入。关于图像类型，大多数论文（24篇）使用了胸部X光片，而其他论文则在其他图像类型上分布较为均匀。总共有32个模型接收单张图像（例如，单个胸部X光视图），6个模型接收2张图像（前视和侧视胸部X光视图），2个模型接收任意数量的图像（例如，多切片腹部CT扫描）。文献中的大多数模型仅处理视觉输入。然而，6项研究（黄等，[2019](#bib.bib62)；Maksoud等，[2019](#bib.bib96)；田等，[2019](#bib.bib133)；Biswal等，[2020](#bib.bib17)；Alsharid等，[2019](#bib.bib8)；Gajbhiye等，[2020](#bib.bib37)）探讨了使用补充输入文本的可能性，在大多数情况下报告了性能提升。例如，两项工作（黄等，[2019](#bib.bib62)；Maksoud等，[2019](#bib.bib96)）用BiLSTM编码了指示段落。同样，MTMA（田等，[2019](#bib.bib133)）先用BiLSTM逐句编码报告的指示和发现部分，然后用LSTM生成最终的向量表示。类似地，两项工作（Alsharid等，[2019](#bib.bib8)；Gajbhiye等，[2020](#bib.bib37)）使用LSTM/BiLSTM编码部分报告或说明作为输入，以预测下一个词。与其他工作不同，CLARA（Biswal等，[2020](#bib.bib17)）使用软件包Lucene（Branko等，[2010](#bib.bib19)）进行基于文本的报告模板检索。输入文本由Lucene处理作为搜索查询，检索到的模板通过编码器-解码器网络进行释义，以生成最终报告。
- en: 'Output. All models output a natural language report. According to the extension
    of the report and the general strategy used to produce it, we group papers into
    five categories: (1) Generative multi-sentence (unstructured): these models generate
    a multi-sentence report, word by word, with freedom to decide the number of sentences
    and the words in each sentence. (2) Generative multi-sentence structured: similar
    to the previous category, but always output a fixed number of sentences, and each
    sentence always has a pre-defined topic. These models are designed for datasets
    where reports follow a rigid structure. (3) Generative single-sentence: generate
    a report word by word, but only output a single sentence. These models are designed
    for datasets with simple one-sentence reports. (4) Template-based: use human-designed
    templates to produce the report, for example performing a classification task
    followed by if-then rules, template selection and template filling. This simplifies
    the report generation task for the model, at the expense of making it less flexible
    and requiring the human designing of templates and rules. And lastly (5) Hybrid
    template - generation/edition: use templates and also have the freedom to generate
    sentences word by word. This can be accomplished by choosing between a template
    or generating a sentence from scratch (Li et al., [2018](#bib.bib86)), or by editing/paraphrasing
    a previously selected template (Li et al., [2019b](#bib.bib87); Biswal et al.,
    [2020](#bib.bib17)).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出。所有模型都生成自然语言报告。根据报告的扩展和生成报告时使用的一般策略，我们将论文分为五类：(1) 生成式多句子（非结构化）：这些模型逐字生成多句子报告，自由决定句子数量和每个句子中的词汇。(2)
    生成式多句子结构化：类似于前一类别，但总是输出固定数量的句子，并且每个句子都有预定义的主题。这些模型设计用于报告遵循严格结构的数据集。(3) 生成式单句：逐字生成报告，但只输出单个句子。这些模型设计用于简单的一句报告的数据集。(4)
    基于模板：使用人工设计的模板生成报告，例如执行分类任务后应用if-then规则、模板选择和模板填充。这简化了模型的报告生成任务，但使其灵活性降低，并需要人工设计模板和规则。最后，(5)
    混合模板 - 生成/编辑：使用模板，同时也可以逐字生成句子。这可以通过选择模板或从头生成句子（Li et al., [2018](#bib.bib86)），或者编辑/改述之前选择的模板（Li
    et al., [2019b](#bib.bib87); Biswal et al., [2020](#bib.bib17)）来实现。
- en: In addition to the report itself, many models also output complementary classification
    predictions, such as presence or absence of abnormalities or diseases, MeSH concepts,
    body parts or organs, among others. These are often referred to as labels or tags,
    and are commonly used in the language component, as will be discussed in section
    [5.2.3](#S5.SS2.SSS3 "5.2.3\. Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"). Many models can also output heatmaps
    over an image highlighting relevant regions using different techniques, such as
    explicit visual attention weights computed during report generation, saliency
    maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention),
    bounding box regression, and pixel-level classification (image segmentation).
    Also, one model (Huang et al., [2019](#bib.bib62)) can output a heatmap over its
    input text and one model (Spinks and Moens, [2019](#bib.bib127)) can generate
    a counter-factual example to justify its decision. We will discuss all these outputs
    more in detail and their use in the explainability section ([5.3](#S5.SS3 "5.3\.
    Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and
    Explainability for Automatic Report Generation from Medical Images")).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了报告本身，许多模型还输出补充的分类预测，例如异常或疾病的存在与否、MeSH概念、身体部位或器官等。这些通常被称为标签或标记，并且在语言组件中经常使用，如在[5.2.3](#S5.SS2.SSS3
    "5.2.3\. 语言组件 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 关于从医学图像自动报告生成的深度学习和可解释性的调查")部分将讨论的那样。许多模型还可以生成覆盖图像的热图，使用不同技术高亮相关区域，例如在报告生成过程中计算的显式视觉注意力权重、显著性图方法（如CAM、Grad-CAM、SmoothGrad或基于激活的注意力）、边界框回归和像素级分类（图像分割）。此外，有一个模型（Huang
    et al., [2019](#bib.bib62)）可以在其输入文本上生成热图，另一个模型（Spinks and Moens, [2019](#bib.bib127)）可以生成反事实示例以解释其决策。我们将在解释性部分（[5.3](#S5.SS3
    "5.3\. 解释性 ‣ 5\. 论文分析 ‣ 关于从医学图像自动报告生成的深度学习和可解释性的调查")）中更详细地讨论所有这些输出及其使用。
- en: 5.2.2\. Visual Component
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 视觉组件
- en: The most important observation is that all surveyed works use CNNs to process
    the input images. This is not surprising since CNNs have dominated the state of
    the art in computer vision for several years (Khan et al., [2020](#bib.bib75)).
    The typical visual processing pipeline consists of a CNN that receives an input
    image and outputs a volume of feature maps of dimensions $W\times H\times C$,
    where $W$ and $H$ denote spatial dimensions (width and height) and $C$ denotes
    the channel dimensions (depth or number of feature maps). These visual features
    are then leveraged by the language component to make decisions for report generation
    (e.g., which sentence to write, which template to retrieve, next word to output,
    etc.), typically by way of an attention mechanism.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的观察结果是，所有调查的工作都使用CNN来处理输入图像。这并不令人惊讶，因为CNN已经主导了计算机视觉领域的最新技术多年（Khan et al.,
    [2020](#bib.bib75)）。典型的视觉处理管道包括一个CNN，该CNN接收输入图像并输出尺寸为$W\times H\times C$的特征图体积，其中$W$和$H$表示空间维度（宽度和高度），$C$表示通道维度（深度或特征图数量）。这些视觉特征随后被语言组件利用，以进行报告生成决策（例如，写什么句子，检索哪个模板，输出下一个词等），通常通过注意机制实现。
- en: However, some works did not strictly follow this pattern. For example, in two
    works (Gu et al., [2019](#bib.bib45); Sun et al., [2019](#bib.bib129)) a CNN is
    used for multi-label classification of tags, which are then mapped to embedded
    vectors via embedding matrix lookup. Thus, the report generation module only has
    access to these tag vectors but no access to the visual features themselves. Similarly,
    two works (Jing et al., [2018](#bib.bib69); Yin et al., [2019](#bib.bib156)) classify
    and look up tag embedding vectors, but unlike the previous works, the language
    component uses co-attention to access both tags vectors and visual features simultaneously.
    Their ablation analysis showed that the semantic information provided by these
    tags complements the visual information and improves the model’s performance in
    report generation. Other works (Li et al., [2019b](#bib.bib87); Zhang et al.,
    [2020b](#bib.bib163)) used graph neural networks immediately after the CNN to
    encode the visual information in terms of medical concepts and their relations.
    Thus, the language component receives the intermediate graph representation instead
    of the raw visual features. The ablation analysis by Zhang et al. (Zhang et al.,
    [2020b](#bib.bib163)) showed some performance gains thanks to the graph neural
    network. Vispi (Li et al., [2019a](#bib.bib90)) implements a two-stage procedure,
    where two distinct CNNs are used. In the first stage a DenseNet 121 (Huang et al.,
    [2017](#bib.bib61)) classifies abnormalities in the image, and then Grad-CAM (Selvaraju
    et al., [2017](#bib.bib119)) is used to localize and crop a region of the image
    for each detected class. Then, in the second stage the multiple image crops are
    treated as independent images and processed by a typical CNN+LSTM architecture,
    with ResNet 101 (He et al., [2016](#bib.bib53)) as the CNN. A similar idea was
    followed in RTMIC (Xiong et al., [2019](#bib.bib152)), where a DenseNet 121 is
    pretrained for classification in ChestX-ray14 (Wang et al., [2017](#bib.bib144))
    and CAM is used to get image crops for each class.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些工作并未严格遵循这一模式。例如，在两项工作（Gu et al., [2019](#bib.bib45)；Sun et al., [2019](#bib.bib129)）中，CNN
    被用于多标签分类，然后通过嵌入矩阵查找将标签映射到嵌入向量。因此，报告生成模块只能访问这些标签向量，而无法访问视觉特征本身。类似地，两项工作（Jing et
    al., [2018](#bib.bib69)；Yin et al., [2019](#bib.bib156)）对标签嵌入向量进行分类和查找，但与前述工作不同，语言组件使用协同注意力同时访问标签向量和视觉特征。他们的消融分析显示，这些标签提供的语义信息补充了视觉信息，并提升了报告生成模型的性能。其他工作（Li
    et al., [2019b](#bib.bib87)；Zhang et al., [2020b](#bib.bib163)）在CNN之后立即使用图神经网络，以医疗概念及其关系来编码视觉信息。因此，语言组件接收到的是中间图表示，而不是原始视觉特征。Zhang
    et al.（Zhang et al., [2020b](#bib.bib163)）的消融分析显示，得益于图神经网络，性能有所提升。Vispi（Li et
    al., [2019a](#bib.bib90)）实现了一个两阶段过程，其中使用了两个不同的CNN。在第一阶段，DenseNet 121（Huang et
    al., [2017](#bib.bib61)）对图像中的异常进行分类，然后使用Grad-CAM（Selvaraju et al., [2017](#bib.bib119)）来定位和裁剪每个检测到的类别的图像区域。接着，在第二阶段，将多个图像裁剪视为独立图像，并由典型的CNN+LSTM架构处理，ResNet
    101（He et al., [2016](#bib.bib53)）作为CNN。在RTMIC（Xiong et al., [2019](#bib.bib152)）中采用了类似的思路，其中DenseNet
    121在ChestX-ray14（Wang et al., [2017](#bib.bib144)）中进行了预训练用于分类，并使用CAM获得每个类别的图像裁剪。
- en: '| Architecture | Used by papers |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 被文献使用 |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| DenseNet (Huang et al., [2017](#bib.bib61)) | (Liu et al., [2019](#bib.bib93);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Zhang et al.,
    [2020b](#bib.bib163); Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Gale
    et al., [2019](#bib.bib39); Yin et al., [2019](#bib.bib156); Biswal et al., [2020](#bib.bib17))
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet (Huang et al., [2017](#bib.bib61)) | (Liu et al., [2019](#bib.bib93);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Zhang et al.,
    [2020b](#bib.bib163); Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Gale
    et al., [2019](#bib.bib39); Yin et al., [2019](#bib.bib156); Biswal et al., [2020](#bib.bib17))
    |'
- en: '| ResNet (He et al., [2016](#bib.bib53)) | (Huang et al., [2019](#bib.bib62);
    Yuan et al., [2019](#bib.bib157); Xue et al., [2018](#bib.bib155); Harzig et al.,
    [2019a](#bib.bib49); Xue and Huang, [2019](#bib.bib154); Li et al., [2019a](#bib.bib90);
    Gu et al., [2019](#bib.bib45); Wang et al., [2018](#bib.bib145); Gasimova, [2019](#bib.bib41);
    Jing et al., [2019](#bib.bib68); Ma et al., [2018](#bib.bib95)) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ResNet (He et al., [2016](#bib.bib53)) | (Huang et al., [2019](#bib.bib62);
    Yuan et al., [2019](#bib.bib157); Xue et al., [2018](#bib.bib155); Harzig et al.,
    [2019a](#bib.bib49); Xue 和 Huang, [2019](#bib.bib154); Li et al., [2019a](#bib.bib90);
    Gu et al., [2019](#bib.bib45); Wang et al., [2018](#bib.bib145); Gasimova, [2019](#bib.bib41);
    Jing et al., [2019](#bib.bib68); Ma et al., [2018](#bib.bib95)) |'
- en: '| VGG (Simonyan and Zisserman, [2014](#bib.bib123)) | (Jing et al., [2018](#bib.bib69);
    Hasan et al., [2018b](#bib.bib52); Maksoud et al., [2019](#bib.bib96); Alsharid
    et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37);
    Zeng et al., [2018](#bib.bib159); Li and Hong, [2019](#bib.bib88); Kisilev et al.,
    [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158); Moradi et al., [2016](#bib.bib99))
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| VGG (Simonyan 和 Zisserman, [2014](#bib.bib123)) | (Jing et al., [2018](#bib.bib69);
    Hasan et al., [2018b](#bib.bib52); Maksoud et al., [2019](#bib.bib96); Alsharid
    et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37);
    Zeng et al., [2018](#bib.bib159); Li 和 Hong, [2019](#bib.bib88); Kisilev et al.,
    [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158); Moradi et al., [2016](#bib.bib99))
    |'
- en: '| Faster R-CNN (Ren et al., [2015](#bib.bib113)) | (Kisilev et al., [2016](#bib.bib77);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN (Ren et al., [2015](#bib.bib113)) | (Kisilev et al., [2016](#bib.bib77);
    Zeng et al., [2020](#bib.bib158)) |'
- en: '| Inception V3 (Szegedy et al., [2016](#bib.bib131)) | (Singh et al., [2019](#bib.bib124))
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Inception V3 (Szegedy et al., [2016](#bib.bib131)) | (Singh et al., [2019](#bib.bib124))
    |'
- en: '| GoogLeNet (Szegedy et al., [2015](#bib.bib130)) | (Shin et al., [2016](#bib.bib121))
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| GoogLeNet (Szegedy et al., [2015](#bib.bib130)) | (Shin et al., [2016](#bib.bib121))
    |'
- en: '| MobileNet V2 (Howard et al., [2017](#bib.bib60)) | (Harzig et al., [2019b](#bib.bib50))
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet V2 (Howard et al., [2017](#bib.bib60)) | (Harzig et al., [2019b](#bib.bib50))
    |'
- en: '| SRN (Zhu et al., [2017a](#bib.bib167)) | (Gu et al., [2019](#bib.bib45))
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SRN (Zhu et al., [2017a](#bib.bib167)) | (Gu et al., [2019](#bib.bib45))
    |'
- en: '| U-Net (Ronneberger et al., [2015](#bib.bib117)) | (Sun et al., [2019](#bib.bib129))
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| U-Net (Ronneberger et al., [2015](#bib.bib117)) | (Sun et al., [2019](#bib.bib129))
    |'
- en: '| EcNet ${}^{\textrm{(*)}}$ | (Zhang et al., [2017a](#bib.bib164)) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| EcNet ${}^{\textrm{(*)}}$ | (Zhang et al., [2017a](#bib.bib164)) |'
- en: '| FCN + shallow CNN ${}^{\textrm{(*)}}$ | (Tian et al., [2018](#bib.bib132))
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| FCN + 浅层 CNN ${}^{\textrm{(*)}}$ | (Tian et al., [2018](#bib.bib132))'
- en: '| RGAN ${}^{\textrm{(*)}}$ | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| RGAN ${}^{\textrm{(*)}}$ | (Han et al., [2018](#bib.bib48)) |'
- en: '| StackGAN (Zhang et al., [2017b](#bib.bib160)) (slightly modified version)
    ${}^{\textrm{(*)}}$ | (Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| StackGAN (Zhang et al., [2017b](#bib.bib160))（稍作修改版）${}^{\textrm{(*)}}$ |
    (Spinks 和 Moens, [2019](#bib.bib127)) |'
- en: '| CNN ${}^{\textrm{(*)}}$ | (Tian et al., [2019](#bib.bib133); Spinks and Moens,
    [2019](#bib.bib127)) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| CNN ${}^{\textrm{(*)}}$ | (Tian et al., [2019](#bib.bib133); Spinks 和 Moens,
    [2019](#bib.bib127)) |'
- en: '| CNN (unspecified architecture) | (Xie et al., [2019](#bib.bib151); Wu et al.,
    [2017](#bib.bib149)) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| CNN（未指定架构） | (Xie et al., [2019](#bib.bib151); Wu et al., [2017](#bib.bib149))
    |'
- en: 'Table 4\. Summary of convolutional neural network architectures used in the
    literature. RGAN stands for recurrent generative adversarial network, FCN for
    fully convolutional network and EcNet is the name given in MDNet (Zhang et al.,
    [2017a](#bib.bib164)) to the custom CNN used. ${}^{\textrm{(*)}}$: indicates an
    ad hoc architecture.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4\. 文献中使用的卷积神经网络架构总结。RGAN 代表递归生成对抗网络，FCN 代表全卷积网络，EcNet 是 MDNet (Zhang et al.,
    [2017a](#bib.bib164)) 中给定的自定义 CNN 名称。${}^{\textrm{(*)}}$: 表示临时架构。'
- en: We observe a wide variety of CNN architectures used in the literature, though
    most works employ standard designs. Table [4](#S5.T4 "Table 4 ‣ 5.2.2\. Visual
    Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on
    Deep Learning and Explainability for Automatic Report Generation from Medical
    Images") presents a summary. The most common ones are ResNet (11 works), VGG (11
    works), and DenseNet (9 works). Other standard architectures used are Faster R-CNN,
    Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and
    U-Net. Five works used ad hoc architectures not previously published (marked with
    (*) in Table [4](#S5.T4 "Table 4 ‣ 5.2.2\. Visual Component ‣ 5.2\. Model Design
    ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")). For example, EcNet is
    an ad hoc architecture used in MDNet (Zhang et al., [2017a](#bib.bib164)) and
    was proposed as an improvement over ResNet. However, the authors acknowledged
    that its design resembles DenseNet, which was published the same year (2017).
    RGAN, proposed by Han et al. (Han et al., [2018](#bib.bib48)), is a novel architecture
    that follows the generative adversarial network (GAN) (Goodfellow et al., [2014](#bib.bib42))
    approach, with a generative module comprising the encoder and decoder parts of
    an atrous convolution autoencoder (ACAE) with a spatial LSTM between them. Similarly,
    Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) used a slightly modified
    version of a StackGAN (Zhang et al., [2017b](#bib.bib160)) to learn the mapping
    from report encoding to chest X-ray images, and a custom CNN to learn the inverse
    mapping. Both are trained together, but only the latter is part of the report
    generation pipeline during inference.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到文献中使用了各种 CNN 架构，尽管大多数工作采用了标准设计。表 [4](#S5.T4 "表 4 ‣ 5.2.2\. 视觉组件 ‣ 5.2\.
    模型设计 ‣ 5\. 论文分析 ‣ 关于深度学习和自动报告生成的解释性调查") 展示了总结。最常见的是 ResNet（11 项工作）、VGG（11 项工作）和
    DenseNet（9 项工作）。其他使用的标准架构包括 Faster R-CNN、Inception V3、GoogLeNet、MobileNet V2、空间正则化网络（SRN）和
    U-Net。五项工作使用了以前未发布的特定架构（在表 [4](#S5.T4 "表 4 ‣ 5.2.2\. 视觉组件 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析
    ‣ 关于深度学习和自动报告生成的解释性调查") 中标记为（*））。例如，EcNet 是在 MDNet（Zhang 等，[2017a](#bib.bib164)）中使用的特定架构，被提出作为对
    ResNet 的改进。然而，作者承认其设计类似于 DenseNet，后者在同一年（2017）发布。RGAN 由 Han 等（Han 等，[2018](#bib.bib48)）提出，是一种新型架构，采用生成对抗网络（GAN）（Goodfellow
    等，[2014](#bib.bib42)）的方法，生成模块包括一个包含空间 LSTM 的 atrous 卷积自编码器（ACAE）的编码器和解码器部分。类似地，Spinks
    和 Moens（Spinks 和 Moens，[2019](#bib.bib127)）使用了略微修改版的 StackGAN（Zhang 等，[2017b](#bib.bib160)）来学习从报告编码到胸部
    X 光图像的映射，并使用自定义 CNN 来学习逆向映射。两者一起训练，但只有后者在推理过程中是报告生成管道的一部分。
- en: 5.2.3\. Language Component
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 语言组件
- en: '| Architecture | Used by papers |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 被文献使用 |'
- en: '| --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GRU | (Shin et al., [2016](#bib.bib121)) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| GRU | （Shin 等，[2016](#bib.bib121)） |'
- en: '| LSTM | (Singh et al., [2019](#bib.bib124); Gu et al., [2019](#bib.bib45);
    Gasimova, [2019](#bib.bib41); Zeng et al., [2018](#bib.bib159); Sun et al., [2019](#bib.bib129);
    Li and Hong, [2019](#bib.bib88); Shin et al., [2016](#bib.bib121); Hasan et al.,
    [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149); Zeng et al., [2020](#bib.bib158))
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | （Singh 等，[2019](#bib.bib124)；Gu 等，[2019](#bib.bib45)；Gasimova，[2019](#bib.bib41)；Zeng
    等，[2018](#bib.bib159)；Sun 等，[2019](#bib.bib129)；Li 和 Hong，[2019](#bib.bib88)；Shin
    等，[2016](#bib.bib121)；Hasan 等，[2018b](#bib.bib52)；Wu 等，[2017](#bib.bib149)；Zeng
    等，[2020](#bib.bib158)）'
- en: '| LSTM with attention | (Wang et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164);
    Li et al., [2019a](#bib.bib90); Gale et al., [2019](#bib.bib39); Tian et al.,
    [2018](#bib.bib132)) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 带注意力的 LSTM | （Wang 等，[2018](#bib.bib145)；Zhang 等，[2017a](#bib.bib164)；Li
    等，[2019a](#bib.bib90)；Gale 等，[2019](#bib.bib39)；Tian 等，[2018](#bib.bib132)） |'
- en: '| Hierarchical LSTM with attention | (Jing et al., [2018](#bib.bib69); Liu
    et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157);
    Zhang et al., [2020b](#bib.bib163); Yin et al., [2019](#bib.bib156); Tian et al.,
    [2019](#bib.bib133)) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 带注意力的层次结构 LSTM | （Jing 等，[2018](#bib.bib69)；Liu 等，[2019](#bib.bib93)；Huang
    等，[2019](#bib.bib62)；Yuan 等，[2019](#bib.bib157)；Zhang 等，[2020b](#bib.bib163)；Yin
    等，[2019](#bib.bib156)；Tian 等，[2019](#bib.bib133)） |'
- en: '| Hierarchical: Sentence LSTM + Dual Word LSTM (normal/abnormal) | (Harzig
    et al., [2019a](#bib.bib49); Xie et al., [2019](#bib.bib151); Jing et al., [2019](#bib.bib68))
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 层次结构：句子 LSTM + 双重词汇 LSTM（正常/异常） | （Harzig 等，[2019a](#bib.bib49)；Xie 等，[2019](#bib.bib151)；Jing
    等，[2019](#bib.bib68)） |'
- en: '| Recurrent BiLSTM-attention-LSTM | (Xue et al., [2018](#bib.bib155); Maksoud
    et al., [2019](#bib.bib96); Xue and Huang, [2019](#bib.bib154)) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 递归BiLSTM-注意力-LSTM | (Xue et al., [2018](#bib.bib155); Maksoud et al., [2019](#bib.bib96);
    Xue and Huang, [2019](#bib.bib154)) |'
- en: '| Partial report encoding + FC layer (next word) | (Alsharid et al., [2019](#bib.bib8);
    Gajbhiye et al., [2020](#bib.bib37)) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 部分报告编码 + FC层（下一个词） | (Alsharid et al., [2019](#bib.bib8); Gajbhiye et al.,
    [2020](#bib.bib37)) |'
- en: '| Transformer | (Xiong et al., [2019](#bib.bib152)) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | (Xiong et al., [2019](#bib.bib152)) |'
- en: '| ARAE | (Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ARAE | (Spinks and Moens, [2019](#bib.bib127)) |'
- en: '| Template based | (Ma et al., [2018](#bib.bib95); Harzig et al., [2019b](#bib.bib50);
    Han et al., [2018](#bib.bib48); Kisilev et al., [2016](#bib.bib77); Moradi et al.,
    [2016](#bib.bib99)) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 基于模板 | (Ma et al., [2018](#bib.bib95); Harzig et al., [2019b](#bib.bib50);
    Han et al., [2018](#bib.bib48); Kisilev et al., [2016](#bib.bib77); Moradi et
    al., [2016](#bib.bib99)) |'
- en: '| Hybrid template retrieval + generation/edition | (Li et al., [2019b](#bib.bib87);
    Biswal et al., [2020](#bib.bib17); Li et al., [2018](#bib.bib86)) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 混合模板检索 + 生成/编辑 | (Li et al., [2019b](#bib.bib87); Biswal et al., [2020](#bib.bib17);
    Li et al., [2018](#bib.bib86)) |'
- en: Table 5\. Summary of language component architectures used in the literature.
    ARAE stands for adversarially regularized autoencoder.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表5\. 文献中使用的语言组件架构总结。ARAE代表对抗正则化自编码器。
- en: The job of the language component is to generate the report. In contrast to
    the visual component, in the literature we find a greater variety of approaches
    and creative ideas applied to this component. Table [5](#S5.T5 "Table 5 ‣ 5.2.3\.
    Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images") presents a high-level summary of this analysis.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 语言组件的工作是生成报告。与视觉组件不同的是，文献中对这个组件采用了更多种类的方法和创意。表[5](#S5.T5 "Table 5 ‣ 5.2.3\.
    Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images")提供了这一分析的高级总结。
- en: The simplest approach is the use of a recurrent neural network, such as LSTM
    or GRU, to generate the full report word by word. Nine works (Singh et al., [2019](#bib.bib124);
    Gu et al., [2019](#bib.bib45); Gasimova, [2019](#bib.bib41); Zeng et al., [2018](#bib.bib159);
    Sun et al., [2019](#bib.bib129); Li and Hong, [2019](#bib.bib88); Hasan et al.,
    [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149); Zeng et al., [2020](#bib.bib158))
    used LSTM and one work (Shin et al., [2016](#bib.bib121)) tried both GRU and LSTM.
    All these works have in common that the GRU/LSTM receives an encoding vector from
    the visual component at the beginning and the full report is decoded from it.
    This encoding vector is typically a vector of global features output by the CNN.
    However, two of these works (Gu et al., [2019](#bib.bib45); Sun et al., [2019](#bib.bib129))
    compute a weighted sum of tag embedding vectors and provide that as input to the
    LSTM. Five works (Wang et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164);
    Li et al., [2019a](#bib.bib90); Gale et al., [2019](#bib.bib39); Tian et al.,
    [2018](#bib.bib132)) used LSTM enhanced with an attention mechanism. In addition
    to the initial input, the LSTM equipped with attention can selectively attend
    to visual features from the visual component at each recurrent step. This typically
    leads to improved performance in all papers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用递归神经网络，如LSTM或GRU，逐字生成完整报告。九篇工作（Singh et al., [2019](#bib.bib124); Gu
    et al., [2019](#bib.bib45); Gasimova, [2019](#bib.bib41); Zeng et al., [2018](#bib.bib159);
    Sun et al., [2019](#bib.bib129); Li and Hong, [2019](#bib.bib88); Hasan et al.,
    [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149); Zeng et al., [2020](#bib.bib158))
    使用了LSTM，一篇工作（Shin et al., [2016](#bib.bib121)）尝试了GRU和LSTM。所有这些工作都有一个共同点，即GRU/LSTM在开始时接收来自视觉组件的编码向量，并从中解码完整报告。这个编码向量通常是CNN输出的全局特征向量。然而，其中的两个工作（Gu
    et al., [2019](#bib.bib45); Sun et al., [2019](#bib.bib129)）计算标签嵌入向量的加权和，并将其作为LSTM的输入。五篇工作（Wang
    et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90);
    Gale et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132)）使用了增强了注意力机制的LSTM。除了初始输入，配备了注意力机制的LSTM可以在每个递归步骤中选择性地关注来自视觉组件的视觉特征。这通常会导致所有论文中性能的提升。
- en: 'A known problem for recurrent networks such as LSTM is that they are not very
    good at generating very long texts (Pascanu et al., [2013](#bib.bib104)). This
    is not a worrying issue when reports are short, however, it can become one for
    long multi-sentence reports. Two papers (Zhang et al., [2017a](#bib.bib164); Tian
    et al., [2018](#bib.bib132)) worked around this by generating each sentence independently
    with a single LSTM and then concatenating these sentences together. They accomplished
    this by providing the LSTM with a vector that indicates the sentence type as first
    input. This worked well in their case because the models were designed for structured
    reports, i.e., a fixed number of sentences per report and a fixed topic per sentence.
    Vispi (Li et al., [2019a](#bib.bib90)) adopts a similar strategy: for each disease
    a dedicated LSTM generates the corresponding sentence, and the final report is
    the concatenation of them.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个已知的问题是，像 LSTM 这样的递归网络在生成非常长的文本时效果不佳（Pascanu 等，[2013](#bib.bib104)）。当报告较短时这并不是一个令人担忧的问题，但对于长的多句报告来说，这可能会成为一个问题。两篇论文（Zhang
    等，[2017a](#bib.bib164)；Tian 等，[2018](#bib.bib132)）通过独立生成每个句子并将这些句子连接在一起来解决了这个问题。他们通过向
    LSTM 提供一个指示句子类型的向量作为初始输入来实现这一点。这在他们的案例中效果很好，因为这些模型是为结构化报告设计的，即每份报告固定数量的句子和每个句子固定的主题。Vispi
    (Li 等，[2019a](#bib.bib90)) 采用了类似的策略：为每种疾病生成相应的句子，最终报告就是这些句子的连接。
- en: '![Refer to caption](img/bf018f99619a8e5d62105ef5a8e55cda.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf018f99619a8e5d62105ef5a8e55cda.png)'
- en: Figure 3\. Illustration of a model following the Hierarchical LSTM with attention
    approach, with attention at the sentence level. The visual component consists
    of a CNN. The global features vector can be computed from the local features in
    many ways, e.g. global average pooling. In each step the sentence LSTM generates
    a topic vector representing the current sentence, and decides whether to stop
    (S) generating or continue (C).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 说明了一个遵循分层 LSTM 注意力方法的模型，其中的注意力机制作用于句子级别。视觉组件由 CNN 组成。全局特征向量可以通过多种方式从局部特征中计算得出，例如全局平均池化。在每一步，句子
    LSTM 生成一个表示当前句子的主题向量，并决定是否停止 (S) 生成或继续 (C)。
- en: 'To tackle the generation of unstructured multi-sentence reports, a group of
    papers followed what we call the Hierarchical LSTM with attention approach: a
    Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives
    a topic vector and generates a sentence word by word. In this setting, the attention
    mechanism can be present at the sentence level, the word level or both. Figure
    [3](#S5.F3 "Figure 3 ‣ 5.2.3\. Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images") shows an illustrative example. Seven works
    (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93); Huang et al.,
    [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Zhang et al., [2020b](#bib.bib163);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133)) followed this
    approach. A common result in these papers is that a Hierarchical LSTM yields better
    performance in multi-sentence report generation than a single, flat LSTM. A few
    papers (Harzig et al., [2019a](#bib.bib49); Xie et al., [2019](#bib.bib151); Jing
    et al., [2019](#bib.bib68)) went one step further and replaced the normal Word
    LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level
    that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly)
    or a healthy case. Thus, there are two Word LSTMs, one for normal and one for
    abnormal sentences. The goal is to improve the generation of abnormal sentences
    by having a Word LSTM that specializes in generating them. In contrast, a single
    Word LSTM for everything can lead to overlearning of normal sentences and underlearning
    of abnormal ones, as the latter are typically less frequent due to class imbalances
    in datasets. The ablation analyses of these works show performance gains, thanks
    to this approach.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决无结构多句报告的生成问题，一些论文采用了我们称之为层次 LSTM 结合注意力的方法：一个句子 LSTM 生成一系列主题向量，而一个词 LSTM
    接收主题向量并逐词生成句子。在这种设置下，注意力机制可以存在于句子级别、词级别或两者都存在。图 [3](#S5.F3 "Figure 3 ‣ 5.2.3\.
    Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images") 展示了一个示例。七项研究（Jing 等，[2018](#bib.bib69)；Liu 等，[2019](#bib.bib93)；Huang
    等，[2019](#bib.bib62)；Yuan 等，[2019](#bib.bib157)；Zhang 等，[2020b](#bib.bib163)；Yin
    等，[2019](#bib.bib156)；Tian 等，[2019](#bib.bib133)）采用了这种方法。这些论文中的一个共同结果是，层次 LSTM
    在多句报告生成方面的表现优于单一的平面 LSTM。一些论文（Harzig 等，[2019a](#bib.bib49)；Xie 等，[2019](#bib.bib151)；Jing
    等，[2019](#bib.bib68)）更进一步，用双重词 LSTM 替代了普通的词 LSTM：该模型在句子级别具有一个门控机制，用于决定句子是否描述异常（例如，检测到的心脏肥大）或正常情况。因此，有两个词
    LSTM，一个用于正常句子，一个用于异常句子。目标是通过让专门生成异常句子的词 LSTM 来提高异常句子的生成。相比之下，单一的词 LSTM 可能会导致对正常句子的过度学习，而对异常句子的学习不足，因为异常句子通常由于数据集中的类别不平衡而较少出现。这些工作的消融分析显示，由于这种方法，性能得到了提升。
- en: Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM
    approach. The basic idea is to have a LSTM generate one sentence at a time, each
    time conditioned on a BiLSTM based encoding of the previous sentence and the output
    of an attention mechanism. The process is repeated recurrently sentence by sentence
    until the full report is generated. Three papers used this approach (Maksoud et al.,
    [2019](#bib.bib96); Xue et al., [2018](#bib.bib155); Xue and Huang, [2019](#bib.bib154)).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种多句报告生成方法是递归的 BiLSTM-注意力-LSTM 方法。基本思路是让 LSTM 一次生成一句话，每次都基于 BiLSTM 对前一句话的编码和注意力机制的输出。这个过程会递归地进行，每次生成一句话，直到完整的报告生成。三篇论文使用了这种方法（Maksoud
    等，[2019](#bib.bib96)；Xue 等，[2018](#bib.bib155)；Xue 和 Huang，[2019](#bib.bib154)）。
- en: Two works (Alsharid et al., [2019](#bib.bib8); Gajbhiye et al., [2020](#bib.bib37))
    approached report generation as simply learning to predict the next word given
    a partial report and an image. The models have dedicated components, such as LSTM
    and BiLSTM, for encoding the partial report and the image, and the next word is
    predicted by an FC layer. This approach simplifies the task (i.e., predict the
    next word given everything that comes before), but in practice requires that the
    model be applied recurrently one word at a time to produce a full report, which
    has quadratic instead of linear complexity.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 两项工作（Alsharid 等，[2019](#bib.bib8)；Gajbhiye 等，[2020](#bib.bib37)）将报告生成视为简单地学习在给定部分报告和图像的情况下预测下一个词。这些模型具有专门的组件，如
    LSTM 和 BiLSTM，用于编码部分报告和图像，接下来的词通过 FC 层进行预测。这种方法简化了任务（即，预测所有前面的内容给定的下一个词），但实际上要求模型每次递归应用一个词来生成完整报告，这具有二次复杂度而非线性复杂度。
- en: Only one work, RTMIC (Xiong et al., [2019](#bib.bib152)), has explored the use
    of the Transformer (Vaswani et al., [2017](#bib.bib140)) architecture for report
    generation. In RTMIC multiple image crops are obtained using Grad-CAM, then from
    each crop a feature vector is obtained, and finally a Transformer converts these
    vectors into a report. The paper’s results show some performance gains in CIDEr
    and BLEU with respect to some baselines that do not use the Transformer. Likewise,
    Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) were the only ones to
    use an adversarially regularized autoencoder (ARAE) (Zhao et al., [2017](#bib.bib165))
    to generate reports. Their model combines an ARAE with a StackGAN and a normal
    CNN, achieving better performance than a convolutional caption generation baseline
    in several NLP metrics.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一项工作，即 RTMIC（Xiong 等，[2019](#bib.bib152)），探索了使用 Transformer（Vaswani 等，[2017](#bib.bib140)）架构来生成报告。在
    RTMIC 中，通过 Grad-CAM 获得多个图像裁剪区域，然后从每个裁剪区域获取一个特征向量，最后 Transformer 将这些向量转换为报告。论文结果显示，与一些不使用
    Transformer 的基线相比，在 CIDEr 和 BLEU 上取得了一些性能提升。同样，Spinks 和 Moens（Spinks 和 Moens，[2019](#bib.bib127)）是唯一使用对抗正则化自编码器（ARAE）（Zhao
    等，[2017](#bib.bib165)）生成报告的人。他们的模型将 ARAE 与 StackGAN 和普通 CNN 结合，在多个 NLP 指标中取得了比卷积标题生成基线更好的性能。
- en: 'We also identify a group of papers (Ma et al., [2018](#bib.bib95); Harzig et al.,
    [2019b](#bib.bib50); Han et al., [2018](#bib.bib48); Kisilev et al., [2016](#bib.bib77);
    Moradi et al., [2016](#bib.bib99)) following a Template based approach. The language
    component in these works operates programmatically by following if-then rules
    or other heuristics in order to retrieve, fill and/or combine templates from a
    database in order to generate a report. The visual component typically outputs
    discrete classification labels that the language component processes programmatically.
    In the case of Harzig et al. 2019b (Harzig et al., [2019b](#bib.bib50)), image
    localizations per class are also recovered using CAM (Zhou et al., [2016](#bib.bib166)),
    and in the case of Han et al. (Han et al., [2018](#bib.bib48)) the visual component
    outputs an image segmentation. In both cases the language component includes special
    localization-based rules or templates, thus incorporating location information
    in the generated report. Kisilev et al. (Kisilev et al., [2016](#bib.bib77)) followed
    a different approach: a multi-layer perceptron learns to map image encodings to
    doc2vec (Le and Mikolov, [2014](#bib.bib84)) representations of corresponding
    reports. During inference, the ground-truth report with the closest doc2vec representation
    is retrieved.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还识别了一组论文（Ma 等，[2018](#bib.bib95)；Harzig 等，[2019b](#bib.bib50)；Han 等，[2018](#bib.bib48)；Kisilev
    等，[2016](#bib.bib77)；Moradi 等，[2016](#bib.bib99)）采用基于模板的方法。这些工作中的语言组件通过遵循 if-then
    规则或其他启发式方法程序化地操作，以从数据库中检索、填充和/或组合模板以生成报告。视觉组件通常输出离散分类标签，语言组件程序化地处理这些标签。在 Harzig
    等人 2019b（Harzig 等，[2019b](#bib.bib50)）的情况下，还使用 CAM（Zhou 等，[2016](#bib.bib166)）恢复每个类别的图像定位；在
    Han 等人（Han 等，[2018](#bib.bib48)）的情况下，视觉组件输出图像分割。在这两种情况下，语言组件包括特殊的基于定位的规则或模板，从而将位置信息纳入生成的报告中。Kisilev
    等人（Kisilev 等，[2016](#bib.bib77)）采用了不同的方法：多层感知机学习将图像编码映射到 doc2vec（Le 和 Mikolov，[2014](#bib.bib84)）表示的相应报告。在推理过程中，检索到与
    doc2vec 表示最接近的真实报告。
- en: 'Lastly, we identify three papers (Li et al., [2019b](#bib.bib87); Biswal et al.,
    [2020](#bib.bib17); Li et al., [2018](#bib.bib86)) following the Hybrid template
    retrieval + generation/edition approach. These works seek to combine the benefits
    of templates with the flexibility of a generative module to either generate sentences
    from scratch or paraphrase templates as needed on a case-by-case basis. KERP (Li
    et al., [2019b](#bib.bib87)) uses Graph Transformers (GTR) to map the visual input
    into a sequence of templates from a curated database. A Paraphrase GTR then maps
    each template to its paraphrased version. HRGR (Li et al., [2018](#bib.bib86))
    follows the hierarchical LSTM approach with a twist—it replaces the Word LSTM
    with a gate module that chooses between two options: retrieving a template or
    generating a sentence from scratch (via a Word LSTM). Lastly, CLARA (Biswal et al.,
    [2020](#bib.bib17)) is somewhat different, as it was designed as an interactive
    tool to assist a human to write reports. A human introduces anchor words and the
    prefix of a sentence, and Lucene (Branko et al., [2010](#bib.bib19)) processes
    them as a query to retrieve sentence templates from a database. A sequence-to-sequence
    network then reads and paraphrases each sentence template to get the final report.
    CLARA can also operate fully automatically by receiving an empty prefix and predicting
    the anchor words itself. According to reported results, the model consistently
    achieved better performance than many baselines.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们识别出三篇论文（Li et al., [2019b](#bib.bib87); Biswal et al., [2020](#bib.bib17);
    Li et al., [2018](#bib.bib86)）采用了混合模板检索+生成/编辑的方法。这些研究旨在结合模板的优点和生成模块的灵活性，根据具体情况生成句子或对模板进行释义。KERP（Li
    et al., [2019b](#bib.bib87)）使用图形变换器（GTR）将视觉输入映射到来自策划数据库的模板序列。然后，释义GTR将每个模板映射到其释义版本。HRGR（Li
    et al., [2018](#bib.bib86)）采用了层次LSTM方法的变体——它用一个门控模块取代了词汇LSTM，选择两个选项中的一个：检索一个模板或从头生成一个句子（通过词汇LSTM）。最后，CLARA（Biswal
    et al., [2020](#bib.bib17)）有所不同，它被设计为一种互动工具，帮助人类编写报告。人类引入锚词和句子的前缀，Lucene（Branko
    et al., [2010](#bib.bib19)）将其处理为查询，从数据库中检索句子模板。然后，序列到序列网络读取并释义每个句子模板以生成最终报告。CLARA
    还可以通过接收一个空前缀并自行预测锚词来完全自动化操作。根据报告结果，该模型的表现始终优于许多基准模型。
- en: 5.2.4\. Domain knowledge
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4. 领域知识
- en: 'Although all works used datasets from the medical domain to train their models,
    which can be considered a form of domain knowledge transfer, some works took special
    steps to explicitly incorporate additional knowledge from experts into their design.
    Concretely, we identify two incipient trends in the application of domain knowledge:
    1) the use of graph neural networks right after the CNN, providing an architectural
    bias to guide the model to identify medical concepts and their relations from
    the images; and 2) enhancing the model’s report generation with access to an external
    template database curated by experts.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有研究都使用了来自医学领域的数据集来训练他们的模型，这可以视为一种领域知识转移，但有些研究采取了特别的步骤，将专家的额外知识明确地融入到他们的设计中。具体而言，我们识别出领域知识应用中的两个初期趋势：1）在CNN之后使用图神经网络，为模型提供架构上的偏置，以引导其从图像中识别医学概念及其关系；2）通过访问专家策划的外部模板数据库，增强模型的报告生成能力。
- en: KERP (Li et al., [2019b](#bib.bib87)) incorporates knowledge at the architectural
    level using graph neural networks. The authors manually designed an abnormality
    graph and a disease graph, where each node represents an abnormality or disease,
    and the edges are built based on their co-occurrences in the training set. Some
    example abnormalities are “low lung volumes” and “enlarged heart size”, whereas
    diseases represent a higher level of abstraction, for example “emphysema” or “consolidation”.
    The information flows from image features (encoded by a CNN) to the abnormality
    graph, and then to the disease graph, via inter-node message passing. This biases
    the network to encode the visual information in terms of abnormalities, diseases
    and their relations. Similarly, Zhang et al. (Zhang et al., [2020b](#bib.bib163))
    created an observations graph, containing 20 nodes of chest abnormalities or body
    parts, where conditions related to the same organ or tissue are connected by edges.
    Their ablation analysis showed some performance gains, thanks to the graph neural
    network.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: KERP (Li et al., [2019b](#bib.bib87)) 使用图神经网络在架构层面上整合知识。作者手动设计了一个异常图和一个疾病图，其中每个节点代表一个异常或疾病，边是根据它们在训练集中的共现关系构建的。一些示例异常包括“肺容积过低”和“心脏尺寸增大”，而疾病则代表了一个更高层次的抽象，例如“肺气肿”或“实变”。信息从图像特征（由CNN编码）流向异常图，然后通过节点间消息传递流向疾病图。这使得网络倾向于以异常、疾病及其关系的形式编码视觉信息。类似地，Zhang
    et al. (Zhang et al., [2020b](#bib.bib163)) 创建了一个观察图，包含20个胸部异常或身体部位的节点，其中与相同器官或组织相关的条件通过边连接。它们的消融分析显示出一些性能提升，这得益于图神经网络。
- en: In seven works (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Biswal et al.,
    [2020](#bib.bib17); Harzig et al., [2019b](#bib.bib50); Han et al., [2018](#bib.bib48);
    Kisilev et al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99)) the authors
    provided their models with a curated set of template sentences that are further
    processed in the language component to output a full report. Three works (Han
    et al., [2018](#bib.bib48); Harzig et al., [2019b](#bib.bib50); Kisilev et al.,
    [2016](#bib.bib77)) used manually curated templates and if-then based programs
    to select and fill them. CLARA (Biswal et al., [2020](#bib.bib17)) uses a database
    indexing all sentences from the training set reports for text-based retrieval,
    which are then paraphrased by a generative module. Similarly, KERP (Li et al.,
    [2019b](#bib.bib87)) has access to a template database mined from the training
    set, which are also paraphrased later. In HRGR (Li et al., [2018](#bib.bib86))
    the most common sentences in the datasets were mined and then manually grouped
    by meaning to further reduce repetitions. In this work the authors showed that
    HRGR learned to prefer templates about 80% of the time and only generate sentences
    from scratch the remaining 20%, suggesting that templates can be quite useful
    to generate most sentences in reports.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在七篇工作中（Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Biswal et al., [2020](#bib.bib17);
    Harzig et al., [2019b](#bib.bib50); Han et al., [2018](#bib.bib48); Kisilev et
    al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99)），作者为他们的模型提供了一组经过精心编排的模板句子，这些句子在语言组件中进一步处理以生成完整报告。三篇工作（Han
    et al., [2018](#bib.bib48); Harzig et al., [2019b](#bib.bib50); Kisilev et al.,
    [2016](#bib.bib77)）使用了手动编排的模板和基于if-then的程序来选择和填写这些模板。CLARA (Biswal et al., [2020](#bib.bib17))
    使用一个数据库索引所有来自训练集报告的句子以进行基于文本的检索，然后由生成模块进行意译。类似地，KERP (Li et al., [2019b](#bib.bib87))
    访问了一个从训练集中挖掘的模板数据库，这些模板随后也会被意译。在HRGR (Li et al., [2018](#bib.bib86)) 中，数据集中的最常见句子被挖掘出来，然后按意义手动分组，以进一步减少重复。在这项工作中，作者展示了HRGR学习到大约80%的时间倾向于使用模板，只有剩下的20%时间从头生成句子，这表明模板在生成报告中的大多数句子时非常有用。
- en: 5.2.5\. Auxiliary Tasks
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 辅助任务
- en: Although the main objective in most papers is to learn a model for report generation
    from medical images, many works also include and optimize auxiliary tasks to boost
    their performance. A summary of these tasks is presented in Table [10](#S9.T10
    "Table 10 ‣ 9.2\. Auxiliary Tasks ‣ 9\. Supplementary Material ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images")
    in appendix [9.2](#S9.SS2 "9.2\. Auxiliary Tasks ‣ 9\. Supplementary Material
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"). The most common auxiliary tasks are multi-label (16 papers)
    and single-label (11 papers) classification. These tasks are generally intended
    to provide additional supervision to the model’s visual component, in order to
    improve the CNN’s capabilities to extract quality visual features. Some common
    tasks are identifying the presence or absence of different abnormalities, diseases,
    organs, body parts, medical concepts, detecting image modality, etc. Datasets
    often used for this purpose are ChestX-ray14 (Wang et al., [2017](#bib.bib144))
    and CheXpert (Irvin et al., [2019](#bib.bib64)), where the common practice is
    to pretrain the CNN in those datasets before moving on to report generation. Many
    papers report better performance in report generation thanks to these auxiliary
    classification tasks. The three works (Harzig et al., [2019a](#bib.bib49); Xie
    et al., [2019](#bib.bib151); Jing et al., [2019](#bib.bib68)) following the hierarchical
    approach with Dual Word LSTM used a classification task to supervise the gating
    mechanism that chooses between generating a normal sentence, an abnormal sentence
    or stopping. Two models (Tian et al., [2018](#bib.bib132); Han et al., [2018](#bib.bib48))
    perform a segmentation task. Tian et al. (Tian et al., [2018](#bib.bib132)) trained
    a fully convolutional network (FCN) with segmentation masks of a liver and tumor,
    and Han et al. (Han et al., [2018](#bib.bib48)) trained an RGAN for pixel level
    classification. Similarly, two models (Kisilev et al., [2016](#bib.bib77); Zeng
    et al., [2020](#bib.bib158)) use a Faster-RCNN (Ren et al., [2015](#bib.bib113))
    trained for detection and classification of bounding boxes enclosing lesions or
    other regions of interest in the images.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数论文的主要目标是从医学图像中学习生成报告的模型，但许多研究还包括并优化辅助任务以提升模型性能。这些任务的总结见附录[9.2](#S9.SS2
    "9.2\. Auxiliary Tasks ‣ 9\. Supplementary Material ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")中的表[10](#S9.T10
    "Table 10 ‣ 9.2\. Auxiliary Tasks ‣ 9\. Supplementary Material ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images")。最常见的辅助任务是多标签（16篇论文）和单标签（11篇论文）分类。这些任务通常旨在为模型的视觉组件提供额外的监督，以提升CNN提取优质视觉特征的能力。一些常见的任务包括识别不同异常、疾病、器官、身体部位、医学概念的存在与否，检测图像模态等。为此目的常用的数据集包括ChestX-ray14（Wang
    et al., [2017](#bib.bib144)）和CheXpert（Irvin et al., [2019](#bib.bib64)），其中常见的做法是在转向报告生成之前，在这些数据集上预训练CNN。许多论文报告了由于这些辅助分类任务，报告生成性能得到了改善。三项工作（Harzig
    et al., [2019a](#bib.bib49)；Xie et al., [2019](#bib.bib151)；Jing et al., [2019](#bib.bib68)）采用了双向词LSTM的层次化方法，使用分类任务来监督选择生成正常句子、异常句子或停止的门控机制。两项模型（Tian
    et al., [2018](#bib.bib132)；Han et al., [2018](#bib.bib48)）执行了分割任务。Tian et al.（Tian
    et al., [2018](#bib.bib132)）使用肝脏和肿瘤的分割掩膜训练了一个全卷积网络（FCN），而Han et al.（Han et al.,
    [2018](#bib.bib48)）训练了一个用于像素级分类的RGAN。类似地，两项模型（Kisilev et al., [2016](#bib.bib77)；Zeng
    et al., [2020](#bib.bib158)）使用了一个经过训练的Faster-RCNN（Ren et al., [2015](#bib.bib113)），用于检测和分类图像中包含病变或其他感兴趣区域的边界框。
- en: 'Two works (Maksoud et al., [2019](#bib.bib96); Yin et al., [2019](#bib.bib156))
    used regularization supervision on attention weights. CORAL8 (Maksoud et al.,
    [2019](#bib.bib96)) receives regularization supervision on its visual attention
    weights to prevent them from degrading into uniform distribution, which would
    offer no advantage over average pooling. Similarly, Yin et al. (Yin et al., [2019](#bib.bib156))
    added two regularizations to their model’s attention weights: one on the weights
    over spatial visual features and another on the weights over tag embedding vectors.
    In both works the attention supervision provided a significant contribution to
    the performance.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 两项工作（Maksoud et al., [2019](#bib.bib96)；Yin et al., [2019](#bib.bib156)）对注意力权重进行了正则化监督。CORAL8（Maksoud
    et al., [2019](#bib.bib96)）对其视觉注意力权重进行了正则化监督，以防止其退化为均匀分布，这样的分布对平均池化没有优势。类似地，Yin
    et al.（Yin et al., [2019](#bib.bib156)）对模型的注意力权重添加了两种正则化：一种用于空间视觉特征的权重，另一种用于标签嵌入向量的权重。在这两项工作中，注意力监督对性能提供了显著贡献。
- en: 'Two works (Yin et al., [2019](#bib.bib156); Moradi et al., [2016](#bib.bib99))
    included a task to enforce a matching between embeddings from two different sources.
    Yin et al. (Yin et al., [2019](#bib.bib156)) projected the topic vectors from
    the Sentence LSTM and the word embeddings from the respective ground-truth sentence
    into a common semantic space, and enforced a matching via contrastive loss (Chopra
    et al., [2005](#bib.bib25)). This task significantly improved the Sentence LSTM’s
    training and the model’s overall performance. Moradi et al. (Moradi et al., [2016](#bib.bib99))
    trained a MLP for mapping image visual encodings (obtained by a VGG network) to
    the vector representation of its corresponding ground-truth report (obtained via
    doc2vec (Le and Mikolov, [2014](#bib.bib84)), which in itself was another auxiliary
    task), by minimizing the Euclidean distance. The trained MLP was then used to
    predict doc2vec representations for unseen images and retrieve the report with
    the closest representation. Two works (Tian et al., [2019](#bib.bib133); Spinks
    and Moens, [2019](#bib.bib127)) used text autoencoders, which allow learning compact
    representations of unlabeled data in a self-supervised manner: an encoder network
    maps the input into a latent representation, and a decoder network has to recover
    the original input back. MTMA (Tian et al., [2019](#bib.bib133)) uses a BiLSTM
    to encode the sentences of the indication and findings sections of a report (input
    text), in order to generate the impression section (output). To improve the encoding
    quality of the BiLSTM, the authors trained the decoder branch of a hierarchical
    autoencoder (Li et al., [2015](#bib.bib89)) to recover the original sentence from
    the BiLSTM encoding. The experimental results showed that the autoencoder supervision
    provided a significant boost to the model’s performance. Spinks and Moens (Spinks
    and Moens, [2019](#bib.bib127)) trained an ARAE (Zhao et al., [2017](#bib.bib165))
    (1) to learn compact representations of reports (serving as input to a GAN that
    generates chest X-ray images) and (2) to recover a report given an arbitrary compact
    representation (used in inference mode for report generation).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 两项工作 (Yin et al., [2019](#bib.bib156); Moradi et al., [2016](#bib.bib99)) 包含了一个任务，用于强制执行来自两个不同来源的嵌入匹配。Yin
    et al. (Yin et al., [2019](#bib.bib156)) 将来自 Sentence LSTM 的主题向量和来自相应真实句子的词嵌入投影到一个共同的语义空间，并通过对比损失
    (Chopra et al., [2005](#bib.bib25)) 强制匹配。这项任务显著提高了 Sentence LSTM 的训练和模型的整体表现。Moradi
    et al. (Moradi et al., [2016](#bib.bib99)) 训练了一个 MLP 来将图像视觉编码 (通过 VGG 网络获得) 映射到其对应真实报告的向量表示
    (通过 doc2vec (Le 和 Mikolov, [2014](#bib.bib84)) 获得，该过程本身是另一个辅助任务)，通过最小化欧几里得距离。训练后的
    MLP 然后用于预测未见图像的 doc2vec 表示，并检索最接近的报告。两项工作 (Tian et al., [2019](#bib.bib133); Spinks
    和 Moens, [2019](#bib.bib127)) 使用文本自编码器，这允许以自我监督的方式学习未标记数据的紧凑表示：编码器网络将输入映射到潜在表示，而解码器网络则需要恢复原始输入。MTMA
    (Tian et al., [2019](#bib.bib133)) 使用 BiLSTM 对报告的指示和发现部分的句子 (输入文本) 进行编码，以生成印象部分
    (输出)。为了提高 BiLSTM 的编码质量，作者训练了层次自编码器 (Li et al., [2015](#bib.bib89)) 的解码器分支，以从 BiLSTM
    编码中恢复原始句子。实验结果显示，自编码器监督对模型性能提供了显著提升。Spinks 和 Moens (Spinks 和 Moens, [2019](#bib.bib127))
    训练了一个 ARAE (Zhao et al., [2017](#bib.bib165))， (1) 学习报告的紧凑表示 (作为生成胸部 X 光图像的 GAN
    的输入) 和 (2) 给定任意紧凑表示恢复报告 (用于报告生成的推理模式)。
- en: Lastly, Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) were the only
    ones to also implement cycle-consistency tasks (Zhu et al., [2017b](#bib.bib168))
    to train a GAN and an inverse mapping CNN together, to make both chest X-ray image
    generation and encoding more robust. These tasks will be further detailed in the
    next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Spinks 和 Moens (Spinks 和 Moens, [2019](#bib.bib127)) 是唯一将循环一致性任务 (Zhu et
    al., [2017b](#bib.bib168)) 实现于训练 GAN 和逆向映射 CNN 的研究，以使胸部 X 光图像生成和编码更为稳健。这些任务将在下一节中进一步详细说明。
- en: 5.2.6\. Optimization Strategies
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 优化策略
- en: In addition to the architecture and the tasks a model can perform, a very important
    aspect is the optimization strategy used to learn the model’s parameters. In this
    section we present an analysis of the optimization strategies used in the literature.
    A summary of this section is presented in Table [11](#S9.T11 "Table 11 ‣ 9.3\.
    Optimization Strategies ‣ 9\. Supplementary Material ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images") in appendix
    [9.3](#S9.SS3 "9.3\. Optimization Strategies ‣ 9\. Supplementary Material ‣ A
    Survey on Deep Learning and Explainability for Automatic Report Generation from
    Medical Images").
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型的架构和模型可以执行的任务外，一个非常重要的方面是用于学习模型参数的优化策略。在这一部分，我们对文献中使用的优化策略进行了分析。该部分的总结见附录[9.3](#S9.SS3
    "9.3\. 优化策略 ‣ 9\. 附录 ‣ 深度学习和医学图像自动报告生成的可解释性调查")中的表[11](#S9.T11 "表 11 ‣ 9.3\. 优化策略
    ‣ 9\. 附录 ‣ 深度学习和医学图像自动报告生成的可解释性调查")。
- en: Visual Component. We first analyze the visual component optimization, identifying
    three general optimization decisions. The first one is whether to use a CNN from
    the literature with its weights pretrained in ImageNet (Deng et al., [2009](#bib.bib30)).
    This is a very common transfer learning practice from the computer vision literature
    in general (Kornblith et al., [2019](#bib.bib79)), so it is natural to see it
    used in the medical domain too. However, it has been shown that ImageNet pretraining
    may not transfer as well to medical image tasks as they normally do to other domains,
    due to very dissimilar image distributions (Raghu et al., [2019](#bib.bib110)).
    Therefore, a very common second decision is whether or not to train/fine-tune
    the visual component with auxiliary medical image tasks, such as most of the classification
    and segmentation tasks discussed in the previous section ([5.2.5](#S5.SS2.SSS5
    "5.2.5\. Auxiliary Tasks ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). The third decision is whether to freeze the visual component
    weights during report generation training or continue updating them in an end-to-end
    manner.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉组件。我们首先分析视觉组件优化，确定了三种一般的优化决策。第一种是是否使用文献中的CNN，其权重在ImageNet上进行了预训练（Deng et al.,
    [2009](#bib.bib30)）。这是计算机视觉文献中一种非常常见的迁移学习实践（Kornblith et al., [2019](#bib.bib79)），因此在医学领域中看到它的应用也很自然。然而，研究表明，ImageNet的预训练可能无法像对其他领域那样有效地迁移到医学图像任务，因为图像分布非常不同（Raghu
    et al., [2019](#bib.bib110)）。因此，第二个常见的决策是是否使用辅助医学图像任务对视觉组件进行训练/微调，例如上一部分讨论的大多数分类和分割任务（[5.2.5](#S5.SS2.SSS5
    "5.2.5\. 辅助任务 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 深度学习和医学图像自动报告生成的可解释性调查")）。第三个决策是是否在报告生成训练过程中冻结视觉组件的权重，还是以端到端的方式继续更新它们。
- en: 'Report Generation. We identify two general optimization strategies in the literature:
    Teacher-forcing (TF) and Reinforcement Learning (RL). Teacher-forcing (Williams
    and Zipser, [1989](#bib.bib147)) is by far the most common, as it is adopted by
    32 papers (Jing et al., [2018](#bib.bib69); Huang et al., [2019](#bib.bib62);
    Yuan et al., [2019](#bib.bib157); Li et al., [2019b](#bib.bib87); Wang et al.,
    [2018](#bib.bib145); Xue et al., [2018](#bib.bib155); Zhang et al., [2017a](#bib.bib164);
    Li et al., [2019a](#bib.bib90); Singh et al., [2019](#bib.bib124); Maksoud et al.,
    [2019](#bib.bib96); Gale et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132);
    Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Alsharid et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Gajbhiye et al.,
    [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17);
    Xie et al., [2019](#bib.bib151); Zeng et al., [2018](#bib.bib159); Xue and Huang,
    [2019](#bib.bib154); Sun et al., [2019](#bib.bib129); Zhang et al., [2020b](#bib.bib163);
    Li and Hong, [2019](#bib.bib88); Jing et al., [2019](#bib.bib68); Shin et al.,
    [2016](#bib.bib121); Hasan et al., [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149);
    Spinks and Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)). The
    basic idea in teacher-forcing is to train a model to predict each word of the
    report conditioned on the previous words, therefore learning to imitate the ground
    truth word by word. The model typically has a softmax layer that predicts the
    next word, and cross entropy is the loss function of choice to measure the error
    and compute gradients for backpropagation. We think teacher-forcing is so widespread
    in the literature because of its simplicity and general applicability, as it is
    agnostic to the application domain (whether it be report generation in medicine
    or captioning of everyday images).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 报告生成。我们在文献中确定了两种一般的优化策略：教师强制（TF）和强化学习（RL）。教师强制（Williams and Zipser, [1989](#bib.bib147)）迄今为止是最常见的，因为有32篇论文采用了这种方法（Jing
    et al., [2018](#bib.bib69); Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157);
    Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Xue et al.,
    [2018](#bib.bib155); Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90);
    Singh et al., [2019](#bib.bib124); Maksoud et al., [2019](#bib.bib96); Gale et al.,
    [2019](#bib.bib39); Tian et al., [2018](#bib.bib132); Gu et al., [2019](#bib.bib45);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133); Alsharid et al.,
    [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37);
    Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17); Xie et al.,
    [2019](#bib.bib151); Zeng et al., [2018](#bib.bib159); Xue and Huang, [2019](#bib.bib154);
    Sun et al., [2019](#bib.bib129); Zhang et al., [2020b](#bib.bib163); Li and Hong,
    [2019](#bib.bib88); Jing et al., [2019](#bib.bib68); Shin et al., [2016](#bib.bib121);
    Hasan et al., [2018b](#bib.bib52); Wu et al., [2017](#bib.bib149); Spinks and
    Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)）。教师强制的基本思想是训练一个模型，根据前面的文字条件预测报告的每个词，因此逐字学习模仿地面实况。模型通常有一个softmax层来预测下一个词，交叉熵是选择的损失函数，用于衡量错误并计算反向传播的梯度。我们认为教师强制在文献中如此普遍，是因为它的简单性和一般适用性，因为它对应用领域（无论是医学报告生成还是日常图像说明）都是无视的。
- en: 'In contrast, 5 works (Liu et al., [2019](#bib.bib93); Li et al., [2018](#bib.bib86);
    Xiong et al., [2019](#bib.bib152); Jing et al., [2019](#bib.bib68); Li and Hong,
    [2019](#bib.bib88)) explored the use of reinforcement learning (RL) (Kaelbling
    et al., [1996](#bib.bib72)). The main reason to use RL is the flexibility it offers
    to optimize non-differentiable reward functions, allowing researchers to be more
    creative and explore new rewards that may guide the model’s learning toward domain-specific
    goals of interest. For example, Liu et al. (Liu et al., [2019](#bib.bib93)) used
    RL to train their model to optimize the weighted sum of two rewards: (1) a natural
    language reward (CIDEr (Vedantam et al., [2015](#bib.bib141))) and (2) a Clinically
    Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy
    of a generated report compared to a ground-truth reference using the CheXpert
    labeler tool (Irvin et al., [2019](#bib.bib64)). Their goal was to equip their
    model with two skills: natural language fluency (encouraged by CIDEr) and clinical
    accuracy (encouraged by CCR). Other examples of the use of RL are: the direct
    optimization of CIDEr (Li et al., [2018](#bib.bib86); Xiong et al., [2019](#bib.bib152)),
    particularly in the training of a complicated hybrid template-retrieval and text
    generation model (Li et al., [2018](#bib.bib86)); directly optimizing BLEU-4 after
    a previous teacher-forcing warmup phase (Jing et al., [2019](#bib.bib68)); and
    the training of the generator network of a GAN used for report generation, where
    the reward is provided by the discriminator network (Li and Hong, [2019](#bib.bib88)).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，5项工作（Liu等人，[2019](#bib.bib93)；Li等人，[2018](#bib.bib86)；Xiong等人，[2019](#bib.bib152)；Jing等人，[2019](#bib.bib68)；Li和Hong，[2019](#bib.bib88)）探讨了强化学习（RL）的使用（Kaelbling等人，[1996](#bib.bib72)）。使用RL的主要原因是它提供了优化不可微分奖励函数的灵活性，允许研究人员更具创意地探索新的奖励，从而引导模型的学习朝向特定领域的目标。例如，Liu等人（Liu等人，[2019](#bib.bib93)）使用RL训练他们的模型，以优化两个奖励的加权和：（1）自然语言奖励（CIDEr（Vedantam等人，[2015](#bib.bib141)））和（2）临床一致性奖励（CCR），后者用于通过CheXpert标注工具（Irvin等人，[2019](#bib.bib64)）测量生成报告的临床准确性与真实参考的对比。他们的目标是使模型具备两个技能：自然语言流畅性（由CIDEr鼓励）和临床准确性（由CCR鼓励）。使用RL的其他例子包括：直接优化CIDEr（Li等人，[2018](#bib.bib86)；Xiong等人，[2019](#bib.bib152)），特别是在训练复杂的混合模板检索和文本生成模型时（Li等人，[2018](#bib.bib86)）；在之前的教师强制预热阶段之后直接优化BLEU-4（Jing等人，[2019](#bib.bib68)）；以及训练用于报告生成的GAN生成器网络，其中奖励由鉴别器网络提供（Li和Hong，[2019](#bib.bib88)）。
- en: As a side note, we would like to highlight the work by Zhang et al. (Zhang et al.,
    [2020a](#bib.bib162)) on medical report summarization (a related task where the
    report is the input and with no images), illustrating how RL can be used in this
    setting to optimize both fluency and factual correctness. As rewards they used
    ROUGE (Lin, [2004](#bib.bib91)) and a Factual Correctness reward based on the
    CheXpert labeler tool (Irvin et al., [2019](#bib.bib64)) (very similar to the
    CCR proposed by Liu et al. (Liu et al., [2019](#bib.bib93))). This work is a good
    example of the benefits of RL over teacher-forcing for text generation in a medical
    domain. The paper presents the results of a human evaluation with two board-certified
    radiologists and the model trained with RL achieved better results than the same
    model trained with teacher-forcing, and even slightly better results than the
    human baseline.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 附带说明，我们想要特别提及张等人（张等人，[2020a](#bib.bib162)）在医疗报告摘要（相关任务是报告作为输入，没有图像）方面的工作，展示了如何在这种设置中使用强化学习（RL）来优化流畅性和事实正确性。他们使用了ROUGE（Lin，[2004](#bib.bib91)）和基于CheXpert标注工具（Irvin等人，[2019](#bib.bib64)）的事实正确性奖励（与Liu等人（Liu等人，[2019](#bib.bib93)）提出的CCR非常相似）。这项工作是强化学习在医疗领域文本生成中优于教师强制法的一个很好的例子。论文展示了与两名经过认证的放射科医生进行的人工评估结果，使用强化学习训练的模型取得了比同一模型使用教师强制法训练的更好的结果，甚至比人工基准结果稍好。
- en: Other Losses or Training Strategies. This category encompasses the remaining
    optimization strategies found in the literature. The most important one is multitask
    learning (Caruana, [1997](#bib.bib22)), adopted by 14 papers (Jing et al., [2018](#bib.bib69);
    Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Zhang et al.,
    [2017a](#bib.bib164); Maksoud et al., [2019](#bib.bib96); Tian et al., [2018](#bib.bib132);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133); Ma et al.,
    [2018](#bib.bib95); Harzig et al., [2019a](#bib.bib49); Jing et al., [2019](#bib.bib68);
    Kisilev et al., [2016](#bib.bib77); Spinks and Moens, [2019](#bib.bib127); Zeng
    et al., [2020](#bib.bib158)). The main idea is to jointly train a model in multiple
    complementary tasks, so that the model can learn robust parameters that perform
    well in all of them. Some works (Jing et al., [2018](#bib.bib69); Wang et al.,
    [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164); Tian et al., [2018](#bib.bib132);
    Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133); Harzig et al.,
    [2019a](#bib.bib49)) trained the visual and language components simultaneously
    in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary
    tasks. Other examples are the simultaneous training of object detection and attribute
    classification (Kisilev et al., [2016](#bib.bib77)), diagnostic classification
    and cycle-consistency tasks (Spinks and Moens, [2019](#bib.bib127)), among others.
    Most of these papers report benefits from training in this way.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其他损失或训练策略。这一类别包括文献中发现的其他优化策略。其中最重要的是多任务学习（Caruana, [1997](#bib.bib22)），被14篇论文采纳（Jing等，[2018](#bib.bib69)；Li等，[2019b](#bib.bib87)；Wang等，[2018](#bib.bib145)；Zhang等，[2017a](#bib.bib164)；Maksoud等，[2019](#bib.bib96)；Tian等，[2018](#bib.bib132)；Yin等，[2019](#bib.bib156)；Tian等，[2019](#bib.bib133)；Ma等，[2018](#bib.bib95)；Harzig等，[2019a](#bib.bib49)；Jing等，[2019](#bib.bib68)；Kisilev等，[2016](#bib.bib77)；Spinks和Moens，[2019](#bib.bib127)；Zeng等，[2020](#bib.bib158)）。其主要思想是联合训练多个互补任务的模型，使模型能够学习到在所有任务中表现良好的鲁棒参数。一些研究（Jing等，[2018](#bib.bib69)；Wang等，[2018](#bib.bib145)；Zhang等，[2017a](#bib.bib164)；Tian等，[2018](#bib.bib132)；Yin等，[2019](#bib.bib156)；Tian等，[2019](#bib.bib133)；Harzig等，[2019a](#bib.bib49)）采用端到端的方式同时训练视觉和语言组件在多个任务中的表现，即报告生成加其他辅助任务。其他例子包括对象检测和属性分类的同时训练（Kisilev等，[2016](#bib.bib77)），诊断分类和周期一致性任务（Spinks和Moens，[2019](#bib.bib127)）等。这些论文大多数报告了这种训练方式的好处。
- en: As already discussed in section [5.2.5](#S5.SS2.SSS5 "5.2.5\. Auxiliary Tasks
    ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images"), two
    works (Maksoud et al., [2019](#bib.bib96); Yin et al., [2019](#bib.bib156)) used
    auxiliary supervision on the attention weights of their models. These auxiliary
    losses were jointly optimized with the rest of the model in report generation,
    effectively having a regularizer effect. Yin et al. (Yin et al., [2019](#bib.bib156))
    are also the only ones that included an auxiliary contrastive loss (Chopra et al.,
    [2005](#bib.bib25)) to provide a direct supervision to the Sentence LSTM, thus
    improving their model’s performance. Notice that all these works are examples
    of multitask learning too. Three papers (Kisilev et al., [2016](#bib.bib77); Moradi
    et al., [2016](#bib.bib99); Zeng et al., [2020](#bib.bib158)) used regression
    losses. Two of them (Kisilev et al., [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158))
    included a bounding box regression loss as part of Faster R-CNN (Ren et al., [2015](#bib.bib113))
    training, and Moradi et al. (Moradi et al., [2016](#bib.bib99)) included a regression
    loss to minimize the Euclidean distance between VGG and doc2vec embeddings. As
    previously discussed in section [5.2.5](#S5.SS2.SSS5 "5.2.5\. Auxiliary Tasks
    ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images"), another
    optimization strategy is the use of autoencoders for the self-supervised learning
    of text representations. In MTMA (Tian et al., [2019](#bib.bib133)) an autoencoder
    was used to provide an auxiliary supervision over the BiLSTM and was jointly trained
    with the rest of the model in a multitask learning fashion. Spinks and Moens (Spinks
    and Moens, [2019](#bib.bib127)) instead trained an ARAE in a first stage, then
    froze its weights and used the learned text embedding to support the subsequent
    training of a GAN.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[5.2.5](#S5.SS2.SSS5 "5.2.5\. Auxiliary Tasks ‣ 5.2\. Model Design ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images")节中讨论的那样，两项工作（Maksoud et al., [2019](#bib.bib96);
    Yin et al., [2019](#bib.bib156)）在其模型的注意力权重上使用了辅助监督。这些辅助损失与模型其余部分在报告生成中共同优化，实际上起到了正则化的作用。Yin
    et al.（Yin et al., [2019](#bib.bib156)）也是唯一包括了辅助对比损失（Chopra et al., [2005](#bib.bib25)）来为句子LSTM提供直接监督的研究，从而提高了他们模型的性能。请注意，这些工作也都是多任务学习的例子。三篇论文（Kisilev
    et al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99); Zeng et al., [2020](#bib.bib158)）使用了回归损失。其中两篇（Kisilev
    et al., [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158)）将边界框回归损失作为Faster
    R-CNN（Ren et al., [2015](#bib.bib113)）训练的一部分，而Moradi et al.（Moradi et al., [2016](#bib.bib99)）则包含了一个回归损失，以最小化VGG和doc2vec嵌入之间的欧氏距离。如在[5.2.5](#S5.SS2.SSS5
    "5.2.5\. Auxiliary Tasks ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")节中讨论的另一种优化策略是使用自编码器进行文本表示的自监督学习。在MTMA（Tian et al., [2019](#bib.bib133)）中，使用了自编码器来对BiLSTM提供辅助监督，并与模型的其余部分以多任务学习的方式共同训练。Spinks和Moens（Spinks
    and Moens, [2019](#bib.bib127)）则在第一阶段训练了一个ARAE，然后冻结了其权重，并利用学习到的文本嵌入支持后续的GAN训练。
- en: 'Lastly, three works used GANs (Han et al., [2018](#bib.bib48); Li and Hong,
    [2019](#bib.bib88); Spinks and Moens, [2019](#bib.bib127)). As mentioned when
    discussing RL, Li et al. (Han et al., [2018](#bib.bib48)) used a GAN strategy
    to train their model for report generation, where the generative module generates
    a report and the discriminator determines whether it is real or fake. Similarly,
    Han et al. (Li and Hong, [2019](#bib.bib88)) proposed RGAN, where the generator
    outputs segmentation maps from spine radiographs and the discriminator determines
    if a given segmentation map is real or fake. Spinks and Moens (Spinks and Moens,
    [2019](#bib.bib127)) implemented a modified version of a StackGAN (Zhang et al.,
    [2017b](#bib.bib160)) to generate chest X-ray images from input text representations.
    In their case, they trained the GAN using two cycle-consistency (Zhu et al., [2017b](#bib.bib168))
    losses: (1) image $\xrightarrow{}$ embedding $\xrightarrow{}$ image and (2) embedding
    $\xrightarrow{}$ image $\xrightarrow{}$ embedding. In both cases, an auxiliary
    inverse mapping CNN was used to close the cycle.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，三项研究使用了GAN（Han et al., [2018](#bib.bib48); Li and Hong, [2019](#bib.bib88);
    Spinks and Moens, [2019](#bib.bib127)）。正如讨论RL时提到的，Li等（Han et al., [2018](#bib.bib48)）使用了GAN策略来训练他们的报告生成模型，其中生成模块生成报告，而鉴别器判断其真假。类似地，Han等（Li
    and Hong, [2019](#bib.bib88)）提出了RGAN，其中生成器从脊柱X光片中输出分割图，而鉴别器判断给定的分割图是真实还是虚假的。Spinks和Moens（Spinks
    and Moens, [2019](#bib.bib127)）实现了StackGAN的修改版（Zhang et al., [2017b](#bib.bib160)），以从输入文本表示生成胸部X光图像。在他们的案例中，他们使用了两个循环一致性（Zhu
    et al., [2017b](#bib.bib168)）损失来训练GAN：(1) 图像 $\xrightarrow{}$ 嵌入 $\xrightarrow{}$
    图像和 (2) 嵌入 $\xrightarrow{}$ 图像 $\xrightarrow{}$ 嵌入。在这两种情况下，使用了辅助逆映射CNN来完成循环。
- en: Synthesis
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 综合分析
- en: 'Overall, we can observe that designing a model for report generation from medical
    images is a complex task that involves engineering decisions at multiple levels:
    inputs and outputs, visual component, language component, domain knowledge, auxiliary
    tasks and optimization strategies. In each of these dimensions there are different
    approaches adopted in the reviewed literature, and the current state of research
    does not allow us to recommend an “optimal model design”, mainly for reasons we
    will discuss in the Metrics and Performance Comparison sections ([5.4](#S5.SS4
    "5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images")
    and [5.5](#S5.SS5 "5.5\. Comparison of papers’ performance ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). Nevertheless, there are valuable insights in the literature
    that may lead to better results, and thus are worth having in mind. For example,
    the use of CNNs (such as DenseNet or ResNet) as visual component and training
    in auxiliary medical image tasks; the use of input text alongside the images;
    providing the language component with tag information in addition to the visual
    features (e.g. medical concepts identified in the image); leveraging template
    databases curated with domain knowledge; or the use of multitask learning combining
    multiple sources of supervision. Lastly, to improve report quality from a medical
    perspective, the use of reinforcement learning with adequate reward functions
    appears as the most promising approach.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们可以观察到，设计用于医学图像报告生成的模型是一项复杂的任务，涉及多个层次的工程决策：输入和输出、视觉组件、语言组件、领域知识、辅助任务和优化策略。在这些维度中，审阅文献中采用了不同的方法，目前的研究状态无法推荐一个“最佳模型设计”，主要原因将在[5.4](#S5.SS4
    "5.4\. 评估指标 ‣ 5\. 文献分析 ‣ 深度学习与医学图像自动报告生成的可解释性调查")和[5.5](#S5.SS5 "5.5\. 论文性能比较
    ‣ 5\. 文献分析 ‣ 深度学习与医学图像自动报告生成的可解释性调查")部分讨论。然而，文献中有价值的见解可能会带来更好的结果，因此值得关注。例如，使用CNN（如DenseNet或ResNet）作为视觉组件，并在辅助医学图像任务中进行训练；在图像旁边使用输入文本；为语言组件提供标签信息，除了视觉特征（例如图像中识别出的医学概念）；利用结合领域知识的模板数据库；或使用多任务学习结合多种监督来源。最后，为了从医学角度提高报告质量，使用具有适当奖励函数的强化学习似乎是最有前途的方法。
- en: 5.3\. Explainability
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 可解释性
- en: 'There have been multiple attempts on providing a definition for explainability
    in the Explainable Artificial Intelligence (XAI) area (Reyes et al., [2020](#bib.bib114);
    Doshi-Velez and Kim, [2017](#bib.bib34); Lipton, [2018](#bib.bib92)). For the
    task of report generation from medical images, we use a similar definition by
    Doshi-Velez and Kim (Doshi-Velez and Kim, [2017](#bib.bib34)): the ability to
    justify an outcome in understandable terms for a human, and we use it interchangeably
    with the term interpretability. In this medical context, an automated system requires
    high explainability levels as two main facts hold: the decisions derived from
    the system will probably have direct consequences for patients, and the diagnosis
    task is not trivial and susceptible to human judgement (Reyes et al., [2020](#bib.bib114);
    Doshi-Velez and Kim, [2017](#bib.bib34)). Furthermore, the explanation methods
    employed in this medical task should attempt to solve several related aspects:
    align with clinicians’ expectations and acquire their trust, increase system transparency,
    assess results quality, and allow addressing accountability, fairness and ethical
    concerns (Ahmad et al., [2018](#bib.bib5); Reyes et al., [2020](#bib.bib114);
    Tonekaboni et al., [2019](#bib.bib135)).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在可解释人工智能（XAI）领域，已经有多次尝试为可解释性提供定义（Reyes 等，[2020](#bib.bib114)；Doshi-Velez 和 Kim，[2017](#bib.bib34)；Lipton，[2018](#bib.bib92)）。对于从医学图像生成报告的任务，我们采用
    Doshi-Velez 和 Kim 提出的类似定义（Doshi-Velez 和 Kim，[2017](#bib.bib34)）：在可理解的术语中为人类提供结果的理由，我们将其与“可解释性”一词交替使用。在这一医学背景下，自动化系统需要较高的可解释性水平，因为有两个主要事实：系统做出的决策可能会直接影响患者，而且诊断任务并非简单且容易受到人工判断的影响（Reyes
    等，[2020](#bib.bib114)；Doshi-Velez 和 Kim，[2017](#bib.bib34)）。此外，这一医学任务中使用的解释方法应尝试解决几个相关方面：符合临床医生的期望并获得他们的信任，提高系统透明度，评估结果质量，以及解决问责、公平和伦理问题（Ahmad
    等，[2018](#bib.bib5)；Reyes 等，[2020](#bib.bib114)；Tonekaboni 等，[2019](#bib.bib135)）。
- en: There are many ways to address the explainability aspect of AI systems in the
    medical domain, as listed in the recent survey on interpretable AI for radiology
    by Reyes et al. (Reyes et al., [2020](#bib.bib114)). Multiple categories can be
    identified, starting with global vs local, the former refers to explanations regarding
    the whole system’s operation, and the latter to explanations for one sample. For
    local explanations, there are different kinds of approaches, such as feature importance,
    concept-based, example-based, and uncertainty, to mention a few. Feature importance
    methods attempt to compute a level of importance for each input value, to understand
    which characteristics were most relevant to make a decision; for example, gradient-based
    methods for CNNs such as Grad-CAM (Selvaraju et al., [2017](#bib.bib119)), Guided
    Backpropagation (Springenberg et al., [2015](#bib.bib128)) or DeepLIFT (Shrikumar
    et al., [2017](#bib.bib122)); and other techniques such as LIME (Ribeiro et al.,
    [2016](#bib.bib115)). In concept-based methods, like TCAV (Kim et al., [2018](#bib.bib76))
    or RCV (Graziani et al., [2018](#bib.bib44)), the contributions to the prediction
    from multiple concepts are quantified, so the user can check if the concepts used
    by the model are correct. Example-based approaches present additional examples
    with the output, either with a similar outcome, so the user can look for a common
    pattern, or with an opposite outcome (counter-factual). Uncertainty methods provide
    the level of confidence of the model for a given prediction. For global explanations,
    there are sample-based approaches, such as SP-LIME (Ribeiro et al., [2016](#bib.bib115)),
    or methods to directly increase the transparency of the system.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，解决AI系统可解释性问题的方法有很多种，如Reyes等人最近对放射学可解释AI的调查所列（Reyes et al., [2020](#bib.bib114)）。可以识别出多个类别，从全局与局部开始，全局指的是关于整个系统操作的解释，局部则是针对单个样本的解释。对于局部解释，有不同种类的方法，例如特征重要性、基于概念、基于示例和不确定性等。特征重要性方法试图计算每个输入值的重要性，以了解哪些特征对决策最相关；例如，CNN的梯度方法如Grad-CAM（Selvaraju
    et al., [2017](#bib.bib119)）、Guided Backpropagation（Springenberg et al., [2015](#bib.bib128)）或DeepLIFT（Shrikumar
    et al., [2017](#bib.bib122)）；以及其他技术如LIME（Ribeiro et al., [2016](#bib.bib115)）。在基于概念的方法中，如TCAV（Kim
    et al., [2018](#bib.bib76)）或RCV（Graziani et al., [2018](#bib.bib44)），预测中来自多个概念的贡献被量化，以便用户检查模型使用的概念是否正确。基于示例的方法提供附加的示例与输出，无论是具有相似结果的示例，以便用户寻找共同模式，还是具有相反结果的示例（对立示例）。不确定性方法提供模型对给定预测的信心水平。对于全局解释，有基于样本的方法，如SP-LIME（Ribeiro
    et al., [2016](#bib.bib115)），或直接增加系统透明度的方法。
- en: 'Despite the importance of explainability in this area, only two reviewed works
    focused explicitly on this topic. Gale et al. (Gale et al., [2019](#bib.bib39))
    proposed the automatic generation of a natural language report as an explanation
    for a classification task; however, their approach does not include an explanation
    for the report. Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) present
    a counter-factual local explanation, as will be detailed in subsection [5.3.1](#S5.SS3.SSS1
    "5.3.1\. Counter factual ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"). Additionally, in 29 works the model architecture generates
    a secondary output that can also be presented as a local explanation. We distinguish
    three types of outputs: classification (section [5.3.2](#S5.SS3.SSS2 "5.3.2\.
    Classification ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")), heatmap over the input image (section [5.3.3](#S5.SS3.SSS3 "5.3.3\.
    Image heatmap ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")), and heatmap over the input text (section [5.3.4](#S5.SS3.SSS4 "5.3.4\.
    Text heatmap ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")). These were already summarized in Table [3](#S5.T3 "Table 3 ‣ 5.2.1\.
    Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images") in the Input and Output section ([5.2.1](#S5.SS2.SSS1 "5.2.1\. Input
    and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on
    Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")). Next, the explanaibility aspects of the outputs are discussed.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一领域解释性的重视程度很高，但只有两项研究专门关注这个话题。Gale 等人（Gale et al., [2019](#bib.bib39)）提出了将自然语言报告作为分类任务的解释自动生成的方法；然而，他们的方法未包含对报告的解释。Spinks
    和 Moens（Spinks and Moens, [2019](#bib.bib127)）提出了一种反事实局部解释，详见[5.3.1](#S5.SS3.SSS1
    "5.3.1\. Counter factual ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")小节。此外，在29项工作中，模型架构生成了一个附加输出，这些输出也可以作为局部解释呈现。我们区分三种类型的输出：分类（[5.3.2](#S5.SS3.SSS2
    "5.3.2\. Classification ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")节）、输入图像上的热图（[5.3.3](#S5.SS3.SSS3 "5.3.3\. Image heatmap ‣
    5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")节）以及输入文本上的热图（[5.3.4](#S5.SS3.SSS4
    "5.3.4\. Text heatmap ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")节）。这些已经在[3](#S5.T3 "Table 3 ‣ 5.2.1\. Input and Output ‣
    5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")表中总结，在输入和输出部分（[5.2.1](#S5.SS2.SSS1
    "5.2.1\. Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")）。接下来，将讨论这些输出的解释性方面。
- en: 5.3.1\. Counter factual
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 反事实
- en: Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) proposed an architecture
    to both classify a disease and generate a caption from a chest X-ray, based on
    GANs and autoencoders, as detailed in the Model Design section ([5.2](#S5.SS2
    "5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")). Thus,
    to provide a local explanation, at inference time the input image is encoded into
    a latent vector, which is used to generate a new chest X-ray and a new report,
    both of them subject to result in the nearest alternative classification, i.e.,
    the nearest diagnosis. With this information, a user could compare the original
    X-ray with the generated image, and attempt to understand why the model has reached
    its decision.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Spinks 和 Moens（Spinks and Moens, [2019](#bib.bib127)）提出了一种架构，用于基于 GANs 和自编码器对胸部
    X 光片进行疾病分类并生成说明，详见模型设计部分（[5.2](#S5.SS2 "5.2\. Model Design ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")）。因此，为了提供局部解释，在推理时，输入图像被编码为一个潜在向量，该向量用于生成新的胸部 X 光片和新报告，两者都受到最近替代分类的影响，即最近的诊断。通过这些信息，用户可以比较原始
    X 光片和生成的图像，并尝试理解模型为何做出该决定。
- en: 5.3.2\. Classification
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 分类
- en: As explained in the Auxiliary Tasks section ([5.2.5](#S5.SS2.SSS5 "5.2.5\. Auxiliary
    Tasks ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images")),
    many deep learning architectures include multi-label classification to improve
    performance, providing a set of classified concepts as secondary output. Even
    though in most papers this kind of output is not presented as an explanation of
    the report, we consider that its nature could improve the transparency of the
    model, which is an important way of improving the interpretability in a medical
    context (Tonekaboni et al., [2019](#bib.bib135)). By providing this detection
    information from an intermediate step of the model’s process, an expert could
    further understand the internal process, validate the decision with their domain
    knowledge and calibrate their trust in the system.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如辅助任务部分([5.2.5](#S5.SS2.SSS5 "5.2.5\. 辅助任务 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 医学图像自动报告生成的深度学习与可解释性调查"))所解释，许多深度学习架构包括多标签分类以提高性能，提供一组分类概念作为次级输出。尽管在大多数论文中这种输出没有作为报告的解释呈现，我们认为其性质可以提高模型的透明度，这是提高医学环境下可解释性的重要方式（Tonekaboni
    et al., [2019](#bib.bib135)）。通过提供来自模型处理的中间步骤的检测信息，专家可以进一步理解内部过程，用他们的领域知识验证决策，并校准对系统的信任。
- en: As shown in Table [3](#S5.T3 "Table 3 ‣ 5.2.1\. Input and Output ‣ 5.2\. Model
    Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images") from section [5.2.1](#S5.SS2.SSS1
    "5.2.1\. Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"), the terms classified are very diverse. Some works classify
    very broad concepts, such as body parts or organs (Zeng et al., [2018](#bib.bib159);
    Alsharid et al., [2019](#bib.bib8); Moradi et al., [2016](#bib.bib99); Zeng et al.,
    [2020](#bib.bib158)), or image modality (Hasan et al., [2018b](#bib.bib52)). Other
    works perform a more specific classification, such as diseases or abnormalities
    (Zeng et al., [2018](#bib.bib159); Wang et al., [2018](#bib.bib145); Biswal et al.,
    [2020](#bib.bib17); Li et al., [2019b](#bib.bib87); Zhang et al., [2020b](#bib.bib163);
    Zeng et al., [2020](#bib.bib158); Spinks and Moens, [2019](#bib.bib127); Kisilev
    et al., [2016](#bib.bib77)), or a normal or abnormal status at sentence level
    (Xie et al., [2019](#bib.bib151)). Lastly, several works (Harzig et al., [2019a](#bib.bib49);
    Jing et al., [2018](#bib.bib69); Tian et al., [2019](#bib.bib133); Gu et al.,
    [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Sun et al., [2019](#bib.bib129);
    Yuan et al., [2019](#bib.bib157); Shin et al., [2016](#bib.bib121)) classify over
    a subset of MeSH terms or similar, which may contain a mix of general broad medical
    concepts and specific abnormalities or conditions. We believe that this additional
    output would be useful for an expert, though the specific concepts should provide
    much richer information. If the classification is more specific, the user will
    be able to validate on a much narrower scope the system’s performance.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格[3](#S5.T3 "表格 3 ‣ 5.2.1\. 输入与输出 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 医学图像自动报告生成的深度学习与可解释性调查")所示，来自[5.2.1](#S5.SS2.SSS1
    "5.2.1\. 输入与输出 ‣ 5.2\. 模型设计 ‣ 5\. 论文分析 ‣ 医学图像自动报告生成的深度学习与可解释性调查")节的术语分类非常多样。一些工作对非常广泛的概念进行分类，如身体部位或器官（Zeng
    et al., [2018](#bib.bib159); Alsharid et al., [2019](#bib.bib8); Moradi et al.,
    [2016](#bib.bib99); Zeng et al., [2020](#bib.bib158)），或图像模态（Hasan et al., [2018b](#bib.bib52)）。其他工作进行更具体的分类，如疾病或异常（Zeng
    et al., [2018](#bib.bib159); Wang et al., [2018](#bib.bib145); Biswal et al.,
    [2020](#bib.bib17); Li et al., [2019b](#bib.bib87); Zhang et al., [2020b](#bib.bib163);
    Zeng et al., [2020](#bib.bib158); Spinks and Moens, [2019](#bib.bib127); Kisilev
    et al., [2016](#bib.bib77)），或句子级别的正常或异常状态（Xie et al., [2019](#bib.bib151)）。最后，一些工作（Harzig
    et al., [2019a](#bib.bib49); Jing et al., [2018](#bib.bib69); Tian et al., [2019](#bib.bib133);
    Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Sun et al., [2019](#bib.bib129);
    Yuan et al., [2019](#bib.bib157); Shin et al., [2016](#bib.bib121)）对MeSH术语或类似术语的子集进行分类，这些术语可能包含一般广泛的医学概念和具体的异常或病状。我们认为这种额外的输出对专家会有用，尽管具体的概念会提供更丰富的信息。如果分类更为具体，用户将能够在更狭窄的范围内验证系统的性能。
- en: 5.3.3\. Image heatmap
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 图像热力图
- en: In the papers reviewed, there are three different approaches to generating heatmaps
    over the input image, each of them with a different interpretation. First, many
    architectures employ an attention mechanism over the image spatial features during
    the report generation, as it was discussed in the Language Component section ([5.2.3](#S5.SS2.SSS3
    "5.2.3\. Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). These mechanisms can be leveraged to produce a heatmap
    indicating the image regions that were most important to generate the report.
    In particular, some models provide a heatmap for each word (Zhang et al., [2017a](#bib.bib164);
    Wang et al., [2018](#bib.bib145); Liu et al., [2019](#bib.bib93)), for each sentence
    (Jing et al., [2018](#bib.bib69); Huang et al., [2019](#bib.bib62); Xue et al.,
    [2018](#bib.bib155); Xue and Huang, [2019](#bib.bib154)), or for the whole report
    (Li et al., [2019b](#bib.bib87)). By showing these feature importance maps, an
    expert should be able to determine if the model is focusing on the correct regions
    of the image, which could improve their trust on the system.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅的论文中，有三种不同的方法生成输入图像上的热图，每种方法有不同的解释。首先，许多架构在生成报告期间对图像空间特征应用注意力机制，如在语言组件部分（[5.2.3](#S5.SS2.SSS3
    "5.2.3\. Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")）中讨论的那样。这些机制可以用于生成指示图像区域的热图，这些区域在生成报告时最为重要。特别是，一些模型为每个词（Zhang
    et al., [2017a](#bib.bib164)；Wang et al., [2018](#bib.bib145)；Liu et al., [2019](#bib.bib93)）、每个句子（Jing
    et al., [2018](#bib.bib69)；Huang et al., [2019](#bib.bib62)；Xue et al., [2018](#bib.bib155)；Xue
    and Huang, [2019](#bib.bib154)）或整个报告（Li et al., [2019b](#bib.bib87)）提供热图。通过展示这些特征重要性图，专家应能够确定模型是否关注图像的正确区域，这可能会增强他们对系统的信任。
- en: Second, some works use particular deep learning architectures to perform image
    segmentation, i.e. classification and localization at the same time. The model
    by Ma et al. (Ma et al., [2018](#bib.bib95)) uses a CNN to classify the severity
    of four different key characteristics of cervical cancer, and then uses an attention
    mechanism over the visual spatial features to generate heatmaps indicating the
    position of each relevant property. Tian et al. (Tian et al., [2018](#bib.bib132))
    used an FCN to classify each pixel of an image with the presence of a liver or
    tumor, and the result is averaged with an attention map to further improve localization.
    Han et al. (Han et al., [2018](#bib.bib48)) proposed the ACAE module (see section
    [5.2.3](#S5.SS2.SSS3 "5.2.3\. Language Component ‣ 5.2\. Model Design ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images") for details), which is used to classify
    at pixel level different parts of the spine (vertebrae, discs or neural foramina),
    and if they show an abnormality or not. Kisilev et al. (Kisilev et al., [2016](#bib.bib77))
    and Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) used a Faster R-CNN
    (Ren et al., [2015](#bib.bib113)) architecture to detect image regions with lesion
    and body parts of interest.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，一些研究使用特定的深度学习架构进行图像分割，即同时进行分类和定位。Ma 等人（Ma et al., [2018](#bib.bib95)）的模型使用
    CNN 来分类宫颈癌四个不同关键特征的严重程度，然后通过对视觉空间特征的注意力机制生成热图，以指示每个相关属性的位置。Tian 等人（Tian et al.,
    [2018](#bib.bib132)）使用 FCN 来分类图像中是否存在肝脏或肿瘤的每个像素，并通过与注意力图的平均结果进一步改善定位。Han 等人（Han
    et al., [2018](#bib.bib48)）提出了 ACAE 模块（见章节 [5.2.3](#S5.SS2.SSS3 "5.2.3\. Language
    Component ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on
    Deep Learning and Explainability for Automatic Report Generation from Medical
    Images") 以获取详细信息），该模块用于在像素级别分类脊柱的不同部分（椎骨、椎间盘或神经孔），以及它们是否显示异常。Kisilev 等人（Kisilev
    et al., [2016](#bib.bib77)）和 Spinks 及 Moens（Spinks and Moens, [2019](#bib.bib127)）使用
    Faster R-CNN（Ren et al., [2015](#bib.bib113)）架构来检测图像中具有病变和感兴趣的身体部位区域。
- en: Lastly, some works use gradient- or activation-based methods for CNNs to generate
    a saliency map indicating the regions of most importance for a classification,
    such as CAM (Zhou et al., [2016](#bib.bib166)), Grad-CAM (Selvaraju et al., [2017](#bib.bib119)),
    SmoothGrad (Smilkov et al., [2017](#bib.bib125)), or the one proposed by Zagoruyko
    and Komodakis (Komodakis and Zagoruyko, [2017](#bib.bib78)). Refer to Table [3](#S5.T3
    "Table 3 ‣ 5.2.1\. Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") in the Input and Output section ([5.2.1](#S5.SS2.SSS1 "5.2.1\.
    Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")) for a list of the papers using each technique. To determine which of
    these methods performs better in a general setting, Adebayo et al. (Adebayo et al.,
    [2018](#bib.bib4)) performed multiple evaluations (“sanity checks”) over Grad-CAM,
    SmoothGrad, and other similar methods, and showed that Grad-CAM should be more
    reliable in terms of correlation with the input images and the classification
    made. As an example of these techniques, Figure [4](#S5.F4 "Figure 4 ‣ 5.3.3\.
    Image heatmap ‣ 5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images") shows two chest X-rays from the ChestX-ray14 dataset (Wang et al., [2017](#bib.bib144))
    with a heatmap generated with CAM, plus an expert-annotated bounding box locating
    the abnormality (provided with the dataset).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一些工作使用基于梯度或激活的方法来生成显著性图，指示分类中最重要的区域，如CAM (Zhou et al., [2016](#bib.bib166))、Grad-CAM
    (Selvaraju et al., [2017](#bib.bib119))、SmoothGrad (Smilkov et al., [2017](#bib.bib125))，或Zagoruyko和Komodakis提出的方法
    (Komodakis and Zagoruyko, [2017](#bib.bib78))。请参见[3](#S5.T3 "Table 3 ‣ 5.2.1\.
    Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")表中的列表，了解使用每种技术的论文。为了确定这些方法在一般环境中的表现，Adebayo等人 (Adebayo et al., [2018](#bib.bib4))
    对Grad-CAM、SmoothGrad及其他类似方法进行了多次评估（“合理性检查”），并表明Grad-CAM在与输入图像及分类结果的相关性方面应更为可靠。作为这些技术的一个示例，[4](#S5.F4
    "Figure 4 ‣ 5.3.3\. Image heatmap ‣ 5.3\. Explainability ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")图展示了来自ChestX-ray14数据集 (Wang et al., [2017](#bib.bib144))的两张胸部X光片，其中使用CAM生成的热力图，以及一个由专家标注的边界框标记异常位置（数据集提供）。
- en: '| ![Refer to caption](img/d91b0c6de4d46792931a39c77edf6fa7.png) ![Refer to
    caption](img/283723587202a941746df095fd073795.png) | ![Refer to caption](img/b6be909de618f84fac82107ecda67f81.png)
    ![Refer to caption](img/23e242054fea3f61b91e47b14c20ac49.png) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/d91b0c6de4d46792931a39c77edf6fa7.png) ![参见说明](img/283723587202a941746df095fd073795.png)
    | ![参见说明](img/b6be909de618f84fac82107ecda67f81.png) ![参见说明](img/23e242054fea3f61b91e47b14c20ac49.png)
    |'
- en: '| --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Cardiomegaly | Pneumothorax |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 心脏肥大 | 气胸 |'
- en: Figure 4\. Examples from the ChestX-ray14 dataset (Wang et al., [2017](#bib.bib144))
    classified with a CNN based on ResNet-50 (He et al., [2016](#bib.bib53)), and
    using CAM (Zhou et al., [2016](#bib.bib166)) to provide a heatmap indicating the
    the spatial regions of most importance as local explanation. The left example
    presents Cardiomegaly and the right Pneumothorax, and both samples were correctly
    classified by the CNN. Red boxes represent a localization of the condition annotated
    by an expert.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 来自ChestX-ray14数据集 (Wang et al., [2017](#bib.bib144)) 的示例，使用基于ResNet-50
    (He et al., [2016](#bib.bib53)) 的CNN进行分类，并使用CAM (Zhou et al., [2016](#bib.bib166))
    提供热力图，指示局部解释中最重要的空间区域。左侧示例显示心脏肥大，右侧示例显示气胸，两个样本都被CNN正确分类。红框代表由专家标注的病症位置。
- en: \Description
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Two chest X-rays showing abnormalities, alongside a heatmap indicating the regions
    of most importance for the neural network, plus a bounding-box locating the abnormality.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 两张胸部X光片显示了异常情况，并附有一个热力图，指示神经网络中最重要的区域，以及一个边界框标出异常位置。
- en: In both segmentation and saliency map methods, the heatmap information provides
    much richer information than classification alone, as it also includes the location
    of an specific concept, such as an abnormality or a body part. Providing this
    type of explanation should allow an expert to assess the localization capabilities
    of the model and the system accuracy, thus improving the model’s transparency
    throughout its process.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割和显著性图方法中，热图信息提供的信息比单纯的分类要丰富得多，因为它还包括特定概念的位置，比如异常或身体部位。提供这种类型的解释应允许专家评估模型的定位能力和系统准确性，从而提高模型在整个过程中的透明度。
- en: 5.3.4\. Text heatmap
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4\. 文本热图
- en: The model proposed by Huang et al. (Huang et al., [2019](#bib.bib62)) also receives
    text as input, which indicates the reason for performing the imaging study on
    the patient. In a similar fashion to the input image cases, the architecture includes
    an attention mechanism over the input text, which provides a heatmap indicating
    the input phrases or sentences that were most relevant to generate each word in
    the output. With this feature importance map an expert should be able to determine
    if the model is focusing on the correct words in the input text.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Huang等人提出的模型（Huang et al., [2019](#bib.bib62)）也接受文本作为输入，这表明进行成像研究的原因。类似于输入图像的情况，该架构包括对输入文本的注意力机制，它提供一个热图，指示生成输出中每个单词所依据的最相关输入短语或句子。通过这种特征重要性图，专家应该能够确定模型是否关注了输入文本中的正确单词。
- en: Synthesis
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 综合
- en: All the explainability approaches are local explanations given by a secondary
    output, either indicating feature importance (image and text heatmap), increasing
    the model’s transparency (classification) or providing a counter-factual example.
    However, in most of the works the authors do not explicitly mention it as an interpretability
    improvement, and in almost all cases there is no formal evaluation, as will be
    discussed in subsection [5.4.3](#S5.SS4.SSS3 "5.4.3\. Explainability metrics ‣
    5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images").
    Hence, we believe this is an understudied aspect of the medical report generation
    task, given the superficial or nonexistent analysis it receives in most of the
    reviewed works. Additionally, counter-factual techniques could be further studied,
    and other approaches not found in the literature could be explored, such as prediction
    uncertainty or global explanations, which may be quite relevant for clinicians
    (Tonekaboni et al., [2019](#bib.bib135)).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的可解释性方法都是由次级输出给出的局部解释，要么指示特征重要性（图像和文本热图），要么提高模型的透明度（分类），或者提供一个反事实例。然而，在大多数研究中，作者并未明确将其作为解释能力的改进提及，几乎所有情况下也没有正式的评估，如[5.4.3](#S5.SS4.SSS3
    "5.4.3\. Explainability metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")小节将讨论的。因此，我们认为这是医学报告生成任务中一个被忽视的方面，因为在大多数被审查的作品中对此的分析都很肤浅或根本不存在。此外，反事实技术可以进一步研究，其他文献中未找到的方法也值得探索，例如预测不确定性或全局解释，这对临床医生可能非常相关（Tonekaboni
    et al., [2019](#bib.bib135)）。
- en: 5.4\. Evaluation Metrics
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 评估指标
- en: 'There are different ways to assess a medical report generated by an automated
    system. We divide the evaluation metrics used in the literature into three categories,
    depending on the aspect being assessed: text quality, medical correctness and
    explainability. Also, each evaluation method can be either automatic or performed
    manually by humans. Each of the categories and metrics are presented next, and
    Table [6](#S5.T6 "Table 6 ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") shows a summary of the metrics used by each paper.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 评估自动化系统生成的医学报告有不同的方法。我们将文献中使用的评估指标分为三个类别，具体取决于被评估的方面：文本质量、医学正确性和可解释性。此外，每种评估方法可以是自动的，也可以由人工进行。接下来将介绍每个类别和指标，表格[6](#S5.T6
    "Table 6 ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")显示了每篇论文使用的指标总结。
- en: '| Category | Metric or evaluation | Used by papers |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 指标或评估 | 论文中使用 |'
- en: '| Text quality (automatic) | BLEU based | (Jing et al., [2018](#bib.bib69);
    Liu et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib62); Yuan et al.,
    [2019](#bib.bib157); Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Wang
    et al., [2018](#bib.bib145); Xue et al., [2018](#bib.bib155); Zhang et al., [2017a](#bib.bib164);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Singh et al.,
    [2019](#bib.bib124); Maksoud et al., [2019](#bib.bib96); Gale et al., [2019](#bib.bib39);
    Tian et al., [2018](#bib.bib132); Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156);
    Tian et al., [2019](#bib.bib133); Alsharid et al., [2019](#bib.bib8); Gasimova,
    [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49);
    Biswal et al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Zeng et al.,
    [2018](#bib.bib159); Xue and Huang, [2019](#bib.bib154); Sun et al., [2019](#bib.bib129);
    Zhang et al., [2020b](#bib.bib163); Li and Hong, [2019](#bib.bib88); Jing et al.,
    [2019](#bib.bib68); Shin et al., [2016](#bib.bib121); Hasan et al., [2018b](#bib.bib52);
    Spinks and Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 文本质量（自动） | 基于BLEU | （Jing等，[2018](#bib.bib69)；Liu等，[2019](#bib.bib93)；Huang等，[2019](#bib.bib62)；Yuan等，[2019](#bib.bib157)；Li等，[2018](#bib.bib86)，[2019b](#bib.bib87)；Wang等，[2018](#bib.bib145)；Xue等，[2018](#bib.bib155)；Zhang等，[2017a](#bib.bib164)；Li等，[2019a](#bib.bib90)；Xiong等，[2019](#bib.bib152)；Singh等，[2019](#bib.bib124)；Maksoud等，[2019](#bib.bib96)；Gale等，[2019](#bib.bib39)；Tian等，[2018](#bib.bib132)；Gu等，[2019](#bib.bib45)；Yin等，[2019](#bib.bib156)；Tian等，[2019](#bib.bib133)；Alsharid等，[2019](#bib.bib8)；Gasimova，[2019](#bib.bib41)；Gajbhiye等，[2020](#bib.bib37)；Harzig等，[2019a](#bib.bib49)；Biswal等，[2020](#bib.bib17)；Xie等，[2019](#bib.bib151)；Zeng等，[2018](#bib.bib159)；Xue和Huang，[2019](#bib.bib154)；Sun等，[2019](#bib.bib129)；Zhang等，[2020b](#bib.bib163)；Li和Hong，[2019](#bib.bib88)；Jing等，[2019](#bib.bib68)；Shin等，[2016](#bib.bib121)；Hasan等，[2018b](#bib.bib52)；Spinks和Moens，[2019](#bib.bib127)；Zeng等，[2020](#bib.bib158)）
    |'
- en: '| ROUGE-L | (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Li et al.,
    [2018](#bib.bib86), [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Xue
    et al., [2018](#bib.bib155); Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90);
    Singh et al., [2019](#bib.bib124); Maksoud et al., [2019](#bib.bib96); Tian et al.,
    [2018](#bib.bib132); Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156);
    Tian et al., [2019](#bib.bib133); Alsharid et al., [2019](#bib.bib8); Gajbhiye
    et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Xie et al., [2019](#bib.bib151);
    Zeng et al., [2018](#bib.bib159); Xue and Huang, [2019](#bib.bib154); Zhang et al.,
    [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68); Spinks and Moens, [2019](#bib.bib127);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L | （Jing等，[2018](#bib.bib69)；Liu等，[2019](#bib.bib93)；Huang等，[2019](#bib.bib62)；Yuan等，[2019](#bib.bib157)；Li等，[2018](#bib.bib86)，[2019b](#bib.bib87)；Wang等，[2018](#bib.bib145)；Xue等，[2018](#bib.bib155)；Zhang等，[2017a](#bib.bib164)；Li等，[2019a](#bib.bib90)；Singh等，[2019](#bib.bib124)；Maksoud等，[2019](#bib.bib96)；Tian等，[2018](#bib.bib132)；Gu等，[2019](#bib.bib45)；Yin等，[2019](#bib.bib156)；Tian等，[2019](#bib.bib133)；Alsharid等，[2019](#bib.bib8)；Gajbhiye等，[2020](#bib.bib37)；Harzig等，[2019a](#bib.bib49)；Xie等，[2019](#bib.bib151)；Zeng等，[2018](#bib.bib159)；Xue和Huang，[2019](#bib.bib154)；Zhang等，[2020b](#bib.bib163)；Jing等，[2019](#bib.bib68)；Spinks和Moens，[2019](#bib.bib127)；Zeng等，[2020](#bib.bib158)）
    |'
- en: '| METEOR based | (Jing et al., [2018](#bib.bib69); Yuan et al., [2019](#bib.bib157);
    Wang et al., [2018](#bib.bib145); Xue et al., [2018](#bib.bib155); Zhang et al.,
    [2017a](#bib.bib164); Singh et al., [2019](#bib.bib124); Maksoud et al., [2019](#bib.bib96);
    Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Gajbhiye et al.,
    [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Zeng et al., [2018](#bib.bib159);
    Xue and Huang, [2019](#bib.bib154); Li and Hong, [2019](#bib.bib88); Spinks and
    Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 基于METEOR | （Jing等，[2018](#bib.bib69)；Yuan等，[2019](#bib.bib157)；Wang等，[2018](#bib.bib145)；Xue等，[2018](#bib.bib155)；Zhang等，[2017a](#bib.bib164)；Singh等，[2019](#bib.bib124)；Maksoud等，[2019](#bib.bib96)；Gu等，[2019](#bib.bib45)；Yin等，[2019](#bib.bib156)；Gajbhiye等，[2020](#bib.bib37)；Harzig等，[2019a](#bib.bib49)；Zeng等，[2018](#bib.bib159)；Xue和Huang，[2019](#bib.bib154)；Li和Hong，[2019](#bib.bib88)；Spinks和Moens，[2019](#bib.bib127)；Zeng等，[2020](#bib.bib158)）
    |'
- en: '| CIDEr based | (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Li et al., [2018](#bib.bib86), [2019b](#bib.bib87);
    Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90); Xiong et al.,
    [2019](#bib.bib152); Singh et al., [2019](#bib.bib124); Yin et al., [2019](#bib.bib156);
    Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49); Biswal
    et al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Zeng et al., [2018](#bib.bib159);
    Xue and Huang, [2019](#bib.bib154); Sun et al., [2019](#bib.bib129); Zhang et al.,
    [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68); Spinks and Moens, [2019](#bib.bib127);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 基于CIDEr | (Jing等，[2018](#bib.bib69); Liu等，[2019](#bib.bib93); Huang等，[2019](#bib.bib62);
    Li等，[2018](#bib.bib86)，[2019b](#bib.bib87); Zhang等，[2017a](#bib.bib164); Li等，[2019a](#bib.bib90);
    Xiong等，[2019](#bib.bib152); Singh等，[2019](#bib.bib124); Yin等，[2019](#bib.bib156);
    Gajbhiye等，[2020](#bib.bib37); Harzig等，[2019a](#bib.bib49); Biswal等，[2020](#bib.bib17);
    Xie等，[2019](#bib.bib151); Zeng等，[2018](#bib.bib159); Xue和Huang，[2019](#bib.bib154);
    Sun等，[2019](#bib.bib129); Zhang等，[2020b](#bib.bib163); Jing等，[2019](#bib.bib68);
    Spinks和Moens，[2019](#bib.bib127); Zeng等，[2020](#bib.bib158)) |'
- en: '| SPICE | (Li and Hong, [2019](#bib.bib88)) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SPICE | (Li和Hong，[2019](#bib.bib88)) |'
- en: '| Grammar Bot | (Alsharid et al., [2019](#bib.bib8)) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 语法机器人 | (Alsharid等，[2019](#bib.bib8)) |'
- en: '| Sentence variability | (Harzig et al., [2019a](#bib.bib49)) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 句子变化程度 | (Harzig等，[2019a](#bib.bib49)) |'
- en: '| Text quality (with humans) | AMT study | (Li et al., [2018](#bib.bib86),
    [2019b](#bib.bib87)) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 文本质量（与人类） | AMT研究 | (Li等，[2018](#bib.bib86)，[2019b](#bib.bib87)) |'
- en: '| Medical correctness (automatic, report based) | MIRQI (precision, recall,
    F1) | (Zhang et al., [2020b](#bib.bib163)) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 医学正确性（自动，报告为基础） | MIRQI（精确度，召回率，F1） | (Zhang等，[2020b](#bib.bib163)) |'
- en: '| MeSH Accuracy | (Huang et al., [2019](#bib.bib62)) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| MeSH准确度 | (Huang等，[2019](#bib.bib62)) |'
- en: '| Keyword ratio (accuracy, sensitivity, specificity) | (Wu et al., [2017](#bib.bib149))
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 关键词比率（准确度，敏感性，特异性） | (Wu等，[2017](#bib.bib149)) |'
- en: '| Keyword Accuracy | (Xue et al., [2018](#bib.bib155); Xie et al., [2019](#bib.bib151))
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 关键词准确度 | (Xue等，[2018](#bib.bib155); Xie等，[2019](#bib.bib151)) |'
- en: '| Medical Abnormality Terminology Detection (precision, FPR) | (Li et al.,
    [2018](#bib.bib86)) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 医学异常术语检测（精确度，FPR） | (Li等，[2018](#bib.bib86)) |'
- en: '| Abnormality Detection (precision, FPR) | (Jing et al., [2019](#bib.bib68))
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 异常检测（精确度，FPR） | (Jing等，[2019](#bib.bib68)) |'
- en: '| Medical Abnormality Detection (accuracy, precision, recall) | (Liu et al.,
    [2019](#bib.bib93)) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 医学异常检测（准确度，精确度，召回率） | (Liu等，[2019](#bib.bib93)) |'
- en: '| Abnormality CNN classifier (accuracy, PR-AUC) | (Biswal et al., [2020](#bib.bib17))
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 异常CNN分类器（准确度，PR-AUC） | (Biswal等，[2020](#bib.bib17)) |'
- en: '|  | Semantic descriptors | (Moradi et al., [2016](#bib.bib99)) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | 语义描述符 | (Moradi等，[2016](#bib.bib99)) |'
- en: '|  | ARS | (Alsharid et al., [2019](#bib.bib8)) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | ARS | (Alsharid等，[2019](#bib.bib8)) |'
- en: '| Medical correctness (automatic, auxiliary tasks) | ROC-AUC | (Li et al.,
    [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Li et al., [2019a](#bib.bib90);
    Zhang et al., [2020b](#bib.bib163)) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 医学正确性（自动，辅助任务） | ROC-AUC | (Li等，[2019b](#bib.bib87); Wang等，[2018](#bib.bib145);
    Li等，[2019a](#bib.bib90); Zhang等，[2020b](#bib.bib163)) |'
- en: '| Accuracy | (Zhang et al., [2017a](#bib.bib164); Zeng et al., [2018](#bib.bib159);
    Shin et al., [2016](#bib.bib121); Ma et al., [2018](#bib.bib95); Kisilev et al.,
    [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 准确度 | (Zhang等，[2017a](#bib.bib164); Zeng等，[2018](#bib.bib159); Shin等，[2016](#bib.bib121);
    Ma等，[2018](#bib.bib95); Kisilev等，[2016](#bib.bib77); Zeng等，[2020](#bib.bib158))
    |'
- en: '| Recall/sensitivity | (Yin et al., [2019](#bib.bib156); Harzig et al., [2019b](#bib.bib50))
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 召回率/敏感性 | (Yin等，[2019](#bib.bib156); Harzig等，[2019b](#bib.bib50)) |'
- en: '| Precision | (Yin et al., [2019](#bib.bib156); Harzig et al., [2019b](#bib.bib50);
    Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | (Yin等，[2019](#bib.bib156); Harzig等，[2019b](#bib.bib50); Kisilev等，[2016](#bib.bib77))
    |'
- en: '| Specificity | (Harzig et al., [2019b](#bib.bib50)) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 特异性 | (Harzig等，[2019b](#bib.bib50)) |'
- en: '| Pixel level accuracy | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 像素级准确度 | (Han等，[2018](#bib.bib48)) |'
- en: '| Pixel level specificity | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 像素级特异性 | (Han等，[2018](#bib.bib48)) |'
- en: '| Pixel level sensitivity | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 像素级敏感性 | (Han等，[2018](#bib.bib48)) |'
- en: '| Pixel level dice score | (Han et al., [2018](#bib.bib48); Tian et al., [2018](#bib.bib132))
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 像素级DICE分数 | (Han等，[2018](#bib.bib48); Tian等，[2018](#bib.bib132)) |'
- en: '| Medical correctness (with experts) | Assess correctness of the nature of
    hip fractures | (Gale et al., [2019](#bib.bib39)) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 医学正确性（与专家） | 评估髋部骨折的性质的正确性 | (Gale等，[2019](#bib.bib39)) |'
- en: '| Accept/reject rating | (Tian et al., [2018](#bib.bib132)) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 接受/拒绝评分 | (Tian等，[2018](#bib.bib132)) |'
- en: '|  | Assess medical and grammatical correctness, and relevance | (Alsharid
    et al., [2019](#bib.bib8)) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | 评估医学和语法正确性，以及相关性 | (Alsharid 等, [2019](#bib.bib8)) |'
- en: '|  | Agree with diagnosis | (Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | 同意诊断 | (Spinks 和 Moens, [2019](#bib.bib127)) |'
- en: '| Explainability (with experts) | Counter factual X-ray vs Saliency map | (Spinks
    and Moens, [2019](#bib.bib127)) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性（与专家） | 反事实 X 光与显著性图 | (Spinks 和 Moens, [2019](#bib.bib127)) |'
- en: '| Reports vs SmoothGrad (classification explanation) | (Gale et al., [2019](#bib.bib39))
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 报告与 SmoothGrad（分类解释） | (Gale 等, [2019](#bib.bib39)) |'
- en: Table 6\. Summary of the evaluation metrics used in the literature. The report
    based medical correctness type includes metrics that are measured from the report
    generated; the auxiliary task medical correctness ones evaluate an auxiliary or
    intermediate task in the process, such as classification or segmentation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 文献中使用的评估指标汇总。基于报告的医学正确性类型包括从生成的报告中测量的指标；辅助任务医学正确性指标评估过程中的辅助或中间任务，如分类或分割。
- en: 5.4.1\. Text quality metrics
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 文本质量指标
- en: 'The methods in this category measure general quality aspects of the generated
    text, and are originated from translation, summarizing or captioning tasks. The
    most widely used metrics in the papers reviewed are BLEU (Papineni et al., [2002](#bib.bib103)),
    ROUGE-L (Lin, [2004](#bib.bib91)), METEOR (Banerjee and Lavie, [2005](#bib.bib13);
    Lavie and Agarwal, [2007](#bib.bib83)) and CIDEr (Vedantam et al., [2015](#bib.bib141)),
    which measure the similarity of a target text (also referred to as candidate),
    against one or more reference texts (ground truth). These metrics are mainly based
    on counting n-gram matchings between the candidate and the ground truth. BLEU
    is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards
    precision or recall with a given parameter, and CIDEr attempts to capture both
    precision and recall through a TF-IDF score. Most of these metrics have variants
    and parameters for their calculation: ROUGE is a set of multiple metrics, being
    ROUGE-L the only one used in this task; METEOR has variants presented by the same
    authors (Denkowski and Lavie, [2010](#bib.bib31), [2011](#bib.bib32), [2014](#bib.bib33));
    and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别的方法测量生成文本的一般质量方面，源自翻译、摘要或字幕任务。在审查的论文中，最广泛使用的指标有 BLEU (Papineni 等, [2002](#bib.bib103))、ROUGE-L
    (Lin, [2004](#bib.bib91))、METEOR (Banerjee 和 Lavie, [2005](#bib.bib13); Lavie
    和 Agarwal, [2007](#bib.bib83)) 和 CIDEr (Vedantam 等, [2015](#bib.bib141))，这些指标测量目标文本（也称为候选文本）与一个或多个参考文本（真实值）之间的相似性。这些指标主要基于计算候选文本与真实值之间的
    n-gram 匹配。BLEU 以精确度为导向，ROUGE-L 和 METEOR 是 F1 分数，可以根据给定参数对精确度或召回率有所偏向，而 CIDEr 通过
    TF-IDF 分数尝试捕捉精确度和召回率。这些指标中的大多数有其变体和计算参数：ROUGE 是一组多重指标，其中 ROUGE-L 是唯一在此任务中使用的；METEOR
    有同一作者（Denkowski 和 Lavie, [2010](#bib.bib31), [2011](#bib.bib32), [2014](#bib.bib33))
    提出的变体；CIDEr 则提出了 CIDEr-D 变体以防止游戏化效果。
- en: SPICE (Anderson et al., [2016](#bib.bib10)) is a metric designed for the image
    captioning task, and evaluates the underlying meaning of the sentences describing
    the image scene, partially disregarding fluency or grammatical aspects. Specifically,
    the text is parsed as a graph, capturing the objects, their described characteristics
    and relations, which are then measured against the ground truth using an F1-score.
    Even though SPICE attempts to assess the semantic information in a caption, we
    believe it is not suitable for medical reports, as the graph parsing is designed
    for general domain objects. Nonetheless, Zhang et al. (Zhang et al., [2020b](#bib.bib163))
    presented the medical correctness metric MIRQI, applying a similar idea in a specific
    medical domain, which we will discuss in the next subsection ([5.4.2](#S5.SS4.SSS2
    "5.4.2\. Medical correctness metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images")).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: SPICE（Anderson et al., [2016](#bib.bib10)）是为图像字幕任务设计的指标，它评估描述图像场景的句子的潜在含义，部分忽略了流畅性或语法方面的内容。具体而言，文本被解析为图形，捕捉到对象、它们的描述特征和关系，然后使用
    F1 分数与真实情况进行比较。尽管 SPICE 试图评估字幕中的语义信息，但我们认为它不适用于医学报告，因为图形解析是针对一般领域的对象设计的。然而，Zhang
    等人（Zhang et al., [2020b](#bib.bib163)）提出了医学准确度指标 MIRQI，在特定医学领域应用了类似的思想，我们将在下一个小节中讨论（[5.4.2](#S5.SS4.SSS2
    "5.4.2\. 医学准确度指标 ‣ 5.4\. 评估指标 ‣ 5\. 论文分析 ‣ 深度学习与自动报告生成解释性的综述")）。
- en: Besides standard captioning metrics, we identified two other approaches to measure
    text quality. First, Alsharid et al. (Alsharid et al., [2019](#bib.bib8)) used
    Grammar Bot³³3[https://www.grammarbot.io/](https://www.grammarbot.io/), a rule
    and statistics based automated system that counts the grammatical errors in sentences.
    Second, Harzig et al. 2019a (Harzig et al., [2019a](#bib.bib49)) measured the
    sentence variability, by counting the different sentences in a set of reports.
    They argue that the sentences indicating abnormalities occur very rarely in the
    dataset, while the ones indicating normality are the most frequent. Hence, a certain
    level of variability is desired, and a system generating reports with low variability
    may indicate that not all medical conditions are being captured.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的字幕评估指标，我们还确定了另外两种文本质量测量方法。首先，Alsharid 等人（Alsharid et al., [2019](#bib.bib8)）使用了
    Grammar Bot³³3[https://www.grammarbot.io/](https://www.grammarbot.io/)，这是一种基于规则和统计的自动化系统，用于统计句子中的语法错误。其次，Harzig
    等人 2019a（Harzig et al., [2019a](#bib.bib49)）通过统计报告集中的不同句子来测量句子的变异性。他们认为，表示异常的句子在数据集中出现的频率非常低，而表示正常的句子则最为频繁。因此，所需的变异性水平是一定的，生成变异性较低报告的系统可能意味着未能捕捉到所有的医疗状况。
- en: 'Lastly, both works from Li et al. (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87))
    performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT),
    following the same procedure. The authors presented two reports to the AMT participants,
    one generated with the proposed model and one generated with the CoAtt model (Jing
    et al., [2018](#bib.bib69)) as baseline, and asked them to choose the most similar
    with the ground truth in terms of fluency, abnormalities correctness and content
    coverage. The results shown that their report was preferred around 50-60% of the
    cases, while the baseline around 20-30% (for the rest, none or both were preferred).
    We categorize this evaluation as a text quality metric, as the participants are
    not experts, and their answers are not fine-grained (i.e., did not specify what
    failed: fluency, correctness or coverage; or by how much they failed).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Li 等人（Li et al., [2018](#bib.bib86), [2019b](#bib.bib87)）的两项工作通过 Amazon Mechanical
    Turk (AMT) 对非专家用户进行了人工评估，采用了相同的程序。作者向 AMT 参与者展示了两份报告，一份是使用提议的模型生成的，另一份是使用 CoAtt
    模型（Jing et al., [2018](#bib.bib69)）作为基线生成的，并要求他们选择在流畅性、异常准确性和内容覆盖方面与真实情况最相似的报告。结果表明，他们的报告在大约
    50-60% 的情况下被偏好，而基线模型则在 20-30% 的情况下被偏好（其余情况则没有或都被偏好）。我们将这项评估归类为文本质量指标，因为参与者不是专家，他们的回答也不够细致（即未具体说明失败的方面：流畅性、准确性或覆盖；或失败的程度）。
- en: 5.4.2\. Medical correctness metrics
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 医学准确度指标
- en: While the most common purpose of the text quality metrics is to measure the
    similarity between the generated report and a ground truth, they do not necessarily
    capture the medical facts in the reports (Boag et al., [2020](#bib.bib18); Zhang
    et al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93); Babar et al., [2021](#bib.bib12);
    Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)). For example, the sentences
    “effusion is observed” and “effusion is not observed” are very similar, thus may
    present a very high score for any metric based on n-gram matching, though the
    medical facts are the exact opposite. Therefore, an evaluation directly measuring
    the reports correctness is required, not necessarily taking into account fluency,
    grammatical rules or text quality in general. From the literature reviewed, in
    ten works (Huang et al., [2019](#bib.bib62); Xue et al., [2018](#bib.bib155);
    Li et al., [2018](#bib.bib86); Jing et al., [2019](#bib.bib68); Liu et al., [2019](#bib.bib93);
    Biswal et al., [2020](#bib.bib17); Alsharid et al., [2019](#bib.bib8); Zhang et al.,
    [2020b](#bib.bib163); Moradi et al., [2016](#bib.bib99); Wu et al., [2017](#bib.bib149))
    the authors presented an automatic metric to address this issue, four works (Gale
    et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132); Alsharid et al.,
    [2019](#bib.bib8); Spinks and Moens, [2019](#bib.bib127)) did a formal expert
    evaluation, and multiple works (Yuan et al., [2019](#bib.bib157); Li et al., [2019b](#bib.bib87);
    Wang et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164); Li et al.,
    [2019a](#bib.bib90); Yin et al., [2019](#bib.bib156); Ma et al., [2018](#bib.bib95);
    Zeng et al., [2018](#bib.bib159); Harzig et al., [2019b](#bib.bib50); Zhang et al.,
    [2020b](#bib.bib163); Shin et al., [2016](#bib.bib121); Tian et al., [2018](#bib.bib132);
    Zeng et al., [2020](#bib.bib158); Spinks and Moens, [2019](#bib.bib127); Kisilev
    et al., [2016](#bib.bib77)) evaluated medical correctness indirectly from auxiliary
    tasks. The methods are listed in Table [6](#S5.T6 "Table 6 ‣ 5.4\. Evaluation
    Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images") and are further discussed
    next.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本质量度量的最常见目的是衡量生成报告与真实情况之间的相似性，但它们不一定能够捕捉报告中的医学事实（Boag et al., [2020](#bib.bib18);
    Zhang et al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93); Babar et al.,
    [2021](#bib.bib12); Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)）。例如，“观察到积液”和“未观察到积液”这两句话非常相似，因此可能会为任何基于n-gram匹配的度量提供非常高的分数，尽管医学事实却恰恰相反。因此，需要一种直接衡量报告正确性的评估方法，不一定考虑流畅性、语法规则或文本质量。从所评审的文献来看，在十项研究中（Huang
    et al., [2019](#bib.bib62); Xue et al., [2018](#bib.bib155); Li et al., [2018](#bib.bib86);
    Jing et al., [2019](#bib.bib68); Liu et al., [2019](#bib.bib93); Biswal et al.,
    [2020](#bib.bib17); Alsharid et al., [2019](#bib.bib8); Zhang et al., [2020b](#bib.bib163);
    Moradi et al., [2016](#bib.bib99); Wu et al., [2017](#bib.bib149))，作者提出了一种自动度量来解决这个问题；四项研究（Gale
    et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132); Alsharid et al.,
    [2019](#bib.bib8); Spinks and Moens, [2019](#bib.bib127)）进行了正式的专家评估，而多个研究（Yuan
    et al., [2019](#bib.bib157); Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Zhang et al., [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90); Yin et al.,
    [2019](#bib.bib156); Ma et al., [2018](#bib.bib95); Zeng et al., [2018](#bib.bib159);
    Harzig et al., [2019b](#bib.bib50); Zhang et al., [2020b](#bib.bib163); Shin et
    al., [2016](#bib.bib121); Tian et al., [2018](#bib.bib132); Zeng et al., [2020](#bib.bib158);
    Spinks and Moens, [2019](#bib.bib127); Kisilev et al., [2016](#bib.bib77)）通过辅助任务间接评估了医学正确性。这些方法列于表[6](#S5.T6
    "Table 6 ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")，并在接下来进行了进一步讨论。
- en: In several works the authors presented a method that detects concepts in the
    generated and ground truth reports, and compare the results using common classification
    metrics, such as accuracy, F1-score, and more. The main difference between these
    methods lies in how the concepts are automatically detected in the reports. The
    simplest approaches are keyword-based, which consists in reporting the ratio of
    a set of keywords found between the generated report and ground truth, like MeSH
    Accuracy (Huang et al., [2019](#bib.bib62)) that uses MeSH terms, and Keyword
    Accuracy that uses 438 MTI terms (presented by Xue et al. (Xue et al., [2018](#bib.bib155))
    and used in A3FN (Xie et al., [2019](#bib.bib151))). Similarly, Medical Abnormality
    Terminology Detection (Li et al., [2018](#bib.bib86)) calculates precision and
    false positive rate of the 10 most frequent abnormality-related terms in the dataset;
    and Wu et al. (Wu et al., [2017](#bib.bib149)) calculated accuracy, sensitivity
    and specificity for a set of keywords.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些研究中，作者们提出了一种方法，用于检测生成报告和实际报告中的概念，并使用常见的分类指标，如准确率、F1分数等来比较结果。这些方法之间的主要区别在于概念如何在报告中自动检测。最简单的方法是基于关键词的，这种方法通过报告生成报告和实际报告之间找到的关键词集合的比例来进行评估，例如使用MeSH术语的MeSH准确率（Huang
    et al., [2019](#bib.bib62)），以及使用438个MTI术语的关键词准确率（由Xue et al.（Xue et al., [2018](#bib.bib155)）提出并用于A3FN（Xie
    et al., [2019](#bib.bib151)））。类似地，医学异常术语检测（Li et al., [2018](#bib.bib86)）计算数据集中10个最常见异常相关术语的精确度和假阳性率；而Wu
    et al.（Wu et al., [2017](#bib.bib149)）计算了一组关键词的准确度、灵敏度和特异性。
- en: 'Other approaches are abnormality-based, which attempt to directly classify
    abnormalities from the report by different means: Abnormality Detection (Jing
    et al., [2019](#bib.bib68)) uses manually designed patterns; Medical Abnormality
    Detection (Liu et al., [2019](#bib.bib93)) uses the CheXpert labeler tool (Irvin
    et al., [2019](#bib.bib64)); Biswal et al. (Biswal et al., [2020](#bib.bib17))
    used a character-level CNN (Zhang et al., [2015](#bib.bib161)) that classifies
    multiple CheXpert labels (Irvin et al., [2019](#bib.bib64)); and Moradi et al.
    (Moradi et al., [2016](#bib.bib99)) used a proprietary software to extract semantic
    descriptors. Lastly, Anatomical Relevance Score (ARS) (Alsharid et al., [2019](#bib.bib8))
    is a body-part-based approach, which detects the anatomical elements mentioned
    in a report considering the vocabulary used. Though these methods may be useful
    for measuring medical correctness to a certain degree, there is no consensus or
    standard, and there is no formal evaluation of the correlation with expert judgement.
    From the discussed techniques, Alsharid et al. (Alsharid et al., [2019](#bib.bib8))
    are the only authors that also performed an expert evaluation of the generated
    reports, though they did not conduct a correlation or similar analysis to validate
    the ARS method.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法则是基于异常的，这些方法尝试通过不同手段直接对报告中的异常进行分类：异常检测（Jing et al., [2019](#bib.bib68)）使用手动设计的模式；医学异常检测（Liu
    et al., [2019](#bib.bib93)）使用CheXpert标注工具（Irvin et al., [2019](#bib.bib64)）；Biswal
    et al.（Biswal et al., [2020](#bib.bib17)）使用了字符级CNN（Zhang et al., [2015](#bib.bib161)）来分类多个CheXpert标签（Irvin
    et al., [2019](#bib.bib64)）；Moradi et al.（Moradi et al., [2016](#bib.bib99)）使用专有软件提取语义描述符。最后，解剖相关评分（ARS）（Alsharid
    et al., [2019](#bib.bib8)）是一种基于身体部位的方法，它考虑到报告中使用的词汇来检测提到的解剖元素。虽然这些方法在一定程度上对医学准确性测量可能有用，但目前尚无共识或标准，也没有正式评估与专家判断的相关性。在讨论的技术中，Alsharid
    et al.（Alsharid et al., [2019](#bib.bib8)）是唯一一个对生成报告进行了专家评估的作者，尽管他们没有进行相关性或类似的分析来验证ARS方法。
- en: Zhang et al. (Zhang et al., [2020b](#bib.bib163)) went further with the concept
    extraction and presented Medical Image Report Quality Index (MIRQI), which works
    in a similar fashion as the SPICE (Anderson et al., [2016](#bib.bib10)) metric
    presented in the text quality subsection ([5.4.1](#S5.SS4.SSS1 "5.4.1\. Text quality
    metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")). MIRQI applies ideas from NegBio (Peng et al., [2018](#bib.bib107))
    and the CheXpert labeler (Irvin et al., [2019](#bib.bib64)) to identify diseases
    or medical conditions in the reports, considering synonyms and negations, and
    uses the Stanford parser (Chen and Manning, [2014](#bib.bib24)) to obtain semantic
    dependencies and finer-grained attributes from each sentence, such as severity,
    size, shape, body parts, etc. With this information, an abnormality graph is built
    for each report, where each node is a disease with its attributes, and the nodes
    are connected if they belong to the same organ or tissue. Lastly, the graphs from
    the ground truth and generated reports are matched node-wise, and MIRQI-p (precision),
    MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly
    discussed correctness metrics, we believe this approach seems more robust to assess
    the medical facts in the reports, as it attempts to capture the attributes and
    relations, opposed to the concepts only. However, the authors did not present
    an evaluation against expert judgement, so we cannot determine if this metric
    is sufficient.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人（张等人，[2020b](#bib.bib163)）进一步发展了概念提取方法，并提出了医学图像报告质量指数（MIRQI），其工作原理类似于文献质量子部分中介绍的SPICE（Anderson等人，[2016](#bib.bib10)）度量标准（[5.4.1](#S5.SS4.SSS1
    "5.4.1\. 文本质量指标 ‣ 5.4\. 评估指标 ‣ 5\. 文献分析 ‣ 关于深度学习和自动报告生成的解释性调查")）。MIRQI应用了NegBio（Peng等人，[2018](#bib.bib107)）和CheXpert标签器（Irvin等人，[2019](#bib.bib64)）中的思想，以识别报告中的疾病或医疗状况，考虑同义词和否定，并使用斯坦福解析器（Chen和Manning，[2014](#bib.bib24)）从每个句子中获取语义依赖关系和更细粒度的属性，如严重程度、大小、形状、身体部位等。利用这些信息，为每个报告构建一个异常图，其中每个节点是一个带有其属性的疾病，节点之间如果属于同一器官或组织则连接在一起。最后，匹配地面真实数据和生成报告的图，并计算MIRQI-p（准确率）、MIRQI-r（召回率）和MIRQI-F1（F1-score）。与先前讨论的正确性指标相比，我们认为这种方法似乎更稳健，可以评估报告中的医学事实，因为它尝试捕捉属性和关系，而不仅仅是概念。然而，作者没有进行专家判断的评估，因此我们无法确定该度量是否充分。
- en: 'Considering human evaluation, only a few works (Gale et al., [2019](#bib.bib39);
    Tian et al., [2018](#bib.bib132); Alsharid et al., [2019](#bib.bib8); Spinks and
    Moens, [2019](#bib.bib127)) present a formal expert medical correctness assessment.
    In the work by Alsharid et al. (Alsharid et al., [2019](#bib.bib8)) a medical
    professional assessed the reports on a Likert Scale from 0 to 2 in four different
    aspects: accurately describes the image, presents no incorrect information, is
    grammatically correct and is relevant for the image; the results were further
    separated for samples from different body parts, showing averages between 0.5
    and 1. Gale et al. (Gale et al., [2019](#bib.bib39)) asked a radiologist to evaluate
    the correctness of the hip fractures description, finding that the fracture’s
    character was properly described 98% of the cases, while the fracture location
    only for 90%. In the work by Tian et al. (Tian et al., [2018](#bib.bib132)) a
    medical expert evaluated 30 randomly selected reports with a rating from 1 (definite
    accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and
    Moens (Spinks and Moens, [2019](#bib.bib127)) asked four questions to three experts
    regarding the generated reports, where the third and fourth questions measured
    correctness: “Do you agree with the proposed diagnosis?”, answering 0 (no) or
    1 (yes) and “How certain are you about your final diagnosis?”, from 1 (not sure)
    to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement
    with the model’s diagnosis, and certainty on the experts’ diagnoses. The other
    questions concerned explainability aspects, and are detailed in the next subsection
    ([5.4.3](#S5.SS4.SSS3 "5.4.3\. Explainability metrics ‣ 5.4\. Evaluation Metrics
    ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")). So far, there is no standard
    approach to perform an expert evaluation, though we believe the first two approaches
    provide finer-grained information than the latter two, and hence should be more
    useful for determining in which cases the models are failing and for designing
    improvements. The certainty question should also be very useful, as diagnoses
    may be susceptible to human judgement.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人工评估，只有少数几项工作（Gale et al., [2019](#bib.bib39); Tian et al., [2018](#bib.bib132);
    Alsharid et al., [2019](#bib.bib8); Spinks and Moens, [2019](#bib.bib127)）进行了正式的专家医学正确性评估。在
    Alsharid et al.（Alsharid et al., [2019](#bib.bib8)）的工作中，一名医疗专业人员在四个不同方面对报告进行了
    Likert 量表从 0 到 2 的评估：准确描述图像、没有不正确信息、语法正确且与图像相关；结果进一步按不同身体部位的样本进行区分，显示平均值在 0.5
    和 1 之间。Gale et al.（Gale et al., [2019](#bib.bib39)）让一名放射科医师评估髋部骨折描述的正确性，发现骨折的特征在
    98% 的情况下得到了正确描述，而骨折位置的正确描述率为 90%。在 Tian et al.（Tian et al., [2018](#bib.bib132)）的工作中，一名医学专家对
    30 份随机选择的报告进行了从 1（完全接受）到 5（完全拒绝）的评分，平均得分为 2.33。最后，Spinks 和 Moens（Spinks and Moens,
    [2019](#bib.bib127)）向三名专家提出了关于生成报告的四个问题，其中第三和第四个问题测量了正确性：“您是否同意提议的诊断？”回答为 0（不同意）或
    1（同意）以及“您对最终诊断的确定性如何？”，从 1（不确定）到 4（非常确定）。平均得分较高（0.88 和 3.75），显示出对模型诊断的一致性以及对专家诊断的确定性。其他问题涉及解释性方面，并在下一小节中详细说明（[5.4.3](#S5.SS4.SSS3
    "5.4.3\. 解释性指标 ‣ 5.4\. 评估指标 ‣ 5\. 论文分析 ‣ 关于深度学习和医学图像自动报告生成的解释性调查")）。目前，还没有标准的方法来进行专家评估，尽管我们认为前两种方法提供的信息比后两种方法更为细致，因此在确定模型失败的情况和设计改进方面应更为有用。确定性问题也应非常有用，因为诊断可能会受到人工判断的影响。
- en: Lastly, multiple papers (Yuan et al., [2019](#bib.bib157); Li et al., [2019b](#bib.bib87);
    Wang et al., [2018](#bib.bib145); Zhang et al., [2017a](#bib.bib164); Li et al.,
    [2019a](#bib.bib90); Yin et al., [2019](#bib.bib156); Ma et al., [2018](#bib.bib95);
    Zeng et al., [2018](#bib.bib159); Harzig et al., [2019b](#bib.bib50); Zhang et al.,
    [2020b](#bib.bib163); Shin et al., [2016](#bib.bib121); Tian et al., [2018](#bib.bib132))
    evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other
    typical classification or segmentation metrics, as shown in Table [6](#S5.T6 "Table
    6 ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep
    Learning and Explainability for Automatic Report Generation from Medical Images").
    Note that in any of these cases, the task is a previous or intermediary step of
    the process and is not derived from the report. In consequence, even if the classification
    has great performance, the language component could be performing poorly, and
    the generated reports still may be inaccurate. Accordingly, we believe this type
    of measure should not be used as the primary report correctness evaluation, unless
    it can be proven that the report reproduces exactly the classification made (e.g.
    by a template-filling process).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，多篇论文（Yuan等，[2019](#bib.bib157)；Li等，[2019b](#bib.bib87)；Wang等，[2018](#bib.bib145)；Zhang等，[2017a](#bib.bib164)；Li等，[2019a](#bib.bib90)；Yin等，[2019](#bib.bib156)；Ma等，[2018](#bib.bib95)；Zeng等，[2018](#bib.bib159)；Harzig等，[2019b](#bib.bib50)；Zhang等，[2020b](#bib.bib163)；Shin等，[2016](#bib.bib121)；Tian等，[2018](#bib.bib132)）评估了辅助任务的性能，使用了ROC-AUC、准确性和其他典型的分类或分割指标，如表[6](#S5.T6
    "表 6 ‣ 5.4\. 评估指标 ‣ 5\. 审查论文的分析 ‣ 关于深度学习和自动报告生成的可解释性调查")所示。请注意，在这些情况下，任务是过程的前置或中间步骤，并不是从报告中派生的。因此，即使分类表现优异，语言组件可能表现不佳，生成的报告仍可能不准确。因此，我们认为这种测量方法不应作为主要的报告正确性评估，除非能够证明报告完全复现了所做的分类（例如，通过模板填充过程）。
- en: 5.4.3\. Explainability metrics
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3\. 可解释性指标
- en: Providing interpretable justifications for the model’s outcome is essential
    in this medical domain, and furthermore, we should be able to evaluate them to
    answer questions such as, does the method justify the model’s decision?, which
    method provides a better explanation? However, there is no consensus on evaluation
    methods for AI explainability, and in many cases the definition of a better explanation
    remains subjective (Doshi-Velez and Kim, [2017](#bib.bib34); Reyes et al., [2020](#bib.bib114);
    Carvalho et al., [2019](#bib.bib23)).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域，为模型结果提供可解释的理由至关重要，而且，我们还应该能够评估这些理由，以回答诸如“该方法是否解释了模型的决策？”、“哪种方法提供了更好的解释？”等问题。然而，对于AI可解释性的评估方法尚无共识，许多情况下，对更好解释的定义仍然是主观的（Doshi-Velez和Kim，[2017](#bib.bib34)；Reyes等，[2020](#bib.bib114)；Carvalho等，[2019](#bib.bib23)）。
- en: 'Consequently, none of the papers reviewed used an automatic metric to assess
    explainability, and only two works (Gale et al., [2019](#bib.bib39); Spinks and
    Moens, [2019](#bib.bib127)) conduct a formal human expert evaluation. Gale et
    al. (Gale et al., [2019](#bib.bib39)) presented the report generation as an explanation
    of a medical image classification task, and evaluated it by comparing three methods:
    (a) SmoothGrad (Smilkov et al., [2017](#bib.bib125)) to highlight the most important
    pixels used, (b) a generated report in natural language, and (c) both placed side
    by side. Five experts assessed 30 images, rating each explanation in a scale from
    1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7
    and (c) 8.8 for each method. Though the authors emphasize the importance of the
    natural language explanations, their approach does not include an explanation
    for the report itself, so it cannot be directly used for the report generation
    task.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，审查的论文中没有使用自动指标来评估可解释性，只有两篇作品（Gale等，[2019](#bib.bib39)；Spinks和Moens，[2019](#bib.bib127)）进行了正式的人工专家评估。Gale等（Gale等，[2019](#bib.bib39)）将报告生成作为医学图像分类任务的解释，并通过比较三种方法进行评估：（a）SmoothGrad（Smilkov等，[2017](#bib.bib125)）以突出最重要的像素，（b）用自然语言生成的报告，以及（c）两者并排放置。五位专家评估了30张图像，按照从1（不满意）到10（完美）的尺度对每个解释进行了评分；每种方法的平均分分别为（a）4.4，（b）7和（c）8.8。虽然作者强调了自然语言解释的重要性，但他们的方法并未包括对报告本身的解释，因此无法直接用于报告生成任务。
- en: 'The model proposed by Spinks and Moens (Spinks and Moens, [2019](#bib.bib127))
    generates a chest X-ray as a counter-factual example, and they compared this explanation
    method against a feature importance heatmap generated with the Zagoruyko and Komodakis
    saliency map technique (Komodakis and Zagoruyko, [2017](#bib.bib78)). Three experts
    evaluated 150 samples answering four questions, the first two regarding explainability
    aspects: “Does the explanation justify the diagnosis?” “Does the model appear
    to understand the important parts of the X-ray?” The answers were in a scale from
    1 (no) to 4 (yes), and their method achieved a higher score than the saliency
    map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing
    their counter-factual approach should be better in this setting. The other two
    questions relate more to medical correctness, and are discussed in the previous
    section ([5.4.2](#S5.SS4.SSS2 "5.4.2\. Medical correctness metrics ‣ 5.4\. Evaluation
    Metrics ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images")).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Spinks 和 Moens（Spinks and Moens, [2019](#bib.bib127)）提出的模型生成了一个胸部 X 光图像作为反事实示例，他们将这种解释方法与使用
    Zagoruyko 和 Komodakis 显著性图技术（Komodakis and Zagoruyko, [2017](#bib.bib78)）生成的特征重要性热图进行了比较。三位专家评估了
    150 个样本，并回答了四个问题，其中前两个涉及可解释性方面：“解释是否能证明诊断结果？” “模型是否能够理解 X 光图像的重要部分？”答案的评分范围从 1（否）到
    4（是），他们的方法在评分上优于显著性图（第一个问题为 2.39 对 1.31，第二个问题为 2.45 对 1.81），显示出他们的反事实方法在这种设置下更具优势。其他两个问题更多地涉及医学正确性，已在前一节中讨论过（[5.4.2](#S5.SS4.SSS2
    "5.4.2\. 医学正确性指标 ‣ 5.4\. 评估指标 ‣ 5\. 论文分析 ‣ 关于深度学习和医学图像自动报告生成的可解释性调查")）。
- en: 'We believe the explanation evaluations should be very important in this area,
    and as there is no consensus, we outline some possible guidelines. Following ideas
    from Tonekaboni et al. (Tonekaboni et al., [2019](#bib.bib135)), we believe three
    aspects from the explanations should be assessed: (1) consistency, (2) alignment
    with domain knowledge, and (3) user impact. First, the consistency across the
    data should be assessed, answering questions such as: do explanations change with
    variations to the input data?; or to the prediction?; or to the model design?;
    or with different images from the same patient? As pointed out by Tonekaboni et
    al. (Tonekaboni et al., [2019](#bib.bib135)), inconsistent explanations may negatively
    affect the clinicians’ trust, and an interpretability method laying them out should
    be reviewed. Examples of consistency or robustness evaluations can be found in
    the work by Adebayo et al. (Adebayo et al., [2018](#bib.bib4)) for image saliency
    maps, and in the work by Jain and Wallace (Jain and Wallace, [2019](#bib.bib67))
    for attention in recurrent neural networks.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为解释评估在这一领域应该非常重要，由于目前尚未达成共识，我们概述了一些可能的指南。根据 Tonekaboni 等人（Tonekaboni et al.,
    [2019](#bib.bib135)）的观点，我们认为应该评估解释的三个方面：（1）一致性，（2）与领域知识的一致性，以及（3）对用户的影响。首先，应评估数据的一致性，回答以下问题：解释是否随着输入数据的变化而改变？；或随着预测的变化？；或随着模型设计的变化？；或随着同一患者的不同图像而变化？正如
    Tonekaboni 等人（Tonekaboni et al., [2019](#bib.bib135)）指出的那样，不一致的解释可能会对临床医生的信任产生负面影响，因此应对展示这些解释的可解释性方法进行审查。在
    Adebayo 等人（Adebayo et al., [2018](#bib.bib4)）的工作中可以找到一致性或稳健性评估的例子，这些例子涉及图像显著性图；在
    Jain 和 Wallace（Jain and Wallace, [2019](#bib.bib67)）的工作中则涉及递归神经网络中的注意力机制。
- en: 'Second, the alignment with domain knowledge should evaluate if the explanation
    is consistent with an expert’s knowledge: would they provide the same explanation
    for that decision? For instance, given a feature importance method, is the model
    focusing on the correct features? As an example, consider the second and third
    questions employed by Spinks and Moens (Spinks and Moens, [2019](#bib.bib127))
    detailed earlier. To mention other examples, Wang et al. (Wang et al., [2017](#bib.bib144))
    evaluated CAM (Zhou et al., [2016](#bib.bib166)) generated heatmaps for disease
    classification against expert provided bounding-boxes locating the diseases, using
    intersection-over-union like metrics; Kim et al. (Kim et al., [2018](#bib.bib76))
    proposed a model to classify Diabetic Retinopathy from retina fundus images, and
    they compared the TCAV (Kim et al., [2018](#bib.bib76)) extracted concepts against
    expert knowledge. Notice many works reviewed in this survey used classification
    or segmentation as an auxiliary task, which can be used as local explanations,
    and evaluated them with common metrics (such as accuracy, precision, etc.), as
    discussed in the previous sections ([5.3](#S5.SS3 "5.3\. Explainability ‣ 5\.
    Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability for
    Automatic Report Generation from Medical Images") and [5.4.2](#S5.SS4.SSS2 "5.4.2\.
    Medical correctness metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")). As the authors did not mention the secondary outputs as
    local explanations, we categorized the said evaluations as medical correctness
    metrics, but they are also measuring alignment with domain knowledge for the interpretability
    methods, and as such may be very useful.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，领域知识的一致性应评估解释是否与专家的知识一致：他们是否会对该决策提供相同的解释？例如，给定特征重要性方法，模型是否专注于正确的特征？作为示例，考虑Spinks和Moens（Spinks
    and Moens, [2019](#bib.bib127)）之前详细讨论的第二和第三个问题。再举其他例子，Wang等人（Wang et al., [2017](#bib.bib144)）评估了CAM（Zhou
    et al., [2016](#bib.bib166)）生成的热图用于疾病分类，并与专家提供的定位疾病的边界框进行了比较，使用了类似于交并比的度量；Kim等人（Kim
    et al., [2018](#bib.bib76)）提出了一个模型来从视网膜眼底图像中分类糖尿病视网膜病变，他们将TCAV（Kim et al., [2018](#bib.bib76)）提取的概念与专家知识进行了比较。注意，本调查中回顾的许多工作使用了分类或分割作为辅助任务，这可以作为局部解释，并用常见指标（如准确性、精确度等）进行评估，正如前面章节（[5.3](#S5.SS3
    "5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")和[5.4.2](#S5.SS4.SSS2
    "5.4.2\. Medical correctness metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images")所讨论的那样。由于作者没有将次要输出视为局部解释，我们将上述评估归类为医学正确性指标，但它们也在测量解释方法与领域知识的一致性，因此可能非常有用。
- en: Lastly, the user impact should attempt to answer questions like, is it a good
    explanation? Does it provide useful or novel information? Does it justify the
    model’s decision? Is it provided with an appropriate representation for the experts?
    As examples, the assessment proposed by Gale et al. (Gale et al., [2019](#bib.bib39))
    and the first question used by Spinks and Moens (Spinks and Moens, [2019](#bib.bib127))
    measure user impact. Notice that most of these concepts are very subjective, and
    the definitions, the questions and assessments will vary for different sub-domains
    and target experts. We believe more specific definitions and fine-grained aspects
    should arise in the future, as research in this topic grows. For reference, this
    category includes the domain appropriate representation and potential actionability
    concepts presented by Tonekaboni et al. (Tonekaboni et al., [2019](#bib.bib135)).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用户影响应尝试回答以下问题，例如，解释是否足够好？它是否提供了有用或新颖的信息？它是否为模型的决策提供了合理的依据？是否为专家提供了适当的表示？例如，Gale等人（Gale
    et al., [2019](#bib.bib39)）提出的评估和Spinks和Moens（Spinks and Moens, [2019](#bib.bib127)）使用的第一个问题衡量用户影响。请注意，这些概念大多是非常主观的，定义、问题和评估在不同的子领域和目标专家中会有所不同。我们相信，随着这一主题研究的增长，未来应出现更具体的定义和更精细的方面。作为参考，本类别包括Tonekaboni等人（Tonekaboni
    et al., [2019](#bib.bib135)）提出的领域适用表示和潜在可操作性概念。
- en: Synthesis
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 综合
- en: Almost all the works include text quality metrics, though these are not able
    to capture the medical facts in a report (Boag et al., [2020](#bib.bib18); Zhang
    et al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93); Babar et al., [2021](#bib.bib12);
    Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)). Several works proposed
    medical correctness assessments over the reports, but unfortunately none of the
    proposals was evaluated against expert judgement. The auxiliary tasks can be evaluated
    to measure correctness indirectly from the process, but often it will not be sufficient
    for the report’s correctness. Only two works evaluate explainability directly
    with experts, and the auxiliary tasks’ assessments could be useful to measure
    alignment between the explanations and domain knowledge. Overall, we believe that
    medical correctness should be the primary aspect to evaluate in the generated
    reports, using one or more automatic metrics. For now, and even though none of
    the metrics proposed has been evaluated against expert judgement, MIRQI (Zhang
    et al., [2020b](#bib.bib163)) seems like the most promising approach to fulfill
    this purpose, as it should be able to capture richer information from the reports.
    Additionally, text quality metrics can be used as a secondary evaluation, since
    they may be useful for measuring fluency, grammar or variability, and to compare
    with previous baselines. Lastly, explainability evaluation methods should arise
    to assess multiple key aspects, such as its consistency, alignment with domain
    knowledge, and the user impact.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的研究都包含文本质量指标，但这些指标无法捕捉报告中的医学事实（Boag et al., [2020](#bib.bib18); Zhang et
    al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93); Babar et al., [2021](#bib.bib12);
    Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)）。一些研究提出了对报告的医学正确性评估，但遗憾的是，没有一个提案经过专家判断评估。辅助任务可以通过过程间接评估正确性，但往往不足以确保报告的正确性。只有两个研究直接与专家评估解释性，而辅助任务的评估可能有助于衡量解释与领域知识之间的对齐。总体而言，我们认为医学正确性应该是生成报告中主要的评估方面，使用一种或多种自动化指标。即便如此，尽管没有任何提议的指标经过专家判断评估，MIRQI
    (Zhang et al., [2020b](#bib.bib163)) 似乎是实现这一目标的最有前途的方法，因为它应能从报告中捕捉到更丰富的信息。此外，文本质量指标可以作为辅助评估，因为它们可能对衡量流畅性、语法或变异性有用，并与以前的基线进行比较。最后，应出现解释性评估方法，以评估多个关键方面，如其一致性、与领域知识的对齐以及对用户的影响。
- en: 5.5\. Comparison of papers’ performance
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 论文性能比较
- en: 'To find out which paper holds the state of the art, we need to find a common
    ground for fair comparison. A natural choice is the IU X-ray dataset (Demner-Fushman
    et al., [2015](#bib.bib29)), since a majority of the surveyed papers report results
    in this dataset. Table [7](#S5.T7 "Table 7 ‣ 5.5\. Comparison of papers’ performance
    ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and Explainability
    for Automatic Report Generation from Medical Images") shows these results, separated
    by which report sections are generated by each paper, findings, impression or
    both. The findings section consists of multiple sentences, and mainly describes
    medical conditions observed, while the impression section is a one sentence conclusion
    or diagnosis. Notice Spinks and Moens (Spinks and Moens, [2019](#bib.bib127))
    filtered the findings section, and kept only sentences referring to one disease
    (Cardiomegaly). The papers that seem to show the best performance in terms of
    NLP metrics are KERP (Li et al., [2019b](#bib.bib87)), CLARA (Biswal et al., [2020](#bib.bib17))
    and Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) for the findings section,
    MTMA (Tian et al., [2019](#bib.bib133)) for the impression section, and Yuan et
    al. (Yuan et al., [2019](#bib.bib157)), MLMA (Gajbhiye et al., [2020](#bib.bib37))
    and Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) for both sections. Of
    these, only MTMA has a large difference to its competitors, and there is no clear
    winner in the other sections. Some caveats, however, should be kept in mind when
    interpreting these results:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定哪篇论文处于最前沿，我们需要找到一个公平比较的共同基础。一个自然的选择是IU X-ray数据集（Demner-Fushman等，[2015](#bib.bib29)），因为大多数调查论文都在这个数据集上报告结果。表[7](#S5.T7
    "表 7 ‣ 5.5\. 论文性能比较 ‣ 5\. 文献分析 ‣ 深度学习与医学图像自动报告生成的可解释性调查")展示了这些结果，按照每篇论文生成的报告部分、发现、印象或两者的不同来分隔。发现部分由多个句子组成，主要描述观察到的医学情况，而印象部分是一个句子的结论或诊断。注意Spinks和Moens（Spinks和Moens，[2019](#bib.bib127)）过滤了发现部分，只保留了涉及一种疾病（心脏扩大）的句子。在NLP指标方面表现最好的论文似乎是KERP（Li等，[2019b](#bib.bib87)）、CLARA（Biswal等，[2020](#bib.bib17)）和Xue等2019（Xue和Huang，[2019](#bib.bib154)）的发现部分，MTMA（Tian等，[2019](#bib.bib133)）的印象部分，以及Yuan等（Yuan等，[2019](#bib.bib157)）、MLMA（Gajbhiye等，[2020](#bib.bib37)）和Xue等2019（Xue和Huang，[2019](#bib.bib154)）的两个部分。在这些中，只有MTMA在其竞争者中差异较大，其他部分没有明显的胜者。然而，在解释这些结果时应注意以下几点：
- en: (1) The results reported in the literature only allow comparisons in terms of
    standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results
    we cannot draw conclusions about medical correctness, since NLP metrics and clinical
    accuracy are not necessarily correlated.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 文献中报告的结果仅允许在标准自然语言指标（BLEU、ROUGE-L等）方面进行比较，但从这些结果中我们无法得出医学正确性的结论，因为NLP指标与临床准确性不一定相关。
- en: (2) MTMA uses additional input, as discussed in section [5.2.1](#S5.SS2.SSS1
    "5.2.1\. Input and Output ‣ 5.2\. Model Design ‣ 5\. Analysis of papers reviewed
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"). Specifically, the model receives the indication and findings
    sections of the report to generate the impression section, at both test and train
    stages. In a sense, this could be seen as an enhanced summarizing approach, since
    the impression section contains a conclusion from the findings.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: (2) MTMA使用了额外的输入，如[5.2.1](#S5.SS2.SSS1 "5.2.1\. 输入与输出 ‣ 5.2\. 模型设计 ‣ 5\. 文献分析
    ‣ 深度学习与医学图像自动报告生成的可解释性调查")节中所讨论的。具体来说，模型接收报告的指示和发现部分以生成印象部分，在测试和训练阶段都是如此。从某种意义上说，这可以被视为一种增强的总结方法，因为印象部分包含了来自发现部分的结论。
- en: (3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters,
    as discussed in section [5.4](#S5.SS4 "5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"). Unfortunately, most papers do not mention
    the specific version or implementation used.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 一些NLP评估指标，如CIDEr、ROUGE-L和METEOR，具有变体和参数，如[5.4](#S5.SS4 "5.4\. 评估指标 ‣ 5\.
    文献分析 ‣ 深度学习与医学图像自动报告生成的可解释性调查")节中所述。不幸的是，大多数论文未提及所使用的具体版本或实现。
- en: (4) The IU X-ray dataset does not have standard training-validation-test splits.
    This has led researchers to define their own splits, as indicated by column Split
    of Table [7](#S5.T7 "Table 7 ‣ 5.5\. Comparison of papers’ performance ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"). These splits are not consistent across
    papers, making results less comparable. For example, if a model was evaluated
    in an easier test split, that would give it an unfair advantage over other models
    evaluated in harder test splits. Additionally, other decisions such the number
    of images per report (frontal, lateral or both), the tokenization algorithm employed,
    the removal of noisy sentences, the removal of words with a frequency under a
    given threshold, the removal of duplicate images, among other preprocessing decisions,
    are not always explicitly stated in papers, and these may have an impact on the
    results as well.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: (4) IU X-ray 数据集没有标准的训练-验证-测试划分。这导致研究人员定义了自己的划分，如表[7](#S5.T7 "Table 7 ‣ 5.5\.
    Comparison of papers’ performance ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images")的“划分”列所示。这些划分在论文中不一致，使结果的可比性降低。例如，如果一个模型在较简单的测试划分中进行评估，这将使其相对于在较困难的测试划分中评估的其他模型具有不公平的优势。此外，其他决策如每份报告的图像数量（正面、侧面或两者）、使用的分词算法、去除噪声句子、去除频率低于某一阈值的词语、去除重复图像等预处理决策，在论文中也不总是明确说明，这些可能也会影响结果。
- en: (5) These are overall results only, so a more fine-grained performance assessment
    on specific abnormalities or diseases is missing. This further shows the need
    for standardizing one or more evaluation metrics to measure the medical correctness
    of a generated report, considering different aspects of interest.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 这些仅是总体结果，因此缺乏对特定异常或疾病的更细致性能评估。这进一步显示了对一个或多个评估指标进行标准化的需求，以衡量生成报告的医学正确性，考虑到不同的兴趣方面。
- en: '| Paper | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | METEOR | CIDEr-D |
    Split |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | ROUGE-L | METEOR | CIDEr-D | 划分
    |'
- en: '| Findings section |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 发现部分 |'
- en: '| Liu et al. (Liu et al., [2019](#bib.bib93)) | 0.369 | 0.246 | 0.171 | 0.115
    | 0.359 | - | 1.490 | 7:1:2 ¹ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. (Liu et al., [2019](#bib.bib93)) | 0.369 | 0.246 | 0.171 | 0.115
    | 0.359 | - | 1.490 | 7:1:2 ¹ |'
- en: '| HRGR (Li et al., [2018](#bib.bib86)) | 0.438 | 0.298 | 0.208 | 0.151 | 0.369
    | - | 0.343 | 7:1:2 ² |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| HRGR (Li et al., [2018](#bib.bib86)) | 0.438 | 0.298 | 0.208 | 0.151 | 0.369
    | - | 0.343 | 7:1:2 ² |'
- en: '| KERP (Li et al., [2019b](#bib.bib87)) | 0.482 | 0.325 | 0.226 | 0.162 | 0.339
    | - | 0.280 | 7:1:2 ² |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| KERP (Li et al., [2019b](#bib.bib87)) | 0.482 | 0.325 | 0.226 | 0.162 | 0.339
    | - | 0.280 | 7:1:2 ² |'
- en: '| TieNet (Wang et al., [2018](#bib.bib145)) ${}^{\textrm{(1)}}$ | 0.330 | 0.194
    | 0.124 | 0.081 | 0.311 | - | 1.334 | 7:1:2 ¹ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| TieNet (Wang et al., [2018](#bib.bib145)) ${}^{\textrm{(1)}}$ | 0.330 | 0.194
    | 0.124 | 0.081 | 0.311 | - | 1.334 | 7:1:2 ¹ |'
- en: '| Xue et al. 2018 (Xue et al., [2018](#bib.bib155)) ${}^{\textrm{(2)}}$ | 0.441
    | 0.320 | 0.231 | 0.181 | 0.366 | 0.220 | 0.343 | 2,525/250 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| Xue et al. 2018 (Xue et al., [2018](#bib.bib155)) ${}^{\textrm{(2)}}$ | 0.441
    | 0.320 | 0.231 | 0.181 | 0.366 | 0.220 | 0.343 | 2,525/250 |'
- en: '| RTMIC (Xiong et al., [2019](#bib.bib152)) | 0.350 | 0.234 | 0.143 | 0.096
    | - | - | 0.323 | 7:2:1 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| RTMIC (Xiong et al., [2019](#bib.bib152)) | 0.350 | 0.234 | 0.143 | 0.096
    | - | - | 0.323 | 7:2:1 |'
- en: '| CLARA (Biswal et al., [2020](#bib.bib17)) ${}^{\textrm{(3)}}$ | 0.471 | 0.324
    | 0.214 | 0.199 | - | - | 0.359 | 7:1:2 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| CLARA (Biswal et al., [2020](#bib.bib17)) ${}^{\textrm{(3)}}$ | 0.471 | 0.324
    | 0.214 | 0.199 | - | - | 0.359 | 7:1:2 |'
- en: '| Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) | 0.477 | 0.332 | 0.243
    | 0.189 | 0.380 | 0.223 | 0.320 | 3,031/300 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) | 0.477 | 0.332 | 0.243
    | 0.189 | 0.380 | 0.223 | 0.320 | 3,031/300 |'
- en: '| CMAS (Jing et al., [2019](#bib.bib68)) | 0.464 | 0.301 | 0.210 | 0.154 |
    0.362 | - | 0.275 | Unk |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| CMAS (Jing et al., [2019](#bib.bib68)) | 0.464 | 0.301 | 0.210 | 0.154 |
    0.362 | - | 0.275 | 不明 |'
- en: '| Findings section - Cardiomegaly sentences only |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 发现部分 - 仅心脏肥大句子 |'
- en: '| Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) | 0.490 | 0.350
    | 0.250 | 0.180 | 0.400 | 0.270 | 0.600 | 8:1:1 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Spinks and Moens (Spinks and Moens, [2019](#bib.bib127)) | 0.490 | 0.350
    | 0.250 | 0.180 | 0.400 | 0.270 | 0.600 | 8:1:1 |'
- en: '| Impression section |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 印象部分 |'
- en: '| MTMA (Tian et al., [2019](#bib.bib133)) | 0.882 | 0.874 | 0.867 | 0.860 |
    0.929 | - | - | 5,461/500/500 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| MTMA (Tian et al., [2019](#bib.bib133)) | 0.882 | 0.874 | 0.867 | 0.860 |
    0.929 | - | - | 5,461/500/500 |'
- en: '| CMAS (Jing et al., [2019](#bib.bib68)) | 0.401 | 0.290 | 0.220 | 0.166 |
    0.521 | - | 1.457 | Unk |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| CMAS (Jing et al., [2019](#bib.bib68)) | 0.401 | 0.290 | 0.220 | 0.166 |
    0.521 | - | 1.457 | 不明 |'
- en: '| Findings + Impression sections |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 发现 + 印象部分 |'
- en: '| CoAtt (Jing et al., [2018](#bib.bib69)) | 0.517 | 0.386 | 0.306 | 0.247 |
    0.447 | 0.217 | 0.327 | 6,470/500/500 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| CoAtt (Jing et al., [2018](#bib.bib69)) | 0.517 | 0.386 | 0.306 | 0.247 |
    0.447 | 0.217 | 0.327 | 6,470/500/500 |'
- en: '| Huang et al. (Huang et al., [2019](#bib.bib62)) | 0.476 | 0.340 | 0.238 |
    0.169 | 0.347 | - | 0.297 | 8:1:1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| Huang et al. (Huang et al., [2019](#bib.bib62)) | 0.476 | 0.340 | 0.238 |
    0.169 | 0.347 | - | 0.297 | 8:1:1 |'
- en: '| Yuan et al. (Yuan et al., [2019](#bib.bib157)) | 0.529 | 0.372 | 0.315 |
    0.255 | 0.453 | 0.343 | - | 8:2 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Yuan et al. (Yuan et al., [2019](#bib.bib157)) | 0.529 | 0.372 | 0.315 |
    0.255 | 0.453 | 0.343 | - | 8:2 |'
- en: '| Xue et al. 2018 (Xue et al., [2018](#bib.bib155)) | 0.464 | 0.358 | 0.270
    | 0.195 | 0.366 | 0.274 | - | 2,775/250 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Xue et al. 2018 (Xue et al., [2018](#bib.bib155)) | 0.464 | 0.358 | 0.270
    | 0.195 | 0.366 | 0.274 | - | 2,775/250 |'
- en: '| Vispi (Li et al., [2019a](#bib.bib90)) | 0.419 | 0.280 | 0.201 | 0.150 |
    0.371 | - | 0.553 | 7:1:2 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Vispi (Li et al., [2019a](#bib.bib90)) | 0.419 | 0.280 | 0.201 | 0.150 |
    0.371 | - | 0.553 | 7:1:2 |'
- en: '| Singh et al. (Singh et al., [2019](#bib.bib124)) | 0.374 | 0.224 | 0.153
    | 0.110 | 0.308 | 0.164 | 0.360 | 6,718/350/350 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Singh et al. (Singh et al., [2019](#bib.bib124)) | 0.374 | 0.224 | 0.153
    | 0.110 | 0.308 | 0.164 | 0.360 | 6,718/350/350 |'
- en: '| Yin et al. (Yin et al., [2019](#bib.bib156)) | 0.445 | 0.292 | 0.201 | 0.154
    | 0.344 | 0.175 | 0.342 | 6,470/500/500 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Yin et al. (Yin et al., [2019](#bib.bib156)) | 0.445 | 0.292 | 0.201 | 0.154
    | 0.344 | 0.175 | 0.342 | 6,470/500/500 |'
- en: '| MLMA (Gajbhiye et al., [2020](#bib.bib37)) | 0.500 | 0.380 | 0.317 | 0.278
    | 0.440 | 0.281 | 1.067 | 6,429/500/500 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| MLMA (Gajbhiye et al., [2020](#bib.bib37)) | 0.500 | 0.380 | 0.317 | 0.278
    | 0.440 | 0.281 | 1.067 | 6,429/500/500 |'
- en: '| Harzig et al. 2019a (Harzig et al., [2019a](#bib.bib49)) | 0.373 | 0.246
    | 0.175 | 0.126 | 0.315 | 0.163 | 0.359 | 90:5:5 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Harzig et al. 2019a (Harzig et al., [2019a](#bib.bib49)) | 0.373 | 0.246
    | 0.175 | 0.126 | 0.315 | 0.163 | 0.359 | 90:5:5 |'
- en: '| A3FN (Xie et al., [2019](#bib.bib151)) | 0.443 | 0.337 | 0.236 | 0.181 |
    0.347 | - | 0.374 | 9:1 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| A3FN (Xie et al., [2019](#bib.bib151)) | 0.443 | 0.337 | 0.236 | 0.181 |
    0.347 | - | 0.374 | 9:1 |'
- en: '| Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) | 0.489 | 0.340 | 0.252
    | 0.195 | 0.478 | 0.230 | 0.565 | 3,031/300 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) | 0.489 | 0.340 | 0.252
    | 0.195 | 0.478 | 0.230 | 0.565 | 3,031/300 |'
- en: '| Zhang et al. (Zhang et al., [2020b](#bib.bib163)) | 0.441 | 0.291 | 0.203
    | 0.147 | 0.367 | - | 0.304 | 5-fold CV |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al. (Zhang et al., [2020b](#bib.bib163)) | 0.441 | 0.291 | 0.203
    | 0.147 | 0.367 | - | 0.304 | 5-fold CV |'
- en: 'Table 7\. Evaluation results of papers that use the IU X-ray dataset. All values
    were extracted from their papers, except in some cases where results were not
    present in the own paper: ${}^{\textrm{(1)}}$ TieNet (Wang et al., [2018](#bib.bib145))
    results were presented in Liu et al. (Liu et al., [2019](#bib.bib93)) as a baseline;
    ${}^{\textrm{(2)}}$ Xue et al. 2018 (Xue et al., [2018](#bib.bib155)) results
    in the findings section were presented in Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154))
    as a baseline. ${}^{\textrm{(3)}}$ CLARA (Biswal et al., [2020](#bib.bib17)) results
    are from the fully automatic version.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 使用 IU X 射线数据集的论文评估结果。所有值均摘自其论文，除非在某些情况下结果未在论文中提供：${}^{\textrm{(1)}}$ TieNet
    (Wang et al., [2018](#bib.bib145)) 的结果在 Liu et al. (Liu et al., [2019](#bib.bib93))
    中作为基准呈现；${}^{\textrm{(2)}}$ Xue et al. 2018 (Xue et al., [2018](#bib.bib155))
    的结果在发现部分中在 Xue et al. 2019 (Xue and Huang, [2019](#bib.bib154)) 中作为基准呈现。${}^{\textrm{(3)}}$
    CLARA (Biswal et al., [2020](#bib.bib17)) 的结果来自完全自动化版本。
- en: 6\. Challenges and future work
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 挑战与未来工作
- en: In this section, we identify unsolved challenges in the literature and potential
    avenues for future research in the task of report generation from medical images.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们识别了文献中的未解决挑战以及未来在医学图像报告生成任务中的潜在研究方向。
- en: Protocol for expert evaluation
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 专家评估协议
- en: If the ultimate goal is to develop a report generation system that meets high-quality
    standards, it makes sense that such a system be thoroughly tested by medical experts
    to evaluate its performance in different clinical settings. Most papers reviewed
    are weak in this regard, as only four of them (Gale et al., [2019](#bib.bib39);
    Tian et al., [2018](#bib.bib132); Alsharid et al., [2019](#bib.bib8); Spinks and
    Moens, [2019](#bib.bib127)) perform a correctness evaluation with medical experts,
    meaning that 90% of the works does not carry out an expert evaluation, feedback
    that should be immensely valuable to understand the strengths and weaknesses of
    a model. Therefore, a clear avenue for improvement is to standardize a protocol
    for human evaluation of these systems by imaging experts, for example starting
    with chest X-rays, which is the medical image type with more datasets available
    and research done. A standard protocol should facilitate fair comparisons between
    studies and allow to assess how close a model is to meet standard criteria for
    deployment in a clinical setting.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最终目标是开发一个符合高质量标准的报告生成系统，那么让医疗专家在不同临床环境下全面测试该系统是合理的。大多数审查的论文在这方面都较为薄弱，因为只有四篇（Gale
    et al., [2019](#bib.bib39)；Tian et al., [2018](#bib.bib132)；Alsharid et al., [2019](#bib.bib8)；Spinks
    and Moens, [2019](#bib.bib127)）对医学专家进行正确性评估，这意味着90%的研究没有进行专家评估，这种反馈对于理解模型的优缺点至关重要。因此，一个明确的改进方向是标准化影像专家对这些系统的人类评估协议，例如从胸部X光片开始，这是医学图像类型中数据集和研究最多的。标准化协议应有助于在研究之间进行公平比较，并评估一个模型是否接近满足临床环境中部署的标准要求。
- en: The expertise of the human evaluators is an important factor to consider as
    well. It stands to reason that the judgement of a board-certified radiologist
    with years of experience should be more reliable than the judgement of a physician
    with limited experience. Similarly, the consensus of a team of radiologists should
    be preferred over a single radiologist. In the same line, measuring the inter-agreement
    of several radiologists can help to better assess the difficulty of the task itself
    (Jain et al., [2021](#bib.bib66)). If radiologists tend to disagree more, this
    may indicate an inherent ambiguity in the task that could be the explanation for
    the possible underperformance of a given model.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 人类评估者的专业性也是一个重要的因素。显而易见，拥有多年经验的认证放射科医生的判断应该比经验有限的医生的判断更可靠。同样，放射科医生团队的共识应该优于单个放射科医生的判断。同样，测量多个放射科医生之间的一致性可以帮助更好地评估任务本身的难度（Jain
    et al., [2021](#bib.bib66)）。如果放射科医生的意见分歧较多，这可能表明任务中存在固有的模糊性，这可能是某个模型表现不佳的原因。
- en: Automatic metrics for medical correctness
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医学正确性的自动化指标
- en: Having a proper expert evaluation is desirable. However, it is not feasible
    to ask radiologists to manually evaluate hundreds of machine-generated reports
    every time a small tweak in a model is performed. Instead, one would like to have
    one or more automatic metrics positively correlated with expert human evaluation,
    in order to speed up the model design and testing cycle. We found that more than
    70% of the works reviewed (29 out of 40) limit the automatic report evaluation
    to traditional NLP metrics such as BLEU, ROUGE-L or CIDEr, which are not designed
    to evaluate a report from a medical correctness point of view (Boag et al., [2020](#bib.bib18);
    Zhang et al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93); Babar et al.,
    [2021](#bib.bib12); Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)). Furthermore,
    these evaluation methods have been recently contested in other NLP tasks (van
    Miltenburg et al., [2020](#bib.bib139); van Miltenburg et al., [2021](#bib.bib138);
    Reiter, [2018](#bib.bib112); Mathur et al., [2020](#bib.bib97)). Some works tried
    to remedy this limitation by devising their own auxiliary metrics to evaluate
    medical correctness to some degree (Liu et al., [2019](#bib.bib93); Huang et al.,
    [2019](#bib.bib62); Li et al., [2018](#bib.bib86); Xue et al., [2018](#bib.bib155);
    Alsharid et al., [2019](#bib.bib8); Biswal et al., [2020](#bib.bib17); Xie et al.,
    [2019](#bib.bib151); Zhang et al., [2020b](#bib.bib163); Jing et al., [2019](#bib.bib68);
    Moradi et al., [2016](#bib.bib99); Wu et al., [2017](#bib.bib149)), which are
    interesting approaches. We highlight the metric MIRQI proposed very recently by
    Zhang et al. (Zhang et al., [2020b](#bib.bib163)), which is very similar to SPICE
    (Anderson et al., [2016](#bib.bib10)) as described in section [5.4.2](#S5.SS4.SSS2
    "5.4.2\. Medical correctness metrics ‣ 5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"), as it attempts to build a graph capturing
    abnormalities, their relations and attributes. We believe this is the most sophisticated
    metric for medical correctness found in the literature, and great ideas can be
    adopted from it.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 进行适当的专家评估是理想的。然而，每次模型进行小调整时，要求放射科医生手动评估数百份机器生成的报告是不切实际的。相反，希望能够有一个或多个与专家人工评估正相关的自动度量标准，以加快模型设计和测试周期。我们发现，超过70%的被审查工作（40项中的29项）将自动报告评估限制在传统的NLP度量标准，如BLEU、ROUGE-L或CIDEr，这些标准并未设计用于从医学正确性的角度评估报告（Boag
    et al., [2020](#bib.bib18); Zhang et al., [2020a](#bib.bib162); Liu et al., [2019](#bib.bib93);
    Babar et al., [2021](#bib.bib12); Pino et al., [2021](#bib.bib108), [2020](#bib.bib109)）。此外，这些评估方法最近在其他NLP任务中受到了质疑（van
    Miltenburg et al., [2020](#bib.bib139); van Miltenburg et al., [2021](#bib.bib138);
    Reiter, [2018](#bib.bib112); Mathur et al., [2020](#bib.bib97)）。一些工作尝试通过制定自己的辅助度量标准来在一定程度上评估医学正确性（Liu
    et al., [2019](#bib.bib93); Huang et al., [2019](#bib.bib62); Li et al., [2018](#bib.bib86);
    Xue et al., [2018](#bib.bib155); Alsharid et al., [2019](#bib.bib8); Biswal et
    al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Zhang et al., [2020b](#bib.bib163);
    Jing et al., [2019](#bib.bib68); Moradi et al., [2016](#bib.bib99); Wu et al.,
    [2017](#bib.bib149)），这些都是有趣的方法。我们特别提到Zhang et al. 最近提出的度量标准MIRQI（Zhang et al.,
    [2020b](#bib.bib163)），它与SPICE（Anderson et al., [2016](#bib.bib10)）非常相似，如第[5.4.2节](#S5.SS4.SSS2
    "5.4.2\. 医学正确性度量 ‣ 5.4\. 评估度量 ‣ 5\. 被审查论文分析 ‣ 自动报告生成的深度学习和可解释性调查")所述，因为它尝试建立一个捕捉异常、它们的关系和属性的图。我们认为这是文献中发现的最复杂的医学正确性度量标准，可以从中借鉴一些优秀的想法。
- en: Unfortunately, all the proposed metrics lack validation by medical experts,
    as none of the papers presents the results of a study assessing the correlation
    between the proposed metric and expert medical judgment. Thus, finding one or
    more golden automatic metrics for medical correctness remains an open problem.
    To solve it, the precision and accuracy of a report is critical in the medical
    domain and need to be captured (Zhang et al., [2020a](#bib.bib162); Babar et al.,
    [2021](#bib.bib12); Pino et al., [2021](#bib.bib108)), whereas other aspects such
    as natural language fluency should probably weigh less in importance. We believe
    designing and validating such metrics is a clear avenue for future research, with
    the potential to have a significant impact on the field.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，所有提出的指标都缺乏医学专家的验证，因为没有一篇论文展示评估所提指标与专家医学判断之间相关性的研究结果。因此，寻找一个或多个黄金自动化指标以确保医学正确性仍然是一个未解的问题。为了解决这个问题，报告的精确性和准确性在医学领域至关重要，需要加以捕捉（Zhang
    等，[2020a](#bib.bib162)；Babar 等，[2021](#bib.bib12)；Pino 等，[2021](#bib.bib108)），而其他方面如自然语言流畅性可能应该权重较低。我们相信，设计和验证这些指标是未来研究的明确方向，有潜力对该领域产生重大影响。
- en: Improve explainability
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 改善可解释性
- en: To build trust in an AI system, a desirable feature is the ability to provide
    clear and coherent explanations for its decisions (Adadi and Berrada, [2018](#bib.bib3);
    Došilović et al., [2018](#bib.bib35)). This is particularly relevant in the healthcare
    domain, where decisions have to be made with extreme caution since the patient’s
    health is at stake. Thus, high levels of transparency, interpretability, and accountability
    are required to justify the outputs delivered, align to the expert’s expectations,
    and acquire their trust (Tonekaboni et al., [2019](#bib.bib135); Tjoa and Guan,
    [2019](#bib.bib134); Reyes et al., [2020](#bib.bib114)).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立对人工智能系统的信任，一个理想的特性是能够提供清晰且连贯的决策解释（Adadi 和 Berrada，[2018](#bib.bib3)；Došilović
    等，[2018](#bib.bib35)）。这在医疗保健领域尤其重要，因为决策必须极其谨慎，因为患者的健康处于风险中。因此，需要高度的透明性、可解释性和问责性来证明所提供的输出，符合专家的期望，并获得他们的信任（Tonekaboni
    等，[2019](#bib.bib135)；Tjoa 和 Guan，[2019](#bib.bib134)；Reyes 等，[2020](#bib.bib114)）。
- en: Only two papers reviewed (Gale et al., [2019](#bib.bib39); Spinks and Moens,
    [2019](#bib.bib127)) have explainability as a primary focus, as discussed in section
    [5.3](#S5.SS3 "5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey
    on Deep Learning and Explainability for Automatic Report Generation from Medical
    Images"), though one of them (Gale et al., [2019](#bib.bib39)) does not provide
    an explanation for the report. Additionally, some works mention some form of local
    explainability in their models, but always as a secondary output and giving it
    a rather superficial treatment, with no rigorous evaluation. In the absence of
    empirical results across all papers, we cannot draw conclusions about which explanation
    techniques are better or worse. Thus, a potential avenue for future research is
    explainability with a more rigorous and empirical focus, and possibly including
    other approaches, such as global explanations, uncertainty, or more, which may
    be necessary for clinicians (Tonekaboni et al., [2019](#bib.bib135)). We believe
    this research avenue will benefit from the feedback and evaluation of medical
    imaging experts, who are the end-users of these systems. What would be a suitable
    explanation for a radiologist? In a multi-sentence report, how should the explanation
    be structured? An expert’s opinion is valuable for answering these and other questions,
    and ultimately for assessing the explanation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 只有两篇论文（Gale 等，[2019](#bib.bib39)；Spinks 和 Moens，[2019](#bib.bib127)）将可解释性作为主要关注点，如[5.3](#S5.SS3
    "5.3\. Explainability ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning
    and Explainability for Automatic Report Generation from Medical Images")部分所讨论，尽管其中一篇（Gale
    等，[2019](#bib.bib39)）未提供报告的解释。此外，一些研究提到了模型中的某种形式的局部可解释性，但始终作为次要输出，并且处理得较为肤浅，没有严格评估。在所有论文中缺乏实证结果的情况下，我们无法得出关于哪些解释技术更好或更差的结论。因此，未来研究的一个潜在方向是更具严格性和实证性的可解释性研究，并可能包括其他方法，如全局解释、不确定性等，这些对于临床医生可能是必要的（Tonekaboni
    等，[2019](#bib.bib135)）。我们相信，这一研究方向将受益于医学影像专家的反馈和评估，他们是这些系统的最终用户。对放射科医生来说，什么样的解释才是合适的？在一个多句报告中，解释应该如何结构化？专家的意见对于回答这些问题以及其他问题是宝贵的，最终也有助于评估解释。
- en: New learning strategies and architectures.
  id: totrans-340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 新的学习策略和架构。
- en: If the ultimate goal is to have a model that learns to generate accurate and
    useful medical reports, the optimization strategy employed should be designed
    to guide the model in this direction. As we saw in section [5.2.6](#S5.SS2.SSS6
    "5.2.6\. Optimization Strategies ‣ 5.2\. Model Design ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images"), most papers used teacher-forcing, a training strategy which
    is domain-agnostic and thus suboptimal for the medical domain (Zhang et al., [2020a](#bib.bib162)).
    Similarly, a few papers used reinforcement learning (Liu et al., [2019](#bib.bib93);
    Li et al., [2018](#bib.bib86); Xiong et al., [2019](#bib.bib152); Li and Hong,
    [2019](#bib.bib88); Jing et al., [2019](#bib.bib68)) with traditional NLP metrics
    as rewards, which are not designed for medicine either. Only Liu et al. (Liu et al.,
    [2019](#bib.bib93)) included a domain-specific reward that explicitly promotes
    medical correctness. Unfortunately, a manual inspection of several generated reports
    conducted by the authors revealed that the model was missing positive findings
    (low recall) as well as failing to provide accurate descriptions of the positive
    findings detected.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最终目标是拥有一个能够生成准确且有用的医疗报告的模型，那么所采用的优化策略应设计为引导模型朝这个方向发展。正如我们在第[5.2.6](#S5.SS2.SSS6
    "5.2.6\. Optimization Strategies ‣ 5.2\. Model Design ‣ 5\. Analysis of papers
    reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images")节中所见，大多数论文使用了teacher-forcing，这是一种领域无关的训练策略，因此对医疗领域并不适用（Zhang
    et al., [2020a](#bib.bib162)）。类似地，一些论文使用了强化学习（Liu et al., [2019](#bib.bib93);
    Li et al., [2018](#bib.bib86); Xiong et al., [2019](#bib.bib152); Li and Hong,
    [2019](#bib.bib88); Jing et al., [2019](#bib.bib68)）并用传统NLP指标作为奖励，这些指标也并非为医学设计。仅有刘等人（Liu
    et al., [2019](#bib.bib93)）包含了一个明确促进医学正确性的领域特定奖励。不幸的是，作者对若干生成报告的人工检查发现，模型缺少积极发现（召回率低）以及未能准确描述检测到的积极发现。
- en: Given these reasons, there is still room for finding better optimization strategies
    for image-based medical report generation. In this regard, reinforcement learning
    appears to be the most promising training paradigm to explore, as illustrated
    by the work of Zhang et al. (Zhang et al., [2020a](#bib.bib162)) on factual correctness
    optimization in a related medical task. If a robust medical correctness metric
    is developed (as previously discussed in this section), then the metric could
    be used as a reward in a reinforcement learning setting to teach the model to
    generate reports that are medically correct.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些原因，图像基础的医疗报告生成仍然有改进优化策略的空间。在这方面，强化学习似乎是最有前景的训练范式，如张等人（Zhang et al., [2020a](#bib.bib162)）在相关医疗任务中对事实正确性的优化工作所示。如果开发出一个稳健的医疗正确性指标（如本节之前讨论的），则该指标可以在强化学习设置中用作奖励，来教导模型生成医学上正确的报告。
- en: Other image modalities and body regions are less explored
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他图像模式和身体区域的探索较少。
- en: Most research has concentrated on chest X-rays, as 24 out of 40 papers focus
    their study on this image type. This modality presents a very specific nature
    and different characteristics from other imaging studies. For example, when a
    radiologist reads a chest X-ray, the focus is on the underlying anatomy and identification
    of possible areas of distortion based on different densities of the image. On
    the other hand, when analyzing a PET image, the focus is on detecting areas of
    increased radiotracer activity; for MRI scans, the radiologist may review several
    images obtained with different configurations at the same time; and for each other
    modality there may be more specific conditions. Hence, the results shown here
    are highly biased towards chest X-rays, which will not necessarily extrapolate
    to other scenarios.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究集中在胸部X光片上，因为40篇论文中有24篇专注于这种图像类型。这种模式具有非常特定的性质和不同于其他成像研究的特征。例如，当放射科医生阅读胸部X光片时，重点是基础解剖结构和根据图像的不同密度识别可能的扭曲区域。另一方面，当分析PET图像时，重点是检测放射性示踪剂活性增加的区域；对于MRI扫描，放射科医生可能会同时查看几张具有不同配置的图像；对于每种其他模式，可能还有更具体的条件。因此，这里展示的结果高度偏向于胸部X光片，可能不会直接推广到其他场景。
- en: Notice there are datasets with multiple image types or body parts, namely ImageCLEF
    caption (Eickhoff et al., [2017](#bib.bib36); García Seco de Herrera et al., [2018](#bib.bib40)),
    ROCO (Pelka et al., [2018](#bib.bib106)) and PEIR Gross (Jing et al., [2018](#bib.bib69)),
    as it was mentioned in section [5.1](#S5.SS1 "5.1\. Datasets ‣ 5\. Analysis of
    papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic Report
    Generation from Medical Images"). However, we believe their broad nature, i.e.,
    the inclusion of many types and regions simultaneously, may be a drawback when
    trying to apply an advanced deep learning approach, for three main reasons. First,
    it is more difficult to include specific domain knowledge in the models, as the
    knowledge should cover all modalities and body parts. Second, assessing medical
    correctness is more complicated, since domain knowledge is needed to design these
    metrics, as noted in section [5.4](#S5.SS4 "5.4\. Evaluation Metrics ‣ 5\. Analysis
    of papers reviewed ‣ A Survey on Deep Learning and Explainability for Automatic
    Report Generation from Medical Images"). Third, it would be more challenging to
    provide interpretability for the model, as the explanations should cover all modalities.
    Ultimately, we believe better solutions can be achieved by designing them for
    a specific problem and setting. In conclusion, there is a clear opportunity to
    extend research into other image types and body regions by raising new collections
    with other image types, evaluating the same methods in different modalities, or
    further covering the existing datasets.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到有一些数据集包含多种图像类型或身体部位，即 ImageCLEF caption（Eickhoff 等，[2017](#bib.bib36)；García
    Seco de Herrera 等，[2018](#bib.bib40)），ROCO（Pelka 等，[2018](#bib.bib106)）和 PEIR
    Gross（Jing 等，[2018](#bib.bib69)），正如在第 [5.1](#S5.SS1 "5.1\. 数据集 ‣ 5\. 文献分析 ‣ 关于医学图像自动报告生成的深度学习与可解释性调查")
    节中提到的。然而，我们认为它们的广泛性质，即同时包含多种类型和区域，可能在应用先进深度学习方法时存在缺陷，主要有三个原因。首先，模型中包含特定领域知识更困难，因为知识需要涵盖所有模态和身体部位。其次，评估医学准确性更复杂，因为设计这些度量需要领域知识，如第
    [5.4](#S5.SS4 "5.4\. 评估指标 ‣ 5\. 文献分析 ‣ 关于医学图像自动报告生成的深度学习与可解释性调查") 节中所述。第三，为模型提供可解释性将更具挑战性，因为解释需要涵盖所有模态。*最终*，我们认为通过为特定问题和设置设计解决方案可以取得更好的结果。总之，通过提出包含其他图像类型的新集合，在不同模态中评估相同的方法，或进一步覆盖现有数据集，明显有机会将研究扩展到其他图像类型和身体区域。
- en: Explore more alternatives to include domain knowledge
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 探索更多包含领域知识的替代方案
- en: As we saw in section [5.2.4](#S5.SS2.SSS4 "5.2.4\. Domain knowledge ‣ 5.2\.
    Model Design ‣ 5\. Analysis of papers reviewed ‣ A Survey on Deep Learning and
    Explainability for Automatic Report Generation from Medical Images"), the approaches
    explored in the literature for incorporating domain knowledge into models are
    (1) the use of graph neural networks at the visual component level and (2) the
    use template databases curated with expert knowledge—in addition to the widespread
    use of auxiliary tasks, that can be viewed as a way of domain knowledge transfer
    as well. However, other approaches remain unexplored. A recent survey by Xie et
    al. (Xie et al., [2020](#bib.bib150)) synthesizing over 270 papers on domain knowledge
    for deep learning-based medical image analysis presents interesting ideas that
    could be applicable to the report generation setting. For example, curriculum
    learning (Bengio et al., [2009](#bib.bib16)) and self-paced learning (Kumar et al.,
    [2010](#bib.bib81)) could be used to imitate the learning curve from easier to
    harder instances that radiologists go through when they learn to interpret and
    diagnose images. Also, the use of handcrafted algorithms to extract visual features
    that better capture what radiologists focus on in an image could be used, which
    many works have verified to have synergistic effects in combination with the features
    learned by the CNN (Xie et al., [2020](#bib.bib150)). This would improve the quality
    of the visual component and potentially translate into better reports. Studying
    how imaging experts analyze an image, how they focus the attention to different
    regions of the image as needed, could be useful to inspire innovations in model
    architectures in order to emulate that process.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[5.2.4节](#S5.SS2.SSS4 "5.2.4\. 领域知识 ‣ 5.2\. 模型设计 ‣ 5\. 文献综述分析 ‣ 关于深度学习和可解释性在医学图像自动报告生成中的调查")中看到的，文献中探讨的将领域知识融入模型的方法包括（1）在视觉组件层面使用图神经网络，以及（2）使用专家知识策划的模板数据库——除了广泛使用的辅助任务，这也可以被视为领域知识转移的一种方式。然而，其他方法尚未被探索。Xie等人（Xie
    et al., [2020](#bib.bib150)）的最新调查综合了270多篇关于深度学习基础医学图像分析的领域知识论文，提出了一些有趣的观点，这些观点可能适用于报告生成场景。例如，课程学习（Bengio
    et al., [2009](#bib.bib16)）和自定节奏学习（Kumar et al., [2010](#bib.bib81)）可以用来模仿放射科医生在学习解释和诊断图像时从简单到困难的学习曲线。此外，使用手工设计的算法提取更好地捕捉放射科医生在图像中关注的视觉特征的算法也可以使用，这在许多研究中已被验证与CNN学习到的特征结合具有协同效应（Xie
    et al., [2020](#bib.bib150)）。这将提高视觉组件的质量，并有可能转化为更好的报告。研究成像专家如何分析图像、如何根据需要将注意力集中到图像的不同区域，可能有助于激发模型架构的创新，以模拟这一过程。
- en: Medical Human-AI interaction
  id: totrans-348
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 医学人机交互
- en: Most reviewed works leave aside important aspects pertaining the model’s integration
    in a real clinical setting and its interaction with clinicians as an AI assistant.
    Besides high levels of accuracy, there are other needs a system should aim to
    meet in a medical human-AI collaboration workflow. For example, Cai et al. (Cai
    et al., [2019](#bib.bib21)) argue that clinicians should have transparent information
    about the model’s overall strengths and weaknesses, its subjective point-of-view,
    its overall design objective and how exactly it uses the information to derive
    a final diagnosis. Also, Amershi et al. (Amershi et al., [2019](#bib.bib9)) proposed
    and validated several design guidelines for general human-AI interaction that
    can be relevant in the context of automatic report generation, such as Make clear
    why the system did what it did via explanations, and Support efficient correction
    by making it easy to edit, refine, or recover when the AI system is wrong. Among
    all papers reviewed, only CLARA (Biswal et al., [2020](#bib.bib17)) targets an
    explicit workflow with human interaction, in which a report is generated cooperatively
    by a human who types some preliminary text and the system autocompletes the rest.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数被审阅的工作忽视了模型在真实临床环境中的整合以及作为AI助手与临床医生的互动等重要方面。除了高准确性，系统还应满足医疗人机协作工作流中的其他需求。例如，Cai
    等（Cai et al., [2019](#bib.bib21)）认为，临床医生应对模型的总体优势和劣势、其主观观点、整体设计目标以及它如何使用信息来得出最终诊断有透明的信息。此外，Amershi
    等（Amershi et al., [2019](#bib.bib9)）提出并验证了几个通用的人机交互设计指南，这些指南在自动报告生成的背景下可能具有相关性，例如通过解释清楚系统为何如此操作，并通过简化编辑、修正或恢复的方式来支持高效纠错。在所有被审阅的论文中，只有
    CLARA（Biswal et al., [2020](#bib.bib17)）针对一个明确的与人互动的工作流，其中由人类输入一些初步文本并由系统自动完成其余部分。
- en: 'Also, there are potential use cases that an AI assistant for report generation
    can face in routine practice which are not addressed in the reviewed literature.
    For example, (1) open-ended visual question answering (VQA): instead of a full
    report with too many details, a clinician might be interested in the model’s opinion
    on a specific aspect of the image(s). This query could be expressed as a natural
    language question that the model would have to answer, which would require a model
    with open-ended VQA capabilities. Although this is a different task than report
    generation, we believe the latter could be approached as giving answers to a sequence
    of questions from physicians, allowing a richer interaction between the expert
    and the system. The multiple ImageCLEF challenges involving a medical VQA task
    (Hasan et al., [2018a](#bib.bib51); Ben Abacha et al., [2019](#bib.bib15), [2020](#bib.bib14))
    and the recently published PathVQA dataset (He et al., [2020](#bib.bib54)) could
    be helpful in exploring this direction. (2) Reporting temporal information: sometimes
    clinicians are interested in the evolution of a health condition by analyzing
    a sequence of imaging snapshots over time, rather than describing a single image.
    None of the surveyed papers considers this use case. (3) Quantitative Radiology:
    in some cases a clinician might be interested in specific numerical measurements
    to further assess the patient’s condition, for example, the degree of a certain
    property in the tissues (Jackson, [2018](#bib.bib65)). This adds more complexity
    to the problem, since models would need the ability to make these accurate numerical
    measurements, in addition to interpreting them through words in the generated
    report. In sum, there may be different ways to fulfill the report generation task,
    and we believe researchers should aim to find the most useful approaches for clinicians
    in each specific environment.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，报告生成的AI助手在日常实践中可能面临的潜在用例在回顾文献中没有被提及。例如，（1）开放式视觉问答（VQA）：临床医生可能对模型对图像某个特定方面的看法感兴趣，而不是详细的完整报告。这个查询可以用自然语言提问，模型需要回答，这需要具备开放式VQA能力的模型。虽然这与报告生成是不同的任务，但我们相信，后者可以通过回答来自医生的一系列问题来进行，从而实现专家与系统之间的更丰富互动。涉及医学VQA任务的多个ImageCLEF挑战（Hasan等，[2018a](#bib.bib51)；Ben
    Abacha等，[2019](#bib.bib15)，[2020](#bib.bib14)）以及最近发布的PathVQA数据集（He等，[2020](#bib.bib54)）可能对探索这一方向有所帮助。（2）报告时间信息：有时临床医生对健康状况的演变感兴趣，通过分析一系列随时间变化的成像快照，而不是描述单张图像。调查的论文没有考虑这一用例。（3）定量放射学：在某些情况下，临床医生可能对特定的数值测量感兴趣，以进一步评估患者的状况，例如，组织中某一特性程度（Jackson，[2018](#bib.bib65)）。这增加了问题的复杂性，因为模型不仅需要能够进行准确的数值测量，还需要通过生成的报告中的文字来解释这些测量。总之，可能有不同的方法来完成报告生成任务，我们相信研究人员应致力于寻找在每个特定环境中对临床医生最有用的方法。
- en: 7\. Limitations
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 局限性
- en: The main limitations of this survey are two. First, new papers on report generation
    from medical images are published relatively often, we tried to be as comprehensive
    as possible and include all of them, but we do not rule out that some papers may
    have been missed. Second, we left out of the analysis works from related tasks,
    such as disease classification, report summarizing, or medical image segmentation.
    These topics may have interesting approaches or insights on how to improve the
    visual features generated, how to optimize the text generation, evaluation techniques,
    and more.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这项调查的主要局限性有两个。首先，关于医学图像报告生成的新论文相对频繁地发表，我们尽力做到尽可能全面地涵盖所有相关论文，但我们不能排除有些论文可能被遗漏。其次，我们在分析中排除了相关任务的工作，如疾病分类、报告总结或医学图像分割。这些主题可能对如何改进生成的视觉特征、如何优化文本生成、评估技术等方面提供有趣的方法或见解。
- en: 8\. Conclusions
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: In this work, we have reviewed the state of research in deep-learning based
    methods for automatic report generation from medical images, in terms of different
    key aspects. First, we described the report and classification datasets available
    and commonly used in the literature, totalling 27 collections, which cover different
    image modalities and body parts, and include useful tags and localization information.
    Second, we presented an analysis of model designs in terms of standard practices,
    inputs and outputs, visual components, language components, domain knowledge,
    auxiliary tasks, and optimization strategies. We cannot recommend an optimal model
    design due to the lack of proper evaluations, but several guidelines can be inferred.
    For instance, a robust visual component should make use of CNNs and would certainly
    benefit from training in auxiliary medical image tasks. Also, complementing the
    visual input with semantic information via tags or input text (e.g the report’s
    indication section) or access to a template database generally improves the language
    component’s performance. Multitask learning to integrate the supervision of multiple
    tasks and reinforcement learning to directly optimize for factual correctness
    or other metrics of interest in generated reports appear as the most promising
    optimization approaches. Third, we analyzed the interpretablity approaches employed
    in the literature, and found that many models provide a secondary output that
    can be used as a local explanation, either by providing a feature importance map,
    a counter-factual example, or by increasing the system’s transparency. However,
    only two works focused explicitly on studying this concern, by discussing extensively
    and providing formal evaluations. Additionally, many other approaches can be explored,
    and hence this remains a heavily understudied aspect of this task. Fourth, we
    discussed usual practices regarding evaluation metrics, and we found that most
    models are only evaluated with traditional n-gram based NLP metrics not designed
    for medicine, which are not able to capture the essential medical facts in a written
    report. Next, we presented a comparison of papers’ performance results on IU X-Ray,
    the most frequently used dataset, but limited to said NLP metrics that papers
    report, making us unable to judge models from a medical perspective.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们审查了基于深度学习的自动报告生成方法在医学图像领域的研究现状，从不同的关键方面进行了探讨。首先，我们描述了文献中可用且常用的报告和分类数据集，共计27个数据集，这些数据集涵盖了不同的图像模态和身体部位，并包含有用的标签和定位信息。其次，我们分析了模型设计的标准实践、输入和输出、视觉组件、语言组件、领域知识、辅助任务和优化策略。由于缺乏适当的评估，我们不能推荐最优的模型设计，但可以推断出一些指导方针。例如，一个强大的视觉组件应该利用卷积神经网络（CNN），并且通过在辅助医学图像任务中的训练将会受益匪浅。此外，通过标签或输入文本（例如报告的指示部分）或访问模板数据库来补充视觉输入通常会提高语言组件的性能。多任务学习用于整合多个任务的监督，而强化学习直接优化生成报告的事实正确性或其他感兴趣的指标，似乎是最有前景的优化方法。第三，我们分析了文献中采用的可解释性方法，发现许多模型提供了可以作为局部解释的二次输出，方式包括提供特征重要性图、对比示例，或提高系统透明度。然而，只有两篇工作明确专注于研究这一问题，通过广泛讨论和提供正式评估。除此之外，许多其他方法还可以进行探索，因此这是一个仍然严重缺乏研究的方面。第四，我们讨论了关于评估指标的常见实践，发现大多数模型仅使用传统的基于n-gram的自然语言处理（NLP）指标进行评估，这些指标并非为医学设计，无法捕捉书面报告中的核心医学事实。接下来，我们对IU
    X-Ray（最常用的数据集）上的论文性能结果进行了比较，但仅限于论文报告的NLP指标，这使得我们无法从医学角度判断模型。
- en: Lastly, we identified challenges in the field that none of the reviewed papers
    has successfully addressed, and we proposed avenues for future research where
    we believe possible solutions could be found. The main challenges lay on improving
    the evaluation methods employed, by developing a standard protocol for expert
    evaluation and automatic metrics for medical correctness. Other important aspects
    are improving the explainability of models, and considering the medical human-AI
    interaction. We intend this survey to serve as an entry point for researchers
    who want an overview of the current advances in the field and also to raise awareness
    of critical problems that future research should focus on, with the end goal of
    developing mature and robust technologies that can bring value to healthcare professionals
    and patients in real clinical settings.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们识别了该领域中未被回顾文献成功解决的挑战，并提出了我们认为可能找到解决方案的未来研究方向。主要挑战在于改进评估方法，通过制定专家评估的标准协议和医学正确性的自动度量标准来实现。其他重要方面包括改进模型的可解释性，并考虑医学领域的人机交互。我们希望这项调查能够作为研究人员了解该领域当前进展的入口点，并提高对未来研究应关注的关键问题的认识，最终目标是开发成熟和可靠的技术，能够在实际临床环境中为医疗专业人员和患者带来价值。
- en: References
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Abràmoff et al. (2013) Michael D Abràmoff, James C Folk, Dennis P Han, Jonathan D
    Walker, David F Williams, Stephen R Russell, Pascale Massin, Beatrice Cochener,
    Philippe Gain, Li Tang, et al. 2013. Automated analysis of retinal images for
    detection of referable diabetic retinopathy. *JAMA ophthalmology* 131, 3 (2013),
    351–357.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abràmoff 等（2013）Michael D Abràmoff、James C Folk、Dennis P Han、Jonathan D Walker、David
    F Williams、Stephen R Russell、Pascale Massin、Beatrice Cochener、Philippe Gain、Li
    Tang 等。2013。用于检测可参考糖尿病视网膜病变的视网膜图像自动分析。*JAMA 眼科学* 131, 3（2013），351–357。
- en: 'Adadi and Berrada (2018) A. Adadi and M. Berrada. 2018. Peeking Inside the
    Black-Box: A Survey on Explainable Artificial Intelligence (XAI). *IEEE Access*
    6 (2018), 52138–52160.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adadi 和 Berrada（2018）A. Adadi 和 M. Berrada。2018。窥视黑箱：关于可解释人工智能（XAI）的调查。*IEEE
    Access* 6（2018），52138–52160。
- en: Adebayo et al. (2018) Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow,
    Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. In *Advances
    in Neural Information Processing Systems 31*. Curran Associates, Inc., 9505–9515.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adebayo 等（2018）Julius Adebayo、Justin Gilmer、Michael Muelly、Ian Goodfellow、Moritz
    Hardt 和 Been Kim。2018。显著性图的合理性检查。在 *神经信息处理系统进展 31*。Curran Associates, Inc.，9505–9515。
- en: Ahmad et al. (2018) Muhammad Aurangzeb Ahmad, Carly Eckert, and Ankur Teredesai.
    2018. Interpretable Machine Learning in Healthcare. In *Proc of the 2018 ACM Intl.
    Conf. on Bioinformatics, Computational Biology, and Health Informatics* (Washington,
    DC, USA) *(BCB ’18)*. ACM, New York, NY, USA, 559–560.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad 等（2018）Muhammad Aurangzeb Ahmad、Carly Eckert 和 Ankur Teredesai。2018。医疗保健中的可解释机器学习。在
    *2018 年 ACM 国际生物信息学、计算生物学与健康信息学大会*（美国华盛顿特区） *(BCB ’18)*。ACM，纽约，美国，559–560。
- en: 'Akazawa et al. (2019) Kentaro Akazawa, Ryo Sakamoto, Satoshi Nakajima, Dan
    Wu, Yue Li, Kenichi Oishi, Andreia V. Faria, Kei Yamada, Kaori Togashi, Constantine G.
    Lyketsos, Michael I. Miller, and Susumu Mori. 2019. Automated Generation of Radiologic
    Descriptions on Brain Volume Changes From T1-Weighted MR Images: Initial Assessment
    of Feasibility. *Frontiers in Neurology* 10 (2019), 7.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akazawa 等（2019）Kentaro Akazawa、Ryo Sakamoto、Satoshi Nakajima、Dan Wu、Yue Li、Kenichi
    Oishi、Andreia V. Faria、Kei Yamada、Kaori Togashi、Constantine G. Lyketsos、Michael
    I. Miller 和 Susumu Mori。2019。基于 T1 加权 MR 图像的脑体积变化放射描述自动生成：初步可行性评估。*神经学前沿* 10（2019），7。
- en: Allaouzi et al. (2018) Imane Allaouzi, M. Ben Ahmed, B. Benamrou, and M. Ouardouz.
    2018. Automatic Caption Generation for Medical Images. In *Proc of the 3rd Intl.
    Conf. on Smart City Applications* (Tetouan, Morocco) *(SCA ’18)*. ACM, New York,
    NY, USA, Article 86, 6 pages.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allaouzi 等（2018）Imane Allaouzi、M. Ben Ahmed、B. Benamrou 和 M. Ouardouz。2018。医学图像的自动标注生成。在
    *第3届国际智慧城市应用大会*（摩洛哥特图安） *(SCA ’18)*。ACM，纽约，美国，第86篇文章，6页。
- en: Alsharid et al. (2019) Mohammad Alsharid, Harshita Sharma, Lior Drukker, Pierre
    Chatelain, Aris T. Papageorghiou, and J. Alison Noble. 2019. Captioning Ultrasound
    Images Automatically. In *Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2019*. Springer Intl. Publishing, Cham, 338–346.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alsharid 等（2019）Mohammad Alsharid、Harshita Sharma、Lior Drukker、Pierre Chatelain、Aris
    T. Papageorghiou 和 J. Alison Noble。2019。自动标注超声图像。在 *医学图像计算与计算机辅助干预 – MICCAI 2019*。Springer
    国际出版公司，Cham，338–346。
- en: Amershi et al. (2019) Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney,
    Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori
    Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human-AI
    Interaction. In *Proc of the 2019 CHI Conf. on Human Factors in Computing Systems*
    (Glasgow, Scotland Uk) *(CHI ’19)*. ACM, 1–13.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amershi 等人 (2019) Saleema Amershi、Dan Weld、Mihaela Vorvoreanu、Adam Fourney、Besmira
    Nushi、Penny Collisson、Jina Suh、Shamsi Iqbal、Paul N. Bennett、Kori Inkpen、Jaime
    Teevan、Ruth Kikin-Gil 和 Eric Horvitz. 2019. 人工智能交互指南. 见于 *2019年CHI计算机系统人因会议论文集*
    (格拉斯哥, 苏格兰, 英国) *(CHI ’19)*. ACM, 1–13.
- en: 'Anderson et al. (2016) Peter Anderson, Basura Fernando, Mark Johnson, and Stephen
    Gould. 2016. SPICE: Semantic Propositional Image Caption Evaluation. In *Computer
    Vision – ECCV 2016*. Springer Intl. Publishing, Cham, 382–398.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anderson 等人 (2016) Peter Anderson、Basura Fernando、Mark Johnson 和 Stephen Gould.
    2016. SPICE: 语义命题图像描述评估. 见于 *计算机视觉 – ECCV 2016*. Springer Intl. Publishing, Cham,
    382–398.'
- en: 'Aronson and Lang (2010) Alan R Aronson and François-Michel Lang. 2010. An overview
    of MetaMap: historical perspective and recent advances. *Journal of the American
    Medical Informatics Association* 17, 3 (2010), 229–236.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aronson 和 Lang (2010) Alan R Aronson 和 François-Michel Lang. 2010. MetaMap
    概述: 历史视角与近期进展. *美国医学信息学协会杂志* 17, 3 (2010), 229–236.'
- en: Babar et al. (2021) Zaheer Babar, Twan van Laarhoven, Fabio Massimo Zanzotto,
    and Elena Marchiori. 2021. Evaluating diagnostic content of AI-generated radiology
    reports of chest X-rays. *Artificial Intelligence in Medicine* 116 (2021), 102075.
    [https://doi.org/10.1016/j.artmed.2021.102075](https://doi.org/10.1016/j.artmed.2021.102075)
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babar 等人 (2021) Zaheer Babar、Twan van Laarhoven、Fabio Massimo Zanzotto 和 Elena
    Marchiori. 2021. 评估AI生成的胸部X光报告的诊断内容. *医学中的人工智能* 116 (2021), 102075. [https://doi.org/10.1016/j.artmed.2021.102075](https://doi.org/10.1016/j.artmed.2021.102075)
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
    An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
    In *Proc of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for
    Machine Translation and/or Summarization*. ACL, Ann Arbor, Michigan, 65–72.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banerjee 和 Lavie (2005) Satanjeev Banerjee 和 Alon Lavie. 2005. METEOR: 一种与人工评判相关性更高的机器翻译评价自动指标.
    见于 *ACL机器翻译和/或摘要评价措施工作坊论文集*. ACL, 安娜堡, 密歇根, 65–72.'
- en: 'Ben Abacha et al. (2020) Asma Ben Abacha, Vivek V. Datla, Sadid A. Hasan, Dina
    Demner-Fushman, and Henning Müller. 2020. Overview of the VQA-Med Task at ImageCLEF
    2020: Visual Question Answering and Generation in the Medical Domain. In *CLEF
    2020 Working Notes* *(CEUR Workshop Proceedings)*. Thessaloniki, Greece.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben Abacha 等人 (2020) Asma Ben Abacha、Vivek V. Datla、Sadid A. Hasan、Dina Demner-Fushman
    和 Henning Müller. 2020. ImageCLEF 2020 VQA-Med 任务概述: 医疗领域的视觉问答和生成. 见于 *CLEF 2020工作笔记*
    *(CEUR工作论文集)*. 塞萨洛尼基, 希腊.'
- en: 'Ben Abacha et al. (2019) Asma Ben Abacha, Sadid A. Hasan, Vivek V. Datla, Joey
    Liu, Dina Demner-Fushman, and Henning Müller. 2019. VQA-Med: Overview of the Medical
    Visual Question Answering Task at ImageCLEF 2019\. In *CLEF2019 Working Notes*
    *(CEUR Workshop Proceedings)*. Lugano, Switzerland.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ben Abacha 等人 (2019) Asma Ben Abacha、Sadid A. Hasan、Vivek V. Datla、Joey Liu、Dina
    Demner-Fushman 和 Henning Müller. 2019. VQA-Med: ImageCLEF 2019 医疗视觉问答任务概述. 见于
    *CLEF2019工作笔记* *(CEUR工作论文集)*. 卢加诺, 瑞士.'
- en: Bengio et al. (2009) Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason
    Weston. 2009. Curriculum Learning. In *Proc of the 26th Annual Intl. Conf. on
    Machine Learning* (Montreal, Quebec, Canada) *(ICML ’09)*. ACM, 41–48.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等人 (2009) Yoshua Bengio、Jérôme Louradour、Ronan Collobert 和 Jason Weston.
    2009. 课程学习. 见于 *第26届国际机器学习大会论文集* (蒙特利尔, 魁北克, 加拿大) *(ICML ’09)*. ACM, 41–48.
- en: 'Biswal et al. (2020) Siddharth Biswal, Cao Xiao, Lucas M. Glass, Brandon Westover,
    and Jimeng Sun. 2020. CLARA: Clinical Report Auto-Completion. In *Proc of The
    Web Conf. 2020* (Taipei, Taiwan) *(WWW ’20)*. ACM, New York, NY, USA, 541–550.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biswal 等人 (2020) Siddharth Biswal、Cao Xiao、Lucas M. Glass、Brandon Westover
    和 Jimeng Sun. 2020. CLARA: 临床报告自动完成. 见于 *2020年网络会议论文集* (台北, 台湾) *(WWW ’20)*. ACM,
    纽约, NY, 美国, 541–550.'
- en: Boag et al. (2020) William Boag, Tzu-Ming Harry Hsu, Matthew Mcdermott, Gabriela
    Berner, Emily Alesentzer, and Peter Szolovits. 2020. Baselines for Chest X-Ray
    Report Generation. In *Proc of the Machine Learning for Health NeurIPS Workshop*
    *(Proc of Machine Learning Research)*, Vol. 116\. PMLR, 126–140.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boag 等人 (2020) William Boag、Tzu-Ming Harry Hsu、Matthew Mcdermott、Gabriela Berner、Emily
    Alesentzer 和 Peter Szolovits. 2020. 胸部X光报告生成的基线. 见于 *健康神经信息处理系统工作坊论文集* *(机器学习研究论文集)*,
    第116卷. PMLR, 126–140.
- en: Branko et al. (2010) Milosavljević Branko, Boberić Danijela, and Surla Dušan.
    2010. Retrieval of bibliographic records using Apache Lucene. *The Electronic
    Library* 28, 4 (01 Jan 2010), 525–539.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Branko 等 (2010) Milosavljević Branko, Boberić Danijela, 和 Surla Dušan. 2010.
    使用 Apache Lucene 检索书目记录。*电子图书馆* 28, 4 (2010年1月1日), 525–539.
- en: 'Bustos et al. (2019) Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and
    Maria de la Iglesia-Vayá. 2019. Padchest: A large chest x-ray image dataset with
    multi-label annotated reports. *arXiv:1901.07441* (2019).'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bustos 等 (2019) Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, 和 Maria
    de la Iglesia-Vayá. 2019. Padchest：一个大型胸部 X 射线图像数据集，具有多标签标注报告。*arXiv:1901.07441*
    (2019).
- en: 'Cai et al. (2019) Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox,
    and Michael Terry. 2019. ” Hello AI”: Uncovering the Onboarding Needs of Medical
    Practitioners for Human-AI Collaborative Decision-Making. *Proc of the ACM on
    Human-computer Interaction* 3, CSCW (2019), 1–24.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等 (2019) Carrie J Cai, Samantha Winter, David Steiner, Lauren Wilcox, 和
    Michael Terry. 2019. “Hello AI”：揭示医疗从业者在人工智能协作决策中的入门需求。*ACM 人机交互学报* 3, CSCW (2019),
    1–24.
- en: Caruana (1997) Rich Caruana. 1997. Multitask learning. *Machine learning* 28,
    1 (1997), 41–75.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caruana (1997) Rich Caruana. 1997. 多任务学习。*机器学习* 28, 1 (1997), 41–75.
- en: 'Carvalho et al. (2019) Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso.
    2019. Machine learning interpretability: A survey on methods and metrics. *Electronics*
    8, 8 (2019), 832.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carvalho 等 (2019) Diogo V Carvalho, Eduardo M Pereira, 和 Jaime S Cardoso. 2019.
    机器学习解释性：方法和指标的综述。*电子学* 8, 8 (2019), 832.
- en: Chen and Manning (2014) Danqi Chen and Christopher Manning. 2014. A Fast and
    Accurate Dependency Parser using Neural Networks. In *Proc of the 2014 Conf. on
    Empirical Methods in Natural Language Processing (EMNLP)*. ACL, Doha, Qatar, 740–750.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Manning (2014) Danqi Chen 和 Christopher Manning. 2014. 使用神经网络的快速准确的依赖解析器。见于
    *2014 年自然语言处理实证方法会议 (EMNLP)*. ACL, 多哈, 卡塔尔, 740–750.
- en: Chopra et al. (2005) Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning
    a similarity metric discriminatively, with application to face verification. In
    *2005 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR’05)*,
    Vol. 1\. IEEE, 539–546.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chopra 等 (2005) Sumit Chopra, Raia Hadsell, 和 Yann LeCun. 2005. 通过辨别性学习相似度度量，应用于面部验证。见于
    *2005 IEEE 计算机视觉与模式识别会议 (CVPR’05)*, 第 1 卷. IEEE, 539–546.
- en: Christ et al. (2017) P Christ, F Ettlinger, F Grün, J Lipkova, and G Kaissis.
    2017. Lits-liver tumor segmentation challenge. *ISBI and MICCAI* (2017).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christ 等 (2017) P Christ, F Ettlinger, F Grün, J Lipkova, 和 G Kaissis. 2017.
    Lits-肝脏肿瘤分割挑战。*ISBI 和 MICCAI* (2017).
- en: Chung et al. (2014) Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua
    Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence
    modeling. In *NIPS 2014 Workshop on Deep Learning, December 2014*.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等 (2014) Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, 和 Yoshua Bengio.
    2014. 门控递归神经网络在序列建模上的实证评估。见于 *NIPS 2014 深度学习研讨会, 2014年12月*。
- en: 'Decencière et al. (2014) Etienne Decencière, Xiwei Zhang, Guy Cazuguel, Bruno
    Lay, Béatrice Cochener, Caroline Trone, Philippe Gain, Richard Ordonez, Pascale
    Massin, Ali Erginay, et al. 2014. Feedback on a publicly distributed image database:
    the Messidor database. *Image Analysis & Stereology* 33, 3 (2014), 231–234.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Decencière 等 (2014) Etienne Decencière, Xiwei Zhang, Guy Cazuguel, Bruno Lay,
    Béatrice Cochener, Caroline Trone, Philippe Gain, Richard Ordonez, Pascale Massin,
    Ali Erginay 等. 2014. 关于公开分发的图像数据库的反馈：Messidor 数据库。*图像分析与立体视觉* 33, 3 (2014), 231–234.
- en: Demner-Fushman et al. (2015) Dina Demner-Fushman, Marc D. Kohli, Marc B. Rosenman,
    Sonya E. Shooshan, Laritza Rodriguez, Sameer Antani, George R. Thoma, and Clement J.
    McDonald. 2015. Preparing a collection of radiology examinations for distribution
    and retrieval. *Journal of the American Medical Informatics Association* 23, 2
    (07 2015), 304–310.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demner-Fushman 等 (2015) Dina Demner-Fushman, Marc D. Kohli, Marc B. Rosenman,
    Sonya E. Shooshan, Laritza Rodriguez, Sameer Antani, George R. Thoma, 和 Clement
    J. McDonald. 2015. 为分发和检索准备放射学检查集合。*美国医学信息学协会期刊* 23, 2 (2015年7月), 304–310.
- en: 'Deng et al. (2009) J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei.
    2009. ImageNet: A large-scale hierarchical image database. In *2009 IEEE Conf.
    on Computer Vision and Pattern Recognition*. 248–255.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2009) J. Deng, W. Dong, R. Socher, L. Li, Kai Li, 和 Li Fei-Fei. 2009.
    ImageNet：大规模层次图像数据库。见于 *2009 IEEE 计算机视觉与模式识别会议*. 248–255.
- en: 'Denkowski and Lavie (2010) Michael Denkowski and Alon Lavie. 2010. Extending
    the Meteor Machine Translation Evaluation Metric to the Phrase Level. In *Human
    Language Technologies: The 2010 Annual Conf. of the North American Chapter of
    the ACL* (Los Angeles, California) *(HLT ’10)*. ACL, USA, 250–253.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denkowski 和 Lavie (2010) Michael Denkowski 和 Alon Lavie。2010。《将 Meteor 机器翻译评价指标扩展到短语级别》。收录于
    *人类语言技术：2010 年北美 ACL 年会* (加利福尼亚州洛杉矶) *(HLT ’10)*。ACL，美国，250–253。
- en: 'Denkowski and Lavie (2011) Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
    Automatic Metric for Reliable Optimization and Evaluation of Machine Translation
    Systems. In *Proc of the Sixth Workshop on Statistical Machine Translation* (Edinburgh,
    Scotland) *(WMT ’11)*. ACL, USA, 85–91.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denkowski 和 Lavie (2011) Michael Denkowski 和 Alon Lavie。2011。《Meteor 1.3：用于可靠优化和评估机器翻译系统的自动化指标》。收录于
    *第六届统计机器翻译研讨会论文集* (苏格兰爱丁堡) *(WMT ’11)*。ACL，美国，85–91。
- en: 'Denkowski and Lavie (2014) Michael Denkowski and Alon Lavie. 2014. Meteor Universal:
    Language Specific Translation Evaluation for Any Target Language. In *Proc of
    the Ninth Workshop on Statistical Machine Translation*. ACL, Baltimore, Maryland,
    USA, 376–380.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denkowski 和 Lavie (2014) Michael Denkowski 和 Alon Lavie。2014。《Meteor Universal：任何目标语言的语言特定翻译评估》。收录于
    *第九届统计机器翻译研讨会论文集*。ACL，美国马里兰州巴尔的摩，376–380。
- en: Doshi-Velez and Kim (2017) Finale Doshi-Velez and Been Kim. 2017. Towards A
    Rigorous Science of Interpretable Machine Learning. *stat* 1050 (2017), 2.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doshi-Velez 和 Kim (2017) Finale Doshi-Velez 和 Been Kim。2017。《迈向可解释机器学习的严谨科学》。*stat*
    1050 (2017)，2。
- en: 'Došilović et al. (2018) F. K. Došilović, M. Brčić, and N. Hlupić. 2018. Explainable
    artificial intelligence: A survey. In *2018 41st Intl. Convention on Information
    and Communication Technology, Electronics and Microelectronics (MIPRO)*. 0210–0215.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Došilović 等 (2018) F. K. Došilović, M. Brčić 和 N. Hlupić。2018。《可解释人工智能：综述》。收录于
    *2018 第41届信息与通信技术、电子与微电子国际会议 (MIPRO)*。0210–0215。
- en: Eickhoff et al. (2017) Carsten Eickhoff, Immanuel Schwall, Alba García Seco de
    Herrera, and Henning Müller. 2017. Overview of ImageCLEFcaption 2017 - the Image
    Caption Prediction and Concept Extraction Tasks to Understand Biomedical Images.
    In *CLEF2017 Working Notes* *(CEUR Workshop Proceedings)*. Dublin, Ireland.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eickhoff 等 (2017) Carsten Eickhoff, Immanuel Schwall, Alba García Seco de Herrera
    和 Henning Müller。2017。《ImageCLEFcaption 2017 概述 - 图像标题预测与概念提取任务以理解生物医学图像》。收录于
    *CLEF2017 工作笔记* *(CEUR 工作论文集)*。爱尔兰都柏林。
- en: 'Gajbhiye et al. (2020) Gaurav O. Gajbhiye, Abhijeet V. Nandedkar, and Ibrahima
    Faye. 2020. Automatic Report Generation for Chest X-Ray Images: A Multilevel Multi-attention
    Approach. In *Computer Vision and Image Processing*. Springer Singapore, Singapore,
    174–182.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gajbhiye 等 (2020) Gaurav O. Gajbhiye, Abhijeet V. Nandedkar 和 Ibrahima Faye。2020。《胸部
    X 射线图像的自动报告生成：多层次多注意力方法》。收录于 *计算机视觉与图像处理*。Springer Singapore，新加坡，174–182。
- en: Gale et al. (2017) William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P
    Bradley, and Lyle J Palmer. 2017. Detecting hip fractures with radiologist-level
    performance using deep neural networks. *arXiv:1711.06504* (2017). arXiv:1711.06504
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale 等 (2017) William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P Bradley
    和 Lyle J Palmer。2017。《利用深度神经网络检测与放射科医师水平相当的髋部骨折》。*arXiv:1711.06504* (2017)。arXiv:1711.06504
- en: Gale et al. (2019) W. Gale, L. Oakden-Rayner, G. Carneiro, L. J. Palmer, and
    A. P. Bradley. 2019. Producing Radiologist-Quality Reports for Interpretable Deep
    Learning.. In *2019 IEEE 16th Intl. Symposium on Biomedical Imaging (ISBI 2019)*.
    1275–1279.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale 等 (2019) W. Gale, L. Oakden-Rayner, G. Carneiro, L. J. Palmer 和 A. P. Bradley。2019。《为可解释深度学习生成放射科医师级报告》。收录于
    *2019 IEEE 第16届国际生物医学影像研讨会 (ISBI 2019)*。1275–1279。
- en: García Seco de Herrera et al. (2018) Alba García Seco de Herrera, Carsten Eickhoff,
    Vincent Andrearczyk, and Henning Müller. 2018. Overview of the ImageCLEF 2018
    Caption Prediction tasks. In *CLEF2018 Working Notes* *(CEUR Workshop Proceedings)*.
    Avignon, France.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: García Seco de Herrera 等 (2018) Alba García Seco de Herrera, Carsten Eickhoff,
    Vincent Andrearczyk 和 Henning Müller。2018。《ImageCLEF 2018 标题预测任务概述》。收录于 *CLEF2018
    工作笔记* *(CEUR 工作论文集)*。法国阿维尼翁。
- en: Gasimova (2019) Aydan Gasimova. 2019. Automated Enriched Medical Concept Generation
    for Chest X-ray Images. In *Interpretability of Machine Intelligence in Medical
    Image Computing and Multimodal Learning for Clinical Decision Support*. Springer
    Intl. Publishing, Cham, 83–92.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gasimova (2019) Aydan Gasimova。2019。《胸部 X 射线图像的自动化丰富医学概念生成》。收录于 *医学图像计算与临床决策支持的机器智能可解释性*。Springer
    Intl. Publishing，Cham，83–92。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative Adversarial Nets. In *Advances in Neural Information Processing Systems
    27*. Curran Associates, Inc., 2672–2680.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。收录于 *Advances in Neural Information Processing Systems 27*。Curran Associates,
    Inc., 2672–2680。
- en: Graves and Schmidhuber (2005) Alex Graves and Jürgen Schmidhuber. 2005. Framewise
    phoneme classification with bidirectional LSTM and other neural network architectures.
    *Neural networks* 18, 5-6 (2005), 602–610.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves and Schmidhuber (2005) Alex Graves 和 Jürgen Schmidhuber. 2005. 使用双向LSTM和其他神经网络架构的逐帧音素分类。*Neural
    networks* 18, 5-6 (2005), 602–610。
- en: Graziani et al. (2018) Mara Graziani, Vincent Andrearczyk, and Henning Müller.
    2018. Regression Concept Vectors for Bidirectional Explanations in Histopathology.
    In *Understanding and Interpreting Machine Learning in Medical Image Computing
    Applications*. Springer Intl. Publishing, Cham, 124–132.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graziani et al. (2018) Mara Graziani, Vincent Andrearczyk, 和 Henning Müller.
    2018. 用于组织病理学中双向解释的回归概念向量。收录于 *Understanding and Interpreting Machine Learning
    in Medical Image Computing Applications*。Springer Intl. Publishing, Cham, 124–132。
- en: Gu et al. (2019) M. Gu, X. Huang, and Y. Fang. 2019. Automatic Generation of
    Pulmonary Radiology Reports with Semantic Tags. In *2019 IEEE 11th Intl. Conf.
    on Advanced Infocomm Technology (ICAIT)*. 162–167.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. (2019) M. Gu, X. Huang, 和 Y. Fang. 2019. 带有语义标签的肺部放射学报告自动生成。收录于 *2019
    IEEE 11th Intl. Conf. on Advanced Infocomm Technology (ICAIT)*。162–167。
- en: Guidotti et al. (2018) Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
    Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A survey of methods
    for explaining black box models. *ACM computing surveys (CSUR)* 51, 5 (2018),
    1–42.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guidotti et al. (2018) Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri,
    Franco Turini, Fosca Giannotti, 和 Dino Pedreschi. 2018. 解释黑箱模型的方法综述。*ACM computing
    surveys (CSUR)* 51, 5 (2018), 1–42。
- en: Gunning (2017) David Gunning. 2017. Explainable artificial intelligence (xai).
    (2017).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gunning (2017) David Gunning. 2017. 可解释人工智能（xai）。(2017)。
- en: Han et al. (2018) Zhongyi Han, Benzheng Wei, Stephanie Leung, Jonathan Chung,
    and Shuo Li. 2018. Towards Automatic Report Generation in Spine Radiology Using
    Weakly Supervised Framework. In *Medical Image Computing and Computer Assisted
    Intervention – MICCAI 2018*. Springer Intl. Publishing, Cham, 185–193.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2018) Zhongyi Han, Benzheng Wei, Stephanie Leung, Jonathan Chung,
    和 Shuo Li. 2018. 基于弱监督框架的脊柱放射学自动报告生成。收录于 *Medical Image Computing and Computer
    Assisted Intervention – MICCAI 2018*。Springer Intl. Publishing, Cham, 185–193。
- en: Harzig et al. (2019a) Philipp Harzig, Yan-Ying Chen, Francine Chen, and Rainer
    Lienhart. 2019a. Addressing Data Bias Problems for Chest X-ray Image Report Generation.
    *ArXiv* abs/1908.02123 (2019).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harzig et al. (2019a) Philipp Harzig, Yan-Ying Chen, Francine Chen, 和 Rainer
    Lienhart. 2019a. 解决胸部X光图像报告生成中的数据偏差问题。*ArXiv* abs/1908.02123 (2019)。
- en: Harzig et al. (2019b) Philipp Harzig, Moritz Einfalt, and Rainer Lienhart. 2019b.
    Automatic Disease Detection and Report Generation for Gastrointestinal Tract Examination.
    In *Proc of the 27th ACM Intl. Conf. on Multimedia* (Nice, France) *(MM ’19)*.
    ACM, New York, NY, USA, 2573–2577.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harzig et al. (2019b) Philipp Harzig, Moritz Einfalt, 和 Rainer Lienhart. 2019b.
    胶囊肠道检查的自动疾病检测与报告生成。收录于 *Proc of the 27th ACM Intl. Conf. on Multimedia*（法国尼斯）*(MM
    ’19)*。ACM, 纽约，NY, USA, 2573–2577。
- en: Hasan et al. (2018a) Sadid A. Hasan, Yuan Ling, Oladimeji Farri, Joey Liu, Matthew
    Lungren, and Henning Müller. 2018a. Overview of the ImageCLEF 2018 Medical Domain
    Visual Question Answering Task. In *CLEF2018 Working Notes* *(CEUR Workshop Proceedings)*.
    Avignon, France.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan et al. (2018a) Sadid A. Hasan, Yuan Ling, Oladimeji Farri, Joey Liu, Matthew
    Lungren, 和 Henning Müller. 2018a. ImageCLEF 2018医学领域视觉问答任务概述。收录于 *CLEF2018 Working
    Notes* *(CEUR Workshop Proceedings)*。法国阿维尼翁。
- en: Hasan et al. (2018b) Sadid A. Hasan, Yuan Ling, Joey Liu, Rithesh Sreenivasan,
    Shreya Anand, Tilak Raj Arora, Vivek Datla, Kathy Lee, Ashequl Qadir, Christine
    Swisher, and Oladimeji Farri. 2018b. Attention-Based Medical Caption Generation
    with Image Modality Classification and Clinical Concept Mapping. In *Experimental
    IR Meets Multilinguality, Multimodality, and Interaction*. Springer Intl. Publishing,
    Cham, 224–230.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hasan et al. (2018b) Sadid A. Hasan, Yuan Ling, Joey Liu, Rithesh Sreenivasan,
    Shreya Anand, Tilak Raj Arora, Vivek Datla, Kathy Lee, Ashequl Qadir, Christine
    Swisher, 和 Oladimeji Farri. 2018b. 基于注意力机制的医学描述生成与图像模态分类及临床概念映射。收录于 *Experimental
    IR Meets Multilinguality, Multimodality, and Interaction*。Springer Intl. Publishing,
    Cham, 224–230。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proc of the IEEE Conf. on Computer
    Vision and Pattern Recognition (CVPR)*. 770–778.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020) Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao
    Xie. 2020. PathVQA: 30000+ Questions for Medical Visual Question Answering. *arXiv:2003.10286*
    (2020).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heath et al. (2001) Michael Heath, Kevin Bowyer, Daniel Kopans, Richard Moore,
    and P Kegelmeyer. 2001. The Digital Database for Screening Mammography. In *Proc
    of the Fifth Intl. Workshop on Digital Mammography, M.J. Yaffe, ed., Medical Physics
    Publishing*, Vol. 58\. 212–218.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hicks et al. (2019) Steven Hicks, Michael Riegler, Pia Smedsrud, Trine B. Haugen,
    Kristin Ranheim Randel, Konstantin Pogorelov, Håkon Kvale Stensland, Duc-Tien
    Dang-Nguyen, Mathias Lux, Andreas Petlund, Thomas de Lange, Peter Thelin Schmidt,
    and Pål Halvorsen. 2019. ACM Multimedia BioMedia 2019 Grand Challenge Overview.
    In *Proc of the 27th ACM Intl. Conf. on Multimedia* (Nice, France) *(MM ’19)*.
    ACM, 2563–2567.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hicks et al. (2018) Steven Alexander Hicks, Konstantin Pogorelov, Thomas de
    Lange, Mathias Lux, Mattis Jeppsson, Kristin Ranheim Randel, Sigrun Eskeland,
    Pål Halvorsen, and Michael Riegler. 2018. Comprehensible Reasoning and Automated
    Reporting of Medical Examinations Based on Deep Learning Analysis. In *Proc of
    the 9th ACM Multimedia Systems Conference* (Amsterdam, Netherlands) *(MMSys ’18)*.
    ACM, 490–493.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    Long short-term memory. *Neural computation* 9, 8 (1997), 1735–1780.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoover (1975) A Hoover. 1975. STARE database. *Available: http://www.ces.clemson.edu/ ahoover/stare*
    (1975).'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Howard et al. (2017) Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
    Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
    Efficient convolutional neural networks for mobile vision applications. *arXiv:1704.04861*
    (2017).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. 2017. Densely connected convolutional networks. In *Proc of the IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*. 4700–4708.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Xin Huang, Fengqi Yan, Wei Xu, and Maozhen Li. 2019. Multi-Attention
    and Incorporating Background Information Model for Chest X-Ray Image Report Generation.
    *IEEE Access* 7 (2019), 154808–154817.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang et al. (2019) Eui Jin Hwang, Sunggyun Park, Kwang-Nam Jin, Jung Im Kim,
    So Young Choi, Jong Hyuk Lee, Jin Mo Goo, Jaehong Aum, Jae-Joon Yim, Julien G.
    Cohen, Gilbert R. Ferretti, Chang Min Park, for the DLAD Development, and Evaluation
    Group. 2019. Development and Validation of a Deep Learning–Based Automated Detection
    Algorithm for Major Thoracic Diseases on Chest Radiographs. *JAMA Network Open*
    2, 3 (03 2019), e191095–e191095.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irvin et al. (2019) Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana
    Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie
    Shpanskaya, et al. 2019. Chexpert: A large chest radiograph dataset with uncertainty
    labels and expert comparison. In *Proc of the AAAI Conf. on Artificial Intelligence*,
    Vol. 33. Association for the Advancement of Artificial Intelligence (AAAI), 590–597.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irvin等（2019）Jeremy Irvin、Pranav Rajpurkar、Michael Ko、Yifan Yu、Silviana Ciurea-Ilcus、Chris
    Chute、Henrik Marklund、Behzad Haghgoo、Robyn Ball、Katie Shpanskaya等。2019。《Chexpert：一个具有不确定性标签和专家比较的大型胸部X射线数据集》。在*AAAI人工智能会议论文集*中，第33卷。人工智能促进协会（AAAI），590–597。
- en: 'Jackson (2018) EF Jackson. 2018. Quantitative Imaging: The Translation from
    Research Tool to Clinical Practice. *Radiology* 286, 2 (2018), 499.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jackson（2018）EF Jackson。2018。《定量成像：从研究工具到临床实践的转变》。*放射学* 286，2（2018），499。
- en: 'Jain et al. (2021) Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven Truong,
    Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P. Lungren, Andrew Y.
    Ng, Curtis Langlotz, and Pranav Rajpurkar. 2021. RadGraph: Extracting Clinical
    Entities and Relations from Radiology Reports. In *Thirty-fifth Conference on
    Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)*.
    [https://openreview.net/forum?id=pMWtc5NKd7V](https://openreview.net/forum?id=pMWtc5NKd7V)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain等（2021）Saahil Jain、Ashwin Agrawal、Adriel Saporta、Steven Truong、Du Nguyen
    Duong、Tan Bui、Pierre Chambon、Yuhao Zhang、Matthew P. Lungren、Andrew Y. Ng、Curtis
    Langlotz和Pranav Rajpurkar。2021。《RadGraph：从放射学报告中提取临床实体和关系》。在*第35届神经信息处理系统数据集与基准跟踪会议（第1轮）*中。[https://openreview.net/forum?id=pMWtc5NKd7V](https://openreview.net/forum?id=pMWtc5NKd7V)
- en: 'Jain and Wallace (2019) Sarthak Jain and Byron C. Wallace. 2019. Attention
    is not Explanation. In *Proc of the 2019 Conf. of the North American Chapter of
    the ACL: Human Language Technologies, Volume 1 (Long and Short Papers)*. ACL,
    Minneapolis, Minnesota.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain和Wallace（2019）Sarthak Jain和Byron C. Wallace。2019。《注意力不是解释》。在*2019年北美ACL分会：人类语言技术会议论文集第1卷（长篇和短篇论文）*中。ACL，明尼苏达州明尼阿波利斯。
- en: 'Jing et al. (2019) Baoyu Jing, Zeya Wang, and Eric Xing. 2019. Show, Describe
    and Conclude: On Exploiting the Structure Information of Chest X-ray Reports.
    In *Proc of the 57th Annual Meeting of the ACL*. ACL, Florence, Italy, 6570–6580.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing等（2019）Baoyu Jing、Zeya Wang和Eric Xing。2019。《展示、描述和总结：利用胸部X射线报告的结构信息》。在*第57届ACL年会论文集*中。ACL，意大利佛罗伦萨，6570–6580。
- en: 'Jing et al. (2018) Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the Automatic
    Generation of Medical Imaging Reports. In *Proc of the 56th Annual Meeting of
    the ACL (Volume 1: Long Papers)*. ACL, Melbourne, Australia, 2577–2586.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing等（2018）Baoyu Jing、Pengtao Xie和Eric Xing。2018。《关于医学影像报告的自动生成》。在*第56届ACL年会论文集（第1卷：长篇论文）*中。ACL，澳大利亚墨尔本，2577–2586。
- en: Johnson et al. (2019b) Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum,
    Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J
    Berkowitz, and Steven Horng. 2019b. MIMIC-CXR-JPG, a large publicly available
    database of labeled chest radiographs. *arXiv:1901.07042* (2019). arXiv:1901.07042
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson等（2019b）Alistair EW Johnson、Tom J Pollard、Nathaniel R Greenbaum、Matthew
    P Lungren、Chih-ying Deng、Yifan Peng、Zhiyong Lu、Roger G Mark、Seth J Berkowitz和Steven
    Horng。2019b。《MIMIC-CXR-JPG，一个大型公开的标记胸部X射线数据库》。*arXiv:1901.07042*（2019）。arXiv:1901.07042
- en: Johnson et al. (2019a) Alistair E. W. Johnson, Tom J. Pollard, Seth J. Berkowitz,
    Nathaniel R. Greenbaum, Matthew P. Lungren, Chih-ying Deng, Roger G. Mark, and
    Steven Horng. 2019a. MIMIC-CXR, a de-identified publicly available database of
    chest radiographs with free-text reports. *Scientific Data* 6, 1 (12 Dec 2019),
    317.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson等（2019a）Alistair E. W. Johnson、Tom J. Pollard、Seth J. Berkowitz、Nathaniel
    R. Greenbaum、Matthew P. Lungren、Chih-ying Deng、Roger G. Mark和Steven Horng。2019a。《MIMIC-CXR，一个去标识化的公开胸部X射线数据库，带有自由文本报告》。*科学数据*
    6，1（2019年12月12日），317。
- en: 'Kaelbling et al. (1996) Leslie Pack Kaelbling, Michael L Littman, and Andrew W
    Moore. 1996. Reinforcement learning: A survey. *Journal of artificial intelligence
    research* 4 (1996), 237–285.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaelbling等（1996）Leslie Pack Kaelbling、Michael L Littman和Andrew W Moore。1996。《强化学习：调查》。*人工智能研究期刊*
    4（1996），237–285。
- en: Kälviäinen and Uusitalo (2007) RVJPH Kälviäinen and H Uusitalo. 2007. DIARETDB1
    diabetic retinopathy database and evaluation protocol. In *Medical Image Understanding
    and Analysis*, Vol. 2007\. Citeseer, 61.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kälviäinen和Uusitalo（2007）RVJPH Kälviäinen和H Uusitalo。2007。《DIARETDB1糖尿病视网膜病数据库和评估协议》。在*医学图像理解与分析*中，第2007卷。Citeseer，61。
- en: 'Kauppi et al. (2006) Tomi Kauppi, Valentina Kalesnykiene, Joni-Kristian Kamarainen,
    Lasse Lensu, Iiris Sorri, Hannu Uusitalo, Heikki Kälviäinen, and Juhani Pietilä.
    2006. DIARETDB0: Evaluation database and methodology for diabetic retinopathy
    algorithms. *Machine Vision and Pattern Recognition Research Group, Lappeenranta
    University of Technology, Finland* 73 (2006), 1–17.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kauppi 等 (2006) Tomi Kauppi、Valentina Kalesnykiene、Joni-Kristian Kamarainen、Lasse
    Lensu、Iiris Sorri、Hannu Uusitalo、Heikki Kälviäinen 和 Juhani Pietilä. 2006. DIARETDB0：糖尿病视网膜病算法的评估数据库和方法。*芬兰拉彭兰塔技术大学机器视觉与模式识别研究组*
    73 (2006)，1–17。
- en: Khan et al. (2020) Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed
    Qureshi. 2020. A survey of the recent architectures of deep convolutional neural
    networks. *Artificial Intelligence Review* (21 Apr 2020), 1–62.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等 (2020) Asifullah Khan、Anabia Sohail、Umme Zahoora 和 Aqsa Saeed Qureshi.
    2020. 近期深度卷积神经网络架构的综述。*人工智能评论*（2020 年 4 月 21 日），1–62。
- en: 'Kim et al. (2018) Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James
    Wexler, Fernanda Viegas, and Rory sayres. 2018. Interpretability Beyond Feature
    Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) *(Proc
    of Machine Learning Research)*, Vol. 80\. PMLR, Stockholmsmässan, Stockholm Sweden,
    2668–2677.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2018) Been Kim、Martin Wattenberg、Justin Gilmer、Carrie Cai、James Wexler、Fernanda
    Viegas 和 Rory Sayres. 2018. 超越特征归因的可解释性：使用概念激活向量 (TCAV) 的定量测试 *(机器学习研究会议论文集)*，第
    80 卷。PMLR，Stockholmsmässan，瑞典斯德哥尔摩，2668–2677。
- en: Kisilev et al. (2016) Pavel Kisilev, Eli Sason, Ella Barkan, and Sharbell Hashoul.
    2016. Medical Image Description Using Multi-task-loss CNN. In *Deep Learning and
    Data Labeling for Medical Applications*. Springer Intl. Publishing, Cham, 121–129.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kisilev 等 (2016) Pavel Kisilev、Eli Sason、Ella Barkan 和 Sharbell Hashoul. 2016.
    使用多任务损失 CNN 的医学图像描述。在 *医学应用的深度学习与数据标注*。Springer Intl. Publishing，Cham，121–129。
- en: 'Komodakis and Zagoruyko (2017) Nikos Komodakis and Sergey Zagoruyko. 2017.
    Paying more attention to attention: improving the performance of convolutional
    neural networks via attention transfer. In *ICLR*. Paris, France.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komodakis 和 Zagoruyko (2017) Nikos Komodakis 和 Sergey Zagoruyko. 2017. 关注更多的注意力：通过注意力转移提高卷积神经网络的性能。在
    *ICLR*。法国巴黎。
- en: Kornblith et al. (2019) Simon Kornblith, Jonathon Shlens, and Quoc V Le. 2019.
    Do better imagenet models transfer better?. In *2019 IEEE/CVF Conf. on Computer
    Vision and Pattern Recognition (CVPR)*. IEEE Computer Society, Los Alamitos, CA,
    USA, 2656–2666.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kornblith 等 (2019) Simon Kornblith、Jonathon Shlens 和 Quoc V Le. 2019. 更好的 ImageNet
    模型是否能更好地迁移？在 *2019 IEEE/CVF 计算机视觉与模式识别大会 (CVPR)*。IEEE 计算机协会，加州洛杉矶，美国，2656–2666。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. In *Advances
    in Neural Information Processing Systems 25*. Curran Associates, Inc., 1097–1105.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E Hinton. 2012.
    使用深度卷积神经网络进行 ImageNet 分类。在 *神经信息处理系统进展 25*。Curran Associates, Inc.，1097–1105。
- en: Kumar et al. (2010) M. P. Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-Paced
    Learning for Latent Variable Models. In *Advances in Neural Information Processing
    Systems 23*. Curran Associates, Inc., 1189–1197.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2010) M. P. Kumar、Benjamin Packer 和 Daphne Koller. 2010. 自适应学习的潜变量模型。在
    *神经信息处理系统进展 23*。Curran Associates, Inc.，1189–1197。
- en: 'Langlotz (2006) Curtis P Langlotz. 2006. RadLex: a new method for indexing
    online educational materials. *Radiographics: a review publication of the Radiological
    Society of North America, Inc* 26, 6 (2006), 1595.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langlotz (2006) Curtis P Langlotz. 2006. RadLex：一种新的在线教育材料索引方法。*Radiographics：北美放射学学会的综述刊物*
    26, 6 (2006)，1595。
- en: 'Lavie and Agarwal (2007) Alon Lavie and Abhaya Agarwal. 2007. Meteor: An Automatic
    Metric for MT Evaluation with High Levels of Correlation with Human Judgments.
    In *Proc of the Second Workshop on Statistical Machine Translation* (Prague, Czech
    Republic) *(StatMT ’07)*. ACL, USA, 228–231.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lavie 和 Agarwal (2007) Alon Lavie 和 Abhaya Agarwal. 2007. Meteor：一种与人工判断高度相关的自动化
    MT 评估指标。在 *第二届统计机器翻译研讨会论文集*（捷克布拉格）*(StatMT ’07)*。ACL，美国，228–231。
- en: Le and Mikolov (2014) Quoc Le and Tomas Mikolov. 2014. Distributed representations
    of sentences and documents. In *Intl. Conf. on machine learning*. 1188–1196.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Mikolov (2014) Quoc Le 和 Tomas Mikolov. 2014. 句子和文档的分布式表示。在 *国际机器学习会议*。1188–1196。
- en: Leaman et al. (2015) Robert Leaman, Ritu Khare, and Zhiyong Lu. 2015. Challenges
    in clinical natural language processing for automated disorder normalization.
    *Journal of biomedical informatics* 57 (2015), 28–37.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaman 等 (2015) Robert Leaman、Ritu Khare 和 Zhiyong Lu. 2015. 临床自然语言处理中的自动化疾病标准化挑战。*生物医学信息学杂志*
    57 (2015)，28–37。
- en: Li et al. (2018) Christy Y. Li, Xiaodan Liang, Zhiting Hu, and Eric P. Xing.
    2018. Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation.
    In *Proc of the 32nd Intl. Conf. on Neural Information Processing Systems* (Montréal,
    Canada) *(NIPS’18)*. Curran Associates Inc., Red Hook, NY, USA, 1537–1547.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等 (2018) Christy Y. Li, Xiaodan Liang, Zhiting Hu 和 Eric P. Xing。2018年。用于医学图像报告生成的混合检索-生成强化代理。发表于*第32届国际神经信息处理系统会议*（蒙特利尔，加拿大）*(NIPS’18)*。Curran
    Associates Inc.，纽约州红钩，美国，1537–1547。
- en: Li et al. (2019b) Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing.
    2019b. Knowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report
    Generation. In *Proc of the AAAI Conf. on Artificial Intelligence*, Vol. 33. 6666–6673.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等 (2019b) Christy Y Li, Xiaodan Liang, Zhiting Hu 和 Eric P Xing。2019b年。知识驱动的编码、检索、释义用于医学图像报告生成。发表于*AAAI人工智能会议录*，第33卷。6666–6673。
- en: Li and Hong (2019) Jiyun Li and Yongliang Hong. 2019. Label Generation System
    Based on Generative Adversarial Network for Medical Image. In *Proc of the 2nd
    Intl. Conf. on Artificial Intelligence and Pattern Recognition* (Beijing, China)
    *(AIPR ’19)*. ACM, 78–82.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li和Hong (2019) Jiyun Li 和 Yongliang Hong。2019年。基于生成对抗网络的医学图像标签生成系统。发表于*第二届国际人工智能与模式识别会议*（北京，中国）*(AIPR
    ’19)*。ACM，78–82。
- en: 'Li et al. (2015) Jiwei Li, Thang Luong, and Dan Jurafsky. 2015. A Hierarchical
    Neural Autoencoder for Paragraphs and Documents. In *Proc of the 53rd Annual Meeting
    of the ACL and the 7th Intl. Joint Conf. on Natural Language Processing (Volume
    1: Long Papers)*. ACL, Beijing, China, 1106–1115.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等 (2015) Jiwei Li, Thang Luong 和 Dan Jurafsky。2015年。用于段落和文档的层次神经自编码器。发表于*ACL年会第53届及第7届国际联合自然语言处理会议（第1卷：长篇论文）*。ACL，北京，中国，1106–1115。
- en: 'Li et al. (2019a) Xin Li, Rui Cao, and Dongxiao Zhu. 2019a. Vispi: Automatic
    Visual Perception and Interpretation of Chest X-rays. *arXiv:1906.05190* (2019).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等 (2019a) Xin Li, Rui Cao 和 Dongxiao Zhu。2019a年。Vispi：胸部X光的自动视觉感知和解释。*arXiv:1906.05190*（2019年）。
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. ACL, Barcelona, Spain, 74–81.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) Chin-Yew Lin。2004年。ROUGE：自动评估摘要的工具包。发表于*文本摘要拓展*。ACL，巴塞罗那，西班牙，74–81。
- en: Lipton (2018) Zachary C Lipton. 2018. The mythos of model interpretability.
    *Commun. ACM* 61, 10 (2018), 36–43.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton (2018) Zachary C Lipton。2018年。模型可解释性的神话。*ACM通讯* 61，第10期（2018年），36–43。
- en: Liu et al. (2019) Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie
    Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. 2019. Clinically Accurate
    Chest X-Ray Report Generation. In *Machine Learning for Healthcare Conference*
    *(Proc of Machine Learning Research)*, Vol. 106\. PMLR, Ann Arbor, Michigan, 249–269.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等 (2019) Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie Boag,
    Wei-Hung Weng, Peter Szolovits, 和 Marzyeh Ghassemi。2019年。临床准确的胸部X光报告生成。发表于*医疗保健会议上的机器学习*
    *(机器学习研究会议录)*，第106卷。PMLR，安娜堡，密歇根州，249–269。
- en: 'Loveymi et al. (2020) Samira Loveymi, Mir Hossein Dezfoulian, and Muharram
    Mansoorizadeh. 2020. Generate Structured Radiology Report from CT Images Using
    Image Annotation Techniques: Preliminary Results with Liver CT. *Journal of Digital
    Imaging* 33, 2 (01 Apr 2020), 375–390.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loveymi等 (2020) Samira Loveymi, Mir Hossein Dezfoulian 和 Muharram Mansoorizadeh。2020年。从CT图像生成结构化放射学报告使用图像注释技术：肝脏CT的初步结果。*数字成像杂志*
    33，第2期（2020年4月1日），375–390。
- en: Ma et al. (2018) Kai Ma, Kaijie Wu, Hao Cheng, Chaochen Gu, Rui Xu, and Xinping
    Guan. 2018. A Pathology Image Diagnosis Network with Visual Interpretability and
    Structured Diagnostic Report. In *Neural Information Processing*. Springer Intl.
    Publishing, Cham, 282–293.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等 (2018) Kai Ma, Kaijie Wu, Hao Cheng, Chaochen Gu, Rui Xu 和 Xinping Guan。2018年。具有视觉可解释性和结构化诊断报告的病理图像诊断网络。发表于*神经信息处理*。Springer
    Intl. Publishing，查姆，282–293。
- en: 'Maksoud et al. (2019) Sam Maksoud, Arnold Wiliem, Kun Zhao, Teng Zhang, Lin
    Wu, and Brian Lovell. 2019. CORAL8: Concurrent Object Regression for Area Localization
    in Medical Image Panels. In *Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2019*. Springer Intl. Publishing, Cham, 432–441.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maksoud等 (2019) Sam Maksoud, Arnold Wiliem, Kun Zhao, Teng Zhang, Lin Wu 和 Brian
    Lovell。2019年。CORAL8：医学图像面板中区域定位的并发对象回归。发表于*医学图像计算与计算机辅助干预 – MICCAI 2019*。Springer
    Intl. Publishing，查姆，432–441。
- en: 'Mathur et al. (2020) Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.
    Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation
    Evaluation Metrics. In *Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics*. Association for Computational Linguistics, Online,
    4984–4997. [https://doi.org/10.18653/v1/2020.acl-main.448](https://doi.org/10.18653/v1/2020.acl-main.448)'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mathur 等（2020） Nitika Mathur、Timothy Baldwin 和 Trevor Cohn. 2020. 深陷于BLEU:
    重新评估自动机器翻译评估指标的评估. 见 *第58届计算语言学协会年会论文集*. 计算语言学协会, 在线, 4984–4997. [https://doi.org/10.18653/v1/2020.acl-main.448](https://doi.org/10.18653/v1/2020.acl-main.448)'
- en: 'Monshi et al. (2020) Maram Mahmoud A. Monshi, Josiah Poon, and Vera Chung.
    2020. Deep learning in generating radiology reports: A survey. *Artificial Intelligence
    in Medicine* 106 (2020), 101878.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Monshi 等（2020） Maram Mahmoud A. Monshi、Josiah Poon 和 Vera Chung. 2020. 生成放射科报告的深度学习：一项调查.
    *医学中的人工智能* 106 (2020), 101878.
- en: Moradi et al. (2016) Mehdi Moradi, Yufan Guo, Yaniv Gur, Mohammadreza Negahdar,
    and Tanveer Syeda-Mahmood. 2016. A Cross-Modality Neural Network Transform for
    Semi-automatic Medical Image Annotation. In *Medical Image Computing and Computer-Assisted
    Intervention – MICCAI 2016*. Springer Intl. Publishing, Cham, 300–307.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moradi 等（2016） Mehdi Moradi、Yufan Guo、Yaniv Gur、Mohammadreza Negahdar 和 Tanveer
    Syeda-Mahmood. 2016. 跨模态神经网络变换用于半自动医学图像标注. 见 *医学图像计算与计算机辅助干预 – MICCAI 2016*. Springer
    Intl. Publishing, Cham, 300–307.
- en: 'Moreira et al. (2012) Inês C Moreira, Igor Amaral, Inês Domingues, António
    Cardoso, Maria Joao Cardoso, and Jaime S Cardoso. 2012. Inbreast: toward a full-field
    digital mammographic database. *Academic radiology* 19, 2 (2012), 236–248.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moreira 等（2012） Inês C Moreira、Igor Amaral、Inês Domingues、António Cardoso、Maria
    Joao Cardoso 和 Jaime S Cardoso. 2012. Inbreast: 朝向全视场数字乳腺摄影数据库. *学术放射学* 19, 2
    (2012), 236–248.'
- en: Mork et al. (2013) J. G. Mork, A. J. J. Yepes, and A. R. Aronson. 2013. The
    NLM medical text indexer system for indexing biomedical literature. In *CEUR Workshop
    Proceedings*, Vol. 1094.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mork 等（2013） J. G. Mork、A. J. J. Yepes 和 A. R. Aronson. 2013. NLM医学文本索引系统用于索引生物医学文献.
    见 *CEUR工作坊论文集*, 第1094卷.
- en: Otter et al. (2020) Daniel W Otter, Julian R Medina, and Jugal K Kalita. 2020.
    A Survey of the Usages of Deep Learning for Natural Language Processing. *IEEE
    Transactions on Neural Networks and Learning Systems* (2020), 1–21.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Otter 等（2020） Daniel W Otter、Julian R Medina 和 Jugal K Kalita. 2020. 深度学习在自然语言处理中的应用调查.
    *IEEE神经网络与学习系统汇刊* (2020), 1–21.
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In
    *Proc of the 40th Annual Meeting of the ACL*. ACL, ACL, Philadelphia, Pennsylvania,
    USA, 311–318.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni 等（2002） Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu. 2002.
    Bleu: 一种自动评估机器翻译的方法. 见 *第40届ACL年会论文集*. ACL, ACL, Philadelphia, Pennsylvania, USA,
    311–318.'
- en: Pascanu et al. (2013) Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013.
    On the Difficulty of Training Recurrent Neural Networks. In *Proc of the 30th
    Intl. Conf. on Intl. Conf. on Machine Learning - Volume 28* *(ICML’13)*. JMLR.org,
    Atlanta, GA, USA, III–1310–III–1318.
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu 等（2013） Razvan Pascanu、Tomas Mikolov 和 Yoshua Bengio. 2013. 训练递归神经网络的难点.
    见 *第30届国际机器学习大会论文集 - 第28卷* *(ICML’13)*. JMLR.org, Atlanta, GA, USA, III–1310–III–1318.
- en: Pavlopoulos et al. (2019) John Pavlopoulos, Vasiliki Kougia, and Ion Androutsopoulos.
    2019. A Survey on Biomedical Image Captioning. In *Proc of the Second Workshop
    on Shortcomings in Vision and Language*. ACL, Minneapolis, Minnesota, 26–36.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlopoulos 等（2019） John Pavlopoulos、Vasiliki Kougia 和 Ion Androutsopoulos.
    2019. 生物医学图像标题生成的调查. 见 *第二届视觉与语言不足研讨会论文集*. ACL, Minneapolis, Minnesota, 26–36.
- en: 'Pelka et al. (2018) Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa,
    and Christoph M. Friedrich. 2018. Radiology Objects in COntext (ROCO): A Multimodal
    Image Dataset. In *Intravascular Imaging and Computer Assisted Stenting and Large-Scale
    Annotation of Biomedical Data and Expert Label Synthesis*. Springer Intl. Publishing,
    Cham, 180–189.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pelka 等（2018） Obioma Pelka、Sven Koitka、Johannes Rückert、Felix Nensa 和 Christoph
    M. Friedrich. 2018. 放射学对象背景（ROCO）：多模态图像数据集. 见 *血管内成像和计算机辅助支架以及生物医学数据的大规模标注和专家标签合成*.
    Springer Intl. Publishing, Cham, 180–189.
- en: 'Peng et al. (2018) Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri,
    Ronald Summers, and Zhiyong Lu. 2018. Negbio: a high-performance tool for negation
    and uncertainty detection in radiology reports. *AMIA Summits on Translational
    Science Proceedings* 2018 (2018), 188.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等（2018） Yifan Peng、Xiaosong Wang、Le Lu、Mohammadhadi Bagheri、Ronald Summers
    和 Zhiyong Lu. 2018. Negbio: 一种高性能的放射科报告否定和不确定性检测工具. *AMIA翻译科学峰会论文集* 2018 (2018),
    188.'
- en: Pino et al. (2021) Pablo Pino, Denis Parra, Cecilia Besa, and Claudio Lagos.
    2021. Clinically Correct Report Generation from Chest X-Rays Using Templates.
    In *Machine Learning in Medical Imaging*, Chunfeng Lian, Xiaohuan Cao, Islem Rekik,
    Xuanang Xu, and Pingkun Yan (Eds.). Springer International Publishing, Cham, 654–663.
    [https://doi.org/10.1007/978-3-030-87589-3_67](https://doi.org/10.1007/978-3-030-87589-3_67)
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pino 等（2021）Pablo Pino、Denis Parra、Cecilia Besa 和 Claudio Lagos。2021年。《基于模板的胸部X光片临床报告生成》。见于
    *《医学影像中的机器学习》*，Chunfeng Lian、Xiaohuan Cao、Islem Rekik、Xuanang Xu 和 Pingkun Yan（编辑）。施普林格国际出版社，查姆，654–663。
    [https://doi.org/10.1007/978-3-030-87589-3_67](https://doi.org/10.1007/978-3-030-87589-3_67)
- en: Pino et al. (2020) Pablo Pino, Denis Parra, Pablo Messina, Cecilia Besa, and
    Sergio Uribe. 2020. Inspecting state of the art performance and NLP metrics in
    image-based medical report generation. *arXiv preprint arXiv:2011.09257* (2020).
    In LXAI at NeurIPS 2020.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pino 等（2020）Pablo Pino、Denis Parra、Pablo Messina、Cecilia Besa 和 Sergio Uribe。2020年。《检查基于图像的医学报告生成中的前沿表现和自然语言处理指标》。*arXiv
    预印本 arXiv:2011.09257*（2020年）。见于 LXAI 在 NeurIPS 2020。
- en: 'Raghu et al. (2019) Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio.
    2019. Transfusion: Understanding transfer learning for medical imaging. In *Advances
    in Neural Information Processing Systems 32*. Curran Associates, Inc., 3347–3357.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghu 等（2019）Maithra Raghu、Chiyuan Zhang、Jon Kleinberg 和 Samy Bengio。2019年。《Transfusion：理解医学影像中的迁移学习》。见于
    *《神经信息处理系统进展 32》*。Curran Associates, Inc.，3347–3357。
- en: 'Rajpurkar et al. (2017) Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon
    Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie
    Shpanskaya, et al. 2017. Chexnet: Radiologist-level pneumonia detection on chest
    x-rays with deep learning. *arXiv:1711.05225* (2017).'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等（2017）Pranav Rajpurkar、Jeremy Irvin、Kaylie Zhu、Brandon Yang、Hershel
    Mehta、Tony Duan、Daisy Ding、Aarti Bagul、Curtis Langlotz、Katie Shpanskaya 等。2017年。《Chexnet：使用深度学习进行胸部X光片的放射科医师级肺炎检测》。*arXiv:1711.05225*（2017年）。
- en: Reiter (2018) Ehud Reiter. 2018. A structured review of the validity of BLEU.
    *Computational Linguistics* 44, 3 (2018), 393–401. [https://doi.org/10.1162/coli_a_00322](https://doi.org/10.1162/coli_a_00322)
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter（2018）Ehud Reiter。2018年。《BLEU的有效性结构化回顾》。*《计算语言学》* 44，3（2018年），393–401。
    [https://doi.org/10.1162/coli_a_00322](https://doi.org/10.1162/coli_a_00322)
- en: 'Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.
    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
    In *Advances in Neural Information Processing Systems 28*. Curran Associates,
    Inc., 91–99.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2015）Shaoqing Ren、Kaiming He、Ross Girshick 和 Jian Sun。2015年。《Faster R-CNN：基于区域提议网络的实时目标检测》。见于
    *《神经信息处理系统进展 28》*。Curran Associates, Inc.，91–99。
- en: 'Reyes et al. (2020) Mauricio Reyes, Raphael Meier, Sérgio Pereira, Carlos A
    Silva, Fried-Michael Dahlweid, Hendrik von Tengg-Kobligk, Ronald M Summers, and
    Roland Wiest. 2020. On the Interpretability of Artificial Intelligence in Radiology:
    Challenges and Opportunities. *Radiology: Artificial Intelligence* 2, 3 (2020),
    e190043.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reyes 等（2020）Mauricio Reyes、Raphael Meier、Sérgio Pereira、Carlos A Silva、Fried-Michael
    Dahlweid、Hendrik von Tengg-Kobligk、Ronald M Summers 和 Roland Wiest。2020年。《人工智能在放射学中的可解释性：挑战与机遇》。*《放射学：人工智能》*
    2，3（2020年），e190043。
- en: 'Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier.
    In *Proc of the 22nd ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining*
    (San Francisco, California, USA) *(KDD ’16)*. ACM, 1135–1144.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等（2016）Marco Tulio Ribeiro、Sameer Singh 和 Carlos Guestrin。2016年。《“我为什么要相信你？”：解释任何分类器的预测》。见于
    *《第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集》*（美国加利福尼亚州旧金山）*(KDD ’16)*。ACM，1135–1144。
- en: Rogers (1963) Frank B Rogers. 1963. Medical subject headings. *Bulletin of the
    Medical Library Association* 51, 1 (1963), 114–116.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rogers（1963）Frank B Rogers。1963年。《医学主题词》。*《医学图书馆协会通报》* 51，1（1963年），114–116。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. In *Medical
    Image Computing and Computer-Assisted Intervention – MICCAI 2015*. Springer Intl.
    Publishing, Cham, 234–241.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ronneberger 等（2015）Olaf Ronneberger、Philipp Fischer 和 Thomas Brox。2015年。《U-Net：用于生物医学图像分割的卷积网络》。见于
    *《医学图像计算与计算机辅助干预 – MICCAI 2015》*。施普林格国际出版社，查姆，234–241。
- en: Rosman et al. (2019) David A Rosman, Judith Bamporiki, Rebecca Stein-Wexler,
    and Robert D Harris. 2019. Developing diagnostic radiology training in low resource
    countries. *Current Radiology Reports* 7, 9 (2019), 27.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosman 等（2019）David A Rosman、Judith Bamporiki、Rebecca Stein-Wexler 和 Robert
    D Harris。2019年。《在资源有限的国家开发诊断放射学培训》。*《当前放射学报告》* 7，9（2019年），27。
- en: 'Selvaraju et al. (2017) Ramprasaath R Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual
    explanations from deep networks via gradient-based localization. In *Proc of the
    IEEE Intl. Conf. on Computer Vision (ICCV)*. 618–626.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shickel et al. (2017) Benjamin Shickel, Patrick James Tighe, Azra Bihorac,
    and Parisa Rashidi. 2017. Deep EHR: a survey of recent advances in deep learning
    techniques for electronic health record (EHR) analysis. *IEEE journal of biomedical
    and health informatics* 22, 5 (2017), 1589–1604.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2016) Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman,
    Jianhua Yao, and Ronald M Summers. 2016. Learning to read chest x-rays: Recurrent
    neural cascade model for automated image annotation. In *Proc of the IEEE Conf.
    on Computer Vision and Pattern Recognition (CVPR)*. 2497–2506.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrikumar et al. (2017) Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.
    2017. Learning Important Features through Propagating Activation Differences.
    In *Proc of the 34th Intl. Conf. on Machine Learning - Volume 70* *(ICML’17)*.
    JMLR.org, Sydney, NSW, Australia, 3145–3153.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Karen Simonyan and Andrew Zisserman. 2014. Very
    deep convolutional networks for large-scale image recognition. *arXiv:1409.1556*
    (2014).
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2019) Sonit Singh, Sarvnaz Karimi, Kevin Ho-Shon, and Len Hamey.
    2019. From Chest X-Rays to Radiology Reports: A Multimodal Machine Learning Approach.
    In *2019 Digital Image Computing: Techniques and Applications (DICTA)*. IEEE,
    1–8.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smilkov et al. (2017) Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas,
    and Martin Wattenberg. 2017. Smoothgrad: removing noise by adding noise. *arXiv:1706.03825*
    (2017).'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soldaini and Goharian (2016) Luca Soldaini and Nazli Goharian. 2016. Quickumls:
    a fast, unsupervised approach for medical concept extraction. In *MedIR workshop,
    sigir*. 1–4.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinks and Moens (2019) Graham Spinks and Marie-Francine Moens. 2019. Justifying
    diagnosis decisions by deep neural networks. *Journal of biomedical informatics*
    96 (2019), 103248.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Springenberg et al. (2015) J Springenberg, Alexey Dosovitskiy, Thomas Brox,
    and M Riedmiller. 2015. Striving for Simplicity: The All Convolutional Net. In
    *ICLR (workshop track)*.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Li Sun, Weipeng Wang, Jiyun Li, and Jingsheng Lin. 2019. Study
    on Medical Image Report Generation Based on Improved Encoding-Decoding Method.
    In *Intelligent Computing Theories and Application*. Springer Intl. Publishing,
    Cham, 686–696.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
    2015. Going deeper with convolutions. In *Proc of the IEEE Conf. on Computer Vision
    and Pattern Recognition (CVPR)*. 1–9.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer
    vision. In *Proc of the IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR)*. 2818–2826.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等 (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens,
    和 Zbigniew Wojna. 2016. 重新思考计算机视觉的 inception 架构。见于 *IEEE计算机视觉与模式识别会议（CVPR）论文集*。2818–2826。
- en: Tian et al. (2018) Jiang Tian, Cong Li, Zhongchao Shi, and Feiyu Xu. 2018. A
    Diagnostic Report Generator from CT Volumes on Liver Tumor with Semi-supervised
    Attention Mechanism. In *Medical Image Computing and Computer Assisted Intervention
    – MICCAI 2018*. Springer Intl. Publishing, Cham, 702–710.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2018) Jiang Tian, Cong Li, Zhongchao Shi, 和 Feiyu Xu. 2018. 基于半监督注意机制的肝脏肿瘤CT体积诊断报告生成器。见于
    *医学图像计算与计算机辅助手术 – MICCAI 2018*。Springer Intl. Publishing，Cham，702–710。
- en: Tian et al. (2019) Jiang Tian, Cheng Zhong, Zhongchao Shi, and Feiyu Xu. 2019.
    Towards Automatic Diagnosis from Multi-modal Medical Data. In *Interpretability
    of Machine Intelligence in Medical Image Computing and Multimodal Learning for
    Clinical Decision Support*. Springer Intl. Publishing, Cham, 67–74.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2019) Jiang Tian, Cheng Zhong, Zhongchao Shi, 和 Feiyu Xu. 2019. 迈向从多模态医疗数据中自动诊断。见于
    *医学图像计算和临床决策支持中的机器智能可解释性与多模态学习*。Springer Intl. Publishing，Cham，67–74。
- en: 'Tjoa and Guan (2019) Erico Tjoa and Cuntai Guan. 2019. A survey on explainable
    artificial intelligence (XAI): towards medical XAI. *arXiv:1907.07374* (2019).'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjoa 和 Guan (2019) Erico Tjoa 和 Cuntai Guan. 2019. 关于可解释人工智能（XAI）的调查：迈向医学XAI。*arXiv:1907.07374*（2019）。
- en: 'Tonekaboni et al. (2019) Sana Tonekaboni, Shalmali Joshi, Melissa D. McCradden,
    and Anna Goldenberg. 2019. What Clinicians Want: Contextualizing Explainable Machine
    Learning for Clinical End Use. In *Proc of the 4th Machine Learning for Healthcare
    Conference* *(Proc of Machine Learning Research)*, Vol. 106\. PMLR, Ann Arbor,
    Michigan, 359–380.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tonekaboni 等 (2019) Sana Tonekaboni, Shalmali Joshi, Melissa D. McCradden, 和
    Anna Goldenberg. 2019. 临床医生的需求：将可解释的机器学习应用于临床终端使用。见于 *第4届医疗保健机器学习会议论文集* *(机器学习研究论文集)*，第106卷。PMLR，美国安娜堡，359–380。
- en: 'Topol (2019) Eric Topol. 2019. *Deep Medicine: How Artificial Intelligence
    Can Make Healthcare Human Again* (1st ed.). Basic Books, Inc., USA.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Topol (2019) Eric Topol. 2019. *深度医学：人工智能如何使医疗再度人性化*（第1版）。Basic Books, Inc.,
    美国。
- en: Tsai and Tao (2019) Min-Jen Tsai and Yu-Han Tao. 2019. Machine Learning Based
    Common Radiologist-Level Pneumonia Detection on Chest X-rays. In *2019 13th Intl.
    Conf. on Signal Processing and Communication Systems (ICSPCS)*. IEEE, 1–7.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 和 Tao (2019) Min-Jen Tsai 和 Yu-Han Tao. 2019. 基于机器学习的普通放射科医师级肺炎检测在胸部X光片上的应用。见于
    *2019年第13届信号处理与通信系统国际会议（ICSPCS）*。IEEE，1–7。
- en: van Miltenburg et al. (2021) Emiel van Miltenburg, Miruna Clinciu, Ondřej Dušek,
    Dimitra Gkatzia, Stephanie Inglis, Leo Leppänen, Saad Mahamood, Emma Manning,
    Stephanie Schoch, Craig Thomson, and Luou Wen. 2021. Underreporting of errors
    in NLG output, and what to do about it. In *Proceedings of the 14th International
    Conference on Natural Language Generation*. Association for Computational Linguistics,
    Aberdeen, Scotland, UK, 140–153. [https://aclanthology.org/2021.inlg-1.14](https://aclanthology.org/2021.inlg-1.14)
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Miltenburg 等 (2021) Emiel van Miltenburg, Miruna Clinciu, Ondřej Dušek,
    Dimitra Gkatzia, Stephanie Inglis, Leo Leppänen, Saad Mahamood, Emma Manning,
    Stephanie Schoch, Craig Thomson, 和 Luou Wen. 2021. 自然语言生成（NLG）输出中的错误报告不足及其解决办法。见于
    *第14届国际自然语言生成会议论文集*。计算语言学协会，苏格兰阿伯丁，英国，140–153。 [https://aclanthology.org/2021.inlg-1.14](https://aclanthology.org/2021.inlg-1.14)
- en: van Miltenburg et al. (2020) Emiel van Miltenburg, Wei-Ting Lu, Emiel Krahmer,
    Albert Gatt, Guanyi Chen, Lin Li, and Kees van Deemter. 2020. Gradations of Error
    Severity in Automatic Image Descriptions. In *Proceedings of the 13th International
    Conference on Natural Language Generation*. Association for Computational Linguistics,
    Dublin, Ireland, 398–411. [https://aclanthology.org/2020.inlg-1.45](https://aclanthology.org/2020.inlg-1.45)
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van Miltenburg 等 (2020) Emiel van Miltenburg, Wei-Ting Lu, Emiel Krahmer, Albert
    Gatt, Guanyi Chen, Lin Li, 和 Kees van Deemter. 2020. 自动图像描述中的错误严重程度分级。见于 *第13届国际自然语言生成会议论文集*。计算语言学协会，爱尔兰都柏林，398–411。
    [https://aclanthology.org/2020.inlg-1.45](https://aclanthology.org/2020.inlg-1.45)
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is All you Need. In *Advances in Neural Information Processing Systems 30*. Curran
    Associates, Inc., 5998–6008.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 2017. 注意力机制才是关键。见于
    *神经信息处理系统30的进展*。Curran Associates, Inc., 5998–6008。
- en: 'Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    2015. Cider: Consensus-based image description evaluation. In *Proc of the IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*. 4566–4575.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, 和 Devi Parikh.
    2015. Cider：基于共识的图像描述评价。在 *IEEE计算机视觉与模式识别会议（CVPR）* 上，4566–4575。
- en: 'Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru
    Erhan. 2015. Show and Tell: A Neural Image Caption Generator. In *Proc of the
    IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*. 3156–3164.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, 和 Dumitru
    Erhan. 2015. 展示与讲述：一种神经图像描述生成器。在 *IEEE计算机视觉与模式识别会议（CVPR）* 上，3156–3164。
- en: Wang et al. (2016) Jinhua Wang, Xi Yang, Hongmin Cai, Wanchang Tan, Cangzheng
    Jin, and Li Li. 2016. Discrimination of Breast Cancer with Microcalcifications
    on Mammography by Deep Learning. *Scientific Reports (Nature Publisher Group)*
    6 (2016), 27327.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2016) Jinhua Wang, Xi Yang, Hongmin Cai, Wanchang Tan, Cangzheng
    Jin, 和 Li Li. 2016. 利用深度学习在乳腺X光片上鉴别微钙化乳腺癌。*科学报告（自然出版集团）* 6 (2016)，27327。
- en: 'Wang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi
    Bagheri, and Ronald M. Summers. 2017. ChestX-ray8: Hospital-Scale Chest X-Ray
    Database and Benchmarks on Weakly-Supervised Classification and Localization of
    Common Thorax Diseases. In *The IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR)*. 3462–3471.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi
    Bagheri, 和 Ronald M. Summers. 2017. ChestX-ray8：医院级胸部X光数据库及其在弱监督分类和常见胸部疾病定位上的基准。在
    *IEEE计算机视觉与模式识别会议（CVPR）* 上，3462–3471。
- en: 'Wang et al. (2018) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M
    Summers. 2018. Tienet: Text-image embedding network for common thorax disease
    classification and reporting in chest x-rays. In *Proc of the IEEE Conf. on Computer
    Vision and Pattern Recognition (CVPR)*. 9049–9058.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, 和 Ronald M
    Summers. 2018. Tienet：用于胸部X光片常见胸部疾病分类和报告的文本-图像嵌入网络。在 *IEEE计算机视觉与模式识别会议（CVPR）*
    上，9049–9058。
- en: 'Wang et al. (2019) Xuwen Wang, Yu Zhang, Zhen Guo, and Jiao Li. 2019. A Computational
    Framework Towards Medical Image Explanation. In *Artificial Intelligence in Medicine:
    Knowledge Representation and Transparent and Explainable Systems*. Springer Intl.
    Publishing, Cham, 120–131.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Xuwen Wang, Yu Zhang, Zhen Guo, 和 Jiao Li. 2019. 面向医学图像解释的计算框架。在
    *医学人工智能：知识表示与透明且可解释的系统*。Springer 国际出版公司，Cham，120–131。
- en: Williams and Zipser (1989) Ronald J Williams and David Zipser. 1989. A learning
    algorithm for continually running fully recurrent neural networks. *Neural computation*
    1, 2 (1989), 270–280.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams and Zipser (1989) Ronald J Williams 和 David Zipser. 1989. 一种用于持续运行的完全递归神经网络的学习算法。*神经计算*
    1, 2 (1989)，270–280。
- en: Wu et al. (2018) C. Wu, H. Chang, J. Liu, and J. R. Jang. 2018. Adaptive Generation
    of Structured Medical Report Using NER Regarding Deep Learning. In *2018 Conf.
    on Technologies and Applications of Artificial Intelligence (TAAI)*. 10–13.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2018) C. Wu, H. Chang, J. Liu, 和 J. R. Jang. 2018. 使用命名实体识别的深度学习自适应生成结构化医学报告。在
    *2018年人工智能技术与应用会议（TAAI）* 上，10–13。
- en: Wu et al. (2017) Luhui Wu, Cheng Wan, Yiquan Wu, and Jiang Liu. 2017. Generative
    caption for diabetic retinopathy images. In *2017 Intl. Conf. on Security, Pattern
    Analysis, and Cybernetics (SPAC)*. 515–519.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2017) Luhui Wu, Cheng Wan, Yiquan Wu, 和 Jiang Liu. 2017. 糖尿病视网膜病变图像的生成描述。在
    *2017国际安全、模式分析与网络安全会议（SPAC）* 上，515–519。
- en: Xie et al. (2020) Xiaozheng Xie, Jianwei Niu, Xuefeng Liu, Zhengsu Chen, and
    Shaojie Tang. 2020. A Survey on Domain Knowledge Powered Deep Learning for Medical
    Image Analysis. *arXiv:2004.12150* (2020).
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2020) Xiaozheng Xie, Jianwei Niu, Xuefeng Liu, Zhengsu Chen, 和 Shaojie
    Tang. 2020. 关于领域知识驱动的深度学习在医学图像分析中的应用综述。*arXiv:2004.12150* (2020)。
- en: Xie et al. (2019) Xiancheng Xie, Yun Xiong, Philip S. Yu, Kangan Li, Suhua Zhang,
    and Yangyong Zhu. 2019. Attention-Based Abnormal-Aware Fusion Network for Radiology
    Report Generation. In *Database Systems for Advanced Applications*. Springer Intl.
    Publishing, Cham, 448–452.
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2019) Xiancheng Xie, Yun Xiong, Philip S. Yu, Kangan Li, Suhua Zhang,
    和 Yangyong Zhu. 2019. 基于注意力的异常感知融合网络用于放射学报告生成。在 *高级应用的数据库系统*。Springer 国际出版公司，Cham，448–452。
- en: Xiong et al. (2019) Yuxuan Xiong, Bo Du, and Pingkun Yan. 2019. Reinforced Transformer
    for Medical Image Captioning. In *Machine Learning in Medical Imaging*. Springer
    Intl. Publishing, Cham, 673–680.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2019) Yuxuan Xiong, Bo Du, 和 Pingkun Yan. 2019. 强化型变压器用于医学图像描述。在
    *医学影像中的机器学习*。Springer 国际出版公司，Cham，673–680。
- en: 'Xu et al. (2015) Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron
    Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show,
    Attend and Tell: Neural Image Caption Generation with Visual Attention. In *Proc
    of the 32nd Intl. Conf. on Intl. Conf. on Machine Learning - Volume 37* *(ICML’15)*.
    JMLR.org, Lille, France, 2048–2057.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue and Huang (2019) Yuan Xue and Xiaolei Huang. 2019. Improved Disease Classification
    in Chest X-Rays with Transferred Features from Report Generation. In *Information
    Processing in Medical Imaging*. Springer Intl. Publishing, Cham, 125–138.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. (2018) Yuan Xue, Tao Xu, L Rodney Long, Zhiyun Xue, Sameer Antani,
    George R Thoma, and Xiaolei Huang. 2018. Multimodal recurrent model with attention
    for automated radiology report generation. In *Intl. Conf. on Medical Image Computing
    and Computer-Assisted Intervention*. Springer, 457–466.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2019) C. Yin, B. Qian, J. Wei, X. Li, X. Zhang, Y. Li, and Q. Zheng.
    2019. Automatic Generation of Medical Imaging Diagnostic Report with Hierarchical
    Recurrent Neural Network. In *2019 IEEE Intl. Conf. on Data Mining (ICDM)*. 728–737.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2019) Jianbo Yuan, Haofu Liao, Rui Luo, and Jiebo Luo. 2019. Automatic
    Radiology Report Generation Based on Multi-view Image Fusion and Medical Concept
    Enrichment. In *Medical Image Computing and Computer Assisted Intervention – MICCAI
    2019*. Springer Intl. Publishing, Cham, 721–729.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2020) Xianhua Zeng, Li Wen, Banggui Liu, and Xiaojun Qi. 2020.
    Deep learning for ultrasound image caption generation based on object detection.
    *Neurocomputing* 392 (2020), 132–141.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2018) Xian-Hua Zeng, Bang-Gui Liu, and Meng Zhou. 2018. Understanding
    and Generating Ultrasound Image Description. *Journal of Computer Science and
    Technology* 33, 5 (2018), 1086–1100.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017b) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. 2017b. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proc of the
    IEEE Intl. Conf. on Computer Vision (ICCV)*. 5907–5915.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-Level
    Convolutional Networks for Text Classification. In *Proc of the 28th Intl. Conf.
    on Neural Information Processing Systems - Volume 1* (Montreal, Canada) *(NIPS’15)*.
    MIT Press, Cambridge, MA, USA, 649–657.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020a) Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D. Manning,
    and Curtis Langlotz. 2020a. Optimizing the Factual Correctness of a Summary: A
    Study of Summarizing Radiology Reports. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*. Association for Computational
    Linguistics, Online, 5108–5120. [https://doi.org/10.18653/v1/2020.acl-main.458](https://doi.org/10.18653/v1/2020.acl-main.458)'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020b) Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan
    Yuille, and Daguang Xu. 2020b. When Radiology Report Generation Meets Knowledge
    Graph. *Proceedings of the AAAI Conference on Artificial Intelligence* 34, 07
    (Apr. 2020), 12910–12917. [https://doi.org/10.1609/aaai.v34i07.6989](https://doi.org/10.1609/aaai.v34i07.6989)
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017a) Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough,
    and Lin Yang. 2017a. Mdnet: A semantically and visually interpretable medical
    image diagnosis network. In *Proc of the IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR)*. 3549–3557.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and
    Yann LeCun. 2017. Adversarially Regularized Autoencoders. arXiv:cs.LG/1706.04223
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. 2016. Learning deep features for discriminative localization.
    In *Proc of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*.
    2921–2929.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017a) Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang
    Wang. 2017a. Learning spatial regularization with image-level supervisions for
    multi-label image classification. In *Proc of the IEEE Conf. on Computer Vision
    and Pattern Recognition (CVPR)*. 2027–2036.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017b) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    2017b. Unpaired image-to-image translation using cycle-consistent adversarial
    networks. In *Proc of the IEEE Intl. Conf. on computer vision*. 2223–2232.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9\. Supplementary Material
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1\. Datasets
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we include Table [8](#S9.T8 "Table 8 ‣ 9.1\. Datasets ‣ 9\. Supplementary
    Material ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") with the main highlights of all datasets, including both
    public and proprietary, and Table [9](#S9.T9 "Table 9 ‣ 9.1\. Datasets ‣ 9\. Supplementary
    Material ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") with details of the additional information provided by each
    collection.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Image Type | # images | # reports | # patients | Used by
    papers |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| Public report datasets |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| IU X-ray (Demner-Fushman et al., [2015](#bib.bib29)) | 2015 | Chest X-Ray
    | 7,470 | 3,955 | 3,955 | (Jing et al., [2018](#bib.bib69); Liu et al., [2019](#bib.bib93);
    Huang et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Li et al.,
    [2018](#bib.bib86), [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145); Xue
    et al., [2018](#bib.bib155); Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152);
    Singh et al., [2019](#bib.bib124); Yin et al., [2019](#bib.bib156); Tian et al.,
    [2019](#bib.bib133); Gasimova, [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37);
    Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17); Xie et al.,
    [2019](#bib.bib151); Xue and Huang, [2019](#bib.bib154); Zhang et al., [2020b](#bib.bib163);
    Jing et al., [2019](#bib.bib68); Shin et al., [2016](#bib.bib121)) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-CXR (Johnson et al., [2019a](#bib.bib71), [b](#bib.bib70)) | 2019 |
    Chest X-Ray | 377,110 | 227,827 | 65,379 | (Liu et al., [2019](#bib.bib93)) |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| PadChest${}^{\textrm{(sp)}}$(Bustos et al., [2019](#bib.bib20)) | 2019 |
    Chest X-Ray | 160,868 | 109,931 | 67,625 | None${}^{\textrm{(5)}}$ |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| ImageCLEF Caption 2017 (Eickhoff et al., [2017](#bib.bib36)) | 2017 | Biomedical${}^{\textrm{(2)}}$
    | 184,614 | 184,614 | - | (Hasan et al., [2018b](#bib.bib52)) |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: '| ImageCLEF Caption 2018 (García Seco de Herrera et al., [2018](#bib.bib40))
    | 2018 | Biomedical${}^{\textrm{(2)}}$ | 232,305 | 232,305 | - | None${}^{\textrm{(5)}}$
    |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
- en: '| ROCO (Pelka et al., [2018](#bib.bib106)) | 2018 | Multiple radiology${}^{\textrm{(3)}}$
    | 81,825 | 81,825 | - | None${}^{\textrm{(5)}}$ |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
- en: '| PEIR Gross (Jing et al., [2018](#bib.bib69)) | 2017 | Gross lesions | 7,442
    | 7,442 | - | (Jing et al., [2018](#bib.bib69)) |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
- en: '| INBreast${}^{\textrm{(pt)}}$(Moreira et al., [2012](#bib.bib100)) | 2012
    | Mammography X-ray | 410 | 115 | 115 | (Sun et al., [2019](#bib.bib129); Li and
    Hong, [2019](#bib.bib88)) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
- en: '| STARE (Hoover, [1975](#bib.bib59)) | 1975 | Retinal fundus | 400 | 400 |
    - | None${}^{\textrm{(5)}}$ |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
- en: '| RDIF${}^{\textrm{(1)}}$(Maksoud et al., [2019](#bib.bib96)) | 2019 | Kidney
    Biopsy | 1,152 | 144 | 144 | (Maksoud et al., [2019](#bib.bib96)) |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
- en: '| Private report datasets |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: '| CX-CHR${}^{\textrm{(ch)}}$(Li et al., [2018](#bib.bib86), [2019b](#bib.bib87);
    Jing et al., [2019](#bib.bib68)) | 2018 | Chest X-Ray | 45,598 | 35,609 | 35,609
    | (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Jing et al., [2019](#bib.bib68))
    |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
- en: '| TJU${}^{\textrm{(ch)}}$(Gu et al., [2019](#bib.bib45)) | 2019 | Chest X-Ray
    | 19,985 | 19,985 | - | (Gu et al., [2019](#bib.bib45)) |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '| Hip fracture (Gale et al., [2017](#bib.bib38), [2019](#bib.bib39)) | 2017
    | Hip X-Ray | 53,279 | 4,010 | 26,639 | (Gale et al., [2019](#bib.bib39)) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
- en: '| Ultrasound (Zeng et al., [2018](#bib.bib159), [2020](#bib.bib158)) | 2018
    | Gallbladder, kidney and liver ultrasound${}^{\textrm{(4)}}$ | 4,302 | 4,302
    | - | (Zeng et al., [2018](#bib.bib159), [2020](#bib.bib158)) |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '| Fetal Ultrasound (Alsharid et al., [2019](#bib.bib8)) | 2019 | Fetal ultrasound${}^{\textrm{(4)}}$
    | 2,800 | 2,800 | - | (Alsharid et al., [2019](#bib.bib8)) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '| CINDRAL (Ma et al., [2018](#bib.bib95)) | 2018 | Cervical neoplasm WSI |
    1,000 | 1,000 | 50 | (Ma et al., [2018](#bib.bib95)) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '| BCIDR (Zhang et al., [2017a](#bib.bib164)) | 2017 | Bladder biopsy | 1,000
    | 1,000 | 32 | (Zhang et al., [2017a](#bib.bib164)) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| Continuous wave (Moradi et al., [2016](#bib.bib99)) | 2016 | Continuous wave
    doppler echocardiography | 722 | 10,479 | - | (Moradi et al., [2016](#bib.bib99))
    |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| Public classification datasets |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| CheXpert (Irvin et al., [2019](#bib.bib64)) | 2019 | Chest X-Ray | 224,316
    | 0 | 65,240 | (Yuan et al., [2019](#bib.bib157); Zhang et al., [2020b](#bib.bib163))
    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '| ChestX-ray14 (Wang et al., [2017](#bib.bib144)) | 2017 | Chest X-Ray | 112,120
    | 0 | 30,805 | (Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Biswal et al.,
    [2020](#bib.bib17); Xue and Huang, [2019](#bib.bib154); Jing et al., [2019](#bib.bib68))
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '| LiTS (Christ et al., [2017](#bib.bib26)) | 2017 | Liver CT scans | 200 |
    0 | - | (Tian et al., [2018](#bib.bib132)) |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
- en: '| ACM Biomedia 2019 (Hicks et al., [2019](#bib.bib56)) | 2019 | Gastrointestinal
    tract ${}^{\textrm{(4)}}$ | 14,033 | 0 | - | (Harzig et al., [2019b](#bib.bib50))
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '| DIARETDB0 (Kauppi et al., [2006](#bib.bib74)) | 2006 | Retinal fundus | 130
    | 0 | - | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '| DIARETDB1 (Kälviäinen and Uusitalo, [2007](#bib.bib73)) | 2007 | Retinal
    fundus | 89 | 0 | - | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
- en: '| Messidor (Decencière et al., [2014](#bib.bib28); Abràmoff et al., [2013](#bib.bib2))
    | 2013 | Retinal fundus | 1,748 | 0 | 874 | (Wu et al., [2017](#bib.bib149)) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '| DDSM (Heath et al., [2001](#bib.bib55)) | 2001 | Mammography X-ray | 10,480
    | 0 | - | (Kisilev et al., [2016](#bib.bib77)) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Private classification datasets |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: '| MRI Spine (Han et al., [2018](#bib.bib48)) | 2018 | Spine MRI scans | $\geq$253
    | 0 | 253 | (Han et al., [2018](#bib.bib48)) |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
- en: 'Table 8\. Datasets used in the literature. WSI stands for Whole Slide Images.
    All reports are written in English, except those marked with ${}^{\textrm{(sp)}}$
    which are in Spanish, with ${}^{\textrm{(ch)}}$ in Chinese, and ${}^{\textrm{(pt)}}$
    in Portuguese. Other notes, ${}^{\textrm{(1)}}$: the RDIF dataset is pending release.
    ${}^{\textrm{(2)}}$: for the ImageCLEF datasets, images were extracted from PubMed
    Central papers and filtered with an automatically to keep only clinical images,
    then it contains samples from other domains. ${}^{\textrm{(3)}}$: contains multiple
    modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI,
    Angiography and PET-CT. ${}^{\textrm{(4)}}$: the images are frames extracted from
    videos. ${}^{\textrm{(5)}}$: none of the papers reviewed used this dataset.'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Text | Tags | Tags annotation method | Localization |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| Public report datasets |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| IU X-ray (Demner-Fushman et al., [2015](#bib.bib29)) | Indication | (1) MeSH
    and RadLex concepts (2) MeSH concepts | (1) Manual (2) MTI and MetaMap | - |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-CXR (Johnson et al., [2019a](#bib.bib71), [b](#bib.bib70)) | Comparis,
    Indicat | 14 CheXpert labels | CheXpert labeler and NegBio | - |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| PadChest (Bustos et al., [2019](#bib.bib20)) | - | 297 labels (findings,
    diagnoses and anatomic) | 27% manual, rest by RNN | - |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: '| ImageCLEF Caption 2017 (Eickhoff et al., [2017](#bib.bib36)) | - | UMLS tags
    | Quick-UMLS | - |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
- en: '| ImageCLEF Caption 2018 (García Seco de Herrera et al., [2018](#bib.bib40))
    | - | UMLS tags | Quick-UMLS | - |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
- en: '| ROCO (Pelka et al., [2018](#bib.bib106)) | - | UMLS tags | Quick-UMLS | -
    |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
- en: '| PEIR Gross (Jing et al., [2018](#bib.bib69)) | - | Top words | Top TF-IDF
    scores | - |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
- en: '| INBreast (Moreira et al., [2012](#bib.bib100)) | - | Abnormalities | Manual
    | Abnormality contours |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
- en: '| STARE (Hoover, [1975](#bib.bib59)) | - | Levels for 39 conditions and presence
    of 13 diagnostics | Manual | - |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| RDIF (Maksoud et al., [2019](#bib.bib96)) | Indication | - | - | - |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| Private report datasets |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| CX-CHR (Li et al., [2018](#bib.bib86), [2019b](#bib.bib87); Jing et al.,
    [2019](#bib.bib68)) | - | - | - | - |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| TJU (Gu et al., [2019](#bib.bib45)) | - | Top abnormality words | 40 most
    frequent words | - |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| Hip fracture (Gale et al., [2017](#bib.bib38), [2019](#bib.bib39)) | - |
    (1) Fracture presence, (2) fracture location and character | (1) CNN (Gale et al.,
    [2017](#bib.bib38)), (2) manual (Gale et al., [2019](#bib.bib39)) | - |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| Ultrasound (Zeng et al., [2018](#bib.bib159), [2020](#bib.bib158)) | - |
    Organ and disease | Manual | Organ bounding boxes |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| Fetal Ultrasound (Alsharid et al., [2019](#bib.bib8)) | - | Body part | Manual
    | - |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| CINDRAL (Ma et al., [2018](#bib.bib95)) | - | Severity level for 4 attributes
    and diagnosis label | Manual | - |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| BCIDR (Zhang et al., [2017a](#bib.bib164)) | - | Disease status (4 possible)
    | Manual | - |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| Continuous wave (Moradi et al., [2016](#bib.bib99)) | - | Valve types | Manual
    | - |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: '| Public classification datasets |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
- en: '| CheXpert (Irvin et al., [2019](#bib.bib64)) | - | 14 CheXpert labels | CheXpert
    labeler | - |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
- en: '| ChestX-ray14 (Wang et al., [2017](#bib.bib144)) | - | 14 disease labels |
    DNorm and MetaMap | Disease bounding boxes for 880 images |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
- en: '| LiTS (Christ et al., [2017](#bib.bib26)) | - | - | - | Liver and tumor segmentation
    masks |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
- en: '| Gastrointestinal challenge (Hicks et al., [2019](#bib.bib56)) | - | 16 labels
    (e.g. anatomic, pathological or surgery findings) | Manual | - |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
- en: '| DIARETDB0 (Kauppi et al., [2006](#bib.bib74)) | - | DR severity level | Manual
    | Abnormality contours |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
- en: '| DIARETDB1 (Kälviäinen and Uusitalo, [2007](#bib.bib73)) | - | DR severity
    level | Manual | Abnormality contours |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
- en: '| Messidor (Decencière et al., [2014](#bib.bib28); Abràmoff et al., [2013](#bib.bib2))
    | - | DR severity level | Manual | - |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
- en: '| DDSM (Heath et al., [2001](#bib.bib55)) | - | Density level | Manual | Abnormalities
    at pixel level |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
- en: '| Private classification datasets |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
- en: '| MRI Spine (Han et al., [2018](#bib.bib48)) | - | - | - | Diseases and body
    parts at pixel level |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
- en: Table 9\. Additional data contained in each dataset. MeSH (Rogers, [1963](#bib.bib116))
    and RadLex (Langlotz, [2006](#bib.bib82)) are sets of medical concepts. MTI (Mork
    et al., [2013](#bib.bib101)), MetaMap (Aronson and Lang, [2010](#bib.bib11)),
    CheXpert labeler (Irvin et al., [2019](#bib.bib64)), NegBio (Peng et al., [2018](#bib.bib107)),
    Quick-UMLS (Soldaini and Goharian, [2016](#bib.bib126)) and DNorm (Leaman et al.,
    [2015](#bib.bib85)) are automatic labeler tools. Manual means manually annotated
    by experts. In all cases, the localization information was manually annotated
    by experts.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Auxiliary Tasks
  id: totrans-594
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [10](#S9.T10 "Table 10 ‣ 9.2\. Auxiliary Tasks ‣ 9\. Supplementary Material
    ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") presents the categories of auxiliary tasks identified in
    the literature and which papers implemented them.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: '| Auxiliary Task | Used by papers |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: '| Multi-label classification | (Jing et al., [2018](#bib.bib69); Yuan et al.,
    [2019](#bib.bib157); Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Li et al., [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Gu et al.,
    [2019](#bib.bib45); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17); Harzig
    et al., [2019b](#bib.bib50); Sun et al., [2019](#bib.bib129); Zhang et al., [2020b](#bib.bib163);
    Jing et al., [2019](#bib.bib68); Shin et al., [2016](#bib.bib121)) |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
- en: '| Single-label classification | (Zhang et al., [2017a](#bib.bib164); Gale et al.,
    [2019](#bib.bib39); Ma et al., [2018](#bib.bib95); Alsharid et al., [2019](#bib.bib8);
    Gasimova, [2019](#bib.bib41); Zeng et al., [2018](#bib.bib159); Hasan et al.,
    [2018b](#bib.bib52); Kisilev et al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99);
    Spinks and Moens, [2019](#bib.bib127); Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| Sentence classification (normal/abnormal/stop) | (Harzig et al., [2019a](#bib.bib49);
    Xie et al., [2019](#bib.bib151); Jing et al., [2019](#bib.bib68)) |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| Segmentation | (Tian et al., [2018](#bib.bib132); Han et al., [2018](#bib.bib48))
    |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '| Object detection | (Kisilev et al., [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158))
    |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| Attention weights regularization | (Maksoud et al., [2019](#bib.bib96); Yin
    et al., [2019](#bib.bib156)) |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| Embedding-to-embedding matching | (Yin et al., [2019](#bib.bib156); Moradi
    et al., [2016](#bib.bib99)) |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| Doc2vec | (Moradi et al., [2016](#bib.bib99)) |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| Text autoencoder | (Tian et al., [2019](#bib.bib133); Spinks and Moens, [2019](#bib.bib127))
    |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| GAN cycle-consistency | (Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: Table 10\. Summary of auxiliary tasks used in the literature.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: 9.3\. Optimization Strategies
  id: totrans-609
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [11](#S9.T11 "Table 11 ‣ 9.3\. Optimization Strategies ‣ 9\. Supplementary
    Material ‣ A Survey on Deep Learning and Explainability for Automatic Report Generation
    from Medical Images") presents the categories of optimization strategies identified
    in the literature and which papers implemented them.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Optimization Strategy | Used by papers |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '| Visual Component | Pretrain in ImageNet | (Huang et al., [2019](#bib.bib62);
    Li et al., [2018](#bib.bib86); Wang et al., [2018](#bib.bib145); Xue et al., [2018](#bib.bib155);
    Li et al., [2019a](#bib.bib90); Singh et al., [2019](#bib.bib124); Maksoud et al.,
    [2019](#bib.bib96); Yin et al., [2019](#bib.bib156); Ma et al., [2018](#bib.bib95);
    Alsharid et al., [2019](#bib.bib8); Gasimova, [2019](#bib.bib41); Zeng et al.,
    [2018](#bib.bib159); Xue and Huang, [2019](#bib.bib154); Jing et al., [2019](#bib.bib68);
    Hasan et al., [2018b](#bib.bib52); Moradi et al., [2016](#bib.bib99); Zeng et al.,
    [2020](#bib.bib158)) |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
- en: '| Train in Auxiliary Medical Image Tasks | (Yuan et al., [2019](#bib.bib157);
    Li et al., [2019b](#bib.bib87); Zhang et al., [2017a](#bib.bib164); Li et al.,
    [2019a](#bib.bib90); Xiong et al., [2019](#bib.bib152); Gale et al., [2019](#bib.bib39);
    Tian et al., [2018](#bib.bib132); Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156);
    Tian et al., [2019](#bib.bib133); Ma et al., [2018](#bib.bib95); Alsharid et al.,
    [2019](#bib.bib8); Harzig et al., [2019a](#bib.bib49); Biswal et al., [2020](#bib.bib17);
    Zeng et al., [2018](#bib.bib159); Harzig et al., [2019b](#bib.bib50); Sun et al.,
    [2019](#bib.bib129); Zhang et al., [2020b](#bib.bib163); Han et al., [2018](#bib.bib48);
    Jing et al., [2019](#bib.bib68); Shin et al., [2016](#bib.bib121); Hasan et al.,
    [2018b](#bib.bib52); Kisilev et al., [2016](#bib.bib77); Zeng et al., [2020](#bib.bib158))
    |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
- en: '| Train in Report Generation (end-to-end) | (Jing et al., [2018](#bib.bib69);
    Liu et al., [2019](#bib.bib93); Wang et al., [2018](#bib.bib145); Zhang et al.,
    [2017a](#bib.bib164); Tian et al., [2018](#bib.bib132); Yin et al., [2019](#bib.bib156);
    Gasimova, [2019](#bib.bib41); Harzig et al., [2019a](#bib.bib49); Xue and Huang,
    [2019](#bib.bib154); Li and Hong, [2019](#bib.bib88); Hasan et al., [2018b](#bib.bib52);
    Tian et al., [2019](#bib.bib133)) |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
- en: '| Report Generation | Teacher-forcing | (Jing et al., [2018](#bib.bib69); Huang
    et al., [2019](#bib.bib62); Yuan et al., [2019](#bib.bib157); Li et al., [2019b](#bib.bib87);
    Wang et al., [2018](#bib.bib145); Xue et al., [2018](#bib.bib155); Zhang et al.,
    [2017a](#bib.bib164); Li et al., [2019a](#bib.bib90); Singh et al., [2019](#bib.bib124);
    Maksoud et al., [2019](#bib.bib96); Gale et al., [2019](#bib.bib39); Tian et al.,
    [2018](#bib.bib132); Gu et al., [2019](#bib.bib45); Yin et al., [2019](#bib.bib156);
    Tian et al., [2019](#bib.bib133); Alsharid et al., [2019](#bib.bib8); Gasimova,
    [2019](#bib.bib41); Gajbhiye et al., [2020](#bib.bib37); Harzig et al., [2019a](#bib.bib49);
    Biswal et al., [2020](#bib.bib17); Xie et al., [2019](#bib.bib151); Zeng et al.,
    [2018](#bib.bib159); Xue and Huang, [2019](#bib.bib154); Sun et al., [2019](#bib.bib129);
    Zhang et al., [2020b](#bib.bib163); Li and Hong, [2019](#bib.bib88); Jing et al.,
    [2019](#bib.bib68); Shin et al., [2016](#bib.bib121); Hasan et al., [2018b](#bib.bib52);
    Wu et al., [2017](#bib.bib149); Spinks and Moens, [2019](#bib.bib127); Zeng et al.,
    [2020](#bib.bib158)) |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
- en: '| Reinforcement Learning | (Liu et al., [2019](#bib.bib93); Li et al., [2018](#bib.bib86);
    Xiong et al., [2019](#bib.bib152); Li and Hong, [2019](#bib.bib88); Jing et al.,
    [2019](#bib.bib68)) |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
- en: '| Other Losses or Training Strategies | Multitask learning | (Jing et al.,
    [2018](#bib.bib69); Li et al., [2019b](#bib.bib87); Wang et al., [2018](#bib.bib145);
    Zhang et al., [2017a](#bib.bib164); Maksoud et al., [2019](#bib.bib96); Tian et al.,
    [2018](#bib.bib132); Yin et al., [2019](#bib.bib156); Tian et al., [2019](#bib.bib133);
    Ma et al., [2018](#bib.bib95); Harzig et al., [2019a](#bib.bib49); Jing et al.,
    [2019](#bib.bib68); Kisilev et al., [2016](#bib.bib77); Spinks and Moens, [2019](#bib.bib127);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
- en: '| Attention weights regularization | (Maksoud et al., [2019](#bib.bib96); Yin
    et al., [2019](#bib.bib156)) |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
- en: '| Contrastive loss | (Yin et al., [2019](#bib.bib156)) |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
- en: '| Regression loss | (Kisilev et al., [2016](#bib.bib77); Moradi et al., [2016](#bib.bib99);
    Zeng et al., [2020](#bib.bib158)) |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
- en: '| Autoencoder | (Tian et al., [2019](#bib.bib133); Spinks and Moens, [2019](#bib.bib127))
    |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
- en: '|  | GAN | (Han et al., [2018](#bib.bib48); Li and Hong, [2019](#bib.bib88);
    Spinks and Moens, [2019](#bib.bib127)) |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
- en: Table 11\. Summary of optimization strategies used in the literature.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
