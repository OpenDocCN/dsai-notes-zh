- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:56:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.05127] Model Complexity of Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.05127] 深度学习的模型复杂性：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05127](https://ar5iv.labs.arxiv.org/html/2103.05127)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05127](https://ar5iv.labs.arxiv.org/html/2103.05127)
- en: '¹¹affiliationtext: School of Computing Science, Simon Fraser University, Burnaby,
    Canada, huxiah@sfu.ca, jpei@cs.sfu.ca²²affiliationtext: Department of Computing
    and Software, McMaster University, Hamilton, Canada, chul9@mcmaster.ca³³affiliationtext:
    Microsoft Research, Beijing, China, {Weiqing.liu, Jiang.bian}@microsoft.com'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹affiliationtext: 计算机科学学院，西蒙弗雷泽大学，加拿大本那比，huxiah@sfu.ca，jpei@cs.sfu.ca²²affiliationtext:
    计算与软件系，麦克马斯特大学，加拿大汉密尔顿，chul9@mcmaster.ca³³affiliationtext: 微软研究院，中国北京，{Weiqing.liu,
    Jiang.bian}@microsoft.com'
- en: 'Model Complexity of Deep Learning: A Survey'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的模型复杂性：综述
- en: Xia Hu Lingyang Chu Jian Pei Weiqing Liu Jiang Bian
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 夏虎 凌阳 储 建裴 卢伟青 卞江
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Model complexity is a fundamental problem in deep learning. In this paper we
    conduct a systematic overview of the latest studies on model complexity in deep
    learning. Model complexity of deep learning can be categorized into expressive
    capacity and effective model complexity. We review the existing studies on those
    two categories along four important factors, including model framework, model
    size, optimization process and data complexity. We also discuss the applications
    of deep learning model complexity including understanding model generalization,
    model optimization, and model selection and design. We conclude by proposing several
    interesting future directions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂性是深度学习中的一个基本问题。本文对深度学习中模型复杂性的最新研究进行了系统的概述。深度学习的模型复杂性可以分为表现能力和有效模型复杂性。我们回顾了这两类研究，并结合模型框架、模型规模、优化过程和数据复杂性这四个重要因素进行了讨论。我们还讨论了深度学习模型复杂性的应用，包括理解模型泛化、模型优化、以及模型选择和设计。最后，我们提出了一些有趣的未来研究方向。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Mainly due to its superior performance, deep learning is disruptive in many
    applications, such as computer vision [[40](#bib.bib40)], natural language processing [[55](#bib.bib55)]
    and computational finance [[91](#bib.bib91)]. At the same time, however, a series
    of fundamental questions about deep learning models remain, such as why deep learning
    can achieve substantially better expressive power comparing to classical machine
    learning models, how to understand and quantify the generalization capability
    of deep models, and how to understand and improve the optimization process. Model
    complexity of deep learning is a core problem and is related to many those fundamental
    questions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其优越的性能，深度学习在许多应用中具有颠覆性，如计算机视觉[[40](#bib.bib40)]、自然语言处理[[55](#bib.bib55)]和计算金融[[91](#bib.bib91)]。然而，深度学习模型仍然存在一系列基本问题，例如为什么深度学习在表现能力上明显优于经典机器学习模型，如何理解和量化深度模型的泛化能力，以及如何理解和改善优化过程。深度学习的模型复杂性是一个核心问题，并且与许多这些基本问题相关。
- en: Model complexity of deep learning is concerned about, for a certain deep learning
    architecture, how complicated problems the deep learning model can express [[15](#bib.bib15),
    [44](#bib.bib44), [70](#bib.bib70), [89](#bib.bib89)]. Understanding the complexity
    of a deep model is a key to precisely understanding the capability and limitation
    of the model. Exploring model complexity is necessary not only for understanding
    a deep model itself, but also for investigating many other related fundamental
    questions. For example, from the statistical learning theory point of view, the
    expressive power of a model is used to bound the generalization error [[69](#bib.bib69)].
    Some recent studies propose norm-based model complexity [[60](#bib.bib60)] and
    sensitivity-based model complexity [[76](#bib.bib76), [81](#bib.bib81)] to explore
    the generalization capability of deep models. Moreover, detecting changes of model
    complexity in a training process can provide insights into understanding and improving
    the performance of model optimization and regularization [[44](#bib.bib44), [74](#bib.bib74),
    [89](#bib.bib89)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的复杂性关注的是，对于某种深度学习架构，该模型能够表达多复杂的问题[[15](#bib.bib15), [44](#bib.bib44),
    [70](#bib.bib70), [89](#bib.bib89)]。理解深度模型的复杂性是准确理解模型能力和局限性的关键。探索模型复杂性不仅对于理解深度模型本身是必要的，还用于研究许多其他相关的基本问题。例如，从统计学习理论的角度看，模型的表达能力用于界定泛化误差[[69](#bib.bib69)]。一些最近的研究提出了基于范数的模型复杂性[[60](#bib.bib60)]和基于敏感性的模型复杂性[[76](#bib.bib76),
    [81](#bib.bib81)]来探讨深度模型的泛化能力。此外，检测训练过程中模型复杂性的变化可以提供对模型优化和正则化性能的理解和改进的见解[[44](#bib.bib44),
    [74](#bib.bib74), [89](#bib.bib89)]。
- en: The investigation of machine learning model complexity dates back to decades
    ago. A series of early studies in 1990s discuss the complexity of classical machine
    learning models [[16](#bib.bib16), [20](#bib.bib20), [21](#bib.bib21), [98](#bib.bib98)].
    One representative model is decision tree [[19](#bib.bib19)], whose complexity
    is always measured by tree depth [[20](#bib.bib20)] and number of leaf nodes [[16](#bib.bib16)].
    Another frequent subject in model complexity analysis is logistic regression,
    which is the foundation of a large number of parameterized models. The model complexity
    of logistic regression is investigated from the perspectives of Vapnik-Chervonenicks
    theory [[26](#bib.bib26), [96](#bib.bib96)], Rademacher complexity [[46](#bib.bib46)],
    Fisher Information matrix [[21](#bib.bib21)], and the razor of model [[6](#bib.bib6)].
    Here, the razor of model is a theoretical index of the complexity of a parametric
    family of models comparing to the true distribution. However, deep learning models
    are dramatically different from those classical machine learning models discussed
    decades ago [[70](#bib.bib70)]. The complexity analysis of classical machine learning
    models cannot be directly applied or straightforwardly extended to deep models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对机器学习模型复杂性的研究可以追溯到几十年前。1990年代的一系列早期研究讨论了经典机器学习模型的复杂性[[16](#bib.bib16), [20](#bib.bib20),
    [21](#bib.bib21), [98](#bib.bib98)]。一个具有代表性的模型是决策树[[19](#bib.bib19)]，其复杂性通常通过树深度[[20](#bib.bib20)]和叶节点数量[[16](#bib.bib16)]来衡量。模型复杂性分析中的另一个常见主题是逻辑回归，它是大量参数化模型的基础。逻辑回归的模型复杂性从Vapnik-Chervonenicks理论[[26](#bib.bib26),
    [96](#bib.bib96)]、Rademacher复杂性[[46](#bib.bib46)]、Fisher信息矩阵[[21](#bib.bib21)]以及模型的剃刀[[6](#bib.bib6)]等角度进行了研究。这里，模型的剃刀是一个理论指标，用于比较参数化模型家族与真实分布的复杂性。然而，深度学习模型与几十年前讨论的那些经典机器学习模型有着显著不同[[70](#bib.bib70)]。经典机器学习模型的复杂性分析不能直接应用或简单扩展到深度模型。
- en: Recently, model complexity in deep learning has attracted more and more attention [[13](#bib.bib13),
    [60](#bib.bib60), [70](#bib.bib70), [78](#bib.bib78), [81](#bib.bib81), [89](#bib.bib89)].
    However, to the best of our knowledge, there is no existing survey on model complexity
    in deep learning. The lack of survey on this emerging and important subject motivates
    us to conduct this survey of the latest studies. In this article, we use the terms
    “deep learning model” and “deep neural network” interchangeably.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习中的模型复杂性引起了越来越多的关注[[13](#bib.bib13), [60](#bib.bib60), [70](#bib.bib70),
    [78](#bib.bib78), [81](#bib.bib81), [89](#bib.bib89)]。然而，据我们所知，目前尚无关于深度学习模型复杂性的现有调查。对这一新兴且重要主题缺乏调查促使我们进行这项最新研究的调查。在本文中，我们将“深度学习模型”和“深度神经网络”这两个术语互换使用。
- en: There are prolific studies on complexity of classical machine learning models
    decades ago, which are summarized in excellent surveys [[20](#bib.bib20), [21](#bib.bib21),
    [61](#bib.bib61), [93](#bib.bib93)]. In the following we very briefly review the
    complexity of several typical models, including decision tree, logistic regression
    and bayesian network models. We also discuss the differences between the model
    complexity of deep neural networks and that of those models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于经典机器学习模型的复杂性，几十年来有大量的研究，这些研究总结在优秀的综述中[[20](#bib.bib20), [21](#bib.bib21),
    [61](#bib.bib61), [93](#bib.bib93)]。接下来，我们将非常简要地回顾几种典型模型的复杂性，包括决策树、逻辑回归和贝叶斯网络模型。我们还讨论了深度神经网络的模型复杂性与这些模型之间的差异。
- en: There are relatively well accepted standard measurements for complexity of decision
    trees. The complexity of a decision tree can be represented by the number of leaf
    nodes [[16](#bib.bib16), [61](#bib.bib61)] or the depth of the tree [[20](#bib.bib20),
    [98](#bib.bib98)]. Since a decision tree is constructed by recursively splitting
    the input space and proposing a local simple model for each resulting region [[71](#bib.bib71)],
    the number of leaf nodes in principle uses the number of resulting regions to
    represent the complexity of the tree. The depth of a decision tree as the complexity
    measure quantifies in the worst case the number of queries needed to make a classification [[20](#bib.bib20)].
    A series of studies [[16](#bib.bib16), [61](#bib.bib61)] investigate tree optimization
    based on the accuracy-complexity tradeoff. Furthermore, Buhrman and De Wolf [[20](#bib.bib20)]
    associate the complexity of decision trees with the complexity of functions represented
    by certificate complexity [[4](#bib.bib4)], sensitivity complexity [[28](#bib.bib28)]
    and approximating polynomials [[80](#bib.bib80)], and use these complexity measures
    of functions to bound the complexity of decision trees.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的复杂性有相对公认的标准测量方法。决策树的复杂性可以通过叶子节点的数量[[16](#bib.bib16), [61](#bib.bib61)]或树的深度[[20](#bib.bib20),
    [98](#bib.bib98)]来表示。由于决策树是通过递归地划分输入空间，并为每个结果区域提出一个局部简单模型[[71](#bib.bib71)]，叶子节点的数量原则上利用结果区域的数量来表示树的复杂性。决策树的深度作为复杂性度量在最坏情况下量化了分类所需的查询数量[[20](#bib.bib20)]。一系列研究[[16](#bib.bib16),
    [61](#bib.bib61)]探讨了基于准确性-复杂性权衡的树优化。此外，Buhrman和De Wolf[[20](#bib.bib20)]将决策树的复杂性与通过证书复杂性[[4](#bib.bib4)]、敏感性复杂性[[28](#bib.bib28)]和逼近多项式[[80](#bib.bib80)]表示的函数复杂性联系起来，并使用这些函数的复杂性度量来界定决策树的复杂性。
- en: Logistic regression is the foundation of a large number of parameterized models [[21](#bib.bib21),
    [46](#bib.bib46)]. In the early 1990s, the degree of effective freedom is proposed
    as a complexity measure for linear models and penalized linear models, which is
    represented by Vapnik-Chervoneniks (VC) dimension [[26](#bib.bib26)]. Besides,
    Rademacher complexity and Gaussian complexity [[8](#bib.bib8), [12](#bib.bib12)]
    are also used to measure model complexity of logistic regression models [[46](#bib.bib46)].
    Comparing to VC dimension, Rademacher complexity takes data distribution into
    consideration and therefore reflects finer-grained model complexity. Later, Bulso et al. [[21](#bib.bib21)]
    and Balasubramanian [[6](#bib.bib6)] suggest that model complexity of logistic
    regression models is related to the number of distinguishable distributions that
    can be represented by the models. Bulso et al. [[21](#bib.bib21)] define a complexity
    measure of logistic regression models based on the determinant of the Fisher Information
    matrix.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是大量参数化模型的基础[[21](#bib.bib21), [46](#bib.bib46)]。在1990年代初，提出了有效自由度作为线性模型和惩罚线性模型的复杂性度量，表示为Vapnik-Chervoneniks
    (VC) 维度[[26](#bib.bib26)]。此外，Rademacher复杂性和高斯复杂性[[8](#bib.bib8), [12](#bib.bib12)]也用于测量逻辑回归模型的复杂性[[46](#bib.bib46)]。与VC维度相比，Rademacher复杂性考虑了数据分布，因此反映了更细致的模型复杂性。后来，Bulso等[[21](#bib.bib21)]和Balasubramanian[[6](#bib.bib6)]建议，逻辑回归模型的复杂性与模型能表示的可区分分布的数量有关。Bulso等[[21](#bib.bib21)]基于Fisher信息矩阵的行列式定义了逻辑回归模型的复杂性度量。
- en: Spiegelhater et al. [[93](#bib.bib93)] systematically investigate the model
    complexity of Bayesian hierarchical models. A unique challenge in measuring the
    complexity of Bayesian hierarchical models is that the number of model parameters
    is not clearly defined. Spiegelhater et al. [[93](#bib.bib93)] define a model
    complexity measure by the number of effective parameters. Using the information
    theoretic argument, they show that this complexity measure can be estimated by
    the difference between the posterior mean of the deviance and the deviance at
    the posterior estimates of the parameters of interest, and is approximately the
    trace of the product of Fisher’s information [[33](#bib.bib33)] and the posterior
    covariance matrix. In addition, they suggest that adding such a complexity measure
    to the posterior mean deviance gives a deviance information criterion for comparing
    models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spiegelhater等人[[93](#bib.bib93)]系统地研究了贝叶斯分层模型的模型复杂性。在测量贝叶斯分层模型复杂性时，一个独特的挑战是模型参数的数量没有明确的定义。Spiegelhater等人[[93](#bib.bib93)]通过有效参数的数量定义了模型复杂性度量。通过信息论的论证，他们展示了这个复杂性度量可以通过偏差的后验均值与感兴趣参数的后验估计下的偏差之间的差异来估计，并且大约等于Fisher信息[[33](#bib.bib33)]和后验协方差矩阵的乘积的迹。此外，他们建议将这样的复杂性度量添加到后验均值偏差中，以得到用于模型比较的偏差信息准则。
- en: Complexity measures are often model specific. The definition of model complexity
    largely depends on model structures. Different model frameworks often call for
    different definitions of complexity measures according to their structural characteristics.
    The complexity measures of different model frameworks usually cannot be directly
    compared.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性度量通常是模型特定的。模型复杂性的定义在很大程度上取决于模型结构。不同的模型框架通常根据其结构特征需要不同的复杂性度量定义。不同模型框架的复杂性度量通常不能直接比较。
- en: Deep learning models are structurally different from traditional machine learning
    models and have dramatically more parameters. Deep learning models are always
    substantially more complex than traditional models. Therefore, the previous methods
    of model complexity of traditional machine learning models cannot be directly
    applied to deep learning models to obtain valid complexity measures. For example,
    measuring the complexity of a decision tree by the depth of the tree [[20](#bib.bib20),
    [98](#bib.bib98)] and the number of leaf nodes [[16](#bib.bib16), [61](#bib.bib61)]
    is obviously not applicable to deep learning models. Measuring model complexity
    by the number of trainable parameters [[46](#bib.bib46)] has a very limited effect
    on deep learning models since deep learning models are often over-parameterized.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在结构上与传统机器学习模型存在显著差异，并且具有更多的参数。深度学习模型通常比传统模型复杂得多。因此，传统机器学习模型的复杂性度量方法不能直接应用于深度学习模型，以获得有效的复杂性度量。例如，通过决策树的深度[[20](#bib.bib20),
    [98](#bib.bib98)]和叶节点数量[[16](#bib.bib16), [61](#bib.bib61)]来衡量决策树的复杂性显然不适用于深度学习模型。通过可训练参数的数量[[46](#bib.bib46)]来衡量模型复杂性对深度学习模型的效果非常有限，因为深度学习模型通常存在过度参数化的情况。
- en: 'The rest of this survey is organized as follows. In Section [2](#S2 "2 Deep
    Learning Model Complexity ‣ Model Complexity of Deep Learning: A Survey"), we
    briefly introduce deep learning model complexity. In Section [3](#S3 "3 Expressive
    Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey"),
    we review the existing studies on the expressive capacity of deep learning models.
    In Section [4](#S4 "4 Effective Complexity of Deep Learning Models ‣ Model Complexity
    of Deep Learning: A Survey"), we survey the existing studies on the effective
    complexity of deep learning models. In Section [5](#S5 "5 Application Examples
    of Deep Learning Model Complexity ‣ Model Complexity of Deep Learning: A Survey"),
    we discuss the applications of deep learning model complexity. In Section [6](#S6
    "6 Conclusions and Future Directions ‣ Model Complexity of Deep Learning: A Survey"),
    we conclude this survey and discuss some future directions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '本综述的其余部分组织如下。在第[2](#S2 "2 Deep Learning Model Complexity ‣ Model Complexity
    of Deep Learning: A Survey)节中，我们简要介绍深度学习模型复杂性。在第[3](#S3 "3 Expressive Capacity
    of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey")节中，我们回顾了关于深度学习模型表现能力的现有研究。在第[4](#S4
    "4 Effective Complexity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey")节中，我们调查了关于深度学习模型有效复杂性的现有研究。在第[5](#S5 "5 Application Examples of Deep
    Learning Model Complexity ‣ Model Complexity of Deep Learning: A Survey")节中，我们讨论了深度学习模型复杂性的应用。在第[6](#S6
    "6 Conclusions and Future Directions ‣ Model Complexity of Deep Learning: A Survey")节中，我们总结了本综述并讨论了一些未来的研究方向。'
- en: 2 Deep Learning Model Complexity
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习模型复杂性
- en: In this section, we first divide deep model complexity into two categories,
    expressive capacity and effective model complexity. Then, we discuss a series
    of important factors of deep learning model complexity, and group the representative
    existing studies accordingly.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将深度模型复杂性分为两个类别：表现能力和有效模型复杂性。然后，我们讨论一系列深度学习模型复杂性的重要因素，并相应地分组现有的代表性研究。
- en: 2.1 What is Deep Learning Model Complexity?
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 什么是深度学习模型复杂性？
- en: The term “model complexity” may refer to two different meanings in deep learning.
    First, model complexity may refer to capacity of deep models in expressing or
    approximating complicated distribution functions [[13](#bib.bib13)]. Second, it
    may describe how complicated the distribution functions are with some parameterized
    deep models [[89](#bib.bib89)]. These two meanings are captured by the notions
    of model expressive capacity and model effective complexity, respectively.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: “模型复杂性”一词在深度学习中可能有两个不同的含义。首先，模型复杂性可能指深度模型在表达或近似复杂分布函数方面的能力[[13](#bib.bib13)]。其次，它可能描述一些参数化深度模型处理复杂分布函数的难度[[89](#bib.bib89)]。这两种含义分别由模型表现能力和模型有效复杂性这两个概念来表达。
- en: Expressive capacity, also known as representation capacity, expressive power,
    and complexity capacity [[86](#bib.bib86), [60](#bib.bib60)], captures the capacity
    of deep learning models in approximating complex problems. Informally, the expressive
    capacity describes the upper bound of the complexity of any model in a family
    of models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表现能力，也称为表示能力、表现力和复杂性能力[[86](#bib.bib86), [60](#bib.bib60)]，捕捉了深度学习模型在近似复杂问题中的能力。非正式地说，表现能力描述了模型家族中任何模型的复杂性上限。
- en: This notion is consistent with the description of hypothesis space complexity [[69](#bib.bib69),
    [96](#bib.bib96)]. The hypothesis space is a family of hypotheses, such as the
    family of all neural networks with a fixed model structure. Considering the hypothesis
    space represented by a fixed model structure, the model expressive capacity is
    also the hypothesis space complexity. In statistical learning theory, the complexity
    of an infinite hypothesis space is represented by its expressive power, that is,
    the richness of the family of hypothesises [[69](#bib.bib69)]. A notion to capture
    hypothesis space complexity is Rademacher complexity [[12](#bib.bib12)], which
    measures the degree to which a hypothesis space can fit random noise. Another
    notion is VC dimension [[26](#bib.bib26)], which reflects the size of the largest
    set that can be shattered by the hypothesis space. Exploring expressive capacity
    helps to obtain the guarantee of learnability of deep models and derive generalization
    bounds [[69](#bib.bib69)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念与假设空间复杂度的描述一致 [[69](#bib.bib69), [96](#bib.bib96)]。假设空间是一个假设的集合，例如所有具有固定模型结构的神经网络的集合。考虑由固定模型结构表示的假设空间，模型的表达能力也是假设空间复杂度。在统计学习理论中，无限假设空间的复杂度由其表达能力表示，即假设集合的丰富程度 [[69](#bib.bib69)]。捕捉假设空间复杂度的一个概念是
    Rademacher 复杂度 [[12](#bib.bib12)]，它衡量假设空间拟合随机噪声的程度。另一个概念是 VC 维度 [[26](#bib.bib26)]，它反映了假设空间可以打破的最大集合的大小。探索表达能力有助于获得深度模型的可学习性保证并推导泛化界限 [[69](#bib.bib69)]。
- en: Effective model complexity, also known as practical complexity, practical expressivity,
    and usable capacity [[37](#bib.bib37), [81](#bib.bib81)], reflects the complexity
    of the functions represented by deep models with specific parameterization [[89](#bib.bib89)].
    The effective model complexity is for a model with parameters fixed. The study
    of effective model complexity helps the exploration of various aspects of deep
    models, such as understanding the optimization algorithms [[81](#bib.bib81)],
    improving model selection strategies [[72](#bib.bib72)] and improving model compression [[25](#bib.bib25)].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有效模型复杂度，也称为实际复杂度、实际表达能力和可用容量 [[37](#bib.bib37), [81](#bib.bib81)]，反映了深度模型在特定参数化下表示的函数的复杂度 [[89](#bib.bib89)]。有效模型复杂度是针对固定参数的模型。对有效模型复杂度的研究有助于探索深度模型的各个方面，例如理解优化算法 [[81](#bib.bib81)]，改进模型选择策略 [[72](#bib.bib72)]
    和改进模型压缩 [[25](#bib.bib25)]。
- en: Expressive capacity and effective model complexity are closely related but are
    two different concepts. Expressive capacity describes the expressive power of
    the hypothesis space of a deep model. Effective model complexity explores the
    complexity of a specific hypothesis within the hypothesis space. Let us use an
    example to demonstrate the distinction and relationship between model expressive
    capacity and effective model complexity.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表达能力和有效模型复杂度密切相关，但它们是两个不同的概念。表达能力描述了深度模型的假设空间的表达能力。有效模型复杂度探讨了假设空间内特定假设的复杂度。让我们用一个例子来演示模型表达能力和有效模型复杂度之间的区别和关系。
- en: Example 1  (Difference between expressive capacity and effective complexity).
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 1  （表达能力和有效复杂度的区别）。
- en: Consider unary polynomial function $f(x)=ax^{2}+bx+c$. The expressive capacity
    of $f(x)$ is unary quadratic. In other words, $f(x)$ cannot express a function
    more complicated than a unary quadratic polynomial. When assigning different values
    to the parameters $a$, $b$ and $c$, the corresponding effective complexity may
    be different. With parameters $a=0$, $b=1$ and $c=1$, for example, $f(x)=x+1$,
    the effective complexity becomes linear, which is obviously lower than the expressive
    capacity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑单变量多项式函数 $f(x)=ax^{2}+bx+c$。$f(x)$ 的表达能力是单变量二次的。换句话说，$f(x)$ 不能表达比单变量二次多项式更复杂的函数。例如，当参数
    $a=0$，$b=1$ 和 $c=1$ 时，$f(x)=x+1$，有效复杂度变为线性，这显然低于表达能力。
- en: Denote by $\mathcal{H}$ the hypothesis space of a fixed deep learning model
    structure, and by $h\in\mathcal{H}$ a hypothesis (i.e., a deep learning model)
    in $\mathcal{H}$. The effective model complexity is the complexity of a specific
    hypothesis, written as $EMC(h)$. The expressive capacity of the deep model, denoted
    by $MEC(\mathcal{H})$, can be written in the form of
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathcal{H}$ 为固定深度学习模型结构的假设空间，$h\in\mathcal{H}$ 为假设（即深度学习模型）在 $\mathcal{H}$
    中。有效模型复杂性是特定假设的复杂性，记作 $EMC(h)$。深度模型的表达能力，记作 $MEC(\mathcal{H})$，可以写成以下形式
- en: '|  | $\sup\{EMC(h):h\in\mathcal{H}\}$ |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sup\{EMC(h):h\in\mathcal{H}\}$ |  |'
- en: This reflects the relationship between model expressive capacity and effective
    model complexity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这反映了模型表达能力与有效模型复杂性之间的关系。
- en: Because of the complex, over-parameterized architectures, deep learning models
    usually have high expressive capacity [[70](#bib.bib70), [87](#bib.bib87)]. However,
    a series of studies [[5](#bib.bib5), [37](#bib.bib37), [81](#bib.bib81)] find
    that the effective complexity of a trained deep model may be much lower than the
    expressive capacity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习模型架构复杂且过度参数化，它们通常具有较高的表达能力 [[70](#bib.bib70), [87](#bib.bib87)]。然而，一系列研究 [[5](#bib.bib5),
    [37](#bib.bib37), [81](#bib.bib81)]发现，训练后的深度模型的有效复杂性可能远低于其表达能力。
- en: Informally and intuitively, a deep learning model can be regarded as a “container”
    of knowledge learned from data. The same model architecture as a “container” may
    contain different amounts of knowledge by learning from different data and thus
    equipped with different parameters. The expressive capacity can be regarded as
    the upper bound of the amount of knowledge that a model architecture can hold.
    The effective model complexity is concerned about, for a specific model, a specific
    training dataset, how much knowledge it actually holds.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式且直观地，深度学习模型可以被视为从数据中学习的知识的“容器”。相同的模型架构作为“容器”可能通过学习不同的数据而包含不同数量的知识，因此具有不同的参数。表达能力可以被视为模型架构能够容纳的知识量的上限。有效模型复杂性则关注于对于特定模型、特定训练数据集，它实际包含了多少知识。
- en: 2.2 Important Factors of Deep Learning Model Complexity
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 深度学习模型复杂性的关键因素
- en: Bonaccorso [[17](#bib.bib17)] points out that a deep learning model consists
    of a static structure part and a dynamic parametric part. The static structure
    part is always determined before the learning process by the model selection principle,
    then stays immutable once decided. The parametric part is the objective of the
    optimization and is determined by a learning process. Both the static and dynamic
    parts contribute to model complexity. We refine this division and summarize four
    aspects affecting model complexity, including both expressive capacity and effective
    complexity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Bonaccorso [[17](#bib.bib17)] 指出，深度学习模型由一个静态结构部分和一个动态参数部分组成。静态结构部分总是在学习过程之前通过模型选择原则确定，然后一旦决定便保持不变。参数部分是优化的目标，由学习过程确定。静态和动态部分都对模型复杂性产生影响。我们细化了这种划分，并总结了影响模型复杂性的四个方面，包括表达能力和有效复杂性。
- en: Model framework
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型框架
- en: The choice of model framework affects model complexity. The factor of model
    framework includes model type (e.g., feedforward neural network, convolutional
    neural network), activation function (e.g., Sigmoid [[29](#bib.bib29)], ReLU [[73](#bib.bib73)]),
    and others. Different model frameworks may require different complexity measure
    criteria and may not be directly comparable to each other.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型框架的选择影响模型复杂性。模型框架的因素包括模型类型（例如前馈神经网络、卷积神经网络）、激活函数（例如 Sigmoid [[29](#bib.bib29)],
    ReLU [[73](#bib.bib73)]）等。不同的模型框架可能需要不同的复杂性度量标准，且可能无法直接进行比较。
- en: Model size
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型规模
- en: The size of deep model affects model complexity. Some commonly used measures
    of model size include the number of parameters, the number of hidden layers, the
    width of hidden layers, the number of filters, and the filter size. Under the
    same model framework, the complexities of models with different sizes can be quantified
    by the same complexity measure criteria and thus become comparable [[54](#bib.bib54)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型的规模影响模型复杂性。一些常用的模型规模度量包括参数数量、隐藏层的数量、隐藏层的宽度、滤波器的数量和滤波器的大小。在相同的模型框架下，不同规模的模型复杂性可以通过相同的复杂性度量标准来量化，从而进行比较 [[54](#bib.bib54)]。
- en: Optimization process
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程
- en: The optimization process affects model complexity, including the form of objective
    functions, the selection of learning algorithms, and the setting of hyperparameters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程影响模型复杂度，包括目标函数的形式、学习算法的选择和超参数的设置。
- en: Data complexity
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据复杂度
- en: The data on which a model is trained affect model complexity, too. The major
    factors include the data dimensionality, data distribution [[69](#bib.bib69)],
    information volume measured by Kolmogorov complexity [[22](#bib.bib22), [59](#bib.bib59)],
    and some others.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的数据也会影响模型复杂度。主要因素包括数据的维度、数据分布 [[69](#bib.bib69)]、由科尔莫哥罗夫复杂度测量的信息量 [[22](#bib.bib22),
    [59](#bib.bib59)] 等。
- en: Among these four aspects, model framework and model size mainly affect the static
    structural part of a deep model, and optimization process and data complexity
    mainly affect the dynamic parametric part.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这四个方面中，模型框架和模型大小主要影响深度模型的静态结构部分，而优化过程和数据复杂度主要影响动态参数部分。
- en: 'Table 1: Summarize the aspects affecting the expressive capacity and effective
    complexity, respectively.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：总结了分别影响表达能力和有效复杂度的各个方面。
- en: '|  |  | Model | Model | Learning | Data |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 模型 | 模型 | 学习 | 数据 |'
- en: '|  |  | Framework | Size | Process | Complexity |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 框架 | 大小 | 过程 | 复杂度 |'
- en: '| Expressive Capacity | Depth Efficiency |  | ✓ |  |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 表达能力 | 深度效率 |  | ✓ |  |  |'
- en: '| Width Efficiency |  | ✓ |  |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 宽度效率 |  | ✓ |  |  |'
- en: '| Expressible Functional Space | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 可表达的函数空间 | ✓ | ✓ | ✓ | ✓ |'
- en: '| VC Dimension and Rademacher Complexity | ✓ | ✓ |  | ✓ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| VC 维度和拉德马赫复杂度 | ✓ | ✓ |  | ✓ |'
- en: '| Effective Complexity | Effective Complexity Measure | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 有效复杂度 | 有效复杂度度量 | ✓ | ✓ | ✓ | ✓ |'
- en: '| High-capacity Low-reality Phenomenon | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 高容量低现实现象 | ✓ | ✓ | ✓ | ✓ |'
- en: The effect of these four aspects on expressive capacity can be understood through
    the influence on the hypothesis space. A selected model framework corresponds
    to a hypothesis space $\mathcal{H}$. Each hypothesis $h\in\mathcal{H}$ represents
    a model with the given framework. Once the model size is determined, the hypothesis
    space shrinks to a subset of $\mathcal{H}$. For example, suppose we set up two
    models with depth $l_{1}$ and $l_{2}$ ($l_{1}\leq l_{2}$), respectively, and both
    with width $m$, the corresponding hypothesis spaces are $\mathcal{H}_{1}$ and
    $\mathcal{H}_{2}$, respectively. We have $\mathcal{H}_{1}\subset\mathcal{H}_{2}$.
    This is because, for each $h\in\mathcal{H}_{1}$, where $h$ is a hypothesis and
    thus a deep network model in this context, there exists $h^{\prime}\in\mathcal{H}_{2}$
    whose first $l_{1}$ layers are identical to those in $h$ and the subsequent layers
    are identical mappings. It is easy to show that the expressive capacity of $\mathcal{H}_{1}$
    will not exceed the expressive capacity of $\mathcal{H}_{2}$. Recently, model
    framework, model size and their effects on expressive capacity of deep models
    have been well explored [[13](#bib.bib13), [51](#bib.bib51), [64](#bib.bib64),
    [89](#bib.bib89)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个方面对表达能力的影响可以通过对假设空间的影响来理解。一个选定的模型框架对应一个假设空间 $\mathcal{H}$。每个假设 $h\in\mathcal{H}$
    代表一个具有给定框架的模型。一旦模型大小确定，假设空间缩小为 $\mathcal{H}$ 的一个子集。例如，假设我们设置两个模型，深度分别为 $l_{1}$
    和 $l_{2}$ （$l_{1}\leq l_{2}$），且宽度均为 $m$，则相应的假设空间为 $\mathcal{H}_{1}$ 和 $\mathcal{H}_{2}$。我们有
    $\mathcal{H}_{1}\subset\mathcal{H}_{2}$。这是因为，对于每个 $h\in\mathcal{H}_{1}$，其中 $h$
    是一个假设，因此在这种情况下是一个深度网络模型，存在 $h^{\prime}\in\mathcal{H}_{2}$，其前 $l_{1}$ 层与 $h$ 中的层相同，后续层是相同的映射。很容易证明，$\mathcal{H}_{1}$
    的表达能力不会超过 $\mathcal{H}_{2}$ 的表达能力。最近，模型框架、模型大小及其对深度模型表达能力的影响已被很好地探索 [[13](#bib.bib13),
    [51](#bib.bib51), [64](#bib.bib64), [89](#bib.bib89)]。
- en: The choice of data distribution and optimization algorithm will further reduce
    the scope of the hypothesis space, thereby affecting the expression capacity.
    For example, Rademacher complexity is a data-dependent expressive capacity measure
    taking into account the effect of data distribution [[12](#bib.bib12), [69](#bib.bib69)].
    However, to the best of our knowledge, the effect of data complexity and optimization
    process on expressive capacity of deep learning models is still rarely explored.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分布和优化算法的选择将进一步缩小假设空间的范围，从而影响表达能力。例如，拉德马赫复杂度是一种依赖于数据的表达能力度量，考虑了数据分布的影响 [[12](#bib.bib12),
    [69](#bib.bib69)]。然而，据我们所知，数据复杂度和优化过程对深度学习模型表达能力的影响仍然很少被探索。
- en: These four aspects also affect the effective model complexity. In general, model
    framework and model size decide the available range of effective model complexity.
    The effective complexity of a model with fixed parameters is a value in this range
    selected by optimization process and training data. The optimization process affects
    the effective complexity, for example, adding $L^{1}$ regularization constrains
    the degrees of freedom in a deep neural network, and thus constrains the effective
    model complexity [[44](#bib.bib44), [46](#bib.bib46)]. The training data affects
    the effecive complexity, for example, using the same model and the same optimization
    process, the effective complexity of a model trained on linearly classifiable
    data is much lower than that trained on the ImageNet [[31](#bib.bib31), [59](#bib.bib59)].
    Specifically, the effect of training data complexity and optimization process
    on effective complexity are reflected in the values of model parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个方面也影响有效模型复杂性。通常，模型框架和模型规模决定了有效模型复杂性的可用范围。具有固定参数的模型的有效复杂性是由优化过程和训练数据选择的该范围内的一个值。优化过程影响有效复杂性，例如，添加$L^{1}$正则化约束深度神经网络中的自由度，从而约束有效模型复杂性 [[44](#bib.bib44),
    [46](#bib.bib46)]。训练数据影响有效复杂性，例如，使用相同的模型和相同的优化过程，在线性可分类数据上训练的模型的有效复杂性远低于在ImageNet上训练的模型 [[31](#bib.bib31),
    [59](#bib.bib59)]。具体来说，训练数据复杂性和优化过程对有效复杂性的影响体现在模型参数的值上。
- en: 'In Table [1](#S2.T1 "Table 1 ‣ 2.2 Important Factors of Deep Learning Model
    Complexity ‣ 2 Deep Learning Model Complexity ‣ Model Complexity of Deep Learning:
    A Survey") we list the corresponding aspects affecting expressive capacity and
    effective complexity, respectively. In Table [2](#S2.T2 "Table 2 ‣ 2.2 Important
    Factors of Deep Learning Model Complexity ‣ 2 Deep Learning Model Complexity ‣
    Model Complexity of Deep Learning: A Survey") we list some representative studies
    on the two major problems, expressive capacity and effective complexity.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [1](#S2.T1 "Table 1 ‣ 2.2 Important Factors of Deep Learning Model Complexity
    ‣ 2 Deep Learning Model Complexity ‣ Model Complexity of Deep Learning: A Survey")
    中，我们列出了分别影响表达能力和有效复杂性的相关方面。在表格 [2](#S2.T2 "Table 2 ‣ 2.2 Important Factors of
    Deep Learning Model Complexity ‣ 2 Deep Learning Model Complexity ‣ Model Complexity
    of Deep Learning: A Survey") 中，我们列出了一些关于这两个主要问题——表达能力和有效复杂性的代表性研究。'
- en: 'Table 2: Overview of a collection of studies on model complexity of deep neural
    networks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：关于深度神经网络模型复杂性的研究综述。
- en: '|  |  | References |  Model-specific  |  Cross-model  |  Measure-based  |  Reduction-based  |
    Objective Model | Measure Metric |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 参考文献 |  特定模型  |  跨模型  |  基于测量  |  基于简化  |  目标模型 | 测量指标 |'
- en: '| Expressive Capacity | Depth Efficiency | [[13](#bib.bib13), [30](#bib.bib30)]
    | $\bullet$ |  |  | $\bullet$ | SPN | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 表达能力 | 深度效率 | [[13](#bib.bib13), [30](#bib.bib30)] | $\bullet$ |  |  | $\bullet$
    | SPN | - |'
- en: '|  | [[67](#bib.bib67)] | $\bullet$ |  |  | $\bullet$ | Binary tree hierarchical
    network, deep Gaussian network | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | [[67](#bib.bib67)] | $\bullet$ |  |  | $\bullet$ | 二叉树分层网络，深度高斯网络 | -
    |'
- en: '|  | [[54](#bib.bib54)] | $\bullet$ |  |  | $\bullet$ | FCNN with $\sigma(z)=\{Signum,Heaviside\}$
    | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | [[54](#bib.bib54)] | $\bullet$ |  |  | $\bullet$ | 带有$\sigma(z)=\{Signum,Heaviside\}$的FCNN
    | - |'
- en: '|  | [[70](#bib.bib70)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma(z)=\{ReLU,Maxout\}$
    | #linear regions |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | [[70](#bib.bib70)] | $\bullet$ |  | $\bullet$ |  | 带有$\sigma(z)=\{ReLU,Maxout\}$的FCNN
    | #线性区域 |'
- en: '|  | [[92](#bib.bib92)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma(z)=\{ReLU,Maxout\}$
    | #linear regions |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | [[92](#bib.bib92)] | $\bullet$ |  | $\bullet$ |  | 带有$\sigma(z)=\{ReLU,Maxout\}$的FCNN
    | #线性区域 |'
- en: '|  | [[14](#bib.bib14), [15](#bib.bib15)] |  | $\bullet$ | $\bullet$ |  | FCNN
    | Sum of Betti number |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | [[14](#bib.bib14), [15](#bib.bib15)] |  | $\bullet$ | $\bullet$ |  | FCNN
    | Betti数之和 |'
- en: '| Width Efficiency | [[64](#bib.bib64)] | $\bullet$ |  |  | $\bullet$ | FCNN
    with $\sigma(z)=\{ReLU\}$ | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 宽度效率 | [[64](#bib.bib64)] | $\bullet$ |  |  | $\bullet$ | 带有$\sigma(z)=\{ReLU\}$的FCNN
    | - |'
- en: '| Expressible Functional Space | [[3](#bib.bib3)] | $\bullet$ |  |  | $\bullet$
    | FCNN with $\sigma(z)=\{ReLU\}$ | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 可表达功能空间 | [[3](#bib.bib3)] | $\bullet$ |  |  | $\bullet$ | 带有$\sigma(z)=\{ReLU\}$的FCNN
    | - |'
- en: '|  | [[36](#bib.bib36)] | $\bullet$ |  |  | $\bullet$ | FCNN with $\sigma(z)=\{ReLU\}$
    | - |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | [[36](#bib.bib36)] | $\bullet$ |  |  | $\bullet$ | 带有$\sigma(z)=\{ReLU\}$的FCNN
    | - |'
- en: '|  | [[52](#bib.bib52)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma(z)=\{\sigma(z)=z^{r}\}$
    | Dimension of functional variety |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | [[52](#bib.bib52)] | $\bullet$ |  | $\bullet$ |  | 带有$\sigma(z)=\{\sigma(z)=z^{r}\}$的FCNN
    | 函数空间的维度 |'
- en: '|  | [[51](#bib.bib51)] |  | $\bullet$ |  | $\bullet$ | FCNN, CNN, RNN | Rank
    of tensor decomposition |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | [[51](#bib.bib51)] |  | $\bullet$ |  | $\bullet$ | FCNN, CNN, RNN | 张量分解的秩
    |'
- en: '| VC Dimension and Rademacher Complexity | [[66](#bib.bib66)] | $\bullet$ |  |
    $\bullet$ |  | FCNN with linear threshold gates | VC dimension |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| VC 维数与 Rademacher 复杂度 | [[66](#bib.bib66)] | $\bullet$ |  | $\bullet$ |  |
    带有线性阈值门的 FCNN | VC 维数 |'
- en: '|  | [[11](#bib.bib11)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma=\{piecewise\
    polynomial\}$ | VC dimension |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | [[11](#bib.bib11)] | $\bullet$ |  | $\bullet$ |  | 使用 $\sigma=\{分段\ 多项式\}$
    的 FCNN | VC 维数 |'
- en: '|  | [[10](#bib.bib10)] | $\bullet$ |  | $\bullet$ |  | Networks with $\sigma=\{piecewise\
    linear\}$ | VC dimension |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | [[10](#bib.bib10)] | $\bullet$ |  | $\bullet$ |  | 网络的 $\sigma=\{分段\ 线性\}$
    | VC 维数 |'
- en: '|  | [[9](#bib.bib9)] | $\bullet$ |  | $\bullet$ |  | Networks with $\sigma=\{ReLU\}$
    | Rademacher complexity |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | [[9](#bib.bib9)] | $\bullet$ |  | $\bullet$ |  | 网络的 $\sigma=\{ReLU\}$
    | Rademacher 复杂度 |'
- en: '|  |  | [[77](#bib.bib77)] | $\bullet$ |  | $\bullet$ |  | Two-layer FCNN with
    $\sigma=\{ReLU\}$ | Rademacher complexity |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[77](#bib.bib77)] | $\bullet$ |  | $\bullet$ |  | 两层 FCNN，$\sigma=\{ReLU\}$
    | Rademacher 复杂度 |'
- en: '| Effective Complexity | Effective Complexity Measure | [[89](#bib.bib89)]
    | $\bullet$ |  | $\bullet$ |  | Networks with $\sigma=\{piecewise\ linear\}$ |
    #linear regions, trajectory length |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 有效复杂度 | 有效复杂度度量 | [[89](#bib.bib89)] | $\bullet$ |  | $\bullet$ |  | 网络的
    $\sigma=\{分段\ 线性\}$ | #线性区域，轨迹长度 |'
- en: '|  | [[81](#bib.bib81)] | $\bullet$ |  | $\bullet$ |  | Networks with $\sigma=\{piecewise\
    linear\}$ | Jacobian norm, trajectory length |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | [[81](#bib.bib81)] | $\bullet$ |  | $\bullet$ |  | 网络的 $\sigma=\{分段\ 线性\}$
    | 雅可比范数，轨迹长度 |'
- en: '|  | [[44](#bib.bib44)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma(z)=\{curve\
    activation\}$ | #approximated linear regions |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | [[44](#bib.bib44)] | $\bullet$ |  | $\bullet$ |  | 使用 $\sigma(z)=\{曲线\
    激活\}$ 的 FCNN | #近似线性区域 |'
- en: '|  | [[60](#bib.bib60)] | $\bullet$ |  | $\bullet$ |  | FCNN with $\sigma(z)=\sigma^{\prime}(z)z$
    | Fisher-Rao norm |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | [[60](#bib.bib60)] | $\bullet$ |  | $\bullet$ |  | 使用 $\sigma(z)=\sigma^{\prime}(z)z$
    的 FCNN | Fisher-Rao 范数 |'
- en: '|  | [[74](#bib.bib74)] |  | $\bullet$ | $\bullet$ |  | Any deep models | max
    #samples to zero training error |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | [[74](#bib.bib74)] |  | $\bullet$ | $\bullet$ |  | 任何深度模型 | 最大 #样本到零训练误差
    |'
- en: '| High-Capacity Low-Reality Phenomenon | [[37](#bib.bib37)] | $\bullet$ |  |
    $\bullet$ |  | FCNN with $\sigma(z)=\{ReLU\}$ | #linear regions, volumn of region
    boundaries |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 高容量低现实现象 | [[37](#bib.bib37)] | $\bullet$ |  | $\bullet$ |  | 使用 $\sigma(z)=\{ReLU\}$
    的全连接前馈神经网络（FCNN） | #线性区域，区域边界的体积 |'
- en: '2.3 Existing Studies on Deep Learning Model Complexity: An Overview'
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度学习模型复杂度的现有研究：概述
- en: The literature on deep model complexity can be categorized from two different
    angles. The first angle focuses on whether an approach is model-specific or cross-model.
    The second angle focuses on whether an approach develops an explicit complexity
    measure. These two angles are applicable to both expressive capacity and effective
    complexity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度模型复杂度的文献可以从两个不同的角度进行分类。第一个角度关注方法是模型特定还是跨模型。第二个角度关注方法是否发展了明确的复杂度度量。这两个角度适用于表达能力和有效复杂度。
- en: 2.3.1 Model-Specific versus Cross-Model
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 模型特定 versus 跨模型
- en: Based on whether an approach focuses on one type of models or crossing multiple
    types of models, the existing studies on model complexity can be divided into
    two groups, model-specific approaches and cross-model approaches.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方法是否专注于一种类型的模型或跨越多种类型的模型，现有的模型复杂度研究可以分为两组：模型特定方法和跨模型方法。
- en: A model-specific approach focuses on a certain type of models, and explores
    complexity based on structural characteristics. For example, Bianchini et al. [[14](#bib.bib14),
    [15](#bib.bib15)] and Hanin et al. [[37](#bib.bib37)] study model complexity of
    fully-connected feedforward neural networks (FCNNs for short), Bengio and Delalleau [[13](#bib.bib13),
    [30](#bib.bib30)] focus on model complexity of sum-product networks. Moreover,
    some studies further propose constraints on the activation functions to constrain
    the nonlinearity properties. For example, Liang et al. [[60](#bib.bib60)] assume
    that activation functions $\sigma(\cdot)$ satisfy $\sigma(z)=\sigma^{\prime}(z)z$.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型特定方法专注于某种类型的模型，并基于结构特征探索复杂度。例如，Bianchini 等人 [[14](#bib.bib14), [15](#bib.bib15)]
    和 Hanin 等人 [[37](#bib.bib37)] 研究了全连接前馈神经网络（简称 FCNN）的模型复杂度，Bengio 和 Delalleau [[13](#bib.bib13),
    [30](#bib.bib30)] 关注于和求和-乘积网络的模型复杂度。此外，一些研究进一步提出了对激活函数的约束，以限制非线性特性。例如，Liang 等人
    [[60](#bib.bib60)] 假设激活函数 $\sigma(\cdot)$ 满足 $\sigma(z)=\sigma^{\prime}(z)z$。
- en: An approach is cross-model when it covers multiple types of models rather than
    one specific type of models, and thus can be applied to compare two or more different
    types of models. For example, Khrulkov et al. [[51](#bib.bib51)] compare the complexity
    of general recurrent neural networks (RNNs for short), convolutional neural networks
    (CNNs for short) and shallow FCNNs by building connections among these network
    architectures and tensor decompositions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个方法涵盖了多种类型的模型而非单一类型的模型，那么它就是跨模型的方法，因此可以用于比较两种或更多不同类型的模型。例如，Khrulkov 等 [[51](#bib.bib51)]
    通过在这些网络架构和张量分解之间建立联系来比较一般递归神经网络（简称 RNNs）、卷积神经网络（简称 CNNs）和浅层 FCNNs 的复杂性。
- en: 2.3.2 Measure-Based versus Reduction-Based
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 基于测量与基于降维
- en: According to whether an explicit measure is designed, the state-of-the-art model
    complexity approaches can be divided into two groups, measure-based approaches
    and reduction-based approaches.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 根据是否设计了明确的度量，最先进的模型复杂性方法可以分为两组：基于测量的方法和基于降维的方法。
- en: A deep model complexity study is measure-based if it defines an appropriate
    quantitative representation of model complexity. For example, the number of linear
    regions is used to represent the complexity of FCNNs with piecewise linear activation
    functions [[37](#bib.bib37), [70](#bib.bib70), [81](#bib.bib81), [89](#bib.bib89)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深度模型复杂性研究定义了适当的模型复杂性定量表示，则该研究为基于测量的。例如，线性区域的数量被用来表示具有分段线性激活函数的全连接神经网络（FCNNs）的复杂性 [[37](#bib.bib37),
    [70](#bib.bib70), [81](#bib.bib81), [89](#bib.bib89)]。
- en: A complexity approach is reduction-based if it investigates a model complexity
    problem by reducing deep networks to some known problems and functions, and does
    not define any explicit complexity measure. For example, Arora et al. [[3](#bib.bib3)]
    build connections between deep networks with ReLU activation functions and Lebesgue
    spaces. Khrulkov et al. [[51](#bib.bib51)] connect deep neural networks with several
    types of tensor decompositions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个复杂性方法通过将深度网络降至一些已知问题和函数来研究模型复杂性问题，并且不定义任何明确的复杂性度量，那么它就是基于降维的方法。例如，Arora
    等 [[3](#bib.bib3)] 建立了带有 ReLU 激活函数的深度网络与 Lebesgue 空间之间的联系。Khrulkov 等 [[51](#bib.bib51)]
    将深度神经网络与几种类型的张量分解联系起来。
- en: 3 Expressive Capacity of Deep Learning Models
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度学习模型的表现能力
- en: Expressive capacity of deep models is also known as representation capacity,
    expressive power, and complexity capacity [[60](#bib.bib60), [86](#bib.bib86)].
    It describes how well a deep learning model can approximate complex problems.
    Informally, the expressive capacity describes the upper bound of the complexity
    in a parametric family of models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型的表现能力也称为表示能力、表现力或复杂性能力 [[60](#bib.bib60), [86](#bib.bib86)]。它描述了深度学习模型在近似复杂问题方面的能力。非正式地，表现能力描述了参数化模型家族中的复杂性上限。
- en: Expressive capacity of deep learning models has been mainly explored from four
    aspects.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的表现能力主要从四个方面进行探讨。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depth efficiency analyzes how deep learning models gain performance (e.g., accuracy)
    from the depth of architectures.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度效率分析了深度学习模型如何通过架构的深度来提升性能（例如，准确性）。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Width efficiency analyzes how the widths of layers in deep learning models affect
    model expressive capacity.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 宽度效率分析了深度学习模型中层的宽度如何影响模型的表现能力。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Expressible functional space investigates the functions that can be expressed
    by a deep model with a specific framework and specified size using different parameters.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可表达的函数空间研究了使用不同参数的特定框架和指定大小的深度模型可以表达的函数。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VC Dimension and Rademacher Complexity are two classic measures of expressive
    capacity in machine learning.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VC 维度和 Rademacher 复杂性是机器学习中表现能力的两个经典度量。
- en: In this section, we review these four groups of studies.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了这四组研究。
- en: 3.1 Depth Efficiency
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度效率
- en: 'A series of recent studies demonstrate that deep architectures significantly
    outperform shallow ones [[13](#bib.bib13), [67](#bib.bib67)]. Depth efficiency [[64](#bib.bib64)],
    which is the effectiveness of depth of deep models, has attracted a lot of interest.
    Specifically, the studies on depth efficiency analyze why deep architectures can
    obtain good performance and measures the effects of model depth on expressive
    capacity. We divide the studies of depth efficiency into two subcategories: model
    reduction methods and expressive capacity measures.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列近期的研究表明，深度架构显著优于浅层架构 [[13](#bib.bib13), [67](#bib.bib67)]。深度效率 [[64](#bib.bib64)]，即深度模型的有效性，受到了广泛关注。具体来说，关于深度效率的研究分析了深度架构为何能够获得良好性能，并衡量了模型深度对表达能力的影响。我们将深度效率的研究分为两类：模型减少方法和表达能力测量。
- en: 3.1.1 Model Reduction
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 模型减少
- en: One way to study the expressive capacity of deep learning models is to reduce
    deep learning models to understandable problems and functions for analysis.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 研究深度学习模型的表达能力的一种方法是将深度学习模型减少为可理解的问题和函数进行分析。
- en: To investigate depth efficiency, one intuitive idea is to compare the representation
    efficiency between deep networks and shallow ones. Bengio and Delalleau [[13](#bib.bib13),
    [30](#bib.bib30)] investigate the depth efficiency problem on deep sum-product
    networks (SPN for short). An SPN consists of neurons computing either products
    or weighted sums of their inputs. They consider two families of functions built
    from deep sum-product networks. The first family of functions is $F=\cup_{n\geq
    4}F_{n}$, where $F_{n}$ is the family of functions represented by deep SPNs with
    $n=2^{k}$ inputs and depth $k$. The second family of functions is $G=\cup_{n\geq
    2,i\geq 0}G_{i,n}$, where $G_{i,n}$ is the family of functions represented by
    SPNs with $n$ inputs and depth $2i+1$.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究深度效率，一个直观的想法是比较深层网络和浅层网络之间的表示效率。Bengio 和 Delalleau [[13](#bib.bib13), [30](#bib.bib30)]
    研究了深度和产品网络（简称SPN）上的深度效率问题。一个SPN由计算输入的乘积或加权和的神经元组成。他们考虑了由深度和产品网络构建的两类函数。第一类函数是
    $F=\cup_{n\geq 4}F_{n}$，其中 $F_{n}$ 是由深度SPN表示的函数族，具有 $n=2^{k}$ 输入和深度 $k$。第二类函数是
    $G=\cup_{n\geq 2,i\geq 0}G_{i,n}$，其中 $G_{i,n}$ 是由具有 $n$ 个输入和深度 $2i+1$ 的SPN表示的函数族。
- en: Then, Bengio and Delalleau establish the lower bounds on the number of hidden
    neurons required by a shallow sum-product network to represent those families
    of functions. To approach a function $f\in F$, a one-hidden-layer shallow SPN
    needs at least $2^{\sqrt{n}-1}$ hidden neurons and at least $2^{\sqrt{n}-1}$ product
    neurons. Similarly, to approach a function $g\in G$, a one-hidden-layer shallow
    SPN needs at least $(n-1)^{i}$ hidden neurons. The comparison of deep and shallow
    sum-product networks representing the same function indicates that, to represent
    the same functions, the number of neurons in a shallow network has to grow exponentially
    but only a linear growth is needed for deep networks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Bengio 和 Delalleau 建立了浅层和产品网络表示这些函数族所需的隐藏神经元的下限。为了逼近函数 $f\in F$，一个具有一层隐藏层的浅层SPN至少需要
    $2^{\sqrt{n}-1}$ 个隐藏神经元和至少 $2^{\sqrt{n}-1}$ 个乘积神经元。类似地，为了逼近函数 $g\in G$，一个具有一层隐藏层的浅层SPN
    至少需要 $(n-1)^{i}$ 个隐藏神经元。深层和浅层和产品网络表示相同函数的比较表明，为了表示相同的函数，浅层网络中的神经元数量必须呈指数增长，而深层网络仅需线性增长。
- en: 'Mhaskar et al. [[67](#bib.bib67)] study functions representable by hierarchical
    binary tree networks, comparing with shallow networks. Fig. [1](#S3.F1 "Figure
    1 ‣ 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity of Deep
    Learning Models ‣ Model Complexity of Deep Learning: A Survey") demonstrates shallow
    networks and hierarchical binary tree networks. Denote by $S_{m}$ the class of
    shallow networks with $m$ neurons, in the form of'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mhaskar 等人 [[67](#bib.bib67)] 研究了由层次化二叉树网络表示的函数，并与浅层网络进行比较。图 [1](#S3.F1 "Figure
    1 ‣ 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity of Deep
    Learning Models ‣ Model Complexity of Deep Learning: A Survey") 展示了浅层网络和层次化二叉树网络。记
    $S_{m}$ 为具有 $m$ 个神经元的浅层网络，形式为'
- en: '|  | $x\rightarrow\sum_{k=1}^{m}a_{k}\sigma(w_{k}\cdot x+b_{k})$ |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $x\rightarrow\sum_{k=1}^{m}a_{k}\sigma(w_{k}\cdot x+b_{k})$ |  |'
- en: where $w_{k}\in\mathbb{R}^{d}$, $b_{k},a_{k}\in\mathbb{R}$ are the parameters
    for the $k$-th hidden neuron, $\sigma$ is the activation function. Denote by $W^{NN}_{r,d}$
    the class of functions with continuous partial derivatives of order $\leq r$ with
    certain assumptions (see [[67](#bib.bib67)] for detail). Correspondingly, a hierarchical
    binary tree network is a network with the structure
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$w_{k}\in\mathbb{R}^{d}$，$b_{k},a_{k}\in\mathbb{R}$是第$k$个隐藏神经元的参数，$\sigma$是激活函数。记$W^{NN}_{r,d}$为具有连续偏导数阶数$\leq
    r$的函数类，并附带某些假设（详见[[67](#bib.bib67)]）。相应地，分层二叉树网络是具有以下结构的网络
- en: '|  | $\begin{split}f(x_{1},\ldots,x_{d})=h_{l1}(&amp;h_{(l-1),1}(\ldots(h_{11}(x_{1},x_{2}),h_{12}(x_{3},x_{4}),\ldots),\\
    &amp;h_{(l-1),2}(\ldots))\end{split}$ |  | (1) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}f(x_{1},\ldots,x_{d})=h_{l1}(&amp;h_{(l-1),1}(\ldots(h_{11}(x_{1},x_{2}),h_{12}(x_{3},x_{4}),\ldots),\\
    &amp;h_{(l-1),2}(\ldots))\end{split}$ |  | (1) |'
- en: 'where each $h_{ij}$ is in $S_{m}$, $l$ is the depth of the network, $h_{l1}$
    corresponds to the root node of the tree structure. See Fig. [1](#S3.F1 "Figure
    1 ‣ 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity of Deep
    Learning Models ‣ Model Complexity of Deep Learning: A Survey") as an example
    of the hierarchical binary tree with depth$=3$. Denote by $D_{m}$ the class of
    hierarchical binary tree networks. Let $W^{NN}_{H,r,d}$ be the class of functions
    with the structure of Eq. ([1](#S3.E1 "In 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency
    ‣ 3 Expressive Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey")), where each $h_{ij}$ is in $W^{NN}_{r,2}$. Define by $\inf_{P\in V}||f-P||$
    the approximation degree of $V$ to function $f$, where $V=\{S_{m},D_{m}\}$.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个$h_{ij}$在$S_{m}$中，$l$是网络的深度，$h_{l1}$对应于树结构的根节点。参见图[1](#S3.F1 "图1 ‣ 3.1.1模型缩减
    ‣ 3.1深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型复杂性：综述")作为深度$=3$的分层二叉树的示例。记$D_{m}$为分层二叉树网络的类。令$W^{NN}_{H,r,d}$为公式([1](#S3.E1
    "在3.1.1模型缩减 ‣ 3.1深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型复杂性：综述"))中结构的函数类，其中每个$h_{ij}$在$W^{NN}_{r,2}$中。定义$\inf_{P\in
    V}||f-P||$为$V$对函数$f$的近似精度，其中$V=\{S_{m},D_{m}\}$。
- en: '![Refer to caption](img/df317fc9337ad3acedc8f7dcc39e9359.png)![Refer to caption](img/0dddaa11b558b6bc47b03684e2934510.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df317fc9337ad3acedc8f7dcc39e9359.png)![参见说明](img/0dddaa11b558b6bc47b03684e2934510.png)'
- en: 'Figure 1: 8 input dimensions fed into a shallow network (a) and a hierarchical
    binary tree network (b), studied by Mhaskar et al. [[67](#bib.bib67)].'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：8个输入维度输入到一个浅层网络（a）和一个分层二叉树网络（b），由Mhaskar等人[[67](#bib.bib67)]研究。
- en: 'Mhaskar et al. [[67](#bib.bib67)] demonstrate that, to approximate a function
    $f\in W^{NN}_{r,d}$ to approximation degree $\epsilon$, a shallow network in $S_{m}$
    requires $O(\epsilon^{-d/r})$ trainable parameters. Meanwhile, to approximate
    a function $f\in W^{NN}_{H,r,d}$ to the same approximation degree, a network in
    $D_{m}$ requires only $O(\epsilon^{-2/r})$ trainable parameters. Then, Mhaskar et
    al. [[67](#bib.bib67)] compare shallow Gaussian networks with hierarchical binary
    tree structures (Eq. ([1](#S3.E1 "In 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency
    ‣ 3 Expressive Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey"))) where each $h_{ij}$ computes a shallow Gaussian network, and obtain
    a similar conclusion. They demonstrate that functions with a designed compositional
    structure can be approximated by both deep and shallow networks to the same degree
    of approximation. However, the numbers of parameters of deep networks are much
    less than those of shallow networks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Mhaskar等人[[67](#bib.bib67)]展示了，为了将一个函数$f\in W^{NN}_{r,d}$近似到精度$\epsilon$，在$S_{m}$中的一个浅层网络需要$O(\epsilon^{-d/r})$个可训练参数。同时，为了将一个函数$f\in
    W^{NN}_{H,r,d}$近似到相同的精度，在$D_{m}$中的一个网络只需要$O(\epsilon^{-2/r})$个可训练参数。然后，Mhaskar等人[[67](#bib.bib67)]比较了浅层高斯网络与分层二叉树结构（公式([1](#S3.E1
    "在3.1.1模型缩减 ‣ 3.1深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型复杂性：综述")）），其中每个$h_{ij}$计算一个浅层高斯网络，并得出类似结论。他们证明了具有设计的组合结构的函数可以通过深层和浅层网络以相同的精度进行近似。然而，深层网络的参数数量远少于浅层网络。
- en: Arora et al. [[3](#bib.bib3)] investigate the importance of depth on deep neural
    networks with ReLU activation function. First, they investigate neural networks
    with one-dimensional input and one-dimensional output. They prove that given any
    natural numbers $k\geq 1$ and $w\geq 2$, there exists a family of functions that
    can be represented by a ReLU neural network with $k$ hidden layers each of width
    $w$. However, to represent this family of functions, a network with $k^{\prime}<k$
    hidden layers requires at least $\frac{1}{2}k^{\prime}w^{\frac{k}{k^{\prime}}}-1$
    hidden neurons. Then, they investigate ReLU neural networks with $d$ input dimensions.
    They prove that, given natural numbers $k,m,d\geq 1$ and $w\geq 2$, there exists
    a family of $\mathbb{R}^{d}\rightarrow\mathbb{R}$ functions that can be represented
    by a ReLU network with $k+1$ hidden layers and $2m+wk$ neurons. This family of
    functions is constructed using the zonotope theory from the polyhedral theory [[102](#bib.bib102)].
    However, to represent this family of functions, the minimum number of hidden neurons
    that a ReLU neural network with $k^{\prime}\leq k$ hidden layers requires is
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Arora 等 [[3](#bib.bib3)] 研究了深度神经网络中深度的重要性，使用ReLU激活函数。首先，他们研究了具有一维输入和一维输出的神经网络。他们证明了，对于任意自然数
    $k\geq 1$ 和 $w\geq 2$，存在一个函数族可以用具有 $k$ 个隐层、每层宽度为 $w$ 的ReLU神经网络表示。然而，要表示这个函数族，具有
    $k^{\prime}<k$ 个隐层的网络需要至少 $\frac{1}{2}k^{\prime}w^{\frac{k}{k^{\prime}}}-1$ 个隐层神经元。接着，他们研究了具有
    $d$ 个输入维度的ReLU神经网络。他们证明了，给定自然数 $k,m,d\geq 1$ 和 $w\geq 2$，存在一个 $\mathbb{R}^{d}\rightarrow\mathbb{R}$
    的函数族，可以用具有 $k+1$ 个隐层和 $2m+wk$ 个神经元的ReLU网络表示。这个函数族是利用来自多面体理论的zonotope理论 [[102](#bib.bib102)]
    构造的。然而，要表示这个函数族，具有 $k^{\prime}\leq k$ 个隐层的ReLU神经网络所需的最小隐层神经元数量是
- en: '|  | $\max\{\frac{1}{2}(k^{\prime}w^{\frac{k}{k^{\prime}d}})(m-1)^{(1-\frac{1}{d})\frac{1}{k^{\prime}}}-1,k^{\prime}(\frac{w^{\frac{k}{k^{\prime}}}}{d^{\frac{1}{k^{\prime}}}})\}.$
    |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max\{\frac{1}{2}(k^{\prime}w^{\frac{k}{k^{\prime}d}})(m-1)^{(1-\frac{1}{d})\frac{1}{k^{\prime}}}-1,k^{\prime}(\frac{w^{\frac{k}{k^{\prime}}}}{d^{\frac{1}{k^{\prime}}}})\}.$
    |  |'
- en: To investigate when deep neural networks are provably more efficient than shallow
    ones, Kuurkova [[54](#bib.bib54)] analyzes the limitation of expressive capacity
    of shallow neural networks with Signum activation function. The Signum activation
    function is defined as
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究深度神经网络在什么情况下比浅层网络更高效，Kuurkova [[54](#bib.bib54)] 分析了浅层神经网络在Signum激活函数下的表达能力的限制。Signum激活函数定义为
- en: '|  | $sgn(z)=\left\{\begin{array}[]{rcl}-1&amp;\quad\mbox{for}&amp;z<0\\ 1&amp;\quad\mbox{for}&amp;z\geq
    0\end{array}\right.$ |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $sgn(z)=\left\{\begin{array}[]{rcl}-1\quad\mbox{当}&amp;z<0\\ 1\quad\mbox{当}&amp;z\geq
    0\end{array}\right.$ |  |'
- en: Kuurkova proves that there exist functions that cannot be $L^{1}$ sparsely represented
    by one-hidden-layer Signum neural networks, which have a limited number of neurons
    and a limited sum of absolute values of output weights (i.e., $L^{1}$-norm). Such
    functions should be nearly orthogonal to any function $f$ from the class of Signum
    perceptrons $\{sgn(vx+b):X\rightarrow\{-1,1\}|v\in\mathbb{R}^{d},b\in\mathbb{R}\}$.
    The functions generated by Hadamard matrices are such examples. A Hadamard matrix
    of order $n$ is a $n\times n$ matrix with entries in $\{-1,1\}$ such that any
    two distinct rows or columns are orthogonal. Kuurkova [[54](#bib.bib54)] proves
    that the functions induced by $n\times n$ Hadamard matrices cannot be computed
    by shallow Signum networks with both the number of hidden neurons and the sum
    of absolute values of output weights smaller than $\frac{\sqrt{n}}{\lceil\log_{2}n\rceil}$.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Kuurkova 证明了存在无法被单隐层Signum神经网络$L^{1}$ 稀疏表示的函数，这些网络具有有限数量的神经元和有限的输出权重绝对值之和（即
    $L^{1}$-范数）。这样的函数应与来自Signum感知机类 $\{sgn(vx+b):X\rightarrow\{-1,1\}|v\in\mathbb{R}^{d},b\in\mathbb{R}\}$
    的任何函数几乎正交。Hadamard矩阵生成的函数就是这样的例子。一个阶数为$n$的Hadamard矩阵是一个 $n\times n$ 的矩阵，矩阵条目在
    $\{-1,1\}$ 中，使得任何两个不同的行或列都是正交的。Kuurkova [[54](#bib.bib54)] 证明了由 $n\times n$ Hadamard矩阵诱导的函数无法被浅层Signum网络计算，这些网络的隐层神经元数量和输出权重绝对值之和都小于
    $\frac{\sqrt{n}}{\lceil\log_{2}n\rceil}$。
- en: To further illustrate the limitation of shallow networks, Kuurkova [[54](#bib.bib54)]
    compares the representation capability of one and two hidden layer networks with
    Heaviside activation function. The Heaviside activation function is defined as
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明浅层网络的局限性，Kuurkova [[54](#bib.bib54)] 比较了具有Heaviside激活函数的单隐层和双隐层网络的表示能力。Heaviside激活函数定义为
- en: '|  | $\sigma(z)=\left\{\begin{array}[]{rcl}0&amp;\quad\mbox{for}&amp;z<0\\
    1&amp;\quad\mbox{for}&amp;z\geq 0\end{array}\right.$ |  | (2) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma(z)=\left\{\begin{array}[]{rcl}0&amp;\quad\mbox{对于}&amp;z<0\\ 1&amp;\quad\mbox{对于}&amp;z\geq
    0\end{array}\right.$ |  | (2) |'
- en: Let $S(k)$ be a $2^{k}\times 2^{k}$ Sylvester-Hadamard matrix, which is constructed
    by starting from <math   alttext="S(2)=\left(\begin{array}[]{rcl}1&amp;1\\
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $S(k)$ 为一个 $2^{k}\times 2^{k}$ 的 Sylvester-Hadamard 矩阵，其构造方式是从
- en: 1&amp;-1\end{array}\right)" display="inline"><semantics ><mrow ><mrow ><mi  >S</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mn >2</mn><mo
    stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo >(</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd  columnalign="right" ><mn >1</mn></mtd><mtd ><mn  >1</mn></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mn >1</mn></mtd><mtd ><mrow ><mo >−</mo><mn >1</mn></mrow></mtd></mtr></mtable><mo
    >)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply ><apply  ><ci
    >𝑆</ci><cn type="integer"  >2</cn></apply><matrix ><matrixrow ><cn type="integer"
    >1</cn><cn type="integer"  >1</cn><cerror ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    ><cn type="integer" >1</cn><apply ><cn type="integer"  >1</cn></apply><cerror
    ><csymbol cd="ambiguous"  >missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></annotation-xml><annotation
    encoding="application/x-tex" >S(2)=\left(\begin{array}[]{rcl}1&1\\ 1&-1\end{array}\right)</annotation></semantics></math>
    and then recursively iterating $S(l+1)=S(2)\otimes S(l)$. Kuurkova [[54](#bib.bib54)]
    shows that, to represent a function induced by $S(k)$, a two-hidden-layer Heaviside
    network requires $k$ neurons in each hidden layer. However, to represent such
    a function, a one-hidden-layer Heaviside network requires at least $\frac{2^{k}}{k}$
    hidden neurons, or the absolute value of some output weights is no less than $\frac{2^{k}}{k}$.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先递归迭代 $S(l+1)=S(2)\otimes S(l)$。Kuurkova [[54](#bib.bib54)] 表明，为了表示由 $S(k)$
    诱导的函数，具有两个隐藏层的 Heaviside 网络在每个隐藏层中需要 $k$ 个神经元。然而，为了表示这样的函数，具有一个隐藏层的 Heaviside
    网络至少需要 $\frac{2^{k}}{k}$ 个隐藏神经元，或者某些输出权重的绝对值不小于 $\frac{2^{k}}{k}$。
- en: In summary, the model reduction approaches reduce neural networks to some kind
    of functions [[3](#bib.bib3), [13](#bib.bib13), [54](#bib.bib54), [67](#bib.bib67)],
    and investigate the effects of model depth on the capacity to express function
    families.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，模型简化方法将神经网络降至某种函数[[3](#bib.bib3), [13](#bib.bib13), [54](#bib.bib54), [67](#bib.bib67)]，并研究模型深度对表达函数家族能力的影响。
- en: 3.1.2 Expressive Capacity Measures
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 表达能力度量
- en: To investigate depth efficiency, another idea is to develop an appropriate measure
    of expressive capacity and study how expressive capacity changes when the depth
    and layer width of a model increase.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究深度效率，另一种思路是制定一个适当的表达能力度量，并研究模型的深度和层宽增加时表达能力的变化。
- en: 'Montufar et al. [[70](#bib.bib70)] focus on fully-connected feed forward neural
    networks (FCNNs) with piecewise linear activation functions (e.g., ReLU [[73](#bib.bib73)]
    and Maxout [[35](#bib.bib35)]), and propose to use the number of linear regions
    as a representation of model complexity (Fig. [2](#S3.F2 "Figure 2 ‣ 3.1.2 Expressive
    Capacity Measures ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity of Deep Learning
    Models ‣ Model Complexity of Deep Learning: A Survey")). The basic idea is that
    an FCNN with piecewise linear activation functions divides the input space into
    a large number of linear regions. Each linear region corresponds to a linear function.
    The number of linear regions can reflect the flexibility and complexity of the
    model.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Montufar等人[[70](#bib.bib70)]关注具有分段线性激活函数（例如ReLU[[73](#bib.bib73)]和Maxout[[35](#bib.bib35)]）的全连接前馈神经网络（FCNNs），并提出使用线性区域的数量作为模型复杂性的表示（图[2](#S3.F2
    "图 2 ‣ 3.1.2 表达能力度量 ‣ 3.1 深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型复杂性：综述")）。基本的观点是，具有分段线性激活函数的FCNN将输入空间划分为大量的线性区域。每个线性区域对应一个线性函数。线性区域的数量可以反映模型的灵活性和复杂性。
- en: '![Refer to caption](img/bc53cf7e720fa49661f712dde2583bab.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/bc53cf7e720fa49661f712dde2583bab.png)'
- en: 'Figure 2: An example from Montufar et al. [[70](#bib.bib70)] illustrating the
    advantage of deep models. A deep ReLU network (the dotted line) captures the boundary
    more accurately by approximating it with a larger number of linear regions than
    a shallow one (the solid line).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: Montufar等人[[70](#bib.bib70)]的一个例子，展示了深度模型的优势。深度ReLU网络（虚线）通过用更多的线性区域来逼近边界，比浅层网络（实线）更准确地捕捉边界。'
- en: 'Montufar et al. [[70](#bib.bib70)] investigate FCNNs with two types of piecewise
    linear activation functions: ReLU [[73](#bib.bib73)] and Maxout [[35](#bib.bib35)].
    They prove that, under certain parameter settings, the model expressive capacity
    of a ReLU network $\mathcal{N}$, which is represented by the maximum number of
    linear regions and denoted by $MEC(\mathcal{N})$, is bounded by'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Montufar等人[[70](#bib.bib70)]研究了两种类型的分段线性激活函数的FCNN：ReLU[[73](#bib.bib73)]和Maxout[[35](#bib.bib35)]。他们证明了，在某些参数设置下，ReLU网络$\mathcal{N}$的模型表达能力，表示为线性区域的最大数量并记作$MEC(\mathcal{N})$，是由下式界定的：
- en: '|  | $MEC(\mathcal{N})\geq(\prod_{i=1}^{l-1}\lfloor\frac{m_{i}}{m_{0}}\rfloor^{m_{0}})\sum_{j=0}^{m_{0}}\tbinom{m_{l}}{j}$
    |  | (3) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | $MEC(\mathcal{N})\geq(\prod_{i=1}^{l-1}\lfloor\frac{m_{i}}{m_{0}}\rfloor^{m_{0}})\sum_{j=0}^{m_{0}}\tbinom{m_{l}}{j}$
    |  | (3) |'
- en: where $l$ is the number of hidden layers, $m_{i}$ is the width of $i$-th hidden
    layer, $m_{0}=d$ is the dimensionality of the input. Based on this bound, they
    show that a ReLU network with $l$ hidden layers and layer width $m_{i}\geq m_{0}$
    is able to approximate any piecewise linear function that has $\Omega((\frac{m}{m_{0}})^{(l-1)m_{0}}m^{m_{0}})$
    linear regions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$l$是隐藏层的数量，$m_{i}$是第$i$个隐藏层的宽度，$m_{0}=d$是输入的维度。基于这一界限，他们展示了一个具有$l$个隐藏层且每层宽度为$m_{i}\geq
    m_{0}$的ReLU网络能够逼近任何具有$\Omega((\frac{m}{m_{0}})^{(l-1)m_{0}}m^{m_{0}})$线性区域的分段线性函数。
- en: Montufar et al. [[70](#bib.bib70)] also prove that, for the rank-$k$ Maxout
    activation function, the expressive capacity of a one-hidden-layer Maxout network
    with $m$ neurons is bounded by $MEC(\mathcal{N})\geq k^{\min(d,m)}$ and $MEC(\mathcal{N})\leq\min\{\sum_{j=1}^{d}\tbinom{k^{2}m}{j},\
    k^{m}\}$. A rank-$k$ Maxout network, which consists of $l$ hidden layers and the
    width of each layer equals $m$, can compute any piecewise linear function with
    $\Omega(k^{l-1}k^{m})$ linear regions. In conclusion, the maximum number of linear
    regions generated by a FCNN with piecewise linear activation functions increases
    exponentially with model depth.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Montufar等人[[70](#bib.bib70)]还证明，对于秩为$k$的Maxout激活函数，具有$m$个神经元的单隐藏层Maxout网络的表达能力由$MEC(\mathcal{N})\geq
    k^{\min(d,m)}$和$MEC(\mathcal{N})\leq\min\{\sum_{j=1}^{d}\tbinom{k^{2}m}{j},\ k^{m}\}$来界定。一个秩为$k$的Maxout网络，由$l$个隐藏层组成，每层的宽度为$m$，可以计算任何具有$\Omega(k^{l-1}k^{m})$线性区域的分段线性函数。总之，具有分段线性激活函数的全连接神经网络（FCNN）生成的线性区域的最大数量随着模型深度的增加而指数增长。
- en: Montufar et al. [[70](#bib.bib70)] provide an explanation for the depth efficiency.
    They suggest that the intermediary layer of a deep model is able to map several
    pieces of the inputs into the same output. As the number of layers increases,
    the layer-wise compositions of the functions re-use lower-level computation exponentially.
    This allows deep models to compute highly complex functions, even with relatively
    fewer parameters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Montufar 等人 [[70](#bib.bib70)] 提供了深度效率的解释。他们认为，深度模型的中间层能够将多个输入映射到相同的输出。随着层数的增加，层级函数的组合会以指数级重复利用较低层次的计算。这使得深度模型即使在参数相对较少的情况下，也能计算出高度复杂的函数。
- en: 'Serra et al. [[92](#bib.bib92)] improve the bounds of the maximum number of
    linear regions proposed by Montufar et al. [[70](#bib.bib70)] (Eq. ([3](#S3.E3
    "In 3.1.2 Expressive Capacity Measures ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity
    of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey"))). Given
    a deep ReLU neural network with $l$ layers, let $m_{i}$ be the width of $i$-th
    hidden layer with $m_{i}\geq 3d$, $d$ is the input dimension. Serra et al. [[92](#bib.bib92)]
    prove that the maximal number of linear regions of this neural network is lower
    bounded by'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Serra 等人 [[92](#bib.bib92)] 改进了 Montufar 等人 [[70](#bib.bib70)] 提出的线性区域最大数量的界限（见公式
    ([3](#S3.E3 "在 3.1.2 表达能力度量 ‣ 3.1 深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型的复杂性：一项调查"))）。给定一个具有
    $l$ 层的深度 ReLU 神经网络，设 $m_{i}$ 为第 $i$ 层隐藏层的宽度且 $m_{i}\geq 3d$，其中 $d$ 为输入维度。Serra
    等人 [[92](#bib.bib92)] 证明了该神经网络的线性区域的最大数量有下界为
- en: '|  | $MEC(\mathcal{N})\geq(\prod_{i=1}^{l-1}(\lfloor\frac{m_{i}}{d}\rfloor+1)^{d})\sum_{j=0}^{d}\tbinom{m_{l}}{j}$
    |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $MEC(\mathcal{N})\geq(\prod_{i=1}^{l-1}(\lfloor\frac{m_{i}}{d}\rfloor+1)^{d})\sum_{j=0}^{d}\tbinom{m_{l}}{j}$
    |  |'
- en: and is upper bounded by
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并且上界为
- en: '|  | $MEC(\mathcal{N})\leq\sum_{(j_{1},\ldots,j_{l+1})\in J}\prod_{i=1}^{l+1}\tbinom{m_{i}}{j_{i}}$
    |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $MEC(\mathcal{N})\leq\sum_{(j_{1},\ldots,j_{l+1})\in J}\prod_{i=1}^{l+1}\tbinom{m_{i}}{j_{i}}$
    |  |'
- en: where $J=\{(j_{1},\ldots,j_{l+1})\in\mathbb{Z}^{l+1}:0\leq j_{i}\leq\min\{d,m_{1}-j_{1},\ldots,m_{i-1}-j_{i-1},m_{i}\}\}$.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $J=\{(j_{1},\ldots,j_{l+1})\in\mathbb{Z}^{l+1}:0\leq j_{i}\leq\min\{d,m_{1}-j_{1},\ldots,m_{i-1}-j_{i-1},m_{i}\}\}$。
- en: Bianchini and Scarselli [[14](#bib.bib14), [15](#bib.bib15)] design a topological
    measure of model complexity for deep neural networks. Given an FCNN with single
    output, denoted by $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}$, they define
    $S=\{x\in\mathbb{R}^{d}|\mathcal{N}(x)\geq 0\}$ the set of instances being classified
    by $\mathcal{N}$ to the positive class. The model complexity of neural network
    $\mathcal{N}$ is measured by $B(S)$, the sum of the Betti numbers of set $S$,
    that is,
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Bianchini 和 Scarselli [[14](#bib.bib14), [15](#bib.bib15)] 设计了一个用于深度神经网络模型复杂性的拓扑度量。给定一个具有单输出的全连接神经网络（FCNN），记作
    $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}$，他们定义 $S=\{x\in\mathbb{R}^{d}|\mathcal{N}(x)\geq
    0\}$ 为被 $\mathcal{N}$ 分类为正类的实例集合。神经网络 $\mathcal{N}$ 的模型复杂性通过 $B(S)$ 来度量，$B(S)$
    是集合 $S$ 的 Betti 数之和，即：
- en: '|  | $MEC(\mathcal{N})=B(S)=\sum_{i=0}^{d-1}b_{i}(S)$ |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $MEC(\mathcal{N})=B(S)=\sum_{i=0}^{d-1}b_{i}(S)$ |  |'
- en: where $b_{i}(S)$ denotes the $i$-th Betti number that counts the number of $(i+1)$-dimensional
    holes in $S$. The Betti number is generally used to distinguish between spaces
    with different characteristics in algebraic topology [[18](#bib.bib18)]. $S$ contains
    instances positively classified by the network $\mathcal{N}$. Therefore, $B(S)$
    can be used to investigate how $S$ is affected by the architecture of $\mathcal{N}$
    and can represent the model complexity.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{i}(S)$ 表示第 $i$ 个 Betti 数，用于计算 $S$ 中 $(i+1)$ 维孔洞的数量。Betti 数通常用于区分代数拓扑学中具有不同特征的空间
    [[18](#bib.bib18)]。$S$ 包含网络 $\mathcal{N}$ 正确分类的实例。因此，$B(S)$ 可以用来研究 $S$ 如何受到 $\mathcal{N}$
    架构的影响，并可以代表模型的复杂性。
- en: 'Bianchini and Scarselli [[14](#bib.bib14), [15](#bib.bib15)] report upper and
    lower bounds of $B(S)$ for a series of network architectures (Table [3](#S3.T3
    "Table 3 ‣ 3.1.2 Expressive Capacity Measures ‣ 3.1 Depth Efficiency ‣ 3 Expressive
    Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey")).
    In particular, Bianchini and Scarselli [[14](#bib.bib14), [15](#bib.bib15)] demonstrate
    that $B(S)$ of a single-hidden-layer network grows polynomially with respect to
    the hidden layer width. That is, $B(S)\in O(m^{d})$, where $m$ is the width of
    the hidden layer. They also prove that $B(S)$ of a deep neural network grows exponentially
    in the total number of hidden neurons. That is, $B(S)\in O(2^{M})$, where $M$
    is the total number of hidden neurons. This indicates that deep neural networks
    have higher expressive capacity, and therefore are able to learn more complex
    functions than shallow ones.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Bianchini 和 Scarselli [[14](#bib.bib14), [15](#bib.bib15)] 报告了 $B(S)$ 对一系列网络架构的上界和下界（表
    [3](#S3.T3 "表 3 ‣ 3.1.2 表达能力度量 ‣ 3.1 深度效率 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型的复杂度：调查")）。特别是，Bianchini
    和 Scarselli [[14](#bib.bib14), [15](#bib.bib15)] 证明了单隐藏层网络的 $B(S)$ 随着隐藏层宽度的多项式增长。也就是说，$B(S)\in
    O(m^{d})$，其中 $m$ 是隐藏层的宽度。他们还证明了深度神经网络的 $B(S)$ 随着隐藏神经元总数的指数增长。也就是说，$B(S)\in O(2^{M})$，其中
    $M$ 是隐藏神经元的总数。这表明深度神经网络具有更高的表达能力，因此能够学习比浅层网络更复杂的函数。
- en: 'Table 3: Upper and lower bounds of $B(S)$ given by Bianchini and Scarselli [[15](#bib.bib15),
    [14](#bib.bib14)], for networks with a number of $M$ hidden units, a number of
    $d$ inputs and a number of $l$ hidden layers.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：Bianchini 和 Scarselli [[15](#bib.bib15), [14](#bib.bib14)] 给出的 $B(S)$ 的上界和下界，适用于具有
    $M$ 个隐藏单元、$d$ 个输入和 $l$ 个隐藏层的网络。
- en: '| #Inputs | #Hidden Layers | Activation Function | Bound of $B(S)$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| #输入 | #隐藏层 | 激活函数 | $B(S)$ 的界限 |'
- en: '| Upper Bounds of $B(S)$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $B(S)$ 的上界 |'
- en: '| d | 1 | threshold | $O(M^{d})$ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| d | 1 | 阈值 | $O(M^{d})$ |'
- en: '| d | 1 | arctan | $O((d+M)^{d+2})$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| d | 1 | 反正切 | $O((d+M)^{d+2})$ |'
- en: '| d | 1 | polynomial, degree $r$ | $\frac{1}{2}(2+r)(1+r)^{d-1}$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| d | 1 | 多项式，次数 $r$ | $\frac{1}{2}(2+r)(1+r)^{d-1}$ |'
- en: '| 1 | 1 | arctan | $M$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 反正切 | $M$ |'
- en: '| d | many | arctan | $2^{M(2M-1)}O((dl+d)^{d+2M})$ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| d | 多个 | 反正切 | $2^{M(2M-1)}O((dl+d)^{d+2M})$ |'
- en: '| d | many | tanh | $2^{M(M-1)/2}O((dl+d)^{d+M})$ |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| d | 多个 | tanh | $2^{M(M-1)/2}O((dl+d)^{d+M})$ |'
- en: '| d | many | polynomial, degree $r$ | $\frac{1}{2}(2+r^{l})(1+r^{l})^{d-1}$
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| d | 多个 | 多项式，次数 $r$ | $\frac{1}{2}(2+r^{l})(1+r^{l})^{d-1}$ |'
- en: '| Lower Bounds of $B(S)$ |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| $B(S)$ 的下界 |'
- en: '| d | 1 | any sigmoid | $(\frac{M-1}{d})^{d}$ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| d | 1 | 任意 sigmoid | $(\frac{M-1}{d})^{d}$ |'
- en: '| d | many | any sigmoid | $2^{l-1}$ |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| d | 多个 | 任意 sigmoid | $2^{l-1}$ |'
- en: '| d | many | polynomial, degree $r\geq 2$ | $2^{l-1}$ |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| d | 多个 | 多项式，次数 $r\geq 2$ | $2^{l-1}$ |'
- en: Bianchini and Scarselli [[14](#bib.bib14), [15](#bib.bib15)] suggest that the
    layer-wise composition mechanism of a deep model allows the model to replicate
    the same behavior in different regions of the input space, thus makes depth more
    effective than width.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Bianchini 和 Scarselli [[14](#bib.bib14), [15](#bib.bib15)] 建议，深度模型的层级组合机制使模型能够在输入空间的不同区域重复相同的行为，从而使深度比宽度更有效。
- en: 3.2 Width Efficiency
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 宽度效率
- en: In addition to depth efficiency, the effect of width on expressive capacity,
    namely width efficiency, is also worth exploring. Width efficiency analyzes how
    width affects the expressive capacity of deep learning models [[64](#bib.bib64)].
    Width efficiency is important for fully understanding expressive capacity and
    helps to validate the insights obtained from depth efficiency [[64](#bib.bib64)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了深度效率外，宽度对表达能力的影响，即宽度效率，也值得探讨。宽度效率分析了宽度如何影响深度学习模型的表达能力 [[64](#bib.bib64)]。宽度效率对于全面理解表达能力很重要，并有助于验证从深度效率中获得的见解 [[64](#bib.bib64)]。
- en: Lu et al. [[64](#bib.bib64)] investigate the width efficiency of neural networks
    with ReLU activation function. They extend the universal approximation theorem [[7](#bib.bib7),
    [43](#bib.bib43)] to width-bounded deep ReLU neural networks. The classical universal
    approximation theorem [[7](#bib.bib7), [43](#bib.bib43)] states that, one-hidden-layer
    neural networks with certain activation functions (e.g., ReLU) can approximate
    any continuous functions on a compact domain to any desired accuracy performance.
    Lu et al. [[64](#bib.bib64)] show that, for any Lebesgue-integrable function $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$
    and any $\epsilon>0$, there exists a ReLU network $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}$
    that can approximate $f$ to $\epsilon$ $L^{1}$-distance. That is,
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Lu 等人 [[64](#bib.bib64)] 研究了具有 ReLU 激活函数的神经网络的宽度效率。他们将通用逼近定理 [[7](#bib.bib7),
    [43](#bib.bib43)] 扩展到宽度受限的深度 ReLU 神经网络。经典的通用逼近定理 [[7](#bib.bib7), [43](#bib.bib43)]
    说明，具有某些激活函数（例如 ReLU）的单隐层神经网络可以在紧致域上以任意所需精度逼近任何连续函数。Lu 等人 [[64](#bib.bib64)] 证明，对于任何勒贝格可积函数
    $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$ 和任意 $\epsilon>0$，存在一个 ReLU 网络 $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}$，可以将
    $f$ 逼近到 $\epsilon$ 的 $L^{1}$ 距离。也就是说，
- en: '|  | $\int_{\mathbb{R}^{d}}&#124;f(x)-F_{\mathcal{N}}(x)&#124;dx<\epsilon.$
    |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $\int_{\mathbb{R}^{d}}|f(x)-F_{\mathcal{N}}(x)|dx<\epsilon.$ |  |'
- en: Here $F_{\mathcal{N}}$ is the function represented by the neural network $\mathcal{N}$.
    The width $m_{i}$ of each hidden layer of $\mathcal{N}$ satisfies $m_{i}\leq d+4$.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $F_{\mathcal{N}}$ 是由神经网络 $\mathcal{N}$ 表示的函数。每个隐藏层的宽度 $m_{i}$ 满足 $m_{i}\leq
    d+4$。
- en: Moreover, to explore the role of layer width in expressive capacity quantitatively,
    Lu et al. [[64](#bib.bib64)] raise the dual question of depth efficiency. That
    is, whether there exists wide, shallow ReLU networks that cannot be approximated
    by any narrow, deep neural network whose size is not substantially increased.
    Denote by $F_{\mathcal{A}}:\mathbb{R}^{d}\rightarrow\mathbb{R}$ the function represented
    by a ReLU neural network $\mathcal{A}$ whose depth $h=3$ and whose width of each
    layer is $2k^{2}$, where $k$ is an integer such that $k\geq d+4$. Let $F_{\mathcal{B}}:\mathbb{R}^{d}\rightarrow\mathbb{R}$
    be the function represented by a ReLU neural network $\mathcal{B}$ whose depth
    $h\leq k+2$ and whose width of each hidden layer $m_{i}\leq k^{3/2}$, where the
    parameter values of $\mathcal{B}$ are bounded by $[-b,b]$. For any constant $b>0$,
    there is $\epsilon>0$ such that $F_{\mathcal{A}}$ can never be approximated by
    $F_{\mathcal{B}}$ to $\epsilon$ $L^{1}$-distance. That is,
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了定量探讨层宽度在表达能力中的作用，Lu 等人 [[64](#bib.bib64)] 提出了深度效率的对偶问题。即是否存在宽而浅的 ReLU 网络，这些网络不能被任何窄而深的神经网络所逼近，而其大小并未显著增加。设
    $F_{\mathcal{A}}:\mathbb{R}^{d}\rightarrow\mathbb{R}$ 是由 ReLU 神经网络 $\mathcal{A}$
    表示的函数，其中深度 $h=3$，每层的宽度为 $2k^{2}$，其中 $k$ 是一个整数，使得 $k\geq d+4$。设 $F_{\mathcal{B}}:\mathbb{R}^{d}\rightarrow\mathbb{R}$
    是由 ReLU 神经网络 $\mathcal{B}$ 表示的函数，其中深度 $h\leq k+2$，每个隐藏层的宽度 $m_{i}\leq k^{3/2}$，其中
    $\mathcal{B}$ 的参数值受限于 $[-b,b]$。对于任何常数 $b>0$，存在 $\epsilon>0$ 使得 $F_{\mathcal{A}}$
    永远无法被 $F_{\mathcal{B}}$ 以 $\epsilon$ 的 $L^{1}$ 距离逼近。也就是说，
- en: '|  | $\int_{\mathbb{R}^{d}}&#124;F_{\mathcal{A}}(x)-F_{\mathcal{B}}(x)&#124;dx\geq\epsilon.$
    |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | $\int_{\mathbb{R}^{d}}|F_{\mathcal{A}}(x)-F_{\mathcal{B}}(x)|dx\geq\epsilon.$
    |  |'
- en: This indicates that there exists a family of shallow ReLU neural networks which
    cannot be approximated by narrow networks whose depth is constrained by polynomial
    bounds.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明存在一类浅层 ReLU 神经网络，这些网络不能被深度受限于多项式界限的窄网络所逼近。
- en: The polynomial lower bound for width efficiency is smaller than the exponential
    lower bound for depth efficiency [[13](#bib.bib13), [15](#bib.bib15), [70](#bib.bib70)].
    That is, to approximate a deep model whose depth increases linearly, a shallow
    model requires at least an exponential increase in width. To approximate a shallow,
    wide model whose width increases linearly, a deep, narrow model requires at least
    a polynomial increase in depth. However, Lu et al. [[64](#bib.bib64)] point out
    that depth cannot be strictly proved to be more effective than width since a polynomial
    upper bound for width is still lacking. The polynomial upper bound ensures that
    to approximate a shallow, wide model, a deep, narrow one requires at most a polynomial
    increase in depth.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式下界对于宽度效率小于指数下界对于深度效率[[13](#bib.bib13), [15](#bib.bib15), [70](#bib.bib70)]。也就是说，要近似一个深度线性增加的模型，浅层模型的宽度需要至少指数级增加。要近似一个宽度线性增加的浅层宽模型，深度狭窄模型的深度需要至少多项式级增加。然而，Lu等人[[64](#bib.bib64)]指出，由于缺乏宽度的多项式上界，无法严格证明深度比宽度更有效。多项式上界确保了要近似一个浅层宽模型，深层狭窄模型的深度最多需要多项式级增加。
- en: 3.3 Expressible Functional Space
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 可表达的函数空间
- en: In addition to the studies of depth efficiency and width efficiency, the third
    line of works explore the classes of functions that can be expressed by deep learning
    models with specific frameworks and specified size. This line of works explore
    the expressible functional space of deep learning models, including model-specific
    and cross-model approaches.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对深度效率和宽度效率的研究外，第三类研究探讨了可以由具有特定框架和指定规模的深度学习模型表达的函数类。这类研究探索了深度学习模型的可表达函数空间，包括特定模型的方法和跨模型的方法。
- en: 3.3.1 Model-specific Approaches
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 特定模型的方法
- en: Arora et al. [[3](#bib.bib3)] investigate the family of functions representable
    by deep neural networks with ReLU activation function. They prove that every piecewise
    linear function $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$ can be represented by
    a ReLU neural network that consists of at most $\lceil\log_{2}(d+1)\rceil$ hidden
    layers. The family of piecewise linear functions is dense in the family of compactly
    supported continuous functions, and the family of compactly supported continuous
    functions is dense in Lebesgue space $L^{p}(\mathbb{R}^{d})$ [[3](#bib.bib3),
    [23](#bib.bib23)].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Arora 等人[[3](#bib.bib3)] 研究了由具有 ReLU 激活函数的深度神经网络表示的函数族。他们证明了每个分段线性函数 $f:\mathbb{R}^{d}\rightarrow\mathbb{R}$
    可以由最多 $\lceil\log_{2}(d+1)\rceil$ 个隐藏层的 ReLU 神经网络表示。分段线性函数族在紧支撑连续函数族中是稠密的，而紧支撑连续函数族在
    Lebesgue 空间 $L^{p}(\mathbb{R}^{d})$ 中是稠密的[[3](#bib.bib3), [23](#bib.bib23)]。
- en: The Lebesgue space $L^{p}(\mathbb{R}^{d})$ is defined as the the family of Lebesgue
    intergrable functions $f$ for which $\int_{\mathbb{R}^{d}}|f|<+\infty$ [[23](#bib.bib23)].
    Define $L^{p}$ norm [[23](#bib.bib23)] as $||f||_{p}=[\int_{\mathbb{R}^{d}}|f|^{p}]^{1/p}$.
    Then, the above conclusion can be extended to the $L^{p}(\mathbb{R}^{d})$-space.
    That is, every function $f\in L^{p}(\mathbb{R}^{d})$ can be approximated to arbitrary
    $L^{p}$ norm by a ReLU neural network which consists of at most $\lceil\log_{2}(d+1)\rceil$
    hidden layers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Lebesgue 空间 $L^{p}(\mathbb{R}^{d})$ 定义为 Lebesgue 可积函数 $f$ 的集合，其中 $\int_{\mathbb{R}^{d}}|f|<+\infty$
    [[23](#bib.bib23)]。定义 $L^{p}$ 范数[[23](#bib.bib23)] 为 $||f||_{p}=[\int_{\mathbb{R}^{d}}|f|^{p}]^{1/p}$。那么，上述结论可以扩展到
    $L^{p}(\mathbb{R}^{d})$ 空间。也就是说，每个 $f\in L^{p}(\mathbb{R}^{d})$ 都可以通过最多 $\lceil\log_{2}(d+1)\rceil$
    个隐藏层的 ReLU 神经网络近似到任意的 $L^{p}$ 范数。
- en: Gühring et al. [[36](#bib.bib36)] study the expressive capacity of deep neural
    networks with ReLU activation function in Sobolev space [[1](#bib.bib1)]. Given
    $\Omega\subset\mathbb{R}^{d}$, $p\in[1,\infty]$, $n\in\mathbb{N}$, $L^{p}(\Omega)$
    is the Lebesgue space on $\Omega$, the Sobolev space [[84](#bib.bib84)] is defined
    as
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Gühring 等人[[36](#bib.bib36)] 研究了具有 ReLU 激活函数的深度神经网络在 Sobolev 空间中的表达能力[[1](#bib.bib1)]。给定
    $\Omega\subset\mathbb{R}^{d}$，$p\in[1,\infty]$，$n\in\mathbb{N}$，$L^{p}(\Omega)$
    是 $\Omega$ 上的 Lebesgue 空间，Sobolev 空间[[84](#bib.bib84)] 定义为
- en: '|  | $W^{n,p}(\Omega)=\{f\in L^{p}(\Omega):D^{\alpha}f\in L^{p}(\Omega)\mbox{
    for }\forall\alpha\in\mathbb{N}^{d}_{0},&#124;\alpha&#124;\leq n\},$ |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{n,p}(\Omega)=\{f\in L^{p}(\Omega):D^{\alpha}f\in L^{p}(\Omega)\mbox{
    对于 }\forall\alpha\in\mathbb{N}^{d}_{0},|\alpha|\leq n\},$ |  |'
- en: where $D^{\alpha}f$ is the $\alpha$-th order derivative of $f$. Sobolev norm
    is defined as
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D^{\alpha}f$ 是 $f$ 的 $\alpha$ 阶导数。Sobolev 范数定义为
- en: '|  | $&#124;&#124;f&#124;&#124;_{W^{n,p}(\Omega)}=(\sum_{0\leq&#124;\alpha&#124;\leq
    n}&#124;&#124;D^{\alpha}f&#124;&#124;^{p}_{L^{p}(\Omega)})^{1/p}.$ |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;f&#124;&#124;_{W^{n,p}(\Omega)}=(\sum_{0\leq&#124;\alpha&#124;\leq
    n}&#124;&#124;D^{\alpha}f&#124;&#124;^{p}_{L^{p}(\Omega)})^{1/p}.$ |  |'
- en: Gühring et al. [[36](#bib.bib36)] analyze the effect of ReLU neural networks
    in approximating functions from Sobolev space, and establish upper and lower bounds
    on the sizes of models to approximate functions in the Sobolev space. Specifically,
    define a subset of Sobolev space $f$ as
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Gühring 等人 [[36](#bib.bib36)] 分析了 ReLU 神经网络在逼近 Sobolev 空间函数方面的效果，并确定了逼近 Sobolev
    空间函数的模型规模的上界和下界。具体来说，定义 Sobolev 空间 $f$ 的一个子集为
- en: '|  | $\mathcal{F}_{n,d,p,B}=\{f\in W^{n,p}((0,1)^{d}):&#124;&#124;f&#124;&#124;_{W^{n,p}((0,1)^{d})}\leq
    B\}.$ |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}_{n,d,p,B}=\{f\in W^{n,p}((0,1)^{d}):&#124;&#124;f&#124;&#124;_{W^{n,p}((0,1)^{d})}\leq
    B\}.$ |  |'
- en: The upper bound shows that, for any $d\in\mathbb{N}$, $n\in\mathbb{N},n\geq
    2$, $\varepsilon>0$, $0\leq s\leq 1$, $1\leq p\leq+\infty$, and $B>0$, for any
    function $f\in\mathcal{F}_{n,d,p,B}$, there exists a neural network $\mathcal{N}_{\varepsilon}$
    and a choice of weights $w_{f}$ such that
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 上界显示，对于任意的 $d\in\mathbb{N}$，$n\in\mathbb{N},n\geq 2$，$\varepsilon>0$，$0\leq
    s\leq 1$，$1\leq p\leq+\infty$，且 $B>0$，对于任何函数 $f\in\mathcal{F}_{n,d,p,B}$，存在一个神经网络
    $\mathcal{N}_{\varepsilon}$ 和一组权重 $w_{f}$，使得
- en: '|  | $&#124;&#124;\mathcal{N}_{\varepsilon}(\cdot&#124;w_{f})-f(\cdot)&#124;&#124;_{W^{s,p}((0,1)^{d})}\leq\varepsilon$
    |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;\mathcal{N}_{\varepsilon}(\cdot&#124;w_{f})-f(\cdot)&#124;&#124;_{W^{s,p}((0,1)^{d})}\leq\varepsilon$
    |  |'
- en: where $\mathcal{N}_{\varepsilon}$ represents a ReLU neural network consisting
    of at most $c\log_{2}(\varepsilon^{-\frac{n}{n-s}})$ layers and $c\varepsilon^{-\frac{d}{n-s}}\log_{2}(\varepsilon^{-\frac{n}{n-s}})$
    neurons, and with non-zero parameters. The value of constant $c$ depends on the
    value of $d,p,n,s$ and $B$.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{N}_{\varepsilon}$ 代表一个最多包含 $c\log_{2}(\varepsilon^{-\frac{n}{n-s}})$
    层和 $c\varepsilon^{-\frac{d}{n-s}}\log_{2}(\varepsilon^{-\frac{n}{n-s}})$ 个神经元的
    ReLU 神经网络，并且参数不为零。常数 $c$ 的值依赖于 $d,p,n,s$ 和 $B$ 的值。
- en: Besides, the lower bound [[36](#bib.bib36)] shows that, for any $d\in\mathbb{N}$,
    $n\in\mathbb{N},n\geq 2$, $\varepsilon>0$, $B>0$, $k\in\{0,1\}$, with $p=+\infty$,
    for any function $f\in\mathcal{F}_{n,d,p,B}$, there exists a ReLU neural network
    $\mathcal{N}_{\varepsilon}$ such that
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，下界 [[36](#bib.bib36)] 显示，对于任意的 $d\in\mathbb{N}$，$n\in\mathbb{N},n\geq 2$，$\varepsilon>0$，$B>0$，$k\in\{0,1\}$，当
    $p=+\infty$ 时，对于任何函数 $f\in\mathcal{F}_{n,d,p,B}$，存在一个 ReLU 神经网络 $\mathcal{N}_{\varepsilon}$，使得
- en: '|  | $&#124;&#124;\mathcal{N}_{\varepsilon}(\cdot&#124;w_{f})-f(\cdot)&#124;&#124;_{W^{k,\infty}((0,1)^{d})}\leq\varepsilon$
    |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;\mathcal{N}_{\varepsilon}(\cdot&#124;w_{f})-f(\cdot)&#124;&#124;_{W^{k,\infty}((0,1)^{d})}\leq\varepsilon$
    |  |'
- en: where $\mathcal{N}_{\varepsilon}$ has at least $c^{\prime}\varepsilon^{-\frac{d}{2(n-1)}}$
    non-zero weights.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{N}_{\varepsilon}$ 至少有 $c^{\prime}\varepsilon^{-\frac{d}{2(n-1)}}$
    个非零权重。
- en: Kileel et al. [[52](#bib.bib52)] explore the functional space of deep neural
    networks with polynomial activation functions. A polynomial activation function
    $\rho_{r}(z)$ raises $z$ to the power of $r$. They suggest that, with polynomial
    activation functions, the study of model complexity can be benefitted from the
    application of powerful mathematical machinery of algebraic geometry. In addition,
    polynomials can approximate any continuous activation function, thus help to explore
    other deep learning models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kileel 等人 [[52](#bib.bib52)] 探索了具有多项式激活函数的深度神经网络的函数空间。一个多项式激活函数 $\rho_{r}(z)$
    将 $z$ 提升到 $r$ 次方。他们建议，使用多项式激活函数时，模型复杂性的研究可以从代数几何的强大数学工具中受益。此外，多项式可以逼近任何连续激活函数，从而有助于探索其他深度学习模型。
- en: Given a deep polynomial neural network $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{c}$
    with depth $h$ and polynomial degree $r$, let $m=\{m_{0},m_{1},\ldots,m_{h}\}$
    represent the architecture of $\mathcal{N}$ with the width $m_{i}$ of layer $i$
    and $m_{0}=d$, $m_{h}=c$. Let $\mathcal{F}_{m,r}$ be the functional space of $\mathcal{N}$.
    Kileel et al. [[52](#bib.bib52)] define the functional variety $\mathcal{V}_{m,r}$
    as $\mathcal{V}_{m,r}=\overline{\mathcal{F}_{m,r}}$, which is the Zariski closure
    of the functional space $\mathcal{F}_{m,r}$. “The Zariski closure of a set $X$
    is the smallest set containing $X$ that can be described by polynomial equations.” [[52](#bib.bib52)]
    The functional variaty $\mathcal{V}_{m,r}$ can be much larger than the functional
    space $\mathcal{F}_{m,r}$. Kileel et al. [[52](#bib.bib52)] propose to use the
    dimensionality of functional variety, denoted by $\dim\mathcal{V}_{m,r}$, as the
    representation of the expressive capacity of deep polynomial neural networks,
    written as
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个深度多项式神经网络 $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{c}$，其深度为 $h$，多项式次数为
    $r$，令 $m=\{m_{0},m_{1},\ldots,m_{h}\}$ 表示 $\mathcal{N}$ 的架构，其中第 $i$ 层的宽度为 $m_{i}$，且
    $m_{0}=d$，$m_{h}=c$。令 $\mathcal{F}_{m,r}$ 为 $\mathcal{N}$ 的功能空间。Kileel 等人 [[52](#bib.bib52)]
    定义了功能变种 $\mathcal{V}_{m,r}$ 为 $\mathcal{V}_{m,r}=\overline{\mathcal{F}_{m,r}}$，它是功能空间
    $\mathcal{F}_{m,r}$ 的 Zariski 闭包。“一个集合 $X$ 的 Zariski 闭包是包含 $X$ 的最小集合，该集合可以用多项式方程描述。”
    [[52](#bib.bib52)] 功能变种 $\mathcal{V}_{m,r}$ 可以比功能空间 $\mathcal{F}_{m,r}$ 大得多。Kileel
    等人 [[52](#bib.bib52)] 提议使用功能变种的维度，记作 $\dim\mathcal{V}_{m,r}$，作为深度多项式神经网络的表达能力的表示，写作
- en: '|  | $MEC(\mathcal{N}_{m,r})=\dim\mathcal{V}_{m,r}.$ |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $MEC(\mathcal{N}_{m,r})=\dim\mathcal{V}_{m,r}.$ |  |'
- en: To measure $\dim\mathcal{V}_{m,r}$, Kileel et al. [[52](#bib.bib52)] build connections
    between deep polynomial networks and tensor decompositions. Specifically, polynomial
    networks with $h=2$ are connected to CP tensor decompositions [[56](#bib.bib56)],
    and deep polynomial networks are connected to an iterated tensor decomposition [[65](#bib.bib65)].
    Based on decompositions, they prove that, for any fixed $m$, there exists $\tilde{r}=r(m)$
    such that for any $r>\tilde{r}$, $\dim\mathcal{V}_{m,r}$ is bounded by
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量 $\dim\mathcal{V}_{m,r}$，Kileel 等人 [[52](#bib.bib52)] 建立了深度多项式网络与张量分解之间的联系。具体来说，$h=2$
    的多项式网络与 CP 张量分解 [[56](#bib.bib56)] 相关，而深度多项式网络与迭代张量分解 [[65](#bib.bib65)] 相关。基于分解，他们证明了，对于任何固定的
    $m$，存在 $\tilde{r}=r(m)$，使得对于任何 $r>\tilde{r}$，$\dim\mathcal{V}_{m,r}$ 是有界的。
- en: '|  | $\dim\mathcal{V}_{m,r}\leq\min\left(m_{h}+\sum_{i=1}^{h}(m_{i-1}-1)m_{i},m_{h}\binom{m_{0}+r^{h-1}+1}{r^{h-1}}\right).$
    |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\dim\mathcal{V}_{m,r}\leq\min\left(m_{h}+\sum_{i=1}^{h}(m_{i-1}-1)m_{i},m_{h}\binom{m_{0}+r^{h-1}+1}{r^{h-1}}\right).$
    |  |'
- en: Besides, Kileel et al. [[52](#bib.bib52)] prove a bottleneck property of deep
    polynomial networks. That is, a too narrow layer is a bottleneck and may “chock”
    the polynomial network such that the network can never fill the ambient space.
    The ambient space $Sym_{r}(\mathbb{R}^{d})$ is the space of homogeneous polynomials
    of degree $r$ in $d$ variables. A polynomial network filling the ambient space
    satisfies $\mathcal{F}_{m,r}=Sym_{r^{h-1}}(\mathbb{R}^{d})^{c}$. They show that,
    network architectures filling the ambient space can be helpful for optimization
    and training.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Kileel 等人 [[52](#bib.bib52)] 证明了深度多项式网络的瓶颈属性。也就是说，过于狭窄的层是瓶颈，可能会“窒息”多项式网络，使得网络无法填充环境空间。环境空间
    $Sym_{r}(\mathbb{R}^{d})$ 是 $d$ 个变量的 $r$ 次同质多项式的空间。一个填充环境空间的多项式网络满足 $\mathcal{F}_{m,r}=Sym_{r^{h-1}}(\mathbb{R}^{d})^{c}$。他们展示了填充环境空间的网络架构对优化和训练是有帮助的。
- en: 3.3.2 Cross-Model Approaches
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 跨模型方法
- en: In addition to model-specific approaches, expressible functional space can be
    investigated in a cross-model manner. Specifically, Khrulkov et al. [[51](#bib.bib51)]
    study the expressive capacity of recurrent neural networks (RNNs). They investigate
    the connections between network architectures and tensor decompositions, then
    make comparison between the expressive capacity of RNNs, CNNs, and shallow FCNNs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特定模型的方法外，可表达的功能空间可以以跨模型的方式进行研究。具体来说，Khrulkov 等人 [[51](#bib.bib51)] 研究了递归神经网络（RNN）的表达能力。他们调查了网络架构与张量分解之间的关系，然后比较了
    RNN、CNN 和浅层 FCNN 的表达能力。
- en: '![Refer to caption](img/878eba45a162384b586b93214c33419e.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/878eba45a162384b586b93214c33419e.png)'
- en: (a) TT-decomposition
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (a) TT-分解
- en: '![Refer to caption](img/a222b32076a8af15ba55940aa45ed71e.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a222b32076a8af15ba55940aa45ed71e.png)'
- en: (b) CP-decomposition
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: (b) CP-分解
- en: '![Refer to caption](img/31c57ce8c70ff1d21928b3dd78bb5d5a.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/31c57ce8c70ff1d21928b3dd78bb5d5a.png)'
- en: (c) HT-decomposition
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: (c) HT-分解
- en: 'Figure 3: Examples of networks corresponding to various tensor decompositions [[51](#bib.bib51)],
    from the left are TT decomposition, CP-decomposition, HT-decomposition.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：对应于各种张量分解的网络示例 [[51](#bib.bib51)]，从左到右依次是 TT 分解、CP 分解、HT 分解。
- en: Let $\mathcal{X}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots\times n_{d}}$ be
    a $d$-dimensional tensor. The Tensor Train (TT) decomposition [[83](#bib.bib83)]
    of a tensor $\mathcal{X}$ is computed by
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathcal{X}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots\times n_{d}}$ 是一个
    $d$ 维张量。张量的 Tensor Train (TT) 分解 [[83](#bib.bib83)] 通过以下方式计算
- en: '|  | $\mathcal{X}^{i_{1}i_{2}\ldots i_{d}}=\sum_{\alpha_{1}=1}^{r_{1}}\ldots\sum_{\alpha_{d-1}=1}^{r_{d-1}}G_{1}^{i_{1}\alpha_{1}}G_{2}^{\alpha_{1}i_{2}\alpha_{2}}\ldots
    G_{d}^{\alpha_{d-1}i_{d}}$ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{X}^{i_{1}i_{2}\ldots i_{d}}=\sum_{\alpha_{1}=1}^{r_{1}}\ldots\sum_{\alpha_{d-1}=1}^{r_{d-1}}G_{1}^{i_{1}\alpha_{1}}G_{2}^{\alpha_{1}i_{2}\alpha_{2}}\ldots
    G_{d}^{\alpha_{d-1}i_{d}}$ |  |'
- en: 'where the tensors $G_{k}\in\mathbb{R}^{r_{k-1}\times n_{k}\times r_{k}}$ are
    called TT-cores. Khrulkov et al. [[51](#bib.bib51)] introduce bilinear units to
    represent TT-cores. Given $x\in\mathbb{R}^{m},y\in\mathbb{R}^{n}$ and $G\in\mathbb{R}^{m\times
    n\times k}$, a bilinear unit performs a map $G:\mathbb{R}^{m}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{k}$,
    written as $G(x,y)=z$. Based on the bilinear units, they show that a recurrent
    neural network realizes the TT-decomposition of the weight tensor (Fig. [3(a)](#S3.F3.sf1
    "In Figure 3 ‣ 3.3.2 Cross-Model Approaches ‣ 3.3 Expressible Functional Space
    ‣ 3 Expressive Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey")). Similarly, Khrulkov et al. [[51](#bib.bib51)] show that the Canonical
    (CP) decomposition [[24](#bib.bib24)], with the form of'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中张量 $G_{k}\in\mathbb{R}^{r_{k-1}\times n_{k}\times r_{k}}$ 被称为 TT 核。Khrulkov
    等人 [[51](#bib.bib51)] 引入双线性单元来表示 TT 核。给定 $x\in\mathbb{R}^{m},y\in\mathbb{R}^{n}$
    和 $G\in\mathbb{R}^{m\times n\times k}$，一个双线性单元执行映射 $G:\mathbb{R}^{m}\times\mathbb{R}^{n}\rightarrow\mathbb{R}^{k}$，记作
    $G(x,y)=z$。基于双线性单元，他们展示了递归神经网络实现了权重张量的 TT 分解（见图[3(a)](#S3.F3.sf1 "图 3 ‣ 3.3.2
    跨模型方法 ‣ 3.3 可表达的功能空间 ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型的复杂性：综述")）。类似地，Khrulkov 等人 [[51](#bib.bib51)]
    展示了 Canonical (CP) 分解 [[24](#bib.bib24)]，形式为
- en: '|  | $\mathcal{X}^{i_{1}i_{2}\ldots i_{d}}=\sum_{\alpha=1}^{r}v_{1,\alpha}^{i_{1}}v_{2,\alpha}^{i_{2}}\ldots
    v_{d,\alpha}^{i_{d}},$ |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{X}^{i_{1}i_{2}\ldots i_{d}}=\sum_{\alpha=1}^{r}v_{1,\alpha}^{i_{1}}v_{2,\alpha}^{i_{2}}\ldots
    v_{d,\alpha}^{i_{d}},$ |  |'
- en: 'corresponds to a one-hidden-layer FCNN (Fig. [3(b)](#S3.F3.sf2 "In Figure 3
    ‣ 3.3.2 Cross-Model Approaches ‣ 3.3 Expressible Functional Space ‣ 3 Expressive
    Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey")).
    Each unit of the network is denoted by $G_{\alpha}=v_{1,\alpha}^{i_{1}}v_{2,\alpha}^{i_{2}}\ldots
    v_{d,\alpha}^{i_{d}}$, where $v_{i,\alpha}\in\mathbb{R}^{n_{i}}$. The Hierarchical
    Tucker (HT) decomposition [[27](#bib.bib27)] corresponds to an CNN structure (Fig. [3(c)](#S3.F3.sf3
    "In Figure 3 ‣ 3.3.2 Cross-Model Approaches ‣ 3.3 Expressible Functional Space
    ‣ 3 Expressive Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey")).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于一个隐藏层的全连接神经网络（见图[3(b)](#S3.F3.sf2 "图 3 ‣ 3.3.2 跨模型方法 ‣ 3.3 可表达的功能空间 ‣ 3 深度学习模型的表达能力
    ‣ 深度学习模型的复杂性：综述")）。网络中的每个单元表示为 $G_{\alpha}=v_{1,\alpha}^{i_{1}}v_{2,\alpha}^{i_{2}}\ldots
    v_{d,\alpha}^{i_{d}}$，其中 $v_{i,\alpha}\in\mathbb{R}^{n_{i}}$。分层 Tucker (HT) 分解
    [[27](#bib.bib27)] 对应于 CNN 结构（见图[3(c)](#S3.F3.sf3 "图 3 ‣ 3.3.2 跨模型方法 ‣ 3.3 可表达的功能空间
    ‣ 3 深度学习模型的表达能力 ‣ 深度学习模型的复杂性：综述")）。
- en: 'Table 4: Comparison of the expressive power of various network architectures
    given by Khrulkov et al. [[51](#bib.bib51)]. Each column represents a specific
    network with width $r$, rows show the upper bounds on the width of other types
    of networks to achieve equivalent expressive capacity.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Khrulkov 等人 [[51](#bib.bib51)] 提供的各种网络架构的表达能力比较。每列表示特定宽度为 $r$ 的网络，行展示了其他类型网络达到等效表达能力的宽度上限。
- en: '|  | TT-Network | HT-Network | CP-Network |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | TT-Network | HT-Network | CP-Network |'
- en: '| --- | --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| TT-Network | $r$ | $r^{\log_{2}(d)/2}$ | $r$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| TT-Network | $r$ | $r^{\log_{2}(d)/2}$ | $r$ |'
- en: '| HT-Network | $r^{2}$ | $r$ | $r$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| HT-Network | $r^{2}$ | $r$ | $r$ |'
- en: '| CP-Network | $\geq r^{d/2}$ | $\geq r^{d/2}$ | $r$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| CP-Network | $\geq r^{d/2}$ | $\geq r^{d/2}$ | $r$ |'
- en: 'Khrulkov et al. [[51](#bib.bib51)] propose the rank of tensor decomposition
    as a measure of neural network complexity, since the rank of decompositions corresponds
    to the width of networks. Based on the correspondence relationships between neural
    networks and tensor decompositions, they compare the model complexity of RNNs,
    CNNs, and shallow FCNNs. The main conclusions are summarized in Table [4](#S3.T4
    "Table 4 ‣ 3.3.2 Cross-Model Approaches ‣ 3.3 Expressible Functional Space ‣ 3
    Expressive Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning:
    A Survey"). In particular, given a random $d$-dimensional tensor whose TT-decomposition
    is with rank $r$ and mode size $n$, this tensor has exponentially large ranks
    of CP-decomposition and HT-decomposition. That tells, to approximate a recurrent
    neural network, a shallow FCNN or CNN requires an exponentially larger width.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 'Khrulkov 等人 [[51](#bib.bib51)] 提出了张量分解的秩作为神经网络复杂度的度量，因为分解的秩与网络的宽度相对应。基于神经网络与张量分解之间的对应关系，他们比较了
    RNN、CNN 和浅层 FCNN 的模型复杂度。主要结论总结在表 [4](#S3.T4 "Table 4 ‣ 3.3.2 Cross-Model Approaches
    ‣ 3.3 Expressible Functional Space ‣ 3 Expressive Capacity of Deep Learning Models
    ‣ Model Complexity of Deep Learning: A Survey") 中。特别地，给定一个随机的 $d$ 维张量，其 TT 分解的秩为
    $r$ 和模式大小为 $n$，这个张量具有指数级大的 CP 分解和 HT 分解的秩。这表明，为了逼近一个递归神经网络，浅层 FCNN 或 CNN 需要一个指数级更大的宽度。'
- en: 3.4 VC Dimension and Rademacher Complexity
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 VC 维度和 Rademacher 复杂度
- en: The VC dimension and Rademacher complexity are widely used to analyze the expressive
    capacity and generalization of classical parametric machine learning models [[8](#bib.bib8),
    [12](#bib.bib12), [26](#bib.bib26), [69](#bib.bib69), [95](#bib.bib95)]. A series
    of works investigate the VC dimension and Rademacher complexity of deep learning
    models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: VC 维度和 Rademacher 复杂度广泛用于分析经典参数化机器学习模型的表达能力和泛化能力 [[8](#bib.bib8), [12](#bib.bib12),
    [26](#bib.bib26), [69](#bib.bib69), [95](#bib.bib95)]。一系列工作研究了深度学习模型的 VC 维度和 Rademacher
    复杂度。
- en: 'The VC dimension is an expressive capacity measure that reflects the largest
    number of samples that can be shattered by the hypothesis space [[69](#bib.bib69)].
    A higher VC dimension means the model can shatter a larger number of samples and
    thus the model has a higher expressive capacity. Maass [[66](#bib.bib66)] studies
    the VC dimension of feedforward neural networks with linear threshold gates. The
    linear threshold gate means that each neuron is composed of a weighted sum function
    followed by a Heaviside activation function (Eq. ([2](#S3.E2 "In 3.1.1 Model Reduction
    ‣ 3.1 Depth Efficiency ‣ 3 Expressive Capacity of Deep Learning Models ‣ Model
    Complexity of Deep Learning: A Survey"))). Let $W$ be the number of parameters
    in the network and $L$ be the depth of the network. Maass [[66](#bib.bib66)] proves
    that for $L\geq 3$, the VC dimension of such networks is $\Theta(W\log W)$.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'VC 维度是一个表达能力度量，反映了假设空间能够破碎的样本的最大数量 [[69](#bib.bib69)]。更高的 VC 维度意味着模型可以破碎更多的样本，从而模型具有更高的表达能力。Maass
    [[66](#bib.bib66)] 研究了具有线性阈值门的前馈神经网络的 VC 维度。线性阈值门表示每个神经元由一个加权和函数及一个 Heaviside
    激活函数（见 Eq. ([2](#S3.E2 "In 3.1.1 Model Reduction ‣ 3.1 Depth Efficiency ‣ 3 Expressive
    Capacity of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey"))）组成。设
    $W$ 为网络中的参数数量，$L$ 为网络的深度。Maass [[66](#bib.bib66)] 证明，当 $L\geq 3$ 时，这类网络的 VC 维度为
    $\Theta(W\log W)$。'
- en: Bartlett et al. [[11](#bib.bib11)] investigate the VC dimension of feedforward
    neural networks with piecewise polynomial activation functions. A piecewise polynimial
    activation function with $p$ pieces has the form $\sigma(z)=\phi_{i}(z)$, where
    $z\in[t_{i-1},t_{i})$, $i\in{1,\ldots,p+1}$, and $t_{i-1}<t_{i}$. Each $\phi_{i}$
    is a polynomial function of degree no more than $r$. Let $W$ be the number of
    parameters in the network and $L$ be the depth of the network. Bartlett et al. [[11](#bib.bib11)]
    prove that the upper bound for the VC dimension of such networks is $O(WL^{2}+WL\log
    WL)$ and the lower bound for the VC dimension is $\Omega(WL)$. Later, Bartlett et
    al. [[10](#bib.bib10)] improve this lower bound to $\Omega(WL\log(W/L))$.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Bartlett 等人 [[11](#bib.bib11)] 研究了具有分段多项式激活函数的前馈神经网络的 VC 维度。具有 $p$ 个片段的分段多项式激活函数形式为
    $\sigma(z)=\phi_{i}(z)$，其中 $z\in[t_{i-1},t_{i})$，$i\in{1,\ldots,p+1}$，且 $t_{i-1}<t_{i}$。每个
    $\phi_{i}$ 是一个次数不超过 $r$ 的多项式函数。设 $W$ 为网络中的参数数量，$L$ 为网络的深度。Bartlett 等人 [[11](#bib.bib11)]
    证明了这类网络的 VC 维度的上界为 $O(WL^{2}+WL\log WL)$，下界为 $\Omega(WL)$。后来，Bartlett 等人 [[10](#bib.bib10)]
    将下界改进为 $\Omega(WL\log(W/L))$。
- en: Bartlett et al. [[10](#bib.bib10)] study the VC dimension for deep neural networks
    with piecewise linear activation functions (e.g., ReLU). Given a deep neural network
    with $L$ layers and $W$ parameters, they prove that the lower bound for VC dimension
    of such networks is $\Omega(WL\log(W/L))$ and the upper bound for VC dimension
    is $O(WL\log W)$.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Bartlett 等人 [[10](#bib.bib10)] 研究了具有分段线性激活函数（如 ReLU）的深度神经网络的 VC 维度。给定一个具有 $L$
    层和 $W$ 参数的深度神经网络，他们证明了此类网络的 VC 维度的下界是 $\Omega(WL\log(W/L))$，而 VC 维度的上界是 $O(WL\log
    W)$。
- en: Rademacher complexity captures the capacity of a hypothesis space to fit random
    labels as a measure of the expressive capacity [[69](#bib.bib69)]. A higher Rademacher
    complexity means that the model can fit a larger number of random labels and thus
    the model has a higher expressive capacity. Bartlett et al. [[9](#bib.bib9)] investigate
    the Rademacher complexity of deep neural networks with ReLU activation function.
    Given a deep ReLU neural network with $L$ layers, let $A_{i}$ be the parameter
    matrix of layer $i$, and $X\in\mathbb{R}^{n\times d}$ the data matrix, where $n$
    is the number of instances and $d$ is the input dimension. Bartlett et al. [[9](#bib.bib9)]
    prove that the lower bound for the Rademacher complexity of such networks is $\Omega(||X||_{F}\prod_{i}||A_{i}||_{\sigma})$,
    where $||\cdot||_{\sigma}$ is the Spectral norm, and $||\cdot||_{F}$ is the Frobenius
    norm. Neyshabur et al. [[77](#bib.bib77)] prove a tighter lower bound for two-layer
    ReLU neural networks. Suppose $||A_{1}||_{\sigma}\leq s_{1}$, $||A_{2}||_{\sigma}\leq
    s_{2}$, and $s_{1}s_{2}$ is the Lipschitz bound of the function represented by
    the network. They prove that the Rademacher complexity is lower bounded by $\Omega(s_{1}s_{2}\sqrt{m}||X||_{F}/n)$,
    where $m$ is the width of the hidden layer. This lower bound improves the bound [[9](#bib.bib9)]
    by a factor of $\sqrt{m}$.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Rademacher 复杂度捕捉了假设空间拟合随机标签的能力，作为表达能力的度量 [[69](#bib.bib69)]。更高的 Rademacher 复杂度意味着模型可以拟合更多的随机标签，因此模型具有更高的表达能力。Bartlett
    等人 [[9](#bib.bib9)] 研究了具有 ReLU 激活函数的深度神经网络的 Rademacher 复杂度。给定一个具有 $L$ 层的深度 ReLU
    神经网络，设 $A_{i}$ 为第 $i$ 层的参数矩阵，$X\in\mathbb{R}^{n\times d}$ 为数据矩阵，其中 $n$ 是实例数量，$d$
    是输入维度。Bartlett 等人 [[9](#bib.bib9)] 证明了此类网络的 Rademacher 复杂度的下界是 $\Omega(||X||_{F}\prod_{i}||A_{i}||_{\sigma})$，其中
    $||\cdot||_{\sigma}$ 是谱范数，$||\cdot||_{F}$ 是 Frobenius 范数。Neyshabur 等人 [[77](#bib.bib77)]
    证明了两层 ReLU 神经网络的更紧的下界。假设 $||A_{1}||_{\sigma}\leq s_{1}$，$||A_{2}||_{\sigma}\leq
    s_{2}$，且 $s_{1}s_{2}$ 是网络表示函数的 Lipschitz 上界。他们证明了 Rademacher 复杂度的下界是 $\Omega(s_{1}s_{2}\sqrt{m}||X||_{F}/n)$，其中
    $m$ 是隐藏层的宽度。这个下界比 [[9](#bib.bib9)] 提出的下界提高了 $\sqrt{m}$ 倍。
- en: Yin et al. [[99](#bib.bib99)] study the Rademacher complexity of adversarial
    trained neural networks. Given a feedforward neural network with ReLU activation
    function, denoted by $f$, let $L$ be the depth of the network, and $A_{i}$ the
    parameter matrix of layer $i$. The function family represented by $f$ with adversarial
    loss function can be written as
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Yin 等人 [[99](#bib.bib99)] 研究了对抗训练神经网络的 Rademacher 复杂度。给定一个具有 ReLU 激活函数的前馈神经网络，记作
    $f$，设 $L$ 为网络的深度，$A_{i}$ 为第 $i$ 层的参数矩阵。由 $f$ 表示的函数家族在对抗损失函数下可以写作
- en: '|  | $\mathcal{F}=\{f_{A}:\min_{x^{\prime}\in\mathbb{B}(\epsilon)}yf_{A}(x),\prod_{i=1}^{L}&#124;&#124;A_{i}&#124;&#124;_{\sigma}\leq
    r\}$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}=\{f_{A}:\min_{x^{\prime}\in\mathbb{B}(\epsilon)}yf_{A}(x),\prod_{i=1}^{L}||A_{i}||_{\sigma}\leq
    r\}$ |  |'
- en: where $\mathbb{B}(\epsilon)=\{x^{\prime}\in\mathbb{R}^{d}:||x^{\prime}-x||\leq\epsilon\}$
    is the set of samples perturbed around $x$ with $l_{\infty}$ distance $\leq\epsilon$.
    Yin et al. [[99](#bib.bib99)] prove that the lower bound for the Rademacher complexity
    of $\mathcal{F}$ is $\Omega(||X||_{F}/n+\epsilon\sqrt{d/n})$. This lower bound
    exhibits an explicit dependence on the input dimension $d$.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{B}(\epsilon)=\{x^{\prime}\in\mathbb{R}^{d}:||x^{\prime}-x||\leq\epsilon\}$
    是在 $x$ 周围扰动的样本集合，满足 $l_{\infty}$ 距离 $\leq\epsilon$。Yin 等人 [[99](#bib.bib99)] 证明了
    $\mathcal{F}$ 的 Rademacher 复杂度的下界是 $\Omega(||X||_{F}/n+\epsilon\sqrt{d/n})$。这个下界显示了对输入维度
    $d$ 的显著依赖。
- en: Some studies [[75](#bib.bib75), [100](#bib.bib100)] suggest that deep learning
    models are often over-parameterized in practice and have significantly more parameters
    than samples. In this case, the VC dimension and Rademacher complexity of deep
    learning models are always too high, so the practical guidance they can provide
    is weak.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究 [[75](#bib.bib75), [100](#bib.bib100)] 表明，在实际中，深度学习模型通常是过度参数化的，参数远远多于样本数量。在这种情况下，深度学习模型的
    VC 维度和 Rademacher 复杂度总是过高，因此它们提供的实际指导是有限的。
- en: 4 Effective Complexity of Deep Learning Models
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习模型的有效复杂度
- en: Effective complexity of deep learning models is also known as practical complexity,
    practical expressivity, and usable capacity [[37](#bib.bib37), [81](#bib.bib81)].
    It reflects the complexity of the functions represented by deep models with specific
    parameterizations [[89](#bib.bib89)].
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的有效复杂度也被称为实际复杂度、实际表达能力和可用容量 [[37](#bib.bib37), [81](#bib.bib81)]。它反映了深度模型在特定参数化下所表示的函数的复杂度 [[89](#bib.bib89)]。
- en: Effective complexity of deep learning models has been mainly explored from two
    different aspects.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的有效复杂度主要从两个不同的方面进行探索。
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: General measures of effective complexity design quantitative measures for effective
    complexity of deep learning models.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有效复杂度的一般测量设计了深度学习模型有效复杂度的定量测量方法。
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Investigations into the high-capacity low-reality phenomenon find that effective
    complexity of deep learning models may be far lower than their expressive capacity.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对高容量低现实现象的研究发现，深度学习模型的有效复杂度可能远低于其表达能力。
- en: In this section, we review these two groups of studies.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们回顾了这两组研究。
- en: 4.1 General Measures of Effective Complexity
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 有效复杂度的一般测量
- en: Compared to expressive capacity, the study of effective model complexity has
    stronger requirements for sensitive and precise complexity measures. This is because
    the effective complexity cannot be directly derived from the model structure alone [[44](#bib.bib44)].
    Different parameter values of the same model structure may lead to different effective
    complexity. An effective complexity measure is expected to be sensitive to different
    parameter values used in models with the same structure.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与表达能力相比，研究有效模型复杂度对敏感和精确的复杂度测量有更高的要求。这是因为有效复杂度不能仅从模型结构直接推导出来 [[44](#bib.bib44)]。相同模型结构的不同参数值可能会导致不同的有效复杂度。有效复杂度的度量期望对使用相同结构模型中的不同参数值敏感。
- en: A series of works devote to proposing feasible measures for effective complexity
    of deep learning models. A major group of the complexity measures depends on the
    linear region splitting of piecewise linear neural networks in the input space.
    We start with those methods and then discuss the others.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列研究致力于提出可行的深度学习模型有效复杂度的测量方法。复杂度测量的一个主要组别依赖于分段线性神经网络在输入空间中的线性区域划分。我们从这些方法开始，然后讨论其他方法。
- en: 4.1.1 Piecewise Linear Property
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 分段线性属性
- en: '![Refer to caption](img/9eb8eb14e33bc89e2bfe361e586d78b4.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9eb8eb14e33bc89e2bfe361e586d78b4.png)'
- en: 'Figure 4: A 2-hidden-layer ReLU network divides the 2-dimensional input space
    into a number of linear regions, studied by Hanin and Rolnick [[37](#bib.bib37)].
    Upon each hidden neuron shows the linear regions divided by the current neuron.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：一个2隐层ReLU网络将二维输入空间划分为多个线性区域，Han和Rolnick研究了这个问题 [[37](#bib.bib37)]。每个隐藏神经元展示了当前神经元划分的线性区域。
- en: 'It is well known that a neural network with piecewise linear activation functions
    generates a finite number of linear regions in the input space [[44](#bib.bib44),
    [70](#bib.bib70), [89](#bib.bib89)]. This property is called the *piecewise linear
    property* and is demonstrated in Fig. [4](#S4.F4 "Figure 4 ‣ 4.1.1 Piecewise Linear
    Property ‣ 4.1 General Measures of Effective Complexity ‣ 4 Effective Complexity
    of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey"). The number
    of linear regions as well as the density of such regions can usually reflect the
    effective complexity. Therefore, a series of studies on effective complexity starts
    from piecewise linear activation functions (e.g., ReLU, Maxout) or are based on
    the piecewise linear property.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，具有分段线性激活函数的神经网络在输入空间中生成有限数量的线性区域 [[44](#bib.bib44), [70](#bib.bib70),
    [89](#bib.bib89)]。这种属性被称为*分段线性属性*，如图[4](#S4.F4 "Figure 4 ‣ 4.1.1 Piecewise Linear
    Property ‣ 4.1 General Measures of Effective Complexity ‣ 4 Effective Complexity
    of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey")所示。线性区域的数量以及这些区域的密度通常可以反映有效复杂度。因此，一系列关于有效复杂度的研究从分段线性激活函数（例如，ReLU、Maxout）开始，或基于分段线性属性。'
- en: Raghu et al. [[89](#bib.bib89)] propose two interrelated effective complexity
    measures for deep neural networks with piecewise linear activation functions,
    such as ReLU and hard Tanh [[82](#bib.bib82)]. Specifically, they define trajectory
    $x(t)$ between two input points $x_{1},x_{2}\in\mathbb{R}^{d}$, which is a curve
    parametrized by a scalar $t\in[0,1]$, with $x(0)=x_{1}$ and $x(1)=x_{2}$. The
    first effective complexity measure they propose is the number of linear regions
    in the input space when sweeping an input point through a trajectory path $x(t)$,
    that is, the effective complexity $EMC(\mathcal{N})$ of model $\mathcal{N}$ is
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Raghu 等人 [[89](#bib.bib89)] 为具有分段线性激活函数的深度神经网络（如 ReLU 和 hard Tanh [[82](#bib.bib82)]）提出了两个相互关联的有效复杂度度量。具体而言，他们定义了在两个输入点
    $x_{1},x_{2}\in\mathbb{R}^{d}$ 之间的轨迹 $x(t)$，这是由标量 $t\in[0,1]$ 参数化的曲线，且 $x(0)=x_{1}$
    和 $x(1)=x_{2}$。他们提出的第一个有效复杂度度量是当输入点沿着轨迹路径 $x(t)$ 移动时，输入空间中的线性区域数量，即模型 $\mathcal{N}$
    的有效复杂度 $EMC(\mathcal{N})$ 为
- en: '|  | $EMC(\mathcal{N})=\mathcal{T}(\mathcal{N}(x(t);W))$ |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(\mathcal{N})=\mathcal{T}(\mathcal{N}(x(t);W))$ |  |'
- en: where $\mathcal{T}$ is the number of linear regions passing through the trajectory
    $x(t)$ and $W$ is a specific set of model parameters. The second effective complexity
    measure they propose is the trajectory length $l(x(t))$ defined as
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{T}$ 是通过轨迹 $x(t)$ 的线性区域数量，而 $W$ 是一组特定的模型参数。他们提出的第二个有效复杂度度量是定义为轨迹长度
    $l(x(t))$ 的度量。
- en: '|  | $l(x(t))=\int_{t}&#124;&#124;\frac{dx(t)}{dt}&#124;&#124;dt$ |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $l(x(t))=\int_{t}&#124;&#124;\frac{dx(t)}{dt}&#124;&#124;dt$ |  |'
- en: which is the standard *arc length* of the trajectory $x(t)$. They prove the
    proportional relationship between these two complexity measures. To obtain the
    estimated value of effective complexity, Raghu et al. [[89](#bib.bib89)] bound
    the expected value of trajectory length of any layer in a deep ReLU neural network.
    Specifically, given a deep ReLU neural network whose weights are initialized by
    $N(0,\sigma^{2}_{w}/m)$ and the biases are initialized by $N(0,\sigma_{b}^{2})$.
    Let $z^{(i)}(x(t))$ denote the new trajectory obtained after the transformation
    of the first $i$ hidden layers of the trajectory $x(t)$. The expected value of
    its trajectory length can be bounded by
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是轨迹 $x(t)$ 的标准 *弧长*。他们证明了这两种复杂度度量之间的比例关系。为了获得有效复杂度的估计值，Raghu 等人 [[89](#bib.bib89)]
    约束了深度 ReLU 神经网络中任何层的轨迹长度的期望值。具体而言，给定一个深度 ReLU 神经网络，其权重初始化为 $N(0,\sigma^{2}_{w}/m)$，偏置初始化为
    $N(0,\sigma_{b}^{2})$。让 $z^{(i)}(x(t))$ 表示在轨迹 $x(t)$ 的前 $i$ 个隐藏层变换后的新轨迹。其轨迹长度的期望值可以被约束为
- en: '|  | $\mathbb{E}[l(z^{(i)}(x(t)))]\geq O(\frac{\sigma_{w}\sqrt{m}}{\sqrt{m+1}})^{i}l(x(t))$
    |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[l(z^{(i)}(x(t)))]\geq O(\frac{\sigma_{w}\sqrt{m}}{\sqrt{m+1}})^{i}l(x(t))$
    |  |'
- en: where $m$ denotes the hidden layer width. Similarly, for a hard Tanh neural
    network under the same initialization, the trajectory length is bounded by
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 表示隐藏层宽度。同样，对于在相同初始化下的 hard Tanh 神经网络，轨迹长度也有约束。
- en: '|  | $\mathbb{E}[l(z^{(i)}(x(t)))]\geq O\left(\frac{\sigma_{w}\sqrt{m}}{\sqrt{\sigma_{w}^{2}+\sigma_{b}^{2}+m\sqrt{\sigma_{w}^{2}+\sigma_{b}^{2}}}}\right)^{i}l(x(t)).$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[l(z^{(i)}(x(t)))]\geq O\left(\frac{\sigma_{w}\sqrt{m}}{\sqrt{\sigma_{w}^{2}+\sigma_{b}^{2}+m\sqrt{\sigma_{w}^{2}+\sigma_{b}^{2}}}}\right)^{i}l(x(t)).$
    |  |'
- en: '![Refer to caption](img/49b80780b5b9efcbf761834d688fcdbb.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/49b80780b5b9efcbf761834d688fcdbb.png)'
- en: 'Figure 5: A circular trajectory (the left most) is fed into a Tanh neural network,
    then the following images show the trajectory after the transformation of each
    hidden layer in turn [[89](#bib.bib89)]. It shows the increase of the length of
    the trajectory after the layer transformation.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：将一个圆形轨迹（最左侧）输入到 Tanh 神经网络中，随后图像显示了每个隐藏层变换后的轨迹 [[89](#bib.bib89)]。这显示了在层变换后的轨迹长度的增加。
- en: 'Using these two complexity measures, Raghu et al. [[89](#bib.bib89)] explore
    the performance of deep neural networks and report some findings. First, the effective
    complexity grows exponentially with respect to the depth of the model and polynomially
    with respect to the width. Fig. [5](#S4.F5 "Figure 5 ‣ 4.1.1 Piecewise Linear
    Property ‣ 4.1 General Measures of Effective Complexity ‣ 4 Effective Complexity
    of Deep Learning Models ‣ Model Complexity of Deep Learning: A Survey") is an
    example showing the evolution of a trajectory after each hidden layer of a deep
    network. Second, how the parameters are initialized affects the effective complexity.
    Third, injecting perturbations to a layer leads to exponentially larger perturbations
    at the remaining layers. Last, regularization approaches, such as Batch Normalization [[45](#bib.bib45)],
    help to reduce trajectory length. This explains why Batch Normalization helps
    model stability and generalization.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '利用这两个复杂度度量，Raghu等人[[89](#bib.bib89)] 探索了深度神经网络的性能，并报告了一些发现。首先，有效复杂度随模型深度呈指数增长，随宽度呈多项式增长。图[5](#S4.F5
    "Figure 5 ‣ 4.1.1 Piecewise Linear Property ‣ 4.1 General Measures of Effective
    Complexity ‣ 4 Effective Complexity of Deep Learning Models ‣ Model Complexity
    of Deep Learning: A Survey")展示了深度网络每个隐藏层之后轨迹的演变。其次，参数初始化的方式影响有效复杂度。第三，对某一层注入扰动会导致其他层的扰动呈指数级增长。最后，正则化方法，如批量归一化[[45](#bib.bib45)]，有助于减少轨迹长度。这解释了为何批量归一化有助于模型稳定性和泛化能力。'
- en: Novak et al. [[81](#bib.bib81)] empirically investigate the relationship between
    model complexity and generalization of neural networks with piecewise linear activation
    functions. They propose to use model sensitivity to measure effective complexity.
    Model sensitivity, also known as robustness, reflects the capacity of a model
    to distinguish between different inputs at small distances. Novak et al. [[81](#bib.bib81)]
    introduce two sensitivity metrics, input-output Jacobian norm and trajectory length [[89](#bib.bib89)].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Novak等人[[81](#bib.bib81)] 实证研究了模型复杂度与具有分段线性激活函数的神经网络的泛化能力之间的关系。他们建议使用模型敏感度来衡量有效复杂度。模型敏感度，也称为鲁棒性，反映了模型在小距离内区分不同输入的能力。Novak等人[[81](#bib.bib81)]
    引入了两个敏感度指标：输入输出Jacobian范数和轨迹长度[[89](#bib.bib89)]。
- en: First, based on the piecewise linear property, Novak et al. [[81](#bib.bib81)]
    propose the Jacobian norm to measure the local sensitivity under the assumption
    that the input is perturbed within the same linear region. This Jacobian norm
    can further represent the effective model complexity. That is,
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基于分段线性特性，Novak等人[[81](#bib.bib81)] 提出了Jacobian范数来测量在输入扰动在同一线性区域内的局部敏感度。该Jacobian范数可以进一步表示有效模型复杂度。即，
- en: '|  | $EMC(\mathcal{N})=\mathbb{E}_{{x}\sim D}[&#124;&#124;J({x})&#124;&#124;_{F}]$
    |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(\mathcal{N})=\mathbb{E}_{{x}\sim D}[&#124;&#124;J({x})&#124;&#124;_{F}]$
    |  |'
- en: where $J(x)=\partial f_{\mathcal{N}}(x)/\partial x^{T}$ is the Jacobian of class
    probabilities, $f_{\mathcal{N}}$ is the function represented by the network $\mathcal{N}$,
    $D$ is the data distribution, and $||\cdot||_{F}$ is the Frobenius norm.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $J(x)=\partial f_{\mathcal{N}}(x)/\partial x^{T}$ 是类别概率的Jacobian，$f_{\mathcal{N}}$
    是由网络 $\mathcal{N}$ 表示的函数，$D$ 是数据分布，$||\cdot||_{F}$ 是Frobenius范数。
- en: Second, they propose a trajectory length metric similar to what developed by
    Raghu et al. [[89](#bib.bib89)] as a sensitivity measure when the input is perturbed
    to other linear regions, further as a measure of effective complexity, written
    as
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，他们提出了一种类似于Raghu等人[[89](#bib.bib89)]所开发的轨迹长度指标，作为当输入扰动到其他线性区域时的敏感度度量，进一步作为有效复杂度的度量，表示为
- en: '|  | $EMC(\mathcal{N})=\mathbb{E}_{{x}\sim D}[t({x})]$ |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(\mathcal{N})=\mathbb{E}_{{x}\sim D}[t({x})]$ |  |'
- en: where $t(x)$ is a trajectory length defined by the number of linear regions
    passing through a trajectory $\tau(x)$, that is,
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t(x)$ 是定义为通过轨迹 $\tau(x)$ 的线性区域数量的轨迹长度，即，
- en: '|  | $\begin{split}t(x)&amp;=\sum_{i=0}^{k-1}&#124;&#124;c(z_{i})-c(z_{(i+1)\%k})&#124;&#124;_{1}\\
    &amp;\approx\oint_{z\in\tau(x)}&#124;&#124;\frac{\partial c(z)}{\partial(dz)}&#124;&#124;_{1}dz\end{split}$
    |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}t(x)&amp;=\sum_{i=0}^{k-1}&#124;&#124;c(z_{i})-c(z_{(i+1)\%k})&#124;&#124;_{1}\\
    &amp;\approx\oint_{z\in\tau(x)}&#124;&#124;\frac{\partial c(z)}{\partial(dz)}&#124;&#124;_{1}dz\end{split}$
    |  |'
- en: where $z_{0},\ldots,z_{k-1}$ are $k$ equidistant points on the trajectory $\tau(x)$,
    $c(z)$ is the status encoding of all hidden neurons at point $z$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z_{0},\ldots,z_{k-1}$ 是轨迹 $\tau(x)$ 上的 $k$ 个等距点，$c(z)$ 是点 $z$ 上所有隐藏神经元的状态编码。
- en: Using these two complexity measures, Novak et al. [[81](#bib.bib81)] study the
    correlation between complexity and generalization. They demonstrate that neural
    networks have strong robustness in the vicinity of the training data manifold
    where deep models have good generalization capability. They also show that the
    factors associated with poor generalization (e.g., full-batch training, random
    labels) correspond to weaker robustness, and the factors associated with good
    generalization (e.g., data augmentation, ReLU) correspond to stronger robustness.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两种复杂度度量，Novak 等人 [[81](#bib.bib81)] 研究了复杂度与泛化之间的关系。他们证明了神经网络在训练数据流形附近具有很强的鲁棒性，其中深度模型具有良好的泛化能力。他们还展示了与较差泛化相关的因素（例如，批量训练、随机标签）对应于较弱的鲁棒性，而与良好泛化相关的因素（例如，数据增强、ReLU）则对应于较强的鲁棒性。
- en: To develop a measure of effective complexity for general smooth activation functions,
    Hu et al. [[44](#bib.bib44)] propose an effective complexity measure for deep
    neural networks with curve activation functions (e.g., Sigmoid [[53](#bib.bib53)],
    Tanh [[48](#bib.bib48)]). Motivated by the piecewise linear property, they suggest
    that, using a piecewise linear function with a minimal number of linear regions
    to approximate a given network, the number of linear regions of the approximation
    function can be a measure of effective complexity of the given network. They learn
    a piecewise linear approximation of a deep neural network with curve activation
    functions, which is called the Linear Approximation Neural Network (LANN for short).
    The LANN is constructed by learning a piecewise linear approximation function
    for the curve activation function on each hidden neuron. Specifically, Hu et al. [[44](#bib.bib44)]
    define the approximation error of a LANN $g$ to the target network $f$ as $\mathcal{E}(g;f)=\mathbb{E}[|g(x)-f(x)|]$.
    They analyze how the approximation error at a certain layer is propagated through
    the remaining hidden layers, and obtain the influence of the approximation error
    of each hidden neuron on $\mathcal{E}(g;f)$. That is,
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为一般平滑激活函数开发有效复杂度度量，Hu 等人 [[44](#bib.bib44)] 提出了适用于具有曲线激活函数（例如，Sigmoid [[53](#bib.bib53)]、Tanh
    [[48](#bib.bib48)]）的深度神经网络的有效复杂度度量。受分段线性特性的启发，他们建议，使用具有最小线性区域数的分段线性函数来近似给定网络，近似函数的线性区域数可以作为给定网络的有效复杂度度量。他们学习了具有曲线激活函数的深度神经网络的分段线性近似，这被称为线性近似神经网络（简称
    LANN）。LANN 通过学习每个隐藏神经元上曲线激活函数的分段线性近似函数来构建。具体地，Hu 等人 [[44](#bib.bib44)] 将 LANN
    $g$ 对目标网络 $f$ 的近似误差定义为 $\mathcal{E}(g;f)=\mathbb{E}[|g(x)-f(x)|]$。他们分析了某一层的近似误差如何传播通过其余隐藏层，并获得了每个隐藏神经元对
    $\mathcal{E}(g;f)$ 的近似误差的影响。也就是说：
- en: '|  | $\small\mathcal{E}(g;f)=\sum_{i,j}\frac{1}{c}\sum(\ &#124;V_{o}&#124;\prod_{q=L}^{i+1}\mathbb{E}[&#124;J_{q}&#124;]\
    )_{*,j}(\mathbb{E}[e_{i,j}]+\mathbb{E}[&#124;\hat{\epsilon}_{i,j}&#124;])$ |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathcal{E}(g;f)=\sum_{i,j}\frac{1}{c}\sum(\ &#124;V_{o}&#124;\prod_{q=L}^{i+1}\mathbb{E}[&#124;J_{q}&#124;]\
    )_{*,j}(\mathbb{E}[e_{i,j}]+\mathbb{E}[&#124;\hat{\epsilon}_{i,j}&#124;])$ |  |'
- en: where $J_{q}$ is the Jacobian matrix of layer $q$ of the network $f$, $e_{i,j}$
    is the approximation error of $g$ on a specific neuron $\{i,j\}$, $V_{o}$ is the
    weight matrix of the output layer, and $\hat{\epsilon}_{i,j}$ is the negligible
    estimation error on neuron $\{i,j\}$. Given an approximation degree $\lambda$,
    the LANN with the smallest number of linear regions is constructed to meet the
    requirement $\mathcal{E}(g;f)\leq\lambda$. The approximated number of linear regions
    of the LANN is used as the effective complexity measure, that is,
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $J_{q}$ 是网络 $f$ 第 $q$ 层的雅可比矩阵，$e_{i,j}$ 是在特定神经元 $\{i,j\}$ 上 $g$ 的近似误差，$V_{o}$
    是输出层的权重矩阵，$\hat{\epsilon}_{i,j}$ 是神经元 $\{i,j\}$ 上的可忽略的估计误差。给定一个近似度 $\lambda$，构建具有最小线性区域数的
    LANN 以满足 $\mathcal{E}(g;f)\leq\lambda$ 的要求。LANN 的近似线性区域数作为有效复杂度度量，即：
- en: '|  | $EMC(f)_{\lambda}=d\sum_{i=1}^{L}\log(\sum_{j=1}^{m_{i}}k_{i,j}-m_{i}+1)$
    |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(f)_{\lambda}=d\sum_{i=1}^{L}\log(\sum_{j=1}^{m_{i}}k_{i,j}-m_{i}+1)$
    |  |'
- en: where $k_{i,j}$ is the number of linear pieces of the approximation function
    on neuron $\{i,j\}$, $m_{i}$ is the width of layer $i$, and $L$ is the depth of
    $f$.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k_{i,j}$ 是神经元 $\{i,j\}$ 上近似函数的线性片段数，$m_{i}$ 是层 $i$ 的宽度，$L$ 是 $f$ 的深度。
- en: 'Using the complexity measure, Hu et al. [[44](#bib.bib44)] investigate the
    trend of model complexity in the training process. They show that the effective
    complexity increases with respect to the number of training iterations. They also
    demonstrate that the occurrence of overfitting is positively correlated with the
    increase of effective complexity, while regularization methods (e.g., $L^{1},L^{2}$
    regularizations) suppress the increase of model complexity (see Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1.1 Piecewise Linear Property ‣ 4.1 General Measures of Effective
    Complexity ‣ 4 Effective Complexity of Deep Learning Models ‣ Model Complexity
    of Deep Learning: A Survey")).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用复杂性度量，胡等人 [[44](#bib.bib44)] 研究了训练过程中模型复杂性的趋势。他们展示了有效复杂性随着训练迭代次数的增加而增加。他们还证明了过拟合的发生与有效复杂性的增加呈正相关，而正则化方法（例如，$L^{1}$、$L^{2}$
    正则化）抑制了模型复杂性的增加（见图 [6](#S4.F6 "图 6 ‣ 4.1.1 分段线性性质 ‣ 4.1 有效复杂性的一般度量 ‣ 4 深度学习模型的有效复杂性
    ‣ 深度学习模型的复杂性：综述")）。
- en: '![Refer to caption](img/03290482b9fe4f6333c991ea39fea8c8.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/03290482b9fe4f6333c991ea39fea8c8.png)'
- en: 'Figure 6: Influence of regularization approaches on effective complexity [[44](#bib.bib44)].
    This figure shows that the decision boundaries of models trained on the Moon dataset.
    NM, L1, L2 are short for normal train, train with $L^{1}$, and $L^{2}$ regularization,
    respectively. In brackets are the value of effective complexity measure.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：正则化方法对有效复杂性的影响 [[44](#bib.bib44)]。该图展示了在 Moon 数据集上训练的模型的决策边界。NM、L1、L2 分别是正常训练、使用
    $L^{1}$ 和 $L^{2}$ 正则化的缩写。括号中是有效复杂性度量的值。
- en: The piecewise linear property may provide novel opportunities for capturing
    model complexity in deep learning. In addition to the above studies on effective
    complexity, piecewise linear neural networks or the piecewise linear property
    can also help the exploration of expressive capacity (see Section 2, [[4](#bib.bib4),
    [36](#bib.bib36), [64](#bib.bib64), [70](#bib.bib70), [92](#bib.bib92)]). Piecewise
    linear activation functions, especially ReLU, are popular and effective activation
    functions in many tasks and applications [[58](#bib.bib58), [70](#bib.bib70)].
    The local linear characteristic and a finite number of regional divisions facilitate
    quantifying and analyzing neural network model complexity with piecewise linear
    activation functions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 分段线性性质可能为深度学习中的模型复杂性捕捉提供新的机会。除了上述对有效复杂性的研究，分段线性神经网络或分段线性性质也可以帮助探索表达能力（见第2节，[
    [4](#bib.bib4), [36](#bib.bib36), [64](#bib.bib64), [70](#bib.bib70), [92](#bib.bib92)
    ]）。分段线性激活函数，特别是 ReLU，是许多任务和应用中流行且有效的激活函数 [[58](#bib.bib58), [70](#bib.bib70)]。局部线性特征和有限数量的区域划分有助于量化和分析具有分段线性激活函数的神经网络模型复杂性。
- en: 4.1.2 Other Measure Metrics
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 其他度量指标
- en: There are other effective complexity measures based on ideas other than the
    piecewise linear property.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他基于不同于分段线性性质的思想的有效复杂性度量。
- en: Nakkiran et al. [[74](#bib.bib74)] introduce an effective complexity measure
    by investigating the double descent phenomenon. The double descent phenomenon
    of deep neural networks is that, as the model size, training epochs, or size of
    training data increases, the test performance often first decreases and then increases.
    They suggest that to help capture the double descent effect in training, a complexity
    measure should be sensitive to training processes, data distribution, and model
    architectures. They propose such an effective complexity measure, that is, the
    maximum number of samples on which the model can be trained to close to zero training
    error, written as
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Nakkiran 等人 [[74](#bib.bib74)] 通过研究双重下降现象引入了一种有效复杂性度量。深度神经网络的双重下降现象是指，随着模型大小、训练周期或训练数据大小的增加，测试性能通常先下降后上升。他们建议，为了帮助捕捉训练中的双重下降效应，复杂性度量应对训练过程、数据分布和模型架构敏感。他们提出了一种有效的复杂性度量，即模型能够在接近零训练误差的情况下训练的最大样本数，表示为
- en: '|  | $EMC_{D,\epsilon}(\mathcal{T})=\max\ \{n&#124;\mathbb{E}_{S\sim D}[Error_{S}(\mathcal{T}(S))]\leq\epsilon\}$
    |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC_{D,\epsilon}(\mathcal{T})=\max\ \{n&#124;\mathbb{E}_{S\sim D}[Error_{S}(\mathcal{T}(S))]\leq\epsilon\}$
    |  |'
- en: where $D$ is the data distribution, $\epsilon$ is the threshold of training
    error, $\mathcal{T}$ is the training procedure, $Error_{S}$ is the mean error
    of the model on the training set $S$, and $n$ the size of training set $S$. Based
    on the effective complexity measure $EMC_{D,\epsilon}(\mathcal{T})$, they investigate
    the evolution of the training process and show that, if $EMC_{D,\epsilon}(\mathcal{T})$
    is sufficiently smaller or sufficiently larger than $n$, the size of the training
    set, then any perturbation of $\mathcal{T}$ that increases its effective complexity
    $EMC_{D,\epsilon}(\mathcal{T})$ helps to improve the test performance. However,
    if $EMC_{D,\epsilon}(\mathcal{T})\approx n$, then any perturbation of $\mathcal{T}$
    that increases its effective complexity $EMC_{D,\epsilon}(\mathcal{T})$ hurts
    the test performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$D$是数据分布，$\epsilon$是训练误差的阈值，$\mathcal{T}$是训练过程，$Error_{S}$是模型在训练集$S$上的均值误差，$n$是训练集$S$的大小。基于有效复杂度测量$EMC_{D,\epsilon}(\mathcal{T})$，他们调查了训练过程的演变，并展示了如果$EMC_{D,\epsilon}(\mathcal{T})$比训练集的大小$n$小得多或大得多，则任何增加其有效复杂度$EMC_{D,\epsilon}(\mathcal{T})$的$\mathcal{T}$扰动都有助于改善测试性能。然而，如果$EMC_{D,\epsilon}(\mathcal{T})\approx
    n$，那么任何增加其有效复杂度$EMC_{D,\epsilon}(\mathcal{T})$的$\mathcal{T}$扰动都会损害测试性能。
- en: To approach the generalization problem of deep learning models, Liang et al. [[60](#bib.bib60)]
    introduce a new notion of model complexity measure, the Fisher-Rao norm. Their
    study focuses on deep fully connected neural networks whose activation function
    $\sigma(\cdot)$ satisfies $\sigma(z)=\sigma^{\prime}(z)z$. The model complexity
    represented by Fisher-Rao norm is given by
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决深度学习模型的泛化问题，Liang等人[[60](#bib.bib60)]引入了模型复杂度测量的新概念，即Fisher-Rao范数。他们的研究集中在深度全连接神经网络上，其激活函数$\sigma(\cdot)$满足$\sigma(z)=\sigma^{\prime}(z)z$。由Fisher-Rao范数表示的模型复杂度为
- en: '|  | $EMC(\mathcal{N})=&#124;&#124;\theta&#124;&#124;^{2}_{fr}=\langle\theta,I(\theta)\theta\rangle$
    |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(\mathcal{N})=&#124;&#124;\theta&#124;&#124;^{2}_{fr}=\langle\theta,I(\theta)\theta\rangle$
    |  |'
- en: where $\theta$ is the set of parameters (i.e., weights, bias) of neural network
    $\mathcal{N}$, $\langle\cdot,\cdot\rangle$ is the inner product, and $I(\theta)$
    is the Fisher information matrix, written as
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta$是神经网络$\mathcal{N}$的参数集合（即权重、偏置），$\langle\cdot,\cdot\rangle$是内积，$I(\theta)$是Fisher信息矩阵，表示为
- en: '|  | $I(\theta)=\mathbb{E}[\nabla_{\theta}\ell(\mathcal{N}(X),Y)\otimes\nabla_{\theta}\ell(\mathcal{N}(X),Y)]$
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(\theta)=\mathbb{E}[\nabla_{\theta}\ell(\mathcal{N}(X),Y)\otimes\nabla_{\theta}\ell(\mathcal{N}(X),Y)]$
    |  |'
- en: where $\ell(\cdot,\cdot)$ is the loss function and $\otimes$ is the tensor product.
    Liang et al. [[60](#bib.bib60)] introduce the geometric invariance property that
    a complexity measure should satisfy in order to study generalization capability.
    The invariance property essentially says that, since many different continuous
    operations may lead to exactly the same prediction, thus the generalization should
    only depend on the equivalence classes obtained by these continuous transformations.
    Specific parameterization should not affect the generalization. The complexity
    measure used to investigate generalization should satisfy the invariance property.
    They demonstrate that this Fisher-Rao norm satisfies the invariance property.
    Let $\theta_{1},\theta_{2}$ denote two parameter settings of a model $f$. If $f_{\theta_{1}}=f_{\theta_{2}}$,
    then their Fisher-Rao norms are equal, that is, $||\theta_{1}||_{fr}=||\theta_{2}||_{fr}$.
    In particular, the Fisher-Rao norm remains invariant during the node-wise rescaling
    of a model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\ell(\cdot,\cdot)$是损失函数，$\otimes$是张量积。Liang等人[[60](#bib.bib60)]引入了复杂度测量应该满足的几何不变性属性，以研究泛化能力。这一不变性属性本质上表示，由于许多不同的连续操作可能导致完全相同的预测，因此泛化应该仅依赖于这些连续变换所获得的等价类。特定的参数化不应影响泛化。用于研究泛化的复杂度测量应该满足不变性属性。他们证明了这种Fisher-Rao范数满足不变性属性。设$\theta_{1},\theta_{2}$表示模型$f$的两个参数设置。如果$f_{\theta_{1}}=f_{\theta_{2}}$，那么它们的Fisher-Rao范数是相等的，即$||\theta_{1}||_{fr}=||\theta_{2}||_{fr}$。特别地，Fisher-Rao范数在模型的节点级别重新缩放过程中保持不变。
- en: Effective complexity can be measured by the size of training samples that a
    model achieves zero training error [[74](#bib.bib74)], or by the Fisher-Rao metric [[60](#bib.bib60)].
    More effective complexity measures along the line may be developed.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有效复杂度可以通过模型达到零训练误差的训练样本大小[[74](#bib.bib74)]或通过Fisher-Rao度量[[60](#bib.bib60)]来衡量。沿着这个方向，可能会开发出更多的有效复杂度测量方法。
- en: 4.2 High-Capacity Low-Reality Phenomenon
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 高容量低现实现象
- en: Several studies explore the gap between the effective complexity and the expressive
    capacity of deep learning models.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了深度学习模型的有效复杂度与表达能力之间的差距。
- en: Ba and Caruana [[5](#bib.bib5)] show that shallow fully connected neural networks
    can learn complex functions previously learned by deep neural networks, sometimes
    even only requiring the same number of parameters as the deep networks. Specifically,
    given a well-trained deep model, they propose to train a shallow model based on
    the outputs of the deep model, to mimic the deep model. They show that, the shallow
    mimic model can achieve an accuracy as high as the deep model. However, the shallow
    model cannot be trained directly on the original labeled training data to achieve
    the same accuracy. This is also well recognized as knowledge distillation [[41](#bib.bib41)].
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Ba 和 Caruana [[5](#bib.bib5)] 证明了浅层全连接神经网络能够学习到深层神经网络之前学习过的复杂函数，有时甚至只需与深层网络相同数量的参数。具体来说，给定一个训练良好的深层模型，他们建议基于深层模型的输出训练一个浅层模型，以模仿深层模型。他们显示，浅层模仿模型可以达到与深层模型一样高的准确率。然而，浅层模型不能直接在原始标记训练数据上进行训练以实现相同的准确率。这也被广泛认可为知识蒸馏
    [[41](#bib.bib41)]。
- en: Based on this phenomenon, Ba and Caruana [[5](#bib.bib5)] conjecture that the
    strength of deep learning may arise in part from a good match between deep architectures
    and current training algorithms. That is, compared with shallow architectures,
    deep architectures may be easier to train by the current optimization techniques.
    Moreover, they propose that when it is able to mimic the function learned by a
    deep complex model using a shallow model, the function learned by the deep model
    is not really too complicated to learn. This study suggests that there may be
    a big gap between the practical effective complexity of a deep learning model
    and the theoretical bound of its expressive capacity. We call this the *high-capacity
    low-reality phenomenon*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一现象，Ba 和 Caruana [[5](#bib.bib5)] 推测深度学习的强大可能部分源于深度架构与当前训练算法的良好匹配。也就是说，与浅层架构相比，深度架构可能更容易通过当前的优化技术进行训练。此外，他们提出，当能够使用浅层模型模仿深层复杂模型学习的函数时，深层模型学习的函数并没有真正复杂到难以学习的程度。这项研究表明，深度学习模型的实际有效复杂度与其表达能力的理论上限之间可能存在较大差距。我们称之为
    *高容量低现实现象*。
- en: Hanin and Rolnick [[37](#bib.bib37)] investigate the high-capacity low-reality
    phenomenon in fully connected neural networks with piecewise linear activation
    functions, especially ReLU. They propose two effective complexity measures using
    the number of linear regions in the input space and the volume of boundaries between
    these linear regions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Hanin 和 Rolnick [[37](#bib.bib37)] 研究了具有分段线性激活函数的全连接神经网络中的高容量低现实现象，特别是 ReLU。他们提出了两种有效的复杂度度量方法，分别是利用输入空间中的线性区域数量和这些线性区域之间的边界体积。
- en: First, they investigate ReLU neural networks whose dimensionality of input and
    that of output are both equal to 1, and use the number of linear regions as the
    effective complexity measure. They show that the average number of linear regions
    grows linearly with respect to the total number of neurons, far below the exponential
    upper bound. Specifically, given a ReLU neural network $\mathcal{N}:\mathbb{R}\rightarrow\mathbb{R}$
    whose weights and biases of neurons $z$ are randomly initialized and are bounded
    by $\mathbb{E}[||\nabla z(x)||]\leq C$ for some $C>0$, the average number of linear
    regions is proportional to the product of the number of neurons and the size of
    training set, that is,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们研究了输入和输出维度均为 1 的 ReLU 神经网络，并使用线性区域的数量作为有效复杂度度量。他们显示，线性区域的平均数量随着神经元总数的增加而线性增长，远低于指数上界。具体而言，给定一个
    ReLU 神经网络 $\mathcal{N}:\mathbb{R}\rightarrow\mathbb{R}$，其神经元 $z$ 的权重和偏置是随机初始化的，并且被
    $\mathbb{E}[||\nabla z(x)||]\leq C$ （其中 $C>0$）所限制，则线性区域的平均数量与神经元数量和训练集大小的乘积成正比，即，
- en: '|  | $\mathbb{E}[\#\{linear\ regions\ in\ S\}]\approx&#124;S&#124;\cdot T\cdot
    M$ |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[\#\{linear\ regions\ in\ S\}]\approx&#124;S&#124;\cdot T\cdot
    M$ |  |'
- en: where $S$ is the training dataset, $T$ is the number of breakpoints in the activation
    function (for ReLU, $T=1$), and $M$ is the total number of hidden neurons.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S$ 是训练数据集，$T$ 是激活函数中的断点数（对于 ReLU，$T=1$），$M$ 是隐藏神经元的总数。
- en: Second, they investigate ReLU neural networks whose input dimensionality exceeds
    1, denoted by $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}\ (d>1)$, and use
    the volume of boundaries between linear regions in the input space as an estimation
    of model complexity. That is,
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，他们研究了输入维度超过1的ReLU神经网络，用 $\mathcal{N}:\mathbb{R}^{d}\rightarrow\mathbb{R}\
    (d>1)$ 表示，并使用输入空间中线性区域之间边界的体积作为模型复杂性的估计。也就是说，
- en: '|  | $EMC(\mathcal{N})=\frac{volume_{d-1}(B_{\mathcal{N}}\cap K)}{volume_{d}(K)}$
    |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $EMC(\mathcal{N})=\frac{volume_{d-1}(B_{\mathcal{N}}\cap K)}{volume_{d}(K)}$
    |  |'
- en: where $B_{\mathcal{N}}=\{x|\nabla\mathcal{N}(x)\text{ is not continuous at }x\}$
    represents the boundaries of the linear regions formed by $\mathcal{N}$, $K\in\mathbb{R}^{d}$
    is the data distribution. They prove that, under the same parameter initialization
    assumption as $d=1$, the expected value of the volume of linear region boundaries
    is approximately equal to
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $B_{\mathcal{N}}=\{x|\nabla\mathcal{N}(x)\text{ 在 }x\text{ 处不连续}\}$ 表示由 $\mathcal{N}$
    形成的线性区域的边界，$K\in\mathbb{R}^{d}$ 是数据分布。他们证明了，在与 $d=1$ 相同的参数初始化假设下，线性区域边界体积的期望值大致等于
- en: '|  | $\mathbb{E}[EMC(\mathcal{N})]\approx T\cdot M$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[EMC(\mathcal{N})]\approx T\cdot M$ |  |'
- en: This demonstrates that the average size of the region boundaries depends only
    on the number of neurons, not on the depth of the model. They conclude that the
    effective complexity of deep neural networks may be much lower than the theoretical
    bound. That is, the function learned by deep neural networks may not be more complex
    than that learned by shallow ones.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明区域边界的平均大小仅依赖于神经元的数量，而与模型的深度无关。他们得出结论，深度神经网络的有效复杂性可能远低于理论上限。也就是说，深度神经网络学习到的函数可能不会比浅层网络学习到的函数更复杂。
- en: 4.3 Discussion
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 讨论
- en: Effective model complexity is a relatively new, promising and useful problem
    in deep learning. Detecting effective model complexity during training helps to
    investigate the usefulness of optimization algorithms [[47](#bib.bib47)], the
    role of regularizations [[44](#bib.bib44), [89](#bib.bib89)], and generalization
    capability [[60](#bib.bib60), [81](#bib.bib81)]. Furthermore, effective model
    complexity can be used to describe model compression ratio, since effective model
    complexity can be considered as a reflection of the information volume in the
    model [[32](#bib.bib32)]. Effective complexity can also be used for model selection
    and design to balance resource utilization and model performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 有效模型复杂性是深度学习中一个相对较新、有前景且有用的问题。在训练过程中检测有效模型复杂性有助于研究优化算法的有效性 [[47](#bib.bib47)]、正则化的作用
    [[44](#bib.bib44), [89](#bib.bib89)] 以及泛化能力 [[60](#bib.bib60), [81](#bib.bib81)]。此外，有效模型复杂性可以用来描述模型压缩比，因为有效模型复杂性可以视为模型信息量的反映
    [[32](#bib.bib32)]。有效复杂性还可以用于模型选择和设计，以平衡资源利用和模型性能。
- en: In addition to the effective complexity measures and the high-capacity low-reality
    phenomenon, there are a series of interesting problems about effective complexity
    of deep learning models. For example, the cross-model comparison of effective
    complexity is worth exploring. That is, how to compare the effective complexity
    of multiple models with different architectures, and how the effective complexity
    is affected by the choice of different model architectures. Moreover, can one
    specify the granularity of effective complexity measures? Different cases may
    have different requirements for effective complexity. Correspondingly, the application
    scopes and granularities of effective complexity measures should be specified
    and clarified. Typically, effective complexity measure by the number of non-zero
    parameters is obviously not sufficient to study the optimization process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 除了有效复杂性度量和高容量低现实现象外，还有一系列关于深度学习模型有效复杂性的有趣问题。例如，跨模型有效复杂性的比较值得探索。即，如何比较具有不同架构的多个模型的有效复杂性，以及不同模型架构的选择如何影响有效复杂性。此外，是否可以指定有效复杂性度量的粒度？不同情况可能对有效复杂性有不同的要求。因此，有效复杂性度量的应用范围和粒度应予以指定和澄清。通常，通过非零参数数量来度量有效复杂性显然不足以研究优化过程。
- en: 5 Application Examples of Deep Learning Model Complexity
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习模型复杂性的5个应用示例
- en: Model complexity of deep learning has many applications. In this section, we
    review three interesting applications of deep learning model complexity, namely
    understanding model generalization capability, model optimization, and model selection
    and design.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的模型复杂度有许多应用。在本节中，我们回顾了深度学习模型复杂度的三个有趣应用，即理解模型泛化能力、模型优化以及模型选择和设计。
- en: 5.1 Model Complexity in Understanding Model Generalization Capability
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 模型复杂度在理解模型泛化能力中的作用
- en: Deep learning models are always over-parameterized, that is, they have far more
    model parameters than the optimal solutions and the number of training samples.
    However, it is often found that large over-parameterized neural networks exhibit
    good generalization capability [[49](#bib.bib49), [76](#bib.bib76), [100](#bib.bib100)].
    Some studies even find that larger and more complex networks usually generalize
    better [[81](#bib.bib81)]. This observation is in contradiction with the classical
    notion of function complexity [[81](#bib.bib81)] and the well-known Occam’s razor [[90](#bib.bib90)],
    which prefer simple models. What leads to the good generalization capability of
    over-parameterized deep learning models?
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型总是过度参数化的，即它们的模型参数远多于最优解和训练样本的数量。然而，通常发现大型过度参数化的神经网络表现出良好的泛化能力 [[49](#bib.bib49),
    [76](#bib.bib76), [100](#bib.bib100)]。一些研究甚至发现，更大、更复杂的网络通常泛化得更好 [[81](#bib.bib81)]。这一观察与函数复杂度的经典观念 [[81](#bib.bib81)]
    和著名的奥卡姆剃刀 [[90](#bib.bib90)] 相矛盾，后者偏好简单模型。是什么导致了过度参数化的深度学习模型具有良好的泛化能力？
- en: In statistical learning theory, expressive capacity (i.e., hypothesis space
    complexity) is used to bound generalization error [[69](#bib.bib69)]. Specifically,
    let $\mathcal{F}$ be the set of functions representable by a certain model structure.
    Let $f_{A(D)}$ be a function $f\in\mathcal{F}$ learned by algorithm $A$ on training
    dataset $D$. Let $E_{D}(f_{A(D)})$ be the emperical error of $f_{A(D)}$ and $E(f_{A(D)})$
    the generalization error of $f_{A(D)}$. The gap between generalization error and
    emperical error is bounded by
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学习理论中，表达能力（即假设空间复杂度）用于界定泛化误差 [[69](#bib.bib69)]。具体来说，设 $\mathcal{F}$ 为由某个模型结构表示的函数集合。设
    $f_{A(D)}$ 为算法 $A$ 在训练数据集 $D$ 上学习的函数 $f\in\mathcal{F}$。设 $E_{D}(f_{A(D)})$ 为 $f_{A(D)}$
    的经验误差，$E(f_{A(D)})$ 为 $f_{A(D)}$ 的泛化误差。泛化误差和经验误差之间的差距由下式界定：
- en: '|  | $E(f_{A(D)})-E_{D}(f_{A(D)})\leq\sup_{f\in\mathcal{F}}\{E(f)-E_{D}(f)\}$
    |  | (4) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(f_{A(D)})-E_{D}(f_{A(D)})\leq\sup_{f\in\mathcal{F}}\{E(f)-E_{D}(f)\}$
    |  | (4) |'
- en: 'The right-hand side can be quantified by analyzing the expressive capacity (e.g.,
    Rademacher complexity) [[49](#bib.bib49)]. For example, Zheng et al. [[101](#bib.bib101)]
    analyze generalization error of deep ReLU neural networks using the basis-path
    norm, a norm measure based on the basis paths. Zheng et al. [[101](#bib.bib101)]
    suggest that there exist a small group of basis paths in a ReLU neural network,
    which are linearly independent. Each input-output path of a ReLU neural network
    can be expressed in the form of multiplication and division of the basis paths.
    Therefore, the basis paths can be used to explain the generalization behavior
    of ReLU neural networks. Zheng et al. [[101](#bib.bib101)] prove that the generalization
    error of ReLU neural networks (i.e. the right-hand side of Eq. ([4](#S5.E4 "In
    5.1 Model Complexity in Understanding Model Generalization Capability ‣ 5 Application
    Examples of Deep Learning Model Complexity ‣ Model Complexity of Deep Learning:
    A Survey"))) can be bounded by a function of basis-path norm.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧可以通过分析表达能力（例如，Rademacher 复杂度）[[49](#bib.bib49)] 来量化。例如，郑 等人 [[101](#bib.bib101)]
    使用基路径范数分析深度 ReLU 神经网络的泛化误差，这是一种基于基路径的范数度量。郑 等人 [[101](#bib.bib101)] 表示，ReLU 神经网络中存在一小组基路径，它们是线性独立的。ReLU
    神经网络的每一个输入输出路径可以用基路径的乘法和除法的形式来表示。因此，基路径可以用来解释 ReLU 神经网络的泛化行为。郑 等人 [[101](#bib.bib101)]
    证明，ReLU 神经网络的泛化误差（即 Eq. ([4](#S5.E4 "在 5.1 模型复杂度在理解模型泛化能力中的作用 ‣ 深度学习模型复杂度的应用示例
    ‣ 深度学习的模型复杂度：综述")) 的右侧）可以由基路径范数的函数来界定。
- en: 'A series of studies investigate model complexity measures that can explain
    generalization capability of deep learning models [[2](#bib.bib2), [60](#bib.bib60),
    [76](#bib.bib76), [81](#bib.bib81)]. Neyshabur et al. [[76](#bib.bib76)] suggests
    that, from the perspective of generalization, a complexity measure should satisfy
    the following property: a learned model with lower complexity generalizes better.
    In particular, they list several requirements which are summarized from observed
    empirical phenomena and are expected to be satisfied by complexity measures.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列研究探讨了可以解释深度学习模型泛化能力的模型复杂度度量[[2](#bib.bib2), [60](#bib.bib60), [76](#bib.bib76),
    [81](#bib.bib81)]。Neyshabur 等人[[76](#bib.bib76)] 提出，从泛化的角度来看，复杂度度量应满足以下属性：具有较低复杂度的学习模型具有更好的泛化能力。他们特别列出了几个要求，这些要求总结自观察到的经验现象，并且预期复杂度度量应满足这些要求。
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: With zero training error, a network trained on real labels, which leads to good
    generalization, is expected to have much lower complexity than a network trained
    on random labels.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在零训练误差的情况下，训练在真实标签上的网络，虽然具有良好的泛化能力，预期复杂度要远低于训练在随机标签上的网络。
- en: •
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Increasing the number of hidden units or the number of parameters, which leads
    to a decreased generalization error, is expected to decrease the complexity measure.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加隐藏单元数量或参数数量，虽然会导致泛化误差减少，但预期会降低复杂度度量。
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When training the same architecture on the same training dataset using two different
    optimization algorithms, if both lead to zero training errors, the model with
    better generalization is expected to have lower complexity.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当使用两种不同的优化算法在相同训练数据集上训练相同架构时，如果两者都导致零训练误差，则预期泛化更好的模型具有较低的复杂度。
- en: Based on these desiderata, Neyshabur et al. [[76](#bib.bib76)] investigate several
    complexity measures including norms [[79](#bib.bib79)], robustness [[97](#bib.bib97)],
    and sharpness [[50](#bib.bib50)]. They show that, these measures can meet some
    of the above requirements, but not all.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些要求，Neyshabur 等人[[76](#bib.bib76)] 研究了几种复杂度度量，包括范数[[79](#bib.bib79)]、鲁棒性[[97](#bib.bib97)]
    和锐度[[50](#bib.bib50)]。他们表明，这些度量可以满足上述要求中的一些，但不是全部。
- en: Novak et al. [[81](#bib.bib81)] define two complexity measures from the perspective
    of model sensitivity, and identify an empirical correlation between the complexity
    measures and model generalization capability. They show that operations that lead
    to poor generalization, such as full batch training, correspond to high sensitivity,
    and in turn imply high effective model complexity. Similarly, operations that
    lead to good generalization, such as data augmentation, correspond to low sensitivity,
    and thus imply low effective model complexity.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 诺瓦克等人[[81](#bib.bib81)] 从模型敏感性的角度定义了两个复杂度度量，并确定了复杂度度量与模型泛化能力之间的经验相关性。他们表明，导致泛化不佳的操作，如全批量训练，对应于高敏感性，并且这反过来意味着较高的有效模型复杂度。同样，导致泛化良好的操作，如数据增强，对应于低敏感性，从而意味着较低的有效模型复杂度。
- en: Liang et al. [[60](#bib.bib60)] define a complexity measure using the Fisher-Rao
    norm to investigate model generalization capability. They suggest that a complexity
    measure used to study generalization should satisfy the invariance property. The
    invariance property requires that the generalization capacity depends on the equivalence
    classes obtained by deep models. In other words, many different parameterizations
    may lead to the same prediction. Thus, the specific parameterization of deep models
    should not affect the generalization and the complexity measure. They show that
    the Fisher-Rao norm honors this invariance property and thus is able to explain
    the generalization capability of deep learning models.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人[[60](#bib.bib60)] 定义了一种使用 Fisher-Rao 范数的复杂度度量，以研究模型的泛化能力。他们建议，用于研究泛化的复杂度度量应满足不变性属性。不变性属性要求泛化能力取决于深度模型所获得的等价类。换句话说，许多不同的参数化可能导致相同的预测。因此，深度模型的具体参数化不应影响泛化能力和复杂度度量。他们表明，Fisher-Rao
    范数符合这种不变性属性，因此能够解释深度学习模型的泛化能力。
- en: 5.2 Model Complexity in Optimization
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 优化中的模型复杂度
- en: Model optimization is concerned about how and why a neural network model can
    be successfully trained [[86](#bib.bib86), [94](#bib.bib94)]. Specifically, the
    optimization of a deep model is to determine model parameters to minimize a loss
    function in general non-convex. The loss function is typically designed based
    on the understanding of a problem and the requirements of the model, and thus
    generally includes a performance measure, which is evaluated on the training set,
    and other constraint terms [[34](#bib.bib34)].
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化关注的是如何以及为何神经网络模型能够成功训练 [[86](#bib.bib86), [94](#bib.bib94)]。具体而言，深度模型的优化是确定模型参数，以最小化一般非凸的损失函数。损失函数通常根据对问题的理解和模型的要求设计，因此通常包括一个性能度量，该度量在训练集上进行评估，以及其他约束项 [[34](#bib.bib34)]。
- en: Model complexity is widely used to provide a metric to make optimization traceable.
    For example, a measure metric of the effective model complexity of neural networks
    helps to monitor the changes of a model during the optimization process and understand
    how the optimization process progresses [[44](#bib.bib44), [47](#bib.bib47), [74](#bib.bib74),
    [89](#bib.bib89)]. Such a metric also helps to verify the effectiveness of new
    improvements of optimization algorithms [[39](#bib.bib39)]. For example, Nakkiran et
    al. [[74](#bib.bib74)] investigate the double descent phenomenon during training
    using effective complexity measured by the maximal size of the dataset on which
    zero training error can be achieved. They show that the double descent phenomenon
    can be represented as a function of the effective complexity. Raghu et al. [[89](#bib.bib89)]
    and Hu et al. [[44](#bib.bib44)] propose new regularization methods and demonstrate
    the effectiveness of these regularizations through their impact on complexity.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂性被广泛用于提供一种度量，使得优化过程可以被追踪。例如，神经网络有效模型复杂性的度量指标有助于监控模型在优化过程中的变化，并理解优化过程的进展 [[44](#bib.bib44),
    [47](#bib.bib47), [74](#bib.bib74), [89](#bib.bib89)]。这样的指标也有助于验证优化算法新改进的有效性 [[39](#bib.bib39)]。例如，Nakkiran et
    al. [[74](#bib.bib74)] 使用通过最大数据集大小测量的有效复杂性研究了训练过程中的双重下降现象。他们展示了双重下降现象可以表示为有效复杂性的函数。Raghu et
    al. [[89](#bib.bib89)] 和Hu et al. [[44](#bib.bib44)] 提出了新的正则化方法，并通过这些正则化对复杂性的影响展示了其有效性。
- en: The study of model complexity inspires explorations on the effectiveness of
    optimization approaches. Hanin and Rolnick [[37](#bib.bib37)] use the boundary
    volumes of linear regions as the complexity measure of ReLU neural networks, and
    find that during the training, the average boundary volume is always linearly
    proportional to the number of neurons, irrelevant to the depth of the model. This
    demonstrates that deep models do not always learn more complex functions than
    shallow ones, the success of deep learning may be related to optimization algorithms.
    Ba and Caruana [[5](#bib.bib5)] suggest that the great performance of deep learning
    may be due to the fact that deep models are easier to train than shallow architectures
    using the current optimization algorithms [[37](#bib.bib37), [81](#bib.bib81)].
    This calls for further exploration of the effectiveness of optimization approaches
    and the relationship with model structures.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂性的研究激发了对优化方法有效性的探索。Hanin和Rolnick [[37](#bib.bib37)] 使用线性区域的边界体积作为ReLU神经网络复杂性的度量，并发现训练过程中，平均边界体积始终与神经元数量线性成正比，与模型的深度无关。这表明深度模型不一定比浅层模型学习到更复杂的函数，深度学习的成功可能与优化算法有关。Ba和Caruana [[5](#bib.bib5)]
    认为，深度学习的优异表现可能是由于当前优化算法使得深度模型比浅层架构更容易训练 [[37](#bib.bib37), [81](#bib.bib81)]。这要求进一步探索优化方法的有效性及其与模型结构的关系。
- en: 5.3 Model Complexity in Model Selection and Design
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 模型选择和设计中的模型复杂性
- en: Given a specific learning task, how can we determine a feasible model structure
    for the task? Given a variety of models with different architectures and different
    complexity, how can we pick the best model from them? This is the model selection
    and design problem [[71](#bib.bib71)].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定的学习任务下，我们如何确定一个可行的模型结构？在各种不同架构和复杂度的模型中，我们如何选择最佳模型？这就是模型选择和设计问题 [[71](#bib.bib71)]。
- en: In general, model selection and design is based on the tradeoff between prediction
    performance and model complexity [[61](#bib.bib61), [72](#bib.bib72)]. On one
    hand, making predictions with high accuracy is the essential goal of learning
    a model [[69](#bib.bib69)]. A model is expected to be able to capture the underlying
    patterns hidden in the training data and achieve predictions of accuracy as high
    as possible. In order to represent a large amount of knowledge and obtain high
    accuracy, a model with a high expressive capacity, a large degree of freedom and
    a large training set is required [[13](#bib.bib13)]. To this extent, a model with
    more parameters and higher complexity is favored. On the other hand, an overly
    complex model may be difficult to train and may incur unnecessary resource consumption,
    such as storage, computation and time cost [[72](#bib.bib72)]. Unnecessary resource
    consumption should be avoided particularly in practical large scale applications [[42](#bib.bib42)].
    To this extent, a simpler model with comparable accuracy is preferred than a more
    complicated one.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型选择和设计是基于预测性能与模型复杂性之间的权衡[[61](#bib.bib61), [72](#bib.bib72)]。一方面，具有高准确性的预测是学习模型的基本目标[[69](#bib.bib69)]。期望模型能够捕捉训练数据中隐藏的潜在模式，并实现尽可能高的预测准确性。为了表示大量知识并获得高准确性，需要一个具有高表达能力、大自由度和大训练集的模型[[13](#bib.bib13)]。在这方面，更具参数和更高复杂性的模型更受青睐。另一方面，过于复杂的模型可能难以训练，并且可能导致不必要的资源消耗，如存储、计算和时间成本[[72](#bib.bib72)]。特别是在实际的大规模应用中，应避免不必要的资源消耗[[42](#bib.bib42)]。因此，相比更复杂的模型，更倾向于选择具有可比准确性的简单模型。
- en: To maintain a good tradeoff between accuracy and complexity, a model selected
    is expected to be complex enough to fit the given data and achieve high accuracy.
    At the same time, the model should not be highly over-complicated. Understanding
    model complexity and developing an effective complexity measure are the premise
    for good model selection strategies. For instance, Michel and Nouy [[68](#bib.bib68)]
    propose a model selection strategy for tree tensor networks (i.e., sum-product
    neural networks). Their method combines the empirical risk minimization and model
    complexity penalty to select a model from a family of models.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在准确性和复杂性之间保持良好的权衡，所选择的模型应足够复杂以适应给定的数据并实现高准确性。同时，模型不应过于复杂。理解模型复杂性并制定有效的复杂性度量是制定良好模型选择策略的前提。例如，Michel和Nouy[[68](#bib.bib68)]提出了一种用于树张量网络（即求和-乘积神经网络）的模型选择策略。他们的方法结合了经验风险最小化和模型复杂性惩罚，从模型族中选择出一个模型。
- en: Neural architecture search (NAS for short) is a popular solution to deep learning
    model selection [[57](#bib.bib57), [62](#bib.bib62), [63](#bib.bib63), [103](#bib.bib103)],
    which automatically selects a good neural network architecture for a given learning
    task. Since an overly complex model may take too-long training time and thus may
    become a serious obstacle of neural architecture search [[57](#bib.bib57), [62](#bib.bib62)],
    the accuracy-complexity tradeoff is an important consideration in neural architecture
    search. Liu et al. [[62](#bib.bib62)] propose Progressive Neural Architecture
    Search, which searches for convolutional neural network architectures in the increasing
    order of model complexity. Therefore, Progressive Neural Architecture Search favors
    low complexity models that meet the requirement on prediction accuracy. Laredo et
    al. [[57](#bib.bib57)] propose Automatic Model Selection, which searches for fully
    connected neural networks that yield a good balance between prediction accuracy
    and model complexity.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构搜索（简称 NAS）是深度学习模型选择的一个热门解决方案[[57](#bib.bib57), [62](#bib.bib62), [63](#bib.bib63),
    [103](#bib.bib103)]，它自动选择适合给定学习任务的良好神经网络架构。由于过于复杂的模型可能需要过长的训练时间，从而可能成为神经网络架构搜索的一个严重障碍[[57](#bib.bib57),
    [62](#bib.bib62)]，因此准确性与复杂性的权衡是神经网络架构搜索中的一个重要考虑因素。Liu等人[[62](#bib.bib62)]提出了渐进式神经网络架构搜索，它按照模型复杂性递增的顺序搜索卷积神经网络架构。因此，渐进式神经网络架构搜索倾向于选择满足预测准确性要求的低复杂性模型。Laredo等人[[57](#bib.bib57)]提出了自动模型选择，它搜索那些在预测准确性和模型复杂性之间取得良好平衡的全连接神经网络。
- en: Radosavovic et al. [[88](#bib.bib88)] investigate the network design spaces
    of model selection and design approaches. They propose to compare design spaces
    by contrasting the estimated distributions of model complexity in the network
    design spaces. Using their proposed method of comparing the distributions of model
    complexity of network design space, they investigate several popular NAS approaches,
    such as ENAS [[85](#bib.bib85)] and DARTS [[63](#bib.bib63)], and find that there
    are significant differences between the network design spaces of these approaches.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Radosavovic 等人[[88](#bib.bib88)] 调查了模型选择和设计方法的网络设计空间。他们提出通过对比网络设计空间中模型复杂性的估计分布来比较设计空间。使用他们提出的比较网络设计空间中模型复杂性的分布的方法，他们调查了几种流行的NAS方法，如
    ENAS [[85](#bib.bib85)] 和 DARTS [[63](#bib.bib63)]，并发现这些方法的网络设计空间之间存在显著差异。
- en: 6 Conclusions and Future Directions
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来方向
- en: 'In this paper, we survey model complexity in deep learning. We summarize four
    aspects affecting deep learning model complexity, and two angles to overview existing
    studies on deep learning model complexity. We discuss the two major problems of
    deep learning model complexity, namely the model expressive capacity and effective
    model complexity. We overview the state-of-the-art studies on the expressive capacity
    from four aspects: depth efficiency, width efficiency, expressible functional
    space, and VC dimension and Rademacher complexity. We overview the state-of-the-art
    studies on the effective complexity from two aspects: general measures of effective
    complexity and the high-capacity low-reality phenomenon. We discuss the application
    of deep learning model complexity, especially in generalization capability, optimization,
    model selection and design.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们综述了深度学习中的模型复杂性。我们总结了影响深度学习模型复杂性的四个方面，以及从两个角度概述现有关于深度学习模型复杂性的研究。我们讨论了深度学习模型复杂性的两个主要问题，即模型表达能力和有效模型复杂性。我们从四个方面概述了关于表达能力的前沿研究：深度效率、宽度效率、可表达的功能空间以及VC维度和Rademacher复杂性。我们从两个方面概述了关于有效复杂性的前沿研究：有效复杂性的一般度量和高容量低现实现象。我们讨论了深度学习模型复杂性的应用，特别是在泛化能力、优化、模型选择和设计方面。
- en: Model complexity of deep learning is still in its infant stage. There are many
    interesting challenges for future works.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的模型复杂性仍处于初期阶段。未来的工作有许多有趣的挑战。
- en: Expressive capacity of deep learning models is a challenging problem. For example,
    in most cases, deep learning models are over-parameterized and have sufficient
    expressive power for given tasks and data. A natural question is what expressive
    capacity is sufficient for a given task. In other words, can we obtain a lower
    bound of expressive capacity of deep learning models that are sufficient for a
    given task? Does a narrow layer limit the expressive capacity of a model even
    if the model itself has a large number of parameters?
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的表达能力是一个具有挑战性的问题。例如，在大多数情况下，深度学习模型是过度参数化的，并且对于给定的任务和数据具有足够的表达能力。一个自然的问题是，对于给定的任务，什么样的表达能力是足够的。换句话说，我们是否可以获得深度学习模型在给定任务中足够的表达能力的下界？即使模型本身具有大量参数，狭窄的层是否会限制模型的表达能力？
- en: Several studies explore the bottleneck of model size (i.e., depth, width) in
    the expressive capacity. That is, when the model size may become a bottleneck
    that restricts the expressive capacity. For example, Hanin and Sellke [[38](#bib.bib38)]
    and Lu et al. [[64](#bib.bib64)] discover that any deep ReLU network with width
    constrained by the input dimensionality has very limited expressive power. Serra et
    al. [[92](#bib.bib92)] find that smaller widths in the first few layers of a deep
    ReLU network cause a bottleneck on the expressive power. Kileel et al. [[52](#bib.bib52)]
    identify a bottleneck of layer width of deep polynomial networks. In a deep polynomial
    network, if there is a very narrow layer, no matter how wide the other layers
    are, the network can never correspond to a convex functional space. A convex functional
    space benefits the optimization and makes the model easier to train. Research
    on the bottleneck of expressive capacity may help to tackle many other problems,
    such as model design, model selection, model compression, and pruning.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究探讨了模型大小（即深度、宽度）在表达能力中的瓶颈。也就是说，当模型大小可能成为限制表达能力的瓶颈时。例如，Hanin和Sellke [[38](#bib.bib38)]
    以及 Lu 等人 [[64](#bib.bib64)] 发现任何深度ReLU网络，在输入维度限制下，其宽度受限会导致非常有限的表达能力。Serra 等人 [[92](#bib.bib92)]
    发现，深度ReLU网络前几层较小的宽度会对表达能力造成瓶颈。Kileel 等人 [[52](#bib.bib52)] 确定了深度多项式网络的层宽瓶颈。在深度多项式网络中，如果有一个非常窄的层，无论其他层多宽，网络都无法对应到一个凸函数空间。凸函数空间有助于优化，并使模型更容易训练。研究表达能力瓶颈可能有助于解决许多其他问题，例如模型设计、模型选择、模型压缩和剪枝。
- en: Though some progress has been made, effective complexity measures are still
    a largely under-developed direction in deep learning. Comparing to expressive
    capacity of deep learning models, measuring effective model complexity is even
    more challenging. Effective complexity measures have to be capable of capturing
    fine granularity differences between two models, such as the same model architecture
    with two different optimization algorithms. Several previous studies [[44](#bib.bib44),
    [60](#bib.bib60), [74](#bib.bib74), [81](#bib.bib81), [89](#bib.bib89)] define
    and explore effective complexity measures mainly from the perspective of piecewise
    linear property [[44](#bib.bib44), [81](#bib.bib81), [89](#bib.bib89)], Fisher-Rao
    metric [[60](#bib.bib60)], or the size of trainable samples [[74](#bib.bib74)].
    However, the effective complexity measure is still a largely unexplored and valuable
    direction.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了一些进展，但有效的复杂度度量仍然是深度学习中一个相对未发展的方向。与深度学习模型的表达能力相比，测量有效模型复杂度更加具有挑战性。有效复杂度度量必须能够捕捉两个模型之间的细粒度差异，例如相同模型架构下的两种不同优化算法。以往的一些研究 [[44](#bib.bib44),
    [60](#bib.bib60), [74](#bib.bib74), [81](#bib.bib81), [89](#bib.bib89)] 从分段线性特性 [[44](#bib.bib44),
    [81](#bib.bib81), [89](#bib.bib89)], Fisher-Rao度量 [[60](#bib.bib60)]，或可训练样本的大小 [[74](#bib.bib74)]
    的角度定义和探讨了有效复杂度度量。然而，有效复杂度度量仍然是一个相对未被充分探索且有价值的方向。
- en: Last, cross-model complexity comparison is a promising direction. Given several
    models with different model frameworks and different model sizes, how can we compare
    their expressive capacity? After these models are trained sufficiently on the
    same dataset, such as obtaining zero training error, how can we compare their
    effective complexity and further understand their generalization capability? In
    these cases, the cross-model complexity comparison is useful. Comparing model
    complexity crossing different deep models can in general help many problems of
    deep learning, in particular model selection and design. However, the exploration
    of cross-model comparison of expressive capacity or effective complexity of deep
    learning models is still very limited. Khrulkov et al. [[51](#bib.bib51)] compare
    expressive capacity between shallow FCNNs, CNNs, and RNNs by connecting network
    architectures to tensor decompositions. However, many more sophisticated models
    are not involved and need to be further explored.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，跨模型复杂度比较是一个有前景的方向。考虑到几个具有不同模型框架和不同模型大小的模型，我们如何比较它们的表达能力？在这些模型在相同数据集上经过充分训练后，例如获得零训练误差，我们如何比较它们的有效复杂度并进一步了解它们的泛化能力？在这些情况下，跨模型复杂度比较是有用的。跨不同深度模型比较模型复杂度通常有助于解决深度学习的许多问题，特别是模型选择和设计。然而，深度学习模型的表达能力或有效复杂度的跨模型比较的探索仍然非常有限。Khrulkov
    等人 [[51](#bib.bib51)] 通过将网络架构连接到张量分解来比较浅层FCNNs、CNNs和RNNs之间的表达能力。然而，许多更复杂的模型未涉及，需要进一步探索。
- en: References
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. A. Adams and J. J. Fournier. Sobolev spaces. Elsevier, 2003.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. A. Adams 和 J. J. Fournier. Sobolev 空间。Elsevier, 2003.'
- en: '[2] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized
    neural networks, going beyond two layers. In Advances in Neural Information Processing
    Systems, pages 6155–6166, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Allen-Zhu, Y. Li 和 Y. Liang. 在超参数化神经网络中的学习与泛化，超越两层。发表于神经信息处理系统进展, 页码
    6155–6166, 2019.'
- en: '[3] R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding deep neural
    networks with rectified linear units. In International Conference on Learning
    Representations, 2018.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Arora, A. Basu, P. Mianjy 和 A. Mukherjee. 理解具有修正线性单元的深度神经网络。发表于国际学习表征会议,
    2018.'
- en: '[4] S. Arora and B. Barak. Computational complexity: a modern approach. Cambridge
    University Press, 2009.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Arora 和 B. Barak. 计算复杂性：现代方法。剑桥大学出版社, 2009.'
- en: '[5] J. Ba and R. Caruana. Do deep nets really need to be deep? In Advances
    in Neural Information Processing Systems, pages 2654–2662, 2014.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Ba 和 R. Caruana. 深度网络真的需要很深吗？发表于神经信息处理系统进展, 页码 2654–2662, 2014.'
- en: '[6] V. Balasubramanian. Statistical inference, occam’s razor, and statistical
    mechanics on the space of probability distributions. Neural computation, 9(2):349–368,
    1997.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] V. Balasubramanian. 统计推断、奥卡姆剃刀与概率分布空间上的统计力学。Neural computation, 9(2):349–368,
    1997.'
- en: '[7] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal
    function. IEEE Transactions on Information theory, 39(3):930–945, 1993.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. R. Barron. Sigmoid 函数叠加的通用逼近界限。IEEE 信息理论汇刊, 39(3):930–945, 1993.'
- en: '[8] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error
    estimation. Machine Learning, 48(1-3):85–113, 2002.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] P. L. Bartlett, S. Boucheron 和 G. Lugosi. 模型选择与误差估计。机器学习, 48(1-3):85–113,
    2002.'
- en: '[9] P. L. Bartlett, D. J. Foster, and M. J. Telgarsky. Spectrally-normalized
    margin bounds for neural networks. In Advances in Neural Information Processing
    Systems, pages 6240–6249, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. L. Bartlett, D. J. Foster 和 M. J. Telgarsky. 神经网络的谱归一化边界。发表于神经信息处理系统进展,
    页码 6240–6249, 2017.'
- en: '[10] P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension
    and pseudodimension bounds for piecewise linear neural networks. Journal of Machine
    Learning Research, 20(63):1–17, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] P. L. Bartlett, N. Harvey, C. Liaw 和 A. Mehrabian. 对于分段线性神经网络的几乎紧 VC 维度和伪维度界限。机器学习研究杂志,
    20(63):1–17, 2019.'
- en: '[11] P. L. Bartlett, V. Maiorov, and R. Meir. Almost linear vc-dimension bounds
    for piecewise polynomial networks. Neural Computation, 10(8):2159–2173, 1998.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] P. L. Bartlett, V. Maiorov 和 R. Meir. 分段多项式网络的几乎线性 VC 维度界限。Neural Computation,
    10(8):2159–2173, 1998.'
- en: '[12] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities:
    Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482,
    2002.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] P. L. Bartlett 和 S. Mendelson. Rademacher 和高斯复杂性：风险界限和结构结果。机器学习研究杂志, 3(Nov):463–482,
    2002.'
- en: '[13] Y. Bengio and O. Delalleau. On the expressive power of deep architectures.
    In International Conference on Algorithmic Learning Theory, pages 18–36\. Springer,
    2011.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Bengio 和 O. Delalleau. 深度架构的表达能力。发表于国际算法学习理论会议, 页码 18–36\. Springer,
    2011.'
- en: '[14] M. Bianchini and F. Scarselli. On the complexity of neural network classifiers:
    A comparison between shallow and deep architectures. IEEE Transactions on Neural
    Networks and Learning Systems, 25(8):1553–1565, 2014.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. Bianchini 和 F. Scarselli. 神经网络分类器的复杂性：浅层与深层架构的比较。IEEE 神经网络与学习系统汇刊,
    25(8):1553–1565, 2014.'
- en: '[15] M. Bianchini and F. Scarselli. On the complexity of shallow and deep neural
    network classifiers. In ESANN, 2014.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] M. Bianchini 和 F. Scarselli. 关于浅层和深层神经网络分类器的复杂性。发表于 ESANN, 2014.'
- en: '[16] M. Bohanec and I. Bratko. Trading accuracy for simplicity in decision
    trees. Machine Learning, 15(3):223–250, 1994.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Bohanec 和 I. Bratko. 在决策树中用简单性换取准确性。机器学习, 15(3):223–250, 1994.'
- en: '[17] G. Bonaccorso. Machine learning algorithms. Packt Publishing Ltd, 2017.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. Bonaccorso. 机器学习算法。Packt Publishing Ltd, 2017.'
- en: '[18] G. E. Bredon. Topology and geometry, volume 139. Springer Science & Business
    Media, 2013.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. E. Bredon. 拓扑学与几何学，第 139 卷。Springer Science & Business Media, 2013.'
- en: '[19] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classification
    and regression trees. CRC press, 1984.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] L. Breiman, J. Friedman, C. J. Stone 和 R. A. Olshen. 分类与回归树。CRC press,
    1984.'
- en: '[20] H. Buhrman and R. De Wolf. Complexity measures and decision tree complexity:
    a survey. Theoretical Computer Science, 288(1):21–43, 2002.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Buhrman 和 R. De Wolf. 复杂性测量与决策树复杂性：综述。理论计算机科学, 288(1):21–43, 2002.'
- en: '[21] N. Bulso, M. Marsili, and Y. Roudi. On the complexity of logistic regression
    models. Neural computation, 31(8):1592–1623, 2019.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] N. Bulso, M. Marsili, 和 Y. Roudi. 逻辑回归模型的复杂性. 神经计算, 31(8):1592–1623, 2019。'
- en: '[22] J.-R. Cano. Analysis of data complexity measures for classification. Expert
    Systems with Applications, 40(12):4820–4831, 2013.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] J.-R. Cano. 分类数据复杂性度量的分析. 应用专家系统, 40(12):4820–4831, 2013。'
- en: '[23] N. L. Carothers. Real analysis. Cambridge University Press, 2000.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] N. L. Carothers. 实分析. 剑桥大学出版社, 2000。'
- en: '[24] J. D. Carroll and J.-J. Chang. Analysis of individual differences in multidimensional
    scaling via an n-way generalization of “eckart-young” decomposition. Psychometrika,
    35(3):283–319, 1970.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. D. Carroll 和 J.-J. Chang. 通过 “Eckart-Young” 分解的 n 维推广分析个体差异. 心理计量学,
    35(3):283–319, 1970。'
- en: '[25] Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compression and acceleration
    for deep neural networks: The principles, progress, and challenges. IEEE Signal
    Processing Magazine, 35(1):126–136, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Cheng, D. Wang, P. Zhou, 和 T. Zhang. 深度神经网络的模型压缩与加速: 原则、进展与挑战. IEEE
    信号处理杂志, 35(1):126–136, 2018。'
- en: '[26] V. Cherkassky, X. Shao, F. M. Mulier, and V. N. Vapnik. Model complexity
    control for regression using vc generalization bounds. IEEE Transactions on Neural
    Networks, 10(5):1075–1089, 1999.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] V. Cherkassky, X. Shao, F. M. Mulier, 和 V. N. Vapnik. 使用 VC 泛化界限的回归模型复杂性控制.
    IEEE 神经网络学报, 10(5):1075–1089, 1999。'
- en: '[27] N. Cohen and A. Shashua. Convolutional rectifier networks as generalized
    tensor decompositions. In International Conference on Machine Learning, pages
    955–963, 2016.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] N. Cohen 和 A. Shashua. 卷积整流网络作为广义张量分解. 在国际机器学习会议上, 页码 955–963, 2016。'
- en: '[28] S. Cook, C. Dwork, and R. Reischuk. Upper and lower time bounds for parallel
    random access machines without simultaneous writes. SIAM Journal on Computing,
    15(1):87–97, 1986.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Cook, C. Dwork, 和 R. Reischuk. 无同时写入的并行随机存取机器的上界和下界. SIAM 计算学报, 15(1):87–97,
    1986。'
- en: '[29] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics
    of Control, Signals and Systems, 2(4):303–314, 1989.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] G. Cybenko. 通过 sigmoid 函数的叠加进行近似. 控制、信号与系统的数学, 2(4):303–314, 1989。'
- en: '[30] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. In
    Advances in Neural Information Processing Systems, pages 666–674, 2011.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] O. Delalleau 和 Y. Bengio. 浅层与深层和积网络. 在神经信息处理系统进展会议上, 页码 666–674, 2011。'
- en: '[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet:
    A large-scale hierarchical image database. In IEEE Conference on Computer Vision
    and Pattern Recognition, pages 248–255\. IEEE, 2009.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei. Imagenet:
    大规模层次图像数据库. 在 IEEE 计算机视觉与模式识别会议上, 页码 248–255\. IEEE, 2009。'
- en: '[32] J. Du. The “weight” of models and complexity. Complexity, 21(3):21–35,
    2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Du. 模型的“权重”和复杂性. 复杂性, 21(3):21–35, 2016。'
- en: '[33] B. Frieden. Science from fisher information: A unification cambridge univ.
    Press, Cambridge, UK [Google Scholar], 2004.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] B. Frieden. 从费希尔信息中获得科学: 统一. 剑桥大学出版社, 剑桥, 英国 [Google Scholar], 2004。'
- en: '[34] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press,
    2016.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] I. Goodfellow, Y. Bengio, 和 A. Courville. 深度学习. MIT出版社, 2016。'
- en: '[35] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio.
    Maxout networks. In International Conference on Machine Learning, pages 1319–1327\.
    PMLR, 2013.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, 和 Y. Bengio. Maxout
    网络. 在国际机器学习会议上, 页码 1319–1327\. PMLR, 2013。'
- en: '[36] I. Gühring, G. Kutyniok, and P. Petersen. Complexity bounds for approximations
    with deep relu neural networks in sobolev norms. 2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] I. Gühring, G. Kutyniok, 和 P. Petersen. 使用 Sobolev 范数的深度 ReLU 神经网络的复杂性界限.
    2019。'
- en: '[37] B. Hanin and D. Rolnick. Complexity of linear regions in deep networks.
    In International Conference on Machine Learning, pages 2596–2604\. PMLR, 2019.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. Hanin 和 D. Rolnick. 深度网络中线性区域的复杂性. 在国际机器学习会议上, 页码 2596–2604\. PMLR,
    2019。'
- en: '[38] B. Hanin and M. Sellke. Approximating continuous functions by relu nets
    of minimal width. arXiv preprint arXiv:1710.11278, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] B. Hanin 和 M. Sellke. 通过最小宽度的 ReLU 网络近似连续函数. arXiv 预印本 arXiv:1710.11278,
    2017。'
- en: '[39] S. Hayou, A. Doucet, and J. Rousseau. On the selection of initialization
    and activation function for deep neural networks. STAT, 1050:7, 2018.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Hayou, A. Doucet, 和 J. Rousseau. 深度神经网络的初始化和激活函数选择. STAT, 1050:7, 2018。'
- en: '[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
    recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 770–778, 2016.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] K. He、X. Zhang、S. Ren 和 J. Sun. 图像识别的深度残差学习. 载于 IEEE 计算机视觉与模式识别会议论文集,
    页码 770–778, 2016.'
- en: '[41] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural
    network. STAT, 1050:9, 2015.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] G. Hinton、O. Vinyals 和 J. Dean. 提炼神经网络中的知识. STAT, 1050:9, 2015.'
- en: '[42] M. Höge, T. Wöhling, and W. Nowak. A primer for model selection: The decisive
    role of model complexity. Water Resources Research, 54(3):1688–1715, 2018.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Höge、T. Wöhling 和 W. Nowak. 模型选择入门：模型复杂度的决定性作用. 水资源研究, 54(3):1688–1715,
    2018.'
- en: '[43] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks
    are universal approximators. Neural Networks, 2(5):359–366, 1989.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. Hornik、M. Stinchcombe 和 H. White. 多层前馈网络是通用逼近器. 神经网络, 2(5):359–366,
    1989.'
- en: '[44] X. Hu, W. Liu, J. Bian, and J. Pei. Measuring model complexity of neural
    networks with curve activation functions. In Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining, pages 1521–1531,
    2020.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] X. Hu、W. Liu、J. Bian 和 J. Pei. 使用曲线激活函数测量神经网络的模型复杂度. 载于第26届 ACM SIGKDD
    国际知识发现与数据挖掘大会论文集, 页码 1521–1531, 2020.'
- en: '[45] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In Proceedings of the 32nd International
    Conference on Machine Learning, Proceedings of Machine Learning Research, pages
    448–456, 2015.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Ioffe 和 C. Szegedy. 批量归一化：通过减少内部协变量偏移加速深度网络训练. 载于第32届国际机器学习大会论文集, 机器学习研究论文集,
    页码 448–456, 2015.'
- en: '[46] S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear
    prediction: Risk bounds, margin bounds, and regularization. In Advances in Neural
    Information Processing Systems, pages 793–800, 2009.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. M. Kakade、K. Sridharan 和 A. Tewari. 线性预测的复杂性：风险界限、边际界限和正则化. 载于神经信息处理系统进展,
    页码 793–800, 2009.'
- en: '[47] D. Kalimeris, G. Kaplun, P. Nakkiran, B. Edelman, T. Yang, B. Barak, and
    H. Zhang. Sgd on neural networks learns functions of increasing complexity. In
    Advances in Neural Information Processing Systems, pages 3491–3501, 2019.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] D. Kalimeris、G. Kaplun、P. Nakkiran、B. Edelman、T. Yang、B. Barak 和 H. Zhang.
    神经网络上的 SGD 学习逐渐复杂的函数. 载于神经信息处理系统进展, 页码 3491–3501, 2019.'
- en: '[48] B. L. Kalman and S. C. Kwasny. Why tanh: choosing a sigmoidal function.
    In [Proceedings 1992] IJCNN International Joint Conference on Neural Networks,
    volume 4, pages 578–581\. IEEE, 1992.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] B. L. Kalman 和 S. C. Kwasny. 为什么选择 tanh: 选择 sigmoid 函数. 载于 [Proceedings
    1992] IJCNN 国际联合神经网络会议, 第4卷, 页码 578–581\. IEEE, 1992.'
- en: '[49] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio. Generalization in deep learning.
    arXiv preprint arXiv:1710.05468, 2017.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] K. Kawaguchi、L. P. Kaelbling 和 Y. Bengio. 深度学习中的泛化. arXiv 预印本 arXiv:1710.05468,
    2017.'
- en: '[50] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang.
    On large-batch training for deep learning: Generalization gap and sharp minima.
    International Conference on Learning Representations, 2017.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. S. Keskar、D. Mudigere、J. Nocedal、M. Smelyanskiy 和 P. T. P. Tang. 关于深度学习的大批量训练：泛化差距和尖锐极小值.
    国际学习表示大会, 2017.'
- en: '[51] V. Khrulkov, A. Novikov, and I. Oseledets. Expressive power of recurrent
    neural networks. In International Conference on Learning Representations, 2018.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] V. Khrulkov、A. Novikov 和 I. Oseledets. 循环神经网络的表达能力. 载于国际学习表示大会, 2018.'
- en: '[52] J. Kileel, M. Trager, and J. Bruna. On the expressive power of deep polynomial
    neural networks. In Advances in Neural Information Processing Systems, pages 10310–10319,
    2019.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] J. Kileel、M. Trager 和 J. Bruna. 深度多项式神经网络的表达能力. 载于神经信息处理系统进展, 页码 10310–10319,
    2019.'
- en: '[53] J. Kilian and H. T. Siegelmann. On the power of sigmoid neural networks.
    In Proceedings of the Sixth Annual Conference on Computational Learning Theory,
    pages 137–143, 1993.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Kilian 和 H. T. Siegelmann. 关于 sigmoid 神经网络的能力. 载于第六届年度计算学习理论大会论文集,
    页码 137–143, 1993.'
- en: '[54] V. Kuurkova. Constructive lower bounds on model complexity of shallow
    perceptron networks. Neural Computing and Applications, 29(7):305–315, 2018.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] V. Kuurkova. 浅层感知机网络模型复杂度的建设性下界. 神经计算与应用, 29(7):305–315, 2018.'
- en: '[55] G. Lample, M. Ott, A. Conneau, L. Denoyer, and M. Ranzato. Phrase-based
    & neural unsupervised machine translation. In Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, pages 5039–5049, 2018.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] G. Lample、M. Ott、A. Conneau、L. Denoyer 和 M. Ranzato. 基于短语和神经的无监督机器翻译.
    载于2018年自然语言处理实证方法会议论文集, 页码 5039–5049, 2018.'
- en: '[56] J. M. Landsberg. Tensors: geometry and applications. Representation Theory,
    381(402):3, 2012.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] J. M. 兰兹伯格. 张量: 几何和应用. 表示理论, 381(402):3, 2012.'
- en: '[57] D. Laredo, S. F. Ma, G. Leylaz, O. Schütze, and J.-Q. Sun. Automatic model
    selection for fully connected neural networks. International Journal of Dynamics
    and Control, 8(4):1063–1079, 2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. 拉雷多, S. F. 马, G. 莱拉兹, O. 舒策, 和 J.-Q. 孙. 完全连接神经网络的自动模型选择. 国际动力学与控制杂志,
    8(4):1063–1079, 2020.'
- en: '[58] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436–444,
    2015.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. 列昆, Y. 本吉奥, 和 G. 兴顿. 深度学习. 自然, 521(7553):436–444, 2015.'
- en: '[59] L. Li. Data complexity in machine learning and novel classification algorithms.
    PhD thesis, California Institute of Technology, 2006.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] L. 李. 机器学习中的数据复杂性和新型分类算法. 博士论文, 加州理工学院, 2006.'
- en: '[60] T. Liang, T. Poggio, A. Rakhlin, and J. Stokes. Fisher-rao metric, geometry,
    and complexity of neural networks. In The 22nd International Conference on Artificial
    Intelligence and Statistics, pages 888–896\. PMLR, 2019.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. 梁, T. 波焦, A. 拉赫林, 和 J. 斯托克斯. 菲舍尔-拉奥度量、几何和神经网络的复杂性. 在第22届国际人工智能与统计会议,
    页码 888–896\. PMLR, 2019.'
- en: '[61] T.-S. Lim, W.-Y. Loh, and Y.-S. Shih. A comparison of prediction accuracy,
    complexity, and training time of thirty-three old and new classification algorithms.
    Machine learning, 40(3):203–228, 2000.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] T.-S. 林, W.-Y. 罗, 和 Y.-S. 施. 三十三种旧有和新分类算法的预测准确性、复杂性和训练时间比较. 机器学习, 40(3):203–228,
    2000.'
- en: '[62] C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei,
    A. Yuille, J. Huang, and K. Murphy. Progressive neural architecture search. In
    Proceedings of the European Conference on Computer Vision (ECCV), pages 19–34,
    2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] C. 刘, B. 佐普, M. 诺伊曼, J. 施伦斯, W. 花, L.-J. 李, L. 费伊-费伊, A. 尤伊尔, J. 黄, 和
    K. 墨菲. 渐进神经架构搜索. 在欧洲计算机视觉会议（ECCV）论文集中, 页码 19–34, 2018.'
- en: '[63] H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search.
    International Conference on Learning Representations, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] H. 刘, K. 西蒙尼扬, 和 Y. 杨. Darts: 可微分架构搜索. 国际学习表征会议, 2019.'
- en: '[64] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural
    networks: A view from the width. In Advances in Neural Information Processing
    Systems, pages 6231–6239, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Z. 卢, H. 蒲, F. 王, Z. 胡, 和 L. 王. 神经网络的表现力: 从宽度的视角. 在《神经信息处理系统进展》, 页码 6231–6239,
    2017.'
- en: '[65] S. Lundqvist, A. Oneto, B. Reznick, and B. Shapiro. On generic and maximal
    k-ranks of binary forms. Journal of Pure and Applied Algebra, 223(5):2062–2079,
    2019.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. 伦德奎斯特, A. 奥内托, B. 雷兹尼克, 和 B. 沙皮罗. 关于二次形式的通用和最大k-秩. 《纯粹与应用代数杂志》, 223(5):2062–2079,
    2019.'
- en: '[66] W. Maass. Neural nets with superlinear vc-dimension. Neural Computation,
    6(5):877–884, 1994.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] W. 马斯. 超线性VC维的神经网络. 神经计算, 6(5):877–884, 1994.'
- en: '[67] H. Mhaskar, Q. Liao, and T. Poggio. When and why are deep networks better
    than shallow ones? In Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence, AAAI’17, page 2343–2349\. AAAI Press, 2017.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. 米哈斯卡尔, Q. 廖, 和 T. 波焦. 深度网络何时以及为何优于浅层网络? 在第三十一届AAAI人工智能会议论文集中, AAAI’17,
    页码 2343–2349\. AAAI出版社, 2017.'
- en: '[68] B. Michel and A. Nouy. Learning with tree tensor networks: complexity
    estimates and model selection. arXiv preprint arXiv:2007.01165, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] B. 米歇尔 和 A. 诺伊. 使用树张量网络的学习: 复杂度估计和模型选择. arXiv预印本 arXiv:2007.01165, 2020.'
- en: '[69] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning.
    MIT press, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] M. 莫赫里, A. 罗斯塔米扎德, 和 A. 塔尔沃克. 机器学习基础. 麻省理工学院出版社, 2018.'
- en: '[70] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of linear
    regions of deep neural networks. In Advances in Neural Information Processing
    Systems, pages 2924–2932, 2014.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] G. F. 蒙图法尔, R. 帕斯卡努, K. 乔, 和 Y. 本吉奥. 关于深度神经网络线性区域的数量. 在《神经信息处理系统进展》, 页码
    2924–2932, 2014.'
- en: '[71] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press,
    2012.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] K. P. 墨菲. 机器学习: 概率视角. 麻省理工学院出版社, 2012.'
- en: '[72] I. J. Myung. The importance of complexity in model selection. Journal
    of Mathematical Psychology, 44(1):190–204, 2000.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] I. J. 美永. 模型选择中复杂性的重要性. 《数学心理学杂志》, 44(1):190–204, 2000.'
- en: '[73] V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann
    machines. In Proceedings of the 27th International Conference on Machine Learning,
    pages 807–814, 2010.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] V. 奈尔 和 G. E. 兴顿. 修正线性单元提高限制玻尔兹曼机的性能. 在第27届国际机器学习会议论文集中, 页码 807–814, 2010.'
- en: '[74] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever.
    Deep double descent: Where bigger models and more data hurt. In International
    Conference on Learning Representations, 2020.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak 和 I. Sutskever. 深度双重下降：更大的模型和更多的数据如何带来负面影响。载于国际学习表征会议,
    2020。'
- en: '[75] B. Neyshabur. Implicit regularization in deep learning. arXiv preprint
    arXiv:1709.01953, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Neyshabur. 深度学习中的隐式正则化。arXiv 预印本 arXiv:1709.01953, 2017。'
- en: '[76] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring
    generalization in deep learning. In Advances in Neural Information Processing
    Systems, pages 5947–5956, 2017.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] B. Neyshabur, S. Bhojanapalli, D. McAllester 和 N. Srebro. 探索深度学习中的泛化。载于神经信息处理系统进展，5947–5956页，2017。'
- en: '[77] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. The role
    of over-parametrization in generalization of neural networks. In International
    Conference on Learning Representations, 2018.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun 和 N. Srebro. 过参数化在神经网络泛化中的作用。载于国际学习表征会议,
    2018。'
- en: '[78] B. Neyshabur, R. Tomioka, and N. Srebro. In search of the real inductive
    bias: On the role of implicit regularization in deep learning. In International
    Conference on Learning Representations (Workshop), 2015.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. Neyshabur, R. Tomioka 和 N. Srebro. 寻找真正的归纳偏差：隐式正则化在深度学习中的作用。载于国际学习表征会议（研讨会），2015。'
- en: '[79] B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in
    neural networks. In Conference on Learning Theory, pages 1376–1401, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] B. Neyshabur, R. Tomioka 和 N. Srebro. 基于范数的神经网络容量控制。载于学习理论会议，第1376–1401页，2015。'
- en: '[80] N. Nisan and M. Szegedy. On the degree of boolean functions as real polynomials.
    Computational Complexity, 4(4):301–313, 1994.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] N. Nisan 和 M. Szegedy. 布尔函数作为实多项式的度数。计算复杂性, 4(4):301–313, 1994。'
- en: '[81] R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein.
    Sensitivity and generalization in neural networks: an empirical study. In International
    Conference on Learning Representations, 2018.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington 和 J. Sohl-Dickstein.
    神经网络中的敏感性与泛化：一项实证研究。载于国际学习表征会议, 2018。'
- en: '[82] C. Nwankpa, W. Ijomah, A. Gachagan, and S. Marshall. Activation functions:
    comparison of trends in practice and research for deep learning. International
    Conference on Computational Sciences and Technology (INCCST), pages 124 – 133,
    Jan 2021.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Nwankpa, W. Ijomah, A. Gachagan 和 S. Marshall. 激活函数：深度学习中实践与研究趋势的比较。国际计算科学与技术会议（INCCST），第124–133页，2021年1月。'
- en: '[83] I. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientific
    Computing, 33(5):2295–2317, 2011.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] I. V. Oseledets. 张量列分解。SIAM科学计算杂志, 33(5):2295–2317, 2011。'
- en: '[84] I. Pérez Arribas. Sobolev spaces and partial differential equations. 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] I. Pérez Arribas. Sobolev 空间与偏微分方程。2017。'
- en: '[85] H. Pham, M. Guan, B. Zoph, Q. Le, and J. Dean. Efficient neural architecture
    search via parameters sharing. In Proceedings of the 35th International Conference
    on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages
    4095–4104, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] H. Pham, M. Guan, B. Zoph, Q. Le 和 J. Dean. 通过参数共享实现高效的神经架构搜索。载于第35届国际机器学习会议论文集，第80卷，第4095–4104页，2018。'
- en: '[86] T. Poggio, K. Kawaguchi, Q. Liao, B. Miranda, L. Rosasco, X. Boix, J. Hidary,
    and H. Mhaskar. Theory of deep learning iii: explaining the non-overfitting puzzle.
    Massachusetts Institute of Technology CBMM Memo No. 73, 2017.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] T. Poggio, K. Kawaguchi, Q. Liao, B. Miranda, L. Rosasco, X. Boix, J.
    Hidary 和 H. Mhaskar. 深度学习理论 III：解释非过拟合难题。麻省理工学院 CBMM 备忘录第73号，2017。'
- en: '[87] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli. Exponential
    expressivity in deep neural networks through transient chaos. In Advances in Neural
    Information Processing Systems, pages 3360–3368, 2016.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein 和 S. Ganguli. 通过瞬态混沌在深度神经网络中实现指数表达能力。载于神经信息处理系统进展，3360–3368页，2016。'
- en: '[88] I. Radosavovic, J. Johnson, S. Xie, W.-Y. Lo, and P. Dollár. On network
    design spaces for visual recognition. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 1882–1890, 2019.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] I. Radosavovic, J. Johnson, S. Xie, W.-Y. Lo 和 P. Dollár. 视觉识别中的网络设计空间。载于IEEE国际计算机视觉会议论文集，第1882–1890页，2019。'
- en: '[89] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. S. Dickstein. On
    the expressive power of deep neural networks. In Proceedings of the 34th International
    Conference on Machine Learning-Volume 70, pages 2847–2854\. JMLR, 2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli 和 J. S. Dickstein. 深度神经网络的表达能力。载于第34届国际机器学习会议论文集-第70卷，第2847–2854页。JMLR,
    2017。'
- en: '[90] C. E. Rasmussen and Z. Ghahramani. Occam’s razor. In Advances in Neural
    Information Processing Systems, pages 294–300, 2001.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] C. E. Rasmussen 和 Z. Ghahramani. 奥卡姆剃刀. 发表在神经信息处理系统进展上, 页码 294–300, 2001.'
- en: '[91] P. Rebentrost, B. Gupt, and T. R. Bromley. Quantum computational finance:
    Monte carlo pricing of financial derivatives. Physical Review A, 98(2):022321,
    2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] P. Rebentrost, B. Gupt, 和 T. R. Bromley. 量子计算金融：金融衍生品的蒙特卡罗定价. 物理评论 A,
    98(2):022321, 2018.'
- en: '[92] T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding and counting
    linear regions of deep neural networks. In International Conference on Machine
    Learning, pages 4558–4566\. PMLR, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] T. Serra, C. Tjandraatmadja, 和 S. Ramalingam. 深度神经网络线性区域的界限与计数. 发表在国际机器学习会议上,
    页码 4558–4566. PMLR, 2018.'
- en: '[93] D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and A. Van Der Linde. Bayesian
    measures of model complexity and fit. Journal of the Royal Statistical Society:
    Series b (Statistical Methodology), 64(4):583–639, 2002.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] D. J. Spiegelhalter, N. G. Best, B. P. Carlin, 和 A. Van Der Linde. 贝叶斯模型复杂度和拟合度的度量.
    皇家统计学会学报：B 系列（统计方法论）, 64(4):583–639, 2002.'
- en: '[94] R. Sun. Optimization for deep learning: theory and algorithms. Journal
    of Operations Research Society of China, 2020.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] R. Sun. 深度学习的优化：理论与算法. 中国运筹学会学报, 2020.'
- en: '[95] Y. Tan and J. Wang. A support vector machine with a hybrid kernel and
    minimal vapnik-chervonenkis dimension. IEEE Transactions on Knowledge and Data
    Engineering, 16(4):385–395, 2004.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Y. Tan 和 J. Wang. 具有混合核和最小 Vapnik-Chervonenkis 维度的支持向量机. IEEE 知识与数据工程学报,
    16(4):385–395, 2004.'
- en: '[96] V. Vapnik. The nature of statistical learning theory. Springer Science
    & Business Media, 2013.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] V. Vapnik. 统计学习理论的本质. 施普林格科学与商业媒体, 2013.'
- en: '[97] H. Xu and S. Mannor. Robustness and generalization. Machine Learning,
    86(3):391–423, 2012.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] H. Xu 和 S. Mannor. 鲁棒性与泛化能力. 机器学习, 86(3):391–423, 2012.'
- en: '[98] A. C.-C. Yao. Decision tree complexity and betti numbers. Journal of Computer
    and System Sciences, 55(1):36–43, 1997.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] A. C.-C. Yao. 决策树复杂度与 Betti 数. 计算机与系统科学学报, 55(1):36–43, 1997.'
- en: '[99] D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adversarially
    robust generalization. In International Conference on Machine Learning, pages
    7085–7094\. PMLR, 2019.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] D. Yin, R. Kannan, 和 P. Bartlett. 对抗性鲁棒泛化的 Rademacher 复杂度. 发表在国际机器学习会议上,
    页码 7085–7094. PMLR, 2019.'
- en: '[100] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding
    deep learning requires rethinking generalization. International Conference on
    Learning Representations, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] C. Zhang, S. Bengio, M. Hardt, B. Recht, 和 O. Vinyals. 理解深度学习需要重新思考泛化.
    国际学习表征会议, 2017.'
- en: '[101] S. Zheng, Q. Meng, H. Zhang, W. Chen, N. Yu, and T.-Y. Liu. Capacity
    control of relu neural networks by basis-path norm. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 33, pages 5925–5932, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] S. Zheng, Q. Meng, H. Zhang, W. Chen, N. Yu, 和 T.-Y. Liu. 通过基础路径范数控制
    relu 神经网络的容量. 发表在 AAAI 人工智能会议论文集上, 第 33 卷, 页码 5925–5932, 2019.'
- en: '[102] G. M. Ziegler. Lectures on polytopes, volume 152. Springer Science &
    Business Media, 2012.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] G. M. Ziegler. 多面体讲座, 第 152 卷. 施普林格科学与商业媒体, 2012.'
- en: '[103] B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning.
    International Conference on Learning Representations, 2016.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] B. Zoph 和 Q. V. Le. 使用强化学习进行神经架构搜索. 国际学习表征会议, 2016.'
