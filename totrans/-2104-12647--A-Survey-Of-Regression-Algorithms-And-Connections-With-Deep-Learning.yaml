- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:55:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:55:27'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2104.12647] A Survey Of Regression Algorithms And Connections With Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2104.12647] 回归算法及其与深度学习的联系综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2104.12647](https://ar5iv.labs.arxiv.org/html/2104.12647)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2104.12647](https://ar5iv.labs.arxiv.org/html/2104.12647)
- en: A Survey Of Regression Algorithms And Connections With Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归算法及其与深度学习的联系综述
- en: Yunpeng Tai Yunpeng Tai is a freshman of Suzhou University of Science and Technology.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yunpeng Tai Yunpeng Tai 是苏州科技大学的一名新生。
- en: 'E-mail: yunpengtai@foxmail.com.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '电子邮件: yunpengtai@foxmail.com。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Regression has attracted immense interest lately due to its effectiveness in
    tasks like predicting values. And Regression is of widespread use in multiple
    fields such as Economics, Finance, Business, Biology and so on. While considerable
    studies have proposed some impressive models, few of them have provided a whole
    picture regarding how and to what extent Regression has developed. With the aim
    of aiding beginners in understanding the relationships among different Regression
    algorithms, this paper characterizes a broad and thoughtful selection of recent
    regression algorithms, providing an organized and comprehensive overview of existing
    work and models utilized frequently. In this paper, the relationship between Regression
    and Deep Learning is also discussed and a conclusion can be drawn that Deep Learning
    can be more powerful as an combination with Regression models in the future.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归在预测值等任务中的有效性，最近引起了极大的关注。而回归在经济学、金融学、商业、生物学等多个领域有着广泛的应用。虽然已有不少研究提出了一些令人印象深刻的模型，但很少有研究提供关于回归如何及在何种程度上发展的完整图景。为了帮助初学者理解不同回归算法之间的关系，本文描述了一系列广泛且深思熟虑的近期回归算法，提供了现有工作和常用模型的有组织、全面的概述。本文还讨论了回归与深度学习之间的关系，并得出结论：未来深度学习可以与回归模型结合，发挥更强大的作用。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Regression, Survey, Comparison Between Algorithms, A Different Vision Of Ordinary
    Least Squares, Insight Of Regression Future.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回归，综述，算法比较，普通最小二乘法的不同视角，回归未来的洞察。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Regression is an approach of obtaining a relationship between input space and
    output space. The relationship is represented by a function $f:X\longmapsto Y$
    and $X$ is known as the independent variable, and $Y$ as the dependent variable.
    It was originally put forward by Legendre [[1](#bib.bib1)] in 1805, who applied
    least squares in Regression. And then Gauss [[2](#bib.bib2)] published a further
    development of the theory of least squares featuring ordinary least squares in
    1821.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是一种获得输入空间与输出空间之间关系的方法。这种关系由函数 $f:X\longmapsto Y$ 表示，其中 $X$ 被称为自变量，$Y$ 为因变量。最早由Legendre
    [[1](#bib.bib1)] 于1805年提出，他在回归中应用了最小二乘法。随后，Gauss [[2](#bib.bib2)] 在1821年发表了最小二乘法的进一步发展，即普通最小二乘法。
- en: Regression belongs to supervised learning and $Y$ is continuous, i.e. $Y\in
    R$. Undoubtedly, Regression is powerful and has made tremendous impact in enormous
    fields. As such, an increasing number of research has made fundamental improvement
    to Regression models over the past few decades.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回归属于监督学习，而 $Y$ 是连续的，即 $Y\in R$。毫无疑问，回归非常强大，并在众多领域产生了巨大影响。因此，过去几十年中，越来越多的研究对回归模型进行了基础性的改进。
- en: There has been such a surge of Regression models proposed recently, that researchers
    and beginners may find it challenging to figure out what exactly every model means
    and the relationship between them. Thus, a survey of the existing Regression models
    is beneficial both to beginners who just want to scratch the surface of Regression
    and researchers willing to have a systematic view of Regression models and gain
    insight from those smart models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近提出的回归模型数量激增，以至于研究人员和初学者可能会发现很难弄清楚每个模型的确切含义以及它们之间的关系。因此，对现有回归模型的综述对初学者（那些只是想了解回归皮毛的人）和希望系统了解回归模型并从这些智能模型中获得洞见的研究人员都是有益的。
- en: The key component of Regression is ordinary least squares. It is capable of
    producing an unbiased linear model of minimum variance as long as six necessary
    assumptions is satisfied according to Gauss-Markov Theorem [[2](#bib.bib2)]. However,
    if OLS is applied in specific areas, some of the assumptions are likely to be
    violated so that OLS fails to play its part in predicting values. Hence, it is
    essential to grasp those assumptions and figure out possible solutions when one
    of them is broken. And those well-known concerns with OLS contributes to extensive
    models designed to fix those violations such as Ridge [[3](#bib.bib3)][[4](#bib.bib4)],
    Lasso [[5](#bib.bib5)], Elastic Net [[6](#bib.bib6)] and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析的关键组成部分是普通最小二乘法（OLS）。根据高斯-马尔可夫定理，只要满足六个必要的假设，它就能够产生一个无偏差的最小方差线性模型[[2](#bib.bib2)]。然而，如果OLS应用于特定领域，其中一些假设可能会被违反，从而使得OLS在预测值时失效。因此，掌握这些假设并在其中一个假设被打破时找出可能的解决方案是非常重要的。这些与OLS相关的广泛关注促成了大量模型的设计，以修正这些违反，例如岭回归[[3](#bib.bib3)][[4](#bib.bib4)]、套索回归[[5](#bib.bib5)]、弹性网回归[[6](#bib.bib6)]等。
- en: What distinguishes this paper from others is the earlier part of this paper
    is OLS-centered and alternative solutions provided by different models are discussed
    at length when some assumptions are broken (Figure 1). This paper views the relationship
    between models on the whole and discusses the details of distinct models specifically
    and explicitly. And this paper also provides a walk-through of some uncommon Regression
    models. What’s more, a possible direction to which Regression is going to develop
    is covered.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文与其他论文的不同之处在于，本文的前半部分以OLS为中心，并详细讨论了当一些假设被打破时不同模型提供的替代解决方案（见图1）。本文将模型之间的关系整体进行考察，并具体而明确地讨论了不同模型的细节。此外，本文还提供了一些不常见的回归模型的详细介绍。更重要的是，本文还涉及了回归未来可能的发展方向。
- en: '![Refer to caption](img/0d4ed67deed173927259be5eefbe4f00.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d4ed67deed173927259be5eefbe4f00.png)'
- en: 'Figure 1: Problems With OLS And Possible Solutions Provided By Distinct Models.
    N is the size of input space. P is the number of features of every sample in input
    space.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：OLS的问题及不同模型提供的可能解决方案。N是输入空间的大小。P是输入空间中每个样本的特征数量。
- en: '![Refer to caption](img/286ee7b250a486ee36360a6ed9e4cc11.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/286ee7b250a486ee36360a6ed9e4cc11.png)'
- en: (a) $X_{T}=log(x)$
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $X_{T}=log(x)$
- en: '![Refer to caption](img/5e6424da5c225b60ecb7d09da74dc384.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e6424da5c225b60ecb7d09da74dc384.png)'
- en: (b) $X_{T}=\sqrt{x}$
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $X_{T}=\sqrt{x}$
- en: '![Refer to caption](img/e6154d26ebb7a883dd22a70e1f9a0165.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e6154d26ebb7a883dd22a70e1f9a0165.png)'
- en: (c) $X_{T}=exp(x)$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (c) $X_{T}=exp(x)$
- en: '![Refer to caption](img/319b0c1e74490f3a06ed843c02ca4b0c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/319b0c1e74490f3a06ed843c02ca4b0c.png)'
- en: (d) $X_{T}=\frac{1}{x}$
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (d) $X_{T}=\frac{1}{x}$
- en: '![Refer to caption](img/b20d6df06247b3c421994db44b1d7e9d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b20d6df06247b3c421994db44b1d7e9d.png)'
- en: (e) $X_{T}=(x-3)^{2}$
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (e) $X_{T}=(x-3)^{2}$
- en: '![Refer to caption](img/cb139e21a0d13fb3d0092e3890b29f88.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cb139e21a0d13fb3d0092e3890b29f88.png)'
- en: (f) $X_{T}=x^{3}$
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (f) $X_{T}=x^{3}$
- en: '![Refer to caption](img/d00c7d9604f5b738277b70d7f876dd50.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d00c7d9604f5b738277b70d7f876dd50.png)'
- en: (g) $X_{T}=x^{4}$
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (g) $X_{T}=x^{4}$
- en: '![Refer to caption](img/45c63672eeba2b228198d9d9d6d396dd.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/45c63672eeba2b228198d9d9d6d396dd.png)'
- en: (h) $X_{T}=x^{5}$
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (h) $X_{T}=x^{5}$
- en: '![Refer to caption](img/f355f04fda2b08a3526047f918d6c71d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f355f04fda2b08a3526047f918d6c71d.png)'
- en: (i) $X_{T}=x+1$
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (i) $X_{T}=x+1$
- en: '![Refer to caption](img/806bed1be11e9148a7eb7b41b2c88e72.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/806bed1be11e9148a7eb7b41b2c88e72.png)'
- en: (j) $X_{T}=\frac{1}{x^{2}}$
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (j) $X_{T}=\frac{1}{x^{2}}$
- en: 'Figure 2: Visualize Transformed Results. Every figure corresponds to a function.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：可视化变换结果。每个图形对应一个函数。
- en: This paper is organized as follows. A brief introduction of the Regression task
    and convention in this paper is included in Section 2\. In Section 3, OLS, its
    assumptions and possible solutions for violation are comprehensively explained.
    Section 4 is composed of a number of Regression models which enable OLS’s potential
    to be stimulated although the real data challenges its assumptions and some unexpected
    situations happen. Generalized Linear Models and an uncommon Regression named
    as Step-Wise are explored in Section 5\. Section 6 sets out to provide a quick
    overview of the strong bond between Regression and Deep Learning. In Section 7,
    conclusions about Regression are drawn and possible combination of Regression
    and Deep Learning in the future is discussed. In a nutshell, this well-established
    paper is an overview of Regression models and the relationship between Regression
    and Deep Learning, and hopefully this paper does make sense.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本文组织如下。第2节简要介绍了回归任务和本文中的约定。在第3节中，OLS及其假设和违反假设的可能解决方案得到了全面解释。第4节包含了多个回归模型，这些模型能够激发OLS的潜力，尽管真实数据挑战了其假设，并且发生了一些意外情况。第5节探讨了广义线性模型和一种不常见的回归方法——逐步回归。第6节旨在快速概述回归与深度学习之间的紧密联系。在第7节中，得出关于回归的结论，并讨论了未来回归与深度学习的可能结合。总之，本文是对回归模型及其与深度学习关系的概述，希望本文是有意义的。
- en: 2 The Regression Task
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 回归任务
- en: Our data looks like $\left\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\right\}$,
    which $y_{i}\in R$. We intend to train a model from our data set and implement
    it in unknown test sets. A standard for machine performs well is that low residuals(distance
    from predicted values to labels). When it comes to regression task, it is common
    way to implement Linear Regression.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据看起来像$\left\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\right\}$，其中$y_{i}\in
    R$。我们打算从数据集中训练一个模型，并在未知的测试集中实施它。机器表现良好的标准是低残差（从预测值到标签的距离）。当涉及到回归任务时，实现线性回归是常见的方法。
- en: '|  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle y$ | $\displaystyle=\hat{y}+\epsilon$ |  | (\theparentequation.1)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y$ | $\displaystyle=\hat{y}+\epsilon$ |  | (\theparentequation.1)
    |'
- en: '|  |  | $\displaystyle=\theta x+b+\epsilon$ |  | (\theparentequation.2) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\theta x+b+\epsilon$ |  | (\theparentequation.2) |'
- en: $\theta$ is called slope(gradient) or coefficient and $b$ is called intercept.
    $\theta$ explains when $x$ changes to what extent $\hat{y}$ is going to change.
    $X=\left\{x_{1},x_{2},...,x_{n}\right\}$ is known as input space and $Y=\left\{y_{1},y_{2},...,y_{n}\right\}$
    as output space. $x_{i}$ is called a sample, and $x_{ij}$ means the j-th feature
    of the i-th sample. $y$ is the label and $\hat{y}$ is the prediction. And $\epsilon$
    is the error accompanied by every prediction and is also called the distance from
    $\hat{y}$ to $y$(residual). In ordinary least squares, the model assumes that
    $y$ is actually sampled from Gaussian Distribution and every sample is with noise.
    Thus, it can also called noise in statistics. But in this paper, I am going to
    use error for that.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta$被称为斜率（梯度）或系数，$b$被称为截距。$\theta$解释了当$x$变化时，$\hat{y}$将变化到什么程度。$X=\left\{x_{1},x_{2},...,x_{n}\right\}$被称为输入空间，而$Y=\left\{y_{1},y_{2},...,y_{n}\right\}$被称为输出空间。$x_{i}$被称为样本，$x_{ij}$表示第$i$个样本的第$j$个特征。$y$是标签，$\hat{y}$是预测值。$\epsilon$是每个预测值伴随的误差，也称为从$\hat{y}$到$y$的距离（残差）。在普通最小二乘法中，模型假设$y$实际上是从高斯分布中采样的，每个样本都有噪声。因此，它也可以称为统计中的噪声。但在本文中，我将用误差来表示。
- en: 3 Ordinary Least Squares
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 普通最小二乘法
- en: In Machine Learning, we always figure out the best model by minimizing our objective
    function, which is also known as cost function. OLS(Ordinary Least Squares) serves
    as an effective loss function as long as the model satisfies six necessary assumptions.
    Then it can choose an unbiased model of minimum variance by minimizing the function
    below. And $J(\theta)$ is convex. Thus, set partial derivative of $\theta$ zero
    and then we can get the best parameter $\theta^{*}$. Note that only when $X^{T}X$
    is full rank, equation(3) does make sense. Some books may multiply $J(\theta)$
    by $1/n$, which is convenient for computation. Note that $X$ and $Y$ are matrices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们总是通过最小化目标函数来找出最佳模型，该目标函数也称为成本函数。OLS（普通最小二乘法）作为一种有效的损失函数，只要模型满足六个必要的假设，就能发挥作用。然后，通过最小化下面的函数，它可以选择一个方差最小的无偏模型。并且$J(\theta)$是凸的。因此，设置$\theta$的偏导数为零，然后我们可以得到最佳参数$\theta^{*}$。请注意，只有当$X^{T}X$是满秩时，方程（3）才有意义。有些书籍可能会将$J(\theta)$乘以$1/n$，这对于计算很方便。请注意，$X$和$Y$是矩阵。
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\sum\limits_{i}^{n}(y_{i}-(\theta
    x_{i}+b))^{2}$ |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\sum\limits_{i}^{n}(y_{i}-(\theta
    x_{i}+b))^{2}$ |  | (2) |'
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=(X^{T}X)^{-1}X^{T}Y$ |  | (3)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=(X^{T}X)^{-1}X^{T}Y$ |  | (3)
    |'
- en: 3.1 Prior Assumptions
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 先验假设
- en: In this section, six necessary assumptions is studied. And possible solutions
    for unsatisfied situations are also covered.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本节研究了六个必要的假设，并涵盖了不满足情况的可能解决方案。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Linearity. In other words, only Straight Line models are permitted. If the relationship
    between $X$ and $y$ is a non-linear model, e.g. $y=X^{4}$, the whole regression
    model crashes. And the recipe for this situation is applying feature transformation.
    By doing so, the whole relationship between $X$ and $y$ is changed for good. Hence,
    we must also take the correlation between $X$ and $y$ into consideration. Correlation
    can be told by calculating $R^{2}$ (Coefficient Of Determination), which stands
    for the ability to predict $y$ by observing $X$. When $R^{2}=1$, it means the
    loss of this predictor is 0\. If $R^{2}=0$, it is equivalent to that the predictor
    is constant, which indicates $X$ has nothing to do with $y$. As shown in Table
    1, in Column $R^{2}$, the value in bracket stands for the original coefficient.
    And the same goes for Column Linearity. Note that I use 10000 random points from
    $y=2(X-3)^{2}+5+N$, which N stands for noise and it varies from 0 to 1\. X follows
    random and even distribution.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性。换句话说，只允许直线模型。如果 $X$ 和 $y$ 之间的关系是非线性模型，例如 $y=X^{4}$，整个回归模型就会崩溃。应对这种情况的方法是应用特征变换。通过这样做，$X$
    和 $y$ 之间的整体关系会得到根本改变。因此，我们还必须考虑 $X$ 和 $y$ 之间的相关性。相关性可以通过计算 $R^{2}$（决定系数）来表示，这表示通过观察
    $X$ 来预测 $y$ 的能力。当 $R^{2}=1$ 时，表示该预测器的损失为0。如果 $R^{2}=0$，则等于该预测器是常数，表明 $X$ 与 $y$
    无关。如表1所示，在 $R^{2}$ 列中，括号中的值表示原始系数。线性列也是如此。注意，我使用了来自 $y=2(X-3)^{2}+5+N$ 的10000个随机点，其中
    N 代表噪声，范围从0到1。X 服从随机且均匀分布。
- en: '|  | $\displaystyle\overline{y}$ | $\displaystyle=\frac{1}{n}\sum\limits_{i=1}^{n}y_{i}$
    |  | (4) |'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\overline{y}$ | $\displaystyle=\frac{1}{n}\sum\limits_{i=1}^{n}y_{i}$
    |  | (4) |'
- en: '|  | $\displaystyle R^{2}$ | $\displaystyle=1-\frac{\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}{\sum\limits_{i=1}^{n}(y_{i}-\overline{y})^{2}}$
    |  | (5) |'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle R^{2}$ | $\displaystyle=1-\frac{\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}{\sum\limits_{i=1}^{n}(y_{i}-\overline{y})^{2}}$
    |  | (5) |'
- en: '![Refer to caption](img/401b37eb0854eb706e9ccb2c1efb0d0c.png)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/401b37eb0854eb706e9ccb2c1efb0d0c.png)'
- en: (a) Errors
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 误差
- en: '![Refer to caption](img/3550fcaffbb74f69e1231d88a5907fd3.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/3550fcaffbb74f69e1231d88a5907fd3.png)'
- en: (b) Norml Distribution
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 正态分布
- en: '![Refer to caption](img/8fa611040d3cd424fc8fd61981b61857.png)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/8fa611040d3cd424fc8fd61981b61857.png)'
- en: (c) Q-Q of ND
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) 正态分布的 Q-Q 图
- en: 'Figure 3: In Figure(a), errors are not symmetrical and Q-Q plot doesn’t look
    like a line,which indicates errors don’t follow Normal Distribution. Figure(b)
    and Figure(c) show when errors follow Normal Distribution, the hist plot should
    look like bell curve and Q-Q plot should be a line.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：在图（a）中，误差不对称且 Q-Q 图看起来不像一条直线，这表明误差不符合正态分布。图（b）和图（c）显示，当误差符合正态分布时，直方图应该呈钟形曲线，而
    Q-Q 图应该是一条直线。
- en: 'TABLE I: Transformation Results About $R^{2}$ And Linearity'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 I：关于 $R^{2}$ 和线性的变换结果
- en: '| $X$ | $y$ | $Transformation$ | $R^{2}$(-1.602) | $Linearity(Non)$ |'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| $X$ | $y$ | $变换$ | $R^{2}$(-1.602) | $线性（非线性）$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1.450 | 9.994 | $X_{T}=log_{10}(x)$ | 0.764 | Non |'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1.450 | 9.994 | $X_{T}=log_{10}(x)$ | 0.764 | 非线性 |'
- en: '| 1.003 | 13.475 | $X_{T}=\sqrt{x}$ | 0.693 | Non |'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1.003 | 13.475 | $X_{T}=\sqrt{x}$ | 0.693 | 非线性 |'
- en: '| 3.179 | 5.935 | $X_{T}=exp(x)$ | 0.281 | Non |'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3.179 | 5.935 | $X_{T}=exp(x)$ | 0.281 | 非线性 |'
- en: '| 3.801 | 6.726 | $X_{T}=\frac{1}{x}$ | 0.870 | Non |'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3.801 | 6.726 | $X_{T}=\frac{1}{x}$ | 0.870 | 非线性 |'
- en: '| 1.294 | 11.704 | $X_{T}=(x-3)^{2}$ | 0.983 | Linear |'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1.294 | 11.704 | $X_{T}=(x-3)^{2}$ | 0.983 | 线性 |'
- en: '| 1.399 | 10.915 | $X_{T}=x^{3}$ | 0.344 | Non |'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1.399 | 10.915 | $X_{T}=x^{3}$ | 0.344 | 非线性 |'
- en: '| 3.591 | 5.951 | $X_{T}=x^{4}$ | 0.253 | Non |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3.591 | 5.951 | $X_{T}=x^{4}$ | 0.253 | 非线性 |'
- en: '| 2.987 | 6.742 | $X_{T}=x^{5}$ | 0.188 | Non |'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2.987 | 6.742 | $X_{T}=x^{5}$ | 0.188 | 非线性 |'
- en: '| 3.161 | 5.906 | $X_{T}=x+1$ | 0.616 | Non |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3.161 | 5.906 | $X_{T}=x+1$ | 0.616 | 非线性 |'
- en: '| 2.065 | 7.647 | $X_{T}=\frac{1}{x^{2}}$ | 0.904 | Non |'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2.065 | 7.647 | $X_{T}=\frac{1}{x^{2}}$ | 0.904 | 非线性 |'
- en: As shown in Table 1, $R^{2}$ changes when feature transformation is applied.
    And the purpose is to find linear model with the best $R^{2}.$ Note that linear
    means y w.r.t transformed X. And if so, scatter plot of original X and predicted
    y should fit the original distribution plot. This assumption is the most significant
    for Linear Regression and it may explain why Ridge or Lasso also performs badly
    when the relationship is nonlinear.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如表 1 所示，当应用特征变换时，$R^{2}$ 发生变化。其目的是找到具有最佳 $R^{2}$ 的线性模型。注意线性是指 $y$ 相对于变换后的 $X$。如果是这样，原始
    $X$ 和预测 $y$ 的散点图应适合原始分布图。这个假设对于线性回归来说是最重要的，它可能解释了为什么岭回归或套索回归在关系是非线性时也表现不佳。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Constant Error Variance [[7](#bib.bib7)]. It means errors are uniformly distributed,
    which in statistics is called no Heteroscedasticity. When we apply our model,
    we can get a bunch of predicted values via observing $X$. Then we can calculate
    errors between true values and predicted values. And we can also calculate the
    variance of errors. If errors follow normal distribution (equation 5), thus its
    variance is constant($\sigma^{2}$). Also, its distribution is symmetrical. In
    turn, the distribution of errors should also be uniform and symmetrical. So we
    can use error plot to detect it (Figure 3). And Q-Q Plot can detect whether the
    errors follow normal distribution. The data I use can be downloaded [here](https://www.kaggle.com/quantbruce/real-estate-price-prediction).
    I choose X2 house age as $X$ and Y house price of unit area for $y$. Note that
    I remove points that $X$ equals $0$. And I choose 200 points for study.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常数误差方差 [[7](#bib.bib7)]。这意味着误差是均匀分布的，在统计学中称为没有异方差性。当我们应用我们的模型时，我们可以通过观察$X$获得一系列预测值。然后，我们可以计算真实值与预测值之间的误差。我们还可以计算误差的方差。如果误差遵循正态分布（公式
    5），则其方差是恒定的（$\sigma^{2}$）。此外，其分布是对称的。反过来，误差的分布也应该是均匀和对称的。因此，我们可以使用误差图来检测它（图 3）。Q-Q
    图可以检测误差是否遵循正态分布。我使用的数据可以从[这里](https://www.kaggle.com/quantbruce/real-estate-price-prediction)下载。我选择
    X2 房屋年龄作为 $X$，Y 单位面积的房屋价格作为 $y$。注意我去除了 $X$ 等于 $0$ 的点。我选择了 200 个点进行研究。
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}}$
    |  | (6) |'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}}$
    |  | (6) |'
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^{2}}{2})}$
    |  | (7) |'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^{2}}{2})}$
    |  | (7) |'
- en: equation(6) represents standard normal distribution, $\mu=0,\sigma=1$. As shown
    in Figure 3, if errors follow normal distribution, the distribution should be
    uniform just like Figure (b) and histogram should be like bell curve. We can also
    draw a safe conclusion that if errors follow normal distribution, their distribution
    should fit the line in Q-Q Plot.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方程（6）表示标准正态分布，$\mu=0,\sigma=1$。如图 3 所示，如果误差遵循正态分布，则分布应像图 (b) 一样均匀，直方图应类似钟形曲线。我们还可以得出一个安全的结论：如果误差遵循正态分布，则其分布应适合
    Q-Q 图中的线。
- en: '![Refer to caption](img/23d138528d172f39a907f635337300c5.png)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考标题](img/23d138528d172f39a907f635337300c5.png)'
- en: (a) Before Log
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 对数前
- en: '![Refer to caption](img/379768c5d81a6e2b6ea5a75ea8297514.png)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考标题](img/379768c5d81a6e2b6ea5a75ea8297514.png)'
- en: (b) After Log
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 对数后
- en: 'Figure 4: The Relationship Between Residuals And Log'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4：残差与对数的关系
- en: After figuring out heteroscedasticity, we can come up with a question that how
    it influences our model and how to improve it. The most common way is to try feature
    transformation e.g. Log. As shown in Figure 4, it can, to some degree, make our
    errors’ distribution slightly more stable. It’s always an option to try, but not
    an effective method to handle the problem.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在确定了异方差性之后，我们可以提出一个问题：它如何影响我们的模型以及如何改进它。最常见的方法是尝试特征变换，例如对数。正如图 4 所示，它在一定程度上可以使我们误差的分布稍微更稳定。这始终是一个尝试的选项，但不是解决问题的有效方法。
- en: What’s more, we can apply Box Cox Transformation, which can make data more close
    to normal distribution. In statistics, if data follows normal distribution and
    then the variance of noise(error) is a constant($\sigma^{2}$). Thus,normality
    of data is likely to relieve heteroscedasticity. And data can be downloaded [here](https://archive.ics.uci.edu/ml/machine-%0Alearning-databases/wine-quality/winequality-white.csv).
    I choose total sulfur dioxide for $X$and quality for $y$. In Figure 5, it may
    relieve heteroscedasticity. Note you can’t always depend on it. Also, it can be
    worse(Figure 6).
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们可以应用Box-Cox变换，这可以使数据更接近正态分布。在统计学中，如果数据服从正态分布，则噪声（误差）的方差是一个常数（$\sigma^{2}$）。因此，数据的正态性可能会缓解异方差性。数据可以从[这里](https://archive.ics.uci.edu/ml/machine-%0Alearning-databases/wine-quality/winequality-white.csv)下载。我选择总硫酸盐作为
    $X$，质量作为 $y$。在图 5 中，它可能会缓解异方差性。注意，你不能总是依赖于它。它也可能变得更糟（图 6）。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Independent Errors(no autocorrelation, AC for short). For instance, you want
    to predict the shares in stock market. But the errors are correlated while they
    should be $i.i.d$(independent identically distributed). When a financial crisis
    happens, the shares is going to be extremely unstable in next few months,which
    means errors are going to increase sharply. It can be detected by Durbin Watson
    Test(Table 3) or drawing AC Plot. And if values of y axis in AC Plot vary from
    $(0,1]$, they mean Positive AC. If values equal 0, they mean Non AC. Otherwise,
    they mean Negative AC.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独立误差（无自相关，简称AC）。例如，你想预测股市中的股票。然而，误差是相关的，而它们应该是 $i.i.d$（独立同分布的）。当发生金融危机时，股票在接下来的几个月会变得极不稳定，这意味着误差会急剧增加。这可以通过杜宾-沃森检验（表
    3）或绘制AC图来检测。如果AC图中的y轴值变化范围为 $(0,1]$，则表示正自相关。如果值等于0，则表示无自相关。否则，表示负自相关。
- en: 'TABLE II: Durbin Watson Test'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '表 II: 杜宾-沃森检验'
- en: '| $Value$ | $Relationship$ |'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| $Value$ | $Relationship$ |'
- en: '| --- | --- |'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 2.0 | no Ac |'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2.0 | 无自相关 |'
- en: '| 0.0-2.0 | positive AC |'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.0-2.0 | 正自相关 |'
- en: '| 2.0 - 4.0 | negative AC |'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2.0 - 4.0 | 负自相关 |'
- en: The autocorrelation can affect errors’ standard deviation while it’s unlikely
    to have an influence on model’s coefficient and intercept [[8](#bib.bib8)].
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自相关可能会影响误差的标准差，但不太可能影响模型的系数和截距 [[8](#bib.bib8)]。
- en: There’re two common ways to fix it. The first is to add omitted variables. For
    example, you want to predict stock performaces by time. Undoubtedly, the model
    is of hight autocorrelation. We can, however, add S&P 500\. And hopefully, it
    may relieve autocorrelation. The second is to switch another function. You can
    transform your linear model into a squared model.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有两种常见的解决方法。第一种是添加遗漏的变量。例如，你想通过时间预测股票表现。毫无疑问，模型具有很高的自相关性。然而，我们可以添加标普500指数。希望它可以缓解自相关性。第二种是更换其他函数。你可以将线性模型转换为平方模型。
- en: •![Refer to caption](img/61f96c1e71017e7761b3ced546fc8a9b.png)
  id: totrans-102
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: •![参见说明](img/61f96c1e71017e7761b3ced546fc8a9b.png)
- en: (a) Density Distribution
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 密度分布
- en: '![Refer to caption](img/023116ed048cbd14fe6f0a979681834a.png)'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/023116ed048cbd14fe6f0a979681834a.png)'
- en: (b) Original Residual
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 原始残差
- en: '![Refer to caption](img/65f1ceba6559b7edbcc94e8e8bc53d09.png)'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/65f1ceba6559b7edbcc94e8e8bc53d09.png)'
- en: (c) Final Residual
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) 最终残差
- en: 'Figure 5: Box Cox Transformation May Help'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5: Box-Cox 变换可能有帮助'
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No Multicollinearity. If independent variables are related to each other, there’s
    Multicollinearity in data. We can use Variance Inflation Factor(VIF) to detect
    it($R^{2}$ is Coefficient Of Determination). If value = 1, it implies that there
    is no Multicollinearity among the predictors. If value >5, it implies there’s
    potential Multicollinearity. If value >10, it implies apparent Multicollinearity.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无多重共线性。如果自变量之间相关，则数据中存在多重共线性。我们可以使用方差膨胀因子（VIF）来检测它（$R^{2}$ 是决定系数）。如果值 = 1，表示预测变量之间没有多重共线性。如果值
    >5，则表示存在潜在的多重共线性。如果值 >10，则表示明显的多重共线性。
- en: '|  | $softmax(x)=\frac{e^{x_{i}}}{\sum_{i=1}^{n}e^{x_{i}}}$ |  | (8) |'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $softmax(x)=\frac{e^{x_{i}}}{\sum_{i=1}^{n}e^{x_{i}}}$ |  | (8) |'
- en: Our goal of Regression model is to figure out the relationship between independent
    variable(X) and dependent variable(y) by finding a proper coefficient. But when
    there’s Multicollinearity, the coefficient is unable to interpret. We actually
    don’t know what exactly the relationship is. However, if we just want to make
    good predictions, it’s still effective [[7](#bib.bib7)]. And if the degree of
    Multicollinearity is moderate, you don’t have to care about it too much. We can
    remove highly correlated variables or increase sample size.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回归模型的目标是通过找到合适的系数来弄清楚自变量（X）和因变量（y）之间的关系。但当存在多重共线性时，系数无法解释。我们实际上不知道具体的关系是什么。然而，如果我们只是想进行良好的预测，这仍然是有效的[[7](#bib.bib7)]。如果多重共线性的程度适中，你不必太过担心。我们可以去除高度相关的变量或增加样本量。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Normality Of Data [[7](#bib.bib7)]. Box-Cox is the efficient transformation
    to make data more close to normal distribution. Normalization and some basic feature
    transformation may help. And also try increasing data size.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据的正态性[[7](#bib.bib7)]。Box-Cox变换是使数据更接近正态分布的有效变换。标准化和一些基本特征变换可能有所帮助。同时尝试增加数据量。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No Exogeneity. If $X$ we choose iteself is of little influence on $y$, which
    means the real prediction is not based on $X$, then there’s exogeneity [[7](#bib.bib7)].
    And the best solution is to make a good analysis about what on earth affects our
    predicted values and choose a suitable $X$.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非外生性。如果我们选择的$X$对$y$的影响很小，这意味着真正的预测不依赖于$X$，那么就存在外生性[[7](#bib.bib7)]。最佳解决方案是对究竟是什么影响我们的预测值进行良好的分析，并选择一个合适的$X$。
- en: 4 Alternative Models
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 替代模型
- en: In this section, OLS’s weaknesses are going to appear in Background. And each
    Background stands for a specific problem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，将会在背景部分展示普通最小二乘法（OLS）的弱点。每个背景代表一个特定的问题。
- en: 4.1 Ridge Regression
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 岭回归
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Background: high variance. OLS enables the predictor to perform good on training
    sets, however, this is an invitation to poor performance on testing sets, which
    is also known as overfitting. Generally speaking, the more complicated a model
    is, the poorer performance of the model on unknown sets. According to Occam’s
    rule, a model is more likely to do a good job on unknown sets if the model is
    simple. And the model is of high generalization ability.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景：高方差。OLS使得预测器在训练集上表现良好，但这也意味着在测试集上的表现可能很差，这也被称为过拟合。一般来说，模型越复杂，其在未知数据集上的表现越差。根据奥卡姆剃刀原则，如果模型简单，则更有可能在未知数据集上表现良好。并且模型具有较高的泛化能力。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Shrinkage. When OLS is applied in real life, the predictor’s coefficients can
    be too large in absolute. The coefficient of variable more correlated to $y$ is
    large while the one of variable less correlated to $y$ is also large,which is
    misleading to figure out the relationship between $X$ and $y$ [[3](#bib.bib3)].
    This phenomenon can account for poor performance on unknown sets. Thus, Ridge
    is going to implement shrinkage on coefficients and the extent of shrinkage counts
    on the degree of correlation. Typically, if a variable holds much predicting power,
    its coefficient is more likely to be big [[4](#bib.bib4)].
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收缩。现实生活中应用OLS时，预测变量的系数可能在绝对值上过大。与$y$相关性较大的变量的系数很大，而与$y$相关性较小的变量的系数也很大，这对于理解$X$与$y$之间的关系是具有误导性的[[3](#bib.bib3)]。这种现象可以解释在未知数据集上的表现不佳。因此，岭回归将对系数实施收缩，收缩的程度取决于相关性。通常，如果一个变量具有很强的预测能力，它的系数更有可能很大[[4](#bib.bib4)]。
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Nonorthogonal Solution [[3](#bib.bib3)]. Whether OLS can be directly computed
    just by derivative relies mainly on if $X^{T}X$ is orthogonal. If $X^{T}X$ is
    nonorthogonal, this means $X^{T}X$ is not reversible and the direct computation
    can’t work. $I$ is a unit matrix. There’re only small positive quantity on the
    diagonal of $kI$, the diagonal looks like a ridge in comparison with zero’s distribution(equation
    9).
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非正交解[[3](#bib.bib3)]。OLS是否可以通过导数直接计算，主要依赖于$X^{T}X$是否是正交的。如果$X^{T}X$是非正交的，这意味着$X^{T}X$不可逆，直接计算无法进行。$I$是单位矩阵。$kI$的对角线上只有很小的正数，与零的分布相比，对角线看起来像是一个岭（公式9）。
- en: '![Refer to caption](img/6460c8bdeb9066d8e0b11077913a09c2.png)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/6460c8bdeb9066d8e0b11077913a09c2.png)'
- en: (a) Density Distribution
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 密度分布
- en: '![Refer to caption](img/837fccf2f29760a886fc625aebc491e1.png)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/837fccf2f29760a886fc625aebc491e1.png)'
- en: (b) Original Residual
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 原始残差
- en: '![Refer to caption](img/29520168b7dbfe8711f56847b83b56ad.png)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/29520168b7dbfe8711f56847b83b56ad.png)'
- en: (c) Final Residual
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) 最终残差
- en: 'Figure 6: Box Cox Transformation May Suck'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6: Box Cox 转换可能很糟糕'
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Biased Model. The potential assumptions of Ridge are that $X^{T}X$ is nonorthogonal
    and coefficients need shrinkage. Hence, Ridge actually do a trade-off between
    bias and variance and it uses increased bias to obtain reduced variance. Because
    Ridge’s assumptions are correct in most cases, it is capable of producing a model
    with low variance.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有偏模型。Ridge 的潜在假设是 $X^{T}X$ 是非正交的且系数需要收缩。因此，Ridge 实际上是在偏差和方差之间进行权衡，它使用增加的偏差来获得降低的方差。由于
    Ridge 的假设在大多数情况下是正确的，它能够生成低方差的模型。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: L2-Penalty. Equation 10 is Ridge’s loss function. The loss function means not
    only an accurate model is required, its coefficients should be small. And $\lambda/2||\theta_{i}||^{2}$
    is called L2-norm. This kind of method is known as regularization in the sense
    that the model generated by regularization is of high generalization ability.
    Except direct computation, gradient descent is a common way to get the best parameter.
    Ridge always minuses the coefficient vector(equation 11.1 & 11.2).
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L2-惩罚。方程 10 是 Ridge 的损失函数。这个损失函数意味着不仅要求模型准确，其系数也应尽可能小。而 $\lambda/2||\theta_{i}||^{2}$
    被称为 L2-范数。这种方法被称为正则化，因为正则化生成的模型具有很高的泛化能力。除了直接计算，梯度下降是获得最佳参数的常用方法。Ridge 总是减去系数向量（方程
    11.1 和 11.2）。
- en: '|  | $\theta^{*}=(X^{T}X+kI)^{-1}X^{T}Y$ |  | (9) |'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\theta^{*}=(X^{T}X+kI)^{-1}X^{T}Y$ |  | (9) |'
- en: '|  | $J(\theta)=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\frac{\lambda}{2}&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (10) |'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $J(\theta)=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\frac{\lambda}{2}&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (10) |'
- en: '|  |'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda\theta$ |  | (\theparentequation.1) |'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda\theta$ |  | (\theparentequation.1) |'
- en: '|  |  | <math   alttext="\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|  |  | <math alttext="\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
- en: \vdots\\
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
- en: \theta_{2}\\
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \theta_{2}\\
- en: \vdots\\
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \theta_{n}\end{bmatrix}" display="inline"><semantics ><mrow ><mo  >=</mo><mrow
    ><mrow ><mo  >[</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mfrac  ><mrow ><mo
    rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mn >1</mn></msub></mrow></mfrac></mtd></mtr><mtr
    ><mtd ><mfrac  ><mrow ><mo rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mn >2</mn></msub></mrow></mfrac></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr ><mtd  ><mfrac ><mrow
    ><mo rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mi >n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo
    >]</mo></mrow><mo >+</mo><mrow  ><mi >λ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >[</mo><mtable rowspacing="0pt" ><mtr  ><mtd ><msub ><mi  >θ</mi><mn >1</mn></msub></mtd></mtr><mtr
    ><mtd ><msub  ><mi >θ</mi><mn >2</mn></msub></mtd></mtr><mtr ><mtd ><mi mathvariant="normal"
    >⋮</mi></mtd></mtr><mtr ><mtd  ><msub ><mi >θ</mi><mi >n</mi></msub></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="latexml"  >absent</csymbol><apply ><apply ><csymbol cd="latexml"
    >matrix</csymbol><matrix ><matrixrow  ><apply ><apply ><apply ><ci >𝑀</ci><ci
    >𝜃</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><cn type="integer"  >1</cn></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><ci >𝑀</ci><ci >𝜃</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci></matrixrow><matrixrow ><apply  ><apply ><apply ><ci >𝑀</ci><ci >𝜃</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑛</ci></apply></apply></apply></matrixrow></matrix></apply><apply
    ><ci >𝜆</ci><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><cn type="integer"  >1</cn></apply></matrixrow><matrixrow
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><cn type="integer"
    >2</cn></apply></matrixrow><matrixrow ><ci  >⋮</ci></matrixrow><matrixrow ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝑛</ci></apply></matrixrow></matrix></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\ \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
    \theta_{2}\\ \vdots\\ \theta_{n}\end{bmatrix}</annotation></semantics></math>
    |  | (\theparentequation.2) |
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \(\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\ \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
    \theta_{2}\\ \vdots\\ \theta_{n}\end{bmatrix}\)
- en: 4.2 Lasso Regression
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Lasso 回归
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Background: Ridge’s poor performance on outliers. Compared to OLS, Ridge is
    quite powerful but shrinkage means it just cuts down on coefficients’ ability
    of affecting the result. As such, each coefficient still has influence on the
    result which further indicates Ridge still cares about every sample’s loss. However,
    when outliers appear in the data, Ridge fails to deal with them in that it is
    sensitive to outliers.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景：Ridge 对离群点的表现不佳。与 OLS 相比，Ridge 非常强大，但收缩意味着它仅减少了系数对结果的影响能力。因此，每个系数仍然对结果有影响，这进一步表明
    Ridge 仍然关心每个样本的损失。然而，当数据中出现离群点时，Ridge 无法处理，因为它对离群点敏感。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feature Selection [[5](#bib.bib5)]. Unlike Ridge, Lasso implements feature selection
    because only one coefficient is saved and others are set zero, which is known
    as sparse solution. Therefore, only one sample has effect on the prediction in
    the sense that Lasso is insensitive to outliers and robust to small changes. Feature
    selection results in oscillation in optimization, which means Ridge is more stable
    than Lasso in gradient descent(Figure 7). What’s more, although Lasso has looked
    at all the data, only one sample does make sense. Thus, Lasso is also able to
    avoid overfitting. If performance of Lasso is excellent, then we can say which
    sample does work. So Lasso is an interpretable model compared to OLS and Ridge.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征选择 [[5](#bib.bib5)]。与 Ridge 不同，Lasso 实现了特征选择，因为只保存一个系数，其余的都设置为零，这称为稀疏解。因此，只有一个样本对预测有影响，这意味着
    Lasso 对离群点不敏感，并且对小的变化具有鲁棒性。特征选择会导致优化过程中的振荡，这意味着 Ridge 在梯度下降中比 Lasso 更稳定（图 7）。此外，尽管
    Lasso 考虑了所有数据，但只有一个样本才有意义。因此，Lasso 也能够避免过拟合。如果 Lasso 的表现出色，那么我们可以说哪个样本有效。因此，与
    OLS 和 Ridge 相比，Lasso 是一个更具可解释性的模型。
- en: '![Refer to caption](img/c16e273cbf0e599a3ec57d48c5723949.png)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/c16e273cbf0e599a3ec57d48c5723949.png)'
- en: 'Figure 7: Ridge is more stable than Lasso in optimization process(2016\. Deep
    Learning. MIT Press).'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7：Ridge 在优化过程中比 Lasso 更稳定（2016\. 深度学习。麻省理工学院出版社）。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: L1-Penalty. Lasso minuses a constant in gradient descent(equation 13.1 & 13.2).
    Suppose we’re on the top of a mountain, what Lasso does is just move a bitter
    farther while Ridge just goes where seems more steap. Hence, Ridge is more faster
    than Lasso. So when values are quite large, Ridge should be a better choice than
    Lasso. But when values are small, Lasso should be a better choice.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L1 惩罚。Lasso 在梯度下降中减去一个常数（公式 13.1 & 13.2）。假设我们站在山顶，Lasso 的做法是继续向前移动一段距离，而 Ridge
    则沿着更陡峭的方向前进。因此，Ridge 比 Lasso 更快。因此，当值相当大时，Ridge 应该是比 Lasso 更好的选择。但当值较小时，Lasso
    应该是更好的选择。
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\lambda&#124;&#124;\theta_{i}&#124;&#124;$
    |  | (12) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\lambda&#124;&#124;\theta_{i}&#124;&#124;$
    |  | (12) |'
- en: '|  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda sign(\theta)$ |  | (\theparentequation.1) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda sign(\theta)$ |  | (\theparentequation.1) |'
- en: '|  |  | <math   alttext="\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  | <math   alttext="\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
- en: \vdots\\
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
- en: sign(\theta_{2})\\
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: sign(\theta_{2})\\
- en: \vdots\\
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: sign(\theta_{n})\end{bmatrix}" display="inline"><semantics ><mrow ><mo  >=</mo><mrow
    ><mrow ><mo  >[</mo><mtable rowspacing="0pt"  ><mtr ><mtd ><mfrac  ><mrow ><mo
    rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mn >1</mn></msub></mrow></mfrac></mtd></mtr><mtr
    ><mtd ><mfrac  ><mrow ><mo rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mn >2</mn></msub></mrow></mfrac></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr ><mtd  ><mfrac ><mrow
    ><mo rspace="0em"  >∂</mo><mrow ><mi >M</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
    ><mo rspace="0em"  >∂</mo><msub ><mi >θ</mi><mi >n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo
    >]</mo></mrow><mo >+</mo><mrow  ><mi >λ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo >[</mo><mtable rowspacing="0pt" ><mtr  ><mtd ><mrow ><mi >s</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >θ</mi><mn >1</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mi >s</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><msub
    ><mi >θ</mi><mn >2</mn></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mi mathvariant="normal" >⋮</mi></mtd></mtr><mtr ><mtd  ><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><msub ><mi >θ</mi><mi >n</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><csymbol cd="latexml"  >absent</csymbol><apply ><apply ><csymbol cd="latexml"
    >matrix</csymbol><matrix ><matrixrow  ><apply ><apply ><apply ><ci >𝑀</ci><ci
    >𝜃</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><cn type="integer"  >1</cn></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply ><apply ><ci >𝑀</ci><ci >𝜃</ci></apply></apply><apply ><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><ci >⋮</ci></matrixrow><matrixrow ><apply  ><apply ><apply ><ci >𝑀</ci><ci >𝜃</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜃</ci><ci >𝑛</ci></apply></apply></apply></matrixrow></matrix></apply><apply
    ><ci >𝜆</ci><apply  ><csymbol cd="latexml"  >matrix</csymbol><matrix ><matrixrow
    ><apply  ><ci >𝑠</ci><ci >𝑖</ci><ci >𝑔</ci><ci >𝑛</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><cn type="integer"  >1</cn></apply></apply></matrixrow><matrixrow ><apply
    ><ci >𝑠</ci><ci >𝑖</ci><ci >𝑔</ci><ci >𝑛</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝜃</ci><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow ><ci >⋮</ci></matrixrow><matrixrow
    ><apply  ><ci >𝑠</ci><ci >𝑖</ci><ci >𝑔</ci><ci >𝑛</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝑛</ci></apply></apply></matrixrow></matrix></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\ \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
    sign(\theta_{2})\\ \vdots\\ sign(\theta_{n})\end{bmatrix}</annotation></semantics></math>
    |  | (\theparentequation.2) |
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \displaystyle=\begin{bmatrix}\frac{\partial M(\theta)}{\partial\theta_{1}}\\
    \frac{\partial M(\theta)}{\partial\theta_{2}}\\ \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
    sign(\theta_{2})\\ \vdots\\ sign(\theta_{n})\end{bmatrix}
- en: 4.3 Support Vector Regression
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 支持向量回归
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Margin Maximization. SVM(Support Vector Machine) is originally invented for
    classification problems [[9](#bib.bib9)]. Unlike other algorithms, SVM not only
    needs to classify all the data correctly but also requires the distance of data
    to the Hyper plane to be the biggest, which is known as widest street. Our linear
    model is $y=\theta^{T}X+b$. Among all the data points, the distance of the closest
    positive point to the hyper plane pluses the same distance of closest negative
    point is Margin($\gamma$). Then SVM is turned into maximizing Margin. And the
    same for SVR. We can turn the maximizing $\frac{2}{||\theta||}$ into minimizing
    $\frac{||\theta||^{2}}{2}$. Because $||\theta||$ is bigger than 0, $||\theta||^{2}$
    is proportional to $||\theta||$. Then the problem for SVR is like equation 15\.
    In equation 15, C is a coefficient for regularization and $L(x)$ is an undefined
    loss function.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边际最大化。SVM（支持向量机）最初是为分类问题发明的 [[9](#bib.bib9)]。与其他算法不同，SVM不仅需要正确分类所有数据，还要求数据到超平面的距离最大，这被称为最宽街道。我们的线性模型是
    $y=\theta^{T}X+b$。在所有数据点中，最接近超平面的正样本点到超平面的距离加上最接近超平面的负样本点的相同距离就是边际（$\gamma$）。然后，SVM被转化为最大化边际。SVR也是如此。我们可以将最大化
    $\frac{2}{||\theta||}$ 转化为最小化 $\frac{||\theta||^{2}}{2}$。因为 $||\theta||$ 大于 0，$||\theta||^{2}$
    与 $||\theta||$ 成正比。因此，SVR 的问题类似于方程 15。在方程 15 中，C 是一个正则化系数，而 $L(x)$ 是一个未定义的损失函数。
- en: '|  | $\min_{\theta,b}\,\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}+C\sum_{i=1}^{n}L(y_{i}-\hat{y_{i}})$
    |  | (14) |'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\min_{\theta,b}\,\frac{1}{2}||\theta||^{2}+C\sum_{i=1}^{n}L(y_{i}-\hat{y_{i}})$
    |  | (14) |'
- en: '![Refer to caption](img/9ff4d8a9d869890b744b3261cc91ce09.png)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/9ff4d8a9d869890b744b3261cc91ce09.png)'
- en: 'Figure 8: $\epsilon$-insensitive loss. Smola and Sch$\ddot{o}$lkopf, 2002'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8：$\epsilon$-不敏感损失。Smola 和 Sch$\ddot{o}$lkopf，2002
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\epsilon$- insensitive loss. SVR can tolerate mistakes which are no more than
    $\epsilon$, if those points predicted wrongly in dashed area $[f(x)-\epsilon,f(x)+\epsilon]$,
    then the losses of those points equal zero, which is called $\epsilon$-insensitive
    loss(Figure 8). In equation 16,z stands for loss. Actually, it just makes a trade-off
    between errors and complexity of the model. Hence, SVR is unlikely to overfit
    the data. Thus, SVR can reduce the variance of OLS.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\epsilon$-不敏感损失。SVR 可以容忍不超过 $\epsilon$ 的错误，如果这些点在虚线区域 $[f(x)-\epsilon,f(x)+\epsilon]$
    内被错误预测，那么这些点的损失为零，这被称为 $\epsilon$-不敏感损失（图 8）。在方程 16 中，z 代表损失。实际上，这只是对误差和模型复杂性之间的权衡。因此，SVR
    不容易过拟合数据。这样，SVR 可以减少 OLS 的方差。
- en: '|  | <math   alttext="\displaystyle\centering l_{\epsilon}(z)=\begin{cases}0&amp;,if\,&#124;z&#124;\leq\epsilon\\
    &#124;z&#124;-\epsilon&amp;otherwise\\'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\displaystyle\centering l_{\epsilon}(z)=\begin{cases}0\,,if\,|z|\leq\epsilon\\
    |z|-\epsilon\,,otherwise\\'
- en: \end{cases}\@add@centering" display="inline"><semantics ><mrow ><mrow ><msub
    ><mi >l</mi><mi >ϵ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mi >z</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    columnalign="left" ><mn >0</mn></mtd><mtd columnalign="left" ><mrow ><mo >,</mo><mi
    >i</mi><mi >f</mi><mo fence="false" lspace="0.170em" rspace="0.167em" stretchy="false"
    >&#124;</mo><mi >z</mi><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >≤</mo><mi >ϵ</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mi >z</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    >−</mo><mi >ϵ</mi></mrow></mtd><mtd  columnalign="left" ><mrow ><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >h</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >r</mi><mo lspace="0em" rspace="0em" >​</mo><mi >w</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\displaystyle\centering l_{\epsilon}(z)=\begin{cases}0&,if\,&#124;z&#124;\leq\epsilon\\
    &#124;z&#124;-\epsilon&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (15) |
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \end{cases}\@add@centering" display="inline"><semantics ><mrow ><mrow ><msub
    ><mi >l</mi><mi >ϵ</mi></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mi >z</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    columnalign="left" ><mn >0</mn></mtd><mtd columnalign="left" ><mrow ><mo >,</mo><mi
    >i</mi><mi >f</mi><mo fence="false" lspace="0.170em" rspace="0.167em" stretchy="false"
    >&#124;</mo><mi >z</mi><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >≤</mo><mi >ϵ</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mi >z</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    >−</mo><mi >ϵ</mi></mrow></mtd><mtd  columnalign="left" ><mrow ><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >h</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >r</mi><mo lspace="0em" rspace="0em" >​</mo><mi >w</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\displaystyle\centering l_{\epsilon}(z)=\begin{cases}0&,if\,&#124;z&#124;\leq\epsilon\\
    &#124;z&#124;-\epsilon&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (15) |
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dual Problem. Slack variables can be introduced to optimization problem(equation
    18). They means how many errors are allowed to make beyond $\epsilon$.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对偶问题。可以在优化问题（方程18）中引入松弛变量。这些变量表示允许的超出$\epsilon$的误差量。
- en: '|  | $\displaystyle\min_{\theta,b}\,\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}$
    | $\displaystyle+C\sum_{i=1}^{n}L(\xi_{i}+\hat{\xi}_{i})$ |  | (16) |'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta,b}\,\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}$
    | $\displaystyle+C\sum_{i=1}^{n}L(\xi_{i}+\hat{\xi}_{i})$ |  | (16) |'
- en: '|  | $\displaystyle y_{i}-\hat{y_{i}}$ | $\displaystyle\leq\epsilon+\xi_{i}$
    |  | (17) |'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle y_{i}-\hat{y_{i}}$ | $\displaystyle\leq\epsilon+\xi_{i}$
    |  | (17) |'
- en: '|  | $\displaystyle\hat{y_{i}}-y_{i}$ | $\displaystyle\leq\epsilon+\hat{\xi_{i}}$
    |  | (18) |'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{y_{i}}-y_{i}$ | $\displaystyle\leq\epsilon+\hat{\xi_{i}}$
    |  | (18) |'
- en: '|  | $\displaystyle\xi_{i}\geq 0\,$ | $\displaystyle,\hat{\xi_{i}}\geq 0$ |  |
    (19) |'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\xi_{i}\geq 0\,$ | $\displaystyle,\hat{\xi_{i}}\geq 0$ |  |
    (19) |'
- en: We can introduce Lagrange Multiplier $\mu_{i}\geq 0,\hat{\mu_{i}}\geq 0,\alpha_{i}\geq
    0,\hat{\alpha_{i}}\geq 0$.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以引入拉格朗日乘子$\mu_{i}\geq 0,\hat{\mu_{i}}\geq 0,\alpha_{i}\geq 0,\hat{\alpha_{i}}\geq
    0$。
- en: '|  |  | $\displaystyle L(\theta,b,\alpha,\hat{\alpha},\xi,\hat{\xi},\mu,\hat{\mu})$
    |  | (20) |'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle L(\theta,b,\alpha,\hat{\alpha},\xi,\hat{\xi},\mu,\hat{\mu})$
    |  | (20) |'
- en: '|  |  | $\displaystyle=\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}+C\sum_{i=1}^{n}(\xi_{i}+\hat{\xi_{i}})-\sum_{i=1}^{n}\mu_{i}\xi_{i}-\sum_{i=1}^{n}\hat{\mu_{i}}\hat{\xi_{i}}$
    |  |'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}+C\sum_{i=1}^{n}(\xi_{i}+\hat{\xi_{i}})-\sum_{i=1}^{n}\mu_{i}\xi_{i}-\sum_{i=1}^{n}\hat{\mu_{i}}\hat{\xi_{i}}$
    |  |'
- en: '|  |  | $\displaystyle+\sum_{i=1}^{n}\alpha_{i}(y_{i}-\hat{y_{i}}-\epsilon-\xi_{i})+\sum_{i=1}^{n}\hat{\alpha_{i}}(\hat{y_{i}}-y_{i}-\epsilon-\hat{\xi_{i}})$
    |  |'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\sum_{i=1}^{n}\alpha_{i}(y_{i}-\hat{y_{i}}-\epsilon-\xi_{i})+\sum_{i=1}^{n}\hat{\alpha_{i}}(\hat{y_{i}}-y_{i}-\epsilon-\hat{\xi_{i}})$
    |  |'
- en: And set partial derivative of w,b,$\xi_{i}$ and $\hat{\xi_{i}}$ zero.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并将w、b、$\xi_{i}$和$\hat{\xi_{i}}$的偏导数设为零。
- en: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})x_{i}$
    |  | (21) |'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})x_{i}$
    |  | (21) |'
- en: '|  | $\displaystyle 0$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})$
    |  | (22) |'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle 0$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})$
    |  | (22) |'
- en: '|  | $\displaystyle C$ | $\displaystyle=\alpha_{i}+\mu_{i}$ |  | (23) |'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle C$ | $\displaystyle=\alpha_{i}+\mu_{i}$ |  | (23) |'
- en: '|  | $\displaystyle C$ | $\displaystyle=\hat{\alpha_{i}}+\hat{\mu_{i}}$ |  |
    (24) |'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle C$ | $\displaystyle=\hat{\alpha_{i}}+\hat{\mu_{i}}$ |  |
    (24) |'
- en: Thus, we can get Dual Problem for SVR.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，我们可以得到 SVR 的对偶问题。
- en: '|  | $\displaystyle\max_{\alpha,\hat{\alpha}}$ | $\displaystyle\sum_{i=1}^{n}\hat{y_{i}}(\hat{\alpha_{i}}-\alpha_{i})-\epsilon(\hat{\alpha_{i}}+\alpha_{i})$
    |  | (25) |'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\alpha,\hat{\alpha}}$ | $\displaystyle\sum_{i=1}^{n}\hat{y_{i}}(\hat{\alpha_{i}}-\alpha_{i})-\epsilon(\hat{\alpha_{i}}+\alpha_{i})$
    |  | (25) |'
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})(\hat{\alpha_{j}}-\alpha_{j})(x_{i})^{T}x_{j}$
    |  |'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})(\hat{\alpha_{j}}-\alpha_{j})(x_{i})^{T}x_{j}$
    |  |'
- en: '|  | $\displaystyle s.t.$ | $\displaystyle\,\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})=0$
    |  |'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle s.t.$ | $\displaystyle\,\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})=0$
    |  |'
- en: •
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Kernel Trick. Typically, the model is nonlinear in 1D space. Thus, we use $\phi(x)$
    to represent the transformed x. Some common kernel functions are listed in table
    3.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 核技巧。通常，模型在一维空间中是非线性的。因此，我们使用 $\phi(x)$ 来表示变换后的 x。一些常见的核函数列在表 3 中。
- en: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})\phi(x)$
    |  | (26) |'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})\phi(x)$
    |  | (26) |'
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})k(x,x_{i})+b$
    |  | (27) |'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle f(x)$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})k(x,x_{i})+b$
    |  | (27) |'
- en: 'TABLE III: Common Kernel Functions'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '表 III: 常见核函数'
- en: '| $Name$ | $Expression$ | $Parameters$ |'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| $名称$ | $表达式$ | $参数$ |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Linear Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(x_{i}\right)^{T}x_{j}$
    | d = 1 |'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 线性核函数 | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(x_{i}\right)^{T}x_{j}$
    | d = 1 |'
- en: '| Polynomial Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(\left(x_{i}\right)^{T}x_{j}\right)^{d}$
    | $\geq 1$ |'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 多项式核函数 | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(\left(x_{i}\right)^{T}x_{j}\right)^{d}$
    | $\geq 1$ |'
- en: '| Gaussian Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\exp\left(-\frac{\left\&#124;x_{i}-x_{j}\right\&#124;^{2}}{2\sigma^{2}}\right)$
    | $\sigma>0$ |'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 高斯核函数 | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\exp\left(-\frac{\left\&#124;x_{i}-x_{j}\right\&#124;^{2}}{2\sigma^{2}}\right)$
    | $\sigma>0$ |'
- en: '| Sigmoid Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\tanh\left(\beta\left(x_{i}\right)^{T}x_{j}+\theta\right)$
    | $\beta>0,\theta\textless 0$ |'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Sigmoid 核函数 | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\tanh\left(\beta\left(x_{i}\right)^{T}x_{j}+\theta\right)$
    | $\beta>0,\theta\textless 0$ |'
- en: 4.4 Random Forest Regression
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 随机森林回归
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tree Structure [[12](#bib.bib12)]. A walk-through of tree structure is going
    to be given in this section. In regression tree, there’re root node,internal nodes
    and leaf node. For instance, if we’re going to predict a man’s height. And we
    get men’s and women’s heights. Thus, the root node is man or not. If a point which
    we need to predict is $(68,171)$. It means the height of a man weighing 68kg is
    171cm. If we use tree regression, then our internal node is $>60?$ And the second
    internal node is $<70?$ and so on. And at last, the leaf node is the result we
    predict. Hence, the leaf node is the predicted height.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 树结构 [[12](#bib.bib12)]。本节将介绍树结构。在回归树中，有根节点、内部节点和叶节点。例如，如果我们要预测一个人的身高，并且我们获取了男性和女性的身高数据。那么，根节点是“是否是男性”。如果我们需要预测的点是
    $(68,171)$，这意味着一个体重 68kg 的男性身高是 171cm。如果我们使用树回归，那么我们的内部节点是 $>60?$ 第二个内部节点是 $<70?$
    以此类推。最后，叶节点就是我们预测的结果。因此，叶节点就是预测的身高。
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Classification And Regression Tree [[13](#bib.bib13)]. CART includes feature
    selection,generating trees and pruning. CART assumes that decision tree is a Binary
    tree. The decision tree is equivalent to dividing features into two groups recursively.
    It divides the input space into limited units and predict the distribution. Suppose
    our data $D=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})\}$. Decision Tree
    splits input space into $M$ units $R_{1},R_{2},\dots,R_{M}$ and at the end of
    every unit is the output value $c_{m}$. And we can minimize squared errors of
    output values and true values. And it’s obvious that the best output value in
    every unit should be the average value. But the question is that how to split
    the input space. And we choose $x_{j}$ randomly and its output value $s$. And
    we split the input space into two space by their output values. $R_{1}(j,s)=\{x|x_{j}\leq
    s\}$ and $R_{2}(j,s)=\{x|x_{j}>s\}$ $x_{j}$ is called splitting variable and $s$
    is splitting point. And we find $c_{1},c_{2}$ in $R_{1},R_{2}$ via squared least
    errors. We also want $c_{1}+c_{2}$ to be small enough. That’s how we finally get
    j, s.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类与回归树[[13](#bib.bib13)]。CART包括特征选择、生成树和剪枝。CART假设决策树是一个二叉树。决策树等同于递归地将特征分为两组。它将输入空间划分为有限的单元并预测分布。假设我们的数据是$D=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})\}$。决策树将输入空间划分为$M$个单元$R_{1},R_{2},\dots,R_{M}$，每个单元的末端是输出值$c_{m}$。我们可以最小化输出值与真实值的平方误差。显然，每个单元中最佳的输出值应为平均值。但问题是如何划分输入空间。我们随机选择$x_{j}$及其输出值$s$。然后我们通过其输出值将输入空间划分为两个空间。$R_{1}(j,s)=\{x|x_{j}\leq
    s\}$和$R_{2}(j,s)=\{x|x_{j}>s\}$，$x_{j}$称为分裂变量，$s$是分裂点。我们通过平方最小误差在$R_{1},R_{2}$中找到$c_{1},c_{2}$。我们还希望$c_{1}+c_{2}$尽可能小。这就是我们最终得到j和s的方法。
- en: '|  | $\displaystyle\min\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}$ |  | (28) |'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\min\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}$ |  | (28) |'
- en: '|  | $\displaystyle\hat{c_{m}}=average(y_{i}&#124;x_{i}\in R_{m})$ |  | (29)
    |'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{c_{m}}=average(y_{i}|x_{i}\in R_{m})$ |  | (29) |'
- en: '|  | $\displaystyle\min_{j,s}(\min_{c1}\sum_{x_{i}\in R_{1}}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\in
    R_{2}}(y_{i}-c_{2})^{2})$ |  | (30) |'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{j,s}(\min_{c1}\sum_{x_{i}\in R_{1}}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\in
    R_{2}}(y_{i}-c_{2})^{2})$ |  | (30) |'
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Pruning [[13](#bib.bib13)]. Decision Tree implements recursive binary splitting
    to make more accurate predictions. And if the size of data and the input space
    is quite large. The structure of the tree is complicated. Sadly, a complex model
    easily gives birth to Overfitting. So it’s necessary to make our model more simple.
    Therefore, from the bottom of the tree, we cut down some child trees. Then we
    can get a tree sequence $\{T_{0},T_{1},\dots,T_{n}\}$($T_{0}$ is the root node).
    Then we employ cross-validation to choose the best child tree from it. At the
    same time, we also expect our model to perform well. Hence, we apply a loss function
    to measure the differences of performances in the process of pruning. In equation(32),
    T is arbitrary child tree. C(T) means errors on the training data. $|T|$ is the
    number of leaf nodes in a child tree. $\alpha$ is a parameter, which decides the
    regularization term. If $\alpha$ is big, it means hard punishment and this results
    in a simple tree. If $\alpha$ is small, it means soft punishment and this leads
    to a more complicated tree respectively.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝[[13](#bib.bib13)]。决策树通过递归二叉分裂来实现更准确的预测。如果数据量和输入空间非常大，树的结构会变得复杂。遗憾的是，复杂的模型容易导致过拟合。因此，有必要简化我们的模型。因此，我们从树的底部剪掉一些子树。然后我们可以得到一个树序列$\{T_{0},T_{1},\dots,T_{n}\}$（$T_{0}$是根节点）。接着，我们使用交叉验证从中选择最佳的子树。同时，我们也希望我们的模型表现良好。因此，我们应用损失函数来测量剪枝过程中性能的差异。在公式(32)中，T是任意的子树。C(T)表示训练数据上的误差。$|T|$是子树中的叶子节点数。$\alpha$是一个参数，它决定了正则化项。如果$\alpha$很大，意味着严厉的惩罚，这将导致一个简单的树。如果$\alpha$很小，意味着温和的惩罚，这将导致一个更复杂的树。
- en: '|  | $C_{\alpha}(T)=C(T)+\alpha&#124;T&#124;$ |  | (31) |'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $C_{\alpha}(T)=C(T)+\alpha|T|$ |  | (31) |'
- en: •
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ensemble Learning [[14](#bib.bib14)]. We use $1/3$ of our data to evaluate our
    model, which is called out of bag data. And $2/3$ of our data to be a new data
    set. Then we select subsets randomly from the new data set, which is known as
    Bagging. Every time we select one subset of the complete data set and then the
    subset is placed back. The number of points in different subsets is the same.
    We train different tree models for every different data. Finally we make an average
    of all trees’ variables as our final model variable in order to cut down on the
    variance. Hence, it’s an accurate model. And it is able to maintain accuracy although
    most of data is missing in that the model only randomly select a subset to train.
    However, it may overfit data when there’re some outliers in data.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成学习 [[14](#bib.bib14)]。我们使用 $1/3$ 的数据来评估模型，这称为袋外数据。使用 $2/3$ 的数据作为新的数据集。然后我们从新的数据集中随机选择子集，这称为
    Bagging。每次我们选择一个完整数据集的子集，然后该子集被放回。不同子集中的点数是相同的。我们为每个不同的数据训练不同的树模型。最后，我们对所有树的变量取平均，作为最终模型变量，以减少方差。因此，它是一个准确的模型。尽管大多数数据缺失，但由于模型只随机选择一个子集进行训练，它能够保持准确性。然而，当数据中存在一些离群点时，可能会过拟合数据。
- en: 4.5 Boosted Regression Tree
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 提升回归树
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Biased Feature Selection [[15](#bib.bib15)]. It’s almost the same as Random
    Forest. They both select the random subset and then a new tree is generated. In
    Random Forest, the probability of data is selected is the same, which is almost
    unbiased. But in Boosted Regression Tree, it is going to give weights to every
    data point. For instance, first time we select a subset and we build a tree model
    for it. Before this subset is placed back, prediction errors are calculated for
    every point. If the error is high, this point is likely to be given large weights,
    which indicates its probability of being selected is higher than others. To summarize,
    Boosted Regression Tree focuses on the errors and is going to mix it. Whereas,
    if there’re many outliers in data, it just sucks. But it is robust to missing
    values just like Random Forest because they both select subsets to fit.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏倚特征选择 [[15](#bib.bib15)]。这与随机森林几乎相同。它们都选择随机子集，然后生成一棵新树。在随机森林中，数据被选择的概率是相同的，几乎没有偏差。但在提升回归树中，它会给每个数据点赋予权重。例如，第一次我们选择一个子集并为其建立一棵树模型。在该子集被放回之前，会计算每个点的预测误差。如果误差很高，该点可能会被赋予较大的权重，这表明它被选择的概率高于其他点。总之，提升回归树关注误差并进行混合。而如果数据中有许多离群点，它表现得很差。但它对缺失值具有鲁棒性，就像随机森林一样，因为它们都选择子集进行拟合。
- en: 4.6 Elastic Net Regression
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 弹性网回归
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Background: Multicollinearity. One of OLS’s assumptions is that no multicollinearity.
    However, in multivariate regression, $X$ can be sometimes dependent. If this happens,
    OLS, Ridge and Lasso fail to play their part.'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景：多重共线性。OLS 的假设之一是没有多重共线性。然而，在多变量回归中，$X$ 有时可能是相关的。如果发生这种情况，OLS、Ridge 和 Lasso
    都无法发挥作用。
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Encourage Group Effect [[6](#bib.bib6)]. In multivariate regression, if some
    samples is correlated to each other, OLS is likely to take one sample, not caring
    which one is selected while strongly correlated samples are on the same boat in
    Elastic Net Regression. And it view them as a whole, which is called group effect.
    It does automatic variable selection and continuous shrinkage, and it select groups
    of correlated samples [[6](#bib.bib6)], which is similar to clustering methods.
    However, this model doesn’t reduce the variance and extra bias increases.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鼓励群体效应 [[6](#bib.bib6)]。在多变量回归中，如果一些样本彼此相关，OLS 可能只选择一个样本，而不在意选择的是哪个，同时强相关的样本在弹性网回归中被视为一整体，这称为群体效应。它进行自动变量选择和连续收缩，并选择相关样本的组
    [[6](#bib.bib6)]，这类似于聚类方法。然而，该模型并不能减少方差，额外的偏差会增加。
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generation Of Lasso And Ridge. Elastic Net is a middle ground between Ridge
    Regression and Lasso Regression. It mixes Lasso’s loss function with Ridge’s.
    It has the parameter r to control the mix ratio. If $r=0$, it’s Ridge. If $r=1$,
    it’s Lasso.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Lasso 和 Ridge 的生成。弹性网是 Ridge 回归和 Lasso 回归之间的折中方案。它将 Lasso 的损失函数与 Ridge 的混合。它有参数
    r 来控制混合比例。如果 $r=0$，就是 Ridge。如果 $r=1$，就是 Lasso。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $N\gg P$. N is the number of samples and P is the number of features of every
    sample. When in multivariate regression, Elastic Net Regression can play a key
    role in $N\gg P$ cases.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $N\gg P$。N 是样本数量，P 是每个样本的特征数量。在多变量回归中，当 $N\gg P$ 时，弹性网回归可以发挥关键作用。
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+r\lambda&#124;&#124;\theta_{i}&#124;&#124;+\frac{1-r}{2}\lambda&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (32) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+r\lambda&#124;&#124;\theta_{i}&#124;&#124;+\frac{1-r}{2}\lambda&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (32) |'
- en: 4.7 Least Angle Regression
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 最小角回归
- en: •
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Joint Least Squares Direction [[16](#bib.bib16)]. To begin with, the model selects
    the coefficient $\beta_{j}$ and calculate errors. When some other sample $x_{k}$
    has more correlation with errors than $x_{j}$ has. Hence, the model increases
    $(\beta_{j},\beta_{k})$ in their joint least squares direction until $x_{m}$ has
    more correlation with errors. Thus, the model increases $(\beta_{j},\beta_{k},\beta_{m})$
    in their joint least squares direction. The model comes to its end until all samples
    in the model. Therefore, it can also settle samples’ autocorrelation in high dimension.
    OLS is the special case of LARS. When LARS doesn’t increase in joint least squares
    direction, the model becoms OLS. By the way, LARS is also powerful when $N\gg
    P$ just like Elastic Net Regression.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联合最小二乘方向 [[16](#bib.bib16)]。首先，模型选择系数 $\beta_{j}$ 并计算误差。当某个其他样本 $x_{k}$ 对误差的相关性比
    $x_{j}$ 更高时，模型将增加 $(\beta_{j},\beta_{k})$ 的联合最小二乘方向，直到 $x_{m}$ 对误差的相关性更高。因此，模型将增加
    $(\beta_{j},\beta_{k},\beta_{m})$ 的联合最小二乘方向。模型将继续进行直到所有样本都被包括在模型中。因此，它也可以解决高维数据的自相关问题。OLS
    是 LARS 的特例。当 LARS 不再增加联合最小二乘方向时，模型变成 OLS。顺便提一下，LARS 在 $N\gg P$ 时也很强大，就像弹性网回归一样。
- en: •
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sensitive To Outliers. LARS is a method which iteratively refit the errors.
    When there’re outliers in data, then LARS doesn’t make sense.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对外点敏感。LARS 是一种迭代重新拟合误差的方法。当数据中存在外点时，LARS 是没有意义的。
- en: •
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Easily Modified. It’s simple for LARS to join with other models such as Lasso.
    LARS-Lasso employs the Lasso’s loss function and applies the LARS’s method of
    coefficient selection.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 易于修改。LARS 很容易与其他模型如 Lasso 结合。LARS-Lasso 采用 Lasso 的损失函数并应用 LARS 的系数选择方法。
- en: 4.8 RANSAC Regression
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 RANSAC 回归
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Background. Although Lasso is insensitive to a few outliers, Lasso doesn’t work
    when data is filled with a large number of outliers,let alone OLS.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景。尽管 Lasso 对少量外点不敏感，但当数据中充满大量外点时，Lasso 失效，更不用说 OLS 了。
- en: •
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Random Sample Consensus Set [[17](#bib.bib17)]. RANSAC is an iterative method.
    And it has a error threshold $\epsilon$. To begin with, it select a subset of
    the whole data and find a model to fit it. Then, use the model to test the rest
    of the data. If point’ loss on the model is no more than $\epsilon$ , then add
    it to the consensus set, which is full of inliers. And this process is iterative.
    When iterative times is reached, the process comes to its end. RANSAC is going
    to make the number of points in consensus set as large as possible.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机样本一致性集 [[17](#bib.bib17)]。RANSAC 是一种迭代方法，并且具有一个误差阈值 $\epsilon$。首先，它选择整个数据的一个子集并找到一个模型来拟合它。然后，使用模型测试其余的数据。如果点在模型上的损失不超过
    $\epsilon$，则将其添加到一致性集，这个集满是内点。这个过程是迭代的。当达到迭代次数时，过程结束。RANSAC 旨在使一致性集中的点数尽可能多。
- en: iteration times ni = 0while *i <n* do       Randomly Select Inliers From Data      
    Find A Model M To Fit       Test Other data Via The Model       if *points fit
    M* then             Inliers Set $\leftarrow$ points      else            Ouliers
    Set $\leftarrow$ points
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代次数 ni = 0当 *i <n* 时       从数据中随机选择内点       找到一个模型 M 来拟合       通过模型测试其他数据      
    如果 *点符合 M*     则             内点集 $\leftarrow$ 点      否则            外点集 $\leftarrow$
    点
- en: Algorithm 1 Random Sample Consensus
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法 1 随机样本一致性
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Voting Scheme. RANSAC is kind of like voting process. A subset of data claims
    its idea and then the rest of the data votes for the idea. In every independent
    process, there are two kinds of data. One agrees with the idea while the other
    is against it. And RANSAC is going to select a process where the number of supporters
    is max. Thus,RANSAC is biased. If the model is going to be robust to outliers,
    then there must be enough good features which vote for correct model and outliers
    can’t vote consistently.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 投票方案。RANSAC 有点像投票过程。数据的一个子集提出其想法，然后其余的数据对这个想法进行投票。在每个独立的过程中，有两种数据。一种同意这个想法，而另一种反对。RANSAC
    将选择支持者最多的过程。因此，RANSAC 是有偏的。如果模型要对外点具有鲁棒性，则必须有足够多的好特征来投票支持正确的模型，而外点不能一致地投票。
- en: •
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Disadvantages. When there’re few outliers in the data, RANSAC can’t make sense
    in that the difference between every process is little. Only when the data is
    heavily contaminated, RANSAC can play its part. Besides, the threshold must be
    set by hand, which requires users to decide specific threshold on different data.
    Last but not least, the cost of computation is high because it is an iterative
    method and the number of times required in the model is unknown.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺点。当数据中离群值较少时，RANSAC 不能发挥作用，因为每个过程之间的差异很小。只有当数据受到严重污染时，RANSAC 才能发挥作用。此外，阈值必须手动设置，这要求用户在不同数据上决定具体阈值。最后但同样重要的是，计算成本较高，因为它是一个迭代方法，模型中所需的次数是未知的。
- en: 4.9 Theil-Sen Regression
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9 Theil-Sen 回归
- en: •
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Median Method [[18](#bib.bib18)]. There’re many data pairs used to calculate
    coefficient. $\theta=y_{a}-y_{b}/x_{a}-x_{b}$ And $\theta$ is the median of all
    $\theta s$. $b=y-\theta x$ b is also the median of $bs$. Hence, it’s a nonparametric
    technique. However, complete computation leads to low speed.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中位数方法 [[18](#bib.bib18)]。有许多数据对用于计算系数。$\theta=y_{a}-y_{b}/x_{a}-x_{b}$ 而 $\theta$
    是所有 $\theta$ 的中位数。$b=y-\theta x$，b 也是 $bs$ 的中位数。因此，它是一种非参数技术。然而，完全计算导致速度较低。
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Breaking Point. In particular, Theil-Sen only can tolerate 29.3% of data is
    outliers. And when the model is applied in high-dimensional regression, the rate
    is going to decrease.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突破点。特别地，Theil-Sen 只能容忍 29.3% 的数据为离群值。当模型应用于高维回归时，这一比例会减少。
- en: 4.10 Huber Regression
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10 Huber 回归
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Huber Loss [[19](#bib.bib19)]. It transforms its loss function when faced with
    different values. When values are large, which is of high possibility of being
    outliers, Huber turns their loss function into Linear Loss in order to minimize
    their influence on the model. $\delta$ serves as a threshold, deciding how large
    data is to need a linear loss. And it is fastest in three robust regression.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Huber 损失 [[19](#bib.bib19)]。它在面对不同值时会调整其损失函数。当值较大时，即有很高的可能性是离群值，Huber 会将其损失函数转变为线性损失，以最小化其对模型的影响。$\delta$
    作为一个阈值，决定数据的大小是否需要线性损失。而且，它在三种鲁棒回归中速度最快。
- en: '|  | <math   alttext="\displaystyle\centering HuberLoss=\begin{cases}\frac{1}{2}a^{2}&amp;,if&#124;a&#124;\leq\delta\\
    \delta(&#124;a&#124;-\frac{1}{2}\delta)&amp;otherwise\\'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\displaystyle\centering HuberLoss=\begin{cases}\frac{1}{2}a^{2}&amp;,if&#124;a&#124;\leq\delta\\
    \delta(&#124;a&#124;-\frac{1}{2}\delta)&amp;otherwise\\'
- en: \end{cases}\@add@centering" display="inline"><semantics ><mrow ><mrow ><mi >H</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >u</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >r</mi><mo lspace="0em" rspace="0em" >​</mo><mi >L</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >s</mi></mrow><mo >=</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd columnalign="left" ><mrow ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em" >​</mo><msup ><mi
    >a</mi><mn >2</mn></msup></mrow></mtd><mtd columnalign="left" ><mrow ><mo >,</mo><mi
    >i</mi><mi >f</mi><mo fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >a</mi><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em" >≤</mo><mi
    >δ</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mi >δ</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mi >a</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    >−</mo><mrow ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><mi >δ</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    columnalign="left" ><mrow ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"
    >\displaystyle\centering HuberLoss=\begin{cases}\frac{1}{2}a^{2}&,if&#124;a&#124;\leq\delta\\
    \delta(&#124;a&#124;-\frac{1}{2}\delta)&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (33) |
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: \end{cases}\@add@centering" display="inline"><semantics ><mrow ><mrow ><mi >H</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >u</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >r</mi><mo lspace="0em" rspace="0em" >​</mo><mi >L</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >s</mi></mrow><mo >=</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd columnalign="left" ><mrow ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em" >​</mo><msup ><mi
    >a</mi><mn >2</mn></msup></mrow></mtd><mtd columnalign="left" ><mrow ><mo >,</mo><mi
    >i</mi><mi >f</mi><mo fence="false" rspace="0.167em" stretchy="false" >&#124;</mo><mi
    >a</mi><mo fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em" >≤</mo><mi
    >δ</mi></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow ><mi >δ</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mi >a</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    >−</mo><mrow ><mfrac ><mn >1</mn><mn >2</mn></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><mi >δ</mi></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    columnalign="left" ><mrow ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >w</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"
    >\displaystyle\centering HuberLoss=\begin{cases}\frac{1}{2}a^{2}&,if&#124;a&#124;\leq\delta\\
    \delta(&#124;a&#124;-\frac{1}{2}\delta)&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (33) |
- en: 4.11 Multivariate Adaptive Regression Splines
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.11 多元自适应回归样条
- en: •
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Partitioning [[20](#bib.bib20)]. MARS begins with partitioning data and then
    runs linear regression on each different partition. And it makes no assumptions
    about the relationship between the labels and samples. MARS originally has a large
    collection of basis functions. Each meeting point of two linear models is called
    a knot. And each knot has a pair of basis functions. And these functions are used
    to describe the relationship between $x$ and $y$. The first basis function is
    $max(0,x-y)$. The second is $max(0,y-x)$.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 划分 [[20](#bib.bib20)]。MARS 以数据划分开始，然后在每个不同的划分上进行线性回归。它对标签和样本之间的关系没有假设。MARS 最初有大量的基函数。两个线性模型的交汇点称为节点。每个节点有一对基函数。这些函数用于描述
    $x$ 和 $y$ 之间的关系。第一个基函数是 $max(0,x-y)$。第二个基函数是 $max(0,y-x)$。
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Remove Basis Functions [[20](#bib.bib20)]. After MARS partitions data and builds
    models, it applies least-squares model to fit data. And each knot has two basis
    functions. The results of them can be viewed as input variables. Least-Squares
    model estimate the loss of each basis function’s output value. If a basis function
    has little influence on model fitting, then it is going to be removed.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 删除基函数 [[20](#bib.bib20)]。在 MARS 划分数据并建立模型后，它应用最小二乘模型来拟合数据。每个结点有两个基函数。这些基函数的结果可以视为输入变量。最小二乘模型估计每个基函数输出值的损失。如果一个基函数对模型拟合的影响较小，则将被移除。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advantages And Disadvantages. It can fit a large number of predictor variables.
    And it is an effective and fast algorithm. Also, it is robust to outliers. However,
    it begins with a large set of models and this easily leads to overfitting. And
    it is vulnerable to missing data problems.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优点和缺点。它可以拟合大量预测变量，并且是一个有效且快速的算法。此外，它对异常值具有鲁棒性。然而，它从大量模型开始，这很容易导致过拟合。同时，它对缺失数据问题非常敏感。
- en: 4.12 Polynomial Regression
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.12 多项式回归
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Background: When the relationship between $X$ and $y$ is nonlinear, OLS sucks.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景：当 $X$ 和 $y$ 之间的关系是非线性时，OLS 的效果较差。
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Polynomial Transformation [[21](#bib.bib21)]. Polynomial Regression replaces
    original $X$ with Polynomial in order to attain a more linear relationship than
    before or change features for some reason. Hence, it is not interpretable. Interestingly,
    it is somewhat like Talyor Extend. When your model breaks the assumption of linearity,
    then you can try all polynomial regression to find a best one, which is a good
    recipe.If feature’s dimension is 2,
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多项式变换 [[21](#bib.bib21)]。多项式回归用多项式替换原始 $X$，以获得比以前更线性的关系，或由于某些原因改变特征。因此，它不可解释。有趣的是，它有点像泰勒展开。当你的模型违反线性假设时，你可以尝试所有的多项式回归，以找到最佳的，这是一种有效的策略。如果特征的维度是
    2，
- en: •
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: order = 1$\Rightarrow$ $[1,X_{1},X_{2}]$
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: order = 1$\Rightarrow$ $[1,X_{1},X_{2}]$
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: order = 2$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2}]$
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: order = 2$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2}]$
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: order = 3$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2},X_{1}^{3},X_{1}^{2}X_{2},X_{1}X_{2}^{2},X_{2}^{3}]$
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: order = 3$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2},X_{1}^{3},X_{1}^{2}X_{2},X_{1}X_{2}^{2},X_{2}^{3}]$
- en: 4.13 Weighted Least Squares
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.13 加权最小二乘法
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Background. One of the OLS’s assumptions is constant error variance. In section
    3, I put forward log method. However, it is ineffective.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景。OLS 的一个假设是误差方差常数。在第 3 节中，我提出了对数方法。然而，它无效。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformed Weights [[22](#bib.bib22)]. When $w_{i}$ all equals 1, it should
    be the OLS. In OLS, the model gives every point the same attention. But it’s under
    homoskedasticity while we come across more heteroskedastic scenarios. The idea
    is that we gives more attention to those points of which error is small. Thus,
    the model gives those points bigger weights.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转换权重 [[22](#bib.bib22)]。当 $w_{i}$ 全部等于 1 时，应为 OLS。在 OLS 中，模型对每个点给予相同的关注。但在遇到更多异方差情形时，它处于同方差性之下。这个想法是我们对错误较小的点给予更多关注。因此，模型给予这些点更大的权重。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stable Intercept and Sensitive Coefficient [[23](#bib.bib23)]. Note I choose
    different values for the first 20 weights and others are always 1\. Different
    weights are equivalent for errors’ abnormal distribution. As shown in Table 4,
    the intercept is right regardless. However, the coefficient changes sharply. Hence,
    we can’t use it to draw inferences and test our hypotheses with regard to coefficient.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定的截距和敏感的系数 [[23](#bib.bib23)]。注意，我为前 20 个权重选择了不同的值，其余的始终为 1。不同的权重对错误的异常分布是等效的。如表
    4 所示，截距始终正确。然而，系数变化剧烈。因此，我们不能用它来推断和检验系数相关的假设。
- en: '|  | $WLS=\sum\limits_{i=1}^{n}w_{i}(y_{i}-\hat{y_{i}})^{2}$ |  | (34) |'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $WLS=\sum\limits_{i=1}^{n}w_{i}(y_{i}-\hat{y_{i}})^{2}$ |  | (34) |'
- en: '![Refer to caption](img/5508595d52e2080b6690dbac95d4ce2b.png)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/5508595d52e2080b6690dbac95d4ce2b.png)'
- en: (a) w = 1
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) w = 1
- en: '![Refer to caption](img/2765653ff4d2f2116bcb0ec31af22443.png)'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/2765653ff4d2f2116bcb0ec31af22443.png)'
- en: (b) w = 10
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) w = 10
- en: '![Refer to caption](img/fb219d65c16b16c9f42f2f01d25b2697.png)'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/fb219d65c16b16c9f42f2f01d25b2697.png)'
- en: (c) w = 20
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) w = 20
- en: 'Figure 9: The middle line shows OLS and MLS fits the data.And others show the
    range of predicted values of different algorithms'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9：中间线显示 OLS 和 MLS 拟合数据。而其他线显示不同算法预测值的范围
- en: 'TABLE IV: Slopes And Intercepts'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 IV：斜率和截距
- en: '| $Weights$ | $Slopes$ | $Intercepts$ |'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| $Weights$ | $Slopes$ | $Intercepts$ |'
- en: '| --- | --- | --- |'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1.0 | 1.444 | 0.059 |'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1.0 | 1.444 | 0.059 |'
- en: '| 10.0 | 1.4887 | 0.059 |'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 10.0 | 1.4887 | 0.059 |'
- en: '| 20.0 | 1.5146 | 0.058 |'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 20.0 | 1.5146 | 0.058 |'
- en: '| 40.0 | 1.5407 | 0.058 |'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 40.0 | 1.5407 | 0.058 |'
- en: '| 80.0 | 1.5615 | 0.057 |'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 80.0 | 1.5615 | 0.057 |'
- en: '| 160.0 | 1.5755 | 0.057 |'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 160.0 | 1.5755 | 0.057 |'
- en: As shown in Figure 9,Figure(a) shows that OLS is the special case of WLS when
    weights = 1.And Figure(b) and Figure(c) show when weights change,the intercept
    almost stays the same.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如图 9 所示，图 (a) 显示当权重 = 1 时，OLS 是 WLS 的特殊情况。而图 (b) 和图 (c) 显示当权重变化时，截距几乎保持不变。
- en: 4.14 Generalized Least Squares
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.14 广义最小二乘法
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Background. OLS assumes that the error must be independent in the sense that
    one error can’t be correlated to others. But when error autocorrelation happens,
    OLS has no way but to fail.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景。OLS 假设误差必须是独立的，即一个误差不能与其他误差相关。但是当发生误差自相关时，OLS 除了失败别无选择。
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Better OLS [[24](#bib.bib24)]. GLS is similar to OLS on a linearly transformed
    version of the data. And GLS is unbaised, consistent and effective. WLS is the
    special case of GLS, which means GLS can also solve heteroskedasticity [[25](#bib.bib25)].
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更好的 OLS [[24](#bib.bib24)]。GLS 在数据的线性变换版本上类似于 OLS。GLS 是无偏的、一致的和有效的。WLS 是 GLS
    的特殊情况，这意味着 GLS 也可以解决异方差性 [[25](#bib.bib25)]。
- en: 4.15 Feasible Generalized Least Squares
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.15 可行广义最小二乘法
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementable GLS. While GLS sounds powerful, but it can’t be applied in specific
    regression tasks. FGLS is an implementable version of GLS. And FGLS needs some
    crucial assumptions to ensure a consistent estimator for errors covariance matrix.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可实现的 GLS。虽然 GLS 听起来很强大，但它不能应用于特定的回归任务。FGLS 是 GLS 的一个可实现版本。FGLS 需要一些关键假设以确保误差协方差矩阵的一致估计量。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inefficiency On Little Data. Whereas GLS is more powerful than OLS under heteroscedasticity
    or autocorrelation, this is not the case for FGLS. When the size of data is quite
    small, FGLS is ineffctive than OLS. Thus, some people prefer FGLS over OLS under
    small data. But when the size of data becomes large, FGLS is a better choice.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 小数据下的低效。虽然在异方差性或自相关性情况下，GLS 比 OLS 更强大，但 FGLS 并非如此。当数据量非常小时，FGLS 比 OLS 更低效。因此，有些人更倾向于在小数据下使用
    FGLS，而不是 OLS。但当数据量变大时，FGLS 是更好的选择。
- en: 4.16 Bayesian Regression
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.16 贝叶斯回归
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Background. As is known to all, OLS exactly makes an estimation of the mean
    of the values, which fails to provide a whole picture of the relationship between
    independent variables and dependent variables. And in some cases, we want to obtain
    a possible distribution of labels instead of a mean value.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景。众所周知，OLS 精确地估计了值的均值，这无法提供自变量和因变量之间关系的全面图景。在某些情况下，我们希望获得标签的可能分布，而不是均值。
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bayesian Theorem [[27](#bib.bib27)]. For instance, we’re going to employ a model
    to distinguish whether a email is normal or spam. So what our model faces is that
    it has to make predictions about the unknown email. Our data has 100 emails and
    10% of them is spam. Hence, the percentage of spam is 10%. But that’s absolutely
    not the whole story. In Bayesian, it’s called Prior Probability, which means the
    Basic Assumption of the distribution and that’s where Bayesian Begins. At the
    beginning of the algorithm, Bayesian is biased in return the model is easily affected
    by the original distribution. For example, if we only have all 10 normal emails,
    it’s impossible that we wouldn’t get any spam emails in future. In other words,
    if the size of our data is quite small, it’s not incentive for us to implement
    Bayesian. However, when training times keep increasing, we should get ideal results
    ultimately. In the equation below, P(B) is a Normalization term and P(A) is Prior
    Probability. $P(A|B)$ is called Posterior Probability(Conditional Probability).
    To conclude, when we have much data, Bayesian may be a good choice to try while
    it exactly performs like other algorithms.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯定理 [[27](#bib.bib27)]。例如，我们将使用一个模型来区分一封电子邮件是正常的还是垃圾邮件。因此，我们的模型面临的是必须对未知的电子邮件进行预测。我们的数据有
    100 封电子邮件，其中 10% 是垃圾邮件。因此，垃圾邮件的百分比是 10%。但这绝不是全部故事。在贝叶斯理论中，这被称为先验概率，它意味着分布的基本假设，这就是贝叶斯开始的地方。在算法开始时，贝叶斯是有偏的，模型容易受到原始分布的影响。例如，如果我们只有
    10 封正常的电子邮件，那么未来不出现垃圾邮件几乎是不可能的。换句话说，如果我们的数据量非常小，使用贝叶斯的激励就不大。然而，当训练次数不断增加时，我们最终应该能获得理想的结果。在下面的方程中，$P(B)$
    是归一化项，$P(A)$ 是先验概率。$P(A|B)$ 被称为后验概率（条件概率）。总之，当我们有大量数据时，贝叶斯可能是一个值得尝试的好选择，它的表现与其他算法相似。
- en: '|  | $\displaystyle P(A&#124;B)$ | $\displaystyle=\frac{P(B&#124;A)P(A)}{P(B)}$
    |  | (35) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P(A|B)$ | $\displaystyle=\frac{P(B|A)P(A)}{P(B)}$ |  |
    (35) |'
- en: •
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maximum Likelihood Estimation [[28](#bib.bib28)]. Generally speaking, our goal
    is to figure out the real data distribution, which is almost impossible. Therefore,
    we want a data distribution which is close to our data distribution from a problem
    domain. MLE(Maximum Likelihood Estimation) indicates that we want to maximize
    the probability that real data is sampled from the Hypothesis Distribution.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大似然估计 [[28](#bib.bib28)]。一般来说，我们的目标是弄清楚真实的数据分布，这几乎是不可能的。因此，我们希望从问题领域中找到一个接近我们数据分布的数据分布。MLE（最大似然估计）表示我们希望最大化真实数据从假设分布中抽样的概率。
- en: '|  | $\beta^{*}=\arg\max\limits_{\beta}P_{\beta}(D)$ |  | (36) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\beta^{*}=\arg\max\limits_{\beta}P_{\beta}(D)$ |  | (36) |'
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maximum Posterior Estimation [[29](#bib.bib29)]. Typically, we can use MAP(Maximum
    A Posterior Estimation) to replace MLE. It’s based on Bayesian Theorem. And MAP
    is fundamental to Bayesian Regression(equation 37). Rather than other standard
    algorithms, Bayesian Regression doesn’t produce a single value but a range of
    possible distribution. And in most cases, MLE and MAP are likely to get the same
    results. However, when the hypothesis of MAP is different from MLE, they fail
    to reach the same destination. When Prior Probability is uniformly distributed,
    they can make it. From another point of view, if we have some precise understanding
    of data, Bayesian Regression is a excellent choice in that it serves as Prior
    Probablity or we can weigh every different choice just like Weighted Least Errors.
    Interestingly, prior can be kind of Regularization or bias of the model, as such
    prior can be interpreted as L2 norm, which is also called Bayesian Ridge Regression.
    Equation(38) means given a model $m$, the probability of output y. And $\beta$(Coefficients)
    and $\sigma$(Standard Deviation) are arbitrary values.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大后验估计 [[29](#bib.bib29)]。通常，我们可以使用MAP（最大后验估计）来替代MLE。它基于贝叶斯定理。而MAP是贝叶斯回归（方程37）的基础。与其他标准算法不同，贝叶斯回归不会产生一个单一的值，而是可能的分布范围。在大多数情况下，MLE和MAP可能会得到相同的结果。然而，当MAP的假设与MLE不同，它们无法达到相同的结果。当先验概率均匀分布时，它们可以得到相同的结果。从另一个角度来看，如果我们对数据有一些精确的理解，贝叶斯回归是一个很好的选择，因为它作为先验概率，或者我们可以像加权最小误差一样对每个不同的选择进行加权。有趣的是，先验可以视为一种正则化或模型的偏差，因此这种先验可以解释为L2范数，也称为贝叶斯岭回归。方程（38）表示给定一个模型
    $m$，输出 $y$ 的概率。而 $\beta$（系数）和 $\sigma$（标准差）是任意值。
- en: '|  | $\displaystyle P(\beta&#124;D)$ | $\displaystyle=\frac{P(D&#124;\beta)P(\beta)}{P(D)}$
    |  | (37) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P(\beta\mid D)$ | $\displaystyle=\frac{P(D\mid \beta)P(\beta)}{P(D)}$
    |  | (37) |'
- en: '|  | $\displaystyle P(y&#124;m)$ | $\displaystyle=\frac{P(\beta,\sigma&#124;m)P(y&#124;X,\beta,\sigma,m)}{P(\beta,\sigma&#124;y,X,m)}$
    |  | (38) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P(y\mid m)$ | $\displaystyle=\frac{P(\beta,\sigma\mid m)P(y\mid
    X,\beta,\sigma,m)}{P(\beta,\sigma\mid y,X,m)}$ |  | (38) |'
- en: '|  | $\displaystyle y$ | $\displaystyle\sim(\beta^{T}X,\sigma^{2})$ |  | (39)
    |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y$ | $\displaystyle\sim(\beta^{T}X,\sigma^{2})$ |  | (39)
    |'
- en: 4.17 Quantile Regression
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.17 分位数回归
- en: •
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformed Loss Function [[30](#bib.bib30)]. QR has a parameter q which decides
    the proportion to split the data. One is q % of the data and the other is (1-q)%
    of the data. For instance, if q = 0.5, then data is split in two. We minimize
    the squared loss in OLS while we now minimize the absolute loss in QR.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转换损失函数 [[30](#bib.bib30)]。QR有一个参数 $q$，决定了数据的分割比例。一个是 $q\%$ 的数据，另一个是 $(1-q)\%$
    的数据。例如，如果 $q = 0.5$，则数据分成两部分。我们在OLS中最小化平方损失，而现在我们在QR中最小化绝对损失。
- en: '|  | $J(\theta)=\sum_{i=1}^{n}q&#124;y_{i}-\hat{y_{i}}&#124;+\sum_{i=1}^{n}(1-q)&#124;y_{i}-\hat{y_{i}}&#124;$
    |  | (40) |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\sum_{i=1}^{n}\left|q-y_{i}-\hat{y_{i}}\right|+\sum_{i=1}^{n}(1-q)\left|y_{i}-\hat{y_{i}}\right|$
    |  | (40) |'
- en: •
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advantages. It goes without saying that QR can provide a more complete view
    of the relationship than OLS. What’s more, it is also robust to outliers and situations
    where the variance of errors is not a constant.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优势。不言而喻，QR能够提供比OLS更全面的关系视角。而且，它对异常值以及误差方差不是常数的情况也很鲁棒。
- en: 4.18 Ordinal Regression
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.18 序数回归
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Background. In some cases, the values for labels are ranking numbers. For instance,
    0-5 can represent his ability of communicating with others in social science.
    And OLS can’t make accurate prediction about them.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景。在某些情况下，标签的值是排名数字。例如，0-5可以表示他在社会科学中的沟通能力。而OLS无法对这些进行准确预测。
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ranking Learning [[31](#bib.bib31)]. In OR, the model has a set of thresholds
    $\theta_{1},\theta_{2},\dots,\theta_{n}$, which is used to split predictions into
    independent intervals and every interval corresponds to a $y$. The model can be
    represented by sigmoid function of which output values stand for possibility.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排名学习 [[31](#bib.bib31)]。在运筹学中，模型有一组阈值$\theta_{1},\theta_{2},\dots,\theta_{n}$，用于将预测分成独立的区间，每个区间对应一个$y$。该模型可以通过sigmoid函数表示，其输出值代表可能性。
- en: '|  | $P(y\leq i&#124;x)=\sigma(\theta_{i}-\hat{y_{i}})$ |  | (41) |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y\leq i\mid x)=\sigma(\theta_{i}-\hat{y_{i}})$ |  | (41) |'
- en: 5 Extra Models
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 额外模型
- en: 5.1 Generalized Linear Models
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 广义线性模型
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generalized Functions [[32](#bib.bib32)]. Just as the name implies, it is generalization
    of different functions. And it consists of two significant parts. The first part
    is the probability distribution of $y$ such as normal distribution(OLS). The second
    is linear predictor, which decides how the coefficients combine with independent
    variables. And GLM includes several regression models such as Binomial Regression,
    Bernoulli Regression, Poisson Regression and so on. Their application is not so
    wide and they just swift the two parts compared to OLS. Hence, it’s left out in
    this paper.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广义函数 [[32](#bib.bib32)]。顾名思义，它是不同函数的泛化。它包括两个重要部分。第一部分是$y$的概率分布，如正态分布（OLS）。第二部分是线性预测器，它决定了系数如何与自变量结合。GLM包括多个回归模型，如二项回归、伯努利回归、泊松回归等。它们的应用不太广泛，与OLS相比，仅仅是在这两个部分上进行了调整。因此，本文中未予讨论。
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advantages And Disadvantages. It can absolutely deal with situations where $y$
    doesn’t follow normal distribution. However, it needs large data sets and it is
    sensitive to outliers.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优势与劣势。它可以有效处理$y$不符合正态分布的情况。然而，它需要大数据集，并且对离群值敏感。
- en: 5.2 Step-Wise Regression
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 步骤回归
- en: •
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Forward Selection [[33](#bib.bib33)]. This method begins with no variables.
    And it involves testing the addition of the variable in an iterative method which
    is of great use to the improvement of the accuracy. The model repeats until no
    improvement.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前向选择 [[33](#bib.bib33)]。该方法从无变量开始。它涉及以迭代方法测试变量的添加，这对提高准确性非常有用。模型重复进行，直到没有改进。
- en: •
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Backward Elimination. This method starts with many candidate variables. It involves
    testing the loss of the model with the deletion of variables. If the loss is small,
    then the variable is going to be deleted. The model repeats until no variable
    can be deleted.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后向消除。该方法从许多候选变量开始。它涉及测试通过删除变量后的模型损失。如果损失较小，则该变量将被删除。模型重复进行，直到没有变量可以删除。
- en: •
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Bidirectional Elimination. This is an combination of the above two methods.
    Whether adding or deleting a variable is decided on every step. To summarize,
    Step-Wise Regression contains a big space of possible models, which can lead to
    overfitting.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 双向消除。这是上述两种方法的结合。是否添加或删除变量在每一步都决定。总之，步骤回归包含了大量可能的模型，这可能导致过拟合。
- en: •
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reasons For Stopping [[34](#bib.bib34)]. First, the tests such as F-tests and
    t-tests are biased, thus it may not be accurate. Second, widespread incorrect
    usage and availability of alternative models such as ensemble learning have led
    to calls to stop the use of this algorithm.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 停止的原因 [[34](#bib.bib34)]。首先，F检验和t检验等测试存在偏差，因此可能不准确。其次，广泛的错误使用和替代模型（如集成学习）的出现导致了停止使用该算法的呼声。
- en: 6 Relationship With Deep Learning
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 与深度学习的关系
- en: 6.1 General Regression Neural Network
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 广义回归神经网络
- en: '![Refer to caption](img/0d7a3559426a04be445926ea40552b03.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0d7a3559426a04be445926ea40552b03.png)'
- en: 'Figure 10: GRNN Structure. Source: https://www.mdpi.com/1424-8220/20/9/2625.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：GRNN 结构。来源：https://www.mdpi.com/1424-8220/20/9/2625。
- en: •
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Network Structure [[35](#bib.bib35)]. GRNN includes input, pattern, summation
    and output layers. The input and output layers are independent vector and dependent
    vector. The pattern layer can be seen as a vector full of coefficients. For instance,
    if we want to apply $y=\theta x$. Then one pattern neuron stands for $\theta_{i}x_{i}$.
    And output layer can be formulated as the equation below. And this model can maintain
    its accuracy with small data and it’s robust to outliers. However, the structure
    of the network is complicated so that it is computationally expensive.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络结构 [[35](#bib.bib35)]。GRNN包括输入层、模式层、求和层和输出层。输入层和输出层是独立向量和依赖向量。模式层可以看作是一个充满系数的向量。例如，如果我们想应用
    $y=\theta x$，那么一个模式神经元代表 $\theta_{i}x_{i}$。输出层可以通过下面的方程来表示。这个模型可以在小数据量下保持其准确性，并且对异常值具有鲁棒性。然而，网络的结构复杂，因此计算成本较高。
- en: '|  | $\displaystyle Y(x)=\frac{\sum_{i=1}^{n}y_{i}K(x,x_{i})}{\sum_{i=1}^{n}K(x,x_{i})}$
    |  | (42) |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y(x)=\frac{\sum_{i=1}^{n}y_{i}K(x,x_{i})}{\sum_{i=1}^{n}K(x,x_{i})}$
    |  | (42) |'
- en: '|  | $\displaystyle K(x,x_{i})=e^{-d_{i}/2\sigma^{2}}$ |  | (43) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle K(x,x_{i})=e^{-d_{i}/2\sigma^{2}}$ |  | (43) |'
- en: '|  | $\displaystyle d_{i}=(x-x_{i})^{T}(x-x_{i})$ |  | (44) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d_{i}=(x-x_{i})^{T}(x-x_{i})$ |  | (44) |'
- en: •
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Widespread Application [[36](#bib.bib36)]. Many regression models like Poisson
    Regression and Ordinal Regression have succeeded in using GRNN. And we can draw
    a safe conclusion that a complicated network structure can represent any kind
    of regression. But only few of them are proved successful. Neural Network is powerful
    and classic regression algorithms are well-structured. Maybe regression can be
    applied in neural network without missing its original function. Humans have made
    fundamental progress in Regression. If we can combine Regression with neural network
    perfectly, then it’s another picture.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛应用 [[36](#bib.bib36)]。许多回归模型如泊松回归和序数回归都成功地使用了GRNN。我们可以得出一个安全的结论：复杂的网络结构可以表示任何类型的回归。但只有少数模型被证明是成功的。神经网络是强大的，而经典的回归算法结构良好。也许回归可以在神经网络中应用而不失去其原有功能。人类在回归方面已经取得了基础性的进展。如果我们能够将回归与神经网络完美结合，那么这将是另一幅画面。
- en: 7 Conclusions
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, I set out necessary assumptions with OLS and little tricks to
    fix the problems when assumptions are violated. Amazingly, it seems that OLS is
    the beginning of almost evey regression model. And a large number of Regression
    models are designed to be a better OLS. They can play their part in situations
    where OLS fails to work. I hold the belief that not evey algorithm needs to be
    introduced in details. Hence, the widespread algorithms are given enough attention
    and others are quickly illustrated. Finally, I give a quick overview of GRNN.
    From this paper, I can draw three conclusions.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我提出了OLS的必要假设，并提供了一些小技巧来修正当假设被违反时的问题。令人惊讶的是，OLS似乎几乎是所有回归模型的起点。大量的回归模型设计成更好的OLS。它们可以在OLS无法工作的情况下发挥作用。我认为并非每种算法都需要详细介绍。因此，广泛使用的算法得到了足够的关注，而其他算法则被迅速说明。最后，我对GRNN进行了快速概述。从本文中，我得出三个结论。
- en: •
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Know Your Model. Note that regression algorithms aren’t plug-and-play. You must
    know evey model’s range of application and are able to deal with situations where
    the model’s assumptions are unsatisfied.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 了解你的模型。请注意，回归算法并不是即插即用的。你必须了解每个模型的应用范围，并能够处理模型假设不满足的情况。
- en: •
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Regression In the Future. Regression is older compared to Deep Learning and
    great ideas behind every classic algorithm is never out of date. And people always
    want to predict unknown values and regression task is really fascinating. Deep
    Learning is quite powerful. If regression can learn from Deep Learning and keeps
    its excellent part, I do believe regression can be more powerful in the near future.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来的回归。与深度学习相比，回归显得较为古老，但每个经典算法背后的伟大思想从未过时。人们总是希望预测未知值，而回归任务确实令人着迷。深度学习非常强大。如果回归能够从深度学习中学习并保留其优点，我相信回归在不久的将来会变得更加强大。
- en: •
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It seems that regression algorithms are out of date. However, as far as I am
    concerned, beautiful ideas behind every algorithm are shared. In other words,
    dipping into these old algorithms can enable us to gain insight and intuition
    about algorithms and put forward exciting algorithms which share the same ideas
    with regression algorithms and are just different implementations of awesome ideas
    to handle new problems.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看起来回归算法已经过时。然而，就我而言，每个算法背后的美妙思想是共享的。换句话说，深入这些旧算法可以让我们获得对算法的洞察和直觉，并提出与回归算法有相同思想的新算法，这些新算法只是处理新问题的不同实现而已。
- en: References
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A.M. Legendre. Nouvelles méthodes pour la détermination des orbites des
    comètes, Firmin Didot, Paris, 1805\. “Sur la Méthode des moindres quarrés” appears
    as an appendix.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A.M. Legendre。《确定彗星轨道的新方法》，Firmin Didot，巴黎，1805 年。 “最小二乘法”作为附录出现。'
- en: '[2] C.F. Gauss. Theoria combinationis observationum erroribus minimis obnoxiae'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] C.F. 高斯。最小误差观察组合理论'
- en: '[3] Arthur E. Hoerl & Robert W. Kennard (1970) Ridge Regression: Biased Estimation
    for Nonorthogonal Problems, Technometrics, 12:1, 55-67, DOI: 10.1080/00401706.1970.10488634'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Arthur E. Hoerl & Robert W. Kennard (1970) 岭回归：对非正交问题的偏倚估计，技术统计学，12:1,
    55-67, DOI: 10.1080/00401706.1970.10488634'
- en: '[4] Arthur E. Hoerl & Robert W. Kennard (1970) Ridge Regression: Applications
    to Nonorthogonal Problems, Technometrics, 12:1, 69-82, DOI: 10.1080/00401706.1970.10488635'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Arthur E. Hoerl & Robert W. Kennard (1970) 岭回归：对非正交问题的应用，技术统计学，12:1, 69-82,
    DOI: 10.1080/00401706.1970.10488635'
- en: '[5] Tibshirani, R. (1996), Regression Shrinkage and Selection Via the Lasso.
    Journal of the Royal Statistical Society: Series B (Methodological), 58: 267-288\.
    https://doi.org/10.1111/j.2517-6161.1996.tb02080.x'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tibshirani, R. (1996), 《Lasso 回归收缩与选择》。皇家统计学会：B 系列（方法论），58: 267-288。 https://doi.org/10.1111/j.2517-6161.1996.tb02080.x'
- en: '[6] Zou, H. and Hastie, T. (2005), Regularization and variable selection via
    the elastic net. Journal of the Royal Statistical Society: Series B (Statistical
    Methodology), 67: 301-320\. https://doi.org/10.1111/j.1467-9868.2005.00503.x'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Zou, H. 和 Hastie, T. (2005), 《通过弹性网的正则化与变量选择》。皇家统计学会：B 系列（统计方法论），67: 301-320。
    https://doi.org/10.1111/j.1467-9868.2005.00503.x'
- en: '[7] Hayashi, Fumio (2000). Econometics. Princeton University Press. p. 15.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Hayashi, Fumio (2000). 《计量经济学》。普林斯顿大学出版社。第 15 页。'
- en: '[8] Gubner, John A. (2006). Probability and Random Processes for Electrical
    and Computer Engineers. Cambridge University Press. ISBN 978-0-521-86470-1.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Gubner, John A. (2006). 《电气与计算机工程师的概率与随机过程》。剑桥大学出版社。ISBN 978-0-521-86470-1。'
- en: '[9] V. Vapnik, “The support vector method of function estimation, ” in J.A.K.
    Suykens and J. Vandewalle (Eds) Nonlinear Modeling: Advanced Black-Box Techniques,
    Kluwer Academic Publishers, Boston, pp. 55–85, 1998.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] V. Vapnik，“函数估计的支持向量方法，” 见 J.A.K. Suykens 和 J. Vandewalle (编)《非线性建模：先进的黑箱技术》，Kluwer
    学术出版社，波士顿，第 55–85 页，1998 年。'
- en: '[10] Drucker, Harris; Burges, Christ. C.; Kaufman, Linda; Smola, Alexander
    J.; and Vapnik, Vladimir N. (1997); ”Support Vector Regression Machines”, in Advances
    in Neural Information Processing Systems 9, NIPS 1996, 155–161, MIT Press.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Drucker, Harris; Burges, Christ. C.; Kaufman, Linda; Smola, Alexander
    J.; 和 Vapnik, Vladimir N. (1997); “支持向量回归机器”，见《神经信息处理系统进展 9》，NIPS 1996，155–161，MIT
    出版社。'
- en: '[11] Smola, A.J., Schölkopf, B. A tutorial on support vector regression. Statistics
    and Computing 14, 199–222 (2004). https://doi.org/10.1023/B:STCO.0000035301.49549.88'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Smola, A.J., Schölkopf, B. 支持向量回归教程。统计与计算 14, 199–222 (2004). https://doi.org/10.1023/B:STCO.0000035301.49549.88'
- en: '[12] S. R. Safavian and D. Landgrebe, ”A survey of decision tree classifier
    methodology,” in IEEE Transactions on Systems, Man, and Cybernetics, vol. 21,
    no. 3, pp. 660-674, May-June 1991, doi: 10.1109/21.97458.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. R. Safavian 和 D. Landgrebe，“决策树分类器方法概述，” 见《IEEE 系统、人工智能与控制论学报》，第 21
    卷，第 3 期，第 660-674 页，1991 年 5-6 月，doi: 10.1109/21.97458。'
- en: '[13] Loh, W.‐Y. (2011), Classification and regression trees. WIREs Data Mining
    Knowl Discov, 1: 14-23\. https://doi.org/10.1002/widm.8'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Loh, W.‐Y. (2011), 分类与回归树。WIREs 数据挖掘与知识发现，1: 14-23。 https://doi.org/10.1002/widm.8'
- en: '[14] Liaw A, Wiener M. Classification and regression by randomForest[J]. R
    news, 2002, 2(3): 18-22.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Liaw A, Wiener M. 《通过随机森林进行分类和回归》[J]。《R 新闻》，2002，2(3): 18-22。'
- en: '[15] Elith J, Leathwick J R, Hastie T. A working guide to boosted regression
    trees[J]. Journal of Animal Ecology, 2008, 77(4): 802-813.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elith J, Leathwick J R, Hastie T. 《提升回归树工作指南》[J]。《动物生态学杂志》，2008，77(4):
    802-813。'
- en: '[16] Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004).
    ”Least Angle Regression” (PDF). Annals of Statistics. 32 (2): pp. 407–499\. arXiv:math/0406456\.
    doi:10.1214/009053604000000067\. MR 2060166.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004).
    “最小角回归”（PDF）。*统计年鉴*。32 (2): 第 407–499 页。arXiv:math/0406456。doi:10.1214/009053604000000067。MR
    2060166。'
- en: '[17] Fischler, M. & Bolles, R. ( 1981). Random Sample Consensus: A Paradigm
    for Model Fitting with Applications to Image Analysis and Automated Cartography.
    Communications of the ACM, 24, 381-395.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Fischler, M. & Bolles, R. (1981). *随机样本一致性：一种模型拟合范式及其在图像分析和自动制图中的应用*。*ACM通讯*，24,
    381-395。'
- en: '[18] Theil, H. (1950), ”A rank-invariant method of linear and polynomial regression
    analysis. I, II, III”, Nederl. Akad. Wetensch., Proc., 53: 386–392, 521–525, 1397–1412,
    MR 0036489'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Theil, H. (1950), “一种对秩不变的线性和多项式回归分析方法 I, II, III”，*荷兰科学院通讯*，53: 386–392,
    521–525, 1397–1412, MR 0036489。'
- en: '[19] Huber, Peter J. (1964). ”Robust Estimation of a Location Parameter”. Annals
    of Statistics. 53 (1): 73–101\. doi:10.1214/aoms/1177703732\. JSTOR 2238020.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Huber, Peter J. (1964). “位置参数的鲁棒估计”。*统计年鉴*。53 (1): 73–101。doi:10.1214/aoms/1177703732。JSTOR
    2238020。'
- en: '[20] Friedman J H. Multivariate adaptive regression splines[J]. The annals
    of statistics, 1991: 1-67.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Friedman J H. *多变量自适应回归样条*[J]. *统计年鉴*，1991: 1-67。'
- en: '[21] Stigler, Stephen M. (November 1974). ”Gergonne’s 1815 paper on the design
    and analysis of polynomial regression experiments”. Historia Mathematica. 1 (4):
    431–439\. doi:10.1016/0315-0860(74)90033-0.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Stigler, Stephen M. (1974年11月). “Gergonne 的 1815 年论文关于多项式回归实验的设计和分析”。*数学史*。1
    (4): 431–439。doi:10.1016/0315-0860(74)90033-0。'
- en: '[22] Ruppert D, Wand M P. Multivariate locally weighted least squares regression[J].
    The annals of statistics, 1994: 1346-1370.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ruppert D, Wand M P. *多变量局部加权最小二乘回归*[J]. *统计年鉴*，1994: 1346-1370。'
- en: '[23] Suykens J A K, De Brabanter J, Lukas L, et al. Weighted least squares
    support vector machines: robustness and sparse approximation[J]. Neurocomputing,
    2002, 48(1-4): 85-105.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Suykens J A K, De Brabanter J, Lukas L, 等. *加权最小二乘支持向量机：鲁棒性和稀疏逼近*[J].
    *神经计算*，2002, 48(1-4): 85-105。'
- en: '[24] Amemiya, Takeshi (1985). ”Generalized Least Squares Theory”. Advanced
    Econometrics. Harvard University Press. ISBN 0-674-00560-0.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Amemiya, Takeshi (1985). “广义最小二乘理论”。*高级计量经济学*。哈佛大学出版社。ISBN 0-674-00560-0。'
- en: '[25] Kmenta, Jan (1986). ”Generalized Linear Regression Model and Its Applications”.
    Elements of Econometrics (Second ed.). New York: Macmillan. pp. 607–650\. ISBN
    0-472-10886-7.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Kmenta, Jan (1986). “广义线性回归模型及其应用”。*计量经济学要素*（第二版）。纽约：Macmillan。第 607–650
    页。ISBN 0-472-10886-7。'
- en: '[26] Kariya T, Kurata H. Generalized least squares[M]. John Wiley & Sons, 2004.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Kariya T, Kurata H. *广义最小二乘*[M]. John Wiley & Sons, 2004。'
- en: '[27] Bernardo J M, Smith A F M. Bayesian theory[M]. John Wiley & Sons, 2009.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Bernardo J M, Smith A F M. *贝叶斯理论*[M]. John Wiley & Sons, 2009。'
- en: '[28] Myung I J. Tutorial on maximum likelihood estimation[J]. Journal of mathematical
    Psychology, 2003, 47(1): 90-100.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Myung I J. *最大似然估计教程*[J]. *数学心理学期刊*，2003, 47(1): 90-100。'
- en: '[29] Gauvain J L, Lee C H. Maximum a posteriori estimation for multivariate
    Gaussian mixture observations of Markov chains[J]. IEEE transactions on speech
    and audio processing, 1994, 2(2): 291-298.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Gauvain J L, Lee C H. *最大后验估计用于马尔可夫链的多变量高斯混合观测*[J]. IEEE *语音与音频处理汇刊*，1994,
    2(2): 291-298。'
- en: '[30] Koenker R, Hallock K F. Quantile regression[J]. Journal of economic perspectives,
    2001, 15(4): 143-156.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Koenker R, Hallock K F. *分位数回归*[J]. *经济学视角期刊*，2001, 15(4): 143-156。'
- en: '[31] Harrell Jr F E. Regression modeling strategies: with applications to linear
    models, logistic and ordinal regression, and survival analysis[M]. Springer, 2015.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Harrell Jr F E. *回归建模策略：应用于线性模型、逻辑回归、序数回归和生存分析*[M]. Springer, 2015。'
- en: '[32] Faraway J J. Extending the linear model with R: generalized linear, mixed
    effects and nonparametric regression models[M]. CRC press, 2016.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Faraway J J. *使用 R 扩展线性模型：广义线性、混合效应和非参数回归模型*[M]. CRC press, 2016。'
- en: '[33] Efroymson, MA (1960) ”Multiple regression analysis.” In Ralston, A. and
    Wilf, HS, editors, Mathematical Methods for Digital Computers. Wiley.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Efroymson, MA (1960) “多重回归分析。” 编者：Ralston, A. 和 Wilf, HS，*数学方法用于数字计算机*。Wiley。'
- en: '[34] Flom, P. L. and Cassell, D. L. (2007) ”Stopping stepwise: Why stepwise
    and similar selection methods are bad, and what you should use,” NESUG 2007.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Flom, P. L. 和 Cassell, D. L. (2007) “停止逐步法：为什么逐步法和类似选择方法不好，以及你应该使用什么，”
    NESUG 2007。'
- en: '[35] Specht, D. F. (2002-08-06). ”A general regression neural network”. IEEE
    Transactions on Neural Networks. 2 (6): 568–576\. doi:10.1109/72.97934\. PMID
    18282872.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Specht, D. F. (2002-08-06). “一般回归神经网络”。IEEE 神经网络汇刊。2 (6): 568–576\. doi:10.1109/72.97934\.
    PMID 18282872。'
- en: '[36] Dreiseitl S, Ohno-Machado L. Logistic regression and artificial neural
    network classification models: a methodology review[J]. Journal of biomedical
    informatics, 2002, 35(5-6): 352-359.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Dreiseitl S, Ohno-Machado L. 逻辑回归与人工神经网络分类模型：方法论回顾[J]. 《生物医学信息学杂志》，2002,
    35(5-6): 352-359。'
- en: '[37] Hutcheson G D. Ordinary least-squares regression[J]. L. Moutinho and GD
    Hutcheson, The SAGE dictionary of quantitative management research, 2011: 224-228.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Hutcheson G D. 普通最小二乘回归[J]. L. Moutinho 和 GD Hutcheson, 《SAGE 定量管理研究词典》，2011:
    224-228。'
- en: '[38] Dismuke C, Lindrooth R. Ordinary least squares[J]. Methods and Designs
    for Outcomes Research, 2006, 93: 93-104.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Dismuke C, Lindrooth R. 普通最小二乘[J]. 《结果研究方法与设计》，2006, 93: 93-104。'
- en: '[39] Kiers H A L. Weighted least squares fitting using ordinary least squares
    algorithms[J]. Psychometrika, 1997, 62(2): 251-266.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Kiers H A L. 使用普通最小二乘算法的加权最小二乘拟合[J]. 《心理计量学》，1997, 62(2): 251-266。'
- en: '[40] Marquardt D W, Snee R D. Ridge regression in practice[J]. The American
    Statistician, 1975, 29(1): 3-20.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Marquardt D W, Snee R D. 实践中的岭回归[J]. 《美国统计学家》，1975, 29(1): 3-20。'
- en: '[41] Hoerl A E, Kannard R W, Baldwin K F. Ridge regression: some simulations[J].
    Communications in Statistics-Theory and Methods, 1975, 4(2): 105-123.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Hoerl A E, Kannard R W, Baldwin K F. 岭回归：一些模拟[J]. 《统计通讯-理论与方法》，1975, 4(2):
    105-123。'
- en: '[42] Le Cessie S, Van Houwelingen J C. Ridge estimators in logistic regression[J].
    Journal of the Royal Statistical Society: Series C (Applied Statistics), 1992,
    41(1): 191-201.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Le Cessie S, Van Houwelingen J C. 逻辑回归中的岭回归估计量[J]. 《皇家统计学会杂志：C系列（应用统计学）》，1992,
    41(1): 191-201。'
- en: '[43] Osborne M R, Presnell B, Turlach B A. On the lasso and its dual[J]. Journal
    of Computational and Graphical statistics, 2000, 9(2): 319-337.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Osborne M R, Presnell B, Turlach B A. 关于套索及其对偶[J]. 《计算与图形统计学杂志》，2000,
    9(2): 319-337。'
- en: '[44] Zou H. The adaptive lasso and its oracle properties[J]. Journal of the
    American statistical association, 2006, 101(476): 1418-1429.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Zou H. 自适应套索及其Oracle性质[J]. 《美国统计协会杂志》，2006, 101(476): 1418-1429。'
- en: '[45] Park T, Casella G. The bayesian lasso[J]. Journal of the American Statistical
    Association, 2008, 103(482): 681-686.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Park T, Casella G. 贝叶斯套索[J]. 《美国统计协会杂志》，2008, 103(482): 681-686。'
- en: '[46] Zhao P, Yu B. On model selection consistency of Lasso[J]. The Journal
    of Machine Learning Research, 2006, 7: 2541-2563.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Zhao P, Yu B. 关于套索的模型选择一致性[J]. 《机器学习研究杂志》，2006, 7: 2541-2563。'
- en: '[47] Meinshausen N. Relaxed lasso[J]. Computational Statistics & Data Analysis,
    2007, 52(1): 374-393.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Meinshausen N. 松弛套索[J]. 《计算统计与数据分析》，2007, 52(1): 374-393。'
- en: '[48] Zou H, Hastie T. Regression shrinkage and selection via the elastic net,
    with applications to microarrays[J]. JR Stat Soc Ser B, 2003, 67: 301-20.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Zou H, Hastie T. 通过弹性网进行回归收缩与选择及其在微阵列中的应用[J]. 《皇家统计学会B系列杂志》，2003, 67:
    301-320。'
- en: '[49] Ogutu J O, Schulz-Streeck T, Piepho H P. Genomic selection using regularized
    linear regression models: ridge regression, lasso, elastic net and their extensions[C]//BMC
    proceedings. BioMed Central, 2012, 6(2): 1-6.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ogutu J O, Schulz-Streeck T, Piepho H P. 使用正则化线性回归模型的基因组选择：岭回归、套索、弹性网及其扩展[C]//BMC
    会议记录。BioMed Central, 2012, 6(2): 1-6。'
- en: '[50] Ceperic E, Ceperic V, Baric A. A strategy for short-term load forecasting
    by support vector regression machines[J]. IEEE Transactions on Power Systems,
    2013, 28(4): 4356-4364.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Ceperic E, Ceperic V, Baric A. 通过支持向量回归机进行短期负荷预测的策略[J]. IEEE 电力系统汇刊，2013,
    28(4): 4356-4364。'
- en: '[51] Angiulli G, Cacciola M, Versaci M. Microwave devices and antennas modelling
    by support vector regression machines[J]. IEEE Transactions on Magnetics, 2007,
    43(4): 1589-1592.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Angiulli G, Cacciola M, Versaci M. 通过支持向量回归机对微波器件和天线进行建模[J]. IEEE 磁学汇刊，2007,
    43(4): 1589-1592。'
- en: '[52] Xu S, An X, Qiao X, et al. Multi-output least-squares support vector regression
    machines[J]. Pattern Recognition Letters, 2013, 34(9): 1078-1084.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Xu S, An X, Qiao X, 等. 多输出最小二乘支持向量回归机[J]. 《模式识别快报》，2013, 34(9): 1078-1084。'
- en: '[53] Segal M R. Machine learning benchmarks and random forest regression[J].
    2004.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Segal M R. 机器学习基准测试与随机森林回归[J]. 2004。'
- en: '[54] Svetnik V, Liaw A, Tong C, et al. Random forest: a classification and
    regression tool for compound classification and QSAR modeling[J]. Journal of chemical
    information and computer sciences, 2003, 43(6): 1947-1958.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Svetnik V, Liaw A, Tong C, 等. 随机森林：用于化合物分类和 QSAR 建模的分类和回归工具[J]. 化学信息与计算科学杂志,
    2003, 43(6): 1947-1958.'
- en: '[55] Cootes T F, Ionita M C, Lindner C, et al. Robust and accurate shape model
    fitting using random forest regression voting[C]//European Conference on Computer
    Vision. Springer, Berlin, Heidelberg, 2012: 278-291.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Cootes T F, Ionita M C, Lindner C, 等. 使用随机森林回归投票的稳健且准确的形状模型拟合[C]//欧洲计算机视觉大会.
    Springer, Berlin, Heidelberg, 2012: 278-291.'
- en: '[56] Jöreskog K G, Goldberger A S. Factor analysis by generalized least squares[J].
    Psychometrika, 1972, 37(3): 243-260.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Jöreskog K G, Goldberger A S. 通过广义最小二乘法进行因子分析[J]. 心理计量学, 1972, 37(3):
    243-260.'
- en: '[57] Orsini N, Bellocco R, Greenland S. Generalized least squares for trend
    estimation of summarized dose–response data[J]. The stata journal, 2006, 6(1):
    40-57.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Orsini N, Bellocco R, Greenland S. 用于总结剂量–反应数据趋势估计的广义最小二乘法[J]. Stata杂志,
    2006, 6(1): 40-57.'
- en: '[58] Browne M W. Generalized least squares estimators in the analysis of covariance
    structures[J]. South African statistical journal, 1974, 8(1): 1-24.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Browne M W. 协方差结构分析中的广义最小二乘估计量[J]. 南非统计杂志, 1974, 8(1): 1-24.'
- en: '[59] Hao L, Naiman D Q, Naiman D Q. Quantile regression[M]. Sage, 2007.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Hao L, Naiman D Q, Naiman D Q. 分位数回归[M]. Sage, 2007.'
- en: '[60] Yu K, Lu Z, Stander J. Quantile regression: applications and current research
    areas[J]. Journal of the Royal Statistical Society: Series D (The Statistician),
    2003, 52(3): 331-350.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yu K, Lu Z, Stander J. 分位数回归：应用与当前研究领域[J]. 皇家统计学会杂志：D 系列（统计学家）, 2003,
    52(3): 331-350.'
- en: '[61] Meinshausen N, Ridgeway G. Quantile regression forests[J]. Journal of
    Machine Learning Research, 2006, 7(6).'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Meinshausen N, Ridgeway G. 分位数回归森林[J]. 机器学习研究杂志, 2006, 7(6).'
- en: '[62] Bishop C M, Tipping M E. Bayesian regression and classification[J]. Nato
    Science Series sub Series III Computer And Systems Sciences, 2003, 190: 267-288.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Bishop C M, Tipping M E. 贝叶斯回归与分类[J]. 北约科学系列子系列 III 计算机与系统科学, 2003, 190:
    267-288.'
- en: '[63] Gelman A, Goodrich B, Gabry J, et al. R-squared for Bayesian regression
    models[J]. The American Statistician, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Gelman A, Goodrich B, Gabry J, 等. 贝叶斯回归模型的 R 平方[J]. 美国统计学家, 2019.'
- en: '[64] Koop G M. Bayesian econometrics[M]. John Wiley & Sons Inc., 2003.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Koop G M. 贝叶斯计量经济学[M]. John Wiley & Sons Inc., 2003.'
- en: '[65] Yu K, Moyeed R A. Bayesian quantile regression[J]. Statistics & Probability
    Letters, 2001, 54(4): 437-447.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Yu K, Moyeed R A. 贝叶斯分位数回归[J]. 统计与概率通讯, 2001, 54(4): 437-447.'
- en: '[66] Willett J B, Singer J D. Another cautionary note about R 2: Its use in
    weighted least-squares regression analysis[J]. The American Statistician, 1988,
    42(3): 236-238.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Willett J B, Singer J D. 关于 R 2 的另一条警告：其在加权最小二乘回归分析中的使用[J]. 美国统计学家, 1988,
    42(3): 236-238.'
- en: '[67] Chang P T, Lee E S. A generalized fuzzy weighted least-squares regression[J].
    Fuzzy Sets and Systems, 1996, 82(3): 289-298.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Chang P T, Lee E S. 广义模糊加权最小二乘回归[J]. 模糊集与系统, 1996, 82(3): 289-298.'
- en: '[68] Blatman G, Sudret B. Adaptive sparse polynomial chaos expansion based
    on least angle regression[J]. Journal of computational Physics, 2011, 230(6):
    2345-2367.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Blatman G, Sudret B. 基于最小角回归的自适应稀疏多项式混沌展开[J]. 计算物理学杂志, 2011, 230(6): 2345-2367.'
- en: '[69] Khan J A, Van Aelst S, Zamar R H. Robust linear model selection based
    on least angle regression[J]. Journal of the American Statistical Association,
    2007, 102(480): 1289-1299.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Khan J A, Van Aelst S, Zamar R H. 基于最小角回归的稳健线性模型选择[J]. 美国统计协会期刊, 2007,
    102(480): 1289-1299.'
- en: '[70] Hesterberg T, Choi N H, Meier L, et al. Least angle and l1 penalized regression:
    A review[J]. Statistics Surveys, 2008, 2: 61-93.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Hesterberg T, Choi N H, Meier L, 等. 最小角度和 l1 惩罚回归：综述[J]. 统计调查, 2008, 2:
    61-93.'
- en: '[71] Christensen R H B. ordinal—regression models for ordinal data[J]. R package
    version, 2015, 28: 2015.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Christensen R H B. ordinal—用于有序数据的回归模型[J]. R包版本, 2015, 28: 2015.'
- en: '[72] Elith J, Leathwick J. Boosted Regression Trees for ecological modeling[J].
    R Documentation. Available online: https://cran. r-project. org/web/packages/dismo/vignettes/brt.
    pdf (accessed on 12 June 2011), 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Elith J, Leathwick J. 用于生态建模的提升回归树[J]. R文档. 在线获取: https://cran.r-project.org/web/packages/dismo/vignettes/brt.pdf（访问日期:
    2011年6月12日），2017.'
- en: '[73] Tyree S, Weinberger K Q, Agrawal K, et al. Parallel boosted regression
    trees for web search ranking[C]//Proceedings of the 20th international conference
    on World wide web. 2011: 387-396.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Tyree S, Weinberger K Q, Agrawal K, 等. 用于网页搜索排序的并行提升回归树[C]//第20届国际万维网会议论文集.
    2011: 387-396.'
- en: '[74] Choi S, Kim T, Yu W. Performance evaluation of RANSAC family[J]. Journal
    of Computer Vision, 1997, 24(3): 271-300.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Choi S, Kim T, Yu W. RANSAC 家族的性能评估[J]. 计算机视觉期刊，1997，24(3): 271-300。'
- en: '[75] Derpanis K G. Overview of the RANSAC Algorithm[J]. Image Rochester NY,
    2010, 4(1): 2-3.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Derpanis K G. RANSAC 算法概述[J]. 图像，罗切斯特 NY，2010，4(1): 2-3。'
- en: '[76] Wilcox R. A note on the Theil‐Sen regression estimator when the regressor
    is random and the error term is heteroscedastic[J]. Biometrical Journal: Journal
    of Mathematical Methods in Biosciences, 1998, 40(3): 261-268.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Wilcox R. 关于 Theil‐Sen 回归估计量的说明，当回归变量是随机的且误差项是异方差时[J]. 生物统计学期刊：数学方法在生物科学中的应用，1998，40(3):
    261-268。'
- en: '[77] Fernandes R, Leblanc S G. Parametric (modified least squares) and non-parametric
    (Theil–Sen) linear regressions for predicting biophysical parameters in the presence
    of measurement errors[J]. Remote Sensing of Environment, 2005, 95(3): 303-316.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Fernandes R, Leblanc S G. 在测量误差存在的情况下，预测生物物理参数的参数（修正最小二乘）和非参数（Theil–Sen）线性回归[J].
    环境遥感，2005，95(3): 303-316。'
- en: '[78] Sun Q, Zhou W X, Fan J. Adaptive huber regression[J]. Journal of the American
    Statistical Association, 2020, 115(529): 254-265.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Sun Q, Zhou W X, Fan J. 自适应 Huber 回归[J]. 美国统计学会杂志，2020，115(529): 254-265。'
- en: '[79] Fox J, Weisberg S. Robust regression[J]. An R and S-Plus companion to
    applied regression, 2002, 91.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Fox J, Weisberg S. 鲁棒回归[J]. 应用回归的 R 和 S-Plus 伴侣，2002，91。'
- en: '[80] Ostertagová E. Modelling using polynomial regression[J]. Procedia Engineering,
    2012, 48: 500-506.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Ostertagová E. 使用多项式回归进行建模[J]. 工程学会学报，2012，48: 500-506。'
- en: '[81] Theil H. A rank-invariant method of linear and polynomial regression analysis[M]//Henri
    Theil’s contributions to economics and econometrics. Springer, Dordrecht, 1992:
    345-381.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Theil H. 线性和多项式回归分析的秩不变方法[M]//Henri Theil 对经济学和计量经济学的贡献。Springer，多德雷赫特，1992:
    345-381。'
- en: '[82] Bendel R B, Afifi A A. Comparison of stopping rules in forward “stepwise”
    regression[J]. Journal of the American Statistical association, 1977, 72(357):
    46-53.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Bendel R B, Afifi A A. 前向“逐步”回归中停止规则的比较[J]. 美国统计学会杂志，1977，72(357): 46-53。'
- en: '[83] Zheng B, Agresti A. Summarizing the predictive power of a generalized
    linear model[J]. Statistics in medicine, 2000, 19(13): 1771-1781.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Zheng B, Agresti A. 总结广义线性模型的预测能力[J]. 医学统计，2000，19(13): 1771-1781。'
- en: '[84] Graybill F A. Theory and application of the linear model[M]. North Scituate,
    MA: Duxbury press, 1976.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Graybill F A. 线性模型的理论与应用[M]. 北斯库特，马萨诸塞州：Duxbury出版社，1976。'
