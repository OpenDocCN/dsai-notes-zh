- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:55:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2104.12647] A Survey Of Regression Algorithms And Connections With Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2104.12647](https://ar5iv.labs.arxiv.org/html/2104.12647)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey Of Regression Algorithms And Connections With Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yunpeng Tai Yunpeng Tai is a freshman of Suzhou University of Science and Technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: yunpengtai@foxmail.com.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regression has attracted immense interest lately due to its effectiveness in
    tasks like predicting values. And Regression is of widespread use in multiple
    fields such as Economics, Finance, Business, Biology and so on. While considerable
    studies have proposed some impressive models, few of them have provided a whole
    picture regarding how and to what extent Regression has developed. With the aim
    of aiding beginners in understanding the relationships among different Regression
    algorithms, this paper characterizes a broad and thoughtful selection of recent
    regression algorithms, providing an organized and comprehensive overview of existing
    work and models utilized frequently. In this paper, the relationship between Regression
    and Deep Learning is also discussed and a conclusion can be drawn that Deep Learning
    can be more powerful as an combination with Regression models in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Regression, Survey, Comparison Between Algorithms, A Different Vision Of Ordinary
    Least Squares, Insight Of Regression Future.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression is an approach of obtaining a relationship between input space and
    output space. The relationship is represented by a function $f:X\longmapsto Y$
    and $X$ is known as the independent variable, and $Y$ as the dependent variable.
    It was originally put forward by Legendre [[1](#bib.bib1)] in 1805, who applied
    least squares in Regression. And then Gauss [[2](#bib.bib2)] published a further
    development of the theory of least squares featuring ordinary least squares in
    1821.
  prefs: []
  type: TYPE_NORMAL
- en: Regression belongs to supervised learning and $Y$ is continuous, i.e. $Y\in
    R$. Undoubtedly, Regression is powerful and has made tremendous impact in enormous
    fields. As such, an increasing number of research has made fundamental improvement
    to Regression models over the past few decades.
  prefs: []
  type: TYPE_NORMAL
- en: There has been such a surge of Regression models proposed recently, that researchers
    and beginners may find it challenging to figure out what exactly every model means
    and the relationship between them. Thus, a survey of the existing Regression models
    is beneficial both to beginners who just want to scratch the surface of Regression
    and researchers willing to have a systematic view of Regression models and gain
    insight from those smart models.
  prefs: []
  type: TYPE_NORMAL
- en: The key component of Regression is ordinary least squares. It is capable of
    producing an unbiased linear model of minimum variance as long as six necessary
    assumptions is satisfied according to Gauss-Markov Theorem [[2](#bib.bib2)]. However,
    if OLS is applied in specific areas, some of the assumptions are likely to be
    violated so that OLS fails to play its part in predicting values. Hence, it is
    essential to grasp those assumptions and figure out possible solutions when one
    of them is broken. And those well-known concerns with OLS contributes to extensive
    models designed to fix those violations such as Ridge [[3](#bib.bib3)][[4](#bib.bib4)],
    Lasso [[5](#bib.bib5)], Elastic Net [[6](#bib.bib6)] and so on.
  prefs: []
  type: TYPE_NORMAL
- en: What distinguishes this paper from others is the earlier part of this paper
    is OLS-centered and alternative solutions provided by different models are discussed
    at length when some assumptions are broken (Figure 1). This paper views the relationship
    between models on the whole and discusses the details of distinct models specifically
    and explicitly. And this paper also provides a walk-through of some uncommon Regression
    models. Whatâ€™s more, a possible direction to which Regression is going to develop
    is covered.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d4ed67deed173927259be5eefbe4f00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Problems With OLS And Possible Solutions Provided By Distinct Models.
    N is the size of input space. P is the number of features of every sample in input
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/286ee7b250a486ee36360a6ed9e4cc11.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) $X_{T}=log(x)$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e6424da5c225b60ecb7d09da74dc384.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) $X_{T}=\sqrt{x}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6154d26ebb7a883dd22a70e1f9a0165.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) $X_{T}=exp(x)$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/319b0c1e74490f3a06ed843c02ca4b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) $X_{T}=\frac{1}{x}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b20d6df06247b3c421994db44b1d7e9d.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) $X_{T}=(x-3)^{2}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb139e21a0d13fb3d0092e3890b29f88.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) $X_{T}=x^{3}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d00c7d9604f5b738277b70d7f876dd50.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) $X_{T}=x^{4}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45c63672eeba2b228198d9d9d6d396dd.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) $X_{T}=x^{5}$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f355f04fda2b08a3526047f918d6c71d.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) $X_{T}=x+1$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/806bed1be11e9148a7eb7b41b2c88e72.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) $X_{T}=\frac{1}{x^{2}}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Visualize Transformed Results. Every figure corresponds to a function.'
  prefs: []
  type: TYPE_NORMAL
- en: This paper is organized as follows. A brief introduction of the Regression task
    and convention in this paper is included in Section 2\. In Section 3, OLS, its
    assumptions and possible solutions for violation are comprehensively explained.
    Section 4 is composed of a number of Regression models which enable OLSâ€™s potential
    to be stimulated although the real data challenges its assumptions and some unexpected
    situations happen. Generalized Linear Models and an uncommon Regression named
    as Step-Wise are explored in Section 5\. Section 6 sets out to provide a quick
    overview of the strong bond between Regression and Deep Learning. In Section 7,
    conclusions about Regression are drawn and possible combination of Regression
    and Deep Learning in the future is discussed. In a nutshell, this well-established
    paper is an overview of Regression models and the relationship between Regression
    and Deep Learning, and hopefully this paper does make sense.
  prefs: []
  type: TYPE_NORMAL
- en: 2 The Regression Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our data looks like $\left\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\right\}$,
    which $y_{i}\in R$. We intend to train a model from our data set and implement
    it in unknown test sets. A standard for machine performs well is that low residuals(distance
    from predicted values to labels). When it comes to regression task, it is common
    way to implement Linear Regression.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y$ | $\displaystyle=\hat{y}+\epsilon$ |  | (\theparentequation.1)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\theta x+b+\epsilon$ |  | (\theparentequation.2) |'
  prefs: []
  type: TYPE_TB
- en: $\theta$ is called slope(gradient) or coefficient and $b$ is called intercept.
    $\theta$ explains when $x$ changes to what extent $\hat{y}$ is going to change.
    $X=\left\{x_{1},x_{2},...,x_{n}\right\}$ is known as input space and $Y=\left\{y_{1},y_{2},...,y_{n}\right\}$
    as output space. $x_{i}$ is called a sample, and $x_{ij}$ means the j-th feature
    of the i-th sample. $y$ is the label and $\hat{y}$ is the prediction. And $\epsilon$
    is the error accompanied by every prediction and is also called the distance from
    $\hat{y}$ to $y$(residual). In ordinary least squares, the model assumes that
    $y$ is actually sampled from Gaussian Distribution and every sample is with noise.
    Thus, it can also called noise in statistics. But in this paper, I am going to
    use error for that.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Ordinary Least Squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Machine Learning, we always figure out the best model by minimizing our objective
    function, which is also known as cost function. OLS(Ordinary Least Squares) serves
    as an effective loss function as long as the model satisfies six necessary assumptions.
    Then it can choose an unbiased model of minimum variance by minimizing the function
    below. And $J(\theta)$ is convex. Thus, set partial derivative of $\theta$ zero
    and then we can get the best parameter $\theta^{*}$. Note that only when $X^{T}X$
    is full rank, equation(3) does make sense. Some books may multiply $J(\theta)$
    by $1/n$, which is convenient for computation. Note that $X$ and $Y$ are matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\sum\limits_{i}^{n}(y_{i}-(\theta
    x_{i}+b))^{2}$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\theta^{*}$ | $\displaystyle=(X^{T}X)^{-1}X^{T}Y$ |  | (3)
    |'
  prefs: []
  type: TYPE_TB
- en: 3.1 Prior Assumptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, six necessary assumptions is studied. And possible solutions
    for unsatisfied situations are also covered.
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linearity. In other words, only Straight Line models are permitted. If the relationship
    between $X$ and $y$ is a non-linear model, e.g. $y=X^{4}$, the whole regression
    model crashes. And the recipe for this situation is applying feature transformation.
    By doing so, the whole relationship between $X$ and $y$ is changed for good. Hence,
    we must also take the correlation between $X$ and $y$ into consideration. Correlation
    can be told by calculating $R^{2}$ (Coefficient Of Determination), which stands
    for the ability to predict $y$ by observing $X$. When $R^{2}=1$, it means the
    loss of this predictor is 0\. If $R^{2}=0$, it is equivalent to that the predictor
    is constant, which indicates $X$ has nothing to do with $y$. As shown in Table
    1, in Column $R^{2}$, the value in bracket stands for the original coefficient.
    And the same goes for Column Linearity. Note that I use 10000 random points from
    $y=2(X-3)^{2}+5+N$, which N stands for noise and it varies from 0 to 1\. X follows
    random and even distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\overline{y}$ | $\displaystyle=\frac{1}{n}\sum\limits_{i=1}^{n}y_{i}$
    |  | (4) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle R^{2}$ | $\displaystyle=1-\frac{\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}}{\sum\limits_{i=1}^{n}(y_{i}-\overline{y})^{2}}$
    |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/401b37eb0854eb706e9ccb2c1efb0d0c.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Errors
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3550fcaffbb74f69e1231d88a5907fd3.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Norml Distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fa611040d3cd424fc8fd61981b61857.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) Q-Q of ND
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3: In Figure(a), errors are not symmetrical and Q-Q plot doesnâ€™t look
    like a line,which indicates errors donâ€™t follow Normal Distribution. Figure(b)
    and Figure(c) show when errors follow Normal Distribution, the hist plot should
    look like bell curve and Q-Q plot should be a line.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE I: Transformation Results About $R^{2}$ And Linearity'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| $X$ | $y$ | $Transformation$ | $R^{2}$(-1.602) | $Linearity(Non)$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1.450 | 9.994 | $X_{T}=log_{10}(x)$ | 0.764 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1.003 | 13.475 | $X_{T}=\sqrt{x}$ | 0.693 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 3.179 | 5.935 | $X_{T}=exp(x)$ | 0.281 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 3.801 | 6.726 | $X_{T}=\frac{1}{x}$ | 0.870 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1.294 | 11.704 | $X_{T}=(x-3)^{2}$ | 0.983 | Linear |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1.399 | 10.915 | $X_{T}=x^{3}$ | 0.344 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 3.591 | 5.951 | $X_{T}=x^{4}$ | 0.253 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 2.987 | 6.742 | $X_{T}=x^{5}$ | 0.188 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 3.161 | 5.906 | $X_{T}=x+1$ | 0.616 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 2.065 | 7.647 | $X_{T}=\frac{1}{x^{2}}$ | 0.904 | Non |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: As shown in Table 1, $R^{2}$ changes when feature transformation is applied.
    And the purpose is to find linear model with the best $R^{2}.$ Note that linear
    means y w.r.t transformed X. And if so, scatter plot of original X and predicted
    y should fit the original distribution plot. This assumption is the most significant
    for Linear Regression and it may explain why Ridge or Lasso also performs badly
    when the relationship is nonlinear.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constant Error Variance [[7](#bib.bib7)]. It means errors are uniformly distributed,
    which in statistics is called no Heteroscedasticity. When we apply our model,
    we can get a bunch of predicted values via observing $X$. Then we can calculate
    errors between true values and predicted values. And we can also calculate the
    variance of errors. If errors follow normal distribution (equation 5), thus its
    variance is constant($\sigma^{2}$). Also, its distribution is symmetrical. In
    turn, the distribution of errors should also be uniform and symmetrical. So we
    can use error plot to detect it (Figure 3). And Q-Q Plot can detect whether the
    errors follow normal distribution. The data I use can be downloaded [here](https://www.kaggle.com/quantbruce/real-estate-price-prediction).
    I choose X2 house age as $X$ and Y house price of unit area for $y$. Note that
    I remove points that $X$ equals $0$. And I choose 200 points for study.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}}$
    |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^{2}}{2})}$
    |  | (7) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: equation(6) represents standard normal distribution, $\mu=0,\sigma=1$. As shown
    in Figure 3, if errors follow normal distribution, the distribution should be
    uniform just like Figure (b) and histogram should be like bell curve. We can also
    draw a safe conclusion that if errors follow normal distribution, their distribution
    should fit the line in Q-Q Plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23d138528d172f39a907f635337300c5.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Before Log
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/379768c5d81a6e2b6ea5a75ea8297514.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) After Log
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4: The Relationship Between Residuals And Log'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After figuring out heteroscedasticity, we can come up with a question that how
    it influences our model and how to improve it. The most common way is to try feature
    transformation e.g. Log. As shown in Figure 4, it can, to some degree, make our
    errorsâ€™ distribution slightly more stable. Itâ€™s always an option to try, but not
    an effective method to handle the problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Whatâ€™s more, we can apply Box Cox Transformation, which can make data more close
    to normal distribution. In statistics, if data follows normal distribution and
    then the variance of noise(error) is a constant($\sigma^{2}$). Thus,normality
    of data is likely to relieve heteroscedasticity. And data can be downloaded [here](https://archive.ics.uci.edu/ml/machine-%0Alearning-databases/wine-quality/winequality-white.csv).
    I choose total sulfur dioxide for $X$and quality for $y$. In Figure 5, it may
    relieve heteroscedasticity. Note you canâ€™t always depend on it. Also, it can be
    worse(Figure 6).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent Errors(no autocorrelation, AC for short). For instance, you want
    to predict the shares in stock market. But the errors are correlated while they
    should be $i.i.d$(independent identically distributed). When a financial crisis
    happens, the shares is going to be extremely unstable in next few months,which
    means errors are going to increase sharply. It can be detected by Durbin Watson
    Test(Table 3) or drawing AC Plot. And if values of y axis in AC Plot vary from
    $(0,1]$, they mean Positive AC. If values equal 0, they mean Non AC. Otherwise,
    they mean Negative AC.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE II: Durbin Watson Test'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| $Value$ | $Relationship$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 2.0 | no Ac |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 0.0-2.0 | positive AC |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 2.0 - 4.0 | negative AC |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: The autocorrelation can affect errorsâ€™ standard deviation while itâ€™s unlikely
    to have an influence on modelâ€™s coefficient and intercept [[8](#bib.bib8)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thereâ€™re two common ways to fix it. The first is to add omitted variables. For
    example, you want to predict stock performaces by time. Undoubtedly, the model
    is of hight autocorrelation. We can, however, add S&P 500\. And hopefully, it
    may relieve autocorrelation. The second is to switch another function. You can
    transform your linear model into a squared model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢![Refer to caption](img/61f96c1e71017e7761b3ced546fc8a9b.png)
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: (a) Density Distribution
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/023116ed048cbd14fe6f0a979681834a.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: (b) Original Residual
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65f1ceba6559b7edbcc94e8e8bc53d09.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: (c) Final Residual
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5: Box Cox Transformation May Help'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Multicollinearity. If independent variables are related to each other, thereâ€™s
    Multicollinearity in data. We can use Variance Inflation Factor(VIF) to detect
    it($R^{2}$ is Coefficient Of Determination). If value = 1, it implies that there
    is no Multicollinearity among the predictors. If value >5, it implies thereâ€™s
    potential Multicollinearity. If value >10, it implies apparent Multicollinearity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $softmax(x)=\frac{e^{x_{i}}}{\sum_{i=1}^{n}e^{x_{i}}}$ |  | (8) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Our goal of Regression model is to figure out the relationship between independent
    variable(X) and dependent variable(y) by finding a proper coefficient. But when
    thereâ€™s Multicollinearity, the coefficient is unable to interpret. We actually
    donâ€™t know what exactly the relationship is. However, if we just want to make
    good predictions, itâ€™s still effective [[7](#bib.bib7)]. And if the degree of
    Multicollinearity is moderate, you donâ€™t have to care about it too much. We can
    remove highly correlated variables or increase sample size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normality Of Data [[7](#bib.bib7)]. Box-Cox is the efficient transformation
    to make data more close to normal distribution. Normalization and some basic feature
    transformation may help. And also try increasing data size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No Exogeneity. If $X$ we choose iteself is of little influence on $y$, which
    means the real prediction is not based on $X$, then thereâ€™s exogeneity [[7](#bib.bib7)].
    And the best solution is to make a good analysis about what on earth affects our
    predicted values and choose a suitable $X$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Alternative Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, OLSâ€™s weaknesses are going to appear in Background. And each
    Background stands for a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Ridge Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Background: high variance. OLS enables the predictor to perform good on training
    sets, however, this is an invitation to poor performance on testing sets, which
    is also known as overfitting. Generally speaking, the more complicated a model
    is, the poorer performance of the model on unknown sets. According to Occamâ€™s
    rule, a model is more likely to do a good job on unknown sets if the model is
    simple. And the model is of high generalization ability.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrinkage. When OLS is applied in real life, the predictorâ€™s coefficients can
    be too large in absolute. The coefficient of variable more correlated to $y$ is
    large while the one of variable less correlated to $y$ is also large,which is
    misleading to figure out the relationship between $X$ and $y$ [[3](#bib.bib3)].
    This phenomenon can account for poor performance on unknown sets. Thus, Ridge
    is going to implement shrinkage on coefficients and the extent of shrinkage counts
    on the degree of correlation. Typically, if a variable holds much predicting power,
    its coefficient is more likely to be big [[4](#bib.bib4)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonorthogonal Solution [[3](#bib.bib3)]. Whether OLS can be directly computed
    just by derivative relies mainly on if $X^{T}X$ is orthogonal. If $X^{T}X$ is
    nonorthogonal, this means $X^{T}X$ is not reversible and the direct computation
    canâ€™t work. $I$ is a unit matrix. Thereâ€™re only small positive quantity on the
    diagonal of $kI$, the diagonal looks like a ridge in comparison with zeroâ€™s distribution(equation
    9).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6460c8bdeb9066d8e0b11077913a09c2.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Density Distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/837fccf2f29760a886fc625aebc491e1.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Original Residual
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29520168b7dbfe8711f56847b83b56ad.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) Final Residual
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6: Box Cox Transformation May Suck'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biased Model. The potential assumptions of Ridge are that $X^{T}X$ is nonorthogonal
    and coefficients need shrinkage. Hence, Ridge actually do a trade-off between
    bias and variance and it uses increased bias to obtain reduced variance. Because
    Ridgeâ€™s assumptions are correct in most cases, it is capable of producing a model
    with low variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2-Penalty. Equation 10 is Ridgeâ€™s loss function. The loss function means not
    only an accurate model is required, its coefficients should be small. And $\lambda/2||\theta_{i}||^{2}$
    is called L2-norm. This kind of method is known as regularization in the sense
    that the model generated by regularization is of high generalization ability.
    Except direct computation, gradient descent is a common way to get the best parameter.
    Ridge always minuses the coefficient vector(equation 11.1 & 11.2).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\theta^{*}=(X^{T}X+kI)^{-1}X^{T}Y$ |  | (9) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $J(\theta)=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\frac{\lambda}{2}&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (10) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda\theta$ |  | (\theparentequation.1) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | <math  class="ltx_Math" alttext="\displaystyle=\begin{bmatrix}\frac{\partial
    M(\theta)}{\partial\theta_{1}}\\ \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \vdots\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \theta_{2}\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \vdots\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \theta_{n}\end{bmatrix}" display="inline"><semantics ><mrow
     ><mo  >=</mo><mrow
     ><mrow 
    ><mo  >[</mo><mtable
    rowspacing="0pt"  ><mtr
     ><mtd 
    ><mfrac  ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mn
     >1</mn></msub></mrow></mfrac></mtd></mtr><mtr
     ><mtd 
    ><mfrac  ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mn
     >2</mn></msub></mrow></mfrac></mtd></mtr><mtr
     ><mtd 
    ><mi mathvariant="normal" 
    >â‹®</mi></mtd></mtr><mtr 
    ><mtd  ><mfrac
     ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mi
     >n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo
     >]</mo></mrow><mo 
    >+</mo><mrow  ><mi
     >Î»</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow
     ><mo 
    >[</mo><mtable rowspacing="0pt" 
    ><mtr  ><mtd
     ><msub 
    ><mi  >Î¸</mi><mn
     >1</mn></msub></mtd></mtr><mtr
     ><mtd 
    ><msub  ><mi
     >Î¸</mi><mn
     >2</mn></msub></mtd></mtr><mtr
     ><mtd 
    ><mi mathvariant="normal" 
    >â‹®</mi></mtd></mtr><mtr 
    ><mtd  ><msub
     ><mi 
    >Î¸</mi><mi 
    >n</mi></msub></mtd></mtr></mtable><mo
     >]</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="latexml"  >absent</csymbol><apply
     ><apply 
    ><csymbol cd="latexml" 
    >matrix</csymbol><matrix 
    ><matrixrow  ><apply
     ><apply
     ><apply
     ><ci
     >ğ‘€</ci><ci
     >ğœƒ</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><cn
    type="integer"  >1</cn></apply></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><apply 
    ><ci 
    >ğ‘€</ci><ci 
    >ğœƒ</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğœƒ</ci><cn type="integer" 
    >2</cn></apply></apply></apply></matrixrow><matrixrow
     ><ci 
    >â‹®</ci></matrixrow><matrixrow 
    ><apply  ><apply
     ><apply
     ><ci
     >ğ‘€</ci><ci
     >ğœƒ</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><ci
     >ğ‘›</ci></apply></apply></apply></matrixrow></matrix></apply><apply
     ><ci 
    >ğœ†</ci><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix
     ><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><cn
    type="integer"  >1</cn></apply></matrixrow><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğœƒ</ci><cn type="integer" 
    >2</cn></apply></matrixrow><matrixrow 
    ><ci  >â‹®</ci></matrixrow><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğœƒ</ci><ci 
    >ğ‘›</ci></apply></matrixrow></matrix></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\begin{bmatrix}\frac{\partial
    M(\theta)}{\partial\theta_{1}}\\ \frac{\partial M(\theta)}{\partial\theta_{2}}\\
    \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}\theta_{1}\\
    \theta_{2}\\ \vdots\\ \theta_{n}\end{bmatrix}</annotation></semantics></math>
    |  | (\theparentequation.2) |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Lasso Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Background: Ridgeâ€™s poor performance on outliers. Compared to OLS, Ridge is
    quite powerful but shrinkage means it just cuts down on coefficientsâ€™ ability
    of affecting the result. As such, each coefficient still has influence on the
    result which further indicates Ridge still cares about every sampleâ€™s loss. However,
    when outliers appear in the data, Ridge fails to deal with them in that it is
    sensitive to outliers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Selection [[5](#bib.bib5)]. Unlike Ridge, Lasso implements feature selection
    because only one coefficient is saved and others are set zero, which is known
    as sparse solution. Therefore, only one sample has effect on the prediction in
    the sense that Lasso is insensitive to outliers and robust to small changes. Feature
    selection results in oscillation in optimization, which means Ridge is more stable
    than Lasso in gradient descent(Figure 7). Whatâ€™s more, although Lasso has looked
    at all the data, only one sample does make sense. Thus, Lasso is also able to
    avoid overfitting. If performance of Lasso is excellent, then we can say which
    sample does work. So Lasso is an interpretable model compared to OLS and Ridge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c16e273cbf0e599a3ec57d48c5723949.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: Ridge is more stable than Lasso in optimization process(2016\. Deep
    Learning. MIT Press).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1-Penalty. Lasso minuses a constant in gradient descent(equation 13.1 & 13.2).
    Suppose weâ€™re on the top of a mountain, what Lasso does is just move a bitter
    farther while Ridge just goes where seems more steap. Hence, Ridge is more faster
    than Lasso. So when values are quite large, Ridge should be a better choice than
    Lasso. But when values are small, Lasso should be a better choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+\lambda&#124;&#124;\theta_{i}&#124;&#124;$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\frac{\partial J(\theta)}{\partial\theta}$ | $\displaystyle=\frac{\partial
    MSE}{\partial\theta}+\lambda sign(\theta)$ |  | (\theparentequation.1) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math  class="ltx_Math" alttext="\displaystyle=\begin{bmatrix}\frac{\partial
    M(\theta)}{\partial\theta_{1}}\\ \frac{\partial M(\theta)}{\partial\theta_{2}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
  prefs: []
  type: TYPE_NORMAL
- en: sign(\theta_{2})\\
  prefs: []
  type: TYPE_NORMAL
- en: \vdots\\
  prefs: []
  type: TYPE_NORMAL
- en: sign(\theta_{n})\end{bmatrix}" display="inline"><semantics ><mrow
     ><mo  >=</mo><mrow
     ><mrow 
    ><mo  >[</mo><mtable
    rowspacing="0pt"  ><mtr
     ><mtd 
    ><mfrac  ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mn
     >1</mn></msub></mrow></mfrac></mtd></mtr><mtr
     ><mtd 
    ><mfrac  ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mn
     >2</mn></msub></mrow></mfrac></mtd></mtr><mtr
     ><mtd 
    ><mi mathvariant="normal" 
    >â‹®</mi></mtd></mtr><mtr 
    ><mtd  ><mfrac
     ><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><mrow
     ><mi
     >M</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >Î¸</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mrow
     ><mo
    rspace="0em"  >âˆ‚</mo><msub
     ><mi
     >Î¸</mi><mi
     >n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo
     >]</mo></mrow><mo 
    >+</mo><mrow  ><mi
     >Î»</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow
     ><mo 
    >[</mo><mtable rowspacing="0pt" 
    ><mtr  ><mtd
     ><mrow 
    ><mi 
    >s</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><msub
     ><mi
     >Î¸</mi><mn
     >1</mn></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    ><mrow  ><mi
     >s</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><msub
     ><mi
     >Î¸</mi><mn
     >2</mn></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    ><mi mathvariant="normal" 
    >â‹®</mi></mtd></mtr><mtr 
    ><mtd  ><mrow
     ><mi 
    >s</mi><mo lspace="0em" rspace="0em"
     >â€‹</mo><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >g</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi
     >n</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow
     ><mo
    stretchy="false"  >(</mo><msub
     ><mi
     >Î¸</mi><mi
     >n</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mtd></mtr></mtable><mo
     >]</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="latexml"  >absent</csymbol><apply
     ><apply 
    ><csymbol cd="latexml" 
    >matrix</csymbol><matrix 
    ><matrixrow  ><apply
     ><apply
     ><apply
     ><ci
     >ğ‘€</ci><ci
     >ğœƒ</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><cn
    type="integer"  >1</cn></apply></apply></apply></matrixrow><matrixrow
     ><apply 
    ><apply 
    ><apply 
    ><ci 
    >ğ‘€</ci><ci 
    >ğœƒ</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğœƒ</ci><cn type="integer" 
    >2</cn></apply></apply></apply></matrixrow><matrixrow
     ><ci 
    >â‹®</ci></matrixrow><matrixrow 
    ><apply  ><apply
     ><apply
     ><ci
     >ğ‘€</ci><ci
     >ğœƒ</ci></apply></apply><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><ci
     >ğ‘›</ci></apply></apply></apply></matrixrow></matrix></apply><apply
     ><ci 
    >ğœ†</ci><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix
     ><matrixrow 
    ><apply  ><ci
     >ğ‘ </ci><ci
     >ğ‘–</ci><ci
     >ğ‘”</ci><ci
     >ğ‘›</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><cn
    type="integer"  >1</cn></apply></apply></matrixrow><matrixrow
     ><apply 
    ><ci 
    >ğ‘ </ci><ci 
    >ğ‘–</ci><ci 
    >ğ‘”</ci><ci 
    >ğ‘›</ci><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >ğœƒ</ci><cn type="integer" 
    >2</cn></apply></apply></matrixrow><matrixrow
     ><ci 
    >â‹®</ci></matrixrow><matrixrow 
    ><apply  ><ci
     >ğ‘ </ci><ci
     >ğ‘–</ci><ci
     >ğ‘”</ci><ci
     >ğ‘›</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >ğœƒ</ci><ci
     >ğ‘›</ci></apply></apply></matrixrow></matrix></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle=\begin{bmatrix}\frac{\partial
    M(\theta)}{\partial\theta_{1}}\\ \frac{\partial M(\theta)}{\partial\theta_{2}}\\
    \vdots\\ \frac{\partial M(\theta)}{\partial\theta_{n}}\end{bmatrix}+\lambda\begin{bmatrix}sign(\theta_{1})\\
    sign(\theta_{2})\\ \vdots\\ sign(\theta_{n})\end{bmatrix}</annotation></semantics></math>
    |  | (\theparentequation.2) |
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Support Vector Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Margin Maximization. SVM(Support Vector Machine) is originally invented for
    classification problems [[9](#bib.bib9)]. Unlike other algorithms, SVM not only
    needs to classify all the data correctly but also requires the distance of data
    to the Hyper plane to be the biggest, which is known as widest street. Our linear
    model is $y=\theta^{T}X+b$. Among all the data points, the distance of the closest
    positive point to the hyper plane pluses the same distance of closest negative
    point is Margin($\gamma$). Then SVM is turned into maximizing Margin. And the
    same for SVR. We can turn the maximizing $\frac{2}{||\theta||}$ into minimizing
    $\frac{||\theta||^{2}}{2}$. Because $||\theta||$ is bigger than 0, $||\theta||^{2}$
    is proportional to $||\theta||$. Then the problem for SVR is like equation 15\.
    In equation 15, C is a coefficient for regularization and $L(x)$ is an undefined
    loss function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta,b}\,\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}+C\sum_{i=1}^{n}L(y_{i}-\hat{y_{i}})$
    |  | (14) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/9ff4d8a9d869890b744b3261cc91ce09.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: $\epsilon$-insensitive loss. Smola and Sch$\ddot{o}$lkopf, 2002'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\epsilon$- insensitive loss. SVR can tolerate mistakes which are no more than
    $\epsilon$, if those points predicted wrongly in dashed area $[f(x)-\epsilon,f(x)+\epsilon]$,
    then the losses of those points equal zero, which is called $\epsilon$-insensitive
    loss(Figure 8). In equation 16,z stands for loss. Actually, it just makes a trade-off
    between errors and complexity of the model. Hence, SVR is unlikely to overfit
    the data. Thus, SVR can reduce the variance of OLS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="\displaystyle\centering
    l_{\epsilon}(z)=\begin{cases}0&amp;,if\,&#124;z&#124;\leq\epsilon\\ &#124;z&#124;-\epsilon&amp;otherwise\\'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \end{cases}\@add@centering" display="inline"><semantics ><mrow
    ><mrow ><msub ><mi
    >l</mi><mi >Ïµ</mi></msub><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"
    >(</mo><mi >z</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow
    ><mo >{</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd class="ltx_align_left"
    columnalign="left" ><mn >0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    >,</mo><mi >i</mi><mi
    >f</mi><mo fence="false" lspace="0.170em" rspace="0.167em"
    stretchy="false" >&#124;</mo><mi >z</mi><mo
    fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >â‰¤</mo><mi >Ïµ</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mrow ><mo stretchy="false"
    >&#124;</mo><mi >z</mi><mo
    stretchy="false" >&#124;</mo></mrow><mo >âˆ’</mo><mi
    >Ïµ</mi></mrow></mtd><mtd class="ltx_align_left" columnalign="left"
    ><mrow ><mi >o</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >t</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >h</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >e</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >r</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >w</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\displaystyle\centering l_{\epsilon}(z)=\begin{cases}0&,if\,&#124;z&#124;\leq\epsilon\\
    &#124;z&#124;-\epsilon&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (15) |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dual Problem. Slack variables can be introduced to optimization problem(equation
    18). They means how many errors are allowed to make beyond $\epsilon$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta,b}\,\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}$
    | $\displaystyle+C\sum_{i=1}^{n}L(\xi_{i}+\hat{\xi}_{i})$ |  | (16) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle y_{i}-\hat{y_{i}}$ | $\displaystyle\leq\epsilon+\xi_{i}$
    |  | (17) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{y_{i}}-y_{i}$ | $\displaystyle\leq\epsilon+\hat{\xi_{i}}$
    |  | (18) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\xi_{i}\geq 0\,$ | $\displaystyle,\hat{\xi_{i}}\geq 0$ |  |
    (19) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: We can introduce Lagrange Multiplier $\mu_{i}\geq 0,\hat{\mu_{i}}\geq 0,\alpha_{i}\geq
    0,\hat{\alpha_{i}}\geq 0$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle L(\theta,b,\alpha,\hat{\alpha},\xi,\hat{\xi},\mu,\hat{\mu})$
    |  | (20) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{1}{2}&#124;&#124;\theta&#124;&#124;^{2}+C\sum_{i=1}^{n}(\xi_{i}+\hat{\xi_{i}})-\sum_{i=1}^{n}\mu_{i}\xi_{i}-\sum_{i=1}^{n}\hat{\mu_{i}}\hat{\xi_{i}}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\sum_{i=1}^{n}\alpha_{i}(y_{i}-\hat{y_{i}}-\epsilon-\xi_{i})+\sum_{i=1}^{n}\hat{\alpha_{i}}(\hat{y_{i}}-y_{i}-\epsilon-\hat{\xi_{i}})$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: And set partial derivative of w,b,$\xi_{i}$ and $\hat{\xi_{i}}$ zero.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})x_{i}$
    |  | (21) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle 0$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})$
    |  | (22) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle C$ | $\displaystyle=\alpha_{i}+\mu_{i}$ |  | (23) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle C$ | $\displaystyle=\hat{\alpha_{i}}+\hat{\mu_{i}}$ |  |
    (24) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Thus, we can get Dual Problem for SVR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\alpha,\hat{\alpha}}$ | $\displaystyle\sum_{i=1}^{n}\hat{y_{i}}(\hat{\alpha_{i}}-\alpha_{i})-\epsilon(\hat{\alpha_{i}}+\alpha_{i})$
    |  | (25) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  |  | $\displaystyle-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})(\hat{\alpha_{j}}-\alpha_{j})(x_{i})^{T}x_{j}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.$ | $\displaystyle\,\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})=0$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel Trick. Typically, the model is nonlinear in 1D space. Thus, we use $\phi(x)$
    to represent the transformed x. Some common kernel functions are listed in table
    3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle w$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})\phi(x)$
    |  | (26) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle f(x)$ | $\displaystyle=\sum_{i=1}^{n}(\hat{\alpha_{i}}-\alpha_{i})k(x,x_{i})+b$
    |  | (27) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'TABLE III: Common Kernel Functions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| $Name$ | $Expression$ | $Parameters$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Linear Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(x_{i}\right)^{T}x_{j}$
    | d = 1 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Polynomial Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\left(\left(x_{i}\right)^{T}x_{j}\right)^{d}$
    | $\geq 1$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Gaussian Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\exp\left(-\frac{\left\&#124;x_{i}-x_{j}\right\&#124;^{2}}{2\sigma^{2}}\right)$
    | $\sigma>0$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| Sigmoid Kernel | $\mathrm{k}\left(\mathrm{x}_{i},x_{j}\right)=\tanh\left(\beta\left(x_{i}\right)^{T}x_{j}+\theta\right)$
    | $\beta>0,\theta\textless 0$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 4.4 Random Forest Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree Structure [[12](#bib.bib12)]. A walk-through of tree structure is going
    to be given in this section. In regression tree, thereâ€™re root node,internal nodes
    and leaf node. For instance, if weâ€™re going to predict a manâ€™s height. And we
    get menâ€™s and womenâ€™s heights. Thus, the root node is man or not. If a point which
    we need to predict is $(68,171)$. It means the height of a man weighing 68kg is
    171cm. If we use tree regression, then our internal node is $>60?$ And the second
    internal node is $<70?$ and so on. And at last, the leaf node is the result we
    predict. Hence, the leaf node is the predicted height.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification And Regression Tree [[13](#bib.bib13)]. CART includes feature
    selection,generating trees and pruning. CART assumes that decision tree is a Binary
    tree. The decision tree is equivalent to dividing features into two groups recursively.
    It divides the input space into limited units and predict the distribution. Suppose
    our data $D=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})\}$. Decision Tree
    splits input space into $M$ units $R_{1},R_{2},\dots,R_{M}$ and at the end of
    every unit is the output value $c_{m}$. And we can minimize squared errors of
    output values and true values. And itâ€™s obvious that the best output value in
    every unit should be the average value. But the question is that how to split
    the input space. And we choose $x_{j}$ randomly and its output value $s$. And
    we split the input space into two space by their output values. $R_{1}(j,s)=\{x|x_{j}\leq
    s\}$ and $R_{2}(j,s)=\{x|x_{j}>s\}$ $x_{j}$ is called splitting variable and $s$
    is splitting point. And we find $c_{1},c_{2}$ in $R_{1},R_{2}$ via squared least
    errors. We also want $c_{1}+c_{2}$ to be small enough. Thatâ€™s how we finally get
    j, s.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}$ |  | (28) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{c_{m}}=average(y_{i}&#124;x_{i}\in R_{m})$ |  | (29)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{j,s}(\min_{c1}\sum_{x_{i}\in R_{1}}(y_{i}-c_{1})^{2}+\min_{c_{2}}\sum_{x_{i}\in
    R_{2}}(y_{i}-c_{2})^{2})$ |  | (30) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning [[13](#bib.bib13)]. Decision Tree implements recursive binary splitting
    to make more accurate predictions. And if the size of data and the input space
    is quite large. The structure of the tree is complicated. Sadly, a complex model
    easily gives birth to Overfitting. So itâ€™s necessary to make our model more simple.
    Therefore, from the bottom of the tree, we cut down some child trees. Then we
    can get a tree sequence $\{T_{0},T_{1},\dots,T_{n}\}$($T_{0}$ is the root node).
    Then we employ cross-validation to choose the best child tree from it. At the
    same time, we also expect our model to perform well. Hence, we apply a loss function
    to measure the differences of performances in the process of pruning. In equation(32),
    T is arbitrary child tree. C(T) means errors on the training data. $|T|$ is the
    number of leaf nodes in a child tree. $\alpha$ is a parameter, which decides the
    regularization term. If $\alpha$ is big, it means hard punishment and this results
    in a simple tree. If $\alpha$ is small, it means soft punishment and this leads
    to a more complicated tree respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $C_{\alpha}(T)=C(T)+\alpha&#124;T&#124;$ |  | (31) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble Learning [[14](#bib.bib14)]. We use $1/3$ of our data to evaluate our
    model, which is called out of bag data. And $2/3$ of our data to be a new data
    set. Then we select subsets randomly from the new data set, which is known as
    Bagging. Every time we select one subset of the complete data set and then the
    subset is placed back. The number of points in different subsets is the same.
    We train different tree models for every different data. Finally we make an average
    of all treesâ€™ variables as our final model variable in order to cut down on the
    variance. Hence, itâ€™s an accurate model. And it is able to maintain accuracy although
    most of data is missing in that the model only randomly select a subset to train.
    However, it may overfit data when thereâ€™re some outliers in data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.5 Boosted Regression Tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biased Feature Selection [[15](#bib.bib15)]. Itâ€™s almost the same as Random
    Forest. They both select the random subset and then a new tree is generated. In
    Random Forest, the probability of data is selected is the same, which is almost
    unbiased. But in Boosted Regression Tree, it is going to give weights to every
    data point. For instance, first time we select a subset and we build a tree model
    for it. Before this subset is placed back, prediction errors are calculated for
    every point. If the error is high, this point is likely to be given large weights,
    which indicates its probability of being selected is higher than others. To summarize,
    Boosted Regression Tree focuses on the errors and is going to mix it. Whereas,
    if thereâ€™re many outliers in data, it just sucks. But it is robust to missing
    values just like Random Forest because they both select subsets to fit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.6 Elastic Net Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Background: Multicollinearity. One of OLSâ€™s assumptions is that no multicollinearity.
    However, in multivariate regression, $X$ can be sometimes dependent. If this happens,
    OLS, Ridge and Lasso fail to play their part.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encourage Group Effect [[6](#bib.bib6)]. In multivariate regression, if some
    samples is correlated to each other, OLS is likely to take one sample, not caring
    which one is selected while strongly correlated samples are on the same boat in
    Elastic Net Regression. And it view them as a whole, which is called group effect.
    It does automatic variable selection and continuous shrinkage, and it select groups
    of correlated samples [[6](#bib.bib6)], which is similar to clustering methods.
    However, this model doesnâ€™t reduce the variance and extra bias increases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation Of Lasso And Ridge. Elastic Net is a middle ground between Ridge
    Regression and Lasso Regression. It mixes Lassoâ€™s loss function with Ridgeâ€™s.
    It has the parameter r to control the mix ratio. If $r=0$, itâ€™s Ridge. If $r=1$,
    itâ€™s Lasso.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $N\gg P$. N is the number of samples and P is the number of features of every
    sample. When in multivariate regression, Elastic Net Regression can play a key
    role in $N\gg P$ cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle J(\theta)$ | $\displaystyle=\frac{1}{n}\sum\limits_{i}^{n}(y_{i}-\hat{y_{i}})^{2}+r\lambda&#124;&#124;\theta_{i}&#124;&#124;+\frac{1-r}{2}\lambda&#124;&#124;\theta_{i}&#124;&#124;^{2}$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: 4.7 Least Angle Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joint Least Squares Direction [[16](#bib.bib16)]. To begin with, the model selects
    the coefficient $\beta_{j}$ and calculate errors. When some other sample $x_{k}$
    has more correlation with errors than $x_{j}$ has. Hence, the model increases
    $(\beta_{j},\beta_{k})$ in their joint least squares direction until $x_{m}$ has
    more correlation with errors. Thus, the model increases $(\beta_{j},\beta_{k},\beta_{m})$
    in their joint least squares direction. The model comes to its end until all samples
    in the model. Therefore, it can also settle samplesâ€™ autocorrelation in high dimension.
    OLS is the special case of LARS. When LARS doesnâ€™t increase in joint least squares
    direction, the model becoms OLS. By the way, LARS is also powerful when $N\gg
    P$ just like Elastic Net Regression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive To Outliers. LARS is a method which iteratively refit the errors.
    When thereâ€™re outliers in data, then LARS doesnâ€™t make sense.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily Modified. Itâ€™s simple for LARS to join with other models such as Lasso.
    LARS-Lasso employs the Lassoâ€™s loss function and applies the LARSâ€™s method of
    coefficient selection.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.8 RANSAC Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background. Although Lasso is insensitive to a few outliers, Lasso doesnâ€™t work
    when data is filled with a large number of outliers,let alone OLS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Sample Consensus Set [[17](#bib.bib17)]. RANSAC is an iterative method.
    And it has a error threshold $\epsilon$. To begin with, it select a subset of
    the whole data and find a model to fit it. Then, use the model to test the rest
    of the data. If pointâ€™ loss on the model is no more than $\epsilon$ , then add
    it to the consensus set, which is full of inliers. And this process is iterative.
    When iterative times is reached, the process comes to its end. RANSAC is going
    to make the number of points in consensus set as large as possible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: iteration times ni = 0whileÂ *i <n*Â doÂ Â Â Â Â Â  Randomly Select Inliers From DataÂ Â Â Â Â Â 
    Find A Model M To FitÂ Â Â Â Â Â  Test Other data Via The ModelÂ Â Â Â Â Â  ifÂ *points fit
    M*Â thenÂ Â Â Â Â Â Â Â Â Â Â Â  Inliers Set $\leftarrow$ pointsÂ Â Â Â Â Â elseÂ Â Â Â Â Â Â Â Â Â Â Â Ouliers
    Set $\leftarrow$ points
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AlgorithmÂ 1 Random Sample Consensus
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voting Scheme. RANSAC is kind of like voting process. A subset of data claims
    its idea and then the rest of the data votes for the idea. In every independent
    process, there are two kinds of data. One agrees with the idea while the other
    is against it. And RANSAC is going to select a process where the number of supporters
    is max. Thus,RANSAC is biased. If the model is going to be robust to outliers,
    then there must be enough good features which vote for correct model and outliers
    canâ€™t vote consistently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages. When thereâ€™re few outliers in the data, RANSAC canâ€™t make sense
    in that the difference between every process is little. Only when the data is
    heavily contaminated, RANSAC can play its part. Besides, the threshold must be
    set by hand, which requires users to decide specific threshold on different data.
    Last but not least, the cost of computation is high because it is an iterative
    method and the number of times required in the model is unknown.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.9 Theil-Sen Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Median Method [[18](#bib.bib18)]. Thereâ€™re many data pairs used to calculate
    coefficient. $\theta=y_{a}-y_{b}/x_{a}-x_{b}$ And $\theta$ is the median of all
    $\theta s$. $b=y-\theta x$ b is also the median of $bs$. Hence, itâ€™s a nonparametric
    technique. However, complete computation leads to low speed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking Point. In particular, Theil-Sen only can tolerate 29.3% of data is
    outliers. And when the model is applied in high-dimensional regression, the rate
    is going to decrease.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.10 Huber Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huber Loss [[19](#bib.bib19)]. It transforms its loss function when faced with
    different values. When values are large, which is of high possibility of being
    outliers, Huber turns their loss function into Linear Loss in order to minimize
    their influence on the model. $\delta$ serves as a threshold, deciding how large
    data is to need a linear loss. And it is fastest in three robust regression.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="\displaystyle\centering
    HuberLoss=\begin{cases}\frac{1}{2}a^{2}&amp;,if&#124;a&#124;\leq\delta\\ \delta(&#124;a&#124;-\frac{1}{2}\delta)&amp;otherwise\\'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \end{cases}\@add@centering" display="inline"><semantics ><mrow
    ><mrow ><mi >H</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >u</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >b</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >e</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >r</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >L</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >s</mi><mo
    lspace="0em" rspace="0em" >â€‹</mo><mi >s</mi></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mfrac
    ><mn >1</mn><mn >2</mn></mfrac><mo
    lspace="0em" rspace="0em" >â€‹</mo><msup ><mi
    >a</mi><mn >2</mn></msup></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    >,</mo><mi >i</mi><mi
    >f</mi><mo fence="false" rspace="0.167em" stretchy="false"
    >&#124;</mo><mi >a</mi><mo
    fence="false" stretchy="false" >&#124;</mo><mo lspace="0.167em"
    >â‰¤</mo><mi >Î´</mi></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mi >Î´</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow
    ><mo stretchy="false" >&#124;</mo><mi
    >a</mi><mo stretchy="false" >&#124;</mo></mrow><mo
    >âˆ’</mo><mrow ><mfrac
    ><mn >1</mn><mn
    >2</mn></mfrac><mo lspace="0em" rspace="0em"
    >â€‹</mo><mi >Î´</mi></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mi
    >o</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >t</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >h</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >e</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >r</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >w</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >i</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >s</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mi
    >e</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation
    encoding="application/x-tex" >\displaystyle\centering HuberLoss=\begin{cases}\frac{1}{2}a^{2}&,if&#124;a&#124;\leq\delta\\
    \delta(&#124;a&#124;-\frac{1}{2}\delta)&otherwise\\ \end{cases}\@add@centering</annotation></semantics></math>
    |  | (33) |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.11 Multivariate Adaptive Regression Splines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partitioning [[20](#bib.bib20)]. MARS begins with partitioning data and then
    runs linear regression on each different partition. And it makes no assumptions
    about the relationship between the labels and samples. MARS originally has a large
    collection of basis functions. Each meeting point of two linear models is called
    a knot. And each knot has a pair of basis functions. And these functions are used
    to describe the relationship between $x$ and $y$. The first basis function is
    $max(0,x-y)$. The second is $max(0,y-x)$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove Basis Functions [[20](#bib.bib20)]. After MARS partitions data and builds
    models, it applies least-squares model to fit data. And each knot has two basis
    functions. The results of them can be viewed as input variables. Least-Squares
    model estimate the loss of each basis functionâ€™s output value. If a basis function
    has little influence on model fitting, then it is going to be removed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages And Disadvantages. It can fit a large number of predictor variables.
    And it is an effective and fast algorithm. Also, it is robust to outliers. However,
    it begins with a large set of models and this easily leads to overfitting. And
    it is vulnerable to missing data problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.12 Polynomial Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Background: When the relationship between $X$ and $y$ is nonlinear, OLS sucks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial Transformation [[21](#bib.bib21)]. Polynomial Regression replaces
    original $X$ with Polynomial in order to attain a more linear relationship than
    before or change features for some reason. Hence, it is not interpretable. Interestingly,
    it is somewhat like Talyor Extend. When your model breaks the assumption of linearity,
    then you can try all polynomial regression to find a best one, which is a good
    recipe.If featureâ€™s dimension is 2,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: order = 1$\Rightarrow$ $[1,X_{1},X_{2}]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: order = 2$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2}]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: order = 3$\Rightarrow$ $[1,X_{1},X_{2},X_{1}^{2},X_{1}X_{2},X_{2}^{2},X_{1}^{3},X_{1}^{2}X_{2},X_{1}X_{2}^{2},X_{2}^{3}]$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.13 Weighted Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background. One of the OLSâ€™s assumptions is constant error variance. In section
    3, I put forward log method. However, it is ineffective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformed Weights [[22](#bib.bib22)]. When $w_{i}$ all equals 1, it should
    be the OLS. In OLS, the model gives every point the same attention. But itâ€™s under
    homoskedasticity while we come across more heteroskedastic scenarios. The idea
    is that we gives more attention to those points of which error is small. Thus,
    the model gives those points bigger weights.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stable Intercept and Sensitive Coefficient [[23](#bib.bib23)]. Note I choose
    different values for the first 20 weights and others are always 1\. Different
    weights are equivalent for errorsâ€™ abnormal distribution. As shown in Table 4,
    the intercept is right regardless. However, the coefficient changes sharply. Hence,
    we canâ€™t use it to draw inferences and test our hypotheses with regard to coefficient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $WLS=\sum\limits_{i=1}^{n}w_{i}(y_{i}-\hat{y_{i}})^{2}$ |  | (34) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '![Refer to caption](img/5508595d52e2080b6690dbac95d4ce2b.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) w = 1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2765653ff4d2f2116bcb0ec31af22443.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) w = 10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb219d65c16b16c9f42f2f01d25b2697.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) w = 20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9: The middle line shows OLS and MLS fits the data.And others show the
    range of predicted values of different algorithms'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE IV: Slopes And Intercepts'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| $Weights$ | $Slopes$ | $Intercepts$ |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1.0 | 1.444 | 0.059 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 10.0 | 1.4887 | 0.059 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 20.0 | 1.5146 | 0.058 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 40.0 | 1.5407 | 0.058 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 80.0 | 1.5615 | 0.057 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 160.0 | 1.5755 | 0.057 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: As shown in Figure 9,Figure(a) shows that OLS is the special case of WLS when
    weights = 1.And Figure(b) and Figure(c) show when weights change,the intercept
    almost stays the same.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.14 Generalized Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background. OLS assumes that the error must be independent in the sense that
    one error canâ€™t be correlated to others. But when error autocorrelation happens,
    OLS has no way but to fail.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better OLS [[24](#bib.bib24)]. GLS is similar to OLS on a linearly transformed
    version of the data. And GLS is unbaised, consistent and effective. WLS is the
    special case of GLS, which means GLS can also solve heteroskedasticity [[25](#bib.bib25)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.15 Feasible Generalized Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementable GLS. While GLS sounds powerful, but it canâ€™t be applied in specific
    regression tasks. FGLS is an implementable version of GLS. And FGLS needs some
    crucial assumptions to ensure a consistent estimator for errors covariance matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inefficiency On Little Data. Whereas GLS is more powerful than OLS under heteroscedasticity
    or autocorrelation, this is not the case for FGLS. When the size of data is quite
    small, FGLS is ineffctive than OLS. Thus, some people prefer FGLS over OLS under
    small data. But when the size of data becomes large, FGLS is a better choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.16 Bayesian Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background. As is known to all, OLS exactly makes an estimation of the mean
    of the values, which fails to provide a whole picture of the relationship between
    independent variables and dependent variables. And in some cases, we want to obtain
    a possible distribution of labels instead of a mean value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian Theorem [[27](#bib.bib27)]. For instance, weâ€™re going to employ a model
    to distinguish whether a email is normal or spam. So what our model faces is that
    it has to make predictions about the unknown email. Our data has 100 emails and
    10% of them is spam. Hence, the percentage of spam is 10%. But thatâ€™s absolutely
    not the whole story. In Bayesian, itâ€™s called Prior Probability, which means the
    Basic Assumption of the distribution and thatâ€™s where Bayesian Begins. At the
    beginning of the algorithm, Bayesian is biased in return the model is easily affected
    by the original distribution. For example, if we only have all 10 normal emails,
    itâ€™s impossible that we wouldnâ€™t get any spam emails in future. In other words,
    if the size of our data is quite small, itâ€™s not incentive for us to implement
    Bayesian. However, when training times keep increasing, we should get ideal results
    ultimately. In the equation below, P(B) is a Normalization term and P(A) is Prior
    Probability. $P(A|B)$ is called Posterior Probability(Conditional Probability).
    To conclude, when we have much data, Bayesian may be a good choice to try while
    it exactly performs like other algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P(A&#124;B)$ | $\displaystyle=\frac{P(B&#124;A)P(A)}{P(B)}$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum Likelihood Estimation [[28](#bib.bib28)]. Generally speaking, our goal
    is to figure out the real data distribution, which is almost impossible. Therefore,
    we want a data distribution which is close to our data distribution from a problem
    domain. MLE(Maximum Likelihood Estimation) indicates that we want to maximize
    the probability that real data is sampled from the Hypothesis Distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\beta^{*}=\arg\max\limits_{\beta}P_{\beta}(D)$ |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum Posterior Estimation [[29](#bib.bib29)]. Typically, we can use MAP(Maximum
    A Posterior Estimation) to replace MLE. Itâ€™s based on Bayesian Theorem. And MAP
    is fundamental to Bayesian Regression(equation 37). Rather than other standard
    algorithms, Bayesian Regression doesnâ€™t produce a single value but a range of
    possible distribution. And in most cases, MLE and MAP are likely to get the same
    results. However, when the hypothesis of MAP is different from MLE, they fail
    to reach the same destination. When Prior Probability is uniformly distributed,
    they can make it. From another point of view, if we have some precise understanding
    of data, Bayesian Regression is a excellent choice in that it serves as Prior
    Probablity or we can weigh every different choice just like Weighted Least Errors.
    Interestingly, prior can be kind of Regularization or bias of the model, as such
    prior can be interpreted as L2 norm, which is also called Bayesian Ridge Regression.
    Equation(38) means given a model $m$, the probability of output y. And $\beta$(Coefficients)
    and $\sigma$(Standard Deviation) are arbitrary values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle P(\beta&#124;D)$ | $\displaystyle=\frac{P(D&#124;\beta)P(\beta)}{P(D)}$
    |  | (37) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle P(y&#124;m)$ | $\displaystyle=\frac{P(\beta,\sigma&#124;m)P(y&#124;X,\beta,\sigma,m)}{P(\beta,\sigma&#124;y,X,m)}$
    |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y$ | $\displaystyle\sim(\beta^{T}X,\sigma^{2})$ |  | (39)
    |'
  prefs: []
  type: TYPE_TB
- en: 4.17 Quantile Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformed Loss Function [[30](#bib.bib30)]. QR has a parameter q which decides
    the proportion to split the data. One is q % of the data and the other is (1-q)%
    of the data. For instance, if q = 0.5, then data is split in two. We minimize
    the squared loss in OLS while we now minimize the absolute loss in QR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $J(\theta)=\sum_{i=1}^{n}q&#124;y_{i}-\hat{y_{i}}&#124;+\sum_{i=1}^{n}(1-q)&#124;y_{i}-\hat{y_{i}}&#124;$
    |  | (40) |'
  prefs: []
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages. It goes without saying that QR can provide a more complete view
    of the relationship than OLS. Whatâ€™s more, it is also robust to outliers and situations
    where the variance of errors is not a constant.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.18 Ordinal Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background. In some cases, the values for labels are ranking numbers. For instance,
    0-5 can represent his ability of communicating with others in social science.
    And OLS canâ€™t make accurate prediction about them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking Learning [[31](#bib.bib31)]. In OR, the model has a set of thresholds
    $\theta_{1},\theta_{2},\dots,\theta_{n}$, which is used to split predictions into
    independent intervals and every interval corresponds to a $y$. The model can be
    represented by sigmoid function of which output values stand for possibility.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $P(y\leq i&#124;x)=\sigma(\theta_{i}-\hat{y_{i}})$ |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: 5 Extra Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Generalized Linear Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized Functions [[32](#bib.bib32)]. Just as the name implies, it is generalization
    of different functions. And it consists of two significant parts. The first part
    is the probability distribution of $y$ such as normal distribution(OLS). The second
    is linear predictor, which decides how the coefficients combine with independent
    variables. And GLM includes several regression models such as Binomial Regression,
    Bernoulli Regression, Poisson Regression and so on. Their application is not so
    wide and they just swift the two parts compared to OLS. Hence, itâ€™s left out in
    this paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages And Disadvantages. It can absolutely deal with situations where $y$
    doesnâ€™t follow normal distribution. However, it needs large data sets and it is
    sensitive to outliers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Step-Wise Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward Selection [[33](#bib.bib33)]. This method begins with no variables.
    And it involves testing the addition of the variable in an iterative method which
    is of great use to the improvement of the accuracy. The model repeats until no
    improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward Elimination. This method starts with many candidate variables. It involves
    testing the loss of the model with the deletion of variables. If the loss is small,
    then the variable is going to be deleted. The model repeats until no variable
    can be deleted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bidirectional Elimination. This is an combination of the above two methods.
    Whether adding or deleting a variable is decided on every step. To summarize,
    Step-Wise Regression contains a big space of possible models, which can lead to
    overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasons For Stopping [[34](#bib.bib34)]. First, the tests such as F-tests and
    t-tests are biased, thus it may not be accurate. Second, widespread incorrect
    usage and availability of alternative models such as ensemble learning have led
    to calls to stop the use of this algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Relationship With Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 General Regression Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d7a3559426a04be445926ea40552b03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: GRNN Structure. Source: https://www.mdpi.com/1424-8220/20/9/2625.'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network Structure [[35](#bib.bib35)]. GRNN includes input, pattern, summation
    and output layers. The input and output layers are independent vector and dependent
    vector. The pattern layer can be seen as a vector full of coefficients. For instance,
    if we want to apply $y=\theta x$. Then one pattern neuron stands for $\theta_{i}x_{i}$.
    And output layer can be formulated as the equation below. And this model can maintain
    its accuracy with small data and itâ€™s robust to outliers. However, the structure
    of the network is complicated so that it is computationally expensive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Y(x)=\frac{\sum_{i=1}^{n}y_{i}K(x,x_{i})}{\sum_{i=1}^{n}K(x,x_{i})}$
    |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle K(x,x_{i})=e^{-d_{i}/2\sigma^{2}}$ |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle d_{i}=(x-x_{i})^{T}(x-x_{i})$ |  | (44) |'
  prefs: []
  type: TYPE_TB
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Widespread Application [[36](#bib.bib36)]. Many regression models like Poisson
    Regression and Ordinal Regression have succeeded in using GRNN. And we can draw
    a safe conclusion that a complicated network structure can represent any kind
    of regression. But only few of them are proved successful. Neural Network is powerful
    and classic regression algorithms are well-structured. Maybe regression can be
    applied in neural network without missing its original function. Humans have made
    fundamental progress in Regression. If we can combine Regression with neural network
    perfectly, then itâ€™s another picture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, I set out necessary assumptions with OLS and little tricks to
    fix the problems when assumptions are violated. Amazingly, it seems that OLS is
    the beginning of almost evey regression model. And a large number of Regression
    models are designed to be a better OLS. They can play their part in situations
    where OLS fails to work. I hold the belief that not evey algorithm needs to be
    introduced in details. Hence, the widespread algorithms are given enough attention
    and others are quickly illustrated. Finally, I give a quick overview of GRNN.
    From this paper, I can draw three conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know Your Model. Note that regression algorithms arenâ€™t plug-and-play. You must
    know evey modelâ€™s range of application and are able to deal with situations where
    the modelâ€™s assumptions are unsatisfied.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression In the Future. Regression is older compared to Deep Learning and
    great ideas behind every classic algorithm is never out of date. And people always
    want to predict unknown values and regression task is really fascinating. Deep
    Learning is quite powerful. If regression can learn from Deep Learning and keeps
    its excellent part, I do believe regression can be more powerful in the near future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It seems that regression algorithms are out of date. However, as far as I am
    concerned, beautiful ideas behind every algorithm are shared. In other words,
    dipping into these old algorithms can enable us to gain insight and intuition
    about algorithms and put forward exciting algorithms which share the same ideas
    with regression algorithms and are just different implementations of awesome ideas
    to handle new problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A.M. Legendre. Nouvelles mÃ©thodes pour la dÃ©termination des orbites des
    comÃ¨tes, Firmin Didot, Paris, 1805\. â€œSur la MÃ©thode des moindres quarrÃ©sâ€ appears
    as an appendix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] C.F. Gauss. Theoria combinationis observationum erroribus minimis obnoxiae'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Arthur E. Hoerl & Robert W. Kennard (1970) Ridge Regression: Biased Estimation
    for Nonorthogonal Problems, Technometrics, 12:1, 55-67, DOI: 10.1080/00401706.1970.10488634'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Arthur E. Hoerl & Robert W. Kennard (1970) Ridge Regression: Applications
    to Nonorthogonal Problems, Technometrics, 12:1, 69-82, DOI: 10.1080/00401706.1970.10488635'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Tibshirani, R. (1996), Regression Shrinkage and Selection Via the Lasso.
    Journal of the Royal Statistical Society: Series B (Methodological), 58: 267-288\.
    https://doi.org/10.1111/j.2517-6161.1996.tb02080.x'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Zou, H. and Hastie, T. (2005), Regularization and variable selection via
    the elastic net. Journal of the Royal Statistical Society: Series B (Statistical
    Methodology), 67: 301-320\. https://doi.org/10.1111/j.1467-9868.2005.00503.x'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Hayashi, Fumio (2000). Econometics. Princeton University Press. p. 15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Gubner, John A. (2006). Probability and Random Processes for Electrical
    and Computer Engineers. Cambridge University Press. ISBN 978-0-521-86470-1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] V. Vapnik, â€œThe support vector method of function estimation, â€ in J.A.K.
    Suykens and J. Vandewalle (Eds) Nonlinear Modeling: Advanced Black-Box Techniques,
    Kluwer Academic Publishers, Boston, pp. 55â€“85, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Drucker, Harris; Burges, Christ. C.; Kaufman, Linda; Smola, Alexander
    J.; and Vapnik, Vladimir N. (1997); â€Support Vector Regression Machinesâ€, in Advances
    in Neural Information Processing Systems 9, NIPS 1996, 155â€“161, MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Smola, A.J., SchÃ¶lkopf, B. A tutorial on support vector regression. Statistics
    and Computing 14, 199â€“222 (2004). https://doi.org/10.1023/B:STCO.0000035301.49549.88'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. R. Safavian and D. Landgrebe, â€A survey of decision tree classifier
    methodology,â€ in IEEE Transactions on Systems, Man, and Cybernetics, vol. 21,
    no. 3, pp. 660-674, May-June 1991, doi: 10.1109/21.97458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Loh, W.â€Y. (2011), Classification and regression trees. WIREs Data Mining
    Knowl Discov, 1: 14-23\. https://doi.org/10.1002/widm.8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Liaw A, Wiener M. Classification and regression by randomForest[J]. R
    news, 2002, 2(3): 18-22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Elith J, Leathwick J R, Hastie T. A working guide to boosted regression
    trees[J]. Journal of Animal Ecology, 2008, 77(4): 802-813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Efron, Bradley; Hastie, Trevor; Johnstone, Iain; Tibshirani, Robert (2004).
    â€Least Angle Regressionâ€ (PDF). Annals of Statistics. 32 (2): pp. 407â€“499\. arXiv:math/0406456\.
    doi:10.1214/009053604000000067\. MR 2060166.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Fischler, M. & Bolles, R. ( 1981). Random Sample Consensus: A Paradigm
    for Model Fitting with Applications to Image Analysis and Automated Cartography.
    Communications of the ACM, 24, 381-395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Theil, H. (1950), â€A rank-invariant method of linear and polynomial regression
    analysis. I, II, IIIâ€, Nederl. Akad. Wetensch., Proc., 53: 386â€“392, 521â€“525, 1397â€“1412,
    MR 0036489'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Huber, Peter J. (1964). â€Robust Estimation of a Location Parameterâ€. Annals
    of Statistics. 53 (1): 73â€“101\. doi:10.1214/aoms/1177703732\. JSTOR 2238020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Friedman J H. Multivariate adaptive regression splines[J]. The annals
    of statistics, 1991: 1-67.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Stigler, Stephen M. (November 1974). â€Gergonneâ€™s 1815 paper on the design
    and analysis of polynomial regression experimentsâ€. Historia Mathematica. 1 (4):
    431â€“439\. doi:10.1016/0315-0860(74)90033-0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Ruppert D, Wand M P. Multivariate locally weighted least squares regression[J].
    The annals of statistics, 1994: 1346-1370.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Suykens J A K, De Brabanter J, Lukas L, et al. Weighted least squares
    support vector machines: robustness and sparse approximation[J]. Neurocomputing,
    2002, 48(1-4): 85-105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Amemiya, Takeshi (1985). â€Generalized Least Squares Theoryâ€. Advanced
    Econometrics. Harvard University Press. ISBN 0-674-00560-0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Kmenta, Jan (1986). â€Generalized Linear Regression Model and Its Applicationsâ€.
    Elements of Econometrics (Second ed.). New York: Macmillan. pp. 607â€“650\. ISBN
    0-472-10886-7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Kariya T, Kurata H. Generalized least squares[M]. John Wiley & Sons, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Bernardo J M, Smith A F M. Bayesian theory[M]. John Wiley & Sons, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Myung I J. Tutorial on maximum likelihood estimation[J]. Journal of mathematical
    Psychology, 2003, 47(1): 90-100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Gauvain J L, Lee C H. Maximum a posteriori estimation for multivariate
    Gaussian mixture observations of Markov chains[J]. IEEE transactions on speech
    and audio processing, 1994, 2(2): 291-298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Koenker R, Hallock K F. Quantile regression[J]. Journal of economic perspectives,
    2001, 15(4): 143-156.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Harrell Jr F E. Regression modeling strategies: with applications to linear
    models, logistic and ordinal regression, and survival analysis[M]. Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Faraway J J. Extending the linear model with R: generalized linear, mixed
    effects and nonparametric regression models[M]. CRC press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Efroymson, MA (1960) â€Multiple regression analysis.â€ In Ralston, A. and
    Wilf, HS, editors, Mathematical Methods for Digital Computers. Wiley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Flom, P. L. and Cassell, D. L. (2007) â€Stopping stepwise: Why stepwise
    and similar selection methods are bad, and what you should use,â€ NESUG 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Specht, D. F. (2002-08-06). â€A general regression neural networkâ€. IEEE
    Transactions on Neural Networks. 2 (6): 568â€“576\. doi:10.1109/72.97934\. PMID
    18282872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Dreiseitl S, Ohno-Machado L. Logistic regression and artificial neural
    network classification models: a methodology review[J]. Journal of biomedical
    informatics, 2002, 35(5-6): 352-359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Hutcheson G D. Ordinary least-squares regression[J]. L. Moutinho and GD
    Hutcheson, The SAGE dictionary of quantitative management research, 2011: 224-228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Dismuke C, Lindrooth R. Ordinary least squares[J]. Methods and Designs
    for Outcomes Research, 2006, 93: 93-104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Kiers H A L. Weighted least squares fitting using ordinary least squares
    algorithms[J]. Psychometrika, 1997, 62(2): 251-266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Marquardt D W, Snee R D. Ridge regression in practice[J]. The American
    Statistician, 1975, 29(1): 3-20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Hoerl A E, Kannard R W, Baldwin K F. Ridge regression: some simulations[J].
    Communications in Statistics-Theory and Methods, 1975, 4(2): 105-123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Le Cessie S, Van Houwelingen J C. Ridge estimators in logistic regression[J].
    Journal of the Royal Statistical Society: Series C (Applied Statistics), 1992,
    41(1): 191-201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Osborne M R, Presnell B, Turlach B A. On the lasso and its dual[J]. Journal
    of Computational and Graphical statistics, 2000, 9(2): 319-337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zou H. The adaptive lasso and its oracle properties[J]. Journal of the
    American statistical association, 2006, 101(476): 1418-1429.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Park T, Casella G. The bayesian lasso[J]. Journal of the American Statistical
    Association, 2008, 103(482): 681-686.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zhao P, Yu B. On model selection consistency of Lasso[J]. The Journal
    of Machine Learning Research, 2006, 7: 2541-2563.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Meinshausen N. Relaxed lasso[J]. Computational Statistics & Data Analysis,
    2007, 52(1): 374-393.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Zou H, Hastie T. Regression shrinkage and selection via the elastic net,
    with applications to microarrays[J]. JR Stat Soc Ser B, 2003, 67: 301-20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Ogutu J O, Schulz-Streeck T, Piepho H P. Genomic selection using regularized
    linear regression models: ridge regression, lasso, elastic net and their extensions[C]//BMC
    proceedings. BioMed Central, 2012, 6(2): 1-6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Ceperic E, Ceperic V, Baric A. A strategy for short-term load forecasting
    by support vector regression machines[J]. IEEE Transactions on Power Systems,
    2013, 28(4): 4356-4364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Angiulli G, Cacciola M, Versaci M. Microwave devices and antennas modelling
    by support vector regression machines[J]. IEEE Transactions on Magnetics, 2007,
    43(4): 1589-1592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Xu S, An X, Qiao X, et al. Multi-output least-squares support vector regression
    machines[J]. Pattern Recognition Letters, 2013, 34(9): 1078-1084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Segal M R. Machine learning benchmarks and random forest regression[J].
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Svetnik V, Liaw A, Tong C, et al. Random forest: a classification and
    regression tool for compound classification and QSAR modeling[J]. Journal of chemical
    information and computer sciences, 2003, 43(6): 1947-1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Cootes T F, Ionita M C, Lindner C, et al. Robust and accurate shape model
    fitting using random forest regression voting[C]//European Conference on Computer
    Vision. Springer, Berlin, Heidelberg, 2012: 278-291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] JÃ¶reskog K G, Goldberger A S. Factor analysis by generalized least squares[J].
    Psychometrika, 1972, 37(3): 243-260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Orsini N, Bellocco R, Greenland S. Generalized least squares for trend
    estimation of summarized doseâ€“response data[J]. The stata journal, 2006, 6(1):
    40-57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Browne M W. Generalized least squares estimators in the analysis of covariance
    structures[J]. South African statistical journal, 1974, 8(1): 1-24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Hao L, Naiman D Q, Naiman D Q. Quantile regression[M]. Sage, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Yu K, Lu Z, Stander J. Quantile regression: applications and current research
    areas[J]. Journal of the Royal Statistical Society: Series D (The Statistician),
    2003, 52(3): 331-350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Meinshausen N, Ridgeway G. Quantile regression forests[J]. Journal of
    Machine Learning Research, 2006, 7(6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Bishop C M, Tipping M E. Bayesian regression and classification[J]. Nato
    Science Series sub Series III Computer And Systems Sciences, 2003, 190: 267-288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Gelman A, Goodrich B, Gabry J, et al. R-squared for Bayesian regression
    models[J]. The American Statistician, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Koop G M. Bayesian econometrics[M]. John Wiley & Sons Inc., 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Yu K, Moyeed R A. Bayesian quantile regression[J]. Statistics & Probability
    Letters, 2001, 54(4): 437-447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Willett J B, Singer J D. Another cautionary note about R 2: Its use in
    weighted least-squares regression analysis[J]. The American Statistician, 1988,
    42(3): 236-238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Chang P T, Lee E S. A generalized fuzzy weighted least-squares regression[J].
    Fuzzy Sets and Systems, 1996, 82(3): 289-298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Blatman G, Sudret B. Adaptive sparse polynomial chaos expansion based
    on least angle regression[J]. Journal of computational Physics, 2011, 230(6):
    2345-2367.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Khan J A, Van Aelst S, Zamar R H. Robust linear model selection based
    on least angle regression[J]. Journal of the American Statistical Association,
    2007, 102(480): 1289-1299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Hesterberg T, Choi N H, Meier L, et al. Least angle and l1 penalized regression:
    A review[J]. Statistics Surveys, 2008, 2: 61-93.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Christensen R H B. ordinalâ€”regression models for ordinal data[J]. R package
    version, 2015, 28: 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Elith J, Leathwick J. Boosted Regression Trees for ecological modeling[J].
    R Documentation. Available online: https://cran. r-project. org/web/packages/dismo/vignettes/brt.
    pdf (accessed on 12 June 2011), 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Tyree S, Weinberger K Q, Agrawal K, et al. Parallel boosted regression
    trees for web search ranking[C]//Proceedings of the 20th international conference
    on World wide web. 2011: 387-396.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Choi S, Kim T, Yu W. Performance evaluation of RANSAC family[J]. Journal
    of Computer Vision, 1997, 24(3): 271-300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Derpanis K G. Overview of the RANSAC Algorithm[J]. Image Rochester NY,
    2010, 4(1): 2-3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Wilcox R. A note on the Theilâ€Sen regression estimator when the regressor
    is random and the error term is heteroscedastic[J]. Biometrical Journal: Journal
    of Mathematical Methods in Biosciences, 1998, 40(3): 261-268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Fernandes R, Leblanc S G. Parametric (modified least squares) and non-parametric
    (Theilâ€“Sen) linear regressions for predicting biophysical parameters in the presence
    of measurement errors[J]. Remote Sensing of Environment, 2005, 95(3): 303-316.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Sun Q, Zhou W X, Fan J. Adaptive huber regression[J]. Journal of the American
    Statistical Association, 2020, 115(529): 254-265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Fox J, Weisberg S. Robust regression[J]. An R and S-Plus companion to
    applied regression, 2002, 91.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] OstertagovÃ¡ E. Modelling using polynomial regression[J]. Procedia Engineering,
    2012, 48: 500-506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Theil H. A rank-invariant method of linear and polynomial regression analysis[M]//Henri
    Theilâ€™s contributions to economics and econometrics. Springer, Dordrecht, 1992:
    345-381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Bendel R B, Afifi A A. Comparison of stopping rules in forward â€œstepwiseâ€
    regression[J]. Journal of the American Statistical association, 1977, 72(357):
    46-53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Zheng B, Agresti A. Summarizing the predictive power of a generalized
    linear model[J]. Statistics in medicine, 2000, 19(13): 1771-1781.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Graybill F A. Theory and application of the linear model[M]. North Scituate,
    MA: Duxbury press, 1976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
