- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:04:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:04:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1909.10527] A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1909.10527] 一种用于从银河系红移调查中获得宇宙学约束的混合深度学习方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.10527](https://ar5iv.labs.arxiv.org/html/1909.10527)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1909.10527](https://ar5iv.labs.arxiv.org/html/1909.10527)
- en: A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一种用于从银河系红移调查中获得宇宙学约束的混合深度学习方法
- en: Michelle Ntampaka Harvard Data Science Initiative, Harvard University, Cambridge,
    MA 02138, USA Center for Astrophysics $|$ Harvard & Smithsonian, Cambridge, MA
    02138, USA Daniel J. Eisenstein Center for Astrophysics $|$ Harvard & Smithsonian,
    Cambridge, MA 02138, USA Sihan Yuan Center for Astrophysics $|$ Harvard & Smithsonian,
    Cambridge, MA 02138, USA Lehman H. Garrison Center for Astrophysics $|$ Harvard
    & Smithsonian, Cambridge, MA 02138, USA Center for Computational Astrophysics,
    Flatiron Institute, New York, NY 10010, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Michelle Ntampaka 哈佛数据科学计划，哈佛大学，马萨诸塞州剑桥市 02138，美国 天体物理中心 $|$ 哈佛与史密松学会，马萨诸塞州剑桥市
    02138，美国 Daniel J. Eisenstein 天体物理中心 $|$ 哈佛与史密松学会，马萨诸塞州剑桥市 02138，美国 Sihan Yuan
    天体物理中心 $|$ 哈佛与史密松学会，马萨诸塞州剑桥市 02138，美国 Lehman H. Garrison 天体物理中心 $|$ 哈佛与史密松学会，马萨诸塞州剑桥市
    02138，美国 计算天体物理中心，Flatiron研究所，纽约州纽约市 10010，美国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We present a deep machine learning (ML)-based technique for accurately determining
    $\sigma_{8}$ and $\Omega_{m}$ from mock 3D galaxy surveys. The mock surveys are
    built from the AbacusCosmos suite of $N$-body simulations, which comprises 40
    cosmological volume simulations spanning a range of cosmological models, and we
    account for uncertainties in galaxy formation scenarios through the use of generalized
    halo occupation distributions (HODs). We explore a trio of ML models: a 3D convolutional
    neural network (CNN), a power-spectrum-based fully connected network, and a hybrid
    approach that merges the two to combine physically motivated summary statistics
    with flexible CNNs. We describe best practices for training a deep model on a
    suite of matched-phase simulations and we test our model on a completely independent
    sample that uses previously unseen initial conditions, cosmological parameters,
    and HOD parameters. Despite the fact that the mock observations are quite small
    ($\sim 0.07h^{-3}\,\mathrm{Gpc}^{3}$) and the training data span a large parameter
    space (6 cosmological and 6 HOD parameters), the CNN and hybrid CNN can constrain
    $\sigma_{8}$ and $\Omega_{m}$ to $\sim 3\%$ and $\sim 4\%$, respectively.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于深度机器学习（ML）的技术，以准确确定$\sigma_{8}$和$\Omega_{m}$，该技术基于模拟的3D银河系调查。这些模拟调查是从AbacusCosmos套件的$N$-体模拟中构建的，该套件包含40个宇宙学体积模拟，涵盖了多种宇宙学模型，我们通过使用广义的晕占据分布（HODs）来考虑银河系形成情景的不确定性。我们探讨了三种ML模型：一个3D卷积神经网络（CNN）、一个基于功率谱的全连接网络，以及一个将这两者结合起来的混合方法，将物理动机的汇总统计与灵活的CNN相结合。我们描述了在一组匹配相位模拟上训练深度模型的最佳实践，并在完全独立的样本上测试了我们的模型，该样本使用了之前未见过的初始条件、宇宙学参数和HOD参数。尽管模拟观测数据相当小（$\sim
    0.07h^{-3}\,\mathrm{Gpc}^{3}$），且训练数据跨越了广泛的参数空间（6个宇宙学参数和6个HOD参数），CNN和混合CNN仍然能够将$\sigma_{8}$和$\Omega_{m}$的约束精度提高到$\sim
    3\%$和$\sim 4\%$。
- en: '^†^†journal: ApJ'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†期刊：ApJ
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In the $\Lambda$CDM cosmological model, tiny density fluctuations in the early
    Universe evolved into today’s cosmic web of overdense dark matter halos, filaments,
    and sheets. Imprinted on this large-scale structure is information about the underlying
    cosmological model, provided one knows how and where to look. Measurements that
    describe the large scale distribution of matter in the Universe carry information
    about the cosmological model that drove its formation. These measurements include
    descriptions of the spatial distribution and clustering of galaxies (e.g., Huchra
    et al., [1990](#bib.bib31); Shectman et al., [1996](#bib.bib63); Percival et al.,
    [2001](#bib.bib52); Tegmark et al., [2004](#bib.bib67)), the abundance of massive
    galaxy clusters (e.g., Vikhlinin et al., [2009](#bib.bib70); Mantz et al., [2015](#bib.bib43);
    de Haan et al., [2016](#bib.bib14)), the weak gravitational lensing of galaxies
    by intervening large-scale structure (e.g., Bacon et al., [2000](#bib.bib5); Kaiser
    et al., [2000](#bib.bib34); Wittman et al., [2000](#bib.bib74); Van Waerbeke et al.,
    [2000](#bib.bib69); DES Collaboration et al., [2017](#bib.bib16); Hildebrandt
    et al., [2018](#bib.bib29); Hikage et al., [2019](#bib.bib28)), and the length
    scale of baryon acoustic oscillations (e.g., Eisenstein et al., [2005](#bib.bib18);
    Cole et al., [2005](#bib.bib12); Alam et al., [2017](#bib.bib2)). A hallmark difference
    between these and probes of the earlier Universe is non-Gaussianity: though the
    early Universe is well-described by a Gaussian random field (e.g., Planck Collaboration
    et al., [2014a](#bib.bib54), [b](#bib.bib55)), gravitational collapse drives the
    formation of non-Gaussian correlations in the late-time matter distribution. See
    Weinberg et al. ([2013](#bib.bib73)) for a review of these and other observational
    cosmological probes.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $\Lambda$CDM 宇宙学模型中，早期宇宙中的微小密度波动演化为今天的宇宙网，其中包含过密的暗物质晕、细丝和层。印在这一大尺度结构上的信息涉及底层的宇宙学模型，只要知道如何和在哪里查找。这些描述宇宙中物质大尺度分布的测量包含有关驱动其形成的宇宙学模型的信息。这些测量包括星系的空间分布和聚类的描述（例如，Huchra
    et al., [1990](#bib.bib31)；Shectman et al., [1996](#bib.bib63)；Percival et al.,
    [2001](#bib.bib52)；Tegmark et al., [2004](#bib.bib67)）、大质量星系团的丰度（例如，Vikhlinin
    et al., [2009](#bib.bib70)；Mantz et al., [2015](#bib.bib43)；de Haan et al., [2016](#bib.bib14)）、星系的弱引力透镜效应（例如，Bacon
    et al., [2000](#bib.bib5)；Kaiser et al., [2000](#bib.bib34)；Wittman et al., [2000](#bib.bib74)；Van
    Waerbeke et al., [2000](#bib.bib69)；DES Collaboration et al., [2017](#bib.bib16)；Hildebrandt
    et al., [2018](#bib.bib29)；Hikage et al., [2019](#bib.bib28)）以及重子声学振荡的长度尺度（例如，Eisenstein
    et al., [2005](#bib.bib18)；Cole et al., [2005](#bib.bib12)；Alam et al., [2017](#bib.bib2)）。这些与早期宇宙探测的一个显著区别是非高斯性：尽管早期宇宙被高斯随机场很好地描述（例如，Planck
    Collaboration et al., [2014a](#bib.bib54)，[b](#bib.bib55)），但引力坍缩驱动了晚期物质分布中的非高斯相关的形成。有关这些及其他观测宇宙学探测的综述，请参见
    Weinberg et al. ([2013](#bib.bib73))。
- en: Galaxies live in dark matter halos and are tracers, albeit biased ones, of large-scale
    structure. Large spectroscopic surveys such as the Sloan Digital Sky Survey (SDSS,
    York et al., [2000](#bib.bib75)) have produced maps of the 3D distribution of
    galaxies in the Universe, and upcoming spectroscopic surveys such as the Dark
    Energy Spectroscopic Instrument (DESI, DESI Collaboration et al., [2016](#bib.bib17)),
    Subaru Prime Focus Spectrograph (PFS, Takada et al., [2014](#bib.bib66)), 4-metre
    Multi-Object Spectroscopic Telescope (4MOST, de Jong et al., [2014](#bib.bib15)),
    and Euclid (Amendola et al., [2013](#bib.bib3)) will produce exquisitely detailed
    maps of the sky. The galaxy power spectrum provides one handle on summarizing
    and interpreting these 3D galaxy maps and can be used to put constraints on the
    parameters that describe a $\Lambda$CDM cosmology (e.g., Tegmark et al., [2004](#bib.bib67)),
    but care must be taken when disentangling the effects of cosmology and galaxy
    bias (e.g., van den Bosch et al., [2013](#bib.bib68); More et al., [2013](#bib.bib45);
    Cacciato et al., [2013](#bib.bib10)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 星系存在于暗物质晕中，是大尺度结构的跟踪者，尽管存在偏差。大型光谱调查如斯隆数字天空调查（SDSS, York et al., [2000](#bib.bib75)）已经绘制了宇宙中星系的三维分布图，而即将进行的光谱调查如暗能量光谱仪（DESI,
    DESI Collaboration et al., [2016](#bib.bib17)）、 Subaru 主焦点光谱仪（PFS, Takada et al.,
    [2014](#bib.bib66)）、4米多目标光谱望远镜（4MOST, de Jong et al., [2014](#bib.bib15)）和欧几里得（Amendola
    et al., [2013](#bib.bib3)）将生成极其详细的天空地图。星系功率谱提供了一种总结和解释这些三维星系地图的工具，并可用于对描述 $\Lambda$CDM
    宇宙学的参数施加约束（例如，Tegmark et al., [2004](#bib.bib67)），但在解开宇宙学和星系偏差的影响时需要小心（例如，van
    den Bosch et al., [2013](#bib.bib68)；More et al., [2013](#bib.bib45)；Cacciato
    et al., [2013](#bib.bib10)）。
- en: Though it is an abundantly useful compression of the information contained in
    the distribution of galaxies, the power spectrum is not a complete accounting
    of this information because the late-time galaxy distribution is not a Gaussian
    random field. The deviations from Gaussian correlations are enormous at small
    length scales ($\lesssim$ a few Mpc), where dark matter halos have collapsed and
    virialized, but remain substantial at intermediate scales due to the cosmic web
    of filaments, walls, and voids. Additional statistics such as the squeezed 3-point
    correlation function (Yuan et al., [2018a](#bib.bib76)), redshift space power
    spectrum (Kobayashi et al., [2019](#bib.bib37)), counts-in-cylinders (Wang et al.,
    [2019](#bib.bib71)), and the minimum spanning tree (Naidoo et al., [2019](#bib.bib46))
    have been shown to be rich in complementary cosmological information by capturing
    non-Gaussian details of the galaxy distribution that are not described by the
    power spectrum alone.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管功率谱是对星系分布信息的极为有用的压缩，但它并没有完全反映这些信息，因为晚期星系分布并不是一个高斯随机场。在小长度尺度（$\lesssim$ 几个
    Mpc）上，暗物质晕已经崩溃并达到动力学平衡，导致高斯相关性偏差巨大，但在中等尺度上，由于宇宙网状结构的丝状物、墙面和空洞，这些偏差仍然显著。额外的统计量，例如压缩的三点相关函数（Yuan
    等，[2018a](#bib.bib76)）、红移空间功率谱（Kobayashi 等，[2019](#bib.bib37)）、圆柱体中的计数（Wang 等，[2019](#bib.bib71)）以及最小生成树（Naidoo
    等，[2019](#bib.bib46)），已被证明能丰富补充宇宙学信息，通过捕捉功率谱无法单独描述的非高斯星系分布细节。
- en: These higher-order statistical descriptions of how galaxies populate 3D space
    typically need to be calibrated on cosmological simulations. Cosmological hydrodynamical
    simulations that trace the formation of galaxies are computationally expensive,
    so a more tractable approach is to use less expensive $N$-body simulations that
    have been populated with galaxies. The can be accomplished through a technique
    that matches galaxies to the simulated structure of dark matter, for example,
    through a halo occupation distribution (HOD, e.g., Peacock & Smith, [2000](#bib.bib50);
    Scoccimarro et al., [2001](#bib.bib62); Berlind & Weinberg, [2002](#bib.bib9);
    Zheng et al., [2005](#bib.bib81)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关于星系如何在三维空间中分布的高阶统计描述通常需要通过宇宙学模拟来进行校准。追踪星系形成的宇宙学流体动力学模拟计算开销较大，因此更可行的方法是使用已经填充了星系的较为经济的`N`-体模拟。这可以通过一种匹配星系与模拟暗物质结构的技术来实现，例如通过晕占据分布（HOD，例如，Peacock
    & Smith，[2000](#bib.bib50)；Scoccimarro 等，[2001](#bib.bib62)；Berlind & Weinberg，[2002](#bib.bib9)；Zheng
    等，[2005](#bib.bib81)）。
- en: Under its simplest assumptions, an HOD uses halo mass as the sole property that
    determines whether a halo will host a particular type of galaxy. The breakdown
    of this assumption is known as galaxy assembly bias, which asserts that mass alone
    is insufficient and that additional environmental and assembly factors come into
    play. These factors include formation time (Gao et al., [2005](#bib.bib21)) and
    halo concentration (Wechsler et al., [2006](#bib.bib72)). Modern HOD implementations
    often provide flexibility to account for assembly bias (e.g., Hearin et al., [2016](#bib.bib27);
    Yuan et al., [2018b](#bib.bib77); Beltz-Mohrmann et al., [2019](#bib.bib7)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的假设下，HOD 使用晕质量作为唯一决定一个晕是否会容纳特定类型星系的属性。这个假设的破裂被称为星系组装偏差，它断言仅靠质量是不够的，还有额外的环境和组装因素发挥作用。这些因素包括形成时间（Gao
    等，[2005](#bib.bib21)）和晕浓度（Wechsler 等，[2006](#bib.bib72)）。现代 HOD 实现通常提供灵活性来考虑组装偏差（例如，Hearin
    等，[2016](#bib.bib27)；Yuan 等，[2018b](#bib.bib77)；Beltz-Mohrmann 等，[2019](#bib.bib7)）。
- en: Machine learning (ML) offers a number of methods that can find and extract information
    from complex spatial patterns imprinted on the 3D distribution of galaxies. ML,
    therefore, is an enticing approach for inferring cosmological models in spite
    of myriad complicating effects. One promising class of tools for this task are
    Convolutional Neural Networks (CNNs, e.g.  Fukushima & Miyake, [1982](#bib.bib20);
    LeCun et al., [1999](#bib.bib42); Krizhevsky et al., [2012](#bib.bib38); Simonyan
    & Zisserman, [2014](#bib.bib64)), which are often used in image recognition tasks.
    CNNs employ many hidden layers to extract image features such as edges, shapes,
    and textures. Typically, CNNs pair layers of convolution and pooling to extract
    meaningful features from the input images, followed by deep fully connected layers
    to output an image class or numerical label. Because these deep networks learn
    the filters necessary to extract meaningful information from the input images,
    they require very little image preprocessing. See Schmidhuber ([2014](#bib.bib61))
    for a review of deep neural networks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）提供了许多方法，可以从印在银河系 3D 分布上的复杂空间模式中发现和提取信息。因此，尽管存在众多复杂的影响因素，ML 仍然是推断宇宙学模型的一个诱人方法。针对这项任务的一类有前途的工具是卷积神经网络（CNNs，例如
    Fukushima & Miyake，[1982](#bib.bib20)；LeCun 等，[1999](#bib.bib42)；Krizhevsky 等，[2012](#bib.bib38)；Simonyan
    & Zisserman，[2014](#bib.bib64)），这些工具通常用于图像识别任务。CNN 使用多个隐藏层来提取图像特征，如边缘、形状和纹理。通常，CNN
    将卷积层和池化层配对以从输入图像中提取有意义的特征，然后通过深度全连接层输出图像类别或数值标签。由于这些深度网络学习提取有意义信息所需的滤波器，因此它们对图像预处理的需求非常少。有关深度神经网络的综述，请参见
    Schmidhuber（[2014](#bib.bib61)）。
- en: CNNs are traditionally applied to 2D images, which may be monochromatic or represented
    in several color bands. 2D CNNs can extract information from non-gaussianities
    in simulated convergence maps, remarkably improving cosmological constraints over
    a more standard statistical approach (e.g., Schmelzle et al., [2017](#bib.bib60);
    Gupta et al., [2018](#bib.bib25); Ribli et al., [2019a](#bib.bib58), [b](#bib.bib59)),
    and recent work has extended this to put cosmological constraints on observations
    using CNNs (Fluri et al., [2019](#bib.bib19)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 通常应用于 2D 图像，这些图像可能是单色的或以多种颜色带表示。2D CNN 可以从模拟的收敛图中提取非高斯性的信息，显著提高了与更标准统计方法相比的宇宙学约束（例如，Schmelzle
    等，[2017](#bib.bib60)；Gupta 等，[2018](#bib.bib25)；Ribli 等，[2019a](#bib.bib58)，[b](#bib.bib59)），最近的研究已经将这一点扩展到使用
    CNN 对观测结果施加宇宙学约束（Fluri 等，[2019](#bib.bib19)）。
- en: However, the application of CNNs is not limited to flat Euclidean images (e.g.
    Perraudin et al., [2019](#bib.bib53)), nor is it limited to two dimensions. The
    algorithm can be extended to three dimensions, where the third dimension may be,
    for example, temporal (e.g., video input, as in Ji et al., [2013](#bib.bib33))
    or spatial (e.g., a data cube, as in Kamnitsas et al., [2016](#bib.bib35)). Ravanbakhsh
    et al. ([2017](#bib.bib57)) employed the first cosmological application of a 3D
    CNN, showing that the tool can infer the underlying cosmological parameters from
    a simulated 3D dark matter distribution.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CNN 的应用不限于平面欧几里得图像（例如，Perraudin 等，[2019](#bib.bib53)），也不限于二维。该算法可以扩展到三维，其中第三维可能是，例如，时间维度（例如，视频输入，如
    Ji 等，[2013](#bib.bib33)）或空间维度（例如，数据立方体，如 Kamnitsas 等，[2016](#bib.bib35)）。Ravanbakhsh
    等（[2017](#bib.bib57)）首次应用了 3D CNN 的宇宙学应用，展示了该工具可以从模拟的 3D 暗物质分布中推断出潜在的宇宙学参数。
- en: 'We present an application of 3D CNNs to learn cosmological parameters from
    simulated galaxy maps. Our hybrid deep learning architecture learns directly from
    the calculated 2D power spectrum and simultaneously harnesses non-Gaussianities
    by also learning directly from the raw 3D distribution of galaxies. In Section
    [2](#S2 "2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"), we describe our mock observations:
    the suite of cosmological simulations ([2.1](#S2.SS1 "2.1 AbacusCosmos Simulations
    ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")), the range of HODs applied to these
    simulations ([2.2](#S2.SS2 "2.2 Halo Occupation Distribution ‣ 2 Methods: Mock
    Observations ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys")), the training and validation mock observations ([2.3](#S2.SS3
    "2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")),
    and the carefully constructed and independent test mock observations at the Planck
    cosmology ([2.4](#S2.SS4 "2.4 Planck Testing Set ‣ 2 Methods: Mock Observations
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")). We describe our trio of deep learning architectures, including the
    hybrid method, in Section [3](#S3 "3 Methods: Machine Learning Models ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").
    We present our results in Section [4](#S4 "4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") and a discussion
    and conclusions in Section [5](#S5 "5 Discussion & Conclusion ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").
    Appendix [A](#A1 "Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") is more pedagogical
    in nature; it describes how the range of model predictions evolves with training
    and suggests new tests for assessing a model’s fit.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了将3D CNNs应用于从模拟星系图中学习宇宙学参数的应用。我们的混合深度学习架构直接从计算得到的2D功率谱中学习，并同时通过直接从原始的3D星系分布中学习来利用非高斯性。在第[2](#S2
    "2 方法：模拟观测 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")部分中，我们描述了我们的模拟观测：宇宙学模拟的套件（[2.1](#S2.SS1
    "2.1 AbacusCosmos 模拟 ‣ 2 方法：模拟观测 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")），应用于这些模拟的HODs的范围（[2.2](#S2.SS2
    "2.2 暗物质晕体占据分布 ‣ 2 方法：模拟观测 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")），训练和验证模拟观测（[2.3](#S2.SS3
    "2.3 训练 & 验证集 ‣ 2 方法：模拟观测 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")），以及在普朗克宇宙学中精心构建和独立的测试模拟观测（[2.4](#S2.SS4
    "2.4 普朗克测试集 ‣ 2 方法：模拟观测 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")）。我们在第[3](#S3 "3 方法：机器学习模型
    ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")部分中介绍了我们的三种深度学习架构，包括混合方法。我们在第[4](#S4 "4 结果 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")部分展示了我们的结果，并在第[5](#S5
    "5 讨论 & 结论 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")部分进行讨论和结论。附录[A](#A1 "附录 A 关于CNN的生命周期 ‣ 从星系红移调查中的宇宙学约束的混合深度学习方法")更具有教学性质；它描述了模型预测范围随着训练的演变，提出了评估模型拟合度的新测试。
- en: '2 Methods: Mock Observations'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法：模拟观测
- en: 'We use the AbacusCosmos suite of simulations¹¹1[https://lgarrison.github.io/AbacusCosmos/](https://lgarrison.github.io/AbacusCosmos/)
    (Garrison et al., [2018](#bib.bib22), [2019](#bib.bib23)) to create three data
    sets: a training set, a validation set, and a testing set. The training set is
    used to fit the machine learning model; it spans a range of CDM cosmologies and
    is populated with galaxies in a way to mimic a variety of galaxy formation models.
    The validation set is used to assess how well the machine learning model has fit;
    it also spans a range of cosmological parameters and galaxy formation models.
    The testing set is independent of both the training and validation sets; it is
    at the Planck fiducial cosmology (Planck Collaboration et al., [2015](#bib.bib56)),
    built from simulations with initial conditions not used in the training or validation
    data sets, and populated with galaxies using HODs not used in the training or
    testing data sets. The creation of the three data sets are described in the following
    subsections.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 AbacusCosmos 模拟系列¹¹1[https://lgarrison.github.io/AbacusCosmos/](https://lgarrison.github.io/AbacusCosmos/)（Garrison
    等， [2018](#bib.bib22)， [2019](#bib.bib23)）来创建三个数据集：训练集、验证集和测试集。训练集用于拟合机器学习模型；它涵盖了各种
    CDM 宇宙学，并以模拟各种星系形成模型的方式填充了星系。验证集用于评估机器学习模型的拟合效果；它也涵盖了多种宇宙学参数和星系形成模型。测试集独立于训练集和验证集；它基于
    Planck 基准宇宙学（Planck Collaboration 等， [2015](#bib.bib56)），由未在训练或验证数据集中使用的初始条件的模拟构建，并使用未在训练或测试数据集中使用的
    HODs 填充星系。三个数据集的创建过程在以下小节中进行了描述。
- en: 2.1 AbacusCosmos Simulations
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 AbacusCosmos 模拟
- en: The AbacusCosmos simulations are a suite of publicly available $N$-body simulations.
    The suite includes the AbacusCosmos 1100box simulations, a sample of large-volume
    $N$-body simulations at a variety of cosmologies, as well as the 1100box Planck
    simulations, a sample of simulations with cosmological parameters consistent with
    the Planck fiducial cosmology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: AbacusCosmos 模拟是一套公开可用的 $N$-体模拟。该系列包括 AbacusCosmos 1100box 模拟，这是一个在多种宇宙学下的大体积
    $N$-体模拟样本，以及 1100box Planck 模拟，这是一个与 Planck 基准宇宙学一致的宇宙学参数模拟样本。
- en: 'The AbacusCosmos 1100box simulations are used to create the training and validation
    sets. This suite of simulations comprises 40 simulations at a variety of cosmologies
    that differ for six cosmological parameters: $\Omega_{CDM}\,h^{2}$, $\Omega_{b}\,h^{2}$,
    $\sigma_{8}$, $H_{0}$, $w_{0}$, and $n_{s}$. The cosmologies for this suite of
    simulations were selected by a Latin hypercube algorithm, and are centered on
    the Planck fiducial cosmology (Planck Collaboration et al., [2015](#bib.bib56)).
    Each simulation has side length $1100h^{-1}\,\mathrm{Mpc}$ and particle mass $4\times
    10^{10}h^{-1}\,\mathrm{M_{\odot}}$. The suite of 40 simulations are phase-matched.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AbacusCosmos 1100box 模拟用于创建训练集和验证集。该模拟系列包括 40 个模拟，涵盖了不同的宇宙学，这些宇宙学在六个宇宙学参数上有所不同：$\Omega_{CDM}\,h^{2}$、$\Omega_{b}\,h^{2}$、$\sigma_{8}$、$H_{0}$、$w_{0}$
    和 $n_{s}$。该系列模拟的宇宙学是通过拉丁超立方体算法选择的，并且以 Planck 基准宇宙学（Planck Collaboration 等， [2015](#bib.bib56)）为中心。每个模拟的边长为
    $1100h^{-1}\,\mathrm{Mpc}$，粒子质量为 $4\times 10^{10}h^{-1}\,\mathrm{M_{\odot}}$。这
    40 个模拟系列是相位匹配的。
- en: 'While the AbacusCosmos 1100box simulations are used to create the training
    and validation sets, the AbacusCosmos Planck simulations are used to create the
    testing set. These 20 simulations have cosmological parameters consistent with
    Planck Collaboration et al. ([2015](#bib.bib56)): $\Omega_{b}\,h^{2}=0.02222$,
    $\Omega_{m}\,h^{2}=0.14212$, $w_{0}=-1$, $n_{s}=0.9652$, $\sigma_{8}=0.830$, $H_{0}=67.26$,
    $N_{\mathrm{eff}}=3.04$. They have identical side length ($1100h^{-1}\,\mathrm{Mpc}$)
    and particle mass ($4\times 10^{10}h^{-1}\,\mathrm{M_{\odot}}$) to the 1100box
    suite of simulations, but each uses unique initial conditions and none are phase-matched
    to the 1100box simulations. See Garrison et al. ([2018](#bib.bib22)) for more
    details about the AbacusCosmos suite of simulations.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AbacusCosmos 1100box 模拟用于创建训练集和验证集，但 AbacusCosmos Planck 模拟用于创建测试集。这 20 个模拟具有与
    Planck Collaboration 等（[2015](#bib.bib56)）一致的宇宙学参数：$\Omega_{b}\,h^{2}=0.02222$，$\Omega_{m}\,h^{2}=0.14212$，$w_{0}=-1$，$n_{s}=0.9652$，$\sigma_{8}=0.830$，$H_{0}=67.26$，$N_{\mathrm{eff}}=3.04$。它们具有与
    1100box 系列模拟相同的边长（$1100h^{-1}\,\mathrm{Mpc}$）和粒子质量（$4\times 10^{10}h^{-1}\,\mathrm{M_{\odot}}$），但每个模拟都使用了独特的初始条件，且没有与
    1100box 模拟相位匹配。有关 AbacusCosmos 模拟系列的更多细节，请参见 Garrison 等（[2018](#bib.bib22)）。
- en: 2.2 Halo Occupation Distribution
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 星系占据分布
- en: A halo occupation distribution (HOD) is a way to populate dark matter halos
    with galaxies. In their most basic form, HODs are probabilisitic models that assume
    that halo mass is the sole halo property governing the halo-galaxy connection
    (Berlind & Weinberg, [2002](#bib.bib9)). A standard HOD models the probability
    of a halo hosting a central galaxy, $\overline{n}_{\mathrm{central}}$, and the
    mean number of satellites, $\overline{n}_{\mathrm{satellite}}$, as a function
    of a single halo property, the mass $M$. The standard HOD by Zheng & Weinberg
    ([2007](#bib.bib80)) gives the mean number of central and satellite galaxies as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 暗物质晕的占据分布（HOD）是一种用来将暗物质晕中填充星系的方法。在其最基本的形式中，HOD 是概率模型，它们假设晕质量是唯一决定晕-星系连接的晕属性（Berlind
    & Weinberg，[2002](#bib.bib9)）。标准 HOD 模型将晕中包含中心星系的概率 $\overline{n}_{\mathrm{central}}$
    和卫星星系的平均数量 $\overline{n}_{\mathrm{satellite}}$ 作为单一晕属性，即质量 $M$ 的函数。Zheng & Weinberg（[2007](#bib.bib80)）的标准
    HOD 给出了中心星系和卫星星系的平均数量：
- en: '|  | $\begin{split}\overline{n}_{\mathrm{central}}&amp;=\frac{1}{2}\mathrm{erfc}\left[\frac{\ln(M_{\mathrm{cut}}/M)}{\sqrt{2}\sigma}\right]\\[12.91663pt]
    \overline{n}_{\mathrm{satellite}}&amp;=\left[\frac{M-\kappa M_{\mathrm{cut}}}{M_{1}}\right]^{\alpha}\overline{n}_{\mathrm{central}},\end{split}$
    |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\overline{n}_{\mathrm{central}}&amp;=\frac{1}{2}\mathrm{erfc}\left[\frac{\ln(M_{\mathrm{cut}}/M)}{\sqrt{2}\sigma}\right]\\[12.91663pt]
    \overline{n}_{\mathrm{satellite}}&amp;=\left[\frac{M-\kappa M_{\mathrm{cut}}}{M_{1}}\right]^{\alpha}\overline{n}_{\mathrm{central}},\end{split}$
    |  | (1) |'
- en: where $M_{\mathrm{cut}}$ sets the halo mass scale for central galaxies, $\sigma$
    sets the width of the error function of $\overline{n}_{\mathrm{central}}$, $M_{1}$
    sets the mass scale for satellite galaxies, $\alpha$ sets the slope of the power
    law, and $\kappa M_{\mathrm{cut}}$ sets the limit below which a halo cannot host
    a satellite galaxy. $M$ denotes the halo mass, and we use the virial mass definition
    $M_{vir}$. The actual number of central galaxies in a halo follows the Bernoulli
    distribution with the mean set to $\overline{n}_{\mathrm{central}}$, whereas the
    number of satellite galaxies follows the Poisson distributions with the mean set
    to $\overline{n}_{\mathrm{satellite}}$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{\mathrm{cut}}$ 设置了中心星系的晕质量尺度，$\sigma$ 设置了 $\overline{n}_{\mathrm{central}}$
    的误差函数宽度，$M_{1}$ 设置了卫星星系的质量尺度，$\alpha$ 设置了幂律的斜率，而 $\kappa M_{\mathrm{cut}}$ 设置了晕无法容纳卫星星系的下限。$M$
    表示晕质量，我们使用的是质心质量定义 $M_{vir}$。晕中中心星系的实际数量遵循伯努利分布，其均值为 $\overline{n}_{\mathrm{central}}$，而卫星星系的数量遵循泊松分布，其均值为
    $\overline{n}_{\mathrm{satellite}}$。
- en: 'While this standard HOD populates halos probabilistically according to halo
    mass, recent variations of the HOD incorporate more flexibility in modeling. These
    flexible HODs allow additional halo properties — beyond the halo mass — to inform
    galaxy occupation (e.g., Hearin et al., [2016](#bib.bib27); Yuan et al., [2018b](#bib.bib77)).
    The HOD implemented here is one such flexible model; it uses the publicly available
    GRAND-HOD package²²2[https://github.com/SandyYuan/GRAND-HOD](https://github.com/SandyYuan/GRAND-HOD).
    This HOD implementation introduces a series of extensions to the standard HOD,
    including flexibility in the distribution of satellite galaxies within the halo,
    velocity distribution of the galaxies, and galaxy assembly bias. To add this flexibility,
    we invoke two extensions: the satellite distribution parameter, $s$, and the galaxy
    assembly bias parameter, $A$. The satellite distribution parameter allows for
    a flexible radial distribution of satellite galaxies within a dark matter halo,
    and the galaxy assembly bias parameter allows for a secondary HOD dependence on
    halo concentration. For complete information about GRAND-HOD and its HOD extensions,
    see Yuan et al. ([2018a](#bib.bib76)).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个标准 HOD 根据晕质量以概率方式填充晕，但最近的 HOD 变体在建模中引入了更多的灵活性。这些灵活的 HOD 允许除了晕质量之外的额外晕属性来影响星系占据（例如，Hearin
    等人，[2016](#bib.bib27)；Yuan 等人，[2018b](#bib.bib77)）。这里实施的 HOD 就是一个这样的灵活模型；它使用了公开的
    GRAND-HOD 包²²2[https://github.com/SandyYuan/GRAND-HOD](https://github.com/SandyYuan/GRAND-HOD)。这个
    HOD 实现对标准 HOD 进行了系列扩展，包括晕内卫星星系的分布灵活性、星系的速度分布和星系组装偏差。为了增加这种灵活性，我们引入了两个扩展：卫星分布参数
    $s$ 和星系组装偏差参数 $A$。卫星分布参数允许暗物质晕内卫星星系的径向分布具有灵活性，而星系组装偏差参数允许 HOD 对晕浓度具有次要依赖。有关 GRAND-HOD
    及其 HOD 扩展的完整信息，请参见 Yuan 等人（[2018a](#bib.bib76)）。
- en: Fifteen sets of HOD model parameters are generated for each AbacusCosmos simulation
    box, and 31 are generated for each Planck box. For each simulation box, a baseline
    HOD model is selected as a function of cosmology; these baseline models vary only
    in $M_{\mathrm{cut}}$ and $M_{1}$, and baseline values of all the other HOD parameters
    remain the same. This ensures that the combined effect of perturbing the cosmology
    and HOD is mild. This is done because, despite the fact that the cosmological
    parameters of each simulation are only perturbed by a few percent, coupling these
    cosmological changes with perturbations to the HOD can lead to drastic changes
    to the mock catalogs and the clustering statistics. To minimize these effects,
    instead of populating galaxies according to HOD parameters in an ellipse aligned
    with the default parameter basis, we populate according to parameter in an ellipse
    defined over a custom parameter basis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 AbacusCosmos 模拟箱，生成了十五组 HOD 模型参数，对于每个 Planck 箱，生成了 31 组。对于每个模拟箱，根据宇宙学选择一个基线
    HOD 模型；这些基线模型仅在 $M_{\mathrm{cut}}$ 和 $M_{1}$ 上有所不同，其他所有 HOD 参数的基线值保持不变。这确保了扰动宇宙学和
    HOD 的综合效应是轻微的。这是因为，尽管每个模拟的宇宙学参数仅被扰动了几个百分点，但将这些宇宙学变化与 HOD 的扰动结合起来可能会导致模拟目录和聚类统计的剧烈变化。为了最小化这些影响，我们不是根据与默认参数基础对齐的椭圆中的
    HOD 参数来填充星系，而是根据在自定义参数基础上定义的椭圆中的参数进行填充。
- en: '| ![Refer to caption](img/9139db13eca03bc63342d1becf75433e.png) | ![Refer to
    caption](img/aad80b94ce21d1cf873eddb2eebebf76.png) | ![Refer to caption](img/b6bfce89985b2ae3911bbdc66580f69b.png)
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/9139db13eca03bc63342d1becf75433e.png) | ![参考说明](img/aad80b94ce21d1cf873eddb2eebebf76.png)
    | ![参考说明](img/b6bfce89985b2ae3911bbdc66580f69b.png) |'
- en: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
- en: '| ![Refer to caption](img/fd0cb0ba31d819de74b42140320b102e.png) | ![Refer to
    caption](img/5f2df0f833d24a38587fd7c18aead440.png) | ![Refer to caption](img/96248bc965edd61d4003d13d39511705.png)
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/fd0cb0ba31d819de74b42140320b102e.png) | ![参考说明](img/5f2df0f833d24a38587fd7c18aead440.png)
    | ![参考说明](img/96248bc965edd61d4003d13d39511705.png) |'
- en: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
- en: 'Figure 1: Top: A sample of train input images. Shown is a two-dimensional projection
    of the three-dimensional image (or “slab”). The train, validate, and test samples
    include a number of choices designed to reduce the likelihood of giving the machine
    learning model an unfair advantage: we employ a zero-point shift to minimize learning
    from images with correlated structure, we use random HODs and seeds to allow for
    uncertainties in galaxy formation physics, we use axial flips of the slabs to
    augment the data, and we use unique portions of the simulation and unique HODs
    in the validation set to provide a way to test that the model does not rely on
    the particulars of the structure or HOD. To highlight the differences in the images
    that are strictly due to cosmology and HOD, the zero-point shift has been omitted
    for these images. Bottom: The same images as above, smoothed with a Gaussian filter
    ($\sigma=1\,\mathrm{pixel}$) to emphasize the differences between images that
    are due to cosmological models.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：上部：一个训练输入图像的样本。展示的是三维图像（或“板材”）的二维投影。训练、验证和测试样本包括了多种选择，旨在减少给机器学习模型带来不公平优势的可能性：我们采用零点偏移以最小化从具有相关结构的图像中学习的情况，使用随机
    HOD 和种子以应对星系形成物理中的不确定性，我们使用板材的轴向翻转来增强数据，并且在验证集中使用模拟的独特部分和独特的 HOD，以提供一种测试模型是否依赖于结构或
    HOD 特性的方式。为了突出图像中严格由于宇宙学和 HOD 引起的差异，这些图像省略了零点偏移。下部：上述相同图像，通过高斯滤波器（$\sigma=1\,\mathrm{pixel}$）平滑，以强调由于宇宙学模型导致的图像差异。
- en: 'For the Planck cosmology, the HOD parameters are chosen in reference to the
    parameter ranges in Kwan et al. ([2015](#bib.bib39)): $\log_{10}(M_{\mathrm{cut}}/h^{-1}\,\mathrm{M_{\odot}})=13.35$,
    $\log_{10}(M_{1}/h^{-1}\,\mathrm{M_{\odot}})=13.8$, $\sigma=0.85$, $\alpha=1$,
    $\kappa=1$, $s=0$, and $A=0$. However, we modify two baseline HOD parameter values
    — $M_{\mathrm{cut}}$ and $M_{1}$ — for the non-Planck simulations. We set the
    baseline value of $M_{\mathrm{cut}}$ in each cosmology box such that the projected
    2-point correlation function $w_{p}(5-10\rm{Mpc})$ of all the halos with $M>M_{\mathrm{cut}}$
    is equal to the $w_{p}(5-10\rm{Mpc})$ of the centrals in the baseline HOD at Planck
    cosmology, where $w_{p}(5-10\rm{Mpc})$ is defined as'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Planck 宇宙学，HOD 参数是参考 Kwan 等人 ([2015](#bib.bib39)) 中的参数范围选择的：$\log_{10}(M_{\mathrm{cut}}/h^{-1}\,\mathrm{M_{\odot}})=13.35$，$\log_{10}(M_{1}/h^{-1}\,\mathrm{M_{\odot}})=13.8$，$\sigma=0.85$，$\alpha=1$，$\kappa=1$，$s=0$
    和 $A=0$。然而，我们对非 Planck 仿真的两个基线 HOD 参数值——$M_{\mathrm{cut}}$ 和 $M_{1}$——进行了修改。我们将每个宇宙学盒子中
    $M_{\mathrm{cut}}$ 的基线值设置为，使得所有 $M>M_{\mathrm{cut}}$ 的光晕的投影 2 点相关函数 $w_{p}(5-10\rm{Mpc})$
    等于基线 HOD 在 Planck 宇宙学中的 $w_{p}(5-10\rm{Mpc})$，其中 $w_{p}(5-10\rm{Mpc})$ 定义为
- en: '|  | $w_{p}(5-10\textrm{Mpc})=\int_{5\textrm{Mpc}}^{10\textrm{Mpc}}w_{p}d(r_{\perp}).$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{p}(5-10\textrm{Mpc})=\int_{5\textrm{Mpc}}^{10\textrm{Mpc}}w_{p}d(r_{\perp}).$
    |  | (2) |'
- en: This effectively holds the baseline $w_{p}$ of the centrals approximately constant
    across all the cosmology boxes. Then $M_{1}$ is selected such that the baseline
    satellite-central fraction in each cosmology box is the same as that of the baseline
    HOD in Planck cosmology.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这有效地保持了中央星系的基线 $w_{p}$ 在所有宇宙学盒子中大致恒定。然后选择 $M_{1}$ 使得每个宇宙学盒子中的基线卫星-中央比例与 Planck
    宇宙学中的基线 HOD 相同。
- en: For each 1100box, seven additional pairs of model parameters uniformly sample
    the parameter space within $5\%$ of the baseline HOD (15 additional pairs for
    each Planck box). For HOD parameters $s$ and $A$, whose baseline parameters are
    0, we draw uniform samples between $-0.05$ and $0.05$. The two HODs of each pair
    are symmetrically offset across the baseline HOD. Excluding the baseline HOD,
    fourteen unique HODs are generated for each AbacusCosmos 1100box simulation, and
    30 unique HODs are generated for each Planck simulation. Four random seeds are
    used to populate the simulations with realizations of galaxies according to the
    HOD; this results in four unique galaxy catalogs for each HOD. The details of
    how these are used are described in the next section. For complete information
    about the HOD implementation, see Yuan et al. ([2019](#bib.bib78)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 1100box，我们在基线 HOD 的 $5\%$ 范围内均匀采样七对额外的模型参数（每个 Planck box 额外采样 15 对）。对于
    HOD 参数 $s$ 和 $A$，基线参数为 0，我们在 $-0.05$ 和 $0.05$ 之间绘制均匀样本。每对 HOD 在基线 HOD 的两侧对称偏移。排除基线
    HOD，每个 AbacusCosmos 1100box 仿真生成十四个独特的 HOD，而每个 Planck 仿真生成三十个独特的 HOD。使用四个随机种子根据
    HOD 填充仿真，这样每个 HOD 生成四个独特的星系目录。如何使用这些细节将在下一节中描述。有关 HOD 实现的完整信息，请参见 Yuan 等人 ([2019](#bib.bib78))。
- en: 2.3 Training & Validation Sets
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 训练与验证集
- en: The training sample of mock observations (for training the deep learning models)
    and validation sample of mock observations (for assessing when the models have
    sufficiently fit) are created from the AbacusCosmos suite of 1100box simulations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟观测的训练样本（用于训练深度学习模型）和模拟观测的验证样本（用于评估模型是否足够拟合）是从 AbacusCosmos 的 1100box 仿真中创建的。
- en: AbacusCosmos includes 40 simulated cosmologies, and for each of these, we select
    a random distance along the $x$ and $y$ axes to become the new 0-point of the
    box ($z=0$, along the line of sight direction, which includes redshift space distortion,
    remains unchanged). Because the 1100box simulations all have the same initial
    conditions, this random reshuffling minimizes the chances of our model learning
    about correlated structure across simulations.³³3Simulations with matched initial
    conditions will produce portions of the cosmic web with, for example, a unique
    or unusual fingerprint of filamentary structure. The evolutionary stage of a particular
    structure is highly dependent on the simulation’s $\sigma_{8}$ and other cosmological
    parameters. Because CNNs are particularly adept at pattern finding, care must
    be taken to prevent a CNN from learning to identify some unique structure — especially
    one which is particular to a suite of simulations and the initial conditions of
    those simulations — and infer cosmological parameters from its details. This is
    not an approach that will generalize to real observations, and can give overly
    optimistic results. The mock observations of the training set are built from the
    portion of the box with $220h^{-1}\,\mathrm{Mpc}\leq z<1100h^{-1}\,\mathrm{Mpc}$,
    while the validation set is built from the structure in the range $0h^{-1}\,\mathrm{Mpc}\leq
    z<220h^{-1}\,\mathrm{Mpc}$. By completely excluding this portion of the simulation
    from the training set, we can test and ensure that the machine learning model
    does not rely on its ability to identify or memorize large-scale structure correlations
    stemming from the matched initial conditions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AbacusCosmos包含40种模拟宇宙学，对于每种模拟，我们沿$x$和$y$轴选择一个随机距离，作为盒子的新的0点（$z=0$，视线方向上的红移空间扭曲保持不变）。由于1100box模拟具有相同的初始条件，这种随机重排最小化了模型学习跨模拟的相关结构的可能性。³³3具有匹配初始条件的模拟将产生例如具有独特或不寻常的丝状结构指纹的宇宙网部分。特定结构的演化阶段高度依赖于模拟的$\sigma_{8}$和其他宇宙学参数。由于卷积神经网络（CNN）特别擅长模式识别，因此必须小心防止CNN学习识别某种独特结构——尤其是特定于一组模拟及其初始条件的结构——并从其细节推断宇宙学参数。这种方法不会推广到实际观察中，可能会给出过于乐观的结果。训练集的虚拟观察数据构建自$220h^{-1}\,\mathrm{Mpc}\leq
    z<1100h^{-1}\,\mathrm{Mpc}$范围内的盒子部分，而验证集则构建自$0h^{-1}\,\mathrm{Mpc}\leq z<220h^{-1}\,\mathrm{Mpc}$范围内的结构。通过完全排除模拟的这一部分，能够测试并确保机器学习模型不依赖于识别或记忆源自匹配初始条件的大规模结构相关性。
- en: The box is divided into 20 non-overlapping slabs, which are $550h^{-1}\,\mathrm{Mpc}$
    in the $x$ and $y$ directions and $220h^{-1}\,\mathrm{Mpc}$ along the line of
    sight $z$ direction. Halo catalogs generated by the ROCKSTAR halo finder (Behroozi
    et al., [2012](#bib.bib6)) become the basis for four mock observations per slab.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 盒子被划分为20个不重叠的板块，每个板块在$x$和$y$方向上为$550h^{-1}\,\mathrm{Mpc}$，在视线$z$方向上为$220h^{-1}\,\mathrm{Mpc}$。由ROCKSTAR光晕探测器（Behroozi等，[2012](#bib.bib6)）生成的光晕目录成为每个板块四个虚拟观察的基础。
- en: For each slab, we select and apply one HOD from the 15 that are available. Eleven
    of the HODs are reused as necessary in the 16 training slabs. The remaining four
    HODs are reserved exclusively for the four validation slabs. By setting aside
    four HODs for the validation set, the validation set is populated with galaxies
    in a way that is unique from the observations used for training, and we can ensure
    that the ML model results are not dependent on memorization or previous knowledge
    of the details of the HOD.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个板块，我们从15个可用的HOD中选择并应用一个。11个HOD会在16个训练板块中根据需要重复使用。剩下的四个HOD专门保留给四个验证板块。通过为验证集保留四个HOD，验证集中的星系与用于训练的观察数据有所不同，从而确保机器学习模型的结果不依赖于记忆或对HOD细节的先前了解。
- en: For each of the four random HOD seeds, the slabs are populated with galaxies.
    These training slabs vary in the number of galaxies, ranging from $\sim 17000$
    to $\sim 46000$ galaxies per slab, with the number of galaxies correlating weakly
    with the underlying cosmology. To prevent the CNN from learning correlations between
    cosmological parameters and the number of galaxies in the mock observation, we
    randomly subselect the galaxy population so that all observations have $15000$
    galaxies.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于四个随机 HOD 种子中的每一个，板上都填充了星系。这些训练板上星系数量不等，范围从 $\sim 17000$ 到 $\sim 46000$ 个星系，星系数量与基础宇宙学的相关性较弱。为了防止
    CNN 学习宇宙学参数与模拟观测中星系数量之间的相关性，我们随机选择星系种群，以确保所有观测均有 $15000$ 个星系。
- en: 'The selected galaxies are binned into a $275\times 275\times 55$, three-dimensional,
    single-color image. Galaxies are assigned to voxels using a triangular shaped
    cloud (TSC) and $2\times 2\times 5h^{-1}\,\mathrm{Mpc}$ voxels. Projected galaxy
    densities for three sample cosmologies are shown in Figure [1](#S2.F1 "Figure
    1 ‣ 2.2 Halo Occupation Distribution ‣ 2 Methods: Mock Observations ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '选择的星系被分成一个 $275\times 275\times 55$ 的三维单色图像。使用三角形云（TSC）和 $2\times 2\times 5h^{-1}\,\mathrm{Mpc}$
    体素将星系分配到体素中。三个样本宇宙学的星系密度投影如图 [1](#S2.F1 "图 1 ‣ 2.2 Halo Occupation Distribution
    ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") 所示。'
- en: 'Because the machine learning model described in Section [3](#S3 "3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") is not invariant under mirroring of images, we
    augment our data by applying an axial flip along the $x$- and/or $y$-directions
    to three of the four slabs. For each of these three mirror images, we use a new
    random seed for the HOD and uniquely subselect to 15000 galaxies.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '由于第 [3](#S3 "3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys") 节中描述的机器学习模型在镜像图像时不保持不变，因此我们通过在
    $x$-和/或 $y$-方向上对四个板中的三个进行轴向翻转来扩充数据。对于这三个镜像图像中的每一个，我们使用新的随机种子进行 HOD，并唯一选择 15000
    个星系。'
- en: 'The power spectrum of the galaxy density field is computed for each slab. To
    perform this calculation, we pad the galaxy density field with zeros to double
    the image size in each direction to account for the lost periodic boundary conditions,
    Fourier transform the resulting $550\times 550\times 110$ image, and convert the
    result to a power spectrum in physical units. This 3-dimensional power spectrum
    is next de-convolved to account for the TSC-aliased window function (as in, e.g.,
    Jeong, [2010](#bib.bib32)), and summarized as a 1-dimensional power spectrum by
    averaging the power in binned spherical annuli. Due to the anisotropic nature
    of the slab and voxel dimensions, the most conservative choices for minimum and
    maximum $k$ values are selected. These are set by the shortest box dimension ($220h^{-1}\,\mathrm{Mpc}$)
    and the Nyquist frequency of the largest pixel dimension ($5h^{-1}\,\mathrm{Mpc}$),
    respectively. Power spectra for a sample of galaxy catalogs are shown in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '计算每个板的星系密度场的功率谱。为了进行这一计算，我们用零填充星系密度场，将图像大小在每个方向上加倍，以补偿丢失的周期性边界条件，对结果 $550\times
    550\times 110$ 图像进行傅里叶变换，并将结果转换为物理单位的功率谱。接下来对这个三维功率谱进行去卷积，以考虑 TSC 伪影窗口函数（如 Jeong，[2010](#bib.bib32)
    所示），并通过在分 bin 的球面环中平均功率来总结为一维功率谱。由于板和体素尺寸的各向异性，选择了最保守的最小和最大 $k$ 值。这些值分别由最短的盒子尺寸
    ($220h^{-1}\,\mathrm{Mpc}$) 和最大像素尺寸的 Nyquist 频率 ($5h^{-1}\,\mathrm{Mpc}$) 确定。样本星系目录的功率谱如图
    [2](#S2.F2 "图 2 ‣ 2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys") 所示。'
- en: '![Refer to caption](img/6b8ecb6c35b8a90ce1e6f080f0c82db7.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6b8ecb6c35b8a90ce1e6f080f0c82db7.png)'
- en: 'Figure 2: Mean galaxy power spectra, $P(k)$, for 4 of the 40 training cosmologies
    (yellow, orange, purple, and blue) as well as for the Planck test cosmology (pink).
    Points indicate the mean power, while error bars show the middle 68% of the mock
    observations. The “Vector Features” input, shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"), is a single
    realization of this power spectrum; for each mock observation, the power spectrum
    is calculated directly from a single 3D mock galaxy observation.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：4个训练宇宙模型（黄色、橙色、紫色和蓝色）以及普朗克测试宇宙模型（粉色）的平均星系功率谱$P(k)$。点表示平均功率，而误差条显示模拟观测的中间68%。图中的“向量特征”输入，是这个功率谱的单次实现；对于每个模拟观测，功率谱直接从一个三维模拟星系观测中计算。
- en: 'To recap, the method for building mock observations from each of the simulations
    is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，从每个模拟中构建模拟观测的方法如下：
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A random $x$ and $y$ value is selected to be the new 0-point of the box. $z=0$,
    along the line of sight direction with redshift space distortion, remains unchanged.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机选择一个$x$和$y$值作为盒子的新的0点。$z=0$，在视线方向上带有红移空间畸变，保持不变。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The box is divided into 20 non-overlapping slabs, each $550\times 550\times
    220h^{-1}\,\mathrm{Mpc}$.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 盒子被分成20个不重叠的板块，每个板块的尺寸为$550\times 550\times 220h^{-1}\,\mathrm{Mpc}$。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'For each slab:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个板块：
- en: –
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: An HOD is selected. Eleven HODs, some of which are reused as necessary, are
    used to populate the 16 training slabs with galaxies. Four unique HODs are reserved
    exclusively for the four validation slabs.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择一个HOD。使用十一种HOD（根据需要重用一些）来填充16个训练板块中的星系。四种独特的HOD专门保留用于四个验证板块。
- en: –
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 15000 galaxies are randomly selected. These are binned in $2\times 2\times 5h^{-1}\,\mathrm{Mpc}$
    bins using a TSC.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机选择15000个星系。这些星系被使用TSC方法在$2\times 2\times 5h^{-1}\,\mathrm{Mpc}$的箱子中分 bin。
- en: –
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The previous step is repeated for each of 4 random seeds, incorporating mirror
    image(s) of the slab.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前一步骤在4个随机种子中重复进行，包含了板块的镜像。
- en: –
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: The power spectrum of the slab is calculated.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算板块的功率谱。
- en: This method results in 3200 mock observations built from 40 simulations, with
    20 slabs per simulation and 4 seeds (with axial flips) per slab.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法产生了3200个模拟观测，这些观测来自40个模拟，每个模拟有20个板块，每个板块有4个种子（包括轴向翻转）。
- en: 'The 2560 slabs built from the portion of the simulation with $z\geq 220h^{-1}\,\mathrm{Mpc}$
    comprise the training set, and are used to train the machine learning model described
    in Section [3](#S3 "3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"). The remaining
    640 slabs are built from a non-overlapping portion of the simulation ($z<220h^{-1}\,\mathrm{Mpc}$).
    These make up the validation set and are used to assess the models’ fit.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '从$z\geq 220h^{-1}\,\mathrm{Mpc}$的模拟部分构建的2560个板块构成训练集，用于训练第[3](#S3 "3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")节中描述的机器学习模型。其余640个板块来自模拟的非重叠部分（$z<220h^{-1}\,\mathrm{Mpc}$）。这些构成验证集，并用于评估模型的拟合效果。'
- en: 'Our creation of the test and validation sets include a number of choices to
    reduce the likelihood of giving the machine learning model an unfair advantage:
    we employ a recentering of the box to minimize learning from images with correlated
    structure, we use random HODs and seeds to allow for uncertainties in galaxy formation
    physics, we use axial flips of the slabs to augment the data to account for rotational
    invariance, and we use unique portions of the simulation and unique HODs in the
    validation fold to provide a way to ensure that the model does not rely on the
    details of the structure or HOD.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在创建测试和验证集时包括了若干选择，以减少给机器学习模型带来不公平优势的可能性：我们使用了盒子的重新中心化以最小化学习具有相关结构的图像，我们使用随机HOD和种子以允许星系形成物理学中的不确定性，我们使用板块的轴向翻转来增强数据以考虑旋转不变性，我们在验证折中使用了独特的模拟部分和独特的HOD，以确保模型不依赖于结构或HOD的细节。
- en: 2.4 Planck Testing Set
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 普朗克测试集
- en: 'The testing sample is built from the AbacusCosmos Planck simulations. The 20
    Planck simulations each have initial conditions that are unique from the simulation
    sample described in Section [2.3](#S2.SS3 "2.3 Training & Validation Sets ‣ 2
    Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"). Mock observations of the Planck testing set are
    built using a similar process as described in Section [2.3](#S2.SS3 "2.3 Training
    & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys") with one exception:
    the 20 non-overlapping slabs are each populated with galaxies according to 20
    unique HODs selected randomly from the 31 HODs available. Accounting for the axial
    flips to augment the data, the resulting testing sample is 1600 slabs with associated
    power spectra. Our testing set is a truly independent sample from the training
    and validation sets. Though the cosmologies used in the training and validation
    sets are near the Planck fiducial cosmology, this exact cosmology is never explicitly
    used for training or testing.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '测试样本来源于 AbacusCosmos Planck 模拟。20 个 Planck 模拟的初始条件各不相同，与第 [2.3](#S2.SS3 "2.3
    Training & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") 节中描述的模拟样本不同。Planck
    测试集的模拟观测采用类似于第 [2.3](#S2.SS3 "2.3 Training & Validation Sets ‣ 2 Methods: Mock
    Observations ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys") 节中描述的方法，但有一个例外：20 个不重叠的板块分别根据从 31 个 HOD 中随机选择的 20 个独特
    HOD 进行填充。考虑到数据增强的轴向翻转，最终测试样本为 1600 个板块及其相关的功率谱。我们的测试集与训练和验证集完全独立。虽然训练和验证集使用的宇宙学模型接近
    Planck 基准宇宙学，但该精确宇宙学模型从未被明确用于训练或测试。'
- en: '3 Methods: Machine Learning Models'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法：机器学习模型
- en: 'We assess three machine learning models: 1. a standard convolutional neural
    network (CNN) that learns from the 3D galaxy images to regress cosmological parameters,
    2. a neural network (NN) that learns from the power spectrum of the galaxy images
    to regress cosmological parameters, and 3. a hybrid CNN (hCNN) that employs a
    standard CNN but also can take advantage of meaningful summary information — in
    this case, the galaxy power spectrum — to inject physically meaningful information
    into the fully connected layers. These three models are described in detail below.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了三种机器学习模型：1. 一个标准卷积神经网络（CNN），从 3D 银河图像中学习以回归宇宙学参数；2. 一个神经网络（NN），从银河图像的功率谱中学习以回归宇宙学参数；3.
    一个混合 CNN（hCNN），它使用标准 CNN，但也可以利用有意义的总结信息——在这种情况下，是银河功率谱——将物理上有意义的信息注入全连接层。这三种模型的详细描述如下。
- en: 3.1 Standard CNN
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 标准 CNN
- en: '![Refer to caption](img/215b1bc1028b6ec653f911d0f1c0b05f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/215b1bc1028b6ec653f911d0f1c0b05f.png)'
- en: 'Figure 3: A visual summary of the three ML models. The neural network (NN)
    uses a vector input (green) with the fully connected layers for processing (orange).
    The standard convolutional neural network (CNN) uses an image input with the image
    processing layers (blue) plus fully connected layers (orange). The hybrid CNN
    (hCNN) joins these by concatenating the vector features with the final layer of
    the image processing; the result is fed into the fully connected layers. For further
    details about the NN, CNN, and hCNN, see Section [3](#S3 "3 Methods: Machine Learning
    Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy
    Redshift Surveys").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：三种 ML 模型的视觉总结。神经网络（NN）使用向量输入（绿色）和全连接层（橙色）进行处理。标准卷积神经网络（CNN）使用图像输入以及图像处理层（蓝色）加全连接层（橙色）。混合
    CNN（hCNN）通过将向量特征与图像处理的最后一层拼接起来来结合这两者；结果送入全连接层。有关 NN、CNN 和 hCNN 的进一步细节，请参见第 [3](#S3
    "3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") 节。'
- en: Convolutional Neural Networks (CNNs, Fukushima & Miyake, [1982](#bib.bib20);
    LeCun et al., [1999](#bib.bib42); Krizhevsky et al., [2012](#bib.bib38)) are a
    class of machine learning algorithms that are commonly used in image recognition
    tasks. Over many cycles, called “epochs,” the network learns the convolutional
    filters, weights, and biases necessary to extract meaningful patterns from the
    input image. For cosmological applications, CNNs are traditionally applied to
    monochromatic (e.g., Lanusse et al., [2018](#bib.bib41); Ntampaka et al., [2018](#bib.bib48);
    Ho et al., [2019](#bib.bib30)) or multiple-color 2D images (e.g., La Plante &
    Ntampaka, [2018](#bib.bib40)). However, CNNs are not confined to 2D training data;
    they can also be used on 3D data cubes. Three-dimensional CNNs became popular
    for interpreting videos, using time as the third dimension (e.g., Ji et al., [2013](#bib.bib33)),
    but recent cosmological applications of this algorithm have applied the technique
    to 3D data (e.g., Ravanbakhsh et al., [2017](#bib.bib57); He et al., [2018](#bib.bib26);
    Mathuriya et al., [2018](#bib.bib44); Peel et al., [2018](#bib.bib51); Aragon-Calvo,
    [2019](#bib.bib4); Berger & Stein, [2019](#bib.bib8); Zhang et al., [2019](#bib.bib79);
    Pan et al., [2019](#bib.bib49)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络 (CNNs, Fukushima & Miyake, [1982](#bib.bib20); LeCun et al., [1999](#bib.bib42);
    Krizhevsky et al., [2012](#bib.bib38)) 是一种常用于图像识别任务的机器学习算法。经过多次迭代，即“轮次”，网络学习卷积滤波器、权重和偏置，以从输入图像中提取有意义的模式。对于宇宙学应用，CNNs
    传统上应用于单色（例如，Lanusse et al., [2018](#bib.bib41); Ntampaka et al., [2018](#bib.bib48);
    Ho et al., [2019](#bib.bib30)）或多色 2D 图像（例如，La Plante & Ntampaka, [2018](#bib.bib40)）。然而，CNNs
    并不限于 2D 训练数据；它们也可以用于 3D 数据立方体。三维 CNNs 因解释视频而变得流行，时间作为第三维度（例如，Ji et al., [2013](#bib.bib33)），但该算法在最近的宇宙学应用中已将技术应用于
    3D 数据（例如，Ravanbakhsh et al., [2017](#bib.bib57); He et al., [2018](#bib.bib26);
    Mathuriya et al., [2018](#bib.bib44); Peel et al., [2018](#bib.bib51); Aragon-Calvo,
    [2019](#bib.bib4); Berger & Stein, [2019](#bib.bib8); Zhang et al., [2019](#bib.bib79);
    Pan et al., [2019](#bib.bib49)）。
- en: 'CNNs typically use pairs of convolutional filters and pooling layers to extract
    meaningful patterns from the input image. These are followed by several fully
    connected layers. Our standard CNN architecture includes several consecutive fully
    convolutional layers at the onset and mean and max pooling branches in parallel.
    It is implemented in Keras (Chollet, [2015](#bib.bib11)) with a Tensorflow (Abadi
    et al., [2016](#bib.bib1)) backend, and is shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"). The full
    architecture is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 通常使用卷积滤波器和池化层的组合来从输入图像中提取有意义的模式。接下来是若干全连接层。我们的标准 CNN 架构包括几个连续的全卷积层，以及并行的平均池化和最大池化分支。它在
    Keras (Chollet, [2015](#bib.bib11)) 中实现，后端使用 Tensorflow (Abadi et al., [2016](#bib.bib1))，并如图
    [3](#S3.F3 "图 3 ‣ 3.1 标准 CNN ‣ 3 方法：机器学习模型 ‣ 一种混合深度学习方法用于从星系红移测量中获取宇宙学约束") 所示。完整的架构如下：
- en: '1.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: $3\times 3\times 3$ convolution with 4 filters
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，带 4 个滤波器
- en: leaky ReLU activation
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带泄漏的 ReLU 激活
- en: batch normalization
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '2.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: $3\times 3\times 3$ convolution with 4 filters
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，带 4 个滤波器
- en: leaky ReLU activation
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带泄漏的 ReLU 激活
- en: batch normalization
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '3.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: $3\times 3\times 3$ convolution with 4 filters
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，带 4 个滤波器
- en: leaky ReLU activation
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带泄漏的 ReLU 激活
- en: batch normalization
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '4.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Max pooling branch (in parallel with # [5](#S3.I1.i5 "item 5 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys")):'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '最大池化分支（与 # [5](#S3.I1.i5 "项 5 ‣ 3.1 标准 CNN ‣ 3 方法：机器学习模型 ‣ 一种混合深度学习方法用于从星系红移测量中获取宇宙学约束")
    并行）：'
- en: (a)
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: $5\times 5\times 1$ max pooling
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 1$ 最大池化
- en: (b)
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: $3\times 3\times 3$ convolution with 4 filters
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，带 4 个滤波器
- en: leaky ReLU activation
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带泄漏的 ReLU 激活
- en: batch normalization
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: (c)
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: $5\times 5\times 5$ max pooling
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 5$ 最大池化
- en: (d)
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: $3\times 3\times 3$ convolution with 32 filters
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，带 32 个滤波器
- en: leaky ReLU activation
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带泄漏的 ReLU 激活
- en: batch normalization
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: (e)
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: $5\times 5\times 5$ max pooling, flattened
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 5$ 最大池化，展平
- en: '5.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Mean pooling branch (in parallel with # [4](#S3.I1.i4 "item 4 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys")):'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '平均池化分支（与 # [4](#S3.I1.i4 "项 4 ‣ 3.1 标准 CNN ‣ 3 方法：机器学习模型 ‣ 一种混合深度学习方法用于从星系红移测量中获取宇宙学约束")
    并行）：'
- en: (a)
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: $5\times 5\times 1$ max pooling
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 1$ 最大池化
- en: (b)
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: $3\times 3\times 3$ convolution with 4 filters
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，4 个滤波器
- en: leaky ReLU activation
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: batch normalization
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: (c)
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: $5\times 5\times 5$ max pooling
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 5$ 最大池化
- en: (d)
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: $3\times 3\times 3$ convolution with 32 filters
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $3\times 3\times 3$ 卷积，32 个滤波器
- en: leaky ReLU activation
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: batch normalization
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化
- en: (e)
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: $5\times 5\times 5$ max pooling, flattened
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: $5\times 5\times 5$ 最大池化，展平
- en: '6.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Concatenation of the max pool branch output ([4e](#S3.I1.i4.I2.i5 "item 4e
    ‣ item 4 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"))
    and mean pool branch output ([5e](#S3.I1.i5.I1.i5 "item 5e ‣ item 5 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys"))'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '最大池化分支输出（[4e](#S3.I1.i4.I2.i5 "item 4e ‣ item 4 ‣ 3.1 Standard CNN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")）和均值池化分支输出（[5e](#S3.I1.i5.I1.i5 "item 5e ‣ item
    5 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys")）的连接'
- en: leaky ReLU activation
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: '7.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 1024 neurons, fully connected
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1024 个神经元，完全连接
- en: leaky ReLU activation
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: 30% dropout
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '8.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 512 neurons, fully connected
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 512 个神经元，完全连接
- en: leaky ReLU activation
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: 30% dropout
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '9.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 512 neurons, fully connected
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 512 个神经元，完全连接
- en: leaky ReLU activation
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: 30% dropout
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '10.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: 256 neurons, fully connected
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 256 个神经元，完全连接
- en: leaky ReLU activation
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: 30% dropout
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '11.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: 128 neurons, fully connected
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 128 个神经元，完全连接
- en: leaky ReLU activation
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活
- en: 30% dropout
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '12.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: 64 neurons, fully connected
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 64 个神经元，完全连接
- en: linear activation
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性激活
- en: 30% dropout
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 30% 随机失活
- en: '13.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: 2 output neurons, one each for $\Omega_{m}$ and $\sigma_{8}$
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2 个输出神经元，分别对应 $\Omega_{m}$ 和 $\sigma_{8}$
- en: 'We use a mean absolute error loss function and the Adam Optimizer (Kingma &
    Ba, [2014](#bib.bib36)). In practice, we scale $\Omega_{m}$ and $\sigma_{8}$ linearly
    so that the range of training values lies between $-1$ and $1$. The output predictions
    are scaled back to physically interpretable values according to the inverse of
    the same linear scaling. While this may not be an important detail for these particular
    cosmological parameters ($\sigma_{8}$ and $\Omega_{m}$ are of the same order of
    magnitude), problems can arise when training multiple outputs with significantly
    different value ranges (e.g. if $H_{0}$ in units of $\mathrm{km}\,s^{-1}\,\mathrm{Mpc}^{-1}$
    were added as a third output parameter). Details about the training scheme and
    learning rate are discussed in Section [3.3](#S3.SS3 "3.3 Training ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用均值绝对误差损失函数和 Adam 优化器（Kingma & Ba，[2014](#bib.bib36)）。实际上，我们线性缩放 $\Omega_{m}$
    和 $\sigma_{8}$，使得训练值范围在 $-1$ 和 $1$ 之间。根据相同线性缩放的逆，将输出预测值缩放回物理上可解释的值。虽然这对这些特定的宇宙学参数（$\sigma_{8}$
    和 $\Omega_{m}$ 在同一数量级）可能不是一个重要细节，但当训练具有显著不同值范围的多个输出时（例如，如果以 $\mathrm{km}\,s^{-1}\,\mathrm{Mpc}^{-1}$
    为单位的 $H_{0}$ 被添加为第三个输出参数），可能会出现问题。关于训练方案和学习率的详细信息见[3.3](#S3.SS3 "3.3 Training
    ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")节。'
- en: In our model, small-scale feature extraction is performed by several consecutive
    layers of 3D $3\times 3\times 3$ convolutional filters. This feature extraction
    is followed by aggressive pooling in parallel max and mean pooling branches that
    each reduce the data cube to 32 neurons. The outputs of these branches are concatenated
    and are followed by fully connected layers. We use a rectified linear unit (ReLU,
    Nair & Hinton, [2010](#bib.bib47)) activation function throughout. The dropout,
    in which 30% of neurons are ignored during training, reduces the likelihood of
    the model overfitting (Srivastava et al., [2014](#bib.bib65)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，小规模特征提取由几个连续的 3D $3\times 3\times 3$ 卷积滤波器完成。特征提取后是最大池化和均值池化分支的激进池化，这两个分支将数据立方体缩减到
    32 个神经元。这些分支的输出被连接起来，接着是完全连接的层。我们在整个过程中使用了整流线性单元（ReLU，Nair & Hinton，[2010](#bib.bib47)）激活函数。30%
    的神经元在训练过程中被忽略的随机失活减少了模型过拟合的可能性（Srivastava et al.，[2014](#bib.bib65)）。
- en: The model takes a $275\times 275\times 55$ image as input and learns the filters,
    weights, and biases necessary to regress two cosmological parameters — the amplitude
    of matter fluctuations ($\sigma_{8}$) and the matter density parameter ($\Omega_{m}$);
    each of the two output neurons maps to a cosmological parameter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以$275\times 275\times 55$的图像作为输入，学习所需的滤波器、权重和偏置，以回归两个宇宙学参数——物质波动幅度（$\sigma_{8}$）和物质密度参数（$\Omega_{m}$）；两个输出神经元分别映射到一个宇宙学参数。
- en: 3.2 Standard NN
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 标准神经网络
- en: 'The standard neural network uses only the fully connected layers, with the
    power spectrum as the only input, fed into steps [8](#S3.I1.i8 "item 8 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys") through [13](#S3.I1.i13
    "item 13 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys") in
    the above architecture. It is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys"). The model takes the binned
    power spectra as input and learns the weights and biases necessary to regress
    the cosmological parameters of interest.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '标准神经网络仅使用全连接层，将功率谱作为唯一输入，通过步骤[8](#S3.I1.i8 "item 8 ‣ 3.1 Standard CNN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")到[13](#S3.I1.i13 "item 13 ‣ 3.1 Standard CNN ‣ 3
    Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")输入上述架构中。见于图[3](#S3.F3 "Figure 3 ‣ 3.1
    Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")。该模型以分 bin 的功率谱作为输入，学习回归所关注的宇宙学参数所需的权重和偏置。'
- en: 3.2.1 Hybrid CNN
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 混合CNN
- en: 'The hybrid convolutional neural network (hCNN) takes advantage of a standard
    CNN, but also utilizes information that is known to be important and meaningful.
    The power spectrum, which carries cosmological information, is folded in by inserting
    this information at step [8](#S3.I1.i8 "item 8 ‣ 3.1 Standard CNN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") in the standard CNN architecture. It should be
    noted that the use of incorporating physically meaningful parameters into a deep
    learning technique is not new to this work, and has been used previous in astronomy
    (Dattilo et al., [2019](#bib.bib13)), though it has not yet been widely adopted.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '混合卷积神经网络（hCNN）利用了标准CNN的优势，同时也利用了已知的重要和有意义的信息。携带宇宙学信息的功率谱通过在标准CNN架构的步骤[8](#S3.I1.i8
    "item 8 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")插入这些信息来折叠。需要注意的是，将物理上有意义的参数融入深度学习技术并非新鲜事，这在天文学中已有应用（Dattilo
    et al., [2019](#bib.bib13)），尽管尚未被广泛采用。'
- en: 'The hCNN model uses both the $275\times 275\times 55$ images as well as the
    binned power spectra as input to learn $\Omega_{m}$ and $\sigma_{8}$. This architecture
    is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine
    Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'hCNN模型使用$275\times 275\times 55$的图像以及分 bin 的功率谱作为输入，以学习$\Omega_{m}$和$\sigma_{8}$。此架构见于图[3](#S3.F3
    "Figure 3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")。'
- en: '| ![Refer to caption](img/35617d67c4d336bb853922d80a9f1045.png) | ![Refer to
    caption](img/995e0cf7838b2b688fe623d4dd6cadd8.png) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/35617d67c4d336bb853922d80a9f1045.png) | ![参考说明](img/995e0cf7838b2b688fe623d4dd6cadd8.png)
    |'
- en: 'Figure 4: Mean squared error (MSE) as a function of scaled epoch, $\mathcal{E}$.
    While the standard neural network (NN, green dotted) quickly settles to a low
    error solution, the convolutional neural network (CNN, blue dashed) and hybrid
    CNN (hCNN, purple solid) have large fluctuations during the initial phase of training
    ($\mathcal{E}\lesssim 0.32$). Here, the error on the validation set predictions
    are regularly worse than a guess of the mean value (gray line) for both $\sigma_{8}$
    (left) and $\Omega_{m}$ (right). The learning rate is decreased at $\mathcal{E}\approx
    0.32$, and the CNN and hCNN settle into a low-error regime. To remove fluctuations
    that visually detract from overall trends in error and slope, the curves shown
    in this figure have been smoothed with a Gaussian filter.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：均方误差 (MSE) 随缩放轮次 $\mathcal{E}$ 的变化。标准神经网络 (NN，绿色虚线) 很快收敛到低误差解，而卷积神经网络 (CNN，蓝色虚线)
    和混合 CNN (hCNN，紫色实线) 在训练初期 ($\mathcal{E}\lesssim 0.32$) 存在较大波动。这里，验证集预测的误差通常比均值猜测
    (灰线) 更差，适用于 $\sigma_{8}$ (左) 和 $\Omega_{m}$ (右)。学习率在 $\mathcal{E}\approx 0.32$
    时降低，CNN 和 hCNN 进入低误差状态。为了去除视觉上干扰整体误差和斜率趋势的波动，本图中的曲线经过高斯滤波平滑处理。
- en: 3.3 Training
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 训练
- en: For training the CNN and hCNN, we adopt a two-phase training scheme. Our training
    approach takes advantage of a large step size during the initial phase of training
    to capture the diversity of cosmologies and HOD models, then transitions to a
    smaller step size during the second phase of training to improve the fit (see
    Appendix [A](#A1 "Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") for a further
    discussion of this). We train for 550 epochs, 175 in the first phase and 375 in
    the second phase. The last 50 epochs will be used to select a model that meets
    criteria more nuanced than simply minimizing the loss function. It is discussed
    further in Section [4.2.1](#S4.SS2.SSS1 "4.2.1 Unbiased Model Selection ‣ 4.2
    Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"). Note that the NN, which is less sensitive
    to the details of training and trains significantly faster than models with convolutional
    layers, is trained for 800 epochs according to the details of phase one, described
    below.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练 CNN 和 hCNN，我们采用两阶段训练方案。我们的训练方法在训练的初始阶段利用较大的步长来捕捉宇宙学和 HOD 模型的多样性，然后在训练的第二阶段转向较小的步长以改进拟合
    (有关详细讨论，请参见附录 [A](#A1 "附录 A CNN 的生命周期 ‣ 一种混合深度学习方法用于从星系红移调查中获得宇宙学约束")。我们训练 550
    轮，其中初始阶段 175 轮，第二阶段 375 轮。最后 50 轮将用于选择一个满足比仅仅最小化损失函数更精细标准的模型。有关详细讨论，请参见第 [4.2.1](#S4.SS2.SSS1
    "4.2.1 无偏模型选择 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 一种混合深度学习方法用于从星系红移调查中获得宇宙学约束") 节。注意，NN 对训练细节的敏感度较低，并且训练速度显著快于具有卷积层的模型，按照下面描述的阶段一的详细信息，训练了
    800 轮。
- en: We use the Adam Optimizer (Kingma & Ba, [2014](#bib.bib36)), which has a step
    size that varies as a function of epoch according to
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Adam 优化器 (Kingma & Ba, [2014](#bib.bib36))，其步长根据轮次变化，如下所示
- en: '|  | $\alpha(t)=\alpha_{0}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}},$
    |  | (3) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha(t)=\alpha_{0}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}},$
    |  | (3) |'
- en: where $\alpha$ is the step size, $t$ denotes a time step or epoch, $\alpha_{0}$
    is the initial step size⁴⁴4The initial step size is denoted, simply, “learning
    rate” in the keras documentation., and parameters $\beta_{1}$ and $\beta_{2}$
    control the step size at each epoch. We adopt the default values of $\beta_{1}=0.9$
    and $\beta_{2}=0.999$.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是步长，$t$ 表示时间步或轮次，$\alpha_{0}$ 是初始步长⁴⁴4 初始步长在 keras 文档中简单地表示为“学习率”。，参数
    $\beta_{1}$ 和 $\beta_{2}$ 控制每轮的步长。我们采用默认值 $\beta_{1}=0.9$ 和 $\beta_{2}=0.999$。
- en: 'Phase one of training is 175 epochs with an initial step size of $\alpha_{0}=1.0\times
    10^{-5}$. We find that this first phase, with its larger initial step size, is
    necessary for the models to learn the diversity of cosmologies. Smaller learning
    rates tend to produce models with predictions that cluster near the mean values
    for $\sigma_{8}$ and $\Omega_{m}$, while larger learning rates tend to produce
    models that fluctuate wildly in bias or overfit the training data. Near epoch
    175, we find evidence in the CNN and hCNN that the learning rate is too large.
    This is characterized by swings in the tendency to over- or underpredict the validation
    set, and can be seen in the large, fluctuating mean squared error (MSE) shown
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"). The MSE is plotted as a function of scaled epoch,
    $\mathcal{E}$, defined as epoch divided by the maximum number of training epochs.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '第一阶段训练是175个时期，初始步长为$\alpha_{0}=1.0\times 10^{-5}$。我们发现，这一阶段由于其较大的初始步长，对于模型学习宇宙学的多样性是必要的。较小的学习率往往会产生模型，其预测值聚集在$\sigma_{8}$和$\Omega_{m}$的均值附近，而较大的学习率则倾向于产生模型，其在偏差上波动剧烈或过拟合训练数据。在接近第175个时期时，我们在CNN和hCNN中发现学习率过大。这一特征表现为对验证集的过度或不足预测的倾向波动，可以在图[4](#S3.F4
    "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")中看到，图中展示了大幅波动的均方误差（MSE）。MSE被绘制为缩放时期$\mathcal{E}$的函数，$\mathcal{E}$定义为时期除以最大训练时期数。'
- en: 'We adopt the model at epoch 175 as a pre-trained model and transition to a
    second phase of training with a lower learning rate. Phase two of training is
    an additional 375 epochs with an initial step size of $\alpha_{0}=0.2\times 10^{-5}$.
    For clarity, we refer to the first training epoch of phase two as “epoch 176”
    for the remainder of this work. However, for the purposes of Equation [3](#S3.E3
    "In 3.3 Training ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") only, $t$
    is reset to $0$. Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard
    NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") shows the effect of decreasing the
    learning rate: at $\mathcal{E}\approx 0.32$, the mean squared error decreases
    dramatically as the model settles into a stable fit that describes the validation
    data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将第175个时期的模型作为预训练模型，并过渡到学习率较低的第二阶段训练。第二阶段训练是额外的375个时期，初始步长为$\alpha_{0}=0.2\times
    10^{-5}$。为了清晰起见，我们将第二阶段训练的第一个训练时期称为“时期176”。然而，仅用于方程[3](#S3.E3 "In 3.3 Training
    ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")的目的，$t$被重置为$0$。图[4](#S3.F4 "Figure 4
    ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models ‣ A
    Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")显示了降低学习率的效果：在$\mathcal{E}\approx 0.32$时，均方误差显著下降，因为模型稳定地拟合了描述验证数据的稳定状态。'
- en: 'Overfitting is defined as the tendency of the model to produce excellent predictions
    on the testing set but to fail on the validation set. (The term “overfit” is occasionally
    used to describe a deep learning method identifying features in a cosmological
    simulation that do not describe actual observations, but we use the term in the
    more traditional sense.) Two changes to the learning scheme tend to result in
    an overfit model: first, an increased learning rate and second, the use of max
    pooling only via eliminating the mean pooling branch. When the model is overfit,
    the validation set dramatically biases toward the mean, despite the fact that
    the training data are well-described even at extreme values of $\sigma_{8}$ and
    $\Omega_{m}$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合被定义为模型在测试集上产生优异预测但在验证集上失败的倾向。（“过拟合”这一术语有时用来描述深度学习方法在宇宙学模拟中识别出不描述实际观测的特征，但我们在这里使用更传统的定义。）学习方案中的两个变化倾向于导致过拟合模型：首先是增加学习率，其次是仅通过去除均值池化分支来使用最大池化。当模型过拟合时，尽管训练数据即使在极端值$\sigma_{8}$和$\Omega_{m}$下也被很好地描述，但验证集却严重偏向于均值。
- en: We caution, however, that we have not explored a full grid of hyperparameters
    for model optimization. It is likely that the two-phase training scheme could
    be avoided with carefully selected values of $\beta_{1}$ and $\beta_{2}$ to smoothly
    decrease step size. Likewise, we have not thoroughly vetted the tendency to overfit
    by increasing learning rate or removing mean pooling under many hyperparameter
    combinations. Such a comprehensive grid search is expensive and intractable with
    current computational resources. Therefore, the effects of learning rate and pooling
    described in this section should serve as a word of caution for those training
    other deep models, but should not be overinterpreted.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要注意的是，我们尚未探索全面的超参数网格以进行模型优化。可能通过仔细选择 $\beta_{1}$ 和 $\beta_{2}$ 的值以平滑减少步长，可以避免这种两阶段的训练方案。同样，我们也没有彻底检查通过增加学习率或在多种超参数组合下去除均值池化的过拟合倾向。这种全面的网格搜索在当前计算资源下成本高昂且难以实现。因此，本节中描述的学习率和池化的效果应作为训练其他深度模型时的警示，但不应过度解读。
- en: 4 Results
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: '| ![Refer to caption](img/3f0b329d94e762eabace8cc4a295f560.png) | ![Refer to
    caption](img/4abfcc8ec2c6a69deb4a79bdb36e5eb2.png) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/3f0b329d94e762eabace8cc4a295f560.png) | ![参见说明](img/4abfcc8ec2c6a69deb4a79bdb36e5eb2.png)
    |'
- en: '| ![Refer to caption](img/a5395f4fe0c0ff7ea6ac5bd68bf0a89e.png) | ![Refer to
    caption](img/0e0b0c714491226fd9694c1fd96fc638.png) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/a5395f4fe0c0ff7ea6ac5bd68bf0a89e.png) | ![参见说明](img/0e0b0c714491226fd9694c1fd96fc638.png)
    |'
- en: 'Figure 5: Top: slope of the best fit line as a function of scaled epoch, $\mathcal{E}$.
    A slope of 1 indicates that the model captures the full range of $\sigma_{8}$
    and $\Omega_{m}$, while a slope of 0 is indicative of the model predicting at
    or near the mean for all data in the validation set. As the models train, they
    increase the diversity of predictions. However, slope never reaches a value of
    $1$ for any model, indicating that the predictions will bias toward the mean for
    any mock observation with extreme values of $\sigma_{8}$ or $\Omega_{m}$. Bottom:
    prediction bias, b, as a function of scaled epoch, $\mathcal{E}$. While the standard
    neural network (NN, green dotted) quickly settles to a solution with low bias,
    the convolutional neural network (CNN, blue dashed) and hybrid CNN (hCNN, purple
    solid) have large fluctuations during the initial phase of training ($\mathcal{E}\lesssim
    0.32$). The learning rate is decreased at $\mathcal{E}=0.32$, and the CNN and
    hCNN settle into a low-bias regime. To remove fluctuations that visually detract
    from overall trends in error and slope, the curves shown in this figure have been
    smoothed with a Gaussian filter.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：上图：最佳拟合线的斜率作为缩放时代 $\mathcal{E}$ 的函数。斜率为1表示模型捕捉到了 $\sigma_{8}$ 和 $\Omega_{m}$
    的全范围，而斜率为0则表明模型在验证集中对所有数据的预测接近或等于均值。随着模型的训练，预测的多样性增加。然而，任何模型的斜率都未达到 $1$，这表明对于具有极端值的
    $\sigma_{8}$ 或 $\Omega_{m}$ 的任何模拟观察，预测将会偏向均值。下图：预测偏差 $b$ 作为缩放时代 $\mathcal{E}$
    的函数。虽然标准神经网络（NN，绿色虚线）迅速稳定到低偏差的解，卷积神经网络（CNN，蓝色虚线）和混合卷积神经网络（hCNN，紫色实线）在训练的初始阶段（$\mathcal{E}\lesssim
    0.32$）有较大的波动。在 $\mathcal{E}=0.32$ 时降低学习率，CNN 和 hCNN 稳定到低偏差的范围。为了去除视觉上影响整体趋势的波动，本图中所示的曲线已通过高斯滤波器进行平滑处理。
- en: Here, we present results from the validation set as a way of assessing the model’s
    fit, both near the median model and also toward extreme values of $\sigma_{8}$
    and $\Omega_{m}$. We also present results from the testing set to explore how
    the technique might generalize into the more realistic case where the cosmological
    model, galaxy formation details, and initial conditions are not explicitly known.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了来自验证集的结果，以评估模型的拟合情况，既包括接近中位数模型的情况，也包括对 $\sigma_{8}$ 和 $\Omega_{m}$
    的极端值的情况。我们还展示了测试集的结果，以探索该技术如何推广到更现实的情况，即宇宙学模型、星系形成细节和初始条件并不明确已知。
- en: 4.1 Validation Set Results
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 验证集结果
- en: We define the prediction bias, $b$, as
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义预测偏差 $b$ 为
- en: '|  | $b\equiv\left<\left&#124;x_{\mathrm{predicted}}-x_{\mathrm{true}}\right&#124;\right>,$
    |  | (4) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $b\equiv\left<\left&#124;x_{\mathrm{predicted}}-x_{\mathrm{true}}\right&#124;\right>,$
    |  | (4) |'
- en: where $<\cdot>$ denotes a mean and $x$ is a placeholder for either $\sigma_{8}$
    or $\Omega_{m}$. Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") shows the
    bias as a function of scaled epoch, $\mathcal{E}$. During phase two of the training,
    the CNN and hCNN bias drop significantly, indicating that the lower learning rate
    is indeed reducing errors in a meaningful way and learning the spatial galaxy
    patterns that correlate with cosmological parameters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $<\cdot>$ 表示均值，$x$ 是 $\sigma_{8}$ 或 $\Omega_{m}$ 的占位符。图 [5](#S4.F5 "Figure
    5 ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys") 显示了偏差作为尺度时期 $\mathcal{E}$ 的函数。在训练的第二阶段，CNN 和 hCNN 的偏差显著下降，表明较低的学习率确实以有意义的方式减少了错误，并学习了与宇宙学参数相关的空间银河模式。
- en: While MSE and bias both assess the typical offset of the validation set predictions,
    these statistics alone cannot tell the full story. It is also important to understand
    how the model might perform near the edges of the training set. For this, we assess
    the slope of a best fit line through the true and predicted values of $\sigma_{8}$,
    and separately, the best fit line through the true and predicted values of $\Omega_{m}$.
    A slope close to $1$ indicates that the model fits well near the extreme values
    of $\sigma_{8}$ and $\Omega_{m}$, while a slope of $0$ is indicative of a model
    biasing toward the mean. Overfit models will tend to have a larger MSE and bias
    coupled with a smaller slope. Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the slope of this linear best fit line. We can infer from the value of this
    fit, $\sim 0.7-0.8$ for both $\sigma_{8}$ and $\Omega_{m}$, that the model may
    not predict well for $\sigma_{8}$ and $\Omega_{m}$ values near the edges of the
    training data, and will likely bias toward the mean when presented with a cosmological
    parameter set far from the mean.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 MSE 和偏差都评估了验证集预测的典型偏差，但这些统计数据本身并不能讲述完整的故事。同样重要的是要了解模型在训练集边缘附近的表现。为此，我们评估了通过真实值和预测值的
    $\sigma_{8}$ 最佳拟合线的斜率，以及通过真实值和预测值的 $\Omega_{m}$ 的最佳拟合线的斜率。斜率接近 $1$ 表示模型在 $\sigma_{8}$
    和 $\Omega_{m}$ 的极端值附近拟合良好，而斜率为 $0$ 则表示模型偏向于均值。过拟合的模型通常会有较大的 MSE 和偏差，并且斜率较小。图 [5](#S4.F5
    "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") 显示了这一线性最佳拟合线的斜率。我们可以从这个拟合的值中推断出，$\sigma_{8}$ 和
    $\Omega_{m}$ 的值在训练数据的边缘附近模型可能预测得不好，并且当面对远离均值的宇宙学参数集时，模型可能会偏向于均值。
- en: 4.2 Testing Set Results
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 测试集结果
- en: While it is an interesting academic exercise to discuss the results of the validation
    set, the Universe, unfortunately, gives us one galaxy sample. This sample may
    differ from our training set in cosmological parameters and galaxy formation physics
    (and most certainly differs in initial conditions!). If we aim to eventually use
    a CNN or hCNN to constrain cosmological models from an observed galaxy sample,
    is imperative to develop tools to assess ML models, going beyond a simple minimization
    of loss or performance on validation data. Though the model trains to minimize
    the mean absolute error, this is not necessarily the most interesting — or the
    most useful — test statistic for a cosmological analysis of a large galaxy survey.
    Next, we lay out a technique for selecting a relatively unbiased model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管讨论验证集结果是一个有趣的学术练习，但不幸的是，宇宙给我们只有一个银河样本。这个样本在宇宙学参数和银河形成物理（以及初始条件！）上可能与我们的训练集不同。如果我们最终打算使用
    CNN 或 hCNN 从观测到的银河样本中限制宇宙学模型，那么开发工具来评估机器学习模型是至关重要的，这些工具需要超越简单的损失最小化或在验证数据上的性能。尽管模型训练是为了最小化平均绝对误差，但这不一定是最有趣或最有用的统计量，用于对大规模银河调查的宇宙学分析。接下来，我们提出了一种选择相对无偏模型的技术。
- en: '| ![Refer to caption](img/8077e4c8cb95b65a399b09a6882e23a2.png) | ![Refer to
    caption](img/aed53de2183b562dd4c3a2bb7564175a.png) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/8077e4c8cb95b65a399b09a6882e23a2.png) | ![参见标题](img/aed53de2183b562dd4c3a2bb7564175a.png)
    |'
- en: '| ![Refer to caption](img/927706b9cb07878beaff4875aac83bf6.png) | ![Refer to
    caption](img/f23c7ce7906caf46f39612ed91f13138.png) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/927706b9cb07878beaff4875aac83bf6.png) | ![参见标题](img/f23c7ce7906caf46f39612ed91f13138.png)
    |'
- en: '| ![Refer to caption](img/ed8ad5f6842d98a3935b46d9e0ba7bf0.png) | ![Refer to
    caption](img/8a63bbaedfec3cc5e4f7f2b9bd00cd20.png) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/ed8ad5f6842d98a3935b46d9e0ba7bf0.png) | ![参见标题](img/8a63bbaedfec3cc5e4f7f2b9bd00cd20.png)
    |'
- en: 'Figure 6: True and predicted values of $\sigma_{8}$ (left) and $\Omega_{m}$
    (right) for the neural network (NN, green, top), convolutional neural network
    (CNN, blue, middle) and hybrid CNN (hCNN, purple, bottom). For the validation
    data of each of the 40 cosmological models, the median (circles) and middle 68%
    (error bars) are shown. While the predictions typically lie close to the one-to-one
    line (black dashed) near the central values of $\sigma_{8}$ and $\Omega_{m}$,
    the bias toward the mean is more pronounced at extreme values. For illustrative
    purposes, $\sigma_{8}$ and $\Omega_{m}$ values below the 16th percentile and above
    the 84th precentile are set against a gray background, while the middle 1-$\sigma$
    are shown against a white background. The CNN and hCNN predictions for the validation
    set display a significantly tighter scatter than the NN. This is unsurprising
    because the NN learns only from the power spectrum (see Figure [2](#S2.F2 "Figure
    2 ‣ 2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")),
    while the CNN and hCNN have more flexibility to learn from the un-preprocessed
    mock galaxy catalog.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：神经网络（NN，绿色，上）、卷积神经网络（CNN，蓝色，中）和混合卷积神经网络（hCNN，紫色，下）对于$\sigma_{8}$（左）和$\Omega_{m}$（右）的真实值和预测值。对于每个40个宇宙学模型的验证数据，显示了中位数（圆圈）和68%的中间值（误差条）。尽管预测值通常接近于$\sigma_{8}$和$\Omega_{m}$的中央值附近的一对一线（黑色虚线），但在极端值处向均值的偏斜更为明显。为了说明，$\sigma_{8}$和$\Omega_{m}$值低于第16百分位数和高于第84百分位数的部分背景为灰色，而中间1-$\sigma$的背景为白色。CNN和hCNN的验证集预测显示出明显比NN更紧密的散布。这并不令人意外，因为NN仅从功率谱学习（见图[2](#S2.F2
    "图 2 ‣ 2.3 训练与验证集 ‣ 2 方法：模拟观测 ‣ 基于混合深度学习的星系红移调查的宇宙学约束")），而CNN和hCNN有更多的灵活性从未经预处理的模拟星系目录中学习。
- en: 4.2.1 Unbiased Model Selection
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 无偏模型选择
- en: As highlighted in Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"), the models
    do not perform well at extreme values of $\sigma_{8}$ and $\Omega_{m}$. This is
    unsurprising; machine learning models tend to interpolate much better than they
    extrapolate. In practice, one would want to train on a large range of simulated
    cosmologies extending well beyond a region containing the expected results. Furthermore,
    one would expect a bias toward the mean for any cosmology near the edges of the
    training sample. Because of this (and for the purposes of model selection only),
    we limit our analysis to the simulations enclosed in a 68% ellipse in the $\sigma_{8}$-$\Omega_{m}$
    plane.⁵⁵5The selection of simulations used here are shown in a lighter shade of
    gray in Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing
    Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"); the simulations shown in dark gray are near the
    edges of the $\sigma_{8}$-$\Omega_{m}$ plane, are expected to have results that
    bias to the mean, and are excluded from this particular analysis for this reason.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[5](#S4.F5 "图 5 ‣ 4 结果 ‣ 基于混合深度学习的星系红移调查的宇宙学约束")所示，模型在$\sigma_{8}$和$\Omega_{m}$的极端值处表现不佳。这并不令人意外；机器学习模型在插值方面往往比在外推方面表现更好。在实践中，人们希望在一个较大的模拟宇宙学范围内进行训练，这个范围应远超包含预期结果的区域。此外，对于训练样本边缘附近的任何宇宙学模型，预计会有一个向均值偏斜的倾向。基于这一点（并且仅用于模型选择），我们将分析限制在$\sigma_{8}$-$\Omega_{m}$平面上一个68%椭圆内的模拟中。⁵⁵
    这里选择的模拟在图[8](#S4.F8 "图 8 ‣ 4.2.2 Planck 测试集结果 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 基于混合深度学习的星系红移调查的宇宙学约束")中显示为较浅的灰色；图中深灰色的模拟靠近$\sigma_{8}$-$\Omega_{m}$平面的边缘，预计结果会向均值偏斜，因此被排除在本次分析之外。
- en: In addition to limiting this analysis to the 27 simulations with $\sigma_{8}$
    and $\Omega_{m}$ values closest to the mean cosmology, we also only assess the
    last 50 epochs of the CNN and hCNN trainings ($0.91<\mathcal{E}\leq 1.0$). Importantly,
    we only use the validation data to assess models. Recall that the training data
    should not be used in such a way because the model has already explicitly seen
    this data. Likewise, the testing data should not be used to assess models because
    doing so would unfairly bias the results.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将此分析限制于27个与$\sigma_{8}$和$\Omega_{m}$值最接近平均宇宙学的模拟外，我们还仅评估CNN和hCNN训练的最后50个时期（$0.91<\mathcal{E}\leq
    1.0$）。重要的是，我们只使用验证数据来评估模型。请记住，训练数据不应以这种方式使用，因为模型已经明确见过这些数据。同样，测试数据也不应用于评估模型，因为这样做会不公平地偏向结果。
- en: 'For each of the 27 simulations and at each epoch, we calculate the distance
    between the predicted and true cosmology according to the following: for each
    of the 16 validation mock observations per simulation, we predict $\sigma_{8}$
    and $\Omega_{m}$. The 68% error ellipse in the $\sigma_{8}$-$\Omega_{m}$ plane
    is calculated, as is the distance between the true cosmological parameters ($\Omega_{m,\,\mathrm{true}}$
    and $\sigma_{8,\,\mathrm{true}}$) and the middle of the ellipse of predicted cosmological
    parameters ($\Omega_{m,\,\mathrm{mid}}$ and $\sigma_{8,\,\mathrm{mid}}$). This
    distance, $\mathcal{Z}$, is calculated according to'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于27个模拟中的每一个，在每个时期，我们根据以下方法计算预测和真实宇宙学之间的距离：对于每个模拟的16个验证模拟观测，我们预测$\sigma_{8}$和$\Omega_{m}$。在$\sigma_{8}$-$\Omega_{m}$平面中计算68%误差椭圆，以及真实宇宙学参数（$\Omega_{m,\,\mathrm{true}}$和$\sigma_{8,\,\mathrm{true}}$）和预测宇宙学参数椭圆中心（$\Omega_{m,\,\mathrm{mid}}$和$\sigma_{8,\,\mathrm{mid}}$）之间的距离。这个距离$\mathcal{Z}$，按以下公式计算：
- en: '|  | <math   alttext="\begin{split}\mathcal{Z}\,=\,&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{Z}\,=\,&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\'
- en: '&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="right" ><mrow  ><mi >𝒵</mi><mo lspace="0.448em"  >=</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mfrac ><mrow  ><mrow ><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi mathvariant="normal" >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em"
    >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi mathvariant="normal"
    >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >cos</mi><mo lspace="0.167em" >⁡</mo><mi >α</mi></mrow></mrow><mo >+</mo><mrow
    ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >σ</mi><mrow ><mn >8</mn><mo
    rspace="0.337em" >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi >σ</mi><mrow
    ><mn >8</mn><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi >α</mi></mrow></mrow></mrow><msup
    ><mi >a</mi><mn >2</mn></msup></mfrac><mo >+</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mfrac  ><mrow ><mrow ><mrow  ><mo stretchy="false"  >(</mo><mrow ><msub ><mi
    mathvariant="normal"  >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em"  >,</mo><mi
    >true</mi></mrow></msub><mo >−</mo><msub ><mi mathvariant="normal"  >Ω</mi><mrow
    ><mi >m</mi><mo rspace="0.337em"  >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em" >⁡</mo><mi >α</mi></mrow></mrow><mo >−</mo><mrow
    ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >σ</mi><mrow ><mn >8</mn><mo
    rspace="0.337em" >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi >σ</mi><mrow
    ><mn >8</mn><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi >α</mi></mrow></mrow></mrow><msup
    ><mi >b</mi><mn  >2</mn></msup></mfrac></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply  ><ci >𝒵</ci><apply ><apply  ><apply ><apply ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci >true</ci></list></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci
    >mid</ci></list></apply></apply><apply ><ci  >𝛼</ci></apply></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜎</ci><list ><cn type="integer"  >8</cn><ci
    >true</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜎</ci><list ><cn type="integer"  >8</cn><ci >mid</ci></list></apply></apply><apply
    ><ci >𝛼</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑎</ci><cn type="integer"  >2</cn></apply></apply><apply ><apply ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci
    >true</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >Ω</ci><list ><ci >𝑚</ci><ci >mid</ci></list></apply></apply><apply ><ci >𝛼</ci></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜎</ci><list
    ><cn type="integer" >8</cn><ci >true</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜎</ci><list ><cn type="integer" >8</cn><ci >mid</ci></list></apply></apply><apply
    ><ci >𝛼</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑏</ci><cn type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{Z}\,=\,&\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\ &\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}</annotation></semantics></math>
    |  | (5) |'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\ &\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  columnalign="right" ><mrow  ><mi >𝒵</mi><mo lspace="0.448em"  >=</mo></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mfrac ><mrow  ><mrow ><mrow ><mo stretchy="false"
    >(</mo><mrow ><msub ><mi mathvariant="normal" >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em"
    >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi mathvariant="normal"
    >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >cos</mi><mo lspace="0.167em" >⁡</mo><mi >α</mi></mrow></mrow><mo >+</mo><mrow
    ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >σ</mi><mrow ><mn >8</mn><mo
    rspace="0.337em" >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi >σ</mi><mrow
    ><mn >8</mn><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi >α</mi></mrow></mrow></mrow><msup
    ><mi >a</mi><mn >2</mn></msup></mfrac><mo >+</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mfrac  ><mrow ><mrow ><mrow  ><mo stretchy="false"  >(</mo><mrow ><msub ><mi
    mathvariant="normal"  >Ω</mi><mrow ><mi >m</mi><mo rspace="0.337em"  >,</mo><mi
    >true</mi></mrow></msub><mo >−</mo><msub ><mi mathvariant="normal"  >Ω</mi><mrow
    ><mi >m</mi><mo rspace="0.337em"  >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false"  >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em" >⁡</mo><mi >α</mi></mrow></mrow><mo >−</mo><mrow
    ><mrow ><mo stretchy="false" >(</mo><mrow ><msub ><mi >σ</mi><mrow ><mn >8</mn><mo
    rspace="0.337em" >,</mo><mi >true</mi></mrow></msub><mo >−</mo><msub ><mi >σ</mi><mrow
    ><mn >8</mn><mo rspace="0.337em" >,</mo><mi >mid</mi></mrow></msub></mrow><mo
    stretchy="false" >)</mo></mrow><mo lspace="0.167em" rspace="0em"  >​</mo><mrow
    ><mi >sin</mi><mo lspace="0.167em"  >⁡</mo><mi >α</mi></mrow></mrow></mrow><msup
    ><mi >b</mi><mn  >2</mn></msup></mfrac></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    ><apply  ><ci >𝒵</ci><apply ><apply  ><apply ><apply ><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci >true</ci></list></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci
    >mid</ci></list></apply></apply><apply ><ci  >𝛼</ci></apply></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝜎</ci><list ><cn type="integer"  >8</cn><ci
    >true</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜎</ci><list ><cn type="integer"  >8</cn><ci >mid</ci></list></apply></apply><apply
    ><ci >𝛼</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑎</ci><cn type="integer"  >2</cn></apply></apply><apply ><apply ><apply  ><apply
    ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci >Ω</ci><list ><ci >𝑚</ci><ci
    >true</ci></list></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >Ω</ci><list ><ci >𝑚</ci><ci >mid</ci></list></apply></apply><apply ><ci >𝛼</ci></apply></apply><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜎</ci><list
    ><cn type="integer" >8</cn><ci >true</ci></list></apply><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝜎</ci><list ><cn type="integer" >8</cn><ci >mid</ci></list></apply></apply><apply
    ><ci >𝛼</ci></apply></apply></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑏</ci><cn type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{Z}\,=\,&\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\ &\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}</annotation></semantics></math>
    |  | (5) |'
- en: where $\alpha$ is the angle of the best fit 68% ellipse, $a$ is the length of
    the semimajor axis, and $b$ is the length of the semiminor axis. $\mathcal{Z}$,
    then, is a 2-dimensional z-score, where $\mathcal{Z}=1$ can be interpreted as
    the true value being on the edge of the 68% ellipse and $\mathcal{Z}=0$ means
    that the true and mean predicted values are identical. We note that this choice
    favors accuracy over precision because larger error ellipses are more forgiving
    of large offsets between the predicted and middle predicted cosmological models.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha$ 是最佳拟合68%椭圆的角度，$a$ 是半长轴的长度，$b$ 是半短轴的长度。$\mathcal{Z}$ 是二维z分数，其中 $\mathcal{Z}=1$
    可以解释为真实值在68%椭圆的边缘，而 $\mathcal{Z}=0$ 表示真实值和预测值完全相同。我们注意到这种选择更偏向于准确性而非精确性，因为较大的误差椭圆对预测值和中间预测宇宙模型之间的大偏移更为宽容。
- en: For each epoch, the mean squared error, MSE, as a function of epoch is calculated
    according to
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时代，均方误差（MSE）作为时代的函数进行计算。
- en: '|  | $\mathrm{MSE(e)}=\frac{1}{N_{\mathrm{sims}}}\sum_{i=1}^{N_{\mathrm{sims}}}\mathcal{Z}^{2}_{i}(e)$
    |  | (6) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{MSE(e)}=\frac{1}{N_{\mathrm{sims}}}\sum_{i=1}^{N_{\mathrm{sims}}}\mathcal{Z}^{2}_{i}(e)$
    |  | (6) |'
- en: We select the epoch with the smallest MSE as the final model — and the model
    least likely to produce biased results — for the CNN and hCNN. Coincidentally,
    these “unbiased” models are from training epochs that are rather close to each
    other, epochs 520 and 524 ($\mathcal{E}\approx 0.95$) for the CNN and hCNN, respectively.
    Selecting, instead, to define a 2-D error ellipse that is averaged over all models
    and epochs selects the same hCNN model, but prefers a CNN model with marginally
    tighter error bars and a more significant offset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择均方误差（MSE）最小的时代作为最终模型——也是最不可能产生偏倚结果的模型——用于CNN和hCNN。巧合的是，这些“无偏”模型来自于训练时代相当接近的时期，CNN和hCNN的时代分别为520和524（$\mathcal{E}\approx
    0.95$）。如果选择定义一个在所有模型和时代上平均的二维误差椭圆，则选择相同的hCNN模型，但更倾向于具有略微更紧的误差条和更显著偏移的CNN模型。
- en: Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the median and middle 68% predictions for each of the 40 cosmologies represented
    in the validation set at these unbiased epochs. As expected, the model visibly
    pulls toward the mean for outlying values of $\sigma_{8}$ and $\Omega_{m}$. The
    CNN and hCNN produce tighter correlations between the true and predicted values
    than does the NN.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")显示了这些无偏时代验证集中40种宇宙学模型的中位数和中间68%预测。正如预期的那样，模型明显趋向于平均值，特别是在$\sigma_{8}$和$\Omega_{m}$的异常值。CNN和hCNN在真实值和预测值之间产生了比NN更紧密的相关性。
- en: 4.2.2 Planck Testing Set Results
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 Planck测试集结果
- en: '![Refer to caption](img/3ae723524bbaa610ea2e2146158e7405.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3ae723524bbaa610ea2e2146158e7405.png)'
- en: 'Figure 7: The MSE of the validation set, calculated in equations [5](#S4.E5
    "In 4.2.1 Unbiased Model Selection ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    and [6](#S4.E6 "In 4.2.1 Unbiased Model Selection ‣ 4.2 Testing Set Results ‣
    4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy
    Redshift Surveys"), is tightly correlated with the testing set error $\mathcal{Z}_{Planck}$.
    Shown are the binned median and 68% scatter for the CNN (blue dash) and hCNN (purple
    solid). The values tabulated here are restricted to epochs 501-550 ($0.91<\mathcal{E}\leq
    1.0$). The tighter correlation between low-MSE and low-$\mathcal{Z}_{Planck}$
    models is mildly more pronounced for the hCNN, suggesting that the hCNN might
    be a more robust approach for producing unbiased results.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：验证集的MSE，计算公式见方程[5](#S4.E5 "In 4.2.1 Unbiased Model Selection ‣ 4.2 Testing
    Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")和[6](#S4.E6 "In 4.2.1 Unbiased Model Selection ‣
    4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")，与测试集误差$\mathcal{Z}_{Planck}$ 紧密相关。图中展示了CNN（蓝色虚线）和hCNN（紫色实线）的分箱中位数和68%散布。这里列出的值限制在时代501-550（$0.91<\mathcal{E}\leq
    1.0$）。低MSE与低-$\mathcal{Z}_{Planck}$ 模型之间的更紧密相关性在hCNN中稍微更明显，暗示hCNN可能是产生无偏结果的更稳健的方法。
- en: '![Refer to caption](img/5826831ae53dae6ee4c825af09153972.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5826831ae53dae6ee4c825af09153972.png)'
- en: 'Figure 8: Testing set predictions of the NN (green dotted), CNN (blue dashed),
    and hCNN (purple filled); shown are the 68% and 95% error ellipses. The NN is
    heavily influenced by the degeneracy of the training simulations (gray x’s) in
    the $\sigma_{8}$-$\Omega_{m}$ plane, and predicts cosmological parameters that
    are significantly biased toward the mean. The CNN and hCNN have tighter error
    ellipses and smaller biases. The bias toward the mean is mildly smaller for the
    hCNN (white circle denoting the center of the error ellipse) compared to the CNN
    (white square).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：NN（绿色虚线）、CNN（蓝色虚线）和 hCNN（紫色填充）的测试集预测；显示了68%和95%的误差椭圆。NN受到训练模拟（灰色 x 标记）在
    $\sigma_{8}$-$\Omega_{m}$ 平面上的退化影响，预测的宇宙学参数显著偏向于均值。CNN 和 hCNN 的误差椭圆更紧密，偏差更小。与
    CNN（白色方块）相比，hCNN（白色圆圈标记误差椭圆中心）对均值的偏差略小。
- en: Recall that the training set comprises mock observations built from 40 matched-phase
    cosmological simulations, while the validation set comprises mock observations
    from a unique portion ($z<200h^{-1}\,\mathrm{Mpc}$) of those same simulations.
    In contrast, the testing set comprises mock observations from non-matched-phase
    simulations at the Planck cosmology which were populated with galaxies according
    to an HOD not yet seen by the trained model. With previously unseen cosmological
    parameters, HOD, and initial conditions, the Planck testing set is a more fair
    test of expected error and biases under a realistic set of conditions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，训练集由 40 个匹配相位宇宙学模拟生成的模拟观测组成，而验证集由这些模拟的唯一部分（$z<200h^{-1}\,\mathrm{Mpc}$）生成的模拟观测组成。相比之下，测试集由在
    Planck 宇宙学下的非匹配相位模拟生成的模拟观测组成，这些模拟中填充的星系使用了训练模型尚未见过的 HOD。由于宇宙学参数、HOD 和初始条件均为之前未见过，Planck
    测试集在现实条件下提供了一个更公正的期望误差和偏差测试。
- en: In the previous section, we posited that the MSE of the validation set might
    serve as a fair proxy assessment for selecting an unbiased model to apply to an
    unseen cosmology. Indeed, the validation MSE and the $\mathcal{Z}$ value for the
    Planck testing data (denoted $\mathcal{Z}_{Planck}$), are highly correlated, as
    shown in Figure [7](#S4.F7 "Figure 7 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2
    Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"). The $\log(MSE)$-$\log(\mathcal{Z}_{Planck})$
    distribution has a Pearson R correlation coefficient of 0.88 for the CNN and a
    slightly tighter correlation of 0.93 for the hCNN. There is no strong evidence
    of evolution in the MSE-$\mathcal{Z}_{Planck}$ plane as a function of epoch; while
    low MSE is correlated with low $\mathcal{Z}_{Planck}$, the model is not taking
    a slow and steady march toward high or low MSE as it trains during epochs 501-550\.
    The model’s loss function should drive a decrease in mean absolute error across
    the 40 training cosmologies as it trains, while the test shown assesses a different
    measure of the goodness of fit.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们假设验证集的均方误差（MSE）可能作为选择一个无偏模型以应用于未见过的宇宙学的合理代理评估。实际上，验证 MSE 和 Planck 测试数据的
    $\mathcal{Z}$ 值（记作 $\mathcal{Z}_{Planck}$）高度相关，如图 [7](#S4.F7 "图 7 ‣ 4.2.2 Planck
    测试集结果 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 一种用于宇宙学约束的混合深度学习方法") 所示。$\log(MSE)$-$\log(\mathcal{Z}_{Planck})$
    分布的 Pearson R 相关系数为 CNN 的 0.88，hCNN 的相关系数略紧密，为 0.93。MSE-$\mathcal{Z}_{Planck}$
    平面上没有明显的演化证据作为时代的函数；尽管低 MSE 与低 $\mathcal{Z}_{Planck}$ 相关，但模型在 501-550 世代的训练过程中并未缓慢而稳定地朝着高或低
    MSE 方向前进。模型的损失函数应驱动训练过程中跨 40 个训练宇宙模型的平均绝对误差降低，而所展示的测试评估了不同的拟合优度度量。
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing
    Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") shows the cosmological constraints for the NN,
    CNN, and hCNN. Despite the goodness of training suggested by the results in Figures
    [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine
    Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"), [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"),
    and [6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"),
    the NN never moves beyond predictions that are heavily influenced by the degeneracy
    of the training simulations. This is, perhaps, unsurprising. The power spectrum
    on which it is trained is calculated from a relatively small volume, $\sim 0.07\,h^{-3}\,\mathrm{Gpc}^{3}$,
    in contrast with the effective volume of $\sim 6\,\mathrm{Gpc}^{3}$ of the SDSS
    DR11 Baryon Oscillation Spectroscopic Survey (BOSS) observation (Gil-Marín et al.,
    [2015](#bib.bib24)). The volume of the mock observations used in this work is
    too small to isolate the baryon acoustic peak and reliably measure the acoustic
    scale. As a result, while the NN predicts $\sigma_{8}$ in an unbiased way, its
    predictions for $\Omega_{m}$ are biased very low and pull toward the mean $\Omega_{m}$
    of training simulations.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set
    Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")展示了NN、CNN和hCNN的宇宙学约束。尽管图[4](#S3.F4 "Figure 4 ‣ 3.2.1
    Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")、[5](#S4.F5
    "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys")和[6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results
    ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys")中的结果表明了训练的优良性，但NN的预测始终受到训练模拟的退化性严重影响。这也许并不令人惊讶。它所训练的功率谱是从相对较小的体积$\sim
    0.07\,h^{-3}\,\mathrm{Gpc}^{3}$中计算得出的，而与之相比，SDSS DR11 Baryon Oscillation Spectroscopic
    Survey (BOSS)观察的有效体积约为$\sim 6\,\mathrm{Gpc}^{3}$（Gil-Marín等， [2015](#bib.bib24)）。本研究中使用的虚拟观测体积过小，无法孤立出重子声学峰并可靠地测量声学尺度。因此，尽管NN以无偏方式预测$\sigma_{8}$，但其对$\Omega_{m}$的预测则偏低，并向训练模拟的均值$\Omega_{m}$拉拽。'
- en: Compared to the NN, the CNN and hCNN predictions are substantially unbiased.
    The cosmological constraints in Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing
    Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys"), as well as the sample
    of low-$\mathcal{Z}_{Planck}$ models in Figure [7](#S4.F7 "Figure 7 ‣ 4.2.2 Planck
    Testing Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") suggest that
    the vector features included in the hCNN may make the model more robust to biasing,
    though the evidence for the effects of bias as a function of vector features is
    not strong.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于NN，CNN和hCNN的预测结果显著地不受偏见影响。图[8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set
    Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")中的宇宙学约束，以及图[7](#S4.F7
    "Figure 7 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set Results ‣ 4 Results
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")中的低-$\mathcal{Z}_{Planck}$模型样本，表明hCNN中包含的矢量特征可能使模型对偏差更具鲁棒性，尽管矢量特征对偏差影响的证据并不强。
- en: Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set
    Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") tabulates the simulation parameters and testing
    set results. For reference, we include the Planck testing set true values; recall
    that all simulations in the Planck suite of simulations were run at identical
    cosmologies, so the scatter of these values is $0$. Table [1](#S4.T1 "Table 1
    ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    also gives parameters that describe the distribution of the training data for
    reference. These include the training set mean $\sigma_{8}$, mean $\Omega_{m}$,
    and the standard deviation of these, and are used as a benchmark for how the distribution
    of simulated cosmologies compares to the error bars presented.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "表 1 ‣ 4.2.2 Planck 测试集结果 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 基于深度学习的宇宙学约束混合方法")
    列出了模拟参数和测试集结果。为了参考，我们包括了 Planck 测试集的真实值；回顾一下，Planck 系列的所有模拟都是在相同的宇宙学模型下运行的，因此这些值的散布是
    $0$。表 [1](#S4.T1 "表 1 ‣ 4.2.2 Planck 测试集结果 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 基于深度学习的宇宙学约束混合方法")
    还给出了描述训练数据分布的参数，供参考。这些包括训练集均值 $\sigma_{8}$、均值 $\Omega_{m}$ 以及这些的标准差，并用作模拟宇宙学分布与所呈现误差条对比的基准。
- en: For the trio of ML models, the mean ($\bar{x}$), offset ($\bar{x}-x_{\textit{Planck}}$),
    standard deviation of the predictions (denoted $\sigma$), and 1D $z$-score (offset$/\sigma$)
    are also given. The NN is the most biased of the trio, particularly in $\Omega_{m}$,
    with the mean prediction $\sim$1.3-$\sigma$ away from the true value. From the
    bias and error bars associated with the NN, we can conclude that the box volume
    is likely not large enough for the power spectrum to be diagnostic. Moving to
    a larger mock observations that can more reliably measure the acoustic scale is
    likely to improve the NN technique.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三种 ML 模型，给出了均值（$\bar{x}$）、偏移量（$\bar{x}-x_{\textit{Planck}}$）、预测的标准差（记作 $\sigma$）以及
    1D $z$-score（偏移量$/\sigma$）。在这三者中，NN 的偏差最大，特别是在 $\Omega_{m}$ 中，均值预测距离真实值约 $1.3-\sigma$。根据
    NN 的偏差和误差条，我们可以得出结论，盒子体积可能不足以使功率谱具有诊断性。转向更大的模拟观测，能够更可靠地测量声学尺度，可能会改善 NN 技术。
- en: The CNN and hCNN, on the other hand, both predict $\sigma_{8}$ to within 3%
    and $\Omega_{m}$ to within 4%. The CNN and hCNN error bars are similarly sized,
    but the hCNN exhibits a bias that is smaller than the CNN by about a factor of
    $2$. However, the bias in both the CNN and hCNN are small, and further studies
    on larger mock observations are needed to make strong claims about the potential
    de-biasing advantage of the hCNN architecture.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CNN 和 hCNN 的预测均在 $\sigma_{8}$ 误差范围内 3% 和 $\Omega_{m}$ 误差范围内 4%。CNN 和 hCNN
    的误差条大小相似，但 hCNN 的偏差比 CNN 小约 $2$ 倍。然而，CNN 和 hCNN 的偏差都很小，需要对更大模拟观测进行进一步研究，以便对 hCNN
    架构的去偏差潜力做出有力的声明。
- en: 'Table 1: Results Summary'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：结果总结
- en: '|  | $\sigma_{8}$ | $\Omega_{m}$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{8}$ | $\Omega_{m}$ |'
- en: '| --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | mean | offset | $\sigma$ | $z$ | mean | offset | $\sigma$ | $z$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 均值 | 偏移量 | $\sigma$ | $z$ | 均值 | 偏移量 | $\sigma$ | $z$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Training Set | $0.818$ |  $\cdots$ | $0.083$ |  $\cdots$ | $0.303$ |  $\cdots$
    | $0.027$ |  $\cdots$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | $0.818$ |  $\cdots$ | $0.083$ |  $\cdots$ | $0.303$ |  $\cdots$ | $0.027$
    |  $\cdots$ |'
- en: '| Planck Testing Set | $0.830$ |  $\cdots$ |  $\cdots$ |  $\cdots$ | $0.314$
    |  $\cdots$ |  $\cdots$ |  $\cdots$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Planck 测试集 | $0.830$ |  $\cdots$ |  $\cdots$ |  $\cdots$ | $0.314$ |  $\cdots$
    |  $\cdots$ |  $\cdots$ |'
- en: '| NN | $0.825$ | $0.005$ | $0.035$ | $0.147$ | $0.299$ | $0.015$ | $0.012$
    | $1.307$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| NN | $0.825$ | $0.005$ | $0.035$ | $0.147$ | $0.299$ | $0.015$ | $0.012$
    | $1.307$ |'
- en: '| CNN | $0.824$ | $0.006$ | $0.022$ | $0.278$ | $0.311$ | $0.003$ | $0.012$
    | $0.268$ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| CNN | $0.824$ | $0.006$ | $0.022$ | $0.278$ | $0.311$ | $0.003$ | $0.012$
    | $0.268$ |'
- en: '| hCNN | $0.827$ | $0.003$ | $0.023$ | $0.144$ | $0.312$ | $0.002$ | $0.012$
    | $0.121$ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| hCNN | $0.827$ | $0.003$ | $0.023$ | $0.144$ | $0.312$ | $0.002$ | $0.012$
    | $0.121$ |'
- en: 5 Discussion & Conclusion
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与结论
- en: We have presented a trio of ML approaches for learning $\sigma_{8}$ and $\Omega_{m}$
    from a mock 3D galaxy survey. The neural network (NN) uses the binned power spectrum
    as input, and is processed through a fully connected neural network architecture.
    The convolutional neural network (CNN) uses a spatially binned 3D galaxy distribution;
    this is processed through a series of convolutions and pooling, and finally through
    a fully connected network. The hybrid CNN (hCNN) merges the two.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了三种机器学习方法用于从模拟的 3D 星系调查中学习 $\sigma_{8}$ 和 $\Omega_{m}$。神经网络（NN）使用分箱的功率谱作为输入，并通过一个全连接的神经网络架构进行处理。卷积神经网络（CNN）使用空间分箱的
    3D 星系分布；这通过一系列卷积和池化处理，最后通过一个全连接网络。混合 CNN（hCNN）将这两者合并。
- en: The methods are trained and tested on a sample of mock surveys are built on
    the AbacusCosmos suite of cosmological $N$-body simulations, and the mock surveys
    include a variety of galaxy formation scenarios through the implementation of
    generalized halo occupation distributions (HODs). The full training sample spans
    a large parameter space — 6 cosmological parameters and 6 HOD parameters.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在 AbacusCosmos 系列的宇宙学 $N$-体模拟上进行训练和测试，模拟数据样本包括通过实现广义晕占据分布（HODs）构建的各种星系形成场景。完整的训练样本涵盖了一个广泛的参数空间——6
    个宇宙学参数和 6 个 HOD 参数。
- en: We describe a number of best practices for preventing the a 3D CNN or 3D hCNN
    from memorizing structure and producing overly-optimistic results on the validation
    data. Most important is setting aside an independent portion of all simulations
    as a validation set to assess the goodness of fit. This validation set should
    ideally drawn from the same portion of the box to prevent the deep network from
    memorizing correlated structure across simulations stemming from simulations with
    matched initial phases. Other best practices include recentering the box, aggressive
    pooling to restrict the models’ knowledge of slab-size length scales, subsampling
    the galaxy catalog to prevent the model from learning from the aggregate number
    of galaxies within a volume, and employing the standard suite of axial flips and
    rotations to account for rotational invariance.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了若干最佳实践，以防止 3D CNN 或 3D hCNN 记忆结构并在验证数据上产生过于乐观的结果。最重要的是将所有模拟的独立部分留作验证集，以评估拟合优度。这个验证集应理想地从箱子的相同部分抽取，以防止深度网络记忆来自初始阶段匹配的模拟的相关结构。其他最佳实践包括重新中心化箱子、通过积极的池化限制模型对厚度尺度的知识、对星系目录进行子采样以防止模型从体积内的星系总数中学习，并使用标准的轴向翻转和旋转来考虑旋转不变性。
- en: We have shown that the validation set MSE is a useful proxy for selecting a
    model that will produce unbiased estimates of the cosmological parameters, even
    when presented with previously unseen cosmological and HOD parameters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了验证集的均方误差（MSE）是选择能够产生无偏宇宙学参数估计的模型的有用代理，即使面对以前未见过的宇宙学和 HOD 参数。
- en: 'The model is limited by the availability of simulated data: it is trained and
    tested on relatively small volumes ($\sim 0.07\,h^{-3}\,\mathrm{Gpc}^{3}$, which
    is 1/20 of the simulation box volume). Furthermore, we train with only 40 training
    simulations at a variety of cosmologies that vary in $\Omega_{CDM}\,h^{2}$, $\Omega_{b}\,h^{2}$,
    $\sigma_{8}$, $H_{0}$, $w_{0}$, and $n_{s}$, which have been populated with galaxies
    according to a flexible HOD with 6 parameters. Yet, even within these limitations
    — the small volumes and large cosmological and HOD parameter space — we have shown
    that it is possible to robustly train a model that can learn $\sigma_{8}$ and
    $\Omega_{m}$ directly from a catalog of galaxies.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 模型受限于模拟数据的可用性：它在相对较小的体积（$\sim 0.07\,h^{-3}\,\mathrm{Gpc}^{3}$，即模拟箱体积的 1/20）上进行训练和测试。此外，我们仅在各种不同的宇宙学条件下用
    40 个训练模拟进行训练，这些宇宙学条件在 $\Omega_{CDM}\,h^{2}$、$\Omega_{b}\,h^{2}$、$\sigma_{8}$、$H_{0}$、$w_{0}$
    和 $n_{s}$ 上有所不同，并根据具有 6 个参数的灵活 HOD 填充了星系。即便在这些限制条件下——小体积和大的宇宙学及 HOD 参数空间——我们已经证明可以稳健地训练出一个模型，直接从星系目录中学习
    $\sigma_{8}$ 和 $\Omega_{m}$。
- en: Developing more realistic mock observations that span the cosmological and galaxy
    formation parameter space is an essential next step for applying 3D hybrid CNNs
    to observational data. These extensions to the existing mock observations include
    adopting more diversity in cosmological parameters, taking advantage of larger
    training mock observations, employing additional flexibility in galaxy models,
    and modeling real survey embeddings. As such training data become available, 3D
    hybrid CNNs have the potential to become a powerful tool for extracting cosmological
    information from next-generation spectroscopic surveys.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 开发更为现实的模拟观测，这些观测涵盖了宇宙学和星系形成的参数空间，是将 3D 混合卷积神经网络（CNN）应用于观测数据的一个重要步骤。这些对现有模拟观测的扩展包括采用更多样化的宇宙学参数、利用更大规模的训练模拟观测、在星系模型中引入额外的灵活性以及对实际调查的嵌入建模。随着此类训练数据的出现，3D
    混合 CNN 有潜力成为从下一代光谱调查中提取宇宙学信息的强大工具。
- en: We thank Alexei Efros, Melanie Fernandez, Zoltán Haiman, Paul La Plante, José
    Manuel, Szymon Nakoneczny, Junier Oliva, Barnabás Póczos, Siamak Ravanbakhsh,
    Dezsö Ribli, Alexey Vikhlinin, and Javier Zazo for their helpful feedback on this
    project.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Alexei Efros、Melanie Fernandez、Zoltán Haiman、Paul La Plante、José Manuel、Szymon
    Nakoneczny、Junier Oliva、Barnabás Póczos、Siamak Ravanbakhsh、Dezsö Ribli、Alexey
    Vikhlinin 和 Javier Zazo 对本项目的宝贵反馈。
- en: References
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abadi et al. (2016) Abadi, M., Barham, P., Chen, J., et al. 2016, in 12th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 16), 265–283
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi 等人（2016）Abadi, M., Barham, P., Chen, J., 等. 2016, 在第十二届 USENIX 操作系统设计与实现研讨会（OSDI
    16），265–283
- en: Alam et al. (2017) Alam, S., Ata, M., Bailey, S., et al. 2017, MNRAS, 470, 2617
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alam 等人（2017）Alam, S., Ata, M., Bailey, S., 等. 2017, MNRAS, 470, 2617
- en: Amendola et al. (2013) Amendola, L., Appleby, S., Bacon, D., et al. 2013, Living
    Reviews in Relativity, 16, 6
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amendola 等人（2013）Amendola, L., Appleby, S., Bacon, D., 等. 2013, 《相对论生活评论》，16,
    6
- en: Aragon-Calvo (2019) Aragon-Calvo, M. A. 2019, MNRAS, 484, 5771
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aragon-Calvo（2019）Aragon-Calvo, M. A. 2019, MNRAS, 484, 5771
- en: Bacon et al. (2000) Bacon, D. J., Refregier, A. R., & Ellis, R. S. 2000, MNRAS,
    318, 625
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacon 等人（2000）Bacon, D. J., Refregier, A. R., & Ellis, R. S. 2000, MNRAS, 318,
    625
- en: 'Behroozi et al. (2012) Behroozi, P., Wechsler, R., & Wu, H.-Y. 2012, Rockstar:
    Phase-space halo finder, Astrophysics Source Code Library, ascl:1210.008'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behroozi 等人（2012）Behroozi, P., Wechsler, R., & Wu, H.-Y. 2012, Rockstar: 相空间暗物质晕探测器,
    天体物理学源代码库, ascl:1210.008'
- en: Beltz-Mohrmann et al. (2019) Beltz-Mohrmann, G. D., Berlind, A. A., & Szewciw,
    A. O. 2019, arXiv e-prints, arXiv:1908.11448
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beltz-Mohrmann 等人（2019）Beltz-Mohrmann, G. D., Berlind, A. A., & Szewciw, A.
    O. 2019, arXiv e-prints, arXiv:1908.11448
- en: Berger & Stein (2019) Berger, P., & Stein, G. 2019, MNRAS, 482, 2861
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berger & Stein（2019）Berger, P., & Stein, G. 2019, MNRAS, 482, 2861
- en: Berlind & Weinberg (2002) Berlind, A. A., & Weinberg, D. H. 2002, ApJ, 575,
    587
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berlind & Weinberg（2002）Berlind, A. A., & Weinberg, D. H. 2002, ApJ, 575, 587
- en: Cacciato et al. (2013) Cacciato, M., van den Bosch, F. C., More, S., Mo, H.,
    & Yang, X. 2013, MNRAS, 430, 767
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cacciato 等人（2013）Cacciato, M., van den Bosch, F. C., More, S., Mo, H., & Yang,
    X. 2013, MNRAS, 430, 767
- en: Chollet (2015) Chollet, F. 2015, keras, [https://github.com/fchollet/keras](https://github.com/fchollet/keras)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet（2015）Chollet, F. 2015, keras, [https://github.com/fchollet/keras](https://github.com/fchollet/keras)
- en: Cole et al. (2005) Cole, S., Percival, W. J., Peacock, J. A., et al. 2005, MNRAS,
    362, 505
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cole 等人（2005）Cole, S., Percival, W. J., Peacock, J. A., 等. 2005, MNRAS, 362,
    505
- en: Dattilo et al. (2019) Dattilo, A., Vanderburg, A., Shallue, C. J., et al. 2019,
    arXiv e-prints, arXiv:1903.10507
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dattilo 等人（2019）Dattilo, A., Vanderburg, A., Shallue, C. J., 等. 2019, arXiv
    e-prints, arXiv:1903.10507
- en: de Haan et al. (2016) de Haan, T., Benson, B. A., Bleem, L. E., et al. 2016,
    ApJ, 832, 95
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Haan 等人（2016）de Haan, T., Benson, B. A., Bleem, L. E., 等. 2016, ApJ, 832,
    95
- en: de Jong et al. (2014) de Jong, R. S., Barden, S., Bellido-Tirado, O., et al.
    2014, in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference
    Series, Vol. 9147, Proc. SPIE, 91470M
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Jong 等人（2014）de Jong, R. S., Barden, S., Bellido-Tirado, O., 等. 2014, 在光学与光电仪器学会（SPIE）会议系列，第
    9147 卷，Proc. SPIE, 91470M
- en: DES Collaboration et al. (2017) DES Collaboration, Abbott, T. M. C., Abdalla,
    F. B., et al. 2017, ArXiv e-prints, arXiv:1708.01530
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DES 合作组等（2017）DES 合作组，Abbott, T. M. C., Abdalla, F. B., 等. 2017, ArXiv e-prints,
    arXiv:1708.01530
- en: DESI Collaboration et al. (2016) DESI Collaboration, Aghamousa, A., Aguilar,
    J., et al. 2016, arXiv e-prints, arXiv:1611.00036
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DESI 合作组等（2016）DESI 合作组，Aghamousa, A., Aguilar, J., 等. 2016, arXiv e-prints,
    arXiv:1611.00036
- en: Eisenstein et al. (2005) Eisenstein, D. J., Zehavi, I., Hogg, D. W., et al.
    2005, ApJ, 633, 560
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eisenstein 等人（2005）Eisenstein, D. J., Zehavi, I., Hogg, D. W., 等. 2005, ApJ,
    633, 560
- en: Fluri et al. (2019) Fluri, J., Kacprzak, T., Lucchi, A., et al. 2019, arXiv
    e-prints, arXiv:1906.03156
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluri 等人（2019）Fluri, J., Kacprzak, T., Lucchi, A., 等. 2019, arXiv e-prints,
    arXiv:1906.03156
- en: Fukushima & Miyake (1982) Fukushima, K., & Miyake, S. 1982, in Competition and
    cooperation in neural nets (Springer), 267–285
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fukushima & Miyake (1982) Fukushima, K., & Miyake, S. 1982, 在《神经网络中的竞争与合作》(Springer)
    中, 267–285
- en: Gao et al. (2005) Gao, L., Springel, V., & White, S. D. M. 2005, MNRAS, 363,
    L66
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2005) Gao, L., Springel, V., & White, S. D. M. 2005, MNRAS, 363, L66
- en: Garrison et al. (2018) Garrison, L. H., Eisenstein, D. J., Ferrer, D., et al.
    2018, The Astrophysical Journal Supplement Series, 236, 43
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garrison 等人 (2018) Garrison, L. H., Eisenstein, D. J., Ferrer, D., 等人. 2018,
    《天体物理学杂志补刊系列》，236, 43
- en: Garrison et al. (2019) Garrison, L. H., Eisenstein, D. J., & Pinto, P. A. 2019,
    MNRAS, 485, 3370
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garrison 等人 (2019) Garrison, L. H., Eisenstein, D. J., & Pinto, P. A. 2019,
    MNRAS, 485, 3370
- en: Gil-Marín et al. (2015) Gil-Marín, H., Noreña, J., Verde, L., et al. 2015, MNRAS,
    451, 539
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gil-Marín 等人 (2015) Gil-Marín, H., Noreña, J., Verde, L., 等人. 2015, MNRAS, 451,
    539
- en: Gupta et al. (2018) Gupta, A., Matilla, J. M. Z., Hsu, D., & Haiman, Z. 2018,
    Phys. Rev. D, 97, 103515
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 (2018) Gupta, A., Matilla, J. M. Z., Hsu, D., & Haiman, Z. 2018, Phys. Rev. D,
    97, 103515
- en: He et al. (2018) He, S., Li, Y., Feng, Y., et al. 2018, arXiv e-prints, arXiv:1811.06533
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 (2018) He, S., Li, Y., Feng, Y., 等人. 2018, arXiv 电子预印本, arXiv:1811.06533
- en: Hearin et al. (2016) Hearin, A. P., Zentner, A. R., van den Bosch, F. C., Campbell,
    D., & Tollerud, E. 2016, MNRAS, 460, 2552
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hearin 等人 (2016) Hearin, A. P., Zentner, A. R., van den Bosch, F. C., Campbell,
    D., & Tollerud, E. 2016, MNRAS, 460, 2552
- en: Hikage et al. (2019) Hikage, C., Oguri, M., Hamana, T., et al. 2019, PASJ, 71,
    43
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hikage 等人 (2019) Hikage, C., Oguri, M., Hamana, T., 等人. 2019, PASJ, 71, 43
- en: Hildebrandt et al. (2018) Hildebrandt, H., Köhlinger, F., van den Busch, J. L.,
    et al. 2018, arXiv e-prints, arXiv:1812.06076
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hildebrandt 等人 (2018) Hildebrandt, H., Köhlinger, F., van den Busch, J. L.,
    等人. 2018, arXiv 电子预印本, arXiv:1812.06076
- en: Ho et al. (2019) Ho, M., Rau, M. M., Ntampaka, M., et al. 2019, arXiv e-prints,
    arXiv:1902.05950
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人 (2019) Ho, M., Rau, M. M., Ntampaka, M., 等人. 2019, arXiv 电子预印本, arXiv:1902.05950
- en: Huchra et al. (1990) Huchra, J. P., Geller, M. J., de Lapparent, V., & Corwin,
    Harold G., J. 1990, ApJS, 72, 433
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huchra 等人 (1990) Huchra, J. P., Geller, M. J., de Lapparent, V., & Corwin, Harold G.,
    J. 1990, ApJS, 72, 433
- en: Jeong (2010) Jeong, D. 2010, PhD thesis, University of Texas at Austin
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeong (2010) Jeong, D. 2010, 博士论文, 德克萨斯大学奥斯汀分校
- en: Ji et al. (2013) Ji, S., Xu, W., Yang, M., & Yu, K. 2013, IEEE transactions
    on pattern analysis and machine intelligence, 35, 221
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等人 (2013) Ji, S., Xu, W., Yang, M., & Yu, K. 2013, IEEE 计算机视觉与模式识别学报, 35,
    221
- en: Kaiser et al. (2000) Kaiser, N., Wilson, G., & Luppino, G. A. 2000, arXiv e-prints,
    astro
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser 等人 (2000) Kaiser, N., Wilson, G., & Luppino, G. A. 2000, arXiv 电子预印本,
    astro
- en: Kamnitsas et al. (2016) Kamnitsas, K., Ledig, C., Newcombe, V. F. J., et al.
    2016, CoRR, abs/1603.05959, arXiv:1603.05959
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamnitsas 等人 (2016) Kamnitsas, K., Ledig, C., Newcombe, V. F. J., 等人. 2016,
    CoRR, abs/1603.05959, arXiv:1603.05959
- en: Kingma & Ba (2014) Kingma, D. P., & Ba, J. 2014, ArXiv e-prints, arXiv:1412.6980
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Ba (2014) Kingma, D. P., & Ba, J. 2014, ArXiv 电子预印本, arXiv:1412.6980
- en: Kobayashi et al. (2019) Kobayashi, Y., Nishimichi, T., Takada, M., & Takahashi,
    R. 2019, arXiv e-prints, arXiv:1907.08515
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kobayashi 等人 (2019) Kobayashi, Y., Nishimichi, T., Takada, M., & Takahashi,
    R. 2019, arXiv 电子预印本, arXiv:1907.08515
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012,
    in Advances in Neural Information Processing Systems 25, ed. F. Pereira, C. J. C.
    Burges, L. Bottou, & K. Q. Weinberger (Curran Associates, Inc.), 1097–1105
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人 (2012) Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012, 在《神经信息处理系统进展
    25》中, 编辑 F. Pereira, C. J. C. Burges, L. Bottou, & K. Q. Weinberger (Curran Associates,
    Inc.), 1097–1105
- en: Kwan et al. (2015) Kwan, J., Heitmann, K., Habib, S., et al. 2015, ApJ, 810,
    35
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwan 等人 (2015) Kwan, J., Heitmann, K., Habib, S., 等人. 2015, ApJ, 810, 35
- en: La Plante & Ntampaka (2018) La Plante, P., & Ntampaka, M. 2018, ArXiv e-prints,
    arXiv:1810.08211
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: La Plante & Ntampaka (2018) La Plante, P., & Ntampaka, M. 2018, ArXiv 电子预印本,
    arXiv:1810.08211
- en: Lanusse et al. (2018) Lanusse, F., Ma, Q., Li, N., et al. 2018, MNRAS, 473,
    3895
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lanusse 等人 (2018) Lanusse, F., Ma, Q., Li, N., 等人. 2018, MNRAS, 473, 3895
- en: LeCun et al. (1999) LeCun, Y., Haffner, P., Bottou, L., & Bengio, Y. 1999, in
    Shape, contour and grouping in computer vision (Springer), 319–345
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 (1999) LeCun, Y., Haffner, P., Bottou, L., & Bengio, Y. 1999, 在《计算机视觉中的形状、轮廓与分组》(Springer)
    中, 319–345
- en: Mantz et al. (2015) Mantz, A. B., von der Linden, A., Allen, S. W., et al. 2015,
    MNRAS, 446, 2205
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mantz 等人 (2015) Mantz, A. B., von der Linden, A., Allen, S. W., 等人. 2015, MNRAS,
    446, 2205
- en: Mathuriya et al. (2018) Mathuriya, A., Bard, D., Mendygral, P., et al. 2018,
    arXiv e-prints, arXiv:1808.04728
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mathuriya 等人 (2018) Mathuriya, A., Bard, D., Mendygral, P., 等人. 2018, arXiv
    电子预印本, arXiv:1808.04728
- en: More et al. (2013) More, S., van den Bosch, F. C., Cacciato, M., et al. 2013,
    MNRAS, 430, 747
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: More 等人 (2013) More, S., van den Bosch, F. C., Cacciato, M., 等人. 2013, MNRAS,
    430, 747
- en: Naidoo et al. (2019) Naidoo, K., Whiteway, L., Massara, E., et al. 2019, arXiv
    e-prints, arXiv:1907.00989
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naidoo 等人 (2019) Naidoo, K., Whiteway, L., Massara, E., 等人. 2019, arXiv 电子预印本,
    arXiv:1907.00989
- en: Nair & Hinton (2010) Nair, V., & Hinton, G. E. 2010, in Proceedings of the 27th
    international conference on machine learning (ICML-10), 807–814
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair & Hinton（2010） Nair, V., & Hinton, G. E. 2010, 在第27届国际机器学习会议（ICML-10）论文集,
    807–814
- en: Ntampaka et al. (2018) Ntampaka, M., ZuHone, J., Eisenstein, D., et al. 2018,
    arXiv e-prints, arXiv:1810.07703
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ntampaka 等人（2018） Ntampaka, M., ZuHone, J., Eisenstein, D., 等. 2018, arXiv e-prints,
    arXiv:1810.07703
- en: Pan et al. (2019) Pan, S., Liu, M., Forero-Romero, J., et al. 2019, arXiv e-prints,
    arXiv:1908.10590
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人（2019） Pan, S., Liu, M., Forero-Romero, J., 等. 2019, arXiv e-prints, arXiv:1908.10590
- en: Peacock & Smith (2000) Peacock, J. A., & Smith, R. E. 2000, MNRAS, 318, 1144
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peacock & Smith（2000） Peacock, J. A., & Smith, R. E. 2000, MNRAS, 318, 1144
- en: Peel et al. (2018) Peel, A., Lalande, F., Starck, J.-L., et al. 2018, ArXiv
    e-prints, arXiv:1810.11030
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peel 等人（2018） Peel, A., Lalande, F., Starck, J.-L., 等. 2018, ArXiv e-prints,
    arXiv:1810.11030
- en: Percival et al. (2001) Percival, W. J., Baugh, C. M., Bland-Hawthorn, J., et al.
    2001, MNRAS, 327, 1297
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Percival 等人（2001） Percival, W. J., Baugh, C. M., Bland-Hawthorn, J., 等. 2001,
    MNRAS, 327, 1297
- en: Perraudin et al. (2019) Perraudin, N., Defferrard, M., Kacprzak, T., & Sgier,
    R. 2019, Astronomy and Computing, 27, 130
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perraudin 等人（2019） Perraudin, N., Defferrard, M., Kacprzak, T., & Sgier, R.
    2019, 《天文学与计算》, 27, 130
- en: Planck Collaboration et al. (2014a) Planck Collaboration, Ade, P. A. R., Aghanim,
    N., et al. 2014a, A&A, 571, A16
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Planck Collaboration 等人（2014a） Planck Collaboration, Ade, P. A. R., Aghanim,
    N., 等. 2014a, A&A, 571, A16
- en: Planck Collaboration et al. (2014b) —. 2014b, A&A, 571, A24
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Planck Collaboration 等人（2014b） —. 2014b, A&A, 571, A24
- en: Planck Collaboration et al. (2015) —. 2015, ArXiv e-prints, arXiv:1502.01589
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Planck Collaboration 等人（2015） —. 2015, ArXiv e-prints, arXiv:1502.01589
- en: Ravanbakhsh et al. (2017) Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al.
    2017, arXiv e-prints, arXiv:1711.02033
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravanbakhsh 等人（2017） Ravanbakhsh, S., Oliva, J., Fromenteau, S., 等. 2017, arXiv
    e-prints, arXiv:1711.02033
- en: Ribli et al. (2019a) Ribli, D., Ármin Pataki, B., Zorrilla Matilla, J. M., et al.
    2019a, arXiv e-prints, arXiv:1902.03663
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribli 等人（2019a） Ribli, D., Ármin Pataki, B., Zorrilla Matilla, J. M., 等. 2019a,
    arXiv e-prints, arXiv:1902.03663
- en: Ribli et al. (2019b) Ribli, D., Pataki, B. Á., & Csabai, I. 2019b, Nature Astronomy,
    3, 93
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribli 等人（2019b） Ribli, D., Pataki, B. Á., & Csabai, I. 2019b, Nature Astronomy,
    3, 93
- en: Schmelzle et al. (2017) Schmelzle, J., Lucchi, A., Kacprzak, T., et al. 2017,
    arXiv e-prints, arXiv:1707.05167
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmelzle 等人（2017） Schmelzle, J., Lucchi, A., Kacprzak, T., 等. 2017, arXiv e-prints,
    arXiv:1707.05167
- en: Schmidhuber (2014) Schmidhuber, J. 2014, ArXiv e-prints, arXiv:1404.7828
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber（2014） Schmidhuber, J. 2014, ArXiv e-prints, arXiv:1404.7828
- en: Scoccimarro et al. (2001) Scoccimarro, R., Sheth, R. K., Hui, L., & Jain, B.
    2001, ApJ, 546, 20
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scoccimarro 等人（2001） Scoccimarro, R., Sheth, R. K., Hui, L., & Jain, B. 2001,
    ApJ, 546, 20
- en: Shectman et al. (1996) Shectman, S. A., Landy, S. D., Oemler, A., et al. 1996,
    ApJ, 470, 172
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shectman 等人（1996） Shectman, S. A., Landy, S. D., Oemler, A., 等. 1996, ApJ, 470,
    172
- en: Simonyan & Zisserman (2014) Simonyan, K., & Zisserman, A. 2014, CoRR, abs/1409.1556,
    arXiv:1409.1556
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan & Zisserman（2014） Simonyan, K., & Zisserman, A. 2014, CoRR, abs/1409.1556,
    arXiv:1409.1556
- en: Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
    I., & Salakhutdinov, R. 2014, The Journal of Machine Learning Research, 15, 1929
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等人（2014） Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
    & Salakhutdinov, R. 2014, 《机器学习研究杂志》, 15, 1929
- en: Takada et al. (2014) Takada, M., Ellis, R. S., Chiba, M., et al. 2014, PASJ,
    66, R1
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takada 等人（2014） Takada, M., Ellis, R. S., Chiba, M., 等. 2014, PASJ, 66, R1
- en: Tegmark et al. (2004) Tegmark, M., Blanton, M. R., Strauss, M. A., et al. 2004,
    ApJ, 606, 702
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tegmark 等人（2004） Tegmark, M., Blanton, M. R., Strauss, M. A., 等. 2004, ApJ,
    606, 702
- en: van den Bosch et al. (2013) van den Bosch, F. C., More, S., Cacciato, M., Mo,
    H., & Yang, X. 2013, MNRAS, 430, 725
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Bosch 等人（2013） van den Bosch, F. C., More, S., Cacciato, M., Mo, H.,
    & Yang, X. 2013, MNRAS, 430, 725
- en: Van Waerbeke et al. (2000) Van Waerbeke, L., Mellier, Y., Erben, T., et al.
    2000, A&A, 358, 30
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Waerbeke 等人（2000） Van Waerbeke, L., Mellier, Y., Erben, T., 等. 2000, A&A,
    358, 30
- en: Vikhlinin et al. (2009) Vikhlinin, A., Kravtsov, A. V., Burenin, R. A., et al.
    2009, ApJ, 692, 1060
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vikhlinin 等人（2009） Vikhlinin, A., Kravtsov, A. V., Burenin, R. A., 等. 2009,
    ApJ, 692, 1060
- en: Wang et al. (2019) Wang, K., Mao, Y.-Y., Zentner, A. R., et al. 2019, arXiv
    e-prints, arXiv:1903.09656
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2019） Wang, K., Mao, Y.-Y., Zentner, A. R., 等. 2019, arXiv e-prints,
    arXiv:1903.09656
- en: Wechsler et al. (2006) Wechsler, R. H., Zentner, A. R., Bullock, J. S., Kravtsov,
    A. V., & Allgood, B. 2006, ApJ, 652, 71
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wechsler 等人（2006） Wechsler, R. H., Zentner, A. R., Bullock, J. S., Kravtsov,
    A. V., & Allgood, B. 2006, ApJ, 652, 71
- en: Weinberg et al. (2013) Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J.,
    et al. 2013, Phys. Rep., 530, 87
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weinberg 等人（2013） Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J., 等. 2013,
    Phys. Rep., 530, 87
- en: Wittman et al. (2000) Wittman, D. M., Tyson, J. A., Kirkman, D., Dell’Antonio,
    I., & Bernstein, G. 2000, Nature, 405, 143
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wittman 等人（2000） Wittman, D. M., Tyson, J. A., Kirkman, D., Dell’Antonio, I.,
    & Bernstein, G. 2000, Nature, 405, 143
- en: York et al. (2000) York, D. G., Adelman, J., Anderson, John E., J., et al. 2000,
    AJ, 120, 1579
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: York 等 (2000) York, D. G., Adelman, J., Anderson, John E., J., 等. 2000, AJ,
    120, 1579
- en: Yuan et al. (2018a) Yuan, S., Eisenstein, D. J., & Garrison, L. H. 2018a, MNRAS,
    478, 2019
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2018a) Yuan, S., Eisenstein, D. J., & Garrison, L. H. 2018a, MNRAS,
    478, 2019
- en: 'Yuan et al. (2018b) —. 2018b, GRAND-HOD: GeneRalized ANd Differentiable Halo
    Occupation Distribution, ascl:1812.011'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等 (2018b) —. 2018b, GRAND-HOD: GeneRalized ANd Differentiable Halo Occupation
    Distribution, ascl:1812.011'
- en: Yuan et al. (2019) Yuan, S., Eisenstein, D. J., & Leauthaud, A. 2019, arXiv
    e-prints, arXiv:1907.05909
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2019) Yuan, S., Eisenstein, D. J., & Leauthaud, A. 2019, arXiv e-prints,
    arXiv:1907.05909
- en: Zhang et al. (2019) Zhang, X., Wang, Y., Zhang, W., et al. 2019, arXiv e-prints,
    arXiv:1902.05965
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019) Zhang, X., Wang, Y., Zhang, W., 等. 2019, arXiv e-prints, arXiv:1902.05965
- en: Zheng & Weinberg (2007) Zheng, Z., & Weinberg, D. H. 2007, ApJ, 659, 1
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng & Weinberg (2007) Zheng, Z., & Weinberg, D. H. 2007, ApJ, 659, 1
- en: Zheng et al. (2005) Zheng, Z., Berlind, A. A., Weinberg, D. H., et al. 2005,
    ApJ, 633, 791
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2005) Zheng, Z., Berlind, A. A., Weinberg, D. H., 等. 2005, ApJ, 633,
    791
- en: Appendix A On the Life Cycle of CNNs
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 关于 CNN 的生命周期
- en: CNNs are traditionally trained to minimize a loss function such as mean squared
    or absolute error, yet it is not obvious that this is an ideal approach for astronomical
    and cosmological applications. In this section, we present more on the life cycle
    of our CNN and show additional plots that have been useful in interpreting fits
    and designing our two-phase training scheme.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 通常被训练以最小化诸如均方误差或绝对误差的损失函数，但这并不明显对于天文学和宇宙学应用是否是理想的做法。在本节中，我们将介绍我们 CNN 的生命周期更多内容，并展示在解释拟合和设计我们的两阶段训练方案中有用的额外图表。
- en: 'While figures showing traditional metrics can be diagnostic, they can be difficult
    to interpret for models that regress more than one parameter. Such traditional
    figures include error as a function of epoch (e.g., Figure [4](#S3.F4 "Figure
    4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models ‣
    A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")) and 1-to-1 scatter of true and predicted values (e.g., Figure [6](#S4.F6
    "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")). It is concerning
    that typical early stopping routines rely on these test statistics to determine
    when a model is well fit because using such diagnostics blindly can lead to unexpected
    or overpessimistic results.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管显示传统度量的图表可以提供诊断，但对于回归多个参数的模型来说，它们可能难以解释。这些传统图表包括错误随纪元的变化（例如，图 [4](#S3.F4 "图
    4 ‣ 3.2.1 混合 CNN ‣ 3.2 标准 NN ‣ 3 方法：机器学习模型 ‣ 从星系红移调查到宇宙学约束的混合深度学习方法")）和真实值与预测值的
    1 对 1 散点图（例如，图 [6](#S4.F6 "图 6 ‣ 4.2 测试集结果 ‣ 4 结果 ‣ 从星系红移调查到宇宙学约束的混合深度学习方法")）。令人担忧的是，典型的早期停止程序依赖于这些测试统计数据来确定模型是否适合，因为盲目使用这些诊断可能会导致意外或过于悲观的结果。
- en: Figure [9](#A1.F9 "Figure 9 ‣ Appendix A On the Life Cycle of CNNs ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the validation data 2D predictions as a function of epoch. Unsurprisingly,
    at epochs as early as 5, the model has learned to predict a mean value but cannot
    differentiate among models. This is encouraging and expected; the model, which
    is initialized to completely random weights and biases, learned a reasonable values
    for $\sigma_{8}$ and $\Omega_{m}$ in the first few epochs.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9](#A1.F9 "图 9 ‣ 附录 A 关于 CNN 的生命周期 ‣ 从星系红移调查到宇宙学约束的混合深度学习方法") 显示了验证数据的二维预测随纪元的变化。不出所料，在早期如第
    5 纪元时，模型已经学会预测均值，但无法区分不同模型。这是令人鼓舞的，也在预期之中；这个模型从完全随机的权重和偏差初始化开始，在前几个纪元中学到了合理的 $\sigma_{8}$
    和 $\Omega_{m}$ 值。
- en: 'The model predictions at epoch 30, though, are a surprising extension of this
    predition of the mean. In Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2
    Standard NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys"), the error as a function
    of epoch slowly and steadily decreases for the first few epochs, then begins to
    oscillate. At epoch $\sim 30$, this initial plunge has come to an end, and an
    error-based early stopping scheme might suggest that these results are sufficient.
    A one-to-one plot of true and predicted $\sigma_{8}$ and $\Omega_{m}$ will tell
    a similar story — the results bias toward the mean and the scatter is larger than
    is to be desired, but the model has clearly learned trends in the data and a diversity
    of $\sigma_{8}$ and $\Omega_{m}$ values. Yet, when viewed as a scatter plot in
    the $\sigma_{8}$-$\Omega_{m}$ plane (in the top right corner of Figure [9](#A1.F9
    "Figure 9 ‣ Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")), it is clear that
    the CNN has learned a 2D version of predicting the mean: it has produced predictions
    that spread along the degeneracy direction of the training simulations, with the
    predictions arranged in a sensible way (i.e. the predictions of the high-$\sigma_{8}$
    simulations are indeed at high $\sigma_{8}$).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '第30个epoch的模型预测，尽管出乎意料地延伸了对均值的预测。在图[4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣
    3.2 Standard NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys")中，误差作为epoch的函数在前几个epoch中缓慢而稳定地减少，然后开始出现波动。在第$\sim
    30$个epoch时，这一初始下降已结束，而基于误差的早停方案可能会表明这些结果已经足够。一对一的真实与预测$\sigma_{8}$和$\Omega_{m}$的图表也会讲述类似的故事——结果偏向均值，且散布比理想的要大，但模型显然已经学习到了数据中的趋势以及$\sigma_{8}$和$\Omega_{m}$值的多样性。然而，当在$\sigma_{8}$-$\Omega_{m}$平面（图[9](#A1.F9
    "Figure 9 ‣ Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")的右上角）中作为散点图查看时，可以明显看出CNN已经学习到了一种预测均值的二维版本：它生成的预测沿着训练模拟的退化方向展开，且预测以合理的方式排列（即高-$\sigma_{8}$的模拟预测确实位于高$\sigma_{8}$处）。'
- en: 'It is only by delving into a “high error” regime that the CNN starts to make
    progress beyond this tight degeneracy. Between epochs 30 and 175, we see large
    oscillations in MSE. Epoch 100 is shown as an example epochs in this region. Despite
    the fact that Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN
    ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") shows error increasing and oscillating
    in this epoch range, something important and meaningful is happening under the
    surface. The model is starting to produce more diversity in predictions, expanding
    the range of predictions in the direction orthogonal to the degeneracy of the
    simulations. At epoch 175, the predictions are still biased toward the mean, but
    at least span a wider spectrum of possibilities.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '只有深入到“高误差”状态，CNN才开始在这种紧密的退化状态下取得进展。在第30到175个epoch之间，我们看到MSE的巨大波动。第100个epoch作为这一范围内的一个示例。尽管图[4](#S3.F4
    "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")显示该epoch范围内误差在增加和波动，但在表面之下发生了重要而有意义的事情。模型开始生成更多样化的预测，扩展了预测的范围，方向正交于模拟的退化。在第175个epoch时，预测仍然偏向均值，但至少涵盖了更广泛的可能性。'
- en: Here, we can take an alternate timeline and continue with phase 1 of the training
    scheme for a few more epochs. Recall that, in the training scheme presented in
    the main text, we transition to a lower learning rate at epoch 175\. At epochs
    219 and 220 in this alternate timeline, we begin to see the oscillations in bias.
    While the results for epoch 219 look reasonable, the results for epoch 220 are
    biased to very low $\sigma_{8}$; such large swings in biases hint that the step
    size is too large.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以选择一个替代时间线，继续进行训练方案的第1阶段，延续几个epoch。回忆一下，在主文本中提出的训练方案中，我们在第175个epoch时转到较低的学习率。在这个替代时间线中的第219和220个epoch，我们开始看到偏差的波动。虽然第219个epoch的结果看起来合理，但第220个epoch的结果却偏向非常低的$\sigma_{8}$；如此大的偏差波动暗示步长过大。
- en: Another alternative timeline transitions from phase 1 (high learning rate) to
    phase 2 (lower learning rate) as early as epoch 30, with disastrous results. The
    epoch 30 model has not yet learned much beyond the degeneracy of the simulations,
    and when it is moved to a much smaller learning rate, it fails to learn a diversity
    of predictions in the $\sigma_{8}$-$\Omega_{m}$ plane, instead producing predictions
    along a tight curve for many epochs.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种备选时间线从第 1 期（高学习率）过渡到第 2 期（较低学习率），最早在第 30 个周期时产生了灾难性的结果。第 30 个周期的模型尚未学习到超出模拟退化的内容，当将其转移到一个更小的学习率时，它未能在
    $\sigma_{8}$-$\Omega_{m}$ 平面上学习到预测的多样性，而是在许多周期中沿着一个紧密的曲线进行预测。
- en: While they are certainly valuable, traditional methods for understanding how
    well a CNN has fit can be difficult to interpret, particularly when assessing
    models trained to predict multiple parameters. Employing early stopping routines
    that assess a single statistical measurement of error can lead to models that
    have not yet learned a range of predictions in the parameter space. Appropriately
    assessing the diversity of predictions, identifying epochs to stop training, and
    developing intuition for training deep models will an essential step toward properly
    using these powerful tools in astronomical and cosmological applications.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们确实很有价值，但传统的理解 CNN 拟合效果的方法可能难以解释，特别是在评估训练以预测多个参数的模型时。采用早停程序来评估单一的统计误差测量可能会导致模型尚未在参数空间中学习到预测的多样性。适当地评估预测的多样性、确定停止训练的周期，并培养训练深度模型的直觉，将是正确使用这些强大工具在天文学和宇宙学应用中的一个重要步骤。
- en: '| ![Refer to caption](img/eaef8a95b46ea9e9a54fad7c23f46550.png) | ![Refer to
    caption](img/bb71554b9708fcb3b64b6e6512209788.png) |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/eaef8a95b46ea9e9a54fad7c23f46550.png) | ![参考说明](img/bb71554b9708fcb3b64b6e6512209788.png)
    |'
- en: '| ![Refer to caption](img/d1339a07da7f6d64b3a7d634edb13f96.png) | ![Refer to
    caption](img/031afe4cbcbcc4ce55fcf64b4bc8e21c.png) |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/d1339a07da7f6d64b3a7d634edb13f96.png) | ![参考说明](img/031afe4cbcbcc4ce55fcf64b4bc8e21c.png)
    |'
- en: '| ![Refer to caption](img/9e5634cc597c74aee0f37fc22b4f60c2.png) | ![Refer to
    caption](img/2005e518fb212253d75eb153640f027d.png) |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/9e5634cc597c74aee0f37fc22b4f60c2.png) | ![参考说明](img/2005e518fb212253d75eb153640f027d.png)
    |'
- en: 'Figure 9: The life cycle of CNNs. Training data (crosses) are colored according
    to their $\sigma_{8}$ values, and predictions on the validation data (circles)
    are likewise colored according to their *true* (not predicted) $\sigma_{8}$ values.
    Early in training, the model learns reasonable values for $\sigma_{8}$ and $\Omega_{m}$,
    eventually learning a tight degeneracy in this space, and finally achieving a
    more diverse representation of the simulations. Shown are, from top left to bottom
    right, epochs 5, 30, 100, 175, 219, and 220 in phase 1 of the training scheme.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：CNN 的生命周期。训练数据（交叉点）根据其 $\sigma_{8}$ 值着色，验证数据的预测（圆圈）同样根据其*真实*（而非预测的）$\sigma_{8}$
    值着色。在训练初期，模型学习到合理的 $\sigma_{8}$ 和 $\Omega_{m}$ 值，最终在这一空间中学习到紧密的退化，最后实现了对模拟结果的更为多样化的表示。从左上角到右下角分别为训练方案第
    1 期中的第 5、30、100、175、219 和 220 个周期。
