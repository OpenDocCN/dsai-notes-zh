- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1909.10527] A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1909.10527](https://ar5iv.labs.arxiv.org/html/1909.10527)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Michelle Ntampaka Harvard Data Science Initiative, Harvard University, Cambridge,
    MA 02138, USA Center for Astrophysics $|$ Harvard & Smithsonian, Cambridge, MA
    02138, USA Daniel J. Eisenstein Center for Astrophysics $|$ Harvard & Smithsonian,
    Cambridge, MA 02138, USA Sihan Yuan Center for Astrophysics $|$ Harvard & Smithsonian,
    Cambridge, MA 02138, USA Lehman H. Garrison Center for Astrophysics $|$ Harvard
    & Smithsonian, Cambridge, MA 02138, USA Center for Computational Astrophysics,
    Flatiron Institute, New York, NY 10010, USA
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We present a deep machine learning (ML)-based technique for accurately determining
    $\sigma_{8}$ and $\Omega_{m}$ from mock 3D galaxy surveys. The mock surveys are
    built from the AbacusCosmos suite of $N$-body simulations, which comprises 40
    cosmological volume simulations spanning a range of cosmological models, and we
    account for uncertainties in galaxy formation scenarios through the use of generalized
    halo occupation distributions (HODs). We explore a trio of ML models: a 3D convolutional
    neural network (CNN), a power-spectrum-based fully connected network, and a hybrid
    approach that merges the two to combine physically motivated summary statistics
    with flexible CNNs. We describe best practices for training a deep model on a
    suite of matched-phase simulations and we test our model on a completely independent
    sample that uses previously unseen initial conditions, cosmological parameters,
    and HOD parameters. Despite the fact that the mock observations are quite small
    ($\sim 0.07h^{-3}\,\mathrm{Gpc}^{3}$) and the training data span a large parameter
    space (6 cosmological and 6 HOD parameters), the CNN and hybrid CNN can constrain
    $\sigma_{8}$ and $\Omega_{m}$ to $\sim 3\%$ and $\sim 4\%$, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†journal: ApJ'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the $\Lambda$CDM cosmological model, tiny density fluctuations in the early
    Universe evolved into today’s cosmic web of overdense dark matter halos, filaments,
    and sheets. Imprinted on this large-scale structure is information about the underlying
    cosmological model, provided one knows how and where to look. Measurements that
    describe the large scale distribution of matter in the Universe carry information
    about the cosmological model that drove its formation. These measurements include
    descriptions of the spatial distribution and clustering of galaxies (e.g., Huchra
    et al., [1990](#bib.bib31); Shectman et al., [1996](#bib.bib63); Percival et al.,
    [2001](#bib.bib52); Tegmark et al., [2004](#bib.bib67)), the abundance of massive
    galaxy clusters (e.g., Vikhlinin et al., [2009](#bib.bib70); Mantz et al., [2015](#bib.bib43);
    de Haan et al., [2016](#bib.bib14)), the weak gravitational lensing of galaxies
    by intervening large-scale structure (e.g., Bacon et al., [2000](#bib.bib5); Kaiser
    et al., [2000](#bib.bib34); Wittman et al., [2000](#bib.bib74); Van Waerbeke et al.,
    [2000](#bib.bib69); DES Collaboration et al., [2017](#bib.bib16); Hildebrandt
    et al., [2018](#bib.bib29); Hikage et al., [2019](#bib.bib28)), and the length
    scale of baryon acoustic oscillations (e.g., Eisenstein et al., [2005](#bib.bib18);
    Cole et al., [2005](#bib.bib12); Alam et al., [2017](#bib.bib2)). A hallmark difference
    between these and probes of the earlier Universe is non-Gaussianity: though the
    early Universe is well-described by a Gaussian random field (e.g., Planck Collaboration
    et al., [2014a](#bib.bib54), [b](#bib.bib55)), gravitational collapse drives the
    formation of non-Gaussian correlations in the late-time matter distribution. See
    Weinberg et al. ([2013](#bib.bib73)) for a review of these and other observational
    cosmological probes.'
  prefs: []
  type: TYPE_NORMAL
- en: Galaxies live in dark matter halos and are tracers, albeit biased ones, of large-scale
    structure. Large spectroscopic surveys such as the Sloan Digital Sky Survey (SDSS,
    York et al., [2000](#bib.bib75)) have produced maps of the 3D distribution of
    galaxies in the Universe, and upcoming spectroscopic surveys such as the Dark
    Energy Spectroscopic Instrument (DESI, DESI Collaboration et al., [2016](#bib.bib17)),
    Subaru Prime Focus Spectrograph (PFS, Takada et al., [2014](#bib.bib66)), 4-metre
    Multi-Object Spectroscopic Telescope (4MOST, de Jong et al., [2014](#bib.bib15)),
    and Euclid (Amendola et al., [2013](#bib.bib3)) will produce exquisitely detailed
    maps of the sky. The galaxy power spectrum provides one handle on summarizing
    and interpreting these 3D galaxy maps and can be used to put constraints on the
    parameters that describe a $\Lambda$CDM cosmology (e.g., Tegmark et al., [2004](#bib.bib67)),
    but care must be taken when disentangling the effects of cosmology and galaxy
    bias (e.g., van den Bosch et al., [2013](#bib.bib68); More et al., [2013](#bib.bib45);
    Cacciato et al., [2013](#bib.bib10)).
  prefs: []
  type: TYPE_NORMAL
- en: Though it is an abundantly useful compression of the information contained in
    the distribution of galaxies, the power spectrum is not a complete accounting
    of this information because the late-time galaxy distribution is not a Gaussian
    random field. The deviations from Gaussian correlations are enormous at small
    length scales ($\lesssim$ a few Mpc), where dark matter halos have collapsed and
    virialized, but remain substantial at intermediate scales due to the cosmic web
    of filaments, walls, and voids. Additional statistics such as the squeezed 3-point
    correlation function (Yuan et al., [2018a](#bib.bib76)), redshift space power
    spectrum (Kobayashi et al., [2019](#bib.bib37)), counts-in-cylinders (Wang et al.,
    [2019](#bib.bib71)), and the minimum spanning tree (Naidoo et al., [2019](#bib.bib46))
    have been shown to be rich in complementary cosmological information by capturing
    non-Gaussian details of the galaxy distribution that are not described by the
    power spectrum alone.
  prefs: []
  type: TYPE_NORMAL
- en: These higher-order statistical descriptions of how galaxies populate 3D space
    typically need to be calibrated on cosmological simulations. Cosmological hydrodynamical
    simulations that trace the formation of galaxies are computationally expensive,
    so a more tractable approach is to use less expensive $N$-body simulations that
    have been populated with galaxies. The can be accomplished through a technique
    that matches galaxies to the simulated structure of dark matter, for example,
    through a halo occupation distribution (HOD, e.g., Peacock & Smith, [2000](#bib.bib50);
    Scoccimarro et al., [2001](#bib.bib62); Berlind & Weinberg, [2002](#bib.bib9);
    Zheng et al., [2005](#bib.bib81)).
  prefs: []
  type: TYPE_NORMAL
- en: Under its simplest assumptions, an HOD uses halo mass as the sole property that
    determines whether a halo will host a particular type of galaxy. The breakdown
    of this assumption is known as galaxy assembly bias, which asserts that mass alone
    is insufficient and that additional environmental and assembly factors come into
    play. These factors include formation time (Gao et al., [2005](#bib.bib21)) and
    halo concentration (Wechsler et al., [2006](#bib.bib72)). Modern HOD implementations
    often provide flexibility to account for assembly bias (e.g., Hearin et al., [2016](#bib.bib27);
    Yuan et al., [2018b](#bib.bib77); Beltz-Mohrmann et al., [2019](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning (ML) offers a number of methods that can find and extract information
    from complex spatial patterns imprinted on the 3D distribution of galaxies. ML,
    therefore, is an enticing approach for inferring cosmological models in spite
    of myriad complicating effects. One promising class of tools for this task are
    Convolutional Neural Networks (CNNs, e.g.  Fukushima & Miyake, [1982](#bib.bib20);
    LeCun et al., [1999](#bib.bib42); Krizhevsky et al., [2012](#bib.bib38); Simonyan
    & Zisserman, [2014](#bib.bib64)), which are often used in image recognition tasks.
    CNNs employ many hidden layers to extract image features such as edges, shapes,
    and textures. Typically, CNNs pair layers of convolution and pooling to extract
    meaningful features from the input images, followed by deep fully connected layers
    to output an image class or numerical label. Because these deep networks learn
    the filters necessary to extract meaningful information from the input images,
    they require very little image preprocessing. See Schmidhuber ([2014](#bib.bib61))
    for a review of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are traditionally applied to 2D images, which may be monochromatic or represented
    in several color bands. 2D CNNs can extract information from non-gaussianities
    in simulated convergence maps, remarkably improving cosmological constraints over
    a more standard statistical approach (e.g., Schmelzle et al., [2017](#bib.bib60);
    Gupta et al., [2018](#bib.bib25); Ribli et al., [2019a](#bib.bib58), [b](#bib.bib59)),
    and recent work has extended this to put cosmological constraints on observations
    using CNNs (Fluri et al., [2019](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: However, the application of CNNs is not limited to flat Euclidean images (e.g.
    Perraudin et al., [2019](#bib.bib53)), nor is it limited to two dimensions. The
    algorithm can be extended to three dimensions, where the third dimension may be,
    for example, temporal (e.g., video input, as in Ji et al., [2013](#bib.bib33))
    or spatial (e.g., a data cube, as in Kamnitsas et al., [2016](#bib.bib35)). Ravanbakhsh
    et al. ([2017](#bib.bib57)) employed the first cosmological application of a 3D
    CNN, showing that the tool can infer the underlying cosmological parameters from
    a simulated 3D dark matter distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We present an application of 3D CNNs to learn cosmological parameters from
    simulated galaxy maps. Our hybrid deep learning architecture learns directly from
    the calculated 2D power spectrum and simultaneously harnesses non-Gaussianities
    by also learning directly from the raw 3D distribution of galaxies. In Section
    [2](#S2 "2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"), we describe our mock observations:
    the suite of cosmological simulations ([2.1](#S2.SS1 "2.1 AbacusCosmos Simulations
    ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys")), the range of HODs applied to these
    simulations ([2.2](#S2.SS2 "2.2 Halo Occupation Distribution ‣ 2 Methods: Mock
    Observations ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From
    Galaxy Redshift Surveys")), the training and validation mock observations ([2.3](#S2.SS3
    "2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")),
    and the carefully constructed and independent test mock observations at the Planck
    cosmology ([2.4](#S2.SS4 "2.4 Planck Testing Set ‣ 2 Methods: Mock Observations
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")). We describe our trio of deep learning architectures, including the
    hybrid method, in Section [3](#S3 "3 Methods: Machine Learning Models ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").
    We present our results in Section [4](#S4 "4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") and a discussion
    and conclusions in Section [5](#S5 "5 Discussion & Conclusion ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").
    Appendix [A](#A1 "Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") is more pedagogical
    in nature; it describes how the range of model predictions evolves with training
    and suggests new tests for assessing a model’s fit.'
  prefs: []
  type: TYPE_NORMAL
- en: '2 Methods: Mock Observations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the AbacusCosmos suite of simulations¹¹1[https://lgarrison.github.io/AbacusCosmos/](https://lgarrison.github.io/AbacusCosmos/)
    (Garrison et al., [2018](#bib.bib22), [2019](#bib.bib23)) to create three data
    sets: a training set, a validation set, and a testing set. The training set is
    used to fit the machine learning model; it spans a range of CDM cosmologies and
    is populated with galaxies in a way to mimic a variety of galaxy formation models.
    The validation set is used to assess how well the machine learning model has fit;
    it also spans a range of cosmological parameters and galaxy formation models.
    The testing set is independent of both the training and validation sets; it is
    at the Planck fiducial cosmology (Planck Collaboration et al., [2015](#bib.bib56)),
    built from simulations with initial conditions not used in the training or validation
    data sets, and populated with galaxies using HODs not used in the training or
    testing data sets. The creation of the three data sets are described in the following
    subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 AbacusCosmos Simulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The AbacusCosmos simulations are a suite of publicly available $N$-body simulations.
    The suite includes the AbacusCosmos 1100box simulations, a sample of large-volume
    $N$-body simulations at a variety of cosmologies, as well as the 1100box Planck
    simulations, a sample of simulations with cosmological parameters consistent with
    the Planck fiducial cosmology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AbacusCosmos 1100box simulations are used to create the training and validation
    sets. This suite of simulations comprises 40 simulations at a variety of cosmologies
    that differ for six cosmological parameters: $\Omega_{CDM}\,h^{2}$, $\Omega_{b}\,h^{2}$,
    $\sigma_{8}$, $H_{0}$, $w_{0}$, and $n_{s}$. The cosmologies for this suite of
    simulations were selected by a Latin hypercube algorithm, and are centered on
    the Planck fiducial cosmology (Planck Collaboration et al., [2015](#bib.bib56)).
    Each simulation has side length $1100h^{-1}\,\mathrm{Mpc}$ and particle mass $4\times
    10^{10}h^{-1}\,\mathrm{M_{\odot}}$. The suite of 40 simulations are phase-matched.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the AbacusCosmos 1100box simulations are used to create the training
    and validation sets, the AbacusCosmos Planck simulations are used to create the
    testing set. These 20 simulations have cosmological parameters consistent with
    Planck Collaboration et al. ([2015](#bib.bib56)): $\Omega_{b}\,h^{2}=0.02222$,
    $\Omega_{m}\,h^{2}=0.14212$, $w_{0}=-1$, $n_{s}=0.9652$, $\sigma_{8}=0.830$, $H_{0}=67.26$,
    $N_{\mathrm{eff}}=3.04$. They have identical side length ($1100h^{-1}\,\mathrm{Mpc}$)
    and particle mass ($4\times 10^{10}h^{-1}\,\mathrm{M_{\odot}}$) to the 1100box
    suite of simulations, but each uses unique initial conditions and none are phase-matched
    to the 1100box simulations. See Garrison et al. ([2018](#bib.bib22)) for more
    details about the AbacusCosmos suite of simulations.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Halo Occupation Distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A halo occupation distribution (HOD) is a way to populate dark matter halos
    with galaxies. In their most basic form, HODs are probabilisitic models that assume
    that halo mass is the sole halo property governing the halo-galaxy connection
    (Berlind & Weinberg, [2002](#bib.bib9)). A standard HOD models the probability
    of a halo hosting a central galaxy, $\overline{n}_{\mathrm{central}}$, and the
    mean number of satellites, $\overline{n}_{\mathrm{satellite}}$, as a function
    of a single halo property, the mass $M$. The standard HOD by Zheng & Weinberg
    ([2007](#bib.bib80)) gives the mean number of central and satellite galaxies as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\overline{n}_{\mathrm{central}}&amp;=\frac{1}{2}\mathrm{erfc}\left[\frac{\ln(M_{\mathrm{cut}}/M)}{\sqrt{2}\sigma}\right]\\[12.91663pt]
    \overline{n}_{\mathrm{satellite}}&amp;=\left[\frac{M-\kappa M_{\mathrm{cut}}}{M_{1}}\right]^{\alpha}\overline{n}_{\mathrm{central}},\end{split}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $M_{\mathrm{cut}}$ sets the halo mass scale for central galaxies, $\sigma$
    sets the width of the error function of $\overline{n}_{\mathrm{central}}$, $M_{1}$
    sets the mass scale for satellite galaxies, $\alpha$ sets the slope of the power
    law, and $\kappa M_{\mathrm{cut}}$ sets the limit below which a halo cannot host
    a satellite galaxy. $M$ denotes the halo mass, and we use the virial mass definition
    $M_{vir}$. The actual number of central galaxies in a halo follows the Bernoulli
    distribution with the mean set to $\overline{n}_{\mathrm{central}}$, whereas the
    number of satellite galaxies follows the Poisson distributions with the mean set
    to $\overline{n}_{\mathrm{satellite}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this standard HOD populates halos probabilistically according to halo
    mass, recent variations of the HOD incorporate more flexibility in modeling. These
    flexible HODs allow additional halo properties — beyond the halo mass — to inform
    galaxy occupation (e.g., Hearin et al., [2016](#bib.bib27); Yuan et al., [2018b](#bib.bib77)).
    The HOD implemented here is one such flexible model; it uses the publicly available
    GRAND-HOD package²²2[https://github.com/SandyYuan/GRAND-HOD](https://github.com/SandyYuan/GRAND-HOD).
    This HOD implementation introduces a series of extensions to the standard HOD,
    including flexibility in the distribution of satellite galaxies within the halo,
    velocity distribution of the galaxies, and galaxy assembly bias. To add this flexibility,
    we invoke two extensions: the satellite distribution parameter, $s$, and the galaxy
    assembly bias parameter, $A$. The satellite distribution parameter allows for
    a flexible radial distribution of satellite galaxies within a dark matter halo,
    and the galaxy assembly bias parameter allows for a secondary HOD dependence on
    halo concentration. For complete information about GRAND-HOD and its HOD extensions,
    see Yuan et al. ([2018a](#bib.bib76)).'
  prefs: []
  type: TYPE_NORMAL
- en: Fifteen sets of HOD model parameters are generated for each AbacusCosmos simulation
    box, and 31 are generated for each Planck box. For each simulation box, a baseline
    HOD model is selected as a function of cosmology; these baseline models vary only
    in $M_{\mathrm{cut}}$ and $M_{1}$, and baseline values of all the other HOD parameters
    remain the same. This ensures that the combined effect of perturbing the cosmology
    and HOD is mild. This is done because, despite the fact that the cosmological
    parameters of each simulation are only perturbed by a few percent, coupling these
    cosmological changes with perturbations to the HOD can lead to drastic changes
    to the mock catalogs and the clustering statistics. To minimize these effects,
    instead of populating galaxies according to HOD parameters in an ellipse aligned
    with the default parameter basis, we populate according to parameter in an ellipse
    defined over a custom parameter basis.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/9139db13eca03bc63342d1becf75433e.png) | ![Refer to
    caption](img/aad80b94ce21d1cf873eddb2eebebf76.png) | ![Refer to caption](img/b6bfce89985b2ae3911bbdc66580f69b.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/fd0cb0ba31d819de74b42140320b102e.png) | ![Refer to
    caption](img/5f2df0f833d24a38587fd7c18aead440.png) | ![Refer to caption](img/96248bc965edd61d4003d13d39511705.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\textsc{$\sigma_{8}$}=0.92$, $\textsc{$\Omega_{m}$}=0.28$ | $\textsc{$\sigma_{8}$}=0.83$,
    $\textsc{$\Omega_{m}$}=0.32$ | $\textsc{$\sigma_{8}$}=0.71$, $\textsc{$\Omega_{m}$}=0.34$
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 1: Top: A sample of train input images. Shown is a two-dimensional projection
    of the three-dimensional image (or “slab”). The train, validate, and test samples
    include a number of choices designed to reduce the likelihood of giving the machine
    learning model an unfair advantage: we employ a zero-point shift to minimize learning
    from images with correlated structure, we use random HODs and seeds to allow for
    uncertainties in galaxy formation physics, we use axial flips of the slabs to
    augment the data, and we use unique portions of the simulation and unique HODs
    in the validation set to provide a way to test that the model does not rely on
    the particulars of the structure or HOD. To highlight the differences in the images
    that are strictly due to cosmology and HOD, the zero-point shift has been omitted
    for these images. Bottom: The same images as above, smoothed with a Gaussian filter
    ($\sigma=1\,\mathrm{pixel}$) to emphasize the differences between images that
    are due to cosmological models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Planck cosmology, the HOD parameters are chosen in reference to the
    parameter ranges in Kwan et al. ([2015](#bib.bib39)): $\log_{10}(M_{\mathrm{cut}}/h^{-1}\,\mathrm{M_{\odot}})=13.35$,
    $\log_{10}(M_{1}/h^{-1}\,\mathrm{M_{\odot}})=13.8$, $\sigma=0.85$, $\alpha=1$,
    $\kappa=1$, $s=0$, and $A=0$. However, we modify two baseline HOD parameter values
    — $M_{\mathrm{cut}}$ and $M_{1}$ — for the non-Planck simulations. We set the
    baseline value of $M_{\mathrm{cut}}$ in each cosmology box such that the projected
    2-point correlation function $w_{p}(5-10\rm{Mpc})$ of all the halos with $M>M_{\mathrm{cut}}$
    is equal to the $w_{p}(5-10\rm{Mpc})$ of the centrals in the baseline HOD at Planck
    cosmology, where $w_{p}(5-10\rm{Mpc})$ is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $w_{p}(5-10\textrm{Mpc})=\int_{5\textrm{Mpc}}^{10\textrm{Mpc}}w_{p}d(r_{\perp}).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: This effectively holds the baseline $w_{p}$ of the centrals approximately constant
    across all the cosmology boxes. Then $M_{1}$ is selected such that the baseline
    satellite-central fraction in each cosmology box is the same as that of the baseline
    HOD in Planck cosmology.
  prefs: []
  type: TYPE_NORMAL
- en: For each 1100box, seven additional pairs of model parameters uniformly sample
    the parameter space within $5\%$ of the baseline HOD (15 additional pairs for
    each Planck box). For HOD parameters $s$ and $A$, whose baseline parameters are
    0, we draw uniform samples between $-0.05$ and $0.05$. The two HODs of each pair
    are symmetrically offset across the baseline HOD. Excluding the baseline HOD,
    fourteen unique HODs are generated for each AbacusCosmos 1100box simulation, and
    30 unique HODs are generated for each Planck simulation. Four random seeds are
    used to populate the simulations with realizations of galaxies according to the
    HOD; this results in four unique galaxy catalogs for each HOD. The details of
    how these are used are described in the next section. For complete information
    about the HOD implementation, see Yuan et al. ([2019](#bib.bib78)).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Training & Validation Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training sample of mock observations (for training the deep learning models)
    and validation sample of mock observations (for assessing when the models have
    sufficiently fit) are created from the AbacusCosmos suite of 1100box simulations.
  prefs: []
  type: TYPE_NORMAL
- en: AbacusCosmos includes 40 simulated cosmologies, and for each of these, we select
    a random distance along the $x$ and $y$ axes to become the new 0-point of the
    box ($z=0$, along the line of sight direction, which includes redshift space distortion,
    remains unchanged). Because the 1100box simulations all have the same initial
    conditions, this random reshuffling minimizes the chances of our model learning
    about correlated structure across simulations.³³3Simulations with matched initial
    conditions will produce portions of the cosmic web with, for example, a unique
    or unusual fingerprint of filamentary structure. The evolutionary stage of a particular
    structure is highly dependent on the simulation’s $\sigma_{8}$ and other cosmological
    parameters. Because CNNs are particularly adept at pattern finding, care must
    be taken to prevent a CNN from learning to identify some unique structure — especially
    one which is particular to a suite of simulations and the initial conditions of
    those simulations — and infer cosmological parameters from its details. This is
    not an approach that will generalize to real observations, and can give overly
    optimistic results. The mock observations of the training set are built from the
    portion of the box with $220h^{-1}\,\mathrm{Mpc}\leq z<1100h^{-1}\,\mathrm{Mpc}$,
    while the validation set is built from the structure in the range $0h^{-1}\,\mathrm{Mpc}\leq
    z<220h^{-1}\,\mathrm{Mpc}$. By completely excluding this portion of the simulation
    from the training set, we can test and ensure that the machine learning model
    does not rely on its ability to identify or memorize large-scale structure correlations
    stemming from the matched initial conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The box is divided into 20 non-overlapping slabs, which are $550h^{-1}\,\mathrm{Mpc}$
    in the $x$ and $y$ directions and $220h^{-1}\,\mathrm{Mpc}$ along the line of
    sight $z$ direction. Halo catalogs generated by the ROCKSTAR halo finder (Behroozi
    et al., [2012](#bib.bib6)) become the basis for four mock observations per slab.
  prefs: []
  type: TYPE_NORMAL
- en: For each slab, we select and apply one HOD from the 15 that are available. Eleven
    of the HODs are reused as necessary in the 16 training slabs. The remaining four
    HODs are reserved exclusively for the four validation slabs. By setting aside
    four HODs for the validation set, the validation set is populated with galaxies
    in a way that is unique from the observations used for training, and we can ensure
    that the ML model results are not dependent on memorization or previous knowledge
    of the details of the HOD.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the four random HOD seeds, the slabs are populated with galaxies.
    These training slabs vary in the number of galaxies, ranging from $\sim 17000$
    to $\sim 46000$ galaxies per slab, with the number of galaxies correlating weakly
    with the underlying cosmology. To prevent the CNN from learning correlations between
    cosmological parameters and the number of galaxies in the mock observation, we
    randomly subselect the galaxy population so that all observations have $15000$
    galaxies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The selected galaxies are binned into a $275\times 275\times 55$, three-dimensional,
    single-color image. Galaxies are assigned to voxels using a triangular shaped
    cloud (TSC) and $2\times 2\times 5h^{-1}\,\mathrm{Mpc}$ voxels. Projected galaxy
    densities for three sample cosmologies are shown in Figure [1](#S2.F1 "Figure
    1 ‣ 2.2 Halo Occupation Distribution ‣ 2 Methods: Mock Observations ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the machine learning model described in Section [3](#S3 "3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") is not invariant under mirroring of images, we
    augment our data by applying an axial flip along the $x$- and/or $y$-directions
    to three of the four slabs. For each of these three mirror images, we use a new
    random seed for the HOD and uniquely subselect to 15000 galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The power spectrum of the galaxy density field is computed for each slab. To
    perform this calculation, we pad the galaxy density field with zeros to double
    the image size in each direction to account for the lost periodic boundary conditions,
    Fourier transform the resulting $550\times 550\times 110$ image, and convert the
    result to a power spectrum in physical units. This 3-dimensional power spectrum
    is next de-convolved to account for the TSC-aliased window function (as in, e.g.,
    Jeong, [2010](#bib.bib32)), and summarized as a 1-dimensional power spectrum by
    averaging the power in binned spherical annuli. Due to the anisotropic nature
    of the slab and voxel dimensions, the most conservative choices for minimum and
    maximum $k$ values are selected. These are set by the shortest box dimension ($220h^{-1}\,\mathrm{Mpc}$)
    and the Nyquist frequency of the largest pixel dimension ($5h^{-1}\,\mathrm{Mpc}$),
    respectively. Power spectra for a sample of galaxy catalogs are shown in Figure
    [2](#S2.F2 "Figure 2 ‣ 2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations
    ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b8ecb6c35b8a90ce1e6f080f0c82db7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mean galaxy power spectra, $P(k)$, for 4 of the 40 training cosmologies
    (yellow, orange, purple, and blue) as well as for the Planck test cosmology (pink).
    Points indicate the mean power, while error bars show the middle 68% of the mock
    observations. The “Vector Features” input, shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"), is a single
    realization of this power spectrum; for each mock observation, the power spectrum
    is calculated directly from a single 3D mock galaxy observation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, the method for building mock observations from each of the simulations
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random $x$ and $y$ value is selected to be the new 0-point of the box. $z=0$,
    along the line of sight direction with redshift space distortion, remains unchanged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The box is divided into 20 non-overlapping slabs, each $550\times 550\times
    220h^{-1}\,\mathrm{Mpc}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each slab:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An HOD is selected. Eleven HODs, some of which are reused as necessary, are
    used to populate the 16 training slabs with galaxies. Four unique HODs are reserved
    exclusively for the four validation slabs.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 15000 galaxies are randomly selected. These are binned in $2\times 2\times 5h^{-1}\,\mathrm{Mpc}$
    bins using a TSC.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The previous step is repeated for each of 4 random seeds, incorporating mirror
    image(s) of the slab.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The power spectrum of the slab is calculated.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: This method results in 3200 mock observations built from 40 simulations, with
    20 slabs per simulation and 4 seeds (with axial flips) per slab.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 2560 slabs built from the portion of the simulation with $z\geq 220h^{-1}\,\mathrm{Mpc}$
    comprise the training set, and are used to train the machine learning model described
    in Section [3](#S3 "3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"). The remaining
    640 slabs are built from a non-overlapping portion of the simulation ($z<220h^{-1}\,\mathrm{Mpc}$).
    These make up the validation set and are used to assess the models’ fit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our creation of the test and validation sets include a number of choices to
    reduce the likelihood of giving the machine learning model an unfair advantage:
    we employ a recentering of the box to minimize learning from images with correlated
    structure, we use random HODs and seeds to allow for uncertainties in galaxy formation
    physics, we use axial flips of the slabs to augment the data to account for rotational
    invariance, and we use unique portions of the simulation and unique HODs in the
    validation fold to provide a way to ensure that the model does not rely on the
    details of the structure or HOD.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Planck Testing Set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The testing sample is built from the AbacusCosmos Planck simulations. The 20
    Planck simulations each have initial conditions that are unique from the simulation
    sample described in Section [2.3](#S2.SS3 "2.3 Training & Validation Sets ‣ 2
    Methods: Mock Observations ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"). Mock observations of the Planck testing set are
    built using a similar process as described in Section [2.3](#S2.SS3 "2.3 Training
    & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys") with one exception:
    the 20 non-overlapping slabs are each populated with galaxies according to 20
    unique HODs selected randomly from the 31 HODs available. Accounting for the axial
    flips to augment the data, the resulting testing sample is 1600 slabs with associated
    power spectra. Our testing set is a truly independent sample from the training
    and validation sets. Though the cosmologies used in the training and validation
    sets are near the Planck fiducial cosmology, this exact cosmology is never explicitly
    used for training or testing.'
  prefs: []
  type: TYPE_NORMAL
- en: '3 Methods: Machine Learning Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We assess three machine learning models: 1. a standard convolutional neural
    network (CNN) that learns from the 3D galaxy images to regress cosmological parameters,
    2. a neural network (NN) that learns from the power spectrum of the galaxy images
    to regress cosmological parameters, and 3. a hybrid CNN (hCNN) that employs a
    standard CNN but also can take advantage of meaningful summary information — in
    this case, the galaxy power spectrum — to inject physically meaningful information
    into the fully connected layers. These three models are described in detail below.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Standard CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/215b1bc1028b6ec653f911d0f1c0b05f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A visual summary of the three ML models. The neural network (NN)
    uses a vector input (green) with the fully connected layers for processing (orange).
    The standard convolutional neural network (CNN) uses an image input with the image
    processing layers (blue) plus fully connected layers (orange). The hybrid CNN
    (hCNN) joins these by concatenating the vector features with the final layer of
    the image processing; the result is fed into the fully connected layers. For further
    details about the NN, CNN, and hCNN, see Section [3](#S3 "3 Methods: Machine Learning
    Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy
    Redshift Surveys").'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs, Fukushima & Miyake, [1982](#bib.bib20);
    LeCun et al., [1999](#bib.bib42); Krizhevsky et al., [2012](#bib.bib38)) are a
    class of machine learning algorithms that are commonly used in image recognition
    tasks. Over many cycles, called “epochs,” the network learns the convolutional
    filters, weights, and biases necessary to extract meaningful patterns from the
    input image. For cosmological applications, CNNs are traditionally applied to
    monochromatic (e.g., Lanusse et al., [2018](#bib.bib41); Ntampaka et al., [2018](#bib.bib48);
    Ho et al., [2019](#bib.bib30)) or multiple-color 2D images (e.g., La Plante &
    Ntampaka, [2018](#bib.bib40)). However, CNNs are not confined to 2D training data;
    they can also be used on 3D data cubes. Three-dimensional CNNs became popular
    for interpreting videos, using time as the third dimension (e.g., Ji et al., [2013](#bib.bib33)),
    but recent cosmological applications of this algorithm have applied the technique
    to 3D data (e.g., Ravanbakhsh et al., [2017](#bib.bib57); He et al., [2018](#bib.bib26);
    Mathuriya et al., [2018](#bib.bib44); Peel et al., [2018](#bib.bib51); Aragon-Calvo,
    [2019](#bib.bib4); Berger & Stein, [2019](#bib.bib8); Zhang et al., [2019](#bib.bib79);
    Pan et al., [2019](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs typically use pairs of convolutional filters and pooling layers to extract
    meaningful patterns from the input image. These are followed by several fully
    connected layers. Our standard CNN architecture includes several consecutive fully
    convolutional layers at the onset and mean and max pooling branches in parallel.
    It is implemented in Keras (Chollet, [2015](#bib.bib11)) with a Tensorflow (Abadi
    et al., [2016](#bib.bib1)) backend, and is shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"). The full
    architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 4 filters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 4 filters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 4 filters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Max pooling branch (in parallel with # [5](#S3.I1.i5 "item 5 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys")):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 1$ max pooling
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 4 filters
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 5$ max pooling
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 32 filters
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 5$ max pooling, flattened
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mean pooling branch (in parallel with # [4](#S3.I1.i4 "item 4 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys")):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 1$ max pooling
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 4 filters
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 5$ max pooling
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $3\times 3\times 3$ convolution with 32 filters
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: batch normalization
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: $5\times 5\times 5$ max pooling, flattened
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Concatenation of the max pool branch output ([4e](#S3.I1.i4.I2.i5 "item 4e
    ‣ item 4 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"))
    and mean pool branch output ([5e](#S3.I1.i5.I1.i5 "item 5e ‣ item 5 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1024 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 512 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 512 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 256 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 128 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: leaky ReLU activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 64 neurons, fully connected
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: linear activation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 30% dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2 output neurons, one each for $\Omega_{m}$ and $\sigma_{8}$
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We use a mean absolute error loss function and the Adam Optimizer (Kingma &
    Ba, [2014](#bib.bib36)). In practice, we scale $\Omega_{m}$ and $\sigma_{8}$ linearly
    so that the range of training values lies between $-1$ and $1$. The output predictions
    are scaled back to physically interpretable values according to the inverse of
    the same linear scaling. While this may not be an important detail for these particular
    cosmological parameters ($\sigma_{8}$ and $\Omega_{m}$ are of the same order of
    magnitude), problems can arise when training multiple outputs with significantly
    different value ranges (e.g. if $H_{0}$ in units of $\mathrm{km}\,s^{-1}\,\mathrm{Mpc}^{-1}$
    were added as a third output parameter). Details about the training scheme and
    learning rate are discussed in Section [3.3](#S3.SS3 "3.3 Training ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys").'
  prefs: []
  type: TYPE_NORMAL
- en: In our model, small-scale feature extraction is performed by several consecutive
    layers of 3D $3\times 3\times 3$ convolutional filters. This feature extraction
    is followed by aggressive pooling in parallel max and mean pooling branches that
    each reduce the data cube to 32 neurons. The outputs of these branches are concatenated
    and are followed by fully connected layers. We use a rectified linear unit (ReLU,
    Nair & Hinton, [2010](#bib.bib47)) activation function throughout. The dropout,
    in which 30% of neurons are ignored during training, reduces the likelihood of
    the model overfitting (Srivastava et al., [2014](#bib.bib65)).
  prefs: []
  type: TYPE_NORMAL
- en: The model takes a $275\times 275\times 55$ image as input and learns the filters,
    weights, and biases necessary to regress two cosmological parameters — the amplitude
    of matter fluctuations ($\sigma_{8}$) and the matter density parameter ($\Omega_{m}$);
    each of the two output neurons maps to a cosmological parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Standard NN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The standard neural network uses only the fully connected layers, with the
    power spectrum as the only input, fed into steps [8](#S3.I1.i8 "item 8 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys") through [13](#S3.I1.i13
    "item 13 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys") in
    the above architecture. It is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Standard
    CNN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to
    Cosmological Constraints From Galaxy Redshift Surveys"). The model takes the binned
    power spectra as input and learns the weights and biases necessary to regress
    the cosmological parameters of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Hybrid CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The hybrid convolutional neural network (hCNN) takes advantage of a standard
    CNN, but also utilizes information that is known to be important and meaningful.
    The power spectrum, which carries cosmological information, is folded in by inserting
    this information at step [8](#S3.I1.i8 "item 8 ‣ 3.1 Standard CNN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") in the standard CNN architecture. It should be
    noted that the use of incorporating physically meaningful parameters into a deep
    learning technique is not new to this work, and has been used previous in astronomy
    (Dattilo et al., [2019](#bib.bib13)), though it has not yet been widely adopted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hCNN model uses both the $275\times 275\times 55$ images as well as the
    binned power spectra as input to learn $\Omega_{m}$ and $\sigma_{8}$. This architecture
    is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Standard CNN ‣ 3 Methods: Machine
    Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys").'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/35617d67c4d336bb853922d80a9f1045.png) | ![Refer to
    caption](img/995e0cf7838b2b688fe623d4dd6cadd8.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: Mean squared error (MSE) as a function of scaled epoch, $\mathcal{E}$.
    While the standard neural network (NN, green dotted) quickly settles to a low
    error solution, the convolutional neural network (CNN, blue dashed) and hybrid
    CNN (hCNN, purple solid) have large fluctuations during the initial phase of training
    ($\mathcal{E}\lesssim 0.32$). Here, the error on the validation set predictions
    are regularly worse than a guess of the mean value (gray line) for both $\sigma_{8}$
    (left) and $\Omega_{m}$ (right). The learning rate is decreased at $\mathcal{E}\approx
    0.32$, and the CNN and hCNN settle into a low-error regime. To remove fluctuations
    that visually detract from overall trends in error and slope, the curves shown
    in this figure have been smoothed with a Gaussian filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For training the CNN and hCNN, we adopt a two-phase training scheme. Our training
    approach takes advantage of a large step size during the initial phase of training
    to capture the diversity of cosmologies and HOD models, then transitions to a
    smaller step size during the second phase of training to improve the fit (see
    Appendix [A](#A1 "Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") for a further
    discussion of this). We train for 550 epochs, 175 in the first phase and 375 in
    the second phase. The last 50 epochs will be used to select a model that meets
    criteria more nuanced than simply minimizing the loss function. It is discussed
    further in Section [4.2.1](#S4.SS2.SSS1 "4.2.1 Unbiased Model Selection ‣ 4.2
    Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"). Note that the NN, which is less sensitive
    to the details of training and trains significantly faster than models with convolutional
    layers, is trained for 800 epochs according to the details of phase one, described
    below.
  prefs: []
  type: TYPE_NORMAL
- en: We use the Adam Optimizer (Kingma & Ba, [2014](#bib.bib36)), which has a step
    size that varies as a function of epoch according to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha(t)=\alpha_{0}\frac{\sqrt{1-\beta_{2}^{t}}}{1-\beta_{1}^{t}},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is the step size, $t$ denotes a time step or epoch, $\alpha_{0}$
    is the initial step size⁴⁴4The initial step size is denoted, simply, “learning
    rate” in the keras documentation., and parameters $\beta_{1}$ and $\beta_{2}$
    control the step size at each epoch. We adopt the default values of $\beta_{1}=0.9$
    and $\beta_{2}=0.999$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase one of training is 175 epochs with an initial step size of $\alpha_{0}=1.0\times
    10^{-5}$. We find that this first phase, with its larger initial step size, is
    necessary for the models to learn the diversity of cosmologies. Smaller learning
    rates tend to produce models with predictions that cluster near the mean values
    for $\sigma_{8}$ and $\Omega_{m}$, while larger learning rates tend to produce
    models that fluctuate wildly in bias or overfit the training data. Near epoch
    175, we find evidence in the CNN and hCNN that the learning rate is too large.
    This is characterized by swings in the tendency to over- or underpredict the validation
    set, and can be seen in the large, fluctuating mean squared error (MSE) shown
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods:
    Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"). The MSE is plotted as a function of scaled epoch,
    $\mathcal{E}$, defined as epoch divided by the maximum number of training epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We adopt the model at epoch 175 as a pre-trained model and transition to a
    second phase of training with a lower learning rate. Phase two of training is
    an additional 375 epochs with an initial step size of $\alpha_{0}=0.2\times 10^{-5}$.
    For clarity, we refer to the first training epoch of phase two as “epoch 176”
    for the remainder of this work. However, for the purposes of Equation [3](#S3.E3
    "In 3.3 Training ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") only, $t$
    is reset to $0$. Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard
    NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") shows the effect of decreasing the
    learning rate: at $\mathcal{E}\approx 0.32$, the mean squared error decreases
    dramatically as the model settles into a stable fit that describes the validation
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting is defined as the tendency of the model to produce excellent predictions
    on the testing set but to fail on the validation set. (The term “overfit” is occasionally
    used to describe a deep learning method identifying features in a cosmological
    simulation that do not describe actual observations, but we use the term in the
    more traditional sense.) Two changes to the learning scheme tend to result in
    an overfit model: first, an increased learning rate and second, the use of max
    pooling only via eliminating the mean pooling branch. When the model is overfit,
    the validation set dramatically biases toward the mean, despite the fact that
    the training data are well-described even at extreme values of $\sigma_{8}$ and
    $\Omega_{m}$.'
  prefs: []
  type: TYPE_NORMAL
- en: We caution, however, that we have not explored a full grid of hyperparameters
    for model optimization. It is likely that the two-phase training scheme could
    be avoided with carefully selected values of $\beta_{1}$ and $\beta_{2}$ to smoothly
    decrease step size. Likewise, we have not thoroughly vetted the tendency to overfit
    by increasing learning rate or removing mean pooling under many hyperparameter
    combinations. Such a comprehensive grid search is expensive and intractable with
    current computational resources. Therefore, the effects of learning rate and pooling
    described in this section should serve as a word of caution for those training
    other deep models, but should not be overinterpreted.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/3f0b329d94e762eabace8cc4a295f560.png) | ![Refer to
    caption](img/4abfcc8ec2c6a69deb4a79bdb36e5eb2.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/a5395f4fe0c0ff7ea6ac5bd68bf0a89e.png) | ![Refer to
    caption](img/0e0b0c714491226fd9694c1fd96fc638.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5: Top: slope of the best fit line as a function of scaled epoch, $\mathcal{E}$.
    A slope of 1 indicates that the model captures the full range of $\sigma_{8}$
    and $\Omega_{m}$, while a slope of 0 is indicative of the model predicting at
    or near the mean for all data in the validation set. As the models train, they
    increase the diversity of predictions. However, slope never reaches a value of
    $1$ for any model, indicating that the predictions will bias toward the mean for
    any mock observation with extreme values of $\sigma_{8}$ or $\Omega_{m}$. Bottom:
    prediction bias, b, as a function of scaled epoch, $\mathcal{E}$. While the standard
    neural network (NN, green dotted) quickly settles to a solution with low bias,
    the convolutional neural network (CNN, blue dashed) and hybrid CNN (hCNN, purple
    solid) have large fluctuations during the initial phase of training ($\mathcal{E}\lesssim
    0.32$). The learning rate is decreased at $\mathcal{E}=0.32$, and the CNN and
    hCNN settle into a low-bias regime. To remove fluctuations that visually detract
    from overall trends in error and slope, the curves shown in this figure have been
    smoothed with a Gaussian filter.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we present results from the validation set as a way of assessing the model’s
    fit, both near the median model and also toward extreme values of $\sigma_{8}$
    and $\Omega_{m}$. We also present results from the testing set to explore how
    the technique might generalize into the more realistic case where the cosmological
    model, galaxy formation details, and initial conditions are not explicitly known.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Validation Set Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We define the prediction bias, $b$, as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $b\equiv\left<\left&#124;x_{\mathrm{predicted}}-x_{\mathrm{true}}\right&#124;\right>,$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $<\cdot>$ denotes a mean and $x$ is a placeholder for either $\sigma_{8}$
    or $\Omega_{m}$. Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") shows the
    bias as a function of scaled epoch, $\mathcal{E}$. During phase two of the training,
    the CNN and hCNN bias drop significantly, indicating that the lower learning rate
    is indeed reducing errors in a meaningful way and learning the spatial galaxy
    patterns that correlate with cosmological parameters.
  prefs: []
  type: TYPE_NORMAL
- en: While MSE and bias both assess the typical offset of the validation set predictions,
    these statistics alone cannot tell the full story. It is also important to understand
    how the model might perform near the edges of the training set. For this, we assess
    the slope of a best fit line through the true and predicted values of $\sigma_{8}$,
    and separately, the best fit line through the true and predicted values of $\Omega_{m}$.
    A slope close to $1$ indicates that the model fits well near the extreme values
    of $\sigma_{8}$ and $\Omega_{m}$, while a slope of $0$ is indicative of a model
    biasing toward the mean. Overfit models will tend to have a larger MSE and bias
    coupled with a smaller slope. Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the slope of this linear best fit line. We can infer from the value of this
    fit, $\sim 0.7-0.8$ for both $\sigma_{8}$ and $\Omega_{m}$, that the model may
    not predict well for $\sigma_{8}$ and $\Omega_{m}$ values near the edges of the
    training data, and will likely bias toward the mean when presented with a cosmological
    parameter set far from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Testing Set Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While it is an interesting academic exercise to discuss the results of the validation
    set, the Universe, unfortunately, gives us one galaxy sample. This sample may
    differ from our training set in cosmological parameters and galaxy formation physics
    (and most certainly differs in initial conditions!). If we aim to eventually use
    a CNN or hCNN to constrain cosmological models from an observed galaxy sample,
    is imperative to develop tools to assess ML models, going beyond a simple minimization
    of loss or performance on validation data. Though the model trains to minimize
    the mean absolute error, this is not necessarily the most interesting — or the
    most useful — test statistic for a cosmological analysis of a large galaxy survey.
    Next, we lay out a technique for selecting a relatively unbiased model.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/8077e4c8cb95b65a399b09a6882e23a2.png) | ![Refer to
    caption](img/aed53de2183b562dd4c3a2bb7564175a.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/927706b9cb07878beaff4875aac83bf6.png) | ![Refer to
    caption](img/f23c7ce7906caf46f39612ed91f13138.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/ed8ad5f6842d98a3935b46d9e0ba7bf0.png) | ![Refer to
    caption](img/8a63bbaedfec3cc5e4f7f2b9bd00cd20.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 6: True and predicted values of $\sigma_{8}$ (left) and $\Omega_{m}$
    (right) for the neural network (NN, green, top), convolutional neural network
    (CNN, blue, middle) and hybrid CNN (hCNN, purple, bottom). For the validation
    data of each of the 40 cosmological models, the median (circles) and middle 68%
    (error bars) are shown. While the predictions typically lie close to the one-to-one
    line (black dashed) near the central values of $\sigma_{8}$ and $\Omega_{m}$,
    the bias toward the mean is more pronounced at extreme values. For illustrative
    purposes, $\sigma_{8}$ and $\Omega_{m}$ values below the 16th percentile and above
    the 84th precentile are set against a gray background, while the middle 1-$\sigma$
    are shown against a white background. The CNN and hCNN predictions for the validation
    set display a significantly tighter scatter than the NN. This is unsurprising
    because the NN learns only from the power spectrum (see Figure [2](#S2.F2 "Figure
    2 ‣ 2.3 Training & Validation Sets ‣ 2 Methods: Mock Observations ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")),
    while the CNN and hCNN have more flexibility to learn from the un-preprocessed
    mock galaxy catalog.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Unbiased Model Selection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As highlighted in Figure [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys"), the models
    do not perform well at extreme values of $\sigma_{8}$ and $\Omega_{m}$. This is
    unsurprising; machine learning models tend to interpolate much better than they
    extrapolate. In practice, one would want to train on a large range of simulated
    cosmologies extending well beyond a region containing the expected results. Furthermore,
    one would expect a bias toward the mean for any cosmology near the edges of the
    training sample. Because of this (and for the purposes of model selection only),
    we limit our analysis to the simulations enclosed in a 68% ellipse in the $\sigma_{8}$-$\Omega_{m}$
    plane.⁵⁵5The selection of simulations used here are shown in a lighter shade of
    gray in Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing
    Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"); the simulations shown in dark gray are near the
    edges of the $\sigma_{8}$-$\Omega_{m}$ plane, are expected to have results that
    bias to the mean, and are excluded from this particular analysis for this reason.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to limiting this analysis to the 27 simulations with $\sigma_{8}$
    and $\Omega_{m}$ values closest to the mean cosmology, we also only assess the
    last 50 epochs of the CNN and hCNN trainings ($0.91<\mathcal{E}\leq 1.0$). Importantly,
    we only use the validation data to assess models. Recall that the training data
    should not be used in such a way because the model has already explicitly seen
    this data. Likewise, the testing data should not be used to assess models because
    doing so would unfairly bias the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the 27 simulations and at each epoch, we calculate the distance
    between the predicted and true cosmology according to the following: for each
    of the 16 validation mock observations per simulation, we predict $\sigma_{8}$
    and $\Omega_{m}$. The 68% error ellipse in the $\sigma_{8}$-$\Omega_{m}$ plane
    is calculated, as is the distance between the true cosmological parameters ($\Omega_{m,\,\mathrm{true}}$
    and $\sigma_{8,\,\mathrm{true}}$) and the middle of the ellipse of predicted cosmological
    parameters ($\Omega_{m,\,\mathrm{mid}}$ and $\sigma_{8,\,\mathrm{mid}}$). This
    distance, $\mathcal{Z}$, is calculated according to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.E5.m1.5" class="ltx_Math" alttext="\begin{split}\mathcal{Z}\,=\,&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}"
    display="block"><semantics id="S4.E5.m1.5a"><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" id="S4.E5.m1.5.5" xref="S4.E5.m1.5.6.1.cmml"><mtr id="S4.E5.m1.5.5a"
    xref="S4.E5.m1.5.6.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S4.E5.m1.5.5b"
    xref="S4.E5.m1.5.6.1.cmml"><mrow id="S4.E5.m1.2.2.2.2.2" xref="S4.E5.m1.5.6.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml">𝒵</mi><mo
    lspace="0.448em" id="S4.E5.m1.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.cmml">=</mo></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S4.E5.m1.5.5c" xref="S4.E5.m1.5.6.1.cmml"><mrow
    id="S4.E5.m1.4.4.4.4.2" xref="S4.E5.m1.5.6.1.cmml"><mfrac id="S4.E5.m1.3.3.3.3.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.cmml"><mrow id="S4.E5.m1.3.3.3.3.1.1.10" xref="S4.E5.m1.3.3.3.3.1.1.10.cmml"><mrow
    id="S4.E5.m1.3.3.3.3.1.1.9.9" xref="S4.E5.m1.3.3.3.3.1.1.9.9.cmml"><mrow id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.2"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.cmml"><msub id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.2.cmml">Ω</mi><mrow id="S4.E5.m1.3.3.3.3.1.1.2.2.2.4"
    xref="S4.E5.m1.3.3.3.3.1.1.2.2.2.3.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.1.1.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.1.1.1.1.cmml">m</mi><mo rspace="0.337em" id="S4.E5.m1.3.3.3.3.1.1.2.2.2.4.1"
    xref="S4.E5.m1.3.3.3.3.1.1.2.2.2.3.cmml">,</mo><mi id="S4.E5.m1.3.3.3.3.1.1.2.2.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.2.2.2.2.cmml">true</mi></mrow></msub><mo id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.1.cmml">−</mo><msub id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.cmml"><mi mathvariant="normal" id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.2"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.2.cmml">Ω</mi><mrow id="S4.E5.m1.3.3.3.3.1.1.4.4.2.4"
    xref="S4.E5.m1.3.3.3.3.1.1.4.4.2.3.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.3.3.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.3.3.1.1.cmml">m</mi><mo rspace="0.337em" id="S4.E5.m1.3.3.3.3.1.1.4.4.2.4.1"
    xref="S4.E5.m1.3.3.3.3.1.1.4.4.2.3.cmml">,</mo><mi id="S4.E5.m1.3.3.3.3.1.1.4.4.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.4.4.2.2.cmml">mid</mi></mrow></msub></mrow><mo stretchy="false"
    id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.3" xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.cmml">)</mo></mrow><mo
    lspace="0.167em" rspace="0em" id="S4.E5.m1.3.3.3.3.1.1.9.9.2" xref="S4.E5.m1.3.3.3.3.1.1.9.9.2.cmml">​</mo><mrow
    id="S4.E5.m1.3.3.3.3.1.1.9.9.3" xref="S4.E5.m1.3.3.3.3.1.1.9.9.3.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.9.9.3.1"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.3.1.cmml">cos</mi><mo lspace="0.167em" id="S4.E5.m1.3.3.3.3.1.1.9.9.3a"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.3.cmml">⁡</mo><mi id="S4.E5.m1.3.3.3.3.1.1.9.9.3.2"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.3.2.cmml">α</mi></mrow></mrow><mo id="S4.E5.m1.3.3.3.3.1.1.10.11"
    xref="S4.E5.m1.3.3.3.3.1.1.10.11.cmml">+</mo><mrow id="S4.E5.m1.3.3.3.3.1.1.10.10"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.cmml"><mrow id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.2"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.cmml"><msub id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.2.cmml">σ</mi><mrow id="S4.E5.m1.3.3.3.3.1.1.6.6.2.4"
    xref="S4.E5.m1.3.3.3.3.1.1.6.6.2.3.cmml"><mn id="S4.E5.m1.3.3.3.3.1.1.5.5.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.5.5.1.1.cmml">8</mn><mo rspace="0.337em" id="S4.E5.m1.3.3.3.3.1.1.6.6.2.4.1"
    xref="S4.E5.m1.3.3.3.3.1.1.6.6.2.3.cmml">,</mo><mi id="S4.E5.m1.3.3.3.3.1.1.6.6.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.6.6.2.2.cmml">true</mi></mrow></msub><mo id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.1.cmml">−</mo><msub id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.2"
    xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.2.cmml">σ</mi><mrow id="S4.E5.m1.3.3.3.3.1.1.8.8.2.4"
    xref="S4.E5.m1.3.3.3.3.1.1.8.8.2.3.cmml"><mn id="S4.E5.m1.3.3.3.3.1.1.7.7.1.1"
    xref="S4.E5.m1.3.3.3.3.1.1.7.7.1.1.cmml">8</mn><mo rspace="0.337em" id="S4.E5.m1.3.3.3.3.1.1.8.8.2.4.1"
    xref="S4.E5.m1.3.3.3.3.1.1.8.8.2.3.cmml">,</mo><mi id="S4.E5.m1.3.3.3.3.1.1.8.8.2.2"
    xref="S4.E5.m1.3.3.3.3.1.1.8.8.2.2.cmml">mid</mi></mrow></msub></mrow><mo stretchy="false"
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.3" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.cmml">)</mo></mrow><mo
    lspace="0.167em" rspace="0em" id="S4.E5.m1.3.3.3.3.1.1.10.10.2" xref="S4.E5.m1.3.3.3.3.1.1.10.10.2.cmml">​</mo><mrow
    id="S4.E5.m1.3.3.3.3.1.1.10.10.3" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3.cmml"><mi
    id="S4.E5.m1.3.3.3.3.1.1.10.10.3.1" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3.1.cmml">sin</mi><mo
    lspace="0.167em" id="S4.E5.m1.3.3.3.3.1.1.10.10.3a" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3.cmml">⁡</mo><mi
    id="S4.E5.m1.3.3.3.3.1.1.10.10.3.2" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3.2.cmml">α</mi></mrow></mrow></mrow><msup
    id="S4.E5.m1.3.3.3.3.1.1.12" xref="S4.E5.m1.3.3.3.3.1.1.12.cmml"><mi id="S4.E5.m1.3.3.3.3.1.1.12.2"
    xref="S4.E5.m1.3.3.3.3.1.1.12.2.cmml">a</mi><mn id="S4.E5.m1.3.3.3.3.1.1.12.3"
    xref="S4.E5.m1.3.3.3.3.1.1.12.3.cmml">2</mn></msup></mfrac><mo id="S4.E5.m1.4.4.4.4.2.2"
    xref="S4.E5.m1.4.4.4.4.2.2.cmml">+</mo></mrow></mtd></mtr><mtr id="S4.E5.m1.5.5f"
    xref="S4.E5.m1.5.6.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E5.m1.5.5h"
    xref="S4.E5.m1.5.6.1.cmml"><mfrac id="S4.E5.m1.5.5.5.1.1.1" xref="S4.E5.m1.5.5.5.1.1.1.cmml"><mrow
    id="S4.E5.m1.5.5.5.1.1.1.10" xref="S4.E5.m1.5.5.5.1.1.1.10.cmml"><mrow id="S4.E5.m1.5.5.5.1.1.1.9.9"
    xref="S4.E5.m1.5.5.5.1.1.1.9.9.cmml"><mrow id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.cmml"><mo
    stretchy="false" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.2" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.cmml">(</mo><mrow
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.cmml"><msub
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.cmml"><mi
    mathvariant="normal" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.2" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.2.cmml">Ω</mi><mrow
    id="S4.E5.m1.5.5.5.1.1.1.2.2.2.4" xref="S4.E5.m1.5.5.5.1.1.1.2.2.2.3.cmml"><mi
    id="S4.E5.m1.5.5.5.1.1.1.1.1.1.1" xref="S4.E5.m1.5.5.5.1.1.1.1.1.1.1.cmml">m</mi><mo
    rspace="0.337em" id="S4.E5.m1.5.5.5.1.1.1.2.2.2.4.1" xref="S4.E5.m1.5.5.5.1.1.1.2.2.2.3.cmml">,</mo><mi
    id="S4.E5.m1.5.5.5.1.1.1.2.2.2.2" xref="S4.E5.m1.5.5.5.1.1.1.2.2.2.2.cmml">true</mi></mrow></msub><mo
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.1" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.1.cmml">−</mo><msub
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.cmml"><mi
    mathvariant="normal" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.2" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.2.cmml">Ω</mi><mrow
    id="S4.E5.m1.5.5.5.1.1.1.4.4.2.4" xref="S4.E5.m1.5.5.5.1.1.1.4.4.2.3.cmml"><mi
    id="S4.E5.m1.5.5.5.1.1.1.3.3.1.1" xref="S4.E5.m1.5.5.5.1.1.1.3.3.1.1.cmml">m</mi><mo
    rspace="0.337em" id="S4.E5.m1.5.5.5.1.1.1.4.4.2.4.1" xref="S4.E5.m1.5.5.5.1.1.1.4.4.2.3.cmml">,</mo><mi
    id="S4.E5.m1.5.5.5.1.1.1.4.4.2.2" xref="S4.E5.m1.5.5.5.1.1.1.4.4.2.2.cmml">mid</mi></mrow></msub></mrow><mo
    stretchy="false" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.3" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.cmml">)</mo></mrow><mo
    lspace="0.167em" rspace="0em" id="S4.E5.m1.5.5.5.1.1.1.9.9.2" xref="S4.E5.m1.5.5.5.1.1.1.9.9.2.cmml">​</mo><mrow
    id="S4.E5.m1.5.5.5.1.1.1.9.9.3" xref="S4.E5.m1.5.5.5.1.1.1.9.9.3.cmml"><mi id="S4.E5.m1.5.5.5.1.1.1.9.9.3.1"
    xref="S4.E5.m1.5.5.5.1.1.1.9.9.3.1.cmml">sin</mi><mo lspace="0.167em" id="S4.E5.m1.5.5.5.1.1.1.9.9.3a"
    xref="S4.E5.m1.5.5.5.1.1.1.9.9.3.cmml">⁡</mo><mi id="S4.E5.m1.5.5.5.1.1.1.9.9.3.2"
    xref="S4.E5.m1.5.5.5.1.1.1.9.9.3.2.cmml">α</mi></mrow></mrow><mo id="S4.E5.m1.5.5.5.1.1.1.10.11"
    xref="S4.E5.m1.5.5.5.1.1.1.10.11.cmml">−</mo><mrow id="S4.E5.m1.5.5.5.1.1.1.10.10"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.cmml"><mrow id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.2"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.cmml"><msub id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.cmml"><mi id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.2"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.2.cmml">σ</mi><mrow id="S4.E5.m1.5.5.5.1.1.1.6.6.2.4"
    xref="S4.E5.m1.5.5.5.1.1.1.6.6.2.3.cmml"><mn id="S4.E5.m1.5.5.5.1.1.1.5.5.1.1"
    xref="S4.E5.m1.5.5.5.1.1.1.5.5.1.1.cmml">8</mn><mo rspace="0.337em" id="S4.E5.m1.5.5.5.1.1.1.6.6.2.4.1"
    xref="S4.E5.m1.5.5.5.1.1.1.6.6.2.3.cmml">,</mo><mi id="S4.E5.m1.5.5.5.1.1.1.6.6.2.2"
    xref="S4.E5.m1.5.5.5.1.1.1.6.6.2.2.cmml">true</mi></mrow></msub><mo id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.1"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.1.cmml">−</mo><msub id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.cmml"><mi id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.2"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.2.cmml">σ</mi><mrow id="S4.E5.m1.5.5.5.1.1.1.8.8.2.4"
    xref="S4.E5.m1.5.5.5.1.1.1.8.8.2.3.cmml"><mn id="S4.E5.m1.5.5.5.1.1.1.7.7.1.1"
    xref="S4.E5.m1.5.5.5.1.1.1.7.7.1.1.cmml">8</mn><mo rspace="0.337em" id="S4.E5.m1.5.5.5.1.1.1.8.8.2.4.1"
    xref="S4.E5.m1.5.5.5.1.1.1.8.8.2.3.cmml">,</mo><mi id="S4.E5.m1.5.5.5.1.1.1.8.8.2.2"
    xref="S4.E5.m1.5.5.5.1.1.1.8.8.2.2.cmml">mid</mi></mrow></msub></mrow><mo stretchy="false"
    id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.3" xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.cmml">)</mo></mrow><mo
    lspace="0.167em" rspace="0em" id="S4.E5.m1.5.5.5.1.1.1.10.10.2" xref="S4.E5.m1.5.5.5.1.1.1.10.10.2.cmml">​</mo><mrow
    id="S4.E5.m1.5.5.5.1.1.1.10.10.3" xref="S4.E5.m1.5.5.5.1.1.1.10.10.3.cmml"><mi
    id="S4.E5.m1.5.5.5.1.1.1.10.10.3.1" xref="S4.E5.m1.5.5.5.1.1.1.10.10.3.1.cmml">sin</mi><mo
    lspace="0.167em" id="S4.E5.m1.5.5.5.1.1.1.10.10.3a" xref="S4.E5.m1.5.5.5.1.1.1.10.10.3.cmml">⁡</mo><mi
    id="S4.E5.m1.5.5.5.1.1.1.10.10.3.2" xref="S4.E5.m1.5.5.5.1.1.1.10.10.3.2.cmml">α</mi></mrow></mrow></mrow><msup
    id="S4.E5.m1.5.5.5.1.1.1.3" xref="S4.E5.m1.5.5.5.1.1.1.3.cmml"><mi id="S4.E5.m1.5.5.5.1.1.1.3.2"
    xref="S4.E5.m1.5.5.5.1.1.1.3.2.cmml">b</mi><mn id="S4.E5.m1.5.5.5.1.1.1.3.3" xref="S4.E5.m1.5.5.5.1.1.1.3.3.cmml">2</mn></msup></mfrac></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S4.E5.m1.5b"><apply id="S4.E5.m1.5.6.1.cmml" xref="S4.E5.m1.5.5"><ci
    id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1">𝒵</ci><apply id="S4.E5.m1.5.6.1.3.cmml"
    xref="S4.E5.m1.5.5"><apply id="S4.E5.m1.3.3.3.3.1.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1"><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10"><apply id="S4.E5.m1.3.3.3.3.1.1.9.9.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9"><apply id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1"><apply id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.1.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.2.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.2.2">Ω</ci><list id="S4.E5.m1.3.3.3.3.1.1.2.2.2.3.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.2.2.2.4"><ci id="S4.E5.m1.3.3.3.3.1.1.1.1.1.1.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.1.1.1.1">𝑚</ci><ci id="S4.E5.m1.3.3.3.3.1.1.2.2.2.2.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.2.2.2.2">true</ci></list></apply><apply id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.1.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.2.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.1.1.1.3.2">Ω</ci><list id="S4.E5.m1.3.3.3.3.1.1.4.4.2.3.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.4.4.2.4"><ci id="S4.E5.m1.3.3.3.3.1.1.3.3.1.1.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.3.3.1.1">𝑚</ci><ci id="S4.E5.m1.3.3.3.3.1.1.4.4.2.2.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.4.4.2.2">mid</ci></list></apply></apply><apply id="S4.E5.m1.3.3.3.3.1.1.9.9.3.cmml"
    xref="S4.E5.m1.3.3.3.3.1.1.9.9.3"><ci id="S4.E5.m1.3.3.3.3.1.1.9.9.3.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.9.9.3.2">𝛼</ci></apply></apply><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.10.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10"><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1"><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2"><csymbol
    cd="ambiguous" id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2">subscript</csymbol><ci
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.2.2">𝜎</ci><list
    id="S4.E5.m1.3.3.3.3.1.1.6.6.2.3.cmml" xref="S4.E5.m1.3.3.3.3.1.1.6.6.2.4"><cn
    type="integer" id="S4.E5.m1.3.3.3.3.1.1.5.5.1.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.5.5.1.1">8</cn><ci
    id="S4.E5.m1.3.3.3.3.1.1.6.6.2.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.6.6.2.2">true</ci></list></apply><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3"><csymbol
    cd="ambiguous" id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3">subscript</csymbol><ci
    id="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.1.1.1.3.2">𝜎</ci><list
    id="S4.E5.m1.3.3.3.3.1.1.8.8.2.3.cmml" xref="S4.E5.m1.3.3.3.3.1.1.8.8.2.4"><cn
    type="integer" id="S4.E5.m1.3.3.3.3.1.1.7.7.1.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.7.7.1.1">8</cn><ci
    id="S4.E5.m1.3.3.3.3.1.1.8.8.2.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.8.8.2.2">mid</ci></list></apply></apply><apply
    id="S4.E5.m1.3.3.3.3.1.1.10.10.3.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3"><ci
    id="S4.E5.m1.3.3.3.3.1.1.10.10.3.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.10.10.3.2">𝛼</ci></apply></apply></apply><apply
    id="S4.E5.m1.3.3.3.3.1.1.12.cmml" xref="S4.E5.m1.3.3.3.3.1.1.12"><csymbol cd="ambiguous"
    id="S4.E5.m1.3.3.3.3.1.1.12.1.cmml" xref="S4.E5.m1.3.3.3.3.1.1.12">superscript</csymbol><ci
    id="S4.E5.m1.3.3.3.3.1.1.12.2.cmml" xref="S4.E5.m1.3.3.3.3.1.1.12.2">𝑎</ci><cn
    type="integer" id="S4.E5.m1.3.3.3.3.1.1.12.3.cmml" xref="S4.E5.m1.3.3.3.3.1.1.12.3">2</cn></apply></apply><apply
    id="S4.E5.m1.5.5.5.1.1.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1"><apply id="S4.E5.m1.5.5.5.1.1.1.10.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10"><apply id="S4.E5.m1.5.5.5.1.1.1.9.9.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9"><apply
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1"><apply
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2"><csymbol
    cd="ambiguous" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2">subscript</csymbol><ci
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.2.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.2.2">Ω</ci><list
    id="S4.E5.m1.5.5.5.1.1.1.2.2.2.3.cmml" xref="S4.E5.m1.5.5.5.1.1.1.2.2.2.4"><ci
    id="S4.E5.m1.5.5.5.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1.1.1.1.1">𝑚</ci><ci
    id="S4.E5.m1.5.5.5.1.1.1.2.2.2.2.cmml" xref="S4.E5.m1.5.5.5.1.1.1.2.2.2.2">true</ci></list></apply><apply
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3"><csymbol
    cd="ambiguous" id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3">subscript</csymbol><ci
    id="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.2.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.1.1.1.3.2">Ω</ci><list
    id="S4.E5.m1.5.5.5.1.1.1.4.4.2.3.cmml" xref="S4.E5.m1.5.5.5.1.1.1.4.4.2.4"><ci
    id="S4.E5.m1.5.5.5.1.1.1.3.3.1.1.cmml" xref="S4.E5.m1.5.5.5.1.1.1.3.3.1.1">𝑚</ci><ci
    id="S4.E5.m1.5.5.5.1.1.1.4.4.2.2.cmml" xref="S4.E5.m1.5.5.5.1.1.1.4.4.2.2">mid</ci></list></apply></apply><apply
    id="S4.E5.m1.5.5.5.1.1.1.9.9.3.cmml" xref="S4.E5.m1.5.5.5.1.1.1.9.9.3"><ci id="S4.E5.m1.5.5.5.1.1.1.9.9.3.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.9.9.3.2">𝛼</ci></apply></apply><apply id="S4.E5.m1.5.5.5.1.1.1.10.10.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10"><apply id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1"><apply id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.2.2">𝜎</ci><list id="S4.E5.m1.5.5.5.1.1.1.6.6.2.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.6.6.2.4"><cn type="integer" id="S4.E5.m1.5.5.5.1.1.1.5.5.1.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.5.5.1.1">8</cn><ci id="S4.E5.m1.5.5.5.1.1.1.6.6.2.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.6.6.2.2">true</ci></list></apply><apply id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.1.1.1.3.2">𝜎</ci><list id="S4.E5.m1.5.5.5.1.1.1.8.8.2.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.8.8.2.4"><cn type="integer" id="S4.E5.m1.5.5.5.1.1.1.7.7.1.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.7.7.1.1">8</cn><ci id="S4.E5.m1.5.5.5.1.1.1.8.8.2.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.8.8.2.2">mid</ci></list></apply></apply><apply id="S4.E5.m1.5.5.5.1.1.1.10.10.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.3"><ci id="S4.E5.m1.5.5.5.1.1.1.10.10.3.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.10.10.3.2">𝛼</ci></apply></apply></apply><apply id="S4.E5.m1.5.5.5.1.1.1.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.5.5.5.1.1.1.3.1.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.3">superscript</csymbol><ci id="S4.E5.m1.5.5.5.1.1.1.3.2.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.3.2">𝑏</ci><cn type="integer" id="S4.E5.m1.5.5.5.1.1.1.3.3.cmml"
    xref="S4.E5.m1.5.5.5.1.1.1.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.E5.m1.5c">\begin{split}\mathcal{Z}\,=\,&\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\cos{\alpha}+(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{a^{2}}+\\
    \\ &\frac{(\Omega_{m,\,\mathrm{true}}-\Omega_{m,\,\mathrm{mid}})\sin{\alpha}-(\sigma_{8,\,\mathrm{true}}-\sigma_{8,\,\mathrm{mid}})\sin{\alpha}}{b^{2}}\end{split}</annotation></semantics></math>
    |  | (5) |'
  prefs: []
  type: TYPE_NORMAL
- en: where $\alpha$ is the angle of the best fit 68% ellipse, $a$ is the length of
    the semimajor axis, and $b$ is the length of the semiminor axis. $\mathcal{Z}$,
    then, is a 2-dimensional z-score, where $\mathcal{Z}=1$ can be interpreted as
    the true value being on the edge of the 68% ellipse and $\mathcal{Z}=0$ means
    that the true and mean predicted values are identical. We note that this choice
    favors accuracy over precision because larger error ellipses are more forgiving
    of large offsets between the predicted and middle predicted cosmological models.
  prefs: []
  type: TYPE_NORMAL
- en: For each epoch, the mean squared error, MSE, as a function of epoch is calculated
    according to
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{MSE(e)}=\frac{1}{N_{\mathrm{sims}}}\sum_{i=1}^{N_{\mathrm{sims}}}\mathcal{Z}^{2}_{i}(e)$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: We select the epoch with the smallest MSE as the final model — and the model
    least likely to produce biased results — for the CNN and hCNN. Coincidentally,
    these “unbiased” models are from training epochs that are rather close to each
    other, epochs 520 and 524 ($\mathcal{E}\approx 0.95$) for the CNN and hCNN, respectively.
    Selecting, instead, to define a 2-D error ellipse that is averaged over all models
    and epochs selects the same hCNN model, but prefers a CNN model with marginally
    tighter error bars and a more significant offset.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the median and middle 68% predictions for each of the 40 cosmologies represented
    in the validation set at these unbiased epochs. As expected, the model visibly
    pulls toward the mean for outlying values of $\sigma_{8}$ and $\Omega_{m}$. The
    CNN and hCNN produce tighter correlations between the true and predicted values
    than does the NN.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Planck Testing Set Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ae723524bbaa610ea2e2146158e7405.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The MSE of the validation set, calculated in equations [5](#S4.E5
    "In 4.2.1 Unbiased Model Selection ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    and [6](#S4.E6 "In 4.2.1 Unbiased Model Selection ‣ 4.2 Testing Set Results ‣
    4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy
    Redshift Surveys"), is tightly correlated with the testing set error $\mathcal{Z}_{Planck}$.
    Shown are the binned median and 68% scatter for the CNN (blue dash) and hCNN (purple
    solid). The values tabulated here are restricted to epochs 501-550 ($0.91<\mathcal{E}\leq
    1.0$). The tighter correlation between low-MSE and low-$\mathcal{Z}_{Planck}$
    models is mildly more pronounced for the hCNN, suggesting that the hCNN might
    be a more robust approach for producing unbiased results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5826831ae53dae6ee4c825af09153972.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Testing set predictions of the NN (green dotted), CNN (blue dashed),
    and hCNN (purple filled); shown are the 68% and 95% error ellipses. The NN is
    heavily influenced by the degeneracy of the training simulations (gray x’s) in
    the $\sigma_{8}$-$\Omega_{m}$ plane, and predicts cosmological parameters that
    are significantly biased toward the mean. The CNN and hCNN have tighter error
    ellipses and smaller biases. The bias toward the mean is mildly smaller for the
    hCNN (white circle denoting the center of the error ellipse) compared to the CNN
    (white square).'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the training set comprises mock observations built from 40 matched-phase
    cosmological simulations, while the validation set comprises mock observations
    from a unique portion ($z<200h^{-1}\,\mathrm{Mpc}$) of those same simulations.
    In contrast, the testing set comprises mock observations from non-matched-phase
    simulations at the Planck cosmology which were populated with galaxies according
    to an HOD not yet seen by the trained model. With previously unseen cosmological
    parameters, HOD, and initial conditions, the Planck testing set is a more fair
    test of expected error and biases under a realistic set of conditions.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we posited that the MSE of the validation set might
    serve as a fair proxy assessment for selecting an unbiased model to apply to an
    unseen cosmology. Indeed, the validation MSE and the $\mathcal{Z}$ value for the
    Planck testing data (denoted $\mathcal{Z}_{Planck}$), are highly correlated, as
    shown in Figure [7](#S4.F7 "Figure 7 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2
    Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys"). The $\log(MSE)$-$\log(\mathcal{Z}_{Planck})$
    distribution has a Pearson R correlation coefficient of 0.88 for the CNN and a
    slightly tighter correlation of 0.93 for the hCNN. There is no strong evidence
    of evolution in the MSE-$\mathcal{Z}_{Planck}$ plane as a function of epoch; while
    low MSE is correlated with low $\mathcal{Z}_{Planck}$, the model is not taking
    a slow and steady march toward high or low MSE as it trains during epochs 501-550\.
    The model’s loss function should drive a decrease in mean absolute error across
    the 40 training cosmologies as it trains, while the test shown assesses a different
    measure of the goodness of fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing
    Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") shows the cosmological constraints for the NN,
    CNN, and hCNN. Despite the goodness of training suggested by the results in Figures
    [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine
    Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys"), [5](#S4.F5 "Figure 5 ‣ 4 Results ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"),
    and [6](#S4.F6 "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep
    Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys"),
    the NN never moves beyond predictions that are heavily influenced by the degeneracy
    of the training simulations. This is, perhaps, unsurprising. The power spectrum
    on which it is trained is calculated from a relatively small volume, $\sim 0.07\,h^{-3}\,\mathrm{Gpc}^{3}$,
    in contrast with the effective volume of $\sim 6\,\mathrm{Gpc}^{3}$ of the SDSS
    DR11 Baryon Oscillation Spectroscopic Survey (BOSS) observation (Gil-Marín et al.,
    [2015](#bib.bib24)). The volume of the mock observations used in this work is
    too small to isolate the baryon acoustic peak and reliably measure the acoustic
    scale. As a result, while the NN predicts $\sigma_{8}$ in an unbiased way, its
    predictions for $\Omega_{m}$ are biased very low and pull toward the mean $\Omega_{m}$
    of training simulations.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the NN, the CNN and hCNN predictions are substantially unbiased.
    The cosmological constraints in Figure [8](#S4.F8 "Figure 8 ‣ 4.2.2 Planck Testing
    Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys"), as well as the sample
    of low-$\mathcal{Z}_{Planck}$ models in Figure [7](#S4.F7 "Figure 7 ‣ 4.2.2 Planck
    Testing Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning
    Approach to Cosmological Constraints From Galaxy Redshift Surveys") suggest that
    the vector features included in the hCNN may make the model more robust to biasing,
    though the evidence for the effects of bias as a function of vector features is
    not strong.
  prefs: []
  type: TYPE_NORMAL
- en: Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set
    Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach to Cosmological Constraints
    From Galaxy Redshift Surveys") tabulates the simulation parameters and testing
    set results. For reference, we include the Planck testing set true values; recall
    that all simulations in the Planck suite of simulations were run at identical
    cosmologies, so the scatter of these values is $0$. Table [1](#S4.T1 "Table 1
    ‣ 4.2.2 Planck Testing Set Results ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    also gives parameters that describe the distribution of the training data for
    reference. These include the training set mean $\sigma_{8}$, mean $\Omega_{m}$,
    and the standard deviation of these, and are used as a benchmark for how the distribution
    of simulated cosmologies compares to the error bars presented.
  prefs: []
  type: TYPE_NORMAL
- en: For the trio of ML models, the mean ($\bar{x}$), offset ($\bar{x}-x_{\textit{Planck}}$),
    standard deviation of the predictions (denoted $\sigma$), and 1D $z$-score (offset$/\sigma$)
    are also given. The NN is the most biased of the trio, particularly in $\Omega_{m}$,
    with the mean prediction $\sim$1.3-$\sigma$ away from the true value. From the
    bias and error bars associated with the NN, we can conclude that the box volume
    is likely not large enough for the power spectrum to be diagnostic. Moving to
    a larger mock observations that can more reliably measure the acoustic scale is
    likely to improve the NN technique.
  prefs: []
  type: TYPE_NORMAL
- en: The CNN and hCNN, on the other hand, both predict $\sigma_{8}$ to within 3%
    and $\Omega_{m}$ to within 4%. The CNN and hCNN error bars are similarly sized,
    but the hCNN exhibits a bias that is smaller than the CNN by about a factor of
    $2$. However, the bias in both the CNN and hCNN are small, and further studies
    on larger mock observations are needed to make strong claims about the potential
    de-biasing advantage of the hCNN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Results Summary'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma_{8}$ | $\Omega_{m}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | mean | offset | $\sigma$ | $z$ | mean | offset | $\sigma$ | $z$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Training Set | $0.818$ |  $\cdots$ | $0.083$ |  $\cdots$ | $0.303$ |  $\cdots$
    | $0.027$ |  $\cdots$ |'
  prefs: []
  type: TYPE_TB
- en: '| Planck Testing Set | $0.830$ |  $\cdots$ |  $\cdots$ |  $\cdots$ | $0.314$
    |  $\cdots$ |  $\cdots$ |  $\cdots$ |'
  prefs: []
  type: TYPE_TB
- en: '| NN | $0.825$ | $0.005$ | $0.035$ | $0.147$ | $0.299$ | $0.015$ | $0.012$
    | $1.307$ |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | $0.824$ | $0.006$ | $0.022$ | $0.278$ | $0.311$ | $0.003$ | $0.012$
    | $0.268$ |'
  prefs: []
  type: TYPE_TB
- en: '| hCNN | $0.827$ | $0.003$ | $0.023$ | $0.144$ | $0.312$ | $0.002$ | $0.012$
    | $0.121$ |'
  prefs: []
  type: TYPE_TB
- en: 5 Discussion & Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented a trio of ML approaches for learning $\sigma_{8}$ and $\Omega_{m}$
    from a mock 3D galaxy survey. The neural network (NN) uses the binned power spectrum
    as input, and is processed through a fully connected neural network architecture.
    The convolutional neural network (CNN) uses a spatially binned 3D galaxy distribution;
    this is processed through a series of convolutions and pooling, and finally through
    a fully connected network. The hybrid CNN (hCNN) merges the two.
  prefs: []
  type: TYPE_NORMAL
- en: The methods are trained and tested on a sample of mock surveys are built on
    the AbacusCosmos suite of cosmological $N$-body simulations, and the mock surveys
    include a variety of galaxy formation scenarios through the implementation of
    generalized halo occupation distributions (HODs). The full training sample spans
    a large parameter space — 6 cosmological parameters and 6 HOD parameters.
  prefs: []
  type: TYPE_NORMAL
- en: We describe a number of best practices for preventing the a 3D CNN or 3D hCNN
    from memorizing structure and producing overly-optimistic results on the validation
    data. Most important is setting aside an independent portion of all simulations
    as a validation set to assess the goodness of fit. This validation set should
    ideally drawn from the same portion of the box to prevent the deep network from
    memorizing correlated structure across simulations stemming from simulations with
    matched initial phases. Other best practices include recentering the box, aggressive
    pooling to restrict the models’ knowledge of slab-size length scales, subsampling
    the galaxy catalog to prevent the model from learning from the aggregate number
    of galaxies within a volume, and employing the standard suite of axial flips and
    rotations to account for rotational invariance.
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that the validation set MSE is a useful proxy for selecting a
    model that will produce unbiased estimates of the cosmological parameters, even
    when presented with previously unseen cosmological and HOD parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is limited by the availability of simulated data: it is trained and
    tested on relatively small volumes ($\sim 0.07\,h^{-3}\,\mathrm{Gpc}^{3}$, which
    is 1/20 of the simulation box volume). Furthermore, we train with only 40 training
    simulations at a variety of cosmologies that vary in $\Omega_{CDM}\,h^{2}$, $\Omega_{b}\,h^{2}$,
    $\sigma_{8}$, $H_{0}$, $w_{0}$, and $n_{s}$, which have been populated with galaxies
    according to a flexible HOD with 6 parameters. Yet, even within these limitations
    — the small volumes and large cosmological and HOD parameter space — we have shown
    that it is possible to robustly train a model that can learn $\sigma_{8}$ and
    $\Omega_{m}$ directly from a catalog of galaxies.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing more realistic mock observations that span the cosmological and galaxy
    formation parameter space is an essential next step for applying 3D hybrid CNNs
    to observational data. These extensions to the existing mock observations include
    adopting more diversity in cosmological parameters, taking advantage of larger
    training mock observations, employing additional flexibility in galaxy models,
    and modeling real survey embeddings. As such training data become available, 3D
    hybrid CNNs have the potential to become a powerful tool for extracting cosmological
    information from next-generation spectroscopic surveys.
  prefs: []
  type: TYPE_NORMAL
- en: We thank Alexei Efros, Melanie Fernandez, Zoltán Haiman, Paul La Plante, José
    Manuel, Szymon Nakoneczny, Junier Oliva, Barnabás Póczos, Siamak Ravanbakhsh,
    Dezsö Ribli, Alexey Vikhlinin, and Javier Zazo for their helpful feedback on this
    project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abadi et al. (2016) Abadi, M., Barham, P., Chen, J., et al. 2016, in 12th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 16), 265–283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alam et al. (2017) Alam, S., Ata, M., Bailey, S., et al. 2017, MNRAS, 470, 2617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amendola et al. (2013) Amendola, L., Appleby, S., Bacon, D., et al. 2013, Living
    Reviews in Relativity, 16, 6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aragon-Calvo (2019) Aragon-Calvo, M. A. 2019, MNRAS, 484, 5771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bacon et al. (2000) Bacon, D. J., Refregier, A. R., & Ellis, R. S. 2000, MNRAS,
    318, 625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behroozi et al. (2012) Behroozi, P., Wechsler, R., & Wu, H.-Y. 2012, Rockstar:
    Phase-space halo finder, Astrophysics Source Code Library, ascl:1210.008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beltz-Mohrmann et al. (2019) Beltz-Mohrmann, G. D., Berlind, A. A., & Szewciw,
    A. O. 2019, arXiv e-prints, arXiv:1908.11448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berger & Stein (2019) Berger, P., & Stein, G. 2019, MNRAS, 482, 2861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berlind & Weinberg (2002) Berlind, A. A., & Weinberg, D. H. 2002, ApJ, 575,
    587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cacciato et al. (2013) Cacciato, M., van den Bosch, F. C., More, S., Mo, H.,
    & Yang, X. 2013, MNRAS, 430, 767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet (2015) Chollet, F. 2015, keras, [https://github.com/fchollet/keras](https://github.com/fchollet/keras)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cole et al. (2005) Cole, S., Percival, W. J., Peacock, J. A., et al. 2005, MNRAS,
    362, 505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dattilo et al. (2019) Dattilo, A., Vanderburg, A., Shallue, C. J., et al. 2019,
    arXiv e-prints, arXiv:1903.10507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Haan et al. (2016) de Haan, T., Benson, B. A., Bleem, L. E., et al. 2016,
    ApJ, 832, 95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Jong et al. (2014) de Jong, R. S., Barden, S., Bellido-Tirado, O., et al.
    2014, in Society of Photo-Optical Instrumentation Engineers (SPIE) Conference
    Series, Vol. 9147, Proc. SPIE, 91470M
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DES Collaboration et al. (2017) DES Collaboration, Abbott, T. M. C., Abdalla,
    F. B., et al. 2017, ArXiv e-prints, arXiv:1708.01530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DESI Collaboration et al. (2016) DESI Collaboration, Aghamousa, A., Aguilar,
    J., et al. 2016, arXiv e-prints, arXiv:1611.00036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eisenstein et al. (2005) Eisenstein, D. J., Zehavi, I., Hogg, D. W., et al.
    2005, ApJ, 633, 560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluri et al. (2019) Fluri, J., Kacprzak, T., Lucchi, A., et al. 2019, arXiv
    e-prints, arXiv:1906.03156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fukushima & Miyake (1982) Fukushima, K., & Miyake, S. 1982, in Competition and
    cooperation in neural nets (Springer), 267–285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2005) Gao, L., Springel, V., & White, S. D. M. 2005, MNRAS, 363,
    L66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garrison et al. (2018) Garrison, L. H., Eisenstein, D. J., Ferrer, D., et al.
    2018, The Astrophysical Journal Supplement Series, 236, 43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garrison et al. (2019) Garrison, L. H., Eisenstein, D. J., & Pinto, P. A. 2019,
    MNRAS, 485, 3370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gil-Marín et al. (2015) Gil-Marín, H., Noreña, J., Verde, L., et al. 2015, MNRAS,
    451, 539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gupta et al. (2018) Gupta, A., Matilla, J. M. Z., Hsu, D., & Haiman, Z. 2018,
    Phys. Rev. D, 97, 103515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) He, S., Li, Y., Feng, Y., et al. 2018, arXiv e-prints, arXiv:1811.06533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hearin et al. (2016) Hearin, A. P., Zentner, A. R., van den Bosch, F. C., Campbell,
    D., & Tollerud, E. 2016, MNRAS, 460, 2552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hikage et al. (2019) Hikage, C., Oguri, M., Hamana, T., et al. 2019, PASJ, 71,
    43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hildebrandt et al. (2018) Hildebrandt, H., Köhlinger, F., van den Busch, J. L.,
    et al. 2018, arXiv e-prints, arXiv:1812.06076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2019) Ho, M., Rau, M. M., Ntampaka, M., et al. 2019, arXiv e-prints,
    arXiv:1902.05950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huchra et al. (1990) Huchra, J. P., Geller, M. J., de Lapparent, V., & Corwin,
    Harold G., J. 1990, ApJS, 72, 433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeong (2010) Jeong, D. 2010, PhD thesis, University of Texas at Austin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2013) Ji, S., Xu, W., Yang, M., & Yu, K. 2013, IEEE transactions
    on pattern analysis and machine intelligence, 35, 221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiser et al. (2000) Kaiser, N., Wilson, G., & Luppino, G. A. 2000, arXiv e-prints,
    astro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamnitsas et al. (2016) Kamnitsas, K., Ledig, C., Newcombe, V. F. J., et al.
    2016, CoRR, abs/1603.05959, arXiv:1603.05959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Ba (2014) Kingma, D. P., & Ba, J. 2014, ArXiv e-prints, arXiv:1412.6980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kobayashi et al. (2019) Kobayashi, Y., Nishimichi, T., Takada, M., & Takahashi,
    R. 2019, arXiv e-prints, arXiv:1907.08515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., & Hinton, G. E. 2012,
    in Advances in Neural Information Processing Systems 25, ed. F. Pereira, C. J. C.
    Burges, L. Bottou, & K. Q. Weinberger (Curran Associates, Inc.), 1097–1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwan et al. (2015) Kwan, J., Heitmann, K., Habib, S., et al. 2015, ApJ, 810,
    35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: La Plante & Ntampaka (2018) La Plante, P., & Ntampaka, M. 2018, ArXiv e-prints,
    arXiv:1810.08211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lanusse et al. (2018) Lanusse, F., Ma, Q., Li, N., et al. 2018, MNRAS, 473,
    3895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1999) LeCun, Y., Haffner, P., Bottou, L., & Bengio, Y. 1999, in
    Shape, contour and grouping in computer vision (Springer), 319–345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mantz et al. (2015) Mantz, A. B., von der Linden, A., Allen, S. W., et al. 2015,
    MNRAS, 446, 2205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathuriya et al. (2018) Mathuriya, A., Bard, D., Mendygral, P., et al. 2018,
    arXiv e-prints, arXiv:1808.04728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More et al. (2013) More, S., van den Bosch, F. C., Cacciato, M., et al. 2013,
    MNRAS, 430, 747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naidoo et al. (2019) Naidoo, K., Whiteway, L., Massara, E., et al. 2019, arXiv
    e-prints, arXiv:1907.00989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair & Hinton (2010) Nair, V., & Hinton, G. E. 2010, in Proceedings of the 27th
    international conference on machine learning (ICML-10), 807–814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ntampaka et al. (2018) Ntampaka, M., ZuHone, J., Eisenstein, D., et al. 2018,
    arXiv e-prints, arXiv:1810.07703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2019) Pan, S., Liu, M., Forero-Romero, J., et al. 2019, arXiv e-prints,
    arXiv:1908.10590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peacock & Smith (2000) Peacock, J. A., & Smith, R. E. 2000, MNRAS, 318, 1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peel et al. (2018) Peel, A., Lalande, F., Starck, J.-L., et al. 2018, ArXiv
    e-prints, arXiv:1810.11030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percival et al. (2001) Percival, W. J., Baugh, C. M., Bland-Hawthorn, J., et al.
    2001, MNRAS, 327, 1297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perraudin et al. (2019) Perraudin, N., Defferrard, M., Kacprzak, T., & Sgier,
    R. 2019, Astronomy and Computing, 27, 130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planck Collaboration et al. (2014a) Planck Collaboration, Ade, P. A. R., Aghanim,
    N., et al. 2014a, A&A, 571, A16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planck Collaboration et al. (2014b) —. 2014b, A&A, 571, A24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planck Collaboration et al. (2015) —. 2015, ArXiv e-prints, arXiv:1502.01589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravanbakhsh et al. (2017) Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al.
    2017, arXiv e-prints, arXiv:1711.02033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribli et al. (2019a) Ribli, D., Ármin Pataki, B., Zorrilla Matilla, J. M., et al.
    2019a, arXiv e-prints, arXiv:1902.03663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribli et al. (2019b) Ribli, D., Pataki, B. Á., & Csabai, I. 2019b, Nature Astronomy,
    3, 93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmelzle et al. (2017) Schmelzle, J., Lucchi, A., Kacprzak, T., et al. 2017,
    arXiv e-prints, arXiv:1707.05167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber (2014) Schmidhuber, J. 2014, ArXiv e-prints, arXiv:1404.7828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoccimarro et al. (2001) Scoccimarro, R., Sheth, R. K., Hui, L., & Jain, B.
    2001, ApJ, 546, 20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shectman et al. (1996) Shectman, S. A., Landy, S. D., Oemler, A., et al. 1996,
    ApJ, 470, 172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2014) Simonyan, K., & Zisserman, A. 2014, CoRR, abs/1409.1556,
    arXiv:1409.1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
    I., & Salakhutdinov, R. 2014, The Journal of Machine Learning Research, 15, 1929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takada et al. (2014) Takada, M., Ellis, R. S., Chiba, M., et al. 2014, PASJ,
    66, R1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tegmark et al. (2004) Tegmark, M., Blanton, M. R., Strauss, M. A., et al. 2004,
    ApJ, 606, 702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Bosch et al. (2013) van den Bosch, F. C., More, S., Cacciato, M., Mo,
    H., & Yang, X. 2013, MNRAS, 430, 725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Waerbeke et al. (2000) Van Waerbeke, L., Mellier, Y., Erben, T., et al.
    2000, A&A, 358, 30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vikhlinin et al. (2009) Vikhlinin, A., Kravtsov, A. V., Burenin, R. A., et al.
    2009, ApJ, 692, 1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang, K., Mao, Y.-Y., Zentner, A. R., et al. 2019, arXiv
    e-prints, arXiv:1903.09656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wechsler et al. (2006) Wechsler, R. H., Zentner, A. R., Bullock, J. S., Kravtsov,
    A. V., & Allgood, B. 2006, ApJ, 652, 71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weinberg et al. (2013) Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J.,
    et al. 2013, Phys. Rep., 530, 87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wittman et al. (2000) Wittman, D. M., Tyson, J. A., Kirkman, D., Dell’Antonio,
    I., & Bernstein, G. 2000, Nature, 405, 143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: York et al. (2000) York, D. G., Adelman, J., Anderson, John E., J., et al. 2000,
    AJ, 120, 1579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2018a) Yuan, S., Eisenstein, D. J., & Garrison, L. H. 2018a, MNRAS,
    478, 2019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2018b) —. 2018b, GRAND-HOD: GeneRalized ANd Differentiable Halo
    Occupation Distribution, ascl:1812.011'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2019) Yuan, S., Eisenstein, D. J., & Leauthaud, A. 2019, arXiv
    e-prints, arXiv:1907.05909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019) Zhang, X., Wang, Y., Zhang, W., et al. 2019, arXiv e-prints,
    arXiv:1902.05965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng & Weinberg (2007) Zheng, Z., & Weinberg, D. H. 2007, ApJ, 659, 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2005) Zheng, Z., Berlind, A. A., Weinberg, D. H., et al. 2005,
    ApJ, 633, 791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A On the Life Cycle of CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNNs are traditionally trained to minimize a loss function such as mean squared
    or absolute error, yet it is not obvious that this is an ideal approach for astronomical
    and cosmological applications. In this section, we present more on the life cycle
    of our CNN and show additional plots that have been useful in interpreting fits
    and designing our two-phase training scheme.
  prefs: []
  type: TYPE_NORMAL
- en: 'While figures showing traditional metrics can be diagnostic, they can be difficult
    to interpret for models that regress more than one parameter. Such traditional
    figures include error as a function of epoch (e.g., Figure [4](#S3.F4 "Figure
    4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN ‣ 3 Methods: Machine Learning Models ‣
    A Hybrid Deep Learning Approach to Cosmological Constraints From Galaxy Redshift
    Surveys")) and 1-to-1 scatter of true and predicted values (e.g., Figure [6](#S4.F6
    "Figure 6 ‣ 4.2 Testing Set Results ‣ 4 Results ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")). It is concerning
    that typical early stopping routines rely on these test statistics to determine
    when a model is well fit because using such diagnostics blindly can lead to unexpected
    or overpessimistic results.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure [9](#A1.F9 "Figure 9 ‣ Appendix A On the Life Cycle of CNNs ‣ A Hybrid
    Deep Learning Approach to Cosmological Constraints From Galaxy Redshift Surveys")
    shows the validation data 2D predictions as a function of epoch. Unsurprisingly,
    at epochs as early as 5, the model has learned to predict a mean value but cannot
    differentiate among models. This is encouraging and expected; the model, which
    is initialized to completely random weights and biases, learned a reasonable values
    for $\sigma_{8}$ and $\Omega_{m}$ in the first few epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model predictions at epoch 30, though, are a surprising extension of this
    predition of the mean. In Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2
    Standard NN ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys"), the error as a function
    of epoch slowly and steadily decreases for the first few epochs, then begins to
    oscillate. At epoch $\sim 30$, this initial plunge has come to an end, and an
    error-based early stopping scheme might suggest that these results are sufficient.
    A one-to-one plot of true and predicted $\sigma_{8}$ and $\Omega_{m}$ will tell
    a similar story — the results bias toward the mean and the scatter is larger than
    is to be desired, but the model has clearly learned trends in the data and a diversity
    of $\sigma_{8}$ and $\Omega_{m}$ values. Yet, when viewed as a scatter plot in
    the $\sigma_{8}$-$\Omega_{m}$ plane (in the top right corner of Figure [9](#A1.F9
    "Figure 9 ‣ Appendix A On the Life Cycle of CNNs ‣ A Hybrid Deep Learning Approach
    to Cosmological Constraints From Galaxy Redshift Surveys")), it is clear that
    the CNN has learned a 2D version of predicting the mean: it has produced predictions
    that spread along the degeneracy direction of the training simulations, with the
    predictions arranged in a sensible way (i.e. the predictions of the high-$\sigma_{8}$
    simulations are indeed at high $\sigma_{8}$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is only by delving into a “high error” regime that the CNN starts to make
    progress beyond this tight degeneracy. Between epochs 30 and 175, we see large
    oscillations in MSE. Epoch 100 is shown as an example epochs in this region. Despite
    the fact that Figure [4](#S3.F4 "Figure 4 ‣ 3.2.1 Hybrid CNN ‣ 3.2 Standard NN
    ‣ 3 Methods: Machine Learning Models ‣ A Hybrid Deep Learning Approach to Cosmological
    Constraints From Galaxy Redshift Surveys") shows error increasing and oscillating
    in this epoch range, something important and meaningful is happening under the
    surface. The model is starting to produce more diversity in predictions, expanding
    the range of predictions in the direction orthogonal to the degeneracy of the
    simulations. At epoch 175, the predictions are still biased toward the mean, but
    at least span a wider spectrum of possibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can take an alternate timeline and continue with phase 1 of the training
    scheme for a few more epochs. Recall that, in the training scheme presented in
    the main text, we transition to a lower learning rate at epoch 175\. At epochs
    219 and 220 in this alternate timeline, we begin to see the oscillations in bias.
    While the results for epoch 219 look reasonable, the results for epoch 220 are
    biased to very low $\sigma_{8}$; such large swings in biases hint that the step
    size is too large.
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative timeline transitions from phase 1 (high learning rate) to
    phase 2 (lower learning rate) as early as epoch 30, with disastrous results. The
    epoch 30 model has not yet learned much beyond the degeneracy of the simulations,
    and when it is moved to a much smaller learning rate, it fails to learn a diversity
    of predictions in the $\sigma_{8}$-$\Omega_{m}$ plane, instead producing predictions
    along a tight curve for many epochs.
  prefs: []
  type: TYPE_NORMAL
- en: While they are certainly valuable, traditional methods for understanding how
    well a CNN has fit can be difficult to interpret, particularly when assessing
    models trained to predict multiple parameters. Employing early stopping routines
    that assess a single statistical measurement of error can lead to models that
    have not yet learned a range of predictions in the parameter space. Appropriately
    assessing the diversity of predictions, identifying epochs to stop training, and
    developing intuition for training deep models will an essential step toward properly
    using these powerful tools in astronomical and cosmological applications.
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Refer to caption](img/eaef8a95b46ea9e9a54fad7c23f46550.png) | ![Refer to
    caption](img/bb71554b9708fcb3b64b6e6512209788.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/d1339a07da7f6d64b3a7d634edb13f96.png) | ![Refer to
    caption](img/031afe4cbcbcc4ce55fcf64b4bc8e21c.png) |'
  prefs: []
  type: TYPE_TB
- en: '| ![Refer to caption](img/9e5634cc597c74aee0f37fc22b4f60c2.png) | ![Refer to
    caption](img/2005e518fb212253d75eb153640f027d.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 9: The life cycle of CNNs. Training data (crosses) are colored according
    to their $\sigma_{8}$ values, and predictions on the validation data (circles)
    are likewise colored according to their *true* (not predicted) $\sigma_{8}$ values.
    Early in training, the model learns reasonable values for $\sigma_{8}$ and $\Omega_{m}$,
    eventually learning a tight degeneracy in this space, and finally achieving a
    more diverse representation of the simulations. Shown are, from top left to bottom
    right, epochs 5, 30, 100, 175, 219, and 220 in phase 1 of the training scheme.'
  prefs: []
  type: TYPE_NORMAL
