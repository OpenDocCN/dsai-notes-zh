- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»ï¼šæœªåˆ†ç±»
- en: 'date: 2024-09-06 20:07:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 20:07:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1810.05587] A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1810.05587] å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸æ‰¹è¯„Â¹Â¹ æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1810.05587](https://ar5iv.labs.arxiv.org/html/1810.05587)
- en: 'A Survey and Critique of Multiagent Deep Reinforcement LearningÂ¹Â¹1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸æ‰¹è¯„Â¹Â¹ æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€
- en: Pablo Hernandez-Leal, Bilal Kartal and Matthew E. Taylor
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pablo Hernandez-Leal, Bilal Kartal å’Œ Matthew E. Taylor
- en: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{pablo.hernandez,bilal.kartal,matthew.taylor}@borealisai.com Borealis AI'
- en: Edmonton, Canada
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ æ‹¿å¤§ï¼ŒåŸƒå¾·è’™é¡¿
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'Deep reinforcement learning (RL) has achieved outstanding results in recent
    years. This has led to a dramatic increase in the number of applications and methods.
    Recent works have explored learning beyond single-agent scenarios and have considered
    multiagent learning (MAL) scenarios. Initial results report successes in complex
    multiagent domains, although there are several challenges to be addressed. The
    primary goal of this article is to provide a clear overview of current multiagent
    deep reinforcement learning (MDRL) literature. Additionally, we complement the
    overview with a broader analysis: (i) we revisit previous key components, originally
    presented in MAL and RL, and highlight how they have been adapted to multiagent
    deep reinforcement learning settings. (ii) We provide general guidelines to new
    practitioners in the area: describing lessons learned from MDRL works, pointing
    to recent benchmarks, and outlining open avenues of research. (iii) We take a
    more critical tone raising practical challenges of MDRL (e.g., implementation
    and computational demands). We expect this article will help unify and motivate
    future research to take advantage of the abundant literature that exists (e.g.,
    RL and MAL) in a joint effort to promote fruitful research in the multiagent community.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‘å¹´æ¥å–å¾—äº†å“è¶Šçš„æˆæœã€‚è¿™å¯¼è‡´äº†åº”ç”¨å’Œæ–¹æ³•æ•°é‡çš„æ˜¾è‘—å¢åŠ ã€‚è¿‘æœŸå·¥ä½œæ¢ç´¢äº†è¶…è¶Šå•ä¸€æ™ºèƒ½ä½“åœºæ™¯çš„å­¦ä¹ ï¼Œå¹¶è€ƒè™‘äº†å¤šæ™ºèƒ½ä½“å­¦ä¹ ï¼ˆMALï¼‰åœºæ™¯ã€‚åˆæ­¥ç»“æœæŠ¥å‘Šäº†åœ¨å¤æ‚çš„å¤šæ™ºèƒ½ä½“é¢†åŸŸä¸­çš„æˆåŠŸï¼Œå°½ç®¡å­˜åœ¨è‹¥å¹²æŒ‘æˆ˜éœ€è¦è§£å†³ã€‚æœ¬æ–‡çš„ä¸»è¦ç›®æ ‡æ˜¯æä¾›å½“å‰å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆMDRLï¼‰æ–‡çŒ®çš„æ¸…æ™°æ¦‚è¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ›´å¹¿æ³›çš„åˆ†ææ¥è¡¥å……è¿™ä¸€æ¦‚è¿°ï¼šï¼ˆiï¼‰æˆ‘ä»¬é‡æ–°å®¡è§†äº†æœ€åˆåœ¨MALå’ŒRLä¸­æå‡ºçš„å…³é”®ç»„ä»¶ï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬å¦‚ä½•è¢«é€‚åº”åˆ°å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­ã€‚ï¼ˆiiï¼‰æˆ‘ä»¬å‘æ–°ä»ä¸šè€…æä¾›ä¸€èˆ¬æ€§æŒ‡å—ï¼šæè¿°ä»MDRLå·¥ä½œä¸­è·å¾—çš„ç»éªŒæ•™è®­ï¼ŒæŒ‡å‘æœ€è¿‘çš„åŸºå‡†ï¼Œå¹¶æ¦‚è¿°å¼€æ”¾çš„ç ”ç©¶é€”å¾„ã€‚ï¼ˆiiiï¼‰æˆ‘ä»¬é‡‡å–æ›´æ‰¹åˆ¤çš„è¯­æ°”ï¼Œæå‡ºMDRLçš„å®é™…æŒ‘æˆ˜ï¼ˆä¾‹å¦‚ï¼Œå®æ–½å’Œè®¡ç®—éœ€æ±‚ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©ç»Ÿä¸€å’Œæ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œåˆ©ç”¨ç°æœ‰çš„ä¸°å¯Œæ–‡çŒ®ï¼ˆä¾‹å¦‚ï¼ŒRLå’ŒMALï¼‰ï¼Œå…±åŒæ¨åŠ¨å¤šæ™ºèƒ½ä½“ç¤¾åŒºçš„æœ‰ç›Šç ”ç©¶ã€‚
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 å¼•è¨€
- en: 'Almost 20 years ago Stone and Velosoâ€™s seminal surveyÂ [[1](#bib.bib1)] laid
    the groundwork for defining the area of multiagent systems (MAS) and its open
    problems in the context of AI. About ten years ago, Shoham, Powers, and GrenagerÂ [[2](#bib.bib2)]
    noted that the literature on multiagent learning (MAL) was growing and it was
    not possible to enumerate all relevant articles. Since then, the number of published
    MAL works continues to steadily rise, which led to different surveys on the area,
    ranging from analyzing the basics of MAL and their challengesÂ [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)], to addressing specific subareas: game theory
    and MALÂ [[2](#bib.bib2), [6](#bib.bib6)], cooperative scenariosÂ [[7](#bib.bib7),
    [8](#bib.bib8)], and evolutionary dynamics of MALÂ [[9](#bib.bib9)]. In just the
    last couple of years, three surveys related to MAL have been published: learning
    in non-stationary environmentsÂ [[10](#bib.bib10)], agents modeling agentsÂ [[11](#bib.bib11)],
    and transfer learning in multiagent RLÂ [[12](#bib.bib12)].'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¹20å¹´å‰ï¼ŒStoneå’ŒVelosoçš„å¼€åˆ›æ€§è°ƒæŸ¥[[1](#bib.bib1)]ä¸ºå®šä¹‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰é¢†åŸŸåŠå…¶åœ¨AIèƒŒæ™¯ä¸‹çš„å¼€æ”¾é—®é¢˜å¥ å®šäº†åŸºç¡€ã€‚å¤§çº¦åå¹´å‰ï¼ŒShohamã€Powerså’ŒGrenager[[2](#bib.bib2)]æŒ‡å‡ºï¼Œå¤šæ™ºèƒ½ä½“å­¦ä¹ ï¼ˆMALï¼‰æ–‡çŒ®åœ¨å¢é•¿ï¼Œæ— æ³•åˆ—ä¸¾æ‰€æœ‰ç›¸å…³æ–‡ç« ã€‚ä»é‚£æ—¶èµ·ï¼Œå·²å‘è¡¨çš„MALä½œå“æ•°é‡æŒç»­ä¸Šå‡ï¼Œè¿™å¯¼è‡´äº†è¯¥é¢†åŸŸçš„ä¸åŒè°ƒæŸ¥ï¼Œä»åˆ†æMALçš„åŸºç¡€åŠå…¶æŒ‘æˆ˜[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]ï¼Œåˆ°è§£å†³ç‰¹å®šå­é¢†åŸŸï¼šåšå¼ˆè®ºå’ŒMAL[[2](#bib.bib2), [6](#bib.bib6)]ï¼Œåˆä½œåœºæ™¯[[7](#bib.bib7),
    [8](#bib.bib8)]ï¼Œä»¥åŠMALçš„è¿›åŒ–åŠ¨æ€[[9](#bib.bib9)]ã€‚ä»…åœ¨è¿‡å»å‡ å¹´å†…ï¼Œå·²å‘å¸ƒäº†ä¸‰é¡¹ä¸MALç›¸å…³çš„è°ƒæŸ¥ï¼šéå¹³ç¨³ç¯å¢ƒä¸­çš„å­¦ä¹ [[10](#bib.bib10)]ï¼Œæ™ºèƒ½ä½“å¯¹æ™ºèƒ½ä½“å»ºæ¨¡[[11](#bib.bib11)]ï¼Œä»¥åŠå¤šæ™ºèƒ½ä½“RLä¸­çš„è¿ç§»å­¦ä¹ [[12](#bib.bib12)]ã€‚
- en: The research interest in MAL has been accompanied by successes in artificial
    intelligence, first, in single-agent video gamesÂ [[13](#bib.bib13)]; more recently,
    in two-player games, for example, playing GoÂ [[14](#bib.bib14), [15](#bib.bib15)],
    pokerÂ [[16](#bib.bib16), [17](#bib.bib17)], and games of two competing teams,
    e.g., DOTA 2Â [[18](#bib.bib18)] and StarCraft IIÂ [[19](#bib.bib19)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹MALçš„ç ”ç©¶å…´è¶£ä¸äººå·¥æ™ºèƒ½çš„æˆåŠŸç›¸ä¼´è€Œç”Ÿï¼Œé¦–å…ˆæ˜¯åœ¨å•æ™ºèƒ½ä½“è§†é¢‘æ¸¸æˆä¸­[[13](#bib.bib13)]ï¼›æ›´è¿‘æœŸçš„æˆåŠŸåˆ™æ˜¯åœ¨åŒäººæ¸¸æˆä¸­ï¼Œä¾‹å¦‚å›´æ£‹[[14](#bib.bib14),
    [15](#bib.bib15)]ï¼Œæ‰‘å…‹[[16](#bib.bib16), [17](#bib.bib17)]ï¼Œä»¥åŠä¸¤ä¸ªç«äº‰å›¢é˜Ÿçš„æ¸¸æˆï¼Œä¾‹å¦‚DOTA 2[[18](#bib.bib18)]å’ŒStarCraft
    II[[19](#bib.bib19)]ã€‚
- en: 'While different techniques and algorithms were used in the above scenarios,
    in general, they are all a combination of techniques from two main areas: reinforcement
    learning (RL)Â [[20](#bib.bib20)] and deep learningÂ [[21](#bib.bib21), [22](#bib.bib22)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åœ¨ä¸Šè¿°åœºæ™¯ä¸­ä½¿ç”¨äº†ä¸åŒçš„æŠ€æœ¯å’Œç®—æ³•ï¼Œä½†ä¸€èˆ¬è€Œè¨€ï¼Œå®ƒä»¬éƒ½æ˜¯æ¥è‡ªä¸¤ä¸ªä¸»è¦é¢†åŸŸçš„æŠ€æœ¯ç»„åˆï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰[[20](#bib.bib20)]å’Œæ·±åº¦å­¦ä¹ [[21](#bib.bib21),
    [22](#bib.bib22)]ã€‚
- en: RL is an area of machine learning where an agent learns by interacting (i.e.,
    taking actions) within a dynamic environment. However, one of the main challenges
    to RL, and traditional machine learning in general, is the need for manually designing
    quality features on which to learn. Deep learning enables efficient representation
    learning, thus allowing the automatic discovery of featuresÂ [[21](#bib.bib21),
    [22](#bib.bib22)]. In recent years, deep learning has had successes in different
    areas such as computer vision and natural language processingÂ [[21](#bib.bib21),
    [22](#bib.bib22)]. One of the key aspects of deep learning is the use of *neural
    networks* (NNs) that can find compact representations in high-dimensional dataÂ [[23](#bib.bib23)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé¢†åŸŸï¼Œå…¶ä¸­ä¸€ä¸ªæ™ºèƒ½ä½“é€šè¿‡åœ¨åŠ¨æ€ç¯å¢ƒä¸­äº¤äº’ï¼ˆå³é‡‡å–è¡ŒåŠ¨ï¼‰æ¥å­¦ä¹ ã€‚ç„¶è€Œï¼ŒRLä»¥åŠä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯éœ€è¦æ‰‹åŠ¨è®¾è®¡è´¨é‡ç‰¹å¾æ¥è¿›è¡Œå­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ å®ç°äº†é«˜æ•ˆçš„è¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œå…è®¸è‡ªåŠ¨å‘ç°ç‰¹å¾[[21](#bib.bib21),
    [22](#bib.bib22)]ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä¸åŒé¢†åŸŸå–å¾—äº†æˆåŠŸ[[21](#bib.bib21), [22](#bib.bib22)]ã€‚æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯ä½¿ç”¨*ç¥ç»ç½‘ç»œ*ï¼ˆNNsï¼‰ï¼Œå®ƒä»¬å¯ä»¥åœ¨é«˜ç»´æ•°æ®ä¸­æ‰¾åˆ°ç´§å‡‘çš„è¡¨ç¤º[[23](#bib.bib23)]ã€‚
- en: In deep reinforcement learning (DRL)Â [[23](#bib.bib23), [24](#bib.bib24)] deep
    neural networks are trained to approximate the optimal policy and/or the value
    function. In this way the deep NN, serving as function approximator, enables powerful
    generalization. One of the key advantages of DRL is that it enables RL to scale
    to problems with high-dimensional state and action spaces. However, most existing
    successful DRL applications so far have been on visual domains (e.g., Atari games),
    and there is still a lot of work to be done for more realistic applicationsÂ [[25](#bib.bib25),
    [26](#bib.bib26)] with complex dynamics, which are not necessarily vision-based.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ä¸­[[23](#bib.bib23), [24](#bib.bib24)]ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œè¢«è®­ç»ƒæ¥é€¼è¿‘æœ€ä¼˜ç­–ç•¥å’Œ/æˆ–ä»·å€¼å‡½æ•°ã€‚è¿™ç§æ–¹å¼ä¸­ï¼Œä½œä¸ºå‡½æ•°é€¼è¿‘å™¨çš„æ·±åº¦ç¥ç»ç½‘ç»œèƒ½å¤Ÿå®ç°å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚DRLçš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯ï¼Œå®ƒä½¿å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ‰©å±•åˆ°å…·æœ‰é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°ç°æœ‰æˆåŠŸçš„DRLåº”ç”¨éƒ½é›†ä¸­åœ¨è§†è§‰é¢†åŸŸï¼ˆä¾‹å¦‚Atariæ¸¸æˆï¼‰ï¼Œå¯¹äºæ›´åŠ ç°å®çš„ã€å¤æ‚åŠ¨æ€çš„åº”ç”¨ä»éœ€å¤§é‡å·¥ä½œ[[25](#bib.bib25),
    [26](#bib.bib26)]ï¼Œè¿™äº›åº”ç”¨ä¸ä¸€å®šæ˜¯åŸºäºè§†è§‰çš„ã€‚
- en: 'DRL has been regarded as an important component in constructing general AI
    systemsÂ [[27](#bib.bib27)] and has been successfully integrated with other techniques,
    e.g., searchÂ [[14](#bib.bib14)], planningÂ [[28](#bib.bib28)], and more recently
    with multiagent systems, with an emerging area of *multiagent deep reinforcement
    learning* *(MDRL)*[[29](#bib.bib29), [30](#bib.bib30)].Â²Â²2We have noted inconsistency
    in abbreviations such as: D-MARL, MADRL, deep-multiagent RL and MA-DRL.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è¢«è§†ä¸ºæ„å»ºé€šç”¨äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†[[27](#bib.bib27)]ï¼Œå¹¶å·²æˆåŠŸåœ°ä¸å…¶ä»–æŠ€æœ¯é›†æˆï¼Œä¾‹å¦‚æœç´¢[[14](#bib.bib14)]ã€è§„åˆ’[[28](#bib.bib28)]ï¼Œä»¥åŠæœ€è¿‘çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå…¶ä¸­åŒ…æ‹¬æ–°å…´é¢†åŸŸ*å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ *ï¼ˆ*MDRL*ï¼‰[[29](#bib.bib29),
    [30](#bib.bib30)]ã€‚Â²Â²2æˆ‘ä»¬æ³¨æ„åˆ°ç¼©å†™å­˜åœ¨ä¸ä¸€è‡´ï¼Œå¦‚ï¼šD-MARLï¼ŒMADRLï¼Œæ·±åº¦å¤šæ™ºèƒ½ä½“RLå’ŒMA-DRLã€‚
- en: Learning in multiagent settings is fundamentally more difficult than the single-agent
    case due to the presence of multiagent pathologies, e.g., the moving target problem
    (non-stationarity)Â [[2](#bib.bib2), [5](#bib.bib5), [10](#bib.bib10)], curse of
    dimensionalityÂ [[2](#bib.bib2), [5](#bib.bib5)], multiagent credit assignmentÂ [[31](#bib.bib31),
    [32](#bib.bib32)], global explorationÂ [[8](#bib.bib8)], and relative overgeneralizationÂ [[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]. Despite this complexity, top AI conferences
    like AAAI, ICML, ICLR, IJCAI and NeurIPS, and specialized conferences such as
    AAMAS, have published works reporting successes in MDRL. In light of these works,
    we believe it is pertinent to first, have an overview of the recent MDRL works,
    and second, understand how these recent works relate to the existing literature.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­å­¦ä¹ åŸºæœ¬ä¸Šæ¯”å•ä¸€æ™ºèƒ½ä½“æƒ…å†µæ›´åŠ å›°éš¾ï¼Œå› ä¸ºå­˜åœ¨å¤šæ™ºèƒ½ä½“ç—…æ€é—®é¢˜ï¼Œä¾‹å¦‚ç§»åŠ¨ç›®æ ‡é—®é¢˜ï¼ˆéé™æ€æ€§ï¼‰[[2](#bib.bib2), [5](#bib.bib5),
    [10](#bib.bib10)]ï¼Œç»´åº¦è¯…å’’[[2](#bib.bib2), [5](#bib.bib5)]ï¼Œå¤šæ™ºèƒ½ä½“ä¿¡ç”¨åˆ†é…[[31](#bib.bib31),
    [32](#bib.bib32)]ï¼Œå…¨å±€æ¢ç´¢[[8](#bib.bib8)]ï¼Œä»¥åŠç›¸å¯¹æ³›åŒ–è¿‡åº¦[[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35)]ã€‚å°½ç®¡å­˜åœ¨è¿™äº›å¤æ‚æ€§ï¼ŒåƒAAAIã€ICMLã€ICLRã€IJCAIå’ŒNeurIPSè¿™æ ·çš„é¡¶çº§äººå·¥æ™ºèƒ½ä¼šè®®ï¼Œä»¥åŠä¸“é—¨çš„ä¼šè®®å¦‚AAMASï¼Œéƒ½å‘è¡¨äº†å…³äºMDRLæˆåŠŸçš„ä½œå“ã€‚é‰´äºè¿™äº›ä½œå“ï¼Œæˆ‘ä»¬è®¤ä¸ºé¦–å…ˆï¼Œæ¦‚è¿°æœ€è¿‘çš„MDRLä½œå“æ˜¯ç›¸å…³çš„ï¼Œå…¶æ¬¡ï¼Œç†è§£è¿™äº›æœ€è¿‘çš„ä½œå“å¦‚ä½•ä¸ç°æœ‰æ–‡çŒ®ç›¸å…³ã€‚
- en: This article contributes to the state of the art with a brief survey of the
    current works in MDRL in an effort to complement existing surveys on multiagent
    learningÂ [[36](#bib.bib36), [10](#bib.bib10)], cooperative learningÂ [[7](#bib.bib7),
    [8](#bib.bib8)], agents modeling agentsÂ [[11](#bib.bib11)], knowledge reuse in
    multiagent RLÂ [[12](#bib.bib12)], and (single-agent) deep reinforcement learningÂ [[23](#bib.bib23),
    [37](#bib.bib37)].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡é€šè¿‡å¯¹å½“å‰MDRLå·¥ä½œçš„ç®€è¦è°ƒæŸ¥ï¼Œä¸ºç°æœ‰å…³äºå¤šæ™ºèƒ½ä½“å­¦ä¹ [[36](#bib.bib36), [10](#bib.bib10)]ã€åˆä½œå­¦ä¹ [[7](#bib.bib7),
    [8](#bib.bib8)]ã€ä»£ç†å»ºæ¨¡ä»£ç†[[11](#bib.bib11)]ã€å¤šæ™ºèƒ½ä½“RLä¸­çš„çŸ¥è¯†é‡ç”¨[[12](#bib.bib12)]ä»¥åŠï¼ˆå•ä¸€æ™ºèƒ½ä½“ï¼‰æ·±åº¦å¼ºåŒ–å­¦ä¹ [[23](#bib.bib23),
    [37](#bib.bib37)]çš„è°ƒæŸ¥è¿›è¡Œè¡¥å……ã€‚
- en: 'First, we provide a short review of key algorithms in RL such as Q-learning
    and REINFORCE (see SectionÂ [2.1](#S2.SS1 "2.1 Reinforcement learning â€£ 2 Single-agent
    learning â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Second, we review DRL
    highlighting the challenges in this setting and reviewing recent works (see SectionÂ [2.2](#S2.SS2
    "2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). Third, we present the multiagent setting
    and give an overview of key challenges and results (see SectionÂ [3.1](#S3.SS1
    "3.1 Multiagent Learning â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Then, we present the
    identified four categories to group recent MDRL works (see FigureÂ [1](#S1.F1 "Figure
    1 â€£ 1 Introduction â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç®€è¦å›é¡¾äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„å…³é”®ç®—æ³•ï¼Œå¦‚ Q-learning å’Œ REINFORCEï¼ˆè§ç¬¬ [2.1](#S2.SS1 "2.1 å¼ºåŒ–å­¦ä¹  â€£ 2
    å•æ™ºèƒ½ä½“å­¦ä¹  â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸è¯„ä¼°1footnote 11footnote 1 æœ¬æ–‡æ—©æœŸç‰ˆæœ¬æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€")
    èŠ‚ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å›é¡¾äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ï¼Œçªå‡ºä»‹ç»äº†è¿™ä¸€é¢†åŸŸä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶è¯„å®¡äº†è¿‘æœŸçš„ç ”ç©¶å·¥ä½œï¼ˆè§ç¬¬ [2.2](#S2.SS2 "2.2 æ·±åº¦å¼ºåŒ–å­¦ä¹ 
    â€£ 2 å•æ™ºèƒ½ä½“å­¦ä¹  â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸è¯„ä¼°1footnote 11footnote 1 æœ¬æ–‡æ—©æœŸç‰ˆæœ¬æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€")
    èŠ‚ï¼‰ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¤šæ™ºèƒ½ä½“ç¯å¢ƒï¼Œå¹¶æ¦‚è¿°äº†å…³é”®æŒ‘æˆ˜å’Œç»“æœï¼ˆè§ç¬¬ [3.1](#S3.SS1 "3.1 å¤šæ™ºèƒ½ä½“å­¦ä¹  â€£ 3 å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹  (MDRL)
    â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸è¯„ä¼°1footnote 11footnote 1 æœ¬æ–‡æ—©æœŸç‰ˆæœ¬æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€")
    èŠ‚ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”¨äºå½’ç±»è¿‘æœŸ MDRL ç ”ç©¶çš„å››ä¸ªç±»åˆ«ï¼ˆè§å›¾ [1](#S1.F1 "å›¾ 1 â€£ 1 å¼•è¨€ â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è°ƒæŸ¥ä¸è¯„ä¼°1footnote
    11footnote 1 æœ¬æ–‡æ—©æœŸç‰ˆæœ¬æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦è°ƒæŸ¥â€")ï¼‰ï¼š
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Analysis of emergent behaviors: evaluate single-agent DRL algorithms in multiagent
    scenarios (e.g., Atari games, social dilemmas, 3D competitive games).'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åº”å¯¹æ–°å…´è¡Œä¸ºçš„åˆ†æï¼šåœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­è¯„ä¼°å•æ™ºèƒ½ä½“ DRL ç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒAtari æ¸¸æˆã€ç¤¾ä¼šå›°å¢ƒã€3D ç«äº‰æ¸¸æˆï¼‰ã€‚
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Learning communication: agents learn communication protocols to solve cooperative
    tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å­¦ä¹ æ²Ÿé€šï¼šæ™ºèƒ½ä½“å­¦ä¹ æ²Ÿé€šåè®®ä»¥è§£å†³åˆä½œä»»åŠ¡ã€‚
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Learning cooperation: agents learn to cooperate using only actions and (local)
    observations.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å­¦ä¹ åˆä½œï¼šæ™ºèƒ½ä½“é€šè¿‡ä»…ä½¿ç”¨åŠ¨ä½œå’Œï¼ˆå±€éƒ¨ï¼‰è§‚å¯Ÿæ¥å­¦ä¹ åˆä½œã€‚
- en: '4.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Agents modeling agents: agents reason about others to fulfill a task (e.g.,
    best response learners).'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ™ºèƒ½ä½“å»ºæ¨¡æ™ºèƒ½ä½“ï¼šæ™ºèƒ½ä½“æ¨ç†å…¶ä»–æ™ºèƒ½ä½“ä»¥å®Œæˆä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œæœ€ä½³å“åº”å­¦ä¹ è€…ï¼‰ã€‚
- en: 'For each category we provide a description as well as outline the recent works
    (see SectionÂ [3.2](#S3.SS2 "3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") and TablesÂ [2](#S3.T2 "Table
    2 â€£ 3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")â€“[4](#S3.T4 "Table 4 â€£ 3.2
    MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€")). Then, we take a step back and
    reflect on how these new works relate to the existing literature. In that context,
    first, we present examples on how methods and algorithms originally introduced
    in RL and MAL were successfully been scaled to MDRL (see SectionÂ [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Second, we provide some
    pointers for new practitioners in the area by describing general *lessons learned*
    from the existing MDRL works (see SectionÂ [4.2](#S4.SS2 "4.2 Lessons learned â€£
    4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€"))
    and point to recent multiagent benchmarks (see SectionÂ [4.3](#S4.SS3 "4.3 Benchmarks
    for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). Third, we take a more critical view and describe practical
    challenges in MDRL, such as reproducibility, hyperparameter tunning, and computational
    demands (see SectionÂ [4.4](#S4.SS4 "4.4 Practical challenges in MDRL â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Then, we outline some
    open research questions (see SectionÂ [4.5](#S4.SS5 "4.5 Open questions â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Lastly, we present our
    conclusions from this work (see SectionÂ [5](#S5 "5 Conclusions â€£ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œæˆ‘ä»¬æä¾›äº†æè¿°ï¼Œå¹¶æ¦‚è¿°äº†æœ€è¿‘çš„ç ”ç©¶ï¼ˆè§ç¬¬[3.2](#S3.SS2 "3.2 MDRL categorization â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")èŠ‚å’Œè¡¨æ ¼[2](#S3.T2 "Table 2 â€£ 3.2 MDRL categorization â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")â€“[4](#S3.T4 "Table 4 â€£ 3.2 MDRL categorization â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å›é¡¾è¿™äº›æ–°ç ”ç©¶å¦‚ä½•ä¸ç°æœ‰æ–‡çŒ®ç›¸å…³ã€‚åœ¨è¿™ä¸€èƒŒæ™¯ä¸‹ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†æœ€åˆåœ¨RLå’ŒMALä¸­å¼•å…¥çš„æ–¹æ³•å’Œç®—æ³•æˆåŠŸæ‰©å±•åˆ°MDRLï¼ˆè§ç¬¬[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")èŠ‚ï¼‰ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æè¿°ç°æœ‰MDRLç ”ç©¶ä¸­çš„ä¸€èˆ¬*ç»éªŒæ•™è®­*ï¼ˆè§ç¬¬[4.2](#S4.SS2
    "4.2 Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")èŠ‚ï¼‰ä¸ºæ–°ä»ä¸šè€…æä¾›ä¸€äº›æŒ‡å¼•ï¼Œå¹¶æŒ‡å‡ºæœ€è¿‘çš„å¤šæ™ºèƒ½ä½“åŸºå‡†ï¼ˆè§ç¬¬[4.3](#S4.SS3 "4.3
    Benchmarks for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")èŠ‚ï¼‰ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬é‡‡å–æ›´ä¸ºæ‰¹åˆ¤çš„è§†è§’ï¼Œæè¿°MDRLä¸­çš„å®é™…æŒ‘æˆ˜ï¼Œå¦‚å¯é‡å¤æ€§ã€è¶…å‚æ•°è°ƒä¼˜å’Œè®¡ç®—éœ€æ±‚ï¼ˆè§ç¬¬[4.4](#S4.SS4
    "4.4 Practical challenges in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€")èŠ‚ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€äº›æœªè§£çš„ç ”ç©¶é—®é¢˜ï¼ˆè§ç¬¬[4.5](#S4.SS5
    "4.5 Open questions â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")èŠ‚ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ¬ç ”ç©¶çš„ç»“è®ºï¼ˆè§ç¬¬[5](#S5 "5 Conclusions â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")èŠ‚ï¼‰ã€‚'
- en: Our goal is to outline a recent and active area (i.e., MDRL), as well as to
    motivate future research to take advantage of the ample and existing literature
    in multiagent learning. We aim to enable researchers with experience in either
    DRL or MAL to gain a common understanding about recent works, and open problems
    in MDRL, and to avoid having scattered sub-communities with little interactionÂ [[2](#bib.bib2),
    [10](#bib.bib10), [11](#bib.bib11), [38](#bib.bib38)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¦‚è¿°ä¸€ä¸ªè¿‘æœŸæ´»è·ƒçš„é¢†åŸŸï¼ˆå³ MDRLï¼‰ï¼Œå¹¶æ¿€åŠ±æœªæ¥ç ”ç©¶åˆ©ç”¨å¤šæ™ºèƒ½ä½“å­¦ä¹ ä¸­çš„ä¸°å¯Œç°æœ‰æ–‡çŒ®ã€‚æˆ‘ä»¬æ—¨åœ¨ä½¿æœ‰ DRL æˆ– MAL ç»éªŒçš„ç ”ç©¶äººå‘˜å¯¹æœ€æ–°å·¥ä½œå’Œ
    MDRL ä¸­çš„å¼€æ”¾é—®é¢˜æœ‰å…±åŒçš„ç†è§£ï¼Œå¹¶é¿å…å½¢æˆç›¸äº’éš”ç»çš„å°åœˆå­Â [[2](#bib.bib2), [10](#bib.bib10), [11](#bib.bib11),
    [38](#bib.bib38)]ã€‚
- en: '![Refer to caption](img/05ff41b4091bf1206fa5a001eb009d06.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/05ff41b4091bf1206fa5a001eb009d06.png)'
- en: (a) Analysis of emergent behaviors
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (a) äº§ç”Ÿè¡Œä¸ºçš„åˆ†æ
- en: '![Refer to caption](img/92d13bf4a92b65ba059ac044451f2559.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/92d13bf4a92b65ba059ac044451f2559.png)'
- en: (b) Learning communication
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (b) å­¦ä¹ æ²Ÿé€š
- en: '![Refer to caption](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/0552b30d046b5206bb4d8f8b4fc3798f.png)'
- en: (c) Learning cooperation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (c) å­¦ä¹ åˆä½œ
- en: '![Refer to caption](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/848d22f0f88e0c4c34d32daabb3340f8.png)'
- en: (d) Agents modeling agents
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (d) æ™ºèƒ½ä½“å»ºæ¨¡æ™ºèƒ½ä½“
- en: 'Figure 1: Categories of different MDRL works. (a) Analysis of emergent behaviors:
    evaluate single-agent DRL algorithms in multiagent scenarios. (b) Learning communication:
    agents learn with actions and through messages. (c) Learning cooperation: agents
    learn to cooperate using only actions and (local) observations. (d) Agents modeling
    agents: agents reason about others to fulfill a task (e.g., cooperative or competitive).
    For a more detailed description see SectionsÂ [3.3](#S3.SS3 "3.3 Emergent behaviors
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")â€“[3.6](#S3.SS6 "3.6 Agents modeling agents â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")
    and TablesÂ [2](#S3.T2 "Table 2 â€£ 3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")â€“[4](#S3.T4 "Table 4 â€£ 3.2
    MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šä¸åŒ MDRL å·¥ä½œçš„åˆ†ç±»ã€‚(a) äº§ç”Ÿè¡Œä¸ºçš„åˆ†æï¼šåœ¨å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­è¯„ä¼°å•æ™ºèƒ½ä½“ DRL ç®—æ³•ã€‚(b) å­¦ä¹ æ²Ÿé€šï¼šæ™ºèƒ½ä½“é€šè¿‡è¡ŒåŠ¨å’Œæ¶ˆæ¯è¿›è¡Œå­¦ä¹ ã€‚(c)
    å­¦ä¹ åˆä½œï¼šæ™ºèƒ½ä½“ä»…é€šè¿‡è¡ŒåŠ¨å’Œï¼ˆå±€éƒ¨ï¼‰è§‚å¯Ÿæ¥å­¦ä¹ åˆä½œã€‚(d) æ™ºèƒ½ä½“å»ºæ¨¡æ™ºèƒ½ä½“ï¼šæ™ºèƒ½ä½“æ¨ç†å…¶ä»–æ™ºèƒ½ä½“ä»¥å®Œæˆä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œåˆä½œæˆ–ç«äº‰ï¼‰ã€‚æœ‰å…³æ›´è¯¦ç»†çš„æè¿°ï¼Œè¯·å‚è§ç« èŠ‚Â [3.3](#S3.SS3
    "3.3 äº§ç”Ÿè¡Œä¸º â€£ 3 å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹  (MDRL) â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç»¼è¿°ä¸æ‰¹åˆ¤1è„šæ³¨ 11è„šæ³¨ 1æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦ç»¼è¿°â€")â€“[3.6](#S3.SS6
    "3.6 æ™ºèƒ½ä½“å»ºæ¨¡æ™ºèƒ½ä½“ â€£ 3 å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹  (MDRL) â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç»¼è¿°ä¸æ‰¹åˆ¤1è„šæ³¨ 11è„šæ³¨ 1æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦ç»¼è¿°â€")
    ä»¥åŠè¡¨æ ¼Â [2](#S3.T2 "è¡¨ 2 â€£ 3.2 MDRL åˆ†ç±» â€£ 3 å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹  (MDRL) â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç»¼è¿°ä¸æ‰¹åˆ¤1è„šæ³¨
    11è„šæ³¨ 1æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦ç»¼è¿°â€")â€“[4](#S3.T4 "è¡¨ 4 â€£ 3.2 MDRL åˆ†ç±» â€£ 3 å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ 
    (MDRL) â€£ å¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç»¼è¿°ä¸æ‰¹åˆ¤1è„šæ³¨ 11è„šæ³¨ 1æ—©æœŸç‰ˆæœ¬çš„æ ‡é¢˜ä¸ºï¼šâ€œå¤šæ™ºèƒ½ä½“æ·±åº¦å¼ºåŒ–å­¦ä¹ æ˜¯ç­”æ¡ˆè¿˜æ˜¯é—®é¢˜ï¼Ÿç®€è¦ç»¼è¿°â€")ã€‚
- en: 2 Single-agent learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 å•æ™ºèƒ½ä½“å­¦ä¹ 
- en: This section presents the formalism of reinforcement learning and its main components
    before outlining *deep* reinforcement learning along with its particular challenges
    and recent algorithms. For a more detailed description we refer the reader to
    excellent books and surveys on the areaÂ [[39](#bib.bib39), [20](#bib.bib20), [23](#bib.bib23),
    [40](#bib.bib40), [24](#bib.bib24)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ çš„å½¢å¼åŒ–ä»¥åŠå…¶ä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œç„¶åæ¦‚è¿°äº†*æ·±åº¦*å¼ºåŒ–å­¦ä¹ åŠå…¶ç‰¹å®šæŒ‘æˆ˜å’Œæœ€æ–°ç®—æ³•ã€‚æœ‰å…³æ›´è¯¦ç»†çš„æè¿°ï¼Œæˆ‘ä»¬æ¨èè¯»è€…å‚è€ƒè¯¥é¢†åŸŸçš„ä¼˜ç§€ä¹¦ç±å’Œç»¼è¿°æ–‡ç« Â [[39](#bib.bib39),
    [20](#bib.bib20), [23](#bib.bib23), [40](#bib.bib40), [24](#bib.bib24)]ã€‚
- en: 2.1 Reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 å¼ºåŒ–å­¦ä¹ 
- en: RL formalizes the interaction of an agent with an environment using a Markov
    decision process (MDP)Â [[41](#bib.bib41)]. An MDP is defined by the tuple <math
    id="S2.SS1.p1.1.m1.5" class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle"
    display="inline"><semantics id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">âŸ¨</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1"
    xref="S2.SS1.p1.1.m1.1.1.cmml">ğ’®</mi><mo id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">ğ’œ</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.3" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3"
    xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">Î³</mi><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">âŸ©</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ’®</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">ğ’œ</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">ğ‘…</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">ğ‘‡</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">ğ›¾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>
    where <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">ğ’®</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ’®</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>
    represents a finite set of states. <math id="S2.SS1.p1.3.m3.1" class="ltx_Math"
    alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ’œ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>
    represents a finite set of actions. The transition function <math id="S2.SS1.p1.4.m4.2"
    class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">ğ’®</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">Ã—</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.2.3.cmml">ğ’œ</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.4.m4.2.3.3.2.1a" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">Ã—</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.4" xref="S2.SS1.p1.4.m4.2.3.3.2.4.cmml">ğ’®</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.1" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">â†’</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3.3.2" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.4.m4.2.3.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">0</mn><mo id="S2.SS1.p1.4.m4.2.3.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.4.m4.2b"><apply id="S2.SS1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3"><ci id="S2.SS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.1">:</ci><ci
    id="S2.SS1.p1.4.m4.2.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.2">ğ‘‡</ci><apply id="S2.SS1.p1.4.m4.2.3.3.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3"><ci id="S2.SS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.1">â†’</ci><apply
    id="S2.SS1.p1.4.m4.2.3.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2"><ci id="S2.SS1.p1.4.m4.2.3.3.2.2.cmml"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2">ğ’®</ci><ci id="S2.SS1.p1.4.m4.2.3.3.2.3.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.3">ğ’œ</ci><ci
    id="S2.SS1.p1.4.m4.2.3.3.2.4.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2.4">ğ’®</ci></apply><interval
    closure="closed" id="S2.SS1.p1.4.m4.2.3.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.3.2"><cn
    type="integer" id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.4.m4.2.2.cmml" xref="S2.SS1.p1.4.m4.2.2">1</cn></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.4.m4.2c">T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]</annotation></semantics></math>
    determines the probability of a transition from any state <math id="S2.SS1.p1.5.m5.1"
    class="ltx_Math" alttext="s\in\mathcal{S}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mrow
    id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2"
    xref="S2.SS1.p1.5.m5.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.5.m5.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.cmml">âˆˆ</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">ğ’®</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml"
    xref="S2.SS1.p1.5.m5.1.1"><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğ‘ </ci><ci
    id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ’®</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">s\in\mathcal{S}</annotation></semantics></math>
    to any state <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="s^{\prime}\in\mathcal{S}"
    display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1"
    xref="S2.SS1.p1.6.m6.1.1.cmml"><msup id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml"><mi
    id="S2.SS1.p1.6.m6.1.1.2.2" xref="S2.SS1.p1.6.m6.1.1.2.2.cmml">s</mi><mo id="S2.SS1.p1.6.m6.1.1.2.3"
    xref="S2.SS1.p1.6.m6.1.1.2.3.cmml">â€²</mo></msup><mo id="S2.SS1.p1.6.m6.1.1.1"
    xref="S2.SS1.p1.6.m6.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.6.m6.1.1.3"
    xref="S2.SS1.p1.6.m6.1.1.3.cmml">ğ’®</mi></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><apply
    id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous"
    id="S2.SS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.1.2">superscript</csymbol><ci
    id="S2.SS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2.2">ğ‘ </ci><ci id="S2.SS1.p1.6.m6.1.1.2.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.2.3">â€²</ci></apply><ci id="S2.SS1.p1.6.m6.1.1.3.cmml"
    xref="S2.SS1.p1.6.m6.1.1.3">ğ’®</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.6.m6.1c">s^{\prime}\in\mathcal{S}</annotation></semantics></math>
    given any possible action <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="a\in\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1"
    xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">a</mi><mo
    id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">ğ’œ</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.p1.7.m7.1.1"><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">ğ‘</ci><ci
    id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">ğ’œ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">a\in\mathcal{A}</annotation></semantics></math>.
    The reward function <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}"
    display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1"
    xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">R</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mrow id="S2.SS1.p1.8.m8.1.1.3.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.2"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2.cmml">ğ’®</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.8.m8.1.1.3.2.1" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">Ã—</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.8.m8.1.1.3.2.3" xref="S2.SS1.p1.8.m8.1.1.3.2.3.cmml">ğ’œ</mi><mo lspace="0.222em"
    rspace="0.222em" id="S2.SS1.p1.8.m8.1.1.3.2.1a" xref="S2.SS1.p1.8.m8.1.1.3.2.1.cmml">Ã—</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.3.2.4" xref="S2.SS1.p1.8.m8.1.1.3.2.4.cmml">ğ’®</mi></mrow><mo
    stretchy="false" id="S2.SS1.p1.8.m8.1.1.3.1" xref="S2.SS1.p1.8.m8.1.1.3.1.cmml">â†’</mo><mi
    id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml">â„</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.p1.8.m8.1.1"><ci id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1">:</ci><ci
    id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">ğ‘…</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3"><ci id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.1">â†’</ci><apply
    id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2"><ci id="S2.SS1.p1.8.m8.1.1.3.2.2.cmml"
    xref="S2.SS1.p1.8.m8.1.1.3.2.2">ğ’®</ci><ci id="S2.SS1.p1.8.m8.1.1.3.2.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.3">ğ’œ</ci><ci
    id="S2.SS1.p1.8.m8.1.1.3.2.4.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2.4">ğ’®</ci></apply><ci
    id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">R:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}</annotation></semantics></math>
    defines the immediate and possibly stochastic reward that an agent would receive
    given that the agent executes action <math id="S2.SS1.p1.9.m9.1" class="ltx_Math"
    alttext="a" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1"
    xref="S2.SS1.p1.9.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">ğ‘</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">a</annotation></semantics></math>
    while in state <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.p1.10.m10.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.10.m10.1c">s</annotation></semantics></math> and it is transitioned
    to state <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.p1.11.m11.1a"><msup id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml"><mi
    id="S2.SS1.p1.11.m11.1.1.2" xref="S2.SS1.p1.11.m11.1.1.2.cmml">s</mi><mo id="S2.SS1.p1.11.m11.1.1.3"
    xref="S2.SS1.p1.11.m11.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.11.m11.1b"><apply id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">superscript</csymbol><ci
    id="S2.SS1.p1.11.m11.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.2">ğ‘ </ci><ci id="S2.SS1.p1.11.m11.1.1.3.cmml"
    xref="S2.SS1.p1.11.m11.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p1.11.m11.1c">s^{\prime}</annotation></semantics></math>, <math id="S2.SS1.p1.12.m12.2"
    class="ltx_Math" alttext="\gamma\in[0,1]" display="inline"><semantics id="S2.SS1.p1.12.m12.2a"><mrow
    id="S2.SS1.p1.12.m12.2.3" xref="S2.SS1.p1.12.m12.2.3.cmml"><mi id="S2.SS1.p1.12.m12.2.3.2"
    xref="S2.SS1.p1.12.m12.2.3.2.cmml">Î³</mi><mo id="S2.SS1.p1.12.m12.2.3.1" xref="S2.SS1.p1.12.m12.2.3.1.cmml">âˆˆ</mo><mrow
    id="S2.SS1.p1.12.m12.2.3.3.2" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml"><mo stretchy="false"
    id="S2.SS1.p1.12.m12.2.3.3.2.1" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml">0</mn><mo id="S2.SS1.p1.12.m12.2.3.3.2.2"
    xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">,</mo><mn id="S2.SS1.p1.12.m12.2.2" xref="S2.SS1.p1.12.m12.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.p1.12.m12.2.3.3.2.3" xref="S2.SS1.p1.12.m12.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.12.m12.2b"><apply id="S2.SS1.p1.12.m12.2.3.cmml"
    xref="S2.SS1.p1.12.m12.2.3"><ci id="S2.SS1.p1.12.m12.2.3.2.cmml" xref="S2.SS1.p1.12.m12.2.3.2">ğ›¾</ci><interval
    closure="closed" id="S2.SS1.p1.12.m12.2.3.3.1.cmml" xref="S2.SS1.p1.12.m12.2.3.3.2"><cn
    type="integer" id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1">0</cn><cn
    type="integer" id="S2.SS1.p1.12.m12.2.2.cmml" xref="S2.SS1.p1.12.m12.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.12.m12.2c">\gamma\in[0,1]</annotation></semantics></math>
    represents the discount factor that balances the trade-off between immediate rewards
    and future rewards.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰[[41](#bib.bib41)]å½¢å¼åŒ–äº†ä»£ç†ä¸ç¯å¢ƒçš„äº¤äº’ã€‚MDPç”±å…ƒç»„<math id="S2.SS1.p1.1.m1.5"
    class="ltx_Math" alttext="\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle" display="inline"><semantics
    id="S2.SS1.p1.1.m1.5a"><mrow id="S2.SS1.p1.1.m1.5.6.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml"><mo
    stretchy="false" id="S2.SS1.p1.1.m1.5.6.2.1" xref="S2.SS1.p1.1.m1.5.6.1.cmml">âŸ¨</mo><mi
    class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">ğ’®</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.2" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">ğ’œ</mi><mo id="S2.SS1.p1.1.m1.5.6.2.3"
    xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml">R</mi><mo
    id="S2.SS1.p1.1.m1.5.6.2.4" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi id="S2.SS1.p1.1.m1.4.4"
    xref="S2.SS1.p1.1.m1.4.4.cmml">T</mi><mo id="S2.SS1.p1.1.m1.5.6.2.5" xref="S2.SS1.p1.1.m1.5.6.1.cmml">,</mo><mi
    id="S2.SS1.p1.1.m1.5.5" xref="S2.SS1.p1.1.m1.5.5.cmml">Î³</mi><mo stretchy="false"
    id="S2.SS1.p1.1.m1.5.6.2.6" xref="S2.SS1.p1.1.m1.5.6.1.cmml">âŸ©</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p1.1.m1.5b"><list id="S2.SS1.p1.1.m1.5.6.1.cmml"
    xref="S2.SS1.p1.1.m1.5.6.2"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ’®</ci><ci
    id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">ğ’œ</ci><ci id="S2.SS1.p1.1.m1.3.3.cmml"
    xref="S2.SS1.p1.1.m1.3.3">ğ‘…</ci><ci id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4">ğ‘‡</ci><ci
    id="S2.SS1.p1.1.m1.5.5.cmml" xref="S2.SS1.p1.1.m1.5.5">ğ›¾</ci></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.1.m1.5c">\langle\mathcal{S},\mathcal{A},R,T,\gamma\rangle</annotation></semantics></math>å®šä¹‰ï¼Œå…¶ä¸­<math
    id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics
    id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1"
    xref="S2.SS1.p1.2.m2.1.1.cmml">ğ’®</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ’®</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{S}</annotation></semantics></math>è¡¨ç¤ºæœ‰é™çŠ¶æ€é›†åˆã€‚<math
    id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.1.1"
    xref="S2.SS1.p1.3.m3.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ’œ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math>è¡¨ç¤ºæœ‰é™åŠ¨ä½œé›†åˆã€‚è½¬ç§»å‡½æ•°<math
    id="S2.SS1.p1.4.m4.2" class="ltx_Math" alttext="T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]"
    display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3"
    xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">T</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">:</mo><mrow
    id="S2.SS1.p1.4.m4.2.3.3" xref="S2.SS1.p1.4.m4.2.3.3.cmml"><mrow id="S2.SS1.p1.4.m4.2.3.3.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.2.3.3.2.2"
    xref="S2.SS1.p1.4.m4.2.3.3.2.2.cmml">ğ’®</mi><mo lspace="0.222em" rspace="0.222em"
    id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.2.1.cmml">Ã—</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p1.4.m4.2.
- en: MDPs are adequate models to obtain optimal decisions in *single* agent fully
    observable environments.Â³Â³3A Partially Observable Markov Decision Process (POMDP)Â [[42](#bib.bib42),
    [43](#bib.bib43)] explicitly models environments where the agent no longer sees
    the true system state and instead receives an *observation* (generated from the
    underlying system state). Solving an MDP will yield a policy <math id="S2.SS1.p2.1.m1.1"
    class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}" display="inline"><semantics
    id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi
    id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">Ï€</mi><mo lspace="0.278em"
    rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ğ’®</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">â†’</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">ğ’œ</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğœ‹</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">â†’</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ğ’®</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">ğ’œ</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>,
    which is a mapping from states to actions. An optimal policy <math id="S2.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msup
    id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2"
    xref="S2.SS1.p2.2.m2.1.1.2.cmml">Ï€</mi><mo id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml"
    xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml"
    xref="S2.SS1.p2.2.m2.1.1.2">ğœ‹</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math> is the one that
    maximizes the expected discounted sum of rewards. There are different techniques
    for solving MDPs assuming a complete description of all its elements. One of the
    most common techniques is the value iteration algorithmÂ [[44](#bib.bib44)], which
    requires a complete and accurate representation of states, actions, rewards, and
    transitions. However, this may be difficult to obtain in many domains. For this
    reason, RL algorithms often learn from experience interacting with the environment
    in discrete time steps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs æ˜¯é€‚ç”¨äºåœ¨*å•ä¸€*ä»£ç†å®Œå…¨å¯è§‚å¯Ÿç¯å¢ƒä¸­è·å¾—æœ€ä½³å†³ç­–çš„æ¨¡å‹ã€‚Â³Â³3éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰[[42](#bib.bib42),
    [43](#bib.bib43)] æ˜ç¡®å»ºæ¨¡äº†ä»£ç†æ— æ³•å†çœ‹åˆ°çœŸå®ç³»ç»ŸçŠ¶æ€ï¼Œè€Œæ˜¯æ¥æ”¶*è§‚æµ‹*ï¼ˆç”±åŸºç¡€ç³»ç»ŸçŠ¶æ€ç”Ÿæˆï¼‰çš„ç¯å¢ƒã€‚è§£å†³ MDP å°†äº§ç”Ÿä¸€ä¸ªç­–ç•¥ <math
    id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\pi:\mathcal{S}\rightarrow\mathcal{A}"
    display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1"
    xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">Ï€</mi><mo
    lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">:</mo><mrow
    id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ğ’®</mi><mo stretchy="false"
    id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">â†’</mo><mi class="ltx_font_mathcaligraphic"
    id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">ğ’œ</mi></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml"
    xref="S2.SS1.p2.1.m1.1.1"><ci id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">:</ci><ci
    id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğœ‹</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3"><ci id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1">â†’</ci><ci
    id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ğ’®</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml"
    xref="S2.SS1.p2.1.m1.1.1.3.3">ğ’œ</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\pi:\mathcal{S}\rightarrow\mathcal{A}</annotation></semantics></math>ï¼Œå®ƒæ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥
    <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="\pi^{*}" display="inline"><semantics
    id="S2.SS1.p2.2.m2.1a"><msup id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi
    id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">Ï€</mi><mo id="S2.SS1.p2.2.m2.1.1.3"
    xref="S2.SS1.p2.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">ğœ‹</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\pi^{*}</annotation></semantics></math>
    æ˜¯æœ€å¤§åŒ–æœŸæœ›æŠ˜æ‰£å›æŠ¥æ€»å’Œçš„ç­–ç•¥ã€‚å¯¹äº MDPs çš„è§£å†³æœ‰ä¸åŒçš„æŠ€æœ¯ï¼Œå‰ææ˜¯å¯¹å…¶æ‰€æœ‰å…ƒç´ æœ‰å®Œæ•´æè¿°ã€‚æœ€å¸¸è§çš„æŠ€æœ¯ä¹‹ä¸€æ˜¯ä»·å€¼è¿­ä»£ç®—æ³• [[44](#bib.bib44)]ï¼Œå®ƒéœ€è¦å¯¹çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œè½¬ç§»æœ‰å®Œæ•´è€Œå‡†ç¡®çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šé¢†åŸŸï¼Œè¿™å¯èƒ½å¾ˆéš¾è·å¾—ã€‚å› æ­¤ï¼ŒRL
    ç®—æ³•é€šå¸¸é€šè¿‡åœ¨ç¦»æ•£æ—¶é—´æ­¥ä¸­ä¸ç¯å¢ƒäº¤äº’çš„ç»éªŒæ¥å­¦ä¹ ã€‚
- en: Q-learning
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q-å­¦ä¹ 
- en: 'One of the most well known algorithms for RL is Q-learningÂ [[45](#bib.bib45)].
    It has been devised for stationary, single-agent, fully observable environments
    with discrete actions. A Q-learning agent keeps the estimate of its expected payoff
    starting in state <math id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="s"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">s</annotation></semantics></math>,
    taking action <math id="S2.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1"
    xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">a</annotation></semantics></math>
    as <math id="S2.SS1.SSS0.Px1.p1.3.m3.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.2a"><mrow id="S2.SS1.SSS0.Px1.p1.3.m3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.1.cmml">â€‹</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.2b"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3"><apply id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.2.2">ğ‘„</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">ğ‘ </ci><ci
    id="S2.SS1.SSS0.Px1.p1.3.m3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.2.2">ğ‘</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.2c">\hat{Q}(s,a)</annotation></semantics></math>.
    Each tabular entry <math id="S2.SS1.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="\hat{Q}(s,a)"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.2a"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.2.3"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"><mover accent="true" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.1.cmml">â€‹</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.2b"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2"><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.1">^</ci><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.2.2">ğ‘„</ci></apply><interval closure="open"
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.3.3.2"><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">ğ‘ </ci><ci
    id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2">ğ‘</ci></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.2c">\hat{Q}(s,a)</annotation></semantics></math>
    is an estimate of the corresponding optimal <math id="S2.SS1.SSS0.Px1.p1.5.m5.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">superscript</csymbol><ci
    id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.2">ğ‘„</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">Q^{*}</annotation></semantics></math>
    function that maps state-action pairs to the discounted sum of future rewards
    starting with action <math id="S2.SS1.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1"
    xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.6.m6.1b"><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">a</annotation></semantics></math>
    at state <math id="S2.SS1.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.7.m7.1a"><mi id="S2.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.7.m7.1b"><ci id="S2.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.7.m7.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.7.m7.1c">s</annotation></semantics></math> and following
    the optimal policy thereafter. Each time the agent transitions from a state <math
    id="S2.SS1.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="s" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.8.m8.1a"><mi id="S2.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml">s</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.8.m8.1b"><ci id="S2.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.8.m8.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.8.m8.1c">s</annotation></semantics></math> to a state <math
    id="S2.SS1.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="s^{\prime}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.9.m9.1a"><msup id="S2.SS1.SSS0.Px1.p1.9.m9.1.1" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3" xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml">â€²</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.9.m9.1b"><apply id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.2">ğ‘ </ci><ci id="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.9.m9.1.1.3">â€²</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.9.m9.1c">s^{\prime}</annotation></semantics></math>
    via action <math id="S2.SS1.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="a"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.10.m10.1a"><mi id="S2.SS1.SSS0.Px1.p1.10.m10.1.1"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1b"><ci id="S2.SS1.SSS0.Px1.p1.10.m10.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.10.m10.1c">a</annotation></semantics></math> receiving
    payoff <math id="S2.SS1.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.11.m11.1a"><mi id="S2.SS1.SSS0.Px1.p1.11.m11.1.1" xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml">r</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.11.m11.1b"><ci id="S2.SS1.SSS0.Px1.p1.11.m11.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.11.m11.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.11.m11.1c">r</annotation></semantics></math>, the <math
    id="S2.SS1.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.12.m12.1a"><mi id="S2.SS1.SSS0.Px1.p1.12.m12.1.1" xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.12.m12.1b"><ci id="S2.SS1.SSS0.Px1.p1.12.m12.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.12.m12.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px1.p1.12.m12.1c">Q</annotation></semantics></math> table is updated
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E1.m1.7" class="ltx_Math" alttext="\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]"
    display="block"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml"><mrow
    id="S2.E1.m1.7.7.3" xref="S2.E1.m1.7.7.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.3.2"
    xref="S2.E1.m1.7.7.3.2.cmml"><mi id="S2.E1.m1.7.7.3.2.2" xref="S2.E1.m1.7.7.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.3.2.1" xref="S2.E1.m1.7.7.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.3.1" xref="S2.E1.m1.7.7.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.7.7.3.3.2"
    xref="S2.E1.m1.7.7.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.1"
    xref="S2.E1.m1.7.7.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">s</mi><mo
    id="S2.E1.m1.7.7.3.3.2.2" xref="S2.E1.m1.7.7.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2"
    xref="S2.E1.m1.2.2.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.3.3.2.3"
    xref="S2.E1.m1.7.7.3.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.7.7.2"
    xref="S2.E1.m1.7.7.2.cmml">â†</mo><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.cmml"><mrow
    id="S2.E1.m1.7.7.1.3" xref="S2.E1.m1.7.7.1.3.cmml"><mover accent="true" id="S2.E1.m1.7.7.1.3.2"
    xref="S2.E1.m1.7.7.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.3.2.2" xref="S2.E1.m1.7.7.1.3.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.3.2.1" xref="S2.E1.m1.7.7.1.3.2.1.cmml">^</mo></mover><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.3.1" xref="S2.E1.m1.7.7.1.3.1.cmml">â€‹</mo><mrow
    id="S2.E1.m1.7.7.1.3.3.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.3.3.2.1" xref="S2.E1.m1.7.7.1.3.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3"
    xref="S2.E1.m1.3.3.cmml">s</mi><mo id="S2.E1.m1.7.7.1.3.3.2.2" xref="S2.E1.m1.7.7.1.3.3.1.cmml">,</mo><mi
    id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">a</mi><mo stretchy="false" id="S2.E1.m1.7.7.1.3.3.2.3"
    xref="S2.E1.m1.7.7.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.2"
    xref="S2.E1.m1.7.7.1.2.cmml">+</mo><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml">Î±</mi><mo lspace="0em"
    rspace="0em" id="S2.E1.m1.7.7.1.1.2" xref="S2.E1.m1.7.7.1.1.2.cmml">â€‹</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.2.cmml"><mo stretchy="false"
    id="S2.E1.m1.7.7.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.7.7.1.1.1.1.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml">r</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml">+</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml">Î³</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">â€‹</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"><munder
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.2.cmml">max</mi><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml">â€²</mo></msup></munder><mo
    lspace="0.167em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml">â¡</mo><mover
    accent="true" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml">Q</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml">^</mo></mover></mrow><mo
    lspace="0em" rspace="0em" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3a" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.3.cmml">â€‹</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.4" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msup
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.5" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo
    id="S2.E1.m1.7.7.1.1.1.1.1.2" xref="S2.E1.m1.7.7.1.1.1.1.1.2.cmml">âˆ’</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.cmml"><mover accent="true"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml">Q</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em"
    id="S2.E1.m1.7.7.1.1.1.1.1.3.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow
    id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml"><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.1" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">(</mo><mi
    id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">s</mi><mo id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.2"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">a</mi><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.2.3" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E1.m1.7.7.1.1.1.1.3" xref="S2.E1.m1.7.7.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7"><ci
    id="S2.E1.m1.7.7.2.cmml" xref="S2.E1.m1.7.7.2">â†</ci><apply id="S2.E1.m1.7.7.3.cmml"
    xref="S2.E1.m1.7.7.3"><apply id="S2.E1.m1.7.7.3.2.cmml" xref="S2.E1.m1.7.7.3.2"><ci
    id="S2.E1.m1.7.7.3.2.1.cmml" xref="S2.E1.m1.7.7.3.2.1">^</ci><ci id="S2.E1.m1.7.7.3.2.2.cmml"
    xref="S2.E1.m1.7.7.3.2.2">ğ‘„</ci></apply><interval closure="open" id="S2.E1.m1.7.7.3.3.1.cmml"
    xref="S2.E1.m1.7.7.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘ </ci><ci
    id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘</ci></interval></apply><apply id="S2.E1.m1.7.7.1.cmml"
    xref="S2.E1.m1.7.7.1"><apply id="S2.E1.m1.7.7.1.3.cmml" xref="S2.E1.m1.7.7.1.3"><apply
    id="S2.E1.m1.7.7.1.3.2.cmml" xref="S2.E1.m1.7.7.1.3.2"><ci id="S2.E1.m1.7.7.1.3.2.1.cmml"
    xref="S2.E1.m1.7.7.1.3.2.1">^</ci><ci id="S2.E1.m1.7.7.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.3.2.2">ğ‘„</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.3.3.2"><ci
    id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğ‘ </ci><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">ğ‘</ci></interval></apply><apply
    id="S2.E1.m1.7.7.1.1.cmml" xref="S2.E1.m1.7.7.1.1"><ci id="S2.E1.m1.7.7.1.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.3">ğ›¼</ci><apply id="S2.E1.m1.7.7.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1"><csymbol
    cd="latexml" id="S2.E1.m1.7.7.1.1.1.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.2">delimited-[]</csymbol><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.4">ğ‘Ÿ</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.4">ğ›¾</ci><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5"><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1">subscript</csymbol><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3">superscript</csymbol><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.2">ğ‘</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.1.3.3">â€²</ci></apply></apply><apply id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.1">^</ci><ci id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.5.2.2">ğ‘„</ci></apply></apply><interval closure="open"
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2"><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘ </ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3">â€²</ci></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2">superscript</csymbol><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.2">ğ‘</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.1.1.1.2.2.2.2.3">â€²</ci></apply></interval></apply></apply><apply
    id="S2.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3"><apply id="S2.E1.m1.7.7.1.1.1.1.1.3.2.cmml"
    xref="S2.E1.m1.7.7.1.1.1.1.1.3.2"><ci id="S2.E1.m1.7.7.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.1">^</ci><ci
    id="S2.E1.m1.7.7.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.2.2">ğ‘„</ci></apply><interval
    closure="open" id="S2.E1.m1.7.7.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.1.1.1.1.3.3.2"><ci
    id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">ğ‘ </ci><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">ğ‘</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E1.m1.7c">\hat{Q}(s,a)\leftarrow\hat{Q}(s,a)+\alpha[(r+\gamma\max_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime}))-\hat{Q}(s,a)]</annotation></semantics></math>
    |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: 'with the learning rate <math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math"
    alttext="\alpha\in[0,1]" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">Î±</mi><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">âˆˆ</mo><mrow
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn
    id="S2.SS1.SSS0.Px1.p1.13.m1.2.2" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo
    stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3" xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">ğ›¼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>.
    Q-learning is proven to converge to <math id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1"
    class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">ğ‘„</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>
    if state and action spaces are discrete and finite, the sum of the learning rates
    goes to infinity (so that each state-action pair is visited infinitely often)
    and that the sum of the squares of the learning rates is finite (which is required
    to show that the convergence is with probability one)Â [[46](#bib.bib46), [45](#bib.bib45),
    [47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)].
    The convergence of single-step on-policy RL algorithms, i.e, SARSA (<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">Î»</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1b"><apply id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1"><ci id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2">ğœ†</ci><cn type="integer" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1c">\lambda=0</annotation></semantics></math>),
    for both decaying exploration (greedy in the limit with infinite exploration)
    and persistent exploration (selecting actions probabilistically according to the
    ranks of the Q values) was demonstrated by Singh et al.Â [[52](#bib.bib52)]. Furthermore,
    Van SeijenÂ [[53](#bib.bib53)] has proven convergence for Expected SARSA (see SectionÂ [3.1](#S3.SS1
    "3.1 Multiagent Learning â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A
    Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") for convergence results
    in multiagent domains).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡<math id="S2.SS1.SSS0.Px1.p1.13.m1.2" class="ltx_Math" alttext="\alpha\in[0,1]"
    display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.13.m1.2a"><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml">Î±</mi><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.1.cmml">âˆˆ</mo><mrow id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">[</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml">0</mn><mo id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">,</mo><mn id="S2.SS1.SSS0.Px1.p1.13.m1.2.2"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2.3"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.13.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3"><ci id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.2">ğ›¼</ci><interval closure="closed" id="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.3.3.2"><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.1.1">0</cn><cn type="integer" id="S2.SS1.SSS0.Px1.p1.13.m1.2.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.13.m1.2.2">1</cn></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.13.m1.2c">\alpha\in[0,1]</annotation></semantics></math>ã€‚Q-learningå·²è¢«è¯æ˜åœ¨çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¸ºç¦»æ•£å’Œæœ‰é™æ—¶æ”¶æ•›è‡³<math
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1" class="ltx_Math" alttext="Q^{*}" display="inline"><semantics
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1a"><msup id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml">Q</mi><mo id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1b"><apply id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1.1.2">ğ‘„</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.14.1.1.m1.1c">Q^{*}</annotation></semantics></math>ï¼Œå¦‚æœå­¦ä¹ ç‡ä¹‹å’Œè¶‹å‘äºæ— ç©·å¤§ï¼ˆä»¥ä¾¿æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹è¢«æ— é™æ¬¡è®¿é—®ï¼‰ï¼Œå¹¶ä¸”å­¦ä¹ ç‡çš„å¹³æ–¹å’Œæœ‰é™ï¼ˆè¿™æ˜¯æ˜¾ç¤ºæ”¶æ•›æ¦‚ç‡ä¸ºä¸€æ‰€éœ€çš„ï¼‰
    [[46](#bib.bib46], [45](#bib.bib45], [47](#bib.bib47], [48](#bib.bib48], [49](#bib.bib49],
    [50](#bib.bib50], [51](#bib.bib51)]ã€‚å•æ­¥åœ¨ç­–ç•¥RLç®—æ³•çš„æ”¶æ•›æ€§ï¼Œå³SARSAï¼ˆ<math id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1"
    class="ltx_Math" alttext="\lambda=0" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1a"><mrow
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.2.cmml">Î»</mi><mo
    id="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1" xref="S2.SS1.SSS0.Px1.p1.15.2.2.m2.1.1.1.cmml">=</mo><mn
    id="S2.S
- en: REINFORCE (Monte Carlo policy gradient)
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: REINFORCEï¼ˆè’™ç‰¹å¡æ´›ç­–ç•¥æ¢¯åº¦ï¼‰
- en: In contrast to value-based methods, which do not try to optimize directly over
    a policy spaceÂ [[54](#bib.bib54)], policy gradient methods can learn parameterized
    policies without using intermediate value estimates.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸ç›´æ¥ä¼˜åŒ–ç­–ç•¥ç©ºé—´çš„åŸºäºä»·å€¼çš„æ–¹æ³•ç›¸æ¯”[[54](#bib.bib54)]ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯ä»¥åœ¨ä¸ä½¿ç”¨ä¸­é—´ä»·å€¼ä¼°è®¡çš„æƒ…å†µä¸‹å­¦ä¹ å‚æ•°åŒ–ç­–ç•¥ã€‚
- en: Policy parameters are learned by following the gradient of some performance
    measure with gradient descentÂ [[55](#bib.bib55)]. For example, REINFORCEÂ [[56](#bib.bib56)]
    uses estimated return by Monte Carlo (MC) methods with full episode trajectories
    to learn policy parameters <math id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml">Î¸</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.1.1.1.m1.1c">\theta</annotation></semantics></math>, with
    <math id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5" class="ltx_Math" alttext="\pi(a;s,\theta)\approx\pi(a;s)"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5a"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"><mrow id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"><mi id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml">Ï€</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.1.cmml">â€‹</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml">s</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">,</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml">Î¸</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.1.cmml">â‰ˆ</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml">Ï€</mi><mo
    lspace="0em" rspace="0em" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.1.cmml">â€‹</mo><mrow
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.1" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml">a</mi><mo
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.2" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">;</mo><mi
    id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2.3" xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5b"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6"><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.2">ğœ‹</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.2.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.1.1">ğ‘</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.2.2">ğ‘ </ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.3.3">ğœƒ</ci></list></apply><apply id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.2">ğœ‹</ci><list id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.6.3.3.2"><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.4.4">ğ‘</ci><ci id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5.cmml"
    xref="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5.5">ğ‘ </ci></list></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.2.2.2.m2.5c">\pi(a;s,\theta)\approx\pi(a;s)</annotation></semantics></math>,
    as follows
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}" display="block"><semantics
    id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.7" xref="S2.E2.m1.6.7.cmml"><msub id="S2.E2.m1.6.7.2"
    xref="S2.E2.m1.6.7.2.cmml"><mi id="S2.E2.m1.6.7.2.2" xref="S2.E2.m1.6.7.2.2.cmml">Î¸</mi><mrow
    id="S2.E2.m1.6.7.2.3" xref="S2.E2.m1.6.7.2.3.cmml"><mi id="S2.E2.m1.6.7.2.3.2"
    xref="S2.E2.m1.6.7.2.3.2.cmml">t</mi><mo id="S2.E2.m1.6.7.2.3.1" xref="S2.E2.m1.6.7.2.3.1.cmml">+</mo><mn
    id="S2.E2.m1.6.7.2.3.3" xref="S2.E2.m1.6.7.2.3.3.cmml">1</mn></mrow></msub><mo
    id="S2.E2.m1.6.7.1" xref="S2.E2.m1.6.7.1.cmml">=</mo><mrow id="S2.E2.m1.6.7.3"
    xref="S2.E2.m1.6.7.3.cmml"><msub id="S2.E2.m1.6.7.3.2" xref="S2.E2.m1.6.7.3.2.cmml"><mi
    id="S2.E2.m1.6.7.3.2.2" xref="S2.E2.m1.6.7.3.2.2.cmml">Î¸</mi><mi id="S2.E2.m1.6.7.3.2.3"
    xref="S2.E2.m1.6.7.3.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.7.3.1" xref="S2.E2.m1.6.7.3.1.cmml">+</mo><mrow
    id="S2.E2.m1.6.7.3.3" xref="S2.E2.m1.6.7.3.3.cmml"><mi id="S2.E2.m1.6.7.3.3.2"
    xref="S2.E2.m1.6.7.3.3.2.cmml">Î±</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1"
    xref="S2.E2.m1.6.7.3.3.1.cmml">â€‹</mo><msub id="S2.E2.m1.6.7.3.3.3" xref="S2.E2.m1.6.7.3.3.3.cmml"><mi
    id="S2.E2.m1.6.7.3.3.3.2" xref="S2.E2.m1.6.7.3.3.3.2.cmml">G</mi><mi id="S2.E2.m1.6.7.3.3.3.3"
    xref="S2.E2.m1.6.7.3.3.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.7.3.3.1a"
    xref="S2.E2.m1.6.7.3.3.1.cmml">â€‹</mo><mfrac id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml"><mrow
    id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mrow id="S2.E2.m1.3.3.3.5" xref="S2.E2.m1.3.3.3.5.cmml"><mo
    rspace="0.167em" id="S2.E2.m1.3.3.3.5.1" xref="S2.E2.m1.3.3.3.5.1.cmml">âˆ‡</mo><mi
    id="S2.E2.m1.3.3.3.5.2" xref="S2.E2.m1.3.3.3.5.2.cmml">Ï€</mi></mrow><mo lspace="0em"
    rspace="0em" id="S2.E2.m1.3.3.3.4" xref="S2.E2.m1.3.3.3.4.cmml">â€‹</mo><mrow id="S2.E2.m1.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.4"
    xref="S2.E2.m1.3.3.3.3.4.cmml">(</mo><msub id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mi
    id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3"
    xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.5"
    xref="S2.E2.m1.3.3.3.3.4.cmml">;</mo><msub id="S2.E2.m1.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.cmml"><mi
    id="S2.E2.m1.2.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.2.2.2.2.2.2.3"
    xref="S2.E2.m1.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.3.3.3.3.3.6"
    xref="S2.E2.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.cmml"><mi
    id="S2.E2.m1.3.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.3.2.cmml">Î¸</mi><mi id="S2.E2.m1.3.3.3.3.3.3.3"
    xref="S2.E2.m1.3.3.3.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.3.3.3.3.3.7"
    xref="S2.E2.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow><mrow id="S2.E2.m1.6.6.6" xref="S2.E2.m1.6.6.6.cmml"><mi
    id="S2.E2.m1.6.6.6.5" xref="S2.E2.m1.6.6.6.5.cmml">Ï€</mi><mo lspace="0em" rspace="0em"
    id="S2.E2.m1.6.6.6.4" xref="S2.E2.m1.6.6.6.4.cmml">â€‹</mo><mrow id="S2.E2.m1.6.6.6.3.3"
    xref="S2.E2.m1.6.6.6.3.4.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.4"
    xref="S2.E2.m1.6.6.6.3.4.cmml">(</mo><msub id="S2.E2.m1.4.4.4.1.1.1" xref="S2.E2.m1.4.4.4.1.1.1.cmml"><mi
    id="S2.E2.m1.4.4.4.1.1.1.2" xref="S2.E2.m1.4.4.4.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.4.4.4.1.1.1.3"
    xref="S2.E2.m1.4.4.4.1.1.1.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.5"
    xref="S2.E2.m1.6.6.6.3.4.cmml">;</mo><msub id="S2.E2.m1.5.5.5.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.cmml"><mi
    id="S2.E2.m1.5.5.5.2.2.2.2" xref="S2.E2.m1.5.5.5.2.2.2.2.cmml">S</mi><mi id="S2.E2.m1.5.5.5.2.2.2.3"
    xref="S2.E2.m1.5.5.5.2.2.2.3.cmml">t</mi></msub><mo id="S2.E2.m1.6.6.6.3.3.6"
    xref="S2.E2.m1.6.6.6.3.4.cmml">,</mo><msub id="S2.E2.m1.6.6.6.3.3.3" xref="S2.E2.m1.6.6.6.3.3.3.cmml"><mi
    id="S2.E2.m1.6.6.6.3.3.3.2" xref="S2.E2.m1.6.6.6.3.3.3.2.cmml">Î¸</mi><mi id="S2.E2.m1.6.6.6.3.3.3.3"
    xref="S2.E2.m1.6.6.6.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E2.m1.6.6.6.3.3.7"
    xref="S2.E2.m1.6.6.6.3.4.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.7.cmml" xref="S2.E2.m1.6.7"><apply
    id="S2.E2.m1.6.7.2.cmml" xref="S2.E2.m1.6.7.2"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.2.1.cmml"
    xref="S2.E2.m1.6.7.2">subscript</csymbol><ci id="S2.E2.m1.6.7.2.2.cmml" xref="S2.E2.m1.6.7.2.2">ğœƒ</ci><apply
    id="S2.E2.m1.6.7.2.3.cmml" xref="S2.E2.m1.6.7.2.3"><ci id="S2.E2.m1.6.7.2.3.2.cmml"
    xref="S2.E2.m1.6.7.2.3.2">ğ‘¡</ci><cn type="integer" id="S2.E2.m1.6.7.2.3.3.cmml"
    xref="S2.E2.m1.6.7.2.3.3">1</cn></apply></apply><apply id="S2.E2.m1.6.7.3.cmml"
    xref="S2.E2.m1.6.7.3"><apply id="S2.E2.m1.6.7.3.2.cmml" xref="S2.E2.m1.6.7.3.2"><csymbol
    cd="ambiguous" id="S2.E2.m1.6.7.3.2.1.cmml" xref="S2.E2.m1.6.7.3.2">subscript</csymbol><ci
    id="S2.E2.m1.6.7.3.2.2.cmml" xref="S2.E2.m1.6.7.3.2.2">ğœƒ</ci><ci id="S2.E2.m1.6.7.3.2.3.cmml"
    xref="S2.E2.m1.6.7.3.2.3">ğ‘¡</ci></apply><apply id="S2.E2.m1.6.7.3.3.cmml" xref="S2.E2.m1.6.7.3.3"><ci
    id="S2.E2.m1.6.7.3.3.2.cmml" xref="S2.E2.m1.6.7.3.3.2">ğ›¼</ci><apply id="S2.E2.m1.6.7.3.3.3.cmml"
    xref="S2.E2.m1.6.7.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.7.3.3.3.1.cmml"
    xref="S2.E2.m1.6.7.3.3.3">subscript</csymbol><ci id="S2.E2.m1.6.7.3.3.3.2.cmml"
    xref="S2.E2.m1.6.7.3.3.3.2">ğº</ci><ci id="S2.E2.m1.6.7.3.3.3.3.cmml" xref="S2.E2.m1.6.7.3.3.3.3">ğ‘¡</ci></apply><apply
    id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6"><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><apply
    id="S2.E2.m1.3.3.3.5.cmml" xref="S2.E2.m1.3.3.3.5"><ci id="S2.E2.m1.3.3.3.5.1.cmml"
    xref="S2.E2.m1.3.3.3.5.1">âˆ‡</ci><ci id="S2.E2.m1.3.3.3.5.2.cmml" xref="S2.E2.m1.3.3.3.5.2">ğœ‹</ci></apply><list
    id="S2.E2.m1.3.3.3.3.4.cmml" xref="S2.E2.m1.3.3.3.3.3"><apply id="S2.E2.m1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.cmml"
    xref="S2.E2.m1.1.1.1.1.1.1.2">ğ´</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">ğ‘¡</ci></apply><apply
    id="S2.E2.m1.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S2.E2.m1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2">subscript</csymbol><ci
    id="S2.E2.m1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.2">ğ‘†</ci><ci id="S2.E2.m1.2.2.2.2.2.2.3.cmml"
    xref="S2.E2.m1.2.2.2.2.2.2.3">ğ‘¡</ci></apply><apply id="S2.E2.m1.3.3.3.3.3.3.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.3.3.3.1.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.3.3.3.2.cmml"
    xref="S2.E2.m1.3.3.3.3.3.3.2">ğœƒ</ci><ci id="S2.E2.m1.3.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3.3">ğ‘¡</ci></apply></list></apply><apply
    id="S2.E2.m1.6.6.6.cmml" xref="S2.E2.m1.6.6.6"><ci id="S2.E2.m1.6.6.6.5.cmml"
    xref="S2.E2.m1.6.6.6.5">ğœ‹</ci><list id="S2.E2.m1.6.6.6.3.4.cmml" xref="S2.E2.m1.6.6.6.3.3"><apply
    id="S2.E2.m1.4.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1"><csymbol cd="ambiguous"
    id="S2.E2.m1.4.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.4.1.1.1">subscript</csymbol><ci
    id="S2.E2.m1.4.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.4.1.1.1.2">ğ´</ci><ci id="S2.E2.m1.4.4.4.1.1.1.3.cmml"
    xref="S2.E2.m1.4.4.4.1.1.1.3">ğ‘¡</ci></apply><apply id="S2.E2.m1.5.5.5.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.5.2.2.2.1.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2">subscript</csymbol><ci id="S2.E2.m1.5.5.5.2.2.2.2.cmml"
    xref="S2.E2.m1.5.5.5.2.2.2.2">ğ‘†</ci><ci id="S2.E2.m1.5.5.5.2.2.2.3.cmml" xref="S2.E2.m1.5.5.5.2.2.2.3">ğ‘¡</ci></apply><apply
    id="S2.E2.m1.6.6.6.3.3.3.cmml" xref="S2.E2.m1.6.6.6.3.3.3"><csymbol cd="ambiguous"
    id="S2.E2.m1.6.6.6.3.3.3.1.cmml" xref="S2.E2.m1.6.6.6.3.3.3">subscript</csymbol><ci
    id="S2.E2.m1.6.6.6.3.3.3.2.cmml" xref="S2.E2.m1.6.6.6.3.3.3.2">ğœƒ</ci><ci id="S2.E2.m1.6.6.6.3.3.3.3.cmml"
    xref="S2.E2.m1.6.6.6.3.3.3.3">ğ‘¡</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E2.m1.6c">\theta_{t+1}=\theta_{t}+\alpha G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}</annotation></semantics></math>
    |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id="S2.E2.m1.6" class="ltx_Math" alttext="\theta_{t+1}=\theta_{t}+\alpha
    G_{t}\frac{\nabla\pi(A_{t};S_{t},\theta_{t})}{\pi(A_{t};S_{t},\theta_{t})}"'
- en: where <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">ğº</ci><ci
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    represents the return, <math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha"
    display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">ğ›¼</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math>
    is the learning rate, and <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math"
    alttext="A_{t}\sim\pi" display="inline"><semantics id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">âˆ¼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">Ï€</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">ğ´</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">ğ‘¡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">ğœ‹</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>.
    A main limitation is that policy gradient methods can have high varianceÂ [[54](#bib.bib54)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <math id="S2.SS1.SSS0.Px2.p2.3.m1.1" class="ltx_Math" alttext="G_{t}" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.3.m1.1a"><msub id="S2.SS1.SSS0.Px2.p2.3.m1.1.1" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml">G</mi><mi
    id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml">t</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.3.m1.1b"><apply id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.2">ğº</ci><ci id="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.3.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.3.m1.1c">G_{t}</annotation></semantics></math>
    ä»£è¡¨å›æŠ¥ï¼Œ<math id="S2.SS1.SSS0.Px2.p2.4.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.4.m2.1a"><mi id="S2.SS1.SSS0.Px2.p2.4.m2.1.1" xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml">Î±</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.4.m2.1b"><ci id="S2.SS1.SSS0.Px2.p2.4.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.4.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p2.4.m2.1c">\alpha</annotation></semantics></math> æ˜¯å­¦ä¹ ç‡ï¼Œè€Œ
    <math id="S2.SS1.SSS0.Px2.p2.5.m3.1" class="ltx_Math" alttext="A_{t}\sim\pi" display="inline"><semantics
    id="S2.SS1.SSS0.Px2.p2.5.m3.1a"><mrow id="S2.SS1.SSS0.Px2.p2.5.m3.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"><msub
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml">A</mi><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml">t</mi></msub><mo
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml">âˆ¼</mo><mi
    id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3" xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml">Ï€</mi></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p2.5.m3.1b"><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.1">similar-to</csymbol><apply id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.1.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.2">ğ´</ci><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.2.3">ğ‘¡</ci></apply><ci id="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3.cmml"
    xref="S2.SS1.SSS0.Px2.p2.5.m3.1.1.3">ğœ‹</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p2.5.m3.1c">A_{t}\sim\pi</annotation></semantics></math>ã€‚ä¸€ä¸ªä¸»è¦çš„é™åˆ¶æ˜¯ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯èƒ½å…·æœ‰è¾ƒé«˜çš„æ–¹å·®
    [[54](#bib.bib54)]ã€‚
- en: 'The policy gradient update can be generalized to include a comparison to an
    arbitrary *baseline* of the stateÂ [[56](#bib.bib56)]. The baseline, <math id="S2.SS1.SSS0.Px2.p3.1.m1.1"
    class="ltx_Math" alttext="b(s)" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em"
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.1.cmml">â€‹</mo><mrow
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">(</mo><mi
    id="S2.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.3.2.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2"><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.2.2">ğ‘</ci><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1">ğ‘ </ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.1.m1.1c">b(s)</annotation></semantics></math>,
    can be any function, as long as it does not vary with the action; the baseline
    leaves the expected value of the update unchanged, but it can have an effect on
    its varianceÂ [[20](#bib.bib20)]. A natural choice for the baseline is a learned
    state-value function, this reduces the variance, and it is bias-free if learned
    by MC.â´â´4Action-dependant baselines had been proposedÂ [[57](#bib.bib57), [58](#bib.bib58)],
    however, a recent study by Tucker et al.Â [[59](#bib.bib59)] found that in many
    works the reason of good performance was because of bugs or errors in the code,
    rather than the proposed method itself. Moreover, when using the state-value function
    for bootstrapping (updating the value estimate for a state from the estimated
    values of subsequent states) it assigns credit (reducing the variance but introducing
    bias), i.e., criticizes the policyâ€™s action selections. Thus, in actor-critic
    methodsÂ [[54](#bib.bib54)], the actor represents the policy, i.e., action-selection
    mechanism, whereas a critic is used for the value function learning. In the case
    when the critic learns a state-action function (<math id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1b"><ci id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.2.1.1.m1.1c">Q</annotation></semantics></math> function)
    and a state value function (<math id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1" class="ltx_Math"
    alttext="V" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1a"><mi
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml">V</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1b"><ci id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1.cmml"
    xref="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS1.SSS0.Px2.p3.3.2.2.m2.1c">V</annotation></semantics></math> function),
    an *advantage function* can be computed by subtracting state values from the state-action
    valuesÂ [[20](#bib.bib20), [60](#bib.bib60)]. The advantage function indicates
    the relative quality of an action compared to other available actions computed
    from the baseline, i.e., state value function. An example of an actor-critic algorithm
    is Deterministic Policy Gradient (DPG)Â [[61](#bib.bib61)]. In DPGÂ [[61](#bib.bib61)]
    the critic follows the standard Q-learning and the actor is updated following
    the gradient of the policyâ€™s performanceÂ [[62](#bib.bib62)], DPG was later extended
    to DRL (see SectionÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning â€£ 2 Single-agent
    learning â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) and MDRL (see SectionÂ [3.5](#S3.SS5
    "3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). For multiagent learning
    settings the variance is further increased as all the agentsâ€™ rewards depend on
    the rest of the agents, and it is formally shown that as the number of agents
    increase, the probability of taking a correct gradient direction decreases exponentiallyÂ [[63](#bib.bib63)].
    Recent MDRL works addressed this high variance issue, e.g., COMAÂ [[64](#bib.bib64)]
    and MADDPGÂ [[63](#bib.bib63)] (see SectionÂ [3.5](#S3.SS5 "3.5 Learning cooperation
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient methods have a clear connection with deep reinforcement learning
    since *the policy might be represented by a neural network* whose input is a representation
    of the state, whose output are action selection probabilities or values for continuous
    controlÂ [[65](#bib.bib65)], and whose weights are the policy parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸æ·±åº¦å¼ºåŒ–å­¦ä¹ æœ‰ç€æ¸…æ™°çš„è”ç³»ï¼Œå› ä¸º*ç­–ç•¥å¯èƒ½ç”±ç¥ç»ç½‘ç»œè¡¨ç¤º*ï¼Œå…¶è¾“å…¥æ˜¯çŠ¶æ€çš„è¡¨ç¤ºï¼Œè¾“å‡ºæ˜¯åŠ¨ä½œé€‰æ‹©çš„æ¦‚ç‡æˆ–è¿ç»­æ§åˆ¶çš„å€¼[[65](#bib.bib65)]ï¼Œæƒé‡åˆ™æ˜¯ç­–ç•¥å‚æ•°ã€‚
- en: 2.2 Deep reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- en: 'While tabular RL methods such as Q-learning are successful in domains that
    do not suffer from the curse of dimensionality, there are many limitations: learning
    in large state spaces can be prohibitively slow, methods do not generalize (across
    the state space), and state representations need to be hand-specifiedÂ [[20](#bib.bib20)].
    Function approximators tried to address those limitations, using for example,
    decision treesÂ [[66](#bib.bib66)], tile codingÂ [[67](#bib.bib67)], radial basis
    functionsÂ [[68](#bib.bib68)], and locally weighted regressionÂ [[69](#bib.bib69)]
    to approximate the value function.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åƒQå­¦ä¹ è¿™æ ·çš„è¡¨æ ¼åŒ–RLæ–¹æ³•åœ¨ä¸å—ç»´åº¦ç¾éš¾å½±å“çš„é¢†åŸŸä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å­˜åœ¨è®¸å¤šé™åˆ¶ï¼šåœ¨å¤§å‹çŠ¶æ€ç©ºé—´ä¸­å­¦ä¹ å¯èƒ½éå¸¸ç¼“æ…¢ï¼Œæ–¹æ³•æ— æ³•è¿›è¡Œå¹¿æ³›çš„æ³›åŒ–ï¼ˆè·¨çŠ¶æ€ç©ºé—´ï¼‰ï¼ŒçŠ¶æ€è¡¨ç¤ºéœ€è¦æ‰‹åŠ¨æŒ‡å®š[[20](#bib.bib20)]ã€‚å‡½æ•°é€¼è¿‘å™¨å°è¯•è§£å†³è¿™äº›é™åˆ¶ï¼Œä¾‹å¦‚ä½¿ç”¨å†³ç­–æ ‘[[66](#bib.bib66)]ã€å¹³é“ºç¼–ç [[67](#bib.bib67)]ã€å¾„å‘åŸºå‡½æ•°[[68](#bib.bib68)]å’Œå±€éƒ¨åŠ æƒå›å½’[[69](#bib.bib69)]æ¥é€¼è¿‘ä»·å€¼å‡½æ•°ã€‚
- en: Similarly, these challenges can be addressed by using deep learning, i.e., neural
    networksÂ [[69](#bib.bib69), [66](#bib.bib66)] as function approximators. For example,
    <math id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">â€‹</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">Î¸</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">ğ‘„</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">ğ‘ </ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">ğ‘</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">ğœƒ</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>
    can be used to approximate the state-action values with <math id="S2.SS2.p2.2.m2.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi
    id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>
    representing the neural network weights. This has two advantages, first, deep
    learning helps to generalize across states improving the sample efficiency for
    large state-space RL problems. Second, deep learning can be used to reduce (or
    eliminate) the need for manually designing features to represent state informationÂ [[21](#bib.bib21),
    [22](#bib.bib22)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œè¿™äº›æŒ‘æˆ˜å¯ä»¥é€šè¿‡ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼Œå³ç¥ç»ç½‘ç»œ[[69](#bib.bib69), [66](#bib.bib66)]ä½œä¸ºå‡½æ•°é€¼è¿‘å™¨æ¥è§£å†³ã€‚ä¾‹å¦‚ï¼Œ<math
    id="S2.SS2.p2.1.m1.3" class="ltx_Math" alttext="Q(s,a;\theta)" display="inline"><semantics
    id="S2.SS2.p2.1.m1.3a"><mrow id="S2.SS2.p2.1.m1.3.4" xref="S2.SS2.p2.1.m1.3.4.cmml"><mi
    id="S2.SS2.p2.1.m1.3.4.2" xref="S2.SS2.p2.1.m1.3.4.2.cmml">Q</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.p2.1.m1.3.4.1" xref="S2.SS2.p2.1.m1.3.4.1.cmml">â€‹</mo><mrow
    id="S2.SS2.p2.1.m1.3.4.3.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml"><mo stretchy="false"
    id="S2.SS2.p2.1.m1.3.4.3.2.1" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">(</mo><mi id="S2.SS2.p2.1.m1.1.1"
    xref="S2.SS2.p2.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.2" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">,</mo><mi
    id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.p2.1.m1.3.4.3.2.3"
    xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">;</mo><mi id="S2.SS2.p2.1.m1.3.3" xref="S2.SS2.p2.1.m1.3.3.cmml">Î¸</mi><mo
    stretchy="false" id="S2.SS2.p2.1.m1.3.4.3.2.4" xref="S2.SS2.p2.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.1.m1.3b"><apply id="S2.SS2.p2.1.m1.3.4.cmml"
    xref="S2.SS2.p2.1.m1.3.4"><ci id="S2.SS2.p2.1.m1.3.4.2.cmml" xref="S2.SS2.p2.1.m1.3.4.2">ğ‘„</ci><vector
    id="S2.SS2.p2.1.m1.3.4.3.1.cmml" xref="S2.SS2.p2.1.m1.3.4.3.2"><ci id="S2.SS2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.p2.1.m1.1.1">ğ‘ </ci><ci id="S2.SS2.p2.1.m1.2.2.cmml" xref="S2.SS2.p2.1.m1.2.2">ğ‘</ci><ci
    id="S2.SS2.p2.1.m1.3.3.cmml" xref="S2.SS2.p2.1.m1.3.3">ğœƒ</ci></vector></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.p2.1.m1.3c">Q(s,a;\theta)</annotation></semantics></math>å¯ä»¥ç”¨æ¥è¿‘ä¼¼çŠ¶æ€-åŠ¨ä½œå€¼ï¼Œå…¶ä¸­<math
    id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics
    id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">Î¸</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.p2.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.p2.2.m2.1c">\theta</annotation></semantics></math>ä»£è¡¨ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚è¿™æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼Œé¦–å…ˆï¼Œæ·±åº¦å­¦ä¹ æœ‰åŠ©äºåœ¨çŠ¶æ€é—´æ³›åŒ–ï¼Œæé«˜å¤§çŠ¶æ€ç©ºé—´å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ ·æœ¬æ•ˆç‡ã€‚å…¶æ¬¡ï¼Œæ·±åº¦å­¦ä¹ å¯ç”¨äºå‡å°‘ï¼ˆæˆ–æ¶ˆé™¤ï¼‰æ‰‹åŠ¨è®¾è®¡ç”¨äºè¡¨ç¤ºçŠ¶æ€ä¿¡æ¯çš„ç‰¹å¾çš„éœ€æ±‚[[21](#bib.bib21),
    [22](#bib.bib22)]ã€‚
- en: However, extending deep learning to RL problems comes with additional challenges
    including non-i.i.d.Â (not independently and identically distributed) data. Many
    supervised learning methods assume that training data is from an i.i.d.Â stationary
    distributionÂ [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]. However,
    in RL, training data consists of highly correlated sequential agent-environment
    interactions, which violates the *independence* condition. Moreover, RL training
    data distribution is non-stationary as the agent actively learns while exploring
    different parts of the state space, violating the condition of sampled data being
    *identically distributed*Â [[72](#bib.bib72)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå°†æ·±åº¦å­¦ä¹ æ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¹Ÿå¸¦æ¥äº†é¢å¤–çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-i.i.d.ï¼‰çš„æ•°æ®ã€‚è®¸å¤šç›‘ç£å­¦ä¹ æ–¹æ³•å‡è®¾è®­ç»ƒæ•°æ®æ¥è‡ªç‹¬ç«‹åŒåˆ†å¸ƒçš„é™æ€åˆ†å¸ƒ
    [[70](#bib.bib70), [22](#bib.bib22), [71](#bib.bib71)]ã€‚ç„¶è€Œï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œè®­ç»ƒæ•°æ®ç”±é«˜åº¦ç›¸å…³çš„é¡ºåºä»£ç†-ç¯å¢ƒäº¤äº’ç»„æˆï¼Œè¿åäº†*ç‹¬ç«‹æ€§*æ¡ä»¶ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ•°æ®åˆ†å¸ƒæ˜¯éé™æ€çš„ï¼Œå› ä¸ºä»£ç†åœ¨æ¢ç´¢çŠ¶æ€ç©ºé—´çš„ä¸åŒéƒ¨åˆ†æ—¶ï¼Œä¸»åŠ¨å­¦ä¹ ï¼Œè¿åäº†æ•°æ®*åŒåˆ†å¸ƒ*çš„æ¡ä»¶
    [[72](#bib.bib72)]ã€‚
- en: 'In practice, using function approximators in RL requires making crucial representational
    decisions and poor design choices can result in estimates that diverge from the
    optimal value functionÂ [[73](#bib.bib73), [69](#bib.bib69), [74](#bib.bib74),
    [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]. In particular, function
    approximation, bootstrapping, and off-policy learning are considered the three
    main properties that when combined, can make the learning to diverge and are known
    as *the deadly triad*Â [[77](#bib.bib77), [20](#bib.bib20)]. Recently, some works
    have shown that non-linear (i.e., deep) function approximators poorly estimate
    the value functionÂ [[78](#bib.bib78), [59](#bib.bib59), [79](#bib.bib79)] and
    another work found problems with Q-learning using function approximation (over/under-estimation,
    instability and even divergence) due to the *delusional* bias: â€œdelusional bias
    occurs whenever a backed-up value estimate is derived from action choices that
    are not realizable in the underlying policy classâ€[[80](#bib.bib80)]. Additionally,
    convergence results for reinforcement learning using function approximation are
    still scarceÂ [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83),
    [80](#bib.bib80)]; in general, stronger convergence guarantees are available for
    policy-gradient methodsÂ [[55](#bib.bib55)] than for value-based methodsÂ [[20](#bib.bib20)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­ï¼Œä½¿ç”¨RLä¸­çš„å‡½æ•°é€¼è¿‘å™¨éœ€è¦åšå‡ºå…³é”®çš„è¡¨ç¤ºå†³ç­–ï¼Œè€Œç³Ÿç³•çš„è®¾è®¡é€‰æ‹©å¯èƒ½å¯¼è‡´ä¼°è®¡å€¼å‡½æ•°åç¦»æœ€ä¼˜å€¼å‡½æ•° [[73](#bib.bib73), [69](#bib.bib69),
    [74](#bib.bib74), [75](#bib.bib75), [76](#bib.bib76), [77](#bib.bib77)]ã€‚ç‰¹åˆ«æ˜¯ï¼Œå‡½æ•°é€¼è¿‘ã€è‡ªä¸¾å’Œç¦»ç­–ç•¥å­¦ä¹ è¢«è®¤ä¸ºæ˜¯å¯¼è‡´å­¦ä¹ å‘æ•£çš„ä¸‰ä¸ªä¸»è¦å±æ€§ï¼Œå¹¶è¢«ç§°ä¸º*è‡´å‘½ä¸‰è¿å‡»*
    [[77](#bib.bib77), [20](#bib.bib20)]ã€‚æœ€è¿‘çš„ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œéçº¿æ€§ï¼ˆå³æ·±åº¦ï¼‰å‡½æ•°é€¼è¿‘å™¨å¯¹ä»·å€¼å‡½æ•°çš„ä¼°è®¡èƒ½åŠ›è¾ƒå·® [[78](#bib.bib78),
    [59](#bib.bib59), [79](#bib.bib79)]ï¼Œå¦ä¸€é¡¹ç ”ç©¶å‘ç°Qå­¦ä¹ åœ¨ä½¿ç”¨å‡½æ•°é€¼è¿‘æ—¶å­˜åœ¨é—®é¢˜ï¼ˆè¿‡åº¦/æ¬ ä¼°è®¡ã€ä¸ç¨³å®šç”šè‡³å‘æ•£ï¼‰ï¼ŒåŸå› æ˜¯*å¦„æƒ³åå·®*ï¼šâ€œæ¯å½“ä»ä¸å¯å®ç°çš„åŠ¨ä½œé€‰æ‹©å¯¼å‡ºæ”¯æŒçš„å€¼ä¼°è®¡æ—¶ï¼Œå°±ä¼šå‡ºç°å¦„æƒ³åå·®â€[[80](#bib.bib80)]ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å‡½æ•°é€¼è¿‘è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›ç»“æœä»ç„¶å¾ˆå°‘è§
    [[74](#bib.bib74), [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [80](#bib.bib80)]ï¼›ä¸€èˆ¬æ¥è¯´ï¼Œä¸åŸºäºå€¼çš„æ–¹æ³•
    [[20](#bib.bib20)] ç›¸æ¯”ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•æä¾›æ›´å¼ºçš„æ”¶æ•›ä¿è¯ [[55](#bib.bib55)]ã€‚
- en: Below we mention how the existing DRL methods aim to address these challenges
    when briefly reviewing value-based methods, such as DQNÂ [[13](#bib.bib13)]; policy
    gradient methods, like Proximal Policy Optimization (PPO)Â [[60](#bib.bib60)];
    and actor-critic methods like Asynchronous Advantage Actor-Critic (A3C)Â [[84](#bib.bib84)].
    We refer the reader to recent surveys on single-agent DRLÂ [[23](#bib.bib23), [37](#bib.bib37),
    [24](#bib.bib24)] for a more detailed discussion of the literature.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬æåŠç°æœ‰çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨ç®€è¦å›é¡¾åŸºäºä»·å€¼çš„æ–¹æ³•æ—¶å¦‚ä½•è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚DQN [[13](#bib.bib13)]ï¼›ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå¦‚Proximal
    Policy Optimizationï¼ˆPPOï¼‰ [[60](#bib.bib60)]ï¼›ä»¥åŠæ¼”å‘˜-è¯„è®ºè€…æ–¹æ³•ï¼Œå¦‚å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºè€…ï¼ˆA3Cï¼‰ [[84](#bib.bib84)]ã€‚æˆ‘ä»¬å»ºè®®è¯»è€…å‚é˜…å…³äºå•ä¸€ä»£ç†DRLçš„æœ€æ–°è°ƒæŸ¥
    [[23](#bib.bib23), [37](#bib.bib37), [24](#bib.bib24)]ï¼Œä»¥è·å–æ›´è¯¦ç»†çš„æ–‡çŒ®è®¨è®ºã€‚
- en: Value-based methods
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åŸºäºä»·å€¼çš„æ–¹æ³•
- en: '![Refer to caption](img/c7d77c308940bcefcd4044d97716b880.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/c7d77c308940bcefcd4044d97716b880.png)'
- en: 'Figure 2: Deep Q-Network (DQN)Â [[13](#bib.bib13)]: Inputs are four stacked
    frames; the network is composed of several layers: *Convolutional* layers employ
    filters to learn features from high-dimensional data with a much smaller number
    of neurons and *Dense* layers are fully-connected layers. The last layer represents
    the actions the agent can take (in this case, <math id="S2.F2.3.1.m1.1" class="ltx_Math"
    alttext="10" display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1"
    xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content"
    id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml" xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.3.1.m1.1d">10</annotation></semantics></math>
    possible actions). Deep Recurrent Q-Network (DRQN)Â [[85](#bib.bib85)], which extends
    DQN to partially observable domainsÂ [[42](#bib.bib42)], is identical to this setup
    except the penultimate layer (<math id="S2.F2.4.2.m2.1" class="ltx_Math" alttext="1\times
    256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">Ã—</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense layer) is replaced with a recurrent LSTM layerÂ [[86](#bib.bib86)].'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šæ·±åº¦ Q ç½‘ç»œ (DQN)Â [[13](#bib.bib13)]ï¼šè¾“å…¥ä¸ºå››ä¸ªå †å çš„å¸§ï¼›ç½‘ç»œç”±å¤šä¸ªå±‚ç»„æˆï¼š*å·ç§¯* å±‚ä½¿ç”¨æ»¤æ³¢å™¨ä»é«˜ç»´æ•°æ®ä¸­å­¦ä¹ ç‰¹å¾ï¼Œå…·æœ‰è¾ƒå°‘çš„ç¥ç»å…ƒï¼Œè€Œ*å…¨è¿æ¥*
    å±‚æ˜¯å®Œå…¨è¿æ¥çš„å±‚ã€‚æœ€åä¸€å±‚è¡¨ç¤ºä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œï¼ˆåœ¨æ­¤æƒ…å†µä¸‹ä¸º<math id="S2.F2.3.1.m1.1" class="ltx_Math" alttext="10"
    display="inline"><semantics id="S2.F2.3.1.m1.1b"><mn id="S2.F2.3.1.m1.1.1" xref="S2.F2.3.1.m1.1.1.cmml">10</mn><annotation-xml
    encoding="MathML-Content" id="S2.F2.3.1.m1.1c"><cn type="integer" id="S2.F2.3.1.m1.1.1.cmml"
    xref="S2.F2.3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex"
    id="S2.F2.3.1.m1.1d">10</annotation></semantics></math> ç§å¯èƒ½åŠ¨ä½œ)ã€‚æ·±åº¦é€’å½’ Q ç½‘ç»œ (DRQN)Â [[85](#bib.bib85)]ï¼Œå®ƒå°†
    DQN æ‰©å±•åˆ°éƒ¨åˆ†å¯è§‚å¯Ÿé¢†åŸŸÂ [[42](#bib.bib42)]ï¼Œé™¤äº†å€’æ•°ç¬¬äºŒå±‚ï¼ˆ<math id="S2.F2.4.2.m2.1" class="ltx_Math"
    alttext="1\times 256" display="inline"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.1"
    xref="S2.F2.4.2.m2.1.1.cmml"><mn id="S2.F2.4.2.m2.1.1.2" xref="S2.F2.4.2.m2.1.1.2.cmml">1</mn><mo
    lspace="0.222em" rspace="0.222em" id="S2.F2.4.2.m2.1.1.1" xref="S2.F2.4.2.m2.1.1.1.cmml">Ã—</mo><mn
    id="S2.F2.4.2.m2.1.1.3" xref="S2.F2.4.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml
    encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.1.cmml"
    xref="S2.F2.4.2.m2.1.1"><cn type="integer" id="S2.F2.4.2.m2.1.1.2.cmml" xref="S2.F2.4.2.m2.1.1.2">1</cn><cn
    type="integer" id="S2.F2.4.2.m2.1.1.3.cmml" xref="S2.F2.4.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F2.4.2.m2.1d">1\times 256</annotation></semantics></math>
    Dense å±‚) è¢«æ›¿æ¢ä¸ºé€’å½’ LSTM å±‚Â [[86](#bib.bib86)]ã€‚
- en: 'The major breakthrough work combining deep learning with Q-learning was the
    Deep Q-Network (DQN)Â [[13](#bib.bib13)]. DQN uses a deep neural network for function
    approximationÂ [[87](#bib.bib87)]âµâµ5Before DQN, many approaches used neural networks
    for representing the Q-value functionÂ [[88](#bib.bib88)], such as Neural Fitted
    Q-learningÂ [[87](#bib.bib87)] and NEAT+QÂ [[75](#bib.bib75)]. (see FigureÂ [2](#S2.F2
    "Figure 2 â€£ Value-based methods â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent
    learning â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) and maintains an *experience
    replay* (ER) bufferÂ [[89](#bib.bib89), [90](#bib.bib90)] to store interactions
    <math id="S2.SS2.SSS0.Px1.p1.1.m1.4" class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle"
    display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">âŸ¨</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml">a</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.4"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.3.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml">r</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.5"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">,</mo><msup id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml">â€²</mo></msup><mo stretchy="false"
    id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.6" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml">âŸ©</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2">ğ‘</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3">ğ‘Ÿ</ci><apply id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.1.1.3">â€²</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>.
    DQN keeps an additional copy of neural network parameters, <math id="S2.SS2.SSS0.Px1.p1.2.m2.1"
    class="ltx_Math" alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><msup
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">Î¸</mi><mo id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3"
    xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">âˆ’</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2">ğœƒ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.1c">\theta^{-}</annotation></semantics></math>,
    for the target network in addition to the <math id="S2.SS2.SSS0.Px1.p1.3.m3.1"
    class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.1a"><mi
    id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">Î¸</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p1.3.m3.1c">\theta</annotation></semantics></math> parameters
    to stabilize the learning, i.e., to alleviate the non-stationary data distribution.â¶â¶6Double
    Q-learningÂ [[91](#bib.bib91)] originally proposed keeping two <math id="footnote6.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="footnote6.m1.1b"><mi
    id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="footnote6.m1.1c"><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="footnote6.m1.1d">Q</annotation></semantics></math>
    functions (estimators) to reduce the overestimation bias in RL, while still keeping
    the convergence guarantees, later it was extended to DRL in Double DQNÂ [[92](#bib.bib92)]
    (see Section [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL
    â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")).
    For each training iteration <math id="S2.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math"
    alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1"
    xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.4.m4.1c">i</annotation></semantics></math>,
    DQN minimizes the mean-squared error (MSE) between the Q-network and its target
    network using the loss function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E3.m1.9" class="ltx_Math" alttext="L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]"
    display="block"><semantics id="S2.E3.m1.9a"><mrow id="S2.E3.m1.9.9" xref="S2.E3.m1.9.9.cmml"><mrow
    id="S2.E3.m1.7.7.1" xref="S2.E3.m1.7.7.1.cmml"><msub id="S2.E3.m1.7.7.1.3" xref="S2.E3.m1.7.7.1.3.cmml"><mi
    id="S2.E3.m1.7.7.1.3.2" xref="S2.E3.m1.7.7.1.3.2.cmml">L</mi><mi id="S2.E3.m1.7.7.1.3.3"
    xref="S2.E3.m1.7.7.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.7.7.1.2"
    xref="S2.E3.m1.7.7.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.7.7.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.2" xref="S2.E3.m1.7.7.1.1.1.1.cmml">(</mo><msub
    id="S2.E3.m1.7.7.1.1.1.1" xref="S2.E3.m1.7.7.1.1.1.1.cmml"><mi id="S2.E3.m1.7.7.1.1.1.1.2"
    xref="S2.E3.m1.7.7.1.1.1.1.2.cmml">Î¸</mi><mi id="S2.E3.m1.7.7.1.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.7.7.1.1.1.3" xref="S2.E3.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow><mo
    id="S2.E3.m1.9.9.4" xref="S2.E3.m1.9.9.4.cmml">=</mo><mrow id="S2.E3.m1.9.9.3.2"
    xref="S2.E3.m1.9.9.3.3.cmml"><msub id="S2.E3.m1.8.8.2.1.1" xref="S2.E3.m1.8.8.2.1.1.cmml"><mi
    id="S2.E3.m1.8.8.2.1.1.2" xref="S2.E3.m1.8.8.2.1.1.2.cmml">ğ”¼</mi><mrow id="S2.E3.m1.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml"><mi id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml">s</mi><mo
    id="S2.E3.m1.4.4.4.4.2" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi id="S2.E3.m1.2.2.2.2"
    xref="S2.E3.m1.2.2.2.2.cmml">a</mi><mo id="S2.E3.m1.4.4.4.4.3" xref="S2.E3.m1.4.4.4.5.cmml">,</mo><mi
    id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">r</mi><mo id="S2.E3.m1.4.4.4.4.4"
    xref="S2.E3.m1.4.4.4.5.cmml">,</mo><msup id="S2.E3.m1.4.4.4.4.1" xref="S2.E3.m1.4.4.4.4.1.cmml"><mi
    id="S2.E3.m1.4.4.4.4.1.2" xref="S2.E3.m1.4.4.4.4.1.2.cmml">s</mi><mo id="S2.E3.m1.4.4.4.4.1.3"
    xref="S2.E3.m1.4.4.4.4.1.3.cmml">â€²</mo></msup></mrow></msub><mo id="S2.E3.m1.9.9.3.2a"
    xref="S2.E3.m1.9.9.3.3.cmml">â¡</mo><mrow id="S2.E3.m1.9.9.3.2.2" xref="S2.E3.m1.9.9.3.3.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.2" xref="S2.E3.m1.9.9.3.3.cmml">[</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1" xref="S2.E3.m1.9.9.3.2.2.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.2"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml">r</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.4.cmml">+</mo><mrow id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml">Î³</mi><mo lspace="0em" rspace="0em"
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">â€‹</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4a" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">â€‹</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml">a</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4b" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">â€‹</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml">x</mi><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml">â€²</mo></msup></msub><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4c" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">â€‹</mo><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4d" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.4.cmml">â€‹</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">(</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">,</mo><msup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml">â€²</mo></msup><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.6" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">;</mo><msubsup
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml">Î¸</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml">i</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.3.cmml">âˆ’</mo></msubsup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.7" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.5.cmml">âˆ’</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.2.cmml">â€‹</mo><mrow
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml"><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">(</mo><mi
    id="S2.E3.m1.5.5" xref="S2.E3.m1.5.5.cmml">s</mi><mo id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.3"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">,</mo><mi id="S2.E3.m1.6.6" xref="S2.E3.m1.6.6.cmml">a</mi><mo
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.4" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">;</mo><msub
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml"><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml">Î¸</mi><mi
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml">i</mi></msub><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.5" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.1.1.1.3" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml">)</mo></mrow><mn
    id="S2.E3.m1.9.9.3.2.2.1.3" xref="S2.E3.m1.9.9.3.2.2.1.3.cmml">2</mn></msup><mo
    stretchy="false" id="S2.E3.m1.9.9.3.2.2.3" xref="S2.E3.m1.9.9.3.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E3.m1.9b"><apply id="S2.E3.m1.9.9.cmml" xref="S2.E3.m1.9.9"><apply
    id="S2.E3.m1.7.7.1.cmml" xref="S2.E3.m1.7.7.1"><apply id="S2.E3.m1.7.7.1.3.cmml"
    xref="S2.E3.m1.7.7.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.3.1.cmml" xref="S2.E3.m1.7.7.1.3">subscript</csymbol><ci
    id="S2.E3.m1.7.7.1.3.2.cmml" xref="S2.E3.m1.7.7.1.3.2">ğ¿</ci><ci id="S2.E3.m1.7.7.1.3.3.cmml"
    xref="S2.E3.m1.7.7.1.3.3">ğ‘–</ci></apply><apply id="S2.E3.m1.7.7.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.1.1.1.1.1.cmml"
    xref="S2.E3.m1.7.7.1.1.1">subscript</csymbol><ci id="S2.E3.m1.7.7.1.1.1.1.2.cmml"
    xref="S2.E3.m1.7.7.1.1.1.1.2">ğœƒ</ci><ci id="S2.E3.m1.7.7.1.1.1.1.3.cmml" xref="S2.E3.m1.7.7.1.1.1.1.3">ğ‘–</ci></apply></apply><apply
    id="S2.E3.m1.9.9.3.3.cmml" xref="S2.E3.m1.9.9.3.2"><apply id="S2.E3.m1.8.8.2.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.8.8.2.1.1.1.cmml"
    xref="S2.E3.m1.8.8.2.1.1">subscript</csymbol><ci id="S2.E3.m1.8.8.2.1.1.2.cmml"
    xref="S2.E3.m1.8.8.2.1.1.2">ğ”¼</ci><list id="S2.E3.m1.4.4.4.5.cmml" xref="S2.E3.m1.4.4.4.4"><ci
    id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1">ğ‘ </ci><ci id="S2.E3.m1.2.2.2.2.cmml"
    xref="S2.E3.m1.2.2.2.2">ğ‘</ci><ci id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3">ğ‘Ÿ</ci><apply
    id="S2.E3.m1.4.4.4.4.1.cmml" xref="S2.E3.m1.4.4.4.4.1"><csymbol cd="ambiguous"
    id="S2.E3.m1.4.4.4.4.1.1.cmml" xref="S2.E3.m1.4.4.4.4.1">superscript</csymbol><ci
    id="S2.E3.m1.4.4.4.4.1.2.cmml" xref="S2.E3.m1.4.4.4.4.1.2">ğ‘ </ci><ci id="S2.E3.m1.4.4.4.4.1.3.cmml"
    xref="S2.E3.m1.4.4.4.4.1.3">â€²</ci></apply></list></apply><apply id="S2.E3.m1.9.9.3.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.5">ğ‘Ÿ</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3"><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.5">ğ›¾</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.6">ğ‘š</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.7">ğ‘</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.2">ğ‘¥</ci><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.2">ğ‘</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.8.3.3">â€²</ci></apply></apply><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.9">ğ‘„</ci><vector id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.4.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3"><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.2">ğ‘ </ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.1.1.1.1.1.3">â€²</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.2">ğ‘</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.2.2.2.2.2.3">â€²</ci></apply><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">superscript</csymbol><apply id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.1.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3">subscript</csymbol><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.2">ğœƒ</ci><ci id="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3.cmml"
    xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.3.3.3.3.3.2.3">ğ‘–</ci></apply></apply></vector></apply></apply><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4"><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.3">ğ‘„</ci><vector
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1"><ci
    id="S2.E3.m1.5.5.cmml" xref="S2.E3.m1.5.5">ğ‘ </ci><ci id="S2.E3.m1.6.6.cmml" xref="S2.E3.m1.6.6">ğ‘</ci><apply
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1"><csymbol
    cd="ambiguous" id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.1.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1">subscript</csymbol><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.2">ğœƒ</ci><ci
    id="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.1.1.1.4.1.1.1.3">ğ‘–</ci></apply></vector></apply></apply><cn
    type="integer" id="S2.E3.m1.9.9.3.2.2.1.3.cmml" xref="S2.E3.m1.9.9.3.2.2.1.3">2</cn></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E3.m1.9c">L_{i}(\theta_{i})=\operatorname{\mathbb{E}}_{s,a,r,s^{\prime}}[(r+\gamma
    max_{a^{\prime}}Q(s^{\prime},a^{\prime};\theta_{i}^{-})-Q(s,a;\theta_{i}))^{2}]</annotation></semantics></math>
    |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: 'where target network parameters <math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math"
    alttext="\theta^{-}" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><msup
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">Î¸</mi><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">âˆ’</mo></msup><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1">superscript</csymbol><ci
    id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">ğœƒ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">\theta^{-}</annotation></semantics></math>
    are set to Q-network parameters <math id="S2.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math"
    alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mi
    id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">Î¸</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p2.2.m2.1c">\theta</annotation></semantics></math> periodically
    and mini-batches of <math id="S2.SS2.SSS0.Px1.p2.3.m3.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.3.m3.4a"><mrow
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">âŸ¨</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml">a</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.4" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml">r</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.5" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">,</mo><msup
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.6" xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml">âŸ©</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.3.m3.4b"><list id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1"><ci id="S2.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.1.1">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.2.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.2.2">ğ‘</ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.3.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.3.3">ğ‘Ÿ</ci><apply id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1">superscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px1.p2.3.m3.4.4.1.1.3">â€²</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.3.m3.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples are sampled from the ER buffer, as depicted in FigureÂ [3](#S2.F3 "Figure
    3 â€£ Value-based methods â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The ER buffer provides stability for learning as random batches sampled from
    the buffer helps alleviating the problems caused by the non-i.i.d. data. However,
    it comes with disadvantages, such as higher memory requirements and computation
    per real interactionÂ [[93](#bib.bib93)]. The ER buffer is mainly used for off-policy
    RL methods as it can cause a mismatch between buffer content from earlier policy
    and from the current policy for on-policy methodsÂ [[93](#bib.bib93)]. Extending
    the ER buffer for the multiagent case is not trivial, see SectionsÂ [3.5](#S3.SS5
    "3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€"), [4.1](#S4.SS1 "4.1 Avoiding
    deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€") and [4.2](#S4.SS2 "4.2 Lessons learned
    â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€").
    Recent works were designed to reduce the problem of catastrophic forgetting (this
    occurs when the trained neural network performs poorly on previously learned tasks
    due to a non-stationary training distributionÂ [[94](#bib.bib94), [95](#bib.bib95)])
    and the ER buffer, in DRLÂ [[96](#bib.bib96)] and MDRLÂ [[97](#bib.bib97)].'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'ER ç¼“å†²åŒºä¸ºå­¦ä¹ æä¾›äº†ç¨³å®šæ€§ï¼Œå› ä¸ºä»ç¼“å†²åŒºéšæœºæŠ½æ ·çš„æ‰¹æ¬¡æœ‰åŠ©äºç¼“è§£ç”±éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®é€ æˆçš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå®ƒä¹Ÿæœ‰ä¸€äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚æ›´é«˜çš„å†…å­˜éœ€æ±‚å’Œæ¯æ¬¡çœŸå®äº¤äº’çš„è®¡ç®—é‡Â [[93](#bib.bib93)]ã€‚ER
    ç¼“å†²åŒºä¸»è¦ç”¨äºç¦»çº¿ç­–ç•¥ RL æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´æ—©æœŸç­–ç•¥ä¸å½“å‰ç­–ç•¥ä¹‹é—´çš„ç¼“å†²åŒºå†…å®¹ä¸åŒ¹é…ï¼Œå¯¹äºåœ¨çº¿ç­–ç•¥æ–¹æ³•Â [[93](#bib.bib93)]ã€‚å°† ER
    ç¼“å†²åŒºæ‰©å±•åˆ°å¤šæ™ºèƒ½ä½“æƒ…å†µå¹¶éæ˜“äº‹ï¼Œè¯·å‚è§ç¬¬Â [3.5](#S3.SS5 "3.5 Learning cooperation â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")ã€[4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") å’Œ [4.2](#S4.SS2 "4.2 Lessons
    learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€") éƒ¨åˆ†ã€‚è¿‘æœŸçš„ç ”ç©¶æ—¨åœ¨å‡å°‘ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼ˆå½“è®­ç»ƒç¥ç»ç½‘ç»œåœ¨ä¹‹å‰å­¦åˆ°çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œç”±äºè®­ç»ƒåˆ†å¸ƒéé™æ€Â [[94](#bib.bib94),
    [95](#bib.bib95)]ï¼‰ï¼Œä»¥åŠ ER ç¼“å†²åŒºåœ¨ DRLÂ [[96](#bib.bib96)] å’Œ MDRLÂ [[97](#bib.bib97)]
    ä¸­çš„åº”ç”¨ã€‚'
- en: 'DQN has been extended in many ways, for example, by using double estimatorsÂ [[91](#bib.bib91)]
    to reduce the overestimation bias with Double DQNÂ [[92](#bib.bib92)] (see Section
    [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) and by decomposing the
    Q-function with a *dueling*-DQN architectureÂ [[98](#bib.bib98)], where two streams
    are learned, one estimates state values and another one advantages, those are
    combined in the final layer to form <math id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1a"><mi
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1b"><ci id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S2.SS2.SSS0.Px1.p4.1.1.1.m1.1c">Q</annotation></semantics></math> values (this
    method improved over Double DQN).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: In practice, DQN is trained using an input of four stacked frames (last four
    frames the agent has encountered). If a game requires a memory of more than four
    frames it will appear non-Markovian to DQN because the future game states (and
    rewards) do not depend only on the input (four frames) but rather on the historyÂ [[99](#bib.bib99)].
    Thus, DQNâ€™s performance declines when given incomplete state observations (e.g.,
    one input frame) since DQN assumes full state observability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-world tasks often feature incomplete and noisy state information resulting
    from *partial observability* (see SectionÂ [2.1](#S2.SS1 "2.1 Reinforcement learning
    â€£ 2 Single-agent learning â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")).
    Deep Recurrent Q-Networks (DRQN)Â [[85](#bib.bib85)] proposed using *recurrent
    neural networks*, in particular, Long Short-Term Memory (LSTMs) cellsÂ [[86](#bib.bib86)]
    in DQN, for this setting. Consider the architecture in FigureÂ [2](#S2.F2 "Figure
    2 â€£ Value-based methods â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") with the first dense layer
    after convolution replaced by a layer of LSTM cells. With this addition, DRQN
    has memory capacity so that it can even work with only one input frame rather
    than a stacked input of consecutive frames. This idea has been extended to MDRL,
    see FigureÂ [6](#S3.F6 "Figure 6 â€£ 3.6 Agents modeling agents â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")
    and SectionÂ [4.2](#S4.SS2 "4.2 Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€"). There are also other approaches
    to deal with partial observability such as finite state controllersÂ [[100](#bib.bib100)]
    (where action selection is performed according to the complete observation history)
    and using an initiation set of options conditioned on the previously employed
    optionÂ [[101](#bib.bib101)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1c2a9ce10012396bd49fcdd20cc483d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Representation of a DQN agent that uses an experience replay bufferÂ [[89](#bib.bib89),
    [90](#bib.bib90)] to keep <math id="S2.F3.2.1.m1.4" class="ltx_Math" alttext="\langle
    s,a,r,s^{\prime}\rangle" display="inline"><semantics id="S2.F3.2.1.m1.4b"><mrow
    id="S2.F3.2.1.m1.4.4.1" xref="S2.F3.2.1.m1.4.4.2.cmml"><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.2"
    xref="S2.F3.2.1.m1.4.4.2.cmml">âŸ¨</mo><mi id="S2.F3.2.1.m1.1.1" xref="S2.F3.2.1.m1.1.1.cmml">s</mi><mo
    id="S2.F3.2.1.m1.4.4.1.3" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi id="S2.F3.2.1.m1.2.2"
    xref="S2.F3.2.1.m1.2.2.cmml">a</mi><mo id="S2.F3.2.1.m1.4.4.1.4" xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><mi
    id="S2.F3.2.1.m1.3.3" xref="S2.F3.2.1.m1.3.3.cmml">r</mi><mo id="S2.F3.2.1.m1.4.4.1.5"
    xref="S2.F3.2.1.m1.4.4.2.cmml">,</mo><msup id="S2.F3.2.1.m1.4.4.1.1" xref="S2.F3.2.1.m1.4.4.1.1.cmml"><mi
    id="S2.F3.2.1.m1.4.4.1.1.2" xref="S2.F3.2.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S2.F3.2.1.m1.4.4.1.1.3"
    xref="S2.F3.2.1.m1.4.4.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S2.F3.2.1.m1.4.4.1.6"
    xref="S2.F3.2.1.m1.4.4.2.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.F3.2.1.m1.4c"><list id="S2.F3.2.1.m1.4.4.2.cmml" xref="S2.F3.2.1.m1.4.4.1"><ci
    id="S2.F3.2.1.m1.1.1.cmml" xref="S2.F3.2.1.m1.1.1">ğ‘ </ci><ci id="S2.F3.2.1.m1.2.2.cmml"
    xref="S2.F3.2.1.m1.2.2">ğ‘</ci><ci id="S2.F3.2.1.m1.3.3.cmml" xref="S2.F3.2.1.m1.3.3">ğ‘Ÿ</ci><apply
    id="S2.F3.2.1.m1.4.4.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S2.F3.2.1.m1.4.4.1.1.1.cmml" xref="S2.F3.2.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S2.F3.2.1.m1.4.4.1.1.2.cmml" xref="S2.F3.2.1.m1.4.4.1.1.2">ğ‘ </ci><ci id="S2.F3.2.1.m1.4.4.1.1.3.cmml"
    xref="S2.F3.2.1.m1.4.4.1.1.3">â€²</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.F3.2.1.m1.4d">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>
    tuples for minibatch updates. The Q-values are parameterized with a NN and a policy
    is obtained by selecting (greedily) over those at every timestep.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient methods
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For many tasks, particularly for physical control, the action space is continuous
    and high dimensional where DQN is not suitable. Deep Deterministic Policy Gradient
    (DDPG)Â [[65](#bib.bib65)] is a model-free off-policy actor-critic algorithm for
    such domains, based on the DPG algorithmÂ [[61](#bib.bib61)] (see SectionÂ [2.1](#S2.SS1
    "2.1 Reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). Additionally, it proposes a new method for
    updating the networks, i.e., the target network parameters slowly change (this
    could also be applicable to DQN), in contrast to the hard reset (direct weight
    copy) used in DQN. Given the off-policy nature, DDPG generates exploratory behavior
    by adding sampled noise from some noise processes to its actor policy. The authors
    also used batch normalizationÂ [[102](#bib.bib102)] to ensure generalization across
    many different tasks without performing manual normalizations. However, note that
    other works have shown batch normalization can cause divergence in DRLÂ [[103](#bib.bib103),
    [104](#bib.bib104)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous Advantage Actor-Critic (A3C)Â [[93](#bib.bib93)] is an algorithm
    that employs a *parallelized* asynchronous training scheme (using multiple CPU
    threads) for efficiency. It is an on-policy RL method that does not use an experience
    replay buffer. A3C allows multiple workers to simultaneously interact with the
    environment and compute gradients locally. All the workers pass their computed
    local gradients to a global NN which performs the optimization and synchronizes
    with the workers asynchronously (see FigureÂ [4](#S2.F4 "Figure 4 â€£ Policy gradient
    methods â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€")). There is also the Advantage Actor-Critic
    (A2C) methodÂ [[105](#bib.bib105)] that combines all the gradients from all the
    workers to update the global NN *synchronously*. The loss function for A3C is
    composed of two terms: policy loss (actor), <math id="S2.SS2.SSS0.Px2.p2.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\pi}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.1.m1.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">â„’</mi><mi
    id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">Ï€</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3">ğœ‹</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.m1.1c">\mathcal{L}_{\pi}</annotation></semantics></math>,
    and value loss (critic), <math id="S2.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{v}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml">â„’</mi><mi
    id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.2.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.2.m2.1c">\mathcal{L}_{v}</annotation></semantics></math>.
    A3C parameters are updated using the *advantage* function <math id="S2.SS2.SSS0.Px2.p2.3.m3.6"
    class="ltx_Math" alttext="A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)" display="inline"><semantics
    id="S2.SS2.SSS0.Px2.p2.3.m3.6a"><mrow id="S2.SS2.SSS0.Px2.p2.3.m3.6.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.4.cmml">â€‹</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">(</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml">s</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml">a</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml">t</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.6" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">;</mo><msub
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml">Î¸</mi><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml">v</mi></msub><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.7" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.4.cmml">=</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml"><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml">Q</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.1.cmml">â€‹</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml">s</mi><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml">a</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml">)</mo></mrow></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.1.cmml">âˆ’</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml">V</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.1.cmml">â€‹</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.1" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml">s</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.3.2.2" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.3.m3.6b"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3"><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.5">ğ´</ci><vector id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.4.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3"><apply id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.4.4.1.1.1.1.3">ğ‘¡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.2">ğ‘</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.5.5.2.2.2.2.3">ğ‘¡</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.2">ğœƒ</ci><ci id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.3.3.3.3.3">ğ‘£</ci></apply></vector></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5"><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.2">ğ‘„</ci><interval
    closure="open" id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.2.3.2"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.1.1">ğ‘ </ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.2.2">ğ‘</ci></interval></apply><apply
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3"><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.6.6.5.3.2">ğ‘‰</ci><ci
    id="S2.SS2.SSS0.Px2.p2.3.m3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p2.3.m3.3.3">ğ‘ </ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.3.m3.6c">A(s_{t},a_{t};\theta_{v})=Q(s,a)-V(s)</annotation></semantics></math>,
    commonly used to reduce variance (see SectionÂ [2.1](#S2.SS1 "2.1 Reinforcement
    learning â€£ 2 Single-agent learning â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). An entropy loss for the policy, <math id="S2.SS2.SSS0.Px2.p2.4.m4.1"
    class="ltx_Math" alttext="H(\pi)" display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.4.m4.1a"><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mi id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.1.cmml">â€‹</mo><mrow
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml">Ï€</mi><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.3.2.2" xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2"><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.2.2">ğ»</ci><ci id="S2.SS2.SSS0.Px2.p2.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.4.m4.1.1">ğœ‹</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.4.m4.1c">H(\pi)</annotation></semantics></math>,
    is also commonly added, which helps to improve exploration by discouraging premature
    convergence to suboptimal deterministic policiesÂ [[93](#bib.bib93)]. Thus, the
    loss function is given by: <math id="S2.SS2.SSS0.Px2.p2.5.m5.3" class="ltx_math_unparsed"
    alttext="\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.5.m5.3a"><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3b"><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.2">â„’</mi><mtext
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.4.3">A3C</mtext></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.5">=</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.2">Î»</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.6.3">v</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.7"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.2">â„’</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.7.3">v</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.8">+</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.2">Î»</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.9.3">Ï€</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.10"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.2">â„’</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.10.3">Ï€</mi></msub><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.11">âˆ’</mo><msub
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12"><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.2">Î»</mi><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.12.3">H</mi></msub><msub id="S2.SS2.SSS0.Px2.p2.5.m5.3.13"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.2">ğ”¼</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3"><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.2">s</mi><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.1">âˆ¼</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.13.3.3">Ï€</mi></mrow></msub><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.1">[</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.2">H</mi><mrow
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3"><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.1">(</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.2">Ï€</mi><mrow id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3"><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.1">(</mo><mi id="S2.SS2.SSS0.Px2.p2.5.m5.1.1">s</mi><mo
    rspace="0em" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.2">,</mo><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p2.5.m5.2.2">â‹…</mo><mo id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.3">,</mo><mi
    id="S2.SS2.SSS0.Px2.p2.5.m5.3.3">Î¸</mi><mo stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.3.4">)</mo></mrow><mo
    stretchy="false" id="S2.SS2.SSS0.Px2.p2.5.m5.3.14.3.4">]</mo></mrow></mrow></mrow><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.5.m5.3c">\mathcal{L}_{\text{A3C}}=\lambda_{v}\mathcal{L}_{v}+\lambda_{\pi}\mathcal{L}_{\pi}-\lambda_{H}\mathbb{E}_{s\sim\pi}[H(\pi(s,\cdot,\theta)]</annotation></semantics></math>
    with <math id="S2.SS2.SSS0.Px2.p2.6.m6.1" class="ltx_Math" alttext="\lambda_{v},\lambda_{\pi},"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.6.m6.1a"><mrow id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1"><mrow
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml">Î»</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml">v</mi></msub><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml">,</mo><msub
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml">Î»</mi><mi
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3" xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml">Ï€</mi></msub></mrow><mo
    id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.6.m6.1b"><list id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2"><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.2">ğœ†</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.1.1.3">ğ‘£</ci></apply><apply id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.2">ğœ†</ci><ci id="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.6.m6.1.1.1.1.2.2.3">ğœ‹</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.6.m6.1c">\lambda_{v},\lambda_{\pi},</annotation></semantics></math>
    and <math id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1" class="ltx_Math" alttext="\lambda_{H}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml">Î»</mi><mi id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml">H</mi></msub><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.2">ğœ†</ci><ci id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1.1.3">ğ»</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.7.1.1.m1.1c">\lambda_{H}</annotation></semantics></math>,
    being weighting terms on the individual loss components. Wang et al.Â [[106](#bib.bib106)]
    took A3Câ€™s framework but used off-policy learning to create the Actor-critic with
    experience replay (ACER) algorithm. Gu et al.Â [[107](#bib.bib107)] introduced
    the Interpolated Policy Gradient (IPG) algorithm and showed a connection between
    ACER and DDPG: they are a pair of reparametrization terms (they are special cases
    of IPG) when they are put under the same stochastic policy setting, and when the
    policy is deterministic they collapse into DDPG.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07d5c0b6b6c365bbbd09b85f59844ef8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Asynchronous Advantage Actor-Critic (A3C) employs multiple (CPUs)
    workers without needing an ER buffer. Each worker has its own NN and independently
    interacts with the environment to compute the loss and gradients. Workers then
    pass computed gradients to the global NN that optimizes the parameters and synchronizes
    with the worker *asynchronously*. This distributed system is designed for single-agent
    deep RL. Compared to different DQN variants, A3C obtains better performance on
    a variety of Atari games using substantially less training time with multiple
    CPU cores of standard laptops without a GPUÂ [[93](#bib.bib93)]. However, we note
    that more recent approaches use both multiple CPU cores for more efficient training
    data generation and GPUs for more efficient learning.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'Jaderberg et al.Â [[84](#bib.bib84)] built the Unsupervised Reinforcement and
    Auxiliary Learning (UNREAL) framework on top of A3C and introduced unsupervised
    *auxiliary tasks* (e.g., reward prediction) to speed up the learning process.
    Auxiliary tasks in general are not used for anything other than shaping the features
    of the agent, i.e., facilitating and regularizing the representation learning
    processÂ [[108](#bib.bib108), [109](#bib.bib109)]; their formalization in RL is
    related to the concept of *general value functions*Â [[20](#bib.bib20), [110](#bib.bib110)].
    The UNREAL framework optimizes a combined loss function <math id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1"
    class="ltx_Math" alttext="\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml">â„’</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml">UNREAL</mtext></msub><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.1.cmml">â‰ˆ</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml">â„’</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml">A3C</mtext></msub><mo rspace="0.055em"
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.1.cmml">+</mo><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml"><mo
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.2.cmml">âˆ‘</mo><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml">i</mi></msub><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml"><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml">Î»</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.1.cmml">â€‹</mo><msub id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml">T</mi><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml">i</mi></msub></mrow></msub><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.1.cmml">â€‹</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml">â„’</mi><mrow id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml">A</mi><mo lspace="0em"
    rspace="0em" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.1.cmml">â€‹</mo><msub
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml">i</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1"><apply id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.2.3">UNREAL</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.2">â„’</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3"><mtext
    mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.2.3">A3C</mtext></ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.1.3">ğ‘–</ci></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2"><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.2">ğœ†</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.2">ğ´</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.2">ğ‘‡</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.2.3.3.3">ğ‘–</ci></apply></apply></apply><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.2">â„’</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3"><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.2">ğ´</ci><apply
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3"><csymbol
    cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3">subscript</csymbol><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.2">ğ‘‡</ci><ci
    id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1.1.3.3.2.3.3.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.1.1.1.1.m1.1c">\mathcal{L}_{\text{UNREAL}}\approx\mathcal{L}_{\text{A3C}}+\sum_{i}\lambda_{AT_{i}}\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    that combines the A3C loss, <math id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1" class="ltx_Math"
    alttext="\mathcal{L}_{\text{A3C}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml">â„’</mi><mtext
    id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml">A3C</mtext></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3a.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1.1.3">A3C</mtext></ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.2.2.2.2.2.m2.1c">\mathcal{L}_{\text{A3C}}</annotation></semantics></math>,
    together with auxiliary task losses <math id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1"
    class="ltx_Math" alttext="\mathcal{L}_{AT_{i}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1a"><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml">â„’</mi><mrow
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml">A</mi><mo
    lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.1.cmml">â€‹</mo><msub
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.2">â„’</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.2">ğ´</ci><apply id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.2">ğ‘‡</ci><ci id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.3.3.3.3.3.m3.1c">\mathcal{L}_{AT_{i}}</annotation></semantics></math>,
    where <math id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1" class="ltx_Math" alttext="\lambda_{AT_{i}}"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1a"><msub id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml">Î»</mi><mrow id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em"
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.1.cmml">â€‹</mo><msub
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml">T</mi><mi
    id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3" xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml">i</mi></msub></mrow></msub><annotation-xml
    encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.2">ğœ†</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3"><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.2">ğ´</ci><apply id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.1.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.2">ğ‘‡</ci><ci id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3.cmml"
    xref="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.4.4.4.4.4.m4.1c">\lambda_{AT_{i}}</annotation></semantics></math>
    are weight terms (see SectionÂ [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€") for use of auxiliary tasks in MDRL). In contrast to A3C, UNREAL
    uses a prioritized ER buffer, in which transitions with positive reward are given
    higher probability of being sampled. This approach can be viewed as a simple form
    of prioritized replayÂ [[111](#bib.bib111)], which was in turn inspired by model-based
    RL algorithms like prioritized sweepingÂ [[112](#bib.bib112), [113](#bib.bib113)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Another distributed architecture is the Importance Weighted Actor-Learner Architecture
    (IMPALA)Â [[114](#bib.bib114)]. Unlike A3C or UNREAL, IMPALA actors communicate
    *trajectories of experience* (sequences of states, actions, and rewards) to a
    centralized learner, thus IMPALA decouples acting from learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Trust Region Policy Optimization (TRPO)Â [[60](#bib.bib60)] and Proximal Policy
    Optimization (PPO)Â [[115](#bib.bib115)] are recently proposed policy gradient
    algorithms where the latter represents the state-of-the art with advantages such
    as being simpler to implement and having better empirical sample complexity. Interestingly,
    a recent workÂ [[79](#bib.bib79)] studying PPO and TRPO arrived at the surprising
    conclusion that these methods often deviate from what the theoretical framework
    would predict: gradient estimates are poorly correlated with the true gradient
    and value networks tend to produce inaccurate predictions for the true value function.
    Compared to vanilla policy gradient algorithms, PPO prevents abrupt changes in
    policies during training through the loss function, similar to early work by KakadeÂ [[116](#bib.bib116)].
    Another advantage of PPO is that it can be used in a distributed fashion, i.e,
    Distributed PPO (DPPO)Â [[117](#bib.bib117)]. Note that *distributed approaches*
    like DPPO or A3C use parallelization only to improve the learning by more efficient
    training data generation through multiple CPU cores for single agent DRL and they
    should not be considered multiagent approaches (except for recent work which tries
    to exploit this parallelization in a multiagent environmentÂ [[118](#bib.bib118)]).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, thereâ€™s a connection between policy gradient algorithms and Q-learningÂ [[119](#bib.bib119)]
    within the framework of entropy-regularized reinforcement learningÂ [[120](#bib.bib120)]
    where the value and <math id="S2.SS2.SSS0.Px2.p6.1.m1.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S2.SS2.SSS0.Px2.p6.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p6.1.m1.1.1"
    xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S2.SS2.SSS0.Px2.p6.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p6.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p6.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p6.1.m1.1c">Q</annotation></semantics></math>
    functions are slightly altered to consider the entropy of the policy. In this
    vein, Soft Actor-Critic (SAC)Â [[121](#bib.bib121)] is a recent algorithm that
    concurrently learns a stochastic policy, two Q-functions (taking inspiration from
    Double Q-learning) and a value function. SAC alternates between collecting experience
    with the current policy and updating from batches sampled from the ER buffer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: We have reviewed recent algorithms in DRL, while the list is not exhaustive,
    it provides an overview of the different state-of-art techniques and algorithms
    which will become useful while describing the MDRL techniques in the next section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 3 Multiagent Deep Reinforcement Learning (MDRL)
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we briefly introduce the general framework on multiagent learning and
    then we dive into the categories and the research on MDRL.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Multiagent Learning
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning in a multiagent environment is inherently more complex than in the
    single-agent case, as agents interact at the same time with environment and potentially
    with each otherÂ [[5](#bib.bib5)]. The *independent* learners, a.k.a. *decentralized*
    learners approachÂ [[122](#bib.bib122)] directly uses single-agent algorithms in
    the multi-agent setting despite the underlying assumptions of these algorithms
    being violated (each agent independently learns its own policy, treating other
    agents as part of the environment). In particular the *Markov property* (the future
    dynamics, transitions, and rewards depend only on the current state) becomes invalid
    since the environment is no longer stationaryÂ [[4](#bib.bib4), [6](#bib.bib6),
    [123](#bib.bib123)]. This approach ignores the multiagent nature of the setting
    entirely and it can fail when an opponent adapts or learns, for example, based
    on the past history of interactionsÂ [[2](#bib.bib2)]. Despite the lack of guarantees,
    independent learners have been used in practice, providing advantages with regards
    to scalability while often achieving good resultsÂ [[8](#bib.bib8)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: To understand why multiagent domains are non-stationary from agentsâ€™ local perspectives,
    consider a simple stochastic (also known as Markov) game <math id="S3.SS1.p2.1.m1.5"
    class="ltx_Math" alttext="(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})"
    display="inline"><semantics id="S3.SS1.p2.1.m1.5a"><mrow id="S3.SS1.p2.1.m1.5.6.2"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.1"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.1.1"
    xref="S3.SS1.p2.1.m1.1.1.cmml">ğ’®</mi><mo id="S3.SS1.p2.1.m1.5.6.2.2" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">ğ’©</mi><mo
    id="S3.SS1.p2.1.m1.5.6.2.3" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.1.m1.3.3" xref="S3.SS1.p2.1.m1.3.3.cmml">ğ’œ</mi><mo id="S3.SS1.p2.1.m1.5.6.2.4"
    xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.4.4"
    xref="S3.SS1.p2.1.m1.4.4.cmml">ğ’¯</mi><mo id="S3.SS1.p2.1.m1.5.6.2.5" xref="S3.SS1.p2.1.m1.5.6.1.cmml">,</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.1.m1.5.5" xref="S3.SS1.p2.1.m1.5.5.cmml">â„›</mi><mo
    stretchy="false" id="S3.SS1.p2.1.m1.5.6.2.6" xref="S3.SS1.p2.1.m1.5.6.1.cmml">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.1.m1.5b"><vector id="S3.SS1.p2.1.m1.5.6.1.cmml"
    xref="S3.SS1.p2.1.m1.5.6.2"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ’®</ci><ci
    id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">ğ’©</ci><ci id="S3.SS1.p2.1.m1.3.3.cmml"
    xref="S3.SS1.p2.1.m1.3.3">ğ’œ</ci><ci id="S3.SS1.p2.1.m1.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4">ğ’¯</ci><ci
    id="S3.SS1.p2.1.m1.5.5.cmml" xref="S3.SS1.p2.1.m1.5.5">â„›</ci></vector></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.1.m1.5c">(\mathcal{S},\mathcal{N},\mathcal{A},\mathcal{T},\mathcal{R})</annotation></semantics></math>,
    which can be seen as an extension of an MDP to multiple agentsÂ [[124](#bib.bib124),
    [125](#bib.bib125)]. One key distinction is that the transition, <math id="S3.SS1.p2.2.m2.1"
    class="ltx_Math" alttext="\mathcal{T}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">ğ’¯</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml"
    xref="S3.SS1.p2.2.m2.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.2.m2.1c">\mathcal{T}</annotation></semantics></math>, and reward
    function, <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{R}" display="inline"><semantics
    id="S3.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1"
    xref="S3.SS1.p2.3.m3.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">â„›</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathcal{R}</annotation></semantics></math>,
    depend on the actions <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1"
    xref="S3.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1.2"
    xref="S3.SS1.p2.4.m4.1.1.2.cmml">ğ’œ</mi><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><msub id="S3.SS1.p2.4.m4.1.1.3.2"
    xref="S3.SS1.p2.4.m4.1.1.3.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.2.2" xref="S3.SS1.p2.4.m4.1.1.3.2.2.cmml">A</mi><mn
    id="S3.SS1.p2.4.m4.1.1.3.2.3" xref="S3.SS1.p2.4.m4.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">Ã—</mo><mi
    mathvariant="normal" id="S3.SS1.p2.4.m4.1.1.3.3" xref="S3.SS1.p2.4.m4.1.1.3.3.cmml">â€¦</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.4.m4.1.1.3.1a" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">Ã—</mo><msub
    id="S3.SS1.p2.4.m4.1.1.3.4" xref="S3.SS1.p2.4.m4.1.1.3.4.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3.4.2"
    xref="S3.SS1.p2.4.m4.1.1.3.4.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.4.m4.1.1.3.4.3" xref="S3.SS1.p2.4.m4.1.1.3.4.3.cmml">ğ’©</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1"><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ğ’œ</ci><apply
    id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><apply id="S3.SS1.p2.4.m4.1.1.3.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.2">ğ´</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.4.m4.1.1.3.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.3">â€¦</ci><apply id="S3.SS1.p2.4.m4.1.1.3.4.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.3.4.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.4.m4.1.1.3.4.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.4.2">ğ´</ci><ci id="S3.SS1.p2.4.m4.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.4.m4.1.1.3.4.3">ğ’©</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{A}=A_{1}\times...\times
    A_{\mathcal{N}}</annotation></semantics></math> of all, <math id="S3.SS1.p2.5.m5.1"
    class="ltx_Math" alttext="\mathcal{N}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">ğ’©</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml"
    xref="S3.SS1.p2.5.m5.1.1">ğ’©</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS1.p2.5.m5.1c">\mathcal{N}</annotation></semantics></math>, agents, this
    means, <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1"
    xref="S3.SS1.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.6.m6.1.1.2"
    xref="S3.SS1.p2.6.m6.1.1.2.cmml">â„›</mi><mo id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml"><msub id="S3.SS1.p2.6.m6.1.1.3.2"
    xref="S3.SS1.p2.6.m6.1.1.3.2.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.2.2" xref="S3.SS1.p2.6.m6.1.1.3.2.2.cmml">R</mi><mn
    id="S3.SS1.p2.6.m6.1.1.3.2.3" xref="S3.SS1.p2.6.m6.1.1.3.2.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">Ã—</mo><mi
    mathvariant="normal" id="S3.SS1.p2.6.m6.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.3.3.cmml">â€¦</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.6.m6.1.1.3.1a" xref="S3.SS1.p2.6.m6.1.1.3.1.cmml">Ã—</mo><msub
    id="S3.SS1.p2.6.m6.1.1.3.4" xref="S3.SS1.p2.6.m6.1.1.3.4.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3.4.2"
    xref="S3.SS1.p2.6.m6.1.1.3.4.2.cmml">R</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.6.m6.1.1.3.4.3" xref="S3.SS1.p2.6.m6.1.1.3.4.3.cmml">ğ’©</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1"><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">â„›</ci><apply
    id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3"><apply id="S3.SS1.p2.6.m6.1.1.3.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.2.1.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.3.2.2.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.2">ğ‘…</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.3.2.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p2.6.m6.1.1.3.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.3">â€¦</ci><apply id="S3.SS1.p2.6.m6.1.1.3.4.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4"><csymbol
    cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.3.4.1.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4">subscript</csymbol><ci
    id="S3.SS1.p2.6.m6.1.1.3.4.2.cmml" xref="S3.SS1.p2.6.m6.1.1.3.4.2">ğ‘…</ci><ci id="S3.SS1.p2.6.m6.1.1.3.4.3.cmml"
    xref="S3.SS1.p2.6.m6.1.1.3.4.3">ğ’©</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathcal{R}=R_{1}\times...\times
    R_{\mathcal{N}}</annotation></semantics></math> and <math id="S3.SS1.p2.7.m7.1"
    class="ltx_Math" alttext="\mathcal{T}=\mathcal{S}\times A_{1}\times...\times A_{\mathcal{N}}"
    display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1"
    xref="S3.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.1.2"
    xref="S3.SS1.p2.7.m7.1.1.2.cmml">ğ’¯</mi><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">=</mo><mrow
    id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">ğ’®</mi><mo lspace="0.222em"
    rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">Ã—</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.3.2"
    xref="S3.SS1.p2.7.m7.1.1.3.3.2.cmml">A</mi><mn id="S3.SS1.p2.7.m7.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.3.cmml">1</mn></msub><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1a" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">Ã—</mo><mi
    mathvariant="normal" id="S3.SS1.p2.7.m7.1.1.3.4" xref="S3.SS1.p2.7.m7.1.1.3.4.cmml">â€¦</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.1b" xref="S3.SS1.p2.7.m7.1.1.3.1.cmml">Ã—</mo><msub
    id="S3.SS1.p2.7.m7.1.1.3.5" xref="S3.SS1.p2.7.m7.1.1.3.5.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.5.2"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2.cmml">A</mi><mi class="ltx_font_mathcaligraphic"
    id="S3.SS1.p2.7.m7.1.1.3.5.3" xref="S3.SS1.p2.7.m7.1.1.3.5.3.cmml">ğ’©</mi></msub></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1"><ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">ğ’¯</ci><apply
    id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.2">ğ’®</ci><apply id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3"><csymbol
    cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3">subscript</csymbol><ci
    id="S3.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2">ğ´</ci><cn type="integer"
    id="S3.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.3">1</cn></apply><ci
    id="S3.SS1.p2.7.m7.1.1.3.4.cmml" xref="S3.SS1.p2.7.m7.1.1.3.4">â€¦</ci><apply id="S3.SS1.p2.7.m7.1.1.3.5.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.5.1.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.5.2.cmml"
    xref="S3.SS1.p2.7.m7.1.1.3.5.2">ğ´</ci><ci id="S3.SS1.p2.7.m7.1.1.3.5.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.5.3">ğ’©</ci></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathcal{T}=\mathcal{S}\times
    A_{1}\times...\times A_{\mathcal{N}}</annotation></semantics></math>.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Given a learning agent <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="i"
    display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1"
    xref="S3.SS1.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ‘–</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">i</annotation></semantics></math>
    and using the common shorthand notation <math id="S3.SS1.p3.2.m2.1" class="ltx_Math"
    alttext="\bm{-i}=\mathcal{N}\setminus\{i\}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow
    id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mrow id="S3.SS1.p3.2.m2.1.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p3.2.m2.1.2.2a" xref="S3.SS1.p3.2.m2.1.2.2.cmml">âˆ’</mo><mi id="S3.SS1.p3.2.m2.1.2.2.2"
    xref="S3.SS1.p3.2.m2.1.2.2.2.cmml">ğ’Š</mi></mrow><mo id="S3.SS1.p3.2.m2.1.2.1"
    xref="S3.SS1.p3.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS1.p3.2.m2.1.2.3" xref="S3.SS1.p3.2.m2.1.2.3.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.2.3.2" xref="S3.SS1.p3.2.m2.1.2.3.2.cmml">ğ’©</mi><mo
    id="S3.SS1.p3.2.m2.1.2.3.1" xref="S3.SS1.p3.2.m2.1.2.3.1.cmml">âˆ–</mo><mrow id="S3.SS1.p3.2.m2.1.2.3.3.2"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.1"
    xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">{</mo><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><mo
    stretchy="false" id="S3.SS1.p3.2.m2.1.2.3.3.2.2" xref="S3.SS1.p3.2.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2"><apply id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2"><ci
    id="S3.SS1.p3.2.m2.1.2.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2.2">ğ’Š</ci></apply><apply
    id="S3.SS1.p3.2.m2.1.2.3.cmml" xref="S3.SS1.p3.2.m2.1.2.3"><ci id="S3.SS1.p3.2.m2.1.2.3.2.cmml"
    xref="S3.SS1.p3.2.m2.1.2.3.2">ğ’©</ci><set id="S3.SS1.p3.2.m2.1.2.3.3.1.cmml" xref="S3.SS1.p3.2.m2.1.2.3.3.2"><ci
    id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ğ‘–</ci></set></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bm{-i}=\mathcal{N}\setminus\{i\}</annotation></semantics></math>
    for the set of opponents, the value function now depends on the joint action <math
    id="S3.SS1.p3.3.m3.2" class="ltx_Math" alttext="\bm{a}=(a_{i},\bm{a_{-i}})" display="inline"><semantics
    id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.4" xref="S3.SS1.p3.3.m3.2.2.4.cmml">ğ’‚</mi><mo id="S3.SS1.p3.3.m3.2.2.3"
    xref="S3.SS1.p3.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml"><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">(</mo><msub
    id="S3.SS1.p3.3.m3.1.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.2"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.3.m3.1.1.1.1.1.3"
    xref="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p3.3.m3.2.2.2.2.4"
    xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.3.m3.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.cmml"><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml">ğ’‚</mi><mrow
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p3.3.m3.2.2.2.2.2.3a" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p3.3.m3.2.2.2.2.5" xref="S3.SS1.p3.3.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.3.m3.2b"><apply id="S3.SS1.p3.3.m3.2.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2"><ci id="S3.SS1.p3.3.m3.2.2.4.cmml" xref="S3.SS1.p3.3.m3.2.2.4">ğ’‚</ci><interval
    closure="open" id="S3.SS1.p3.3.m3.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2"><apply
    id="S3.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.2">ğ‘</ci><ci
    id="S3.SS1.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1.1.3">ğ‘–</ci></apply><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2"><csymbol cd="ambiguous"
    id="S3.SS1.p3.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2">subscript</csymbol><ci
    id="S3.SS1.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.2">ğ’‚</ci><apply
    id="S3.SS1.p3.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.2.2.2.3"><ci id="S3.SS1.p3.3.m3.2.2.2.2.2.3.2.cmml"
    xref="S3.SS1.p3.3.m3.2.2.2.2.2.3.2">ğ’Š</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.3.m3.2c">\bm{a}=(a_{i},\bm{a_{-i}})</annotation></semantics></math>,
    and the joint policy <math id="S3.SS1.p3.4.m4.4" class="ltx_Math" alttext="\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})"
    display="inline"><semantics id="S3.SS1.p3.4.m4.4a"><mrow id="S3.SS1.p3.4.m4.4.4"
    xref="S3.SS1.p3.4.m4.4.4.cmml"><mrow id="S3.SS1.p3.4.m4.4.4.3" xref="S3.SS1.p3.4.m4.4.4.3.cmml"><mi
    id="S3.SS1.p3.4.m4.4.4.3.2" xref="S3.SS1.p3.4.m4.4.4.3.2.cmml">ğ…</mi><mo lspace="0em"
    rspace="0em" id="S3.SS1.p3.4.m4.4.4.3.1" xref="S3.SS1.p3.4.m4.4.4.3.1.cmml">â€‹</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.3.3.2" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.3.3.2.1" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.3.3.2.2"
    xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">,</mo><mi id="S3.SS1.p3.4.m4.2.2" xref="S3.SS1.p3.4.m4.2.2.cmml">ğ’‚</mi><mo
    stretchy="false" id="S3.SS1.p3.4.m4.4.4.3.3.2.3" xref="S3.SS1.p3.4.m4.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.SS1.p3.4.m4.4.4.2" xref="S3.SS1.p3.4.m4.4.4.2.cmml">=</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1" xref="S3.SS1.p3.4.m4.4.4.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.2.cmml"><mo id="S3.SS1.p3.4.m4.4.4.1.2.2" xref="S3.SS1.p3.4.m4.4.4.1.2.2.cmml">âˆ</mo><mi
    id="S3.SS1.p3.4.m4.4.4.1.2.3" xref="S3.SS1.p3.4.m4.4.4.1.2.3.cmml">j</mi></msub><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.cmml"><msub id="S3.SS1.p3.4.m4.4.4.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.3.2" xref="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml">Ï€</mi><mi
    id="S3.SS1.p3.4.m4.4.4.1.1.3.3" xref="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml">j</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p3.4.m4.4.4.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.2.cmml">â€‹</mo><mrow
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.2" xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">(</mo><mi
    id="S3.SS1.p3.4.m4.3.3" xref="S3.SS1.p3.4.m4.3.3.cmml">s</mi><mo id="S3.SS1.p3.4.m4.4.4.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml">a</mi><mi id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.4"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p3.4.m4.4b"><apply id="S3.SS1.p3.4.m4.4.4.cmml"
    xref="S3.SS1.p3.4.m4.4.4"><apply id="S3.SS1.p3.4.m4.4.4.3.cmml" xref="S3.SS1.p3.4.m4.4.4.3"><ci
    id="S3.SS1.p3.4.m4.4.4.3.2.cmml" xref="S3.SS1.p3.4.m4.4.4.3.2">ğ…</ci><interval
    closure="open" id="S3.SS1.p3.4.m4.4.4.3.3.1.cmml" xref="S3.SS1.p3.4.m4.4.4.3.3.2"><ci
    id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ğ‘ </ci><ci id="S3.SS1.p3.4.m4.2.2.cmml"
    xref="S3.SS1.p3.4.m4.2.2">ğ’‚</ci></interval></apply><apply id="S3.SS1.p3.4.m4.4.4.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1"><apply id="S3.SS1.p3.4.m4.4.4.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.2.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2">subscript</csymbol><csymbol
    cd="latexml" id="S3.SS1.p3.4.m4.4.4.1.2.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.2">product</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.2.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.2.3">ğ‘—</ci></apply><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1"><apply id="S3.SS1.p3.4.m4.4.4.1.1.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.3.1.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.2">ğœ‹</ci><ci id="S3.SS1.p3.4.m4.4.4.1.1.3.3.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.3.3">ğ‘—</ci></apply><interval closure="open" id="S3.SS1.p3.4.m4.4.4.1.1.1.2.cmml"
    xref="S3.SS1.p3.4.m4.4.4.1.1.1.1"><ci id="S3.SS1.p3.4.m4.3.3.cmml" xref="S3.SS1.p3.4.m4.3.3">ğ‘ </ci><apply
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1">subscript</csymbol><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.2">ğ‘</ci><ci
    id="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.4.4.1.1.1.1.1.3">ğ‘—</ci></apply></interval></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p3.4.m4.4c">\bm{\pi}(s,\bm{a})=\prod_{j}\pi_{j}(s,a_{j})</annotation></semantics></math>:â·â·7In
    this setting each agent independently executes a policy, however, there are other
    cases where this does not hold, for example when agents have a coordinated exploration
    strategy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.E4.m1.6" class="ltx_Math" alttext="V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})]." display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6.1"
    xref="S3.E4.m1.6.6.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.6" xref="S3.E4.m1.6.6.1.1.6.cmml"><msubsup id="S3.E4.m1.6.6.1.1.6.2"
    xref="S3.E4.m1.6.6.1.1.6.2.cmml"><mi id="S3.E4.m1.6.6.1.1.6.2.2.2" xref="S3.E4.m1.6.6.1.1.6.2.2.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.6.2.3" xref="S3.E4.m1.6.6.1.1.6.2.3.cmml">i</mi><mi id="S3.E4.m1.6.6.1.1.6.2.2.3"
    xref="S3.E4.m1.6.6.1.1.6.2.2.3.cmml">ğ…</mi></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E4.m1.6.6.1.1.6.1" xref="S3.E4.m1.6.6.1.1.6.1.cmml">â€‹</mo><mrow id="S3.E4.m1.6.6.1.1.6.3.2"
    xref="S3.E4.m1.6.6.1.1.6.cmml"><mo stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.1"
    xref="S3.E4.m1.6.6.1.1.6.cmml">(</mo><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">s</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.6.3.2.2" xref="S3.E4.m1.6.6.1.1.6.cmml">)</mo></mrow></mrow><mo
    rspace="0.111em" id="S3.E4.m1.6.6.1.1.5" xref="S3.E4.m1.6.6.1.1.5.cmml">=</mo><mrow
    id="S3.E4.m1.6.6.1.1.4" xref="S3.E4.m1.6.6.1.1.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.5"
    xref="S3.E4.m1.6.6.1.1.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.5.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.6.6.1.1.4.5.3" xref="S3.E4.m1.6.6.1.1.4.5.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.5.3.2" xref="S3.E4.m1.6.6.1.1.4.5.3.2.cmml">ğ’‚</mi><mo id="S3.E4.m1.6.6.1.1.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.5.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.5.3.3.cmml">ğ’œ</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.6"
    xref="S3.E4.m1.6.6.1.1.4.4.6.cmml">ğ…</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">â€‹</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.7.2" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.1" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">(</mo><mi
    id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.7.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">,</mo><mi id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml">ğ’‚</mi><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.7.2.3" xref="S3.E4.m1.6.6.1.1.4.4.7.1.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.5.cmml">â€‹</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.cmml"><munder id="S3.E4.m1.6.6.1.1.4.4.4.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.cmml"><mo movablelimits="false" id="S3.E4.m1.6.6.1.1.4.4.4.5.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.2.cmml">âˆ‘</mo><mrow id="S3.E4.m1.6.6.1.1.4.4.4.5.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml"><msup id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml">â€²</mo></msup><mo id="S3.E4.m1.6.6.1.1.4.4.4.5.3.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml">ğ’®</mi></mrow></munder><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.E4.m1.6.6.1.1.4.4.4.4.6" xref="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml">ğ’¯</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">â€‹</mo><mrow
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.4" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml">ğ’‚</mi><mrow id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.7" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.8" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.5a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.5.cmml">â€‹</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">[</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml"><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml"><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.4.cmml">â€‹</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E4.m1.5.5" xref="S3.E4.m1.5.5.cmml">s</mi><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.5"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.6"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml">ğ’‚</mi><mrow id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.7" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.8" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml">Î³</mi><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">â€‹</mo><msub
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml">V</mi><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2a" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.2.cmml">â€‹</mo><mrow
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.3" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1"><apply
    id="S3.E4.m1.6.6.1.1.6.cmml" xref="S3.E4.m1.6.6.1.1.6"><apply id="S3.E4.m1.6.6.1.1.6.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">subscript</csymbol><apply id="S3.E4.m1.6.6.1.1.6.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.6.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2">superscript</csymbol><ci id="S3.E4.m1.6.6.1.1.6.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.6.2.2.2">ğ‘‰</ci><ci id="S3.E4.m1.6.6.1.1.6.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.2.3">ğ…</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.6.2.3.cmml" xref="S3.E4.m1.6.6.1.1.6.2.3">ğ‘–</ci></apply><ci
    id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">ğ‘ </ci></apply><apply id="S3.E4.m1.6.6.1.1.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4"><apply id="S3.E4.m1.6.6.1.1.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3"><ci id="S3.E4.m1.6.6.1.1.4.5.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.5.3.2">ğ’‚</ci><ci id="S3.E4.m1.6.6.1.1.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.5.3.3">ğ’œ</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.6">ğ…</ci><interval closure="open" id="S3.E4.m1.6.6.1.1.4.4.7.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.7.2"><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">ğ‘ </ci><ci
    id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3">ğ’‚</ci></interval><apply id="S3.E4.m1.6.6.1.1.4.4.4.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4"><apply id="S3.E4.m1.6.6.1.1.4.4.4.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5">subscript</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.2">ğ‘ </ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.2.3">â€²</ci></apply><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.5.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.5.3.3">ğ’®</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.6.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.6">ğ’¯</ci><vector id="S3.E4.m1.6.6.1.1.3.3.3.3.3.4.cmml"
    xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3"><ci id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4">ğ‘ </ci><apply
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci
    id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.2">ğ’‚</ci><apply
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3"><ci
    id="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.2.2.2.2.2.2.2.3.2">ğ’Š</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.2">ğ‘ </ci><ci
    id="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3.3.3.3.3.3.3">â€²</ci></apply></vector><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1"><csymbol
    cd="latexml" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.2.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3"><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.2">ğ‘…</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.5.3">ğ‘–</ci></apply><vector
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3"><ci
    id="S3.E4.m1.5.5.cmml" xref="S3.E4.m1.5.5">ğ‘ </ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.1.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.2">ğ’‚</ci><apply id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3"><ci id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2.cmml"
    xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.2.2.2.2.3.2">ğ’Š</ci></apply></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.2">ğ‘ </ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.3.3.3.3.3">â€²</ci></apply></vector></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4"><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.3">ğ›¾</ci><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4">subscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.2">ğ‘‰</ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.4.3">ğ‘–</ci></apply><apply
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.2">ğ‘ </ci><ci
    id="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.4.4.4.4.4.1.1.4.1.1.1.3">â€²</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E4.m1.6c">V^{\bm{\pi}}_{i}(s)=\sum_{\bm{a}\in\mathcal{A}}\bm{\pi}(s,\bm{a})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V_{i}(s^{\prime})].</annotation></semantics></math> |  | (4) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: Consequently, the optimal policy is dependent on the other agentsâ€™ policies,
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | <math id="S3.E5X.2.1.1.m1.6" class="ltx_Math" alttext="\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)="
    display="inline"><semantics id="S3.E5X.2.1.1.m1.6a"><mrow id="S3.E5X.2.1.1.m1.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.2" xref="S3.E5X.2.1.1.m1.6.6.2.cmml"><msubsup
    id="S3.E5X.2.1.1.m1.6.6.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.4.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml">Ï€</mi><mi id="S3.E5X.2.1.1.m1.6.6.2.4.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml">i</mi><mo id="S3.E5X.2.1.1.m1.6.6.2.4.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.4.3.cmml">âˆ—</mo></msubsup><mo lspace="0em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.2.3" xref="S3.E5X.2.1.1.m1.6.6.2.3.cmml">â€‹</mo><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml"><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.3.3" xref="S3.E5X.2.1.1.m1.3.3.cmml">s</mi><mo
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.4" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub
    id="S3.E5X.2.1.1.m1.5.5.1.1.1.1" xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml">a</mi><mi id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.6.6.2.2.2.5"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.6.6.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml">ğ…</mi><mrow id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.6.6.2.2.2.6" xref="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml">)</mo></mrow></mrow><mo
    id="S3.E5X.2.1.1.m1.6.6.4" xref="S3.E5X.2.1.1.m1.6.6.4.cmml">=</mo><mrow id="S3.E5X.2.1.1.m1.6.6.5"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml"><munder
    id="S3.E5X.2.1.1.m1.6.6.5.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml"><mrow id="S3.E5X.2.1.1.m1.6.6.5.2.1.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em"
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.1.cmml">â€‹</mo><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml">max</mi></mrow><msub
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml">Ï€</mi><mi id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml">i</mi></msub></munder><mo id="S3.E5X.2.1.1.m1.6.6.5.2a"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.cmml">â¡</mo><msubsup id="S3.E5X.2.1.1.m1.6.6.5.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml">V</mi><mi
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml">i</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5X.2.1.1.m1.1.1.1.1.1" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml">Ï€</mi><mi id="S3.E5X.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5X.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5X.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5X.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml">ğ…</mi><mrow
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5X.2.1.1.m1.2.2.2.2.2.3a" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    stretchy="false" id="S3.E5X.2.1.1.m1.2.2.2.2.5" xref="S3.E5X.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5X.2.1.1.m1.6.6.5.1" xref="S3.E5X.2.1.1.m1.6.6.5.1.cmml">â€‹</mo><mrow
    id="S3.E5X.2.1.1.m1.6.6.5.3.2" xref="S3.E5X.2.1.1.m1.6.6.5.cmml"><mo stretchy="false"
    id="S3.E5X.2.1.1.m1.6.6.5.3.2.1" xref="S3.E5X.2.1.1.m1.6.6.5.cmml">(</mo><mi id="S3.E5X.2.1.1.m1.4.4"
    xref="S3.E5X.2.1.1.m1.4.4.cmml">s</mi><mo stretchy="false" id="S3.E5X.2.1.1.m1.6.6.5.3.2.2"
    xref="S3.E5X.2.1.1.m1.6.6.5.cmml">)</mo></mrow></mrow><mo id="S3.E5X.2.1.1.m1.6.6.6"
    xref="S3.E5X.2.1.1.m1.6.6.6.cmml">=</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.E5X.2.1.1.m1.6b"><apply id="S3.E5X.2.1.1.m1.6.6.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply
    id="S3.E5X.2.1.1.m1.6.6b.cmml" xref="S3.E5X.2.1.1.m1.6.6"><apply id="S3.E5X.2.1.1.m1.6.6.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2"><apply id="S3.E5X.2.1.1.m1.6.6.2.4.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.4.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">superscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4"><csymbol cd="ambiguous"
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.2">ğœ‹</ci><ci
    id="S3.E5X.2.1.1.m1.6.6.2.4.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.4.2.3">ğ‘–</ci></apply></apply><vector
    id="S3.E5X.2.1.1.m1.6.6.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.2.2.2"><ci id="S3.E5X.2.1.1.m1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.3.3">ğ‘ </ci><apply id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.1.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.2">ğ‘</ci><ci id="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.5.5.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.2">ğ…</ci><apply id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.2.2.2.2.3.2">ğ’Š</ci></apply></apply></vector></apply><apply
    id="S3.E5X.2.1.1.m1.6.6.5.cmml" xref="S3.E5X.2.1.1.m1.6.6.5"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2"><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.1.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1">subscript</csymbol><apply
    id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2"><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.2">arg</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.2.3">max</ci></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3">subscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.2">ğœ‹</ci><ci id="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">subscript</csymbol><apply id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2"><csymbol cd="ambiguous" id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.1.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2">superscript</csymbol><ci id="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2.cmml"
    xref="S3.E5X.2.1.1.m1.6.6.5.2.2.2.2">ğ‘‰</ci><interval closure="open" id="S3.E5X.2.1.1.m1.2.2.2.3.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2"><apply id="S3.E5X.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.2">ğœ‹</ci><ci
    id="S3.E5X.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5X.2.1.1.m1.1.1.1.1.1.3">ğ‘–</ci></apply><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5X.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.2">ğ…</ci><apply
    id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3"><ci id="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2.cmml"
    xref="S3.E5X.2.1.1.m1.2.2.2.2.2.3.2">ğ’Š</ci></apply></apply></interval></apply><ci
    id="S3.E5X.2.1.1.m1.6.6.5.2.2.3.cmml" xref="S3.E5X.2.1.1.m1.6.6.5.2.2.3">ğ‘–</ci></apply></apply><ci
    id="S3.E5X.2.1.1.m1.4.4.cmml" xref="S3.E5X.2.1.1.m1.4.4">ğ‘ </ci></apply></apply><apply
    id="S3.E5X.2.1.1.m1.6.6c.cmml" xref="S3.E5X.2.1.1.m1.6.6"><csymbol cd="latexml"
    id="S3.E5X.2.1.1.m1.6.6.7.cmml" xref="S3.E5X.2.1.1.m1.6.6.7">absent</csymbol></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5X.2.1.1.m1.6c">\displaystyle\pi_{i}^{*}(s,a_{i},\bm{\pi_{-i}})=\operatorname*{arg\,max}_{\pi_{i}}V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s)=</annotation></semantics></math>
    |  | (5) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '|  |  | <math id="S3.E5Xa.2.1.1.m1.7" class="ltx_Math" alttext="\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})]." display="inline"><semantics id="S3.E5Xa.2.1.1.m1.7a"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"><munder id="S3.E5Xa.2.1.1.m1.7.7.1.1.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml">arg</mi><mo
    lspace="0.170em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.1.cmml">â€‹</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml">max</mi></mrow><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml">Ï€</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml">i</mi></msub></munder><mo
    lspace="0.167em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.7.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.2.cmml">âˆ‘</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml">ğ’‚</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.1.cmml">âˆˆ</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml">ğ’œ</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml">Ï€</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.3.3" xref="S3.E5Xa.2.1.1.m1.3.3.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">â€‹</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml">ğ…</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml">âˆ’</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml">ğ’Š</mi></mrow></msub><mo lspace="0em"
    rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7b" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.4.4" xref="S3.E5Xa.2.1.1.m1.4.4.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml">ğ’‚</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml">âˆ’</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7c" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.7.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml"><mstyle
    displaystyle="true" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><munder
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml"><mo
    movablelimits="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.2.cmml">âˆ‘</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml"><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml">â€²</mo></msup><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.1.cmml">âˆˆ</mo><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml">ğ’®</mi></mrow></munder></mstyle><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml">ğ’¯</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.5.5" xref="S3.E5Xa.2.1.1.m1.5.5.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.6"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml">ğ’‚</mi><mrow id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml">âˆ’</mo><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.5.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">[</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml"><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml"><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml">R</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml">i</mi></msub><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.4.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">(</mo><mi
    id="S3.E5Xa.2.1.1.m1.6.6" xref="S3.E5Xa.2.1.1.m1.6.6.cmml">s</mi><mo id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.5"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml">a</mi><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.6" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msub
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml">ğ’‚</mi><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml">âˆ’</mo><mi id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.7" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">,</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.8" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.5.cmml">+</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml">Î³</mi><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">â€‹</mo><msubsup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml">V</mi><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml">i</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml"><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">(</mo><msub
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml">Ï€</mi><mi id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3"
    xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E5Xa.2.1.1.m1.2.2.2.2.4"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E5Xa.2.1.1.m1.2.2.2.2.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml">ğ…</mi><mrow
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml"><mo
    class="ltx_mathvariant_bold" mathvariant="bold" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3a"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml">âˆ’</mo><mi id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2"
    xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml">ğ’Š</mi></mrow></msub><mo stretchy="false"
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.5" xref="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo
    lspace="0em" rspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2a" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.2.cmml">â€‹</mo><mrow
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">(</mo><msup
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml"><mi
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml">s</mi><mo
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml">â€²</mo></msup><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.3" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    lspace="0em" id="S3.E5Xa.2.1.1.m1.7.7.1.2" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.E5Xa.2.1.1.m1.7b"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1"><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.2">arg</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.2.3">max</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.2">ğœ‹</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.8.3.3">ğ‘–</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.2">ğ’‚</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.7.3.3">ğ’œ</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.2">ğœ‹</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.8.3">ğ‘–</ci></apply><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1"><ci
    id="S3.E5Xa.2.1.1.m1.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.3.3">ğ‘ </ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></interval><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.2">ğ…</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.9.3.2">ğ’Š</ci></apply></apply><interval closure="open"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1"><ci
    id="S3.E5Xa.2.1.1.m1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.4.4">ğ‘ </ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.2">ğ’‚</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.2.2.2.1.1.3.2">ğ’Š</ci></apply></apply></interval><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.2">ğ‘ </ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.2.3">â€²</ci></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.5.3.3">ğ’®</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.6">ğ’¯</ci><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.5.5.cmml" xref="S3.E5Xa.2.1.1.m1.5.5">ğ‘ </ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.2">ğ‘</ci><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.3.3.3.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.2">ğ’‚</ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3"><ci id="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.4.4.4.2.2.2.2.3.2">ğ’Š</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.2">ğ‘ </ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.5.5.5.3.3.3.3.3">â€²</ci></apply></vector><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1"><csymbol
    cd="latexml" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.2">delimited-[]</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3"><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.2">ğ‘…</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.5.3">ğ‘–</ci></apply><vector
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3"><ci
    id="S3.E5Xa.2.1.1.m1.6.6.cmml" xref="S3.E5Xa.2.1.1.m1.6.6">ğ‘ </ci><apply id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.cmml"
    xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.2">ğ‘</ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.2">ğ’‚</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.2.2.2.2.3.2">ğ’Š</ci></apply></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.2">ğ‘ </ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.3.3.3.3.3">â€²</ci></apply></vector></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4"><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.3">ğ›¾</ci><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">subscript</csymbol><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.2.2">ğ‘‰</ci><interval
    closure="open" id="S3.E5Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2"><apply
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.2">ğœ‹</ci><ci
    id="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.1.1.1.1.1.3">ğ‘–</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.2">ğ…</ci><apply
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3"><ci
    id="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E5Xa.2.1.1.m1.2.2.2.2.2.3.2">ğ’Š</ci></apply></apply></interval></apply><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.4.3">ğ‘–</ci></apply><apply
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1"><csymbol
    cd="ambiguous" id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.1.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1">superscript</csymbol><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.2">ğ‘ </ci><ci
    id="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3.cmml" xref="S3.E5Xa.2.1.1.m1.7.7.1.1.6.6.6.4.4.1.1.4.1.1.1.3">â€²</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.E5Xa.2.1.1.m1.7c">\displaystyle\operatorname*{arg\,max}_{\pi_{i}}\sum_{\bm{a}\in\mathcal{A}}\pi_{i}(s,a_{i})\bm{\pi_{-i}}(s,\bm{a_{-i}})\sum_{s^{\prime}\in\mathcal{S}}\mathcal{T}(s,a_{i},\bm{a_{-i}},s^{\prime})[R_{i}(s,a_{i},\bm{a_{-i}},s^{\prime})+\gamma
    V^{(\pi_{i},\bm{\pi_{-i}})}_{i}(s^{\prime})].</annotation></semantics></math>
    |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: Specifically, the opponentsâ€™ joint policy <math id="S3.SS1.p4.1.m1.2" class="ltx_Math"
    alttext="\bm{\pi_{-i}}(s,\bm{a_{-i}})" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow
    id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml"><msub id="S3.SS1.p4.1.m1.2.2.3"
    xref="S3.SS1.p4.1.m1.2.2.3.cmml"><mi id="S3.SS1.p4.1.m1.2.2.3.2" xref="S3.SS1.p4.1.m1.2.2.3.2.cmml">ğ…</mi><mrow
    id="S3.SS1.p4.1.m1.2.2.3.3" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml"><mo class="ltx_mathvariant_bold"
    mathvariant="bold" id="S3.SS1.p4.1.m1.2.2.3.3a" xref="S3.SS1.p4.1.m1.2.2.3.3.cmml">âˆ’</mo><mi
    id="S3.SS1.p4.1.m1.2.2.3.3.2" xref="S3.SS1.p4.1.m1.2.2.3.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.2.2.2" xref="S3.SS1.p4.1.m1.2.2.2.cmml">â€‹</mo><mrow
    id="S3.SS1.p4.1.m1.2.2.1.1" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml"><mo stretchy="false"
    id="S3.SS1.p4.1.m1.2.2.1.1.2" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1"
    xref="S3.SS1.p4.1.m1.1.1.cmml">s</mi><mo id="S3.SS1.p4.1.m1.2.2.1.1.3" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">,</mo><msub
    id="S3.SS1.p4.1.m1.2.2.1.1.1" xref="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.2.2.1.1.1.2"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml">ğ’‚</mi><mrow id="S3.SS1.p4.1.m1.2.2.1.1.1.3"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold"
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3a" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml">âˆ’</mo><mi
    id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml">ğ’Š</mi></mrow></msub><mo
    stretchy="false" id="S3.SS1.p4.1.m1.2.2.1.1.4" xref="S3.SS1.p4.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><apply id="S3.SS1.p4.1.m1.2.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2"><apply id="S3.SS1.p4.1.m1.2.2.3.cmml" xref="S3.SS1.p4.1.m1.2.2.3"><csymbol
    cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.2.3">subscript</csymbol><ci
    id="S3.SS1.p4.1.m1.2.2.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.2">ğ…</ci><apply id="S3.SS1.p4.1.m1.2.2.3.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.3.3"><ci id="S3.SS1.p4.1.m1.2.2.3.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.3.3.2">ğ’Š</ci></apply></apply><interval
    closure="open" id="S3.SS1.p4.1.m1.2.2.1.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1"><ci
    id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğ‘ </ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.2.2.1.1.1.1.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.2.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.2">ğ’‚</ci><apply id="S3.SS1.p4.1.m1.2.2.1.1.1.3.cmml"
    xref="S3.SS1.p4.1.m1.2.2.1.1.1.3"><ci id="S3.SS1.p4.1.m1.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.2.2.1.1.1.3.2">ğ’Š</ci></apply></apply></interval></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">\bm{\pi_{-i}}(s,\bm{a_{-i}})</annotation></semantics></math>
    can be non-stationary, i.e., changes as the opponentsâ€™ policies change over time,
    for example with learning opponents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Convergence results
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LittmanÂ [[125](#bib.bib125)] studied convergence properties of reinforcement
    learning joint action agentsÂ [[126](#bib.bib126)] in Markov games with the following
    conclusions: in adversarial environments (zero-sum games) an optimal play can
    be guaranteed against an arbitrary opponent, i.e., Minimax Q-learningÂ [[124](#bib.bib124)].
    In coordination environments (e.g., in cooperative games all agents share the
    same reward function), strong assumptions need be made about other agents to guarantee
    convergence to optimal behaviorÂ [[125](#bib.bib125)], e.g., Nash Q-learningÂ [[127](#bib.bib127)]
    and Friend-or-Foe Q-learningÂ [[128](#bib.bib128)]. In other types of environments
    no value-based RL algorithms with guaranteed convergence properties are knownÂ [[125](#bib.bib125)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Recent work on MDRL have addressed scalability and have focused significantly
    less on convergence guarantees, with few exceptionsÂ [[129](#bib.bib129), [130](#bib.bib130),
    [131](#bib.bib131), [132](#bib.bib132)]. One notable work has shown a connection
    between update rules for actor-critic algorithms for multiagent partially observable
    settings and (counterfactual) regret minimization:â¸â¸8 Counterfactual regret minimization
    is a technique for solving large games based on regret minimizationÂ [[133](#bib.bib133),
    [134](#bib.bib134)] due to a well-known connection between regret and Nash equilibriaÂ [[135](#bib.bib135)].
    It has been one of the reasons of successes in PokerÂ [[16](#bib.bib16), [17](#bib.bib17)].
    the advantage values are scaled counterfactual regrets. This lead to new convergence
    properties of independent RL algorithms in zero-sum games with imperfect informationÂ [[136](#bib.bib136)].
    The result is also used to support policy gradient optimization against worst-case
    opponents, in a new algorithm called Exploitability DescentÂ [[137](#bib.bib137)].â¹â¹9This
    algorithm is similar to CFR-BRÂ [[138](#bib.bib138)] and has the main advantage
    that the current policy convergences rather than the average policy, so there
    is no need to learn the average strategy, which requires large reservoir buffers
    or many past networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: We refer the interested reader to seminal works about convergence in multiagent
    domainsÂ [[139](#bib.bib139), [140](#bib.bib140), [141](#bib.bib141), [142](#bib.bib142),
    [143](#bib.bib143), [144](#bib.bib144), [145](#bib.bib145), [146](#bib.bib146),
    [147](#bib.bib147), [148](#bib.bib148), [149](#bib.bib149)]. Note that instead
    of convergence, some MAL algorithms have proved learning a best response against
    classes of opponentsÂ [[150](#bib.bib150), [151](#bib.bib151), [152](#bib.bib152)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other common problems in MAL, including action shadowingÂ [[34](#bib.bib34),
    [33](#bib.bib33)], the curse of dimensionalityÂ [[5](#bib.bib5)], and multiagent
    credit assignmentÂ [[32](#bib.bib32)]. Describing each problem is out of the scope
    of this survey. However, we refer the interested reader to excellent resources
    on general MALÂ [[4](#bib.bib4), [153](#bib.bib153), [154](#bib.bib154)], as well
    as surveys in specific areas: game theory and multiagent reinforcement learningÂ [[5](#bib.bib5),
    [6](#bib.bib6)], cooperative scenariosÂ [[7](#bib.bib7), [8](#bib.bib8)], evolutionary
    dynamics of multiagent learningÂ [[9](#bib.bib9)], learning in non-stationary environmentsÂ [[10](#bib.bib10)],
    agents modeling agentsÂ [[11](#bib.bib11)], and transfer learning in multiagent
    RLÂ [[12](#bib.bib12)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 MDRL categorization
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In SectionÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning â€£ 2 Single-agent
    learning â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") we outlined some recent
    works in single-agent DRL since an exhaustive list is out of the scope of this
    article. This explosion of works has led DRL to be extended and combined with
    other techniquesÂ [[23](#bib.bib23), [37](#bib.bib37), [29](#bib.bib29)]. One natural
    extension to DRL is to test whether these approaches could be applied in a multiagent
    environment.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'We analyzed the most recent works (that are not covered by previous MAL surveysÂ [[10](#bib.bib10),
    [11](#bib.bib11)] and we do not consider genetic algorithms or swarm intelligence
    in this survey) that have a clear connection with MDRL. We propose 4 categories
    which take inspiration from previous surveysÂ [[1](#bib.bib1), [5](#bib.bib5),
    [7](#bib.bib7), [11](#bib.bib11)] and that conveniently describe and represent
    current works. Note that some of these works fit into more than one category (they
    are not mutually exclusive), therefore their summaries are presented in all applicable
    TablesÂ [2](#S3.T2 "Table 2 â€£ 3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")-[4](#S3.T4 "Table 4 â€£ 3.2
    MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€"), however, for the ease of exposition
    when describing them in the text we only do so in one category. Additionally,
    for each work we present its learning type, either a value-based method (e.g.,
    DQN) or a policy gradient method (e.g., actor-critic); also, we mention if the
    setting is evaluated in a fully cooperative, fully competitive or mixed environment
    (both cooperative and competitive).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Analysis of emergent behaviors*. These works, in general, do not propose learning
    algorithms â€” their main focus is to analyze and evaluate DRL algorithms, e.g.,
    DQNÂ [[155](#bib.bib155), [156](#bib.bib156), [157](#bib.bib157)], PPOÂ [[158](#bib.bib158),
    [157](#bib.bib157)] and othersÂ [[159](#bib.bib159), [157](#bib.bib157), [160](#bib.bib160)],
    in a multiagent environment. In this category we found works which analyze behaviors
    in the three major settings: cooperative, competitive and mixed scenarios; see
    SectionÂ [3.3](#S3.SS3 "3.3 Emergent behaviors â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") and TableÂ [2](#S3.T2 "Table
    2 â€£ 3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning communication*Â [[161](#bib.bib161), [160](#bib.bib160), [162](#bib.bib162),
    [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)]. These works explore
    a sub-area in which agents can share information with communication protocols,
    for example through direct messagesÂ [[162](#bib.bib162)] or via a shared memoryÂ [[165](#bib.bib165)].
    This area is attracting attention and it had not been explored much in the MAL
    literature. See SectionÂ [3.4](#S3.SS4 "3.4 Learning communication â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€") and TableÂ [2](#S3.T2 "Table 2 â€£ 3.2 MDRL categorization â€£ 3
    Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€").'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning cooperation*. While learning to communicate is an emerging area,
    fostering cooperation in learning agents has a long history of research in MALÂ [[7](#bib.bib7),
    [8](#bib.bib8)]. In this category the analyzed works are evaluated in either cooperative
    or mixed settings. Some works in this category take inspiration from MAL (e.g.,
    leniency, hysteresis, and difference rewards concepts) and extend them to the
    MDRL settingÂ [[35](#bib.bib35), [166](#bib.bib166), [167](#bib.bib167)]. A notable
    exceptionÂ [[168](#bib.bib168)] takes a key component from RL (i.e., experience
    replay buffer) and adapts it for MDRL. See SectionÂ [3.5](#S3.SS5 "3.5 Learning
    cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€") and TableÂ [3](#S3.T3 "Table 3 â€£ 3.2 MDRL categorization
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€").'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Agents modeling agents*. Albrecht and StoneÂ [[11](#bib.bib11)] presented a
    thorough survey in this topic and we have found many works that fit into this
    category in the MDRL setting, some taking inspiration from DRLÂ [[169](#bib.bib169),
    [170](#bib.bib170), [171](#bib.bib171)], and others from MALÂ [[172](#bib.bib172),
    [173](#bib.bib173), [64](#bib.bib64), [174](#bib.bib174), [175](#bib.bib175)].
    Modeling agents is helpful not only to cooperate, but also for modeling opponentsÂ [[172](#bib.bib172),
    [169](#bib.bib169), [171](#bib.bib171), [173](#bib.bib173)], inferring goalsÂ [[170](#bib.bib170)],
    and accounting for the learning behavior of other agentsÂ [[64](#bib.bib64)]. In
    this category the analyzed algorithms present their results in either a competitive
    setting or a mixed one (cooperative and competitive). See SectionÂ [3.6](#S3.SS6
    "3.6 Agents modeling agents â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") and TableÂ [4](#S3.T4 "Table
    4 â€£ 3.2 MDRL categorization â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the rest of this section we describe each category along with the summaries
    of related works.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: These papers analyze *emergent behaviors* in MDRL. Learning type is
    either value-based (VB) or policy gradient (PG). Setting where experiments were
    performed: cooperative (CO), competitive (CMP) or mixed. A detailed description
    is given in SectionÂ [3.3](#S3.SS3 "3.3 Emergent behaviors â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '| Work | Summary | Learning | Setting |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Tampuu et al.Â [[155](#bib.bib155)] | Train DQN agents to play Pong. | VB
    | CO&CMP |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Leibo et al.Â [[156](#bib.bib156)] | Train DQN agents to play sequential social
    dilemmas. | VB | Mixed |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: '| Lerer and PeysakhovichÂ [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
- en: '| Leibo et al.Â [[159](#bib.bib159)] | Propose Malthusian reinforcement learning
    which extends self-play to population dynamics. | VB | Mixed |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
- en: '| Bansal et al.Â [[158](#bib.bib158)] | Train PPO agents in competitive MuJoCo
    scenarios. | PG | CMP |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
- en: '| Raghu et al.Â [[157](#bib.bib157)] | Train PPO, A3C, and DQN agents in attacker-defender
    games. | VB, PG | CMP |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| Lazaridou et al.Â [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Mordatch and AbbeelÂ [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: 'Table 2: These papers propose algorithms for *learning communication*. Learning
    type is either value-based (VB) or policy gradient (PG). Setting were experiments
    were performed: cooperative (CO) or mixed. A more detailed description is given
    in SectionÂ [3.4](#S3.SS4 "3.4 Learning communication â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| Lazaridou et al.Â [[161](#bib.bib161)] | Train agents represented with NN
    to learn a communication language. | PG | CO |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Mordatch and AbbeelÂ [[160](#bib.bib160)] | Learn communication with an end-to-end
    differentiable model to train with backpropagation. | PG | CO |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| RIALÂ [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| DIALÂ [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| CommNetÂ [[163](#bib.bib163)] | Use a continuous vector channel for communication
    on a single network. | PG | CO |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| BiCNetÂ [[164](#bib.bib164)] | Use the actor-critic paradigm where communication
    occurs in the latent space. | PG | Mixed |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| MD-MADDPGÂ [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| MADDPG-MDÂ [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: 'Table 3: These papers aim to *learn cooperation*. Learning type is either value-based
    (VB) or policy gradient (PG). Setting where experiments were performed: cooperative
    (CO), competitive (CMP) or mixed. A more detailed description is given in SectionÂ [3.5](#S3.SS5
    "3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| Lerer and PeysakhovichÂ [[176](#bib.bib176)] | Propose DRL agents able to
    cooperate in social dilemmas. | VB | Mixed |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| MD-MADDPGÂ [[165](#bib.bib165)] | Use of a shared memory as a means to multiagent
    communication. | PG | CO |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| MADDPG-MDÂ [[177](#bib.bib177)] | Extend dropout technique to robustify communication
    when applied in multiagent scenarios with direct communication. | PG | CO |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| RIALÂ [[162](#bib.bib162)] | Use a single network (parameter sharing) to train
    agents that take environmental and communication actions. | VB | CO |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| DIALÂ [[162](#bib.bib162)] | Use gradient sharing during learning and communication
    actions during execution. | VB | CO |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| DCH/PSROÂ [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | VB | CO & CMP |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| FingerprintsÂ [[168](#bib.bib168)] | Deal with ER problems in MDRL by conditioning
    the value function on a fingerprint that disambiguates the age of the sampled
    data. | VB | CO |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Lenient-DQNÂ [[35](#bib.bib35)] | Achieve cooperation by leniency, optimism
    in the value function by forgiving suboptimal (low-rewards) actions. | VB | CO
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Hysteretic-DRQNÂ [[166](#bib.bib166)] | Achieve cooperation by using two learning
    rates, depending on the updated values together with multitask learning via policy
    distillation. | VB | CO |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| WDDQNÂ [[178](#bib.bib178)] | Achieve cooperation by leniency, weighted double
    estimators, and a modified prioritized experience replay buffer. | VB | CO |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| FTWÂ [[179](#bib.bib179)] | Agents act in a mixed environment (composed of
    teammates and opponents), it proposes a two-level architecture and population-based
    learning. | PG | Mixed |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| VDNÂ [[180](#bib.bib180)] | Decompose the team action-value function into
    pieces across agents, where the pieces can be easily added. | VB | Mixed |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| QMIXÂ [[181](#bib.bib181)] | Decompose the team action-value function together
    with a mixing network that can recombine them. | VB | Mixed |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| COMAÂ [[167](#bib.bib167)] | Use a centralized critic and a counter-factual
    advantage function based on solving the multiagent credit assignment. | PG | Mixed
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| PS-DQN, PS-TRPO, PS-A3CÂ [[182](#bib.bib182)] | Propose parameter sharing
    for learning cooperative tasks. | VB, PG | CO |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| MADDPGÂ [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: 'Table 4: These papers consider *agents modeling agents*. Learning type is either
    value-based (VB) or policy gradient (PG). Setting where experiments were performed:
    cooperative (CO), competitive (CMP) or mixed. A more detailed description is given
    in SectionÂ [3.6](#S3.SS6 "3.6 Agents modeling agents â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Summary | Learning | Setting |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| MADDPGÂ [[63](#bib.bib63)] | Use an actor-critic approach where the critic
    is augmented with information from other agents, the actions of all agents. |
    PG | Mixed |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| DRONÂ [[169](#bib.bib169)] | Have a network to infer the opponent behavior
    together with the standard DQN architecture. | VB | Mixed |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| DPIQN, DPIRQNÂ [[171](#bib.bib171)] | Learn policy features from raw observations
    that represent high-level opponent behaviors via auxiliary tasks. | VB | Mixed
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| SOMÂ [[170](#bib.bib170)] | Assume the reward function depends on a hidden
    goal of both agents and then use an agentâ€™s own policy to infer the goal of the
    other agent. | PG | Mixed |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| NFSPÂ [[173](#bib.bib173)] | Compute approximate Nash equilibria via self-play
    and two neural networks. | VB | CMP |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| PSRO/DCHÂ [[172](#bib.bib172)] | Policies can overfit to opponents: better
    compute approximate best responses to a mixture of policies. | PG | CO & CMP |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| M3DDPGÂ [[183](#bib.bib183)] | Extend MADDPG with minimax objective to robustify
    the learned policy. | PG | Mixed |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| LOLAÂ [[64](#bib.bib64)] | Use a learning rule where the agent accounts for
    the parameter update of other agents to maximize its own reward. | PG | Mixed
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| ToMnetÂ [[174](#bib.bib174)] | Use an architecture for end-to-end learning
    and inference of diverse opponent types. | PG | Mixed |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Deep Bayes-ToMoPÂ [[175](#bib.bib175)] | Best respond to opponents using Bayesian
    policy reuse, theory of mind, and deep networks. | VB | CMP |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| Deep BPR+[[184](#bib.bib184)] | Bayesian policy reuse and policy distillation
    to quickly best respond to opponents. | VB | CO & CMP |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: 3.3 Emergent behaviors
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some recent works have analyzed the previously mentioned *independent* DRL
    agents (see SectionÂ [3.1](#S3.SS1 "3.1 Multiagent Learning â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€"))
    from the perspective of types of emerging behaviors (e.g., cooperative or competitive).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: One of the earliest MDRL works is by Tampuu et al.Â [[155](#bib.bib155)], which
    had two independent DQN learning agents to play the Atari Pong game. Their focus
    was to adapt the reward function for the learning agents, which resulted in either
    cooperative or competitive emergent behaviors.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Leibo et al.Â [[156](#bib.bib156)] meanwhile studied independent DQNs in the
    context of *sequential social dilemmas*: a Markov game that satisfies certain
    inequalitiesÂ [[156](#bib.bib156)]. The focus of this work was to highlight that
    cooperative or competitive behaviors exist not only as discrete (atomic) actions,
    but they are temporally extended (over policies). In the related setting of one
    shot Markov social dilemmas, Lerer and PeysakhovichÂ [[176](#bib.bib176)] extended
    the famous Tit-for-Tat (TFT)^(10)^(10)10TFT originated in an iterated prisonerâ€™s
    dilemma tournament and later inspired different strategies in MALÂ [[185](#bib.bib185)],
    its generalization, Godfather, is a representative of *leader strategies*Â [[186](#bib.bib186)].
    strategyÂ [[187](#bib.bib187)] for DRL (using function approximators) and showed
    (theoretically and experimentally) that such agents can maintain cooperation.
    To construct the agents they used self-play and two reward schemes: selfish and
    cooperative. Previously, different MAL algorithms were designed to foster cooperation
    in social dilemmas with Q-learning agentsÂ [[188](#bib.bib188), [189](#bib.bib189)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-play is a useful concept for learning algorithms (e.g., fictitious playÂ [[190](#bib.bib190)])
    since under certain classes of games it can guarantee convergence^(11)^(11)11The
    average strategy profile of fictitious players converges to a Nash equilibrium
    in certain classes of games, e.g., two-player zero-sum and potential gamesÂ [[191](#bib.bib191)].
    and it has been used as a standard technique in previous RL and MAL worksÂ [[192](#bib.bib192),
    [14](#bib.bib14), [193](#bib.bib193)]. Despite its common usage self-play can
    be brittle to forgetting past knowledgeÂ [[194](#bib.bib194), [172](#bib.bib172),
    [195](#bib.bib195)] (see SectionÂ [4.5](#S4.SS5 "4.5 Open questions â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") for a note on the role
    of self-play as an open question in MDRL). To overcome this issue, Leibo et al.Â [[159](#bib.bib159)]
    proposed Malthusian reinforcement learning as an extension of self-play to population
    dynamics. The approach can be thought of as community coevolution and has been
    shown to produce better results (avoiding local optima) than independent agents
    with intrinsic motivationÂ [[196](#bib.bib196)]. A limitation of this work is that
    it does not place itself within the state of the art in evolutionary and genetic
    algorithms. Evolutionary strategies have been employed for solving reinforcement
    learning problemsÂ [[197](#bib.bib197)] and for evolving function approximatorsÂ [[75](#bib.bib75)].
    Similarly, they have been used multiagent scenarios to compute approximate Nash
    equilibriaÂ [[198](#bib.bib198)] and as metaheuristic optimization algorithmsÂ [[199](#bib.bib199),
    [200](#bib.bib200), [7](#bib.bib7), [201](#bib.bib201)].'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Bansal et al.Â [[158](#bib.bib158)] explored the emergent behaviors in competitive
    scenarios using the MuJoCo simulatorÂ [[202](#bib.bib202)]. They trained independent
    learning agents with PPO and incorporated two main modifications to deal with
    the MAL nature of the problem. First, they used *exploration rewards*Â [[203](#bib.bib203)]
    which are dense rewards that allow agents to learn basic (non-competitive) behaviors
    â€” this type of reward is annealed through time giving more weight to the environmental
    (competitive) reward. Exploration rewards come from early work in roboticsÂ [[204](#bib.bib204)]
    and single-agent RLÂ [[205](#bib.bib205)], and their goal is to provide dense feedback
    for the learning algorithm to improve sample efficiency (Ng et al.Â [[206](#bib.bib206)]
    studied the theoretical conditions under which modifications of the reward function
    of an MDP preserve the optimal policy). For multiagent scenarios, these dense
    rewards help agents in the beginning phase of the training to learn basic non-competitive
    skills, increasing the probability of random actions from the agent yielding a
    positive reward. The second contribution was *opponent sampling* which maintains
    a pool of older versions of the opponent to sample from, in contrast to using
    the most recent version.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Raghu et al.Â [[157](#bib.bib157)] investigated how DRL algorithms (DQN, A2C,
    and PPO) performed in a family of two-player zero-sum games with tunable complexity,
    called Erdos-Selfridge-Spencer gamesÂ [[207](#bib.bib207), [208](#bib.bib208)].
    Their reasoning is threefold: (i) these games provide a parameterized family of
    environments where (ii) optimal behavior can be completely characterized, and
    (iii) support multiagent play. Their work showed that algorithms can exhibit wide
    variation in performance as the algorithms are tuned to the gameâ€™s difficulty.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Lazaridou et al.Â [[161](#bib.bib161)] proposed a framework for language learning
    that relies on multiagent communication. The agents, represented by (feed-forward)
    neural networks, need to develop an *emergent language* to solve a task. The task
    is formalized as a *signaling game*Â [[209](#bib.bib209)] in which two agents,
    a sender and a receiver, obtain a pair of images. The sender is told one of them
    is the target and is allowed to send a message (from a fixed vocabulary) to the
    receiver. Only when the receiver identifies the target image do both agents receive
    a positive reward. The results show that agents can coordinate for the experimented
    visual-based domain. To analyze the semantic properties^(12)^(12)12The vocabulary
    that agents use was arbitrary and had no initial meaning. To understand its emerging
    semantics they looked at the relationship between symbols and the sets of images
    they referred toÂ [[161](#bib.bib161)]. of the learned communication protocol they
    looked whether symbol usage reflects the semantics of the visual space, and that
    despite some variation, many high level objects groups correspond to the same
    learned symbols using a t-SNEÂ [[210](#bib.bib210)] based analysis (t-SNE is a
    visualization technique for high-dimensional data and it has also been used to
    better understand the behavior of trained DRL agentsÂ [[211](#bib.bib211), [212](#bib.bib212)]).
    A key objective of this work was to determine if the agentâ€™s language could be
    human-interpretable. To achieve this, learned symbols were grounded with natural
    language by extending the signaling game with a supervised image labelling task
    (the sender will be encouraged to use conventional names, making communication
    more transparent to humans). To measure the interpretability of the extended game,
    a crowdsourced survey was performed, and in essence, the trained agent receiver
    was replaced with a human. The results showed that 68% of the cases, human participants
    picked the correct image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Mordatch and AbbeelÂ [[160](#bib.bib160)] investigated the emergence
    of language with the difference that in their setting there were no explicit roles
    for the agents (i.e., sender or receiver). To learn, they proposed an end-to-end
    differentiable model of all agent and environment state dynamics over time to
    calculate the gradient of the return with backpropagation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Learning communication
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, one of the desired emergent behaviors
    of multiagent interaction is the emergence of communicationÂ [[161](#bib.bib161),
    [160](#bib.bib160)]. This setting usually considers a set of *cooperative agents*
    in a *partially observable* environment (see SectionÂ [2.2](#S2.SS2 "2.2 Deep reinforcement
    learning â€£ 2 Single-agent learning â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")) where agents need to maximize their shared utility by means
    of communicating information.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning
    (DIAL) are two methods using deep networks to learn to communicateÂ [[162](#bib.bib162)].
    Both methods use a neural net that outputs the agentâ€™s <math id="S3.SS4.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi
    id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">Q</annotation></semantics></math>
    values (as done in standard DRL algorithms) and a message to communicate to other
    agents in the next timestep. RIAL is based on DRQN and also uses the concept of
    *parameter sharing*, i.e., using a single network whose parameters are shared
    among all agents. In contrast, DIAL directly passes gradients via the communication
    channel during learning, and messages are discretized and mapped to the set of
    communication actions during execution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '*Memory-driven* (MD) communication was proposed on top of the Multi-Agent Deep
    Deterministic Policy Gradient (MADDPG)Â [[63](#bib.bib63)] method. In MD-MADDPG
    [[165](#bib.bib165)], the agents use a shared memory as a communication channel:
    before taking an action, the agent first reads the memory, then writes a response.
    In this case the agentâ€™s policy becomes dependent on its private observation and
    its interpretation of the collective memory. Experiments were performed with two
    agents in cooperative scenarios. The results highlighted the fact that the communication
    channel was used differently in each environment, e.g., in simpler tasks agents
    significantly decrease their memory activity near the end of the task as there
    are no more changes in the environment; in more complex environments, the changes
    in memory usage appear at a much higher frequency due to the presence of many
    sub-tasks.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: DropoutÂ [[213](#bib.bib213)] is a technique to prevent overfitting (in supervised
    learning this happens when the learning algorithm achieves good performance only
    on a specific data set and fails to generalize) in neural networks which is based
    on randomly dropping units and their connections during training time. Inspired
    by dropout, Kim et al.Â [[177](#bib.bib177)] proposed a similar approach in multiagent
    environments where direct communication through messages is allowed. In this case,
    the messages of other agents are dropped out at training time, thus the authors
    proposed the Message-Dropout MADDPG algorithmÂ [[177](#bib.bib177)]. This method
    is expected to work in fully or limited communication environments. The empirical
    results show that with properly chosen message dropout rate, the proposed method
    both significantly improves the training speed and the robustness of learned policies
    (by introducing communication errors) during execution time. This capability is
    important as MDRL agents trained in simulated or controlled environments will
    be less fragile when transferred to more realistic environments.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'While RIAL and DIAL used a discrete communication channel, CommNetÂ [[163](#bib.bib163)]
    used a continuous vector channel. Through this channel agents receive the summed
    transmissions of other agents. The authors assume full cooperation and train a
    single network for all the agents. There are two distinctive characteristics of
    CommNet from previous works: it allows multiple communication cycles at each timestep
    and a dynamic variation of agents at run time, i.e., agents come and go in the
    environment.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to previous approaches, in Multiagent Bidirectionally Coordinated
    Network (BiCNet)Â [[164](#bib.bib164)], communication takes place in the latent
    space (i.e., in the hidden layers). It also uses parameter sharing, however, it
    proposes bidirectional recurrent neural networksÂ [[214](#bib.bib214)] to model
    the actor and critic networks of their model. Note that in BiCNet agents do not
    *explicitly* share a message and thus it can be considered a method for learning
    cooperation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Learning communication is an active area in MDRL with many open questions, in
    this context, we refer the interested reader to a recent work by Lowe et al.Â [[215](#bib.bib215)]
    where it discusses common pitfalls (and recommendations to avoid those) while
    measuring communication in multiagent environments.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Learning cooperation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although *explicit communication* is a new emerging trend in MDRL, there has
    already been a large amount of work in MAL for cooperative settings^(13)^(13)13There
    is a large body of research on coordinating multiagent teams by specifying communication
    protocolsÂ [[216](#bib.bib216), [217](#bib.bib217)]: these expect agents to know
    the teamâ€™s goal as well as the tasks required to accomplish the goal. that do
    not involve communicationÂ [[7](#bib.bib7), [8](#bib.bib8)]. Therefore, it was
    a natural starting point for many recent MDRL works.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Foerster et al.Â [[168](#bib.bib168)] studied the simple scenario of *cooperation
    with independent Q-learning agents* (see SectionÂ [3.1](#S3.SS1 "3.1 Multiagent
    Learning â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")), where the agents use the standard DQN architecture
    of neural networks and an experience replay buffer (see FigureÂ [3](#S2.F3 "Figure
    3 â€£ Value-based methods â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). However, for the ER to
    work, the data distribution needs to follow certain assumptions (see SectionÂ [2.2](#S2.SS2
    "2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")) which are no loger valid due to the multiagent
    nature of the world: the dynamics that generated the data in the ER no longer
    reflect the current dynamics, making the experience obsoleteÂ [[168](#bib.bib168),
    [90](#bib.bib90)]. Their solution is to add information to the experience tuple
    that can help to *disambiguate the age of the sampled data* from the replay memory.
    Two approaches were proposed. The first is Multiagent Importance Sampling which
    adds the probability of the joint action so an importance sampling correctionÂ [[70](#bib.bib70),
    [218](#bib.bib218)] can computed when the tuple is later sampled for training.
    This was similar to previous works in adaptive importance samplingÂ [[219](#bib.bib219),
    [220](#bib.bib220)] and off-environment RLÂ [[221](#bib.bib221)]. The second approach
    is Multiagent Fingerprints which adds the estimate (i.e., fingerprint) of other
    agentsâ€™ policies (loosely inspired by Hyper-QÂ [[150](#bib.bib150)], see SectionÂ [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). For the practical implementation,
    good results were obtained by using the training iteration number and exploration
    rate as the fingerprint.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Gupta et al.Â [[182](#bib.bib182)] tackled cooperative environments in partially
    observable domains without explicit communication. They proposed *parameter sharing*
    (PS) as a way to improve learning in homogeneous multiagent environments (where
    agents have the same set of actions). The idea is to have one globally shared
    learning network that can still behave differently in execution time, i.e., because
    its inputs (individual agent observation and agent index) will be different. They
    tested three variations of this approach with parameter sharing: PS-DQN, PS-DDPG
    and PS-TRPO, which extended single-agent DQN, DDPG and TRPO algorithms, respectively.
    The results showed that PS-TRPO outperformed the other two. Note that Foerster
    et al.Â [[162](#bib.bib162)] concurrently proposed a similar concept, see SectionÂ [3.4](#S3.SS4
    "3.4 Learning communication â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'Lenient-DQN (LDQN)Â [[35](#bib.bib35)] took the *leniency* conceptÂ [[222](#bib.bib222)]
    (originally presented in MAL) and extended their use to MDRL. The purpose of leniency
    is to overcome a pathology called relative overgeneralizationÂ [[34](#bib.bib34),
    [223](#bib.bib223), [224](#bib.bib224)]. Similar to other approaches designed
    to overcome relative overgeneralization (e.g., distributed Q-learningÂ [[225](#bib.bib225)]
    and hysteretic Q-learningÂ [[8](#bib.bib8)]) lenient learners initially maintain
    an optimistic disposition to mitigate the noise from transitions resulting in
    miscoordination, preventing agents from being drawn towards sub-optimal but wide
    peaks in the reward search spaceÂ [[97](#bib.bib97)]. However, similar to other
    MDRL worksÂ [[168](#bib.bib168)], the LDQN authors experienced problems with the
    ER buffer and arrived at a similar solution: adding information to the experience
    tuple, in their case, the leniency value. When sampling from the ER buffer, this
    value is used to determine a leniency condition; if the condition is not met then
    the sample is ignored.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar vein, Decentralized-Hysteretic Deep Recurrent Q-Networks (DEC-HDRQNs)Â [[166](#bib.bib166)]
    were proposed for fostering cooperation among independent learners. The motivation
    is similar to LDQN, making an optimistic value update, however, their solution
    is different. Here, the authors took inspiration from Hysteretic Q-learningÂ [[8](#bib.bib8)],
    originally presented in MAL, where two learning rates were used. A difference
    between lenient agents and hysteretic Q-learning is that lenient agents are only
    *initially* forgiving towards teammates. Lenient learners over time apply less
    leniency towards updates that would lower utility values, taking into account
    how frequently observation-action pairs have been encountered. The idea being
    that the transition from optimistic to average reward learner will help make lenient
    learners more robust towards misleading stochastic rewardsÂ [[222](#bib.bib222)].
    Additionally, in DEC-HDRQNs the ER buffer is also extended into *concurrent experience
    replay trajectories*, which are composed of three dimensions: agent index, the
    episode, and the timestep; when training, the sampled traces have the same starting
    timesteps. Moreover, to improve on generalization over different tasks, i.e.,
    multi-task learning[[226](#bib.bib226)], DEC-HDRQNs make use of policy distillationÂ [[227](#bib.bib227),
    [228](#bib.bib228)] (see SectionÂ [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). In contrast to other approaches, DEC-HDRQNS are fully decentralized
    during learning and execution.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Weighted Double Deep Q-Network (WDDQN)Â [[178](#bib.bib178)] is based on having
    double estimators. This idea was originally introduced in Double Q-learningÂ [[91](#bib.bib91)]
    and aims to remove the existing overestimation bias caused by using the maximum
    action value as an approximation for the maximum expected action value (see SectionÂ [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). It also uses a *lenient*
    rewardÂ [[222](#bib.bib222)] to be optimistic during initial phase of coordination
    and proposes a *scheduled* replay strategy in which samples closer to the terminal
    states are heuristically given higher priority; this strategy might not be applicable
    for any domain. For other works extending the ER to multiagent settings see MADDPGÂ [[63](#bib.bib63)],
    SectionsÂ [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL â€£
    4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")
    and [4.2](#S4.SS2 "4.2 Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€").'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4d8897e26b6d80a96325b534c94c1ee5.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A schematic view of the architecture used in FTW (For the Win)Â [[179](#bib.bib179)]:
    two unrolled recurrent neural networks (RNNs) operate at different time-scales,
    the idea is that the *Slow RNN* helps with long term temporal correlations. Observations
    are latent space output of some convolutional neural network to learn non-linear
    features. Feudal NetworksÂ [[229](#bib.bib229)] is another work in single-agent
    DRL that also maintains a multi-time scale hierarchy where the slower network
    sets the goal, and the faster network tries to achieve them. Fedual Networks were
    in turn, inspired by early work in RL which proposed a hierarchy of Q-learnersÂ [[230](#bib.bib230),
    [231](#bib.bib231)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'While previous approaches were mostly inspired by how MAL algorithms could
    be extended to MDRL, other works take as base the results by single-agent DRL.
    One example is the For The Win (FTW)Â [[179](#bib.bib179)] agent which is based
    on the actor-learner structure of IMPALAÂ [[114](#bib.bib114)] (see SectionÂ [2.2](#S2.SS2
    "2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). The authors test FTW in a game where two
    opposing teams compete to capture each otherâ€™s flagsÂ [[232](#bib.bib232)]. To
    deal with the MAL problem they propose two main additions: a *hierarchical two-level
    representation* with recurrent neural networks operating at different timescales,
    as depicted in FigureÂ [5](#S3.F5 "Figure 5 â€£ 3.5 Learning cooperation â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€"), and a *population based training*Â [[233](#bib.bib233), [234](#bib.bib234),
    [235](#bib.bib235)] where 30 agents were trained in parallel together with a stochastic
    matchmaking scheme that biases agents to be of similar *skills*. The Elo rating
    systemÂ [[236](#bib.bib236)] was originally devised to rate chess player skills,^(14)^(14)14Elo
    uses a normal distribution for each player skill, and after each match, both playersâ€™
    distributions are updated based on measure of *surprise*, i.e., if a user with
    previously lower (predicted) skill beats a high skilled one, the low-skilled player
    is significantly increased. TrueSkillÂ [[237](#bib.bib237)] extended Elo by tracking
    uncertainty in skill rating, supporting draws, and matches beyond 1 vs 1; <math
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\alpha-" display="inline"><semantics
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"><mi
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml">Î±</mi><mo
    id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.3.cmml">âˆ’</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.1.1.1.1.1.1.m1.1b"><apply id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.1.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1">limit-from</csymbol><ci id="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2.cmml"
    xref="S3.SS5.p7.1.1.1.1.1.1.m1.1.1.2">ğ›¼</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p7.1.1.1.1.1.1.m1.1c">\alpha-</annotation></semantics></math>Rank
    is a more recent alternative to ELOÂ [[238](#bib.bib238)]. FTW did not use TrueSkill
    but a simpler extension of Elo for <math id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1" class="ltx_Math"
    alttext="n" display="inline"><semantics id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1a"><mi
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1" xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1b"><ci id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1.cmml"
    xref="S3.SS5.p7.2.2.2.2.2.2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.2.2.2.2.2.2.1.m1.1c">n</annotation></semantics></math> vs <math
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1a"><mi id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1" xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml">n</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1b"><ci id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1.cmml"
    xref="S3.SS5.p7.3.3.3.3.3.3.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p7.3.3.3.3.3.3.2.m2.1c">n</annotation></semantics></math> games (by
    adding individual agent ratings to compute the team skill). Hierarchical approaches
    were previously proposed in RL, e.g., Feudal RLÂ [[230](#bib.bib230), [231](#bib.bib231)],
    and were later extended to DRL in Feudal networksÂ [[229](#bib.bib229)]; population
    based training can be considered analogous to evolutionary strategies that employ
    self-adaptive hyperparameter tuning to modify how the genetic algorithm itself
    operatesÂ [[234](#bib.bib234), [239](#bib.bib239), [240](#bib.bib240)]. An interesting
    result from FTW is that the population-based training obtained better results
    than training via self-playÂ [[192](#bib.bib192)], which was a standard concept
    in previous worksÂ [[14](#bib.bib14), [193](#bib.bib193)]. FTW used heavy compute
    resources, it used 30 agents (processes) in parallel where every training game
    lasted 4500 agent steps (<math id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1" class="ltx_Math"
    alttext="\approx" display="inline"><semantics id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1a"><mo
    id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1" xref="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1.1.cmml">â‰ˆ</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.4.4.4.4.4.4.3.1.1.m1.1c">\approx</annotation></semantics></math>
    five minutes) and agents were trained for two billion steps (<math id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1"
    class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1a"><mo
    id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1" xref="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1.1.cmml">â‰ˆ</mo><annotation
    encoding="application/x-tex" id="S3.SS5.p7.5.5.5.5.5.5.4.2.2.m2.1c">\approx</annotation></semantics></math>
    450K games).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Lowe et al.Â [[63](#bib.bib63)] noted that using standard policy gradient methods
    (see SectionÂ [2.1](#S2.SS1 "2.1 Reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) on multiagent environments
    yields high variance and performs poorly. This occurs because the variance is
    further increased as all the agentsâ€™ rewards depend on the rest of the agents,
    and it is formally shown that as the number of agents increase, the probability
    of taking a correct gradient direction decreases exponentiallyÂ [[63](#bib.bib63)].
    Therefore, to overcome this issue Lowe et al. proposed the Multi-Agent Deep Deterministic
    Policy Gradient (MADDPG)Â [[63](#bib.bib63)], building on DDPGÂ [[65](#bib.bib65)]
    (see SectionÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")), to train a centralized
    critic per agent that is given all agentsâ€™ policies during training to reduce
    the variance by removing the non-stationarity caused by the concurrently learning
    agents. Here, the actor only has local information (turning the method into a
    centralized training with decentralized execution) and the ER buffer records experiences
    of *all* agents. MADDPG was tested in both cooperative and competitive scenarios,
    experimental results show that it performs better than several decentralized methods
    (such as DQN, DDPG, and TRPO). The authors mention that traditional RL methods
    do not produce consistent gradient signals. This is exemplified in a challenging
    competitive scenarios where agents continuously adapt to each other causing the
    learned best-response policies oscillate â€” for such a domain, MADDPG is shown
    to learn more robustly than DDPG.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach based on policy gradients is the Counterfactual Multi-Agent
    Policy Gradients (COMA)Â [[167](#bib.bib167)]. COMA was designed for the fully
    centralized setting and the *multiagent credit assignment problem*Â [[241](#bib.bib241)],
    i.e., how the agents should deduce their contributions when learning in a cooperative
    setting in the presence of only global rewards. Their proposal is to compute a
    *counterfactual baseline*, that is, marginalize out the action of the agent while
    keeping the rest of the other agentsâ€™ actions fixed. Then, an advantage function
    can be computed comparing the current <math id="S3.SS5.p9.1.1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS5.p9.1.1.1.m1.1a"><mi id="S3.SS5.p9.1.1.1.m1.1.1"
    xref="S3.SS5.p9.1.1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS5.p9.1.1.1.m1.1b"><ci id="S3.SS5.p9.1.1.1.m1.1.1.cmml" xref="S3.SS5.p9.1.1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS5.p9.1.1.1.m1.1c">Q</annotation></semantics></math>
    value to the counterfactual. This counterfactual baseline has its roots in *difference
    rewards*, which is a method for obtaining the individual contribution of an agent
    in a cooperative multiagent teamÂ [[241](#bib.bib241)]. In particular, the *aristocrat*
    utility aims to measure the difference between an agentâ€™s actual action and the
    average actionÂ [[31](#bib.bib31)]. The intention would be equivalent to sideline
    the agent by having the agent perform an action where the reward does not depend
    on the agentâ€™s actions, i.e., to consider the reward that would have arisen assuming
    a world without that agent having ever existed (see SectionÂ [4.2](#S4.SS2 "4.2
    Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'On the one hand, fully centralized approaches (e.g., COMA) do not suffer from
    non-stationarity but have constrained scalability. On the other hand, independent
    learning agents are better suited to scale but suffer from non-stationarity issues.
    There are some hybrid approaches that learn *a centralized but factored* <math
    id="S3.SS5.p10.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS5.p10.1.m1.1a"><mi id="S3.SS5.p10.1.m1.1.1" xref="S3.SS5.p10.1.m1.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS5.p10.1.m1.1b"><ci id="S3.SS5.p10.1.m1.1.1.cmml"
    xref="S3.SS5.p10.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS5.p10.1.m1.1c">Q</annotation></semantics></math> value functionÂ [[242](#bib.bib242),
    [243](#bib.bib243)]. Value Decomposition Networks (VDNs)Â [[180](#bib.bib180)]
    decompose a team value function into an additive decomposition of the individual
    value functions. Similarly, QMIXÂ [[181](#bib.bib181)] relies on the idea of factorizing,
    however, instead of sum, QMIX assumes a *mixing network* that combines the local
    values in a non-linear way, which can represent monotonic action-value functions.
    While the mentioned approaches have obtained good empirical results, the factorization
    of value-functions in multiagent scenarios using function approximators (MDRL)
    is an ongoing research topic, with open questions such as how well factorizations
    capture complex coordination problems and how to learn those factorizationsÂ [[244](#bib.bib244)]
    (see SectionÂ [4.4](#S4.SS4 "4.4 Practical challenges in MDRL â€£ 4 Bridging RL,
    MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Agents modeling agents
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c33d4d724302e5f821fb15a7441cfe4d.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: (a) Deep Policy Inference Q-Network: receives four stacked frames
    as input (similar to DQN, see FigureÂ [2](#S2.F2 "Figure 2 â€£ Value-based methods
    â€£ 2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). (b) Deep Policy Inference Recurrent Q-Network:
    receives one frame as input and has an LSTM layer instead of a fully connected
    layer (FC). Both approachesÂ [[171](#bib.bib171)] condition the <math id="S3.F6.4.1.m1.1"
    class="ltx_Math" alttext="Q_{M}" display="inline"><semantics id="S3.F6.4.1.m1.1b"><msub
    id="S3.F6.4.1.m1.1.1" xref="S3.F6.4.1.m1.1.1.cmml"><mi id="S3.F6.4.1.m1.1.1.2"
    xref="S3.F6.4.1.m1.1.1.2.cmml">Q</mi><mi id="S3.F6.4.1.m1.1.1.3" xref="S3.F6.4.1.m1.1.1.3.cmml">M</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.4.1.m1.1c"><apply id="S3.F6.4.1.m1.1.1.cmml"
    xref="S3.F6.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F6.4.1.m1.1.1.1.cmml" xref="S3.F6.4.1.m1.1.1">subscript</csymbol><ci
    id="S3.F6.4.1.m1.1.1.2.cmml" xref="S3.F6.4.1.m1.1.1.2">ğ‘„</ci><ci id="S3.F6.4.1.m1.1.1.3.cmml"
    xref="S3.F6.4.1.m1.1.1.3">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.4.1.m1.1d">Q_{M}</annotation></semantics></math> value outputs on the
    policy features, <math id="S3.F6.5.2.m2.1" class="ltx_Math" alttext="h^{PI}" display="inline"><semantics
    id="S3.F6.5.2.m2.1b"><msup id="S3.F6.5.2.m2.1.1" xref="S3.F6.5.2.m2.1.1.cmml"><mi
    id="S3.F6.5.2.m2.1.1.2" xref="S3.F6.5.2.m2.1.1.2.cmml">h</mi><mrow id="S3.F6.5.2.m2.1.1.3"
    xref="S3.F6.5.2.m2.1.1.3.cmml"><mi id="S3.F6.5.2.m2.1.1.3.2" xref="S3.F6.5.2.m2.1.1.3.2.cmml">P</mi><mo
    lspace="0em" rspace="0em" id="S3.F6.5.2.m2.1.1.3.1" xref="S3.F6.5.2.m2.1.1.3.1.cmml">â€‹</mo><mi
    id="S3.F6.5.2.m2.1.1.3.3" xref="S3.F6.5.2.m2.1.1.3.3.cmml">I</mi></mrow></msup><annotation-xml
    encoding="MathML-Content" id="S3.F6.5.2.m2.1c"><apply id="S3.F6.5.2.m2.1.1.cmml"
    xref="S3.F6.5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F6.5.2.m2.1.1.1.cmml" xref="S3.F6.5.2.m2.1.1">superscript</csymbol><ci
    id="S3.F6.5.2.m2.1.1.2.cmml" xref="S3.F6.5.2.m2.1.1.2">â„</ci><apply id="S3.F6.5.2.m2.1.1.3.cmml"
    xref="S3.F6.5.2.m2.1.1.3"><ci id="S3.F6.5.2.m2.1.1.3.2.cmml" xref="S3.F6.5.2.m2.1.1.3.2">ğ‘ƒ</ci><ci
    id="S3.F6.5.2.m2.1.1.3.3.cmml" xref="S3.F6.5.2.m2.1.1.3.3">ğ¼</ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.F6.5.2.m2.1d">h^{PI}</annotation></semantics></math>,
    which are also used to learn the opponent policy <math id="S3.F6.6.3.m3.1" class="ltx_Math"
    alttext="\pi_{o}" display="inline"><semantics id="S3.F6.6.3.m3.1b"><msub id="S3.F6.6.3.m3.1.1"
    xref="S3.F6.6.3.m3.1.1.cmml"><mi id="S3.F6.6.3.m3.1.1.2" xref="S3.F6.6.3.m3.1.1.2.cmml">Ï€</mi><mi
    id="S3.F6.6.3.m3.1.1.3" xref="S3.F6.6.3.m3.1.1.3.cmml">o</mi></msub><annotation-xml
    encoding="MathML-Content" id="S3.F6.6.3.m3.1c"><apply id="S3.F6.6.3.m3.1.1.cmml"
    xref="S3.F6.6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F6.6.3.m3.1.1.1.cmml" xref="S3.F6.6.3.m3.1.1">subscript</csymbol><ci
    id="S3.F6.6.3.m3.1.1.2.cmml" xref="S3.F6.6.3.m3.1.1.2">ğœ‹</ci><ci id="S3.F6.6.3.m3.1.1.3.cmml"
    xref="S3.F6.6.3.m3.1.1.3">ğ‘œ</ci></apply></annotation-xml><annotation encoding="application/x-tex"
    id="S3.F6.6.3.m3.1d">\pi_{o}</annotation></semantics></math>.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'An important ability for agents to have is to reason about the behaviors of
    other agents by constructing models that make predictions about the modeled agentsÂ [[11](#bib.bib11)].
    An early work for modeling agents while using deep neural networks was the Deep
    Reinforcement Opponent Network (DRON)Â [[169](#bib.bib169)]. The idea is to have
    two networks: one which evaluates <math id="S3.SS6.p1.1.m1.1" class="ltx_Math"
    alttext="Q" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1"
    xref="S3.SS6.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">Q</annotation></semantics></math>-values
    and a second one that *learns a representation of the opponentâ€™s policy*. Moreover,
    the authors proposed to have several expert networks to combine their predictions
    to get the estimated <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="Q"
    display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1"
    xref="S3.SS6.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">Q</annotation></semantics></math>
    value, the idea being that each expert network captures one type of opponent strategyÂ [[245](#bib.bib245)].
    This is related to previous works in type-based reasoning from game theoryÂ [[246](#bib.bib246),
    [139](#bib.bib139)] later applied in AIÂ [[245](#bib.bib245), [11](#bib.bib11),
    [247](#bib.bib247)]. The mixture of experts idea was presented in supervised learning
    where each expert handled a subset of the data (a subtask), and then a gating
    network decided which of the experts should be usedÂ [[248](#bib.bib248)].'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'DRON uses hand-crafted features to define the opponent network. In contrast,
    Deep Policy Inference Q-Network (DPIQN) and its recurrent version, DPIRQNÂ [[171](#bib.bib171)]
    learn *policy features* directly from raw observations of the other agents. The
    way to learn these policy features is by means of auxiliary tasksÂ [[84](#bib.bib84),
    [110](#bib.bib110)] (see SectionsÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning
    â€£ 2 Single-agent learning â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")
    and [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) that provide additional
    learning goals, in this case, the auxiliary task is to learn the opponentsâ€™ policies.
    This auxiliary task modifies the loss function by computing an auxiliary loss:
    the cross entropy loss between the inferred opponent policy and the ground truth
    (one-hot action vector) of the opponent. Then, the <math id="S3.SS6.p2.1.m1.1"
    class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mi
    id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content"
    id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">Q</annotation></semantics></math>
    value function of the learning agent is conditioned on the opponentâ€™s policy features
    (see FigureÂ [6](#S3.F6 "Figure 6 â€£ 3.6 Agents modeling agents â€£ 3 Multiagent Deep
    Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")),
    which aims to reduce the non-stationarity of the environment. The authors used
    an adaptive training procedure to adjust the attention (a weight on the loss function)
    to either emphasize learning the policy features (of the opponent) or the respective
    <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml"
    xref="S3.SS6.p2.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p2.2.m2.1c">Q</annotation></semantics></math> values of the agent.
    An advantage of these approaches is that modeling the agents can work for both
    opponents and teammatesÂ [[171](#bib.bib171)].'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: In many previous works an opponent model is learned from observations. Self
    Other Modeling (SOM)Â [[170](#bib.bib170)] proposed a different approach, this
    is, using *the agentâ€™s own policy as a means to predict the opponentâ€™s actions*.
    SOM can be used in cooperative and competitive settings (with an arbitrary number
    of agents) and infers other agentsâ€™ goals. This is important because in the evaluated
    domains, the reward function depends on the goal of the agents. SOM uses two networks,
    one used for computing the agentsâ€™ own policy, and a second one used to infer
    the opponentâ€™s goal. The idea is that these networks have the same input parameters
    but with different values (the agentâ€™s or the opponentâ€™s). In contrast to previous
    approaches, SOM is not focused on learning the opponent policy, i.e., a probability
    distribution over next actions, but rather on estimating the opponentâ€™s goal.
    SOM is expected to work best when agents share a set of goals from which each
    agent gets assigned one at the beginning of the episode and the reward structure
    depends on both of their assigned goals. Despite its simplicity, training takes
    longer as an additional optimization step is performed given the other agentâ€™s
    observed actions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a long-standing history of combining game theory and MALÂ [[2](#bib.bib2),
    [6](#bib.bib6), [193](#bib.bib193)]. From that context, some approaches were inspired
    by influential game theory approaches. Neural Fictitious Self-Play (NFSP)Â [[173](#bib.bib173)]
    builds on fictitious (self-) playÂ [[190](#bib.bib190), [249](#bib.bib249)], together
    with two deep networks to find *approximate Nash equilibria*^(15)^(15)15Nash equilibriumÂ [[250](#bib.bib250)]
    is a solution concept in game theory in which no agent would choose to deviate
    from its strategy (they are a best response to othersâ€™ strategies). This concept
    has been explored in seminal MAL algorithms like Nash-Q learningÂ [[127](#bib.bib127)]
    and Minimax-Q learningÂ [[124](#bib.bib124), [128](#bib.bib128)]. in two-player
    imperfect information gamesÂ [[251](#bib.bib251)] (for example, consider Poker:
    when it is an agentâ€™s turn to move it does not have access to all information
    about the world). One network learns an *approximate best response* (<math id="S3.SS6.p4.1.m1.1"
    class="ltx_Math" alttext="\epsilon-" display="inline"><semantics id="S3.SS6.p4.1.m1.1a"><mrow
    id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml"><mi id="S3.SS6.p4.1.m1.1.1.2"
    xref="S3.SS6.p4.1.m1.1.1.2.cmml">Ïµ</mi><mo id="S3.SS6.p4.1.m1.1.1.3" xref="S3.SS6.p4.1.m1.1.1.3.cmml">âˆ’</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><apply id="S3.SS6.p4.1.m1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS6.p4.1.m1.1.1.1.cmml"
    xref="S3.SS6.p4.1.m1.1.1">limit-from</csymbol><ci id="S3.SS6.p4.1.m1.1.1.2.cmml"
    xref="S3.SS6.p4.1.m1.1.1.2">italic-Ïµ</ci></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">\epsilon-</annotation></semantics></math>greedy
    over <math id="S3.SS6.p4.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics
    id="S3.SS6.p4.2.m2.1a"><mi id="S3.SS6.p4.2.m2.1.1" xref="S3.SS6.p4.2.m2.1.1.cmml">Q</mi><annotation-xml
    encoding="MathML-Content" id="S3.SS6.p4.2.m2.1b"><ci id="S3.SS6.p4.2.m2.1.1.cmml"
    xref="S3.SS6.p4.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex"
    id="S3.SS6.p4.2.m2.1c">Q</annotation></semantics></math> values) to the historical
    behavior of other agents and the second one (called the average network) learns
    to imitate its own past best response behaviour using supervised classification.
    The agent behaves using a mixture of the average and the best response networks
    depending on the probability of an anticipatory parameterÂ [[252](#bib.bib252)].
    Comparisons with DQN in Leduc Holdâ€™em Poker revealed that DQNâ€™s deterministic
    strategy is highly exploitable. Such strategies are sufficient to behave optimally
    in single-agent domains, i.e., MDPs for which DQN was designed. However, imperfect-information
    games generally require stochastic strategies to achieve optimal behaviourÂ [[173](#bib.bib173)].
    DQN learning experiences are both highly correlated over time, and highly focused
    on a narrow state distribution. In contrast to NFSP agents whose experience varies
    more smoothly, resulting in a more stable data distribution, more stable neural
    networks and better performance.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The (N)FSP concept was further generalized in Policy-Space Response Oracles
    (PSRO)Â [[172](#bib.bib172)], where it was shown that fictitious play is one specific
    meta-strategy distribution over a set of previous (approximate) best responses
    (summarized by a meta-game obtained by empirical game theoretic analysisÂ [[253](#bib.bib253)]),
    but there are a wide variety to choose from. One reason to use mixed meta-strategies
    is that it prevents overfitting^(16)^(16)16Johanson et al.Â [[254](#bib.bib254)]
    also found â€œoverfittingâ€ when solving large extensive games (e.g., poker) â€” the
    performance in an abstract game improved but it was worse in the full game. the
    responses to one specific policy, and hence provides a form of opponent/teammate
    regularization. An approximate scalable version of the algorithm leads to a graph
    of agents best-responding independently called Deep Cognitive Hierarchies (DCHs)Â [[172](#bib.bib172)]
    due to its similarity to behavioral game-theoretic modelsÂ [[255](#bib.bib255),
    [256](#bib.bib256)].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimax is a paramount concept in game theory that is roughly described as
    minimizing the worst case scenario (maximum loss)Â [[251](#bib.bib251)]. Li et
    al.Â [[183](#bib.bib183)] took the minimax idea as an approach to robustify learning
    in multiagent environments so that the learned robust policy should be able to
    behave well even with strategies not seen during training. They extended the MADDPG
    algorithmÂ [[63](#bib.bib63)] to Minimax Multiagent Deep Deterministic Policy Gradients
    (M3DDPG), which updates policies considering a worst-case scenario: assuming that
    all other agents act adversarially. This yields a minimax learning objective which
    is computationally intractable to directly optimize. They address this issue by
    taking ideas from robust reinforcement learningÂ [[257](#bib.bib257)] which implicitly
    adopts the minimax idea by using the *worst noise* conceptÂ [[258](#bib.bib258)].
    In MAL different approaches were proposed to assess the robustness of an algorithm,
    e.g., guarantees of safetyÂ [[152](#bib.bib152), [259](#bib.bib259)], securityÂ [[260](#bib.bib260)]
    or exploitabilityÂ [[261](#bib.bib261), [262](#bib.bib262), [263](#bib.bib263)].'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Previous approaches usually learned a model of the other agents as a way to
    predict their behavior. However, they do not explicitly *account for anticipated
    learning of the other agents*, which is the objective of Learning with Opponent-Learning
    Awareness (LOLA)Â [[64](#bib.bib64)]. LOLA optimizes the expected return *after
    the opponent updates its policy one step*. Therefore, a LOLA agent directly shapes
    the policy updates of other agents to maximize its own reward. One of LOLAâ€™s assumptions
    is having access to opponentsâ€™ policy parameters. LOLA builds on previous ideas
    by Zhang and LesserÂ [[264](#bib.bib264)] where the learning agent predicts the
    opponentâ€™s policy parameter update but only uses it to learn a best response (to
    the anticipated updated parameters).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Theory of mind is part of a group of *recursive reasoning* approaches[[265](#bib.bib265),
    [245](#bib.bib245), [266](#bib.bib266), [267](#bib.bib267)] in which agents have
    explicit beliefs about the mental states of other agents. The mental states of
    other agents may, in turn, also contain beliefs and mental states of other agents,
    leading to a nesting of beliefsÂ [[11](#bib.bib11)]. Theory of Mind Network (ToMnet)Â [[174](#bib.bib174)]
    starts with a simple premise: when encountering a novel opponent, *the agent should
    already have a strong and rich prior about how the opponent should behave*. ToMnet
    has an architecture composed of three networks: (i) a character network that learns
    from historical information, (ii) a mental state network that takes the character
    output and the recent trajectory, and (iii) the prediction network that takes
    the current state as well as the outputs of the other networks as its input. The
    output of the architecture is open for different problems but in general its goal
    is to predict the opponentâ€™s next action. A main advantage of ToMnet is that it
    can predict general behavior, for all agents; or specific, for a particular agent.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Deep Bayesian Theory of Mind Policy (Bayes-ToMoP)Â [[175](#bib.bib175)] is another
    algorithm that takes inspiration from theory of mindÂ [[268](#bib.bib268)]. The
    algorithm assumes the opponent has different stationary strategies to act and
    changes among them over timeÂ [[269](#bib.bib269)]. Earlier work in MAL dealt with
    this setting, e.g., BPR+Â [[270](#bib.bib270)] extends the Bayesian policy reuse^(17)^(17)17Bayesian
    policy reuse assumes an agent with prior experience in the form of a library of
    policies. When a novel task instance occurs, the objective is to reuse a policy
    from its library based on observed signals which correlate to policy performanceÂ [[271](#bib.bib271)].
    frameworkÂ [[271](#bib.bib271)] to multiagent settings (BPR assumes a single-agent
    environment; BPR+ aims to best respond to the opponent in a multiagent game).
    A limitation of BPR+ is that it behaves poorly against itself (self-play), thus,
    Deep Bayes-ToMoP uses theory of mind to provide a higher-level reasoning strategy
    which provides an optimal behavior against BPR+ agents.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Deep BPR+Â [[184](#bib.bib184)] is another work inspired by BPR+ which uses neural
    networks as value-function approximators. It not only uses the environment reward
    but also uses the online learned opponent modelÂ [[272](#bib.bib272), [273](#bib.bib273)]
    to construct a rectified belief over the opponent strategy. Additionally, it leverages
    ideas from policy distillationÂ [[227](#bib.bib227), [228](#bib.bib228)] and extends
    them to the multiagent case to create a distilled policy network. In this case,
    whenever a new acting policy is learned, distillation is applied to consolidate
    the new updated library which improves in terms of storage and generalization
    (over opponents).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 4 Bridging RL, MAL and MDRL
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section aims to provide directions to promote fruitful cooperations between
    sub-communities. First, we address the pitfall of *deep learning amnesia*, roughly
    described as missing citations to the original works and not exploiting the advancements
    that have been made in the past. We present examples on how ideas originated earlier,
    for example in RL and MAL, were successfully extended to MDRL (see SectionÂ [4.1](#S4.SS1
    "4.1 Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and
    MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Second, we outline *lessons
    learned* from the works analyzed in this survey (see SectionÂ [4.2](#S4.SS2 "4.2
    Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). Then we point the readers to recent benchmarks for MDRL (see
    SectionÂ [4.3](#S4.SS3 "4.3 Benchmarks for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) and we discuss the practical
    challenges that arise in MDRL like high computational demands and reproducibility
    (see SectionÂ [4.4](#S4.SS4 "4.4 Practical challenges in MDRL â€£ 4 Bridging RL,
    MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Lastly, we pose some
    open research challenges and reflect on their relation with previous open questions
    in MALÂ [[11](#bib.bib11)] (see SectionÂ [4.5](#S4.SS5 "4.5 Open questions â€£ 4 Bridging
    RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Avoiding deep learning amnesia: examples in MDRL'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This survey focuses on recent *deep* works, however, in previous sections, when
    describing recent algorithms, we also point to original works that inspired them.
    Schmidhuber said â€œMachine learning is the science of credit assignment. The machine
    learning community itself profits from proper credit assignment to its membersâ€Â [[274](#bib.bib274)].
    In this context, we want to avoid committing the pitfall of not giving credit
    to original ideas that were proposed earlier, a.k.a. *deep learning amnesia*.
    Here, we provide some specific examples of research milestones that were studied
    earlier, e.g., RL or MAL, and that now became highly relevant for MDRL. Our purpose
    is to highlight that existent literature contains pertinent ideas and algorithms
    that should not be ignored. On the contrary, they should be examined and citedÂ [[275](#bib.bib275),
    [276](#bib.bib276)] to understand recent developmentsÂ [[277](#bib.bib277)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with non-stationarity in independent learners
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is well known that using independent learners makes the environment non-stationary
    from the agentâ€™s point of viewÂ [[4](#bib.bib4), [123](#bib.bib123)]. Many MAL
    algorithms tried to solve this problem in different waysÂ [[10](#bib.bib10)]. One
    example is *Hyper-Q*Â [[150](#bib.bib150)] which accounts for the (values of mixed)
    strategies of other agents and includes that information in the state representation,
    which effectively turns the learning problem into a stationary one. Note that
    in this way it is possible to even consider adaptive agents. Foerster et al.Â [[162](#bib.bib162)]
    make use of this insight to propose their *fingerprint* algorithm in an MDRL problem
    (see SectionÂ [3.5](#S3.SS5 "3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement
    Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")). Other examples include
    the leniency conceptÂ [[222](#bib.bib222)] and Hysteretic Q-learningÂ [[8](#bib.bib8)]
    originally presented in MAL, which now have their â€œdeepâ€ counterparts, LDQNsÂ [[35](#bib.bib35)]
    and DEC-HDRQNs[[166](#bib.bib166)], see SectionÂ [3.5](#S3.SS5 "3.5 Learning cooperation
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Multiagent credit assignment
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In cooperative multiagent scenarios, it is common to use either *local rewards*,
    unique for each agent, or *global rewards*, which represent the entire groupâ€™s
    performanceÂ [[278](#bib.bib278)]. However, local rewards are usually harder to
    obtain, therefore, it is common to rely only on the global ones. This raises the
    problem of *credit assignment*: how does a single agentâ€™s actions contribute to
    a system that involves the actions of many agentsÂ [[32](#bib.bib32)]. A solution
    that came from MAL research that has proven successful in many scenarios is *difference
    rewards*Â [[241](#bib.bib241), [278](#bib.bib278), [279](#bib.bib279)], which aims
    to capture an agentâ€™s contribution to the systemâ€™s global performance. In particular
    the *aristocrat* utility aims to measure the difference between an agentâ€™s actual
    action and the average actionÂ [[31](#bib.bib31)], however, it has a self-consistency
    problem and in practice it is more common to compute the *wonderful life utility*Â [[280](#bib.bib280),
    [31](#bib.bib31)], which proposes to use a clamping operation that would be equivalent
    to removing that player from the team. COMAÂ [[167](#bib.bib167)] builds on these
    concepts to propose an *advantage function* based on the contribution of the agent,
    which can be efficiently computed with deep neural networks (see SectionÂ [3.5](#S3.SS5
    "3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£
    A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Multitask learning
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the context of RL, multitask learningÂ [[226](#bib.bib226)] is an area that
    develops agents that can act in *several related tasks* rather than just in a
    single oneÂ [[281](#bib.bib281)]. *Distillation*, roughly defined as transferring
    the knowledge from a large model to a small model, was a concept originally introduced
    for supervised learning and model compressionÂ [[282](#bib.bib282), [228](#bib.bib228)].
    Inspired by those works, Policy distillationÂ [[227](#bib.bib227)] was extended
    to the DRL realm. Policy distillation was used to train a much smaller network
    and to merge *several task-specific policies* into a single policy, i.e., for
    multitask learning. In the MDRL setting, Omidshafiei et al.Â [[166](#bib.bib166)]
    successfully adapted policy distillation within Dec-HDRQNs to obtain a more general
    multitask multiagent network (see SectionÂ [3.5](#S3.SS5 "3.5 Learning cooperation
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). Another example is Deep BPR+Â [[184](#bib.bib184)] which uses
    distillation to generalize over multiple opponents (see SectionÂ [3.6](#S3.SS6
    "3.6 Agents modeling agents â€£ 3 Multiagent Deep Reinforcement Learning (MDRL)
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary tasks
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Jaderberg et al.Â [[84](#bib.bib84)] introduced the term auxiliary task with
    the insight that (single-agent) environments contain a variety of possible training
    signals (e.g., pixel changes). These tasks are naturally implemented in DRL in
    which the last layer is split into multiple parts (heads), each working on a different
    task. All heads propagate errors into the same shared preceding part of the network,
    which would then try to form representations, in its next-to-last layer, to support
    all the headsÂ [[20](#bib.bib20)]. However, the idea of multiple predictions about
    arbitrary signals was originally suggested for RL, in the context of general value
    functionsÂ [[110](#bib.bib110), [20](#bib.bib20)] and there still open problems,
    for example, better theoretical understandingÂ [[109](#bib.bib109), [283](#bib.bib283)].
    In the context of neural networks, early work proposed *hints* that improved the
    network performance and learning time. Suddarth and KergosienÂ [[284](#bib.bib284)]
    presented a minimal example of a small neural network where it was shown that
    adding an auxiliary task effectively removed local minima. One could think of
    extending these auxiliary tasks to modeling other agentsâ€™ behaviorsÂ [[285](#bib.bib285),
    [160](#bib.bib160)], which is one of the key ideas that DPIQN and DRPIQNÂ [[171](#bib.bib171)]
    proposed in MDRL settings (see SectionÂ [3.6](#S3.SS6 "3.6 Agents modeling agents
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LinÂ [[90](#bib.bib90), [89](#bib.bib89)] proposed the concept of experience
    replay to speed up the credit assignment propagation process in single agent RL.
    This concept became central to many DRL worksÂ [[72](#bib.bib72)] (see SectionÂ [2.2](#S2.SS2
    "2.2 Deep reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). However, Lin stated that a condition for
    the ER to be useful is that â€œthe environment should not change over time because
    this makes past experiences irrelevant or even harmfulâ€Â [[90](#bib.bib90)]. This
    is a problem in domains where many agents are learning since the environment becomes
    non-stationary from the point of view of each agent. Since DRL relies heavily
    on experience replay, this is an issue in MDRL: the non-stationarity introduced
    means that the dynamics that generated the data in the agentâ€™s replay memory no
    longer reflect the current dynamics in which it is learningÂ [[162](#bib.bib162)].
    To overcome this problem different methods have been proposedÂ [[168](#bib.bib168),
    [35](#bib.bib35), [166](#bib.bib166), [178](#bib.bib178)], see SectionÂ [4.2](#S4.SS2
    "4.2 Lessons learned â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of
    Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of
    this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€").'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Double estimators
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Double Q-learningÂ [[91](#bib.bib91)] proposed to reduce the overestimation
    of action values in Q-learning, this is caused by using the maximum action value
    as an approximation for the maximum expected action value. Double Q-learning works
    by keeping two Q functions and was proven to convergence to the optimal policyÂ [[91](#bib.bib91)].
    Later this idea was applied to arbitrary function approximators, including deep
    neural networks, i.e., Double DQNÂ [[92](#bib.bib92)], which were naturally applied
    since two networks were already used in DQN (see Section [2.2](#S2.SS2 "2.2 Deep
    reinforcement learning â€£ 2 Single-agent learning â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€")). These ideas have also been recently applied to MDRLÂ [[178](#bib.bib178)].'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Lessons learned
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have exemplified how RL and MAL can be extended for MDRL settings. Now, we
    outline general *best practices* learned from the works analyzed throughout this
    paper.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Experience replay buffer in MDRL. While some works removed the ER buffer in
    MDRLÂ [[162](#bib.bib162)] it is an important component in many DRL and MDRL algorithms.
    However, using the standard buffer (i.e., keeping <math id="S4.I1.i1.p1.1.m1.4"
    class="ltx_Math" alttext="\langle s,a,r,s^{\prime}\rangle" display="inline"><semantics
    id="S4.I1.i1.p1.1.m1.4a"><mrow id="S4.I1.i1.p1.1.m1.4.4.1" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml"><mo
    stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.2" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">âŸ¨</mo><mi
    id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.2.2" xref="S4.I1.i1.p1.1.m1.2.2.cmml">a</mi><mo
    id="S4.I1.i1.p1.1.m1.4.4.1.4" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><mi id="S4.I1.i1.p1.1.m1.3.3"
    xref="S4.I1.i1.p1.1.m1.3.3.cmml">r</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.5" xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">,</mo><msup
    id="S4.I1.i1.p1.1.m1.4.4.1.1" xref="S4.I1.i1.p1.1.m1.4.4.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.4.4.1.1.2"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml">s</mi><mo id="S4.I1.i1.p1.1.m1.4.4.1.1.3"
    xref="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S4.I1.i1.p1.1.m1.4.4.1.6"
    xref="S4.I1.i1.p1.1.m1.4.4.2.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S4.I1.i1.p1.1.m1.4b"><list id="S4.I1.i1.p1.1.m1.4.4.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1"><ci
    id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">ğ‘ </ci><ci id="S4.I1.i1.p1.1.m1.2.2.cmml"
    xref="S4.I1.i1.p1.1.m1.2.2">ğ‘</ci><ci id="S4.I1.i1.p1.1.m1.3.3.cmml" xref="S4.I1.i1.p1.1.m1.3.3">ğ‘Ÿ</ci><apply
    id="S4.I1.i1.p1.1.m1.4.4.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous"
    id="S4.I1.i1.p1.1.m1.4.4.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1">superscript</csymbol><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.2">ğ‘ </ci><ci
    id="S4.I1.i1.p1.1.m1.4.4.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.4.4.1.1.3">â€²</ci></apply></list></annotation-xml><annotation
    encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.4c">\langle s,a,r,s^{\prime}\rangle</annotation></semantics></math>)
    will probably fail due to a lack of theoretical guarantees under this setting,
    see SectionsÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning â€£ 2 Single-agent learning
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") and [4.1](#S4.SS1 "4.1
    Avoiding deep learning amnesia: examples in MDRL â€£ 4 Bridging RL, MAL and MDRL
    â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote
    1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€"). *Adding information in
    the experience tuple* that can help disambiguate the sample is the solution adopted
    in many works, whether a value based methodÂ [[168](#bib.bib168), [35](#bib.bib35),
    [166](#bib.bib166), [178](#bib.bib178)] or a policy gradient methodÂ [[63](#bib.bib63)].
    In this regard, it is an open question to consider how new DRL ideas could be
    best integrated into the ERÂ [[286](#bib.bib286), [111](#bib.bib111), [287](#bib.bib287),
    [288](#bib.bib288), [96](#bib.bib96)] and how those ideas would fare in a MDRL
    setting.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Centralized learning with decentralized execution. Many MAL works were either
    fully centralized or fully decentralized approaches. However, inspired by *decentralized
    partially observable Markov decison processes* (DEC-POMDPs)Â [[289](#bib.bib289),
    [290](#bib.bib290)],^(18)^(18)18Centralized planning and decentralized execution
    is also a standard paradigm for multiagent planningÂ [[291](#bib.bib291)]. in MDRL
    this new mixed paradigm has been commonly used Â [[168](#bib.bib168), [35](#bib.bib35),
    [181](#bib.bib181), [172](#bib.bib172), [167](#bib.bib167), [63](#bib.bib63)]
    (a notable exception are DEC-HDRQNsÂ [[166](#bib.bib166)] which perform learning
    and execution in a decentralized manner, see SectionÂ [3.5](#S3.SS5 "3.5 Learning
    cooperation â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). Note that not all real-world problems fit
    into this paradigm and it is more common for robotics or games where a simulator
    is generally availableÂ [[162](#bib.bib162)]. The main benefit is that during learning
    *additional information can be used* (e.g., global state, action, or rewards)
    and during execution this information is removed.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter sharing. Another frequent component in many MDRL works is the idea
    of sharing parameters, i.e., training a single network in which agents share their
    weights. Note that, since agents could receive different observations (e.g., in
    partially observable scenarios), they can still behave differently. This method
    was proposed concurrently in different worksÂ [[292](#bib.bib292), [162](#bib.bib162)]
    and later it has been successfully applied in many othersÂ [[163](#bib.bib163),
    [164](#bib.bib164), [168](#bib.bib168), [180](#bib.bib180), [181](#bib.bib181)].
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recurrent networks. Recurrent neural networks (RNNs) enhanced neural networks
    with a memory capability, however, they suffer from the vanishing gradient problem,
    which renders them inefficient for long-term dependenciesÂ [[293](#bib.bib293)].
    However, RNN variants such as LSTMsÂ [[86](#bib.bib86), [294](#bib.bib294)] and
    GRUs (Gated Recurrent Unit)Â [[295](#bib.bib295)] addressed this challenge. In
    single-agent DRL, DRQNÂ [[85](#bib.bib85)] initially proposed idea of using recurrent
    networks in single-agent *partially observable* environments. Then, Feudal NetworksÂ [[229](#bib.bib229)]
    proposed a hierarchical approachÂ [[230](#bib.bib230)], *multiple LSTM networks
    with different time-scales*, i.e., the observation input schedule is different
    for each LSTM network, to create a temporal hierarchy so that it can better address
    the long-term credit assignment challenge for RL problems. Recently, the use of
    recurrent networks has been extended to MDRL to address the challenge of partially
    observabilityÂ [[158](#bib.bib158), [162](#bib.bib162), [164](#bib.bib164), [166](#bib.bib166),
    [180](#bib.bib180), [181](#bib.bib181), [170](#bib.bib170), [171](#bib.bib171),
    [174](#bib.bib174)] for example, in FTWÂ [[179](#bib.bib179)], depicted in FigureÂ [5](#S3.F5
    "Figure 5 â€£ 3.5 Learning cooperation â€£ 3 Multiagent Deep Reinforcement Learning
    (MDRL) â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€") and DRPIRQNÂ [[171](#bib.bib171)]
    depicted in FigureÂ [6](#S3.F6 "Figure 6 â€£ 3.6 Agents modeling agents â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€"). See SectionÂ [4.4](#S4.SS4 "4.4 Practical challenges in MDRL
    â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€")
    for practical challenges (e.g., training issues) of recurrent networks in MDRL.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting in MAL. In single-agent RL, agents can overfit to the environmentÂ [[296](#bib.bib296)].
    A similar problem can occur in multiagent settingsÂ [[254](#bib.bib254)], agents
    can overfit, i.e., an agentâ€™s policy can easily get stuck in a local optima and
    the learned policy may be only locally optimal to other agentsâ€™ current policiesÂ [[183](#bib.bib183)].
    This has the effect of limiting the generalization of the learned policiesÂ [[172](#bib.bib172)].
    To reduce this problem, a solution is to have a set of policies (an ensemble)
    and learn from them or best respond to the mixture of themÂ [[172](#bib.bib172),
    [63](#bib.bib63), [169](#bib.bib169)]. Another solution has been to robustify
    algorithms â€” a robust policy should be able to behave well even with strategies
    different from its training (better generalization)Â [[183](#bib.bib183)].
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Benchmarks for MDRL
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standardized environments such as the Arcade Learning Environment (ALE)Â [[297](#bib.bib297),
    [298](#bib.bib298)] and OpenAI GymÂ [[299](#bib.bib299)] have allowed single-agent
    RL to move beyond toy domains. For DRL there are open-source frameworks that provide
    compact and reliable implementations of some state-of-the-art DRL algorithmsÂ [[300](#bib.bib300)].
    Even though MDRL is a recent area, there are now a number of open sourced simulators
    and benchmarks to use with different characteristics, which we describe below.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/91df0d0e84687b786d6dc17105cf6138.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: (a) Multiagent object transportation
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d77570179eaad9ecdf463801adbe644.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: (b) Pommerman
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: (a) A fully cooperative benchmark with two agents, Multiagent Object
    Trasportation. (b) A mixed cooperative-competitive domain with four agents, Pommerman.
    For more MDRL benchmarks see SectionÂ [4.3](#S4.SS3 "4.3 Benchmarks for MDRL â€£
    4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep Reinforcement
    Learning1footnote 11footnote 1Earlier versions of this work had the title: â€œIs
    multiagent deep reinforcement learning the answer or the question? A brief surveyâ€").'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fully Cooperative Multiagent Object Transporation Problems (CMOTPs)^(19)^(19)19[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    were originally presented by Busoniu et al.Â [[36](#bib.bib36)] as a simple two-agent
    coordination problem in MAL. Palmer et al.Â [[35](#bib.bib35)] proposed two pixel-based
    extensions to the original setting which include narrow passages that test the
    agentsâ€™ ability to master fully-cooperative sub-tasks, stochastic rewards and
    noisy observations, see FigureÂ [7(a)](#S4.F7.sf1 "In Figure 7 â€£ 4.3 Benchmarks
    for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€").'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Apprentice Firemen Game^(20)^(20)20[https://github.com/gjp1203/nui_in_madrl](https://github.com/gjp1203/nui_in_madrl)
    (inspired by the classic climb gameÂ [[126](#bib.bib126)]) is another two-agent
    pixel-based environment that simultaneously confronts learners with four pathologies
    in MAL: relative overgeneralization, stochasticity, the moving target problem,
    and alter exploration problemÂ [[97](#bib.bib97)].'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PommermanÂ [[301](#bib.bib301)] is a multiagent benchmark useful for testing
    cooperative, competitive and mixed (cooperative and competitive) scenarios. It
    supports partial observability and communication among agents, see FigureÂ [7(b)](#S4.F7.sf2
    "In Figure 7 â€£ 4.3 Benchmarks for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey
    and Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€"). Pommerman is a very challenging
    domain from the exploration perspective as the rewards are very sparse and delayedÂ [[302](#bib.bib302)].
    A recent competition was held during NeurIPS-2018^(21)^(21)21[https://www.pommerman.com/](https://www.pommerman.com/)
    and the top agents from that competition are available for training purposes.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starcraft Multiagent ChallengeÂ [[303](#bib.bib303)] is based on the real-time
    strategy game StarCraft II and focuses on micromanagement challenges,^(22)^(22)22[https://github.com/oxwhirl/smac](https://github.com/oxwhirl/smac)
    that is, fine-grained control of individual units, where each unit is controlled
    by an independent agent that must act based on local observations. It is accompanied
    by a MDRL framework including state-of-the-art algorithms (e.g., QMIX and COMA).^(23)^(23)23[https://github.com/oxwhirl/pymarl](https://github.com/oxwhirl/pymarl)
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Multi-Agent Reinforcement Learning in MalmÃ¶ (MARLÃ–) competitionÂ [[304](#bib.bib304)]
    is another multiagent challenge with multiple cooperative 3D games^(24)^(24)24[https://github.com/crowdAI/marlo-single-agent-starter-kit/](https://github.com/crowdAI/marlo-single-agent-starter-kit/)
    within Minecraft. The scenarios were created with the open source MalmÃ¶ platformÂ [[305](#bib.bib305)],
    providing examples of how a wider range of multiagent cooperative, competitive
    and mixed scenarios can be experimented on within Minecraft.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanabi is a cooperative multiplayer card game (two to five players). The main
    characteristic of the game is that players do not observe their own cards but
    other players can reveal information about them. This makes an interesting challenge
    for learning algorithms in particular in the context of self-play learning and
    ad-hoc teamsÂ [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)]. The
    Hanabi Learning EnvironmentÂ [[309](#bib.bib309)] was recently released^(25)^(25)25[https://github.com/deepmind/hanabi-learning-environment](https://github.com/deepmind/hanabi-learning-environment)
    and it is accompanied with a baseline (deep RL) agentÂ [[310](#bib.bib310)].
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ArenaÂ [[311](#bib.bib311)] is platform for multiagent research^(26)^(26)26[https://github.com/YuhangSong/Arena-BuildingToolkit](https://github.com/YuhangSong/Arena-BuildingToolkit)
    based on the Unity engineÂ [[312](#bib.bib312)]. It has 35 multiagent games (e.g.,
    social dilemmas) and supports communication among agents. It has basseline implementations
    of recent DRL algorithms such as independent PPO learners.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MuJoCo Multiagent SoccerÂ [[313](#bib.bib313)] uses the MuJoCo physics engineÂ [[202](#bib.bib202)].
    The environment simulates a 2 vs.Â 2 soccer game with agents having a 3-dimensional
    action space.^(27)^(27)27[https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural MMOÂ [[314](#bib.bib314)] is a research platform^(28)^(28)28[https://github.com/openai/neural-mmo](https://github.com/openai/neural-mmo)
    inspired by the human game genre of Massively Multiplayer Online (MMO) Role-Playing
    Games. These games involve a large, variable number of players competing to survive.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.4 Practical challenges in MDRL
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we take a more critical view with respect to MDRL and highlight
    different practical challenges that already happen in DRL and that are likely
    to occur in MDRL such as reproducibility, hyperparameter tuning, the need of computational
    resources and conflation of results. We provide pointers on how we think those
    challenges could be (partially) addressed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility, troubling trends and negative results
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Reproducibility is a challenge in RL which is only aggravated in DRL due to
    different sources of stochasticity: baselines, hyperparameters, architecturesÂ [[315](#bib.bib315),
    [316](#bib.bib316)] and random seedsÂ [[317](#bib.bib317)]. Moreover, DRL does
    not have common practices for statistical testingÂ [[318](#bib.bib318)] which has
    led to bad practices such as only reporting the results when algorithms perform
    well, sometimes referred as *cherry picking*Â [[319](#bib.bib319)] (Azizzadenesheli
    also describes *cherry planting* as adapting an environment to a specific algorithmÂ [[319](#bib.bib319)]).
    We believe that together with following the advice on how to design experiments
    and report resultsÂ [[320](#bib.bib320)], the community would also benefit from
    reporting *negative results*Â [[321](#bib.bib321), [322](#bib.bib322), [318](#bib.bib318),
    [323](#bib.bib323)] for carefully designed hypothesis and experiments.^(29)^(29)29This
    idea was initially inspired by the Workshop â€œCritiquing and Correcting Trends
    in Machine Learningâ€ at NeurIPS 2018 where it was possible to submit *Negative
    results* papers: â€œPapers which show failure modes of existing algorithms or suggest
    new approaches which one might expect to perform well but which do not. The aim
    is to provide a venue for work which might otherwise go unpublished but which
    is still of interest to the community.â€ [https://ml-critique-correct.github.io/](https://ml-critique-correct.github.io/)
    However, we found very few papers with this characteristic[[324](#bib.bib324),
    [325](#bib.bib325), [326](#bib.bib326)] â€” we note that this is not encouraged
    in the ML community; moreover, negative results reduce the chance of paper acceptanceÂ [[320](#bib.bib320)].
    In this regard, we ask the community to reflect on these practices and find ways
    to remove these obstacles.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Implementation challenges and hyperparameter tuning
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One problem is that canonical implementations of DRL algorithms often contain
    additional non-trivial optimizations â€” these are sometimes necessary for the algorithms
    to achieve good performanceÂ [[79](#bib.bib79)]. A recent study by Tucker et al.Â [[59](#bib.bib59)]
    found that several published works on action-dependant baselines contained bugs
    and errors â€” those were the real reason of the high performance in the experimental
    results, not the proposed method. Melis et al.Â [[327](#bib.bib327)] compared a
    series of works with increasing innovations in network architectures and the vanilla
    LSTMsÂ [[86](#bib.bib86)] (originally proposed in 1997). The results showed that,
    when properly tuned, LSTMs outperformed the more recent models. In this context,
    Lipton and Steinhardt noted that the community may have benefited more by learning
    the details of the hyperparameter tuningÂ [[320](#bib.bib320)]. A partial reason
    for this surprising result might be that this type of networks are known for being
    difficult to trainÂ [[293](#bib.bib293)] and there are recent works in DRL that
    report problems when using recurrent networksÂ [[182](#bib.bib182), [328](#bib.bib328),
    [329](#bib.bib329), [330](#bib.bib330)]. Another known complication is catastrophic
    forgetting (see SectionÂ [2.2](#S2.SS2 "2.2 Deep reinforcement learning â€£ 2 Single-agent
    learning â€£ A Survey and Critique of Multiagent Deep Reinforcement Learning1footnote
    11footnote 1Earlier versions of this work had the title: â€œIs multiagent deep reinforcement
    learning the answer or the question? A brief surveyâ€")) with recent examples in
    DRLÂ [[157](#bib.bib157), [92](#bib.bib92)] â€” we expect that these issues would
    likely occur in MDRL. The effects of hyperparameter tuning were analyzed in more
    detail in DRL by Henderson et al.Â [[315](#bib.bib315)], who arrived at the conclusion
    that hyperparameters can have significantly different effects across algorithms
    (they tested TRPO, DDPG, PPO and ACKTR) and environments since there is an intricate
    interplay among themÂ [[315](#bib.bib315)]. The authors urge the community to report
    *all* parameters used in the experimental evaluations for accurate comparison
    â€” we encourage a similar behavior for MDRL. Note that hyperparameter tuning is
    related to the troubling trend of cherry picking in that it can show a carefully
    picked set of parameters that make an algorithm work (see previous challenge).
    Lastly, note that hyperparameter tuning is computationally very expensive, which
    relates to the connection with the following challenge of computational demands.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Deep RL usually requires millions of interactions for an agent to learnÂ [[331](#bib.bib331)],
    i.e., low sample efficiencyÂ [[332](#bib.bib332)], which highlights the need for
    large computational infrastructure in general. The original A3C implementationÂ [[93](#bib.bib93)]
    uses 16 CPU workers for 4 days to learn to play an Atari game with a total of
    200M training frames^(30)^(30)30It is sometimes unclear in the literature what
    is the meaning of frame due to the â€œframe skipâ€ technique. It is therefore suggested
    to refer to â€œgame framesâ€ and â€œtraining framesâ€Â [[333](#bib.bib333)]. (results
    are reported for 57 Atari games). Distributed PPO used 64 workers (presumably
    one CPU per worker, although this is not clearly stated in the paper) for 100
    hours (more than 4 days) to learn locomotion tasksÂ [[117](#bib.bib117)]. In MDRL,
    for example, the Atari Pong game, agents were trained for 50 epochs, 250k time
    steps each, for a total of 1.25M training framesÂ [[155](#bib.bib155)]. The FTW
    agentÂ [[179](#bib.bib179)] uses 30 agents (processes) in parallel and every training
    game lasts for five minues; FTW agents were trained for approximately 450K games
    <math id="S4.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics
    id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml">â‰ˆ</mo><annotation
    encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">\approx</annotation></semantics></math>4.2
    years. These examples highlight the computational demands sometimes needed within
    DRL and MDRL.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent works have reduced the learning of an Atari game to minutes (Stooke
    and AbbeelÂ [[334](#bib.bib334)] trained DRL agents in less than one hour with
    hardware consisting of 8 GPUs and 40 cores). However, this is (for now) the exception
    and computational infrastructure is a major bottleneck for doing DRL and MDRL,
    especially for those who do not have such large compute power (e.g., most companies
    and most academic research groups)Â [[212](#bib.bib212), [322](#bib.bib322)].^(31)^(31)31One
    recent effort by Beeching et al.Â [[212](#bib.bib212)] proposes to use only â€œmid-range
    hardwareâ€ (8 CPUs and 1 GPU) to train deep RL agents. Within this context we propose
    two ways to address this problem. (1) Raising awareness: For DRL we found few
    works that study the computational demands of recent algorithmsÂ [[335](#bib.bib335),
    [331](#bib.bib331)]. For MDRL most published works do not provide information
    regarding computational resources used such as CPU/GPU usage, memory demands,
    and wall-clock computation. Therefore, the first way to tackle this issue is by
    raising awareness and encouraging authors to report metrics about computational
    demands for accurately comparison and evaluation. (2) Delve into algorithmic contributions.
    Another way to address these issues is to prioritize the algorithmic contribution
    for the new MDRL algorithms rather than the computational resources spent. Indeed,
    for this to work, it needs to be accompanied with high-quality reviewers.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'We have argued to raise awareness on the computational demands and report results,
    however, there is still the open question on *how* and *what* to measure/report.
    There are several dimensions to measure efficiency: sample efficiency is commonly
    measured by counting state-action pairs used for training; computational efficiency
    could be measured by number of CPUs/GPUs and days used for training. How do we
    measure the impact of other resources, such as external data sources or annotations?^(32)^(32)32NeurIPS
    2019 hosts the â€œMineRL Competition on Sample Efficient Reinforcement Learning
    using Human Priorsâ€ where the primary goal of the competition is to foster the
    development of algorithms which can efficiently leverage human demonstrations
    to drastically reduce the number of samples needed to solve complex, hierarchical,
    and sparse environmentsÂ [[336](#bib.bib336)]. Similarly, do we need to differentiate
    the computational needs of the algorithm itself versus the environment it is run
    in? We do not have the answers, however, we point out that current standard metrics
    might not be entirely comprehensive.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we believe that high compute based methods act as a frontier to
    showcase benchmarksÂ [[19](#bib.bib19), [18](#bib.bib18)], i.e., they show what
    results are possible as data and compute is scaled up (e.g., OpenAI Five generates
    180 years of gameplay data each day using 128,000 CPU cores and 256 GPUsÂ [[18](#bib.bib18)];
    AlphaStar uses 200 years of Starcraft II gameplayÂ [[19](#bib.bib19)]); however,
    lighter compute based algorithmic methods can also yield significant contributions
    to better tackle real-world problems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Occamâ€™s razor and ablative analysis
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finding the simplest context that exposes the innovative research idea remains
    challenging, and if ignored leads to a conflation of fundamental research (working
    principles in the most abstract setting) and applied research (working systems
    as complete as possible). In particular, some deep learning papers are presented
    as learning from pixels without further explanation, while object-level representations
    would have already exposed the algorithmic contribution. This still makes sense
    to remain comparable with established benchmarks (e.g., OpenAI GymÂ [[299](#bib.bib299)]),
    but less so if custom simulations are written without open source access, as it
    introduces unnecessary variance in pixel-level representations and artificially
    inflates computational resources (see previous point about *computational resources*).^(33)^(33)33Cuccu,
    Togelius and CudrÃ©-Mauroux achieved state-of-the-art policy learning in Atari
    games with only 6 to 18 neuronsÂ [[337](#bib.bib337)]. The main idea was to decouple
    image processing from decision-making. In this context there are some notable
    exceptions where the algorithmic contribution is presented in a minimal setting
    and then results are scaled into complex settings: LOLAÂ [[64](#bib.bib64)] first
    presented a minimalist setting with a two-player two-action game and then with
    a more complex variant; similarly, QMIXÂ [[181](#bib.bib181)] presented its results
    in a two-step (matrix) game and then in the more involved Starcraft II micromanagement
    domainÂ [[303](#bib.bib303)].'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Open questions
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, here we present some open questions for MDRL and point to suggestions
    on how to approach them. We believe that there are solid ideas in earlier literature
    and we refer the reader to SectionÂ [4.1](#S4.SS1 "4.1 Avoiding deep learning amnesia:
    examples in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€") to avoid deep learning amnesia.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the challenge of sparse and delayed rewards.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Recent MDRL competitions and environments have complex scenarios where many
    actions are taken before a reward signal is available (see SectionÂ [4.3](#S4.SS3
    "4.3 Benchmarks for MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and Critique
    of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier versions
    of this work had the title: â€œIs multiagent deep reinforcement learning the answer
    or the question? A brief surveyâ€")). This sparseness is already a challenge for
    RLÂ [[20](#bib.bib20), [338](#bib.bib338)] where approaches such as count-based
    exploration/intrinsic motivationÂ [[196](#bib.bib196), [339](#bib.bib339), [340](#bib.bib340),
    [341](#bib.bib341), [342](#bib.bib342)] and hierarchical learningÂ [[343](#bib.bib343),
    [344](#bib.bib344), [111](#bib.bib111)] have been proposed to address it â€” in
    MDRL this is even more problematic since the agents not only need to learn basic
    behaviors (like in DRL), but also to learn the strategic element (e.g., competitive/collaborative)
    embedded in the multiagent setting. To address this issue, recent MDRL approaches
    applied *dense* rewardsÂ [[206](#bib.bib206), [205](#bib.bib205), [204](#bib.bib204)]
    (a concept originated in RL) at each step to allow the agents to learn basic motor
    skills and then decrease these *dense* rewards over time in favor of the environmental
    rewardÂ [[158](#bib.bib158)], see SectionÂ [3.3](#S3.SS3 "3.3 Emergent behaviors
    â€£ 3 Multiagent Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent
    Deep Reinforcement Learning1footnote 11footnote 1Earlier versions of this work
    had the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€"). Recent works like OpenAI FiveÂ [[18](#bib.bib18)] uses hand-crafted
    intermediate rewards to accelerate the learning and FTWÂ [[179](#bib.bib179)] lets
    the agents learn their internal rewards by a hierarchical two-tier optimization.
    In single agent domains, RUDDERÂ [[345](#bib.bib345)] has been recently proposed
    for such delayed sparse reward problems. RUDDER generates a new MDP with *more
    intermediate rewards* whose optimal solution is still an optimal solution to the
    original MDP. This is achieved by using LSTM networks to redistribute the original
    sparse reward to earlier state-action pairs and automatically provide reward shaping.
    How to best extend RUDDER to multiagent domains is an open avenue of research.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the role of self-play.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Self-play is a cornerstone in MAL with impressive resultsÂ [[147](#bib.bib147),
    [127](#bib.bib127), [145](#bib.bib145), [346](#bib.bib346), [143](#bib.bib143)].
    While notable results had also been shown in MDRLÂ [[173](#bib.bib173), [193](#bib.bib193)],
    recent works have also shown that *plain* self-play does not yield the best results.
    However, adding diversity, i.e., evolutionary methodsÂ [[239](#bib.bib239), [240](#bib.bib240),
    [233](#bib.bib233), [234](#bib.bib234)] or sampling-based methods, have shown
    good resultsÂ [[158](#bib.bib158), [179](#bib.bib179), [159](#bib.bib159)]. A drawback
    of these solutions is the additional computational requirements since they need
    either parallel training (more CPU computation) or memory requirements. Then,
    it is still an open problem to improve the computational efficiency of these previously
    proposed successful methods, i.e., achieving similar training stability with smaller
    population sizes that uses fewer CPU workers in MAL and MDRL (see SectionÂ [4.4](#S4.SS4
    "4.4 Practical challenges in MDRL â€£ 4 Bridging RL, MAL and MDRL â€£ A Survey and
    Critique of Multiagent Deep Reinforcement Learning1footnote 11footnote 1Earlier
    versions of this work had the title: â€œIs multiagent deep reinforcement learning
    the answer or the question? A brief surveyâ€") and Albrecht et al.Â [[11](#bib.bib11),
    Section 5.5]).'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the challenge of the combinatorial nature of MDRL.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monte Carlo tree search (MCTS)Â [[347](#bib.bib347)] has been the backbone of
    the major breakthroughs behind AlphaGoÂ [[14](#bib.bib14)] and AlphaGo ZeroÂ [[15](#bib.bib15)]
    that combined search and DRL. A recent workÂ [[348](#bib.bib348)] has outlined
    how search and RL can be better combined for potentially new methods. However,
    for multiagent scenarios, there is an additional challenge of the exponential
    growth of all the agentsâ€™ action spaces for centralized methodsÂ [[349](#bib.bib349)].
    One way to tackle this challenge within multiagent scenarios is the use of search
    parallelizationÂ [[350](#bib.bib350), [351](#bib.bib351)]. Given more scalable
    planners, there is room for research in combining these techniques in MDRL settings.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To learn complex multiagent interactions some type of abstractionÂ [[352](#bib.bib352)]
    is often needed, for example, factored value functionsÂ [[353](#bib.bib353), [354](#bib.bib354),
    [242](#bib.bib242), [243](#bib.bib243), [355](#bib.bib355), [356](#bib.bib356)]
    (see QMIX and VDN in SectionÂ [3.5](#S3.SS5 "3.5 Learning cooperation â€£ 3 Multiagent
    Deep Reinforcement Learning (MDRL) â€£ A Survey and Critique of Multiagent Deep
    Reinforcement Learning1footnote 11footnote 1Earlier versions of this work had
    the title: â€œIs multiagent deep reinforcement learning the answer or the question?
    A brief surveyâ€") for recent work in MDRL) try to exploit independence among agents
    through (factored) structure; however, in MDRL there are still open questions
    such as understanding their representational powerÂ [[244](#bib.bib244)] (e.g.,
    the accuracy of the learned Q-function approximations) and how to learn those
    factorizations, where ideas from transfer planning techniques could be usefulÂ [[357](#bib.bib357),
    [103](#bib.bib103)]. In transfer planning the idea is to define a simpler â€œsource
    problemâ€ (e.g., with fewer agents), in which the agent(s) can planÂ [[357](#bib.bib357)]
    or learnÂ [[103](#bib.bib103)]; since it is less complex than the real multiagent
    problem, issues such as the non-stationarity of the environment can be reduced/removed.
    Lastly, another related idea are *influence* abstractionsÂ [[358](#bib.bib358),
    [359](#bib.bib359), [10](#bib.bib10)], where instead of learning a complex multiagent
    model, these methods try to build smaller models based on the influence agents
    can exert on one another. While this has not been sufficiently explored in actual
    multiagent settings, there is some evidence that these ideas can lead to effective
    inductive biases, improving effectiveness of DRL in such local abstractionsÂ [[360](#bib.bib360)].'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5 Conclusions
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep reinforcement learning has shown recent success on many frontsÂ [[13](#bib.bib13),
    [14](#bib.bib14), [16](#bib.bib16)] and a natural next step is to test multiagent
    scenarios. However, learning in multiagent environments is fundamentally more
    difficult due to non-stationarity, the increase of dimensionality, and the credit-assignment
    problem, among other factorsÂ [[1](#bib.bib1), [5](#bib.bib5), [10](#bib.bib10),
    [147](#bib.bib147), [241](#bib.bib241), [361](#bib.bib361), [97](#bib.bib97)].
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'This survey provides broad overview of recent works in the emerging area of
    Multiagent Deep Reinforcement Learning (MDRL). First, we categorized recent works
    into four different topics: emergent behaviors, learning communication, learning
    cooperation, and agents modeling agents. Then, we exemplified how key components
    (e.g., experience replay and difference rewards) originated in RL and MAL need
    to be adapted to work in MDRL. We provided general lessons learned applicable
    to MDRL, pointed to recent multiagent benchmarks and highlighted some open research
    problems. Finally, we also reflected on the practical challenges such as computational
    demands and reproducibility in MDRL.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Our conclusions of this work are that while the number of works in DRL and
    MDRL are notable and represent important milestones for AI, at the same time we
    acknowledge there are also open questions in both (deep) single-agent learningÂ [[38](#bib.bib38),
    [298](#bib.bib298), [362](#bib.bib362), [79](#bib.bib79)] and multiagent learningÂ [[363](#bib.bib363),
    [364](#bib.bib364), [365](#bib.bib365), [366](#bib.bib366), [367](#bib.bib367),
    [368](#bib.bib368)]. Our view is that there are practical issues within MDRL that
    hinder its scientific progress: the necessity of high compute power, complicated
    reproducibility (e.g., hyperparameter tuning), and the lack of sufficient encouragement
    for publishing negative results. However, we remain highly optimistic of the multiagent
    community and hope this work serves to raise those issues, encounter good solutions,
    and ultimately take advantage of the existing literature and resources available
    to move the area in the right direction.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Chao Gao, Nidhi Hegde, Gregory Palmer, Felipe Leno Da
    Silva and Craig Sherstan for reading earlier versions of this work and providing
    feedback, to April Cooper for her visual designs for the figures in the article,
    to Frans Oliehoek, Sam Devlin, Marc Lanctot, Nolan Bard, Roberta Raileanu, Angeliki
    Lazaridou, and Yuhang Song for clarifications in their areas of expertise, to
    Baoxiang Wang for his suggestions on recent deep RL works, to Michael Kaisers,
    Daan Bloembergen, and Katja Hofmann for their comments about the practical challenges
    of MDRL, and to the editor and three anonymous reviewers whose comments and suggestions
    increased the quality of this work.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] P.Â Stone, M.Â M. Veloso, Multiagent Systems - A Survey from a Machine Learning
    Perspective., Autonomous Robots 8Â (3) (2000) 345â€“383.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Y.Â Shoham, R.Â Powers, T.Â Grenager, If multi-agent learning is the answer,
    what is the question?, Artificial Intelligence 171Â (7) (2007) 365â€“377.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] E.Â Alonso, M.Â Dâ€™inverno, D.Â Kudenko, M.Â Luck, J.Â Noble, Learning in multi-agent
    systems, Knowledge Engineering Review 16Â (03) (2002) 1â€“8.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K.Â Tuyls, G.Â Weiss, Multiagent learning: Basics, challenges, and prospects,
    AI Magazine 33Â (3) (2012) 41â€“52.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L.Â Busoniu, R.Â Babuska, B.Â DeÂ Schutter, A Comprehensive Survey of Multiagent
    Reinforcement Learning, IEEE Transactions on Systems, Man and Cybernetics, Part
    C (Applications and Reviews) 38Â (2) (2008) 156â€“172.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A.Â NowÃ©, P.Â Vrancx, Y.-M. DeÂ Hauwere, Game theory and multi-agent reinforcement
    learning, in: Reinforcement Learning, Springer, 2012, pp. 441â€“470.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] L.Â Panait, S.Â Luke, Cooperative Multi-Agent Learning: The State of the
    Art, Autonomous Agents and Multi-Agent Systems 11Â (3).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] L.Â Matignon, G.Â J. Laurent, N.Â LeÂ Fort-Piat, Independent reinforcement
    learners in cooperative Markov games: a survey regarding coordination problems,
    Knowledge Engineering Review 27Â (1) (2012) 1â€“31.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] D.Â Bloembergen, K.Â Tuyls, D.Â Hennes, M.Â Kaisers, Evolutionary Dynamics
    of Multi-Agent Learning: A Survey., Journal of Artificial Intelligence Research
    53 (2015) 659â€“697.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] P.Â Hernandez-Leal, M.Â Kaisers, T.Â Baarslag, E.Â MunozÂ de Cote, [A Survey
    of Learning in Multiagent Environments - Dealing with Non-Stationarity](http://arxiv.org/abs/1707.09183).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.09183](http://arxiv.org/abs/1707.09183)
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[11] S.Â V. Albrecht, P.Â Stone, Autonomous agents modelling other agents: A
    comprehensive survey and open problems, Artificial Intelligence 258 (2018) 66â€“95.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] F.Â L. Silva, A.Â H.Â R. Costa, A survey on transfer learning for multiagent
    reinforcement learning systems, Journal of Artificial Intelligence Research 64
    (2019) 645â€“703.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] V.Â Mnih, K.Â Kavukcuoglu, D.Â Silver, A.Â A. Rusu, J.Â Veness, M.Â G. Bellemare,
    A.Â Graves, M.Â Riedmiller, A.Â K. Fidjeland, G.Â Ostrovski, S.Â Petersen, C.Â Beattie,
    A.Â Sadik, I.Â Antonoglou, H.Â King, D.Â Kumaran, D.Â Wierstra, S.Â Legg, D.Â Hassabis,
    Human-level control through deep reinforcement learning, Nature 518Â (7540) (2015)
    529â€“533.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D.Â Silver, A.Â Huang, C.Â J. Maddison, A.Â Guez, L.Â Sifre, G.Â vanÂ den Driessche,
    J.Â Schrittwieser, I.Â Antonoglou, V.Â Panneershelvam, M.Â Lanctot, S.Â Dieleman, D.Â Grewe,
    J.Â Nham, N.Â Kalchbrenner, I.Â Sutskever, T.Â Lillicrap, M.Â Leach, K.Â Kavukcuoglu,
    T.Â Graepel, D.Â Hassabis, Mastering the game of Go with deep neural networks and
    tree search, Nature 529Â (7587) (2016) 484â€“489.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D.Â Silver, J.Â Schrittwieser, K.Â Simonyan, I.Â Antonoglou, A.Â Huang, A.Â Guez,
    T.Â Hubert, L.Â Baker, M.Â Lai, A.Â Bolton, etÂ al., Mastering the game of Go without
    human knowledge, Nature 550Â (7676) (2017) 354.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M.Â MoravÄÃ­k, M.Â Schmid, N.Â Burch, V.Â LisÃ½, D.Â Morrill, N.Â Bard, T.Â Davis,
    K.Â Waugh, M.Â Johanson, M.Â Bowling, DeepStack: Expert-level artificial intelligence
    in heads-up no-limit poker, Science 356Â (6337) (2017) 508â€“513.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N.Â Brown, T.Â Sandholm, Superhuman AI for heads-up no-limit poker: Libratus
    beats top professionals, Science 359Â (6374) (2018) 418â€“424.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Open AI Five, [https://blog.openai.com/openai-five](https://blog.openai.com/openai-five),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] O.Â Vinyals, I.Â Babuschkin, J.Â Chung, M.Â Mathieu, M.Â Jaderberg, W.Â M. Czarnecki,
    A.Â Dudzik, A.Â Huang, P.Â Georgiev, R.Â Powell, T.Â Ewalds, D.Â Horgan, M.Â Kroiss,
    I.Â Danihelka, J.Â Agapiou, J.Â Oh, V.Â Dalibard, D.Â Choi, L.Â Sifre, Y.Â Sulsky, S.Â Vezhnevets,
    J.Â Molloy, T.Â Cai, D.Â Budden, T.Â Paine, C.Â Gulcehre, Z.Â Wang, T.Â Pfaff, T.Â Pohlen,
    Y.Â Wu, D.Â Yogatama, J.Â Cohen, K.Â McKinney, O.Â Smith, T.Â Schaul, T.Â Lillicrap,
    C.Â Apps, K.Â Kavukcuoglu, D.Â Hassabis, D.Â Silver, AlphaStar: Mastering the Real-Time
    Strategy Game StarCraft II, [https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)
    (2019).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R.Â S. Sutton, A.Â G. Barto, Reinforcement learning: An introduction, 2nd
    Edition, MIT Press, 2018.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y.Â LeCun, Y.Â Bengio, G.Â Hinton, Deep learning, Nature 521Â (7553) (2015)
    436.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J.Â Schmidhuber, Deep learning in neural networks: An overview, Neural
    networks 61 (2015) 85â€“117.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K.Â Arulkumaran, M.Â P. Deisenroth, M.Â Brundage, A.Â A. Bharath, [A Brief
    Survey of Deep Reinforcement Learning](http://arXiv.org/abs/1708.05866v2) .'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1708.05866v2](http://arXiv.org/abs/1708.05866v2)
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[24] V.Â FranÃ§ois-Lavet, P.Â Henderson, R.Â Islam, M.Â G. Bellemare, J.Â Pineau,
    etÂ al., An introduction to deep reinforcement learning, Foundations and TrendsÂ®
    in Machine Learning 11Â (3-4) (2018) 219â€“354.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y.Â Yang, J.Â Hao, M.Â Sun, Z.Â Wang, C.Â Fan, G.Â Strbac, Recurrent Deep Multiagent
    Q-Learning for Autonomous Brokers in Smart Grid, in: Proceedings of the Twenty-Seventh
    International Joint Conference on Artificial Intelligence, Stockholm, Sweden,
    2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J.Â Zhao, G.Â Qiu, Z.Â Guan, W.Â Zhao, X.Â He, Deep reinforcement learning
    for sponsored search real-time bidding, in: Proceedings of the 24th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining, ACM, 2018, pp.
    1021â€“1030.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] B.Â M. Lake, T.Â D. Ullman, J.Â Tenenbaum, S.Â Gershman, Building machines
    that learn and think like people, Behavioral and Brain Sciences 40.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A.Â Tamar, S.Â Levine, P.Â Abbeel, Y.Â Wu, G.Â Thomas, Value Iteration Networks.,
    NIPS (2016) 2154â€“2162.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G.Â Papoudakis, F.Â Christianos, A.Â Rahman, S.Â V. Albrecht, Dealing with
    non-stationarity in multi-agent deep reinforcement learning, arXiv preprint arXiv:1906.04737.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] T.Â T. Nguyen, N.Â D. Nguyen, S.Â Nahavandi, Deep reinforcement learning
    for multi-agent systems: A review of challenges, solutions and applications, arXiv
    preprint arXiv:1812.11794.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D.Â H. Wolpert, K.Â Tumer, Optimal payoff functions for members of collectives,
    in: Modeling complexity in economic and social systems, 2002, pp. 355â€“369.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A.Â K. Agogino, K.Â Tumer, Unifying Temporal and Structural Credit Assignment
    Problems., in: Proceedings of 17th International Conference on Autonomous Agents
    and Multiagent Systems, 2004.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] N.Â Fulda, D.Â Ventura, Predicting and Preventing Coordination Problems
    in Cooperative Q-learning Systems, in: Proceedings of the Twentieth International
    Joint Conference on Artificial Intelligence, Hyderabad, India, 2007, pp. 780â€“785.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] E.Â Wei, S.Â Luke, Lenient Learning in Independent-Learner Stochastic Cooperative
    Games., Journal of Machine Learning Research.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] G.Â Palmer, K.Â Tuyls, D.Â Bloembergen, R.Â Savani, Lenient Multi-Agent Deep
    Reinforcement Learning., in: International Conference on Autonomous Agents and
    Multiagent Systems, 2018.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] L.Â Busoniu, R.Â Babuska, B.Â DeÂ Schutter, Multi-agent reinforcement learning:
    An overview, in: D.Â Srinivasan, L.Â C. Jain (Eds.), Innovations in Multi-Agent
    Systems and Applications - 1, Springer Berlin Heidelberg, Berlin, Heidelberg,
    2010, pp. 183â€“221.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Y.Â Li, [Deep reinforcement learning: An overview](http://arxiv.org/abs/1701.07274),
    CoRR abs/1701.07274. [arXiv:1701.07274](http://arxiv.org/abs/1701.07274).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1701.07274](http://arxiv.org/abs/1701.07274)
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[38] A.Â Darwiche, Human-level intelligence or animal-like abilities?, Commun.
    ACM 61Â (10) (2018) 56â€“67.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] M.Â Wiering, M.Â VanÂ Otterlo, Reinforcement learning, Adaptation, learning,
    and optimization 12.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] L.Â P. Kaelbling, M.Â L. Littman, A.Â W. Moore, Reinforcement learning: A
    survey, Journal of artificial intelligence research 4 (1996) 237â€“285.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] M.Â L. Puterman, Markov decision processes: Discrete stochastic dynamic
    programming, John Wiley & Sons, Inc., 1994.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A.Â R. Cassandra, Exact and approximate algorithms for partially observable
    Markov decision processes, Ph.D. thesis, Computer Science Department, Brown University
    (May 1998).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] K.Â J. Astrom, Optimal control of Markov processes with incomplete state
    information, Journal of mathematical analysis and applications 10Â (1) (1965) 174â€“205.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] R.Â Bellman, A Markovian decision process, Journal of Mathematics and Mechanics
    6Â (5) (1957) 679â€“684.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J.Â Watkins, Learning from delayed rewards, Ph.D. thesis, Kingâ€™s College,
    Cambridge, UK (Apr. 1989).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] T.Â Kamihigashi, C.Â LeÂ Van, [Necessary and Sufficient Conditions for a
    Solution of the Bellman Equation to be the Value Function: A General Principle](https://halshs.archives-ouvertes.fr/halshs-01159177).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://halshs.archives-ouvertes.fr/halshs-01159177](https://halshs.archives-ouvertes.fr/halshs-01159177)
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[47] J.Â Tsitsiklis, Asynchronous stochastic approximation and Q-learning, Machine
    Learning 16Â (3) (1994) 185â€“202.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T.Â Jaakkola, M.Â I. Jordan, S.Â P. Singh, Convergence of stochastic iterative
    dynamic programming algorithms, in: Advances in neural information processing
    systems, 1994, pp. 703â€“710.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C.Â SzepesvÃ¡ri, M.Â L. Littman, A unified analysis of value-function-based
    reinforcement-learning algorithms, Neural computation 11Â (8) (1999) 2017â€“2060.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] E.Â Even-Dar, Y.Â Mansour, Learning rates for Q-learning, Journal of Machine
    Learning Research 5Â (Dec) (2003) 1â€“25.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C.Â SzepesvÃ¡ri, Algorithms for reinforcement learning, Synthesis lectures
    on artificial intelligence and machine learning 4Â (1) (2010) 1â€“103.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S.Â Singh, T.Â Jaakkola, M.Â L. Littman, C.Â SzepesvÃ¡ri, Convergence results
    for single-step on-policy reinforcement-learning algorithms, Machine learning
    38Â (3) (2000) 287â€“308.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H.Â VanÂ Seijen, H.Â VanÂ Hasselt, S.Â Whiteson, M.Â Wiering, A theoretical
    and empirical analysis of Expected Sarsa, in: IEEE Symposium on Adaptive Dynamic
    Programming and Reinforcement Learning, Nashville, TN, USA, 2009, pp. 177â€“184.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] V.Â R. Konda, J.Â Tsitsiklis, Actor-critic algorithms, in: Advances in Neural
    Information Processing Systems, 2000.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R.Â S. Sutton, D.Â A. McAllester, S.Â P. Singh, Y.Â Mansour, Policy Gradient
    Methods for Reinforcement Learning with Function Approximation., in: Advances
    in Neural Information Processing Systems, 2000.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R.Â J. Williams, Simple statistical gradient-following algorithms for connectionist
    reinforcement learning, Machine learning 8Â (3-4) (1992) 229â€“256.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H.Â Liu, Y.Â Feng, Y.Â Mao, D.Â Zhou, J.Â Peng, Q.Â Liu, Action-depedent control
    variates for policy optimization via steinâ€™s identity, in: International Conference
    on Learning Representations, 2018.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S.Â Gu, T.Â Lillicrap, Z.Â Ghahramani, R.Â E. Turner, S.Â Levine, Q-prop: Sample-efficient
    policy gradient with an off-policy critic, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] G.Â Tucker, S.Â Bhupatiraju, S.Â Gu, R.Â E. Turner, Z.Â Ghahramani, S.Â Levine,
    The mirage of action-dependent baselines in reinforcement learning, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] J.Â Schulman, S.Â Levine, P.Â Abbeel, M.Â I. Jordan, P.Â Moritz, Trust Region
    Policy Optimization., in: 31st International Conference on Machine Learning, Lille,
    France, 2015.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] D.Â Silver, G.Â Lever, N.Â Heess, T.Â Degris, D.Â Wierstra, M.Â Riedmiller,
    Deterministic policy gradient algorithms, in: ICML, 2014.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] R.Â Hafner, M.Â Riedmiller, Reinforcement learning in feedback control,
    Machine learning 84Â (1-2) (2011) 137â€“169.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] R.Â Lowe, Y.Â Wu, A.Â Tamar, J.Â Harb, P.Â Abbeel, I.Â Mordatch, Multi-Agent
    Actor-Critic for Mixed Cooperative-Competitive Environments., in: Advances in
    Neural Information Processing Systems, 2017, pp. 6379â€“6390.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J.Â N. Foerster, R.Â Y. Chen, M.Â Al-Shedivat, S.Â Whiteson, P.Â Abbeel, I.Â Mordatch,
    Learning with Opponent-Learning Awareness., in: Proceedings of 17th International
    Conference on Autonomous Agents and Multiagent Systems, Stockholm, Sweden, 2018.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] T.Â P. Lillicrap, J.Â J. Hunt, A.Â Pritzel, N.Â Heess, T.Â Erez, Y.Â Tassa,
    D.Â Silver, D.Â Wierstra, Continuous control with deep reinforcement learning, in:
    International Conference on Learning Representations, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] L.Â D. Pyeatt, A.Â E. Howe, etÂ al., Decision tree function approximation
    in reinforcement learning, in: Proceedings of the third international symposium
    on adaptive systems: evolutionary computation and probabilistic graphical models,
    Vol.Â 2, Cuba, 2001, pp. 70â€“77.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R.Â S. Sutton, Generalization in reinforcement learning: Successful examples
    using sparse coarse coding, in: Advances in neural information processing systems,
    1996, pp. 1038â€“1044.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] R.Â M. Kretchmar, C.Â W. Anderson, Comparison of CMACs and radial basis
    functions for local function approximators in reinforcement learning, in: Proceedings
    of International Conference on Neural Networks (ICNNâ€™97), Vol.Â 2, IEEE, 1997,
    pp. 834â€“837.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J.Â A. Boyan, A.Â W. Moore, Generalization in reinforcement learning: Safely
    approximating the value function, in: Advances in neural information processing
    systems, 1995, pp. 369â€“376.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C.Â M. Bishop, Pattern recognition and machine learning, Springer, 2006.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M.Â Riemer, I.Â Cases, R.Â Ajemian, M.Â Liu, I.Â Rish, Y.Â Tu, G.Â Tesauro, [Learning
    to learn without forgetting by maximizing transfer and minimizing interference](http://arxiv.org/abs/1810.11910),
    CoRR abs/1810.11910. [arXiv:1810.11910](http://arxiv.org/abs/1810.11910).'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1810.11910](http://arxiv.org/abs/1810.11910)
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[72] V.Â Mnih, K.Â Kavukcuoglu, D.Â Silver, A.Â Graves, I.Â Antonoglou, D.Â Wierstra,
    M.Â Riedmiller, [Playing Atari with Deep Reinforcement Learning](http://arxiv.org/abs/1312.5602v1).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1312.5602v1](http://arxiv.org/abs/1312.5602v1)
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[73] G.Â J. Gordon, Approximate solutions to Markov decision processes, Tech.
    rep., Carnegie-Mellon University (1999).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] L.Â Baird, Residual algorithms: Reinforcement learning with function approximation,
    in: Machine Learning Proceedings 1995, 1995, pp. 30â€“37.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S.Â Whiteson, P.Â Stone, Evolutionary function approximation for reinforcement
    learning, Journal of Machine Learning Research 7Â (May) (2006) 877â€“917.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J.Â Achiam, E.Â Knight, P.Â Abbeel, [Towards Characterizing Divergence in
    Deep Q-Learning](http://arxiv.org/abs/1903.08894), CoRR abs/1903.08894. [arXiv:1903.08894](http://arxiv.org/abs/1903.08894).'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.08894](http://arxiv.org/abs/1903.08894)
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[77] H.Â van Hasselt, Y.Â Doron, F.Â Strub, M.Â Hessel, N.Â Sonnerat, J.Â Modayil,
    [Deep reinforcement learning and the deadly triad](http://arxiv.org/abs/1812.02648),
    CoRR abs/1812.02648. [arXiv:1812.02648](http://arxiv.org/abs/1812.02648).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.02648](http://arxiv.org/abs/1812.02648)
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[78] S.Â Fujimoto, H.Â van Hoof, D.Â Meger, Addressing function approximation
    error in actor-critic methods, in: International Conference on Machine Learning,
    2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A.Â Ilyas, L.Â Engstrom, S.Â Santurkar, D.Â Tsipras, F.Â Janoos, L.Â Rudolph,
    A.Â Madry, [Are deep policy gradient algorithms truly policy gradient algorithms?](http://arxiv.org/abs/1811.02553),
    CoRR abs/1811.02553. [arXiv:1811.02553](http://arxiv.org/abs/1811.02553).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1811.02553](http://arxiv.org/abs/1811.02553)
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[80] T.Â Lu, D.Â Schuurmans, C.Â Boutilier, Non-delusional Q-learning and value-iteration,
    in: Advances in Neural Information Processing Systems, 2018, pp. 9949â€“9959.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J.Â N. Tsitsiklis, B.Â VanÂ Roy, Analysis of temporal-diffference learning
    with function approximation, in: Advances in neural information processing systems,
    1997, pp. 1075â€“1081.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] F.Â S. Melo, S.Â P. Meyn, M.Â I. Ribeiro, An analysis of reinforcement learning
    with function approximation, in: Proceedings of the 25th international conference
    on Machine learning, ACM, 2008, pp. 664â€“671.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D.Â Ernst, P.Â Geurts, L.Â Wehenkel, Tree-based batch mode reinforcement
    learning, Journal of Machine Learning Research 6Â (Apr) (2005) 503â€“556.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] M.Â Jaderberg, V.Â Mnih, W.Â M. Czarnecki, T.Â Schaul, J.Â Z. Leibo, D.Â Silver,
    K.Â Kavukcuoglu, Reinforcement Learning with Unsupervised Auxiliary Tasks., in:
    International Conference on Learning Representations, 2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] M.Â Hausknecht, P.Â Stone, Deep Recurrent Q-Learning for Partially Observable
    MDPs, in: International Conference on Learning Representations, 2015.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S.Â Hochreiter, J.Â Schmidhuber, Long short-term memory, Neural computation
    9Â (8) (1997) 1735â€“1780.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] M.Â Riedmiller, Neural fitted Q iterationâ€“first experiences with a data
    efficient neural reinforcement learning method, in: European Conference on Machine
    Learning, Springer, 2005, pp. 317â€“328.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] R.Â H. Crites, A.Â G. Barto, Elevator group control using multiple reinforcement
    learning agents, Machine learning 33Â (2-3) (1998) 235â€“262.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] L.Â J. Lin, Programming robots using reinforcement learning and teaching.,
    in: AAAI, 1991, pp. 781â€“786.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] L.-J. Lin, Self-improving reactive agents based on reinforcement learning,
    planning and teaching, Machine learning 8Â (3-4) (1992) 293â€“321.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] H.Â V. Hasselt, Double Q-learning, in: Advances in Neural Information Processing
    Systems, 2010, pp. 2613â€“2621.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] H.Â VanÂ Hasselt, A.Â Guez, D.Â Silver, Deep reinforcement learning with double
    Q-learning, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] V.Â Mnih, A.Â P. Badia, M.Â Mirza, A.Â Graves, T.Â Lillicrap, T.Â Harley, D.Â Silver,
    K.Â Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016, pp. 1928â€“1937.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M.Â McCloskey, N.Â J. Cohen, Catastrophic interference in connectionist
    networks: The sequential learning problem, in: Psychology of learning and motivation,
    Vol.Â 24, Elsevier, 1989, pp. 109â€“165.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] I.Â J. Goodfellow, M.Â Mirza, D.Â Xiao, A.Â Courville, Y.Â Bengio, [An empirical
    investigation of catastrophic forgetting in gradient-based neural networks](https://arxiv.org/abs/1312.6211).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://arxiv.org/abs/1312.6211](https://arxiv.org/abs/1312.6211)
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[96] D.Â Isele, A.Â Cosgun, Selective experience replay for lifelong learning,
    in: Thirty-Second AAAI Conference on Artificial Intelligence, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] G.Â Palmer, R.Â Savani, K.Â Tuyls, Negative update intervals in deep multi-agent
    reinforcement learning, in: 18th International Conference on Autonomous Agents
    and Multiagent Systems, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Z.Â Wang, T.Â Schaul, M.Â Hessel, H.Â VanÂ Hasselt, M.Â Lanctot, N.Â DeÂ Freitas,
    Dueling network architectures for deep reinforcement learning, in: International
    Conference on Machine Learning, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M.Â Hauskrecht, Value-function approximations for partially observable
    Markov decision processes, Journal of Artificial Intelligence Research 13Â (1).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N.Â Meuleau, L.Â Peshkin, K.-E. Kim, L.Â P. Kaelbling, Learning finite-state
    controllers for partially observable environments, in: Proceedings of the Fifteenth
    conference on Uncertainty in artificial intelligence, 1999, pp. 427â€“436.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] D.Â Steckelmacher, D.Â M. Roijers, A.Â Harutyunyan, P.Â Vrancx, H.Â Plisnier,
    A.Â NowÃ©, Reinforcement learning in pomdps with memoryless options and option-observation
    initiation sets, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] S.Â Ioffe, C.Â Szegedy, Batch normalization: Accelerating deep network
    training by reducing internal covariate shift, Proceedings of the 32nd International
    Conference on Machine Learning (2015) 448â€“456.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] E.Â VanÂ der Pol, F.Â A. Oliehoek, Coordinated deep reinforcement learners
    for traffic light control, in: Proceedings of Learning, Inference and Control
    of Multi-Agent Systems at NIPS, 2016.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T.Â Salimans, D.Â P. Kingma, Weight normalization: A simple reparameterization
    to accelerate training of deep neural networks, in: Advances in Neural Information
    Processing Systems, 2016, pp. 901â€“909.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] OpenAI Baselines: ACKTR & A2C, [https://openai.com/blog/baselines-acktr-a2c/](https://openai.com/blog/baselines-acktr-a2c/),
    [Online; accessed 29-April-2019] (2017).'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z.Â Wang, V.Â Bapst, N.Â Heess, V.Â Mnih, R.Â Munos, K.Â Kavukcuoglu, N.Â deÂ Freitas,
    Sample efficient actor-critic with experience replay, arXiv preprint arXiv:1611.01224.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] S.Â S. Gu, T.Â Lillicrap, R.Â E. Turner, Z.Â Ghahramani, B.Â SchÃ¶lkopf, S.Â Levine,
    Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
    for deep reinforcement learning, in: Advances in Neural Information Processing
    Systems, 2017, pp. 3846â€“3855.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] E.Â Shelhamer, P.Â Mahmoudieh, M.Â Argus, T.Â Darrell, Loss is its own reward:
    Self-supervision for reinforcement learning, ICLR workshops.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M.Â G. Bellemare, W.Â Dabney, R.Â Dadashi, A.Â A. TaÃ¯ga, P.Â S. Castro, N.Â L.
    Roux, D.Â Schuurmans, T.Â Lattimore, C.Â Lyle, [A Geometric Perspective on Optimal
    Representations for Reinforcement Learning](http://arxiv.org/abs/1901.11530),
    CoRR abs/1901.11530. [arXiv:1901.11530](http://arxiv.org/abs/1901.11530).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.11530](http://arxiv.org/abs/1901.11530)
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[110] R.Â S. Sutton, J.Â Modayil, M.Â Delp, T.Â Degris, P.Â M. Pilarski, A.Â White,
    D.Â Precup, Horde: A scalable real-time architecture for learning knowledge from
    unsupervised sensorimotor interaction, in: The 10th International Conference on
    Autonomous Agents and Multiagent Systems-Volume 2, International Foundation for
    Autonomous Agents and Multiagent Systems, 2011, pp. 761â€“768.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T.Â Schaul, J.Â Quan, I.Â Antonoglou, D.Â Silver, Prioritized Experience
    Replay, in: International Conference on Learning Representations, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A.Â W. Moore, C.Â G. Atkeson, Prioritized sweeping: Reinforcement learning
    with less data and less time, Machine learning 13Â (1) (1993) 103â€“130.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D.Â Andre, N.Â Friedman, R.Â Parr, Generalized prioritized sweeping, in:
    Advances in Neural Information Processing Systems, 1998, pp. 1001â€“1007.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] L.Â Espeholt, H.Â Soyer, R.Â Munos, K.Â Simonyan, V.Â Mnih, T.Â Ward, Y.Â Doron,
    V.Â Firoiu, T.Â Harley, I.Â Dunning, etÂ al., IMPALA: Scalable distributed Deep-RL
    with importance weighted actor-learner architectures, in: International Conference
    on Machine Learning, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J.Â Schulman, F.Â Wolski, P.Â Dhariwal, A.Â Radford, O.Â Klimov, [Proximal
    Policy Optimization Algorithms](http://arxiv.org/abs/1707.06347).'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[116] S.Â M. Kakade, A natural policy gradient, in: Advances in neural information
    processing systems, 2002, pp. 1531â€“1538.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] N.Â Heess, D.Â TB, S.Â Sriram, J.Â Lemmon, J.Â Merel, G.Â Wayne, Y.Â Tassa,
    T.Â Erez, Z.Â Wang, S.Â M.Â A. Eslami, M.Â A. Riedmiller, D.Â Silver, [Emergence of
    Locomotion Behaviours in Rich Environments.](http://arXiv.org/abs/1707.02286v2)'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1707.02286v2](http://arXiv.org/abs/1707.02286v2)
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[118] G.Â Bacchiani, D.Â Molinari, M.Â Patander, Microscopic traffic simulation
    by cooperative multi-agent deep reinforcement learning, in: AAMAS, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] J.Â Schulman, P.Â Abbeel, X.Â Chen, [Equivalence Between Policy Gradients
    and Soft Q-Learning](http://arxiv.org/abs/1704.06440), CoRR abs/1704.06440. [arXiv:1704.06440](http://arxiv.org/abs/1704.06440).'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1704.06440](http://arxiv.org/abs/1704.06440)
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[120] T.Â Haarnoja, H.Â Tang, P.Â Abbeel, S.Â Levine, Reinforcement learning with
    deep energy-based policies, in: Proceedings of the 34th International Conference
    on Machine Learning-Volume 70, 2017, pp. 1352â€“1361.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] T.Â Haarnoja, A.Â Zhou, P.Â Abbeel, S.Â Levine, Soft actor-critic: Off-policy
    maximum entropy deep reinforcement learning with a stochastic actor, in: International
    Conference on Machine Learning, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] M.Â Tan, Multi-Agent Reinforcement Learning: Independent vs. Cooperative
    Agents, in: Machine Learning Proceedings 1993 Proceedings of the Tenth International
    Conference, University of Massachusetts, Amherst, June 27â€“29, 1993, 1993, pp.
    330â€“337.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] G.Â J. Laurent, L.Â Matignon, L.Â Fort-Piat, etÂ al., The world of independent
    learners is not Markovian, International Journal of Knowledge-based and Intelligent
    Engineering Systems 15Â (1) (2011) 55â€“64.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] M.Â L. Littman, Markov games as a framework for multi-agent reinforcement
    learning, in: Proceedings of the 11th International Conference on Machine Learning,
    New Brunswick, NJ, USA, 1994, pp. 157â€“163.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M.Â L. Littman, Value-function reinforcement learning in Markov games,
    Cognitive Systems Research 2Â (1) (2001) 55â€“66.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] C.Â Claus, C.Â Boutilier, The dynamics of reinforcement learning in cooperative
    multiagent systems, in: Proceedings of the 15th National Conference on Artificial
    Intelligence, Madison, Wisconsin, USA, 1998, pp. 746â€“752.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] J.Â Hu, M.Â P. Wellman, Nash Q-learning for general-sum stochastic games,
    The Journal of Machine Learning Research 4 (2003) 1039â€“1069.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M.Â L. Littman, Friend-or-foe Q-learning in general-sum games, in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Williamstown, MA, USA, 2001, pp. 322â€“328.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] X.Â Song, T.Â Wang, C.Â Zhang, Convergence of multi-agent learning with
    a finite step size in general-sum games, in: 18th International Conference on
    Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] D.Â Balduzzi, S.Â Racaniere, J.Â Martens, J.Â Foerster, K.Â Tuyls, T.Â Graepel,
    The mechanics of n-player differentiable games, in: Proceedings of the 35th International
    Conference on Machine Learning, Proceedings of Machine Learning Research, Stockholm,
    Sweden, 2018, pp. 354â€“363.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] J.Â PÃ©rolat, B.Â Piot, O.Â Pietquin, Actor-critic fictitious play in simultaneous
    move multistage games, in: 21st International Conference on Artificial Intelligence
    and Statistics, 2018.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] G.Â Bono, J.Â S. Dibangoye, L.Â Matignon, F.Â Pereyron, O.Â Simonin, Cooperative
    multi-agent policy gradient, in: European Conference on Machine Learning, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] T.Â W. Neller, M.Â Lanctot, An introduction to counterfactual regret minimization,
    in: Proceedings of Model AI Assignments, The Fourth Symposium on Educational Advances
    in Artificial Intelligence (EAAI-2013), 2013.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] M.Â Zinkevich, M.Â Johanson, M.Â Bowling, C.Â Piccione, Regret minimization
    in games with incomplete information, in: Advances in neural information processing
    systems, 2008, pp. 1729â€“1736.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A.Â Blum, Y.Â Monsour, Learning, regret minimization, and equilibria, in:
    Algorithmic Game Theory, Cambridge University Press, 2007, Ch.Â 4.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S.Â Srinivasan, M.Â Lanctot, V.Â Zambaldi, J.Â PÃ©rolat, K.Â Tuyls, R.Â Munos,
    M.Â Bowling, Actor-critic policy optimization in partially observable multiagent
    environments, in: Advances in Neural Information Processing Systems, 2018, pp.
    3422â€“3435.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] E.Â Lockhart, M.Â Lanctot, J.Â PÃ©rolat, J.Â Lespiau, D.Â Morrill, F.Â Timbers,
    K.Â Tuyls, [Computing approximate equilibria in sequential adversarial games by
    exploitability descent](http://arxiv.org/abs/1903.05614), CoRR abs/1903.05614.
    [arXiv:1903.05614](http://arxiv.org/abs/1903.05614).'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.05614](http://arxiv.org/abs/1903.05614)
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[138] M.Â Johanson, N.Â Bard, N.Â Burch, M.Â Bowling, Finding optimal abstract
    strategies in extensive-form games, in: Twenty-Sixth AAAI Conference on Artificial
    Intelligence, 2012.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] E.Â Kalai, E.Â Lehrer, Rational learning leads to Nash equilibrium, Econometrica:
    Journal of the Econometric Society (1993) 1019â€“1045.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] T.Â W. Sandholm, R.Â H. Crites, Multiagent reinforcement learning in the
    iterated prisonerâ€™s dilemma, Biosystems 37Â (1-2) (1996) 147â€“166.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S.Â Singh, M.Â Kearns, Y.Â Mansour, Nash convergence of gradient dynamics
    in general-sum games, in: Proceedings of the Sixteenth conference on Uncertainty
    in artificial intelligence, Morgan Kaufmann Publishers Inc., 2000, pp. 541â€“548.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] B.Â Banerjee, J.Â Peng, Adaptive policy gradient in multiagent learning,
    in: Proceedings of the second international joint conference on Autonomous agents
    and multiagent systems, ACM, 2003, pp. 686â€“692.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A.Â Greenwald, K.Â Hall, Correlated Q-learning, in: Proceedings of 17th
    International Conference on Autonomous Agents and Multiagent Systems, Washington,
    DC, USA, 2003, pp. 242â€“249.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] M.Â Bowling, Convergence problems of general-sum multiagent reinforcement
    learning, in: International Conference on Machine Learning, 2000, pp. 89â€“94.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] M.Â Bowling, Convergence and no-regret in multiagent learning, in: Advances
    in Neural Information Processing Systems, Vancouver, Canada, 2004, pp. 209â€“216.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M.Â Zinkevich, A.Â Greenwald, M.Â L. Littman, Cyclic equilibria in Markov
    games, in: Advances in Neural Information Processing Systems, 2006, pp. 1641â€“1648.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M.Â Bowling, M.Â Veloso, Multiagent learning using a variable learning
    rate, Artificial Intelligence 136Â (2) (2002) 215â€“250.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] M.Â Kaisers, K.Â Tuyls, FAQ-learning in matrix games: demonstrating convergence
    near Nash equilibria, and bifurcation of attractors in the battle of sexes, in:
    AAAI Workshop on Interactive Decision Theory and Game Theory, San Francisco, CA,
    USA, 2011, pp. 309â€“316.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M.Â Wunder, M.Â L. Littman, M.Â Babes, Classes of Multiagent Q-learning
    Dynamics with epsilon-greedy Exploration, in: Proceedings of the 35th International
    Conference on Machine Learning, Haifa, Israel, 2010, pp. 1167â€“1174.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] G.Â Tesauro, Extending Q-learning to general adaptive multi-agent systems,
    in: Advances in Neural Information Processing Systems, Vancouver, Canada, 2003,
    pp. 871â€“878.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] M.Â Weinberg, J.Â S. Rosenschein, Best-response multiagent learning in
    non-stationary environments, in: Proceedings of the 3rd International Conference
    on Autonomous Agents and Multiagent Systems, New York, NY, USA, 2004, pp. 506â€“513.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] D.Â Chakraborty, P.Â Stone, Multiagent learning in the presence of memory-bounded
    agents, Autonomous Agents and Multi-Agent Systems 28Â (2) (2013) 182â€“213.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] G.Â Weiss (Ed.), Â Multiagent Systems, 2nd Edition, (Intelligent Robotics
    and Autonomous Agents series), MIT Press, Cambridge, MA, USA, 2013.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Multiagent Learning, Foundations and Recent Trends, [https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****â£main.bblâ£Lineâ£775â£****](https://www.cs.utexas.edu/~larg/ijcai17_tutorial/multiagent_learning.pdf%****%20main.bbl%20Line%20775%20****),
    [Online; accessed 7-September-2018] (2017).'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] A.Â Tampuu, T.Â Matiisen, D.Â Kodelja, I.Â Kuzovkin, K.Â Korjus, J.Â Aru, J.Â Aru,
    R.Â Vicente, Multiagent cooperation and competition with deep reinforcement learning,
    PLOS ONE 12Â (4) (2017) e0172395.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J.Â Z. Leibo, V.Â Zambaldi, M.Â Lanctot, J.Â Marecki, Multi-agent Reinforcement
    Learning in Sequential Social Dilemmas, in: Proceedings of the 16th Conference
    on Autonomous Agents and Multiagent Systems, Sao Paulo, 2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M.Â Raghu, A.Â Irpan, J.Â Andreas, R.Â Kleinberg, Q.Â Le, J.Â Kleinberg, Can
    Deep Reinforcement Learning solve Erdos-Selfridge-Spencer Games?, in: Proceedings
    of the 35th International Conference on Machine Learning, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] T.Â Bansal, J.Â Pachocki, S.Â Sidor, I.Â Sutskever, I.Â Mordatch, Emergent
    Complexity via Multi-Agent Competition., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J.Â Z. Leibo, J.Â Perolat, E.Â Hughes, S.Â Wheelwright, A.Â H. Marblestone,
    E.Â DuÃ©Ã±ez-GuzmÃ¡n, P.Â Sunehag, I.Â Dunning, T.Â Graepel, Malthusian reinforcement
    learning, in: 18th International Conference on Autonomous Agents and Multiagent
    Systems, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] I.Â Mordatch, P.Â Abbeel, Emergence of grounded compositional language
    in multi-agent populations, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A.Â Lazaridou, A.Â Peysakhovich, M.Â Baroni, Multi-Agent Cooperation and
    the Emergence of (Natural) Language, in: International Conference on Learning
    Representations, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J.Â N. Foerster, Y.Â M. Assael, N.Â DeÂ Freitas, S.Â Whiteson, Learning to
    communicate with deep multi-agent reinforcement learning, in: Advances in Neural
    Information Processing Systems, 2016, pp. 2145â€“2153.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] S.Â Sukhbaatar, A.Â Szlam, R.Â Fergus, Learning Multiagent Communication
    with Backpropagation, in: Advances in Neural Information Processing Systems, 2016,
    pp. 2244â€“2252.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] P.Â Peng, Q.Â Yuan, Y.Â Wen, Y.Â Yang, Z.Â Tang, H.Â Long, J.Â Wang, [Multiagent
    Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games.](http://arxiv.org/abs/1703.10069)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1703.10069](http://arxiv.org/abs/1703.10069)
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[165] E.Â Pesce, G.Â Montana, [Improving coordination in multi-agent deep reinforcement
    learning through memory-driven communication](http://arxiv.org/abs/1901.03887),
    CoRR abs/1901.03887. [arXiv:1901.03887](http://arxiv.org/abs/1901.03887).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.03887](http://arxiv.org/abs/1901.03887)
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[166] S.Â Omidshafiei, J.Â Pazis, C.Â Amato, J.Â P. How, J.Â Vian, Deep Decentralized
    Multi-task Multi-Agent Reinforcement Learning under Partial Observability, in:
    Proceedings of the 34th International Conference on Machine Learning, Sydney,
    2017.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J.Â N. Foerster, G.Â Farquhar, T.Â Afouras, N.Â Nardelli, S.Â Whiteson, Counterfactual
    Multi-Agent Policy Gradients., in: 32nd AAAI Conference on Artificial Intelligence,
    2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] J.Â N. Foerster, N.Â Nardelli, G.Â Farquhar, T.Â Afouras, P.Â H.Â S. Torr,
    P.Â Kohli, S.Â Whiteson, Stabilising Experience Replay for Deep Multi-Agent Reinforcement
    Learning., in: International Conference on Machine Learning, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H.Â He, J.Â Boyd-Graber, K.Â Kwok, H.Â Daume, Opponent modeling in deep reinforcement
    learning, in: 33rd International Conference on Machine Learning, 2016, pp. 2675â€“2684.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] R.Â Raileanu, E.Â Denton, A.Â Szlam, R.Â Fergus, Modeling Others using Oneself
    in Multi-Agent Reinforcement Learning., in: International Conference on Machine
    Learning, 2018.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Z.-W. Hong, S.-Y. Su, T.-Y. Shann, Y.-H. Chang, C.-Y. Lee, A Deep Policy
    Inference Q-Network for Multi-Agent Systems, in: International Conference on Autonomous
    Agents and Multiagent Systems, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M.Â Lanctot, V.Â F. Zambaldi, A.Â Gruslys, A.Â Lazaridou, K.Â Tuyls, J.Â PÃ©rolat,
    D.Â Silver, T.Â Graepel, A Unified Game-Theoretic Approach to Multiagent Reinforcement
    Learning., in: Advances in Neural Information Processing Systems, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] J.Â Heinrich, D.Â Silver, [Deep Reinforcement Learning from Self-Play in
    Imperfect-Information Games](http://arxiv.org/abs/1603.01121).'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1603.01121](http://arxiv.org/abs/1603.01121)
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[174] N.Â C. Rabinowitz, F.Â Perbet, H.Â F. Song, C.Â Zhang, S.Â M.Â A. Eslami, M.Â Botvinick,
    Machine Theory of Mind., in: International Conference on Machine Learning, Stockholm,
    Sweden, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] T.Â Yang, J.Â Hao, Z.Â Meng, C.Â Zhang, Y.Â Z.Â Z. Zheng, Towards Efficient
    Detection and Optimal Response against Sophisticated Opponents, in: IJCAI, 2019.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A.Â Lerer, A.Â Peysakhovich, [Maintaining cooperation in complex social
    dilemmas using deep reinforcement learning](http://arxiv.org/abs/1707.01068),
    CoRR abs/1707.01068. [arXiv:1707.01068](http://arxiv.org/abs/1707.01068).'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1707.01068](http://arxiv.org/abs/1707.01068)
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[177] W.Â Kim, M.Â Cho, Y.Â Sung, Message-Dropout: An Efficient Training Method
    for Multi-Agent Deep Reinforcement Learning, in: 33rd AAAI Conference on Artificial
    Intelligence, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Y.Â Zheng, J.Â Hao, Z.Â Zhang, [Weighted double deep multiagent reinforcement
    learning in stochastic cooperative environments](http://arXiv.org/abs/1802.08534).'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1802.08534](http://arXiv.org/abs/1802.08534)
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[179] M.Â Jaderberg, W.Â M. Czarnecki, I.Â Dunning, L.Â Marris, G.Â Lever, A.Â G.
    CastaÃ±eda, C.Â Beattie, N.Â C. Rabinowitz, A.Â S. Morcos, A.Â Ruderman, N.Â Sonnerat,
    T.Â Green, L.Â Deason, J.Â Z. Leibo, D.Â Silver, D.Â Hassabis, K.Â Kavukcuoglu, T.Â Graepel,
    [Human-level performance in 3d multiplayer games with population-based reinforcement
    learning](https://science.sciencemag.org/content/364/6443/859), Science 364Â (6443)
    (2019) 859â€“865. [arXiv:https://science.sciencemag.org/content/364/6443/859.full.pdf](http://arxiv.org/abs/https://science.sciencemag.org/content/364/6443/859.full.pdf),
    [doi:10.1126/science.aau6249](https://doi.org/10.1126/science.aau6249).'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://science.sciencemag.org/content/364/6443/859](https://science.sciencemag.org/content/364/6443/859)
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[180] P.Â Sunehag, G.Â Lever, A.Â Gruslys, W.Â M. Czarnecki, V.Â F. Zambaldi, M.Â Jaderberg,
    M.Â Lanctot, N.Â Sonnerat, J.Â Z. Leibo, K.Â Tuyls, T.Â Graepel, Value-Decomposition
    Networks For Cooperative Multi-Agent Learning Based On Team Reward., in: Proceedings
    of 17th International Conference on Autonomous Agents and Multiagent Systems,
    Stockholm, Sweden, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] T.Â Rashid, M.Â Samvelyan, C.Â S. deÂ Witt, G.Â Farquhar, J.Â N. Foerster,
    S.Â Whiteson, QMIX - Monotonic Value Function Factorisation for Deep Multi-Agent
    Reinforcement Learning., in: International Conference on Machine Learning, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] J.Â K. Gupta, M.Â Egorov, M.Â Kochenderfer, Cooperative multi-agent control
    using deep reinforcement learning, in: G.Â Sukthankar, J.Â A. Rodriguez-Aguilar
    (Eds.), Autonomous Agents and Multiagent Systems, Springer International Publishing,
    Cham, 2017, pp. 66â€“83.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S.Â Li, Y.Â Wu, X.Â Cui, H.Â Dong, F.Â Fang, S.Â Russell, Robust multi-agent
    reinforcement learning via minimax deep deterministic policy gradient, in: AAAI
    Conference on Artificial Intelligence, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y.Â Zheng, Z.Â Meng, J.Â Hao, Z.Â Zhang, T.Â Yang, C.Â Fan, A Deep Bayesian
    Policy Reuse Approach Against Non-Stationary Agents, in: Advances in Neural Information
    Processing Systems, 2018, pp. 962â€“972.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] R.Â Powers, Y.Â Shoham, Learning against opponents with bounded memory,
    in: Proceedings of the 19th International Joint Conference on Artificial Intelligence,
    Edinburg, Scotland, UK, 2005, pp. 817â€“822.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] M.Â L. Littman, P.Â Stone, Implicit Negotiation in Repeated Games, ATAL
    â€™01: Revised Papers from the 8th International Workshop on Intelligent Agents
    VIII.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] R.Â Axelrod, W.Â D. Hamilton, The evolution of cooperation, Science 211Â (27)
    (1981) 1390â€“1396.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] E.Â MunozÂ de Cote, A.Â Lazaric, M.Â Restelli, Learning to cooperate in multi-agent
    social dilemmas, in: Proceedings of the 5th International Conference on Autonomous
    Agents and Multiagent Systems, Hakodate, Hokkaido, Japan, 2006, pp. 783â€“785.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J.Â L. Stimpson, M.Â A. Goodrich, Learning to cooperate in a social dilemma:
    A satisficing approach to bargaining, in: Proceedings of the 20th International
    Conference on Machine Learning (ICML-03), 2003, pp. 728â€“735.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] G.Â W. Brown, Iterative solution of games by fictitious play, Activity
    analysis of production and allocation 13Â (1) (1951) 374â€“376.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] D.Â Monderer, L.Â S. Shapley, Fictitious play property for games with identical
    interests, Journal of economic theory 68Â (1) (1996) 258â€“265.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] G.Â Tesauro, Temporal difference learning and TD-Gammon, Communications
    of the ACM 38Â (3) (1995) 58â€“68.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] M.Â Bowling, N.Â Burch, M.Â Johanson, O.Â Tammelin, Heads-up limit holdâ€™em
    poker is solved, Science 347Â (6218) (2015) 145â€“149.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J.Â Z. Leibo, E.Â Hughes, M.Â Lanctot, T.Â Graepel, [Autocurricula and the
    emergence of innovation from social interaction: A manifesto for multi-agent intelligence
    research](http://arxiv.org/abs/1903.00742), CoRR abs/1903.00742. [arXiv:1903.00742](http://arxiv.org/abs/1903.00742).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.00742](http://arxiv.org/abs/1903.00742)
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[195] S.Â Samothrakis, S.Â Lucas, T.Â Runarsson, D.Â Robles, Coevolving game-playing
    agents: Measuring performance and intransitivities, IEEE Transactions on Evolutionary
    Computation 17Â (2) (2013) 213â€“226.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] M.Â Bellemare, S.Â Srinivasan, G.Â Ostrovski, T.Â Schaul, D.Â Saxton, R.Â Munos,
    Unifying count-based exploration and intrinsic motivation, in: Advances in Neural
    Information Processing Systems, 2016, pp. 1471â€“1479.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] D.Â E. Moriarty, A.Â C. Schultz, J.Â J. Grefenstette, Evolutionary algorithms
    for reinforcement learning, Journal of Artificial Intelligence Research 11 (1999)
    241â€“276.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] F.Â A. Oliehoek, E.Â D. DeÂ Jong, N.Â Vlassis, The parallel Nash memory for
    asymmetric games, in: Proceedings of the 8th annual conference on Genetic and
    evolutionary computation, ACM, 2006, pp. 337â€“344.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L.Â Bull, T.Â C. Fogarty, M.Â Snaith, Evolution in multi-agent systems:
    Evolving communicating classifier systems for gait in a quadrupedal robot, in:
    Proceedings of the 6th International Conference on Genetic Algorithms, Morgan
    Kaufmann Publishers Inc., 1995, pp. 382â€“388.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] L.Â Bull, Evolutionary computing in multi-agent environments: Operators,
    in: International Conference on Evolutionary Programming, Springer, 1998, pp.
    43â€“52.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H.Â Iba, Emergent cooperation for multiple agents using genetic programming,
    in: International Conference on Parallel Problem Solving from Nature, Springer,
    1996, pp. 32â€“41.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] E.Â Todorov, T.Â Erez, Y.Â Tassa, MuJoCo - A physics engine for model-based
    control, Intelligent Robots and Systems (2012) 5026â€“5033.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] V.Â Gullapalli, A.Â G. Barto, Shaping as a method for accelerating reinforcement
    learning, in: Proceedings of the 1992 IEEE international symposium on intelligent
    control, IEEE, 1992, pp. 554â€“559.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] S.Â Mahadevan, J.Â Connell, Automatic programming of behavior-based robots
    using reinforcement learning, Artificial intelligence 55Â (2-3) (1992) 311â€“365.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] G.Â Konidaris, A.Â Barto, Autonomous shaping: Knowledge transfer in reinforcement
    learning, in: Proceedings of the 23rd international conference on Machine learning,
    ACM, 2006, pp. 489â€“496.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A.Â Y. Ng, D.Â Harada, S.Â J. Russell, Policy invariance under reward transformations:
    Theory and application to reward shaping, in: Proceedings of the Sixteenth International
    Conference on Machine Learning, 1999, pp. 278â€“287.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] P.Â ErdÃ¶s, J.Â L. Selfridge, On a combinatorial game, Journal of Combinatorial
    Theory, Series A 14Â (3) (1973) 298â€“301.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] J.Â Spencer, Randomization, derandomization and antirandomization: three
    games, Theoretical Computer Science 131Â (2) (1994) 415â€“429.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] D.Â Fudenberg, J.Â Tirole, Game Theory, The MIT Press, 1991.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] L.Â v.Â d. Maaten, G.Â Hinton, Visualizing data using t-SNE, Journal of
    machine learning research 9Â (Nov) (2008) 2579â€“2605.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] T.Â Zahavy, N.Â Ben-Zrihem, S.Â Mannor, Graying the black box: Understanding
    DQNs, in: International Conference on Machine Learning, 2016, pp. 1899â€“1908.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] E.Â Beeching, C.Â Wolf, J.Â Dibangoye, O.Â Simonin, [Deep Reinforcement Learning
    on a Budget: 3D Control and Reasoning Without a Supercomputer](http://arxiv.org/abs/1904.01806),
    CoRR abs/1904.01806. [arXiv:1904.01806](http://arxiv.org/abs/1904.01806).'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1904.01806](http://arxiv.org/abs/1904.01806)
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[213] N.Â Srivastava, G.Â Hinton, A.Â Krizhevsky, I.Â Sutskever, R.Â Salakhutdinov,
    Dropout: a simple way to prevent neural networks from overfitting, The Journal
    of Machine Learning Research 15Â (1) (2014) 1929â€“1958.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] M.Â Schuster, K.Â K. Paliwal, Bidirectional recurrent neural networks,
    IEEE Transactions on Signal Processing 45Â (11) (1997) 2673â€“2681.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] R.Â Lowe, J.Â Foerster, Y.-L. Boureau, J.Â Pineau, Y.Â Dauphin, On the pitfalls
    of measuring emergent communication, in: 18th International Conference on Autonomous
    Agents and Multiagent Systems, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] M.Â Tambe, Towards flexible teamwork, Journal of artificial intelligence
    research 7 (1997) 83â€“124.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] B.Â J. Grosz, S.Â Kraus, Collaborative plans for complex group action,
    Artificial Intelligence 86Â (2) (1996) 269â€“357.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] D.Â Precup, R.Â S. Sutton, S.Â Singh, Eligibility traces for off-policy
    policy evaluation, in: Proceedings of the Seventeenth International Conference
    on Machine Learning., 2000.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J.Â Frank, S.Â Mannor, D.Â Precup, Reinforcement learning in the presence
    of rare events, in: Proceedings of the 25th international conference on Machine
    learning, ACM, 2008, pp. 336â€“343.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] T.Â I. Ahamed, V.Â S. Borkar, S.Â Juneja, Adaptive importance sampling technique
    for markov chains using stochastic approximation, Operations Research 54Â (3) (2006)
    489â€“504.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] K.Â A. Ciosek, S.Â Whiteson, Offer: Off-environment reinforcement learning,
    in: Thirty-First AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] D.Â Bloembergen, M.Â Kaisers, K.Â Tuyls, Lenient frequency adjusted Q-learning,
    in: Proceedings of the 22nd Belgian/Netherlands Artificial Intelligence Conference,
    2010.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] L.Â Panait, K.Â Sullivan, S.Â Luke, Lenience towards teammates helps in
    cooperative multiagent learning, in: Proceedings of the 5th International Conference
    on Autonomous Agents and Multiagent Systems, Hakodate, Japan, 2006.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] L.Â Panait, K.Â Tuyls, S.Â Luke, Theoretical advantages of lenient learners:
    An evolutionary game theoretic perspective, JMLR 9Â (Mar) (2008) 423â€“457.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] M.Â Lauer, M.Â Riedmiller, An algorithm for distributed reinforcement learning
    in cooperative multi-agent systems, in: In Proceedings of the Seventeenth International
    Conference on Machine Learning, 2000.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] R.Â Caruana, Multitask learning, Machine learning 28Â (1) (1997) 41â€“75.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] A.Â A. Rusu, S.Â G. Colmenarejo, C.Â Gulcehre, G.Â Desjardins, J.Â Kirkpatrick,
    R.Â Pascanu, V.Â Mnih, K.Â Kavukcuoglu, R.Â Hadsell, Policy Distillation, in: International
    Conference on Learning Representations, 2016.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] G.Â Hinton, O.Â Vinyals, J.Â Dean, Distilling the knowledge in a neural
    network, in: NIPS Deep Learning Workshop, 2014.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] A.Â S. Vezhnevets, S.Â Osindero, T.Â Schaul, N.Â Heess, M.Â Jaderberg, D.Â Silver,
    K.Â Kavukcuoglu, FeUdal Networks for Hierarchical Reinforcement Learning., International
    Conference On Machine Learning.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] P.Â Dayan, G.Â E. Hinton, Feudal reinforcement learning, in: Advances in
    neural information processing systems, 1993, pp. 271â€“278.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] S.Â P. Singh, Transfer of learning by composing solutions of elemental
    sequential tasks, Machine Learning 8Â (3-4) (1992) 323â€“339.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Capture the Flag: the emergence of complex cooperative agents, [https://deepmind.com/blog/capture-the-flag/](https://deepmind.com/blog/capture-the-flag/),
    [Online; accessed 7-September-2018] (2018).'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] C.Â D. Rosin, R.Â K. Belew, New methods for competitive coevolution, Evolutionary
    computation 5Â (1) (1997) 1â€“29.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] J.Â Lehman, K.Â O. Stanley, Exploiting open-endedness to solve problems
    through the search for novelty., in: ALIFE, 2008, pp. 329â€“336.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] M.Â Jaderberg, V.Â Dalibard, S.Â Osindero, W.Â M. Czarnecki, J.Â Donahue,
    A.Â Razavi, O.Â Vinyals, T.Â Green, I.Â Dunning, K.Â Simonyan, etÂ al., [Population
    based training of neural networks](http://arxiv.org/abs/1711.09846).'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1711.09846](http://arxiv.org/abs/1711.09846)
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[236] A.Â E. Elo, The rating of chessplayers, past and present, Arco Pub., 1978.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] R.Â Herbrich, T.Â Minka, T.Â Graepel, TrueSkillâ„¢: a Bayesian skill rating
    system, in: Advances in neural information processing systems, 2007, pp. 569â€“576.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] S.Â Omidshafiei, C.Â Papadimitriou, G.Â Piliouras, K.Â Tuyls, M.Â Rowland,
    J.-B. Lespiau, W.Â M. Czarnecki, M.Â Lanctot, J.Â Perolat, R.Â Munos, <math id="bib.bib238.1.m1.1"
    class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib238.1.m1.1a"><mi
    id="bib.bib238.1.m1.1.1" xref="bib.bib238.1.m1.1.1.cmml">Î±</mi><annotation-xml
    encoding="MathML-Content" id="bib.bib238.1.m1.1b"><ci id="bib.bib238.1.m1.1.1.cmml"
    xref="bib.bib238.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex"
    id="bib.bib238.1.m1.1c">\alpha</annotation></semantics></math>-Rank: Multi-Agent
    Evaluation by Evolution, Scientific Reports 9.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] T.Â Back, Evolutionary algorithms in theory and practice: evolution strategies,
    evolutionary programming, genetic algorithms, Oxford university press, 1996.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] K.Â A. DeÂ Jong, Evolutionary computation: a unified approach, MIT press,
    2006.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] K.Â Tumer, A.Â Agogino, Distributed agent-based air traffic flow management,
    in: Proceedings of the 6th International Conference on Autonomous Agents and Multiagent
    Systems, Honolulu, Hawaii, 2007.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] C.Â Guestrin, D.Â Koller, R.Â Parr, Multiagent planning with factored MDPs,
    in: Advances in neural information processing systems, 2002, pp. 1523â€“1530.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] J.Â R. Kok, N.Â Vlassis, Sparse cooperative Q-learning, in: Proceedings
    of the twenty-first international conference on Machine learning, ACM, 2004, p.Â 61.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] J.Â Castellini, F.Â A. Oliehoek, R.Â Savani, S.Â Whiteson, The Representational
    Capacity of Action-Value Networks for Multi-Agent Reinforcement Learning, in:
    18th International Conference on Autonomous Agents and Multiagent Systems, 2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] P.Â J. Gmytrasiewicz, P.Â Doshi, A framework for sequential planning in
    multiagent settings, Journal of Artificial Intelligence Research 24Â (1) (2005)
    49â€“79.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] J.Â C. Harsanyi, Games with incomplete information played by â€œBayesianâ€
    players, Iâ€“III Part I. The basic model, Management science 14Â (3) (1967) 159â€“182.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] S.Â Barrett, P.Â Stone, S.Â Kraus, A.Â Rosenfeld, Teamwork with Limited Knowledge
    of Teammates., in: Proceedings of the Twenty-Seventh AAAI Conference on Artificial
    Intelligence, Bellevue, WS, USA, 2013, pp. 102â€“108.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] R.Â A. Jacobs, M.Â I. Jordan, S.Â J. Nowlan, G.Â E. Hinton, etÂ al., Adaptive
    mixtures of local experts., Neural computation 3Â (1) (1991) 79â€“87.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] J.Â Heinrich, M.Â Lanctot, D.Â Silver, Fictitious self-play in extensive-form
    games, in: International Conference on Machine Learning, 2015, pp. 805â€“813.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J.Â F. Nash, Equilibrium points in n-person games, Proceedings of the
    National Academy of Sciences 36Â (1) (1950) 48â€“49.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J.Â VonÂ Neumann, O.Â Morgenstern, Theory of games and economic behavior,
    Vol.Â 51, Bull. Amer. Math. Soc, 1945.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] J.Â S. Shamma, G.Â Arslan, Dynamic fictitious play, dynamic gradient play,
    and distributed convergence to Nash equilibria, IEEE Transactions on Automatic
    Control 50Â (3) (2005) 312â€“327.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] W.Â E. Walsh, R.Â Das, G.Â Tesauro, J.Â O. Kephart, Analyzing complex strategic
    interactions in multi-agent systems, AAAI-02 Workshop on Game-Theoretic and Decision-Theoretic
    Agents (2002) 109â€“118.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] M.Â Johanson, K.Â Waugh, M.Â Bowling, M.Â Zinkevich, Accelerating best response
    calculation in large extensive games, in: Twenty-Second International Joint Conference
    on Artificial Intelligence, 2011.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] C.Â F. Camerer, T.-H. Ho, J.-K. Chong, A cognitive hierarchy model of
    games, The Quarterly Journal of Economics 119Â (3) (2004) 861.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] M.Â CostaÂ Gomes, V.Â P. Crawford, B.Â Broseta, Cognition and Behavior in
    Normalâ€“Form Games: An Experimental Study, Econometrica 69Â (5) (2001) 1193â€“1235.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] J.Â Morimoto, K.Â Doya, Robust reinforcement learning, Neural computation
    17Â (2) (2005) 335â€“359.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] L.Â Pinto, J.Â Davidson, R.Â Sukthankar, A.Â Gupta, Robust adversarial reinforcement
    learning, in: Proceedings of the 34th International Conference on Machine Learning-Volume
    70, JMLR. org, 2017, pp. 2817â€“2826.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] R.Â Powers, Y.Â Shoham, T.Â Vu, A general criterion and an algorithmic framework
    for learning in multi-agent systems, Machine Learning 67Â (1-2) (2007) 45â€“76.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] J.Â W. Crandall, M.Â A. Goodrich, Learning to compete, coordinate, and
    cooperate in repeated games using reinforcement learning, Machine Learning 82Â (3)
    (2011) 281â€“314.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] M.Â Johanson, M.Â A. Zinkevich, M.Â Bowling, Computing Robust Counter-Strategies.,
    in: Advances in Neural Information Processing Systems, Vancouver, BC, Canada,
    2007, pp. 721â€“728.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] P.Â McCracken, M.Â Bowling, Safe strategies for agent modelling in games,
    in: AAAI Fall Symposium, 2004, pp. 103â€“110.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] S.Â Damer, M.Â Gini, Safely using predictions in general-sum normal form
    games, in: Proceedings of the 16th Conference on Autonomous Agents and Multiagent
    Systems, Sao Paulo, 2017.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] C.Â Zhang, V.Â Lesser, Multi-agent learning with policy prediction, in:
    Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] P.Â J. Gmytrasiewicz, E.Â H. Durfee, Rational Coordination in Multi-Agent
    Environments, Autonomous Agents and Multi-Agent Systems 3Â (4) (2000) 319â€“350.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C.Â F. Camerer, T.-H. Ho, J.-K. Chong, Behavioural Game Theory: Thinking,
    Learning and Teaching, in: Advances in Understanding Strategic Behavior, New York,
    2004, pp. 120â€“180.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] D.Â Carmel, S.Â Markovitch, Incorporating opponent models into adversary
    search, in: AAAI/IAAI, Vol. 1, 1996, pp. 120â€“125.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] H.Â deÂ Weerd, R.Â Verbrugge, B.Â Verheij, How much does it help to know
    what she knows you know? An agent-based simulation study, Artificial Intelligence
    199-200Â (C) (2013) 67â€“92.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] P.Â Hernandez-Leal, M.Â Kaisers, Towards a Fast Detection of Opponents
    in Repeated Stochastic Games, in: G.Â Sukthankar, J.Â A. Rodriguez-Aguilar (Eds.),
    Autonomous Agents and Multiagent Systems: AAMAS 2017 Workshops, Best Papers, Sao
    Paulo, Brazil, May 8-12, 2017, Revised Selected Papers, 2017, pp. 239â€“257.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] P.Â Hernandez-Leal, M.Â E. Taylor, B.Â Rosman, L.Â E. Sucar, E.Â MunozÂ de
    Cote, Identifying and Tracking Switching, Non-stationary Opponents: a Bayesian
    Approach, in: Multiagent Interaction without Prior Coordination Workshop at AAAI,
    Phoenix, AZ, USA, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] B.Â Rosman, M.Â Hawasly, S.Â Ramamoorthy, Bayesian Policy Reuse, Machine
    Learning 104Â (1) (2016) 99â€“127.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] P.Â Hernandez-Leal, Y.Â Zhan, M.Â E. Taylor, L.Â E. Sucar, E.Â MunozÂ de Cote,
    Efficiently detecting switches against non-stationary opponents, Autonomous Agents
    and Multi-Agent Systems 31Â (4) (2017) 767â€“789.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] P.Â Hernandez-Leal, M.Â Kaisers, Learning against sequential opponents
    in repeated stochastic games, in: The 3rd Multi-disciplinary Conference on Reinforcement
    Learning and Decision Making, Ann Arbor, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] J.Â Schmidhuber, Critique of Paper by â€œDeep Learning Conspiracyâ€ (Nature
    521 p 436), [http://people.idsia.ch/~juergen/deep-learning-conspiracy.html](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html)
    (2015).'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Do I really have to cite an arXiv paper?, [http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/](http://approximatelycorrect.com/2017/08/01/do-i-have-to-cite-arxiv-paper/),
    [Online; accessed 21-May-2019] (2017).'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Collaboration & Credit Principles, How can we be good stewards of collaborative
    trust?, [http://colah.github.io/posts/2019-05-Collaboration/index.html](http://colah.github.io/posts/2019-05-Collaboration/index.html),
    [Online; accessed 31-May-2019] (2019).'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] H.Â Wang, B.Â Raj, E.Â P. Xing, [On the origin of deep learning](http://arxiv.org/abs/1702.07800),
    CoRR abs/1702.07800. [arXiv:1702.07800](http://arxiv.org/abs/1702.07800).'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.07800](http://arxiv.org/abs/1702.07800)
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[278] A.Â K. Agogino, K.Â Tumer, Analyzing and visualizing multiagent rewards
    in dynamic and stochastic domains, Autonomous Agents and Multi-Agent Systems 17Â (2)
    (2008) 320â€“338.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] S.Â Devlin, L.Â M. Yliniemi, D.Â Kudenko, K.Â Tumer, Potential-based difference
    rewards for multiagent reinforcement learning., in: 13th International Conference
    on Autonomous Agents and Multiagent Systems, AAMAS 2014, Paris, France, 2014.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] D.Â H. Wolpert, K.Â R. Wheeler, K.Â Tumer, General principles of learning-based
    multi-agent systems, in: Proceedings of the Third International Conference on
    Autonomous Agents, 1999.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] M.Â E. Taylor, P.Â Stone, Transfer learning for reinforcement learning
    domains: A survey, The Journal of Machine Learning Research 10 (2009) 1633â€“1685.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] C.Â Bucilua, R.Â Caruana, A.Â Niculescu-Mizil, Model compression, in: Proceedings
    of the 12th ACM SIGKDD international conference on Knowledge discovery and data
    mining, ACM, 2006, pp. 535â€“541.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Y.Â Du, W.Â M. Czarnecki, S.Â M. Jayakumar, R.Â Pascanu, B.Â Lakshminarayanan,
    Adapting auxiliary losses using gradient similarity, arXiv preprint arXiv:1812.02224.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] S.Â C. Suddarth, Y.Â Kergosien, Rule-injection hints as a means of improving
    network performance and learning time, in: Neural Networks, Springer, 1990, pp.
    120â€“129.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] P.Â Hernandez-Leal, B.Â Kartal, M.Â E. Taylor, Agent Modeling as Auxiliary
    Task for Deep Reinforcement Learning, in: AAAI Conference on Artificial Intelligence
    and Interactive Digital Entertainment, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] M.Â Andrychowicz, F.Â Wolski, A.Â Ray, J.Â Schneider, R.Â Fong, P.Â Welinder,
    B.Â McGrew, J.Â Tobin, P.Â Abbeel, W.Â Zaremba, Hindsight experience replay, in: Advances
    in Neural Information Processing Systems, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Z.Â C. Lipton, K.Â Azizzadenesheli, A.Â Kumar, L.Â Li, J.Â Gao, L.Â Deng, [Combating
    Reinforcement Learningâ€™s Sisyphean Curse with Intrinsic Fear](http://arxiv.org/abs/1611.01211v8).'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1611.01211v8](http://arxiv.org/abs/1611.01211v8)
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[288] T.Â DeÂ Bruin, J.Â Kober, K.Â Tuyls, R.Â BabuÅ¡ka, Experience selection in
    deep reinforcement learning for control, The Journal of Machine Learning Research
    19Â (1) (2018) 347â€“402.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] D.Â S. Bernstein, R.Â Givan, N.Â Immerman, S.Â Zilberstein, The complexity
    of decentralized control of Markov decision processes, Mathematics of operations
    research 27Â (4) (2002) 819â€“840.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] F.Â A. Oliehoek, C.Â Amato, etÂ al., A concise introduction to decentralized
    POMDPs, Springer, 2016.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] F.Â A. Oliehoek, M.Â T. Spaan, N.Â Vlassis, Optimal and approximate Q-value
    functions for decentralized POMDPs, Journal of Artificial Intelligence Research
    32 (2008) 289â€“353.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] J.Â K. Gupta, M.Â Egorov, M.Â J. Kochenderfer, Cooperative Multi-agent Control
    using deep reinforcement learning, in: Adaptive Learning Agents at AAMAS, Sao
    Paulo, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] R.Â Pascanu, T.Â Mikolov, Y.Â Bengio, On the difficulty of training recurrent
    neural networks, in: International conference on machine learning, 2013, pp. 1310â€“1318.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] K.Â Greff, R.Â K. Srivastava, J.Â Koutnik, B.Â R. Steunebrink, J.Â Schmidhuber,
    LSTM: A Search Space Odyssey, IEEE Transactions on Neural Networks and Learning
    Systems 28Â (10) (2017) 2222â€“2232.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] J.Â Chung, C.Â Gulcehre, K.Â Cho, Y.Â Bengio, Empirical evaluation of gated
    recurrent neural networks on sequence modeling, in: Deep Learning and Representation
    Learning Workshop, 2014.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] S.Â Whiteson, B.Â Tanner, M.Â E. Taylor, P.Â Stone, Protecting against evaluation
    overfitting in empirical reinforcement learning, in: 2011 IEEE Symposium on Adaptive
    Dynamic Programming and Reinforcement Learning (ADPRL), IEEE, 2011, pp. 120â€“127.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] M.Â G. Bellemare, Y.Â Naddaf, J.Â Veness, M.Â Bowling, The arcade learning
    environment: An evaluation platform for general agents, Journal of Artificial
    Intelligence Research 47 (2013) 253â€“279.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] M.Â C. Machado, M.Â G. Bellemare, E.Â Talvitie, J.Â Veness, M.Â Hausknecht,
    M.Â Bowling, Revisiting the arcade learning environment: Evaluation protocols and
    open problems for general agents, Journal of Artificial Intelligence Research
    61 (2018) 523â€“562.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] G.Â Brockman, V.Â Cheung, L.Â Pettersson, J.Â Schneider, J.Â Schulman, J.Â Tang,
    W.Â Zaremba, OpenAI Gym, arXiv preprint arXiv:1606.01540.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] P.Â S. Castro, S.Â Moitra, C.Â Gelada, S.Â Kumar, M.Â G. Bellemare, [Dopamine:
    A Research Framework for Deep Reinforcement Learning](http://arxiv.org/abs/1812.06110).'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1812.06110](http://arxiv.org/abs/1812.06110)
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[301] C.Â Resnick, W.Â Eldridge, D.Â Ha, D.Â Britz, J.Â Foerster, J.Â Togelius, K.Â Cho,
    J.Â Bruna, [Pommerman: A Multi-Agent Playground](http://arxiv.org/abs/1809.07124).'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.07124](http://arxiv.org/abs/1809.07124)
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[302] C.Â Gao, B.Â Kartal, P.Â Hernandez-Leal, M.Â E. Taylor, On Hard Exploration
    for Reinforcement Learning: a Case Study in Pommerman, in: AAAI Conference on
    Artificial Intelligence and Interactive Digital Entertainment, 2019.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] M.Â Samvelyan, T.Â Rashid, C.Â S. deÂ Witt, G.Â Farquhar, N.Â Nardelli, T.Â G.Â J.
    Rudner, C.Â Hung, P.Â H.Â S. Torr, J.Â N. Foerster, S.Â Whiteson, [The StarCraft Multi-Agent
    Challenge](http://arxiv.org/abs/1902.04043), CoRR abs/1902.04043. [arXiv:1902.04043](http://arxiv.org/abs/1902.04043).'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1902.04043](http://arxiv.org/abs/1902.04043)
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[304] D.Â PÃ©rez-LiÃ©bana, K.Â Hofmann, S.Â P. Mohanty, N.Â Kuno, A.Â Kramer, S.Â Devlin,
    R.Â D. Gaina, D.Â Ionita, [The multi-agent reinforcement learning in MalmÃ¶ (MARLÃ–)
    competition](http://arxiv.org/abs/1901.08129), CoRR abs/1901.08129. [arXiv:1901.08129](http://arxiv.org/abs/1901.08129).'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1901.08129](http://arxiv.org/abs/1901.08129)
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[305] M.Â Johnson, K.Â Hofmann, T.Â Hutton, D.Â Bignell, The Malmo platform for
    artificial intelligence experimentation., in: IJCAI, 2016, pp. 4246â€“4247.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] P.Â Stone, G.Â Kaminka, S.Â Kraus, J.Â S. Rosenschein, Ad Hoc Autonomous
    Agent Teams: Collaboration without Pre-Coordination., in: 32nd AAAI Conference
    on Artificial Intelligence, Atlanta, Georgia, USA, 2010, pp. 1504â€“1509.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] M.Â Bowling, P.Â McCracken, Coordination and adaptation in impromptu teams,
    in: Proceedings of the Nineteenth Conference on Artificial Intelligence, Vol.Â 5,
    2005, pp. 53â€“58.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] S.Â V. Albrecht, S.Â Ramamoorthy, A game-theoretic model and best-response
    learning method for ad hoc coordination in multiagent systems, in: Proceedings
    of the 12th International Conference on Autonomous Agents and Multi-agent Systems,
    Saint Paul, MN, USA, 2013.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N.Â Bard, J.Â N. Foerster, S.Â Chandar, N.Â Burch, M.Â Lanctot, H.Â F. Song,
    E.Â Parisotto, V.Â Dumoulin, S.Â Moitra, E.Â Hughes, I.Â Dunning, S.Â Mourad, H.Â Larochelle,
    M.Â G. Bellemare, M.Â Bowling, [The Hanabi Challenge: A New Frontier for AI Research](https://arxiv.org/abs/1902.00506).'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://arxiv.org/abs/1902.00506](https://arxiv.org/abs/1902.00506)
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[310] M.Â Hessel, J.Â Modayil, H.Â VanÂ Hasselt, T.Â Schaul, G.Â Ostrovski, W.Â Dabney,
    D.Â Horgan, B.Â Piot, M.Â Azar, D.Â Silver, Rainbow: Combining improvements in deep
    reinforcement learning, in: Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Y.Â Song, J.Â Wang, T.Â Lukasiewicz, Z.Â Xu, M.Â Xu, Z.Â Ding, L.Â Wu, [Arena:
    A general evaluation platform and building toolkit for multi-agent intelligence](http://arxiv.org/abs/1905.08085),
    CoRR abs/1905.08085. [arXiv:1905.08085](http://arxiv.org/abs/1905.08085).'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1905.08085](http://arxiv.org/abs/1905.08085)
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[312] A.Â Juliani, V.Â Berges, E.Â Vckay, Y.Â Gao, H.Â Henry, M.Â Mattar, D.Â Lange,
    [Unity: A general platform for intelligent agents](http://arxiv.org/abs/1809.02627),
    CoRR abs/1809.02627. [arXiv:1809.02627](http://arxiv.org/abs/1809.02627).'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.02627](http://arxiv.org/abs/1809.02627)
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[313] S.Â Liu, G.Â Lever, J.Â Merel, S.Â Tunyasuvunakool, N.Â Heess, T.Â Graepel,
    Emergent coordination through competition, in: International Conference on Learning
    Representations, 2019.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] J.Â Suarez, Y.Â Du, P.Â Isola, I.Â Mordatch, [Neural MMO: A massively multiagent
    game environment for training and evaluating intelligent agents](http://arxiv.org/abs/1903.00784),
    CoRR abs/1903.00784. [arXiv:1903.00784](http://arxiv.org/abs/1903.00784).'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1903.00784](http://arxiv.org/abs/1903.00784)
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[315] P.Â Henderson, R.Â Islam, P.Â Bachman, J.Â Pineau, D.Â Precup, D.Â Meger, Deep
    Reinforcement Learning That Matters., in: 32nd AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] P.Â Nagarajan, G.Â Warnell, P.Â Stone, [Deterministic implementations for
    reproducibility in deep reinforcement learning](http://arxiv.org/abs/1809.05676).'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1809.05676](http://arxiv.org/abs/1809.05676)
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[317] K.Â Clary, E.Â Tosch, J.Â Foley, D.Â Jensen, Letâ€™s play again: Variability
    of deep reinforcement learning agents in Atari environments, in: NeurIPS Critiquing
    and Correcting Trends Workshop, 2018.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] J.Â Z. Forde, M.Â Paganini, The scientific method in the science of machine
    learning, in: ICLR Debugging Machine Learning Models workshop, 2019.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] K.Â Azizzadenesheli, Maybe a few considerations in reinforcement learning
    research?, in: Reinforcement Learning for Real Life Workshop, 2019.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Z.Â C. Lipton, J.Â Steinhardt, Troubling trends in machine learning scholarship,
    in: ICML Machine Learning Debates workshop, 2018.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] R.Â Rosenthal, The file drawer problem and tolerance for null results.,
    Psychological bulletin 86Â (3) (1979) 638.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] D.Â Sculley, J.Â Snoek, A.Â Wiltschko, A.Â Rahimi, Winnerâ€™s curse? on pace,
    progress, and empirical rigor, in: ICLR Workshop, 2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] O.Â Gencoglu, M.Â van Gils, E.Â Guldogan, C.Â Morikawa, M.Â SÃ¼zen, M.Â Gruber,
    J.Â Leinonen, H.Â Huttunen, Hark side of deep learningâ€“from grad student descent
    to automated machine learning, arXiv preprint arXiv:1904.07633.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] K.Â Azizzadenesheli, B.Â Yang, W.Â Liu, E.Â Brunskill, Z.Â Lipton, A.Â Anandkumar,
    Surprising negative results for generative adversarial tree search, in: Critiquing
    and Correcting Trends in Machine Learning Workshop, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] C.Â Lyle, P.Â S. Castro, M.Â G. Bellemare, A comparative analysis of expected
    and distributional reinforcement learning, in: Thirty-Third AAAI Conference on
    Artificial Intelligence, 2019.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] B.Â Kartal, P.Â Hernandez-Leal, M.Â E. Taylor, Using Monte Carlo tree search
    as a demonstrator within asynchronous deep RL, in: AAAI Workshop on Reinforcement
    Learning in Games, 2019.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] G.Â Melis, C.Â Dyer, P.Â Blunsom, On the state of the art of evaluation
    in neural language models, in: International Conference on Learning Representations,
    2018.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Deep Reinforcement Learning: Pong from Pixels, [https://karpathy.github.io/2016/05/31/rl/](https://karpathy.github.io/2016/05/31/rl/),
    [Online; accessed 7-May-2019] (2016).'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] V.Â Firoiu, W.Â F. Whitney, J.Â B. Tenenbaum, [Beating the Worldâ€™s Best
    at Super Smash Bros. with Deep Reinforcement Learning](http://arxiv.org/abs/1702.06230),
    CoRR abs/1702.06230. [arXiv:1702.06230](http://arxiv.org/abs/1702.06230).'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1702.06230](http://arxiv.org/abs/1702.06230)
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[330] C.Â Gao, P.Â Hernandez-Leal, B.Â Kartal, M.Â E. Taylor, Skynet: A Top Deep
    RL Agent in the Inaugural Pommerman Team Competition, in: 4th Multidisciplinary
    Conference on Reinforcement Learning and Decision Making, 2019.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] D.Â Amodei, D.Â Hernandez, [AI and Compute](https://blog.%20openai.%20com/ai-and-compute)
    (2018).'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [https://blog.openai.com/ai-and-compute](https://blog.openai.com/ai-and-compute)
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[332] Y.Â Yu, Towards sample efficient reinforcement learning., in: IJCAI, 2018,
    pp. 5739â€“5743.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] F.Â P. Such, V.Â Madhavan, E.Â Conti, J.Â Lehman, K.Â O. Stanley, J.Â Clune,
    [Deep neuroevolution: Genetic algorithms are a competitive alternative for training
    deep neural networks for reinforcement learning](http://arxiv.org/abs/1712.06567),
    CoRR abs/1712.06567.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1712.06567](http://arxiv.org/abs/1712.06567)
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[334] A.Â Stooke, P.Â Abbeel, [Accelerated methods for deep reinforcement learning](http://arxiv.org/abs/1803.02811),
    CoRR abs/1803.02811. [arXiv:1803.02811](http://arxiv.org/abs/1803.02811).'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1803.02811](http://arxiv.org/abs/1803.02811)
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[335] M.Â Babaeizadeh, I.Â Frosio, S.Â Tyree, J.Â Clemons, J.Â Kautz, Reinforcement
    learning through asynchronous advantage actor-critic on a GPU, in: International
    Conference on Learning Representations, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] W.Â H. Guss, C.Â Codel, K.Â Hofmann, B.Â Houghton, N.Â Kuno, S.Â Milani, S.Â P.
    Mohanty, D.Â P. Liebana, R.Â Salakhutdinov, N.Â Topin, M.Â Veloso, P.Â Wang, [The MineRL
    Competition on Sample Efficient Reinforcement Learning using Human Priors](http://arxiv.org/abs/1904.10079),
    CoRR abs/1904.10079. [arXiv:1904.10079](http://arxiv.org/abs/1904.10079).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1904.10079](http://arxiv.org/abs/1904.10079)
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[337] G.Â Cuccu, J.Â Togelius, P.Â CudrÃ©-Mauroux, Playing Atari with six neurons,
    in: Proceedings of the 18th International Conference on Autonomous Agents and
    MultiAgent Systems, International Foundation for Autonomous Agents and Multiagent
    Systems, 2019, pp. 998â€“1006.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] A.Â Ecoffet, J.Â Huizinga, J.Â Lehman, K.Â O. Stanley, J.Â Clune, Go-explore:
    a new approach for hard-exploration problems, arXiv preprint arXiv:1901.10995.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] R.Â I. Brafman, M.Â Tennenholtz, R-max-a general polynomial time algorithm
    for near-optimal reinforcement learning, Journal of Machine Learning Research
    3Â (Oct) (2002) 213â€“231.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] A.Â L. Strehl, M.Â L. Littman, An analysis of model-based interval estimation
    for Markov decision processes, Journal of Computer and System Sciences 74Â (8)
    (2008) 1309â€“1331.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] J.Â Schmidhuber, A possibility for implementing curiosity and boredom
    in model-building neural controllers, in: Proc. of the international conference
    on simulation of adaptive behavior: From animals to animats, 1991, pp. 222â€“227.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] A.Â G. Barto, Intrinsic motivation and reinforcement learning, in: Intrinsically
    motivated learning in natural and artificial systems, Springer, 2013, pp. 17â€“47.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] T.Â D. Kulkarni, K.Â Narasimhan, A.Â Saeedi, J.Â Tenenbaum, Hierarchical
    deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,
    in: Advances in neural information processing systems, 2016, pp. 3675â€“3683.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] T.Â G. Dietterich, Ensemble Methods in Machine Learning, in: MCS Proceedings
    of the First International Workshop on Multiple Classifier Systems, Springer Berlin
    Heidelberg, Cagliari, Italy, 2000, pp. 1â€“15.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] J.Â A. Arjona-Medina, M.Â Gillhofer, M.Â Widrich, T.Â Unterthiner, S.Â Hochreiter,
    [RUDDER: Return Decomposition for Delayed Rewards](http://arxiv.org/abs/1806.07857).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.07857](http://arxiv.org/abs/1806.07857)
  id: totrans-681
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[346] V.Â Conitzer, T.Â Sandholm, AWESOME: A general multiagent learning algorithm
    that converges in self-play and learns a best response against stationary opponents,
    Machine Learning 67Â (1-2) (2006) 23â€“43.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] C.Â B. Browne, E.Â Powley, D.Â Whitehouse, S.Â M. Lucas, P.Â I. Cowling, P.Â Rohlfshagen,
    S.Â Tavener, D.Â Perez, S.Â Samothrakis, S.Â Colton, A survey of Monte Carlo tree
    search methods, IEEE Transactions on Computational Intelligence and AI in games
    4Â (1) (2012) 1â€“43.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] T.Â Vodopivec, S.Â Samothrakis, B.Â Ster, On Monte Carlo tree search and
    reinforcement learning, Journal of Artificial Intelligence Research 60 (2017)
    881â€“936.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] B.Â Kartal, J.Â Godoy, I.Â Karamouzas, S.Â J. Guy, Stochastic tree search
    with useful cycles for patrolling problems, in: Robotics and Automation (ICRA),
    2015 IEEE International Conference on, IEEE, 2015, pp. 1289â€“1294.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] B.Â Kartal, E.Â Nunes, J.Â Godoy, M.Â Gini, Monte Carlo tree search with
    branch and bound for multi-robot task allocation, in: The IJCAI-16 Workshop on
    Autonomous Mobile Service Robots, 2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[351] G.Â Best, O.Â M. Cliff, T.Â Patten, R.Â R. Mettu, R.Â Fitch, Dec-MCTS: Decentralized
    planning for multi-robot active perception, The International Journal of Robotics
    Research 38Â (2-3) (2019) 316â€“337.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[352] Y.-M. DeÂ Hauwere, P.Â Vrancx, A.Â Nowe, Learning multi-agent state space
    representations, in: Proceedings of the 9th International Conference on Autonomous
    Agents and Multiagent Systems, Toronto, Canada, 2010, pp. 715â€“722.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[353] C.Â Guestrin, M.Â Lagoudakis, R.Â Parr, Coordinated reinforcement learning,
    in: ICML, Vol.Â 2, 2002, pp. 227â€“234.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[354] C.Â Guestrin, D.Â Koller, R.Â Parr, S.Â Venkataraman, Efficient solution
    algorithms for factored MDPs, Journal of Artificial Intelligence Research 19 (2003)
    399â€“468.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[355] C.Â Amato, F.Â A. Oliehoek, Scalable Planning and Learning for Multiagent
    POMDPs, in: AAAI, 2015, pp. 1995â€“2002.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[356] F.Â A. Oliehoek, Interactive Learning and Decision Making - Foundations,
    Insights & Challenges., International Joint Conference on Artificial Intelligence.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[357] F.Â A. Oliehoek, S.Â Whiteson, M.Â T. Spaan, Approximate solutions for factored
    Dec-POMDPs with many agents, in: Proceedings of the 2013 international conference
    on Autonomous agents and multi-agent systems, International Foundation for Autonomous
    Agents and Multiagent Systems, 2013, pp. 563â€“570.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[358] R.Â Becker, S.Â Zilberstein, V.Â Lesser, C.Â V. Goldman, Solving transition
    independent decentralized Markov decision processes, Journal of Artificial Intelligence
    Research 22 (2004) 423â€“455.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[359] F.Â A. Oliehoek, S.Â J. Witwicki, L.Â P. Kaelbling, Influence-based abstraction
    for multiagent systems, in: Twenty-Sixth AAAI Conference on Artificial Intelligence,
    2012.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[360] M.Â Suau de Castro, E.Â Congeduti, R.Â A. Starre, A.Â Czechowski, F.Â A. Oliehoek,
    Influence-based abstraction in deep reinforcement learning, in: Adaptive, Learning
    Agents workshop, 2019.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[361] E.Â Wei, D.Â Wicke, D.Â Freelan, S.Â Luke, [Multiagent Soft Q-Learning](http://arXiv.org/abs/1804.09817).'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arXiv.org/abs/1804.09817](http://arXiv.org/abs/1804.09817)
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[362] R.Â R. Torrado, P.Â Bontrager, J.Â Togelius, J.Â Liu, D.Â Perez-Liebana, [Deep
    Reinforcement Learning for General Video Game AI](http://arxiv.org/abs/1806.02448).'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1806.02448](http://arxiv.org/abs/1806.02448)
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[363] P.Â A. Ortega, S.Â Legg, [Modeling friends and foes](http://arxiv.org/abs/1807.00196).'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL [http://arxiv.org/abs/1807.00196](http://arxiv.org/abs/1807.00196)
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[364] Y.Â Yang, R.Â Luo, M.Â Li, M.Â Zhou, W.Â Zhang, J.Â Wang, Mean field multi-agent
    reinforcement learning, in: Proceedings of the 35th International Conference on
    Machine Learning, Stockholm Sweden, 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[365] A.Â Grover, M.Â Al-Shedivat, J.Â K. Gupta, Y.Â Burda, H.Â Edwards, Learning
    Policy Representations in Multiagent Systems., in: International Conference on
    Machine Learning, 2018.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[366] C.Â K. Ling, F.Â Fang, J.Â Z. Kolter, What game are we playing? end-to-end
    learning in normal and extensive form games, in: Twenty-Seventh International
    Joint Conference on Artificial Intelligence, 2018.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[367] S.Â Omidshafiei, D.Â Hennes, D.Â Morrill, R.Â Munos, J.Â Perolat, M.Â Lanctot,
    A.Â Gruslys, J.-B. Lespiau, K.Â Tuyls, Neural Replicator Dynamics, arXiv e-prints
    (2019) arXiv:1906.00190[arXiv:1906.00190](http://arxiv.org/abs/1906.00190).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[368] S.Â Khadka, S.Â Majumdar, K.Â Tumer, Evolutionary Reinforcement Learning
    for Sample-Efficient Multiagent Coordination, arXiv e-prints (2019) arXiv:1906.07315[arXiv:1906.07315](http://arxiv.org/abs/1906.07315).'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[â—„](/html/1810.05586) [![ar5iv homepage](img/ed0f3cf5a019c4f8e48e41de62929bb0.png)](/)
    [Feeling'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: lucky?](/feeling_lucky) [Conversion
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: report](/log/1810.05587) [Report
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: an issue](https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1810.05587)
    [ViewÂ original
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: onÂ arXiv](https://arxiv.org/abs/1810.05587)[â–º](/html/1810.05588)[](javascript:toggleColorScheme()
    "Toggle ar5iv color scheme")[Copyright](https://arxiv.org/help/license) [Privacy
    Policy](https://arxiv.org/help/policies/privacy_policy)Generated on Tue Mar 19
    07:19:24 2024 by [LaTeXML![Mascot Sammy](img/70e087b9e50c3aa663763c3075b0d6c5.png)](http://dlmf.nist.gov/LaTeXML/)
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
